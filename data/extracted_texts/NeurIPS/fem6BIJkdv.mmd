# Representation Learning via Consistent Assignment of Views over Random Partitions

Thalles Silva

Institute of Computing

University of Campinas

thalles.silva@students.ic.unicamp.br

&Adin Ramirez Rivera

Department of Informatics

University of Oslo

adinr@uio.no

###### Abstract

We present Consistent Assignment of Views over Random Partitions (CARP), a self-supervised clustering method for representation learning of visual features. CARP learns prototypes in an end-to-end online fashion using gradient descent without additional non-differentiable modules to solve the cluster assignment problem. CARP optimizes a new pretext task based on random partitions of prototypes that regularizes the model and enforces consistency between views' assignments. Additionally, our method improves training stability and prevents collapsed solutions in joint-embedding training. Through an extensive evaluation, we demonstrate that CARP's representations are suitable for learning downstream tasks. We evaluate CARP's representations capabilities in 17 datasets across many standard protocols, including linear evaluation, few-shot classification, \(k\)-NN, \(k\)-means, image retrieval, and copy detection. We compare CARP performance to 11 existing self-supervised methods. We extensively ablate our method and demonstrate that our proposed random partition pretext task improves the quality of the learned representations by devising multiple random classification tasks. In transfer learning tasks, CARP achieves the best performance on average against many SSL methods trained for a longer time.

+
Footnote †: Code at https://sthalles.github.io/carp/.

## 1 Introduction

Learning from unlabeled data has been one of the main challenges in computer vision. Recent approaches based on self-supervised learning (SSL) have significantly reduced the gap between supervised and unsupervised pre-trained representations. Nowadays, self-supervised pre-training on vast quantities of unlabeled data, prior to learning a downstream supervised task of interest, can be more effective than supervised pre-training for many tasks .

Current SSL methods can be divided into two classes: (1) self-supervised embedding prediction  and (2) clustering . As the name suggests, embedding prediction methods work directly in the representation space. They are trained with either contrastive or non-contrastive loss functions. Instead of reconstructing the input signal, their loss function maximizes agreement between embeddings of the same view and optionally pushes representations from different views apart. On the other hand, clustering methods discretize the representation space by learning a finite set of prototypes. These prototypes aggregate representations from different images that are similar enough to be assigned together. Nevertheless, recent SSL methods build a joint-embedding architecture that can be pure siamese  or use different encoders.

The most challenging and significant difference among these methods is how they avoid trivial solutions when training joint-embedding architectures. Contrastive methods avoid trivial solutions by explicitly pushing representations of negative samples away from the anchor representation, while non-contrastive methods avoid trivial solutions by regularization or architectural designs [10; 22].

Among self-supervised clustering methods, recent work proposed avoiding trivial solutions by using non-differentiable modules such as the Sinkhorn-Knopp algorithm [1; 7; 14] and classic machine learning methods [29; 42] such as \(k\)-Means clustering or \(k\)-Nearest Neighbor, to solve the cluster assignment problem.

Consistent assignments [37; 38] were recently proposed as a way to learn prototypes to improve the representations. Similarly, our learning objective imposes two constraints (1) consistent assignment of views over learnable prototypes and (2) a uniform distribution for the average predictions within a batch. However, we show that such a strategy does not scale well to enormous datasets containing millions of classes. In such situations, we need to model a large number of prototypes while enforcing consistent assignment between views and avoiding collapsed solutions. We show that a naive implementation of this strategy makes the learning problem challenging from a training stability perspective, where the model quickly settles for a trivial solution by assigning all views' embeddings to the same prototype.

To overcome these issues, we propose a novel self-supervised approach based on the consistent assignment of views over random partition sets (CARP). We train CARP to minimize a consistency loss, which encourages the model to assign different views of the same unlabeled example to the same prototype. We solve the dimensionality problem by enforcing smaller pseudo-classification problems through the introduction of random partitions that enforce consistency and regularize the model. The energy between the views' representations and the trainable prototypes (within random partitions) allows us to automatically bootstrap predictions and targets to our consistency loss. Our contributions are three-fold:

1. A novel and entirely online joint-embedding learning strategy based on self-supervised clustering, see Figure 1. We propose a divide-and-conquer pretext task based on randomly generated partitions of learnable prototypes. Our loss function allows stable training of joint-embedding architectures in a self-supervised context.
2. A framework that simplifies self-supervised training and does not require normalization techniques [7; 9] or the necessity of mining negatives for contrastive training [9; 31; 32].
3. A differentiable assigner module that generates soft pseudo-labels by comparing the representations of image views to prototypes within random partition subsets. To avoid trivial solutions, we enforce the average predictions over a batch to be non-informative over the set of prototypes within a random subset.

## 2 Related work

**Self-supervised embedding prediction methods** operate directly in the representation space by learning a metric such that embeddings from views of the same image are closer to one another while embeddings from views of different images are far away in the feature space. These methods can be trained using contrastive or non-contrastive loss functions. Methods that minimize a loss function with a contrastive term date back to 1990s [4; 13; 21]. They must explicitly find representations from non-correlated images to use as negatives. Recent contrastive methods include InstDisc , CPC , SimCLR  and MoCo [11; 24]. These methods learn unsupervised representations by minimizing nearly the same contrastive loss function, i.e., the InfoNCE . CARP does not directly optimize the views' embeddings, nor is it a contrastive method. Instead, we learn a set of general prototypes using a random partition strategy that stabilizes the learning process and avoids trivial solutions commonly found when training joint-embedding SSL models.

On the other hand, non-contrastive methods work by approximating embeddings of views from the same image. The main advantage is not requiring explicit opposing representations in the loss formulation. To avoid trivial solutions, a common approach is to implement a "stop-gradient" operation that prevents the gradient signal from flowing to the two branches of the join-embedding architecture simultaneously. BYOL  and SimSiam  are examples of such methods. CARP takes advantage of non-contrastive training since it does not require mining negatives for optimization. Also, different from Grill et al.'s  work, CARP does not require a momentum encoder, though using it significantly improves the learned representations. CARP trains a joint-embedding architecture and uses the "stop-gradient" operation in conjunction with a regularized pretext task based on random partitions of prototypes to avoid mode collapse.

**Self-supervised clustering methods** do not work directly on the views' embeddings. Instead, they learn a set of prototypes that are used to solve subsequent pretext tasks in different ways. Caron et al. , for instance, used \(k\)-Means clustering at every epoch to cluster the representations from the entire dataset and produce pseudo-labels for the training images. Then, a classifier head is trained to predict the pseudo-labels. Caron et al.  proposed a method to combine the rotation prediction pretext task  with clustering. Li et al.  presented a method based on expectation-maximization (EM) that merges clustering with the contrastive learning framework from He et al. . Recent work by Caron et al.  and Asano et al.  combine SSL with clustering. They utilize the non-differentiable Sinkhorn-Knopp algorithm to solve the cluster assignment problem without falling into collapsed solutions. Silva and Ramirez Rivera  proposed, CARL, an online clustering method that does not require non-differential algorithms to avoid trivial solutions.

**Contrast to previous approaches.** Instead of solving the cluster assignment problem using a non-differentiable algorithm such as the Sinkhorn-Knopp, CARP is trained end-to-end with gradient descent. Different from Caron et al.'s  work, our model does not require extra momentum encoders or data structures to store previous predictions as a way to avoid trivial solutions. Unlike CARL [37; 38], CARP avoids trivial solutions by posing the optimization problem at the level of random partitions of prototypes that solve the high-dimensionality nature of the task. Unlike Caron et al.'s  work, our method does not require clustering the entire dataset every epoch to generate pseudo-labels. Instead, CARP generates soft pseudo-labels in an online fashion from examples in a single minibatch.

Currently, the best self-supervised methods [8; 12] use vision transformers [15; 44] as their backbones. Recent methods  employ a masking pretext task where some patches of the views are manually hidden, and the network is tasked to infer the missing pieces. In this paper, we consider transformer-based methods to be out-of-scope of our compared backbones. Thus, we do not include them in our results to maintain a fair comparison.

## 3 Consistent Assignment of Views

From an image \(x_{i}\), we create two views, \(_{i}^{1}=T(x_{i})\) and \(_{i}^{2}=T(x_{i})\), using a stochastic function \(T\) that applies a set of random image transformations to \(x_{i}\) (cf. Appendix C.1). CARP is a joint-embedding architecture with two modules: a differentiable (student) and a non-differentiable (teacher)

Figure 1: CARP’s training architecture. From two views, we train an encoder \(f_{()}\), followed by a projection \(g_{()}\) to produce representation vectors \(z^{v}\) and \(w^{v}\), respective to the parameters \(\) and \(\) for each branch, for each view indexed by \(v\). The representations are fed to an assigner function \(q_{()}\) that produces normalized distributions of views w.r.t. the learnable prototypes. Note that \(\) are the trainable weights, and \(\) are an exponential moving average of \(\). We create partitions by randomly arranging the prototypes into a predefined number of blocks. E.g., from a total of \(K=6\) prototypes, we create \(N_{}=2\) blocks, each containing \(N_{B}=3\) prototypes. Then, we enforce consistent assignment of views over prototypes within the blocks.

branch. Each module has its own set of weights and the same architectural design. Both contain an encoder \(f_{()}\) and a projection head \(g_{()}\). The differentiable student receives a pair of views and produces embedding vectors \(z_{i}^{v}=g_{}(f_{}(_{i}^{v}))\), for \(v\{1,2\}\). Similarly, the non-differentiable teacher produces target embeddings \(w_{i}^{v}=g_{}(f_{}(_{i}^{v}))\).

The objective is to learn a set of prototype vectors \(C\) to discretize the embedding space. These prototypes are not meant to represent the true classes of the data. Instead, they may be interpreted as anchors to attract views of a given image to a commonplace in the embedding space. The function \(q(,)\) receives the views' representations, \(z_{i}^{v}\) and \(w_{i}^{v}^{1 d}\), as input and outputs normalized probability vectors relating the views' embeddings with the prototypes such that \(s_{i}^{v}=q(z_{i}^{v},C)\) and \(t_{i}^{v}=q(w_{i}^{v},C)\), where \(s_{i}^{v}\) and \(t_{i}^{v}^{1 K}\) are the normalized probabilities of a view, \(_{i}^{v}\), w.r.t. the prototypes \(C^{K d}\). Note that \(d\) is the dimensionality of the embedding vector, \(K\) is the number of prototypes, and the assigner \(q(h,C)=(h C^{T})\).

To avoid trivial solutions in the joint-embedding training, we need a loss function that prevents the assignment of all representation vectors \(z_{i}\) to a unique prototype. Unlike previous work, we seek a method that solves the cluster assignment problem in an online fashion using gradient descent.

We propose a loss function composed of two terms: consistency and entropy. The consistency term learns the relations between embedding vectors and prototypes. It enforces different views of the same image to be assigned to the same prototype with high confidence. For normalized probability vectors \(a\) and \(b\), we define the consistency term as

\[_{c}(a,b)=- a,b,\] (1)

where \(,\) is a dot product.

The consistency loss is optimized when the two views \(_{i}^{1}\) and \(_{i}^{2}\) are assigned to the same prototype with maximal confidence, i.e., when the probability distributions of the two views \(s_{i}^{1}\) and \(s_{i}^{2}\) resemble equal one-hot vectors.

If we optimize the consistency loss, \(_{c}\), training collapses to a state where all views are assigned to the same prototype. A common approach to avoid such failure is to ensure that all the prototypes get roughly the same number of assignments. Let us define the function

\[(\{(a_{i},b_{i})\}_{i=1}^{L})= _{i=1}^{L}a_{i}+b_{i}\] (2)

as the average probability across the representations within a batch of size \(L\). For our distributions, we define \(=(\{(s_{i}^{v},t_{i}^{v})\}_{i=1}^{N})\). If we maximize the entropy of the mean probabilities of a batch, \(H()\), we will encourage the average predictions to be closer to a uniform distribution. Previous work  has used this entropy term in various scenarios, ranging from discriminative unsupervised clustering to semi-supervised learning. In our case, this term enforces a non-informative prior over the prototypes with a particular schedule learning. Thus, the final proposed objective to

Figure 2: Instead of posing a pseudo-classification problem overall prototypes (circles and squares), the views are assigned (colored dashed lines) to a subset of prototypes (blocks), devising multiple pseudo-classification problems. Then, we contrast their distributions of assignments. (For this example, \(K=8\), \(N_{B}=4\) with \(N_{}=2\).)

minimize is

\[=_{i}^{N}(_{c}(s_{i}^{1},t_{i}^{2})+ _{c}(s_{i}^{2},t_{i}^{1}))-_{e}H(),\] (3)

where \(_{e}>0\) trades off consistency at the view level with the average uniform assignment at the batch level. Note that the contribution of the entropy term decays as training progresses.

### Limitations of Consistent Assignments

The formulation of the consistent assignments (3) is similar to CARL [37; 38], except for the additional teacher stream that stabilizes training and improves performance. Nevertheless, training an unsupervised system by minimizing the loss (3) is challenging. The main limitation is how to avoid trivial solutions in unsupervised training of joint-embedding architectures. For some cases, tuning the contribution of the entropy term might be enough to optimize the loss (3) stably. However, adjusting such a hyperparameter is difficult because one configuration does not hold for all training conditions. For instance, if the value of \(_{e}\) is too small, the consistency term wins the arms race, and the average distribution over the batch, \(H()\) becomes one-hot alike, i.e., all views end up assigned to the same prototype. If the value of \(_{e}\) is too large, the entropy term gets the upper hand, and collapse is avoided. However, the process of view assignment is neglected over the policy of distributing views uniformly, which results in poor performance of the learned representations, as shown by Table B.2.

For a small number of general prototypes, training is more stable, and the model avoids collapse with a simple tuning of the \(_{e}\) parameter. However, for a larger number of general prototypes, stability becomes an issue. The main problem lies with the entropy term. When the distribution is larger, i.e., \(K N\), regular batch sizes, such as \(N=64\) or \(N=128\), become too small to properly model the distribution, i.e., the signal is too weak for most prototypes. Consequently, to avoid collapse, we need to increase the strength of the entropy term or increase the batch size, which in turn decreases performance.

To address such limitation, we propose to decouple the loss function (3) into smaller sub-problems. Instead of enforcing both consistency and uniform assignments over all the general prototypes, we propose a pretext task over subsets or blocks of a random partition of the general prototype set \(C\).

## 4 Assignment based on Random Partitions

Given the set of \(K\) trainable prototypes \(C=\{c_{1},c_{2},...,c_{K}\}\), we define a partition of \(C\) as \(=\{B_{i} C\}_{i=1}^{N_{}}\), such that \(\), \(_{i}B_{i}=\) where \(B_{i}\), and \(B_{i} B_{j}=\) for all \(B_{i},B_{j}\), and \(i j\). We refer to \(B_{i}\) as a block or subset of the partition. We are interested in a partition set \(=\{B_{i}\}_{i=1}^{N_{}}\) of size \(N_{}\), i.e., \(||=N_{}\).

Using the concept of a partition of a set, we can define a framework of pretext tasks over partition blocks that satisfies the learning problem defined in Section 3. If the size of a partition block, \(B_{i}\), equals the number of prototypes, \(N_{B}=K\) then the partition \(\) is trivial, i.e., \(=\{B_{1}\}=\{C\}\). If the size of the partition blocks equals \(N_{B}=1\), then we have \(K\) blocks in \(\), and each block has a unique prototype. Here, the learning task is equivalent to multiple binary classification problems, where each output score, if normalized, expresses the likelihood of a data point \(x_{i}\) to independently belong to each prototype.

However, if the block size \(1<N_{B}<K\), and \(N_{B}\) divides \(K\), then the partition \(\) will be composed of \(N_{}= K/N_{P}\) blocks. We define \(\) by randomly assigning \(N_{B}\) prototypes \(c_{j}\), for \(j=0,1,,N_{B}\), to each block \(B_{i}=\{c_{j}\}_{j}\), where \(i=0,1,,N_{}\).

Instead of mapping a single representation \(z_{i}^{v}\) as a linear combination of all prototypes in \(C\), we compare the view's representations \(z_{i}^{v}\) and \(w_{i}^{v}\) against all the prototypes in the \(j\)-th block. That is, \(s_{i,j}^{v}=q(z_{i}^{v},B_{j})\) and \(t_{i,j}^{v}=q(w_{i}^{v},B_{j})\), for every block in the partition \(\), to obtain the normalized probability distribution relating a view from image \(i\) with the prototypes of the \(j\)-th block of the random partition, where \(s_{i,j}^{v}\) and \(t_{i,j}^{v}^{1 1 N_{P}}\).

To ensure that views are consistent among the blocks, we optimize the views' distributions \(s_{i,j}^{v}\) and \(t_{i,j}^{v}\) over the prototypes of a block indexed by \(j\), so that the two distributions are consistent with one another. Thus, the consistency term of our partition loss is \(_{c}(s_{i,j}^{1},t_{i,j}^{2})\), where for each block \(B_{j}\), the loss is minimized when the pair of views, \(_{i}^{1}\) and \(_{i}^{2}\), gets assigned to the same prototypes across blocks. In other words, we look for the agreement between student and teacher assignments' probabilities across views of a given sample.

Following similar reasoning, the block-wise entropy term is defined as \(H(_{j})\), where \(_{j}=(\{(s_{i,j}^{v},t_{i,j}^{v})\}_{i=1}^{N})\) is the average prediction over each block \(B_{j}\) for a batch of size \(N\). Thus, the final objective for consistent assignment of random partition is,

\[=}}_{i}^{N}_{j}^{N_{}} (_{c}(s_{i,j}^{1},t_{i,j}^{2})+_{c}(s_{i,j}^{2},t_{ i,j}^{1}))-H(_{j}).\] (4)

Note that to fully use the pair of views at each iteration, we symmetrically use the \(_{c}\) consistency function. The probability vectors \(t_{i,j}^{v}\) come from the momentum teacher and are used as target distributions.

We can view the random partition pretext task as posing multiple pseudo-classification problems over subsets of prototypes. At each iteration, the pseudo-classification tasks change because the partitions are recreated with different prototypes, cf. Figure 2. One of the benefits of such a strategy is that we no longer require tuning the hyperparameter \(_{e}\) to avoid trivial solutions. The stochastic nature of the random partition pretext task, blended with the multiple prediction tasks over subsets of prototypes, provides a regularization effect that improves the learned representations and training stability.

Other clustering-based methods [7; 8] rely on sharpening the distributions to improve their self-supervised signals used as targets in a cross-entropy loss. On the contrary, our formulation does not require the temperature parameter for sharpening the predictions and guiding the learning of the student. We can think of the consistency loss as implicitly learning the temperature parameter to make the predictions sharper at each iteration. This is an important advantage of our consistency loss in contrast to previous methods.

## 5 Main results

### Transfer learning evaluation

Table 1 shows that CARP's pre-trained representations are suitable for learning new downstream tasks over multiple datasets with distinct difficulty levels. **We compare CARP's \(k\)-NN performance against nine SSL methods across eight datasets** and report average results for \(k\)= {10, 20, 100, 200}, over all datasets. We advocate for \(k\)-NN instead of linear evaluation, where a linear classifier is trained on top of the frozen features, for the following reasons: (1) \(k\)-NN is faster, (2) \(k\)-NN demands fewer resources, and (3) \(k\)-NN requires less hyperparameter tuning. For individual references, Table 1 shows top-1 accuracy for \(k=20\) on each dataset. **For fixed \(k=20\), CARP outperforms competitors in 5 out of 8 datasets, and in the Flowers and Country datasets, it is a close second**. Moreover, the average performances at \(k\) show that CARP performs comparably well on all datasets without significant performance differences when varying the number of labeled examples \(k\). We show the detailed results in Appendix A.1.

### Clustering evaluation

Table 2 reports clustering performance metrics of various clustering-based SSL methods on the ImageNet-1M , CIFAR-10/100 , and the GTSRB  datasets. For the ImageNet-1M, only 1% of the labeled data is used following the subset provided by Chen et al. . See Appendix D.4 for the detailed \(k\)-means evaluation protocol.

### Image retrieval and copy detection

Motivated by the strong \(k\)-NN performance of CARP and following the previous evaluation protocol by Caron et al. , we assess the performance of ImageNet pre-trained CARP encoders on image retrieval and copy detection downstream tasks. We took the officially released weights from the competing methods, used the frozen encoders as feature extractors, and performed retrieval using \(k\)-NN. For both tasks, the nearest neighbor computation is done on the 2048-dim representation from the ResNet-50 encoder. We report the top-3 best-performing methods. For an extended evaluation, see Appendix A.2.

Image retrieval.We consider the revisited Oxford and Paris landmark image retrieval datasets . Given a query image of a landmark, the objective is to retrieve all database images depicting the same landmark. Each dataset contains three different difficulty levels. In Table 3, we report the mean Average Precision (mAP) on the Medium and Hard subsets for various SSL algorithms. CARP's representations perform well on Oxford 5k and take second place for Paris 6k. See Appendix D.5 for the evaluation protocol.

Copy detection.We benchmark self-supervised ResNet-50 encoders on the INRIA Copolymers dataset . In Table 4, we report the mean Average Precision (mAP) on the "strong" subset and compare CARP's performance against other state-of-the-art methods.

    &  & Flowers & Aircraft & Cars & Country & Food & STL & GTSRB &  \\  Methods & Ep &  &  & 20 & 100 & 200 \\  oBoW (mc)  & 200 & 57.3 & 61.9 & 18.1 & 11.5 & 12.0 & 47.4 & 96.6 & 50.6 & 44.3 & 44.4 & 43.5 & 43.0 \\ SeLa-v2 (mc)  & 400 & 66.8 & 58.6 & 20.7 & 13.3 & 10.5 & 46.8 & 94.0 & 59.0 & 46.1 & 46.2 & 45.5 & 45.1 \\ InfoMin  & 800 & 77.8 & 61.9 & 18.2 & 14.4 & 11.6 & 52.4 & 96.4 & 54.8 & 48.6 & 48.4 & 47.3 & 46.6 \\ DeepC-v2 (mc)  & 800 & 78.3 & 76.3 & 32.0 & 25.0 & 13.6 & 62.3 & 95.6 & 63.4 & 56.0 & 55.8 & 54.4 & 53.3 \\ SwAV (mc)  & 800 & 77.0 & 75.2 & 29.0 & 22.7 & 13.8 & 59.1 & 95.2 & 63.2 & 54.5 & 54.4 & 53.1 & 52.2 \\ DINO (mc)  & 800 & 80.9 & **81.6** & 35.3 & 30.1 & **14.4** & 62.0 & 95.6 & 62.9 & 57.9 & 57.8 & 56.9 & 56.0 \\ Triplet  & 980 & 83.5 & 77.7 & 33.4 & 25.2 & 14.1 & 61.5 & 95.6 & 63.5 & 56.5 & 56.8 & 56.2 & 55.6 \\ BarlowT  & 1000 & 82.9 & 78.8 & 32.7 & 26.3 & 13.3 & 61.4 & 94.8 & 65.6 & 56.8 & 57.0 & 56.4 & 55.7 \\ MoCo-v3  & 1000 & 86.4 & 79.0 & 36.9 & 29.3 & 12.4 & 60.0 & **96.7** & 72.8 & 59.2 & 59.2 & 58.4 & 57.8 \\  CARP & 400 & **86.8** & 80.0 & **42.1** & **33.5** & 12.3 & 58.4 & 95.9 & **75.3** & **60.4** & **60.5** & **59.7** & **59.2** \\ CARP (mc) & 400 & 83.9 & 80.3 & 34.8 & 27.1 & 14.2 & **62.9** & 95.5 & 62.8 & 57.6 & 57.7 & 56.8 & 56.0 \\   

Table 1: **Transfer learning evaluation. We report top-1 accuracy (\(k=20\)) for individual datasets and averages over all datasets for \(k\{10,20,100,200\}\). Top performing in bold, top-2 underlined.**

    &  &  &  &  \\  Method & NMI & AMI & ARI & NMI & AMI & ARI & NMI & AMI & ARI & NMI & AMI & ARI \\  PCL v2  & 69.7 & 47.5 & 22.2 & 46.7 & 46.6 & 34.8 & 49.1 & 42.2 & 17.1 & 44.1 & 44.1 & 13.0 \\ SeLA-v2  & 68.7 & 45.5 & 21.3 & 42.0 & 41.9 & 30.6 & 49.7 & 42.9 & 18.2 & 45.7 & 43.2 & 12.0 \\ DeepC-v2  & 69.7 & 47.1 & 22.4 & 47.0 & 46.9 & 35.5 & 53.2 & 46.7 & 21.8 & 48.1 & 45.7 & 13.5 \\ SwAV  & 68.5 & 45.1 & 20.5 & 46.8 & 46.8 & 37.0 & 52.1 & 45.5 & 20.0 & 51.0 & 48.8 & 15.0 \\ DINO  & 69.2 & 46.2 & 21.7 & 39.6 & 39.5 & 28.0 & 47.6 & 40.4 & 16.2 & 52.0 & 49.8 & 15.4 \\ MIRA  & 68.9 & 45.7 & 21.2 & 39.5 & 39.4 & 28.8 & 49.0 & 42.1 & 17.6 & 51.6 & 49.4 & 15.8 \\ CoKe  & 68.9 & 45.6 & 21.3 & 45.9 & 45.8 & 34.2 & 51.9 & 45.2 & 19.5 & 49.4 & 47.1 & 13.7 \\   CARP & **70.3** & **48.0** & **23.9** & **49.0** & **48.9** & **38.7** & **54.5** & **48.2** & **23.1** & **54.8** & **52.7** & **19.6** \\   

Table 2: **Clustering evaluation. We report (NMI) normalized mutual information, (AMI) adjusted mutual information, and (ARI) adjusted rand index. Top performing in bold, top-2 underlined.**

    & \)x} & \)par} \\  Method & ep & M & H & M & H \\  Supervised  & 100 & 49.8 & 8.5 & 74.0 & 52.1 \\ Random & – & 1.6 & 0.7 & 4.1 & 2.5 \\  DINO (mc)  & 800 & 35.4 & 11.1 & 55.9 & 27.5 \\ Triplet  & 980 & 35.3 & 12.0 & 58.2 & 28.7 \\ MoCo-v3  & 1000 & 33.1 & 10.9 & **59.1** & **31.3** \\  CARP & 200 & **38.8** & **15.5** & 58.8 & 30.4 \\   

Table 3: **Image retrieval evaluation. We report mAP on the revisited Oxford and Paris for the (M) Medium and (H) Hard subsets.**

### Few-shot classification

Table 5 compares the few-shot classification performance of SSL methods on the VOC07  and INat2018  datasets. We train linear SVMs following Li et al.  and linear classifiers on top of the frozen representations from the self-supervised ResNet-50 encoders for VOC07 and INat2018, respectively.

The INat2018 dataset is especially challenging for low-shot classification. It contains 8142 classes with a long tail distribution, where the number of images per class varies between 1000 maximum and 2 minimum. **CARP remains a strong feature extractor for both tasks while competitors oscillate between datasets.** CARP pre-trained for 400 epochs demonstrates an efficient learning performance and wins most configurations. We report a complete evaluation with standard deviations in Appendix A.3.

### Linear evaluation

Following the linear evaluation protocol proposed by Zhou et al. , we trained linear classifiers on top of CARP's frozen 2048-dim representations for 100 epochs.

We assess the linear evaluation performance of CARP's pre-trained representations on ImageNet-1M for three pre-training configurations, 100, 200, and 400 epochs, varying the utilization of multi-crop (mc) augmentation, Table 6. CARP surpasses SwAV on all pre-trained configurations, +0.4% on 100 epochs, +0.4% on 200 epochs, +0.4% on 400 epochs w/o multi-crop, and +0.4% on 400 epochs with multi-crop. Indeed, CARP's 400 epochs pre-trained representations perform on par with DINO and SwAV, both pre-trained for 800 epochs.

In addition, we evaluated CARP's representations using a weighted \(k\)-Nearest Neighbor (\(k\)-NN) classifier. CARP achieves better \(k\)-NN performance than SwAV (+1.4%), DINO (+0.2%), and BYOL (+1.1%), all trained for 800 epochs or more. These results emphasize the efficiency of the proposed random partition pretext task.

## 6 Ablations

In this section, we assess whether a consistent assignment of views over random partitions benefits the learned representations and improves training stability. We ablate CARP's main hyperparameters to establish a good baseline for pre-training on ImageNet-1M. For ablations, we trained CARP using the full ImageNet-1M dataset for 50 epochs. The batch size was set to 256, the number of prototypes \(K=65\,536\), and the number of random partition blocks \(N_{p}=128\). Hence, each block contains \(N_{B}=512\) prototypes. We report results for single runs. See Appendix B for more results and Appendix C for implementation details.

### Training CARP with different batch sizes

Most SSL methods [7; 8; 22; 48] report their best results when using substantially large batch sizes. In Table 7, we observe a similar pattern when training CARP. Our default configuration of 1024

    & &  &  \\  Method & Ep & n=1 & n=2 & n=4 & n=8 & n=16 & full & n=1 & n=2 & n=4 & n=8 & n=16 & full \\  PCL v2  & 200 & **47.9** & 59.6 & 66.2 & 74.5 & 78.3 & 85.4 & 1.4 & 1.6 & 2.3 & 2.9 & 4.8 & 2.1 \\ DINO (mc)  & 800 & 45.6 & 58.4 & 66.6 & 74.8 & 79.6 & 88.2 & 6.5 & 12.0 & 20.4 & 29.6 & 35.9 & 30.4 \\ Triplet  & 980 & 43.6 & 56.2 & 64.6 & 73.8 & 79.6 & **88.3** & 11.4 & 19.1 & 28.9 & 37.6 & 44.0 & 41.4 \\ MoCo-v3  & 1000 & 46.6 & 59.6 & 67.0 & 75.4 & **80.2** & 87.4 & 8.1 & 12.2 & 18.5 & 27.2 & 33.5 & 28.0 \\  CARP (mc) & 200 & 46.0 & 58.3 & 66.5 & 75.5 & 79.5 & 88.0 & 8.6 & 14.4 & 23.6 & 32.7 & 38.2 & 33.9 \\  & 400 & 47.1 & **59.8** & **67.3** & **75.8** & 80.0 & 88.2 & **11.5** & **19.6** & **29.6** & **39.1** & **45.1** & **42.6** \\   

Table 5: **Few-shot classification on VOC07 and INat2018**. We report mAP for VOC07 and top-1 accuracy for INat2018, at \(n\), across 5 independent runs, where \(n\) denotes the number of training examples. Top performing in **bold**, top-2 underlined.

observations yields a \(k\)-NN top-1 performance 10% higher than a batch size of 128. Table 7 confirms that training with large batch sizes benefits the learned representations. However, training with smaller batch sizes requires further tuning of other hyperparameters, such as the block size \(N_{B}\). Specifically, we observed that reducing the block size \(N_{B}\) improves the learned representations when training with small batch sizes, which makes CARP robust to low-resource training.

### Exploring different strategies to build partitions

Table 8 explores different ways of creating random partition blocks from the learnable prototypes. CARP's default strategy recreates the random partitions at every training step. In other words, for each iteration, we assign \(N_{B}\) randomly chosen prototypes to the \(N_{}\) partition blocks. Table 8 contrasts CARP's default strategy with one in which the partition blocks are created only once, in a sequential manner, and kept fixed throughout training. We observe that training CARP with fixed partition blocks still produce useful representations. However, as measured by \(k\)-NN performance, randomly recreating the partition blocks at each iteration further benefits the learned representations. Since the partition blocks are randomly recreated at every iteration of gradient descent, the classification subproblems are always different. In practice, this variance allows for many unique pretext tasks at each iteration, which provides a positive regularization effect on CARP.

## 7 Limitations

Even though CARP's representations transfer well to many downstream tasks, our experiments showed that CARP's representations do not transfer well to dense prediction tasks such as detection and segmentation. We hypothesize this limitation is due to the architectural characteristics of ConvNets that collapse local feature maps to an average global representation in the last layer, combined with our classification-like loss function. Refer to Appendix A.4 for quantitative results.

   Method & Ep & Linear & \(k\)-NN \\  Supervised & 100 & 76.5 & – \\  SwAV (mc)  & 100 & 72.1 & 61.8\({}^{}\) \\  & 200 & 73.9 & 63.7\({}^{}\) \\  & 400 & 74.6 & 65.0\({}^{}\) \\  & 800 & **75.3** & 66.3\({}^{}\) \\ DINO (mc)  & 800 & **75.3** & 67.5 \\ BYOL  & 1000 & 74.3 & 66.6 \\ MoCo-v3  & 1000 & 74.6 & **68.9\({}^{}\)** \\  CARP & 400 & 73.0 & 67.6 \\ CARP (mc) & 100 & 72.5 & 63.5 \\  & 200 & 74.2 & 66.5 \\  & 400 & **75.3** & 67.7 \\   

Table 6: **Linear evaluation**. We report top-1 linear and \(k\)-NN (\(k=20\)) accuracy for the ImageNet-1M dataset. \({}^{}\) Results computed by us using the officially released pre-trained models. Top performing in **bold**, top-2 underlined.

   bs & 128 & 256 & 512 & 1024 & 2048 & 4096 \\  \(k\)-NN & 46.56 & 51.32 & 54.23 & 56.63 & 57.0 & **58.5** \\    
   Epochs & 25 & 50 & 75 & 100 \\  Constant & 45.15 & 49.89 & 52.81 & 53.32 \\ Random & **48.68** & **53.98** & **55.93** & **56.38** \\   

Table 7: CARP learns better representations when larger batch sizes (bs) are employed.

Conclusion

We presented consistent assignment of views over random partitions (CARP), a self-supervised clustering-based method for visual feature learning. CARP learns prototypes in an online fashion end-to-end using gradient descent by minimizing a cost function that optimizes consistency between views' assignments and uniform distribution across prototypes within a random partition. Our experiments demonstrate that posing the optimization problem at the level of random partitions of learnable prototypes stabilizes training by avoiding trivial solutions in joint-embedding architectures and increases the performance of the learned representation. We compared the performance of CARP against the state-of-the-art ResNet-based SSL methods across multiple pretext tasks and datasets. The results demonstrated that the representations learned by CARP performed well on many visual downstream tasks.