# Do's and Don'ts:

Learning Desirable Skills with Instruction Videos

 Hyunseung Kim\({}^{1,2}\) Byungkun Lee\({}^{1}\) Hojoon Lee\({}^{1}\)

Dongyoon Hwang\({}^{1}\) Donghu Kim\({}^{1}\) Jaegul Choo\({}^{1}\)

\({}^{1}\)KAIST, \({}^{2}\)KRAFTON

{mynsng,byungkun.lee,joonleesky,godnpeter,quagmire,jchoo}@kaist.ac.kr

###### Abstract

Unsupervised skill discovery is a learning paradigm that aims to acquire diverse behaviors without explicit rewards. However, it faces challenges in learning complex behaviors and often leads to learning unsafe or undesirable behaviors. For instance, in various continuous control tasks, current unsupervised skill discovery methods succeed in learning basic locomotions like standing but struggle with learning more complex movements such as walking and running. Moreover, they may acquire unsafe behaviors like tripping and rolling or navigate to undesirable locations such as pitfalls or hazardous areas. In response, we present **DoDont** (Do's and Don's), an instruction-based skill discovery algorithm composed of two stages. First, in instruction learning stage, DoDont leverages action-free instruction videos to train an instruction network to distinguish desirable transitions from undesirable ones. Then, in the skill learning stage, the instruction network adjusts the reward function of the skill discovery algorithm to weight the desired behaviors. Specifically, we integrate the instruction network into a distance-maximizing skill discovery algorithm, where the instruction network serves as the distance function. Empirically, with less than 8 instruction videos, DoDont effectively learns desirable behaviors and avoids undesirable ones across complex continuous control tasks. Code and videos are available at https://mynsng.github.io/dodont/

## 1 Introduction

Recent advancements in unsupervised pre-training methodologies have led to the creation of large-scale foundational models across diverse domains, including computer vision  and natural language processing . These methodologies exploit self-supervised learning objectives to extract meaningful representations without using explicit, supervised learning signals. In attempts to expand this paradigm to reinforcement learning, researchers have explored crafting self-supervised objectives to develop foundational policies capable of learning diverse behaviors without predefined reward signals, termed unsupervised skill discovery (USD) .

Despite notable advancements in USD methodologies, acquiring foundational policies in environments with large state and action spaces (e.g., multi-jointed quadrupeds) remains a significant challenge. Two major issues arise when training agents with USD in these complex environments. First, the vast state and action spaces allow the agent to acquire a wide variety of behaviors. While learning simple behaviors like standing may be feasible, acquiring complex behaviors that require intricate joint movements, such as walking or running, can take an exceedingly long time. Second, agents can develop undesirable and risky behaviors during training, such as tripping, rolling, or navigating hazardous areas like pitfalls. These challenges raise a key question: _Is the purely unsupervised assumption of USD ideal for learning foundational policies in the real-world?_According to social learning theory, humans learn behaviors through both internal and external motivations [4; 5; 16]. Internally, they seek to perform diverse behaviors they haven't tried before. Externally, they adopt socially desirable behaviors and avoid those that are not. In the skill discovery algorithm, this can be mimicked by combining internal motivation (USD objective) with external motivation based on a human's intention. A straightforward way to incorporate external motivation in USD is to use a hand-designed reward function [26; 10]. However, hand-designing reward functions are not scalable for learning diverse and desirable behaviors for several reasons. One of the reasons is the complexity of designing an intricate reward signal for guiding a desired behavior, and another is the difficulty of balancing multiple behavioral intentions into a single reward function.

To address these challenges, we propose **DoDont**, a skill discovery algorithm that integrates USD objectives with intended behavioral goals. Instead of relying on a hand-designed reward, DoDont _learns a reward function from a small set of instruction videos_ that demonstrate desirable and undesirable behaviors. Videos are chosen because they are inexpensive to collect and do not require action or reward labels [13; 46].

As illustrated in Figure 1, DoDont starts by collecting instruction videos of desirable (Do's) and undesirable behaviors (Don'ts). We then train an instruction network which assigns higher values to desirable behaviors and lower values to undesirable ones. This network re-weights the internal USD objective for the skill discovery phase. We utilize a distance-maximizing skill discovery algorithm as our main USD objective [35; 38; 39] where the instruction network serves as the distance metric.

To validate DoDont, we conduct experiments on various continuous control tasks that require complex locomotion (e.g., Cheetah and Quadruped ) or precise manipulation (e.g., Kitchen ). Our results show that with fewer than eight instruction videos, DoDont effectively learns complex locomotion skills (e.g., running quadruped), which are challenging to acquire with standard USD algorithms . Moreover, our instruction network effectively captures human intentions better than hand-designed reward functions in balancing multiple behavioral objectives. Additionally, DoDont learns diverse skills while avoiding undesirable and unsafe behaviors (e.g., backflipping), which are difficult to prevent even with previous skill discovery algorithms which utilize prior knowledge .

## 2 Related work

### Learning diverse behaviors without pre-defined task

Numerous unsupervised skill discovery (USD) methods have been developed to create foundational policies capable of learning a diverse behaviors without pre-defined task. The most com

Figure 1: **(a) The offline instruction video dataset includes videos of desirable behaviors (Do’s) and undesirable behaviors (Don’ts). (b) Unsupervised skill discovery algorithms tend to learn undesirable behaviors. (c) In DoDont, an instruction network is first trained with the Do’s and Don’ts videos to distinguish desirable and undesirable behaviors. Then, this instruction network adjusts the intrinsic reward of the skill discovery algorithm, promoting desirable skills while avoiding undesirable ones.**

mon approach involves maximizing mutual information (MI) between states and skills (\(I(S;Z)=D_{KL}(p(s,z)||p(s)p(z))\)) [14; 43; 31; 9; 27]. This is typically done by using an auxiliary neural network \(q_{}(z|s)\) to estimate a lower bound of \(I(S;Z)\) (\(I(S;Z)z\), \(s[ q_{}(z|s)]\)). This network acts as a skill discriminator, predicting the skill \(z\) from the state \(s\), which encourages the policy to create distinct trajectories for different skills and promotes learning a wide range of skills. However, these methods often struggle to learn complex, dynamic skills, as the MI objective can be met with simple, static skills, leaving agents unmotivated to explore more intricate behaviors [44; 23; 30; 31; 24].

To overcome this problem, distance-maximizing skill discovery (DSD) methods have been introduced [35; 38; 39]. Instead of maximizing MI between states and skills, DSD explicitly maximizes a predefined distance function of states \(d:_{0}^{+}\). For instance, LSD  uses Euclidean distance between states to encourage agents to move further distances (\(d_{}=||s^{}-s||\)). CSD  employs a density model based distance, promoting agents to visit less frequented states (\(d_{}=- q_{}(s^{}|s)\), where \(q_{}\) is the density model). METRA  defines the distance temporally, pushing agents to learn skills that are temporally far apart (\(d_{}\) = minimum number of environment steps to reach \(s^{}\) from \(s\)). DoDont is built upon the DSD framework, which will be elaborated in Sections 3 and 4.

### Learning diverse behaviors with pre-defined task

In contrast to USD, there is a line of research focused on learning policies that exhibit diverse behaviors by integrating intrinsic motivation (e.g., MI-based rewards) with human intention, as the form of task-specific reward functions [10; 26; 55; 54] or using demonstration datasets . While traditional RL methods typically aim to find a single optimal policy, these approaches aim to learn multiple policies \(_{}(a|s,z_{i=1:N})\). This allows for multiple ways to solve given tasks, resulting in increased robustness to environmental changes and the development of policies with distinct characteristics.

SMERL  and DGPO  combine intrinsic and task rewards, maximizing their sum when task rewards exceed a given threshold. RSPO  iteratively finds novel policies by switching between task rewards and intrinsic diversity rewards, ensuring each new policy is distinct from previous ones. DOMiNO  addresses a constrained optimization problem by maximizing intrinsic diversity rewards while ensuring that all policies achieve sufficiently high performance (task rewards as constraints). ASE  utilizes a action-free demonstration dataset instead of a task reward function, minimizing a behavioral cloning loss while simultaneously maximizing a diversity intrinsic reward. DoDont also employs action-free demonstration datasets to extract human prior knowledge through an instruction network, pioneering the incorporation of both positive and negative behavior examples to guide learning towards desired behaviors and away from undesirable ones.

## 3 Preliminaries and problem setting

Markov decision process and skill-conditioned policy.Unsupervised skill discovery (USD) focuses on identifying a range of skills without pre-defined reward function. In this approach, we use a reward-free Markov Decision Process defined as \(=(,,,p)\). Here, \(\) represents the state space, \(\) is the action space, \(\) denotes the initial state distribution, and \(p\) is transition dynamics function. We utilize a latent vector \(z\) and their associated policy \((a|s,z)\), which we refer to as skills. To generate a skill trajectory (i.e., behavior), we first sample a skill \(z\) from the prior distribution, denoted as \(p(z)\), and then execute a trajectory using the skill policy \((a|s,z)\). For the skill prior, we utilize a standard normal distribution to represent continuous skills by following previous works [35; 38; 39].

Distance-maximizing skill discovery.As discussed in Section 2.1, numerous distance-maximizing skill discovery methods have been proposed [35; 38; 39]. Specifically, DSD introduces the Wasserstein dependency measure (WDM) as a learning objective for unsupervised skill discovery (USD) .

\[I_{}(S;Z)=(p(s,z),p(s)p(z))\] (1)

Here, \(\) is the 1-Wasserstein distance on the metric space \((S Z,d)\), where \(d\) is a _distance metric_. Unlike MI-based skill discovery methods (maximizing \(I(S;Z)=D_{KL}(p(s,z)||p(s)p(z))\) which does not explicitly motivate the agent to learn more complex behaviors (as discussed in Section 2.1), DSD maximizes the _distance-aware_\(I_{}(S;Z)\). By doing so, DSD not only discovers a set of distinguishable skills but also maximizes the distance \(d\) between different states. By defining the distance function to encourage desired behaviors, DSD effectively induces the agent to learn these behaviors.

In practice, by leveraging the Kantorovich-Rubenstein duality [48; 34] under some simplifying assumptions, the following concise objective can be derived:

\[I_{}(S_{T};Z)_{\|\|_{L} 1}_{p(,z)} [_{t=0}^{T-1}((s_{t+1})-(s_{t}))^{}z],\] (2)

where \(:S^{D}\) is a function that maps states into \(D\)-dimensional space, \(\|\|_{L}\) denotes the Lipschitz constant for the function \(\) under the given distance metric \(d\). Now, we can rewrite Equation 2 with an arbitrary distance function \(d:S S_{0}^{+}\) as:

\[_{,}\ _{p(,z)}[_{t=0}^{T-1}((s_{t+1} )-(s_{t}))^{}z]\|(x)-(y)\|_{2 } d(x,y),(x,y) S.\] (3)

For a detailed derivation, we advise readers to refer to Section 4 of METRA . It's important to note that a generic distance function, \(d\), does not necessarily need to meet the criteria of a valid distance metric, such as symmetry or the triangle inequality. As shown in previous work , the original constraint in Equation 3 can be implicitly converted into one with a valid pseudometric, allowing us to use any non-negative function as a distance metric.

## 4 Do's and don'ts (DoDont)

Our goal is to integrate human intention into USD, where diverse policies are learned without predefined reward signals. The core concept of this work involves training an instruction network using video data to distinguish desired and undesired behvaiors (Section 4.1). Then, we set the trained instruction network as the distance metric in the distance-maximizing skill discovery framework (Section 4.2). Consequently, this will result in an agent that not only learns diverse behaviors but also incorporate human intention since the instruction network is set as the distance metric.

### Instruction network

We aim to train an instruction network that assigns high values for desirable behaviors (Do's) and low values for undesirable behaviors (Don'ts). The distance metric in the WDM objective (in Equation 3) takes two states as input and should predict a non-negative value.

To meet this requirement, we first prepare videos depicting desirable behaviors (i.e., Do's) and videos illustrating behaviors to avoid (i.e., Don'ts) according to our intentions. We then label our video dataset by assigning \(y=1\) to adjacent state pairs \((s_{t},s_{t+1})\) in Do's videos and \(y=0\) to those in Don'ts videos. This results in our Do's and Don't video dataset \(_{V}\) which is comprised of triplets \((s_{t},s_{t+1},y)\). After preparing our video dataset \(_{V}\), we use it to train our instruction network \(_{}\), which is designed to classify whether a given pair of adjacent states is from a Do's video or a Don'ts video. The training objective for the instruction network is a simple binary classification loss:

\[^{}=-_{(s_{t},s_{t+1},y) D_{V}} [y_{}(s_{t},s_{t+1})+(1-y)(1-_{}(s_{t},s_ {t+1}))].\] (4)

### DoDont

Here, we integrate the learned instruction network into the online distance-maximizing skill discovery algorithm. As \(_{}\) is a non-negative function, we can directly use this instruction network as the distance metric of the WDM objective in the Equation 3 as:

\[_{,}_{p(,z)}[_{t=0}^{T-1}((s_{t+1} )-(s_{t}))^{}z]\ \ \|(s)-(s^{})\|_{2}_{}(s,s^{}),\ \ (s,s^{}) S_{adj},\] (5)

where \(S_{adj}\) represents the set of adjacent state pairs. By assigning higher values to behaviors deemed desirable by humans (i.e., large distances) and lower values to behaviors that should be avoided, it guides the agent to learn skills that correspond to human intentions.

Then, following previous work [38; 39; 49], we can optimize Equation 5 with dual gradient descent, incorporating a Lagrange multiplier \(\) and a small slack variable \(>0\):

\[ r^{}&:=((s^{})- (s))^{}z,\\ ^{,}&:=[((s ^{})-(s))^{}z]+(,_{}(s,s^{ })-\|(s)-(s^{})\|),\\ ^{,}&:=- [(,_{}(s,s^{})-\|(s)-(s^{ })\|)],\] (6)

where \(r^{}\) is the intrinsic reward for the latent conditioned policy, and \(^{,}\) and \(^{,}\) are the objectives for \(\) and \(\). We note that since the instruction network \(_{}\) is already trained on \(D_{V}\), we keep \(_{}\) frozen for training stability.

However, one drawback of Equation 5 is that \(r^{}\) can reflect instruction signals from \(_{}\) only after \((s)\) has been sufficiently trained through \(^{,}\). This indicates that in the early stages of training where \((s)\) has not yet been sufficiently optimized, the agent may not properly receive instruction signals. Such delay in receiving instruction signals could potentially slow down the training process. Therefore, to ensure the agent receives instruction signals regardless of the training state of \((s)\), we note that the following equation can be easily derived from Equation 5 :

\[_{,}_{p(,z)}[_{t=0 }^{T-1}_{}(s,s^{})((s_{t+1})-(s_{t}))^{ }z]&\|(s)-(s^{})\|_{2} 1,\ \ (s,s^{}) S_{adj},\] (7)

For a detailed derivation, please refer to the Appendix B. Through this rephrasing, we now separate \(_{}\) from the training of \((s)\). This allows the agent to receive a direct instruction signal, as \(_{}\) explicitly multiplies with the original intrinsic reward, independent of the training of \((s)\). In our experiment, we found that this direct signal significantly enhances the agent's ability to learn diverse and desirable behaviors (Section 5.3). In addition, another advantage of Equation 7 is that it can be interpreted as simply multiplying instruction network \(_{}\) to the original learning objective function of METRA. This allows us to implement our algorithm by simply adding a single line of code on top of the METRA framework. We provide the pseudocode of DoDont Algorithm 1. Furthermore, our instruction network can be applied to zero-shot offline RL to learn diverse behaviors while prioritizing desirable behaviors within the offline unlabeled dataset. Detailed explanations and experiments are presented in Appendix A.

## 5 Experiments

### Experimental setup

Our experiments aim to evaluate the effectiveness of the DoDont method in learning desirable behaviors while avoiding undesirable ones across various environments and instruction scenarios. We test DoDont in two contexts: complex locomotion tasks (Cheetah and Quadruped environments from the DeepMind Control (DMC) Suite ) and precise manipulation tasks (Kitchen environment [17; 33]) (Figure 2). For pixel-based inputs in the DMC environments, we colored the floors to help the agent be aware of its location from the pixel data, consistent with previous studies [36; 20; 39].

For quantitative comparisons in locomotion tasks, we use two main metrics: (i) state coverage and (ii) zero-shot task reward. State coverage is measured by counting the \(x\)-axis coverage for Cheetah and \(x,y\)-axis coverage for Quadruped achieved by the learned skills at each evaluation epoch, following prior literature . Zero-shot task reward evaluates the learned skills without task-specific training rewards to assess whether the agent efficiently learns or avoids specific behaviors. Our quantitative analysis uses four random seeds and provides 95% confidence intervals, represented by error bars or shaded areas. For each instruction scenario, we use eight videos (four "do" videos and four "don't" videos), unless otherwise specified. Additional details are elaborated in Appendix D.

Figure 2: **Benchmark environments.**

To comprehensively evaluate DoDont's performance, we compare it against three types of baselines. First, to determine if an instruction network facilitates acquiring intended behavior, we compare DoDont with METRA , a state-of-the-art online unsupervised skill discovery algorithm. Second, to assess the effectiveness of our video-based intention network against a hand-designed reward function, we introduce METRA\({}^{}\), a modified version of METRA that uses hand-designed reward functions as the distance metric. Finally, to verify if using an instruction network as a distance function effectively integrates our intention with skill discovery algorithms, we compare it to SMERL  and DGPO , which integrate task rewards in specialized ways. In this comparison, we use our intention network as the task reward function for both SMERL and DGPO. Details are in Appendix D.

### Main experiment

In this section, our experiments are designed to answer the following research questions:

* How efficiently does DoDont learn complex behaviors in locomotion tasks? (Section 5.2.1)
* Does DoDont learn diverse behaviors while avoiding hazardous areas? (Section 5.2.2)
* Can DoDont learn diverse behaviors without learning unsafe behaviors? (Section 5.2.3)
* Can DoDont be applied to a manipulation environment? (Section 5.2.4)

#### 5.2.1 How efficiently does DoDont learn complex behaviors in locomotion tasks?

In this experiment, we aim to evaluate whether DoDont can effectively learn diverse complex behaviors by simply providing videos of desirable but complex behaviors. To accomplish this, we set videos of agents successfully running as Do's and random action videos as Don'ts for the Cheetah and Quadruped environment. As shown in Figure 3, DoDont achieves higher running rewards than METRA in both environments. Specifically, in the Quadruped environment, METRA primarily learns rolling movements for locomotion, while DoDont learns to run upright. Please refer to the project page videos for further details (link). Additionally, DoDont exhibits higher state coverage than METRA, since DoDont effectively learn running behaviors which allows the agent to cover longer distances whereas METRA only learn mediocre rolling behaviors.

We also observe that both SMERL and DGPO fail to learn diverse behaviors and only learns simple behaviors such as standing upright at the starting point with minimal movement. This limitation is likely due to their use of the MI objective, which appears insufficient for promoting diverse behavior learning in complex continuous control environments. METRA\({}^{}\), a variant of METRA where we set the run task reward as the distance metric, is effective for simple environments with low-dimensional action spaces (Cheetah), but struggle in learning effective behaviors for environments with high-dimensional action spaces (Quadruped). We speculate that as the environment complexity increases, designing a single reward function which captures a variety of desirable behaviors becomes increasingly challenging.

An important point which we would like to emphasize is that DoDont only requires a total of eight videos to learn running behaviors. Despite the direction of each running video is limited (instruction videos in Figure 3), DoDont learns skill which run in all directions. This indicates that DoDont is not

Figure 3: **Left**: State coverage and zero-shot task reward for Cheetah and Quadruped. **Right**: Visualization of Do videos in our instruction video dataset and learned skills by DoDont. We are able to observe that DoDont does not simply mimic instruction videos but extracts desirable behaviors (e.g., run) from the videos and learn diverse skills.

merely imitating the behaviors in the Do's videos but discover a variety of behaviors which resemble those seen in the Do's videos. In addition, to further evaluate the meaningfulness of the learned behaviors, we conduct downstream task evaluation for each method in the Appendix A.

#### 5.2.2 Does DoDon't learn diverse behaviors while avoiding hazardous areas?

In practical scenarios, it is crucial for agents to avoid hazardous areas. For instance, in navigation tasks, robots must avoid dangerous areas such as pitfalls or private property. However, traditional skill discovery methods, which aim to cover the entire state space, face difficulties in controlling the regions an agent explores. We now aim to evaluate whether the Don't videos in DoDon't can reliably prevent agents from learning unwanted area navigation. To replicate real-world conditions, we perform experiments where we designate the left side of the environment as hazardous areas and the right side as safe areas. Thus, we use videos that move to the right as Do's and videos that move to the left as Don'ts, aiming to direct the agent away from hazardous zones and towards safer regions.

To evaluate the agent's ability to cover safe areas while avoiding hazardous ones, we introduced the concept of _safe state coverage_. This metric assigns a score of +1 for states in the safe area and -1 for states in the hazardous area. As illustrated in Figure 4, METRA without prior knowledge attempts to cover the entire state space, including hazardous areas, whereas DoDon effectively incorporates human intentions, avoiding dangerous areas while adequately covering the safe regions. We also observe that SMERL and DGPO fail to learn diverse skills, resulting in static behavior within the safe region, possibly due to the difficulty of learning diverse behaviors in pixel-based environments using the mutual information objective. Furthermore, METRA\({}^{}\) demonstrates superior performance in this experiment, likely because designing a reward function that incorporates human intentions is straightforward (assigning +1 for the safe region and 0 for the hazardous region).

#### 5.2.3 Can DoDon't learn diverse behaviors without learning unsafe behaviors?

In real-world scenarios, agents should also avoid learning risky behaviors. For instance, actions such as rolling on the ground or flipping upside down can potentially damage the robot. Therefore, in this experiment, we aim to investigate whether DoDon't can learn a variety of desirable behaviors while avoiding certain unwanted behaviors. We trained our agent using running videos as desirable behaviors (Do's) and rolling or flipping videos as undesirable behaviors (Don'ts) for both Quadruped and Cheetah environments.

As shown in Figure 5, compared to METRA without any prior knowledge, DoDon effectively learns running behaviors while avoiding hazardous actions such as rolling or flipping using our instruction videos. Again, SMERL and DGPO fail to learn diverse behaviors, merely learning the simple behavior of standing upright at the starting point. For METRA\({}^{}\), we designed the reward function as \(r_{}-r_{}\) to encourage running while avoiding flipping or rolling. We observe that METRA\({}^{}\) cannot fully encapsulate human intention in Cheetah environment. We believe this issue

Figure 4: **Visualization and comparison of learned skills. In both environments, the left side is hazardous and the right side is safe. Safe state coverage assesses the agent’s ability to cover safe areas and avoid hazards.**

Figure 5: **Learning safe and diverse behaviors. Zero-shot rewards assess how effectively each method learns desired behaviors while avoiding hazardous ones.**

arises from the entanglement of the task signals in the reward function which makes the agent difficult from discerning which type of behaviors are beneficial and which are not. To receive the run reward, the agent must move forward, but as the body moves, it sometimes triggers the flip reward, which penalizes the agent, hindering the agent from properly learning the run behavior. This illustrates that balancing multiple behavioral intentions is challenging, making the creation of hand-crafted reward functions difficult and unscalable.

#### 5.2.4 Manipulation task

In this experiment, we aim to demonstrate that our intention network can be applied to a manipulation environment to learn multiple tasks. We use six target tasks following previous works . To effectively learn these six tasks, we use the D4RL dataset as Do's and random action videos as Don'ts. As shown in Figure 6, through our instruction network, DoDon successfully performs more tasks compared to METRA. SMERL and DOPO only learn two to three tasks, which we believe is due to the failure of the MI-based objective to capture diverse behaviors in pixel-based environments. For METRA\({}^{}\), we use a hand-designed reward function by summing the rewards for all six tasks. Nevertheless, METRA\({}^{}\) does not effectively learn the tasks, likely due to the entanglement of multiple behavioral signals within a single reward function.

### Ablation Studies

#### 5.3.1 Model components

We conduct an ablation analysis to demonstrate the importance of each component of our algorithm in achieving high performance. Specifically, we experimented with three variants: (i) using only one Do's video, (ii) optimizing Equation 5, referred to as delayed reward, and (iii) focusing solely on encouraging Do's behaviors. We tested these variants in a Quadruped environment, using run videos as Do's and random action videos as Don'ts.

Impact of data quantity.While we are already training with a relatively small amount of videos as our default setting (four Do's, four Don'ts), we also conduct an ablation study where we use only two videos (one Do, one Don't) to see how well our method performs under extremely limited data conditions. According to Figure 7, even with only one Do's video, the agent can mimic the running behavior, as evidenced by the zero-shot run reward. Remarkably, despite being exposed to just one video showing movement in a single direction, the agent is still able to learn movements in multiple directions; please refer to the project page videos (link).

Direct reward signal.To evaluate the effectiveness of Equation 7, we trained an agent using Equation 5. As shown in Figure 7, the direct instruction signal effectively captures the desired behavior from the early stages of training. In contrast, the delayed instruction signal initially fails to capture the desired behavior, as evidenced by drop in the zero-shot reward at the beginning. Consequently, the agent takes a long time to overcome before it can stand up again.

Only encouraging Do's behaviors.To further analyze the importance of penalizing Don's behaviors, we trained an agent only to encourage Do's behaviors. As illustrated in Figure 7, solely encouraging Do's behaviors fails to capture desirable behaviors. We speculate that without penalizing unwanted behaviors, the agent cannot avoid these undesirable actions, leading to a drop in the zero

Figure 6: **Policy task coverage.** The number of tasks successfully completed by sampled skills.

Figure 7: **Ablation study on model components.**

shot Run return at the beginning. Consequently, it takes a significant amount of time for the agent to learn the desired behaviors. Further details of this experiment in Appendix C.

#### 5.3.2 The importance of utilizing instruction network as the distance metric

In this study, we use the instruction network as the distance metric for the distance-maximizing skill discovery algorithm . This setup is convenient because it only requires multiplying the instruction network with the intrinsic USD reward to train the policy. However, an alternative approach is to add the output of the instruction network with the intrinsic USD reward, similar to SMERL  and DGPO . To compare these two approaches (multiplication and addition), we conducted experiments in the Cheetah environment following the setup from Section 5.2.2.

Our qualitative results, shown in Figure 8, indicate that none of the additive variants (\(r_{}=r_{}+_{}(s,s^{})\)) match the performance of the multiplicative approach (\(r_{}=r_{}_{}(s,s^{})\)). We attribute this to the fact that multiplying \(_{}(s,s^{})\) with the intrinsic reward function controls the scale of the total reward effectively since it directly impacts the influence of \(r_{}\) while adding \(_{}(s,s^{})\) indirectly impacts the influence of \(r_{}\). Although additive variants might achieve similar performance, they would likely require careful tuning of coefficients to balance the multiple reward components.

## 6 Conclusions, limitations, and future work

In this paper, we introduce DoDont, an instruction-based skill discovery algorithm designed to combine human intention with unsupervised skill discovery. We empirically demonstrate that our instruction network enables the agent to effectively learn desirable behaviors and avoid undesirable ones across a range of realistic instruction scenarios.

However, our study faces certain limitations. Specifically, in this research, we trained the instruction network using in-domain video data, which is not a readily available resource in real-world applications. We propose that training the instruction model with general, in-the-wild video data represents a scalable and compelling direction for future investigation.

Despite these constraints, DoDont exhibits promising results even with a limited video dataset. Moreover, we believe that videos are a cost-effective form of data for representing behaviors as they do not require action and reward labels. Thus, preparing data is both practical and feasible. On the other hand, as video generation models advance, several recent works have utilized these models as simulators [50; 51; 12; 6]. There is potential to generate Do's and Don'ts videos with these models, which could allow for the use of generated data without the need for additional data collection.

As final remarks, in unsupervised skill discovery, there are many excellent prior works that have succeeded in learning diverse behaviors without any supervision, data, or prior knowledge. However, these methods often struggle to acquire complex behaviors and may learn hazardous behaviors, making them unsuitable for direct real-world application. Therefore, we believe that the next step in unsupervised reinforcement learning focuses on leveraging minimal supervision or data (i.e., resources that are easily obtainable in the real world) to become scalable and applicable in complex control tasks and practical for the real world.

Figure 8: **Left:** Visualization of acquired skills, with the hazardous zone on the left and the safe zone on the right. **Right:** Quantitative comparison of each method.