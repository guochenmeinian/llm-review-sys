# UniGAD: Unifying Multi-level Graph Anomaly Detection

Yiqing Lin\({}^{1}\), Jianheng Tang\({}^{2,3}\), Chenyi Zi\({}^{3}\), H.Vicky Zhao\({}^{1}\), Yuan Yao\({}^{2}\), Jia Li\({}^{2,3}\)

\({}^{1}\)Tsinghua University

\({}^{2}\)Hong Kong University of Science and Technology

\({}^{3}\)Hong Kong University of Science and Technology (Guangzhou)

linyq20@mails.tsinghua.edu.cn, jtangbf@connect.ust.hk,

barristanzi666@gmail.com, vzhao@tsinghua.edu.cn, {yuany,jialee}@ust.hk

Work done as a visiting student at Hong Kong University of Science and Technology.Corresponding Author.

###### Abstract

Graph Anomaly Detection (GAD) aims to identify uncommon, deviated, or suspicious objects within graph-structured data. Existing methods generally focus on a single graph object type (node, edge, graph, etc.) and often overlook the inherent connections among different object types of graph anomalies. For instance, a money laundering transaction might involve an abnormal account and the broader community it interacts with. To address this, we present UniGAD, the first unified framework for detecting anomalies at node, edge, and graph levels jointly. Specifically, we develop the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler) that unifies multi-level formats by transferring objects at each level into graph-level tasks on subgraphs. We theoretically prove that MRQSampler maximizes the accumulated spectral energy of subgraphs (i.e., the Rayleigh quotient) to preserve the most significant anomaly information. To further unify multi-level training, we introduce a novel GraphStitch Network to integrate information across different levels, adjust the amount of sharing required at each level, and harmonize conflicting training goals. Comprehensive experiments show that UniGAD outperforms both existing GAD methods specialized for a single task and graph prompt-based approaches for multiple tasks, while also providing robust zero-shot task transferability. All codes can be found at https://github.com/lllyygq1121/UniGAD.

## 1 Introduction

Graph Anomaly Detection (GAD) involves identifying a minority of uncommon graph objects that significantly deviate from the majority within graph-structured data [17; 2]. These anomalies can manifest as abnormal nodes, unusual relationships, irregular substructures within the graph, or entire graphs that deviate significantly from others. GAD has many practical applications in various contexts, including the identification of bots and fake news on social media [3; 1; 4; 30], detection of sensor faults and internet invasions in IoT networks [8; 14], and prevention of fraudsters and money laundering activities in transaction networks [19; 55]. The mainstream GAD models originate from the Graph Neural Networks (GNNs), which have recently gained popularity for mining graph data [52; 24; 57; 16]. To address the specific challenges of graph anomalies such as label imbalance [34; 31], relation camouflage [12; 38], and feature heterophily [50; 15], numerous adaptations of standard GNNs have been proposed [9; 67; 13; 40; 39; 10; 53; 41; 62].

However, existing GAD approaches typically focus on a single type of graph object, such as node-level or graph-level anomaly detection, often overlooking the inherent correlations between different typesof objects in graph-structured data. For example, a money laundering transaction might involve both an abnormal account and the broader community it interacts with, while the specific cancer of a cell is determined by particular proteins or protein complexes within the cell. Although some unsupervised methods fuse information from nodes, edges, and subgraphs through reconstruction [27; 10; 47] or contrastive pre-training [58; 13; 36], they are still limited to single-level label supervision or prediction. There is a need for a unified approach that considers these correlations information across different levels and performs multi-level anomaly detection.

To design a unified model for addressing multi-level GAD, we identify two key challenges:

**1. _How to unify multi-level formats?_** Addressing node-level, edge-level, and graph-level tasks uniformly is challenging due to their inherent differences. Some recent works provide insights into unifying these tasks through the use of large language models (LLMs) or prompt tuning. While some methods leverage the generalization capability of LLMs [32; 54; 29] on text-attributed graphs, such semantic information is often unavailable in anomaly detection scenarios due to privacy concerns. On the other hand, graph prompt learning methods [48; 37; 61] design induced \(k\)-hop graphs to transform node or edge levels into graph-level tasks. Nevertheless, their sampling strategies are not specifically tailored to anomaly data, resulting in inappropriate node selections that 'erase' critical anomaly information. This oversight can severely impact the effectiveness of anomaly detection.

**2. _How to unify multi-level training?_** Training a single model for multi-level tasks involves various influencing factors, such as transferring information between different levels and achieving a balanced training of these level tasks. There is limited research on multi-task learning in the graph learning domain. Efforts like ParetoGNN  employ multiple self-supervised learning objectives (e.g., similarity, mutual information) as independent tasks, but these are insufficient for managing multi-level supervision. A comprehensive approach is needed to effectively integrate and balance the training of different level tasks in multi-level GAD.

In this paper, we propose UniGAD, a unified GAD model that leverages the transferability of information across node-level, edge-level, and graph-level tasks. To address the first challenge, we develop a novel subgraph sampler, **MRQSampler**, that maximizes accumulated spectral energy (i.e., the Rayleigh quotient) in the sampled subgraph with theoretical guarantee, ensuring that the sampled subgraphs contain the most critical anomaly information from nodes and edges. For the second challenge, we introduce the **GraphStitch** Network, which unifies multi-level training by integrating separate but identical networks for nodes, edges, and graphs into a unified multi-level model. This is achieved using a novel GraphStitch Unit that facilitates information sharing across different levels while maintaining the effectiveness of individual tasks. We perform comprehensive experiments on 14 GAD datasets and compare 17 state-of-the-art methods covering both node-level and graph-level GAD techniques, as well as prompt-based general multi-task graph learning methods. Results show that UniGAD achieves superior performance and offers robust zero-shot transferability across different tasks.

## 2 Related Work and Preliminaries

**Graph Anomaly Detection.** Leveraging deep learning techniques in GAD has led to significant advancements and a wide range of applications [3; 14; 1; 4; 19; 65], thoroughly reviewed in a comprehensive survey . Node-level anomaly detection, the most prevalent scenario in GAD, has witnessed numerous adaptations and improvements in graph neural networks (GNNs) aimed at

Figure 1: The overall framework of UniGAD.

enhancing performance from either a spatial [34; 38; 25] or spectral [28; 50; 15] perspective. Despite these advancements, recent benchmarks such as BOND  for unsupervised settings and GADBench  for supervised settings reveal that no single model excels across all datasets, highlighting the need for model selection tailored to specific datasets and task characteristics. For graph-level anomaly detection, various methodologies have been proposed, including transformation learning , knowledge distillation , and evolutionary mapping . SIGNET  employs information bottleneck to generate informative subgraphs for explaining graph-level anomalies, while Rayleigh Quotient GNN  explores the spectral properties of anomalous graphs. Although both node-level and graph-level anomaly detection are rapidly evolving fields, to the best of our knowledge, there is no existing model that supports the joint detection of both node-level and graph-level anomalies.

**Multi-task Learning on Graphs.** Multi-task learning involves training a model to handle multiple tasks simultaneously, utilizing shared representations and relationships within the graph to enhance performance across all tasks. Recently, techniques such as graph prompt-based approaches and large language model (LLM)-based approaches have shown promise in this area. Prompt frameworks  like GraphPrompt , All-in-One , PRODIGY , MultiGPrompt , and SGL-PT  are designed to address a wide array of graph tasks. These approaches transform tasks at other levels into graph-level tasks by leveraging induced graphs. The All-in-One framework enhances connectivity by adding links between the prompt graph and the original graph, whereas GraphPrompt inserts the prompt token into graph nodes through element-wise multiplication. On the other hand, LLM-based frameworks [32; 54; 29; 6; 51] utilize the power of LLMs to learn from different levels, but they require graphs with text attributes or descriptions, which are not applicable in most anomaly detection scenarios. Additionally, some multi-task GNN efforts  focus on multiple self-supervised specific objectives (such as similarity and mutual information) as independent tasks, which are not suitable for unifying GAD with multi-level label supervision and prediction.

**Notation.** Let \(=\{,,\}\) denote a connected undirected graph, where \(=\{v_{1},v_{2},...,v_{N}\}\) is the set of \(N\) nodes, \(=\{e_{ij}\}\) is the set of edges, and \(^{n F}\) is node features. Let \(\) be the corresponding adjacency matrix, \(\) be the degree matrix with \(_{ii}=_{j}_{ij}\). Laplacian matrix \(\) is then defined as \(-\) (regular) or as \(-^{-}^{-}\) (normalized), where \(\) is an identity matrix. The Laplacian matrix is a symmetric matrix and can be eigen-decomposed as \(=^{T}\), where the diagonal matrix \(\) consists of real eigenvalues (graph spectrum). Besides, we define the subgraph as \(_{i}\) centered on node \(v_{i}\) and our sampled subgraph for node \(v_{i}\) as \(_{i}\).

**Problem Formulation.** The multi-level graph anomaly detection problem introduces a more universal challenge compared to traditional single-level approaches, described as follows:

**Definition 2.1** (Multi-level GAD).: _Given a training set \(r(,,)\) containing nodes, edges, and graphs with arbitrary labels at any of these levels, the goal is to train a unified model to predict anomalies in a test set \(e(,,)\), which also contains arbitrary labels at any of these levels._

Note that our approach does not require the presence of labels at all three levels simultaneously. It is feasible to have labels at one or more levels. Our proposed model aims to leverage the transferability of information across different levels to enhance its predictive capability.

## 3 Methodology

This section details the proposed model UniGAD for multi-level GAD, comprising a GNN encoder, MRQSampler, and GraphStitch Network, as shown in Fig. 1. Firstly, a shared pre-trained unsupervised GNN encoder is utilized to learn a more generalized node representation. To unify multi-level formats, the MRQSampler employs spectral sampling to extract subgraphs that contain the highest amount of anomalous information from nodes and edges, thus converting tasks at all three levels into graph-level tasks (Sec. 3.1). To unify multi-level training, the GraphStitch Network integrates information from different levels, adjusts the amount of sharing required at each level, and harmonizes conflicting training goals. (Sec. 3.2).

### Spectral Subgraph Sampler for Unifying Multi-level Formats

In this subsection, we present the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler), the core module of our unified framework. By sampling subgraphs of nodes or edges, we transform node-level and edge-level tasks into graph-level tasks. Our sampler optimizes these subgraphs to maximize the Rayleigh quotient, ensuring that the sampled subgraphs retain a higher concentration of anomaly information.

#### 3.1.1 Analysis of the Subgraph Sampling

**What is a suitable subgraph for GAD?** Existing methods on selecting subgraphs for target nodes or edges often use straightforward approaches like \(r\)-ego or \(k\)-hop subgraphs . However, the size of the subgraph is critical for classification outcomes. If the subgraph is too large, it includes too many irrelevant nodes, while if it is too small, it may not align effectively with graph-level tasks.

To measure anomaly information in a subgraph, recent studies  have identified a 'right-shift' phenomenon in the spectral energy distribution, moving from low to higher frequencies. This accumulated spectral energy can be quantified by the Rayleigh quotient :

\[RQ(,)=^{T}}{^{T}}=}A_{ij}(x_{j}-x_{i})^{2}}{_{i}x_{i}^{2}}.\] (1)

The following lemma  illustrates the relationship between the Rayleigh quotient \(RQ(,)\) and anomaly information:

**Lemma 1** (Tang, 2022).: _Rayleigh quotient \(RQ(,)\), i.e. the accumulated spectral energy of the graph signal, is monotonically increasing with the anomaly degree._

Thus, for any node \(v_{i}\), our sampling objective is to identify the induced subgraph with the highest Rayleigh quotient containing the most anomaly information.

**Where to Sample Subgraph From?** To preserve the properties of target nodes, it is essential to sample subgraphs centered around these nodes, capturing key surrounding nodes. The most intuitive methods are \(r\)-ego graphs or \(k\)-hop graphs. However, considering the message-passing mechanisms of most GNNs , a classical work  provides valuable insight:

**Lemma 2** (Xu, 2018).: _A GNN recursively updates each node's feature vector through its rooted subtree structures to capture the network structure and features of surrounding nodes._

As shown in Fig. 2, the message-passing process of GNNs suggests that a rooted subtree centered on the target node is more consistent with the GNN's architecture. Therefore, we sample subgraphs from these rooted subtree structures. The remaining question is: _How to implement subgraph sampling based on the above?_ To address this, we introduce a novel MRQSampler in the next subsection.

#### 3.1.2 Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler)

Building on the motivation in Section 3.1.1, our approach involves sampling subgraphs for each node starting from the rooted subtree with the node as its root. The target node is always included. We then select the subtree with the maximum Rayleigh quotient from all possible subtrees as the representative subgraph for that node to ensure it contain the maximum anomaly information. We formulate this as the following optimization problem:

\[^{}=*{arg\,max}_{ }&_ {S}}(x_{p}-x_{q})^{2}}{_{p}x_{p}^{2}},\\ & v,\\ & v_{p},\;(v,v_{p})\] (2)

where \(\) represents \(k\)-depth rooted subtree from \(v\), and \(\) is a possible subgraph from \(\). The first constraint ensures the target node is included, and the second constraint ensures message passability. Generally, similar selecting subgraphs in this manner is considered an NP-Hard problem . However, leveraging the properties of trees, we propose an algorithm to solve **the optimal solution**.

Figure 2: Message passing in GNNs and rooted subtree sampling.

We first determine the conditions that increase a subgraph's Rayleigh quotient when adding a node, presented in the following theorem:

**Theorem 1**.: _For a graph \(\), let one of its subgraphs be \(\), and let its Rayleigh quotient be \(RQ()\). If a new node \(v_{new}-\) is added to \(\), the Rayleigh quotient \(RQ()\) will increase if and only if:_

\[(v_{new})=}(x_{new}-x_{r})^{2}}{x_{new}^ {2}}>RQ().\] (3)

The proof of Theorem 1 can be found in Appendix A.1. We can extend this theorem from a single new node \(v_{new}\) to a new node set \(_{new}\), leading to the following corollary:

**Corollary 1**.: _For a graph \(\), let one of its subgraphs be \(\), and let its Rayleigh quotient be \(RQ()\). If a new nodeset \(new-\) is added to \(\), the Rayleigh quotient \(RQ()\) will increase if and only if:_

\[(_{new})=+_{new}} (x_{new_{i}}-x_{r})^{2}+_{(i,j)_{V_{new}}}(x_{new_{i}}-x_{ new_{j}})^{2}}{_{v_{new}_{new}}x_{new}^{2}}>RQ().\] (4)

The proof details are also in Appendix A.2. While the above analysis can indeed increase the Rayleigh quotient of the sampled subgraph, the sampling order may cause the results to fall into a local optimum, which may not guarantee a globally optimal solution. To identify the nodes that must be sampled in the optimal subgraph, we present the following theorem:

**Theorem 2**.: _For a graph \(\), let one of its subgraph be \(\), the \(^{*}\) be its final optimal subgraph, and \(^{*}\). For a new **connected** nodeset \(}_{new} S=\), it is contained in \(^{*}\) when it satisfies:_

\[_{max}(}_{new})=_{}_{new} -}(}_{new}),_{max}(}_{new})>RQ().\] (5)

We refer readers to Appendix A.3 for the rigorous proof. Through the above analysis, we derive the conditions of the nodeset contained in the optimal subtree (Theorem 2). When \(}_{new}\) satisfies Eq. (5), it always increases the Rayleigh quotient based on the current subgraph, ensuring that \(_{new}\) is contained in the optimal solution. Thus, we decouple the problem of finding the subgraph with the maximum Rayleigh quotient into a process of finding the maximum \(_{max}(_{new})\) each time, until adding any node/node set fails to increase the \(RQ()\). Following this, we design a dynamic programming (DP) algorithm to ensure the optimal subset satisfies these conditions.

**MRQSampler Algorithm.** We introduce the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler), which uses dynamic programming (DP) to find the optimal solution. We break down the computation for the central node into sub-problems, storing the results of sub-problems to avoid redundant computations in future calculations. For a rooted subtree with the target node (edge) as the root, its children are unconnected to each other. In Fig. 3, we consider a 2-depth subtree and summarize our algorithm as follows:

* **Stage 1:** We recursively compute and store the maximum \((}_{new})\) for each subtree, which can be down into simpler sub-problems similar to the previous one and calculates each layer in the tree recursively from the bottom up.

Figure 3: MRQSampler: (i) Derive the condition (Theorem 2) satisfied with the optimal subtree. (ii) Decompose the problem into simpler sub-problems by recursing through the tree depth to solve the optimal subtree with the dynamic programming (DP) algorithm.

* **Stage 2:** Based on Theorem 2, we iteratively select the descendant with the maximum \(_{max}(}_{new})\) (within its own rooted subtree) of the target node and the currently selected nodeset, until the conditions of Theorem 2 are no longer satisfied, i.e., when the Rayleigh quotient of the sampled subgraph no longer increases.

For efficiency, this approach can obtain the subgraph with the maximum Rayleigh quotient of the target node/edge's rooted subtree while reducing the algorithmic complexity to \(O(N N)\). It can be further accelerated in parallel since the computation for different nodes is independent. Additionally, the sampling process only needs to be computed once in training and inference processes, minimally impacting model efficiency. For the detailed pseudocode of the algorithm, please refer to Appendix B. Note that we use mean pooling for entire graphs, but for subgraphs, we use weighted pooling to highlight central nodes/edges, with an exponential decay based on the number of hops to the central nodes/edges. This method transforms node-level and edge-level tasks into graph-level tasks, ensuring that the most anomaly information is retained in the sampled subgraphs.

### GraphStitch Network for Unifying Multi-level Training

After obtaining graph representations, training them together through a fully connected layer can negatively impact individual levels due to the inherent differences across different-level anomalies. This can result in mediocre performance at all levels. A key challenge, therefore, is to facilitate information transfer between multi-levels without compromising single-level effectiveness. Inspired by work in the computer vision field , we introduce the novel GraphStitch Network to jointly consider multi-level representations.

Specifically, we train separate but identical networks for each level and use the GraphStitch unit to combine these networks into a multi-level network, managing the degree of sharing required at different levels. This approach aims to maintain single-level effectiveness while enhancing multi-level information transfer. The network structure is illustrated in Fig. 4.

To elaborate, we denote \(_{N}\), \(_{E}\), and \(_{G}\) as the embeddings for nodes, edges, and graphs, respectively. The node embedding \(_{N}=(_{nn},_{ne},_{ng})^{}\) consists of outputs from three separate but identically structured networks specialized for nodes, edges, and graphs. Similarly, the edge and graph embeddings are represented as \(_{E}=(_{cn},_{ce},_{eg})^{}\) and \(_{G}=(_{gn},_{ge},_{gg})^{}\).

We define a GraphStitch operation as follows:

\[(}_{N},}_{E},}_{G})=diag [(_{nn}&_{ne}&_{ng}\\ _{en}&_{ee}&_{eg}\\ _{gn}&_{ge}&_{gg})(_{N}, _{E},_{G})].\] (6)

The sharing of representations is achieved by learning a linear combination of the outputs from the three networks. This linear combination is parameterized using learnable \(\). In particular, when training data lacks a certain level, the influence of that level on other levels is defined as zero during training but still retains the influence of other levels on this level. In this way, it allows the labels for training and testing to be arbitrary. Besides, if all the cross terms (\(_{ne}\), \(_{ng}\), \(_{cn}\), \(_{eg}\), \(_{gn}\), \(_{ge}\)) are equal to 0 means that training the three networks jointly is equivalent to training them independently. Finally, the embeddings for nodes, edges, and graphs are fed into three independent multi-layer perceptrons (MLPs) to compute the abnormal probabilities \(p_{i}^{}\), \(p_{i}^{}\), and \(p_{i}^{Q}\), respectively.

Figure 4: GraphStitch network structure in UniGAD. Node level is highlighted.

[MISSING_PAGE_EMPTY:7]

anomaly detection, we consider six state-of-the-art methods: OCGIN , OCGTL , GLocalKD , iGAD , GmapAD , and RQGNN . Additionally, to compare multi-task models, we include two recent multi-task graph prompt methods: GraphPrompt  and All-in-One . While these methods were not originally proposed for joint multi-task training, we adapt their ideas and develop multi-task versions for our comparison, GraphPrompt-U and All-in-One-U, whose modifications were limited to the data preprocessing component to accommodate the simultaneous handling of multiple object types (node/edge or node/graph) within induced graphs.

**Implementations.** We evaluate three metrics: AUROC (Sec. 4), Macro F1-score and AUPRC (Appendix E). For each result, we conduct 5 runs and report the mean results. In UniGAD, we choose two backbone GNN encoders: GCN  and BWGNN . We use a shared graph pre-training method, GraphMAE , to obtain a more generalized node representation. For multi-dimensional feature vectors, we normalize all feature dimensions and then take the norm (1-norm in our case) to obtain a composite feature for each node, allowing us to identify the most anomalous nodes in MRQSampler based on this comprehensive feature. To avoid data leakage, for single-graph datasets, edges between the training set and the testing set are not considered; for multi-graph datasets, the training set and the testing set consist of different graphs and their nodes. More details on the implementation can be found in the Appendix C.

### Multi-Level Performance Comparison (RQ1)

To compare the performance of multi-level anomaly detection, we conduct experiments under two settings. For the single-graph datasets, we compare the performance of unified training on node-level and edge-level data. For the multi-graph datasets, we compare the performance of unified training on node-level and graph-level data.

**Node-level and edge-level jointly.** We first evaluate the performance of unified training on node-level and edge-level data. We compare UniGAD against three groups of GNN models mentioned above: node-level models, edge-level models, and multi-task graph learning methods. Table 2 reports the AUROC of each model on six datasets, with the best result on each dataset highlighted in boldface. Overall, we find that UniGAD achieves state-of-the-art performance in nearly all scenarios. UniGAD outperforms single-level specialized models, indicating that unified training with UniGAD leverages information from other levels to enhance the performance of individual tasks. Multi-task approaches (GraphPrompt-U and All-in-One-U) tend to negatively impact multi-task performance, potentially because they are unable to effectively handle different types of anomaly label supervision. Meanwhile, UniGAD is designed for a multi-task setting, the performance on a single level might be slightly compromised to ensure the model performs well across all tasks in some datasets.

**Node-level and graph-level jointly.** We then evaluate the unified training of node-level and graph-level tasks under similar settings. Table 3 shows the results, and UniGAD achieves state-of-the-art performance in nearly all scenarios. Our observations are as follows. First, there is a multi-level synergy in UniGAD, where strong performance in one task benefits the performance of other tasks.

   &  &  &  &  &  &  &  &  \\   & **Task-level** & **Node** & **Edge** & **Node** & **Edge** & **Node** & **Edge** & **Node** & **Edge** & **Node** & **Edge** & **Node** & **Edge** \\   & GCN & 62.60 & / & 97.97 & / & 82.37 & / & 57.62 & / & 75.21 & / & 70.15 & / & 89.70 & / \\  & GIN & 65.59 & / & 95.64 & / & 92.17 & / & 74.46 & / & 75.15 & / & 69.13 & / & 86.43 & / \\  & GraphSAGE & 622.5 & / & 94.45 & / & 84.53 & / & 82.12 & / & 79.74 & / & 72.47 & / & 78.16 & / \\  & SOC & 52.12 & / & 97.11 & / & 80.24 & / & 53.03 & / & 69.51 & / & 70.29 & / & 74.21 & / \\  & GAT & 65.87 & / & 94.40 & / & 96.24 & / & 77.40 & / & 78.90 & / & 71.38 & / & 96.90 & / \\  & Bennet & 66.68 & / & 93.93 & / & 96.62 & / & 81.48 & / & 76.68 & / & 70.28 & / & 92.37 & / \\  & PNA & 65.28 & / & 97.42 & / & 81.41 & / & 78.18 & / & 75.82 & / & 71.78 & / & 86.17 & / \\  & AMNet & 68.31 & / & 94.17 & / & 97.31 & / & 81.42 & / & 76.67 & / & 68.63 & / & 93.53 & / \\  & BWGNN & 64.65 & / & 97.42 & / & 97.80 & / & 81.31 & / & 80.51 & / & 70.25 & / & 96.03 & / \\   & GCN & / & 63.10 & / & 99.03 & / & 78.63 & / & 57.80 & / & 73.59 & / & 79.05 & / & 87.63 \\  & GINE & & 67.26 & / & 98.00 & / & 72.94 & / & 67.58 & / & 69.27 & / & 80.75 & / & 92.05 \\  & GSAGADE & & / & **67.52** & / & 98.67 & / & 78.92 & / & 73.30 & / & **76.98** & / & **87.51** & / & 77.14 \\  & SGCC & & / & 53.36 & / & 98.55 & / & 76.41 & / & 52.02 & / & 70.59 & / & 72.41 & / & 69.01 \\  & GATE & & / & 77.07 & / & 97.92 & / & 90.20 & / & 72.96 & / & 71.92 & / & 81.64 & / & 83.00 \\  & Bennet & & 65.57 & / & 97.87 & / & 89.60 & / & 73.83 & / & 73.39 & / & 84.78 & / & 87.80 \\  & PNAE & & / & 64.15 & / & 99.10 & / & 75.71 & / & 67.88 & / & 70.9 & / & 84.05 & / & 83.91 \\  & AME & & / & 66.73 & / & 97.08 & / & 89.36 & / & 73.89 & / & 71.99 & / & 81.93 & / & 86.19 \\  & BWE & & 67.73 & / & 98.93 & / & 91.61 & / & 75.63 & / & 75.66 & / & 80.00 & / & 92.27 \\   & GraphPrompt-U & 50.03 & 49.78 & 52.90 & 50.71 & 50.01 & 50.96 & 49.83 & 49.56 & 51.24 & 49.66 & 55.16 & 50.00 & OOT & OOT \\  & All-in-One-U & 51.35 & 54.10 & 48.61 & 52.63 & 56.11 & 54.80 & 49.77 & 49.13 & 50.41 & 49.29 & 51.49 & 62.41 & OOT & OOT \\   UniGAD \\ (Ours) \\  } & UniGAD & GCN & **71.65** & 63.46 & 99.02 & **99.13** & 82.92 & 80.04 & 63.22 & 61.74 & 77.26 & 72.89 & **73.92** & 74.27 & 56.63 & 91.75 \\  & UniGAD & BWG & 64.42 & 53.60 & **99.07** & 99.10 & **97.84** & **92.18** & **68.23** & **79.05** & **80.62** & 74.85 & 70.97 & 73.45 & **96.49** & **94.32** \\  

Table 2: Comparison of unified performance (AUROC) at both node and edge levels with different single-level methods, multi-task methods, and our proposed method.

For example, in MNIST-0 and MNIST-1, compared to other graph-level GAD methods, UniGAD significantly boosts graph-level performance by leveraging strong node-level results. Second, UniGAD performs better on large graphs, likely because graph structure plays a more significant role in smaller datasets. However, the backbones of UniGAD (GCN, BWGNN) are primarily node-level models, which may not effectively encode graph-level structural information. This limitation's impact diminishes in large-scale graph datasets. Besides, methods like All-in-One-U often run out of time (OOT) with large datasets because they redundantly learn the same node representations across different subgraphs, making processing impractically slow, especially for large graph-level datasets like T-Group. UniGAD addresses this issue by using a shared GNN encoder across tasks, avoiding redundant learning and enhancing efficiency.

### The Transferability in Zero-Shot Learning (RQ2)

To assess the transfer capability of UniGAD, we explore zero-shot learning scenarios where labels for a given level have never been exposed during training, as shown in Tables 4 and 5. In these experiments, UniGAD is trained solely with labels from alternative levels. The notation \(\) indicates using node labels to infer edge labels, with analogous notations for other label transfers. Our findings indicate that in zero-shot scenarios, UniGAD outperforms existing multi-task prompt learning methods. Moreover, the classification performance of UniGAD under zero-shot transfer learning even surpasses some of the leading baselines in supervised settings on Yelp and BM-MS. It highlights the superior transfer capability of UniGAD across various GAD tasks.

### Ablation Study (RQ3)

To investigate the contribution of each module in UniGAD, we present the ablation study results in Table 6. For the sampler module, we compare the results without subgraph sampling (w/o GS.), using a simple sampler with all 2-hop neighbors (w 2hop.), and using random sampling (w RS.). For the Graph-Stitch module, we replace it with a unified MLP (w/o ST.). The results indicate that both the subgraph sampler (SG.) and the GraphStitch (ST.) modules enhance the overall performance of UniGAD. Additionally,

    &  &  \\  Metrics &  &  &  &  \\ Task-level & node & graph & node & graph & node & edge & node & edge \\   GroupPrompt-U \\ All-in-One-U \\  } & 97.13 & 98.99 & 80.35 & 95.79 & 68.69 & 66.06 & 53.83 & 52.78 \\  & 97.49 & 99.94 & 67.29 & 84.20 & 67.53 & 63.62 & 51.77 & 50.69 \\  & 93.85 & 84.92 & 85.88 & 72.21 & 65.32 & 61.85 & 52.32 & 51.03 \\   w/o ST. \\ UniGAD \\  } & 99.94 & 95.51 & 99.47 & 84.91 & 67.74 & 65.92 & 54.35 & 52.52 \\  UniGAD & 99.60 & 99.67 & 99.57 & 95.86 & 71.65 & 65.46 & 56.70 & 53.80 \\   

Table 6: Performance of UniGAD and its variants.

    &  &  &  &  &  &  &  \\   & **N-\%** & **E-\(\)N** & **N-\(\)E** & **\(\)N** & **N-\(\)E** & **\(\)N** & **N-\(\)E** & **\(\)N** & **N-\(\)E** & **\(\)N** & **N-\(\)E** & **\(\)N** \\  GraphPrompt-U & \(54.06\) & \(47.43\) & \(57.03\) & \(42.85\) & \(49.76\) & \(50.26\) & \(49.97\) & \(49.94\) & \(48.56\) & \(51.08\) & \(54.26\) & \(51.97\) & OOT & OOT \\ Ali-in-One-U & \(49.23\) & \(49.93\) & \(52.22\) & \(54.30\) & \(52.61\) & \(42.35\) & \(49.48\) & \(44.50\) & \(48.34\) & \(50.22\) & \(49.83\) & \(51.97\) & OOT & OOT \\  UniGAD - GCN & **59.67** & **59.46** & **98.31** & **98.95** & 76.20 & 82.38 & 58.28 & 60.92 & 71.45 & 73.35 & 69.54 & **65.37** & 91.63 & \(90.17\) \\ UniGAD - BWG & \(53.32\) & \(57.63\) & \(94.71\) & \(96.87\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(62.72\) & \(\) & \(\) \\   

Table 4: Zero-shot transferability (AUROC) at node and edge levels.

    &  &  &  &  &  &  &  \\   & **Task-level** & **Node** & **Graph** & **Node** & **Graph** & **Node** & **Graph** & **Node** & **Graph** & **Node** & **Graph** & **Node** & **Graph** \\   Node-level \\  } & GCN & 86.11 & 90.17 & - & 92.30 & 92.38 & 91.40 & - & 91.34 & - & 91.81 & - & 91.81 & - \\  & GIN & \(56.73\) & - & - & - & - & - & - & - & - & - & - & - \\  & \(50.08\) & - & - & - & - & - & - & - & - & - & - & - & - \\  & \(50.08\) & - & - & - & - & - & - & - & - & - & - & - \\  & GAT & \(58.47\) & - & - & - & - & - & - & - & - & - & - & - \\  & \(60.06\) & - & - & - & - & - & - & - & - & - & - & - & - \\  & \(72.96\) & - & - & - & - & - & - & - & - & - & - & - & - & - \\  & BWGNN & \(50.65\) & - & - & - & - & - & - & - & - & - & - & - & - \\   Graph-level \\  } & OCGNN & 98.46 & - & 81.37 & - & 58.05 & - & 80.50 & - & - & - & - & - & - & - & - & - & - & - & - & - \\  & \(7\) & - & - & - & - & - & - & - & - & - & - & - & - & - & - & -inappropriate subgraph sampling may perform worse than no subgraph sampling, likely due to the loss of anomalous information during the sampling process.

### Efficiency Analysis (RQ4)

we conduct a comprehensive evaluation of both time and space efficiency on the large-scale, real-world T-Group dataset. To provide a more straightforward comparison between single-task and multi-task baselines, we calculate the average, minimum, and maximum for combinations of single-task node-level and graph-level models, and compare these with multi-task models. The results, as shown in Fig. 5 (a), indicate that in terms of execution time, our method is slower than the combination of the fastest single-level models but faster than the average of the combination. Regarding peak memory usage, Fig. 5 (b) demonstrates that graph-level models consume significantly more memory than node-level models. Our method maintains memory consumption comparable to node-level models and substantially lower than both graph-level GAD models and prompt-based methods.

## 5 Conclusion

This paper presents UniGAD, a unified graph anomaly detection framework that jointly addresses anomalies at the node, edge, and graph levels. The model integrates two novel components: the MRQSampler and the GraphStitch network. MRQSampler maximizes spectral energy to ensure subgraphs capture critical anomaly information, addressing the challenge of unifying different graph object formats. The GraphStitch Network unifies multi-level training by using identical networks for nodes, edges, and graphs, facilitated by the GraphStitch Unit for effective information sharing. Our thorough evaluations across 14 GAD datasets, including two real-world large-scale datasets (T-Finance and T-Group), and comparisons with 17 graph learning methods show that UniGAD not only surpasses existing models in various tasks but also exhibits strong zero-shot transferability capabilities. A limitation of our paper is that the GNN encoder primarily focuses on node-level embeddings, which may result in lost information about the graph structure. We leave the exploration of multi-level tasks pre-training in the future works.

    &  &  &  &  &  &  \\  & \(6}\) & \(N}\) & \(6}\) & \(N}\) & \(6}\) & \(N}\) & \(6}\) & \(N}\) & \(6}\) & \(N}\) & \(6}\) & \(N}\) \\  GraphPrompt-U & \(50.60\) & \(51.57\) & \(51.97\) & \(46.95\) & \(46.62\) & \(48.06\) & \(59.62\) & \(64.26\) & \(83.98\) & \(\) & \(58.28\) & \(58.35\) \\ All-in-One-U & \(\) & \(65.69\) & \(52.63\) & \(40.88\) & \(44.86\) & \(34.27\) & \(61.63\) & \(36.13\) & OOT & OOT & OOT \\  UniGAD - GCN & \(72.82\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(70.57\) & \(\) & \(\) \\ UniGAD - BWG & \(64.61\) & \(57.56\) & \(65.33\) & \(51.34\) & \(55.78\) & \(53.41\) & \(66.92\) & \(87.03\) & \(74.23\) & \(63.70\) & \(86.81\) & \(64.81\) \\   

Table 5: Zero-shot transferability (AUROC) at node and graph levels.

Figure 5: The evaluation of time and space efficiency metrics. We highlight the percentage of total execution time spent by MRQSampler.