# Neural (Tangent Kernel) Collapse

Mariia Seleznova\({}^{1}\)1  Dana Weitzner\({}^{2}\)  Raja Giryes\({}^{2}\)  Gitta Kutyniok\({}^{1}\)  Hung-Hsu Chou\({}^{1}\)

\({}^{1}\)Ludwig-Maximilians-Universitat Munchen \({}^{2}\)Tel Aviv University

###### Abstract

This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.

## 1 Introduction

Deep Neural Networks (DNNs) are advancing the state of the art in many real-life applications, ranging from image classification to machine translation. Yet, there is no comprehensive theory that can explain a multitude of empirical phenomena observed in DNNs. In this work, we provide a theoretical connection between two such empirical phenomena, prominent in modern DNNs: _Neural Collapse (NC)_ and _Neural Tangent Kernel (NTK) alignment_.

Neural Collapse.NC  emerges while training modern classification DNNs past zero error to further minimize the loss. During NC, the class means of the DNN's last-layer features form a symmetric structure with maximal separation angle, while the features of each individual sample collapse to their class means. This simple structure of the feature vectors appears favourable for generalization and robustness in the literature . Though NC is common in modern DNNs, explaining the mechanisms behind its emergence is challenging, since the complex non-linear training dynamics of DNNs evade analytical treatment.

Neural Tangent Kernel.The NTK  describes the gradient descent dynamics of DNNs in the function space, which provides a dual perspective to DNNs' evolution in the parameters space. This perspective allows to study the dynamics of DNNs analytically in the infinite-width limit, where the NTK is constant during training . Hence, theoretical works often rely on the infinite-width NTK to analyze generalization of DNNs . However, multiple authors have argued that the infinite-width limit does not fully reflect the behaviour of realistic DNNs , since constant NTK implies that no feature learning occurs during DNNs training.

NTK Alignment.While the infinite-width NTK is label-agnostic and does not change during training, the empirical NTK rapidly aligns with the target function in the early stages of training . In the context of classification, this manifests itself as the emergence of a block structurein the kernel matrix, where the correlations between samples from the same class are stronger than between samples from different classes. The NTK alignment implies the so-called local elasticity of DNNs' training dynamics, i.e., samples from one class have little impact on samples from other classes in Stochastic Gradient Descent (SGD) updates . Several recent works have also linked the local elasticity of training dynamics to the emergence of NC [33; 53]. This brings us to the main question of this paper: _Is there a connection between NTK alignment and neural collapse?_

Contribution.In this work, we consider a model of NTK alignment, where the kernel has a _block structure_, i.e., it takes only three distinct values: an inter-class value, an intra-class value and a diagonal value. We describe this model in Section 3. Within the model, we establish the connection between NTK alignment and NC, and identify the conditions under which NC occurs. Our main contributions are as follows:

* We derive and analyze the training dynamics of DNNs with MSE loss and block-structured NTK in Section 4. We identify three distinct convergence rates in the dynamics, which correspond to three components of the training error: error of the global mean, of the class means, and of each individual sample. These components play a key role in the dynamics.
* We show that NC emerges in DNNs with block-structured NTK under additional assumptions in Section 5.3. To the best of our knowledge, this is the first work to connect NTK alignment and NC. While previous contributions rely on the unconstrained features models [21; 38; 48] or other imitations of DNNs' training dynamics  to derive NC (see Appendix A for a detailed discussion of related works), we consider standard gradient flow dynamics of DNNs simplified by our assumption on the NTK structure.
* We analyze when NC does or does not occur in DNNs with NTK alignment, both theoretically and empirically. In particular, we identify an invariant of the training dynamics that provides a necessary condition for the emergence of NC in Section 5.2. Since DNNs with block-structured NTK do not always converge to NC, we conclude that NTK alignment is a more widespread phenomenon than NC.
* We support our theory with large-scale numerical experiments in Section 6. Source code to reproduce the results is available in the project's GitHub repository.

## 2 Preliminaries

We consider the classification problem with \(C\) classes, where the goal is to build a classifier that returns a class label for any input \(x\). In this work, the classifier is a DNN trained on a dataset \(\{(x_{i},y_{i})\}_{i=1}^{N}\), where \(x_{i}\) are the inputs and \(y_{i}^{C}\) are the one-hot encodings of the class labels. We view the output function of the DNN \(f:^{C}\) as a composition of parametrized last-layer _features_\(h:^{n}\) and a linear _classification_ layer parametrized by weights \(^{C n}\) and biases \(^{C}\). Then the logits of the training data \(X=\{x_{i}\}_{i=1}^{N}\) can be expressed as follows:

\[f(X)=+_{N}^{},\] (1)

where \(^{n N}\) are the features of the entire dataset stacked as columns and \(_{N}^{N}\) is a vector of ones. Though we omit the notion of the data dependence in the text to follow, i.e. we write \(\) without the explicit dependence on \(X\), we emphasize that the features \(\) are a function of the data and the DNN's parameters, unlike in the previously studied unconstrained feature models [21; 38; 48].

We assume that the dataset is _balanced_, i.e. there are \(m:=N/C\) training samples for each class. Without loss of generality, we further assume that the inputs are reordered so that \(x_{(c-1)m+1},,x_{cm}\) belong to class \(c\) for all \(c[C]\). This will make the notation much easier later on. Since the dimension of features \(n\) is typically much larger than the number of classes, we also assume \(n>C\) in this work.

### Neural Collapse

Neural Collapse (NC) is an empirical behaviour of classifier DNNs trained past zero error . Let \( h:=N^{-1}_{i=1}^{N}h(x_{i})\) denote the global features mean and \( h_{c}:=m^{-1}_{x_{i}c}h(x_{i})\), \(c[C]\) be the class means. Furthermore, define the matrix of normalized centered class means as \(:=[_{1}/\|_{1} \|_{2},,_{C}/\|_{C} \|_{2}]^{}^{n C}\), where \(_{c}= h_{c}- h,c[C]\). We say that a DNN exhibits NC if the following four behaviours emerge as the training time \(t\) increases:

**(NC1)**: **Variability collapse:** for all samples \(x_{i}^{c}\) from class \(c[C]\), where \(i[m]\), the penultimate layer features converge to their class means, i.e. \(\|h(x_{i}^{c})- h_{c}\|_{2} 0\).
**(NC2)**: **Convergence to Simplex Equiangular Tight Frame (ETF):** for all \(c,c^{}[C]\), the class means converge to the following configuration:

\[\| h_{c}- h\|_{2}-\| h_{c^{}} - h\|_{2} 0,^{}( \,_{C}-_{C}_{C}^{}).\]
**(NC3)**: **Convergence to self-duality:** the class means \(\) and the final weights \(^{}\) converge to each other:

\[\|/\|\|_{F}-^{}/\|^{}\|_ {F}\|_{F} 0.\]
**(NC4)**: **Simplification to Nearest Class Center (NCC):** the classifier converges to the NCC decision rule behaviour:

\[*{argmax}_{c}(h(x)+)_{c} *{argmin}_{c}\|h(x)- h_{c}\|_{2}.\]

Though NC is observed in practice, there is currently no conclusive theory on the mechanisms of its emergence during DNN training. Most theoretical works on NC adopt the unconstrained features model, where features \(\) are free variables that can be directly optimized [21; 38; 48]. Training dynamics of such models do not accurately reflect the dynamics of real DNNs, since they ignore the dependence of the features on the input data and the DNN's trainable parameters. In this work, we make a step towards realistic DNN dynamics by means of the Neural Tangent Kernel (NTK).

### Neural Tangent Kernel

The NTK \(\) of a DNN with the output function \(f:^{C}\) and trainable parameters \(^{P}\) (stretched into a single vector) is given by

\[_{k,s}(x_{i},x_{j}):=_{}f_{k}(x_{i}), _{}f_{s}(x_{j}), x_{i},x_{j},  k,s[C].\] (2)

We also define the last-layer features kernel \(^{h}\), which is a component of the NTK corresponding to the parameters up to the penultimate layer, as follows:

\[^{h}_{k,s}(x_{i},x_{j}):=_{}h_{k}(x_{i}), _{}h_{s}(x_{j}), x_{i},x_{j},  k,s[n].\] (3)

Intuitively, the NTK captures the correlations between the training samples in the DNN dynamics. While most theoretical works consider the infinite-width limit of DNNs [30; 52], where the NTK can be computed theoretically, empirical studies have also extensively explored the NTK of finite-width networks [19; 45; 49; 36]. Unlike the label-agnostic infinite-width NTK, the empirical NTK aligns with the labels during training. We use this observation in our main assumption (Section 3).

### Classification with MSE Loss

We study NC for DNNs with the mean squared error (MSE) loss given by

\[(,,)=\|f(X)-\|_ {F}^{2},\] (4)

where \(^{C N}\) is a matrix of stacked labels \(y_{i}\). While NC was originally introduced for the cross-entropy (CE) loss , which is more common in classification problems, the MSE loss is much easier to analyze theoretically. Moreover, empirical observations suggest that DNNs with MSE loss achieve comparable performance to using CE [14; 29; 41], which motivates the recent line of research on MSE-NC [21; 38; 48].

## 3 Block Structure of the NTK

Numerous empirical studies have demonstrated that the NTK becomes aligned with the labels \(^{}\) during the training process [7; 32; 45]. This alignment constitutes feature learning and is associated with better performance of DNNs [9; 13]. For classification problems, this means that the empiricalNTK develops an approximate block structure with larger kernel values corresponding to pairs of samples \((x_{i}^{c},x_{j}^{c})\) from the same class . Figure 1 shows an example of such a structure emergent in the empirical NTK of ResNet20 trained on MNIST.2 Motivated by these observations, we assume that the NTK and the last-layer features kernel exhibit a block structure, defined as follows:

**Definition 3.1** (Block structure of a kernel).: _We say a kernel \(:^{K K}\) has a block structure associated with \((_{1},_{2},_{3})\), if \(_{1}>_{2}>_{3} 0\) and_

\[(x,x)=_{1}_{K},(x_{i}^{c},x_{j}^{c})= _{2}_{K},(x_{i}^{c},x_{j}^{c^{}})=_{3} _{K},\] (5)

_where \(x_{i}^{c}\) and \(x_{j}^{c}\) are two distinct inputs from the same class, and \(x_{j}^{c^{}}\) is an input from class \(c^{} c\)._

**Assumption 3.2**.: _The NTK \(:^{C C}\) has a block structure associated with \((_{d},_{c},_{n})\), and the penultimate kernel \(^{h}:^{n n}\) has a block structure associated with \((_{d},_{c},_{n})\)._

This assumption means that every kernel \(_{k,k}(X):=[_{k,k}(x_{i},x_{j})]_{i,j[N]}\) corresponding to an output neuron \(f_{k},k[C]\) and every kernel \(^{h}_{p,p}(X)\) corresponding to a last-layer neuron \(h_{p},p[n]\) is aligned with \(^{}\) (see Figure 1, panes a-b). Additionally, the "non-diagonal" kernels \(_{k,s}(X)\) and \(^{h}_{k,s}(X)\), \(k s\) are equal to zero (see Figure 1, panes c-d).3 Moreover, if \(_{c}_{n}\) and \(_{c}_{n}\), Assumption 3.2 can be interpreted as _local elasticity_ of DNNs, defined below.

**Definition 3.3** (Local elasticity ).: _A classifier is said to be locally elastic (LE) if its prediction or feature representation on point \(x_{i}^{c}\) from class \(c[C]\) is not significantly affected by performing SGD updates on data points from classes \(c^{} c\)._

To see the relation between Assumption 3.2 and this definition, consider a Gradient Descent (GD) step of the output neuron \(f_{k},k[C]\) with step size \(\) performed on a single input \(x_{j}^{c^{}}\) from class \(c^{} c\). By the chain rule, block-structured \(\) implies locally-elastic predictions since

\[f^{t+1}(x_{i}^{c})-f^{t}(x_{i}^{c})=-(x_{i}^{c},x_{j}^{c^{}}) (x_{j}^{c^{}})}{ f(x_{j}^{c^{}}) }+O(^{2}),\] (6)

i.e., the magnitude of the GD step of \(f(x_{i}^{c})\) is determined by the value of \((x_{i}^{c},x_{j}^{c^{}})\). Similarly, block-structured kernel \(^{h}\) implies locally-elastic penultimate layer features because

\[h^{t+1}(x_{i}^{c})-h^{t}(x_{i}^{c})=-^{h}(x_{i}^{c},x_{j}^{c^{ }})^{}(x_{j}^{c^{}})}{ f( x_{j}^{c^{}})}+O(^{2}).\] (7)

This observation provides a connection between our work and recent contributions suggesting a connection between NC and local elasticity .

Figure 1: The NTK block structure of ResNet20 trained on MNIST. **a)** Traced kernel \(_{k=1}^{C}_{k,k}(X)\) computed on a random data subset with 12 samples from each class. The samples are ordered as described in Section 2, so that the diagonal blocks correspond to pairs of inputs from the same class. **b)** Traced kernel \(_{k=1}^{n}^{h}_{k,k}(X)\) computed on the same subset. **c)** Norms of the kernels \(_{k,s}(X)\) for all \(k,s[C]\). **d)** Norms of the kernels \(^{h}_{k,s}(X)\) for all \(k,s[n]\). The color bars show the values in each heatmap as a fraction of the maximal value in the heatmap. **e)** The alignment of the traced kernels from panes **a** and **b** with the class labels.

## 4 Dynamics of DNNs with NTK Alignment

### Convergence

As a warm up for our main results, we analyze the effects of the NTK block structure on the convergence of DNNs. Consider a GD update of an output neuron \(f_{k},k[C]\) with the step size \(\):

\[f_{k}^{t+1}(X)=f_{k}^{t}(X)-_{k,k}(X)(f_{k}^{t}(X)-_{k})+O( ^{2}), k=1,,C.\] (8)

Note that we have taken into account that \(_{k,s}\) is zero for \(k s\) by our assumption. Denote the residuals corresponding to \(f_{k}\) as \(_{k}^{}:=f_{k}^{}(X)-_{k}^{N}\). Then we have the following dynamics for the residuals vector:

\[_{k}^{t+1}=(1-_{k,k}(X))_{k}^{t}+O(^{2}).\] (9)

The eigendecomposition of the block-structured kernel \(_{k,k}(X)\) provides important insights into this dynamics and is summarized in Table 1. We notice that the NTK has three distinct eigenvalues \(_{}_{}_{}\), which imply different convergence rates for certain components of the error. Moreover, the eigenvectors associated with each of these eigenvalues reveal the meaning of the error components corresponding to each convergence rate. Indeed, consider the projected dynamics with respect to eigenvector \(_{0}\) and eigenvalue \(_{}\) from Table 1:

\[_{k}^{t+1},_{0}=(1-_{})_{k}^{t},_{0},\] (10)

where we omitted \(O(^{2})\) for clarity. Now notice that the projection of \(_{k}^{t}\) onto the vector \(_{0}\) is in fact proportional to the average residual over the training set:

\[_{k}^{t},_{0}=_{k}^{t}, _{N}=N_{k}^{t}\] (11)

where \(\) denotes the average over all the training samples \(x_{i} X\). By a similar calculation, for all \(c[C]\) and \(i[m]\) we get interpretations of the remaining projections of the residual:

\[_{k}^{t},_{c}=(_{k}^{t}_{c}-_{k}^{t}),_{k}^{t}, _{i}^{c}=(_{k}^{t}(x_{i}^{c})- _{k}^{t}_{c}),\] (12)

We where \(_{c}\) denotes the average over samples \(x_{i}^{c}\) from class \(c\), and \(_{k}^{}(x_{i}^{c})\) is the \(k\)th component of \(f^{}(x_{i}^{c})-y_{i}^{c}\). Combining (10), (11) and (12), we have the following convergence rates:

\[_{k}^{t+1} =(1-_{})_{k}^{t},\] (13) \[_{k}^{t+1}_{c}-_{k}^{t+ 1} =(1-_{})(_{k}^{t}_{c}- _{k}^{t}),\] (14) \[_{k}^{t+1}(x_{i}^{c})-_{k}^{t+1} _{c} =(1-_{})(_{k}^{t}(x_{i}^{c})- _{k}^{t}_{c}).\] (15)

Overall, this means that the global mean \(\) of the residual converges first, then the class means, and finally the residual of each sample \((x_{i}^{c})\). To simplify the notation, we define the following quantities:

\[ =f(X)-=[(x_{1}),,(x_{N})],\] (16) \[_{} =^{}=_{1},,_{C}]}_{:= _{1}}_{m}^{},\] (17) \[_{} =_{N}_{N}^{}= _{N}^{},\] (18)

 Eigenvalue 

Table 1: Eigendecomposition of the block-structured NTK.

where \(^{C N}\) is the matrix of residuals, \(_{}^{C N}\) are the residuals averaged over each class and stacked \(m\) times, and \(_{}^{C N}\) are the residuals averaged over the whole training set stacked \(N\) times. According to the previous discussion, \(_{}\) converges to zero at the fastest rate, while \(\) converges at the slowest rate. The last phase, which we call the _end of training_, is when \(_{}\) and \(_{}\) have nearly vanished and can be treated as zero for the remaining training time. We will use this notion in several remarks, as well as in the proof of Theorem 5.2.

### Gradient Flow Dynamics with Block-Structured NTK

We derive the dynamics of \(,,\) under Assumption 3.2 in Theorem 4.1. One can see that the block-structured kernel greatly simplifies the complicated dynamics of DNNs and highlights the role of each of the residual components identified in Section 4.1. We consider gradient flow, which is close to gradient descent for sufficiently small step size , to reduce the complications caused by higher order terms. The proof is given in Appendix B.1.

**Theorem 4.1**.: _Suppose Assumption 3.2 holds. Then the gradient flow dynamics of a DNN can be written as_

\[}=&-^{}[(_{d}-_{c}) +(_{c}-_{n})m_{}+_{n}N _{}]\\ }=&-^{}\\ }=&-_{}_{N}.\] (19)

We note that at the end of training, where \(_{}\) and \(_{}\) are zero, the system (19) reduces to

\[}=-(_{d}-_{c})_{}},}=-_{}}, }(,):=\| +_{N}^{}-\|_{F}^{2},\] (20)

and \(}=0\). This system differs from the unconstrained features dynamics only by a factor of \(_{d}-_{c}\) before \(\). Moreover, such a form of the loss function also appears in the literature of implicit regularization [4; 6; 11], where the authors show that \(\) converges to a low rank matrix.

## 5 NTK Alignment Drives Neural Collapse

The main goal of this work is to demonstrate how NC results from the NTK block structure. To this end, in Section 5.1 we further analyze the dynamics presented in Theorem 4.1, in Section 5.2 we derive the invariant of this training dynamics, and in Section 5.3 we finally derive NC.

### Features Decomposition

We first decompose the features dynamics presented in Theorem 4.1 into two parts: \(_{1}\), which lies in the subspace of the labels \(\), and \(_{2}\), which is orthogonal to the labels and eventually vanishes. To achieve this, note that the SVD of \(\) has the following form:

\[^{}=_{C},,\] (21)

where \(^{C(N-C)}\) is a matrix of zeros, and \(^{C C}\) and \(^{N N}\) are orthogonal matrices. Moreover, we can choose \(\) and \(\) such that \(=_{C}\) and

\[=_{1},_{2},_{1}= }_{C}_{m}^{N C },_{2}=_{C}}_{2}^{N(N-C)},\] (22)

where \(\) is the Kronecker product. Note that by orthogonality, \(}_{2}^{m(m-1)}\) has full rank and \(_{m}^{}}_{2}=\). We can now decompose \(\) into two components as follows:

\[=[_{1},_{2}],_ {1}=}_{1},_{2}=}_{2}.\] (23)

The following equations reveal the meaning of these two components:

\[_{1}= h_{1},, h_{C}, _{2}=}^{(1)} }_{2},,^{(C)}}_{2},\] (24)

where \( h_{c}^{n}\) is the mean of \(h\) over inputs \(x_{i}^{c}\) from class \(c[C]\), and \(^{(c)}^{n m}\) is the submatrix of \(\) corresponding to samples of class \(c\), i.e., \(=^{(1)},,^{(C)}\). We see that is simply the matrix of the last-layer features' class means, which is prominent in the NC literature. We also see that the columns of \(^{(c)}}_{2}\) are \(m-1\) different linear combinations of \(m\) vectors \(h(x_{i}^{c})\), \(i[m]\). Moreover, the coefficients of each of these linear combinations sum to zero by the choice of \(}_{2}\). Therefore, \(_{2}\) must reduce to zero in case of variability collapse (NC1), when all the feature vectors within the same class become equal. We prove that \(_{2}\) indeed vanishes in DNNs with block-structured NTK as part of our main result (Theorem 5.2).

### Invariant

We now use the former decomposition of the last-layer features to further simplify the dynamics and deduce a training invariant in Theorem 5.1. The proof is given in Appendix B.2.

**Theorem 5.1**.: _Suppose Assumption 3.2 holds. Define \(_{1}\) and \(_{2}\) as in (23). Then the class-means of the residuals (defined in (17)) are given by \(_{1}=_{1}+_{C}^{}- _{C}\), and the training dynamics of the DNN can be written as_

\[}_{1}&=-^{}_{1}(_{ }_{C}+_{n}m_{C}_{C}^{})\\ }_{2}&=-_{}^{}_{2}\\ }&=-m(_{1}_{1}^{}+_ {2}_{2}^{})\\ }&=-m_{1}_{C},\] (25)

_where \(_{}:=_{d}-_{c}\) and \(_{}:=_{}+m(_{c}-_{n})\) are the two smallest eigenvalues of the kernel \(_{k,k}^{h}(X)\) for any \(k[n]\). Moreover, the quantity_

\[:=^{}- }}_{1}(_{C}-_{C}_{C}^{}) _{1}^{}-}}_{2}_ {2}^{}\] (26)

_is invariant in time. Here \(:=m}{_{}+C_{n}m}\)._

We note that the invariant \(\) derived here resembles the conservation laws of _hyperbolic_ dynamics that take the form \(_{}:=a^{2}-b^{2}=const\) for time-dependent quantities \(a\) and \(b\). Such dynamics arise when gradient flow is applied to a loss function of the form \((a,b):=(ab-q)^{2}\) for some \(q\). Since the solutions of such minimization problems, given by \(ab=q\), exhibit symmetry under scaling \(a a,b b/\), the value of the invariant \(_{}\) uniquely specifies the hyperbola followed by the solution. In machine learning theory, hyperbolic dynamics arise as the gradient flow dynamics of linear DNNs , or in matrix factorization problems [3; 15]. Moreover, the end of training dynamics defined in (20) has a hyperbolic invariant given by

\[_{}:=^{}-}}^{}.\] (27)

Therefore, the final phase of training exhibits a typical behavior for the hyperbolic dynamics, which is also characteristic for the unconstrained features models [21; 38]. Namely, "scaling" \(\) and \(\) by an invertible matrix does not affect the loss value but changes the dynamic's invariant. On the other hand, minimizing the invariant \(_{}\) has the same effect as joint regularization of \(\) and \(\).

However, we also note that our invariant \(\) provides a new, more comprehensive look at the DNNs' dynamics. While unconstrained features models effectively make assumptions on the end-of-training invariant \(_{}\) to derive NC [21; 38; 48], our dynamics control the value of \(_{}\) through the more general invariant \(\). This way we connect the properties of end-of-training hyperbolic dynamics with the previous stages of training.

### Neural Collapse

We are finally ready to state and prove our main result in Theorem 5.2 about the emergence of NC in DNNs with NTK alignment. We include the proof in Appendix B.3.

**Theorem 5.2**.: _Assume that the NTK has a block structure as defined in Assumption 3.2. Then the DNN's training dynamics are given by the system of equations in (25). Assume further that the last-layer features are centralized, i.e \( h=0\), and the dynamics invariant (26) is zero, i.e., \(=\). Then the DNN's dynamics exhibit neural collapse as defined in (NC1)-(NC4)._Below we provide several important remarks and discuss the implications of this result:

**(1) Zero invariant assumption:** We assume that the invariant (26) is zero in Theorem 5.2 for simplicity and consistency with the literature. Indeed, similar assumptions arise in matrix decomposition papers, where zero invariant guarantees "balance" of the problem [3; 15]. However, our proofs in fact only require a weaker assumption that the invariant terms containing features \(\) are aligned with the weights \(^{}\), i.e.

\[^{}}}_{1} _{1}^{}-}}_{2}_{2 }^{},\] (28)

where we have taken into account our assumption on the zero global mean \( h=0\).

**(2) Necessity of the invariant assumption:** The relaxed assumption on the invariant (28) is necessary for the emergence of NC in DNNs with block-structured NTK. Indeed, NC1 implies \(_{2}=\), and NC3 implies \(_{1}_{1}^{}^{}\). Therefore, DNNs that do not satisfy this assumption do not display NC. Our numerical experiments described in Section 6 strongly support this insight (see Figure 2, panes a-e). Thus, we believe that the invariant derived in this work characterizes the difference between models that do and do not exhibit NC.

**(3) Zero global mean assumption:** We note that the zero global mean assumption \( h=0\) in Theorem 5.2 ensures that the biases are equal to \(=_{C}\) at the end of training. This assumption is common in the NC literature [21; 38] and is well-supported by our numerical experiments (see figures in Appendix C, pane i). Indeed, modern DNNs typically include certain normalization (e.g. through batch normalization layers) to improve numerical stability, and closeness of the global mean to zero is a by-product of such normalization.

**(4) General biases case:** Discarding the zero global mean assumption allows the biases \(\) to take an arbitrary form. In this general case, the following holds for the matrix of weights:

\[(^{})^{2}=}}_{C}-_{C}_{C}^{}+(1- C)(C ^{}-_{C}^{}-_{C}^{ }).\] (29)

For optimal biases \(=_{C}\), this reduces to the ETF structure that emerges in NC. Moreover, if biases are all equal, i.e. \(=_{C}\) for some \(\), the centralized class means still form an ETF (i.e., NC2 holds), and the weights exhibit a certain symmetric structure given by

\[^{}_{C}-_{C} _{C}^{},^{} _{C}-_{C}_{C}^{},\] (30)

where \(:=(1-|1- C|)<\). The proof and a discussion of this result are given in Appendix B.4. In general, the angles of these two frames are different, and thus NC3 does not hold. This insight leads us to believe that normalization is an important factor in the emergence of NC.

**(5) Partial NC:** Our proofs and the discussion suggest that all the four phenomena that form NC do not have to always coincide. In particular, our proof of NC1 only requires the block-structured NTK and the invariant to be P.S.D, which is much weaker than the total set of assumptions in Theorem 5.2. Therefore, variability collapse can occur in models that do not exhibit the ETF structure of the class-means or the duality of the weights and the class means. Moreover, as shown above, NC2 can occur when NC3 does not, i.e., the ETF structure of the class means does not imply duality.

## 6 Experiments

We conducted large-scale numerical experiments to support our theory. While we only showcase our results on a single dataset-architecture pair in the main text (see Figure 2) and refer the rest to the appendix, the following discussion covers all our experiments.

Datasets and models.Following the seminal NC paper , we use three canonical DNN architectures: VGG , ResNet  and DenseNet . Our datasets are MNIST , FashionMNIST  and CIFAR10 . We choose VGG11 for MNIST and FashionMNIST, and VGG16 for CIFAR10. We add batch normalization after every layer in the VGG architecture, set dropout to zero and choose the dimensions of the two fully-connected layers on the top of the network as \(512\) and \(256\). We use ResNet20 architecture described in the original ResNet paper , and DenseNet40 with bottleneck layers, growth \(k=12\), and zero dropout for all the datasets.

Optimization and initialization.We use SGD with Nesterov momentum \(0.9\) and weight decay \(5 10^{-4}\). Every model is trained for 400 epochs with batches of size 120. To be consistent with the theory, we balance the batches exactly. We train every model with a set of initial learning rates spaced logarithmically in the range \([10^{-4},10^{0.25}]\). The learning rate is divided by \(10\) every 120 epochs. On top of the varying learning rates, we try three different initialization settings for every model: **(a)** LeCun normal initialization (default in Flax), **(b)** uniform initialization on \([-,]\), where \(k=1/n_{-1}\) for a linear layer, and \(k=1/(Kn_{-1})\) for a convolutional layer, where \(K\) is the convolutional kernel size (default in PyTorch), **(c)** He normal initialization in fan_out mode.

Results.Our experiments confirm the validity of our assumptions and the emergence of NC as their result. Specifically, we make the following observations:

* While most of the DNNs that achieve high test performance exhibit NC, we are able to identify DNNs with comparable performance that do not exhibit NC (see Figure 2, panes f-h). We note that such models still achieve near-zero error on the training set in our setup.
* Comparing DNNs that do and do not exhibit NC, we find that our assumption on the invariant (see Theorem 5.2 and (28)) holds only for the models with NC (see Figure 2, panes a-e). This confirms our reasoning about the necessity of the invariant assumption for NC emergence.
* The kernels \(\) and \(^{h}\) are strongly aligned with the labels \(^{}\) in the models with the best performance, which is in agreement with the NTK alignment literature and justifies our assumption on the NTK block structure.

We include the full range of experiments along with the implementation details and the discussion of required computational resources in Appendix C. Specifically, we present a figure analogous to Figure 2 for every considered dataset-architecture pair. Additionally, we report the norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\), as well as the alignment of both the NTK \(\) and the last-layer features kernel \(^{h}\) in the end of training, to further justify our assumptions.

Figure 2: ResNet20 trained on MNIST with three initialization settings and varying learning rates (see Section 6 for details). We chose a model that exhibits NC (red lines, filled markers) and a model that does not exhibit NC (blue lines, empty markers) for each initialization. The vertical lines indicate the epoch when the training accuracy reaches 99.9% (over the last 10 batches). **a)** Frobenious norm of the invariant \(\|\|_{F}\). **b)** Alignment of the invariant terms as defined in (28). **c)** NC1: standard deviation of \(h(x_{c}^{c})\) averaged over classes. **d)** NC2: \(\|^{}/\|^{}\|_{F}-\|_{F}\), where \(\) is an ETF. **e)** NC3: \(\|^{}/\|\|_{F}-/\|\|_{F}\|_{F}\). The legend displays the test accuracy achieved by each model and the last-layer features kernel alignment given by \(^{h}/\|^{h}\|_{F},^{}/\| ^{}\|_{F}_{F}\). The curves in panes a-e are smoothed by Savitzkyâ€“Golay filter with polynomial degree 1 over window of size \(10\). Panes **f**, **g** and **h** show the NC metrics and the test accuracy as functions of the learning rate.

Conclusions and Broad Impact

This work establishes the connection between NTK alignment and NC, and thus provides a mechanistic explanation for the emergence of NC within realistic DNNs' training dynamics. It also contributes to the underexplored line of research connecting NC and local elasticity of DNNs' training dynamics.

The primary implication of this research is that it exposes the potential to study NC through the lens of NTK alignment. Indeed, previous works on NC focus on the top-down approach (layer-peeled models) [18; 21; 38; 48], and fundamentally cannot explain how NC develops through earlier layers of a DNN and what are the effects of depth. On the other hand, NTK alignment literature focuses on the alignment of individual layers , and recent theoretical results even quantify the role of each hidden layer in the final alignment . Therefore, we believe that the connection between NTK alignment and NC established in this work provides a conceptually new method to study NC.

Moreover, this work introduces a novel approach to facilitate theoretical analysis of DNNs' training dynamics. While most theoretical works consider the NTK in the infinite-width limit to simplify the dynamics [1; 20; 28; 49], our analysis shows that making reasonable assumptions on the empirical NTK can also lead to tractable dynamics equations and new theoretical results. Thus, we believe that the analysis of DNNs' training dynamics based on the properties of the empirical NTK is a promising approach also beyond NC research.

## 8 Limitations and Future Work

The main limitation of this work is the simplifying Assumption 3.2 on the kernel structure. While the NTK of well-trained DNNs indeed has an approximate block structure (as we discuss in detail in Section 3), the NTK values also tend to display high variance in real DNNs [22; 44]. Thus, we believe that adding stochasticity to the dynamics considered in this paper is a promising direction for the future work. Moreover, the empirical NTK exhibits so-called specialization, i.e., the kernel matrix corresponding to a certain output neurons aligns more with the labels of the corresponding class . In block-structured kernels, specialization implies different values in blocks corresponding to different classes. Thus, generalizing our theory to block-structured kernels with specialization is another promising short-term research goal. In addition, our theory relies on the assumption that the dataset (or the training batch) is balanced, i.e., all the classes have the same number of samples. Accounting for the effects of non-balanced datasets within the dynamics of DNNs with block-structured NTK is another possible future work direction.

More generally, we believe that empirical observations are essential to demistify the DNNs' training dynamics, and there are still many unknown and interesting connections between seemingly unrelated empirical phenomena. Establishing new theoretical connections between such phenomena is an important objective, since it provides a more coherent picture of the deep learning theory as a whole.

[MISSING_PAGE_FAIL:11]

International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 854-863. PMLR, 2017.
*  Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S. Kandola. On kernel-target alignment. In _Advances in Neural Information Processing Systems 14: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada_, pages 367-373. MIT Press, 2001.
*  Ahmet Demirkaya, Jiasi Chen, and Samet Oymak. Exploring the role of loss functions in multiclass classification. In _54th Annual Conference on Information Sciences and Systems, CISS 2020, Princeton, NJ, USA, March 18-20, 2020_, pages 1-5. IEEE, 2020.
*  Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 382-393, 2018.
*  Omer Elkabetz and Nadav Cohen. Continuous vs. discrete optimization of deep neural networks. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 4947-4960, 2021.
*  Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 3004-3014. PMLR, 2021.
*  C Fang, H He, Q Long, and WJ Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences of the United States of America_, 118(43), 2021.
*  Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
*  Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stephane d'Ascoli, Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with number of parameters in deep learning. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(2):023401, 2020.
*  X. Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and dynamics on the central path. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
*  Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
*  Hangfeng He and Weijie J. Su. The local elasticity of neural networks. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
*  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 770-778. IEEE Computer Society, 2016.
*  Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020.
*  Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pages 2261-2269. IEEE Computer Society, 2017.

*  Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 4542-4551. PMLR, 2020.
* A neural tangent kernel perspective. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
*  Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
*  Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 8580-8589, 2018.
*  Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep networks with margin distributions. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* ICANN 2020
- 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15-18, 2020, Proceedings, Part II_, volume 12397 of _Lecture Notes in Computer Science_, pages 168-179. Springer, 2020.
*  Vignesh Kothapalli, Ebrahim Rasromani, and Vasudev Awatramani. Neural collapse: A review on modelling principles and generalization. _arXiv preprint arXiv:2206.04041_, 2022.
*  Alex Krizhevsky et al. Learning multiple layers of features from tiny images, 2009.
*  Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
*  Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. _Advances in Neural Information Processing Systems_, 33:15156-15172, 2020.
*  Yizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep neural networks. In _International Conference on Machine Learning_, pages 14248-14282. PMLR, 2022.
*  Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. _arXiv preprint arXiv:2011.11619_, 2020.
*  Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020.
*  Federico Pernici, Matteo Bruni, Claudio Baecchi, and Alberto Del Bimbo. Fix your features: Stationary and maximally discriminative embeddings using regular polytope (fixed classifier) networks. _arXiv preprint arXiv:1902.10441_, 2019.
*  Tomaso A. Poggio and Qianli Liao. Explicit regularization and implicit bias in deep network classifiers trained with the square loss. _arXiv preprint arXiv:2101.00072_, 2021.
*  Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
*  Mariia Seleznova and Gitta Kutyniok. Analyzing finite neural networks: Can we trust neural tangent kernel theory? In _Mathematical and Scientific Machine Learning, 16-19 August 2021, Virtual Conference /Lausanne, Switzerland_, volume 145 of _Proceedings of Machine Learning Research_, pages 868-895. PMLR, 2021.
*  Mariia Seleznova and Gitta Kutyniok. Neural tangent kernel beyond the infinite-width limit: Effects of depth and initialization. In _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 19522-19560. PMLR, 2022.

*  Haozhe Shan and Blake Bordelon. A theory of neural tangent kernel alignment and its influence on training. _arXiv preprint arXiv:2105.14301_, 2021.
*  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
*  Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei. Circle loss: A unified perspective of pair similarity optimization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6398-6407, 2020.
*  Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 21478-21505, 2022.
*  Tom Tirer, Joan Bruna, and Raja Giryes. Kernel-based smoothness analysis of residual networks. In _Mathematical and Scientific Machine Learning_, volume 145, pages 921-954, 2021.
*  Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed. Perturbation analysis of neural collapse. _arXiv preprint arXiv:2210.16658_, 2022.
*  Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
*  Greg Yang. Tensor programs II: neural tangent kernel for any architecture. _arXiv preprint arXiv:2006.14548_, 2020.
*  Jiayao Zhang, Hua Wang, and Weijie J. Su. Imitating deep learning dynamics via locally elastic stochastic differential equations. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 6392-6403, 2021.

Related works

NC with MSE loss.NC was first introduced for DNNs with cross-entropy (CE) loss, which is commonly used in classification problems . Since then, numerous papers discussed NC with MSE loss, which provides more opportunities for theoretical analysis, especially after the MSE loss was shown to perform on par with CE loss for classification tasks [14; 29].

Most previous works on MSE-NC adopt the so-called unconstrained features model [21; 38; 48]. In this model, the last-layer features \(\) are free variables that are directly optimized during training, i.e., the features do not depend on the input data or the DNN's trainable parameters. Fang _et al._ also introduced a generalization of this approach called \(N\)-layer-peeled model, where features of the \(N\)-th-to-last layer are free variables, and studied the 1-layer-peeled model (equivalent to the unconstrained features model) with CE loss as a special case.

One line of research on MSE-NC in unconstrained/layer-peeled models aims to derive global minimizers of optimization problems associated with DNNs [17; 18; 48]. In particular, Tirer _et al._ showed that global minimizers of the MSE loss with regularization of both \(\) and \(\) exhibit NC. Moreover, Ergen & Pilanci  showed that NC emerges in global minimizers of optimization problems with general convex loss in the context of the 2-layer-peeled model. In comparison to our work, these contributions do not consider the training dynamics of DNNs, i.e., they do not discuss whether and how the model converges to the optimal solution.

Another line of research on MSE-NC explicitly considers the dynamics of the unconstrained features models [21; 38]. In particular, Han _et al._ considered the gradient flow of the unconstrained renormalized features along the "central path", where the classifier is assumed to take the form of the optimal least squares (OLS) solution for given features \(\). Under this assumption, they derive a closed-form dynamics that implies NC. While they empirically show that DNNs are close to the central path in certain scenarios, they do not provide a theoretical justification for this assumption. The dynamics considered in their work is also distinct from the standard gradient flow dynamics of DNNs considered in our work. On the other hand, an earlier work by Mixon _et al._ considered the gradient flow dynamics of the unconstrained features model, which is equivalent (up to rescaling) to the end-of-training dynamics (20) that we discuss in Sections 4.2 and 5.2. Their work relies on the linearization of these dynamics to derive a certain subspace, which appears to be an invariant subspace of the non-linearized unconstrained features model dynamics. Then they show that minimizers of the loss from this subspace exhibit NC. We note that, in terms of our paper, assuming that the unconstrained features model dynamics follow a certain invariant subspace means making assumptions on the end-of-training invariant (27). In comparison to these works, we make a step towards realistic DNNs dynamics by considering the standard gradient flow of DNNs simplified by Assumption 3.2 on the NTK structure, which is supported by the extensive research on NTK alignment [7; 9; 44; 45]. In our setting, the NTK captures the dependence of the features on the training data, which is missing in the unconstrained features model. Moreover, while other works focus only on the dynamics that converge to NC, we show that DNNs with MSE loss may not exhibit NC in certain settings, and the invariant of the dynamics (26) characterizes the difference between models that do and do not converge to NC.

Notably, works by Poggio & Liao  adopt a model different from the unconstrained features model to analyze gradient flow of DNNs. They consider the dynamics of homogeneous DNNs, in particular ReLU networks without biases, with normalization of the weights matrices and weights regularization. The goal of weights normalization in their model is to imitate the effects of batch normalization in DNNs training. In this model, certain fixed points of the gradient flow exhibit NC. While the approach taken in their work captures the dependence of the features on the data and the DNN's parameters, it fundamentally relies on the homogeneity of the DNN's output function. However, most DNNs that exhibit NC in practice are not homogeneous due to biases and skip-connections.

NC and local elasticity.A recent extensive survey of NC literature  discussed local elasticity as a possible mechanism behind the emergence of NC, which has not been sufficiently explored up until now. One of the few works in this research direction is by Zhang _et al._, who analyzed the so-called locally-elastic stochastic differential equations (SDEs) and showed the emergence of NC in their solutions. They model local elasticity of the dynamics through an effect matrix, which has only two distinct values: a larger intra-class value and a smaller inter-class value. These values characterize how much influence samples from one class have on samples from other classes in the SDEs. While the aim of their work is to imitate DNNs' training dynamics through SDEs, the authorsdo not provide any explicit connection between their dynamics and real gradient flow dynamics of DNNs. On the other hand, we derive our dynamics directly from the gradient flow equations and connect local elasticity to the NTK, which is a well-studied object in the deep learning theory.

Another work by Tirer _et al._ provided a perturbation analysis of NC to study "inexact collapse". They considered a minimization problem with MSE loss, regularization of \(\) and \(\), and additional regularization of the distance between \(\) and a given matrix of initial features. In the "near-collapse" setting, i.e., when the initial features are already close to collapse, they showed that the optimal features can be obtained from the initial features by a certain linear transformation with a block structure, where the intra-class effects are stronger than the inter-class ones. While this transformation matrix resembles the block-structured effect matrices in locally-elastic training dynamics, it does not originate from the gradient flow dynamics of DNNs and is not related to the NTK.

## Appendix B Proofs

### Proof of Theorem 4.1

Proof of Theorem 4.1.: We will first derive the dynamics of \(h_{s}(x_{i}^{c})\), which is the \(s\)-th component of the last-layer features vector on sample \(x_{i}^{c} X\) from class \(c[C]\). Let \(^{P}\) be the trainable parameters of the network stretched into a single vector. Then its gradient flow dynamics is given by

\[}=-_{}(f)=-_{k=1}^{C}_{i^{ }=1}^{N}(f(X)_{ki^{}}-_{ki^{}})_{ }f(X)_{ki^{}},\] (31)

where \(_{}f(X)_{ki^{}}^{P}\) is the component of the DNN's Jacobian corresponding to output neuron \(k\) and the input sample \(x_{i^{}}^{c^{}}\). Since entries of \(f(X)\) can be written as

\[f(X)_{ki^{}}=_{s^{}=1}^{n}_{ks^{}}_ {s^{}i^{}}+_{k}=_{s^{}=1}^{n}_{ks^{ }}h_{s^{}}(x_{i^{}}^{c^{}})+_{k},\] (32)

we obtain

\[}=-_{k=1}^{C}_{i^{}=1}^{N}_{s^{}=1}^{n} (f(X)_{ki^{}}-_{ki^{}})_{}(_{ ks^{}}h_{s^{}}(x_{i^{}}^{c^{}})+_{k}).\] (33)

By chain rule, we have \(_{s}(x_{i}^{c})=_{}h_{s}(x_{i}^{c}),}\). Then, taking into account that

\[_{}h_{s}(x_{i}^{c}),_{}(_{ks ^{}}h_{s^{}}(x_{i^{}}^{c^{}})+_{k})= _{ks^{}}_{}h_{s}(x_{i}^{c}),_{ }h_{s^{}}(x_{i^{}}^{c^{}}),\] (34)

and that \(_{}h_{s}(x_{i}^{c}),_{}h_{s^{}}(x _{i^{}}^{c^{}})=_{s,s^{}}^{h}(x_{i}^{c},x_{i^{ }}^{c^{}})\) by definition of \(^{h}\), we have

\[_{s}(x_{i}^{c})=-_{k=1}^{C}_{i^{}=1}^{N}_{s^{}= 1}^{n}(f(X)_{ki^{}}-_{ki^{}})_{ks^{}} _{s,s^{}}^{h}(x_{i}^{c},x_{i^{}}^{c^{}}).\] (35)

Now by Assumption 3.2 we have \(_{s,s^{}}^{h}=0\) if \(s s^{}\). Therefore, the above expression simplifies to

\[_{s}(x_{i}^{c}) =-_{i^{}=1}^{N}_{s,s}^{h}(x_{i}^{c},x_{i^{}} ^{c^{}})_{k=1}^{C}(f(X)_{ki^{}}-_{ki^{}}) _{ks}\] \[=-_{i^{}=1}^{N}[^{}(+ _{N}^{}-)]_{si^{}}_{s,s}^{h}(x_{ i}^{c},x_{i^{}}^{c^{}}).\]

To express \(}=[_{s}(x_{i}^{c})]_{s,i}^{n N}\) in matrix form, it remains to express \(_{s,s}^{h}(x_{i}^{c},x_{i^{}}^{c^{}})\) as the \((i^{},i)\)-th entry of some matrix. We will separate the sum into three cases: 1) \(i=i^{}\), 2) \(i i^{}\) and \(c=c^{}\), and 3) \(c c^{}\). According to Assumption 3.2, the first case corresponds to the multiple of identity \(_{d}_{N}\). The second corresponds to the block matrix of size \(m\) with zeros on the diagonal,which can be written as \(_{c}(^{}-_{N})\). The third matrix equals to \(_{n}(_{N}_{N}^{}-^{})\). Therefore we can express the dynamics of \(\) as follows:

\[}= -[^{}(+_{N} ^{}-)][_{d}+_{c}(^{}-)+_{n}(_{N}_{N}^{}-^{} )]\] \[= -(_{d}-_{c})^{}(+ _{N}^{}-)\] \[-(_{c}-_{n})^{}( ^{}+m_{N}^{}-m)\] \[-_{n}^{}( _{N}_{N}^{}+N_{N}^{}- _{C}_{N}^{}).\]

Now we notice that \(^{}/m\) is the matrix of stacked class means repeated \(m\) times each and \(_{N}_{N}^{}/N\) is a matrix of the global mean repeated \(N\) times. Therefore, we have

\[^{}+m_{N}^{ }-m=m_{},\]

according to the definitions of global and class-mean residuals in (18) and (17).

The expressions for the gradient flow dynamics of \(\) and \(\) follow directly from the derivatives of \(f(X)\) w.r.t. \(\) and \(\). This completes the proof. 

### Proof of Theorem 5.1

Proof of Theorem 5.1.: Recall from (23) in Section 5.1 that we have the following decomposition

\[=[_{1},_{2}],_ {1}=}_{1},_{2}=}_{2}\]

with orthogonal \(=[_{1},_{2}]^{N N}\). We now artificially add \(^{}(=_{N})\) to the dynamics (19) in Theorem 4.1 and obtain

\[}=&-(_{d}-_{c})^{ }(+_{N}^{}-Y)\\ &-(_{c}-_{n})m^{}( ^{}^{}+ _{N}^{}-Y)\\ &-_{n}N^{}( ^{}_{N}_{N}^{}+ _{N}^{}-_{C}_{N}^{ })\\ }=&-(+_{N}^{ }-Y)^{}^{}\\ }=&-(+_{N}^{ }-Y)^{}_{N}.\] (36)

Let us simplify the expression. Since \(_{1}=}_{C}_{m}\) and \(_{2}=_{C}}_{2}\), we have

\[_{N}^{}=[_{C}^{},], =[_{C},].\] (37)

Plugging (37) into (36), we see the dynamics can be decomposed into

\[}_{1}=&-(_{d}-_{c})^{}( _{1}+_{C}^{}-_{C})\\ &-(_{c}-_{n})m^{}(_{1}+_{C}^{}-_{C})\\ &-_{n}N^{}(_{1}_{C} _{C}^{}+_{C}^{}-_{ C}_{C}^{})\\ }_{2}=&-(_{d}-_{c})^{}_{2}\\ }=&-m(_{1}+_{C}^{}- _{C})_{1}^{}-m_{2}_{2}^{ }\\ }=&-m(_{1}+_{C}^{}- _{C})_{C}.\] (38)

To further simplify (38), we define the following quantities

\[_{}:=_{d}-_{c},_{}:=_{}+m(_{c}-_{n}),_{1}:=_{1}+ _{C}^{}-_{C}.\] (39)

Notice that \(_{}\) and \(_{}\) are the two largest eigenvalues of the block-structured kernel \(_{s,s}^{h}(X)\) (see Table 1 for the eigindecomposition of a block-structured matrix), and \(_{1}\) is a matrix of the stacked class-mean residuals, which is also defined in (17). The the dynamics (38) simplifies to

\[}_{1}=&-^{}(_{}_ {1}+_{n}N(_{1}_{C}_{C}^{ }+_{C}^{}-_{C}_{C}^{ }))\\ }_{2}=&-_{}^{}_{2} \\ }=&-m(_{1}_{1}^{}-_{2} _{2}^{})\\ }=&-m_{1}_{C}.\] (40)It remains to simplify the expression for \(}_{1}\). By using the relation

\[_{1}_{C}_{C}^{}+_{C}^{}-_{C}_{C}^{}= {C}_{1}_{C}_{C}^{},\] (41)

we can deduce that the dynamics for \(}_{1}\) in (40) can be expressed as (recalling that \(N=mC\))

\[}_{1}=-^{}_{1}(_{} +_{n}m_{C}_{C}^{}).\] (42)

We notice that \((_{C}+m}{_{}}_{C}_{C}^{})^{-1}=_{C}-_{C}_{C}^{}\), where \(:=m}{_{}+C_{n}m}\). Then we can derive the invariant of the training dynamics by direct computation of the time-derivative \(}\), where

\[:=^{}-}}_{1}(_{C}-_{C}_{C}^{}) _{1}^{}-}}_{2}_ {2}^{}\] (43)

Since \(}=\), we get that the quantity \(\) remains constant in time. This completes the proof.

### Proof of Theorem 5.2

We divide the proof into two main parts: the first one shows the emergence of NC1, and the second one shows NC2-4.

(Nci)Following the analysis in Section 3, the dynamics eventually enters the end of training phase (see Section 4.1). Then the dynamics in Theorem 5.1 simplifies to the following form:

\[}_{1}=\\ }_{2}=-_{}^{} _{2}\\ }=-m_{2}_{2}^{}\\ }=\] (44)

As we note in Section 4, this dynamics is similar to the gradient flow of the unconstrained features models and is an instance of the class of hyperbolic dynamics, which is discussed in Section 5.2. During this phase the quantity

\[}:=_{}^{}-m_{2}_{2}^{}=m_{}(+}}_{1}(-_{C}_{C}^{ })_{1}^{})\] (45)

does not change in time. Hence we can decouple the dynamic using the invariant as follows:

\[}_{2}=-_{}(}+m _{2}_{2}^{})_{2}\\ }=-(_{}^{}- })\] (46)

Since \(\) is p.s.d (or zero, as a special case), \(}\) is p.s.d as well, and the eigendecomposition of the invariant is given by \(}=_{k}c_{l}v_{k}v_{k}^{}\) for some coefficients \(c_{k} 0\) and a set of orthonormal vectors \(v_{k}^{n}\). Then we also have \(_{2}_{2}^{}=_{k,l}_{kl}v_{k}v_{l}^{}\), where \(_{kl}\) are symmetric (i.e. \(_{kl}=_{lk}\)) and \(_{kk} 0\) for all \(k=1, n\) (since \(_{2}_{2}^{}\) is symmetric and p.s.d.). Note that coefficients \(c_{k}\) here are constant while coefficients \(_{kl}\) are time-dependent. Let us then write the dynamics for \(_{kl}\) using the dynamics of \(_{2}_{2}^{}\):

\[(_{2}_{2}^{})=-}_{2}_{2}^{}-_{2}_{2}^{}}-2( _{2}_{2}^{})^{2}\] (47)

Then for the elements of \(\) we have:

\[_{kl}=-_{kl}(c_{k}+c_{l})-2_{j}_{kj}_{jl}\] (48)

For the diagonal elements \(_{kk}\), this gives:

\[_{kk}=-2c_{k}_{kk}-2_{j}_{kj}^{2}\] (49)Since \(c_{k} 0\), \(_{kk} 0\) and \(_{kj}^{2} 0\), we get that

\[_{kk}[t]{}0 k\] (50)

And, therefore, all the non-diagonal elements also tend to zero. Thus, we get that

\[_{2}_{2}^{}[t]{}\] (51)

and thus

\[_{2}[t]{}\] (52)

Now we notice that from the expression for \(_{2}\) in (24) it follows that \(_{2}=\) implies variability collapse, since it means that all the feature vectors within the same class are equal. Indeed, \(^{(c)}}_{2}=^{n(m-1)}\) means that there is a set of \(m-1\) orthogonal vectors, which are all also orthogonal to \([h_{i}(x_{1}^{c}),,h_{i}(x_{m}^{c})]\) for any \(i=1,,n\), where \(x_{i}^{c}\) are inputs from class \(c\). However, there is only one vector (up to a constant) orthogonal to all the columns of \(}_{2}\) in \(^{m}\) and this vector is \(_{m}\). Therefore, \([h_{i}(x_{1}^{c}), h_{i}(x_{m}^{c})]=_{m}\) for some constant \(\) for any \(i=1,,n\). Thus, we indeed have \(h(x_{1}^{c})==h(x_{m}^{c})\), which constitutes variability collapse within classes. 

_(NC2-4)_. Set \(=\). We first show that zero global feature mean implies \(=_{C}\). At the end of training, since \(_{1}=\), we have

\[_{1}+_{C}^{}=_{C}\] (53)

On the other hand, zero global mean implies \(_{1}_{C}=C h=\). Then multiplying (53) by \(_{C}\) on the right, we get the desired expression for the biases. Given the zero global mean, we have

\[^{}-}}_{1}_{1}^{}-}}_{2}_{2}^{}=-}{_{}}  h h^{}=\] (54)

By the proof of NC1, \(_{2}\). Together with the assumption that \(\) is proportional to the limit of \(^{}\) (or zero, as a special case), we obtain

\[_{}^{}-m_{1}_{1}^{ }^{}\] (55)

for some \( 0\). Note that since \(_{1}_{1}^{}\) is p.s.d. this implies \(_{c}:=_{}- 0\). By multiplying the left and right with appropriate factors, we have

\[_{1}^{}(_{c}^{} -m_{1}_{1}^{})_{1}\\ (_{c}^{}-m_{1} _{1}^{})^{}.\] (56)

Consequently (according to (53))

\[_{c}(_{C}-_{C}_{C}^{})^{2}-m(_{1}^{}_{1})^{2}\\ _{c}(^{})^{2}-(_{C}- _{C}_{C}^{})^{2}\] (57)

Since both \(^{}\) and \(_{1}^{}_{1}\) are p.s.d., we have

\[_{1}^{}_{1}}{m }}(_{C}-_{C}_{C}^{})\\ ^{}}}(_{C}- _{C}_{C}^{}).\] (58)

To establish NC2, recall that \(_{1}=[ h_{1},, h_{C}]\) and that \(\), as a normalized version of \(_{1}\), satisfies

\[^{}(_{C}-_{C}_{C}^{})=(_{C}-_{C}_{C}^{}).\]

To establish NC3, note that from (55) and (58) together, it follows that the limits of \(\) and \(^{}\) only differ by a constant multiplier.

To establish NC4, note that using NC3 we can write

\[*{argmax}_{c}(h(x)+)_{c} =*{argmax}_{c}(h(x))_{c} (=_{C})\] \[*{argmax}_{c}(^{}h(x))_{c} ()\] \[=*{argmin}_{c}\|h(x)- h_{c}\|_{2}.\]

This completes the proof.

### General biases case

Proof.: As in the proof of Theorem 5.2, at the end of training we have \(_{1}+_{C}^{}=_{C}\). Moreover, since \(=\) and \(_{2}\), we have

\[^{}-}}_{ 1}(-_{C}_{C}^{})_{1}^{} .\] (59)

Multyplying the above expression to the left by \(\) and to the right by \(^{}\), we obtain the general expression (29) for the matrix \((^{})^{2}\) mentioned in the main text:

\[(^{})^{2}}} _{C}-_{C}_{C}^{}+(1- C)(C ^{}-_{C}^{}-_{C} ^{}).\] (60)

This expression implies that the rows of the weights matrix may have varying separation angles in the general biases case, i.e., there is no symmetric structure is general. However, for constant biases \(=_{C}\), the above expression simplifies to

\[(^{})^{2}}} _{C}-1-(1- C)(1- C)^{2}_{C}_{C}^{}.\] (61)

Since \(<1/C\) and \((1- C)^{2} 0\), we have that \((1-(1- C)(1- C)^{2})/C 1/C\). Therefore, the RHS of (61) is always p.s.d. and has a unique p.s.d square root proportional to \(_{C}-_{C}_{C}^{}\) for some constant \(<1/C\). Denote \(:=(1-(1- C)(1- C)^{2})/C\), then we have \(=(1-)/C\). Note that \(<1/C\) ensures that \(\) is well defined. Then the configuration of the final weights is given by

\[^{}}}} _{C}-_{C}_{C}^{}.\] (62)

This means that the norms of all the weights rows are still equal, as in NC2. However, since \(<1/C\) if \( 1/C\), the angle between these rows is smaller than in the ETF structure.

We can derive the configuration of the class means similarly by multtyplying (59) to the left by \(_{1}^{}\) and to the right by \(_{1}\). In the general biases case, we get

\[_{1}^{}_{1}(_{C}-_{C} _{C}^{})_{1}^{}_{1}}}{m}_{C}-_{C}^{}-_ {C}^{}+\|\|_{2}^{2}_{C}_{C}^{ }.\] (63)

As with the weights, we see that this is not a symmetric structure in general. Thus, NC2 does not hold in the general biases case. However, for the constant biases \(=_{C}\), the above expression simplifies to

\[_{1}^{}_{1}(_{C}-_{C} _{C}^{})_{1}^{}_{1}}}{m}(_{C}-_{C}_{C}^{})^{2}.\] (64)

Analogously to the previous derivations, we get that the unique p.s.d. square root of the RHS is given by \(_{C}-_{C}_{C}^{}\), where \(:=(1-|1- C|)/C<1/C\) for \( 1/C\). On the other hand, the unique p.s.d root of \(-_{C}_{C}^{}\) is given by \(_{C}-_{C}_{C}^{}\), where \(:=(1-)/C\). Thus, we have the following

\[}}}_{1}^{}_{1}( _{C}-_{C}_{C}^{})_{C}- _{C}_{C}^{}.\] (65)

Therefore, the structure of the last-layer features class means is given by

\[_{1}^{}_{1}}}{m}} _{C}-_{C}_{C}^{} _{C}-_{C}_{C}_{C}^{ }=}}{m}}_{C}- _{C}_{C}^{},\] (66)

where \(:=+/(1+ C)-C/(1+ C)<1/C\) for \( 1/C\). Thus, similarly to the classifier weights \(\), the last-layer features class means form a symmetric structure with equal lengths and a separation angle smaller than in the ETF. However, the centralized class means given by \(=_{1}(_{C}-_{C}_{C}^{}/C)\) still form the ETF structure:

\[^{}}}{m}} _{C}-_{C}_{C}^{}.\] (67)

This holds since the component proportional to \(_{C}_{C}^{}\) on the RHS of equation (66) lies in the kernel of the ETF matrix \((_{C}-_{C}_{C}^{}/C)\). Thus, we conclude that NC2 holds in case of equal biases, while NC3 does not.

[MISSING_PAGE_FAIL:21]

gradient with respect to numerous parameters of a DNN. Additionally, the graphs in panes f-h of the same figures take around 1.5 hours for each learning rate value for ResNet20, 3 hours for DenseNet40, and 4 hours for VGG11 and VGG16, which adds up to approximately 1350 computational hours.

ResultsWe include experiments on the following architecture-dataset pairs:

* Figure 3: VGG11 trained on MNIST
* Figure 4: VGG11 trained on FashionMNIST
* Figure 5: VGG16 trained on CIFAR10
* Figure 6: ResNet20 trained on MNIST
* Figure 7: ResNet20 trained on FashionMNIST
* Figure 8: ResNet20 trained on CIFAR10
* Figure 9: DenseNet40 trained on MNIST
* Figure 10: DenseNet40 trained on FashionMNIST
* Figure 11: DenseNet40 trained on CIFAR10

The experiments setup is described in Section 6. Panes a-h of Figures 3, 4, 5, 6, 7, 8, 9, 10, 11 are analogous to the same panes of Figure 2. We include additional pane i here, which displays the norms of the invariant terms corresponding to the feature matrix components \(_{1}\) and \(_{2}\), and the global features mean \( h\) at the end of training. One can see that the global features mean is relatively small in comparison with the class-means in every setup, and the "variance" term \(_{2}\) is small for models that exhibit NC. We also add pane j, which displays the alignment of kernels \(\) and \(^{h}\) for every model at the end of training. One can see that the kernel alignments is typically stronger in models that exhibit NC.

### Additional examples of the NTK block structure

We include the following additional illustrative figures (analogous to Figure 1 in the main text) that show the NTK block structure in dataset-architecture pairs covered in our experiments:

* Figure 13: VGG11 trained on MNIST
* Figure 14: VGG11 trained on FashionMNIST
* Figure 15: VGG16 trained on CIFAR10
* Figure 16: ResNet20 trained on FashionMNIST
* Figure 17: ResNet20 trained on CIFAR10
* Figure 18: DenseNet40 trained on MNIST
* Figure 19: DenseNet40 trained on FashionMNIST
* Figure 11: DenseNet40 trained on CIFAR10

Overall, the block structure pattern is visible in the traced kernels in all the figures. As expected, the block structure is more pronounced in the kernels where the final alignment values are higher. While the norms of the "non-diagonal" components of the kernels are generally smaller than the "diagonal" components in panes c) and d), we notice that there is a large variability in the norms of the "diagonal" components in some settings. This means that different neurons of the penultimate layer and different classification heads may contribute to the kernel unequally in some settings. Moreover, certain "non-diagonal" components of the last-layer kernel may have non-negligible effect in some settings. We discuss how one could generalize our analysis to account for these properties of the NTK in Appendix D.

### Preliminary experiments with CE loss

While CE loss is a common choice for training DNN classifiers, our theoretical analysis and the experimental results only cover DNNs trained with MSE loss. For completeness, we provide experimental results for ResNet20 trained on MNIST with CE loss in Figure 12. One can see that smaller invariant norm and higher invariant alignment correlate with NC in the figure. However, DNNs trained with CE loss overall reach better NC metrics but have much larger norm of the invariant in comparison with DNNs trained with MSE loss.

## Appendix D Relaxation of the NTK Block-Structure Assumption

In this section, we first derive the dynamics equations of DNNs with a general block structure assumption on the last-layer kernel \(^{h}\) (analogous to the equations presented in Theorem 4.1 and Theorem 5.1). Then we discuss a possible relaxation of Assumption 3.2, under which our main result regarding NC in Theorem 5.2 still holds.

### Dynamics under General Block Structure Assumption

We first formulate the most general form of the block structure assumption on \(^{h}\) as follows:

**Assumption D.1**.: _Assume that \(^{h}:^{n n}\) has the following block structure_

\[^{h}(x,x)=_{d}+_{c}+_{n},^{h}( x_{i}^{c},x_{j}^{c})=_{c}+_{n},^{h}(x_{i}^{c},x_{j}^ {c^{}})=_{n},\] (71)

_where \(_{d,c,n}^{n n}\) are arbitrary p.s.d. matrices. Here \(x_{i}^{c}\) and \(x_{j}^{c}\) are two distinct inputs from the same class, and \(x_{j}^{c^{}}\) is an input from class \(c^{} c\)._

This assumption means that every kernel matrix \(^{h}_{k,s}(X),k,s[1,n]\) still has at most three distinct values, corresponding to the inter-class, intra-class, and the diagonal values of the kernel. However, these values are arbitrary and may depend on the choice of \(k,s[1,n]\).

Under the general block structure assumption, the gradient flow dynamics of DNNs with MSE loss takes the following form:

\[}=&-_{d}^{}+m _{c}^{}_{}+N_{n}^{}_{}\\ }=&-^{}\\ }=&-_{}_{N}.\] (72)

This is the generalized version of the dynamics presented in Theorem 4.1. Consequently, the decomposed dynamics presented in Theorem 5.1 takes the following form under the general block structure assumption:

\[}_{1}&=-(_{d}+m_{c})^{}_{1}-m_{n}^{}_{1}_{C}_{C}^{}\\ }_{2}&=-_{d}^{}_{2} \\ }&=-m(_{1}_{1}^{}+_{ 2}_{2}^{})\\ }&=-m_{1}_{C}.\] (73)

The derivation of the above dynamics equations are identical to the proofs of Theorem 4.1 and Theorem 5.1 presented in Appendix B.

Rotation invarianceWe notice that the dynamics of \((,)\) in (72) has to be rotation invariant, i.e., the equations should not be affected by a change of variables \(\), \(^{}\) for any orthogonal matrix \(\). This holds since the loss function only depends on the product \(\), which does not change under rotation. This requirement puts conditions on the behavior of \(_{d,c,n}\) under rotation. Indeed, assume that the rotation \(\), \(^{}\) for some \(\) corresponds to the following change of the kernel:

\[_{d,c,n}}_{d,c,n}(),\] (74)

then the rotation invariance of the dynamics implies the following equality for any \(\):

\[}_{d}()^{} ^{}+m}_{c}() ^{}^{}_{}+N }_{n}()^{}^{}_{}\] (75) \[=_{d}^{}+m_{c}^{}_{}+N_{n}^{}_{}.\] (76)

These equations are satisfied trivially with our initial assumption, where \(_{d,c,n}=}_{d,c,n}()_{n}\). However, as we can see, any generalized assumption should specify the behavior of the kernel under rotation, and satisfy the above equation.

For general \(_{d,c,n}\), the following behavior under rotation trivially satisfies the above condition: \(}_{d,c,n}()=^{}_{d,c,n} \). This behaviour of the kernel under rotation is intuitive, since it implies that the gradients of the last-layer features \(h\) are rotated in the same way as the features. However, we note that gradients of parametrized functions do not in general behave this way, since the rotation of the function has to be realized by a certain change of parameters. Consider, for instance, a one-hidden-layer linear network with weights \(\) in the first layer. Then we have \(=X\), and a rotation \(^{}\) corresponds to the change of parameters \(^{}\). In this case, the kernel does not change under rotation, i.e., \(}_{d,c,n}()=_{d,c,n}\).

Dynamics invariantWe note that the dynamics in 73 does not in general have an invariant analogous to the one we identified in Theorem 5.1. Indeed, if we define a quantity \(:=^{}-c_{1}_{1}_{1}^{ }-c_{2}_{2}_{2}^{}\) for some constants \(c_{1,2}\), and additionally assume centered global means \(_{1}_{C}=0\), we get the following expression for the derivative of \(\):

\[} =c_{1}(_{d}+m_{c})-m_{n} ^{}_{1}_{1}^{}-_{1} _{1}^{}c_{1}(_{d}+m_{c})^{ }-m_{n}\] (77) \[+c_{2}_{d}-m_{n}^{ }_{2}_{2}^{}-_{2}_{ 2}^{}^{}c_{2}_{d}^{}-m _{n},\] (78)

which is not equal to zero with arbitrary matrices \(_{d,c}\).

### Neural Collapse under Relaxed Block Structure Assumption

We now propose a relaxation of our main assumption, under which our main result regarding NC in Theorem 5.2 still holds. In terms of Assumption D.1 on the general block structure of \(^{h}\), our initial Assumption 3.2 in the main text is the special case with \(_{n}=_{n}_{n}\), \(_{c}=(_{c}-_{n})_{n}\), \(_{d}=(_{d}-_{c})_{n}\). The relaxed assumption can be formulated as follows in terms of matrices \(_{d,c,n}\):

**Assumption D.2**.: _Assume that \(_{n}\) is an arbitrary p.s.d. matrix and \((_{c},_{d})\) satisfy the following conditions:_

\[_{c}=_{c}_{n}+_{c},_{d}=_ {d}_{n}+_{d},\] (79)

_where \(_{c,d}^{}(^{})\), i.e., \(_{c,d}^{}=\). Further, assume that the kernel changes under rotation with an orthogonal matrix \(\) as follows:_

\[}_{d,c,n}()=^{}_{d,c,n} .\] (80)

Since \(_{n}\) is arbitrary, this relaxation allows arbitrary non-zero values of non-diagonal kernels \(^{h}_{k,s}\) with \(k s\). The following observations justify the consistency of the above assumption:

* Since \(^{C n}\), \(^{C N}\) and \(N>n>C\), \(^{}\) has a non-empty kernel (possibly time-dependent).
* The dynamics is rotation invariant under the assumption, i.e., the equation (75) holds.
* The expression of the assumption is rotation invariant, in a sense that \(}_{d,c}()=_{d,c}_{n}+}_{d,c}()\), where \(}_{d,c}^{}(^{})\) for any orthogonal \(\).

Under the above assumption, the derivative in 77 becomes zero, so the dynamics has an invariant of the form \(:=^{}-c_{1}_{1}_{1}^{ }-c_{2}_{2}_{2}^{}\). Moreover, the statement and the proof of our main Theorem 5.2 remains unchanged. Thus, DNNs satisfying the conditions of Theorem 5.2 display NC under Assumption D.2.

### Discussion

The analysis of the DNNs dynamics is simplified significantly by assuming that \(^{h}\) has a block structure. However, formulating a reasonable and consistent assumption on the NTK and its components is non-trivial. The Assumption 3.2 that we used in the main text is justified by the empirical results but may not capture all the relevant properties of the NTK. We believe that studying DNNs' dynamics under a more general or a more reasonable assumption on the NTK is a promising future work direction. The relaxed block structure assumption proposed in this section is the first step into this direction.

Figure 4: VGG11 trained on FashionMNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 3: VGG11 trained on MNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 5: VGG16 trained on CIFAR10. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{},_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 6: ResNet20 trained on MNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{},_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 8: ResNet20 trained on CIFAR10. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 7: ResNet20 trained on FashionMNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 10: DenseNet40 trained on FashionMNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 9: DenseNet40 trained on MNIST. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 11: DenseNet40 trained on CIFAR10. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 12: ResNet20 trained on MNIST with _CE loss_. See Figure 2 for the description of panes a-h. **i)** Norms of matrices \(_{1}_{1}^{}\), \(_{2}_{2}^{}\), and \( h h^{}\) at the end of training. **j)** Alignment of kernels \(\) and \(^{h}\) at the end of training. The color in panes i-j is the color of the same model in panes a-e.

Figure 16: NTK block structure of ResNet20 trained on FashionMNIST. LeCun normal initialization, initial learning rate \(0.094\). The kernel is computed on a random data subset with 12 samples from each class. See Figure 1 for the description of panes.

Figure 14: NTK block structure of VGG11 trained on FashionMNIST. LeCun normal initialization, initial learning rate \(0.049\). The kernel is computed on a random data subset with 4 samples from each class. See Figure 1 for the description of panes.

Figure 13: NTK block structure of VGG11 trained on MNIST. LeCun normal initialization, initial learning rate \(0.131\). The kernel is computed on a random data subset with 4 samples from each class. See Figure 1 for the description of panes.

Figure 15: NTK block structure of VGG11 trained on CIFAR10. LeCun normal initialization, initial learning rate \(0.131\). The kernel is computed on a random data subset with 4 samples from each class. See Figure 1 for the description of panes.

Figure 19: NTK block structure of DenseNet40 trained on FashionMNIST. LeCun normal initialization, initial learning rate \(0.094\). The kernel is computed on a random data subset with 12 samples from each class. See Figure 1 for the description of panes.

Figure 17: NTK block structure of ResNet20 trained on CIFAR10. LeCun normal initialization, initial learning rate \(0.068\). The kernel is computed on a random data subset with 12 samples from each class. See Figure 1 for the description of panes.

Figure 18: NTK block structure of DenseNet40 trained on MNIST. LeCun normal initialization, initial learning rate \(0.049\). The kernel is computed on a random data subset with 12 samples from each class. See Figure 1 for the description of panes.

Figure 20: NTK block structure of DenseNet40 trained on CIFAR10. LeCun normal initialization, initial learning rate \(0.094\). The kernel is computed on a random data subset with 12 samples from each class. See Figure 1 for the description of panes.