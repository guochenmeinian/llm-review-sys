# Increasing Brain-LLM Alignment via

Information-Theoretic Compression

 Mycal Tucker

mycal@mit.edu

Equal contribution by both authors.

Greta Tuckute

gretatu@mit.edu

Equal contribution by both authors.

###### Abstract

Recent work has discovered similarities between learned representations in large language models (LLMs) and human brain activity during language processing. However, it remains unclear what information LLM and brain representations share. In this work, inspired by a notion that brain data may include information not captured by LLMs, we apply an information bottleneck method to generate compressed representations of fMRI data. For certain brain regions in the frontal cortex, we find that compressing brain representations by a small amount increases their similarity to both BERT and GPT2 embeddings. Thus, our method not only improves LLM-brain alignment scores but also suggests important characteristics about the amount of information captured by each representation scheme.

## 1 Introduction

Artificial large language models (LLMs) have emerged as the most accurate models of human language processing. LLMs generate probabilities of upcoming words that predict human reading patterns  and internal LLM representations can predict brain signals of humans reading or listening at the granularity of fMRI voxels and intracranial recordings . Such LLMs have fundamentally shifted the neuroscience of language: for the first time, we have models that are able to predict brain activity to the extent where LLMs are used to simulate experiments _in silico_, generate sentences for driving or suppressing brain responses , or infer the story content that an individual was listening to . All these studies leverage the predictive power of LLMs. However, relatively few studies () investigate the _representational alignment_ between LLM internal states and human brain units. In our work, we first investigate the representational similarity between two widely used LLM architectures  and human fMRI voxels for a very large (1,000) set of diverse, naturalistic sentences. Second, we ask whether we can leverage an information bottleneck (IB) approach  to generate compressed representations of brain activity and, in doing so, increase representational alignment between LLMs and humans. Surprisingly, we find that, in some frontal regions of the brain, compressing brain data increases alignment with LLM representations. This finding suggests that brain data encode information that LLM representations do not contain. More broadly, our work establishes the use of information theoretic tools as a "dial" to modify representational complexity and to better unify representational spaces, including those from artificial and biological language processing.

## 2 Approach

We investigate representational alignment between brain responses during language processing, and LLM representations. In compressing brain representations, we seek to explore how the two systems represent linguistic information in a unified manner (Figure 1).

Human Brain DataWe recorded brain activity in event-related 3T fMRI from N=5 participants (4 female, aged 21-30, native English speakers) during a sentence reading task. Participants read 1,000 6-word, corpus-extracted sentences that were selected to maximize semantic and stylistic diversity. Participants read each sentence presented on the screen one at a time for 2s with an inter-stimulus interval of 4s. Blood oxygenation level dependent (BOLD) responses to each sentence were estimated using GLMsingle  (5 noise regressors and a ridge regression fraction of 0.05). To mitigate the effect of collecting data across two separate scan sessions, the estimates from each session were z-scored for each voxel in each participant. We were interested in brain responses from language-selective areas of the brain, so we identified the language network functionally in each participant using an extensively validated language localizer task contrasting reading of _sentences_ with _non-words strings_). We identified the top 10% language-selective voxels in 5 broad anatomical parcels in the left hemisphere: three frontal parcels (inferior frontal gyrus [IFG], its orbital portion [IFGorb], and middle frontal gyrus [MFG]) and two temporal ones (anterior temporal [AntTemp], posterior temporal [PostTemp]). In addition, we included a language network [network] region, which consisted of all voxels in these 5 regions, yielding a total of 6 regions of interest (ROIs) in our study. All participants gave informed written consent in accordance with the requirements of an institutional review board. Further information can be found in Appendix 5 and .

Large Language Model RepresentationsWe obtained sentence representations, for each of the 1,000 sentences, from the pre-trained BERT-large-cased model  (\(24\) layers, embedding dimension of \(768\)). To obtain a summary representation of each sentence, we used the classification token, [CLS]. In the main paper, we focused on results using BERT representations, but we also conducted experiments using GPT2-XL embeddings (further details in Appendix 6).

Information BottleneckIn our approach, we use Information Bottleneck (IB) methods to generate compressed representations of brain data. In classic IB literature, one considers a stochastic encoder \(q(z|x)\) and decoder \(p(|z)\) that seek to reconstruct an input, \(x\), via a lossy representation, \(z\). IB systems weigh a trade-off between competing terms: maximizing informativeness - \(I(X;)\), how well an input can be reconstructed from \(z\) - and minimizing complexity - \(I(X;Z)\), how many bits about an input are contained in \(z\). This tradeoff is expressed more formally via the optimization

\[ I(X;)- I(X;Z)\] (1)

where \(X\) is an input, \(Z\) is a representation of \(X\), and \(\) is a reconstruction of the input, given \(Z\). By varying the scalar weight, \(\), one can control representational complexity.

Substantial prior work establishes exact and approximate methods for solving Equation 1 for varying \(\), generating encoders across a spectrum of complexity values . Similar to such work, we propose a neural network architecture that supports variational bounds on complexity, which we use to generate compressed representations of brain data for improved representational alignment.

Neural Compression Model: Fixed Variance VAEWe adopted a variational approach to the IB optimization, trading off informativeness and complexity, using a neural architecture based upon Variational Autoencoders (VAE) . In a VAE, given an input, \(x\), a deterministic encoder outputs

Figure 1: Overall approach: we measured fMRI brain activations (top) and LLM activations (bottom) for 1,000 sentences. By compressing brain representations using a variational information bottleneck method, we sought to improve representational alignment with LLMs.

parameters to a Gaussian distribution (a mean, \(\), and a diagonal standard deviation matrix, \(\)), from which a latent variable, \(z\), is sampled: \(z q_{}(z|x)=((x),(x))\)[12; 10]. This Gaussian parametrization of the encoder supports a variational bound on complexity (see details in Appendix 7.

In our work, we propose a novel neural network method, the fixed-variance VAE (fv-VAE), that makes a simple modification to existing VAE architectures to set \((x)=I\), where \(I\) is the identity matrix. Using this constant variance, the bound on complexity in Equation 3 is easily updated to remove terms dependent upon \((x)\) and yield the overall fv-VAE training loss:

\[(x,)=||x-^{2}||+(x)^{2}\] (2)

where the first term represents the mean squared error (MSE) of a reconstruction, and the second term is the simplified bound on complexity, weighted by a scalar \(\). By varying \(\), one may limit the amount of information about \(x\) encoded in \(z\).

The small difference between VAE and fv-VAE affords important benefits for Representational Similarity Analysis (RSA; ). In a standard VAE, \((x)\) can vary by dimension in the latent space, leading to some dimensions appearing more "stretched" than others. The relative scales of such dimensions are ignored in RSA using similarity metrics such as Euclidean distance or Pearson correlation, as these metrics weigh each feature dimension equally. Thus, in our work, we used fv-VAE to fix a constant scale across dimensions.

## 3 Experiments

TrainingWe trained fv-VAE models with latent dimension 128 to reconstruct the fMRI data described in Section 2: for five participants and each of the six brain ROIs, we minimized the reconstruction MSE of the 1,000 sentences (with the additional complexity loss). Models were trained for 5,000 epochs to first converge to low MSE; after epoch 5,000, \(\) increased every epoch to penalize representation complexity, which in turn increased MSE. By saving checkpoints as \(\) increased, and by replicating training across five random seeds, we generated a suite of compressed brain representations at different complexity levels. Implementation details are included in Appendix 6.

EvaluationWe used Representational Similarity Analysis (RSA; ) to evaluate representational similarity between brain and LLM representations. RSA characterizes the similarities for all pairs of sentences across all features in either the brain or the LLM representation space. Intuitively, RSA assesses the correspondence between human and LLM representations under the assumption that all features contribute equally to capture the population-level representation-space geometry.

To evaluate RSA, we first generated compressed fMRI representations for each of the 1,000 sentences in our dataset (generated as \((x)\) using a trained fv-VAE). Next, we computed the Pearson similarity of all sentence pairs of the compressed representations, yielding a square similarity matrix. Lastly, we calculated the RSA metric via the Spearman correlation coefficient between the upper triangulars of the brain similarity matrix and the LLM similarity matrix at a given layer. Overall, this RSA metric corresponded to the notion of "are sentences that are similar in the compressed brain representation space similar in the LLM embedding space." Figure 1 depicts this overall process.

Inferior Frontal Gyrus (IFG) ResultsFigure 2 depicts the similarity between LLM representations and compressed brain responses (the frontal IFG language region), demonstrating how compressing representations increased RSA scores for some participants. Figure 2 a shows the RSA scores vs. MSE of the compressed brain representations for BERT Layer 6. The MSE value captures the reconstruction error of the trained fv-VAE model used to generate the representations; greater MSE corresponds to greater compression. At a high level, RSA scores for all participants (different colors) decrease at the highest MSE values. However, critically, for low MSE values, RSA scores for participants B, C, and D _increase_ as MSE increases. That is, compressing the fMRI data to a certain extent increases similarity to BERT representations.

Figure 2 b shows systematic improvements across all BERT layers, for participants B, C, and D. Each faded line represents RSA scores for non-compressed fMRI data from each participant; bold lines represent RSA scores using compressed fMRI representations (RSA scores were highly significant (Appendix 9)). We chose the optimal level of compression via 5-way cross-validation, selecting

the optimal MSE for a single fv-VAE model and averaging the corresponding RSA values for other models at that MSE. For participants for whom the bold lines are above the faded lines (B, C, and D), we find a consistent benefit in compressing brain representations.

Frontal Brain Region ResultsWe performed similar RSA analyses for compressed representations for frontal (IFG, IFGorb, and MFG) and temporal (AntTemp and PostTemp) brain regions of interest. Compressing frontal region representations increased alignment for participants B, C, and D, but we found no such benefit for temporal regions. For example, for BERT layers \(<10\), compressed scores for participant C improved over the uncompressed RSA scores in regions IFGorb and MFG (Figures 3 a and b, respectively), but RSA scores for AntTemp were largely unchanged for compressed data (Figure 3 c). Further experiments, computing RSA scores between GPT2 embeddings and compressed brain representations corroborate this trend: frontal regions showed some improvements from compression whereas temporal regions did not. Notably, the lack of increased alignment for _all_ brain regions highlights that the positive results for frontal brain regions cannot be solely attributed to simple explanations such as all compressed brain representations being more similar to LLM representations. Results for all regions, for both BERT and GPT2 RSA scores, are included in Appendix 8. Most scores were highly significant (Appendix 9).

## 4 Contributions

In this work, we proposed an information bottleneck method to generate compressed representations of brain activity during sentence processing and showed that, for some brain regions, such compression increased alignment with LLM representations. These early findings suggest that traditional fMRI methods employed during language experiments capture data that LLMs do not represent; future work may improve LLMs or seek to better understand sources of variation in brain activity. Our work hints at ideas that warrant further investigation such as 1) characterizing differences between brain regions that have previously been treated as a single language network, and 2) understanding what information remains in optimally-compressed brain representations. Ultimately, we advocate for an information-theoretic approach to understand and align representation spaces.

Figure 3: RSA for frontal (a, b) and temporal (c) regions, as a function of BERT embedding layer. In frontal regions, compressing fMRI data improved alignment, particularly for participants B and D.

Figure 2: RSA scores between the inferior frontal gyrus (IFG) and BERT embeddings. a) For BERT representations from one layer, we calculated the RSA for compressed brain data (increased MSE reflects increased compression). For some subjects, like B and D, small increases in compression improved RSA. b) Using the optimal MSE for each layer, we compared compressed RSA scores (bold) to uncompressed (faded) across BERT layers. Subjects B, C, and D had improved alignment.