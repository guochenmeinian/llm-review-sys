# Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation

Shen Yuan\({}^{1}\)   Haotian Liu\({}^{1}\)   Hongteng Xu\({}^{1,2}\)

\({}^{1}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\)Beijing Key Laboratory of Big Data Management and Analysis Methods

{shenyuan721, haotianliu, hongtengxu}@ruc.edu.cn

A part of this work was done when Haotian Liu was affiliated with the Beijing Institute of Technology.Corresponding author

###### Abstract

While following different technical routes, both low-rank and orthogonal adaptation techniques can efficiently adapt large-scale pre-training models in specific tasks or domains based on a small piece of trainable parameters. In this study, we bridge the gap between these two techniques, proposing a simple but effective adaptation method based on Householder reflections. Given a pre-trained model, our method fine-tunes its layers by multiplying each frozen weight matrix with an orthogonal matrix constructed by a chain of learnable Householder reflections (HRs). This HR-based orthogonal fine-tuning is equivalent to an adaptive low-rank adaptation. Moreover, we show that the orthogonality of the reflection planes corresponding to the HRs impacts the model capacity and regularity. The analysis motivates us to regularize the orthogonality of the HRs, leading to different implementations of the proposed Householder reflection adaptation (HRA) method. Compared with state-of-the-art methods, HRA achieves superior performance with fewer learnable parameters when adapting large language models and conditional image generators. The code of the experiments is available at https://github.com/DaShenZi721/HRA, and the method has been merged into the PEFT package.

## 1 Introduction

In recent large foundation model competitions, "Scaling Laws"  motivate researchers to increase model size continuously, which brings significantly improved model capabilities in understanding, generation, reasoning, and generalization but with more and more unbearable model adaptation costs. For instance, the GPU memory for fine-tuning a LLaMA-65B model with 16bit precision exceeds 780GB . The adaptation of image generative models (like the ControlNet in  did) may suffer the same issue when applying large vision foundation models as backbones. Consequently, fine-tuning large foundation models efficiently for adapting various downstream tasks has become a challenge in practice.

To overcome the above challenge, Parameter-Efficient Fine-Tuning (PEFT) methods  provide promising solutions, which aim to reduce the trainable parameters and memory consumption of fine-tuning while maintaining even improving model adaptation performance. Among various PEFT methods, the adapter-based fine-tuning  attracts a lot because it only inserts limited trainable parameters into existing models during fine-tuning but without adding extra complexity or overhead in the inference phase. Currently, given a parameter matrix of a pre-trained model, i.e., \(^{d_{} d}\), there are roughly two strategies implementing the adapter-based fine-tuning. The mainstream strategy is applying Low-Rank Adaptation (LoRA)  and its variants , modifying the weight matrix by adding a trainable low-rank decomposition matrix, i.e., \(+\), where \(^{d_{} r}\) and \(^{r d}\), and \(r(d,d_{})\) is the intrinsic rank of the modification \(\). Another strategy is Orthogonal Fine-Tuning (OFT) [37; 34], which multiplies the weight matrix with a structured orthogonal matrix \(^{d d}\) determined by limited trainable parameters, i.e., \(\). Both strategies can reduce VRAM usage because they merely leverage limited trainable parameters and do not store the optimizer state of the original weight matrices. At the same time, they achieve encouraging model adaptation performance in various vision and NLP tasks.

Essentially, LoRA hypothesizes that the additive modifications of weight matrices are intrinsically low-rank, while OFT preserves the pairwise angles between neuron vectors and theoretically penalizes the discrepancy between pre-trained and fine-tuned models. The difference between their principles prevents us from building a unified adapter-based fine-tuning framework. To bridge the gap between these two strategies, we propose a simple but effective adapter-based fine-tuning method called **Householder Reflection Adaptation (HRA)**. This method provides a new perspective connecting LoRA to OFT and achieves encouraging performance in various downstream tasks. As illustrated in Figure 0(a), our method adapts a pre-trained model by multiplying each frozen weight matrix with a chain of \(r\) learnable Householder reflections (HRs) . HRA can be interpreted as either an OFT adapter or an adaptive LoRA. Consequently, it harnesses the advantages of both strategies, reducing parameters and computation costs while penalizing the loss of pre-training knowledge.

Moreover, we show that the orthogonality of HR planes impacts the capacity and regularity of HRA. Accordingly, we leverage an orthogonality regularizer of the HR planes when applying HRA, achieving a trade-off between the model capacity and regularity by controlling the strength of the regularizer. When the weight of the regularizer (i.e., the \(\) in Figure 0(a)) goes to infinity, we constrain the orthogonality strictly by Gram-Schmidt orthogonalization , resulting in a strictly-orthogonal HRA implementation. We apply HRA to adapt different models, including large language models (LLMs) and conditional image generators. Experiments show that HRA consistently outperforms state-of-the-art adapters in various tasks, achieving better performance with fewer trainable parameters. Figures 0(b)-0(c) highlight the superiority of our method in natural language understanding and mathematical reasoning tasks, and more results can be found in the following content.

Figure 1: (a) An illustration of our HRA method. (b) Comparisons for various methods on GLUE benchmark . The x-axis corresponds to the number of trainable parameters (M), and the y-axis corresponds to the average score (%). (c) Comparisons for various methods on the ratio of trainable parameters and accuracy (%) when adapting LLaMA2-7B  in mathematical reasoning tasks.

Related Work and Preliminaries

The early PEFT methods [57; 44; 10] apply **model fine-tuning**, which keep model architectures unchanged and only update a small portion of model parameters. To achieve better performance, **soft prompt fine-tuning**[13; 28; 30; 49] is proposed, introducing additional trainable parameters into inputs and/or hidden layers when adapting models. Recently, **adapter-based fine-tuning**[20; 62] is proposed to improve model adaptation performance without changing model architecture. As we mentioned before, it is often implemented based on the following strategies.

### Low-rank Adaptation (LoRA)

LoRA  formulates trainable parameters as decomposed low-rank matrices and aggregates them to frozen weight matrices linearly during fine-tuning, which achieves a trade-off between efficiency and effectiveness. Following LoRA, many improved low-rank adaptation methods [9; 21; 25; 31; 33; 35; 48; 54; 60; 62; 4; 11] have been proposed, which can be coarsely categorized into three classes.

* **Structure Adjustment.** The work in [48; 25; 62; 60] further reduces trainable parameters by adjusting the structure of inserted low-rank matrices. VeRA  incorporates frozen low-rank matrices shared across all layers with few trainable scaling vectors. DyLoRA  learns low-rank matrices with different ranks and determines optimal ranks automatically. AdaLoRA  prunes trainable parameters based on the importance scores of the original weight matrices.
* **Initialization Improvement.** Some methods [33; 35] utilize matrix decomposition methods on the original weight matrices to initialize parameters. DoRA  decomposes each original weight matrix into magnitudes and directions for fine-tuning. PiSSA  performs singular value decomposition (SVD) on each original weight matrix, where the low-rank principal matrix serves as trainable parameters, while the residual matrix is frozen.
* **Parameter Quantization.** Some methods [54; 9; 31] quantize the pre-trained model to further reduce computational costs in both training and inference. For example, QA-LoRA  achieves a trade-off between quantization strength and adaptation performance with the help of a group-wise quantization operator.

These methods have empirically demonstrated decent performance in various downstream tasks, however, they lack theoretical guarantees regarding the retention of pre-training knowledge.

### Orthogonal Fine-tuning (OFT)

LoRA and its variants have empirically demonstrated decent performance in various downstream tasks. However, they lack theoretical guarantees regarding the retention of pre-training knowledge. In order to address this issue, orthogonal fine-tuning (OFT) [37; 34] is proposed, which transforms neuron vectors within the same layer using the same set of orthogonal matrices. It preserves the pairwise angles between neuron vectors and thus guarantees a bounded discrepancy between pre-trained and fine-tuned models. For instance, the OFT method in  adopts Cayley parameterization to generate the block-diagonal orthogonal matrix. The BOFT  introduces butterfly factorization to generate a denser orthogonal matrix from a chain of structured sparse matrices, which improves OFT's performance with fewer trainable parameters.

Note that, besides fine-tuning, the principle of imposing orthogonality constraints on trainable parameters has been applied in designing robust and training-efficient neural network architectures, e.g., convolution neural networks (CNNs) [14; 27], recurrent neural networks (RNNs) [51; 26], and Transformers . In particular, by constraining the orthogonality of these models' weight matrices, we can ensure the models are 1-Lipschitz in theory and thus make them achieve provable robustness against adversarial perturbations [47; 29] and generalization bounds [43; 24].

## 3 Proposed Method

### Model Adaptation via Learning A Chain of Householder Reflections

Denote \(^{d-1}=\{^{d}\|\|_{2}=1\}\) as a \(d\)-dimensional hypersphere. For each \(^{d-1}\), we can construct a Householder reflection matrix, denoted as \(\), by \(-2^{}\), which corresponds to 

[MISSING_PAGE_FAIL:4]

number of parameters comparable to OFT, BOFT often applies a smaller block size (e.g., \(b=2\) or \(4\)). It is easy to find that when \(r=b\) (\(=mb\)), HRA has the same number of parameters as OFT (BOFT). Therefore, HRA is generally comparable to OFT and BOFT regarding model size.
* **Computational complexity.** For the forward step of OFT, i.e., \(=^{(b)}\), the computational complexity is \((d(b^{2}+b+d_{}))\). Here, \((db^{2})\) corresponds to applying Cayley transformation to construct \(\) orthogonal sub-matrices, and \((db)\) and \((dd_{})\) correspond to the matrix block multiplication used for \(=^{(b)}\) and the matrix-vector multiplication for \(=\), respectively. For the forward step of BOFT, i.e., \(=^{(m,b)}\), where \(^{(m,b)}=_{i=1}^{m}_{i}^{(b)}\), the computational complexity is \((d(mb^{2}+mb+d_{}))(d(mb^{2}+md+d_{ {out}}))\). Similar to OFT, \((dmb^{2})\) means applying Cayley transformation to construct \(\) orthogonal sub-matrices, \((dmb)(d^{2}m)\) corresponds to the matrix-vector multiplications of \(m\) butterfly matrices4 to compute \(=_{i=1}^{m}_{i}^{(b)}\), and \((dd_{})\) corresponds to \(=\). When setting \(r b^{2}+b\) (\( m(b^{2}+b)\)), HRA can be more efficient than OFT (BOFT). * **The trade-off between model capacity and regularity.** The ranges of OFT, BOFT, and HRA correspond to different subsets of \(_{d d}\), achieving a trade-off between model capacity and regularity. To cover the whole \(_{d d}\), OFT needs to set \(b=d\) and compute a dense orthogonal matrix with high complexity. With the help of the butterfly structure, BOFT can derive a dense orthogonal matrix with fewer parameters. However, we need to construct \(\{_{i}^{(m,b)}\}_{i=1}^{d-1}\) with \(m= d\) and \(b=2\), such that each \(_{i}^{(m,b)}(_{i}^{(m,b)})^{}\) can represent a Householder reflection matrix , and accordingly, the chained product of the \(d-1\) Householder reflections can represent an arbitrary orthogonal matrix in \(_{d d}\). Similarly, for HRA, we need to construct \(d-1\) Householder reflections based on \(\{_{i}\}_{i=1}^{d-1}\) to represent an arbitrary orthogonal matrix in \(_{d d}\). According to the above analysis, BOFT and HRA are relatively easy to achieve a trade-off between model capacity and regularity with mild computation costs -- by setting small \(m,b,\) and \(r\), they can achieve dense orthogonal matrices that have intrinsic low-dimensional manifold structures.

### Connections with Low-rank Adaptation

Different from OFT and BOFT, HRA can also be viewed as an adaptive low-rank adapter. Specifically, we can rewrite the chain of HRs in equation 1 in the following equivalent format:

\[^{(r)}=_{i=1}^{r}(-2_{i}_{i}^{ })=+_{r}_{r}_{r}^{},\] (3)

where \(_{r}=[_{1},...,_{r}]^{d r}\), \(_{r}=[_{ij}]^{r r}\) is an upper-triangular matrix, and it can be defined as

\[_{1}=-2,\ _{r}=_{r-1}&-2_{r-1}_{r-1}^{}_{r}\\ _{r-1}^{}&-2.\] (4)

Accordingly, we have

\[^{(r)}=+_{r}_{r}_{r}^{}=+ (,).\] (5)

The above formulation can be viewed as an adaptive LoRA that inherits the theoretical guarantee of OFT on the retention of pre-training knowledge. The low-rank matrix \(=_{r}\), is constructed by normalized vectors, while the low-rank matrix \(=[_{1},...,_{r}]\) is parameterized as \(_{r}_{r}\), which can be treated as a function of \(\) and \(\). Therefore, similar to OFT and BOFT, HRA ensures that the columns of \(^{(r)}\) are always in the column space of \(\).

### Enhancing The Orthogonality of Householder Reflections for Stronger Regularity

Besides the number of Householder reflections, the orthogonality of them also impacts the regularity of HRA. Specifically, the supreme change of the weight matrix \(\), i.e., \(_{^{(r)}}\|-^{(r)}\|_{F}\)

Figure 2: A 2D illustration indicating that when the reflection planes \(_{1}\) and \(_{2}\) are orthogonal, the distance \(\|_{2}_{1}-\|_{2}\) is maximized.

(or, equivalently, \(_{_{r}}\|_{r}_{r}_{r}^{}\|_{F}\)), is achieved when \(_{r}\) consists of the top-\(r\) right singular vectors of \(\). In such a situation, \(_{r}\) is an orthogonal matrix, i.e., \(_{r}^{}_{r}=_{r}\). When the orthogonality is not held, \(\|_{r}_{r}_{r}^{}\|_{F}\) is reduced, as illustrated in Figure 2. In other words, when adapting the pre-trained model, enhancing the orthogonality of \(_{r}\) imposes stronger regularity on the adapter -- it encourages the discrepancy between the target model and the original pre-trained model while restricting the feasible domain of the adapter's parameter matrix.

Motivated by the above analysis, we can implement HRA with an orthogonality regularizer. Typically, given a pre-trained model \(\), we can adapt \(L\) weight matrices of the model based on a dataset \(\) by solving the following optimization problem:

\[_{\{_{r}^{(l)}\}_{l=1}^{L}}(;\{_{r}^{(l )}\}_{l=1}^{L})+_{l=1}^{L}\|_{r}-(_{r}^{(l)} )^{}_{r}^{(l)}\|_{F}^{2},\] (6)

where \(_{r}^{(l)}\) denotes the parameters of HRA for the \(l\)-th weight matrix. In equation 6, the first term denotes the loss function, while the second term is the proposed regularizer that encourages the orthogonality of all \(_{r}^{(l)}\)'s, whose significance is controlled by \(>0\). Because it does not change the forward step of HRA, this regularizer only increases the adaptation cost slightly.

As shown in Figure 0(a), by controlling the strength of the orthogonality regularizer, we can achieve a trade-off between the model capacity and regularity. When \(=0\), the feasible domain of \(_{r}\) is the set of column-normalized matrices, and accordingly, the model capacity is maximized. In contrast, when \(\), the feasible domain of \(_{r}\) is the set of orthogonal matrices (i.e., \(_{d r}\)), leading to the strongest regularity. When \(=\), we implement a strictly-orthogonal HRA based on Gram-Schmidt (GS) orthogonalization. For each layer's HRA adapter, we initialize its parameter matrix as \(_{r}^{d r}\) and applying Gram-Schmidt orthogonalization  to it, i.e., \(_{r}=(_{r})\). As shown in Figure 0(a), in such a situation, the forward step of each adapter becomes \(=(-2_{r}_{r}^{})\), and the computational complexity becomes \((d(r^{2}+r+d_{}))\), where additional \((dr^{2})\) operations are used for Gram-Schmidt orthogonalization. According to Table 1, the complexity of this strictly-orthogonal HRA is comparable to OFT  when \(r=b\).

## 4 Experiments

To demonstrate the effectiveness of HRA, we conduct comparative experiments for HRA and state-of-the-art adaptation methods on three models oriented to different tasks, including DeBERTaV3-base  for natural language understanding, LLaMA2-7B  for mathematical reasoning, and Stable Diffusion  for conditional text-to-image generation. Typical results are shown below. **More results and implementation details are provided in the Appendix.**

In each experiment, we set the number of HRs (i.e., \(r\)) to ensure that HRA has comparable or fewer trainable parameters than existing adaptation methods (including LoRA, OFT, and their variants). Setting \((0,)\) leads to the proposed HRA method, and we demonstrate the robustness of HRA to the \(\) in a wide range. By default, we set \([10^{-5},10^{-3}]\) in the experiments. Furthermore, to

   Method & \#Param (M) & MNLI & SST-2 & CoLA & QQP & QNLI & RTE & MRPC & STS-B & All \\  Full Fine-tune & 184 & 89.90 & 95.63 & 69.19 & **92.40** & 94.03 & 83.75 & 89.46 & 91.60 & 88.25 \\ BitFit & 0.10 & 89.37 & 94.84 & 66.96 & 88.41 & 92.24 & 78.70 & 87.75 & 91.35 & 86.20 \\ H-Adapter & 1.22 & 90.13 & 95.53 & 68.64 & 91.91 & 94.11 & 84.48 & 89.95 & 91.48 & 88.28 \\ P-Adapter & 1.18 & 90.33 & 95.61 & 68.77 & 92.04 & 94.29 & 85.20 & 89.46 & 91.54 & 88.41 \\ LoRA \({}_{r=8}\) & 1.33 & 90.65 & 94.95 & 69.82 & 91.99 & 93.87 & 85.20 & 89.95 & 91.60 & 88.50 \\ AdaLoRA & 1.27 & **90.76** & 96.10 & 71.45 & 92.23 & 94.55 & 88.09 & 90.69 & 91.84 & 89.46 \\ OFT \({}_{b=16}\) & 0.79 & 90.33 & 96.33 & **73.91** & 92.10 & 94.07 & 87.36 & 92.16 & 91.91 & 89.77 \\ BOFT \({}_{b=8}^{m=2}\) & 0.75 & 90.25 & 96.44 & 72.95 & 92.10 & 94.23 & 88.81 & 92.40 & **91.92** & 89.89 \\  HRA \({}_{r=8}\), \(0\) & 0.66 & 90.70 & 96.45 & 73.70 & 91.29 & **94.66** & 88.45 & 93.69 & 91.86 & **90.10** \\ HRA \({}_{r=8}\), \(10^{-5}\) & 0.66 & 90.43 & **96.79** & 71.91 & 91.02 & 94.44 & **89.53** & **94.10** & 91.74 & 90.00 \\ HRA \({}_{r=8}\), \(\) & 0.66 & 90.52 & 95.87 & 70.71 & 90.71 & 94.12 & 87.00 & 92.59 & 91.54 & 89.13 \\   

Table 2: Results (%) of various methods on the GLUE development set. The best results on each dataset are shown in **bold**, and the second best results are shown in underline. We report the matched accuracy for MNLI, Matthew’s correlation for CoLA, and average correlation for STS-B.

analyze the trade-off between model capacity and regularity, we consider two variants of HRA: \(i)\) the HRA without the orthogonality regularization (\(=0\)) and \(ii)\) the strictly-orthogonal HRA using GS orthogonalization (i.e., \(=\)). For convenience, we denote \(_{r,}\) as the HRA learning \(r\) Householder reflections per layer with an orthogonality regularizer weighted by \(\).

### Natural Language Understanding

We adapt DeBERTaV3-base  by different methods and test the performance of the adapted models on the General Language Understanding Evaluation (GLUE) benchmark . Following AdaLoRA  and BOFT , we consider eight tasks of GLUE in this experiment, including two single-sentence tasks, three similarity and paraphrase tasks, and three inference tasks. The experimental results in Figure 0(b) and Table 2 show that using fewer trainable parameters, HRA achieves the best or comparable results across all datasets and thus leads to the best average performance. These results demonstrate the efficiency and effectiveness of HRA.

In this experiment, the HRA without the regularization (i.e., \(=0\)) and that using weak regularization (i.e., \(=10^{-5}\)) achieves comparable adaptation results, while imposing strong regularity by strict orthogonality (i.e., \(=\)) harms the model performance. This interesting phenomenon implies that the adaptation tasks in GLUE are challenging enough to apply the adapter with sufficient capacity. Figure 3 shows the performance of HRA on MRPC when \([10^{-7},10^{-3}]\). We can find that HRA achieves relatively stable performance when \(\) changes in a wide range.

### Mathematical Reasoning of LLM

We adapt LLaMA2-7B  on the MetaMathQA dataset  by different adaptation methods and test the adaptation performance on the GSM8K  and MATH  validation sets. Following LoRA , each method only adapts the query and value projection matrices of LLaMA2-7B. The results in Figure 0(c) show that HRA outperforms its competitors on both GSM8K and MATH when we set \(r=32\) and make it have the same number of trainable parameters as BOFT . Furthermore, the HRA with \(r=16\) utilizes only half the trainable parameters of BOFT yet still surpasses its performance, which demonstrates the efficiency of HRA. In addition, the HRA with the orthogonality regularization achieves a trade-off between model capacity and regularity. In Figure 4, we test the robustness of HRA to \(\), demonstrating that the performance of the HRA with \(r=8\) is stable when \([10^{-5},10^{-3}]\). For the HRA using more HRs (e.g., \(r=16\) and \(32\)), we set \(=10^{-4}\) based on this robustness test result, balancing the performance on GSM8K and MATH. As shown in Table 0(c), the best performance is achieved when \(r=32\) and \(=10^{-4}\).

Besides, to verify whether HRA can better retain pre-training knowledge, we fine-tune LLaMA-2 7B on the MATHQA dataset by LoRA and HRA, respectively, and check the degradation of model performance on classic NLP tasks, including typical language tasks in ARC , HellaSwag , MMLU , Winogrande , and a coding task in HumanEval . Ideally, after adaptation, we hope that the model can still maintain its high performance in the NLP tasks. The results in Table 3 show that compared to LoRA, HRA retains more of the original model's knowledge, whose performance degradation is less severe than LoRA's. In the HumanEval task, its performance is even

Figure 4: The robustness of HRA (\(r=8\)) to \(\) in mathematical reasoning tasks.

Figure 3: The robustness of HRA (\(r=8\)) to \(\) on MRPC.

better than that of the original model (which we think is because the MATHQA dataset contains many samples relevant to logic and reasoning tasks and thus is useful in the HumanEval task).

### Controllable Text-to-Image Diffusion Models

Following OFT  and BOFT , we evaluate HRA on adapting pre-trained Stable Diffusion (SD)  for subject-driven generation and controllable generation, respectively. For a fair comparison, we employ experimental procedures and evaluation metrics as the same as OFT :

* **Subject-driven generation.** Given several images of a specific subject and a textual prompt, subject-driven generation aims to generate images of the same subject in a context aligning with the prompt. Taking SD as the backbone model, we evaluate the generation performance of different model adaptation methods, including DreamBooth , LoRA , OFT and its variant COFT , and our HRA. Following DreamBooth , we train and evaluate on generating 25 subjects, each of which corresponds to 30 prompts.
* **Controllable generation.** Controllable generation aims to generate images aligning with a textual prompt and additional control signals (such as facial landmark annotations, canny edges, and segmentation maps). We conduct experiments on three challenging controllable generation tasks: Canny edge to image (C2I) on the COCO dataset , landmark to face (L2F) on the CelebA-HQ dataset [23; 52], and segmentation map to image (S2I) on the ADE20K dataset . In this experiment, we use DreamBooth , ControlNet , T2I-Adapter , LoRA , OFT and its variant COFT , and BOFT  as baselines.

Table 4 shows the quantitative experimental results. In the subject-driven generation task, we evaluate three crucial aspects of generated images: subject fidelity (DINO , CLIP-I ), textual prompt fidelity (CLIP-T ), and sample diversity (LPIPS ). It can be observed that HRA achieves remarkable improvement across almost all metrics. In addition, we find that without the orthogonality (\(=0\)), HRA achieves the highest subject fidelity while sacrificing textual prompt fidelity and sample diversity to some extent, while the strictly-orthogonal HRA (\(=\)) shows opposite tendencies. Applying the orthogonality regularization with \(=10^{-3}\) makes HRA balance the performance in all the metrics. Similarly, in the three controllable generation tasks, HRA demonstrates stronger and more precise control compared to the baselines. However, in these tasks, the strictly-orthogonal HRA leads

    &  &  &  &  \\  & & & & & & & & & & & \\  & & & & & & & & & & & \\  & & & & & & & & & & & \\  Real Images & - & 0.764 & 0.890 & - & 0.562 & - & - & - & - & - & - \\  DreamBooth & 859.52 & 0.614 & 0.778 & 0.239 & 0.737 & 859.52 & 0.049 & 0.093 & 7.72 & 14.40 & 33.61 & 146.19 \\ ControlNet & - & - & - & - & - & 361.30 & 0.189 & 0.317 & 20.88 & 30.91 & 61.42 & 7.61 \\ T2I-Adapter & - & - & - & - & - & 77.00 & 0.078 & 0.143 & 16.38 & 26.31 & 51.63 & 23.75 \\ LoRA & 0.8 & 0.613 & 0.765 & 0.237 & 0.744 & 1.25 & 0.168 & 0.286 & 22.98 & 35.52 & 58.03 & 7.68 \\ COFT \({}_{b=4}\) & 23.3 & 0.630 & 0.783 & 0.235 & 0.744 & 26.40 & 0.195 & 0.325 & 26.92 & 40.08 & 62.96 & 6.92 \\ OFT \({}_{b=4}\) & 23.3 & 0.632 & 0.785 & 0.237 & 0.746 & 26.40 & 0.193 & 0.323 & 27.06 & 40.09 & 62.42 & 7.07 \\ BOFT \({}_{r=8}^{m=4}\) & - & - & - & - & - & 20.76 & - & - & 28.83 & 41.24 & 67.74 & 5.67 \\  HRA \({}_{r=7,8\ =0}\) & 0.69 & **0.670** & **0.803** & 0.238 & 0.758 & 0.89 & **0.213** & **0.350** & **29.45** & **42.02** & 66.83 & 5.56 \\ HRA \({}_{r=7,8\ =10^{-3}}\) & 0.69 & 0.661 & 0.799 & 0.255 & 0.760 & 0.89 & 0.205 & 0.339 & 29.27 & 40.89 & **67.86** & **5.46** \\ HRA \({}_{r=7,8\ =}\) & 0.69 & 0.651 & 0.794 & **0.274** & **0.778** & 0.89 & 0.201 & 0.334 & 28.15 & 40.22 & 64.95 & 11.11 \\   

Table 4: Results of various methods on subject-driven generation and controllable generation. For each evaluation metric, the best result is shown in **bold**, and the second best result is shown in underline. For HRA and its variants, we set \(r=7\) and \(8\) for subjective-driven generation and controllable generation, respectively.

   Method & ARC & HellaSwag & MMLU & Winogrande & HumanEval & Overall Impact \\  LLaMA2-7B & 49.74 & 58.90 & 45.92 & 74.11 & 12.80 & — \\  Fine-tuned by LoRA & 48.81 & 56.89 & 40.60 & 71.27 & 11.59 & -6.03\% \\ Fine-tuned by HRA & 49.57 & 57.72 & 41.20 & 73.32 & 13.41 & -1.79\% \\   

Table 3: The results (%) of LLaMA2-7B on classic natural language processing tasks after fine-tuned on MATHQA by LoRA and HRA, respectively.

to suboptimal performance. It means that these tasks require our adapter to have sufficient capacity, but the strict orthogonality constrains its capacity too much. In both tasks, HRA demonstrates the smallest number of trainable parameters among the compared methods. Figures 5 and 6 provide typical qualitative results, demonstrating that the images generated based on HRA have good visual effects and well-aligned semantics.

## 5 Conclusion

In this study, we have proposed a simple but effective Householder reflection adaptation method and have demonstrated its usefulness in various adaptation tasks. The proposed HRA method bridges the gap between low-rank and orthogonal adaptation strategies. It simplifies the implementation of OFT while inheriting its theoretical guarantees on the retention of pre-training knowledge. In addition, we show that controlling the orthogonality of the Householder reflections can achieve the trade-off between HRA's model capacity and its regularity. In the future, we would like to improve HRA for practical applications, including accelerating its computation, reducing its memory cost, exploring other regularizers for parameter matrices, and adjusting the weights of the regularizers automatically. We also plan to test HRA on adapting more advanced LLMs, e.g., LLaMA3 and Grok-1.

Figure 5: Qualitative results on subject-driven generation.

Figure 6: Comparisons for various adaptation methods on controllable generation, in which the control signals include Canny edges, face landmarks, and semantic segmentation results of reference images.