# Optimal Parameter and Neuron Pruning for

Out-of-Distribution Detection

 Chao Chen\({}^{1}\), Zhihang Fu\({}^{1}\)1, Kai Liu\({}^{2}\), Ze Chen\({}^{1}\), Mingyuan Tao\({}^{1}\), Jieping Ye\({}^{1}\)

\({}^{1}\)Alibaba Cloud \({}^{2}\)Zhejiang University

{ercong.cc, zhihang.fzh, yejieping}@alibaba-inc.com

Corresponding Author

###### Abstract

For a machine learning model deployed in real world scenarios, the ability of detecting out-of-distribution (OOD) samples is indispensable and challenging. Most existing OOD detection methods focused on exploring advanced training skills or training-free tricks to prevent the model from yielding overconfident confidence score for unknown samples. The training-based methods require expensive training cost and rely on OOD samples which are not always available, while most training-free methods can not efficiently utilize the prior information from the training data. In this work, we propose an **O**ptimal **P**arameter and **N**euron **P**runing (**OPNP**) approach, which aims to identify and remove those parameters and neurons that lead to over-fitting. The main method is divided into two steps. In the first step, we evaluate the sensitivity of the model parameters and neurons by averaging gradients over all training samples. In the second step, the parameters and neurons with exceptionally large or close to zero sensitivities are removed for prediction. Our proposal is training-free, compatible with other post-hoc methods, and exploring the information from all training data. Extensive experiments are performed on multiple OOD detection tasks and model architectures, showing that our proposed OPNP consistently outperforms the existing methods by a large margin.

## 1 Introduction

Over the past decade, deep neural networks have achieved dramatic performance gains in computer vision , natural language processing , and AI for Science . However, when deploying those deep learning models in real world scenarios, the model will provide a false prediction result for unseen categories, which will lead to serious security issues. A promising approach to address this problem is Out-of-Distribution (OOD) detection , which aims to distinguish whether the given sample is from training class or unknown category. The deep learning models with good OOD detection ability know what they don't know and can be used safely in real world applications.

Recently, a large number of works have been proposed to address the OOD detection problem . Early representative methods utilized the Maximum Softmax Probability (MSP)  or Mahalanobis distance  as score function. The main challenge is that modern overparameterized deep neural networks can easily produce overconfident predictions on OOD samples, making the in-distribution (ID) data and OOD data inseparable. To alleviate the overconfident problem, some training-based methods and post-hoc methods have been proposed. The training-based methods mitigate the overconfident problem by incorporating OOD samples in training process  or synthesizing virtual outliers to regularize the model's decision boundary . The post-hoc methods mainly focused on optimizing score function , or rectifying activations  to make the ID and OOD samples more separable. The training-based methods are controllable and interpretable, but they rely on expensive training cost and additional OOD samples, which are not always available.

In contrast, the post-hoc methods are training-free, low-cost and plug and play, however they can not effectively leverage the prior information from the training data and trained models.

It has been widely observed that overparameterized deep neural networks often suffer from redundant parameters and neurons, which consequently result in overconfident predictions. Conversely, a precisely pruned sub-network is capable of achieving comparable performance [13; 17; 5; 30; 46]. This motivates a straightforward question: _Can we identify the parameters and neurons that lead to overconfident outputs by leveraging the prior information from the off-the-shelf models and training samples?_ To answer this question, we first demonstrate the parameter sensitivity of the last fully-connected (FC) layer for ResNet50  and Vision Transformer (ViT-B/16)  in Fig. 1. As can be observed, the distribution of parameter sensitivity is heavily positively skewed. More specifically, a significant proportion of parameter sensitivities are close to zero, while only a few parameters exhibit exceptionally high sensitivities. The gap between the maximum and minimum sensitivity values is more than 200 times.

The above observation naturally inspires a simple yet surprisingly effective method -- **O**ptimal **P**arameter and **N**euron **P**runing (**OPNP**) for OOD detection. The motivations of our OPNP are in two aspects: (1) The parameters and neurons with sensitivities close to zero are redundant and can lead to overconfident predictions [23; 15; 44]. (2) The parameters with exceptionally large sensitivity result in a sharp landscape, which hurts the model generalization [11; 55; 54]. Therefore, in this study, we present empirical and theoretical evidences to show that the OOD detection performance can be significantly improved by pruning parameters and neurons with exceptionally large or close to zero sensitivities. The main contribution of this paper are as follows:

* A simple yet effective training-free method, which significantly improves OOD detection performance by removing weights and neurons with exceptionally large or close to zero sensitivities.
* We evaluate OPNP on different OOD detection tasks and model architectures, including ResNet and ViT. Compared to the baseline model, OPNP achieves 32.5% FPR95 reduction on a large-scale ImageNet-1k benchmark, and outperforms existing state-of-the-art post-hoc OOD detection method by 5.5% in FPR95.
* Extensive ablation experiments are performed to reveal the insight and effectiveness of the proposed method. We show that OPNP is compatible with other post-hoc methods and can benefit model calibration. We believe our insights can inspire and accelerate future research in related tasks.

## 2 Related Work

**General OOD Detection.** OOD detection is highly relevant to several early research tasks, including outlier detection (OD), anomaly detection (AD)  and open-set recognition (OSR) . All

Figure 1: Illustration of parameter sensitivity distribution for (a) ResNet50 and (b) ViT-B/16. The parameters are selected from the last fully-connected layer for both ResNet50 and ViT-B/16. The dotted line in red indicates the average sensitivity and the maximum sensitivity is normalized to 1.

these tasks are aim to identifying the OOD samples in the open world scenario . The most promising OOD detection methods can be divided into five categories, including density-based methods , distance-based methods [26; 45], outlier exposure methods [22; 28; 2], virtual OOD synthesis methods [9; 47], and post-hoc methods [31; 43]. Besides, some recent advances also pay attention to exploring large-scale pretrained vision-language model for OOD detection [35; 10], and extending OOD detection from classification to other learning tasks, such as object detection  and segmentation . In this study, we mainly focus on post-hoc OOD detection, which is a training-free approach that does not require any OOD samples. For a more comprehensive understanding of the general OOD detection task, we suggest referring to  for further details.

**Post-hoc OOD Detection.** Recently, the post-hoc OOD detection methods have achieved promising performance and drawn increasing attention [44; 43; 57; 31; 29]. To alleviate the over-confident prediction caused by the softmax function, ODIN  introduces a temperature scaling strategy to make the softmax scores between ID and OOD images more separable. Liu et al.  replace the MSP score with energy score, which is theoretically aligned with the probability density and less susceptible to the overconfident problem. In huang et. al , GradNorm is presented which utilizes the magnitude of gradients as OOD score. Another line of work relieve overconfident problem by rectifying typical features in the penultimate layer [43; 6; 57]. In particular, ReAct  truncates features with a global threshold, which is chosen to preserve the activations for ID data while rectifying that of OOD data. ASH  shows that simply removing lower activations or binarizing representations leads to promising OOD detection performance. The most relevant method to our proposal is DICE , which ranks weights based on a measure of contribution, and selectively use the most salient weights to derive the output for OOD detection. Both DICE and our proposal alleviate over-fitting by model pruning. DICE selects the most important connections while our proposal removes the most sensitive and insensitive parameters and neurons. Besides, the method used to measure the weight importance or sensitivity is different.

**Parameter and Neuron Pruning.** In deep neural networks, parameter and neuron pruning is widely used for reducing over-fitting [23; 42; 15; 5] and model compression [33; 17; 27]. Dropout is the most well-known method to improve the robustness of deep models, which randomly drops some neurons  or connections  in training time. Based on dropout, Gomez et al.  introduce targeted dropout, which drops units and weights with low magnitude. They experimentally demonstrated that target dropout benefit to post-hoc pruning of units and weights. Besides, parameter and neuron pruning have also been widely used in model compression. Han et al.  indicate that the weights with low magnitude is less important and propose to remove those weights for model compression. In contrast, Li et al.  propose to drop the feature maps based on weight norms, the model performance is well preserved after pruning more than 30% units. In [30; 3], the authors demonstrate the effectiveness of ensembling multiple sparse networks, which are trained from scratch with dynamic sparsity constraint, for OOD detection. The aforementioned researches have demonstrated that the weights and neurons are significantly redundant in deep networks, which inspires us to prune the parameters and neurons that result in over-fitting for OOD detection. We believe optimal post-hoc parameter and neuron pruning is a promising approach for OOD detection.

## 3 Method

### Problem statement

Assume that we have an in-distribution dataset \(_{in}\) of pairs \((_{in},y_{in})\) and an out-of-distribution dataset \(_{out}\) of pairs \((_{out},y_{out})\), where \(x_{in},x_{out}\) denote the input feature vector of ID and OOD samples, \(y_{in}_{in}:=\{1,2,,K\}\) denotes the ID class label, and \(y_{out}_{out}\) denotes the output class label, \(_{in}_{out}=\). Given a classification model \(f(;)\) trained from in-distribution dataset \(_{in}\). The goal of post-hoc OOD detection is to design a binary classifier \(g_{}(x)\) which is able to distinguish whether the test sample is from ID or OOD distribution. Therefore, the challenge of OOD detection is to find an optimal score function \(S()\) such that for a given test sample \(x\),

\[g_{}(x)=ID,&S()\\ OOD,&S()<\] (1)

where samples with higher score \(S()\) are classified as ID and vice versa, \(\) is the threshold which usually set to ensure 95% ID samples are correctly classified. Maximum softmax probability and energy score  are the most widely used score functions in post-hoc OOD detection. We follow previous post-hoc methods [43; 44] to utilize energy score as OOD detection metric, which consistently outperforms MSP score.

We denote \(h()^{L}\) the feature representation from the penultimate layer, denote \(^{L K}\) and \(^{K}\) the output weights and bias of the last FC layer. Then, the output logit can be given as

\[f(;)=^{}h()+\] (2)

The energy score function  maps the output logit to a energy value by,

\[E(;)=-_{i=1}^{K}(f_{i}())\] (3)

where \(f_{i}()\) denotes the logit output for class \(i\). The energy score reduces over-fitting caused by softmax function, but the overparameterized connection weights \(\) in the last fully-connected layer may still cause overconfident logit output. Therefore, in the following section, we aim to provide an optimal parameter and neuron pruning strategy to reduce over-fitting.

### Parameter sensitivity estimation

In this section, we propose to estimate parameter sensitivity by measuring how sensitive the output energy score to a small change of parameters. For a given sample \(_{k}\) and corresponding energy output \(E(_{k};)\), a small change \(_{ij}\) is added to the parameter \(_{ij}\), which results in a change in the output energy score,

\[E(_{k};+)-E(_{k};)_{i,j}g_{ij}(_{k})_{ij}\] (4)

\[g_{ij}(_{k})=_{k};)}{_{ij}}\] (5)

Here, \(g_{ij}(_{k})\) denotes the gradient of model output to the parameter \(_{ij}\) at data point \(_{k}\). Since \(_{ij}\) is a small constant, the parameter sensitivity to model output can be measured by the magnitude of the gradient \(g_{ij}\). In this respect, given a batch of samples \(\{_{k}\}_{k=1}^{m}\), the parameter sensitivity can be estimated by accumulating the gradients over all input samples,

\[_{ij}=_{k=1}^{m} g_{ij}(_{k})\] (6)

where \(_{ij}\) denotes the sensitivity of parameter \(_{ij}\). It's worth noting that we utilize the change in energy score as the sensitivity measure, which is better aligned with the OOD detection metric. Practically, the parameter sensitivity can also be measured by other model output, such as the change of logit norm \( f(;)_{2}\), which has been exploited in lifelong learning .

In Fig. 1, we illustrate the sensitivity of parameter \(\) for two representative deep networks ResNet50  and ViT-B/16 , where the maximum sensitivity is normalized to 1. It can be seen that the maximum parameter sensitivity of the ResNet50 and ViT-B/16 is nearly 20 times than the average sensitivity. In addition, compared with ViT-B/16, there are more parameters with sensitivity close to zero in ResNet50, which indicates that the last FC layer of ResNet50 has more redundant parameters. Based on the intuition that the parameters and neurons with exceptionally large sensitivity or with sensitivity close to zero tend to result in overconfident prediction, an optimal parameter and neuron pruning strategy is introduced to improve the OOD detection performance.

### Optimal parameter and neuron pruning

In this section, we mainly introduce how to prune the connection weights and neurons in the last fully-connected layer, which directly result in overconfident results. The pruning strategy in other layers can be achieved in the same manner.

**Parameter Pruning.** Given the connection weights \(\) that maps the feature representation to logit, the corresponding parameter sensitivity \(\) can be computed according to Eq. 6. As mentioned above, the parameters with exceptionally large or close to zero sensitivity tend to result in over-fitting. Therefore, a simple threshold function can be utilized to remove the risky connections,

\[}_{ij}=0,&_{ij}<_{min}^{w}\\ 0,&_{ij}>_{max}^{w}\\ _{ij}&\] (7)

where \(}_{ij}\) denotes the weights after pruning, \(_{min}^{w}\) and \(_{max}^{w}\) denote the minimum and maximum thresholds. To align with previous post-hoc methods [43; 44; 6], we obtain the threshold by a percentile \(\), which indicates that the threshold is set to the \(\)th-percentile of the entire sensitivity matrix. For example, \(_{max}^{w}=1\%\) represents that \(_{max}^{w}\) is set to the 1% largest sensitivity value in \(\), and \(_{min}^{w}=10\%\) represents that \(_{min}^{w}\) is set to the 10% smallest sensitivity value in \(\). In Fig. 2(b), the first row illustrates the sensitivity distribution of parameter \(\) in ResNet50 before and after parameter pruning. After pruning, the connection weights that larger than \(_{max}^{w}\) or smaller than \(_{min}^{w}\) are removed, and will not contribute to the output logit, which reduces the risk of over-fitting.

**Neuron Pruning.** In additional to parameter pruning, we also propose to prune the neurons in the pre-logit layer, which has also been shown to mitigate over-fitting [42; 15]. For the \(i\)-th neuron in the pre-logit layer, it contributes to all output neurons, therefore the sensitivity of the \(i\)-th neuron should be defined based on the sensitivity of the connection weights between the \(i\)-th hidden neuron and all output neurons. Considering that the \(_{1}\) or \(_{2}\) norm of the weights are usually used to measure the importance of the units in deep networks [41; 15]. We define the neuron sensitivity as the average sensitivity of weights that connected with the neuron, which is equivalent to \(_{1}\) norm of the weight sensitivity, and reflects the average sensitivity to all classes, i.e.,

\[_{i}=_{p=1}^{K}_{ip}\] (8)

where \(_{i}\) denotes the sensitivity of \(i\)-th neuron in pre-logit layer, \(K\) represents the number of output neurons. In this respect, we can use a similar threshold function as Eq. 7 to remove the risky neurons,

\[^{i}()=0,&_{i}<_{min}^{o}\\ 0,&_{i}>_{max}^{o}\\ h^{i}()&\] (9)

where \(^{i}()\) denotes the output feature from the \(i\)-th pruned neuron in the pre-logit layer, \(_{min}^{o}\) and \(_{max}^{o}\) represent the minimum and maximum sensitivity thresholds determined by the pruning percentage \(_{min}^{o}\) and \(_{max}^{o}\). The second row in Fig. 2(b) illustrates the sensitivity distribution of the hidden neurons in ResNet50 before and after neuron pruning, where the maximum sensitivity is

Figure 2: (a) Illustration of the last fully-connected layer before and after OPNP, the connections and neurons in grey color represent the pruned ones. (b) The first row illustrate the parameter sensitivity before and after pruning and the second row illustrate the neuron sensitivity before and after pruning.

normalized to 1. As observed, after neuron pruning, the redundant neurons (with sensitivity close to zero) and risky neurons (with sensitivity far above the average) are removed and the distribution of sensitivity across neurons becomes more uniform, which potentially reduces over-fitting.

### Insight Justification

The following remarks are provided to explain why OPNP improves OOD detection performance.

**Remark 1. Parameter and neuron pruning avoid overconfident predictions.** The over-parameterized deep neural networks tend to generate overconfident predictions even for OOD samples [19; 37; 16]. Therefore, most existing methods improve OOD performance by avoiding overconfident predictions [29; 31; 44]. For a deep network, the last fully connected layer can be regarded as a linear classifier. The most widely used technique to prevent a classifier from overfitting is to employ a \(_{1}\) or \(_{2}\) regularization, which can be formulated as \(_{}_{(,) D}\|^{} h ()-\|_{2}^{2}+()\), where \(()\) represents \(_{1}\)- or \(_{2}\)-norm of \(\). As the parameter pruning is able to reduce \(()\), it can be regarded as an effective post regularization technique that reduces the model complexity and avoids overconfident predictions. Besides, the neuron pruning is similar to target dropout  which has also been demonstrated to reduce overfitting. Therefore, the proposed OPNP avoids overconfident predictions and potentially improves the OOD detection performance.

**Remark 2. Pruning the least sensitive parameters and neurons improve separability between ID and OOD samples.** We denote \(f_{j}(x)\) the logit output of \(j\)-th class, after pruning the least sensitive parameters, the logit reduction of the \(j\)-th class can be estimated as

\[ f_{j}()=_{_{jk}<_{min}^{w}}_{jk} |_{jk}| h_{k}()\] (10)

It shows that the logit reduction is positively correlated with the average sensitivity of the pruned weights. As the parameter sensitivity is computed over the training ID set, the least sensitive parameters on ID distribution should be more sensitive for OOD samples on average, i.e.,

\[_{_{jk}<_{min}^{w}}_{jk}^{OOD}>_{_ {jk}<_{min}^{w}}_{jk}^{ID}\] (11)

Therefore, the logit reduction on OOD samples is larger than on ID samples \( f_{j}(_{out})> f_{j}(_{in})\), which leads to better separability between ID and OOD samples, and improves OOD detection performance. We also show the parameter sensitivity distribution (on ID and OOD sets) of the pruned weights in Fig. 9, which experimentally verifies Eq. 11.

**Remark 3. Pruning the most sensitive parameters and neurons improves generalization.** We follow  to define the first-order flatness as

\[R_{}()_{^{} B( {},)}\| f(^{})\|, \] (12)

where \( f(^{})\) denotes the derivative at point \(^{}\), \(B(,)=\{^{}:\|-^{} \|<\}\) denotes the open ball of radius \(\) centered at the point \(\) in the Euclidean space and \(\) denotes the perturbation radius that controls the magnitude of the neighbourhood. The flatness \(R_{}()\) describes how flat the function landscape is . It has been demonstrated that a flatter landscape could lead to better generalization [11; 55; 54; 53]. Eq. 12 indicates that the first-order flatness is determined by the largest gradient, therefore, our proposed method pruning the most sensitive parameters is able to improve the flatness of the function landscape and lead to better generalization. However, according to **Remark 2**, pruning the most sensitive parameters and neurons may also hurt the separability between ID and OOD samples. Therefore, there is a trade-off between better generalization and better ID-OOD separability. This explains why OOD performance improves with very few sensitive parameters pruned and drops with a large pruning ratio, as demonstrated in Fig. 3.

## 4 Experiments

In this section, we describe our experimental setup and implementation details, then evaluate the effectiveness of the proposed OPNP method in different model architectures and OOD detection benchmarks, followed by extensive ablation studies.

[MISSING_PAGE_FAIL:7]

[43; 44], which utilizes the same experimental setup as ours. OPP, ONP and OPNP represent only utilize optimal parameter pruning, only utilize optimal neuron pruning and utilize both parameter and neuron pruning, respectively. The results in Table 1 reveal several interesting observations: (1) Optimal parameter pruning (OPP) achieves similar performance as DICE which also removes unimportant connections for OOD detection, and significantly outperforms MSP , Mahalanobis distance  and Energy  baselines. (2) Optimal neuron pruning (OPN) achieves much better performance than OPP, outperforms DICE  by a large margin and outperforms SOTA feature rectification method (ReAct)  by 2.1% in FPR95 and 0.7% in AUROC. (3) Compared to OPP and ONP, combining parameter pruning and neuron pruning (OPNP) also reduces FPR95 by 3.3% and improves AUROC by 0.6% based on ONP. (4) OPNP outperforms both SOTA weights pruning method (DICE) and SOTA feature rectification method (ReAct) by a large margin (more than 5% in FPR95). (5) Our proposed OPNP does not outperform ReAct and DICE in Texture dataset, which can be compensated by combining ReAct.

In Table 2, we compare our proposal with competitive post-hoc OOD detection methods based on ViT-B/16 model. We note that the performance of MSP  and Energy Score  in previous work  is worse than our implementation based on the same model, therefore, we reported the performance reproduced by ourselves. The results show that: (1) Both OPP and ONP outperforms Energy baseline  by a large margin, which demonstrates the effectiveness of our proposal in ViT model. (2) OPP outperforms ONP and combining parameter and neuron pruning (OPNP) does not further improve the OOD performance, which is different from the results in ResNet50. We think this is because the number of neurons in ViT-B/16 (768) is much less than in ResNet50 (2048), besides, there is a relu layer before pre-logit layer in ResNet, which results in more risky parameters and neurons in ResNet50 model. (3) OPP outperforms ReAct by 1.85% and outperforms DICE by 3.91%in FPR95, combining OPP and ReAct brings additional improvement.

**Evaluation on CIFAR benchmark.** The main results on CIFAR10 and CIFAR100 benchmarks are show in Table 5 and Table 6. As can be seen, we consider the three most commonly used post-hoc methods and compare the OOD detection performance with and without applying the OPNP. The results show that: (1) On both CIFAR10 and CIFAR100, using OPNP consistently outperforms the counterpart without OPNP, which indicates that our proposal is compatible with other post-hoc methods. (2) The performance improvement brought by OPNP on CIFAR100 is more significant than on CIFAR10 and less significant than on ImageNet benchmark. We believe this is because the FC layer on CIFAR10 classification model is much less overparameterized than CIFAR100 and ImageNet classification models. (3) The OPNP achieves similar performance as ReAct On CIFAR10 benchmark, and outperforms ReAct on CIFAR100 benchmark. Besides, utilizing OPNP and ReAct jointly reduces FPR95 by 1.55% and 2.49% in CIFAR10 and CIFAR100 benchmark, respectively.

    &  \\   &  &  &  &  &  \\   & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) & FPR95\(\) & AUROC\(\) \\  MSP & 21.28 & 91.63 & 51.96 & 85.08 & 50.63 & 84.92 & 50.57 & 87.50 & 43.61 & 87.28 \\ Energy & 7.61 & 98.23 & 40.30 & 90.77 & 46.89 & 88.62 & **33.54** & **93.21** & 32.09 & 92.71 \\ ReAct & **2.28** & **99.42** & 30.68 & 93.94 & 35.32 & 91.40 & 37.08 & 92.84 & 26.34 & 94.40 \\ DICE & 4.51 & 98.87 & 32.43 & 93.30 & 37.46 & 91.02 & 39.19 & 92.44 & 28.40 & 93.91 \\ DICE+ReAct & 2.65 & 99.38 & 29.45 & 93.52 & 38.45 & 91.17 & 33.78 & 93.27 & 26.08 & 94.34 \\ 
**OPP** & 3.12 & 99.18 & 25.28 & 93.99 & 34.00 & 91.43 & 35.56 & 92.21 & 24.49 & 94.20 \\
**ONP** & 3.87 & 99.13 & 29.63 & 93.08 & 35.68 & 90.73 & 35.60 & 91.54 & 26.20 & 93.62 \\
**OPNP** & 3.16 & 99.38 & 24.32 & 93.86 & 34.52 & 91.45 & 38.76 & 91.96 & 25.19 & 94.16 \\
**OPP+ReAct** & 2.52 & 99.35 & **23.96** & **94.50** & **32.80** & **92.10** & 36.03 & 91.77 & **23.83** & **94.43** \\   

Table 2: OOD detection results on ImageNet-1k benchmark with ViT-B/16 model. All numbers are percentages.

  
**Percentage** & \(^{w}=0\) & \(^{w}_{max}=0.1\) & \(^{w}_{max}=0.3\) & \(^{w}_{max}=1\) & \(^{w}_{max}=5\) & \(^{w}_{min}=5\) & \(^{w}_{min}=10\) & \(^{w}_{min}=20\) & \(^{w}_{min}=40\) \\ 
**ID Acc** & 76.13 & 75.14 & 74.72 & 71.60 & 56.37 & 76.06 & 75.88 & 75.86 & 75.06 \\   

Table 3: Changes in ID classification accuracy by varying pruning percentage in ResNet50 model.

### Ablation Studies.

**Effect of pruning percentage.** In Fig. 3, we demonstrate the impact on OOD detection performance by varying pruning percentages in two different tasks. Fig. 3(a) shows that pruning only 0.5% high sensitivity parameters brings significant improvement. In Fig. 3(b), we observe considerable performance improvement with a large pruning percentage for low sensitive parameters. Fig. 3(c) and Fig. 3(d) suggest that pruning high sensitive neurons is more effective than pruning low sensitive neurons, which brings marginal improvement. While OPNP achieves the SOTA performance, the performance could be significantly improved by pruning only the weights or neurons, which is much simple to determine the thresholds. Besides, the performance is improved and insensitive in a wide range of pruning ratio. For example, the optimal pruning ratio can be set to \(^{w}_{min}\) and \(^{w}_{max}[0.5,3]\) across different OOD sets. In Tabel 3, we demonstrate the impact of different pruning percentages on ID classification accuracy. As observed, pruning only 1.0% high sensitive parameters decreases the ID accuracy by 4.53%. In contrast, pruning 40% low sensitive parameters only reduces ID accuracy by 1.07%. This highlights the effectiveness of our sensitivity estimation method, and also demonstrates the significant parameter redundancy in deep networks.

**Ablation on pruning methods.** In this ablation, we compare the proposed sensitivity guided parameter and neuron pruning method with other pruning method, including: (1) Random parameter pruning (RPP) ; (2) Target parameter pruning (TPP) , which prunes weights with low magnitude; (3) Random neuron pruning (RNP) ; and (4) Target neuron pruning (TNP) , which prunes neurons with low feature norm. For the comparison methods, we try different pruning percentages and report the best results. The ablation results in Table. 4 reveal that: (1) pruning

  
**Method** & RPP & TPP & OPP & RNP & TNP & ONP \\ 
**SUN** & 49.42 & 43.36 & 30.40 & 52.44 & 47.76 & **26.67** \\
**Places** & 54.40 & 51.86 & 40.76 & 53.92 & 52.84 & **32.69** \\   

Table 4: OOD detection performance with different parameter and neuron pruning methods. We use ImageNet-1K as ID data, SUN and Places as OOD data. FPR95 performance in ResNet50 is reported.

Figure 4: Illustration of confidence reliability diagrams. (a) Sample distribution histogram in different confidence bins. (b) Confidence reliability diagrams (CRD) in the original calibrated model. (c) CRD in the model with optimal parameter and neuron pruning.

Figure 3: Effect of varying pruning percentage parameters in ResNet50 model. (a) Effect of varying \(^{w}_{max}\); (b) Effect of varying \(^{w}_{min}\) when set \(^{w}_{max}=0.5\); (c) Effect of varying \(^{o}_{max}\); (d) Effect of varying \(^{o}_{min}\) when set \(^{o}_{max}=30\). All numbers are percentages.

parameters and neurons with low magnitude outperforms random parameter and neuron pruning. (2) The introduced optimal parameter and neuron pruning outperforms other pruning methods by a margin margin, with 16.68% improvement in SUN dataset and 18.95% in Places dataset.

**OPNP benefits model calibration.** A well calibrated model should have better OOD detection performance . In this ablation, we explore how OPNP influences model calibration. In Fig. 4, we evaluate model calibration performance with Confidence Reliability Diagrams (CRD) and Expected Calibration Error (ECE), which was introduced in . As observed in Fig. 4(b), for an uncalibrated model, the confidence obviously exceeds accuracy, which indicates overconfident confidence. Fig. 4(c) illustrates the effect of utilizing OPNP, which shows that the consistency between confidence and accuracy is improved, and the ECE is reduced by 1.5%. It demonstrates that OPNP is beneficial to model calibration, which also explains why OPNP improves OOD detection performance.

**How OPNP changes the score distribution** In Fig. 5, we illustrate the OOD score distribution with the Energy baseline  and our proposed OPNP. We utilize SUN and Places benchmarks with ResNet50 model to exhibit how the OPNP changes the OOD score distributions. From the illustration, several interesting observations are: (1) Utilizing OPNP increases OOD scores for both ID and OOD samples. (2) The utilization of OPNP has a significant impact on the score distribution of OOD samples, resulting in a more condensed distribution. (3) The OOD score distributions of ID and OOD samples become more separable after applying OPNP, which validates the effectiveness of optimal parameter and neuron pruning.

## 5 Conclusion

**Conclusion and future work.** In this paper, we propose a simple yet effective post-hoc OOD detection method. In particular, a gradient-based method is proposed to estimate the sensitivity of model parameters and neurons. We show that the OOD detection performance could be significantly improved by simply removing the connection weights and neurons with exceptionally large or close to zero sensitivities. Extensive experiments and ablations are performed to demonstrate the effectiveness of our proposal. Compared to energy score baseline, our OPNP reduces FPR95 by 32.5% in ImageNet-1K benchmark. Besides, the OPNP outperforms the SOTA feature rectification method by 5.5% in FPR95 and outperforms the SOTA weight pruning method by 8.8% in FPR95. We believe the optimal parameter and neuron pruning is a promising direction for OOD detection tasks, and hope our findings can bring new ideas and breakthroughs to other researchers. In our future work, we will explore other post-hoc model pruning and quantization method, as well as low rank decomposition of model parameters for OOD detection.

**Limitation and societal impact.** The main limitation of this work is lack of theoretical guarantee. Therefore, we call for further application and explanation of the sensitivity guided parameter and neuron pruning method for OOD detection. This work aims to improve the safety of modern deep learning models, which tends to benefit a wide range of applications in social life, such as AI for medical, smart city and driverless system. We hope to provide a plug-and-play tool for AI model users to reduce the false recognition caused by OOD samples in the real world.

Figure 5: Illustration of OOD score distributions in two tasks with the Energy baseline and our proposed OPNP. (a) Energy baseline in SUN benchmark. (b) OPNP in SUN benchmark. (c) Energy baseline in Places benchmark. (d) OPNP in Places benchmark.