# Uncertainty Quantification via Neural Posterior Principal Components

Elias Nehme

Technion - Israel Institute of Technology

seliasne@campus.technion.ac.il

&Omer Yair

Technion - Israel Institute of Technology

omeryair@campus.technion.ac.il

&Tomer Michaeli

Technion - Israel Institute of Technology

tomer.m@ee.technion.ac.il

###### Abstract

Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. Yet, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap around a pre-trained model that was trained to minimize the mean square error (MSE), or can be trained from scratch to output both a predicted image and the posterior PCs. We showcase our method on multiple inverse problems in imaging, including denoising, inpainting, super-resolution, colorization, and biological image-to-image translation. Our method reliably conveys instance-adaptive uncertainty directions, achieving uncertainty quantification comparable with posterior samplers while being orders of magnitude faster. Code and examples are available on our webpage.

## 1 Introduction

Reliable uncertainty quantification is central to making informed decisions when using predictive models. This is especially important in domains with high stakes such as autonomous cars and biological/medical imaging, where based on visual data, the system is asked to provide predictions that could influence human life. In such domains, communicating predictive uncertainty becomes a necessity. In the particular case of image-to-image inverse problems, efficient _visualization_ of predictive uncertainty is required. For example, in biological and medical image-to-image translation , the predicted image as a whole is supposed to inform a scientific discovery or affect the diagnosis of a patient. Hence, an effective form of uncertainty to consider in this case is semantically-coordinated pixel variations that could alter the output image.

Currently, the majority of existing methods handle uncertainty in image-valued inverse problems by factorizing the output posterior into per-pixel marginals, in which case the strong correlations between pixels are completely ignored. This leads to per-pixel uncertainty estimates in the formof variance heatmaps  or confidence intervals , which are of little practical use as they do not describe the _joint_ uncertainty of different pixels. As a result, such models suffer from model misspecification, absorbing inter-pixel covariates into per-pixel marginals, and presenting the user with unnecessarily inflated uncertainty estimates (see Fig. 0(b)). For example, consider a set of pixels along an edge in an image, in the task of image super-resolution. It is clear that for severe upsampling factors, a model would typically be uncertain around object edges. However, since these pixels are all correlated with the same object, the underlying uncertainty is whether the entire edge is shifted _jointly_, and not whether individual pixels can be shifted independently.

Recently, the field of image-to-image regression has seen a surge of probabilistic methods inspired by advancements in deep generative models, such as diffusion models (DMs). The recurring theme in these approaches is the use of a deep generative prior in order to sample from the posterior distribution [8; 19; 50]. In principle, using the resulting samples, it is possible to present the practitioner with the main modes of variations of the posterior uncertainty, for example using principal components analysis (PCA). However, these powerful posterior samplers come with a heavy computational price. Despite tremendous efforts over the past few years [27; 31; 43; 51], sampling from state-of-the-art methods is still unacceptably slow for many practical applications.

In this work, we bypass the need to learn a data-hungry and extremely slow conditional generative model, by directly training a neural model to predict the principal components of the posterior. Our method, which we term _neural posterior principal components_ (NPPC), can wrap around any pre-trained model that was originally trained to minimize the mean square error (MSE), or alternatively can be trained to jointly predict the conditional mean alongside the posterior principal components. In particular, when used as a post-hoc method with MSE pre-trained models, NPPC is a general technique that can be transferred across datasets and model architectures seamlessly. This is because our network architecture inherits the structure and the learning hyper-parameters of the pre-trained model with only one key change: Increasing the filter count at the last layer and employing a Gram-Schmidt process to the output to ensure our found principal components are orthogonal by construction. NPPC is then trained on the pre-trained model residuals, to endow the model's point prediction with efficient input-adaptive uncertainty visualization.

Several prior works proposed modeling the posterior as a correlated Gaussian distribution. In particular, Dorta et al.  approximated the output precision matrix by a matrix of the form \(^{T}\), where \(\) is a lower triangular matrix with a pre-determined sparsity pattern. More recently, Monteiro et al.  and Meng et al.  approximated the covariance matrix by a sum of a diagonal matrix and a low-rank one. However, unlike NPPC, both  and  are limited to very low-resolution

Figure 1: **Comparison between per-pixel methods and NPPC. In structured output posteriors such as in image-to-image regression (left), uncertainty visualization in the form of per-pixel variance maps (top) fail to convey semantic variations in the prediction, leaving the user with no clue regarding the different possibilities of the solution set. Our method - NPPC (bottom) captures a more natural measure of uncertainty, by providing the user with input-adaptive PCs around the mean prediction. These can then be used to navigate the different possibilities in a meaningful manner. _e.g.,_ in 0(a) NPPC is used to traverse the existence of uncertain cells in an image-to-image translation task from biological imaging. Panel 0(b) presents a 2D motivating example illustrating the practical benefit of moving along PCs as opposed to the “standard” coordinate system (_e.g.,_ pixels in an image).**

images, because either they explicitly construct the (huge) covariance matrix during training or they capture only short-range correlations. Moreover,  suffers from training instability as multivariate Gaussians with both unknown mean and covariance are known to be notoriously unstable . This limits its applicability to image segmentation, requiring proper background masking to avoid infinite covariance and overflow errors. Our method, on the other hand, does not construct the covariance matrix at any point. It directly outputs the top eigenvectors of the covariance and trains using the objective of PCA. As a result, our method is generally applicable to any inverse problem, enjoys faster and more stable training, and can handle high-resolution images as we demonstrate in Sec. 4. This is enabled by several design choices introduced to the architecture and training, including a Gram-Schmidt output layer, a PCA loss function maximizing correlation with the residuals, and a Stopgrad trick that enables learning all principal directions jointly, mitigating the need for training separate models for separate PCs.

We compare our PCs to those extracted from samples from a conditional generative model as recently proposed in . As we show, we obtain comparable results, but orders of magnitude faster. Finally, we showcase NPPC on multiple inverse problems in imaging showing promising results across tasks and datasets. In particular, we apply NPPC to scientific biological datasets, showing the practical benefit of our uncertainty estimates in their ability to capture output correlations.

## 2 Related Work

Model uncertaintySome methods attempt to report uncertainty that is due to model misspecification and/or out-of-distribution data. Early work on quantification of such uncertainty in deep models has focused on Bayesian modeling by imposing distributions on model weights/feature activations [6; 17; 34]. These methods employ various techniques for approximating the posterior of the weights, including using MCMC , variational inference [6; 25], Monte-Carlo Dropout , and Laplace approximation . Our work rather focuses on _data_ uncertainty as described in Section 3.

Per-pixel methodsSome methods for predicting per-pixel variances assume a Gaussian distribution and learn it by maximizing the log-likelihood . This approach was later combined with hierarchical priors on likelihood parameters to infer a family of distributions in a single deterministic neural network [1; 28]. Similarly, multi-modal approximations assuming a Gaussian Mixture Model per pixel have been proposed . Deep Ensembles  and Test-time augmentations  have also been used to estimate uncertainty by measuring the variance of model predictions. More recently, distribution-free methods such as quantile regression and conformal prediction  have taken over their counterparts with firm statistical guarantees. Specifically for image-to-image regression, a work that stands out is that of Angelopoulos et al. . The main shortcoming of these methods is the underlying assumption of _i.i.d._ pixels.

Distribution-free risk controlElaborate distribution-free methods, such as Risk Controlling Prediction Sets (RCPS) , take pixel correlations into account by controlling some risk factorizing all pixels in an image (such as the false discovery rate in binary image segmentation). Recently, this approach has been deployed in image-to-image regression problems using a technique called Conformal Prediction Masks . The main drawback of these methods is their inability to explore the different possible options, but rather, only impose upper and lower bounds on the possible solution set. In addition, they also require an extra data split for calibration, which is not always readily available. An interesting recent work in this field  has proposed to control the risk of disentangled factors in the latent space of StyleGAN, and demonstrated informative uncertainty visualizations in inverse problems of facial images. However, the main drawbacks of this work are the key assumption of access to disentangled latent representations and the generalization gap from fake to real images.

Generative models and posterior samplersGenerative models have been widely used to navigate prediction uncertainty, either in the form of conditional variational autoencoders  and conditional GANs, or more recently using state-of-the-art score-based and denoising diffusion models [8; 12; 14; 19; 26; 41; 42; 48; 49; 54]. While the latter have achieved astounding results in the last two years, when aiming for high-quality samples, they remain extremely slow to sample from, despite promising results reported in recent efforts [27; 31; 43; 51]. The recent Confusion method  finetunes pre-trained diffusion models to output interval bounds in a single forward pass to achieve fast confidence interval prediction for posterior samplers. Nonetheless, the result is a per-pixel uncertainty map.

Gaussian covariance approximationWorks that are particularly related to the approach presented here are ,  and . Specifically, Dorta et al.  proposed to model the output posterior with a correlated Gaussian, and approximated the precision matrix using the factorization \(=^{}\), where \(\) is a lower triangular matrix with predefined sparsity patterns capturing short-term correlations. The matrix \(\) is then learned during training using a maximum likelihood objective. Similarly,  and  suggested approximating the posterior covariance matrix using a low-rank factorization with the latter involving second-order scores. However, the main drawback of  and  is their limited ability to generalize to high-resolution RGB images while efficiently capturing long-range correlations . Additionally,  is limited by training instability, limiting its applicability to image segmentation. In contrast, our method directly estimates eigenvectors of the covariance matrix without the need for explicitly storing the covariance matrix in memory during training as in . This enables us to seamlessly generalize to arbitrary inverse problems and high-resolution RGB images with more stable and less expensive training steps.

Concurrent workA related approach was recently proposed in the concurrent work of , where the authors demonstrate a training-free method to calculate the posterior PCs in the task of Gaussian denoising. The key advantage of our approach over this work is that we are not constrained to the task of Gaussian denoising and can handle arbitrary inverse problems, as we show in our experiments.

## 3 Neural Posterior Principal Components

We address the problem of predicting a signal \(^{d_{x}}\) based on measurements \(^{d_{y}}\). In the context of imaging, \(\) often represents a degraded version of \(\) (_e.g.,_ noisy, blurry), or a measurement of the same specimen/scene acquired by a different modality. We assume that \(\) and \(\) are realizations of random vectors \(\) and \(\) with an unknown joint distribution \(p(,)\), and that we have a training set \(=\{(_{i},_{i})\}_{i=1}^{N_{d}}\) of matched input-output pairs independently sampled from that distribution.

Many image restoration methods output a single prediction \(}\) for any given input \(\). A common choice is to aim for the posterior mean \(}=[|=]\), which is the predictor that minimizes the MSE. However, a single prediction does not convey to the user the uncertainty in the restoration. To achieve this goal, here we propose to also output the top \(K\) principal components of the posterior \(p(|)\), _i.e.,_ the top \(K\) eigenvectors of the posterior covariance \([(-})(-})^{} |=]\). The PCs capture the main directions along which \(\) could vary, given the input \(\), and thus provide the user with valuable information. However, their direct computation is computationally infeasible in high-dimensional settings. The main challenge we face is, therefore, how to obtain the posterior PCs without having to ever store or even compute the entire posterior covariance matrix. Before we describe our method, let us recall the properties of PCA in the standard (unconditional) setting.

Unconditional PCAGiven a set of \(N\) data points \(\{_{i}\}\) where \(_{i}^{d_{x}}\), the goal in PCA is to find a set of \(K\) orthogonal principal directions \(_{1},,_{K}\) along which the data varies the most. The resulting directions are ordered such that the variance of \(\{_{1}^{}_{i}\}\) is biggest, the variance of

Figure 2: **Method overview**. Here, NPPC is demonstrated for image inpainting, where \(\) is the ground truth image, \(\) is the masked input, and \(}\) is the posterior mean prediction. Our method can wrap around pre-trained conditional mean predictors \(f(;)\), replicating their architecture with a slight modification at the output (see text). Using a PCA loss on the errors (\(=-}\)), we learn to predict the first \(K\) PCs of the posterior \(_{1},,_{K}\) directly in a single forward pass of a neural network \((,};)\). On the right, we visualize the uncertainty captured by two PCs (\(_{1},_{4}\)) around the mean prediction \(}\).

\(\{_{2}^{}_{i}\}\) is the second biggest, etc. The objective function for finding these directions has multiple equivalent forms and interpretations. The one we exploit here is the iterative maximization of variance. Specifically, let \(\) denote the centered data matrix whose rows consist of different observations \(_{i}^{}\) after subtracting their column-wise mean, and let \(_{1},,_{K}\) denote the first \(K\) PCs. Then the \(k^{}\) PC is given by

\[_{k}=*{arg\,max}_{}\|\|_{2}^{2}, \|\|=1,\ \{_{1},,_{k-1}\},\] (1)

where for the first PC, the constraint is only \(\|\|=1\). The variance along the \(k^{}\) PC is given by \(_{k}^{2}=\|_{k}\|_{2}^{2}\).

The challenge in posterior PCAGoing back to our goal of computing the posterior PCs, let us assume for simplicity we are given a pre-trained conditional mean predictor \(}=f(;)\) obtained through MSE minimization on the training set \(\). Let \(_{i}=_{i}-}_{i}\) denote the error of the conditional mean predictor given the \(i^{}\) measurement, \(=_{i}\). The uncertainty directions we wish to capture per sample \(_{i}\), are the PCs of the error \(_{i}\) around the conditional mean estimate \(}_{i}\). Conceptually, this task is extremely difficult as during training for every measurement \(_{i}\), we have access only to a _single_ error sample \(_{i}\). If we were to estimate the PCs directly, the result would be a trivial single principal direction equaling \(_{i}\) which is unpredictable at test time. To address this challenge, here we propose to harness the implicit bias of neural models and to learn these directions from a dataset of triplets \(^{}=\{(_{i},_{i},}_{i})\}_{i=1}^{N_{d}}\). The key implicit assumption underlying our approach (and empirical risk minimization in general) is that the posterior mean \([|=]\) and the posterior covariance \([(-})(-})^{} |=]\) vary smoothly with \(\). Hence, with the right architecture, such models can capitalize on inter-sample dependencies and properly generalize to unseen test points, by learning posterior PCs that change gracefully as a function of \(\). This is much like models trained with MSE minimization to estimate the conditional mean \([|=]\), while being presented during training only with a single output \(_{i}\) for every measurement \(_{i}\).

### Naive solution: Iterative learning of PCs

Following the intuition from the previous section, we can parameterize the \(k^{}\) PC of the error using a neural network \(_{k}(,};_{k})\) with parameters \(_{k}\), which has similar capacity to the pre-trained model \(f(;)\) outputting the conditional mean. This model accepts the measurement \(\) and (optionally) the conditional mean estimate \(}\), and outputs the \(k^{}\) PC of the error \(\).

Let \(_{1}(,};_{1})\) be a model for predicting the first unnormalized direction, such that \(_{1}(_{i},}_{i};_{1})=_{1}(_{i},}_{i};_{1})/\|_{1}(_{i},}_{i}; _{1})\|\). Given a dataset of triplets \(^{}=\{(_{i},_{i},}_{i})\}\), we adopt the objective employed in (1), and propose to learn the parameters of the input-dependent first PC by minimizing

\[_{_{1}}(^{},_{1})=- _{(_{i},_{i},}_{i}^{})}|_ {1}(_{i},}_{i};_{1})^{}_{i}| ^{2}.\] (2)

Next, given the model predicting the first PC, we can train a model to predict the second PC, \(_{2}(_{i},}_{i};_{2})\), by manipulating the output of a model \(_{2}(_{i},}_{i};_{2})\). Specifically, following the approach in (1), we optimize the same loss (2), but construct the output of the model \(_{2}\) by removing the projection of \(_{2}\) onto \(_{1}\), and normalizing the result. This ensures that \(_{2}(_{i},}_{i};_{2})_{1}( _{i},}_{i};_{1})\).

While in principle this approach can be iterated \(K\) times to learn the first \(K\) PCs, it has several drawbacks that make it impractical. First, it requires a prolonged iterative training of \(K\) neural networks sequentially, preventing parallelization and leading to very long training times. Second, this approach is also inefficient at test time, as we need to compute \(K\) dependent forward passes. Finally, in this current formulation, different PCs have their own set of weights \(\{_{k}\}_{k=1}^{K}\), and do not share parameters. This is inefficient as for a given input \(\) and corresponding mean prediction \(}\), it is expected that the initial feature extraction stage for predicting the different PCs, would be similar. A better strategy is therefore to design an architecture that outputs all PCs at once.

### Joint learning of PCs

To jointly learn the first \(K\) PCs of the error using a single neural network \((_{i},}_{i};)\), we introduce two key changes to the architecture inherited from the pre-trained mean estimator \(f(;)\). First,the number of filters at the output layer is multiplied by \(K\), to accomodate the \(K\) PCs, \(_{1},,_{K}\). Second, we introduce a Gram-Schmidt procedure at the output layer, making the directions satisfy the orthogonality constraint by construction. Formally, denote the non-orthonormal predicted set of directions by \(_{1},,_{K}\). Then, we transform them to be an orthonormal set \(_{1},,_{K}\) as

\[_{1} =_{1}}{\|_{1}\|},\] \[_{k} =_{k}-_{=1}^{k-1}(_{k}^{}_{ })_{}}{\|_{k}-_{=1}^{k-1}(_{k}^{}_ {})_{}\|}, k=2, K.\] (3)

Note, however, that these changes do not yet guarantee proper learning of the PCs. Indeed, if we were to learn the directions by naively minimizing the loss

\[_{}(^{},)=-_{ (_{i},_{i},}_{i}^{})} _{k=1}^{K}|_{k}(_{i},}_{i};) ^{}_{i}|^{2},\] (4)

then we would only recover them up to an orthogonal matrix. To see this, let \(_{i}\) denote the matrix that has \(_{k}(_{i},}_{i};)\) in its \(k^{}\) column. Then the inner sum in (4) can be rewritten as \(\|_{i}^{}_{i}\|_{2}^{2}\). Now, it is easy to see that neither the loss nor the constraints are affected by replacing each \(_{i}\) with \(}_{i}=_{i}_{i}\), for some orthogonal matrix \(_{i}\). Indeed, this solution satisfies the orthogonality constraint \(_{i}^{}}_{i}=\), and attains the same loss value as the original PCs.

Note that this rotation ambiguity did not exist in the naive approach of Sec. 3.1 because there, when finding the \(k^{}\) PC given the preceding \(k-1\) pre-trained PCs, the loss term \(|_{k}(_{i},}_{i};)^{}_{i}|^{2}\) did not affect the learning of \(_{1},,_{k-1}\). However, when attempting to learn all directions jointly, the preceding PCs receive a gradient signal from the \(k^{}\) loss term as \(_{k}\) is a function of \(_{1},,_{k-1}\) in the Gram-Schmidt procedure.

To solve this problem and decouple the learning of the different PCs while still maintaining their orthogonality, we propose a simple modification to the Gram-Schmidt procedure: Using Stopgrad for the previously derived PCs within the projection operators in (3) (see the red term in Fig. 2). This way, in each learning step the different PCs are guaranteed to be orthogonal, while solely optimizing their respective objective. This allows learning them jointly and recovering the solution of the iterative scheme in a single forward pass of a neural network with shared parameters. Please see App. D.2 for validation of the equivalence between sequential and joint PC learning.

### Learning variances along PCs

Recall that the variance along the \(k^{}\) direction corresponds to the average squared projection of the data along that direction. We can use that to output a prediction of the variances \(\{_{k}^{2}\}\) of the PCs, by using a loss that minimizes \(_{i}(_{k}^{2}-|_{k}^{}_{i}|^{2})^{2}\) for every \(k\). However, instead of adding \(K\) additional outputs to the architecture, we can encode the variances in the norms of the unnormalized directions. To achieve this without altering the optimization objective for finding the \(k^{}\) PC, we again invoke the Stopgrad operator on the term \(|_{k}^{}_{i}|\) which is the current loss function value, and match the norm of the found PCs at the current step to this projected variance by minimizing the loss

\[_{}(^{},)=_{( {x}_{i},_{i},}_{i}^{})}_{k=1}^{K} (\|_{k}-_{=1}^{k-1}(_{k}^{}_{}) {w}_{}\|_{2}^{2}-|_{k}^{}_{i}|^{2})^{2}.\] (5)

With this, our prediction for \(_{k}^{2}\) at test-time is simply \(\|_{k}-_{=1}^{k-1}(_{k}^{}_{})_{}\| ^{2}\). Please see App. D.3 for quantitative validation of our estimated variances.

### Joint prediction of posterior mean

Thus far, we assumed we have access to a pre-trained posterior mean predictor \(}=f(;)\), obtained through MSE minimization on a dataset \(=\{(_{i},_{i})\}_{i=1}^{N_{d}}\) of matched pairs. Hence, our entire derivation revolved around a dataset of triplets \(^{}=\{(_{i},_{i},}_{i})\}_{i=1}^{N_{d}}\). However, this is not strictly necessary. In particular, the posterior mean predictor can be learned jointly alongside the PCs with an 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

applied in several contexts, including for predicting fluorescent labels from bright-field images  and for predicting one fluorescent label (_e.g.,_ nuclear stainings) from another (_e.g.,_ actin stainings) . However, unlike its use for artistic purposes, the use of image-to-image translation in biological imaging requires caution. Specifically, without proper uncertainty quantification, predicting cell nuclei from other fluorescent labels could lead to biased conclusions, as cell counting and tracking play central roles in microscopy-based scientific experiments (_e.g.,_ drug testing). Here, we applied NPPC to a dataset of migrating cells imaged live for 14h (1 picture every 10min) using a spinning-disk microscope . The dataset consisted of 1753 image pairs of resolution \(1024 1024\), out of which 1748 were used for training, and 5 were used for testing following the original split by the authors. We started by training a standard U-Net [11; 40] model using MSE minimization to predict nuclear stainings from actin stainings (see Fig. 6), and then trained NPPC on the residuals. As we show in Fig. 6, the PCs learned by NPPC convey important information to experimenters. For example, the first PC highlights the fact that accurate intensity estimation is not possible in this task, and thereby a global bias is a valid uncertainty component. Furthermore, the remaining PCs reflect semantic uncertainty by adding/removing cells from the conditional mean estimate \(}\), thereby clearly communicating reconstruction ambiguity to the user.

## 5 Discussion

We proposed an approach for directly predicting posterior PCs and showed its applicability across multiple tasks and datasets. Nonetheless, our method does not come without limitations. First, as evident by both the PCs of NPPC and of posterior samplers, for severely ill-posed inverse problems a linear subspace with a small number of PCs captures very little of the error. Hence, a large number of PCs is required to faithfully reconstruct the error. However, the main premise of this work was scientific imaging where scientists usually take reliable measurements, and the uncertainty is not as severe as ultimately the result should drive scientific discovery. Second, throughout this paper, we

    &  &  \\   & \(4\) noiseless & \(4\) noisy & \(8\) noiseless & \(8\) noisy & Eyes & Mouth & NFEs\(\) \\ DDRM  & 8.94/8.89 & 11.29/11.25 & 13.68/13.47 & 16.20/16.00 & -/- & -/- & \(2 10^{3}\) \\ DDNM  & 8.43/8.38 & 10.74/10.70 & 13.12/12.95 & 15.73/15.55 & 13.27/11.24 & 13.30/10.37 & 10\( 10^{3}\) \\ RePaint  & -/- & -/- & -/- & **12.24/10.3** & 12.55/**97.2** & 457\( 10^{3}\) \\ MAT  & -/- & -/- & -/- & -/- & 14.12/12.94 & 13.03/11.74 & 100 \\ NPPC (Ours) & **8.4/8.24** & **10.41/10.35** & **13.06/12.87** & **15.29/15.11** & 13.55/11.43 & **12.23**/10.27 & **1** \\   

Table 2: Quantitative comparison of \(\|-}\|_{2}/\|-^{}\|_{2}\) with posterior samplers on 100 test images from FFHQ. Mean prediction and PCs were computed using 100 samples per test image, and compute is reported in neural function evaluations (NFEs).

Figure 5: **CelebA-HQ inpainting and \(8\) noisy super-resolution**. (a) Application of NPPC to image inpainting with a mask around the mouth. NPPC received the masked image \(\) and the mean prediction \(}\), and predicted the first 5 posterior PCs \(_{1},,_{5}\). On the bottom, we show a traversal of 2 standard deviations around the mean estimate \(}\) for \(_{2}\) and \(_{3}\), capturing uncertainty regarding open/closed mouth, and background vs shadow next to the jawline. (b) Application of NPPC to \(8\) noisy super-resolution with a noise standard deviation \(_{e}=0.05\). Similarly, the PC traversals on the bottom capture uncertainty of eye size and cheek color/position.

learned the PCs on the same training set of the conditional mean estimator. However, generalization error between training and validation could lead to biased PCs estimation as the learning process is usually stopped after some amount of overfitting (_e.g.,_ until the validation loss stagnates). This can be solved by using an additional data split for "calibration" as is typically the case with distribution-free methods . Third, different inputs \(_{i}\) may have posteriors of different complexities, and may thus require a different number of PCs \(K\). In our approach, \(K\) is hard-coded within the network's architecture (it is the number of network outputs). Namely, we treat \(K\) as a hyper-parameter that needs to be set in advance prior to training, acting as an upper bound on the number of directions the user may be interested in exploring at test time. However, recall that NPPC also predicts the standard deviations along each of the \(K\) PCs. These may serve to decide whether to ignore some of the PCs. Fourth, our learned PCs cover the entire image, whereas in some cases the interesting uncertainty structure could be local (_e.g.,_ cell data). In such circumstances, NPPC should either be applied patch-wise or the number of PCs \(K\) should be sufficiently increased. Finally, our method is tailored towards capturing uncertainty and not sampling from the posterior. While we provide the directed standard deviations, shifting along a certain direction does not guarantee samples along the way to be on the image data manifold. This can be tackled by employing NPPC in the latent space of a powerful encoder, in which case small enough steps could lead to changes on the manifold in the output image. However, this is beyond the scope of this current work.

## 6 Conclusion

To conclude, in this work we proposed a technique for directly estimating the principal components of the posterior distribution using a single forward pass in a neural network. We discussed key design choices, including a Gram-Schmidt procedure with a Stopgrad operator, and a principled PCA loss function. Through extensive experiments, we validated NPPC across tasks and domains, showing its wide applicability. We showed that NPPC achieves comparable uncertainty quantification to the naive approach of applying PCA on samples generated by posterior samplers while being orders of magnitude faster. Finally, we applied NPPC to the challenging task of biological image-to-image translation, demonstrating its practical benefit for safety-critical applications. In terms of broader impact, proper uncertainty quantification is crucial for trustworthy interpretable systems, particularly in healthcare applications. Thus, a method for reporting and conveniently visualizing prediction uncertainty could support users and help them avoid making flawed decisions.

Figure 6: **Biological image-to-image translation**. NPPC applied to the task of translating one fluorescent label \(\) (actin staining) to another fluorescent label \(\) (nuclear staining) in migrating cells (left). On the right, we show the predicted PCs at the top, and traversals along the rows. Yellow arrows point to captured uncertain cells in \(}\) (either deforming or being completely erased).