# Describe, Explain, Plan and Select:

Interactive Planning with Large Language Models

Enables Open-World Multi-Task Agents

 Zihao Wang\({}^{1,2}\), Shaofei Cai\({}^{1,2}\), Guanzhou Chen\({}^{3}\), Anji Liu\({}^{4}\), Xiaojian Ma\({}^{4}\), Yitao Liang\({}^{1,5}\)

\({}^{1}\)Institute for Artificial Intelligence, Peking University

\({}^{2}\)School of Intelligence Science and Technology, Peking University

\({}^{3}\)School of Computer Science, Beijing University of Posts and Telecommunications

\({}^{4}\)Computer Science Department, University of California, Los Angeles

\({}^{5}\)Beijing Institute for General Artificial Intelligence (BIGAI)

{zhwang,caishaofei}@stu.pku.edu.cn,rayment@bupt.edu.cn

liuanji@cs.ucla.edu,xiaojian.ma@ucla.edu,yitaol@pku.edu.cn

###### Abstract

We investigate the challenge of task planning for multi-task embodied agents in open-world environments.2 Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "Describe, Explain, Plan and Select" (**DEPS**), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated _plan_ by integrating _description_ of the plan execution process and providing self-_explanation_ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal _selector_, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the ObtainDiamond grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.

## 1 Introduction

Developing multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains has been viewed as one of the key milestones towards generally capable artificial intelligence [36; 1; 5; 10; 25]. To enable such capabilities, earlier works have suggested employing a hierarchical goal execution architecture [2; 4], where a planner generates action plans that would then be executed by low-level goal-conditioned controllers. This architecture has been delivering promising progress inmany robotics domains, including table-top and mobile manipulation [46; 4], 2D shape drawing  and table rearrangement . However, whether such success can be transferred to a more open-ended world with unlimited exploration areas and internet-scale knowledge remains open [14; 10; 13; 12; 19].

To understand the gap, we run Inner Monologue , a general and competitive hierarchical goal execution model on a typical open-world domain Minecraft [18; 14; 10] and two classical robotic environments ALFWorld  and Tabletop environments [40; 4]. The algorithm uses a Large Language Model (LLM) based planner that contains domain-specific knowledge for all three environments. In all environments, we use either an Oracle goal-conditioned controller or a learned one. Results are shown in the bar plot in Figure 1. First, even when the Oracle controller is used, the success rate of executing Minecraft tasks is much less than that of the other environments. Next, the task failure rate becomes even higher in Minecraft when the learned controller is substituted. Both failures originate from unique challenges brought by open-world environments, which we identify in the following.

First, compared to canonical environments (e.g., Atari  and robotic control suite ), open worlds have highly abundant object types with complex dependency and relation. As a result, ground-truth plans typically involve a long sequence of sub-goals with strict dependencies. As Figure 1 challenge #1 suggests, it requires at least 13 sub-goals executed in proper order to obtain a diamond in Minecraft, while in Tabletop a task is typically no more than a few consecutive sub-goals.

Another challenge brought by the complicated tasks in an open-ended world is the feasibility of the produced plans. Consider the example shown in Figure 1 (challenge #2). To craft a bed in Minecraft, the fastest way is by either slaughtering a sheep to obtain wool, which can be used to craft beds, or collecting beds from a village. However, since no sheep or village is reachable by the agent within 3 minutes of gameplay, to craft a bed efficiently, the agent should choose to slaughter a spider and use materials (e.g., string) it drops to craft wool, and then a bed. That is, when dealing with a task that can be completed by executing multiple possible sequences of sub-goals, the planner should be able to select the best route based on the current state of the agent. However, the complex and diverse state distribution of open-world environments makes state awareness hard to achieve.

To tackle these problems, we propose "Describe, Explain, Plan and Select" (**DEPS**), an interactive planning approach based on Large Language Models (LLMs) to alleviate the aforementioned issues. The key to tackling the first challenge is to effectively adjust the generated plan upon failure. Specifically, whenever the controller fails to complete a sub-goal, a _descriptor_ will summarize the current situation as text and send it back to the LLM-based planner. We then prompt the LLM as an _explainer_ to locate the errors in the previous plan. Finally, a _planner_ will refine the plan using information from the descriptor and explainer. To improve the feasibility of generated plans conditioned on the current state, which is the second identified challenge, we use a learned goal-_selector_ to choose the most accessible sub-task based on the proximity to each candidate sub-goal.

Our experiments are conducted on 71 tasks in open-ended Minecraft without any demonstration. Given the goal-conditioned controller for atom sub-tasks (i.e., mine log and mine stone), our zero

Figure 1: **Planning success rates plummet in open worlds due to new challenges**.

shot3 LLM-based planner can finish all tasks within a limited horizon (3000-12000 steps for different tasks). We find DEPS outperforms all language planner baselines by nearly doubling the overall success rate, with the same initial state and goal-conditioned controller. Our ablation and exploratory studies then explain how our approach beats the counterparts and becomes the first planning-based agent that accomplishes the challenging ObtainDiamond task. DEPS does not require any planning training for the environment. Additionally, DEPS achieves between on-par and more than 50% relative improvement over existing or concurrent LLM-based planning methods on non-open-ended robotics domains such as ALFWorld  and Tabletop environments .

## 2 Background

We aim to develop an agent capable of solving long-horizon goal-reaching tasks using image observations and language goals. To accomplish this, we propose a combined approach involving goal-conditioned policies (termed controllers) and a planner. The goal-conditioned policies are trained to complete sub-goals, while the planner decomposes long-horizon tasks into a series of \(K\) short-horizon sub-goals, \(g_{1},,g_{K}\), to be executed by the controller. At each time step \(t\), the goal-conditioned policy \((a_{t} s_{t},g_{k})\) generates an action \(a_{t}\) based on the current state \(s_{t}\) and the specified sub-goal \(g_{k}\).

Planning with Large Language ModelsPrevious works have shown that LLMs such as Instruct-GPT  and Codex  can be used as zero-shot planners to generate sub-goal sequences for various tasks in embodied environments [16; 42]. Formally, given the task description \(T\) as prompt \(p\), LLM acts as a planner to decode \(T\) into \(K\) sub-goals, \(g_{1},,g_{K}\), which are then executed one by one by the low-level controller \((a_{t} s_{t},g_{k})\) to accomplish the task.

However, the above pipeline suffers from both challenges identified in Section 1. Regarding the first challenge, the probability of generating a flawless plan directly from the task description decreases significantly as the required number of sub-goals increases. Moreover, even when the LLM generates a correct plan, it is very likely that the plan is highly inefficient given the agent's current state (challenge #2). Prior works mostly focus on solving the first challenge by providing environmental feedback to the LLM through affordance functions , success detector  or scene descriptor . However, although these approaches work well on many non-open-ended domains, they still suffer from high failure rates in open-world environments.

Figure 2: **Overview of our proposed interactive planner architecture.**

Towards Reliable Planning in Embodied Open-World Environments

In this section, we first give an overview of our proposed interactive planning framework "Descibe, Explain, Plan, and Select" (DEPS) for solving complex and long-horizon tasks in open-world environments (Sec. 3.1). Next, in Section 3.2, we elaborate how DEPS iteratively refines its plan to combat the first identified challenge. Section 3.3 introduces the _selector_ module that is used to identify efficient plans in response to the second identified challenge.

### DEPS Overview

As demonstrated in Figure 2, our agent (DEPS) consists of an event-triggered Descriptor, a Large Language Model (LLM) as Explainer and Planner, a goal Selector based on horizon prediction and a goal-conditioned controller. In the following, we use Minecraft as a running example to better elaborate our agent. Note that DEPS can be directly applied to other (non-)open-ended tasks.

We take a large language model (LLM) as a zero-shot _planner_ of the agent to complete tasks. Given a goal command (e.g., ObtainDiamond) as task \(T\), the LLM-based planner decomposes this high-level task into a sequence of sub-goals \(\{g_{1},,g_{K}\}\), as the initial plan \(P_{0}\). The goals are instructions in natural language, such as mine oak wood (in Minecraft), find two cups (in ALFWorld), put block A on top of block B (in Tabletop Manipulation).

As described in Section 2, a controller is then invoked to execute the provided sub-goals sequentially through a goal-conditioned policy \((a s,g)\). However, the initial plan provided by the planner often contains errors, which results in execution failures of the controller. For example, the goal () can not be finished only with a wooden pickaxe () as shown in Figure 2. When failure pops up, the _descriptor_ will summarize the current state \(s_{t}\) and execution outcome of the most recent goal into text \(d_{t}\) and send it to the LLM. The LLM will first try to locate the errors in the previous plan \(P_{t-1}\) by _self-explanation_, e.g., the goal () need to be executed with a stone pickaxe (). Then it will re-plan the current task \(T\) and generate a revised plan \(P_{t}\) according to the explanation. In this process, the LLM is also treated as an _explainer_ in addition to the _planner_ role. The Descriptor, Explainer, and Planner will be detailed in Section 3.2.

\[:& d_{t}=f_{}(s_{t-1}),\\ :& e_{t}=f_{}(d_{t}),\\ :& p_{t}=(p_{t-1},d_{t},e_{t}),\\ :& P_{t}=f_{}(p_{t}),\\ :& g_{t} f_{}(_{t},s_ {t-1}),\\ :& a_{t}(a_{t} s_{t-1},g_{t}) \] (1)

As shown in Equation (1), DEPS will iteratively update the plan \(P_{t}\) until the task is finished, where \(f_{}\) is the descriptor model, \(f_{}\) denotes the language model as explainer and planner, \(f_{}\) is the selector model, \(\) is goal-conditioned policies from the controller.

To filter out inefficient plans, the _selector_ is trained to predict the number of time steps remaining to achieve every goal \(g_{k}\) in a set of parallel goals given the current state \(s_{t}\). When the generated plan contains alternative routes, the selector uses this information to choose a suitable goal as the current goal \(g_{t}\). For example, the horizon predicted by the selector of goal acacia tree () is less than goal oak tree () in Savanna biome, which leads to chop acacia tree as current goal \(g_{t}\).

### Describe, Explain and Plan with LLM Generates Executable Plans

Current LLM-based planners usually query the LLM once at the beginning of every episode and use the output plan throughout the episode [16; 42]. However, as demonstrated by Figure 1, such one-shot planning methods often fail on long-horizon tasks that require many sub-goals. This is caused by two major issues. First, since the correct plan for long-horizon tasks needs to respect various complex preconditions, it is extremely hard for the LLM to generate a flawless plan directly from the task instructions, resulting in failure when simply following the initial plan. Additionally, due to the unpredictable transition dynamics, some incidents may happen during the execution and make the initial plan non-executable. To remedy these problems, existing methods introduce feedback (e.g.,from success detector or scene descriptor) to reflect on the results of previous executions [17; 20; 4]. However, merely informing the LLM whether a sub-goal is completed is often insufficient to correct the planning error.

To remedy this, we propose "describe, explain and plan", a new interactive planning method to generate more executable and explainable plans. We start with rewriting the prompt into an interactive dialogue format as in ChatGPT  so that subsequent feedback can be passed to the LLM effectively. The produced plan is also augmented with the preconditions and effects of each goal. The structured prompt improves the readability and interpretability of the plan and facilitates error-locating when the execution fails later, as demonstrated in Prompt 1.

The _descriptor_ will then collect the feedback generated by the agent during the execution of the task. The feedback can be practically obtained either by a person (human feedback ), or by a pre-trained vision-language model CLIP . While the previous type of feedback needs intensive human involvement, the latter from the pre-trained model needs to be fine-tuned for the specific domain, which decreases the automation and generalization of the agent. On the contrary, Minecraft returns the 'info' and other high-level observations (such as biome, GPS, and compass), we can easily translate the unstructured information into structured language. Therefore we take the symbolic information available in the game and translate it into feedback description \(d_{t}\) in this work. To avoid carrying unrelated information in the prompt, we further distill plan-related messages (e.g., inventory information, biome) as final event-level description \(d_{t}\) as demonstrated in Figure 2.

Notably, we also treat the LLM as an _explainer_ to explain why the previous plans \(P_{t-1}\) failed. Specifically, by analyzing the current state from description \(d_{t}\) and precondition of current goal \(g_{t}\), the explainer can identify the reason why the current goal cannot be executed successfully. As shown in Figure 2, the reason may be _the current goal requires the use of an iron pickcase, but the tool is not prepared in advance, or the current goal requires the use of 3 planks, but the currently available planks are not enough._ To implement this, we provide few-shot demonstrations to the LLM as in chain-of-thoughts prompting , as shown in Prompt 1. Finally, the LLM goes back to its role as a _planner_ and re-plans the task with the explicit explanation of existing bugs in the previous plan \(P_{t-1}\), ultimately generating an updated plan \(P_{t}\) according to the explanation.

### Horizon-Predictive Selector Yields Efficient Plans

Due to the abundance of objects and the compositional nature of their functionalities, there often exist multiple feasible plans to complete a task, i.e., there are usually multiple paths for the completion of a particular goal. However, despite the feasibility of all such plans, most of them are highly inefficient to execute in the current episode. For example, as shown in Figure 2, obtaining a wood can be done by chopping oak trees , birch trees , or acacia trees . But only oak trees are available in the plains biome. So the planner needs to choose oak trees since it is more efficient, as the agent does not need to travel to another biome.

On the other hand, there is no strict sequential requirement for some goals in the plan \(P_{t}\), i.e., \(g_{i},g_{j} P_{t}\) enjoy the same precondition, which means \(g_{i}\) and \(g_{j}\) can be executed in any order. As shown in Figure 1, the choice of different paths (sequences) may affect the execution efficiency of the plan \(P_{t}\) as one goal might be closer to the agent. Always choosing the closer goal to execute first could yield more efficient plans and improve the final success rate under a limited episode length. Moreover, the dynamic nature of open-world environments further amplifies the impact of efficient plans on the success rate. For example, in Minecraft, if the agent chooses to execute a further goal like collect wood first, the much closer target sheep may disappear and be hard to find again.

In order to improve the efficiency of our plans, we propose to use a _selector_ that selects the most efficient path with the highest execution success rate as the final plan. Specifically, we design a state-aware selector to choose the nearest goal under state \(s_{t}\) as the current goal \(g_{t}\) from the candidate goal sets in plan \(P_{t}\). It predicts the goal distribution \(p(g_{t}|s_{t},P_{t})\) under the current state \(s_{t}\) and plan \(P_{t}\), where \(g_{t} G_{t}\), \(G_{t}\) describes all current executable goals in \(P_{t}\). A straight way to implement the selector is to leverage the semantic similarity between the current state and the goal text using a vision-language model (VLM) such as CLIP . Nevertheless, this may not exactly reflect the difficulty of completing the goal since VLM lacks practical experience. For example, an "oak tree" in front of the agent could lead to high semantic similarity for the "chopping tree" goal, but it may be inefficient to achieve this goal if a canyon is in the middle between the agent and the oak tree.

To mitigate this, we implement a horizon-predictive selector that embeds practical task experience to accurately rank the goals based on their efficiency and feasibility. Here, we define the horizon of a goal \(h_{t}(g):=T_{q}-t\) as the remaining time steps to complete the given goal, where \(T_{q}\) is the time of completing goal \(g\). This metric accurately reflects how quickly we can achieve the given goal from the current state. To estimate the horizon, we learn a neural network \(\) to fit the offline trajectories by minimizing the entropy loss \(-\ (h_{t}(g) s_{t},g)\), where \(h_{t}\) is the ground-truth horizon in trajectories of completing goal \(g\). Therefore, the goal distribution can be formulated as follows:

\[f(g_{t} s_{t},P_{t})=,s_{t}))}{_{g G_{t}}( -(g,s_{t}))}.\] (2)

We set goal-sensitive Impala CNN  as the backbone of the selector. In practice, the horizon predictive selector can be jointly trained with the controller policies and share the backbone parameters .

## 4 Experiments

This section analyzes and evaluates our proposed "describe, explain, plan, and select" (DEPS) method. To minimize performance variation caused by the low-level controller, we standardize all experiments with one controller learned by behavior cloning. We refer to the details of this controller in Appendix C. In Section 4.1, we introduce our testing environments and our evaluation task set, consisting of the hardest 71 tasks from MCU SkillForgeChain . In Section 4.2, we report our performance in the context of existing LLM-based planners. Ablation studies are conducted in Section 4.3. Finally, we pay close attention to the hardest task, ObtainDiamond, which is long-hailed as a major challenge in the community. The experiments on ALFWorld and Tabletop Manipulation environments are shown in Appendix A.

### Experimental Setup

Environment and Task SettingWe first evaluate our proposed method in Minecraft, a popular open-world environment with both challenges discussed in Section 1. For better reflecting the performance of DEPS, we choose three Minecraft environments with different versions for better

Figure 3: **Selection Demonstration from “Selector”. Given parallel sub-goals, i.e. candidate skills, our Selector will determine the sequence in which to carry out these sub-goals based on their current proximity to the agent and modify the original plan produced by the LM planner.**

evaluation, including Minedojo  with Minecraft 1.11.2, MineRL  with Minecraft 1.16.5, and MC-TextWorld  with Minecraft 1.19.2. Rules and items have something different in the above three Minecraft environments, which can better evaluate the dynamic and interactive planning abilities of DEPS.

We choose 71 tasks from the Minecraft Universe Benchmark SkillForgeChain  for evaluation. These tasks are related to items that can be obtained in the Minecraft overworld. To better present the results, we divide the 71 Minecraft tasks into 8 meta groups according to the ingredients and function of the tasks, i.e., MT1-MT8. The instruction for every task is written in natural language, e.g., make a wooden door in MT1 (Basic group) and obtain a diamond in MT8 (Challenge group), as illustrated in Table 1. Considering how long it typically takes human players to complete each task as a ballpark , we set different maximum episode steps for different meta tasks from 3000 (for easiest **Basic** tasks) to 12000 (for the hardest **Challenge** tasks). The names, number of required skills, and functions of all tasks are listed in Appendix B. We give an empty inventory for every task in Survival mode and require the agent to obtain every item from the environment by itself. Note that our agent will be summoned in different environments randomly for each evaluation. Biomes and initial positions are also different each time. Following the previous work , we take the success rate as the evaluation metric.

BaselinesWe compare DEPS with other language-based planners, including GPT as Zero-shot Planner(GPT) , ProgPrompt(PP) , Chain-of-Thought(CoT) , Inner Monologue(IM) , and Code as Policies(CaP) . For all baseline models, we use the same demonstration example in the prompt, the same LM model from OpenAI, and the same controller in all tasks for a fair comparison. Since these methods were not originally experimented with Minecraft, we reproduce them to conform to the Minecraft specification based on prompt and feedback template design. All planner methods access the LLM model through OpenAI API (text-davinci-03 model  for GPT, CoT, and IM, and code-davinci-02 model  for PP, CaP, and Ours). All hyper-parameters of LLM (including the _temperature_ and _best_of_, etc.) are kept as default. We also list the full prompt of all different methods in Appendix G.

### Main Results

Every task is executed 30 times and the average results in Minedo  for every meta task are listed in Table 2. Our approach achieves the best performance with all meta tasks. As the complexity of the task increases from MT1-MT8, the planner usually needs to give more accurate task steps (i.e., longer goal sequence) to achieve the final task. Therefore the success rate of all agents decreases with the reasoning steps increasing. Starting from MT6, almost all existing LLM-based planners fail (nearly 0 success rate). DEP (w/o Selector) already consistently beats existing LLM-based planners in all meta tasks with a significant margin. This validates that "describe, explain and plan" can estimate the reason for current plan failure and correct the original flawed plans. Due to the limited maximum episode length and restricted control success rate for a hard goal (e.g., Mine diamond with iron_pickaxe), the final success rate is still capped.

   Meta & Name & Number & Example Task & Max. Steps & Initial Inventory & Given Tool \\  MT1 & Basic & 14 & Make a wooden door. & 3000 & Empty & Axe \\ MT2 & Tool (Simple) & 12 & Make a stone pickaxe. & 3000 & Empty & Axe \\ MT3 & Hunt and Food & 7 & Cook the beef. & 6000 & Empty & Axe \\ MT4 & Dig-Down & 6 & Mine coal. & 3000 & Empty & Axe \\ MT5 & Equipment & 9 & Equip the leather helmet. & 6000 & Empty & Axe \\ MT6 & Tool (Complex) & 7 & Make shears and bucket. & 6000 & Empty & Axe \\ MT7 & IronStage & 13 & Obtain an iron sword. & 6000 & Empty & Axe \\ MT8 & Challenge & 1 & Obtain a diamond! & 12000 & Empty & Axe \\   

Table 1: **Attributes of 8 meta tasks covering Task101**: We evaluate the algorithm on Minecraft Task101. We group the consisted 71 task into 8 different meta groups, with each focusing on testing a different aspect of our proposed method.

In addition, _selector_ also greatly improves the final task success rate of the agent (from **DEP w/o Selector** to **DEPS**). Hard meta tasks usually require the completion of multiple sub-goals (up to dozens of goals), thus bringing more flexibility and providing more candidate goals for the Selector. At the same time, as the agent conducts experiments with limited episode length, it also places high demands on the efficiency of the plan. Therefore, the Selector brings a significant improvement on efficiency-sensitive tasks such as MT7 (up to **+2.7** times success rate).

Robustness on different controller and different Minecraft versionsWe also evaluate DEPS on MineRL  and MC-Textworld . Note that DEPS is a planning method, which needs to equip the goal-conditioned controller for interacting with the Minecraft environments. We choose MC-Controller  and Steve-1  as controllers to interact with Minedojo and MineRL, respectively. These two methods are all control policies that perceive visual partial observations and produce mouse and keyboard actions. While MC-Textworld is a text world, which only keeps the Minecraft crafting recipes and mining rules. So MC-Textworld does not require the controller. The DEPS results of the task set MT1-MT8 on different Minecraft environments are shown in Table 3. The results report that DEPS can generate effective plans in various Minecraft environments. The results on MC-Textworld  also show that the performance drops on more difficult task sets from MT6 to MT8 are mainly from the controller limitation.

### Ablation Study

We conduct ablation experiments to investigate the number of candidate executable goals for different Selector models and the specific impact of the rounds of DEPS.

#### 4.3.1 Ablation on Selector

We verify the robustness of our proposed Selector under different parallel goals. The agent is asked to complete 2, 3, and 4 candidate goals (the precondition is consistent for all goals), respectively. The goals of the task correspond to different kinds of mobs or materials.

We report the final success rate of our method (DEP) with different selector implementations, including using a fixed sequence of goals, a random sequence of goals, and selecting a goal based on MineCLIP , CLIP , and our horizon-predictive Selector (HPS). As Figure 4 shows, in one round of parallel candidate goals, an improvement of \(\)=+22.3%, +29.2%, +32.6% is obtained using our horizon-predictive Selector compared to not any selector (i.e., fixed plan), respectively.

At a limited episode length, e.g., 1000 steps, goal-model shows a greater advantage, which proves that goal-model can improve the execution efficiency of the plan in embodied environments. In addition, compared to using vision-language models such as CLIP  and MineCLIP  as a goal model, horizon-predictive has the best performance due to better estimation of the horizon information. The curve trend also demonstrates that agents with Selector scale up under large amounts of goals in an open-world environment.

   Environments & Version & Controller & MT1 & MT2 & MT3 & MT4 & MT5 & MT6 & MT7 & MT8 \\  MineDojo  & 1.11.2 &  & 79.77 & 79.46 & 62.40 & 53.32 & 29.24 & 13.80 & 12.56 & 0.59 \\ MineRL  & 1.16.5 &  & 84.05 & 80.32 & 24.25 & 36.21 & 9.16 & 17.22 & 16.79 & 1.84 \\ MC-Textworld  & 1.19.2 & - & 100.00 & 90.00 & 80.00 & 56.25 & 64.71 & 57.14 & 69.57 & 50.00 \\   

Table 3: Success rates of DEPS under different Minecraft environments.

   Methods & MT1 & MT2 & MT3 & MT4 & MT5 & MT6 & MT7 & MT8 & AVG \\  GPT[16; 32] & 25.85\(\)24.8 & 47.88\(\)31.5 & 10.78\(\)14.6 & 7.14\(\)9.0 & 1.98\(\)5.9 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 15.42 \\ PP & 30.61\(\)23.6 & 40.09\(\)30.6 & 17.13\(\)19.1 & 16.00\(\)17.3 & 3.21\(\)4.9 & 0.47\(\)1.3 & 0.60\(\)2.2 & 0.0\(\)0.0 & 16.88 \\ CoT & 40.24\(\)30.8 & 55.21\(\)26.8 & 6.82\(\)11.6 & 4.76\(\)8.2 & 1.73\(\)5.2 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 18.89 \\ IM & 46.89\(\)31.4 & 53.73\(\)20.8 & 3.64\(\)69.9 & 18.41\(\)17.4 & 4.57\(\)7.4 & 0.64\(\)1.7 & 1.02\(\)2.5 & 0.0\(\)0.0 & 21.64 \\ CaIP & 60.08\(\)17.3 & 60.11\(\)20.4 & 8.78\(\)9.7 & 20.33\(\)21.0 & 2.84\(\)4.6 & 0.63\(\)1.3 & 0.60\(\)2.2 & 0.0\(\)0.0 & 25.77 \\ 
**DEP** & 75.02\(\)10.4 & 66.13\(\)13.4 & 45.69\(\)16.2 & 43.35\(\)20.2 & 1.59\(\)13.9 & 5.71\(\)3.7 & 4.60\(\)7.1 & 0.50\(\)0.5 & **39.36** \\
**DEPS** & 79.77\(\)8.5 & 79.46\(\)10.6 & 62.40\(\)17.9 & 53.32\(\)29.3 & 29.24\(\)27.3 & 13.80\(\)8.0 & 12.65\(\)13.3 & 0.59\(\)0.5 & **48.56** \\   

Table 2: Success rates of DEPS and existing LLM planners on Minecraft Task101. The full task-by-task list is in Appendix F.

#### 4.3.2 Ablation on Re-Planning Rounds

We evaluate our agent on all tasks with increasing maximum rounds of DEPS. The round is defined as a cycle of interactive LLM-based planning with description, explanation, and planning and selecting, i.e., an updated plan. All tasks for every maximum round are executed 30 times and the average success rate is reported in Table 4. We take the vanilla LLM planner as the baseline, in which the model takes the initially generated plan as the final execution plan, without involving any description, re-planning, or self-explanation processes during the task execution. Our results in the previous subsection utilize the maximum rounds possible under maximum tokens capped by OpenAI. We also report the success rate increment from vanilla planner to DEPS of every meta task in column \(\) in Table 4. This set of experiments demonstrates that DEPS can iteratively improve its plan in open-world environments. More description, self-explanation, and re-planning rounds produce better results, especially for hard tasks.

### ObtainDiamond Challenge

Mining diamonds in the open-world game Minecraft, i.e. MT8 in Table 2, has been a long-standing challenge for the community . It is challenging because mining diamonds from scratch in Minecraft involves acquiring a sequence of difficult-to-obtain items that require complex planning on goals like mining, inventory management, crafting with and without a crafting table, tool use, smelting iron ingot in a furnace, and mining at the lowest depths. We take the ObtainDiamond task as a bonus experiment to show the capabilities of our zero-shot planner on complex tasks in embodied environments. Previous methods' success rates on this challenge further vouch for its difficulty. [43; 34] leverages domain-specific reward functions and RL fine-tuning to achieve \(\)0.1% success rate in 15 minutes of game play. VPT further boosts the success rate to \(20\%\) within 20 minutes of play through pre-training on collects \(\)70k hours human demonstrations and finetuning with human-designed reward function . DreameV3 is trained from scratch to collect diamonds in a modified Minecraft environment (easier to break blocks) with world models to achieve a success rate of 2% .

Our DEPS manages to achieve on-par performance in this grand challenge; our agent achieves a 0.59% success rate within 10 minutes of gameplay. Note our method does not specifically fine-tune for this challenge. It is designed to be multi-task in its nature. Furthermore, considering our planner operates with demonstration prompts on a fixed Large Language Model, it can be straightforwardly adapted to other open-ended environments with modifications.

## 5 Related Works

Task planning with LLMsThere have been some methods leveraging the large language model to generate action plans for high-level tasks in embodied environments [46; 9; 11].  decompose natural language commands into sequences of executable actions by text completion and semantic

    &  &  &  &  & \(\) & \(\) \\  & & & & & & (\(0\)) \\  MT1 & 28.6 & 50.6 & 68.1 & 79.8 & 79.8 & **+51.2** \\ MT2 & 37.1 & 71.2 & 71.4 & 79.2 & 79.5 & **+42.4** \\ MT3 & 15.1 & 20.1 & 40.3 & 40.8 & 62.4 & **+47.3** \\ MT4 & 15.9 & 17.4 & 48.3 & 50.7 & 53.3 & **+37.4** \\ MT5 & 3.2 & 3.2 & 3.2 & 15.2 & 29.2 & **+26.0** \\ MT6 & 0.5 & 0.5 & 1.1 & 1.9 & 13.8 & **+13.3** \\ MT7 & 0.6 & 2.3 & 2.9 & 2.9 & 12.6 & **+12.0** \\ MT8 & 0.0 & 0.0 & 0.0 & 0.0 & 0.6 & **+0.6** \\   

Table 4: **Success Rate of DEPS under different maximum rounds of re-planning.** Round 0 represents the vanilla Planner w/o the re-planning process. \(\) represents the re-planning process will not end until task success or reaching the maximum horizon, which is still limited by the maximum tokens of LLMs. The maximum number of rounds for Codex is around 7-8 rounds.

Figure 4: **The success rates of DEPS with different selectors under varying numbers of parallel goals and maximum episode lengths.**translation, while SayCan generates feasible plans for robots by jointly decoding an LLM weighted by skill affordances from value functions . For better executing the plan in embodied environments, some methods use an object detector describing the initial environment into the language prompt to produce environment-suitable plans and adopt success detectors to check that each step is executed successfully [17; 20].  and  use the pythonic-style prompt to produce more executable plans. However, all of the above methods assume that the initial plan from the LLM is correct. When there are bugs in the initial plan, it's difficult for the agent to finish the task successfully.

Interactive Planning with LLMsInner Monologue  pilots the front of interactive planning with LLMs, which introduces the feedback (including success detection and scene description) to the planner. However, we found it could still suffer from accumulative planning error, especially in long-horizon open-world tasks. Rather, our "_Describe, Explain, Plan and Select_" (DEPS) method can produce more reliable plans by leveraging chain-of-thought thinking and explanation to locate the errors in previous plans. Moreover, we also propose a goal Selector to further improve the efficiency of the plan, thereby yielding much better performances. Readers are encouraged to refer to the comparative results in Section 4.2 between DEPS and these prior arts. There are also some concurrent works on planning with LLMs [39; 26; 23; 33; 47].

Agents in MinecraftSome previous works have employed the hierarchical architecture to solve long-horizon tasks in Minecraft [30; 27; 24]. Recently, based on the internet-scale corpus,  pre-trains a language-conditioned reward function and learns multi-task MineAgent.  collects a vast amount of human demonstrations to train a behavior cloning agent. More recently,  utilized a learned world model to distill a policy that can efficiently explore in Minecraft. There are also some works focus on learning goal-conditioned policies for better instruction-following [6; 7; 21]. While these efforts all focus on improving the low-level controller. Rather, the planner in our architecture emphasizes applying domain knowledge to propose and arrange the sub-goals. It significantly influences the complexity and breadth of tasks that the agent can handle. Moreover, our planner is zero-shot, making it possible to generalize to other long-horizon open worlds.

## 6 Limitations

Albeit the impressive results of our approach, we believe there are at least two major limitations within our approach. First of all, our framework relies on privately-held LLMs like GPT-3 and ChatGPT, which makes it less accessible to those who cannot afford or access the service. However, we're fully committed to ensuring a more democratized method and will explore using open-sourced models including OPT  and BLOOM . Another issue is the explicit step-by-step planning in our system. Although it brings us superior performances over the baselines, the planning bottleneck can also prevent our model from being further scaled up. A more appealing approach will be amortizing the planning within an end-to-end trainable goal-conditioned policy, which is worth exploring next. Furthermore, some previous fundamental challenges in planning (e.g., dead ends) may not prevalent in our adopted environments and hence could be inadvertently overlooked by our paper. We are dedicated to addressing more fundamental challenges present in building a multi-task generalist agent in our series of following work.

## 7 Conclusion

We investigate the problem of planning in open worlds. We identify two major challenges unique to these environments: 1) long-term planning requires precise and multi-step reasoning, and 2) planning efficiency could be compromised since canonical planners do not take the agent's proximity to parallel goals/subtasks into consideration. We propose "Describe, Explain, Plan and Select" (**DEPS**), an interactive approach based on Large Language Models (LLMs) to tackle them both. Our experiments in the challenging Minecraft domain verify the advantages of our approach over counterparts by marking the milestone of robustly accomplishing 70+ Minecraft tasks and nearly doubling the overall performances. DEPS also is the first planning-based agent that can reach the diamond in this game.