# Continual Audio-Visual Sound Separation

Weiguo Pian\({}^{1}\)   Yiyang Nan\({}^{2}\)   Shijian Deng\({}^{1}\)   Shentong Mo\({}^{3}\)   Yunhui Guo\({}^{1}\)   Yapeng Tian\({}^{1}\)

\({}^{1}\) The University of Texas at Dallas  \({}^{2}\) Brown University  \({}^{3}\) Carnegie Mellon University

###### Abstract

In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (**Cont**inual Audio-**V**isual Sound **S**eparation). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024.

## 1 Introduction

Humans can effortlessly separate and identify individual sound sources in daily experience . This skill plays a crucial role in our ability to understand and interact with the complex auditory environments that surround us . However, replicating this capability in machines remains a significant challenge due to the inherent complexity of real-world auditory scenes . Inspired by the multisensory perception of humans , audio-visual sound separation tackles this challenge by utilizing visual information to guide the separation of individual sound sources in an audio mixture.

Recent advances in deep learning have led to significant progress in audio-visual sound separation . Benefiting from more advanced architectures (_e.g.,_ U-Net , Transformer , and diffusion models ) and discriminative visual cues (_e.g.,_ grounded visual objects , motion , and dynamic gestures ), audio-visual separation models are able to separate sounds ranging from domain-specific speech, musical instrument sounds to open-domain general sounds within training sound categories. However, a limitation of these studies is their focus on scenarios where all sound source classes are presently known, overlooking the potential inclusion of unknown sound source classes during inference in real-world applications. This oversight leads to the _catastrophic forgetting_ issue , where the fine-tuning of models on new classes detrimentally impacts their performance on previously learned classes. Despite Chen et al.  demonstrating that their iQuery model can generalize to new classes well through simple fine-tuning, it still suffers from the catastrophic forgetting problem on old classes. This prevents the trained models fromcontinuously updating in real-world scenarios, impeding their adaptability to dynamic environments. The question _how to effectively leverage visual guidance to continuously separate sounds from new categories while preserving separation ability for old sound categories_ remains open.

To bridge this gap, we introduce a novel _continual audio-visual sound separation_ task by integrating audio-visual sound separation with continual learning principles. The goal of this task is to develop an audio-visual model that can continuously separate sound sources in new classes while maintaining performance on previously learned classes. The key challenge we need to address is catastrophic forgetting during continual audio-visual learning, which occurs when the model is updated solely with data from new classes or tasks, resulting in a significant performance drop on old ones. We illustrate our new task and the catastrophic forgetting issue in Fig. 1.

Unlike typical continual learning problems such as task-, domain-, or class-incremental classification in visual domains , which result in progressively increasing logits (or probability distribution) across all observed classes at each incremental step, our task uniquely produces fixed-size separation masks throughout all incremental steps. In this context, each entry in the mask does not directly correspond to any specific classes. Additionally, the new task involves both audio and visual modalities. Therefore, simply applying existing visual-only methods cannot fully exploit and preserve the inherent cross-modal semantic correlations. Very recently, Pian _et al._ and Mo _et al._ extended continual learning to the audio-visual domain, but both focused on classification tasks.

To address these challenges, in this paper, we propose a novel approach named ContAV-Sep (**Cont**inual **A**udio-**Visual Sound **S**eparation). Upon the framework, we introduce a novel _Cross-modal Similarity Distillation Constraint (CrossSDC)_ to not only maintain the cross-modal semantic similarity through incremental tasks but also preserve previously learned knowledge of semantic similarity in old models to counter catastrophic forgetting. The CrossSDC is a generic constraint that can be seamlessly integrated into the training process of different audio-visual sound separators. To evaluate the effectiveness of our proposed ContAV-Sep, we conducted experiments on the MUSIC-21 dataset within the framework of continual learning, using the state-of-the-art audio-visual sound separation model iQuery  and a representative audio-visual sound separation model Co-Separation , as our separation base models. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance than other continual learning baselines. In summary, this paper contributes follows:

**(i)** To explore more practical audio-visual sound separation, in which the separation model should be generalized to new sound source classes continually, we pose a _Continual Audio-Visual Sound Separation_ task that trains the separation model under the setting of continual learning. To the best of our knowledge, this is the first work on continual learning for audio-visual sound separation.

**(ii)** We propose ContAV-Sep for the new task. It uses a novel cross-modal similarity distillation constraint to preserve cross-modal semantic similarity knowledge from previously learned models.

**(iii)** Experiments on the MUSIC-21 dataset can validate the effectiveness of our ContAV-Sep, demonstrating promising performance gain over baselines.

Figure 1: **Top: Illustration of the continual audio-visual sound separation task, where the model (separator) learns from sequential audio-visual sound separation tasks. Bottom: Illustration of the catastrophic forgetting problem in continual audio-visual sound separation and its mitigation by our proposed method. Fine-tuning: Directly fine-tune the separation model on new sound source classes; Upper bound: Train the model using all training data from seen sound source classes.**

Related Work

**Audio-Visual Sound Separation.** Audio-visual sound separation aims to separate individual sound sources from an audio mixture guided by visual cues. A line of research emerges under various scenarios, such as separating musical instruments [23; 84; 79; 21; 83; 67], human speech [20; 1; 18; 49; 15], or sound sources in in-the-wild videos [22; 70]. Many frameworks and methods have been proposed to address challenges within specific problem settings. For instance, the extraction of face embeddings proves beneficial for speech audio separation . Moreover, incorporating object detection can provide an additional advantage [23; 22]. The utilization of trajectory optical flows to leverage temporal motion information in videos, as demonstrated by , also yields improvements. In this work, not competing on designing stronger separators, we would advance the exploration of the audio-visual sound separation within the paradigm of continual learning. We investigate how a model can learn to consistently separate sound sources from sequential separation tasks without forgetting previously acquired knowledge.

**Continual Learning.** The field of continual learning has drawn significant attention, especially in visual domains, with various approaches addressing this challenge. Notable among these are regularization-based methods, exemplified in works such as [32; 3; 31; 39]. These approaches involve applying regularization to crucial parameters associated with old tasks to maintain the model's capabilities and during incremental steps, less important parameters are given higher priority for updates compared to important ones. Conversely, several works [57; 9; 5; 26; 12; 54; 8; 10; 40] applied rehearsal-based pipelines to enable the model review previously learned knowledge. For instance, Rebuffi _et al._ proposed one of the most representative exemplars selection strategy Nearest-Mean-of-Exemplars (NME), selects the most representative exemplars in each class based on the distance to the feature center of the class. Meanwhile, pseudo-rehearsal [47; 48; 66] employs generative models to create pseudo-exemplars based on the estimated distribution of data from previous classes. Moreover, architecture-based/dynamic architecture methods[52; 4; 46; 24; 28; 37; 40] proposed to modify the model architecture itself to enable the model to acquire new knowledge while mitigating the forgetting of old knowledge. Specifically, Pham _et al._ proposed a dual network architecture, in which one is to learn new tasks while the other one is for retaining knowledge learned from old tasks. Wang _et al._ combined the dynamic architecture and distillation constraint to mitigate the issue of continual-increasing overhead problem in dynamic architecture-based continual learning method. However, above studies mainly concentrate on the area of continual image classification. Recently, researchers also explored other continual learning scenarios beyond image classification. For instance, Park _et al._ extend the knowledge distillation-based [2; 17] continual image classification method to the domain of video by proposing the time-channel distillation constraint. Douillard _et al._ proposed to tackle the continual semantic segmentation task with multi-view feature distillation and pseudo-labeling. Xiao _et al._ further addressed the continual semantic segmentation problem through weights fusion strategy between old and current models. Wang _et al._ addressed the continual sound classification task through generative replay. Furthermore, continual learning has also been explored in the domain of language/vision-language learning tasks [30; 43; 59; 61; 19; 86], self-supervised representation learning [19; 42; 55; 80; 35; 74], audio classification [6; 73] and fake audio detection [41; 82], etc. Despite the success of existing continual learning methods in various scenarios, their applicability in the domain of continual audio-visual sound separation is still unexplored. Although Pian _et al._ and Mo _et al._ proposed to tackle the catastrophic problem in audio-visual learning, their studies mainly concentrated in the area of audio-visual video classification. In contrast to existing works in continual learning, in this paper, we delves into the continual audio-visual sound separation, aiming to tackle the challenge of catastrophic forgetting specifically in the context of separation mask prediction for complicated mixed audio signals within joint audio-visual modeling.

## 3 Method

### Problem Formulation

**Audio-Visual Sound Separation.** Audio-visual sound separation aims to separate distinctive sound signals according to the given associated visual guidance. Following previous works [14; 23; 67; 21; 79], we adopt the common "mix-and-separation" training strategy to train the model. Given two videos \(_{1}(_{1},_{1})\) and \(_{2}(_{2},_{2})\), we can obtain the input mixed sound signal \(\) by mixing two video sound signals \(_{1}\) and \(_{2}\), and then we can have the ratio masks \(^{1}=_{1}/\) and \(^{2}=_{2}/\)1. The goal of the task is to utilize the corresponding visual guidance \(_{1}\) and \(_{2}\) to predict the ratio masks for reconstructing the two individual audio signals. This process can be formulated as:

\[}^{1} =_{}(,_{1}),\] (1) \[}^{2} =_{}(,_{2}),\]

where \(_{}\) is the separation model with trainable parameters \(\). And then, the original sound signals \(_{1}\) and \(_{2}\) are used to calculate the loss function for optimizing the model:

\[^{*}=*{argmin}_{}_{(_{1}, _{2})}(}^{1},^{1})+ (}^{2},^{2}),\] (2)

where \(\) denotes the training set, and \(\) is the loss function between the prediction and ground-truth.

**Continual Audio-Visual Sound Separation.** Our proposed continual audio-visual sound separation task aims to train a model \(_{}\) continually on a sequence of \(T\) separation tasks \(\{_{1},_{2},...,_{T}\}\). For the \(t\)-th task \(_{t}\) (incremental step \(t\)), we have a training set \(_{t}=\{^{i}(^{i},^{i}),y^{i}_{t}\}_{i=1}^{n_{t}}\), where \(i\) and \(n_{t}\) denote the \(i\)-th video sample and the total number of samples in \(_{t}\) respectively, and \(y^{i}_{t}_{t}\) is the corresponding sound source class of video \(^{i}\), where \(_{t}\) is the training sound class label space of task \(_{t}\). For any two tasks \(_{t_{1}}\) and \(_{t_{2}}\) and their corresponding training sound class label space \(_{t_{1}}\) and \(_{t_{2}}\), we have \(_{t_{1}}_{t_{2}}=\). Following previous works in continual learning [57; 2; 53; 44; 29; 76], for a task \(_{t}\), where \(t>1\), holding a small size of memory/exemplar set \(_{t}\) to store some data from old tasks is permitted in our setting. Therefore, with the memory/exemplar set \(_{t}\), all available data that can be used for training in task \(_{t}\) (\(t>1\)) can be denoted as \(^{}_{t}=_{t}_{t}\). Finally, the training process of Eq. 2 in our continual audio-visual sound separation setting can be denoted as:

\[_{t}=*{argmin}_{_{t-1}}_{( _{1},_{2})^{}_{t}}(}^ {1},^{1})+(}^{2},^{2}),\] (3) \[\ \ }^{1}=_{_{t-1}}(,_{1}),\ }^{2}=_{_{t-1}}(,_{2}),\]

which means that the new model \(_{t}\) is obtained by updating the old model \(_{t-1}\) which was trained on the previous task, using the current task's available data \(^{}_{t}\). After the training process for task \(_{t}\) with \(^{}_{t}\), the updated model will be evaluated on a testing set which includes video samples from all seen sound source classes up to continual step \(t\) (task \(_{t}\)). And the evaluation also follows the common "mix-and-separation" strategy. During this continual learning process, the model's separation performance on the previously learned tasks drops significantly after training on new tasks. This learning issue is referred to as the _catastrophic forgetting_[32; 38; 3] problem, which poses a considerable challenge in continual audio-visual sound separation.

### Overview

To address the challenge of catastrophic forgetting in continual audio-visual sound separation, we introduce **ContAV-Sep**. This new framework, illustrated in Fig. 2, consists of three key components: a separation base model, an output mask distillation module, and our proposed _Cross-modal Similarity Distillation Constraint (CrossSDC)_. We use a recent state-of-the-art audio-visual separator: iQuery  as the base model of our approach, which contains a video encoder to extract the global motion feature, an object detector and image encoder to obtain the object feature, a U-Net  for mixture sound encoding and separated sound decoding, and an audio-visual Transformer to get the separated sound feature through multi-modal cross-attention mechanism and class-aware audio queries. For the object detector, we follow iQuery  and use the pre-trained Detic , a universal object detector, to detect the sound source objects in each frame. For the video encoder and the image encoder, inspired by the excellent generalization ability of recent self-supervised pre-trained models, which has been proven to be effective and appropriate in continual learning , we apply two self-supervised pre-trained models VideoMAE  and CLIP  as the video encoder and the image encoder, respectively. Note that, during the training process, the object detector, video encoder, and image encoder are frozen.

Given a pair of videos \(_{1}(_{1},_{1})\) and \(_{2}(_{2},_{2})\), at incremental step \(t\) (task \(_{t}\)), the U-Net audio encoder \(_{t}^{AE}\) takes the mixed audio signal \(\) obtained by mixing \(_{1}\) and \(_{2}\) as input, and generates the latent mixed audio feature. This process can be expressed as:

\[_{t}^{lat.}=_{t}^{AE}(),\] (4)

Then, the audio-visual Transformer \(_{t}^{Trans.}\) is employed to generate the separated sound feature by taking the latent mixed audio feature and visual features as inputs:

\[&_{t}^{a,1}=_{t}^{Trans.}(_{t}^ {lat.},_{t}^{o,1},_{t}^{m,1}),\\ &_{t}^{o,1}=_{t}^{o}(^{1}),\ _{t}^{m,1}=_{t}^{m}(^{1}),\] (5)

where \(_{t}^{a,1}\) denotes the separated sound feature of video \(_{1}\); \(^{1}\) and \(^{1}\) denote the object and motion features extracted by the frozen pre-trained image and video encoders respectively from the visual signal \(_{1}\) of video \(_{1}\), \(_{t}^{o}()\) and \(_{t}^{m}()\) are learnable projection layers to map the object and motion features into the same dimension. Similarly, we can also obtain the separated sound feature of \(_{2}\) guided by the associated visual features.

The extracted separated sound feature and the latent mixed audio feature are combined to generate a mask. This mask is subsequently applied to the mixed audio, leading to the reconstruction of the separated sound spectrogram.

\[}_{t}^{1}&=_{t}^ {AD}(_{t}^{lat.}) MLP_{t}(_{t}^{a,1}),\\ }_{t}^{2}&=_{t}^{AD}(_{t }^{lat.}) MLP_{t}(_{t}^{a,2}),\] (6)

where \(}_{t}^{1}\) and \(}_{t}^{2}\) denote the predicted masks for audio signals of video \(_{1}\) and \(_{2}\), respectively; \(_{t}^{AD}\) is the U-Net decoder at incremental step \(t\); \(MLP_{t}()\) denotes a MLP module; and \(\) denotes channel-wise multiplication. The sound \(_{1}\) at this incremental step can be reconstructed by applying \(}_{t}^{1}\) and then performing an inverse STFT to obtain the audio waveform.

### Cross-modal Similarity Distillation Constraint

Recent studies [53; 45] have highlighted the importance of cross-modal semantic correlation in audio-visual modeling. However, this correlation tends to diminish during subsequent incremental phases, which leads to catastrophic forgetting in our continual audio-visual sound separation task. To address this challenge, we propose a novel Cross-modal Similarity Distillation Constraint (CrossSDC)

Figure 2: Overview of our proposed ContAV-Sep, which consists of an audio-visual sound separation base model architecture, an Output Mask Distillation, and our proposed Cross-modal Similarity Distillation Constraint. The fire icon denotes the module is trainable, while the snowflake icon denotes that the module is frozen. The (i)STFT stands for (inverse) Short-Time Fourier Transform. Please note that, the old model \(_{_{t-1}}\) is frozen during training.

that serves two crucial purposes (1) maintaining cross-modal semantic similarity through incremental tasks, and (2) preserving previous learned semantic similarity knowledge from old tasks.

CrossSDC preserves cross-modal semantic similarity from two perspectives: instance-aware semantic similarity and class-aware semantic similarity. Both similarities are enforced by integrating contrastive loss and knowledge distillation. Instead of exclusively focusing on the similarities within current and memory data generated by the current training model, CrossSDC incorporates the cross-modal similarity knowledge acquired from previous tasks into the contrastive loss. This integration not only facilitates the learning of cross-modal semantic similarities in new tasks but also ensures the preservation of previously acquired knowledge. In the incremental step \(t\) (\(t>1\)), the instance-aware part of our CrossSDC can be formulated as:

\[_{inst.}=-_{^{i}_{t}^{i}}[ [i=j]}_{j}[i=j](_{_{1},i}^{mod_{1}},_{_{2},j}^{mod_{2}}))}{_{k} ((_{_{1},i}^{mod_{1}},_{_{2},k}^{mod_{2}}))} ],\] (7)

where \([i=j]\) is an indicator that equals 1 when \(i=j\), denoting that video samples \(^{i}\) and \(^{j}\) are the same video; The sim function represents the cosine similarity function with temperature scaling; The modalities \(mod_{1}\) and \(mod_{2}\), where \((mod_{1},mod_{2})\{(a,o),(a,m),(m,o)\}\), denote different pairs of features to be compared: separated sound and object features, sound and motion features, and motion and object features. Here, \(\) denotes the incremental step, for which we have:

\[_{1},_{2},\ where\ =\{\{t,t -1\},&_{t},\\ \{t\},&_{t},.\] (8)

which means that, for current task's data \(_{t}\), we calculate the contrastive loss using features from the current model (\(_{1}=_{2}=t\)), while for memory set data \(_{t}\), we use features from _both the old and current models_ (_e.g._, \(_{1}=t\) and \(_{2}=t-1\)). In this way, knowledge distillation would be integrated into the cross-modal semantic similarity constraint for the current task, which ensures better preservation of learned cross-modal semantic similarity from previous tasks.

While the instance-aware similarity provides valuable semantic correlation modeling, it does not account for the class-level semantic correlations, which is also crucial for audio-visual similarity modeling. To capture and preserve the semantic similarity within each class across incremental tasks, we also incorporate a class-aware component specifically designed for inter-class cross-modal semantic similarity, which can be formulated as:

\[_{cls.}=-_{(^{i},y^{i})_{t}^{i}} [[y^{i}=y^{j}]}_{j}[y^{i}=y^{ j}](_{_{1},i}^{mod_{1}},_{_{2},j}^{ mod_{2}}))}{_{k}((_{_{1},i}^{mod_{1}},_{_{2},k}^{ mod_{2}}))}].\] (9)

In this context, visual and audio features from two videos are encouraged to be close when they belong to the same class. The overall formulation of our CrossSDC is as follows:

\[_{CrossSDC}=_{ins}_{ins}+_{cls}_{cls},\] (10)

where \(_{ins}\) and \(_{cls}\) are two scalars that balance the two loss terms. In this way, the model captures and preserves semantic correlations not just between instances but also within the same classes.

### Overall Loss Function

In the previous subsection, we introduced our proposed CrossSDC constraint. To effectively combine CrossSDC with the overall objective, we incorporate it alongside output distillation and the main separation loss function.

Output distillation is a widely used technique in continual learning [38; 2; 53] to preserve the knowledge gained from previous tasks while learning new ones. In our approach, we utilize the output of the old model as the distillation target to preserve this knowledge. Note that we only distill knowledge for data from the memory set, as represented by:

\[_{dist.}=_{(_{1}^{i},_{2}^{j})_{t}}||}_{t}^{1}-}_{t-1}^{1}||_{1}+|| }_{t}^{2}-}_{t-1}^{2}||_{1},\] (11)

where \(}_{t-1}^{1}\) and \(}_{t-1}^{2}\) are predicted masks generated by the old model that is trained at incremental step \(t-1\). For the loss function here, we follow [84; 14] and adopt the per-pixel \(L_{1}\) loss . For the main separation loss function, we also apply the per-pixel \(L_{1}\) loss:

\[_{main}=_{(_{1}^{1},_{2}^{1})_{t }}||}_{t}^{1}-^{1}||_{1}+||}_{t}^{2}-^{ 2}||_{1},\] (12)

Finally, our overall loss function is denoted as:

\[_{ContAV-Sep}=_{main}+_{dist.}_{dist. }+_{CrossSDC}.\] (13)

### Management of Memory Set

In alignment with the work of , our framework maintains a compact memory set throughout incremental updates. Each old class is limited to a maximum number of exemplars. After completing training for each task, we adopt the exemplar selection strategies in [2; 53] by randomly selecting exemplars for each current class and combining these new exemplars with the existing memory set.

## 4 Experiments

In this section, we first introduce the setup of our experiments, _i.e._, dataset, baselines, evaluation metrics, and the implementation details. After that, we present the experimental results of our ContAV-Sep compared to the baselines, as well as ablation studies. We also conduct experiments on the AVE  and the VGGSound  datasets, which contain sound categories beyond the music domain. We put the experimental results on the AVE and the VGGSound datasets, the comparison to the uni-modal semantic similarity preservation method, the performance evaluation on old classes in incremental tasks, and the visualization of separating results in the Appendix.

### Experimental Setup

**Dataset.** Following common practice [83; 88; 14], we conducted experiments on _MUSIC-21_, which contains solo videos of 21 instruments categories: accordion, acoustic guitar, cello, clarinet, erhu, flute, saxophone, trumpet, tuba, violin, xylophone, bagpipe, banjo, bassoon, congas, drum, electric, bass, guzheng, piano, pipa, and ukulele. In our experiments, we randomly selected 20 of them to construct the continual learning setting. Specifically, we split the selected 20 classes into 4 incremental tasks, each of which involves 5 classes. The total number of available videos is 1040, and we randomly split them into training, validation, and testing sets with 840, 100, and 100 videos, respectively. To further validate the efficacy of our method across a broader sound domain, we conduct experiments using the AVE  and the VGGSound  datasets in the appendix.

**Baselines.** We compare our proposed approach with vanilla Fine-tuning strategy, and continual learning methods EWC  and LwF . As we mentioned before, typical continual learning methods, _e.g._, class-incremental learning methods, which yield progressively increasing logits (or probability distribution) across all observed classes at each incremental step and design specific technique in the classifier, we consider that these methods are not an optimal choice for our proposed continual audio-visual sound separation problem. Thus, considering that continual semantic segmentation problem has a more similar form compared to conventional class-incremental learning, we also select two state-of-the-art continual semantic segmentation methods PLOP  and EWF  as our baselines. Moreover, we compare our method to the recently proposed audio-visual continual learning method AV-CIL , in which we adapt the original class-incremental version to the form of continual audio-visual sound separation by replacing their task-wise logits distillation with the output mask distillation. Further, we also present the experimental results of the Oracle/Upper Bound, which means that using the training data from all seen classes to train the model. **For fair comparison, all compared continual learning methods and our ContAV-Sep use the same state-of-the-art separator, _i.e._ iQuery , as the base separation model**. Further, we also incorporate our proposed and baseline methods into another representative audio-visual sound separation model Co-Separation . Notably, the Co-Separation model does not utilize the motion modality. Therefore, when CrossSDC is applied to Co-Separation, the \((mod_{1},mod_{2})\) in Eq. 7 and 9 is constrained to \((mod_{1},mod_{2})=(a,o)\). For baselines that involve memory sets, we ensure that each of them is allocated the same number of memory as our proposed method for fair comparison.

**Implementation Details.** Following , we use a 7-layers U-Net  as the audio net, and sub-sample the audio at 11kHz, each of which is approximately 6 seconds. We apply the STFT with the Hann window size of 1022 and the hop length of 256, to obtain the \(512 256\) Time-Frequency representation of each audio signal, followed by a re-sampling on the log-frequency scale to generate the magnitude spectrogram with \(T,F=256\). We set the video frame rate (FPS) to 1, and detect the object using the pre-trained universal detector Detic  to detect the sound source object on each frame, and then, each detected object is resized and randomly cropped to the size of \(224 224\). For the image encoder and the video encoder, we apply the self-supervised pre-trained CLIP  and VideoMAE  to yield the object feature and motion feature, respectively. For the audio-visual Transformer module, we follow the design in . For all the baseline methods, we apply the same model architecture and modules with ours for them, including the mentioned Detic, CLIP, VideoMAE, audio-visual Transformer, etc. Please note that, during our training process, the pre-trained Detic, CLIP, and VideoMAE are frozen. In our proposed Cross-modal Similarity Distillation Constraint (CrossSDC), the balance weights \(_{ins}\) and \(_{cls}\) are set to 0.1 and 0.3, respectively. And the balance weight \(_{dist}\). for the output distillation loss is set to 0.3 in our experiments. For the memory set, we set the number of samples in each old class to 1, so as other baselines that involve the memory set. All the experiments in this paper are implemented by Pytorch . We train our proposed method and all baselines on a NVIDIA RTX A5000 GPU. We follow previous works [67; 14] in sound separation, and evaluate the performance of all the methods using three common metrics in sound separation tasks: Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), and Signal to Artifact Ratio (SAR). The SDR measures the interference and artifacts, while SIR and SAR measure the interference and artifacts, respectively. In our experiments, we report the SDR, SIR, and SAR of all the methods after training at last incremental steps, _i.e._, testing results on all classes. For all these three metrics, higher values denote better results.

### Experimental Comparison

The main experimental comparisons are shown in Tab. 1. Our proposed method, ContAV-Sep, outperforms the state-of-the-art baselines by a substantial margin. Notably, compared to baselines using state-of-the-art audio-visual sound separator iQuery  as the separation base model, ContAV-Sep achieves a 0.3 improvement in SDR over the compared best-performing method. Additionally, our method surpasses the top baseline by 0.25 in SIR and 0.41 in SAR. Furthermore, compared to continual learning baselines with Co-Separation , our ContAV-Sep still outperforms other approaches. This consistent superior performance across different model architectures highlights not only the effectiveness but also the broad applicability and generalizability of our proposed CrossSDC.

Our observations further demonstrate that retaining a small memory set significantly enhances the performance of each baseline method. For instance, for the iQuery-based continual learning methods, equipping LwF  with a small memory set results in improvements of 3.31, 3.99, and 1.94 on SDR, SIR, and SAR, respectively. Similarly, the addition of a small memory set to EWC  leads to enhancements of 2.98, 3.43, and 1.43 in the respective metrics. The memory-augmented version of PLOP  exhibits superior performance with margins of 3.21, 3.24, and 1.68 for SDR, SIR, and SAR, respectively. Finally, incorporating memory into EWF  results in improvements of 1.37,

  Method & SDR\(\) & SIR\(\) & SAR\(\) & Method & SDR\(\) & SIR\(\) & SAR\(\) \\  _w/o memory_ & & & & & & & & \\ iQuery  + Fine-tuning & 3.46 & 9.30 & 10.57 & Co-Sep.  + Fine-tuning & 1.93 & 8.75 & 9.75 \\ iQuery  + LwF  & 3.45 & 8.78 & 10.66 & Co-Sep.  + LwF  & 2.32 & 7.84 & 10.28 \\ iQuery  + EWC  & 3.67 & 9.58 & 10.30 & Co-Sep.  + EWC  & 2.01 & 8.36 & 9.61 \\ iQuery  + PLOP  & 3.82 & 10.06 & 10.22 & Co-Sep.  + PLOP  & 3.24 & 9.17 & 9.59 \\ iQuery  + EWF  & 3.98 & 9.68 & 11.52 & Co-Sep.  + EWF  & 2.61 & 7.77 & 10.85 \\  _w/ memory_ & & & & & & & \\ iQuery  + LwF  & 6.76 & 12.77 & 12.60 & Co-Sep.  + LwF  & 3.85 & 9.62 & 10.74 \\ iQuery  + EWC  & 6.65 & 13.01 & 11.73 & Co-Sep.  + EWC  & 3.31 & 9.55 & 9.80 \\ iQuery  + PLOP  & 7.03 & 13.30 & 11.90 & Co-Sep.  + PLOP  & 3.88 & 9.92 & 9.99 \\ iQuery  + EWF  & 5.35 & 11.35 & 11.81 & Co-Sep.  + EWF  & 3.63 & 9.07 & 10.58 \\ iQuery  + AV-CIL  & 6.86 & 13.13 & 12.31 & Co-Sep.  + AV-CIL  & 3.61 & 9.76 & 9.68 \\
**ContAV-Sep (with iQuery )** & **7.33** & **13.55** & **13.01** & **ContAV-Sep (with Co-Sep. )** & **4.06** & **10.06** & **11.07** \\  Upper Bound (with iQuery) & 10.36 & 16.64 & 14.68 & Upper Bound (with Co-Sep.) & 7.30 & 14.34 & 11.90 \\  

Table 1: Main results of different methods on MUSIC-21 dataset under the setting of Continual Audio-Visual Sound Separation with base separation models of iQuery  and Co-Separation , respectively. The bold part denotes the best results. Our proposed ContAV-Sep achieves the best performance among all baselines.

[MISSING_PAGE_FAIL:9]

paired with data from previously unseen new classes -- this is different from conventional continual learning tasks, where old classes do _not_ acquire new knowledge in new tasks. This could be a potential reason why the baseline continual learning methods do not perform well in our continual audio-visual sound separation problem. In this work, our method also mainly focuses on preserving old knowledge of old tasks, which may prevent the model from acquiring new knowledge of old classes when training in new tasks. Recognizing this, we identify the exploration of this problem as a key avenue for future research in this field.

Additionally, the base model architectures used in our approach and baselines require object detectors to identify sounding objects. Although iQuery  can supplement object features with global video representations, it may still suffer from undetected objects. It is a fundamental limitation of the **object-based audio-visual sound separators**[23; 14]. While our work, unlike previous efforts, does not compete on designing a stronger audio-visual separation base model, enhancing the robustness of sounding object detection presents a promising direction for future research.

## 5 Conclusion

In this paper, we explore training audio-visual sound separation models under a more practical continual learning scenario, and introduce the task of continual audio-visual sound separation. To address this novel problem, we propose ContAV-Sep, which incorporates a Cross-modal Similarity Distillation Constraint to maintain cross-modal semantic similarity across incremental tasks while preserving previously learned semantic similarity knowledge. Experiments on the MUSIC-21 dataset demonstrate the effectiveness of our method in this new continual separation task. This paper opens a new direction for real-world audio-visual sound separation research.

**Broader Impact.** Our proposed continual audio-visual sound separation allows the model to adapt to new environments and sounds without full retraining, which could enhance efficiency and privacy by reducing the need to transmit and store sensitive audio data.

**Acknowledgments.** We thank the anonymous reviewers and area chair for their valuable suggestions and comments. This work was supported in part by a Cisco Faculty Research Award, an Amazon Research Award, and a research gift from Adobe. The article solely reflects the opinions and conclusions of its authors but not the funding agents.

Figure 4: Testing results with different memory size (number of samples per class in the memory) on the metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step.

    & \# of samples per class & SDR\(\) & SIR\(\) & SAR\(\) \\   & 1 & 7.33 & 13.55 & 13.01 \\  & 2 & 7.26 & 13.10 & 12.65 \\  & 3 & 7.88 & 13.66 & 13.43 \\  & 4 & 8.16 & 14.16 & 13.21 \\  & 10 & 8.97 & 15.16 & 13.72 \\  & 20 & 9.39 & 15.93 & 13.69 \\  & 30 & **10.09** & **16.34** & **14.10** \\   

Table 3: Experimental results of our proposed ContAV-Sep with different memory size from 1 to 30 samples per memory class.