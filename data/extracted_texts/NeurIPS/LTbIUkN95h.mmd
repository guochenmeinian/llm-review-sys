# Proof Outline:

Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks

 Honghao Wei

Washington State University

honghao.wei@wsu.edu

&Xin Liu

ShanghaiTech University

liuxin7@shanghaitech.edu.cn

&Weina Wang

Carnegie Mellon University

weinaw@cs.cmu.edu

&Lei Ying

University of Michigan, Ann Arbor

leiying@umich.edu

###### Abstract

This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic _given_ the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the optimality gap decreases as \(}(+),\) where \(n\) is the number of real samples and \(m\) is the number of augmented samples per real sample. It is important to note that without augmented samples, the optimality gap is \(}(1)\) due to insufficient data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network applications confirm that the proposed method indeed significantly accelerates learning in both deep Q-learning and deep policy gradient.

## 1 Introduction

Reinforcement learning (RL) algorithms have recently achieved superhuman performance in gaming, such as AlphaGo (Silver et al., 2017), and AlphaStar (Vinyals et al., 2019), under the premise that vast amounts of training data can be collected. However, collecting data in real-world could be expensive and time-consuming applications such as clinical trials and autonomous driving, posing a significant challenge to extending the success of RL to broader applications. In this paper, we are interested in sample efficient algorithms for a class of problems in which environments (also called systems in this paper) include both stochastic and deterministic transitions, resulting in two types of states: stochastic states and pseudo-stochastic states. For example, in a queueing system, customers arrive and depart, following certain stochastic processes, but the evolution of the queues given arrivals and departures are deterministic. We call these kinds of systems mixed systems. Mixed systems are common in various RL applications, including data centers, ride-sharing systems, and communication networks,which can be modeled as queueing networks so as mixed systems. The broad application of mixed systems makes it crucial to explore whether current RL approaches are efficient in learning optimal policies for such systems. However, existing RL approaches are often not sample efficient due to the curse of dimensionality. For instance, in a queueing system, the state space (i.e. the queue lengths) is unbounded, which means that a tremendous amount of data samples are required to adequately cover the state space in order to learn a near-optimal policy. To highlight the challenge, in a recent paper (Dai and Gluzman, 2022), a 96-core processor with \(1,510\) GB of RAM was used to train a queueing policy for a queueing network with only a few nodes.

This paper proposes a new approach to handling the curse of dimensionality based on augmented samples. Our approach is based on two observations:

* When dealing with a mixed system that has a large state space, model-based approaches are incredibly challenging to use. For example, as of today, it remains impossible to analytically determine the optimal policy for a queueing network. On the other hand, using a deep neural network can be regarded as a numerical method for solving a large-scale optimization problem. Therefore, a data-driven, neural network-based solution could be a much more efficient approach.
* It is well-known that training deep learning is data-hungry. While, in principle, model-free approaches can be directly used for mixed systems, they are likely to be very inefficient in sample complexity for mixed systems whose state space is large. In this paper, we will utilize the knowledge of deterministic transitions to generate new data samples by augmenting existing real samples. In other words, we can generalize real samples to a large (even infinite) number of samples that cover the unobserved pseudo-stochastic states. These samples are equally useful for training the neural network as the real samples.

Based on the two observations above, we consider a mixed system that includes two types of states, stochastic states and pseudo-stochastic states, where the transitions of the stochastic states are driven by a stochastic kernel, and the transitions of the pseudo-states are deterministic and conditioned on the current and next stochastic states. We comment that without conditioning the stochastic states, the pseudo-stochastic states become stochastic. The distributions of stochastic states and pseudo-stochastic states are correlated, which makes the problem different from MDPs with exogenous inputs.

With this state separation, we propose an augmented sample generator (ASG). The sample generator generates virtual samples from real ones while keeping stochastic states and augmenting the pseudo-stochastic states. Both the real samples and virtual samples are then used to train the deep neural networks, e.g., Q-networks or policy networks.

We analyze the sample complexity of the proposed approach for mixed systems under Fitted Q Iteration (FQI Ernst et al. (2005)) which is equivalent to DQN (Mnih et al., 2015) for tabular setting. Specifically, we consider the scenario where the size of pseudo-stochastic state space is much larger than that of stochastic state space, and the set of available real data samples does not provide sufficient coverage of the joint state space. This is the situation where the proposed approach is expected to be particularly advantageous. Our analysis demonstrates that the proposed approach yields a significant improvement in the convergence rate for tabular settings. In particular, by generating \(m\) virtual samples for each real sample, the optimality gap between the learned policy and the optimal policy decreases as \(}(+),\) where \(n\) is the number of real samples. Note that without augmented samples, the error is \(}(1)\) due to the lack of data coverage of the pseudo-stochastic states, which reduces to \(}()\) when we generate at least \(n\) augmented samples for each real sample. We also would like to emphasize that \(()\) is also the fundamental lower bound due to the coverage of the stochastic states.

### Related Work

Deep reinforcement learning with augmented samples is not new. However, the approach in this paper has fundamental differences compared with existing works in the literature, which we will explain in detail.

**Dyna and its variants:** The first group of related methods is Dyna (Sutton, 1988) and its extensions. Dyna-type algorithms are an architecture that integrates learning and planning for speeding up learning or policy convergence for Q-learning. However, our proposed method differs fundamentally from these approaches in several aspects: \((1)\) Dyna-type algorithms are model-based methods and need to estimate the system model which will be used to sample transitions. On the contrary, ASG does not require such an estimation and instead leverages the underlying dynamics of the system to generate many augmented samples from one real sample. \((2)\) Dyna is limited to making additional updates only for states that have been observed previously, whereas ASG has the potential to update all pseudo-stochastic states, including those that have not yet been explored. \((3)\) Since Dyna is a model-based approach, the memory complexity is \(||^{2}||^{2}||\), where \(\) is the stochastic state space, \(\) is the pseudo-stochastic state space and \(\) is the action space. ASG employs a replay buffer only for real data samples, which typically requires far less memory space. Moreover, some Dyna-type approaches like Dyna-Q\({}^{+}\)(Sutton, 1988), Linear Dyna (Sutton et al., 2012), and Dyna-Q\(()\)(Yao et al., 2009), require more computational resources to search the entire state space for updating the order of priorities.

Dyna-type approaches have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020). As we already mentioned above, ASG is fundamentally different from these model-based approaches. The key differences between Dyna-type algorithms and ASG are summarized in Table 1.

**Data augmentation:** It has been shown that data augmentation can be used to efficiently train RL algorithms. Laskin et al. (2020) showed that simple data augmentation mechanisms such as cropping, flipping, and rotating can significantly improve the performance of RL algorithms. Fan et al. (2021) used weak and strong image augmentations to learn a robust policy. The images are weakly or strongly distorted to make sure the learned representation is robust. However, these methods rely solely on image augmentation, and none of them consider the particular structure of the mixed system. The purpose of augmentation and distortion is to improve robustness and generalization. Our approach is to use virtual samples, which are as good as real samples to learn an optimal policy. The purpose of introducing the virtual samples is to improve the data coverage for learning the optimal policy.

Encoding symmetries into the designs of neural networks to enforce translation, rotation, and reflection equivariant convolutions of images have also been proposed in deep learning, like G-Convolution (Cohen and Welling, 2016a), Steerable CNN (Cohen and Welling, 2016b), and E(2)-Steerable CNNs (Weiler and Cesa, 2019). Mondal et al. (2020); Wang et al. (2022) investigated the use of Equivariant DQN to train RL agents. van der Pol et al. (2020) imposed symmetries to construct equivariant network layers, i.e., imposing physics into the neural network structures. Lin et al. (2020) used symmetry to generate feasible virtual trajectories for training robotic manipulation to accelerate learning. Our approach relies on the structure of mixed systems to generate virtual samples for learning an optimal policy and can incorporate much more general knowledge than symmetry.

In addition, existing works on low-dimensional state and action representations in RL are also related to our research focus on representing the MDP in a low-dimensional latent space to reduce the complexity of the problem. For example, Modi et al. (2021); Agarwal et al. (2020) studied provably representation learning for low-rank MDPs. Misra et al. (2020); Du et al. (2019) investigated the sample complexity and practical learning in Block MDPs. There are also practical algorithms (Ota et al., 2020; Sekar et al., 2020; Machado et al., 2020; Pathak et al., 2017) with non-linear function approximation from the deep reinforcement learning literature. Although the mixed model studied in this paper can be viewed as an MDP with a low-dimensional stochastic transition kernel, the state space of the MDP does not have a low-dimensional structure, which makes the problem fundamentally different from low-dimensional representations in RL.

   & **estimate** & **update** & **computational** & **store** & **convergence** \\  & **model?** & **unseen states?** & **efficient?** & **all transitions?** & **analysis?** \\  Dyna-type & ✓ & ✗ & ✗ & ✗ & ✗ \\  ASG & ✗ & ✓ & ✓ & ✓ & ✓ \\  

Table 1: Dyna-type v.s. ASG

**Factored MDP:** A factored MDP (Kearns and Koller, 1999) is an MDP whose reward function and transition kernel exhibit some conditional independence structure. In a factored MDP, the state space is factored into a set of variables or features, and the action space is factored into a set of actions or control variables. This factorization simplifies the representation and allows for more efficient computation and decision-making. While factored MDPs and mixed systems share some similarities, they are actually quite different. In a factored MDP, state variables are grouped into sets, allowing for the factoring of rewards that depend on specific state subsets, and localized (or factorized) transition probabilities. In a mixed system, the cost/reward function is _not_ assumed to be factored. Furthermore, although the transition probabilities of stochastic states are localized, depending only on stochastic states and actions, the transitions of pseudo-stochastic states generally depend on the full state vector and are _not_ factorized. However, these transitions are _deterministic_, which allows us to generate augmented samples. Given the fundamental differences between the two, the analysis and the results are quite different despite the high-level similarity. Additionally, for the batch offline RL setting without enough coverage used in our paper, there are no existing approaches, including factored MDPs, that guarantee performance in this scenario.

**MDPs with exogenous inputs:** A recent paper considers MDP with exogenous inputs (Sinclair et al., 2022), where the system has endogenous states and exogenous states. The fundamental difference between exogenous-state/endogenous state versus stochastic-state/pseudo-stochastic-state is that the evolution of the exogenous state is independent of the endogenous state, while the evolution of the stochastic state depends on the action. Since the action is chosen based on the pseudo-stochastic state, the evolution of a stochastic state, therefore, depends on the pseudo-stochastic state.

The most significant difference between this paper and existing works is that we propose the concept of mixed systems and mixed system models. Based on the mixed system models, we develop principled approaches to generate virtual data samples, which are as informative for learning as real samples and enhance RL algorithms with much lower sample complexity. Equally significant, we provide sample complexity analysis that theoretically quantifies the sample complexity improvement under the proposed approach and explains the reason that principled data augmentation improves learning in RL.

## 2 Mixed Systems and Mixed System Models

We consider a discrete-time Markov decision process \(M=(,,P,R,,_{0}),\) whose state space is \(\), where \(\) is the set of stochastic states and \(\) is the set of pseudo-stochastic states, and both \(\) and \(\) are assumed to be finite. Further, \(\) is a finite action space, \(R:[0,r_{}]\) is a deterministic and known cost function, and \(P:( )\) is the transition kernel (where \(()\) is the probability simplex).

The transition kernel \(P\) is specified in the following way. The transitions of the stochastic state follow a stochastic transition kernel. In particular, the transition of the stochastic state \(S_{t}\) at time \(t\) can be represented as

\[p_{ij}(a)=P(S_{t+1}=j|S_{t}=i,a_{t}=a).\] (1)

We assume that this transition is independent of the pseudo-stochastic state \(X_{t}\) (and everything else at or before time \(t\)). The transition of the pseudo-stochastic state is then deterministic, governed by a function \(g\) as follows:

\[X_{t+1}=g(S_{t},X_{t},a_{t},S_{t+1}).\] (2)

Our system \(M\) is said to be a _mixed system_ because it includes both stochastic and deterministic transitions. Our mixed system model then consists of the stochastic transition kernel and the deterministic transition function \(g\).

We focus on discounted costs in this paper. Given a mixed system, the value function of a policy \(\) is defined as

\[V^{}(s,x):=[_{h=0}^{}^{h}R(s_{h},x_{h},a_{h })|(s_{0},x_{0})=(s,x),a_{h}=(s_{h},x_{h})],\] (3)

where \(\) is the discount factor, and the expectation is taken over the transitions of stochastic states and randomness in the policy. Let \(v^{}=_{(s_{0},x_{0})_{0}}[V^{}(s_{0},x_{0})]\), i.e., the expected cost when the initial distribution of the state is \(_{0}\). Let \(v^{*}=_{}v^{*}\).

The \(Q\)-value function of a policy \(\) is defined as

\[Q^{}(s,x,a):=[._{h=0}^{}^{h}R(s_{h},x_{h},a _{h})|(s_{0},x_{0})=(s,x),a_{0}=a,a_{h}=(s_{h},x_{h})].\] (4)

Let \(Q\) be the optimal \(Q\)-value function. Then the Bellman equation can be written as

\[Q(s,x,a)=R(s,x,a)+_{s^{} P(|s,a)}[_{a ^{}}Q(s^{},g(s,x,a,s^{}),a^{})].\] (5)

Note that the value function and the \(Q\)-value function both take values in \([0,V_{}]\) for some finite \(V_{}\) under any policy, due to the assumption that the cost is in \([0,r_{}]\).

### Example: A Wireless Downlink Network

To better illustrate the structure of mixed systems. Consider the example of a wireless downlink network shown in Fig. 1 with three mobile users. We model it as a discrete-time system such that \(_{t}(i)\) is the number of newly arrived data packets for mobile \(i\) at the beginning of time slot \(t,\) and \(O_{t}(i)\) is the number of packets that can be transmitted to mobile \(i\) during time slot \(t\) if mobile \(i\) is scheduled.

The base station can transmit to one and only one mobile during each time slot, and let \(A_{t}\{1,2,3\}\) denote the mobile scheduled at time slot \(t.\) The length of queue \(i\) evolves as

\[q_{t+1}(i)=(q_{t}(i)+_{t}(i)-O_{t}(i)(A_{t}=i))^{ +},\] (6)

where \(()\) is the indicator function and \(()^{+}=\{,0\}.\)

To minimize the total queue lengths, the problem can be formulated as an RL problem such that the state is \((_{t},_{t},_{t}),\) the action is \(A_{t},\) and the cost is \(_{i}q_{t}(i).\)

We can see that in this problem, \(S_{t}:=(_{t},_{t})\) is the stochastic state and \(_{t}\) is the pseudo-stochastic state. In general, the state space of the stochastic states is bounded, e.g., both \(_{t}\) and \(_{t}\) may be Bernoulli random variables, but the pseudo-stochastic state such as \(_{t}\) is large or even unbounded. Therefore, while the distributions of the stochastic states can be learned with a limited number of samples, learning the distribution of the queue lengths, even under a given policy, will require orders of magnitude more samples. Therefore, vanilla versions of model-free approaches that do not distinguish between stochastic and pseudo-stochastic states require an unnecessary amount of data samples and are not efficient.

Therefore, we propose a sample-efficient, data-driven approach based on deep RL and augmented samples. Augmented samples guarantee enough coverage based on a limited number of real data samples. Note that given \(_{t},\)\(_{t}\) and \(A_{t},\) we can generate one-slot queue evolution starting from any queue length.

## 3 Sample Efficient Algorithms for Mixed Systems

### Augmented Sample Generator (ASG)

We first introduce the Augmented Sample Generator (ASG, Algorithm 1), which augments a dataset by generating virtual samples. In particular, ASG takes as input a dataset \(D\), an integer \(m,\) and a probability distribution \(()\) over the pseudo-stochastic states in \(\). For each sample \((s,x,a,r,s^{},x^{}) D,\) we first sample a pseudo-stochastic state \(\) from \(()\), and then construct a virtual sample \((s,,a,,s^{},^{})\), where \(=R(s,,a)\) and \(^{}=g(s,,a,s^{})\). Note that we are able to do this since we assume that functions \(R\) and \(g\) are given in our applications. For each sample in \(D\), we repeat this procedure \(m\) times independently to generate \(m\) virtual samples. Taking the downlink wireless network as an example, where \(S_{t}:=(_{t}},}),X_{t}:=},\) assume that at some timeslot \(t,\) one of the real samples is

\[(s,x,a,r,s^{},x^{}):=((\{(3,4,5),\{1,2,0\}),\{4,6,6\},1,16,(\{(2,3,4\},\{2,2,0\})\},\{6,10,11\})\,,\]

Figure 1: A Downlink Wireless Network

where the queue length \(}\) is calculated according to Eq. (6). Using ASG, we generate two pseudo-stochastic states (queue length) \(}_{1}=(1,2,3),}_{2}=(0,2,1)\), then we are able to obtain two virtual samples

\[(s,_{1},a,_{1},s^{},_{1}^{}):=(( \{3,4,5\},\{1,2,0\}),\{1,2,3\},1,6,(\{2,3,4\},\{2,2,0\}),\{3,6,8\})\,,\] \[(s,_{2},a,_{2},s^{},_{2}^{}):=(( \{3,4,5\},\{1,2,0\}),\{0,2,1\},1,3,(\{2,3,4\},\{2,2,0\}),\{2,6,6\})\,.\]

Note that all the virtual samples represent true transitions of the mixed system if the queue lengths were \(}_{1}\) and \(}_{2}\).

```
1Input: A dataset \(D=\{(s,x,a,r,s^{},x^{})\}\), a positive integer \(m\), a distribution \(()\) on \(\) ;
2Initialize virtual sample dataset: \(D^{}=\) ;
3foreach sample \((s,x,a,r,s^{},x^{}) D\)do
4 Sample \(m\) virtual pseudo-stochastic states \(_{1},_{2},,_{m}\) from \(()\) independently;
5 Compute \(_{i}=R(s,_{i},a)\) and \(_{i}^{}=g(s,_{i},a,s^{})\) for \(i=1,2,,m\);
6 Add \(m\) virtual samples to virtual dataset: \(\{(s,_{i},a,_{i},s^{},_{i}^{}),i=1,2,, m\} D^{}\);
7Output:\(D^{} D\) ; ```

**Algorithm 1**Augmented Sample Generator (ASG)

### Batch FQI with ASG

We now formally present our algorithm, Batch FQI with ASG (Algorithm 2), for mixed systems. As in a typical setup for batch reinforcement learning (Chen and Jiang, 2019), we assume that we have a dataset \(D\) with \(n\) samples. The samples are i.i.d. and the distribution of each sample \((s,x,a,r,s^{},x^{})\) is specified by a distribution \(()\) for \((s,x,a)\), \(r=R(s,x,a)\), \(s^{} P(|s,a)\), and \(x^{}=g(s,x,a,s^{})\). The distribution \(\) is unknown to the agent.

Our algorithm follows the framework of a batch FQI algorithm. We consider a class of candidate value-functions \(([0,V_ {}])\). We focus on the scenario where the number of pseudo-stochastic states, \(||\), is far more than that of stochastic states, \(||\), and the dataset \(D\) does not provide sufficient coverage of the pseudo-stochastic states (\(n<||\)).

The goal is to compute a near-optimal policy from the given dataset \(D\), via finding an \(f\) that approximates the optimal Q-function \(Q\). The algorithm runs in episodes. For each episode \(k\), it first generates an augmented dataset \(D_{k}\) using ASG\((D,m,_{k})\), where \(_{k}\) is some distribution of the pseudo-stochastic states satisfies that, for each typical pseudo-stochastic state, it can be generated with at least probability \(_{1}\). We assume that the pseudo-stochastic states will not transition to atypical states, such as extremely large values (possibly infinite) in the queuing example, under a reasonable policy. Details can be found in Assumption A.4 in the supplemental material. We remark that we can also use the same \(\) for all the episodes, and in practice, we can simply adopt the estimation of the pseudo-stochastic states (i.e., with Gaussian distribution) as \(_{k}\). The algorithm then computes \(f_{k}\) as the minimizer that minimizes the squared loss regression objective over \(\). The complete algorithm is given in Algorithm 2.

### Guarantee and Analysis

The guarantee of Algorithm 2 is summarized below.

**Theorem 1**.: _We assume the data coverage and completeness assumptions (details in the section A in the supplemental material). Given a dataset \(D=\{s,x,a,r,s^{},x^{}\}\) with \(n\) samples, w.p. at least \(1-\), the output policy of Algorithm 2 after \(k\) iterations, \(_{f_{k}}\), satisfies_

\[v^{_{f_{k}}}-v^{*}} }(5(+ )V_{}^{2}(|^{2}}{})+^{2}}{n})}\] \[+_{2}}V_{}+^{k}(1-)V_{} ,\] (7)

_where \(C\) is a constant related to the data coverage assumption A.3, and \(c_{0},_{1},_{2}\) are constants defined in Assumption A.4 to ensure a typical set of reasonable typical pseudo-stochastic states._Theorem 1 shows that ASG significantly improves the (real) sample complexity, i.e., the number of real data samples needed. When the tail of pseudo-stochastic states decays fast, e.g. an exponential tails link in a typical queueing network, we can choose a sampling distribution \(\) that guarantees that \(_{2}=}(1/n)\) and \(_{1}=(1/ n).\) Therefore, for sufficiently large \(k\) and sufficiently small \(_{2},\) the first term is the dominating term. As \(m\) increases from 1 to \(n,\) the first term of the bound decreases from \(}(1)\) to \(}().\) Remark that we cannot establish a convergence result without using ASG (when \(m=0\)) since using the dataset \(D\) alone doesn't have a sufficient data coverage guarantee, which is critical for batch RL. Due to the page limit, we only present the outline behind our analysis. The detailed proof is deferred to the supplemental material.1. First we will show that using performance difference lemma (Kakade and Langford, 2002) it is sufficient by bounding \(\|f_{k}-Q^{*}\|_{2,},\) where \(\) is some distribution.
2. Then, by considering how rare the distribution of the pseudo-stochastic states is under \(\) and then classify the pseudo-stochastic states into typical and atypical sets based on a threshold \(_{2}\) on the distribution. Along with the augmented data generator under \(_{k}().\) Then later we show that the term \(\|f_{k}-Q^{*}\|_{2,}\) can be bounded by \((\|f_{k}-f_{k-1}\|_{2,_{_{k}}}+\|f_ {k+1}-Q^{*}\|_{2,^{}}+}V_{}),\) where \(_{_{k}}\) is the data distribution after augmenting virtual samples. The first two terms are related to the typical set given that a sufficient coverage of the data after data augmentation. The last term is easy to obtain by considering the atypical set.
3. Therefore, it is obvious that if we can have a bound on \(\|f_{k}-f_{k-1}\|_{2,_{_{k}}}\) which is independent of the episode \(k,\) we can prove the final result by expanding \(\|f_{k}-Q^{*}\|_{2,}\) iteratively for \(k\) times. We finally show that the term \(\|f_{k}-f_{k-1}\|_{2,_{_{k}}}\) can indeed be bounded by using the FQI minimizer at each episode and a concentration bound on the offline dataset after augmenting virtual samples.

### Extension to the case when \(||\) is infinite

Theorem 1 assumes the number of the pseudo-stochastic states is finite. The result can be generalized to infinite pseudo-stochastic state space \(||\) if we make the following additional assumption:

**Assumption 1**.: _For the typical set \(\) of pseudo-stochastic states (formally defined in Assumption A.4 in the supplemental material), for any \(s,a,f,\) if \(x,\) then \(f(s,x,a) V_{}\) otherwise if \(x,\) we have \(|f(s,x,a)-Q^{*}(s,x,a)| V_{}.\) Furthermore, for any given \(f,(s,x),x,\) we have \(|V_{f}(s^{},x^{})-V_{f}(s^{},x^{})| V _{},\) where \(x^{}=g(s,x,_{f},s^{}),x^{}=g(s,x,_{f},s^{ }).\)_

The first part in assumption 1 means that the function \(f\) is always bounded when \(x.\) Otherwise, the difference between \(f\) and the optimal \(Q-\)function \(Q^{*}\) is bounded, which indicates that although \(f\) could be extremely large or even unbounded for the pseudo-stochastic states, including the infinite case, in the atypical set, it is also true for the optimal \(Q-\)function \(Q^{*}\). Therefore the difference between \(f\) and \(Q^{*}\) is assumed to be bounded, which is reasonable in practice. The second part states that for any given \(x,\) the difference between the value functions of any possible next transitions \((s^{},x^{})\) and \((s^{},x^{})\) is always bounded, which is also true in general especially in queuing literature since the changing of queue lengths between two timeslots is small.

Assumption 1 is easy to be satisfied in a typical queueing system (e.g. the wireless downlink network in Section 2.1). If the control/scheduling policy is a stabilizing policy, then the queues have exponential tails, i.e., the probability decays exponentially as queue length increases, and the policy can drive very large queues to bounded values exponentially fast. Therefore, if all functions in \(F\) are from stabilizing policies, Assumption 1 holds because any policy in \(F\) including the optimal policy can reduce the queue lengths exponentially fast (in expectation) and the difference in \(Q\)-functions therefore is bounded.

Under the additional Assumption 1, we show that the same order of the result is achievable. Details can be found in Section A.2 in the supplemental material.

## 4 Experiments

To evaluate the performance of our approach, we compared our results with several baselines in extensive queuing environments. In our simulations, when augmenting virtual samples, we only require that the augmented samples are valid samples for which the action from the real sample remains to be feasible.

### The Criss-Cross Network

The criss-cross network is shown in Fig. 1(a), which consists of two servers and three classes of jobs. Each job class is queued in a separate buffer if not served immediately. Each server can only serve one job at a time. Server \(1\) can process jobs of both class \(1\) and class \(3;\) server \(2\) can only process jobs of class \(2.\) Class \(3\) jobs become class \(2\) jobs after being processed, and class \(2\) and class \(3\) jobs leave the system after being processed. The service time of a class \(i\) job is exponentially distributed with mean \(m_{i}.\) The service rate of a class \(i\) job is defined to be \(_{i}:=1/m_{i},\) and jobs of class \(1\) and class \(3\) arrive to the system following the Poisson processes with rates \(_{1}\) and \(_{3}.\) To make sure the load can be supportable (i.e. queues can be stabilized), we assume \(_{1}m_{1}+_{3}m_{3}<1\) and \(_{1}m_{2}<1.\) In the simulation, we consider the imbalanced medium traffic regime, where \(_{1}=_{3}=2,_{2}=1.5,_{1}=_{3}=0.6.\)

All queues are first-in-first-out (FIFO). Let \(e_{t}(i)\{-1,0,+1\}\) denote the state of class \(i\) jobs at time \(t,\) where \(-1\) means a departure, \(+1\) means a arrival and \(0\) denotes no changes for job \(i.\) Then \(e_{t}=(e_{t}(1),e_{t}(2),e_{t}(3))\) be the event happens at time \(t\) in the system, which is the stochastic state in this mixed system. Let \(q_{t}=(q_{t}(1),q_{t}(2),q_{t}(3))\) be the vector of queue lengths at time \(t,\) where \(q_{t}(i)\) is the number of class \(i\) jobs in the system. Obviously, \(q_{t}\) is the pseudo-stochastic state in the system, which can be derived from \((q_{t-1},e_{t-1})\) and action \(a_{t}.\) We combine ASG with \(Q-\)learning and compare it with several baselines: (i) the vanilla Q-learning, (ii) a proximal policy optimization (PPO) based algorithm proposed in Dai and Gluzman (2022) designed for queuing networks, (iii) a random policy, and (iv) a priority policy such that the server always serves class \(1\) jobs when its queue length is not empty, otherwise it serves class \(3\) jobs. Simulation results in Fig. 1(d) demonstrate that using ASG significantly improves the performance, and achieves the performance of the optimal policy, which was obtained by solving a large-scale MDP problem, after only \(4\) training episode (\(4k\) training steps). All other policies are far away from the optimal policy, and barely learn anything, as we mentioned that the state space is quite large and is not even sufficiently explored after \(4k\) training steps. To further demonstrate the fundamental difference between ASG and Dyna-type algorithms, we also compared ASQ with the model-based approach: Q-learning with Dyna. We can observe that Dyna also fails to learn a good policy since, as we mentioned before, ASG is built for improving sample efficiency with augmented samples that represent _true_ transitions for possible all the pseudo-stochastic states, but dyna-type algorithms can only generate addition samples from an estimated model, which are limited by the samples that have been seen before.

### Two-Phase Criss-Cross Network:

To further evaluate the power of ASG beyond the tabular setting, We combine ASG with Deep Q-network (Mnih et al., 2015) named DQN-ASG and evaluate it on a more complicated variant of the Criss-Cross network presented in Fig. 2b, where class \(3\) jobs have two phases. Class \(3\) jobs at phase \(1\) become phase \(2\) after being processed with probability \(1-p,\) leave the system with probability \(p;\) and class \(3\) jobs at phase \(2\) leave the system after completed processing. Results in Fig. 2 indicate that combining ASG with DQN also has a significant improvement on the performance, and our method only needs \(100\) episodes to achieve a near-optimal policy. We also remark here that the training time for _DQN-ASG_ is three times faster than the baseline (\(4\) hours v.s. \(12\) hours). In the simulation, all the parameters are set to be the same as those in the previous case, and we use \(p=0.8\). Both the criss-cross and two-phase criss-cross networks are continuous-time Markov chains (CTMCs). We use the standard uniformization (Puterman, 2014; Serfozo, 1979) approach to simulate them using discrete-time Markov chains (DTMCs).

We remark here that the two-phase Criss-Cross network cannot be modeled as MDPs with exogenous inputs. Note that the phase a job is in is a stochastic state, which depends on the action (whether a job is selected for service). Since the scheduling action depends on the queue lengths, the evolution of stochastic states depends on the pseudo-stochastic states (the queue lengths). However, the exogenous input in an MDP with exogenous inputs has to be an observable random process independent of the action. Therefore, the current phase of a job of the phases Criss-Cross network is not an exogenous input and the system is not an MDP with exogenous inputs.

### Wireless Networks

In addition to the criss-cross networks, we also evaluate our approach on the downlink network described in Section 2.1 with three mobiles. We let the arrivals are Poisson with rates \(=\{2,4,3\},\) and the channel rates \(\) to be \(\{12,12,12\}.\) We use the Max-Weight (Tassiulas and Ephremides, 1992) algorithm, a throughput-optimal and heavy-traffic delay optimal algorithm, as the baseline. As shown in Fig. 2f, the learned policy outperforms Max-Weight. To the best of our knowledge, no efficient RL algorithms exist that can outperform Max-Weight.

### Additional Simulations on the criss-cross network

To further verify the performance of our algorithm on more scenarios, we first consider a more general criss-cross network where the size of both class \(1\) and class \(3\) jobs can vary over time. Then all class \(1\) and class \(3\) jobs need multiple timeslots to be processed instead of \(1\). In particular, when a new job of either class \(1\) or \(2\) joins the system, the job size of such job is chosen uniformly. The environment in Fig. 2b can be seen as a special case where class \(1,\) and \(3\) have a fixed job size \(1\) and \(2,\) respectively. The performances of different cases are presented in Fig. 3. Our approach still obtains the best result.

Figure 2: Performance on queuing systems

The details of the parameters can be found in section B.2. We do not include the optimal solution since, for the general case, the state space is quite large, and using value iteration to obtain the optimal solution is very time-consuming. The priority policy already provides a reasonable good baseline.

### Additional Simulations on the Wireless Network

We also evaluate the performance of our algorithm on the wireless network (Fig. 1) under different sets of arrival rates. The results of average queue length compared with those of Max-Weight are summarized in Table 2. We can observe that ASG performs better than Max-Weight in all the cases. Simulation results on a more complicated two-phase wireless network can be found in the supplemental material (Section B.3).

### Combining ASG with Policy Gradient-type Algorithm

We also investigate the possibilities of using ASG in policy gradient-type algorithms (i.e., TD3 (Fu et al., 2018), SAC (Haarnoja et al., 2018)) and compare our approach with one of the state-of-art algorithms MBPO (Janner et al., 2019). Our approach still achieves the best performance. Due to page limit, the detailed simulation and algorithm are deferred to Section B in the supplemental material.

## 5 Conclusions

In this work, we considered RL problems for mixed systems which have two types of states: stochastic states and pseudo-stochastic states. We proposed a sample efficient RL approach that accelerates learning by augmenting virtual data samples. We theoretically quantified the (real) sample complexity gain by reducing the optimality gap from \(}(1)\) to \(}(1/)\). Experimental studies further confirmed the effectiveness of our approach on several queuing systems. The possible limitation of using ASG is that it introduces more computation during each learning iteration when generating virtual samples, though it is not obvious in our experiments.