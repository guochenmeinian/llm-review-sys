# Efficient Training of Energy-Based Models

Using Jarzynski Equality

 Davide Carbone

Dipartimento di Scienze Matematiche, Politecnico di Torino

Istituto Nazionale di Fisica Nucleare, Sezione di Torino

davide.carbone@polito.it

Mengjian Hua

Courant Institute of Mathematical Sciences, New York University

mh5113@nyu.edu

Simon Coste

LPSM, Universite Paris-Cite

simon.coste@u-paris.fr

Eric Vanden-Eijnden

Courant Institute of Mathematical Sciences, New York University

eve2@nyu.edu

###### Abstract

Energy-based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is well measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by slow mixing of ULA. We illustrate these results with numerical experiments on Gaussian mixture distributions as well as the MNIST and CIFAR-10 datasets. We show that the proposed approach outperforms methods based on the contrastive divergence algorithm in all the considered situations.

## 1 Introduction

Probabilistic models have become a key tool in generative artificial intelligence (AI) and unsupervised learning. Their goal is twofold: explain the training data, and allow the synthesis of new samples. Many flavors have been introduced in the last decades, including variational auto-encoders [1; 2; 3] generative adversarial networks [4; 5], normalizing flows [6; 7; 8; 9; 10], diffusion-based models [11; 12; 13], restricted Boltzmann machines [14; 15; 16], and energy-based models (EBMs) [17; 18; 19].

Inspired by physics, EBMs are unnormalized probability models, specified via an energy function \(U\), with the underlying probability density function (PDF) defined as \(=(-U)/Z\), where \(Z\) is a normalization constant: this constant is often intractable but crucially it is not needed for data generation via Monte-Carlo sampling. In statistical physics, such PDFs are called Boltzmann-Gibbs densities [20; 21; 22] and their energy function is often known in advance; in the context of EBMs, the aim is to estimate this energy function in some parametric class using the available data. Such models benefit from a century of intuitions from computational physics to guide the design of the energy \(U\), and help the sampling; hence, interpretability is a clear strength of EBMs, since in many applications the parameters of the model have a direct meaning. Unfortunately, training EBMs is a challenging task as it typically requires to sample complex multimodal (i.e. non-log-concave) distributions in high-dimension.

This issue arises because real-world datasets are often clustered into different modes with imbalanced weights. A skewed estimation of these weights can lead to harmful biases when it comes to applications: for example, under-representation of elements from a particular population. It is important to ensure that these biases are either avoided or compensated, leading to methods that ensure fairness (); however, the training routines for EBMs often have a hard time properly learning the weights of different modes, and can often completely fail at this task.

Two popular classes of methods are used to train an EBM. In the first, the Fisher divergence between the model and the data distributions is taken as the objective to minimize, which amounts to learning the gradient of the energy function, \( U=-\). Although computationally efficient due to score-matching techniques , methods in this class are provably unable to learn the relative weights of modes when they are separated by low-density regions.

In the second class of methods, the cross-entropy between the model and the data distributions is used as the objective. Unlike the Fisher divergence, the cross-entropy is sensitive to the relative weights in multimodal distributions, but it unfortunately leads to training algorithms that are less justified theoretically and more delicate to use in practice. Indeed, gradient-based optimization procedures on the cross-entropy require sampling from the model distribution using Markov-Chain Monte-Carlo (MCMC) methods or the unadjusted Langevin algorithm (ULA) until mixing, which in a high-dimensional context or without log-concavity can be prohibitive, even considering the large computational power available today.

As a result, one typically needs to resort to approximations to estimate the gradient of the cross-entropy, for example by using the contrastive divergence (CD) or the persistent contrastive divergence (PCD) algorithms, see [25; 26; 27]. Unfortunately, the approximations made in these algorithms are uncontrolled and they are known to induce biases similar to those observed with score-based methods (the CD algorithm reduces to gradient descent over the Fisher divergence in the limit of frequent resetting from the data ). Many techniques have been proposed to overcome this issue, for example, based on MCMC sampling [29; 30; 31] or on various regularizations of CD [32; 33]. In practice though, these techniques still do not handle well multimodal distributions and they come with little theoretical guarantees.

Main contributions.In this work, we go back to the original problem of training EBMS using the cross-entropy between the model and the data distributions as objective, and:

* We derive exact expressions for the cross-entropy and its gradient that involve expectations over an ensemble of weighted walkers whose weights and positions evolve concurrently with the model energy during optimization. Our main tool is Jarzynski equality , an exact relation between the normalizing constants of two distributions linked by an out-of-equilibrium dynamics.
* Based on these formulas, we design sequential Monte Carlo sampling procedures  to estimate the cross-entropy and its gradient in a practical way at essentially no additional computational cost compared to using the CD or PCD algorithms with ULA.
* We show that reweighting the walkers is necessary in general and that training procedures based on the CD algorithm lead to uncontrolled biases whereas those based on using the PCD algorithm lead to mode collapse in general.
* We illustrate these results numerically on synthetic examples involving Gaussian mixture models where these effects can be demonstrated.

* We also apply our method on the MNIST and CIFAR-10 datasets. In the first, we intentionally bias the proportion of the different digits, to demonstrate that our method allows the retrieval of these proportions whereas the other methods fail to do so.

Related works.How to train EBMs is a longstanding question, and we refer e.g. to [17; 36] for overviews on this topic. The use of modern deep neural architectures to model the energy was then proposed in , and a relation between EBMs and classifiers has been highlighted in . How to sample from unnormalized probability models is also an old and rich problem, see [39; 40] for general introductions.

Score-matching techniques and variants originate from [24; 41; 42]; their shortcoming in the context of EBM training is investigated in  and their blindness to the presence of multiple, imbalanced modes in the target density has been known for long: we refer to [44; 45] for discussions. Contrastive divergence (CD) algorithms originate from [25; 26; 27]. These methods only perform one or a few sampling steps of the algorithm, with random walkers that are repeatedly restarted at the data points. Persistent contrastive divergence (PCD) algorithm, introduced in , eliminates the restarts and evolves walkers using ULA. Unlike the approach proposed in this paper, these methods are known to give estimates of the gradient of the cross-entropy that have uncontrolled biases which are difficult to remove; there were many attempts in this direction, also in cooperation with other unsupervised techniques, see e.g. [46; 47; 48; 49; 50; 51; 52].

It is worth noting that the original papers on CD proposed to use an objective which is different from the cross-entropy, and their proposed implementation (what we call the CD algorithm) does not perform gradient descent on this objective, due to some gradient terms being neglected . This sparked some debate about which objective, if any, was actually minimized by CD algorithms; for example, [53; 54] showed that the CD and PCD algorithms are essentially adversarial procedures;  introduced a way to approximate the missing term in the gradient of the CD objective; and [28; 55] showed that in the limit of small noise, CD is essentially equivalent to score matching. In contrast, there is no ambiguity about the objective used in the method we propose: it is always the cross-entropy.

Jarzynski equality (JE) was introduced in  and gives an exact expression relating the normalizing constants (or equivalently free energy ) of two distributions linked by out-of-equilibrium continuous-time dynamics. A discrete-time analog of JE is used in Neal's annealed importance sampling , which belongs to the framework of sequential Monte-Carlo methods . These methods have been used in the context of generative models based on variational autoencoders [57; 58], normalizing flows [59; 60], and diffusion-based models [61; 62]. In contrast, here we use sequential Monte-Carlo method to train EBM on the cross-entropy directly.

## 2 Energy-Based Models

Setup, notations, and assumptions.The problem we consider can be formulated as follows: we assume that we are given \(n\) data points \(\{x_{i}^{*}\}_{i=1}^{n}\) in \(^{d}\) drawn from an unknown probability distribution that is absolutely continuous with respect to the Lebesgue measure on \(^{d}\), with a positive probability density function (PDF) \(_{*}(x)>0\) (also unknown). Our aim is to estimate this PDF via an energy-based model (EBM), i.e. to find a suitable energy function in a parametric class, \(U_{}:^{d}[0,)\) with parameters \(\), such that the associated Boltzmann-Gibbs PDF

\[_{}(x)=Z_{}^{-1}e^{-U_{}(x)}; Z_{}=_{ ^{d}}e^{-U_{}(x)}dx\] (1)

is an approximation of the target density \(_{*}(x)\). The normalization factor \(Z_{}\) is known as the partition function in statistical physics . This factor is hard to estimate and one advantage of EBMs is that they provide generative models that do not require the explicit knowledge of \(Z_{}\) since Markov Chain Monte-Carlo (MCMC) methods can in principle be used to sample \(_{}\) knowing only \(U_{}\) - the design of such MCMC methods is an integral part of the problem of building an EBM.

To proceed we will assume that the parametric class of energy we use is such that, for all \(\),

\[ U_{} C^{2}(^{d}); L _{+}&:\| U_{}(x)\| L  x^{d};\\  a_{+}& ^{d}:\ x U_{}(x) a|x|^{2}  x^{d}.\] (2)These assumptions guarantee that \(Z_{}<\) (i.e. we can associate a PDF \(_{}\) to \(U_{}\) via (1) for any \(\)) and that the Langevin equations as well as their time-discretized versions we will use to sample \(_{}\) have global solutions and are ergodic [63; 64; 65]. We stress that (2) _does not_ imply that \(U_{}\) is convex (i.e. that \(_{}\) is log-concave): in fact, we will be most interested in situations where \(U_{}\) has multiple local minima so that \(_{}\) is multimodal. For simplicity we will also assume that \(_{*}\) is in the parametric class, i.e. \(2_{*}\ :\ _{_{*}}=_{*}\). Our aims are primarily to identify this \(_{*}\) and to sample \(_{_{*}}\); in the process, we will also show how to estimate \(Z_{_{*}}\).

Cross-entropy minimization.To measure the quality of the EBM and train its parameters one can use the cross-entropy of the model density \(_{}\) relative to the target density \(_{*}\)

\[H(_{},_{*})=-_{^{d}}_{}(x)_{*}( x)dx= Z_{}+_{^{d}}U_{}(x)_{*}(x)dx\] (3)

where we used the definition of \(_{}\) in (1) to get the second equality. The cross-entropy is related to the Kullback-Leibler divergence via \(H(_{},_{*})=H(_{*})+D_{}(_{*}||_{})\), where \(H(_{*})\) is the entropy of \(_{*}\), and its gradient with respect to the parameter \(\) can be calculated using the identity \(_{} Z_{}=-_{^{d}}_{}U_{ }(x)_{}(x)dx\), to obtain

\[_{}H(_{},_{*})& =_{^{d}}_{}U_{}(x)_{*}(x) dx-_{^{d}}_{}U_{}(x)_{}(x)dx\\ &_{*}[_{}U_{}]-_{ }[_{}U_{}].\] (4)

The cross-entropy is more stringent, and therefore better, than other objectives like the Fisher divergence: for example, unlike the latter, it is sensitive to the relative probability weights of modes on \(_{*}\) separated by low-density regions . Unfortunately, the cross entropy is also much harder to use in practice since evaluating it requires estimating \(Z_{}\), and evaluating its gradient requires calculating the expectation \(_{}[_{}U_{}]\) (in contrast \(_{*}[U_{}]\) and \(_{*}[_{}U_{}]\) can be readily estimated on the data). Typical training methods, e.g. based on the CD or the PCD algorithms, give up on estimating \(Z_{}\) and resort to various approximations to calculate the expectation \(_{}[_{}U_{}]\)--see Appendix A.5 for more discussion about these methods. While these approaches have proven successful in many situations, they are prone to training instabilities that limit their applicability. They also come with no theoretical guarantees in terms of convergence.

## 3 Training via sequential Monte-Carlo methods based on Jarzynski equality

In this section we use tools from nonequilibrium statistical mechanics [34; 56] to write exact expressions for both \(_{}[_{}U_{}]\) and \(Z_{}\) (Sec 3.1) that are amenable to empirical estimation via sequential Monte-Carlo methods , thereby enabling gradient descent-type algorithms for the optimization of EBMs (Sec. 3.2).

### Jarzynski equality in discrete-time

**Proposition 1**.: _Assume that the parameters \(\) are evolved by some time-discrete protocol \(\{_{k}\}_{k_{0}}\) and that (2) hold. Given any \(h(0,L)\), let \(X_{k}^{d}\) and \(A_{k}\) be given by the iteration rule_

\[X_{k+1}=X_{k}-h U_{_{k}}(X_{k})+\,_{k},& X_{0}_{_{0}},\\ A_{k+1}=A_{k}-_{k+1}(X_{k+1},X_{k})+_{k}(X_{k},X_{k+1}),&A_{0}=0, \] (5)

_where \(U_{}(x)\) is the model energy, \(\{_{k}\}_{k_{0}}\) are independent \(N(0_{d},I_{d})\), and we defined_

\[_{k}(x,y)=U_{_{k}}(x)+(y-x) U_{_{k}} (x)+h| U_{_{k}}(x)|^{2}\] (6)

_Then, for all \(k_{0}\),_

\[_{_{k}}[_{}U_{_{k}}]=[ _{}U_{_{k}}(X_{k})e^{A_{k}}]}{[e^{A_{k}}]},  Z_{_{k}}=Z_{_{0}}[e^{A_{k}}]\] (7)

_where the expectations on the right-hand side are over the law of the joint process \((X_{k},A_{k})\)._The proof of the proposition is given in Appendix A.2: for completeness we also give a continuous-time version of this proposition in Appendix A.1. We stress that the inclusion of the weights in (7) is key, as \([_{}U_{_{k}}(X_{k})]_{_{k}}[ _{}U_{_{k}}]\) in general. We also stress that (7) holds _exactly_ despite the fact that for \(h>0\) the iteration step for \(X_{k}\) in (5) is that of the unadjusted Langevin algorithm (ULA) with no Metropolis correction as in MALA [66; 67]. That is, the inclusion of the weights \(A_{k}\) exactly corrects for the biases induced by both the slow mixing and the time-discretization errors in ULA.

Proposition 1 shows that we can evolve the parameters by gradient descent over the cross-entropy by solving (5) concurrently with

\[_{k+1}=_{k}+_{k}_{k},_{k}= -_{}H(_{_{k}},_{*})=[_{ }U_{_{k}}(X_{k})e^{A_{k}}]}{[e^{A_{k}}]}-_{*} [_{}U_{_{k}}],\] (8)

where \(_{k}>0\) is the learning rate and \(k_{0}\) with \(_{0}\) given. We can also replace the gradient step for \(_{k}\) in (8) by any update optimization step (via AdaGrad, ADAM, etc.) that uses as input the gradient \(_{k}\) of the cross-entropy evaluated at \(_{k}\) to get \(_{k+1}\). Assuming that we know \(Z_{_{0}}\) we can track the evolution of the cross-entropy via

\[H(_{_{k}},_{*})=[e^{A_{k}}]+ Z_{_{0}}+ _{*}[U_{_{k}}].\] (9)

### Practical implementation

Empirical estimators and optimization step.We introduce \(N\) independent pairs of walkers and weights, \(\{X_{k}^{i},A_{k}^{i}\}_{i=1}^{N}\), which we evolve independently using (5) for each pair. To evolve \(_{k}\) from some prescribed \(_{0}\) we can then use the empirical version of (8):

\[_{k+1}=_{k}+_{k}}_{k},\] (10)

where \(}_{k}\) is the estimator for the gradient in \(\) of the cross-entropy:

\[}_{k}=^{N}_{}U_{_{k}}(X_ {k}^{i})(A_{k}^{i})}{_{i=1}^{N}(A_{k}^{i})}-_{j=1} ^{n}_{}U_{_{k}}(x_{*}^{j}),\] (11)

These steps are summarized in Algorithm 1, which is a specific instance of a sequential Monte-Carlo algorithm. We can also use mini-batches of \(\{X_{k}^{i},A_{k}^{i}\}_{i=1}^{N}\) and the data set \(\{x_{*}^{j}\}_{i=1}^{n}\) at every iteration (see Algorithm 2 in Appendix A.3), and switch to any optimizer step that uses \(_{k}\) and \(_{k}\) as input to get the updated \(_{k+1}\). During the calculation, we can monitor the evolution of the partition function and the cross-entropy using as estimators

\[_{_{k}}=Z_{_{0}}_{i=1}^{N}(A_{k}^{i}),_{k}=_{_{k}}+_{j=1}^{n}U_{ _{K}}(x_{*}^{j})\] (12)These steps are summarized in Algorithm 1, which is a specific instance of a sequential Monte-Carlo algorithm. We adapted our routine to mini-batches in Algorithm 2 without any explicit additional source of error: in fact, the particles outside the mini-batch have their weights updated too. The only sources of error in these algorithms come from the finite sample sizes, \(N<\) and \(n<\). Regarding \(n\), we may need to add a regularization term in the loss to avoid overfitting: this is standard. Regarding \(N\), we need to make sure that the effective sample size of the walkers remains sufficient during the evolution. This is nontrivial since the \(A_{k}^{i}\)'s will spread away from zero during the optimization, implying that the weights \((A_{k}^{i})\) will become non-uniform, thereby reducing the effective sample size. This is a known issue with sequential Monte-Carlo algorithms that can be alleviated by resampling as discussed next.

Resampling step.A standard quantity to monitor the effective sample size  is the ratio between the square of the empirical mean of the weights and their empirical variance, i.e.

\[_{k}=_{i=1}^{N}(A_{k}^{i}))^{2}}{N^ {-1}_{i=1}^{N}(2A_{k}^{i})}(0,1]\] (13)

The effective sample size of the \(N\) walkers is \(_{k}N\). Initially, since \(A_{0}^{i}=0\), \(_{0}=1\), but it decreases with \(k\). At each iteration \(k_{r}\) such that \(_{k_{r}}<c_{k_{r}}\), where \(\{c_{k}\}_{k}\) is a set of predefined positive constants in \((0,1)\), we then:

1. Resample the walkers \(X_{k_{r}}^{i}\) using \(p_{k_{r}}^{i}=e^{A_{k_{r}}^{i}}/_{j=1}^{N}e^{A_{k_{r}}^{j}}\) as probability to pick walker \(i\);
2. Reset \(A_{k_{r}}^{i}=0\);
3. Use the update \(Z_{_{k}}=Z_{_{k_{r}}}N^{-1}_{i=1}^{N}(A_{k}^{i})\) for \(k k_{r}\) until the next resampling step.

This resampling is standard  and can be done with various levels of sophistication, as discussed in Appendix A.4. Other criteria, based e.g. on the entropy of the weights, are also possible, see e.g. .

Generative modeling.During the training stage, i.e. as the weights \(A_{k}^{i}\) evolve, the algorithm produces weighted samples \(X_{k}^{i}\). At any iteration, however, equal-weight samples can be generated by resampling. Notice that, even if we no longer evolve the model parameters \(_{k}\), the algorithm is such that it removes the bias from ULA coming from \(h>0\) - this bias removal is not perfect, again because \(N\) is finite, but this can be controlled by increasing \(N\) at the stage when the EBM is used as a generative model.

## 4 Numerical experiments

### Gaussian Mixtures

In this section, we use a synthetic model to illustrate the advantages of our approach. Specifically, we assume that the data is drawn from the Gaussian mixture density with two modes given by

\[_{*}(x)=Z_{*}^{-1}(e^{-|x-a_{*}|^{2}}+e^{-|x- b_{*}|^{2}-z_{*}}), Z_{*}=(2)^{d/2}(1+e^{-z_{*}})\] (14)

where \(a_{*},b_{*}^{d}\) specify the means of the two modes and \(z_{*}\) controls their relative weights \(p_{*}=1/(1+e^{-z^{*}})\) and \(q_{*}=1-p_{*}=e^{-z_{*}}/(1+e^{-z^{*}})\). The values of \(a_{*},b_{*},z_{*}\) are carefully chosen such that the modes are well separated and the energy barrier between the modes is high enough such that jumps of the walkers between the modes are not observed during the simulation with ULA. Consistent with (14) we use an EBM with

\[U_{}(x)=-(e^{-|x-a|^{2}}+e^{-|x-b|^{2}-z }),\] (15)

where \(=(a,b,z)\) are the parameters to be optimized. We choose this model as it allows us to calculate the partition function of the model at any value of the parameters, \(Z_{}=(2)^{d/2}(1+e^{-z})\). We use this information as a benchmark to compare the prediction with those produced by our method.

In our numerical experiments, we set \(d=50\), use \(N=10^{5}\) walkers with a mini-batch of \(N^{}=10^{4}\) and \(n=10^{5}\) data points. We initialize the model at \(_{0}=(a_{0},b_{0},z_{0})\) with \(a_{0}\) and \(b_{0}\) drawn from an \(N(0,^{2}I_{d})\) with \(=0.1\) and \(z_{0}=0\), meaning that the initial \(_{_{0}}\) is close to the PDF of an \(N(0,I_{d})\). The training is performed using Algorithm 2 with \(h=0.1\) and fixed learning rates \(_{k}=0.2\) for \(a_{k}\) and \(b_{k}\) and \(_{k}=1\) for \(z_{k}\). We perform the resampling step by monitoring \(ESS_{k}\) defined in (13) with constant \(1/c_{k}=1.05\) and using the systematic method. We also compare our results to those obtained using ULA with these same parameters (which is akin to training with the PCD algorithm) and with those obtained with the CD algorithm: in the latter case, we evolve the walkers by ULA with \(h=0.1\) for 4 steps between resets at the data points, and we adjust the learning rates by multiplying them by a factor 10. In all cases, we use the full batches of walkers, weights, and data points to estimate the empirical averages. We also use (12) to estimate the cross-entropy \(H(_{_{k}},_{*})\) during training by our method (CD and PCD do not provide estimates for these quantities), and in all cases compare the result with the estimate

\[_{k}=((2)^{d/2}(1+e^{-z_{*}}))-_{j=1}^{n}(e^{-|x_{*}^{j}-a_{k}|^{2}}+e^{-|x_{*} ^{j}-b_{k}|^{2}-z_{k}})\] (16)

The results are shown in Figure 1. As can be seen, all three methods learn well the values of \(a_{*}\) and \(b_{*}\) specifying the positions of the modes. However, only our approach learns the value of \(z\) specifying their relative weights. In contrast, the PCD algorithm leads to mode collapse, consistent with the theoretical explanation given in Appendix C.1, and the CD algorithm returns a biased value of \(z\), consistent with the fact that it effectively uses the Fisher divergence as the objective. The results also show that the cross-entropy decreases with our approach, but bounces back up with the PCD algorithm and stalls with the CD algorithms: this is consistent with the fact that only our approach actually performs the GD on the cross-entropy, which, unlike the other algorithms, our approach estimates accurately during the training.

### Mnist

Next, we perform empirical experiments on the MNIST dataset to answer the following question: when it comes to high-dimensional datasets with multiple modes, can our method produces an EBM that generates high-quality samples and captures the relative weights of the modes accurately?

To this end, we select a subset of MNIST consisting of only three digits: \(2\), \(3\), and \(6\). Then, we choose \(5600\) images of label \(2\), \(2800\) images of label \(3\), and \(1400\) images of label \(6\) from the training set (for a total of \(n=9800\) data points), so that in this manufactured dataset the digits are in have respective weights \(4/7\), \(2/7\), and \(1/7\).

Figure 1: _GMM experiments:_ Evolution of the parameters and the cross entropy during training by Algorithm 2, PCD, and CD. Average of \(20\) runs. _Left panels:_ evolution of \(p_{k}=1/(1+e^{-z_{k}})\); _middle panel:_ evolution of \(a_{k}\) and \(b_{k}\); _right panel:_ evolution of the Kullback-Leibler divergence. All three methods capture the location of the modes accurately, but only ours get the relative weights of these modes accurately (whereas PCD leads to mode collapse, and CD to an inaccurate estimate). Our method is also the only one that allows for direct estimation of the cross-entropy during training, and the only one performing GD on this cross-entropy–for better visualization we subtract the entropy of the target \(H(_{*})\) and plot the Kullback-Leibler divergence instead of the cross-entropy.

We train two EBMs to represent this data set, the first using our Algorithm 2, and the second using the PCD Algorithm 4. We represent the energy using a simple six-layer convolutional neural network with the swish activation and about \(77K\) parameters. We use the ADAM optimizer for the training with a learning rate starting from \(10^{-4}\) and linearly decaying to \(10^{-10}\) until the final training step. The sample size of the walkers is set to \(N=1024\) and it is fixed throughout the training.

The results are shown in Figure 2. Both our Algorithm 1 and the PCD Algorithm 4 generate images of reasonable quality; as discussed in Appendix B.2, we observe that the Jarzynski weights can help us track the quality of generated images, and the resampling step is also helpful for improving this quality as well as the learned energy function.

However, the real difference between the methods comes when we look at the relative proportion of the digits generated in comparison to those in the data set. In Figure 3, we show the relative error on the weight estimation of each mode obtained by using a classifier pre-trained on the data set (see Appendix B.2 for details). Although the EBM trained with the PCD algorithm can discover all the modes and generate images of all three digits present in the training set, it cannot accurately recover the relative proportions of each digit. In contrast, our method is successful in both mode exploration and weight recovery.

Unlike in the numerical experiments done on Gaussian mixtures with teacher-student models, for the MNIST dataset, we cannot estimate the KL divergence throughout the training as we do not know the normalization constant of the true data distribution. Nevertheless, we can still use the formula (16) to track the cross-entropy between the data distribution and the walker distribution a plot of which is given in the right panel Figure 4 for the mini-batched training algorithm (Algorithm 2).

In these experiments with MNIST, we used the adaptive resampling scheme described after equation 13. In practice, we observed that few resamplings are needed during training, and they can often by avoided altogether if the learning rate is sufficiently small. For example, in the experiment reported in the left panel of Figure 4) a single resampling step was made. We also found empirically that the results are insensitive to the choice of of the parameters \(c_{k}\) used for resampling: the rule of

Figure 3: _MNIST:_ Relative error of the weight estimation of the three modes (i.e. three digits). Our method outperforms the PCD algorithm in terms of recovery of the weight of each mode.

Figure 2: _MNIST: Left panel:_ Examples of images generated by our method right after resampling in the last epoch. _Middle panel:_ Images randomly selected from the test dataset of MNIST. _Right panel:_ Examples of images generated by training using the persistent contrastive divergence (PCD) algorithm.

thumb we found is to not resample at the beginning of training, to avoid possible mode collapse, and use resampling towards the end, to improve the image quality.

### Cifar-10

We perform an empirical evaluation of our method on the full CIFAR-10 (32 \(\) 32) image dataset. We use the setup of , with the same neural architecture with \(n_{f}=128\) features, and compare the results obtained with our approach with mini-batching (Algorithm 2) to those obtained with PCD and PCD with data augmentation of  (which consists of a combination of color distortion, horizontal flip, rescaling, and Gaussian blur augmentations, to help the mixing of the MCMC sampling and stabilize training).

Figure 4: _MNIST dataset: Left Panel:_ Convergence of the cross-entropy estimated by using formulae (12) with the mini-batched algorithm (Algorithm 2). _Right Panel:_ Evolution of the effective sample size (ESS) defined in (13) – here resampling was started after \(240\) epochs (with \(c_{k}=0\) before and \(c_{k}=0.5\) afterwards), and occurred only once immediately after being switched on.

Figure 5: _CIFAR-10 dataset: Left panel:_ images generated by training with Algorithm 2. _Right panel_: Images generated from the PCD with mini-batches. Up to date images available at https://github.com/submissionx12/EBMs_Jarzynski.

The code used to perform these new experiments is available in the anonymized GitHub referenced in our paper. The hyperparameters are the same in all cases: we take \(N=4096\) Langevin walkers with a mini-batch size \(N^{}=256\). We use the Adam optimizer with learning rate \(=10^{-4}\) and inject a Gaussian noise of standard deviation \(=3 10^{-2}\) to the dataset while performing gradient clipping in Langevin sampling for better performance. All the experiments were performed on a single A100 GPU. Training for 600 epochs took about 34 hours with the PCD algorithm (w/ and w/o data augmentation) and about 36 hours with our method.

Some of the images generated by our method are shown in Figure 5. We also quantitatively evaluate the performance of our models with the commonly used metrics (e.g. FID and Inception Score): the results are given in Table 1. They indicate that our method can achieve slightly better performance than the PCD algorithms w/ and w/o data augmentation at a similar computational cost. Furthermore, these results on CIFAR-10 suggest that our method scales well to complicated training tasks on more realistic data sets.

It is worth stressing that a key component of the success of our method is the resampling step, as discussed in B.3 where we plot the Effective Sample Size (ESS) in Figure 13.

## 5 Concluding Remarks

In this work, we proposed a simple modification of the persistent contrastive divergence algorithm which corrects its biases and provably allows one to minimize the cross-entropy between the model and the target densities to train EBMs. Our approach rests on results from non-equilibrium thermodynamics (in particular, Jarzynski equality) that show how to take exact expectations over a probability density function (PDF) that is evolving in time--in the present context the PDF is that of the EBM, and it is changing as we train the model parameters. These formulas naturally lend themselves to practical implementation using sequential Monte Carlo sampling methods. The only difference with respect to training methods based on ULA is that the proposed approach maintains a set of weights associated with every walker. These weights correct for the bias induced by the evolution of the energy, and they also give a way to assess the quality of the samples generated by the method. On synthetic examples using Gaussian mixture densities as well as on some simple data sets involving MNIST, we showed that our method dodges some common drawbacks of other EBM training methods. In particular, it is able to learn the relative weights of various high-density regions in a multimodal distribution, thus ensuring a higher level of fairness than common EBM training techniques based on contrastive divergence.

To focus on the core idea, in the present study we put aside tuning considerations and did not sweep over the various hyperparameters and design choices, like learning rates, number of walkers, or resampling criteria and strategies. These considerations could greatly enhance the performance of our method. We believe that our results show that the method deserves deeper practical investigation on more realistic datasets and with more complex neural architectures.

  
**Method** & _FID_ & _Inception Score (IS)_ \\  PCD with mini-batches & \(38.25\) & \(5.96\) \\ PCD with mini-batches and data augmentation & \(36.43\) & \(6.54\) \\ Algorithm 2 with multinomial resampling & **32.18** & **6.88** \\ Algorithm 2 with systematic resampling & **30.24** & **6.97** \\   

Table 1: textitCIFAR-10 dataset: Comparison of FID and Inception Score (IS) for PCD and Algorithm 2. Experiments performed using the neural architecture in  to model the energy.