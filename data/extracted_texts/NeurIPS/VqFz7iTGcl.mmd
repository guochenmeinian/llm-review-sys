# When is an Embedder More Promising

than Another?

Maxime Darrin\({}^{1,2,3,4}\) \({}^{}\)Philippe Formont\({}^{1,2,4,5}\) Ismail Ben Ayed\({}^{1,5}\)

**Jackie Chi Kit Cheung\({}^{2,3}\) Pablo Piantanida\({}^{1,2,4,6}\)**

\({}^{1}\)International Laboratory on Learning Systems, \({}^{2}\)Mila - Quebec AI Institute, \({}^{3}\)McGill University

\({}^{4}\)Universite Paris-Saclay, \({}^{5}\)ETS Montreal, \({}^{6}\)CNRS, CentraleSupelec, \({}^{}\)equal contribution

maxime.darrin@mila.quebec, philippe.formont@mila.quebec

###### Abstract

Embedders play a central role in machine learning, projecting any object into numerical representations that can, in turn, be leveraged to perform various downstream tasks. The evaluation of embedding models typically depends on domain-specific empirical approaches utilizing downstream tasks, primarily because of the lack of a standardized framework for comparison. However, acquiring adequately large and representative datasets for conducting these assessments is not always viable and can prove to be prohibitively expensive and time-consuming. In this paper, we present a unified approach to evaluate embedders. First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness. We then leverage these concepts to devise a tractable comparison criterion (information sufficiency), leading to a task-agnostic and self-supervised ranking procedure. We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology. This effectively offers practitioners a valuable tool for prioritizing model trials.1

## 1 Introduction

Embeddings are a prominent tool in machine learning and are used in multiple fields, such as natural language processing , computer vision  or bioinformatics . These models embed objects such as images, texts, or molecules into numerical representations that can be used to perform numerous downstream tasks by preserving key features of the object .

Depending on the data modalities, intended purpose, and available resources, embedders showcase a wide variety of architectures, training settings (unsupervised, supervised, self-supervised, etc.), objectives (masked language modeling, contrastive learning, etc.) , and datasets . And more recently, foundation models have become a natural starting point to create embedders .

This diversity and variety of options makes selecting the most promising embedders for a data distribution challenging . Most work evaluates embedders focusing on the performance they enable on a finite set of downstream tasks . Nevertheless, this evaluation process encounters two primary limitations. Firstly, it is **not scalable** concerning the number of embedders and tasks, as it requires fitting a downstream model for each task. Hence, prioritizing the evaluation of the most promising models becomes essential to mitigate computational costs. Secondly, **acquiring high-quality labels** can be a **time-consuming** and notably **expensive** endeavor in various applications. To overcome these limitations, in this paper, we explore task-agnostic evaluation metricsfor embedders relying solely on pairwise comparisons between embedders, i.e., without the need for labeled data in downstream tasks.

More specifically, our contributions can be summarized as follows:

1. **An innovative theoretical framework for comparing embedding models:** We cast the problem of ranking embedders into the noisy communication channels ordering (Sec. 2.2) and statistical experiments comparison settings (Sec. 2.3). We exploit the notions of sufficiency and informativeness and relax them, leveraging the concept of deficiency introduced by Le Cam  (Sec. 2.4), which is reframed to account for concepts and features. These concepts provide us with tools to establish an embedder ranking.
2. **A practical relaxation:** Estimating deficiency presents significant challenges. We propose the concept of information sufficiency (IS), which quantifies the information required to simulate one embedder from another (Sec. 3). We estimate the information efficiency to get a task-agnostic and label-free comparison tool for embedders evaluation.
3. **Extensive experimental validation:** The expected IS correlates with the ability of embedders to enable a wide range of downstream tasks. In NLP (Sec. 5) and molecular modeling (Sec. 6), our method respectively achieves Spearman ranking correlations of \(0.90\) (\(56\) tasks) and \(0.94\) (\(31\) tasks); providing an efficient model trial prioritization tool for practitioners.

### Related works

Embedding evaluation.Embedding evaluation is mainly performed based on a limited set of downstream tasks [22; 91; 81; 24], for which the embeddings are used as inputs to smaller models. Therefore, embedders evaluation is field- and task-specific. In NLP, [41; 85] they rely on a limited set of tasks; more recently, the Massive Text Embedding Benchmark (MTEB)  followed this task-oriented trend and offered standardized test bed for embedders encompassing various downstream tasks in NP. Devising statistical tests to compare models and learning algorithms has a long history . However, most works propose statistical tests relying on the performance of the downstream tasks of interests [60; 11]. Other works study the expressiveness of embedders and connect it to performance on downstream tasks [107; 25], but mostly focus on geometrical properties of the high dimensional representation in self-supervised learning settings [2; 42; 45].

**Probing.** While probing methods do not aim at comparing embedders, they evaluate their representations to discover what these models have learned. They train small models on the internal representations of large models to perform specific downstream tasks. These procedures allow researchers to assess what information is present and recoverable from these embeddings [10; 1; 88; 84]. Other work proposed measuring mutual information (MI) between internal representations and labels. It has been used to evaluate the difficulty of a dataset as the predictiveness of the labels using the features . For instance,  evaluates the utility of representations in astrophysics to predict physical properties. Following this trend,  leverages the point-wise MI between Gaussian distributions to evaluate text-to-images and image-to-text generative models. However, none of these methods have focused on comparing embedders in the general case to the best of our knowledge.

## 2 Theoretical Foundations for Comparing Embedding Models

### Background and notation

We assume that all considered spaces are standard Borel  Each such space \(\) is equipped with its Borel \(\)-algebra \(()\). The set of all probability measures on \(\) is denoted by \(()\) The total variation distance between \(P\) and \(Q\) is denoted by \(\|P-Q\|_{}\). Given a joint probability measure \(P_{XY}\) induced by two random variables \(X\) and \(U\), the Mutual Information  is denoted by \(I(X;U)\). A Markov (or transition probability) kernel between \(\) and \(\) is a mapping \(P_{U|X}:()\). The space of all such \(P_{U|X}\) is denoted by \((|)\) and \((M P_{U|X})(V|x)\) indicates the composition of Markov kernels \(M(|)\) and \(P_{U|X}(|)\). For further details, refer to Appendix A.

### Sufficiency and informativeness ordering of embedding models

We aim to compare embedding models without relying on labeled data for downstream tasks. Let us consider two embedding models represented by their Markov kernels (or transition probabilities)\(P_{U|X}(|)\) and \(P_{V|X}(|)\), any target set \(\) of (discrete or continuous) concepts and feature space \(\) with joint probability measure \(P_{YX}()\) induced by random variables \((Y,X)\), as illustrated in Figure 1. First, we study the question:

**What sufficient conditions must be met by the embedding model \(U\) relative to \(V\) to guarantee that \(I(Y;U) I(Y;V)\) for all distributions \(P_{YX}\)?**

From an information-theoretic perspective , the quality of an embedding model can be likened to the capacity of a noisy communication channel with an uncoded input (e.g., a text, a molecule...), where a downstream task of interest is performed at the output (the embedding) of the channel. Let \(Y\) represent the message (the source) to be communicated over both channels; \(X\) represents the transmitted signal; and \(P_{U|X}\) and \(P_{V|X}\) the communication channels with outputs \(U\) and \(V\), respectively. This process is illustrated in Figure 1. It naturally satisfies the Markov chain \(Y X(U,V)\). A desirable property is that the embedding models \(U\) and \(V\) retain as much pertinent information as feasible to predict \(Y\).

We shall be interested in the underlying information relationships between those embedding models that can be interpreted as channel \(U\) being "more informative" for communicating \(Y\) than channel \(V\). The first attempt to introduce an ordering between communication channels appears in Shannon . Korner and Marton later introduced  the concepts of "less noisy" (or more informative) and "degraded" (or sufficiency) orderings between channels.

**Definition 1** (Sufficiency and informativeness orderings ).: Let \(P_{U|X}\) and \(P_{V|X}\) be two Markov kernels (embedding models).

* **Sufficiency \(U_{S}V\).** The embedding model \(P_{U|X}\) is said to be "sufficient" for the embedding model \(P_{U|X}\) (or \(V\) to be degraded w.r.t. \(U\)) if and only if there exists another Markov kernel \(M(|)\) such that \(\|M P_{U|X}-P_{V|X}\|_{}=0\), i.e. \(V\) can be simulated from \(U\) using \(M\) without information loss).
* **More informative \(U_{I}V\).** The embedding model \(P_{U|X}\) is said to be "more informative" (or less noisy) than \(P_{V|X}\) if and only if the embedding models always satisfy the inequality \[I(Y;U) I(Y;V), P_{YX}( ).\]

**Proposition 1** (Relationships of sufficiency and information).: _The following relationships hold:_

1. _Sufficiency_ \(\) _informativeness. If the embedding model_ \(P_{U|X}\) _is sufficient for the embedding model_ \(P_{V|X}\)_, i.e._ \(U_{S}V\)_, then_ \(U_{I}V\)_. However,_ **Informativeness \(\) _sufficiency._**__
2. _Informativeness_ \(\) _higher capacity to distinguish concepts. If the embedding model_ \(P_{U|X}\) _is more informative than embedding model_ \(P_{V|X}\)_, i.e._ \(U_{I}V\)_, then_ \[P_{U|Y}(|y_{0})\|P_{U|Y}(|y_{1}) P_{V|Y}(|y_{0})\|P_{V|Y}(|y_{1}),\] _for any pair of concepts_ \((y_{0},y_{1})\) _and all probability distributions_ \(P_{YX}\)_._

_Remark 1_.: An immediate consequence of claim (i) is that the sufficient condition between embedding models implies that the embedding model \(U\) is more informative than the embedding model \(V\) relative to all target concepts in \(\) over all possible data distributions: \(I(Y;U) I(Y;V)\), for all probability distributions \(P_{YX}\).

Although \(U\) being more informative than \(V\) does not necessarily imply \(U_{S}V\); (ii) states that being more informative ensures a higher statistical discrimination capacity between any pairs of target concepts (for further discussion, see Sec. B.2).

Motivated by the concepts of sufficiency and informativeness between embedding models, we can inquire about their statistical consequences for a learner conducting an inference task on these embeddings. More precisely, given a finite set of concepts \(\), **if \(U_{S}V\), is the Bayes risk expected to be smaller when the inference is based on \(U\) than when it is based on \(V\)?**

Figure 1: Communicating a concept \(y\) over two embedding models with prediction \(_{V}(V)\).

### Comparing statistical experiments with embedding Models

The pursuit of comparing statistical experiments originated from the seminal paper by Bohnenblust, Shapley, and Sherman , followed by subsequent contributions by Blackwell [13; 14]. They formally established the relationships between sufficiency (Def. 1) and inference procedures.

In our framework, a statistical experiment  consists of a mathematical abstraction (see Appendix A for further details) intended to represent a downstream task where a learner aims at inferring a concept \(y\) from the embeddings \(U\) or \(V\). Deciding what embedder should be used to perform a given task is too general. In this work, we do not take into account the computational cost or the size of an embedder and solely focus on the following question:

**What are the necessary and sufficient conditions that ensure that employing the embedding \(U\) for any task \(P_{YX}\) leads to lower risk compared to using the embedding \(V\)?**

Drawing parallels with the theoretical framework established for comparing statistical experiments, a relationship can be derived between the concept of sufficiency and the expected risk for a specific task (see Sec. B.5 for further discussion).

We concentrate on the scenario where \(\) consists of a finite number of concepts (e.g., classification tasks), as it is a significant case in its own right  and provide fundamental insights for the present work. The next Proposition states an important **relation between the concept of sufficiency and the expected Bayes risk on any classification task.**

**Proposition 2** (Comparison of embedding models through Bayes risks).: _Given two embedding models \(P_{U|X}(|)\) and \(P_{V|X}(|)\), the following statements are equivalent:_

1. _The embedding model_ \(P_{U|X}\) _is sufficient relative to_ \(P_{V|X}\)_, i.e._ \(U_{S}V\)_._
2. _For all conditional probability measures_ \(P_{Y|X}\) _on finite alphabet_ \(\)_, the Bayes risks satisfy_ \[_{_{U}:()}_{U} Y _{_{V}:()} _{V} Y,\] _where_ \(_{U}\) _and_ \(_{V}\) _are distributed according to_ \(_{U}(U)\) _and_ \(_{V}(V)\)_, respectively._

_Remark 2_.: In other words, if we can fully simulate an embedder \(V\) from another embedder \(U\), the expected risk across all potential classification tasks cannot be greater when using \(U\) compared to \(V\). The proof of this Proposition is given in Sec. B.3. It is worth mentioning that various versions of this result are available in the literature . However, our extension here, in a simpler setting, incorporates concepts and features into the experiment comparison framework.

### Challenges in ranking embedding models and their deficiency

According to the notion of "sufficiency", we can distinguish the three following possibilities:

* Equivalence: \(U_{S}V\) and \(V_{S}U\) denoted \(U V\); \(U\) and \(V\) can simulate each other.
* Comparability: \(U_{S}V\) but \(V_{S}U\) only \(V\) can be simulated from \(U\).
* Non-comparability: \(U_{S}V\) and \(V_{S}U\), neither \(U\) nor \(V\) can simulate each other.

Our results up to now only account for the two first possibilities. However, two embedders are generally not comparable (Sec. B.4). This issue was addressed by Le Cam , who introduced the notion of "deficiency".

**Definition 2**.: The deficiency \((P_{U|X} P_{V|X})\) of \(P_{V|X}\) relative to \(P_{U|X}\) is defined as 

\[(P_{U|X} P_{V|X})_{M(| )}\|M\!\!P_{U|X}-P_{V|X}\|_{},\]

where the infimum is taken over all Markov kernels (or transition probabilities) \(M(|)\), mapping stochastically \(\) and \(\), and \(\) measures error between the simulated and true embedders.

\(\) **indicates how well one model can be reconstructed from the other**, it induces a natural relaxation of the sufficiency where the reconstruction does not have to be perfect2 for us to obtain guarantees onthe downstream tasks performance (See Corollary 1). It avoids the non-comparability problem by evaluating **"how much information" we lose when passing from one model to the other one**.

Le Cam  showed that, for a given task \(Y\), the deficiency \((P_{U|Y} P_{V|Y})\) is directly related to the expected Bayes risks on the task (see Sec. B.6). We extend this result to the comparison of two embedding models \(P_{U|X}\) and \(P_{V|X}\) in a task-agnostic manner and build the relation to the expected Bayes risks for any classification task \(Y\).

**Corollary 1**.: _Given two embedding models \(P_{U|X}\) and \(P_{V|X}\) satisfying:_

1. _The deficiency_ \((P_{U|X} P_{V|X})\)_._
2. _For any conditional distribution_ \(P_{V|X}\) _on finite alphabets_ \(\)_,_ \[_{_{U}:()}_{U} Y -_{_{V}:( )}_{V} Y.\]

_Statement (ii) implies (i) provided that \( 2||\) and conversely, (i) implies (ii) provided that \(\)._

The proof of this Corollary is relegated to Sec. B.3.

_Remark 3_.: In particular, we can infer that for any classification task \(Y\), the expected Bayes risk of the embedding model \(U\), denoted by \(_{U}\), is upper bounded by the expected Bayes risk of the embedding model \(V\), denoted by \(_{V}\):

\[_{U}-_{V}(P_{U|X} P_{V|X}), $,}\]

and similarly, \(|_{U}-_{V}|(P_{U|X} P_{V| X}),(P_{V|X} P_{U|X})}\), for all conditional distributions \(P_{Y|X}\). If both deficiencies are small, the resulting expected Bayes risks of the embedding models \(U\) and \(V\) will be close to each other for any target task \(Y\).

## 3 Quantifying Information Sufficiency Between Embedding Models

We want to compare embedding models using the concept of deficiency, leveraging Prop. 2 and Corollary 1. These propositions suggest that the performance on any classification task of an embedding model \(U\) relative to the model \(V\) is bounded by \((P_{U|X} P_{V|X})\). However, estimating the deficiency from data samples is notably challenging , and while upper bounds derivation exists, they do not necessarily make it tractable.

### Estimating Information Sufficiency

The deficiency \((P_{U|X} P_{V|X})\) between two embedding models \(P_{U|X}\) and \(P_{V|X}\), measures how well \(U\) can be used to simulate \(V\) using a Markov kernel \(M(|)\). This section aims to build a tractable proxy for this reconstruction cost. To this end, we estimate how much we can reduce the uncertainty about \(Z\) by observing \(U\) by learning an appropriate Markov kernel. This corresponds to the information sufficiency  and can be interpreted as the information-theoretic counterpart of the deficiency. The information deficiency between \(U\) and \(V\) is then defined as:

**Definition 3** (Information sufficiency).: The information sufficiency \(_{S}(U V)\), relative to parametric classes of distributions \(_{}()\) and \(_{}(|)\) (multivariate Gaussian mixtures ) is defined:

\[_{S}(U V)_{}( )}[- f(V)]}_{}-[_{M_{}( |)}[- M(V|U)|U]]}_{}.\] (1)

_Remark 4_.: When the information sufficiency \(_{S}(U V)\) is large, it signifies that \(U\) offers a substantial amount of information to simulate \(V\), a proxy for a small deficiency. Conversely, when \(_{S}(U V)\) is lower, it implies that the channel \(P_{V|Y}\) is subject to considerable noise or randomness, leading to a greater loss of statistical information.

We hence attempt to simulate \(V\) from \(U\) by learning a Markov kernel \(M_{}(|)\), via a mixture of multivariate Gaussians, and measure the uncertainty reduction it induces.

Pairwise embedder evaluation.For set of embbeders \((Z_{k})_{k}\) represented by their Markov kernels \(\{P_{Z_{k}|X}\}_{k}\), we compute the pairwise information sufficiency \(_{S}(Z_{k} Z_{l})\). The pairwise information sufficiency matrix defines the adjacency matrix of a directed graph of embedders (Figure 2). Corollary 1 shows that embedders sharing high information sufficiency are expected to perform similarly on any downstream tasks, motivating the identification of communities in the graph. While the graph construction is in \((N^{2})\); where \(N\) is the number of embedders, it is in practice tractable for a reasonable number of embedders (refer to Sec. E.6) for more details).

Practical embedding evaluation.We construct the set of all information sufficiency using \(Z_{k}\): \(_{_{S}}(k)=\{_{S}(Z_{k} Z_{l })\}_{l k}\). We build our information sufficiency score (\(_{S}}\) score) by taking the median of \(_{_{S}}(k)\). Details on the \(_{S}}\) score's estimation can be found in Sec. E.1.

## 4 Experimental Setup

We aim to evaluate the practical utility of the \(_{S}}\) score to rank and select the best embedders for a given data distribution. We compare this ranking to those obtained on various downstream tasks. Our experimental protocol is divided into three main steps:

1. We evaluate the \(_{S}}\) score of the models by identifying a large and diverse dataset that is supposed to be representative of the data distribution of interest.
2. We train a small feedforward neural network (\(_{Z_{k}}\)) per embedder \(P_{Z_{k}|X}\) to perform each downstream task and record its performances (\(R^{2}\) score for regression, AUROC/accuracy for binary/multiclass classification).
3. We compare the models' performances on the downstream tasks and the \(_{S}}\) score by measuring three types of correlations: the Pearson correlation, the Spearman correlation, and the Kendall-Tau coefficient.3(See Sec. E.5 for additional baselines).

[FOOTNOTE:3

Figure 3: Correlation between \(_{S}}\) scores and downstream task performances in (a) NLP and (b) Molecular Modelling. \(_{p}\) is the Pearson correlation, \(_{s}\) the spearman correlation, and \(\) is the Kendall-Tau coefficient. See Sec. C.3.1 for unaggregated results in NLP and Sec. D.3 in molecular modeling.

Figure 2: Pairwise \(_{S}\) for text embedders.

Text Embeddings Evaluation

### Experimental setting

Embedders & Datasets.We compared \(34\) models with different training objectives, training datasets, and architectures. We included embedders derived from modern LLM such as LLaMA , Mistral , Gemma , Croissant  and T5 encoders ; common embedders derived from BERT architectures  or RobERTa  and embedders trained on specific embeddings objectives such Angle , Stella4, E5 models , LaBSE . A comprehensive list of the models can be found in Sec. C.1, Tab. 1 with their main characteristics and links to the Huggingface Hub for reproducibility. We used them to extract embeddings for many different datasets from the MTEB benchmark such as Banking77 , Sickr , Amazon polarity , SNLI  and IMDB . We provide the datasets statistics in Sec. C.1, Tab. 2.

Downstream tasks evaluation.We rely on the results released on the MTEB leaderboard5 and compare our rankings to the rankings and scores obtained by the different models on the different tasks. We evaluate additional tasks that are not included in the MTEB benchmark, such as tweet_eval , DAIR Emotion , agnews topic classification , Clinc intent detection  PAWS-X  and Rotten Tomatoes .

### Model's Information Sufficiency analysis

Correlation with downstream tasks performance.The MTEB Benchmark offers a natural starting point to compare models' ranking according to their performance on downstream tasks and their \(_{S}}\) score. In Figure 2(c), we show that the \(_{S}}\) score of an embedder correlates positively with its performance on a wide range of downstream tasks, from classification and similarity tasks to retrieval and clustering tasks. Overall, our \(_{S}}\) score correlates strongly with MTEB's average score (Spearman correlation of \(0.90\) and a Pearson correlation of \(0.94\), see Figure 2(c)) and with the subtask

Figure 4: Figure 3(a), presents the information sufficiency directed graph and the induced communities. Figure 3(b) displays the performance on additional downstream tasks and models not evaluated in the MTEB leaderboard. Figure 3(c) shows that instruction finetuning positively impacts the models’ performance on the downstream tasks and that this improvement is captured by \(_{S}}\).

performance Figure 2(a)). We extended our experiments to a more extensive set of models not included in the MTEB benchmark and observed a similar trend (Figure 3(b)). Per-datasets results are reported in Sec. C.3.1 and ablations in Sec. C.3.2. All our results show that our estimation of the information sufficiency between models is a good proxy for the performance of the models on a wide range of tasks.

Embedder communities.The pair-wise information sufficiency evaluation between the models can be used to cluster them into communities (Figure 3(a), Figure 2)6. We observe that the extracted clusters group together models that are similar in their training objectives and architectures. LLM-based models such as LLaMA, Mistral, Gemma, and Croissant are clustered together, while BERT-based models share another cluster. Similarly, models trained specifically for embedding purposes, such as UAE-Large-V1 and ember-v1, are grouped together. This suggests that the ordering induced by information sufficiency is meaningful and can be used to identify models with similar properties and behaviors. Consistently with Corollary 1, we observe that the performance of the models on the downstream tasks is similar within the same cluster (Figure C.3.5). In addition, we found that it captures improvements by both steps of pretraining and instruction fine-tuning (Figure 3(c), Sec. C.3.2)

## 6 Molecular Modeling

### Experimental setting

**Embedders.** To process molecular data, embedders can leverage different representations of the molecules, providing an interesting benchmark to evaluate the \(_{S}}\) score. We evaluated models derived from the molecular representation learning literature, summed up in Sec. D.1. We considered various input modalities such as string representations (SMILES , SELFIES ), 2D-graphs by using graph neural networks (GNNs), and 3D-representations (using the TorchMD-net architecture ). We added a randomly initialized baseline GNN model that was not trained on any dataset.

**Datasets.** To evaluate the information sufficiency between embedders, we compared the models on the ZINC 250k dataset, designed to gather compounds that could be relevant to a wide range of therapeutic projects. This dataset contains 250k commercially available compounds meant to be used in diverse therapeutic projects.

**Downstream tasks.** We evaluated the embedders on 31 downstream tasks extracted from the Therapeutic Data Commons  platform. This section focuses on ADMET tasks (Absorption, Distribution, Metabolism, Excretion, and Toxicity). Results on Drug-Target interaction tasks can be found in Sec. D.4. Datasets collected are split into a training, validation, and test set, following the scaffold-split strategy, further described in see Sec. D.3.

### Model's Information Sufficiency analysis

**Global results.** The \(_{S}}\) score ranking is consistent with the results of the embedders on the ADMET downstream tasks, achieving a Spearman correlation of 0.95 and a Kendall-tau coefficient of 0.80, as reported in Figure 2(d). Detailed results for each of the 31 tasks are available in Sec. D.3 in Tab. 6. Table 2(b) shows the correlation between the \(_{S}}\) score rankings and the performances obtained on the ADMET tasks within each category. High correlations are achieved within most task categories, especially when large tasks are available (containing an important number of molecules). On excretion tasks, the correlation is lower (below \(0.8\)), which can be explained by the fact that these tasks are the most challenging regression tasks available, where the fine-tuned models reach the lowest \(R^{2}\) scores between \(0\) and \(0.2\) (see Sec. D.3).

**Most / Least promising models.** We observe in Figure 4(b) that the most promising models are the (_X_bm)Bert-MTR models7 and MolR, the former trained on SMILES representations to predict a variety of computationally available molecular properties, and the latter trained on 2D graphs to preserve equivalence of molecules w.r.t chemical reactions. Surprisingly, these models share high predictive mutual information (being assigned to the same Louvain community in Figure 4(a)),suggesting that they capture similar information despite significant differences in their training methods. These models also appear to be the most competitive on the ADMET tasks. On the other hand, and consistently with Sun _et al._'s observation, training methods for 2D-GNNs such as following attribute masking and context prediction objective are deemed as the least informative according to the \(_{S}}\) score. This is explained by the simplicity of these pretraining objectives for this data modality. These methods are also among the least competitive methods on the ADMET downstream tasks.

**NLP-inspired models.**_(\(\)_)Bert-MLM , MolBert  and _\(\)_/GPT leverage masked language model objective applied to string representations (SMILES and SELFIES). Unsurprisingly, as seen in Figure 4(a), these models are clustered, suggesting they capture similar information. However, they fail to simulate other models in the pool, resulting in low \(_{S}}\) scores, a result consistent with the known limitations of these pretraining objectives . A noticeable exception is _(\(\)_/GPT-1.2B (the biggest model of the pool by far), which displays a significantly higher \(_{S}}\) score.

**"Not-trained" GNN.** Figure 4(b) helps visualize the performances of the different models relative to our baseline "Not-trained" GNN. Surprisingly, some models are ranked less promising than this baseline by the \(_{S}}\) score. However, all of these less promising models obtain poorer performances on the downstream tasks. Similarly, except for InfoGraph , every model ranked more promising than the "Not-trained" GNN baseline and obtained better results on ADMET tasks. This surprising result validates evaluation of the \(_{S}}\) score w.r.t this baseline.

## 7 Limitations and Conclusions

We proposed a principled approach to embedding model evaluation by framing model ranking as a variation of comparing statistical experiments. Utilizing concepts of sufficiency, informativeness, and deficiency, we developed mathematically grounded metrics for pairwise comparisons between embedders without relying on labeled data in downstream tasks. Our tractable relaxation, termed information sufficiency, demonstrated strong correlations with rankings based on downstream task performance in extensive experiments. Although successful, our method still has at least two primary

Figure 5: (a) Pairwise information sufficiency graph between the embedders. The center color represents the ability to simulate other models, while the surrounding colors represent the ability to be simulated by other models. Red indicates a high ability to simulate or be simulated, while blue indicates a low ability. (b) Mean rank of the models (ordered by \(_{S}}\) score) on downstream tasks.

limitations. First, its effectiveness depends on the number and diversity of available embedders (see Sec. E.4). Future work could explore using randomly initialized embedders (random projections) instead of pre-trained ones. Second, we can enhance our proxy for predicting the deficiency between models by exploring better methods (e.g., estimating the \(f\)-divergence) to directly learn the Markov kernel that minimizes the total variation distance, which we leave for future research.