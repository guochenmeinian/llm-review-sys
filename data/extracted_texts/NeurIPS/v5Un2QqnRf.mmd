# Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models

Yang Jiao\({}^{*}\)\({}^{1,2,3}\), Shaoxiang Chen\({}^{3}\), Zequn Jie\({}^{13}\), Jingjing Chen\({}^{1}\)\({}^{1}\)\({}^{,}\)\({}^{2}\)

Lin Ma\({}^{3}\), Yu-Gang Jiang\({}^{1}\)\({}^{,}\)\({}^{2}\)

\({}^{1}\)Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University

\({}^{2}\)Shanghai Collaborative Innovation Center on Intelligent Visual Computing

\({}^{3}\)Meituan

This work is done when Yang Jiao is an intern at Meituan. Corresponding authors.

###### Abstract

Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to language-oriented formats. This adaptation leads to the convenient development of such LMMs with minimal modifications, however, it overlooks the inductive biases within diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named **Lumen**, which decouples the learning of perception capabilities into task-agnostic and task-specific stages. Firstly, Lumen promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all vision-centric tasks we address in this paper. Afterward, the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Comprehensive experimental results on a series of vision-centric and VQA benchmarks indicate that our Lumen model not only achieves or surpasses the performance of existing LMM-based approaches in a range of vision-centric tasks while maintaining general visual understanding and instruction following capabilities.

## 1 Introduction

As the Large Language Models (LLMs)  currently ignite the spark of Artificial General Intelligence (AGI), Large Multimodal Models (LMMs)  take a step forward by integrating visual modalities with the linguistic prowess of LLMs. With the instruction-following and content-reasoning capabilities inherited from LLMs, the LMM has successfully functioned as a versatile assistant across a wide range of tasks, including visual question answering , image captioning , text-to-image generation , etc.

In pursuit of more convenient and efficient human-AI interaction, it is crucial to further explore fundamental vision-centric capabilities encapsulated in the LMMs, which aid in detailed object referencing and dialogue responses. Early works, e.g., MiniGPT-v2 , Kosmos-2  and Qwen-VL , equip the LMM with the grounding ability by reformulating bounding boxes as a sequence of coordinate tokens and adapting them to the language model's output space. Griffon  extends this design to object detection by meticulously curating a language-prompted detection dataset. Owing to such language model-oriented reformulation, these methods can be implemented with minimal modifications to existing LMMs.

Although convenient, the aforementioned approaches encounter challenges when scaling up to more intricate scenarios and vision-centric tasks. First, those LLMs highly relies on language models's auto-regressive sequence generation method, which leads to high uncertainty when multiple objects are concurrently addressed. As shown in the first row of Fig 1, the three persons highlighted with bounding boxes of different colors lack an inherent order. And imposing a sequential order on them would exacerbate the confusion in the decoding process, as the model would be compelled to produce drastically different outputs after the same word "person". Although sequence augmentation techniques introduced in Pix2Seq  can alleviate this issue, they are tailored for the object detection task and can not be seamlessly transferred to other vision-centric tasks such as instance segmentation and pose estimation. On the other hand, in contrast to tasks within the NLP field, which often exhibit stronger inter-task correlations , vision-centric tasks are inherently discrete due to the inductive bias introduced by their distinct task definitions . Therefore, reformulating heterogeneous vision-centric task outputs into language-oriented formats tends to overemphasize the format consistency with language models, while the learning of the underlying visual perception capabilities and intrinsic characteristics of diverse vision tasks is overlooked.

When delving into fundamental vision-centric tasks, namely object detection, instance segmentation, and pose estimation1, we observe that they can be decoupled into task-agnostic and task-specific learning processes. Essentially, these tasks share a common task-agnostic objective of identifying individual instances with an instruction like _"finding the area addressed in instructions"_, while their task-specific definitions introduce different decoding rules for generating diverse formats of outputs (e.g., boxes, masks, or points). Compared with the general task-agnostic objective, task-specific outputs relies less on semantics but is more difficult for the LMMs to learn to output. Based on the above analysis, in this paper, we propose **Lumen**, a **L**arge **m**ultimodal **m**odel with vision-centric capabilities **e**nhancement, which decouples the task-agnostic and task-specific learning in two consecutive stages as shown in the Fig 1. Concretely, in the first stage, the aforementioned visual tasks are unified by reformulating them into the same matching problem. We start by feeding the user's instruction and image into a LMM for content comprehension. The obtained responses contain a designed special token (i.e, [LOC] in Fig 1) that encapsulates the visual concepts conveyed in the provided instruction, regardless of the specific task. Subsequently, this special token interacts with image patches via a transformer-based aligner to generate a heatmap, wherein the response at each location indicates the matching probability between the instruction and the corresponding image region. In the second stage, utilizing this heatmap as indicative guidance, task-specific decoding

Figure 1: **Comparison of our proposed Lumen with previous methods.** Previous methods (e.g., Griffon ) serialize bounding box coordinates into discrete token sequences to conform to the language-oriented outputs format of LMMs, disregarding the unordered nature inherent in bounding boxes. Our Lumen first predicts unified heatmaps for various tasks. These heatmaps are further used for guiding simple decoding tools with the parsed task-type indicators to support versatile visual tasks. We omit the user instruction of referring to all persons in the image for conciseness.

processes are further managed by flexibly assembling predefined decoding rules and lightweight decoders to generate the final outputs with different formats. Due to such decoupled learning behaviors, on the one hand, Lumen can concentrate on promoting fine-grained multimodal content comprehension, rather than being trapped in learning diverse specialized decoding rules lacking in semantics. On the other hand, Lumen can be affected less by the various inductive biases associated with vision-centric tasks within the LLM token space, thereby seamlessly maintaining general visual understanding and instruction following capabilities, as demonstrated in Table 2.

In summary, our contributions are three folds: (1) We introduce **Lumen**, a **L**arge **m**ultimodal **m**odel with vision-centric capabilities **e**n**ahcement, which unleashes the vision-centric potential of the LMM by decoupling the task-agnostic and task-specific learning processes; (2) Our Lumen can seamlessly adapt to tasks such as object detection, instance segmentation, and pose estimation without requiring meticulously curated specialized dialogue datasets as done in the previous work . (3) Our Lumen not only matches or exceeds the performances of existing LMM-based approaches on a series of vision-centric tasks, but also maintains general visual understanding and instruction following capabilities.

## 2 Related Work

### Large Multimodal Models (LMMs)

Benefiting from the remarkable reasoning capabilities of Large Language Models (LLMs), LMMs [23; 9; 24; 25] transfer these abilities to the vision domain by aligning visual tokens with LLMs' input space. To achieve this, pioneering work, Flamingo  resamples the visual features and feeds them into attention-based adapter layers inserted in the LLM. Aiming at more comprehensive vision-language alignment, BLIP-2  designs a Q-Former and jointly performs cross-modal representation learning and generative learning. Inspired by the instruction tuning technique [21; 28] in NLP field, Instruct-BLIP , LLaVA  and Mini-GPT4  curate high-quality multi-modal instruction data for enhanced instruction-following abilities. Meanwhile, to systematically evaluate their general-purpose conversational capabilities, more advanced benchmarks [30; 31; 32; 33; 34; 35] have been explored. However, these methods focus on high-level visual content comprehension and reasoning, ignoring the fundamental visual perception functions, such as object detection, instance segmentation, pose estimation, etc.

### Vision Generalist Models

Generalist models in the vision domain aim at unifying a wide range of vision tasks using one model. Motivated by the success of sequence-to-sequence models in NLP field , OFA  and GIT  unify various tasks in the sequence generation format. Following this trend, Unified-IO , Pix2Seq v2  and UniTab  add discrete coordinate tokens into the vocabulary, so as to accommodating more tasks. Moreover, Gato  successfully unifies reinforcement learning tasks as the sequence generation format. Nevertheless, the sequence generation modeling can lead to low inference speeds and degraded performances. Toward this end, Uni-Perceivers [42; 43] unify different tasks as the maximum likelihood matching problem by calculating representation similarities of all targets and each input. With such an objective, generative and non-generative tasks can be unified by selecting corresponding input and target candidates. However, these generalist models are restricted to pre-defined tasks, failing to be flexibly instructed by natural languages like LMMs.

### LMMs with Vision-Centric Capabilities

To endow LMMs with vision-centric capabilities, two research directions are investigated. On the one hand, a line of work regards LLMs/LMMs as intelligent planners, and allows them to trigger a wide range of task-specific Application Program Interfaces (APIs) according to user's instructions. HuggingGPT  connect GPT with a suite of visual experts. AutoGPT  can further execute post-processing programs after detection. Moreover, BuboGPT  further combines visual grounding specialists with LMMs. On the other hand, an alternative approach explores the intrinsic localization potential of LMMs with task-specific modifications. Kosmos-2 , MiniGPT-v2 , Qwen-VL  and Shikra  enlarge the vocabulary size of LMMs with discrete coordinate tokens to deal with the visual grounding task . LISA  merges the LMM with SAM  for enhanced reasoning capability in the referring image segmentation scenario . However, these methods do not address fundamental vision tasks like object detection and instance segmentation, where multiple objects should be detected or segmented simultaneously. To address this issue, VisionLLM  regards an LLM as a DETR-like task decoder and customizes structural prompts for the detection task with Hungarian matching  for label assignment. Griffon  takes a different approach by leveraging the inherent detection capabilities of the LMM, introducing a language-prompted detection dataset for the instruction tuning. However, these methods leverage discrete coordinate tokens as outputs for different vision tasks, while ignoring their inherent disparities. In this paper, we disentangle the task-agnostic and task-specific learning of various vision-centric tasks to unlock the visual potential of LMMs.

## 3 Method

As shown in Fig 2, our proposed Lumen comprises two consecutive stages. In the first stage, we concentrate on promoting fine-grained multimodal content comprehension via densely aligning visual regions and language instructions, disregarding the discrepancies between various vision tasks. In the second stage, with the resulting alignment heatmap from the first stage, task-specific decoding is performed with specialized operations or decoders. In the following parts, we will elaborate on the detailed designs within each stage.

### Stage 1: Task-Agnostic Matching

#### 3.1.1 Conversation Reformulation

A preliminary step for adapting vision-centric tasks to LMMs is to reformulate the visual data into conversation formats. For different tasks, we employ a unified instruction-response template: "USER: [IMG]. _Please find the location of_ {description}. _Respond with_ {format}. ASSISTANT: _Sure, the location is_ [LOC]. _The task output is_ [DET]/[SEG]/..." Here, {description} and {format} can be customized according to specific tasks. For vision-centric tasks like object detection, instance segmentation and pose estimation, {description} is a certain class name, and {format} can be boxes, masks, or points. For vision-language tasks like visual grounding and referring segmentation,

Figure 2: **Overall framework of the proposed Lumen**. Our Lumen consists of two stages. In the first stage, the input image and the instruction with designed special tokens are embedded and fed into a large language model to interact and comprehend visual and textual contents. Then, the [LOC] token output and high-resolution image features are further aligned to produce a heatmap denoting cross-modal matching probabilities. In the second stage, the heatmap can serve as a strong indication for various vision tasks, and the task outputs can be obtained with lightweight task-specific decoders. The routing of the decoding pathway is determined by the task token (e.g., [DET] in image) output generated in the first stage.

{description} is the referring sentence and {format} can be boxes or masks. Apart from [IMG] that denotes image features, we also introduce a task-agnostic special token [LOC] and a set of task-specific special tokens, including [DET], [SEG], [POINT], [GROUND] and [REFSEG]. As shown in Fig 2, regardless of task types, [LOC] is tasked to summarize the visual concepts in instructions and further densely align with image regions. Task-specific tokens merely serve as routers for guiding the decoding pathway in the second stage **without encoding task-specific inductive bias**. Examples of reformulated conversation data of various tasks are illustrated in Fig 3.

#### 3.1.2 Model Architecture

With the given image \(x_{img}\) and reformulated instruction \(x_{txt}\) as inputs, we first feed them into a conventional LMM (LLaVA-1.5  in our implementation) to generate language responses. We also extract high resolution image features \(f_{img}^{32 32 256}\) following prior works :

\[f_{img}=_{}(x_{img}),\] (1)

where \(_{}\) denotes the vision encoder. Since the adopted vision encoder, CLIP-ViT , is pretrained to accept image inputs of \(336 336\) pixels, we resize its positional embeddings to adapt to higher resolution, e.g., \(448 448\) pixels. Afterward, the feature of [LOC] token \(h_{loc}^{1 256}\) from the generated response, together with high-resolution image features, are fed into the proposed V-L dense aligner to calculate their matching probabilities, resulting in a heatmap \(^{32 32}\):

\[=_{}(f_{img},h_{loc}),\] (2)

where \(_{}\) represents the V-L dense aligner. In our implementation, we employ a lightweight transformer-based architecture as \(_{}\) to interact high resolution image feature tokens and the [LOC] token. More discussions on the architecture design are included in Sec.4.3 and Appendix A.1.

#### 3.1.3 Model Training

To enrich semantics during the dense alignment, we merge data from various tasks by reformulating them into uniform conversation formats as shown in Fig 3. For vision-centric tasks, we hide diverse task-specific output format details and their learning targets are reformulated and unified as heatmaps, where each element denotes the matching probability between the instruction and the corresponding

Figure 3: **The illustration of reformulated conversation data examples of different tasks. For each reformulated data example, we sequentially present the original image (left), the heatmap generated from annotations (middle), and the task-specific ground truth (right) in the figure.**

region. To construct the ground-truth heatmap \(H\), we use a Gaussian kernel to fill the probabilities into a blank map following prior works [56; 57]:

\[H_{xy}=(-)^{2}+(y-l_{y})^{2}}{2^{2}}),\] (3)

where the \((l_{x},l_{y})\) is the location of the one referred by the instruction, and \(\) is the standard deviation. For object detection and visual grounding, \((l_{x},l_{y})\) is the center coordinate of each object, and \(\) is object size-adaptive following . For instance segmentation and referring segmentation, we first convert their masks into bounding boxes by calculating enclosing rectangles of masks, and then calculate \((l_{x},l_{y})\) and \(\) with the same rules as detection and visual grounding tasks. For pose estimation, \((l_{x},l_{y})\) is the coordinate of the annotated keypoint, \(\) is a hyper-parameter designated as 2.

With the ground-truth heatmap \(H\) and predicted heatmap \(\), we apply Gaussian focal loss following  as:

\[_{h}=-_{xy}\{(1-_{xy} )^{}(_{xy})&H_{xy}=1,\\ (1-H_{xy})^{}(_{xy})^{}(1-_{xy})&. .\] (4)

\(\) and \(\) are hyper-parameters of the focal loss, and \(N\) is the number of the location where the ground-truth matching probability equals to 1. We set \(=2\) and \(=4\) in our implementation. For supervising the language outputs, we use the standard cross-entropy loss \(_{t}\) following previous LMM-based methods [19; 18]. The overall loss function \(\) can be formulated as:

\[=_{h}_{h}+_{t}_{t},\] (5)

where \(_{h}\) and \(_{t}\) are hyper-parameters for balancing two losses. During training, since the vision encoder, adapter, tokenizer, and large language model parts in Fig 2 inherit the weights of LLaVA, we only finetune the large language model using the LoRA  technique and train the V-L dense aligner under the supervision of the loss function \(\).

### Stage 2: Task-Specific Decoding

Decoding Modules.We first introduce the operations of the three key modules illustrated in Fig 2. (1) **Peak point selection** parses the heatmap into a set of points, where each point indicates the center location of an identified object or keypoint. Specifically, we filter the locations with heatmap response greater than their 8 surrounding neighbors as candidate peak points, and then retain the top \(K\) candidates as the selected results. The value of \(K\) can vary across different tasks, which will be elaborated on in Sec.4.1. (2) **Box decoder** is used for further regressing the extent of the objects designated by the selected peak points. For efficiency, instead of introducing an additional network as the box decoder, we reuse the V-L dense aligner by appending two new learnable special tokens after the [LOC] token as additional text-side inputs. Accordingly, the V-L dense aligner will generate two additional 1-channel map predictions, which are used for regressing the height and width under the supervision of the L1 loss functions similar to . (3) **Promptable mask decoder** can accept both points and boxes as visual instructions to generate the mask for the referred object. We directly use the SAM model  for executing the above process without finetuning. In general, these decoding modules introduce negligible training costs and can be easily implemented.

Decoding Pathways.On the basis of these three decoding modules, we customize their different cooperation patterns to perform diverse tasks, which will be introduced as follows: (1) **Pose estimation** requires to return the keypoint coordinates. We can easily obtain these coordinates by parsing the predicted heatmap with the peak point selection operation. (2) **Object detection** and **visual grounding** share the same output format of bounding box, therefore, we employ the identical decoding pathway for them. Specifically, as shown in Fig 2, we feed the heatmap into the cascaded peak point selection and box decoder modules to generate bounding boxes. (3) **Instance segmentation** and **referring segmentation** share a relationship analogous to the one between object detection and visual grounding, therefore, we also adopt the same decoding pathway for them. Concretely, we first parse both the center point and bounding box of the object, and then, we use them as visual instructions to guide the promptable mask decoder to generate the mask prediction.

Experiments

### Experimental Setup

Datasets.Our training data consists of datasets from the following different tasks. (1) For object detection, we use MSCOCO , Objects365  and OpenImages . (2) For visual grounding, we use RefCOCO, RefCCO+ and RefCOCOg . (3) For pose estimation, we use MSCOCO keypoints  and AIC . (4) For visual question answering, we employ a subset of ShareGPT4V dataset  with 665K samples. It is worth noting that for instance segmentation and referring segmentation, since we first transform their masks into bounding boxes as mentioned in Sec 3.1.2, the constructed heatmaps are the same as the object detection. Additionally, given that we do not finetune the mask decoder in the second decoding stage, we actually do not use segmentation annotations throughout the entire training process.

Model Configurations.For the first task-agnostic matching stage, we utilize pre-trained CLIP ViT-L/14-336  and LLaVA-7B-v1.5  as our vision encoder and large multimodal model, respectively. For the second decoding stage, since the different decoding modules and pathways have been introduced in Sec 3.2, we primarily specify the \(K\) value choices and corresponding post-processing details of different tasks here. (1) For object detection and instance segmentation, which generally includes multiple objects in a given image, we set \(K=100\) to generate 100 box candidates. Then we apply regular NMS  to filter redundant boxes, and the remaining boxes are used for further prompt mask decoder to generate instance segmentation results. (2) For visual grounding and referring segmentation, given that only one object is referred by a referring sentence, we set \(K=1\) to only generate one predicted box/mask, and no post-processing is required. (3) For pose estimation, we follow previous works [65; 20] to first crop the single object from the image using bounding boxes. Then, for each keypoint category, we set \(K=1\) to extract the point with the highest matching probability as the prediction result.

Training Details.For the task-agnostic stage, our training comprises two phases. (1) In Phase 1, we mix the object detection, visual grounding and pose estimation data with sampling rates of 0.69, 0.23, 0.08, respectively, for balanced data sampling. (2) In Phase 2, we mix the visual question-answering, object detection, visual grounding and pose estimation data with sample rates of 0.67, 0.23, 0.07, 0.03, respectively. We set the batch size to 160 and train the first step for 50,000 steps and the second step for 10,000 steps on 8 NVIDIA 80G A100 GPUs. The loss function balance terms \(_{h}\) and \(_{t}\) are both set to 1. For each phase, we use AdamW as the optimizer with an initial learning rate of \(3 10^{-4}\) and weight decay of 0. During training, we do not calculate heatmap loss \(_{h}\) for visual question-answering data. We do not use any data augmentation techniques for all tasks.

Evaluation Metrics.We adopt evaluation metrics commonly used within each field of task. For object detection and instance segmentation, we use mAP based on box IoU and mask IoU, respectively. For pose estimation, we use mAP based on OKS (object keypoint similarity). For visual question-answering, we comply with the evaluation protocol of each individual benchmark.

### Results on Versatile Tasks

We evaluate our method on vision-centric and vision-language tasks as shown in Table 1 and 2. We categorize existing approaches into three groups, namely _"task-specific specialists"_, _"vision generalists"_ and _"LMM generalists"_, according to their functions and architectures. (1) **Task-specific specialists** are customized models in different fields. They have diverse architectural designs and are limited to a single task. (2) **Vision generalists** pursue handling multiple tasks with a unified architecture. To excel in fundamental visual perception tasks, they typically utilize a powerful vision encoder or additional designs (e.g, data augmentations or decoding strategies) adaptive to vision-centric tasks. (3) **LMM generalists** aim to resolve every task in a conversational format. Focusing on improving the conversational quality, they adopt vision encoders proficient in multimodal content comprehension. In line with these methods, our Lumen pursues _unleashing the inherent vision-centric capabilities of LMMs_.

Object Detection & Instance Segmentation.Object detection and instance segmentation require the model to make dense predictions across the image, therefore they pose great challenges in capturing fine-grained object cues. COCO val set is used for evaluation. (1) Compared with other LMM generalists, our Lumen surpasses them by clear margins, achieving a 15.6 AP\({}_{50}\) boost over Griffon  and a 4.7 AP\({}_{50}\) boost over InstructCV . This indicates the effectiveness of our method in discovering dense object cues. (2) Compared with vision generalists and task-specific specialists, our method further approaches their performances. We analyze that the performance gap might originate from two major aspects. First, we use the input size of \(448 448\), which is much smaller than these competitors, for example, \(1024 1024\) in [20; 75]. Second, we do not introduce DETR-like object decoding techniques as done in [52; 75].

Pose Estimation.We employ COCO human 2D body keypoint val set for evaluation. Following the top-down paradigm employed by previous works [65; 20], we first crop a single object from the image using the bounding box. Therefore, the pose estimation is simplified to discovering keypoints of a single object. Since the LMM generalists do not perform this task, we only compare our method with vision generalists and task-specific specialists. As illustrated in Table 1, our method outperforms the vision generalist model Pix2Seq-v2  with 2.4 AP. Meanwhile, our Lumen also achieves comparable performances with task-specific specialists.

Visual Grounding & Referring Segmentation.Compared to object detection and instance segmentation, visual grounding and referring segmentation underscore the language comprehension ability. Here, we report the results on RefCOCOg val set for comparison because the referring expressions in it are more diverse and abundant than those in RefCOCO and RefCOCO+. We also provide complete results on these three benchmarks in the Appendix B. As illustrated in Table 1, our method achieves better performances than Shikra  and Griffon  on visual grounding task, with AP\({}_{50}\) 83.6 vs 82.3 and 83.6 vs 81.5, respectively.

   Method &  &  &  &  & Refer Seg. \\   & **AP** & **AP\({}_{50}\)** & **AP\({}_{75}\)** & **AP** & **AP\({}_{50}\)** & **AP\({}_{75}\)** & **AP** & **AP\({}_{50}\)** & **AP\({}_{75}\)** & **AP\({}_{50}\)** & **cIoU** \\   \\  Faster R-CNN  & \(40.3\) & \(61.0\) & \(44.0\) & & & & & & & & & \\ DETR  & \(43.3\) & \(63.1\) & \(45.9\) & & & & & & & & \\ Mask R-CNN  & \(41.0\) & \(61.7\) & \(44.9\) & \(37.1\) & \(58.4\) & \(40.1\) & & & & & \\ PolarMask  & & & & \(30.5\) & \(52.0\) & \(31.1\) & & & & & \\ CPM  & & & & & & \(62.7\) & \(86.2\) & \(70.9\) & & & \\ RTMPose  & & & & & & \(68.2\) & \(88.3\) & \(75.9\) & & & \\ MDETR  & & & & & & & & & & \(83.4\) & \\ SeqTR  & & & & & & & & & & \(82.7\) & \(64.7\) \\ LAVT  & & & & & & & & & & & \(61.2\) \\ ReLA  & & & & & & & & & & \(65.0\) \\   \\  Pix2Seq-v2  & \(46.5\) & - & - & \(38.2\) & - & - & \(64.8\) & - & - & & & \\ Uni-Perceiver-v2  & \(58.6\) & - & - & \(50.6\) & - & - & & & & & \\ mPLUG-2  & \(46.9\) & - & - & \(40.6\) & - & - & & & & \(84.7\) & \\ VisionLLM  & \(44.6\) & \(64.0\) & \(48.1\) & \(25.1\) & \(50.0\) & \(22.4\) & & & & - & \\   \\  Shikra-7B  & & & & & & & & & & \(82.3\) & \\ MiniGFr-v2-7B  & & & & & & & & & \(84.4\) & \\ Griffon-13B  & 23.2 & 37.6 & 23.4 & & & & & & \(81.5\) & \\ InstructCV  & - & 48.5 & - & & & & & & \(81.5\) & \\
**Lumen-7B (Ours)** & **35.3** & **53.2** & **35.8** & **30.4** & **49.8** & **31.0** & **67.2** & **90.4** & **75.6** & 83.6 & **65.1** \\   

Table 1: **Results on fundamental vision-centric tasks and vision-language tasks. We use “-” and gray cell to indicate the result is not reported and the corresponding task is not supported by the method, respectively.**

   Method & Param & MMBench & SEED & MME & MMMU & MathVista \\  InstructBLIP  & 7B & 36.0 & 58.8 & 1213/292 & 32.9 & **25.3** \\ MiniGPT-4  & 7B & 24.3 & 47.4 & 582/144 & - & 23.1 \\ Shikra  & 7B & 58.8 & - & - & - & - & - \\ Owen-VL-Chat  & 7B & 60.6 & 58.2 & 1488/**361** & **35.9** & - \\ LLaVA-v1.5  & 7B & 64.3 & **66.1** & **1511**/296 & 35.6 & 23.5 \\
**Lumen (Ours)** & 7B & **64.9** & 65.8 & 1426/332 & 35.2 & 24.6 \\   

Table 2: **Results on prevalent VQA benchmarks. Here, we employ English MMBench dev, SEEDBench image, MME test, MMMU val and MathVista mini sets for evaluation.**Visual Question Answering.To examine general visual understanding and instruction following of our model, we follow common practices and employ MMBench , MME , SEEDBench , MMMU  and MathVista  for evaluation. As shown in Table 2, our Lumen achieves VQA results comparable with state-of-the-art LMMs while extending versatile vision-centric capabilities.

### Ablation Studies

For efficiency, we default to training the model for 10,000 iterations in the first phase of task-agnostic learning. This protocol is standard for our ablation studies

Multi-task Training.We use different task combinations for training the model, and the results are shown in Table 4. Based on the results, we have the following observations: (1) Compared with adopting datasets from all tasks (#5), discarding object detection data (#1) will reduce visual grounding performances. Similarly, removing visual grounding data (#2) also results in performance degradation on object detection. This demonstrates that object detection and visual grounding can benefit from each other. (2) Excluding pose estimation data leads to performance enhancements on both visual grounding and object detection (#3 vs #5). This might be caused by the data conflict problem addressed in many generalist models [52; 23]. (3) Excluding VQA data (i.e., discarding Phase 2) does not significantly affect model's performances on both detection and visual grounding, which is reasonable as VQA data do not provide explicit object-level vision-language alignment cues.

Training Recipe.We ablate the effects of 2-phase training strategy to inspect the contribution of vision-centric data on VQA tasks. In Table 5, we list detailed scores on diverse aspects covered by MMBench. #1 is initialized from LLaVA's weights, without any tuning. By comparing #2 and #3, we observe that FP-C (i.e, Cross-instance Perception) improves remarkably, which indicates that incorporating vision-centric data in Phase 1 can promote multi-instance perception in VQA (as well as the overall performance). Scores of Phase 1-only are not reported as the model trained without rich instruction data tends to respond template answers like _"Sure, the location is_ [LOC] _."_

V-L Dense Aligner Architectures.We ablate different architecture design choices as shown in Table (a)a, where "Conv." indicates the operation of concatenating feature of [LOC] token with every image patch features and fusing them with two convolutional layers, and "Trans." represents a light-weight transformer in our main method. Substituting our transformer-based aligner with simple

   \# & Phase 1 & Phase 2 & AR & CP & FP-C & FP-S & LR & RR & Overall \\ 
1 & & & **69.3** & 74.3 & 54.5 & 66.6 & 28.0 & 55.7 & 62.5 \\
2 & & ✓ & 67.3 & **80.0** & 49.0 & **67.9** & 28.8 & **58.3** & 63.7 \\
3 & ✓ & ✓ & 67.3 & 77.0 & **58.7** & 67.2 & **33.0** & 56.5 & **64.2** \\   

Table 5: **Effect of the training recipe on VQA performances.** MMBench is used for evaluation2.

   Arch. & AP & AP\({}_{50}\) & AP\({}_{75}\) \\  Conv. & 18.4 & 30.2 & 15.7 \\ Trans. & **28.4** & **45.0** & **28.1** \\       Mask Dec. & RC & RC+ & RCg \\  LLaVA-v1.0 & 24.0 & 39.0 & 23.0 \\ LLaVA-v1.0* & 26.8 & 43.2 & 26.0 \\ LLaVA-v1.5 & **28.4** & **45.0** & **28.1** \\       Baseline & AP & AP\({}_{50}\) & AP\({}_{75}\) \\  LLaVA-v1.0 & 24.0 & 39.0 & 23.0 \\ LLaVA-v1.0* & 26.8 & 43.2 & 26.0 \\ LLaVA-v1.5 & **28.4** & **45.0** & **28.1** \\    
   \# & Object Det. & Grounding & Pose Est. & VQA & RefCOCOg & COCO \\ 
1 & & & ✓ & ✓ & ✓ & 72.3 & 26.3 \\
2 & ✓ & & ✓ & ✓ & 24.6 & 41.2 \\
3 & ✓ & ✓ & & ✓ & **75.8** & **45.7** \\
4 & ✓ & ✓ & ✓ & & 75.0 & 44.8 \\
5 & ✓ & ✓ & ✓ & ✓ & 74.8 & 45.0 \\   

Table 3: **Ablation studies on model designs.**convolutional layers incurs significant performance degradation. This result indicates that complete vision-language interaction is crucial in promoting explicit dense vision-language alignment.

**Pretrained Mask Decoder.** We adopt the mask annotations from RefCOCO (RC), RefCOCO+ (RC+) and RefCOCOg (RCg) to train a lightweight transformer-like mask decoder from scratch (TransDec.) and report cIoU on their val set to ablate the effect of leveraging pretrained SAM decoder in Table 2(b). Although SAM outperforms TransDec., the narrow performance gap suggests that the primary challenge in advancing segmentation performances within our framework is the need to improve the dense vision-language alignment in the learned heatmap.

**LMM Baseline.** We ablate the effect of different LMM baselines on vision-centric task as shown in Table 2(c). Since LLaVA-v1.5 employs CLIP ViT-L/14-336 as a stronger vision backbone, we use it to substitute the original CLIP ViT-L/14-224 in LLaVA-v1.0, denoted as "LLaVA-v1.0*" in Table 2(c), to respectively ablate the effects of stronger vision backbone and multimodal language model. It can be proved that both stronger vision backbone and multimodal language model can benefit dense vision-language alignment.

### Generalization Evaluation

**Generalization to Unseen Datasets.** To evaluate the generalization ability of our method, we perform zero-shot evaluation on PASCAL VOC2007 val set . As illustrated in Table 6, our method demonstrates superior generalization ability than other generalist models. It is worth noting that compared with InstructCV which also inherits the enormous word vocabulary of LLM, our method outperforms it on VOC zero-shot evaluation by 16.2 AP. Besides, even compared with specialist models trained on VOC dataset (i.e, RetinaNet and Faster R-CNN in Table 6), our method still achieves comparable performances.

**Generalization to Unseen Tasks.** To prove that the heatmap produced by our first-stage task-agnostic training is a powerful intermediate representation for scaling up to more vision-centric tasks, we apply simple decoding rules on the heatmap to make our Lumen support the object counting task . As shown in Fig 4 our Lumen can make the correct prediction even without specifically training on the object counting task.

## 5 Conclusions

In this paper, we present **Lumen**, a **L**arge **m**ultimodal **m**odel with versatile vision-centric capabilities **e**nhancement. Within the Lumen, we employ a decoupled design to initially promote learning task-agnostic vision-language dense alignment, and subsequently collaborate with task-specific decoding operations to address diverse tasks. With the task-agnostic and task-specific design, Lumen significantly broadens the range of visual tasks that existing LMM generalists can tackle, and also maintains general visual understanding and instruction following capabilities.

   Method & COCO & VOC\({}^{}\) \\  RetinaNet  & - & 77.3 \\ Faster R-CNN  & - & 80.4 \\  Pix2Seq-v2  & **57.4** & 38.5 \\ InstructCV  & 48.5 & 61.7 \\ Lumen (Ours) & 53.2 & **77.9** \\   

Table 6: Generalization ability evaluation on object detection. \(\) indicates that VOC is not included in our training data, and thereby used for cross-dataset generalization evaluation.

Figure 4: Qualitative results of our Lumen when generalizing to the object counting task.

Acknowledgements

This work was supported in part by National Natural Science Foundation of China Project (No. 623B2027) and National Natural Science Foundation of China Project (No. 62072116).