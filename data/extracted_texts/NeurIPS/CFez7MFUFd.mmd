# Cross-Scale Self-Supervised Blind Image Deblurring

via Implicit Neural Representation

 Tianjing Zhang\({}^{1}\), Yuhui Quan\({}^{2}\), Hui Ji\({}^{1}\)

\({}^{1}\) Department of Mathematics, National University of Singapore

\({}^{2}\) School of Computer Science and Engineering, South China University of Technology

tianjingzhang@u.nus.edu, csyhquan@scut.edu.cn, matjh@nus.edu.sg

###### Abstract

Blind image deblurring (BID) is an important yet challenging image recovery problem. Most existing deep learning methods require supervised training with ground truth (GT) images. This paper introduces a self-supervised method for BID that does not require GT images. The key challenge is to regularize the training to prevent over-fitting due to the absence of GT images. By leveraging an exact relationship among the blurred image, latent image, and blur kernel across consecutive scales, we propose an effective cross-scale consistency loss. This is implemented by representing the image and kernel with implicit neural representations (INRs), whose resolution-free property enables consistent yet efficient computation for network training across multiple scales. Combined with a progressively coarse-to-fine training scheme, the proposed method significantly outperforms existing self-supervised methods in extensive experiments.

## 1 Introduction

Uniform blurring, a degradation commonly encountered in optics, leads to the loss of important details within a captured image. Uniform blurring usually can be described as the convolution:

\[=+, \]

where \(\) denotes the 2D discrete convolution operation, \((,)\) is the pair of blurred and sharp (latent) images. \(\) represents the blur kernel responsible for degradation, and \(\) denotes measurement noise. For example, in digital photography, motion blur is caused by camera shake during exposure. When scene depth variation is small and camera movement is mainly translational within the image plane, motion blur can be approximated by the convolution model (1), with \(\) representing camera motion. In fluorescence microscopy, specimens stained with fluorescent dyes and exposed to specific wavelength light often exhibit blurring due to light diffraction and optical path imperfections, such as optical component misalignment or lens aberrations. These factors obscure critical details, like micro-tubule arrangements in cells. Such blurring can also be modeled by (1). In either case, both the latent image \(\) and blur kernel \(\) are unknown and must be estimated from the blurred image \(\).

BID aims at estimating \((,)\), the pair of latent image and blur kernel, from the degraded image \(\). BID is a challenging non-linear inverse problem with many plausible solutions due to its inherent solution ambiguity. This ambiguity stems from the fact that the kernel can be decomposed into \(=_{1}_{2}\), suggesting that the pair \((_{1},_{2})\) are also viable solutions since \(=(_{1}_{2})=_{1}(_{2} )\). One such example is the pair \((,=,)\), where \(\) denotes the delta kernel, which gives a trivial solution saying a blurred image is the convolution of a blurred image and a delta kernel. This ambiguity indicates the ill-posed-ness of the BID problem.

### Discussion on existing approaches for BID

The existing deep learning methods for BID can be roughly classified into the following categories:* _Supervised deep learning for BID with paired data_: Supervised learning methods (_e.g._, [50; 78; 68; 34; 69]) trains a neural network (NN) using paired data of blurred images (or blur kernels) and GT images, to predict the latent image or/and blur kernel from an input blurred image. These methods mainly depend on a large amount of paired data for training, limiting their application to specific scenarios where such paired data is challenging to collect.
* _Supervised deep learning for BID with un-paired data_: Some works (_e.g._[36; 61]) use generative adversarial networks (GANs) to train on unpaired blurred and latent images. Although effective for domain-specific images like faces and text, GANs perform poorly on general natural images due to domain shift. Additionally, GAN-based methods often suffer from mode collapse and training instability, resulting in suboptimal outcomes.
* _BID with pre-trained generative model_: These methods employ pre-trained generative models, such as diffusion models , which are well-suited for processing images with structural consistency features like human faces. However, their reliance on pre-trained models to generate outputs that conform to the learned image distribution can lead to inauthentic results. This is particularly problematic in fields that require high fidelity, such as medical imaging and microscopy.
* _Self-supervised learning for BID_: To circumvent the challenges of data collection and mitigate potential biases and inauthentic outcomes from generative models, a growing body of research (_e.g._, [1; 47; 32; 33; 9; 18; 77]) focuses on developing self-supervised deep learning approaches for BID that do not require training dataset or pre-trained model.

In this paper, we focus on self-supervised BID, which is challenging due to the lack of GT images but offers many practical benefits. Most existing works on self-supervised BID are based on the _deep image prior_ (DIP)  of convolutional NNs (CNNs), which introduces the implicit prior from CNN for preferring structured patterns over random noise during training. The DIP-based self-supervised BID methods typically use two NN-based generators, \(_{}\) and \(_{}\), to re-parameterize the blur kernel \(\) and the latent sharp image \(\). The generators are then trained to maximize the likelihood of the blurred image \(\), minimizing the following self-supervised reconstruction loss:

\[_{sr}(_{},_{}):=||_{}( ;_{})_{}(;_{})-||_{2}^{2}. \]

For instance, SelfDeblur  employed a CNN for \(_{}\) and a Multi-Layer Perceptron (MLP) for \(_{}\). MCEM  used U-Net for both \(_{}\) and \(_{}\), and Zhuang _et al._ used INR for both \(_{}\) and \(_{}\).

Despite lacking access to GT images, these self-supervised BID methods perform competitively against supervised or pre-trained model based approaches, particularly with severely blurred images. However, their performance is less impressive on modestly blurred images and on real-world images. There is practical need for further studies to improve the performance of self-supervised BID across various blurring degrees and real-world images from different optics systems.

### Main Idea and Contributions

Without GT images, self-supervised BID methods must address two vital questions for NN training:

1. Without accessing GT images, how to formulate a self-supervised loss to teach the NN-based generators to accurately predict the latent image and kernel from only the blurred image?
2. The non-linear structure of BID makes training NN-based generators challenging. How can we efficiently train them to ensure accurate convergence to the latent images and kernels?

Our answer to Question 1 is a cross-scale loss function that leverages the cross-scale consistency of the estimates from consecutive scales for regularization. Our answer to Question 2 is a progressive cross-scale training scheme to training the NNs, enhancing training efficiency and ensuring the convergence to GT image/kernel.

**Resolution-free INR for effective cross-scale interaction in BID:** Our proposed method is built on re-parametrizatrion of both latent images and kernels by INR [53; 51]. INR represents signals as continuous functions rather than discrete valued arrays, which in our case is expressed as follows,

\[(i,j)=_{}(i,j;_{})(i,j)= _{}(i,j;_{}), \]

where \((i,j)^{2}\) denotes the spatial co-ordinates of images/kernels, and \(_{},_{}\) denotes the weights of NNs \(_{}\), \(_{}\) for the kernel and image, respectively.

Using INR for representing images/kernels is due to its inherent resolution-free property. By enabling the model to generate the prediction with higher/lower resolutions from the same learned model, INR facilitates seamless multi-scale processing and cross-scale interaction. In contrast, DIP-based NNs that directly maps noise to an image handle cross-scale interaction clumisly, requiring manual re-scaling or interpolation, which likely introduces artifacts in the prediction.

**Self-supervised cross-scale loss for BID:** Without GT images, the sole constraint we have for training is \(=\). Additional priors for regularization are necessary to alleviate over-fitting. Historically, the down-sampled version of \(\), denoted as \(_{_{s}}\) for scale \(s\), has often been used to initiate the blur kernel estimate. However, it is important to note that the blurred image after down-sampling cannot be modeled by a convolution between the down-sampled image and a blur kernel:

\[()\!_{2}\!\!_{_{2}} {k}\!_{2}.\]

In this paper, we present a cross-scale constraint that accurately characterizes the connection between \((,,)\) at different scales. For instance, when \(s=2\), the cross-scale constraint is

\[(\!_{2})(\!_{2})=( )\! 2)+_{d=1}^{3}(_{d}) \!_{2}=\!_{2}+_{d=1}^{ 3}(_{d})\!_{2},\]

where \(\{_{d}\}_{d=1}^{3}\) denote three quadrature mirror filters (QMFs)  of the kernel \(\). This constraint can be easily implemented with INR-based resolution-free generators, and helps regularize the training of two generators to prevent likely over-fitting caused by the lack of GT images in loss function.

**Progressive cross-scale learning for BID**: A well-established practice in traditional alternating iterative methods of BID to avoid convergence to trivial solutions involves solving the problem in a coarse-to-fine manner [67; 71]. This means the kernel is initially estimated for the blurred image at a coarse scale, followed by propagation of the kernel estimate to finer scales. This approach has proven effective in preventing iterations from converging to trivial solutions.

Despite its effectiveness, the coarse-to-fine strategy remains under-exploited in existing self-supervised BID methods, likely due to the resolution-fixed limitations of CNN or MLP based re-parametrization. Utilizing INR's resolution-free properties, we introduce a progressive learning strategy that begins training INR-based generators at a coarser scale and then refines training at finer scales. This scheme effectively addresses over-fitting and ensures convergence to the truth.

**Main contribution:** Our main contributions are summarized as follows:

* Leveraging the resolution-independent properties of INR for latent images/kernels, we propose a self-supervised cross-scale loss for training the NN without requiring GT images.
* We introduce a progressive multi-scale learning approach for BID, specifically designed to mitigate potential over-fitting due to the non-linear nature of the BID problem.

Extensive experiments conducted on a variety of datasets reveals that our proposed method outperforms existing self-supervised BID techniques as well as the supervised alternatives.

## 2 Related Works

**Traditional non-learning methods for BID:** Before deep learning became prevalent, regularization methods were the prominent approach for BID. These methods resolve solution ambiguities by imposing pre-defined priors on images and blur kernels, such as image gradient sparsity [8; 4; 24; 5; 67], image patch recurrence [54; 38], and Laplace priors in dark channels [42; 70]. These methods are based on some iteration scheme, and edge selection is an effective technique for better robustness and stability of the iteration [12; 65; 41; 15; 71]. Since regularization methods can be recast as Maximum-A-Posteriori (MAP) estimators in Bayesian inference, another class of BID methods is derived from variational Bayesian estimators [35; 14; 28; 63; 2], as well as variational expectation maximization [29; 71]. The success of these methods requires rigorous tuning of hyper-parameters related to priors. In contrast, deep learning based methods can automatically learn priors from data.

**Supervised deep learning for BID:** In recent years, there has been rapid progress in supervised deep learning methods for BID. By training over many pairs of blurred/truth images, these methods either explicitly estimate the blur kernel (_e.g._, [50; 7; 40; 34] or only estimate latent images (_e.g._,[66; 56; 25; 75; 20; 6; 11; 73; 72]). The former is more efficient for handling uniform blurring. The latter is more general and can handle non-uniform blurring.

**Deep learning for BID with unpaired data or pre-trained model:** There are also methods that are trained on unpaired dataset. Lu _et al._ train the GAN for domain-specific deblurring. Wen _et al._ propose a structure-aware deblurring method, and Chung _et al._ utilize generative priors from diffusion models to jointly estimate the blur kernel and latent images . Nevertheless, these methods still require GT images in unpaired data for training or depend on pre-trained models, which can be challenging and present difficulties in adaptation. In contrast, our method goes through a self-supervised manner, addressing this limitation.

**Self-supervised deep learning for BID:** These methods address the issues of data collection and dataset bias in supervised methods. Built on the DIP prior for image/kernel, Ren _et al._ introduced SelfDeblur, leveraging two NN-based generators trained through a loss with optional TV regularization. The ensemble NN  aggregates deblurring outcomes from multiple NNs to improve performance. Li _et al._ proposed using Monte-Carlo methods to sample NN weights as an approximation of the MAP estimator of images and kernels. Li _et al._ presented a self-supervised training scheme derived from the EM method. Dong _et al._ combined the implicit prior from NN architecture and hand-crafted prior to regularize the NN training. Zhuang _et al._ re-parametrized images and kernels by INR, and reply on the implicit prior induced by INR and early stopping for regularization. Built on resolution-free INR, we propose a cross-scale self-supervised loss function and an efficient coarse-to-fine training scheme for self-supervised BID.

Most self-supervised BID methods are limited to handling uniform blurring, with Li _et al._ being the only one capable of addressing both uniform and non-uniform blurring caused camera shake. Additionally, self-supervised super-resolution methods[3; 55] also involve the estimation of blur kernel. While both estimate the blur kernel. However, these methods differ in their input: low-resolution images versus high-resolution images. Moreover, the blur degree in BID usually is much more severe than in super-resolution.

**Coarse-to-fine estimation for BID:** Coarse-to-fine schemes, which gradually refine the at different scales, have proven effective in traditional alternating iterative methods [65; 12; 52]. The traditional multi-scale methods typically use estimates from coarser scales solely as initial estimates for finer scales. In contrast, our approach for self-supervised deep BID leverages the cross-scale interaction of estimates with exact relations, rather than approximations. Coarse-to-fine scheme used in recent supervised NN-based methods for BID (_e.g._, [39; 56; 74; 11; 73]) train the NN such that the estimates fit well the input image at different scales. Our multi-scale scheme is more for introducing a new scale consistency loss function specifically tailored for self-supervised BID. This allows for more precise and consistent refinement of the estimations across scales.

**INR for image recovery:** In the domain of images, INR [10; 37; 43] encodes images as NN weights, mapping coordinates to pixel values for a compact, continuous representation. There are also multi-scale extensions [27; 49] of INR for more efficient image compression. It has been used for image restoration tasks such as image in-painting, denoising, and super-resolution [51; 21; 64]. Besides images, INR also has been used for encoding defocus blur kernels  and motion blur kernels . Our work is the first to exploit the resolution-free properties of INR for BID, leveraging these properties for a multi-scale approach and cross-scale interaction.

## 3 Methodology

In this paper, we propose an INR-based progressive cross-scale self-supervised BID method. We first introduce the INR-based modeling for latent images and blur kernels, followed by the formulation of the self-supervised cross-scale loss function and the progressive learning strategy.

**Double-INR model for BID:** In our approach, the blur kernel \(\) and the latent image \(\) are re-parameterized by two INR models, \(_{}\) and \(_{}\), respectively. Each model maps a spatial coordinate \([i,j]\) to a pixel value. Let \(_{},_{}^{2}\) denote the sets of spatial coordinates within the feasible domain for the blur kernel \(\) and the latent image \(\), respectively. Then, they can be expressed as

\[\{[_{}]=_{}( _{};_{})&:&[i,j]=_{}[i,j],\,[i,j]_{};\\ [_{}]=_{}(_{};_{}) &:&[i,j]=_{}[i,j],\,[i,j]_{}, . \]where \(_{},_{}\) denote the NN weights of \(,\). The INR-based re-parameterization (4) allows for the generation of kernel and image at any coordinates, providing a representation at arbitrary scales.

**INR-based multi-scale representation:** Let \(_{m}\) denotes the standard down-sampling operator:

\[(\!_{m})[i,j]=[i m,j m]( \!_{m})[i,j]=[i m,j m].\]

Then, we can form both the kernel and the image in a dyadic pyramid, from the original scale and to coarser scales: Let \(^{(0)}=,^{(0)}=\). Then define

\[^{(s)}=(^{(s-1)})_{2}^{(s)}=( ^{(s-1)})_{2},1 s S_{0}. \]

For any co-ordinate set \(\), we define its co-ordinate set at scale \(s\) as \(^{(s)}=\{[i,j]:[2^{s}i,2^{s}j]\}\). Then, using INR-based model (4), \(^{(s)}\) and \(^{(s)}\) at scale \(s\) can be expressed as

\[^{(s)}[^{(s)}_{}]=[2^{s}^{(s)}_{}] =_{}2^{s}_{};_{} {and}^{(s)}[^{(s)}_{}]=[2^{s}^{(s)} _{}]=_{}2^{s}_{};_{}. \]

Such a multi-scale representation can facilitate the training of the INR-based generators for BID, as it allows for the generation of image details at arbitrary scales.

### NN architecture of INR-based generators for kernel/image

**Kernel generator:** The kernel generator \(_{}\) in our approach is a three-layer MLP with 128 feature nodes that takes coordinates normalized to \([-1,1]\) as input and adopts sinusoidal activation functions in every layer, following the Sinusoidal Representation Networks (SIRENs) . The output layer employs a Softmax activation function to ensure the prediction satisfies the two physical constraints:

\[[i,j] 0[i,j];_{i,j}[i,j]=1. \]

Then, a Kernel Centering layer is applied to address possible positional shifts in estimated kernels for better training stability. For a kernel \(}^{H W}\) satisfying (7), calculating its centroids \((_{x},_{y})\) by

\[_{y}=_{i,j}i}(i,j),_{x}=_{i,j} j}(i,j)\]

The kernel \(}\) is then shifted by \([ H/2-_{y}, W/2-_{x}]\) to ensure it is centered geometrically.

**Image generator:** The image generator \(_{}\) adopts a U-Net structure with five blocks, separated by down- or up-sampling layers and connected by skip connections. Each block integrates a sequence of Convolution, Batch Normalization, and ReLU. The NN concludes with a \(1 1\) convolution layer followed by a Sigmoid layer to ensure the output image values remain within \(\). To efficiently generate high-frequencies of images, following , the input spatial coordinates are first transformed into a higher dimensional space using a high-frequency function \(()\) (sinusoidal):

\[()=,2^{0}\,, 2^{L-1}\,,2^{L-1}\,, \]

where \(:=(i,j)\) represents the normalized coordinate values within \([-1,1]\), and \(L\) is an positive integer. Note this encoding operation is used only in \(_{}\), not in \(_{}\).

### Self-Supervised scale consistency loss

Without GT images, the only readily available loss function to train the generators is the fitting loss:

\[L_{}(_{},_{})=_{f}- =_{}_{}( _{};_{})_{}(_{}; _{}),, \]

where \(_{f}()\) is some distance metric. Such a fitting loss clearly is not sufficient to resolve solution ambiguities in BID. To address this, we introduce a scale consistency loss that regularize the training by enforcing cross-scale consistency of the estimation, which is based on the following proposition.

**Proposition 1**.: _For a kernel (filter) \(\), let \(_{1},_{2},_{3}\) denote its associated QMF filters  defined by_

\[_{1}[m,n]=(-1)^{m}[m,n],\,_{2}[m,n]=(-1)^{n}[m,n],\, _{3}[m,n]=(-1)^{m+n}[m,n], \]

_for any \([m,n]_{}\). Then, we have the following relation between consecutive two dyadic scales:_

\[(\!_{2})(\!_{2})=( )\!_{2}+_{d=1}^{3}(_{d}) \!_{2}. \]Proof.: See Appendix A for the proof. 

As seen from Proposition 1, the down-sampled blurred image \(\!\!_{2}\) does not equal to the convolution of the down-sampled latent image \(\!\!_{2}\) and the down-sampled kernel \(\!\!_{2}\), incurring additional term \(_{d=1}^{3}(_{d})\!\!_{2}\). Therefore, down-sampled blurred images are fine for some initial estimation, not for regularizing the NN to obtain accurate estimation. To address this, based on Proposition 1, we introduce a scale consistency loss across two consecutive scales: for each scale \(s\),

\[L_{}^{(s)}(_{},_{}) =_{c}4(^{(s)}\!\!_{2})( ^{(s)}\!\!_{2}),(^{(s)}^{(s)})\! \!_{2}+_{1 d 3}(^{(s)}_{d}^{(s)})\!\!_{2}  \] \[=_{c}4(^{(s+1)})(^{(s+1)}),( ^{(s)}^{(s)})\!\!_{2}+_{1 d 3}(^{(s)} _{d}^{(s)})\!\!_{2}, \]

where \(\{_{d}^{(s)}\}_{d=1}^{3}\) denotes the QMF filter bank of the kernel \(^{(s)}\) defined by (10), and by (6),

\[^{(s)}[_{}^{(s)}]=_{}2^{s} _{}^{(s)};_{}^{(s )}[_{}^{(s)}]=_{}2^{s}_{}^{(s) };_{}. \]

The scale-consistency loss (13)-(14) enforces cross-scale consistency of the estimations at different scales, providing additional regularization for training two INR-based generators.

### Progressively coarse-to-fine training for BID

To address the non-linear nature of BID and avoid convergence to trivial solutions, we introduce a progressive learning strategy that trains the INR-based generators at multiple scales. The training process consists of three stages at different scales. Define the fitting terms at different scale \(s\) by

\[L_{}^{(s)}=_{f}\!\!_{2^{s}},_{ }2^{s}_{}^{(s)};_{} _{}2^{s}_{}^{(s)};_{} . \]

The first stage serves as the initialization, operating at the coarsest scale with the fitting loss \(_{}^{(S_{0})}\). The second stage progressive refines the training from the scale \(S_{0}\) to \(0\) with both the fitting loss and cross-scale consistency loss. Specifically, at scale \(s\), the loss is \(_{}^{(s)}+_{}^{(s)}\) where \(\) is a weight and \(_{}^{(s)}\) is the cross-scale consistency loss defined by (13)-(14). The third stage is the final tuning stage at scale \(0\) with \(_{}^{(0)}\) only. The training process is summarized in Algorithm 1.

```
input: a blurred image \(\) output: an estimated kernel \(^{*}\) and latent image \(^{*}\)
1: Initializing two generator NNs \(_{},_{}\) with random weights \(_{}\) and \(_{}\);
2:Initial training: at the scale \(S_{0}\), training the NNs with only the fitting loss \(_{}^{(s)}\);
3:%% Progressively training the NNs
4:for\(s S_{0}\)to\(0\)do
5: Training the NNs at the scale \(s\) with the loss \(_{}^{(s)}+_{}^{(s)}\);
6:Final tuning: training the NNs at scale \(0\) with only the fitting loss \(_{}^{(0)}\);
7: Define \(^{*}=_{}(_{};_{}^{*})\) and \(^{*}=_{}(_{};_{}^{*})\).
```

**Algorithm 1**Self-supervised progressively coarse-to-fine training for BID

Ineural Similarity Index Measure (SSIM)  is used for \(_{f}()\) in (15):

\[_{f}(,)=(_{x}^{2}+_{y}^{2}+c_{1})^{-1}(_{x} ^{2}+_{y}^{2}+c_{2})^{-1}(2_{x}_{y}+c_{1})(2_{x,y}+c_{2}).\]

For \(_{c}()\) in (13), note that convolution process in (13) can be efficiently computed by by transforming the convolution operation into pointwise multiplication in its discrete Fourier transform (DFT), denoted by \(()\). Thus, we define \(_{c}()\) in frequency domain with \(_{1}\)-norm:

\[_{c}(,)=\|()-()\|_{1}.\]

## 4 Experiments

**Implementation details:** The training consists of 5000 iterations across three stages. The first stage operates at the coarsest scale \(S_{0}\) with 500 iterations. The second stage refines training from scale \(S_{0}\) to scale 0, with 500 iterations per scale. The final stage is tuning at scale 0 for the remaining iterations. The NN is trained using the Adam optimizer with a batch size of 1. The initial learning rates for the image and kernel generators are set to \(5 10^{-3}\) and \(5 10^{-5}\), respectively, decreasing to half their values every 2000 iterations. The weight \(\) in the loss function is set to 0.001 to keep the values of the two loss terms \(_{}\) and \(_{}\) in the same order. For comparison, we use results from the literature when available; otherwise, we use pre-trained models or train from the provided code to achieve optimal performance. The code of the proposed method is available on Github.1.

### Evalution of motion deblurring on synthesized datasets

Two metrics, PSNR (peak signal-to-noise ratio) and SSIM, are used for performance evaluation. Following SelfDeblur , we compute PSNR/SSIM after finding the best shift between GT and the result to handle shift ambiguity. In the tables, **Bold** in blue indicates the best among all supervised methods, **Bold** in black indicates the best, and _underline_ the second-best among all GT-free methods.

**Synthetic dataset with uniform blurring from Lai _et al._:** This dataset consists of 100 images categorized into five groups: Manmade, Natural, People, Saturated, and Text, and covers \(4\) different kernels whose size ranges from \(31 31\) to \(75 75\). For this dataset, \(S_{0}\) is set to \(2\). As the focus of BID is on accurately estimating the blur kernel. Thus, a two-stage evaluation protocol, as outlined by , is also used in our test. Note that most supervised methods only generate clear images without kernels, and thus their evaluation is solely on their output images. The competing methods include \(7\) non-learning methods, \(5\) supervised methods, and \(4\) self-supervised methods. For all

    &  &  &  &  &  &  &  \\    } & Xu \& Jia\({}^{}\) & 19.23/0.654 & 23.03/0.754 & 25.32/0.852 & 14.79/0.563 & 18.56/0.7173 & 20.18/0.708 \\  & Xu _et al.\({}^{}\)_ & 17.99/0.597 & 21.38/0.679 & 24.40/0.813 & 14.53/0.538 & 17.64/0.668 & 19.23/0.659 \\  & Zhong _et al.\({}^{}\)_ & 17.32/0.556 & 21.07/0.695 & 24.39/0.761 & 14.86/0.602 & 15.86/0.532 & 18.70/0.628 \\  & Michaeli \& Irani\({}^{}\) & 17.43/0.418 & 20.70/0.511 & 23.35/0.699 & 14.14/0.491 & 16.23/0.468 & 18.37/0.518 \\  & Pan-DCP\({}^{}\) & 18.59/0.594 & 22.60/0.698 & 24.03/0.772 & 16.52/0.632 & 17.42/0.619 & 19.89/0.666 \\  & Yan _et al.\({}^{}\)_ & 19.32/0.579 & 23.69/0.678 & 27.01/0.842 & 16.46/0.588 & 18.64/0.689 & 21.02/0.675 \\  & Yang \& Ji\({}^{}\) & 19.99/0.599 & 24.33/0.692 & 27.22/0.861 & 17.04/0.605 & 20.35/0.762 & 21.79/0.704 \\    } & DeblurGAN-v2  & 15.93/0.321 & 18.95/0.429 & 21.53/0.694 & 13.79/0.488 & 14.82/0.519 & 17.04/0.490 \\  & Kaufman \& Fattal  & **18.94/0.517** & **22.05/0.586** & **27.05/0.833** & 15.18/0.599 & **17.85/0.717** & **20.22/0.650** \\  & MIMO-UNet  & 15.49/0.301 & 18.36/0.415 & 20.03/0.653 & 13.65/0.473 & 14.26/0.464 & 16.36/0.461 \\  & MPRNet  & 15.58/0.309 & 18.56/0.429 & 20.08/0.656 & 13.67/0.478 & 12.83/0.400 & 16.15/0.454 \\  & MPRNet*  & 17.39/0.419 & 20.53/0.510 & 22.85/0.673 & 15.35/0.551 & 16.01/0.499 & 18.42/0.531 \\  & Restormer  & 15.63/0.324 & 18.55/0.433 & 20.29/0.665 & 13.70/0.499 & 13.40/0.451 & 16.31/0.474 \\  & Restormer* & 17.87/0.453 & 21.07/0.553 & 23.15/0.674 & **15.59/0.550** & 16.67/0.543 & 18.89/0.555 \\    } & SelfDeblur\({}^{}\) & 20.08/0.538 & 22.50/0.581 & 27.41/0.850 & 16.58/0.654 & 19.06/0.731 & 21.13/0.671 \\  & SelfDeblur & 20.35/0.754 & 22.05/0.709 & 25.94/0.883 & 16.35/0.636 & 20.16/0.779 & 20.97/0.752 \\  & DEBID\({}^{}\) & 19.62/0.692 & 24.12/0.807 & 28.23/0.890 & 17.12/0.692 & 19.44/0.711 & 21.71/0.751 \\  & DEBID & 22.14/0.803 & 26.18/0.894 & 31.25/0.923 & 18.43/0.714 & 23.00/0.822 & 24.20/0.831 \\  & MCEM\({}^{}\) & 21.01/0.682 & 24.67/0.751 & 28.17/0.863 & 16.63/0.651 & 20.51/0.760 & 22.20/0.741 \\  & MCEM & 23.06/0.751 & 26.00/0.774 & 31.02/0.902 & 17.21/0.679 & 25.46/0.892 & 24.55/0.800 \\  & VDIP\({}^{}\) & 20.97/0.647 & 24.51/0.770 & 27.53/0.862 & 17.18/0.716 & 20.23/0.743 & 22.08/0.747 \\  & VDIP & 22.86/0.868 & 26.18/0.895 & 30.76/0.927 & **18.55/0.727** & 27.24/0.927 & 25.12/0.869 \\  & Ours\({}^{}\) & 21.06/0.698 & 24.70/0.811 & 28.31/0.890 & 16.63/0.655 & 20.67/0.733 & 22.27/0.756 \\  & Ours & **23.24/0.893** & **26.27/0.933** & **31.53/0.944** & 17.76/0.683 & **27.01/0.930** & **25.16/0.879** \\   

Table 1: Average PSNR/SSIM of the results for Lai _et al._ dataset . The methods marked with \({}^{}\) deblur the image by  using the estimated kernel, a standard protocol for evaluating kernel estimation accuracy in BID. The methods marked with * are retrained on the BSD-D dataset .

supervised methods, except for , we use their models pre-trained on the GoPro dataset  with both uniform and non-uniform blurring. For two recent methods, MPRNet  and Restormer , we also include their models retrained on the BSD-D dataset  with only uniform blurring.

The comparison in Tab. 1 shows that our method achieved the best performance. Note that for two supervised methods, MPRNet  and Restormer , their models trained on a dataset with only uniform blur performs better on testing data with uniform blur, when compared to the ones trained on the dataset with non-uniform blurring. However, this improvement is still not enough to match the performance of our method. The main reason is that existing supervised methods target general blurring and overlook the physics prior of image formation, specifically the convolution model for uniform blurring. As a result, they underperform compared to our self-supervised method, which leverages this prior. This highlights the generalization issues inherent in supervised learning approaches, whose performance heavily depends the correlation degree between the training and testing data. Refer to Appendix F.1 for visual comparisons of different methods.

**Synthetic dataset with modest non-uniform blurring from Kohler _et al._:** The benchmark dataset Kohler  comprises 48 motion-blurred images which blurring is not exactly uniform. This dataset is for evaluating the robustness of BID method to handle modest non-uniform blurring. Totally 17 methods are selected for comparison, and \(S_{0}\) is also set to \(2\). Following the evaluation protocol in , we use the average PSNR and MSSIM (multi-scale SSIM) as the evaluation metrics. Tab. 2 shows that our method outperforms all others in terms of both PSNR and MSSIM, indicating its robustness in handling modest non-uniform blur. Refer to Appendix F.2 for visual comparisons.

### Evaluation of motion deblurring on real-world datasets

**Real-world dataset from Lai _et al._'s :** Lai _et al._'s real-world dataset  consists of 100 real blurred images captured in diverse scenarios using various capturing settings. As no GT images for quantitative evaluation, we present only visual comparisons of many samples. Please refer to Appendix F.3 for visual comparisons of different methods. Overall, our method generates the images with the best visual quality, consistent with its performance on the synthetic datasets.

**Real-world dataset RealBlur  with small/modest blurring:** Our proposed method is also evaluated in another real-world dataset published in : RealBlur-J and RealBlur-R, both containing

    &  &  &  &  \\   & & &  Xu \& Jia \\  \\  &  Pan-DCP \\  \\  &  Hu _et al._ \\  \\  &  DeepDelblur \\  \\  &  Deblur \\ -v2 \\  &  Bestormer \\  \\  &  SelfDeblur \\  \\  & 
 MCEM \\  \\  & Ours \\   & PSNR & 27.14 & 27.22 & 26.41 & 27.87 & 28.70 & **28.96** & 27.92 & 28.17 & **28.31** \\  & SSIM & 0.8303 & 0.7901 & 0.8028 & 0.8274 & 0.8662 & **0.8790** & 0.8420 & 0.8499 & **0.8511** \\   & PSNR & 34.46 & 34.01 & 33.67 & 32.51 & 35.26 & **36.19** & 34.49 & 35.46 & **35.62** \\  & SSIM & 0.9368 & 0.9162 & 0.9158 & 0.8406 & 0.9440 & **0.9570** & 0.9270 & 0.9436 & **0.9448** \\   

Table 3: Average PSNR/SSIM of the results on the RealBlur dataset .

\(980\) real-world blurry images. See Tab. 3 for the results. It can be seen that, unlike the experiments on synthetic datasets, there is a gap between the proposed method and supervised methods. This is understandable, as our method cannot access GT images, whereas the supervised methods can. However, our method still outperforms all traditional methods and existing self-supervised methods. Refer to Appendix F.4 for visual comparisons of different methods.

### Evaluation on microscopic deconvolution

In microscopic imaging, acquired images often suffer from blur due to optical limitations, out-of-focus elements, specimen motion, and the diffraction limit of light. Following , the test dataset consists of 120 images, covering 24 images from the test subsets "Confocal_BPAE_B" and "TwoPhoton_MICE", and includes \(3\) Gaussian point spread functions (PSFs) and \(2\) Poisson PSFs as the blur kernels. For the compared supervised methods, Restomer  and INIKNet , we retrained their models using the microscopic dataset . It can be seen from Tab. 4 that our method outperforms all other self-supervised methods and is comparable to the supervised methods trained on the microscopic dataset. Refer to Appendix F.5 for visual comparisons of different methods.

### Ablation study

The ablation study is conducted on the Lai _et al._ dataset . The results are shown in Tab. 5

**Effectiveness of self-supervised cross-scale consistency loss:** To evaluate the gain from the proposed cross-scale consistency loss functions, we retrain the NN using only the fitting loss \(_{}\) for each scale, _i.e._, without (w/o) \(_{}\). Tab. 5 shows an average gain of about 0.72 dB in PSNR with the cross-scale consistency loss, demonstrating its effectiveness.

**Effective of progressive coarse-to-fine training:** We first examine the effectiveness of multi-scale training by performing the training only at the original scale. Tab. 5 shows about a 1 dB loss in PSNR, indicating multi-scale training's contribution. To evaluate the progressive training strategy, we train the NN with the sum of all loss functions at three scales, which resulting in a 3dB loss. This clearly indicates that progressive training is critical for effectively utilizing the multi-scale scheme. The reason is that the loss function at the coarser scale emphasizes lower frequencies since the image at coarser scale retains low but loses high frequencies. Thus, a NN trained on the sum of loss functions across 3 scales focuses more on low frequencies than one trained only at the finest scale, which fits both low and high frequencies. Therefore, a scale-progressive training scheme is more effective than a joint multi-scale loss. In scale-progressive training, the estimation at the coarser scale provides

    &  &  &  \\  PSFs & Restormer  & INIKNet  & BlindDPS  & SelfDeblur  & MCEM  & VDIP  & Ours \\  Gaussian & 36.32/0.936 & **37.32/0.941** & 26.78/0.661 & 35.52/0.927 & 36.12/0.933 & 35.73/0.932 & **36.65/0.940** \\  Poisson & 40.73/0.958 & **41.56/0.961** & 26.94/0.667 & 39.88/0.950 & 39.26/0.945 & 38.47/0.942 & **40.71/0.960** \\   

Table 4: Average PSNR/SSIM of the results from different methods on microscopic deconvolution.

   Category & Manmade & Natural & People & Saturated & Text & Average \\  w/o \(_{}\) & 21.19/0.778 & 25.84/0.887 & 30.74/0.918 & 17.69/0.682 & 26.75/0.917 & 24.44/0.836 \\ Single-scale & 22.04/0.803 & 25.93/0.890 & 30.33/0.933 & 17.68/0.688 & 24.76/0.886 & 24.14/0.840 \\ w/o Progressive & 20.36/0.742 & 23.91/0.829 & 26.35/0.821 & 17.22/0.675 & 22.88/0.857 & 22.14/0.790 \\  INR/CNN as \(_{k}\)/\(_{}\) & 16.62/0.370 & 26.21/0.865 & 28.83/0.877 & 17.03/0.668 & 23.34/0.776 & 23.01/0.767 \\ MLP/INR as \(_{k}\)/\(_{}\) & 19.88/0.661 & 19.47/0.479 & 26.77/0.718 & 16.07/0.667 & 15.66/0.537 & 19.17/0.521 \\ MLP/CNN as \(_{k}\)/\(_{}\) & 15.87/0.331 & 19.20/0.430 & 23.32/0.578 & 15.58/0.627 & 16.57/0.481 & 18.19/0.470 \\ Multi-scale by direct \(\) & 18.64/0.616 & 22.61/0.722 & 25.95/0.781 & 16.64/0.601 & 20.65/0.861 & 20.89/0.716 \\  Ours & **23.24/0.893** & **26.27/0.933** & **31.53/0.944** & **17.76/0.683** & **27.01/0.930** & **25.16/0.879** \\   

Table 5: Ablation study of the proposed method in terms of of PSNR/SSIM.

a strong initialization for the finer scales. The final result is obtained by applying the fitting term exclusively at the finest scale.

**INR (coordinate NN) vs. MLP/CNN (image-to-image NN)** This ablation study is to evaluate the necessity of using INR for re-parametrizatrion of image and/or kernel, instead of using the NN that maps an image to an image. In the study, we separately replace our INR-based kernel/image generator with MLP/CNN based representation adopted in , whose down-sampled versions of images/kernels at different scales are generated by standard down-sampling process.

**Direct down-sampling vs. using multi-scale grid in INR**: To verify the benefit of resolution-free property of INR, we also consider generating multi-scale versions of image/kernel by standard down-sampling using bilinear interpolation, on their high resolution version from INPs, same to a CNN-based representation. This is referred to as "Multi-scale by direct \(\)". Tab. 5 shows that the performance of multi-scale by standard down-sampling process is very poor. Clearly multi-scale representation by standard downsampling process is not as effective as resolution-free INR.

### Comparison of computational effciency

The computational effciency of the propoposed method is compared to three most related self-supervised BID methods. The results are reported in terms of running time, number of parameters, and memory usage, when processing a \(256 256\) image with a \(31 31\) blur kernel on an NVIDIA 3090 RTX GPU. See Table 6 for the details. It can be seen that the proposed method achieves a balance between computational cost and deblurring performance.

### More details, studies, experiments and visual comparisons in the appendix

The appendix includes (1) ablation study on setting scale \(S_{0}\), (2) details on hyper-parameter settings, (3) details on NN architecture, (4) more comparison on running time; (5) visualization of some sample results, (6) visualization of intermediate results, and (7) experiments on Levin _et al._'s dataset .

## 5 Discussion and Conclusion

This paper introduces a self-supervised method for BID, which does not require GT images. Leveraging on the resolution-free representation of INR for image/kernel, we propose a self-supervised cross-scale consistency loss function and a progressive coarse-to-fine training strategy. The proposed method significantly outperforms existing methods in extensive experiments.

There are two limitations of the proposed self-supervised method. The first is the computational cost for processing a large number of images, as the method requires training the model for each individual sample. A potential solution is to explore the usage of the proposed techniques in meta-learning or testing-time adaptation. This would allow the proposed technique to rapidly adapt a pre-trained model instead of to train the NN from scratch. The second limitation is that the proposed method is only applicable to handle uniform blurring, as it relies on the convolution model. Extending this approach to handle non-uniform blur will be another important direction for future research.

   Methods & SelfDeblur  & MCEM  & VDIP  & Ours \\  Running time (s) & 219.71 & 226.31 & 245.04 & 213.02 \\ Number of parameters (k) & 3427.2 & 2409.0 & 3523.2 & 2342.4 \\ Memory usage (GB) & 6.31 & 1.27 & 9.19 & 1.82 \\   

Table 6: Model complexity comparison.