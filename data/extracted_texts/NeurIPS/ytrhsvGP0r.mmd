# Epidemic Learning: Boosting Decentralized Learning with Randomized Communication

Martijn de Vos &Sadegh Farhadkhani &Rachid Guerraoui

Anne-Marie Kermarrec &Rafael Pires &Rishi Sharma

EPFL, Switzerland

Authors are listed in alphabetical order.Corresponding author <sadegh.farhadkhani@epfl.ch>.

###### Abstract

We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a _random sample_ of \(s\) other nodes (in a system of \(n\) nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, _i.e._, the rounds required to achieve asymptotic linear speedup, is in \((n^{3}/s^{2})\) which outperforms the best-known bound \((n^{3})\) by a factor of \(s^{2}\), indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our results illustrate that EL converges up to \(1.7\) quicker than baseline DL algorithms and attains 2.2% higher accuracy for the same communication volume.

## 1 Introduction

In Decentralized Learning (DL), multiple machines (or nodes) collaboratively train a machine learning model without any central server . Periodically, each node updates the model using its local data, sends its model updates to other nodes, and averages the received model updates, all without sharing raw data. Compared to centralized approaches , DL circumvents the need for centralized control, ensures scalability , and avoids imposing substantial communication costs on a central server . However, DL comes with its own challenges. The exchange of model updates with all nodes can become prohibitively expensive in terms of communication costs as the network size grows . For this reason, nodes in DL algorithms usually exchange model updates with only a small number of other nodes in a particular round, _i.e._, they perform partial averaging instead of an All-Reduce (network-wide) averaging of local model updates .

A key element of DL algorithms is the communication topology, governing how model updates are exchanged between nodes. The properties of the communication topology are critical for the performance of DL approaches as it directly influences the speed of convergence . The seminal decentralized parallel stochastic gradient descent (D-PSGD) algorithm and many of its proposed variants rely on a static topology, _i.e._, each node exchanges its model with a set of neighboring nodes that remain fixed throughout the training process . More recent approaches study changing topologies, _i.e._, topologies that change during training, with notable examples being time-varying graphs , one-peer exponential graphs , EquiTopo , and Gossip Learning . We discuss these works in more detail in Section 5.

This paper investigates the benefits of _randomized communication_ for DL. Randomized communication, extensively studied in distributed computing, has been proven to enhance the performance of fundamental algorithms, including consensus and data dissemination protocols . In the case of DL, randomized communication can reduce the convergence time and therefore communication overhead . In this work, we specifically consider the setting where each node communicates with a random subset of other nodes that changes at each round, as with _epidemic_ interaction schemes . We also focus on the scenario where data is unevenly distributed amongst nodes, _i.e_., non independent and identically distributed (non-IID) settings, a common occurrence in DL .

To illustrate the potential of randomized communication, we empirically compare model convergence in Figure 1 for static \(s\)-regular topologies (referred to as Static-Topo) and randomized topologies (referred to as Rand.-Topo (EL), our work) in 96-node networks on the CIFAR-10 learning task. As particular static topologies can lead to sub-optimal convergence, we also experiment with the setting in which nodes are stuck in an _unlucky_ static \(s\)-regular topology (referred to as Static-Topo-Unlucky). This unlucky topology consists of two network partitions, connected by just two edges. Figure 1 reveals that dynamic topologies converge quicker than static topologies. After \(3000\) communication rounds, Rand.-Topo (EL, our work) achieves 65.3% top-1 test accuracy, compared to 63.4% and 61.8% for Static-Random and Static-Unlucky, respectively. Additional experiments can be found in Section 4, and their setup is elaborated in Appendix C.

**Our contributions** Our paper makes the following four contributions:

* We formulate, design, analyze, and experimentally evaluate **Epidemic Learning (EL)**, a novel DL algorithm in which nodes collaboratively train a machine learning model using a dynamically changing, randomized communication topology. More specifically, in EL, at each round, each node sends its model update to a _random sample_ of \(s\) other nodes (out of \(n\) total nodes). This process results in a randomized topology that changes every round.
* We first analyze an EL variant, named **EL-Oracle**, where the union of the random samples by all nodes forms an \(s\)-regular random graph in each round. EL-Oracle ensures a perfectly balanced communication load among the participating nodes as each node sends and receives exactly \(s\) model updates every round. Nevertheless, achieving an \(s\)-regular graph necessitates coordination among the nodes, which is not ideal in a decentralized setting. To address this challenge, we also analyze another EL variant, named **EL-Local**. In EL-Local, each node independently and locally draws a uniformly random sample of \(s\) other nodes at each round and sends its model update to these nodes. We demonstrate that EL-Local enjoys a comparable convergence guarantee as EL-Oracle without requiring any coordination among the nodes and in a fully decentralized manner.
* Our theoretical analysis in Section 3 shows that EL surpasses the best-known static and randomized topologies in terms of convergence speed. More precisely, we prove that EL converges with the rate \(({1/}+{1/[n]{s}^{2}}+{ 1/T}),\) where \(T\) is the number of learning rounds. Similar to most state-of-the-art algorithms for decentralized optimization  and centralized stochastic gradient descent (SGD) (_e.g_., with a parameter server) , our rate asymptotically achieves **linear speedup**, _i.e_., when \(T\) is sufficiently large, the first term in the convergence rate \(({1/})\) becomes dominant and improves with respect to the number of nodes. Even though linear speedup is a very desirable property, DL algorithms often require many more rounds to reach linear speedup compared to centralized SGD due to the additional error (the second term in the above convergence rate) arising from partial averaging of the local updates. To capture this phenomenon and to compare different decentralized learning algorithms, previous works  adopt the concept of **transient iterations** which are the number of rounds before a decentralized algorithm reaches its linear speedup stage, _i.e_., when \(T\) is relatively small such that the second term of the convergence rate dominates the first term. We derive that EL requires \(({n^{3}/s^{2}})\) transient iterations, which improves upon the best known bound by a factor of \(s^{2}\). We also show this result in Table 1. We note that while EL matches the state-of

Figure 1: Randomized topologies can converge quicker than static ones.

the-art bounds when \(s(1)\), it offers additional flexibility over other methods through parameter \(s\) that provably improves the theoretical convergence speed depending on the communication capabilities of the nodes. For instance, when \(s( n)\) as in Erdos-Renyi and EquiStatic topologies, the number of transient iterations for EL reduces to \((n^{3}/^{2}n)\), outperforming other methods. This improvement comes from the fact that the second term in our convergence rate is superior to the corresponding term \((}{{T^{2}}}})\) in the rate of D-PSGD, where \(p(0,1]\) is the spectral gap of the mixing matrix. We expound more on this in Section 3.
* We present in Section 4 our experimental findings. Using two standard image classification datasets, we compare EL-Oracle and EL-Local against static regular graphs and the state-of-the-art EquiTopo topologies. We find that EL-Oracle and EL-Local converge faster than the baselines and save up to 1.7\(\) communication volume to reach the highest accuracy of the most competitive baseline.

## 2 Epidemic Learning

In this section, we first formally define the decentralized optimization problem. Then we outline our EL algorithm and its variants in Section 2.2.

### Problem statement

We consider a system of \(n\) nodes \([n]:=\{1,,n\}\) where the nodes can communicate by sending messages. Similar to existing work in this domain , we consider settings in which a node can communicate with all other nodes. The implications of this assumption are further discussed in Appendix D. Consider a data space \(\) and a loss function \(f:^{d}\). Given a parameter \(x^{d}\), a data point \(\) incurs a loss of value \(f(x,\,)\). Each node \(i[n]\) has a data distribution \(^{(i)}\) over \(\), which may differ from the data distributions of other nodes. We define the local loss function of \(i\) over distribution \(^{(i)}\) as \(f^{(i)}(x):=_{^{(i)}}[f(x,)]\). The goal is to collaboratively minimize the _global average loss_ by solving the following optimization problem:

\[_{x^{d}}[F(x):=_{i[n]}f^{(i)}(x)].\] (1)

**Method** & **Per-Iter Out Mgss.** & **Transient Iterations** & **Topology** & **Communication** \\  
**Ring ** & 2 & \((n^{11})\) & static & undirected \\ 
**Torus ** & 4 & \((n^{7})\) & static & undirected \\ 
**E.-R. Rand ** & \(( n)\) & \(}(n^{3})\) & static & undirected \\ 
**Static Exp. ** & \( n\) & \((n^{3}^{4}n)\) & static & directed \\
**One-Peer Exp. ** & 1 & \((n^{3}^{4}n)\) & semi-dynamic1 ** & directed \\ 
**D-EquiStatic ** & \( n\) & \((n^{3})\) & static & directed \\
**U-EquiStatic ** & \( n\) & \((n^{3})\) & static & undirected \\
**OD-EquiDyn ** & 1 & \((n^{3})\) & semi-dynamic1 ** & directed \\
**OU-EquiDyn ** & 1 & \((n^{3})\) & semi-dynamic1 ** & directed \\ 
**EL-Oracle (ours)** & \(s\) & \((n^{3}/s^{2})\) & rand.-dynamic2 ** & undirected \\
**EL-Local (ours)** & \(s\) & \((n^{3}/s^{2})\) & rand.-dynamic2 ** & directed \\  

Table 1: Comparison of EL with state-of-the-art DL approaches (grouped by topology family). We compare EL-Oracle and EL-Local to ring, torus, Erd≈ës-Renyi, exponential and EquiTopo topologies.

### Description of EL

We outline EL, executed by node \(i\), in Algorithm 1. We define the initial model of node \(i\) as \(x_{0}^{(i)}\) and a step-size \(\) used during the local model update. The EL algorithm runs for \(T\) rounds. Each round consists of two phases: a _local update phase_ (line 3-5) in which the local model is updated using the local dataset of node \(i\), and a _random communication phase_ (line 6-9) in which model updates are sent to other nodes chosen randomly. In the local update phase, node \(i\) samples a data point \(_{t}^{(i)}\) from its local data distribution \(^{(i)}\) (line 3), computes the stochastic gradient \(g_{t}^{(i)}\) (line 4) and partially updates its local model \(x_{t+1/2}^{(i)}\) using step-size \(\) and gradient \(g_{t}^{(i)}\) (line 5).

The random communication phase follows, where node \(i\) first selects \(s\) of other nodes from the set of all nodes excluding itself: \([n]\{i\}\) (line 6). This sampling step is the innovative element of EL, and we present two variants later. It then sends its recently updated local model \(x_{t+1/2}^{(i)}\) to the selected nodes and waits for model updates from other nodes. Subsequently, each node \(i\) updates its model based on the models it receives from other nodes according to Equation (2). The set of nodes that send their models to node \(i\) is denoted by \(_{t}^{(i)}\). The new model for node \(i\) is computed as a weighted average of the models received from the other nodes and the local model of node \(i\), where the weights are inversely proportional to the number of models received plus one.

\[x_{t+1}^{(i)}:=_{t}^{(i)}|+1}(x_{t+1/2}^{ (i)}+_{j_{t}^{(i)}}x_{t+1/2}^{(j)}).\] (2)

We now describe two approaches to sample \(s\) other nodes (line 6), namely EL-Oracle and EL-Local:

**EL-Oracle** With EL-Oracle, the union of selected communication links forms a \(s\)-regular topology in which every pair of nodes has an equal probability of being neighbors. Moreover, if node \(i\) samples node \(j\), \(j\) will also sample \(i\) (communication is undirected). Figure 2 (left) depicts EL-Oracle sampling from the perspective of node \(j\) in two consecutive iterations. One possible way to generate such a dynamic graph is by generating an \(s\)-regular structure and then distributing a random permutation of

```
1:Require: Initial model \(x_{0}^{(i)}=x_{0}^{d}\), number of rounds \(T\), step-size \(\), sample size \(s\).
2:for\(t=0,,\:T-1\)do\(\) Line 3-5: Local training phase
3: Randomly sample a data point \(_{t}^{(i)}\) from the local data distribution \(^{(i)}\)
4: Compute the stochastic gradient \(g_{t}^{(i)}:= f(x_{t}^{(i)},_{t}^{(i)})\)
5: Partially update local model \(x_{t+1/2}^{(i)}:=x_{t}^{(i)}-\,g_{t}^{(i)}\)\(\) Line 6-9: Random communication phase
6: Sample \(s\) other nodes from \([n]\{i\}\) using EL-Oracle or EL-Local
7: Send \(x_{t+1/2}^{(i)}\) to the selected nodes
8: Wait for the set of updated models \(_{t}^{(i)}\)\(\)\(_{t}^{(i)}\) is the set of received models by node \(i\) in round \(t\)
9: Update \(x_{t+1}^{(i)}\) to the average of available updated models according to (2)
10:endfor ```

**Algorithm 1** Epidemic Learning as executed by a node \(i\)

Figure 2: EL-Oracle (left) and EL-Local (right), from the perspective of node \(j\), with \(s=3\) and for two rounds. We show both outgoing model updates (solid line) and incoming ones (dashed line).

nodes at each round. Our implementation (see Section 4) uses a central coordinator to randomize and synchronize the communication topology each round.

**EL-Local** Constructing the \(s\)-regular topology in EL-Oracle every round can be challenging in a fully decentralized manner as it requires coordination amongst nodes to ensure all nodes have precisely \(s\) incoming and outgoing edges. This motivates us to introduce EL-Local, a sampling approach where each node \(i\) locally and independently samples \(s\) other nodes and sends its model update to them, without these \(s\) nodes necessarily sending their model back to \(i\). The union of selected nodes now forms a \(s\)-out topology. Figure 2 (right) depicts EL-Local sampling from the perspective of node \(j\) in two consecutive iterations. In practice, EL-Local can be realized either by exchanging peer information before starting the learning process or using a decentralized peer-sampling service that provides nodes with (partial) views on the network . While both the topology construction in EL-Oracle as well as peer sampling in EL-Local add some communication and computation overhead, this overhead is minimal compared to the resources used for model exchange and training.

Even though each node sends \(s\) messages for both EL-Oracle and EL-Local, in EL-Local, different nodes may receive different numbers of messages in each training round as each node selects the set of its out-neighbors locally and independent from other nodes. While this might cause an imbalance in the load on individual nodes, we argue that this does not pose a significant issue in practice. To motivate this statement, we run an experiment with \(n=100\), \(1000\) and \(10\,000\) and for each value of \(n\) set \(s= log_{2}(n)\). We simulate up to \(5000\) rounds for each configuration. We show in Figure 3 a CDF with the number of incoming models each round and observe that the distribution of the number of models received by each node is very light-tailed. In a \(10\,000\) node network (and \(s=13\)), nodes receive less than \(22\) models in \(99\)% of all rounds. As such, it is improbable that a node receives a disproportionally large number of models in a given round. In practice, we can alleviate this imbalance issue by adopting a threshold value \(k\) on the number of models processed by each node in a particular round and ignoring incoming models after having received \(k\) models already. The sender node can then retry model exchange with another random node that is less occupied.

## 3 Theoretical Analysis

In this section, we first present our main theoretical result demonstrating the finite-time convergence of EL. We then compare our result with the convergence rate of D-PSGD on different topologies.

### Convergence of EL

In our analysis, we consider the class of smooth loss functions, and we assume that the variance of the noise of stochastic gradients is bounded. We use of the following assumptions that are classical to the analysis of stochastic first-order methods and hold for many learning problems .

**Assumption 1** (Smoothness).: _For all \(i[n]\), the function \(f^{(i)}:^{d}\) is differentiable and there exists \(L<\), such that for all \(x,y^{d}\),_

\[\| f^{(i)}(y)- f^{(i)}(x)\| L\|y-x\|.\]

**Assumption 2** (Bounded stochastic noise).: _There exists \(<\) such that for all \(i[n]\), and \(x^{d}\),_

\[_{^{(i)}}[\| f(x,)-f^{(i)}(x) \|^{2}]^{2}.\]

Moreover, we assume that the heterogeneity among the local loss functions measured by the average distance between the local gradients is bounded.

Figure 3: The distribution of incoming models, for different values of \(n\) and \(s\).

**Assumption 3** (Bounded heterogeneity).: _There exists \(<\), such that for all \(x^{d}\),_

\[_{i[n]}\| f^{(i)}(x)- F(x)\|^{2} ^{2}.\]

We note that this assumption is standard in _heterogeneous_ (a.k.a. non-i.i.d) settings, _i.e._, when nodes have different data distributions . In particular, \(\) can be bounded based on the closeness of the underlying local data distributions . We now present our main theorem.

**Theorem 1**.: _Consider Algorithm 1. Suppose that assumptions 1, 2 and 3 hold true. Let \(_{0}\) be a real value such that \(F(x_{0})-_{x^{d}}F(x)_{0}\). Then, for any \(T 1\), \(n 2\), and \(s 1\): **a)** For **EL-Oracle**, setting_

\[(\{}{TL^{2}}}, }{T_{s}L^{2}(^{2}+^{2} )}},\}),\]

_we have_

\[_{i[n]}_{t=0}^{T-1}[\|  F(x_{t}^{(i)})\|^{2}](^{2}}{nT}}+L^{2}_{0}^{2 }(^{2}+^{2})}{T^{2}}}+}{T}),\]

_where_

\[_{s}:=(1-)().\]

_b)_ _For **EL-Local**, setting_

\[(\{}{T(^{2}+ _{s}^{2})L}},}{T_{s}L^{2 }(^{2}+^{2})}},\}),\]

_we have_

\[_{i[n]}_{t=0}^{T-1}[\|  F(x_{t}^{(i)})\|^{2}]((^{2}+_{s}^{2})}{nT}}+ L^{2}_{0}^{2}(^{2}+^{2} )}{T^{2}}}+}{T}),\]

_where_

\[_{s}:=(1-(1-)^{n})-().\]

To check the tightness of this result, we consider the special case when \(s=n-1\). Then, by Theorem 1, we have \(_{s}=_{s}=0\), and thus both of the convergence rates become \((^{2}}}{{nT}}+}}{{T}}})\), which is the same as the convergence rate of (centralized) SGD for non-convex loss functions . This is expected as, in this case, every node sends its updated model to all other nodes, corresponding to all-to-all communication in a fully-connected topology and thus perfectly averaging the stochastic gradients without any drift between the local models.

The proof of Theorem 1 is given in Appendix A, where we obtain a tighter convergence rate than existing methods. It is important to note that as the mixing matrix of a regular graph is doubly stochastic, for EL-Oracle, one can use the general analysis of D-PSGD with (time-varying) doubly stochastic matrices  to obtain a convergence guarantee. However, the obtained rate would not be as tight and would not capture the \((}{{}})\) improvement in the second term, which is the main advantage of randomization (see Section 3.2). Additionally, it is unclear how these analyses can be generalized to the case where the mixing matrix is not doubly stochastic, which is the case for our EL-Local algorithm. Furthermore, another line of work  provides convergence guarantees for decentralized optimization algorithms based on the PushSum algorithm , with communicating the mixing weights. It may be possible to leverage this proof technique to prove the convergence of EL-Local. However, this approach yields sub-optimal dimension-dependent convergence guarantees (_e.g._, see parameter \(C\) in Lemma 3 of ) and does not capture the benefit of randomized communication.

**Remark 1**.: _Most prior work [24; 32] provides the convergence guarantee on the average of the local models \(_{t}=_{i[n]}x_{t}^{(i)}\). However, as nodes cannot access the global averaged model, we provide the convergence rate directly on the local models. Nonetheless, the same convergence guarantee as Theorem 1 also holds for the global averaged model._

### Discussion and comparison to prior results

To provide context for the above result, we note that the convergence rate of decentralized SGD with non-convex loss functions and a doubly stochastic mixing matrix [24; 30] is

\[(^{2}}{nT}}+ _{0}^{2}(p^{2}+^{2})}{p^{2}T^{2}}}+}{pT}),\] (3)

where \(p(0,1]\) is the spectral gap of the mixing matrix and \(}{{p}}\) is bounded by \((n^{2})\) for ring , \((n)\) for torus , \((1)\) for Erdos-Renyi random graph , \(( n)\) for exponential graph , and \((1)\) for EquiTopo . We now compare the convergence of EL against other topologies across two key properties: linear speed-up and transient iterations.

**Linear speed-up** Both of our convergence rates preserve a linear speed-up of \((}{{}})\) in the first term. For EL-Oracle, this term is the same as (3). However, in the case of EL-Local, in addition to the stochastic noise \(\), this term also depends on the heterogeneity parameter \(\) that vanishes when increasing the sample size \(s\). This comes from the fact that, unlike EL-Oracle, the communication phase of EL-Local does not preserve the exact average of the local models (_i.e_., \(_{i[n]}x_{t+1}^{(i)}_{i[n]}x_{t+}{{2}}}^{(i)}\)), and it only preserves the average in expectation. This adds an error term to the rate of EL-Local. However, as the update vector remains an unbiased estimate of the average gradient, this additional term does not violate the linear speed-up property. Our analysis suggests that setting \(s^{2}}{^{2}}\) can help mitigate the effect of heterogeneity on the convergence of EL-Local. Intuitively, more data heterogeneity leads to more disagreement between the nodes, which requires more communication rounds to converge.

**Transient iterations** Our convergence rates offer superior second and third terms compared to those in (3). This is because first, \(p\) can take very small values, particularly when the topology connectivity is low (_e.g_., \((n^{2})\) for a ring) and second, even when the underlying topology is well-connected and \((1)\), such as in EquiTopo , the second term in our rates still outperforms the one in (3) by a factor of \(\). This improvement is reflected in the number of transient iterations before the linear speed-up stage, _i.e_., the number of rounds required for the first term of the convergence rate to dominate the second term . In our rates, the number of transient iterations is in \((}}{{s^{2}}})\), whereas in (3), it is \((}}{{p^{2}}})\) for the homogeneous case and \((}}{{p^{4}}})\) for the heterogeneous case. We remark that \(p(0,1]\), but \(s 1\) is an integer; therefore, when \(s(1)\) the number of transient iterations for EL matches the state-of-the-art bound. However, it can be provably improved by increasing \(s\) depending on the communication capabilities of the nodes, which adds more flexibility to EL with theoretical guarantees compared to other methods. For instance, for \(s( n)\) as in Erdos-Renyi and EquiStatic topologies, the number of transient iterations for EL becomes \((n^{3}/^{2}n)\) which outperforms other methods (also see Table 1). Crucially, a key implication of this result is that our algorithm requires fewer rounds and, therefore, less communication to converge. We empirically show the savings in the communication of EL in Section 4.

## 4 Evaluation

We present here the empirical evaluation of EL and compare it with state-of-the-art DL baselines. We first describe the experimental setup and then show the performance of EL-Oracle and EL-Local.

### Experimental setup

**Network setup and implementation** We deploy 96 DL nodes for each experiment, interconnected according to the evaluated topologies. When experimenting with \(s\)-regular topologies, each node maintains a fixed degree of \( log_{2}(n)\), _i.e_., each node has 7 neighbors. For EL-Oracle we introduce a centralized coordinator (oracle) that generates a random \(7\)-Regular topology at the start of eachround and informs all nodes about their neighbors for the upcoming round. For EL-Local we make each node aware of all other nodes at the start of the experiment. To remain consistent with other baselines, we fix \(s= log_{2}(n)=7\) when experimenting with EL, _i.e._, each node sends model updates to 7 other nodes each round. Both EL-Oracle and EL-Local were implemented using the DecentralizePy framework  and Python 3.83. For reproducibility, a uniform seed was employed for all pseudo-random generators within each node.

**Baselines** We compare the performance of EL-Oracle and EL-Local against three variants of D-PSGD. Our first baseline is a fully-connected topology (referred to as Fully connected), which presents itself as the upper bound for performance given its optimal convergence rate . We also compare with a \(s\)-regular static topology, the non-changing counterpart of EL-Oracle (referred to as 7-Regular static). This topology is randomly generated at the start of each run according to the random seed, but is kept fixed during the learning. Finally, we compare EL against the communication-efficient topology U-EquiStatic . Since U-EquiStatic topologies can only have even degrees, we generate U-EquiStatic topologies with a degree of 8 to ensure a fair comparison. We refer to this setting as 8-U-EquiStatic.

**Learning task and partitioning** We evaluate the baseline algorithms using the CIFAR-10 image classification dataset  and the FEMNIST dataset, the latter being part of the LEAF benchmark . In this section we focus on the results for CIFAR-10 and present the results for FEMNIST in Appendix C.4. We employ a non-IID data partitioning using the Dirichlet distribution function , parameterized with \(=0.1\). We use a GN-LeNet convolutional neural network . Full details on our experimental setup and hyperparameter tuning can be found in Appendix C.1.

**Metrics** We measure the average top-1 test accuracy and test loss of the model on the test set in the CIFAR-10 learning task every 20 communication rounds. Furthermore, we present the average top-1 test accuracy against the cumulative outgoing communication per node in bytes. We also emphasize the number of communication rounds taken by EL to reach the best top-1 test accuracy of static 7-Regular topology. We run each experiment five times with different random seeds, and we present the average metrics with a 95% confidence interval.

### EL against baselines

Figure 4 shows the performance of EL-Oracle and EL-Local against the baselines for the CIFAR-10 dataset. D-PSGD over a fully-connected topology achieves the highest accuracy, as expected, but incurs more than an order of magnitude of additional communication. EL-Oracle converges faster than its static counterparts of 7-Regular static and 8-U-EquiStatic. After \(3000\) communication rounds, EL-Oracle and EL-Local achieve up to 2.2% higher accuracy compared to 7-Regular static (the most competitive baseline). Moreover, EL-Oracle takes up to 1.7\(\) fewer communication rounds and saves \(2.9\,\) of communication to reach the best accuracy attained by 7-Regular static. Surprisingly, 8-U-EquiStatic shows worse performance compared to 7-Regular static. The plots further highlight that the less constrained EL-Local variant has a very competitive performance compared to EL-Oracle: there is negligible utility loss when sampling locally compared to generating a \(s\)-Regular graph every round. We provide additional observations and results in Appendix C.

Figure 4: Communication rounds vs. top-1 test accuracy and (left) and communication volume per node vs. test accuracy (right) for the CIFAR-10 dataset.

We summarize our main experimental findings in Table 2, which outlines the highest achieved top-1 test accuracy, lowest top-1 test loss, and communication cost to a particular target accuracy for our evaluated baselines. This target accuracy is chosen as the best top-1 test accuracy achieved by the 7-Regular static topology (64.32%), and the communication cost to reach this target accuracy is presented for all the topologies in the right-most column of Table 2. In summary, EL-Oracle and EL-Local converge faster and to higher accuracies compared to 7-Regular static and 8-U-EquiStatic topologies, and require 1.7\(\) and 1.6\(\) less communication cost to reach the target accuracy, respectively.

### Sensitivity Analysis of sample size \(s\)

The sample size \(s\) determines the number of outgoing neighbours of each node at each round of EL-Oracle and EL-Local. We show the impact of this parameter on the test accuracy in the two left-most plots in Figure 5, for varying values of \(s\), against the baseline of a 7-regular static graph. We chose the values of \(s\) as \((n)}{2}=4\), and \( 2log_{2}(n)=14\), over and above \( log_{2}(n)=7\). We observe that, as expected, increasing \(s\) leads to quicker convergence for both EL-Oracle and EL-Local (see Theorem 1). Increasing \(s\) also directly increases the communication volume. The two right-most plots in Figure 5 show the test accuracy of EL-Oracle when the communication volume increases. After \(3000\) communication rounds, EL-Oracle with \(s=14\) has incurred a communication volume of \(15.1\,\), compared to \(4.3\,\) for \(s=4\). An optimal value of \(s\) depends on the network of the environment where EL is deployed. In data center settings where network links usually have high capacities, one can employ a high value of \(s\). In edge settings with limited network capacities, however, the value of \(s\) should be smaller to avoid network congestion.

## 5 Related Work

#### Decentralized Parallel Stochastic Gradient Descent (D-PSGD)

Stochastic Gradient Descent is a stochastic variant of the gradient descent algorithm and is widely used to solve optimization problems at scale [14; 39; 46]. Decentralized algorithms using SGD have gained significant adoption as a method to train machine learning models, with Decentralized Parallel SGD (D-PSGD) being the most well-known DL algorithm [15; 31; 32; 33; 49]. D-PSGD avoids a server by relying on a communication topology describing how peers exchange their model updates .

  
**Topology** & **Top-1 Test Accuracy** & **Top-1 Test Loss** & **Communication to Target Accuracy** \\  & (\%) & & (\(\)) \\ 
**Fully connected** & \(68.67\) & \(0.98\) & \(30.54\) \\
**7-Regular static** & \(64.32\) & \(1.21\) & \(7.03\) \\
**8-U-EquiStatic1** & \(62.72\) & \(1.28\) & - \\
**EL-Oracle** & \(66.56\) & \(1.10\) & \(4.12\) \\
**EL-Local** & \(66.14\) & \(1.12\) & \(4.37\) \\   

* The 8-U-EquiStatic topology did not reach the 64.32% target accuracy.

Table 2: A summary of key experimental findings for the CIFAR-10 dataset.

Figure 5: The test accuracy of EL-Oracle and EL-Local (left) and communication volume per node of EL-Oracle (right) for different value of sample size \(s\) and a 7-Regular static topology.

Static topologiesIn a static topology, all nodes and edges remain fixed throughout the training process. The convergence speed of decentralized optimization algorithms closely depends on the mixing properties of the underlying topologies. There is a large body of work [2; 3; 6; 40] studying the mixing time of different random graphs such as the Erdos-Renyi graph and the geometric random graph. As the Erdos-Renyi graphs have better mixing properties , we compare EL with this family in Table 1. In , the authors prove that static exponential graphs in which nodes are connected to \(( n)\) neighbors are an effective topology for DL. EquiStatic is a static topology family whose consensus rate is independent of the network size .

Semi-dynamic topologiesSeveral DL algorithms impose a fixed communication topology at the start of the training but have nodes communicate with random subsets of their neighbors each round. We classify such topologies as semi-dynamic topologies. The one-peer exponential graph has each node cycling through their \(( n)\) neighbors and has a similar convergence rate to static exponential graphs . In the AD-PSGD algorithm, each node independently trains and averages its model with the model of a randomly selected neighbour . EquiDyn is a dynamic topology family whose consensus rate is independent of the network size .

Time-varying and randomized topologiesA time-varying topology is a topology that changes throughout the training process . The convergence properties of time-varying graphs in distributed optimization have been studied by various works [23; 35; 36; 43; 45]. While these works provide convergence guarantees for decentralized optimization algorithms over time-varying (or random) topologies, they do not show the superiority of randomized communication, and they do not prove a convergence rate that cannot be obtained with a static graph . Another work  considers client subsampling in decentralized optimization where at each round, only a fraction of nodes participate in the learning procedure. This approach is orthogonal to the problem we consider in this paper.

Gossip LearningClosely related to EL-Local is gossip learning (GL), a DL algorithm in which each node progresses through rounds independently from other peers . In each round, a node sends their model to another random node and aggregates incoming models received by other nodes, weighted by age. While GL shows competitive performance compared to centralized approaches [16; 17], its convergence on non-convex loss functions has not been theoretically proven yet . While at a high level, EL-Local with \(s=1\) may look very similar to GL, there are some subtle differences. First, GL operates in an asynchronous manner whereas EL proceeds in synchronous rounds. Second, GL applies weighted averaging when aggregating models, based on model age, and EL aggregates models unweighted. Third, if a node in GL receives multiple models in a round, this node will aggregate the received model with its local model for each received model separately, whereas in EL, there will be a single model update per round, and all the received models from that round are aggregated together.

In contrast to the existing works, EL leverages a fully dynamic and random topology that changes each round. While static and semi-dynamic topologies have shown to be effective in certain settings, EL surpasses their performance by enabling faster convergence, both in theory and in practice.

## 6 Conclusions

We have introduced Epidemic Learning (EL), a new DL algorithm that accelerates model convergence and saves communication costs by leveraging randomized communication. The key idea of EL is that in each round, each node samples \(s\) other nodes in a \(n\)-node network and sends their model updates to these sampled nodes. We introduced two variants of the sampling approach: EL-Oracle in which the communication topology forms a \(s\)-regular graph each round, and EL-Local which forms a \(s\)-out graph. We theoretically proved the convergence of both EL variants and derived that the number of transient iterations for EL is in \((n^{}{{s^{2}}}})\), outperforming the best-known bound \((n^{3})\) by a factor of \(s^{2}\) for smooth non-convex functions. Our experimental evaluation on the CIFAR-10 learning task showcases the 1.7\(\) quicker convergence of EL compared to baselines and highlights the reduction in communication costs.