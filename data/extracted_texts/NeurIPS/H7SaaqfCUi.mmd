# Learning the Infinitesimal Generator

of Stochastic Diffusion Processes

 Vladimir R. Kostic

CSML, Istituto Italiano di Tecnologia

University of Novi Sad

vladimir.kostic@iit.it

&Karim Lounici

CMAP-Ecole Polytechnique

karim.lounici@polytechnique.edu

&Helene Halconruy

SAMOVAR, Telecom Sud-Paris

MODALYX, Universite Paris Nanterre

helene.halconruy@telecom-sudparis.eu

&Timothee Devergne

CSML & ATSIM, Istituto Italiano di Tecnologia

timothee.devergne@iit.it

&Massimiliano Pontil

CSML, Istituto Italiano di Tecnologia

AI Centre, University College London

massimiliano.pontil@iit.it

###### Abstract

We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds.

## 1 Introduction

Continuous-time processes are often modeled using ordinary differential equations (ODEs), assuming deterministic dynamics. However, real systems in science and engineering that are modeled by ODEs are subject to unfeasible-to-model influences, necessitating the extension of deterministic models through _stochastic differential equations_ (SDEs), see  and references therein. SDEs are advantageous for modeling inherently random phenomena. For instance, in finance, they specify the stochastic process governing asset behavior, a crucial step in constructing pricing models for financial derivatives . Another compelling application arises in atomistic simulations, where SDEs are used to model the evolution of atomic systems subjected to thermal fluctuations through the Boltzmann distribution .

A diverse range of SDEs can be represented as \(dX_{t}=a(X_{t})dt+b(X_{t})dW_{t}\), where \(X_{0}=x\). Here, \(W\) denotes a (possibly multi-dimensional) Brownian motion, and the functions \(a\) and \(b\) are commonlyknown as the _drift_ and _diffusion_ coefficients, respectively. Determining these coefficients from one or more trajectories, whether discretized or continuous, has been a key pursuit in "diffusion statistics" since the 1980s, as seen in works like . However, uncovering the drift and diffusion coefficients alone does not reveal all the intrinsic properties of a complex system, such as the metastable states of Langevin dynamics in atomistic simulations [see e.g. 49, and references therein]. Consequently, there has been a shift and growing interest in the Infinitesimal Generator (IG) of an SDE, as its spectral decomposition offers a more in-depth understanding of system dynamics and behavior, thus providing a comprehensive picture beyond the mere identification of coefficients.

Recovering the spectral decomposition of the IG can theoretically be achieved by exploiting the well-studied Transfer Operators (TO) [see 27, and references therein]. TOs represent the average evolution of state functions (observables) over time and, being linear and amenable to spectral decomposition under certain conditions, they offer a valuable means of interpreting and analyze nonlinear systems. However, they require evenly sampled data at a high rate, which may be impractical. Additionally, TO approaches are purely data-driven, complicating the incorporation of partial or full knowledge of an SDE into the learning process. Thus, there is growing interest in learning the IG directly from data, as it can handle uneven sampling and integrate SDE knowledge. The challenge lies in the IG being an unbounded operator, unlike TO which are often well-approximated by Hilbert-Schmidt operators with comprehensive statistical theory . Unfortunately, the existing statistical theory collapses when applied to unbounded operators, prompting the need to completely rethink the problem.

Related workExtensive research has explored learning dynamical systems from data [see the monographs 7, 32, and references therein]. Analytical models for dynamics are often unavailable, motivating the need for data-driven methods. Two prominent approaches have emerged: deep learning-based methods , effective for learning complex data representations but lacking statistical analysis, and kernel methods , offering solid statistical guarantees for the estimation of the TO but requiring careful selection of the kernel function. A related question, tackling the challenging problem of learning invariant subspaces of the TO, has recently led to the development of several methodologies , some of which are based on deep canonical correlation analysis . In comparison, there have been significantly fewer works on learning the IG and only in very specific settings. In  a deep learning approach is developed for Langevin diffusion, while  presents an extended dynamical mode decomposition method for learning the generator and clarifies its connection to Galerkin's approximation. However, neither of these two works provides any learning guarantees. In this respect the only previous work we are aware of are , revising the Galerkin method for Laplacian diffusion, , presenting a kernel approach for general diffusion SDE with full knowledge of drift and diffusion coefficients, and  addressing Langevin diffusion. As highlighted in Table 1, the bounds and analysis in these works are either restricted to specific SDEs or are incomplete. Notably, none of these works proposed an adequate framework to handle the unboundedness of the IG, resulting in an incomplete theoretical analysis and suboptimal rates, sometimes explicitly depending on state space dimension. Moreover, the estimators proposed in these works are prone to spurious eigenvalues and do not offer guarantees on the accurate estimation of eigenvalues and eigenfunctions.

ContributionsIn summary, our main contributions are: **1)** Proposing a fundamentally new idea to estimate the spectrum of self-adjoint generators of stable Ito SDE from a single trajectory. In contrast

 Aspect &  &  &  & Our work \\   Covers many SDEs & ✗ (only Laplacian) & ✓ & ✗ (only Langevin) & ✓ \\  Risk metric & \(_{}^{2}()\) metric & \(_{}^{2}()\) metric & \(_{}^{2}()\) metric & energy (8) \\  Physics-informed method & ✗ & ✓(full info. needed) & ✗ & ✓ (partial info. needed) \\  Avoids spurious eigenvalues & ✗ & ✗ & ✗ & ✓ \\  IG error bound & \((n^{-})\) & \(=(}{^{4}})\) & \((n^{-})\) & \((n^{-})\), \(\) \\  Spectral rates & ✗ & ✗ & ✗ & ✗ \\  Time complexity & \((n^{2}+n^{3/2}d)\) & \((n^{3}d^{3})\) & \((n^{3}d^{3})\) & \((rn^{2}d^{2})\) \\  

Table 1: Comparison to previous kernel-based works on generator learning. Sample size is \(n\), state-space dimension is \(d\), \(\) is the regularization parameter of KKR and RRR and \(r\) is RRR rank parameter. Our learning bounds are derived in Theorem 2 where the parameters \(,,\) quantify the intrinsic difficulty of the problem and impact of kernel choice on learning IG.

to all existing works, we exploit the geometry of the process via a novel energy (risk) functional; **2)** In a certain sense we "_fight fire (resolvent) with fire (generator)_" to derive a new efficient learning method that is able to infer the best approximation of the resolvent of IG on the RKHS, independently of the time-sampling; **3)** We prove the first IG spectral estimation finite sample bounds using (imperfect) partial knowledge, which notably, overcome the curse of dimensionality present in classical numerical methods; **4)** Each important aspect of our learning method, especially in relation to the most relevant existing works, is empirically demonstrated to complement our theoretical analysis.

## 2 Background and the problem

Our drive to estimate the eigenvalues of the infinitesimal generator for an SDE like (1) stems from its crucial role in characterizing dynamics in physical systems. This operator's closed form (3), relies on the drift \(a\) and diffusion coefficient \(b\), where we have partial knowledge: \(b\) is assumed to be known, but \(a\) is not. To compensate for the lack of prior knowledge about \(a\), we introduce the system's _energy_ as an additional known quantity. Below, we detail the mathematical concepts framing the problem (generator, spectrum, energy) exemplified through the Langevin and Cox-Ingersoll-Ross processes. For detailed list of notation used throughout the paper we refer Table 7 in the appendix.

Transfer operator and infinitesimal generatorA variety of physical, biological, and financial systems evolve through stochastic processes \(X=(X_{t})_{t^{+}}\), where \(X_{t}^{d}\) denotes the system's state at time \(t\). A commonly employed model for such dynamics is captured by _stochastic differential equations_ (SDEs) of the form

\[dX_{t}=a(X_{t})dt+b(X_{t})dW_{t} X_{0}=x,\] (1)

where \(x\), \(W=(W_{t}^{1},,W_{t}^{p})_{t_{+}}\) is a \(^{p}\)-dimensional (\(p\)) standard Brownian motion, the _drift_\(a:^{d}\), and the _diffusion_\(b:^{d p}\) are assumed to be globally Lipschitz and sub-linear, so that the SDE (1) admits an unique solution \(X=(X_{t})_{ 0}\) with values in \((,())\). Processes akin to equations like (1) are diverse, spanning models like Langevin and Cox-Ingersoll-Ross processes (see examples below), with broad applications in science and engineering. The process \(X\) is a continuous-time Markov process with almost surely continuous sample paths whose dynamics is described by a family of probability densities \((p_{t})_{t_{+}}\) and _transfer operators_\((A_{t})_{t_{+}}\) such that for all \(t_{+}\), \(E()\), \(x\) and measurable function \(f:\),

\[(X_{t} E|X_{0}=x)=_{E}p_{t}(x,y)dy\,\,A_{t}f= _{}f(y)p_{t}(,y)dy=f(X_{t})\,|\,X_{0}= .\] (2)

Evaluating \(A_{t}f\) at \(x\) yields the expectation of \(f\) starting from \(x\) and evolving until time \(t\), making the transfer operator crucial for understanding \(X\) dynamics. The family \((A_{t})_{t_{+}}\) satisfies the fundamental semigroup equation \(A_{t+s}=A_{t} A_{s}\), for \(s,t_{+}\). Here, we focus on the transfer operator's effect on the set \(_{}^{2}()\), a choice driven by the existence of an invariant measure \(\) for \(A_{t}\) on \((,())\) which satisfies \(A_{t}^{*}=\) for all \(t_{+}\). Then, the process \(X\) is characterized by the infinitesimal generator \(L\) defined for every \(f_{}^{2}()\) such that the limit \(Lf=_{t 0^{+}}(A_{t}f-f)/t\) exists in \(_{}^{2}()\). The operator \(L\) is closed on its domain \(}(L)\) which is equal to the Sobolev space

\[_{}^{1,2}()=\{f_{}^{2}() \|f\|_{}^{2}=\|f\|_{_{}^{2}}+\| f\|_{ _{}^{2}}<\}.\]

For SDE dynamics of the form (1), we can prove (see A.2 for details) that \(L\) is the second-order differential operator given, for any \(f_{}^{2}()\), \(x\), by

\[Lf(x)= f(x)^{}a(x)+b(x)^{}(^{ 2}f(x))b(x),\] (3)

where \(^{2}f=(_{ij}^{2}f)_{i[d],j[p]}\) denotes the Hessian matrix of \(f\).

Spectral decompositionKnowing only the drift \(a\) and diffusion \(b\) is not enough to compute (2) or to understand quantitative aspects of dynamical phase transitions, such as time scales and metastable states. The eigenvalues and eigenvectors of the generator are crucial for capturing these effects. To address the possible unbounded nature of \(L\), one can turn to an auxiliary operator, the resolvent, which, under certain conditions, shares the same eigenfunctions as \(L\) and becomes compact. When it exists and is continuous for some \(\), the operator \(L_{}=( I-L)^{-1}\) is the _resolvent_ of \(L\) and the corresponding _resolvent set_ is defined by

\[(L)=( I-L)\,L_{}\,}.\]We assume that \(L\) has a _compact resolvent_, meaning \((L)\) and there exists \(_{0}(L)\) such that \((_{0}I-L)^{-1}\) is compact. Under this assumption, and given that \((L,(L))\) is self-adjoint, we can prove the spectral decomposition of the generator (see A.1 for details) as follows:

\[L=_{i}_{i}\,f_{i} f_{i},\] (4)

where \((_{i})_{i}\) are the eigenvalues of \(L\), and the corresponding eigenfunctions \(f_{i}_{}^{2}()\), forming an orthonormal basis \((f_{i})_{i}\), are also eigenfunctions of the transfer operator \(A_{t}\).

**Dirichlet forms and energy** To handle the initial lack of knowledge about the drift \(a\), we assume to have access to another quantity, called the _energy_, defined as \((f)=_{t 0}_{}(f(f-A_{t}f))/td\) for all functions \(f_{}^{2}()\) for which this limit exists, defining in the way the domain \(()\). The associated _Dirichlet form_ is the bilinear form defined by polarization for any \(f,g()\) by

\[(f,g)=-_{}f(Lg)d=_{}(-Lf)gd.\] (5)

For every \(f()\), we have \((f)=(f,f)\). As for every \(i\), \(0(f_{i},f_{i})=-_{}f_{i}(_{i}f_{i})d=- _{i}\), we check that the eigenvalues of \(L\) are negative. To relate \(L\) to Dirichlet form, we assume there exists a _Dirichlet operator_\(B=s^{}\) where \(s=[s_{1}\,||\,s_{p}]:x s(x)\!=\![s_{1}(x)\,||\, s_{p}(x)]\!\!^{d p}\) is a smooth function s.t. \(Lf=s(s^{} f)=s(Bf)\) and so that

\[_{}(-Lf)gd=_{}(s(x)s(x)^{} f(x))^{ } g(x)(dx)=_{}(Bf(x))^{}(Bg(x))(dx).\]

We get that for any \(f()\)

\[(f)=_{}\|s^{} f\|^{2}d=_{x }[\|Bf(x)\|^{2}],\] (6)

which is reminiscent of the expected value of the _kinetic energy_ in quantum mechanics .

In the following, while we discuss in detail the examples of Overdamped Langevin and the Cox-Ingersoll-Ross processes, we briefly mention other ones that have a Dirichlet form: the Wright-Fisher diffusion (in dimension one), which can be defined in the context of population genetics and can be adapted to model interest rates, see , the geometric Brownian motion which models the price process of a financial asset, the multi-dimensional Brownian motion (\(a\)\(=\)\(0\)) that corresponds to the heat equation, the transport processes associated with advection-diffusion equation, see , and the process associated with Poisson's equation in electrostatics, see .

**Example 1** (Langevin).: _Let \(k_{b},T_{+}^{*}\). The overdamped Langevin equation driven by a potential \(V:^{d}\) is given by \(dX_{t}=- V(X_{t})dt+T)}dW_{t} X_{0}=x\), where \(k_{b}\) and \(T\) respectively represent the coefficient of friction and the temperature of the system. Its infinitesimal generator \(L\) is defined by \(Lf=- V^{} f+(k_{b}T) f\), for \(f_{}^{1,2}()\). Since \((-Lf)g\,d=-(k_{b}T) f(x) ^{-(k_{b}T)^{-1}V(x)}}{Z}g(x)dx=(k_{b}T) f^{}  g\,d= f(-Lg)\,d\), generator \(L\) is self-adjoint and associated to a gradient Dirichlet form with \(s(x)=(k_{b}T)^{1/2}(_{ij})_{i[d],j[p]}\)._

**Example 2** (Cox-Ingersoll-Ross process).: _Let \(d=1\), \(a,b\), \(_{+}^{*}\). The Cox-Ingersoll-Ross process is solution of the SDE \(dX_{t}=(a+bX_{t})dt+}dW_{t} X_{0}=x\). Its infinitesimal generator \(L\) is defined for \(f_{}^{2}()\) by \(Lf=(a+bx) f+x}{2} f\). By integration by parts, we can check that the generator \(L\) satisfies \((-Lf)g\,d=x}{2}f^{}(x)g^{}(x)\,(dx)=  f(-Lg)\,d\), and it is associated to a gradient Dirichlet form with \(s(x)=/\)._

Learning in reproducing kernel Hilbert spaces (RKHSs)Throughout the paper we let \(\) be an RKHS and let \(k:\) be the associated kernel function. We let \(:\) be a _feature map_ such that \(k(x,x^{})=(x),(x^{})\) for all \(x,x^{}\). We consider RKHSs satisfying \(_{}^{2}()\)[45, Chapter 4.3], so that one can approximate \(L:_{}^{2}()_{}^{2}()\) with an operator \(G:\). Notice that despite \(_{}^{2}()\), the two spaces have different metric structures, that is for all \(f,g\), one in general has \( f,g_{} f,g_{_{}^{2}}\). In order to handle this ambiguity, we introduce the _injection operator_\(S_{}:_{}^{2}()\) such that for all \(f\), the object \(S_{}f\) is the element of \(_{}^{2}()\) which is pointwise equal to \(f\), but endowed with the appropriate \(_{}^{2}\) norm.

Then, the infinitesimal generator restricted to \(\) is simply \(LS_{}\) which can be estimated by \(S_{}G\) for some \(G()\). This approach is based on the embedding \(\) of the generator in the RKHS that can be defined for kernels \(k^{2}()\) whenever one knows drift and diffusion coefficients, see App. B, so that the reproducing property \((x),h_{}\!=\![LS_{}h](x)\) holds true. Based on this observation,  developed empirical estimators of \(LS_{}\) that essentially minimize the risk \(\|LS_{}-S_{}G\|^{2}_{(,_{}^{2})}= _{x}\|(x)-G^{*}(x)\|^{2}_{}\). In scenarios where drift and diffusion coefficients are not known, then \(\) becomes non-computable. However if the process has the Dirichlet form (6), one can still empirically estimate the Galerkin projection \((S_{}^{*}S_{})^{}S_{}^{*}LS_{}\) onto \(\), as considered in , which in fact minimizes the same risk. Yet this approach is problematic due to the unbounded nature of the generator and the associated estimators typically suffer from a large number of spurious eigenvalues around zero, making the estimation of physically most relevant eigenfunctions unreliable even in the self-adjoint case. Conversely, classical numerical methods can compute the leading part of a spectrum without spuriousness issues, suggesting that data-driven approaches should achieve similar reliability. In this paper, we address this problem by designing a novel notion of risk, leading to principled estimators designed to surmount these challenges.

## 3 Novel statistical learning framework

In this section, we tackle the challenges in developing suitable generator estimators highlighted earlier. To this end, we introduce a risk metric for resolvent estimation that can be efficiently minimized empirically, leading to good spectral estimation. Since \(L\) and \(( I-L)^{-1}\) share the same eigenfunctions, the main idea is to learn the (compact) resolvent, which can be effectively approximated by finite-rank operators, instead of learning the generator directly. However, this approach is challenging due to the lack of closed analytical forms for the action of the resolvent.

First, given \(>0\), in order to approximate the action of the resolvent on the RKHS by some operator \(G\), we introduce its embedding \(_{}\) via the reproducing property \(_{}(x),h_{}=[( I-L)^{-1}S_{} h](x)\), formally given by \(_{}(x)=_{0}^{}[(X_{t})e^{- t}\,|\,X_{0}=x]dt\), see App. B for details. Using this notation, we aim to estimate \([( I-L)^{-1}S_{}h](x)[Gh](x)\), \(h\), i.e. the objective is to estimate \(_{}(x) G^{*}(x)\)\(\)-a.e.

An obvious metric for the risk would be the mean square error (MSE) w.r.t. distribution \(\) of the data. However, this becomes intractable since in general \(_{}\) is not computable in closed form when either full or partial knowledge of the process is at hand. To mitigate this issue, we introduce a different ambient space in which we study the resolvent,

\[_{}^{}()=\{f(L)\|f\|^{2}_{ _{}^{}()}_{}[f]= f,( I{-}L)f_{_{}^{2}}<\},\]

where the norm now balances the energy of an observable \(f\) w.r.t. the invariant distribution \(\|f\|^{2}_{_{}^{2}}\) and its energy w.r.t. the transient dynamics \(- f,Lf_{_{}^{2}}\). Indeed

\[_{}[f]=_{x}[|f(x)|^{2}-f(x)[Lf](x)]=_{x}[|\,f(x)|^{2}+\|s(x)^{} f(x)\|^{2}],\] (7)

where the last equality holds for Dirichlet gradient form (6), in which case \(_{}^{}()\) is simply a weighted Sobolev space. Importantly, this energy functional can be empirically estimated from data sampled from \(\), whenever full knowledge, that is drift and diffusion coefficients of the SDE (1), or partial knowledge, i.e. the diffusion coefficient and Dirichlet operator \(B\) in (6), is at hand. With that in mind, instead of the standard MSE, we introduce the energy-based risk functional as

\[_{G}(G)=_{}[\| _{}()-G^{*}()\|_{}].\] (8)

Denoting by \(Z_{}_{}^{}()\) the canonical injection, (8) can be equivalently written as

\[(G)=\|( I-L)^{-1}Z_{}-Z_{}G\|^{2}_{(,)}=\|( I{-}L)^{-1/2}S_{}-( I{-}L)^{1/2}S_{}G\|^{2}_{ (,_{}^{2})},\] (9)

where we abbreviated \(=_{}^{}()\) and used \(Z_{}^{*}=S_{}^{*}( I{-}L)\), recalling that Hilbert-Schmidt and spectral norms for operators \(A\), are \(\|A\|_{(,)}{=}(A^{*}( I{-}L)A)}\) and \(\|A\|_{}{=}(A^{*}( I{-}L)A)} ^{-1/2}\|A\|_{_{}^{2}}\).

Therefore, (9) implies that the regularized energy norm, while dominating the classical \(_{}^{2}\) norm, exerts a _balancing effect_ on the estimation of the resolvent. This leads us to the first general result regarding the well-posedness of this framework.

**Proposition 1**.: _Given \(>0\), let \(\!\!\!_{}^{}()\) be the RKHS associated to kernel \(k^{2}(\!\!)\) such that \(Z_{}(,_{}^{}())\), and let \(P_{}\) be the orthogonal projector onto the closure of \((Z_{})_{}^{}()\). Then, for every \(>0\), there exists a finite rank operator \(G\) such that \((G)\!\!\!\|(I-P_{})( I-L)^{-1}Z_{}\| _{(,)}^{2}+\). Consequently, when \(k\) is universal, \((G)\)._

The previous proposition reveals that whenever the hypothetical domain is dense in the true domain and the injection operator is Hilbert-Schmidt, there is no irreducible risk and one can find arbitrarily good finite rank approximations of the generator's resolvent. Note that \(Z_{}(,_{}^{}())\) is equivalent to \(Z_{}^{*}Z_{}\!=\!S_{}^{*}( I-L)S_{}\) being a trace class operator, which is assured for our Examples 1 and 2, see the discussion in App. E.

Now, to address how minimization of the risk impacts the estimation of the spectral decomposition, let us define the _operator norm error_ and the _metric distortion_ functional, respectively, as

\[(G)=\|( I-L)^{-1}Z_{}-Z_{}G\|_{}, \;G(),\;\;\;\;(h)=\|h\|_{ }/\|h\|_{},\;h.\] (10)

**Proposition 2**.: _Let \(=_{i[]}(-_{i})^{-1}\,_{ i}_{i}\) be the spectral decomposition of \(\), where \(_{i}_{i+1}\) and let \(_{i}=S_{}_{i}\,/\,\|S_{}_{i}\|_{ _{}^{2}}\), for \(i[r]\). Then for every \(>0\) and \(i[r]\)_

\[-_{i}|}{|-_{i}||-_{i}|}()(_{i})\|_{i}-f_{i}\|_{_{}^{2}}^{2}()(_{i})}{[_{i}- ()(_{i})]_{+}},\] (11)

_where gap\({}_{i}\) is the difference between \(i\)-th and \((i+1)\)-th eigenvalue of \(( I-L)^{-1}\)._

Note that the estimation of the eigenfunctions is first obtained in the norm with respect to the energy space, and then transformed to the \(_{}^{2}\)-norm, as \(\|f\|_{}\|f\|_{_{}^{2}}\), \(f\!\!W_{,}\). Therefore, by controlling the operator norm error and metric distortion (see App. C), we can guarantee accurate spectral estimation. Consequently, this allows us to approximately solve the SDE (1) starting from an initial condition

\[[h(X_{t})\,|\,X_{0}\!=\!x]\!=\![e^{Lt}S_{}h](x)_{i [r]}e^{_{i}t}\,_{i},h_{} _{i}(x).\] (12)

## 4 Empirical risk minimization

In this section we address empirical risk minimization (ERM), deriving two main estimators. The first one minimizes empirical risk with Tikhonov regularization, while the second introduces additional regularization in the form of rank constraints.

To present the estimator, we denote the covariance operator w.r.t. \(_{}^{2}\) by \(C=S_{}^{*}S_{}=_{x}[(x)(x)]\), the cross-covariance operator \(T=S_{}^{*}LS_{}\), capturing correlations between input and the outputs of the generator, and the covariance operator \(W_{}=Z_{}^{*}Z_{}=S_{}^{*}( I-L)S_{}\) w.r.t. energy space \(\). All operators can be estimated from data, depending on the available prior knowledge. In this work we focus on the case when Dirichlet gradient form is known, i.e. we can define the embedding of the Dirichlet operator \(B=s^{}_{}^{2}()[_{} ^{2}()]^{p}\) into RKHS \(d^{p}\) via the reproducing property as \( d(x),h_{}=[BS_{}h](x)=s(x)^{} Dh(x)^{p}\), \(h\). More precisely, we have that \(d(x)\) is a \(p\)-dimensional vector with components \(d_{k}(x)\), where \(d_{k}\) is given via reproducing property \( d_{k}(x),h_{}=s_{k}(x)^{}Dh(x)\), \(k[p]\). Hence, in this case we have that

\[T=-_{x}[d(x) d(x)]=-_{k[p]}_ {x}[d_{k}(x) d_{k}(x)].\] (13)

Moreover, defining \(w^{p+1}\) by \(w(x)=[(x),d_{1}(x), d_{p}(x)]^{}^{p+1}\), we get

\[W_{}=_{x}[(x)(x)+d(x) d( x)]=_{x}[w(x) w(x)].\] (14)

Regularized riskLet us first introduce the regularized risk defined for some \(>0\) by

\[_{}(G)=\|( I\!-\!L)^{-1/2}S_{}-( I\!-\!L)^{1/2}S_{} \!\!\!\!G\|_{(,_{}^{2})}^{2}+\|G \|_{()}^{2}, G(),\] (15)

which, after some algebra, can be written as

\[_{}(G) = [G^{*}( C\!-\!T\!+\!\! I)G\!-\!2CG\!+\!S_ {}^{*}( I-L)^{-1}S_{}]\] \[= ^{-1/2}C-W_{,}^{1/2}G\|_{ ()}^{2}}_{}+_{}[_{}()]-\|W_{,}^{-1/2}C\|_{( )}^{2}}_{}\]where \(W_{,}=W_{}+ I\) is the regularized covariance w.r.t. \(\).

Hence, assuming the access to the dataset \(_{n}=(x_{i})_{i[n]}\) made of i.i.d. samples from the invariant distribution \(\), replacing the regularized energy \(_{}\) with its empirical estimate \(}_{}\) leads to the regularized empirical risk functional expressed as

\[}_{}(G)=\|_{,}^{-1/2}-_{,}^{1/2}G\|_{()}^{2}+}_{}[_{}()]-\|_{,}^{-1/2} \|_{()}^{2},\] (16)

where \(W_{,}\) and \(C\) are estimated by their empirical counterparts \(W_{,}\) and \(C\), respectively, via (13). Therefore, our regularized empirical risk minimization approach reduces to

\[_{G}\|_{,}^{-1/2}-_{, }^{1/2}G\|_{()}^{2},\] (17)

and we analyze two different estimators, the first one \(_{,}\) is obtained by minimizing regularized empirical risk (17) over all \(G()\), and, hence, the name _Kernel Ridge Regression_ (KRR) of the generators resolvent. The second one \(_{,}^{r}\) minimizes (17) subject to the (hard) constraint that \(G\) is at most of (a priori fixed) rank \(r\) and, hence, is named _Reduced Rank Regression_ (RRR) of the generator's resolvent. Notice that when \(r=n\), the two estimators coincide. After some algebra, one sees that both minimization problems have closed form solutions

\[_{,}=_{,}^{-1} { and }_{,}^{r}=_{,}^{-1/2}[_{,}^{-1/2}]_{r},\] (18)

where \([]_{r}\) denotes the \(r\)-truncated SVD of a compact operator.

To conclude this section, we show how to compute the eigenvalue decomposition of (18). To this end, we define the sampling operators \(^{n}\) and \(_{}^{(1+p)n}\) by

\[(h)_{i}=}h(x_{i}),i\!\![n], (_{}h)_{kn+i}=\{} {}\,h(x_{i}),&k=0,i[n],\\ }s_{k}(x_{i})^{}Dh(x_{i}),&k\!\![p],i\!\![n]..\]

Further, let \(=n^{-1}[k(x_{i},x_{j})]_{i,j[n]}\!^{n n}\) be kernel Gram matrix, and introduce the Gram matrices \(\!\!^{n pn}\) and \(\!\!^{pn pn}\) whose elements, for \(k[1\!+\!p],i,j[n]\) are

\[_{i,(k-1)n+j}\!=\!n^{-1}(x_{i}),d_{k}(x_{j})_{ }\,\,\,\,\,\,_{(k\!-\!1)n\!+i,(-1)n\!+\!j}\!= \!n^{-1} d_{k}(x_{i}),d_{}(x_{j})_{}.\] (19)

We note that although we have introduced the above matrices via inner products in \(\), they can be readily computed via the kernel and its gradients knowing the Dirichlet form, see D.

**Theorem 1**.: _Given \(>0\) and \(>0\), let \(_{,}\!=\!\!-\!(\!+\! I )^{-1}^{}\!+\! I\). Let \((_{i}^{2},v_{i})_{i[r]}\) be the leading eigenpairs of the following generalized eigenvalue problem_

\[^{-1}(_{,}- I)v_{i}=_{ i}^{2}_{,}v_{i}, v_{i}^{}v_{j}\!=\!_{ij}, \,\,i,j[r].\] (20)

_Denoting \(_{r}\!=\![v_{1}\!\!]v_{r}\!\!^{n r}\) and \(_{r}\!=\!(_{1},,_{r})\), if \((_{i},w_{i}^{},w_{i}^{r})_{i[r]}\) are eigentriples of matrix \(_{r}^{}_{r}_{r}^{2}^{r r}\), then the eigenvalue decomposition the RRR estimator \(_{,}^{r}=_{}_{r}_{t}^{ }\) is given by \(_{,}^{r}=_{i[r]}(-_{i})^{-1} _{i}_{i}\), where \(_{i}=\!-\!1/_{i}\), \(_{i}=_{i}^{-1/2}^{*}_{r}w_{i}^{}\) and \(_{i}\!=\!_{}^{*}_{r}w_{i}^{r}\) for \(_{r}\!=\!()^{-1}[^{-1/2}I-(\!+ \! I)^{-1}]^{}(_{r}\!-\!_{r} _{r}^{2})\!\!^{(1\!+\!p)n r}\)._

The main computational cost of our method, in view of (12), to solve SDE (1) lies in the implicit inversion of \(_{,}\) when solving (20). When computed with direct solvers this inversion is of the order \((n^{3}p^{3})\), however leveraging on the fact that \(_{,}\) is Schur's complement of the \((1\!+\!p)n\!\!(1\!+\!p)n\) symmetric positive definite matrix and using classical iterative solvers, like Lanczos or the generalized Davidson method, when \(r n\) this cost can significantly be reduced to \((r\,n^{2}p^{2})\), c.f. .

## 5 Spectral learning bounds

Recalling Prop. 2, in order to obtain the bounds on eigenvalues and eigenfunctions of the generator, it suffices to analyze the learning rates for the operator norm error \(\) and metric distortion \(\). For this purpose, we analyze the operator norm error of empirical estimator \(_{,}^{r}\) using the decomposition

\[()Z_{}-Z_{}G_{, }\|_{}}_{}+(G_{, }\!-\!G_{,}^{r})\|_{}}_{}+(G_{,}^{r}\!-\!_{,}^{r})\|_{ }}_{},\]

[MISSING_PAGE_FAIL:8]

Proof sketch.The _regularization bias_ is bounded by \(c_{}\,^{/2}\) by Prop. 9 of App. E.2, the _rank reduction bias_ is upper bounded by \(_{r+1}(S_{}^{*}( I-L)^{-1}S_{})\), while in the exact knowledge case (\(0\)) the bounds on the variance terms critically rely on the well-known perturbation result for spectral projectors reported in Prop. 4, App. A. The latter is then chained to Pinelis-Sakhanenko's inequality and Minker's inequality for self-adjoint HS-operators, Props. 12 and 13 in App. E.3.1, respectively. When the knowledge is not exact, that is \(>0\) in **(DF)**, this relative bound implies that \(_{}{}_{}{-}_{}^{ }{}\,\,_{}\), where the empirical covariance with the inexact Dirichlet coefficient \(s_{}\) is denoted by \(_{}^{*}\). This allows one to control the additional approximation error in the analysis of variance, paying the price of additive term \(\). Combining the bias and variance terms, we obtain the balancing equations for the regularization parameter and then the next result follows. \(\)

First, note that the learning rate (23) implies the \(_{}^{2}\)-norm learning rate. Moreover, for \(\), it matches information theoretic lower bounds for transfer operator learning upon replacing parameters \(\), \(\) and \(\) related to the space \(\) with their \(_{}^{2}\) analogues , see App. E.6. This motivates the development of the first mini-max optimality for the IG learning, for which our results are an important first step. Next, remark that Theorem 2 guarantees the reliability of fully data-driven methods when the diffusion coefficients are not known but estimated. Furthermore, when \(b\) is constant or linear (e.g. Overdamped Langevin and CIR), the classical estimation bounds coincide with the relative error bound of assumption **(DF)**.

To conclude this section, we address the spectral learning bounds stemming from the Prop. 2. The main task to achieve this is to control the metric distortions, which we demonstrate in App. E.5. In this context, an additional assumption \( 1\) is needed, since otherwise the metric distortions can blow-up due to eigenfunctions being out of the RKHS space. Importantly, our analysis reveals that

\[-_{i}|}{|-_{i}||-_{i}|}(_{i}\,\,2^{}/ _{r}^{}})+c(_{n}^{}\,^{-1}+ ),\ i[r],\] (24)

where \(_{i}=_{i}\,_{i}\) is the empirical spectral bias that informs how good is the estimation of the particular eigenpair is (see Fig. 1 a), \(_{i}\) being given in (20) and \(_{i}{=}\,\|_{i}\|_{}/\|(_{} ^{})^{1/2}_{i}\|_{}\). Importantly, (24) reveals that our data-driven method for spectral decomposition of differential operator \(L\) does not suffer from the curse of dimensionality as present in the classical numerical methods, see App. E.6.

## 6 Experiments

In this section, we showcase the key features of our method outlined in Table 1. We demonstrate that our approach: (1) avoids the spurious effects noted in other IG methods [19; 1], (2) is more effective than transfer operator methods , and (3) validates our bounds in a prediction task for a model with a non-constant diffusion term. Further details are available in App. F.

**One dimensional four well potential** We first investigate the overdamped Langevin dynamics in a potential that presents four different wells, two principal wells and then in each of them two smaller wells, given by \(V(x){=}\,4(x^{8}{+}0.8({-}80(x^{2})){+}0.2({-}80(x{-}0.5)^{2}){+}0.5 ({-}40(x{+}0.5)^{2}))\). This leads to three relevant eigenpairs: the slowest mode corresponds to the transition between the two principal wells, while the others two capture transitions between the smaller wells. In Fig. 1 panel **a)**, we show that the empirical bias \(_{1}=_{1}\,_{1}\) allows us to choose the hyperparameters of the model, that is higher empirical bias coincides with unreliable estimation of the operator's eigenfunction. In panel **b)** we observe how it varies w.r.t. land-scale (y-axis) and regularization \(\) (x-axis) hyperparameters showcasing the robustness of the model, see also Fig. 3 of App. F for hyperparameter \(\). Further, in panel **c)** we show the consistency of our model with the true Boltzmann distribution. Namely, we use our model to forecast the conditional probability density function (pdf) of the system. We perform the same procedure with prefect knowledge and imperfect diffusion coefficient estimated from data. We also report that if the same approach is used with the method described in [19; 43; 1], no dynamical quantity can be forecast due to the presence of numerous spurious eigenpairs, which prevent the system from relaxing towards the Boltzmann distribution. This issue is further illustrated in panel **d)** where we show how, contrary to KRR method of [19; 1], we avoid spuriousness in the estimation of eigenvalues.

**CIR model** Next, with the CIR model we show that our method is not limited to Langevin process with constant diffusion. For this process, the conditional expectation of the state \(X_{t}\) is analytically known. We can thus compare the prediction of our model with respect to this expectation usingroot mean squared error (RMSE) and compute it for different number of samples to validate our bounds. Conditional expectation were computed on 100 different simulations at \(t=(2)/a\) which corresponds to the half life of the mean reversion. Results are shown in panel **e)** of Fig. 1.

**US mortgage rates** We have trained our method on a real 30-year US mortgage rates dataset and contrasted it with the fitted CIR model using continuous ranked probability scores that are estimated from the forecasts obtained by of each of them, see panel **f)** of Fig. 1. Each model has been trained using data from January 2009 to December 2016. The initial condition was the last week of December 2016 and the predictions were made for the years 2017 and 2018. Since the dataset is real, we used the imperfect partial knowledge, that is, for our method, we estimated the diffusion coefficient only via a least squares calibration of a CIR model over the training set. This allows more flexibility on the drift term in our model.

**Muller-Brown potential** We next study Langevin dynamics under more challenging conditions: the Muller-Brown potential. Panels **g)-h)-i)** of Fig. 1 depict the second eigenfunction obtained by our method compared to the ground truth one, as well as the one found by the transfer operator approach, with the same number of samples. Notably, our physics informed approach outperforms transfer operator learning for this task. Note that with different lag times, we were able to recover this second eigenfunction.

## 7 Conclusion

We developed a novel energy-based framework for learning the Infinitesimal Generator of stochastic diffusion SDEs using kernel methods. Our approach integrates physical priors, achieves fast error rates, and provides the first spectral learning guarantees for generator learning. A limitation is its computational complexity, scaling as \(n^{2}d^{2}\). Future work will explore alternative methods to enhance computational efficiency and investigate a broader suite of SDEs beyond stochastic diffusion.

Figure 1: **a)** Empirical biases \(_{1}=_{1}\,_{1}\) and estimation of the first (nontrivial) eigenfunction of the IG of a Langevin process under a four-well potential. The ground truth is shown in black, our method RRR is red and blue for two different kernel lengthscales. **d)** Estimation by our method (black) of the eigenvalues for the same process (red) compared to the methods in , for which eigenvalue histogram in blue shows spuriousness. **e)** Prediction RMSE for the CIR model w.r.t. number of samples. **f)** Performance of our data-driven method and fitted CIR model on the real data of US mortgage rates. **g)** The second eigenfunction of a Langevin process under Muller brown potential (white level lines) with its estimation by RRR **h)** and Transfer Operator (TO) in **i)**. Observe that TO fails to recover the metastable state.