# DiPlomat:

A Dialogue Dataset for Situated Pragmatic Reasoning

Hengli Li\({}^{1,2}\)

lihengli@stu.pku.edu.cn

&Song-Chun Zhu\({}^{1,3,4,5}\)

s.c.zhu@pku.edu.cn

&Zilong Zheng\({}^{1}\)

zlzheng@bigai.ai

\({}^{1}\) National Key Laboratory of General Artificial Intelligence, BIGAI

\({}^{2}\) Yuanpei College, PKU

\({}^{3}\) Institute for Artificial Intelligence, PKU

\({}^{4}\) School of Intelligence Science and Technology, PKU

\({}^{5}\) Department of Automation, THU

\({}^{}\) Corresponding author.

https://diplomat-dataset.github.io

###### Abstract

The ability to discern and comprehend pragmatic meanings is a cornerstone of social and emotional intelligence, referred to as pragmatic reasoning. Despite the strides made in the development of Large Language Models (LLMs), such as Chat-GPT, these models grapple with capturing the nuanced and ambiguous facets of language, falling short of the aspiration to build human-like conversational agents. In this work, we introduce a novel benchmark, the **DiPlomat**, which delves into the fundamental components of conversational pragmatic reasoning, encompassing situational context reasoning, open-world knowledge acquisition, and unified figurative language understanding. We start by collecting a new human-annotated dialogue dataset, composed of 4,177 multi-turn dialogues and a vocabulary of 48,900 words. Along with the dataset, two tasks are proposed to evaluate machines' pragmatic reasoning capabilities, namely, Pragmatic Reasoning and Identification(PIR) and Conversational Question Answering (CQA). Furthermore, we probe into a zero-shot natural language inference task, where the significance of context in pragmatic reasoning is underscored. Experimental findings illustrate the existing limitations of current prevailing LLMs in the realm of pragmatic reasoning, shedding light on the pressing need for further research to facilitate the emergence of emotional intelligence within human-like conversational agents.

Figure 1: Illustration of **DiPlomat** dataset. Left: Example of a pragmatic conversation. Right: Pragmatic Identification and Reasoning task.

Introduction

The fabric of human sociality is made up of complicated relations that evolve through different dimensions of interaction and communication channels [1; 2]. Social consensuses, such as social norms and values, are thereby formed between humans that convey meanings of individual minds, including beliefs, intentions and desires . In a process of effective negotiation and social conversation, particularly, such social behaviors are only partly driven by **literal** meanings that are _objective_, _rational_ and _explicit_[4; 5]. Instead, these behaviors are commonly governed by affective or **pragmatic** meanings of dialogue utterances that refer to the emotional and cultural meanings of conversational partners and are _subjective_, _emotional_ and _implicit_. For instance in Fig. 1, the lady responds with "I doubt that" rather than "I am not tired of you" to express a sense of humor and politeness. The competency of perceiving such _pragmatic_ meanings is crucial to social and emotional intelligence (EI)  and is referred to as **pragmatic reasoning**.

The rapid developments of large language models (LLMs), such as ChatGPT and InstructGPT , have set off a wave of the next generation of conversational AI over the recent years. Despite the inspiring capabilities of language generation  and reasoning  achieved with massive computational resources and tremendous natural language data, LLMs barely show convincing communicative skills , _i.e._, they fail to capture pragmatic and ambiguous meanings of input prompts [11; 10; 12]. Critically, current neural generative models are trained to be objective with safe and satisfiable responses [13; 14], which largely deviates from the long-standing goal of building a human-like agent. Recently, Meta Research Team _et al._ introduce a ChatBot that demonstrates human-level play in a language board game _Diplomacy_ where lying and misleading commonly occur. However, their main focus is on game policy learning rather than pragmatic reasoning.

**What are the core components of real-life conversational pragmatic reasoning?** Motivated by theories of cognitive linguistics [1; 2; 3] and conversational modeling [4; 5; 15; 16], we anticipate it to be three-fold:

* _Situational Context_ Reasoning. Understanding pragmatic meaning requires a detailed understanding of conversational contexts. Consider the utterance "You are making the rest of us looking bad", under different situations of praise and sarcasm, the sentence may convey completely opposite meanings. Furthermore, typical conversational reasoning challenges such as coreference resolution and intention prediction are largely dependent on the success of situated context modeling.
* _Open-world Knowledge_ Acquisition. The open-world knowledge includes commonsense knowledge (_e.g._, social ethics) that can be learned from different domains of dialogue corpus and domain-specific knowledge (_e.g._, American histories). Successful pragmatic reasoning requires the acquisition of open-world knowledge and joint reasoning over the conversation.
* Unified _figurative language_ understanding. Figurative language is one of the most frequently used tricks for conveying implicit meanings with subjective emotions. Previous works treat different forms of figurative language understanding as individual tasks, such as metaphors , idioms [18; 19], pun , _etc_. Pragmatic reasoning provides a feasible unified perspective that considers all these tasks as recovering their literal meanings.

In order to step towards a general human-like communicative agent, in this work, we introduce **DiPlomat**, a real-life **Dialogue** dataset that focuses on **Pragmatic** reasoning. **DiPlomat** stems from an interview dataset , and experiences three steps of curation: automatic selection, fine-grained manual annotation and human refinement (Sec. 3). Our dataset comprises \(4,177\) dialogues and covers a vocabulary of \(48,900\) words. More than that, human-annotated answers reach the amount of \(6,494\) and hold a vocabulary size of \(20,000\). Along with the dataset, we propose two tasks, Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA), to benchmark machines' pragmatic reasoning capabilities (Sec. 4). The CQA task possesses \(19,482\) questions concerning the content of collected dialogues and the answers to the questions are written by humans. We run extensive experiments on previous state-of-the-art models on **DiPlomat** (Sec. 5). The best model achieves less than 0.70 accuracy score in PIR, and none of the models achieve more than 0.50 accuracy score for CQA. Moreover, we test previous pre-trained LLMs' (including ChatGPT) zero-shot reasoning capability with a natural language inference (NLI) task. Regarding the experimental results provided, the significance of pragmatic reasoning speaks for itself. Throughout a thorough analysis of the limitations of current models, we aim to shed light on future research toward building general conversational agents.

## 2 Related Work

Conversational DatasetTab. 1 provides a comparative analysis of our dataset with similar conversational datasets. The Dream  dataset formalizes dialogues from English exams into question-answering task with a focus on in-depth dialogue comprehension. With question-answer pairs compiled by two annotators, CoQA  focuses on reasoning in conversation understanding. By utilizing English listening comprehension tests, MuTual  is built to address the issue of general dialogue reasoning. In contrast to these preceding datasets, the GRICE  dataset represents a significant advancement in the field of pragmatic reasoning as it incorporates implicature and reasoning. However, both MuTual and GRICE exhibit a shared limitation in that they do not possess data that closely resembles real-world interactions, leading to a lack of diversity in their respective datasets. Additionally, other relevant datasets, including commonsense , reasoning , and natural language inference (NLI) , are also included in Tab. 1. By examining various dimensions such as domain, manual annotation, task variety, implicature, reasoning, and multi-turn interactions, our dataset offers unique advantages in addressing the challenge of pragmatic reasoning in dialogues.

Language Models for Dialogue GenerationConversational AI has emerged as a prominent research area within the field of natural language processing (NLP), attracting significant attention and interest. Numerous pre-trained models have been proposed to tackle dialogue generation tasks, such as DialogGPT , GODEL , LaMDA , and Meena , and they have achieved marvelous results on competitions . Furthermore, a pivotal milestone has been achieved with the advent of ChatGPT, garnering widespread interest and stimulating further investigation in the domain of conversational AI. ChatGPT, built upon the principles of transformer models , undergoes training on an extensive corpus of data, resulting in its profound efficacy. Notably, this system boasts an impressive magnitude of billions of parameters. In a notable study conducted by OpenAI , it has been demonstrated that the augmentation of parameters, referred to as the Scaling Law, substantially enhances the model's capabilities. Also, the enlargement of the number of parameters triggers the emergence of miraculous ability. After the success of ChatGPT, more models such as PaLM 2  appeared in the field.

Pragmatic ReasoningPragmatic reasoning is a significant subject within the field of pragmatics, attracting considerable attention from linguists. The Gricean maxims, which is one of the most important achievements, serves as a foundational theory within the domain of pragmatics. This theoretical framework comprises four distinct maxims: (1) The Maxim of Quality, (2) The Maxim of Quantity, (3) The Maxim of Relevance, and (4) The Maxim of Manner . In contrast to rigid

  
**Dataset** & **Domain** & **Manually** & **Task** & **Implicature** & **Reasoning** & **Multi-Turn** \\  Ubuntu [ACL, 2015] & Technique & ✗ & NUP & ✗ & ✗ & ✓ \\ RACE [EMNLP 2017] & Open & ✗ & QA & ✗ & ✓ & ✗ \\ ARC [ANKW 2018] & Science & ✗ & QA & ✗ & ✓ & ✗ \\ MNLI [NAACL 2018] & Open & ✓ & NLI & ✗ & ✓ & ✗ \\ Persona-**Chat** [ACL, 2018] & Persona & ✓ & NUP & ✗ & ✗ \\ SWAG [EMNLP 2018] & Movie & ✗ & PI & ✗ & ✓ & ✗ \\ Cosmos QA [EMNLP 2019] & Persona & ✓ & QA & ✗ & ✓ & ✗ \\ CoQA [NAACL, 2019] & Diverse & ✓ & QA & ✗ & ✓ & ✓ \\ DREAM [ACL, 2019] & Open & ✓ & QA & ✗ & ✓ & ✓ \\ Dialogue NLI [ACL, 2019] & Persona & ✗ & NLI & ✗ & ✗ & ✓ \\ DROP [ACL, 2019] & Open & ✗ & QA & ✗ & ✓ & ✗ \\ MuTual [ACL, 2020] & Open & ✓ & NUP & ✗ & ✓ & ✓ \\ IMPPRES [ACL, 2020] & Open & ✗ & NLI & ✓ & ✓ & ✗ \\ GRICE [ACL, 2021] & Daily & ✗ & IR \& QA & ✓ & ✓ & ✓ \\ 
**DiPlomat** & Open & ✓ & PIR \& QA & ✓ & ✓ & ✓ \\   

Table 1: Comparisons on similar datasets and our dataset. QA: Question Answering. NUP: Next Utterance Prediction. NLI: Natural Language Inference. PI: Plausible Inference. IR: Implicature Recovery. PIR: Pragmatic Identification and Reasoning. Manually: the dataset is being checked or collected by humans. Diverse: the dataset lies in several specific domains. Open: the dataset doesn’t fall into particular domains.

rules or theorems, the Gricean maxims, which capture the prevalent dynamics of conversations, are susceptible to frequent breaches in the context of human communication. These breaches, stemming from the intricacies of real-world interaction, notably manifest in the violation of one or more of these maxims. Such breaches, aligned with the cooperative principle, give rise to pragmatic phenomena that necessitate the engagement of pragmatic reasoning by recipients of the communication . In the field of natural language processing, previous work tries to model the problem, but predominantly focus on specific types of phenomena. For example, EPIE , PIE  center around idiomatic expressions, while MOVER  emphasizes hyperbole, and MERMAID  investigates metaphor usage. However, paronomasia is much more under-studied , with researchers frequently intertwining it with humor . In a related vein, GRICE  endeavors to study implicature in a unified manner, but its data does not originate from real-world contexts, thereby lacking diversity and exhibiting conspicuous patterns. Insights into the comprehension of figurative language are provided by Stowe et al.  and Chakrabarty et al. , with the former specifically delving into the realm of metaphors and idioms, and the latter investigating idioms and similes. The successful completion of our task needs the incorporation of commonsense knowledge. Extensive scholarly efforts have been dedicated to addressing the challenge of leveraging commonsense in various works .

## 3 The DiPlomat Dataset

### Data Source Construction

The **DiPlomat** dataset stems from the Interview dataset , which consists of two subsets, a two-party subset comprising 23,714 dialogues and a multi-party subset with 105,848 dialogues. Given our specific focus on conversations involving only two communicators, we exclusively utilize the two-party subset for our research purposes. The Interview dataset itself is a real-world collection of NPR radio transcripts, spanning a period of 20 years of NPR programs. The curation process for our dataset involved several stages, including automatic selection, fine-grained annotation, and human refinement. Details are provided as follows.

**Step I. Automatic Selection.**  The extensive size of the source dataset introduces redundancy, and thus requires automatic measures to alleviate the burden of human annotation. Therefore, we employ algorithms and models to perform an initial filtering process. In order to establish a unified framework,

Figure 2: **DiPlomat dataset samples. Each row illustrates an exemplar case with its reasoning type, dialogue context, pragmatic turn, and the corresponding rationale. Evidence that support the pragmatic identification are marked in orange.**

we consider various types of pragmatic phenomena and utilize different techniques to extract relevant instances from the source dataset. For instance, we utilize the EPIE list  for a string-matching method to identify idioms in dialogues, and we train RoBERTa  on Hypo-XL  for hyperbole detection; refer to Appendix B.

Topic SegmentationTopic segmentation is a small operation taken after automatic selection. The original dialogues employed in our study consist of lengthy and multi-turn exchanges, which are ill-suited for our research objectives. Consequently, we implement a segmentation process to break down these dialogues into shorter units. To achieve this, we employ two techniques, namely BERTScore  and TextTiling . The segmentation procedure starts with computing the BERTScore between adjacent turns and subsequently applying the TextTiling algorithm to the generated BERTScores.

Step II. Fine-grained Annotation.We leverage Amazon Mechanical Turk (AMT) to conduct detailed annotation of pragmatic turns within our dialogues. Workers participating in the annotation task are instructed to select all turns that exhibit a divergence between their literal meaning and their intended meaning. Due to the subjective nature of pragmatic reasoning, we request the workers to provide confidence scores along with reasons for their choices and for each dialogue two workers are recruited. All annotators shall meet the following criteria: (i) from English-speaking countries; (ii) Completion of a minimum of 1,300 tasks with a 98% approval rate. We also present them with detailed instructions and four examples so that workers are clear about our objectives and requirements. The instructions part outlines the step-by-step procedures for accomplishing the assigned tasks and highlights some key points that workers should pay attention to. The four examples offered are representative of classical pragmatic conversations drawn from the field of linguistics, serving as practical references for the workers. To mitigate the intricacies arising from the identities of dialogue communicators, a simplified representation is adopted, whereby the speakers are denoted as PersonA and PersonB. To ensure the reasonableness and quality of the data, we manually examined 30 data samples and blocked workers who are unqualified. As a result, a total of 5,869 dialogues are selected; refer to Appendix B for details.

Step III. Human Refinement.In this process, tasks for workers are formulated as multiple-choice questions. Previously collected human-annotated reasons are transformed into choices, utilizing a template format: [turn {turn_id}: {reason}]. In addition, to mitigate the impact of careless workers, we introduce a distractor choice for each gold choice. These distractor choices are generated using BERTScore  by selecting the reason with the highest score from other unrelated dialogues. Of note, for each dialogue, an equal number of gold choices and distractor choices are provided. Workers are requested to select all reasonable choices for each conversation and are warned of the presence of distractor choices. Workers who frequently select distractor choices are blocked, and their work is rejected. Through this refinement process, 1,692 dialogues are filtered out, while 4,177 dialogues are preserved, ensuring the integrity and reliability of our dataset; refer to Appendix B for more details.

### Dataset Statistics

The **DiPlomat** dataset comprises a total of 4,177 multi-turn dialogues, with each dialogue averaging 4.1 turns. On average, there are 1.56 pragmatic turns per dialogue. The distribution of dialogues with different quantities of pragmatic turns is illustrated in Fig. 4; see Tab. 2 for detailed dataset statistics.

With respect to the motivation introduced in Sec. 1, we categorize the process of transitioning from dialogue to human-annotated rationales into five reasoning types:

* **Contextual Reasoning:** The comprehension of the context is paramount for this reasoning process.
* **Figrative Language Reasoning:** Proficiency in understanding figurative language, such as idioms and metaphors, is indispensable for advancing this type of reasoning.
* **Commonsense Reasoning:** The utilization of commonsense knowledge, such as recognizing that a chateau cannot fall from the sky, is vital for this category.

   \# Dialogues & \(4.17 10^{3}\) \\ Avg. Turns per Dialogue & \(4.10\) \\ Avg. Words per Turn & \(42.80\) \\ Avg. Human Reason per Dialogue & \(1.56\) \\ Avg. Words per Human Annotated Reason & \(25.31\) \\ Vocabulary Size (dialogue) & \(4.89 10^{4}\) \\ Vocabulary Size (human-annotated reasons) & \(2.00 10^{4}\) \\   

Table 2: Statistical feature of **DiPlomat**.

* **External Knowledge Reasoning:** This form of reasoning necessitates knowledge that extends beyond commonsense and is not explicitly mentioned in the dialogue.
* **Others:** This category includes pragmatic dialogues that fit none of the above.

Fig. 4 demonstrates the proportion of each type. The prevalence of data within the context partition prove the significance of context in pragmatic reasoning of real life. Fig. (a)a depicts a sunburst visualization illustrating the distribution of trigram words within pragmatic turns. The diverse range of trigram words indicates that the **DiPlomat** dataset enjoys the rich diversity from real-life corpora, and covers a wide array of topics. In addition, the recurring occurrence of the words "president" and "world" is observed, demonstrating **DiPlomat**'s slight bias to politics and world-wide events.

## 4 Task Definition

We propose 2 distinct tasks for our dataset: (i) Pragmatics Identification and Reasoning and (ii) Conversational Question Answering. The former task focuses on assessing the capability of models to identify the presence of pragmatic phenomena and their ability to select a suitable answer for such identification. The latter task aims to evaluate the models' adeptness in employing pragmatic reasoning by presenting them with carefully designed questions.

### Task 1: Pragmatics Identification and Reasoning (PIR)

In this task, models are provided with dialogues and are required to identify turns whose actual meanings deviate from their literal interpretations, commonly referred to as pragmatic turns. If their selections are accurate, a set of rationales is presented and they are expected to choose the most plausible reason for each pragmatic turn. For each turn, there are 5 candidate reasons available, comprising one gold choice and four disturbing choices. The model's success in this task depends on the precise execution of both steps. We consider three diagnostic settings to test the machine's capability on pragmatic understanding:

* Conversation \(\) Pragmatic Turn (\(\)). For each instance, models are presented with a dialogue and a specific turn extracted from that dialogue. They are then required to determine whether the given turn qualifies as a pragmatic turn. Consequently, the dataset is flattened to a total of 17,129 instances, with each instance corresponding to a single queried turn. It's important to highlight that turns without pragmatic meanings are also extracted for evaluation.
* Conversation \(+\) Pragmatic Turn \(\) Rationale (\(\)). In this subtask, we offer the model both the dialogue and the pragmatic turn and it needs to choose the most plausible rationale out of five candidate choices.
* Conversation \(\) Pragmatic Turn \(+\) Rationale (\(\)) In this subtask, models pre-trained on the previous two subtasks are combined to infer the final results. Specifically, the model obtained from the first subtask is utilized for determining pragmatic turns, while its version finetuned on the second subtask is employed for selecting the most suitable rationale. It is worth noting that, in contrast to \(\), in this subtask, extracted turns are limited to pragmatic turns only.

Pragmatic Turns and Gold ChoicesRecall that in our data collecting procedure outlined in Sec. 3.1, besides asking workers to select pragmatic turns, they are also instructed to fill in reasons to explain their choices. To simplify the evaluation process, the selected turns are directly utilized as answers for the first subtask, while the reasons provided by the workers serve as the designated correct choices (referred to as "gold choices") for the second subtask.

Distractor ChoiceAs a result of the time-consuming nature of BERTScore , an alternative approach is adopted for measuring sentence similarity using Sentence-Transformers , which is a significantly faster method. In our methodology, for each gold choice, four alternative choices with high similarity scores are selected from the pool of gold choices to serve as distracting answers. Despite their high similarity scores, upon careful examination within the given context, it's apparent that the distracting answers convey entirely different meanings from the gold answer. This characteristic makes them appropriate components to build our task.

### Task 2: Conversational Question Answering (CQA)

The ability to apply pragmatic reasoning is crucial for effective communication and achieving a thorough grasp of the natural language system of human beings. To address this, we propose conversational question-answering, wherein multiple questions are formulated for each dialogue, and an example is shown in Fig. 5. The questions focus on delving deeper into dialogues, often needs insights into intended meanings to answer. ChatGPT plays a pivotal role in question generation and thanks to AMT, we can ensure the collection of high-quality answers. Ultimately, \(19,482\) question-answer pairs are assembled.

Question GenerationChatGPT is employed to generate questions with prompts consisting of dialogues and human-annotated reasons. We task it to generate questions challenging for individuals who are unaware of the dialogues' intended meanings. More than that, for the convenience of evaluation, the question is also asked to be able to answer within one or two words. Furthermore, to ensure diversity, we instruct ChatGPT to start the questions with "What", "Which" or "How". A preliminary assessment is carried out by sampling a few examples out of the question pool to guarantee quality.

Answer CollectionThe answers to the questions are obtained through the utilization of AMT. Each worker is provided with a dialogue along with several associated questions and it is requested to answer the questions in one single word. To minimize the potential for misinterpretation, we offer an example coming from our dataset, which is annotated by the author itself. Through the process of human annotation, we consistently evaluate the collected data and reject unqualified answers as well as block workers who fail to meet our standards.

Statistical features of QuestionsFig. 2(b) showcases the diverse range of our questions. These questions encompass a variety of sentence structures, starting with interrogative words: What, Which, and How, and possessing a large diversity of the words that follow the interrogative ones. Apart from the questions, the answer set holds a vocabulary size of 8,179, which is also of great diversity and raises a challenge for models.

## 5 Experiment

### Pragmatics Identification and Reasoning

For \(\), we partitioned the dataset into distinct subsets for training, validation, and testing. The training set consists of 13,708 examples, surpassing the 1,361 instances in the validation set and the 2,060 instances in the test set in terms of size. Models are trained on the training set and their

Figure 5: Conversational Question Answering example.

performance is evaluated on the validation set after each epoch to determine the optimal checkpoint. The best checkpoint is subsequently loaded for the final evaluation on the test dataset. The evaluation metric employed is the accuracy score, calculated as the ratio of correct predictions to the total number of instances. Similarly, for the task of \(\), the dataset is also partitioned into training, validation, and test subsets. The respective sizes of these subsets are 5,188, 244, and 1,062 examples. The training and evaluation procedures are identical to those of the previous subtask. For \(\), the test sets in previous subtasks are taken for evaluation, and it's worth noting that their test sets consist of exactly the same instances. This design ensures the prevention of any leakage of the test set into the training set, thereby maintaining the integrity of the evaluation process.

Results and AnalysisWe present four key observations for this task:

\(\) The primary performance bottleneck lies in the subtask \(\). As shown in Tab. 3, the best model achieves an accuracy score of 92.0% in the \(\) subtask, indicating that models possess the capability to reason to some extent. However, when it comes to the \(\) task, the best-performing model achieves only 50.2% accuracy, while the highest accuracy achieved in the \(\) task is 65.0%. The substantial difference between the score of 92.0% and the score of 65.0% suggests that the difficulty in accomplishing the task primarily stems from the \(\) subtask.

\(\) Accumulated variance in the \(\) subtask. The models exhibit significant variance in the results of the third subtask, which can be attributed to the variance introduced by its constituent tasks.

\(\) The significance of pragmatic awareness in language models. Both the \(\) and \(\) subtasks require pragmatic awareness, and the poor performances of the models on these subtasks highlight their limitations in accurately determining the optimal timing for deploying reasoning abilities.

\(\) As depicted in Fig. 6, we assess the performance of existing models across five dimensions by leveraging the taxonomy outlined in Fig. 2. It becomes evident that models exhibit a nearly uniform performance across these dimensions, leading us to the inference that pragmatic reasoning constitutes a cohesive task, and a segregated approach is ill-suited for its treatment.

### Conversational Question Answering

Similarly to the previous task, the question-answering dataset is divided into training, validation, and test sets, comprising 15,585, 1,559, and 2,338 instances, respectively. Experimental subjects includes BART , T5 , UnifiedQA , and mT5 . The metric adopted is also accuracy score. Given ChatGPT's impressive performance in MMLU and AI2 Reasoning Challenge , we further examine ChatGPT's capability in the context of CQA by prompting it to provide one-word answers to questions. However, due to its uncontrollable nature, the generated answers may not always align with our desired settings. Hence, we introduce two evaluation metrics for ChatGPT: (1) em (exact match), which requires ChatGPT to produce the exact same word as our answer, and (2) pm (partially match), where we consider ChatGPT to be correct as long as our answer appears in its generated output. Two configurations are employed for evaluation. In the first configuration, models receive dialogues and questions while remaining blind to human-annotated rationales. The

   & \(\) & \(\) & \(\) \\  Random & 50 & 20 & 10 \\ \(_{}\) & \(63.2 1.1\) & 91.3 \(\) 0.7 & \(\) \\ \(_{}\) & \(64.4 1.3\) & 92.0 \(\) 0.4 & 50.0 \(\) 11.28 \\ \(_{}\) & \(63.8 0.0\) & 60.8 \(\) 0.5 & 0.0 \(\) 0.0 \\ \(_{}\) & \(64.4 0.7\) & 90.9 \(\) 0.9 & 13.06 \(\) 1.1 \\ \(_{}\) & \(65.0 0.6\) & 24.5 \(\) 1.9 & 3.8 \(\) 1.5 \\ \(_{}\) & \(64.9 0.2\) & \(\) & 43.9 \(\) 1.2 \\ \(_{}\) & \(\) & 90.6 \(\) 0.2 & 34.9 \(\) 1.8 \\  

Table 3: Pragmatics Identification and Reasoning Results. The numerical results are accuracy scores in their percentage.

Figure 6: Models performances of different reasoning types. The taxonomy of reasoning types comes from Fig. 2.

second configuration is a contrasting experiment with human-annotated rationales provided. For each configuration, we run each model three times using different random seeds and report the mean and variance of their results as the final outcomes.

Results and AnalysisThe experimental results are presented in Tab. 4. Our observations can be categorized into three main aspects. Firstly, the importance of pragmatic meaning is proved. As shown in Tab. 4, there exists a notable disparity between the results of models that have access to human-annotated answers and those that do not. On average, the model performances improve by 38.47% after the introduction of human-annotated rationales. Even the lowest-performing model in the initial experiment, mT5-small, demonstrates a 9.4% increase in accuracy. The substantial discrepancy in results between the two configurations underscores the significance of elucidating intended meanings in the development of effective communicators. Second, the models display deficiencies in applying pragmatic reasoning. Our questions are designed to demand a deeper understanding of conversations, however, the models struggle to perform well on our task. The best-performing model, ChatGPT, achieves an accuracy score of 40.6%. It is worth noting that our questions are generated by ChatGPT itself, and our source dataset, Interview, was proposed prior to the emergence of ChatGPT, which means that ChatGPT may have encountered our text during training. These characteristics render its result unsatisfactory. Third, generalization across different types of pragmatic reasoning poses challenges. In this analysis, we focus exclusively on models other than ChatGPT due to the lack of clarity regarding its training process. As demonstrated in Tab. 4, these models showcase a substantial improvement in performance following the inclusion of human-annotated rationale. The extent of this improvement exhibits slight fluctuations among the various models, suggesting a shared obstacle that hinders their overall performance. Noticed that the models are fine-tuned on a training set that is 5.2 times larger than the test set, we can conclude that achieving effective generalization from one pragmatic reasoning process to another remains a formidable and challenging task.

### Zero-Shot Natural Language Inference

We conduct the natural language inference (NLI) task  to evaluate the model's comprehension of language and to emphasize the importance of context in pragmatic reasoning. Different from previous tasks, zero-shot NLI task sheds a light on models' initial ability as they are tested without finetuning. The task involves providing two sentences: a premise and a hypothesis, and models are required to determine the relationship between the two sentences, which can be entailment, contradiction, or neutral. As there are no negative samples in our dataset, we simplify the task by asking the model to judge whether there is entailment. In this task, models are presented with a dialogue, a turn of the dialogue, and an intended meaning, they need to judge whether the turn entails the intended meaning. Noticed that collected data as described in Sec. 3.1 consists of reasons and implied meanings, to better fit our task, we abandon the reasons and preserve the implied meanings. Models are tested under zero-shot setting, which means that they are not allowed to train on any data before testing. Thus the innate abilities of models play a decisive role. Baseline models include T5 , DeBERTa , and ChatGPT. It's important to note that ChatGPT and the other two models are tested on different settings. ChatGPT is tested with the whole dialogue and the implied meaning as a prompt. However, to inspect the significance of context, the other two models are only provided with the pragmatic turn and the corresponding pragmatic meaning. ChatGPT is evaluated using two types

    & BART-base & T5-small & mT5-small & UnifiedQA-base & UnifiedQA-large & ChatGPT (em) & ChatGPT (pm) \\  w/o rationale & 20.2 \(\) 1.1 & 24.9 \(\) 0.7 & 19.7 \(\) 0.4 & 28.9 \(\) 2.0 & 32.8 \(\) 0 & 1.0 & **40.6** \\ w/ rationale & 29.6 \(\) 0.6 & 34.1 \(\) 1.4 & 29.8 \(\) 0.8 & 38.8 \(\) 0.2 & 42.4 \(\) 0.6 & 1.5 & **45.1** \\ \(\) & +9.4 (46.5\%) & +9.0 (35.8\%) & +**10.1 (51.3\%)** & +9.9 (34.1\%) & +9.6 (29.3\%) & +0.5 (60\%) & +5 (12.3\%) \\   

Table 4: CQA task results with/without human annotated rationales. The numerical results are accuracy scores in their percentage. em: exact match, pm: partially match.

  
**Method** & **Acc.** \\  Random & 50.0 \\  DeBERTa-Large\({}^{}\) & 44.3 \\ T5-XXL\({}^{}\) & 45.3 \\ ChatGPT & **85.7** \\ ChatGPT w/ CoT & 63.8 \\   

Table 5: Results of Natural Language Inference Task. \({}^{}\): T5-XXL fine-tuned on true NLI mixture . \({}^{}\): DeBERTa-v3 trained on MNLI  and SNLI .

of prompts: with and without step-by-step instructions. "Let's think step by step" is a prompt discovered  to improve the model's reasoning ability; refer to Appendix C for more details.

Results and AnalysisResults are listed in Tab. 5. As this task shares similar settings as binary classification, randomized answer accuracy is expected to be 50%. We observe below randomized performance on some previous SOTA models. Note that, each of the data is annotated by two humans, thus it's reasonable to view human performance as 100%. ChatGPT achieves the highest result but still shows a huge gap compared with human annotations. The outcomes highlight the imperfectness of the models' reasoning abilities. For T5-large and DeBERTa, context is blinded, but for ChatGPT, it is reachable. Hence, the performance gap among T5-large, DeBERTa, and ChatGPT shows the importance of context in our task. Interestingly, CoT doesn't offer help to ChatGPT but is harmful to ChatGPT's performance.

## 6 Discussions and Future Work

In this paper, we propose **DiPlomat**, a high-quality manually annotated multi-turn dataset of pragmatic reasoning in conversations. Along with the dataset, we propose two tasks and baselines. Comparing experimental results, we emphasize the nonnegligible impact of contexts and reasoning on building perfect communicators. We also highlight the importance of pragmatic awareness and its bottleneck effect on our tasks. There is still a significant disparity between current performances and established standards.

Memorization _vs._ ReasoningNoticed that models exhibit outstanding performance on \(\) of Sec. 5.1. On the contrary, for \(\), models achieve poor results. Since the underlying knowledge is consistent for both tasks, the disparity in performance is hypothesized to be attributed to _memorization_. Instead of truly understanding the knowledge, the models tend to memorize patterns.

Subjectiveness _vs._ ObjectivenessJi et al.  emphasize the importance of modeling a distribution that encompasses a diverse range of possibilities, rather than solely relying on a single "best" prediction. During the annotation process, we observe a phenomenon that different workers hold diverse opinions regarding pragmatic turns and their intended meanings. Their annotations often exhibit significant variations, sometimes even presenting completely opposing interpretations. We maintain the possibility of subjectiveness with careful task metric design (Sec. 4.1).

Larger Models _vs._ Smaller ModelsConventional wisdom suggests that larger models, endowed with a greater number of parameters, generally exhibit superior performance in comparison to their smaller counterparts. However, an intriguing observation comes to the forefront within the context of the Pragmatic Identification and Reasoning (PIR) experiment, as elucidated in Table 3: RoBERTa\({}_{}\) surpasses RoBERTa\({}_{}\) in performance. We posit that this phenomenon can be attributed to a dual-fold rationale. First and foremost, a divergence emerges between the domain of pragmatic data and the domain of pretraining data for language models. Consequently, larger models exhibit a more substantial and consistent influence of their pretraining data, rendering them less adaptable to domain shifts towards pragmatic data. Secondly, the current landscape of pragmatic data is characterized by its inherent diversity and relative scarcity. This inherent diversity poses a particular challenge for larger models in adapting to such heterogeneity; refer to Appendix A.

Future WorkAchieving generalization across multiple types of pragmatic reasoning processes poses significant challenges. Consequently, we propose that the construction of a proficient communicator necessitates the incorporation of methods beyond purely data-driven approaches. Furthermore, the availability of comprehensive evaluation data is of utmost importance. As a result, we target more high-quality datasets and new methods other than data-driven for the problem.