# When Does Perceptual Alignment

Benefit Vision Representations?

 Shobhita Sundaram\({}^{1}\)1

Stephanie Fu\({}^{2}\)1

Lukas Muttenthaler\({}^{3,4}\)2

Netanel Y. Tamir\({}^{5}\)

Lucy Chai\({}^{1}\)

Simon Kornblith\({}^{6}\)

Trevor Darrell\({}^{2}\)

Phillip Isola\({}^{1}\)

\({}^{1}\)MIT \({}^{2}\)U.C. Berkeley \({}^{3}\)TU Berlin \({}^{4}\)BIFOLD \({}^{5}\)Weizmann Institute of Science \({}^{6}\)Anthropic

###### Abstract

Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in general-purpose settings remains unclear. Here, we investigate how aligning vision representations to human perceptual judgments impacts their usability across diverse vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard benchmarks. We find that perceptual alignment yields representations that _improve_ upon the original backbones across many tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation, while deteriorating performance on natural classification. Performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.

Figure 1: **Does human perceptual alignment improve vision representations?** Vision models have been shown to learn useful image representations through large-scale pretraining (e.g., CLIP, DINO). We find that additionally aligning these models to human perceptual judgments yields representations that _improve_ upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation, while degrading performance in natural classification tasks. Our blog post and code are available at percep-align.github.io.

Introduction

Our sense of similarity is crucial to how we perceive and act in the world. Consider the range of factors that might influence your judgment of visual similarity: layout, color, perspective, semantics, and more. These characteristics shape our local inferences about object relationships, enabling us to build a comprehensive understanding of the visual world.

Given the importance of similarity judgments in our visual perception, it follows that aligning vision models to these judgments could help them develop more human-like visual capabilities. In the language domain, we have seen the power of aligning LLMs to human feedback [RLHF; 9], resulting in safer and more useful models . Similar trends are emerging in vision, where there is growing interest in alignment with human perceptual judgments to improve vision representations [e.g., 61; 17; 42; 18; 43]. Human feedback has also been used to improve the aesthetic quality of diffusion-generated images [58; 16; 30], increase retrieval capabilities , and improve downstream task performance of image/text models . While acknowledging this wide space of human preference data used for alignment in vision and language, we focus our study on image similarity judgments, which give us a more stable and shared measure of human visual perception [18; 41; 54; 43].

Although there is consensus that alignment to perceptual judgements can enable specific goals (e.g., similarity [18; 61; 41; 42]), their utility in creating _general-purpose_ representations is less clear. Does human alignment improve model performance by leveraging human-provided labels as an inductive bias, or compromise the model's original representational power by diverting it towards a separate task? Recent work suggests that the conclusions are nuanced: naively incorporating human perceptual knowledge into model finetuning can distort representations, requiring strong regularization to maintain downstream performance while increasing the representations' interpretability and alignment . Furthermore, objective function and training data appear to matter more than architecture or model size for aligning to perceptual judgments . The nature of human preference labels also plays a crucial role, as different types of labels produce distinct learning signals. For instance, fine-tuning vision backbones with mid-level perceptual similarity judgments yields representations well-suited for image retrieval tasks , while low-level similarity labels are more effective for image reconstruction losses .

It is clear from this body of work that careful perceptual alignment helps with specific tasks, however the effect of these adjustments on the models' representation spaces is less well understood. That is, are models aligned to perceptual judgments only better at specific tasks - such as predicting image similarity - or do they, like humans, actually have better general-purpose representations? Here, we evaluate the usefulness of human-aligned representations not just in predicting perceptual judgments but also on standard vision benchmarks requiring diverse notions of visual understanding. We finetune several state-of-the-art models - including CLIP, DINO, DINOv2, and SynCLR - on NIGHTS, a dataset of human similarity judgments over synthetic image triplets , and evaluate the finetuned models on standard tasks. Our experiments suggest that these human-aligned representations demonstrate strong gains compared to the original model backbones, even on tasks requiring skills beyond those needed during training or perceptual alignment. We also identify limitations of human perceptual alignment, and find that finetuning can sacrifice performance on some natural data tasks in which models had strong prior performance. In summary, our contributions are the following:

* We investigate the effects of aligning pretrained vision models to human perceptual judgments on various downstream visual recognition tasks.
* We find that propagating global image-level human similarity annotations to ViT patch tokens benefits downstream dense prediction tasks such as depth prediction and segmentation.
* We show that human-aligned representations are also beneficial in retrieval-based tasks requiring global image understanding, including retrieval-augmented generation for recent vision-language models, counting-based retrieval, and instance-based retrieval.
* We ablate the effect of various human-annotated visual similarity datasets, and find that mid-level image similarity, as opposed to low-level pixel-level variations or high-level semantic associations, offers the largest improvements in generalization capabilities.

## 2 Related Works

**Vision backbones as learned feature extractors.** Using the intermediate activations of deep networks has been a long-standing strategy, originally used as a way of harnessing data-driven priorsfrom models trained on fewer large datasets and repurposing them for downstream tasks where such expensive supervision is less readily available [12; 50]. For instance, ImageNet pretrained models have proven useful for tasks such as texture synthesis , style-transfer , and super-resolution . But given the challenges of collecting large-scale labelled datasets using human annotators, other approaches such as self-supervised learning [6; 7; 59; 4; 44] and contrastive learning on image/alt text pairs [46; 8; 26] have since superseded supervised learning as the standard approach for training vision models. Such models have been shown to yield rich multipurpose representations that can generalize to a variety of tasks involving visual perception, scene understanding, and image generation [51; 38; 52; 31; 39; 57].

**View selection for self-supervised learning.** The self-supervised, contrastive-learning objective aims to maximize the feature similarity between similar views of the data and minimize the similarity between different (negative) views. While originally the negative views are selected randomly from a pool of images [6; 22], this often results in requiring large batch sizes to learn useful representations. The process of selecting these positive and negative learning examples remains an active research area. Recent approaches have suggested that alternative strategies including hard-negative mining , nearest neighbors for positive pairs , and supervised labels when available  can provide useful learning signals. Other avenues include training the contrastive framework on synthetic data from generative models [25; 55], which provides an infinite data source of image variations rather than the traditional data augmentations typically used to generate alternative views.

**Learning with human alignment.** Learning directly from human feedback can provide models with more targeted supervision using fewer examples , and, thus, has been beneficial for fine-tuning large models towards specific human preferences [53; 62]. Ding et al.  show that image similarity metrics, trained on human feedback, can be subsequently used for evaluating the diversity of a text-to-image model. Several datasets aim to annotate human visual preferences in image similarity, including low-level  and high-level  image variations. In particular, Zhang et al.  and Fu et al.  use these datasets to learn a more human-aligned perceptual metric that improves the image retrieval abilities of vision models, while Muttenthaler et al.  used THINGS for learning a linear transform on top of vision representations to increase downstream task performance and improve alignment with human similarity judgments. These annotated human preference datasets serve as useful signals for contrastive objectives, providing direct supervision for positive and negative pairings that are aligned with human decisions; here, we investigate how fine-tuning self-supervised models using these human annotations impacts model performance on various downstream tasks.

## 3 Learning from perceptual judgments

We propose to use the method described below as a "second pretraining stage", which aligns the feature representations from large vision models with human perceptual judgments before applying them to downstream tasks. We note that prior work on this dataset aimed to develop a model for measuring image similarity based on human judgments. Here, we investigate if pretraining on this dataset leads to a better general-purpose representation, as measured by performance on different downstream tasks.

### Human Similarity Annotations

We use the NIGHTS dataset to produce human-aligned variations of several large vision models . The NIGHTS dataset consists of 20k synthetically generated image triplets, annotated with two alternative forced-choice human similarity judgments. These triplets are collected so that each has 6-10 unanimous human ratings, thus eliminating ambiguous cases where humans are likely to disagree.

NIGHTS consists of image triplets varying in _mid-level_ information. Images in a triplet roughly share the same semantic content; however, they vary in pose, layout, shape, color, and the number of objects (see Fig. 15 in the Appendix for examples). Thus, the perceptual judgments indicate the shared visual appearance properties, as opposed to requiring higher-level semantic knowledge about the image content.

### Image-level objective

Given a pre-trained backbone \(f_{}\), we fine-tune its parameters \(\) on a dataset of triplets \(=\{(x,},}),y\}\), where \(x\) denotes a reference image, and \(}\) and \(}\) denote two variation images. The judgement \(y\{0,1\}\) indicates which of \(}\) and \(}\) is more similar to \(x\). We measure distance (dissimilarity) between two images \((x,})\) using the cosine distance between their respective image features \((f_{}(x),f_{}(}))\), which is defined as:

\[d(x,})=1-(x) f_{}(})}{|f_{ }(x)||f_{}(})|}.\] (1)

We use an alignment loss to encourage the model to match human preferences:

\[_{}()=(0,m- d),\] (2)

where \( d=d(x,})-d(x,})\), \(\) maps \(y\{0,1\}\{-1,1\}\), and \(m\) is the margin, which we set to 0.05 following . Note that this loss is equivalent to the triplet loss , and thus minimizes the cosine distance between the representations of the more similar pair and maximizes the distance between the representations of the other pair.

### Patch-level objective

The NIGHTS dataset contains global annotations of similarity at the image level, but the holistic label is the result of several local attributes, such as perspective, layout, foreground appearance etc. In addition to the global \(\) token from the Vision Transformer model backbones, each model also contains a set of spatial patch embeddings. Propagating this global human annotation to individual patch tokens allows for spatial representations that are aligned with human similarity preferences. Thus, we formulate a patch alignment objective to optimize these local patch features jointly with the global image label.

The local objective only differs from the global objective in how the features are extracted. Instead of computing \((_{},_{})\), we compute

\[([_{},( _{})],[_{},(_{})]).\]

\(\) is of dimension \((1,d)\) and \(\) is \((s,s,d)\) where \(s\) is the number of patches along each spatial dimension. We spatially average the patch tokens to get dimension \((1,d)\). We then concatenate the \(\) and pooled patch tokens to get dimension \((1,2d)\). We fine-tune the same alignment loss (see Eq. 2) on concatenated \(\) and averaged-pooled patch tokens, and train heads for semantic segmentation and depth estimation on the resulting patch embeddings. Note that only experiments reported in Sec. 4.1 use this objective, as they require local features. For all other evaluations, we exclusively use \(\) tokens (see Eq. 2) if not mentioned otherwise.

### Implementation details

**Vision Model Backbones.** We fine-tune several state-of-the-art Vision Transformer [VIT; 13] backbones including DINO , DINOv2 , CLIP , OpenCLIP , and SynCLR . For DINO, DINOv2, MAE, and SynCLR, we use the \(\) token of the final layer. We extract the \(\) token before layer norm for MAE and after it for the other backbones. For CLIP and OpenCLIP, we use the representations of the image encoder. We also experiment with concatenated features from DINO, CLIP, and OpenCLIP, used to train the DreamSim Ensemble in  (referred to as "ensemble" in our results). For each model, we use its base size, ViT-B.

We implicitly ablate the usage of synthetic triplets in NIGHTS by including SynCLR in our experiments; as that backbone was contrastively trained on generated images, additional performance changes can be attributed to human perceptual alignment.

Figure 2: **Diagram of our feature extraction method when training with a patch-level objective. Left: We extract the \(\) and patch embeddings from DINO and DINOv2, perform a spatial average-pool on the patch embeddings, and concatenate [\(\), patch] vectors. Right: We train these concatenated features with a hinge loss, identical to the image-level objective.**

**Finetuning with human preference labels.** We finetune each backbone using Low-Rank Adaptation (LoRA), which was found to achieve better alignment performance and efficiency than full fine-tuning in . For more training and technical details see Sections C.1 and C.6 in the Appendix.

## 4 Experiments

In this section, we evaluate perceptually-aligned backbones against base models on common vision tasks. We study global representations through instance retrieval, object-counting, and retrieval-augmented generation experiments. Additionally, we find that local patch-level representations can be improved by tuning on image-level perceptual judgments, and show performance increases on semantic segmentation and depth estimation.

### Dense Prediction

**Semantic segmentation.** Following the procedure detailed in Section 3.3 and Fig. 2, we LoRA-tune new backbones with perceptually-aligned CLS and patch tokens. To evaluate segmentation performance, we freeze these backbones and train a single linear layer transforming patch tokens to a segmentation map. We evaluate DINO and DINov2 on standard segmentation benchmarks in Table 1 and show that human-aligned models boost performance in 16 out of 20 cases. Across all datasets and metrics, human-aligned DINO (denoted as DINO-HA) outperforms the base model and often achieves the highest mIoU and Pixel Accuracy (P.A.) overall. DINov2-HA also outperforms its nonaligned counterpart on COCO and DAVIS2017.

We flag datasets already seen in the DINov2 retrieval pretraining , such as Pascal VOC, ADE20k, and Cityscapes. If a dataset is already in-distribution for a backbone, fine-tuning on different data may be more likely to change the feature space such that that dataset is more out-of-distribution. Thus this is a potential confounding factor.

**Depth estimation.** We follow the evaluation protocol of [44; 37] and train a single linear layer on frozen patch tokens to output a depth map with values mapped into 256 uniformly-distributed bins. This head is trained with a scale-invariant log loss introduced in  and a scale-invariant gradient-matching term as described in . In Table 2, we report performance on monocular depth estimation and show that human-aligned models outperform base models in 27 out of 36 cases. Consistent with segmentation performance, human-aligned DINO outperforms the base model on all metrics across all datasets, and is often the highest-performing model overall (denoted by \(\)). We also evaluate out-of-distribution generalization by training a depth head on NYUv2 and evaluating on the 4D Light Field dataset; combined with the performance boost even on datasets that DINov2 was trained on (NYUv2 and SUN-RGBD), these results demonstrate that human-aligned models have strong generalization capabilities prior to any training for downstream tasks.

### Retrieval-augmented generation

First introduced for text generation with non-parametric memory , retrieval-augmented generation (RAG) has become a popular method for selecting relevant few-shot examples when prompting large vision-language models (VLMs) [1; 2; 32; 33]. RAG evaluations go beyond conventional retrieval benchmarks on top-k recall, offering a more informative indicator of downstream large-model performance and utility in the multimodal domain. We evaluate OpenFlamingo 's few-shot classification accuracy by using a vision backbone to retrieve a query image's 3 nearest neighbors and prepending the query image with those examples along with their class labels. Following OpenFlamingo's image classification evaluation framework , we extract the model's logits per

    & ^{*}\)**} & ^{*}\)**} & ^{*}\)**} &  &  \\  & mIoU & P.A. & mIoU & P.A. & mIoU & P.A. & mIoU & P.A. \\  DINO & 0.729 & 0.800 & 0.342 & 0.548 & 0.562 & 0.749 & 0.810 & 0.901 & 0.809 & 0.886 \\ DINO-HA & **0.745\({}^{}\)** & **0.840\({}^{}\)** & **0.391** & **0.602** & **0.573\({}^{}\)** & **0.769\({}^{}\)** & **0.816\({}^{}\)** & **0.914\({}^{}\)** & **0.818\({}^{}\)** & **0.913\({}^{}\)** \\  DINOv2 & **0.686** & **0.803** & **0.441\({}^{}\)** & 0.645 & **0.535** & 0.737 & 0.759 & 0.877 & 0.794 & 0.906 \\ DINOv2-HA & 0.635 & 0.771 & 0.418 & **0.700\({}^{}\)** & 0.528 & **0.739** & **0.761** & **0.891** & **0.810** & **0.908** \\   

Table 1: **Base and human-aligned model performance on semantic segmentation.** Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINov2’s retrieval pretraining. \(\) indicates best score in the column.

object class and determine the model's decision by selecting the class with the largest log probability. See the Appendix for full details on the RAG experimental setup.

As illustrated in Fig. 3, classification accuracy on a wide variety of data domains improves with prompts retrieved by human-aligned models, compared to the original model backbones. Even in out-of-distribution domains such as medical imagery and 2D renders of game scenes, human-aligned models retrieve in-context examples that boost OpenFlamingo classification accuracy. These results suggest that human-aligned models can select more informative examples for in-context learning, thereby boosting the few-shot generalization abilities of a downstream multimodal VLM.

    & ^{*}\)**} \\   & RMSE (\(\)) & AbsRel (\(\)) & log10 (\(\)) & \(>1.25\) (\(\)) & \(>1.25^{2}\) (\(\)) & \(>1.25^{3}\) (\(\)) \\  DINO & 1.034 & **3.517** & 0.173 & 0.415 & 0.746 & 0.895 \\ DINO-HA & **1.032** & 3.759 & **0.169\({}^{}\)** & **0.445\({}^{}\)** & **0.761** & **0.900** \\  DINOv2 & **1.003\({}^{}\)** & 3.188 & **0.178** & **0.445\({}^{}\)** & **0.785\({}^{}\)** & **0.907\({}^{}\)** \\ DINOv2-HA & 1.062 & **3.167\({}^{}\)** & 0.185 & 0.419 & 0.749 & 0.891 \\   \\   & RMSE (\(\)) & AbsRel (\(\)) & log10 (\(\)) & \(>1.25\) (\(\)) & \(>1.25^{2}\) (\(\)) & \(>1.25^{3}\) (\(\)) \\  DINO & 3.817 & 0.543 & 0.380 & 0.160 & 0.335 & 0.454 \\ DINO-HA & **3.757** & **0.538** & **0.362** & **0.187** & **0.346** & **0.463** \\  DINOv2 & **3.460\({}^{}\)** & 0.497 & **0.337\({}^{}\)** & 0.194 & 0.337 & **0.464\({}^{}\)** \\ DINOv2-HA & 3.720 & **0.487\({}^{}\)** & 0.356 & **0.203\({}^{}\)** & **0.356\({}^{}\)** & 0.455 \\  ^{*}\)**} \\   & RMSE (\(\)) & AbsRel (\(\)) & log10 (\(\)) & \(>1.25\) (\(\)) & \(>1.25^{2}\) (\(\)) & \(>1.25^{3}\) (\(\)) \\  DINO & 4.900 & 2.350 & 0.533 & 0.205 & 0.385 & 0.524 \\ DINO-HA & **4.788\({}^{}\)** & **2.300\({}^{}\)** & **0.526\({}^{}\)** & **0.230** & **0.418\({}^{}\)** & **0.531\({}^{}\)** \\  DINOv2 & 5.200 & 3.023 & 0.615 & 0.173 & 0.309 & 0.444 \\ DINOv2-HA & **5.082** & **2.904** & **0.599** & **0.237\({}^{}\)** & **0.409** & **0.487** \\   

Table 2: **Human-aligned DINO and DINOv2 performance on monocular depth estimation benchmarks.** Note that NYUv2 and SUN-RGBD were included in DINOv2’s retrieval pretraining set, yet human-aligned DINOv2 still outperforms the base model on SUN-RGBD. Along with the results on an unseen test data domain (train on NYUv2 \(\) test on 4D Light Field), these results demonstrate strong generalization performance of models aligned to human perceptual judgments. \(\) indicates best score in the column.

Figure 3: **Left: Diagram of evaluation setup for retrieval-augmented generation.** We retrieve the top-3 nearest image-prompt examples for each datasets and prompt OpenFlamingo with them before inputting the query image. **Right: Classification accuracy on VTAB  from wide-varying domains.** Error bars indicate 95% confidence interval over 5 random seeds.

### Counting

A well-documented limitation of large vision backbones is their performance on compositional tasks: in particular, on object counting . We investigate how aligning to perceptual judgments affects performance on counting tasks via the FSC147, CARPK, and Clevr-Count (adapted from the original Clevr dataset by ) benchmarks by computer k-Nearest Neighbors accuracy on frozen vision representations. We report results in Table 3 and retrieval visualizations for few (\(n=3\)) and many (\(n=8,10\)) objects in Fig. 4 and find that, across 6 different models, the human-aligned versions outperform their counterparts in 35 out of 36 cases. See the Appendix for full details on the counting experimental setup.

Given that the perceptual similarity dataset we use for finetuning contains image-level similarity judgments, the consistent improvements that we observe on counting tasks, which requires local object awareness, is somewhat surprising. We hypothesize that the sensitivity of our human-aligned models to object counts may be a byproduct of NIGHTS examples themselves, many of which include image triplets with varying numbers of objects (see Fig. 15 in the Appendix). In terms of human perception, we note that humans consider object count when evaluating image similarity as soon as they develop counting proficiency . Thus, it is possible that given the prevalence of triplets with object-count variations in NIGHTS, the human annotations naturally capture this counting aware effect in the global image-level labels and propagate this information to the human-aligned models.

### Instance retrieval

Figure 4: **Visualizations of nearest-neighbor examples retrieved by CLIP, DINO, and Ensemble models as well as their human-aligned versions. Overall, we see retrieved images with more accurate object counts in CLIP-HA, DINO-HA, and Ensemble-HA across multiple nearest neighbors.**

    &  &  &  \\  & MAE & RMSE & MAE & RMSE & MAE & RMSE \\  DINO & 44.1 & **118.7** & 51.4 & 56.8 & 1.25 & 1.70 \\ DINO-HA & **41.3\({}^{}\)** & 119.3 & **48.7** & **54.5\({}^{}\)** & **1.08** & **1.50** \\  DINOv2 & 57.5 & 128.3 & 52.4 & 59.5 & 1.18 & 1.60 \\ DINOv2-HA & **44.9** & **113.6\({}^{}\)** & **52.1** & **58.2** & **0.84** & **1.20** \\  CLIP & 59.1 & 158.4 & 52.5 & 60.3 & 1.09 & 1.52 \\ CLIP-HA & **53.2** & **156.0** & **52.3** & **58.8** & **0.81** & **1.15\({}^{}\)** \\  OpenCLIP & 54.4 & 153.5 & 54.1 & 60.3 & 0.97 & 1.36 \\ OpenCLIP-HA & **50.2** & **139.1** & **49.9** & **55.8** & **0.80\({}^{}\)** & **1.15\({}^{}\)** \\  SynCLR & 50.6 & 139.6 & 54.3 & 60.3 & 1.05 & 1.45 \\ SynCLR-HA & **46.4** & **128.1** & **51.3** & **58.2** & **0.98** & **1.37** \\  Ensemble & 48.4 & 132.4 & 49.6 & 60.3 & 1.10 & 1.51 \\ Ensemble-HA & **45.4** & **130.3** & **48.4\({}^{}\)** & **55.2** & **0.83** & **1.19** \\   

Table 3: Error comparisons for base and human-aligned models on standard counting benchmarks. Though FSC147 and CARPK have examples with extreme object counts (tens and hundreds) unseen in the NIGHTS data, human-aligned models still achieve higher performance in each pair. \(\) indicates best score in the column, lower is better.

Figure 5: Performance improvements on Clevr-Count visualized by backbone for RMSE (top) and MAE (bottom), averaged across all datasets. Lower is better.

In this section, we evaluate how aligning models to perceptual judgments affects their performance on the instance retrieval task. This task aims to retrieve images from a gallery containing a shared subject or object with the query image. At test time, retrieval is performed by computing the cosine similarity between the extracted features of the query and gallery images. Succeeding at this task requires that a representation be robust to recognizing instance identities under different lighting, backgrounds, poses, and other such shifts.

We evaluate all base models and their human-aligned counterparts on the Consumer-to-Shop benchmark of the DeepFashion2 dataset . The benchmark consists of 10990 consumer "in the wild images" as the query set, and 21438 gallery images with matching clothing items to consumer images. Following the evaluation protocol from , we report Top-1, 3, and 5 accuracy in Table 4. Human aligned models outperform base models by a significant margin across all metrics and backbones (visualized in Fig. 6. In Fig. 7 we provide qualitative retrieval results. These results agree with prior work showing that training on NIGHTS improves performance in retrieving similar images to queries .

### What type of human similarity annotation is most beneficial?

As evidenced by the current widespread use of large vision models [4; 44; 46], trained on massive datasets, model performance largely correlates with data scale. This raises the question: are our performance gains purely due to training the base models on additional data, or as a result of the perceptual qualities embedded in NIGHTS? To investigate this, we ablate the training dataset - rather than tune on 13,900 NIGHTS triplets, we train on three other image triplet datasets of the same size:

1. BAPPS : Originally the training set for the LPIPS perceptual similarity metric , BAPPS consists of image patch triplets with various low-level distortions applied (e.g. color jitter, gaussian blur, JPEG compression artifacts).
2. THINGS : This dataset contains image triplets with each image encoding a different concept (e.g. a triplet of images categorized as {airplane, elephant, football})), labeled by humans tasked to determine which concept is the odd-one-out.
3. ImageNet : To ablate whether perceptual judgments at any level are needed, we construct an image triplet dataset by randomly selecting two images from one category and one image from another, labeling the first image pair as more similar to each other.

    &  \\  & Top-1 & Top-3 & Top-5 \\  DINO & 8.02 & 12.15 & 14.44 \\ DINO-HA & **11.69** & **17.55** & **20.84** \\  DINO/2 & 5.95 & 8.47 & 9.98 \\ DINO-HA & **9.03** & **13.57** & **16.30** \\  CLIP & 4.59 & 6.89 & 8.23 \\ CLIP-HA & **8.62** & **13.02** & **15.60** \\  OpenCLIP & 14.88 & 22.74 & 27.36 \\ OpenCLIP-HA & **16.85** & **24.58** & **28.64** \\  SynCLR & 4.88 & 7.34 & 9.02 \\ SynCLR-HA & **8.35** & **12.31** & **14.86** \\  Ensemble & 13.54 & 20.01 & 23.54 \\ Ensemble-HA & **23.39** & **32.47\({}^{}\)** & **37.20\({}^{}\)** \\   

Table 4: Top-1, -3, and -5 recall scores for instance retrieval on DeepFashion 2. \(\) indicates best score in the column, higher is better.

Figure 6: Performance improvements on the DeepFashion2 instance retrieval, task visualized by backbone and averaged across all \(k\) for top-\(k\) recall. Higher is better.

Figure 7: Examples of top-3 retrievals for a given query image on DeepFashion2. Overall, the human-aligned models return matching clothing items more frequently.

For all three datasets, we apply the same training settings as with the original LoRA-tuning on NIGHTS. See Fig. 8 for dataset ablations on object counting and instance retrieval.

Tuning on NIGHTS indeed provides the largest improvements across these tasks, with THINGS worsening performance overall and BAPPS/ImageNet having minimal effect. These trends may appear because BAPPS' photometric distortions are too low-level to impart any perceptual signal onto the backbones, THINGS encodes a higher-level conceptual similarity irrelevant to these mid-level vision tasks, and pre-trained vision models already perform quite well at discriminating ImageNet categories. Indeed, previous work  has found that similarity judgments by perceptual metrics trained on BAPPS correlate better to low-level metrics such as color than to semantic attributes.

In Section A.2 of the Appendix we find that the performance boost from NIGHTS over other datasets is also consistent across semantic segmentation and depth estimation. Conversely, tuning on NIGHTS fails to improve over other datasets on classification datasets; we further discuss this finding in Sections 5 and A.1.

## 5 Discussion

Recently, the vision research community has converged on the idea that using human perception to improve machine perception can bolster the transfer of vision representations to downstream tasks [61; 17; 41; 18; 42; 43]. However, it remained unclear which tasks benefit most from alignment with human perceptual judgments; different tasks may demand representations that encode different levels of granularity and semantics. Here, we fine-tune modern backbones, pretrained on different tasks, on human perceptual judgments  and subsequently investigate which downstream tasks benefit. We develop a better understanding of how perceptual alignment affects performance on important tasks - e.g., segmentation, depth estimation, RAG - which may in turn inform future model training and data curation decisions across different applications. By evaluating competency at downstream tasks, we quantify what these representations capture and make decodable, enabling a better understanding of the human-aligned feature space. While we probe representations in terms of competency, understanding them in terms of their mechanism is a rich direction for future work.

We find widespread benefits from perceptual alignment across both image- and patch-level tasks. At the global level, performance consistently improves for retrieval-augmented generation, counting-based and instance-based retrieval. Moreover, propagating the supervision from image-level similarity judgments to ViT patch tokens improves performance on dense prediction tasks (semantic segmentation and depth prediction). Since model performance is closely linked to data scale, we also ablate the choice of perceptual dataset. While fine-tuning on NIGHTS -- a dataset of mid-level perceptual judgments -- leads to improvements across various tasks, fine-tuning on triplets from ImageNet , BAPPS , and THINGS  preserves or deteriorates transfer performance.

Why does fine-tuning on NIGHTS in particular lead to improvements? We hypothesize that the variations found in BAPPS and THINGS are solely high- or low-level, whereas the mid-level distortions in NIGHTS cover salient features that humans use when making inferences about what they see; these characteristics include style, pose, color, and count (see Fig.15), and largely correlate with the characteristics a model must successfully extract for many computer vision tasks. Previous work  found that models fine-tuned on NIGHTS seem to attend to both low-level and semantic attributes. Aligning a feature space to these concepts may be useful for visual tasks requiring both visual and semantic knowledge, such as retrieval, counting, segmentation, etc. This hypothesis may

Figure 8: Evaluations comparing dataset utility on counting tasks (lower RMSE is better) and DeepFashion2 instance retrieval (higher recall is better). Across each task, tuning on NIGHTS yields the largest improvements while THINGS worsens performance and BAPPS/ImageNet makes minimal changes.

also explain why tuning on NIGHTS hurts performance on fine-grained tasks, in which perceptually similar images may belong to different categories.

**Limitations.** Perceptual alignment does not appear to improve performance for standard image classification tasks such as natural image datasets in the VTAB benchmark  (see Tables 5a-b in the Appendix). This is surprising in light of recent findings that demonstrate downstream task improvements in image classification tasks for human-aligned representations . Although it is hard to pinpoint the exact cause, we hypothesize two reasons: First, perceptual judgments at different levels of abstraction may be helpful for different downstream tasks. While the mid-level perceptual judgments in NIGHTS boost performance for retrieval-based and dense prediction tasks, they may not impart a useful inductive bias for standard image classification tasks; high-level semantic associations could simply be better suited for these kinds of tasks. Alternatively, the visual features that humans use to judge similarity may not be appropriately captured in classification accuracy metrics. Some VTAB datasets have numerical ground truth in which the distance from the correct answer is meaningful; thus, it may be ill-suited for classification, and better suited to continuous evaluations, which we report in sections of the paper.

Additionally, a key insight from our dataset ablations is that not all human preferences improve performance. Finetuning on perceptual datasets with solely high-level (THINGS) or low-level (BAPPS) variations hurts performance for many downstream tasks (Section 4.5 and A.2). Similarly, Mutenthaler et al.  recently discovered that finetuning vision models on THINGS can hurt downstream task transfer. Due to our evaluation of the synthetically-pretrained SynCLR , we attribute the drop in performance to perceptual alignment. Furthermore, by ablating the number of fine-tuning steps in Section A.2, we show that overfitting to perceptual judgments may also harm downstream performance.

**Societal impacts.** Beyond our findings, there are other possibilities for harm outside the scope of the type of perceptual annotations we have studied: Human preferences may reflect unwanted biases. A long-standing problem in both language and vision is that biases (e.g. gender, racial) reflected in Internet language/images are inherited by large models, and reflected in their embeddings. One can imagine similar phenomena to happen in visual preferences . Humans may disagree on a preference label, or even disagree with themselves if asked at different points in time. This may lead to noisy data if not filtered carefully, thus harming representations . Without sufficient demographic diversity in the annotator group, emerging biases may be reflected in the model. For example, some RLHF-trained language models have been shown to develop a bias towards the opinions of high-income, liberal individuals over their non-RLHF counterparts .