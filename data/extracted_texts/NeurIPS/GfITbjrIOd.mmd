# Human-Aligned Calibration for AI-Assisted Decision Making

Nina L. Corvelo Benz

Max Planck Institute for Software Systems

ETH Zurich

ninacobe@mpi-sws.org &Manuel Gomez Rodriguez

Max Planck Institute for Software Systems

manuel@mpi-sws.org

###### Abstract

Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values--an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. However, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. Further, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for alignment. Experiments on four different AI-assisted decision making tasks where a classifier provides decision support to real human experts validate our theoretical results and suggest that alignment may lead to better decisions.

## 1 Introduction

In recent years, there has been an increasing excitement on the potential of machine learning models to improve decision making in a variety of high-stakes domains such as medicine, education or criminal justice . One of the main focus has been binary classification tasks, where a classifier helps a decision maker by predicting a binary label of interest using a set of observable features . For example, in medical treatment, the classifier may help a doctor by predicting whether a patient may benefit from a treatment. In college admissions, it may help an admissions committee by predicting whether a candidate may successfully complete an undergraduate program. In loan decisions, it may help a bank by predicting whether a prospective customer may default on a loan. In all these scenarios, the decision maker--the doctor, the committee or the bank--aim to use these predictions, together with their own predictions, to take good decisions that maximize a given utility function. In this context, since the predictions are unlikely to always match the truth, it has been widely agreed that the classifier should also provide a confidence value together with each prediction .

While the conventional wisdom is that the confidence value should be a well calibrated estimate of the probability that the predicted label matches the true label , multiple lines of empirical evidence have recently shown that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values . Therein, Vodrahali et al.  have shown that, in certain scenarios, decision makers take better decisions using uncalibrated probability estimates rather than calibrated ones. However, a theoretical framework explaining this puzzling observation has been missing and it is yet unclear what properties we should be looking for to guarantee that confidence values are useful for AI-assisted decision making. In our work, we aim to bridge this gap.

**Our contributions.** We start by formally characterizing AI-assisted decision making using a structural causal model (SCM) , as seen in Figure 1. Building upon this characterization, we first argue that, if a decision maker is rational, the level of trust she places on predictions will be monotone on the confidence values--she will place more (less) trust on predictions with higher (lower) confidence values. Then, we show that, for a broad class of utility functions, there are data distributions for which a rational decision maker can never take optimal decisions using calibrated estimates of the probability that the predicted label matches the true label as confidence values. However, we further show that, if the confidence values a decision maker uses satisfy a natural alignment property with respect to the confidence she has on her own predictions, which we refer to as human-alignment, then the decision maker can both be rational and take optimal decisions. In addition, we demonstrate that human-alignment can be achieved via multicalibration , a statistical notion introduced in the context of algorithmic fairness. In particular, we show that multicalibration with respect to the decision maker's confidence on her own predictions is a sufficient condition for human-alignment. Finally, we validate our theoretical framework using real data from four different AI-assisted decision making tasks where a classifier provides decision support to human decision makers in four different binary classification tasks. Our results suggest that, comparing across tasks, classifiers providing human-aligned confidence values facilitate better decisions than classifiers providing confidence values that are not human-aligned. Moreover, our results also suggest that rational decision makers' trust level increases monotonically with the classifier's provided confidence.

**Further related work.** Our work builds upon a rapidly increasing literature on AI-assisted decision making (refer to Lai et al.  for a recent review). More specifically, it is motivated by several empirical studies showing that decision makers have difficulties at modulating trust using confidence values , as discussed previously. In this context, it is also worth noting that other empirical studies have analyzed how other factors such as model explanations and accuracy modulate trust . However, except for a very recent notable exception , theoretical frameworks, which could be used to better understand the mixed findings found by these empirical studies, have been missing. More broadly, our work also relates to a flurry of recent work on reinforcement learning with human feedback , which aims to better align the outputs of large language models (LLMs) with human preferences. However, our formulation is fundamentally different and our technical contributions are orthogonal to theirs.

## 2 A Causal Model of AI-Assisted Decision Making

We consider an AI-assisted decision making process where, for each realization of the process, a decision maker first observes a set of features \((x,v)\), then takes a binary decision \(t\{0,1\}\) informed by a classifier's prediction \(=*{argmax}_{y}f_{y}(x)\), as well as confidence \(f_{}(x)\), of a binary label of interest \(y\{0,1\}\), and finally receives a utility \(u(t,y)\). Such an AI-assisted decision making process fits a variety of real-world applications. For example, in medical treatment, the features \((x,v)\) may comprise multiple sources of information regarding a patient's health1, the label \(y\) may indicate whether a patient would benefit from a specific treatment, the decision \(t\) may indicate whether the doctor applies the specific treatment to the patient, and the utility \(u(t,y)\) may quantify the trade-off between health benefit to the patient and economic cost to the decision maker.

In what follows, rather than working with both \(\) and \(f_{}(x)\), we will work with just \(b=f_{1}(x)\), which we will refer to as classifier's confidence, without loss of generality2. Moreover, we will assume that the utility \(u(t,y)\) is greater if the value of \(t\) and \(y\) coincide, _i.e._,

\[u(1,1)>u(1,0), u(1,1)>u(0,1), u(0,0)>u(1,0), u(0,0 ) u(0,1), \]

a condition that we think it is natural under an appropriate choice of label and decision values. For example, in medical diagnosis, if \(t=1\) means the patient is tested early for a disease and \(y=1\) means the patient suffers the disease, the above condition implies that the utility of either testing a patient who suffers the disease or not testing a patient who does not suffer the disease are greater than the utility of either not testing a patient who suffers the disease or testing a patient who does not suffer the disease. In condition 1, we allow for a non-strict inequality \(u(0,0) u(0,1)\) because, in settings in which the label \(Y\) is only realized whenever the decision \(t=1\) (_e.g._, in our previous example on medical treatment, we can only observe if a treatment is eventually beneficial or not if the patient is treated), it has been argued that, whenever \(t=0\), any choice of utility must be independent of the label value , _i.e._, \(u(0,0)=u(0,1)=u(0)\).

Next, we characterize the above AI-assisted decision making process using a structural causal model (SCM) , which we denote as \(\). The SCM \(\) is defined by a set of assignments, which entail a distribution \(P^{}\) and divide naturally into two subsets. One subset comprises the features and the label3, _i.e._,

\[X=f_{X}(D) V=f_{V}(D) Y=f_{Y}(D), \]

where \(D\) is an independent exogenous random variable, often called exogenous noise, characterizing the data generating process and \(f_{X}\), \(f_{V}\) and \(f_{Y}\) are given functions4. The second subset comprises the decision maker and the classifier, _i.e._,

\[H=f_{H}(X,V,Q), B=f_{B}(X,H), T=(H,B,W) U=u(T,Y), \]

where \(f_{H}\) and \(f_{B}\) are given functions, which determine the decision maker's confidence \(H\) and classifier's confidence \(B\) that the value of the label of interest is \(Y=1\), \(\) is a given AI-assisted decision policy, which determines the decision maker's decision \(T\), \(u\) is a given utility function, which determines the utility \(U\), and \(Q\) and \(W\) are independent exogenous variables modeling the decision maker's individual characteristics influencing her own confidence \(H\) and her decision \(T\), respectively. By distinguishing both sources of noise, we allow for the presence of uncertainty on the decision \(T\) even after conditioning on fixed confidence values \(h\) and \(b\). This accounts for the fact that, in reality, a decision maker may take different decisions \(T\) for instances with the same confidence values \(h\) and \(b\). For example, in medical treatment, for two different patients with the same confidence \(h\) and \(b\), a doctor's decision may differ due to limited resources.

In our SCM \(\), the decision maker's confidence \(H\) refers to the confidence the decision maker has that the label \(Y=1\)_before_ observing the classifier's confidence \(B\). Moreover, following previous behavioral studies showing that human's confidence \(H\) is discretized in a few distinct levels , we assume \(H\) takes values \(h\) from a totally ordered discrete set \(\). We say that the decision maker's

Figure 1: Our structural causal model \(\). Orange circles represent endogenous random variables and blue boxes represent exogenous random variables. The value of each endogenous variable is given by a function of the values of its ancestors in the structural causal model, as defined by Eqs. 2 and 3. The value of each exogenous variable is sampled independently from a given distribution.

confidence \(f_{H}\) is monotone (with respect to the probability distribution \(P(Y=1)\)) if, for all \(h,h^{}\) such that \(h h^{}\), it holds that \(P(Y=1\;\;H=h) P(Y=1\;\;H=h^{})\). Further, we allow the classifier's confidence \(B\) to depend on the decision maker's confidence \(H\) because this will be necessary to achieve human-alignment via multicalibration in Section 5. However, our negative result in Section 3 also holds if the classifier's confidence \(f_{B}(X,H)=f_{B}(X)\) only depends on the features, as usual in most classifiers designed for AI-assisted decision making. In the remainder, we will use \(Z=(X,H)\) and denote the space of features and human confidence values as \(=\). Figure 1 shows a visual representation of our SCM \(\).

Under this characterization, we argue that, if a rational decision maker has decided \(t\) under confidence values \(b\) and \(h\), then she would have decided \(t^{} t\) had the confidence values been \(b^{} b\) and \(h^{} h\) while holding "everything else fixed" . For example, in medical treatment, assume a doctor's and a classifier's confidence that a patient would benefit from treatment is \(b=h=0.7\) and the doctor decides to treat the patient, then we argue that, if the doctor is rational, she would have treated the patient had the doctor's and the classifier's confidence been \(b^{}=h^{}=0.8>0.7\). Further, we say that any AI-assisted decision policy \(\) that satisfies this property is monotone, _i.e._,

**Definition 1** (Monotone AI-assisted decision policy).: _An AI-assisted decision policy \(\) is monotone if and only if, for any \(b,b^{}\) and \(h,h^{}\) such that \(b b^{}\) and \(h h^{}\), it holds that \((h,b,w)(h^{},b^{},w)\) for any \(w P^{}(w)\)._

Finally, note that, under any monotone AI-assisted decision policy, it trivially follows that

\[[T\;\;H=h,B=b][T\;\;H=h^{},B=b^{}], \]

where the expectation is over the uncertainty on the decision maker's individual characteristics and the data generating process.

## 3 Impossibility of AI-Assisted Decision Making Under Calibration

In AI-assisted decision making, classifiers are usually demanded to provide calibrated confidence values . A confidence function \(f_{B}:\) is said to be perfectly calibrated if, for any \(b\), it holds that \(P(Y=1\;\;f_{B}(Z)=b)=b\). Unfortunately, using finite amounts of (calibration) data, one can only hope to construct approximately calibrated confidence functions. There exist many different notions of approximate calibration, which have been proposed over the years. Here, for concreteness, we adopt the notion of \(\)-calibration5 introduced by Hebert-Johnson et al. , however, our theoretical results can be easily adapted to other notions of approximate calibration6.

**Definition 2** (Calibration).: _A confidence function \(f_{B}:\) satisfies \(\)-calibration with respect to \(\) if there exists some \(^{}\), with \(|^{}|(1-)||\), such that, for any \(b\), it holds that_

\[|P(Y=1\;\;f_{B}(Z)=b,Z^{})-b|, \]

If the decision maker's decision \(T\) only depends on the classifier's confidence \(B\), _i.e._, \((H,B,W)=(B)\) and \(f_{B}\) satisfies \(\)-calibration with respect to \(\), then, it readily follows from previous work that, for any utility function that satisfies Eq. 1, a simple monotone AI-assisted decision policy \(_{B}^{*}\) that takes decisions by thresholding the confidence values is optimal , _i.e._, \(_{B}^{*}=*{argmax}_{(B)}_{}[u(T,Y)]\), where the expectation is with respect to the probability distribution \(P^{}\) and \((B)\) denotes the class of AI-assisted decision policies using \(B\). However, one of the main motivations to favor AI-assisted decision making over fully automated decision making is that the decision maker may have access to additional features \(V\) and may like to weigh the classifier's confidence \(B\) against her own confidence \(H\). Hence, the decision maker may seek for the optimal decision policy \(^{*}\) over the class \((H,B)\) of AI-assisted decision policies using \(H\) and \(B\), _i.e._, \(^{*}=*{argmax}_{(H,B)}_{}[u(T,Y)]\), since it may offer greater expected utility than \(_{B}^{*}\).

Unfortunately, the following negative result shows that, in general, a rational decision maker may be unable to discover such an optimal decision policy \(^{*}\) using (perfectly) calibrated confidence values and this is true even if \(f_{H}\) is monotone:

**Theorem 3**.: _There exist (infinitely many) AI-assisted decision making processes \(\) satisfying Eqs. 2 and 3, with utility functions \(u(T,Y)\) satisfying Eq. 1, such that \(f_{B}\) is perfectly calibrated and \(f_{H}\) is monotone but any AI-assisted decision policy \((H,B)\) that satisfies monotonicity is suboptimal, i.e., \(_{}[u(T,Y)]<_{^{*}}[u(T,Y)]\)._

In the proof of the above result in Appendix A.2, we show that there always exist a perfectly calibrated \(f_{B}(Z)=f_{B}(X,H)\) that depends on both \(X\) and \(H\) for which any monotone AI-assisted decision policy is suboptimal. This is due to the fact that \(f_{B}(Z)\) is calibrated on average over \(H\), however, it may not be calibrated, nor even monotone, after conditioning on a specific value \(H=h\). Further, we also show that, even if \(f_{B}(Z)=P^{}(Y=1\;\;X)\) matches the true distribution of the label \(Y\) given the features \(X\), which has been typically the ultimate goal in the machine learning literature, there always exists a monotone \(f_{H}\) for which any monotone AI-assisted decision policy is suboptimal. This is due to the fact that the decision maker's confidence \(H\) can differ across instances with the same value for features \(X\) because it also depends on the features \(V\) and noise \(Q\). Hence, \(f_{H}\) may not be monotone after conditioning on a specific value \(X=x\). In both cases, when a rational decision maker compares pairs of confidence values \(h,b\) and \(h^{},b^{}\), the rate of positive outcomes \(Y=1\) for each pair may appear _contradictory_ with the magnitude of confidence. In what follows, we will show that, if \(f_{B}\) satisfies a natural alignment property with respect to \(f_{H}\), which we refer to as human-alignment, there always exists an optimal AI-assisted decision policy that is monotone.

## 4 AI-Assisted Decision Making Under Human-Aligned Calibration

Intuitively, to avoid that pairs of confidence values \(B\) and \(H\) appear as contradictory to a rational decision maker, we need to make sure that, with high probability, both \(f_{B}\) and \(f_{H}\) are monotone after conditioning on specific values of \(H\) and \(B\), respectively. Next, we formalize this intuition by means of the following property, which we refer to as \(\)-alignment:

**Definition 4** (Human-alignment).: _A confidence function \(f_{B}\) satisfies \(\)-alignment with respect to a confidence function \(f_{H}\) if, for any \(h\), there exists some \(}_{h}_{h}\), with \(_{h}=\{(x,H)\;\;H=h\}\) and \(|}_{h}|(1-/2)|_{h}|\), such that, for any \(b^{},b^{}\) and \(h^{},h^{}\) such that \(b^{} b^{}\) and \(h^{} h^{}\), it holds that_

\[P(Y=1\;\;f_{B}(X,H)=b^{},(X,H)}_{h^{}})- P(Y=1\;\;f_{B}(X,H)=b^{},(X,H)}_{h^{ }}) \]

The above definition just means that, if \(f_{B}\) is \(\)-aligned with respect to \(f_{H}\) then, for any \(h,h^{}\), we can bound any violation of monotonicity by \(f_{B}\) between at least a \((1-/2)\) fraction of the subspaces of features \(_{h}\) and \(_{h^{}}\). Moreover, note that, if \(f_{B}\) is \(0\)-aligned with respect to \(f_{H}\), then there are no violations of monotonicity, _i.e._, \(P(Y=1\;\;f_{B}(X,H)=b^{},(X,H)}_{h^{}})  P(Y=1\;\;f_{B}(X,H)=b^{},(X,H)}_{h^ {}})\), and we say that \(f_{B}\) is perfectly aligned with respect to \(f_{H}\).

Given the above definition, we are now ready to state our main result, which shows that human-alignment allows for AI-assisted decision policies that satisfy monotonicity and (near-)optimality:

**Theorem 5**.: _Let \(\) be any AI-assisted decision making process satisfying Eqs. 2 and 3, with an utility function \(u(T,Y)\) satisfying Eq. 1 If \(f_{B}\) satisfies \(\)-alignment w.r.t. \(f_{H}\), then there always exists an AI-assisted decision policy \((H,B)\) that satisfies monotonicity and is near-optimal, i.e.,_

\[_{^{*}}[u(T,Y)]_{}[u(T,Y)]+[u(1,1 )-u(0,1)+(u(0,0)-u(1,0))] \]

_where \(^{*}=*{argmax}_{(H,B)}_{}[u(T,Y)]\) is the optimal policy._

**Corollary 1**.: _If \(f_{B}\) is perfectly aligned with respect to \(f_{H}\), then there always exists an AI-assisted decision policy \((H,B)\) that satisfies monotonicity and is optimal._

Finally, in many high-stakes applications, we may like to make sure that the confidence values provided by \(f_{B}\) are both useful and interpretable . Hence, we may like to seek for confidence functions \(f_{B}\) that satisfy human-aligned calibration, which we define as follows:

**Definition 6** (Human-aligned calibration).: _A confidence function \(f_{B}\) satisfies \(\)-aligned calibration with respect to a confidence function \(f_{H}\) if and only if \(f_{B}\) satisfies \(\)-alignment with respect to \(f_{H}\) and it satisfies \(\)-calibration with respect to \(\)._

In the next section, we will show how to achieve human-alignment and human-aligned calibration via multicalibration, a statistical notion introduced in the context of algorithmic fairness .

Achieving Human-Aligned Calibration via Multicalibration

Multicalibration was introduced by Hebert-Johnson et al.  as a notion to achieve fairness in supervised learning. It strengthens the notion of calibration by requiring that the confidence function is calibrated simultaneously across a large collection of subspaces of features \( 2^{}\) which may or may not be disjoint. More formally, it is defined as follows:

**Definition 7** (Multicalibration).: _A confidence function \(f_{B}:\) satisfies \(\)-multicalibration with respect to \( 2^{}\) if \(f_{B}\) satisfies \(\)-calibration with respect to every \(\)._

Then, we can show that, for an appropriate choice of \(\), if \(f_{B}\) satisfies \(\)-multicalibration with respect to \(\), then it satisfies \(\)-aligned calibration with respect to \(f_{H}\). More specifically, we have the following result:

**Theorem 8**.: _If \(f_{B}\) satisfies \((/2)\)-multicalibration with respect to \(\{_{h}\}_{h}\), with \(_{h}=\{(x,H)\ \ H=h\}\), then \(f_{B}\) satisfies \(\)-aligned calibration with respect to \(f_{H}\)._

The above theorem suggests that, given a classifier's confidence function \(f_{B}\), we can multicalibrate \(f_{B}\) with respect to \(\{_{h}\}_{h}\) to achieve \(\)-aligned calibration with respect to \(f_{H}\). To achieve multicalibration guarantees using finite amounts of (calibration) data, multicalibration algorithms need to discretize the range of \(f_{B}\). In what follows, we briefly revisit two algorithms, which carry out this discretization differently, and discuss their complexity and data requirements with respect to achieving \(\)-aligned calibration.

**Multicalibration algorithm via \(\)-discretization.** This algorithm, which was introduced by Hebert-Johnson et al. , discretizes the range of \(f_{B}\), _i.e._, the interval \(\), into bins of fixed size \(>0\) with values \(=\{,,,1-\}\).

Let \((b)=[b-/2,b+/2)\). The algorithm partitions each subspace \(_{h}\) into \(1/\) groups \(_{h,(b)}=\{(x,h)_{h}\ |\ f_{B}(x,h)(b)\}\), with \(b\). It iteratively updates the confidence values of function \(f_{B}\) for these groups until \(f_{B}\) satisfies a discretized notion of \(^{}\)-multicalibration over these groups. The algorithm then returns a discretized confidence function \(f_{B,}(x,h)=[f_{B}(X,H)\ |\ f_{B}(X,H)(b)]\), with \(b\) such that \(f_{B}(x,h)(b)\), which is guaranteed to satisfy \((^{}+)\)-multicalibration. Refer to Algorithm 1 in Appendix B for a pseudocode of the algorithm.

Then, as a direct consequence of Theorem 8, we can obtain a (discretized) confidence function \(f_{B,}\) that satisfies \(\)-aligned calibration by setting \(^{}==/4\). However, the following proposition shows that, to satisfy just \(\)-alignment, it is enough to set \(^{}=>/4\) and \(=/4\):

**Proposition 1**.: _The discretized confidence function \(f_{B,}\) returned by Algorithm 1 satisfies \((2^{}+)\)-alignment with respect to \(f_{H}\)._

Finally, it is worth noting that, to implement Algorithm 1, we need to compute empirical estimates of the expectations and probabilities above using a calibration set \(\). In this context, Theorem 2 in Hebert-Johnson et al.  shows that, if we use a calibration set of size \(O((||/())/^{11/2}^{3/2})\), with \(P((X,H)_{h})>\) for all \(h\), then \(f_{B,}\) is guaranteed to satisfy \(\)-multicalibration with probability at least \(1-\) in time \(O(||(1/,1/))\).

**Multicalibration algorithm via uniform mass binning.** Uniform mass binning (UMD)  has been originally designed to calibrate \(f_{B}\) with respect to \(\) using a calibration set \(\). However, since the subspaces \(\{_{h}\}_{h}\) are disjoint, _i.e._, \(_{h}_{h^{}}=\) for every \(h h^{}\), we can multicalibrate \(f_{B}\) with respect to \(\{_{h}\}_{h}\) by just running \(||\) instances of UMD, each using the subset of samples \(_{h}\). Here, we would like to emphasize that we can use UMD to achieve multicalibration because, in our setting, the subspaces \(\{_{h}\}_{h}\) are disjoint.

Each instance of UMD discretizes the range of \(f_{B}\), _i.e._, the interval \(\), into \(N=1/\) bins with values \(_{h}=\{(Y=1\ |\ f_{B}(X,h)[0,_{1})],,(Y=1\ |\ f_{B}(X,h)[_{N-1},_{N}])\}\), where \(_{i}\) denotes the \((i/N)\)-th empirical quantile of the confidence values \(f_{B}(x,h)\) of the samples \((x,h)_{h}\) and \(\) denotes an empirical estimate of the probability using samples from \(_{h}\), aswell. Here, note that, by construction, the bins have similar probability mass. Then, for each \((x,h)\), the corresponding instance of UMD provides the value of the discretized confidence function \(f_{B,}(x,h)=b\), where \(b_{h}\) denotes the value of the bin whose corresponding defining interval includes \(f_{B}(x,h)\). Finally, we have the following theorem, which guarantees that, as long as the calibration set is large enough, the discretized confidence function \(f_{B,}\) satisfies \(\)-aligned calibration with respect to \(f_{H}\) with high probability:

**Theorem 9**.: _The discretized confidence function \(f_{B,}\) returned by \(||\) instances of UMD, one per \(_{h}\), satisfies \(\)-aligned calibration with respect to \(f_{H}\) with probability at least \(1-\) as long as the size of the calibration set \(||=O(|}{^{2}}( |}{}))\), with \(P((X,H)_{h})\)._

## 6 Experiments

In this section, we validate our theoretical results using a dataset with real expert predictions in an AI-assisted decision making scenario comprising four different binary classification tasks7.

**Data description.** We experiment with the publicly available Human-AI Interactions dataset . The dataset comprises \(34,\!783\) unique predictions from \(1,\!088\) different human participants on four different binary prediction tasks ("Art", "Sarcasm", "Cities" and "Census"). Overall, there are approximately \(32\) different instances per task. In the "Art" task, participants need to determine the art period of a painting given two choices and, overall, there are paintings from four art periods. In the "Sarcasm" task, participants need to detect if sarcasm is present in text snippets from the Reddit sarcasm dataset . In the "Cities" task, participants need to determine which large US city is depicted in an image given two choices and, overall, there are images of four different US cities. Finally, in the "Census" task, participants need to determine if an individual earns more than \(50\)k a year based on certain demographic information in tabular form. For "Sarcasm", \(x\) is a representation of the text snippets and we set \(y=1\) if sarcasm is present, for "Art" and "Cities", \(x\) is a representation of the images and we set \(y=1\) and \(y=0\) at random for each different instance and, for "Census", \(x\) summarizes demographic information and we set \(y=1\) if an individual earns more than \(50\)k a year. In each of the tasks, human participants provide confidence values about their predictions before (\(h\)) and after (\(h\)+AI) receiving AI advice from a classifier in form of the classifier's confidence values \(b\).8 The original dataset contains predictions by participants from different, but overlapping, sets

Figure 2: Empirical estimate of the probabilities \(P(Y=1(X,Y)_{h,(b)})\), where \(b\) and \(h\{,,\}\) are the discretized confidence values for the classifiers and human participants, respectively. Error bars represent \(90\)% confidence intervals and hatched bars mark alignment violations between confidence pairs \((h,b)\) with \(|_{h,(b)}| 30\).

of countries across tasks, who were told the AI advice had different values of accuracy.9 In our experiments, to control for these confounding factors, we focus on participants from the US who were told the AI advice was \(80\)% accurate, resulting in \(15{,}063\) unique predictions from \(471\) different human participants.

**Experimental setup and evaluation metrics.** For each of the tasks, we first measure (i) the degree of misalignment between the classifiers' confidence values \(b\) and the participants' confidence values \(h\) before receiving AI advice \(b\) and (ii) the difference \((h_{}-h)\) between the human participant's confidence values before and after receiving AI advice \(b\). Then, we compare the utility achieved by a AI-assisted decision policy \(_{H_{}}\) that predicts the value of \(y\) by thresholding the humans' confidence values \(h_{}\) after observing the classifier's confidence values against two baselines: (i) a decision policy \(_{B}\) that predicts the value of \(y\) by thresholding the classifier's confidence values \(b\) and (ii) a decision policy \(_{H}\) that predicts the value of \(y\) by thresholding the humans' confidence values \(h\) before observing the classifier's confidence values.

To measure the degree of misalignment, we discretize the confidence values \(b\) and \(h\) into bins. For the classifiers' confidence \(b\), we use \(8\) uniform sized bins per task with (centered) values \(\), where \(=1/8\). For the human participants' confidence \(h\) before receiving AI advice \(b\), we use three bins per task ('low','mid' and 'high'), where we set the bin boundaries so that each bin contains approximately the same probability mass and set the bin values to the average confidence value within each bin. In what follows, we refer to the pairs of discretized confidence values \((h,b)\) as cells, where samples \((x,y)\) whose confidence values lie in the cell \((h,b)\) define the group \(_{h,(b)}\), and note that we choose a rather low number of bins for both \(b\) and \(h\) so that most cells have sufficient data samples to reliable estimate several misalignment metrics, which we describe next.

We use three different misalignment metrics: (i) the number of alignment violations between cell pairs, (ii) the expected alignment error (EAE) and (iii) the maximum alignment error (MAE). There is an alignment violation between cells pairs \((h,b)\) and \((h^{},b^{})\), with \(h h^{}\) and \(b b^{}\), if

\[P(Y=1|(X,Y) S_{h,(b)})>P(Y=1|(X,Y) S_{h^{},(b^{ })}).\]

Moreover, we have that:

\[ =_{h h^{},b b^{}}[P( Y=1\,\,(X,Y)_{h,(b)})-P(Y=1\,\,(X,Y) _{h^{},(b^{})})]_{+},\] \[ =_{h h^{},b b^{}}P(Y=1\,\,(X,Y) _{h,(b)})-P(Y=1\,\,(X,Y)_{h^{}, (b^{})}),\]

where \(N=|\{h h^{},b b^{}\}|\). Here, note that the number of alignment violations tells us how frequently is the left hand side of Eq. 6 positive across cell pairs given \(_{h}=_{h}\) and the EAE and MAE quantify the average and maximum value of the left hand side of Eq. 6 across cells violating alignment. To obtain reliable estimates of the above metrics, we only consider cells \((h,b)\) with \(|_{h,(b)}| 30\) samples. Moreover, we also report the expected calibration error (ECE) and maximum calibration error (MCE) , which are natural counterparts to EAE and MAE, respectively.

As a measure of utility, we estimate the true positive rate (TPR) and false positive rate (FPR) of the decision policies \(_{B}\), \(_{H}\) and \(_{H_{}}\) for all possible choices of threshold values, which we summarize using the area under the ROC curve (AUC) and, in Appendix C, we also report ROC curves.

    &  &  &  \\  & **EAE** & **MAE** & **ECE** & **MCE** & \(_{B}\) & \(_{H}\) & \(_{H_{}}\) \\  Art & \(4.5 10^{-4}\) & \(0.058\) & \(0.084\) & \(0.186\) & \(86.7\)\% & \(72.7\)\% & \(82.0\)\% \\ Sarcasm & \(3.8 10^{-3}\) & \(0.224\) & \(0.085\) & \(0.310\) & \(89.9\)\% & \(82.5\)\% & \(86.5\)\% \\ Cities & \(6.2 10^{-5}\) & \(0.013\) & \(0.066\) & \(0.158\) & \(84.4\)\% & \(79.0\)\% & \(84.7\)\% \\ Census & \(9.0 10^{-3}\) & \(0.298\) & \(0.109\) & \(0.270\) & \(80.0\)\% & \(77.3\)\% & \(79.9\)\% \\   

Table 1: Misalignment, miscalibration and AUC.

**Results.** We start by looking at the empirical estimates of the probabilities \(P(Y=1(X,Y)_{h,(b)})\) and of our measures of misalignment (EAE, MAE) and miscalibration (ECE, MCE) in Figure 2 and Table 1 (left and middle columns). The results show that, for "Cities", the probabilities \(P(Y=1(X,Y)_{h,(b)})\) are (approximately) monotonically increasing with respect to the classifier's confidence values \(b\). More specifically, as shown in Figure 2, there is only one alignment violation between cell pairs and, hence, our metrics of misalignment acquire also very low values. In contrast, for "Art", "Sarcasm" and especially "Census", there is an increasing number of alignment violations and our misalignment metrics acquire higher values, up to several orders of magnitude higher for "Census". These results also show that misalignment and miscalibration go hand in hand, however, in terms of miscalibration, "Census" does not stand up so strongly.

Next, we look at the difference \(h_{}-h\) between the human participant's recorded confidence values before and after receiving AI advice \(b\) across samples in each of the subsets \(_{h,(b)}\) induced by the discretized confidence values used above. Figure 3 summarizes the results, which reveal that the difference \(h_{}-h\) increases monotonically with respect to the classifier's confidence \(b\). This suggests that participants always expect \(b\) to reflect the probability of a positive outcome irrespectively of their confidence value \(h\) before receiving AI advice, providing support for our hypothesis that (rational) decision makers implement monotone AI-assisted decisions policies. Further, this finding also implies that, for "Art", "Sarcasm" and "Census", any policy \(_{H_{}}\) that predicts the value of the label \(y\) by thresholding the confidence value \(h_{}\) will be necessarily suboptimal because the probabilities \(P(Y=1(X,Y)_{h,(b)})\) are not monotone increasing with \(b\).

Finally, we look at the AUC achieved by decision policies \(_{B}\), \(_{H}\) and \(_{H_{}}\). Table 1 (right columns) summarize the results, which shows that \(_{H_{}}\) outperforms \(_{H}\) consistently across all tasks but it only outperforms \(_{B}\) in a single task ("Cities") out of four. These findings provide empirical support for Theorem 3, which predicts that, in the presence of human-alignment violations as those observed in "Art", "Sarcasm" and "Census", any monotone AI-assisted decision policy will be suboptimal, and they also provide support for Theorem 5, which predicts that, under human-alignment, there exist near-optimal AI-assisted decision policies satisfying monotonicity.

## 7 Discussion and Limitations

In this section, we discuss the intended scope of our work and identify several limitations of our theoretical and experimental results, which may serve as starting points for future work.

**Decision making setting.** We have focused on decision making settings where both decisions and outcomes are binary. However, we think that it may be feasible to extend our theoretical analysis to settings with multi-categorical (or real-valued) outcome variables and decisions. One of the main challenges would be to identify which natural conditions utility functions may satisfy in such settings. Further, we also think that it would be significantly more challenging to extend our theoretical analysis to sequential settings--multicalibration in sequential settings is an open area of research--but our ideas may still be a useful starting point. In addition, our theoretical analysis assumes that the decision makers aim to maximize the average utility of their decisions. However, whenever human decisions are consequential to individuals, the decision maker may have fairness desiderata.

**Confidence values.** In our causal model of AI-assisted decision making, we allow the classifier's confidence values to depend on the decision maker's confidence values because this is necessary to achieve human-alignment via multicalibration as described in Section 5. However, we would like to clarify that both Theorems 3 and 5 still hold if the classifier's confidence values do not depend on the decision maker's confidence, as it is typically the status quo today. Looking into the future, our work questions this status quo by showing that, by allowing the classifier's confidence values to depend on the decision maker's confidence values, a decision maker may end up taking decisions with higher utility. Moreover, we would also like to clarify that, while the motivation behind our work is AI-assisted human decision making, our theoretical results do not depend on who--be it a classifier or another human--gives advice. As long as the advice comes in the form of confidence values, our results are valid. Finally, while we have shown that human-alignment can be achieved via multicalibration, we hypothesize that algorithms specifically designed to achieve human-alignment may have lower data and computational requirements than multicalibration algorithms.

**Experimental results.** Our experimental results demonstrate that, _across tasks_, the average utility achieved by decision makers is relatively higher if the classifier they use satisfies human-alignment. However, they do not empirically demonstrate that, _for a fixed task_, there is an improvement in average utility achieved by decision makers if the classifier they use satisfies human-alignment. The reason why we could not demonstrate the latter is because, in our experiments, we used an observational dataset gathered by others . Looking into the future, it would be very important to run a human subject study to empirically demonstrate the latter and, for now, treat our conclusions with caution.

## 8 Conclusions

We have introduced a theoretical framework to investigate what properties confidence values should have to help decision makers take better decisions. We have shown that there exists data distribution for which a rational decision maker using calibrated confidence values will always take suboptimal decisions. However, we have further shown that, if the confidence values satisfy a natural alignment property, which can be achieved via multicalibration, then a rational decision maker using these confidence values can take optimal decisions. Finally, we have illustrated our theoretical results using real human predictions on four AI-assisted decision making tasks.

**Acknowledgements.** We would like to thank Nastaran Okati for fruitful discussions at an early stage of the project. Gomez-Rodriguez acknowledges support from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 945719).