# M\({}^{3}\)GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation

Mingshuang Luo\({}^{1,2,3}\), Ruibing Hou\({}^{1}\), Zhuo Li\({}^{4}\), Hong Chang\({}^{1,3}\),

Zimo Liu\({}^{2}\), Yaowei Wang\({}^{2,5}\), Shiguang Shan\({}^{1,3}\)

\({}^{1}\)Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),

Institute of Computing Technology, CAS, China

\({}^{2}\)Peng Cheng Laboratory, China, \({}^{3}\)University of Chinese Academy of Sciences, China

\({}^{4}\)WeChat, Tencent Inc, \({}^{5}\)Harbin Institute of Technology, Shenzhen

mingshuang.luo@vipl.ict.ac.cn,{houruibing,changhong,sgshan}@ict.ac.cn

albertzli@tencent.com,liuzm@pcl.ac.cn,wangyaowei@hit.edu.cn

Corresponding author

###### Abstract

This paper presents M\({}^{3}\)GPT, an advanced **M**ultimodal, **M**ultitask framework for **M**otion comprehension and generation. M\({}^{3}\)GPT operates on three fundamental principles. The first focuses on creating a unified representation space for various motion-relevant modalities. We employ discrete vector quantization for multimodal conditional signals, such as text, music and motion/dance, enabling seamless integration into a large language model (LLM) with a single vocabulary. The second involves modeling motion generation directly in the raw motion space. This strategy circumvents the information loss associated with a discrete tokenizer, resulting in more detailed and comprehensive motion generation. Third, M\({}^{3}\)GPT learns to model the connections and synergies among various motion-relevant tasks. Text, the most familiar and well-understood modality for LLMs, is utilized as a bridge to establish connections between different motion tasks, facilitating mutual reinforcement. To our knowledge, M\({}^{3}\)GPT is the first model capable of comprehending and generating motions based on multiple signals. Extensive experiments highlight M\({}^{3}\)GPT's superior performance across various motion-relevant tasks and its powerful zero-shot generalization capabilities for extremely challenging tasks. Project page: https://luomingshuang.github.io/M3GPT/.

Figure 1: M\({}^{3}\)GPT can handle core motion comprehension and generation tasks, including text-to-motion, motion-to-text, music-to-dance, dance-to-music, motion prediction, and motion in-between. The motion sequences within the dashed-line areas are masked in the input.

Introduction

Motion comprehension and generation in multimodality are crucial for diverse applications, including AR/VR creation, video games, and virtual reality. Numerous studies  focus on motion comprehension, including captioning 3D human motions and generating music from 3D human dances2. Recent advancements in AI  have paved the way for motion generation, allowing for various control signals including textual descriptions, music pieces, and human poses. A significant shortcoming of most existing works is their focus on single-modality control signals, overlooking the potential for multimodal information integration. More importantly, the comprehension and generation of motions are predominantly studied in isolation. In reality, human motion cognition and communication indispensably require seamless transitions between any motion-relevant modalities. Therefore, it is vital to develop a unified framework for motion comprehension and generation that can efficiently utilize multiple signals simultaneously.

Recent works  have shown success in developing a unified multitask motion framework which integrates text-driven and audio-driven motion generation through a single architecture. Employing a large language model (LLM),  adeptly handles multimodal control signals, such as text and single-frame pose, to generate consecutive motions. Despite their promising performance in motion generation, these approaches often fall short in comprehending motion. MotionGPT , a recent innovation, constructs a unified motion-language model to generate plausible human motions and natural language descriptions through prompt instructions. However, MotionGPT focuses solely on a single non-motion modality, _i.e_., text. While aligning motion with one additional modality is relatively straightforward, integrating three or more modalities within a single framework and achieving bidirectional alignment among them to cover a broad range of modalities for motion comprehension and generation presents a formidable challenge.

Two main challenges need to be solved for building a unified multimodal framework for motion comprehension and generation. _The first is how to create a unified representation space across different motion-relevant modalities._ MotionGPT  and SpeechGPT  separately treat motion and speech as specific language for seamlessly integrating with text. Inspired by these efforts , we view both motion and music as distinct forms of language, facilitating better associations with text via LLMs. Specifically, akin to language, we compress raw motion and music into a sequence of discrete semantic tokens. By encoding motion, music, and language within a single vocabulary, we can build a unified representation space across these different modalities. _The second is how to model the connections and synergies among various motion tasks._ Different motion-relevant tasks are interconnected and can mutually enhance each other. Since text is the most familiar and well-understood modality for LLMs, we propose employing text as a bridge to establish connections between different motion tasks. Specifically, to better learn the complex music-to-dance task where both input and output modalities are unfamiliar to LLMs, we introduce two auxiliary tasks: music-to-text and text-to-dance, aimed at aligning music and dance modalities with the structured text embedding space. This strategy enables us to establish connections and synergies between music-to-dance and text-to-motion tasks, facilitating the alignment and collaboration of text, music, and motion/dance modalities across different tasks.

In this work, we propose a uniform **M**ultimodal, **M**ultitask framework for **M**otion comprehension and generation, namely M\({}^{3}\)GPT, that leverages the strong language generation capability of LLMs for unifying various motion-relevant tasks, as depicted in Fig. 1. M\({}^{3}\)GPT comprises three tires. **Firstly**, M\({}^{3}\)GPT is equipped with multimodal tokenizers capable of compressing raw multimodal data, including motion, music, and text, into a sequence of discrete semantic tokens. These discrete representations allow the core LLM to unify motion comprehension and generation in an autoregressive manner, operating at the discrete semantic representation space. **Secondly**, different from  that solely optimize LLM in discrete semantic space, we jointly train LLM and motion de-tokenizer, optimizing LLM in both discrete semantic space and raw continuous motion space. This operation enables the motion-space error signals from de-tokenizer to backpropagate to LLM, enhancing LLM's ability to generate the details of motion. **Thirdly**, we construct paired text descriptions for music, and design two auxiliary music-to-text and text-to-dance tasks, which aid in aligning music and dance with the text embedding space. Also, we build up a shared tokenizer for motion and dance data to project them into a shared semantic space. These auxiliary tasks and shared tokenizer establish connections between music-to-dance and text-to-motion, enabling mutual reinforcement.

We employ a multimodal pre-training + instruction-tuning pipeline to train M\({}^{3}\)GPT, enhancing inter-modal alignment and effectively aligning them with human intent. To our knowledge, M\({}^{3}\)GPT is the first approach to integrate six core tasks of motion comprehension and generation--text-to-motion, motion-to-text, music-to-dance, dance-to-music, motion prediction, and motion in-between--into a uniform framework. Extensive experiments demonstrate that M\({}^{3}\)GPT achieves competitive performance across multiple motion-relevant tasks. Additionally, through qualitative results, we demonstrate that M\({}^{3}\)GPT possesses powerful zero-shot generalization capabilities, _e.g._, long-term dance generation and music-text conditioned dance generation.

## 2 Related Work

Motion comprehension and Generation.Many existing works focus on studying human appearance, pose, detection, attribute, part parsing and so on [61; 19; 45; 40; 23; 17]. This work focuses on studying human motion, including motion comprehension and motion generation. Motion comprehension involves two core tasks: motion-to-text and dance-to-music. _Motion-to-text_ aims to describe human motion with natural language . For example, recurrent networks have been used in  to accomplish this task. _Dance-to-music_ involves creating a piece of music from a given dance [20; 27; 64]. For example, Zhun _et al._ utilizes a generative adversarial network to generate music from dance videos. On the other hand, motion generation involves generating diverse human motions using multimodal inputs, such as text [44; 56; 15; 5; 57], music [27; 18; 52] and incomplete motion [31; 1; 3]. _Text-to-motion_ is one of the most important motion generation tasks. Recent works typically map text to motion using different architectures: diffusion model  and autoregressive transformer model . _Music-to-dance_ focuses on generating dance movements from music. For example,  predicts discrete token sequences conditioned on music, which are then used to regenerate the dance sequence. _Motion Completion_ generates motion conditioning on partial motions, such as motion prediction [31; 1] and motion-in-between [34; 43]. Although these methods have shown promising results in various human motion tasks, most are limited to handling a single task. Until recently, some works [12; 60; 62] attempt to integrate two or more tasks into a unified model, as shown in Tab. 1. However, these works either lack the ability of motion comprehension [62; 60] or fail to handle music modality [12; 21]. In this work, we propose a unified motion comprehension and generation framework that can handle multiple control signals simultaneously.

Language Models and Multimodal.Large language models (LLMs) enabled by extensive datasets and model size, such as T5 , Flan-T5 , LLaMA , LLaMA-2  and Vicuna , have demonstrated impressive comprehension and generation capabilities. Researchers have leveraged the capabilities of LLMs to handle multimodal tasks, expanding them to multimodal large language models (MLLMs). For example, AnyGPT  employs LLaMA-2  to construct an any-to-any multimodal language model. NExT-GPT  employs Vicuna  with multimodal adaptors and diffusion decoders to perform tasks across arbitrary combinations of text, images, videos, and audio. Recently, the works [21; 60] attempt to use LLMs for motion-related tasks.  uses LLaMA  to build a general-purpose motion generator, which, however, lacks the ability to comprehend motion.  leverages T5 to construct a unified motion-language model, but cannot deal with music modality.

 Methods & T2M & M2T & A2D & D2A & M2M & Random M & Random T & Random A \\  TM2D & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ \\ UDE & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ \\ MotionGPT & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ \\ MotionGPT & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ \\  M\({}^{3}\)GPT (Ours) & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ & ✔ \\ 

Table 1: Comparison of recent multimodal, multitask methods across various motion comprehension and generation tasks. T2M: text-to-motion; M2T: motion-to-text; A2D: music-to-dance; D2A: dance-to-music; M2M: motion-to-motion that includes motion prediction and motion in-between. Random M, Random T, and Random A represent the unconstrained generation of motion, text, and music3, respectively.

## 3 Method

To enhance the comprehension and generation of motion-relevant modalities, we propose a unified multimodal framework named M\({}^{3}\)GPT. As illustrated in Fig. 2, M\({}^{3}\)GPT consists of multimodal tokenizers responsible for compressing raw motion and music data into discrete tokens (Sec. 3.1), and a motion-aware language model that learns to understand and generate motion tokens from LLMs by corresponding text and music (Sec. 3.2). To address motion-relevant tasks, we employ a three-stage training scheme encompassing multimodal tokenizers training, modality-alignment pre-training, and instruction tuning (Sec. 3.3). During the inference process, multimodal tokens are decoded back into their original representations by associated _de-tokenizers_ (decoders of multimodal tokenizers), enabling various motion-relevant tasks to be executed via instructions (Sec. 3.4).

### Multimodal tokenizers

As shown in Fig. 2, Multimodal tokenizers aim to discretize continuous human motion and music into language-like tokens, allowing the three modalities to be unified within a single language model.

**3D Human Motion Tokenizer.** To represent motion in discrete semantic tokens, we build a 3D human motion tokenizer based on Vector Quantized Variational Autoencoders (VQ-VAE) following [12; 62; 21; 60]. The motion tokenizer consists of a motion encoder \(_{m}\) and a motion decoder \(_{m}\), along with a codebook \(_{m}=\{b^{1},b^{2},,b^{N_{m}}\}\) containing \(N_{m}\) discrete semantic vectors. Notably, to facilitate mutual enhancement between motion and dance data, we employ a shared tokenizer for both motions and dances, projecting them into a consistent and shared semantic space. Formally, given a 3D motion sequence \(^{T_{m} d_{m}}\), where \(T_{m}\) is the time length and \(d_{m}\) is the dimensionality of each frame's pose, the motion encoder \(_{m}\) that consists of several 1-D convolutional layers projects \(\) to a latent embeddings \(^{L_{m} d}\). Here, \(L_{m}\) is the time interval after downsampling and \(d\) is the latent dimension. Next, we transform \(\) into a collection of codebook entries through discrete quantization. Specifically, the quantization process replaces each item of \(\) with its nearest embedding in the codebook \(_{m}\), obtaining the quantized latent vectors \(^{L_{m} d}\) as follows:

\[=*{arg\,min}_{b^{k}_{m}}\|-b^{k} \|_{2}.\] (1)

The motion decoder \(_{m}\), which consists of several 1-D deconvolutional layers, projects the quantized embeddings back to raw motion space as \(}=_{m}()\). Following [21; 60], the motion tokenizer can be trained by the reconstruction loss, embedding loss and commitment loss as follows:

\[_{vq}=\|}-\|_{1}+\|[]- \|_{2}^{2}+\|-[]\|_{2}^{2}\,.\] (2)

where \([]\) is the stop gradient, and \(\) is the factor that adjusts the weight of the commitment loss.

Figure 2: An overview of the M\({}^{3}\)GPT framework. M\({}^{3}\)GPT consists of multimodal tokenizers and a motion-aware language model. The training process of M\({}^{3}\)GPT consists of three stages: multimodal tokenizers training, modality-alignment pre-training, and instruction tuning.

After training the motion tokenizer, a motion sequence \(\) can be represented as a sequence of discrete codebook-indices of quantized embedding vector, namely _motion tokens_\(}^{L_{m}}\), as follows:

\[}=*{arg\,min}_{k\{1,,N_{m}\}}\|_ {m}()-b^{k}\|_{2}.\] (3)

**Music Tokenizer.** For the music data, we adopt the VQ-VAE in Jukebox  as the music tokenizer, which consists of a music encoder \(_{a}\), a music decoder \(_{a}\) and a music codebook \(_{a}\). Notably, the limited number of music samples in dance datasets makes it inadequate for training an effective music tokenizer. To leverage the strong representation ability of the tokenizer trained on the large-scale musical dataset, we use the pre-trained VQ-VAE from Jukebox , which has been trained on a dataset of 1.2 million songs. Specifically, we first segment each input music sample into 5-second music segments. Then, for each 5 seconds segment \(^{T_{a} d_{a}}\), we use the pre-trained music tokenizer \(\{_{a},_{a}\}\) to encode \(\) into a sequence of discrete codebook-indices \(}^{L_{a}}\) (namely _music tokens_) following Eq. 3.

### Language Model Backbone

**Expanding Vocabulary.** To incorporate multimodal discrete representations into a pre-trained LLM, we expand the original text vocabulary \(V_{t}\) in LLM with motion vocabulary \(_{m}\) and music vocabulary \(_{a}\), forming a new unified vocabulary \(V=\{V_{t},_{m},_{a}\}\). To accommodate the expanded vocabulary, we extend the corresponding embedding and prediction layer of LLM, where the newly incorporated parameters are initialized randomly.

**Unified Multimodal Language Model.** Equipped with multimodal tokenizers, we can compress multimodal data into discrete token sequences. To be specific, employing the trained motion tokenizer and music tokenizer, the input motion \(^{T_{m} d_{m}}\) and music \(^{T_{a} d_{a}}\) can be mapped into a sequence of discrete motion tokens \(}^{L_{m}}\) and music tokens \(}^{L_{a}}\). Then equipped with a unified vocabulary \(V\), we can formulate various motion-relevant tasks in a general format, where both input and output tokens come from the same vocabulary. These tokens can represent natural language, human motion, music, or any combination, depending on the specific task to be solved. This naturally enables the core LLM to unify motion comprehension and generation tasks in an autoregressive manner.

Following , we employ T5  as the language model backbone, which is pre-trained on 750 GB of text tokens. By leveraging this pre-trained large language model, we can harness its powerful modeling capabilities and generalizability to develop a more user-friendly, motion-related human-computer interaction model.

### Training Strategy

The training process is divided into three stages. The first stage is Multimodal Tokenizers Training, which focuses on learning the motion/music tokenizer to represent motion/music as discrete tokens. The second stage is Modality-Alignment Pre-training, which aims to align motion, music, and text modalities, and facilitate collaboration across different motion-relevant tasks. The third stage is Instruction Fine-Tuning, aimed at enhancing the model's instruction-following capability.

**Stage1: Multimodal Tokenizers Training.** We first train a motion tokenizer using the objective defined in Eq. 2. As for the music tokenizer, due to the limited music samples in existing dance datasets, we directly use the pre-trained VQ-VAE model from Jukebox . This process allows any motion sequence and music to be represented as a sequence of tokens, enabling seamless integration with text within LLM. To ensure the stability of LLM training, the encoder of motion tokenizer and whole music tokenizer remain unchanged. Notably, we continue to optimize the decoder of motion tokenizer in subsequent training stages to further enhance the quality of generated motions.

**Stage2: Modality-Alignment Pre-training.** To enable LLM to handle discrete modalities, we utilize paired motion corpus to train LLM in a next-token prediction task. This process aims to align the text, music, and motion modalities for unified reasoning in LLM.

* **Joint optimization of LLM and motion de-tokenizer.** Human motion (especially dance) encompasses intricate details. Previous works [21; 60] keep the motion de-tokenizer fixed during training LLM, which hinders LLM's ability to perceive the distribution and details of motions. Specifically, in the output space of LLM, different motion tokens are treated as independent classes; therefore, the cost of classifying a motion token as semantic-similar token and semantic-distanttoken is the same. Apparently, relying solely on LLM's autoregressive loss is insufficient for capturing the details of motion. To address this problem, we jointly optimize LLM and motion de-tokenizer in stage2 and stage3. This strategy enables the reconstruction error signals in raw motion space to backpropagate to LLM, enhancing LLM's ability to generate the details of motion. With the goal of minimizing L1 loss between the predicted and real motion, we search for the motion's token sequence that could minimize this L1 loss in original motion space. As the motion de-tokenizer continuously optimizes, the target motion's token sequence, which supervises LLM training, dynamically changes. This dynamic adjustment reduces L1 loss progressively, achieving joint optimization.
* **Synergy learning of multitasks.** Although aligning text with one additional modality is relatively straightforward, integrating multiple modalities (_e_.\(g\)., motion, text, and music) within a single framework poses a significant challenge. Additionally, as noted in , multitask joint training usually achieves inferior performance on each individual task compared to single-task training. This phenomenon is also observed in our _text-to-motion_ task, as shown in Tab. 2. We argue that the large modality difference among different motion-relevant tasks (_e_.\(g\)., _music-to-dance_ and _text-to-motion_) prevents the model from effectively establishing connections between these tasks. Thus it is difficult for the model to identify a common optimization direction that benefits all tasks. As 'text' serves as a highly semantic descriptor for other modalities and is the most familiar and well-modeled modality to LLM, we use 'text' as a bridge to align motion, text, and music data, thereby mitigating conflicts in aligning multiple modalities. Initially, we construct paired textual descriptions for music samples in the dance datasets. Specifically, we use the style annotations of the music to create paired texts, such as 'a person is dancing Jazz'. Then, we construct two auxiliary tasks using the generated pairs of music and text, \(i\)._e_., _music-to-text_ and _text-to-dance_. Through these two auxiliary tasks, M\({}^{3}\)GPT implicitly learns to decompose the complex _music-to-dance_ task into two simpler tasks _music-to-text_ and _text-to-dance_. Additionally, with a shared tokenizer for motion and dance, _text-to-dance_ and _text-to-motion_ tasks occupy the same matching space, and thus can mutually reinforce each other. In this way, M\({}^{3}\)GPT builds the synergies between the two primary motion generation tasks, music-to-dance and text-to-motion, facilitating mutual reinforcement, as shown in Tab. 2. Combining the above analysis, we jointly train LLM and motion de-tokenizer using a mixture of motion comprehension and generation tasks, along with two auxiliary _music-to-text_ and _text-to-dance_ tasks. Besides the auxiliary tasks, we consider \(2\) basic motion comprehension tasks, \(i\)._e_., motion-to-text and dance-to-music, and \(4\) basic motion generation tasks, \(i\)._e_., text-to-motion, music-to-dance, motion prediction and motion in-between. Formally, for a specific task, we denote the source input consisting of a sequence of tokens as \(_{s}=\{_{s}^{i}\}_{i=1}^{L_{s}}\), the target output as \(_{t}=\{_{t}^{i}\}_{i=1}^{L_{t}}\), LLM predicts the probability distribution of potential next token at each step \(p_{}(_{t}^{i}|_{t}^{<i},_{s})\) in an autoregressive manner. For motion generation tasks, we add a reconstruction loss. Specifically, when the output tokens are motion tokens, we pass them to motion de-tokenizer to generate a motion sequence (denoted as \(}\)), where a reconstruction loss is then employed for guidance. Overall, during this training process, the objective is to maximize the log-likelihood of the data distribution and minimize the reconstruction error within raw motion space: \[=_{i=0}^{L_{t}-1} p_{}(_{t}^{i}|_{t }^{<i},_{s})+\|}-\|_{1},\] (4) where \(\) denotes the ground-truth for \(}\) generated by motion de-tokenizer, and \(\) is a hyper-parameter to adjust the weight for reconstruction loss. Stage3: Instruction Fine-Tuning. To enhance the generalization and instruction-following capability of M\({}^{3}\)GPT, we construct a multimodal instruction dataset with resort to GPT-4, building upon existing motion datasets. Specifically, we define \(11\) core tasks, each comprising \(200/50/50\) training/validation/test instruction templates. For example, an instruction for text-to-motion task could be "Create a motion that complements the poetic elements in <Caption_Placeholder>", with <Caption_Placeholder> standing for any text sequence; an instruction for music-to-dance could be "Script a dance that adapts to the tempo shifts in <Audio_Placeholder>", with <Audio_Placeholder> standing for any music sequence. Further details are available in Appendix B.4.

### Inference M\({}^{3}\)Gpt

During inference, we evaluate M\({}^{3}\)GPT's performance across multiple motion-relevant tasks and datasets (Sec. 4.2 and Appendix (C, D). Also, we consider two challenging dance generation tasks to evaluate the zero-shot generalization ability of M\({}^{3}\)GPT:

**(1) Generating long-duration dances from long music.** Long-duration dance generation involves creating uninterrupted, coherent dance sequences based on a single piece of music. Due to the limitation of computational cost and memory overload, we train M\({}^{3}\)GPT on the task of 5-second music-to-dance generation. Conversely, during inference, we can combine this training task _music-to-dance_ to generate an initial 5-second dance segment, and an unseen zero-shot task _music+dance-to-dance_ that recursively generates subsequent dance segments conditioned on both music and previously generated dance segments, to perform long-duration and coherent dance generation.

**(2) Generating dance controlled by both music and text.** Integrating music and text as control signals in dance generation (_music+text-to-dance_) augments music-to-dance task with text modality. This process guides the generated dances to synchronize with particular actions described in input texts. Thanks to the tokenizer mechanism, M\({}^{3}\)GPT can seamlessly combine music and text in LLM's input, enabling the integration of text instructions to produce a wide variety of dance sequences.

## 4 Experiments

### Experimental setup

**Datasets and Preprocessing.** We use a large-scale text-to-motion dataset: Motion-X , and two music-to-dance datasets: AIST++  and FineDance . Notably, the 3D pose annotations differ among these datasets, therefore, we standardize their processing for uniform usage. Specifically, we select \(22\) joints common to these datasets and preprocess the motion samples following , resulting in motion sequences with identical representation. Further details on datasets and preprocessing are provided in Appendix B.1.

**Evaluation Metrics.** Different tasks employ distinct evaluation metrics. We use the most common evaluation metrics to assess the performance of M\({}^{3}\)GPT for each task. **(1) Text-to-Motion**. Following [21; 12], we use _Frechet Inception Distance (FID)_, _Diversity (Div)_, _R-Precition_ that calculates the top-1 motion-to-text retrieval accuracy (R TOP1). **(2) Motion-to-Text**. Following , we use linguistic metrics like _BLEU_, _CIDEr_, along with _R-Precision_ for evaluating motion-to-text alignment. **(3) Music-to-Dance**. Following [26; 52], we use _FID_, _Diversity_ and _Beat Align Score (BAS)_ on kinetic features  (denoted as "_k_") to evaluate the dance generation quality. Notably, as noted in , the geometric features  are unreliable as a measure of dance generation quality. So we only use the kinetic features for evaluation. **(4) Dance-to-Music**. Following , we use _Beats Coverage Scores (BCS)_, _Beats Hit Scores (BHS)_, and _F1_ score to evaluate music generation quality. **(5) Motion Prediction and In-Between**. Following , we use _FID_ and _Diversity_ to measure the consistency between the provided pose conditions and generated motion. More details and results on other evaluation metrics are provided in Appendix B.2 and C.

**Implementation Details.** For motion tokenizer, we set the codebook size to \(512\). As for music tokenizer, we use the pre-trained VQ-VAE from Jukebox with a codebook size of \(2048\). In term of temporal downsampling rate, the motion encoder uses a rate of \(4\), while the music encoder uses a rate of \(128\). We utilize T5 base  as our language model backbone. For training the motion tokenizer, we use Adam as the optimizer with a batch size of \(1000\) and an initial learning rate of \(10^{-4}\). To train the language model backbone, we employ the Adafactor_dev optimizer and use CosineAnnealingLR as the learning rate scheduler. The learning rate is set to \(2 10^{-4}\) for pre-training stage, and \(10^{-4}\) for instruction fine-tuning stage. For hyperparameter settings, \(\) in Eq. 4 is set to \(0.2\), and \(\) in Eq. 2 is set to \(0.02\). All our experiments are conducted on 8 NVIDIA A40 GPUs. To evaluate the model's performance across different platforms, we also test our trained M\({}^{3}\)GPT with T5-base on Ascend 910B NPUs. Further details on implementation and hyperparameter analysis are provided in Appendix E.

### Ablation Studies

In this section, we conduct ablation studies to validate the effectiveness of our method. We use the same model architecture throughout the experiments. The ablation results are shown in Tab. 2.

**Effectiveness of joint optimization of LLM and motion de-tokenizer.** Different from previous works [21; 60] that fix motion de-tokenizer during training LLM, we jointly optimize LLM and motion de-tokenizer in stage2 and stage3, as detailed in Sec. 3.3. As shown in Tab. 2, the joint optimization consistently brings performance gains across various metrics and most settings. Specifically, it largely enhances the fidelity of generated dances, reflected in a notable decrease in FID\({}_{k}\) score. We also notice a minor increase (less than \(0.003\)) in FID of text-to-motion task in M\({}^{3}\)GPT. The possible reason is that the motion patterns controlled by text are relatively simple, making LLM optimized solely in discrete semantic space adequate for text-to-motion. Conversely, dances involve greater complexity, necessitating the joint optimization of motion decoder to accurately capture intricate dance movements without compromising information.

**Effectiveness of synergy learning of multitasks.** During the training of M\({}^{3}\)GPT, we introduce a synergy multitask learning strategy by constructing two auxiliary tasks: Text-to-Dance (T2D) and Music-to-Text (A2T), as detailed in Sec. 3.3. As shown in Tab. 2, the inclusion of T2D and A2T consistently brings performance gains across various metrics on both text-to-motion and music-to-dance tasks. Specifically, for music-to-dance, the FID\({}_{k}\) score is decreased by nearly \(10\) points, indicating that the synergy learning helps generate more realistic dances. We argue that by incorporating these two auxiliary tasks, M\({}^{3}\)GPT implicitly learns to decompose the complex music-to-dance into two simpler tasks, music-to-text and text-to-dance. This way, the text-to-motion task can assist in learning the music-to-dance task, thereby enhancing its performance.

### Comparisons with State-of-the-arts

In this section, we compare our M\({}^{3}\)GPT with state-of-the-arts on multiple core motion-relevant tasks. We respectively report the comparison results on text-to-motion dataset, Motion-X , and music-to-dance datasets, AIST++  and FineDance . More quantitative and qualitative results are provided in Appendix C and D. Also, in the supplementary material's zip file, we provide the render videos of generated motions/dances and generated music files by our M\({}^{3}\)GPT.

**Main results on text-to-motion dataset.** On the text-to-motion dataset, Motion-X, we evaluate M\({}^{3}\)GPT on \(4\) tasks, _i.e_., text-to-motion, motion-to-text, motion prediction, and motion in-between. The comparison results are shown in Tab. 3. As shown, M\({}^{3}\)GPT achieves competitive performance across all evaluated tasks, highlighting its capability to address diverse motion tasks in a single model. Also, for text-to-motion task, _M\({}^{3}\)GPT (instruction-tuned only T2M)_, which combines multitask pre-training and instruction fine-tuning solely on T2M task, yields better performance than _Trained single

   & Re-Optimizing &  &  \\   & motion de-tokenizer & R TOP1 \(\) & FID \(\) & Div\(\) & FID\(\) & Div\({}_{k}\)\(\) & BAS \(\) \\  Ground Truth & - & 0.675 & 0.009 & 2.316 & 17.10 & 8.19 & 0.2374 \\  Trained single task & & 0.645 & 0.081 & 2.124 & 83.33 & 5.18 & 0.1835 \\ Trained single task & ✓ & **0.656** & **0.078** & 2.133 & 75.47 & 5.57 & 0.1884 \\  T2M+A2D & & 0.564 & 0.094 & 2.080 & 51.26 & 6.73 & 0.2037 \\ T2M+A2D & ✓ & 0.578 & 0.092 & 2.106 & 47.71 & 7.47 & 0.1958 \\  T2M+A2D+T2D+A2T & & 0.617 & 0.093 & 2.110 & 42.70 & 7.54 & 0.2084 \\ T2M+A2D+T2D+A2T & ✓ & 0.626 & 0.088 & 2.197 & 25.24 & **7.63** & **0.2217** \\  M\({}^{3}\)GPT (Pretrained without T2D and A2T) & & 0.526 & 0.105 & 2.058 & 40.71 & 7.47 & 0.2030 \\ M\({}^{3}\)GPT (Pretrained without T2D and A2T) & ✓ & 0.547 & 0.104 & 2.099 & 37.14 & 7.61 & 0.2005 \\  M\({}^{3}\)GPT (Pre-trained) & & 0.598 & 0.089 & 2.218 & 32.71 & 7.43 & 0.2090 \\ M\({}^{3}\)GPT (Pre-trained) & ✓ & 0.601 & 0.092 & 2.251 & 27.65 & 7.52 & 0.2105 \\  M\({}^{3}\)GPT (Instruction-tuned) & & 0.606 & 0.091 & 2.251 & 28.46 & 7.49 & 0.2052 \\ M\({}^{3}\)GPT (Instruction-tuned) & ✓ & 0.615 & 0.093 & **2.253** & **24.34** & 7.50 & 0.2056 \\  

Table 2: Evaluation of synergy learning and joint optimization of LLM and motion de-tokenizer on Text-to-Motion (Motion-X ) and Music-to-Dance (AIST++ ). T2M: Text-to-Motion. A2D: Music-to-Dance. T2D: Text-to-Dance. A2T: Music-to-Text. _Trained single task_ refers to a model trained and tested on a single task. _Pre-trained and _Instruction-tuned_ indicate the model after pre-training (stage2) and instruction tuning (stage3), followed by direct testing on each task. The arrows (\(\)) indicate that higher values are better. The arrows (\(\)) indicate that smaller values are better. **Bold** indicates the best result.

[MISSING_PAGE_FAIL:9]

method on the FineDance dataset is Lodge . This approach features a specialized two-stage architecture for generating long-duration dance sequences, progressively refining movements from coarse to fine granularity using a diffusion model. On AIST++ dataset, M\({}^{3}\)GPT reports the best FID\({}_{k}\) of \(24.34\) for music-to-dance task, the best BHS and F1 of \(94.0\) and \(94.9\) for dance-to-music task. The results in Tab. 6, tested on the Ascend 910B NPUs, also demonstrate that multitask training can enhance the performance of both music-to-dance and dance-to-music tasks.

### Evaluation on Zero-Shot Tasks

In this section, we explore M\({}^{3}\)GPT's capabilities in handling zero-shot tasks, as mentioned in Sec. 3.4. Fig. 3 (a) shows the long-term dance generation. As seen, M\({}^{3}\)GPT can generate a coherent dance sequence by seamlessly integrating the music-to-dance and zero-shot _music+dance-to-dance_ tasks. Fig. 3 (b) shows the generated 3D dance with both music and text instruction. We can see that M\({}^{3}\)GPT maintains plausible visual results in accordance with input text instructions (_cartweet_), underscoring its remarkable zero-shot generalization capability.

## 5 Conclusion

In this paper, we present M\({}^{3}\)GPT, a unified framework for comprehending and generating motion aligned with both text and music modalities. By employing text as a bridge, we build connections and synergies between different motion-relevant tasks, facilitating mutual reinforcement. Additionally, the joint optimization of LLM and motion de-tokenizer further enriches the details of generated motion, enhancing overall motion generation quality. Our extensive evaluations across various motion-relevant tasks demonstrate the effectiveness of M\({}^{3}\)GPT in both motion comprehension and generation. Besides, M\({}^{3}\)GPT exhibits strong zero-shot generalization abilities, enabling it to handle previously unseen and challenging motion-relevant tasks.

**Limitations and Broader Impacts.** Although our M\({}^{3}\)GPT has successfully processed signals from motion, text, and music modalities for motion comprehension and generation, it focuses on modeling human body movements, excluding hands and faces details. Future research can extend the scope of M\({}^{3}\)GPT to include hands and facial modeling.

**Acknowledgments.** This work is sponsored by the National Natural Science Foundation of China (NSFC) under Grant 62306301, the National Postdoctoral Program for Innovative Talents under Grant BX20220310, and the CAAI-CANN Open Fund developed on the OpenI Community. It is also supported by the project of Peng Cheng Laboratory (PCL2023A08).