# Enhancing User Intent Capture in Session-Based Recommendation with Attribute Patterns

Xin Liu\({}^{1}\)1 Zheng Li\({}^{2}\) Yifan Gao\({}^{2}\) Jingfeng Yang\({}^{2}\)

**Tianyu Cao\({}^{2}\) Zhengyang Wang\({}^{2}\) Bing Yin\({}^{2}\) Yangqiu Song\({}^{1}\)2**

\({}^{1}\)Department of Computer Science and Engineering, HKUST \({}^{2}\)Amazon.com Inc {xliucr,yqsong}@cse.ust.hk

{amzzhe,yifangao,jingfe,caoty,zhengywa,alexbyin}@amazon.com

Work was done during Xin's internship at Amazon. Corresponding author: Xin Liu and Zheng Li.Prof. Yangqiu Song is a visiting academic scholar at Amazon.

###### Abstract

The goal of session-based recommendation in E-commerce is to predict the next item that an anonymous user will purchase based on the browsing and purchase history. However, constructing global or local transition graphs to supplement session data can lead to noisy correlations and user intent vanishing. In this work, we propose the Frequent Attribute Pattern Augmented Transformer (FAPAT) that characterizes user intents by building attribute transition graphs and matching attribute patterns. Specifically, the frequent and compact attribute patterns are served as memory to augment session representations, followed by a gate and a transformer block to fuse the whole session information. Through extensive experiments on two public benchmarks and 100 million industrial data in three domains, we demonstrate that FAPAT consistently outperforms state-of-the-art methods by an average of 4.5% across various evaluation metrics (Hits, NDCG, MRR). Besides evaluating the next-item prediction, we estimate the models' capabilities to capture user intents via predicting items' attributes and period-item recommendations.

## 1 Introduction

With the explosive demand for E-commerce services , numerous user behaviors are emerging. Understanding these historical action records is critical in comprehending users' interests and intent evolution, particularly in a cold-start regime that lacks sufficient context. This has spurred research on session-based recommendations (SBR)  that capture user-side dynamics from a short-time period (namely a session) using temporally historical information. Numerous SBR algorithms have been proposed, ranging from sequence-based methods  to graph-based methods  for learning dynamic user characterization. However, both lines of methods have their limitations. Specifically, sequence-based methods treat each user behaviors in a session as an action sequence and model the local dependencies inside. This can only capture users' preference evolution via chronological order while failing to identify the complex non-adjacent item correlation, especially when the session length is insufficient to support temporal prediction . To address this issue, graph-based methods adopt a higher perspective by introducing a global item transition graph, which aggregates local session graphs constructed from historical session sequences. Thus, a newly-emerging short session sequence can benefit from the global topology (e.g., global co-occurrences) and representations (e.g., item semantics) . Unfortunately, existing session graph construction ignores temporal signals. Figure 1 shows that two different sessions result in the same session graph, leading to vanishing of user intent variation during sequence-to-graph conversion. And such global graphs are fragile due to noise from random clicks.

Besides the global item transition graph, there are other possible solutions for item correlations. One feasible solution involves item-side knowledge. Items can be connected through shared attributes, such as being manufactured by the same company . We argue that the current use of item-side metadata provides little assistance in SBR models as user intent may change over time. However, such meta-data are still useful from the view of graphs. As illustrated in Figure 2, session attributes can be organized into attribute graphs and anchored to the local session graph to create multiplexes. Such multiplexes provide several intent clues, in addition to item-side correlations. For example, the color pattern \(silver silver blue blue\) reveals the user's color intent, while the brand pattern \(Apple Apple Samsung\) implies a potential change in intent. Different sessions can benefit from shared attribute topologies and representations, which can ultimately entail implicit high-order correlations. However, the session graph is essentially a general conditional random field, and optimization becomes intractable due to the large candidate size. Graph neural networks (GNNs) also face challenges due to the possibility of over-smoothing and data noise [46; 18; 45; 32].

To alleviate the aforementioned issues, we propose a novel framework called Frequent Attribute Pattern Augmented Transformer (**FAPAT**) that considers highly frequent attribute patterns as supplementary instead of directly learning multiplex graphs. In traditional graph learning, graphlets (such as triangles, triangular pyramids, etc.) have been proven to be useful features in graph classification and representation learning [29; 15]. Therefore, we employ frequent graph pattern mining algorithms to find consequential graphlets and view them as compact hyper-edges (e.g., _cell phone_, _tablet_, _notebook_ in Figure 2). We then use these attribute patterns as accessible memory to augment session sequence encoding. Before encoding patterns, we use Jaccard similarities to rank and retrieve the most highly correlated patterns, which significantly reduces the graph density and computational cost. It has been shown that GNNs can estimate the isomorphism and frequency of substructures [39; 21; 20]. Thus, we leverage multi-head graph attention to learn pattern and local session graph representations in the aligned space [10; 22]. To incorporate temporal signals and capture user intents, we distribute graph representations back to session sequences and use external pattern memory to augment sequence representations via memory attention with relative position bias. Finally, the sequence is fully fused by a transformer block. In other words, graph information is used to aggregate attribute patterns, while temporal actions are used to encode items.

To validate the effectiveness of FAPAT, we conduct extensive experiments on two public benchmark datasets and three real-world large-scale industrial datasets with around 100 million clicks, and experimental results demonstrate significant improvement with an average boost of 4.5% across various evaluation metrics (Hits, NDCG, MRR). Compared with baselines, the attribute pattern density can significantly relieve over-smoothing. Besides, we also extend evaluation to attribute estimations and sequential recommendations to measure the model capability to capture user intents. Code and data are available at https://github.com/HKUST-KnowComp/FAPAT.

## 2 Related Work

_Neural Methods for Session-based Recommendation Systems_. Table 1 presents a summary of the distinctions between current neural techniques and our novel FAPAT. The concept of _Temporal Information_ implies Markov decision processes with previous histories. The technique of _History Attention_ employs attention for learning long-distance sequences. The approach of _Local Session

Figure 1: Local session graph construction. Figure 2: A session graph enriched by multiplex attribute graphs.

_Topology_ involves modeling session sequences from the view of session graphs. Lastly, _Global Item Correlation_ and _Attribute Association_ place emphasis on capturing item-side and attribute knowledge.

_Sequence-based Models_. FPMC  uses first-order Markov chain and matrix factorization to identify sequential patterns of long-term dependencies. However, the Markov-based method usually has difficulty exploring complicated temporal patterns beyond first-order relationships. Recently, neural networks have shown power in exploiting sequential data in SBR tasks, such as GRU4Rec . NARM  extends GRUs with attention to emphasize the user's primary purchase purpose. Similarly, STAMP  uses an attention-based memory network to capture the user's current interest. These attention-based models separately deal with the user's last behaviors and the whole session history to detect the general and latest interests. But they mainly focus on the user's preference from a temporal view but ignore the item correlations. Pre-training techniques  and multi-task learning  also demonstrate effectiveness in injecting item metadata to embeddings and predicting item attributes. Besides, some recent sequence-based approaches leverage generative pretrained language models to provide explicit explanations for recommendation systems [4; 6].

_Graph-based Models_. Graph neural networks (GNNs) have recently been explored in SBRs due to the substantial implications behind natural transition topologies. SR-GNN  adopts a gate GNN to obtain item embeddings over the local session graph and predict the next item with weighted sum pooling, showing impressive results on benchmark data. Some advanced variants have further boosted performance, such as GC-SAN  with self-attention mechanism and FGNN  with weighted attention graph layers. To acquire further collaborative information, S2-DHCN  constructs line graphs to capture correlations among neighbor sessions, and GCE-GNN  directly applies a graph convolution over the global transitions to aggregate more relevant items for local sessions. However, GNN-based methods still face challenges in capturing temporal signals, filtering noise, and leveraging implicit high-order collaborative information. Other methods, such as LESSR  and MSGIFSR , have also shown significant improvement by building multigraphs and shortcut graphs for session representation learning and user intents from different granularities, respectively.

_Pattern Mining for Recommendation Systems_. Pattern mining is an important data mining technique with board applications. In recommendation systems, sequential pattern mining assists in analyzing customer purchase behaviors through frequent sequential patterns. Such mining focuses on item patterns with frequencies above a threshold in all sessions, which reduces the diversity of the recommended items . Personalized sequential pattern mining  effectively learns user-specific sequence importance knowledge and improves the accuracy of recommendations for target users. It can be challenging to generalize to SBR systems when neural networks have already implicitly captured such behavior patterns. But attribute graph patterns still need to be explored in SBR.

## 3 Background and Motivations

### Problem Definition

Session-Based Recommendation (SBR) assumes that users' historical behaviors outside the current session are inaccessible, in contrast to general recommendations. For example, users do not log in due to user privacy and security reasons. SBR predicts the next item that an anonymous user is most

   Methods & Temporal Information & History Attention & Local Session Topology & Global Item Correlation & Attribute Association \\  FPMC & ✓ & ✗ & ✓ & ✗ \\ GRU4Rec & ✓ & ✗ & ✗ & ✗ \\ NARM & ✓ & ✓ & ✗ & ✗ \\ STAMP & ✓ & ✓ & ✗ & ✗ \\ CSRM & ✓ & ✓ & ✗ & ✓ \\ S3-Rec & ✓ & ✓ & ✗ & ✓ \\ M2TRec & ✓ & ✓ & ✗ & ✓ \\  SR-GNN & ✗ & ✓ & ✗ & ✗ \\ GC-SAN & ✗ & ✓ & ✓ & ✗ \\ S2-DHCN & ✗ & ✗ & ✓ & ✓ \\ GCE-GNN & ✗ & ✓ & ✓ & ✗ \\ LESSR & ✓ & ✗ & ✓ & ✗ \\ MSGIFSR & ✓ & ✓ & ✓ & ✗ \\  FAPAT & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison with existing popular methods.

likely to click on or purchase based on historical behaviors within a short period. Despite the lack of personal profiles, this universal setting can better reflect the quality of item-side recommendations. Suppose that there are \(N\) unique items in the database, and each session is represented as a repeatable sequence of items \(S=[v_{1},v_{2},,v_{L}]\), \(v_{i}\)\((1 i L)\) represents the \(i\)-th behavioral item of the anonymous user within session \(S\), where \(\) is the item set collected from overall sessions, and \(L\) is the length of the session. Given a session \(S\), the goal is to recommend the top-\(K\) items (\(1 K N\)) that have the highest probabilities of being clicked by the anonymous user.

### Session Graphs and Transition Graphs

Session sequence modeling is not always sufficient for SBR as it only reflects transitions from the user side. To account for item correlations, SR-GNN  converts session sequences to session graphs. Each session graph \(_{S}=(_{S},_{S})\) is a directed graph with node set \(_{S}\), consisting of unique items in the session, and edge set \(_{S}\), recording adjacent relations between two items in session \(S\). Edge weights can be normalized by indegrees or outdegrees to model transition probabilities. GCE-GNN  extends this graph modeling by merging all session graphs as a global transition graph, which aggregates more item correlations but faces over-smoothing and data noise [46; 18].

Instead of modeling global item transitions, we enrich session graphs with attributes and patterns. Assume there are \(M\) different kinds of attributes, and the \(m\)-th attribute type \(^{(m)}\)\((1 m M)\) consists of \(|^{(m)}|\) possible values. Thus, each item \(v_{S}\) has attribute list \([a_{v}^{(1)},a_{v}^{(2)},,a_{v}^{(M)}]\), where \(a_{v}^{(m)}^{(m)}\) denotes the \(m\)-th attribute value of \(v\). Each session sequence \(S\) corresponds to \(M\) attribute histories, with the \(m\)-th attribute sequence denoted as \(S^{(m)}=[a_{v_{1}}^{(m)},a_{v_{2}}^{(m)},,a_{v_{L}}^{(m)}]\). Using the sequence-to-graph transform, we convert \(S^{(m)}\) to \(_{S}^{(m)}\), whi is usually denser than the session graph \(_{S}\). Finally, the \(M\) attribute sequences are separately transformed into \(M\) attribute session graphs in different property-specific channels, anchored in items \(_{S}\). Finally, we add edges for item \(v_{i}\) and attribute \(a_{v_{i}}^{(m)}\) to construct a multiplex session graph, preserving attribute values and transitions. We represent the session graph with attributes as the multiplex \(_{S}^{}\) (as Figure 2).

### Frequent Pattern Mining

Frequent pattern mining aims to extract inductive clues from data to comprehend data distributions, which includes two main categories: sequential pattern mining and graph pattern mining. The former is concerned with sequence databases composed of ordered elements, while the latter statistics the important graph structures. For two sequences \(S^{}=[v^{}{}_{1},v^{}{}_{2},,v^{}{}_{L^{}}]\) and \(S=[v_{1},v_{2},,v_{L}]\), we refer to \(S^{}\) as the pattern of \(S\) if \(S^{}\) is a subsequence of \(S\). Similarly, a graph \(_{S^{}}=(_{S^{}},_{S^{}})\) is a subgraph of \(_{S}=(_{S},_{S})\) if \(_{S^{}}_{S}\) and \(_{S^{}}_{S}\). Compared with sequence pattern mining, graph pattern mining is more general since it involves the structural topology and attribute information. For instance, as depicted in Figure 2, {_cell phone_, _tablet_, _notebook_} corresponds to a triangle. It may be challenging to discover the triangle from the temporal sequence, but it is a vital clue from the graph view. Thus, we stick on frequent graph patterns rather than sequence patterns.

## 4 Methodology

We present the Frequent Attribute Pattern Augmented Transformer (FAPAT), a novel framework that captures user intents and item correlations. Our method is built upon session sequences and corresponding attribute graphs. Initially, we mine frequent attribute patterns from the attribute graphs to explore coarse-grained item correlations. These patterns are then used as memory to enhance the session encoder, which consists of graph-nested transformer layers. Figure 3 illustrates the overview.

### Frequent Attribute Pattern Acquisition

In this subsection, we describe how to extract frequent patterns from training recommendation sessions. The aim of frequent pattern mining is to minimize the impact of random clicks in the global transition graph and avoid over-smoothing when learning multiplex attribute graphs. To achieve this, we design a mining-filtering paradigm to ensure representativeness.

#### 4.1.1 Graph Pattern Mining

Small graph patterns, also called motifs or graphlets, are valuable features in graph learning and property prediction [29; 15], exhibiting strong statistical correlations between graph structures and node semantics. In this study, we focus on similar graphlet structures but extend to SBR scenarios. We collect patterns consisting of no more than four nodes (representing different attribute values), and further restrict them to those containing either a circle or a triangle to significantly reduce the number of candidates. We adopt gSpan  to acquire undirected patterns from attribute session graphs and keep patterns belonging to one of twenty types shown in Figure 3.

#### 4.1.2 Loose Pattern Filtering

However, complex patterns may contain smaller ones. For instance, the first pattern in Figure 3 is a subgraph of the second. While each subgraph has an equal or higher frequency than its supergraph, such loose patterns do not convey much information. To eliminate them, we employ VF2  subgraph isomorphism algorithm to filter. If a pattern \(P^{}\) is a subgraph of another pattern \(P\), then \(P^{}\) is excluded from the pattern candidates. Finally, we retain compact and frequent patterns for session encoding.

### Intent-aware Sequence Encoding

To recommend items of high interest to users, we use a GAT-based encoder to learn pattern representations from the item side, which are then served as memory to augment session encoding.

#### 4.2.1 Relevant Graph Pattern Retrieval

The input item sequence is converted to a multiplex session graph representing the transitional information of different item attributes, as depicted in Figure 1. To improve graph representations by utilizing frequent attribute subgraphs, we retrieve relevant patterns from those mined in SS4.1. For the \(m\)-th attribute type \(^{(m)}\)\((1 m M)\), we denote an arbitrary subgraph mined from the previous step as \(^{(m)}_{P}=(^{(m)}_{P},^{(m)}_{P})\). Then we retrieve at most \(I\) subgraph patterns that have the most considerable Jaccard similarities to the transition graph \(^{(m)}_{S}\) as Eq. (1). This can be done within \((|^{(m)}_{S}||^{(m)}_{P}|)\), but this is always linear because of \(|^{(m)}_{P}| 4\).

\[^{(m)}_{S},^{(m)}_{P}= ^{(m)}_{S}^{(m)}_{P}|}{|^{(m)}_{S} |+|^{(m)}_{P}|-|^{(m)}_{S}^{(m)}_{P}|}.\] (1)

Figure 3: An overview of FAPAT.

[MISSING_PAGE_FAIL:6]

where \(_{}^{(m)}^{d d}\) and \(_{}^{(m)}^{d}\) are trainable parameters (\(d\) is the dimension of hidden states), \(}_{i}^{(m)}\) is the memory-augmented results, and \(_{i}^{(m)}\) controls the importance of attribute patterns. While \(}_{i}\) combines attribute information, it disregards long-range histories as \(}_{i}^{(m)}\) solely accesses one-hop neighbors. Thus, we incorporate a transformer block to aggregate global session information:

\[=}_{} _{1 i L}}_{i}}_{},\] (7)

where \(}_{}\) and \(}_{}\) are directly taken from Eq. (6) since they do not belong to the original session but still observe the memory patterns and relative temporal signals.

### Next-item Recommendation

Once the sequence representations are obtained, the next step is to predict the next item that may interest the user for clicking or purchasing. We adopt the approach used in previous work [35; 34; 37] to concatenate additional reversed positional embeddings as follows:

\[_{i}=_{z}_{i}_{L-i +1}+_{z},0 i L+1,\] (8)

where \(_{i}\) is the \(i\)-th item representation \([i]\) from Eq. (7), and \(_{z}^{2d d}\) and \(_{z}^{d}\) are trainable parameters. This concatenation of positional embeddings is intended to prioritize nearest intents over long-distance historical purposes. Figure 2 demonstrates this idea through two cases: the male user is more likely to be interested in Android cell phones since his last click was in that category, while the female user sticks to Apple products as she reviews iPhone and iPad once again.

We utilize the representation of the MASK to compute soft attention and then represent the session and the user's latest intent through a weighted average:

\[=_{i=1}^{L}_{i}_{i},\ _{i}=_{ }^{}(_{}_{i}+_{}+ _{}),\] (9)

where \(_{}^{d d}\) and \(_{}^{d}\) are trainable parameters. We compute the prediction of the next item using a similarity-based approach rather than a linear layer. This approach is similar to the optimization of Bayesian Personalized Ranking (BPR) . Alternatively, we can train the model using cross-entropy minimization. Finally, the prediction is obtained by \(=*{argmax}_{v}(^{}_{v})\).

## 5 Experiment

### Setup

_Datasets_. We first evaluate our method on public benchmarks. _diginetica_ contains the browser logs and anonymized transactions, _Tmall_ collects anonymous users' shopping logs on the Tmall online website. We also acquire sessions from the browse and purchase logs from our E-commerce platform. We target at _beauty_, _books_, and _electronics_ and gather 20-minute interactions within last successful purchases into one session after removing long-tail items. Appendix C provides more details.

_Baselines_. We compare our method with seven sequence-based baselines (FPMC , GRU4Rec , NARM , STAMP , CSRM , S3-Rec , and M2TRec ) and six graph-based baselines (SR-GNN , GC-SAN , S2-DHCN , GCE-GNN , LESSR , and MSGIFSR ). Model details are given in Appendix D. Each model is aligned with the official code implementation.

_Evaluation_. We evaluate SBRs as a ranking problem and employ Hits@\(K\), NDCG@\(K\), MRR@\(K\) as standard metrics. Hits@\(K\) measures the percentage of ranks up to and including \(K\), while NDCG@\(K\)

Figure 4: Schema of pattern augmented attention: relative position bias is added, and one item can only access memory and previous histories.

[MISSING_PAGE_FAIL:8]

### Ablation Study

Attribute Pattern Augmentation.To evaluate the effect of attribute patterns, we conduct experiments on variants with single attributes or without any attribute. The same soft attention strategy from Eq. (6) is employed to fuse attribute embeddings for competitive baselines. Results in Figure 5 show that attribute pattern augmentation is more stable than attribute soft attention. FAPAT benefits from graph-nested attention, where the graph attention aligns the hidden space, and the memory attention captures item correlations and user intents. But attribute embeddings may have side effects on optimization in baselines, especially in graph neural networks. We also discover that not all attributes have a positive impact. Comparing among attribute pattern numbers, attribute patterns with significant frequencies (slightly lower than or similar to the item number) can have adverse effects.

Graph-nested Attention.The graph-nested attention is one of our contributions, distinguishing it from GraphFormer . Unlike GraphFormer, our graph attention is integrated inside the blocks, which allows for direct benefit from the broader attention in the following self-attention via back-propagation. To ensure fairness, we replace the encoding module of FAPAT with GraphFormer and vanilla Transformer. Results in Table 4 demonstrate the advantages of our proposed graph-nested attention. Our experiments also show that even a simple vanilla Transformer can outperform previous state-of-the-art models by a significant margin, indicating the importance of emphasizing temporal information in SBRs and the appropriateness of attention for capturing long-distance dependencies.

### Intent Capture Inspection

Attribute Estimation.Beyond item predictions, we also estimate the awareness of user intents from the product attribute side. We do not require models (except M2TRec) to predict attributes but to retrieve attributes from predicted items instead. We consider it a successful estimation if the top-ranked items have the same attribute value as the ground truth. M2TRec performs well on public data with multi-task learning but struggles on industrial E-commerce data with four attributes, as shown in Table 5. Pretraining in S3-Rec is not helpful due to catastrophic forgetting. GNN-based models perform similarly, except on _Tmall_, where the data are too sparse so that global collaborative information assists. On the contrary, FAPAT achieves robust predictions across four datasets. Even when session data are sufficient, frequent patterns remain effective.

    &  &  &  &  \\  & Hits@10 & MRR@10 & Hits@10 & MRR@10 & Hits@10 & MRR@10 & Hits@10 & MRR@10 \\  FAPAT w/o Attr. & **36.82** & 16.29 & **31.53** & **18.86** & **88.70** & **67.11** & **77.18** & **49.66** \\ GraphFormer & 36.05* & 16.17 & 30.05* & 18.58 & 88.48 & 65.65 & 77.03 & 47.86 \\ Transformer & 36.30* & 16.02* & 28.83* & 18.30* & 88.10 & 65.90 & 74.67 & 46.54 \\   

Table 4: Results of encoder comparison, where * indicates the p-value < 0.05 in t-test.

Figure 5: Effects of different attribute settings.

Period-item Recommendation.In addition to single-step evaluation, we revisit recommenders. A sound and robust SRB system must understand user intents in deep and foresee the possible consistency and the potential change. Autoregressive settings pose challenges for GNNs due to short click histories and error accumulation. Therefore, we evaluate period recommendations like search engines by comparing the top-10 predicted items with the next 3/5/10 clicks. Figure 6 demonstrates period-recommendation performance. Sequence models offer steady results in Recall, indicating that temporal information is one of the prerequisites to analyzing users' latest intents. However, pretraining may hinder models' ability to generalize over long periods, resulting in a severe decline in NDCG. Our FAPAT achieves the best performance in all metrics, indicating the effectiveness of attribute graphlets in capturing deep user intents.

## 6 Conclusion

Our paper introduces FAPAT, a novel framework that leverages attribute graph patterns to augment anonymous sequence encoding for session-based recommendations. Compared to other GNN-based methods, frequent attribute graphlets can reduce noise and topology densities for enhancing user intent capture. Our sequence encoder can better preserve temporal signals and forecast the user's latest intents. Experimental results clearly illustrate the effectiveness, and extensive ablation studies and intent capture inspections provide additional support. We discuss limitations in Appendix A. One of the future works is to improve ranking priority by combining pretraining and pattern augmentation.