# Enhancing Large Language Models

through Adaptive Tokenizers

 Mengyu Zheng

The University of Sydney

Huawei Noah's Ark Lab

mzhe4259@uni.sydney.edu.au &Hanting Chen

Huawei Noah's Ark Lab

chenhanting@huawei.com &Tianyu Guo

Huawei Noah's Ark Lab

tianyu.guo@huawei.com &Chong Zhu

Huawei Noah's Ark Lab

zhuchong4@huawei.com &Binfan Zheng

Huawei GTS AI Computing LAB

zhengbinfani@huawei.com &Chang Xu

The University of Sydney

c.xu@sydney.edu.au &Yunhe Wang

Huawei Noah's Ark Lab

yunhe.wang@huawei.com

Corresponding author

###### Abstract

Tokenizers serve as crucial interfaces between models and linguistic data, substantially influencing the efficacy and precision of large language models (LLMs). Traditional tokenization methods often rely on static frequency-based statistics and are not inherently synchronized with LLM architectures, which may limit model performance. In this study, we propose a simple but effective method to learn tokenizers specifically engineered for seamless integration with LLMs. Initiating with a broad initial vocabulary, we refine our tokenizer by monitoring changes in the model's perplexity during training, allowing for the selection of a tokenizer that is closely aligned with the model's evolving dynamics. Through iterative refinement, we develop an optimized tokenizer. Our empirical evaluations demonstrate that this adaptive approach significantly enhances accuracy compared to conventional methods, maintaining comparable vocabulary sizes and affirming its potential to improve LLM functionality.

## 1 Introduction

In recent years, large language models (LLMs) have emerged as foundational tools across a spectrum of applications in natural language processing . From generating human-like text to enabling complex question-answering systems , LLMs have proven to be exceptionally versatile and capable. At the core of these models lies the tokenizer, a critical component that dictates how natural language is transformed into a format amenable to computational processing. The effectiveness of a tokenizer directly influences the model's ability to understand and generate language, thus playing a pivotal role in the overall performance of the LLM. Recognizing this integral relationship, it becomes essential to develop tokenizers that are not only effective but also dynamically adaptable to the evolving architectures of contemporary LLMs.

Current tokenization methods for large language models (LLMs) primarily include Byte Pair Encoding (BPE) , WordPiece , and Unigram , each serving to enhance text preprocessing by splittingit into manageable subwords. BPE focuses on reducing the dataset size through a greedy merging strategy based on character or subword frequency, effectively addressing the issue of infrequent words by splitting them into more common subunits. WordPiece, similar to BPE, starts with a base vocabulary and iteratively refines it by merging the most frequent pairs but incorporates a likelihood maximization step, which makes it slightly more context-aware than BPE. Unigram tokenization operates somewhat inversely, beginning with a large vocabulary and iteratively pruning it down based on token utility calculated through negative log likelihood, aiming to optimize the vocabulary against corpus loss metrics. Despite their efficiency in handling large vocabularies and improving computational feasibility, these tokenization methods are typically fixed once developed and are not designed to adapt or learn from the model's evolving understanding of language during training.

While traditional tokenization methods have been instrumental in enhancing the efficiency and effectiveness of large language models (LLMs), they are typically decoupled from the model's learning mechanisms. This means they do not adapt or evolve based on the model's performance or the specific requirements of the tasks being addressed. Instead, these methods prioritize compressing the vocabulary size, which can sometimes lead to suboptimal performance in complex language tasks where adaptability and contextual understanding are crucial. Recent advances in end-to-end learnable tokenization [17; 16; 38] aim to address these deficiencies by more closely integrating tokenization with the model's learning processes. However, these systems, while innovative, introduce significant computational overhead (_e.g._, gradient-based tokenization and pooling modules) and lack the flexibility of traditional tokenization methods, which can be easily transferred across different models.

In this study, we address the limitations of traditional tokenization methods by creating a system where the tokenizer's development is coupled with the performance of the LLM itself. Specifically, we introduce an adaptive tokenizer that begins with a comprehensive initial vocabulary. As training progresses, we fine-tune this tokenizer by closely monitoring the model's perplexity. This ongoing adjustment allows the tokenizer to evolve in tandem with the LLM, ensuring that the tokenization process remains optimally aligned with the model's dynamic learning patterns. Our empirical results confirm that this adaptive approach markedly improves accuracy over traditional methods, demonstrating its potential to significantly enhance LLM functionality.

## 2 Related Works

### Subword tokenizer

Subword tokenizers are widely applied in many large language models (LLMs) [32; 19; 42; 35; 20], such as GPT-3 , BERT , and T5 . This is because subword tokenizers do not face the out-of-vocabulary issues that word-level tokenizers do. Unlike character-level tokenizers [36; 27], they do not require processing longer sequences at the character level, which significantly increases the complexity of the models quadratically . Specifically, BPE (Byte Pair Encoding)  applies a compression algorithm  to the task of word segmentation. Unlike BPE, which builds a vocabulary from smaller to larger units, Unigram  starts by preparing a large seed vocabulary. The vocabulary is pruned until it reaches the specified size. Similar to the Unigram framework and its assumptions, BytePiece  trims the vocabulary based on token frequency to the required size. In its initial text encoding phase, BytePiece converts the corpus into bytes, enhancing the training speed of the tokenizer and achieving a higher compression rate. Obviously, these tokenizers are data-driven , with the generated vocabulary built based on the frequencies of word fragments . However, they cannot directly interact with LLMs to enhance model capabilities .

### Learnable tokenizer

Unlike traditional tokenizers that offer a predefined and fixed vocabulary, learnable tokenizers [17; 16; 38; 37; 34; 8] can be integrated with large language models into an end-to-end learning framework, resulting in task-specific tokenization to enhance the performance of LLMs. MANTa  introduces a gradient-based tokenization and pooling module that can be jointly learned with an encoder-decoder LLM . RETVec  embeds words into a high-dimensional vector with a pre-trained model to be robust against adversarial attacks. Neural  adapts the tokenization behavior to the downstream task after pre-training the tokenization by distilling from a language-specific subword tokenizer. However, such tokenizers have high requirements for the quantity and quality of the training data. Ifthe data distribution is unbalanced or contains too much noisy data, it can lead to poor generalization of the tokenizer and negatively affect the performance of LLMs. For instance, Neural  requires a pre-training dataset curated with space-separated tokens and two carefully crafted heuristics to improve the ground label of the dataset. Therefore, stringent requirements for the quality of training data limit their widespread application.

In addition, some existing works have applied the concept of adaptive tokenizers in several fields, such as neural machine translation , domain adaptation , and text generation . These task-adaptive tokenizers integrate tokenizers generated from different data distributions, focusing on how to combine the task-specific tokenizer with the other one. In contrast, our proposed model, ADAT, is designed to learn a general tokenizer. Therefore, the purpose of ADAT is different from that of the aforementioned adaptive tokenizers.

## 3 Adaptive Tokenizers

For Large Language Models (LLMs), two critical aspects are accuracy and inference speed, both of which are deeply intertwined with the design of the tokenizer. Specifically, an optimal vocabulary \(V\) with maximum size \(N\) in an objective dataset \(D\) can be described by the following optimization problem:

\[_{V}(D_{o},V)-(D,M,V),|V| N,\] (1)

where \(M\) denotes the trained LLM, **Length** and **Acc** denotes the sequence length and accuracy (or the performance) using vocabulary \(V\) in dataset \(D\), and the \(\) is a hyper-parameter to balance the two terms. This problem incorporates two primary objectives: given a fixed vocabulary size, the first is to maximize inference speed given a fixed vocabulary size, and the second is to maximize model accuracy. However, existing tokenizer schemes typically focus on one aspect over the other; for instance, traditional frequency-based schemes emphasize speed, while end-to-end approaches prioritize accuracy. To address this, we propose a method that optimizes both aspects. Therefore, we introduce an improved approach based on traditional frequency statistics, termed "adaptive tokenizers." This method aims to refine the balance between vocabulary efficiency and performance, thereby enhancing both the speed and accuracy of the model.

Figure 1: Illustration of the proposed ADAT pipeline. (a) The traditional tokenizer algorithm that directly extracts vocabulary from data. (b) The framework of the LLM-enhanced tokenizer, iteratively refining vocabulary based on model feedback. (c) Overview of ADAT, encompassing initial tokenizer acquisition, training and inference to derive token losses, token pruning based on scores and losses.

### Unigram Model

Traditional tokenization methods generally fall into two categories: one approach, exemplified by Byte Pair Encoding (BPE) and WordPiece, starts with a small set of symbols and incrementally builds a larger vocabulary by merging the most frequent adjacent pairs. The other approach, typified by the Unigram method, begins with a large initial vocabulary which is progressively pruned based on token utility, a method found to generally offer superior performance due to its probabilistic foundation.

The Unigram model operates on the principle that the probability of a sentence is determined by the individual probabilities of its tokens. Here we briefly review the Unigram model. Initially, a large vocabulary \(V\) is established. This extensive initial set includes potentially every unique word or subword unit observed in the training corpus, ensuring that the vocabulary can cover all possible textual inputs. The process of refining the vocabulary involves several key steps repeated in cycles: 1.

**Probability Estimation**: For each token \(x_{i}\) in the current vocabulary, we calculate its probability \(p(x_{i})\) based on its frequency of occurrence in the corpus. 2. **Loss Calculation**: We then compute the loss for each token, which is determined by how much the overall loss of the model would decrease if that token were removed. The loss function is calculated as:

\[_{P}(V)=_{s=1}^{|V|}(p(X^{(s)})=_{i=1}^{|V|}(_{  S((X^{(s)}))}^{P}()),\] (2)

where \(S(X)\) is a set of segmentation candidates built from the input sentence \(X\), and \(=(x_{1},...,x_{K})\) is a subword sequence that \(P()=_{i=1}^{K}p(x_{i})\). The loss for each token \(x_{i}\) is then formulated as \(_{P}(x_{i})=_{P}(V)-_{P}(V-x_{i})\). 3. **Token Pruning**: Tokens are ranked according to their calculated loss. Finally, a proportion of tokens contributing the most to increasing the overall loss is pruned from the vocabulary.

### LLM-Enhanced Tokenization

To enhance the integration of Large Language Models (LLMs) with our tokenization process, we have developed a simple but effective refined method for calculating the loss associated with each token, incorporating insights directly from the LLM's performance metrics. This approach aims to optimize the tokenizer's vocabulary to better align with the LLM's understanding and generation of text. The framework is illustrated in Figure 1.

Our revised loss calculation method integrates the traditional Unigram model's frequency-based loss with a performance-driven loss derived from an LLM. Specifically, we first train an LLM \(M\) in the vocabulary \(D\) using a training dataset \(T\). This model is designed to capture the linguistic nuances relevant to the tasks it is trained for, providing a robust framework for assessing token utility. For each token \(x_{i}\) in the vocabulary, we measure its individual contribution to the model's error using a cross-entropy loss function. The loss for each token is calculated as:

\[_{M}(x_{i})=_{x_{i} T}CE(M(x_{i-1}),x_{i}).\] (3)

Here, \(CE\) represents the cross-entropy function, \(M(x_{i-1})\) is the LLM's output given the previous token \(x_{i-1}\), and \(x_{i}\) is the actual next token. This formula assesses how well the LLM predicts each token following its predecessor, providing a direct measure of each token's impact on model performance. Finally, The cross-entropy loss for each token is then combined with the traditional Unigram frequency-based loss. This combined loss ensures that tokens are evaluated not only on their frequency of occurrence but also on their actual contribution to the LLM's task performance. The final loss for pruning the vocabulary is given by:

\[(x_{i})=F(_{P}(x_{i}),_{M}(x_{i})),\] (4)

where \(F(,)\) is a function to balance the importance of frequency-based loss and LLM-driven loss, which will be discussed in experiments. Using this enhanced loss metric, we iteratively refine the vocabulary by pruning tokens that contribute the least to the combined loss, thus optimizing the vocabulary for both general language understanding and specific task performance. This process continues until the vocabulary is compact enough to manage while still being comprehensive enough to support the LLM effectively.

**Random sampling.** In the training of Large Language Models (LLMs), ensuring that each token within the set vocabulary receives equal and substantial training is crucial to prevent loss bias due to uneven training. While iterating over all possible tokenizations of the training corpus would ideally provide the most comprehensive learning experience, this approach is computationally prohibitive due to the immense variety of potential segmentations. To address this, we adopt the classic Viterbi algorithm  to perform randomized tokenization of the training data. This method allows for a diverse and balanced exposure of all tokens within the vocabulary to the learning process. The Viterbi algorithm efficiently determines the most probable tokenization paths through a probabilistic model of token occurrence, which significantly reduces the computational overhead compared to exhaustive methods. By leveraging this approach, our LLM can learn each token in the vocabulary more uniformly, enhancing the overall robustness and performance of the model.

**Loss momentum.** In the iterative process of training Large Language Models, maintaining the stability of the vocabulary is crucial to ensure consistent learning outcomes. To achieve this, we propose a momentum-based improvement for calculating the loss during each iteration. Specifically, the loss for iteration \(j\) of token \(x_{i}\), denoted as \(^{j}(x_{i})\), is not solely computed based on the current data but is also weighted by the loss from the previous iteration \(L^{j-1}\). This approach allows for a smoother convergence and mitigates fluctuations in training dynamics. The formula for updating the loss at each iteration is given by:

\[^{j}_{}(x_{i})=^{j-1}_{}(x_{i})+^{j}(x_{i}),\] (5)

where \(\) is the momentum coefficient that controls the extent to which the previous loss influences the current loss. This methodology not only stabilizes the vocabulary updates across iterations but also enhances the model's ability to generalize from the training data by reducing the variability in loss across successive training epochs.

## 4 Experiments

In this section, we outline the comprehensive experimental framework designed to assess the effectiveness of our proposed tokenizer, **Ad**aptive **T**okenizer (**ADAT**), in comparison to established methods such as Byte Pair Encoding (BPE)  and the Unigram model . These evaluations utilize the Pythia  suite of models at various scales, leveraging a substantial corpus to ensure robust and generalizable results.

### Experimental Setup

Model FrameworkWe deploy the Pythia framework  for its lightweight design and adaptability across different computational setups. Pythia's flexibility facilitates reproducibility and consistent assessment of performance, making it an ideal choice for evaluating the scalability and efficiency of various tokenization strategies across model sizes of 70M, 160M, and 410M parameters.

Data CorpusThe study utilizes a substantial corpus extracted from The Pile , consisting of 56GB of raw data across 91 files. We specifically excluded subsets from DM_Mathematics and Github to ensure the relevance and quality of the data. The remaining data, approximately 16 billion tokens after a random shuffle, was tokenized using a Unigram  tokenizer with a vocabulary size of 50,000 tokens. A detailed enumeration of the data files used is available in Supp. A.8.

Baseline MethodsOur investigation compares four tokenization methods: Bytepiece , Byte Pair Encoding (BPE) , Unigram , and our proposed **ADAT**. These tokenizers were selected based on their established efficacy in handling large corpora and their theoretical implications for processing complex linguistic data.

Evaluation MetricsThe effectiveness of each tokenization strategy is rigorously evaluated using several metrics. These include Perplexity (PPL), which measures the model's predictive accuracy, and Compression Rate(refer to A.1), assessing how efficiently the tokenization process reduces vocabulary size while preserving linguistic diversity. We calculate PPL for all models on PG19  dataset. Specifically, we use its test set and the first 2048 tokens for each book. Furthermore, we use the Language Model Evaluation Harness  to run five-shot evaluations on eight common languagemodeling benchmarks: Lambda (OpenAI) , PIQA , WinoGrande , ARC-Easy , ARC-Challenge , SciQ , LogiQA , and SST-2 [33; 41], to provide a comprehensive insight into each method's capabilities.

By analyzing the impact of tokenization on model scalability and the influence of vocabulary size variations, this study aims to enhance our understanding of how tokenization strategies can optimize language models for efficiency and linguistic performance. The findings are expected to contribute significantly to the development of more robust and adaptable language processing tools, catering to a wide array of NLP applications. The Pythia models are trained using a corpus of 15B tokens, where training the 70M model consumes approximately 48 GPU hours with FlashAttention . The models used for loss calculation require additional 2 GPU hours.

### Tokenization Methods Evaluation

In this section, we examine the effects of different tokenization strategies on the training effectiveness. The core objective is to explore how variations in the vocabulary, induced by different tokenization methods, affect model training and performance.

The Pythia-70M model is selected due to its moderate size and efficiency, which help mitigate the complexities associated with larger model architectures. It is initialized with random weights and undergoes a single training epoch using pre-training data. This data is processed with vocabularies generated from 1/10th of the training corpus (approximately 1.5 billion tokens), each containing 50,000 tokens--a size consistent with the Pythia  setup.

Baseline tokenization methods including BPE, Unigram, and BytePiece, generate vocabularies consisting of 50,000 tokens directly from initial data that approximately one-tenth of the training corpus (about 1.5 billion tokens). In contrast, for the proposed ADAT method, initial vocabularies are generated using either BytePiece or Unigram with 150,000 tokens. These are then methodically refined down to 50,000 tokens over 5 iterative steps, matching the baseline vocabulary size. At each step, a randomly initialized model is trained on approximately 0.3 billion tokens from the initial dataset. Subsequently, the model performs inference on a subset of 0.1 billion tokens, during which token loss is calculated. This loss data, when combined with token frequency using the formula \(\), guides the vocabulary pruning process. An ablation study on the combination methods will be discussed in Section 4.6.4.

As illustrated in Table 1, our method achieves its best performance when initialized with the Unigram vocabulary, recording a score of 44.51. This score represents a considerable improvement of 2.29 points over the standard Unigram model and surpasses the BPE model by 2.6 points. Additionally, our approach shows a notable enhancement of 2.7 points when utilizing BytePiece as the initial vocabulary. Although the BytePiece vocabulary generally exhibits inferior baseline results, our method effectively elevates its performance, indicating robustness across both high-quality (Unigram) and lower-quality (BytePiece) vocabularies. These results not only affirm the efficacy of our method but also demonstrate its adaptability to different initial conditions, thereby validating its potential for broad adeptness on diverse vocab initialization.

  
**Metric** & **BPE** & **BytePiece** & **+ADAT(Ours)** & **Unigram** & **+ADAT(Ours)** \\  PPL & 22.31 & 71.5 & 67.19(-4.31) & 16.52 & **6.97(-9.55)** \\  ARC-C & \(17.32 1.11\) & \(18.69 1.14\) & \(18.94 1.15\) & **19.54\( 1.16\)** & \(18.46 1.12\) \\ ARC-E & \(37.58 0.99\) & \(33.80 0.97\) & \(33.71 0.97\) & \(37.04 0.99\) & **40.57\( 0.99\)** \\ Boolq & \(61.28 0.85\) & \(42.12 0.87\) & \( 0.85\) & \(53.06 0.87\) & \(61.19 0.85\) \\ Lambda & \(10.89 0.43\) & \(8.80 0.39\) & \(13.55 0.48\) & \(17.27 0.53\) & **17.97\( 0.52\)** \\ LogiQA & \(23.04 1.65\) & \(20.28 1.58\) & \(22.27 1.63\) & \(23.20 1.66\) & \( 1.70\) \\ PIQA & \(59.25 1.15\) & \(57.83 1.15\) & \(56.96 1.16\) & \( 1.14\) & \(59.93 1.14\) \\ SciQ & \(66.00 1.49\) & \(54.01 1.58\) & \(51.90 1.58\) & \(68.10 1.47\) & \( 1.44\) \\ SST-2 & \(51.26 1.69\) & \(49.08 1.69\) & \(50.23 1.69\) & \(49.77 1.69\) & \( 1.69\) \\ Winogrande & \(49.96 1.41\) & \(50.31 1.41\) & \(49.41 1.41\) & \(51.46 1.40\) & \( 1.40\) \\  Avg. (\%) & \(41.91\) & \(37.21\) & \(39.91(+2.70)\) & \(42.22\) & \((+)\) \\   

Table 1: Performance Comparison of Different Tokenization Methods.

### Scalability

We examine the scalability of a proposed tokenization method that tailors the vocabulary to model size, unlike the static Unigram method which maintains a consistent vocabulary across various model capacities. The scalability of the tokenization methods is tested using the Pythia framework configured at three different levels of computational complexity: 70M, 160M, and 410M parameters. For each model size, our method generates an optimized vocabulary specific to that configuration, allowing us to analyze how adjustments in vocabulary affect performance as model size increases. In contrast, the Unigram method employs a uniform 50,000-word vocabulary across all sizes, serving as a baseline. We gauge performance using Perplexity (PPL) and scores from benchmark datasets designed to assess the linguistic capabilities of each model under various conditions, providing insights into the efficiency and adaptability of the tokenization methods at scale. For training larger models, the same volume of data will lead to insufficient warm-up, potentially resulting in a slight decline in the accuracy of loss computations used for determining token priority. As a result, we increase the data volume for training the loss calculation model according to the size of model.

The results of this experimental framework, as presented in Table 2, indicate substantial performance variations across different model sizes employing varied tokenization strategies. Specifically, average performance scores across all evaluated metrics demonstrate consistent improvements with increases in model sizes: ADAT achieves a score of 44.51 in the 70M model, significantly surpassing Unigram's 42.22; 46.05 compared to 44.04 in the 160M model; and 48.32 versus 46.50 in the 410M model. These findings highlight the superior efficacy of ADAT in managing diverse model volumes compared to the more static approach of Unigram, which exhibits limited scalability with increasing model size. Remarkably, the performance of the 70M model using ADAT exceeded that of the Unigram on the 160M model by nearly 5%, illustrating the substantial enhancement and ability of our method to bridge a parameter gap of over double. Furthermore, the performance of our 160M model approaches that of the 410M model, emphasizing the robust adaptability of the ADAT method across varying computational scales.

### Cross-Model Adaptability

This experiment evaluates the adaptability of vocabularies generated by our proposed tokenization method across various configurations of the Pythia model, particularly assessing whether vocabularies optimized for one model size can effectively scale to others. We initially create vocabularies using the 70M and 410M configurations. These are then used to train models at both scales to evaluate performance in downstream tasks, allowing us to assess how vocabularies designed for a specific

  
**Metric** &  &  &  \\  & **Unigram** & **ADAT** & **Unigram** & **ADAT** & **Unigram** & **ADAT** \\  PPL & 16.52 & **6.97(-9.55)** & 13.97 & **6.19(-7.78)** & 10.92 & **5.78(-5.14)** \\  ARC-C & \( 1.16\) & \(18.46 1.12\) & \(18.69 1.14\) & \( 1.15\) & \( 1.19\) & \(19.29 1.21\) \\ ARC-E & \(37.04 0.99\) & \( 0.99\) & \(39.52 1.00\) & \( 1.01\) & \(44.65 1.02\) & \( 1.03\) \\ Boola & \(53.06 0.87\) & \( 0.85\) & \( 0.86\) & \(57.68 0.86\) & \(54.80 0.87\) & \( 0.87\) \\ Lambda & \(17.27 0.53\) & \( 0.52\) & \(19.06 0.55\) & \( 0.60\) & \(27.81 0.62\) & \( 0.66\) \\ LogiQA & \(23.20 1.66\) & \( 1.70\) & \( 1.71\) & \(25.04 1.65\) & \(23.20 1.66\) & \( 1.68\) \\ PIQA & \( 1.14\) & \(59.93 1.14\) & \(60.83 1.14\) & \( 1.14\) & \(63.38 1.12\) & \( 1.11\) \\ SciQ & \(68.10 1.47\) & \( 1.44\) & \(72.10 1.42\) & \( 1.28\) & \(80.70 1.25\) & \( 1.14\) \\ SST-2 & \(49.77 1.69\) & \( 1.69\) & \(52.06 1.69\) & \( 1.69\) & \(50.69 1.69\) & \( 1.69\) \\ Winogrande & \(51.46 1.40\) & \( 1.40\) & \(49.88 1.41\) & \( 1.41\) & \(52.41 1.40\) & \( 1.41\) \\ 
**Avg** & 42.22 & **44.51** & 44.04 & **46.05** & 46.50 & **48.42** \\   

Table 2: Evaluation on Different Scale Model Size.

  
**Model Size** & **Unigram** & **70M-Model Vocabulary** & **410M-Model Vocabulary** \\ 
70M & 42.22 & 44.51 & 42.62 \\
160M & 44.04 & 45.03 & 45.83 \\
410M & 46.50 & 47.66 & 48.42 \\   

Table 3: Cross-Model Adaptability of Vocabularies.

size perform when applied to both smaller and larger models, thus examining their cross-model adaptability.

Table 3 illustrates the cross-model adaptability of vocabularies across different model sizes. By applying vocabularies derived from different model sizes to various models, we observe that vocabularies generated by the 70M and 410M models surpass the performance of the standard Unigram model. This indicates the adaptability of the ADAT vocabularies across different model sizes. Furthermore, we note that the vocabulary from the 410M model achieves only a marginal improvement of 0.4 when applied to the 70M model, significantly less than the 2.29 increase afforded by the 70M model's vocabulary. This suggests that the vocabularies selected by ADAT possess a strong capacity for targeted optimization, enabling the selection of tokenization strategies that are specifically tailored to the characteristics of different models.

### Model and Vocabulary Size

The experiment aims to assess the impact of different tokenizer strategies on model performance across two vocabulary sizes, comparing a standard 50,000-token set with a reduced 30,000-token set. We utilize two configurations of the Pythia model--70M and 160M--to explore how vocabulary size influences model efficiency. Each model is tested using both the standard Unigram and our proposed tokenization method. This setup allows us to directly observe the effects of reduced vocabulary sizes on the performance dynamics, providing insights into how smaller vocabularies impact the computational efficiency and efficacy of language models.

The experimental results, as presented in Table 4, support the hypothesis that changes in vocabulary size can significantly affect model performance, with different impacts observed across varying model sizes. For large language models, it is common for models of vastly different sizes to utilize vocabularies of similar or identical sizes . This practice can lead to issues of performance or efficiency. Our method offers a more effective solution by tailoring tokenization strategies to the specific sizes of models, thereby mitigating these challenges. For the 70M model, ADAT achieved a notable improvement in accuracy from 42.22 to 44.51 (+2.29) and a substantial reduction in perplexity from 16.52 to 6.97 when using a 50,000-word vocabulary. Even with a reduced vocabulary of 30,000, ADAT enhances accuracy to 43.33 (+2.40) and decreases perplexity to 7.38, suggesting robustness against vocabulary size reduction. In contrast, the 160M model, which has a greater parameter capacity, also shows improvements with ADAT: accuracy increases from 44.04 to 46.05 (+2.01), and perplexity drops sharply from 13.97 to 6.19 for the 50,000 vocabulary size. With a 30,000 vocabulary, accuracy still increases to 45.26 (+2.14), and perplexity remains low at 6.11, underscoring that larger models not only handle vocabulary reductions well but also benefit significantly in terms of computational efficiency and model quality.

### Ablation Study

This ablation study is structured into three distinct parts to explore how variations in the inference corpus size used for calculating token loss, initial vocabulary sizes, momentum strategy, and balance function \(F(a,b)\) influence the efficacy of our proposed tokenization method on a 70M parameter model. More ablation results can be referred to in the supplementary.

  
**Model Size** & **Vocabulary Size** & **Tokenization Method** & **Accuracy** & **Perplexity (PPL)** \\   &  & **Unigram** & 42.22 & 16.52 \\  & & **ADAT** & 44.51(+2.29) & 6.97 \\   &  & **Unigram** & 40.93 & 32.53 \\  & & **ADAT** & 43.33(+2.40) & 7.38 \\   &  & **Unigram** & 44.04 & 13.97 \\  & & **ADAT** & 46.05(+2.01) & 6.19 \\    &  & **Unigram** & 43.08 & 15.21 \\   & & **ADAT** & 45.26(+2.14) & 6.11 \\   

Table 4: Impact of Vocabulary Size on Model Performance Across Different Model Sizes.

#### 4.6.1 Corpus Size used in Loss Calculation

We conducted an experimental study to investigate the effects of varying corpus sizes on the accuracy of token loss calculations. The experiment assessed the performance of models trained on different sizes of inference data, specifically 1 million (1M), 10 million (10M), and 100 million (100M) tokens. The results are summarized in the table 5.

These results indicate a direct correlation between the volume of the corpus used during the loss calculation phase and the overall accuracy of token loss estimates. When smaller corpora are used, a significant number of tokens are absent, resulting in numerous instances where loss values cannot be computed. Furthermore, the precision of token loss estimations tends to decrease with smaller data sets. Even with just 1M tokens, there was a noticeable improvement over the baseline unigram vocabulary accuracy of 42.22. This enhancement became more pronounced with larger data volumes, reaching an increase of 44.51 in accuracy with 100M tokens.

#### 4.6.2 Initial Vocabulary Size

This segment of our study assesses the effect of different initial vocabulary sizes on model performance. Adjusting the vocabulary from a baseline of 150,000 tokens to either 100,000 or 75,000 tokens, we explore the influence of vocabulary scale on training outcomes. The results, detailed in Table 5, illustrate the trade-offs associated with varying vocabulary sizes.

From the experiment, it is evident that models equipped with a larger initial vocabulary of 150,000 tokens tend to achieve lower Perplexity and higher Accuracy, indicating a robust ability to capture diverse linguistic nuances that significantly enhance performance. In contrast, reducing the vocabulary size to 75,000 tokens results in increased perplexity and decreased accuracy, highlighting a potential compromise in linguistic detail that adversely affects model functionality, especially in complex linguistic scenarios.

#### 4.6.3 Momentum Strategy

This experiment evaluates the impact of incorporating a momentum strategy into our tokenization algorithm's vocabulary pruning process. The performance of vocabularies pruned under both the momentum and non-momentum conditions is directly compared in Table 5.

The results in Table 5 demonstrate that the momentum approach significantly enhances model accuracy, with a notable improvement from 43.16% to 44.51% in the ADAT method with Unigram initialization vocabulary when momentum is applied. Similarly, the Unigram method shows a baseline performance of 42.20% accuracy. These results confirm that integrating momentum allows for a more refined pruning process by effectively utilizing historical performance data to make more informed decisions, thereby preserving valuable linguistic features.

#### 4.6.4 Balance Strategy

In this ablation study, we investigate various functions to balance token frequency and loss value in our tokenization algorithm. The primary objective is to adhere to the principle that tokens with higher frequency and lower loss should be assigned higher priority. Given the significant difference in their magnitudes, we explored subtraction and division methods. We evaluated three functions, detailed in Table 5. Here, \(\) is a scaling factor introduced to adjust the balance between frequency and loss, and we set it as 1 in practice.

The results demonstrate that the subtraction methods \(a- b\) and \(a-(b)\) yielded accuracies of 42.70 and 43.23 respectively. These results indicate relatively poor performance, even with

  
**Infer Data Volume** &  &  &  \\
**Tokens** & **Acc.** & **Init Size** & **Acc.** & **Methods** & **Acc.** & **Methods** & **Acc.** \\ 
1M & 43.13 & 75k & 43.42 & Unigram & 42.20 & \(a- b\) & 42.70 \\
10M & 43.74 & 100k & 43.78 & ADAT+By & 44.51 & \(log(a)- b\) & 43.23 \\
100M & 44.51 & 150k & 44.51 & -w/o Mnt. & 43.16 & \(a/(b+1)\) & 44.51 \\   

Table 5: Ablation Studies Results.

the logarithmic transformation applied to the score \((a)- b\). This underperformance is likely attributable to the significant disparity in the magnitudes of frequency and loss values, which the subtraction methods struggle to reconcile effectively. In contrast, the division method \(\) significantly outperformed the subtraction approaches with an accuracy of 44.51. This superior performance suggests that the division method more naturally balances the influence of frequency and loss by scaling the loss logarithmically before the division, thereby mitigating the impact of numerical range discrepancies. This method's ability to integrate frequency and loss without requiring additional adjustments for scale disparities results in more stable and effective prioritization of tokens.

### Analysis of the Compute Costs

To prove that the proposed method is feasible in practice. We analyzed the empirical runtime introduced by ADAT. To measure runtime, we used 8 NVIDIA A100 GPUs, an Intel 8378A CPU, and PyTorch 2.1.2 with CUDA 12.1. The tokenizer optimization involves 5 epochs, where each epoch consists of training the LLM on a 0.3B corpus, followed by inference on a 0.1B corpus, and concludes with a vocabulary pruning step (90 seconds for a 100K tokens vocabulary). Therefore, the total computational cost of the tokenizer optimization process is calculated as:

\[5(0.3B+0.1B+)=1.5B+0.5B+450s.\]

ADAT introduces an additional training cost of 1.5B tokens and an inference cost of 0.5B tokens, along with minimal vocabulary pruning time. Compared to the hundreds of billions or even trillions of tokens required for LLM training, these computational costs are negligible. As shown in Table 6, the full-scale training of the LLM incurs significantly higher computational costs. For instance, when training models with a 16B and 60B corpus, the tokenizer optimization accounts for only 4.17% and 1.04% of the total training time, respectively. The Pythia-70M model takes 510 GPU hours to train with the full Pile dataset , and exceeds the tokenizer optimization's computational cost by over 255 times. Therefore, the additional computational cost introduced by our method is minimal, making it feasible in practice.

## 5 Limitations

The adaptive nature of our proposed tokenizer method introduces variations in tokenizers across different model sizes, leading to inconsistent vocabularies. This inconsistency complicates tasks such as knowledge distillation and speculative decoding, which rely on the assumption of a uniform vocabulary across both smaller and larger models.

## 6 Conclusion

In this paper, we have presented a novel approach to tokenizer design that integrates key aspects of both accuracy and inference speed, addressing the inherent limitations found in existing tokenizer schemes. By innovating beyond the traditional frequency-based and end-to-end methodologies, our adaptive tokenizer framework strategically optimizes vocabulary construction, ensuring both rapid processing and high precision in language modeling tasks. Our results demonstrate that the adaptive tokenizer significantly enhances the performance of Large Language Models (LLMs) across various benchmarks, providing a balanced solution that does not sacrifice speed for accuracy or vice versa. Future work will focus on refining these adaptive tokenization techniques, exploring further integration with neural network architectures, and expanding their applicability to a broader range of languages and complex linguistic tasks.

    & **Tokenizer Optimization** & **Training on 16B** & **Training on 60B** & **Pythia Report** \\  Runtime & 2 GPU hours & 48 GPU hours & 192 GPU hours & 510 GPU hours \\   

Table 6: Runtime of ADAT optimization and training models.