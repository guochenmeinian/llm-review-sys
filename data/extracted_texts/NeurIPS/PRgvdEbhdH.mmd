# Policy Space Diversity for Non-Transitive Games

Jian Yao\({}^{1}\), Weiming Liu \({}^{1}\), Haobo Fu\({}^{1}\), Yaodong Yang\({}^{2}\),

**Stephen McAleer\({}^{3}\)**, **Qiang Fu\({}^{1}\)**, **Wei Yang\({}^{1}\)**

\({}^{1}\)Tencent AI Lab, Shenzhen, China

\({}^{2}\)Peking University, Beijing, China

\({}^{3}\)Carnegie Mellon University

Equal contribution. Correspondence to: Haobo Fu (haobofu@tencent.com).

###### Abstract

Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games. Many previous studies have been trying to promote policy diversity in PSRO. A major weakness in existing diversity metrics is that a more diverse (according to their diversity metrics) population does not necessarily mean (as we proved in the paper) a better approximation to a NE. To alleviate this problem, we propose a new diversity metric, the improvement of which guarantees a better approximation to a NE. Meanwhile, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, _Policy Space Diversity_ PSRO (PSD-PSRO). We present the convergence property of PSD-PSRO. Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in producing significantly less exploitable policies than state-of-the-art PSRO variants. The experiment code is available at https://github.com/nigelyaoj/policy-space-diversity-psro.

## 1 Introduction

Most real-world games demonstrate strong non-transitivity , where the winning rule follows a cyclic pattern (e.g., the strategy cycle in Rock-Paper-Scissors) [6; 3]. A common objective in solving non-transitive games is to find a Nash Equilibrium (NE), which has the best worst-case performance in the whole policy space. Traditional algorithms, like simple self-play, fail to converge to a NE in games with strong non-transitivity . Recently, many game-theoretic methods have been proposed to approximate a NE in such games. For example, Counterfactual Regret Minimization (CFR)  minimizes the so-called counterfactual regret. Neural fictitious self play [18; 19] extends the classical game-theoretic approach, Fictitious Play (FP) , to larger games using Reinforcement Learning (RL) to approximate a Best Response (BR). Another well-known algorithm is Policy-Space Response Oracles (PSRO) , which generalizes the double oracle approach  by adopting a RL subroutine to approximate a BR.

Improving the performance of PSRO on approximating a NE is an active research topic, and many PSRO variants have been proposed so far, which generally fall into three categories. The first category [36; 45; 12; 29] aims to improve the training efficiency at each iteration. For instance, pipeline-PSRO  trains multiple BRs in parallel at each iteration. Neural population learning  enables fast transfer learning across policies via representing a population of policies within a single conditional model. The second category [37; 53] incorporates no-regret learning into PSRO, which solves an unrestricted-restricted game with a no-regret learning method to guarantee the decrease of _exploitability_ of the meta-strategy across each iteration. The third category [2; 41; 32; 33]promotes policy diversity in the population, which is usually implemented by incorporating a diversity regularization term into the BR solving in the original PSRO.

Despite achieving promising improvements over the original PSRO, the theoretical reason why the diversity metrics in existing diversity-enhancing PSRO variants [2; 41; 32; 33] help PSRO in terms of approximating a NE is unclear. More specifically, those diversity metrics are 'justified' in the sense that adding the corresponding diversity-regularized BR strictly enlarges the _gamescape_. However, as we prove and demonstrate later in the paper, a population with a larger _gamescape_ is neither a sufficient nor a necessary condition for a better approximation (we will define the precise meaning later) to a NE. One fundamental reason is that _gamescape_ is a concept that varies significantly according to the choice of opponent policies. In contrast, _exploitability_ (the distance to a NE) measures the worst-case performance that is invariant to the choice of opponent policies.

In this paper, we seek for a new and better justified diversity metric that improves the approximation of a NE in PSRO. To achieve this, we introduce a new concept, named _Population Exploitability_ (PE), to quantify the strength of a population. The PE of a population is the optimal _exploitability_ that can be achieved by selecting a policy from its _Policy Hull_ (PH), which is simply a complete set of polices that are convex combinations of individual polices in the population. In addition, we show that a larger PH means a lower PE. Based on these insights, we make the following contributions:

* We point out a major and common weakness of existing diversity-enhancing PSRO variants: their goal of enlarging the _gamescape_ of the population in PSRO is somewhat deceptive to the extent that it can lead to a weaker population in terms of PE. In other words, a more diverse (according to their diversity metrics) population \(\) a larger _gamescape_\(\) closer to a full game NE.
* We develop a new diversity metric that encourages the enlargement of a population's PH. In addition, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. We then incorporate our diversity metric (as a regularization term) into the BR solving in the original PSRO and obtain a new algorithm: Policy Space Diversity PSRO (PSD-PSRO). Our method PSD-PSRO establishes the causality: a more diverse (according to our diversity metric) population \(\) a larger PH \(\) a lower PE \(\) closer to a full game NE.
* We prove that a full game NE is guaranteed once PSD-PSRO is converged. In contrast, it is not clear, in other state-of-the-art diversity-enhancing PSRO variants [2; 41; 32; 33], whether a full game NE is found once they are converged in terms of their optimization objectives. Notably, PSRO\({}_{rN}\) is not guaranteed to find a NE once converged .

## 2 Notations and Preliminary

### Extensive-form Games, NE, and Exploitability

Extensive-form games are used to model sequential interaction involving multiple agents, which can be defined by a tuple \(,,P,,u\). \(=\{1,2\}\) denotes the set of players (we focus on the two-player zero-sum games). \(\) is a set of information states for decision-making. Each information state node \(s\) includes a set of actions \((s)\) that lead to subsequent information states. The player function \(P:\{c\}\), with \(c\) denoting chance, determines which player takes action in \(s\). We use \(s_{i}\), \(_{i}=\{s|P(s)=i\}\), and \(_{i}=_{s}(s)\) to denote player \(i\)'s state, set of states, and set of actions respectively. We consider games with _perfect recall_, where each player remembers the sequence of states to the current state.

A player's _behavioral strategy_ is denoted by \(_{i}(s)((s)), s_{i}\), and \(_{i}(a|s)\) is the probability of player \(i\) taking action \(a\) in \(s\). A _strategy profile_\(=(_{1},_{2})\) is a pair of strategies for each player, and we use \(_{-i}\) to refer to the strategy in \(\) except \(_{i}\). \(u_{i}()=u_{i}(_{i},_{-i})\) denotes the payoff for player \(i\) when both players follow \(\). The BR of player \(i\) to the opponent's strategy \(_{-i}\) is denoted by \((_{-i})=_{_{i}^{}}u_{i}(_{i}^{},_{ -i})\). The _exploitability_ of strategy profile \(\) is defined as:

\[()=_{i}[_{_{i}^{}}u_{i }(_{i}^{},_{-i})-u_{i}(_{i},_{-i})].\] (1)

When \(()=0\), \(\) is a NE of the game.

### Meta-Games, PH, and PSRO

Meta-games are introduced to represent games at a higher level. Denoting a population of mixed strategies for player \(i\) by \(_{i}:=\{_{i}^{1},_{i}^{2},...\}\), the payoff matrix on the joint population \(=_{i}_{-i}\) is denoted by \(_{_{i},_{-i}}\), where \(_{_{i},_{-i}}[j,k]:=u_{i}(_{i}^{j},_{-i}^{k})\). The meta-game on \(\) and \(_{_{i},_{-i}}\) is simply a normal-form game where selecting an action means choosing which \(_{i}\) to play for player \(i\). Accordingly, we use \(_{i}\) (\(_{i}\) is called a meta-strategy and could be, e.g., playing \([_{i}^{1},_{i}^{2}]\) with probability \([0.5,0.5]\)) to denote a mixed strategy over \(_{i}\), i.e., \(_{i}_{_{i}}\). A meta-policy \(_{i}\) over \(_{i}\) can be viewed as a convex combination of polices in \(_{i}\), and we define the PH of a population \((_{i})\) as the set of all convex combinations of the policies in \(_{i}\). Meta-games are often open-ended in the sense that there exist an infinite number of mixed strategies and that new policies will be successively added to \(_{i}\) and \(_{-i}\) respectively. We give a summary of notations in Appendix A.

PSRO operates on meta-games and consists of two components: an oracle and a meta-policy solver. At each iteration \(t\), PSRO maintains a population of policies, denoted by \(_{i}^{t}\), for each player \(i\). The joint meta-policy solver first computes a NE meta-policy \(^{t}\) on the restricted meta-game represented by \(_{_{i}^{t},_{-i}^{t}}\). Afterwards, for each player \(i\), the oracle computes an approximate BR (i.e., \(_{i}^{t+1}\)) against the meta-policy \(_{-i}^{t}\): \(_{i}^{t+1}(_{-i}^{t})\). The new policy \(_{i}^{t+1}\) is then added to its population (\(_{i}^{t+1}=_{i}^{t}\{_{i}^{t+1}\}\)), and the next iteration starts. In the end, PSRO outputs a meta-policy NE on the final joint population as an approximation to a full game NE.

### Previous Diversity Metrics for PSRO

**Effective diversity** measures the variety of effective strategies (strategies with support under a meta-policy NE) and uses a rectifier to focus on how these effective strategies beat each other. Let \((_{i}^{*},_{-i}^{*})\) denote a meta-policy NE on \(_{_{i},_{-i}}\). The _effective diversity_ of \(_{i}\) is:

\[(_{i})={_{i}^{*}}^{T}[_{_{i},_{-i}}]_{+} _{-i}^{*},\] (2)

where \( x_{+}:=x\) if \( 0\;\;0\).

**Expected Cardinality**, inspired by the determinantal point processes , measures the diversity of a population \(_{i}\) as the expected cardinality of the random set \(\) sampled according to \(det(_{})\):

\[(_{i})=_{_{_{}} }[||]=(-(_{}+)^{-1}),\] (3)

where \(||\) is the cardinality of \(\), and \(_{}=_{_{i},_{-i}}^{T}_{_{i},_{-i}}\).

**Convex Hull Enlargement** builds on the idea of enlarging the convex hull of all row vectors in the payoff matrix:

\[(_{i}\{_{i}^{}\})=_{^{T}=1,  0}||_{_{i},_{-i}}^{T}-||,\] (4)

where \(_{i}^{}\) is the new strategy to add, and \(\) is the payoff vector of policy \(_{i}^{}\) against each opponent policy in \(_{-i}\): \([j]=u_{i}(_{i}^{},_{-i}^{j})\).

**Occupancy Measure Mismatching** considers the state-action distribution \(_{}(s,a)\) induced by a joint policy \(\). When considering adding a new policy \(_{i}^{}\), the corresponding diversity metric is:

\[(_{i}\{_{i}^{}\})=D_{f}(_{(_{i}^{}, _{-i}^{*})}||_{(_{i}^{*},_{-i}^{*})}),\] (5)

where \(_{i}^{}\) is the new policy to add; \((_{i}^{*},_{-i}^{*})\) is a meta-policy NE on \(_{_{i},_{-i}}\), and \(D_{f}\) is a general \(f\)-divergence between two distributions. It is worth noting that Equation 5 only considers the difference between two policies (\(_{i}^{}\) and \(_{i}^{*}\)), instead of \(_{i}^{}\) and \(_{i}\). In practice, this diversity metric is used together with the **convex hull enlargement** in .

**Unified Diversity Measure** offers a unified view on existing diversity metrics and is defined as:

\[(_{i})=_{m=1}^{|_{i}|}f(_{m}),\] (6)

where \(f\) takes different forms for different existing diversity metrics; \(_{m}\) is the eigenvalues of \([K(_{m},_{n})]_{|_{i}||_{i}|};K(,)\) is a predefined kernel function; and \(_{m}\) is the strategy feature for the \(m\)-th policy in \(_{i}\). It is worth mentioning that only payoff vectors in \(_{_{i},_{-i}}\) were investigated for the strategy feature of the new diversity metric proposed in .

A Common Weakness of Existing Diversity-Enhancing PSRO Variants

As shown in last section, all previous diversity-enhancing PSRO variants [2; 41; 32; 33] try to enlarge the _gamescape_ of \(_{i}\), which is the convex hull of the rows in the empirical payoff matrix:

\[(_{i}|_{-i}):=\{_{j}_{j}_{j}: {} 0,^{T}=1\},\]

where \(_{j}\) is the \(j\)-th row vector in \(_{_{i},_{-i}}\). However, the _gamescape_ of a population depends on the choice of opponent policies, and two policies with the same payoff vector are not necessarily the same. Moreover, enlarging the _gamescape_ without careful tuning would encourage the current player to deliberately lose to the opponent to get 'diverse' payoffs. We suspect this might be the reason why the optimization of the _gamescape_ is activated later in the training procedure in . More importantly, it is not theoretically clear from previous diversity-enhancing PSRO variants why enlarging the _gamescape_ would help in approximating a full game NE in PSRO.

To rigorously answer the question whether a diversity metric is helpful in approximating a NE in PSRO, we need a performance measure to monitor the progress of PSRO across iterations in terms of finding a full game NE. In other words, we need to quantify the strength of a population of policies. Previously, the _exploitability_ of a meta NE of the joint population is usually employed to monitor the progress of PSRO. Yet, as demonstrated in , this _exploitability_ may increase after an iteration. Intuitively, a better alternative is the _exploitability_ of the least exploitable mixed strategy supported by a population. We define this _exploitability_ as the _population exploitability_:

**Definition 3.1**.: For a joint population \(=_{i}_{-i}\), let \((_{i}^{*},_{-i}^{*})\) be a meta NE on \(_{_{i},_{-i}}\). The _relative population performance_ of \(_{i}\) against \(_{-i}\) is:

\[_{i}(_{i},_{-i})={_{i}^{*}}^{T}_{_{i}, _{-i}}_{-i}^{*}.\] (7)

The _population exploitability_ of the joint population \(\) is defined as:

\[()=_{i=1,2}_{_{i}^{}_ {i}}_{i}(_{i}^{},_{-i}),\] (8)

where \(_{i}\) is the full set of all possible mixed strategies of player \(i\).

We notice that PE is equal to the sum of negative _population effectivity_ defined in . Yet, we prefer PE as it is more of a natural extension to _exploitability_ in Equation 1. Some properties of PE and its relation to PH are presented in the following.

**Proposition 3.2**.: _Considering a joint population \(=_{i}_{-i}\), we have:_

1. \(() 0\)_,_ \(\,\)_._
2. _For another joint population_ \(^{}\)_, if_ \((_{i})(_{2})(_{i}^{ })(_{-i}^{})\)_, then_ \(()(^{})\)_._
3. _If_ \(_{i}=\{_{i}\}\) _and_ \(_{-i}=\{_{-i}\}\)_, then_ \(()=()\)_, where_ \(=(_{i},_{-i})\)_._
4. \(=(_{i},_{-i})(_{i})(_{-i})\) _s.t._ \(()=()=_{^{}(_{i}) (_{-i})}(^{})\)_._
5. _Let_ \((_{i}^{*},_{-i}^{*})\) _denote an arbitrary NE of the full game._ \(()=0\) _if and only if_ \((_{i}^{*},_{-i}^{*})(_{i})(_{ -i})\)_._

The proof is in Appendix B.2. Once we use PE to monitor the progress of PSRO, we have:

**Proposition 3.3**.: _The PE of the joint population \(^{t}\) at each iteration \(t\) in PSRO is monotonically decreasing and will converge to \(0\) in finite iterations for finite games. Once \((^{T})=0\), a meta NE on \(^{T}\) is a full game NE._

The proof is in Appendix B.3. From Proposition 3.2 and 3.3, we are convinced that PE is indeed an appropriate performance measure for populations of polices. Using PE, we can now formally present why enlarging the _gamescape_ of the population in PSRO is somewhat deceptive:

**Theorem 3.4**.: _The enlargement of the gamescape is neither sufficient nor necessary for the decrease of PE. Considering two populations (\(_{i}^{1}\) and \(_{i}^{2}\)) for player \(i\) and one population \(_{-i}\) for player \(-i\), and denoting \(^{j}=_{i}^{j}_{-i}\), \(j=1,2\) we have_

\[(_{i}^{1}|_{-i})(_{i}^{2}|_{ -i})(^{1})(^{2})\] \[(^{1})(^{2}) (_{i}^{1}|_{-i})(_{i}^{2}|_{ -i})\] (9)The proof of Theorem 3.4 is in Appendix B.5, where we provide concrete examples.

In other words, enlarging the gamescape in either short term or long term does not necessarily lead to a better approximation to a full game NE.

## 4 Policy Space Diversity PSRO

In this section, we develop a new diversity-enhancing PSRO variant, i.e., PSD-PSRO. In contrast to methods that enlarge the _gamescape_, PSD-PSRO encourages the enlargement of PH of a population, which helps reduce a population's PE (according to Proposition 3.2). In addition, we develop a well-justified state-action sampling method to optimize our diversity metric in practice. Finally, we present the convergence property of PSD-PSRO and discuss its relation to the original PSRO.

### A New Diversity Regularization Term for PSRO

Our purpose of promoting diversity in PSRO is to facilitate the convergence to a full game NE. We follow the conventional scheme in previous diversity-enhancing PSRO variants [41; 32; 33], which introduces a diversity regularization term to the BR solving in PSRO. Nonetheless, our diversity regularization encourages the enlargement of PH of the current population, which is in contrast to the enlargement of _gamescape_ in previous methods [2; 41; 32; 33]. We thus name our diversity metric _policy space diversity_. Intuitively, the larger the PH of a population is, the more likely it will include a full game NE. More formally, a larger PH means a lower PE (Proposition 3.2), which means our diversity metric avoids the common weakness (Section 3) of existing ones.

Recall that the PH of a population is simply the complete set of polices that are convex combinations of individual polices in the population. To quantify the contribution of a new policy to the enlargement of the PH of the current population, a straightforward idea is to maximize a distance between the new policy and the PH. Such a distance should be \(0\) for any policy that belongs to the PH and greater than \(0\) otherwise. Without loss of generality, the distance between a policy and a PH could be defined as the minimal distance between the policy and any policy in the PH. We can now write down the diversity regularized BR solving objective in PSD-PSRO, where at each iteration \(t\) for player \(i\) we add a new policy \(_{i}^{t+1}\) by solving:

\[_{i}^{t+1}=_{_{i}}\{u(_{i},_{-i}^{t})+ _{_{i}^{k}(_{i}^{t})}(_{i},_{i}^{k}) \},\] (10)

where \(_{-i}^{t}\) is the opponent's meta NE policy at the \(t\)-th iteration, \(\) is a Lagrange multiplier, and \((,)\) is a distance function (will be specified in the next subsection) between two polices.

### Diversity Optimization in Practice

To be able to optimize our diversity metric (the right part in Equation 10) in practice, we need to encode a policy into some representation space and specify a distance function there. Such a representation should be a one-to-one mapping between a policy and its representation. Also, to ensure that enlarging the convex hull in the representation space results in the enlargement of the PH, we require the representation to satisfy the linearity property. Formally, we have the following definition:

**Definition 4.1**.: A _fine policy representation_ for our purpose is a function \(:_{i} R^{N_{i}}\), which satisfies the following two properties:

* (bijection) For any representation \((_{i})\), there exists a unique behavior policy \(_{i}\) whose representation is \((_{i})\), and vice-versa.
* (linearity) For any two policies (\(_{i}^{j}\) and \(_{i}^{k}\)) and \(\) (\(0 1\)), the following holds: \[(_{i}^{j}+(1-)_{i}^{k})=(_{i}^{j})+(1- )(_{i}^{k}),\] where \(_{i}^{j}+(1-)_{i}^{k}\) means playing \(_{i}^{j}\) with probability \(\) and \(_{i}^{k}\) with probability \((1-)\).

Existing diversity metrics explicitly or implicitly define a policy representation . For instance, the _gamescape_-based methods  represent a policy using its payoff vector against the opponent's population. Yet, this representation is not a _fine policy representation_ as it is not a bijection (different policies can have the same payoff vector). The (joint) occupancy measure, which is a _fine policy representation_, is usually used to encode a policy in the RL community .The \(f\)-divergence is then employed to measure the distance between two policies . However, computing the \(f\)-divergence based on the occupancy measure is usually intractable and often in practice roughly approximated using the prediction of neural networks .

Instead, we use another _fine policy representation_, i.e., the sequence-form representation , which was originally developed for representing a policy in multi-agent games. We then define the distance between two policies using the Bregman divergence, which can be further simplified to a tractable form and optimized using only state-action samples in practice.

The sequence-form representation of a policy remembers the realization probability of reaching a state-action pair. We follow the definition in , where the sequence form representation \(_{i}_{i}^{|_{i}_{i}|}\) of \(_{i}\) is a vector:

\[_{i}(s,a)=_{_{i},(s,a)}_{i}( |_{i}),\] (11)

where \((s,a)\) is a trajectory from the beginning to \((s,a)\). By the perfect-recall assumption, there is a unique \(\) that leads to \((s,a)\). The policy \(_{i}\) can be written as \(_{i}(a|s)=_{i}(s,a)/\|_{i}(s)\|_{1}\), where \(_{i}(s)\) is \((_{i}(s,a_{1}),,_{i}(s,a_{n}))\) with \(a_{1},,a_{n}(s)\). Unlike the payoff vector representation or the occupancy measure representation, \(_{i}\) is independent of the opponent's policy as well as the environmental dynamics. Therefore, it should be more appropriate in representing a policy for the diversity optimization. Without loss of generality and following , we define the distance \((_{i},_{i}^{})\) between two policies as the Bregman divergence on the sequence form representation \(_{d}(_{i}\|_{i}^{})\), which can be further written in terms of state-action pairs (the derivation is presented in Appendix B.1) in the following:

\[(_{i},_{i}^{}):=_{d}(_{i}\| _{i}^{})=_{s,a_{i}_{i}}( _{_{i},(s,a)}_{i}(| _{i}))_{s}_{d_{s}}(_{i}(s)\|_{i}^{ }(s)),\] (12)

where \(_{d_{s}}(_{i}(s)\|_{i}^{}(s))\) is the Bregman divergence between \(_{i}(s)\) and \(_{i}^{}(s)\). In our experiment, we let \(_{d_{s}}(_{i}(s)\|_{i}^{}(s))=_{a}_{i}(a|s) _{i}(a|s)/_{i}^{}(a|s)=(_{i}(s)\|_{i}^{ }(s))\), i.e., the KL divergence. In previous work , the coefficient \(_{s}\) usually declines monotonically as the length of the sequence increases. In our case, we make \(_{s}\) depend on an opponent's policy \(b_{-i}\): \(_{s}=_{_{-,i},(s,a)}b_{-i}( |_{-i})\)2. This weighting method allows us to estimate the distance using the sampled average KL divergence and avoids importance sampling:

\[(_{i},_{i}^{})= _{s,a_{i}_{i}}(_{ _{i},(s,a)}_{i}(|_{i}))(_{_{-,i},(s,a)}b_{-i}( |_{-i}))_{d_{s}}(_{i}(s)\|_{i} ^{}(s))\] \[= _{s_{i}_{i},b_{-i}}[(_{i}(s _{i})\|_{i}^{}(s_{i}))],\] (13)

where \(s_{i}_{i},b_{-i}\) means sampling player \(i\)'s information states from the trajectories that are collected by playing \(_{i}\) against \(b_{-i}\). As a result, we can rewrite the diversity regularized BR solving objective in PSD-PSRO as follows:

\[_{i}^{t+1}=_{_{i}}\{u(_{i},_{-i}^{t})+_{ _{i}^{t}(_{i}^{t})}_{s_{i}_{i},b_{-i}}[ (_{i}(s_{i})\|_{i}^{k}(s_{i}))]\}.\] (14)

Now we provide a practical way to optimize Equation 14. By regarding the opponent and chance as the environment, we can use the policy gradient method  in RL to train \(_{i}^{t+1}\). Denote the probability of generating \(\) by \(_{i}()\), and the payoff of player \(i\) for the trajectory \(\) by \(R()\). Let \(_{i}^{min}=_{_{i}^{k}(_{i}^{t})}_{s_{i }_{i},b_{-i}}[(_{i}(s_{i})\|_{i}^{k}(s_{i}))]\) be the policy in \((_{i}^{t})\) that minimizes the distance and let \(R^{KL}()=_{s_{i}}[(_{i}(s_{i})\|_{i}^ {min}(s_{i}))]\).

The gradient of the first term \(u(_{i},^{t}_{-i})\) in Equation 14 with respect to \(_{i}\) can be written as:

\[ u(_{i},^{t}_{-i})= _{i}()R()=_{i}( )()\] \[= _{i}()_{i}()R()\] \[= _{_{i},^{t}_{-i}}[_{i}( )R()].\] (15)

The gradient of the diversity term in Equation 14 with respect to \(_{i}\) can be written as:

\[_{_{i}}_{_{i}^{k}(^{t}_{i})} _{s_{i},_{i},b_{-i}}[(_{i}(s_{i}) \|_{i}^{k}(s_{i}))]\] \[= _{s_{i},_{i},b_{-i}}[(_{i}(s_{i})\|_{i}^{min}(s_{i}))]\] \[= _{i}()R^{KL}()\] \[= _{i}()R^{KL}()+_{i}( )_{s_{i}}[\,(_{i}(s_{i})\|_{i}^{ min}(s_{i}))]\] \[= _{_{i},b_{-i}}[_{i}()R^{KL} ()]+_{s_{i}_{i},b_{-i}}[\,(_{i}(s_{i} )\|_{i}^{min}(s_{i}))],\] (16)

where we use the property that \([KL(_{i}(s_{i})|_{i}^{min}(s_{i})]_{_{ i}^{k}}[KL(_{i}(s_{i})|_{i}^{k}(s_{i})]\), whose correctness can be shown from the following proposition:

**Proposition 4.2**.: _For any local Lipschitz continuous function \(f(x,y)\), assume \( x,_{y}f(x,y)\) exists, then \(_{x}f(x,y)|_{y f(x,y)}_{x}_{y}f(x,y)\), where \( f\) is the generalized gradient ._

Proof.: According to Theorem 2.1 (property (4) in , the result is immediate, as \(_{x}_{y}f(x,y)\) is the convex hull of \(\{_{x}f(x,y)|y g(x,y)\}\). 

Combining the above two equations, we have,

\[_{_{i}}(u(_{i},^{t}_{-i})+_{ _{i}^{k}(^{t}_{i})}_{s_{i}_{i},b_{-i}}[ (_{i}(s_{i})\|_{i}^{k}(s_{i}))])\] (17) \[= _{_{i},^{t}_{-i}}[_{i}( )R()]+_{_{i},b_{-i}}[_{i}( )R^{KL}()]\] \[+_{s_{i}_{i},b_{-i}}[\, (_{i}(s_{i})\|_{i}^{min}(s_{i}))].\]

According to Equation 17, we can see that optimizing \(_{i}^{t+1}\) requires maximizing two types of rewards: \(R()\) and \(R^{KL}()\). This can be done by averaging the gradients using samples that are generated by playing \(_{i}\) against \(^{t}_{-i}\) and \(b_{-i}\) separately. The last term in Equation 17 is easily estimated by sampling states in the sampled trajectories via playing \(_{i}\) against \(b_{-i}\). For training efficiency, we simply set \(b_{-i}=^{t}_{-i}\) in our experiments, although other settings of \(b_{-i}\) are possible, e.g., the uniform random policy. We also want to emphasize that we optimize \(_{i}^{t+1}\) for each iteration \(t\), and \(^{t}_{-i}\) is fixed during an iteration and thus can be viewed as a part of the environment. Finally, to estimate the distance between \(_{i}\) and \(_{i}^{min}\), we should ideally compute \(_{i}^{min}\) exactly first by solving a convex optimization problem. In practice, we find sampling the policies in \((^{t}_{i})\) and using the minimal sampled distance already gives us satisfactory performance. The pseudo-code of PSD-PSRO is provided in Appendix C.

### The Convergence Property of PSD-PSRO

We first show how the PH evolves in the original PSRO:

**Proposition 4.3**.: _Before the PE of the joint population reaches \(0\), adding a BR to the meta NE policy from last iteration in PSRO will strictly enlarge the PH of the current population._

The proof is in Appendix B.3. From Proposition 4.3, we can see that adding a BR serves one way (an implicit way) of enlarging the PH and hence reducing the PE. In contrast, the optimization of our diversity term in Equation 14 aims to explicitly enlarge the PH. In other words, adding a diversity regularized BR in PSD-PSRO serves as a mixed way of enlarging the PH. More formally, we have the following theorem:

**Theorem 4.4**.: _(1) In PSD-PSRO, before the PE of the joint population reaches \(0\), adding an optimal solution in Equation 14 will strictly enlarge the PH and hence reduce the PE. (2) Once the PH can not be enlarged (i.e., PSD-PSRO converges) by adding an optimal solution in Equation 14, the PE reaches 0, and PSD-PSRO finds a full game NE._

The proof is in Appendix B.6. In terms of convergence property, one significant benefit of PSD-PSRO over other state-of-the-art diversity-enhancing PSRO variants [2; 41; 32; 33] is the convergence of PH guarantees a full game NE in PSD-PSRO. Yet, it is not clear in those papers whether a full game NE is found once the PH of their populations converge. Notably, \(_{rN}\) is not guaranteed to find a NE once converged . In practice, we expect a significant performance improvement of PSD-PSRO over PSRO in approximating a NE. As for different games, there might exist different optimal trade-offs between 'exploitation' (adding a BR) and 'exploration' (optimizing our diversity metric) in enlarging the PH and reducing the PE. In other words, PSD-PSRO generalizes PSRO (a PSD-PSRO instance when \(\) = 0) in ways of enlarging the PH and reducing the PE.

## 5 Related Work

Diversity has been widely studied in evolutionary computation , with a central focus that mimics the natural evolution process. One of the ideas is novelty search , which searches for policies that lead to novel outcomes. By hybridizing novelty search with a fitness objective, quality-diversity  aims for diverse behaviors of good qualities. Despite these methods achieving good empirical results [8; 23], the diversity metric is often hand-crafted for different tasks.

Promoting diversity is also intensively studied in RL. By adding a distance regularization between the current policy and a previous policy, a diversity-driven approach has been proposed for good exploration . Unsupervised learning of diverse policies  has been studied to serve as an effective pretraining mechanism for downstream RL tasks. A diversity metric based on DPP  has been proposed to improve exploration in population-based training. Diverse behaviors were learned in order to improve generalization ability for test environments that are different from training . A diversity-regularized collaborative exploration strategy has been proposed in . Reward randomization  has been employed to discover diverse strategies in multi-agent games. Trajectory diversity has been studied for better zero-shot coordination in a multi-agent environment . Quality-similar diversity has been investigated in .

Diversity also plays a role in game-theoretic methods. Smooth FP  adds a policy entropy term when finding a BR. \(_{rN}\) encourages effective diversity, which considers amplifying the strength over the weakness in a policy. DPP-PSRO  introduces a diversity metric based on DPP and provides a geometric interpretation of behavioral diversity. BD&RD-PSRO  combines the occupancy measure mismatch and the diversity on payoff vectors as a unified diversity metric. UDM  summarizes existing diversity metrics, by providing a unified diversity framework. In both opponent modeling  and opponent-limited subgame solving , diversity has been shown to have a large impact on the performance.

Figure 1: (a): _Exploitability_ of the meta NE. (b): PE of the joint population.

## 6 Experiments

The main purpose of the experiments is to compare PSD-PSRO with existing state-of-the-art PSRO variants in terms of approximating a full game NE. The baseline methods include PSRO , Pipeline-PSRO (P-PSRO) , PSRO\({}_{rN}\), DPP-PSRO , and BD&RD-PSRO . The benchmarks consist of single-state games (AlphaStar888 and non-transitive mixture game) and complex extensive games (Leduc poker and Goofspiel). For AlphaStar888 and Leduc poker, we report the _exploitability_ of the meta NE and the PE of the joint population through the training process. For the non-transitive mixture game, we illustrate the 'diversity' of the population and report the final _exploitability_. For Goofspiel where the exact _exploitability_ is intractable, we report the win rate between the final agents. In addition, we illustrate how the PH evolves for each method using the Disc game  in Appendix D.3, where PSD-PSRO is more effective at enlarging the PH and approximating a NE. An ablation study on \(\) of PSD-PSRO in Appendix D.2 reveals that, for different benchmarks, an optimal trade-off between 'exploitation' and 'exploration' in enlarging the PH to approximate a NE usually happens when \(\) is greater than zero. Error bars or stds in the results are obtained via \(5\) independent runs. In Appendix D.3, we also investigate the time cost of calculating our policy space diversity. More details for the environments and hyper-parameters are given in Appendix E.

**AlphaStar888** is an empirical game generated from the process of solving Starcraft II , which contains a payoff table for \(888\) RL policies. be viewed as a zero-sum symmetric two-player game where there is only one state \(s_{0}\). In \(s_{0}\), there are 888 legal actions. Any mixed strategy is a discrete probability distribution over the 888 actions. Hence, the distance function in Equation 14 for AlphaStar888 reduces to the KL divergence between two 888-dim discrete probability distributions.

In Figure 1, we can see that PSD-PSRO is more effective at reducing both the _exploitability_ and PE than other methods.

**Non-Transitive Mixture Game** consists of \(7\) equally-distanced Gaussian humps on the \(2\)D plane. Each strategy can be represented by a point on the \(2\)D plane, which is equivalent to the weights (the likelihood of that point in each Gaussian distribution) that each player puts on the humps. The optimal strategy is to stay close to the center of the Gaussian humps and explore all the distributions. In Figure 2, we show the exploration trajectories for different methods during training, where PSRO and PSRO\({}_{rN}\) get trapped and fail in this non-transitive game. In contrast, PSD-PSRO tends to find diverse strategies and explore all Gaussians. Also, the _exploitability_ of the meta NE of the final population is significantly lower in PSD-PSRO than others.

Figure 3: (a): _Exploitability_ of the meta NE. (b): PE of the joint population.

Figure 2: Non-Transitive Mixture Game. Exploration trajectories during training. For each method, the final _exploitability_\( 100\) (Exp) is reported at the bottom.

**Leduc Poker** is a simplified poker , where the deck consists of two suits with three cards in each suit. Each player bets one chip as an ante, and a single private card is dealt to each player. Since DPP-PSRO cannot scale to the RL setting  and the code of BD&RD-PSRO for complex games is not available, we compare PSD-PSRO only to P-PSRO, PSRO\({}_{FN}\) and PSRO. As demonstrated in Figure 3, PSD-PSRO is more effective at reducing both the _exploitability_ and PE.

**Goofspiel** is commonly used as a large-scale multi-stage simultaneous move game. Goofspiel features strong non-transitivity, as every pure strategy can be exploited by a simple counter-strategy. We compare PSD-PSRO with PSRO, P-PSRO, and PSRO\({}_{FN}\) on Goofspiel with 5 point cards and 8 point cards settings. In the game with 5 point cards setting, due to the relatively small game size, we can calculate the exact _exploitability_. We report the results in Appendix D.1, in which we see that PSD-PSRO reduces the _exploitability_ more effectively than other methods. In the game with 8 point cards setting, the game size is too large to show exact _exploitability_ for each iteration. In this setting, we provide a comparison among final solutions produced by different methods. We report the win rate between each two methods in Table 1, where we can see that PSD-PSRO consistently beats existing methods with a \(62\%\) win rate on average.

## 7 Conclusions and Limitations

In this paper, we point out a major and common weakness of existing diversity metrics in previous diversity-enhancing PSRO variants, which is their goal of enlarging the _gamescape_ does not necessarily result in a better approximation to a full game NE. Based on the insight that a larger PH means a lower PE (a better approximation to a NE), we develop a new diversity metric (_policy space diversity_) that explicitly encourages the enlargement of a population's PH. We then develop a practical method to optimize our diversity metric using only state-action samples, which is derived based on the Bregman divergence on the sequence form of policies. We incorporate our diversity metric into the BR solving in PSRO to obtain PSD-PSRO. We present the convergence property of PSD-PSRO, and extensive experiments demonstrate that PSD-PSRO is significantly more effective in approximating a NE than state-of-the-art PSRO variants.

The diversity regularization term \(\) in PSD-PSRO plays an important role in balancing the 'exploitation' and 'exploration' in terms of enlarging the PH to approximate a NE. In this paper, other than showing that different problems have different optimal settings of \(\), we have not discussed about the guidance of choosing \(\) optimally. Although there has been related work on how to adapt a diversity regularization term using online bandits , future work is still needed on how our _policy space diversity_ could benefit the approximation of a full game NE most. Another interesting direction of future work is to extend PSD-PSRO to larger scale games, such as poker , Mahjong , and dark chess .