# Reading Relevant Feature from Global Representation Memory for Visual Object Tracking

Xinyu Zhou\({}^{1}\)  Pinxue Guo\({}^{2}\)  Lingyi Hong\({}^{1}\)  Jinglun Li\({}^{2}\)  Wei Zhang\({}^{1}\)

Corresponding author.

Weifeng Ge\({}^{1}\)1  Wenqiang Zhang\({}^{1,2}\)1

\({}^{1}\)School of Computer Science, Fudan University, Shanghai, China

\({}^{2}\)Academy for Engineering and Technology, Fudan University, Shanghai, China

zhouxinyu20@fudan.edu.cn, pxguo21@m.fudan.edu.cn,lyhong22@m.fudan.edu.cn,

jingli960423@gmail.com, weizhang@fudan.edu.cn,

weifeng.ge.ic@gmail.com,wqzhang@fudan.edu.cn

###### Abstract

Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.

## 1 Introduction

Visual object tracking (VOT) is a fundamental task in computer vision, which provides only the location of an object in the first frame and then accomplishes continuous predictions of the object's position in subsequent video frames. At present, visual object tracking is widely used in autonomous vehicle , video surveillance, and other scenes. As video frames are dynamic and continuously changing, it presents a significant challenge in addressing the appearance variation of the target and the changes in the environment. The current mainstream approach to addressing this problem is in two perspectives: the first type, as shown in Fig. 1(a), is to perform interactive computations on all patches of a fixed template and the search region. The second type, as shown in Fig.1(b), is to perform the interactive computation of template memory and the search region. Despite their great success, they neglect the appearance variation of the target and the changes in the environment. In other words, the relevant reference information for the search region at different time steps should be diverse. Retrieving all information from thetemplate or memory to the search region is redundant, and some unnecessary historical information that does not match the current time step may potentially degrade tracking performance.

In this paper, we propose a novel tracking paradigm (RFGM) to address the aforementioned problems. As shown in Fig.1(c), the core insight of our approach is to extract relevant historical cues from memory for different search frames at different time steps. The approach dynamically selects historical information from memory that is more suitable for the current search region, thereby improving tracking performance. Additionally, to obtain the target appearance features of the entire video sequence as reference features, we utilize the dynamic selection ability of our tracker to construct a global representation memory(GR Memory), which dynamically stores the target appearance features of historical frames for adapting to the appearance variation of the target and improving the robustness of the model.

Specifically, as shown in Fig.1(c) we design relevance attention specifically for tracking, which is different from previous attention[54; 51; 5] based on global information reading. The relevance attention mechanism can read global information across frames on the timeline of the video sequence, and decide which token to use in the attention mechanism based on dynamic ranking. The most suitable historical information for the current frame is adaptively selected from the memory through the attention mechanism. Each stage of the relevance attention discards unselected tokens, so the computational complexity of later layers is smaller than that of previous layers. Finally, to construct the global representation memory, unlike directly replacing the earliest template with a new one in Fig.2(b), we utilize the relevance attention to automatically select the target tokens from the new template and the original memory as the new memory. The constructed GR memory retains the most important target features of the original memory and updates new target features into the memory, as presented in Fig.1(c). After multiple updates, it will obtain the most representative target features in the entire video sequence.

Our contribution can be concluded in three-fold: **(i)**We propose a novel tracking framework that adapts to changes in target appearance and background by constructing a global representation memory at the token level across frames and reading from this memory to capture the most relevant features at the current time step;**(ii)** We design a relevance attention mechanism for the search region to selectively extract template features from memory. Simultaneously, it is utilized to update the global representation memory at the token level, reduce memory consumption and enhance tracking speed; **(iii)** We conduct systematic experiments and validate the effectiveness of the proposed designs. Our tracker achieves competitive performance on five widely used benchmarks.

## 2 Related work

### Visual Object Tracking Paradigms

In the past few years, many trackers have achieved great success in the field of visual object tracking. Typically, the linear matching method with cross-correlation is used to locate the target position in the search frame[1; 49; 6; 42; 26; 16; 57; 44; 2]. SiamFC is the first to propose the use of Siamese networks for feature extraction and linear correlation operation for visual object tracking.

Figure 1: Three different methods of tracking pipeline. The purple dots represent the selected points from the new template that are updated into the memory, while the red dots and blue dots represent the selected points that are fed into the relation model.

SiamFC++ further extends this method by feeding the features after linear correlation into three different prediction heads for classification, quality assessment, and regression, respectively, to decompose the tracking task. After the emergence of the transformer, the cross-attention operation[15; 41; 9; 14; 43; 54; 5; 32; 47; 51] based on the attention mechanism replace the original cross-correlation as the mainstream paradigm for object tracking. TransT incorporates a CFA module to achieve bidirectional interaction between the template and the search region. AiAtrack utilizes an attention in attention mechanism to achieve matching from the template to the search region, while OSTrack and MixFormaer use a one-stream paradigm to extract features and interact between the reference features and search region simultaneously.

However, the historical information required for different search regions at different time steps should be different. Therefore, cross-correlation and cross-attention operations introduce redundant information, potentially deleting tracking performance. To address this problem, we propose a novel tracking paradigm, RFGM, which automatically extracts the most suitable information for the search region from the GR memory, improving the adaptability to the appearance variation of the target and the changes in the environment.

### Attention mechanism

The attention mechanism is widely employed in the tracking domain, and it has demonstrated remarkable performance, as exemplified by TransT, STARK, and OSTrack, etc. However, these mechanisms integrate information from all templates with the information from the search region, resulting in redundant template features. In contrast, deformable attention possesses feature selection. Deformable attention, derived from deformable CNN[10; 59], has been applied in various fields such as object detection and image classification. It can adaptively select neighboring tokens based on the current query. For instance, Deformable DETR uses deformable attention to regress the offset of the neighborhood based on the query coordinate in order to assist the query in selecting the most appropriate key and value, which accelerates the convergence of the DETR model for object detection. DAT extends deformable attention to the backbone for classification. However, these deformable methods are based on regressing the coordinates of neighboring tokens within the current frame and cannot globally read all tokens across frames. Inspired by Dynamic ViT, we design a relevance attention mechanism specifically for visual object tracking. Instead of regressing the coordinate offsets, we utilize ranking to dynamically assist the search region in globally reading useful features across historical frames.

### Memory networks

Information from historical frames enables the network to adapt to the variation of appearance and background. Many approaches are devoted to template updates for improving performance in different fields, such as video object segmentation[36; 17; 53; 8; 21; 7; 58], video obejct tracking[52; 14; 51; 9], etc. In video object segmentation, STM and STCN construct memory networks, enabling the model to store extensive historical frame information for robust object tracking and segmentation. However, since memory incurs memory overhead and reduces model computation speed, it becomes challenging to store all historical frames during long-term video object segmentation. Therefore, XMem introduces three distinct types of memory to facilitate long-term object tracking and segmentation. In object tracking, GradNet updates the template by backwarding the gradients. With a template controller based on LSTM, MemTrack  design a memory network for robust object tracking. STMTrack proposes a memory-based tracker, which updates templates at a fixed interval. Besides, Mixformer and STARK design a scoring head to select the representative template. We utilize the proposed relevance attention to construct a GR memory. In contrast to previous methods that update the memory with the entire template, the GR memory adaptively selects relevant tokens from both the original memory and new template tokens using relevance attention. After multiple updates, the GR memory contains the most representative tokens of the target in the entire video sequence.

## 3 Method

### Tracking with GR memory and relevance attention

**Pipeline**.

\(Encoder\). As shown in Fig.2, The RFGM utilizes the vanilla ViT as the encoder for feature interaction. The initial input of RFGM is a pair of images, which is referred to as template \(z^{3 H_{z} W_{z}}\) and search \(x^{3 H_{x} W_{x}}\). The template and search in RFGM are first divided into \(N_{z}\) and \(N_{x}\) non-overlapping patches \(z^{p}\) and \(x^{p}\), respectively. The size of these patches is \(S S\), and \(N_{z}=H_{z}W_{z}/S^{2}\), \(N_{x}=H_{x}W_{x}/S^{2}\). These patches are then linearly mapped to a series of patch embeddings (tokens) by a convolutional layer, \(T^{z}=\{T^{z}_{1},T^{z}_{2},,T^{z}_{N_{z}}\}\) and \(T^{x}=\{T^{x}_{1},T^{x}_{2},,T^{x}_{N_{z}}\}\), \(T^{1 C}\), where \(C\) is the dimension of patch embedding. We concatenate the \(T^{x}\) and initial \(T^{z}\) and feed them into the encoder to obtain the tracking results of the first frame. In the subsequent tracking process, the predicted box will be used to crop a new template to adapt to changes in the target's appearance. Besides, we establish a GR memory \(M\) to store the most representative target features throughout the video. Then, we feed \(M\), a new template, and \(T^{x}\) into the Token Filter (TF) module to get the selected tokens from memory and the new template. Finally, we feed the selected token and \(T^{x}\) into the encoder for feature interaction. The entire encoder consists of 12 transformer blocks, with relevance attention transformer blocks employed at the 4-th, 7-th, and 10-th layers.

\(Decoder\). The prediction head is based on the decoder of  and consists of three branches: score branch, offset branch, and size branch. Each branch is composed of three convolutional layers. The score branch is used to predict the position \(R^{}{16}}{16}}\) of the target center, the offset branch is used to compensate for the error \(E^{2}{16}}{16}}\) of caused by downsampling, and the size branch predicts the height and width \(O^{2}{16}}{16}}\) of the target. The position with the highest score in the score map of the score branch is selected, \(i.e.,(x_{r},y_{r})=argmax(x_{r},y_{r})R_{xy}\)and combined with the target size to obtain the final bounding box:

\[(x,y,w,h)=(x_{r}+E(0,x_{r},y_{r}),y_{r}+E(1,x_{r},y_{r}),O(0,x_{r},y_{r}),O(1, x_{r},y_{r}))\] (1)

**Global Representation Memory**. Previous visual object tracking methods[15; 9; 43; 51] generally push a template \(z_{t}\) to memory \(M_{t-1}=\{z_{0},z_{1},,z_{t-1}\},M^{n 3 H_{z}  W_{z}}\) at fixed time intervals and discard the earliest template \(z_{1}\) in a first-in-first-out manner, where \(n\) is the number of templates, and \(t\) is the time step in the video. \(z_{0}\) is the template cropped with ground truth, it will be stored in memory all the time. The memory updating can be formulated as:

\[M_{t}=\{z_{0},z_{2},z_{t-1}\} z_{t}\] (2)

However, directly pushing the entire template to memory will introduce distractor information, and discarding the earliest template is prone to losing representative features.

Figure 2: The framework of RFGM. It consists of a GR memory, token filter(TF), an encoder, and a decoder. The encoder is composed of Attention and relevance attention, while the decoder consists of a prediction head.

To address the aforementioned issues, instead of updating the whole template, we propose a global representation memory (GR memory) updating as token level, which can adaptively select the most representative tokens from both the previous time step memory \(M_{t-1}\) and new template of \(z_{t}\). We initialize a memory \(M_{0}=\{T^{z_{0}}\}\) and add the new coming template tokens \(T^{z_{t}}\) of \(z_{t}\) to it when the memory is not full. When the memory \(M_{t-1}=\{T^{z_{0}},T^{z_{1}},,T^{z_{t-1}}\}\) reaches a certain maximum number \(N_{max}\) of tokens, as shown in Fig.3, we feed \(M_{t-1}\) and \(T^{z_{t}}\) into the Token Filter module to obtain relevant scores, and subsequently rank these tokens score in order, selecting the top \(k\) tokens that are most relevant to the current search region and merge them with the initial memory \(M_{0}\) to obtain the GR memory \(M_{t}\). Because \(T^{p}=\{T^{z}_{1},T^{z}_{2},,T^{z}_{N_{z}}\}\), the GR memory updating can be formulated as:

\[m=Topk(Rank(M_{t-1} T^{z_{t}}))\] (3)

\[ Rank(M_{t-1} T^{z_{t}})=Rank(\{T^{z_{1}}_{1},T^{z_{1}}_{2},,T^{z_{1}}_{N_{z}},T^{z_{1}}_{2},T^{z_{2}}_{2},,T^{ z_{2}}_{N_{z}},,T^{z_{1}}_{1},T^{z_{t-1}}_{2},\\ ,T^{z_{t-1}}_{N_{z}}\}\{T^{z_{t}}_{1},T^{z_{t}}_{2}, ,T^{z_{t}}_{N_{z}}\})\] (4)

\[M_{t}=M_{0} m=\{T^{z_{0}}_{1},T^{z_{0}}_{2},,T^{z_{0}}_{N_{z}},T^{z_{ 1}}_{1},T^{z_{10}}_{1},T^{z_{0}}_{3},,T^{z_{1}}_{N_{z}}\}\] (5)

where \(m=\{T^{z_{1}}_{1},T^{z_{2}}_{10},T^{z_{5}}_{3},,T^{z_{t}}_{N_{z}}\}\) is the selected top \(k\) tokens, the size of \(m\) is \(N_{max}\). The way of memory updating can automatically discard some distractor information and select the most suitable tokens for the search region. Through multiple rounds of updates, the final memory includes the most representative target tokens throughout the entire video sequence.

**Relevance attention**. Previous feature interaction for the search region typically involves reading all information from a regular memory \(M_{t}=\{z_{0},z_{1},,z_{t}\}\) using linear correlation or cross-attention, which can be represented by the following formula:

\[x_{*}=M_{t}x=_{i=0}^{t}(z_{t}x)\] (6)

\[q=x,k=v=M_{t},x_{*}=x+MHA(q,k,v)\] (7)

where \(\) is linear correlation, the variables q, k, and v are used to denote the query, key, and value that are inputted into the multi-head attention(\(MHA\)) block. However, utilizing all the information in the memory is redundant and some harmful information may deteriorate the performance.

Therefore, we propose relevance attention that can automatically select the most relevant tokens from memory, which can be represented by the following formula:

\[m^{}=Topk(Rank(M^{l}_{t}))\] (8)

where \(M^{l}_{t}\) is the memory tokens at time step t and serves as the input to the \(l\)-th layer of the transformer block. Then, we take the tokens \(m^{}\) that are most relevant to the current search region as the query, key, and value. We also add the search region tokens \(T^{x_{t}}_{l}\) to the query, key and value, resulting in the

Figure 3: The token filter consists of three regular transformer blocks and an adaptive token rank, which effectively updates the features in memory. GR-M represents the global representation memory, T stands for the new template, and S represents the search region.

final query, key, and value. The relevance attention can be formulated as:

\[q=k=v=m^{} T_{l}^{x_{t}}\] (9) \[o^{l}= m^{} T_{l}^{x_{t}}+MHA(q,k,v)\] \[o_{*}^{l}=o^{l}+FFN(o^{l})\]

where \(T_{l}^{x_{t}}\) represents the search region tokens of \(l\)-th layer transformer block, \(FFN\) represents a feed-forward network (FFN), and \(o_{*}^{l}\) is the output of \(l\)-th layer transformer block.

### Adaptive Token Ranking

To enable ranking in relevance attention and GR memory, we design an adaptive token ranking module in the \(4\)-th, \(7\)-th, and \(10\)-th transformer layers. Attention weights can represent the relevance between tokens in memory and the current search area feature. Therefore, we input the attention weights into a multi-layer perceptron(MLP) to enhance the relevance, obtaining the relevant score for the current search area feature. Finally, we select the most relevant tokens by adaptive token ranking.

To simplify, we only use \(T^{z_{t}}\) of one template from the memory to demonstrate the formula. Therefore, the multi-head attention weights of \(T^{z_{t}}\) and \(T^{x_{t}}\) in \(l\)-th layer in encoder is \(w^{h N_{z} N_{x}}\), where \(h=12\) is the number of multi-head. The score prediction can be represented as follows:

\[w^{}=^{N_{x}}w_{j}}{N_{x}},w_{j}^{h N _{z}} w\] (10)

\[=Softmax(MLP(w^{^{T}}))^{N_{z} 2}\] (11)

where \(_{y,0}\) represents the score of selecting the \(y\)-th token, \(_{y,1}\) represents the score of discarding the \(y\)-th token, \(y[0 N_{z})\)

**Training stage**. We keep all tokens that \(_{y,0}\) is greater than \(_{y,1}\). As the token selection based on the score is non-differentiable, we exploit the Gumbel-Softmax function to enable gradient backpropagation. It can be formulated as:

\[D=Gumbel-Softmax()\{0,1\}^{N_{z}}\] (12)

where \(D\) is a one-hot tensor with length \(N_{z}\), the differentiability of Gumbel-Softmax enables end-to-end training. Finally, multiplying \(D\) with attention weights \(w\) to discard low-scoring tokens:

\[w_{masked}=Concat(w_{0,0}D,w_{0,1}D,,w_{m,n}D),m[0,h),n[0,N_{x})\] (13)

where \(w_{masked}\) represents the masked attention weights, and \(Concat\) represents recombining all (\(w_{m,n}D^{N_{z}}\)) to \(w_{masked}^{h N_{z} N_{x}}\).

**Inference stage**.Instead of simply using \(D\) as a mask for binary classification (keeping or discarding), we rank the scores of all tokens in \(T^{z}=\{T_{1}^{z},T_{2}^{z},,T_{N_{z}}^{z}\}\) and select the top \(k\) tokens, which allows selecting the tokens that are most relevant to \(T^{x}\). It can be formulated as:

\[T_{*}^{z}=Topk(Rank(T^{z}))\] (14)

### Loss Fucntion

In the training stage, we generate a Gaussian map using ground truth and use focal loss to supervise the score branch. In addition, we use L1 loss and GIoU loss to supervise the offset and size branches respectively. We also set up a ratio loss to supervise \(D\) in the adaptive token ranking, which can help us to constrain the ratio of kept tokens:

\[L_{ratio}=_{b=1}^{B}_{s=1}^{S}(q^{(s)}-} _{j=1}^{N_{z}}D_{j}^{b,s})^{2}\] (15)

where \(q=[0.9,0.8,0.7]\) represents kept target ratio for \(s\) stages, and stages include \(4\)-th, \(7\)-th, \(10\)-th layer in encoder. \(B\) is the batchsize. Finally, the total loss of a combination of the above objectives:

\[L_{total}=_{score}L_{focal}+_{iou}L_{giou}+_{l1}L_{l1}+ _{ratio}L_{ratio}\] (16)

where \(_{score}=1\), \(_{iou}=2\), \(_{l1}=5\) and \(_{ratio}=1\) are the weights to balance the objectives.

## 4 Experiments

### Implementation Details

**Model**. The proposed RFGM uses ViT-B as the encoder, namely RFGM-B256. ViT-B is initialized using the pretrained weights from the MAE model. RFGM-B256: the search region, which is \(4\) times the area of the target object, is resized to \(256 256\) pixels. The template, which is \(2\) times the area of the target object, is resized to \(128 128\) pixels. The proposed relevance attention consists of three linear layers and the GELU activation function. The output dimensions of the three linear layers are 384, 192, and 2, respectively.

**Training.**Our experiments are conducted on Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz with 252GB RAM and 4 NVIDIA GeForce RTX 3090 GPUs with 24GB memory. This model training is divided into two stages. In the first stage, the number of templates is set to 3 frames and trained for 300 epochs, with 60k image pairs per epoch. The learning rate decreases by a factor of 10 after 240 epochs. In the second stage, fine-tuning is performed based on the first stage, with the number of templates increased to 7 frames and trained for 50 epochs. For GOT-10k, the model is trained for 100 epochs and decreases the learning rate at epoch 80 in the first stage. In the second stage, the model is finetuned for an extra 30 epochs. We set the learning rate of the prediction module in the relevance attention of the encoder and the decoder to 4e-4 and set the learning rate of the remaining parameters in the encoder to 4e-5. Additionally, the weight decay is set to 1e-4. Our training datasets includes COCO, LaSOT, GOT-10k, and TrackingNet. Data augmentation techniques include Random horizontal flip and brightness jittering.

**Inference**. The size of GR memory \(N_{max}\) is set to \(3 N_{z}\) by default. The update interval of the GR memory is set to 5 for \(t<=100\), doubled every 100 frames until \(t=500\), and then remains 160. The gradual increase in the update interval is to reduce the accumulation of model errors caused by the template from inaccurate tracking results. 3 stages relevance attention is set to \(floor(3 N_{z} 0.9)\), \(floor(3 N_{z} 0.8)\) and \(floor(3 N_{z} 0.7)\) respectively, where \(floor\) means rounding down to the nearest integer. We employ a single NVIDIA GeForce RTX 3090 for inference.

### State-of-the-Art Comparisons

We compare the proposed RFGM with state-of-the-art methods on five tracking benchmarks, including large-scale benchmarks(TrackingNet, GOT-10k, and LaSOT) and two small-scale benchmarks(OTB and UAV123).

**TrackingNet**. TrackingNet is a dataset containing large-scale testing videos, with a total of 511 testing videos. As reported in Tab.1, RFGM-B256 performs better than the recent state-of-the-art method SimTrack with 1.4 %AUC score improvement. Importantly, we only use ViT-B as our encoder and SimTrack use ViT-L as the encoder. In addition, our model significantly outperforms other models with search resolution smaller than 300, achieving an AUC of 84.7%, P\({}_{norm}\) of 89.6 %

    &  &  &  &  \\   & & AUC & P\({}_{Norm}\) & P & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & AUC & P\({}_{Norm}\) & P \\   & **RFGM-B256** & **84.7** & **89.6** & **83.6** & **74.1** & **84.6** & **71.8** & 70.3 & **82.0** & **76.4** \\   & SimTrack  & 83.4 & 87.4 & - & 69.8 & 78.8 & 66.0 & **70.5** & 79.7 & - \\  & OSTrack-256  & 83.1 & 87.8 & 82.0 & 71.0 & 80.4 & 68.2 & 69.1 & 78.7 & 75.2 \\  & SwinTrack  & 81.1 & - & 78.4 & 71.3 & 81.9 & 64.5 & 67.2 & - & 70.8 \\  & SLT  & 82.8 & 87.5 & 81.4 & 67.5 & 76.5 & 60.3 & 66.8 & 75.5 & - \\  & SBT  & - & - & - & 70.4 & 80.8 & 64.7 & 66.7 & - & 71.1 \\  & AutoMatch  & 76.0 & - & 72.6 & 65.2 & 76.6 & 54.3 & 58.3 & - & 59.9 \\  & TransT  & 81.4 & 86.7 & 80.3 & 67.1 & 76.8 & 60.9 & 64.9 & 73.8 & 69.0 \\  & SiamAtt  & 75.2 & 81.7 & - & - & - & - & 56.0 & 64.8 & - \\  & SiamBAN  & - & - & - & - & - & - & 51.4 & 59.8 & - \\  & DSTrpn  & 64.9 & - & 58.9 & - & - & - & 43.4 & 54.4 & - \\  & Ocean  & - & - & - & 61.1 & 72.1 & 47.3 & 56.0 & 65.1 & 56.6 \\  & SiamPRN++  & 73.3 & 80.0 & 69.4 & 51.7 & 61.6 & 32.5 & 49.6 & 56.9 & 49.1 \\  & MDNet  & 60.6 & 70.5 & 56.5 & 29.9 & 30.3 & 9.9 & 39.7 & 46.0 & 37.3 \\   

Table 1: Comparisons with state-of-the-art methods with search resolution \(<300 300\) on three large-scale benchmarks. The top three metrics are highlighted with **red**, blue, and green fonts.

and P of 83.6. Notably, as shown in Tab.2 comparisons, our model also surpasses all methods with search resolution greater than 300 in the TrackingNet benchmark.

**GOT-10k**. GOT-10k consists of a training set of 10,000 videos and a validation set of 180 videos. Moreover, the training and test sets do not overlap, requiring trackers to have a strong generalization ability for unseen data. Following the official evaluation rules, we only evaluate models trained on the GOT-10k training set. As reported in Tab.1 and Tab.2, similar to the results on TrackingNet, our method surpasses all methods with resolutions greater than 300 and smaller than 300 by using only a search resolution of 256, achieving a 74.1% of AO, 84.6%of SR0.5, and 71.8% of SR0.75. This demonstrates the strong generalization ability of our model to unseen data.

**LaSOT**. LaSOT is a comprehensive benchmark for long-term tracking tasks, including a test set comprising 280 videos. The average length of the videos in the test set is 2448 frames. As shown in Tab.1, our model achieves the highest performance in P\({}_{norm}\) and P, with scores of 82.0% and 76.4% respectively, outperforming other methods. It only lags behind SimTrack by 0.2% in terms of AUC, mainly due to their use of ViT-L while we only use ViT-B. In comparison with methods using search resolutions greater than 300 in Tab.2, our model still demonstrates competitive performance on the Last dataset.

**OTB**. As shown in Tab.3, OTB is a classic dataset in the field of object tracking, consisting of 100 video sequences. RFGM demonstrates excellent performance on this dataset, This demonstrates that our method also exhibits generalization performance on the classical object tracking dataset.

**UAV123**. As shown in Tab.3, UAV123 is a dataset specifically designed for UAV (Unmanned Aerial Vehicle) target tracking, consisting of 123 video sequences with relatively small objects. RFGM achieves competitive performance on the dataset. Achieving a frame rate of 71 FPS, our approach significantly outperforms other methods in terms of speed.

### Ablation study

**Why does GR memory work?** We conduct ablation experiments on different memory types, as shown in Tab.4. When using only one template, the performance is low because there is no template update to adapt to the appearance variations of the target. When using regular memory updating with a fixed interval, competitive performance is achieved on GOT-10k and TrackingNet, but the AUC on LaSOT is only 64.5. This is because LaSOT is a long-term benchmarks, and errors can accumulate in the memory when updating the template, resulting in performance degradation. By using the center score from the prediction head, errors in the memory can be reduced, but it cannot store the

    &  &  &  &  \\   & & AUC & P\({}_{Norm}\) & P & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & AUC & P\({}_{Norm}\) & P \\   & **RFGM-B256** & **84.7** & **89.6** & **83.6** & **74.1** & **84.6** & **71.8** & 70.3 & **82.0** & 76.4 \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & \\   &  &  &  &  &  &  &  &  &  &  \\   

Table 2: Comparisons with state-of-the-art methods with search resolution \(>300 300\) on three large-scale benchmarks. The top three metrics are highlighted with **red**, blue and green fonts.

    &  & PrDiMP & TransT & TrDiMP & STARK & AiTrack & CSwinTT & ToMP & Mixformer-L &  \\  &  &  &  &  &  &  &  &  &  & \\  OTB & 69.6 & 69.6 & 69.4 & 71.1 & 68.5 & 69.6 & - & 70.1 & 70.4 & **71.5** \\ UAV123 & 61.3 & 68.0 & 69.1 & 67.5 & 69.1 & **70.6** & 70.5 & 66.9 & 68.7 & 68.5 \\  Speed(fps) & 35 & 30 & 50 & 35 & 42 & 38 & 12 & 24 & 18 & 71 \ representative features. In contrast, our proposed GR memory can store the most representative features throughout the entire video sequence and also reduce error accumulation in the memory, resulting in the highest performance. The visualization of GR memory updates is illustrated in Fig5.

**Attention comparison**. As reported in Tab.5, using only the attention mechanism from ViT cannot adaptively select the most suitable features from the reference for the current search frame. Therefore, its performance is lower than our proposed relevance attention. It is worth noting that the baseline here includes GR memory, where all features have already undergone a round of filtering using relevance attention within the GR memory. In other words, even within the filtered GR memory,

    &  &  &  \\   & AUC & P\({}_{Norm}\) & P & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & AUC & P\({}_{Norm}\) & P \\  One template & 84.0 & 88.2 & 82.5 & 70.4 & 80.0 & 66.3 & 69 & 80.1 & 74.0 \\ Memory & 84.5 & 88.8 & 82.65 & 71.9 & 82.0 & 69.4 & 64.5 & 73.9 & 68.2 \\ Score memory & 84.6 & 88.9 & 82.7 & 74.3 & 84.0 & 70.8 & 69.7 & 81.5 & 76.2 \\ GR memory & 84.7 & 89.6 & 83.6 & 74.1 & 84.6 & 71.8 & 70.3 & 82.0 & 76.4 \\   

Table 4: Ablation study on our proposed GR memory. One template represents only the use of the first frame as the template. Memory represents replacing an old template with a new template in a fixed interval. Score memory represents replacing the template according to the center score of the prediction head. The best metrics are highlighted with red fonts.

Figure 4: Visualization of relevance attention. Taking one template as an example, white areas represent discarded regions, while the remaining areas represent the regions selected by relevance attention. Stages 1 to 3 indicate the progressive application of three relevance attention layers.

Figure 5: Visualization of GR memory updates. Over time t, the GR memory accumulates an increasing number of representative penguin features. In the second and third rows, white tokens represent discards, while the other tokens are retained in memory.

further performance improvement can be achieved by applying relevance attention. Additionally, relevance attention can reduce the number of computational parameters (40.00G VS 37.66G) while maintaining negligible parameter overhead(92.12M VS 92.35M). Tab.7 also presents the macs and parameters with or without relevance attention. The visualization of relevance attention is shown in Fig.4.

**What is the best size of GR memory?** As shown in Tab. 6, we conducted ablation experiments on the size of the memory. We found that the larger memory size does not lead to better performance. In fact, when the memory size exceeds 192 tokens, the performance starts to decline. This is because storing templates in the memory based on incorrect tracking results can lead to error accumulation. With a larger memory size, more errors are accumulated as the tracking progresses. Therefore, the optimal memory size for RFGM is 192 tokens.

## 5 Conclusion and Limitation

In this paper, we present a novel tracking framework tracking(RFGM), consisting of relevance attention and GR memory. Relevance attention adaptively selects the most suitable features for the current search region, allowing for adaptation to target appearance variations and environmental changes. Additionally, we construct a GR memory that utilizes relevance attention to select features from incoming templates. Through multiple rounds of updates, the GR memory stores the most representative features from the entire video sequence. Comprehensive experiments demonstrate that our method achieves state-of-the-art performance, and ablation experiments verify the effectiveness of the proposed GR memory and relevance attention. Due to the template updates, incorrect information will be stored into the memory, resulting in the accumulation of distractors over the process of the object tracking, we plan to further investigate methods to reduce the accumulation of errors in the memory.