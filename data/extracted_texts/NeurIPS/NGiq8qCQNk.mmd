# Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient Computation of Nash Equilibria

**Fivos Kalogiannis**

Department of Computer Science

University of California, Irvine

Irvine, CA

fkalogia@uci.edu &**Ioannis Panageas**

Department of Computer Science

University of California, Irvine

Irvine, CA

ipanagea@ics.uci.edu

###### Abstract

The works of (Daskalakis et al., 2009, 2022; Jin et al., 2022; Deng et al., 2023) indicate that computing Nash equilibria in multi-player Markov games is a computationally hard task. This fact raises the question of whether or not computational intractability can be circumvented if one focuses on specific classes of Markov games. One such example is two-player zero-sum Markov games, in which efficient ways to compute a Nash equilibrium are known. Inspired by zero-sum polymatrix normal-form games (Cai et al., 2016), we define a class of zero-sum multi-agent Markov games in which there are only pairwise interactions described by a graph that changes per state. For this class of Markov games, we show that an \(\)-approximate Nash equilibrium can be found efficiently. To do so, we generalize the techniques of (Cai et al., 2016), by showing that the set of coarse-correlated equilibria collapses to the set of Nash equilibria. Afterwards, it is possible to use any algorithm in the literature that computes approximate coarse-correlated equilibria Markovian policies to get an approximate Nash equilibrium.

## 1 Introduction

Multi-agent reinforcement learning (MARL) is the discipline that is concerned with strategic interactions between agents who find themselves in a dynamically changing environment. Early aspects of MARL can be traced back to, as early as, the initial text on two-player zero-sum stochastic/Markov games (Shapley, 1953). Today, Markov games have been established as the theoretical framework for MARL (Littman, 1994). The connection between game theory and MARL has lead to several recent cornerstone results in benchmark domains in AI (Bowling et al., 2015; Brown and Sandholm, 2019, 2018; Brown et al., 2020; Silver et al., 2017; Moravcik et al., 2017; Perolat et al., 2022; Vinyals et al., 2019). The majority of the aforementioned breakthroughs relied on computing _Nash equilibria_(Nash, 1951) in a scalable and often decentralized manner. Although the theory of single agent reinforcement learning (RL) has witnessed an outstanding progress (_e.g._, see (Agarwal et al., 2020; Bertsekas, 2000; Jin et al., 2018; Li et al., 2021; Luo et al., 2019; Panait and Luke, 2005; Sidford et al., 2018; Sutton and Barto, 2018), and references therein, the landscape of multi-agent settings eludes a thorough understanding. In fact, guarantees for provably efficient computation of Nash equilibria remain limited to either environments in which agents strive to coordinate towards a shared goal (Chen et al., 2022; Claus and Boutilier, 1998; Ding et al., 2022; Fox et al., 2022; Leonardos et al., 2021; Maheshwari et al., 2022; Wang and Sandholm, 2002; Zhang et al., 2021) or fully competitive such as two-player zero-sum games (Cen et al., 2021; Condon, 1993; Daskalakis et al., 2020; Sayin et al., 2021, 2020; Wei et al., 2021) to name a few. Part of the lack of efficient algorithmic results in MARL is the fact that computing approximate Nash equilibria in (general-sum) games is computationally intractable (Daskalakis et al., 2009; Rubinstein, 2017; Chen et al., 2009; Etessami and Yannakakis, 2010) even when the games have a single state, _i.e._, normal-form two-player games.

We aim at providing a theoretical framework that captures an array of real-world applications with multiple agents -- which admittedly correspond to a big portion of all modern applications. A recent contribution that computes NE efficiently in a setting that combines both collaboration and competition, (Kalogiannis et al., 2022), concerns adversarial team Markov games, or competition between an adversary and a group of uncoordinated agents with common rewards. Efficient algorithms for computing Nash equilibria in settings that include both cooperation and competition are far fewer and tend to impose assumptions that are restrictive and difficult to meet in most applications (Bowling, 2000; Hu and Wellman, 2003). The focus of our work is centered around the following question:

_Are there any other settings of Markov games that encompass both competition and coordination while mantaining the tractability of Nash equilibrium computation?_

Inspired by contemporary works in algorithmic game theory and specifically zero-sum _polymatrix_ normal-form games (Cai et al., 2016), we focus on the problem of computing Nash equilibria in zero-sum _polymatrix Markov_ games. Informally, a polymatrix Markov game is a multi-agent Markov decision process with \(n\) agents, state-space \(\), action space \(_{k}\) for agent \(k\), a transition probability model \(\) and is characterized by a graph \(_{s}(,_{s})\) which is potentially different in every state \(s\). For a fixed state \(s\), the nodes of the graph \(\) correspond to the agents, and the edges \(_{s}\) of the graph are two-player normal-form games (different per state). Every node/agent \(k\) has a fixed set of actions \(_{k}\), and chooses a strategy from this set to play in all games corresponding to adjacent edges. Given an action profile of all the players, the node's reward is the sum of its rewards in all games on the edges adjacent to it. The game is globally zero-sum if, for all strategy profiles, the rewards of all players add up to zero. Afterwards, the process transitions to a state \(s^{}\) according to \(\). In a more high-level description, the agents interact over a network whose connections change at every state.

Our results.We consider a zero-sum polymatrix Markov game with the additional property that a single agent (not necessarily the same) controls the transition at each state, _i.e._, the transition model is affected by a single agent's actions for each state \(s\). These games are known as _switching controller_ Markov games. We show that we can compute in time \((||,n,_{i[n]}|_{i}|,1/)\) an \(\)-approximate Nash equilibrium. The proof relies on the fact that zero-sum polymatrix Markov games with a switching controller have the following important property: the marginals of a coarse-correlated equilibrium constitute a Nash equilibrium (see Section 3.2). We refer to this phenomenon as _equilibrium collapse_. This property was already known for zero-sum polymatrix normal-form games by Cai et al. (2016) and our results generalize the aforementioned work for Markov games. As a corollary, we get that any algorithm in the literature that guarantees convergence to approximate coarse-correlated equilibria Markovian policies--_e.g._, (Daskalakis et al., 2022)--can be used to get approximate Nash equilibria. Our contribution also unifies previous results that where otherwise only applicable to the settings of _single_ and _switching-control two-player zero-sum games_, or _zero-sum polymatrix normal-form games_. Finally, we show that the equilibrium collapsing phenomenon does not carry over if there are two or more controllers per state (see Section 3.3).

Technical overview.In order to prove our results, we rely on nonlinear programming and, in particular, nonlinear programs whose optima coincide with the Nash equilibria for a particular Markov game (Filar et al., 1991; Filar and Vrieze, 2012). Our approach is analogous to the one used by (Cai et al., 2016) which uses linear programming to prove the collapse of the set of CCE to the set of NE. Nevertheless, using the duality of linear programming in our case is not possible since a Markov game introduces nonlinear terms in the program. It is noteworthy that we do not need to invoke (Lagrangian) duality or an argument that relies on stationary points of a Lagrangian function. Rather, we use the structure of the zero-sum polymatrix Markov games with a switching controller to conclude the relation between a correlated policy and the individual policies formed by its marginals in terms of the individual utilities of the game.

### Importance of zero-sum polymatrix Markov games

Strategic interactions of agents over a network is a topic of research in multiple disciplines that span computer science (Easley and Kleinberg, 2010), economics (Schweitzer et al., 2009), control theory (Tipsuwan and Chow, 2003), and biology (Szabo and Fath, 2007) to name a few.

In many environments where multiple agents interact with each other, they do so in a localized manner. That is, every agent is affected by the set of agents that belong to their immediate "neighborhood". Further, it is quite common that these agents will interact independently with each one of their neighbors; meaning that the outcome of their total interactions is a sum of pairwise interactions rather than interactions that depend on joint actions. Finally, players might remain indifferent to actions of players are not their neighbors.

To illustrate this phenomenon we can think of multiplayer e-games (_e.g._, CS:GO, Fortnite, League of Legends, etc) where each player interacts through the same move only with players that are present on their premises and, in general, the neighbors cannot combine their actions into something that is not a mere sum of their individual actions (_i.e._, they rarely can "multiply" the effect of the individual actions). In other scenarios, such as strategic games played on social networks (_e.g._, opinion dynamics) agents clearly interact in a pairwise manner with agents that belong to their neighborhood and are somewhat oblivious to the actions of agents who they do not share a connection with.

With the proposed model we provide the theoretical framework needed to reason about such strategic interactions over dynamically changing networks.

### Related work

From the literature of Markov games, we recognize the settings of _single controller_(Filar and Raghavan, 1984; Sayin et al., 2022; Guan et al., 2016; Qiu et al., 2021) and _switching controller_(Vrieze et al., 1983) Markov games to be one of the most related to ours. In these settings, all agents' actions affect individual rewards, but in every state one particular player (_single controller_), or respectively a potentially different one (_switching controller_), controls the transition of the environment to a new state. To the best of our knowledge, prior to our work, the only Markov games that have been examined under this assumption are either zero-sum or potential games.

Further, we manage to go beyond the dichotomy of absolute competition or absolute collaboration by generalizing zero-sum polymatrix games to their Markovian counterpart. In this sense, our work is related to previous works of Cai et al. (2016); Anagnostides et al. (2022); Ao et al. (2022) which show fast convergence to Nash equilibria in zero-sum polymatrix normal-form games for various no-regret learning algorithms including optimistic gradient descent.

## 2 Preliminaries

Notation.We define \([n]\{1,,n\}\). Scalars are denoted using lightface variables, while, we use boldface for vectors and matrices. For simplicity in the exposition, we use \(O()\) to suppress dependencies that are polynomial in the parameters of the game. Additionally, given a collection \(\) of policies or strategies for players \([n]\), \(_{-k}\) denotes the policies of every player excluding \(k\).

### Markov games

In its most general form, a Markov game (MG) with a finite number of \(n\) players is defined as a tuple \((H,,\{_{k}\}_{k[n]},,\{r_{k}\}_{k[ n]},,)\). Namely,

* \(H_{+}\) denotes the _time horizon_, or the length of each episode,
* \(\), with cardinality \(S||\), stands for the state space,
* \(\{_{k}\}_{k[n]}\) is the collection of every player's action space, while \(_{1}_{n}\) denotes the _joint action space_; further, an element of that set --a joint action-- is generally noted as \(=(a_{1},,a_{n})\),
* \(\{_{k}\}_{h[H]}\) is the set of all _transition matrices_, with \(_{h}:()\); further, \(_{h}(|s,)\) marks the probability of transitioning to every state given that the joint action \(\) is selected at time \(h\) and state \(s\) -- in infinite-horizon games \(\) does not depend on \(h\) and the index is dropped,
* \(r_{k}\{r_{k,h}\}\) is the reward function of player \(k\) at time \(h\); \(r_{k,h}:,[-1,1]\) yields the reward of player \(k\) at a given state and joint action -- in infinite-horizon games, \(r_{k,h}\) is the same for every \(h\) and the index is dropped,
* a discount factor \(>0\), which is generally set to \(1\) when \(H<\), and \(<1\) when \(H\),
* an initial state distribution \(()\).

Policies and value functions.We will define stationary and nonstationary Markov policies. When the horizon \(H\) is finite, a stationary policy equilibrium need not necessarily exist even for a single-agent MG, _i.e._, a Markov decision process; in this case, we seek nonstationary policies. For the case of infinite-horizon games, it is folklore that a stationary Markov policy Nash equilibrium always exists.

We note that a policy is _Markovian_ when it depends on the present state only. A _nonstationary_ Markov policy \(_{k}\) for player \(k\) is defined as \(_{k}\{_{k,h}:(_{k}),\  h[H]\}\). It is a sequence of mappings of states \(s\) to a distribution over actions \((_{k})\) for every timestep \(h\). By \(_{k,h}(a|s)\) we will denote the probability of player \(k\) taking action \(a\) in timestep \(h\) and state \(s\). A Markov policy is said to be _stationary_ in the case that it outputs an identical probability distribution over actions whenever a particular state is visited regardless of the corresponding timestep \(h\).

Further, we define a nonstationary Markov _joint policy_\(\{_{h},\  h[H]\}\) to be a sequence of mappings from states to distributions over joint actions \(()(_{1}_{n})\) for all times steps \(h\) in the time horizon. In this case, the players can be said to share a common source of randomness, or that the joint policy is correlated.

A joint policy \(\) will be said to be a _product policy_ if there exist policies \(_{k}:[H](_{k}),\  k [n]\) such that \(_{h}=_{1,h}_{n,h},\  h[H]\). Moreover, given a joint policy \(\) we let a joint policy \(_{-k}\) stand for the _marginal joint policy_ excluding player \(k\), _i.e._,

\[_{-k,h}(|s)=_{a^{}_{k}}_{h}(a^{}, {a}|s),\  h[H], s,_{-k}.\]

By fixing a joint policy \(\) we can define the value function of any given state \(s\) and timestep \(h\) for every player \(k\) as the expected cumulative reward they get from that state and timestep \(h\) onward,

\[V^{}_{k,h}(s_{1})=_{}[_{=h}^{H}^{ -1}r_{k,}(s_{},_{})s_{1}]=_{s_{1}}^{ }_{=h}^{H}(^{-1}_{=h}^{h}_{ }(_{}))_{k,}(_{}).\]

Depending on whether the game is of finite or infinite horizon we get the followin displays,

* In finite-horizon games, \(=1\), the value function reads, \[V^{}_{k,h}(s_{1})=_{s_{1}}^{}_{=h}^{H}(_{ ^{}=h}^{}_{^{}}(_{^{}}) )_{k,}(_{}), V^{}_{k}(s_{1})=_{s_{1}}^{}(- \,())^{-1}().\]

Where \(_{h}(_{h}),()\) and \(_{h}(_{h}),()\) denote the state-to-state transition probability matrix and expected per-state reward vector for a given policy \(_{h}\) or \(\) accordingly. Additionally, \(_{s_{1}}\) is an all-zero vector apart of a value of \(1\) in its \(s_{1}\)-th position. Also, we denote \(V^{}_{k,h}()=_{s}(s)V^{}_{k,h}(s)\).

Best-response policies.Given an arbitrary joint policy \(\), we define the _best-response policy_ of a player \(k\) to be a policy \(_{k}^{}\{_{k,h}^{},\  h[H]\}\), such that it is a maximizer of \(_{_{k}^{}}V^{_{k}_{-k}}_{k,1}(s_{1})\). Additionally, we will use the following notation \(V^{,_{-k}}_{k,h}(s)_{_{k}^{}}V^{ _{k}^{}_{-k}}_{k,h}(s)\).

Equilibrium notions.Having defined what a best-response is, it is then quite direct to define different notions of equilibria for Markov games.

**Definition 2.1** (Cc).: _We will say that a joint (potentially correlated) policy \(()^{H S}\) is an \(\)-approximate coarse-correlated equilibrium if it holds that, for an \(>0\),_

\[V^{,_{-k}}_{k,1}(s_{1})-V^{}_{k,1}(s_{1}) ,\  k[n].\] (CCE)

Further, we will define a Nash equilibrium policy,

**Definition 2.2** (Ne).: _A joint, product policy \(_{k[n]}(_{k})^{H S}\) is an \(\)-approximate Nash equilibrium if it holds that, for an \(>0\),_

\[V^{,_{-k}}_{k,1}(s_{1})-V^{}_{k,1}(s_{1}),\  k[n].\] (NE)

It is quite evident that an approximate Nash equilibrium is also an approximate coarse-correlated equilibrium while the converse is not generally true. For infinite-horizon games the definitions are analogous and are deferred to the appendix.

### Our setting

We focus on the setting of zero-sum polymatrix switching-control Markov games. This setting encompasses two major assumptions related to the reward functions in every state \(\{r_{k}\}_{k[n]}\) and the transition kernel \(\). The first assumption imposes a zero-sum, polymatrix structure on \(\{r_{k}\}_{k[n]}\) for every state and directly generalizes zero-sum polymatrix games for games with multiple states.

**Assumption 1** (Zero-sum polymatrix games).: The reward functions of every player in any state \(s\) are characterized by a _zero-sum_, _polymatrix_ structure.

Polymatrix structure.For every state \(s\) there exists an undirected graph \(_{s}(,_{s})\) where,

* the set of nodes \(\) coincides with the set of agents \([n]\); the \(k\)-th node is the \(k\)-th agent,
* the set of edges \(_{s}\) stands for the set of pair-wise interactions; each edge \(e=(k,j),k,j[n],k j\) stands for a general-sum normal-form game played between players \(k,j\) and which we note as \((r_{kj}(s,,),r_{jk}(s,,))\) with \(r_{kj},r_{jk}:_{k}_{j}[-1,1]\).

Moreover, we define \((s,k)\{j[n](k,j)_{s}\} [n]\) to be the set of all neighbors of an arbitrary agent \(k\) in state \(s\). The reward of agent \(k\) at state \(s\) given a joint action \(\) depends solely on interactions with their neighbors,

\[r_{k,h}(s,)=_{j(k)}r_{kj,h}(s,a_{k},a_{j}),\;  h[H], s,.\]

Further, the _zero-sum_ assumption implies that,

\[_{k}r_{k,h}(s,)=0, h[H], s, .\] (1)

In the infinite-horizon setting, the subscript \(h\) can be dropped.

A further assumption (_switching-control_) is necessary in order to ensure the desirable property of equilibrium collapse.

**Assumption 2** (Switching-control).: In every state \(s\), there exists a single player (not necessarily the same), or _controller_, whose actions determine the probability of transitioning to a new state.

The function \(:[n]\) returns the index of the player who controls the transition probability at a given state \(s\). On the other hand, the function \(:_{ (s)}\) gets an input of a joint action \(\), for a particular state \(s\), and returns the action of the controller of that state, \(a_{(s)}\).

**Remark 1**.: _It is direct to see that Markov games with a single controller and turn-based Markov games (Daskalakis et al., 2022), are special case of Markov games with switching controller._

## 3 Main results

In this section we provide the main results of this paper. We shall show the collapsing phenomenon of coarse-correlated equilibria to Nash equilibria in the case of zero-sum, single switching controller polymatrix Markov games. Before we proceed, we provide a formal definition of the notion of collapsing.

**Definition 3.1** (CCE collapse to NE).: _Let \(\) be any \(\)-CCE policy of a Markov game. Moreover, let the marginal policy \(^{}:=(_{1}^{},...,_{n}^{})\) be defined as:_

\[_{k}^{}(a|s)=_{_{-k}_{-k}}(a,_{-k}|s),\; k, s, a_{k}.\]

_If \(^{}\) is a \(O()\)-NE equilibrium for every \(\) then we say the set of approximate CCE's collapses to that of approximate NE's._

We start with the warm-up result that the set of CCE's collapses to the set of NE's for two-player zero-sum Markov games.

### Warm-up: equilibrium collapse in two-player zero-sum MG's

Since we focus on two-player zero-sum Markov games, we simplify the notation by using \(V_{h=1}^{}(s):=V_{2,1}^{}(s)\)--_i.e._, player \(1\) is the minimizing player and player \(2\) is the maximizer. We show the following theorem:

**Theorem 3.1** (Collapse in two-player zero-sum MG's).: _Let a two-player zero-sum Markov game \(^{}\) and an \(\)-approximate CCE policy of that game \(\). Then, the marginalized product policies \(_{1}^{},_{2}^{}\) form a \(2\)-approximate NE._

Proof.: Since \(\) is an \(\)-approximate CCE joint policy, by definition it holds that for any \(_{1}\) and any \(_{2}\),

\[V_{h=1}^{_{-2}_{2}}(s_{1})- V _{h=1}^{}(s_{1}) V_{h=1}^{_{1}_{-1}}( s_{1})+.\]

Due to Claim A.1, the latter is equivalent to the following inequality,

\[V_{h=1}^{_{1}^{_{1}}_{2}}(s_{1})-  V_{h=1}^{}(s_{1}) V_{h=1}^{_{1}_{2} ^{}}(s_{1})+.\]

Plugging in \(_{1}^{},_{2}^{}\) alternatingly, we get the inequalities:

\[V_{h=1}^{_{1}^{_{1}}_{2}}(s_{1}) - V_{h=1}^{}(s_{1}) V_{h=1}^{_{1}^{_{1}}_{2}^{}}(s_{1})+\\ V_{h=1}^{_{1}^{_{1}}_{2}^{}}(s_{1}) - V_{h=1}^{_{1}}(s_{1}) V_{h=1}^{_{1} _{2}^{}}(s_{1})+\]

The latter leads us to conclude that for any \(_{1}\) and any \(_{2}\),

\[V_{h=1}^{_{1}^{_{1}}_{2}}(s_{1})-2  V_{h=1}^{_{1}^{_{1}}_{2}^{}} (s_{1}) V_{h=1}^{_{1}_{2}^{}}(s_{1})+2,\]

which is the definition of a NE in a zero-sum game. 

### Equilibrium collapse in finite-horizon polymatrix Markov games

In this section, we focus on the more challenging case of polymatrix Markov games which is the main focus of this paper. For any finite horizon Markov game, we define (\(_{}\)) to be the following nonlinear program with variables \(,\):

(P \[{}_{}\] ) \[&_{k[n]}(w_{k,1} (s_{1})-_{s_{1}}^{}_{h=1}^{H}(_{=1}^ {h}_{}(_{}))_{k,h}(_{h})) \\ & w_{k,h}(s) r_{k,h}(s,a,_{ -k,h})+_{h}(s,a,_{-k,h})_{k,h+1},\\ & s, h[H], k[n],  a_{k};\\ & w_{k,H}(s)=0, k[n], s; \\ &_{k,h}(s)(_{k}),\\ & s, h[H], k[n],  a_{k}.\]

Using the following theorem, we are able to use (\(_{}\)) to argue about equilibrium collapse.

**Theorem 3.2** (NE and global optima of (\(_{}\))).: _If \((^{},^{})\) yields an \(\)-approximate global minimum of (\(_{}\)), then \(^{}\) is an \(n\)-approximate NE of the zero-sum polymatrix switching controller MG, \(\). Conversely, if \(^{}\) is an \(\)-approximate NE of the MG \(\) with corresponding value function vector \(^{}\) such that \(w_{k,h}^{}(s)=V_{k,h}^{_{h}^{}}(s)(k,h,s)[n][H] \), then \((^{},^{})\) attains an \(\)-approximate global minimum of (\(_{}\))._

Following, we are going to use (\(_{}\)) in proving the collapse of CCE's to NE's. We observe that the latter program is nonlinear and in general nonconvex. Hence, duality cannot be used in the way it was used in (Cai et al., 2016) to prove equilibrium collapse. Nevertheless, we can prove that given a CCE policy \(\), the marginalized, product policy \(_{k[n]}_{k}^{}\) along with an appropriate vector \(^{}\) achieves a global minimum in the nonlinear program (\(_{}\)). More precisely, our main result reads as the following statement.

**Theorem 3.3** (CCE collapse to NE in polymatrix MG).: _Let a zero-sum polymatrix switching-control Markov game, i.e., a Markov game for which Assumptions 1 and 2 hold. Further, let an \(\)-approximate CCE of that game \(\). Then, the marginal product policy \(^{}\), with \(^{}_{k,h}(a|s)=_{_{-k}_{-k}}_ {h}(a,_{-k}),\  k[n], h[H]\) is an \(n\)-approximate NE._

**Proof.** Let an \(\)-approximate CCE policy, \(\), of game \(\). Moreover, let the best-response value-vectors of each agent \(k\) to joint policy \(_{-k},^{}_{k}\).

Now, we observe that due to Assumption 1,

\[w^{}_{k,h}(s)  r_{k,h}(s,a,_{-k,h})+_{h}(s,a,_{-k,h})^{}_{k,h+1}\] \[=_{j(k)}r_{(k,j),h}(s,a,^{ }_{j})+_{h}(s,a,_{-k,h})^{}_{k,h+1}.\]

Further, due to Assumption 2,

\[_{h}(s,a,_{-k,h})^{}_{k,h+1}= _{h}(s,a,^{}_{(s),h})^{ }_{k,h+1},\]

or,

\[_{h}(s,a,_{-k,h})^{}_{k,h+1}= _{h}(s,a,^{})^{}_{k,h+1}.\]

Putting these pieces together, we reach the conclusion that \((^{},^{})\) is feasible for the nonlinear program (\(_{}\)).

What is left is to prove that it is also an \(\)-approximate global minimum. Indeed, if \(_{k}^{}_{k,h}(s_{1}){}\) (by assumption of an \(\)-approximate CCE), then the objective function of (\(_{}\)) will attain an \(\)-approximate global minimum. In turn, due to Theorem 3.2 the latter implies that \(^{}\) is an \(n\)-approximate NE. \(\)

We can now conclude that due to the algorithm introduced in (Daskalakis et al., 2022) for CCE computation in general-sum MG's, the next statement holds true.

**Corollary 3.1** (Computing a NE--finite-horizon).: Given a finite-horizon switching control zero-sum polymatrix Markov game, we can compute an \(\)-approximate Nash equilibrium policy that is Markovian with probability at least \(1-\) in time \((n,H,S,_{k}|_{k}|,,(1/ ))\).

In the next section, we discuss the necessity of the assumption of switching control using a counter-example of non-collapsing equilibria.

### No equilibrium collapse with more than one controllers per-state

Although Assumption 1 is sufficient for the collapse of any CCE to a NE in single-state (_i.e._, normal-form) games, we will prove that Assumption 2 is indispensable in guaranteeing such a collapse in zero-sum polymatrix Markov games. That is, if more than one players affect the transition probability from one state to another, a CCE is not guaranteed to collapse to a NE.

**Example 1**.: _We consider the following \(3\)-player Markov game that takes place for a time horizon \(H=3\). There exist three states, \(s_{1},s_{2},\) and \(s_{3}\) and the game starts at state \(s_{1}\). Player \(3\) has a single action in every state, while players \(1\) and \(2\) have two available actions \(\{a_{1},a_{2}\}\) and \(\{b_{1},b_{2}\}\) respectively in every state._

Reward functions._If player \(1\) (respectively, player \(2\)) takes action \(a_{1}\) (resp., \(b_{1}\)), in either of the states \(s_{1}\) or \(s_{2}\), they get a reward equal to \(\). In state \(s_{3}\), both players get a reward equal to \(-\) regardless of the action they select. Player \(3\) always gets a reward that is equal to the negative sum of the reward of the other two players. This way, the zero-sum polymatrix property of the game is ensured (Assumption 1)._Transition probabilities._If players \(1\) and \(2\) select the joint action \((a_{1},b_{1})\) in state \(s_{1}\), the game will transition to state \(s_{2}\). In any other case, it will transition to state \(s_{3}\). The converse happens if in state \(s_{2}\) they take joint action \((a_{1},b_{1})\); the game will transition to state \(s_{3}\). For any other joint action, it will transition to state \(s_{1}\). From state \(s_{3}\), the game transitions to state \(s_{1}\) or \(s_{2}\) uniformly at random._

_At this point, it is important to notice that two players control the transition probability from one state to another. In other words, Assumption 2 does not hold._

_Next, we consider the joint policy \(\),_

\[(s_{1})=(s_{2})=&b_{1}&b_{2}\\ &a_{1}&0&1/2\\ &a_{2}&(1/2&0).\]

**Claim 3.1**.: The joint policy \(\) that assigns probability \(\) to the joint actions \((a_{1},b_{2})\) and \((a_{2},b_{1})\) in both states \(s_{1},s_{2}\) is a CCE and \(V^{}_{1,1}(s_{1})=V^{}_{2,1}(s_{1})=\).

_Yet, the marginalized product policy of \(\) which we note as \(_{1}^{}_{2}^{}\) does not constitute a NE. The components of this policy are,_

\[_{1}^{}(s_{1})=_{1}^{}(s_{ 2})=a_{1}&a_{2}\\ (1/2&1/2),\\ &\\ _{2}^{}(s_{1})=_{2}^{}(s_{2})= b_{1}&b_{2}\\ (1/2&1/2).\]

_I.e., the product policy \(_{1}^{}_{2}^{}\) selects any of the two actions of each player in states \(s_{1},s_{2}\) independently and uniformly at random. With the following claim, it can be concluded that in general when more than one player control the transition the set of equilibria do not collapse._

**Claim 3.2**.: The product policy \(_{1}^{}_{2}^{}\) is not a NE.

_In conclusion, Assumption 1 does not suffice to ensure equilibrium collapse._

**Theorem 3.4**.: _There exists a zero-sum polymatrix Markov game (Assumption 2 is not satisfied) that has a CCE which does not collapse to a NE._

### Equilibrium collapse in infinite-horizon polymatrix Markov games

In proving equilibrium collapse for infinite-horizon polymatrix Markov games, we use similar arguments and the following nonlinear program with variables \(,\),

Figure 1: A graph of the state space with transition probabilities parametrized with respect to the policy of each player.

\[ _{k[n]}^{}(_{k}-( -\,())^{-1}_{k}( ))\] \[ w_{k}(s) r_{k}(s,a,_{-k})+\,(s,a, _{-k})_{k},\] \[ s, k[n], a_ {k};\] \[_{k}(s)(_{k}),\] \[ s, k[n], a_ {k}.\]

We note that Example 1 can be properly adjusted to show that the switching-control assumption is necessary for equilibrium collapse in infinite-horizon games as well. Compared to finite-horizon games, infinite-horizon games cannot be possibly solved using backward induction. They pose a genuine computational challenge and, in that sense, the importance of the property of equilibrium collapse gets highlighted.

Computational implications.Equilibrium collapse in infinite-horizon MG's allows us to use the CCE computation technique found in (Daskalakis et al., 2022) in order to compute an \(\)-approximate NE. Namely, given an accuracy threshold \(\), we truncate the infinite-horizon game to its _effective horizon_\(H\). Then, we define reward functions that depend on the time-step \(h\), _i.e._, \(r_{k,h}=^{h-1}r_{k}\). Finally,

**Corollary 3.2**.: (Computing a NE--infinite-horizon) Given an infinite-horizon switching control zero-sum polymatrix game \(\), it is possible to compute a Nash equilibrium policy that is Markovian and nonstationary with probability at least \(1-\) in time \((n,,S,_{k}|_{k}|, {},(1/))\).

## 4 Conclusion and open problems

In this paper, we unified switching-control Markov games and zero-sum polymatrix normal-form games. We highlighted how numerous applications can be modeled using this framework and we focused on the phenomenon of equilibrium collapse from the set of coarse-correlated equilibria to that of Nash equilibria. This property holds implications for computing approximate Nash equilibria in switching control zero-sum polymatrix Markov games; it ensures that it can be done efficiently.

Open problems.In light of the proposed problem and our results there are multiple interesting open questions:

* Is it possible to use a policy optimization algorithm similar to those of (Erez et al., 2022; Zhang et al., 2022) in order to converge to an approximate Nash equilibrium? We note that the question can be settled in one of two ways; _either_ extend the current result of equilibrium collapse to policies that are non-Markovian _or_ guarantee convergence to Markovian policies. The notion of _regret_ in (Erez et al., 2022) gives rise to the computation of a CCE that is a non-Markovian policy in the sense that the policy at every timestep depends on the policy sampled from the history of no-regret play and not only the given state.
* We conjecture that a convergence rate of \(O()\) to a NE is possible, _i.e._, there exists an algorithm with running time \(O(1/)\) that computes an \(\)-approximate NE.
* Are there more classes of Markov games in which computing Nash equilibria is computationally tractable?