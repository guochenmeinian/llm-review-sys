# Neural Isometries:

Taming Transformations for Equivariant ML

 Thomas W. Mitchel

PlayStation

tommy.mitchel@sony.com &Michael Taylor

PlayStation

mike.taylor@sony.com &Vincent Sitzmann

MIT

sitzmann@mit.edu

###### Abstract

Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by _isometries_ whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.

## 1 Introduction

We constantly capture lossy observations of our world - images, for instance, are 2D projections of the 3D world. Observations captured consecutively in time are often related by transformations which are easily described in world space, but are intractable in the space of observations. For instance, video frames captured by a camera moving through a static scene are fully described by a combination of the 3D scene geometry and \((3)\) camera poses. In contrast, the image space transformations between these frames can only be characterized by optical flow, a high-dimensional vector field that does not itself have any easily tractable low-dimensional representation.

Geometric deep learning seeks to build neural network architectures that are provably robust to transformations acting on their inputs, such as rotations , dilations , and projective transformations . However, such approaches are only tractable for transformations that have group structure, and, even in those cases, still require meticulously handcrafted and complex architectures. Yet, many real-world transformations of interest, for instance in vision and geometry processing, altogether lack identifiable group structure, such as the effect of camera motion in image space--see Fig. 1 to the right. Even when group-structured, they are often non-linear and non-compact, such as is the case of image

Figure 1: Neural Isometries find latent spaces where complex transformations become tractable.

homographies and non-rigid shape deformations where existing approaches can be prohibitively expensive.

In this paper, we take a first step towards a class of models that learns to be equivariant to unknown and difficult geometric transformations in world space. We propose Neural Isometries **(NIso)**, an autoencoder framework which learns to map the observation space to a latent space in which encodings are related by tractable, highly-structured linear maps whenever their corresponding observations are geometrically related in world space.

Specifically, observations are encoded into latents preserving their spatial dimensions. Images, for instance, can be encoded into latent functions defined over a lower resolution grid or the patch tokens of a ViT. For observations sharing some potentially unknown relationship in world space, we enforce their encodings be related by a functional map \(\) - a linear transformation on the space of latent functions. In particular, we require that \(\) is an _isometry_ such that it preserves a learned inner product and commutes with a learned functional operator, in the sense that rigid body transformations commute with the Laplace operator in Euclidean space.

Neural Isometries exhibit unique properties that make them a promising step towards an architecture-agnostic regime for self-supervised equivariant representation learning. We experimentally validate two principle claims regarding the efficacy and applicability of our approach:

* Neural Isometries recover a general-purpose latent space in which challenging symmetries in the observation space can be reduced to compact, tractable maps in the latent space. We show that this can be exploited by simple isometry-equivariant networks to achieve results on par with leading hand-crafted equivariant networks in tasks with complex non-linear symmetries.
* The latent space constructed is geometrically informative in that it encodes information about the transformations in world space. We demonstrate that robust camera poses can be regressed directly from the isometric functional maps between encodings of adjacent views of a scene.

## 2 Related Work

Geometric Deep Learning.Geometric deep learning is generally concerned with hand-crafting architectures that are equivariant to (_i.e._ that commute with) _known_ transformations acting on the data . To this end, many successful architectures exploit group representations by using established mappings, such as the Fourier or spherical harmonic transforms, to map features onto domains where the group actions manifest equivalently as linear transformations . In most cases, the representations considered are finite-dimensional and _irreducible_ which, loosely speaking, means that the group action in observation space can be expressed exactly by frequency-preserving block-diagonal matrices acting on the transform coefficients. While finite-dimensional irreducible representations (**IRs**) are attractive building blocks for equivariance due to their computationally exploitable structure, they often don't exist for non-compact groups, precluding generalizations to most non-linear symmetries, let alone those ill-modeled by groups. We instead avoid a heuristic choice of symmetry model and seek an approach that enables robustness to arbitrary transformations that may not even be group-structured.

Functional Maps.Essential to our approach is the parameterization of transformations between observations in the latent space not as group representations, but instead as _functional maps_. Introduced in the seminal work of Ovsjanikov _et al._, functional maps (**FMs**) provide a powerful medium for interpreting, manipulating, and constructing otherwise intractable mappings between 3D shapes via their realization as linear transformations on spaces of functions on meshes, forming the basis for state-of-the-art pipelines in shape matching and correspondence . Beyond 3D shapes, FMs can be seen as a tool to parameterize transformations between observations viewed as functions, in the sense that images, for instance, are functions that map 2D pixel coordinates to RGB colors. Integral to their study and implementation is the Laplace-Beltrami operator (the generalization of the Laplace operator to function spaces on manifolds), and in particular the expression of FMs in its eigenbasis which can expose specific geometric properties of deformations. In particular, isometries (distance-preserving transformations) manifest as highly-structured matrices not unlike IRs, being orthogonal and commuting with the diagonal matrix of eigenvalues and are thus approximately block-diagonal. That said, FMs are recovered through regularized linear solves and as such lack the consistency inherent in the analytical expressions of IRs for compact groups. However, freed from nagging theoretical constraints, FMs have displayed remarkable representational capacity, well modeling a variety of highly-complex non-rigid deformations  including those without group structure, such as partial  and inter-genus correspondence . Please see  for an outstanding introduction to functional maps.

Discovering Latent Symmetries.A number of recent approaches have proposed autoencoder frameworks wherein given symmetries in the base space manifest as simple operations in the latent space [28; 29; 30; 31; 32; 33; 34; 35]. Perhaps most similar to our approach is recent work on a Neural Fourier Transform (NFT) that has sought to manifest group actions in observation space as IRs in the the latent space [36; 37]. These methods offer impressive theoretical guarantees under the conditions that the observed transformations are either known or are a group action, although these assumptions may not hold for real-world data with complex symmetries. In contrast, our method is wholly unsupervised, and assumes no knowledge of the transformations in observation space nor that they even form a group.

## 3 Method Overview

Neural Isometries are an architecture-agnostic autoencoder framework which learns to map pairs of observations related in world space to latents which are related by an approximately isometric FM \(\) - see Fig. 2. We first formulate encoding and decoding, and define a FM in the latent space. Next, we show how a functional operator \(\) and mass matrix \(\) can be learned in the latent space to regularize \(\) by requiring it to be an isometry. Specifically, for observations sharing some potentially unknown relationship in world space, we enforce there exist a FM \(\) between their encodings satisfying two key properties: 1) That \(\) preserves the functional inner product in the latent space determined by \(\); and 2). \(\)_commutes_ with the functional operator \(\). Subsequently, we show that such maps can be recovered analytically through a differentiable, closed-form least squares solve in the operator eigenbasis. Last, we formulate NIso as an optimization problem incorporating both the strictness of the isometric correspondence between latents and the eigenvalue multiplicity of the operator, the latter of which controls the structure of the maps.

In experiments, we demonstrate that NIso are capable of both discovering approximations of known operators and constructing latent spaces where complex, non-linear symmetries in the observation space manifest equivalently as isometries. We show how the latter property can be exploited by demonstrating that a simple vector neuron MLP  acting in our pre-trained latent space can achieve results on par with state-of-the-art handcrafted equivariant networks operating in observation space. Subsequently, we consider the task of pose estimation, and demonstrate that robust \((3)\) camera

Figure 2: **Overview of Neural Isometries (NIso).** NIso learn a latent space where transformations of observations manifest as isometries, achieved by regularizing the functional maps \(\) between latents to commute with a learned operator \(\), parameterized via its spectral decomposition into a mass matrix \(\), eigenfunctions \(\), and eigenvalues \(\) (sec. 4.1). Given two observations \(\) and \(T\) related by some unknown transformation \(T\) (in this case, camera motion in a 3D scene), they are first encoded into latent _functions_\(()\) and \((T)\) and projected into the operator eigenbasis. An isometric functional map \(_{}\) is estimated between them, and used to map one to the other. Losses promote isometry-equivariance in the latent space, reconstruction of transformed latents, and distinct, low-multiplicity eigenvalues \(\), with the latter encouraging a diagonal as possible \(_{}\). An optional spectral dropout layer can be applied before the basis unprojection to encourage a physically meaningful ordering of the learned spectrum (sec. 4.2).

poses can be extracted from latent transformations, serving as evidence that NIso encourages models to encode information about transformations in world space.

## 4 Neural Isometries

We consider an observation space \(O L^{2}(M,^{n})\) consisting of functions defined over some domain \(M\) (_e.g._ with \(M\) the plane and \(n=3\) for RGB images). Here, elements of \(O\) are in fact captures from some world space \(W\) with \(:W O\) representing the mechanism from which \(O\) is formed from \(W\). Furthermore, we assume there is a potentially unknown collection of phenomena \(\{T\}\) acting on the world space that relates observations. That is, for some \(w W\), \(=(w) O\), and denoting \(T(Tw)\), we assume that \(T\) is also in \(O\) and that we are able to associate it with \(\).

Additionally, we consider an autoencoder consisting of an encoder and decoder

\[:L^{2}(M,^{n}) L^{2}(N,^{d}) :L^{2}(N,^{d}) L^{2}(M,^{n}) \]

mapping between observation space and a space of latent functions over some domain \(N\). In practice, we operate over discretizations of \(M\) and \(N\), with \( O\) and \(() L^{2}(N,^{d})\) represented as tensors \(^{|M| n}\) and \(()^{|N| d}\). For example, if \(M\) consists of the pixel indices of an image, then \(N\) could be grid or token indices if \(\) is a ConvNet or ViT, respectively.

Goal: Equivariance of Latent Functions.Our aim is to train the autoencoder such that for any \(T\) acting in the world space and corresponding observations \(,T O\), there exists a linear map \(:L^{2}(N,^{d}) L^{2}(N,^{d})\) such that

\[(T)(). \]

In other words, we desire our latent space to be _equivariant_ under world space transformations. As our problem is discrete, \(\) is a _functional map_ - an \(|N||N|\) matrix representation of maps on \(L^{2}(N,^{d})\). In the case of latents with \(|N|=H W\) pixels, \(\) is a matrix whose rows express each pixel in \((T)\) as a linear combination of pixels in \(()\), similar to the weight matrix one might obtain from a cross-attention operation.

### Regularization Through Isometries

We will find \(\) by solving a least-squares problem of the form \(=_{}(T)-()\). Unfortunately, as we will show in experiments, a direct solve without additional regularization leads to uninformative maps that capture little information about the actual world-space transformations \(T\). To add structure, we might ask that \(\) be _orthogonal_ with \(^{}=I_{|N|}\), generating gradients promoting latent codes having the property \(()\)\(\)\((T)\).

However, we can obtain more structure yet. We propose to learn a representation of the latent geometry by jointly regressing a diagonal mass matrix \(\) and positive semi-definite (**PSD**) operator \(^{|N||N|}\) such that \(\) manifests as an _isometry_. That is, \(\) preserves the functional inner product defined by \(- f,g_{}=f^{}g\) for \(f,g L^{2}(N,^{d})\) - and is \(\)-commutative with

\[^{}==. \]

Together, the conditions in Equation (3) form a strong regularizer, the effects of which are best seen in the expression of \(\) in the eigenspectrum of \(\). As \(\) is a PSD matrix with respect to the inner product defined by \(\), it can be expressed in terms of its spectral decomposition as

\[=^{}^{} =I_{|N|}, \]

where \(=(\{_{i}\}_{1 i|N|})\) is the diagonal matrix of (non-negative) eigenvalues and \(^{|N||N|}\) is the matrix whose columns are the \(\)-orthogonal eigenfunctions of \(\). Denoting

\[_{}^{}\,\, \]

as the projection of \(\) into the eigenbasis, it can be shown that the conditions in Eq. (3) reduce to \(_{}\) being orthogonal and \(\)-commutative . This is equivalent to asking that \(_{}\) be both _sparse_ and _condensed_ in that it forms an _orthogonal, block-diagonal matrix_, with the size of each block determined by the multiplicity of the eigenvalues in \(\).

### Estimating \(\) and End-to-End Optimization

Fig. 2 visualizes Neural Isometries's training loop. First, \(\) is estimated between pairs of encoded \(T\)-related observations such that it approximately satisfies the conditions for an \(\)-isometry as in Eq. (3). Second, the weights of the autoencoder, \(\), and \(\) are jointly updated with respect to a combined loss term, promoting: a) latent equivariance as in Eq. (2), b) the ability of the decoder to reconstruct observations, and c) distinct eigenvalues \(\) which encourage a diagonal-as-possible \(_{}\).

Recovering \(\) Between Latents.Instead of estimating \(\) directly, we equivalently estimate \(_{}\) in the eigenbasis of \(\), motivated by the corresponding simplification of the conditions in Eq. (3). Let

\[_{}^{} \]

be the map given by encoding followed by projection into the eigenbasis of \(\). Then, given observations \(\) and \(T\), we define \(_{}\) to be the solution to the least squares problem

\[_{}==I,=}{} \|\,_{}()-_{}(T)\|. \]

While Eq. (7) has an exact analytical solution , we instead approximate \(_{}\) with a fuzzy analogue which we find better facilitates backwards gradient flow to the parameters of \(\).

Specifically, letting \(:^{|N||N|}(|N|)\) denote the Procrustes projection to the nearest orthogonal matrix (_e.g._ through the SVD), we recover \(_{}\) via the approximation

\[_{}(P_{}_{}(T) [_{}()]^{}), \]

where \([P_{}]_{ij}=(-|_{i}-_{j}|)\) is a smooth multiplicity mask over the eigenvalues \(\) applied element-wise. See the supplement for details.

To facilitate the recovery of \(_{}\), we parameterize \(\) directly by its diagonal elements and \(\) in terms of its spectral decomposition, learning a \(\)-orthogonal matrix of eigenfunctions \(\) and non-negative eigenvalues \(\). This has the added benefit of enabling a low-rank approximation of \(\) by parameterizing only the first \(k\) eigenvalues and eigenfunctions, _i.e._\(^{|N| k}\) and \(=(\{_{i}\}_{1 i k})\) with \(k|N|\), mirroring similar approaches in SoTA FM pipelines . This reduces the complexity of the orthogonal projection in Eq. (8) from \(|N||N|\) to \(k k\).

Optimization.During training, the autoencoder is given pairs of \(T\)-related observations \((,T)\), which are mapped to the latent space and \(_{}\) is estimated as in Eq. (8) giving \(=\,_{}\,^{}\). First, an **equivariance loss** is formed between the eigenspace projections of the encodings,

\[_{E}=\|_{}\,_{}()-_{ }(T)\|. \]

We note that for full rank \(\), this loss is equivalent to measuring the degree to which the equivariance condition in Eq. (2) holds due to the orthogonality of \(\). Next we compute a **reconstruction loss**,

\[_{R}=\|(\,())-T\|+\|( ^{-1}\,(T))-\|, \]

with \(^{-1}=_{}{}^{}^{}\), forcing the decoder to map the transformed latents to the corresponding \(T\)-related observations. Last, we formulate a **multiplicity loss** which promotes distinct eigenvalues and ensures \(\) is "interesting" by preventing it from regressing to the identity. We observe that the eigenvalue mask \(P_{}\) can be viewed as a graph in which the number of connected components (_i.e._ the number of distinct eigenvalues) is equivalent to the dimension of the nullspace of the graph Laplacian \(_{P_{}}\) formed from the mask . As a measure of the nullspace dimension, we use the norm of the eigenvalues of \(_{P_{}}\), given by \(_{M}=\|_{P_{}}\|\). The NFT  takes a similar approach, wherein a diagonalization loss is imposed on estimated transformations themselves, though our experiments show it be far less effective in enforcing structure. The total loss is the sum of aforementioned terms

\[=_{R}+_{E}+_{M}, \]

with \(, 0\) weighting the contributions of the equivariance and multiplicity losses.

In experiments, we also consider a similar triplet regime as proposed in , where the autoencoder is given triples of \(T\)-related observations \((,T,T^{2})\) (assuming \(T\) is composable) and the estimated map \(\) between the encodings of \((,T)\) is used to form equivariance and reconstruction losses between \((T,T^{2})\) and vice-versa. This works to prevent \(\) from "cheating" by encodingprivileged information about the relationship between pairs beyond \(T\), a property we show to be critical in enforcing a useful notion of latent equivariance. However, triples of \(T\)-related observations are rare in practical settings, and we show in experiments that a major benefit of our isometric regularization is that our multiplicity loss (promoting a diagonal-as-possible and thus sparse and condensed \(_{}\)) can serve as an effective substitute for access to triples.

Spectral Dropout.While the composite loss in Equation (11) promotes latent equivariance and distinct eigenvalues, it does not, however, promote a physically meaningful ordering of the eigenvalues - _i.e._ that small eigenvalues correspond to smooth, low-frequency eigenfunctions and larger eigenvalues to eigenfunctions with sharper, high-frequency details. Though such an ordering amounts to a permutation in the spectral dimension and is not a necessary condition for the existence of diagonalized isometric maps, it could be potentially useful in downstream applications where a classical notion of eigenvalues as frequencies is desired.

To this end we propose an _optional_ spectral dropout layer applied _after_ computing the equivariance loss \(_{E}\) and _before_ the basis unprojection. The layer is implemented as follows: During training, there is a \(50\%\) chance that dropout will be applied to given example in a batch. Then, for each such example, a spectral index \(1<i k\) is randomly chosen and all features with index \(j i\) are masked out. Intuitively, this forces the decoder to produce the best possible reconstructions with the lowest frequency eigenfunctions, which appear most often and thus must capture large scale features, while reserving higher frequencies for filling in fine details.

### A Simple Example: Approximating the Toric and Spherical Laplacians

We demonstrate that NIso are able to learn a compact representation of isometries that reflect the dynamics of transformations in world space. To do so, we perform two experiments in which we consider pairs of observations formed by \(16 16\) images from ImageNet  viewed functions on toric and spherical grids, and respectively transformed by random circular shifts and SO(3) rotations to form pairs. Thus pairs are related by the isometries of the torus and sphere which commute with the Laplacian on each domain. Taking the encoder and decoder to be the identity map (making the equivariance and reconstruction losses equivalent) we optimize for \(,^{256 256}\) via the pairwise training procedure described in sec. 4.2, including the use of spectral dropout. For the toric experiments, we learn a full-rank parameterization of \(\) with \(k=256\); for the sphere, we learn a

Figure 3: **Approximating the Laplacian.** Forced to map between shifted images on the torus (first row, left) and rotated images on the sphere (second row, left), NIso regress operators (center right) structurally similar to the toric and spherical Laplacian (right). Maps \(_{}\) between projected images are strongly diagonal (center left), with individual blocks (inset) preserving the subspaces spanned by eigenfunctions (center, first \(64\) shown) sharing nearly the same eigenvalues. These experiments result in the discovery of basis with the similar properties to the the toric and spherical harmonics. In particular, the estimated spherical \(_{}\) manifest _exactly_ the same structure as the ground truth Wigner-D matrices corresponding to the rotation, with square blocks of size \((2+1)(2+1)\) for the \(\)-th distinct eigenvalue. Please zoom in to view structural details.

low-rank approximation with \(k=64\). As seen in Fig 3, NIso regresses operators with significant structural similarities to the Laplacian matrices formed by the standard \(3 3\) stencil on the torus and the low rank approximation using the first \(64\) spherical harmonics. In both cases, NIso recovers an eigenspace that diagonalizes \(_{}\) between shifted images, with eigenfunctions ordered by their energy. As our approach is data-driven and our estimated maps are only _approximately_ isometric, our learned operator and its eigenspectrum do not perfectly correspond to the ground-truth Laplacian. Instead, we are able to characterize similar, non-trivial spatial relationships that are preserved under shifts.

### Representation Learning with NIso

Viewed in terms of representation learning, NIso can be seen as a recipe for the self-supervised pre-training of a network backbone \(\) satisfying the equivariance condition in Eq. (2) such that transformations \(T\) in the world space manifest as isometries \(\) in the sense of Eq. (3).

Exploiting Equivariance in Latent SpaceAs we demonstrate in experiments, a simple off-the-shelf isometry-equivariant head can be appended to the pre-trained backbone and fine-tuned to achieve competitive results in tasks with challenging symmetries. We employ a simple strategy wherein a low rank \(k\) approximation of \(\) is learned during the pre-training stage. Thus, the eigenspace projections of the encodings of \(T\)-related observations are \(k d\) tensors that are nearly equivalent up to an _orthogonal_ transformation \(_{}\). As such, we pass the projected encodings to a head consisting of an O(\(k\))-equivariant vector neuron (**VN**) MLP . We note that for large-scale tasks NIso is potentially well-suited to pair with DiffusionNet , which can make use of the learned eigenbasis to perform accelerated operations in the latent space, though we do not consider this regime here.

Pose Extraction from Latent Isometries.We propose that the recovered functional maps \(\) encode information about world-space transformations \(T\). To test this, we consider a simple pose estimation paradigm consisting of a pre-training phase and fine-tuning phase. In the first phase, a NIso autoencoder is trained using \(T\)-related pairs of observations consisting of adjacent frames from video sequences. Subsequently, the decoder is discarded and the same pairs of observations are considered during fine-tuning. In the second phase, isometries \(_{}\) are estimated between the eigenspace projections of the encoded observations, vectorized, and passed directly to an MLP which predicts the parameters of the SE(\(3\)) transformation corresponding to the relative camera motion in world space between the adjacent frames. In our experiments, the weights of the NIso backbone are frozen during fine-tuning to better evaluate the information about world space transformations encoded during the unsupervised pre-training phase. At evaluation, trajectories are recovered by composing estimated frame-to-frame poses over the length of the sequence.

## 5 Experiments

In this section, we provide empirical evidence through experiments that NIso 1) recovers a general-purpose latent space that can be exploited by isometry-equivariant networks to handle challenging symmetries (5.1, 5.2); and 2) NIso encodes information about transformations in world space through the construction of isometric maps in the latent space from which geometric quantities such as camera poses can be directly regressed (5.3). Here we pre-train NIso without spectral dropout, as the ordering of the learned spectrum is irrelevant in our target applications and we find it slightly decreases accuracy of the prediction head. We provide reproducibility details in the supplement in addition to experiments quantitatively evaluating the degree of learned equivariance. We note that a consistent theme in our experiments are comparisons against the unsupervised variant of the NFT  (the semi-supervised variants cannot be applied because the symmetries we consider have no finite-dimensional IRs). Like our approach, the NFT seeks to relate latents via linear transformations though, it differs fundamentally in that maps are guaranteed additional structure only if the world space transformations are a compact group action. While not originally proposed by the authors, we evaluate it in place of our approach in the same self-supervised representation learning regimes discussed in sec. 4.4. Thus the role of these comparisons is to show that our proposed _isometric_ regularization better and more consistently provides a tractable and informative latent space.

### Homography-Perturbed MNIST

In our first set of experiments, we consider classification on the homNIST dataset  consisting of homography-perturbed MNIST digits. Following the procedure outlined in sec. 4.4, the classification network consists of a pre-trained NIso encoder backbone followed by a VN-MLP. Specifically, pre-training is performed by randomly sampling homographies from the distribution proposed in  which are applied to the elements of the standard MNIST training set to create pairs of observations. Here the weights of the encoder backbone are _frozen_ and the equivariant head is trained only on the _original_ (unperturbed) MNIST training set and evaluated on the _perturbed_ test set. Thus the aim of these experiments is to directly quantify the degree to which pre-trained latent space is both equivariant _and_ distinguishable.

With this in mind, we perform three ablations. In the first, we train the NIso autoencoder in a triplet regime (4.2) made possible by the synthetic parameterization of \(T\) as homographies. In the second and third, we train the autoencoder in the standard pairwise regime without considering the equivariance loss \(_{E}\) and multiplicity loss \(_{M}\), respectively. Additionally we compare the efficacy of our approach versus an NFT backbone pre-trained in the same manner. Last, we pre-train and evaluate a baseline backbone which considers only a reconstruction loss without respect to \(T\)-related pairs, but trains with data-augmentation during the fine-tuning phase by applying randomly sampled homographies.

Results are shown in Tab. 1, averaged over five randomly initialized pre-training and fine-tuning runs with standard errors. Also included are those reported by homConv  and LieDecomp , top-performing homography-equivariant networks which serve as a handcrafted baseline. While NIso pre-trained with the triplet regime produces results on par with the handcrafted baselines, pre-training with the pairwise regime-which reflects a real-world scenario--achieves a classification accuracy above 90\(\%\), significantly better than all but the three aforementioned approaches. Critically, performance drops when the multiplicity loss \(_{M}\) is omitted, which corresponds to a regime where \(\) must only preserve the inner product and \(\) converges to a multiple of the identity operator. This suggests that sparsifying \(_{}\) by filtering it through a low-multiplicity eigenspace (_i.e._ enforcing that it is an "interesting" isometry) is fundamental in forcing the network to disentangle the structure of the observed transformations from the content of the observations themselves. In the same vein, the equivariance loss \(_{M}\) is also clearly instrumental, as the reconstruction loss alone does not explicitly enforce that \(()\) is in fact mapped to \((T)\) under the estimated \(\). Furthermore, while the authors report that latent maps tend to converge to orthogonal maps for compact group actions in world space [36; 37], both NFT regimes preform poorly, implying that the learned maps do not replicate the properties of finite-dimensional IRs when the group is non-compact.

### Conformal Shape Classification

Next, we apply NIso to classify conformally-related 3D shapes from the augmented SHREC '11 dataset [7; 46]. We follow  by mapping each mesh to the sphere and subsequently rasterizing to a grid. During pre-training, \(T\)-related pairs are selected from the sets of conformally-augmented meshes derived from the same base shape in the train split. In the fine-tuning phase, the encoder weights are unfrozen and are jointly optimized with the equivariant head, representing the practical implementation of our proposed approach for equivariant tasks.

Results are shown in Tab. 2, averaged over five randomly initialized pre-training and fine-tuning runs with standard errors. NIso outperforms the NFT and the autoencoder baseline (with random Mobius transformations applied during the fine-tuning phase) in addition to Mobius Convolutions (**MC**) , a SoTA handcrafted spherical network equivariant to Mobius transformations. We consider this dataset to present a significant challenge as shape classes are roughly conformally-related and thus the maps between their spherical parameterizations are only approximated by Mobius transformations.

    & Acc. \\ 
**NIso** & 92.52 (\(\) 0.91) \\ w/ triplet & 97.38 (\(\) 0.23) \\ w/o \(E_{E}\) & 77.30 (\(\) 2.56) \\ w/o \(_{M}\) & 45.27 (\(\) 1.20) \\  NFT  & 41.93 (\(\) 0.84) \\ w/ triplet & 67.15 (\(\) 1.10) \\  AE w/ aug. & 80.96 (\(\) 1.95) \\ homConv  & 95.71 (\(\) 0.09) \\ LieDecomp  & **98.30 (\(\) 0.10)** \\   

Table 1: **Hom. MNIST.**

    & Acc. \\ 
**NIso** & **90.26 (\(\) 1.27)** \\ NFT  & 83.24 (\(\) 2.03) \\ AE w/ aug. & 69.36 (\(\) 2.81) \\ MC  & 86.5 \\   

Table 2: **Conf. SHREC \({}^{}\)11.**

[MISSING_PAGE_FAIL:9]

against two strong representation learning baselines. We extract features from both images using two pre-trained state-of-the-art vision foundation models -- DINOv2 and BeIT -- and pass the tokens into the modified DUST3R-style decoder which is trained to predict the pose.

We additionally train and evaluate an ablative version of NIso which does not learn an operator. Here \(\) is predicted directly between encodings and is required only to be orthogonal with the equivariance and reconstruction losses alone enforced during training. We note that computing a block-diagonalization loss on \(\) directly lacks justification as it would promote maps that preserve contiguous chunks of spatial indices in the latent tensors which are ordered arbitrarily. Thus, this regime serves to evaluate the degree to which the regularization through learned \(\)-commutativity forces the latent transformations to reflect geometric relationships in world space.

Results are shown in Tab. 3, averaged over five randomly initialized pre-training and fine-tuning runs with standard errors. All methods perform similarly with \(0\) frame skip but diverge afterwards, with NIso achieving significantly lower ATE values as the skip length increases. Notably, the ablative version of our method is consistently among the worst performers, suggesting that isometric regularization is also critical to encode information about world space transformations. Overall the NFT achieves the second best performance, slightly outperforming NIso at 0 frame skip but diverging thereafter. However, the latent maps it recovers are neither sparse nor exhibit condensed structure (Tab. 3, center left), and we hypothesize that its inability to effectively regularize maps beyond linearity makes it difficult for the network to discover a consistent transformation model that applies across scales. This could cause the network to focus on a specific regime at the expense of others, which may explain its relatively strong performance at 0 frame skip. The transformer baseline and representation learners are also highly flexible, and an analogous line of reasoning could explain their similar error profile.

## 6 Discussion

Limitations.A key factor limiting the broader applicability of our approach to geometry processing and graph-based tasks is an inability to learn and transfer an operator between domains with varying connectivity. In addition, NIso does not explicitly handle partiality or occlusion between input pairs which are ubiquitous in real-world data and likely degrade its performance in the pose estimation task. We seek to address these limitations in future work.

Conclusion.In this paper we introduce Neural Isometries, a method which converts challenging observed symmetries into isometries in the latent space. Our approach forms an effective backbone for self-supervised representation learning, enabling simple off-the-shelf equivariant networks to achieve strong results in tasks with complex, non-linear symmetries. Furthermore, isometric regularization produces latent representations that are geometrically informative by encoding information about transformations in world space, and we demonstrate that robust camera poses can be extracted from the isometric maps between latents in a general baseline setting.

Acknowledgements.Use of the CO3Dv2 dataset is solely for benchmarking purposes and limited exclusively to the experiments described in sec. 5.3. We thank Ishaan Chandratreya and Hyunwoo Ryu for helpful discussions and David Charatan for his aesthetic oversight and for granting us permission to use his images captured for FlowMap  in Fig. 1-2.

Vincent Sitzmann was supported by the National Science Foundation under Grant No. 2211259, by the Singapore DSTA under DST000ECI20300823 (New Representations for Vision and 3D Self-Supervised Learning for Label-Efficient Vision), by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) under 140D0423C0075, by the Amazon Science Hub, and by IBM.