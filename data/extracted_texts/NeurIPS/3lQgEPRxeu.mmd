# Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

 Qinbo Bai

Purdue University

West Lafayette, IN 47906

bai113@purdue.edu &Washim Uddin Mondal

Indian Institute of Technology Kanpur

Kanpur, UP, India 208016

wmondal@iitk.ac.in &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47906

vaneet@purdue.edu

Qinbo Bai

Purdue University

West Lafayette, IN 47906

bai113@purdue.edu &Washim Uddin Mondal

Indian Institute of Technology Kanpur

Kanpur, UP, India 208016

wmondal@iitk.ac.in &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47906

vaneet@purdue.edu

###### Abstract

This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDPs). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, our proposed algorithm achieves \(}(T^{4/5})\) objective regret and \(}(T^{4/5})\) constraint violation bounds.

## 1 Introduction

The framework of Reinforcement Learning (RL) is concerned with a class of problems where an agent learns to yield the maximum cumulative reward in an unknown environment via repeated interaction. RL finds applications in diverse areas, such as wireless communication, transportation, and epidemic control [1; 2; 3]. RL problems are mainly categorized into three setups: episodic, infinite horizon discounted reward, and infinite horizon average reward. Among them, the infinite horizon average reward setup is particularly significant for real-world applications. It aligns with most of the practical scenarios and captures their long-term goals. Some applications in real life require the learning procedure to respect the boundaries of certain constraints. In an epidemic control setup, for example, vaccination policies must take the supply shortage (budget constraint) into account. Such restrictive decision-making routines are described by constrained Markov Decision Processes (CMDP) [4; 5; 6]. Existing papers on CMDPs utilize either a tabular or a linear MDP structure. This work provides the first algorithm for an infinite horizon average reward CMDP with general parametrization and proves its sub-linear regret and constraint violation bounds.

There are two primary ways to solve a CMDP problem in the infinite horizon average reward setting. The first one, known as the model-based approach, involves constructing estimates of the transition probabilities of the underlying CMDP, which are subsequently utilized to derive policies [6; 7; 5]. The caveat of this approach is the large memory requirement to store the estimated parameters, which effectively curtails its applicability to CMDPs with large state spaces. The alternative strategy, known as the model-free approach, either directly estimates the policy function or maintains an estimate of the \(Q\) function, which is subsequently used for policy generation . Model-free algorithms typically demand lower memory and computational resources than their model-based counterparts. Although the CMDP has been solved in a model-free manner in the tabular  and linear  setups, its exploration with the general parameterization is still open and is the goal of this paper.

General parameterization indexes the policies by finite-dimensional parameters (e.g., weights of neural networks) to accommodate large state spaces. The learning is manifested by updating these parameters using policy gradient (PG)-type algorithms. Note that PG algorithms are primarily studied in discounted reward setups. For example,  characterizes the sample complexities of the PG and the Natural PG (NPG) algorithms with softmax and direct parameterization. Similar results for general parameterization are obtained by . The regret analysis of a PG algorithm with the general parameterization has been recently performed for an infinite horizon average reward MDP without constraints . Similar regret and constraint violation analysis for the average reward CMDP is still missing in the literature. In this paper, we bridge this gap.

**Challenges and Contribution:** We propose a PG-based algorithm with general parameterized policies for the average reward CMDP and establish its sublinear regret and constraint violation bounds. In particular, assuming the underlying CMDP to be ergodic, we demonstrate that our PG algorithm achieves an average optimality rate of \(}(T^{-})\) and average constraint violation rate of \(}(T^{-})\). Invoking this convergence result, we establish that our algorithm achieves regret and constraint violation bounds of \(}(T^{})\). Apart from providing the first sublinear regret guarantee for the average reward CMDP with general parameterization, our work also improves the state-of-the-art regret guarantee, \(}(T^{5/6})\) in the model-free tabular setup .

Despite the availability of sample complexity analysis of PG algorithms with constraints in the discounted reward setup  and PG algorithms without constraint in average reward setup , obtaining sublinear regret and constraint violation bounds for their average reward counterpart is challenging.

\(\) solely needs an estimate of the value function \(V\) while we additionally need the estimate of the gain function, \(J\).

\(\) assume access to a simulator to generate unbiased value estimates. In contrast, our algorithm uses a sample trajectory of length \(H\) to estimate the values and gains and does not assume the availability of a simulator.

\(\)The first-order convergence analysis (Lemma 6) differs from that in . Note that both of these papers use an ascent-like inequality. In , this bounds the term \(J(_{k+1})-J(_{k})\). The final result is obtained by calculating a sum over \(k\) which cancels the intermediate terms and leaves us with \(J(_{K})-J(_{1})\). We would like to emphasize that the cancellation of the intermediate terms is crucial to establishing the result. However, a similar effort in our case only leads to a bound of \(J_{}(_{k+1},_{k})-J_{}(_{k},_{k})\). Note that directly performing a sum over this difference does not lead to the cancellation of intermediate terms. We had to take a different route and apply the bounds of the Lagrange multipliers and the estimate of the constraint function to achieve that goal.

\(\)After solving the problems mentioned above, we prove \(}(T^{-})\) convergence rate of the Lagrange function. Unfortunately, the strong duality property, which is central to proving convergence results of CMDPs for tabular and softmax policies, does not hold under the general parameterization. As a result, the convergence result for the dual problem does not automatically translate to that for the primal problem, which is a main difference from . We overcome this barrier by introducing a novel constraint violation analysis and a series of intermediate results (Lemma 16-18) that help disentangle the regret and constraint violation rates from the Lagrange convergence. It is important to mention that although the techniques applied are inspired by the , those techniques cannot be directly adopted for average reward MDPs. This is primarily because the estimate \(_{c}(_{k})\) is biased in the average case. To the best of our knowledge, constraint violation analysis with a biased estimate of the cost value is not available in the literature and is performed for the first time in our paper.

  Algorithm & Regret & Violation & Model-free & Setting \\  Algorithm 1 in  & \(}()\) & \(}()\) & No & Tabular \\  Algorithm 2 in  & \(}(T^{2/3})\) & \(}(T^{2/3})\) & No & Tabular \\  UC-CURL and PS-CURL  & \(}()\) & \(0\) & No & Tabular \\  Algorithm 2 in  & \(}((dT)^{3/4})\) & \(}((dT)^{3/4})\) & No & Linear MDP \\  Algorithm 3 in  & \(}()\) & \(}()\) & No & Linear MDP \\  Triple-QA  & \(}(T^{5/6})\) & \(0\) & Yes & Tabular \\  This paper & \(}(T^{})\) & \(}(T^{})\) & Yes & General Parameterization \\  

Table 1: This table summarizes the different model-based and mode-free state-of-the-art algorithms available in the literature for average reward CMDPs. We note that our proposed algorithm is the first to analyze the regret and constraint violation for average reward CMDP with general parametrization. Here, the parameter \(d\) refers to the dimension of the feature map for linear MDPs.

* Due to the presence of the Lagrange multiplier, the convergence analysis of a CMDP is much more convoluted than its unconstrained counterpart. The learning rate of the Lagrange update, \(\), turns out to be pivotal in determining the growth rate of regret and constraint violation. Low values of \(\) push the regret down while simultaneously increasing the constraint violation. Finding the optimal value of \(\) that judiciously balances these two competing goals is one of the cornerstones of our analysis.

**Related work for unconstrained average reward RL:** In the absence of constraints, both model-based and model-free tabular setups have been widely studied for infinite horizon average reward MDPs. For example, the model-based algorithms proposed by [15; 16] achieve the optimal regret bound of \(}()\). Similarly, the model-free algorithm proposed by  for tabular MDP results in \(}()\) regret. Regret analysis for average reward MDP with general parametrization has been recently studied in , where a regret bound of \(}(T^{3/4})\) is derived.

**Related work for constrained RL:** The constrained reinforcement learning problem has been extensively studied both for infinite horizon discounted reward and episodic MDPs. For example, discounted reward CMDPs have been recently studied in the tabular setup , with both softmax [14; 19], and general policy parameterization [14; 19; 4; 12]. Moreover, [20; 21; 22] investigated episodic CMDPs in the tabular setting.

Recently, the infinite horizon average reward CMDPs have been investigated in model-based setups [5; 6; 7], tabular model-free setting  and linear CMDP setting . For model-based CMDP setup,  proposed a model-based online mirror descent algorithm in the ergodic setting which achieves \(}()\) for regret and violation at the same time.  proposed algorithms based on the posterior sampling and the optimism principle that achieve \(}()\) regret with zero constraint violations in the ergodic setting. However, the above model-based algorithms cannot be extended to large state space. In the tabular model-free setup, the algorithm proposed by  achieves a regret of \(}(T^{5/6})\) with zero constraint violations. Finally, in the linear CMDP setting,  achieves \(}()\) regret bound with zero constraint violation. Note that the linear CMDP setting assumes that the transition probability has a certain linear structure with a known feature map which is not realistic. Table 1 summarizes all relevant works. Unfortunately, none of these papers study the infinite horizon average reward CMDPs with general parametrization which is the main focus of our article.

Additionally, for the weakly communicating setting,  proposed a model-based algorithm achieving \(}(T^{2/3})\) for both regret and violation in tabular case.  further extends such result to linear MDP setting with \(}(T^{3/4})\) regret and violation. In general, it is difficult to propose a model-free algorithm with provable guarantees for Constrained MDPs (CMDPs) without considering the ergodic model.  pointed out several extra challenges in Weakly communicating MDP compared to the ergodic case. For example, there is no uniform bound for the span of the value function for all stationary policies. It is also unclear how to estimate a policy's bias function accurately without the estimated model, which is an important step for estimating the policy gradient.

## 2 Formulation

This paper analyzes an infinite-horizon average reward constrained Markov Decision Process (CMDP) denoted as \(=(,,r,c,P,)\) where \(\) denotes the state space, \(\) is the action space of size \(A\), \(r:\) is the reward function, \(c:[-1,1]\) is the constraint cost function, \(P:^{||}\) is the state transition function where \(^{||}\) denotes a probability simplex with dimension \(||\), and \(^{||}\) is the initial distribution of states. A policy \(:^{A}\) maps the current state to an action distribution. The average reward and cost of a policy, \(\), is,

\[J^{}_{g,}_{T} _{t=0}^{T-1}g(s_{t},a_{t})s_{0},\] (1)

where \(g=r,c\) for average reward and cost respectively. The expectation is calculated over the distribution of all sampled trajectories \(\{(s_{t},a_{t})\}_{t=0}^{}\) where \(a_{t}(s_{t})\), \(s_{t+1} P(|s_{t},a_{t})\), \( t\{0,1,\}\). For notational convenience, we shall drop the dependence on \(\) whenever there is no confusion. Our goal is to maximize the average reward function while ensuring that the average cost is above a given threshold. Without loss of generality, we can mathematically write this problem as,

\[_{}\;J^{}_{r}\;\;\;J^{}_{c} 0\] (2)However, the above problem becomes difficult to handle when the underlying state space, \(\) is large. Therefore, we consider a class of parametrized policies, \(\{_{}|\}\) whose elements are indexed by a \(\)-dimensional parameter, \(^{}\) where \(||||\). Thus, the original problem in Eq (2) can be reformulated as the following parameterized problem.

\[_{}\;J_{r}^{_{}}\;\;\;J_{c}^{_{ }} 0\] (3)

We denote \(J_{g}^{_{}}=J_{g}()\), \(g\{r,c\}\) for notational convenience. Let, \(P^{_{}}:^{||}\) be a transition function induced by \(_{}\) and defined as, \(P^{_{}}(s,s^{})=_{a}P(s^{}|s,a)_{ }(a|s)\), \( s,s^{}\). If \(\) is such that for every policy \(\), the function, \(P^{}\) is irreducible and aperiodic, then \(\) is called ergodic.

**Assumption 1**.: The CMDP \(\) is ergodic.

Ergodicity is a common assumption in the literature . If \(\) is ergodic, then \(\), there exists a unique stationary distribution, \(d^{_{}}^{||}\) given as follows.

\[d^{_{}}(s)=_{T}_{t=0}^{T-1}(s _{t}=s|s_{0},_{})\] (4)

Ergodicity implies that \(d^{_{}}\) is independent of the initial distribution, \(\), and obeys \(P^{_{}}d^{_{}}=d^{_{}}\). Hence, the average reward and cost functions can be expressed as,

\[J_{g}()=_{s d^{_{}},a_{}(s)}[g(s,a) ]=(d^{_{}})^{T}g^{_{}}\] (5)

where \(g^{_{}}(s)_{a}g(s,a)_{}(a|s),\; g\{r,c\}\). Note that the functions \(J_{g}()\), \(g\{r,c\}\) are also independent of the initial distribution, \(\). Furthermore, \(\), there exist a function \(Q_{g}^{_{}}:\) such that the following Bellman equation is satisfied \((s,a)\).

\[Q_{g}^{_{}}(s,a)=g(s,a)-J_{g}()+_{s^{} P( |s,a)}[V_{g}^{_{}}(s^{})]\] (6)

where \(g\{r,c\}\) and \(V_{g}^{_{}}:\) is given as \(V_{g}^{_{}}(s)=_{a}_{}(a|s)Q_{g}^{_{ }}(s,a),\; s\). Note that if \(Q_{g}^{_{}}\) satisfies \(()\), then it is also satisfied by \(Q_{g}^{_{}}+c\) for any arbitrary, \(c\). To uniquely define the value functions, we assume that \(_{s}d^{_{}}(s)V_{g}^{_{}}(s)=0\). In this case, \(V_{g}^{_{}}(s)\) is given by,

\[V_{g}^{_{}}(s)=_{t=0}^{}_{s^{}} [(P^{_{}})^{t}(s,s^{})-d^{_{}}(s^{}) ]g^{_{}}(s^{})=_{t=0}^{}[\{g(s_{ t},a_{t})-J_{g}()\}|s_{0}=s]\] (7)

where the expectation is computed over all \(_{}\)-induced trajectories. In a similar way, \((s,a)\), one can uniquely define \(Q_{g}^{_{}}(s,a)\), \(g\{r,c\}\) as follows.

\[Q_{g}^{_{}}(s,a)=_{t=0}^{}[g(s_{t}, a_{t})-J_{g}()}s_{0}=s,a_{0}=a]\] (8)

Moreover, the advantage function \(A_{g}^{_{}}:\) is defined such that \(A_{g}^{_{}}(s,a) Q_{g}^{_{}}(s,a)-V_{g}^{_{ }}(s)\), \((s,a)\), \( g\{r,c\}\). Assumption 1 also implies the existence of a finite mixing time. Specifically, for an ergodic MDP, \(\), the mixing time is defined as follows.

**Definition 1**.: The mixing time, \(t_{}^{}\), of the CMDP \(\) for a parameterized policy, \(_{}\), is defined as, \(t_{}^{}t 1(P^{_{ }})^{t}(s,)-d^{_{}}, s}\). The overall mixing time is \(t_{}_{}t_{}^{}\). In this paper, \(t_{}\) is finite due to ergodicity.

Mixing time characterizes how fast a CMDP converges to its stationary state distribution, \(d^{_{}}\), under a given policy, \(_{}\). We also define the hitting time as follows.

**Definition 2**.: The hitting time of an ergodic CMDP \(\) with respect to a policy, \(_{}\), is defined as \(t_{}^{}_{s}[d^{_{}}(s) ]^{-1}\). The overall hitting time is defined as \(t_{}_{}t_{}^{}\). In this paper, \(t_{}\) is finite due to ergodicity as well.

Define \(^{*}\) as the optimal solution to the unparameterized problem (2). For a given CMDP \(\), and a time horizon \(T\), the regret and constraint violation of any algorithm \(\) is defined as follows.

\[_{T}(,)_{t=0}^{T-1}(J_{r}^{ ^{*}}-r(s_{t},a_{t})),\;_{T}(,) -_{t=0}^{T-1}c(s_{t},a_{t})\] (9)where the algorithm, \(\), executes the actions, \(\{a_{t}\}\), \(t\{0,1,\}\) based on the trajectory observed up to time, \(t\), and the state, \(s_{t+1}\) is decided according to the state transition function, \(P\). For simplicity, we shall denote the regret and constraint violation as \(_{T}\) and \(_{T}\) respectively. Our goal is to design an algorithm \(\) that achieves low regret and constraint violation bounds.

## 3 Proposed Algorithm

We solve (3) via a primal-dual algorithm based on the following problem.

\[_{}_{ 0}\ J_{}(,),\] (10)

where \(J_{}(,) J_{r}()+ J_{c}()\). The function, \(J_{}(,)\), is called the Lagrange function and \(\) the Lagrange multiplier. Our algorithm updates the pair \((,)\) following the policy gradient iteration as shown below \( k\{1,,K\}\) with an initial point \((_{1},_{1})\), \(_{1}=0\).

\[_{k+1}=_{k}+_{}J_{}(_{k}, _{k}),\ _{k+1}=_{[0,]}[_{k}- J_{c}( _{k})]\] (11)

where \(\) and \(\) are learning parameters and \(\) is the Slater parameter introduced in the following assumption. Finally, for any set, \(\), \(_{}[]\) denotes projection onto \(\). The assumption stated below ensures that we have at least one feasible interior point solution to (2).

**Assumption 2** (Slater condition).: There exists a \((0,1)\) and \(\) such that \(J_{c}()\).

Note that in (11), the dual update is projected onto the set \([0,]\) because the optimal dual variable for the parameterized problem is bounded in Lemma 16. The gradient of \(J_{}(,)\) can be computed by invoking a variant of the well-known policy gradient theorem .

**Lemma 1**.: _The gradient of \(J_{}(,)\) is computed as,_

\[_{}J_{}(,)=_{s d^{_{ }},a_{}(s)}A^{_{}}_{,}(s, a)_{}_{}(a|s)\]

```
1:Input: Episode length \(H\), learning rates \(,\), initial parameters \(_{1},_{1}\), initial state \(s_{0}()\),
2:\(K=T/H\)
3:for\(k\{1,,K\}\)do
4:\(_{k}\)
5:for\(t\{(k-1)H,,kH-1\}\)do
6: Execute \(a_{t}_{_{k}}(|s_{t})\)
7: Observe \(r(s_{t},a_{t})\), \(c(s_{t},a_{t})\) and \(s_{t+1}\)
8:\(_{k}_{k}\{(s_{t},a_{t})\}\)
9:endfor
10:for\(t\{(k-1)H,,kH-1\}\)do
11: Obtain \(^{_{_{k}}}_{,_{k}}(s_{t},a_{t})\) via Algorithm 2 and \(_{k}\)
12:endfor
13: Compute \(_{k}\) using (15)
14: Update the parameters: \[_{k+1}=_{k}+_{k},\] (12) \[_{k+1}=_{[0,]}_{k }-_{c}(_{k})\] \[_{c}(_{k})=_{t=(k-1)H+N}^{ kH-1}c(s_{t},a_{t})\]
15:endfor ```

**Algorithm 1** Primal-Dual Parameterized Policy Gradient

**Lemma 1**.: _The gradient of \(J_{}(,)\) is computed as,_

\[_{}J_{}(,)=_{s d^{_{ }},a_{}(s)}A^{_{}}_{,}(s, a)_{}_{}(a|s)\]

_where \((s,a)\), \(A^{_{}}_{,}(s,a) A^{_{}}_{r}(s,a)+  A^{_{}}_{c}(s,a)\), and \(\{A^{_{}}_{g}\}_{g\{r,c\}}\) are the advantage functions corresponding to reward and cost. In typical RL scenarios, learners do not have access to the state transition function, \(P\), and thereby to the functions \(d^{_{}}\) and \(A^{_{}}_{,}\). This makes computing the exact gradient a difficult task. In Algorithm 1, we demonstrate how one can still obtain good estimates of the gradient using sampled trajectories.

Algorithm 1 runs \(K\) epochs, each of duration \(H=16t_{}t_{}T^{}( T)^{2}\) where \((0,1)\) defines a constant whose value is specified later. Clearly, \(K=T/H\). Note that the learner is assumed to know the horizon length, \(T\). This can be relaxed utilizing the well-known doubling trick . Additionally, it is assumed that the algorithm is aware of the mixing time and the hitting time. This assumption is common in the literature [13; 17]. The first step in obtaining a gradient estimate is estimating the advantage value for a given pair \((s,a)\). This can be accomplished via Algorithm 2. At the \(k\)th epoch, a \(_{_{k}}\)-induced trajectory, \(_{k}=\{(s_{t},a_{t})\}_{t=(k-1)H}^{kH-1}\) is obtained and subsequently passed to Algorithm 2 that searches for subtrajectories within it that start with a given state \(s\), are of length \(N=4t_{}( T)\), and are at least \(N\) distance apart from each other. Assume that there are \(M\) such subtrajectories. Let the total reward and cost of the \(i\)th subtrajectory be \(\{r_{i},c_{i}\}\) respectively and \(_{i}\) be its starting time. The value function estimates for the \(k\)th epoch are

\[_{g}^{_{_{k}}}(s,a)=}(a|s)}[ _{i=1}^{M}g_{i}1(a_{_{i}}=a)],\;_{g}^{_{ _{k}}}(s)=_{i=1}^{M}g_{i},\;\; g\{r,c\}\] (13)

This leads to the following advantage estimator.

\[_{,_{k}}^{_{_{k}}}(s,a)=_{r}^{_{ _{k}}}(s,a)+_{k}_{c}^{_{_{k}}}(s,a),\] (14)

where \(_{g}^{_{_{k}}}(s,a)=_{g}^{_{_{k}}}(s,a)-_{g}^{_{_{k}}}(s)\), \(g\{r,c\}\). Finally, the gradient estimator is,

\[_{k}_{}J_{}(_{k},_{ k})=_{t=t_{k}}^{t_{k+1}-1}_{,_{k}}^{ _{_{k}}}(s_{t},a_{t})_{}_{_{k}}(a_{t}|s_{t})\] (15)

where \(t_{k}=(k-1)H\) is the starting time of the \(k\)th epoch. The parameters are updated following (12). To update the Lagrange multiplier, we need an estimation of \(J_{c}(_{k})\), which is obtained as the average cost of the \(k\)th epoch. It should be noted that we remove the first \(N\) samples from the \(k\)th epoch because we require the state distribution emanating from the remaining samples to be close enough to the stationary distribution \(d^{_{_{k}}}\), which is the key to make \(_{c}(_{k})\) close to \(J_{c}(_{k})\). The following lemma demonstrates that \(_{,_{k}}^{_{_{k}}}(s,a)\) is a good estimator of \(A_{,_{k}}^{_{_{k}}}(s,a)\).

```
1:Input: Trajectory \((s_{t_{1}},a_{t_{1}},,s_{t_{2}},a_{t_{2}})\), state \(s\), action \(a\), Lagrange multiplier \(\), and parameter \(\)
2:Initialize:\(M 0\), \( t_{1}\)
3:Define:\(N=4t_{}_{2}T\).
4:while\( t_{2}-N\)do
5:if\(s_{}=s\)then
6:\(M M+1,\;_{M}\)
7:\(g_{M}_{t=}^{+N-1}-14.226378ptg(s_{t},a_{t}),\;  g\{r,c\}\)
8:\(+2N\).
9:else
10:\(+1\).
11:endif
12:endwhile
13:if\(M>0\)then
14: Compute \(_{g}(s,a)\), \(_{g}(s)\) via (13), \( g\{r,c\}\)
15:else
16:\(_{g}(s)=0\), \(_{g}(s,a)=0,\;\; g\{r,c\}\)
17:endif
18:return\((_{r}(s,a)-_{r}(s))+(_{c}(s,a)-_{c}(s))\) ```

**Algorithm 2** Advantage Estimation

**Lemma 2**.: _The following inequality holds \( k\), \((s,a)\) and sufficiently large \(T\)._

\[_{,_{k}}^{_{_{k}}}(s, a)-A_{,_{k}}^{_{_{k}}}(s,a)^{2} (}N^{3} T}{^{2}H_{_{k}}(a|s)} )=(}^{2}( T)^{2}}{^{2}T ^{}_{_{k}}(a|s)})\] (16)

[MISSING_PAGE_FAIL:7]

**Lemma 4**.: _With a slight abuse of notation, let \(J_{}(,)=J_{r}^{}+ J_{}^{}\). For any two policies \(\), \(^{}\), the following result holds \(>0\)._

\[J_{}(,)-J_{}(^{},)=_{s  d^{*}}_{a(s)}A_{,}^{^{} }(s,a)\]

We now present a general framework for the convergence analysis of Algorithm 1.

**Lemma 5**.: _If the policy parameters, \(\{_{k},_{k}\}_{k=1}^{K}\) are updated via (12) and assumptions 3, 4,and 5 hold, then we have the following inequality for any \(K\),_

\[_{k=1}^{K}J_{}(^{ *},_{k})-J_{}(_{k},_{k}) }}+_{k}^{K} \|(_{k}-_{k}^{*})\|+_{k=1}^{K} \|_{k}\|^{2}\] \[+_{s d^{*^{*}}}[KL(^{*}( |s)\|_{_{1}}(|s))]\]

_where \(_{k}^{*}:=_{_{k},_{k}}^{*}\), \(_{_{k},_{k}}^{*}\) is defined in (19), and \(^{*}\) is the optimal solution to the problem (2)._

Lemma 5 proves that the optimality error of the Lagrange sequence can be bounded by the average first-order and second-order norms of the intermediate gradients. Note the presence of \(_{}\) in the result. If the policy class is severely restricted, the optimality bound loses its importance. Consider the expectation of the second term in (20). Note that,

\[_{k=1}^{K}\|_{k}-_{k }^{*}\|^{2}_{k=1}^{K}\|_{k }-_{k}^{*}\|^{2}=_{k=1}^{K}\| _{k}-F_{}(_{k})^{}_{}J_{}( _{k},_{k})\|^{2}\] \[_{k=1}^{K}\|_ {k}-_{}J_{}(_{k},_{k})\|^{2}+ \|_{}J_{}(_{k},_{k})-F_ {}(_{k})^{}_{}J_{}(_{k},_ {k})\|^{2}}\] \[_{k=1}^{K} \|_{k}-_{}J_{}(_{k},_{k})\|^{2} +_{k=1}^{K}1+^{2}} \,\|_{}J_{}(_{k},_{k})\|^ {2}\]

where \((a)\) follows from Assumption 5. The expectation of the third term in (20) can be bounded as

\[_{k=1}^{K}\|_{k}\|^{2} {1}{K}_{k=1}^{K}[\|_{}J_{}(_{k},_{k})\|^{2}]+_{k=1}^{K}\| _{k}-_{}J_{}(_{k},_{k})\|^{2}\]

In both (4) and (20), \(\|_{k}-_{}J_{}(_{k},_{k}) \|^{2}\) is bounded above by Lemma 3. To bound the term, \(\|_{}J_{}(_{k},_{k})\|^{2}\), the following lemma is applied.

**Lemma 6**.: _Let \(J_{g}()\) be L-smooth, \( g\{r,c\}\) and \(=)}\). Then the following holds._

\[_{k=1}^{K}\|_{}J_{}(_{k},_{k })\|^{2}K}+_{k=1}^{K}\|_{ }J_{}(_{k},_{k})-_{k}\|^{2}+\] (20)

Note the presence of \(\) in (20). To ensure convergence, \(\) must be a function of \(T\). Invoking Lemma 3, we get the following relation under the same set of assumptions and the choice of parameters as in Lemma 6.

\[_{k=1}^{K}\|_{}J_{}(_{k}, _{k})\|^{2}}(t_{}^{ 2}}{^{2}T^{}})+}(} t_{}}{^{2}T^{1-}})+\] (21)

Applying Lemma 3 and (21) in (20), we arrive at,

\[_{k=1}^{K}\|_{k}\|^{2} }(t_{}^{2}}{^{2}T^{}}+ }t_{}}{^{2}T^{1-}})+\] (22)

Similarly, using \(()\), we deduce the following.

\[_{k=1}^{K}\|_{k}-_{k}^{*}\|(1+ })+(1+})}(t_{}}{ T^{/2}}+}t_{}}}{ T^{(1-)/2}})\] (23)

Inequalities (22) and (23) lead to the following global convergence of the Lagrange function.

**Lemma 7**.: _Let \(\{_{k}\}_{k=1}^{K}\) be as described in Lemma 5. If assumptions 1\(-\)5 hold, \(\{J_{g}()\}_{g\{r,c\}}\) are \(L\)-smooth functions, \(=)}\), \(K=\), and \(H=16t_{ mix}t_{ hit}T^{}(_{2}T)^{2}\), then_

\[_{k=1}^{K}J_{ L}(^{*}, _{k})-J_{ L}(_{k},_{k}) G(1+})}(+Gt_{ mix }}{ T^{/2}}+t_{ hit}}}{ T^{(1-)/2 }})\] \[+}(t_{ mix}^{2} }{^{2}T^{}}+t_{ hit}}{^{2}T^{1-}}+ )+}t_{ hit}_{s d^{*}}[KL(^{*}(|s))\|_{_{1}}(|s))]}{T^{1-} }+}\]

Lemma 7 establishes that the average difference between \(J_{ L}(^{*},_{k})\) and \(J_{ L}(_{k},_{k})\) is \(}(+T^{-/2}+T^{-(1-)/2})\). Expanding the function, \(J_{ L}\), and utilizing the update rule of the Lagrange multiplier, we achieve the global convergence for the objective and the constraint in Theorem 1 (stated below). In its proof, Lemma 18 (stated in the appendix) serves as an important tool in disentangling the convergence rates of regret and constraint violation. Interestingly, Lemma 18 is built upon the strong duality property of the unparameterized optimization (2) and has no apparent direct connection with the parameterized setup.

**Theorem 1**.: _Consider the same parameters as in Lemma 7 and set \(=T^{-2/5}\), \(=2/5\). We have,_

\[_{k=1}^{K}J_{r}^{^{*}}-J_{r} (_{k})}+G^{2}t_{ mix }}{}(1+})}(T^{-1/5})\] \[_{k=1}^{K}-J_{c}(_{k}) }+}(t_{ hit}}{ T^{1/5}})+G^{2}t_{ mix}(1+ })}(T^{-1/5})\]

_where \(^{*}\) is a solution to (2). In the above bounds, we write only the dominating terms of \(T\)._

Theorem 1 establishes \(}(T^{-1/5})\) convergence rates for both the objective and the constraint violation.

## 5 Regret and Violation Analysis

In this section, we use the convergence result of the previous section to bound the expected regret and constraint violation of Algorithm 1. Note that the regret and constraint violation decompose as,

\[_{T} =_{t=0}^{T-1}(J_{r}^{^{*}}-r(s_{t},a_{t}))=H _{k=1}^{K}(J_{r}^{^{*}}-J(_{k}))+_{k=1}^{K}_{t _{k}}(J(_{k})-r(s_{t},a_{t}))\] \[_{T} =_{t=0}^{T-1}(-c(s_{t},a_{t}))=H_{k=1}^{K} (-J_{c}(_{k}))+_{k=1}^{K}_{t_{k}}( J_{c}(_{k})-c(s_{t},a_{t}))\]

where \(_{k}\{(k-1)H,,kH-1\}\). Observe that the expectation of the first terms in regret and violation can be bounded by Theorem 1. The expectation of the second term in regret and violation can be expanded as follows,

\[[_{k=1}^{K}_{t_{k}}(J_{ g}(_{k})-g(s_{t},a_{t}))][ _{k=1}^{K}_{t_{k}}_{s^{} P(|s_{ t},a_{t})}[V_{g}^{_{_{k}}}(s^{})]-Q_{g}^{_{_{k}}}(s_{t},a_{t})]\] \[[_{k=t}^{K}_{t _{k}}V_{g}^{_{_{k}}}(s_{t+1})-V_{g}^{_{_{k}}}(s_{t })]=[_{k=1}^{K}V_{g}^{_{_{k}}}(s_{kH})-V_{g}^ {_{_{k}}}(s_{(k-1)H})]\] (24) \[=[_{k=1}^{K-1}V_{g}^{_{_{k+1}}}(s_{kH })-V_{g}^{_{_{k}}}(s_{kH})]+[V_{g}^{_{_{K} }}(s_{T})-V_{g}^{_{_{0}}}(s_{0})]\]

where \(g\{r,c\}\). Equality \((a)\) uses the Bellman equation and \((b)\) follows from the definition of \(Q_{g}\). The first term in the last line of Eq. (24) can be upper bounded by Lemma 8 (stated below). On the other hand, the second term can be upper bounded as \((t_{ mix})\) using Lemma 9.

**Lemma 8**.: _If assumptions 1 and 3 hold, then for \(K=\) where \(H=16t_{}t_{}T^{}(_{2}T)^{2}\), the following inequalities hold \( k\), \((s,a)\) and sufficiently large \(T\):_

\[(a)|_{_{k+1}}(a|s)-_{_{k}}(a|s)| G \|_{k+1}-_{k}\|\] \[(b)_{k=1}^{K}|J_{g}(_{k+1})-J_{g}(_{k}) |}(}}[ (t_{}+)T^{}+}t_{}}T^{}])\] \[(c)_{k=1}^{K}|V_{g}^{_{_{k+1}}}(s_{k})-V_ {g}^{_{_{k}}}(s_{k})|}(}}{ t_{}}[(t_{}+ )T^{}+}t_{}}T^{ {3}{10}}])\]

_where \(g\{r,c\}\), and \(\{s_{k}\}_{k=1}^{K}\) is an arbitrary sequence of states._

Lemma 8 states that the obtained policy parameters are such that the average consecutive difference in the sequence \(\{J_{g}(_{k})\}_{k=1}^{K}\), \(g\{r,c\}\) decreases with time horizon, \(T\). We would like to emphasize that Lemma 8 works for both reward and constraint functions. Hence, we can prove our regret guarantee and constraint violation as shown below.

**Theorem 2**.: _If assumptions 1-5 hold, \(J_{g}()\)'s are \(L\)-smooth, \( g\{r,c\}\) and \(T\) are sufficiently large, then our proposed Algorithm 1 achieves the following expected regret and constraint violation bounds with learning rates \(=)}\) and \(=T^{-2/5}\)._

\[[_{T}]  T}}+}(T^{4/5} )+(t_{})\] (25) \[[_{T}]  T}}+}(T ^{4/5})+(t_{})\] (26)

The detailed expressions of these bounds are provided in the Appendix. Here, we keep only those terms that emphasize the order of \(T\). Note that our result outperforms the state-of-the-art model-free tabular result in average-reward CMDP . However, our regret bound is worse than that achievable in average reward unconstrained MDP with general parameterization . Interestingly, the gap between the convergence results of constrained and unconstrained setups is a common observation across the literature. For example, in the tabular model-free average reward MDP, the state-of-the-art regret bound for unconstrained setup, \(}(T^{1/2})\), is better than that in the constrained setup, \(}(T^{5/6})\).

## 6 Conclusions

This paper establishes the first sublinear regret and constraint violation bounds in the average reward CMDP setup with general parametrization (and do not assume the underlying constrained Markov Decision Process (CMDP) to be tabular or linear). We show that our proposed algorithm achieves \(}(T^{4/5})\) regret and constraint violation bounds where \(T\) is the time horizon. Note that the state of the art in unconstrained counterpart is \(}(T^{3/4})\). Closing this gap by designing more efficient algorithms is an open question in the average reward CMDP literature with the general parametrization. Moreover, our current algorithm requires the knowledge of mixing time. Relaxing such assumptions is another important future direction in realistic settings. For further discussions on future work directions, the readers are referred to .

## 7 Acknowledgement

This research was supported in part by the National Science Foundation under grant CCF-2149588 and Cisco, Inc.