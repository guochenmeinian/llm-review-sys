# Minimax-Optimal Location Estimation

Shivam Gupta

The University of Texas at Austin

shivamgupta@utexas.edu

&Jasper C.H. Lee

University of Wisconsin-Madison

jasper.lee@wisc.edu

Eric Price

The University of Texas at Austin

ecprice@cs.utexas.edu

&Paul Valiant

Purdue University

pvaliant@gmail.com

###### Abstract

Location estimation is one of the most basic questions in parametric statistics. Suppose we have a known distribution density \(f\), and we get \(n\) i.i.d. samples from \(f(x-)\) for some unknown shift \(\). The task is to estimate \(\) to high accuracy with high probability. The maximum likelihood estimator (MLE) is known to be asymptotically optimal as \(n\), but what is possible for finite \(n\)? In this paper, we give two location estimators that are optimal under different criteria: 1) an estimator that has minimax-optimal estimation error subject to succeeding with probability \(1-\) and 2) a confidence interval estimator which, subject to its output interval containing \(\) with probability at least \(1-\), has the minimum expected squared interval width among all shift-invariant estimators. The latter construction can be generalized to minimizing the expectation of any loss function on the interval width.

## 1 Introduction

We revisit one of the most basic questions in parametric statistics: location estimation. Suppose there is a shift-invariant parametric model \(\{f^{()}\}_{}\), namely that \(f^{()}=f(x-)\) for some distribution \(f\), and we get \(n\) i.i.d. samples from \(f^{()}\) for some unknown parameter \(\). The task is to estimate \(\) as accurately as possible, succeeding with probability at least \(1-\). This problem includes Gaussian mean estimation as a special case. Compared to general mean estimation, where we know nothing about the distribution beyond mild moment assumptions, here we are given the shape of the distribution, and only the shift is unknown. This allows us to aim for better performance than in mean estimation, and even handle cases where the mean of the distribution does not exist.

The problem has been widely studied as a special case of parametric estimation. In particular, the classic asymptotic theory  recommends using the maximum likelihood estimator (MLE), which, when we fix a distribution \(f=f^{(0)}\) and take the number of samples \(n\) to \(\), satisfies \(_{}-(0,1/(n))\) for any parameter \(\). Here, the variance of the asymptotic Gaussian is \(1/(n)\), where \(\) is the _Fisher information_ of the distribution \(f\), defined by \(_{}(f^{}(x))^{2}/f(x)\,x\). It is a standard fact that \(1/^{2}\) for any variance-\(^{2}\) distribution, meaning that the asymptotic performance of the MLE is always at least as good as the sample mean. Conversely, the well-known Cramer-Rao theorem states that, for any shift-equivariant estimator \(\), its variance must be at least \(1/(n)\) as well.

However, the classic asymptotic guarantee can be misleading, since the convergence to the asymptotic Gaussian may be arbitrarily slow in the number of samples, depending on the underlying distribution \(f\). Indeed, recent work by Gupta et al.  showed that the issue is in fact informationtheoretic, that no estimator can converge to the Fisher information rate in a distribution-independent manner as \(n\). They  also studied the finite sample _algorithmic_ theory of location estimation, with the goal of yielding finite-sample estimation error tight to within a \(1+o(1)\) multiplicative factor. To circumvent the impossibility of converging to the Fisher information in a distribution-independent finite-sample manner, they instead proposed a theory based on the _smoothed_ Fisher information \(_{r}\). That is, the Fisher information of the distribution \(f\) after convolving with \((0,r^{2})\). They showed that applying the MLE to perturbed samples and the smoothed distribution will yield an estimation error that converges to \((0,1/(n_{r}))\) in a distribution-independent fashion, for some smoothing radius \(r\) that vanishes as \(n\).

The key issue with the above theory, however, is that the Fisher information can be extremely sensitive to smoothing. As a simple example, a Dirac-\(\) spike has infinite Fisher information, yet smoothing it by \((0,r^{2})\) reduces its Fisher information to \(1/r^{2}\). Thus, to get \(1+o(1)\) tightness in estimation error, one would also need to also completely optimize away the smoothing. This is highly challenging and not yet achieved by these prior works on a distribution-by-distribution basis.

In this work, we instead directly construct minimax-optimal location estimators, under two different criteria. The first construction is a point estimator \(\) which, for a given failure probability \(^{*}\), attains the minimum estimation error \(\) such that \(|-|\) with probability at least \(1-^{*}\). The second construction is a confidence interval estimator \([_{},_{r}]\). Consider a loss function on the size of the output interval \(L(_{r}-_{})\) that is monotonically increasing. The second estimator is one such that, subject to the "correctness" constraint that the true parameter \(\) lies in the output interval \([_{},_{r}]\) with probability at least \(1-^{*}\), the estimator minimizes the mean loss.

The distinction is whether we want to minimize the _worst-case_ or the _average_ confidence interval. For example, consider the distribution \((1-1/n)N(,1)+(1/n)N(+n,r^{2})\) for \(r=}\) (see Figure 1). There is about a \(1/e\) chance that you don't sample the narrow spike, in which case your uncertainty is effectively the Gaussian \(N(0,)\); but if you do sample the spike \(k>0\) times, it gives a much more precise \(N(0,}{k})\) uncertainty. That is, some observed samples \(x\) could give very precise estimates, while other observed \(x\) cannot. The second algorithm gives worse worst-case behavior in exchange for much better average case behavior.

Our first algorithm only cares about the maximum size confidence interval it returns. That will be dominated by the \(1/e\) chance the narrow spike isn't sampled; so to get a 95% confidence interval, it will always return essentially a \((1-)\)-confidence interval for the \(N(0,)\) error, which is \([-},}]\). Our second algorithm could instead think: by returning the slightly larger interval \([-},}]\) in the cases where the narrow spike is not sampled, it can return _much_ smaller \((r)\) size intervals when the spike is seen. It will optimize this tradeoff according to the given loss function; for square loss, this will be with \((r)\) in the (more common!) "good" case, and a \(1+1/(n)\) factor worse than optimal interval in the "bad" case.

Figure 1: Example comparing our algorithms

### Our results and techniques

Minimax-optimal point estimationIn the first setting we consider in this paper: Given a point estimator \(\), for a given failure probability \(^{*}\), we measure its estimation error \(_{,^{*}}\) by its worst-case error over parameters \(\), namely

\[\{>0\ |\ _{}_{x_{1},,x_{n} f^{()}} (|(x_{1},,x_{n})-|) 1-^{*}.\}\]

Our first main contribution is that the estimator defined in Algorithm 2 achieves (essentially) minimal estimation error.

**Theorem 1.1**.: _For any small \(>0\), the estimator defined in Algorithm 2 has estimation error within a \((1+)\) factor of minimax optimal._

In order to construct this estimator, we solve the dual problem: fixing an estimation accuracy \(\), find an estimator that minimizes the failure probability \(\). The optimal estimator for this dual problem can be defined as the limit of a family of Bayes-optimal estimators. Specifically, fixing estimation error \(\), the estimator \(_{}\) defined in Algorithm 1 attains the minimum failure probability when the true parameter \(\) is drawn from the prior \([-t,t]\). The following theorem shows that the estimator in the limit as \(t\), \(_{}\), attains the minimax-optimal failure probability.

**Theorem 1.2**.: _The algorithm \(_{}\) (limit of Algorithm 1 as \(t\)) is minimax-optimal with respect to the 0-1 loss \(L(,)=[|-|>]\), namely, with respect to the failure probability._

The estimator \(_{}\) induces a function of the failure probability \(\) in terms of the estimation error \(\). Thus, given a desired a failure probability \(^{*}\), the \(()\) function can be inverted to define the optimal estimator in Algorithm 2 in Section 3.2. Since \(()\) is monotonic, we can use binary search to approximately compute the optimal \(\). The binary search is explicitly given and analyzed in Appendix A.1.

Minimax-optimal confidence-interval estimationThe second setting we consider in this paper concerns confidence-interval estimators. Suppose we are given an increasing loss function \(L:_{+}_{+}\) mapping an interval width to a loss, and given an estimator \(\) whose output interval we denote by \([_{},_{r}]\), we measure its _mean loss_ by

\[_{}*{}_{x_{1},,x_{n} f^ {()}}[L(_{r}-_{})]\]

Our main result for this second setting is that the estimator defined in Algorithm 4 achieves minimax-optimal mean loss, subject to the constraint that for all parameters \(\), the probability that \([_{},_{r}]\) is at least \(1-^{*}\) for a given failure probability \(^{*}\).

**Theorem 1.3**.: _Consider the set \(_{^{*}}\) of estimators \(\) whose output we denote by \([_{},_{r}]\), such that for all parameters \(\),_

\[_{x_{1},,x_{n} f^{()}}([_{},_{r}]) 1- ^{*}\]

_Consider the estimator defined in Algorithm 4 using failure probability \(^{*}\), and denote its mean loss by \(R\). Then,_

\[R_{_{^{*}}}_{} *{}_{x_{1},,x_{n} f^{()}}[L(_{r}- _{})]\]

The construction of Algorithm 4 and proof of Theorem 1.3 come in two parts. First, we show in Section 4.1 that Algorithm 4 is minimax-optimal among _equivariant_ estimators. That is, estimators which, the probability (density) of the estimator outputting the interval \([_{},_{r}]\) on input \((x_{1},,x_{n})\) is equal to the probability of the estimator outputting \([_{}+,_{r}+]\) on input \((x_{1}+,,x_{n}+)\). Second, we show in Section 4.2 that, for any estimator, equivariant or not, its performance can be arbitrarily well-approximated by an equivariant estimator. Together, this shows Theorem 1.3, that Algorithm 4 is minimax optimal.

The bulk of the technical work comes from the first step, constructing Algorithm 4 and showing that it is minimax-optimal among equivariant estimators. The challenge is that we need to optimize themean loss of an estimator _subject to_ the constraint on its probability of success, which is the event that its output interval does in fact include the true parameter.

Our approach is to use convex optimization techniques: for each equivariant estimator \(\), we consider its mean loss \(R_{}\) and its failure probability \(_{}\), both of which are independent of the true parameter due to the equivariance of \(\). It is easy to check that the set \(\{(R_{},_{})\}\) over equivariant estimators \(\) is a convex set. We give Algorithm 3, which, for any angle \([0,/2)\), is an equivariant estimator which attains the minimum possible \(R_{}+_{}\) among equivariant estimators. We also show the optimality of Algorithm 3 in Theorem 4.3. This estimator thus defines a failure probability \(_{}\) and a supporting hyperplane of angle \(\) for the \(\{(R_{},_{})\}\) set. Noting that \(_{}\) is monotonic in \(\), this again means we can binary search on \(\) to yield the desired failure probability \(^{*}\). Appendix B.1 gives the explicit binary search algorithm and its correctness guarantees.

## 2 Related work

Location estimation, as a special case of parametric estimation, has a well-established asymptotic theory  concerning the MLE, whose asymptotic performance is captured by the Fisher information. There have also been works on the finite-sample performance of the MLE, see for example . However, these works impose strong regularity conditions on the parametric model, and lose at least constant multiplicative factors in their error guarantees.

More recently, Gupta et al.  focused on location estimation, and aimed to characterize the finite-sample theory to within \(1+o(1)\)-tightness in the estimation error. They proposed a theory in terms of the _smoothed_ Fisher information. Yet, there are examples of distributions whose Fisher information decrease significantly even after little smoothing, meaning that their theory does not always capture the optimal finite-sample bounds up to a \(1+o(1)\)-factor. By contrast, in this work, we directly construct minimax-optimal estimators.

The related problem of mean estimation has also seen renewed interest in its finite-sample theory, also with the aim of yielding \(1+o(1)\)-tightness in the estimation error. Catoni  showed in his seminal work that, if the variance of the underlying distribution is known, then it is possible to achieve \(1+o(1)\)-tight estimation error. Catoni  and Devroye et al.  also showed that, as long as the kurtosis (normalized central \(4^{}\) moment) is bounded, knowledge of the variance is unnecessary. Recently, Lee and Valiant  proposed an estimator also yielding \(1+o(1)\)-tight error, but removing the knowledge assumption on the variance entirely, and instead assuming only its existence and finiteness.

## 3 Minimax-optimal point estimator

The goal of this section is to construct a point estimator \(\) that yields minimax-optimal estimation error, where error for \(\) is measured by \(\{>0\ _{}_{x_{1},,x_{n} f^{()}} (|(x_{1},,x_{n})-|) 1-^{*}\}\).

Section 3.1 first gives and analyzes an algorithm which takes as input an estimation accuracy \(\) and returns an estimate \(\) whose failure probability \((|-|>)\) is minimax-optimal. Section 3.2 then uses binary search to construct an estimator that instead takes in a desired failure probability, and outputs an estimation that (almost) optimally minimizes the estimation error.

### Estimator with minimax failure probability

The goal of this section is to construct an algorithm that takes in a desired estimation accuracy \(\) and optimizes for the failure probability \(\). The key intuition is that, since we do not know what the true parameter \(\) is, we might as well start by constructing an estimator that is Bayes optimal with respect to the uniform prior over the real line \(()\). However, there is no such thing as a uniform prior over the real line. So instead, Algorithm 1 below is constructed to be Bayes optimal with respect to the prior \([-t,t]\), and we take the limit of \(t\) to get a translation-equivariant estimator, whose risk (failure probability) is independent of the underlying parameter \(\). We then use standard tools to relate Bayes optimality to minimax optimality, under such translation equivariance.

**Algorithm 1** The algorithm \(_{t}\) for a fixed estimation accuracy \(\)

As mentioned earlier, the optimal algorithm we propose is the limit \(_{}\), namely, in Step 2 of Algorithm 1, the \(\) on \(\) is over the entire real line. Note, as a basic observation, that \(_{}\) is an equivariant estimator.

To show that \(_{}\) is minimax optimal in failure probability, we will use the standard technique of relating it to (a sequence) of Bayes-optimal estimators for some (sequence of) Bayesian priors.

Let us define the relevant notation here.

**Definition 3.1**.: _Throughout this section, we consider the 0-1 loss function \(L(,)=[|-|>]\) for some fixed \(\). The failure probability of an \(n\)-sample estimator \(\) with respect to a parameter \(\) can then be re-expressed as the risk \(R(,)=_{x_{1},,x_{n} f^{()}}[L(, (x_{1},,x_{n})]\)._

_Consider a prior \(\) on the parameter \(\) over \(\). We denote the Bayes risk of an estimator \(\) with respect to prior \(\) as \(R(,)=_{}[R(,)]\), under the usual abuse of notation._

To show that a given estimator \(\) has minimax optimal risk (namely, failure probability), the following standard fact states that it suffices to demonstrate a sequence of priors, such that the worst-case risk of \(\) is the limit of the optimal Bayes risk of these priors. We prove this fact in Appendix A.

**Fact 3.2**.: _Given an \(n\)-sample estimator \(\), suppose there exists a sequence of priors \(\{_{i}\}\) such that the worst-case risk of \(\) is equal to the limit of optimal Bayes risk of \(_{i}\), namely_

\[_{}R(,)=_{i}_{^{ }}R(_{i},^{})\]

_Then \(\) is minimax optimal._

To show the minimax optimality of \(_{}\), we will consider the sequence of priors \(_{t}=([-t,t])\). The first step is to characterize the Bayes-optimal estimators for the prior \(_{t}\).

**Lemma 3.3**.: _The algorithm \(_{t}\) is Bayes-optimal for the prior \(_{t}\)._

Proof.: Given the samples \(x_{1},,x_{n}\), the likelihood function \(l_{x}()\) constructed in Step 1 of \(_{t}\), limited to the range \([-t,t]\), is equal to the posterior distribution of \(\) given \(x\) up to a normalization factor. Thus, by construction, Step 2 finds the \(\) that maximizes the posterior success probability, or equivalently, \(\) that minimizes the expected posterior loss (the posterior failure probability) given \(x\). Hence \(_{t}\) is a Bayes-optimal estimator. 

With Fact 3.2 and Lemma 3.3, we can now state and prove that the algorithm \(_{}\) (Algorithm 1) is minimax-optimal with respect to the 0-1 loss, namely that it has minimax-optimal failure probability.

**Theorem 1.2**.: _The algorithm \(_{}\) (limit of Algorithm 1 as \(t\)) is minimax-optimal with respect to the 0-1 loss \(L(,)=[|-|>]\), namely, with respect to the failure probability._

Proof.: Fact 3.2 implies that, it suffices for us to show

\[_{}R(,_{})-_{^{}}R(_{i}, {}^{}) 0\]

as \(i\). Since \(_{}\) is equivariant, the risk \(R(,A_{})\) is independent of \(\). Also, recall by Lemma 3.3 that \(_{t}\) is Bayes-optimal for the prior \(_{t}\). The above claim is thus equivalent to

\[R(_{i},_{})-R(_{i},_{i}) 0\]

as \(i\).

Since \(_{i}\) is Bayes-optimal for \(_{i}\), the left hand side is always positive. Also observe that the left hand side is upper bounded by the probability that \(_{}\) has a different output from \(_{i}\). We will show that such probability, explicitly, \(_{_{i};x_{1},,x_{n} f^{()}}(_{}(x) _{i}(x))\), tends to 0 as \(i\).

We first claim that, fixing a distribution shape \(f\), for any small probability \(>0\), there exists a sufficiently large \(i\) such that, if the true parameter \([-i+,i-]\), then there is at most \(/2\) probability that \(_{}\) has a different output from \(_{i}\) when given i.i.d. samples from \(f^{()}\). This is easy to see, since \(_{}\) outputting differently from \(_{i}\) implies the output of \(_{}\) is outside of \([-i+,i-]\). Furthermore, since \(_{}\) is equivariant, we have

\[_{x_{1},,x_{n} f^{()}}(_{}(x)[-t, +t]) 0\]

as \(t\) for any fixed \(\). For \([-i+,i-]\), the locations \(-i+\) and \(i-\) are at least \(t=()\) away from \(\). Thus we can set \(i\) sufficiently large such that \(_{}\) outputs outside of \([-i+,i-]\) with probability at most \(/2\), for any fixed \([-i+,i-]\).

Further observe that, under prior \(_{i}\), there is only \(2/\) probability that the parameter \(\) is sampled be to outside of \([-i+,i-]\). By setting \(i\) sufficiently large, this probability is at most \(/2\), for any given \(\).

In summary, for any given small probability \(\), there exists a sufficiently large \(i\) such that \(_{}\) outputs differently from \(_{i}\) under prior \(_{i}\) with probability at most \(\). We have thus shown that

\[R(_{i},_{})-R(_{i},_{i}) 0\]

as \(i\), completing the proof. 

### Estimator with (almost) optimal estimation error

In Section3.1, we presented an algorithm which, when given the distribution shape \(f\) and a fixed estimation error \(\), finds an estimate which minimizes the failure probability \(\). Observing that \(\) is a monotonic function in \(\), we can also interpret \(\) as a monotonic function of \(\). Thus, if we are given a fixed \(\) (and distribution \(f\)), there is an infimum over achievable \(\). Algorithm1 therefore also induces an optimal algorithm that optimizes for \(\) when given \(\), as follows.

**Algorithm 2**.: _Consider the optimal failure probability \(\) as a function of the estimation accuracy \(\). Given a desired failure probability \(^{*}\), define \(^{*}=\{>0()^{*}\}\). Using Algorithm1 with estimation accuracy \((1+)^{*}\) for \(>0\) results in an estimator whose failure probability is by definition at most \(^{*}\). By construction, this estimator has accuracy that is within a \((1+)\) factor of minimax optimality._

The optimal estimation error \(^{*}\) can be computed using binary search. We defer the details to AppendixA.1.

## 4 Minimax-optimal confidence-interval estimator

Recall the second problem setting of this paper concerning confidence-interval estimators. Fix a loss function \(L:_{+}_{+}\) mapping an interval width to a loss, and assume it is increasing. Given an estimator \(\) whose output interval we call \([_{},_{r}]\), its _mean loss_ is the worst-case expected loss

\[_{}}_{x_{1},,x_{n} f^{()} }[L(_{r}-_{})]\]

Consider the set \(_{^{*}}\) of estimators \(\) that are "correct" with probability \(1-^{*}\). That is, for all parameters \(\),

\[_{x_{1},,x_{n} f^{()}}([_{},_{r}]) 1- ^{*}\]

Then, the goal of this section is to find an estimator in \(_{^{*}}\). that minimizes its mean loss, for any desired failure probability \(^{*}(0,1]\).

Section4.1 shows how to define the optimal _equivariant_ estimator subject to the \(1-^{*}\) probability correctness constraint. AppendixB.1 gives a binary search procedure for approximately computing this optimal equivariant estimator. Finally, Section4.2 shows that the optimal equivariant estimator is also optimal across all estimators.

### Optimality among equivariant estimators

We use convex optimization techniques to find the best equivariant estimator. To do so, consider the following feasible set of estimator performances, where for each equivariant estimator \(\), we consider the pair \((R_{},_{})\) which is respectively its mean loss and its failure probability. This set is always convex.

**Definition 4.1**.: _Fix a distribution \(f=f^{(0)}\). Given a (potentially) randomized equivariant \(n\)-sample estimator \(\) with output \([_{},_{r}]\), we say that \(\) achieves the performance pair \((R_{},_{})\) if \(_{x_{1},,x_{n} f}[(_{r}-_{})^{2}] R_{}\) and \(_{x_{1},,x_{n} f}(0[_{},_{r}])_{}\)._

_Define the feasible set for distribution \(f\) to be the set_

\[_{f}=\{(R_{},_{})$}\}\]

_See Figure 2 for a pictorial illustration of the set._

**Fact 4.2**.: _Given an arbitrary distribution \(f\), the feasible set \(_{f}\) is convex. As a corollary, the boundary \(R^{*}()=\{R_{}(R_{},) _{f}\}\) is a non-increasing convex function._

Proof.: Given two algorithms \(_{0}\) and \(_{1}\) achieving \((R_{_{0}},_{_{0}})\) and \((R_{_{1}},_{_{1}})\) respectively, consider a new randomized algorithm which runs \(_{0}\) with probability \(\) and \(_{1}\) otherwise, for an arbitrary \(\). The new algorithm achieves \(( R_{_{0}}+(1-)R_{_{1}},_{ _{0}}+(1-)_{_{1}})\). Thus, \(_{f}\) is a convex set.

Since \(_{f}\) is a convex set, the boundary \(R^{*}\) is also a convex function. The definition of \(_{f}\) also directly implies that \(R^{*}\) is non-increasing. 

Our goal then is to define the equivariant algorithm \(_{}\{R_{}(R_{},)_{f}\}\), that in fact, the boundary \(R^{*}\) being an inf is actually achievable. The key idea is to consider the (sub-)derivative (or just "slope", from now on) of the boundary function \(R^{*}()\), which is always non-positive and also increasing as a function of \(\), since \(R^{*}\) is non-increasing and convex. We will first show how, given a slope, we can find an estimator whose \((R_{},_{})\) pair lies on the boundary of \(_{f}\) and has a supporting hyperplane with the desired slope (see "Generic Alg 3" in Figure 2). Having defined this estimator, its failure probability \(_{}\) can be evaluated. Since the failure probability is an increasing function of the slope, we will then do a binary search on the value of the slope to yield the correct value of \(_{}=^{*}\) (see "Alg 4" in Figure 2).

Specifically, we will parameterize the slope by its angle \([0,/2]\). Given an angle \(\), we will define an estimator \(\) in \(\{R_{}+_{}(R_{ },_{})_{f}\}\). Additionally, since the \(\) set may contain many estimators, we will also have the choice to additionally find the algorithm in the \(\) with the minimum \(_{}\) (equivalently, maximizing \(R_{}\)) or the largest \(_{}\).

As we see in the following theorem, the quantity \(\|l_{}\|_{1}\) in Step 3 above is in fact the probability (density) of observing any shifted versions of \(\), and hence exists and is finite. Thus the normalization step is well-defined.

Figure 2: Illustration of the feasible set \(_{f}\) in Definition 4.1, and Algorithm 3 and Algorithm 4The following theorem shows that Algorithm 3 is indeed an optimal estimator that minimizes \(R_{}+_{}\) among all equivariant estimators.

**Theorem 4.3** (Correctness of Algorithm 3).: _Suppose Algorithm 3 with angle \(\) and flag smallest-Delta set to true achieves the pair \((R_{},_{})\). Then, for an arbitrary equivariant estimator \(}\) whose performance is \((R_{}},_{}})\), we have that_

\[R_{}+_{} R_{ }}+_{}}\]

_Furthermore, if \(R_{}+_{}=R_{}} +_{}}\), then \(_{}_{}}\)._

_The analogous statement holds for Algorithm 3 with the flag smallestDelta set to false._

_Moreover, as a corollary, we have \(R^{*}(_{})=R_{}\) by the convexity of \(_{f}\)._

Proof.: For the rest of the proof, we use the notation \(v\) for the unit vector \((,)\).

On the set of \(n\)-tuples of samples (namely \(^{n}\)), define the equivalence relation if two \(n\)-tuples are shifts of each other, that is, \(=(x_{1},,x_{n})\) is in the same class as \(=(y_{1},,y_{n})\) if and only if \(y_{1}-x_{1}==y_{n}-x_{n}\). Given a tuple \(\), we use the notation \([]\) to denote the corresponding equivalence class. We will also abuse notation and use \([]\) to denote a unique representative chosen for that equivalence class, and use \(-[]\) to denote the _scalar_ shift between \(\) and the representative \([]\) of its equivalence class.

Then, for an equivariant algorithm \(\) whose output is denoted by \([_{},_{r}]\), we can write \(R_{}\) as

\[R_{}=}_{[]}[}_{[ ][]}[}_{]}[L(_ {r}-_{})]]]\]

meaning that there are three levels of randomness: first draw an equivalence class \([]\) (according to the distribution \(f=f^{(0)}\)), then draw a random shift according to the conditional distribution \([]\), then finally, the algorithm \(\) can itself be random conditioned on its input \(x\).

Similarly, we can write \(_{}\) as

\[_{}=}_{[]}[} _{[][]}[}_{]} [_{0[_{},_{r}]}]]]\]

Given that the goal of Algorithm 3 is to optimize \((R_{},_{})\), linearity implies that it suffices to optimize, for each equivalence class \([]\), the inner product

\[(}_{[]}[}_{]}[L(_{r}-_{})]],}_{[]}[}_{[ ]}[_{0[_{},_{r}]}]])\]

To see that Algorithm 3 optimizes the above inner product among all equivariant algorithms, first observe that by linearity, it suffices to consider deterministic algorithms \(\) which output a deterministic interval on a given input \(n\)-tuple of samples.

Moreover, observe that the conditional distribution \([]\) is, up to translation, equal to the normalized likelihood \(l_{}()/\|l_{}\|_{1}\) for any \(n\)-tuple \([]\). This is because \(\|l_{}\|_{1}\) is exactly the probability (density) of the equivalence class \([]\). Therefore, in Step 4 of Algorithm 3, the probability \(_{[_{},_{}]}\) is equal to the probability, under the conditional distribution \([]\), that an equivariant algorithm whose output is \([_{}-(-[]],_{}-(-[])]\) under input \([]\) fails to output an interval that contains the parameter \(0\). That is, \(_{[_{},_{}]}=_{[] }[_{0[_{},_{r}]}]\) where \([_{},_{r}]\) is the output of a deterministic equivariant algorithm that outputs \([_{}-(-[]),_{r}-(-[])]\) under input \([]\).

To conclude, then, Algorithm 3 by construction chooses a deterministic interval to output so as to minimize the inner product

\[(*{}_{[]}[ *{}_{]}[L(_{r}-_{})] ],*{}_{[]}[*{ }_{[]}[_{0[_{},_{r}]} ]])\]

among all equivariant algorithms. The same reasoning also shows that the tiebreaking per input \(n\)-tuple sample \(\) implies tiebreaking on \(_{}\) which is an expectation over the distribution of the \(n\)-tuple sample input. 

Given Algorithm 3, we can now define the optimal estimator for a given confidence parameter \(\). which is by construction optimal given Theorem 4.3.

**Algorithm 4** (Minimax-optimal confidence-interval estimator).: _Fix a distribution \(f\). For an angle \(\), let \(_{}()\) be the failure probability of Algorithm 3 with the flag smallestDelta set to true, denote this estimator by \(_{}()\), and \(_{u}()\) be the failure probability of Algorithm 3 with the flag set to false, similarly denoting this estimator by \(_{r}()\). Given a failure probability \(^{*}\), by the supporting hyperplane theorem there exists an angle value \(\) (e.g. the arctangent of a sub-derivative of \(R^{*}()\) at \(=^{*}\)) such that \(^{*}[_{}(),_{u}()]\). Let \(^{*}=_{}()+(1-)_{u}()\). Then, define the optimal MSE estimator for failure probability \(\) to be the randomized algorithm which runs \(_{}()\) with probability \(\) and runs \(_{r}()\) otherwise. By definition, this estimator achieves \((R^{*}(^{*}),^{*})\)._

The optimal slope angle \(\) can be approximately computed using binary search. We give the details in Appendix B.1.

### Minimax optimality of Algorithm 4

In Section 4.1, we showed that Algorithm 4 is optimal among equivariant estimators. We now show that is in fact optimality among _all_ estimators.

We first extend Definition 4.1 to cover also non-equivariant estimators.

**Definition 4.4**.: _Given a (potentially) randomized \(n\)-sample estimator \(\), whose output interval we denote by \([_{},_{r}]\), we say that \(\) achieves its \((R_{},_{})\) pair if, for all parameters \(\), \(_{x_{1},,x_{n} f^{()}}[L(_{r}-_{})] R_{ }\) and \(_{x_{1},,x_{n} f^{()}}([_{},_{r}]) _{}\)._

The key step in the proof of minimax optimality of Algorithm 4 is to show that the performance of any estimator can be well-approximated by an equivariant estimator.

**Proposition 4.5**.: _Fix an arbitrary distribution \(f\) and an arbitrarily small approximation parameter \(b(0,)\). Suppose there is an estimator \(\) (not necessarily equivariant) achieving \((R_{},_{})\), then there exists an equivariant estimator \(^{}\) that achieves \(((1+b)R_{},_{}+2b)\)._

We prove Proposition 4.5 in Appendix B. The main idea for constructing \(^{}\) from \(\) is that, on input an \(n\)-tuple of samples \((x_{1},,x_{n})\) whose sample median is at \(s\), we 1) shift the samples by a random shift drawn from \([-t-s,t-s]\) for sufficiently large \(t\), 2) call \(\) on the shifted samples and 3) output the returned interval and subtract off the random shift we made to the samples. This estimator is equivariant by construction, and we relate its execution with high probability to applying \(\) to samples drawn according to the prior \([-t,t]\). This allows us to bound the performance guarantees of \(^{}\) by those of \(\).

With Proposition 4.5, we can then show that Algorithm 4 is in fact minimax optimal among all estimators, not just equivariant ones.

**Theorem 4.6**.: _Fix a distribution \(f\). Recall the definition of \(R^{*}()\) from Fact 4.2, which is the infimum of all achievable \((R_{},_{}=)\) over all equivariant estimator \(\). Now consider an arbitrary (potentially non-equivariant) estimator \(^{}\). If \(^{}\) achieves \((R_{^{}},_{^{}})\), then \(R_{^{}} R^{*}(_{^{}})\)._

_As a result, since \((R^{*}(^{*}),^{*})\) is achievable by Theorem 4.3 and Algorithm 4 for any \(^{*}>0\), we have that Algorithm 4 is in fact minimax optimal among all estimators._The proof is in Appendix B.

## 5 Experimental results

We compare various location estimation methods on synthetic data from a fairly simple, but irregular, piecewise linear distribution (Figure 3(a)). We set \(n=10\) and aim for \(90\%\) confidence intervals. In Figure 3(b), we plot the CDF of the point error produced by the MLE, the 0.3-smoothed MLE, and our two algorithms (Algorithm 1 and Algorithm 4). We find that our algorithms get 25% smaller error than the MLE at the target 90% error rate. The smoothed MLE and Algorithm 4 error distributions both dominate the MLE, with Algorithm 4 having better performance close to its target and smoothed MLE better further away. Algorithm 1 does 2% better than Algorithm 4 at the target 90%, but worse over most of the range.

In Figure 3(c), we compare the confidence intervals produced by our algorithms. Algorithm 1 uses a fixed interval radius regardless of the samples it sees, while Algorithm 4 has a distribution over interval widths that is usually smaller but occasionally bigger.