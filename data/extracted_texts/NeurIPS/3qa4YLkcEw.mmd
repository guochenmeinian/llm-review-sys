# TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs due to their homogeneous task types and low task complexity. To bridge this gap, we introduce TRACE, a benchmark designed to rigorously assess continual learning capabilities in LLMs. TRACE comprises eight challenging tasks from the scope of domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. Through systematic experiments on TRACE with six different aligned models ranging from 7B to 70B, we discovered significant declines in both general performance and instruction-following abilities. For example, the accuracy of llama2-chat 13B on the gsm8k dataset declined precipitously from 43.14% to 2.12% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Our results demonstrate that integrating task-specific cues with meta-rationales significantly reduces catastrophic forgetting and improves task convergence, offering a viable strategy to enhance the adaptability of LLMs in dynamic environments.

## 1 Introduction

Large Language Models (LLMs) [1; 2] have revolutionized natural language processing through a two-step process: initial pretraining on extensive corpora, followed by fine-tuning on human-generated instructions and preference data, aligning them with human language and intentions. Aligned LLMs have showcased impressive capabilities and ensured safer responses. However, as the demands for language models grow, there's a pressing need to enhance their abilities in areas such as domain-specific knowledge [3; 4], multilingual proficiency , complex task-solving , and tool usage . Yet, retraining and realigning them from scratch to meet these demands is impractical due to prohibitive training costs and the challenge of acquiring high-quality data. Therefore, incrementally training existing Aligned LLMs through continual learning (CL ) is crucial. However, when dealing with tasks in sequential manners, it is challenging to retain the performance of previous tasks, which is known as "catastrophic forgetting" . Therefore, we prompt the pressing question: _To what degree do Aligned LLMs exhibit catastrophic forgetting when subjected to incremental training?_

Existing continual learning benchmarks [10; 11; 12] are not suitable for evaluating the state-of-the-art LLMs. Firstly, many of these benchmarks predominantly consist of simplistic natural language understanding datasets. These tasks, due to their inherent simplicity, fail to challenge the capabilities of large-scale models adequately. Secondly, prior benchmarks have primarily focused on metrics that assess the performance of the models on target sequential tasks. Yet, for aligned models, aspects likegeneralization to new tasks, the ability to follow human instructions, and safety preservation are of paramount importance. Regrettably, these dimensions have not been thoroughly researched.

In this paper, we present **TRACE**, a continual learning benchmark designed for aligned LLMs. Our benchmark consists of eight distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets have been standardized into a unified format, simplifying the evaluation process. To evaluate continual learning in aligned LLMs, we introduce three metrics: "General Ability Delta," "Instruction Following Delta," and "Safety Delta" to assess models' forgetfulness in such scenarios.

We conduct a comprehensive evaluation of 6 aligned LLMs on TRACE. Evaluation results reveal several key findings: 1) Nearly all models exhibit a significant decline in general abilities after training on TRACE, especially in math and reasoning. For instance, the accuracy of llama2-chat 13B on the gsm8k dataset dropped from 43.14% to merely 2.12%. 2) Catastrophic forgetting remains a substantial issue for LLMs and does not diminish with the model size increase. Llama2-chat 70B also shows significant forgetting (with -17.5% backward transfer) on previous tasks. 3) Full-parameter training, compared to LoRA training, more easily fits the target tasks, but it also leads to a more pronounced decline in general abilities. 4) LLMs' instruction-following capabilities also suffer a significant reduction after continual learning.

Through experimentation, we observed that tasks augmented with reasoning paths are notably effective in preserving certain capabilities of LLMs, preventing them from substantial declines. Such findings lead us to ponder on leveraging a model's inherent strengths for rapid transfer on new tasks, rather than starting the learning curve from scratch. This motivation birthed our novel training strategy: Reasoning-augmented Continual Learning (RCL). RCL prompts the model to generate task analyses and rationales during training. As our results indicate, this approach not only boosts performance on target tasks but also significantly upholds the inherent strengths of LLMs.

## 2 Related Work

### Continual Learning

Continual learning  aims to develop learning algorithms that can accumulate knowledge on non-stationary data. Existing works can be broadly categorized into rehearsal-based, regularization-based, and architecture-based approaches. Rehearsal-based approaches [13; 14] leverage a memory buffer that stores examples from previous tasks, training the model jointly with the current task. Regularization-based approaches [15; 16; 17] incorporate additional terms into the loss function to penalize changes in crucial weights. Architecture-based approaches [18; 12] focus on dynamically expanding model capacity or isolating existing model weights to mitigate interference between new and old tasks.

Figure 1: An overview of TRACE benchmark. TRACE consists of two main components: 1) A selection of eight datasets constituting a tailored set of tasks for continual learning, covering challenges in domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. 2) A post-training evaluation of LLM capabilities. In addition to traditional continual learning metrics, we introduce General Ability Delta, Instruction Following Delta, and Safety Delta to evaluate shifts in LLM’s inherent abilities.

### CL Benchmarks in NLP

The most recognized CL benchmark for NLP encompasses five text classification datasets , including AG News, Amazon Reviews, Yelp Reviews, DBpedia, and Yahoo Answers. Building upon this,  proposed a long CL benchmark which fuses the aforementioned five datasets with an additional four from the GLUE benchmark , five from the SuperGLUE benchmark , and the IMDB dataset . However, this benchmark is limited in scope as it solely emphasizes Natural Language Generation (NLG) tasks and is restricted to English, thus lacking task diversity. In contrast, TRACE is a more varied and challenging benchmark, designed to test aligned LLMs across multiple sequential tasks. It assesses general capability, instruction adherence, and safety shifts. Recognizing that TRACE, like earlier benchmarks, may be incorporated into future pre-training, we acknowledge potential impacts on its long-term efficacy as a CL benchmark. Nonetheless, the current findings from TRACE provide essential insights into catastrophic forgetting in LLMs.

## 3 Preliminaries

Continual learning [22; 8] focuses on developing learning algorithms to accumulate knowledge on non-stationary data. In supervised continual learning, a sequence of tasks \(\{_{1},,_{T}\}\) arrive in a streaming fashion. Each task \(_{t}=\{(_{i}^{t},y_{i}^{t})\}_{i=1}^{}\) contains a separate target dataset, where \(_{i}^{t}_{t}\), \(_{i}^{t}_{t}\). A single model needs to adapt to them sequentially, with only access to \(_{t}\) at the t-th task. In general, given a prediction model \(h_{}\) parameterized by \(\), continual learning seeks to optimize for the following objective across all tasks:

\[_{}_{k=1}^{T}_{x,y_{k}} p_{}(y x)\] (1)

In this paper, we utilize overall performance (OP ), forward transfer (FWT ), and backward transfer (BWT ) scores as the main metrics. After incrementally learning the t-th task, the model's score on the i-th task (where \(i t\)) is denoted as \(R_{t,i}^{D}\).

## 4 TRACE: A Comprehensive Benchmark for CL in LLMs

TRACE is designed to offer a comprehensive continual learning evaluation for LLMs. Illustrated in Figure 1, TRACE encompasses two primary components: a curated set of tasks tailored for continual learning, followed by an in-depth evaluation of an LLM's post-training capabilities. In this section, we detail TRACE's sequential tasks and introduce our evaluation metrics. In Section 4.4, we evaluate six models using TRACE and present our key findings.

### Data Creation

There are three principles for the creation of TRACE. First, the datasets should be novel enough that most LLMs have not been trained on them. Second, they should be challenging for large language models. Third, a variety of tasks should be covered in our benchmark.

According to these three principles, in this section, we will provide a detailed introduction to the data collection process for each dataset. As these datasets have varying sizes, we create a balanced version by randomly sampling 5000 training examples and 2000 testing examples from the original datasets. As shown in Table 1, we get 40,000 training examples and 16,000 testing examples in total.

Domain-Specific.ScienceQA  is a multi-hop QA dataset collected from elementary and high school science curricula, with a rich domain diversity from natural science, social science, and language science, requiring the model of reasoning ability and science knowledge. In TRACE, we include only non-multimodal examples to exclusively test LLM performance. FOMC  is a hawkish-dovish classification task, which is novel in the financial domain. The dataset is divided into three subsets: data on meeting minutes, press conference data, and speech data. We use a combination of them. MeetingBank  is a new benchmark dataset for city council meeting summarization, an unstudied domain. It demands a global understanding of the whole long context.

Multi-lingual.LLMs' cross-lingual abilities are constrained by their vocabulary and pre-training corpus. For instance, LLaMA's vocabulary contains few Chinese tokens, affecting its efficiency with Chinese text. C-STANCE  is the first Chinese dataset for zero-shot stance detection collected from Sina Weibo, one of the most popular Chinese social media sites. It includes two challenging subtasks: target-based stance detection and domain-based stance detection. In TRACE, we include the target-based one, which means the targets in testing examples are unseen during training. 20Minuten  is a text simplification dataset consisting of full articles paired with shortened, simplified summaries from the Swiss news magazine. We use this dataset to evaluate the ability to generate German text.

Code completion.Code completion is another challenging task to evaluate long context modeling ability, and it is one of the most widely used features in software development through IDEs. We select the line-level code completion task of CodeXGLUE , which requires the model to generate the next line given the lengthy code input. The corpus Py150  contains 150,000 Python programs collected from GitHub repositories. Since the golden labels of the testing dataset are not available by , we randomly divide each Python code in Py150 into two parts, taking the first part as inputs and the next line as labels.

Mathematical reasoning.Mathematical problems are always used to evaluate the reasoning ability of models. NumGLUE is an 8-task benchmark far from solved including state-of-the-art large-scale language models performing significantly worse than humans. Both of the two tasks require arithmetic reasoning ability. It is worth noting that both datasets have original labels consisting only of numbers, without associated inference processes. The first one evaluates the common sense of models, while the second one requires some grade-school scientific knowledge.

### CL Metrics Design

Unlike traditional continual learning benchmarks focused on sequential target tasks, evaluating aligned LLMs should also account for the preservation of their inherent capabilities. SoTA LLMs, through instruction tuning, exhibit impressive task-solving abilities. Aligning these models with human preferences further boosts their safety and usefulness. Hence, TRACE introduces a broader evaluation, including three unique metrics.

In the TRACE benchmark, we incorporate three metrics tailored to evaluate distinct dimensions of model performance following training: **General Ability Delta**, **Instruction Following Delta**, and **Safety Delta**. Each metric is designed to capture shifts in different areas--general abilities, instruction-following, and safety of responses, respectively. Despite their targeted focuses, all three

  
**Dataset** & **Source** & **Avg len** & **Metric** & **Language** & **\#data** \\  _Domain-specific_ & & & & & \\ ScienceQA & Science & 210 & Accuracy & English & 5,000 \\ FOMC & Finance & 51 & Accuracy & English & 5,000 \\ MeetingBank & Meeting & 2853 & ROUGE-L & English & 5,000 \\  _Multi-lingual_ & & & & & \\ C-STANCE & Social media & 127 & Accuracy & Chinese & 5,000 \\
20Minuten & News & 382 & SARI & Germany & 5,000 \\  _Code completion_ & & & & & \\ Py150 & Github & 422 & Edim similarity & Python & 5,000 \\  _Mathematical reasoning_ & & & & & \\ NumGLUE-cm & Math & 32 & Accuracy & English & 5,000 \\ NumGLUE-ds & Math & 21 & Accuracy & English & 5,000 \\   

Table 1: An overview of dataset statistics in TRACE. ’Source’ indicates the context’s origin. ’Avg len’ represents word count for English, German, and code datasets, and character count for Chinese. ’SARI’ is a score specific to simplification.

metrics are calculated using a consistent formula:

\[ R_{t}^{X}=_{i=1}^{N}(R_{t,i}^{X}-R_{0,i}^{X})\]

where \(X\) represents the specific metric focus--general ability, instruction-following, or safety--and \(N\) is the number of datasets considered within each respective category. This unified formula underscores the systematic approach TRACE adopts to evaluate different dimensions of LLM performance enhancements or declines post-training.

In the development of evaluation metrics, integrating existing and newly introduced indicators into a single, unified metric might appear straightforward through a weighted summary. However, the implementation of such a composite metric necessitates careful prioritization of the capabilities most valued by users. Therefore, this paper does not propose a universal metric for ranking models.

### Experimental Setup

#### 4.3.1 Models & Baselines

To evaluate the resilience of aligned LLMs from diverse training backgrounds and strategies, we select six backbones from three organizations: Meta: LLaMa-2-7B-Chat, LLaMa-2-13B-Chat, LLaMa-2-70B-Chat , BaiChuan: Baichuan 2-7B-Chat , and Large Model Systems Organization: Vicuna-13B-V1.5, Vicuna-7B-V1.5 .

We evaluate the performance of LLMs in a continual learning setting using different approaches:

Sequential Full-Parameter Fine-Tuning (SeqFT)It trains all model parameters in sequence.

LoRA-based Sequential Fine-Tuning (LoraSeqFT)Only the low-rank LoRA matrices are fine-tuned, leaving the LLM backbone fixed . This method is chosen based on prior findings of reduced forgetting with "Efficient Tuning" .

Replay-based Sequential Fine-Tuning (Replay)We incorporate data from LIMA along with 10% of data from previous tasks into our training dataset.

In-Context Learning (ICL)Task demonstrations are supplied as part of the language prompt, acting as a form of prompt engineering . A 6-shot setting is used for our experiments.

Other Continual Learning BaselinesWe also demonstrate the experiment results on a few continual learning baselines, including EWC , ODG , PP , L2P , LFPT5 . Given that EWC and OGD require storing parameters or gradients from past tasks, which is impractical for the full parameter setting of LLMs, we validate these methods using LoRA.

#### 4.3.2 Datasets

To evaluate a model's _general ability_, we assess across six key dimensions: **Factual Knowledge**: Using MMLU dataset , reporting 5-shot accuracy. **General Reasoning**: Evaluated with BBH , reporting EM scores with chain-of-thought prompts with 3-shot in-context examples. **Multilinguality**: Using TyDiQA , a multilingual QA benchmark across 11 languages, reporting 0-shot F1 scores. **Commonsense Reasoning**: Assessed with PIQA , reporting 0-shot accuracy. **Code Generation**: Using MBPP , reporting 0-shot Pass@1. **Reading Comprehension**: Using BoolQ , reporting 0-shot accuracy.

For _instruction-following capability_, we use Self-instruct dataset , comprising 175 user-oriented prompts, and the LIMA dataset , which assembles 300 prompts from community Q&A and manual examples.

To evaluate _safety_ changes, we use the CoNa dataset , which consists of 178 expert-annotated samples focused on instructions related to hateful speech generation.

#### 4.3.3 Implementation Details

In the training phase, we trained models with and without LoRA adapters using 5000 samples at learning rates of 1e-5 and 1e-4, respectively. For the testing phase, we use a temperature of 0.1. All our training and inference experiments were conducted on a machine equipped with 8x80G Nvidia A100. All general benchmark evaluations were conducted using the Open-Compass toolkit , adopting its default configuration. The detailed settings can be found in Appendix A.

### Main Results

We conducted experiments with various task orders. Results for the default order are presented in the main text, while results for additional orders are detailed in Appendix D.7.

#### 4.4.1 Performance of Target Sequential Tasks

Table 2 showcases the performance of six distinct LLMs on TRACE benchmark, after their continual learning phase. From this evaluation, we can draw the following conclusions:

In-Context Learning (ICL)ICL methods generally perform lower than SeqFT and Replay methods. This suggests that the TRACE benchmark is indeed challenging, and LLMs can't readily identify solutions just through simple demonstrations.

Replay PerformanceAmong all the baselines, Replay achieved the highest OP score. With its BWT score being positive, it indicates that Replay effectively retains its performance on sequential tasks without significant forgetting. This makes Replay a straightforward and efficient strategy in a continual learning context.

    &  &  &  \\   & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** \\  LLaMA2-7B & \(48.7\) & \(-8.3\%\) & \(2.4\%\) & \(12.7\) & \(-45.7\%\) & \(0.8\%\) & \(55.5\) & \(\) & \(-0.2\%\) \\ LLaMA2-13B & \(49.9\) & \(-7.0\%\) & \(\) & \(28.0\) & \(-36.5\%\) & \(1.5\%\) & \(56.5\) & \(0.4\%\) & \(2.2\%\) \\ LLaMA2-70B & - & - & - & \(\) & \(-17.5\%\) & \(\) & \(\) & \(0.4\%\) & \(1.9\%\) \\ Vicuna-7B & \(49.2\) & \(-8.4\%\) & \(1.7\%\) & \(33.4\) & \(-23.7\%\) & \(0.9\%\) & \(55.3\) & \(0.2\%\) & \(0.5\%\) \\ Vicuna-13B & \(\) & \(\) & \(2.1\%\) & \(31.6\) & \(-28.4\%\) & \(1.1\%\) & \(\) & \(0.6\%\) & \(\) \\ Baichuan2-7B & \(43.4\) & \(-15.4\%\) & \(1.8\%\) & \(\) & \(\) & \(1.0\%\) & \(51.7\) & \(1.1\%\) & \(-4.0\%\) \\   &  &  &  \\   & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** \\  LLaMA2-7B & \(32.3\) & \(-20.6\%\) & \(1.0\%\) & \(29.7\) & \(-22.6\%\) & \(0.9\%\) & \(49.2\) & \(-0.3\%\) & \(1.9\%\) \\ LLaMA2-13B & \(35.7\) & \(-19.7\%\) & \(\) & \(36.6\) & \(-18.6\%\) & \(0.7\%\) & \(51.3\) & \(-1.1\%\) & \(\) \\ LLaMA2-70B & - & - & - & - & - & \(\) & \(-0.4\%\) & \(1.7\%\) \\ Vicuna-7B & \(37.2\) & \(-16.8\%\) & \(\) & \(39.9\) & \(-15.7\%\) & \(\) & \(50.4\) & \(\) & \(1.6\%\) \\ Vicuna-13B & \(36.5\) & \(-18.6\%\) & \(1.1\%\) & \(40.4\) & \(-16.7\%\) & \(0.6\%\) & \(\) & \(-0.6\%\) & \(2.0\%\) \\ Baichuan2-7B & \(\) & **-7.6\%** & \(0.8\%\) & \(\) & **-7.6\%** & \(1.1\%\) & \(46.6\) & \(-1.7\%\) & \(0.9\%\) \\   &  &  &  \\   & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** & **OP** & **BWT** & **FWT** \\  LLaMA2-7B & \(43.7\) & \(-9.5\%\) & \(\) & \(49.2\) & \(-6.7\%\) & \(1.5\%\) & \(41.3\) & \(-6.2\%\) & \(\) \\ LLaMA2-13B & \(46.4\) & \(-6.4\%\) & \(1.6\%\) & \(50.4\) & **-5.6\%** & \(1.1\%\) & \(43.7\) & \(-4.4\%\) & \(0.7\%\) \\ LLaMA2-70B & \(\) & **-1.6\%** & \(1.4\%\) & - & - & - & \(\) & \(-5.7\%\) & \(0.4\%\) \\ Vicuna-7B & \(48.6\) & \(-9.2\%\) & \(1.5\%\) & \(\) & \(-7.8\%\) & \(0.7\%\) & \(42.6\) & \(-6.8\%\) & \(1.1\%\) \\ Vicuna-13B & \(\) & \(-4.3\%\) & \(\) & \(\) & \(-6.6\%\) & \(\) & \(\) & **-3.2\%** & \(0.9\%\) \\ Baichuan2-7B & \(47.6\) & \(-8.6\%\) & \(0.6\%\) & \(48.3\) & \(-6.7\%\) & \(\) & \(45.6\) & \(-5.4\%\) & \(1.2\%\) \\   

Table 2: Overall Performance (OP), Forward Transfer (FWT), and Backward Transfer (BWT) for all baseline models and 4 baseline methods.

Full Parameter Training vs. LoRAFull parameter training demonstrates better task-specific adaptability compared to LoRA, with a smaller BWT score. For instance, LLaMA-2-7B-Chat's SeqFT OP(BWT) is 48.7 (-8.3%), while LoRASeqFT stands at 12.7 (-45.7%). This suggests that when the focus is primarily on sequential tasks, full parameter fine-tuning should be prioritized over parameter-efficient methods like LoRA.

Catastrophic Forgetting in LLaMA-2-70B-ChatDespite achieving a high overall performance (OP) score of 45.2, the llama2-70B-Chat model exhibits significant catastrophic forgetting, as evidenced by its BWT of -17.5%. This substantial decline in retaining previously learned information indicates a vulnerability of larger models to catastrophic forgetting in continual learning scenarios.

#### 4.4.2 Variation of General Ability

Table 3 presents the evaluations of various LLM models concerning general abilities. The degree of general ability forgetting in LLMs can be analyzed from three perspectives.

From the Model Perspective **1)** Nearly all models display a negative General Ability Delta, indicating a general decline in overall capabilities after continual learning. **2)** Both larger and smaller models experience significant forgetting in general abilities. For instance, the General Ability Delta for llama2-7B-Chat-LoraSeq stands at -7.88, whereas llama2-70B-Chat-LoraSeq is -10.04.

From the Task Perspective **1)** Despite the presence of CoT prompts, all models show a noticeable decline in math and reasoning abilities, highlighting their sensitivity to new task learning. **2)** Excluding the llama2-7b model, most models exhibit a significant drop in performance on MMLU, suggesting a gradual loss of factual knowledge through continual learning. **3)** TydiQA task sees a general boost post-training, possibly due to the inclusion of Chinese and German datasets in our sequential tasks. Intriguingly, other languages on TydiQA also show enhancements and declines, suggesting potential cross-linguistic transfer effects. **4)** Performance shifts on PIQA for most models are subtle, indicating the relative robustness of commonsense knowledge during continual learning.

From the Method Perspective **1)** The Replay method proves beneficial in preserving reasoning and factuality skills. Especially for larger models, the mitigation of forgetting through Replay is more pronounced. For instance, for LLaMA-2-7B-Chat, Replay offers a 6.5 EM score boost compared to methods without Replay, while for LLaMA-2-13B-Chat, the increase is 17.1 EM score.

#### 4.4.3 Instruction Following Ability Analysis

We evaluate the instruction-following ability of LLMs based on LLaMA-2-7B-Chat and LLaMA-2-13B-Chat. Figure 2 (a) illustrates the win rate % for instruction following sequentially trained LLMs and their original versions. Here, the win rate can be approximated as an indicator for the Instruction-following delta. All three training methods show a significant decline in instruction-following capabilities, with the most pronounced decline in the LoRA method. Therefore, be cautious when exploring approaches like LoRA for continual learning in LLMs.

#### 4.4.4 Safety Analysis

We test the safety of answers from models LLaMA-2-7B-Chat and LLaMA-2-13B-Chat. Figure 2 (b) shows the win rate % for instruction following between the new LLMs and their starting versions. Here, the win rate can be used as a measure for the Safety Delta. Compared to the original models, most answers were rated as 'Tie'. This suggests that the safety of the model's answers is largely unaffected by continual learning on general tasks.

### Ablation Study

**Data Quantity & Training Steps.** Figure 3 shows the impact of data volumes and training steps on target task performance. For LLaMA-2-7B-Chat's SeqFT, we tested with 500, 1000, and 5000 samples from each dataset, training for 1, 3, 5, 10 epochs. Results indicate performance improves with more data, with optimal results at 5000 samples. Furthermore, up to 5 epochs improve performance, aligning with our baseline for balancing task optimization and capabilities retention.

**Decoupling the Impact of Different Tasks.** Results from section 4.4.2 reveal a notable decline in reasoning and mathematical skills after training on our benchmark. This brings forth the question: _How exactly does the reasoning capability of LLMs transform during the continual learning process?_ Figure 4 tracks the reasoning ability (assessed via BBH performance) after the completion of each training task. Interestingly, the model's reasoning skill improves after the ScienceQA task but declines for others. Unlike NumGLUE tasks, which lack clear reasoning paths, ScienceQA provides explicit reasoning paths in its answers, indicating that including reasoning paths in training maybe benefit the model's reasoning skills.

### Reasoning-augmented Continual learning

_Why do LLMs underperform on the TRACE benchmark, both in target tasks and maintaining inherent capabilities?_ Two likely reasons: First, neural networks tend to overfit the specific output formats of new tasks . Second, as most tasks involve direct result prediction, LLMs may learn shortcuts  within the training data, limiting their ability to generalize to new data.

With these insights and our experimental findings in 4.5, we propose the Reasoning-augmented Continual Learning (RCL) method. As depicted in Figure 5, RCL involves two phases: automated reasoning annotation and sequential training on the augmented datasets. GPT-4 generates reasoning paths for all entries, based on prompts designed by domain experts for each task. These are then validated against the ground truth. Manual inspection of a 100-sample subset results in a 94% approval rate, demonstrating GPT-4's reliability. Following this, supervised training is conducted on the target LLM, keeping hyperparameter settings consistent with baselines.

#### 4.6.1 Performance on Sequential Tasks

Table 4 provides comparisons of the performance of RCL against other baselines. Through an ablation study contrasting single-task training (SingleFT) with multi-task training, and assessing the impact of reasoning-augmented data, we observed that integrating reasoning pathways consistently boosts performance over the original dataset. Our approach yields results comparable to the SeqFT method using only 500 samples instead of 5000. By utilizing fewer datasets and training steps, our method also helps preserve the inherent capabilities of LLMs more effectively.

#### 4.6.2 Original Performance Retention

Impacts on General AbilityFigure 6 shows RCL's performance on general abilities matches SeqFT and Replay on MMLU, TydQA, BoolQA, and PIQA. Yet, RCL excels in reasoning tasks like GSM and BBH, where it surpasses SeqFT and Replay by 12.7 and 13.2 points, respectively. This highlights RCL's effectiveness in enhancing reasoning skills through reasoning paths. Additionally, combining RCL with replay boosts its reasoning task performance.

Impacts on Instruction-FollowingThe impact of incorporating RCL on instruction-following capabilities is presented in Table 5. It's evident that RCL enhances the model's ability to follow instructions by 8% and 5% compared to SeqFT and Replay, respectively.

## 5 Conclusion

Existing continual learning benchmarks are insufficient in thoroughly evaluating LLMs, due to their oversimplification and lack of key metrics like instruction following and safety. To tackle this, we introduce TRACE, a comprehensive benchmark with eight challenging tasks and well-rounded metrics. Our experiments show that for LLMs, catastrophic forgetting remains, and a clear drop in general abilities is observed during continual learning. Besides, our RCL method highlights the importance of using reasoning in training while alleviating the above phenomena. We believe this area is crucial and hope our work serves as a foundation for future research.

## 6 Limitations

While our proposed TRACE benchmark covers a variety of tasks, expanding it to include real-time interaction and multi-modal integration could provide a more comprehensive assessment of LLM capabilities. Additionally, future versions should aim to minimize any potential biases introduced by the current task selection or dataset composition, ensuring a more balanced and comprehensive representation of tasks that better reflect diverse real-world applications.

 
**Methods** & **OP** & **BWT** & **FWT** \\  EWC & \(32.3\) & \(-20.6\%\) & \(1.0\%\) \\ OGD & \(29.7\) & \(-22.6\%\) & \(0.9\%\) \\ L2P & \(43.7\) & \(-9.5\%\) & \(2.6\%\) \\ LFPT5 & \(49.2\) & \(-6.7\%\) & \(1.5\%\) \\ PP & \(49.2\) & \(-0.3\%\) & \(1.9\%\) \\ O-Lora & \(41.3\) & \(-6.2\%\) & \(1.6\%\) \\  LCL & \(39.5\) & - & - \\ ICL+Re & \(41.1\) & - & - \\ SeqFT(0.5k) & \(23.0\) & \(-19\%\) & \(0.3\%\) \\
**RCL(0.5k)** & \(46.6\) & \(-13\%\) & \(2.7\%\) \\ SeqFT(5k) & \(48.7\) & \(-8.3\%\) & \(2.4\%\) \\
**RCL(5k)** & \(\) & \(-6.5\%\) & \(3.2\%\) \\  SingleFT & \(57.6\) & - & - \\ SingleFT+Re & \(58.1\) & - & - \\ MT w/o. Re & \(52.3\) & - & - \\ MT w. Re & \(58.2\) & - & - \\  

Table 4: Comparison of RCL with different baselines. **Single FT** refers to fine-tuning the model on a single task, **Re** refers to reasoning-augmented, and **MT** refers to Multi-task training.