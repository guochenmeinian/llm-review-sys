# S\({}^{2}\)FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity

Xinyu Yang\({}^{1}\), Jixuan Leng\({}^{1}\), Geyang Guo\({}^{2}\), Jiawei Zhao\({}^{3}\), Ryumei Nakada\({}^{4}\),

Linjun Zhang\({}^{4}\), Huaxiu Yao\({}^{5}\), Beidi Chen\({}^{1}\)

\({}^{1}\)CMU, \({}^{2}\)Georgia Tech, \({}^{3}\)Caltech, \({}^{4}\)Rutgers, \({}^{5}\)UNC-Chapel Hill

xinyuya2, beidic@andrew.cmu.edu

###### Abstract

Current PEFT methods for LLMs can achieve high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (**S\({}^{2}\)FT**) methods for LLMs, which _concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability_. S\({}^{2}\)FT accomplishes this by "selecting sparsely and computing densely". Based on the coupled structures in LLMs, S\({}^{2}\)FT selects a few attention heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes the weight matrices on both sides of all coupled structures to connect the selected subsets in each layer into a dense submatrix. Finally, S\({}^{2}\)FT performs in-place gradient updates on all selected submatrices. Through theoretical analyses and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6\(\%\) and 1.3\(\%\) average improvements compared to LoRA, and surpasses full FT by 11.5\(\%\) when generalizing to various domains after instruction tuning. Using our partial back-propagation algorithm, S\({}^{2}\)FT saves training memory up to 3\(\) and improves latency by 1.5-2.7\(\) compared to full FT, while achieving an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S\({}^{2}\)FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism when serving multiple fine-tuned models.

## 1 Introduction

Recently, Large Language Models (LLMs) have achieved significant success . With these models being applied in diverse domains, full fine-tuning (FT) is commonly employed to enhance their downstream capabilities . However, retraining all parameters comes with three drawbacks: (i) Full FT suffers from catastrophic forgetting, where a model forgets pre-trained knowledge while acquiring new information . (ii) As the model and dataset sizes grow at scale, full FT becomes increasingly computation-demanding and memory-intensive . (iii) It is impractical to store and serve thousands of fine-tuned LLMs on modern GPUs if each requires full parameter storage .

Parameter-efficient fine-tuning (PEFT) methods propose to address these bottlenecks by updating a small fraction of parameters . Rather than merely reducing the number of learnable parameters, an ideal PEFT method should possess three key properties to be practically effective and efficient:

**High Quality**: It should exhibit both memorization and generalization capabilities, balancing the acquisition of new information from fine-tuning tasks with the retention of pre-trained knowledge.

**Efficient Training**: It should minimize the memory footprint for model gradient and optimization states, and further translate such memory efficiency into less computation and fine-tuning speedup.

**Scalable Serving**: It should avoid adding inference overhead when serving a single PEFT model. For multiple models, new parameters should be partially stored as adapters to save memory, and allows for effective fusion , fast switch , and efficient parallelism  among thousands of adapters.

However, achieving all the aforementioned goals simultaneously is challenging. Common PEFT approaches, such as LoRA , DoRA , and Galore , project the model's weights or gradients onto a low-rank subspace. While this significantly reduces memory footprint, their performance lags behind full fine-tuning in most large-scale scenarios. Recent state-of-the-art PEFT methods have aimed to improve performance but at the cost of serving efficiency. ReFT operates on a frozen base model and learns task-specific interventions on hidden representations that cannot be merged into the original model, leading to a \(2.2\) increase in inference latency. LISA  employs a coarse-grained selective method by randomly freezing most Transformer blocks during optimization, which requires significantly more trainable parameters. Consequently, in scaled serving settings like S-LoRA , LISA can only serve at most \(\) as many fine-tuned models as LoRA under the same memory budget.

Prior to the era of LLMs, PEFT methods based on unstructured sparse fine-tuning (SpFT) have shown a strong trade-off between low number of parameters and high model performance without sacrificing serving efficiency [63; 3; 71]. We hypothesize that SpFT, which selectively updates a small subset of model parameters, can outperform LoRA and its variants in generalization capabilities. In Figure 2, our findings across various generalization tasks support this hypothesis. However, the unstructured nature of SpFT necessitates sparse operations in computation, hindering its efficient training and scalable serving on modern hardware. This makes SpFT less practical for adapting LLMs at scale.

In this work, we propose a family of Structured Sparse Fine-Tuning (**S\({}^{2}\)FT**) methods to "select sparsely and compute densely" (See Figure 1), thereby closing the efficiency gap in SpFT. Inspired by structured weight pruning techniques [45; 42], we first identify several coupled structures inherent in LLMs that are connected by intermediate activations. For example, in the multi-head attention (MHA) module, each attention head in the query, key, and value projections is linked to only a few rows in the output projection. Similarly, in the feed-forward network (FFN) module, each column in the up and gate projections corresponds to a single row in the down projection. By co-permuting the matrices on both sides of these coupled structures, we can preserve the original output of these structures, with only the order of the intermediate activations changed. Exploiting this property, our S\({}^{2}\)FT strategically selects a subset of attention heads for the MHA module and a subset of channels for the FFN module. We then permute the coupled structures to connect the selected components within each linear layer into a dense submatrix. Finally, through our partial back-propagation algorithm with only two-line code modification, S\({}^{2}\)FT performs in-place gradient updates exclusively for all selected submatrices, boosting training efficiency by eliminating redundant forward activations and backward calculation.

Through our theoretical analysis, S\({}^{2}\)FT mitigates forgetting under distribution shifts while simplifying optimization. Empirically, S\({}^{2}\)FT outperforms other PEFT methods on LLAMA and Mistral family models, improving 1.2-4.1% on commonsense reasoning tasks and 0.6-1.9% on arithmetic reasoning ones. It also surpasses full FT by 11.5% when generalize to various domains after instruction tuning.

Finally, we conduct a comprehensive analysis to verify the training efficiency and serving scalability of S\({}^{2}\)FT. Compared to existing PEFT methods, S\({}^{2}\)FT not only saves 1.4-3.0\(\) memory, but also increases latency by 1.5 to 2.7\(\), making LLM fine-tuning more accessible. Additionally, S\({}^{2}\)FT's parameter updates can be decomposed into adapters, enabling adapter fusion with smaller performance drop than LoRA. Our method also results in more scalable and efficient adapter switch and parallelism through reduced matrix multiplications, showcasing strong potential for large-scale LLM serving scenarios.

Figure 1: **An Overview of the S\({}^{2}\)FT Family for LLMs: First, we perform sparse selection of specific attention heads and channels within the coupled structures of the MHA and FFN modules. Next, we apply co-permutation to the weight matrices on both sides of these structures, enabling dense gradient computation only for the selected components. While we demonstrate S\({}^{2}\)FT by selecting the same heads/channels on both sides for clarity, our approach also supports asymmetric selection strategies.**

Memorization or Generalization?

In this section, we evaluate the memorization and generalization capabilities of various fine-tuning methods, including full FT, LoRA, and SpFT. We hypothesize that SpFT can generalize better to downstream tasks. To support this hypothesis, we present detailed observations and analyses. Further theoretical analysis about the generalization capabilities of the S\({}^{2}\)FT family can be found in Section 4.

**Hypothesis.** We hypothesize that SpFT offers superior generalization than both full FT and LoRA, while maintaining comparable memorization to LoRA with the same number of trainable parameters.

**Experimental Setup.** We fine-tune the Llama3-8B on the Math10K data  using SpFT, LoRA, and full FT. In addition to training losses, accuracies are measured on downstream tasks in LLM-Adapters, including near out-of-distribution (OOD) generalization on both easy (i.e, MultiArith, AddSub, SingleEq, MAWPS) and hard (i.e, GSM8K, AQuA, SVAMP) arithmetic reasoning tasks, and far OOD generalization on commonsense reasoning ones. For PEFT methods, we set three ratios of trainable parameters (\(p=10\%,1\%,0.1\%\)) and search for the optimal hyperparameters on the valid set. In SpFT, trainable parameters are selected randomly with given ratios. See details in Appendix C.

**Observations.** Figure 2 indicates several key findings. First, SpFT achieves lower training losses than LoRA when using the same ratio of trainable parameters, especially at very small ratios. This gap arises from the more complex optimization process in LoRA, which requires the simultaneous updating of two matrices . Second, we observe both elevated training loss and reduced average accuracy on easier math tasks as the ratio decreases, suggesting a positive correlation between memorization abilities and trainable parameters. Notably, with only 10% of the parameters updated, PEFT methods learn comparable memorization abilities to full FT when trained on a 10k-sample dataset.

When generalizing to complex mathematical problems or commonsense reasoning tasks, the performance ranking emerges as: SpFT \(>\) Full FT \(>\) LoRA. SpFT effectively transfers reasoning abilities to commonsense domains, while LoRA exhibits significant performance drops in far OOD generalization. This indicates (i) freezing a larger fraction of the parameters can retain more pre-trained abilities, and (ii) approximating high-dimensional gradients with low-rank decomposition may overfit fine-tuned data and hinder the model from generalization. Since LLMs are pre-trained on high-quality data, SpFT emerges as the preferred choice for fine-tuning on task-specific data of varying quality.

## 3 The S\({}^{2}\)FT family of methods

While SpFT demonstrates strong generalization ability and good overall performance in Section 2, its unstructured nature poses challenges for efficient training and scalable serving on modern hardware (e.g., GPU). This is because of the need for sparse operations when storing and computing weights, gradients, and optimization states, which are significantly slower than their dense variants on GPU. This motivates our investigation into structured sparsity approaches that utilize only dense operations: _Can structured sparsity improve hardware efficiency while preserving performance by selecting sparsely but computing densely? If so, how far can the flexibility of selection be pushed in this context?_ To answer this question, we design a family of Structured Sparse Fine-Tuning (**S\({}^{2}\)FT**) methods with dense-only computations, making PEFT effective, efficient and scalable. We begin by discovering the coupled structure in LLMs in Section 3.1. Leveraging this property, Section 3.2 introduce the selection and permutation strategies of S\({}^{2}\)FT, with overall pipeline illustrated in Figure 0(b). In Section 3.3, we present our partial back-propagation algorithm that enables end-to-end training latency reduction.

### Discover Coupled Structures in LLMs

We initiate our pursuit of flexible structured sparsity by examining the coupled structures in LLMs.

Figure 2: Accuracy comparison of SpFT, LoRA and Full FT at varying ratios of trainable parameters in various settings. SpFT exhibits strong generalization ability while full FT excels in memorization.

**Structure Dependency in LLMs.** Inspired by prior work on structured pruning [45; 17], our study start by building the dependencies between activations and weights for LLMs. Let \(A\) denote an activation and \(W\) denote a weight in the model. We define \((A)\) as the set of parameters that directly contribute to the computation of \(A\), and \((A)\) as the set of parameters that depend on \(A\) in the computation of subsequent activations. The dependency between structures can be defined as follows:

\[W_{1}(A)^{+}(W_{1})=1  AW_{1}\] (1) \[W_{2}(A)^{-}(W_{2})=1  W_{2}A\] (2)

where \(^{+}(W_{1})\) represents the out-degree of weight \(W_{1}\), and \(^{-}(W_{2})\) represents the in-degree of weight \(W_{2}\). Each equation represents a unique directional dependency between activations and weights. When both equations hold simultaneously, a coupled structure exists between \(W_{1}\) and \(W_{2}\). In Figure 3, we employ deep linear networks to illustrate two types of coupled structures in LLMs:

_Basic Structures_: In Figure 2(a), these structures exist in both the multi-head attention (MHA) and feed-forward network (FFN) modules. Taking LLaMA as an example, in the MHA module, we consider the Query (\(\)), \(\) (\(\)), and Value (\(\)) projections as \(W_{1}\), and the Output (\(\)) projection as \(W_{2}\), while \((^{})(x)\) acting as the activation between weight matrices. Similarly, in the FFN module, the Up (\(\)) and Gate (\(\)) projections function as \(W_{1}\), with the Down (\(\)) projection corresponding to \(W_{2}\). Here, \((x)((x))\) serves as the activations connecting \(W_{1}\) and \(W_{2}\).

_Residual Structures_: In Figure 2(b), this type of coupled structures exists between the MHA and FFN modules. We further consider how residual connections influence the activations in these structures.

**Permutation Invariance of Coupled Structures.** Figure 3 demonstrates that \(W_{1}\) and \(W_{2}\) can be co-permuted using the same order, which only affects the order of activations between them while preserving the original output from the coupled structure. Since residual dependencies require an additional run-time step to permute the residuals, we will focus on basic dependencies in our method.

### Sparse Selection and Permutation

At this point, all coupled structures within the model have been identified. The subsequent sparse selection and permutation processes are straightforward, with overall pipeline illustrated in Figure 0(b).

**MHA Module**: There are four linear layers in a MHA module: \(Q,K,V,O^{d d}\). For a model with \(h\) attention heads, each head \(i[h]\) has its own projections denoted as \(Q_{i}^{d d_{h}}\), \(K_{i}^{d d_{h}}\), \(V_{i}^{d d_{h}}\), and \(O_{i}^{d_{h} d}\), where \(d_{h}=d/h\) is the dimension per head. Let \(S_{}[h]\) denote a small subset of attention heads. By permuting \(S_{}\) to the beginning of each weight matrix, we are able to update these selected heads using dense-only operations, while keeping the other ones frozen.

**FFN Module**: There are three linear layers in an FFN module: \(U,G^{k d}\) and \(D^{d k}\). In \(^{2}\), only a few channels require gradient updates. Let \(S_{}[d]\) denote the selected channels. We can permute \(S_{}\) to the beginning of each weight matrix and only fine-tune this compact subset.

Next, we provide several strategies for identifying and selecting important subsets in each module.

1. [leftmargin=*]
2. \(^{2}\) (\(^{2}\)): In this strategy, a subset of channels is randomly selected and set to be trainable.
3. \(^{2}\): This variant selects subsets based on the magnitude of the weights for linear layers.
4. \(^{2}\): This variant selects subsets based on the magnitude of activations on a calibration set.
5. \(^{2}\): Top-K subsets are ranked and selected by the product of weight and activation magnitudes.
6. \(^{2}\): This variant selects subsets based on the magnitude of gradients on a calibration set.

Here, 1 and 2 can be applied directly without pre-processing. 3 and 4 only require a forward pass on a small calibration dataset. While 5 necessitates a backward pass on this dataset, it does not store optimization states and can mitigate memory footprints for activations through gradient checkpointing . By default, we use \(^{2}\) for a fair comparison and discuss other variants in Section 5.4.

Figure 3: Grouped model weights with basic structure and residual structure. All highlighted weights must be permuted simultaneously. Residual structures require additional permutation during runtime.

### Partial Back-propagation Algorithm

Finally, we introduce our partial back-propagation algorithm with only two line modifications in PyTorch. our algorithm stores trainable channels based on their start and end positions, thereby improving training efficiency by eliminating redundant forward activations and backward calculations.

``` defsetup_context(ctx, inputs, output): activation, weight, bias, start, end = inputs #only save partial input tensors for gradient calculation in forward ctx.save_for_backward(activation[:, start:end], weight, bias, start, end) defgradient_update(parameter, gradient, start, end): #only modify the assigned positions of weight matrices during optimization parameter[:, start:end].add_(gradient) ```

## 4 Theoretical Analysis

In this section, we theoretically explain why S\({}^{2}\)FT demonstrates stronger generalization capabilities compared to LoRA. Following previous work [23; 79; 53; 52], we further show that S\({}^{2}\)FT is simple and efficient in optimization by maintaining stability in both the magnitude and direction of updates.

### Stronger Generalization Capability

First, we theoretically explore why S\({}^{2}\)FT demonstrates stronger generalization capabilities compared to LoRA. We consider a pre-trained \(L\)-layer deep linear network, which has been widely used to facilitate the theoretical analysis of complex DNNs [59; 30; 43; 22; 34; 5]. Let \(f^{}(x):=W^{}_{L}W^{}_{L-1} W^{}_{1 }x\) be the pre-trained deep linear network, where \(W^{}_{}^{d_{} d_{-1}}\), with \(d_{0}=p\) and \(d_{L}=q\). We fine-tune the \(\)-th layer with low-rankness level \(r\{d_{},d_{-1}\}\) or sparsity level \(s= r+d_{-1}}{d_{-1}}\). Denote a class of adaptation with parameters \(U^{d_{} d}\) and \(V^{d_{-1} d}\) as

\[f_{,U,V}(x):=^{}_{+1}(W^{}_{}+UV^{ })^{}_{-1}x,\] (3)

where \(^{}_{}:=W^{}_{L}W^{}_{L-1} W ^{}_{}^{d_{L} d_{-1}}\) and \(^{}_{}:=W^{}_{}W^{}_{- 1} W^{}_{1}^{d_{} d_{0}}\) with \(^{}_{0}=I_{p}\) and \(^{}_{L}=I_{q}\). In a transformer-based LLM, each row of \(W_{}\) can represent the parameters in a single attention head for the MHA module or in a single channel for the FFN module.

Given \(n\) observations \((x^{}_{i},y^{}_{i})^{p}^ {q}\), we fine-tune \(f^{}\) by minimizing the empirical risk \(^{}_{n}(f_{,U,V}):=(1/n)_{i[n]}\|y^{ }_{i}-f_{,U,V}(x^{}_{i})\|^{2}\) via gradient descent. For LoRA, we train both low-rank matrices \((U,V)\) in Equation (3) with \(d r\). For S\({}^{2}\)FT, we train only \(V\) in Equation (3) with \(d s\) and fixed \(U U^{^{2}}_{}:=[e_{a_{1}};e_{a_{2}};;e_{a _{s}}]\), where \(S=\{a_{1},,a_{s}\}[d_{}]\) and \(e_{a}\) is the \(a\)-th standard basis. Similar conclusions hold when we fine-tune only \(U\). Motivated by the implicit regularization in gradient descent [77; 19; 5], we directly consider minimum norm solutions.

We consider a multiple linear regression setting. Assume that the in-distribution training data \((x^{}, y^{})^{p+q}\) and out-of-distribution test data \((x^{},y^{})^{p+q}\) are generated i.i.d. according to

\[y^{}=B^{}x^{}+^{},\ \ k\{\},\]

where \(B^{}^{q p}\) is the coefficient matrix, \(x^{}\) and \(^{}\) are mean zero sub-Gaussian signal and noise with covariance matrices \(^{}_{x}\) and \(^{}_{}\), respectively. The generalization capacity is measured by the fine-tuned model's excess risk \((f):=[\|y^{}-f(x^{})\|^{2}]-_{f^{ }}[\|y^{}-f^{}(x^{})\|^{2}]\).

For these OOD data, LoRA suffers from forgetting, while S\({}^{2}\)FT can maintain pre-training knowledge.

**Assumption 4.1** (Distribution Shift).: Assume that \(^{}_{x}=^{}_{x}=_{x}\) for some \(_{x}^{p p}\), and \(\|(^{}_{+1}U^{^{}}_{S})( ^{}_{+1}U^{^{2}}_{S})^{}(B^{}-B ^{})^{1/2}_{x}\|_{}^{2}^{2}^{ }(f^{})\) for some \(>0\).

Assumption 4.1 states that while the covariate distribution remains unchanged, the label distribution conditioned on covariates may shift, but not exceeding a factor of \(^{2}\) of the OOD risk of \(f^{}\). This holds for fine-tuning with proper channel selection, where primarily the output distribution is changed.

**Theorem 4.2** (Out-of-distribution Excess Risk, Informal).: _Suppose Assumption 4.1 holds. Consider \(n\). If \(B^{}=^{}_{+1}^{} ^{}_{-1}\) holds for some \(^{}^{d_{} d_{-1}}\), and \(s(^{}_{f})\), then,_

\[^{}(f_{,U^{^{2}}_{S},V^{^{2} }})(1+3^{2})^{}(f^{}),\ \ ^{}(f_{,U^{},V^{}})\|(B^{ }-B^{})^{1/2}_{x}\|_{}^{2}.\]Theorem 4.2 indicates that the OOD risk of S\({}^{2}\)FT is bounded above by that of \(f^{}\), while that of LoRA is bounded below by the label shift magnitude. If \(f^{}\) already has a low risk for OOD tasks, and the label shift is significant, S\({}^{2}\)FT is expected to outperform LoRA. Essentially, when the OOD task deviates significantly from the FT distribution, LoRA may forget pre-trained knowledge and overfit to the FT data, compromising its generalization capabilities. See formal statements in Theorem F.8.

### Simple and Efficient Optimization

Next, we explain why S\({}^{2}\)FT is a simple and efficient optimization method. In Equation (3), S\({}^{2}\)FT can be viewed as a LoRA variant that fixes \(U_{S}^{^{2}}\) as a combination of multiple orthogonal standard basis vectors while optimizing \(V^{^{2}}\) with zero initialization. The gradient is given by \(}{ V^{^{2}}}=(_{ -1}^{})^{}}{ W^{} _{+1}}U_{S}^{^{2}}\). Ignore \(_{-1}^{}\), \(_{-1}^{}\) and denote \(}{ W^{}_{+1}}\) as \(\), at step \(t\) with learning rate \(\),

\[ f_{,t}(x):=f_{,t}(x)-f_{,t-1}(x)=U_{S}^{^{2}}(V_{t}^{^{2}}-V_{t-1}^{^{2}})^{}x=- U_{S}^{ ^{2}}U_{S}^{^{2}^{}}^{ }||x||^{2}.\]

Since \(U_{S}^{^{2}}\) is an orthogonal matrix, the update simplifies to \( f_{,t}(x)=-^{}||x||^{2}\). Following LoRA+ , assuming that \(x=_{n}(1)\), where \(n\) is the width of the layers in LLMs, we expect \( f_{,t}(x)=(1)\) to ensure stability and feature learning in the infinite-width limit . S\({}^{2}\)FT can achieve this when \(=(n^{-1})\) while LoRA requires \(_{U}=(1)\) and \(_{V}=(n^{-1})\) for optimal performance. These rates become impractical for modern LLMs with very large \(n\). Therefore, S\({}^{2}\)FT aligns with LoRA variants that fix one matrix [52; 79], offering more stable and efficient optimization.

Furthermore, under a given sparsity level as regularization, our model simplifies optimization when approximating the full fine-tuning gradients at non-zero positions. Similar to LoRA-SB , let \(G_{V}\) denote the gradient of \(V^{^{2}}\). The equivalent gradient \(\), which describes the virtual gradient of the pretrained weight matrices, can be expressed as \(U_{S}^{^{2}}G_{V}^{}\). Then, the gradient with respect to \(V^{^{2}}\) can be expressed in terms of the gradient of the pretrained weight \(W^{}\) as: \(G_{V}^{O}=U_{S}^{^{2}^{}}G\). Using this relationship, our objective is to minimize the distance between the equivalent gradient and the full gradient as \(_{G_{V}}\|-G\|_{F}^{2}\), where the optimal solution is given by \(G_{V}=(U_{S}^{^{2}^{}}U_{S}^{^{2}})^{-1} G_{V}^{O}\). Since \(U_{S}^{^{2}}\) is orthogonal, we have \(G_{V}=G_{V}^{O}\). This shows that S\({}^{2}\)FT can keep the optimal update directions throughout the training process, establishing it as an efficient sparse optimization method.

## 5 Experiments

In this section, we conduct a series of experiments across three diverse benchmarks covering more than 20 datasets. Our goal is to provide a rich picture of how S\({}^{2}\)FT performs in different scenarios. Here, we compare our method with different fine-tuning strategies and categories including: (i) Full fine-tuning (FT), (ii) _reparameterized fine-tuning_: LoRA , DoRA , and Galore , (iii) _adapter-based fine-tuning_: Series Adapter , Parallel Adapter , and LoReFT , (iv) _prompt-based fine-tuning_: Prefix-Tuning , (v) _sparse fine-tuning_: LISA . For a fair comparison, we keep a comparable number of trainable parameters in S\({}^{2}\)FT to that of LoRA. The design choices for trainable parameter allocations in S\({}^{2}\)FT will be detailed in Section 5.4. All other hyperparameters are selected via cross-validation. Detailed setups and dataset descriptions are provided in Appendix E.

### Commonsense Reasoning

The results of eight common sense reasoning tasks in Table 1 show that S\({}^{2}\)FT consistently outperforms existing PEFT methods in the LLaMA-7B / 13B, LLaMA2-7B and LLaMA3-8B models. Compared to LoRA and DoRA, it achieves average performance gains of 4.6% and 2.8%, respectively. Furthermore, S\({}^{2}\)FT also shows superior performance against recent approaches, including Galore, LoReFT, and LISA, with improvements of at least 1.0%. Remarkably, despite using less than 1% of trainable parameters, our method surpasses full FT by 0.5%. The 3.0% improvement on the LLaMA3-8B suggests that keeping most pre-trained parameters frozen enables better generalization to test distributions.

### Arithmetic Reasoning

As showcased in Table 2, S\({}^{2}\)FT consistently outperforms other PEFT methods for different base models. On average, it achieves improvements of 1.3% and 0.9% over LoRA and DoRA, respectively. These results highlight the versatility and effectiveness of our approach across a diverse range of tasks. Additionally, we observe substantial improvements even when compared to Full FT for the LLaMA3-8B model, particularly on complex tasks such as GSM8K and AQuA. This suggests that S\({}^{2}\)FT better preserves the original reasoning capabilities of this stronger model while acquiring new skills from the fine-tuning data, thereby validating the enhanced generalization ability of our method.

### Instruction Following

Table 3 comprehensively compares various methods on eight tasks in the MT-Bench dataset . It is observed that S\({}^{2}\)FT \(>\) LISA \(>\) Full FT \(>\) LoRA/Galore \(\) Vanilla for both the Mistral-7B and LLama2-7B model. This is because sparse FT methods like S\({}^{2}\)FT and LISA retain more pre-trained knowledge while acquiring new skills on the FT dataset, thereby generalizing better to diverse tasks in the MT-Bench dataset. Moreover, our method outperforms LISA due to its fine-grained and flexible selection strategy, enabling all layers to learn to follow instructions on the full fine-tuning set.

  
**Model** & **Method** & **\# Param(\%)** & **Bool** & **PIQA** & **SIQA** & **HellaSwag** & **Wino** & **ARC-e** & **ARC-e** & **OBQA** & **Avg. \(\)** \\    } & Full FT\({}^{2}\) & - & 73.1 & 85.4 & 68.5 & 78.5 & 66.1 & 89.8 & 79.9 & 74.8 & 77.0 \\   & Full FT\({}^{2}\) & 100 & 70.3 & 84.2 & 80.1 & 92.3 & 85.4 & 86.6 & 72.8 & 83.4 & 81.9 \\  & Prefix \({}^{1}\) & 0.11 & 64.3 & 76.8 & 73.9 & 42.1 & 72.1 & 72.9 & 54.0 & 60.6 & 64.6 \\  & Series \({}^{1}\) & 0.99 & 63.0 & 79.2 & 76.3 & 67.9 & 75.7 & 74.5 & 57.1 & 72.4 & 70.8 \\  & Parallel \({}^{1}\) & 3.54 & 67.9 & 76.4 & 78.8 & 69.8 & 78.9 & 73.7 & 57.3 & 75.2 & **72.2** \\ LLAMA-7B & LoRA \({}^{3}\) & 0.83 & 69.2 & 81.7 & 78.4 & 83.4 & 80.8 & 79.0 & 62.4 & 78.4 & 76.7 \\  & DoRA \({}^{1}\) & 0.84 & 68.5 & 82.9 & 79.6 & 84.8 & 80.8 & 81.4 & 65.8 & 81.0 & 78.1 \\  & Galore \({}^{3}\) & 0.83\({}^{1}\) & 68.6 & 79.0 & 78.5 & 84.7 & 80.1 & 80.3 & 62.1 & 77.3 & 76.3 \\  & LoRAF \({}^{2}\) & 0.03 & 69.3 & 84.4 & 80.3 & 93.1 & 84.2 & 83.2 & 68.2 & 78.9 & 80.2 \\  & LISA \({}^{3}\) & 9.1 & 70.4 & 82.1 & 78.7 & 92.4 & 82.9 & 84.9 & 70.2 & 78.4 & 80.0 \\  & **S\({}^{2}\)FT (Ours)** & 0.81 & **72.7** & **83.7** & **79.6** & **93.4** & **83.5** & **86.1** & **72.2** & **83.4** & **81.8** \\    } & Full FT\({}^{3}\) & 100 & 74.5 & 86.3 & 81.3 & 94.4 & 86.9 & 89.7 & 77.9 & 88.8 & 85.0 \\  & Prefix \({}^{1}\) & 0.03 & 65.3 & 75.4 & 72.1 & 55.2 & 68.6 & 79.5 & 62.9 & 68.0 & 68.4 \\  & Series \({}^{1}\) & 0.80 & 71.8 & 83.0 & 79.2 & 88.1 & 82.4 & 82.5 & 67.3 & 81.8 & 79.5 \\  & Parallel \({}^{1}\) & 2.89 & 72.5 & 84.9 & 79.8 & 92.1 & 84.7 & 84.2 & 71.2 & 82.4 & 81.4 \\  & LoRA \({}^{1}\) & 0.67 & 72.1 & 83.5 & 80.5 & 90.5 & 83.7 & 82.8 & 68.3 & 82.4 & 80.5 \\  & DoRA \({}^{1}\) & 0.68 & 72.4 & 84.9 & 81.5 & 92.4 & 84.2 & 84.2 & 69.6 & 82.8 & 81.5 \\  & LoRFT \({}^{2}\) & 0.03 & 72.1 & **86.3** & **81.8** & **95.1** & **87.2** & 86.2 & 73.7 & 84.2 & 83.3 \\  & **S\({}^{2}\)FT (Ours)** & 0.65 & **74.2** & 85.7 & 80.7 & 94.9 & 86.4 & **88.4** & **76.3** & **87.8** & **84.3** \\    } & Full FT\({}^{3}\) & 100 & 74.7 & 84.9 & 78.7 & 93.7 & 84.1 & 87.5 & 75.2 & 85.0 & 83.0 \\  & LoRA \({}^{1}\) & 0.83 & 69.8 & 79.9 & 79.5 & 83.6 & 82.6 & 79.8 & 64.7 & 81.0 & 77.6 \\  & DoRA \({}^{1}\) & 0.84 & 71.8 & 83.7 & 76.0 & 89.1 & 82.6 & 83.7 & 68.2 & 82.4 & 79.7 \\  & **S\({}^{2}\)FT (Ours)** & 0.81 & **72.9** & **86.1** & **80.2** & **94.3** & **85.5** & **87.2** & **74.6** & **83.4** & **83.0** \\    } & Full FT\({}^{3}\) & 100 & 73.9 & 86.2 & 79.1 & 93.1 & 85.8 & 88.1 & 78.2 & 84.0 & 83.6 \\  & DoRA \({}^{1}\) & 0.70 & 70.8 & 85.2 & 79.7 & 92.5 & 84.9 & 88.9 & 78.7 & 84.4 & 82.5 \\   & DoRA \({}^{1}\) & 0.71 & 74.6 & **89.3** & 79.9 & 95.5 & 85.6 & 90.5 & 80.4 & 85.8 & 85.2 \\   & **S\({}^{2}\)FT (Ours)** & 0.70 & **75.0** & 89.0 & **80.7** & **96.5** & **88.0** & **92.5** & **83.4** & **87.8** & **86.6** \\   

Table 1: Comparison among various fine-tuning methods for the LLAMA-7B/13B, LLAMA2-7B, and LLAMA3-8B models on eight commonsense reasoning tasks. Non-PEFT methods are marked in gray. (\({}^{1}\): from DoRA paper, \({}^{2}\): from ReFT paper, \({}^{3}\): reproduced by us, \({}^{1}\): projected trainable parameters)

  
**Model** & **Method** & **\# Param(\%)** & **MultiArith** & **GSMSK** & **AddSub** & **AQua** & **SingleEq** & **SVAMP** & **MAWPS** & **Avg. \(\)** \\    } & Full FT\({}^{2}\) & 100 & 98.8 & 43.1 & 91.1 & 20.9 & 94.3 & 60.6 & 88.2 & 71.0 \\  & & & & & & & & & & & \\  & LoRA \({}^{2}\) & 0.83 & 98.0 & 40.0 & 91.2 & 21.7 & 93.1 & 56.7 & 85.3 & 69.7 \\  & **DoRA \({}^{1}\)** & 0.84 & 97.3 & 38.9 & 89.6 & **22.4** & **93.9** & **58.4** & **85.3** & **69.4** \\  & **SIFT (Ours)** & 0.8

### Design Choices for Trainable Parameter Allocations

Finally, we detail how S\({}^{2}\)FT distribute trainable parameters across layers, modules, and channels.

**Uniform across Layers**: Following Chen et al. (2018), we allocate parameters to each layer uniformly.

**Fine-tune Important Modules**: Figure 4 analyzes the effectiveness of different components in a LLaMA-like Transformer Block for fine-tuning, including Query, Key, Value, Output, Up, Gate, and Down projections. To ensure a fair comparison, we maintain a fixed number of trainable parameters when fine-tuning each component. The results show that the effectiveness of components in fine-tuning follows the order: Query/Key \(\) Value/Up/Gate \(<\) Output/Down. This is because Query/Key are only used to measure token similarities, while others serve as persistent memories of training data. Based on this finding, we allocate our parameter budget fairly to the Output and Down projections. For the LLama3-8B and Mistral-7B models, we only fine-tune the Down projection due to the inflexible selection in multi-query attention. Further analysis of this setting is left for future research.

**Selection across Channels**: In Section 3.2, we discuss several strategies for channel selection. In our main experiments, we employ random selection to ensure fair comparisons with baseline methods, as these approaches treat all channels with equal importance. However, the sparse structure of S\({}^{2}\)FT offers controllability during fine-tuning, allowing us to prioritize important channels in the selection process to further boost performance. Table 4 compared nine different strategies, incorporating five varying selection metrics (i.e., random, weight, activation, weight-activation product, and gradient), each choosing either the largest or smallest values. For S\({}^{2}\)FT-A, S\({}^{2}\)FT-S, and S\({}^{2}\)FT-G, we employ 1% of the fine-tuning data as a calibration set, introducing only negligible overhead during inference.

Our results demonstrate that random selection serves as a strong baseline due to its unbiased nature. Among heuristic metrics, selecting channels with the smallest activations (i.e., S\({}^{2}\)FT-A and S\({}^{2}\)FT-S) outperforms random selection. This indicates that these channels contain less task-specific information, enabling us to inject new knowledge through fine-tuning while preserving pre-trained capabilities in other channels. In contrast, other strategies introduce bias that compromises model performance. Notably, the counterintuitive accuracy decrease in S\({}^{2}\)FT-G (Large) suggests that channels with large gradients contain task-related pre-trained knowledge, and modifying them will disrupt these abilities.

    & ^{2}\)FT-R**} & ^{2}\)FT-W**} & ^{2}\)FT-A**} & ^{2}\)FT-S**} & ^{2}\)FT-G**} \\   & & Large & Small & Large & Small & Large & Small & Large & Small \\  Commonsense & 86.6 & 85.9\({}_{(0.7)}\) & 85.3\({}_{(1.1)}\) & 84.7\({}_{(1.5)}\) & 87.3\({}_{(4.7)}\) & 85.1\({}_{(1.5)}\) & 87.2\({}_{(0.66)}\) & 85.4\({}_{(1.2)}\) & 86.2\({}_{(0.4)}\) \\ Arithmetic & 79.6 & 78.4\({}_{(1.2)}\) & 78.4\({}_{(1.2)}\) & 77.1\({}_{(1.2)}\) & 80.0\({}_{(0.4)}\) & 76.8\({}_{(2.5)}\) & 79.8\({}_{(0.22)}\) & 77.8\({}_{(1.5)}\) & 79.5\({}_{(0.1)}\) \\   

Table 4: Comparison of various channel selection strategies on the commonsense and arithmetic reasoning datasets for the LLama3-8B. We report the average accuracy (%) as the evaluation metric.

Figure 4: The impact of different components in fine-tuning, including Query, Key, Value, Output, Up, Gate, and Down projection. We fix the trainable parameter budget and only fine-tune one component.

  
**Model** & **Method** & **Writing** & **Roleplay** & **Reasoning** & **Code** & **Math** & **Extraction** & **STEM** & **Humanities** & **Avg.** \\   & Vanilla & 5.25 & 3.20 & 4.50 & 1.60 & 2.70 & 6.50 & 6.17 & 4.65 & 4.32 \\  & Full FT & 5.50 & 4.45 & 5.45 & 2.50 & 3.25 & 5.78 & 4.75 & 5.45 & 4.64 \\  & LoRA & 5.30 & 4.40 & 4.65 & 2.35 & 3.30 & 5.50 & 5.55 & 4.30 & 4.41 \\  & Galore & 5.05 & 5.27 & 4.45 & 1.70 & 2.50 & 5.21 & 5.52 & 5.20 & 4.36 \\  & LISA & 6.84 & 3.65 & 5.45 & 2.20 & 2.75 & 5.65 & 5.95 & 6.35 & 4.85 \\  & **Ours** & **6.95** & 4.40 & **5.50** & **2.70** & **3.55** & 5.95 & **6.35** & **6.75** & **5.27** \\   & Vanilla & 2.75 & 4.40 & 2.80 & 1.55 & 1.80 & 3.20 & 5.25 & 4.60 & 3.29 \\  & Full FT & 5.55 & 6.45 & 3.60 & 1.75 & 2.00 & 4.70 & 6.45 & 7.50 & 4.75 \\   & LoRA & 6.30 & 5.65 & 4.05 & 1.60 & 1.45 & 4.17 & 6.20 & 6.20 & 4.45 \\   & Galore & 5.60 & 6.40 & 3.20 & 1.25 & 1.95 & 5.05 & 6.57 & 7.00 & 4.63 \\   & LISA & 6.55 & **6.90** & 3.45 & 1.60 & **2.16** & 4.50 & 6.75 & 7.65 & 4.94 \\   & **Ours** & **6.75** & 6.60 & **4.15** & **1.65** & 1.85 & 4.75 & **7.45** & **8.38** & **5.20** \\   

Table 3: Performance comparison of LLM fine-tuning methods trained on the Alpaca GPT-4 dataset. We report the MT-Bench score as the evaluation metric. All baseline results are cited from LISA.

Analysis

Having demonstrated the strong generalization capability and overall performance of S\({}^{2}\)FT, we now further explore its training efficiency and serving scalability compared to other fine-tuning techniques.

### Training Efficiency

To evaluate training efficiency, we examine two crucial metrics: peak memory footprint and average training latency. These numbers are measured on a single Nvidia A100 (80G) SXM GPU. We keep a comparable number of parameters for all methods. To obtain the average latency, we fine-tune the model for 50 runs, each run including 200 iterations, with 10 warmup runs excluded in measurement.

In Figure 5, we thoughtfully profile S\({}^{2}\)FT on various model sizes, sequence lengths, and batch sizes. Compared to Full FT, S\({}^{2}\)FT saves 1.4-3.0\(\) memory, and speedups fine-tuning by 1.5-2.7 times. When benchmarking against other PEFT methods, S\({}^{2}\)FT establishes new standards for efficiency, offering average reductions of 2% in memory usage and 9% in latency. Notably, S\({}^{2}\)FT outperforms the widely adopted LoRA, achieving about 10% improvement in both metrics by avoiding the need to store new parameters and perform additional calculations. Our partial back-propagation algorithm further improves efficiency by saving unnecessary forward activations and backward calculations.

### Serving Scalability

While S\({}^{2}\)FT avoids additional inference overhead for a single fine-tuned model through in-place gradient updates, we will now discuss its scalability for serving thousands of fine-tuned models. To begin, we introduce the unmerged computation paradigm of S\({}^{2}\)FT: Given a pre-trained weight matrix \(W^{pre}^{d k}\) and its corresponding fine-tuned weight matrix \(W\) with sparsity level \(s\), we define the weight difference as \( W=W-W^{}\). Similar to Section 4, \( W\) can be decomposed into the product of a weight matrix \(V^{k s}\) and a permutation matrix \(U^{d s}\). This decomposition allows us to "unmerge" an adapter \( W=UV^{}\) from \(W\), thereby sharing similarities with other adapters during inference. Following Zhong et al. , we consider three different adapter composition scenarios:

**Adapter Fusion.** To combine knowledge from multiple trained adapters, we employ weighted fusion when fine-tuning is impractical due to limited data access or computational resources. However, this approach degrades performance. In Table 5, we compare the effectiveness of LoRA and S\({}^{2}\)FT when combining adapters trained separately on commonsense and arithmetic reasoning tasks, where we consider both fine-tuning overlapped and non-overlapped parameters for different adapters in S\({}^{2}\)FT. Our results show that S\({}^{2}\)FT with non-overlapped parameters achieves the best performance, while the overlapped variant shows inferior results. This is because S\({}^{2}\)FT (non-overlap) modifies orthogonal low-rank spaces for different tasks. Similarly, LoRA largely retains task-specific capabilities during adapter fusion by optimizing low-rank projection matrices to create separate spaces for each adapter.

Figure 5: Comparison of memory and computation efficiency during training on the LLaMA2-7B/13B with varying sequence lengths and batch sizes. Average latency and peak memory usage are reported. S\({}^{2}\)FT significantly improves training latency while reducing memory footprint compared to baselines.

    &  & ^{2}\)FT**} \\   & Commonsense & Arithmetic & Fused & Commonsense & Arithmetic & Fused (overlap) & Fused (non-overlap) \\  Commonsense & 83.1 & 32.1 & 79.8\({}_{4,33.3}\) & 86.6 & 42.3 & 82.0\({}_{-4.0}\) & 84.0\({}_{ 6}\) \\ Arithmetic & 12.0 & 77.2 & 71.6\({}_{4,66}\) & 12.8 & 79.6 & 72.2\({}_{-4.0}\) & 75.3\({}_{4,+9}\) \\   

Table 5: Adapter Fusion Results for LoRA and S\({}^{2}\)FT trained on the commonsense and arithmetic reasoning datasets using the LLaMA3-8B. We report the average accuracy (%) as the evaluation metric.

**Adapter Switch.** Another way to leveraging multiple adapters is to dynamically switch between them. This process involves four steps: unfusing the old adapter, unloading it from memory, loading the new adapter, and fusing it into the model. In such setting, LoRA needs two matrix multiplications (matmul) and two additions (add) on GPU whereas S\({}^{2}\)FT only requires two sparse addition (scatter_add). In Figure 5(a), we increase the base weight dimension while maintaining a sparsity of 32 for S\({}^{2}\)FT and a low-rankness of 16 for LoRA. Notably, we observe that LoRA's switching time scales quadratically, while S\({}^{2}\)FT remains nearly constant. Moreover, in I/O-constrained scenarios such as deployment on CPU, S\({}^{2}\)FT further accelerates adapter switch by only updating a small fraction of the original weights, reducing the volume of I/O transfers, as time compared between scatter_add and add in Figure 5(b).

**Adapter Parallelism.** To serve thousands of adapters in parallel, we decompose the computation into separate batched computations for \(W^{pre}\) and \( W\) following S-LoRA . While LoRA requires two matmul and one add on GPU, S\({}^{2}\)FT reduces this to a matmul, an add, and either a scatter or gather for \(W_{1}\) and \(W_{2}\) in Section 3.1. Figure 5(c) shows that S\({}^{2}\)FT achieves up to 22% faster inference than LoRA under the same memory constraints, with more speedup as the number of adapters scales.

## 7 Related Work

PEFT methods reduce the fine-tuning cost for large models, which can be categorized into 4 groups:

**Adapter-based Fine-tuning** introduces additional trainable module into the original model. Series Adapters insert components between MHA or FFN layers , while parallel adapters add modules alongside existing components . Recently, ReFT  was introduced to directly learn interventions on hidden representations. However, they introduce additional latency during inference.

**Prompt-based Fine-tuning** adds randomly-initialized soft tokens to the input (usually as a prefix) and train their embeddings while freezing the model weights . These approaches result in poor performance compared to other groups, while come at the cost of significant inference overhead.

**Reparameterized Fine-tuning** utilizes low-rank projections to reduce trainable parameters while allowing operations with high-dimensional matrices. LoRA and its recent variants like DoRA, AsyLoRA , and FLoRA , use low-rank matrices to approximate additive weight updates during training. To alleviate the limitations of low-rank structure, other work also add or multiply orthogonal matrices to enable high-rank updating, including MoRA , OFT , and BOFT . These methods require no additional inference cost as the weight updates can be merged into models.

**Sparse Fine-tuning** aims to reduce the number of fine-tuned parameters by selecting a subset of pre-trained parameters that are critical to downstream tasks while discarding unimportant ones. This kind of methods are commonly used in the pre-LLM era . However, they cannot reduce the memory footprints due to their unstructured nature. Recent approaches address this limitation through three directions: (1) developing structured variants that sacrifice selection flexibility for better hardware efficiency , (2) incorporating sparsity into LoRA  but yield limited efficiency gains, or (3) using sparse operators for lower memory cost but slow down training .

Our work is based on the last category but achieving better performance and efficiency simultaneously. Additionally, we focus on scalable inference of PEFT methods, with S\({}^{2}\)FT being the only approach that enables effective fusion, rapid switching, and efficient parallelism when serving multiple adapters.

## 8 Conclusion

This paper introduces S\({}^{2}\)FT, a novel PEFT family that simultaneously achieves high quality, efficient training, and scalable serving for LLM fine-tuning. S\({}^{2}\)FT accomplishes this by selecting sparsely and compute densely. It selects a subset of heads and channels to be trainable for the MHA and FFN modules, respectively. The weight matrices from the two sides of the coupled structures in LLMs are co-permuted to connect the selected components into dense matrices, and only these parameters are updated using dense operations. We hope S\({}^{2}\)FT can be considered as a successor to LoRA for PEFT.

Figure 6: Comparison of latency for adapter switch and parallelism on a single linear layer. S\({}^{2}\)FT improves scalability for switch on GPU and CPU, while saving 22% time during parallelism on GPU.

Acknowledgement

We would like to thank Songlin Yang, Kaustubh Ponskhe, Raghav Singhal, Jinqi Luo, Tianqi Chen, Hanshi Sun, and Chris De Sa for their helpful discussions, and the authors of LLM-Adapters, ReFT, and DoRA for providing detailed results.