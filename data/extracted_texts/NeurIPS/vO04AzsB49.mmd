# Imitation Learning from Imperfection:

Theoretical Justifications and Algorithms

Ziniu Li\({}^{*}\)

Tian Xu\({}^{*}\)

Zeyu Qin

Yang Yu

Zhi-Quan Luo

###### Abstract

Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for sequential decision-making tasks. But, their effectiveness is hampered when faced with limited expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data has been proposed [25; 61], which enhances learning by incorporating an additional yet imperfect dataset obtained inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the potential inclusion of out-of-expert-distribution samples. In this work, we propose a mathematical formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a naive approach--applying the behavioral cloning (BC) algorithm concept to the combined set of expert and supplementary data--may fall short of vanilla BC, which solely relies on expert data. This deficiency arises due to the distribution shift between the two data sources. To address this issue, we propose a new importance-sampling-based technique for selecting data within the expert distribution. We prove that the proposed method eliminates the gap of the naive approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that our method outperforms previous state-of-the-art methods in tasks including robotic locomotion control, Atari video games, and image classification.1 Overall, our work underscores the potential of improving IL by leveraging diverse data sources through effective data selection.

2

3

4

5

## 1 Introduction

Imitation learning (IL) [3; 34] is an essential technique in artificial intelligence, allowing machines to enhance their performance by imitating expert behaviors. This technique has showcased remarkable achievements across various domains such as robotics , self-driving cars , and language models [55; 8]. Among the IL approaches, behavioral cloning (BC)  stands out as a popular method. In particular, BC leverages temporally-correlated state-action pairs extracted from expert demonstrations, employing them as training samples to learn a mapping from states to actions via maximum likelihood estimation. In this manner, IL expands upon the traditional supervised learning framework, enabling the acquisition of sequential decision-making capabilities.

The quantity of expert trajectories plays a crucial role in achieving satisfactory performance. Previous studies have shown that BC works well when the dataset contains a large number of expert-leveltrajectories . However, the well-known compounding errors issue  renders any offline IL algorithm, including BC, ineffective when the number of expert trajectories is small . Furthermore, collecting more trajectories from the expert is costly and impractical in many domains, such as robotics and healthcare.

To overcome the challenge of scarce expert data, we propose to use an additional yet imperfect dataset to supplement the expert data; see Figure 1 for illustration. In particular, this additional dataset can be cheaply obtained by executing sub-optimal policies.2 However, the incorporation of supplementary dataset introduces a distribution shift issue due to the presence of out-of-expert-distribution trajectories.3 The distribution shift issue may hamper the model's performance in utilizing the supplementary data, as we will argue later.

We realize that a number of empirical successes have been reported in this direction . Most algorithms rely on a discriminator to distinguish between expert-style and sub-optimal samples, followed by optimization of a weighted BC objective to learn a good policy. For example, DemoDICE  uses a regularized state-action distribution matching objective to train the discriminator, while DWBC  employs a cooperative training framework for the policy and discriminator. Despite the empirical successes in certain scenarios, there remains a notable absence of systematic theoretical studies, particularly in terms of _imitation gap_ (i.e., performance difference between the expert and learner), which may hinder deep understanding and impede future algorithmic advances.

We aim to bridge the gap between theory and practice in the (offline) IL with supplementary data framework by developing effective algorithms and providing rigorous theoretical guarantees. To the best of our knowledge, only  provided imitation gap bounds for a model-based adversarial imitation learning approach in a similar problem. However, our focus lies on the widely used and simpler BC and its variants, which are model-free in nature. Our contributions are summarized below.

* We develop a formal mathematical framework of the IL with supplementary data framework and conduct corresponding theoretical analysis. Our findings highlight the impact of the distribution shift between expert and supplementary data. Our results are summarized in Table 1.4 
In particular, our analysis shows that naively applying BC on the union of expert and supplementary data (referred to as the NBCU method in this paper) has a non-vanishing error term in the imitation gap bound (see the second row of Table 1). This means that the direct use of additional data might yield inferior performance compared with solely using the expert data with BC. This necessitates the development of more advanced algorithms.

   & Imitation Gap \\  BC & \((|H^{2}}{N_{}})\) \\ NBCU & \(}((1-)(V(^{})-V(^{}))+|H^{2}}{N_{}+N_{}})\) \\ ISW-BC & \((|H^{2}}{N_{}+N_{}/})\) \\  

Table 1: The theoretical guarantees of three methods: (1) BC, which relies solely on expert data; (2) NBCU, which directly uses the union of expert data and supplementary data without selection; and (3) ISW-BC, a new method that employs importance sampling for data selection. Compared with BC, NBCU suffers a non-vanishing error due to the distribution shift between two datasets while ISW-BC does not.

Figure 1: Compared with the conventional IL framework (shown in cyan), the _supplementary data_ helps address the expert data scarcity issue, and the _data selection_ technique helps address the distribution shift issue in model training.

* To address the distribution shift issue, we propose a new approach called ISW-BC, which uses the importance sampling technique to select data within the expert distribution. In contrast to prior methods [25; 61], ISW-BC re-weights data in an unbiased way. We develop a new imitation gap bound for ISW-BC (see the last row of Table 1), revealing that it not only eliminates the gap exhibited by the naive approach but also offers a superior guarantee over BC.
* Our theoretical analysis has been validated through extensive experiments, including robotic locomotion control, Atari video games, and image classification. The results affirm the superiority of ISW-BC over existing methods, thus demonstrating the potential of our method in addressing the distribution shift issue in IL with supplementary data.

## 2 Related Work

We review briefly relevant studies in the main text and provide a detailed discussion in Appendix B.

**Behavioral Cloning.** Behavioral cloning (BC) is a popular algorithm in the offline setting, where the learner cannot interact with the environment. According to the learning theory in , only using an expert dataset, BC has an imitation gap of \((||H^{2}/N_{})\), where \(||\) is the state space size, \(H\) is the planning horizon, and \(N_{}\) is the number of expert trajectories. Our work investigates the use of a supplementary dataset to enhance the dependence on the data size.

**Adversarial Imitation Learning.** In contrast to BC, adversarial imitation learning (AIL) methods, such as GAIL , perform imitation through state-action distribution matching. It has been demonstrated both empirically and theoretically that AIL methods do not suffer from the compounding errors issue when the expert data is limited [21; 16; 59; 60; 28; 63]. Under mild conditions,  provided a horizon-free bound of \((\{1,|/N_{}}\})\), which is much better than BC in terms of dependence on \(H\). However, AIL methods work naturally in the online setting (i.e., the interaction is allowed), which is not directly applicable in the offline setting that we study in this paper. Although the proposed method has a discriminator and a policy like AIL, our discriminator and policy are not designed to compete with each other adversarially, as we will explain in detail later.

**Imitation Learning with Supplementary Data.** Our theoretical study is motivated by recent empirical successes in IL with supplementary data [25; 61; 32; 24]. Compared with [25; 61], a related setting, learning from observation, is studied in [32; 24]. In this setting, expert actions are absent, and only expert states are observed. The importance sampling technique used in our method for addressing distribution shift is also studied in (semi-)supervised learning [49; 11; 30; 15]. Our contribution is to show this technique is also effective in the imitation learning, where data has a Markovian structure.

## 3 Preliminaries

**Markov Decision Process.** In this paper, we consider the episodic Markov decision process (MDP) framework . An MDP is defined by the tuple \(=(,,,r,H,)\), where \(\) and \(\) are the state and action space, respectively. \(H\) is the maximum length of a trajectory, and \(\) is the initial state distribution. The non-stationary transition function is specified by \(=\{P_{1},,P_{H}\}\), where \(P_{h}(s_{h+1}|s_{h},a_{h})\) determines the probability of transiting to state \(s_{h+1}\) given the current state \(s_{h}\) and action \(a_{h}\) in time step \(h\), for \(h[H]\). Here the symbol \([x]\) means the set of integers from \(1\) to \(x\). Similarly, the reward function \(r=\{r_{1},,r_{H}\}\) specifies the reward received at each time step, where \(r_{h}:\) for \(h[H]\). A policy in an MDP is a function that maps each state to a probability distribution over actions. We consider time-dependent policies \(_{h}:()\), where \(()\) is the probability simplex. The policy at each time step \(h\) is denoted as \(_{h}\), and we use \(\) to denote the collection of time-dependent policies \(\{_{h}\}_{h=1}^{H}\) when the context is clear.

We measure the quality of a policy \(\) by the policy value (i.e., environment-specific long-term return): \(V()=_{h=1}^{H}r_{h}(s_{h},a_{h}) s_{1};a_{ h}_{h}(|s_{h}),s_{h+1} P_{h}(|s_{h},a_{h}), h [H]\). To facilitate later analysis, we need to introduce the state-action distribution \(d_{h}^{}(s,a)=(s_{h}=s,a_{h}=a|)\). We use the convention that \(d^{}\) is the collection of time-dependent state-action distributions.

**Imitation Learning.** Imitation learning (IL) aims to learn a policy that mimics an expert policy based on expert demonstrations. In this paper, we assume that there exists a good expert policy \(^{}\) that generates a dataset \(^{}\) consisting of \(N_{}\) trajectories of length \(H\).

\[^{}==(s_{1},a_{1},s_{2},a_{2}, ,s_{H},a_{H})\,;s_{1},a_{h}_{h}^{}(|s_{h} ),s_{h+1} P_{h}(|s_{h},a_{h}), h[H]}.\]The learner aims to imitate the expert using the expert dataset \(^{}\). The quality of the imitation is measured by the _imitation gap_, defined as \([V(^{})-V()]\), where the expectation is taken over the randomness of data collection. It is worth noting that in the training phase, the IL learner does _not_ have access to reward information. A good learner should closely mimic the expert, resulting in a small imitation gap. We assume that the expert policy is deterministic, a common assumption in the literature [42; 43; 60], and applicable to tasks such as MuJoCo locomotion control.

**Behavioral Cloning.** Behavioral cloning (BC) is a popular imitation learning algorithm that aims to learn a policy from an expert dataset \(^{}\) via supervised learning. Specifically, BC seeks to find a policy \(^{}\) that maximizes the log-likelihood of the expert actions in the dataset:

\[^{}*{argmax}_{}_{h=1}^{H}_{(s,a) }}_{h}}(s,a)_{h}(a|s),\] (1)

where \(}_{h}}(s,a)\) is the empirical state-action distribution in the expert dataset. By the maximum likelihood estimation (MLE), BC can make good decisions by duplicating expert actions on states visited in \(^{}\). However, BC may take sub-optimal actions on non-visited states, resulting in compounding errors and a large imitation gap. This issue is significant when the expert data is limited.

## 4 Imitation Learning with Supplementary Data

To address the challenge of limited availability of expert data, we consider an IL with a supplementary dataset framework. We assume that a supplementary yet imperfect dataset \(^{}==(s_{1},a_{1},s_{2},a_{2}, ,s_{H},a_{H})}\) is collected by a behavior policy \(^{}\). A naive approach is to perform MLE on the _union_ of the expert and supplementary dataset \(^{}=^{}^{}\):

\[^{}*{argmax}_{}_{h=1}^{H}_{(s,a)} }_{h}}(s,a)_{h}(a|s),\] (2)

where \(}_{h}}(s,a)\) is the empirical state-action distribution in \(^{}\). We refer to this approach as NBCU (naive BC with the union dataset). NBCU treats these two datasets equally and is brittle to distribution shift, as we will demonstrate later. For theoretical analysis purpose, we assume expert data represents a \(\) fraction of the total union data.

**Assumption 1**.: _The expert dataset \(^{}\) and supplementary dataset \(^{}\) are collected in the following way: each time, we roll-out a behavior policy \(^{}\) with probability \(1-\) and the expert policy with probability \(\). Such an experiment is independent and identically conducted \(N_{}\) times._

Under Assumption 1, we slightly overload our notations: we use \(N_{}\) to denote the _expected_ number of expert trajectories, which is given by \(N_{}= N_{}\), and \(N_{}\) to denote the _expected_ number of supplementary trajectories, which is given by \(N_{}=(1-)N_{}\). Note that the probabilistic sampling procedure does not change the nature of our theoretical insights. In practice, one may collect a fixed number of expert and supplementary trajectories, respectively.

To establish a common ground, we begin by specifying the policy representations. Here, we adopt tabular representations, which assume that the parameterized value functions can take any possible form. Specifically, we define \(_{h}(a|s;)=(s,a),\), where \((s,a)^{d}\) is the feature representation and \(^{d}\) is the parameter to optimize. In tabular representations, we use one-hot features for \((s,a)\). For a discussion on general function approximation schemes, please refer to Appendix E.

**Imitation Gap of BC.** In order to evaluate the usefulness of the supplementary dataset, we use BC with only the expert dataset as a baseline. The analysis of this approach has been done in the conventional IL set-up in , and we re-state their results in our setting.

**Theorem 1**.: _Under Assumption 1, if we apply BC only on the expert dataset, we have that \([V(^{})-V(^{})]=( |H^{2}}{N_{}})\), where the expectation is taken over the randomness in the dataset collection (same as other expectations)._

Proofs of Theorem 1 and other theoretical results are deferred to the Appendix. The proof of Theorem 1 builds on , with the main difference being that the number of expert trajectories is a random variable in our set-up. We handle this difficulty by using Lemma 3 in the Appendix. The quadratic dependence on the planning horizon \(H\) indicates the compounding errors issue of BC.

If the expert data is limited (i.e., \(N_{}\) is small), the performance gap can be large, suggesting poor performance of BC in this scenario.

**Imitation Gap of NBCU** Guarantees of naively using the supplementary data are presented below.

**Theorem 2**.: _Under Assumption 1, if we apply BC on the union dataset, we have \([V(^{})-V(^{})]=( (1-)(V(^{})-V(^{}))+|H^{2}(N_{ })}{N_{}})\)._

**Remark 1**.: _In case when the behavior policy is inferior to the expert policy, we have \(V(^{})-V(^{})>0\). In this case, even if \(N_{}\) is large enough to make the second term negligible, there is still a non-vanishing gap of \(V(^{})-V(^{})\) due to the behavior policy's potential to collect non-expert actions. In other words, the recovered policy may select wrong actions even on expert states, leading to the sub-optimal performance of NBCU._

_It is worthy to note that this non-vanishing error term can also be interpreted from the viewpoint of the distribution shift. Specifically, using the analysis in , we can show that_

\[V(^{})-V(^{})=(H_{d})= (H^{2}_{}),\]

_where \(_{d}=_{h}(d_{h}^{^{}},d_{h}^{^{ }})\) is the state-action distribution total variation (TV) distance and \(_{}=_{h}_{s}(_{}^{}( |s),_{h}^{}(|s))\) is the policy distribution TV distance. Hence, we can also view Theorem 2 in the context of state-action or policy distribution shifts._

_Note also that we do not claim that NBCU is always worse than BC. Instead, Theorem 2 implies that if the distribution shift between two data datasets is small, NBCU could be better than BC by leveraging more data, which we will also show in experiments later._

The next proposition establishes the inevitability of the gap \(V(^{})-V(^{})\) in the worst case.

**Proposition 1**.: _Under Assumption 1, there exists an MDP \(\), an expert policy \(^{}\) and a behavior policy \(^{}\), such that we have \([V(^{})-V(^{})]=((1- )(V(^{})-V(^{})))\)._

The hard instance in Proposition 1 builds on the following idea: NBCU considers all action labels in the union dataset equally important. Therefore, we can build an instance where the expert \(\) selects a good action with an one-step reward of \(1\), while the behavior policy \(^{}\) chooses a bad action with an one-step reward of \(0\). The noise introduced by \(^{}\) results in incorrect learning goals, causing NBCU to make a mistake with probability \(1-\), which is the fraction of the noise in the union dataset. By a carefully designed transition probability, we can obtain the expected bound in Proposition 1.

## 5 Addressing Distribution Shift with Importance Sampling

In this section, we propose a data selection approach to alleviate the distribution shift issue between expert data and supplementary data. Our approach is inspired by recent works , where a discriminator is trained to re-weight samples, and a weighted BC objective is used for policy optimization. Specifically, we define the weighted BC objective as follows:

\[^{}*{argmax}_{}_{h=1}^{H}_{(s,a )}^{}}(s,a) [w_{h}(s,a)_{h}(a|s)][w_{h}(s,a) ]},\] (3)

where \(^{}}(s,a)\) is the empirical state-action distribution of the union dataset, and \(w_{h}(s,a)[0,)\) is the weight decided by the discriminator. We introduce a hyper-parameter \([0,)\) to better select samples.

We propose using the importance sampling technique [46, Chapter 9] to transfer samples in the union dataset to the expert policy distribution, which is the key idea behind our method. This technique helps address the failure mode of NBCU. In an ideal scenario where there are infinite samples (i.e., at the population level), \(^{}}\) would equal \(d_{h}^{}\). By setting \(w_{h}(s,a)=d_{h}^{}(s,a)/d_{h}^{}(s,a)\), we obtain \(^{}}(s,a)w_{h}(s,a)=d_{h}^{}(s,a)\), and the objective (3) enables the learning of a policy as if samples were collected by the expert policy. However, in practice, \(d_{h}^{}(s,a)\) and \(d_{h}^{}(s,a)\) are unknown, and we only have a finite number of samples from each of these distributions. Therefore, we must estimate the grounded importance sampling ratio \(d_{h}^{}(s,a)/d_{h}^{}(s,a)\) from the expert data and union data.

We emphasize that estimating the probability densities of high-dimensional distributions separately for expert and union data and then calculating their quotient can be a challenging task. We take a different approach. Inspired by , we directly train a discriminator to estimate the importance sampling ratio \(d_{h}^{}(s,a)/d_{h}^{}(s,a)\). To this end, we introduce time-dependent parameterized discriminators \(\{c_{h}:\}_{h=1}^{H}\), each of which is optimized according to the objective function

\[_{c_{h}}_{(s,a)}^{ }}(s,a)[(c_{h}(s,a))]+_{(s,a) }^{}}(s,a)[ (1-c_{h}(s,a))].\] (4)

Solving the optimization problem in (4) is equivalent to training a binary classifier that assigns positive labels to expert data and negative labels to union data. We can obtain the optimal discriminator at the population level, from which we can derive the importance sampling ratio formula:

\[c_{h}^{}(s,a)=^{}(s,a)}{d_{h}^{}(s,a)+d_{h }^{}(s,a)}, w_{h}(s,a)=^{}(s,a)}{1-c_{h}^{ }(s,a)}.\] (5)

Based on the previous discussion, we present the implementation of our proposed method, named ISW-BC (importance-sampling-weighted BC), in Algorithm 1. It is worth noting that ISW-BC employs an unbiased weighting rule since it directly estimates the importance sampling ratio. In contrast, previous approaches such as [25; 61] use regularized weighting rules that may fail to recover the expert policy even with infinite samples. For further details on the differences between our method and previous ones, please refer to Appendix B.

### Negative Result of ISW-BC with Tabular Representations of Discriminator

We have not yet specified the representations of the discriminator. One natural choice is to use tabular representations, which correspond to linear function approximation with one-hot features. Tabular representations have a strong representation power since they can span all possible functions. However, surprisingly, we show that tabular representations can fail when considering generalization.

**Proposition 2**.: _If the discriminator uses the one-hot feature with \(=0\), we have \(^{BC}}=^{}\)._

Proposition 2 suggests that even if we have a large number of supplementary samples and use importance sampling, ISW-BC is not guaranteed to outperform BC based on tabular representations. To illustrate, suppose we have a sample \((s,a)\) that is an expert-style sample but only appears in the supplementary dataset, meaning that \(d_{h}^{}(s,a)>0,^{}}(s,a)=0\) and \(^{}}(s,a)>0\). Using tabular representations, we can compute the closed-form solution \(c_{h}^{}(s,a)=^{}}(s,a)/(^{ }}(s,a)+^{}}(s,a))=0\). This implies that the importance sampling ratio \(w_{h}(s,a)=c_{h}^{}(s,a)/(1-c_{h}^{}(s,a))=0\), so this good sample does not contribute to the learning objective (3). The failure of tabular representations is due to their discrete treatment of data, ignoring internal correlations. Consequently, although they work well in minimizing the empirical loss, they are not good at _generalization_. This kind of failure mode is also mentioned in the GAN literature .

### Positive Result of ISW-BC with Function Approximation of Discriminator

In this section, we address the issue raised in the previous section by investigating ISW-BC with a specific function approximation. To avoid the limitations of tabular representations, we consider that the discriminator is parameterized by \(c_{h}(s,a;_{h})=(s,a),_{h}))}\), where \(_{h}:^{d}\) is a fixed feature mapping and \(_{h}^{d}\) is the parameter to be trained. Note that we require \(d<||||\) to avoid the tabular representations. Let \(g(x)=(1+(x))\). Then, the optimization problem of

Figure 2: Illustration for ISW-BC. Please refer to the text below Assumption 2 for a detailed explanation.

the discriminator becomes:

\[_{_{h}}_{h}(_{h})_{(s,a)}^{ }}(s,a)g(-_{h}(s,a),_{h})+_{(s,a)} {d_{h}^{}}(s,a)g(_{h}(s,a),_{h})\] (6)

Let \(^{}=\{_{1}^{},,_{H}^{}\}\) be the optimal solution obtained from Eq. (6). With the feature vector, samples are no longer treated independently, and the discriminator can perform _structured_ estimation. To be consistent with the previous results, the policy is still based on tabular representations.

In the context of general linear function approximation, it is no longer possible to obtain a closed-form solution for \(c^{}\) as in Eq. (5). This raises the question: what can we infer about \(c^{}\)? Our intuition can be described as follows. We can envision the supplementary dataset containing two types of samples: some that were in-expert distribution, and others that were out-of-expert distribution. We expect that \(w_{h}(s,a)\) is large in the former case and small in the latter case. Note that \(w_{h}(s,a)\) is monotonic with respect to the inner product \(_{h}(s,a),\). Therefore, we conclude that a larger value of \(_{h}(s,a),\) implies a more significant contribution to the learning objective (3). In the following part, we demonstrate that the aforementioned intuition can be achieved under mild assumptions.

**Assumption 2**.: _Let \(_{h}^{}\) denote the set of state-action pairs in \(^{}\) in \(h\). Define \(_{h}^{,1}=\{(s,a)_{h}^{}:d_{h}^{ }(s)>0,a=_{h}^{}(s)\}\) as the in-expert-distribution dataset in \(_{h}^{}\) and \(_{h}^{,2}=_{h}^{}_{h}^{,1}\) as the out-of-expert-distribution dataset. There exists a ground truth parameter \(_{h}^{d}\), for any \((s,a)_{h}^{}_{h}^{,1}\) and \((s^{},a^{})_{h}^{,2}\), it holds that \(_{h},_{h}(s,a)>0\) and \(_{h},_{h}(s^{},a^{})<0\)._

Readers may realize that Assumption 2 is closely related to the notion of "margin" in the classification problem. Define \(_{h}()_{(s,a)_{h}^{} _{h}^{,1}},_{h}(s,a)-_{(s^{ },a^{})_{h}^{,2}},_{h}(s ^{},a^{})\). From Assumption 2, we have \(_{h}(_{h})>0\). This means that there _exists_ a classifier that recognizes samples from both \(_{h}^{}\) and \(_{h}^{,1}\) as in-expert-distribution samples and samples from \(_{h}^{,2}\) as out-of-expert-distribution samples. Note that such a nice classifier is assumed to exist, which is not identical to what is learned via Eq. (6). Before further discussion, we note that \(_{h}\) is not unique if it exists. Without loss of generality, we define \(_{h}\) as that can achieve the maximum margin (among all unit vectors).

Let us delve into the technical challenge that arises from Assumption 2. Although we assume two modes in the supplementary dataset, the learner is not aware of them beforehand. To gain a better understanding, refer to Figure 2, where the "star" corresponds to the expert data and the "triangle" corresponds to the supplementary data. The green and red parts of the triangle represent \(^{,1}\) and \(^{,2}\), respectively. While training the discriminator, we assign positive labels (shown in "+") to the expert data and negative labels (shown in "-") to the union data. Consequently, it becomes challenging to determine the learned decision boundary theoretically. To address this challenge, we develop the landscape properties, Lipschitz continuity and quadratic growth conditions, in Lemma 1 and Lemma 2, respectively. These terminologies are from the optimization literature; see . Incorporating these properties will enable us to infer the learned decision boundary.

**Lemma 1**.: _For any \(^{d}\), the margin function is \(L_{h}\)-Lipschitz continuous in the sense that \(_{h}(_{h})-_{h}() L_{h}\| _{h}-\|\), where \(L_{h}=\|_{h}(s^{1},a^{1})-_{h}(s^{2},a^{2})\|\) with \((s^{1},a^{1})*{argmin}_{(s,a)_{h}^{} _{h}^{,1}},_{h}(s,a)\) and \((s^{2},a^{2})*{argmax}_{(s,a)_{h}^{,2}} ,_{h}(s,a)\)._

**Lemma 2**.: _For any \(h\), let \(A_{h}^{N_{} d}\) be the matrix that aggregates the feature vectors of samples in \(_{h}^{}\). Assume that \((A_{h})=d\), then \(_{h}\) (defined in Eq. (6)) has a (one-sided) quadratic growth condition. That is, there exists \(_{h}>0\) such that \(_{h}(_{h})_{h}(_{h}^{})+}{2}\|_{h}-_{h}^{}\|^{2}.\)_

Using Lemma 1 and Lemma 2, we are ready to obtain the imitation gap bound of ISW-BC.

**Theorem 3**.: _Under Assumptions 1 and 2, let \(=_{(s,h)[H]}d_{h}^{}(s,_{h}^{} (s))/d_{h}^{}(s,_{h}^{}(s))<\), if the feature is designed such that \(_{h}(_{h})-_{h}(_{h}^ {}))}{_{h}}}<(_{h})}{L_{h}}\) holds, then we have \(_{h}(_{h}^{})>0\). Furthermore, we have the imitation gap bound \([V(^{})-V(^{})]=(|^{H}}{N_{}+N_{}/})\)._

In order to interpret Theorem 3, it is important to note that \(_{h}(_{h}^{})>0\) means that there exists a \(>0\) such that \(w_{h}(s,a;_{h}^{})>\) for \((s,a)_{h}^{}_{h}^{,1}\) and \(w_{h}(s,a;_{h}^{})<\) for \((s,a)_{h}^{,2}\). As a result, all samples from \(_{h}^{_{h}^{}\) and \(_{h}^{}\). This number is represented as \(N_{}+N_{}/\), where \(\) is a state-action coverage parameter. It is important to mention that a similar notation is used in the literature of offline RL, as seen in [33; 10]. Additionally, ISW-BC has the ability to eliminate the gap of NBCU, meaning there is no non-vanishing error in Theorem 3. Moreover, ISW-BC can perform well even when \(_{h}^{}\) has noisy action labels, a scenario where NBCU may fail.

Although Theorem 3 produces desirable outcomes, it does have some limitations. First, the theoretical analysis necessitates knowledge of \(\), which is typically challenging to determine beforehand. However, our empirical findings in Section 6 demonstrate that setting \(=0\) is effective in practice. Second, Theorem 3 mandates the use of good smooth features to ensure the required inequality holds, thereby avoiding the undesirable case presented in Proposition 2. Our paper does not offer a solution for finding such feature representations. Nevertheless, our experiments indicate that neural networks can usually learn suitable features. We present a simple mathematical example corresponding to Theorem 3 in Appendix D.5. We leave more general results of ISW-BC to future work.

## 6 Experiments

To validate the theoretical claims, we perform numerical experiments. We provide a brief overview of the experiment set-up below, and the details can be found in Appendix H.

### Robotic Locomotion Control

In this section, we present our experiment on locomotion control, where we train a robot to run like a human in four environments from the Gym MuJoCo suite : Ant, Hopper, Halfcheetah, and Walker. We adopt online SAC  to train an agent for each environment with 1M steps, and consider the resultant policy as the expert. For each environment, the expert data contains 1 trajectory collected by the expert policy. We consider two types of supplementary datasets:

* Full Replay (small distribution shift): the supplementary dataset (1 million samples) is directly sampled from the experience replay buffer of the online SAC agent, which is suggested by . This setting has a small distribution shift as the online agent quickly converges to the expert policy (see Figure 5 in the Appendix), resulting in abundant expert trajectories in the replay buffer.
* Noisy Expert (large distribution shift): the supplementary dataset consists of 10 clean expert trajectories and 5 noisy expert trajectories where the action labels are corrupted (i.e., replaced by random actions). This introduces a large state-action distribution shift, as discussed in Remark 1. For further discussion on dataset corruption and distribution shift, please refer to Appendix E.2.

Besides our proposed methods, we also evaluate two state-of-the-art methods in the locomotion control domain: DemoDICE  and DWBC . We report the experiment results in Table 2.

We observe that BC suffers since the amount of expert data is limited. In the full replay task, NBCU performs well due to the small distribution shift. However, in the noisy expert task, NBCU's average

    & & Ant & HalfCheetah & Hopper & Walker & Avg \\   & Random & \(-326\) & \(-280\) & \(-20\) & \(2\) & \(0\%\) \\  & Expert & \(5229\) & \(11115\) & \(3589\) & \(5082\) & \(100\%\) \\  & BC & \(1759_{ 287}\) & \(931_{ 273}\) & \(2468_{ 164}\) & \(1738_{ 311}\) & \(38\%\) \\   & NBCU & \(4932_{ 148}\) & \(10566_{ 86}\) & \(3241_{ 276}\) & \(4462_{ 105}\) & \(92\%\) \\  & DemoDICE & \(}\) & \(10781_{ 67}\) & \(3394_{ 93}\) & \(}\) & **94\%** \\  & DWBC & \(2951_{ 155}\) & \(1485_{ 377}\) & \(2567_{ 88}\) & \(1572_{ 225}\) & \(44\%\) \\  & ISW-BC & \(4933_{ 110}\) & \(}\) & \(}\) & \(4475_{ 164}\) & **94\%** \\   & NBCU & \(3259_{ 159}\) & \(5561_{ 539}\) & \(558_{ 23}\) & \(518_{ 56}\) & \(35\%\) \\  & DemoDICE & \(2523_{ 244}\) & \(6020_{ 346}\) & \(1990_{ 90}\) & \(1685_{ 160}\) & \(49\%\) \\   & DWBC & \(}\) & \(5688_{ 557}\) & \(}\) & \(1985_{ 175}\) & \(62\%\) \\   & ISW-BC & \(3075_{ 298}\) & \(}\) & \(2624_{ 249}\) & \(}\) & **69\%** \\   

Table 2: Environment return of algorithms on 4 robotic locomotion control tasks. Digits correspond to the mean performance over 5 random seeds and the subscript \(\) indicates the standard deviation. “Avg” computes the normalized score over environments. Same as the other tables.

performance is inferior to that of BC5, while ISW-BC outperforms NBCU in average significantly, demonstrating the robustness of ISW-BC to distribution shift. It is worth noting that among all evaluated methods, only our proposed method ISW-BC performs well in both settings. Prior methods such as DemoDICE and DWBC only perform well in one of the two settings.

### Atari Video Games

In this section, we evaluate algorithms on Atari games , which involve video frames as inputs and discrete controls as outputs. Furthermore, environment transitions are stochastic for these games. We consider 5 games, namely Alien, MsPacman, Phoenix, Qbert, and SpaceInvaders. We obtain the offline expert data and supplementary data from the replay buffer of an online DQN agent, as released by . We use the expert data from the buffer with the last index, which only contains 50k frames, to create a challenging learning setting. To augment this data, we use earlier replay buffer data to obtain supplementary data with approximately 200k frames. We consider the same baselines as in Section 6.1. All methods build on the classical convolutional neural networks used in DQN.

Similar to Section 6.1, we consider two types of supplementary data. The full replay setting involves supplementary data that is close to the expert data, exhibiting a small distribution shift. The noisy expert setting has noisy action labels, leading to a large distribution shift. Experiment details can be found in Appendix H.1.2. We report the game scores of the trained policies in Table 3.

Our observations are consistent with those of the previous experiments. NBCU performs well when the distribution shift is small, while only ISW-BC is robust when the distribution shift is large.

### Image Classification

In our final experiment, we tackle an image classification task. This task is a special type of imitation learning where the planning horizon is 1 and there are no environment transitions. The reward is classification accuracy. Please note that our main purpose here is to use this degraded one-step task to verify the theoretical results.

We use a famous dataset, DomainNet , which comprises 6 sub-datasets (clipart, infograph, painting, quickdraw, real, and sketch) that have different feature patterns and hence distribution shifts; see Figure 3 for an illustration. Following , our task is to perform 10-class classification (bird, feather, headphones, ice_cream, teapot, tiger, whale, windmill, wine_glass, and

    & & Alien & MsPacman & Phoenix & Qbert & SpaceInvaders & Avg \\   & Random & \(-228\) & \(307\) & \(761\) & \(164\) & \(148\) & \(0\%\) \\  & Expert & \(2443\) & \(3601\) & \(4869\) & \(10955\) & \(1783\) & \(100\%\) \\  & BC & \(1051_{ 31}\) & \(1799_{ 27}\) & \(1520_{ 56}\) & \(4769_{ 111}\) & \(472_{ 10}\) & \(32\%\) \\   & NBCU & \(1405_{ 28}\) & \(2089_{ 48}\) & \(}\) & \(}\) & \(600_{ 13}\) & **50\%** \\  & DemoDICE & \(1401_{ 16}\) & \(2164_{ 52}\) & \(2192_{ 72}\) & \(7820_{ 200}\) & \(558_{ 29}\) & \(48\%\) \\  & DWBC & \(122_{ 4}\) & \(1251_{ 66}\) & \(583_{ 33}\) & \(1078_{ 30}\) & \(287_{ 8}\) & \(7\%\) \\  & ISW-BC & \(}\) & \(2162_{ 36}\) & \(2299_{ 76}\) & \(7848_{ 237}\) & **613\({}_{ 16}\)** & **50\%** \\   & NBCU & \(944_{ 22}\) & \(1378_{ 30}\) & \(1491_{ 55}\) & \(4366_{ 458}\) & \(418_{ 14}\) & \(27\%\) \\  & DemoDICE & \(1054_{ 58}\) & \(1604_{ 59}\) & \(1448_{ 112}\) & **5354\({}_{ 295}\)** & \(395_{ 10}\) & \(31\%\) \\   & DWBC & \(643_{ 18}\) & \(656_{ 16}\) & \(1165_{ 87}\) & \(3860_{ 104}\) & \(296_{ 5}\) & \(16\%\) \\   & ISW-BC & \(}\) & \(}\) & \(}\) & \(5247_{ 328}\) & **497\({}_{ 6}\)** & **36\%** \\   

Table 3: Environment return of algorithms on 5 Atari video games.

Figure 3: Samples of tiger class from 6 sub-datasets of the DomainNet  dataset. Infograph and quickdraw have quite different patterns (i.e., distribution shift) compared with the others.

zebra) using 80% of the images for training and 20% for test. Each sub-dataset has roughly 2000-5000 images.

We build the classifier on the pretrained ResNet-18 , as directly training ResNet-18 on the DomainNet dataset failed. We then optimize a 2-hidden-layer neural network, where inputs are from the feature representations extracted by the pretrained and fixed ResNet-18. We create 6 sub-tasks, where one of the 6 sub-datasets is used as the expert data while the other 5 sub-datasets are used as the supplementary datasets. We evaluate the classification accuracy on the expert test data. Note that there is no natural extension of DemoDICE for this task. More details can be found in Appendix H.1.3.

The results of our experiment are presented in Table 4. We observe that due to the presence of distribution shifts, NBCU performs even worse than BC, even though NBCU use more data than BC. On the other hand, ISW-BC can improve the performance over BC on 5 out of 6 tasks by re-weighting the supplementary data. At the same time, ISW-BC is more effective than DWBC.

## 7 Discussion and Conclusion

This paper introduces a formal mathematical framework for imitation learning with a supplementary yet imperfect dataset, which is designed to address the scarcity of expert data. Within this framework, we present new theoretical insights that illuminate the distribution shift challenge between expert and supplementary data. To deal with this challenge, we devise a new method named ISW-BC, employing the importance sampling technique to select data within the expert distribution. Through both theoretical analysis and empirical evaluations, we show the superiority of the proposed approach.

Our research is closely connected with data-centric artificial intelligence (AI) [39; 54; 64]. Here, the emphasis lies in the quality, availability, and management of data as foundational elements for constructing effective AI models and applications. The importance sampling technique developed in this paper proves valuable for processing imperfect data.

Furthermore, our methods can extend beyond the scope of consideration in this paper. To illustrate, the core concept of data re-weighting and selection can find utility in the realm of large language models. In specific downstream tasks with limited expert data, we can judiciously select a set of pre-training tasks to construct supplementary data and to enhance overall performance of language models; for additional insights, please refer to recent progress in [58; 18]. Other potential avenues for future exploration include the extension to multi-task imitation learning, as well as the unsupervised case where the expert data is not available.6

Overall, our findings demonstrate the potential of improving imitation learning performance by leveraging diverse data sources through effective data selection. We aspire for this work to serve as inspiration for future advancements in the field.