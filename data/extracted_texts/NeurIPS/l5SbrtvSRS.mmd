# Parameter Competition Balancing for Model Merging

Guodong Du\({}^{1}\) Junlin Lee\({}^{1}\) Jing Li\({}^{1}\) Runhua Jiang\({}^{2}\) Yifei Guo\({}^{2}\) Shuyang Yu\({}^{2}\)

Hanting Liu\({}^{3}\) Sim Kuan Goh\({}^{2}\) Ho-Kin Tang\({}^{1}\) Daojing He\({}^{1}\) Min Zhang\({}^{1}\)

\({}^{1}\)Harbin Institute of Technology, Shenzhen, China

\({}^{2}\)Xiamen University Malaysia

\({}^{3}\)Johns Hopkins University

duguodong7@gmail.com jingli.phd@hotmail.com denghaojian@hit.edu.cn

 Equal contribution.Corresponding authors.

###### Abstract

While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named Pcb-Merging (Parameter Competition Balancing), a _lightweight_ and _training-free_ technique that adjusts the coefficients of each parameter for effective model merging. Pcb-Merging employs intra-balancing to gauge parameter significance within individual tasks and interbalancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: https://github.com/duguodong7/pcb-merging.

## 1 Introduction

Pre-trained models (PTMs) are fundamental in deep learning, underpinning many current techniques due to their ability to learn generalized features from large datasets . Fine-tuning PTMs for specific tasks is a common practice to boost performance . This approach is prevalent, resulting in thousands of fine-tuned checkpoints , based on widely used PTMs . However, fine-tuning the same model for different tasks can result in performance variations, posing a significant challenge . Multi-task learning  has been proposed as a solution, but it incurs substantial training costs and requires simultaneous access to data and labels for all tasks . Recently, some researchers have developed methods to merge multiple independently-trained models into a single model without the need for original training data . This merging technique not only adheres to data privacy regulations  but also enhances efficiency by eliminating the need for retraining.

Previous research [20; 86; 27] has shown that averaging the weights of multiple task-specific models, fine-tuned from the same pre-trained initialization, can enhance performance across various tasks. Many studies [46; 30] have explored the creation of additional matrices, matching the model dimensions, to adjust parameter coefficients for different tasks. Other studies [28; 89; 90; 96; 94] focus on task vectors , defined as the differences between the parameter values of the fine-tuned model and the original pre-trained model. While these task vector-based methods have shown promising results, they typically apply a uniform coefficient for each task and parameter, which may limit their effectiveness. Our research seeks to fully harness task vector-based methods by fine-tuning parameter-level coefficients through a balancing mechanism that resolves parameter competition.

Parameter competition is crucial in model fusion, occurring both within parameters of the same task and among models for different tasks. Firstly, within a single model, task-specific fine-tuned parameters often compete, where some are critical while many prove redundant. Previous research [89; 94] has demonstrated that dropping numerous parameters based on task vector magnitude can maintain performance close to the original. Additionally, appropriately rescaling important parameters and suppressing redundant ones can further enhance the performance of the fine-tuned model (see Fig. 1). Secondly, between different models, parameters also engage in competition (see Fig. 2). Rescaling a task vector for one task can boost performance for that specific task but may negatively affect cross-task capabilities. Therefore, balancing the coefficients assigned to task vectors requires careful consideration of their impact on overall performance.

We argue that merging methods capable of managing intra-parameter competition within tasks demonstrate _self-awareness_, while those that balance inter-parameter competition between tasks exhibit _cross-awareness_. We systematically compare and analyze existing model merging methods in terms of these criteria, as presented in Tab. 1. To establish a balancing matrix that is both self-aware and cross-aware for parameter scaling, we introduce Pcb-Merging (**P**arameter **C**ompetition **B**alancing for Model Merging), a _training-free_ and _dataless_ method for merging models. Specifically, we use intra-balancing to weight the importance of parameters within tasks and inter-balancing to assess parameter similarities across tasks. Low-scoring parameters are then dropped, and the remaining ones are rescaled. Finally, we merge the modulated task vectors into the pretrained model to create the final merged model.

To empirically demonstrate the effectiveness of Pcb-Merging, we conducted extensive experiments comparing it with existing model merging approaches. We showcased the superiority of our

   Method & Drop & Scale & Self-aware & Cross-aware & Granularity Level \\  Fisher Merging (Pcb-Merging)  & - & Fisher Matric & ✓ & ✗ & Parameter \\ RegMean(Pcb-Merging)  & - & Inner Product Matric & ✗ & ✓ & Parameter \\ Task Arithmetic(Pcb-Merging)  & - & Uniformed & ✗ & ✗ & Task \\ TIES-Merging(Pcb-Merging)  & Magnitude & Uniformed & ✓ & ✓ & Parameter \\ DARE(Pcb-Merging)  & Bernoulli (\(p\)) & \(1/(1-p)\) & ✓ & ✗ & Parameter \\ LoraHub(Pcb-Merging)  & - & Evolver Searched & ✗ & ✓ & Task \\ AdaMerging(Pcb-Merging)  & - & Unsupervised Optimized & ✗ & ✓ & Layer \\ 
**Pcb-Merging (ours)** & Competition & Balancing Matric & ✓ & ✓ & Parameter \\   

Table 1: Comparison of different model merging methods. A merging method is deemed _self-aware_ if it manages parameter competition within individual task models, and _cross-aware_ if it balances competition within a population of task models. For more details, please refer to App. A.

Figure 1: Parameter competition within individual task models. Intra-balancing enhances performance beyond finetuning.

Figure 2: Parameter competition within task model populations. Inter-balancing improves cross-task generalization.

approach from four perspectives: (1) Cross-task merging: We evaluated our approach across a range of NLP and Vision tasks using various models, such as T5 , ViT , and Llama2 . We also assessed its ability to fuse multiple PEFT [42; 24] adapters. All experiments demonstrated significant improvements over previous state-of-the-art methods, notably achieving a 4.3% performance increase with the T5-base model. (2) Cross-domain merging: Our approach merged multiple domain-specific models for tasks like emotion classification [53; 30], demonstrating its effective handling of diverse domain data. (3) Cross-training configurations: Merging multiple models from different training environments on single tasks, highlighting its flexibility and robustness. (4) Out-of-Domain Generalization: We assessed multi-task and multi-domain fusion performance on domain shift datasets, testing generalizability across various frameworks.

This paper makes three significant **contributions**: (1) We re-examine existing model merging methods, highlighting the critical role of parameter competition awareness; (2) We introduce a novel approach called Pcb-Merging, which effectively adjusts parameter coefficients through balancing parameter competition; (3) Our proposed method stabilizes and enhances model merging performance across various application scenarios without additional training.

## 2 Related Work

### Overview of model fusion

Deep model fusion is gaining attention due to data privacy and resource conservation concerns, with potential applications across various domains [39; 14]. It's typically divided into three main categories. Ensemble learning , combines model outputs to improve prediction accuracy and robustness but requires parallel deployment of multiple models. An alternative method involves mode connectivity  and alignment , aiming to bring solutions closer together for better initial conditions in averaging. This is achieved by either linking optimization paths [15; 98] or addressing permutation invariances [72; 79; 40; 30]. Recent researches [88; 75] focus on training-free approaches to enhance model fusion usability. The third approach, weight averaging [20; 86], requires models with identical structures. While advancements like  support merging diverse large language models (LLMs), they require knowledge distillation  and complex training. This paper follows the third type of track due to its simplicity, efficiency, and broad applicability.

### Merging fine-tuned models with same initialization

Previous studies found that when multiple models are fine-tuned from the same pre-trained initialization, averaging their weights can lead to improved performance on single tasks [20; 86; 13; 29; 92] different tasks  and out-of-distribution generalization [3; 60]. Fisher Merging  goes beyond simple averaging to identify the importance of individual parameters using Fisher information matrix  and uses it to weigh the parameters in each model when merging. RegMean  proposed a closed-form solution for the merged model's parameters by solving a local linear regression problem for each individual linear layer in the model. However, both the Fisher Merging and RegMean methods are time-consuming and computationally intensive.

Task Arithmetic  introduces the concept of _task vectors_, demonstrating their effectiveness and lightweight nature in facilitating cross tasks generalization. Expanding on this groundwork, PEM Composition  extends the task arithmetic framework to merge LoRA  models, while Ties-Merging  addresses task conflicts by resetting redundant parameters and resolving sign conflicts. However, these methods share a merging coefficient across all task vectors, limiting flexibility. In contrast, Lorahub  and AdaMerging  utilize different coefficients for enhanced adaptability, but Lorahub's performance is restricted as it only searches coefficients at the task level. AdaMerging also demands complex training and unlabeled test datasets and is applicable solely to classification problems. DARE  proposes drop and rescale as a preprocessing step when merging fine-tuned LLMs. Our approach primarily employs strategies of dropping to minimize interference and rescaling at the parameter level, while considering both self-awareness and cross-model awareness.

## 3 Method

In Sec. 3.1, we established the notation and outlined the problem of model merging. Sec. 3.2 delves into the detailed exposition of the proposed Pcb-Merging method, which aims to balance parameter competition. Furthermore, in Sec. 3.3, we employ evolutionary algorithms to further enhance the performance of our approach.

### Preliminaries

Initially, we are faced with a set of tasks \(\{T_{1},,T_{n}\}\) and various pre-trained models, such as ViT , T5 , or llama2 . We have the option to fine-tune the entire model or employ a parameter-efficient fine-tuning (PEFT) method [42; 24]. During fine-tuning, we represent the trainable parameters as \(\), initialized as \(_{}\), and the fine-tuned parameters as \(_{}\). The model merging problem involves how to combine the weight sets \(\{_{1},,_{n}\}\) to form a new weight \(_{m}\), without the need to retrain using the initial training data for each task, and ensuring that \(_{m}\) can simultaneously perform tasks \(\{1,,N\}\).

Recent research  introduced the concept of _task vectors_ and completed various task arithmetic operations and model merging based on task vectors. Specifically, for task \(T_{i}\), the task vector \(_{i}^{}\) is defined as the vector obtained by subtracting the fine-tuned weights \(_{i}\) from the pre-trained weights \(_{}\), i.e., \(_{i}=_{i}-_{}\). This allows us to focus on the changes that occur during each task-specific model's fine-tuning phase. The task vector-based multi-task model merging method can be expressed as \(_{m}=_{}+*_{i=1}^{n}_{i}\), where the coefficient \(\) represents the importance of merged task vector \(_{m}\). This concept is simple yet effective, significantly outperforming simple weight averaging schemes, i.e., \(_{m}=(1/N)_{i=1}^{n}_{i}\).

### Parameter Competition Balancing

Our approach aims to modulate the scaling factors for each task and parameter, achieving intra-balancing and inter-balancing within and between tasks. Specifically, we use the parameter competition balancing (PCB) matrix \(_{i}^{d}\) to adjust the scale of parameters in each task model \(_{i}^{d}\), resulting in the final fused model, as shown in Fig. 3. The specific calculation process is as follows:

1. **Intra-Balancing:** Initially, we implement self-awareness by applying a nonlinear activation function (i.e., softmax) to the magnitudes of task vectors, emphasizing important parameters while suppressing redundant ones to some extent. As the number of fusion tasks increases, competition among parameters intensifies. Therefore, the number of tasks \(N\) is used to control the extent of suppression applied to redundant parameters. "Norm" refers to normalization. \[_{intra,i}=(N*(_{i}_{i}))\] (1)

Figure 3: An illustration of the steps in Pcb-Merging. Different colored blocks represent parameters with varying values. We start with multiple fine-tuned models and a pretrained model, establishing a **PCB** matrix through intra-balancing and inter-balancing. Low-scoring parameters are dropped, and the remaining ones are rescaled. Finally, we merge the modulated task vectors into the pretrained model to create the final merged model.

2. **Inter-Balancing:** Next, we realize cross-awareness to enable the parameters within a population of tasks to interact with others, addressing potential conflicts and complex correlations between tasks. To achieve this, we compute the similarity between parameters at the same positions across different task vectors, allowing each parameter to update its score based on information from other tasks. The calculation process is as follows: \[_{inter,i}=_{j=1}^{n}((_{i} _{j}))\] (2)
3. **Drop and Rescale:** Subsequently, we obtain \(_{i}=_{intra,i}_{inter,i}\). Next, we construct a mask \(m_{i}^{d}\) based on \(_{i}\) to focus on the more important parameters. Specifically, this mask \(m_{i}\) is used to select high-scoring elements from the \(D\) elements of \(_{i}\). We define the mask ratio as \(r\), where \(0<r 1\). The mask \(m_{i}\) can be derived from: \[m_{i,d}=1,&_{i,d}(_{i})[(1-r)  D]\\ 0,&\] (3) The importance score is defined as \(=m_{i}_{i}\). Finally, we use the score of the masked balancing matrix to weight the importance of each parameter in each task vector. The final merged task vector \(_{m}\) is as follows: \[_{m}=_{i=1}^{n}(_{i}_{i})/ _{i=1}^{n}_{i}\] (4)

From the final merged task vector \(_{m}\), we can further adjust its magnitude proportionally and integrate it with the initial parameter values to yield the amalgamated model parameters \(_{m}\), represented by \(_{m}=_{}+*_{m}\), with \(\) serving as a scaling hyperparameter. More details about the method workflow are presented in App. A and Algorithm 1.

### Searching Coefficients

Research from articles [28; 90] shows that model merging methods based on task vectors are highly sensitive to the merging coefficient \(\). Even with an appropriately chosen uniform \(\), achieving further improvements in fusion performance necessitates grid searching the merging coefficients for each task vector, which becomes increasingly complex and time-consuming, especially when managing a large number of tasks.

Inspired by prior research [77; 25], we employ intelligent optimization algorithms to search for mixing coefficients, aiming for greater improvements compared to using a uniform coefficient. The optimization process seeks the best set \(\{_{1},,_{n}\}\) to enhance validation accuracy, with the ultimate goal of maximizing validation accuracy with the merged model.

\[_{m}=_{}+_{i=1}^{n}(_{i} _{i}_{i})/_{i=1}^{n}_{i}\] (5)

In most of our experimental setups, we primarily utilize Covariance Matrix Adaptive Evolution Strategies (CMA-ES) . As a probabilistic population-based optimization algorithm, CMA-ES dynamically adjusts the search distribution defined by the covariance matrix. It systematically updates the mean and covariance of this distribution at each iteration to learn and exploit the underlying structure of the search space for optimization efficiency.

## 4 Experimental setup

Evaluation Settings.We anticipate that merging models will offer two significant advantages for developers. Firstly, by integrating insights from individual models \(_{1..n}\) trained in different environments (such as tasks, domains, or various training configurations within a single task), we expect the resulting merged model \(_{m}\) to demonstrate competitive test performance across tasks, domains, or within a single task. Secondly, this merged model is poised to exhibit enhanced cross-domain (OOD) generalization capability. For further details about compute resources and fine-tuning procedures, please refer to App. F.1 and F.2.

Baseline Methods.Our baselines are primarily divided into two categories: non-model merging, which involves fine-tuned individual models and multitask learning, and various advanced model merging methods such as simple averaging , Fisher merging , RegMean , Task Arithmetic , Ties-Merging , and AdaMerging . Detailed information on these baselines can be found in App. E. Notably, Task Arithmetic, Ties-Merging, AdaMerging, and our proposed Pcb-Merging method are all based on task vectors. In addition, when merging LLMs across different tasks, we present the results with DARE  as preprocessing. Since AdaMerging demands unlabeled test datasets and is applicable solely to classification problems, we compare with it only when merging finetuned ViT models for image classification, as shown in App. C.2.

Validation Set.Most model merging methods necessitate access to a validation set, utilized for computing the Fisher matrix or tuning hyperparameters. While ReMean can derive inner product matrices for each task using unlabeled training data, additional validation is required to ascertain the optimal value of the non-diagonal multiplier \(\). Both Fisher merging and ReMean are time-consuming and require significant computational resources. In contrast, task vector-based methods are more lightweight and training-free to implement and can be utilized even without a validation set. Therefore, we conducted additional experiments to compare task vector-based methods without a validation set.

Hyperparameters.When no additional validation is performed, we use a default value of \(=1\) for all task-vector based methods. For TIES-Merging and Pcb-Merging, which require a masking ratio, we set mask ratio \(r=0.2\) as the default value for all experiments, except in LLM experiments where \(r=0.1\).

When validation is allowed, we set the non-diagonal multiplier \(\) in RegMean to 0.9, except for the T5-base model where it is set to 0.1. For Task Arithmetic, we conduct a search over \(\) ranging from 0.2 to 1.5 with a step size of 0.1. For TIES-Merging and Pcb-Merging, we search over ratios in {0.05, 0.1, 0.2}, and \(\) ranging from 0.8 to 2.5 with a step size of 0.1. In cases where evolutionary strategies are employed for coefficient search for each task, we conduct continuous variable searches within the range of 0.8 to 2.5. For more hyperparameter details, please refer to App. F.3 and Tab. 17.

## 5 Results

In this section, we evaluated the performance of the Pcb-Merging method across various experimental settings, including cross-task, cross-domain, cross-training configurations, and out-of-domain scenarios. Additionally, we conducted several experiments to further assess the effectiveness of our method: merging different numbers of tasks (App. C.1 and Fig. 8), comparison with AdaMerging on vision tasks (App. C.2 and Tab. 7), and providing additional results using evolutionary strategies (ES) (App. C.3 and Tab. 8). Lastly, we present comprehensive task-level results in App. C.4.

### Cross Task Merging

Merging NLP Models.For the NLP domain, we adhere to the experimental setting from . We employ the T5-base and T5-large  models and fine-tune both on seven tasks. This setting considers a variety of NLP domains such as question answering, paraphrase identification, sentence completion, and coreference resolution (dataset details in App. D). Tab. 2 shows that using Pcb-Merging to merge fully fine-tuned T5-base and T5-large models leads to an average improvement of 4.3% and 3.5% over 7 tasks, without extra data. With validation datasets, Pcb-Merging improves by 1.8% and 1.8% over other methods for T5-base and T5-large, respectively. Notably, Pcb-Merging without validation outperforms TIES-merging  by 5.4% for T5-large. For more detailed results, refer to App. Tab. 9 and 10.

Merging PEFT Model Adapters.Following the work of , we consider merging parameters used for efficient fine-tuning calculations and employ the (IA)\({}^{3}\) method for experimentation. This approach, a form of Parameter-Efficient Fine-Tuning (PEFT), extends the activations of base models with learned vectors. We select T0-3B  as the base model and fine-tune (IA)\({}^{3}\) models on the training sets of eleven datasets, including sentence completion, natural language inference, coreference resolution, and word sense disambiguation (dataset details in App. D). During fine-tuning of the T0-3B model, we utilize prompt templates from the Public Prompt Pool (P3 ) to convert each example in each dataset into a text-to-text format, where each label corresponds to a different string. For experiments with (IA)3, we report the median score across all templates for each dataset. Tab. 2 illustrates that Pcb-Merging achieves an average improvement of 1.2% and 1.3% across 11 tasks compared to the top baseline, both with and without validation set. For further details, please refer to App. Tab. 11.

Merging LLMs.In our experiment, we merged three specialized large language models based on the Llama-2-7b architecture --focusing on Chinese language proficiency4, mathematical reasoning 4, and code generation 5. Each model was assessed using tailored benchmarks: CMMLU  for Chinese, GSM8K  for math, and HumanEval  for code generation (dataset details in App. D). As shown in Tab. 3, Pcb-Merging improved overall performance by an average of 0.8% (no DARE) and 0.6% (with DARE). The most significant performance gain was in code generation, with 3.7% improvement without DARE and 2.5% with DARE . The results indicate that although the DARE preprocessing provided modest improvements, our proposed methodology notably enhanced the overall performance.

Merging Vision Models.For image classification tasks, we adopt the experimental setup outlined by Ilharco et al. . We utilize two versions of the CLIP model  featuring ViT-B/32 and ViT-L/14 models  as visual encoders. Subsequently, we fine-tune the visual encoder on eight tasks sourced from Ilharco et al.  and Radford et al. , while maintaining the text encoder unchanged. This configuration encompasses diverse classification domains including remote sensing, traffic classification, and satellite imagery recognition (dataset details in App. D). Pcb-Merging performs better than the top baseline by 3.5% and 0.9% for ViT-B/32 and ViT-L/14, respectively, when validation is not utilized. With additional data, these improvements are 2.7% and 1.5%, respectively, and further increase to 3.4% and 2.1% after incorporating evolutionary search. For more detailed findings, please refer to App. Tab. 11, 13 and Fig. 9.

    &  &  &  &  &  \\  Method (\(\)) & &  &  &  &  &  & ViT-L/14 \\  Fine-tuned & - & 83.1 & 88.9 & 71.4 & 40.4 & 90.5 & 94.2 \\ Multitask & - & 83.6 & 88.1 & 73.1 & - & 88.9 & 93.5 \\  Averaging\(\) & ✗ & 65.3 & 54.7 & 57.9 & 30.3 & 65.8 & 79.6 \\ Task Arithmetic\(\) & ✗ & 53.5 & 73.6 & 59.2 & 30.4 & 60.4 & 83.3 \\ Ties-Merging\(\) & ✗ & 69.5 & 71.7 & 64.9 & 34.2 & 72.4 & 86.0 \\
**Pcb-Merging (ours)** & ✗ & **73.8 (+4.3)** & **77.1 (+3.5)** & **66.1 (+1.2)** & **35.1 (+0.9)** & **75.9 (+3.5)** & **86.9 (+0.9)** \\  Fisher Merging\(\) & ✓ & 68.3 & 68.7 & 62.2 & - & 68.3 & 82.2 \\ RegMean\(\) & ✓ & 72.7 & 79.8 & 58.0 & - & 71.8 & 83.7 \\ Task Arithmetic\(\) & ✓ & 73.0 & 80.2 & 63.9 & 30.4 & 70.1 & 84.5 \\ Ties-Merging\(\) & ✓ & 73.6 & 80.3 & 66.8 & 34.2 & 73.6 & 86.0 \\
**Pcb-Merging (ours)** & ✓ & **75.4 (+1.8)** & **82.1 (+1.8)** & **68.1 (+1.3)** & **35.1 (+0.9)** & **76.3 (+2.7)** & **87.5 (+1.5)** \\
**Pcb-Merging + ES (ours)** & ✓ & **76.7 (+3.1)** & **83.2 (+2.9)** & **68.8 (+2.0)** & **35.3 (+1.1)** & **77.0 (+3.4)** & **88.1 (+2.1)** \\   

Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.

   Model & DARE & CMMLU & GSM8K & Human-Eval & Average \\  Chinese & - & 38.6 & 2.3 & 13.4 & 18.1 \\ Math & - & 31.2 & 65.6 & 0 & 32.3 \\ Code & - & 33.3 & 0 & 17.1 & 16.8 \\  Averaging & ✗ & 35.6 & 48.5 & 6.7 & 30.3 \\ Rot

[MISSING_PAGE_FAIL:8]

We fine-tuned 10 models for each dataset using a random hyperparameter search over learning rate, batch size, and number of epochs (training details in App. F.2). Additionally, we randomly selected training subsets with 1000 examples from the entire training datasets, resulting in each subset having different label distributions. We use the standard metric for each dataset: average of accuracy and \(F_{1}\) score for MRPC, accuracy for RTE, Matthews correlation  for CoLA and accuracy for SST-2. We repeated this experiment with different random seeds and reported the average results across five seeds. Tab. 4 presents the corresponding metrics on the validation set, showing consistent performance improvements with Pcb-Merging across all datasets.

## 6 Analysis

### Ablation of Pcb-Merging Components

We conducted ablation experiments on various components of our approach to assess their importance. Tab. 5 compares the performance of our method with different components removed, testing ViT-B/32 and T5-base models on the validation set. Removing the _Rescale_ step implies using a uniform scale \(=1\) and computing a disjoint mean as in TIES-Merging , ignoring zero values. The table demonstrates the crucial importance of all components for achieving optimal performance. Specifically, the _Drop_ component was found to be the most critical, resulting in performance drops of \(5.1\%\) for ViT-B/32 and \(4.9\%\) for T5-base, respectively. More ablation study details are provided in App. B.1 and Tab. 6.

### Effect of Hyper-Parameters on the Performance.

We examined the impact of hyper-parameters \(\) and \(r\) on the performance when merging multiple NLP tasks, as discussed in Section 5.1. Initially, we illustrate the performance of various models across different values of \(\) while keeping \(r=0.1\). Our method is compared against the state-of-the-art baseline method, TIES-Merging. From Fig. 6, We can observe that our approach demonstrates a higher performance ceiling within the suitable range of 1.4 to 1.8. As \(\) increases, the performance initially decreases and then saturates. Additionally, we provide a performance analysis for different ratios \(r\). We conduct a grid search for \(\) to determine its optimal performance for each ratio. Notably, for \(r<0.3\), our method consistently showcases significant improvements. This underscores the importance of the information filtered out by our parameter competition balancing approach in the merging process. More analysis about hyper-parameters are shown in App. B.2 and Fig. 7.

### Limitation and Future Work

While our approach provides valuable insights into model merging, several limitations should be noted: (1) Pcb-Merging, like previous methods, relies on identical model architectures and shared initializations, constraining its applicability across various model types. (2) Limited theoretical understanding: model merging effectiveness may be influenced by task independence  and weight disentanglement [55; 54], warranting further exploration. (3) Our approach does not effectively address parameter redundancy, still relying on drop operations to mitigate interference and improve performance. (4) Task vector magnitudes may not always effectively represent parameter importance, necessitating further exploration for more efficient methods.

Figure 6: Performance with various hyperparameters \(\) and \(r\).

 
**Task(\(\))** & **Vision** & **NLP** \\ Method(\(\)) & ViT-B/32 & T5-base \\  w/o Intra-Balance & 74.4 & 73.7 \\ w/o Inter-Balance & 74.8 & 73.9 \\ w/o Drop & 71.2 & 70.5 \\ w/o Rescale & 73.8 & 72.9 \\  Pcb-Merging & **76.3** & **75.4** \\  

Table 5: Ablation study on individual components of Pcb-Merging.

Conclusions

In summary, we introduce Pcb-Merging to tackle challenges in model merging by incorporating parameter competition balancing to rescale task vectors at the parameter level. Our method enhances model merging performance without requiring additional training, leading to improved stability and effectiveness across various scenarios. We demonstrate significant advancements in cross-task merging, cross-domain merging, different training configurations, and out-of-domain generalization, highlighting its potential impact in practical applications.