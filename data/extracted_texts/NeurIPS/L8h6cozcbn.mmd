# Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression

Deqing Fu   Tian-Qi Chen   Robin Jia   Vatsal Sharan

Department of Computer Science

University of Southern California

{deqingfu,tchen939,robinjia,vsharan}@usc.edu

###### Abstract

Transformers excel at _in-context learning_ (ICL)--learning from demonstrations without parameter updates--but how they do so remains a mystery. Recent work suggests that Transformers may internally run Gradient Descent (GD), a first-order optimization method, to perform ICL. In this paper, we instead demonstrate that Transformers learn to approximate second-order optimization methods for ICL. For in-context linear regression, Transformers share a similar convergence rate as _Iterative Newton's Method_; both are exponentially faster than GD. Empirically, predictions from successive Transformer layers closely match different iterations of Newton's Method _linearly_, with each middle layer roughly computing 3 iterations; thus, Transformers and Newton's method converge at roughly the same rate. In contrast, Gradient Descent converges _exponentially_ more slowly. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, to corroborate our empirical findings, we prove that Transformers can implement \(k\) iterations of Newton's method with \(k+(1)\) layers.

+
Footnote †: Our codes are available at https://github.com/DeqingFu/transformers-icl-second-order.

## 1 Introduction

Transformer neural networks (Vaswani et al., 2017) have become the default architecture for natural language processing (Devlin et al., 2019; Brown et al., 2020; OpenAI, 2023). As first demonstrated by GPT-3 (Brown et al., 2020), Transformers excel at _in-context learning_ (ICL)--learning from prompts consisting of input-output pairs, without updating model parameters. Through in-context learning, Transformer-based Large Language Models (LLMs) can achieve state-of-the-art few-shot performance across a variety of downstream tasks (Rae et al., 2022; Smith et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022).

Given the importance of Transformers and ICL, many prior efforts have attempted to understand how Transformers perform in-context learning. Prior work suggests Transformers can approximate various linear functions well in-context (Garg et al., 2022). Specifically to linear regression tasks, prior work has tried to understand the ICL mechanism, and the dominant hypothesis is that Transformers learn in-context by running optimization internally through gradient-based algorithms (von Oswald et al., 2022, 2023; Ahn et al., 2023; Dai et al., 2023; Mahankali et al., 2024).

This paper presents strong evidence for a competing hypothesis: Transformers trained to perform in-context linear regression learn a strategy much more similar to a second-order optimization method than a first-order method like Gradient Descent (GD). In particular, Transformers approximately implement a second-order method with a convergence rate very similar to Newton-Schulz's Method, also known as the _Iterative Newton's Method_, which iteratively improves an estimate of the inverse ofthe data matrix to compute the optimal weight vector. Across many Transformer layers, subsequent layers approximately compute more and more iterations of Newton's Method, with increasingly better predictions; both eventually converge to the optimal minimum-norm solution found by ordinary least squares (OLS). Interestingly, this mechanism is specific to Transformers: LSTMs do not learn these same second-order methods, as their predictions do not even improve across layers.

We present both empirical and theoretical evidence for our claims. Empirically, Transformer layers demonstrate a similar rate of convergence to the OLS solution as second-order methods such as Iterative Newton, which is substantially faster than the rate of convergence of GD (Figure 2). The predictions made by the Transformer at successive layers closely match the predictions made by Iterative Newton after a proportional number of iterations, showing that they progress in similar ways at the same rate. In contrast, to match the Transformer's predictions after \(k\) layers, GD would have to run for exponential in \(k\) many steps (Figure 3). Some individual Transformer layers make progress equivalent to hundreds of GD steps: these layers must be doing something more sophisticated than GD. Furthermore, a crucial aspect of second-order methods is that they can handle ill-conditioned problems by correcting the curvature. We find that the convergence rate of Transformers is not significantly affected by ill-conditioning, which again matches Iterative Newton but not GD. To provide theoretical grounding to our empirical results, we show that Transformer circuits can efficiently implement Iterative Newton: one transformer layer can compute one Newton iteration (given \((1)\) pre/post-processing layers), and requires hidden states of dimension \((d)\) for a \(d\)-dimensional linear regression problem. Overall, our work provides a mechanistic account of how Transformers perform ICL that explains model behavior better than previous hypotheses, and hints at why Transformers are well-suited for ICL relative to other architectures.

## 2 Related Work

In-context learning by large language models.GPT-3 (Brown et al., 2020) first showed that Transformer-based large language models can "learn" to perform new tasks from in-context demonstrations (i.e., input-output pairs). Since then, a large body of work in NLP has studied in-context learning, for instance by understanding how the choice and order of demonstrations affects results (Lu et al., 2022; Liu et al., 2022; Rubin et al., 2022; Su et al., 2023; Chang and Jia, 2023; Nguyen and Wong, 2023), studying the effect of label noise (Min et al., 2022; Yoo et al., 2022; Wei et al., 2023), and proposing methods to improve ICL accuracy (Zhao et al., 2021; Min et al., 2022; 2022; 2022).

In-context learning beyond natural language.Inspired by the phenomenon of ICL by large language models, subsequent work has studied how Transformers learn in-context beyond NLP tasks. Garg et al. (2022) first investigated Transformers' ICL abilities for various classical machine learning problems, including linear regression. We largely adopt their linear regression setup in this work. Li et al. (2023) formalize in-context learning as an algorithm learning problem. Han et al. (2023) suggests that Transformers learn in-context by performing Bayesian inference on prompts, which can be asymptotically interpreted as kernel regression. Other work has analyzed how Transformers do in-context classification (Tarzanagh et al., 2023; 2023; Zhang et al., 2023), the role of pertaining data (Raventos et al., 2023), and the relationship between model architecture and ICL (Lee et al., 2023).

Do Transformers implement Gradient Descent?A growing body of work has suggested that Transformers learn in-context by implementing gradient descent within their internal representations. Akyurek et al. (2022) summarize operations that Transformers can implement, such as multiplication and affine transformations, and show that Transformers can implement gradient descent for linear regression using these operations. Concurrently, von Oswald et al. (2022) argue that Transformers learn in-context via gradient descent, where one layer performs one gradient update. In subsequent work, von Oswald et al. (2023) further argue that Transformers are strongly biased towards learning to implement gradient-based optimization routines. Ahn et al. (2023) extend the work of von Oswald et al. (2022) by showing Transformers can learn to implement preconditioned Gradient Descent, where the pre-conditioner can adapt to the data. Bai et al. (2023) provide detailed constructions for how Transformers can implement a range of learning algorithms via gradient descent. Finally, Dai et al. (2023) conduct experiments on NLP tasks and conclude that Transformer-based language models performing ICL behave similarly to models fine-tuned via gradient descent; however, concurrent work (Shen et al., 2023) argues that real-world LLMs do not perform ICL via gradient descent. Mahankali et al. (2024) showed that implementing gradient descent is a global minima for single layer linear self-attention. However, we study deeper models in this work, which can behave differently from single-layer models. In this paper, we argue that Transformers actually learn to perform in-context learning by implementing a second-order optimization method, not gradient descent1.

Mechanistic interpretability for Transformers.Our work attempts to understand the mechanism through which Transformers perform in-context learning. Prior work has studied other aspects of Transformers' internal mechanisms, including reverse-engineering language models (Wang et al., 2022), the grokking phenomenon (Power et al., 2022; Nanda et al., 2023), manipulating attention maps (Hassid et al., 2022), and circuit finding (Conmy et al., 2023).

Theoretical Expressivity of Transformers. Giannou et al. (2023) provide a construction of looped transformers to implement Iterative Newton's method for solving pseudo-inverse, and each Newton iteration can be implemented by 13 looped Transformer layers. In contrast, our construction needs only one Transformer layer to compute one Newton iteration.

## 3 Problem Setup

In this paper, we focus on the following linear regression task. The task involves \(n\) examples \(\{_{i},y_{i}\}_{i=1}^{n}\) where \(_{i}^{d}\) and \(y_{i}\). The examples are generated from the following data generating distribution \(P_{}\), parameterized by a distribution \(\) over \((d d)\) positive semi-definite matrices. For each sequence of \(n\) in-context examples, we first sample a ground-truth weight vector \(^{}}{}(,) ^{d}\) and a matrix \(}{}\). For \(i[n]\), we sample each \(_{i}}{}(,)\). The label \(y_{i}\) for each \(_{i}\) is given by \(y_{i}=^{}_{i}\). Note that for much of our experiments \(\) is only supported on the identity matrix \(\) and hence \(=\), but we also consider some distributions over ill-conditioned matrices, which give rise to ill-conditioned regression problems. Most of our results are on this noiseless setup and results with the noisy setup are in Appendix A.3.2.

### Standard Methods for Solving Linear Regression

Our central research question is:

_What convergence rate does the algorithm Transformers learn for linear regression achieve?_

To investigate this question, we first discuss various known algorithms for linear regression. We then compare them with Transformers empirically in SS4 and theoretically in SS5, to evaluate if Transformers are more similar to first-order or second-order methods. We care particularly about algorithms' convergence rates (the number of steps required to reach an \(\) error).

For any time step \(t\), let \(^{(t)}=[_{1}_{t}]^{}\) be the data matrix and \(^{(t)}=[y_{1} y_{t}]^{}\) be the labels for all the datapoints seen so far. Note that since \(t\) can be smaller than the data dimension \(d\), \(^{(t)}\) can be singular. We now consider various algorithms for making predictions for \(_{t+1}\) based on \(^{(t)}\) and \(^{(t)}\). When it is clear from context, we drop the superscript and refer to \(^{(t)}\) and \(^{(t)}\) as \(\) and \(\), where \(\) and \(\) correspond to all the datapoints seen so far.

Ordinary Least Squares.This method finds the minimum-norm solution to the objective:

\[(,)=\|-\|_{2}^ {2}.\] (1)

The Ordinary Least Squares (OLS) solution has a closed form given by the Normal Equations:

\[}^{}=(^{})^{}^{}\] (2)

Figure 1: Illustration of how Transformers are trained to do in-context linear regression.

where \(:=^{}\) and \(^{}\) is the pseudo-inverse (Moore, 1920) of \(\).

**Gradient Descent.** Gradient descent (GD) is a first-order method which finds the weight vector \(}^{ GD}\) with initialization \(}_{0}^{ GD}=\) using the iterative update rule:

\[}_{k+1}^{ GD}=}_{k}^{ GD}-_{} (}_{k}^{ GD},).\] (3)

It is known that GD requires \(\) (\(()(1/)\)) steps to converge to an \(\) error where \(()=()}{_{ min}()}\) is the _condition number_. Thus, when \(()\) is large, GD converges slowly (Boyd and Vandenberghe, 2004).

**Online Gradient Descent.** While GD computes the gradient with respect to the full data matrix \(\) at each iteration, Online Gradient Descent (OGD) is an online algorithm that only computes gradients on the newly received data point \(\{_{k},y_{k}\}\) at step \(k\):

\[}_{k+1}^{ OGD}=}_{k}^{ OGD}-_{k}_{}(}_{k}^{ OGD}_{k},y_{k}).\] (4)

Picking \(_{k}=_{k}\|_{2}^{2}}\) ensures that the new weight vector \(}_{k+1}^{ OGD}\) makes zero error on \(\{_{k},y_{k}\}\).

**Iterative Newton's Method.** This is a second-order method which finds the weight vector \(}^{ Newton}\) by iteratively apply Newton's method to finding the pseudo inverse of \(=^{}\)(Schulz, 1933; Ben-Israel, 1965).

\[&_{0}=,\,\,= ^{}\|_{2}},\,\,\,}_{0}^{ Newton}= {M}_{0}^{},\\ &_{k+1}=2_{k}-_{k}_{k},\,\,\, {}_{k+1}^{ Newton}=_{k+1}^{}.\] (5)

This computes an approximation of the psuedo inverse using the moments of \(\). In contrast to GD, the Iterative Newton's method only requires \((()+(1/))\) steps to converge to an \(\) error (Soderstrom and Stewart, 1974; Pan and Schreiber, 1991). Note that this is exponentially faster than the convergence rate of GD. We discuss additional algorithms such as Conjugate Gradient, BFGS, and L-BFGS in the Appendix A.2.3.

### Solving Linear Regression with Transformers

We will use neural network models such as Transformers to solve this linear regression task. As shown in Figure 1, at time step \(t+1\), the model sees the first \(t\) in-context examples \(\{_{i},y_{i}\}_{i=1}^{t}\), and then makes predictions for \(_{t+1}\), whose label \(y_{t+1}\) is not observed by the Transformers model.

We randomly initialize our models and then train them on the linear regression task to make predictions for every number of in-context examples \(t\), where \(t[n]\). Training and test data are both drawn from \(P_{}\). To make the input prompts contain both \(_{i}\) and \(y_{i}\), we follow same the setup as Garg et al. (2022)'s to zero-pad \(y_{i}\)'s, and use the same GPT-2 model (Radford et al., 2019) with softmax activation and causal attention mask (discussed later in Definition 3.1).

We now present the key mathematical details for the Transformer architecture, and how they can be used for in-context learning. First, the causal attention mask enforces that attention heads can only attend to hidden states of previous time steps, and is defined as follows.

**Definition 3.1** (Causal Attention Layer).: A **causal** attention layer with \(M\) heads and activation function \(\) is denoted as \(\) on any input sequence \(=[_{1},,_{N}]^{D N}\), where \(D\) is the dimension of hidden states and \(N\) is the sequence length. In the vector form,

\[}_{t}=[()]_{t}=_{t}+_{m=1}^{M}_{ j=1}^{t}(_{m}_{t},_{m}_{j} )_{m}_{j}.\] (6)

Vaswani et al. (2017) originally proposed the Transformer architecture with the Softmax activation function for the attention layers. Later works have found that replacing \(()\) with \(()\) does not hurt model performance (Cai et al., 2022; Shen et al., 2023; Wortsman et al., 2023). The Transformers architecture is defined by putting together attention layers with feed forward layers:

**Definition 3.2** (Transformers).: An \(L\)-layer decoder-based transformer with Causal Attention Layers is denoted as \(_{}\) and is a composition of a MLP Layer (with a skip connection) and a Causal Attention Layers. For input sequence \(^{(0)}\), the transformers \(\)-th hidden layer is given by

\[^{}_{}(^{(0)}):=^{()}=_{ ^{()}_{}}(_{^{()}_ {}}(^{(-1)})).\]

where \(=\{^{()}_{},^{()}_{ }\}_{=1}^{L}\) and \(^{()}_{}=\{^{()}_{m},^{()}_{ m},^{()}_{m}\}_{m=1}^{M}\) has \(M\) heads at layer \(\).

In particular for the linear regression task, Transformers perform in-context learning as follows

**Definition 3.3** (Transformers for Linear Regression).: Given in-context examples \(\{_{1},y_{1},,_{t},y_{t}\}\), Transformers make predictions on a query example \(_{t+1}\) through a readout layer parameterized as \(_{}=\{,v\}\), and the prediction \(^{}_{t+1}\) is given by

\[^{}_{t+1}:=^{L }_{}(\{_{1},_{1},,_{t},_{t},_{t+ 1}\})}_{^{(L)}}=^{}^{(L)}_{:,2t+1}+v.\]

To compare the rate of convergence of iterative algorithms to that of Transformers, we treat the layer index \(\) of Transformers as analogous to the iterative step \(k\) of algorithms discussed in SS3.1. Note that for Transformers, we need to re-train the \(\) layer for every layer index \(\) so that they can improve progressively (see SS4.1 and for experimental details) for linear regression tasks.

### Measuring Algorithmic Similarity

We propose two metrics to measure the similarity between linear regression algorithms.

**Similarity of Errors.** This metric aims to measure similarity of algorithms through comparing prediction errors. For a linear regression algorithm \(\), let \((_{t+1}\{_{i},y_{i}\}_{i=1}^{t})\) denote its prediction on the \((t+1)\)-th in-context example \(_{t+1}\) after observing the first \(t\) examples (see Figure 1). We write \((_{t+1}):=(_{t+1}\{_{i},y_{i}\}_{ i=1}^{t})\) for brevity. Errors (i.e., residuals) on the sequence are:2

\[(\{_{i},y_{i}\}_{i=1}^{n+1})=(_{2})-y_{2},,(_{n+1})-y_{n+1}^{}.\]

The similarity of errors for two algorithms \(_{a}\) and \(_{b}\) is the expected cosine similarity of their errors on a randomly sampled data sequence:

\[(_{a},_{b})=}_{\{_{ i},y_{i}\}_{i=1}^{n+1} P_{}} (_{a}|\{_{i},y_{i}\}_{i=1}^{n+1}),( _{b}|\{_{i},y_{i}\}_{i=1}^{n+1}).\]

Figure 2: **Convergence of Algorithms. Similar to Iterative Newton and GD, Transformer’s performance improve over the layer index \(\). When \(n>d\), the Transformer model, from layers 3 to 8, demonstrates a superlinear convergence rate, similar to Iterative Newton, while GD, with fixed step size, is sublinear. Later layers of Transformers show a slower convergence rate, and we hypothesize they have little incentive to implement the algorithm precisely since the error is already very small. A 24-layer Transformer model exhibits the same superlinear convergence (Figure 25 in §A.4.2).**Here \((,)=,}{\|\|_{2}\|\|_{2}}\) is the cosine similarity, \(n\) is the total number of in-context examples, and \(P_{}\) is the data generation process discussed previously.

**Similarity of Induced Weights.** All standard algorithms for linear regression estimate a weight vector \(}\). While neural ICL models like Transformers do not explicitly learn such a weight vector, similar to Akyurek et al. (2022), we can _induce_ an implicit weight vector \(}\) learned by any algorithm \(\) by fitting a weight vector to its predictions. We can then measure similarity of algorithms by comparing the induced \(}\). To do this, for any fixed sequence of \(t\) in-context examples \(\{_{i},y_{i}\}_{i=1}^{t}\), we sample \(T d\) query examples \(}_{k}}}{{}}( ,)\), where \(k[T]\). For this fixed sequence of in-context examples \(\{_{i},y_{i}\}_{i=1}^{t}\), we create \(T\) in-context prediction tasks and use the algorithm \(\) to make predictions \((}_{k}\{_{i},y_{i}\}_{i=1}^{t})\). We define the induced data matrix and labels as

\[}=}_{1}^{}\\ \\ }_{T}^{}}= (}_{1}\{_{i},y_{i}\}_{i=1}^{t})\\ \\ (}_{T}\{_{i},y_{i}\}_{i=1}^{t}).\] (7)

The induced weight vector for \(\) and these \(t\) examples is:

\[}_{t}():=}_{t}( \{_{i},y_{i}\}_{i=1}^{t})=(}^{}})^{-1} }^{}}.\] (8)

The similarity of induced weights between two algorithms \(_{a}\) and \(_{b}\) is the expected average cosine similarity3 of induced weights \(}_{t}(_{a})\) and \(}_{t}(_{b})\) over all possible \(1 t n\), on a randomly sampled data sequence:

\[(_{a},_{b})=_{i},y_{i}\} _{i=1}^{n} P_{}}{}_{t=1}^{n} }_{t}(_{a}|\{_{i},y_{i}\}_{i= 1}^{t}),}_{t}(_{b}|\{_{i},y_{i}\}_{i=1}^{t})) .\]

**Matching steps between algorithms.** Each algorithm converges to its predictions after several **steps** -- for example the number of iterations for Iterative Newton and GD, and the number of layers for Transformers (see Section 4.1). When comparing two algorithms, given a choice of steps for the first algorithm, we match it with the steps for the second algorithm that maximize similarity.

**Definition 3.4** (Best-matching Steps).: Let \(\) be the metric for evaluating similarities between two algorithms \(_{a}\) and \(_{b}\), which have steps \(p_{a}[0,T_{a}]\) and \(p_{b}[0,T_{b}]\), respectively. For a given choice of \(p_{a}\), we define the best-matching number of steps of algorithm \(_{b}\) for \(_{a}\) as:

\[p_{b}^{}(p_{a}):=[0,T_{b}]}{} (_{a}( p_{a}),_{b}( p_{b})).\] (9)

In our experiments, we chose \(T_{a},T_{b}\) be large enough integers so the algorithms converge. The matching processes can be visualized as heatmaps as shown in Figure 3, where best-matching steps are highlighted. This enables us to compare the rate of convergence of algorithms. In particular, if two algorithms converge at the same rate, the best matching steps between the two algorithms should follow a linear trend. We will discuss these results in SS4. See Figure 26 on how best-matching steps help compare the convergence rates.

## 4 Experimental Evidence

We primarily study the Transformers-based GPT-2 model with 12 layers and 8 heads per layer. Alternative configurations with fewer heads per layer, or with more layers, also support our findings; we defer them to SSA.4.1 and SSA.4.2. We initially focus on isotropic cases where \(=\) and later consider ill-conditioned \(\) in SS4.3. Our training setup is exactly the same as Garg et al. (2022): models are trained with at most \(n=40\) in-context examples for \(d=20\) (with the same learning rate, batch size etc.).

We claim that Transformers learn high-order optimization methods in-context. We provide evidence that Transformers improve themselves with more layers in SS4.1; Transformers share the same rate of convergence as Iterative Newton, exponentially faster than that of GD, in SS4.2; and they also perform well on ill-conditioned problems in SS4.3. Finally, we contrast Transformers with LSTMs in SS4.5.

### Transformers improve progressively over layers

Many known algorithms for linear regression, including GD, OGD, and Iterative Newton, are _iterative_: their performance progressively improves as they perform more iterations, eventually converging to a final solution. How can a Transformer implement such an iterative algorithm? von Oswald et al. (2022) propose that deeper _layers_ of the Transformer may correspond to more iterations; in particular, they show that there exist Transformer parameters such that each attention layer performs one step of GD.

Following this intuition, we first investigate whether the predictions of a trained Transformer improve as the layer index \(\) increases. For each layer of hidden states \(^{()}\) (see Definition 3.2), we re-train the ReadOut to predict \(y_{t}\) for each \(t\); the new predictions are given by \(^{()}[^{()}]\). Thus for each input prompt, there are \(L\) Transformer predictions parameterized by layer index \(\). All parameters besides the ReadOut layer parameters are kept frozen.

As shown in Figure 2(a) (and Figure 7(a) in the Appendix), as we increase the layer index \(\), the prediction performance improves progressively. Hence, Transformers progressively improve their predictions over layers \(\), similar to how iterative algorithms improve over steps. Such observations are consistent with language tasks where Transformers-based language models also improve their predictions along with layer progressions Geva et al. (2022); Chuang et al. (2023).

### Transformers are more similar to second-order methods, such as Iterative Newton

We now test the more specific hypothesis that the iterative updates performed across Transformer layers are similar to the iterative updates for known iterative algorithms. First, Figure 2 shows that the middle layers of Transformers converge at a rate similar to Iterative Newton, and faster than GD. In particular, the Transformer and Iterative Newton both converge at a superlinear rate, while GD converges at a sublinear rate.

Next, we analyze whether each layer \(\) of the Transformer corresponds to performing \(k\) steps of some iterative algorithm, for some \(k\) depending on \(\). We focus here on GD and Iterative Newton's Method; we will discuss online algorithms in Section 4.5, and additional optimization methods in Appendix A.2.3. We will discuss results on noisy linear regression tasks in Appendix A.3.2.

For each layer \(\) of the Transformer, we measure the best-matching similarity (see Def. 3.4) with candidate iterative algorithms with the optimal choice of the number of steps \(k\). As shown in Figure 3,

Figure 3: **Heatmaps of Similarity.** The best matching steps are highlighted in yellow. Transformers shows show a linear trend with Iterative Newton steps but an exponential trend with GD. This suggests Transformers and Iterative Newton have the same convergence rate that is exponentially faster than GD. See Figure 10 for an additional heatmap where GD’s steps are shown in log scale: on that plot there is a linear correspondence between Transformers and GD’s steps. This further strengthens the claim that Transformers have an exponentially faster rate of convergence than GD.

the Transformer has very high error similarity with Iterative Newton's method at all layers. Moreover, we see a clear _linear_ trend between layer 3 and layer 9 of the Transformer, where each layer appears to compute roughly 3 additional iterations of Iterative Newton's method. This trend only stops at the last few layers because both algorithms converge to the OLS solution; Newton is known to converge to OLS (see SS3.1), and we verify in Appendix A.2 that the last few layers of the Transformer also basically compute OLS (see Figure 14 in the Appendix). We observe the same trends when using similarity of induced weights as our similarity metric (see Figure 9 in the Appendix). Figure 11 in the Appendix shows that there is a similar _linear_ trend between Transformer and BFGS, an alternative quasi-Newton method. This is perhaps not surprising, given that BFGS also gets a superlinear convergence rate for linear regression Nocedal and Wright (1999). Thus, we do not claim that Transformers specifically implement Iterative Newton, only that they (approximately) implement some second-order method.

In contrast, even though GD has a comparable similarity with the Transformers at later layers, their best matching follows an _exponential_ trend. As discussed in the Section 3.1, for well-conditioned problems where \( 1\), to achieve \(\) error, the rate of convergence of GD is \(((1/))\) while the rate of convergence of Iterative Newton is \(((1/))\). Therefore the rate of convergence of Iterative Newton is exponentially faster than GD. Transformer's _linear_ correspondence with Iterative Newton and its _exponential_ correspondence with GD provides strong evidence that the rate of convergence of Transformers is similar to Iterative Newton, i.e., \(((1/))\). We also note that it is not possible to significantly improve GD's convergence rate without using second-order methods: Nemirovski and Yudin (1983) showed a \((1/)\) lower bound on the convergence rate of gradient-based methods for smooth and strongly convex problems, and Arjevani et al. (2016) shows a similar lower bound specifically for quadratic problems. In the Appendix, we show that limited-memory BFGS Liu and Nocedal (1989) and conjugate gradient (see Figure 12), which do not use full-second order information, also converge slower than Transformers. This provides further evidence for the usage of second-order information by Transformers. We also show more evidence by investigating alternative function classes such as linear regression with noises in Appendix A.3.2 and 2-layer neural network with ReLU or Tanh activation function in Appendix A.3.3.

Overall, we conclude that a Transformer trained to perform in-context linear regression learns to implement an algorithm that is very similar to second-order methods, such as Iterative Newton's method, not GD. Starting at layer 3, subsequent layers of the Transformer compute more and more iterations of Iterative Newton's method. This algorithm successfully solves the linear regression problem, as it converges to the optimal OLS solution in the final layers.

### Transformers perform well on ill-conditioned data

We repeat the same experiments with data \(_{i}}}{{}}(, )\) sampled from an ill-condition covariance matrix \(\) with condition number \(()=100\), and eigenbasis chosen uniformly at random. The first

Figure 4: Transformers performance on ill-conditioned data. Given 40 in-context examples, Transformers and Iterative Newton converge similarly and they both can converge to the OLS solution quickly whereas GD suffers.

Figure 5: In the left figure, we measure model predictions with normalized MSE. Though LSTM is seemingly most similar to Newton’s Method with only 5 steps, neither algorithm converges yet. OGD also has a similar trend as LSTM. In the right figure, we measure the model’s error rate on example \(_{n-g}\) after seeing \(n\) examples, for different values of the time stamp gap \(g\) (see Appendix A.6), and find both Transformers and not-converged Newton have better memorization than LSTM and OGD.

\(d/2\) eigenvalues of \(\) are 100, and the last \(d/2\) are 1. Note that choosing the eigenbasis uniformly at random for _each_ sequence ensures that there is a different covariance matrix \(\) for each sequence of datapoints.

As shown in Figure 4, the Transformer model's performance still closely matches Iterative Newton's Method with 21 iterations, same as when \(=\) (see layer 10-12 in Figure 3). The convergence of second-order methods has a mild logarithmic dependence on the condition number since they correct for the curvature. On the other hand, GD's convergence is affected polynomially by conditioning. As \(()\) increase from 1 to 100, the number steps required for GD's convergence increases significantly (see Fig. 4 where GD requires 2,000 steps to converge), making it impossible for a 12-layer Transformers to implement these many gradient updates. We also note that preconditioning the data by \((^{})^{}\) can make the data well-conditioned, but since the eigenbasis is chosen uniformly at random, with high probability there is no sparse pre-conditioner or any fixed pre-conditioner which works across the data distribution. Computing \((^{})^{}\) appears to be as hard as computing the OLS solution (Eq. 1)--in fact Sharan et al. (2019) conjecture that first-order methods such as gradient descent and its variants cannot avoid polynomial dependencies in condition number in the ill-conditioned case.4 See Appendix A.3.1 for detailed experiments on ill-conditioned problems. These experiments further strengthen our thesis that Transformers learn to perform second-order optimization methods in-context, not first-order methods such as GD.

### Transformers Require \((d)\) Hidden Dimension

We ablate 12-layer 1-head Transformers with various hidden sizes on \(d=20\) problems. As shown in Figure 6, we observe that Transformers can mimic OLS solution when the hidden size is 32 or 64, but fail with smaller sizes. This resonates with our theoretical results on \((d)\) hidden dimension in Theorem 5.1, and in this case, the theorem ensures a construction of transformers to implement Iterative Newton's method.

### LSTM is more similar to OGD than Transformers

As discussed in SSA.1, LSTM is an alternative auto-regressive model widely used before the introduction of Transformers. Thus, a natural research question is: _If Transformers can learn in-context, can LSTMs do so as well? If so, do they learn the same algorithms?_ To answer this question, we train a LSTM model in an identical manner to the Transformers studied in the previous sections.

Figure 5 plots the error of Transformers, LSTMs, and other standard methods as a function of the number of in-context (i.e., training) examples provided. While LSTMs can also learn linear regression in-context, they have much higher mean-squared error than Transformers. Their error rate is similar to Iterative Newton's Method after only 5 iterations, a point where it is far from converging to the OLS solution. Finally, we show that LSTMs behave more like an online learning algorithm than Transformers. In particular, its predictions are biased towards getting more recent training examples correct, as opposed to earlier examples, as shown in Figure 5. This property makes LSTMs similar to

Figure 6: Ablation on Transformer’s Hidden Size. For linear regression problems with \(d=20\), Transformers need \((d)\) hidden dimension to mimic OLS solutions.

online GD. In contrast, five steps of Newton's method has the same error on average for recent and early examples, showing that the LSTM implements a very different algorithm from a few iterations of Newton. We hypothesize that since LSTMs have limited memory, they must learn in a roughly online fashion; in contrast, Transformer's attention heads can access the entire sequence of past examples, enabling it to learn more complex algorithms. See SSA.1 for more discussions.

## 5 Theoretical Justification

Our empirical evidence demonstrates that Transformers behave much more similarly to Iterative Newton's than to GD. Iterative Newton is a second-order optimization method, and is algorithmically more involved than GD. We begin by first examining this difference in complexity. As discussed in Section 3, the updates for Iterative Newton are of the form,

\[}_{k+1}^{}=_{k+1}^{} _{k+1}=2_{k}-_{k}_{k}\] (10)

and \(_{0}=\) for some \(>0\). We can express \(_{k}\) in terms of powers of \(\) by expanding iteratively, for example \(_{1}=2-4^{2}^{3},_{2}=4-12 ^{2}^{3}+16^{3}^{5}-16^{4}^{7}\), and in general \(_{k}=_{s=1}^{2^{k+1}-1}_{s}^{s}\) for some \(_{s}\) (see Appendix B.3 for detailed calculations). Note that \(k\) steps of Iterative Newton's requires computing \((2^{k})\) moments of \(\). Let us contrast this with GD. GD updates for linear regression take the form,

\[}_{k+1}^{}=}_{k}^{}-( }_{k}^{}-^{}).\] (11)

Like Iterative Newton, we can express \(}_{k}^{}\) in terms of powers of \(\) and \(^{}\). However, after \(k\) steps of GD, the highest power of \(\) is only \(O(k)\). This exponential separation is consistent with the exponential gap in terms of the parameter dependence in the convergence rate--\((()(1/))\) for GD vs. \((()+(1/))\) for Iterative Newton. Therefore, a natural question is whether Transformers can actually as complicated of a method such as Iterative Newton with only polynomially many layers? Theorem 5.1 shows that this is indeed possible.

**Theorem 5.1**.: _For any \(k\), there exist Transformer weights such that on any set of in-context examples \(\{_{i},y_{i}\}_{i=1}^{n}\) and test point \(_{}\), the Transformer predicts on \(_{}\) using \(_{}^{}}_{k}^{}\). Here \(}_{k}^{}\) are the Iterative Newton updates given by \(}_{k}^{}=_{k}^{}\) where \(_{j}\) is updated as_

\[_{j}=2_{j-1}-_{j-1}_{j-1},1 j k,_{0}=,\]

_for some \(>0\) and \(=^{}\). The dimensionality of the hidden layers is \((d)\), and the number of layers is \(k+8\). One transformer layer computes one Newton iteration. 3 initial transformer layers are needed for initializing \(_{0}\) and 5 layers at the end are needed to read out predictions from the computed pseudo-inverse \(_{k}\)._

We note that our proof uses full attention instead of causal attention and ReLU activations for the self-attention layers. The definitions of these and the full proof appear in Appendix B.

## 6 Conclusion and Discussion

In this work, we studied how Transformers perform in-context learning for linear regression. In contrast with the hypothesis that Transformers learn in-context by implementing gradient descent, our experimental results show that different Transformer layers match iterations of Iterative Newton _linearly_ and Gradient Descent _exponentially_. This suggests that Transformers share a similar rate of convergence to Iterative Newton but not to Gradient Descent. Moreover, Transformers can perform well empirically on ill-conditioned linear regression, whereas first-order methods such as Gradient Descent struggle. This empirical evidence -- when combined with existing lower bounds in optimization -- suggests that Transformers use second-order information for solving linear regression, and we also prove that Transformers can indeed represent second-order methods.

An interesting direction is to explore a wider range of second-order methods that Transformers can implement. It also seems promising to extend our analysis to classification problems, especially given recent work showing that Transformers resemble SVMs in classification tasks (Li et al., 2023; Tarzanagh et al., 2023). Finally, a natural question is to understand the differences in the model architecture that make Transformers better in-context learners than LSTMs. Based on our investigations with LSTMs, we hypothesize that Transformers can implement more powerful algorithms because of having access to a longer history of examples. Investigating the role of this additional memory in learning appears to be an intriguing direction.