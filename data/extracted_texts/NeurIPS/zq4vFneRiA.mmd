# The Crucial Role of Normalization in Sharpness-Aware Minimization

Yan Dai

IIIS, Tsinghua University

yan-dai20@mails.tsinghua.edu.cn

&Kwangjun Ahn

EECS, MIT

kjahn@mit.edu

&Suvrit Sra

TU Munich / MIT

suvrit@mit.edu

The first two authors contribute equally. Work done while Yan Dai was visiting MIT.

###### Abstract

Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding _the role played by normalization_, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima - a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.

## 1 Introduction

We study the recently proposed gradient-based optimization algorithm _Sharpness-Aware Minimization (SAM)_(Foret et al., 2021) that has shown impressive performance in training deep neural networks to generalize well (Foret et al., 2021; Bahri et al., 2022; Mi et al., 2022; Zhong et al., 2022). SAM updates involve an ostensibly small but key modification to Gradient Descent (GD). Specifically, for a loss function \(\) and each iteration \(t 0\), instead of updating the parameter \(w_{t}\) as \(w_{t+1}=w_{t}-(w_{t})\) (where \(\) is called the _learning rate_), SAM performs the following update:1

\[w_{t+1}=w_{t}-w_{t}+( w_{t})}{\|(w_{t})\|}\,,\] (1)

where \(\) is an additional hyper-parameter that we call the _perturbation radius_. Foret et al. (2021) motivate SAM as an algorithm minimizing the robust loss \(_{\|t\|}(w+)\), which is roughly the loss at \(w\) (i.e., \((w)\)) plus the "sharpness" of the loss landscape around \(w\), hence its name.

The empirical success of SAM has driven a recent surge of interest in characterizing its dynamics and theoretical properties (Bartlett et al., 2022; Wen et al., 2023; Ahn et al., 2023). However, a major component of SAM remains unexplained in prior work: the role and impact of the normalization factor \((w_{t})\|}\) used by SAM. In fact, quite a few recent works drop the normalization factor for simplicity when analyzing SAM (Andriushchenko and Flammarion, 2022; Behdin and Mazumder, 2023; Agarwala and Dauphin, 2023; Kim et al., 2023; Compagnoni et al., 2023). Instead of the SAM update (1), these works consider the following update that we call _Un-normalized SAM (USAAM)_:

\[w_{t+1}=w_{t}-(w_{t}+(w_{t}))\,.\] (2)Apart from experimental justifications in (Andriushchenko and Flammarion, 2022), the effect of this simplification has not yet been carefully investigated, although it is already widely adopted in the community. Thus, is it really the case that such normalization can be omitted "for simplification" when theoretically analyzing SAM? These observations raise our main question:

_What is the role of the normalization factor \((w_{t})\|}\) in the SAM update (1)?_

### Motivating Experiments and Our Contributions

We present our main findings through two motivating experiments. For the setting, we choose the well-known over-parameterized matrix sensing problem (Li et al., 2018); see Appendix A for details.

1. **Normalization helps with stability.** We first pick a learning rate \(\) that allows GD to converge, and we gradually increase \(\) from \(0.001\) to \(0.1\). Considering the early stage of training shown in Figure 1. One finds that _SAM has very similar behavior to GD_, whereas _USAM diverges even with a small \(\)_ - it seems that normalization helps stabilize the algorithm.
2. **Normalization permits moving along minima.** We reduce the step size by \(10\) times and consider their performance of reducing test losses in the long run. One may regard Figure 2 as the behavior of SAM, USAM, and GD when close to a "manifold" of minima (which exists since the problem is over-parametrized) as the training losses are close to zero. The first plot compares SAM and USAM with the same \(=0.1\) (the largest \(\) for which USAM doesn't diverge): notice that USAM and GD both converge to a minimum and do not move further; on the other hand, SAM keeps decreasing the test loss, showing its ability to drift along the manifold. We also vary \(\) and compare their behaviors (shown on the right): _the ability of SAM to travel along the manifold of minimizers seems to be robust_ to the size of \(\), while _USAM easily gets stuck at a minimum._

**Overview of our contributions.** In this work, as motivated by Figure 1 and Figure 2, we identify and theoretically explain the two roles of normalization in SAM. The paper is organized as follows.

1. In Section 2, we study the role of normalization in the algorithm's stability and show that normalization helps stabilize. In particular, we demonstrate that normalization ensures that GD's convergence implies SAM's non-divergence, whereas USAM can start diverging much earlier.
2. In Section 3, we study the role of normalization near a manifold of minimizers and show that the normalization factor allows iterates to keep drifting along this manifold - giving better performance in many cases. Without normalization, the algorithm easily gets stuck and no longer progresses.

Figure 1: Role of normalization for stabilizing algorithms (\(=0.05\)).

Figure 2: Role of normalization when close to a manifold of minimizers (\(=0.005\)).

3. In Section 4, to illustrate our main findings, we adopt the sparse coding example of Ahn et al. (2023). Their result implies a dilemma in hyper-parameter tuning for GD: a small \(\) gives worse performance, but a large \(\) results in divergence. We show that this dilemma extends to USAM - but not SAM. In other words, SAM easily solves the problem where GD and USAM often fail.

These findings also shed new light on why SAM is practical and successful, as we highlight below.

**Practical importance of our results.** The main findings in this work explain and underscore several practical aspects of SAM that are mainly due to the normalization step. One practical feature of SAM is the way the hyper-parameter \(\) is tuned: Foret et al. (2021) suggest that \(\) can be tuned independently after tuning the parameters of base optimizers (including learning rate \(\), momentum \(\), and so on). In particular, this feature makes SAM a perfect "add-on" to existing gradient-based optimizers. Our findings precisely support this practical aspect of SAM. Our results suggest that _the stability of SAM is less sensitive to the choice of \(\), thanks to the normalization factor._

The same principle applies to the behavior of the algorithm near the minima: Recent theoretical works (Bartlett et al., 2022; Wen et al., 2023; Ahn et al., 2023) have shown that the drift along the manifold of minimizers is a main feature that enables SAM to reduce the sharpness of the solution (which is believed to boost generalization ability in practice) - our results indicate that _the ability of SAM to keep drifting along the manifold is independent of the choice of \(\), again owing to normalization_. Hence, our work suggests that the normalization factor plays an important role towards SAM's empirical success.

### Related Work

**Sharpness-Aware Optimizers.** Inspired by the empirical and theoretical observation that the generalization effect of a deep neural network is correlated with the "sharpness" of the loss landscape (see (Keskar et al., 2017; Jastrzekbski et al., 2017; Jiang et al., 2020) for empirical observations and (Dziugaite and Roy, 2017; Neyshabur et al., 2017) for theoretical justifications), several recent papers (Foret et al., 2021; Zheng et al., 2021; Wu et al., 2020) propose optimizers that penalize the sharpness for the sake of better generalization. Subsequent efforts were made on making such optimizers scale-invariant (Kwon et al., 2021), more efficient (Liu et al., 2022; Du et al., 2022), and generalize better (Zhuang et al., 2022). This paper focuses on the vanilla version proposed by Foret et al. (2021).

**Theoretical Advances on SAM.** Despite the success of SAM in practice, theoretical understanding of SAM was absent until two recent works: Bartlett et al. (2022) analyze SAM on locally quadratic losses and identify a component reducing the sharpness \(_{}(^{2}(w_{t}))\), while Wen et al. (2023) characterize SAM near the manifold \(\) of minimizers and show that SAM follows a Riemannian gradient flow reducing \(_{}(^{2}(w))\) when i) initialized near \(\), and ii) \(\) is "small enough". Note that while the results of Wen et al. (2023) apply to more general loss functions, our result in Theorem 16 applies to i) any initialization far from the origin, and ii) any \(=o(1)\) and \(=O(1)\). A recent work by Ahn et al. (2023) formulates the notion of \(\)-approximate flat minima and analyzed the iteration complexity of practical algorithms like SAM to find such approximate flat minima. A concurrent work by Si and Yun (2023) also analyzes the original version of SAM with the normalization in (1), and makes a case that practical SAM does not converge all the way to optima.

**Unnormalized SAM (USAM).** USAM was first proposed by Andriushchenko and Flammario (2022) who observed a similar performance between USAM and SAM when training ResNet over CIFAR-10. This simplification is further accepted by Behdin and Mazumder (2023) who study the regularization effect of USAM over a linear regression model, by Agarwala and Dauphin (2023) who study the initial and final dynamics of USAM over a quadratic regression model, and by Kim et al. (2023) who study the convergence instability of USAM near saddle points. To our knowledge, (Compagnoni et al., 2023) is the only work comparing SAM and USAM dynamics. More preciously, they consider the continuous-time behavior of SGD, SAM, and USAM and find different behaviors of SAM and USAM: USAM attracts local minima while SAM aims at global ones. Still, we remark that as they are considering continuous-time variants of algorithms while we consider discrete (original) versions, our results directly apply to the SAM deployed in practice and the USAM studied in theory.

**Edge-of-Stability.** In the optimization theory literature, Gradient Descent (GD) was only shown to find minima if the learning rate \(\) is smaller than an "Edge-of-Stability" threshold, which is related to the sharpness of the nearest minimum. However, people recently observe that when training neural networks, GD with a \(\) much larger than that threshold often finds good minima as well (see (Cohenet al., 2021) and references therein). Aside from convergence, GD with large \(\) is also shown to find _flatter_ minima (Arora et al., 2022; Ahn et al., 2022; Wang et al., 2022; Damian et al., 2023).

## 2 Role of Normalization for Stability

In this section, we discuss the role of normalization in the stability of the algorithm. We begin by recalling a well-known fact about the stability of GD: for a convex quadratic cost with the largest eigenvalue of Hessian being \(\) (i.e., \(\)-smooth), GD converges to a minimum iff \(<}{{}}\). Given this folklore fact, we ask: how do the ascent steps in SAM (1) and USAM (2) affect their stability?

### Strongly Convex and Smooth Losses

Consider an \(\)-strongly-convex and \(\)-smooth loss function \(\) where GD is guaranteed to converge once \(<}{{}}\). We characterize the stability of SAM and USAM in the following result.

**Theorem 1** (Strongly Convex and Smooth Losses).: _For any \(\)-strongly-convex and \(\)-smooth loss function \(\), for any learning rate \(<}{{}}\) and perturbation radius \( 0\), the following holds:_

1. _SAM. The iterate_ \(w_{t}\) _converges to a local neighborhood around the minimizer_ \(w^{}\)_. Formally,_ \[(w_{t})-(w^{})1-(2- )^{t}((w_{0})-(w^{}))+ ^{2}}{2(2-)}, t.\] (3)
2. _USAM. In contrast, there exists some_ \(\)_-strongly-convex and_ \(\)_-smooth loss_ \(\) _such that the USAM with_ \(}{{(+^{2})}},}{{}}\) _diverges for all except measure zero initialization_ \(w_{0}\)_._

As we discussed, it is well-known that GD converges iff \(<}{{}}\), and Theorem 1 shows that SAM also does not diverge and stays within an \(()\)-neighborhood around the minimum as long as \(<}{{}}\). However, USAM diverges with an even lower learning rate: \(>}{{(+^{2})}}\) can already make USAM diverge. Intuitively, the larger the value of \(\), the easier it is for USAM to diverge.

One may notice that Equation 3, compared to the standard convergence rate of GD, exhibits an additive bias term of order \((^{2})\). This term arises from the unstable nature of SAM: the perturbation in (1) (which always has norm \(\)) prevents SAM from decreasing the loss monotonically. Thus, SAM can only approach a minimum up to a neighborhood. For this reason, in this paper whenever we say SAM "finds" a minimum, we mean its iterates approach and stay within a neighborhood of that minimum.

Due to space limitations, the full proof is postponed to Appendix C and we only outline it here.

Proof Sketch.: For SAM, we show an analog to the descent lemma of GD as follows (see Lemma 9):

\[(w_{t+1})(w_{t})-(2-)\| (w_{t})\|^{2}+^{3}^{2}}{2}\,.\] (4)

By invoking the strong convexity that gives \((w_{t})-(w^{})\| f(w_{t })\|^{2}\), we obtain

\[(w_{t+1})-(w^{})1-(2- )((w_{t})-(w^{}))+^{3} ^{2}}{2}\,.\]

Recursively applying this relation gives the first conclusion. For USAM, we consider the quadratic loss function same as (Bartlett et al., 2022). Formally, suppose that \((w)=w^{} w\) where \(=(_{1},_{2},,_{d})\) is a PSD matrix such that \(_{1}>_{2}_{d}>0\). Let the eigenvectors corresponding to \(_{1},_{2},,_{d}\) be \(e_{1},e_{2},,e_{d}\), respectively. Then we show the following in Theorem 10: for any \((_{1}+_{1}^{2})>2\) and \( w_{0},e_{1} 0\), USAM must diverge. As \((w)=w^{} w\) is \(_{1}\)-smooth and \(_{d}\)-strongly-convex, the second conclusion also follows. 

Intuitively, the difference in stability can be interpreted as follows: during the early stage of training, \(w_{t}\) and \((w_{t})\) often have large norms. The normalization in SAM then makes the ascent step \(w_{t}+(w_{t})}{\|(w_{t})\|}\) not too far away from \(w_{t}\). Hence, if GD does not diverge for this \(\), SAM does not either (unless the \(\)-perturbation is non-negligible, i.e., \(\|w_{t}\|\) no longer holds). This is not true for USAM: since the ascent step is un-normalized, it leads to a point far away from \(w_{t}\), making the size of USAM updates much larger. In other words, the removal of normalization leads to much more aggressive steps, resulting in a different behavior than GD and also an easier divergence.

### Generalizing to Non-Convex Cases: Scalar Factorization Problem

Now let us move on to non-convex losses. We consider a _scalar version_ of the matrix factorization problem \(_{U,V}\|UV^{T}-A\|_{2}^{2}\), whose loss function is defined as \((x,y)=(xy)^{2}\). Denote the initialization by \((x_{0},y_{0})\), then \((x,y)\) is \((x_{0}^{2}+y_{0}^{2})\)-smooth inside the region \(\{(x,y):x^{2}+y^{2}\}\). Hence, a learning rate \(<}{{}}\) again allows GD to converge due to the well-known descent lemma. The following result compares the behavior of SAM and USAM under this setup.

**Theorem 2** (Scalar Factorization Problem; Informal).: _For the loss function \((x,y)=(xy)^{2}\) restricted to a \(\)-smooth region, if we set \(=}{{}}<}{{}}\) (so GD finds a minimum), then_

1. _SAM. SAM never diverges and approaches a minimum within an_ \(O()\)_-neighborhood (in fact, SAM with distinct_ \(\)_'s always find the same minimum_ \((0,0)\)_)._
2. _USAM. On the other hand, USAM diverges once_ \( 15\) _- which could be much smaller than 1._

Thus, our observation in Theorem 1 is not limited to convex losses - for our non-convex scalar-factorization problem, the stability of SAM remains robust to the choice of \(\), while USAM is provably unstable. One may refer to Appendix D for the formal statement and proof of Theorem 2.

### Experiment: Early-Stage Behaviors when Training Neural Networks

As advertised, our result holds not only for convex or toy loss functions but also for practical neural networks. In Figure 3, we plot the early-stage behavior of GD, SAM, and USAM with different \(\) values (while fixing \(\)). We pick two neural networks: a convolutional neural network (CNN) with tanh activation and a fully-connected network (FCN) with ReLU activation. We train them over the CIFAR10 dataset and report the early-stage training losses. Similar to Figure 1, Theorem 1 and Theorem 2, _the stability of SAM is not sensitive to the choice of \(\), while USAM diverges easily_.

## 3 Role of Normalization for Drifting Near Minima

Now, we explain the second role of normalization: enabling the algorithm to drift near minima. To convince why this is beneficial, we adopt a loss function recently considered by Ahn et al. (2023) when understanding the behavior of GD with large learning rates. Their result suggests that GD needs a "large enough" \(\) for enhanced performance, but this threshold can never be known a-priori in practice. To verify our observations from Figure 2, we study the dynamics of SAM and USAM over the same loss function and find that: i) _no careful tuning is needed for SAM_; instead, SAM with any configuration finds the same minimum (which is the "best" one according to Ahn et al. (2023)); and ii) _such property is only enjoyed by SAM_ - for USAM, careful tuning remains essential.

### Toy Model: Single-Neuron Linear Network Model

To theoretically study the role of normalization near minima, we consider the simple two-dimensional non-convex loss \((x,y)\) defined over all \((x,y)^{2}\) as

\[(x,y)(x y)\,,.\] (5)

Figure 3: Behaviors of different algorithms when training neural networks (\(=0.025\)).

This \(\) was recently studied in (Ahn et al., 2023a) to understand the behavior of GD with large \(\)'s. By direct calculation, the gradient and Hessian of \(\) at a given \((x,y)\) can be written as:

\[(x,y)=^{}(xy)y\\ x,^{2}(x,y)=^{}(xy) y\\ x^{ 2}+^{}(xy)0&1\\ 1&0.\] (6)

Without loss of generality, one may assume \(\) is minimized at \(0\) (see Appendix E for more details regarding \(\)). Then, \(\) achieves minimum at the entire \(x\)- and \(y\)-axes, making it a good toy model for studying the behavior of algorithms near a continuum of minima. Finally, note that the parametrization \(x y\) can be interpreted as a single-neuron linear network model - hence its name.

Before moving on to SAM and USAM we first briefly introduce the behavior of GD on such loss functions characterized in (Ahn et al., 2023a). Since \(\) is even, without loss of generality, we always assume that the initialization \(w_{0}=(x_{0},y_{0})\) satisfies \(y_{0} x_{0}>0\).

**Theorem 3** (Theorems 5 and 6 of (Ahn et al., 2023a); Informal).: _For any \(=/(y_{0}^{2}-x_{0}^{2})\), the GD trajectory over the loss function \((x,y)=(xy)\) has two possible limiting points:_

1. _If_ \(<2\)_, then the iterates converge to_ \((0,y_{})\) _where_ \(y_{}^{2}[}{{}}-()- (}{{}}),}{{}}+( }{{}})]\)_._
2. _If_ \(>2\)_, then the iterates converge to_ \((0,y_{})\) _where_ \(y_{}^{2}[}{{}}-(),}{{ }}]\)_._

Intuitively, the limiting point of GD (denoted by \((0,y_{})\)) satisfies \(y_{}^{2}\{y_{0}^{2}-x_{0}^{2},}{{}}\}\). For simplicity, we denote \(_{}}{{y_{0}^{2}-x_{0}^{2}}}\) as the threshold of \(\) that distinguishes these two cases.

**Interpretation of Ahn et al. (2023a).** Fixing the initialization \((x_{0},y_{0})\), it turns out this model has a nice connection to the sparse coding problem, wherein it's desirable to get a smaller \(y_{}^{2}\) (which we will briefly discuss in Section 4). According to Theorem 3, to get a smaller \(y_{}^{2}\), one must increase the learning rate \(\) beyond \(_{}\). Hence we mainly focus on the case where \(>_{}\) - in which case we abbreviate \(y_{}^{2}}{{}}\) (see Table 1). However, GD diverges once \(\) is too large - in their language, \(\) cannot be much larger than \(2\). This dilemma of tuning \(\), as we shall illustrate in Section 4 in more detail, makes GD a brittle choice for obtaining a better \(y_{}^{2}\).

On the other hand, from the numerical illustrations in Figure 4, one can see that _SAM keeps moving along the manifold of minimizers_ (i.e., the \(y\)-axis) until the origin. This phenomenon is characterized in Theorem 4; in short, any moderate choice of \(\) and \(\) suffices to drive SAM toward the origin - no difficult tuning needed anymore!

In contrast, USAM does not keep moving along the axis. Instead, a lower bound on \(y_{}^{2}\) also presents - although smaller than the GD version. As we will justify in Theorem 5, _USAM does get trapped_ at some non-zero \(y_{}^{2}\) Thus, a dilemma similar to that of GD shows up: for enhanced performance, an aggressive \((,)\) is needed; however, as we saw from Section 2, this easily results in a divergence.

**Assumptions.** To directly compare with (Ahn et al., 2023a), we focus on the cases where \(y_{0}^{2}-x_{0}^{2}=/\) and \([,2]\) is a constant of moderate size; hence, \(\) is not too different from the \(_{}\) defined in Theorem 3. In contrast to most prior works which assume a tiny \(\) (e.g., (Wen et al., 2023)), we allow \(\) to be as large as a constant (i.e., we only require \(=(1)\) in Theorem 4 and Theorem 5).

#### 3.1.1 SAM Keeps Drifting Toward the Origin

We characterize the trajectory of SAM when applied to the loss defined in Equation 5 as follows:

**Theorem 4** (SAM over Single-Neuron Networks; Informal).: _For any \([_{},2_{}]\) and \(=(1)\), the SAM trajectory over the loss function \((x,y)=(xy)\) can be divided into three phases:_

1. _Initial Phase._ \(x_{t}\) _drops so rapidly that_ \(|x_{t}|=()\) _in_ \((}{{}})\) _steps. Meanwhile,_ \(y_{t}\) _remains large: specifically,_ \(y_{t}=(}{{}}})\)_. Thus, SAM approaches the_ \(y\)_-axis (the set of global minima)._

Figure 4: Trajectories of different algorithms for the \((xy)\) loss (\(=0.4\) and \(=0.1\); initialization \((x_{0},y_{0})=(2,)\) is marked by a black dot).

2. **Middle Phase.**\(x_{t}\) _oscillates closely to the axis such that_ \(|x_{t}|=()\) _always holds. Meanwhile,_ \(y_{t}\) _decreases fast until_ \(y_{t}|x_{t}|^{2}\) _- that is,_ \(|x_{t}|\) _remains small and SAM approaches the origin._
3. **Final Phase.**\(w_{t}=(x_{t},y_{t})\) _gets close to the origin such that_ \(|x_{t}|,|y_{t}|=(+)\)_. We then show that_ \(w_{t}\) _remains in this region for the subsequent iterates._

Informally, SAM first approaches the minimizers on \(y\)-axis (which form a manifold) and then keeps moving until a specific minimum. Moreover, SAM always approaches this minimum for almost all \((,)\)'s. This matches our motivating experiment in Figure 2: No matter what hyper-parameters are chosen, SAM _always_ drift along the set of minima, in contrast to the behavior of GD. This property allows SAM always to approach the origin \((0,0)\) and remains in its neighborhood, while GD converges to \((0,}{{}}})\) (see Table 1). The formal version of Theorem 4 is in Appendix F.

#### 3.1.2 USAAM Gets Trapped at Different Minima

We move on to characterize the dynamics of USAAM near the minima. Like GD or SAM, the first few iterations of USAAM drive iterates to the \(y\)-axis. However, unlike SAM, USAAM does not keep drifting along the \(y\)-axis and stops at some threshold - in the result below, we prove a lower bound on \(y_{t}^{2}\) that depends on both \(\) and \(\). In other words, the lack of normalization factor leads to diminishing drift.

**Theorem 5** (USAAM over Single-Neuron Networks; Informal).: _For any \([_{},2_{}]\) and \(=(1)\), the USAAM trajectory over the loss function \((x,y)=(xy)\) have the following properties:_

1. **Initial Phase.** _Similar to Initial Phase of Theorem_ 4_,_ \(|x_{t}|=()\) _and_ \(y_{t}=(}{{}}})\) _hold for the first_ \((}{{}})\) _steps. That is, USAAM also approaches_ \(y\)_-axis, the set of global minima._
2. **Final Phase.** _However, for USAAM, once the following condition holds for some round_ \(\)_:_3 
**Remark.** Note that USAAM becomes GD as we send \( 0+\), and our characterized threshold \(y_{}^{2}\) indeed recovers that of GD (i.e., \(}{{}}\) from Theorem 3) because \(_{ 0+}(}{{}}+1}-1)/2=}{{ }}\). Compared with SAM, the main difference occurs when close to minima, i.e., \(|x_{t}|=()\). Consistent with our motivating experiment (Figure 2), the removal of normalization leads to diminishing drift along the minima. Thus, USAAM is more like an improved version of GD rather than a simplification of SAM, and the comparison between Theorem 3 and Theorem 5 reveals that USAAM only improves over GD if \(\) is large enough - in which case USAAM is prone to diverges as we discussed in Section 2.

See Appendix G for a formal version of Theorem 5 together with its proof.

#### 3.1.3 Technical Distinctions Between GD, SAM, and USAAM

Before moving on, we present a more technical comparison between the results stated in Theorem 3 versus Theorem 4 and Theorem 5. We start with an intuitive explanation of why GD and USAAM get stuck near the manifold of minima but SAM does not: when the iterates approach the set of minima, both \(w_{t}\) and \((w_{t})\) become small. Hence the normalization plays an important role: as \((w_{t})\) are small, \(w_{t}\) and \(w_{t}+(w_{t})\) become nearly identical, which leads to a diminishing updates of GD and USAAM near the minima. On the other hand, having the normalization term, the SAM update doesn't diminish, which prevents SAM from converging to a minimum and keeps drifting along the manifold.

  Algorithm & GD & SAM & USAAM \\  Limiting Point \((0,y_{})\) & \(y_{}^{2}}{{}}\) & \(y_{}^{2} 0\) & \((1+ y_{}^{2})y_{}^{2}}{{}}\) \\  

Table 1: Limiting points of GD, SAM, and USAAM for the \((xy)\) loss (assuming \(>_{}\)).

This high-level idea is supported by the following calculation: recall Equation 6 that \((x_{t},y_{t})=^{}(x_{t}y_{t})[y_{t} x_{t}]^{}\). Hence, when \(|x_{t}| y_{t}\) in Final Phase, the "ascent gradient" direction \((x_{t},y_{t})\) (i.e., the ascent steps in Equation 1 and Equation 2) is almost perpendicular to the \(y\)-axis. We thus rewrite the update direction (i.e., the difference between \(w_{t+1}\) and \(w_{t}\)) for each algorithm as follows.

1. For SAM, after normalization, \((w_{t})}{\|(w_{t})\|}\) is roughly a unit vector along the \(x\)-axis. Hence, the update direction is the gradient at \(w_{t+}{{2}}}[ y_{t}]^{}\). Once \(y_{t}\) is large (making \(w_{t+}{{2}}}\) far from minima), \((w_{t+}{{2}}})\) thus have a large component along \(y_{t}\), which leads to drifting near minima.
2. For GD, by approximating \(^{}(u) u\), we derive \((x_{t},y_{t})[x_{t}y_{t}^{2} x_{t}^{2}y_{t}]^{}\). When \(}{{}}>y_{t}^{2}\), the magnitude of \(x_{t}\) is updated as \(|x_{t+1}||x_{t}- x_{t}y_{t}^{2}|=|(1- y_{t}^{2})x_{t}|\), which allows an exponential decay. Thus, GD converges to a minimum and stop moving soon after \(}{{}}>y_{t}^{2}\).
3. For USAAM, the descent gradient is taken at \(w_{t}+(w_{t})[(1+ y_{t}^{2})x_{t}(1+  x_{t}^{2})y_{t}]^{}\). Thus, \((w_{t}+(w_{t}))[(1+ y_{t}^{2 })(1+ x_{t}^{2})^{2}x_{t}y_{t}^{2}(1+ y_{t}^{2})^{2}(1+ x_{t }^{2})^{2}x_{t}y_{t}^{}]^{}\) by writing \(^{}(u) u\). This makes USAAM deviate away from SAM and behave like GD: by the similar argument as GD, USAAM stops at a minimum soon after \(}{{}}>(1+ y_{t}^{2})(1+ x_{t}^{2})^{2}y_{t}^{2} (1+ y_{t}^{2})y_{t}^{2}\)!

Hence, the normalization factor in the ascent gradient helps maintain a non-diminishing component along the minima, leading SAM to keep drifting. This distinguishes SAM from GD and USAAM.

### USAAM Gets Trapped Once Close to Minima

In this section, we extend our arguments to nonconvex costs satisfying Polyak-Lojasiewicz (PL) functions (see, e.g., (Karimi et al., 2016)). Recall that \(f\) satisfies the \(\)-PL condition if \(\|(w)\|^{2}((w)-_{w}(w))\) for all \(w\). Building upon the analysis of Andriushchenko and Flammarion (2022), we show the following result when applying USAAM to \(\)-smooth and \(\)-PL losses.

**Theorem 6** (USAAM over PL Losses; Informal).: _For \(\)-smooth and \(\)-PL loss \(\), for any \(<}{{}}\) and \(<}{{}}\), and for any initialization \(w_{0}\), \(\|w_{t}-w_{0}\|(,,,)(w_{0} )-_{w}(w)}\), \( t\)._

This theorem has the following consequence: Suppose that USAAM encounters a point \(w_{0}\) that is close to some minimum (i.e., \((w_{0})_{w}(w)\)) during training. Then Theorem 6 implies that _the total distance traveled by USAAM from \(w_{0}\) is bounded_ - in other words, the distance USAAM moves along the manifold of minimizers can only be of order \(((w_{0})-_{w}(w)})\).

As a remark, we compare Theorem 6 with the recent result by Wen et al. (2023): their result essentially implies that, for small enough \(\) and \(\), SAM iterates initialized close to a manifold of the minimizers approximately track some continuous dynamics (more precisely, a Riemannian gradient flow induced by a "sharpness" measure they find) and keep drifting along the manifold. This property is indeed in sharp contrast with USAAM whose total travel distance is bounded.

The formal statement and proof of Theorem 6 are contained in Appendix H.

### Experiments for Practical Neural Networking Training

We close this section by verifying our claims in practical neural network training. We train a ResNet18 on the CIFAR-10 dataset, initialized from a poor global minimum gener

Figure 5: Training ResNet18 on CIFAR-10 from a bad global minimum (\(=0.001\), batch size \(=128\)).

(we used the "adversarial checkpoint" released by Damian et al. (2021)). This initialization has \(100\%\) training accuracy but only \(48\%\) test accuracy - which lets us observe a more pronounced algorithmic behavior near the minima via tracking the test accuracy. From Figure 5, we observe:

1. GD gets stuck at this adversarial minimum, in the sense that the test accuracy stays at \(48\%\).
2. SAM keeps drifting while staying close to the manifold of minimizers (because the training accuracy remains \(100\%\)), which results in better solutions (i.e., the test accuracy keeps increasing).
3. USAAM with small \(\) gets stuck like GD, while USAAM with larger \(\)'s deviate from this manifold.

Hence, USAAM faces the dilemma that we describe in Subsection 3.1: a conservative hyper-parameter configuration does not lead to much drift along the minima, while a more aggressive choice easily leads to divergence. However, the stability of SAM is quite robust to the choice of hyper-parameter and they all seem to lead to consistent drift along the minima.

**Remark.** Apart from the "adversarial checkpoint" which is unrealistic but can help highlight different algorithms' behavior when they are close to a bad minimum, we also conduct the same experiments but instead initialized from a "full-batch checkpoint" (Damian et al., 2021), which is the 100% training accuracy point reached by running full-batch GD on the training loss function. The result is plotted as Figure 8 in Subsection B.1. One can observe that USAAM still gets stuck at the "full-batch checkpoint", while SAM keeps increasing its test accuracy via drifting along the minima manifold.

## 4 Case Study: Learning Threshold Neurons for Sparse Coding Problem

To incorporate our two findings into a single example, we consider training one-hidden-layer ReLU networks for the sparse coding problem, a setup considered in (Ahn et al., 2023a) to study the role of \(\) in GD. Without going into details, the crux of their experiment is to understand how GD with large \(\) finds desired structures of the network - in this specific case, the desired structure is the negative bias in ReLU unit (also widely known as "thresholding unit/neuron"). In this section, we evaluate SAM and USAAM under the same setup, illustrating the importance of normalization.

**Main observation of Ahn et al. (2023a).** Given this background, the main observation of Ahn et al. (2023a) is that i) when training the ReLU network with GD, different learning rates induce very different trajectories; and ii) the desired structure, namely a _negative bias in ReLU, only arises with large "unstable" learning rates_ for which GD exhibits unstable behaviors. We reproduce their results in Figure 6, plotting the test accuracy on the left and the bias of ReLU unit on the right. As they claimed, GD with larger \(\) learns more negative bias, which leads to better test accuracy.

Their inspiring observation is however a bit discouraging for practitioners. According to their theoretical results, such learning rates have to be quite large - large to the point where GD shows very unstable behavior (a la Edge-of-Stability (Cohen et al., 2021)). In practice, without knowledge of the problem, this requires a careful hyper-parameter search to figure out the correct learning rate. More importantly, such large and unstable learning rates may cause GD to diverge or lead to worse performance. More discussions can be found in the recent paper by Kaur et al. (2022).

In contrast, as we will justify shortly, _SAM does not suffer from such a "dilemma of tuning"_ - matching with our results in Theorem 4. Moreover, _the removal of normalization no longer attains such a property_, as we demonstrated in Theorem 5. In particular, for USAAM, one also needs to carefully tune \(\) and \(\) for better performance - as we inspired in Theorem 5 and Theorem 6, small \((,)\) makes the iterates get stuck early; on the other hand, as we presented in Section 2, an aggressive choice causes USAAM to diverge. The following experiments illustrate these claims in more detail.

In Figure 7, we plot the performance of SAM, USAAM, and GD with different \(\)'s (while fixing \(\)) - gray lines for GD, solid lines for SAM, and dashed lines for USAAM. From the plot, USAAM behaves more similarly to GD than SAM: the bias does not decrease sufficiently when the learning rate is not large enough, which consequently to leads poor test accuracy. On the other hand, no matter what

Figure 6: Behavior of GD for sparse coding problem.

\(\) is chosen for SAM, bias is negative enough and ensures better generalization. Hence, Figure 7 illustrates that compared to SAM, USAM is less robust to the tuning of \(\).

In Figure 9 (deferred to Subsection B.2), we also compare these three algorithms when varying \(\) and fixing \(\). In addition to what we observe in Figure 7, we show that normalization also helps stability - USAM quickly diverges as we increase \(\), while SAM remains robust to the choice of \(\). Thus, USAM is also less robust to the tuning of \(\). In other words, our observation in Figure 7 extends to \(\).

Hence, putting Figure 7 and Figure 9 together, we conclude that _SAM is robust to different configurations of \((,)\) while USAM is robust to neither of them_. Hence, the normalization of SAM eases hyper-parameter tuning, which is typically a tough problem for GD and many other algorithms - normalization boosts the success of SAM in practice.

## 5 Conclusion

In this paper, we investigate the role played by normalization in SAM. By theoretically characterizing the behavior of SAM and USAM on both convex and non-convex losses and empirically verifying our conclusions via real-world neural networks, we found that normalization i) helps stabilize the algorithm iterates, and ii) enables the algorithm to keep moving along the manifold of minimizers, leading to better performance in many cases. Moreover, as we demonstrate via various experiments, these two properties make SAM require less hyper-parameter tuning, supporting its practicality.

In this work, we follow a recent research paradigm of "physics-style" approaches to understanding deep neural networks based on simplified models (c.f. (Zhang et al., 2022; Garg et al., 2022; von Oswald et al., 2023; Abernethy et al., 2023; Allen-Zhu and Li, 2023; Liu et al., 2023; Li et al., 2023; Ahn et al., 2023, 2023, 2023)). We found such physics-style approaches quite helpful, especially for complex modern neural networks. We hope that our work builds stepping stones for future works on understanding working mechanisms of modern deep neural networks.