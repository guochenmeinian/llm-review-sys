# A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data

Andrej Tschalzev

University of Mannheim

&Sascha Marton

University of Mannheim

&Stefan Ludtke

University of Rostock

&Christian Bartelt

University of Mannheim

&Heiner Stuckenschmidt

University of Mannheim

Correspondence to: andrej.tschalzev@uni-mannheim.de

###### Abstract

Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing, which includes feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: **1.** After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. **2.** Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. **3.** While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics. Our framework is available under: https://github.com/atschalz/dc_tabeval.

## 1 Introduction

Since ancient times, tables have been used as a data structure, i.e., to record astronomical observations  or financial transactions . Many traditional machine learning (ML) methods, like logistic regression or the first artificial neural networks, were initially developed for tabular data . Even nowadays, in the age of AI, tabular data is the most prevalent modality in real-world applications, including medicine , finance , manufacturing , retail , and many others . Several novel deep learning architectures have been contributed in recent years to improve supervised machine learning for tabular data .

To evaluate existing approaches, various comparative studies were conducted in recent years . While motivated by different goals, they all have one in common: The focus is on evaluating models on tabular datasets using predefined cross-validation splits and one standardized preprocessing for all datasets. In this paper, we challenge such model-centric evaluation setups byhighlighting two major limitations (Section 2): 1) The evaluation setups are overly standardized and do not reflect the actual routine of practitioners, which typically includes dataset-specific feature engineering . 2) There is no external reference for the highest possible performance on a task beyond a study's own reporting, which limits its reliability.

To address these issues, we advocate for shifting the research perspective in the tabular data field from model-centric to data-centric. Therefore, our main contribution is an **evaluation framework that includes a collection of ten relevant real-world datasets, dataset-specific expert-level preprocessing pipelines, and an external measure of top performance for each dataset** (Section 3). The datasets were carefully selected by screening Kaggle competitions involving tabular data, and, to our knowledge, our contribution represents the largest existing collection of implemented expert-level solutions for tabular datasets. To assess the potential bias from the first limitation, we **investigate how the model comparison changes when considering dataset-specific preprocessing instead of standardized evaluation setups** (Subsection 4.1). To address the second limitation, we use the leaderboard from Kaggle competitions as an external performance reference and **reassess what is possible with modern methods that were not available when the Kaggle competitions took place** (Subsection 4.2). We find that when considering dataset-specific expert preprocessing, performance differences between the best models shrink, and the importance of selecting the 'right' model diminishes. In addition, we dissect expert solutions for tabular data competitions and **quantify the importance of different modeling components** (Subsection 4.3). We find that measurable progress has been made in automating human effort, but feature engineering is still the most important aspect of many tabular data problems. No model fully automates this aspect and comparisons that don't consider feature engineering merely scratch the surface of the potential performance achievable on many datasets. This paper focuses on independent and identically distributed (i.i.d.) tabular data in line with related work. However, our analysis of Kaggle competitions shows strong evidence that this focus in the research community might not align with practitioners' needs. In particular, we find that **many tabular data competitions on Kaggle have temporal characteristics** (i.e., timestamp features) and we identify test-time adaptation (TTA) as an overlooked but important part of some supposedly static competitions (Subsection 4.4).

Our findings indicate that current academic evaluation setups and benchmarks for tabular data are biased due to their overly model-centric focus. We conclude by **discussing possible directions to improve machine learning for tabular data from a data-centric perspective** (Section 5).

## 2 Related Work

Machine Learning for tabular data.Unlike domains like computer vision and natural language processing, an established state-of-the-art neural network architecture does not exist for tabular data [64; 5]. Therefore, recent research has primarily concentrated on developing general-purpose deep learning models often inspired by architectures from other domains [59; 33; 46; 37; 2; 42; 79; 66; 28; 9; 70; 67; 77; 30; 56; 48; 72; 10; 11; 53; 24]. Despite these efforts, Gradient Boosted Decision Trees (GBDTs) remain the state-of-the-art, outperforming even the novel neural models in many studies [5; 26; 55]. This paper aims to motivate more research inspired by tabular data-specific techniques like feature engineering instead of architectures established in other domains.

Limitations of current evaluation frameworks.Several benchmarks exist for evaluating tabular machine learning models, focusing on general model comparisons [4; 22; 21; 26; 55] and specific sub-problems [40; 20; 13; 63; 18]. However, these benchmarks do not provide preprocessing settings for the included datasets. Consequently, **most studies adopt a fixed, standardized preprocessing for all datasets** to concentrate on model comparisons [64; 25; 55; 26; 41]. While this model-centric approach is suitable for AutoML, it limits the real-world transferability of model comparisons, as models in practical applications typically follow dataset-specific preprocessing pipelines containing feature engineering techniques [68; 74; 31]. Our evaluation framework is the first to explicitly incorporate a more detailed distinction through diverse preprocessing pipelines. Furthermore, **existing benchmarks lack an external reference (i.e., a leaderboard) for the current best task performance**, hindering comparability across different studies. In contrast, we leverage datasets from ML competitions as an external benchmark for high performance on tasks. Many existing evaluation frameworks **prioritize usability at the expense of representativeness** by limiting sample sizes and removing high-cardinality categorical features, thus evaluating models on artificially constrained dataset versions [4; 26]. Our evaluation framework solely consists of tasks meaningful to the real world without imposing artificial restrictions on datasets. Finally, **most evaluation frameworks concentrate on tasks where samples are identically and independently distributed (i.i.d.)**. However, distribution shifts are prevalent in many machine learning applications [45; 71; 73; 51; 78; 20], and adapting to these shifts in tabular data has received limited attention [39; 20]. In this paper, we point out that excluding tabular data with temporal characteristics undermines the reliability of benchmarks, as many real-world applications using the benchmarked models include such data.

**Using Kaggle for model evaluation.** Kaggle is an online platform renowned for its machine learning competitions, hosted by companies and organizations to solve real-world problems in various domains. Some studies have retrospectively compared the performance of new approaches in Kaggle competitions [16; 61; 74]. However, most of these studies are limited to a few competitions or only compared against the leaderboard without investing the high effort of implementing expert solutions. In Subsection 3.1, we will explain that using Kaggle competitions to evaluate new approaches has several benefits. The evaluation framework most similar to ours is presented by Erickson et al. , where the proposed AutoML framework was compared to the leaderboard in Kaggle competitions. However, the methods leading to high performance on the leaderboard remain a black box. As we will show, some methods (i.e., test-time adaptation) prevent a fair comparison, and simply evaluating against the leaderboard is not helpful for gaining deeper insights. In contrast, we implement high-performing expert-level solutions, allowing us to dissect the components of interest and truly understand what drives high performance on specific tasks.

## 3 A Data-Centric Evaluation Framework for Tabular Machine Learning

We propose an evaluation framework built upon three crucial aspects that are often overlooked in tabular data research: 1) Evaluation on realistic datasets without removing frequently occurring challenging aspects like high cardinality categorical features, 2) Dataset-specific expert preprocessing pipelines containing feature engineering techniques, and 3) Evaluation against human expert performance on hidden test sets. Figure 1 depicts an overview of our framework. Our design choices are additionally justified by the fact that for each dataset, at least one model in our evaluation ranks among the top 1% of all competition participants (note that not all participants are experts, and leaderboard distributions can vary across datasets).

### Collection of Relevant and Challenging Datasets

We rely on the Kaggle community and competitions hosted by companies and institutions to select datasets with expert solutions. Figure 2 illustrates our dataset selection process, and Table 1 summarizes the main properties of the included datasets. Using data from Kaggle competitions has various benefits: 1) The selected tasks are challenging and meaningful to the real world, as companies and institutions only spend money on hosting competitions from which they benefit. 2) Each competition has a clear evaluation setup, including metrics selected to reflect the practitioners' needs. 3) Each competition has a large hidden test set, which has been shown to reduce the risk of adaptive overfitting . 4) The competition leaderboard serves as an external reference for truly high performance,

Figure 1: Illustration of the components of our evaluation framework.

as many expert teams participated in the competitions. Furthermore, our framework ensures a fair comparison by including a data loading function for each dataset that removes potential side issues, like data leakage or faulty data. This distinguishes our framework from related work that compares approaches to Kaggle solutions [16; 74]. An important insight from screening the competitions is that most tabular datasets had temporal characteristics - i.e., datasets with weak temporal correlations that benefit from time-sensitive feature engineering but not from models with temporal inductive biases (i.e., ). This finding will be further discussed in Subsection 4.4.

### Expert Solutions and Preprocessing Pipelines

In the context of our paper, _preprocessing_ refers to a pipeline that combines a "set of techniques used prior to the application of a [model]" . Feature engineering (FE) refers to techniques that "construct novel features from given data with the goal of improving predictive learning performance" . Consequently, feature engineering is a subset of preprocessing. Our proposed evaluation framework includes three preprocessing pipelines. One is dataset-agnostic and closely resembles the pipelines researchers currently use for model evaluation. The other two are dataset-specific and directly derived from expert solutions. All preprocessing pipelines are model-agnostic, and model-specific preprocessing steps are considered part of the model in our framework.

Standardized PreprocessingThe main purpose of this pipeline in our framework is to evaluate single models in a scenario with minimal dataset-specific human effort invested. Continuous missing values are replaced with the mean, and missing categorical feature values are treated as a new category. Furthermore, constant columns are removed, and heavy-tailed targets are log-transformed for regression tasks. As these preprocessing steps are almost universally applied across related work [25; 26; 55], this pipeline represents current evaluation setups in academia well.

   Name & Year & N (train/test) & D (raw/fe) & Categorical & Task & Metric & Model & TTA \\  MBGM & 2017 & 4209 / 4209 & 377 / 59 & 8 / 47 & Reg & r2 & XGBoost & No \\ SVPC & 2018 & 12296 / 49342 & 4992 / 1420 & 0 / 0 & Reg & rmsle & LGBM & No \\ AEAC & 2013 & 32769 / 58921 & 9 / 315 & 9 / 7,518 & Bin & auc & Ensemble & Yes \\ OGPCC & 2015 & 61878 / 144368 & 93 / 104 & 0 / 0 & Multi & logloss & Ensemble & Yes \\ SCS & 2016 & 76020 / 75818 & 370 / 224 & 0 / 0 & Bin & auc & Ensemble & Yes \\ BPCCM & 2016 & 114321 / 114393 & 132 / 313 & 19 / 18,210 & Bin & logloss & XGBoost & No \\ SCTP & 2019 & 200000 / 200000 & 200 / 600 & 0 / 0 & Reg & auc & NN & Yes \\ HQC & 2015 & 260753 / 173836 & 299 / 300 & 29 / 868 & Bin & auc & XGBoost & No \\ IFD & 2019 & 590540 / 506691 & 432 / 263 & 49 / 13,553 & Bin & auc & CatBoost & Yes \\ PSSDP & 2017 & 595212 / 892816 & 57 / 53 & 8 / 104 & Bin & gini & NN & Yes \\   

Table 1: Datasets included in our framework. \(N\) denotes the sample size in thousands, and \(D\) dimensionality of the raw data and after expert feature engineering. _Categorical_ lists the no. of categorical features and the no. of clusters of the highest-cardinality categorical feature. _Metric_ corresponds to the competition metric. MBGM and SVPC are regression tasks, OGPCC is a multi-class classification task, and the remaining are binary classification tasks. _Model_ corresponds to the best single model used in the original expert solution. For some datasets, no best single model could be distinguished due to the heavy ensembling used. _TTA_ denotes if test-time feature engineering has been used in the implemented expert solution.

Figure 2: Illustration of the dataset selection process. Details on the criteria and all screened datasets can be found in the Appendix. The Figure only lists the competitions as temporal, which were not already excluded for other reasons. In total, we identified 46 competition datasets with temporal characteristics (i.e., timestamps as a feature). Consistent with related work, we include competitions that have timestamps but can be approached without time-sensitive feature engineering.

Expert Feature EngineeringWe select one high-performance expert solution from Kaggle for each dataset. The solution was chosen based on the private leaderboard rank and the descriptions' quality and sufficiency. For each solution, we separate the data preparation from the remaining parts of the solution. For most datasets, this pipeline solely consists of feature engineering techniques. Besides a few distinctions between tree-based and deep learning models, the pipelines are model-agnostic. Model-specific preprocessing steps (i.e., feature normalization for neural networks) are considered part of the model in our framework and are explained in the Appendix. This paper focuses on a pipeline perspective and does not discuss single feature engineering steps further. Implementation details and feature engineering techniques used for specific datasets are provided in the Appendix and in our publicly available code. For this pipeline, we ensured that all feature engineering operations included were on the training data and that a model could have learned the same patterns without external information. Some of the feature engineering techniques used by the experts occur across multiple datasets: groupby interactions of categorical and numeric features (4), 2-order categorical interactions (3), feature selection (3), categorical frequency encoding (3), dimensionality reduction (2), 3-order categorical interaction (2), 2-order arithmetic interactions (2), sum of missing values in a row (2), and sum of zeros in a row (2). A common pattern is that the most frequently applied feature engineering steps include categorical features and that feature interactions are frequently manually engineered while transformations of single features are rare.

Test-Time AdaptationThis pipeline is exactly the same as the expert feature engineering pipeline, with the key difference that the test data is used for feature engineering where applicable. Most ML competitions are organized so that the test features (but not the targets) are given. We found that the top solutions used the test data in their data preparation for six of the datasets in our framework. Hence, this pipeline represents the actual preprocessing used by the experts. While this might be considered an unfair and unrealistic setup, there are applications where using unlabeled test data for unsupervised learning is applicable (see Appendix A.3). We argue that this conceptualization makes many tabular ML competitions a test-time adaptation (TTA) task. TTA is a type of domain adaptation where test samples are used at test time in an unsupervised or self-supervised way to update or retrain a model [76; 44; 43; 65; 57]. We term the common Kaggle practice of engineering domain-invariant features at test time as **test-time feature engineering**. The feature engineering techniques most frequently used to this end are groupby interactions, frequency encodings, and learning joint low-dimensional representations. With this preprocessing pipeline, we are the first to closer examine test-time feature engineering in Kaggle competitions.

### Modeling and Evaluation Framework

Modeling Pipeline and ModelsWe implement a unified modeling pipeline for all datasets with a dataset-specific cross-validation (CV) ensembling procedure. The validation sets are used for early stopping and determining the best hyperparameters. The final test data predictions are an ensemble of averaging the test predictions of each fold. We use three gradient-boosted tree libraries (_XGBoost_, _LightGBM_, and _CatBoost_) because each was used in at least one of the expert solutions. Each expert solution that used neural networks developed a highly customized network for the particular competition. We want to assess whether recently developed general-purpose architectures can replace the high effort of building custom networks. Hence, we chose _ResNet_ and _FTTransformer_ because they have been frequently used in recent benchmark comparisons and have shown strong performance [26; 55]. Because the Resnet essentially is an MLP with skip connections, it serves as a baseline representing what was already possible before the recent developments in DL for tabular data. In addition, we use two more recent approaches: _MLP-PLR_, which can help learn high-frequency functions, mitigating a major weakness of deep learning for tabular data ; and _GRANDE_, a recent representative of hybrid neural-tree models. Although other recent architectures exist, we don't include more, as our focus is not on benchmarking particular models but rather on demonstrating the importance of data-centric evaluation. To assess how well fully automated solutions perform without any preprocessing, we additionally evaluate _AutoGluon_, which has been shown to be the current best AutoML solution .

Hyperparameter OptimizationHyperparameter optimization is done per fold to obtain a diverse CV ensemble. Each model is evaluated in three HPO regimes: 1) _Default_: Either library default or hyperparameters suggested in related work, 2) _Light HPO_: 20 random search iterations. 3) _Extensive HPO_: 20 random search warmup iterations + 80 iterations of the tree-structured Parzen estimator algorithm . More details on the hyperparameter optimization can be seen in the Appendix.

EvaluationWe use the Kaggle API to automatically submit predictions and retrieve performance results after evaluating against the hidden targets. Each dataset is evaluated on the metric specified by the competition host. Instead of reporting this metric directly, we report the solution's private leaderboard position as the percentile. This has the benefit that although different metrics are used to evaluate the model, comparisons across datasets are possible. Note that the leaderboard position is always a snapshot of the end of each competition. In the Appendix, we additionally report performances on the actual metrics for each dataset. Throughout the paper, higher values represent a better performance (leaderboard position). As we use only one test set per dataset and less datasets compared to academic benchmarks, concerns about overfitting might be raised. However, ten datasets with one test set are less of an issue in our framework than it would be in conventional benchmarks, because: 1) The datasets in our framework, especially the test data, are comparably large and overfitting them is harder. Roelofs et al.  found that at least 10,000 test examples is a reasonable minimum test set size to protect against adaptive overfitting in Kaggle challenges. All test sizes in our framework, except for the MBGM dataset, are at least of size 50,000. 2) Test labels are unknown making it hard to purposefully overfit on particular samples. 3) The need of submitting to Kaggle, although automated, is an additional overfitting barrier.

## 4 Experimental Evaluation

Our framework allows us to assess the dataset-specific individual performance impact of model selection, hyperparameter optimization, feature engineering, and test-time adaptation. Whenever not stated otherwise, we report the results for the extensively tuned hyperparameter setup. As a general overview, Figure 3 shows how each of the analyzed modeling components improves over the default baseline for each model and dataset. The length of each segment indicates the performance gain relative to the previous configuration. The results demonstrate the importance of an external performance reference: If we only considered the standardized evaluation setup (blue/orange/green segments), we would only be scratching the surface of achievable task performance for many data sets.

Figure 3: Performance gains from different modeling components on the private Kaggle leaderboard by dataset and model. Higher values correspond to a better position. Each segment represents the performance gain of adding the modeling component to the previous configuration. ’Default’ corresponds to the model performance with default hyperparameters in a standardized preprocessing pipeline. Light and extensive HPO correspond to tuning hyperparameters in the same preprocessing pipeline. Expert FE and FE-TTA correspond to the model performance with extensively tuned hyperparameters in the feature engineering and the test-time adaptation pipeline respectively.

### How Model Comparisons Change When Considering Dataset-specific Preprocessing

Three observations stand out when evaluating models in different preprocessing pipelines (Figure 4). **1) The model rankings change considerably**, as indicated by the relatively low Spearman coefficients between the standardized preprocessing pipeline and the other pipelines. **2) The performance gap between all models diminishes when considering expert preprocessing.** On average, all models benefit from feature engineering, and multiple models can reach top performance. While all models benefit from TTA, the performance increase varies. **3) The superiority of CatBoost vanishes when considering dataset-specific preprocessing.** The reason is that CatBoost already incorporates specific feature engineering steps in its algorithm for which other models need manual engineering, as we will further elaborate in Subsection 4.3.

### Measurable Progress Through Recent Efforts

Figure 5 shows the model ranking on the private Kaggle leaderboard when trained after standardized preprocessing. CatBoost achieves top ranks in three competitions (MBGM, BPCCM, HQC) where a high manual effort in feature engineering was previously necessary. Similar to Erickson et al. , AutoGluon achieves top ranks in two of these (BPCCM, HQC) and one additional competition (OGPC). Regarding neural networks, novel architectures rank higher than the ResNet baseline on nine datasets. In two competitions, neural networks were originally the single-best models (SCTP, PSSDP - see Table 1). In our analysis, MLP-PLR and FTTransformer are able to reach top ranks on these datasets after feature engineering and test-time adaptation, while ResNet performs worse (see Figure 3). All neural networks originally used in the competitions were custom-designed for the particular competition. Hence, our analysis confirms that meaningful progress has been made in developing general-purpose architectures for tabular data as they reduce the necessity of custom-designed networks. Although the progress in the tabular data field is clearly visible, top performance cannot be reached without human effort for six datasets.

### Feature Engineering is Still the Most Important Factor for Top Performance

**The most remarkable performance gains are achieved through feature engineering.** Figure 6 shows that expert feature engineering is the most important modeling component on average. This holds true for all models, indicating that unlike for modalities like imaging, neural networks do not automate feature engineering for tabular data. When comparing the performance of different models in the standardized preprocessing pipeline (blue/orange/green bars in Figure 3), we can observe that using any other model than CatBoost rarely brings large gains. Only for the SCS dataset does FTTransformer clearly outperform all other models. For all other datasets, the average performance gains achievable solely with model selection are small.

Figure 4: Average leaderboard position of models with different preprocessing. Black horizontal lines denote the Spearman correlation between all results with the respective preprocessing.

Figure 5: Progress made through recent models trained in the standardized preprocessing pipeline, illustrated by retrospective comparison to the Kaggle leaderboard. Best NN denotes the best model of FTTransformer, MLP-PLR, and GRANDE.

Hence, our results confirm the findings of McElfresh et al.  that model selection is less important than HPO on a strong tree-based baseline for most datasets. Furthermore, we extend this finding by quantifying the even more important aspect of dataset-specific feature engineering.

Feature engineering is responsible for the high performance of CatBoost.Our analysis of different preprocessing pipelines reveals that CatBoost benefits much less from feature engineering than other models. The reason is that CatBoost incorporates explicit feature engineering techniques in its learning procedure. In particular, counts and target-based statistics are used to generate encodings for categorical features, and combinatorial encoding methods capture categorical feature interactions . When considering the same feature engineering techniques for the other models, the gap to CatBoost drastically shrinks for most models, and XGBoost  success in recent benchmarking studies [55; 20] can, at least to some extent, be attributed to feature engineering.

The optimal treatment of categorical features can be dataset-specific.Table 2 shows that a different treatment of categorical features than the model-inherent treatment was necessary for two datasets to achieve top performance. Furthermore, each of the two datasets required a different encoding method. This shows that standardized preprocessing can be biased for categorical features. Furthermore, the performance of deep learning models on categorical data can be greatly improved with feature engineering techniques on categorical data. I.e., for the AEAC dataset, which consists entirely of categorical features, all neural networks gain from feature engineering techniques like categorical feature interactions, as can be seen in Figure 3. This suggests that current architectures do not adequately capture the complex patterns within categorical data. Hence, whenever the goal is not to evaluate models as AutoML solutions, categorical data treatment methods in comparative studies should not only be model-specific, but also dataset-specific.

### The Importance of Test-Time Adaptation and Temporal Characteristics

Test-time feature engineering consistently improves the performance of single models.Table 3 shows that test-time feature engineering leads to performance gains over solely using the train data for feature engineering for all datasets. From the task perspective, the feature engineering used for AEAC and OGPCC only leads to performance gains when used as a test-time adaptation method. This shows that some of the feature engineering techniques used in Kaggle competitions actually serve the purpose of test-time adaptation. For three datasets, ranking among the top 1% on the leaderboard was not achieved without test-time adaptation. Our results indicate that simply comparing approaches to the Kaggle

    &  &  \\  & Default & OHE & Default & Target \\  XGBoost & 0.69 & **0.99** & 0.4 & **0.99** \\ LightGBM & 0.54 & **0.94** & 0.35 & **0.99** \\ CatBoost & 0.71 & **0.97** & **1.0** & **1.0** \\   

Table 2: Performance of tree-based models with different categorical data treatment methods. ‘Default’ corresponds to the model-inherent method.

Figure 6: Leaderboard performance gains from different modeling components per model. ‘Default’ corresponds to the model with default hyperparameters. The results for Expert FE and FE-TTA are reported after extensively tuning hyperparameters.

    &  &  \\  & Stand. & FE & TTA & Stand. & FE & TTA \\  AEAC & 0.953 & 0.937 & 0.991 & 0.618 & 0.953 & **0.993** \\ OGPCC & 0.896 & 0.871 & 0.923 & **0.996** & 0.983 & 0.995 \\ SCS & 0.945 & 0.953 & 0.975 & 0.92 & 0.999 & **1.0** \\ SCTP & 0.518 & 0.962 & **0.992** & 0.498 & 0.531 & 0.991 \\ IFD & 0.662 & 0.988 & **0.992** & 0.205 & 0.351 & 0.432 \\ PSSDP & 0.656 & 0.994 & **0.995** & 0.562 & 0.707 & 0.742 \\   

Table 3: Performance comparison in different preprocessing pipelines with a focus on top performance. AutoGluon is displayed separately to prevent bias in the single-model comparison.

leaderboard, as done in previous studies [16; 74], is insufficient. Techniques like test-time adaptation are frequently used in Kaggle competitions and limit comparability to approaches that don't use the test data. Hence, a fair model comparison to expert solutions using the Kaggle leaderboard can only be ensured under controlled conditions through implemented expert solutions such as our pipelines.

Models in real-world applications are often applied to non-i.i.d. tabular data.By definition, TTA should only be effective if the data violates the i.i.d. assumption and contains distribution shifts to adapt to. Indeed, the data collection process likely happened over time for most of the datasets used in our framework. However, timestamps were not always provided as the competitions were conceptualized as static tabular data tasks. Therefore, most of the datasets were also used in at least one comparative study for tabular data, although non-i.i.d. was a criterion for exclusion (e.g., SVPC, AEAC, and PSSDP in , SCTP and OGPCC in , or MBGM in ). Our results show, that despite treating datasets as static, the samples remain non-i.i.d. and approaches like test-time adaptation can improve performance. Furthermore, there is evidence that other datasets treated as i.i.d. in related work actually have a temporal nature. I.e., the electricity dataset  is frequently used in academic benchmarks [26; 55]. At the same time, this dataset would actually require a time-based data split and is used as a benchmark in online learning to measure the ability of models to adapt to concept drifts . As models for tabular data assume the data to be i.i.d., most benchmarks for evaluating tabular general-purpose models either directly name the data being non-i.i.d. as an exclusion criterion [26; 21] or exclude data that requires special CV procedures , which leads to the same results. In contrast, our analysis of Kaggle competitions revealed that most tabular data competitions have temporal characteristics and that the best solutions for such datasets typically engineer time-invariant features and utilize tabular data models assuming the data to be i.i.d (i.e. ). We conclude that there might be a mismatch between current evaluation frameworks for tabular data in academia and the tabular data tasks practitioners were interested in getting solved through ML competitions on Kaggle.

### Limitations

Despite the outlined advantages our evaluation has some limitations compared to evaluation designs in academic benchmarks:

1. The distribution of leaderboard rankings varies across competitions, and advancements in one competition's leaderboard do not necessarily translate equivalently in another. Moreover, large gains on a leaderboard may correspond to only minor increases on the actual task metric, particularly in competitions where solutions are widely shared and numerous similar entries are submitted. As an alternative, we repeat our evaluation based on the original task metric in Appendix F.
2. Not all leaderboard submissions are made by experts, resulting in a tail of lower-quality entries in each competition. Users of our framework should exercise caution when interpreting the leaderboard as a performance metric, ensuring that only factually accurate statements about its implications are used.
3. Due to the extent of our experiments, it was infeasible to repeat them to obtain error bars. This limitation is important, as it leaves the randomness introduced by data splits, model-specific characteristics, and hyperparameter optimization unquantified. Hence, small differences between models on single datasets need to be interpreted with caution. Nevertheless, our primary focus was to assess the effects of various modeling components. In this regard, the extensive nature of our experiments and the clear distinctions observed between preprocessing pipelines across multiple models and datasets suggest a minimal impact of randomness on our main findings. For users applying our framework to systematically compare individual models, alternative configurations with repeated evaluations would be necessary to increase reliability.
4. Due to the focus on incentivized competitions, most datasets are from the finance domain and primarily represent North America and Europe, leading to an underrepresentation of other domains and regions. To address this limitation, future analyses could incorporate competitions from additional platforms. However, as our framework was designed to be user-friendly, we focused on Kaggle, which contains an API for effortlessly downloading datasets and submitting predictions.

A more extensive discussion of limitations can be found in Appendix E.

Implications for Future Work

We challenged the prevalent model-centric evaluation setups in tabular data research by comparing evaluations with standardized preprocessing pipelines to evaluations with expert preprocessing pipelines. We have shown that current research is overly model-centric, while tabular datasets often require dataset-specific feature engineering or violate the i.i.d. assumption the models are based on. This reveals important insights and directions for future work in Machine Learning for tabular data.

More careful choice of preprocessing for model evaluation.Our findings highlight that standardized evaluation setups do not necessarily ensure fair model comparisons. In standardized preprocessing setups, models are evaluated as if they were AutoML solutions, whereas in real-world applications, they are components of highly dataset-specific pipelines. Researchers should be aware a) whether their datasets are amenable to feature engineering, b) that standardized preprocessing setups treat models as AutoML systems, and, c) that true ceteris paribus (c.p.) comparisons are hard if some models (i.e., CatBoost) apply feature engineering internally and others don't. Feature-engineered evaluation setups can be more suitable if a study aims for truly c.p. conditions or for evaluating models in realistic scenarios. Standardized evaluation setups are more suitable for benchmarking AutoML solutions and can also be suitable if models are expected to learn features without human effort. Future research could emphasize incorporating dataset-specific (expert) preprocessing pipelines into benchmarks or separate raw data benchmarks from fully preprocessed benchmarks, as done in our study. However, gathering high-quality expert solutions at a large scale is tedious and may require a community effort.

Need for external performance references.Our analysis shows that evaluations without considering the highest achievable performance on a task don't actually measure the state-of-the-art. Despite numerous benchmarks, there is no established standard to measure progress. A benchmark with a public leaderboard and a dynamic collection of meaningful and unsolved real-world datasets could facilitate progress.

Investigate why some feature engineering operations are not inherently learned by models.Researchers developing general-purpose models should recognize the impact of feature engineering on model performance. CatBoost has advanced the field by automating feature engineering on categorical data. However, significant feature engineering effort is still necessary for datasets where this is not the only challenge. Our study made it evident that there are transformations of the feature space which are not learned by models without manual feature engineering. While we focused on a pipeline perspective, future work could look at particular feature engineering techniques to uncover modes of failure for current models and develop novel architecture components. I.e., our experiments show that deep learning models benefit from feature engineering on categorical features. Hence, unlike previously claimed , categorical features can indeed be an important challenge for deep learning models and future work could focus on improvements over conventional embeddings. Our expert feature engineering pipelines can serve as a starting point for evaluating and developing new methods. Furthermore, AutoGluon was sometimes outperformed by single models in our analysis, although it contains the same models. Future work could investigate why AutoGluon does not always benefit from feature engineering to the same extent as single models.

Methods for tabular data with temporal characteristics.Our analysis highlights the temporal nature of many real-world tabular data tasks, as well as the importance of accounting for distribution shifts. Future work could investigate test-time adaptation methods specifically for tabular data, using our datasets and the identified test-time feature engineering techniques as baselines. Furthermore, our findings indicate that the current research focus on static i.i.d. data might hinder the development of techniques to handle weak temporal correlations in tabular data. Future work should focus on developing models with inductive biases for tabular data with temporal characteristics.

Align tabular benchmarks with practitioners needs.We have shown that models developed for tabular data are often applied to datasets with temporal characteristics, while existing tabular data benchmarks are overly focused on i.i.d. data. General-purpose tabular benchmarks should consider including tabular datasets with temporal characteristics instead of excluding them. Furthermore, a benchmark solely for tabular datasets with temporal characteristics could significantly advance model development for this relevant data problem.