# Contrastive Sampling Chains in Diffusion Models

Junyu Zhang

Central South University

zhangjunyu@csu.edu.cn &Daochang Liu

The University of Sydney

daochang.liu@sydney.edu.au &Shichao Zhang

Central South University

zhangsc@csu.edu.cn &Chang Xu

The University of Sydney

c.xu@sydney.edu.au

Corresponding author.

###### Abstract

The past few years have witnessed great success in the use of diffusion models (DMs) to generate high-fidelity images with the help of stochastic differential equations (SDEs). However, discretization error is an inevitable limitation when utilizing numerical solvers to solve SDEs. To address this limitation, we provide a theoretical analysis demonstrating that an appropriate combination of the contrastive loss and score matching serves as an upper bound of the KL divergence between the true data distribution and the model distribution. To obtain this bound, we utilize a contrastive loss to construct a contrastive sampling chain to fine-tuning the pre-trained DM. In this manner, our method reduces the discretization error and thus yields a smaller gap between the true data distribution and our model distribution. Moreover, the presented method can be applied to fine-tuning various pre-trained DMs, both with or without fast sampling algorithms, contributing to better sample quality or slightly faster sampling speeds. To validate the efficacy of our method, we conduct comprehensive experiments. For example, on CIFAR10, when applied to a pre-trained EDM, our method improves the FID from 2.04 to 1.88 with 35 neural function evaluations (NFEs), and reduces NFEs from 35 to 25 to achieve the same 2.04 FID. _The code is available at Contrastive-Sampling_.

## 1 Introduction

Diffusion models (DMs) [57; 22] have emerged as powerful generative models, breaking records in image generation [67; 29; 26; 32], and finding rapid applications in other domains such as video generation , 3D point cloud generation , text-to-image generation , speech synthesis [7; 6], inverse problems [62; 30], and lossless compression . While score-based generative models (SGMs) [57; 58] and denoising diffusion probabilistic models (DDPMs) [47; 22] are two branches of DMs, a certain parameterization reveals an equivalence between them . In a seminal work, Song et al.  generalized DMs through the lens of stochastic differential equations (SDEs). Specifically, for any given stochastic diffusion process that progressively diffuses a data point into random noise with a continuum noise schedule, a DM learns to remove the added noise with a reverse-time SDE . For SGMs, SDEs utilize deep neural networks to match the gradient of the log probability density with respect to data at each noise scale, which is dubbed denoising score matching [25; 68; 59]. In this manner, DMs enable not only exact likelihood computation  like variational auto-encoders (VAE)  but also higher sample quality  than widely-used generative adversarial networks (GANs) .

In practice, directly solving the reverse-time SDE to obtain an image is intractable. Instead, numerical solvers [69; 2; 44] are utilized to discretize the problem by constructing a discrete sampling chain with many sampling steps. Concretely, numerical solvers decompose the intractable integration of an SDE into multiple integration intervals  and solve them iteratively. However, due to the intractability of the integration in high dimensions, numerical solvers are unable to obtain the exact solution for the integration of each interval. Instead, they provide approximate solutions, which introduce a discretization error. This error represents the discrepancy between the approximate solution and the exact solution. It is important to note that any discretization scheme used by numerical solvers introduces discretization errors  in each integration interval. Consequently, there is a gap between the intermediate data distribution and its corresponding model distribution at each discretization step . The accumulation of these discretization errors results in a larger gap between the true data distribution and its model distribution. It is worth noting that the estimation error, resulting from score matching or noise prediction during training, can also contribute to the gap between these two distributions [75; 17]. However, in this paper, we solely concentrate on the discretization error. From an integration point of view, the discretization error gradually decreases as the step size of the interval decreases. However, it cannot be completely avoided because the step size cannot be reduced to infinitesimal values.

To alleviate this issue, more discretization steps can be used to make each approximate solution of the discretization step closer to the exact solution, but this significantly increases the computational cost during sampling. For example, DDPM  requires 1000 steps to produce an image, with each step requiring the evaluation of the neural network once, which is substantially slower than GANs [28; 55]. While recent works have made it possible to achieve high-quality images with significantly reduced sampling steps [29; 32; 70], they still encounter the issue of discretization errors. On the other hand, some fast sampling DMs [27; 65; 72; 53] speed up sampling process for SDEs or ordinary differential equations (ODEs) , but suffer from a sharp decline in image quality since the discretization error grows larger. Hence, discretization errors have an severe impact on DMs.

More recently, several studies have been conducted to enhance numerical solvers which aim to reduce the discretization error. DPM-Solver  utilizes the semi-linear structure to avoid the corresponding discretization error by analytically computing the linear part of the solutions. By comparison, Liu et al.  combine high-order methods with Denoising Diffusion Implicit Model (DDIM)  to solve the ODE and achieve further acceleration. However, the accuracy of this approximation is not theoretically justified and may suffer from significant discretization error if the step size is large . To fill the theoretical gap, Zhang and Chen  propose to utilize an exponential integrator to remedy the discretization error, achieving much better sample quality with smaller step numbers compared to previous fast sampling approaches. Orthogonal to this direction, our focus lies in minimizing the discretization error by optimizing the upper bound of the Kullback-Leibler (KL) divergence between the true sampling chain and a simulated chain at each time step.

The objective of our work is to establish a contrastive sampling chain to fine-tune any pre-trained DMs so as to reducing the discretization error, contributing to a small gap between data distribution and model distribution. Our method is motivated by the observation that minimizing the KL divergence between the true sampling chain and a simulated chain at each corresponding time step effectively reduces the discretization error. This reduction in discretization error directly translates into an improvement in sample quality. To achieve this, we propose minimizing a contrastive loss [11; 9; 21] that effectively reduces the gap between intermediate data distribution and the model data distribution at each time step. Specifically, by selecting instances on the sampling chain of a pre-trained DM for a same image as positive pairs, and choosing negative instances from other training images, a contrastive loss function is formed. To keep the pre-trained model stable in its generative ability, in practice, we combine the contrastive loss with the original generative loss using dynamic weighting schedules during the fine-tuning process. Moreover, the contrastive loss is optimized with backpropagation through time (BPTT) to spread the gradients on the whole sampling chain. In this manner, our method reduces the gap between true data distribution and model distribution, which improves sample quality without increasing sampling time overhead. Comprehensive experiments validate that our method can improve generation quality for various pre-trained models when using the same neural function evaluations (NFEs), or require less NFEs to achieve the same generation quality.

In a nutshell, our work makes the following contributions: 1) We demonstrate that the discretization error results in a gap between each intermediate data distribution and its corresponding model data distribution. 2) We analyze that an appropriate combination of contrastive loss and score matching serves as an upper bound for the KL divergence between the data distribution and the model distribution. 3) We propose a contrastive sampling chain to fine-tune a pre-trained DM with the assistance of our derived upper bound. 4) We present dynamic weighting schedules and BPTT as optimization techniques for our method.

## 2 Background

As mentioned previously, SGMs and DDPMs have been considered almost equivalent when a certain parameterization is applied. For the sake of simplicity, we solely focus on utilizing denoising score matching under SDEs for further investigation in this paper. Below, we present a comprehensive review of the entire DMs process with the lens of SGMs.

**Forward noising diffusion:** The forward diffusion of a DM for \(D\)-dimensional data is a linear diffusion described by the stochastic differential equation (SDE) 

\[dx=_{t}xdt+_{t}d,\] (1)

where \(_{t}^{D D}\) denotes the linear drift coefficient, \(_{t}^{D D}\) denotes the diffusion coefficient, and \(\) is a standard Wiener process. The diffusion Eq. (1) is initiated at the training data and simulated over a fixed time schedule \([0,T]\). Denote by \(p_{t}(x_{t})\) the marginal distribution of \(x_{t}\) and by \(p_{0t}(x_{t} x_{0})\) the conditional distribution from \(x_{0}\) to \(x_{t}\), then \(p_{0}(x_{0})\) represents the underlying distribution of the training data. The simulated stochastic process is represented by \(\{x_{t}^{ SDE}\}_{t[0,T]}\), where \(p_{T}(x_{T})\) is a prior \((x_{T})\) which is is easy to sample from, like Gaussian distribution. The parameters \(_{t}\) and \(_{t}\) are chosen such that the conditional marginal distribution \(p_{0t}(x_{t} x_{0})\) is a simple Gaussian transition kernel, denoted as \((_{t}x_{0},_{t})\). Three popular SDEs in DMs are summarized by Song et al. , which are variance preserving SDE (VP SDE), variance exploding SDE (VE SDE) and sub-variance preserving (subVP SDE). Ideally, we enable to diffuse any data distribution to a prior distribution \((x_{T})\) with one of those three SDEs.

**Backward denoising diffusion:** Under mild assumptions , the forward diffusion Eq. (1) is associated with a reverse-time diffusion process

\[dx=[_{t}x-_{t}_{t}^{T} p_{t}(x)]dt+ {G}_{t}d,\] (2)

where \(\) denotes a standard Wiener process in the reverse-time direction, \( p_{t}(x)\) denotes the gradient of the log probability density with respect to data at each time step \(t\). In general, with a known prior distribution \((x_{T})\), one can model the data distribution \(p_{0}(x_{0})\) with Eq. (2) as \(_{t}\) and \(_{t}\) are fixed according to the forward SDEs. However, to solve Eq. (2), one needs to match the score function \( p_{t}(x)\), which is not accessible.

**Training:** The basic idea of DMs is to use a time-dependent score matching network \(s_{}(x_{t},t)\) to approximate the score \( p_{t}(x)\). This is achieved by score matching techniques [59; 25; 68] where the score network \(s_{}\) is trained by minimizing the denoising score matching loss

\[_{ SM}(;(t))=_{t U[0,T]} _{p(x_{0})p_{0t}(x_{t}|x_{0})}[(t)\| p_{0t }(x_{t}|x_{0})-s_{}(x_{t},t)\|_{2}^{2}].\] (3)

Here \( p_{0t}(x_{t}|x_{0})\) has a closed form expression as \(p_{0t}(x_{t}|x_{0})\) is a simple Gaussian distribution which represents the discretized form of a given SDE, \((t)\) denotes a time-dependent weighting function. This loss can be evaluated using empirical samples by Monte Carlo methods and thus standard stochastic optimization algorithms can be used for training.

**Sampling:** Once the score network \(s_{}(x_{t},t) p_{t}(x)\) is matched for almost all \(x^{D}\) and \(t U[0,T]\), it can be used to generate new samples by solving the backward SDE Eq. (2) with \( p_{t}(x)\) replaced by \(s_{}(x_{t},t)\). It turns out there are infinitely many diffusion processes one can use. In this work, to show the scalability of our method, we consider a general expression of SDEs

\[dx^{ SDE}=[_{t}x^{ SDE}-}{2}_{t}_{t}^{T}s_{}(x^{ SDE},t)]dt+_{t}d,\] (4)

parameterized by \(>0\). When \(=0\), Eq. (4) reduces to an ordinary differential equation (ODE) known as the probability flow ODE . The reverse-time diffusion Eq. (2) with an approximated score is a special case of Eq. (4) with \(=1\).

## 3 Discretization Error Analysis

To generate a new image, one can sample \(x_{T}\) from a standard distribution \((x_{T})\) and solve Eq. (4) to obtain an image \(x_{0}^{ SDE}\). However, in practice, exact solutions are not attainable as it is intractable to solve Eq. (4) directly. To remedy this, one needs to discretize Eq. (4) over time to get an approximated solution, which leads to a discretization error. For brevity, we next investigate the discretization error of solving the probability flow ODE (\(=0\))

\[=_{t}x-_{t}_{t}^{T}s_{}(x_{t}^ { SDE},t),\] (5)

and \(x_{t}^{ SDE}\) represents the discretization samples solved by Eq. (4) with a numerical solver. The exact solution to this ODE is

\[x_{t}^{ SDE}=(t,T)x_{T}^{ SDE}+_{T}^{t}(t,)[-_{}_{}^{T}s_{}(x_{}^{ SDE}, )]d,\] (6)

where \((t,T)\) satisfying \(=F_{T}(t,T)\), \((t,T)=I\) is known as the transition matrix from time \(T\) to \(t\) associated with \(F_{}\). There exist many numerical solvers for Eq. (5) associated with different discretization schemes to approximate Eq. (6). As the discretization step size goes to infinitesimal, the solutions obtained from all these methods converge to that of Eq. (5).

However, the performances of these methods can be dramatically different when the step size is large. On the other hand, to achieve fast sampling in DMs, one needs to approximately solve Eq. (5) with a small number of discretization steps, and thus large step size. Concretely, the discretization of Eq. (4) equals to build a discretization sampling chain, which is \(\{x_{t}^{ SDE}\}_{t[0,T]}\), and iteratively convert it to a new image \(x_{0}^{ SDE}\) with randomly initialize a sample from \(x_{T}\), where \(T\) is the total sampling steps. Following the philosophy of discretization sampling, a small number of discretization steps is equivalent to small sampling steps \(T\) and a large step size \( t\) from \(x_{T}\) to \(x_{t}\). When Euler method applied to Eq. (5), the discretization form can be expressed as

\[x_{t}^{ SDE}=x_{T}^{ SDE}-[_{T}x_{T}^{ SDE}- _{T}_{T}^{T}s_{}(x_{T}^{ SDE},T)] t.\] (7)

From the integral point of view, samples \(x_{0}^{ SDE}\) obtained from Eq. (7) equals the exact solutions of Eq. (5) if and only if the step size between \(x_{T}^{ SDE}\) and \(x_{t}^{ SDE}\) goes to infinitesimal. In practice, it's impossible to achieve it, especially the fast sampling demand in DMs that requires small \(T\) even one-step sampling , which means \(x_{0}^{ SDE}\) is an approximate solution. Hence, the discretization error is essentially a gap between the approximate solution and the exact solution

\[(x_{t}^{ SDE},x_{T}^{ SDE}) =\|_{T}^{t}(t,)[_{} _{}^{T}s_{}(x_{}^{ SDE},)]d.\] (8) \[.-[2*_{T}x_{T}^{ SDE}-_{T}_{T}^{T} s_{}(x_{T}^{ SDE},T)] t\|_{2}^{2}.\]

In light of this, the presence of \((x_{t}^{ SDE},x_{T}^{ SDE})\) creates a gap between \(p_{t}(x_{t})\) and \(p_{t}^{ SDE}(x_{t}^{ SDE})\) since each sample \(x_{t}^{ SDE}\) only approximates the exact solution \(x_{t}\). Consequently, one enables to optimize the KL divergence between \(p_{t}(x_{t})\) and \(p_{t}^{ SDE}(x_{t}^{ SDE})\) to minimize this gap, denoted as \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\). By minimizing this KL term, one can effectively reduce the discretization error. This is because each \(p_{t}^{ SDE}(x_{t}^{ SDE})\) from the simulated chain will closely approximate \(p_{t}(x_{t})\) from the true sampling chain.

## 4 Theoretical Analysis

It is well-known that maximizing the log-likelihood of a probabilistic model is equivalent to minimizing the KL divergence from the data distribution to the model distribution [60; 43]. Similarly, in order to improve the log-likelihood of DMs, we can optimize the KL divergence \(D_{ KL}(p_{0}\|p_{0}^{ SDE})\) between the true distribution \(p_{0}(x_{0})\) and its corresponding model distribution \(p_{0}^{ SDE}(x_{0}^{ SDE})\).

In what follows, we demonstrate that with a appropriate combination of \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) and score matching losses \(_{ SM}(;g(t)^{2})\), they actually becomes an upper bound on \(D_{ KL}(p_{0}\|p_{0}^{ SDE})\). Notably, the weighting function \((t)\) in Eq. (3) is replaced by \(g(t)^{2}\) which is the diffusion coefficient of a SDE in Eq. (1).

**Theorem 1**.: _Let \(p_{0}(x_{0})\) be the true data distribution, \((x_{T})\) be a known prior distribution. Suppose \(\{x_{t}\}_{t[0,T]}\) is a stochastic process defined by the SDE in Eq. (1) with \(x_{0} p_{0}(x_{0})\), where the marginal distribution of \(x_{t}\) is denoted as \(p_{t}(x_{t})\). By comparison, \(\{x_{t}^{ SDE}\}_{t[0,T]}\) is another stochastic process obtained by the reverse-SDE in Eq. (4) from a pre-trained DM with \(x_{T}^{ SDE} p_{T}(x_{T})\) and \(x_{0}^{ SDE} p_{0}^{ SDE}(x_{0}^{ SDE})\), where the marginal distribution of \(x_{t}^{ SDE}\) is denoted as \(p_{t}^{ SDE}(x_{t}^{ SDE})\). Under some regularity conditions detailed in Appendix B, we have_

\[D_{ KL}(p_{0}\|p_{0}^{ SDE}) D_{ KL}(p_{t}\|p_{t} ^{ SDE})+_{ SM}(;g(t)^{2}).\] (9)

_Sketch of proof._ Let \(\) and \(\) denote the path measures  of SDEs in Eq. (1) and Eq. (4) with \(=1\). Conceptually, \(\) is the joint distribution of the forward diffusion process \(\{x_{t}\}_{t[0,T]}\) given by Eq. (1) and \(\) represents another joint distribution of the process \(x_{0}^{ SDE} p_{0}^{ SDE}(x_{0}^{ SDE})\) simulated by Eq. (4). Since we can marginalize \(\) and \(\) to obtain distributions \(p_{0}(x_{0})\) and \(p_{0}^{ SDE}(x_{0}^{ SDE})\), the data processing inequality  gives \(D_{ KL}(p_{0}\|p_{0}^{ SDE}) D_{KL}(_{t}\|_{t})\), where \(_{t}\) and \(_{t}\) are subset of \(\) and \(\) respectively. Intuitively, \(D_{ KL}(p_{0}\|p_{0}^{ SDE})\) is a subset of the path measure between \(\) and \(\) which accumulate all the KL divergence \(D_{ KL}(p_{t}\|p_{t}^{ SDE})_{t[0,T]}\). Hence, \(D_{ KL}(p_{0}\|p_{0}^{ SDE}) D_{KL}(_{t}\|_{t})=_{i}^{t}D_{ KL}(p_{i}\|p_{i}^{ SDE}),t[0,T]\). From the chain rule for the KL divergence , we also have \(D_{KL}(_{t}\|_{t})=D_{ KL}(p_{t}\|p_{t}^{ SDE}) +_{p_{t}()}[D_{ KL}(( x_{i}= })\|( x_{i}^{ SDE}=} ))]\) with \(i[0,t]\), where the KL divergence in the final term can be computed by applying the Girsanov theorem  to Eq. (4) and the reverse-time SDE of Eq. (1). Combining above analysis completes the proof, detailed see in Appendix B.

Though we demonstrate that the combination of score matching losses \(_{ SM}(;g()^{2})\) and a KL term \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) is an upper bound of \(D_{ KL}(p_{0}\|p_{0}^{ SDE})\), it is intractable to optimize Eq. (9) due to the unknown function form of \(p_{t}\). To circumvent this problem, we combined with the mutual information (MI) theory  that MI between \(p_{t}(x_{t})\) and \(p_{t}^{ SDE}(x_{t}^{ SDE})\) can be expressed as \(I(p_{t}^{ SDE}(x_{t}^{ SDE}),p_{t}(x_{t})) D_{KL}(p_{t}^{ SDE} p _{t})\). Moreover, when applied the Jensen's inequality to this term, we enable to obtain an upper bound of \(D_{KL}(p_{t} p_{t}^{ SDE})\) by a InfoNCE loss: \(I_{ InfoNCE}(x_{t}^{ SDE},x_{j}^{ SDE}) D_{KL}(p_{t} p_ {t}^{ SDE})\), where \(j[0,T]\) and \(j t\) (detailed in Appendix B). Hence, we obtain a new upper bound of the KL divergence of \(D_{ KL}(p_{0}\|p_{0}^{ SDE})\) when applies this term to Eq. (9), which is

\[D_{ KL}(p_{0}\|p_{0}^{ SDE})(x_{j}^{ SDE},x_{t}^ { SDE};(t))+_{ SM}(;g(t)^{2}),\] (10)

where \((x_{t}^{ SDE},x_{j}^{ SDE};(t))=(t)*I_{ InfoNCE}(x_{t} ^{ SDE},x_{j}^{ SDE})\) is a scaling of \(I_{ InfoNCE}\) with the weighting function \((t)\). Hence, it is obvious that the the combination of score matching losses \(_{ SM}\) and a contrastive loss \(I_{ InfoNCE}(x_{t}^{ SDE},x_{j}^{ SDE})\) in Eq. (10) is an upper bound of the KL divergence between data distribution \(p_{0}(x_{0})\) and the model distribution \(p_{0}^{ SDE}(x_{0}^{ SDE})\).

Importantly, the score matching term \(_{ SM}(;g(t)^{2})\) is stable and almost fixed in the Eq. (10), as we fine-tune the pre-trained DMs that the score network \(s_{}(x_{t}) p_{t}(x)\) is matched for almost all \(x^{D}\) and \(t U[0,T]\). In this context, optimizing the Eq. (10) equals to minimize \((x_{t}^{ SDE},x_{j}^{ SDE};(t))\), which is the upper bound of \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\). Therefore, minimizing \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) results in a smaller gap between \(x_{t}\) and \(x_{t}^{ SDE}\), indicating that the approximate solution obtained through numerical solvers is closer to the exact solution with the help of contrastive loss. On the other hand, each minimized \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) will decreased the corresponding \((x_{t}^{ SDE},x_{T}^{ SDE})\), contributing to a smaller discretization error between step \(T\) and any step \(t\). From this perspective, utilizing a contrastive loss to increase the similarity between \(x_{j}^{ SDE}\) and each \(x_{t}^{ SDE}\) in a sampling chain is actually minimizing \(D_{ KL}(p_{t}\|p_{t}^{ SDE})_{t[0,T]}\). Hence, by minimizing the Eq. (10), one can achieve a better model distribution \(p_{0}^{ SDE}(x_{0}^{ SDE})\) with a reduction in all the discretization error present in the sampling chain. Moreover, the decreased discretization error also benefits the faster sampling speed .

## 5 Methodology

Contrastive learning has recently achieved remarkable performance  and has made significant waves in deep learning for computer vision tasks . These influential works leveragethe contrastive loss to bring similar images closer together in high-dimensional space, resulting in notable improvements in downstream tasks. Building on the insights provided in Section 4, we employ the InfoNCE loss  to formulate our objective function. By doing so, we effectively reduce the discretization error between \(x_{t}^{}\) and \(x_{t-1}^{}\) by optimizing the upper bound of \(D_{}(p_{t}\|p_{t}^{})\). Consequently, numerical solvers are capable of providing more precise solutions when solving Eq. (5), and a fine-tuned DM naturally enhances the quality of generated samples. In the following sections, we will present our method, focusing on constructing contrastive sampling chains and optimizing DMs.

### Contrastive Sampling Chain

As previously mentioned, our objective is to enhance the quality of samples by reducing the discretization error. Additionally, we illustrate that the presence of this discretization error creates a gap between \(x_{t}^{}\) and \(x_{t}\). Analogously, optimizing this gap is equivalent to minimizing the corresponding discretization error. While directly decreasing this gap is infeasible, minimizing its upper bound provides an alternative approach to achieve the same outcome. Leveraging Theorem 1 and Eq. (10), we propose a contrastive sampling chain for fine-tuning pre-trained DMs using the contrastive loss. To accomplish this, we construct the contrastive loss and combine it with the score matching loss \(_{}(;g(t)^{2})\) to jointly update the parameter \(\).

To construct the contrastive loss, we randomly select an image \(x_{t}^{}\) and another image \(x_{j}^{}\) from the defined sampling chain to form a positive pair. Meanwhile, negative instances are sampled from the training images. Next, we extract 128-dimensional latent representations of these images using the pre-trained MoCov2 . The contrastive loss, known as the InfoNCE loss , is then computed in the subsequent steps. When applying the pre-trained MoCov2 \(\), the InfoNCE loss can be expressed as follows:

\[I_{}(x_{t}^{},x_{j}^{})=(x_{t}^{})(x_{j}^{})/ )}{((x_{j}^{})(x_{t}^{})/)+_{k^{-}}((x_{j}^{}) (x^{-})/)},\] (11)

where \(x_{j}^{}\) and \(x_{t}^{}\) form a positive pair which all generated via iteratively calculating Eq. (7) during the fine-tuning process. By comparsion, \(x^{-}\) are negative instances which sampled from training images and \(\) is a temperature hyper-parameter. Conceptually, the InfoNCE loss is crafted to bring similar features closer, thereby reducing the distance between \(x_{t}^{}\) and \(x_{j}^{}\). In this manner, the sampling chain will become tighter and the discretization error decreased accordingly because the integration interval between sampling steps decreased.

Though we show that \(I_{}(x_{t}^{},x_{j}^{})\) is an upper bound of \(D_{KL}(p_{t} p_{t}^{})\) in Section 4, directly using Eq. (11) to fine-tune DMs will destroy the optimal result of the previous score matching in practice. To circumvent this problem, we combine the generative loss Eq. (3) and contrastive loss Eq. (11) to optimize pre-trained DMs

\[_{}=_{}(;g(t)^{2})+ (t)*I_{}(x_{t}^{},x_{j}^{}),\] (12)

where \((t)\) is the weighting schedule to balance score matching term and contrastive loss term, which will be analyzed in detail in the following subsection. In Eq. (12), we apply the contrastive loss via calculating \(x_{t}^{}\) and all \(x_{j}^{}\) in the same sampling chain with Eq. (11). Though we amplify the \(I_{}\) with \((t)\), the combination of score matching term \(_{}(;g(t)^{2})\) is still an upper bound of \(D_{}(p_{0}\|p_{0}^{})\).

### Optimization

Considering the previous discussion, it is crucial to strike a balance between the InfoNCE loss and the generative loss. Placing excessive emphasis on the InfoNCE loss may disrupt the stability of the pre-trained generative task, while assigning a high weight to the generative loss may have a negligible impact. To address this, we draw inspiration from the fact that the InfoNCE loss quantifies the similarity between \(x_{t}^{}\) and \(x_{j}^{}\), where the similarity increases as \(x_{j}^{}\) approaches \(x_{t}^{}\). Consequently, it is reasonable to implement a dynamic weighting schedule based on the time step \(t\). This schedule assigns higher weights to the loss when \(x_{j}^{}\) is closer to \(x_{t}^{}\), and lower weights otherwise. To strike an appropriate balance between the two losses during the refinement of pre-trained DMs, we have devised two dynamic weighting schedules: the linear and nonlinear weighting schedules.

**Linear weighting schedule:** The weights in this schedule increase progressively with the discretization step as it gets closer to \(x_{j}^{ SDE}\). For instance, larger weights are assigned to \(x_{t}^{ SDE}\) if it shares a higher similarity with \(x_{j}^{ SDE}\). In contrast, smaller weights are assigned to the earlier step of the sampling chain. At this extreme, a linear weighting schedule can be expressed as

\[(t)=*(T-t),\] (13)

where \(\) is a hyper-parameter to scale the weighting schedule.

**Nonlinear weighting schedule:** The nonlinear weighting schedule works in a similar fashion to the linear weighting schedule, with the only difference being that the weights increase at different rates. Specifically, we calculate the noise ratio in \(x_{t}^{ SDE}\)compared to \(x_{j}^{ SDE}\)using the peak signal-to-noise ratio (PSNR) [76; 14] and subsequently apply the PSNR as the elements of this weighting schedule. Therefore, the nonlinear weighting schedule can be expressed as:

\[(t)=*{ PSNR}(x_{j}^{ SDE},x_{t}^{ SDE}).\] (14)

In general, the nonlinear weighting schedule is more consistent with the principle of the sampling chain in DMs, as the weights gradually remove the noise in the samples. However, in practice, both weighting schedules demonstrate similar effectiveness.

**Computational graph:** As mentioned earlier, we construct a contrastive sampling chain to fine-tune a pre-trained DM. Though we utilize Eq. (6) to directly obtain the sample \(x_{t}^{ SDE}\), in practice, \(x_{t}^{ SDE}\) is iterated solved by Eq. (4). Hence, we need to consider the accumulated discretization error such as \((x_{T-1}^{ SDE},x_{T}^{ SDE})\) and \((x_{T-2}^{ SDE},x_{T-1}^{ SDE})\), leads to a larger gap between \(x_{t}^{ SDE}\) and \(x_{T}^{ SDE}\). To address this issue and influence the parameters of previous steps while optimizing the KL divergence \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) for the current step \(t\), we propose to fine-tune pre-trained DMs using a gradient propagation mechanism similar to BPTT , shown in Figure 1.

Specifically, for a given sampling chain, we randomly select one sample \(x_{t}^{ SDE}\) and another sample \(x_{j}^{ SDE}\), and calculate the InfoNCE loss with the previously mentioned weighting function. By combining it with the generative loss (Eq. (3)), we employ an optimizer to propagate the gradients of these two losses in reverse order through the chain from the current step \(t\) to the final step \(T\). In this manner, the gradients influence the entire sampling chain and update the parameters of DMs at each time step \(t\). Consequently, the training objective can be reformulated as follows:

\[_{}=_{ SM}(;(t))+ (x_{t}^{ SDE},x_{j}^{ SDE};(t));t U[0,T],\] (15)

where \((x_{t}^{ SDE},x_{j}^{ SDE};(t))\) represents the InfoNCE loss Eq. (11) with weighting function \((t)\). By following this design philosophy, the InfoNCE loss does not solely update parameters independently; rather, it operates as a unified entity. Consequently, we enable to optimize the current KL divergence \(D_{ KL}(p_{t}\|p_{t}^{ SDE})\) and simultaneously influence KL divergences in previous time steps. This approach ensures a cohesive and synchronized optimization process throughout the entire contrastive sampling chain. In summary, our method effectively mitigates the cumulative discretization errors, as each discretization error is correspondingly reduced. Below, we demonstrate experiment results to further prove our analysis.

Figure 1: Optimizing Diffusion Models via Back Propagation Through Time.

## 6 Experiments

In this section, we demonstrate effectiveness of the contrastive sampling chain by experimental results, namely higher image quality, better log-likelihood, or slightly faster sampling speed. Comprehensive experiments are conducted on various datasets, including CIFAR-10, CelebA/FFHQ 64x64, and ImageNet 64x64. We first utilize contrastive sampling chain to improve pre-trained DMs, for which we select EDM  and LSGM  on CIFAR-10, EDM  on FFHQ, and STDDPM  on CelebA. Additionally, we further verify the performance of our method with sampling chains defined by training-free fast sampling methods, such as DDIM , DPM-Solver , and DEIS , when they are combined with various pre-trained DMs. Specifically, we combine these fast sampling methods with IDDPM  on ImageNet 64x64, DDPM  and SDE VP  on CIFAR-10. Our objective is not to focus on achieving state-of-the-art metrics in generative models, but rather to demonstrate the significant performance of our method in enhancing pre-trained DMs, either combined with or without training-free fast-sampling methods.

It is worth noting that we maintain all the training settings of the pre-trained DMs and only modify the part that constructs the contrastive loss, thereby ensuring fair comparison and demonstrating the flexibility of our method. Similarly, to showcase the applicability of our method to sampling chains defined by fast sampling methods, we replace the original chains provided by pre-trained DMs with new chains for fast sampling, while retaining all other settings the same as fine-tuning pre-trained DMs. Once the fine-tuning process is completed, we test the performance of the refined model by drawing 50,000 samples from it and measuring the widely adopted Frechet Inception Distance (FID) score, Negative Log-Likelihoods (NLL), and Neural Function Evaluations (NFEs), where lower values indicate better performance. Moreover, we also present the generated images for qualitative comparison, shown in Figure 2.

### Performance on Pre-trained DMs

We first showcase the performance of our method in refining the original pre-trained DMs. Tables 1, 4 and 5 present the results of our method on CIFAR-10, FFHQ and CelebA respectively. Our method can achieve better generation quality than baselines when using the same NFEs. On the other hand, our method requires less NFEs to achieve the same quality as baselines. To evaluate on CIFAR-10, we apply our method to LSGM  and EDM  under unconditional or conditional settings. For a fair comparison, we report performances on EDM  under the random seed from [32; 63]. Specifically, our method improves LSGM from 2.10 FID to 1.99 FID and achieves slightly better NLL with 138 NFEs, while requiring only 100 NFEs to attain the same 2.10 FID. Moreover, we enhance EDM from 2.04 FID to 1.88 FID and reduce the NLL from 2.60 to 2.55 with 35 NFEs, while requiring only 25 NFEs for the same 2.04 FID. Furthermore, we improve the performance on conditional EDM, with 1.82 FID reduced to 1.73 FID using 35 NFEs or maintaining the same 1.82 FID with only 27 NFEs. For the evaluation on CelebA and FFHQ, we apply our method to refine STDDPM  and EDM

  Method & Space & NFE\({}_{1}\) & NLL\({}_{1}\) & FID \\ 
**Disordered** & Data & 1000 & **2.49** & 7.41 \\ DDPM  & Data & 1000 & 3.75 & 3.17 \\
**jDDPM** & Data & 1000 & 3.37 & 2.90 \\ STDDPM  & Data & 2000 & 2.91 & 2.47 \\
**INDM** & Latent & 2000 & 3.09 & 2.28 \\ CLD-SGM  & Data & 312 & 3.31 & 2.25 \\
**NCSN** & Data & 2000 & 3.45 & 2.20 \\ LSGM  & Latent & 138 & 3.43 & 2.10 \\ LSGM++ (Ours) & Latent & 1**00 & 3.40 & 2.10 \\ LSGM++ (Ours) & Latent & 138 & **3.40** & **1.99** \\ EDM  & Data & 35 & 2.60 & 2.04 \\ EDM++ (Ours) & Data & **25** & 2.55 & 2.04 \\ EDM-C++ (Ours) & Data & 35 & **2.55** & **1.88** \\ 
**Conditional** & & & & \\  NCSN++(\(\)5) & Data & 2000 & - & 2.25 \\ EDM & Data & 35 & 2.60 & 1.82 \\ EDM-C++ (Ours) & Data & **27** & 2.55 & 1.82 \\ EDM-C++ (Ours) & Data & 35 & **2.55** & **1.73** \\  

Table 1: Performance on CIFAR-10. Our method, denoted as C++, has better quality than baselines with the same NFEs, and fewer NFEs for the same quality.

 respectively. Specifically, our method improves the performance of STDDPM on CelebA 64x64 from 1.90 FID to 1.73 FID with the same 131 NFEs, and achieves the same 1.90 FID with fewer NFEs (100). Similarly, we significantly enhance the performance of EDM on FFHQ 64x64 from 2.39 FID to 2.07 FID with the same 79 NFEs, and achieve the same 2.39 FID with fewer NFEs (63).

### Performance on Fast Samplers

To demonstrate the applicability of our approach to sampling chains defined by training-free fast sampling methods, we carry out a sequence of experiments on CIFAR-10 and ImageNet 64x64, with detailed results in Table 2. We test our method on different fast samplers when combined with various pre-trained DMs. We utilize DDIM , DPM-Solver , and DEIS  to replace the original chains given by DDPM , SDE VP  and IDDPM . Subsequently, for a pre-trained model with sampling chain defined by these fast samplers, we refine it with Eq. (15). In this manner, our method remarkably improves the results as illustrated in Table 2. For example, our method obtains better sample quality when applied to refine the SDE VP with DEIS. Concretely, our method improve the FID from 4.17 to 4.02 in 10 NFEs and increase the 2.57 FID to 2.48 FID in 50 NFEs. In comparison, the improvement on ImageNet is not significant as our main objective is to demonstrate the effectiveness of our method rather than achieving state-of-the-art performance metrics. For instance, when we apply our method to IDDPM with a conditional setting and replace the original chain with DEIS, we observe improvements in FID. With 10 NFEs, the FID decreases from 6.65 to 6.59, and with 14 NFEs, the FID decreases from 3.67 to 3.60. It is worth noting that the symbol "\(\)" indicates that the actual NFE is smaller than the NFE reported in the table provided by Lu et al. . In summary, our method seamlessly integrates with training-free fast sampling algorithms and enables us to achieve better overall performance.

### Ablation Study

We conduct ablation studies in Table 3 to assess the impact of different techniques. Table 3 compares four different settings, i.e., the proposed contrastive loss with a single pair \((x_{t},x_{j})\), a variant of contrastive loss with all samples \(_{i=1,i t}^{T}(x_{t},x_{j})\) in the chain, the proposed contrastive loss with BPTT, and naive fine-tuning without contrastive loss for the same number of epochs as the previous

Figure 2: Randomly selected ImageNet Samples from IDDPM Improved by Our Method.

settings. We also test three weighting schedules, including two previously mentioned dynamic weighting schedules and no schedule with a fixed weight during the entire process. As we mentioned in Section 5.2, calculating the contrastive loss with all samples in the chain disregards the consistency of the entire chain, which can destabilize pre-trained DMs on generative tasks. Therefore, it is reasonable that we obtained a worse FID when fine-tuning DMs with this setting. Moreover, the two dynamic weighting schedules show equivalent performance, both of which are much better than the fixed weight. Our results demonstrate that our contrastive method trained with BPTT for updating parameters can yield the best performance.

## 7 Conclusions

In this paper, we demonstrate the effectiveness of optimizing the KL divergence between the true sampling chain and the simulated chain at each time step in reducing the discretization error associated with numerical solvers used for solving SDEs. Our theoretical analysis supports the use of our objective function as an upper bound of the KL divergence between the data distribution and the model distribution. Notably, optimizing our objective function is equivalent to minimizing the KL divergence between the true sampling chain and the simulated chain at each time step. To address this, we propose a contrastive sampling chain that leverages the derived upper bound to reduce the discretization error. Additionally, we introduce the use of backpropagation through time (BPTT) to propagate gradients in the reverse direction of the sampling chain, and we design dynamic weighting schedules to enhance the stability of the refinement process. Our empirical results demonstrate that our approach significantly improves both the sample quality and the log-likelihood, while slightly accelerating pre-trained DMs without compromising image quality.

**Limitations and broader impact:** Although our method has shown significant improvements, there remains potential for further optimization of our method. For instance, implementing the BPTT computational graph demands a significant amount of GPU memory. Additionally, obtaining analytical solution of our weighting function will undoubtedly tighten the upper bound of the KL divergence between the data distribution and the model distribution. On the other hand, the issue of inefficient sampling remains a major obstacle to the practical application of DMs. It is reasonable to expect that the sampling speed can be greatly improved by a method that effectively optimizes the discretization error. However, it is important to acknowledge that the generation of deepfake images using our method also carries the potential risk of negative misuse of this technology.