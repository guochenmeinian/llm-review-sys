# The Ladder in Chaos: Improving Policy Learning

by Harnessing the Parameter Evolving Path

in A Low-dimensional Space

 Hongyao Tang\({}^{1,2}\)1, Min Zhang\({}^{1}\), Chen Chen\({}^{3}\), Jianye Hao\({}^{1}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University

\({}^{2}\)Mila, Universite de Montreal

\({}^{3}\)Department of Automation, Tsinghua University

###### Abstract

Knowing the learning dynamics of policy is significant to unveiling the mysteries of Reinforcement Learning (RL). It is especially crucial yet challenging to Deep RL, from which the remedies to notorious issues like sample inefficiency and learning instability could be obtained. In this paper, we study how the policy networks of typical DRL agents evolve during the learning process by empirically investigating several kinds of temporal change for each policy parameter. In popular MuJoCo and DeepMind Control Suite (DMC) environments, we find common phenomena for TD3 and RAD agents: (1) the activity of policy network parameters is highly asymmetric and policy networks advance monotonically along a very limited number of major parameter directions; (2) severe detours occur in parameter update and harmonic-like changes are observed for all minor parameter directions. By performing a novel temporal SVD along the policy learning path, the major and minor parameter directions are identified as the columns of the right unitary matrix associated with dominant and insignificant singular values respectively. Driven by the discoveries above, we propose a simple and effective method, called Policy Path Trimming and Boosting (PPTB), as a general plug-in improvement to DRL algorithms. The key idea of PPTB is to trim the policy learning path by canceling the policy updates in minor parameter directions, and boost the learning path by encouraging the advance in major directions. In experiments, we demonstrate that our method improves the learning performance of TD3, RAD, and DoubleDQN regarding scores and efficiency in MuJoCo, DMC, and MinAtar tasks respectively.

## 1 Introduction

Deep Reinforcement Learning (DRL) is far from well understood, although its great potential has been demonstrated with a lot of achievements in different practical problems (Badia et al., 2020; Shah et al., 2022; Fawzi et al., 2022; Degrave et al., 2022; OpenAI, 2022). Consistent efforts are made to gain a better understanding of the learning dynamics of RL agents. Different from the studies with tabular (Sutton and Barto, 1988) and linear approximation (Ghosh and Bellemare, 2020), understanding the learning dynamics of DRL agents is challenging. The difficulty comes from the complex interplay between Deep Learning models and RL algorithms, and further escalates when only limited online interactions or offline logged data are considered.

Recently, there have been a few works that study the learning dynamics of DRL agents from different perspectives. A major stream of works among them studies the co-learning dynamics between representation and DRL functions (usually the value network) (Dabney et al., 2021; Kumar et al.,2021, Lyle et al., 2022, Nikishin et al., 2022, Tang et al., 2022b]. The focus of this stream is that the DRL agent over-shapes its representation toward early experiences and objectives, and becomes less capable of learning for the later learning process. This degradation finally leads to myopic convergence or even divergence. A related work [Sokar et al., 2023] presents a phenomenon called Dormant Neuron in DRL. It reveals that the neurons of typical value networks gradually become inactive during the learning process, leading to the loss of network expressivity. From another angle, a phenomenon called Policy Churn is discovered by [Schaul et al., 2022]. It shows that the greedy policy of a typical value network changes on about 10% of the states after only a single update. These discoveries provide useful insights for understanding the learning behavior of the DRL agent, based on which new methods can be proposed to improve the learning performance of DRL.

In this paper, we aim to unveil the learning dynamics of the policy network during the training process of typical DRL agents. Specifically, we focus on how the parameters of the policy network evolve along the _policy learning path_, i.e., the sequence of historical policy networks. Taking MuJoCo [Brockman et al., 2016] and DeepMind Control Suite (DMC) [Tassa et al., 2018] as typical DRL environments, we conduct a series of empirical investigations on the policy learning path of TD3 [Fujimoto et al., 2018] and RAD [Laskin et al., 2020] agents respectively. From the perspective of accumulated absolute parameter change and a novel temporal SVD view, we present four commonly observed phenomena on the policy learning path. The main message from the phenomena is that the policy networks of DRL agents advance nearly monotonically along only very few major parameter directions and show harmonic-like oscillations on almost all other minor ones.

This drives us to ask the question: _can we make the DRL agent focus on the policy learning along major directions and suppress the oscillation in minor directions?_ To this end, we propose a new method called **Policy Path Trimming and Boosting (PPTB)**. The key idea of PPTB is to trim the policy learning path by canceling the policy parameter updates in minor dimensions, and boost the learning path by encouraging the advance in major directions. The method is a simple and general plug-in improvement for DRL agents, which can be implemented with only a few lines of code modification to most DRL methods. Finally, we evaluate the effectiveness of PPTB based on TD3, RAD, and DoubleDQN [van Hasselt et al., 2016] in MuJoCo, DMC, and MinAtar [Young and Tian, 2019] environments.

Key contributions of this work are summarized below: (1) We propose a temporal analysis view to study the policy learning path of typical DRL agents. (2) We present four common phenomena from our empirical investigations, which reveal the policy evolving path in a low-dimensional space. (3) We propose an easy-to-implement and general improvement for general DRL algorithms, which is demonstrated to improve the learning performance of TD3, RAD, and DoubleDQN regarding scores and efficiency in MuJoCo, DMC, and MinAtar environments.

## 2 Preliminaries

Markov Decision Process (MDP)Consider a Markov Decision Process (MDP) \(,,,,,_{0},T\), defined with a state set \(\), an action set \(\), the transition function \(:\), the reward function \(:\), the discounted factor \([0,1)\), the initial state distribution \(_{0}\) and the horizon \(T\). The agent interacts with the MDP by performing its policy \(: P()\) that defines the distribution over all actions for each state. The objective of an RL agent is to optimize its policy to maximize the expected discounted cumulative reward \(J()=_{}[_{t=0}^{T}^{t}r_{t}]\), where \(s_{0}_{0}(s_{0})\), \(a_{t}(s_{t})\), \(s_{t+1}(s_{t+1} s_{t},a_{t})\) and \(r_{t}=(s_{t},a_{t})\). The state-action value function \(Q^{}\) is defined as \(Q^{}(s,a)=_{}[_{t=0}^{T}^{t}r_{t} s_{0}=s,a _{0}=a]\) for all \(s,a\).

Deep Reinforcement Learning (DRL)With function approximation based on deep neural networks, an RL agent is able to deal with large and continuous state-action space. Conventionally, \(Q^{}\) can be approximated by \(Q_{}\) with parameters \(\) typically through minimizing Temporal Difference loss [Sutton and Barto, 1988], i.e., \(L()=[Q_{}(s,a)-_{a^{}(s^{ })}(r+ Q_{}(s^{},a^{}))]^{2}\). A parameterized policy \(_{}\), with parameters \(\), can be updated by taking the gradient of the objective, i.e., \(^{}+_{}J(_{})\) with a learning rate \(\). Therefore, starting from a initial policy \(_{_{1}}\), the DRL agent proceeds along the learning process and obtains a sequence of policies \(\{_{_{1}},_{_{2}},\}\). For two representative DRL algorithms, Deterministic Policy Gradient (DPG) theorem (Silver et al., 2014) is often used to update a deterministic policy with the gradient: \(_{}J(_{})=_{s^{_{}}}[_{ }_{}(s)_{a}Q_{}(s,a)_{a=_{}(s)}]\), where \(^{_{}}\) is the discounted state distribution under policy \(_{}\); Soft Actor-Critic (SAC) (Haarnoja et al., 2018) updates a stochastic policy with the gradient: \(_{}(_{})=_{s^{_{}}} _{}_{}(a|s)+(_{a}_{}(a|s))- _{a}Q_{}(s,a))_{}f_{}(;s))|_{a=f_{}( ;s)}\) (with noise \(\) and implicit function \(f_{}\) for re-parameterization), based on the maximum-entropy objective \((_{})=_{}_{t=0}^{T}^{t}r_{t}+ (_{}(|s)]\) where \(\) is the temperature of entropy term.

## 3 Phenomena on DRL Policy Learning Path

In this section, we conduct empirical investigations on the typical DRL policy learning process. In specific, we use official codes of TD3 (Fujimoto et al., 2018) and RAD (Laskin et al., 2020) for OpenAI MuJoCo (Brockman et al., 2016) continuous control environments and visual-input DeepMind Control (DMC) Suite (Tassa et al., 2018) environments respectively. Both of them use a policy architecture of two-layer MLP (where the output layer is viewed as _Layer 3_). Note that we exclude the convolution layers for image representation in RAD here and focus on the policy part (Chung et al., 2019). All the experimental details can be found in Appendix D.

First of all, we call the sequence of policies \(\{_{1},_{2},,_{n}\}\) or policy parameters \(\{_{1},_{2},,_{n}\}\) obtained during the learning process of a typical DRL algorithm as _policy learning path_ or _policy parameter path_ throughout this paper. The policy parameters \(_{i}\) is an \(m\)-dimensional vector \(_{i}=[_{i,1},_{i,2},,_{i,m}]^{m}\). In the following, we investigate how policy network parameters evolve along the policy learning path from different angles.

### Policy Parameter Change and Detour

We start by tracking the absolute change amount of each parameter in policy network. Given any \(j\{1,2,,m\}\), we first introduce three quantities for our following investigations.

* Accumulated Parameter Change: \(_{j}^{}}(\{_{i}\}_{i=1}^{n})_{i=1 }^{n-1}|_{i+1,j}-_{i,j}|\).
* Final Parameter Change: \(_{j}^{}}(\{_{i}\}_{i=1}^{n})|_{ n,j}-_{1,j}|\).
* Parameter Update Detour Ratio: \(r_{j}^{}}(\{_{i}\}_{i=1}^{n})^{n-1}|_{i+1,j}-_{i,j}|}{|_{n,j}-_{i,j}|}\).

We use \(_{j}^{}},_{j}^{}},r_{j}^{}}\) as abbreviations when the context is clear and write \((_{j}^{}}),(_{j}^{}}),(r_{j}^{ }})\) as the vectors consisting of the quantities from all parameters. It can be derived that the larger \(r_{j}^{}}\) is, the more detours there are in the evolving path of parameter \(j\). For each environment, we run TD3 or RAD for three trials and collect the policies along the learning process at intervals. We then calculate vectors \((_{j}^{}})\), \((r_{j}^{}})\) and plot their CDF histograms in a layer-wise manner (i.e., Layer 1,2,3). The results are shown in Fig. 1. The results are similar in almost all other MuJoCo and DMC environments, and the complete figures can be found in Appendix D.1.

We begin by assessing the magnitude of accumulated changes in the policy parameters. CDF histograms of vector \((_{j}^{}})\) (as shown in left panels of Fig. 1 (a), (b)) indicate that a certain portion of parameters (ranging from 10% to 40% across environments) in the second and output layers experience minor accumulated changes. In contrast, the cumulative changes for nearly all parameters in the first layer are significantly higher, with thresholds around 0.7 for Hopper and 0.6 for walker-walk. This observation marks our initial key finding.

**Phenomenon 1.1 (Parameter Change Asymmetry):** There is a significant discrepancy in the change amount among policy parameters. The second and the output layers have similar patterns which differ from that of the first layer.

Further discussion about Phenomenon 1.1 can be found in Appendix D.1.

Moving forward, we examine how much each policy parameter detours from its initial value to its final value. The CDF histograms of \((r_{j}^{}})\) (as shown in the right panels of Fig. 1 (a) and (b)) indicate that a significant proportion of parameters exhibit substantial detour ratio, which we identify as the second phenomenon.

**Phenomenon 2.1** (Parameter Update Detour):There are severe detours in policy parameter update. All the three layers show similar patterns.

It can be easy to consider that such detours or oscillations in policy parameter updates can be mainly attributed to noisy policy gradients. This also reveals the correlation between policy parameters where most of them can not be updated independently.

### Temporal SVD Analysis of Policy Learning Path

The two phenomena observed above naturally raise further questions: 1) The asymmetry of parameter change shown by Phenomenon 1.1 indicates the imbalanced importance of parameters. Can we _rule the important parameters or parameter directions off from the less important ones_? 2) For the detour shown by Phenomenon 2.1, can we _identify the difference among parameters or parameter directions in their detour behaviors?_ And do important parameters or parameter directions detour less?

In this section, we make use of a _temporal_ viewpoint on policy path. We introduce Temporal Singular Value Decomposition (SVD) as our main tool for the following investigations. Given a policy path \(\{_{1},_{2},,_{n}\}\) with \(_{i}=[_{i,1},_{i,2},,_{i,m}]\), where \(m\) (\( 10^{5}\) usually) is the dimenionality of

Figure 1: **Policy Parameter Change and Detour on policy paths obtained by TD3 on MuJoCo/Hopper and RAD on DMC/walker-walk. We use the colors () to denote Layer 1,2,3, respectively. Each subfigure contains: (_left_) the CDF histogram of the elements in the vectors \((_{jk}^{})\), and (_right_) the CDF histograms of the elements in the vectors \((r_{jk}^{})\) for \(k\{1,2,3\}\). Only upper 80% values according to \((_{j}^{})\) are taken to plot the histogram for \((r_{j}^{})\) for meaningful analysis; extreme elements in \((r_{j}^{})\) are neglected for clarity. See Phenomenon 1.1 and 2.1 for conclusions.**

Figure 2: **Temporal SVD Analysis of policy paths obtained by RAD on DMC/walker-walk. _(a)_ Curves of \(()\) against various threshold \(\) for the three layers and three periods (i.e., early, middle, later) of the learning process for a temporal view. X-axis represents \(()\) and y-axis represents different \(\). _(b)_ The second and third panels depict the curves of \(u_{*,k}\), with each curve illustrating the evolving path of the \(k\)-th coordinate. The fourth and fifth panels display the curves of detour ratio \((r_{i}^{})\) and final change \((_{i}^{})\). The X-axis represents the index \(i\) of the singular values.**policy parameters. We then stack the parameter vectors along the policy path and form a temporal policy parameter matrix, for which standard SVD can be performed:

\[_{1}\\ _{2}\\ \\ _{n}=_{1,1}&&_{1,m}\\ _{2,1}&&_{2,m}\\ &&\\ _{n,1}&&_{n,m}=u_{ 1,1}&&u_{1,d}\\ &&\\ u_{n,1}&&u_{n,d}}_{}\ _{1}&&0\\ &&\\ 0&&_{d}}_{}v_ {1,1}&&v_{1,m}\\ &&\\ v_{d,1}&&v_{d,m}}_{V^{}}\]

where \(U,V\) are left and right unitary matrices, the singular values are indexed in decreasing order with \(d=(n,m)\). For the convenience of expression, we use \(u_{i,*}\) for the \(i\)-th row vector of \(U\) and use \(u_{*,j}\) for the \(j\)-th column vector (and the same way for \(V^{}\)). The temporal SVD offers us **an angle to view how policy evolves in a lower-dimensional space**: we can now take the row vectors of \(\) as _new \(d\)-dimensional coordinates_ for policies along the policy path (i.e., \(u_{i,*}\) for the \(i\)-th policy) regarding the scaling vector \(()\) and the parameter subspace spanned by the row vectors of \(V^{}\), i.e., \((\{v_{1,*}^{},v_{2,*}^{},,v_{d,*}^{}\}) ^{d}\). In the following, we also call \(\{v_{i,*}\}\) as _SVD directions_. For any index \(j<d\), it is obvious that \(u_{*,j}\) is the evolving path of the \(j\)-th new coordinate.

To answer the questions listed at the beginning of this subsection, we introduce two more quantities:

* Singular Value Information Amount for a dimensionality number \(k\{1,,d\}\): \(a_{k}=^{k}_{i}}{_{i=1}^{d}_{i}}\).
* SVD Major Dimensionality regarding an information threshold \((0,1]\): \(()=\{k\{1,,d\}:_{k}\}\).

Note that \(d=(1)\) and \((0.99)\) recovers _approximate rank_ used in a few recent works (Yang et al., 2020; Kumar et al., 2021; Lyle et al., 2022). Additionally, a smaller \(()\) implies fewer dominant SVD directions when \(\) is held constant.

We also use the policy path data obtained by TD3 on MuJoCo and RAD on DMC as in Sec. 3.1. First, we plot \(()\) against various threshold candidates \(\{0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.99\}\). In addition to a layer-wised manner, we consider three periods (i.e., early, middle, and later) of learning process for a temporal view. The results for DMC/walker-walk are shown in Fig. 2(a). The results are similar in almost all other MuJoCo and DMC environments and the complete figures can be found in Appendix D.2.

The following observations can be made: (i) Within the range of \(\) from 0.5 to 0.8, we observe that \(()\) for all three layers in both environments consistently remain near \(0\). This observation suggests that the policy learning path primarily proceeds within a low-dimensional subspace of the entire parameter space, particularly within the dimensions spanned by the most dominant SVD directions. This provides additional insights into Phenomenon 1.1, elucidating which parameters undergo more significant changes. (ii) The curves of the second layer show a more pronounced steepness near \(=0\), indicating that the parameters in this layer evolve within a subspace characterized by fewer dominant SVD directions compared to those in other layers. This observation also matches the results in Fig. 1, as the second layer is often more over-parameterized than the other layers. (iii) As training advances from period 1 to period 3, the curve's steepness around \(=0\) correspondingly increases, revealing a growing concentration on a diminishing number of dominant singular values. The higher concentration of later periods is easy to understand since policy improvement gradually becomes slower. We can summarize these observations as our third phenomenon.

**Phenomenon 1.2 (Singular Value Information Concentration):** The singular value information is highly concentrated on the first several singular values. The concentration is more evident for the second layer and later periods.

Further, regarding the second question raised in the beginning, we expect to find some correlation between the policy parameter path in the low-dimensional subspace obtained by SVD and the improvement of policy performance. To this end, in Figure 2(b), we examine the evolving path of the \(k\)-th coordinate by plotting coordinate vectors \(u_{*,k}\) (panels 2, 3). We further present the curves of detour ratio \((r_{i}^{})\) (panel 4) and final change \((_{i}^{})\) (panel 5) with respect to the singular value index \(i\). These results are against the policy performance curve (panel 1), and we focus on only the period with significant policy improvement, i.e., the left of the blue dashed vertical line.

According to the results in Fig. 2(b), we observe the final phenomenon.

Phenomenon 2.2 (Policy Path in Major and Minor SVD Directions):The extent of the detour becomes increasingly pronounced as the coordinate index \(k\) increases (from the major SVD directions to the minor ones). Specially, \(u_{*,1}\) is almost monotonic while \(u_{*,k}\) shows harmonic-wave-like changes with overall increasingly higher frequencies for \(k 2\).

It is surprising to observe Phenomenon 2.2. Intuitively, it indicates that the policy parameter path proceeds monotonically along one major direction and oscillates in other directions with frequencies inversely proportional to singular values. This empirically supports our hypothesis that important (i.e., major) parameter directions detour less while insignificant directions detour severely. Since typical policy gradients are derived regarding approximate value estimates, we suggest that the dynamics of policy parameters may be closely related to recent studies on the learning dynamics of value function (Lyle et al., 2021, 2022). We leave it as a major direction of future work.

For a brief summary, till now we have observed that the policy parameter path evolves mainly in a low-dimensional parameter subspace (spanned by \(\{v_{i,*}\}\)). This is commonly seen in our empirical investigations for typical DRL algorithms in popular environments with proprioceptive or visual observations. A natural idea is: why not let the agent focus on the policy update in the parameter subspace, by _following the major SVD directions and neglecting the insignificant SVD directions_? Moreover, somewhat excitingly, it seems that \(u_{*,1},u_{*,2}\) has a strong correlation to policy performance. We are curious about whether it is possible to _leverage the correlation to boost the learning process_. In the next section, we study on these points along with the proposal of Policy Path Trimming and Boosting.

## 4 Policy Path Trimming and Boosting

Driven by the phenomena we discovered in the previous section, we propose a simple and effective method, called Policy Path Trimming and Boosting (**PPTB**), as a general plug-in improvement to DRL algorithms. In the following, we introduce the details of the two components of PPTB, i.e., Policy Path Trimming (Sec. 4.1) and Policy Path Boosting (Sec. 4.2), and then the general implementation of DRL with PPTB (Sec. 4.3).

### Policy Path Trimming

As summarized in Phenomenon 1.2, we have observed that the policy path mainly evolves in a low-dimensional parameter subspace with a large proportion of singular value information concentrated in the first several singular values. Our first idea is to truncate the parameter change in minor SVD directions and only keep the change in major ones based on the Temporal SVD of the policy path. We call this method as Policy Path Trimming (PPT).

Figure 3: A conceptual illustration of Policy Path Trimming and Boosting (PPTB). The 2D view of an exemplary policy learning path in policy parameter space _(left)_ and the corresponding policy learning path improved by PPTB _(right)_.

Given a policy path \(\{_{1},_{2},,_{n}\}\) and the number of major directions to remain \(r_{t}\) (usually \( d=(n,m)\)). For a policy with original parameters \(_{i}\), PPT reconstructs policy parameters by only taking into consideration the first \(r_{t}\) SVD directions:

\[_{i}=u_{i,1}&&u_{i,r_{t}}\\ }_{u_{i,1:r_{t}}}_{1}&&0\\ &&\\ 0&&_{r_{t}}\\ }_{[1:r_{t}]}v_{1,*}\\ \\ v_{r_{t},*}\\ }_{V^{}[1:r_{t}]}\] (1)

Note that we use \(,\) in subscripts and \([,]\) to denote the slices of vector and matrix respectively.

A conceptual illustration of PPT is shown in Fig. 3. As shown by the red arrows and cross, PPT trims the policy parameter update in minor SVD directions and enforces the policy path proceeds in major directions, i.e., in the subspace \((\{v_{1,*}^{},v_{2,*}^{},,v_{r_{t},*}^{}\})\). Intuitively, this suppresses the detours and oscillations of parameter update (recall the results in Fig. 2(b)). Therefore, we expect PPT to improve the efficiency of policy learning process in this sense.

One may worry about whether the trimmed policy parameters \(_{i}^{}\) still ensure an effective policy. For sanity check, we compare policy performance between \(_{i}\) and \(_{i}^{}\) regarding different choices of \(r_{t}\). We refer the readers to Table 4 to 9 in Appendix D.3. We found that a small \(r_{t}\) (\( 128\)) is sufficient to ensure a valid recovery of policy performance, while increasing \(r_{t}\) shows no significant difference. Somewhat surprisingly, we found \(r_{t}\) (\( 64\)) often achieves improved performance after reconstruction, which supports the effectiveness of PPT to some extent. In addition, PPT can be viewed as a particular way to do network resetting to alleviate the primacy bias [Nikishin et al., 2022]. More discussion on this can be found in Appendix B.

### Policy Path Boosting

In addition to suppressing the parameter change in minor directions, we are interested in accelerating policy learning by leveraging the dynamics pattern of major SVD directions as noted in Phenomenon 2.2. In particular, we propose to boost the change of \(u_{*,1},u_{*,2}\) since they show near monotonic changes corresponding to the improvement of policy performance in Fig. 2(b). We call this method as Policy Path Boosting (PPB).

For a policy with original parameters \(_{i}\) among the policy path \(\{_{1},_{2},,_{n}\}\), PPB modifies \(_{n}\) by increasing \(u_{n,1},u_{n,2}\) along the temporal direction as follows with a boosting coefficient \(p_{b}\):

\[_{n,*}&=u_{n,*}+p_{b}(u_{n,*}-u_ {1,*})\\ _{n}&=_{n,1}&_{n,2}&u_{n,3}&&u_{n,d}\\ }_{(_{n,1:2},\;u_{n,3:d})} V^{}\] (2)

PPB only modifies \(u_{n,1},u_{n,2}\) while keeps other parts unchanged. As illustrated by the green arrows in Fig. 3, Intuitively, PPB boosts the policy parameter update in the first two major directions to accelerate the improvement of policy performance.

### DRL with PPTB

Now, we are ready to propose PPTB as a combination of PPT and PPB. Formally,

\[_{i}^{}=(_{i,1:2}\;,\;u_{i,3:r_{t}})[1: r_{t}]V^{}[1:r_{t}].\] (3)

Although we can perform PPTB for any policy on an arbitrary policy path, in practice we consider the policy path that consists of the historical policies within a recent window and the current policy, and we perform PPTB for the current policy.

Apparently, PPTB is algorithmic-agnostic. For almost all off-the-shelf policy-based DRL algorithms, PPTB can be implemented and incorporated by adding the following three steps to the conventional policy update scheme: (1) Initialize a policy buffer and store the parameters of the current policy at intervals along the policy learning process; (2) On certain occasions, retrieve the recent policy parameter path and perform Temporal SVD; then do policy path trimming and boosting for the current policy. (3) Load the consequent policy parameters processed by PPTB back to the current policy.

In our practical implementation used by our experiments, we only make the modifications of about 10-line core codes in the official implementations of TD3 (Fujimoto et al., 2018) and RAD (Laskin et al., 2020). We refer the readers to Algorithm 1 in the appendix for a pytorch-like pseudocode.

## 5 Experiments

### Performance Evaluation of PPTB

SetupsTo evaluate the performance of our proposed method PPTB, we consider the continuous control environments in OpenAI MuJoCo (Brockman et al., 2016) and DeepMind Control Suite (DMC) (Tassa et al., 2018), which cover both proprioceptive and visual observation. We use TD3 (Fujimoto et al., 2018) and RAD (Laskin et al., 2020) as the base algorithms for MuJoCo and DMC respectively, due to their simplicity and effectiveness. We use the official codes of TD3 and RAD and modify them according to Algorithm 1 to implement PPTB with no other change to the original implementation. Our code is available in https://github.com/bluecontra/PPTB.

Moreover, we also evaluate the performance of PPTB for value-based RL, although the greedy policy is implicitly derived from the value networks. Specifically, we use three tasks from MinAtar (Young and Tian, 2019) and use DoubleDQN (van Hasselt et al., 2016) as the value-based RL baseline. More implementation details are provided in Appendix C.

Results for AC MethodsWe train the agent for 1 million timesteps and evaluate the agent every 5k timesteps for each agent-environment configuration. We run each configuration with six random seeds. We consider two evaluation metrics: (1) Score: the _best average_ evaluation returns (across multiple runs) over the course of learning, which is used by TD3 (Fujimoto et al., 2018); (2) AUC: the _mean of average_ evaluation returns over the course of learning, which is also used in (Kumar et al., 2022). The former cares about _effectiveness_ while the latter measures _efficiency_ and _stability_, which is also significant to the practical use of DRL algorithms.

For MuJoCo environments, we report Score and AUC for 1M time step training; for DMC, we report for 100k, 500k as usually done in prior works (Laskin et al., 2020). For the convenience of comparing across different return scales, we report the comparative improvement (\(\)) and the aggregate results by normalizing with a random-agent baseline as 0 and the DRL base algorithm (i.e., TD3 or RAD) as 1. The score of the random-agent baseline is omitted. We report the means and standard deviation errors across six independent trials.

The results for MuJoCo and DMC are reported in Table 1 and 2, respectively. The results show the overall improvements brought by PPTB for both TD3 and RAD, indicating its compatibility and effectiveness. The improvement is more obvious in AUC, which means PPTB improves the learning efficiency and stability of the base algorithms. Somewhat surprisingly, we can observe that PPTB outperforms the base algorithms by a large margin in several environments, e.g., Walker2d and Ant in MuJoCo, and Walker-walk in DMC. To some extent, this reveals the potential of studying and modifying the policy learning path to achieve general improvements to DRL agents.

  Environments & Metrics & TD3 & TD3-PPTB \\   & Score & 10548 \(\) 357 & 11103 \(\) 60 (5.12\% \(\)) \\   & AUC & 7981 \(\) 304 & 8689 \(\) 76 (8.55\% \(\)) \\   & Score & 3394 \(\) 59 & 3420 \(\) 54 (0.77\% \(\)) \\   & AUC & 2005 \(\) 99 & 2249 \(\) 114 (12.28\% \(\)) \\   & Score & 3406 \(\) 436 & 4385 \(\) 164 (28.76\% \(\)) \\   & AUC & 1977 \(\) 353 & 2805 \(\) 85 (41.92\% \(\)) \\   & Score & 4177 \(\) 451 & 5147 \(\) 449 (22.81\% \(\)) \\   & AUC & 2624 \(\) 293 & 3542 \(\) 392 (34.02\% \(\)) \\   & Score & 1.0 & 1.1436 \\   & AUC & 1.0 & 1.2419 \\  

Table 1: Performance evaluation of Policy Path Trimming and Boosting (PPTB) for TD3 (Fujimoto et al., 2018) in four MuJoCo environments. The learning curves are in Figure 10.

Results for Value-based MethodsWe run 3M time steps for SpaceInvader and Breakout, and 5M time steps for Seaquest. We reported the means and standard errors of the final episode returns across six seeds in Table 3.

The results show that PPTB also improves the learning performance of DoubleDQN somewhat significantly in MinAtar tasks with discrete actions. Note that for DoubleDQN, value networks are learned to derive the greedy actions. Intuitively, the learning dynamics of the value learning path and the policy learning path in AC methods are different as typical TD learning adopts bootstrapping and semi-gradient (Sutton and Barto, 1988). We view our experimental results here as preliminary evidence of the effectiveness of PPTB for value-based methods, which also inspired us to further investigate this point in the future.

### Investigation on Policy Learning Path of Behavior Clone

The policy learning path is influenced by multiple factors, including, policy learning algorithm, distribution of training data, network architecture, optimizer, etc. To expand our study on typical online RL a bit, we investigate the policy learning path of Behavior Cloning (BC) with Adam and vanilla SGD on halfcheetah-medium-replay, an offline dataset in D4RL (Fu et al., 2020). This additional setting provides two comparison angles, i.e., RL v.s. BC, Adam v.s. SGD (where using BC and the same offline dataset fixes the influence of data distribution). Similar to our investigation in Sec. 3, we perform parameter analysis and Temporal SVD analysis to the logged policy paths through learning under different configurations. The results are presented in Figure 4 and 12.

For the parameter analysis, we can observe that BC and RL show different distributions of network parameter update amount. Compared with the RL cases reported in Figure 5(a), the parameter update amount is more Gaussian distribution like for BC (Adam) in Figure 4(a). In addition, by comparing the parameter analysis of BC (Adam) and BC (SGD), somewhat surprisingly, we can observe that the asymmetry of parameter update amount turns out to be more severe in BC (SGD). The momentum mechanism used in Adam leads to a relatively more evenly distributed network parameter update amount, while SGD has a large number of minor updates (also reflected by the flat curve in Figure 4(d), the first column) and a long tail in distribution.

For the Temporal SVD analysis, we can observe that BC (Adam), BC (SGD) and the RL cases show different patterns in terms of temporal SVD evolvement. BC policies show wavelet-like oscillations (Figure 4(b)), where the amplitude of the oscillations seems to decrease throughout training; while the oscillations of RL are more harmonic-wave-like (Figure 8(a)).

These observations reveal that low-dimensional policy paths can exist in many policy learning problems with different features influenced by multiple factors. We expect further studies in this direction in the future.

   Environments & Metrics & RAD (100k) & RAD-PPTB (100k) & RAD (500k) & RAD-PPTB (500k) \\   & Score & 553 \(\) 71 & 592 \(\) 55 (7.09\%\(\)) & 898 \(\) 55 & 970 \(\) 5 (8.04\%\(\)) \\   & AUC & 201 \(\) 33 & 254 \(\) 40 (26.76\%\(\)) & 695 \(\) 41 & 767 \(\) 20 (10.40\%\(\)) \\   & Score & 210 \(\) 48 & 349 \(\) 35 (86.87\%\(\)) & 923 \(\) 9 & 940 \(\) 6 (1.94\%\(\)) \\   & AUC & 104 \(\) 22 & 148 \(\) 15 (81.48\%\(\)) & 583 \(\) 30 & 672 \(\) 11 (16.69\%\(\)) \\   & Score & 235 \(\) 15 & 268 \(\) 18 (30.84\%\(\)) & 836 \(\) 12 & 860 \(\) 9 (3.38\%\(\)) \\   & AUC & 158 \(\) 6 & 189 \(\) 2 (103.33\%\(\)) & 530 \(\) 18 & 572 \(\) 20 (10.44\%\(\)) \\   & Score & 360 \(\) 8 & 394 \(\) 10 (10.11\%\(\)) & 574 \(\) 13 & 605 \(\) 10 (5.63\%\(\)) \\   & AUC & 194 \(\) 5 & 207 \(\) 13 (7.64\%\(\)) & 428 \(\) 10 & 452 \(\) 6 (5.94\%\(\)) \\   & Score & 1.0 & 1.3372 & 1.0 & 1.0474 \\   & AUC & 1.0 & 1.5480 & 1.0 & 1.1086 \\   

Table 2: Performance evaluation of Policy Path Trimming and Boosting (PPTB) for RAD (Laskin et al., 2020) in four DeepMind Control (DMC) environments. The learning curves are in Figure 11.

   Environments & DoubleDQN & DoubleDQN-PPTB \\  SpaceInvader & 77.63 \(\) 6.85 & **99.08 \(\) 6.76** \\  Seaquest & 11.00 \(\) 1.70 & **25.43 \(\) 4.38** \\  Breakout & 28.85 \(\) 4.01 & **32.33 \(\) 2.92** \\   

Table 3: Performance evaluation regarding Score of Policy Path Trimming and Boosting (PPTB) for DoubleDQN (van Hasselt et al., 2016) in three MinAtar environments.

## 6 Related Works

In the topic of general Deep Learning, there have been a large number of efforts devoted consistently during the past decade for the purpose of understanding the learning dynamics and behaviors of deep neural networks, e.g., Neural Tangent Kernel (NTK) (Jacot et al., 2018) and Lazy Training (Chizat et al., 2018). Recently, there have been a few works that study the learning dynamics of DRL agents from different perspectives. Among them, a major stream of works study the co-learning dynamics between representation and RL functions (i.e., value network or policy network) (Kumar et al., 2021; Lyle et al., 2022; Nikishin et al., 2022). The focus of this stream is that the DRL agent over-shapes its representation towards early experiences and objectives (e.g., TD targets of early policies) and becomes less capable for the later learning process. Such a degradation becomes more severe and detrimental gradually due to the non-stationary learning nature, finally leading to myopic convergence or even divergence. In this work, we study the learning dynamics of typical DRL policy networks, mainly from the angle of policy network parameters. To the best of our knowledge, we are almost the first to study how the parameters of practical DRL policy networks evolve.

To our knowledge, there are some works that conduct their study from the angle of the path of learning. Dabney et al. (2021) takes inspiration from the study in (Bellemare et al., 2019) and proposes the concept of the _value improvement path_. With this concept, a new method for representation learning is proposed by approximating the value functions along the path. From another angle, Tang et al. (2022) studies the generalization along the policy learning path. Through learning a low-dimensional policy embedding, an improved generalized policy iteration (Sutton and Barto, 1988) that leverages policy generalization is proposed. Closely related to our work, a very recent work (Schneider et al., 2024) demonstrates the existence of gradient subspaces in deep policy gradient algorithms with experimental investigations. In this paper, we investigate the dynamics of policy network parameters from the perspective of Temporal SVD. Moreover, we propose a new method that leverages the parameter evolving path in the low-dimensional space, which demonstrates the potential for improving learning. In essence, the policy learning path is a special scenario of stochastic gradient descent (SGD) in DRL, thus this work is related to studies on the dynamics of SGD or optimization (Gur-Ari et al., 2018; Gressmann et al., 2020; Li et al., 2023) in more general speaking.

## 7 Conclusion

In this paper, we aim to unveil the learning dynamics of policy network over the training course of typical DRL agents, mainly from an empirical perspective. Focusing on the concept of policy learning path, we conduct a series of empirical investigations and summarize four common phenomena, revealing that the policy learning path of typical DRL agents evolves mainly in a low-dimensional space with monotonic and waggling changes in major and minor parameter directions respectively. Driven by our discovery, we propose a simple method, called Policy Path Trimming and Boosting (PPTB), as a plug-in improvement to general DRL algorithms. We demonstrate the effectiveness of PPTB based on TD3 and RAD in a few MuJoCo and DMC environments.

Figure 4: The parameter and Temporal SVD analysis for Behavior Cloning with Adam and SGD optimizers in D4RL halfcheetah-medium-replay.