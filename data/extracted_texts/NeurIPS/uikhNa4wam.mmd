# FIFO-Diffusion: Generating Infinite Videos from Text without Training

Jihwan Kim\({}^{*}\)1  Junoh Kang\({}^{*}\)1  Jinyoung Choi\({}^{1}\)  Bohyung Han\({}^{1,2}\)

Computer Vision Laboratory, \({}^{1}\)ECE & \({}^{2}\)IPAI, Seoul National University

{kjh26720,junoh.kang, jin0.choi, bhhan}@snu.ac.kr

###### Abstract

We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation. Our approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without additional training. This is achieved by iteratively performing diagonal denoising, which simultaneously processes a series of consecutive frames with increasing noise levels in a queue; our method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail. However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner frames by forward reference but such a strategy induces the discrepancy between training and inference. Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing. Practically, FIFO-Diffusion consumes a constant amount of memory regardless of the target video length given a baseline model, while well-suited for parallel inference on multiple GPUs. We have demonstrated the promising results and effectiveness of the proposed methods on existing text-to-video generation baselines. Generated video examples and source codes are available at our project page3.

Figure 1: Illustration of 10K-frame long videos generated by FIFO-Diffusion based on a pretrained text-conditional video generation model, VideoCrafter2 . The number at the top-left corner of each image indicates the frame index. The results clearly show that FIFO-Diffusion can generate extremely long videos effectively based on the model trained on short clips (16 frames) without quality degradation while preserving the dynamics and semantics of scenes.

Introduction

Diffusion probabilistic models have achieved remarkable success in generating high-quality images [8; 25; 5; 18]. On top of the success in the image domain, there has been rapid progress in the generation of videos [9; 22; 37; 31]. Despite the progress, long video generation still lags behind compared to image generation. One reason is that video diffusion models (VDMs) often consider a video as a single 4D tensor with an additional axis corresponding to time, which prevents the models from generating videos at scale. An intuitive approach to generating a long video is autoregressive generation, which iteratively predicts a future frame given the previous ones. However, in contrast to the transformer-based models [10; 28], diffusion-based models cannot directly adopt the autoregressive generation strategy due to the heavy computational costs incurred by iterative denoising steps for a single frame generation. Instead, several recent works [9; 7; 29; 12; 4; 1] adopt a chunked autoregressive generation strategy, which predicts several frames in parallel conditioned on few preceding ones, consequently reducing computational burden. While these approaches are computationally tractable, they often leads to temporal inconsistency and discontinuous motion, especially between the chunks predicted separately, because the model captures a limited temporal context available in the last few--only one or two in practice--frames.

To address the limitation, we propose a novel inference technique, FIFO-Diffusion, which realizes arbitrarily long video generation without training based on a pretrained video generation model for short clips. Our approach effectively alleviates the limitations of the chunked autoregressive method by enabling every frame to refer to a sufficient number of preceding frames. Our approach generates frames through diagonal denoising (Section 3.1) in a first-in-first-out manner using a queue, which contains a sequence of frames with different--monotonically increasing--noise levels over time. At each step, a completely denoised frame at the head is popped out from the queue while a new random noise image is pushed back at the tail. Diagonal denoising offers both advantage and disadvantage; noisier frames benefit from referring to cleaner ones while the model may suffer from training-inference gap because video models are generally trained to denoise frames with the same noise level. To overcome this trade-off and embrace the advantage of diagonal denoising, we propose latent partitioning (Section 3.2) and lookahead denoising (Section 3.3). Latent partitioning reduces training-inference gap by narrowing the range of noise levels in to-be-denoised frames and enables inference with finer steps. Lookahead denoising allows to-be-denoised frames to reference cleaner frames, thereby performing more accurate noise prediction. Furthermore, both latent partitioning and lookahead denoising offer parallelizability on multiple GPUs.

Our main contributions are summarized below.

* We propose FIFO-Diffusion through diagonal denoising, which is a training-free video generation technique for VDMs pretrained on short clips. Our approach denoises images with different noise levels for seamless video generation, enabling us to generate arbitrarily long videos.
* We introduce latent partitioning and lookahead denoising, which respectively reduce the training-inference gap incurred by diagonal denoising and allow the reference to less noisy frames for denoising, improving generation quality.
* FIFO-Diffusion requires a constant amount of memory regardless of the length of the generated videos given a baseline model. It is straightforward to run FIFO-Diffusion in parallel on multiple GPUs.
* Our experiments on four strong baselines, based on the U-Net  or DiT  architectures, show that FIFO-Diffusion generates extremely long videos including natural motion without degradation on quality over time.

## 2 Text-to-Video Diffusion Models

We summarize the basic idea of text-conditional video generation techniques based on diffusion models. They consist of a few key components: an encoder \(()\), a decoder \(()\), and a noise prediction network \(_{}()\). They learn the distribution of videos corresponding to text conditions, and the video is denoted by \(^{f H W 3}\), where \(f\) is the number of frames and \(H W\) indicates the image resolution. The encoder projects each frame onto the latent image space and the decoder reconstructs the frame from the latent. A video latent \(_{0}=()=[_{0}^{1};...;_{}^{f}] ^{f h w c}\) is obtained by concatenating projected frames and the latent diffusion model is trained to denoise its perturbed version, \(_{t}\). For noise \((,)\), a diffusion time step \(t([1,...,T])\), and a text condition \(\), the model is trained to minimize the following loss:

\[_{,,t}[\|_{}(_{t };,t)-\|],\] (1)

where the perturbed latent, \(_{t}=s_{t}_{0}+_{t}\), is obtained using predefined constants \(\{s_{t}\}_{t=0}^{T}\) and \(\{_{t}\}_{t=0}^{T}\), with the constraints \(s_{0}=1\), \(_{0}=0\) and \(_{T}/s_{T} 1\).

Following a time step schedule, \(0=_{0}<_{1}<...<_{S}=T\), initialized by a diffusion scheduler, the model generates a video by iteratively denoising \([_{_{S}}^{1};...;_{_{S}}^{f}](,)\) over \(S\) steps using a sampler \(()\) such as the DDIM sampler. Each denoising step is expressed as

\[[_{_{t-1}}^{1};...;_{_{t-1}}^{f}]=([_{_{t}}^ {1};...;_{_{t}}^{f}],[_{t};...;_{t}],;_{ }),\] (2)

where \(_{_{t}}^{i}\) denotes the latent of the \(i^{}\) frame at time step \(_{t}\).

## 3 FIFO-Diffusion

This section discusses how FIFO-Diffusion generates long videos consisting of \(N\) frames using a pretrained model only for \(f\) frames (\(f N\)). The proposed approach iteratively employs diagonal denoising (Section 3.1) over a predefined number of frames with different levels of noise. Our method also incorporates latent partitioning (Section 3.2) and lookahead denoising (Section 3.3) to improve the output quality of FIFO-Diffusion based on diagonal denoising.

### Diagonal denoising

Diagonal denoising processes a series of consecutive frames with increasing noise levels as depicted in Figure 2. To be specific, for a time step schedule \(0=_{0}<_{1}<...<_{f}=T\), each denoising step is defined as

\[[_{_{0}}^{1};...;_{_{f-1}}^{f}]=([_{_{1}}^{ 1};...;_{_{f}}^{f}],[_{1};...;_{f}],;_{ }).\] (3)

Note that the latents along the diagonal, \([_{_{1}}^{1};...;_{_{f}}^{f}]\), are stored in a queue, \(Q\), and diagonal denoising jointly considers the latents with different noise levels of \([_{1};...;_{f}]\), in contrast to the standard method specified in Equation (2). Algorithm 1 in Appendix C illustrates how diagonal denoising in FIFO-Diffusion works. After each denoising step with \([_{_{1}}^{1};...;_{_{f}}^{f}]\), the foremost frame is dequeued as it arrives at the noise level \(_{0}=0\), and the new latent at noise level \(_{f}\) is enqueued. As a result, the model generates frames in a first-in-first-out manner.

Additionally, the initial diagonal latents \([_{_{1}}^{1};...;_{_{f}}^{f}]\) to initiate the diagonal denoising can be generated from \(f\) random noises at time step \(_{f}\), similar to the the process described above. Notably, our approach does not require pregenerated videos or additional training for the initial latent construction. The detailed algorithm is presented in Algorithm 2 in Appendix C.

Figure 2: Illustration of diagonal denoising with \(f=4\). The frames surrounded by solid lines are model inputs while frames surrounded by dotted line are their denoised version. After denoising, the fully denoised instance at the top-right corner is dequeued while random noise is enqueued.

FIFO-Diffusion takes \(f\) frames as input, regardless of the target video length, and generates an arbitrary number of frames by producing one frame per iteration using a sliding window approach. Note that generating \(N\)\(( f)\) frames for a video requires \((f)\) memory in each step (see Table 2), which is independent of \(N\).

Diagonal denoising allows us to generate consistent videos by sequentially propagating context to later frames. Figure 3 illustrates the conceptual difference between chunked autoregressive methods [9; 7; 29; 12; 4; 1] and FIFO-Diffusion. The former often struggles to maintain long-term context across chunks since their conditioning--only the last generated frame--lacks contextual information propagated from previous frames. In contrast, diagonal denoising progresses through the frame sequence with a stride of 1, allowing each frame to reference a sufficient number of preceding frames during generation. This approach enables the model to naturally extend the local consistency of a few frames to longer sequences. Additionally, FIFO-Diffusion requires no subnetworks or extra training, depending solely on a base model. This distinguishes it from existing autoregressive methods, which often require an additional prediction model or fine-tuning for masked frame outpainting.

### Latent partitioning

Although diagonal denoising enables infinitely long video generation, it introduces a training-inference gap, as the model is trained to denoise all frames at uniform noise levels. To address this, we aim to reduce noise level differences in the input latents by extending the queue length \(n\) times (from \(f\) to \(nf\) with \(n>1\)), partitioning it into \(n\) blocks, and processing each block independently. Note that the extended queue length increases the number of inference steps. Algorithm 3 in Appendix C provides the procedure of FIFO-Diffusion with latent partitioning. Let a queue \(Q\) has diagonal latents \([_{_{1}}^{1};...;_{_{nf}}^{nf}]\). We partition \(Q\) into \(n\) blocks, \([_{0};\!...;_{n-1}]\), of equal size \(f\), then each block \(_{k}\) contains the latents at time steps \(_{k}=[_{kf+1};...;_{(k+1)f}]\). Next, we apply diagonal denoising to each block in a divide-and-conquer manner (See Figure 4 (a)). At \(k=0\),\(...,n-1\), each denoising step updates the queue as follows:

\[_{k}(_{k},_{k},;_{}).\] (4)

Latent partitioning offers three key advantages for diagonal denoising. First, it significantly reduces the maximum noise level difference between the latents from \(|_{_{nf}}-_{_{1}}|\) to \(_{k}|_{_{k+1)f}}-_{_{kf+1}}|\). The effectiveness of latent partitioning is supported theoretically and empirically by Theorem 3.3 and Table 3, respectively. Second, latent partitioning improves throughput of inference by processing partitioned blocks in parallel on multiple GPUs (see Table 2). Last, it allows the diffusion process to leverage a large number of inference steps, \(nf\) (\(n 2\)), reducing discretization error during inference. We now show in Theorem 3.3 that the gap incurred by diagonal denoising is bounded by the maximum noise level difference, which implies that the error can be reduced by narrowing the noise level differences of model inputs.

**Definition 3.1**.: We define \(_{t}^{}[_{t}^{1};...;_{t}^{f}]\), where \(_{t}^{i}\) is the latent of the \(i^{}\) frame at time step \(t\) (noise level of \(_{t}=ct\) for a constant \(c\)). \(_{t}^{}\) satisfies the following ODE from :

\[d_{t}^{}=c(_{t}^{},t {1})dt,\] (5)

for \(=[1;...;1]\) and \(()\) is the scaled score function \(-_{} p()\).

Figure 3: Comparison between the chunked autoregressive methods and FIFO-Diffusion proposed for long video generation. The random noises (black) are iteratively denoised to image latents (white) by the models. The red boxes indicate the denoising network in the pretrained base model while the green boxes denote the prediction network obtained by additional training.

**Lemma 3.2**.: _If \(()\) is bounded, then_

\[||_{t}^{i}-_{s}^{i}||=O(|t-s|)\;\;\;\; i.\]

Proof.: Refer to Appendix A.1. 

**Theorem 3.3**.: _Assume the system satisfies the following two hypotheses:_

_(Hypothesis 1)_ \(()\) _is bounded._

_(Hypothesis 2) The diffusion model_ \(_{}()\) _is_ \(K\)_-Lipschitz continuous._

_Then, for diagonal latents_ \(^{}=[_{_{1}}^{1};...;_{_{f}}^{f}]\) _and corresponding time steps_ \(^{}=[_{1};...;_{f}]\)_,_

\[||_{}(^{},^{})^{i}- (_{_{i}}^{},_{i})^{i}||=|| {}_{}(_{_{i}}^{},_{i})^{i}- (_{_{i}}^{},_{i})^{i}||+O(| _{_{f}}-_{_{1}}|),\] (6)

_where the \(_{}()^{i}\) and \(()^{i}\) are \(i^{}\) element of \(_{}()\) and \(()\), and \(_{1}<...<_{f}\). In other words, the error introduced by diagonal denoising is bounded by the noise level difference._

Proof.: The left-hand side of Equation (6) is bounded as:

\[||_{}(^{},^{})^{i} -(_{_{i}}^{},_{i})^{i}|| |+||_{}(_{_{i}}^{},_{i})^{i}-(_{_{i}}^{},_{i})^{i} ||,\]

by triangle inequality. Then, the first term of the right-hand side satisfies the following inequality:

\[||_{}(^{},^{})^{i}-_{}(_{_{i}}^{},_{i})^{i}||  K||(^{},^{})-(_{ _{i}}^{},_{i})||\] \[ K_{j=1}^{f}(||_{_{j}}^{j}-_{_{i}}^{j }||+|_{j}-_{i}|)=O(|_{_{f}}-_{_{1}}|),\]

which is from the Lipshitz continuity and Lemma 3.2. Furthermore, we provide justification for _(Hypothesis 2)_ in Appendix A.2. 

### Lookahead denoising

Although our diagonal denoising introduces training-inference gap, it is advantageous in another respect because noisier frames benefit from observing cleaner ones, leading to more accurate denoising. As empirical evidence, Figure 5 shows the relative MSE losses in noise prediction of diagonal denoising with respect to the original denoising strategy. The formal definition of the relative MSE is given by

\[_{}(^{},^{})^ {i}-(_{_{i}}^{},_{i})^{i}||_ {2}}{||_{}(_{_{i}}^{},_{i} )^{i}-(_{_{i}}^{},_{i})^ {i}||_{2}}.\] (7)

Figure 4: Illustration of latent partitioning and lookahead denoising where \(f=4\) and \(n=2\). (a) Latent partitioning divides the diffusion process into \(n\) parts to reduce the maximum noise level difference. (b) Lookahead denoising on (a) enables all frames to be denoised with an adequate number of former frames at the expense of two times more computation than (a).

As depicted in Figure 4 (b), we estimate noise only for the benefited later half of the frames. In other words, we perform diagonal denoising with a stride of \(f^{}=\), updating only the last \(f^{}\) frames to ensure that each frame is denoised with reference to a sufficient number--at least \(f^{}\)--of clearer frames. Precisely, for \(k=0,\)...\(,2n-1\), each denoising step updates the queue as

\[Q_{k}^{f^{}+1:f}(Q_{k},_{k},;_{ })^{f^{}+1:f}.\] (8)

Algorithm 4 in Appendix C outlines the detailed procedure of FIFO-Diffusion with lookahead denoising. We illustrate the effectiveness of lookahead denoising with the red line in Figure 5. Except for a few early time steps, lookahead denoising enhances the baseline models noise prediction performance, nearly eliminating the training-inference gap described in Section 3.2. Note that, this approach requires twice the computation of the original diagonal denoising since we only update the half of the queue each step. However, the concerns about the additional computational overhead are easily addressed via parallelization in the same manner as latent partitioning (see Table 2).

## 4 Experiment

This section presents the examples generated by existing long video generation methods including FIFO-Diffusion, and evaluates their performance qualitatively and quantitatively. We also perform the ablation study to verify the benefit of latent partitioning and lookahead denoising introduced in FIFO-Diffusion.

### Implementation details

We implement FIFO-Diffusion based on existing open-source text-to-video diffusion models trained on short video clips, including three U-Net-based models, VideoCrafter1 , VideoCrafter2 , and zeroscope2, as well as a DiT-based model, Open-Sora Plan3. We employ the DDIM sampling  with \(\{0.5,1\}\). Appendix B provides more details about our implementations.

For quantitative evaluation, we measure \(_{128}\) and IS  scores using Latte  as a base model, which is a DiT-based video model trained on UCF-101 . We generate 2,048 videos with 128 frames each to calculate \(_{128}\), and randomly sample a 16-frame clip from each video to measure IS score, following evaluation guidelines in . To calculate computational cost, we adopt VideoCrafter2 as the baseline model, using a DDPM scheduler with 64 inference steps on A6000 GPUs.

### Qualitative results

We first evaluate the performance of the proposed approach qualitatively. Figure 1 illustrates examples of long videos (longer than 10K frames) generated by FIFO-Diffusion based on VideoCrafter2. It demonstrates the ability of FIFO-Diffusion to generate significantly longer videos than the target length of pretrained baseline models--16 frames in this case. The individual frames exhibit outstanding visual quality with no perceptual quality degradation even in the later part of the videos while preserving semantic information across all frames. Figure 6 (a) and (b) present the generated videos with natural motion of scenes and cameras; the consistency of motion is effectively maintained by referencing earlier frames through the generation process.

Furthermore, Figure 6 (c) illustrates that FIFO-Diffusion can generate videos with extensive motion driven by a sequence of changing prompts. The capability to generate multiple motions and seamless transitions between scenes highlight the practicality of our method. Please refer to Appendices D and E for more examples and our project page1 for video demos, in comparisons with the videos from other baselines.

Figure 5: The relative MSE losses of the noise prediction of \(_{_{i}}^{i}\) (see Equation (7)) when \(n=4\). VDM’ indicates the original denoising strategy as a reference line. ‘LP’ and ‘LD’ denote latent partitioning and lookahead denoising, respectively.

Figure 6: Illustrations of long videos generated by FIFO-Diffusion based on (a) Open-Sora Plan and (b) VideoCrafter2, as well as (c) multiple prompts based on VideoCrafter2. The number on the top-left corner of each frame indicates the frame index.

Figure 7: Sample videos generated by (first) FIFO-Diffusion on VideoCrafter2, (second) FreeNoise on VideoCrafter2, (third) Gen-L-Video on VideoCrafter2, and (last) LaVie + SEINE. The number on the top-left corner of each frame indicates the frame index.

[MISSING_PAGE_FAIL:8]

### Computational cost

To evaluate computational efficiency, we assess memory usage and inference time per frame for training-free, long video generation methods. As shown in Table 2, FIFO-Diffusion generates videos of arbitrary lengths with a constant memory allocation, while FreeNoise requires memory proportional to the target video length. Although Gen-L-Video maintains nearly constant memory usage, it exhibits the slowest inference speed due to redundant computations. Notably, FIFO-Diffusion leverages parallel computation; while incorporating lookahead denoising increases computational demand, utilizing multiple GPUs for parallel processing significantly reduces sampling time.

### Ablation study

We conduct ablation study to analyze the effect of latent partitioning and lookahead denoising on the performance of FIFO-Diffusion. Figure 9 shows that latent partitioning significantly improves both visual quality and temporal consistency of the generated videos. Moreover, lookahead denoising further refines the quality of generated videos by facilitating temporal coherency and reducing flickering effects. The videos on our project page5 clearly demonstrate the benefit of FIFO-Diffusion. Additionally, Table 3 compares the relative MSE loss (see Equation (7)) averaged over all time steps across different ablation settings. The results show that latent partitioning effectively reduces the training-inference gap caused by diagonal denoising as the number of partitions increases. Furthermore, lookahead denoising enhances the model's noise prediction accuracy, achieving low relative MSE losses (below 1.0) when used in conjunction with latent partitioning.

## 5 Related work

This section discusses existing diffusion-based generative models for videos including long video generation techniques.

### Video diffusion models

Diffusion models, originally developed for high-quality image synthesis, have become a prominent approach in video generation [2; 9; 22; 37; 31]. VDM  modifies the structure of U-Net  and proposes a 3D U-Net architecture to incorporate temporal information for denoising. On the

    & \# of partitions & without LD & with LD \\  without LP & 1 & 1.09 & 1.01 \\ with LP & 2 & 1.04 & 0.99 \\ with LP & 4 & 1.02 & **0.98** \\   

Table 3: Relative MSE losses of ablations. ‘LP’ and ‘LD’ denote latent partitioning and lookahead denosing, respectively.

Figure 9: Ablation study. DD, LP, and LD signifies diagonal denoising, latent partitioning, and lookahead denoising, respectively. The number on the top-left corner of each frame indicates the frame index.

other hand, Make-A-Video  employs a 1D temporal convolution layer following a 2D spatial convolutional layer to approximate 3D convolution. This design enables the model to capture visual-textual relationships by training spatial layers with image-text pairs before incorporating temporal context through 1D temporal layers. Recently,  introduce a transformer architecture, known as DiT, for diffusion models. Additionally, several open-sourced text-to-video models have emerged [31; 2; 32; 3], trained on large-scale text-image and text-video datasets.

### Long video generation

Long video generation approaches typically involve training models to predict future frames sequentially [29; 6; 1; 4]. or generate a set of frames in a hierarchical manner [7; 34]. For instance, Video LDM  and MCVD  employ autoregressive techniques to sequentially predict frames given several preceding ones, while FDM  and SEINE  generalize masked learning strategies for both prediction and interpolation. Autoregressive methods are capable of producing indefinitely long videos in theory, but they often suffer from quality degradation due to error accumulation and limited temporal consistency across frames. Alternatively, NUWA-XL  adopts a hierarchical approach, where a global diffusion model generates sparse key frames with local diffusion models filling in frames using the key frames as references. However, this hierarchical setup requires batch processing, making it unsuitable for generating infinitely long videos.

There are a few training-free long video generation techniques. Gen-L-Video  treats a video as overlapped short clips and introduces temporal co-denoising, which averages multiple predictions for one frame. FreeNoise  employs window-based attention fusion to sidestep the limited attention scope issue and proposes local noise shuffle units for the initialization of long video. FreeNoise requires memory proportional to the video length for the computation of cross, limiting its scalability for generating infinitely long videos.

### Diffusion models with latents of different noise levels

Recent studies have adopted diffusion models for sequence generation by leveraging a sliding window approach with temporally varying noise levels [36; 20]. These methods train diffusion models from scratch to accommodate latents with different noise levels, addressing tasks such as motion generation  and video prediction . However, training diffusion models from scratch introduces significant computational costs, especially for text-to-video generation tasks. In contrast, our approach is a training-free inference technique based on the standard diffusion models, trained on latents with uniform noise, for sequence generation within the sliding window framework. While  is implemented with a nested loop to deal with two different axes corresponding to video frame index and diffusion time step, FIFO-Diffusion combines these two dimensions using a 1D queue, improving efficiency with a single loop.

## 6 Conclusion

We introduced FIFO-Diffusion, a novel inference algorithm that enables the generation of infinitely long videos from text without tuning video diffusion models pretrained on short clips. Our approach achieves this by introducing diagonal denoising, which processes latents with increasing noise levels using a queue in a first-in-first-out fashion. While diagonal denoising presents a trade-off, we addressed its limitations with latent partitioning and leveraged its strengths with lookahead denoising. Together, these techniques allow FIFO-Diffusion to generate high-quality, long videos that maintain strong scene consistency and expressive dynamic motion. Although latent partitioning reduces the training-inference gap of diagonal denoising, the gap persists due to changes in the model's input distribution. However, we believe that this gap could be addressed by integrating the diagonal denoising paradigm into the training phase, and the benefits of FIFO-Diffusion remains for training as well. We leave this integration as future work; aligning the training and inference environments can significantly enhance FIFO-Diffusion's performance.