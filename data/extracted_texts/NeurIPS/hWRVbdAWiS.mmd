# Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning

Ruoqi Zhang Ziwei Luo Jens Sjolund Thomas B. Schon Per Mattsson

Department of Information Technology, Uppsala University

{ruoqi.zhang,ziwei.luo,jens.sjolund,thomas.schon,per.mattsson}@it.uu.se

Corresponding authors

###### Abstract

Diffusion policy has shown a strong ability to express complex action distributions in offline reinforcement learning (RL). However, it suffers from overestimating Q-value functions on out-of-distribution (OOD) data points due to the offline dataset limitation. To address it, this paper proposes a novel entropy-regularized diffusion policy and takes into account the confidence of the Q-value prediction with Q-ensembles. At the core of our diffusion policy is a mean-reverting stochastic differential equation (SDE) that transfers the action distribution into a standard Gaussian form and then samples actions conditioned on the environment state with a corresponding reverse-time process. We show that the entropy of such a policy is tractable and that can be used to increase the exploration of OOD samples in offline RL training. Moreover, we propose using the lower confidence bound of Q-ensembles for pessimistic Q-value function estimation. The proposed approach demonstrates state-of-the-art performance across a range of tasks in the D4RL benchmarks, significantly improving upon existing diffusion-based policies. The code is available at [https://github.com/ruoqizzz/entropy-offlineRL](https://github.com/ruoqizzz/entropy-offlineRL).

## 1 Introduction

Offline reinforcement learning (RL), also known as batch RL  focuses on learning optimal policies from a previously collected dataset without further active interactions with the environment . Although offline RL offers a promising avenue for deploying RL in real-world settings where online exploration is infeasible, a key challenge lies in deriving effective policies from fixed datasets, which usually are diversified and sub-optimal. The direct application of standard policy improvement approaches is hindered by the distribution shift problem . Previous works mainly address this issue by either regularizing the learned policy close to the behavior policy  or by making conservative updates for Q-networks .

Diffusion models have rapidly become a prominent class of highly expressive policies in offline RL . While this expressiveness is beneficial when modeling complex behaviors, it also means that the model has a higher capacity to overfit the noise or specific idiosyncrasies in the training data. To address this, existing work introduce Q-learning guidance and regard the diffusion loss as a special regularizer adding to the policy improvement process . Such a framework has achieved impressive results on offline RL tasks. However, its performance is limited by pre-collected datasets (or behavior policies) and the learning suffers severe overestimation of Q-value functions on unseen state-action samples .

One promising approach is to increase exploration for out-of-distribution (OOD) actions, with the hope that the RL agent can be more robust to diverse Q-values and estimation errors . Previous online RL algorithms achieve this by maximizing the entropy of pre-defined tractable policies suchas Gaussians [35; 13; 15]. Unfortunately, directly computing the log probability of a diffusion policy is almost impossible since its generative process is a stochastic denoising sequence. Moreover, it is worth noting that entropy is seldom used in offline settings because it may lead to a distributional shift issue which may cause overestimation of Q-values on unseen actions in the offline dataset.

Another line of work addresses the overestimation problem by enforcing the Q-values to be more pessimistic [28; 20]. Inspired by this, uncertainty-driven RL algorithms employ an ensemble of Q-networks to provide different Q-value predictions for the same state-action pairs [1; 3]. The variation in these predictions serves as a measure of uncertainty. For state-action pairs exhibiting high predictive variance (e.g., OOD data points), these methods preferentially adopt pessimistic Q-value estimations as policy guidance.

In this paper, we present an entropy-regularized diffusion policy with Q-ensembles for offline RL. At the core of our method is a mean-reverting stochastic differential equation (SDE)  which allows us to sample actions from standard Gaussian conditioned on the environment state. We show that such an SDE provides a tractable entropy regularization that can be added in training to increase the exploration of OOD data points. In addition, we approximate the lower confidence bound (LCB) of Q-ensembles to alleviate potential distributional shifts, thereby learning a pessimistic policy to handle high uncertainty scenarios from offline datasets. As illustrated in Figure 1, both entropy regularization and Q-ensembles can improve RL performance on unbalanced offline datasets. The LCB approach further reduces the variance between different trials and provides a better estimation of unseen state-action pairs.

Our model achieves highly competitive performance across a range of offline D4RL benchmark tasks  and, in particular, significantly outperforms other diffusion-based approaches in the Antmaze environment. The superior performance demonstrates the effectiveness of the entropy-regularization and Q-ensembles. Overall, the proposed method encourages policy diversity and cautious decision-making, enhancing exploration while grounding the policy in the confidence of its value estimates derived from the offline dataset.

## 2 Background

This section reviews the core concepts of offline RL and then introduces the mean-reverting SDEs and shows how we sample actions from its reverse-time process. Note that there are two types of timesteps for RL and SDE. To clarify that, we use \(i\{0,,N\}\) to denote the RL trajectories' step and \(t\{0,,T\}\) to index diffusion discrete times.

Offline RL.We consider learning a Markov decision process (MDP) defined as \(M=\{,,P,R,,d_{0}\}\), where \(\) and \(\) are the state and action spaces, respectively. The state transition probability is denoted \(P(_{i+1}_{i},_{i})\) and \(R:\) represents a reward function,

Figure 1: A toy RL task in which the agent sequentially takes two steps (starting from 0) to seek a state with the highest reward. **Left**: The reward function is a mixture of Gaussian, and the offline data distribution is unbalanced with most samples located in low-reward states. **Center**: Training different policies on this task with 5 random seeds for 500 epochs. We find that a diffusion policy with entropy regularization and Q-ensembles yields the best results with low training variance. **Right**: Learned Q-value curve for the first step actions in state 0. The approximation of the lower confidence bound (LCB) of Q-ensembles is also plotted.

\((0,1]\) is the discount factor, and \(d_{0}\) is the initial state distribution. The goal of RL is to maximize the cumulative discounted reward \(_{i=0}^{}^{i}_{_{i}(_{i})} r(_{i},_{i})\) with a learned policy \(\). In contrast to online RL which requires continuous interactions with the environment, offline RL directly learns the policy from the static dataset \(=\{(_{i},_{i},r_{i},_{i+1})\}_{i=1}^{N _{}}\). In the offline setting, two primary challenges are frequently encountered: over-conservatism and a limited capacity to effectively utilize diversified datasets . To address the issue of limited capacity, diffusion models have recently been employed to learn complex behavior policies from datasets .

Mean-Reverting SDE.Assume that we have a random variable \(^{0}\) sampled from an unknown distribution \(p_{0}()\). The mean-reverting SDE  is a diffusion process \(\{^{t}\}_{t[0,T]}\) that gradually injects noise to \(^{0}\):

\[=-_{t}\,t+_{t}\,, ^{0} p_{0}(), \]

where \(\) is the standard Wiener process, \(_{t}\) and \(_{t}\) are predefined positive parameters that characterize the speed of mean reversion and the stochastic volatility, respectively. Compared to IR-SDE , we set the mean to 0 to let the process drift to pure noise to fit the RL environment. The mean can however be tuned to high-reward actions in the offline dataset or prior knowledge. By setting \(_{t}^{2}=2_{t}\) for all diffusion steps, the solution to the forward SDE (\(<t\)) is given by

\[p(^{t}^{})=(^{t}^{}^{-_{:t}},(1-^{-2\,_{:t}})), \]

where \(_{:t}_{}^{t}_{z}\,z\) are known coefficients . In the limit \(t\), the marginal distribution \(p_{t}()=p(^{t}^{0})\) converges to a standard Gaussian \((0,)\). This gives the forward process its informative name, i.e. "_mean-reverting_". Then, Anderson  states that we can generate new samples from Gaussian noises by reversing the SDE (1) as

\[=-_{t}\,-_{t}^{2}\,_{}  p_{t}()\,t+_{t}\,}, \]

where \(^{T}(0,)\) and \(}\) is the reverse-time Wiener process. This reverse-time SDE provides a strong ability to fit complex distributions, such as the policy distribution represented in the dataset \(\). Moreover, the ground truth score \(_{} p_{t}()\) is acquirable in training. We can thus combine it with the reparameterization trick

\[^{t}=^{0}^{-_{t}}+^{-2 _{t}}}_{t} \]

to train a time-dependent neural network \(_{}\) using noise matching on randomly sampled timesteps:

\[L_{}()_{t[0,T]}_{}(^{t},t)-_{t}), \]

where \(_{t}(0,)\) is a Gaussian noise and \(\{^{t}\}_{t=0}^{T}\) denotes the discretization of the diffusion process. See Appendix A.1 for more details about the solution, reverse process, and loss function.

Sample Actions with SDE.Most existing RL algorithms employ unimodal Gaussian policies with learned mean and variance. However, this approach encounters a challenge when applied to offline datasets, which are typically collected by a mixture of policies and therefore hard to represent by a simple Gaussian model. Thus we prefer to represent the policy with an expressive model such as the reverse-time SDE. More specifically, the forward SDE provides theoretical guidance to train the neural network, and the reverse-time SDE (3) generates actions from Gaussian noise conditioned on the current environment state as a typical score-based generative process .

## 3 Method

We present the three core components of our method: 1) an efficient sampling strategy based on the mean-reverting SDE; 2) an entropy regularization term that enhances action space exploration; and 3) a pessimistic evaluation with Q-ensembles that avoids overestimation of unseen actions.

### Optimal Sampling with Mean-Reverting SDE

We have shown how to sample actions with reverse-time SDEs in Section 2. However, generating data from the standard mean-reverting SDE  requires many diffusion steps and is sensitive to the noise scheduler . To improve sample efficiency, we propose generating actions from the posterior distribution \(p(^{t-1}^{t})\) conditioned on \(^{0}\). This approach ensures fast convergence of the generative process while preserving its stochasticity.

**Proposition 3.1**.: _Given an initial variable \(^{0}\), for any diffusion state \(^{t}\) at time \(t[1,T]\), the posterior of the mean-reverting SDE (1) conditioned on \(^{0}\) is_

\[p(^{t-1}^{t},^{0})=(^{t-1} _{t}(^{t},\,^{0}),\ _{t}), \]

_which is a Gaussian with mean and variance given by:_

\[_{t}(^{t},^{0}) ^{-2_{t-1}}}{1-^{-2_{t}}}^{- ^{}_{t}}^{t}+^{-2^{}_{t}}}{1- ^{-2_{t}}}^{-_{t-1}}^{0} _{t}^{-2_ {t-1}})(1-^{-2^{}_{t}})}{1-^{-2_{ t}}}, \]

_where \(^{}_{i}_{i-1}^{i}_{t}dt\) and \(_{t}\) is to substitute \(_{0:t}\) for clear notation._

The proof is provided in Appendix A.2. Moreover, thanks to the reparameterization trick , we can approximate the variable \(_{0}\) by reformulating Eq. (4) to

\[}^{0}=^{_{t}}^{t}- ^{-2_{t}}}_{}(^{t},\,t) , \]

where \(_{}\) is the learned noise prediction network. Then we combine Eq. (8) with Eq. (6) to iteratively construct the sampling process. In addition, it can be proved that the distribution mean in addition, it can be proved that the distribution mean in addition.

Notation NoteRecall that we have two distinct types of timesteps for RL and SDE denoted by \(i\) and \(t\), respectively. To clarify the notation, in the following sections, we use \(^{t}_{i}\) to represent the intermediate variable of an action taken at RL trajectory step \(i\) with SDE timestep \(t\), as \(^{t}_{i}=^{t}\) at state \(_{i}\). Therefore, the action to take for state \(_{i}\) is the final sampled action \(_{i}\) denoted by \(^{0}_{i}\). Hence, the policy is given by

\[_{}(^{0}_{i}_{i})=p_{}(^{0}) \]

While we cannot sample directly from this distribution we can efficiently sample the SDE's reverse joint distribution as

\[p_{}(^{0:T})=p(^{T})_{i=1}^{T}p_{}(^{t-1}^{t}), \]

where \(p(^{T})=(0,)\) is Gaussian noise and the generative process is conditioned on the environment state \(_{i}\). So to take an action from \(_{}(^{0}_{i}_{i})\), we sample from the joint distribution using Eq. (6) and Eq. (8) and finally pick out \(_{0}\) as our sampled action. The visualization of is out method is provided in Appendix A.5.

### Diffusion Policy with Entropy Regularization

The simplest strategy of learning a diffusion policy is to inject Q-value function guidance to the noise matching loss (5), in the hope that the reverse-time SDE (3) would learn to sample actions with higher values. This can be easily achieved by minimizing the following objective:

\[J_{}()=L_{}()-_{_{i} ,^{0}_{i}_{}}[Q_{}(_{i},^{0} _{i})], \]

where \(Q_{}\) is the state-action value function approximated by a neural network, see Section 3.3.

Figure 2: Comparison of the reverse-time SDE and optimal sampling process in data reconstruction.

This combination regards diffusion loss as a behavior-cloning term that learns the overall action distribution from offline datasets. However, the training is limited to existing data samples and the Q-learning term is sensitive to unseen actions. To address it, we propose to add an additional entropy term \(=_{_{i}}[-_{}( _{i})]\) to increase the exploration of the action space during training and rewrite the policy loss (11) to

\[J_{}()=\ L_{}()-\,_{_{i} ,_{i}^{0}_{}}[Q_{}(_{i},_{i}^ {0})-_{}(_{i}^{0}_{i})]. \]

where \(\) is a hyperparameter that determines the relative importance of the entropy term versus Q-values, and \(=\,/\,_{(s,a)}[|Q_{}(s,a)|]\) to normalize the scale of the Q-values and balance loss terms. Iteratively generating the action \(_{i}^{0}\) though a reverse diffusion process is computationally costly but, with an estimated noise \(_{}\) from diffusion term (5), we can thus directly use it to approximate \(_{i}^{0}\) based on Eq. (8) for more efficient training.

Entropy Approximation.It is worth noting that the log probability of the policy \((_{}(_{i}^{0}_{i}))\) is in general intractable in the diffusion process. However, we found that the log probability of the joint distribution in Eq. (10) is tractable when conditioned on the sampled action \(_{i}^{0}\). Proposition 3.1 further shows that the conditional posterior from \(_{i}^{1}\) to \(_{i}^{0}\) is Gaussian, meaning that

\[-_{}(_{i}^{0}_{i})=-_{}(_{i}^{1} _{i})+, \]

where \(\) is a constant and \(_{i}^{1}\) can be approximated using Eq. (2) similar to \(_{i}^{0}\). The proof is provided in Appendix A.4. Then we can focus on the conditional reverse marginal distribution \(p_{}(_{i}^{1}_{i}^{T},_{i})\) that determines the exploration of actions and is acquirable via Bayes' rule:

\[p_{}(_{i}^{1}_{i}^{T},s_{i})=(_{i}^{T} _{i}^{1},_{i})\,p_{}(_{i}^{1}_{i}^{0},_{i})}{p_{}(_{i}^{T}_{i}^{0},_{i})}. \]

Since all terms in Eq. (14) can be computed with Eq. (2), we can rewrite the policy objective as

\[J_{}()=L_{}()-\,_{_{i} ,(_{i}^{0},_{i}^{1})_{}}Q_{}(_{i},_{i}^{0})-(p(_{i}^{1}_{i}^{T},_{i}) ))}, \]

where \(}_{i}^{0}\) and \(}_{i}^{1}\) are approximate values calculated based on samples from the diffusion term. Note that the temperature \(\) usually plays an important role in the maximum entropy RL framework and we thus provide a detailed analysis in Section 4.3.

### Pessimistic Evaluation via Q-ensembles

Entropy regularization encourages diffusion policies to explore the action space, reducing the risk of overfitting pre-collected data. However, in offline RL, since the agent cannot collect new data during training, this exploration can lead to inaccuracies in value estimation for unseen state-action pairs [3; 11]. Instead of staying close to the behavior policy and being overly conservative, considering the uncertainty in the value function is an alternative approach.

In this work, we consider a pessimistic variant of a value-based method to manage the uncertainty and risks, i.e., the lower confidence bounds (LCB) with Q-ensembles. More specifically, we use an ensemble of Q-functions with independent targets to obtain an accurate LCB of Q-values. Each Q-function is updated based on its own Bellman target without sharing targets among ensemble members , as follows:

\[J_{Q}(^{i}) =_{_{i},_{i},r_{i},_{i+1}}[Q_{^{m}}(_{i},_{i})-y^{m}(r_{i},_{i+1},_{})] \] \[y^{m} =r_{i}+_{_{i+1}_{}}[Q_{^{m}}(_{i+1},_{i+1})]\]

where \(^{m},^{m}\) are the parameters of the Q network and Q-target network for the \(m\)th Q-function. Then, the pessimistic LCB values are derived by subtracting the standard deviation from the mean of the Q-value ensemble,

\[Q_{}^{}=_{}[Q_{^{m}}(,) ]-[}[Q_{^{m}}(,)]}] \]

where \( 0\) is a hyperparameter determining the amount of pessimism, \([Q_{^{m}}]\) is the variance of the ensembles, and \(m\{1,,M\}\) where \(M\) the number of ensembles. Then, \(Q_{}^{}\) is used in the policy improvement step to balance entropy regularization and ensure robust performance. Finally, we use \(Q_{}^{}\) as the \(Q_{}\) to (15). We summarize our method in Algorithm 1.

Experiment

In this section, we evaluate our methods on standard D4RL offline benchmark tasks  and provide a detailed analysis of entropy regularization, Q-ensembles, and training stability.

### Setup

DatasetsWe evaluate our approach on four D4RL benchmark domains: Gym, AntMaze, Adroit, and Kitchen. In Gym, we examine three robots (halfcheetah, hopper, walker2d) across sub-optimal (medium), near-optimal (medium-expert), and diverse (medium-replay) datasets. The AntMaze domain challenges a quadrupedal ant robot to navigate mazes of varying complexities. The Adroit domain focuses on high-dimensional robotic hand manipulation, using datasets from human demonstrations and robot-imitated human actions. Lastly, the Kitchen domain explores different tasks within a simulated kitchen. These domains collectively provide a comprehensive framework for assessing RL algorithms across diverse scenarios.

Implementation DetailsFollowing Diffusion-QL , we keep the network structure the same for all tasks with three MLP layers (hidden size 256, Mish activation ), and train models for \(2000\) epochs for Gym and \(1000\) epochs for others. Each epoch consists of 1000 training steps with a batch size of 256. We use Adam  to optimize both SDE and the Q-ensembles. Each model is evaluated by 10 trajectories for Gym tasks and 100 trajectories for others. In addition, our model is trained on an A100 GPU with 40GB memory for about 8 hours per task, and results are averaged over five random seeds.

HyperparametersWe keep key hyperparameters consistent: Q-ensemble size 64, LCB coefficient \(=4.0\). The entropy temperature \(=0.01\) for Gym and AntMaze tasks and automated for Adroit and Kitchen tasks. The SDE sampling step is set to \(T=5\) for Gym and Antmaze tasks, \(T=10\) for Adroid and Kitchen tasks. For'medium' and 'large' datasets of AntMaze, we use max Q-backup following Wang et al.  and Kumar et al. . We also introduce the maximum likelihood loss for SDE training as proposed by Luo et al. . More details are in Appendix B.2.

### Comparison with other Methods

We compare our method with extensive baselines for each domain to provide a thorough evaluation and to understand the contributions of different components in our approach. The most fundamental among these are the behavior cloning (BC) method, BCQ  and BEAR  which restrict the policy to dataset behavior, highlighting the need for policy regularization and exploration. We also assess against Diffusion-QL  which integrates a diffusion model for policy regularization guided by Q-values. This comparison isolates the benefits of our enhanced sampling process and Q-ensemble integration. Our comparison includes CQL  and IQL , known for conservative Q-value updates. Additionally, we consider EDP , a variant of IQL with an efficient diffusion policy, and IDQL , which combines IQL as a critic with behavior cloning diffusion policy reweighted by learned Q-values. These comparisons evaluate the effectiveness of integrating diffusion policies with conservative value estimation. Finally, we include MSG , which combines independent Q-ensembles with CQL, and DT , treating offline RL as a sequence-to-sequence translation problem. These baselines help assess the robustness and generalizability of our method across different approaches. The performance comparison between baselines and ours is reported in Table 1 (Gym, Adroit and Kitchen) and Table 2 (AntMaze). Detailed results are discussed below.

Gym tasksMost approaches perform well on Gym'medium-expert' and'medium-replay' tasks with high-quality data but drop severely on'medium' tasks with suboptimal trajectories. Diffusion-QL  achieves a better performance through a highly expressive diffusion policy. Our method further improves performance across all three'medium' tasks. The results illustrate the efficacy of combining diffusion policy with entropy regularization and Q-ensembles in preventing overfitting to suboptimal behaviors. By maintaining policy stochasticity, our algorithm encourages the exploration of action spaces, potentially discovering better strategies than those in the dataset.

Advit and KitchenMost offline approaches cannot achieve expert performance on these tasks due to the narrowness of human demonstrations in Adroit and the indirect, multitask data in Kitchen . Our method outperforms all other approaches in the Kitchen tasks which suggests its ability to "stitching" the dataset and generalization. In addition, we fix the entropy coefficient \(\) to be the same as other tasks for a robust setting. Even so, our method still achieves a competitive performance in Adroit tasks. This fixed \(\) leads the agent to continuously explore the action space throughout the entire training process, even when encountering unseen states. While exploration is generally advantageous, it can be detrimental in environments with limited data variability. Additionally, unlike in antmaze tasks, random actions are more likely to negatively impact performance in tasks where precise control is essential like Adroit. Moreover, it's worth noting that slightly tuning \(\) leads to a SOTA performance, as illustrated in Table 3.

   Gym Tasks & BC & DT & CQL & IQL & IDQL-A & IQL+EDP & Diff-QL & Ours \\  Halfcheetah-medium-v2 & 42.6 & 42.6 & 44.0 & 47.4 & 51.0 & 48.1 & 51.1 & **54.9** \\ Hopper-medium-v2 & 52.9 & 67.6 & 58.5 & 66.3 & 65.4 & 63.1 & 90.5 & **94.2** \\ Walker2d-medium-v2 & 75.3 & 74.0 & 72.5 & 78.3 & 82.5 & 85.4 & 87.0 & **92.5** \\ Halfcheetah-medium-replay-v2 & 36.6 & 36.6 & 45.5 & 44.2 & 45.9 & 43.8 & 47.8 & **57.0** \\ Hopper-medium-replay-v2 & 18.1 & 82.7 & 95.0 & 94.7 & 92.1 & 99.1 & 101.3 & **102.7** \\ Walker2d-medium-replay-v2 & 26.0 & 66.6 & 77.2 & 73.9 & 85.1 & 84.0 & **95.5** & 94.2 \\ Halfcheetah-medium-expert-v2 & 55.2 & 86.8 & 91.6 & 86.7 & 95.9 & 86.7 & **96.8** & 90.3 \\ Hopper-medium-expert-v2 & 52.5 & 107.6 & 105.4 & 91.5 & 108.6 & 99.6 & 111.1 & **111.9** \\ Walker2d-medium-expert-v2 & 107.5 & 108.1 & 108.8 & 109.6 & **112.7** & 109.0 & 110.1 & 111.2 \\ 
**Average** & 51.9 & 74.7 & 77.6 & 77.0 & 82.1 & 79.9 & 88.0 & **89.9** \\   Adroit Tasks & BC & BCQ & BEAR & CQL & IQL & IQL+EDP & Diff-QL & Ours \\  Pen-human-v1 & 63.9 & 68.9 & -1.0 & 37.5 & 71.5 & 72.7 & **72.8** & 70.0 \\ Pen-cloned-v1 & 37.0 & 44.0 & 26.5 & 39.2 & 37.3 & **70.0** & 57.3 & 68.4 \\ 
**Average** & 50.5 & 56.5 & 12.8 & 38.4 & 54.4 & **71.4** & 65.1 & 69.2 \\   Kitchen Tasks & BC & BCQ & BEAR & CQL & IQL & IQL+EDP & Diff-QL & Ours \\  kitchen-complete-v0 & 65.0 & 8.1 & 0.0 & 43.8 & 62.5 & 75.5 & 84 & **92.7** \\ kitchen-partial-v0 & 38.0 & 18.9 & 13.1 & 49.8 & 46.3 & 46.3 & 60.5 & **66.3** \\ kitchen-mixed-v0 & 51.5 & 8.1 & 47.2 & 51 & 51 & 56.5 & 62.6 & **68.0** \\ 
**Average** & 51.5 & 11.7 & 20.1 & 48.2 & 53.3 & 59.4 & 69.0 & **75.7** \\   

Table 1: Average normalized scores on D4RL benchmark tasks. Results of BC, CQL, IQL, and IQL+EDP are taken directly from Kang et al. , and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.

   AntMaze Tasks & BC & DT & CQL & IQL & MSG & IDQL-A & IQL+EDP & Diff-QL & Ours \\  Antmaze-unaze-v0 & 54.6 & 59.2 & 74 & 87.5 & 97.8 & 94.0 & 87.5 & 93.4 & **100** \\ Antmaze-unaze-diverse-v0 & 45.6 & 53.0 & 84.0 & 62.2 & **81.8** & 80.2 & 62.2 & 66.2 & 79.8 \\ Antmaze-medium-play-v0 & 0.0 & 0.0 & 61.2 & 71.2 & 89.6 & 84.5 & 71.2 & 76.6 & **91.4** \\ Antmaze-medium-diverse-v0 & 0.0 & 0.0 & 53.7 & 70.0 & 88.6 & 84.8 & 70.0 & 78.6 & **91.6** \\ Antmaze-large-play-v0 & 0.0 & 0.0 & 15.8 & 39.6 & 72.6 & 63.5 & 39.6 & 46.4 & **81.2** \\ Antmaze-large-diverse-v0 & 0.0 & 0.0 & 14.9 & 47.5 & 71.4 & 67.9 & 47.6 & 56.6 & **76.4** \\ 
**Average** & 16.7 & 18.7 & 50.6 & 63.0 & 83.6 & 79.1 & 63.0 & 69.6 & **86.7** \\   

Table 2: Average normalized scores on D4RL **AntMaze** tasks. Results of BC, DT, CQL, IQL, and IQL+EDP are taken directly from Kang et al. , and all other results are taken from their original papers. Our results are reported by averaging 5 random seeds.

AntMazeAntMaze tasks are more challenging, requiring point-to-point navigation with sparse rewards from sub-optimal trajectories . As shown in Table 2, traditional behavior cloning methods (BC and DT) get 0 rewards on AntMaze medium and large environments. Our method shows excellent performance on all the tasks in AntMaze even with large complex maze settings and outperforms other methods by a margin. The result is not surprising because the entropy regularization incentivizes the policy to explore various sub-optimal trajectories within the dataset and stitch them to find a path toward the goal. In tasks with sparse rewards, this can be crucial because it prevents premature convergence to suboptimal deterministic policies. Additionally, employing the LCB of Q-ensembles effectively reduces the risk of taking low-value actions, enabling the development of robust policies.

In general, employing consistent hyperparameters for each domain, along with fixed entropy temperature \(\), LCB coefficient \(\), and ensemble size \(M\) across all tasks, our method not only achieves substantial overall performance but also outperforms prior works in the challenging AntMaze tasks. By the comparison with MSG (Q-ensemble alone) and Diffusion-QL (Diffusion alone), our method further improves results demonstrating its effectiveness in handling complex environments with sparse rewards by effectively combining suboptimal trajectories to find better solutions via action space exploration.

### Analysis and Discussion

We first study the core components of our method: entropy regularization and Q-ensemble. Then we show that adding both significantly improves the training robustness of diffusion-based policies.

Entropy RegularizationThe core idea of applying entropy regularization in offline RL is to increase the exploration of new actions such that the estimation of Q-functions is more accurate, especially for datasets with unbalanced action distribution such as the toy example in Figure 1. Here we report the results of training the diffusion policy with different entropy temperatures in Table 3. It is observed that our method with positive entropy coefficients performs better than that without the entropy term. In addition, we can extend our model with an automatic entropy adjustment similar to the work in . This approach is marked as "_auto_" in Table 3. The results show that auto-tuning the entropy temperature further improves the performance in the Adroit and Kitchen domains. Please refer to Appendix B.3 for more details.

Q-EnsemblesWe evaluate our method under different numbers of Q networks \(M\{2,4,64\}\) in the AntMaze environment to explore the effectiveness of Q-ensembles. The results with average performance within 5 different seeds are provided in Table 4. The key observations are 1) As the \(M\) increases, the model gets better performance and the training process becomes more stable; 2) The standard deviation in the results decreases as \(M\) increases, suggesting larger ensembles not only perform better on average but also provide more reliable and consistent results. 3) While increasing \(M\) from 2 to 4 shows a substantial improvement, the performance gains decrease with an even larger size. It is worth noting that other offline RL approaches like Diffusion-QL  also adopt two Q networks for training robustness. See Appendix B for more detailed results.

LCB coefficients \(\)We evaluate our method with \(\) values of 1, 2, and 4 on AntMaze-medium environments Figure 3 demonstrates that adjusting the LCB coefficient improves performance, partic

   Ensemble Size & \(2\) & \(4\) & \(64\) \\  Antmaze-medium-play-v0 & \(84.0\) & \(87.2\) & **91.4** \\ Antmaze-medium-diverse-v0 & \(71.8\) & \(87.2\) & **91.6** \\ Antmaze-large-play-v0 & \(54.2\) & \(52.4\) & **81.2** \\ Antmaze-large-diverse-v0 & \(43.2\) & \(69.0\) & **76.4** \\ 
**Average** & \(63.3\) & \(74.0\) & **85.2** \\   

Table 4: Ablation experiments of our entropy-based diffusion policy with different ensemble sizes on selected AntMaze tasks.

   Entropy temperature \(\) & 0 & 0.01 & 0.05 & 0.1 & auto \\  Antmaze-medium-play-v0 & 85.7 & **91.4** & 91 & 88.3 & 92.0 \\ Antmaze-medium-diverse-v0 & 89.0 & 91.6 & 90.7 & **98.5** & 90.8 \\ Antmaze-large-play-v0 & 77.7 & 81.2 & 78.3 & **82** & 82.0 \\ Antmaze-large-diverse-v0 & 73.7 & 76.4 & 71.3 & **78.3** & 76.0 \\  HalfCheeta-Medium-v2 & 53.7 & 54.9 & 54.0 & **55.3** & 54.2 \\ Hopper-Medium-v2 & 94.8 & 94.2 & 93.3 & **97.1** & 94.0 \\ Walker2D-Medium-v2 & 89.6 & **92.5** & 91.6 & 89.9 & 91.9 \\  Pen-human-v1 & 60.9 & 67.2 & 63.6 & 69.8 & **78.5** \\ Pen-cloned-v1 & 57.9 & 66.3 & 61.8 & 56.5 & **79.8** \\  Kitchen-complete-v0 & 80.6 & 82.3 & 77.6 & 54.4 & **84.4** \\ Kitchen-Mixed-v0 & 57.0 & 60.2 & 50.8 & 56.5 & **60.4** \\   

Table 3: Ablation study on entropy temperatures.

ularly for higher values, which helps in managing the exploration-exploitation trade-off effectively. In addition, the numerical results are provided in Appendix Table 8.

Training Stability and Computational TimeEmpirically we observe that the training of diffusion policies is always unstable, particularly for sparse-reward environments such as AntMaze medium and large tasks. Our method alleviates this problem by incorporating the entropy regularization and Q-ensembles as stated in the introduction. Here, we further show the comparison of training Diffusion-QL and our method on four AntMaze tasks as illustrated in Figure 4, maintaining the same number of diffusion steps \(T=5\) for both. It is observed that the performance of Diffusion-QL even drops down as the training step increases, while our method is substantially more stable and achieves higher results throughout all the training processes. We also included a detailed comparison of training and evaluation times for Gaussian and diffusion policies with Q-ensembles in Table 5. Increasing \(M\) from 2 to 64 almost does not influence the evaluation time. The diffusion step \(T\) has more impact on both training and evaluation time which is a common problem in diffusion models.

## 5 Related Work

Generative Diffusion Models and Mean-reverting SDEsRecent advancements have integrated diffusion models [18; 42; 41; 40] and SDEs [43; 32; 45; 39] for realistic generative modeling. The development of Denoising Diffusion Probabilistic Models  showcases the ability of diffusion models to generate high-fidelity images through iterative reverse diffusion processes guided by deep neural networks, achieving state-of-the-art performance in generative tasks. In [33; 45; 39], mean-reverting SDEs are applied to speech processing and image restoration tasks. These SDEs, similar to (1) but with different parameters, ensure our policy adapts across various distributions without bias. The general applicability of our method is demonstrated in 4 D4RL benchmark domains. The comparison between our SDE and  are provided in Appendix A.6.

Figure 4: Learning curves of the Diffusion-QL and our method on selected Antmaze tasks over 5 random seeds.

Figure 3: Ablation experiments of our method with different values of LCB coefficient \(=1,2,4\) on AntMaze-Medium environments over 5 different random seeds.

   Policy & Diffusion Step \(T\) & \# Critics \(M\) & Training Time & Eval Time \\  Gaussian & 1 & 2 & 5m 35s & 1s 450ms \\ Gaussian & 1 & 64 & 7m 20s & 1s 450ms \\  Diffusion & 5 & 2 & 9m 30s & 4s 800ms \\ Diffusion & 5 & 64 & 11m & 4s 800ms \\ Diffusion & 10 & 2 & 12m 23s & 8s \\ Diffusion & 10 & 64 & 13m 55s & 8s \\   

Table 5: Computational time comparison with different settings on Antmaze-medium-play-v0. Training time is for 1 epoch (1000 training steps) and eval time is for 1000 RL steps.

Diffusion Models in Offline RLDiffusion models in offline RL have gained growing attention for their potent modeling capabilities. In Janner et al. , diffusion models are introduced as trajectory planners trained with offline datasets for guided sampling, significantly mitigating compounding errors in model-based planning . Diffusion models are also used as data synthesizers [5; 48], generating augmented training data to enhance offline RL robustness. Additionally, diffusion models approximate behavior policies [44; 21; 17], integrating Q-learning for policy improvement, though this can lead to overly conservative policies.

Entropy RegularizationIn online RL, maximum entropy strategies encourage exploration by maximizing rewards while maintaining high entropy [14; 15]. This approach develops diverse skills  and adapts to unseen goals . However, its application in offline RL is challenging due to the multi-modal nature of datasets from various policies and expert demonstrations.

Uncertainty MeasurementBalancing exploration and exploitation is crucial when data is limited. Online RL methods like bootstrapped DQN  and Thompson sampling  estimate uncertainty for exploration guidance. In offline RL, handling uncertainty is critical due to the lack of environment interaction. Model-based methods like MOPO  and MORel  measure and penalize uncertain model dynamics. Similarly, model-free methods like EDAC  and MSG  use Q-network ensembles to obtain pessimistic value estimations for policy guidance.

## 6 Conclusion

In this work, we present an entropy-regularized diffusion policy for offline RL, introducing mean-reverting SDEs as the base framework to provide tractable entropy. Our theoretical contributions include deriving an approximated entropy for a diffusion model, enabling its integration as an entropy regularization component within the policy loss function. We also propose an optimal sampling process, ensuring the fast convergence of action generation from diffusion policy. Additionally, we enhance our method by incorporating Q-ensembles to handle the data uncertainty. Our experimental results show that combining entropy regularization with the LCB approach leads to a more robust policy, achieving state-of-the-art performance across offline RL benchmarks, particularly in AntMaze tasks with sparse rewards and suboptimal trajectories.

Future WorkWhile the proposed method performs well on most D4RL tasks, the diffusion policy requires longer time when executed on compute- and power-constrained devices. Our future work will investigate real-time policy distillation under time and compute constraints to address this challenge.