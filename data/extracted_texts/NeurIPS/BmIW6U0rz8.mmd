# Koopman Kernel Regression

Petar Bevanda

TU Munich

petar.bevanda@tum.de &Max Beier

TU Munich

max.beier@tum.de &Armin Lederer

TU Munich

armin.lederer@tum.de &Stefan Sosnowski

TU Munich

sosnowski@tum.de &Eykie Hullermeier

LMU Munich

eyke@ifi.lmu.de &Sandra Hirche

TU Munich

hirche@tum.de

###### Abstract

Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging. Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear time-invariant (LTI) ODEs, turning multi-step forecasts into sparse matrix multiplication. Though there exists a variety of learning approaches, they usually lack crucial learning-theoretic guarantees, making the behavior of the obtained models with increasing data and dimensionality unclear. We address the aforementioned by deriving a universal Koopman-invariant reproducing kernel Hilbert space (RKHS) that solely spans transformations into LTI dynamical systems. The resulting _Koopman Kernel Regression (KKR)_ framework enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than existing work. Our experiments demonstrate superior forecasting performance compared to Koopman operator and sequential data predictors in RKHS.

## 1 Introduction

Dynamical systems theory is a fundamental paradigm for understanding and modeling the time evolution of a phenomenon governed by certain underlying laws. Such a perspective has been successful in describing countless real-world phenomena, ranging from engineering mechanics  and human movement modeling  to molecular and quantum systems . However, as the laws governing dynamical systems are often unknown, modeling and understanding the underlying phenomena may have to rely on data rather than first principles. In this regard, machine learning methods, which have shown immense potential in tackling complex tasks in domains such as language models  and computer vision , are coming to the fore. Though powerful, state-of-the-art neural vector fields  or flows  commonly compose highly nonlinear maps for forecast, i.e. computing

\[x(t)=x(0)+_{0}^{t}f(x(t))t \]

for, e.g. a scalar ODE \(=f(x)\). Hence, it is often challenging to use such models in optimization-based decision making that relies on simulators or predictive models, e.g., reinforcement learning . A particularly beneficial perspective for dealing with the aforementioned comes from Koopman operator theory . Through a point-spectral decomposition of Koopman operators, forecasts become superpositions of solution curves of a set of linear ODEs \(\{_{j}=_{j}z_{j}\}_{j=1}^{D}\)

\[x(t)=_{k=1}^{D}^{_{j}t}\,z_{j}(0),\{x g_{j}}{}z_{j}\}_{j=1}^{D} \]where a vector valued function \((\{g_{j}\}_{j=1}^{D})\) "lifts" \(x\) onto a manifold \(:=\{z_{j}\}_{j=1}^{D}\). Throughout, we refer to these models as _linear time-invariant (LTI) predictors_. The learning objective of such representations is twofold: spanning system trajectories by the learned manifold \(\) and constraining the LTI dynamics to it. The latter is a long-standing challenge of Koopmanism , as manifold dynamics of existing approaches "leak-out"  and limit predictive performance.

To tackle the aforesaid, we connect the representation theories of reproducing kernel Hilbert spaces (RKHS) and Koopman operators. As a first in the literature, we derive a universal kernel whose RKHS exclusively spans manifolds invariant under the dynamics, as depicted in Figure 1. A key corollary of unconstrained manifold dynamics is the lack of essential learning-theoretic guarantees, making the behavior of existing learned models unclear for increasing data and dimensionality. To address this, we utilize equivalences to function regression in RKHS to formalize a statistical learning framework for learning LTI predictors from sample trajectories of a dynamical system. This, in turn, enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than before . Thus, we believe that our Koopman Kernel Regression (KKR) framework takes the best of both RKHS and Koopmanism by leveraging modular kernel learning tools to build provably effective LTI predictors.

The remainder of this paper is structured as follows: We briefly introduce LTI predictors and discuss related work in Section 2. The derivation of the KKR framework, including the novel Koopman RKHS, is presented in Section 3. In Section 4, we show the novel learning guarantees in terms of convergence and generalization error bounds. They are validated in comparison to the state-of-the-art through numerical experiments in Section 5.

**Notation**: Lower/upper case bold symbols \(/\) denote spatial vector/matrix-valued quantities. A _trajectory_ defines a curve \(_{T}\) traced out by the flow over time \([0,T]\) from any \((,)\). In discretizing \(\), collection of points \(_{}\) from discrete time steps \(\{t_{0} t_{}\}\) is considered. The state/output trajectory spaces are denoted as \(_{T}{}L^{2}(,)\) / \(_{T}{}L^{2}(,)\), with discrete-time analogues \(_{}{}^{2}(,)\) / \(_{}{}^{2}(,)\) with domain and co-domain separated by ",". The vector space of continuous functions on \(_{T}\) endowed with the topology of uniform convergence on compact domain subsets is denoted \(C(_{T})\). The collection of bounded linear operators from \(_{T}\) to \(_{T}\) is denoted as \((_{T})\). The adjoint of \(()\) is \(^{*}\). Discrete-time eigenvalues read \(^{ t},\). A random variable \(X\) defined on a probability space \((,,)\) has expectation \([X]=_{}X()()\).

## 2 Problem Statement and Related Work

To begin, we formalize our problem statement and put our work into into context with existing work.

### Problem Statement

Consider a forward-complete system1 comprising a nonlinear state-space model

\[} =(),\ _{0}{=}(0), \] \[y =q(), \]

on a compact domain \(^{d}\) with a quantity of interest \(q{}C()\). The above system class includes all systems with Lipschitz flow \(^{t}(_{0}):=_{0}^{t}(())d\), e.g., mechanical systems .

Inspired by the spectral decomposition of Koopman operators, we look to replace the nonlinear state-space model (3) by an _LTI predictor_

\[} =,\ _{0}{=}(_{0}), \] \[y =^{}, \]

Figure 1: Illustration of on-manifold dynamics of LTI predictors.

with \(\) and \(\) a \(\)-dimensional function approximator dense in \(C()\). Then, from initial conditions \(_{0}\) that form a _non-recurrent_ domain \(_{T}\), (4) admits a universal approximation of the flow of (3) such that \(>0,\ \) so that \(_{_{0}}\|y_{T}(t)-^{}^{t}\, ()|<,\  t[0,T]\)2. In this work, we aim to find a solution to the following constrained, functional optimization problem

**(OR)**: _Output reconstruction:_

\[_{,,}\|y_{T}-^{}_{T}\|_{_{T}}, \]
**(KI)**: _Koopman-invariance:_

\[((t))=^{t}\,((0) ),\ \ \ \ \  t[0,T]. \]

Although the sought-out model (4) is simple, the above problem is non-trivial and much of the existing body of work utilizes different simplifications that often lead to undesirable properties. In the following, we elaborate on these properties and motivate our novel sample-based solution to (5), which remains relatively simple but nonetheless ensures a well-defined solution with strong learning guarantees.

### Related Work

**Koopman operator regression in RKHS** Equipped with a rich set of estimators, operator regression in RKHS seeks a sampled-data solution to

\[_{}\|((t))-^{}\!((0))\|_{L^{2}}, \]

with \(\) a Hilbert-Schmid operator  -- commonly known as KRR, and EDMD (PCR) or RRR when under different fixed-rank constraints [23; 24]. The choice of RKHS \(\) is commonly one that is dense in a suitable \(L^{2}\) space, e.g. that of the RBF kernel. By an additional projection, a quantity of interest can be predicted via a mode decomposition of the estimated operator, leading to a model akin to (4). In the light of (5), the feature map \(\) is predetermined while violating **(KI)** is merely minimized for a single time-instant \(t\). As a consequence, such approaches are oblivious to the time-series structure -- offering limited predictive power over the time interval \([0,T]\) of a trajectory as displayed in Figure 1. The extent to which **(KI)** is violated due to spectral properties  or estimator bias  is known as spectral pollution . The strong implications of this phenomena, motivate regularization  and spectral bias measures  to reduce its effects. Due to the above challenges, guarantees for Koopman operator regression (KOR) have only recently gained increased attention. Often, however, existing theoretical results [29; 33] are generally not applicable to nonlinear dynamics  due to the commonly unavoidable misspecification  of the problem (6) incurred by neglecting **(KI)**. The first more general statistical learning results [23; 24] are derived in a stochastic setting under the assumption that the underlying operator is compact and self-adjoint. In stark contrast, the same set of assumptions is restrictive for the deterministic setting : compactness only holds for affine deterministic dynamics [36; 37] while self-adjointness is known to generally not hold for Koopman operators [13; 38; 39]. Regardless of the setting, however, the state-of-the-art exhibits alarming properties: forecasting error not necessarily vanishing with LTI predictor (4) rank [23; Theorem 1] and risk based on a single time-instant.

**Learning via Koopman eigenspaces** Geared towards LTI predictors and closer to our own problem setting (2), another distinct family of approaches aims to directly learn the operator's invariant subspaces [40; 41; 28; 42; 43; 44; 42]. The goal is to fit \(()\) based on approximate Koopman operator eigenfunctions that still fit the output of interest **(OR)**. However, existing data-driven approaches in this line of work rely on ad-hoc choices and lack essential learning-theoretic properties such as feasibility and uniqueness of solutions -- prohibiting provably accurate and automated LTI predictor learning.

**Kernels for sequential data** Motivated by the lack of priors that naturally incorporate streaming and sequential data, there is an increasing interest in _signatures_. They draw from the rich theory of controlled differential equations (CDEs) [44; 45] and build models that depend on a time-varying observation history. An RKHS suitable for sequence modeling is induced by a signature transformation of a base/static RKHS. Generally, if the latter is universal, so are the signature kernels . While arguably more general and well-versed for discriminative and generative tasks , forecasting using signature kernels  comes at a price, as their nonlinear dependence on observation streams leads to a significant complexity increase compared to LTI predictors.

Motivated by the restrictions of existing Koopman-based predictors, we propose a _function approximation_ approach that exploits exploits time-series data and Koopman operator theory to provably learn LTI predictors. Through a novel _invariance transform_ we can satisfy **(KI)** by construction and directly minimize the forecasting risk over an entire time-interval (**OR**). In simple terms: Koopman operator regression fixes \(()\) and regresses \(\) and \(\) in (5), whereas our KKR approach selects \(\) to jointly regress \(\) and \(()\). Similar in spirit to generalized Laplace analysis [21; 49], our approach allows the construction of eigenmodes from data without inferring the operator itself. Crucially, we demonstrate that selecting \(\) requires no prior knowledge as confirmed by our theoretical results and experiments. To facilitate learning LTI predictors, we derive _universal_ RKHSs that are _guaranteed_ to satisfy **(KI)** over trajectories -- a first in the literature. The resulting equivalences to function regression in RKHS allow for more general and complete learning guarantees in terms of consistency and risk bounds that are free of restrictive operator-theoretic assumptions.

## 3 Koopman Kernel Regression

With the optimization (5) being prohibitively hard due to nonlinear and possibly high dimensional constraints, we eliminate the constraints (5b) by enforcing the feature map \(()\) to have the dynamics of intrinsic LTI coordinates associated with Koopman operators, i.e., their (open) eigenfunctions .

**Definition 1**.: _A Koopman eigenfunction \(_{}\!\!\!C()\) satisfies \(_{}()\!=\!^{- t}\,_{}(^{t} ()), t[0,T]\)._

It is proven that Koopman eigenfunctions from Definition 1 are universal approximators of continuous functions  -- making them a viable replacement for the feature map \(()\) in (4). However, following their definition, it is evident that Koopman eigenfunctions are by no means arbitrary due to their inherent dependence on the dynamics' flow. Using the well-established fact that Koopman operators compose a function with the flow, i.e., \(^{t}g()=g(^{t}())\), it becomes evident the eigenfunctions from Definition 1 are (semi)group invariants, as they remain unchanged after applying \(\{^{- t}\,^{t}\}_{t=0}^{T}\). Thus, inspired by the seminal work of Hurwitz on constructing invariants , we can equivalently reformulate (5) as an unconstrained problem and jointly optimize over eigenfunctions3.

**Lemma 1** (Invariance transform).: _Consider a function \(g C(_{0})\) over a set of initial conditions \(_{0}\) that form a non-recurrent domain \(_{T}\). The invariance transform \(_{T}^{}\) transforms \(g\) into an Koopman eigenfunction \(_{} C(_{T})\) for (3a) with LTI dynamics described by \(\)_

\[_{}(_{T})=_{}^{T}g(_{0}):=_{= 0}^{T}^{-(-t)}\,g(^{}(_{0}))d. \]

The above Lemma 1 is a key stepping stone towards deriving a representer theorem for LTI predictors. However, it is also interesting in its own right as it provides an explicit expression for the flow of an eigenfunction from any point in the state space. Thus, it provides a recipe to obtain a function space that fulfills **(KI)** by construction. As we show in the following, a sufficiently rich set of eigenvalues  and Lemma 1 will allow for a reformulation of (5) into an unconstrained problem

\[_{M}\|y_{T}-M(_{T})\|_{_{T}}. \]

where the operator \(M()\!\!:=\!\!^{}[_{_{1}}()_{_ {D}}()]^{}\) is universal and consisting of Koopman-invariant functions.

### Functional Regression Problem

Notice that the problem reformulation (8) is still intractable, as a closed-form expression for the flow map is generally unavailable even for known ODEs. This requires integration schemes that can introduce inaccuracies over a time interval \([0,T]\). Thus, to make the above optimization problem tractable, data samples are used -- ubiquitous in learning dynamical systems.

**Assumption 1**.: _A collection of \(N\) pairs of trajectories \(_{N}\!=\!\{_{T}^{(i)},y_{T}^{(i)}\}_{i=1}^{N}\!\!( _{T}\!\!_{T})^{N}\) is available._

By aggregating different invariance transformations (7) into the _mode decomposition operator_

\[M()\!\!\!_{j=1}^{}\!_{_{j}}()\,\,\,_{T}_{T}, \]we can formulate a supervised learning approach in the following.

**Learning Problem** With Assumption 1 and Lemma 1, the sample-based approximation of problem (8) reduces to solving

\[_{M}_{i=1}^{N}\|y_{T}^{(i)}-M(_{T}^{(i)})\|_{_{T}}. \]

while preserving the mode decomposition structure (9). To realize the above learning problem, we resort to the theory of reproducing kernels  and look for an operator \(\), where \(\) is an RKHS. A well-established approach using RKHS theory is to select \(\) as a solution to the _regularized least squares problem_

\[=*{arg\,min}_{M}_{i=1}^{N}\|y_{T}^{(i)}- M(_{T}^{(i)})\|_{_{T}}^{2}+\|M\|_{}^{2}, \]

with \(\!\!_{+}\) and \(\|\|_{}\) a corresponding RKHS norm. As our target is a function-valued mapping \(M()\) - an operator - \(\|\|_{}\) is induced by an _operator_-valued kernel \(K\!:_{T}\!\!_{T}\!\!(_{ T})\) mapping to the space of bounded operators over the output space . The salient feature of the above formulation (11) is its well-posedness: its solution exists and is unique for any \(\), expressed as

\[()\!=\!_{i=1}^{N}K(,_{T}^{(i)})_{i}, _{i}_{T} \]

through a representer theorem . Still, due to the Koopman-invariant structure (9) from Lemma 1, the choice of the RKHS \(\) for \(\) is not arbitrary. Thus, the question is how to craft \(\) so the solution \(\) is decomposable into Koopman operator eigenfunctions (9), forming an _LTI predictor_.

Firstly, it is obvious that (9) consists of summands that may lie in different RKHS, denoted as \(\{^{_{j}}\}_{j=1}^{D}\). Then, \(\) is constructed from the following direct sum of Hilbert spaces :

\[}=^{_{1}}^{ _{D}}\ \ \ \ =( )\!\!:=\!\!\{f_{1}\!+\!\!+\!f_{D}:f_{1}\!\!\!^{ _{1}},,f_{D}\!\!\!^{_{D}}\} \]

with \(:},(f_{1} f_{D}) f_{1 }++f_{D}\) the summation operator . Thus, to construct \(\), a specification of the RKHS collection \(\{^{_{j}}\}_{j=1}^{D}\) is required, so that it represents Koopman eigenfunctions from (9).

**Theorem 1** (Koopman eigenfunction kernel).: _Consider trajectory data \(\{_{T}^{(i)}\}_{i=1}^{N}\) from Assumption 1, a \(\) and a universal (base) kernel \(k\!:\!\!\!\). Then, the kernel \(K^{}:_{T}\!\!_{T}(_{T})\)_

\[K^{}(_{T},_{T}^{})=_{=0}^{T}_{^{ }=0}^{T}^{-(-t)}k(_{T}(),_{T}^{}(^{}))^{-^{*}(^{ }-t)}d d^{}, \]

1. _defines an RKHS_ \(^{}\)_,_
2. _is universal for every eigenfunction of Definition_ 1 _corresponding to_ \(\)_,_
3. _induces a data-dependent function space_ \(\{K^{}(,_{T}^{(1)}),,K^{ }(,_{T}^{(N)})\}\) _that is Koopman-invariant over trajectory-data_ \(\{_{T}^{(i)}\}_{i=1}^{N}\)_._

In Theorem 1, we derive an eigenfunction RKHS by defining its corresponding kernel that embeds the invariance transformation (7) over data samples. Also, we would like to highlight that the above result addresses a long-standing open challenge in the Koopman operator community , i.e., defining universal function spaces that are guaranteed to be Koopman-invariant. Now, we are ready to introduce the _Koopman kernel_ as the kernel obtained by combining "eigen-RKHS" as described in (13).

**Proposition 1** (Koopman kernel).: _Consider trajectory data \(_{N}\) of Assumption 1 and a set of kernels \(\{K^{_{j}}\}_{j=0}^{}\) from Theorem 1. Then, the kernel \(K:_{T}\!\!_{T}(_{T})\) given by_

\[K(_{T},_{T}^{})=_{j=1}^{}K^{_{j}}(_{ T},_{T}^{}) \]

1. _defines an RKHS_ \(:=(^{_{1}}^{_{D}})\)_,_
2. _is universal for any output (_3b_), provided a sufficient amount_4 _of eigenspaces_ \(\)_._ 
Above, we have derived the "Koopman-RKHS" \(\) for solving the problem (11) with a universal RKHS spanning Koopman eigenfunctions. Thus, the sample-data solution for an eigenfunction flow follows from the functional regression problem (11) and takes the form \(_{_{j}}()=_{i=1}^{N}K^{_{j}}(,_{T}^{(i)} )_{i},\ _{i}\!\!_{T}\) -- providing a basis for the LTI predictor.

[MISSING_PAGE_FAIL:6]

### Selecting Eigenvalues

Until now, we have used the sufficient cardinality \(\) of an eigenvalue set that encloses  or is the true spectrum. However, we have provided no insight regarding the selection of \(\) spectral components or how they can be estimated. Here, we go beyond the learning-independent and non-constructive existence result of  and provide a consistency guarantee and relate it to sampling eigenvalues without the knowledge of the true spectrum.

**Proposition 3**.: _Consider the oracle Koopman kernel \((_{},_{^{}})\) and a dense set \(\{_{j}\}_{j=1}^{}\) in \(_{1}()}\). Then, \(\|(_{},_{^{}})-_{j=1}^{D} ^{_{j}}(_{},_{^{}})\|_{( _{})} 0\), \(\ _{},_{^{}}_{ }\) as \(D\)._

As shown in Proposition 3, even if we do not know the _oracle_ kernel, we can arbitrarily approximate it by sampling from a dense set supported on the closed complex unit disk \(_{1}()}\)[57, Theorem 3.0.2] with the error vanishing in the limit \(D\!\!\). There is no loss of generality when considering the unit disk as any finite radius disk can be scaled in the interval \([0,T]\). Furthermore, approximation of the oracle kernel by sampling a distribution over \(_{1}()}\) leads to an almost sure \((}{{}})\) convergence rate. It is conceivable that faster rates can be obtained in practice by including prior knowledge to shape the spectral distribution, e.g. using well-known concepts such as leverage-scores or subspace orthogonality . Based on spectral priors one can include a more biased sampling technique by precomputing components of the operator spectrum, e.g. computing Fourier averages , to determine the phases \(_{j}\) of complex-conjugate pairs \(_{j,}=|_{j}|^{_{j}}\) and sample the modulus from another physics-informed distribution. However, rigorous considerations of optimized and efficient sampling are beyond the scope of this paper and rather a topic of future work.

### Numerical Algorithm and Time-Complexity

```
Data \(\!=\!\{_{}^{(i)},y_{}^{(i)}\}_{i=1}^{N}\), Eigenvalues \(\{_{j}\}_{j=1}^{D}\) functionRegress(\(\), \(\{_{j}\}_{j=1}^{D}\))  form Gramians \(k_{_{}_{}}\{k_{_{}_{ }}^{}\}_{j=1}^{D},_{_{}_{}}\) fit mode operator \(()_{}_{}\) (18, right) recover eigenfunctions \(}()_{0}_{0}\) (18, left) construct \(_{0}_{}\) (20, right) return\(LTI\) predictor \(}()_{0}_{ }\) endfunctionfunctionForecast(\(_{0}\)) "lift" \(_{0}=}(_{0})\)  rollout \(_{H}=_{0}\) return trajectory \(_{H}\) endfunction
```

**Algorithm 1** Regression and LTI Forecasts using KKR

For a better overview, the pseudocode for regression and forecasting of our method are shown in Algorithm 1. We also put the time-complexity of our algorithm into perspective w.r.t. Koopman operator regression of PCR/RRR  and ridge regression using state-of-the-art signature kernels  (RR-Sig-PDE) in Table 1. The training complexity of our KKR is comparable to that of RR-Sig-PDE regression and generally better than that of PCR/RRR. Given that accurate LTI forecasts require higher-rank predictors, the seemingly mild quadratic dependence makes \(D^{2}>NH\) and leads to a more costly matrix inversion. Furthermore, our LTI predictor also has a slightly better forecast complexity due to non-linear map, LTI predictors have a significantly lower evaluation complexity than the nonlinear predictor of Sig-PDE's. Due to requiring updated observation sequences as inputs, Sig-PDE kernels introduce a raw evaluation complexity that is also quadratic in sequence length.

## 4 Learning Guarantees

With a completely defined KKR estimator, we assess its essential learning-theoretic properties, i.e., the behavior of the learned functions w.r.t. to the ground truth with increasing dataset size.

### Consistency

Although well-established in most function approximation settings [65; 66; 67], the setting of Koopman-based LTI predictor learning for nonlinear systems is void of consistency guarantees. Here we use a definition of universal consistency from  that describes the uniform convergence of the learned function to the target function as the sample size goes to infinity for any compact input space \(\) and every target function \(q\!\!C()\). The existing convergence results for Koopman-based LTI predictors  are in the sense of strong operator topology -- allowing the existence of empirical eigenvalues that are not guaranteed to be close to true ones even with increasing data . This lack of spectral convergence has a cascaded effect in Koopman operator regression as, in turn, the convergence of eigenfunctions and mode coefficients is not guaranteed. Here, the convergence of modes is replaced by the convergence of eigenfunctions, and convergence of spectra is replaced by the convergence of (20) to the mode decomposition operator \(} M\!\! \) with the estimate denoted by \(()\).

**Theorem 2** (Universal consistency).: _Consider a universal kernel \(\) (17) and a data distribution supported on \(_{}\!\!_{}\). Then, as \(N\), \(\|M\!-\!\|_{_{}} 0\) and \(\|_{_{j}}\!-\!_{_{j}}\|_{_{}} 0, j \!=\!1,,D\)._

### Generalization Gap: Uniform Bounds

Due to formulating the LTI predictor learning problem as a function regression problem in an RKHS, we can utilize well-established concepts from statistical learning to provide bounds on the generalization capabilities of KKR. Given a dataset of trajectories, the following _empirical risk_ is minimized

\[}_{N}():=_{i[N]}\|y_{}^{(i) }-(_{}^{(i)})\|_{_{}}^{2}\]

which is "in-sample" mean square error (MSE) w.r.t. a trajectory-data generating distribution \(_{}\) of i.i.d. initial conditions. The _true risk_/generalization error of an estimator is the "out-of-sample" MSE of the model on the entire domain and denoted as \(()\). Those quantities are, in essence, the model's performance on test and training data, respectively. Allowing for statements on the test performance with an increasing amount of data by means of training performance is a desirable feature in data-driven learning. Hence, we analyze our model in terms of the _generalization gap_

\[|()-}_{N}()|=|_{( _{},y_{})_{}}[\|y_{ }\!-\!(_{})\|_{_{ }}^{2}]-_{i=1}^{N}\|y_{}^{(i)}\!-\!( _{}^{(i)})\|_{_{}}^{2}|. \]

To ensure a well-specified problem, we require models in the hypothesis to admit a bounded norm.

**Assumption 2** (Bounded RKHS Norm).: _The unknown function \(M\) has a bounded norm in the RKHS \(^{ t}\) attached to the Koopman kernel \((,)\), i.e., \(\|M\|_{t^{ t}} B\) for some \(B_{+}\)._

The above smoothness assumption is mild, e.g., satisfied by band-limited continuous trajectories  and computable from data [72; 73]. In stark contrast, well-specified Koopman operator regression  requires the operator to map the RKHS onto itself, which is a very strong assumption [34; 35].

To derive the main result of this section, we utilize the framework of Rademacher random variables for measuring complexity of our model's hypothesis space, a concept generally explored in  and more particularly for classes of operator-valued kernels in . Conveniently, the derivation is, in terms of the RKHS \(^{ t}\), similar to standard methods on RKHS-based complexity bounds . We use well-known results based on concentration inequalities to provide high probability bounds on a model's generalization gap in terms of those complexities. Finally, we upper bound any constant with quantities specified in our assumptions and can state the following result.

**Theorem 3** (Generalization Gap of KKR).: _Let \(_{N}^{ t}=\{_{}^{(i)},y_{}^ {(i)}\}_{1=1}^{N}\) be a dataset as in Assumption 1 consistent with a Lipschitz system on a non-recurrent domain. Then the generalization gap (21) of a model \(\) from Proposition 2 under Assumption 2 is, with probability \(1-\), upper bounded by_

\[|()-}_{N}()| 4RB }{N}}+}{N}}(}), \]

_where \(R\) is an upper bound on the loss in the domain, and \(\) the supremum of the base kernel._

We observe an overall dependence of order \((}{{}})\) w.r.t. data points, resembling the regular Monte Carlo rate to be expected when working with Rademacher complexities. Remarkably, an increase in the order of the predictor \(D\) cannot widen the generalization gap but will eventually decrease theempirical risk due to the consistency of eigenspaces (Proposition 3). Combined, our findings are a substantial improvement, both quantitatively and in terms of interpretability, over existing risk bounds on forecasting error [23, Theorem 1]. Additionally, our intuitive non-recurrence requirement is easily verifiable from data. In contrast, the Koopman operator regression in RKHS comes with various strong assumptions  that require commonly unavailable expert knowledge. Also, the generalization of existing Koopman-based statistical learning approaches depends on rank while ours is rank-independent. The significant implications of our results are demonstrated in the following.

## 5 Numerical Experiments

In our experiments6, we report the squared error of the forecast vector for the length of data trajectories averaged over multiple repetitions with corresponding min-max intervals. We validate our theoretical guarantees and compare to state-of-the-art operator and time-series approaches in RKHS. For fairness, the same kernel and hyperparameters are chosen for our KKR, PCR (EDMD), RRR  and regression with signature kernels (Sig-PDE) . Note, PCR and RRR are provided with the same trajectory data split into one-step data pairs while the time and observation time-delays are fed as data to the Sig-PDE regressor due to its recurrent structure. Along with code for reproduction of our experiments, we provide a JAX reliant Python module implementing a sklearn compliant KKR estimator at [https://github.com/TUM-ITR/koopcore](https://github.com/TUM-ITR/koopcore).

**Bi-stable system** Consider an ODE \(=ax+bx^{3}\) that arises in modeling of nonlinear friction. The parameters are \(a=4\), \(b=-4\), making for a bi-stable system at fixed points \( 1\). The numerical results are depicted in Figure 2. Sample trajectories both on training and testing data indicate the utility of the forecast risk minimization of KKR. While EDMD correctly captures the initial trend of most trajectories it fails to match the accuracy of Sig-PDE or our KKR predictors that utilize time-series structure. Furthermore, the behavior of KKR's generalization gap for an increasing time horizon \(T=H t, t=1/14s\) closely matches our theoretical analysis.

**Van der Pol oscillator** Consider an ODE \(=(2-10x^{2})-0.8x\) describing a dissipative system whose nonlinear damping induces a stable limit cycle -- a phenomenon present in various dynamics. In Figure 3 two fundamental effects are validated: the generalization gap with increasing data and consistency with test risk that does not deteriorate for increasing eigenspance cardinality. The performance of PCR/RRR is strongly tied to predictor rank while Sig-PDE's less so w.r.t. delay length. Crucially, our KKR approach does not require a careful choice of the eigenspace cardinality to perform for a specific amount of data. Although the eigenvalues that determine the eigenspaces are randomly chosen from a uniform distribution in the unit ball, KKR consistently outperforms PCR/RRR. In Table 2 we show the spectral sampling and hyperparameter effects. We employ the following strategies: _uniform_ - uniform distribution on the complex unit disk, _boundary-biased_ - a distribution on the complex unit disk skewed towards the unit circle, _physics-informed_ - eigenvalues of various vector field Jacobians. As expected, physics-informed performs well with lower rank

 \(()\) & uniform & boundary-biased & physics-informed \\  \(D\) & 16 & 200 & 16 & 200 & 16 & 200 \\  \(_{=10^{1}}\) & 13.7 & **5.38** & 11.2 & **5.38** & **5.60** & 5.58 \\ \(_{=10^{0}}\) & 6.46 & **0.78** & 4.10 & **0.78** & 0.97 & 0.92 \\ \(_{=10^{-1}}\) & 7.12 & **1.74** & 4.33 & **1.74** & 1.83 & 1.80 \\  to uninformed approaches. However, it is outperformed by unit-ball sampling approaches for higher rank due to a lack of coverage. Table 3 includes CPU timings for completeness.

Flow past a cylinderWe consider high-dimensional data of velocity magnitudes in a Karman vortex street under varying initial cylinder placement, as illustrated in Figure 5. The cylinder position is varied on a \(7{}7\) grid in a \(50{}100\)-dimensional space and the flow is recorded over \(H{=}99\). The quantity of interest is a velocity magnitude sensor placed in the wake of the cylinder. In forecasting from an initial velocity field, KKR outperforms PCR by orders-of-magnitude as shown in Figure 4. We omit Sig-PDE regression due to persistent divergence after \(\) 20 steps. The latter is hardly surprising, given that Sig-PDE models iterate one step predictions based only on the shapes of time-delays while LTI models directly output time-series from initial conditions.

## 6 Conclusion

We presented a novel statistical learning framework for learning LTI predictors using trajectories of a dynamical system. The method is rooted in the derivation of a novel RKHS over trajectories, which solely consists of universal functions that have LTI dynamics. Equivalences with function regression in RKHS allow us to provide consistency guarantees not present in previous literature. Another key contribution is a novel rank-independent generalization bound for i.i.d. sampled trajectories that directly describes forecasting performance. The significant implications of the proposed approach are confirmed in experiments, leading to superior performance compared to Koopman operator and sequential data predictors in RKHS. In this work, we confined our forecasts to a non-recurrent domain for a specific length of trajectory data, where the choice of spectra is arbitrary. However, exploring more efficacious spectral sampling schemes is a natural next step for extending our results to asymptotic regimes that include, e.g., periodic and quasi-periodic behavior. It has to be noted that vector-valued kernel methods have limited scalability with a growing number of training data and output dimensionality. Therefore, exploring solutions that improve scalability is an important topic for future work. Furthermore, to enable the use of LTI predictors in safety-critical domains, the quantification of the forecasting error is essential. Hence, deriving uniform prediction error bounds for KKR is of great interest.

Figure 4: Cumulative error and forecast risks (5 train-test splits) for flow past cylinder data and \(H=99\). Our KKR with orders-of-magnitude greater usable \(\)-range and accuracy. **Left:** Cumulative absolute error for the best \(D/\) (ours 200/70, PCR 200/35) is depicted over timesteps. **Right:** Forecast risk for 99 steps within a range of RBF lengthscales. Shaded areas depict min-max intervals.

Figure 5: Flow illustration. Area of initial cylinder positions shaded.

Figure 3: Forecasting risks (20 i.i.d. runs) for the Van der Pol system over a time-horizon \(H=14\) (\(T=1s\)). **Left:** Generalization gap for the best \(D\) / \(l\) (ours 500, PCR 62, RRR 100, RR-Sig-PDE 10) is depicted with a growing number of data points. **Right:** Test risk behavior with an increasing amount of eigenspaces is shown for \(N=200\). Shaded areas depict min-max risk intervals.