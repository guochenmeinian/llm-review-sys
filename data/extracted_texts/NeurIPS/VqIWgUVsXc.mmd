# Does Graph Distillation See Like Vision Dataset Counterpart?

Beining Yang\({}^{1,2}\), Kai Wang\({}^{3}\), Qingyun Sun\({}^{1,2}\), Cheng Ji\({}^{1,2}\), Xingcheng Fu\({}^{1,2}\),

**Hao Tang\({}^{4}\), Yang You\({}^{3}\), Jianxin Li\({}^{1,2}\)\({}^{1}\)School of Computer Science and Engineering, Beihang University**

\({}^{2}\)Advanced Innovation Center for Big Data and Brain Computing, Beihang University

\({}^{3}\)National University of Singapore \({}^{4}\)Carnegie Mellon University

Equal contribution (yangbeining@buaa.edu.cn, kai.wang@comp.nus.edu.sg).Project lead sunay@buaa.edu.cnCorresponding author lijx@buaa.edu.cn.

###### Abstract

Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel **S**tructure-broadcasting **G**raph **D**ataset **D**istillation (**SGDD**) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD are expected to have smaller LED shifts than previous works, leading to superior performance in both cross-architecture settings and specific tasks. We validate the proposed SGDD across 9 datasets and achieve state-of-the-art results on all of them: for example, on the YelpChi dataset, our approach maintains 98.6% test accuracy of training on the original graph dataset with 1,000 times saving on the scale of the graph. Moreover, we empirically evaluate there exist 17.6% \(\) 31.4% reductions in LED shift crossing 9 datasets. Extensive experiments and analysis verify the effectiveness and necessity of the proposed designs. The code is available in the https://github.com/RingBDStack/SGDD.

## 1 Introduction

Graphs have been applied in many research areas and achieved remarkable results, including social networks , physical , and chemical interactions . Graph neural networks (GNNs), a classical and wide-studied graph representation learning method , is proposed to extract information via modeling the features and structures from the given graph. Nevertheless, the computational and memory costs are extremely heavy when training on a large graph . One of the most straightforward ideas is to reduce the redundancy of the large graph. For example, graph sparsification  and coarsening  are proposed to achieve this goal by dropping redundant edges and grouping similar nodes. These methods have shown promising results in reducing the size and complexity of large graphs while preserving their essential properties.

However, graph sparsification and coarsening heavily rely on heuristics  (_e.g._, the largest principle eigenvalues , pairwise distances ), which may lead to poor generalization on various architectures or tasks and sub-optimal for the downstream GNNs training . Most recently, as shown in Fig. 1(a), GCond  follows the vision dataset distillation methods  and proposes to condense the original graph by the gradient matching strategy. The feature of the synthetic graph dataset is optimized by minimizing the gradient differences between training on original and synthetic graph datasets. Then, the synthetic feature is fed into the function \(f(^{})\) (_e.g._, pair-wise feature similarity ) to obtain the structure of the graph. The synthetic graph dataset (including feature and structure) is expected to preserve the task-relevant information and achieve comparable results as training on the original graph dataset at an extremely small cost.

Although the gradient matching strategy has achieved significant success in graph dataset condensation, most of these works  follow the previous vision dataset distillation methods to synthesize the condensed graphs, which results in the following limitations: 1) In Fig. 1(b), we can find that the original graph structure information is not well preserved in the condensed graph. That's because they build the structure (\(^{}\)) upon the learned feature (\(^{}\)) (Fig. 1(a)), which may cause the loss of original structure information. 2) In vision dataset distillation, applying the gradient matching strategy may result in an entanglement between the synthetic dataset and the architecture that is used for condensing . This can detrimentally impact their performance, especially when it comes to generalizing to unseen architectures . This issue is compounded when dealing with graph data that comprise both features and structures. As shown in Fig. 1(c) and Tab. 2, GCond  explicitly displays inferior performance, demonstrating that generating structure only based on the feature degrades the generalization of the condensed graph.

To investigate the relation between the structure of the condensed graph and the performance of training on it, we follow previous works  to analyze it from a spectral view. Specifically, we explore the relationship between the Laplacian Energy Distribution (LED) shift  and the generalization performance of the condensed graph. We empirically find a positive correlation between LED shift and the performance in cross-architecture settings. To address these issues, we introduce a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme to condense the original graph dataset by broadcasting the original adjacency matrix to the generation of the synthetic one, named SGDD, which explicitly prevents overlooking the original structure information. As shown in Fig. 1(d), we propose the graphon approximation to broadcast the original structure \(\) as supervision to the generation of condensed graph structure \(^{}\). Then, we optimize \(^{}\) by minimizing

Figure 1: (a) and (d) illustrate that the pipelines of SGDD and GCond . One can find that SGDD broadcast \(\) from the original graph to the generation of \(^{}\) while GCond synthesizes \(^{}\) via the operation \(f()\) (_e.g._, pair-wise feature similarity ) on \(^{}\). We also show the condensed graph and its shift coefficient (SC) of GCond and SGDD in (b) and (e), respectively. Note that we introduce \(SC\) as an approximation of the LED shift. We show the cross-architecture performance of GCond and SGDD in (c) and (f), 1, 2, 3, 4, and 5 denote APPNP , Cheby , GCN , SAGE , and SGC , more cross-architecture results can be found in Tab. 2.

the optimal transport distance of these two structures. For \(^{}\), we follow the  to synthesize the feature of the condensed graph. The \(^{}\) and \(^{}\) are jointly optimized in a bi-level loop. Both theoretical analysis (Sec. 3.2) and empirical studies (Fig. 1(c) and 1(f)) consistently show that our method reduces the LED shift significantly (\(0.46 0.18\)).

We further evaluate the impact of LED shifts and conduct experiments in the cross-architecture setting. Comparing Fig. 1(f), 1(c), and Tab. 2, the improvement of SGDD is up to 11.3%. To evaluate the generalization of our method, we extend SGDD to node classification, anomaly detection, and link prediction tasks. SGDD achieves state-of-the-art results on these tasks in most cases. Notably, we obtain new state-of-the-art results on YelpChi and Amazon with 9.6% and 7.7% improvements, respectively. Our main contributions can be summarized as follows:

* Based on the analysis of the difference between graph and vision dataset distillation, we introduce SGDD, a novel framework for graph dataset distillation via broadcasting the original structure information to the generation of a condensed graph.
* In SGDD, the graphon approximation provides a novel scheme to broadcast the original structure information to condensed graph generation, and the optimal transport is proposed to minimize the LED shifts between original and condensed graph structures.
* SGDD effectively reduces the LED shift between the original and the condensed graphs, consistently surpassing the performance of current state-of-the-art methods across a variety of datasets.

## 2 Related Work

**Dataset Distillation & Dataset Condensation.** Dataset distillation (DD) [88; 106; 24; 51; 110; 23; 86; 49; 87; 6] aims to distill a large dataset into a smaller but informative synthetic one. The proposed method imposes constraints on the synthetic samples by minimizing the training loss difference, while ensuring that the samples remain informative. This technique is useful for applications such as continual learning [105; 106; 39; 44; 104], as well as neural architecture search [61; 62; 96]. Recently, Gcond  is proposed to reduce a large-scale graph to a smaller one for node classification using the gradient matching scheme in DC . Unlike Gcond , who build the structure only through the learned feature, we explicitly broadcast the original graph structure to the generations to prevent the overlooking of the original graph structure information.

**Graph Coarsening & Graph Sparsification.** Graph Coarsening [53; 52; 14] follows the intuition that nodes in the original graph can naturally group the similiar node to a super-nodes. Graph Sparsification [66; 38; 78] is aimed at reducing the edges in the original graph, as there are many redundant relations in the graphs. We can simply sum up both two methods by trying to reduce the "useless" component in the graph. Nevertheless, these methods primarily rely on unsupervised techniques such as the largest \(k\) eigenvalue  or the multilevel incomplete LU factorization , which cannot guarantee the behavior of the synthetic graph in downstream tasks. Moreover, they fail to reduce the graph size to an extremely small degree (_e.g._, reduce the number of nodes to 0.1% of the original). In contrast, graph condensation can aggregate information into a smaller yet informative graph in a supervised way, thereby overcoming the aforementioned limitations.

## 3 Preliminary and Analysis

### Formulation of graph condensation

Consider a graph dataset \(=\{,,\}\), where \(^{N N}\) denotes the adjacency matrix, \(^{N d}\) is the feature, and \(^{N 1}\) represents the labels. \(=\{^{},^{},^{}\}\) is defined as the synthetic dataset, the first dimension of \(^{}\), \(^{}\), and \(^{}\) are \(N^{}\) (\(N^{} N\)).

The goal of graph condensation is achieving comparable results as training on the original graph dataset \(\) via training on the synthetic one \(\). The optimization process can be formulated as follow,

\[^{*}=_{}(_{}^{*}, _{}^{*})_{t}^{*}=_{} (_{}(t)),\] (1)

where \(_{}(t)\) denotes a GNN parameterized with \(\), \(_{}\) and \(_{}\) are the parameters that are trained on \(\) and \(\), and \(t\{,\}\), \(()\) denotes a matching function and \(()\) is the loss function.

### Analysis of the structure of the condensed graph and its effects

In this section, we first introduce the definition of Laplacian Energy Distribution (LED), then analyze the LED in previous work, and finally explore its influence on generalization performance.

**Laplacian Energy Distribution.** Following the previous works , we introduce Laplacian energy distribution (LED) to analyze the structure of graph data. Given an adjacency matrix \(^{N N}\), the Laplacian matrix \(\) is defined as \(-\), where the \(\) is the degree matrix of \(\). We utilize the normalized Lapalacian matrix \(}=-^{-1/2}^{-1/2}\), where \(\) is an identity matrix. The eigenvalues of \(}\) are then defined as \(_{1},_{n},_{N}\) by ascending order, _i.e._, \(_{1}_{n}_{N}\), with orthonormal eigenvectors \(=(_{1},_{n},_{N})\) correspond to these eigenvalues.

**Definition 1** (Laplacian Energy Distribution ).: _Let \(=(x_{1}, x_{n}, x_{N})^{}^{N d}\) be the feature of graph, we have \(}=(_{1}_{n}_{N})^{}=^{}\) as the post-Graph-Fourier-Transform of \(\). The formulation of LED is defined as:_

\[_{n}(,})=_{n}^{2}}{_{i=1}^{N }_{i}^{2}}=}.\] (2)

**LED analysis of previous work.** First, we define \(^{}\) and \(^{}\) as the LED of the original graph and condensed graph, respectively. Then the LED shift of the above two graphs can be formulated as: \(||^{}-^{}||=||_{i=1}^{N}_{i}(,})-_{j=1}^{N^{}}_{j}(^{}, }^{})||\). In GCond , the matching function \(\) defaults as the MSE loss. Therefore, the objective of GCond can be written as \(||_{}-_{}||\), where \(\) is a small number as expected. Incorporating such an objective in the LED shift formula, the lower bound is shown as follows.

**Proposition 1**.: _Refer to , the GNN can be recognized as the bandpass filter. Assume the frequency response area of GNN is \((a,b)\), where \((a,b)\) is architecture-specific. The lower bound of GCond is shown in Eq. (3). Detailed proof can be found in Appendix B.1._

\[||^{}-^{}||+_{i=1}^{a}||_{i}^{2}}-_{i}^{{}^{}2}}||+_{j=b}^{N^{}} ||_{j}^{2}}-_{j}^{{}^{}2}}||.\] (3)

According to Eq. (3), we find the lower bond of LED in GCond is related to the frequency response area \((a,b)\) (_i.e._, specific GNN). For example, GCN  or SGC  (utilized in GCond) is a low-pass filter , which emphasizes the lower Laplacian energy. As shown in Fig. 1(a), the \(^{}\) is built upon the learned "low-frequency" feature \(^{}\), which may fail to generalize to cross-architectures (_i.e._, high-pass filters) and specific tasks.

**Exploration of the effects of LED shift on the generalization performance.** Although the LED shift phenomenon occurs in GCond, quantifying the LED shift is challenging because \(\) and \(\) have

Figure 2: (a): Illustrations of the LEDs (use the density plotting, each peak represents the LED of a graph), SC, and Avg. (average test performances on GCN, SGC, APPNP, and SAGE) in various architectures. (b): Evaluation of these three metrics in frequency-adaptive BWGNN. We empirically find that the LED shifts (_i.e._, SC) are high-consistent with Avg. performances, thus SC could be a good indicator to evaluate the performance of the condensed graph.

different numbers of nodes. To enable comparison, we follow an intuitive assumption that two nodes with similar eigenvalue distribution proportions can be aligned in comparison. Thus, we first convert LEDs of \(\) and \(\) into probability distributions using the Kernel Density Estimation (KDE) method . Then we quantify the LED shifts as the distance of the probability distributions using the Jensen-Shannon (JS) divergence . We define the LED shift coefficient (\(SC\)) in Definition 2:

**Definition 2** (LED shift coefficient, \(Sc\)).: _The LED shift coefficient between \(\) and \(\) is:_

\[SC=(}|h}_{x_{i} X_{ }}K(_{i}}{h})}|h}_{ _{j} X_{}}K(_{j}}{h})),\] (4)

_where the \((||)\) denotes the JS-divergence, the \(|V_{}|\) and the \(|V_{}|\) is the number of the nodes to corresponding graphs, the \(K()\) represents the Gaussian kernel function with bandwidth parameter \(h\). \(SC\) reflects the divergence between \(\) and \(\) (a smaller \(SC\) indicates more similar)._

In Fig. 2, we empirically study the influences of \(SC\) in two settings: various GNNs in Fig. 2(a) and fixed BWGNN  with adaptive bandpass in Fig. 2(b). **Based on the results in Fig. 2, We have several observations:** (1) The entangled learning paradigm that building structure (_i.e._, adjacency matrix) upon on feature matrix will significantly lead to the LED shift phenomenon. (2) The positive correlation exists between the LED shift and the generalization performance of the condensed graph. (3) Preserving more information about the original graph structure may alleviate the LED shift phenomenon and improve the generalization performance of the condensed graph.

## 4 Structure-broadcasting Graph Dataset Distillation

In this section, we first present the pipeline and overview of SGDD in Fig. 3. Then, we introduce two modules of SGDD. Finally, we summarize the training pipeline of our SGDD.

### Learning graph structure via graphon approximation

To prevent overlooking the original structure \(\) from \(\), we broadcast \(\) as supervision for the generation of \(^{}\). Considering the different shapes between \(\) and \(^{}\) (\(N^{} N\)), we introduce graphon  to distill the original structure information to the condensed structure \(^{}\). Specifically, given random noise \((N^{})^{N^{} N^{}}\) as the input coordinates, through the generative model, we then synthesize an adjacency matrix \(^{}\) with \(N^{}\) nodes. This process can be formulated as \(^{}=((N^{});)\), where the \(()\) is a generative model with parameter \(\), and the optimization process is then defined as:

\[_{}=(,( (N^{});)),\] (5)

where \(\) is supervision and \(()\) is a metric that measure the difference of \(\) and \(^{}\). The details of \(()\) can be found in Sec. 4.2. To avoid overlooking of the inherent relation  between \(^{}\) and the corresponding node information (_i.e._, \(^{}\) and \(^{}\)), we jointly input \(^{}\) and \(^{}\) as conditions to generate \(^{}\). Therefore, the final version of the generative model can be written as \(^{}=((N^{})^{ }^{};)\), the \(\) denotes the concatenate operation.

Figure 3: The Training pipeline of the SGDD (left). We first fix \(^{}\) to optimize \(^{}\) through the gradient matching strategy and we broadcast the supervision of \(\) to the generation of the graph structure \(^{}\). To mitigate the Laplacian Energy Distribution (LED) shift phenomenon, we propose the LED Matching strategy to optimize the \(^{}\), which optimizes the learned structure with the optimal transport (OT) distance (right).

To study the performance of SGDD in the above paradigm, we theoretically prove the upper bound of LED shift in SGDD by invoking graphon theory. The result can be presented as follows.

**Proposition 2**.: _The upper bound of the LED shift on SGDD is shown as:_

\[||^{}-^{}||_{}(W_{}, ^{}),\] (6)

_where \(_{}\) denotes the cut distance  and \(W_{}\) is the graphon of \(\). See details in Appendix B.2._

Note minimizing the upper bound of Eq. (6) is equal to optimizing the \(L_{structure}\) on Eq. (5). Compared to the lower bound in (Eq. (3)), our upper bound is not related to any frequency response of specific GNN (_i.e._, the terms of \(_{i=1}^{a}||_{i}^{2}}-_{i}^{}}^{2}||+_ {i=b}^{N^{}}||_{i}^{2}}-_{i}^{}}^{2}||\)). As a result, SGDD may perform better than the previous work, especially in the cross-architecture setting and specific tasks.

### Optimizing the graph structure via optimal transport

To mitigate the LED shift between \(\) and \(^{}\), ideally, we can directly minimize the proposed \(SC\). However, \(SC\) requires an extremely time-consuming (\(O(N^{3})\))  eigenvalue decomposition operation. Therefore, we propose the LED Matching strategy based on the _optimal transport_ theory to form an efficient optimizing process.

Recall in the Eq.(4) of calculating \(SC\), we first decompose the eigenvalue, followed by aligning the node through the JS divergence, which essentially compares the distribution proportions. The key point is to decide the node mapping strategy to align such two graphs. Alternatively, assuming we know the prior distribution of the alignment of \(\) (_i.e._, we know the bijection of nodes in \(\) to the \(\)) and denoting such alignment as \(*\), we can directly measure the distance between \(\) and \(*\) by employing the 2-Wasserstein metric.

\[||^{}-^{*}||=W_{2}^{2}(^{}, ^{*})\] (7)

Furthermore, following the assumption in previous work, we have \(^{G}(0,L_{}^{})\) and \(^{S}(0,L_{*}^{})\). Then, the Eq.(7) have a closed-form expression as follows:

\[||^{}-^{*}||=N(L_{ }^{})+N^{}(L_{* }^{})-2(*}^{}L_{}^{}L_{*}^{}}),\] (8)

the \(L_{}^{}\) denotes the Laplacian pseudo-inverse operation and the \(\) indicates the trace operation of matrix. Therefore, even though we could not know the actual mapping strategy \(*\), we can use the infimum of all possible strategies as a proxy solution. Formally, following the prior work , we employ the function \(T\) as a transport plan in the metric space \(\) to represent all feasible mapping strategies. Then, we use the \(T_{\#}^{}\) to represent the pushing forward process of transferring the distribution of \(^{}\) to the \(^{}\). As a result, the distance can be regarded as finding the infimum.

\[||^{}-^{}|| =_{T_{\#}^{}=^{}}_{}\|x-T(x)\|^{2}d^{}(x)\] (9) \[=N^{}(L_{}^{})-2 (((L_{}^{})^{1/2}P^{T}L_{ }^{}P(L_{}^{})^{1/2})^{1 /2}).\]

Here, due to the transport plan \(T\) is impractical in optimizing, following the previous work, we use \(P^{N^{} N}\) denotes as a free parameter serving as the direct mapping strategy between nodes. Thus, the \(\) in the Eq. (5) could be directly optimized by the Eq.(9) (_i.e._, use the \(P\) represents all possible mapping strategies, the optimizing of \(P\) is equal to choosing a more optimal mapping strategy). In the experimental setting, we use the Sinkhorn-Knopp algorithm to optimize \(P\).

The overall time complexity is reduced to the \(O(N^{}) O(N^{2.373})\). Note that the \(L_{}^{}\) may be too large for computing, so we empirically sample a medium size (_e.g._, 2,000 nodes) sub-structure in the experiment and ablate its influence in Appendix C.7.

### Training pipeline of SGDD

As illustrated in Fig. 1(d) and Fig. 3, we commence by introducing a novel graph structure learning paradigm termed "graphon approximation". This paradigm integrates both the feature \(X^{}\) and auxiliary information \(Z\) to generate the structure. Subsequently, the learned structure \(A^{}\) is forced to be closer to the original graph structure \(A\) in terms of Laplacian energy distribution.

Additionally, our proposed methodology, SGDD, implements a bi-loop optimization schema. Within this framework, we concurrently optimize the parameters \(X^{}\) and \(A^{}\). More specifically, the refinement of \(X^{}\) is achieved through a gradient matching strategy, whereas the \(A^{}\) is enhanced using the LED matching technique. During each step, the other component is frozen to ensure effective refinement.

Our overall training loss function can be summarized as \(=_{feature}+_{structure}+||||_{2}\), where the \(||||_{2}\) is proposed as a sparsity regularization term and \(_{feature}\) denotes the gradient matching strategy . \(\) and \(\) are trade-off parameters, we study their sensitiveness in the Sec. 5.3. The algorithm can be found in Appendix C.3.

## 5 Experiments

### Datasets and implementation details

**Datasets.** We evaluate SGDD on five node classification datasets: Cora , Citeseer , Ogbn-arxiv , Flickr , Reddit , two anomaly detection datasets: YelpChi , Amazon , and two link prediction datasets: Citeseer-L , DBLP . For the node classification and anomaly detection tasks, we follow the public settings  of train and test. To make a fair comparison, we also follow the previous setting [101; 83], we randomly split 80% nodes for training, 10% nodes for validation, and the remaining 10% for testing. To avoid data leakage, we only utilize 80% training samples for condensation. More details of each dataset can be found in Appendix C.1.

    & **Ratio (\(\))** & **Random** & **Herding** & **K-Center** & **Coarsening** & **GDC** & **Geond** & **SCDD** & **Whole** \\   &  & 0.90\% & 54.4\({}_{+ 4.4}\) & 57.1\({}_{+ 1.5}\) & 52.4\({}_{ 2.8}\) & 52.2\({}_{ 0.4}\) & 66.8\({}_{ 1.5}\) & **70.5\({}_{ 1.2}\)** & 69.5\({}_{ 0.4}\) & \\  & & 1.80\% & 64.2\({}_{+1.7}\) & 66.7\({}_{+1.9}\) & 64.3\({}_{ 1.0}\) & 59.0\({}_{ 0.5}\) & 66.9\({}_{ 0.9}\) & **70.6\({}_{ 0.9}\)** & 70.2\({}_{ 0.8}\) & 71.7\({}_{ 0.1}\) \\  & & 3.60\% & 69.1\({}_{+0.1}\) & 69.0\({}_{+0.1}\) & 69.1\({}_{+0.1}\) & 65.3\({}_{+0.5}\) & 66.3\({}_{ 1.5}\) & 69.8\({}_{ 1.4}\) & **70.3\({}_{ 1.7}\)** & \\   &  & 1.30\% & 63.3\({}_{+ 7.7}\) & 67.0\({}_{+1.3}\) & 64.0\({}_{+2.3}\) & 31.2\({}_{+0.2}\) & 67.3\({}_{ 1.9}\) & 79.8\({}_{ 1.3}\) & **80.1\({}_{ 0.7}\)** & \\  & & 2.60\% & 72.8\({}_{+1.1}\) & 73.4\({}_{+1.4}\) & 73.2\({}_{+1.2}\) & 65.2\({}_{+0.6}\) & 67.6\({}_{+3.5}\) & 80.1\({}_{+0.6}\) & **80.6\({}_{ 0.8}\)** & 81.2\({}_{ 0.2}\) \\  & & 5.20\% & 76.8\({}_{+1.0}\) & 76.8\({}_{+0.1}\) & 76.7\({}_{+0.1}\) & 70.6\({}_{+0.1}\) & 67.7\({}_{+2.2}\) & 79.3\({}_{+0.3}\) & **80.4\({}_{+1.6}\)** & \\   &  & 0.05\% & 47.1\({}_{+3.9}\) & 52.4\({}_{+1.8}\) & 47.2\({}_{+3.0}\) & 35.4\({}_{+0.3}\) & 58.6\({}_{ 0.4}\) & 59.2\({}_{+1.1}\) & **60.8\({}_{ 1.3}\)** & \\  & & 0.25\% & 57.3\({}_{+1.1}\) & 58.6\({}_{+1.2}\) & 56.8\({}_{+0.5}\) & 43.5\({}_{+0.2}\) & 59.9\({}_{+0.3}\) & 63.2\({}_{+0.3}\) & **65.8\({}_{ 1.2}\)** & 71.4\({}_{ 0.1}\) \\  & & 0.50\% & 60.0\({}_{+0.0}\) & 60.4\({}_{+0.0}\) & 60.3\({}_{+0.0}\) & 50.4\({}_{+0.1}\) & 59.5\({}_{+0.3}\) & 64.0\({}_{+0.4}\) & **66.3\({}_{+0.7}\)** & \\   &  & 0.10\% & 41.8\({}_{+2.0}\) & 42.5\({}_{+1.8}\) & 42.0\({}_{+0.7}\) & 41.9\({}_{+2.6}\) & 46.3\({}_{+0.2}\) & 46.5\({}_{+0.4}\) & **46.9\({}_{+1.0}\)** & \\  & & 0.50\% & 44.0\({}_{+0.4}\) & 43.9\({}_{+0.9}\) & 43.2\({}_{+0.1}\) & 44.5\({}_{+0.1}\) & 45.9\({}_{+0.1}\) & 47.1\({}_{+0.1}\) & 47.1\({}_{+0.3}\) & 47.2\({}_{ 0.1}\) \\  & & 1.00\% & 44.6\({}_{+0.2}\) & 44.4\({}_{+0.6}\) & 44.1\({}_{+0.0}\) & 44.6\({}_{+0.1}\) & 45.8\({}_{+0.1}\) & 47.1\({}_{+0.1}\) & 47.1\({}_{+0.1}\) & 47.1\({}_{+0.1}\) \\   &  & 0.01\% & 46.4\({}_{+1.4}\) & 53.1\({}_{+2.5}\) & 46.6\({}_{+2.3}\) & 40.9\({}_{+0.9}\) & 88.2\({}_{+0.2}\) & 88.0\({}_{+1.8}\) & **90.5\({}_{+1.1}\)** & 93.9\({}_{ 0.0}\) \\  & & 0.05\% & 58.0\({}_{+2.2}\) & 67.1\({}_{+1.9}\) & 50.5\({}_{+3.3}\) & 42.8\({}_{+0.8}\) & 89.5\({}_{+0.1}\) & 89.6\({}_{+0.7}\) & **91.8\({}_{+1.9}\)** & 93.9\({}_{ 0.0}\) \\  & & 0.50\% & 66.3\({}_{+1.9}\) & 71.0\({}_{+1.6}\) & 58.5\({}_{+2.2}\) & 47.4\({}_{+0.9}\) & 90.5\({}_{+1.2}\) & 90.1\({}_{+0.5}\) & **91.6\({}_{+1.8

**Implementation details.** Without specific designation, **in the condense stage**, we adopt the 2-layer GCN with 128 hidden units as the backbone, and we adopt the settings on , which use 2-layer MLP to represent the structure generative model (_i.e._, \(\)). The learning rates for structure and feature are set to 0.001 (0.0001 for Ogbn-arxiv and Reddit) and 0.0001, respectively. We set \(\) to 0.1, and \(\) to 0.1. **In the evaluation stage**, we train the same network for 1,000 epochs on the condensed graph with a learning rate of 0.001. Following the settings in , we repeat all experiments ten times and report average performance and variance. More details4 can be found in Appendix C.2.

### Comparison with state-of-the-art methods

We compare our proposed SGDD with six baselines: Random, which randomly selected nodes to form the original graph, corset methods (Herding  and K-Center ), graph coarsening methods (Corasening ), and the state-of-the-art graph condensation methods (GCond ). GDC is proposed as a baseline in , where cosine similarity is added as a constraint to generate the structure of the condensed graph. In Table 1, we present the performance metrics including accuracy, F1-macro, and AUC. For clarity and simplicity, percentages are represented without the \(\%\) symbol, and variance values are provided. Based on the results, we have the following observations: 1) Our proposed SGDD achieves the highest results in most settings, which shows the superiority and generalization of our method. 2) On anomaly detection datasets, the improvements are more significant than other tasks, _i.e._, improving GCond with 9.6% and 7.7% on YelpChi and Amazon datasets, which can be explained that our method captures the structure information from the original graph dataset more efficiently.

### Ablation Study

**Cross-architecture generalization analysis.** To evaluate the generalization ability of SGDD on unseen architectures, we conduct experiments that train on the condensed graph with different

   &  &  &  \\   & & MLP  & GAT  & APPNP  & Cheby  & GCN  & SAGE  & SOC  & Avg. & **Stid** & \((\%)\) \\   & GDC & 50.3 & 54.8 & 81.2 & 77.5 & 89.5 & 89.7 & 90.5 & 76.2 & **16.9** & \(\) \\  & GCond & 42.5 & 60.2 & 87.8 & 75.5 & 89.4 & 89.1 & 89.6 & 76.3 & **18.5** & \(\) 1.0 \\  & Ours & 56.1 & 74.4 & 89.2 & 78.4 & 89.4 & 89.4 & 89.4 & **80.9** & **12.6** & \(\) **4.7** \\   & GDC & 67.2 & 64.2 & 67.1 & 67.7 & 67.9 & 66.2 & 72.8 & 67.6 & 2.6 & \(\) \\  & GCond & 73.1 & 66.2 & 78.5 & 76 & 80.1 & 78.2 & 79.3 & 75.9 & **4.9** & \(\) 8.3 \\  & Ours & 76.7 & 75.8 & 78.4 & 78.5 & 79.8 & 80.4 & 78.5 & **78.3** & **1.6** & \(\) **10.7** \\   & GDC & 74.4 & 76.8 & 77.4 & 76.7 & 78.9 & 74.8 & 78.4 & 76.8 & \(\) **1.7** & \(\) 1.1 \\  & GCond & 75.3 & 77.6 & 78.9 & 76.1 & 79.6 & 77.4 & 79.9 & 77.8 & \(\) 1.7 & \(\) 1.1 \\  & Ours & 78.4 & 79.6 & 80.1 & 80.6 & 82.1 & 80.7 & 81.4 & **80.4** & **1.2** & \(\) **1.3** & \(\) **3.6** \\   & GDC & 30.7 & 36.4 & 43.7 & 41.5 & 49.6 & 47.4 & 50.1 & 42.8 & 7.2 & \(\) \\  & GCond & 48.9 & 31.8 & 46.7 & 48.6 & 50.1 & 42.5 & 48.7 & 45.3 & 6.5 & \(\) 2.6 \\   & Ours & 54.2 & 56.4 & 58.2 & 56.8 & 59.7 & 54.1 & 56.7 & **56.6** & **2.0** & \(\) **13.8** \\  

Table 2: Results of cross-architecture setting, we test condensed graphs in APPNP, Cheby, GCN, GraphSAGE, and SGC. Avg. and Std.: the average performance and the standard deviation of the results, the \((\%)\) denotes the improvements upon the GDC. We mark the best performance by **bold**.

Figure 4: (a) Ablation of components in SGDD. (b) Evaluation of the scalability of SGDD. (c) and (d): the trade-off parameters analysis on \(\) and \(\).

architectures and report their performances in Tab. 2. Here, the condensed graph is obtained by optimizing SGDD with SGC . We test its cross-architecture generalization performances on 2-layer-MLP , GAT , APPNP , Cheby , GCN , and SAGE .

To better understand, we also show several statistics metrics, including Avg., Std., and \(\). SGDD and GCond improves GDC significantly, which indicates there exists a large difference between graph dataset condensation and vision dataset distillation, _i.e._, the structure information should be specially considered. Compared to GCond, the improvement of our method is up to 11.3%, demonstrating the effectiveness of broadcasting the original structure to the condensed graph structure generation. More experiments on other datasets can be found in Appendix C.6.

**Versatility of SGDD.** Following the setting of GCond , we also study whether our proposed SGDD is robust on various architectures. We first condense the Ogbn-arxiv graph dataset with five architectures, including APPNP , Cheby , GCN , SAGE , and SGC , respectively. Then, we evaluate these condensed graphs on the above five architectures and report their performances in Tab. 3. The experiment results show that SGDD achieves non-trivial improvements than GCond in most cases, which demonstrates the strong versatility of our method.

**Evaluation on neural architecture search.** Similar to vision dataset distillation, graph dataset distillation is also expected to reduce the high cost of neural architecture search (NAS). In order to make a fair comparison, we follow the experimental setting in : searching architectures on condensed Obgn-arxiv, YelpChi, and DBLP datasets with 0.25%, 0.2%, and 0.5% condensing ratios. We report Pearson correlation  and performance of random, GCond, and SGDD in Tab. 4. Our SGDD consistently achieves the highest Pearson correlations as well as performances, which indicates the architectures searched by our method are efficient for the whole graph dataset training.

**Evaluation of components in SGDD.** To explore the effect of the conditions (mentioned in Sec. 4.1) in the condensed graph generation, we design the ablation study of \(^{}\) and \(\). As shown in Fig 4(a), \(^{}\) and \(\) are complementary with each other. SGDD w/o \(\) performs poorly on Obgn-arxiv and YelpChi datasets, which demonstrates the effect of original graph structure information. Jointly using \(^{}\) and \(\) achieves the highest performances on both datasets, improves GCond with 3.1% on Ogbn-arxiv, and 8.0% on YelpChi.

**Evaluation of the scalability of SGDD.** To investigate the scalability of SGDD, we evaluate the SGDD on various condensing ratios with \(r\{0.02,0.2,2,5,10,20\}\). As shown in Fig. 4(b), the performance of our method continuously increase as the condensing ratio rises, which indicates the strong scalability of our method. GCond obtains marginal improvements than GDC at all ratios while our SGDD outperforms them significantly. More important, SGDD achieves lossless performance as training on the original graph data when the condensing ratio is 10%.

**Exploring the sensitivity of \(\) and \(\).** We conduct experiments to test such two parameters sensitivity on YelpChi, Cora, and Ogbn-arxiv. As shown in Fig. 4(c), we empirically find that the performance of our SGDD is not sensitive to the \(\). Specifically, compared to the case that \(\) is zero, we have a significant improvement, which proves the effectiveness of our method. Another finding is that the \(\) should be set higher on anomaly detection than on node classification tasks. It could be explained by the original graph structure information being more important on the complex task (such as anomaly detection). We define \(\) as a regularization coefficient to control the sparsity of the condensed graph. As shown in Fig. 4(d), we evaluate the \(\) from 0 to 10 on the YelpChi dataset. The results illustrate that the performance is not sensitive with \(\) and achieve the highest result (F1-macro) when \(\) is set to our default value (\(=0.1\)). More experiments can be found in Appendix C.5.

    &  & Whole \\   & Random & GCond & SGDD & Per. (\%) \\   &  &  &  &  &  \\  & & & & & \\    & & & & & \\    & & & & & \\   

Table 4: Neural Architecture Search. Methods are compared in validation accuracy correlation and test accuracy on obtained architecture.

  
**CY** & **APFP** & **Chord** & **GCN** & **SAGE** & **SGC** \\   & GCond/SGD & GCond/SGD & GCond/SGD & GCond/SGD \\   & \(\).21 & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) \\  & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) \\  & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) \\  & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) & \(\)/\(\) \\   

Table 3: Comparison of the cross-architecture generalization performance between GCond and SGDD on Ogbn-arxiv. **Bold entries** are the best results. \(\): our method show increase or decrease performance.

### Visualizations

To better understand the effectiveness of our SGDD, we visualize the condensed graphs of Gcond and SGDD that are synthesized from real and synthetic graph datasets. For the synthetic graph dataset, we use the Stochastic Block Model (SBM)  to synthesize graphs with \(5\) community (Fig. 5(d)). As shown in Fig. 5, one can find that our method consistently achieves better performances and \(SC\). SGDD reduces 29.0% (comparing 5(b) and 5(c)) and 62.2% (comparing 5(e) and 5(f)) \(SC\) on Ogbn-arxiv and synthetic datasets. Visually, the condensed graphs of our method preserve the original graph structure information obviously better than Gcond (see the second row of Fig. 5), which proves SGDD is a powerful graph dataset distillation method.

## 6 Conclusion

We present SGDD, a novel framework for graph dataset distillation via broadcasting the original structure information to the generation of the synthetic one. SGDD shows its robustness on various tasks and datasets, achieving state-of-the-art results on YelpChi, Amazon, Ogbn-arxiv, and DBLP. SGDD reduces the scale of the Yelpchi dataset by 1,000 times while maintaining 98.6% as training on the original data. We provide sufficient experiments and theoretical analysis in this paper and hope it can help the following research in this area. **Limitations and future work:** Although broadcasting the original information to the generated graph shows remarkable success, some informative properties (_e.g._, the heterogeneity) may lose during the current condense process, which results in sub-optimal performance in the downstream tasks. We are going to explore a more general method in the future.