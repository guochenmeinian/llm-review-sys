# Optimal, Efficient and Practical Algorithms for Assortment Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, and fine-tuning language models, amongst many others. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a'strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected--all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with _Plackett Luce_ (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using '_Pairwise Rank-Breaking_', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. Empirical evaluations corroborate our findings and outperform the existing baselines.

## 1 Introduction

Studies have shown that it is often easier, faster and less expensive to collect feedback on a relative scale rather than asking ratings on an absolute scale. E.g., to understand the liking for a given pair of items, say (A,B), it is easier for the users to answer preference-based queries like: "Do you prefer Item A over B?", rather than their absolute counterparts: "How much do you score items A and B in a scale of [0-10]?". Due to the widespread applicability and ease of data collection with relative feedback, learning from preferences has gained much popularity in the machine-learning community, especially the active learning literature which has applications in Medical surveys, AI tutoring systems, Multi-player sports/games, or any real-world systems that have ways to collect feedback in terms of preferences. The problem is famously studied as the _Dueling-Bandit_ (DB) problem in the active learning community , which is an online learning framework for identifying a set of 'good' items from a fixed decision-space (set of items) by querying preference feedback of actively chosen item-pairs. Consequently, the generalization of Dueling-Bandits, with _subset-wise_ preferences has also been developed into an active field of research. For instance, applications like Web search (e.g. Google, Bing, or even in some versions of ChatGPT), online shopping (Amazon, App stores, Google Flights), recommender systems (e.g. Youtube, Netflix, Google News/Maps, Spotify) typically involve users expressing preferences by choosing one result (or a handful of results) from a subset of offered items and often the objective of the system is toidentify the'most-profitable' subset to offer to their users. The problem, popularly termed as 'Assortment Optimization' is studied in many interdisciplinary literature, e.g. Online learning and bandits , Operations research [40; 2], Game theory , RLHF [20; 30], to name a few.

**Problem (Informal): Active Optimal Assortment (AOA)** Active Assortment Optimization (a.k.a. Utility Maximization with Subset Choices) [13; 2; 23; 22] is an active learning framework for finding the 'optimal' profit-maximizing subset. Formally, assume we have a decision set of \([K]:=\{1,2, K\}\) of \(K\) items, with each item being associated with the score (or utility) parameters \(:=(_{1},_{2},,_{K})\) (without loss of generality assume \(_{1}_{2}_{K} 0\)). At each round \(t=1,2,\), the learner or the algorithm gets to query an assortment (typically subsets containing up to \(m\)-items) \(S_{t}[K]\), upon which it gets to see some (noisy) relative preferences across the items in \(S_{t}\), typically generated according to an underlying Plackett-Luce (PL) choice model with parameters \(\) (1). Further, to allow the event where no items are selected, we also model a No-Choice (NC) item, indexed by item-0, with PL parameter \(_{0}_{+}\).

**(Objective 1.) Top-\(m\):** identify the top-\(m\) item-set: \(\{_{1},,_{m}\}\), for some \(m[1,K]\).

**(Objective 2.) Wtd-Top-\(m\):** A more general objective could also consider a weight (or price) \(r_{i}_{+}\) associated with the item \(i[K]\), and the goal could be to identify the assortment (subset) with maximum weighted utility 1, as detailed in Sec. 2.

**Related Works and Limitations:** As stated above, the problem of AOA is fundamental in many practical scenarios, and thus widely studied in multiple research areas, including Online ML/learning theory and operations research.

* In the Online ML literature, the problem is well-studied as _Multi-Dueling Bandits_[39; 14], or Battling Bandits [35; 34; 11], which is an extension of the famous _Dueling Bandit_ problem [46; 45]. The main limitation of this line of work is the lack of practical objectives, which either aim to identify the 'best-item' \(1(=_{i[K]}_{i})\) within a PAC (probably approximately correct) framework [36; 16; 17; 31] or quantifying regret against the best items [35; 12]. Note the latter actually leads to the optimal subset choice of repeatedly selecting the optimal item, \(_{i}_{i}\), \(m\) times, i.e. \((1,1, 1)\), which is unrealistic from the viewpoint of real-world system design. Selecting an assortment of distinct top-\(m\) items (Top-\(m\)-AOA) or maximum expected utility (Wtd-Top-\(m\)-AOA) makes more sense.
* On the other hand, a similar line of the problem has been studied in operations research and dynamic assortment selection literature, where the goal is to offer a subset of items to the customers in order to maximize expected revenue. The problem has been studied under different user choice models, e.g. PL or Multinomial-Logit models , Mallows and mixture of Mallows , Markov chain-based choice models , single transition model  etc. While these works indeed consider a more practical objective of finding the best assortment (subset) with the highest expected utility for a regret minimization objective, (1) a major drawback in their approach lies in the algorithm design which _requires to keep on querying the same set multiple times_, e.g. [2; 29; 18; 1]. Such design techniques could be impractical to be deployed in real systems where users could easily get annoyed if the same items are shown again and again. For example, in ad-placement, music/movies/news/tweets/reels recommendations, offering the same assortment could increase user dissatisfaction and disengagement.
* The second major drawback of this line of work lies in the _structural assumption of their underlying choice models which requires the existence of a reference/default item, that needs to be part of every assortment \(S_{t}\)_. This leads to assuming a No-Choice item, typically denoted as item-0, which is a default choice of any assortment \(S_{t}\). Further a stronger and more unrealistic assumption lies in the fact that they require to assume that the above pivot is stronger than the rest of the \(K\) items, i.e. \(_{0}_{i[K]}_{i}\), i.e. the No-Choice (NC) action is the most likely outcome of any assortment \(S_{t}\). This is often unrealistic, e.g., during user interactions with language models, or online shopping, or Route recommendation in GPS navigation, a NC action is highly improbable. Consequently, such assumption limits the use in real-systems. In the existing literature [2; 28; 1; 24], such assumptions are primarily adapted solely for theoretical needs, precisely for maintaining concentration bounds of the PL parameters \(\), and hence not well justified from a practical viewpoint. Some recent developments also generalized the AOA  problem to linear MNL scores to incorporate large actions embedded in \(d\)-dimension , however, their approaches are either limited to the above restrictions or suffer sub-optimal regret guarantees without those assumptions (e.g. the regret bound of  is \(O(d^{3/2})\) which is suboptimal by a \(d\)-factor). Considering the above limitations of the AOA  literature, we set to answer two questions:

1. Can we consider a general AOA model where the default item, like the NC item defined above, is not necessarily the strongest one, i.e. \(_{0}_{i[K]}_{i}\)?
2. Can we design a practical and regret optimal algorithm for the AOA framework, without needing to play the same repetitive actions and yet converge to the optimal assortment?

ContributionsWe answer these questions in the affirmative and present best of all scenarios. We design practical algorithms on practical AOA  framework with practical objectives-Unlike the existing approaches of the AOA, literature , we do not have to keep playing the same assortment multiple times, neither require a strongest default item (like NC satisfying \(_{0}_{i[K]}_{i}\)). Moreover, our objectives do not require us to converge to a multiset of replicated arms like \((1,1, 1)\), but converge to the utility-maximizing set of distinct items. We list our contributions below:

1. **A General AOA Setup:** We work with a general problem of AOA  for PL model, which requires no additional structural assumption of the \(\) parameters such as \(_{0}_{i}_{i}\), unlike the existing works. We designed algorithms for two separate objectives Top-\(m\)  and Wtd-Top-\(m\)  as discussed above (Sec. 2).
2. **Practical, Efficient and Optimal Algorithm:** In Sec. 3, we give a practical, efficient and optimal algorithm for MNL Assortment (up to log factors and the magnitude of \(_{}\)). The regret bound of our algorithm AOA-RB\({}_{ PL}\)  (Alg. 1) yields \(()\) regret for both Top-\(m\) and Wtd-Top-\(m\)  objective. Our algorithms use a novel parameter estimation technique for discrete choice models based on the concept of _Rank-Breaking_ (RB) which is one of our key contributions towards designing the efficient and optimal algorithm. This enables our algorithm to perform optimally without requiring the No-Choice item to be the strongest. Appendix A details the key concept of our parameter estimation technique exploiting the concept of RB. Our resulting algorithm plays optimistically based on the UCB estimates of PL parameters and does not require repeating the same subset multiple times, justifying our title.
3. **Improvement with Adaptive Pivots:** In Sec. 4, we refine the performance of our algorithm by employing the novel idea of 'adaptive pivots' (a reference item) and proposed AOA-RB\({}_{ PL}\)-Adaptive. Performance-wise this removes the asymptotic dependence on \(_{}=_{i}_{i}/_{0}\) in the regret analysis. This enables the algorithm to work effectively in scenarios where the No-Choice item is less likely to be selected, i.e., \(_{} 1\). This leads to a huge improvement in our experiments, especially in the range of low \(_{0}\), where AOA-RB\({}_{ PL}\)-Adaptive  drastically outperforms over the existing baseline. Comparison of our regret bound with existing work is detailed in Table 1.

4. **Emperical Analysis.** Finally, we corroborate our theoretical results with empirical evaluations (Sec. 5), which certify our superior performance in the general AOA setups.

 
**Work** & **Framework** & **Assume**\(_{0}=_{}=1\) & **Regret** \\  Our (Alg. 1) & MNL model (Obj. 2) & No & \(,K\}KT T}\) \\ 
 (Thm 1) & MNL model (Obj. 2) & Yes & \(\) \\ 
 (Thm 4) & MNL model (Obj. 2) & No & \(KT T}\) \\ 
 & MNL model (Obj. 2) & Yes & \((mT)}\) \\ 
 & MNL model with & No & \(}} T\) \\  & constraints (Obj. 2) & & \(}} T\) \\  

Table 1: Our Contribution vs the Existing Results in the \(K\)-armed MNL-Assortment literature It is also worth mentioning that our proposed algorithm and their respective regret analysis could be extended to any general random utility (RUM) based preference models , as explained in Rem. 1. However, to keep the focus on the AOA problem and ease the presentation, we stick to the special case of MNL choice model based preferences.

## 2 Problem Setup

We write \([n]=\{1,2,...,n\}\) and \(\{\}\) denotes the indicator function. The symbol \(\), employed in the proof sketches, represents a coarse inequality.

We consider the sequential decision-making problem of Active Optimal Assortment (AOA), with preference/choice feedback. Formally, the learner is given \([K]\), a finite set of \(K\) items (\(K>2\)). At each decision round \(t=1,2,\), the learner selects a subset \(S_{t}[K]\) of up to \(m\) items, and receives some (stochastic) feedback about the item preferences of \(S_{t}\), drawn according to some unknown underlying Plackett-Luce (PL) choice model (1) with parameters \(=(_{1},_{2},,_{K})_{+}^{K}\). We assume \(_{1}_{2}_{K}\) without loss of generality. An interested reader may check App. A.1 for a detailed discussion on PL models. Given any assortment \(S_{t}\) we also consider the possibility of 'no-selection' of any items given an \(S_{t}\). Following the literature of , we model this mathematically as a No-Choice (NC) item, indexed by item-0, and its corresponding PL utility parameter \(_{0}\). Unlike most existing literature on assortment selection, we are not assuming \(_{0}_{i[K]}_{i}\). Further, since the PL model is scale independent, we set \(_{0}=1\) and scale the rest of the PL parameters.

Feedback modelThe feedback model formulates the information received (from the 'environment') once the learner plays a subset \(S_{t}[K]\) of at most \(m\) items. Given \(S_{t}\) we consider the algorithm receives a winner feedback (or index of an item) \(i_{t} S_{t}\{0\}\), drawn according to the underlying PL choice model as:

\[(i_{t}=i|S_{t})=_{i}/_{0}+_{j S_{t}} _{j},\ \  i S_{t}.\] (1)

We consider the following two objectives for the learner:

1. Top-\(m\)-Ojective.One simple objective could be to identify the top-\(m\) item-set: \(\{_{1},,_{m}\}\), for some \(m[1,K]\). The performance of the learner can be captured by minimizing the following regret: \[_{T}^{}:=_{t=1}^{T}}- _{S_{t}}}{m},\ \ \ \ \ \ S^{*}:=*{argmax}_{S[K]:|S|=m}\{_{S}:= _{i S}_{i}\}.\]

2. Wtd-Top-\(m\)-Objective.Here, each item-\(i\) is associated with a weight (for example price) \(r_{i}_{+}\), and the goal is to identify the set of size at most \(m\) with maximum weighted utility. One could measure the regret of the learner as: \[_{T}^{}:=_{t=1}^{T}((S^{*},)-(S_{t},)),\ \ \ (S,):=_{i S} _{i}}{_{0}+_{j S}_{j}},\  S[K],\] (2) denotes \(S^{*}:=*{argmax}_{S[K]||S| m}(S,)\) is the optimal utility-maximizing subset. This objective corresponds to the standard objective in the MNL litterature .

## 3 A Practical and Efficient Algorithm for AOA with PL

In this section, we introduce our first algorithm, which works for both objectives.

### Algorithm Design

At each time \(t\), our algorithm (Alg. 1) maintains a pairwise preference matrix \(}_{t}^{n n}\), whose \((i,j)\)-th entry \(_{ij,t}\) records the empirical probability of \(i\) having beaten \(j\) in a pairwise , and a corresponding upper confidence bound \(p_{ij,t}^{}\). Let \([]:=[K]\{0\}\). We define for each pair \((i,j)[][]\),

\[p_{ij,t}^{}:=_{ij,t}+_{ij,t}(1- _{ij,t})x}{n_{ij,t}}}+}, _{ij,t}:=}{n_{ij,t}}\,,\] (3)

where \(w_{ij,t}=_{s=1}^{t-1}\{i_{s}=i,j S_{s}\}\) denotes the number of pairwise wins of item-\(i\) over \(j\) and \(n_{ij,t}=w_{ij,t}+w_{ji,t}\) being the number of times \((i,j)\) has been compared. The above UCB estimates \(p_{ij,t}^{}\) are further used to design UCB estimates of the PL parameters \(_{i}\) as follows

\[_{i,t}^{}=p_{i0,t}^{}/(1-p_{i0,t}^{})_{+}.\]

The estimates \(_{i,t}^{}\)s are then used to select the set \(S_{t}\), that maximizes the underlying objective. This optimization problem transforms into a static assortment optimization problem with upper confidence bounds \(_{i,t}^{}\) as the parameters, and efficient solution methods for this case are available (see e.g., ).

```
1:input:\(x>0\)
2:init:\( K+1\), \([]=[K]\{0\}\), \(_{1}_{}\)
3:for\(t=1,2,3,,T\)do
4: Set \(_{t}=_{t}+_{t}^{}\), and \(}_{t}=_{t}}{_{t}}\). Denote \(_{t}=[n_{ij,t}]_{}\) and \(}_{t}=[_{ij,t}]_{}\).
5: Define for all \(i\), \(p_{ii,t}^{}=\) and for all \(i,j[],i j\)
6:\(_{i,t}^{}:=p_{i0,t}^{}/(1-p_{i0,t}^{})_{+}\)
7:\(S_{t}m(\{_{1,t}^{},,_{K,t}^{}\}), \\ m\\ _{S[K]||S| m}\,(S,_{t}^{}), \\ m\)
8: Play \(S_{t}\)
9: Receive the winner \(i_{t}[]\) (drawn as per (1))
10: Update: \(_{t+1}=[w_{ij,t+1}]_{}\) s.t. \(w_{i,j,t+1} w_{i,j,t}+1\)\( j S_{t}\{0\}\)
11:endfor ```

**Algorithm 1****AOA for PL model with RB (AOA-RB\({}_{}\))**

### Analysis: Concentration Lemmas

We start the analysis by providing two technical lemmas, whose proofs are deferred to the appendix and that provide confidence bounds for the \(_{i}\).

**Lemma 1**.: _Let \(T 1\) and \(x>0\). Then, with probability at least \(1-3KTe^{-x}\), for all \(t[T]\) and \(i[K]\): \(_{i}_{i,t}^{}\) atleast one of the following two inequalities is satisfied_

\[n_{i0,t}<69x(_{0}+_{i})_{i,t}^{} _{i}+4(_{0}+_{i})_{i}x}{n_ {i0,t}}}++_{i})^{2}}{n_{i0,t}}\,.\]

The above lemma depends on \(n_{i0,t}\) the number of times items \(i\) have been compared with item \(0\) up to round \(t\). The latter is controlled using the following lemma:

**Lemma 2**.: _Let \(T 1\) and \(x>0\). Then, with probability at least \(1-KTe^{-x}\): simultaneously for all \(t[T]\) and \(i[K]\)_

\[_{i,t}<2x(_{0}+_{S^{*}})^{2}\ \ n_{i0,t}+_{i})_{i,t}}{2(_{0}+_{S^{*}})}\,,\] (4)

_where \(_{i,t}=_{s=1}^{t-1}\{i S_{s}\}\) denotes the number of rounds item \(i\) got selected before round \(t\)._

[MISSING_PAGE_FAIL:6]

**Remark 1** (Beyond MNL Models).: _Although, in this paper, we primarily focused on MNL based choice models, it is worth mentioning that our proposed algorithms can be generalized to more general random utility based models (RUMs)  pursuing the ideas from  that extends the RB based parameter estimation technique to any RUM\(()\) choice models. Our algorithms and analyses thus apply to any general RUM\(()\) based choice models; we stick to the special case of MNL models in this paper for brevity and keep the main focus on the AOA problem and the related algorithmic novelties._

Proof sketch of Thm. 5.: Let \(\) be the high-probability event such that both Lemma 1 and 2 are satisfied. Then,

\[_{T}^{}}}=_{t=1}^{T} (S^{*},)-(S_{t},)_{t=1}^{ T}((S^{*},)-(S_{t},)) \{\}+T(^{c})\] \[_{t=1}^{T}((S_{t}, _{t}^{}}})-(S_{t},))\{\}+T(^{c})\] (5)

because \((S_{t},_{t}^{}}})(S^{*}, _{t}^{}}})(S^{*},)\) under the event \(\) by Lemma 4. We now upper-bound the first term of the right-hand-side

\[_{t=1}^{T}(S_{t}, _{t}^{}}})-(S_{t},) \{\}=_{t=1}^{T}_ {i S_{t}}_{i,t}^{}}}}{_{0}+_ {S_{t}}}-_{i}}{_{0}+_{S_{t}}}\{ \}\] \[_{t=1}^{T}_{i S_{t }}(_{i,t}^{}}}-_{i})}{_{0}+ _{S_{t}}}\{\}\]

Because \(_{S_{t},t}^{}}}_{S_{t}}\) under the event \(\) by Lemma 1. Then, using \(r_{i} 1\), we further upper-bound using an exploration parameter \(_{0}=O((T))\) so that the upper-confidence-bounds in Lemmas 1 and 2 are satisfied

\[_{t=1}^{T}(S_{t}, _{t}^{}}})-(S_{t},) \{\}_{i=1}^{K}_{t=1}^ {T}^{}}}-_{i}|}{_{0}+ _{S_{t}}}\{i S_{t},\}\] \[ O(_{0})+_{i=1}^{K}^{T} \{i S_{t}\}}{_{0}+_{S_{t}}} }^{T} {_{i,t}^{}}}-_{i}}{_{0}+_{S_{t}}} ^{2}+_{S_{t}}}{_{i}}\{i S_{t },_{i,t}_{0},\}}_{=:A_{T}(i)}\] (6)

where the last inequality is by Cauchy-Schwarz inequality. Now, the term \(A_{T}(i)\) above may be upper-bounded using Lemmas 1 and 2,

\[A_{T}(i)=^{ }}}-_{i})^{2}}{_{i}(_{0}+_{S_{t}})}\{i S_ {t},_{i,t}_{0},\}_{t=1}^{T}+_{i})^{2}x}{n_{i0,t}(_{0}+_{S_{t}} )}\{i S_{t}\}\] \[_{}x_{t=1}^{T}+_{i})\{i S_{t}\}}{(_{0}+_{S_{t}})n_{ i0,t}}=_{}x_{t=1}^{T}\{i_{t}\{i,0\},i S_{t}\}}{n_{i0,t}}_{ }x T\]

where in the last inequality we used that \(_{n=1}^{T}n^{-1} 1+ T\). Substituting into (6), Jensen's inequality entails,

\[_{t=1}^{T}(S_{t},_{t}^{}}})-(S_{t},)\{\}  O(_{0})+x T}_{i=1}^{ K}^{T}\{i S_{t}\}}{_{0}+ _{S_{t}}}}\,.\] (7)The proof is finally concluded by applying Cauchy-Schwarz inequality which yields:

\[_{i=1}^{K}^{T}\{i S_{t}\}}{ _{0}+_{S_{t}}}}^{T}^{K}_ {i}\{i S_{t}\}}{_{0}+_{S_{t}}}}\,.\]

Finally, combining the above result with (5) and (7) concludes the proof

\[_{T}^{} TP(^{c})+O(_{0})+ xKT T}\,.\]

Choosing \(x=2 T\) ensures \(TP(^{c}) O(1)\) and \(_{0} O( T)\). 

## 4 Improved dependance on \(_{}\) with Adaptive Pivot Selection

A problem with Algorithm 1 stems from estimating all \(_{i}\) based on pairwise comparisons with item 0. When \(_{}_{0}=1\), item 0 may not be sampled enough as the winner, leading to poor estimators. This deficiency contributes to the suboptimal dependence on \(_{}\) observed in Theorems 3 and 5 and in prior work, such as . We propose the following fix to optimize the pivot. For all \(i,j[K]\{0\}\) we define \(_{ij}=}{_{j}}\), and the estimators:

\[_{ij,t}^{}=p_{ij,t}^{}/(1-p_{ij,t}^{})_{+}_{ii,t}^{}=1\,,\]

where \(p_{ij,t}^{}\) are defined in (3). For all rounds \(t\), the algorithm AOA-RB\({}_{}\)-Adaptive selects

\[S_{t}=*{argmax}_{|S| m}(S,_{t}^{ })_{i,t}^{}:= _{j[K]\{0\}}_{ij,t}^{}_{j0,t}^{}\,.\]

We offer below a regret bound that underscores the value of optimizing the pivot when \(_{} K\). Note that while the algorithm and analysis are presented for the weighted objective with winner feedback only, it can be adapted to other objectives by replacing \((S,)\) with the new objective in the analysis, as long as Lemma 4 remains valid.

**Theorem 6**.: _Let \(_{} 1\). For any \([0,_{}]^{K}\) and weights \(^{K}\), the weighted regret of AOA-RB\({}_{}\)-Adaptive is upper-bounded as_

\[_{T}^{}=O,K\} KT} T\]

_as \(T\) for the choice \(x=2 T\) (when definining \(p_{ij,t}^{}\))._

Asymptotically, when \(_{}\) is constant, the regret is \(O(K T)\), eliminating any dependence on \(_{}\). This allows for handling scenarios where the No-Choice item is highly unlikely, which is not achievable in previous works such as .  did attempt in their Thm. 4 to relax the assumption of \(_{}=_{0}\) and shows a bound of order \(O\{_{}/_{0},1\}^{1/2}\), which unfortunately blows to \(\) as \(_{0} 0\) or equivalently \(_{}\), leading to a vacuous bound. Here, lies the stark improvement and one of the key contributions, as also corroborated in our experimental evaluation Sec. 5 (Fig. 2).

The proof is deferred to the App. B, with a key step relying on selecting the pivot \(j_{t}=*{argmax}_{j S_{t}\{0\}}_{j}\). The use of \(_{i,t}^{}-_{i}_{ij,t }^{}-_{i}\) provides confidence upper-bounds with an improved dependence on \(_{}\), leveraging the fact that \(_{j_{t}}_{i}\). Due to the varying pivot over time, a telescoping argument introduces an additive factor \(\).

## 5 Experiments

We provide here a synthetic experiments. All results are averaged across 100 runs. We evaluate the performance of our main algorithm AOA-RB\({}_{}\)-Adaptive (Sec. 4), referred as "Our Alg-1 (Adaptive Pivot)", with the following two algorithms: AOA-RB\({}_{}\) (Sec. 3) referred as "Our Alg-2 (No-Choice Pivot)", and MNL-UCB, the state-of-the-art algorithm for AOA (, Alg. 1).

**Different PL (\(\)) Environments.** We report our experiment results on two datasets with \(K=50\) items: (1) Arith50 with PL parameters \(_{i}=1-(i-1)0.2,\  i\). (2) Bad50with PL parameters \(_{i}=0.6,\  i\{25\}\) and \(_{25}=0.8\). For simplicity of computing the assortment choices \(S_{t}\), we assume \(r_{i}=1,\  i[K]\).

**(1). Averaged Regret with weak NC (\(_{}/_{0} 1\)) (Fig. 1):** In our first experiment, we set set \(m=5\) and \(_{0}/_{}=0.01\) and report the average regret of the above three algorithms for our two objectives.

**(3). Averaged Regret vs Length of the rank-ordered feedback (\(k\)) (Fig. 3):** We also run a thought experiment to understand the tradeoff between learning rate with \(k\)-length rank-ordered feedback, where given any assortment \(S_{t}[K]\) of size \(m\), the learner gets to see the top-\(k\) draws (\(k m\)) from the PL model without replacement. This is a stronger feedback than the winner (i.e. top-1 for \(k=1\)) feedback and, as expected, we see in Fig. 3 an improved regret (for both notions) when increasing \(k\). The experiment are run on the Artith50 dataset with \(m=30\) and \(k\{1,2,4,8\}\).

## 6 Conclusion

We address the Active Optimal Assortment Selection problem with PL choice models, introducing a versatile framework (_AOA_) that eliminates the need for a strong default item, typically assumed as the No-Choice (NC) item in the existing literature. Our proposed algorithms employ a novel 'Rank-Breaking' technique to establish tight concentration guarantees for estimating the score parameters of the PL model. Our approach stands out for its practicality and avoids the suboptimal practice of repeatedly selecting the same set of items until the default item prevails. This is beneficial when the default item's quality (\(_{0}\)) is significantly lower than the quality of the best item (\(_{}\)). Our algorithms are computationally efficient, optimal (up to log factors), and free from restrictive assumptions on the default item.

**Future Works.** Among many interesting questions to address in the future, it will be interesting to understand the role of the No-Choice (NC) item in the algorithm design, precisely, can we design efficient algorithms without the existence of NC items with a regret rate still linear in \(_{}\)? Further, it will be interesting to extend our results to more general choice models beyond the PL model . What is the tradeoff between the subsetsize \(m\) and the regret for such general choice models? Extending our results to large (potentially infinite) decision spaces and contextual settings would also be a very useful and practical contribution to the literature of assortment optimization.