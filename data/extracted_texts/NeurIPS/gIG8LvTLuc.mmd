# How Does Adaptive Optimization Impact Local Neural Network Geometry?

Kaiqi Jiang

Department of Electrical and Computer Engineering

Princeton University

Princeton, NJ 08540

kaiqij@princeton.edu

&Dhruv Malik

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

dhruvm@andrew.cmu.edu

&Yuanzhi Li

Machine Learning Department

Carnegie Mellon University

Pittsburgh, PA 15213

yuanzhi1@andrew.cmu.edu

###### Abstract

Adaptive optimization methods are well known to achieve superior convergence relative to vanilla gradient methods. The traditional viewpoint in optimization, particularly in convex optimization, explains this improved performance by arguing that, unlike vanilla gradient schemes, adaptive algorithms mimic the behavior of a second-order method by adapting to the _global_ geometry of the loss function. We argue that in the context of neural network optimization, this traditional viewpoint is insufficient. Instead, we advocate for a _local_ trajectory analysis. For iterate trajectories produced by running a generic optimization algorithm OPT, we introduce \(R^{}_{}\), a statistic that is analogous to the condition number of the loss Hessian evaluated at the iterates. Through extensive experiments on language models where adaptive algorithms converge faster than vanilla gradient methods like SGD, we show that adaptive methods such as Adam bias the trajectories towards regions where \(R^{}_{}\) is small, where one might expect faster optimization. By contrast, SGD (with momentum) biases the trajectories towards regions where \(R^{}_{}\) is comparatively large. We complement these empirical observations with a theoretical result that provably demonstrates this phenomenon in the simplified setting of a two-layer linear network. We view our findings as evidence for the need of a new explanation of the success of adaptive methods, one that is different than the conventional wisdom.

## 1 Introduction

The efficient minimization of a parameterized loss function is a core primitive in statistics, optimization and machine learning. Gradient descent (GD), which iteratively updates a parameter vector with a step along the gradient of the loss function evaluated at that vector, is a simple yet canonical algorithm which has been applied to efficiently solve such minimization problems with enormous success. However, in modern machine learning, and especially deep learning, one frequently encounters problems where the loss functions are high dimensional, non-convex and non-smooth. The optimization landscape of such problems is thus extremely challenging, and in these settings gradient descent often suffers from prohibitively high iteration complexity.

To deal with these difficulties and improve optimization efficiency, practitioners in recent years have developed many variants of GD. One prominent class of these GD variants is the family of _adaptive_ algorithms . At a high level, adaptive methods scale the gradient with an adaptively selected preconditioning matrix, which is constructed via a moving average of past gradients. These methods are reminiscent of second order gradient descent, since they construct approximations to the Hessian of the loss functions, while remaining computationally feasible since they eschew full computation of the Hessian. A vast line of empirical work has demonstrated the superiority of adaptive methods over GD to optimize deep neural networks, especially on Natural Language Processing (NLP) tasks with transformers .

From a theoretical perspective, adaptive methods are well understood in the traditional context of convex optimization. For instance, Duchi et al.  show that when the loss function is convex, then the Adagrad algorithm yields regret guarantees that are provably as good as those obtained by using the best (diagonal) preconditioner in hindsight. The key mechanism that underlies this improved performance, is that the loss function has some global geometric property (such as sparsity or a coordinate wise bounded Lipschitz constant), and the algorithm adapts to this global geometry by adaptively selecting learning rates for features that are more informative.

However, in non-convex optimization, and deep learning in particular, it is highly unclear whether this simple characterization is sufficient to explain the superiority of adaptive methods over GD. Indeed, for large scale neural networks, global guarantees on the geometric properties of the loss are typically vacuous. For instance, for a 20-layer feedforward neural network, if we scale up the weights in each layer by a factor of \(1.5\), then the global Lipschitz constant of the network is scaled up by a factor of at least \(e^{10}\). Hence it only makes sense to study convergence by looking at the local geometry of the loss along the trajectory of the optimization algorithm .

Moreover, the interaction between an optimization algorithm and neural network geometry is highly complex -- recent work has shown that geometric characteristics of iterates encountered during optimization is highly dependent on the choice of optimization algorithm and associated hyperparameters . For instance, Cohen et al.  demonstrate that while training neural networks with GD, the maximum eigenvalue of the Hessian evaluated at the GD iterates first increases and then plateaus at a level that is inversely proportional to the step size. The viewpoint from convex optimization, where a loss function has some (potentially) non-uniform but fixed underlying geometry that we must adapt to, is thus insufficient for neural networks, since the choice of optimization algorithm can actually _interact_ with and _influence_ the observed geometry **significantly**.

To provide another example of this interactive phenomenon, we consider the following experiment. On the same network training loss function \(f\), we run stochastic gradient descent with momentum (SGD+M) and Adam to obtain two different trajectories. We select an iterate \(x_{}\) from the Adam trajectory and an iterate \(x_{}\) from the SGD trajectory, such that \(f(x_{})=f(x_{})\). We then run SGD+M with the same configuration twice, once from \(x_{}\) and once from \(x_{}\). If the underlying geometry of the loss function \(f\) was truly fixed, then we would not expect a significant difference in the performance of running SGD+M from either of the two iterates. However, as shown in Figure 2, there is a noticeable difference in performance, and running SGD+M from \(x_{}\) achieves lower

Figure 1: (left) Training losses of SGD+M starting from \(x_{}\) and \(x_{}\). (right) The 10th largest value over median in the diagonal of loss Hessian (which can be viewed as a variant of \(R^{}_{}(t)\) defined in eq. (1)) for Adam and SGD+M. Since the full Hessian is too big, here we selected several layers and randomly sampled 200 coordinates per layer to compute.

Figure 2: Training losses of Adam and SGD+M on the sentence classification task described in Section 4.1.

loss than running SGD+M from \(x_{}\). This suggests that Adam may bias the optimization trajectory towards a region which is more favorable for rapid training. This motivates the following question.

_How does adaptive optimization impact the observed geometry of a neural network loss function, relative to SGD (with momentum)?_

The remainder of this paper is dedicated to answering the above question. To this end, for each iterate in a trajectory produced by running an optimization algorithm OPT, where the Hessian of the \(t\)th iterate is given by \(H^{(t)}^{d d}\), we define the second order statistic \(R_{}^{}(t)\) in the following fashion. For the \(t\)th iterate in the trajectory, let \(R_{}^{}(t)\) be the ratio of _maximum_ of the absolute entries of the diagonal of \(H^{(t)}\), to the _median_ of the absolute entries of the diagonal of \(H^{(t)}\). Concretely, we define

\[R_{}^{}(t)=^{(t)}|\}_{i=1}^{d}}{\{|H_{ii}^{(t)}|\}_{i=1}^{d}}.\] (1)

This statistic thus measures the _uniformity_ of the diagonal of Hessian, where a smaller value of \(R_{}^{}(t)\) implies that the Hessian has a more uniform diagonal. It can also be viewed as a stable1 variant of the condition number. Instead of singular values, we choose diagonal entries because adaptive methods used in practice are _coordinate-wise_, which can be viewed as the diagonal scaling approaches.2 In Appendix A.9 we discuss this intuition in detail and compare \(R_{}^{}(t)\) with singular value-based metrics. As a supplementary result, in Appendix F, we demonstrate that the loss Hessian approaches diagonal during training for Adam and SGD+M. There has been prior theoretical work on overparameterized neural networks showing that a smaller condition number of Hessian, Neural Tangent Kernel  etc. could yield to faster convergence rate for (S)GD . As for (diagonal) adaptive methods (_e.g._ Adagrad), they were original designed to adapt to the nonuniform diagonal geometry. Intuitively, a smaller \(R_{}^{}(t)\), which implies more uniform diagonal geometry, could lead to faster convergence.

Armed with this statistic, we make the following contributions:

1. We focus on language models as on NLP tasks, adaptive algorithms show significantly faster convergence than SGD (with momentum). On a wide variety of neural network transformer architectures and language modeling datasets, we conduct experiments to compare how \(R_{}^{}(t)\) and \(R_{}^{}(t)\) evolve over time, when Adam and SGD+M are run from the same initialization and with their optimal (initial) learning rates respectively. In each case, we demonstrate that the Adam trajectory attains \(R_{}^{}(t)\) values that are significantly smaller than the \(R_{}^{}(t)\) values found by SGD+M. We show a simple example of this phenomenon in Figure 1(right). This suggests that relative to SGD+M, Adam biases the optimization trajectory to a region where the Hessian diagonal is more uniform. We call this phenomenon _the uniformity of diagonal geometry_ for adaptive methods. Moreover, we demonstrate that a more uniform Hessian diagonal, characterized by smaller \(R_{}^{}(t)\), is a contributing factor to faster optimization (see Section 4.3 for discussion). This suggests that a region where the Hessian diagonal is more uniform is also a region that is more amenable to rapid optimization.

2. We complement our empirical results with a theoretical analysis of this phenomenon in the simplified setting of large batch Adam and SGD+M, on a two-layer linear network with \(d\)-dimensional input and hidden layer, and one dimensional output. We show that for a wide range of \(t\), \(R_{}^{}(t)=1 o(1)\) but \(R_{}^{}(t)=( d)\). Our proof reveals that Adam induces the weight matrices to have low rank whose leading singular vectors have certain type of uniformity (see Section 6 for discussion), a fact that we also observe empirically in large scale neural networks, suggesting that this may be a mechanism by which adaptive methods bias trajectories to have uniformity of diagonal geometry.

## 2 Related work

**Existing analyses of adaptive methods.** The vast majority of prior theoretical work on adaptive methods has focused on the blackbox setting .

ENV21]. These works make minimal assumptions about the structure of the loss function, beyond (possibly) some global properties such as convexity or smoothness. These global properties (governed by parameters such as the smoothness parameter) are assumed to hold over the entire domain. Hence this style of analysis is worst case, since the resulting convergence bounds depend on polynomially on these global parameters. However, as we show in Section 3.1, in neural networks these parameters are prohibitively large. This worst case analysis is hence unlikely to explain the success of adaptive methods on neural networks. By contrast, our focus is on analyzing the local trajectory that is induced by running the optimization method.

**Existing analyses of (S)GD on neural networks.** There is an extensive literature on the analysis of GD/SGD in the non-blackbox setting, _e.g._ overparameterized neural networks, . However, it is unclear how to translate these analyses of GD/SGD, to an analysis that explains the gap between GD/SGD and adaptive methods.

**Influence of algorithms on the loss geometry.** In many simple convex settings, _e.g._ linear or logistic regression and the Neural Tangent Kernel , the loss geometry is usually fixed and not influenced by learning algorithms. However, in neural networks the interaction between algorithms and loss landscapes is more complicated. Lewkowycz et al.  find a so-called catapult effect of initial learning rate on the training trajectory of SGD and related loss curvature. Cohen et al.  demonstrate that while training neural networks with GD, the maximum eigenvalue of the Hessian evaluated at the GD iterates first increases and then plateaus at a level that is inversely proportional to the step size. However, Cohen et al.  leave open the problem of whether similar interactive phenomena occur in algorithms that are not GD, including adaptive methods.

## 3 Overview of main results

### Issues of prior analyses on adaptive methods

As is mentioned in Section 2, existing work on adaptive algorithms has mainly focused on black-box analysis assuming some global worst-case parameters. However, these global bounds can be extremely bad in complicated deep learning models, as is discussed in Section 1. To see this, we initialized a transformer model3 with default initialization in Pytorch but chose a large gain4, and computed the smoothness parameter (denoted as \(l\)) and the condition number (denoted as \(\)) of loss Hessian on one layer. We observed that setting the gain as a large constant (_e.g._ 800) results in extremely large \(l\) and \(\) (\(l 10^{7}\) and \( 10^{10}\)), which makes the convergence rates in prior black-box analysis vacuous.

The failure of global worst-case analysis implies that we need to focus on the local trajectory of algorithms. However, it is unclear whether when two optimization algorithms are used, they will have the same geometry in local trajectory or not. In particular, although in theory, adaptive algorithms can yield to a convergence rate with better dependency on certain local geometry of the function comparing to SGD (with momentum), it could still be the case that the local geometry along the trajectory of adaptive algorithm can be much worse than that of SGD (with momentum).

That motivates us to study the local geometry, especially that obtained by adaptive methods comparing to SGD (with momentum) in the paper. Motivated by the diagonal scaling of Adagrad and Adam for neural network training, we ask the follow **main question** in our paper:

How does the local diagonal geometry (diagonal of the loss Hessian) along the local trajectory of adaptive algorithms compare to that of SGD (with momentum)?

### Overview of the experiments

As is discussed in Section 1, we consider \(R^{}_{}(t)\) defined in eq. (1) as a measurement of the uniformity of the loss Hessian diagonal and conduct experiments on different NLP tasks where adaptive methods converge faster. The detailed experimental setup will be stated in Section 4. To explore potential different patterns of different layers, we do the computation layer by layer. On a wide variety of transformer architectures and language modeling datasets from the same initialization, we observe that for the vast majority of layers:

**When we train the neural network using Adam, the uniformity of diagonal geometry, measured by \(R_{}^{}(t)\) is smaller than that when we train using SGD+M from the same initialization.**

Table 1 shows a typical example of \(R_{}^{}(t)\) compared to \(R_{}^{}(t)\) on a sentence classification task using BERT-small  (See Section 4.1 for details). We repeated the experiments for 12 times starting from the same initialization. Table 1 shows the averaged \(R_{}^{}(t)\) and \(R_{}^{}(t)\) in some randomly selected layers. We also report the averaged \(}^{}(t)}{R_{}^{}(t)}\) and their standard deviations in the brackets.5 Figure 2 shows the corresponding training losses of one in these 12 experiments.

To understand this phenomenon in a more principled point of view, in Section 5 we provide a formal proof of the statement in a simplified setting: large batch Adam and SGD+M on a 2-layer linear network with 1-dimensional output.

## 4 The uniformity of diagonal geometry

As is mentioned in Section 3.2, we computed \(R_{}^{}(t)\) defined in eq. (1) on different language models. In this section, we present the results of SGD+M and Adam on different architectures and datasets. In Appendix A, we present the results of other adaptive algorithms.

During training we started from the same initial weights and used the same learning rate schedule (constant or decreasing) for SGD+M and Adam. We tuned and chose the best (initial) learning rate of SGD+M. The (initial) learning rate of Adam was set as a value under which Adam converged faster than SGD+M with its best learning rate. The concrete values will be stated in later parts of this section. We used large batch sizes to make the training procedure stable. When computing Hessian, we also used large batch sizes. Due to the extremely large dimension, we did the computation on some uniformly selected coordinates, more precisely, 200 coordinates per layer.

### Experiments on real datasets

**Sentence classification task on BERT-small.** We fine-tuned BERT-small  on the IMDB dataset : the task is to classify whether movie reviews are positive or negative.6 The momentum parameter \(\) in SGD was set as 0.9. The two momentum parameters \((_{1},_{2})\) of Adam were set as (0.9, 0.999). We trained the model using linearly decreasing learning rates for 10 epochs (2500 iterations). The initial learning rates of SGD+M and Adam were 0.001 and 5e-5, respectively. As mentioned in Section 3.2, Figure 2 and Table 1 show the training losses and the comparison between \(R_{}^{}(t)\) and \(R_{}^{}(t)\), respectively.

**Translation task.** We trained a Seq2Seq network that uses Transformer to solve a machine translation task on Multi30k : this task is to train a German to English translation model.7 The

   Layer\# &  &  &  \\  & \(}^{}(t)}{R_{}^{}(t)}\) & \(R_{}^{}(t)\) & \(R_{}^{}(t)\) & \(R_{}^{}(t)\) & \(R_{}^{}(t)\) & \(R_{}^{}(t)\) & \(R_{}^{}(t)\) & \(}^{}(t)}{R_{}^{}(t)}\) \\ 
9 & 15.7 & 15.7 & 12.76 & 9.65 & 1.45 (0.65) & 11.43 & 14.24 & 0.94 (0.40) \\ 
12 & 22.63 & 22.63 & 13.17 & 7.41 & 1.92 (0.67) & 10.62 & 9.67 & 1.33 (0.75) \\ 
15 & 9.35 & 9.35 & 80.57 & 53.52 & 1.65 (0.65) & 100.65 & 61.80 & 2.01 (1.00) \\ 
17 & 82.37 & 82.37 & 405.02 & 223.56 & 1.91 (0.53) & 423.28 & 337.32 & 1.43 (0.63) \\ 
18 & 31.32 & 31.32 & 17.07 & 13.24 & 1.43 (0.58) & 18.15 & 15.63 & 1.21 (0.36) \\ 
22 & 47.13 & 47.13 & 233.72 & 72.67 & 3.54 (1.21) & 158.38 & 93.13 & 2.28 (1.18) \\ 
24 & 31.17 & 31.17 & 17.52 & 17.34 & 1.13 (0.40) & 13.51 & 14.23 & 1.05 (0.36) \\   

Table 1: \(R_{}^{}(t)\) and \(R_{}^{}(t)\) in some layers, on the sentence classification task (see Section 4.1).

momentum parameter \(\) in SGD was set as 0.9. The two momentum parameters \((_{1},_{2})\) of Adam were set as (0.9, 0.98). We trained the model using constant learning rates (0.03 for SGD+M and 1e-4 for Adam) for 60 epochs (1800 iterations). The experiments were repeated for 8 times starting from the same initialization. Figure 3(left) shows the training losses for one among them. Table (a)a shows the averaged \(R_{}^{}(t)\), \(R_{}^{}(t)\) and \(}^{}(t)}{R_{}^{}(t)}\) (with standard deviation in the brackets) in some randomly selected layers.

### Experiments on random datasets

We used the same model and momentum parameters as in the translation task described in Section 4.1 but generated random integers as targets. Similar to the setting on real targets, the model was trained using constant learning rates (0.015 for SGD+M and 5e-5 for Adam) for 60 epochs (1800 iterations), and we repeated the experiments for 8 times starting from the same initialization. Figure 3(right) shows the training losses for one among them. Table (b)b shows the averaged \(R_{}^{}(t)\), \(R_{}^{}(t)\) and \(}^{}(t)}{R_{}^{}(t)}\) (with standard deviation in the brackets) of the same 10 layers as in Table (a)a.8

### Summarization of the empirical results and discussion

Overall, through extensive experiments on language models, we demonstrate that **starting from the same initialization, for the vast majority of layers, the \(R_{}^{}(t)\) values found by Adam are smaller than those found by SGD+M.** This suggests that Adam biases the trajectory towards a region with more uniform Hessian diagonal than SGD+M. In Appendix A.10 we also validate this observation on the in-distribution test data.

**Contribution of uniform Hessian diagonal to fast convergence.** We observe that on dataset with random targets, SGD+M plateaus after about 400 steps and thus converges much slower when compared to Adam than on real dataset (see Figure 3). On the other hand, the gaps of \(R_{}^{}(t)\) and \(R_{}^{}(t)\) are more significant on random data than on real data (see Table 2a and Table 2b) as well. In Appendix A.4, we conduct another experiment where we switch from SGD to Adam in the middle and compare it with the model trained by Adam from the beginning. The observation is that both the loss gap and the gap of \(R_{}^{}(t)\) are gradually closed after switching (see Figure 7 and Table 8). Hence we find a positive correlation between fast convergence and uniform Hessian diagonal (small \(R_{}^{}(t)\)). In Appendix A we **study other adaptive algorithms (Adagrad, RMSprop and AMSGrad) and get similar observation**: all these adaptive methods converge faster than SGD or SGD+M and also bias the trajectory to regions with smaller \(R_{}^{}(t)\), suggesting the universality of our observation.

Despite the above positive correlation, it is reasonable to ask whether small \(R_{}^{}(t)\) indeed contributes to fast convergence or is just a byproduct of adaptive methods. To address this concern, in Appendix B.1, we add a supplementary experiment similar to that in Figure 1. We select two iterates \(x_{1}\) and \(x_{2}\) from two trajectories that both come from SGD+M (instead of one from Adam and one from SGD+M in Figure 1), such that the loss \(f(x_{1})=f(x_{2})\) but \(x_{2}\) has a smaller \(R_{}^{}(t)\) than \(x_{1}\). We then run SGD+M with the same configuration twice, once from \(x_{1}\) and once from \(x_{2}\). Under this setting, we get similar observation as before: running SGD+M from \(x_{2}\) (with smaller \(R_{}^{}(t)\)) achieves faster convergence than from \(x_{1}\). This suggests that the uniformity of the diagonal of loss Hessian (measured by \(R_{}^{}(t)\)) reveals some intrinsic trajectory property beyond the algorithm choice and is indeed a contributing factor to fast optimization. In Appendix B.2 we theoretically prove the contribution of small \(R_{}^{}(t)\) to fast optimization in a simplified setting.

**More discussions on the trajectory difference.** Considering the fact that our comparison between \(R_{}^{}(t)\) and \(R_{}^{}(t)\) is conditioned on the same iteration when SGD+M has larger training loss than Adam, there is a potential alternative explanation of the Hessian diagonal uniformity. That is, the global minimum has uniform Hessian, and Adam simply converges faster to it than SGD+M. To rule out this possibility, in Appendix A.3 we add a comparison of our measurements \(R_{}^{}(t)\) and \(R_{}^{}(t^{})\), where \(t,t^{}\) are picked such that \(t\)th Adam iterate and \(t^{}\)th SGD+M iterate have the _same training loss_. The results (in Table 7) show that \(R_{}^{}(t)<R_{}^{}(t^{})\) for most layers, thus demonstrating that the trajectories of Adam and SGD+M are truly different and that the difference is because Adam biases the local geometry (as opposed to faster convergence).

**Adding regularization.** People in practice usually add weight decay (equivalent to \(l_{2}\) regularization) to encourage better generalization ability. In Appendix A.7 we compare SGD+M and Adam when both using small weight decay values (0.001). The results in Figure 1(a) and Table 9 suggest that in this case, our observation still holds: Adam converges faster than SGD+M and in the vast majority of layers, \(R_{}^{}(t)\) values are smaller than \(R_{}^{}(t)\). This reveals the robustness of our observation under weak regularization. However, under large weight decay parameters, we observed cases where Adam still converged faster but \(R_{}^{}(t)\) values were larger rather than smaller. In the case of strong regularization, the adaptivity of Adam requires further exploration and we hope to find new mechanisms in the future.

## 5 Theoretical analysis

In Section 4, we empirically demonstrate the uniformity of diagonal geometry. In this section, we theoretically analyze this property for large batch Adam and SGD+M on a two-layer linear network with 1-dimensional output. Although simple, the choice of 2-layer linear networks to understand learning dynamics is common in prior works (_e.g._). Moreover, in language transformer models when the weights are small, the softmax in the key-value-query structure is near the linear regime. Then this structure might be approximated by the product of 3 linear operators, similar to a three-layer linear network. Hence the theoretical analysis of this phenomenon on linear networks would be a good starting point for further understanding of more complicated language models.

### Problem setup

NotationLet \([d]=\{1,2,...,d\}\). We use \(\|\|_{2}\) to denote the \(l_{2}\) norm of a vector, and \(\|\|_{F}\) to denote the Frobenius norm of a matrix. Let \(,\) be the Euclidean inner product between vectors or matrices. Let \((,^{2})\) be the one-dimensional Gaussian distribution with mean \(\) and variance \(^{2}\). For a scalar (vector, matrix) \(A\) which evolves over time, we use \(A^{(t)}\) to denote its value at time \(t\).

Let there be \(m\) data points. The data matrix is \(X^{d_{x} m}\) and the label matrix is \(Y^{d_{y} m}\). We assume that the input dataset is whitened, i.e. \(_{xx}:=XX^{T}^{d_{x} d_{x}}\) is an identity matrix.

The parameters of a 2-layer linear network are given by \(W:=(W_{2},W_{1})\). Assume \(W_{i}^{d_{i} d_{i-1}}\) for \(i=1,2\). We have \(d_{2}=d_{y},d_{0}=d_{x}\). We consider the square loss \(L(W):=\|W_{2}W_{1}X-Y\|_{F}^{2}\).

Denote \(A:=YX^{T}^{d_{y} d_{x}}\). [AGCH19] show that with whitened dataset,

\[L(W):=\|W_{2}W_{1}X-Y\|_{F}^{2}=(W)+c,(W):= \|W_{2}W_{1}-A\|_{F}^{2}.\] (2)

where \(c\) does not depend on \(W\). We consider the following model with small Gaussian initialization.

**Assumption 1** (Setup).: _The input covariance \(_{xx}:=XX^{T}^{d_{x} d_{x}}\) is an identity matrix. The input and hidden layers are both of dimension \(d\), i.e. \(d_{1}=d_{0}=d\). Without loss of generality, we can assume that \(A\) is a row vector (i.e. \(d_{2}=1\)) whose coordinates are positive9 and \((1)\) in terms of \(d\)._

**Assumption 2** (Gaussian Initialization).: \( i,j:w_{2i}^{(0)}(0,}),W_{1}^{(0)} [i,j](0,})\) _are independently initialized with sufficiently large \(>0\)._

Denote \(\) and \(_{xx}\) as the batch versions of \(A\) and \(_{xx}\). We make the following large-batch assumption. We emphasize that large batches are commonly used in NLP tasks (_e.g._ [BMR\({}^{+}\)20]).

**Assumption 3** (Large Batch).: _For the randomly selected batches, assume \([]=A\), \([_{xx}]=_{xx}\). \( i,j[d]:[(_{i}-A_{i})^{2}]^{2}\), \([(_{xx}[i,j]-_{xx}[i,j])^{2}] ^{2}\), and \(^{2}=()\)._

Denote \(^{(t)}\) as the batch gradient at time \(t\). The update rules of SGD+M and Adam are given by

SGD+M: \[u^{(t+1)}= u^{(t)}+^{(t)}, W^{(t+1)}=W^{(t)}-  u^{(t)},\] Adam: \[_{t}=^{t+1}}/(1-_{1}^{t+1}),  m^{(t+1)}=_{1}m^{(t)}+(1-_{1})^{(t)},\] (3) \[v^{(t+1)}=_{2}v^{(t)}+(1-_{2})^{(t)} ^{(t)}, W^{(t+1)}=W^{(t)}-_{t}}{}+ },\]

where \(\) is the learning rate, \(,_{1},_{2}\) are momentum parameters, and \(\) is for numerical stability. All operations on vectors are element-wise. Here and throughout, the notation \(f(x)=(g(x))\) (resp. \(f(x)=(g(x)),f(x)=(g(x))\)) means that \( C_{1},C_{2}>0\) such that \(f(x) C_{2}g(x)\) (resp. \(f(x) C_{1}g(x)\), \(C_{1}g(x) f(x) C_{2}g(x)\)). Here \(C_{1},C_{2}\) may depend on \(,_{1},_{2}\). We will also use the notation with \(\), i.e. \(}(),(),()\) to hide factors of order \( d\). In our theoretical analysis, "with high probability", or "w.h.p." for short, means that with probability at least \(1-\).

Since the weights and Hessians in different layers may have different magnitudes, we compute the \(R_{,k}^{}(t)\) layer by layer. Denote \(R_{,k}^{}(t)\) (resp. \(R_{,k}^{}(t)\)) as the \(R_{}^{}(t)\) found by SGD+M (resp. Adam) w.r.t. \(W_{k}\) at time \(t\) where \(k=1,2\).

### Main results

**Theorem 1**.: _Under Assumption 1, 2 and 3, consider the weights \(\{W_{}^{(t)}\}_{t 0}\) (resp. \(\{W_{}^{(t)}\}_{t 0}\)) obtained by SGD+M (resp. Adam) defined in (3)._

_1. For any_ \(p>0\)_, pick_ \(0<<}\)_,_ \((})\) _and_ \( 4(p+2)\)_. Suppose_ \(}{d^{/2+1}}\)_, then there exists_ \(T_{,1},T_{,2}\) _such that w.h.p.,_ \((W_{}^{(T_{,1})})=(d)\)_,_ \((W_{}^{(T_{,2})})} (})\)_, and_

\[ t[T_{,1},T_{,2}]: R_{,k}^{ }(t)=( d), k=1,2.\]_2. For any \(p>0\), pick \((}),}}\), \(\) and \(_{2}=_{1}^{2}\).10 Suppose \(^{2}}{d^{13/4}}\). Then \( T_{,1},T_{,2}\) such that w.h.p., \((W_{,1}^{(T_{,1})})=(d)\), \((W_{}^{(T_{,2})})}(})\), and_

\[ t[T_{,1},T_{,2}]: R_{,k}^{ }(t)=1+}(^{}+-}}), k=1,2.\]

RemarkTheorem 1 holds for all values of hyperparameters (such as \(,\)) in certain ranges instead of just particular values. The ranges of SGD+M and Adam overlap with each other. That means we can choose the same hyperparameters for SGD+M and Adam in the overlapped region to make a fair comparison, for example, same \(,\) such that \( 4(p+2)\) and \(\{}{d^{/2+1}},^{2}}{d^{ 13/4}}\}\).

An immediate corollary of this theorem below gives the difference between iterates of Adam and SGD+M that have the same loss.

**Corollary 1**.: _Under the setup in Theorem 1, w.h.p., for any \(t[T_{,1},T_{,2}]\) and \(t^{}[T_{,1},T_{,2}]\) such that \((W_{}^{(t)})=(W_{}^{(t^{ })})[(}),(d)]\), we have_

\[R_{,k}^{}(t)=( d), R_{,k}^{ }(t^{})=1+}(^{}+-}}), k=1,2.\]

Theorem 1 and Corollary 1 tell us that during a long training period when the loss decreases from \((d)\) to \(}(})\), the diagonal of loss Hessian for Adam keeps nice uniformity: \(R_{,k}^{}(t)=1 o(1),k=1,2\). On the other hand, the diagonal of loss Hessian for SGD+M is less uniform. Appendix C gives a proof sketch of Theorem 1. The detailed proof can be found in Appendix D and E.

## 6 Low-rank weight matrices and uniformity of leading singular vectors

The proof sketch in Appendix C highlights one crucial intuition of Theorem 1: After \(T_{,1}\) (resp. \(T_{,1}\)) steps, \(W_{1}\) of SGD+M (resp. Adam) becomes an approximately rank-1 matrix. Consider the left singular vector \(:=[u_{1},u_{2},...,u_{d}]^{T}\) which corresponds to the leading singular value \(_{1}\). We can show that the distribution of \(u_{1}^{2},u_{2}^{2},...,u_{d}^{2}\) for Adam is more uniform than that of SGD+M. This property, we call _the uniformity of the leading singular vector_, is related to the uniformity of the diagonal of loss Hessian, see Appendix G for more details.

Similar low rank bias after training has been studied in prior works (_e.g._[1, 17, 18, CGMR20]). For more complicated models, we want to check whether the weight matrices also have low rank structures and if so, whether we can still observe _the uniformity of leading singular vectors_. More formally, consider the weight matrix in some layer \(W^{m n}\), we want to check

(A) Whether \(W^{m n}\) is approximately a rank \(k\) matrix with \(k\{m,n\}\), and if true,

(B) Consider the top \(k\) singular values \(_{1},...,_{k}\) and left singular vectors \(_{1},...,_{k}\). Define \(}:=_{i=1}^{k}_{i}^{2}_{i}_{i}:=[ _{1},...,_{d}]^{T}\) and compute \(R_{u}:=_{i}}{_{i}}\), a generalized version of \(u_{i}^{2}}{\ u_{i}^{2}}\) in the rank-1 case. We want to see whether \(R_{u}\) obtained by Adam is smaller than that of SGD+M.

After reviewing the weight matrices we got in different settings, we observed that (A) and (B) hold for many layers in those models. For example, on the translation task mentioned in Section 4.1, we found 12 layers which had approximately low rank structures and for 10 of them, \(R_{u}\) values (defined in (B)) obtained by Adam were smaller than those found by SGD+M. Figure 4 shows the result on one typical layer. Results of more layers can be found in Appendix A.5.

RemarkThe definition of \(R_{u}\) is based on the connection between diagonal of loss Hessian and weight matrices. Appendix G shows that for a 2-layer linear network, \(R_{,2}^{}(t)=\|W_{1}^{(t)}[:,]\|_{2}^{2} }{\|W_{1}^{(t)}[:,]\|_{2}^{2}}\). When \(W_{1}^{m n}\) is approximately rank \(k\), i.e. \(W_{1}_{i=1}^{k}_{i}_{i}_{i}^{T}\), denote \(_{i}=[u_{i1},u_{i2},...,u_{im}]^{T}\) and \(_{i}=[v_{i1},v_{i2},...,v_{in}]^{T}\), we have that for the \(j\)-th row,\(\|W_{1}[j,:]\|_{2}^{2}\ \ \|_{i=1}^{k}_{i}u_{ij}_{i}^{T} \|_{2}^{2}=_{i=1}^{k}_{i}^{2}u_{ij}^{2}\). By defining \(}=[_{1},_{2},...,_{d}]^{T}:=_{i=1} ^{k}_{i}^{2}_{i}_{i}\), we have that \(\|W_{1}[j,:]\|_{2}^{2}_{j}\). Although in multi-layer nonlinear neural networks, the connection between diagonal of loss Hessian and the weight matrices is more complicated and \(R_{,2}^{}(t)\) may depend on the product of many weight matrices rather than one single matrix, we still believe that this definition of \(R_{u}\) is a reasonable ratio to consider.

## 7 Conclusion and future work

We demonstrate that adaptive optimization methods bias the training trajectory towards a region where the diagonal of loss Hessian is more uniform, through extensive experiments on language models and theoretical analysis in a simplified setting of two-layer linear networks. Although our findings may not directly lead to an improved algorithm for practical use, they provide a new way of thinking when designing new algorithms: in contrast with the traditional view which tries to design a method that performs better _in_ the bad loss geometry, our findings suggest that we can design algorithms which _implicitly avoid_ regions with bad geometry. There are a lot of future directions along this line. For example, our theoretical results on the two-layer linear networks may be able to generalize to multi-layer networks. As is discussed in Section 5, the key-value-query structure in the transformer models might be approximated by a three-layer linear network and the analysis of multi-layer networks might provide more connection to real deep models and could be an interesting and challenging future direction. Moreover, it is also possible to relax our large-batch assumption (Assumption 3) and prove similar results in the general stochastic setting.