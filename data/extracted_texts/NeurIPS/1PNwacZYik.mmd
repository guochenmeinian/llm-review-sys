# FastDrag: Manipulate Anything in One Step

Xuanjia Zhao\({}^{1}\), Jian Guan\({}^{1,}\), Congyi Fan\({}^{1}\), Dongli Xu\({}^{4}\),

**Youtian Lin\({}^{2}\), Haiwei Pan\({}^{1}\), Pengming Feng\({}^{3}\)**

Corresponding author: j.guan@hrbeu.edu.cn

###### Abstract

Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks. However, prevailing methods typically adopt \(n\)-step iterations for latent semantic optimization to achieve drag-based image editing, which is time-consuming and limits practical applications. In this paper, we introduce a novel one-step drag-based image editing method, _i.e._, FastDrag, to accelerate the editing process. Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space. This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds. Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy. This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity. Additionally, a consistency-preserving strategy is introduced to maintain the consistency between the edited and original images by adopting semantic information from the original image, saved as key and value pairs in self-attention module during diffusion inversion, to guide the diffusion sampling. Our FastDrag is validated on the DragBench dataset, demonstrating substantial improvements in processing time over existing methods, while achieving enhanced editing performance. Project page: https://fastdrag-site.github.io/.

## 1 Introduction

The drag editing paradigm [28; 15; 20] leverages the unique properties of generative models to implement a point-interaction mode of image editing, referred to as drag-based image editing. Compared with text-based image editing methods [18; 10; 3; 30], drag-based editing enables more precise spatial control over specific regions of the image while maintaining semantic logic coherence, drawing considerable attention from researchers.

However, existing methods typically involve \(n\)-step iterative semantic optimization in latent space to obtain optimized latent with desired semantic based on the user-provided drag instructions. They focus on optimizing a small region of the image at each step, requiring \(n\) small-scale and short-distance adjustments to achieve overall latent optimization, leading to a significant amount of time. These optimization approaches can be categorized primarily into motion-based [23; 28; 15; 16; 32; 4] and gradient-based [20; 19]\(n\)-step iterative optimizations, as shown in Fig. 1(a). \(n\)-step iterations in motion-based methods are necessary to avoid abrupt changes in the latent space, preventing imagedistortions and ensuring a stable optimization process. This is exemplified in DragDiffusion  and GoodDrag , which require 70 to 80 iterations of point tracking and motion supervision for optimization. In addition, gradient-based methods align sampling results with the drag instructions through gradient guidance . In this way, they also require multiple steps due to the optimizer [13; 25] needing multiple iterations for non-convex optimization. For instance, DragonDiffusion  requires around 50 gradient steps to accomplish the latent optimization. Therefore, existing drag-based image editing methods often suffer from significant time consumption due to \(n\)-step iterations required for latent semantic optimization, thus limiting the practical applications.

To this end, we present a novel one-step drag-based image editing method based on diffusion, _i.e._, FastDrag, which significantly accelerates editing speeds while maintaining the quality and precision of drag operations. Specifically, a novel one-step wrapper optimization strategy is proposed to accelerate editing speeds, which can achieve the latent semantic optimization in a single step with an elaborately designed latent warpage function (LWF), instead of using motion or gradient-based \(n\)-step optimizations, as illustrated in Fig. 1(b). By simulating strain patterns in stretched materials, we treat drag instructions on the noisy latent as external forces stretching a material, and introduce a stretch factor in LWF, which enables the LWF to generate warpage vectors to adjust the position of individual pixels on the noisy latent with a simple latent relocation operation, thus achieving one-step optimization for drag-based editing. Meanwhile, a bilateral nearest neighbor interpolation (BNNI) strategy is proposed to enhance the semantic integrity of the edited content, by interpolating null values using similar features from their neighboring areas to address semantic losses caused by null regions emerging after latent relocation operation, thus enhancing the quality of the drag editing.

Additionally, a consistency-preserving strategy is introduced to maintain the consistency of the edited image, which adopts the original image information saved in diffusion inversion (_i.e._, key and value pairs of self-attention in the U-Net structure of diffusion model) to guide the diffusion sampling for desired image reconstruction, thus achieving precise editing effect. To further reduce time consumption for inversion and sampling, the latent consistency model (LCM)  is employed in the U-Net architecture of our diffusion-based FastDrag. Therefore, our FastDrag can significantly accelerate editing speeds while ensuring the quality of drag effects.

Experiments on DragBench demonstrate that the proposed FastDrag is the fastest drag-based editing method, which is nearly 700% faster than the fastest existing method (_i.e._, DiffEditor ), and 2800% faster than the typical baseline method (_i.e._, DragDiffusion ), with comparable editing performance. We also conduct rigorous ablation studies to validate the strategies used in FastDrag.

**Contributions:** 1) We propose a novel drag-based image editing approach based on diffusion _i.e._, FastDrag, where a LWF strategy is proposed to achieve one-step semantic optimization, tremendously enhancing the editing efficiency. 2) We propose a novel interpolation method (_i.e._, BNNI), which effectively addresses the issue of null regions, thereby enhancing the semantic integrity of the edited content. 3) We introduce a consistency-preserving strategy to maintain the image consistency during editing process.

## 2 Related Work

### Text-based Image Editing

Text-based image editing has seen significant advancements, allowing users to manipulate images through natural language instructions. DiffusionCLIP  adopts contrastive language-image pre

Figure 1: (a) Existing methods usually require multiple iterations to transform an image from its original semantic to desired semantic; (b) Our method utilizes latent warpage function (LWF) to calculate the warpage vectors (_i.e._, \(_{y}\)) to move each individual pixel on feature map and achieve semantic optimization in one step.

training (CLIP)  for diffusion process fine-tuning to enhance the diffusion model, enabling high-quality zero-shot image editing. The study in  manipulates the cross-attention maps within the diffusion process and achieves text-based image editing. Imagic  further enhances these methods by optimizing text embeddings and using text-driven fine-tuning of the diffusion model, enabling complex semantic editing of images. InstructPix2Pix  leverages a pre-trained large language model combined with a text-to-image model to generate training data for a conditional diffusion model, allowing it to edit images directly based on textual instructions during forward propagation. Moreover, Null-text Inversion  enhances text-based image editing by optimizing the default null-text embeddings to achieve desired image editing. Although text-based image editing methods enable the manipulation of image content using natural language description, they often lack the precision and explicit control provided by drag-based image editing.

### Drag-based Image Editing

Drag-based image editing achieves precise spatial control over specific regions of the image based on user-provided drag instructions. Existing drag-based image editing methods generally rely on \(n\)-step latent semantic optimization in latent space to achieve image editing. These methods fall into two main categories: motion-based [23; 28; 32; 4; 16; 15; 9] and gradient-based [20; 19] optimizations. For example, DragGAN  employs generative adversarial network (GAN) for drag-based image editing with iterative point tracking and motion supervision steps. However, the image quality of the methods using GAN for image generation is worse than diffusion models . Therefore, a series of diffusion-based methods have been proposed for drag-based image editing. For instance, DragDiffusion  employs iterative point tracking and motion supervision for latent semantic optimization to achieve drag-based editing. Building on this foundation, GoodDrag , StableDrag , DragNoise , and FreeDrag  have made significant improvements to the motion-based methods. Without coincidence, by utilizing feature correspondences, DragonDiffusion  and its improved version DiffEditor  formulate an energy function that conforms to the desired editing results, thereby transforming the image editing task into a gradient-based process that enables drag-based editing. However, these methods inherently require \(n\)-step iterations for latent optimization, which significantly increases the time consumption. Although SDEDrag  does not require \(n\)-step iterative optimization, it is still time-consuming due to the stochastic differential equation (SDE) process for diffusion. In addition, while EasyDrag  offers user-friendship editing, its requirement for over 24GB of memory (_i.e._, a 3090 GPU) limits its broad applicability. To this end, based on latent diffusion model (LDM) , we propose a novel one-step optimization method that substantially accelerates the image editing speeds.

## 3 Proposed Method

FastDrag is based on LDM  to achieve drag-based image editing across four phases. The overall framework is given in Fig. 2, and the detailed description of strategies in FastDrag are presented as follows: (1) Initially, FastDrag is based on a traditional image editing framework including diffusion inversion and sampling processes, which will be elaborated in Sec. 3.1. (2) The core phase in Sec. 3.2 is a one-step wurpage optimization, employing LWF and a latent relocation operation to simulate the behavior of stretched material, allowing for fast semantic optimization. (3) BNNI is then applied in Sec. 3.3 to enhance the semantic integrity of the edited content, by interpolating the null regions emerging after the one-step wurpage optimization. (4) The consistency-preserving strategy is introduced in Sec. 3.4 to maintain the desired image consistency with original image, by utilizing the key and value of self-attention in inversion to guide the sampling.

### Diffusion-based Image Editing

Similar to most existing drag editing methods [28; 32; 20], FastDrag is also built upon diffusion model (_i.e._, LDM), including diffusion inversion and diffusion sampling.

**Diffusion Inversion** is about mapping a given image to its corresponding noisy latent representation in the model's latent space. We perform semantic optimization on the noisy latent \(_{t}^{w h c}\), due to it still captures the main semantic features of the image but is perturbed by noise, making it suitable as a starting point for controlled modifications and sampling . Here, \(w\), \(h\), \(c\) represent the width, height and channel of \(_{t}\), respectively. This process for a latent variable at diffusion step \(t\) can be expressed as:

\[_{t}=}}{}}(_{t-1}-}_{t})+}_{t},\] (1)

where \(z_{0}=(_{0})\) denotes the initial latent of the original image \(_{0}\) from the encoder \(()\). \(_{t}\) is the noise variance at diffusion step \(t\), and \(_{t}\) is the noise predicted by U-Net. Subsequently, we perform a one-step warpage optimization on \(_{t}\) in Sec. 3.2.

**Diffusion Sampling** reconstructs the image from the optimized noisy latent \(_{t}^{}\) by progressively denoising it to the desired latent \(_{0}^{}\). This sampling process can be formulated as:

\[_{t-1}^{}=}(_{t}^{}- }_{t}}{}})+-^{2}}_{t}+^{2},\] (2)

where \(\) is the Gaussian noise and \(\) denotes the noise level. By iterating the process from \(t\) to 1, \(_{0}^{}\) is reconstructed, and the desired image can be obtained by \(_{0}^{}=(_{0}^{})\), with \(()\) being the decoder .

### One-step Warpage Optimization

Building upon the phases in Sec.3.1, we propose a one-step warpage optimization for fast drag-based image editing. The core idea involves simulating strain patterns in stretched materials, where drag instructions on the noisy latent are interpreted as external forces stretching the material. This enables us to adjust the position of individual pixels on the noisy latent, optimizing the semantic of noisy latent in one step, thus achieving extremely fast drag-based editing speeds. To this end, we design the LWF in Sec. 3.2.1 to obtain warpage vector, which is utilized by a straightforward latent relocation operation in Sec. 3.2.2 to adjust the position of individual pixels on the noisy latent.

#### 3.2.1 Warpage Vector Calculation using LWF

In drag-based image editing, each drag instruction \(_{i}\) in a set of \(k\) drag instructions \(=\{_{i} i=1,,k;k\}\) can simultaneously influence a feature point \(p_{j}\) on the mask region \(=\{p_{j} j=1,,m;m\}\) provided by the user. As shown in Fig. 3, the mask region is represented by the brighter areas in the image, indicating the specific image area to be edited. To get a uniquely determined vector, _i.e._, warpage vector \(}\) to adjust the position of feature point \(p_{j}\) (will be discussed in Sec.3.2.2), we propose a latent warpage function \(f_{LWF}()\) to aggregate multiple component warpage vectors caused by different drag instructions, _i.e._, \(_{j}^{}\), with balanced weights to avoid

Figure 2: Overall framework of FastDrag with four phases: diffusion inversion, diffusion sampling, one-step warpage optimization and BNNL Diffusion inversion yields a noisy latent \(}\) and diffusion sampling reconstructs the image from the optimized noisy latent \(_{t}^{}\). One-step warpage optimization is used for noisy latent optimization, where LWF is proposed to generate warpage vectors to adjust the location of individual pixels on the noisy latent with a simple latent relocation operation. BNNI is used to enhance the semantic integrity of noisy latent. A consistency-preserving strategy is introduced to maintain the consistency between original image and edited image.

deviating from the desired drag effect. The function is given as follows:

\[_{j}=f_{LWF}(,,j)=_{i}^{k}w_{j}^{i}_{j}^{*},\] (3)

where \(w_{j}^{i}\) is the normalization weight for component warpage vector \(_{j}^{i*}\). Here, drag instruction \(_{i}\) is considered as a vector form handle point \(s_{i}\) to target point \(e_{i}\). During dragging, we aim for the semantic changes around the handle point \(s_{i}\) to be determined by the corresponding drag instruction \(_{i}\), rather than other drag instructions far from the \(s_{i}\). Therefore, \(w_{j}^{i}\) is calculated as follows:

\[w_{j}^{i}=s_{i}|}{_{i}^{k}(1/|p_{j}s_{i}|)},\] (4)

where \(s_{i}\) is considered as the "point of force" of \(_{i}\), and the weight \(w_{j}^{i}\) is inversely proportional to the Euclidean distance from \(s_{i}\) to \(p_{j}\).

It is worth noting that under an external force, the magnitude of component forces at each position within the material is inversely proportional to the distance from the force point, while the movement direction at each position typically aligns with the direction of the applied force . Similarly, the component warpage vector \(_{j}^{i*}\) on each \(p_{j}\) aligns with the direction of drag instruction \(_{i}\), and magnitudes of \(_{j}^{i*}\) are inversely proportional to the distance from \(s_{i}\). Hence, \(_{j}^{i*}\) can be simplified as:

\[_{j}^{i*}=_{j}^{i}_{i},\] (5)

where \(_{j}^{i}\) is the stretch factor that denotes the proportion between \(_{j}^{i*}\) and \(_{i}\).

To appropriately obtain the stretch factor \(_{j}^{i}\) and facilitate the calculation, we delve into the geometric representation of the component warpage vector \(_{j}^{i*}\). As shown in Fig. 3, \(_{j}^{i*}\) can be depicted as the guidance vector from point \(p_{j}\) to point \(p_{j}^{i*}\), where \(p_{j}^{i*}\) is the expected new position of \(p_{j}\) under the drag effect of \(_{i}\). Recognizing that the content near to mask edge should remain unaltered, we define a reference circle \(O\) where every \(_{j}^{i*}\) will gradually reduce to \(0\) as \(p_{j}\) approaches the circle. Consequently, since \(_{j}^{i*}\) and \(_{i}\) are parallel, magnitudes of \(_{j}^{i*}\) are inversely proportional to the distance from \(s_{i}\) and \(_{j}^{i*}\) is reduced to \(0\) on circle \(O\), the extended lines from \(s_{i}p_{j}\) and \(e_{i}p_{j}^{i*}\) will intersect at \(q_{j}^{i}\) on circle \(O\). Hence, based on the Eq. (5) and the geometric principle in Fig. 3, we calculate \(_{j}^{i}\) as follows:

\[_{j}^{i}=_{j}^{i*}|}{|_{i}|}=p _{j}^{i*}}|}{|s_{i}^{j}}|}=q_{j}^{i}|}{|s_{i}q_{j} ^{i}|}.\] (6)

Finally, we obtain the warpage vector \(_{j}\) using only \(_{i}\) and two factors as follows:

\[f_{LWF}(,,j)=_{i}^{k}w_{j}^{i}_{j}^{i} _{i}\] (7)

Note that, for the special application of drag-based editing, such as object moving as shown in Fig. 8, drag editing is degenerated to a mask region shifting operation, requiring the spatial semantics of the mask region to remain unchanged. In that case, we only process a single drag instruction, and all component drag effects will be set equal to the warpage vector, _i.e._, \(_{j}=_{1}\) and \(=\{_{1}\}\).

#### 3.2.2 Latent Relocation with Warpage Vector

Consequently, we utilize the warpage vector \(v_{j}\) to adjust the position of feature point \(p_{j}\) via a latent relocation operation \(F_{WR}\), achieving the semantic optimization of noisy latent for drag-based editing.

Figure 3: Geometric representation of \(_{j}^{i*}\). Circle \(O\) is the circumscribed circle of the circumscribed rectangle enclosing the mask’s shape. \(p_{j}\) is the feature point requiring relocation, and \(p_{j}^{i*}\) is its new position following the drag instruction \(_{i}\)

Establishing a Cartesian coordinate system on the latent space, let \((x_{p_{j}},y_{p_{j}})\) denote the position of point \(p_{j}\) within this coordinate system. The new location of the point \(p_{j}^{*}\) after applying the vector \(_{j}=(v_{j}^{x},v_{j}^{y})\) can be written as:

\[(x_{p_{j}}^{*},y_{p_{j}}^{*})=(x_{p_{j}},y_{p_{j}})+(v_{j}^{x},v_{j}^{y})\] (8)

Then the new coordinates set \(\) of all feature points in \(\) can be written as:

\[=F_{WR}(,})=\{(x_{p_{j}}^{*},y_{p_{j}}^{*})|(x_{p_{ j}}^{*},y_{p_{j}}^{*})=(x_{p_{j}},y_{p_{j}})+(v_{j}^{x},v_{j}^{y});j=1,,m\},\] (9)

where \(}=\{_{j}|j=1,,m,m\}\). If \((x_{p_{j}},y_{p_{j}})\) has already been a new position for a feature point, it no longer serves as a new position for any other points. Consequently, by assigning corresponding values to these new positions, the optimized noisy latent \(_{t}^{}\) can be obtained as shown in the following equation:

\[_{t}^{}\,_{(x_{p_{j}}+v_{j}^{x},y_{p_{j}}+v_{j}^{y})}=_{t(x_{p_{j}},y_{p_{j}})}\] (10)

In essence, the latent relocation operation optimizes semantics efficiently by utilizing the LWF-generated warpage vector, eliminating the need for iterative optimization.

However, as certain positions in the noisy latent may not be occupied by other feature points, \(_{t}^{}\) obtained from one-step warpage optimization may contain regions with null values as shown in Fig. 4, leading to semantic losses that can adversely impact the drag result. We address this issue in Sec. 3.3.

### Bilateral Nearest Neighbor Interpolation

To enhance the semantic integrity, BNNI interpolates points in null region using similar features from their neighboring areas in horizontal and vertical directions, thus ensuring the semantic integrity and enhancing the quality of drag editing. Let \(\) be a point with coordinate \((x_{},y_{})\) in null regions, we identify the nearest points of \(\) containing value in four directions: up, right, down, and left, as illustrated in Fig. 4, which are used as reference points for interpolation. Then, the interpolated value for null point \(\) can be calculated as:

\[_{t(x_{},y_{})}^{}=_{loc=u,r,d,l}w_{loc}  ref_{loc}\] (11)

where \(ref_{loc}\) denotes the value of reference point, and \(loc\) indicates the direction, with \(u\), \(r\), \(d\) and \(l\) representing up, right, down and left, respectively. \(w_{loc}\) is the interpolation weight for each reference point, which is calculated based on its distance to \(\), as follows:

\[w_{loc}=}{_{loc=u,r,d,l}1/len_{loc}}\] (12)

where \(len_{loc}\) represents the distance between the reference point and \(\). Such that we can obtain the optimized noisy latent \(_{t}^{}\) with complete semantic information by using BNNI to exploit similar semantic information from surrounding areas, further enhancing the quality of the drag editing.

### Consistency-Preserving Strategy

Following [20; 2; 28], we introduce a consistency-preserving strategy to maintain the consistency between the edited image and the original image by adopting the semantic information of the original image (_i.e._, key and value pairs) saved in self-attention module during diffusion inversion to guide the diffusion sampling, as illustrated in Fig. 5. Specifically, during the diffusion sampling, the calculation of self-attention \(_{}\) within the upsampling process of the U-Net is as follows:

\[_{}(_{},_{},_{ })=(_{}_{}}{ })_{}\] (13)

where query \(_{}\) is still used from diffusion sampling but key \(_{}\) and value \(_{}\) are correspondingly from diffusion inversion. Thus, the consistency-preserving strategy maintains the overall content consistency between the desired image and original image, ensuring the effect of drag-based editing.

## 4 Experiments

### Qualitative Evaluation

We conduct experiments to demonstrate the drag effects of our FastDrag method, comparing it against state-of-the-art techniques such as DragDiffusion , FreeDrag , and DragNoise . The qualitative comparison results are presented in Fig. 6. Notably, FastDrag maintains effective drag performance and high image quality even in images with complex textures, where \(n\)-step iterative methods typically falter. For instance, as shown in the first row of Fig. 6, FastDrag successfully rotates the face of an animal while preserving intricate fur textures and ensuring strong structural integrity. In contrast, methods like DragDiffusion and DragNoise fail to rotate the animal's face, and FreeDrag disrupts the facial structure.

In the stretching task, FastDrag outperforms all other methods, as shown in the second row of Fig. 6, where the goal is to move a sleeve to a higher position. The results show that other methods lack robustness to slight deviations in user dragging, where the drag point is slightly off the sleeve.

Figure 6: Illustration of qualitative comparison with the state-of-the-art methods.

Despite this, FastDrag accurately moves the sleeve to the desired height, understanding the underlying semantic intent of dragging the sleeve.

Additionally, we perform multi-point dragging experiments, illustrated in the fourth row of Fig. 6. Both DragDiffusion and DragNoise fail to stretch the back of the sofa, while FreeDrag incorrectly stretches unintended parts of the sofa. Through the LWF introduced in Sec. 3.2.1, FastDrag can manipulate all dragged points to their target locations while preserving the content in unmasked regions. More results of FastDrag are illustrated in supplementary Sec. E.

### Quantitative Comparison

To better demonstrate the superiority of FastDrag, we conduct quantitative comparison using DragBench dataset , which consists of 205 different types of images with 349 pairs of handle and target points. Here, mean distance (MD)  and image fidelity (IF)  are employed as performance metrics, where MD evaluates the precision of drag editing, and IF measures the consistency between the generated and original images by averaging the learned perceptual image patch similarity (LPIPS) . Specifically, 1-LPIPS is employed as the IF metric in our experiment to facilitate comparison. In addition, we compare the average time required per point to demonstrate the time efficiency of our proposed FastDrag. The results are given in Table 1.

Apart from ,,, two other state-of-the-art methods, _i.e._, GoodDrag  and DiffEditor , are also adopted for comparison, with DiffEditor being the current fastest drag-based editing method. Due to well-designed one-step warpage optimization and consistency-preserving strategy, our FastDrag does not require LoRA training preparation, resulting in significantly reduced time consumption (_i.e._, 3.12 seconds), which is nearly 700% faster than DiffEditor (_i.e._, 21.68 seconds), and 2800% faster than the typical baseline DragDiffusion (_i.e._, 1 min and 21.54 seconds). Moreover, even using standard U-Net without LCM, our method is still much faster than DiffEditor and far outperforms all other state-of-the-art methods. It is particularly noteworthy that, even with an A100 GPU, DiffEditor still requires 13.88 seconds according to , whereas FastDrag only requires 3.12 seconds on an RTX 3090.

In addition, our FastDrag also achieves competitive quantitative evaluation metrics (_i.e._, IF and MD) comparable to the state-of-the-art methods, and even better drag editing quality, as illustrated in Fig. 6. These results demonstrate the effectiveness and superiority of our method.

### Ablation Study

**Inversion Step:** To determine the number of inversion steps in diffusion inversion with LCM-equipped U-Net, we conduct an ablation experiment with number of inversion steps set as \(t\) = 4, 6, 8, 10, 12, 14, 20, and 30, where IF and MD are used to evaluate the balance between the consistency with original image and the desired drag effects. The results are given in Fig. 8 and Fig. 7, where we can see that when \(t<6\), the generated images lack sufficient detail to accurately reconstruct the original images. Conversely, when \(t>6\), it can successfully recover complex details such as intricate fur textures and dense stone while maintaining high image quality. However, when \(t>14\), some image details lost, which negatively impacts the effectiveness of the drag effect. By comprehensive

   Approach & Verao & MD \(\) & 1 - LPIPS \(\) &  \\  DragDiffusion  & CVPR2024 & 33.70 & 0.89 & 1 min (LoRA) & 21.54 \\ DragNoise  & CVPR2024 & 33.41 & 0.63 & 1 min (LoRA) & 20.41 \\ FreeDrag  & CVPR2024 & 35.00 & 0.70 & 1 min (LoRA) & 52.63 \\ GoodDrag  & arXiv:2024 & 22.96 & 0.86 & 1 min (LoRA) & 45.83 \\ DiffEditor  & CVPR2024 & 28.46 & 0.89 & ✗ & 21.68 \\ FastDrag/Ours & 33.22 & 0.87 & ✗ & 5.66 \\ FastDrag (Ours) & 33.23 & 0.86 & ✗ & **3.12** \\   

Table 1: Quantitative comparison with state-of-art methods on DragBench. Here, lower MD indicates more precise drag results, while higher 1-LPIPS reflects greater similarity between the generated and original images. The time metric represents the average time required per point based on RTX 3090. Preparation denotes LoRA training. \(\) means FastDrag without LCM-equipped U-Net.

evaluation of both the drag effect and the similarity to the original images, we select 10 as the number of inversion steps for our method with LCM to balance the drag effect.

**BNNI:** To demonstrate the effectiveness of BNNI, we compare it with several interpolation methods on null point \(\), including maintaining the original value of this position, interpolation by zero-value, and interpolation by random noise, denoted as "original value", "0 interpolation", and "random interpolation", respectively. The results are given in Fig. 9, where we can see that, by effectively utilizing surrounding feature values to interpolate null points, BNNI can address semantic losses, and enhance the quality of the drag editing.

**Consistency-Preserving:** We also conduct an experiment to validate the effectiveness of the consistency-preserving strategy in maintaining image consistency. The results are illustrated in Fig. 10, where "w/ CP" and "w/o CP" denote our FastDrag with and without using consistency-preserving strategy, respectively. It is obviously that our method with consistency-preserving strategy can effectively preserve image consistency, resulting in better drag editing effect.

Figure 8: Ablation study on number of inversion steps in terms of drag effect.

Figure 10: Ablation study on consistency-preserving strategy.

Figure 9: Ablation study on bilateral nearest neighbor interpolation.

## 5 Limitations

Despite FastDrag's impressive editing speed compared to SOTA methods, it shares some common limitations. 1) **Overly Smooth and Finer Details Loss:** Similar to other diffusion-based methods [3; 28], FastDrag occasionally loses fine textures from the original images, as shown in Fig. 6, row 4. Despite this, FastDrag outperforms other methods in speed and overall performance. 2) **Extremely Long-distance Drag Editing:** In such case, object details may be lost due to the lower-dimensional latent space, in which significant changes in detail (i.e., long-drag editing) can disrupt the semantics, making it harder to preserve all details. Nevertheless, FastDrag handles long-distance editing better than other SOTA methods, as illustrated in Fig. 11, where our method successfully achieves long-distance drag editing that others fail to achieve. 3) **Highly Relying on Precise Drag Instruction:** Achieving optimal results depends heavily on clear drag instructions. As with other SOTA methods, precise input, such as excluding irrelevant areas from the mask (e.g., the face in Fig. 12, row 2) or correctly placing the handle point (e.g., beak in Fig. 12), is essential for better performance.

## 6 Conclusion

This paper has presented a novel drag-based image editing method, _i.e._, FastDrag, which achieved faster image editing speeds than other existing methods. By proposing one-step warpage optimization and BNNI strategy, our approach achieves high-quality image editing according to the drag instructions in a very short period of time. Additionally, through the consistency-preserving strategy, it ensures the consistency of the generated image with the original image. Moving forward, we plan to continue refining and expanding our approach to further enhance its capabilities and applications.

Figure 11: Illustration of failure cases for limitation analysis under extremely long-distance drag editing. Our FastDrag method may lose some detailed information in these cases but still achieves better editing performance compared to state-of-the-art (SOTA) methods.

Figure 12: Illustration of the limitation analysis with failed and successful drag editing for highly relying on precise drag instruction. (a) It is best to exclude the face from the mask region. (b) The handle point should ideally be placed where the “beak” feature is more prominent.

Acknowledgments

This work was partly supported by Beijing Nova Program (20230484261).