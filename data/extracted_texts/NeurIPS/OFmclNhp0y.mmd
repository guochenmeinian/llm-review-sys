# Deterministic Uncertainty Propagation for Improved Model-Based Offline Reinforcement Learning

Abdullah Akgul   Manuel Haussmann   Melih Kandemir

Department of Mathematics and Computer Science

University of Southern Denmark

Odense, Denmark

{akgul,haussmann,kandemir}@imada.sdu.dk

###### Abstract

Current approaches to model-based offline reinforcement learning often incorporate uncertainty-based reward penalization to address the distributional shift problem. These approaches, commonly known as pessimistic value iteration, use Monte Carlo sampling to estimate the Bellman target to perform temporal difference-based policy evaluation. We find out that the randomness caused by this sampling step significantly delays convergence. We present a theoretical result demonstrating the strong dependency of suboptimality on the number of Monte Carlo samples taken per Bellman target calculation. Our main contribution is a deterministic approximation to the Bellman target that uses progressive moment matching, a method developed originally for deterministic variational inference. The resulting algorithm, which we call Moment Matching Offline Model-Based Policy Optimization (MOMBO), propagates the uncertainty of the next state through a nonlinear Q-network in a deterministic fashion by approximating the distributions of hidden layer activations by a normal distribution. We show that it is possible to provide tighter guarantees for the suboptimality of MOMBO than the existing Monte Carlo sampling approaches. We also observe MOMBO to converge faster than these approaches in a large set of benchmark tasks.

## 1 Introduction

Offline reinforcement learning (Lange et al., 2012; Levine et al., 2020) aims to solve a control task using an offline dataset without interacting with the target environment. Such an approach is essential in cases where environment interactions are expensive or risky (Shortreed et al., 2011; Singh et al., 2020; Nie et al., 2021; Micheli et al., 2022). Directly adapting traditional online off-policy reinforcement learning methods to offline settings often results in poor performance due to the adverse effects of the distributional shift caused by the policy updates (Fujimoto et al., 2019; Kumar et al., 2019). The main reason for the incompatibility is the rapid accumulation of overestimated action-values during policy improvement. Pessimistic Value Iteration (PEVI) (Jin et al., 2021) offers a theoretically justified solution to this problem that penalizes the estimated rewards of synthetic state transitions proportionally to the uncertainty of the predicted next state. The framework encompasses many state-of-the-art offline reinforcement learning algorithms as special cases (Yu et al., 2020; Sun et al., 2023).

Model-based offline reinforcement learning approaches first fit a probabilistic model on the real state transitions and then supplement the real data with synthetic samples generated from this model throughout policy search (Janner et al., 2019). One line of work addresses distributional shift by constraining policy learning (Kumar et al., 2019; Fujimoto and Gu, 2021) where the policy is trained to mimic the behavior policy and is penalized based on the discrepancy between its actions and thoseof the behavior policy, similarly to behavioral cloning. A second strategy introduces conservatism to training by _(i)_ perturbing the training objective with a high-entropy behavior policy (Kumar et al., 2020; Yu et al., 2021), _(ii)_ penalizing state-action pairs proportionally to their estimated variance (Yu et al., 2020; An et al., 2021; Bai et al., 2022; Sun et al., 2023), or _(iii)_ adversarially training the environment model to minimize the value function (Rigter et al., 2022) to prevent overestimation in policy evaluation for out-of-domain state-action pairs.

In the offline reinforcement learning literature, uncertainty-driven approaches exist for both model-free (An et al., 2021; Bai et al., 2022) and model-based settings (Yu et al., 2020; Sun et al., 2023), all aimed at learning a pessimistic value function (Jin et al., 2021) by penalizing it with an uncertainty estimator. The impact of uncertainty quantification has been investigated in both online (Abbas et al., 2020) and offline (Lu et al., 2021) scenarios, particularly in model-based approaches, which is the focus of our work.

Despite the strong theoretical evidence suggesting that the quality of the Bellman target uncertainties has significant influence on model quality (O'Donoghue et al., 2018; Luis et al., 2023; Jin et al., 2021), the existing implementations rely on Monte Carlo samples and heuristics-driven uncertainty quantifiers being used as reward penalizers (Yu et al., 2020; Sun et al., 2023) which results in a high degree of randomness that manifests itself as significant delays in model convergence. Figure 1 demonstrates why this is the case by a toy example. The uncertainty around the estimated action-value of the next state does not shrink even after taking \(10000\) samples on the next state and passing them through a Q-network. Our moment matching-based approach predicts a similar mean with significantly smaller variance at the cost of only two Monte Carlo samples.

We identify the randomness introduced by Bellman target approximation via Monte Carlo sampling as the primary limiting factor for the performance of the PEVI approaches used in offline reinforcement learning. We have three main contributions:

* We present a new algorithm that employs progressive moment matching, an idea originally developed for deterministic variational inference of Bayesian neural networks (Wu et al., 2019), for the first time to propagate deterministically environment model uncertainties through Q-function estimates. We refer to this algorithm as _Moment Matching Offline Model-Based Policy Optimization_ (MOMBO).
* We develop novel suboptimality guarantees for both MOMBO and existing sampling-based approaches highlighting that MOMBO is provably more efficient.
* We present comprehensive experiment results showing that MOMBO significantly accelerates training convergence while maintaining asymptotic performance.

## 2 Preliminaries

Reinforcement learning.We define a Markov Decision Process (MDP) as a tuple \((,,r,,_{1}, ,H)\) where \(\) represents the state space and \(\) denotes the action space. We have \(r:[0,R_{}]\) as a bounded deterministic reward function and \(:()\) as the probabilistic state transition kernel, where \(()\) denotes the probability simplex defined over the state space. The MDP has an initial state distribution \(_{1}()\), a discount factor \((0,1)\), and an episode length \(H\). We use deterministic policies \(:\) and deterministic rewards in analytical demonstrations for simplicity and without loss of generality. Our results extend straightforwardly to probabilistic policies and reward functions. The randomness on the next state may result from the system stochasticity or the estimation error of a probabilistic model trained on the data collected from a deterministic environment. We define the action-value of a policy

Figure 1: _Moment Matching versus Monte Carlo Sampling._ Moment matching offers sharp estimates of the action-value of the next state at the cost of only two forward passes through a critic network. A similar sharpness cannot be reached even with \(10000\) Monte Carlo samples, which is \(5000\) times more costly. See Appendix C.1.1 for details.

\(\) for a state-action pair \((s,a)\) as

\[Q_{}(s,a)_{}[_{i=h}^{H}^{i-h}r(s_{i},a_{i })s_{h}=s,a_{h}=a].\]

This function maps to the range \([0,}}{{1-}}]\).

Offline reinforcement learningalgorithms perform policy search using an offline collected dataset \(=\{(s,a,r,s^{})\}\) from the target environment via a behavior policy \(_{}\). The goal is to find a policy \(\) that minimizes the suboptimality, defined as \((;s) Q_{^{*}}(s,^{*}(s))-Q_{}(s,(s))\) for an initial state \(s\) and optimal policy \(^{*}\). For brevity, we omit the dependency on \(s\) in \(()\). Model-based reinforcement learning approaches often train state transition probabilities and reward functions on the offline dataset by maximum likelihood estimation with an assumed density, modeling these as an ensemble of heteroscedastic neural networks (Lakshminarayanan et al., 2017). Mainstream model-based offline reinforcement learning methods adopt the Dyna-style (Sutton, 1990; Janner et al., 2019), which suggests training an off-the-shelf model-free reinforcement learning algorithm on synthetic data generated from the learned environment model \(}\) using initial states from \(\). It has been observed that mixing minibatches collected from synthetic and real datasets improves performance in both online (Janner et al., 2019) and offline (Yu et al., 2020, 2021) settings.

Scope and problem statement.We focus on model-based offline reinforcement learning due to its superior performance over model-free methods (Yu et al., 2020; Kidambi et al., 2020; Yu et al., 2021; Rigter et al., 2022; Sun et al., 2023). The primary challenge in offline reinforcement learning is _distributional shift_ caused by a limited coverage of the state-action space. When the policy search algorithm probes unobserved state-action pairs during training, significant value approximation errors arise. In standard supervised learning, such errors diminish as new observations are collected. However, in reinforcement learning, overestimation errors are exploited by the policy improvement step and accumulate, resulting in a phenomenon known as the _overestimation bias_(Thrun and Schwartz, 1993). Techniques developed to overcome this problem in the online setting such as min-clipping (Fujimoto et al., 2018) are insufficient for offline reinforcement learning. Algorithms that use reward penalization proportional to the estimated uncertainty of an unobserved next state are commonly referred to as _Pessimistic Value Iteration_(Yu et al., 2020; Sun et al., 2023). These algorithms are grounded in a theory that provides performance improvement guarantees by bounding the variance of next state predictions (Jin et al., 2021; Uehara and Sun, 2022). We demonstrate the theoretical and practical limitations of PEVI-based approaches and address them with a new solution.

Bellman operators.Both our analysis and the algorithmic solution build on a problem that arises from the inaccurate estimation of the Bellman targets in critic training. The error stems from the fact that during training the agent has access only to the _sample Bellman operator_,

\[}_{}Q(s,a,s^{}) r(s,a)+ Q(s^{ },(s^{})),\]

where \(s^{}\) is a Monte Carlo sample of the next state. However, the actual quantity of interest is the _exact Bellman operator_ that is equivalent to the expectation of the sample Bellman operator,

\[_{}Q(s,a)_{s^{}(|s,a )}[}_{}Q(s,a,s^{})].\]

## 3 Pessimistic value iteration algorithms

A pessimistic value iteration algorithm (Jin et al., 2021), denoted as \(_{}(}_{}^{}Q,})\), performs Dyna-style model-based learning using the following _pessimistic sample Bellman target_ for temporal difference updates during critic training:

\[}_{}^{}Q(s,a,s^{})}_{}Q(s,a,s^{})-_{}}^{Q}(s,a),\]

where \(_{}}^{Q}(s,a)\) admits a learned state transition kernel \(}\) and a value function \(Q\) to map a state-action pair to a penalty score. Notably, this penalty term does not depend on an observed or previously sampled \(s^{}\) as it quantifies the uncertainty around \(s^{}\) using \(}\). The rest follows as running an off-the-shelf model-free online reinforcement learning algorithm, for instance Soft Actor-Critic (SAC)(Haarnoja et al., 2018), on a replay buffer that comprises a blend of real observations and synthetic data generated from \(}\) using the recent policy of an intermediate training stage. We study the following two PEVI variants in this paper due to their representative value:

1. _Model-based Offline Policy Optimization (MOPO)_(Yu et al., 2020) directly penalizes the uncertainty of a state as inferred by the learned environment model \(^{Q}_{}}(s,a)_{}}[s^{}]\). MOPO belongs to the family of methods where penalties are based on the uncertainty of the next state.
2. _MOdel-Bellman Inconsistency penalized offLinE Policy Optimization (MOBILE)_(Sun et al., 2023) propagates the uncertainty of the next state to the Bellman target through Monte Carlo sampling and uses it as a penalty: \[^{Q}_{}}(s,a)}^{N}_{s ^{}}}[}_{}Q(s,a,s^{})]\] where \(}^{N}_{s^{}}}\) denotes the empirical variance of the quantity in brackets with respect to \(N\) samples drawn i.i.d. from \(}\). MOBILE represents the family of methods that penalize rewards based on the uncertainty of the Bellman target.

Both approaches approximate the mean of the Bellman target by evaluating the sample Bellman operator \(}_{}Q(s,a,s^{})\) with a single \(s^{}\) available either from real environment interaction within the dataset or a single sample taken from \(}\). The following theorem establishes the critical role the penalty term plays in the model performance.

**Theorem 1** (_Suboptimality of PEVI (Jin et al., 2021)_).: _For any \(\) derived with \(_{}(}_{}^{}Q,})\) that satisfies_

\[|_{}Q(s,a)-}_{}Q(s,a,s^{})| ^{Q}_{}}(s,a),(s,a) ,\]

_with probability at least \(1-\) for some error tolerance \((0,1)\) where \(s^{}(|s,a)\), the following inequality holds for any initial state \(s_{1}\):_

\[() 2_{i=1}^{H}_{^{*}}[^{Q} _{}}(s_{i},a_{i})s_{1}].\]

When deep neural networks are used as value function approximators, calculating their variance becomes analytically intractable even for normally distributed inputs. Consequently, reward penalties \(^{Q}_{}}\) are typically derived from Monte Carlo samples, which are prone to high estimator variance (see Figure 1). Our key finding is that using high-variance estimates for reward penalization introduces three major practical issues in the training process of offline reinforcement learning algorithms:

1. The information content of the distorted gradient signal shrinks, causing delayed convergence to the asymptotic performance.
2. The first two moments of the Bellman target are poorly approximated for feasible sample counts.
3. Larger confidence radii need to be used to counter the instability caused by _(i)_ and the high overestimation risk caused by the inaccuracy described in _(ii)_, which unnecessarily restricts model capacity.

### Theoretical analysis of sampling-based PEVI

We motivate the benefit of our deterministic uncertainty propagation scheme by demonstrating the prohibitive relationship of the sample Bellman operator when used in the PEVI context. The analysis of the distribution around the Bellman operator can be characterized as follows. We are interested in the distribution of a deterministic map \(y=f(x)\) for a normally distributed input \(x(,^{2})\). We analyze this complex object via a surrogate of it. For a sample set \(S_{N}=\{x_{i}\}_{i=1}^{N}\) obtained i.i.d. from \((,^{2})\), let \(_{N}\) and \(^{2}_{N}\) be its empirical mean and variance. Now consider the following two random variables \(y_{N}=_{i=1}^{N}f(x_{i})\) and \(y^{}_{N}=f(x^{})\) for \(x^{}(_{N},^{2}_{N})\). Note that and \(y^{}_{1}\) are equal in distribution. Furthermore, both \(y_{N}\) and \(y^{}_{N}\) converge to the true \(y\) as \(N\) tends to infinity. We perform our analysis on \(y^{}_{N}\) where analytical guarantees are easier to build. Furthermore, \(y^{}_{N}\) is a tighter approximation of both \(y_{1}\) and \(y^{}_{1}\). We construct the following suboptimality proof for the PEVI algorithmic family that approximates the uncertainty around the Bellman operator in the way \(y^{}_{N}\) is built. In this theorem and elsewhere, \(A_{l}\) stands for the weights of the \(l\)-th layer of a Multi-Layer Perceptron (MLP) which has \(1\)-Lipschitz activation functions and \(\) denotes \(L1\)-norm. See Appendix B.2 for the proof.

**Theorem 2** (_Suboptimality of sampling-based PEVI_).: _For any policy \(_{}}\) learned by \(_{}}(}_{}^{}Q, {})\) using \(N\) Monte Carlo samples to approximate the Bellman target with respect to an action-value network defined as an \(L\)-layer MLP with \(1\)-Lipschitz activation functions, the following inequality holds for any error tolerance \((0,1)\) with probability at least \(1-\)_

\[(_{}}) 2H_{l=1}^{L} A_{l} }{{4}})R_{}^{2}}{ N/2 (1-)^{2}}}.\]

The bound in Theorem 2 is prohibitively loose since \(R_{}^{2}/(1-)^{2}\) is a large number in practice. For instance, the maximum realizable step reward in the HalfCheetah-v4 environment of the MuJoCo physics engine (Todorov et al., 2012) is at least around \(11.7\) according to Hui et al. (2023). Choosing the usual \(=0.99\), we obtain \(R_{}^{2}/(1-)^{2}=1.37 10^{6}\). Furthermore, as the bound depends on the number of samples for the next state \(N\), it becomes looser as \(N 1\). The bound is not defined for \(N=1\), but the loosening trend is clear.

## 4 MOMBO: Moment matching offline model-based policy optimization

Our key contribution is the finding that quantifying the uncertainty around the Bellman target brings significant benefits to the model-based offline reinforcement learning setting. We obtain a deterministic approximation using a moment matching technique originally developed for Bayesian inference. This method propagates the estimated uncertainty of the next state \(s^{}\) obtained from a learned environment model \(}\) through an action-value network by alternating between an affine transformation of a normally distributed input to another normally distributed linear activation and projecting the output of a nonlinear activation to a normal distribution by analytically calculating its first two moments. The resulting normal distributed output is then used to build a lower confidence bound on the Bellman target, which is an equivalent interpretation of reward penalization. Such a deterministic approximation is both a more accurate uncertainty quantifier and a more robust quantity to be used during training in a deep reinforcement learning setting. Its contribution to robust and sample-efficient training has been observed in other domains (Wang and Manning, 2013; Wu et al., 2019). Prior work attempted to propagate full covariances through deep neural nets (see, e.g. Wu et al., 2019; Look et al., 2023; Wright et al., 2024) at a prohibitive computational cost (quadratic in the number of neurons) that does not bring a commensurate empirical benefit. Therefore, we choose to propagate only means and variances.

Assuming the input of a fully-connected layer to be \(X(X|,^{2})\), the pre-activation \(Y\) for a neuron associated with weights \(\) is given by \(Y=^{}X\), i.e., \(Y(Y|^{},(^{2})^{}^{2})\), where we absorb the bias into \(\) for simplicity and the square on \(\) is applied element-wise. For a ReLU activation function \((x)(0,x)\),1 mean \(\) and variance \(^{2}\) of \(Y=(0,X)\) are analytically tractable (Frey and Hinton, 1999). We approximate the output with a normal distribution \((|, ^{2})\) and summarize several properties regarding \(\) and \(^{2}\) below. See Appendix B.1 for proofs of all results in this subsection.

**Lemma 1** (_Moment matching_).: _For \(X(X|,^{2})\) and \(Y=(0,X)\), we have_

_(i)_ \([Y]=()+( ),^{2}[Y]=( ^{2}+^{2})()+()-^{2},\)__

_where \(=/\), and \(()\), \(()\) are the probability density function (pdf) and cumulative distribution function (cdf) of the standard normal distribution, respectively. Additionally, we have that_

_(ii)_ \(^{2 }^{2}.\)__Algorithm 1 outlines the moment matching process. This process involves two passes through the linear layer: one for the first moment and one for the second, along with additional operations that take negligible computation time. In contrast, sampling-based methods require \(N\) forward passes, where \(N\) is the number of samples. Thus, for any \(N>2\), sampling-based methods introduce computational overhead. MOBILE (Sun et al., 2023), e.g., uses \(N=10\). See Figure 1 and Figure 3 for illustrations on the effect of varying \(N\).

``` functionMomentMatchingThroughLinear(\(,b,X\)) \(\): Weights of the layer, \(b\): bias of the layer \(X=(X|_{X},_{X}^{2})\)\(\) input a normal distribution \((_{Y},_{Y}^{2})(^{}_{X}+b,^{2} _{X}^{2})\)\(\) transform mean and variance return\(Y=(Y|_{Y},_{Y}^{2})\)\(\) output the transformed distribution endfunctionMomentMatchingThroughReLU(\(X\)) \(X=(X|_{X},_{X}^{2})\)\(\) input a normal distribution \(}}{{_{X}}}\)\(_{Y}_{X}()+_{X}()\)\(\) compute the first two moments of \((X)\) \(_{Y}^{2}(_{X}^{2}+_{X}^{2})()+ _{X}_{X}()-_{Y}^{2}\)\(\)\(/\) are the normal pdf/cdf return\(Y=(Y|_{Y},_{Y}^{2})\)\(\) output a normal distribution with these moments endfunction ```

**Algorithm 1** Deterministic uncertainty propagation through moment matching

We propagate the distribution of the next state predicted by the environment model through the action-value function network using moment matching. We define a _moment matching Bellman target distribution_ as:

\[}_{}Q(s,a,s^{})r(s,a)+ Q_{MM} (s^{},(s^{}))\]

for \(s^{}}(|s,a)\) and \(Q_{MM}(s^{},a^{})\) a normal distribution with mean \(_{MM}(s^{},a^{})\) and variance \(_{MM}^{2}(s^{},a^{})\) obtained as the outcome of a progressive application of Algorithm 1 through the layers of a critic network \(Q\). The sign \(\) denotes equality in distribution, i.e., the expressions on the two sides share the same probability law. We perform pessimistic value iteration using a lower confidence bound on \(}_{}Q(s,a,s^{})\) as the Bellman target

\[}_{}^{}Q(s,a,s^{})r(s,a)+ _{MM}(s^{},(s^{}))-_{MM}(s^{}, (s^{}))\] (1)

for some radius \(>0\).

### Theoretical analysis of moment matching-based uncertainty propagation

The following lemma provides a bound on the \(1\)-Wasserstein distance \(W_{1}\) between the true distribution \(_{Y}\) of a random variable after being transformed by a ReLU activation function and its moment matched approximation \(_{}\). See Appendix B.3 for the proofs of all results presented in this subsection.

**Lemma 2** (_Moment matching bound_).: _For the following three random variables_

\[X_{X}, Y=(0,X),( |,^{2})\]

_with \(=[Y]\) and \(^{2}=[Y]\) the following inequality holds_

\[W_{1}(_{Y},_{})_{-}^{0}F_{}(u )du+W_{1}(_{X},_{})\]

_where \(F_{}()\) is cdf of \(\). If \(_{X}=(X|,^{2})\), it can be further simplified to_

\[W_{1}(_{Y},_{})(}{})-(-}{})+|-|+|- |.\]

Applying this to a moment matching \(L\)-layer MLP yields the following deterministic bound.

**Lemma 3** (_Moment matching MLP bound_).: _Let \(f(X)\) be an \(L\)-layer MLP with ReLU activation \((x)=(0,x)\). For \(l=1,,L-1\), the sampling-based forward-pass is_

\[Y_{0}=X_{s}, Y_{l}=(f_{l}(Y_{l-1})), Y_{L}=f_{L}(Y_{L-1})\]

_where \(f_{l}()\) is the \(l\)-th layer and \(X_{s}\) a sample of \((X|_{X},_{X}^{2})\). Its moment matching pendant is_

\[_{0}(_{0}|_{X},_{X}^{2}), _{l}(_{l}[r(f_{l}(_{l-1}))],[r(f_{l}( _{l-1}))]).\]

_The following inequality holds for \(_{Y}=_{_{L}}=(_{L}| [f(_{L-1})],[f(_{L-1})])\)._

\[W_{1}(_{Y},_{Y})_{l=2}^{L}(G(_{ l-1})+C_{l-1})_{j=l}^{L} A_{j},\]

\[ G(_{l})=_{l}(_{l}}{_{l}})-_{l} (-_{l}}{_{l}}) 1,  C_{l}|A_{l}_{l-1}-_{l}|+| ^{2}_{l-1}^{2}}-_{l}|\]

_where \(\) and \(()^{2}\) are applied elementwise._

Relying on Lemma 6 again, this result provides an upper bound on the suboptimality of our moment matching approach.

**Theorem 3** (_Suboptimality of moment matching-based PEVI algorithms_).: _For any policy \(_{}\) derived with \(_{}(}_{}^{}Q, })\) learned by a penalization algorithm that uses moment matching to approximate the Bellman target with respect to an action-value network defined as an \(L\)-layer MLP with \(1\)-Lipschitz activation functions, the following inequality holds_

\[(_{}) 2H_{l=2}^{L}(G(_{ l-1})+C_{l-1})_{j=l}^{L} A_{j}.\]

The bound in Theorem 3 is much tighter than Theorem 2 in practice as \(R_{}^{2}/(1-)^{2}\) is very large while the Lipschitz continuities \( A_{j}\) of the two-layer MLPs used in the experiments are in the order of low hundreds according to empirical investigations (Khromov and Singh, 2024). Another favorable property of Theorem 3 is that its statement is exact, as opposed to the probabilistic statement made in Theorem 2. The provable efficiency of MOMBO could be of independent interest for safety-critical use cases of offline reinforcement learning where the availability of analytical performance guarantees is a fundamental requirement.

### Implementation details of MOMBO

We adopt the model learning scheme from Model-Based Policy Optimization (MBPO) (Janner et al., 2019) and use SAC (Haarnoja et al., 2018) as the policy search algorithm. These approaches represent the best practices in many recent model-based offline reinforcement learning methods (Yu et al., 2020, 2021; Sun et al., 2023). However, we note that most of our findings are more broadly applicable. Following MBPO, we train an independent heteroscedastic neural ensemble of transition kernels modeled as Gaussian distributions over the next state and reward. We denote each ensemble element as \(}_{n}(s^{},r|s,a)\) for \(n\{1,,N_{}\}\). After evaluating the performance of each model on a validation set, we select the \(N_{}\) best-performing ensemble elements for further processing. We use these elite models to generate \(k\)-step rollouts with the current policy and to create the synthetic dataset \(}\), which we then combine with the real observations \(\). We retain the mean and variance of the predictive distribution for the next state to propagate it through the action-value function while assigning zero variance to the real tuples.

The lower confidence bound given in Equation (1) builds on our MDP definition, which assumes a deterministic reward function and a deterministic policy for illustrative purposes. In our implementation, the reward model also follows a heteroscedastic ensemble of normal distributions. We also incorporate the uncertainty around the predicted reward of a synthetic sample into the Bellman target calculation. Furthermore, our policy function is a squashed Gaussian distribution trained by SAC in the maximum entropy reinforcement learning setting. For further details, refer to the Appendix C.2.

## 5 Experiments

We compare MOMBO against MOPO and MOBILE, the two representative PEVI variants, across twelve tasks from the D4RL dataset (Fu et al., 2020), which consists of data collected from three MuJoCo environments (halfcheetah, hopper, walker2d) with behavior policies exhibiting four degrees of expertise (random, medium, medium-replay, and medium-expert). We use the datasets collected with 'v2' versions of the MuJoCo environments to be commensurate with the state of the art. We evaluate model performance using two scores:

1. _Normalized Reward:_ Total episode reward collected in evaluation mode after offline training ends, normalized by the performances of random and expert policies.
2. _Area Under Learning Curve (AULC):_ Average normalized reward computed at the intermediate steps of training.

AULC indicates how fast a model converges to its final performance. A higher AULC reflects more efficient learning other things being equal. We report the main results in Table 1 and provide the learning curves of the halfcheetah environment in Figure 2 for visual investigation. We present the learning curves for the remaining tasks in Figure 4. We obtain the results for the baseline models from the log files provided by the OfflineRL library (Sun, 2023). We implement MOMBO into the MOBILE (Sun et al., 2023) code base shared by its authors and use their experiment configurations wherever applicable. See Appendix C for details. The source code of our algorithm is available at https://github.com/adinlab/MOMBO. The OfflineRL library does not contain the log files for the random datasets for MOPO at the time of writing. We replicate these experiments using the MOBILE code based on the prescribed configurations.

Theorem 1 indicates that the performance of a PEVI algorithm depends on how tightly its reward penalizer \(_{}^{Q}(s,a)\) upper bounds the Bellman operator error \(|_{}Q(s,a)-}_{}Q(s,a)|\). We use this theoretical result to compare the quality of the reward penalizers of the three models based on average values of the following two performance scores calculated across data points observed during ten evaluation episodes:

   &  &  &  \\   & & MOPO & MOBILE & MOMBO & MOPO & MOBILE & MOMBO \\   & halfcheetah & 37.2\(\)1.6 & 41.2\(\)1.1 & **43.6\(\)**1.1 & 36.3\(\)1.0 & 39.5\(\)1.2 & **41.4\(\)**1.0 \\  & hopper & **31.7\(\)**1.0 & 31.3\(\)0.1 & 25.4\(\)1.02 & **28.6\(\)**1.4 & 23.6\(\)3.7 & 17.3\(\)1.3 \\  & walker2d & 8.2\(\)5.6 & **22.1\(\)**0.9 & 21.5\(\)0.1 & 5.4\(\)3.2 & 18.0\(\)0.4 & **19.2\(\)**0.5 \\   & Average on random & 25.7 & **31.5** & 30.2 & 23.4 & **27.1** & 26.0 \\    & halfcheetah & 72.4\(\)4.2 & 75.8\(\)0.8 & **76.1\(\)**0.8 & 70.9\(\)2.0 & 72.1\(\)1.0 & **73.0\(\)**0.9 \\  & hopper & 62.8\(\)3.1 & 103.6\(\)1.0 & **104.2\(\)**0.5 & 37.0\(\)15.3 & 82.2\(\)7.3 & **95.9\(\)**2.5 \\  & walker2d & 85.4\(\)2.9 & **88.3\(\)**2.5 & 86.4\(\)1.2 & 77.6\(\)1.3 & 79.0\(\)1.3 & 84.0\(\)**1.1 \\  & Average on medium & 73.6 & **89.3** & **88.9** & 68.8 & 77.8 & **84.3** \\    & halfcheetah & **72.1\(\)3.8** & 71.9\(\)3.2 & 72.0\(\)4.3 & 68.4\(\)4.7 & 67.9\(\)2.8 & **68.7\(\)3.9** \\  & hopper & 92.7\(\)20.7 & **105.1\(\)**1.3 & 104.8\(\)1.0 & 81.7\(\)4.6 & 78.7\(\)4.0 & **87.3\(\)**2.0 \\  & walker2d & 85.9\(\)5.3 & **90.5\(\)**1.7 & 89.6\(\)3.8 & 65.3\(\)12.7 & 79.9\(\)4.3 & **80.8\(\)**5.6 \\   & Average on medium+replay & 83.4 & **89.2** & 88.8 & 72.4 & 75.5 & **78.9** \\   & halfcheetah & 83.6\(\)12.5 & 100.9\(\)1.5 & **103.3\(\)**0.8 & 77.1\(\)4.0 & 94.5\(\)1.8 & **95.2\(\)**0.7 \\  & hopper & 74.9\(\)4.2 & 112.5\(\)0.2 & **112.6\(\)**0.3 & 55.6\(\)17.3 & 82.7\(\)7.3 & **84.3\(\)**4.7 \\   & walker2d & 108.2\(\)4.3 & **114.5\(\)**2.2 & 113.9\(\)0.9 & 88.3\(\)3.6 & 94.3\(\)0.9 & **98.9\(\)**3.3 \\   & Average on medium+expert & 88.9 & 109.3 & **109.9** & 73.6 & 90.5 & **92.8** \\   Average Score \\ Average Ranking \\  } & Average Score & 67.6 & **79.8** & 79.5 & 57.5 & 67.7 & **70.5** \\   & Average Ranking & 2.7 & **1.7** & **1.7** & 2.7 & 2.2 & **1.2** \\   \({}^{}\)_High standard deviation due to failure in one repetition, which can be mitigated by increasing \(\). Median rank: 31.3_

Table 1: _Performance evaluation on the D4RL dataset._ Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\(\)std) scores are averaged across four repetitions. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.

1. _Accuracy:_ \((_{}}^{Q}(s,a)|_{}Q(s,a)- }_{}Q(s,a)|)\) for the indicator function \(\), i.e., how often the reward penalizer is a valid \(-\)uncertainty quantifier as assumed by Theorem 1.
2. _Tightness:_ \(_{}}^{Q}(s,a)-|_{}Q(s,a)-}_{}Q(s,a)|\), i.e., how sharp a \(-\)uncertainty quantifier the reward penalizer is.

Table 2 shows that MOMBO provides more precise uncertainty estimates compared to the sampling-based approaches. It also indicates that MOMBO provides tighter estimates of the Bellman operator error, meaning that the sampling-based approaches use larger confidence radii. See Appendix C.1.2 for further details.

The D4RL dataset is a heavily studied benchmark database where many hyperparameters, such as penalty coefficients, are tuned by trial and error. We argue that this is not feasible in a realistic offline reinforcement learning setting where the central assumption is that policy search needs to be performed without real environment interactions. Furthermore, it is more realistic to assume that collecting data from expert policies is more expensive than random exploration. One would then need to perform offline reinforcement learning on a dataset that comprises observations obtained at different expertise levels. The mixed offline reinforcement learning dataset (Hong et al., 2023) satisfies both desiderata, as the baseline models are not engineered for its setting and its tasks comprise data from two policies: an expert or medium policy and a random policy, presented in varying proportions. We compare MOMBO against MOBILE and MOPO on this dataset for a fixed and shared penalty coefficient for all models. We find that MOMBO outperforms both baselines in final performance and learning speed in this more realistic setup. See Appendix C.1.3 and Table 3 for further details and results.

Our key experimental findings can be summarized as follows:

1. _MOMBO converges faster and trains more stably._ It learns more rapidly and reaches its final performance earlier as visible from the AULC scores. Its moment matching approach

    &  &  &  \\   & & MOPO & MOBILE & MOMBO & MOPO & MOBILE & MOMBO \\   & halfcheetah & \(0.02 0.0\) & \( 0.02\) & \(0.24 0.07\) & \(-14.58 0.63\) & \(-6.88 0.61\) & \( 0.28\) \\  & hopper & \(0.14 0.04\) & \( 0.03\) & \(0.61 0.07\) & \(-1.68 0.22\) & \(-0.64 0.2\) & \( 0.65\) \\  & walker2d & \(0.01 0.01\) & \(0.55 0.04\) & \( 0.06\) & \(-17 10^{i} 16 10^{j}\) & \(-0.27 0.08\) & \( 0.02\) \\   & halfcheetah & \(0.06 0.01\) & \(0.25 0.04\) & \( 0.04\) & \(-15.71 0.68\) & \(-10.03 0.55\) & \( 0.26\) \\  & hopper & \(0.04 0.01\) & \(0.41 0.01\) & \( 0.03\) & \(-4.16 1.24\) & \(-31.40 0.08\) & \( 0.21\) \\  & walker2d & \(0.02 0.01\) & \(0.16 0.01\) & \( 0.02\) & \(-8.91 0.61\) & \(-5.02 0.52\) & \( 0.24\) \\   & halfcheetah & \(0.09 0.01\) & \(0.04 0.0\) & \( 0.0\) & \(-11.67 0.94\) & \(-11.85 0.41\) & \( 0.06\) \\  & hopper & \(0.02 0.01\) & \(0.03 0.01\) & \( 0.01\) & \(-5.35 0.71\) & \(-3.41 0.01\) & \( 0.04\) \\  & walker2d & \(0.08 0.01\) & \(0.14 0.01\) & \( 0.02\) & \(-4.47 0.43\) & \(-4.56 0.13\) & \( 0.39\) \\   & halfcheetah & \(0.13 0.02\) & \(0.36 0.03\) & \( 0.03\) & \(-21.25 2.87\) & \(-13.22 0.47\) & \( 0.5\) \\  & hopper & \(0.04 0.02\) & \(0.43 0.02\) & \( 0.04\) & \(-9.77 2.44\) & \(-3.5 0.03\) & \( 0.02\) \\   & walker2d & \(0.05 0.01\) & \( 0.02\) & \(0.45 0.02\) & \(-9.77 0.02\) & \(-5.52 0.36\) & \( 0.14\) \\   

Table 2: _Uncertainty quantification on the D4RL dataset._ Accuracy and tightness (mean\(\)std) scores are averaged across four repetitions. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score.

Figure 2: Evaluation results on halfcheetah for four settings. The dashed, dotted, and solid curves represent the mean of the normalized rewards across ten evaluation episodes and four random seeds. The shaded area indicates one standard deviation from the mean.

provides more informative gradient signals and better estimates of the first two moments of the Bellman target compared to sampling-based approaches, which suffer from high estimator variance caused by Monte Carlo sampling.
2. _MOMBO delivers a competitive final training performance._ It ranks highest across all tasks and outperforms other model-based offline reinforcement learning approaches, including COMBO (Yu et al., 2021) with an average normalized reward of \(66.8\), TT (Janner et al., 2021) with \(62.3\), and RAMBO (Rigter et al., 2022) with \(67.7\), all evaluated on the same twelve tasks in the D4RL dataset. We took these scores from Sun et al. (2023).
3. _MOMBO delivers superior final training performance when expert data is limited._ MOMBO outperforms the baselines (at \(1\%\) in the mixed dataset) in both AULC and normalized reward. For normalized reward, MOMBO achieves \(37.0\), compared to \(32.4\) for MOBILE and \(27.9\) for MOPO, averaged across all tasks at \(1\%\). In terms of AULC, MOMBO attains \(29.5\), outperforming the \(28.0\) of MOBILE and the \(23.5\) of MOPO. See Table 3 for details.
4. _MOMBO provides more precise estimates of Bellman target uncertainty._ It outperforms both baselines in accuracy for \(9\) out of \(12\) tasks and in tightness all \(12\) tasks in the D4RL dataset.

## 6 Conclusion

The main objective of MOMBO is to accurately quantify the uncertainty surrounding a Bellman target estimate caused by the error in predicting the next state. We achieve this by deterministically propagating uncertainties through the value function with moment matching and introducing pessimism by constructing a lower confidence bound on the target Q-values. We analyze our model theoretically and evaluate its performance in various environments. Our findings may lay the groundwork for further theoretical analysis of model-based reinforcement learning algorithms in continuous state-action spaces. Our algorithmic contributions can also be instrumental in both model-based online and model-free offline reinforcement learning setups (An et al., 2021; Bai et al., 2022).

Limitations.The accuracy of the learned environment models sets a bottleneck on the performance of MOMBO. The choice of the confidence radius \(\) is another decisive factor in model performance. MOMBO shares these weaknesses with the other state-of-the-art model-based offline reinforcement learning methods (Yu et al., 2020; Lu et al., 2021; Sun et al., 2023) and does not contribute to their mitigation. Furthermore, our theoretical analyses rely on the assumption of normally distributed Q-values, which may be improved with heavy-tailed assumed densities. Considering the bounds, a limitation is that we claim a tighter bound compared to a baseline bound that we derived ourselves. However, the most important factor in that bound, \(R_{}^{2}/(1-)^{2}\), arises from Hoeffding's inequality, i.e., a classical statement. As such, we assume our bound to be rigorous and not inadvertently biased. Finally, our moment matching method is limited to activation functions for which the first two moments are either analytically tractable or can be approximated with sufficient accuracy, which includes all commonly used activation functions. Extensions to transformations such as, e.g., BatchNorm (Ioffe and Szegedy, 2015), or other activation functions remove the analytical tractability for moment matching. However, these are usually not used in our specific applications.