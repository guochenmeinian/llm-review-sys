# Learning Human-like Representations to Enable Learning Human Values

Andrea H. Wynn

Department of Computer Science

Princeton University

Princeton, NJ 08542

awynn13@jhu.edu

 Andrea Wynn is currently affiliated with the Department of Computer Science at Johns Hopkins University, Baltimore, MD.

&Ilia Sucholutsky

Department of Computer Science

Princeton University

Princeton, NJ 08542

is3060@nyu.edu

Thomas L. Griffiths

Department of Psychology

Department of Computer Science

Princeton University

Princeton, NJ 08542

tong@princeton.edu

 Andrea Wynn is currently affiliated with the Department of Computer Science at Johns Hopkins University, Baltimore, MD.

###### Abstract

How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values - including ethics, honesty, and fairness - training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.

## 1 Introduction

Machine learning models are becoming more powerful and operating in increasingly open environments. This makes it important to ensure that they learn to achieve an explicit objective without causing harm or violating human standards for acceptable behavior. This problem has motivated a growing interest in research on value alignment , which aims to ensure that models trained with few explicit restrictions still learn solutions that humans consider acceptable.

Creating reliably value-aligned models is a notoriously difficult challenge. One of the key challenges for many alignment methods has been that they seem to work only at the surface level, decreasing the rate of explicitly problematic model outputs while preserving internal biases that end up resurfacing during further interaction . Several recent studies have found implicit biases in large language models (LLMs) originating from biases represented in their training data, including widespread racial and gender biases [12; 15] that create major roadblocks for safely using these models in domains like education  and medicine . Current approaches to correcting these biases, such as reinforcement learning from human feedback (RLHF), show promise but also have significant limitations [e.g., for RLHF; 8; 11]. The difficulty of the alignment problem is further compounded in the case of personalization, where we want pre-trained agents to align with user preferences, values, or morals after only a small number of interactions . When deployed models are learning by directly interacting with users, it becomes crucial to ensure safe exploration [14; 2] - the model should not harm the user as it learns their preferences. Surprisingly, even though AI systems like GPT4  are now broadly available for customization and interact with millions of users every day, there has been little research on enabling safe, personalized alignment.

In this work, we aim to take a step towards understanding how machines can quickly and safely learn human values by identifying a previously overlooked factor that influences safe exploration in reinforcement learning agents. Specifically, we study how learning human-like representations

Figure 1: A visualization of our experimental setup. Representation spaces are modeled via pairwise similarity judgments given by language models and humans over the same set of stimuli. A machine learning agent takes such a representation space and tries to learn a human value function over those representations. We simulate personalization (the process of learning the value function), evaluating the agent on safe exploration, and evaluate the agentâ€™s ability to generalize to unseen examples.

can help language models learn human values quickly and safely. While researchers have explored correspondences between the representations of the world formed by humans and machines for a long time [9; 46; 48], recent work has explicitly shown that learning human-like representations (i.e., achieving "representational alignment") improves model generalization and robustness  and that explicitly modeling human concepts may be a pre-requisite for inferring human values from demonstrations [39; 38].

We propose that representational alignment is a helpful (although not sufficient) factor that contributes to achieving value alignment through safe exploration. Intuitively, sharing someone's representation of the world should make it easier to communicate with them and understand their values and preferences. While non-human representations may be better for some tasks, the task of learning human values and morals is intrinsically tied to learning things in a human way. Modern models often do not learn human-aligned representations, and are misaligned across many domains .

We design a simple reinforcement learning task involving morally-salient action choices, where the agent is tasked with learning human value preferences safely and efficiently. To accomplish this, we collect a new human value and similarity judgment dataset, encompassing human evaluations of textual action descriptions in the context of different human values. Our task and dataset allow us to simulate AI personalization settings where a pre-trained model interacts with users who provide feedback on the model's actions, which is then used to update the model. We simulate the full trajectory of user-AI interactions, tracking both how quickly and how safely the model learns a particular set of values, and how well it generalizes when presented with options it did not see during personalization. We use this task to demonstrate that representational alignment can support safe exploration and improved generalization ability over a wide range of human values.

## 2 Related Work

We experiment with classical machine learning algorithms, language embedding models, and state-of-the-art LLMs and find a strong, positive correlation between representational alignment and task performance. Models with more human-like representations learn human values more safely and efficiently, in addition to improved generalization. Further, we find that representational alignment has a negative correlation with unsafe actions taken during personalization. Our results suggest that developing AI systems whose internal representations are aligned with those of humans may enable quickly and safely learning human values when interacting with users, such as through the example interaction in Figure 1.

**Representational alignment:** Representational alignment is the degree of agreement between the internal representations of two (biological or artificial) information processing systems [48; 36], and is often assessed by comparing similarity judgments given by different agents over the same set of stimuli. Recent work has increasingly explored representational alignment between humans and machines [29; 24; 30; 27; 25; 10] and has shown that machine learning models that learn human-aligned representations often perform better in few-shot learning settings, have better generalization ability, and are more robust to adversarial attacks and domain shifts than non-human-aligned models . Representational misalignment has also recently been proposed as one of the two key drivers of disagreement between agents . Having more human-aligned representations of the world helps to improve trust in these systems because humans can better understand what they learn, increasing opportunities for deployment for a wider set of human-centric use cases . However, many modern machine learning models do not naturally learn human-like representations of the world [26; 34; 28; 32; 31]. These models are also not actively encouraged to learn more human-like representations, despite the known benefits. In this work, we seek to provide additional motivation for pushing ML agents to learn more human-aligned representations.

**Value alignment and safe exploration:** In many settings, it is difficult to measure value alignment because the task is simple or not well suited to asking value-related questions. For example, there is no clear set of values that we would want an image classification or object detection model to learn, outside of completing its relatively well-defined objective, although researchers are increasingly identifying additional objectives related to fairness and bias-minimization [22; 20]. In contrast, the question of value alignment and safe exploration arises quite naturally in reinforcement learning (e.g. [4; 44]), where an agent is given autonomy to act within its environment and thus can make harmful and poorly aligned choices as it learns. Therefore, we focus on reinforcement learning in this work,in particular a multi-armed bandit setting where we study morally relevant actions with a clear link to human values. In particular, in addition to studying whether AI agents are capable of learning a human-centric reward function with feedback, we also study characteristics of the agent's learning process with respect to safe exploration (taking fewer harmful actions) and fast learning (needing less feedback to learn), and show how representational alignment with humans impacts these.

**Connecting representations with human values:** Some related work  has shown that modifying the objective function of a machine learning model trained to classify images has a significant impact on how human-aligned the model's representations become after training. Value alignment is also inherently related to objective functions, as the model's main goal is to optimize its objective. A standard practice in machine learning includes adding some kind of explicit regularization term or constraints to the model to try to constrain the model to learn an "acceptable" solution. However, there is currently no work that directly explores the relationship between representational alignment and safe exploration of human values. Much of the existing literature on representational alignment is in the context of classification in computer vision, and does not consider settings such as reinforcement learning where the model has more autonomy and alignment becomes much more critical. Further, many existing papers provide empirical evaluations of how changing certain model parameters affect representational alignment, but do not provide significant motivation to improve representational alignment. In our work, we directly study the relationship between representational and value alignment by demonstrating that more representationally-aligned agents are able to learn human value functions much faster and more safely and generalize better to new examples.

## 3 Problem Formulation

We begin by presenting a motivating example. Let's say that we are building a robot assistant that should be personalized to learn an individual human's values. Each time the robot takes an action, the human gives it feedback, and it learns from the feedback. Of course, eventually the assistant should learn the human's value function; but perhaps more importantly, it should not take harmful actions as it learns. For example, the robot should not harm another person before it learns that the human thinks this (and other actions they consider "harmful") are bad. Instead, as soon as the robot does something the human considers slightly harmful (for instance, stealing candy), and the human gives it a penalty, we want the robot to learn that not only should it not steal candy, it also should not perform similar actions that the human also considers harmful; this relates to the idea of safe exploration. Additionally, we do not want the robot to require many (e.g., hundreds or thousands) rounds of feedback to learn the human's values, because the human may be unwilling or unable to provide this much real-time feedback.

Re-using a particular learned representation space is already a common approach, such as when using embeddings from a pretrained model to perform another task. In our experiment, we seek to study how representational alignment affects learning human values, so we freeze the agent's representations, letting them learn only the mapping from representations over inputs to a value function. This requires the ability to define a particular representation space that does not change even as the agent learns to solve some particular task.

In our experiment, we characterize the representations of machine learning agents using kernels, defined using pairwise similarity judgments. The kernel trick is then used to make predictions. Mathematically, the kernel trick re-formulates the agent's optimization problem such that instead of depending directly on the input features, it depends only on a sum over the dot products between all input pairs. We can then approximate an agent's representation space using a pairwise similarity matrix, and provide this representation space to the agent (rather than allowing the agent to learn its own representation space, which is a common approach in deep learning methods). For a mathematical introduction to the methods used (particularly the kernel trick), please refer to Section A.1. We collect data on two metrics: mean morality reward received by the agent per time-step and the number of bad (i.e., morality score \(<50\)) actions taken. These metrics help to measure safe exploration and generalization ability of the agents during the personalization and generalization phases. The number of bad actions taken is particularly relevant for safe exploration settings where agents learn in the real world and must avoid causing harm during their learning process . We describe our setup in more detail in Algorithm 1. We first justify our experimental approach by presenting a theoretical analysis, then perform a set of simulated experiments to empirically validate the theory.

### Theory

Consider a setting where we have two sets of actions for which we would like a student to learn preference or morality scores. The first set of actions, \(X^{p}\), are the ones for which we have feedback available (we later refer to these as actions from the "personalization" phase) and we teach the student our preferences, \(y^{p}\), over these actions. The second set of actions, \(X^{g}\), are ones for which we do not have demonstrations available (we later refer to these as actions from the "generalization" phase), and we hope that students generalize their understanding of our preferences to these new actions (where our associated values are \(y^{g}\)) based on what they have learned from the first set of actions. Suppose a teacher has a set of representations that can be described by a kernel function \(k_{T}(x_{i},x_{j})\), corresponding to the degree of similarity between actions \(x_{i},x_{j}\), and a student has a (potentially different) set of representations described by a kernel function \(k_{S}(x_{i},x_{j})\). For simplicity, we denote the set of pairwise similarity judgments across all unique pairs of actions in \(X\) by \(k(X)\). Representational alignment is the degree of agreement between these two kernels. In our experiments, we instantiate this as the correlation of similarities across a fixed set of stimuli, \(R(T,S):=(k_{T}(X),k_{S}(X))\). Our goal is to identify the relationship between \(R\) and student generalization performance.

Let us consider the case where the student's learning function can be described by a Gaussian process regression3. Suppose the student has already been trained on the personalization actions (\(X^{p}\)) with associated values \(y^{p}\). For Gaussian processes, the covariance matrix is defined based on the similarity matrix, so if the student had the same kernel function as the teacher, then the student would have covariance matrix \(K_{T}\) (corresponding to kernel function \(k_{t}\)) and the student's estimate of the mean values for the new set of actions (\(X^{g}\)) would be \(^{g}=K_{T}^{*}K_{T}^{}y^{p}\), where \(K^{*}\) corresponds to covariance between new actions and old actions (i.e., \(k(x_{i}^{g},x_{j}^{p}), x_{i}^{g} X^{g},x_{j}^{p} X^{p}\)). However, if the student was representationally misaligned from the teacher (i.e., \(k_{S}\) is different from \(k_{T}\)) then the student's estimate of the mean values for the new set of actions (\(X^{g}\)) would be \(^{g}=K_{S}^{}K_{S}^{}y^{p}\). Thus, the change in student predictions (i.e., the error) due to representational misalignment can be defined as \(|^{g}-^{g}|\).

Say \(K\) is a matrix where every element is an iid r.v., and overload our notation to refer to that random variable as \(K\). Given \((K_{S},K_{T})=:_{0}\), then \(^{2}_{K_{S}-K_{T}}=^{2}_{K_{S}}+^{2}_{K_{T}}-2(K_ {S},K_{T}),(K_{S},K_{T})=_{K_{S}}_{K_{T}}\). Using Chebyshev's Inequality, \(P(|K_{S}-K_{T}-_{S}+_{T}| c_{K_{S}-K_{T}})}\). Applying some simplifying assumptions (\(^{2}_{K_{S}}=^{2}_{K_{T}}=^{2},_{S}=_{T}=_{S}=_{T}\)\(0\)) we get that \(P(|K_{S}-K_{T}| c)}))}\). Thus, in expectation, the gap between \(K_{S}\) and \(K_{T}\) (similarly for \(K_{S}^{*}\) and \(K_{T}^{*}\)) grows sublinearly with decreasing correlation.

Analyzing the effect of misalignment over the inverse correlation matrix on student performance is more difficult. To explore this, consider the case where we have two training examples (\(x_{0}^{p},x_{1}^{p}\)) and one test example (\(x^{g}\)). Let \(c_{i}^{g}:=(x_{i}^{p},x^{g}),c^{p}:=(x_{0}^{p},x_{1}^{p}), _{i}^{2}:=(x_{i}^{p})\). We can analytically write out the prediction,

\[^{g}=K_{T}^{*}K_{T}^{-1}y^{p}=^{g}y_{0}^{p}_{1}^{2 }-c_{0}^{g}y_{1}^{p}c^{p}+c_{1}^{g}y_{1}^{p}_{0}^{2}-c_{1}^{g}y_{0}^{p}c^ {p}}{_{0}^{2}_{1}^{2}-{c^{p}}^{2}}.\]

Applying some simplifying assumptions (\(c_{0}^{g}=c_{1}^{g}=c^{g},_{0}^{2}=_{1}^{2}=^{2}\)), we get that \(^{g}=(y_{0}^{p}+y_{1}^{p})}{^{2}+c^{p}}\). First, consider the case where we have misalignment between the teacher and student in \(K^{*}\), which means differing student and teacher estimates of the covariance between training and test examples (\(c_{T}^{g} c_{S}^{g}\)). The error is then \(|^{g}-^{g}|=|^{g}-c_{S}^{g})(y_{0}^{p}+y_{1}^{p}) }{^{2}+c^{p}}|\). Next, consider the case where we have misalignment between the teacher and student in \(K\), which means \(c_{T}^{p} c_{S}^{p}\). The error is then \(|^{g}-^{g}|=|)(y_{0}^{p}+y_{1}^{p})}{^{2}+c _{T}^{p}}-)(y_{0}^{p}+y_{1}^{p})}{^{2}+c_{S}^{g}}|=|^{p}-c_{S}^{g})c^{g}(y_{0}^{p}+y_{1}^{p})}{(^{2}+c_{S}^{g})(^{2}+c _{T}^{p})}|\). Thus, the error grows monotonically as representational alignment decreases. Furthermore, misalignment in \(K^{*}\) has a larger effect on student performance than the same degree of misalignment in \(K\) does.

We can extend this result to the case where there are \(n\) training examples and \(m\) test examples. Let \(e_{m},e_{n}\) be column vectors consisting of \(m\) and \(n\) ones, respectively. To allow us to find the analytical form of the prediction expression, suppose that covariance between each pair of training examples is \(c^{p} 1\), that training examples are normalized to have variance \(1\), and that the covariance between each pair of train and test examples is \(c^{g}\). Then \(K_{T}=(1-c^{p})I+c^{p}e_{n}e_{n}^{}\) and \(K_{T}^{*}=c^{g}y_{m}y_{n}^{}\). Applying the Sherman-Morrison formula and simplifying the resulting expression we get \(K_{T}^{-1}=(1-c^{p})^{-1}(I-}{1+(n-1)c^{p}}e_{n}e_{n}^{})\). Thus, the prediction is now \(^{g}=K_{T}^{*}K_{T}^{-1}y^{p}=[1+(k-2)c^{p}]}{(1-c^{p} )[1+(k-1)c^{p}]}e_{m}e_{k}^{}y^{p}\). Misalignment in \(K^{*}\), which can be represented by \(|c_{T}^{g}-c_{S}^{g}|=\), results in error \(|^{g}-^{g}|=| d^{p}|y^{p}\) where \(d^{p}\) is a function of \(c^{p}\) but constant in \(c^{g}\). Thus, error due to misalignment in \(K^{*}\) grows linearly. Misalignment in \(K\), which can be represented as \(|c_{T}^{p}-c_{S}^{p}|=\), results in error \(|^{g}-^{g}|=|(^{p}}-^{p}+ }d^{g}|y^{p}\) where \(d^{g}\) is a function of \(c^{g}\) but constant in \(c^{p}\). Thus, error due to misalignment in \(K\) ranges from \(0y^{p}\) to \(|(1-c_{T}^{p})^{-1}d^{g}|y^{p}\) and grows sublinearly with \(\). The resulting conclusions are therefore the same as in the special case of two training examples and one test example, the error grows monotonically as representational alignment decreases and misalignment in \(K^{*}\) has a larger effect on student performance than the same degree of misalignment in \(K\) does.

### Synthetic Experiments

To evaluate the predictions of the theoretical model presented above, we begin with a contextual multi-armed bandit experiment as described in Algorithm 1. The reward distribution for each action (arm of the bandit) is parameterized by a morality score \(m_{i}[-3,3]\) for each action \(i\). This range was inspired by , in which there are 3 tiers of severity when measuring both moral and immoral actions, translating nicely into 3 numbers above and 3 below zero in morality scores. We are interested in studying whether more representationally-aligned agents are better at learning a value function. In particular, in addition to the mean reward and bad actions taken (as described previously), we measure how many times the agent takes an action that is not the most moral available (non-optimal actions); how long it takes for the agent to effectively learn the value function (iterations to convergence); and the number (out of 50) unique actions the agent needed to take before it was able to learn the value function (unique actions taken).

We define a kernel using a similarity matrix indicating pairwise similarity between all actions, which is directly provided to the agent as a kernel. We begin with a perfectly representationally-aligned agent, where this similarity matrix directly reflects the simulated human value function to learn (i.e. morality scores). We generate multiple such environments and run a representationally-aligned agent until it converges (ie. takes the most moral action available 5 times in a row), collecting data on the mean morality and number of bad actions taken by the agent. We then repeat this process while corrupting the similarity matrix that is passed to the agent, which decreases the agent'srepresentational alignment. To do this, we choose between 0 and 50 actions and, for each action, replace its morality score with a new randomly sampled score that does not reflect the ground truth. We then use these random scores, along with the original ground truth morality scores for the remaining actions, to compute a corrupted similarity matrix between all actions that is given to the agent as a kernel.

The representational alignment of a particular agent is measured as the Spearman correlation between the upper triangular, off-diagonal entries of the corrupted and actual similarity matrix (because diagonal entries are all the same, and the similarity matrix is symmetric). A Spearman correlation of 1.0 corresponds to a perfectly representationally-aligned agent, and a lower correlation corresponds to a lower amount of representational alignment. We collected data over a total of over 2300 individual experiments for three different agents (Gaussian process regression, kernel regression, and support vector regression), where each experiment had a different amount of corruption in the kernel matrix.

The results of the experiment are shown in Figure 2. The results are binned by representational alignment between similarity matrices (with a bin size of 0.05) and the average for each bin is displayed, with shaded intervals in each figure corresponding to one standard error. We used Thompson sampling, a popular Bayesian approach for solving multi-armed bandit problems , as a baseline method for comparison with the kernel-based agents. On all subsequent plots of results, the Thompson sampling baseline is indicated via a dotted red line for all metrics. More details can be found in the Appendix in Section A.1.2. The results confirm the theoretical prediction that as representational alignment decreases, the agent's mean reward decreases and number of immoral actions taken increases. These results are statistically significant (see Table 1 in the Appendix for details). It is also worth noting that there is a point after which leveraging a representation space is helpful and before which it is harmful (relative to baseline). Further, we observe similar results for all three models, indicating that these results are systematic and not model-dependent.

## 4 Learning Human Morality Judgments

As a further test of our theoretical predictions and a demonstration of the empirical approach with real humans, we set up an experiment where we train an agent to learn human morality judgments. We focus on morality as a specific aspect of human values that can be approximated using a single numerical score. Humans take complex moral considerations into account when evaluating the morality of an action, and mapping these to a single scalar judgment is a common approach [19; 18], though in general human values are still difficult to quantify. We first create a set of 50 textual descriptions of morally relevant actions (adapted from the Justice category of actions in the ETHICS dataset ). We begin with the situations described in ETHICS and manually re-write them into atomic actions that can be taken by a reinforcement learning agent. For instance, "I think individuals deserve to pick up illegal items at the post office." is changed to "Pick up illegal items at the post office." The full list of action descriptions is provided in the Appendix (see Section A.12).

Figure 2: Agent performance in simulated experiments, plotted against representational alignment.

We collect scalar human morality judgments over these actions to use as a reward signal in the multi-armed bandit setting. Humans take complex ethical considerations into account when evaluating the morality of an action, and mapping these to a single scalar judgment is a common approach [19; 18], though in general human values are still difficult to quantify. Additionally, we collect human pairwise similarity judgments over the set of actions for measuring the representational alignment of each language model. Detailed methods for collecting human judgments are outlined in Section A.11.

### Embedding Models

We retrieve embeddings for each textual action description from a total of 16 embedding models, consisting of 13 embedding models from the HuggingFace sentence-transformers model zoo (see Table 5 for the full list of models) [41; 43; 49; 42; 51; 50], Google's USE model, Doc2Vec , and OpenAI's text-embedding-ada-002 model. Distances between each pair of embeddings are computed and used to construct a similarity matrix between actions for each embedding model.

**GPT similarity judgments:** Similarity judgments were additionally collected from GPT-4o (OpenAI's gpt-4o) GPT-3.5 Turbo (OpenAI's gpt-35-turbo), GPT-4 , and GPT-4-1106-preview (OpenAI's gpt-4-1106-preview) via the following prompt: "How related are these two concepts on a scale of 0 (unrelated) to 1 (highly related)? Reply with a numerical rating and no other text. Concept 1: First Action Description Concept 2: Second Action Description Rating:"

**Measuring representational alignment:** Each language model's similarity matrix is used as a kernel for our machine learning agent. The representational alignment is measured the same way as in the simulated experiments. A Spearman correlation of 1.0 corresponds to a perfectly representationally-aligned agent, and a lower correlation corresponds to a lower amount of representational alignment. The degree of representational alignment for all language models is shown in Table 5.

**Personalization phase:** In the personalization phase, the agent takes actions in its environment and learns from these actions. The agent is only allowed to take 25 of the 50 actions (the personalization set). We limit the agent to 1000 time-steps to reflect real-world constraints on human-in-the-loop learning. In any situation where human feedback is required for learning, it is expensive, difficult, or sometimes impossible to collect a larger number of training examples. We summarize our procedure for the personalization phase of a single experiment in Algorithm 1.

**Generalization phase:** In the generalization phase, we repeat Algorithm 1, with two differences. First, instead of the 25 actions seen during the personalization phase, the agent can only choose from the 25 other actions that it has not yet seen. Additionally, the agent's parameters are not updated, so it is evaluated purely on its ability to generalize its learned human value function to previously unseen actions, using its pre-defined representations.

### Results

Figure 3 shows the agents' overall performance during both personalization and generalization. We measure performance of agents in terms of mean reward (i.e. mean morality score), as well as number of immoral actions taken. We seek to develop learning agents who can both learn human values effectively (generalization ability) and perform their learning process in a safe, harmless manner (personalization and safe exploration), and these metrics help us to evaluate agents' performance with respect to both of these goals.

Each data point corresponds to a single language model, and the mean reward and immoral actions taken are measured as an average over 100 experiments run per language model. We evaluate the statistical significance of these results by computing Spearman correlations between representational alignment and the two metrics used to measure performance, which are presented in Table 2. As predicted by our theoretical analysis, misalignment in \(K^{*}\) (similarities between personalization and generalization actions) is a bigger driver of decreasing student performance than misalignment in \(K\) (similarities over personalization actions only); see Table 2. We report results for a kernel regression agent as defined in Section 3.2.

## 5 Representational Alignment Supports Learning Multiple Human Values

To extend the previous experiments on human data, we would like to see if aligning representations with humans can help learn not only morality, but also a wide range of tasks that draw on different human values. In particular, we ask participants to evaluate the same action descriptions on a scale of 0-100, but over a total of 10 distinct values, as listed in Table 2. The prompts shown to human survey respondents are listed in the Appendix (see Section A.11). We then average the ratings from 20 human evaluators to determine a score for our machine learning agents to learn. Following this, we repeat the embedding model kernel experiment from Section 4.1 for each of the human values.

Results showing the mean reward of the agents for both personalization and generalization are shown in Figure 4. These results support the claim that representational alignment with humans enables learning a wide range of human values quickly and safely, as well as improving generalization ability to apply these values in previously unseen contexts. However, there is a notable exception to this. When learning to predict the difficulty of tasks, agents with higher representational alignment exhibited somewhat safer exploration, but there was not a statistically significant effect of representational alignment in the generalization phase. We suspect that this is because the difficulty or challenge level of a particular action can vary greatly based on each individual human and their own abilities or

Figure 4: Results of running the experiment across 10 different human values. Representational alignment vs. mean reward for all models (including best fit lines) for both personalization and generalization.

Figure 3: We evaluate agents on both personalization (safe exploration) and generalization ability for 100 experiments each and observe the results from both phases. Results are shown for all models.

comfort level, meaning that these ratings are less reflective of some shared notion of human values than originally anticipated and are thus not strongly correlated with a common human representation. However, future work could study if representational alignment with a particular individual could help to learn these highly individualized values as well. Additional results are reported in Section A.8 of the appendix, including correlations with p-values (Table 2) and bad actions taken (Figure A.8).

**Control experiment:** We also performed a control experiment using a trivial reward function, defined as the number of characters in each action description. This control experiment validates our results by demonstrating that the human kernel is not the best choice for all tasks, and that a kernel based on length will be helpful for learning that particular reward but ineffective at learning human values. The full results of the control experiment are in the Appendix (see Section A.9).

## 6 Discussion

AI systems rely on their representations when learning to follow human values. Our results provide strong evidence that an AI system's representational alignment with humans affects its ability to learn, and thus act in line with, human values. This is true even when these representations are not directly correlated with the values we would like the AI system to learn. The additional complexity of realistic settings makes human value functions much harder to learn, even with perfect representational alignment (or the ability to perfectly simulate human representations). This would indicate that for a more complex learning environment, there will be an upper bound on value alignment driven by the agent trying to learn a human value function from misaligned representations.

**Limitations and Future Directions.** Though our experiments demonstrate that representational alignment supports learning a fixed set of human values, this may not be true for all models and architectures. In addition, our human value experiments focused only on a limited set of actions an agent could take, namely actions taken from [the "Justice" category defined by the ETHICS dataset; 18]. In reality, there are many dimensions to human values, with significant variations at both cultural and individual levels (as shown in , which specifically quantifies this variation). Our human value scores were collected from English-speaking internet users from the US and, as a result, are not representative of all perspectives. While we believe our study confirms that representational alignment is an important component of solving the AI alignment problem, future work should collect a larger and more diverse set of human judgments and examine the role of representational alignment in adapting models to individual and cultural differences. Future work should also explore how the action selection strategy used by LLMs in a text-based reinforcement learning setting differs from traditional methods like Thompson sampling and kernel regression and could enable them to converge faster on tasks like ours.

This work could potentially introduce another dimension to consider when working towards building more ethical AI systems that are aligned with societal values. While we hope that our study will provide a new avenue for creating safe, moral, and aligned AI systems, we acknowledge that morality is a significantly more complex and multi-faceted concept than can be captured in a small number of ratings by English-speaking internet users. Our study is intended only to highlight the importance of aligning models' internal representations with the representations of their users. Our dataset should not be used as a benchmark for determining whether models are safe or moral.

**Conclusion.** Our results pave the way for future work studying the relationship between representational and value alignment for more complex AI systems. One potential application would be using representational alignment with humans as a criterion for choosing model architectures, training datasets, and tuning hyperparameters. We hope our work encourages greater collaboration between studying AI safety and alignment researchers and representation learning researchers.