# Causal Fairness for Outcome Control

Drago Plecko

Elias Bareinboim

Department of Computer Science

Columbia University

dp3144@columbia.edu, eb@cs.columbia.edu

###### Abstract

As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called _outcome control_ in which an automated system aims to optimize an outcome variable \(Y\) while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of _benefit_, which captures how much a specific individual would benefit from a positive decision, counterfactually speaking, when contrasted with an alternative, negative one. We introduce the notion of benefit fairness, which can be seen as the minimal fairness requirement in decision-making, and develop an algorithm for satisfying it. We then note that the benefit itself may be influenced by the protected attribute, and propose causal tools which can be used to analyze this. Finally, if some of the variations of the protected attribute in the benefit are considered as discriminatory, the notion of benefit fairness may need to be strengthened, which leads us to articulating a notion of causal benefit fairness. Using this notion, we develop a new optimization procedure capable of maximizing \(Y\) while ascertaining causal fairness in the decision process.

## 1 Introduction

Decision-making systems based on artificial intelligence and machine learning are being increasingly deployed in real-world settings where they have life-changing consequences on individuals and on society more broadly, including hiring decisions, university admissions, law enforcement, credit lending, health care access, and finance . Issues of unfairness and discrimination are pervasive in those settings when decisions are being made by humans, and remain (or are potentially amplified) when decisions are made using machines with little transparency or accountability. Examples include reports on such biases in decision support systems for sentencing , face-detection , online advertising , and authentication . A large part of the underlying issue is that AI systems designed to make decisions are trained with data that contains various historical biases and past discriminatory decisions against certain protected groups, and such systems may potentially lead to an even more discriminatory process, unless they have a degree of fairness and transparency.

In this paper, we focus on the specific task of _outcome control_, characterized by a decision \(D\) which precedes the outcome of interest \(Y\). The setting of outcome control appears across a broad range of applications, from clinical decision-making  and public health , to criminal justice  and various welfare interventions . In general, outcome control will cover settings in which aninstitution (such as a hospital or a social service) may attempt to maximize an outcome (such as survival or well-being) using a known control (such as surgery or a welfare intervention)1. Importantly, due to historical biases, certain demographic groups may differ in their distribution of covariates and therefore their benefit from treatment (e.g., from surgery or intervention) may be lower than expected, leading to a lower allocation of overall resources. We next discuss two lines of related literature.

Firstly, a large body of literature in reinforcement learning  and policy learning  analyzes the task of optimal decision-making. Often, these works consider the conditional average treatment effect (CATE) that measures how much probabilistic gain there is from a positive versus a negative decision for a specific group of individuals when experimental data is available. Subsequent policy decisions are then based on the CATE, a quantity that will be important in our approach as well. The focus of this literature is often on developing efficient procedures with desirable statistical properties, and issues of fairness have not traditionally been explored in this context.

On the other hand, there is also a growing literature in fair machine learning, that includes various different settings. One can distinguish three specific and different tasks, namely (1) bias detection and quantification for currently deployed policies; (2) construction of fair predictions of an outcome; (3) construction of fair decision-making policies. Most of the work in fair ML falls under tasks (1) and (2), whereas our setting of outcome control falls under (3). Our work also falls under the growing literature that explores fairness through a causal lens . For concreteness, consider the causal diagram in Fig. 1 that represents the setting of outcome control, with \(X\) the protected attribute, \(Z\) a possibly multidimensional set of confounders, \(W\) a set of mediators. Decision \(D\) is based on the variables \(X,Z\), and \(W\), and the outcome \(Y\) depends on all other variables in the model. In this setting, we also assume that the decision-maker is operating under budget constraints.

Previous work introduces a fairness definition that conditions on the potential outcomes of the decision \(D\), written \(Y_{d_{0}},Y_{d_{1}}\), and ensures that the decision \(D\) is independent of the protected attribute \(X\) for any fixed value of \(Y_{d_{0}},Y_{d_{1}}\). Another related work, in a slightly different setting of risk assessment , proposes conditioning on the potential outcome under a negative decision, \(Y_{d_{0}}\), and focuses on equalizing counterfactual error rates. Finally, the work  is also related to our setting, and proposes that fairness should be assessed with respect to three causal effects of interest: (i) total effect \(X Y\); (ii) total effect \(D Y\); and (iii) total effect \(X D\). Our framework, however, integrates considerations of the above effects more coherently, and brings forward a new definition of fairness based on first principles.

However, the causal approaches mentioned above take a different perspective from the policy learning literature, in which policies are built based on the CATE of the decision \(D\), written \([Y_{d_{1}}-Y_{d_{0}} x,z,w]\), which we will refer to as _benefit2_. Focusing exclusively on the benefit, though, will provide no fairness guarantees apriori. In particular, as can be seen from Fig. 1, the protected attribute \(X\) may influence the effect of \(D\) on \(Y\) in three very different ways: (i) along the direct pathway \(X Y\); (ii) along the indirect pathway \(X W Y\); (iii) along the spurious pathway \(X Z Y\). Often, the decision-maker may view these causal effects differently, and may consider only some of them as discriminatory. Currently, no approach in the literature allows for a principled way of detecting and removing discrimination based on the notion of benefit, while accounting for different underlying causal mechanisms that may lead to disparities.

In light of the above, the goal of this paper is to analyze the foundations of outcome control from a causal perspective of the decision-maker. Specifically, we develop a causal-based decision-making framework for modeling fairness with the following contributions:

1. We introduce Benefit Fairness (BF, Def. 2) to ensure that at equal levels of the benefit, the protected attribute \(X\) is independent of the decision \(D\). We then develop an algorithm for achieving BF (Alg. 1) and prove optimality guarantees (Thm. 2),
2. We develop Alg. 2 that determines which causal mechanisms from \(X\) to the benefit (direct, indirect, spurious) explain the difference in the benefit between groups. The decision-maker can then decide which causal pathways are considered as discriminatory.

Figure 1: Standard Fairness Model (SFM)  extended for outcome control.

3. We define the notion of causal benefit fairness (CBF, Def. 3), which models discriminatory pathways in a fine-grained way. We further develop an algorithm (Alg. 3) that ensures the removal of such discriminatory effects, and show some theoretical guarantees (Thm. 4).

### Preliminaries

We use the language of structural causal models (SCMs) as our basic semantical framework . A structural causal model (SCM) is a tuple \(:= V,U,,P(u)\), where \(V\), \(U\) are sets of endogenous (observable) and exogenous (latent) variables respectively, \(\) is a set of functions \(f_{V_{i}}\), one for each \(V_{i} V\), where \(V_{i} f_{V_{i}}((V_{i}),U_{V_{i}})\) for some \((V_{i}) V\) and \(U_{V_{i}} U\). \(P(u)\) is a strictly positive probability measure over \(U\). Each SCM \(\) is associated to a causal diagram \(\) over the node set \(V\), where \(V_{i} V_{j}\) if \(V_{i}\) is an argument of \(f_{V_{j}}\), and \(V_{i} V_{j}\) if the corresponding \(U_{V_{i}},U_{V_{j}}\) are not independent. Throughout, our discussion will be based on the specific causal diagram known as the standard fairness model (SFM, see Fig. 1). Further, an instantiation of the exogenous variables \(U=u\) is called a _unit_. By \(Y_{x}(u)\) we denote the potential response of \(Y\) when setting \(X=x\) for the unit \(u\), which is the solution for \(Y(u)\) to the set of equations obtained by evaluating the unit \(u\) in the submodel \(_{x}\), in which all equations in \(\) associated with \(X\) are replaced by \(X=x\).

## 2 Foundations of Outcome Control

In the setting of outcome control, we are interested in the following decision-making task:

**Definition 1** (Decision-Making Optimization).: _Let \(\) be an SCM compatible with the SFM. We define the optimal decision problem as finding the (possibly stochastic) solution to the following optimization problem given a fixed budget \(b\):_

\[D^{*}= *{arg\,max}_{D(x,z,w)} [Y_{D}]\] (1) _subject to_ \[P(d) b.\] (2)

The budget \(b\) constraint is relevant for scenarios when resources are scarce, in which not all patients possibly requiring treatment can be given treatment. In such a setting, the goal is to treat patients who are most likely to benefit from the treatment, as formalized later in the text. We next discuss two different perspectives on solving the above problem. First, we discuss the problem solution under perfect knowledge, assuming that the underlying SCM and the unobserved variables are available to us (we call this the oracle's perspective). Then, we move on to solving the problem from the point of view of the decision-maker, who only has access to the observed variables in the model and a dataset generated from the true SCM.

### Oracle's Perspective

The following example, which will be used throughout the paper, is accompanied by a vignette that performs inference using finite sample data for the different computations described in the sequel. We introduce the example by illustrating the intuition of outcome control through the perspective of an all-knowing oracle:

**Example** (Cancer Surgery - continued).: _A clinical team has access to information about the sex of cancer patients (\(X=x_{0}\) male, \(X=x_{1}\) female) and their degree of illness severity determined from tissue biopsy (\(W\)). They wish to optimize the 2-year survival of each patient (\(Y\)), and the decision \(D=1\) indicates whether to perform surgery. The following SCM \(^{*}\) describes the data-generating mechanisms (unknown to the team):_

\[^{*},P^{*}(U):\{ & X U_{X}\\ & W\{ &}X=x_{0},\\ & 1-}X=x_{1}.\\ & D f_{D}(X,W)\\ & Y(U_{Y}+WD-W>0.5).\\ & U_{X}\{0,1\},P(U_{X}=1)=0.5,\\ & U_{W},U_{Y},.\] (3)_where the \(f_{D}\) mechanism is constructed by the team._

_The clinical team has access to an oracle that is capable of predicting the future perfectly. In particular, the oracle tells the team how each individual would respond to surgery. That is, for each unit \(U=u\) (of the 500 units), the oracle returns the values of_

\[Y_{d_{0}}(u),Y_{d_{1}}(u).\] (9)

_Having access to this information, the clinicians quickly realize how to use their resources. In particular, they notice that for units for whom \((Y_{d_{0}}(u),Y_{d_{1}}(u))\) equals \((0,0)\) or \((1,1)\), there is no effect of surgery, since they will (or will not) survive regardless of the decision. They also notice that surgery is harmful for individuals for whom \((Y_{d_{0}}(u),Y_{d_{1}}(u))=(1,0)\). These individuals would not survive if given surgery, but would survive otherwise. Therefore, they ultimately decide to treat 100 individuals who satisfy_

\[(Y_{d_{0}}(u),Y_{d_{1}}(u))=(0,1),\] (10)

_since these individuals are precisely those whose death can be prevented by surgery. They learn there are 100 males and 100 females in the \((0,1)\)-group, and thus, to be fair with respect to sex, they decide to treat 50 males and 50 females. _

The space of units corresponding to the above example is represented in Fig. 1(a). The groups described by different values of \(Y_{d_{0}}(u),Y_{d_{1}}(u)\) in the example are known as _canonical types_ or _principal strata_. Two groups cannot be influenced by the treatment decision (which will be called "Safe" and "Doomed", see Fig. 1(a)). The third group represents those who are harmed by treatment (called "Harmed"). In fact, the decision to perform surgery for this subset of individuals is harmful. Finally, the last group represents exactly those for whom the surgery is life-saving, which is the main goal of the clinicians (this group is called "Helped").

This example illustrates how, in presence of perfect knowledge, the team can allocate resources efficiently. In particular, the consideration of fairness comes into play when deciding which of the individuals corresponding to the \((0,1)\) principal stratum will be treated. Since the number of males and females in this group is equal, the team decides that half of those treated should be female. The approach described above can be seen as appealing in many applications unrelated to to the medical setting, and motivated the definition of principal fairness . As we show next, however, this viewpoint is often incompatible with the decision-maker's perspective.

### Decision-Maker's Perspective

We next discuss the policy construction from the perspective of the decision-maker:

**Example** (Cancer Surgery - continued).: _The team of clinicians constructs the causal diagram associated with the decision-making process, shown in Fig. 1(b). Using data from their electronic health records (EHR), they estimate the benefit of the treatment based on \(x,w\):_

\[[Y_{d_{1}}-Y_{d_{0}} w,x_{1}]=[Y_{d_{1}}-Y_{d_{0}}  w,x_{0}]=.\] (11)

_In words, at each level of illness severity \(W=w\), the proportion of patients who benefit from the surgery is the same, regardless of sex. In light of this information, the clinicians decide to construct

Figure 2: (a) Units in the Oracle example; (b) Standard Fairness Model (SFM) for the cancer surgery example, corresponding to \(W=\{W\}\) and \(Z=\) in the SFM in Fig. 1.

the decision policy \(f_{D}\) such that \(f_{D}(W)=(W>)\). In words, if a patient's illness severity \(W\) is above \(\), the patient will receive treatment._

_After implementing the policy and waiting for the 2-year follow-up period, clinicians estimate the probabilities of treatment within the stratum of those helped by surgery, and compute that_

\[P(d y_{d_{0}}=0,y_{d_{1}}=1,x_{0})=,\] (12)

\[P(d y_{d_{0}}=0,y_{d_{1}}=1,x_{1})=,\] (13)

_indicating that the allocation of the decision is not independent of sex. That is, within the group of those who are helped by surgery, males are more likely to be selected for treatment than females. \(\)_

The decision-making policy introduced by the clinicians, somewhat counter-intuitively, does not allocate the treatment equally within the principal stratum of those who are helped, even though at each level of illness severity, the proportion of patients who benefit from the treatment is equal between the sexes. What is the issue at hand here?

To answer this question, the perspective under perfect knowledge is shown in Fig. 2(a). In particular, on the horizontal axis the noise variable \(u_{y}\) is available, which summarizes the patients' unobserved resilience. Together with the value of illness severity (on the vertical axis) and the knowledge of the structural causal model, we can perfectly pick apart the different groups (i.e., principal strata) according to their potential outcomes (groups are indicated by color). In this case, it is clear that our policy should treat patients in the green area since those are the ones who benefit from treatment.

In Fig. 2(b), however, we see the perspective of the decision-makers under imperfect knowledge. Firstly, the decision-makers have no knowledge about the values on the horizontal axis, since this represents variables that are outside their model. From their point of view, the key quantity of interest is the conditional average treatment effect (CATE), which we call _benefit_ in this context3, defined as

\[(x,z,w)=P(y_{d_{1}} x,z,w)-P(y_{d_{0}} x,z,w).\] (14)

Importantly, in our setting, the benefit \(\) can be uniquely computed (identified) from observational data, implying that we may use the notion of benefit for practical data analyses:

**Proposition 1** (Benefit Identifiability).: _Let \(\) be an SCM compatible with the SFM in Fig. 1. Then the benefit \((x,z,w)\) is identifiable from observational data and equals:_

\[(x,z,w)=P(y d_{1},x,z,w)-P(y d_{0},x,z,w)\] (15)

_for any choice of \(x,z,w\)._

Figure 3: Difference in perspective between perfect knowledge of an oracle (left) and imperfect knowledge of a decision-maker in a practical application (right).

The benefit is simply the increase in survival associated with treatment. After computing the benefit (shown in Eq. 11), the decision-makers visualize the male and female groups (lighter color indicates a larger increase in survival associated with surgery) according to it. It is visible from the figure that the estimated benefit from surgery is higher for the \(x_{0}\) group than for \(x_{1}\). Therefore, to the best of their knowledge, the decision-makers decide to treat more patients from the \(x_{0}\) group.

The example illustrates why the oracle's perspective may be misleading for the decision-makers. The clinicians can never determine exactly which patients belong to the group

\[Y_{d_{0}}(u)=0,Y_{d_{1}}(u)=1,\]

that is, who benefits from the treatment. Instead, they have to rely on illness severity (\(W\)) as a proxy for treatment benefit (\(\)). In other words, our understanding of treatment benefit will always be probabilistic, and we need to account for this when considering the decision-maker's task. A further discussion on the relation to principal fairness  is given in Appendix A.

### Benefit Fairness

To remedy the above issue, we propose an alternative definition, which takes the viewpoint of the decision-maker:

**Definition 2** (Benefit Fairness).: _Let \(\) denote the benefit of treatment in Eq. 14 and let \(\) be a fixed value of \(\). We say that the pair \((Y,D)\) satisfies benefit fairness (BF, for short) if_

\[P(d=,x_{0})=P(d=,x_{1})\;\;.\] (16)

The above definition pertains to the population-level setting, but without oracle knowledge. In this setting, the objective function in Eq. 1 is expected to be lower than in the oracle case. Further, in finite samples, the constraint from Def. 2 is expected to hold approximately. Notice that BF takes the perspective of the decision-maker who only has access to the unit's attributes \((x,z,w)\), as opposed to the exogenous \(U=u\)4. In particular, the benefit \((x,z,w)\) is _estimable_ from the data, i.e., attainable by the decision-maker. BF then requires that at each level of the benefit, \((x,z,w)=\), the rate of the decision does not depend on the protected attribute. The benefit \(\) is closely related to the canonical types discussed earlier, namely

\[(x,z,w)=c(x,z,w)-d(x,z,w).\] (21)

The values of \(c,d\) in Eq. 21 are covariate-specific, and indicate the proportions of patients helped and harmed by the treatment, respectively, among all patients coinciding with covariate values \(x,z,w\) (i.e.,all \(u U\) s.t. \((X,Z,W)(u)=(x,z,w)\)). The proof of the claim in Eq. 21 is given in Appendix B. Using this connection of canonical types and the notion of benefit, we can formulate a solution to the problem in Def. 1, given in Alg. 1. In particular, Alg. 1 takes as input the observational distribution \(P(V)\), but its adaptation to inference from finite samples follows easily. In Step 2, we check whether we are operating under resource scarcity, and if not, the optimal policy simply treats everyone who stands to benefit from the treatment. Otherwise, we find the \(_{b}>0\) which uses the entire budget (Step 3) and separate the interior \(\) of those with the highest benefit (all of whom are treated), and the boundary \(\) (those who are to be randomized) in Step 4. The budget remaining to be spent on the boundary is \(b-P()\), and thus individuals on the boundary are treated with probability \()}{P()}\). Importantly, male and female groups are treated separately in this random selection process (Eq. 20), reflecting BF. The BF criterion can be seen as a minimal fairness requirement in decision-making, and is often aligned with maximizing utility, as seen from the following theorem:

**Theorem 2** (Alg. 1 Optimality).: _Among all feasible policies \(D\) for the optimization problem in Eqs. 1-2, the result of Alg. 1 is optimal and satisfies benefit fairness._

A key extension we discuss next relates to the cases in which the benefit itself may be deemed as discriminatory towards a protected group.

## 3 Fairness of the Benefit

As discussed above, benefit fairness guarantees that at each fixed level of the benefit \(=\), the protected attribute plays no role in the treatment assignment. However, benefit fairness does not guarantee that treatment probability is equal between groups, i.e., that \(P(d x_{1})=P(d x_{0})\). In fact, benefit fairness implies equal treatment allocation in cases where the distribution of the benefit \(\) is equal across groups (shown formally in Appendix F), which may not always be the case:

**Example** (Cancer Surgery - continued).: _After applying benefit fairness and implementing the optimal policy \(D^{*}=W>\), the clinicians compute that \(P(d x_{1})-P(d x_{0})=-50\%,\) that is, females are 50% less likely to be treated than males. _

In our example benefit fairness results in a disparity in resource allocation. Whenever this is the case, it implies that the benefit \(\) differs between groups. In Alg. 2 we describe a formal procedure that helps the decision-maker to obtain a causal understanding of why that is, i.e., which underlying causal mechanisms (direct, indirect, spurious) lead to the difference in the benefit. In Appendix G we discuss in detail how the terms appearing in the decompositions in Eqs. 22 and 23 are defined. The key subtlety here is how to define counterfactuals with respect to the random variable \(\). In the appendix we define this formally and show that the variable \(\) can be considered as an auxillary variable in the structural causal model. Furthermore, we show that the notions of direct, indirect, and spurious effects are identifiable under the SFM in Fig. 1 and provide the identification expressions for them, allowing the data analyst to compute the decompositions in Alg. 2 in practice (see Appendix G). We next ground the idea behind Alg. 2 in our example:

**Example** (Decomposing the disparity).: _Following Alg. 2, the clinicians first decompose the observed disparities into their direct, indirect, and spurious components:_

\[P(d x_{1})-P(d x_{0}) =_{}+_{}+ _{},\] (24) \[( x_{1})-( x_{0}) =_{}+}_{}+ _{},\] (25)

_showing that the difference between groups is entirely explained by the levels of illness severity, that is, male patients are on average more severely ill than female patients (see Fig. 3(a)). Direct and spurious effects, in this example, do not explain the difference in benefit between the groups._

_Based on these findings, the clinicians realize that the main driver of the disparity in the benefit \(\) is the indirect effect. Thus, they decide to compute the distribution of the benefit \(P(_{x_{1},W_{x_{0}}} x_{1})\), which corresponds to the distribution of the benefit had \(X\) been equal to \(x_{0}\) along the indirect effect. The comparison of this distribution, with the distribution \(P( x_{0})\) is shown in Fig. 3(b), indicating that the two distributions are in fact equal. _

In the above example, the difference between groups is driven by the indirect effect, although generally, the situation may be more complex, with a combination of effects driving the disparity. Still, the tools of Alg. 2 equip the reader for analyzing such more complex cases. The key takeaway here is that the first step in analyzing a disparity in treatment allocation is to obtain a _causal understanding_ of why the benefit differs between groups. Based on this understanding, the decision-maker may decide that the benefit \(\) is unfair, which is what we discuss next.

### Controlling the Gap

A causal approach.The first approach for controlling the gap in resource allocation takes a counterfactual perspective. We first define what it means for the benefit \(\) to be causally fair:

**Definition 3** (Causal Benefit Fairness).: _Suppose \(=(C_{0},C_{1})\) describes a causal pathway from \(X\) to \(Y\) which is deemed unfair, with \(C_{0},C_{1}\) representing possibly counterfactual interventions. The pair \((Y,D)\) satisfies counterfactual benefit fairness (CBF) if_

\[(y_{C_{1},d_{1}}-y_{C_{1},d_{0}} x,z,w) =(y_{C_{0},d_{1}}-y_{C_{0},d_{0}} x,z,w)\  x,z,w\] (26) \[P(d,x_{0}) =P(d,x_{1}).\] (27)

To account for discrimination along a specific causal pathway (after using Alg. 2), the decision-maker needs to compute an adjusted version of the benefit \(\), such that the protected attribute has no effect along the intended causal pathway \(\). For instance, \(=(\{x_{0}\},\{x_{1}\})\) describes the total causal effect, whereas \(=(\{x_{0}\},\{x_{1},W_{x_{0}}\})\) describes the direct effect. In words, CBF requires that treatment benefit \(\) should not depend on the effect of \(X\) on \(Y\) along the causal pathway \(\), and this condition is covariate-specific, i.e., holds for any choice of covariates \(x,z,w\). Additionally, the decision policy \(D\) should satisfy BF, meaning that at each degree of benefit \(=\), the protected attribute plays no

Figure 4: Elements of analytical tools from Alg. 2 in Ex. 3.

role in deciding whether the individual is treated or not. This statement is for fixed value of \(=\), and possibly considers individuals with different values of \(x,z,w\). CBF can be satisfied using Alg. 3. In Step 1, the factual benefit values \(\), together with the adjusted, counterfactual benefit values \(_{C}\) (that satisfy Def. 3) are computed. Then, \(_{CF}\) is chosen to match the budget \(b\), and all patients with a counterfactual benefit above \(_{CF}\) are treated5, as demonstrated in the following example:

**Example** (Cancer Surgery - Counterfactual Approach).: _The clinicians realize that the difference in illness severity comes from the fact that female patients are subject to regular screening tests, and are therefore diagnosed earlier. The clinicians want to compute the adjusted benefit, by computing the counterfactual values of the benefit \(_{x_{1},W_{x_{0}}}(u)\) for all \(u\) such that \(X(u)=x_{1}\). For the computation, they assume that the relative order of the illness severity for females in the counterfactual world would have stayed the same (which holds true in the underlying SCM). Therefore, they compute that_

\[_{x_{1},W_{x_{0}}}(u)=},\] (28)

_for each unit \(u\) with \(X(u)=x_{1}\). After applying Alg. 1 with the counterfactual benefit values \(_{C}\), the resulting policy \(D^{CF}=(_{C}>)\) has a resource allocation disparity of \(0\). _

The above example illustrates the core of the causal counterfactual approach to discrimination removal. BF was not appropriate in itself, since the clinicians are aware that the benefit of the treatment depends on sex in a way they deemed unfair. Therefore, to solve the problem, they first remove the undesired effect from the benefit \(\), by computing the counterfactual benefit \(_{C}\). After this, they apply Alg. 3 with the counterfactual method (CF) to construct a fair decision policy. Notably, the causal approach to reducing the disparity relies on the counterfactual values of the benefit \(_{C}(u)\), as opposed to the factual benefit values \((u)\). As it turns out, measuring (or removing) the covariate-specific direct effect of \(X\) on the benefit \(\) is relatively simple in general, while the indirect effect may be more challenging to handle:

**Remark 3** (Covariate-Specific Direct Effect of \(X\) on Benefit \(\) is Computable).: _Under the assumptions of the SFM, the potential outcome \(_{x_{1},W_{x_{0}}}(u)\) is identifiable for any unit \(u\) with \(X(u)=x_{0}\) for which the attributes \(Z(u)=z,W(u)=w\) are observed. However, the same is not true for the indirect effect. While the counterfactual distribution of the benefit when the indirect effect is manipulated, written \(P(_{x_{0},W_{x_{1}}} x_{0})\), is identifiable under the SFM, the covariate-level values \(_{x_{0},W_{x_{1}}}(x_{0},z,w)\) are not identifiable without further assumptions._

A utilitarian/factual approach.An alternative, utilitarian (or factual) approach to reduce the disparity in resource allocation uses the factual benefit \((u)\), instead of the counterfactual benefit \(_{C}(u)\) used in the causal approach. This approach is also described in Alg. 3, with the utilitarian (UT) method. Firstly, in Step 5, the counterfactual values \(_{C}\) are used to compute the disparity that would arise from the optimal policy in the hypothetical, counterfactual world:

\[M:=|P(_{C}_{CF} x_{1})-P(_{C}_{CF} x_ {0})|.\] (29)

The idea then is to introduce different thresholds \(^{(x_{0})},^{(x_{1})}\) for \(x_{0}\) and \(x_{1}\) groups, such that they introduce a disparity of at most \(M\). In Step 6 we check whether the optimal policy introduces a disparity bounded by \(M\). If the disparity is larger than \(M\) by an \(\), in Step 7 we determine how much slack the disadvantaged group requires, by finding thresholds \(^{(x_{0})},^{(x_{1})}\) that either treat everyone in the disadvantaged group, or achieve a disparity bounded by \(M\). The counterfactual (CF) approach focused on the counterfactual benefit values \(_{C}\) and used a single threshold. The utilitarian (UT) approach focuses on the factual benefit values \(\), but uses different thresholds within groups. However, the utilitarian approach uses the counterfactual values \(_{C}\) to determine the maximum allowed disparity. Alternatively, this disparity can be pre-specified, as shown in the following example:

**Example** (Cancer Surgery - Utilitarian Approach).: _Due to regulatory purposes, clinicians decide that \(M=20\%\) is the maximum allowed disparity that can be introduced by the new policy \(D\). Using Alg. 3, they construct \(D^{UT}\) and find that for \(^{(x_{0})}=0.21,^{(x_{1})}=0.12\),_

\[P(>^{(x_{0})} x_{0}) 60\%,P(>^{(x_{1})}  x_{1}) 40\%,\] (33)

_which yields \(P(d^{UT}) 50\%\), and \(P(d^{UT} x_{1})-P(d^{UT} x_{0}) 20\%\), which is in line with the hospital resources and the maximum disparity allowed by the regulators. _Finally, we describe the theoretical guarantees for the methods in Alg. 3 (proof given in Appendix C):

**Theorem 4** (Alg. 3 Guarantees).: _The policy \(D^{CF}\) is optimal among all policies with a budget \( b\) that in the counterfactual world described by intervention \(C\). The policy \(D^{UT}\) is optimal among all policies with a budget \( b\) that either introduce a bounded disparity in resource allocation \(|P(d x_{1})-P(d x_{0})|\) or treat everyone with a positive benefit in the disadvantaged group._

We remark that policies \(D^{CF}\) and \(D^{UT}\) do not necessarily treat the same individuals in general. In Appendix D, we discuss a formal condition called _counterfactual crossing_ that ensures that \(D^{CF}\) and \(D^{UT}\) treat the same individuals, therefore explaining when the causal and utilitarian approaches are equivalent . In Appendix E we provide an additional application of our outcome control framework to the problem of allocating respirators  in intensive care units (ICUs), using the MIMIC-IV dataset .

## 4 Conclusion

In this paper we developed causal tools for understanding fairness in the task of outcome control. We introduced the notion of benefit fairness (Def. 2), and developed a procedure for achieving it (Alg. 1). Further, we develop a procedure for determining which causal mechanisms (direct, indirect, spurious) explain the difference in benefit between groups (Alg. 2). Finally, we developed two approaches that allow the removal of discrimination from the decision process along undesired causal pathways (Alg. 3). The proposed framework was demonstrated through a hypothetical cancer surgery example (see vignette) and a real-world respirator allocation example (see Appendix E). We leave for future work the extensions of the methods to the setting of continuous decisions \(D\), and the setting of performing decision-making under uncertainty or imperfect causal knowledge.