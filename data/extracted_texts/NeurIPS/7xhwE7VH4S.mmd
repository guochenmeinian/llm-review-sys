# QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos

Sharath Girish

University of Maryland

sgirish@cs.umd.edu

&Tianye Li

NIODIA

tianyel@nvidia.com

&Amrita Mazumdar

NIODIA

amritan@nvidia.com

&Abhinav Shrivastava

University of Maryland

abhinav@cs.umd.edu

&David Luebke

NVIDIA

dluebke@nvidia.com

&Shalini De Mello

NVIDIA

shalinig@nvidia.com

SG's work was done during an internship at NVIDIA. \({}^{}\)TL and AM contributed equally to this project. Project website: https://research.nvidia.com/labs/amri/projects/queen.

###### Abstract

Online free-viewpoint video (FVV) streaming is a challenging problem, which is relatively under-explored. It requires incremental on-the-fly updates to a volumetric representation, fast training and rendering to satisfy real-time constraints and a small memory footprint for efficient transmission. If achieved, it can enhance user experience by enabling novel applications, _e.g._, 3D video conferencing and live volumetric video broadcast, among others. In this work, we propose a novel framework for QUantized and Efficient ENcoding (QUEEN) for streaming FVV using 3D Gaussian Splitting (3D-GS). QUEEN directly learns Gaussian attribute residuals between consecutive frames at each time-step without imposing any structural constraints on them, allowing for high quality reconstruction and generalizability. To efficiently store the residuals, we further propose a quantization-sparsity framework, which contains a learned latent-decoder for effectively quantizing attribute residuals other than Gaussian positions and a learned gating module to sparsify position residuals. We propose to use the Gaussian viewspace gradient difference vector as a signal to separate the static and dynamic content of the scene. It acts as a guide for effective sparsity learning and speeds up training. On diverse FVV benchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all metrics. Notably, for several highly dynamic scenes, it reduces the model size to just \(0.7\) MB per frame while training in under \(5\) sec and rendering at \(\)\(350\) FPS.

## 1 Introduction

The dynamic world that we perceive around us is not 2D, but rather 3D. Unlike 2D videos, which are ubiquitous, the question of how to effectively capture, encode and disseminate free-viewpoint videos (FVV) of dynamic 3D scenes, which can be viewed at any instance of time and from any viewpoint, has intrigued computer vision and graphics researchers for much time. Free-viewpoint video transmission, if achieved, has the potential to transform and enrich user experience in profound ways by offering novel immersive experiences, _e.g._, FVV video playback and live streaming, 3D video conferencing and telepresence, gaming, virtual spatial tutoring and teleoperation, among others.

The underlying problem of reconstructing FVV involves learning a 6D plenoptic function of a dynamic scene \(P(,,t)\) from sparse multiple views acquired over a window of time, with \(^{3}\) being a position in 3D space, \(=(,)\) a viewing direction and \(t\) an instance of time. Neural volumetric representations, which learn a 5D plenoptic function of a scene \(P(,)\) at a fixed time instance, _e.g._,neural radiance fields (NeRFs)  and its variants [2; 7; 18; 55] present a compact and high-fidelity representation for 3D scenes. NeRFs have also been extended to dynamic 4D scenes [1; 5; 19] providing a powerful tool for reconstructing FVV. However, NeRFs require compositing dense information across a 3D volume and hence are slow to train and render. Recently, 3D Gaussian Splatting (3D-GS)  has emerged as a promising technique with significantly faster training and rendering speeds in comparison with NeRFs, and they have also been extended to dynamic 4D scenes [42; 83; 88]. While these representations accurately model 4D scenes, they are trained in an _offline_ fashion requiring full multi-view video sequences to learn temporal relationships between frames. They also require long training times to achieve high reconstruction quality and are mostly not streamable.

_Online_ FVV, _e.g._, for broadcast and teleconferencing applications, presents additional challenges versus offline. It requires incremental _on-the-fly_ updates to volumetric representation at each time-step of the dynamic scene, _fast_ training and rendering times to maintain real-time operation, and _small_ packet sizes per frame to enable effective transmission on bandwidth-limited channels. Consequently, the more challenging problem of online FVV reconstruction remains relatively under-explored. Notable prior solutions are those based on NeRFs using voxel grids [37; 78] or triplanes  to learn 3D representations that are updated on-the-fly. Unsurprisingly, they suffer from slow rendering speeds. Recently, Sun et al. proposed 3DGStream , which uses 3D-GS to model a 3D scene along with InstantNGP  to model its geometric transformation over time. It achieves high rendering speeds but imposes heuristic structural constraints on the volumetric representation to achieve efficiency, which compromises model expressiveness and quality.

In this work, we propose a novel QUantized and Efficient ENcoding (QUEEN) framework, which uses 3D-GS for online FVV. Similarly to prior approaches , we also learn Gaussian attribute residuals between consecutive time-steps. To reduce memory requirements, however,  learns only a subset of the Gaussian attributes at each time-step, limiting model expressiveness. Our first insight, therefore, is to model residuals for _all_ attributes instead, which does not compromise quality. However, encoding all Guassian attributes increases the per frame memory requirement and hence necessitates a means to compress them more effectively. Our second insight, then, is to learn to directly compress the Gaussian residuals in proportion to the real-time scene dynamics, _e.g._, motion and illumination changes. This contrasts with existing methods [37; 69; 78; 84] that employ a single fixed-sized structure, _e.g._, a voxel-grid, a triplane, or hash encoding at all time-steps, and the result is higher efficiency in terms of model size, training speeds, and rendering speeds. Lastly, we also exploit temporal redundancies across time-steps to limit computations to the highly dynamic parts of the scene only and achieve further efficiencies.

Specifically, to achieve this, we propose a learned quantization-sparsity framework to simultaneously learn and compress Gaussian attribute residuals for each time-step. We quantize all attribute residuals, except Gaussian positions, via an end-to-end trainable integer-based latent-decoder. Once learned, we efficiently encode the integer latents via entropy coding to achieve high compression factors. For position residuals that exhibit greater sensitivity to quantization, we propose a learned gating mechanism to sparsify them, which identifies the static (corresponding to \(0\) value) and dynamic Gaussians and retains the sparse dynamic ones only at full precision. Finally, to achieve further efficiencies in terms of training time and storage, we utilize the differences between the 2D viewspace Gaussian gradients of consecutive frames to initialize our learnable gates, and to selectively render local image regions corresponding to highly dynamic scene content.

We evaluate our approach, QUEEN, on two benchmark datasets, containing diverse scenes with large geometric motion and illumination changes. QUEEN outperforms all prior state-of-the-art approaches for online FVV and significantly reduces the per-frame memory cost (\(\)\(10\)), all while achieving higher reconstruction quality, as well as faster training and rendering speeds. Extensive ablations show the efficacy of the various components of our approach.

To summarize, our key contributions are:

* We propose a Gaussian residual-based framework to model 3D dynamic scenes for online FVV without any structural constraints, which allows free learning of all 3D-GS attribute residuals, resulting in higher model expressiveness.
* We introduce a learned quantization-sparsity framework for compressing per-frame residuals, and we initialize and train it efficiently using viewspace gradient differences that separate the dynamic and static scene content.

* On various challenging real-world dynamic scenes, we surpass existing state-of-the-art approaches on all metrics: reconstruction quality, memory utilization, as well as training and rendering speed.

## 2 Related Work

### Traditional Free-viewpoint Video

Ever since early FVV work such as , a series of geometry-based FVV methods [12; 57] has been pushing for high reconstruction quality and streamable performance. However, their rendering and compression quality rely on the accuracy of a sophisticated pipeline of geometry reconstruction [20; 28], tracking , and texturing . They also require high-end hardware for capturing complex and dynamic appearance [6; 14; 26]. Purely image-based rendering [10; 13; 35; 53] relaxes the requirement for geometric accuracy. Although methods such as [4; 95] support view interpolation with layered representations in the dynamic setting, they require a high count of views as input to ensure interpolation quality.

### Neural and Gaussian-based Free-viewpoint Video

**Offline Methods.** Compared to the traditional representations, the emergence of neural representations [47; 54; 86; 67; 72] opened a new door for capturing FVV for dynamic humans [21; 31; 32; 39; 63; 82; 94] and monocular videos [22; 43; 73; 75; 85]. In this work, we focus on general dynamic scenes  from multiple views to push the quality of streamable FVV _without_ requiring a strong human prior [40; 48] or a very constrained input. [16; 62; 74; 80] model the scene dynamics via explicit deformation. Although suitable for motion analysis, they inevitably face a trade-off between motion accuracy and visual quality . To tackle this, [41; 52; 59] use a spatial-temporal formulation via time-conditioned latent codes to implicitly encode the 4D scene, enabling reconstruction of topological changes and volumetric effects. [5; 19; 66] factorize the 4D scene into multiple space-time feature planes and achieves higher model compactness and training efficiency. [68; 76] decompose the 4D scene into static and dynamic volumes. [1; 46; 77; 87; 93] incorporate efficient NeRF representations [9; 19; 91] for higher fidelity. Although, these NeRF-based method achieve high compactness, they suffer from low rendering efficiency, even when converted  to a more efficient NeRF formulation [9; 55]. Seeing their great potential for efficiency, recent works extend 3D Gaussian representations  to dynamic scenes, with temporal attributes [42; 89], generalized 4D Gaussians  and a hybrid representation . While these methods achieve high quality in modeling 3D dynamic scenes, they, together with the aforementioned NeRF-based methods, are mostly _offline_, i.e., they require all the input video and a long time for training, which is inherently difficult for streaming applications.

**Online Methods.**_Online_ reconstruction for FVVs is relatively under-explored, as it imposes additional challenges of on-the-fly reconstruction using only local temporal information instead of the full recordings. Furthermore, toward the goal of streamable FVVs, the encoding system is evaluated by multiple metrics including compression rate, encoding and rendering speed and visual quality.  tracks dense 3D Gaussians by solving their motion over time. Visual quality and dynamic appearance is not their focus.  models motion by rendering scene dynamics, however their method is not optimized for efficiency.  focuses on generalizable NeRF reconstruction and shows good promise to adapt to a new frame but has a high memory footprint due to an MVSNet-style neural network [8; 90].  accelerates training and rendering speed with a special tuning strategy and sparse voxels, however, their representation still has high temporal redundancy.  proposes an incremental training scheme with natural information partitioning and achieves high compression, but its encoding is slow. Several works [78; 79; 84] use video codec-inspired encoding paradigms for data efficiency.  achieves a decent compression rate and near interactive rendering with compact motion and residual grids. However, their training requires \(~{}10\) minutes per frame.  focuses on real-time decoding, streaming and rendering instead of on-the-fly encoding.  performs grouped training on a hybrid representation of triplanes and volume grid. While achieving high compression rate, their fixed encoding paradigm and aggressive quantization limits their reconstruction quality along-with low rendering speeds.  is the closest work to ours for streaming FVV via 3D-GS. They encode the position and rotation residuals via an Instant-NGP  based transformation cache. While achieving faster training and rendering speeds than prior work, they have high data redundancy due to a fixed structured modeling. Additionally, they focus on geometric transformations only and can approximate only small changes in new scene content or lighting variations. Our work updates and compresses all 3D-GS attributes freely without any structural constraints while still obtaining much better memory costs via our quantization-sparsity framework along with better training times.

### 3D Scene Representation Compression

Several works propose a variety of compression methodologies for reducing the memory, training time, or rendering speed of standard static scene 3D representations. [9; 71] decompose NeRFs via low-rank approximations. [15; 38] prune voxels along with vector quantization by . [25; 70] compress multi-resolution feature-grids via codebook/vector quantization. A large number of approaches target 3D-GS compression [17; 24; 58; 34] and acceleration via pruning. While these approaches can be applied to static representations on a per-frame basis, their trivial frame-wise application would result in extremely high training costs as well as large memory per frame. To enable streaming, our work, instead, explicitly focuses on effectively leveraging the temporal redundancies across frames by compressing the residual information between them to achieve greater efficiency.

## 3 QUEEN: Quantized Efficient Encoding for Streaming FVV

A solution for streamable FVV must have low-latency encoding (training) and decoding (rendering), and low data bandwidth (memory) for transmission on a common network infrastructure. Motivated by these constraints, we aim to generate streamable FVVs with compact representations that are fast to train and render incrementally. In this section, we first provide an overview of 3D-GS (Sec. 3.1). In Sec. 3.2, we propose a compression framework to efficiently represent and train Gaussian attribute residuals at each time step. Sec. 3.3 discusses utilizing an approach based on viewspace gradient differences to achieve greater efficiencies. An overview of our method is shown in Fig. 1.

### Preliminary: 3D Gaussian Splatting

Our efficient representation for dynamic scenes is based on 3D Gaussian Splatting (3DGS) . Given multi-view images \(\), a 3D scene is modeled by a set of Gaussians with attributes \(\).

**Representation.** The shape of each Gaussian \(i\) is defined by its mean \(_{i}^{3}\) and covariance matrix \(_{i}\). The covariance matrix is represented by \(_{i}=_{i}_{i}_{i}^{T}_ {i}^{T}\), where \(_{i}\) is a rotation matrix parameterized by a quaternion vector \(_{i}^{4}\), and the scale matrix \(_{i}\) is a diagonal matrix with elements \(_{i}^{3}\). Each Gaussian also contains opacity \(o_{i}\) and spherical harmonic coefficients \(_{i}\) for view-dependent appearance with dimensions based on the number of degrees.

**Rendering.** For rasterization, 3D Gaussians are projected into 2D Gaussians for any given view. Given a camera with intrinsic matrix \(\) and viewing transform \(\), the 2D mean and covariance are:

\[_{i}^{}=(_{i};,), _{i}^{}=_{i}^{T}^{T},\] (1)

where \(()\) denotes the perspective projection and \(\) is the Jacobian of the affine approximation of the projective transform . The image color \(}\) at pixel location \(\) is obtained by blending \(N\) depth-sorted Gaussians with their view-dependent RGB color value \(_{i}\) computed from \(_{i}\):

\[}()=_{i=1}^{N}_{i}_{i}_{j=1 }^{i-1}(1-_{j}),_{i}=o_{i}(-( -_{i}^{})^{T}_{i}^{-1}( -_{i}^{})),\] (2)

Figure 1: **Overview of QUEEN for online FVV. We incrementally update Gaussian attributes at each time-step (gray block) by simultaneously learning and compressing residuals between consecutive time-steps via a quantization (orange block) and sparsity (yellow block) framework. We additionally render only the dynamic Gaussians for masked regions to achieve faster convergence (green block).**

where \(_{i}\) is the conic opacity of Gaussian \(i\) at pixel location \(\) multiplied by the Gaussian opacity \(o_{i}\).

**Training.** With a differentiable rasterizer , the attributes \(=\{_{i},_{i},_{i},o_{i},_{i} \}_{i=1}^{N}\) are optimized to produce renderings \(}=R()\) that fit to input images \(\) by optimizing a reconstruction loss which combines the D-SSIM loss  and \(L_{1}\) loss with a hyperparameter \(\):

\[L= L_{}+(1-)L_{1}.\] (3)

### Attribute Residual Compression

Given a multi-view image sequence \(\{_{t}\}_{t=0}^{T-1}\), our goal is to reconstruct the dynamic scene via Gaussian attributes \(_{t}\) for each time-step \(t\). We model the attributes based on the trained attributes from the previous time-step \(t-1\) as

\[_{t}=_{t-1}+_{t},\] (4)

where \(_{t}\) consists of learnable residuals for each attribute (in Fig. 1 (gray block)). For time-step \(t=0\), we perform vanilla Gaussian splatting training to obtain attributes \(_{0}\). This sequential formulation allows us to freely and adaptively update the residuals \(_{t}\) on-the-fly with incoming streaming training views, without any structural constraints as in prior works . However, representing the 4D scene with uncompressed residuals is still highly inefficient. As residuals have low magnitudes in comparison with the attributes themselves, they can be efficiently compressed, for which we propose a novel quantization-sparsity framework.

#### 3.2.1 Attribute Residual Quantization

There exists spatial redundancy within the Gaussian attributes of the same time-step. Nearby Gaussians have highly correlated residuals for shape, orientation and appearance. To reduce the storage cost of the residuals, we propose to utilize a quantization framework during training .

At each time-step \(t\), we represent the residuals via quantized latents and a shared compact decoder. Specifically, to obtain the residuals for each category2\(_{i}^{M}\), we maintain corresponding quantized integer latents \(_{i}^{L}\) for each Gaussian \(i\). These latents are passed through a shared linear decoder \(D\) with learnable parameters \(^{M L}\) to obtain the decoded attribute residual \(_{i}\). Such a compact decoder has small time and memory costs due to few parameters and arithmetic operations. To allow differentiable training of the integer latents via gradient optimization, we use a continuous approximation \(}_{i}^{L}\) instead. \(}_{i}\) are rounded to the nearest integer values for the forward pass but can still receive backpropagated gradients via the Straight-Through Estimator (STE) :

\[_{i}=(}_{i}),_{i}=D(_{i};)=(_{i}).\] (5)

The continuous latents \(}=\{}_{i}\}_{i=1}^{N}\), and the shared decoder's parameters \(\) are learnable during training. After adding the decoded residuals to the previous time-step's attributes (Eq. 4), the standard rasterization process (Eq. 2) is used to obtain the rendered image. This differentiable quantization module is trained end-to-end with the main training process by optimizing the reconstruction loss. Post-training, we entropy code the quantized latents \(\) and directly store the decoder \(\). Entropy coding results in as much as \(10\) reduction in model size from \(44\) to \(4\) MB without quality degradation.

#### 3.2.2 Position Residual Gating

**Sparse Representation.** While most of the attribute residuals can be quantized effectively with our proposed method in Sec 3.2.1, we observe that the position residuals are sensitive to quantization and require high precision during rendering3. Storing all the full-precision position residuals, however, still results in high per-frame memory costs. To tackle this, we propose a learned gating methodology, which enforces sparsity in the residuals instead of quantization. This mechanism allows us to set a vast majority of the position residuals to zeros, while maintaining full-precision non-zero values. Specifically, we represent the positional residual for each Gaussian \(i\) as \(_{i}=g_{i}_{p_{i}}\), where the scalar \(g_{i}\) is the learnable gate variable and \(_{p_{i}}^{3}\) is the learnable pre-gated residual in full precision during training. After training, the sparse \(_{i}\) can be efficiently stored via sparse matrix formats  to reduce memory costs. Thus, our goal is to encourage the sparsity for the variable \(g_{i}\) across all Gaussians. This goal also aligns with the observation that a large portion of a dynamic scene is static or nearly static, which can be leveraged to attain high compression performance.

**Hard Concrete Gate.** Sparsity can be induced via \(L_{0}\) or \(L_{1}\) norm regularization penalties. However, \(L_{1}\) norm induces shrinkage, _i.e_., lowers the magnitude of even non-zero values. \(L_{0}\) norm is the ideal sparsity loss without shrinkage, but is computationally intractable with non-differentiability and combinatorial complexity. To enforce sparsity, we instead propose to use the hard concrete gate . For each Gaussian \(i\), the concrete gate  is a continuous relaxation of the Bernoulli distribution:

\[_{i}=^{(_{i}/)},\] (6)

where \(_{i}\) is a learnable parameter and \(\) is the temperature parameter. Although the concrete gate approximates the discrete Bernoulli gate, it does not include the end points \(\{0,1\}\), which does not directly result in sparsity. The hard concrete gate "stretches" the range of the concrete gate to the interval \((_{0},_{1})\) and then applies a hard-sigmoid:

\[_{i}=_{i}(_{1}-_{0})+_{0}, g _{i}=(1,(0,_{i})).\] (7)

This includes the end points \(\{0,1\}\) for \(g_{i}\), required for achieving sparse residuals.

**Sparsity Loss.** The hard concrete gate formulation leads to a convenient regularization loss for encouraging \(L_{0}\) sparsity  in the gates \(g_{i}\):

\[L_{}=_{i=1}^{N}p_{i}=_{i=1}^{N}( _{i}-}{_{1}}).\] (8)

Here, we treat \(=\{_{i}\}_{i=1}^{N}\) to be learnable parameters for all \(N\) Gaussians at a given time-step and \(\{,_{0},_{1}\}\) as hyperparameters that are shared for all Gaussians and all time-steps.

### Viewspace Gradient Difference for Adaptive Training

Real-world dynamic scenes contain high amounts of temporal redundancy with only a fraction of the content changing between consecutive time-steps. The proposed quantization-sparsity framework can learn to identify Gaussians corresponding to static scene content and set their residuals to \(0\). However, they still forward/backward pass through static regions resulting in wasted training computation. Additionally, initializing the gates with \(1\)s requires more iterations for convergence. We thus propose a proxy metric to identify Gaussians, which are static or dynamic at the start of training. We use this metric to initialize our gates while also identifying dynamic image regions to perform local rendering in, during training.

Viewspace Gradient Difference.The ground-truth (GT) training images contain information of the dynamic scene content, which we leverage to separate static and dynamic Gaussians. A simple pixel difference between consecutive frames does not account for illumination changes and is a noisy signal for the geometric position residuals. 3D-GS utilizes 2D viewspace gradients \(^{}}\) to identify poorly fitted Gaussians based on the reconstruction loss \(L\) between the rendered (\(}\)), GT image (\(\)). More concretely, after training Gaussians at time-step \(t-1\) with an MSE loss, \(L_{t-1}\) and 2D Gaussian means \(}_{t-1}\), we compute the MSE loss for the next time-step \(L_{t}\) and then compute the gradient difference. The score vector \(_{t}\) is the average of gradient differences across all training views \(v\):

\[_{t}=_{v=0}^{V-1}[^{(v)}}{ }}_{t-1}^{(v)}}-^{ (v)}}{}}_{t-1}^{(v)}}], L_{t- 1}^{(v)}=L(_{t-1}^{(v)},}_{t-1}^{(v)}),  L_{t}^{(v)}=L(_{t}^{(v)},}_{t-1}^{(v)} ).\] (9)

Figure 2: **Viewspace Gradient Difference. We use the difference of viewspace gradients between consecutive frames to identify dynamic scene content.**

As shown in Fig. 2, \(_{t}\) identifies the dynamic scene regions while factoring out the noise from imperfect reconstructions at time-step \(t-1\). We use the norm of the score vector \(|}_{t_{i}}|\) to initialize the gate parameters. We define the probability of a gate being active for Gaussian \(i\) at time-step \(t\) as

\[d_{t_{i}}=}_{t_{i}}|}{|}_{t_{i}}|+ {i=1,2,,N}{}(|}_{t_{i}}|)}.\] (10)

We set \(p_{i}\) in Eq. 8 to be \(d_{t_{i}}\) to solve for the initial \(_{i}\). This initialization leads to better convergence by identifying Gaussians corresponding to \(0\) position residuals at the start of training itself.

**Adaptive Masked Training.** In addition to gate initialization, we propose to utilize the score vector \(_{t}\) for an adaptive masked training scheme. We split the Gaussians into static or dynamic parts by applying a threshold \(t_{}\) on the norm of \(_{t}\). We render dynamic Gaussians for each training view to identify corresponding dynamic image regions. We then only render and backpropagate through these pixel locations. We perform this masked training for a fraction of the full training iterations, and find it to improve training speeds with little to no loss of reconstruction quality.

### Efficient End-to-end Learnable Residuals

**Initial Frame Reconstruction.** For an incrementally updating online approach, it is important to make sure the initial frame is well reconstructed. COLMAP used to initialize the positions of Gaussians can result in sparse 3D points for regions with sparse camera views. Hence, we use an off-the-shelf monocular depth estimation network to estimate point locations in these empty regions and predict a more complete initial point cloud. Further details and results are in the supplementary.

**End-to-end Training.** We train separate decoders and quantized latents \(\{}_{c},}_{c}|c\{,,o,\}\}\) for all attributes except position. For position, we learn the gate parameters \(\) and positional residuals \(}_{p}\). All variables are end-to-end differentiable. The total loss function that we minimize is the reconstruction loss (Eq. 3) and the sparsity gate regularization loss (Eq. 8):

\[L_{}=L+_{}L_{},\] (11)

where \(_{}\) controls tradeoffs between memory and reconstruction quality. By simultaneously quantizing while training we achieve high compression while maintaining quality, unlike [84; 78] with post-training compression that lead to quality degradations. We also apply the 3D-GS densification stage at each time-step and is sufficient in modeling new or finer scene content. 3DGSream  adds Gaussians relative to the first time-step only, which limits their approach to small scene changes.

## 4 Experiments

### Datasets and Implementation

We evaluate our method on two challenging FVV video datasets. **(1) Neural 3D Videos (N3DV)** consists of six indoor scenes with forward-facing 20-view videos. **(2) Immersive Videos** consists of seven indoor and outdoor scenes captures with 46 cameras. In both datasets, the central view is held out for testing. We implement QUEEN on . We train for 500 and 350 epochs for the first time-step, and for 10 and 15 epochs for the subsequent time-steps, for N3DV and Immersive, respectively, on an NVIDIA A100 GPU. One epoch contains all training views. We evaluate visual quality in terms of average frame-wise PSNR, SSIM, and LPIPS (VGG) across all videos. We also compute the average storage size and training time for each time-step, and the rendering speed. Additional details are provided in the supplementary materials.

### Quantitative Comparisons

We compare QUEEN against state-of-the-art existing online FVV methods (3DGSream , StreamRF  and TeTriRF ) on N3DV and Immersive (Tab. 1). 3DGSream  is the overall best-performing prior method. Since 3DGSream  was originally run on an older NVIDIA V100 GPU on N3DV, we re-run 3DGSream on an NVIDIA A100 GPU on both N3DV and Immersive and denote it as 3DGSream* in Tab. 1 for consistency with QUEEN. For brevity, in Tab. 1 we additionally compare against only selected top-performing offline FVV methods. We include a more extensive comparison to all existing offline FVV methods in the supplementary (Tab. 8). Lastly, we evaluate three variants of QUEEN: QUEEN-s (small), QUEEN-m (medium) and QUEEN-l (large), with residuals trained for 6, 8 and 10 epochs, respectively.

From Tab. 1, on N3DV, QUEEN-1 results in the best quality among all online FVV methods and achieves a \(10\) reduction in storage size compared to 3DGStream. Although TeTriRF requires less memory than QUEEN, it has much worse quality (\(-1.5\)dB) and rendering speed (\(4\)FPS), and higher training time (\(39\) sec). On Immersive, which contains more pronounced scene changes than N3DV, we limit our comparisons to 3DGStream with longer iterations. TeTriRF requires long convergence times to achieve reasonable reconstruction quality, limiting their training feasibility. QUEEN-1 significantly outperforms 3DGStream, obtaining \(+4\)dB PSNR, \(5\) smaller size, and lower training times. These results on the more challenging scenes from the Immersive datasets reveal the structural constraints brought by the heuristic compression design of 3DGStream. In contrast, our quantization-sparsity framework shows higher flexibility and quality in capturing changing appearances and scene density as well as learning compact and effective representations.

### Qualitative Comparisons

In Fig. 3 we compare the reconstruction results of the various methods. On N3DV, we reconstruct finer details than 3DGStream, _e.g._, the hand and the dog, and minimize artifacts such as the tongs in the top scene or the coffee and metal tumbler in the bottom scene. TeTriRF produces blurry outputs, _e.g._, the cap or metal tumbler in the bottom scene. On Immersive, we better model illumination changes and new scene content such as the person (first patch) and the flame (third patch) in the top scene or the face in the bottom scene (second patch) versus 3DGStream.

    \\   &  & PSNR &  &  & Storage & Training & Rendering \\  & & (dB) \(\) & & & (MB) \(\) & (sec) \(\) & (FPS) \(\) \\   & NeRFlayer  & 30.69 & 0.932 & 0.209 & 17.10 & 72 & 0.05 \\  & HyperReel  & 31.10 & 0.928 & - & 1.20 & 104 & 2.00 \\  & SpaceTime  & **32.05** & **0.948** & - & **0.67** & **20** & **140** \\   & StreamRF  & 30.68 & - & - & 31.4 & 15 & 8.3 \\  & TeTriRF  & 30.43 & 0.906 & 0.248 & **0.06** & 39 & 4 \\  & 3DGStream  & 31.67 & - & - & 7.83 & 12 & 215 \\  & 3DGStream*  & 31.58 & 0.941 & 0.140 & 7.80 & 8.5 & 261 \\   & QUEEN-s (ours) & 31.89 & 0.945 & 0.139 & 0.68 & **4.65** & **345** \\  & QUEEN-m (ours) & 32.03 & 0.946 & 0.137 & 0.69 & 5.96 & 321 \\  & QUEEN-l (ours) & **32.19** & **0.946** & **0.136** & 0.75 & 7.9 & 248 \\    \\   &  & PSNR &  &  & Storage & Training & Rendering \\  & & (dB) \(\) & & & (MB) \(\) & (sec) \(\) & (FPS) \(\) \\   & NeRFlayer  & 25.8 & 0.848 & 0.329 & 17.1 & \(\)72 & 0.12 \\  & HyperReel  & 28.8 & 0.874 & - & 1.2 & \(\)108 & 4 \\  & SpaceTime  & **29.2** & **0.916** & - & **1.2** & \(\)**72** & **99** \\   & 3DGStream*  & 25.18 & 0.876 & 0.255 & 8.83 & 32.4 & **221** \\  & QUEEN-l (ours) & **29.22** & **0.915** & **0.208** & **1.79** & **19.7** & 183 \\   

Table 1: **Quantitative Results. We compare QUEEN against state-of-the-art online and (a few for brevity) offline FVV methods on N3DV  and Immersive . We include many more offline methods in the supplementary (Tab. 8). 3DGStream* refers to our re-implementation on the same NVIDIA A100 GPU used by QUEEN for fairness. Bold and underlined numbers indicate the **best** and the _second_ best results, respectively, within each category.**

    \\   &  & Storage & Training & Rendering & PSNR & Storage & Training & Rendering \\  & & (dB) \(\) & (MB) \(\) & (sec) \(\) & (FPS) \(\) & (dB) \(\) & (MB) \(\) & (sec) \(\) & (FPS) \(\) \\  Baseline & 31.66 & 44.36 & 7.29 & 214 & 28.54 & 78.4 & 20.85 & **276** \\  & 32.04 & 4.18 & **7.28** & **285** & 29.01 & 4.57 & 25.17 & 199 \\  & Position Gating & 32.05 & 0.72 & 6.95 & 274 & 28.99 & **2.01** & 26.99 & 190 \\  & + Gate Initialization & 32.14 & **0.60** & 7.92 & 271 & 29.08 & **1.33** & 27.81 & 177 \\  & **32.19** & 0.74 & 7.88 & 248 & **29.22** & 1.79 & **19.70** & 183 \\   

Table 2: **Effect of Various Components Ablated on N3DV and Immersive Datasets.**

### Ablations

Effect of Updating Appearance Attributes.To ablate the importance of updating the Gaussian appearance attributes (color and opacity) per frame in QUEEN, we run experiments on N3DV for 2 settings: (1) learning only geometric attribute residuals (position, scale and covariance) with appearance residuals set to zero and (2) learning all residuals per frame (Tab. 3). Updating only geometric attributes results in a drop of \(0.4\) dB PSNR versus updating all attributes.Visually, for the Flame Steak scene in N3DV (Fig. 4), updating all attributes results in the highest quality, while fixing opacity introduces artifacts at the edge of the flamethrower. Fixing color, additionally, results in a significant drop in PSNR (-0.6 dB) producing a discolored flame (rightmost column).

Effect of Attribute Compression and Masked Training.We show results for five variants of QUEEN with incrementally added sub-components: (1) a baseline with uncompressed residual training (Sec. 3.2), (2) adding quantization to all attributes except position (Sec. 3.2.1), (3) adding gating of position residuals (Sec. 3.2.2), (4) gate initialization with viewspace gradient differences and (5) masked image training (Sec. 3.3). Results are summarized in Tab. 2 for both N3DV and Immersive datasets. Compressing attributes and gating position residuals results in significant model size reduction on both datasets (\(60,40\)). This is further reduced by gate initialization with

Figure 4: **Effect of Updating Appearance Attributes.** QUEEN updates all Gaussian attributes, resulting in improved quality versus keeping appearance attributes fixed across a video.

Figure 3: **Qualitative Results.** A visualization of various scenes in the N3DV and Immersive datasets. PSNR (\(\)) values are shown. We include additional video results in the supplement.

viewspace gradient differences due to faster convergence of the gates, without loss of quality. By masked training via localized image rendering, we reduce training time by \(8\) seconds for Immersive and marginally for N3DV. Overall, from the baseline, we obtain significant model size reduction with equivalent or lower training and rendering speed. Attribute quantization framework even improves PSNR compared to the baseline for both datasets. This largely stems from quantizing the scaling attribute leading to a more stable optimization with better reconstruction quality while reducing storage size (Tab. 4).

**Effect of Gating.** As shown in Fig. 5, more than half of the gates are set to be inactive at the start of training with viewspace gradient initialization (Sec. 3.3) and a large portion of the image is active. However, post-training, most gates become inactive while the remaining active gates successfully focus on the dynamic scene content, _e.g._, the person's hands or the dog's face. This validates that our gating mechanism effectively separates static and dynamic scene content.

**Effect of Adaptive Image Mask.** We visualize the masks obtained by our viewspace gradient difference module in Sec. 3.3. Results on 2 scenes in the Immersive dataset are shown in Fig. 6. For various time instance of the video (columns), we adaptively identify image regions corresponding to the dynamic scene content. We can therefore perform local image rendering and backpropagation for faster training skipping computation for the static parts of the scene such as the background.

## 5 Conclusion

We proposed QUEEN, a framework to model 3D dynamic scenes for online FVV using 3D-GS. We utilized an attribute residual framework, which freely updates all parameters leading to better modeling of complex scenes. We show that the residuals can be successfully compressed via our learned quantization-sparsity mechanism, which adapts to the dynamic scene content to achieve very small model sizes, improved training and rendering speeds, and improved visual quality. In future work, we aim to extend QUEEN for sparse view reconstruction or sequences with long duration.

   Configuration & PSNR (dB) & Storage (MB) & Train. (sec) \\  w/o Scaling Quant. & 31.69 & 4.39 & 11.01 \\ w/ Scaling Quant. & **32.08** & **0.69** & **7.07** \\   

Table 4: **Effect of Quantizing Scaling Attribute on N3DV.** PSNR improves while also reducing model size and training time due to faster rendering.

Figure 5: **Effect of Gating. While a large number of gates (\(47\%\)) are active at start of training (a, c), they are pruned and only gates corresponding to changing scene content (\(2\%\)) remain active (b, d).**

Figure 6: **Adaptive Image Mask Visualization. We separate out the dynamic scene content at different time-steps of the video through our viewspace gradient difference approach in Sec. 3.3.**

   Update Attributes &  PSNR (dB) \\  &  Storage \\ (MB) \\  & 
 Train. \\ (sec) \\  \\  Geometric & 31.61 & **0.61** & 7.87 \\ + Appearance & **32.03** & 0.74 & **7.57** \\   

Table 3: **Updating Appearance Attributes on N3DV.** PSNR significantly improves by updating all attributes but with a small storage overhead.