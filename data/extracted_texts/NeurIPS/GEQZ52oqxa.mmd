# (L_{2}\)-Uniform Stability of Randomized Learning Algorithms: Sharper Generalization Bounds and Confidence Boosting

(L_{2}\)-Uniform Stability of Randomized Learning Algorithms: Sharper Generalization Bounds and Confidence Boosting

 Xiao-Tong Yuan

School of Intelligence Science and Technology

Nanjing University

Suzhou, 215163, China

xtyuan1980@gmail.com Ping Li

VecML Inc. www.vecml.com

Bellevue, WA 98004, USA

pingli98@gmail.com

###### Abstract

Exponential generalization bounds with near-optimal rates have recently been established for uniformly stable algorithms (Feldman and Vondrak, 2019; Bousquet et al., 2020). We seek to extend these best known high probability bounds from deterministic learning algorithms to the regime of randomized learning. One simple approach for achieving this goal is to define the stability for the expectation over the algorithm's randomness, which may result in sharper parameter but only leads to guarantees regarding the on-average generalization error. Another natural option is to consider the stability conditioned on the algorithm's randomness, which is way more stringent but may lead to generalization with high probability jointly over the randomness of sample and algorithm. The present paper addresses such a tension between these two alternatives and makes progress towards relaxing it inside a classic framework of confidence-boosting. To this end, we first introduce a novel concept of \(L_{2}\)-uniform stability that holds uniformly over data but in second-moment over the algorithm's randomness. Then as a core contribution of this work, we prove a strong exponential bound on the first-moment of generalization error under the notion of \(L_{2}\)-uniform stability. As an interesting consequence of the bound, we show that a bagging-based meta algorithm leads to near-optimal generalization with high probability jointly over the randomness of data and algorithm. We further substantialize these generic results to stochastic gradient descent (SGD) to derive sharper exponential bounds for convex or non-convex optimization with natural time-decaying learning rates, which have not been possible to prove with the existing stability-based generalization guarantees.

## 1 Introduction

In many statistical learning problems, we are interested in designing a randomized algorithm \(A:^{N}\) that maps a training data sample \(S=\{Z_{i}\}_{i[N]}^{N}\) with an algorithm's random parameter \(\) to a model \(A(S,)\). Here \(\) and \(\) are some measurable sets, and \(\) is a closed subset of an Euclidean space. The ultimate goal is to find a suitable algorithm such that the following population risk evaluated at the model should be as small as possible:

\[R(A(S,)):=_{Z}[(A(S,);Z)],\]

where \(Z\) and \(:^{+}\) is a non-negative bounded loss function whose value \((w;z)\) measures the loss evaluated at \(z\) with parameter \(w\). It is generally the case that the underlying data distribution is unknown, and in this case the data points \(Z_{i}\) are usually assumed to be independent.

Then, a natural alternative measurement that mimics the computationally intractable population risk is the empirical risk given by

\[R_{S}(A(S,)):=_{Z(S)}[(A(S,);Z)]= _{i=1}^{N}(A(S,);Z_{i}).\]

The bound on the difference between the population and empirical risks is of central interest in understanding the generalization performance of a learning algorithm. In particular, we hope to derive a suitable law of large numbers, i.e., a sample size vanishing rate \(b_{N}\) such that the generalization bound \(|R_{S}(A(S,))-R(A(S,))| b_{N}\) holds with high probability over the randomness of \(S\) and hopefully the randomness of \(\) as well. Let \(R^{*}:=_{w}R(w)\) be the optimal value of the population risk. Conditioned on \(S\), suppose that \(A(S,)\) is an almost minimizer of the empirical risk \(R_{S}\) such that \(R_{S}(A(S,))-_{w}R_{S}(w)\), then the generalization bound immediately implies an _excess risk_ bound \(R(A(S,))-R^{*} b_{N}+}+\) based on the standard risk decomposition and Hoeffding's inequality. Therefore, generalization guarantees also play a crucial role in understanding the stochastic optimization performance of a learning algorithm.

A powerful proxy for analyzing the generalization bounds is the _stability_ of learning algorithms to changes in the training dataset. Since the seminal work of Bousquet and Elisseeff (2002), stability has been extensively demonstrated to beget dimension-independent generalization bounds for deterministic learning algorithms (Mukherjee et al., 2006; Shalev-Shwartz et al., 2010), as well as for randomized learning algorithms such as bagging and SGD (Elisseeff et al., 2005; Hardt et al., 2016). So far, the best known results about generalization bounds are offered by approaches based on the notion of uniform stability (Feldman and Vondrak, 2018, 2019; Bousquet et al., 2020; Klochkov and Zhivotovskiy, 2021) which is independent to the underlying distribution of data. For randomized algorithms, the definition of uniform stability can be extended in two natural ways by respectively considering 1) the stability averaged over the algorithm's randomness (Hardt et al., 2016) and 2) the stability conditioned on the algorithm's randomness (Feldman and Vondrak, 2019). The former is simpler to show but typically leads to on-average generalization bounds, while the latter is relatively more stringent but may yield deviation bounds given that the conditional stability holds with high probability over the algorithm's randomness. Between these two extreme cases, however, the generalization behavior of randomized learning algorithm still remains largely under explored.

To address the above mentioned theoretical gap between the current lines of results, we explore the opportunities of _deriving exponential generalization bounds for randomized learning algorithms beyond the notions of on-average stability and conditional stability_. A concrete working example of our study is the widely used stochastic gradient descent (SGD) algorithm that carries out the following recursion for all \(t 1\) with learning rate \(_{t}>0\):

\[w_{t}:=_{}(w_{t-1}-_{t}_{w}(w_{t-1};Z_{i_{t}} )),\] (1)

where \(i_{t}[N]\) is a random index of data under with or without replacement sampling, and \(_{}\) is the Euclidean projection operator associated with \(\). The in-expectation generalization of SGD has been studied under on-average stability (Hardt et al., 2016; Zhou et al., 2022; Lei and Ying, 2020), while the exponential bounds have recently been established given that the stability holds with high probability over the sampling path of SGD (Feldman and Vondrak, 2019; Bassily et al., 2020).

### Prior results

Let us start by briefly reviewing some state-of-the-art exponential generalization bounds under the notion of uniform stability and its randomized variants. We denote by \(S\) if a pair of data sets \(S\) and \(\) differ in a single element. A randomized learning algorithm \(A\) is said to have _on-average \(_{N}\)-uniform stability_(Elisseeff et al., 2005) if it satisfies the following uniform bound:

\[_{S,Z Z}|_{}[(A(S,);Z)- (A(,);Z)]|_{N}.\] (2)

This definition is equivalent to the concept of uniform stability defined for the expectation of loss \(_{}[(A(S,);Z)]\). Suppose that the loss function is bounded in the interval \([0,M]\). Then essentially it has been shown in Feldman and Vondrak (2019) that for any \((0,1)\), with probability at least \(1-\) over \(S\), the on-average generalization error is upper bounded by

\[|_{}[R(A(S,))-R_{S}(A(S,))]|_{N} (N)()+M}.\] (3)Bousquet et al. (2020) later derived a slightly improved exponential bound that implies

\[|_{}[R(A(S,))-R_{S}(A(S,))]| _{N}(N)()+M}.\] (4)

These bounds are near-tight (up to logarithmic factors) in the sense of an lower deviation bound on sum of random functions with \(_{N}\)-uniform stability (Bousquet et al., 2020, Proposition 9). Concerning the excess risk bound, Klochkov and Zhivotovskiy (2021) essentially derived the following result using the sample-splitting techniques of Bousquet et al. (2020):

\[_{}[R(A(S,))]-R^{*}_{}+ [_{}]+_{N}(N)( )+,\] (5)

where \(_{}:=_{}[R_{S}(A(S,))]-_{w }R_{S}(w)\) represents the in-expectation empirical risk sub-optimality, and \(B\) is the constant of the generalized Bernstein condition (Koltchinskii, 2006). While sharp in the dependence on sample size, one common limitation of the above uniform stability implied generalization and risk bounds lies in that these high-probability results only hold _in expectation_ with respect to \(\), the internal randomness of algorithm.

Alternatively, consider that \(A\) has \(_{N}\)-uniform stability with probability at least \(1-^{}\) for some \(^{}(0,1)\) over the random draw of \(\), i.e.,

\[\{_{S S,Z Z}|(A(S,);Z)-(A(,);Z)|_{N}\} 1-^{}.\] (6)

Suppose that the randomness of \(A\) is independent of the training set \(S\). Then the bound of Bousquet et al. (2020) naturally implies that with probability at least \(1--^{}\) over \(S\) and \(\),

\[|R(A(S,))-R_{S}(A(S,))|_{N}(N)()+M}.\] (7)

This is by far the best known generalization bound of randomized stable algorithms that hold with high probability jointly over the randomness of data and algorithm. The result, however, relies heavily on the high-probability uniform stability as expressed in (6). For the SGD recursion (1) with fixed learning rate \(_{t}\), it is possible to show that \(_{N}+\) and \(^{}=N(-)\) in (6) (Bassily et al., 2020). For SGD with time decaying learning rates, which has been widely studied in theory (Harvey et al., 2019; Rakhlin et al., 2012) and applied in practice for training popular deep nets such as ResNet and DenseNet (Bengio et al., 2017), it is not clear if the condition in (6) is still valid for \(_{N}\) and \(^{}\) of interest. Madden et al. (2020) indeed have established a high-probability uniform stability bound for minibatch SGD with learning rates \(_{t}\). However, such a fairly conservative choice of learning rates tends to impair the empirical minimization performance of SGD and thus is of limited interest from the perspective of risk minimization.

More specially for randomized learning methods such as bagging (Breiman, 1996) and SGD, the randomness of algorithm can be precisely characterized by a vector of i.i.d. parameters \(=\{i_{1},...,i_{t}\}\) which are independent on data \(S\). In such cases, assume additionally that \(A(S,)\) has uniform stability with respect to \(\) conditioned on \(S\), i.e., \(_{}|(A(S,))-(A(S,))|_{T}\). Then the following exponential bound has been derived by Elisseeff et al. (2005):

\[|R(A(S))-R_{S}(A(S))|_{N}+(}{}+ _{T}))}.\] (8)

Provided that \(_{N}\) and \(_{T}\), the above bound shows that the generalization bound scales as \(}+}\) with high probability. However, the rate of the above bound is sub-optimal and will show no guarantee on convergence if \(_{N}}\) and/or \(_{T}}\). As an example, for non-convex SGD with learning rate \(_{t}=O\), it can be shown that \(_{N}}{N}\) and \(_{T}\) scales as large as \((1)\).

**Open problem.** So far, it still remains open if the exponential generalization bounds for deterministic uniformly stable algorithms might be extended to randomized learning algorithms under the variants of uniform stability tighter than the on-average version (2) but less restrictive than the high-probabilityversion (6). Particularly, we are interested in the following notion of \(L_{2}\)_-uniform stability_ (as formally introduced in Definition 1) with parameter \(_{_{2},N}\):

\[_{S,Z}_{}[((A(S,) ;Z)-(A(,);Z))^{2}]_{_{2},N}^{2},\] (9)

which represents a second-moment variant of the uniform stability for randomized learning algorithms. For example, as we will shortly show in Section 4 that SGD with practical time-decaying learning rates has \(L_{2}\)-uniform stability with favorable parameters. The main goal of the present work is to derive sharper exponential generalization bounds for randomized learning algorithms under the notion of \(L_{2}\)-uniform stability.

### Overview of our contribution

The fundamental contribution of this work is a near-optimal first-moment generalization error bound for \(L_{2}\)-uniformly stable algorithms, which is summarized in Theorem 1 and highlighted below:

\[_{}[|R(A(S,))-R_{S}(A(S,))|]_{ _{2},N}(N)()+M}.\]

While our first-moment bound above has an identical convergence rate to that of the on-average bound in (4), the former is stronger in the sense that the expectation is taken outside the generalization gap and thus implies the latter where the expectation is taken inside. The key ingredients of our analysis are a set of fine-grained concentration inequalities for randomized functions (Proposition 1) and sums of randomized functions (Proposition 2), which respectively generalize the classic bounded-difference inequalities and a prior result of Bousquet et al. (2020) under the considered \(L_{2}\)-uniform bounded difference conditions. These generalized concentration inequalities and their proof arguments are novel to our knowledge and should be of independent interests in analyzing randomized functions.

As an important consequence of our main result, we reveal that a bagging-based meta procedure (see Algorithm 1) can be used to boost the confidence of generalization for \(L_{2}\)-uniformly stable algorithms. More specifically, in the presented bagging procedure we independently run a randomized algorithm \(A\) multiple \(K\) times over a fraction of the training set to obtain \(K\) solutions. Then we evaluate the validation error of these candidate solutions over a holdout training subset, and output the solution that has the smallest training-validation gap. Our result in Theorem 2 shows that for any confidence level \((0,1)\), setting \(K()\) yields a near-optimal generalization bound for the selected solution that holds with high probability jointly over the randomness of data and algorithm.

We have substantialized our results to SGD with smooth (Corollary 1) or non-smooth (Corollary 2) convex losses, and smooth non-convex losses (Corollary 3) as well. For an instance, our result in Corollary 1 shows that when invoked to SGD with smooth convex loss and learning rates \(_{t}=(})\), the generalization bound of the output of Algorithm 1 may scale as \((N)}+}{N}\).

To compare with the \(}{N}\) in-expectation bound of smooth convex SGD (Hardt et al., 2016), our bound above for the boosted SGD is comparable in convergence rate while it holds with high probability jointly over the randomness of data and sampling path.

## 2 \(L_{2}\)-Uniform Stability and Generalization

### Notation and definitions

Let us introduce some notation to be used in our analysis. We abbreviate \([N]:=\{1,...,N\}\). Recall that \(S=\{Z_{i}\}_{i[N]}\) is a set of i.i.d. training data. Denote by \(S^{}=\{Z_{i}^{}\}_{i[N]}\) an independent copy of \(S\) and we write \(S^{(i)}=\{Z_{1},...,Z_{i-1},Z_{i}^{},Z_{i+1},...,Z_{N}\}\). For a real-valued random variable \(Y\), its \(L_{q}\)-norm for \(q 1\) is given by \(\|Y\|_{q}=([|Y|^{q}])^{1/q}\). By definition it can be verified that \( q 2\),

\[\|Y\|_{q}^{2}=([|Y|^{q}])^{2/q}=([|Y^{2}|^{q/2}] )^{2/q}=\|Y^{2}\|_{q/2}.\] (10)

Let \(h:^{N}\) be some measurable function and consider the random variable \(h(S)=h(Z_{1},...,Z_{N})\). For \(h(S)\) and any index set \(I[N]\), we define the following abbreviations:

\[h(S_{I}):=[h(S) S_{I}],\|h\|_{q}(S_{I}):=( [|h(S)|^{q} S_{I}])^{1/q}.\]We say a function \(f\) to be \(G\)-Lipschitz continuous over \(\) if \(|f(w)-f()| G\|w-\|\) for all \(w,\), and it is \(L\)-smooth if \(\| f(w)- f()\| L\|w-\|\). For a pair of functions \(f,f^{} 0\), we use \(f f^{}\) (or \(f^{} f\)) to denote \(f cf^{}\) for some universal constant \(c>0\).

In the following definition, we formally introduce the concept of \(L_{2}\)-uniform stability for randomized learning algorithms to be investigated in this work.

**Definition 1** (\(L_{2}\)-Uniform stability of randomized learning algorithms).: _We say a randomized learning algorithm \(A:^{N}\) to have \(L_{2}\)-uniform stability with parameter \(_{L_{2},N} 0\) if_

\[_{S,Z_{i}^{},}_{}[((A(S,);Z )-(A(S^{(i)},);Z))^{2}]_{L_{2},N}^{2}.\]

**Remark 1**.: _By definition the \(L_{2}\)-uniform stability has a second-moment dependence on the internal randomness of algorithm conditioned on data, while it is invariant to the data distribution. This justifies the name of such a notion of mixed algorithmic stability._

**Remark 2**.: _On one hand, by Jensen's inequality the \(L_{2}\)-uniform stability implies the on-average uniform stability defined in (2). On the other hand, the second-order form of \(L_{2}\)-uniform stability is by definition weaker than the high-probability uniform stability in (6). If the algorithm's randomness \(\) can be expressed as a set of i.i.d. random bits, then the \(L_{2}\)-uniform stability is also weaker than the conditional uniform stability conditioned on data \(S\)(Elisseeff et al., 2005)._

Throughout this paper, we assume for simplicity that the output models \(A(S^{(i)},)\) and \(A(S,)\) share the same internal random bit \(\) which is invariant to data. With similar analysis techniques, it is indeed possible to extend Definition 1 and our main results to the general setting where the randomness of algorithm is allowed to be dependent on data, such as in posterior sampling for Bayesian learning.

### Concentration inequalities for randomized functions

We begin by establishing in the following result a group of first- and second-order concentration inequalities (in moments) for _randomized_ functions of independent random variables.

**Proposition 1**.: _Let \(S=\{Z_{1},Z_{2},...,Z_{N}\}\) be a set of independent random variables valued in \(\) and \(\) be a random variable valued in \(\). Let \(g:^{N}\) be a measurable function that satisfies the following \(L_{2}\)-bounded-difference condition:_

\[_{S,Z_{i}^{}}_{}[(g(S,)-g(S^{(i)},) )^{2}]^{2}.\]

_Then for any \(q 2\),_

\[\|_{}[|g(S,)-_{S}[g(S,) ]|]\|_{q} 3,\] (11)

_and_

\[\|_{}[(g(S,)-_{S}[g(S,) ])^{2}]\|_{q} 68N^{2}q.\] (12)

Proof in sketch.: Let us consider \(h(S):=_{}[|g(S,)-_{S}[g(S,)|]]\). The given \(L_{2}\)-bounded-difference condition implies that \(h(S)\) has bounded-difference property. Then the desired first-order bound in (11) can be obtained by respectively invoking a moment Efron-Stein inequality (Boucheron et al., 2005, Theorem 2) to upper bound \(\|h(S)-[h(S)]\|_{q}\) and a slightly modified Efron-Stein inequality to bound the mean \([h(S)]\). To prove the second-order concentration bound, we consider the function \(h^{}(S):=_{}[(g(S,)-_{S}[g(S,) ])^{2}]\), which can be shown to be _weakly self-bounding_ (see Definition 2) under the \(L_{2}\)-bounded-difference condition. Then the desired bound (12) can be derived by applying the upper tail bound of Boucheron et al. (2005, Theorem 6.19) and lower tail bound of Klochkov and Zhivotovskiy (2021, Proposition 3.1) for weakly self-bounding functions. See Appendix A.2 for a detailed proof of this result. 

The moment bound in (11) extends the McDiarmid's (bounded difference) inequality (McDiarmid et al., 1989) to randomized functions with the \(L_{2}\)-bounded-difference property. The second-order concentration bound in (12) is crucial for proving the moment bound of sums in Proposition 2, as it can be used to sharply control some second-order components involved in the arguments. These generic inequalities are expected to be of independent interests for understanding the first-/second-order concentration behavior of randomized functions.

### A moment inequality for sums of randomized functions

As a key intermediate result, we further establish in the following proposition a moment concentration inequality for sums of randomized functions that satisfy the \(L_{2}\)-bounded-difference condition. This result extends the moment bound for sums of functions (Bousquet et al., 2020, Theorem 4) to sums of randomized functions.

**Proposition 2**.: _Let \(S=\{Z_{1},Z_{2},...,Z_{N}\}\) be a set of independent random variables valued in \(\) and \(\) be a random variable valued in \(\). Let \(g_{1},...,g_{N}\) be a set of measurable functions \(g_{i}:^{N}\) that satisfy the following conditions for any \(i[N]\):_

* \([g_{i}(S,) S Z_{i},]=0\) _and_ \(|[g_{i}(S,) Z_{i},]| M\)_, almost surely;_
* \(g_{i}(S,)\) _has the following_ \(L_{2}\)_-bounded-difference property with respect to all variables in_ \(S\) _except_ \(Z_{i}\)_, i.e.,_ \( j i\)_,_ \[_{S,Z_{j}^{}}_{}[(g_{i}(S,)-g_{i}(S^{(j)},))^{2}]^{2}.\]

_Then for all \(q 2\),_

\[\|_{}[|_{i=1}^{N}g_{i}(S,)|] \|_{q} 3M+38N_{2}N q.\]

Proof in sketch.: The main idea is inspired by the sample-splitting arguments of Feldman and Vondrak (2019); Bousquet et al. (2020), with some new ingredients developed to handle the first-moment operator taken over the internal randomness of functions. Here we just highlight a fundamental difference, which arises from using a newly developed moment inequality (Lemma 9) for bounding the sums of _conditionally independent randomized functions_ inside each individual data splits. Different from the version of Marcinkiewicz-Zygmund's inequality used in the original analysis of Bousquet et al. (2020), our new bound in Lemma 9 relies on some second-order (over the function's randomness) components which might be tightly bounded by the second-order concentration inequality in Proposition 1. A full proof is provided in Appendix A.3. 

**Remark 3**.: _For sums of deterministic functions, our result in Proposition 2 reduces to the existing moment bound of Bousquet et al. (2020, Theorem 4) which is known to be near-tight up to logarithmic factors. We comment in passing that the tightness analysis of Bousquet et al. (2020, Proposition 9) for deterministic functions can be more or less straightforwardly extended to randomized functions._

**Remark 4**.: _The bound of Proposition 2 would still be valid when the bounded-loss condition \(|[g_{i}(S,) Z_{i},]| M\) is relaxed to certain sub-Gaussian or sub-exponential stochastic versions._

### Main result on generalization bound

Consequently from Proposition 2, we can now establish our main result on the generalization bound of \(L_{2}\)-uniformly stable randomized learning algorithms.

**Theorem 1**.: _Let \(A:^{N}\) be a randomized learning algorithm that has \(L_{2}\)-uniform stability with parameter \(_{L_{2},N}\). Assume that the loss function \(\) is valued in \([0,M]\). Then for any \((0,1)\), the following bound holds with probability at least \(1-\) over the draw of \(S\):_

\[_{}[|R(A(S,))-R_{S}(A(S,))|] _{L_{2},N}(N)()+M}.\]

Proof.: See Appendix A.4 for a proof of this result. 

**Remark 5**.: _The first-moment bound in Theorem 1 naturally implies the on-average bound in (4) with an identical rate of convergence, though the former is obtained under the relatively stronger notion of \(L_{2}\)-uniform stability. As we will see shortly that the \(L_{2}\)-uniform stability can indeed be fulfilled by the popularly applied SGD algorithm and thus Theorem 1 is of practical importance for showcasing sharper generalization performance of SGD. When \(A\) is deterministic, our bound reduces to the near-optimal (up to logarithmic factors on sample size and failure tail) generalization bound for uniformly stable algorithms (Bousquet et al., 2020)._In view of the standard risk decomposition, the following excess risk tail bound can be readily obtained via applying Theorem 1 and Hoeffding's inequality:

\[_{}[R(A(S,))-R^{*}]_{}+ _{_{2},N}(N)()+M}.\] (13)

Here recall that \(_{}:=_{}[R_{S}(A(S,))]-_{w }R_{S}(w)\) is the sub-optimality of empirical risk minimization. Since the excess risk is by definition non-negative, the above bound can also be obtained under the weaker notion of on-average uniform stability (2) via applying (4). In this sense, the first-moment generalization error bound in Theorem 1 is substantially more challenging to derive than the excess risk bound. Additionally, under the generalized Bernstein condition (Koltchinskii, 2006), the risk bound (13) can be readily improved to (5) by directly applying the corresponding deviation optimal risk bound of Klochkov and Zhivotovskiy (2021) to the on-average loss function \(_{}[(A(S,);Z)]\) under on-average uniform stability condition.

## 3 Boosting the Confidence of Generalization

The confidence-boosting technique of Schapire (1990) is a classic meta approach that allows one to boost the dependence of a learning algorithm on the failure probability \(\) from \(1/\) to \((1/)\), at a certain cost of computational complexity. In this section, we show an implication of our first-moment bound in Theorem 1 for achieving high-probability generalization jointly over the randomness of data and algorithm, inside a natural framework of confidence-boosting.

### Confidence boosting via bagging

Given a randomized learning algorithm \(A\), we propose to study a bagging based confidence-boosting procedure as outlined in Algorithm 1. In this meta procedure, we independently run the algorithm \(A\) for \(K\) times over \(S_{1}\), a fraction of the training set, to obtain \(K\) different candidate solutions \(\{A(S_{1},_{k})\}_{k[K]}\). Then we evaluate the validation error of these candidate solutions over the holdout training subset \(S_{2}\), and cherry pick \(A(S_{1},_{k^{*}})\) that has the smallest gap between the training error and validation error, i.e., \(k^{*}=*{arg\,min}_{k[K]}|R_{S_{2}}(A(S_{1},_{k}))-R_{S_{1}} (A(S_{1},_{k}))|\). Particularly, consider that the internal randomness of \(A\) arises from random sampling with replacement of data points, such as SGD under with-replacement sampling. Then in this setting, the procedure can be regarded as a version of bagging (Breiman, 1996) with a greedy model ensemble scheme, which is invoked to the deterministic counterpart of \(A\) with fixed random bits (e.g., SGD with identity permutation) over the training subset \(S_{1}\).

``` Input :Randomized learning algorithm \(A\), data set \(S=\{Z_{i}\}_{i[N]}\), \((0,1)\) and \(K^{+}\). Output :\(A(S,_{k^{*}})\). Uniformly divide \(S\) into two disjoint subsets \(S_{1}\) and \(S_{2}\) with \(|S_{1}|=(1-)N,|S_{2}|= N\). for\(k=1,2,...,K\)do  Estimate \(A(S_{1},_{k})\) as an output of \(A\) over the subset \(S_{1}\) with random bit \(_{k}\). end for Select the random bit \(k^{*}\) according to \(k^{*}=*{arg\,min}_{k[K]}|R_{S_{2}}(A(S_{1},_{k}))-R_{S_{1}} (A(S_{1},_{k}))|\). ```

**Algorithm 1**Confidence-Boosting for Randomized Learning Algorithms

### Jointly exponential bounds

The following theorem is our main result about the generalization error bound of the output \(A(S_{1},_{k^{*}})\) that holds with high probability over the entire training set \(S\) and the random seeds \(\{_{k}\}_{k[K]}\).

**Theorem 2**.: _Suppose that a randomized learning algorithm \(A:^{N}\) has \(L_{2}\)-uniform stability with parameter \(_{_{2},N}\). Assume that the loss function \(\) is valued in \([0,M]\). Then for any \((0,1)\) and \(K 2()\), with probability at least \(1-\) over the randomness of \(S\) and \(\{_{k}\}_{k[K]}\), the output of Algorithm 1 satisfies_

\[|R(A(S_{1},_{k^{*}}))-R_{S}(A(S_{1},_{k^{*}}))|_{ _{2},(1-)N}(N)()+}}.\]Proof in sketch.: Based on Theorem 1, we first prove an intermediate result to show that the minimal generalization error of the \(K\) outputs satisfies \(_{k[K]}|R(A(S_{1},_{k}))-R_{S_{1}}(A(S_{1},_{k}))|_{ _{2},(1-)N}(N)()+}}\) provided that \(K()\). Next we show that the used greedy model selection strategy guarantees that the selected \(A(S,_{k^{*}})\) mimics the generalization behavior of that best performer among the \(K\) candidates, with a slightly expanded \((K/)\) factor representing the overhead of simultaneously bounding the generalization performance of \(K\) different candidate solutions over the holdout validation set. Finally the desired bound follows from the union probability argument. See Appendix B.1 for its full proof. 

**Remark 6**.: _The bound in Theorem 2 holds with high probability jointly over the randomness of sample and algorithm. Different from the bound in (7) that requires high probability uniform stability, Theorem 2 is valid under a substantially milder notion of \(L_{2}\)-uniform stability, though at the cost of multiple running of algorithm for confidence boosting. Compared to the bound in (8) that requires certain conditional uniform stability over the random bits of algorithm, our bound has sharper dependence on the uniform stability parameter yet under a weaker notion of stability._

**Remark 7**.: _Regarding the scale of the factor \(1/\) in the bound of Theorem 2, if setting \(=0.01\) (i.e., \(99\%\) of \(S\) are used as \(S_{1}\) for training), then the factor is around \(10.05\)._

Concerning the excess risk of Algorithm 1, we consider a slightly modified output \(A(S_{1},_{k^{*}})\) such that \(k^{*}=_{k[K]}R_{S_{2}}(A(S_{1},_{k}))\). Then based on the in-expectation risk bound (13), we can derive the following excess risk bound under the conditions of Theorem 2 using similar arguments:

\[R(A(S_{1},_{k^{*}}))-R^{*}_{}+_{ _{2},(1-)N}(N)()+}}.\] (14)

Again, the above risk bound is still valid under the weaker notion of on-average uniform stability (2).

## 4 Implications for SGD

This section is devoted to demonstrating the implications of Theorem 1 and Theorem 2 for the widely used SGD algorithm and its confidence-boosted versions as well. We focus on a variant of SGD under with-replacement sampling as outlined in Algorithm 2, which we call \(A_{}\). In what follows, we substantialize \(=\{i_{t}\}_{t[T]}\) the sample path of \(A_{}\) over a given data set, and \(\{_{k}\}_{k[K]}\) the \(K\) independent copies of \(\) when implemented with bagging as shown in Algorithm 1. Our results can also be extended to the without-replacement variant of SGD and the corresponding results are provided in Appendix D for the sake of completeness.

### Convex optimization with smooth loss

We first present the following lemma that establishes the \(L_{2}\)-uniform stability of \(A_{}\) with convex and smooth loss functions, such as logistic loss. See Appendix C.2 for its proof.

**Lemma 1**.: _Suppose that the loss function \((;)\) is convex, \(G\)-Lipschitz and \(L\)-smooth with respect to its first argument. Assume that \(_{t} 2/L\) for all \(t 1\). Then \(A_{}\) has \(L_{2}\)-uniform stability with parameter_

\[_{_{2},N}=2G^{2}_{t= 1}^{T}_{t}^{2}+}(_{t=1}^{T}_{t})^{2} )}.\]Given Lemma 1, we can apply Theorem 1 and Theorem 2 to immediately obtain the following generalization result for \(A_{}\) and its confidence-boosted version with smooth and convex losses.

**Corollary 1**.: _Suppose that the loss function \((;)[0,M]\) is convex, \(G\)-Lipschitz and \(L\)-smooth with respect to its first argument. Then for any \((0,1)\), it holds with probability at least \(1-\) over the randomness of \(S\) that \(_{}[|R(A_{}(S,))-R_{S}(A_{} (S,))|]\)_

\[G^{2}(N)()_{t=1}^{T} _{t}^{2}+}(_{t=1}^{T}_{t})^{2}}+M}.\]

_Moreover, consider Algorithm 1 specified to \(A_{}\) with learning rate \(_{t} 2/L\) and \(K()\). Then with probability at least \(1-\) over the randomness of \(S\) and \(\{_{k}\}_{k[K]}\), it holds that \(|R(A_{}(S_{1},_{k^{*}}))-R_{S}(A_{}(S_{1}, _{k^{*}}))|\)_

\[G^{2}(N)()_{t=1 }^{T}_{t}^{2}+N^{2}}(_{t=1}^{T}_{t} )^{2}}+}}.\]

**Remark 8**.: _For the conventional choice of \(_{t}=}\), the high-probability (w.r.t. data) generalization bounds in Corollary 1 for SGD and its confidence boosted version are roughly of scale \((N)}+}{N}\), which matches the corresponding \(}{N}\) in-expectation bound of SGD with smooth and convex losses (Hardt et al., 2016)._

Combining with the standard in-expectation optimization error bound of convex SGD (see, e.g., Shamir and Zhang, 2013), we can show the following excess risk bound of (modified) Algorithm 1 as a direct consequence of the generic bound (14) to \(A_{}\) with convex and smooth losses:

\[R(A_{}(S_{1},_{k^{*}}))-R^{*} G^{2}(N)()_{t=1 }^{T}_{t}^{2}+N^{2}}(_{t=1}^{T}_{t} )^{2}}\] \[+}}+ (w_{0},W^{*})+G^{2}_{t=1}^{T}_{t}^{2}}{_{t=1}^{T}_ {t}},\]

where \(W^{*}:=*{Argmin}_{w}R(w)\) and \(D(w,W^{*})=_{w^{*} W^{*}}\|w-w^{*}\|\). With learning rate \(_{t}=}\), the right hand side of the above roughly scales as \(}+}{N}+}\) which matches the prior high-probability excess risk bounds of SGD with convex losses (Harvey et al., 2019, Remark 3.7).

### Convex optimization with non-smooth loss

Now we turn to study the case where the loss is convex but not necessarily smooth, such as the hinge loss and absolute loss. We first establish the following lemma about the \(L_{2}\)-uniform stability parameter of \(A_{}\) in the considered setting. See Appendix C.3 for its proof.

**Lemma 2**.: _Suppose that the loss function \((;)\) is convex and \(G\)-Lipschitz with respect to its first argument. Then \(A_{}\) has \(L_{2}\)-uniform stability with parameter_

\[_{L_{2},N}=G^{2}^{T}_{t}^{2}+}( _{t=1}^{T}_{t})^{2}}.\]

With Lemma 2 in place, we can readily apply Theorem 1 and Theorem 2 to establish the following corollary about the generalization bounds of \(A_{}\) and its confidence-boosted version with convex and non-smooth loss functions.

**Corollary 2**.: _Suppose that the loss function \((;)[0,M]\) is convex and \(G\)-Lipschitz with respect to its first argument. Then for any \((0,1)\), it holds with probability at least \(1-\) over the randomness of \(S\) that \(_{}[|R(A_{}(S,))-R_{S}(A_{} (S,))|]\)_

\[G^{2}(N)()^{T}_{t}^{2}+ }(_{t=1}^{T}_{t})^{2}}+M}.\]_Moreover, consider Algorithm 1 specified to \(A_{}\) with \(K()\). Then with probability at least \(1-\) over \(S\) and \(\{_{k}\}_{k[K]}\), it holds that \(|R(A_{}(S_{1},_{k^{*}}))-R_{S}(A_{}(S_{1},_{ k^{*}}))|\)_

\[G^{2}(N)()^{T}_{t}^{2}+ N^{2}}(_{t=1}^{T}_{t})^{2}+}}}.\]

**Remark 9**.: _For SGD with decaying learning rates \(_{t}=}\), Corollary 2 admits high-probability generalization bounds of scale \((N)+}}+}\). With fixed rates \(_{t}\), Corollary 2 yields deviation bounds of scale \((N)}\) which matches the near-optimal rate by Bassily et al. (2020, Theorem 3.3)._

### Non-convex optimization with smooth loss

We further study the performance of Algorithm 1 for \(A_{}\) with smooth but not necessarily convex loss functions, such as normalized sigmoid loss (Mason et al., 1999). The following lemma estimates the \(L_{2}\)-uniform stability of \(A_{}\) in the considered setting. See Appendix C.4 for its proof.

**Lemma 3**.: _Suppose that the loss function \((;)\) is \(G\)-Lipschitz and \(L\)-smooth with respect to its first argument. Consider \(_{t} 1/L\). Then \(A_{}\) has \(L_{2}\)-uniform stability with parameter_

\[_{L_{2},N}=2G^{2}_{t=1}^{T}(3L_{= t+1}^{T}_{})u_{t}},\]

_where_

\[u_{t}:=_{t}^{2}+2_{t}_{=1}^{t-1}(L_{i=+1}^{t -1}_{i})_{}.\]

Based on Lemma 3, we can invoke Theorem 1 and Theorem 2 to show the following generalization result for \(A_{}\) and its confidence-boosted version with non-convex and smooth loss functions.

**Corollary 3**.: _Suppose that the loss function \((;)[0,M]\) is \(G\)-Lipschitz and \(L\)-smooth with respect to its first argument. Then for any \((0,1)\), it holds with probability at least \(1-\) over the randomness of \(S\) that \(_{}[|R(A_{}(S,))-R_{S}(A_{} (S,))|]\)_

\[G^{2}(N)()_{t=1}^{T} (L_{=t+1}^{T}_{})u_{t}}+M},\]

_where \(u_{t}:=_{t}^{2}+2_{t}_{=1}^{t-1}(L_{i=+1}^{t-1} _{i})_{}\) for all \(t 1\). Moreover, consider Algorithm 1 specified to \(A_{}\) with \(_{t}\) and \(K()\). Then with probability at least \(1-\) over \(S\) and \(\{_{k}\}_{k[K]}\), it holds that \(|R(A_{}(S_{1},_{k^{*}}))-R_{S}(A_{}(S_{1},_ {k^{*}}))|\)_

\[G^{2}(N)()_{t= 1}^{T}(L_{=t+1}^{T}_{})u_{t}}+}}.\]

**Remark 10**.: _For the decaying learning rates \(_{t}=\) with arbitrary \( 1\), the generalization bounds in Corollary 3 are of scale \((N)(T)}{ N}}+}\). For the constant learning rates \(_{t}\), the bounds are of scale \((N)}\)._

## 5 Conclusion

In this paper, we have introduced a novel concept of \(L_{2}\)-uniform stability for randomized learning algorithms and proved a strong first-moment generalization bound that holds with high probability over training sample. Equipped with this result, we have further developed a bagging based confidence-boosting procedure and shown that it yields near-optimal generalization bounds with high confidence jointly over the randomness of sample and algorithm. The power of our theory has been demonstrated through an application to SGD with time-decaying learning rates, where sharper generalization bounds have been obtained for both convex and non-convex loss functions.