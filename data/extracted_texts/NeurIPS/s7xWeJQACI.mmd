# Don't Stop Pretraining? Make Prompt-based

Fine-tuning Powerful Learner

 Zhengxiang Shi

University College London

London, United Kingdom

zhengxiang.shi.19@ucl.ac.uk &Aldo Lipani

University College London

London, United Kingdom

aldo.lipani@ucl.ac.uk

###### Abstract

Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the target task. Our empirical evaluations on 21 benchmarks demonstrate that the PCP consistently improves the performance of state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both semi-supervised and fully-supervised settings, even with only hundreds of unlabelled examples. Additionally, prompt-based FT with the PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity, eliminating the need for an iterative process and extra data augmentation. Our further analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets. Code is available at https://github.com/ZhengxiangShi/PowerfulPromptFT.

## 1 Introduction

Pre-training language models (LMs)  over massive unlabelled data and then fine-tuning on task-specific labelled data for the specific downstream task offer large performance gains across NLP tasks. In this study, we re-visit the commonly held belief in NLP  that continued pre-training LMs on either task-specific data  or in-domain data  is generally beneficial for improving the performance of fine-tuning (FT) on downstream tasks. As shown in Figure 1, our experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi- and fully-supervised settings reveal that conventional continued pre-training on task-specific data , known as task adaptive pre-training (TAPT) (see

Figure 1: Mean performance of CLS- and prompt-based FT across 16 NLP tasks when trained by _themselves_ or in combination with either TAPT or our proposed PCP in the semi-supervised setting. Please refer to Table 1 for details.

Figure 2e): (1) can lead to a substantial drop in performance of CLS-based FT (see Figure 2a) on sentence-pair tasks; and (2) may perform unstably across different tasks for prompt-based FT (see Figure 2b), which is typically considered a better alternative to CLS-based FT by previous studies  (SS4.2). These findings suggest that exclusively **presenting task-related texts to LMs** through continued pre-training may not be the most effective approach for improving the performance of FT in the aforementioned situations.

Recent research  on cross-task generalization has demonstrated the impressive improvement on zero-shot or few-shot learning capabilities of LMs (see Figure 2f). These studies suggest that **presenting appropriate instructions/prompt templates to LMs** through training on a range of NLP tasks improves their downstream performance on held-out tasks. Although these works train LMs with different objectives from pre-training phases, we interpret "fine-tuning LMs on a range of NLP tasks" as a special type of continued pre-training. Therefore, we hypothesize that **presenting both task-related texts and instructions/prompt templates to LMs** can relieve the above-mentioned issues for conventional continued pre-training and be beneficial for the target task performance. Rather than improve the generalizability of the LMs with supervised objectives, our work places a greater emphasis on enhancing specific target task performance with unsupervised pre-training objectives.

In this work, we propose Prompt-based Continued Pre-training (PCP) (SS3), which integrates instructions/prompt templates into task-related texts with golden or pseudo labels (see Figure 2g). Our experiments demonstrate that PCP consistently improves the performance of state-of-the-art prompt-based FT approaches  in both semi- and fully-supervised settings, covering both single sentence tasks and sentence pair tasks, and that the performance gains from PCP exceed those from conventional continued pre-training (TAPT) by a substantial margin (SS4.2). In the most favourable case, PCP boosts the performance of prompt-based FT by more than 20% absolute while TAPT results in a 9.2% performance decline. Furthermore, our results show that PCP outperforms state-of-the-art semi-supervised approaches  with greater simplicity, eliminating the need for an iterative process and extra data augmentation (SS4.3). Additionally, our analysis suggests that the PCP can efficiently improve the performance of prompt-based FT with only hundreds of unlabelled examples. Meanwhile, our analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets (SS4.4). Finally, we outline the limitations of our study and suggest avenues for future research (SS6).

In summary, the main contributions of this paper are as follows:

* Our study empirically demonstrates that conventional continued pre-training might not be as effective as initially thought and can even negatively impact fine-tuning performance, particularly in sentence pair tasks or when utilising prompt-based FT;
* Our evaluation on 21 classification and regression NLP tasks shows that our proposed method PCP provides a superior option to conventional continue pre-training for prompt

Figure 2: The overview of **Prompt-based Continued Pre-training** (g), in comparison to conventional continued pre-training (e) and instruction tuning (f), along with fine-tuning methods (a,b) and continued pre-training techniques (c,d). The verbalizer functions as a mapping from the task label space to individual words. We use masked language modelling for illustrative purposes, where <mask> represents a masked token in the LM vocabulary.

based FT. This approach consistently yields performance improvements in diverse model and dataset settings, even with only a few hundred unlabelled examples. Moreover, it can outperform state-of-the-art semi-supervised approaches with greater simplification;
* Our result shows the effectiveness of presenting both task-related texts and templates/instructions to the LMs through unsupervised pre-training objectives on improving the performance of prompt-based FT on downstream tasks. To the best of our knowledge, this is the first work to perform instruction tuning via unsupervised objectives.

## 2 Background

Suppose that we focus on the LMs trained with the masked language modelling (MLM) objective [25; 53]. Let \(X=\{x_{1},x_{2},...,x_{N}\}\) be a sequence of tokens, where \(N\) represents the total number of tokens. LMs are designed to encode the input text \(X\) into a corresponding sequence of hidden vectors \(\{_{i}^{d}\}\). As shown in Figure 1(a), the conventional CLS-based FT [25; 35; 76] trains the output vector \(\) corresponding to the [CLS] token with an additional head layer (_e.g.,_ an MLP layer). However, there is a discrepancy between the pre-training objective (see Figure 1(c)) and the CLS-based FT objective, which has led to research on prompt-based techniques for better LM performance.

The prompt-based FT is formulated as a MLM problem where the objective is to predict masked tokens [73; 74]. Specifically, the input text \(X\) is conditioned with a specific prompt template \(=(X)\), which includes one special token [MASK]. The prompt-based FT then maps the output vector associated with the [MASK] token to a label word. The probability of predicting class \(y\) is computed as:

\[p(y|X)=p()=(y)|),\] (1)

where the verbalizer \(:\) is a mapping from the task label space to individual words in the vocabulary \(\).

Prompt-based FT can use either hard or soft prompt templates \(\), with label words potentially being a part of the prompt templates as well [36; 100]. Hard prompt template [73; 28; 78] requires careful designs of prompts and label words for each task. The use of hard prompts, however, was found to be sub-optimal and sensitive to the choice of the prompt [102; 52]. Soft prompt [52; 100] was then proposed to use unused tokens from the vocabulary \(\) or additional tokens as tuneable embeddings for prompt template and can be directly trained with the task-specific supervision. This design allows the token embeddings in the prompt template to be updated independently of specific word embeddings after initialization, thus reducing the effort of searching for prompt templates and label words.

## 3 Our Approach: Prompt-based Continued Pre-training (PCP)

In this section, we introduce the proposed method, Prompt-based Continued Pre-training (PCP), which aims to improve the performance of LMs on downstream tasks through continued pre-training with prompt templates, as shown in Figure 1(g). Let \(L\{(X_{1},y_{1}),,(X_{n},y_{n})\}\) denote \(n\) labelled examples and \(U\{X^{}_{1},,X^{}_{m}\}\) denote \(m\) unlabelled examples. Our approach consists of two main steps, as described below.

Step 1: Construct Continued Pre-training Corpus.Initially, we select a model \(F\), pre-trained with the MLM objective and parameterized by \(\). We then train this model using the prompt-based FT, minimizing the target loss function \(\) on the labelled examples \(L\), as illustrated in Figure 1(b):

\[(L)=_{X_{i},y_{i} L}(y_{i},F((X_{i}),)),\] (2)

Next, we use the trained model \(F\) with the learned parameters \(^{}\) to generate predictions (termed "pseudo-labels") on the unlabelled samples \(U\):

\[y^{}_{i}=F((X^{}_{i}),^{}),\] (3)

For each text example \(X\) and its associated (golden or pseudo) label \(y\), we create an example for our proposed PCP as \(X^{pcp}=(X,(y))\), where the original [MASK] position is substituted with \((y)\). This results in a new corpus, \(=\{X^{pcp}_{i}\}_{i=1}^{n+m}\). In the fully-supervised setting, \(m=0\) and all examples use the golden labels.

Step 2: Perform continued pre-training and prompt-based FT.We then proceed to further pre-train another model \(G\), parameterized by \(\), using the MLM objective on the newly generated corpus \(\), to obtain the PCP checkpoint \(\) (see Figure 1(d)). Finally, we train model \(G\), initialised by \(\), using Equation 2 with prompt-based FT for downstream tasks.

In comparison to conventional continued pre-training, PCP does not require any modification for the model architecture or training process. The sole difference is the addition of a few extra tokens to the input text during continued pre-training. This modification does not hinder the efficiency of the method, _i.e.,_ both conventional continued pre-training and PCP maintain equal levels of efficiency. In this study, we primarily focus on LMs pre-trained with the MLM objective . It is noteworthy to mention that comprehensive exploration of other architectures [25; 70; 14; 65] remains an avenue for future research. Nonetheless, considering prompt-based fine-tuning approaches [52; 47; 51] have already been adapted for different model architectures and pre-training objectives [25; 70; 14; 65]. This implies that extending our method to alternative architectures should be a feasible undertaking.

## 4 Experiments and Results

In this section, we evaluate the proposed method PCP by comparing it with conventional continued pre-training and four state-of-the-art semi-supervised approaches. We assess their relative performance across 21 different classification and regression NLP tasks, including single-sentence and sentence-pair tasks. We conduct additional analysis concerning the performance lower bound of PCP and the effectiveness of the PCP across varying datasets and model sizes.

### Experimental Setup

Datasets.Our study conducts a comprehensive analysis of 21 NLP datasets, including classification and regression tasks. Following previous studies [28; 36; 100] on prompt-based FT, we derive 8 single-sentence tasks and 8 sentence-pair English tasks from the GLUE benchmark , SNLI , and 6 other widely used sentence classification tasks (_i.e.,_ SST-5, MR, CR, MPQA, Subj, TREC). Additionally, we use 5 popular benchmarks for semi-supervised learning from previous research [34; 21; 94; 48; 29; 77], including IMDB , AG News , Yelp Review1, Yahoo! Answer , and Amazon Review. See dataset details in Appendix SSA. We train the model with two different settings: (1) fully-supervised setting, where we train the model with the full training set; and (2) semi-supervised setting, where we sample the same amount of labelled data per class from the full training set. We re-sample the labelled data using the same five seeds for all comparison approaches and report the average performance with an error bar.

All Comparison Approaches.In our study, we mainly experiment using the RoBERTa-Base (\(125\)M) and the RoBERTa-Large (\(355\)M) models. We utilise the conventional CLS-based FT and two state-of-the-art prompt-based FT approaches: (1) "CLS-based FT": fine-tuning with the [CLS] token representation with an extra MLP layer; (2) "Prompt-based FT (hard)": fine-tuning with high-quality manual or auto-generated prompts and label words [73; 28]; and (3) "Prompt-based FT (soft)": fine-tuning with soft prompts using additional tokens for both templates and label words . Since the objective of soft prompt FT is to minimize the reliance on human-designed templates, we unify the template for all tasks here. See the template specifics used for each dataset in Appendix SSA. We train these three types of FT approaches from three different types of checkpoints to evaluate their relative effectiveness: (i) the off-the-shelf RoBERTa-Large checkpoint; (ii) the task-adaptive pre-training (TAPT) checkpoint  (represents the conventional continued pre-training). For sentence pair tasks, we concatenate the two sentences as an input example; and (iii) the proposed PCP checkpoint, obtained in SS3. For both (ii) and (iii), we perform MLM on all full training sets except MNLI, MNLI-mm, SNLI, QNLI, and QQP, where we select up to 10k unlabelled examples from the full training sets (see supplementary experiments on the full training sets in Appendix SSAD). Additionally, we compare the proposed PCP with four state-of-the-art semi-supervised approaches, including FixMatch , Dash , FlexMatch , and AdaMatch  (see descriptions of these approaches in Appendix SSAC), where back-translation  is used for data augmentation as previous works [94; 77] and prompt-based FT (hard) is used as the backbone. See hyperparameter and implementation details in Appendix SSAE.

### Comparison of the PCP and conventional continued pre-training

Table 1 presents and summarises our experimental results on 8 single-sentence tasks and 8 sentence-pair tasks. Below we delve deeper into our two major findings.

#1. TAPT is not consistently beneficial for sentence pair tasks, nor when prompt-based FT is employed.Initially, we re-visit the impact of TAPT (representing the conventional continued pre-training) on the CLS-based FT, as shown in Table 1. Our experimental results align with earlier

    &  \\   & **SST-2** & **SST-5** & **MR** & **CR** & **MPQA** & **Subi** & **TREC** & **CoLA** \\  & (acc) & (acc) & (acc) & (acc) & (acc) & (acc) & (acc) & (acc) & (Mar) \\  Majority (full) & 50.9 & 23.1 & 50.0 & 50.0 & 50.0 & 50.0 & 18.8 & 0.0 \\ Prompt-based zero-shot\({}^{}\) & 83.6 & 35.0 & 80.8 & 79.5 & 67.6 & 51.4 & 32.0 & 2.0 \\ in-context learning & \(84.8_{1.3}\) & \(30.6_{0.9}\) & \(80.5_{1.7}\) & \(87.4_{0.8}\) & \(63.8_{1.1}\) & \(53.6_{1.0}\) & \(26.2_{2.4}\) & \(-1_{5.2.4}\) \\   & & & & & & & \\ CLS-based FT & \(95.1\) & \(59.4\) & \(90.8\) & \(90.8\) & \(89.1\) & \(96.9\) & \(96.8\) & \(54.3\) \\ + TAPT & \(96.0\) & \(60.1\) & \(91.0\) & \(91.0\) & \(92.9\) & \(89.9\) & \(96.9\) & \(96.9\) & \(96.7\) & \(43.6\) \\ Prompt-based FT (hard) & \(95.2\) & 60.0 & 90.8 & 92.4 & 89.4 & 95.3 & 97.8 & 97.8 & 94.7 \\ + TAPT & \(93.5\) & \(41.7\) & \(60.4\) & \(90.3\) & \(90.8\) & \(94.6\) & \(89.5\) & \(96.1\) & \(95.9\) & \(97.6\) & \(44.0\) \\ + PCP (ours) & \(95.5\) & \(71.0\) & \(91.7\) & \(90.7\) & \(92.8\) & \(98.6\) & \(96.2\) & \(96.8\) & \(97.8\) & \(56.0\) \\ Prompt-based FT (soft) & \(94.2\) & \(59.8\) & \(90.4\) & \(92.7\) & \(87.8\) & \(96.4\) & \(97.4\) & \(61.3\) \\ + TAPT & \(92.7\) & \(91.8\) & \(90.5\) & \(91.8\) & \(91.4\) & \(92.5\) & \(98.5\) & \(96.8\) & \(97.8\) & \(97.8\) & \(52.6\) \\ + PCP (ours) & \(94.3\) & \(90.7\) & \(91.8\) & \(91.4\) & \(92.8\) & \(90.4\) & \(97.1\) & \(98.0\) & \(96.0\) & \(97.0\) \\   & & & & & & & & \\ CLS-based FT & \(81.2_{1.7}\) & \(41.7_{1.3}\) & \(76.3_{2.2}\) & \(79.5_{1.8}\) & \(65.1_{12.6}\) & \(91.0_{4.1}\) & \(80.3_{5.8}\) & \(26.7_{7.8}\) \\ + TAPT & \(82.8_{1.5}\) & \(71.3\) & \(86.1_{0.7}\) & \(86.2_{1.7}\) & \(73.74_{1.7}\) & \(96.4_{1.2}\) & \(91.5\) & \(80.6_{4.0}\) & \(1.9_{1.4}\) & \(1.9_{1.4}\) \\ Prompt-based FT (hard) & \(92.7_{1.3}\) & \(46.7_{1.5}\) & \(86.2_{1.2}\) & \(90.7_{0.8}\) & \(80.8_{0.5}\) & \(91.0_{1.1}\) & \(88.74_{4.4}\) & \(72.5\) \\ + TAPT & \(92.9_{1.0}\) & \(102.5\) & \(48.9_{1.1}\) & \(72.8\) & \(88.4_{0.2}\) & \(72.9_{3.2}\) & \(84.6_{1.9}\) & \(93.5_{1.1}\) & \(72.5\) & \(85.2\) & \(1.43_{2.5}\) \\ + PCP (ours) & \(93.6_{0.3}\) & \(50.9_{1.8}\) & \(80.0_{0.6}\) & \(92.0_{0.6}\) & \(92.0_{0.4}\) & \(87.9_{0.5}\) & \(96.7_{0.4}\) & \(96.6_{0.3}\) & \(25.6\) & \(26.9_{1.9}\) \\ Prompt-based FT (soft) & \(92.5\) & \(48.0_{1.7}\) & \(85.8\) & \(98.4_{0.2}\) & \(85.1_{0.8}\) & \(81.5_{0.3}\) & \(81.6_{0.2}\) & \(83.3\) & \(83.0_{0.0}\) & \(94.7\) \\ + TAPT & \(93.4_{0.5}\) & \(74.0_{1.2}\) & \(140.8\) & \(85.8_{0.8}\) & \(71.7\) & \(89.6_{1.2}\) & \(83.4_{1.5}\) & \(72.3_{0.7}\) & \(84.5_{2.4}\) & \(21.1_{1.8}\) \\ + PCP (ours) & \(93.9_{0.3}\) & \(71.4\) & \(50.7_{1.2}\) & \(87.9_{0.8}\) & \(92.0_{0.6}\) & \(11.2\) & \(88.3_{0.5}\) & \(77.9_{0.4}\) & \(94.0_{1.4}\) & \(88.6_{1.6}\) & \(21.5_{2.5}\) & \(11.6_{2.5}\) \\   & & & & & & & & & \\   & & & & & & & & & \\ CLS-based FT & \(82.1\) & \(82.7\) & \(88.1\) & \(90.2\) & \(83.4\) & \(91.9\) & \(79.7\) & \(91.2\) \\ + TAPT & \(81.0\) & \(41.1\) & \(80.1\) & \(86.1_{0.4}\) & \(85.6\) & \(46.4_{1.6}\) & \(83.4_{1.0}\) & \(91.6\) & \(80.2\) & \(78.5\) & \(90.4\) & \(1.8\) \\ Prompt-based FT (hard) & \(85.4\) & \(85.8\) & \(89.0\) & \(89.6\) & \(88.1\) & \(88.1\) & \(93.1\) & \(73.8\) & \(89.5\) \\ + TAPT & \(82.8\) & \(82.8\) & \(83.2\) & \(12.6\) & \(88.3\) & \(84.0\) & \(90.9\) & \(11.3\) & \(83.4\) & \(92.7\) & \(104.7\) & \(78.2\) & \(91.2\) \\ + PCP (ours) & \(86.5\) & \(71.1\) & \(86.2\) & \(71.4\) & \(89.5\) & \(91.5\) & \(71.5\) & \(88.5\) & \(76.4\) & \(93.3\) & \(102.0\) & \(79.6 [39; 35; 77], showing that TAPT generally improves the performance of the CLS-based FT on 7 out of 8 single sentence tasks in both semi-supervised and full-supervised setting. However, intriguingly, we observe that TAPT negatively affects the performance of CLS-based FT on 6 out of 8 sentence pair tasks, as summarised in Figure 1 and the at the bottom of Table 1. This finding implies that conventional continued pre-training (TAPT) may not be beneficial for sentence pair tasks.

Moreover, our investigation reveals that TAPT may negatively affect prompt-based FT. Specifically, in the fully supervised setting, TAPT results in reduced performance on 11 out of 16 tasks for prompt-based FT (hard) and on 9 out of 16 tasks for prompt-based FT (soft). In the most favourable scenario, TAPT enhances the performance of prompt-based FT (soft) from 73.9% to 79.9% on the QQP dataset. Conversely, in the least favourable situation, TAPT diminishes the performance of prompt-based FT (hard) from 54.7% to 44.0% on the CoLA dataset. In the semi-supervised setting, TAPT leads to a decline in performance on 12 out of 16 tasks for both prompt-based FT (hard) and prompt-based FT (soft) (see the summary of results in Figure 1 and the at the bottom of Table 1). Particularly, for sentence pair tasks, TAPT results in an average absolute decrease of 9.5% in performance for prompt-based FT. These results suggest that the effectiveness of TAPT varies across different tasks and cannot be universally applied. We conduct additional experiments to confirm the limitations of TAPT persist across different sizes of the pre-training corpus in Appendix D.

#2. PCP offers consistent and substantial improvements in both semi- and fully-supervised settings.As depicted in Table 1, our experiments covering 16 datasets in both semi- and fully-supervised settings, including single sentence tasks and sentence pair tasks, reveal that (1) PCP consistently boosts the performance of prompt-based FT; and that (2) the performance gains achieved by PCP consistently exceed those obtained by TAPT by a substantial margin. Specifically, compared to prompt-based FT, PCP leads to more than a 1.0% average absolute improvement in the fully-supervised setting and contributes to an average absolute performance boost of 6.8% in the semi-supervised setting across 16 tasks. Compared to TAPT, PCP yields over a 1.8% average absolute improvement in the fully-supervised setting and contributes to an average absolute performance increase of 11.2% in the semi-supervised setting across 16 tasks. Notably, PCP can produce considerable gains in certain datasets. For instance, it elevates the performance of prompt-based FT (hard) from 7.2% (Matthews Correlation Coefficient) to 25.0%, while TAPT even reduces the performance of the prompt-based FT. Additionally, PCP improves the performance of prompt-based FT (soft) on the QNLI dataset from 64.2% to 84.3% with 31% improvement, while TAPT leads to a 9.2% absolute performance decline. We attribute the improvements to presenting the prompt template to the LMs through the further pre-training phrase, which implies that merely showing task-related texts to the LMs may not be the optimal approach for prompt-based FT.

### Comparison of the PCP and state-of-the-art semi-supervised approaches

Table 2 presents our experimental results on five datasets, comparing the proposed PCP with state-of-the-art semi-supervised approaches. Below we delve deeper into our main finding with a discussion.

The proposed PCP outperforms state-of-the-art semi-supervised approaches on 4 out of 5 tasks.As shown in Table 2, our proposed PCP approach with either hard or soft variants of prompt-based

    &  &  &  &  &  &  \\   & 20 & 100 & 40 & 200 & 40 & 200 & 40 & 200 & 40 & 200 \\  DASH  & 93.34\({}^{}\), 94.53\({}^{}\), 93.36\({}^{}\), 85.03\({}^{}\), 85.03\({}^{}\), 85.09\({}^{}\), 87.90\({}^{}\), 47.42\({}^{}\), 58.51\({}^{}\), 60.07\({}^{}\), 66.69\({}^{}\), 44.09\({}^{}\), 65.39\({}^{}\), 59.5\({}^{}\), 69.04 \\ FixMatch  & 95.26\({}^{}\),94.20\({}^{}\), 85.48\({}^{}\), 85.41\({}^{}\), 88.21\({}^{}\), 47.67\({}^{}\), 26.1\({}^{}\), 58.51\({}^{}\), 61.56\({}^{}\), 86.37\({}^{}\), 44.26\({}^{}\), 52.53\({}^{}\), 71.69\({}^{}\), 51.28\({}^{}\), 61.34\({}^{}\), 45.83\({}^{}\), 51.41\({}^{}\), 51.91\({}^{}\), 69.71 \\ FixMatch  & 95.26\({}^{}\),94.94\({}^{}\), 85.31\({}^{}\), 85.79\({}^{}\), 85.40\({}^{}\), 60.02\({}^{}\), 58.93\({}^{}\), 58.93\({}^{}\), 63.87\({}^{}\), 66.38\({}^{}\), 64.08\({}^{}\), 44.66\({}^{}\), 53.05\({}^{}\), 70.35 \\ AtomMatch  & 95.20\({}^{}\), 89.94\({}^{}\), 85.12\({}^{}\), 87.29\({}^{}\), 87.26\({}^{}\), 50.42\({}^{}\), 58.95\({}^{}\), 63.68\({}^{}\), 69.08\({}^{}\), 44.60\({}^{}\), 45.30\({}^{}\), 70.35 \\  Prompt-based FT (hard) & 86.78\({}^{}\), 80.52\({}^{}\), 84.87\({}^{}\), 86.91\({}^{}\), 86.93\({}^{}\), 46.69\({}^{}\), 82.75\({}^{}\), 60.63\({}^{}\), 69.41\({}^{}\), 44.34\({}^{}\), 57.01\({}^{}\), 68.20 \\ + PCP (ours) & 92.49\({}^{}\), 94.24\({}^{}\), 87.09\({}^{}\), 88.94\({}^{}\), 52.92\({}^{}\), 63.15\({}^{}\), 65.58\({}^{}\), 70.22\({}^{}\), 53.43\({}^{}\), 59.69\({}^{}\), 72.77\({}^{}\) \\ Prompt-based FT (soft) & 88.14\({}^{}\), 94.00\({}^{}\), 85.15\({}^{}\), 87.64\({}^{}\), & 87.64\({}^{}\), 55.06\({}^{}\), 45.06\({}^{}\), 63.62\({}^{}\), 65.20\({}^{}\), 18.15\({}^{}\), 67.53\({}^{}\), 54.22\({}^{}\), 56.50\({}^{}\), 51.0\({}^{}\), 68.34 \\ + PCP (ours) & 93.53\({}^{}\), 54.36\({}^{}\), 87.26\({}^{}\), **88.986\({}^{}\)**, 50.06\({}^{}\), 60.63\({}^{}\), 62.92\({}^{}\), 65.20\({}^{}\), 16.52\({}^{}\), 70.08\({}^{}\), 52.78\({}^{}\), 89.16\({}^{}\), 72.49 \\  Prompt-based FT (hard)\({}^{}\) & 95.60 & 91.06 & 68.71 & 30.30 & 63.85 & 78.70 \\ Prompt-based FT (soft)\({}^{}\) & 95.50 & 91.10 & 69.63 & 75.66 & 63.32 & 79.04 \\   

Table 2: Comparison between the PCP and four semi-supervised approaches using RoBERTa-Large. Each dataset is evaluated with two different labelled data sizes and full training set is used as unlabelled data. \(\) indicates that full training set is used as the labelled data. We report the average Macro-\(F_{1}\) score on the test set across five seeds, with standard deviations as subscripts. For each column, blue represents the best performance and orange stands for the second-best performance.

FT outperforms the best-performing semi-supervised approaches on 4 out of 5 datasets. Notably, the prompt-based FT (hard) with the PCP outperforms the best performing semi-supervised approaches (FlexMatch) with an absolute 5.5% Macro-\(F_{1}\) score on the Amazon Review dataset when 200 labelled training examples are used. While the best performing semi-supervised approach, FixMatch, outperforms PCP by 1.7% in absolute value on the IMDB dataset using 20 labelled examples, the performance discrepancy narrows as the number of labelled training examples increases. Overall, the prompt-based FT (hard) and (soft) with the PCP outperform all these semi-supervised approaches with an average absolute performance improvement of more than 2% across various datasets and labelled dataset sizes, demonstrating the effectiveness of our proposed approach.

Discussion.State-of-the-art semi-supervised approaches typically rely on generating pseudo labels for unlabelled examples in order to train student and teacher models iteratively [4; 15; 95; 27; 94; 29]. However, this iterative process is prone to _confirmation bias_[83; 2; 31], which can result in error accumulation if the pseudo label is incorrect at any iterative step [49; 88; 31; 20]. Various efforts have been made to mitigate _confirmation bias_, such as using only high-confidence pseudo labels [80; 99; 12] or relying heavily on data augmentation [94; 21; 11]. While these efforts make the training process more sophisticated, the issue remains difficult to fully address [20; 77]. Our proposed method offers an alternative way to utilise pseudo labels different from previous semi-supervised approaches [98; 58]. Instead of relying on an iteration process with direct supervision signals from pseudo labels, we incorporate pseudo labels through continued pre-training with an unsupervised objective (_i.e.,_ MLM). While our proposed approach may not always outperform semi-supervised approaches across all benchmarks, it delivers highly competitive performance while significantly streamlining the process by removing the necessity for iteration and additional data augmentation. We will discuss the efficiency of the proposed PCP later (SS4.4). Additionally, PCP is orthogonal to these semi-supervised approaches and can be combined easily by initialising their backbone from the PCP checkpoint. In future work, we plan to investigate the more specific use cases where our proposed PCP may be preferred over these semi-supervised approaches.

### Further Analysis

#1. What is the lower bound of the model performance using the PCP?To understand the lower bound of PCP performance, we conduct additional analysis with two different configurations of pseudo labels in PCP: (1) all pseudo labels are incorrect; and (2) all labels are randomly selected. Figure 3 depicts the performance using different types of pseudo labels. We use the prompt-based FT without PCP (shown in yellow) and with PCP (shown in red) as baselines. Experimental results indicate that using incorrect pseudo labels (shown in blue) typically leads to inferior performance. In experiments using two prompt-based FT on 16 datasets, we find that using random labels leads to improved outcomes in 19 out of 32 scenarios. This suggests that PCP with random labels has over a 50% chance of improving the performance of prompt-based FT, indicating that the performance lower

Figure 3: The performance lower bound of the PCP, where “wrong labels” indicates that all labels in the PCP are incorrect and “random labels” indicates that all labels in the PCP are randomly selected. For each dataset, 16 examples per class are used as labelled data and the full training set is used as unlabelled data. The mean performance on test sets is reported over 5 different seeds.

bound is satisfactory. Additionally, PCP with random labels improves the performance on sentence pair tasks in 8 out of 16 cases, while TAPT leads to poorer results in 15 of 16 cases (refer to Table 1). This suggests that PCP can be advantageous even when using random labels, providing benefits in scenarios where TAPT falls short. Interestingly, unlike prior study  on in-context learning , where LMs using random labels in demonstrations perform close to those using ground-truth labels, our results show that using pseudo labels assigned by a trained model (shown in red) consistently leads to the better performance, highlighting the importance of accurate pseudo labels.

### #2. What are the requirements of data size and computational resources for the PCP?

To gain a deeper understanding of the efficacy of our proposed PCP method, we conduct additional analysis to determine the number of data points necessary for the PCP. Figure 4 (left) presents the performance of prompt-based FT methods, including both hard and soft variants, across four datasets. The prompt-based FT performance generally improves when the PCP is implemented with more than 1000 unlabelled examples, and some enhancements can be observed even with just 100 unlabelled examples. This indicates that continued pre-training (both TAPT and PCP) is not necessarily computationally demanding and can be used efficiently even with only hundreds of training examples. In our experiments, performing the PCP on 1k unlabelled example takes less than 10 minutes using two 24GB NVIDIA 3090 GPUs, and all PCP performance achieved in SS4.2 use fewer than 10k unlabelled examples. This is a stark contrast to the previous work  that pursued similar objectives (for parameter-efficient fine-tuning) to ours but utilised 10GB of English text data.

### #3. Power of scale.

Our empirical analysis investigates the impact of increasing the backbone LM size on the model performance using the PCP. Figure 4 (right) shows the results of prompt-based FT methods, including hard and soft variants, trained using either TAPT or PCP, on four datasets. The performance of the PCP method largely improves as the backbone LM size expands, which aligns with the scaling laws observed in LMs . Furthermore, the PCP method consistently surpasses other baseline approaches, highlighting the advantages of the PCP across different model sizes.

Figure 4: (Left) The effect of different unlabelled data sizes using RoBERTa-Large. (Right) The effect of Scaling Laws, where RoBERTa-Base (123M) and RoBERTa-Large (354M). All comparison approaches are trained with 16 examples per class for each dataset.

  
**Dataset** & **Size** & **FT** & **+TAPT** & **+PCP** \\  IMDB & 23K & \(87.3_{1.2}\) & \(88.9_{1.3}\) & \(91.4_{0.5}\) \\ AG News & 100K & \(86.4_{0.9}\) & \(87.6_{1.1}\) & \(88.0_{0.4}\) \\ Yelp Review & 250K & \(52.4_{2.5}\) & \(60.3_{1.9}\) & \(61.44_{2.0}\) \\ Amazon Review & 250K & \(51.2_{1.8}\) & \(56.8_{1.2}\) & \(57.0_{1.5}\) \\ Yahoo! Answer & 500K & \(64.9_{0.8}\) & \(64.9_{1.1}\) & \(69.0_{1.4}\) \\   

Table 3: Test Results for prompt-based FT (soft) using RoBERTa-Base with varying continued pre-training corpus sizes. Average Macro-\(F_{1}\) with standard deviations are reported across five seeds. The model is trained on the IMDB dataset using 100 labelled examples and uses 200 labelled examples for other datasets. The best performance for each dataset is highlighted in blue.

### The impact of a larger continued pre-training corpus on the model performance using PCP and TAPT.

Here we expand our investigation to whether the advantage of the proposed PCP approach persists as the size of the continued pre-training corpus increases. Table 3 presents the performance of prompt-based FT (soft), trained using either TAPT or PCP, across five datasets with varying sizes of unlabelled training examples. These experimental results are consistent with our findings in SS4.2 and SS4.3, showing that the proposed PCP approach consistently outperforms the model performance using the TAPT even when the larger corpus for continued pre-training is used.

### Ablation study on the label and template inclusion in PCP.

To gain a deeper understanding of the individual contributions of pseudo labels and templates in our proposed PCP method, we conduct an additional ablation study, where we solely utilize pseudo labels or templates. This ablation study is carried out using soft prompt-based fine-tuning. As shown in Table 4, the experimental results reveals that using either labels or templates exclusively will hurt the model's performance compared to our proposed PCP method, highlighting the vital importance of integrating both templates and pseudo labels.

### The impact of prolonged fine-tuning on the model performance.

To ascertain that the effectiveness of our proposed method is not simply due to an extended fine-tuning duration, we conduct additional experiments. We train Cls-based FT 5 times more steps (5k steps in total) from the TAPT checkpoint. As shown in Table 5, our results reveal that prolonged fine-tuning only brings about a marginal improvement of only 0.1% across the eight tasks. Notably, this still falls significantly short of our proposed method (8.1% in absolute).

## 5 Related Work

Prompt-based Approaches.In recent years, researchers have been exploring prompt-based approaches to improve the performance of fine-tuning. These approaches can be broadly divided into two research directions. The first direction, known as prompt-based FT, optimizes all parameters in LMs for better performance [73; 28; 52; 100], as discussed in SS2. Adaprompt  improved the performance of hard prompt-based FT [73; 28] on single sentence tasks through conventional continued pre-training, which is generally consistent with our experimental results. The second direction is parameter-efficient fine-tuning (PEFT) [51; 68; 47; 81; 86], which aims to achieve competitive results while maintaining low computational costs. PPT  strives to improve the performance of PEFT  by further pre-training the T5 model , which pursues a similar idea as ours. However, this method relies on a series of hand-crafted and task-dependent designs for further pre-training, making it less adaptable to novel downstream tasks . Furthermore, it demands a much larger training corpus, as discussed in SS4.4. In contrast, our work offers a uniform design across all tasks and focuses on prompt-based FT. In future work, we plan to explore the compatibility of continued pre-training (including both TAPT and our proposed PCP) and PEFT methods.

    & **SST-2** & **SST-5** & **MR** & **CR** & **MPQA** & **Subj** & **TREC** & **CoLA** & **Mean** \\  Prompt FT & 92.5 & 48.0 & 86.8 & 90.8 & 81.2 & 90.3 & 83.0 & 4.9 & 72.2 \\ Prompt FT +PCP & 93.9 & 50.7 & 89.8 & 92.0 & 88.3 & 94.9 & 88.6 & 21.5 & 77.5 \\ Prompt FT +PCP (Labels Only) & 93.7 & 50.8 & 87.7 & 91.3 & 85.1 & 94.3 & 85.7 & -0.7 & 73.5 \\ Prompt FT +PCP (Template Only) & 90.7 & 43.5 & 88.6 & 92.6 & 82.0 & 95.1 & 84.1 & 0.7 & 72.2 \\   

Table 4: Ablation study on the inclusion of the template and labels in our proposed PCP. The test Results using soft prompt FT and RoBERTa-Large are reported. The best performance for each dataset is highlighted in blue.

    & **SST-2** & **SST-5** & **MR** & **CR** & **MPQA** & **Subj** & **TREC** & **CoLA** & **Mean** \\  Cls-based FT (1k steps) + TAPT & 88.2 & 43.4 & 86.1 & 86.2 & 73.7 & 94.2 & 80.4 & 1.9 & 69.3 \\ Cls-based FT (5k steps) + TAPT & 89.6 & 43.4 & 86.7 & 87.0 & 72.9 & 94.6 & 79.0 & 1.7 & 69.4 \\ Prompt FT (1k steps) + PCP & 93.9 & 50.7 & 89.8 & 92.0 & 88.3 & 94.9 & 88.6 & 21.5 & 77.5 \\   

Table 5: Ablation study on the prolonged fine-tuning, where RoBERTa-Large is used as the backbone model. The test Results using Cls-based FT and soft prompt FT are reported. The best performance for each dataset is highlighted in blue.

Train LMs with Instructions/Templates.Our work is related to training LMs with templates. Recent studies [42; 3; 65; 91; 61; 72; 89; 59] have explored the idea of LMs training on a variety of NLP tasks with natural language instructions/templates, with the goal of generalizing to unseen tasks. Similar ideas, prompt transfer, have also been explored in the context of PEFT [33; 81; 86; 75], which seeks to learn an effective representation of the soft prompt for the target task by training on other tasks. In our approach, we transfer knowledge from task-related texts with prompt templates that are tailored to a single target task to LMs.

Semi-supervised Learning.Our work is related to _semi-supervised learning_[32; 19; 43], with the goal of utilising unlabelled data effectively. Continued pre-training followed by fine-tuning [39; 82; 35] is one type of semi-supervised approaches. While the benefits of continued pre-training are well acknowledged [6; 1; 56], it is commonly assumed that large amounts of data are necessary for continued pre-training [_e.g.,_ 50; 38; 33]. Contrarily, our research demonstrates that continued pre-training can improve performance using only a few hundred unlabelled samples. _Self-training_[98; 58] is another powerful semi-supervised approach, which typically uses student-teacher models to assign pseudo-labels to the unlabelled data [46; 44; 83; 62; 4; 15; 27; 94; 80; 29]. Our work offers an alternative way to use pseudo-labels without resorting to an iterative process, as discussed in SS4.3.

## 6 Epilogue

Conclusion.This study challenges the widely accepted notion in NLP, showing that conventional continued pre-training can be detrimental to model performance, especially for sentence pair tasks and prompt-based FT. As an alternative, we propose Prompt-based Continued Pre-training (PCP), which consistently improves the performance of state-of-the-art prompt-based FT approaches over conventional continued pre-training. Additionally, our proposed PCP outperforms state-of-the-art semi-supervised approaches with a more streamlined process. Further analysis reveals that the advantages of PCP remain consistent across different sizes of models and datasets. This study emphasizes the importance of presenting both task-related texts and templates/instructions to LMs during pre-training for better fine-tuning performance on downstream tasks, contributing to the growing body of research on the optimisation of pre-training and fine-tuning strategies in NLP.

Limitations and Broader Impact.We outline several limitations inherent to our research:

* **The scale of language models.** Our experiments utilise relatively modestly-sized language models . The implications of scaling up to more advanced language models, such as the Llama-2  or the mixture-of-experts approach like GPT-4 , remains an open question. In the context of large language models, applying PCP with a full set of parameter updates for a specific task may not be justifiable in terms of computational costs. Future research could explore multi-task learning strategies or parameter-efficient continued pretraining.
* **The architecture of language models.** Our work is limited to encoder-only models [25; 53]. To generalize our findings, future research should investigate the effects of our method PCP on encoder-decoder  and decoder-only architectures .
* **The diversity of tasks.** Our evaluation is confined to text classification and regression tasks. Future research should investigate generative or multi-modal tasks, which may offer more comprehensive insights into the applicability of our methods PCP.

In addition, our work is based on pre-training and prompting methods for LMs. Previous works [8; 14; 7] have extensively discussed the risks and potential harms associated with LMs, including the amplification of undesirable biases learned from unlabelled training data [8; 5; 16]. The energy cost and carbon footprint for our work were approximately 125 kWh and 70 kg CO\({}_{2}\)e, which are comparatively smaller than LM pre-training [25; 53; 14; 23].