# Energy-based Hopfield Boosting for

Out-of-Distribution Detection

 Claus Hofmann \({}^{1}\) Simon Schmid \({}^{2}\) Bernhard Lehner \({}^{3}\)

Daniel Klotz \({}^{4}\) Sepp Hochreiter \({}^{1}\)

\({}^{1}\) Institute for Machine Learning, JKU LIT SAL IWS Lab,

Johannes Kepler University, Linz, Austria

\({}^{2}\) Software Competence Center Hagenberg GmbH, Austria

\({}^{3}\) Silicon Austria Labs, JKU LIT SAL IWS Lab, Linz, Austria

\({}^{4}\) Department of Computational Hydrosystems,

Helmholtz Centre for Environmental Research-UFZ, Leipzig, Germany

hofmann@ml.jku.at

###### Abstract

Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to focus on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 from 2.28 to 0.92 on CIFAR-10, from 11.76 to 7.94 on CIFAR-100, and from 50.74 to 36.60 on ImageNet-1K.

## 1 Introduction

Out-of-distribution (OOD) detection is crucial when using machine learning systems in the real world (Ruff et al., 2021; Yang et al., 2021; Liu et al., 2021). Deployed models will -- sooner or later -- encounter inputs that deviate from the training distribution. For example, a system trained to recognize music genres might also hear a sound clip of construction site noise. In the best case, a naive deployment can then result in overly confident predictions. In the worst case, we will get erratic model behavior and completely wrong predictions (Hendrycks and Gimpel, 2017). The purpose of OOD detection is to classify these inputs as OOD, such that the system can then, for instance, notify users that no prediction is possible. In this paper we propose Hopfield Boosting, a novel OOD detection method that leverages the energy component of modern Hopfield networks (MHNs; Ramsauer et al., 2021) and advances the state-of-the-art of OOD detection. This energy represents a measure of dissimilarity between a set of data instances \(\) and a query instance \(\). It is therefore a natural fit for doing OOD detection (as shown in Zhang et al., 2023).

Hopfield Boosting uses an auxiliary outlier data set (AUX) to _boost_ the model's OOD detection capacity. This allows the training process to learn a boundary around the in-distribution (ID) data, improving the performance at the OOD detection task. In summary, our contributions are as follows:1. We propose Hopfield Boosting, an OOD detection approach that samples weak learners by using the MHE (Ramsauer et al., 2021).
2. Hopfield Boosting achieves a new state-of-the-art in OOD detection. It improves the average false positive rate at 95% true positives (FPR95) from 2.28 to 0.92 on CIFAR-10, from 11.38 to 7.94 on CIFAR-100, and from 50.74 to 36.60 on ImageNet-1K.
3. We provide theoretical background that motivates Hopfield Boosting for OOD detection.

## 2 Related Work

Some authors (e.g., Bishop, 1994; Roth et al., 2022; Yang et al., 2022) distinguish between anomalies, outliers, and novelties. These distinctions reflect different goals within applications (Ruff et al., 2021). For example, when an anomaly is found, it will usually be removed from the training pipeline. However, when a novelty is found it should be studied. We focus on the detection of samples that are not part of the training distribution and consider sample categorization as a downstream task.

Post-hoc OOD detection.A common and straightforward OOD detection approach is to use a post-hoc strategy, where one employs statistics obtained from a classifier. The perhaps most well known and simplest approach in this class is the Maximum Softmax Probability (MSP; Hendrycks and Gimpel, 2017), where one utilizes \(p(\,y\,)\) of the most likely class \(y\) given a feature vector \(^{D}\) to estimate whether a sample is OOD. Despite good empirical performances, this view is intrinsically limited, since OOD detection should focus on \(p()\)(Motreza and Li, 2022). A wide range of post-hoc OOD detection approaches have been proposed to address the shortcomings of MSP (e.g., Lee et al., 2018; Hendrycks et al., 2019; Liu et al., 2020; Sun et al., 2021, 2022; Wang et al., 2022; Zhang et al., 2023; Djurisic et al., 2023; Liu et al., 2023; Xu et al., 2024). Most related to Hopfield Boosting is the work of Zhang et al. (2023) -- to our knowledge, they are the first to apply the MHE to OOD detection. Specifically, they use the ID data set to produce stored patterns and then use a modified version of MHE as the OOD score. While post-hoc approaches can be deployed out of the box on any model, a crucial limitation is that their performance heavily depends on the employed model itself.

Figure 1: The Hopfield Boosting concept. The first step (weight) creates weak learners by firstly choosing in-distribution samples (ID, orange), and by secondly choosing auxiliary outlier samples (AUX, blue) according to their assigned probabilities; the second step (evaluate) computes the losses for the resulting predictions (Section 3); and the third step (update) assigns new probabilities to the AUX samples according to their position on the hypersphere (see Figure 2).

Training methods.In contrast to post-hoc strategies, training-based methods modify the training process to improve the model's OOD detection capability (e.g., Hendrycks et al., 2019; Tack et al., 2020; Sehwag et al., 2021; Du et al., 2022; Hendrycks et al., 2022; Wei et al., 2022; Ming et al., 2023; Tao et al., 2023; Lu et al., 2024). For example, Self-Supervised Outlier Detection (SSD; Sehwag et al., 2021) leverages contrastive self-supervised learning to train a model for OOD detection.

Auxiliary outlier data and outlier exposure.A third group of OOD detection approaches are outlier exposure (OE) methods. Like Hopfield Boosting, they incorporate AUX data in the training process to improve the detection of OOD samples (e.g., Hendrycks et al., 2019; Liu et al., 2020; Ming et al., 2022; Zhang et al., 2023; Wang et al., 2023; Zhu et al., 2023; Jiang et al., 2024). We provide more detailed discussions on a range of OE methods in Appendix C.1. As far as we know, all OE approaches optimize an objective (\(_{}\)), which aims at improving the model's discriminative power between ID and OOD data using the AUX data set as a stand-in for the OOD case. Hendrycks et al. (2019) were the first to use the term OE to describe a more restrictive OE concept. Since their approach uses the MSP for incorporating the AUX data we refer to it as MSP-OE. Further, we refer to the OE approach introduced in Liu et al. (2020) as EBO-OE (to differentiate it from EBO, their post-hoc approach). In general, OE methods conceptualize the AUX data set as a large and diverse data set (e.g., ImageNet for vision tasks). As a consequence, usually, only a small subset of the samples bear semantic similarity to the ID data set -- most data points are easily distinguishable from the ID data. Recent approaches therefore actively try to find informative samples for the training. The aim is to refine the decision boundary, ensuring the ID data is more tightly encapsulated (e.g., Chen et al., 2021; Ming et al., 2022). For example, Posterior Sampling-based Outlier Mining (POEM; Ming et al., 2022) selects samples close to the decision boundary using Thompson sampling: They first sample a linear decision boundary between ID and AUX data and then select those data instances which are closest to the sampled decision boundary. Hopfield Boosting also makes use of samples close to the boundary by giving them higher weights for the boosting step.

Continuous modern Hopfield networks.MHNs are energy-based associative memory networks. They advance conventional Hopfield networks (Hopfield, 1984) by introducing continuous queries and states with the MHE as a new energy function. MHE leads to exponential storage capacity, while retrieval is possible with a one-step update (Ramsauer et al., 2021). The update rule of MHNs coincides with attention as it is used in the Transformer (Vaswani et al., 2017). Examples for successful applications of MHNs are Widrich et al. (2020); Furst et al. (2022); Sanchez-Fernandez et al. (2022); Paischer et al. (2022); Schafl et al. (2022); Schimunek et al. (2023) and Auer et al. (2023). Section 3.2 gives an introduction to MHE for OOD detection. For further details on MHNs, we refer to Appendix A.

Boosting for classification.Boosting, in particular, AdaBoost (Freund and Schapire, 1995), is an ensemble learning technique for classification. It is designed to focus ensemble members toward data instances that are hard to classify by assigning them higher weights. These challenging instances often lie near the maximum margin hyperplane (Ratsch et al., 2001), akin to support vectors in support vector machines (SVMs; Cortes and Vapnik, 1995). Popular boosting methods include Gradient boosting (Breiman, 1997), LogitBoost (Friedman et al., 2000), and LPBoost (Demiriz et al., 2002).

Radial basis function networks.Radial basis function networks (RBF networks; Moody and Darken, 1989) are function approximators of the form

\[()=_{i=1}^{N}_{i}(-- _{i}||_{2}^{2}}{2_{i}^{2}}),\] (1)

where \(_{i}\) are linear weights, \(_{i}\) are the component means and \(_{i}^{2}\) are the component variances. RBF networks can be described as a weighted linear superposition of \(N\) radial basis functions and have previously been used as hypotheses for boosting (Ratsch et al., 2001). If the linear weights are strictly positive, RBF networks can be viewed as an unnormalized weighted mixture of Gaussian distributions \(p_{i}()=(;_{i},_{i}^{2})\) with \(i=\{1,,N\}\). Appendix H.1 explores the connection between RBF networks and MHNs via Gaussian mixtures in more depth. We refer to Bishop (1995) and Muller et al. (1997) for more general information on RBF networks.

Method

This section presents Hopfield Boosting: First, we formalize the OOD detection task. Second, we give an overview of the MHE and why it is suitable for OOD detection. Finally, we introduce the AUX-based boosting framework. Figure 1 shows a summary of the Hopfield Boosting concept.

### Classification and OOD Detection

Consider a multi-class classification task denoted as \((^{},^{},)\), where \(^{}^{D N}\) represents a set of \(N\)\(D\)-dimensional feature vectors (\(_{1}^{},_{2}^{},,_{N}^{}\)), which are i.i.d. samples \(_{i}^{} p_{}\). \(^{}^{N}\) denotes the labels associated with these feature vectors, and \(\) is a set containing possible classes (\(||||=K\) signifies the number of distinct classes). We consider observations \(^{}^{D}\) that deviate considerably from the data generation \(p_{}(^{})\) that defines the "normality" of our data as OOD. Following Ruff et al. (2021), an observation is OOD if it pertains to the set

\[\ =\ \{^{}^{D}\ |\ p_{}(^{})<\} 0.\] (2)

Since the probability density of the data generation \(p_{}\) is in general not known, one needs to estimate \(p_{}(^{})\). In practice, it is common to define an outlier score \(s()\) that uses an encoder \(\), where \(=(^{})\). The outlier score should -- in the best case -- preserve the density ranking. In contrast to a density estimation, the score \(s()\) does not have to fulfill all requirements of a probability density (like proper normalization or non-negativity). Given \(s()\) and \(\), OOD detection can be formulated as a binary classification task with the classes ID and OOD:

\[(^{},)\ =\ &s((^{ }))\\ &s((^{}))<.\] (3)

It is common to choose the threshold \(\) so that a portion of 95% of ID samples from a previously unseen validation set are correctly classified as ID. However, metrics like the area under the receiver operating characteristic (AUROC) can be directly computed on \(s()\) without specifying \(\) since the AUROC computation sweeps over the threshold.

### Modern Hopfield Energy

The log-sum-exponential (\(\)) function is defined as

\[(,)=\ ^{-1}\ (_{i=1}^{N}( z_{i })),\] (4)

where \(\) is the inverse temperature and \(^{N}\) is a vector. The \(\) can be seen as a soft approximation to the maximum function: As \(\), the \(\) approaches \(_{i}z_{i}\).

Given a set of \(N\)\(d\)-dimensional stored patterns \((_{1},_{2},,_{N})\) arranged in a data matrix \(\), and a \(d\)-dimensional query \(\), the MHE is defined as

\[(;)\ =\ -(,^{T})+\ \ ^{T}\ +\ C,\] (5)

where \(C=^{-1} N\ +M^{2}\) and \(M\) is the largest norm of a pattern: \(M=_{i}||x_{i}||\). \(\) is also called the memory of the MHN. Intuitively, Equation (5) can be explained as follows: The dot-product within the \(\) computes a similarity for a given \(^{d}\) to all patterns in the memory \(^{d N}\). The \(\) function aggregates the similarities to form a single value, where the \(\) parameterizes the aggregation operation: If \(\), the maximum similarity of \(\) to the patterns in \(\) is returned.

To use the MHE for OOD detection, Hopfield Boosting acquires the memory patterns \(\) by feeding raw data instances \((_{1}^{},_{2}^{},,_{N}^{})\) of the ID data set arranged in the data matrix \(^{}^{D N}\) to an encoder \(:^{D}^{d}\) (e.g., ResNet): \(_{i}=(_{i}^{})\). We denote the component-wise application of \(\) to the patterns in \(^{}\) as \(=(^{})\). Similarly, a raw query \(^{}^{D}\) is fed through the encoder to obtain the query pattern: \(=(^{})\). One can now use \((;)\) to estimate whether \(\) is ID or OOD: A low energy indicates \(\) is ID, and a high energy signifies that \(\) is OOD.

### Boosting Framework

Sampling of informative outlier data.Hopfield Boosting uses AUX data to learn a decision boundary between the ID and OOD region during the training. Similar to Chen et al. (2021) and Ming et al. (2022), Hopfield Boosting selects informative outliers close to the ID-OOD decision boundary. For this selection, Hopfield Boosting weights the AUX data similar to AdaBoost (Freund and Schapire, 1995) by sampling data instances close to the decision boundary more frequently. We consider samples close to the decision boundary as weak learners -- their nearest neighbors consist of samples from their own class as well as from the foreign class. An individual weak learner represents a classifier that is only slightly better than random guessing (Figure 6). Vice versa, a strong learner can be created by forming an ensemble of a set of weak learners (Figure 2).

We denote the matrix containing the raw AUX data instances as \(=(^{})\). The boosting process and the memory containing the encoded AUX patterns as \(=(^{})\). The boosting process proceeds as follows: There exists a weight \((w_{1},w_{2},,w_{N})\) for each data point in \(^{}\) and the individual weights are aggregated into the weight vector \(_{t}\). Hopfield Boosting uses these weights to draw mini-batches \(^{}_{s}\) from \(^{}\) for training, where weak learners are sampled more frequently.

We introduce an MHE-based energy function which Hopfield Boosting uses to determine how weak a specific learner \(\) is (with higher energy indicating a weaker learner):

\[_{b}(;,)\ =\ -2\ (,( )^{T})\ +\ (,^{T})\ +\ (,^{T}),\] (6)

where \(^{d N}\) contains ID patterns, \(^{d M}\) contains AUX patterns, and \(()^{d(N+M)}\) denotes the concatenated data matrix containing the patterns from both \(\) and \(\). Before computing \(_{b}\), we normalize the feature vectors in \(\), \(\), and \(\) to unit length. Figure 3 displays the energy landscape of \(_{b}(;,)\) using exemplary data on a 3-dimensional sphere. \(_{b}\) is maximal at the decision boundary between ID and AUX data and decreases with increasing distance from the decision boundary in both directions.

As we show in our theoretical discussion in Appendix G, when modeling the class-conditional densities of the ID and AUX data set as mixtures of Gaussian distributions

\[p(\ \ ) =_{i=1}^{N}(;_{i},^{- 1}),\] (7) \[p(\ \ ) =_{i=1}^{N}(;_{i},^{- 1}),\] (8)

with equal class priors \(p()=p()=1/2\) and normalized patterns \(||_{i}||=1\) and \(||_{i}||=1\), we obtain \(_{b}(;,)}{{=}}^{-1} (p(\ ) p(\ \ ))\), where \(}{{=}}\) denotes equality up to an irrelevant additive constant. The exponential of \(_{b}\) is the variance of a Bernoulli random variable with the outcomes \(\{,\}\) conditioned on \(\). Thus, according to \(_{b}\), the weak learners are situated at locations where the model defined in Equations (7) and (8) is uncertain.

Given a set of query values \((_{1},_{2},,_{n})\) assembled in a query matrix \(^{d n}\), we denote a vector of energies \(^{n}\) with \(e_{i}=_{b}(_{i};,)\) as

\[=_{b}(;,).\] (9)

To calculate the weights \(_{t+1}\), we use the memory of AUX patterns as a query matrix \(=\) and compute the respective energies \(_{b}\) of those patterns. The resulting energy vector \(_{b}(;,)\) is then normalized by a \(\). This computation provides the updated weights:

\[_{t+1}=(_{b}(;,)).\] (10)

Appendix J provides theoretical background on how informative samples close to the decision boundary are beneficial for training an OOD detector.

Training the model with MHE.In this section, we introduce how Hopfield Boosting uses the sampled weak learners to improve the detection of patterns outside the training distribution. We follow the established training method for OE (Hendrycks et al., 2019, Liu et al., 2020, Ming et al., 2022): Train a classifier on the ID data using the standard cross-entropy loss and add an OOD loss that uses the AUX data set to sharpen the decision boundary between the ID and OOD regions. Formally, this yields the loss

\[=_{}+_{},\] (11)

where \(\) is a hyperparamter indicating the relative importance of \(_{}\). Hopfield Boosting explicitly minimizes \(_{b}\) (which is also the energy function Hopfield Boosting uses to sample weak learners). Given the weight vector \(_{t}\), and the data sets \(^{}\) and \(^{}\), we obtain a mini-batch \(^{}_{s}\) containing N samples from \(^{}\) by uniform sampling, and a mini-batch of N weak learners \(^{}_{s}\) from \(^{}\) by sampling according to \(_{t}\) with replacement. We then feed the respective mini-batches into the neural network \(_{}\) to create a latent feature (in our experiments, we always use the feature of the penultimate layer of a ResNet). Our proposed approach then uses two heads:

1. A linear classification head that maps the latent feature to the class logits for \(_{}\).
2. A 2-layer MLP \(_{}\) maps the features from the penultimate layer to the output for \(_{}\).

Hopfield Boosting computes \(_{}\) on the representations it obtains from \(=_{}_{}\) as follows:

\[_{}=_{}_{b}(; {X}_{s},_{s}),\] (12)

where the memories \(_{s}\) and \(_{s}\) contain the encodings of the sampled data instances: \(_{s}=(^{}_{s})\) and \(_{s}=(^{}_{s})\). The sum is taken over the observations \(\), which are drawn from \((_{s}_{s})\). Hopfield Boosting computes \(_{}\) for each mini-batch by first calculating the pairwise similarity matrix between the patterns in the mini-batch, followed by determining the \(_{b}\) values of the individual observations \(\), and, finally a mean reduction. To the best of our knowledge, Hopfield Boosting is the first method that uses Hopfield networks in this way to train a deep neural network. We note that there is a relation between Hopfield Boosting and SVMs with an RBF kernel (see Appendix H.3). However, the optimization procedure of SVMs is in general not differentiable. In contrast, our novel energy function is fully differentiable. This allows us to use it to train neural networks.

Summary.Algorithm 1 provides an outline of Hopfield Boosting. Each iteration \(t\) consists of three main steps: 1. weight, 2. evaluate, and 3. update. First, Hopfield Boosting samples a mini-batch from the ID data and **weights** the AUX data by sampling a mini-batch according to \(_{t}\). Second, Hopfield Boosting **evaluates** the composite loss on the sampled mini-batch. Third, Hopfield Boosting **updates** the model parameters and, every \(N\)-th step, also the sampling weights for the AUX data set \(_{t+1}\).

Inference.At inference time, the OOD score \(s()\) is

\[s()\ =\ }(,^{T})\ -\ }(,^{T}).\] (13)

Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows \(}(_{b}(;,))\), where \(\) is \(60\). Only the sampled (i.e., highlighted) points serve as memories \(\) and \(\).

For computing \(s()\), Hopfield Boosting uses the 50,000 random samples from the ID and AUX data sets, respectively. As we show in Appendix 1.8, this step entails only a very moderate computational overhead in relation to a complete forward pass (e.g., an overhead of 7.5% for ResNet-18 on an NVIDIA Titan V GPU with 50,000 patterns stored in each of the memories \(\) and \(\)). We additionally experimented with using only \((,^{T})\) as a score, which also gives reasonable results. However, the approach in Equation (13) has turned out to be superior. Equation (13) uses information from both ID and AUX samples. This can, for example, be beneficial for handling query patterns \(\) that are dissimilar from both the memory patterns in \(\) as well as from the memory patterns in \(\).

### Comparison of Hopfield Boosting to HE and SHE

Zhang et al. (2023) propose two post-hoc methods for OOD detection with MHE:"Hopfield Energy" (HE) and "Simplified Hopfield Energy" (SHE). In contrast to Hopfield Boosting, HE and SHE do not use AUX data to get a better boundary between ID and OOD data. Rather, their methods evaluate the MHE on ID patterns only to determine whether a sample is ID or OOD. Additional differences include the selection of patterns stored in the memory or the normalization of the patterns. The OE process of Hopfield Boosting drastically improves the OOD detection performance compared to HE and SHE. We verify that the unique contributions of Hopfield Boosting (like the energy-based loss and the boosting process) are responsible for the superior performance with two extensions of HE that include AUX data (the comparison can be found in Appendix 1.9). For further information on the differences to HE and SHE, we refer to Appendix H.4.

## 4 Experiments

### Toy Example

This section presents a toy example illustrating the main intuitions behind Hopfield Boosting. For the sake of clarity, the toy example does not consider the inlier classification task that would induce secondary processes, which would obscure the explanations. Formally, we do not consider the first term on the right-hand side of Equation (11). For further toy examples, we refer to Appendix F.

Figure 2 demonstrates how the weighting in Hopfield Boosting allows good estimations of the decision boundary, even if Hopfield Boosting only samples a small number of weak learners. This is advantageous because the AUX data set contains a large number of data instances that are uninformative for the OOD detection task. For small, low dimensional data, one can always use all the data to compute \(_{b}\) (Figure 2, a). For large problems (like in Ming et al., 2022), this strategy is difficult, and the naive solution of uniformly sampling N data points would also not work. This will yield many uninformative points (Figure 2, b). When using Hopfield Boosting and sampling N weak learners according to \(_{t}\), the result better approximates the decision boundary of the full data (Figure 2, c).

### Data & Setup

**CIFAR-10 & CIFAR-100.** Our training and evaluation proceeds as follows: We train Hopfield Boosting with ResNet-18 (He et al., 2016) on the CIFAR-10 and CIFAR-100 data sets (Krizhevsky, 2009), respectively. In these settings, we use ImageNet-RC (Chrabaszcz et al., 2017) (a low-resolution version of ImageNet) as the AUX data set. For testing the OOD detection performance, we use the data sets SVHN (Street View House Numbers) (Netzer et al., 2011), Textures (Cimpoi et al., 2014), iSUN (Xu et al., 2015), Places 365 (Lopez-Cifuentes et al., 2020), and two versions of the LSUN data set (Yu et al., 2015) -- one where the images are cropped, and one where they are resized to match the resolution of the CIFAR data sets (32x32 pixels). We refer to the two LSUN data sets as LSUN-Crop and LSUN-Resize, respectively. We compute the scores \(s()\) as described in Equation (13) and then evaluate the discriminative power of \(s()\) between CIFAR and the respective OOD data set using the FPR95 and the AUROC. We use a validation process with different OOD data for model selection. Specifically, we validate the model on MNIST (LeCun et al., 1998), and ImageNet-RC with different pre-processing than in training (resize to 32x32 pixels instead of crop to 32x32 pixels), as well as Gaussian and uniform noise.

ImageNet-1K.We evaluate Hopfield Boosting on the large-scale benchmark: We use ImageNet-1K (Russakovsky et al., 2015) as ID data set and ImageNet-21K (Ridnik et al., 2021) as AUX data set. The OOD test data sets are Textures (Cimpoi et al., 2014), SUN (Xu et al., 2015), Places 365 (Lopez-Cifuentes et al., 2020), and iNaturalist (Van Horn et al., 2018). In this setting, all images are scaled to a resolution of 224x224. To keep our method comparable to other OE methods, we closely follow the training and evaluation protocol of (Zhu et al., 2023). This implies that we fine-tune a ResNet-50 that was pre-trained on the ImageNet-1K ID classification task (as provided by TorchVision, 2016).

  & Metric & HB (ours) & DOS & DOE & DOE & DOE & DOE & Dall & Mixo & POOM & EBO-OE & MSP-OE \\    & FPR95\(\) & **32.91\(\)0.05** & **30.99\(\)0.05** & **1.37\(\)0.05** & **32.91\(\)0.05** & **72.91\(\)0.05** & **72.91\(\)0.05** & **72.91\(\)0.05** & **72.91\(\)0.05** & **72.91\(\)0.05** \\  & AUROC\(\) & **9.97\(\)0.05** & **30.99\(\)0.05** & **1.30\(\)0.05** & **9.60\(\)0.05** & **9.61\(\)0.05** & **9.37\(\)0.05** & **9.33\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.92\(\)0.05** \\  & FPR95\(\) & \(0.82\)1.51 & 3.66\(\)0.05 & 3.22\(\)0.05 & 1.85\(\)0.05 & **1.71\(\)0.05** & **1.41\(\)0.05** & **1.42\(\)0.05** & **1.52\(\)0.05** & **9.62\(\)0.05** & **7.02\(\)0.05** \\  & AUROC\(\) & **9.90\(\)0.05** & **30.99\(\)0.05** & **30.99\(\)0.05** & **9.90\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **8.33\(\)0.05** \\    & FPR95\(\) & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** \\   & FPR95\(\) & **0.16\(\)0.05** & **1.25\(\)0.02** & **2.75\(\)0.02** & **1.29\(\)0.05** & **1.29\(\)0.05** & **1.35\(\)0.05** & **1.48\(\)0.02** & **0.49\(\)0.05** & **1.11\(\)1.15** & **2.29\(\)0.05** \\   & AUROC\(\) & **9.94\(\)0.05** & **9.93\(\)0.05** & **9.93\(\)0.05** & **9.93\(\)0.05** & **9.94\(\)0.05** & **9.93\(\)0.05** & **9.94\(\)0.05** & **9.92\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** & **9.91\(\)0.05** \\    & FPR95\(\) & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** \\   & AUROC\(\) & **9.997\(\)0.05** & **9.999\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** & **0.90\(\)0.05** \\    & FPR95\(\) & **4.28\(\)0.03** & **2.12\(\)0.05** & **1.79\(\)0.05** & **1.27\(\)0.05** & **1.24\(\)0.05** & **1.24\(\)0.05** & **1.60\(\)1.06** & **7.70\(\)0.05** & **1.17\(\)0.05** & **21.24\(\)0.05** & **21.24\(\)0.05** \\   & AUROC\(\) & **9.85\(\)0.10** & **9.66\(\)0.03** & **9.50\(\)0.02** & **9.65\(\)0.02** & **9.55\(\)0.00** & **9.67\(\)0.07** & **9.62\(\)0.02** & **9.75\(\)0.02** & **9.56\(\)0.02** & **9.36\(\)0.03** & **9.51\(\)0.07** \\    & FPR95\(\) & **0.92** & 3.38 & 4.61 & 3.83 & 3.33 & 8.17 & 2.28 & 3.73 & 5.84 \\   & AUROC\(\) & **9.55\(\)0.0** & **9.55** & 9.07 & 9.88 & 9.96 & 9.18 & 9.58 & 9.21 & 9.92 & 9.89 & 9.50 \\   

Table 1: OOD detection performance on CIFAR-10. We compare results from Hopfield Boosting, DOS (Jiang et al., 2024), DOE (Wang etBaselines.As mentioned earlier, previous works offer vast experimental evidence that OE methods offer superior OOD detection compared to methods without OE (see e.g., Ming et al., 2022; Wang et al., 2023a). Our experiments in Appendix 1.14 confirm this. Thus, we focus on a comprehensive comparison of Hopfield Boosting to eight OE methods: MSP-OE (Hendrycks et al., 2019), EBO-OE (Liu et al., 2020), POEM (Ming et al., 2022), MixOE (Zhang et al., 2023), DAL (Wang et al., 2023a), DivOE (Zhu et al., 2023), DOE (Wang et al., 2023b) and DOS (Jiang et al., 2024).

Training setup.The network trains for 100 epochs (CIFAR-10/100) or 4 epochs (ImageNet-1K), respectively. In each epoch, the model processes the entire ID data set and a selection of AUX samples (sampled according to \(_{t}\)). We sample mini-batches of size 128 per data set, resulting in a combined batch size of 256. We evaluate the composite loss from Equation (11) for each resulting mini-batch and update the model accordingly. After an epoch, we update the sample weights, yielding \(_{t+1}\). For efficiency reasons, we only compute the weights for 500,000 AUX data instances (\(\)40% of ImageNet), which we denote as \(\). The weights of the remaining samples are set to 0. During the sample weight update, Hopfield Boosting does not compute gradients or update model parameters. The update of the sample weights \(_{t+1}\) proceeds as follows: First, we fill the memories \(\) and \(\) with 50,000 ID samples and 50,000 AUX samples, respectively. Second, we use the obtained \(\) and \(\) to get the energy \(_{b}(;,)\) for each of the 500,000 AUX samples in \(\) and compute \(_{t+1}\) according to Equation (10). In the following epoch, Hopfield Boosting samples the mini-batches \(_{s}^{}\) according to \(_{t+1}\) with replacement. To allow the storage of even more patterns in the Hopfield memory during the weight update process, one could incorporate a vector similarity engine (e.g., Douze et al., 2024) into the process. This would potentially allow a less noisy estimate of the sample weights. For the sake of simplicity, we did not opt to do this in our implementation of Hopfield Boosting. As we show in section 4.3, Hopfield Boosting achieves state-of-the-art OOD detection results and can scale to large datasets (ImageNet-1K) even without access to a similarity engine.

Hyperparameters & Model Selection.Like Yang et al. (2022), we use SGD with an initial learning rate of \(0.1\) and a weight decay of \(5 10^{-4}\). We decrease the learning rate during the training process with a cosine schedule (Loshchilov and Hutter, 2016). Appendix 1.2 describes the image transformations and pre-processing. We apply optimizer, weight decay, learning rate, scheduler, and transformations consistently to all OOD detection methods of the comparison. For training Hopfield Boosting, we use a single value for \(\) throughout the training and evaluation process and for all OOD data sets. For model selection, we use a grid search with \(\), chosen from the set \(\{0.1,0.25,0.5,1.0\}\), and \(\), chosen from the set \(\{2,4,8,16,32\}\). From these hyperparameter configurations, we select the model with the lowest mean FPR95 metric (where the mean is taken over the validation OOD data sets) and do not consider the ID classification accuracy for model selection. In our experiments, \(=4\) and \(=0.5\) yields the best results for CIFAR-10 and CIFAR-100. For ImageNet-1K, we set \(=32\) and \(=0.25\).

  & \)} & \)} & \)} \\ Projection Head & & & & & \\ \(_{}\) & & & & & \\   & **FPR95\(\)** & \(}}\)** & \(0.70^{}\)** & \(1.01^{}\)** & \(2.02^{}\)** & \(5.65^{}\)** \\  & **AIROC** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(50.99^{}\)** \\   & **FPR95\(\)** & \(0.28^{}\)** & \(1.55^{}\)** & \(2.22^{}\)** & \(3.26^{}\)** & \(5.65^{}\)** \\  & **AIROC** & \(}}\)** & \(0.92^{}\)** & \(9.28^{}\)** & \(5.28^{}\)** & \(5.40^{}\)** \\  & **FPR95\(\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** \\   & **AIROC** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(89.15^{}\)** \\   & **FPR95\(\)** & \(}}\)** & \(0.26^{}\)** & \(0.38^{}\)** & \(0.38^{}\)** & \(49.36^{}\)** \\  & **AIROC** & \(}}\)** & \(\)** & \(}}\)** & \(}}\)** & \(89.15^{}\)** \\   & **FPR95\(\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** \\  & **AIROC** & \(}}\)** & \(}}\)** & \(}}\)** & \(}}\)** & \(88.91^{}\)** \\   & **FPR95\(\)** & \(4.28^{}\)** & \(0.11^{}\)** & \(2.00^{}\)** & \(5.73^{}\)** & \(17.44^{}\)** & \(17.31^{}\)** \\  & **AIROC** & \(}}\)** & \(97.68^{}\)** & \(30.21^{}\)** & \(94.77^{}\)** & \(78.30^{}\)** \\   & **FPR95\(\)** & \(\)** & \(1.46\) & \(2.06\) & \(50.40\) \\  & **AIROC** & \(}}\)** & \(99.38\) & \(96.65\) & \(88.50\) \\  

Table 3: Ablated training procedures on CIFAR-10. We compare the result of Hopfield Boosting to the results of our method when not using weighted sampling, the projection head, or the OOD loss. \(\) indicates “lower is better” and \(\) “higher is better”. All values in \(\%\). Standard deviations are estimated across five training runs.

### Results & Discussion

Table 1 summarizes the results for CIFAR-10. Hopfield Boosting achieves equal or better performance compared to the other methods regarding the FPR95 metric for all OOD data sets. It surpasses POEM (the previously best OOD detection approach with OE in our comparison), improving the mean FPR95 metric from 2.28 to 0.92. On CIFAR-100 (Appendix I.1), Hopfield Boosting improves the mean FPR95 metric from 11.76 to 7.94. It is notable that all methods achieve perfect FPR95 results on the LSUN-Resize and iSUN data sets. This is somewhat problematic since there exists evidence that the LSUN-Resize data set can give misleading results due to image artifacts resulting from the resizing procedure (Tack et al., 2020; Yang et al., 2022). We hypothesize that a similar issue exists with the iSUN data set, as in our experiments, LSUN-Resize and iSUN behave very similarly.

On ImageNet-1K (Table 2), Hopfield Boosting surpasses all methods in our comparison in terms of both mean FPR95 and mean AUROC. Compared to POEM (the previously best method) Hopfield Boosting improves the mean FPR95 from 50.74 to 36.60. This demonstrates that Hopfield Boosting scales very favourably to large-scale settings.

We observe that all methods tested perform worst on the Places 365 data set. To gain more insights regarding this behavior, we look at the data instances from the Places 365 data set that Hopfield Boosting trained on CIFAR-10 most confidently classifies as in-distribution (i.e., which receive the highest scores \(s()\)). Visual inspection shows that among those images, a large portion contains objects from semantic classes included in CIFAR-10 (e.g., airplanes, horses, automobiles). We refer to Appendix I.6 for more details.

We evaluate the performance of the following 3 ablated training procedures on the CIFAR-10 benchmark to gauge the importance of the individual contributions of Hopfield Boosting: (a) Random sampling instead of weighted sampling, (b) Random sampling instead of weighted sampling and no projection head, (c) No application of \(_{}\). The results (Table 3) show that all contributions (i.e, weighted sampling, the projection head, and \(_{}\)) are important factors for Hopfield Boosting's performance. For additional ablations, we refer to Appendix I.

When subjecting Hopfield Boosting to data sets that were designed to show the weakness of OOD detection approaches (Appendix I.7), we identify instances where a substantial number of outliers are wrongly classified as inliers. Testing with EBO-OE yields comparable outcomes, indicating that this phenomenon extends beyond Hopfield Boosting.

## 5 Limitations

Lastly, we would like to discuss two limitations that we found: First, we see an opportunity to improve the evaluation procedure for OOD detection. Specifically, it remains unclear how reliably the performance on specific data sets can indicate the general ability to detect OOD inputs. Our results from iSUN and LSUN-Resize (Section 4.3) indicate that issues like image artifacts in data sets greatly influence model evaluation. Second, although OE-based approaches improve the OOD detection capability, their reliance on AUX data can limit their applicability. For one, the selection of the AUX data is crucial (since it determines the characteristics of the decision boundary surrounding the inlier data). Furthermore, the use of AUX data can be prohibitive in domains where only a few or no outliers at all are available for training the model.

## 6 Conclusions

We introduce Hopfield Boosting: an approach for OOD detection with OE. Hopfield Boosting uses an energy term to _boost_ a classifier between inlier and outlier data by sampling weak learners that are close to the decision boundary. We illustrate how Hopfield Boosting shapes the energy surface to form a decision boundary. Additionally, we demonstrate how the boosting mechanism creates a sharper decision boundary than with random sampling. We compare Hopfield Boosting to eight modern OOD detection approaches using OE. Overall, Hopfield Boosting shows the best results.