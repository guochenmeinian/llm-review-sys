# Performance Bounds for Policy-Based Average

Reward Reinforcement Learning Algorithms

 Yashaswini Murthy

Electrical and Computer Engineering

University of Illinois Urbana-Champaign

Urbana, IL 61801

ymurthy2@illinois.edu

&Mehrdad Moharrami

Electrical and Computer Engineering

University of Illinois Urbana-Champaign

Urbana, IL 61801

moharami@illinois.edu

&R. Srikant

Electrical and Computer Engineering

University of Illinois Urbana-Champaign

Urbana, IL 61801

rsrikant@illinois.edu

###### Abstract

Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, discounted reward formulations are often used with the discount factor being close to \(1\), which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first finite-time error bounds for average-reward MDPs, and show that the asymptotic error goes to zero in the limit as policy evaluation and policy improvement errors go to zero.

## 1 Introduction

Reinforcement Learning algorithms can be broadly classified into value-based methods and policy-based methods. In the case of discounted-reward Markov Decision Processes (MDPs), value-based methods such as Q-learning , fitted value iteration  and target network Q-learning  can be viewed as approximately solving the fixed point of the Bellman optimality equation to find the value function and an approximately optimal control policy. In other words value iteration is approximately implemented for discounted reward MDPs . In policy-gradient methods , a gradient step is used to improve the policy, somewhat similar to the policy improvement step in policy iteration for discounted-reward MDPs. In a recent paper , it has been shown that many policy-based methods for discounted-reward MDPs can be viewed as special cases of approximate policy iteration. The classical results on approximate policy iteration  assume that the error in the policy evaluation and improvement steps are constant, independent of the iteration. The key idea in  is to show that a simple modification of the proof in  can be used to allow for iteration-dependent policy evaluation and improvement errors, which is then used to make the connection to policy-based methods. Ourgoal in this paper is to derive similar results for average-reward problems. Average-reward problems  are, in some sense, harder to study than their discounted-reward counterparts. We now discuss why this is so by recalling the error bounds for discounted-reward problems and examining them in an appropriate limit to study their applicability to average-reward problems.

The fundamental result on approximate policy iteration for discounted reward MDPs which allows the connection to policy-based methods is the following :

\[_{k}\|J_{_{k}}-J_{*}^{}\|_{} H_{}^{2}( +2),\] (1)

where \(\) is the discount factor, \(H_{}=1/(1-),\)\(J_{*}^{}\) is the optimal value function, \(J_{_{k}}\) is the value function associated with the policy obtained after \(k\) iterations of approximate policy iteration, \(\) is the policy improvement error, and \(\) is the policy evaluation error. Thus, as \(, 0,\) we recover the result that standard policy iteration converges. On the other hand, to understand whether the above bound is useful to study average-reward problems, we write Equation (1) as

\[_{k}}\|J_{_{k}}-J_{*}^{}\|_{ } H_{}(+2).\]

Under mild conditions , it is well known that each element of the value function vector approaches the average-reward for each fixed policy and for the optimal policy, in the limit \( 1\). Hence, the left-hand side of the above equation is an approximation to the error in approximate policy iteration for average-reward MDPs; the right-hand side which gives an upper bound on this error blows up to infinity, i.e., when \( 1\). Thus, unlike in the discounted-reward case, the above bound fails to even recover the well-known convergence of standard policy iteration in average-reward case in the limit \(, 0\) (since we have to let \( 1\) before letting \(, 0\) ). However, as mentioned in , it is also well known that approximate policy iteration performs much better than the bound suggests. The main goal of our paper is to resolve this discrepancy between theory and practice.

### Contributions

It is well known that the error bound for discounted-reward, approximate policy iteration is tight for unichain MDPs . Thus, it is impossible to improve upon the bound in general. However, as will be explained in a later section, it is easy to convert most reasonable unichain MDPs to MDPs where every stationary policy results in an irreducible Markov chain with an arbitrarily small loss of reward. So the natural question to ask is whether the bound can be dramatically improved for MDPs where every stationary policy results in an irreducible Markov chain. To the best of our knowledge, no such bound was available in the prior literature.

Our main contributions are as follows:

* Under the assumption that every stationary policy induces an irreducible Markov chain, we first perform a Schweitzer transformation of the MDP  and obtain finite-time error bounds for average-reward approximate policy iteration.
* Using the above finite-time error bounds, we prove an error bound of the form \[_{k}J^{*}-J_{_{k}} f(,),\] for average-reward approximate policy iteration for a function \(f\) such that \(f(,) 0\) in the limit as \(, 0.\) Note that this is in sharp contrast to the result that one can obtain from the discounted-reward analysis where the error bound blows up to infinity when applied to discount factors approaching \(1,\) which is the appropriate limit to convert discounted-reward problems to average-reward problems.
* The main difficulty in obtaining the above bound when compared to the discounted-reward case is the lack of infinity-norm contraction property for the Bellman operator in the average-reward case. In most analysis of average-reward problems, this difficulty is circumvented by the use of a span-contraction property . However, the span contraction property is insufficient for our purpose and therefore, we use a technique due to  for the study of a different algorithm called modified policy iteration in .

* Next, we extend the above analysis to obtain finite-iteration error bounds for the case where the policy evaluation and improvement errors can potentially vary from iteration to iteration. Further, we allow these errors to be random (as would be the case, for example, when one uses TD learning for policy evaluation and soft policy improvement) and obtain expected error bounds on approximate policy iteration after a finite number of iterations.
* Our error bounds are in a form such that one can then apply them to many policy-based RL algorithms, i.e., we can simply plug in error bounds for specific policy evaluation or policy improvement algorithms. We illustrate the applicability of the results to several different policy-based RL methods (softmax update, greedy update and mirror descent update) studied in the literature .
* While the main focus of our paper is on offline learning, we provide connections to regret bounds for online learning in average-reward MDPS as in .

### Related Work

Our work in this paper is most closely related to the work in . The main contribution there was to recognize that an extension of the approximate policy iteration results in  can be used to derive finite-time error bounds for many RL-based algorithms. The idea of considering iteration-dependent policy evaluation and improvement bounds was also used to study policy iteration with function approximation in discounted-reward MDPs in . Our contribution here is to derive the first error bound for approximate policy iteration for average-reward MDPs and then write it in a form which can be applied to policy-based RL methods for average-reward TD learning. To the best of our knowledge, no known error bounds existed in prior literature for average-reward approximate policy iteration due to the fact that the corresponding bounds for discounted MDPs increase proportional to the square of length of the horizon. Thus, in the limit as the horizon increases to infinity, we have essentially reduced the dependence of the error from the square of the length of the horizon to linear in the length of the horizon.

RL algorithms have not been studied as extensively for the average-reward case as they are for discounted-reward MDPs. There is recent work  on average-reward TD-learning which we leverage to illustrate the applicability of our results. There are also asymptotic convergence results for policy gradient and actor-critic methods for average-reward MDPs . However, these results only prove convergence to a stationary point and there are no global convergence results. The original paper on natural policy gradient (NPG) is written for average-reward MDPs, but the extensive performance analysis of NPG in subsequent work such as  seems to only consider the discounted-reward case. Recent work  considers mirror descent in average reward RL but do not provide performance results for other RL algorithms. In a later section, we compare our results to  in the special case of mirror descent-type policy updates

## 2 Model and Preliminaries

### Average Reward Formulation

We consider the class of infinite horizon MDPs with finite state space \(\), finite action space \(\), and transition kernel \(\), where \(||=n\) and \(||=m\). Let \(\) denote the probability simplex over actions. We consider the class of randomized policies \(=\{:\}\), so that a policy \(\) assigns a probability vector over actions to each state. Given a policy \(\), the transition kernel for the underlying Markov process is denoted by \(_{}:\), where \(_{}(s^{}|s):=_{a}(a|s)(s^{ }|s,a)\) is the probability of moving to state \(s^{}\) from \(s\) upon taking action \((s)\). Associated with each state-action pair \((s,a)\), is a one-step reward which is denoted by \(r_{}(s):=_{a(s)}r(s,a)\).

Let \(J_{}\) be the average reward associated with the policy \(\), i.e. \(J_{}\) is defined as:

\[J_{}=_{T}_{}(_{i=0}^{T-1} r_{}(s_{i}))}{T}.\]

Here the expectation is taken with respect to the measure \(_{}\) associated with the policy \(\). Let \(h_{}^{n}\) be the relative value function associated with the policy \(\). Defining \(^{n}\) to be the vector of all ones, the pair \((J_{},h_{})\) satisfies the following average reward Bellman equation:

\[J_{}+h_{}=r_{}+_{}h_{}.\] (2)

Let \(_{}^{n}\) denote the stationary distribution associated with the kernel \(_{}\). Let \(_{}^{*}=_{}^{}^{n n}\) denote the matrix whose rows are \(_{}^{}\). Using \(_{}^{*}=_{}^{*}_{}\), we get the following characterization of the average reward by multiplying both sides of Equation (2) with \(_{}^{*}\): \(J_{}=_{}^{*}r_{}\).

Let \(J^{*}:=_{}J_{}\) be the optimal average reward. From standard MDP theory, there exists \(h^{*}^{n}\), for which the pair (\(J^{*}\),\(h^{*}\)) satisfies the following Bellman optimality equation:

\[J^{*}+h^{*}=_{}r_{}+_{}h^{*}.\] (3)

Let \(^{*}\) be the optimizing policy in Equation (3). Then, similar reasoning shows that \(J^{*}=_{^{*}}^{*}r_{^{*}}\).

The goal of dynamic programming and reinforcement learning is to determine this optimal average reward \(J^{*}\) and its corresponding optimal policy \(^{*}\). Policy iteration is one of the most widely used dynamic programming algorithms for this purpose. It consists of two steps: (i) evaluation of the value function associated with a policy, and (ii) determining a greedy policy with respect to this evaluation. There are primarily two challenges that arise when applying such an algorithm: (i) high memory and time complexity when the state space is too large, and (ii) the unknown transition probability kernel that governs the dynamics of the MDP.

When the state space is too large, it may be computationally infeasible to perform policy iteration exactly. Existing literature suggests that approximate policy iteration in the context of discounted reward MDPs yields good performance. However, such an algorithm has not been studied in the context of average reward MDPs. In fact, the results obtained for the discounted reward MDPs yield vacuous performance bounds for average reward MDPs given the standard relationship between discounted reward and average reward MDPs. We explore this further in Section 2.2.

### Relationship to the Discounted Reward MDP

To provide some background, we first discuss the approximate policy iteration algorithm in the context of discounted reward MDPs. Let \([0,1)\) denote the discount factor. The discounted reward associated with a deterministic policy \(\) starting from some state \(s\) is denoted \(J_{}^{}(s)\) and is defined as:

\[J_{}^{}(s)=_{}[_{i=0}^{}^{i}r(s_{i },(s_{i}))s_{0}=s].\]

The value function \(J_{}^{}^{n}\) is the unique fixed point of the Bellman operator \(_{}^{}:^{n}^{n}\) associated with the policy \(\), which is defined as \(_{}^{}J=r_{}+_{}J\). For each state \(s\), let \(J_{s}^{}(s)\) denote the optimal discounted reward, i.e., \(J_{s}^{}(s)=_{}J_{}^{}(s)\). Similarly, \(J_{s}^{}^{n}\) is the unique fixed point of the optimality Bellman operator \(^{}:^{n}^{n}\), which is defined as \(^{}J=_{}(r_{}+_{}J)\). Algorithm 1 is Approximate Policy Iteration for the discounted reward MDP:

``` Require \(J_{0}^{n}\) for\(k=0,1,2,\)do  1. Compute \(_{k+1}\!\!\) such that \(\|^{}J_{k}-_{_{k+1}}^{}\!J_{k}\|_{} \!\!\ \) Approximate Policy Improvement  2. Choose \(J_{k+1}\) such that \(\|J_{k+1}-J_{_{k+1}}\|_{}\)\(\) Approximate Policy Evaluation  where \(J_{_{k+1}}=_{_{k+1}}^{}J_{_{k+1}}\) endfor ```

**Algorithm 1** Approximate Policy Iteration: Discounted Reward

**Theorem 2.1**.: _Let \(_{k}\) be the sequence of policies generated from the approximate policy iteration algorithm (Algorithm 1). Then the performance error is bounded as:_

\[_{k}\|J_{_{k}}-J_{*}^{}\|_{}}.\] (4)

Proof.: The proof of this theorem can be found in From literature , we know that the average reward \(J_{}\) associated with any policy \(\) is related to its discounted reward \(J_{}^{}(s)\) counterpart as follows:

\[J_{}=_{ 1}(1-)J_{}^{}(s).\]

Note that the above relation is independent of the state \(s\), as the average reward does not depend on the initial state. Multiplying Equation (4) with \((1-)\), and letting \( 1\), still yields an approximation performance bound with \((1-)\) in the denominator. This term blows up to infinity as \( 1\). Note that this bound is known to be tight under unichain Markov structure of the probability transition kernel . However, in practice it is observed that approximate policy iteration works well in the average reward MDP case although these theoretical bounds are not representative of this performance. To the best of our knowledge, we are unaware of an average reward approximate policy iteration performance bound, when the policies induce an irreducible Markov chain. We bridge this gap between theory and practice by providing a theoretical analysis of approximate policy iteration in the average reward MDP setting, with non trivial performance bounds.

## 3 Approximate Policy Iteration for Average Reward

A crucial component of the proof of convergence of approximate policy iteration in the context of discounted reward MDPs is the contraction due to the discount factor \(\) which is absent in the average reward setting (since \(=1\)). To get some source of contraction, we have to make some assumptions on the MDP.

**Assumption 3.1**.: We make the following assumptions:

1. Every deterministic policy \(\) induces an irreducible Markov Chain \(_{}\).
2. For all policies, the diagonal elements of the probability transition matrix are positive, i.e., the Markov chain stays in the same state with non-zero probability.

Assumption (a) need not be satisfied by all MDPs. However, in order to satisfy this assumption, we consider a modified MDP where at every time step with probability \(\), an action is chosen from the set of all possible actions with equal probability. Simultaneously, with probability \(1-\), we choose an action dictated by some policy. The problem then is to choose this policy optimally. For most MDPs of interest, this small modification will satisfy our assumption with a small \(O()\) loss in performance. It is straightforward to show the \(O()\) loss, but we include the proof in the appendix for completeness. Assumption (b) is without loss of generality in the following sense: there exists a simple transformation which essentially leaves the MDP unchanged but ensures that this assumption is satisfied. This transformation known as the aperiodicity transformation or Schweitzer transformation was introduced in . For more details, the reader is referred to the appendix. In the remainder of this paper, we assume that all MDPs are transformed accordingly to ensure the assumptions are satisfied. One consequence of Assumption (b) is the following lemma which will be useful to us later.

**Lemma 3.2**.: _There exists a \(>0\) such that, for any policy \(\),_

\[_{i}_{}(i),\]

_where \(_{}\) is stationary distribution over states associated with the policy \(\)._

Proof.: This lemma has been proved in a more general sense in the context of deterministic policies in . Since the aperiodicity transformation ensures a non-zero probability of staying in the same state, the expected return times are bounded and the lemma holds true for randomized policies as well. 

Prior to presenting the algorithm, consider the following definitions. The average-reward Bellman operator \(_{}:^{n}^{n}\) corresponding to a policy \(\) is defined as \(_{}h=r_{}+_{}h\). The average-reward optimal Bellman operator \(:^{n}^{n}\) is defined as \(h=_{}r_{}+_{}h\). The value function \(h_{}\) associated with a policy \(\) satisfies the Bellman Equation (2) with single step reward \(r_{}\) and the transition kernel \(_{}\). Note that \(h_{}\) is unique up to an additive constant.

Solving for \((J_{},h_{})\) using Equation (2) involves setting the value function for some fixed state \(x^{*}\) as \(0\) (since this reduces the system from being underdetermined to one with a unique solution). Further, the value function \(h_{}\) can be alternatively expressed as the \(_{m}_{}^{m}h_{0}\) for any \(h_{0}^{n}\) as is the case with discounted reward MDPs. However, unlike the discounted reward Bellman Equation, there is no discount factor \(<1\) preventing the value function from exploding to infinity. Hence, we consider value function computed using the relative Bellman operator \(}_{}\) defined as

\[}_{}h=r_{}+_{}h-r_{}(x^{*})-(_{}h)(x^{*}).\]

We now state the algorithm for Approximate Policy Iteration for Average Reward MDPs.

### Approximate Policy Iteration Algorithm for Average Reward MDPs

```
1:Require \(h_{0}^{n}\)
2:for\(k=0,1,2,\)do
3: 1. Compute \(_{k+1}\!\!\) such that \(\|h_{k}\!-\!_{_{k+1}}h_{k}\|_{}\)\(\) Approx. Policy Improvement
4: 2. Compute \(h_{k+1}\) such that \(\|h_{k+1}-h_{_{k+1}}\|_{}\)\(\) Approx. Policy Evaluation
5: where \(h_{_{k+1}}=_{m}}_{_{k+1}}^{m}h_ {k}\)
6:endfor ```

**Algorithm 2** Approximate Policy Iteration: Average Reward

### Performance bounds for Average Reward Approximate Policy Iteration

We are now ready to present the main result of this work.

**Theorem 3.3**.: _Let \(J^{*}\) be the optimal average reward of an MDP that satisfies Assumption 3.1. The sequence of policies \(_{k}\) generated by Algorithm 2 and their associated average rewards \(J_{_{k}}\) satisfy the following bound:_

\[(J^{*}\!-\!J_{_{k+1}}))}{}((+1)+2)}_{}+(J^{*}-_{i}(h_{0}-h_{0} )(i)+)}_{}.\]

**Interpretation of the Bound:** Since \(0<<1\), as \(k\), the error due to initial condition \(h_{0}\) drops to zero. The approximation error consists of two components, \(\) and \(\). Here, \(\) represents the approximation error due to a suboptimal policy and \(\) represents the approximation error associated with evaluating the value function corresponding to a policy. The limiting behavior of the average reward associated with the policies obtained as a consequence of the algorithm is captured in the following corollary.

**Corollary 3.4**.: _The asymptotic performance of the policies obtained from Algorithm 2 satisfies:_

\[_{k}(J^{*}-J_{_{k}}).\] (5)

Note that the asymptotic bound in Equation (5) is in a very similar form as the asymptotic performance bound for the discounted reward MDP in Equation (4) where \(\) plays the role of \((1-)^{2}\). However, when \(\) and \(\) got to zero, this bound also goes to zero which is not the case if we try to approximate average-reward problems by using the results for discounted-reward problems in the limit where the horizon goes to infinity.

## 4 Application to Reinforcement Learning

Approximate policy iteration is closely related to RL. In general, approximate policy improvement and approximate policy evaluation in Algorithm 2 are executed through approximations to policy improvement (such as greedy update, mirror descent, softmax) and TD learning for value function approximation. First, we present a generic framework to analyze policy-based RL algorithms by building upon the theory presented in the last section. For this purpose, we define the state-action relative value function \(Q\) (instead of the relative state value function \(h\) as before) to evaluate a policy. Let \(_{}^{}Q\) denote the Bellman operator with respect to \(Q\) and policy \(\) where \((_{}^{}Q)(s,a)=r(s,a)+(_{}Q)(s,a),\) where \(_{}(s^{},a^{}|s,a)=(a^{}|s^{})(s^{}|s,a).\) The relative state action value function corresponding to policy \(\) is represented by \(Q_{}\) and is the solution to the following Bellman equation: \(J_{}+Q_{}(s,a)=r(s,a)+_{s^{},a^{} }_{}(s^{},a^{}|s,a)Q_{}(s^{},a^{ }),\) for all state action pairs \((s,a)(,)\). We present a generic policy-based algorithm for average-reward problem below.

``` Require \(Q_{0}^{|||}\) for\(k=0,1,2,,T\)do  1. Determine \(_{k+1}\!\!\) as a function of \(Q_{k}\) using a possibly random policy improvement step  2. Compute \(Q_{k+1}\) as an approximation to \(Q_{_{k+1}}\) using (state, action, reward) samples from a trajectory generated by policy \(_{k+1}\) endfor ```

**Algorithm 3** Generic Policy Based Algorithm: \(Q\)-function Average Reward

Note that the error in Steps 1 (policy improvement) and 2 (policy evaluation) of the above algorithm could be random. Thus, the analysis for Theorem 3.3 has to be adapted to Algorithm 3. The resulting expected deviation from optimality is characterized in the lemma below.

**Lemma 4.1**.: _Let \(_{k}\) and \(Q_{k}\) be the sequence of policies and relative state-action value function iterates generated by Algorithm 3. For all \(k 0,,T,\) we have:_

\[(J^{*}-(_{(s,a)}(^{}Q_{k}-Q _{k})(s,a))) (1-)(J^{*}-(_{(s,a)}(^{ }Q_{k-1}-Q_{k-1})(s,a)))\] \[+\|^{}Q_{k-1}\!-\!_{_{k}}^{ }Q_{k-1}\!\|_{}\!+\!2\!\|Q_{k}\!-\!Q_{}\| _{},\]

_where_

\[=_{_{1},_{2},,_{T}\\ (s,a)}_{}^{*}(s,a)>0,\]

_and \(_{}^{*}\) is a matrix with identical rows corresponding to the invariant distribution of \(_{}\)._

Iteratively applying Lemma 4.1 yields the following proposition for a generic policy-based algorithm.

**Proposition 4.2**.: _For any \(T>0\), the iterates of Algorithm 3 satisfy_

\[\![J^{*}\!-\!J_{_{T+1}}] \![J^{*}\! -\!_{i}(^{}Q_{0}\!-\!Q_{0})(i)]}_{ }\!+\!^{T-1}(1\!-\! )^{}\![\|Q_{T-}\!-\!Q_{_{T-}} \|_{}]}_{}\] \[+ ^{T-1}(1-)^{-1} \![\|_{_{T+1}-}^{}Q_{T-} \!-\!^{}Q_{T-}\|_{}]+ [\|_{_{T+1}}^{}Q_{T}-^{} Q_{T}\|_{}]}_{}.\]

We now illustrate the applicability of the above result to specific RL algorithms. More specifically, we characterize finite time performance bounds when state-action relative value functions are learnt through TD-Learning , while several different update rules are employed for policy improvement.

### Performance bounds for RL algorithms

For every iteration of Algorithm 3, it is necessary to observe trajectories of length \(\) in order to learn the state action value function associated with any policy. Using linear function approximations for value functions, the error in policy evaluation at iteration \(k\) in Algorithm 3 can be expressed in the following form:

\[\|Q_{k}-Q_{_{k}}\|_{}\|(_{k}-_{_{k}}^{*})\|_{ }+\|Q_{_{k}}-_{_{k}}^{*}\|_{},\] (6)where \(^{|||| d}\) corresponds to the feature vector matrix and \(^{d}\) is the parameter vector. Further, \(_{k}\) is the parameter vector obtained as a result of the TD learning algorithm. The projection of \(Q_{_{k}}\) onto the span of \(\) is given by \(_{_{k}}^{*}\). The last term in Equation (6) corresponds to the function approximation error which depends on the span of the feature matrix \(\). We denote this error term by \(_{0,k}\). The error due to TD learning (first term in Equation (6)) is denoted by \(_{,k}\). Assuming \(_{(s,a)}\|(s,a)\|_{2} 1\), and \(\|_{_{k}}^{*}\|_{2}\) is bounded uniformly in \(k\),  shows that there exists \(C>0\) such that the length of the sample trajectory \(\) and \(_{,k}\) are related as follows:

\[[_{,k}]=}.\] (7)

Let \(c_{0}=(J^{*}-_{i}(^{Q}Q_{0}-Q_{0}) (i))\) be the error due to initial condition and \(_{0}=_{t}_{0,t}\) be the maximum function approximation error across all iterations. Then we obtain the following performance bounds when TD learning is used for policy evaluation.

**Corollary 4.3**.: _(Greedy policy update) Let \((s)=*{argmax}_{a}Q_{k}(s,a^{}).\) Let \(>0\) be such that the sequence of policies in Algorithm 3 corresponding to all state action pairs \((s,a)\) are obtained using the following algorithm, which we call the greedy policy update,_

\[_{k+1}(a|s)=|},&a (s)\\ |}+1-,&a=(s),\] (8)

_Let \(=_{s,a\\  1..T}|Q_{k}(s,a)|.\) Using TD Learning with linear value function approximation for policy evaluation, we get the following finite time performance bound:_

\[[J^{*}-J_{_{T+1}}](1-)^{T}c_{0}+( {1+}{})+(_{0}+}).\] (9)

**Corollary 4.4**.: _(Softmax policy update) Let \(>0\) be such that the sequences of policies in Algorithm 3 corresponding to all state action pairs \((s,a)\) are obtained using the following algorithm, which we call the softmax policy update,_

\[_{k+1}(a|s)=(s,a))}{_{a^{} }( Q_{k}(s,a^{}))}.\] (10)

_Using TD Learning with linear value function approximation for policy evaluation, we get the following finite time performance bound:_

\[[J^{*}-J_{_{T+1}}](1-)^{T}c_{0}+( {1+}{})|)}{}+(_{0}+}).\] (11)

**Corollary 4.5**.: _(Mirror descent update) Let \(>0\) be such that the sequences of policies in Algorithm 3 corresponding to all state action pairs \((s,a)\) are obtained using the following algorithm, which we call the mirror descent policy update._

\[_{k+1}(a|s)_{k}(a|s)\{ Q_{k}(s,a)\}.\] (12)

_Let \(=_{k 1..T\\ s}_{k}(a^{*}(s)|s)\) where \(a^{*}(s)\) is the optimal action at state \(s\). Using TD Learning with linear value function approximation for policy evaluation, we get the following finite time performance bound:_

\[[J^{*}-J_{_{T+1}}](1-)^{T}c_{0}+( {1+}{})()+(_{0}+}).\] (13)

_Remark 4.6_.: The following remarks are in order:

* In each of the above performance bounds, there is an inevitable constant error \((_{0})\) due to function approximation. If the value function lies in the class of functions chosen, then this error will be zero.
* The error due to policy update (characterized by \(\)) can be made arbitrarily small by choosing \(\) large. However, choosing a large \(\) may increase \(C\) due to the fact that \(C\) is a function of the mixing time of each policy, which is affected by the choice of \(\). However, \(C\) cannot be arbitrarily large due to Assumption (a).
* The error due to the initial condition decays exponentially fast but the rate of decay can be slow because \(\) can be very small. In this regard, it is interesting to compare our work with the results in . We do so in the next subsection.

### Comparison with 

Our model for approximate policy iteration is intended to be a general framework to study policy-based RL algorithms. A specific class of such algorithms has been studied in , where regret bounds are presented for a mirror descent like algorithm referred to as Politex. Although our model is intended for offline algorithms, it is possible to adapt the techniques in  to obtain regret bounds for our model as well.

Let \(r(s_{t},a_{t})\) be the reward obtained at time \(t\). Let \(K\) be the total number of time units for which Algorithm 3 is executed. Hence \(K=T\), where \(\) is the length of the sample trajectory for every iteration of policy evaluation. The regret, defined as \(R(K)=_{t=1}^{K}(J^{*}-r(s_{t},a_{t}))\) can be further decomposed as:

\[R(K)=_{t=1}^{K}(J^{*}-J_{_{t}})+(J_{_{t}}-r(s_{t},a_{ t})).\] (14)

The first term i.e., \(_{t=1}^{K}(J^{*}-J_{_{t}})\) is referred to as the pseudo regret \(R_{}(K)\). The second term is essentially bounding the average value from its estimate and can be bounded using a martingale analysis identical to . Here, we focus on obtaining an upper bound on the pseudo regret with mirror descent update and TD learning. The pseudo regret can be further decomposed as:

\[R_{}(K)=_{t=1}^{T}(J^{*}-J_{_{t}}).\]

For the mirror descent policy update in Corollary 4.5, we get the following regret bound

\[R_{}(K)_{t=1}^{T}(1-)^{T}c_{0}+()+( {}_{0}+}{}).\] (15)

Let \(=\), then the above regret can be expressed as:

\[R_{}(K)( c_{0}+2K_{0} +}{}).\]

Optimizing for the regret yields \(=O(K^{})\) and the following pseudo regret upper bound,

\[R_{}(K)(2K_{0}+c_{6}K^{ }).\] (16)

We now compare our regret bounds to the ones in . Since all of the bounds will have an inevitable error \((_{0})\) due to function approximation, the comparison is with respect to the other terms in the regret bound.

* We get a better order-wise bound on the regret compare to  which obtains a bound of \(O(K^{3/4})\). However, they have better constants which are in terms of mixing-time coefficients rather than in terms of \(\).
*  gets a better regret bound \((O())\) than us.

The results in  use Monte Carlo policy evaluation, which allows for a closed-form expression for the value function estimate. The structure of the closed-form expressions allows them to exploit the fact that the policy varies slowly from one iteration to the next (which is specific to the mirror descent policy update) and hence \(\) does not appear in their analysis. However, the Monte Carlo policy evaluation algorithms use matrix inversion which is often too expensive to implement in practice unlike the TD learning algorithm in our analysis which is widely used. Nevertheless, the ideas in  provide a motivation to investigate whether one can extend their analysis to more general policy evaluation and policy improvement algorithms. This is an interesting topic for future research.

Conclusions

We present a general framework for analyzing policy-based reinforcement learning algorithms for average-reward MDPs. Our main contribution is to obtain performance bounds without using the natural source of contraction available in discounted reward formulations. We apply our general framework to several well-known RL algorithms to obtain finite-time error bounds on the average reward. We conclude with a comparative regret analysis and an outline of possible future work.