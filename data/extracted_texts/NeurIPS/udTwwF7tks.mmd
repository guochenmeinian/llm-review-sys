# Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval

Ashwin Ramachandran\({}^{1*}\)   Vaibhav Raj\({}^{2*}\)   Indrayumna Roy\({}^{2}\)

Soumen Chakrabarti\({}^{2}\)   Abir De\({}^{2}\)

\({}^{1}\)UC San Diego  \({}^{2}\)IIT Bombay

ashwinramg@ucsd.edu

{vaibhavraj, indraroy15, soumen, abir}@cse.iitb.ac.in

Equal contribution. Ashwin Ramachandran did this work while at IIT Bombay.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Graph retrieval based on subgraph isomorphism has several real-world applications such as scene graph retrieval, molecular fingerprint detection and circuit design. Roy et al.  proposed IsoNet, a late interaction model for subgraph matching, which first computes the node and edge embeddings of each graph independently of paired graph and then computes a trainable alignment map. Here, we present IsoNet++, an early interaction graph neural network (GNN), based on several technical innovations. First, we compute embeddings of all nodes by passing messages within and across the two input graphs, guided by an _injective alignment_ between their nodes. Second, we update this alignment in a lazy fashion over multiple _rounds_. Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment. After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round. Third, IsoNet++ incorporates a novel notion of node-pair partner interaction. Traditional early interaction computes attention between a node and its potential partners in the other graph, the attention then controlling messages passed across graphs. In contrast, we consider _node pairs_ (not single nodes) as potential partners. Existence of an edge between the nodes in one graph and non-existence in the other provide vital signals for refining the alignment. Our experiments on several datasets show that the alignments get progressively refined with successive rounds, resulting in significantly better retrieval performance than existing methods. We demonstrate that all three innovations contribute to the enhanced accuracy. Our code and datasets are publicly available at https://github.com/structlearning/isonetpp.

## 1 Introduction

In graph retrieval based on subgraph isomorphism, the goal is to identify a subset of graphs from a corpus, denoted \(\{G_{c}\}\), wherein each retrieved graph contains a subgraph isomorphic to a given query graph \(G_{q}\). Numerous real-life applications, _e.g._, molecular fingerprint detection , scene graph retrieval , circuit design  and frequent subgraph mining , can be formulated using subgraph isomorphism. Akin to other retrieval systems, the key challenge is to efficiently score corpus graphs against queries.

Recent work on neural graph retrieval  has shown significant promise. Among them, Lou et al. [23, Neuromatch] and Roy et al. [35, IsoNet] focus specifically on subgraph isomorphism. They employ graph neural networks (GNNs) to obtain embeddings of query and corpus graphs and compute the relevance score using a form of order embedding . In addition, IsoNet also approximates an _injective alignment_ between the query and corpus graphs. These two models operate in a _late interaction_ paradigm, where the representations of the query and corpus graphs arecomputed independent of each other. In contrast, GMN  is a powerful _early interaction_ network for graph matching, where GNNs running on \(G_{q}\) and \(G_{c}\) interact with each other at every layer.

Conventional wisdom suggests that early interaction is more accurate (even if slower) than late interaction, but GMN was outperformed by IsoNet. This is because of the following reasons. (1) GMN does not explicitly infer any alignment between \(G_{q}\) and \(G_{c}\). The graphs are encoded by two GNNs that interact with each other at every layer, mediated by attentions from each node in one graph on nodes in the other. These attentions are functions of node embeddings, so they change from layer to layer. While these attentions may be interpreted as approximate alignments, they induce at best non-injective mappings between nodes. (2) In principle, one wishes to propose a consistent alignment across all layers. However, GMN's attention based 'alignment' is updated in every layer. (3) GMN uses a standard GNN that is known to be an over-smoother [36; 40]. Due to this, the attention weights (which depend on the over-smoothed node representations) also suffer from oversmoothing. These limitations raise the possibility of a _third_ approach based on early interaction networks, enabled with explicit alignment structures, that have the potential to outperform both GMN and IsoNet.

### Our contributions

We present IsoNet++, an early interaction network for subgraph matching that maintains a chain of explicit, iteratively refined, injective, approximate alignments between the two graphs.

Early interaction GNNs with alignment refinementWe design early interaction networks for scoring graph pairs, that ensure the node embeddings of one graph are influenced by both its paired graph and the alignment map between them. In contrast to existing works, we model alignments as an explicit "data structure". An alignment can be defined between either nodes or edges, thus leading to two variants of our model: IsoNet++ (Node) and IsoNet++ (Edge). Within IsoNet++, we maintain a sequence of such alignments and refine them using GNNs acting on the two graphs. These alignments mediate the interaction between the two GNNs. In our work, we realize the alignment as a doubly stochastic approximation to a permutation matrix, which is an injective mapping by design.

Eager or lazy alignment updatesIn our work, we view the updates to the alignment maps as a form of gradient-based updates in a specific quadratic assignment problem or asymmetric Gromov-Wasserstein (GW) distance minimization [30; 41]. The general form of IsoNet++ allows updates that proceed lockstep with GNN layers (_eager_ layer-wise updates), but it also allows _lazy_ updates. Specifically, IsoNet++ can perform \(T\)_rounds_ of updates to the alignment, each round including \(K\)_layers_ of GNN message passing. During each round, the alignment is held fixed across all propagation layers in GNN. At the end of each round, we update the alignment by feeding the node embeddings into a neural Gumbel-Sinkhorn soft permutation generator [10; 26; 37].

Node-pair partner interaction between graphsThe existing remedies to counter oversmoothing [8; 33; 40] entail extra computation; but they may be expensive in an early-interaction setting. Existing early interaction models like  perform _node partner interaction_; interactions are constrained to occur between a node and it's _partner_, the node in the paired graph aligned with it. Instead, we perform _node-pair partner_ interaction; the interaction is expanded to include the _node-pairs_ (or edges) in the paired graph that correspond to node-pairs containing the node. Consequently, the embedding of a node not only depends on nodes in the paired graph that align with it, but also captures signals from nodes in the paired graph that are aligned with its neighbors.

ExperimentsThe design components of IsoNet++ and their implications are subtle -- we report on extensive experiments that tease out their effects. Our experiments on real world datasets show that, IsoNet++ outperforms several state-of-the-art methods for graph retrieval by a substantial margin. Moreover, our results suggest that capturing information directly from node-pair partners can improve representation learning, as compared to taking information only from node partner.

## 2 Preliminaries

NotationGiven graph \(G=(V,E)\), we use \((u)\) to denote the neighbors of a node \(u V\). We use \(u v\) to indicate a message flow from node \(u\) to node \(v\). Given a set of corpus graphs \(C=\{G_{c}\}\) and a query graph \(G_{q}\), we denote \(y(G_{c}\,|\,G_{q})\) as the binary relevance label of \(G_{c}\) for \(G_{q}\). Motivated by several real life applications like substructure search in molecular graphs , object search in scene graphs , and text entailment , we consider subgraph isomorphism to significantly influence the relevance label, similar to previous works [23; 35]. Specifically, \(y(G_{c}\,|\,G_{q})=1\) when \(G_{q}\) is a subgraph of \(G_{c}\), and \(0\) otherwise. We define \(C_{q+} C\) as the set of corpus graphs that are relevant to \(G_{q}\) and set \(C_{q-}=C C_{q+}\). Mildly overloading notation, we use \(\) to indicate a 'hard' (0/1) permutation matrix or its'soft' doubly-stochastic relaxation. \(_{n}\) denotes the set of all \(n n\) doubly stochastic matrices, and \(_{n}\) denotes the set of all \(n n\) permutation matrices.

IsoNet Given a graph \(G=(V,E)\), IsoNet uses a GNN, which initializes node representations \(\{_{0}(u):u V\}\) using node-local features. Then, messages are passed between neighboring nodes in \(K\)_propagation layers_. In the \(k\)th layer, a node \(u\) receives messages from its neighbors, aggregates them, and then combines the result with its state after the \((k-1)\)th layer:

\[_{k}(u)=_{}(_{k-1}(u),_{v(u)}\{_{}(_{k-1}(u),_{k-1}(v))\} ).\] (1)

Here, \(_{}()\) and \(_{}(,)\) are suitable networks with parameters collectively called \(\). Edges may also be featurized and influence the messages that are aggregated . The node representations at the final propagation layer \(K\) can be collected into the matrix \(=\{_{K}(u)\,|\,u V\}\). Given a node \(u G_{q}\) and a node \(u^{} G_{c}\), we denote the embeddings of \(u\) and \(u^{}\) after the propagation layer \(k\) as \(_{k}^{(q)}(u)\) and \(_{k}^{(c)}(u^{})\) respectively. \(^{(q)}\) and \(^{(c)}\) denote the \(K\)th-layer node embeddings of \(G_{q}\) and \(G_{c}\), collected into matrices. Note that, here the set of vectors \(^{(q)}\) and \(^{(c)}\) do not dependent on \(G_{c}\) and \(G_{q}\). In the end, IsoNet compares these embeddings to compute the distance \((G_{c}\,|\,G_{q})\), which is inversely related to \((G_{c}\,|\,G_{q})\).

\[(G_{c}\,|\,G_{q})=_{u,i}[^{(c)}-^{(q)} ][u,i]\] (2)

Since subgraph isomorphism entails an asymmetric relevance, we have: \((G_{c}\,|\,G_{q})(G_{q}\,|\,G_{c})\). IsoNet also proposed another design of \(\), where it replaces the node embeddings with edge embeddings and node alignment matrix with edge alignment matrix in Eq. (2).

In an **early** interaction network, \(^{(q)}\) depends on \(G_{c}\) and \(^{(c)}\) depends on \(G_{q}\) for any given \((G_{q},G_{c})\) pair. Formally, one should write \(^{(q\,|\,c)}\) and \(^{(c\,|\,q)}\) instead of \(^{(q)}\) and \(^{(c)}\) respectively for an early interaction network, but for simplicity, we will continue using \(^{(q)}\) and \(^{(c)}\).

Our goalGiven a set of corpus graphs \(C=\{G_{c}\,|\,c[|C|]\}\), our high-level goal is to build a graph retrieval model so that, given a query \(G_{q}\), it can return the corpus graphs \(\{G_{c}\}\) which are relevant to \(G_{q}\). To that end, we seek to develop (1) a GNN-based early interaction model, and (2) an appropriate distance measure \((\,|\,)\), so that \((^{(c)}\,|\,^{(q)})\) is an accurate predictor of \(y(G_{c}\,|\,G_{q})\), at least to the extent that \((|)\) is effective for ranking candidate corpus graphs in response to a query graph.

## 3 Proposed early-interaction GNN with multi-round alignment refinement

In this section, we first write down the subgraph isomorphism task as an instance of the quadratic assignment problem (QAP) or the Gromov-Wasserstein (GW) distance optimization task. Then, we design IsoNet++, by building upon this formulation.

### Subgraph isomorphism as Gromov-Wasserstein distance optimization

QAP or GW formulation with asymmetric costWe are given a graph pair \(G_{q}\) and \(G_{c}\) padded with appropriate number of nodes to ensure \(|V_{q}|=|V_{c}|=n\) (say). Let their adjacency matrices be \(_{q},_{c}\{0,1\}^{n n}\). Consider the family of hard permutation matrices \(_{n}\) where \([u,u^{}]=1\) indicates \(u V_{q}\) is "matched" to \(u^{} V_{c}\). Then, \(G_{q}\) is a subgraph of \(G_{c}\), if for some permutation matrix \(\), the matrix \(_{q}\) is covered by \(_{c}^{}\), _i.e._, for each pair \((u,v)\), whenever we have \(_{q}[u,v]=1\), we will also have \(_{c}^{}[u,v]=1\). This condition can be written as \(_{q}_{c}^{}\). We can regard a deficit in coverage as a cost of distance:

\[(;_{q},_{c}) =_{u[n],v[n]}[(_{q}-_{c}^{})_{+}][u,v]\] (3) \[=_{u,v[n]}_{u^{},v^{}[n]}(_{q}[u,v]-_{c}[u^{},v^{}])_{+}\,[u,u^{}]\;[v,v^{ }]\] (4)

Here, \([]_{+}=\,\{,0\}\) is the ReLU function, applied elementwise. The function \((;_{q},_{c})\) can be driven down to zero using a suitable choice of \(\) iff \(G_{q}\) is a subgraph of \(G_{c}\). This naturally suggests the relevance distance

\[(G_{c}\,|\,G_{q})=_{_{n}}(;_{q}, _{c})\] (5)

Xu et al.  demonstrate that this QAP is a realization of the Gromov-Wasserstein distance minimization in a graph setting.

Updating \(\) with projected gradient descentAs shown in Benamou et al. , Peyre et al. , Xu et al. , one approach is to first relax \(\) into a doubly stochastic matrix, which serves as a continuous approximation of the discrete permutation, and then update it using projected gradient descent (PGD). Here, the soft permutation \(_{t-1}\) is updated to \(_{t}\) at time-step \(t\) by solving the following linear optimal transport (OT) problem, regularized with the entropy of \(\{[u,v]\,|\,u,v[n]\}\) with a temperature \(\).

\[_{t}*{arg\,min}_{_{u}} *{Trace}(^{}_{}(; {A}_{q},_{c})_{=_{t-1}})+_{u,v}[u, v][u,v].\] (6)

Such an OT problem is solved using the iterative Sinkhorn-Knopp algorithm [10; 37; 26]. Similar to other combinatorial optimization problems on graphs, a QAP (4) does not capture the coverage cost in the presence of dense node or edge features, where two nodes or edges may exhibit graded degrees of similarity represented by continuous values. Furthermore, the binary values of the adjacency matrices result in inadequate gradient signals in \(_{}()\). Additionally, the computational bottleneck of solving a fresh OT problem in each PGD step introduces a significant overhead, especially given the large number of pairwise evaluations required in typical learning-to-rank setups.

### Design of IsoNet++ (Node)

Building upon the insights from the above GW minimization (3) and the successive refinement step (6), we build IsoNet++ (Node), the first variant of our proposed early interaction model.

Node-pair partner interactions between graphsFor simpler exposition, we begin by describing a synthetic scenario, where \(\) is a hard node permutation matrix, which induces the alignment map as a bijection \(:V_{q} V_{c}\), so that \((a)=b\) if \([a,b]=1\). We first initialize layer \(k=0\) embeddings as \(_{0}^{(q)}(u)=_{}((u))\) using a neural network \(_{}\). (Throughout, \(_{k}^{(c)}(u)\) are treated likewise.) Under the given alignment map \(\), a simple early interaction model would update the node embeddings as follows:

\[_{k+1}^{(q)}(u)=_{}(_{k}^{(q)}(u), \;_{v(u)}_{}(_{k}^{(q)}(u), _{k}^{(q)}(v)),\;_{k}^{(c)}((u)))\] (7)

In the above expression, the update layer uses representation of the partner node \(u^{} V_{c}\) during the message passing step, to compute \(_{k+1}^{(q)}(u)\), the embedding of node \(u V_{q}\). Li et al.  use a similar update protocol, by approximating \(_{k}^{(c)}((u))=_{u^{} V_{c}}a_{u^{} u}^{(k)} _{k}^{(c)}(u^{})\), where \(a_{u^{} u}^{(k)}\) is the \(k\)th layer attention from \(u V_{q}\) to potential partner \(u^{} V_{c}\), with \(_{u^{} V_{c}}a_{u^{} u}^{(k)}=1\). Instead

Figure 1: Overview of IsoNet++. Panel (a) shows the pipeline of IsoNet++. Given a graph pair \((G_{q},G_{c})\), we execute \(T\)_rounds_, each consisting of \(K\) GNN _layer_ propagations. After a round \(t\), we use the node embeddings to update the node alignment \(=_{t}\) from its previous estimate \(=_{t-1}\). Within each round \(t[T]\), we compute the node embeddings of \(G_{q}\) by gathering signals from \(G_{c}\) and vice-versa, using GNN embeddings in the previous round and the node-alignment map \(_{t}\). The alignment \(_{t}\) remains consistent across all propagation layers \(k[K]\) and is updated at the end of round \(t\). Panel (b) shows our proposed node pair partner interaction in IsoNet++ (Node). When computing the message value of the node pair \((u,v)\), we also feed the node embeddings of the partners \(u^{}\) and \(v^{}\) in addition to the embeddings of the pairs \((u,v)\), where \(u^{}\) and \(v^{}\) is approximately aligned with \(u\) and \(v\), respectively (when converted to soft alignment, \(u^{},v^{}\) need not be neighbors). Panel (c) shows the node pair partner interaction in IsoNet++ (Edge). In contrast to IsoNet++ (Node), here we feed the information from the message value of the partner pair \((u^{},v^{})\) instead of their node embeddings into the message passing network \(_{}\).

of regarding only nodes as potential partners, IsoNet++ will regard _node pairs_ as partners. Given \((u,v) E_{q}\), the partners \(((u),(v)) E_{c}\) should then greatly influence the intensity of assimilation of \(_{k}^{(c)}(u^{})\) into \(_{k+1}^{(c)}(u)\). The first key innovation in IsoNet++ is to replace (7) to recognize and implement this insight:

\[_{k+1}^{(q)}(u)=_{}([_{k}^{(q )}(u),_{k}^{(c)}((u))],.\\ ._{v}(u)}_{} ([_{k}^{(q)}(u),_{k}^{(c)}((u))],[_{k}^{(q)}(v),_{k}^{(c)}((v))]))\] (8)

Embeddings \(_{k+1}^{(c)}(u^{})\) for nodes \(u^{} V_{c}\) are updated likewise in a symmetric manner. The network \(_{}\) is provided embeddings from partners \((u),(v)\) of \(u,v V_{q}\) -- this allows \(_{k+1}^{()}(u)\) to capture information from all nodes in the paired graph, that match with the \((k+1)\)-hop neighbors of \(u\). We schematically illustrate the interaction between the paired graphs in IsoNet, GMN and IsoNet++ in Figure 2.

Multi-round lazy refinement of node alignmentIn reality, we are not given any alignment map \(\). This motivates our second key innovation beyond prior models , where we decouple GNN layer propagation from updates to \(\). To achieve this, IsoNet++ (Node) executes \(T\)_rounds_, each consisting of \(K\)_layer_ propagations in both GNNs. At the end of each round \(t\), we refine the earlier alignment \(_{t-1}\) to the next estimate \(_{t}\), which will be used in the next round. Henceforth, we will use the double subscript \(t,k\) instead of the single subscript \(k\) as in traditional GNNs. We denote the node embeddings at layer \(k\) and round \(t\) by \(_{t,k}^{(q)}(u),_{t,k}^{(c)}(u^{})^{_{h}}\) for \(u V_{q}\) and \(u^{} V_{c}\), which are (re-)initialized with node features \(_{t,0}^{}\) for each round \(t\). We gather these into matrices

\[_{t,k}^{(q)}=[_{t,k}^{(q)}(u)\,|\,u V_{q}]^{n _{h}}_{t,k}^{(c)}=[_{t,k}^{(c)}(u^{} )\,|\,u^{} V_{c}]^{n_{h}}.\] (9)

\(\) no longer remains an oracular hard permutation matrix, but becomes a doubly stochastic matrix indexed by rounds, written as \(_{t}\). At the end of round \(t\), a differentiable _aligner_ module takes \(_{t,K}^{(q)}\) and \(_{t,K}^{(c)}\) as inputs and outputs a doubly stochastic node alignment (relaxed permutation) matrix \(_{t}\) as follows:

\[_{t} =_{}(_{t,K}^{(q)}, _{t,K}^{(c)})\] (10) \[=(_{}(_{t,K}^{(q) })\,_{}(_{t,K}^{(c)})^{})_{n}\] (11)

In the above expression, \(()\) performs iterative Sinkhorn normalization on the input matrix added with Gumbel noise ; \(_{}\) is a neural module consisting of two linear layers with a ReLU activation after the first layer. As we shall see next, \(_{t}\) is used to gate messages flowing _across_ from one graph to the other during round \(t+1\), i.e., while computing \(_{t+1,1:K}^{(q)}\) and \(_{t+1,1:K}^{(c)}\). The soft alignment \(_{t}\) is kept frozen for the duration of all layers in round \(t+1\). \(_{t}[u,u^{}]\) may be interpreted as the probability that \(u\) is assigned to \(u^{}\), which naturally requires that \(_{t}\) should be

Figure 2: Illustration of the three interaction modes. IsoNet has no/late interaction between \(^{(q)}\) and \(^{(c)}\). IsoNet++ and GMN allow interaction between the representations of the query and corpus nodes. Under **node pair interaction**, the individual node embeddings \(^{(q)}\) are used for message passing directly, thereby exposing them only to their neighbors. In the corresponding \(_{}\) step, nodes interact only with their respective partners, therefore missing out on information from the partners of its neighbors. However, under **node pair partner interaction**, the representation of a node is combined with that of its partner(s) first, using the \(_{}\) block to obtain \(^{(q)}\) (12), which is used for message passing. Thus, when interacting with its neighbors, a node also gets information from the partners of its neighbors.

row-equivariant (column equivariant) to the shuffling of the node indices of \(G_{q}\) (\(G_{c}\)). As shown in Appendix D, the above design choice (11) ensures this property.

**Updating node representation using early-interaction GNN** Here, we describe the early interaction GNN for the query graph \(G_{q}\). The GNN on the corpus graph \(G_{c}\) follows the exact same design and is deferred to Appendix E.1. In the initial round (\(t=1\)), since there is no prior alignment estimate \(_{t=0}\), we employ the traditional late interaction GNN (1) to compute all layers \(_{1,1:K}^{(q)}\) and \(_{1,1:K}^{(c)}\) separately. These embeddings are then used to estimate \(_{t=1}\) using Eq. (11). For subsequent rounds (\(t>1\)), given embeddings \(_{t,1:K}^{(q)}\), and the alignment estimate matrix \(_{t}\), we run an early interaction GNN from scratch. We start with a fresh initialization of the node embeddings as before; i.e., \(_{t+1,0}^{(q)}(u)=_{}((u))\). For each subsequent propagation layer \(k+1\) (\(k[0,K-1]\)), we approximate (8) as follows. We read previous-round, same-layer embeddings \(_{t,k}^{(c)}(u^{})\) of nodes \(u^{}\) from the other graph \(G_{c}\), incorporate the alignment strength \(_{t}[u,u^{}]\), and aggregate these to get an intermediate representation of \(u\) that is sensitive to \(_{t}\) and \(G_{c}\).

\[_{t+1,k}^{(q)}(u)=_{}(_{t+1,k}^{(q)}(u), _{u^{} V_{c}}_{t,k}^{(c)}(u^{})_{t}[u,u^{ }])\] (12)

Here, \(_{}\) is a neural network that computes interaction between the graph pairs; \(_{t+1,k}^{(q)}(u)\) provides a soft alignment guided representation of \([_{k}^{(q)}(u),_{k}^{(c)}((u))]\) in Eq. (8), which can be relaxed as:

\[_{t+1,k+1}^{(q)}(u)=_{}(_{t+1,k}^{(q)}(u),_{v(u)}_{}(_{t+1,k}^{(q)}(u), _{t+1,k}^{(q)}(v)))\] (13)

In the above expression, we explicitly feed \(_{t+1,k}^{(q)}(v),v(u)\) in the \(_{}\) network, capturing embeddings of nodes in the corpus \(G_{c}\) aligned with the _neighbors_ of node \(u V_{q}\) in \(_{t+1,k+1}^{(q)}(u)\). This allows the model to perform node-pair partner interaction. Instead, if we were to feed only \(_{t+1,k}^{(q)}(u)\) into the \(_{}\) network, then it would only perform node partner interaction. In this case, the computed embedding for \(u\) would be based solely on signals from nodes in the paired graph that directly correspond to \(u\), therefore missing additional context from other neighbourhood nodes.

**Distant supervision of alignment** Finally, at the end of \(T\) rounds, we express the relevance distance \((G_{c}\,|\,G_{q})\) as a soft distance between the set \(_{T,K}^{(q)}=[_{T,K}^{(q)}(u)\,|\,u V_{q}]\) and \(_{T,K}^{(c)}=[_{T,K}^{(c)}(u^{})\,|\,u^{} V_{c}]\), measured as

\[_{,}(G_{c}\,|\,G_{q})=_{u}_{d}(_{T, K}^{(q)}[u,d]-(_{T}_{T,K}^{(c)}][u,d])\] (14)

Our focus is on graph retrieval applications. It is unrealistic to assume direct supervision from a gold alignment map \(^{*}\). Instead, training query instances are associated with pairwise preferences between two corpus graphs, in the form \( G_{q},G_{c+},G_{c-}\), meaning that, ideally, we want \(_{,}(G_{c-}|G_{q})+_{,}(G_{c+}|G_{q})\), where \(>0\) is a margin hyperparameter. This suggests a minimization of the standard hinge loss as follows:

\[_{,}_{q Q}_{c+c C_{q+},c- C_{q-}}[+ _{,}(G_{c+}\,|\,G_{q})-_{,}(G_{c-}\,|\,G_{q})]_ {+}\] (15)

This loss is back-propagated to train model weights \(\) in \(_{},_{},_{}\) and weights \(\) in the Gumbel-Sinkhorn network.

**Multi-layer eager alignment variant** Having set up the general multi-round framework of IsoNet++, we introduce a structurally simpler variant that updates \(\) eagerly after every layer, eliminating the need to re-initialize node embeddings every time we update \(\). The eager retains the benefits of node-pair partner interactions, while ablating IsoNet++ toward GMN. Updating \(\) via Sinkhorn iterations is expensive compared to a single GNN layer. In practice, we see a non-trivial tradeoff between computation cost, end task accuracy, and the quality of our injective alignments, depending on the value of \(K\) for eager updates, and the values \((T,K)\) for lazy updates. Formally, \(_{k}\) is updated across layers as follows:

\[_{k} =_{}(_{k}^{(q)},_{k}^{(c)})\] (16) \[=(_{}(_{k}^{(q)}) \,_{}(_{k}^{(c)})^{}).\] (17)We update the GNN embeddings, layerwise, as follows:

\[_{k}^{(q)}(u)=_{}_{k}^{(q)}(u),_{u^{ } V_{e}}_{k}^{(c)}(u^{})_{k}[u,u^{}],\] (18)

\[_{k+1}^{(q)}(u)=_{}_{k}^{(q)}(u),_{v }(u)}_{}(_{k}^{(q)}(u),_{k}^{(q )}(v))\] (19)

Analysis of computational complexityHere, will compare the performance of IsoNet (Node)  with multi-layer IsoNet++ (Node) and multi-round IsoNet++ (Node) for graphs with \(|V|\) nodes. For multi-layer IsoNet++ (Node) and IsoNet (Node), we assume \(K\) propagation steps and for multi-round IsoNet++ (Node), \(T\) rounds, each with \(K\) propagation steps.

--_IsoNet (Node):_ The total complexity is \(O(|V|^{2}+K|E|)\), computed as follows: **(1)** Initialization of layer embeddings at layer \(k=0\) takes \(O(|V|)\) time. **(2)** The node representation computation incurs a complexity of \(O(|E|)\) for each message passing step since it aggregates node embeddings across all neighbors. **(3)** The computation of \(\) takes \(O(|V|^{2})\) time.

--_Multi-layer eager IsoNet++ (Node):_ The total complexity is \(O(K|V|^{2}+K|E|+K|V|^{2})=O(K|V|^{2})\), computed as follows: **(1)** Initialization (layer \(k=0\)) takes \(O(|V|)\) time. **(2)** The computation of intermediate embeddings \(^{()}\) (Eq. 18) involves the evaluation of the expression \(_{u^{} V_{e}}_{k}^{()}(u^{})_{k}[u,u^{ }]\) and hence admits a complexity of \(O(|V|)\) for each node per layer. The total complexity for \(K\) steps and \(|V|\) nodes is thus \(O(K|V|^{2})\). **(3)** Next, for each node in every layer, we compute \(_{k+1}^{()}\) (Eq. 19) which gathers messages \(^{()}\) from all its neighbors, contributing a total complexity of \(O(K|E|)\). **(4)** Finally, we update \(_{k}\) for each layer which has a complexity of \(O(K|V|^{2})\).

--_Multi-round IsoNet++ (Node):_ Here, the key difference from the multi-layer version above is that the doubly stochastic matrix \(_{t}\) from round \(t\) is used to compute \(\) and the \(K\)-step-GNN runs in each of the \(T\) rounds. This multiplies the complexity of steps 2 and 3 with \(T\), raising it to \(O(KT|V|^{2}+KT|E|)\). Matrix \(_{t}\) is updated a total of \(T\) times, which changes the complexity of step 4 to \(O(T|V|^{2})\). Hence, the total complexity is \(O(KT|V|^{2}+T|V|^{2}+KT|E|)=O(KT|V|^{2})\).

Hence, the complexity of IsoNet is \(O(|V|^{2}+K|E|)\), multi-layer IsoNet++ is \(O(K|V|^{2})\) and multi-round IsoNet++ is \(O(KT|V|^{2})\). This increased complexity of the latter comes with the benefit of a significant performance boost, as our experiments suggest.

### Extension of IsoNet++ (Node) to IsoNet++ (Edge)

We now extend IsoNet++ (Node) to IsoNet++ (Edge) which uses explicit edge alignment for interaction across GNN and relevance distance surrogate.

Multi-round refinement of edge alignmentIn IsoNet++ (Edge), we maintain a soft edge permutation matrix \(\) which is frozen at \(=_{t-1}\) within each round \(t[T]\) and gets refined after every round \(t\) as \(_{t-1}_{t}\). Similar to IsoNet++ (Node), within each round \(t\), GNN runs from scratch: it propagates messages across layers \(k[K]\) and \(_{t-1}\) assists it to capture cross-graph signals. Here, in addition to node embeddings \(_{t,k}^{()}\), we also use edge embeddings \(_{t,k}^{(q)}(e),\ _{t,k}^{(c)}(e^{})^{_{m}}\) at each layer \(k\) and each round \(t\), which capture the information about the subgraph \(k K\) hop away from the edges \(e\) and \(e^{}\). Similar to Eq. (9), we define \(_{t,k}^{(q)}=[_{t,k}^{(q)}(e)]_{e E_{t}},\) and \(_{t,k}^{(c)}=[_{t,k}^{(c)}(e^{})]_{e^{} E_{e}}\). \(_{t,0}^{()}\) are initialized using the features of the nodes connected by the edges, and possibly local edge features. Given the embeddings \(_{t,K}^{(q)}\) and \(_{t,K}^{(c)}\) computed at the end of round \(t\), an edge aligner module (\(_{}()\)) takes these embedding matrices as input and outputs a soft edge permutation matrix \(_{t}\), similar to the update of \(_{t}\) in Eq. (11).

\[_{t} =_{}(_{t,K}^{(q)},_{t,K}^{(c)})\] (20) \[=(_{}(_{t,K}^{(q)})\ _{}(_{t,K}^{(c)})^{})\] (21)

Here, \(_{t,K}^{()}\) are appropriately padded to ensure that they have the same number of rows.

Edge alignment-induced early interaction GNNFor \(t=1\), we start with a late interaction model using vanilla GNN (1) and obtain \(_{t=1}\) using Eq. (21). Having computed the edge embeddings \(_{t,1:K}^{()}()\) and node embeddings \(_{t,1:K}^{()}()\) upto round \(t\), we compute \(_{t}\) and use it to build a fresh early interaction GNN for round \(t+1\). To this end, we adapt the GNN guided by \(_{t}\) in Eqs. (12)(13),to the GNN guided by \(_{t}\). We overload the notations for neural modules and different embedding vectors from IsoNet++ (Node), whenever their roles are similar.

Starting with the same initialization as in IsoNet++ (Node), we perform the cross-graph interaction guided by the soft edge permutation matrix \(_{t}\), similar to Eq. (12). Specifically, we use the embeddings of edges \(\{e^{}=(u^{},v^{})\} E_{c}\), computed at layer \(k\) at round \(t\), which share soft alignments with an edge \(e=(u,v) E_{q}\), to compute \(_{t+1,k}^{(q)}(e)\) and \(_{t+1,k}^{(q)}(e^{})\) as follows:

\[_{t+1,k}^{(q)}(e)=_{}(_{t+1,k}^{(q) }(e),_{e^{} E_{c}}_{t,k}^{(c)}(e^{})_{t}[e,e^{ }])\] (22)

Finally, we update the node embeddings \(_{t+1,k+1}^{()}\) for propagation layer \(k+1\) as

\[_{t+1,k+1}^{(q)}(u)=_{}(_{t+1,k}^{ (q)}(u),_{a(u)}_{}(_{t+1,k}^ {(q)}(u),_{t+1,k}^{(q)}(a),_{t+1,k}^{(q)}((u,a))))\] (23)

In this case, we perform the cross-graph interaction at the edge level rather than the node level. Hence, \(_{}\) acquires cross-edge signals separately as \(_{t+1,k}^{()}\). Finally, we use \(_{t+1,k+1}^{()}\) and \(_{t+1,k+1}^{()}\) to update \(_{t+1,k+1}^{()}\) as follows:

\[_{t+1,k+1}^{(q)}(u,v)=_{}( _{t+1,k+1}^{(q)}(u),_{t+1,k+1}^{(q)}(v),_{t+1,k}^{(q)}((u,v) ))\] (24)

Likewise, we develop \(_{t+1,k+1}^{(c)}\) for corpus graph \(G_{c}\). Note that \(_{t+1,k+1}^{(q)}((u,v))\) captures signals not only from the matched pair \((u^{},v^{})\), but also signals from the nodes in \(G_{c}\) which share correspondences with the neighbor nodes of \(u\) and \(v\). Finally, we pad zero vectors to \([_{T,K}^{(q)}(e)]_{e E_{q}}\) and \([_{T,K}^{(c)}(e^{})]_{e^{} E_{c}}\) to build the matrices \(_{T,K}^{(q)}\) and \(_{T,K}^{(c)}\) with same number of rows, which are finally used to compute the relevance distance

\[_{,}(G_{c}\,|\,G_{q})=_{u}_{d}( _{T,K}^{(q)}[e,d]-(_{T}_{T,K}^{(c)}][e,d]).\] (25)

## 4 Experiments

We report on a comprehensive evaluation of IsoNet++ on six real datasets and analyze the efficacy of the key novel design choices. In Appendix G, we provide results of additional experiments.

### Experimental setup

DatasetsWe use six real world datasets in our experiments, _viz._, AIDS, Mutag, PTC-FM (FM), PTC-FR (FR), PTC-MM (MM) and PTC-MR (MR), which were also used in [27; 35]. Appendix F provides the details about dataset generation and their statistics.

State-of-the-art baselinesWe compare our method against eleven state-of-the-art methods, _viz._, (1) GraphSim  (2) GOTSim , (3) SimGNN , (4) EGSC , (5) H2MN , (6) Neuromatch , (7) GREED , (8) GEN , (9) GMN  (10) IsoNet (Node) , and (11) IsoNet (Edge) . Among them, Neuromatch, GREED, IsoNet (Node) and IsoNet (Edge) apply asymmetric hinge distances between query and corpus embeddings for \((G_{c}\,|\,G_{q})\), specifically catered towards subgraph matching, similar to our method in Eqs. (14) and (25). GMN and GEN use symmetric Euclidean distance between their (whole-) graph embeddings \(^{(q)}\) (for query) and \(^{(c)}\) (for corpus) as \(||^{(q)}-^{(c)}||\) in their paper , which is not suitable for subgraph matching and therefore, results in poor performance. Hence, we change it to \((G_{c}\,|\,G_{q})=[^{(q)}-^{(c)}]_{+}\). The other methods first compute the graph embeddings, then fuse them using a neural network and finally apply a nonlinear function on the fused embeddings to obtain the relevance score.

Training and evaluation protocolGiven a fixed corpus set \(C\), we split the query set \(Q\) into \(60\%\) training, \(15\%\) validation and \(25\%\) test set. We train all the models on the training set by minimizing a ranking loss (15). During the training of each model, we use five random seeds. Given a test query \(q^{}\), we rank the corpus graphs \(C\) in the decreasing order of \(_{,}(G_{c}\,|\,G_{q^{}})\) computed using the trained model. We evaluate the quality of the ranking by measuring Average Precision (AP) and HITS@20, described in Appendix F. Finally, we report mean average precision (MAP) and mean HITS@20, across all the test queries. By default, we set the number of rounds \(T=3\), the number of propagation layers in GNN \(K=5\). In Appendix F, we discuss the baselines, hyperparameter setup and the evaluation metrics in more detail.

### Results

Comparison with baselinesFirst, we compare IsoNet++ (Node) and IsoNet++ (Edge) against all the baselines, across all datasets. In Table 3, we report the results. The key observations are as follows: (**1**) IsoNet++ (Node) and IsoNet++ (Edge) outperform all the baselines by significant margins across all datasets. IsoNet++ (Edge) consistently outperforms IsoNet++ (Node). This is because edge alignment allows us to compare the graph pairs more effectively than node alignment. A similar effect was seen for IsoNet (Edge) vs. IsoNet (Node) . **(2)** Among all state-of-the-art competitors, IsoNet (Edge) performs the best followed by IsoNet (Node). Similar to us, they also use edge and node alignments respectively. However, IsoNet does not perform any interaction between the graph pairs and the alignment is computed once only during the computation of \((G_{c}\,|\,G_{q})\). This results in modest performance compared to IsoNet++. **(3)** GMN uses "attention" to estimate the alignment between graph pairs, which induces a non-injective mapping. Therefore, despite being an early interaction model, it is mostly outperformed by IsoNet, which uses injective alignments.

Lazy vs. eager updatesIn lazy multi-round updates, the alignment matrices remain unchanged across all propagation layers and are updated only after the GNN completes its \(K\)-layer message propagations. To evaluate its effectiveness, we compare it against the eager multi-_layer_ update (described at the end of Section 3.2), where the GNN executes its \(K\)-layer message propagations only once; the alignment map is updated across \(K\) layers; and, the alignment at \(k\)th layer is used to compute the embeddings at \((k+1)\)th layer. In Table 4, we compare the performance in terms MAP, which shows that lazy multi-round updates significantly outperform multi-layer updates.

Node partner vs. node-pair partner interactionTo understand the benefits of node-pair partner interaction, we contrast IsoNet++ (Node) against another variant of our method, which performs _node partner_ interaction rather than node pair partner interaction, similar to Eq. (7). For lazy multi-round updates, we compute the embeddings as follows:

\[_{t+1,k+1}^{(q)}(u)=_{}(_{t+1,k}^{(q)}(u),\, _{v(u)}_{}(_{t,k}^{(q)}(u),_{ t,k}^{(q)}(v)),\,_{u^{} V_{c}}_{t}[u,u^{}]_{t,k}^{(c)}(u^{ }))\]

For eager multi-layer updates, we compute the embeddings as:

\[_{k+1}^{(q)}(u)=_{}(_{k}^{(q)}(u),\,_{v (u)}_{}(_{k}^{(q)}(u),_{k}^{(q)}(v)),\,_{u^{} V_{c}}_{k}[u,u^{}]_{k}^{(c)}(u^{ }))\]

   & AIDS & Mutag & FM & FR & MM & MR \\   Eager \\ Lazy \\  } & 0.756 & 0.81 & 0.859 & 0.802 & 0.827 & 0.841 \\  & **0.825** & **0.851** & **0.888** & **0.855** & **0.838** & **0.874** \\   Eager \\ Lazy \\  } & 0.795 & 0.805 & 0.883 & 0.812 & 0.862 & 0.886 \\  & **0.847** & **0.858** & **0.902** & **0.875** & **0.902** & **0.902** \\  

Table 4: Lazy multi-round vs. eager multi-layer. First (Last) two rows report MAP for IsoNet++ (Node) (IsoNet++ (Edge)). **Green** shows the best method.

   &  &  \\   & AIDS & Mutag & FM & FR & MM & MR & AIDS & Mutag & FM & FR & MM & MR \\   GraphSim  & 0.356 & 0.472 & 0.477 & 0.423 & 0.415 & 0.453 & 0.145 & 0.257 & 0.261 & 0.227 & 0.212 & 0.23 \\ GOTSim  & 0.324 & 0.272 & 0.355 & 0.373 & 0.323 & 0.317 & 0.112 & 0.088 & 0.147 & 0.166 & 0.119 & 0.116 \\ SimCNN  & 0.341 & 0.283 & 0.473 & 0.341 & 0.298 & 0.379 & 0.138 & 0.087 & 0.235 & 0.155 & 0.111 & 0.160 \\ EGSC  & 0.505 & 0.476 & 0.609 & 0.607 & 0.586 & 0.58 & 0.267 & 0.243 & 0.364 & 0.382 & 0.484 & 0.325 \\ HEMN  & 0.267 & 0.276 & 0.436 & 0.412 & 0.312 & 0.243 & 0.076 & 0.084 & 0.200 & 0.189 & 0.119 & 0.069 \\ Neuromatch  & 0.489 & 0.576 & 0.615 & 0.559 & 0.519 & 0.606 & 0.262 & 0.376 & 0.389 & 0.350 & 0.282 & 0.385 \\ GREED  & 0.472 & 0.567 & 0.558 & 0.512 & 0.546 & 0.528 & 0.245 & 0.371 & 0.316 & 0.287 & 0.311 & 0.277 \\ GEN  & 0.557 & 0.605 & 0.661 & 0.575 & 0.539 & 0.631 & 0.321 & 0.429 & 0.448 & 0.368 & 0.292 & 0.391 \\ GMN  & 0.622 & 0.710 & 0.730 & 0.662 & 0.655 & 0.708 & 0.397 & 0.544 & 0.537 & 0.453 & 0.423 & 0.49 \\ IsoNet (Node)  & 0.659 & 0.697 & 0.729 & 0.68 & 0.708 & 0.738 & 0.438 & 0.509 & 0.525 & 0.475 & 0.493 & 0.532 \\ IsoNet (Edge)  & 0.690 & 0.706 & 0.783 & 0.722 & 0.753 & 0.774 & 0.479 & 0.529 & 0.613 & 0.538 & 0.571 & 0.601 \\  IsoNet++ (Node) & 0.825 & 0.851 & 0.888 & 0.855 & 0.838 & 0.874 & 0.672 & 0.732 & 0.797 & 0.737 & 0.702 & 0.755 \\ IsoNet++ (Edge) & **0.847** & **0.858** & **0.902** & **0.875** & **0.902** & **0.902** & **0.705** & **0.749** & **0.813** & **0.769** & **0.809** & **0.803** \\  

Table 3: Comparison of the two variants of IsoNet++ (IsoNet+ (Node) and IsoNet+ (Edge)) against all the state-of-the-art graph retrieval methods, across all six datasets. Performance is measured in terms average precision (MAP) and mean HITS@20. In all cases, we used \(60\%\) training, \(15\%\) validation and \(25\%\) test sets. The numbers highlighted with green and yellow indicate the best, second best method respectively, whereas the numbers with blue indicate the best method among the baselines. (MAP values for IsoNet++ (Edge) across FM, MM and MR were verified to be not exactly the same, but they match up to the third decimal place.)

   & AIDS & Mutag & FM & FR & MM & MR \\   Eager \\ Lazy \\  } & 0.776 & 0.829 & 0.851 & 0.819 & **0.844** & 0.84 \\  & **0.825** & **0.851** & **0.888** & **0.85** & **0.838** & **0.874** \\   Eager \\ Lazy \\  } & 0.668 & 0.783 & 0.821 & 0.752 & 0.753 & 0.794 \\  & **0.756** & **0.81** & **0.859** & **0.802** & **0.827** & **0.841** \\  

Table 5: Node partner vs. node pair partner interaction. First (Last) two rows report MAP for multi-round (multi-layer) update. **Green** shows the best method.

Table 5 summarizes the results, which shows that IsoNet++ (Node) (node partner pair) performs significantly better than Node partner for both multi-round lazy updates (top-two rows) and multi-layer eager updates (bottom tow rows).

Quality of injective alignmentsNext we compare between multi-round and multi-layer update strategies in terms of their ability to refine the alignment matrices, as the number of updates of these matrices increases. For multi-round (layer) updates, we instrument the alignments \(_{t}\) and \(_{t}\) (\(_{k}\) and \(_{k}\)) for different rounds \(t[T]\) (layers \(k[K]\)). Specifically, we look into the distribution of the similarity between the learned alignments \(_{t},_{t}\) and the correct alignments \(^{*},^{*}\) (using combinatorial routine), measured using the inner products \((_{t}^{}^{*})\) and \((_{t}^{}^{*})\) for different \(t\). Similarly, we compute \((_{k}^{}^{*})\) and \((_{k}^{}^{*})\) for different \(k[K]\). Figure 6 summarizes the results, which shows that **(i)** as \(t\) or \(k\) increases, the learned alignments become closer to the gold alignments; **(2)** multi-round updates refine the alignments approximately twice as faster than the multi-layer variant. The distribution of \((_{t}^{}^{*})\) at \(t=1\) in multi-round strategy is almost always close to \((_{k}^{}^{*})\) for \(k=2\). Note that, our aligner networks learn to refine the \(_{t}\) and \(_{t}\) through end-to-end training, without using any form of supervision from true alignments or the gradient computed in Eq. (6).

Accuracy-inference time trade-offHere, we analyze the accuracy and inference time trade-off. We vary \(T\) and \(K\) for IsoNet++'s lazy multi-round variant, and vary \(K\) for IsoNet++'s eager multi-layer variant and for GMN. Figure 7 summarizes the results. Notably, the eager multi-layer variant achieves the highest accuracy for \(K=8\) on the AIDS dataset, despite the known issue of oversmoothing in GNNs for large \(K\). This unexpected result may be due to our message passing components, which involve terms like \(_{u^{}}[u,u^{}](u^{})\), effectively acting as a convolution between alignment scores and embedding vectors. This likely enables \(\) to function as a filter, countering the oversmoothing effect.

## 5 Conclusion

We introduce IsoNet++ as an early-interaction network for estimating subgraph isomorphism. IsoNet++ learns to identify explicit alignments between query and corpus graphs despite having access to only pairwise preferences and not explicit alignments during training. We design a graph neural network (GNN) that uses an alignment estimate to propagate messages, then uses the GNN's output representations to refine the alignment. Experiments across several datasets confirm that alignment refinement is achieved over several rounds. Design choices such as using node-pair partner interaction (instead of node partner) and lazy updates (over eager) boost the performance of our architecture, making it the state-of-the-art in subgraph isomorphism based subgraph retrieval. We also demonstrate the accuracy v/s inference time trade offs for IsoNet++, which show how different knobs can be tuned to utilize our models under regimes with varied time constraints.

This study can be extended to graph retrieval problems which use different graph similarity measures, such as maximum common subgraph or graph edit distance. Extracting information from node-pairs is effective and can be widely used to improve GNNs working on multiple graphs at once.

Figure 6: Empirical probability density of similarity between the estimated alignments and the true alignments \(^{*},^{*}\) for both multi-round and multi-layer update strategies across different stages of updates (\(t\) for multi-round and \(k\) for multi-layer), for AIDS. Similarly is measured using \(p((_{t}^{}^{*})),p((_{t}^{}^ {*}))\) for multi-round lazy updates and \(p((_{k}^{}^{*})),p((_{k}^{}^ {*}))\) for multi-layer eager updates.

Figure 7: Trade-off between MAP and inference time (batch size=128).