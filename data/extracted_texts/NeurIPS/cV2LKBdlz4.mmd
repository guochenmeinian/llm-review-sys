# On Statistical Rates and Provably Efficient Criteria

of Latent Diffusion Transformers (DiTs)

 Jerry Yao-Chieh Hu1,2 Weimin Wu1,2 Zhuoru Li3

Sophia Pi3 Zhao Song4 Han Liu2

2Center for Foundation Models and Generative AI, 3Department of Computer Science, 3Department of Statistics

and Data Science, Northwestern University, Evanston, IL 60208, USA

4Simons Institute for the Theory of Computing, UC Berkeley, Berkeley, CA 94720, USA

{jhu,wum}@u.northwestern.edu;

magic.linuukde@gmail.com; hanliu@northwestern.edu

Equal contribution. Version: January 3, 2025. Future updates are on arXiv.

###### Abstract

We investigate the statistical and computational limits of latent Diffusion Transformers (DiTs) under the low-dimensional linear latent space assumption. Statistically, we study the universal approximation and sample complexity of the DiTs score function, as well as the distribution recovery property of the initial data. Specifically, under mild data assumptions, we derive an approximation error bound for the score network of latent DiTs, which is sub-linear in the latent space dimension. Additionally, we derive the corresponding sample complexity bound and show that the data distribution generated from the estimated score function converges toward a proximate area of the original one. Computationally, we characterize the hardness of both forward inference and backward computation of latent DiTs, assuming the Strong Exponential Time Hypothesis (SETH). For forward inference, we identify efficient criteria for all possible latent DiTs inference algorithms and showcase our theory by pushing the efficiency toward almost-linear time inference. For backward computation, we leverage the low-rank structure within the gradient computation of DiTs training for possible algorithmic speedup. Specifically, we show that such speedup achieves almost-linear time latent DiTs training by casting the DiTs gradient as a series of chained low-rank approximations with bounded error. Under the low-dimensional assumption, we show that the statistical rates and the computational efficiency are all dominated by the dimension of the subspace, suggesting that latent DiTs have the potential to bypass the challenges associated with the high dimensionality of initial data.

## 1 Introduction

We investigate the statistical and computational limits of latent diffusion transformers (DiTs), assuming the data is supported on an unknown low-dimensional linear subspace. This analysis is not only practical but also timely. On one hand, DiTs have demonstrated revolutionary success in generative AI and digital creation by using Transformers as score networks [Eser et al., 2024, Ma et al., 2024, Chen et al., 2024a, Mo et al., 2023, Peebles and Xie, 2023]. On the other hand, they require significant computational resources [Liu et al., 2024], making them challenging to train outside of specialized industrial labs. Therefore, it is natural to ask whether it is possible to make them lighter and faster without sacrificing performance. Answering these questions requires a fundamental understanding of the DiT architecture. This work provides a timely theoretical analysis of the fundamental limits of DiT architecture, aided by the analytical feasibility provided by the low-dimensional data assumption.

Empirically, Latent Diffusion is a go-to design for effectiveness and computational efficiency (Rombach et al., 2022; Liu et al., 2021; Pope et al., 2021; Su and Wu, 2018). Theoretically, it is capable of hosting the assumption of low-dimensional data structure (see Assumption 2.1 for formal definition) for detailed analytical characterization (Chen et al., 2023; Bortoli, 2022). In essence, diffusion models with low-dimensional data structures manifest a natural lower-dimensional diffusion process through an encoder/decoder within a robust and informative latent representation feature space (Rombach et al., 2022; Pope et al., 2021). Such lower-dimensional diffusion improves computational efficiency by reducing data complexity without sacrificing essential information (Liu et al., 2021). With this assumption, Chen et al. (2023) decompose the score function of U-Net based diffusion models into on-support and orthogonal components. This decomposition allows for the characterization of the distinct behaviors of the two components: the on-support component facilitates latent distribution learning, while the orthogonal component facilitates subspace recovery.

In our work, we utilize low-dimensional data structure assumption to explore statistical and computational limits of latent DiTs. Our analysis includes the characterizations of statistical rates and provably efficient criteria. Statistically, we pose two questions and provide a theory to characterize the statistical rates of latent DiT under the assumption of a low-dimensional data:

**Question 1**.: What is the approximation limit of using transformers to approximate the DiT score function, particularly in the low-dimensional data subspace?

**Question 2**.: How accurate is the estimation limit for such a score estimator in practical training scenarios? With the score estimator, how well can diffusion transformers recover the data distribution?

Computationally, the primary challenge of DiT lies in the transformer blocks' quadratic complexity. This computational burden applies to both inference and training, even with latent diffusion. Thus, it is essential to design algorithms and methods to circumvent this \((L^{2})\) where \(L\) is the latent DiT sequence length. However, there are no formal results to support and characterize such algorithms. To address this gap, we pose the following questions and provide a fundamental theory to fully characterize the complexity of latent DiT under the low-dimensional linear subspace data assumption:

**Question 3**.: Is it possible to improve the \((L^{2})\) time complexity with a bounded approximation error for both forward and backward passes? What is the computational limit for such an improvement?

Contributions.We study the fundamental limits of latent DiT. Our contributions are threefold:

* **Score Approximation.** We address Question 1 by characterizing the approximation limit of matching the DiT score function with a transformer-based score estimator. Specifically, under mild data assumptions, we derive an approximation error bound for the score network, sub-linear in the latent space dimension (Theorem 3.1). These results not only explain the expressiveness of latent DiT (under mild assumptions) but also provide guidance for the structural configuration of the score network for practical implementations (Theorem 3.1).
* **Score and Distribution Estimation.** We address Question 2 by exploring the limitations of score and distribution estimations of latent DiTs in practical training scenarios. Specifically, we provide a sample complexity bound for score estimation (Theorem 3.2), using norm-based covering number bound of transformer architecture. Additionally, we show that the learned score estimator is able to recover the initial data distribution (Corollary 3.2.1).
* **Provably Efficient Criteria and Existence of Almost Linear Time Algorithms.** We address Question 3 by providing provably efficient criteria for latent DiTs in both forward inference and backward computation/training. For forward inference, we characterize all possible efficient DiT algorithms using a norm-based efficiency threshold for both conditional and unconditional generation (Proposition 4.1). Efficient algorithms, including almost-linear time algorithms (Proposition 4.2), are possible only below this threshold. For backward computation, we prove the existence of almost-linear time DiT training algorithms (Theorem 4.1) by utilizing the inherent low-rank structure in DiT gradients through a chained low-rank approximation.

Interestingly, both our statistical and computational results are dominated by the subspace dimension under the low-dimensional assumption, suggesting that latent DiT can potentially bypass the challenges associated with the high dimensionality of initial data.

Organization.Section 2 includes background on score decomposition and Transformer-based score networks. Section 3 includes DiTs' statistical rates. Section 4 includes DiTs' provably efficient criteria. Section 5 includes concluding remarks. We defer discussions of related works to Appendix C.

Notations.We use lower case letters to denote vectors, e.g., \(z^{D}\). \( z_{2}\) and \( z_{}\) denote its Euclidean norm and Infinite norm respectively. We use upper case letters to denote matrix, e.g., \(Z^{d L}\). \( Z_{2}\), \( Z_{}\), and \( Z_{F}\) denote the \(2\)-norm, operator norm and Frobenius norm respectively. \( Z_{p,q}\) denotes the \(p,q\)-norm where the \(p\)-norm is over columns and \(q\)-norm is over rows. Given a function \(f\), let \( f(x)_{L^{2}}( f(x) _{2}^{2}x)^{1/2}\), and \( f()_{Lip}=_{x y}( f(x)-f(y) _{2}/ x-y_{2})\). With a distribution \(P\), we denote \( f_{L^{2}(P)}=(_{p} f(x)_{2} ^{2}x)^{1/2}\) as the \(L^{2}(P)\) norm. Let \(f_{}P\) be a pushforward measure, i.e., for any measurable \(\), \((f_{}P)()=P(f^{-1}())\). We use \(\) for (conditional) Gaussian density functions.

## 2 Background

This section reviews the ideas we built on, including an overview of diffusion models (Section 2.1), the score decomposition under the linear latent space assumption (Section 2.2), and the transformer backbone in DiT (Section 2.3).

### Score-Matching Denoising Diffusion Models

We briefly review forward process, backward process and score matching in diffusion models.

Forward and Backward Process.In the **forward** process, Diffusion models gradually add noise to the original data \(x_{0}^{D}\), and \(x_{0} P_{0}\). Let \(x_{t}\) denote the noisy data at the timestamp \(t\), with marginal distribution and destiny as \(P_{t}\) and \(p_{t}\). The conditional distribution \(P(x_{t}|x_{0})\) follows \(N((t)x_{0},(t)I_{D})\), where \((t)=(-_{0}^{t}w(s)s/2)\), \((t)=1-^{2}(t)\), and \(w(t)>0\) is a nondecreasing weighting function. In practice, the forward process terminates at a large enough \(T\) such that \(P_{T}\) is close to \(N(0,I_{D})\). In the **backward** process, we obtain \(y_{t}\) by reversing the forward process. The generation of \(y_{t}\) depends on the score function \( p_{t}()\). However, this is unknown in practice, we use a score estimator \(s_{W}(,t)\) to replace \( p_{t}()\), where \(s_{W}(,t)\) is usually a neural network with parameters \(W\). See Appendix D.1 for the details.

Score Matching.To estimate the score function, we use the following loss

\[_{W}_{T_{0}}^{T}(t)_{x_{t} P_{t}}[ s _{W}(x_{t},t)- p_{t}(x_{t})_{2}^{2}]t,\]

where \((t)\) is the weight function, and \(T_{0}\) is a small value to stabilize training and prevent score function from blowing up (Vahdat et al., 2021). However, it is hard to compute \( p_{t}()\) with available data samples. Therefore, we minimize the equivalent denoising score matching objective

\[_{W}_{T_{0}}^{T}(t)_{x_{0} P_{0}}[_ {x_{t}|x_{0}}[ s_{W}(x_{t},t)-_{x_{t}}_{t}(x_{t } x_{0})_{2}^{2}]]t, \]

where \(_{t}(x_{t}|x_{0})\) is the transition kernel, then \(_{x_{t}}_{t}(x_{t}|x_{0})=((t)x_{0}-x_{t})/(t)\).

To train the parameters \(W\) in the score estimator \(s_{W}(,t)\), we use the empirical version of (2.1). We select \(n\) i.i.d. data samples \(\{x_{0,i}\}_{i=1}^{n} P_{0}\), and sample time \(t_{i}\)\((1 i n)\) uniformly from interval \([T_{0},T]\). Given \(x_{0,i}\), we sample \(x_{t_{i}}\) from \(N((t_{i})x_{0,i},(t_{i})I_{D})\). The empirical loss is

\[}(W)=_{i=1}^{n} s_{W}(x_{t_{i}},t_{i})-x_{0,i}_{2}^{2}. \]

For convenience of notation, we denote population loss \((W)=_{P_{0}}[}(W)]\).

### Score Decomposition in Linear Latent Space

In this part, we review the score decomposition in (Chen et al., 2023). We consider that the \(D\)-dimensional input data \(x\) supported on a \(d_{0}\)-dimensional subspace, where \(d_{0} D\).

**Assumption 2.1** (Low-Dimensional Linear Latent Space).: Let \(x\) denote the initial data at \(t=0\). \(x\) has a latent representation via \(x=Bh\), where \(B^{D d_{0}}\) is an unknown matrix with orthonormal columns. The latent variable \(h^{d_{0}}\) follows the distribution \(P_{h}\) with a density function \(p_{h}\).

**Remark 2.1**.: By "linear latent space," we mean that each entry of a given latent vector is a linear combination of the corresponding input, i.e., \(x=Bh\). This is also known as the "low-dimensional data" assumption in literature (Chen et al., 2023).

Let \(\) and \(\) denote the perturbed data and its associated latent variable at \(t>0\), respectively. Based on the low-dimensional data structure assumption, we have the following score decomposition theory: on-support score \(s_{+}(B^{},t)\) and orthogonal score \(s_{-}(,t)\).

**Lemma 2.1** (Score Decomposition, Lemma 1 of [Chen et al., 2023]).: Let data \(x=Bh\) follow Assumption 2.1. The decomposition of score function \( p_{t}()\) is

\[ p_{t}()=^{h}( )}_{s_{+}(,t)}-BB^{})/(t)}_{s_{-}(,t)},=B^{}, \]

where \(p_{t}^{h}()_{t}(|h)p_{h}(h)h\), \(_{t}(|h)\) is the Gaussian density function of \(N((t)h,(t)I_{d_{0}})\), \((t)=e^{-t/2}\) and \((t)=1-e^{-t}\). We restate the proof in Appendix D.2 for completeness.

Additionally, our theoretical analysis is based on two following assumptions as in [Chen et al., 2023].

**Assumption 2.2** (Tail Behavior of \(P_{h}\)).: The density function \(p_{h}>0\) is twice continuously differentiable. Moreover, there exist positive constants \(A_{0}\), \(A_{1},A_{2}\) such that when \( h_{2} A_{0}\), the density function \(p_{h}(h)(2)^{-d_{0}/2}A_{1}(-A_{2}\|h\|_{2}^{2}/2)\).

**Assumption 2.3** (\(L_{s_{+}}\)-Lipschitz of \(s_{+}(,t)\)).: The on-support score function \(s_{+}(,t)\) is \(L_{s_{+}}\)-Lipschitz in \(^{d_{0}}\) for any \(t[0,T]\).

### Score Network and Transformers

In this part, we introduce the score network architecture and Transformers. Transformers are the backbone of the score network in DiT. By Assumption 2.1, \(=B^{}^{d_{0}}\) with \(d_{0}<D\).

(Latent) Score Network.Following [Chen et al., 2023], we rearrange (2.3) into

\[ p_{t}()=B^{h}( ))+B^{})/(t)-/ (t)}_{:=q(B^{},t):^{d_{0}}[T_{0},T] ^{d_{0}}} \]

We use \(W_{B}^{D d_{0}}\) to approximate \(B^{D d_{0}}\), and a neural network \(f(W_{B}^{},t)\) to approximate \(q(B^{},t)\). We adopt the following score network class for diffusion in latent space (i.e., in \(^{d_{0}}\))

\[=\{s_{W}(,t)=W_{B}f(W_{B}^{},t)/ (t)-/(t),\;W=\{W_{B},f\}\}, \]

where the columns in \(W_{B}\) are orthogonal, \(f:^{d_{0}}[T_{0},T]^{d_{0}}\) is a neural network. In this work, we focus on the diffusion transformers (DiTs), i.e., using Transformer for \(f\)[Peebles and Xie, 2023].

Transformers.A Transformer block consists of a self-attention layer and a feed-forward layer, with both layers having skip connection. We use \(^{r,m,l}:^{d L}^{d L}\) to denote a Transformer block. Here \(r\) and \(m\) are the number of heads and head size in self-attention layer, and \(l\) is the hidden dimension in feed-forward layer. Let \(X^{d L}\) be the model input, then we have the model output

\[(X)=X+_{i=1}^{r}W_{O}^{i}W_{V}^{i}X((W_{K}^{i}X)^{}W_{Q}^{i}X), \]

\[(X)=(X)+W_{2}(W_{ 1}(X)+b_{1}_{L}^{})+b_{2}_{L}^ {}, \]

where \(W_{K}^{i},W_{Q}^{i},W_{V}^{i}^{m d},W_{O}^{i}^{ d m},W_{1}^{l d},W_{2}^{d l},b_{1} ^{l},b_{2}^{d}\).

In our work, we use Transformer networks with positional encoding \(E^{d L}\). We define the Transformer networks as the composition of Transformer blocks

\[_{P}^{r,m,l}=\{f_{}:^{d L}^ {d L} f_{}^{r,m,l}\}.\]

For example, the following is a Transformer network consisting \(K\) blocks and positional encoding

\[f_{}(X)=^{(K)}^{(K)}^{(1)}^{(1)}(X+E). \]

## 3 Statistical Rates of Latent DiTs with Subspace Data Assumption

In this section, we analyze the statistical rates of latent DiTs. Section 3.1 introduces the class of latent DiT score networks. In Section 3.2, we prove the approximation limit of matching the DiT score function with the score network class, and characterize the structural configuration of the score network when a specified approximation error is required. Following this, in Section 3.3, utilizing the characterized structural configuration, we prove the score and distribution estimation for latent DiTs.

### DiT Score Network Class

Here, we provide the details about DiT score network class used in our analysis. In (2.5), \(f\) is a network with Transformer as the backbone, and \((,t)^{d_{0}}[T_{0},T]\) denotes the input data. Following , DiT uses time point \(t\) to calculate the scale and shift value in the Transformer backbone, and it transforms an input picture into a sequential version. To achieve the transformation, we introduce a reshape layer.

**Definition 3.1** (DiT Reshape Layer \(R()\)).: Let \(R():^{d_{0}}^{d L}\) be a reshape layer that transforms the \(d_{0}\)-dimensional input into a \(d L\) matrix. Specifically, for any \(d_{0}=i i\) image input, \(R()\) converts it into a sequence representation with feature dimension \(d p^{2}\) (where \(p 2\)) and sequence length \(L(i/p)^{2}\). Besides, we define the corresponding reverse reshape (flatten) layer \(R^{-1}():^{d L}^{d_{0}}\) as the inverse of \(R()\). By \(d_{0}=dL\), \(R,R^{-1}\) are associative w.r.t. their input.

To simplify the self-attention block in (2.6), let \(W_{OV}^{i}=W_{O}^{i}W_{V}^{i}\) and \(W_{KQ}^{i}=(W_{K}^{i})^{}W_{Q}^{i}\).

**Definition 3.2** (Transformer Network Class \(_{p}^{r,m,l}\)).: We define the Transformer network class as

\(_{p}^{r,m,l}(K,C_{},C_{OV}^{2,},C_{OV},C_{KQ}^{2, },C_{KQ},C_{F}^{2,},C_{F},C_{E},L_{})\), satisfying the constraints

* Model architecture with \(K\) blocks: \(f_{}(X)=^{(K)}(K) ^{(1)}(^{1})(X)\);
* Model output bound: \(_{X} f_{}(X)_{2} C_{}\);
* Parameter bound in \((^{i})\): \((W_{OV}^{i})^{}_{2,} C_{OV}^{2,}\), \((W_{OV}^{i})^{}_{2} C_{OV}\), \( W_{KQ}^{i}_{2,} C_{KQ}^{2,}\), \( W_{KQ}^{i}_{2} C_{KQ}\), \( E^{}_{2,} C_{E}, i[K]\);
* Parameter bound in \(^{(i)}\): \( W_{g}^{i}_{2,} C_{F}^{2,}\), \( W_{g}^{i}_{2} C_{F}, j,i[K]\);
* Lipschitz of \(f_{}\): \( f_{}(X_{1})-f_{}(X_{2})_{F} L _{}\|X_{1}-X_{2}\|_{F}, X_{1},X_{2}^{d L}\).

**Definition 3.3** (DiT Score Network Class \(_{_{p}^{r,m,l}}\) (Figure 1)).: We denote \(_{_{p}^{r,m,l}}\) as the DiT score network class in (2.5), replacing \(f\) with \(R^{-1} f_{} R\), and \(f_{}\) is from the Transformer class \(_{p}^{r,m,l}\).

### Score Approximation of DiT

Here, we explore the approximation limit of latent DiT score network class \(_{_{p}^{r,m,l}}\) under linear latent space assumption. Recall that \(P_{t}\) is the distribution of \(x_{t}\), \((t)\) is the variance of \(P(x_{t}|x_{0})\), \(d_{0}\) is the dimension of latent space, \(L\) is the sequence length of transformer input, \(T\) is the stopping time in forward process, \(T_{0}\) is the early stopping time in backward process, and \(L_{s_{+}}\) is the Lipschitz coefficient of on-support score function. Then we have the following Theorem3.1.

**Theorem 3.1** (Score Approximation of DiT).: For any approximation error \(>0\) and any data distribution \(P_{0}\) under Assumptions2.1 to 2.3, there exists a DiT score network \(s_{}\) from \(_{_{p}^{2,1,4}}\) (defined in Definition3.2), where \(=\{_{B},_{}\}\), such that for any \(t[T_{0},T]\), we have:

\[ s_{}(,t)- p_{t}()_{L^{2 }(P_{t})}}/(t),\]

where \((t)=1-e^{-t}\), and the upper bound of hyperparameters in \(_{_{p}^{2,1,4}}\) are

\[K=(^{-2L}),\;C_{}=(d_{0}L_{s_{+} }(d_{0}/T_{0})+(1/)}),\]

Figure 1: **Overview of DiT Score Network Architecture \(s_{W}(,t)\). \(W_{B}^{T}\) denotes the linear layer from the input data space to the linear latent space. \(f()=R^{-1} f_{} R()\) denotes the transformer network \(f_{}()\) with reshaping layer \(R()\), where \(f_{}()_{p}^{r,m,l}\). \(W_{B}\) denotes the linear layer from the linear latent space to the input data space. \((t)\) denote the variance of the conditional distribution \(P(x_{t} x_{0})\).**\[C_{OV}^{2,}=(1/)^{(1)},\ C_{OV}=(1/ )^{(1)},\ C_{KQ}^{2,}=(1/)^{(1)},\ C_{KQ}=(1/)^{(1)},\] \[C_{E}=(L^{3/2}),\,C_{F}^{2,}=(1/)^{ (1)},\,C_{F}=(1/)^{(1)},\,L_{}= (d_{0}L_{s+}).\]

Proof Sketch.: Our proof is built on the key observation that there is a tail behavior of the low-dimensional latent variable distribution \(P_{h}\) (Assumption 2.2). Recall that \( p_{t}()=Bq(,t)/(t)-/ (t)\), where \(=B^{}\) (defined in (2.4)). By taking \(_{B}=B\), our aim reduces to construct a transformer network to approximate \(q(,t)\). To achieve this, we firstly approximate \(q(,t)\) with a compact-supported continuous function, based on the tail behavior of \(P_{h}\). Then we construct a transformer to approximate the compact-supported continuous function using the universal approximation capacity of transformer (Yun et al., 2020). See Appendix F.1 for a detailed proof. 

Intuitively, Theorem 3.1 indicates the capability of the transformer-based score network to approximate the score function with precise guarantees. Furthermore, Theorem 3.1 provides empirical guidance for the design choices of the score network when a specified approximation error is required.

**Remark 3.1** (Comparing with Existing Works).: Theoretical analysis of DiTs is limited. Previous works that do not specify the model architecture assume that the score estimator is well-approximated (Benton et al., 2024; Wibisono et al., 2024). To the best of our knowledge, this work is the first to present an approximation theory for DiTs, offering the estimation theory in Theorem 3.2 and Corollary 3.2.1 based on the estimated score network, rather than a perfectly trained one.

**Remark 3.2** (Latent Dimension Dependency).: Theorem 3.1 suggests that the approximation capacity and Transformer network size primarily depend on the latent variable dimension \(d_{0}=d L\). This indicates that DiTs can potentially bypass the challenges associated with the high dimensionality of initial data by transforming input data into a low-dimensional latent variable.

### Score Estimation and Distribution Estimation

Besides score approximation capability, Theorem 3.1 also characterizes the structural configuration of the score network for any specific precision, e.g., \(K,C_{E},C_{F}\), etc. This characterization enables further analysis of the performance of score network in practical scenarios. In Theorem 3.2, we provide a sample complexity bound for score estimation. In Corollary 3.2.1, show that the learned score estimator is able to recover the initial data distribution.

Score Estimation.To derive a sample complexity for score estimation using \(_{_{p}^{2,1,4}}\), we rewrite the score matching objective in (2.2) as \(*{argmin}_{s_{W}_{_{p}^{2, 1,4}}}}(s_{W}),\ =\{_{B},_{ }\}\).

Theorem 3.2 shows that as sample size \(n\), \(s_{W}(,t)\) convergences to \( p_{t}()\).

**Theorem 3.2** (Score Estimation of DiT).: Under Assumptions 2.1 to 2.3, we choose \(_{_{p}^{2,1,4}}\) as in Theorem 3.1 using \((0,1)\) and \(L>1\), With probability \(1-1/(n)\), we have

\[}_{T_{0}}^{T}\|s_{}(,t)-  p_{t}()\|_{L^{2}(P_{t})}t=}(T_{0}T} 2^{(1/)^{2L}}+T_{0}T}+ T}^{2}), \]

where \(}\) hides the factors related to \(D,d_{0},d,L_{s+}\), and \( n\).

Proof.: See Appendix F.2 for a detailed proof. 

Intuitively, Theorem 3.2 shows a sample complexity bound for score estimation in practice.

**Remark 3.3** (Comparing with Existing Works).: (Zhu et al., 2023) provides a sample complexity for simple ReLU-based diffusion models under the assumption of an accurate score estimator. To the best of our knowledge, we are the first to provide a sample complexity for DiTs, based on the learned score network in Theorem 3.1 and the quantization (piece-wise approximation) approach for transformer universality (Yun et al., 2020). Furthermore, our first term shows a convergence rate of \(1/T\), outperforming (Chen et al., 2023), in which the first term is independent of \(T\).

**Remark 3.4** (Double Exponential Factor and Inconsistent Convergence).: Theorem3.2 reports an explicit result on sample complexity bounds for score estimation of latent DiTs: a double exponential factor \(2^{(1/)^{2L}}\) in the first term. We remark that this arises from the required depth \(K\) is \((^{-2L})\), and the norm of required weight parameters is \((1/)^{(1)}\) as shown in Theorem3.1, assuming the universality of transformers requires dense layers (Yun et al., 2020). This double exponential factor causes inconsistent convergence with respect to sample size \(n\), as its large value prevents setting \(\) as a function of \(n\) to balance the first and second terms in (3.1). This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work.

**Definition 3.4**.: For later convenience, we define \((n,,L):=} 2^{(1/)^{2L}}+}+^{2}\).

Distribution Estimation.In practice, DiTs generate data using the discretized version with step size \(\), see AppendixD.1 for details. Let \(_{T_{0}}\) be the distribution generated by \(s_{}\) using the discretized backward process in Theorem3.2. Let \(P^{h}_{T_{0}}\) and \(p^{h}_{T_{0}}\) be the distribution and density function of on-support latent variable \(\) at \(T_{0}\). We have the following results for distribution estimation.

**Corollary 3.2.1** (Distribution Estimation of DiT, Modified From Theorem 3 of (Chen et al., 2023)).: Let \(T=( n),T_{0}=(\{c_{0},1/L_{s_{+}}\})\), where \(c_{0}\) is the minimum eigenvalue of \(_{P_{h}}[hh^{}]\). With the estimated DiT score network \(s_{}\) in Theorem3.2, we have the following with probability \(1-1/(n)\).

1. The accuracy to recover the subspace \(B\) is \(\|W_{B}W_{B}^{}-BB^{}\|_{F}^{2}=}( (n,,L)/c_{0})\).
2. With the conditions \((P_{h}||N(0,I_{d_{0}}))<\), there exists an orthogonal matrix \(U^{d d}\) such that we have the following upper bound for the total variation distance \[(P^{h}_{T_{0}},(W_{B}U)_{}^{}_{T_{0}})= }(),\] (3.2) where \(}\) hides the factor about \(D,d_{0},d,L_{s_{+}}, n\), and \(T-T_{0}\). and \((W_{B}U)_{}^{}_{T_{0}}\) denotes the pushforward distribution.
3. For the generated data distribution \(_{T_{0}}\), the orthogonal pushforward \((I-W_{B}W_{B}^{})_{}_{T_{0}}\) is \(N(0,)\), where \( aT_{0}I\) for a constant \(a>0\).

Proof.: See AppendixF.3 for a detailed proof. 

Intuitively, Corollary3.2.1 shows the estimation results in 3 parts: (i) the accuracy of recovering the subspace \(B\); (ii) the estimation error between \(_{T_{0}}\) and \(P^{h}_{T_{0}}\); and (iii) the vanishing behavior of \(_{T_{0}}\) in the orthogonal space. These indicate that the learned score estimator is capable of recovering the initial data distribution. Notably, Corollary3.2.1 is agnostic to the specifics of \((n,,L)\).

**Remark 3.5** (Comparing with Existing Works).: Oko et al. (2023) analyze the distribution estimation under the assumption that the initial density is supported on \([-1,1]^{D}\) and smooth in the boundary. Our Assumption2.2 demonstrates greater practical relevance. This suggests that our method of distribution estimation aligns more closely with empirical realities.

**Remark 3.6** (Subspace Recovery Accuracy).: (i) of Corollary3.2.1 confirms that the subspace is learned by DiTs. The error is proportional to the sample complexity for score estimation and depends on the minimum eigenvalue of the covariance of \(P_{h}\).

## 4 Provably Efficient Criteria

Here, we analyze the computational limits of latent DiTs under low-dimensional linear subspace data assumption (i.e., Assumption2.1). The hardness of DiT models ties to both forward and backward passes of the score network in Definition3.3. We characterize them separately.

### Computational Limits of Backward Computation

Following Section2, suppose we have \(n\) i.i.d. data samples \(\{x_{0,i}\}_{i=1}^{n} P_{d}\), and time \(t_{i_{0}}\)\((1 i n)\) uniformly sampled from \([T_{0},T]\). For each data \(x_{0,i}^{D}\), we sample \(x_{t_{i_{0}}}^{D}\) from \(N((t_{i_{0}})x_{0,i},(t_{i_{0}})I_{D})\). Let \((W_{A}R^{-1}())^{}\) be the inverse transformation of \(W_{A}R^{-1}()\), and denote\(Y_{0,i}(W_{A}R^{-1})^{}(x_{0,i})^{d L}\). We rewrite the empirical denoising score-matching loss (2.2) as

\[_{i=1}^{n}W_{A}R^{-1}(f_{}(R( ^{}x_{t_{0}})}_{d_{0} 1}))-x_{0,i}_{F}^{2}= _{i=1}^{n}}_{D d_{0}} (}(R(W_{A}^{}x_{t_{0}})}^{d  L})}_{d_{0} 1})-Y_{0,i})_{F}^{2}. \]

For efficiency, it suffices to focus on just transformer attention heads of the DiT score network due to their dominating quadratic time complexity in both passes. Thus, we consider only a single layer attention for \(f_{}\), to simplify our analysis. Further, we consider the following simplifications:

1. To prove the hardness of (4.1) for both full gradient descent and stochastic mini-batch gradient descent methods, it suffices to consider training on a single data point.
2. For the convenience of our analysis, we consider the following expression for attention mechanism. Let \(X,Y^{d L}\). Let \(W_{K},W_{Q},W_{V}^{s d}\) be attention weights such that \(Q=W_{Q}X^{d L}\), \(K=W_{K}X^{s L}\) and \(V=W_{V}X^{s L}\). We write attention mechanism of hidden size \(s\) and sequence length \(L\) as \[(X)=W_{V}X)}_{V}X^{}W_{K}^{}W_{Q}X }_{KQ}^{d L},\] (4.2) with \(D(XW_{Q}W_{K}^{}X^{}_{L})\). Here, \(()\) is entry-wise exponential function, i.e., \((A)_{i,j}=(A_{i,j})\) for any matrix \(A\), \(()\) converts a vector into a diagonal matrix with the vector's entries on the diagonal, and \(_{L}\) is the length-\(L\) all ones vector.
3. Since \(V\) multiplication is linear in weight while \(K\)-\(Q\) multiplication is exponential in weights, we only need to focus on the gradient update of \(K\)-\(Q\) multiplication. Therefore, for efficiency analysis of gradient, it is equivalent to analyzing a reduced problem with fixed \(W_{O}W_{V}X=\)..
4. To focus on the DiT, we consider the low-dimensional linear encoder \(W_{A}\) to be pretrained and to not participate in gradient computation. This aligns with common practice [Rombach et al., 2022] and is justified by the trivial computation cost due to the linearity of \(W_{A}\)2. 5. To further simplify, we introduce \(A_{1},A_{2},A_{3}^{s L}\) and \(W^{d d}\) via \[W_{A}R^{-1}f_{}(^{ }x_{t_{0}})}_{ X^{d L}})-} _{ Y^{d L}}_{F}^{2}\] (By (S0), (S1) and (S2)) \[=W_{A}R^{-1}W_{V}}_{=W_{OV} ^{d d}}_{=A_{3}^{d L}}D^{-1} }}_{ A_{1}^{}^ {L d}}^{}W_{Q}}_{=W^{d d }}_{=A_{2}^{d L}}-Y_{F}^ {2}.\] (4.3) Notably, \(A_{1},A_{2},A_{3},X,Y\) are constants w.r.t. training above loss with gradient updates.

Therefore, we simplify the objective of training DiT into

**Definition 4.1** (Training Generic DiT Loss).: Given \(A_{1},A_{2},A_{3},Y^{d L}\) and \(W_{OV},W^{d d}\) following (S4), Training a DiT with \(_{2}\) loss on a single data point \(X,Y^{d L}\) is formulated as

\[_{W}\ _{0}(W)=_{W}\ W_{A}R^{-1}W_{OV}A _{3}D^{-1}A_{1}^{}WA_{2}-Y_{F}^{2}. \]

Here \(D(A_{1}^{}WA_{2} _{n})^{L L}\).

**Remark 4.1** (Conditional and Unconditional Generation).: \(_{0}\) is generic. If \(A_{1} A_{2}^{d L}\), Definition 4.1 reduces to cross-attention in DiT score net (for conditional generation). If \(A_{1}=A_{2}^{d L}\), Definition 4.1 reduces to self-attention in DiT score net (for unconditional vanilla generation).

We introduce the next problem to characterize all possible gradient computations of optimizing (4.4).

**Problem 1** (Approximate DiT Gradient Computation (\((L,d,,)\))).: Given \(A_{1},A_{2},A_{3},Y^{d L}\). Let \(>0\). Assume all numerical values are in \(((L))\)-bits encoding. Let loss function \(_{0}\) follow Definition4.1. The problem of approximating gradient computation of optimizing empirical DiT loss (4.4) is to find an approximated gradient matrix \(^{(W)}^{d d}\) such that \(\|^{(W)}-}{} \|_{} 1/(L)\). Here, \(\|A\|_{}_{i,j}|A_{ij}|\) for any matrix \(A\).

In this work, we aim to investigate the computational limits of all possible efficient algorithms of ADiTGC with \(=1/(L)\). Yet, the explicit gradient of DiT denoising score matching loss (4.4) is too complicated to characterize ADiTGC. To combat this, we make the following observations.

1. Let \(g_{1}() W_{A}R^{-1}():^{d L}^ {d_{0}}\), \(g_{2}()():^{d L}^ {d L}\), and \(g_{3}() R(W_{A}^{}):^{D}^{d L}\) such that \(g_{3}(x)=X\) for \(x^{D}\) (with \(D>d_{0}=dL\)).
2. **Vectorization of \(f_{}\).** For the ease of presentation, we use notation flexibly that \(f_{}\) to denote both a matrix in \(^{d L}\) and a vector in \(^{dL}\) in the following analysis. This practice does not affect correctness. The context in which \(f_{}\) is used should clarify whether it refers to a matrix or a vector. Explicit vectorization follows DefinitionD.1.
3. **Linearity of \(g_{1}\).** By linearity of \(W_{A}R^{-1}()\), we treat \(g_{1}\) as a matrix in \(^{d_{0} dL}\) acting on vector \(f_{}()^{dL}\).

Therefore, we have \(_{0}=\|g_{1}[g_{2}(g_{3})-Y]\|_{2}^{2}\), such that its gradient involves \(_{0}}{W}=g_{1}g_{2}}{ W}\). From above, we only need to focus on proving the computation time and error control of term \(g_{2}}{W}\) for gradient w.r.t \(W\). Luckily, with tools from fine-grained complexity theory  and tensor trick (see AppendixD.3), we prove the existence of almost-linear time algorithms for Problem1 in the next theorem. Let \((W)\) for any matrix \(W\) following DefinitionD.1.

**Theorem 4.1** (Existence of Almost-Linear Time Algorithms for ADiTGC).: Suppose all numerical values are in \(( L)\)-bits encoding. Let \((\|W_{OV}A_{3}\|_{},\|W_{K}A_{1}\|_{},\| W_{Q}A_{2}\|_{})\). There exists a \(L^{1+o(1)}\) time algorithm to solve \((L_{p},L,d=( L),=o())\) (i.e., Problem1) with loss \(_{0}\) from Definition4.1 up to \(1/(L)\) accuracy. In particular, this algorithm outputs gradient matrices \(^{(W)}^{d d}\) such that \(\|^{(W)}-}{} \|_{} 1/(L)\).

Proof Sketch.: Our proof is built on the key observation that there exist low-rank structures within the DiT training gradients. Using the tensor trick  and computational hardness results of attention , we approximate DiT training gradients with a series of low-rank approximations and carefully match the multiplication dimensions so that the computation of \(g_{2}}{W}\) forms a chained low-rank approximation. We complete the proof by demonstrating that this approximation is bounded by a \(1/(L)\) error and requires only almost-linear time. See AppendixG.2 for a detailed proof. 

**Remark 4.2**.: We remark that Theorem4.1 is dominated by the relation between \(L\) and \(d\), hence by the subspace dimension3\(d_{0}=dL\). A smaller \(d_{0}\) makes Theorem4.1 more likely to hold.

### Computational Limits of Forward Inference

Since the inference of score-matching diffusion models is a forward pass of the trained score estimator \(s_{W}\), the computational hardness of DiT ties to the transformer-based score network,

\[s_{W}(A_{1},A_{2},A_{3})=W_{A}R^{-1}A_{3}}_{d L }}_{L L}^{} W_{K}^{}}_{L s}A_{2}}_{s L}, \]

following notation in Definition4.1. For inference, we study the following approximation problem. Notably, by Remark4.1, (4.5) subsumes both conditional and unconditional DiT inferences.

**Problem 2** (Approximate DiT Inference \((d,L,,_{F})\)).: Let \(_{F}>0\) and \(B>0\). Given \(A_{1},A_{2},A_{3}^{d L}\), and \(W_{OV},W_{K},W_{Q}^{d d}\) with guarantees that \(\|W_{OV}A_{3}\|_{} B\), \(\|W_{K}A_{1}\|_{} B\) and \(\|W_{Q}A_{2}\|_{} B\), we aim to study an approximation problemADiTI(\(d,L,B,_{F}\)), that approximates \(s_{W}(A_{1},A_{2},A_{3})\) with a vector \(^{d_{0}}\) (with \(d_{0}=d L\)) such that \(\|-W_{A}R^{-1}(W_{OV}A_{3}D^{-1}A_{1}^{} W_{K}^{}W_{Q}A_{2})\|_{}_{F}\). Here, \(\|A\|_{}_{i,j}|A_{ij}|\) for any matrix \(A\).

By (O2) and (O3), we make an observation that Problem 2 is just a special case of (Alman and Song, 2023). Hence, we characterize the all possible efficient algorithms for ADiTI with next proposition.

**Proposition 4.1** (Norm-Based Efficiency Phase Transition).: Let \(\|W_{Q}A_{2}\|_{} B\), \(\|W_{K}A_{1}\|_{} B\) and \(\|W_{OV}A_{3}\|_{} B\) with \(B=()\). Assuming SETH (Hypothesis 1), for every \(q>0\), there are constants \(C,C_{a},C_{b}>0\) such that: there is no \(O(n^{2-q})\)-time (sub-quadratic) algorithm for the problem ADiTI(\(L,d=C L,B=C_{b},_{F}=L^{-C_{a}}\)).

**Remark 4.3**.: Proposition 4.1 suggests an efficiency threshold for the upper bound of \(\|W_{K}A_{1}\|_{}\), \(\|W_{Q}A_{2}\|_{}\), \(\|W_{OV}A_{3}\|_{}\). Only below this threshold are efficient algorithms for Problem 2 possible.

Moreover, there exist almost-linear DiT inference algorithms following (Alman and Song, 2023).

**Proposition 4.2** (Almost-Linear Time DiT Inference).: Assuming SETH, the DiT inference problem ADiTI(\(L,d=( L),B=o(),_{F}=1/(L)\)) can be solved in \(L^{1+o(1)}\) time.

**Remark 4.4**.: Proposition 4.2 is a special case of Proposition 4.1 under the efficiency threshold.

**Remark 4.5**.: Propositions 4.1 and 4.2 are dominated by the relation between \(L\) and \(d\), hence by the subspace dimension \(d_{0}=dL\). A smaller \(d_{0}\) makes Propositions 4.1 and 4.2 more likely to hold.

## 5 Discussion and Concluding Remarks

We explore the fundamental limits of latent DiTs with 3 key contributions. First, we prove that transformers are universal approximators for the score functions in DiTs (Theorem 3.1), with approximation capacity and model size dependent only on the latent dimension, suggesting DiTs can handle high-dimensional data challenges. Second, we show that Transformer-based score estimators converge to the true score function (Theorem 3.2), ensuring the generated data distribution closely approximates the original (Corollary 3.2.1). Third, we provide provably efficient criteria (Proposition 4.1) and prove the existence of almost-linear time algorithms for forward inference (Proposition 4.2) and backward computation (Theorem 4.1). Our computational results hold for both unconditional and conditional generation of DiTs (Remark 4.1). These results highlight the potential of latent DiTs to achieve both computational efficiency and robust performance in practical scenarios.

Practical Guidance from Computational Results.Section 4 analyzes the computational feasibility and identifies all possible efficient DiT algorithms/methods for both forward inference and backward training. These results provide practical guidance for designing efficient methods:

* The latent dimension should be sufficiently small: \(d=( L)\) (Theorem 4.1, Propositions 4.1 and 4.2).
* Normalization of \(K\), \(Q\), and \(V\) in DiT attention heads enhances performance and efficiency:
* For efficient inference: \(\{\|W_{K}A_{1}\|,\|W_{Q}A_{2}\|,\|W_{OV}A_ {3}\|\} B\) with \(B=o()\) (Proposition 4.2) and \(A_{1},A_{2},A_{3}\) being the input data associated with \(K,Q,V\).
* For efficient training: \(\{\|W_{K}A_{1}\|,\|W_{Q}A_{2}\|,\|W_{OV}A_ {3}\|\}\) with \(=o()\) (Theorem 4.1).

We remark that these conditions are necessary but not sufficient; sufficient conditions depend on the specific design of the methods used. This is due to the best- or worst-case nature of hardness results.

Limitations and Future Direction.As discussed in Remark 3.4, the double exponential factor in our explicit sample complexity bound (Theorem 3.2) suggests a possible gap in our understanding of transformer universality and its interplay with DiT architecture. This motivates us to rethink transformer universality and explore new proof techniques for DiTs, which we leave for future work. Besides, due to its formal nature, this work does not provide immediate practical implementations. However, we expect that our findings provide valuable insights for future diffusion generative models.

Post-Acceptance Note (October, 29, 2024).A follow-up work by Hu et al. (2024) alleviates the double exponential factor and achieves minimax optimal statistical rates for DiTs under Holder smoothness data assumptions.

### Broader Impact

This theoretical work aims to shed light on the foundations of diffusion generative models and is not anticipated to have negative social impacts.

JH would like to thank to Minshuo Chen, Sophia Pi, Yi-Chen Lee, Yu-Chao Huang, Yibo Wen, Damien Jian, Jialong Li, Zijia Li, Tim Tsz-Kit Lau, Chenwei Xu, Dino Feng and Andrew Chen for enlightening discussions on related topics; Ting-Chun Liu for pointing out typos; and the Red Maple Family for support. The authors would like to thank the anonymous reviewers and program chairs for constructive comments. JH is partially supported by the Walter P. Murphy Fellowship. HL is partially supported by NIH R01LM1372201, AbbVie and Dolby. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.