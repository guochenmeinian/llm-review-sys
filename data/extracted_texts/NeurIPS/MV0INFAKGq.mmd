# Tanimoto Random Features for Scalable Molecular Machine Learning

Austin Tripp

University of Cambridge

ajt212@cam.ac.uk

&Sergio Bacallado

University of Cambridge

sb2116@cam.ac.uk

&Sukriti Singh

University of Cambridge

ss2971@cam.ac.uk

&Jose Miguel Hernandez-Lobato

University of Cambridge

jmh233@cam.ac.uk

###### Abstract

The Tanimoto coefficient is commonly used to measure the similarity between molecules represented as discrete fingerprints, either as a distance metric or a positive definite kernel. While many kernel methods can be accelerated using random feature approximations, at present there is a lack of such approximations for the Tanimoto kernel. In this paper we propose two kinds of novel random features to allow this kernel to scale to large datasets, and in the process discover a novel extension of the kernel to real-valued vectors. We theoretically characterize these random features, and provide error bounds on the spectral norm of the Gram matrix. Experimentally, we show that these random features are effective at approximating the Tanimoto coefficient of real-world datasets and are useful for molecular property prediction and optimization tasks. Future updates to this work will be available at http://arxiv.org/abs/2306.14809.

## 1 Introduction

In recent years there have been notable advances in the use of machine learning (ML) for drug discovery, including molecule generation and property prediction (Dara et al., 2022). Despite ceaseless progress in deep learning, conventional methods such as support vector machines or random forest trained on _molecular fingerprints_ are still competitive in the low-data regime (Walters and Barzilay, 2020; Stanley et al., 2021). These fingerprints essentially encode fragments from a molecule into a sparse vector, thereby compactly representing a large number of molecular substructures (David et al., 2020). They are extensively used in virtual screening for substructure and similarity searches as well as an input for ML models (Cereto-Massague et al., 2015; Granda et al., 2018).

The _Tanimoto coefficient_ (also known as the _Jaccard index_) stands out as a natural way to compare such fingerprints. This coefficient is most commonly expressed as a function on sets \(T_{S}\) or as a function of non-negative vectors \(T_{MM}\)(Tancard, 1912; Tanimoto, 1958; Ralaivola et al., 2005; Costa, 2021; Tan et al., 2016):

\[T_{S}(X,X^{})=|}{|X X^{}|},\ \ X,X^{}, T_{MM}(x,x^{})=(x _{i},x^{}_{i})}{_{i}(x_{i},x^{}_{i})},\ \ x,x^{}_{ 0 }^{d}\,.\] (1)

If \(a\) and \(b\) are binary indicator vectors representing sets \(A\) and \(B\) respectively, then \(T_{S}(A,B)=T_{MM}(a,b)\). Therefore \(T_{MM}\) can be viewed as a generalization of \(T_{S}\); for this reason it is sometimes called the "weighted Jaccard coefficient" or min-max coefficient.

The Tanimoto coefficient is widely used in machine learning and cheminformatics to compute similarities between molecular fingerprints (Bajusz et al., 2015; O'Boyle and Sayle, 2016; Miranda-Quintana et al., 2021), chiefly because of the following properties:

1. **Clear Interpretation:** The value of \(T_{MM}(x,x^{})\) represents the degree of overlap between \(x\) and \(x^{}\) and is always between \(0\) and \(1\). \(T(x,x^{})=1\) only when \(x=x^{}\).
2. **Kernel:**\(T_{MM}(,)\) is positive definite (Gower, 1971; Ralaivola et al., 2005), meaning it can be used as the kernel for algorithms like support vector machines or Gaussian processes.
3. **Metric:**\(1-T_{MM}(x,x^{})\) is a valid distance metric (typically called _Jaccard/Soergel distance_) and can therefore be used in nearest-neighbour and clustering algorithms (Marczewski and Steinhaus, 1958; Levandowsky and Winter, 1971).

In this paper, we present and characterize two efficient low-rank approximations for large matrices of Tanimoto coefficients. The first method, presented in section 3, uses a random hash function to index a random tensor and enjoys exceptionally low variance. The second method, presented in section 4, uses a power series expansion of the Tanimoto similarity for binary vectors. This line of research also unexpectedly led to the discovery of a new generalization of the Tanimoto coefficient to arbitrary vectors in \(^{d}\), \(T_{DP}\), which is also a kernel and can be used to form a distance metric. In section 6 we demonstrate experimentally that our random features are effective at approximating Tanimoto matrices of real-world fingerprint data and demonstrate its application to molecular property prediction and optimization problems.

## 2 Background: kernel methods and random features

Kernel methods are a broad class of machine learning algorithms which make predictions using a positive definite _kernel function_\(k:\)(Scholkopf et al., 2002). Common methods in this class are support vector machines (Cortes and Vapnik, 1995) and Gaussian processes (Williams and Rasmussen, 2006). Given a dataset of \(n\) data points, training most kernel methods requires computing the \(n n\) kernel matrix1\(K_{i,j}=k(x^{(i)},x^{(j)})\) (with \(O(n^{2})\) time complexity) and possibly inverting it (with \(O(n^{3})\) time complexity). Because of this, applying kernel methods to large datasets generally requires approximations.

Given a kernel \(k\), a _random features map_ is a random function \(f:^{M}\), with the property that \(f(x) f(x^{})\) approximates \(k(x,x^{})\) for every pair \(x,x^{}\). The approximation is often exact in expectation:

\[_{f}[f(x) f(x^{})]=k(x,x^{})x,x^{}.\] (2)

Random features allow the kernel matrix to be approximated as \(_{i,j}=f(x^{(i)}) f(x^{(j)})\). Because this matrix has rank at most \(M\), this approximation generally reduces the cost of \(O(n^{3})/O(n^{2})\) computations to \(O(M^{3})/O(nM^{2})\), i.e. at most linear in \(n\).

The seminal work of Rahimi and Recht (2007), which coined the term random features, gave a general method based on Fourier analysis to construct random features for any _Bochner_ or _stationary kernel_, for which \(k(x,x^{})\) is a function of \(x-x^{}\). This class includes many common kernels including the RBF and Matern kernels, but excludes \(T_{MM}\). Subsequent works have proposed random features for other kernels including the polynomial kernel and the arc-cosine kernel (Liu et al., 2021). However, there is no general formula to define random features for non-stationary kernels, such as \(T_{MM}\).

A random features map is sometimes called a _data-oblivious sketch_, to distinguish it from other _data-dependent_ low-rank approximation methods which depend on a given dataset \(x^{(1)},,x^{(n)}\). Examples of data-dependent low rank sketches are the Nystrom approximation and leverage-score sampling (Drineas et al., 2005, 2012). Although data-dependent methods may result in lower approximation errors for a given dataset, data-oblivious sketches are naturally parallelizable and useful in cases where the dataset changes over time (e.g. streaming or optimization) or for ultra-large datasets which may not fit in memory.

## 3 Low-variance random features for Tanimoto and MinMax kernels

Outside of chemistry, the Tanimoto coefficient has been widely used to measure the similarity between text documents and rank results in search engines. To quickly find documents with high similarity to a user's query, many prior works have studied random _hashes_ for the Tanimoto coefficient, i.e. a family of random functions \(h:\{1,,K\}\) such that

\[_{h}(h(x)=h(x^{}))=T_{MM}(x,x^{}).\] (3)

Although initially these hashes were only applicable to binary inputs (Broder, 1997; Broder et al., 1998; Charikar, 2002), more recent work has produced efficient random hashes for arbitrary non-negative vectors (Manasse et al., 2010; Ioffe, 2010; Shrivastava, 2016). In this section we propose a novel family of low-variance random features for \(T_{MM}\) (and by extension \(T_{S}\)) which is based on random hashes.

It is important to clarify that although the definition of random hashes in equation 3 resembles the definition of random features in equation 2, they are actually distinct. Random hash functions output _discrete_ objects (typically an integer or tuple of integers) whose probability of _equality_ is \(T_{MM}\), while random features must output _vectors_ in \(^{M}\) whose expected _inner product_ is \(T_{MM}\). If a random hash maps to \(\{1,,K\}\), a naive approach may be to use a \(K\)-dimensional indicator vector as a random feature. Because hash equality is a binary outcome, the variance of such random features would be \(T_{MM}(1-T_{MM})\). Realistic hash functions like that of Ioffe (2010) use \(K 10^{3}\), implying a "variance per feature" of \( 10^{2}\), which is undesirably high.

Our main insight is that low-variance scalar random features can be created by using a random hash to _index_ a suitably distributed random vector. In the following theorem, we show that a vector of i.i.d. samples from any distribution with the correct first and second moments can be combined with random hashes to produce random features for \(T_{MM}\).

**Theorem 3.1**.: _Let \(h:\) be a random hash for \(T_{MM}\) satisfying equation 3, with \(||=K\). Furthermore, let \(\) be a random variable such that \([]=0\) and \([^{2}]=1\), and let \(=[_{1},,_{K}]\) be a vector of independent copies of \(\). Then the 1D random features_

\[_{,h}(x)=_{h(x)}\] (4)

_estimate \(T_{MM}\) without bias: \(_{,h}(_{,h}(x)_{,h}(x^{}))=T_{MM}(x,x^{ })\), and with variance_

\[_{,h}[_{,h}(x)_{,h}(x^{})]=1 +T_{MM}(x,x^{})(E[^{4}]-1-T_{MM}(x,x^{})) 1 -T_{MM}(x,x^{})^{2}.\] (5)

_Furthermore, the lower bound is tight and achieved when \(\) is Rademacher distributed (i.e. uniform in \(\{-1,1\}\))._

The proof is given in Appendix D.1. This theorem shows that Rademacher \(\) yields the smallest possible variance in the class of random features defined in eq. (4).

These random features have many desirable properties. First, unlike random features for many other kernels such as the Gaussian kernel (Liu et al., 2021), the variance does not depend on the dimension of the input data or norms of the input vectors. Second, because these random features are 1-dimensional scalars, \(M\) independent random feature functions can be concatenated to produce \(M\)-dimensional random feature vectors with variance at most \(1/M\). This suggests that as few as \( 10^{3}\) random features could be used in practical problems. Third, although each instance of \(\) requires storing a \(K\) dimensional random vector, if \(\) is chosen to be Rademacher distributed, then each entry can be stored with a single bit, requiring just \( 100\) kB of memory when \(K=10^{6}\).

One disadvantage of these random features is that they are not continuous or differentiable with respect to their inputs. For applications such as Bayesian optimization which require optimizing over model inputs this would create difficulties as gradient-based optimization could no longer be done. It was this disadvantage which motivated us to search for other random features, leading to the discoveries in the following section.

## 4 Tanimoto dot product kernel and its random features

Ralaivola et al. (2005) gave a definition for the Tanimoto coefficient involving dot products:

\[T_{DP}(x,x^{})=}{\|x\|^{2}+\|x^{}\|^{2}-x  x^{}},\] (6)with \(T_{DP}(x,x^{})=1\) when \(x,x^{}=0\). It is easy to check that \(T_{DP}(x,x^{})=T_{MM}(x,x^{})\) on binary vectors, which was used by Ralaivola et al. (2005) to prove that \(T_{DP}\) is a kernel on the space \(\{0,1\}^{d}\), referencing prior work by Gower (1971). However, \(T_{DP}\) is not identical to \(T_{MM}\) for general inputs \(x,x^{}_{ 0}^{d}\). Here, we give the first proof that \(T_{DP}\) is a positive definite function in \(^{d}\) and thus, also a valid kernel in this space.

**Theorem 4.1**.: _For \(x,x^{} 0\) in \(^{d}\), we have_

\[T_{DP}(x,x^{})=_{r=1}^{}(x x^{})^{r} (\|x\|^{2}+\|x^{}\|^{2})^{-r},\] (7)

_where the series is absolutely convergent. The function \(T_{DP}\) is a positive definite kernel in \(^{d}\)._

It has been noticed previously that, unlike \(1-T_{MM}\), the function \(1-T_{DP}\) is _not_ a distance metric on non-binary inputs (Kosub, 2019). Indeed, when \(d=1\), the inputs \(\{1,2,4\}\) violate the triangle inequality. However, we can easily derive a distance metric from \(T_{DP}\).

**Corollary 4.2**.: \(d_{DP}(x,x^{})=(x,x^{})}\) _corresponds to the RKHS norm of the function \([T_{DP}(x,)-T_{DP}(x^{},)]\) and is therefore a valid distance metric on \(^{d}\)._

Proofs are given in Appendix D.2. These results imply that \(T_{DP}\), like \(T_{MM}\), is an extension of the set-valued Tanimoto coefficient (equation 1) to real vectors and can be used as a substitute for \(T_{MM}\) in machine learning algorithms that require a kernel or distance metric. Unlike \(T_{MM}\), the kernel \(T_{DP}\) is differentiable everywhere with respect to its inputs. It can also be computed in batches using matrix-matrix multiplication, allowing for efficient vectorized computation.

We now consider producing a random features approximation to \(T_{DP}\) for large-scale applications. Motivated by the close relationship between \(T_{DP}\) and \(T_{MM}\), one may be tempted to find a random hash for \(T_{DP}\) and apply the techniques developed in section 3. Unfortunately, we are able to prove that this is not possible.

**Proposition 4.3**.: _There exists no random hash function for \(T_{DP}\) over non-binary vectors._

Proof.: Charikar (2002) proved that if \(s(x,x^{})\) is a similarity function for which there exists a random hash, then \(1-s(x,x^{})\) must satisfy the triangle inequality (see their Lemma 1). Because \(1-T_{DP}(x,x^{})\) does not satisfy the triangle inequality it follows by contradiction that there does not exist a random hash for \(T_{DP}\). 

Therefore producing random features for \(T_{DP}\) will require another approach. In the remainder of this section we present a framework to produce random features for \(T_{DP}\) by directly approximating its power series (equation 7). We first describe a method to produce random features for \((\|x\|^{2}+\|x^{}\|^{2})^{-r}\) (4.1). Then we describe how these features can be combined with existing random features for the polynomial kernel to approximate \(T_{DP}\)'s truncated power series (4.2-4.3). Lastly, we present an error bound for the kernel matrix of a dataset in the spectral norm, showing that the required dimension for the sketch scales optimally with the stable rank of the kernel matrix (4.4).

### Random features for the "prefactor" \((\|x\|^{2}+\|x^{}\|^{2})^{-r}\)

In this section we present a random feature map for the positive definite kernel \((x,x^{})(\|x\|^{2}+\|x^{}\|^{2})^{-r}\), which we will refer to as the the _prefactor_. We defer all proofs to Appendix D.3. We begin with the following lemma, which defines scalar random features for the prefactor:

**Lemma 4.4**.: _If \(Z(s,c)\) (where \(c\) is a rate parameter), then_

\[_{r,Z}(x)=e^{(1/2-\|x\|^{2})Z}Z^{(r-s)/2}e^{(c-1)Z}(s )/(r)}\] (8)

_is an unbiased scalar random feature for the prefactor \((\|x\|^{2}+\|x^{}\|^{2})^{-r}\) for all \(s,c>0\)._

Although independent copies of \(Z\) could be combined to form an \(M\)-dimensional sketch, we instead propose to use a dependent point set \(Z_{1},,Z_{M}\) where each element \(Z_{i}\) has a \((s,c)\) distribution whilst maximally covering the real line. This is a well-established Quasi-Monte Carlo (QMC) technique which generally attains lower variance. We define our \(M\)-dimensional QMC features in the following lemma:

**Lemma 4.5**.: _Let \(_{s,c}\) be the inverse cumulative distribution function of a \((s,c)\) random variable. Fix \(M,r\), \(u(0,1)\), \(c,s>0\) and let \(u_{i}=u+i/M- u+i/M\) for \(i=1,,M\). Define \(_{u,r}(x)=(_{u,r,1}(x),,_{u,r,M}(x))\), where:_

\[_{u,r,i}(x)=}(s)}{ (r)}}e^{-(\|x\|^{2}-c/2)_{s,c}(u_{i})}(_{s,c}(u_{i}))^{(r-s) /2}\;.\] (9)

_If \(u(0,1)\) then \(_{u,r}(x)\) forms unbiased random features of the prefactor \((\|x\|^{2}+\|x^{}\|^{2})^{-r}\)._

Although the random features are unbiased for all \(s,c>0\), the value of these parameters will impact the error. We show that if \(s,c\) are suitably tuned, then the relative error can be bounded:

**Lemma 4.6**.: _Let \(x^{(1)},,x^{(n)}^{d}\) with \(\|x^{(i)}\|^{2}}{_{i}\|x^{(i)}\|^{2}}\), and fix \(u\). Define the relative error_

\[E_{i,j}=(x^{(i)})_{u,r}(x^{(j)})-(\|x^ {(i)}\|^{2}+\|x^{(j)}\|^{2})^{-r}}{(\|x^{(i)}\|^{2}+\|x^{(j)}\|^{2})^{-r}}.\] (10)

_If \(c=2^{2}\), \(s=r\), then for some constant \(C\) independent of \(r\) this error satisfies,_

\[_{1 i,j n}|E_{i,j}|}{(r)}(r/e)^{r(-1)}(1.3)^{r} C(M)^{-1}.\] (11)

Together, these lemmas suggest random features for the prefactor can be created by first estimating \(\) (the minimum ratio of norms of input vectors), then using the random features from Lemma 4.5 with the values of \(s,c\) specified in Lemma 4.6.

### A framework to produce random features for \(T_{dp}\)

There are straightforward rules for producing random features for sums and products of kernels whose individual random features are known (Duvenaud, 2014, sec. 2.6.2). Random features for kernels \(k_{1},k_{2}\) can be _concatenated_ (denoted \(\)) to form random features for the sum kernel \(k_{1}+k_{2}\), while their _tensor product_2 (denoted \(\)) forms random features for the product kernel \(k_{1} k_{2}\). Our strategy to produce features for \(T_{DP}\) is to combine random features for the "prefactor" (presented in section 4.1) with random features for the polynomial kernel to produce random features for \(T_{DP}\)'s power series (equation 7) truncated at \(R\) terms.

Fix \(R\), and for \(r=1,,R\), let \(_{r}\) be a \(m_{r}\)-dimensional random features map for the prefactor \((\|x\|^{2}+\|x^{}\|^{2})^{-r}\) and let \(_{r}\) be a \(m^{}_{r}\)-dimensional random features map for \((x x^{})^{r}\). The function:

\[_{R}(x)=_{r=1}^{R}[_{r}(x)_ {r}(x)]\] (12)

is therefore a random feature estimate for \(T_{DP}\)'s power series, truncated at \(R\) terms. Unfortunately, these random features have dimension \(M=_{r=1}^{R}m_{r}m^{}_{r}\) which depends on the _product_ of the random features dimension of \(_{r}\) and \(_{r}\). Furthermore, the dimension \(m^{}_{r}\) of the random features \(_{r}\) required to approximate the polynomial kernel \((x x^{})^{r}\) with good accuracy can scale poorly with \(r\). For even modest values of \(m_{r},m^{}_{r}\) the resulting value of \(M\) will likely be prohibitively large.

To remedy this, we turn to recent works which propose powerful linear maps to approximate tensor products with a lower-dimensional vector. Assuming \(x^{(1)},y^{(1)}^{d_{1}}\) and \(x^{(2)},y^{(2)}^{d_{2}}\), these maps are effectively random matrices \(^{m(d_{1}d_{2})}\), which exhibit a _subspace embedding property_ whereby \([(x^{(1)} x^{(2)})][(y^{(1)} y^{(2)})]\) concentrates sharply around \((x^{(1)} x^{(2)})(y^{(1)} y^{(2)})\). Critically, the product \((x^{(1)} x^{(2)})\) can be computed _without_ instantiating either matrix \(\) or the tensor product \(x^{(1)} x^{(2)}\). Examples of such methods include TensorSketch and TensorSRHT(Pagh, 2013; Pham and Pagh, 2013; Ahle et al., 2020), but for generality we will simply refer to these methods as Sketch. Defining a series of sketches Sketch\({}_{r}:^{m_{r}}^{m^{}_{r}} ^{_{r}}\) we can modify \(_{R}\) from equation 12 into:

\[_{R}(x)=_{r=1}^{R}_{r}[_{r}(x), _{r}(x)]\] (13)which has output dimension \(M=_{r=1}^{R}_{r}\), i.e. without any pathological dependencies on the dimensions of \(_{r}(x),_{r}(x)\). However, because these features approximate a _truncated_ power series, they will be biased downward due to the monotonicity of the power series (7). We propose two bias correction techniques to potentially improve empirical accuracy. One approach is to normalize the random features such that \((x)(x)=1=T_{DP}(x,x)\) for all \(x^{d}\), i.e., such that the diagonal entries of the kernel matrix \(K\) are estimated exactly. A second approach is based on sketching the residual of the power series. These approaches are presented in detail in Appendix E.

### Implementing the random features

Instantiating the random features from the previous subsection (13) requires making concrete choices for \(R\), \(_{r},m_{r},m_{r}^{},_{r},_{r},_{r}\) for all \(r\), and choosing a bias correction technique. There are many reasonable choices for \(_{r}\), such as TensorSketch(Pham and Pagh, 2013) and TensorSRHT(Ahle et al., 2020). These sketches generally allow \(m_{r},m_{r}^{},_{r}\) to be chosen freely (although naturally error will increase as \(_{r}\) decreases). Many of these sketches can also be used as random features for the polynomial kernel \(_{r}\)(Wacker et al., 2022), either directly or as part of more complex algorithms like TreeSketch(Ahle et al., 2020) or complex-to-real sketches (Wacker et al., 2023). The QMC random features from section 4.1 can be used for the prefactor \(_{r}\), with the parameters \(s,c\) chosen based on the anticipated norms of the input vectors.

The only remaining inputs are \(R\) (the number of power series terms to approximate) and \(_{1},,_{R}\) (how many random features to use for each term). Assuming a fixed dimension \(M\) for the final random features, this choice involves a bias variance trade-off, as a higher value of \(R\) will reduce bias but require each term in the power series to have fewer features, thereby increasing variance. Intuitively, because the terms of the power series decrease monotonically the variance of terms for small \(r\) is likely to dominate the overall variance, and therefore we surmise that a _decreasing_ sequence for \(\{_{r}\}_{r=1}^{R}\) will be the best choice. Ultimately however we do not have theoretical results to dictate this choice in practice. We will evaluate these choices empirically in section 6.

### Asymptotic error bound for \(T_{dp}\) random features

Because many kernel methods use kernel matrices as linear operators, it is natural to examine the error of kernel approximations in the operator norm. Previous works have produced asymptotic error bounds of the random feature dimension \(m\) required to achieve a relative approximation error of \(\) in the operator norm. Defining \(}(K)=(K)/\|K\|_{}\), Cohen et al. (2015) show that \(m=(}(K)/^{2})\) is essentially optimal for data-oblivious random features of linear kernels, even though it is possible to eliminate logarithmic factors. Our main theoretical result is that with the correct choices of base random features, the random features for \(T_{DP}\) presented in section 4.2 achieve similar scaling. We now state this as a theorem.

**Theorem 4.7**.: _For any \(n 1\), let \(x^{(1)},,x^{(n)}^{d}\) be a set of inputs with \(\|x^{(i)}\|^{2}}{_{i}\|x^{(i)}\|^{2}}\). Let \(K\) be the matrix with entries \(K_{i,j}=T_{DP}(x^{(i)},x^{(j)})\). For all \(>0\), there exists an oblivious sketch \(:^{d}^{m}\) with \(m=(}(K)/^{2})\), such that_

\[_{}\|-K\|_{}\|K\|_{ }(n)}\] (14)

_where \(_{i,j}=(x^{(i)})(x^{(j)})\). Furthermore, the sketch can be computed in time \((}(K)n^{-2}+(X)^{-2 }+n^{-1}^{-3})\)._

The random features in the theorem follow equation 13, with specific choices for \(M\),\(R\), and \(\{_{r},_{r},_{r},_{r}\}_{r=1}^{R}\) given in the proof in Appendix D.4. Theorem 4.7 essentially suggests that, with the correct settings, the error of the random features proposed in section 4.2 scales as well as one could reasonably expect for a kernel of this type. We would highlight that the computational cost of the sketch is sub-quadratic in \(n\), and compares favourably with the cost of data-dependent low-rank approximation methods.

## 5 Related work

Our work on random features fits into a large body of literature random features for kernels (Liu et al., 2021). The majority of work in this area focuses on stationary kernels (i.e. \(k(x,x^{})=f(x-x^{})\)), because the Fourier transform can be applied to any stationary kernel to produce random features in a systematic way (Rahimi and Recht, 2007). There is however no analogous universal formula to produce random features for non-stationary kernels like \(T_{MM}\) and \(T_{DP}\); therefore each kernel requires a bespoke approach. Although our random features are novel, they build upon ideas present in prior works. Our random features for \(T_{MM}\) critically rely on previously-proposed random hashes for \(T_{MM}\). Our approach to create random features for \(T_{DP}\) via approximating its power series follows was inspired by the random features for the Gaussian kernel from Cotter et al. (2011), which were subsequently improved upon by Ahle et al. (2020). Similar techniques have also been used to create random features for the neural tangent kernel (Zandieh et al., 2021). However, to the best of our knowledge no prior works have proposed random features specifically for the Tanimoto kernel or its variants.

Other works have proposed other types of scalable approximations for Tanimoto coefficients which are not based on random features. Haque and Pande (2010) propose SCISSORS, an optimization-based approach to estimate Tanimoto coefficients which is akin to a data-dependent sketch. A large number of works use hash-based techniques to find approximate nearest neighbours with the Tanimoto distance metric (Nasr et al., 2010; Kristensen et al., 2011; Tabei and Tsuda, 2011; Anastasiu and Karypis, 2017). Although these techniques are useful for information retrieval, unlike random features they cannot be used to directly scale kernel methods to larger datasets.

## 6 Experiments

In this section we apply the techniques in this paper to realistic datasets of molecular fingerprints. All experiments were performed in python using the numpy (Harris et al., 2020), pytorch(Paszke et al., 2019), gpytorch(Gardner et al., 2018), and rdkit(Landrum et al., 2023) packages. Molecules were represented with both binary (B) and count (C) Morgan fingerprints (Rogers and Hahn, 2010) of dimension 1024 (additional details in Appendix F.1). These vectors indicate the presence (B) or count (C) of different subgraphs in a molecule. Code to reproduce all experiments is available at: https://github.com/AustinT/tanimoto-random-features-neurips23.

### Errors of random features on real datasets

Here we study the error of approximating matrices of Tanimoto coefficients using our random features, with the general goal of verifying the claims in sections 3-4 on a realistic dataset of molecules. We choose to study a sample of 1000 small organic molecules from the GuacaMol dataset (Brown et al., 2019; Mendez et al., 2019) which exemplify the types of molecules typically considered in drug discovery projects. We use both binary (B) and count (C) fingerprints of radius 2.

Figure 1: **Left:** MSE of \(T_{MM}\) matrix reconstruction as a function of number of random features (median over 5 trials, shaded regions are first/third quartiles). **Right:** empirical variance of scalar \(T_{MM}\) random feature estimates for \(M=10^{5}\), closely matching theoretical predictions (dashed lines).

First, we investigate the random features for \(T_{MM}\) proposed in section 3. We instantiate these features using the random hash from Ioffe (2010) (explained further in Appendix F.2) with \(\) both Gaussian and Rademacher distributed. The results are shown in Figure 1. The left subplot shows the median mean squared error (MSE), i.e. \(_{i,j}[(T_{MM}(x^{(i)},x^{(j)})-(x^{(i)})(x^{( j)}))^{2}]\) as a function of the random feature dimension \(M\). As expected for a Monte Carlo estimator, the square error decreases with \(O(1/M)\) (i.e. increasing the number of random features by 10 reduces the MSE by a factor of 10). As predicted by Theorem 3.1, the estimation error seems to depend only on the distribution of \(\) and not on the input vectors themselves; therefore the error curves for count and binary fingerprints overlap completely. The error is lowest when \(\) is Rademacher distributed, although the empirical difference in error seems small. The right subplot looks at the variance across across scalar random features, showing close matching with the predictions of Theorem 3.1. Overall these features behave exactly as expected.

Next, we investigate the random features for \(T_{DP}\) from section 4. These features are more complex, so we start by studying the random features for the "prefactor" from section 4.1. Recall that these features had free parameters \(s,c>0\). Fixing the number of features \(M=10^{4}\), Figure 2 (left) shows the MSE for the \(r=1\) and \(r=3\) terms for both the binary and count fingerprints (which have different norms) as a function of \(s\) and \(c\). The values which minimize the relative error bound from lemma 4.6, denoted \(s^{*},c^{*}\), seem to lie in a broad plateau of low error in all settings, suggesting that these values of \(s,c\) are a prudent choice. Using these values of \(s,c\), Figure 2 (right) shows the MSE with respect to the number of features \(M\). As expected for a QMC method, the error dependence appears to be _quadratic_\(O(1/M^{2})\) (i.e. a ten-fold increase in \(M\) reduces MSE by 100-fold).

Figure 3: **Left:** MSE for \(M=10^{4}\) dimensional random features when allocating features by \(_{r} r^{p}\), \(r=1.,4\). **Right:** MSE of \(T_{DP}\) random features using \(R=4,p=-1\) and various bias correction strategies. Both subplots use binary fingerprints (the equivalent plot for count fingerprints is Figure F.2). As in Figure 1, lines are medians over 5 trials, shaded regions are first/third quartiles.

Figure 2: **Left:** Contour plots of MSE for prefactor random features with \(M=10^{4}\) with varying \(s,c\). **Right:** MSE vs number of prefactor random features with \(s,c\) values from Lemma 4.6. As in Figure 1, lines are medians over 5 trials, shaded regions are first/third quartiles.

Because it is implemented in scikit-learn (Pedregosa et al., 2011), we use TensorSketch (Pagh, 2013) both as the polynomial random feature map and to combine the polynomial and prefactor random features. We fix the number of random features for the prefactor to be \(10^{4}\) (recall it can be chosen freely without impacting the final random feature dimension). Figure F.1 shows the MSE of approximating both \((x x^{})^{r}\) and \(((x x^{})/(\|x\|^{2}+\|x^{}\|^{2}))^{r}\). In both cases, the MSE decreases approximately with \(O(1/M)\). Finally, we empirically examine how to allocate \(M\) random features across \(R\) terms. Using \(R=4\), Figure 3 (left) shows that allocating most of the features to the terms with small \(R\) results in lower error. We therefore heuristically suggest allocating features according to \(_{r} r^{-1}\). Recall that truncating the power series biases the random features downward, and in section 4.2 two bias correction techniques were proposed. Figure 3 (right) studies the overall MSE for the plain features and both bias correction techniques. It appears that, in practice, neither technique is particularly helpful (normalization in fact appears harmful for large \(M\)). All techniques show an error dependence of approximately \(O(1/M)\).

### Molecular property prediction and uncertainty quantification

To evaluate their efficacy in practice, we use our random features to approximate large-scale Gaussian processes (GPs) (Williams and Rasmussen, 2006) for molecular property prediction. Specifically, we study 5 tasks from the dockstring benchmark which entail predicting protein binding affinity from a molecular graph structure (Garcia-Ortegon et al., 2022). Each task contains \(250\)k molecules, making exact GP regression infeasible. We represent molecules with count fingerprints of radius 1.

We use \(M=5000\) random features for all methods. We compare to two approximate GP baselines. The first is an exact GP on a random subset of size \(M\). Since this approach ignores most of the dataset, one should expect a reasonable approximate GP to perform better. The second is a sparse variational GP (SVGP) which approximates the dataset using \(M\) pseudo-data points \(Z\)(Titsias, 2009; Hensman et al., 2013). The locations of \(Z\) are typically chosen based on the input dataset (we use K-means clustering), making this method effectively a data-dependent sketch. Accordingly, one might expect the performance of this approximation to be _better_ than data-oblivious random features. Details of Gaussian process training are given in appendix F.4.

Table 1 shows the average log probability of test set labels for all types of GP with \(T_{MM}\) and \(T_{DP}\) kernels. Several trends are evident. First, for each kernel random feature GPs (RFGPs) consistently outperform random subset GPs, but underperform SVGP. Second, for each kernel the difference between the RFGP varieties is small (generally less than the standard deviation). Third, on most targets \(T_{DP}\) seems to perform better than the \(T_{MM}\) kernel. The reason for this is unclear. Similar trends can be see in the \(R^{2}\) metric (Table F.1). This suggests that the RFGPs in this paper can be used in large-scale regression, although it seems in practice that data-dependent approximations are more accurate.

### Bayesian optimization in molecule space via Thompson sampling

Bayesian optimization (BO) uses a probabilistic surrogate model to guide optimization and is generally considered one of the most promising techniques for sample-efficient optimization (Shahriari et al., 2015). Because wet-lab experiments are expensive and time-consuming, there is considerable interest in using BO for experiment design. Chemistry experiments are often done in large batches, and

   Kernel & Method & ESR2 & F2 & KIT & PARP1 & PGR \\  \(T_{MM}\) & Rand subset GP & -1.084 \(\) 0.004 & -0.951\(\) 0.002 & -1.094\(\) 0.002 & -0.999\(\) 0.002 & -1.183\(\) 0.005 \\ SVGP & -0.908 \(\) 0.005 & -0.502\(\) 0.005 & -0.846\(\) 0.002 & -0.606\(\) 0.005 & -1.030\(\) 0.005 \\ RFGP (\(\) Rad.) & -0.954 \(\) 0.009 & -0.658\(\) 0.013 & -1.222\(\) 0.004 & -0.968\(\) 0.037 & -1.127\(\) 0.023 \\ RFGP (\(\) Gauss.) & -0.956 \(\) 0.010 & -0.663\(\) 0.015 & -1.230\(\) 0.048 & -0.967\(\) 0.036 & -1.124\(\) 0.025 \\  \(T_{DP}\) & Rand subset GP & -1.073 \(\) 0.002 & -0.940\(\) 0.001 & -1.077\(\) 0.002 & -0.988\(\) 0.001 & -1.187\(\) 0.006 \\ SVGP & -0.880 \(\) 0.004 & -0.459\(\) 0.002 & -0.804\(\) 0.002 & -0.568\(\) 0.002 & -1.010\(\) 0.004 \\ RFGP (plain) & -0.902 \(\) 0.003 & -0.513\(\) 0.004 & -0.979\(\) 0.015 & -0.690\(\) 0.022 & -1.029\(\) 0.002 \\ RFGP (norm) & -0.902 \(\) 0.003 & -0.515\(\) 0.003 & -0.980\(\) 0.015 & -0.691\(\) 0.022 & -1.028\(\) 0.002 \\ RFGP (sketch) & -0.904 \(\) 0.002 & -0.515\(\) 0.004 & -0.979\(\) 0.014 & -0.690\(\) 0.021 & -1.030\(\) 0.002 \\   

Table 1: Average log probability of test set labels with various approximate GPs for 5 targets from dockstring dataset (Garcia-Ortegon et al., 2022). \(\) values are standard deviations over 5 trials.

therefore algorithms which use functions sampled from a probabilistic model are of particular interest (Hernandez-Lobato et al., 2017). To sample from a normal distribution \((,K)\) one typically transforms i.i.d. samples \(Z\ (0,1)\) via \(+K^{1/2}Z\). This requires computing \(K^{1/2}\), and thereby causes exact GP sampling to scale cubically in the number of _evaluation_ points. By approximating \(K^{T}\), our random features allow for approximate sampling in _linear_ time. In this section we apply this to a real-world dataset.

As a demonstration, we consider a single round of selecting 100 molecules from a random sub-sample of \(n\) molecules using _Thompson sampling_, a procedure for Bayesian optimization which selects molecules that maximize a function sampled from a GP prior using the \(T_{MM}\) and \(T_{DP}\) kernels. Similar to the setup from section 6.2, we use molecules and labels from the dockstring dataset. Molecules are represented as count fingerprints. \(M=5000\) random features used, with Rademacher \(\) for \(T_{MM}\) and no bias correction for \(T_{DP}\). All other implementation details are the same as in the previous subsection. Figure 4 (left) shows that, as expected, exact Thompson sampling scales worse than approximate Thompson sampling with random features. Figure 4 (right) shows that using approximate instead of exact Thompson sampling does not seem to change the average F2 docking scores of the molecules chosen. This suggests that approximate Thompson sampling could fruitfully be applied to large datasets of molecules in Bayesian optimization tasks.

## 7 Discussion and conclusion

In this paper we presented two kinds of random features to estimate Tanimoto kernel matrices: one based on random hashes and another based on a power series expansion. To our knowledge, this is the first investigation into random features for the Tanimoto kernel. We theoretically analyze their approximation quality and demonstrate that they can effectively approximate the Tanimoto kernel matrices on realistic molecular fingerprint data. In the process we discovered a new Tanimoto-like kernel over all of \(^{d}\) which is a promising substitute for the more established \(T_{MM}\) on regression and optimization tasks.

Despite promising theoretical and experimental results, our random features do have some limitations. We found that it was difficult to efficiently vectorize the computation of the random features for \(T_{MM}\), making them undesirably slow to compute. For \(T_{DP}\), we were able to exhibit an error bound on the spectral norm which depends on certain choices for the base sketch and sketch dimensions; however, it is unclear whether these choices are optimal in practice. Nonetheless, in Appendix D.5 we prove that _exact_ low-rank factorizations of \(T_{MM}\) and \(T_{DP}\) kernel matrices are not possible; this means that follow-up works could reduce but never eliminate the approximation error.

We are most optimistic about the potential of our random features to be applied in Bayesian optimization, in particular by enabling scalable approximate sampling from GP posteriors (Wilson et al., 2020). Although we briefly explored this technique in section 6.3, in the future it could allow for sample-efficient Bayesian algorithms for complex tasks like Pareto frontier exploration and diverse optimization using the Bayesian algorithm execution framework (Neiswanger et al., 2021). These tasks are highly relevant to real-world drug discovery and there are scant new methods poised to solve them in a sample-efficient way. We hope that the methods presented in this paper enable impactful, large-scale applications of the Tanimoto kernel and its two extensions in chemoinformatics.

Figure 4: Run time and docking scores of BO using exact and approximate Thompson sampling. Solid lines are means over 5 trials, shaded regions are standard errors.