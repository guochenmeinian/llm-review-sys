# Distribution-Aware Data Expansion with

Diffusion Models

 Haowei Zhu\({}^{1}\), Ling Yang \({}^{*}\)\({}^{2}\), Jun-Hai Yong\({}^{1}\), Hongzhi Yin\({}^{3}\), Jiawei Jiang\({}^{4}\), Meng Xiao\({}^{5}\),

**Wentao Zhang\({}^{}\)\({}^{2}\)**, **Bin Wang \({}^{}\)\({}^{1}\)

\({}^{1}\)Tsinghua University \({}^{2}\)Peking University

\({}^{3}\)University of Queensland \({}^{4}\) Wuhan University \({}^{5}\) CNIC, CAS

wentao.zhang@pku.edu.cn wangbins@tsinghua.edu.cn

Equal Contribution.Corresponding author.

###### Abstract

The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion techniques include image transformation and image synthesis methods. Transformation-based methods introduce only local variations, leading to limited diversity. In contrast, synthesis-based methods generate entirely new content, greatly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose **DistDiff**, a training-free data expansion framework based on the **dist**ribution-aware **diffusion** model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its capability to generate distribution-consistent samples, significantly improving data expansion tasks. DistDiff consistently enhances accuracy across a diverse range of datasets compared to models trained solely on original data. Furthermore, our approach consistently outperforms existing synthesis-based techniques and demonstrates compatibility with widely adopted transformation-based augmentation methods. Additionally, the expanded dataset exhibits robustness across various architectural frameworks. Our code is available at https://github.com/haoweiz23/DistDiff.

## 1 Introduction

A substantial number of training samples are essential for unlocking the full potential of deep networks. However, the manual collection and labeling of large-scale datasets are both costly and time-intensive. This makes it difficult to expand data-scarce datasets. Therefore, it is of great value to study how to expand high-quality training data in an efficient and scalable way .

Automatic data expansion technology can alleviate the data scarcity problem by augmenting or creating diverse samples, it mitigates the bottleneck associated with limited data, thereby improving model's downstream performance and fostering greater generalization . One simple yet effective strategy is employing image transformation techniques such as cropping, rotation, and erasing to augment samples . Although these methods prove effective and have been widely applied in various fields, their pre-defined perturbations only introduce local variations to the images, thereby falling short in providing a diverse range of content change. In recenttimes, generative models have gained considerable attention [18; 44; 47; 50; 54; 45], exhibit impressive performance in various areas like image inpainting [36; 53], super-resolution [26; 55], and video generation [25; 41]. Generative models leverage text and image conditions to create images with entirely novel content, harnessing the expansive potential of data expansion . Nevertheless, there is a risk of generating images that deviate from the real data distribution. Therefore, when employing diffusion models for dataset expansion tasks, further research is necessary to ensure a match between synthetic data distributions and real data distributions.

There are several strategies aimed at mitigating the risk of distribution shift, which can be broadly categorized into two groups: training-based and training-free methods. The training-based methods [42; 52; 65] fine-tune pre-trained diffusion models to adapt target dataset, necessitating additional training costs and increasing the likelihood of overfitting on small-scale datasets. Other training-free methods [17; 22] eliminate potentially noisy samples by designing optimizing and filtering strategies, but they still struggle to generate data that conforms to the real data distribution.

In this work, we propose a training-free data expansion framework, dubbed **Dist**ribution-Aware **Diff**usion (DistDiff) to optimize generation results. As shown in Figure 1, DistDiff initially approximates the true data distribution using class-level and group-level prototypes obtained through hierarchical clustering. Subsequently, DistDiff utilizes these prototypes to formulate two synergistic energy functions. A residual multiplicative transformation is then applied to the latent data points, enabling the generation of data distinct from the original. Following this, the hierarchical energy guidance process refines intermediate predicted data points, optimizing the diffusion model to generate data samples that are consistent with the underlying distribution. DistDiff ensures fidelity and diversity in the generated samples through distribution-aware energy guidance. Experimental results demonstrate that DistDiff outperforms advanced data expansion techniques, producing better expansion effects and significantly improving downstream model performance. Our contributions can be summarized as follows:

* We introduce a novel diffusion-based data expansion algorithm, named DistDiff, which facilitates distribution-consistent data augmentation without requiring re-training.
* By leveraging hierarchical prototypes to approximate data distribution, we propose an effective distribution-aware energy guidance at both class and group levels in the diffusion sampling process.
* The experimental results illustrate that our DistDiff is capable of generating high-quality samples, surpassing existing image transformation and synthesis methods significantly.

## 2 Related Work

### Transformation-Based Data Augmentation

Traditional data augmentation techniques [5; 10; 23; 74; 76; 81; 82] typically involve expanding the dataset through distortive transformations, aiming to enhance the model's ability to capture data invariance and mitigate overfitting . For instance, scale invariance is cultivated through random cropping and scaling, while rotation invariance is developed through random rotation and flipping. Region mask-based methods [5; 12; 82] enhance model robustness against target occlusion by strategically obscuring portions of the target area. Interpolation-based methods [23; 74; 76] generate virtual samples by randomly blending content from two images. RandAugment  further boosts augmentation effectiveness by sampling from a diverse range of augmentation strategies. However, these methods induce only subtle changes on the original data through transformation, deletion, and blending, leading to a lack of diversity. Moreover, they are predefined and uniformly applied across the entire dataset, which may not be optimal for varying data types or scenarios.

Figure 1: A comparison unveils distinctions between conventional data expansion methods and our innovative distribution-aware diffusion framework, benefiting from hierarchical clustering and multi-step energy guidance.

### Synthesis-Based Data Augmentation

Generative data augmentation aims to leverage generative models to approximate the real data distribution, generating samples with novel content to enhance data diversity. GAN  excels at learning data distributions and producing unseen samples in an unsupervised manner [1; 20; 33; 39; 79; 80; 67]. While their efficacy has been demonstrated across diverse downstream tasks, studies indicate that training existing models like ResNet50  on images synthesized by BigGAN  yields subpar results compared to training on real images. This disparity in performance can be attributed to the limited diversity and potential domain gap between synthesized samples and real images. Additionally, the training processes of GAN are notoriously unstable, particularly with a low-data regime, and suffer from mode collapse, resulting in a lack of diversity [3; 19; 48]. In contrast, diffusion model-based methods  can offer better controllability and superior customization capabilities. Text-to-image models such as Stable Diffusion , DALL-E 2  and RPG  have demonstrated the creation of compelling high-resolution images [70; 72; 77]. Recently, large-scale text-to-image models have been used for data generation [2; 15; 58; 64; 63]. For example, LECF  utilized GLIDE  to generate images, filtering low-confidence samples to enhance zero-shot and few-shot image classification performance. SGID  leverages image descriptions generated by BLIP  to enhance the semantic consistency of generated samples. Feng et al.  filters out low-quality samples based on feature similarity between generated and reference images. GIF  creates new informative samples through prediction entropy and feature divergence optimization. However, it's crucial to note that datasets generated by existing methods may exhibit distribution shifts, impacting image classification performance significantly. Zhou et al.  address this issue by employing diffusion inversion to mitigate distributional shifts. In contrast, we propose a training-free approach, leveraging hierarchical prototypes as optimization targets to guide the generation process, thereby addressing distributional shifts. This approach offers the advantage of avoiding additional computational costs and overfitting issues associated with fine-tuning diffusion models.

## 3 Method

In this study, we introduce a distribution-aware data expansion framework utilizing Stable Diffusion as a prior generation model. This framework guides the diffusion model based on hierarchical prototype guidance criteria. As illustrated in Figure 2, our DistDiff initially employs an image encoder \(\) to extract instance features and subsequently derives hierarchical prototypes to approximate the real data distribution. Next, for a given seed image and corresponding text prompts, we extract image's

Figure 2: Overview of the DistDiff pipeline. DistDiff enhances the generation process in diffusion models with distribution-aware optimization. It approximates the real data distribution using hierarchical prototypes \(_{c}\) and \(_{g}\), optimizing the sampling process through distribution-aware energy guidance. Subsequently, original generated data point \(_{t}\) is refined for improved alignment with the real distribution.

latent feature and apply stochastic noise to it. Subsequently, in the denoising process, we optimize the latent features using a training-free hierarchical energy guidance process. Our optimization strategy ensures that the generated samples not only match the distribution but also carry new information to enhance model training.

### Task Definition

In the context of a small-scale training dataset, the data expansion task is designed to augment the original dataset \(_{o}=\{_{i},_{i}\}_{i=1}^{n_{a}}\) with a new set of synthetic samples, referred to as \(_{s}=\{^{}_{i},^{}_{i}\}_{i=1}^{n_{a}}\). Here \(_{i}\) and \(_{i}\) represent the sample and its corresponding label, where \(n_{o}\) and \(n_{s}\) respectively denote the original sample quantity and the synthetic sample quantity. The objective is to enhance the performance of a deep learning model trained on both the original dataset \(_{o}\) and the expanded dataset \(_{s}\) compared to a model trained solely on the original \(_{o}\). The crucial aspect lies in ensuring that the generated dataset is highly consistent with the distribution of the original dataset while being as informative as possible.

### Hierarchical Prototypes Approximate Data Distribution

Prototypes have been widely employed in class incremental learning methods to retain information about each class [40; 49]. In this work, we propose two levels of prototypes to capture the original data distribution. Firstly, the class-level prototypes \(_{c}\) are obtained by averaging feature vectors within the same class. The class vector aggregates high-level statistical information to characterize all samples from the same class as a collective entity. However, as class-level prototypes represent the class feature space as a single vector, potentially reducing informativeness, we further introduce group-level prototypes to capture the structure of the class feature space. Specifically, we divide all samples from the same class into \(K\) groups using agglomerative hierarchical clustering, followed by averaging feature vectors within each group to obtain \(K\) group prototypes \(_{}=\{^{}_{},^{2}_{},...,^{K}_{s}\}\). Instances with similar patterns are grouped together. Transitioning from class-level to group-level, the prototypes encapsulate abstract distribution information of the class at different scales.

Thanks to these hierarchical prototypes, we design two function \(^{c}_{}\) and \(^{c}_{}\) to evaluate the degree of distribution matching:

\[^{c}_{}(,_{})=\|() -_{}\|_{2},\] (1)

\[^{c}_{}(,_{})=\|() -^{j}_{}\|_{2},\] (2)

\[j=(((),\{^{j}_{}\}_{j=1} ^{K})),\]

where \(()\) means feature extractor, which could be ResNet , CLIP  or other deep models.

Note that these two functions evaluate the score of distribution matching from two perspectives. The value will be lower when \(\) is more consistent with the real distribution. As shown in Figure 2 (a), \(^{c}_{}\) gauges the distance of sample features to the class center, resulting in low scores for easy samples while high scores for hard samples that are situated at the boundaries of the distribution. On the other hand, \(^{c}_{}\) assesses distance from the group-level, offering lower scores for hard samples, while still maintaining relatively high scores for outlier samples. Consequently, these two scores mutually reinforce each other and are indispensable.

### Transform Data Points

Given a reference sample \((,)\), the pre-trained large-scale diffusion model \(\) can generate new samples \(^{}\) with novel content. We formalize this process as \(^{}=(()+)\), where \(()\) represents the latent feature representation and \(\) is the perturbation applied to latent features. Drawing inspiration from GIF , we introduce residual multiplicative transformation to the latent feature \(=()\) using randomly initialized channel-level noise \((0,1)\) and \((0,1)\). We impose an \(\)-ball constraint on the transformed feature to control the degree of adjustment within a reasonable range, _i.e._, \(\|-}\|_{}\), and derive \(}\) as follows.

\[}=_{z,}(())=_{z,}( (1+)+),\] (3)

where \(()\) represents the transformation function and \(_{z,}()\) denotes the projection of the transformed feature \(}\) onto the \(\)-ball of the original latent feature \(\).

Now, the key challenge lies in optimizing \(\) and \(\) to create new samples that align closely with the real data distribution. Another intuitive approach is to directly optimize latent features instead of performing residual multiplication transformations. However, directly optimizing latent features leads to minimal perturbations, making it challenging to achieve performance gains. We discuss this alternative approach in Section 4.4.

### Distribution-Aware Diffusion Generation

In a typical diffusion sampling process, the model iteratively predicts noise to progressively map the noisy \(_{T}\) into clean \(_{0}\). While existing data expansion methods [17; 22; 32] treat the generative model as a black box, focusing on filtering or optimizing the final generated \(_{0}\). The importance of the intermediate sampling stage is ignored, which plays a crucial role in ensuring data quality, especially as the image begins to take on a stable shape appearance. Diverging from prior approaches, we advocate for intervention at the intermediate denoising step for optimization.

Specifically, we first introduce energy guidance into standard reverse sampling process to optimize the transformation using Equation 4. As our energy guidance step is applied to the transformed data point, the transformed data point \(}_{t}\) is denoted as \(_{t}\) for simplicity.

\[^{} =-_{}(_{t},c),\] (4) \[^{} =-_{}(_{t},c),\]

where \(\) is the learning rate and \((_{t},)\) is the energy function measuring the compatibility between the transformed noisy data point \(_{t}\) and the given condition \(\), representing the real data distribution in this work. Equation 4 guides the sampling process and generates distribution-consistent samples. After that, the optimized \(_{t}^{}\) is obtained via Equation 3. However, directly measuring the distance between intermediate results \(_{t}\) with condition \(\) is impractical due to the difficulty in finding a pre-trained network that provides meaningful guidance when the input is noisy.

To address this issue, we leverage the capability that the diffusion model can predict the noise added to \(_{t}\), and thus predict a clean data point \(_{0|t}\), as shown in Equation 5. Then, the new energy function \(_{}(_{0|t},)\) based on the predicted clean data point is constructed to approximate \((_{t},)\).

\[_{0|t}=_{t}-}(_{t},t)}{}},\] (5)

where \(_{t}\) represents the noise scale and \(\) is the learned denoising network. Finally, we employ hierarchical prototypes \(_{}\) and \(_{}\) as conditions to construct our energy guidance in the following manner:

\[^{} =-_{}(^{}_{} (_{0|t},_{})+^{}_{}(_ {0|t},_{})),\] (6) \[^{} =-_{b}(^{}_{}(_{0|t},_{})+^{}_{}(_{0|t}, _{})).\]

Unlike existing methods that exclusively optimize the final sampling result \(_{0}\), our approach focuses on optimizing intermediate denoising steps within the sampling process. The detailed algorithm is shown in Appendix C. This novel strategy leads to substantial improvements in optimization results and will be further explored in Section 4.4.

## 4 Experiments

### Experimental Setups

DatasetsWe assess the performance of DistDiff across six image classification datasets, encompassing diverse tasks such as general object classification (Caltech-101 , CIFAR100-Subset , ImageNet ), fine-grained classification (Cars ), textual classification (DTD ) and medical imaging (PathMNIST ). More details are provided in Appendix B.1.

Compared MethodsWe conduct a comparative analysis between DistDiff and conventional image transformation methods, as well as diffusion-based expansion methods. Traditional image transformation techniques considered in the comparison comprise AutoAugment , RandAugment , Random Erasing , GridMask , and interpolation-based techniques like MixUp  and CutMix. For generative-based methods, we include the direct application of stable diffusion for data expansion, as well as the most recent state-of-the-art method, Stable Diffusion 1.4 , LECF , GIF-SD . The implementation details of these techniques are provided in Appendix B.3 and B.4.

### Implementation Details

In our experimental setup, we implement DistDiff based on Stable Diffusion 1.4 . The images created by Stable Diffusion have a resolution of \(512 512\) for all datasets. Throughout the diffusion process, we employ the DDIM  sampler for a \(50\)-step latent diffusion, with hyper-parameters for noise strength set at \(0.5\) and classifier free guidance scale at \(7.5\). The \(\) in Equation 3 is \(0.2\) by default. We use a ResNet-50  model trained from scratch on the original datasets as our guidance model. We assign \(K=3\) to each class when constructing group-level prototypes, the learning rate \(\) is \(10.0\), and optimization step \(M\) is set to \(20\) unless specified otherwise. After expansion, we concatenate the original dataset with synthetic data to create expanded datasets. We then train the classification model from random initialization for 100 epochs using these expanded datasets. During model training, we process images through random cropping to \(224 224\) using random rotation and random horizontal flips. Our optimization strategy involves using the SGD optimizer with a momentum of \(0.9\), and cosine decay with an initial learning rate of \(0.1\). All results are averaged over three runs with different random seeds. More implementation details can be found in the Appendix B.2.

### Main Results

**Comparison with Synthesis-Based Methods**

To investigate the effectiveness of methods for generating high-quality datasets for downstream classification model training, we initially compare our method, DistDiff, with existing synthesis-based methods on Caltech-101in terms of classification performance. Figure 3 highlights the superiority of our method over state-of-the-art techniques. Compared to the original stable diffusion, DistDiff exhibits an average improvement of 6.25%, illustrating that our method retains more distribution-aligned information from the original datasets. Additionally, GIF-SD , which uses a pre-trained CLIP  model to enhance class-maintained information and employs KL-divergence to encourage batch-wise sample diversity, is also surpassed by our method by 5.46% in accuracy. This can be attributed to DistDiff guiding the generation process from the distribution level with hierarchical prototypes, providing better optimization signals.

We also evaluated DistDiff against LECF , which enhances language prompts and filters samples with low confidence. We use LE enhanced to synthesize \(5\) synthetic datasets and filter with different thresholds. We assessed multiple Clip Filtering strengths in LECF and found that LECF did not achieve better performance compared to the original Stable Diffusion. This is due to our SD baseline already generates high-quality samples, and the additional filtering post-process may lead to data loss. Besides, both LECF and GIF-SD use auxiliary models to filter/guide the diffusion generation results. However, there are two main strengths of our DistDiff compared to these methods. First, DistDiff generates images end-to-end without requiring filter-based postprocessing. Second, our method is not sensitive to the classification performance of the auxiliary model, which is proved in the Appendix D.2. This suggests that DistDiff is simpler and more generalizable for data expansion tasks.

**Comparison with Transformation-Based Augmentation Methods**

In Table 1, we present a comparison of our methods with widely adopted data augmentation techniques for Caltech-101image classification. Our DistDiff method surpasses transformation-based augmentation methods by introducing a broader range of new content into images. Additionally, we demonstrate the compatibility of our approach with transformation-based data augmentation methods, leading to further improvements.

Scaling in Number of DataWe evaluate the scalability of our approach by assessing its advantages in classification model training across four datasets. We compare the performance of DistDiff with the original real dataset and strong augmentation method AutoAug  with varying numbers of

Figure 3: Our method outperforms state-of-the-art data expansion methods when trained on expanded datasets, underscoring the importance of a high-quality generator in training a classifier.

#### Versatility to Various Architectures

We conduct an in-depth assessment of the expanded datasets generated by DistDiffacross four distinct backbones: ResNet-50 , ResNeXt-50 , WideResNet-50 , and MobileNetv2 . These backbones are trained from scratch on \(5\) expanded Caltech-101dataset by DistDiff. The results presented in Table 3 affirm that our innovative methodology is effective and versatile across a spectrum of architectures.

#### Comparison with Stronger Classification Models

As we know, data expansion is typically applied in scenarios with data scarcity. However, if we use models pre-trained on large-scale datasets, the performance on the original training set can be significantly enhanced. In such cases, does our expanded dataset still offer improvements? To validate our method, we fine-tuned a ResNet-50 model pre-trained on ImageNet-1k  for ImageNette, Caltech-101, and StanfordCars, and a LAION  pre-trained CLIP-ViT-B32  model for PathMNIST. As shown in Table 2, the model achieved a high accuracy of 99.4% on ImageNette, which is a subset of its pretrained datasets. Further data expansion resulted in a slight decrease in performance. Similarly, on the general image dataset Caltech-101, which shares significant overlap with ImageNet data, our method demonstrated only slight improvement. However, on the more challenging fine-grained dataset StanfordCars, our method demonstrated obvious 3.56% accuracy improvement. For the medical image dataset PathMNIST, which exhibits a significantly different distribution, using DistDiff for data expansion effectively boosted classification performance by 5.18%. This highlights the importance of scaling up data when transferring pre-trained models to downstream tasks that exhibit significant distribution shifts.

#### Qualitative Analysis

In addition to the quantitative experiment results, we also gain a more intuitive understanding of the diverse changes facilitated by our method through visualization of the generated results. As shown in Figure 5, the images generated using our distribution-aware guidance approach exhibit high fidelity and diverse synthetic changes, including object texture, background, and color contrast. More visualization results can be found in Appendix D.3.

   Dataset & ImageNette & Caltech-101 & StanfordCars & PathMNIST \\  Original & 99.40 & 96.87 & 87.61 & 84.29 \\ Expanded 5\(\) by SD & 98.51 \((-0.89)\) & 96.91 \((+0.04)\) & 90.19 \((+2.58)\) & 86.81 \((+2.52)\) \\ Expanded 5\(\) by DistDiff & 99.30 \((-0.10)\) & 97.00 \((+0.13)\) & 91.17 \((+3.56)\) & 89.47 \((+5.18)\) \\   

Table 2: Comparison of using stronger pre-trained baseline models. On ImageNette , Caltech-101 , and StanfordCars  datasets, we employ an ImageNet-1k  pre-trained ResNet-50  model. For the PathMNIST  dataset, we fine-tune using the stronger CLIP-ViT-B/32 baseline.

Figure 4: Performance comparison across different scale data sizes. Our method demonstrates significant improvements in classification model performance in both low-data and large-scale data scenarios, outperforming the transformation method AutoAug and the synthesized method Stable Diffusion 1.4.

    & Default & AutoAug & RandAug & Random Erasing & GridMask & MixUp & CutMix \\  Original & 66.71 & 74.34 & 74.07 & 74.22 & 73.88 & 78.64 & 70.13 \\ DistDiff & 83.38 & 82.93 & 83.21 & 83.05 & 83.48 & 81.06 & 85.27 \\   

Table 1: Comparison of transformation-based augmentation methods on Caltech-101. Our approach, combined with default augmentation (crop, flip, and rotate), consistently outperforms existing advanced transform-based methods and can be further improved by combining these techniques.

### Ablation Study

Hierarchical PrototypesWe delved further into how each component of DistDiff impacts its data expansion performance. As depicted in Table 4, utilizing both \(_{}\) and \(_{}\) contributes to enhancing the model's expansion performance, showcasing their ability to optimize the generated sample distribution at the class-level and group-level, respectively. Moreover, combining \(_{}\) and \(_{}\) results in a further performance improvement, validating the effectiveness of integrating representations from different hierarchical levels. Additionally, with the introduction of our approach, the Frechet Inception Distance (FID) values notably decrease, indicating that our proposed FID effectively optimizes the model to generate samples more aligned with the real distribution, thereby reducing the domain gap between the generated dataset and the real dataset.

Augmentation within DiffusionIn the application of energy guidance, perturbation is introduced at the \(M\)-th step, and these subsequent predicted data points are optimized. Theoretically, optimizing predictions at different stages yields distinct effects. Our exploration focused on the optimization step \(M\), and the experimental results are illustrated in Table 5. When \(M\) is small, indicating optimization at a later stage (i.e., the refinement stage), the change in generated results is already minimal, resulting in relatively consistent optimization outcomes. Conversely, when \(M\) increases, corresponding to optimization at an intermediate stage (i.e., the semantic stage), the generated results are in the stage of forming semantics and exhibit significant changes. Hence, this stage plays a crucial role in determining the final generated results. Furthermore, there is a decline in performance during the early chaos stage (\(M\)=25), as the data points in this initial phase are too chaotic to establish an optimal target for optimization. We observed that achieving higher data expansion performance is possible when optimized in the semantic stage, with optimal results obtained when \(M\)=20.

   \(_{}\) & \(_{}\) & Accuracy \(\) & FID-3K \(\) \\   & \(76.57 0.35\) & 72.56 \\  & ✓ & \(82.70 0.07\) & 68.82 \\ ✓ & & \(82.84 0.54\) & 68.66 \\ ✓ & ✓ & \(83.09 0.11\) & 67.72 \\   

Table 4: Comparison of accuracy and FID in expanding Caltech-101 by \(5\), with and without hierarchical prototypes in DistDiff.

   Backbone & Original & DistDiff \\  ResNet-50  & 66.71 & 83.09 \\ ResNeXt-50  & 67.60 & 83.75 \\ WideResNet-50  & 66.51 & 83.51 \\ MobileNetV2  & 74.39 & 83.85 \\   

Table 3: Performance comparison of models trained on original Caltech-101 datasets and 5x expanded datasets by DistDiff.

Figure 5: The visualization of synthetic samples generated by our method, showcasing high fidelity, diversity, and alignment with the original data distribution.

More Optimization StepsFurthermore, a natural idea arises regarding the potential improvement in effectiveness through the optimization of more optimization steps. Therefore, we further explored increasing the number of steps in the semantic stages. As shown in Table 7, increasing the number of optimization steps in semantic stages enhances performance. However, further increases in optimization steps lead to a decline in performance. This can be attributed to excessive optimization strength in energy guidance, which causes data distortion.

Compared with Direct Guidance on Latent PointWe evaluate our transform guidance strategy against an alternative strategy that directly guides the latent data points while ignoring the residual transform preprocess, a method found useful in previous works . We initially conducted a grid search for this alternative strategy to find the optimal learning rate, \(\) (i.e., [0.1, 1, 10, 20]), and guide step, \(M\) (i.e., ). We found the best result of 76.77% was achieved with \(=10\) and \(M=10\), which is only slightly better than the original Stable Diffusion but lags behind our DistDiff, which achieved 83.19%. This indicates that applying the residual multiplicative transformation to the latent feature offers more optimization potential.

Determination of Group-Level Prototype Number \(K\)The determination of the number \(K\) of group-level prototypes is crucial for accurately approximating the real data distribution. In Table 6, we compare the outcomes associated with varying numbers of prototypes. The results highlight that the optimal number of prototypes is found at \(K=3\). We posit that an insufficient number of prototypes may impede the characterization of the real distribution, leading to diminished performance. Conversely, an excessive number of prototypes may lead to overfitting of noisy sample points, also resulting in suboptimal performance. Furthermore, we present a visualization analysis of group-level prototypes in Figure 6. The visual representation demonstrates that an appropriate number of group-level prototypes can effectively cover the real distribution space, aligning with the underlying motivation of our DistDiff.

Computational EfficiencyOur DistDiff is not only training-free but also highly efficient in processing. As illustrated in Figure 2, DistDiff introduces only a few optimization steps in the original diffusion process. We analyze the time costs of our methods. Stable Diffusion generates per sample in 12.65 seconds on average, while our DistDiff achieves the same in 13.13 seconds, with the increased time costs being negligible. We can conclude that DistDiff achieves a notable improvement over stable diffusion models, with only a slight increase in computation costs.

## 5 Conclusion

This paper presents DistDiff, a distribution-aware data expansion method employing a stable diffusion model for data expansion. The proposed method optimizes the diffusion process to align the synthesized data distribution with the real data distribution. Specifically, DistDiff constructs hierarchical prototypes to effectively represent the real data distribution and refines intermediate features within the sampling process using energy guidance. We evaluate our method through extensive experiments on six datasets, showcasing its superior performance over existing methods.

## 6 Acknowledgments

This work was supported by the National Natural Science Foundation of China under Grant 62072271.

Figure 6: The visualization of group-level prototypes alongside original sample features. Here \(\) is the sample point and \(\) is group-level prototype. By selecting an appropriate number \(K\), these prototypes effectively span the feature space, providing an approximation of the real data distribution.