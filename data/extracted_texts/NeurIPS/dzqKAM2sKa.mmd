# Hypernetwork-based Meta-Learning

for Low-Rank Physics-Informed Neural Networks

 Woojin Cho\({}^{}\)  Kookjin Lee\({}^{}\)1  Donsub Rim\({}^{@sectionsign}\)  Noseong Park\({}^{}\)1

\({}^{}\) Yonsei University

\({}^{}\) Arizona State University

\({}^{@sectionsign}\) Washington University in St. Louis

snowmoon@yonsei.ac.kr, kookjin.lee@asu.edu,

rim@wustl.edu, noseong@yonsei.ac.kr

###### Abstract

In various engineering and applied science applications, repetitive numerical simulations of partial differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape optimization over many design parameters) and solvers are required to perform rapid execution. In this study, we suggest a path that potentially opens up a possibility for physics-informed neural networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver. Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they require repetitive time-consuming training of neural networks, which is not suitable for _many-query_ scenarios. To address this issue, we propose lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allow efficient solution approximations for varying PDE input parameters. Moreover, we show that the proposed method is effective in overcoming a challenging issue, known as "failure modes" of PINNs.

## 1 Introduction

Physics-informed neural networks (PINNs)  are a particular class of coordinate-based multi-layer perceptrons (MLPs), also known as implicit neural representations (INRs), to numerically approximate solutions of partial differential equations (PDEs). That is, PINNs are taking spatiotemporal coordinates \((,t)\) as an input and predict PDE solutions evaluated at the coordinates \(u_{}(,t)\) and are trained by minimizing (implicit) PDE residual loss and data matching loss at initial and boundary conditions. PINNs have been successfully applied to many different important applications in computational science and engineering domain: computational fluid dynamics [2; 3], cardiac electrophysiology simulation , material science , and photonics , to name a few.

PINNs are, however, sharing the same weakness with coordinate-based MLPs (or INRs), which hinders the application of PINNs/INRs to more diverse applications; for a new data instance (e.g., a new PDE for PINNs or a new image for INRs), training a new neural network (typically from scratch) is required. Thus, using PINNs to solve PDEs (particularly, in parameterized PDE settings) is usually computationally demanding, and this burden precludes the application of PINNs to important scenarios that involve _many queries_ in nature as these scenarios require the parameterized PDE models to be simulated thousands of times (e.g., design optimization, uncertainty propagation), i.e., requiring PDE solutions \(u(,t;)\) at many PDE parameter settings \(\{^{(i)}\}_{i=1}^{N_{}}\) with very large \(N_{}\).

To mitigate the above described issue, we propose i) a low-rank structured neural network architecture for PINNs, denoted as low-rank PINNs (LR-PINNs), ii) an efficient rank-revealing training algorithm, which adaptively adjust ranks of LR-PINNs for varying PDE inputs, and iii) a two-phase procedure (offline training/online testing) for handling _many-query_ scenarios. This study is inspired by the observations from the studies of numerical PDE solvers  stating that numerical solutions of parametric PDEs can be often approximated in a low-rank matrix or tensor format with reduced computational/memory requirements. In particular, the proposed approach adopts the computational formalism used in reduced-order modeling (ROM) , one of the most dominant approaches in solving parameteric PDEs, which we will further elaborate in Section 3.

In essence, LR-PINNs represent the weight of some internal layers as a low-rank matrix format. Specifically, we employ a singular-value-decomposition (SVD)-like matrix decomposition, i.e., a linear combination of rank-1 matrices: the weight of the \(l\)-th layer is \(W^{l}=_{j=1}^{r}s_{i}^{l}_{i}^{l}_{i}^{l}\) with the rank \(r=(n_{l},n_{l+1})\), where \(W^{l}^{n_{l+1} n_{l}},_{i}^{l}^{n_{l+1}}\), \(_{i}^{l}^{n_{l}}\), and \(_{i}^{l}\). The ranks of the internal layers, however, typically are not known a priori. To address this issue, we devise a novel hypernetwork-based neural network architecture, where the rank-structure depending on the PDE parameters \(\) is learned via training. In short, the proposed architecture consists of i) a _lightweight_ hypernetwork module and ii) a low-rank solution network module; the hypernetwork takes in the PDE parameters and produces the coefficients of the rank-1 series expansion (i.e., \(()=f^{}()\)). The low-rank solution network module i) takes in the spatiotemporal coordinates \((,t)\), ii) takes the forward pass through the linear layers with the weights \(W^{l}()=_{i=1}^{r}s_{i}^{l}()_{i}^{l}_{i}^{l }\), of which \(s_{i}()\) comes from the hypernetwork, and iii) produces the prediction \(u_{}(,t;)\). Then, the training is performed via minimizing the PINN loss, i.e., a part of the PINN loss is the PDE residual loss, \(\|(u_{}(x,t;);)\|_{2}\), where \((,;)\) denotes the parameterized PDE residual operator.

We show the efficacy and the efficiency of our proposed method for solving some of the most fundamental parameterized PDEs, called _convection-diffusion-reaction equations_, and _Helmholtz equations_. Our contributions include:

1. We employ a low-rank neural network for PINNs after identifying three research challenges to address.
2. We develop a hypernetwork-based framework for solving parameterized PDEs, which computes solutions in a rank-adaptive way for varying PDE parameters.
3. We demonstrate that the proposed method resolves the "_failure modes_" of PINNs.
4. We also demonstrate that our method outperforms baselines in terms of accuracy and speed.

## 2 Naive low-rank PINNs

Let us being by formally defining LR-PINNs and attempt to answer relevant research questions. LR-PINNs are a class of PINNs that has hidden fully-connected layers (FC) represented as a low-rank weight matrix. We denote this intermediate layer as lr-FC: the \(l\)-th hidden layer is defined such that

\[^{l+1}=^{l}(^{l})^{l+ 1}=U_{r}^{l}(_{r}^{l}(V_{r}^{l}^{l}))+^{l},\] (1)

where \(U_{r}^{l}^{n_{l+1} r}\) and \(V_{r}^{l}^{n_{l} r}\) denote full column-rank matrices (i.e., rank \(r n_{l},n_{l+1}\)) containing a set of orthogonal basis vectors, and \(_{r}^{l}^{r r}\) is a diagonal matrix, \(_{r}^{l}=(_{r}^{l})\) with \(_{r}^{l}^{r}\).

Memory efficiency:LR-PINNs with a rank of \(r\) and \(L\) hidden layers require \(O((2n_{l}+1)rL)\) memory as opposed to \(O(n_{l}^{2}L)\) required by regular PINNs.

Computational efficiency:The forward/backward pass of LR-PINNs can be computed efficiently by utilizing a factored representation of the weights. To simplify the presentation, we describe only the forward pass computation; the forward pass is equivalent to perform three small matrix-vector products (MVPs) in sequence as indicated by the parentheses in Eq. (1).

Challenges:Representing the weights of hidden layers itself is straightforward and indeed has been studied actively in many different fields of deep learning, e.g., NLP . However, those approaches typically assume that there exist pre-trained models and approximate the model weights by running the truncated SVD algorithm. Our approach is different from these approaches in that we attempt to reveal the ranks of internal layers as the training proceeds, which brings unique challenges. These challenges can be summarized with some research questions, which include: **C1**) "should we make all parameters learnable (i.e., \((U^{l},V^{l},^{l}))\)", **C2**) "how can we determine the ranks of each layer separately, and also adaptively for varying \(^{}}\)", and **C3**) "can we utilize a low-rank structure to avoid expensive and repetitive training of PINNs for every single new \(\) instances?". In the following, we address these questions by proposing a novel neural network architecture.

## 3 Hyper-LR-PINNs: hypernetwork-based meta-learning low-rank PINNs

We propose a novel neural network architecture based on a hypernetwork and an associated training algorithm, which address the predescribed challenges. To distinguish them, we hereinafter call LR-PINN with (resp. w/o) our proposed hypernetwork as "Hyper-LR-PINN" (resp. "Naive-LR-PINN").

Design goals:Here we attempt to resolve all challenges by setting up several design goals that are inspired from our domain knowledge in the field (and also from some preliminary results that can be found in Appendix D):

1. build a _single set_ of basis vectors \(U^{l}\) and \(V^{l}\), preferably as _orthogonal_ as possible, that perform well over a range of PDE parameters,
2. build LR-PINNs with an _adaptive and layer-wise rank structure_ that depends on PDE parameters (e.g., a higher rank for a higher convective PDE) and,
3. make _only the diagonal elements_, denoted \(^{l}_{r}\) in Eq. (1), _learnable_ to achieve high efficiency once a proper set of basis vectors and the rank structure are identified.

Our design largely follows the principles of ROMs, where the expensive computation is offloaded to an _offline_ phase to build a cheap surrogate model that can perform an efficient computation in an _online_ phase for a test set. To make an analogy, consider parameterized dynamical systems (which may arise in semi-discretization of time-dependent PDEs): \((t;)}{t}=((t;); {})\), where \(^{N}\). In the offline phase, the method seeks a low-dimensional linear trial basis where the reduced representation of the solution lie on, which is achieved by performing high-fidelity simulations on a set of PDE parameter instances \(\{^{(i)}\}_{i=1}^{n_{}}\) and constructing a trial linear subspace \(:=(_{p})\) with \(_{p}=[_{1},,_{p}]^{N p}\) from the solution snapshots collected from the high-fidelity simulations. In the online phase, the solutions at a set of test PDE parameter instances \(\{^{(i)}\}_{i=1}^{n_{}}\) are approximated as \((t,)_{p}(t,)\) with \(^{p}\), and a low-dimensional surrogate problem is derived as \((t;)}{t}=_{p}^{}( _{p}(t;);)=}((t;);) ^{p}\), which can be rapidly solved, while not losing too much accuracy. See Appendix C for an illustrative explanation and details of ROMs.

Taking a cue from the ROM principles, we design our model to operate on a common set of basis vectors \(\{U^{l}_{r}\}\) and \(\{V^{l}_{r}\}\), which are obtained during the offline phase (analogous to \(_{p}\) in ROMs) and update only the diagonal elements \(\{^{l}_{r}\}\) during the online phase (analogous to \(\) in ROMs). Now, we elaborate our network design and the associated two-phase algorithm. For the connection to the ROM, which explains details on the context/query sets, can be found in Appendix C.

Figure 1: The architecture of Hyper-LR-PINN consisting of i) the hypernetwork generating model parameters (i.e., diagonal elements) of LR-PINN and ii) LR-PINN approximating solutions.

### Hypernetwork-based neural network architecture

The proposed framework has two computational paths: a path for the hypernetwork and a path for LR-PINN (Figure 1). The hypernetwork path reads the PDE parameter \(\) and outputs the diagonal elements \(\{^{l}\}_{l=1}^{L}\) of lr-FCs. The LR-PINN path reads \((,t)\) and the output of the hypernetwork, and outputs the approximated solution \(u_{}\) at \((,t;)\), which can be written as follows:

\[u_{}((,t);)=u_{}((,t);f^{}()),\]

where \(f^{}()\) denotes the hypernetwork such that \(\{^{l}()\}_{l=1}^{L}=f^{}()\). We denote \(\) as a function of \(\) to make it explicit that it is dependent on \(\). The internals of LR-PINN can be described as with \(^{0}=[,t]^{}\)

\[^{1} =(W^{0}^{0}+^{0}),\] \[^{l+1} =(U^{l}(^{l}()(V^{l^{}}^{l}))+ ^{l}),l=1,,L,\] \[u_{}((,t);) =(W^{L+1}^{L+1}+^{L+1}),\]

where \(^{l}()=(^{l}())\). The hypernetwork can be described as the following initial embedding layer, where \(^{0}=\), followed by an output layer:

\[^{m}=(W^{,m}^{m-1}+^{,m}), m =1,,M,\]

\[^{l}()=(W^{,l}^{M}+^{,l}), l=1,,L,\]

where ReLU is employed to automatically truncate the negative values so that the adaptive rank structure for varying PDE parameters can be revealed (i.e., the number of non-zeros (NNZs)2 in \(^{l}()\) varies depending on \(\)).

### Two-phase training algorithm

Along with the framework, we present the proposed two-phase training algorithm. Phase 1 is for learning the common set of basis vectors and the hypernetwork and Phase 2 is for fine-tuning the network for a specific set of test PDE parameters. Table 1 shows the sets of model parameters that are being trained in each phase. (See Appendix E for the formal algorithm.)

In Phase 1, we train the hypernetwork and the LR-PINN jointly on a set of collocation points that are collected for varying PDE parameters. Through the computational procedure described in Section 3.1, the approximated solutions at the collocation points are produced \(u_{}((_{j},t_{j});^{(i)})\). Then, as in regular PINNs, the PDE residual loss and the data matching loss can be computed. The small difference is that the PDE operator, \(\), for the residual loss is also parameterized such that \((u_{}((_{j},t_{j});^{(i)});^{(i)})\). As we wish to obtain basis vectors that are close to orthogonal, we add the following orthogonality constraint based on the Frobenius norm to the PINN loss :

\[w_{1}\|U^{l}U^{l}-I\|_{F}^{2}+w_{2}\|V^{l}V^{l}-I\|_{F}^{2},\] (2)

where \(w_{1}\) and \(w_{2}\) are penalty weights. (See Appendix J.)

In Phase 2, we continue training LR-PINN for approximating the solutions of a target PDE parameter configuration. We i) fix the weights, the biases, and the set of basis vectors of lr-FC obtained from Phase 1, ii) convert the diagonal elements to a set of learnable parameters after initializing them with the values from the hypernetwork, and iii) detach the hypernetwork. Thus, only the trainable parameters from this point are the set of diagonal elements, first and last linear layers. The hypernetwork-initialized diagonal elements serve as a good starting point in (stochastic) gradient update optimizers (i.e., require less number of epochs). Moreover, significant computational savings can be achieved in the gradient update steps as only the diagonal elements are updated (i.e., \(_{l+1}_{l}+_{}L\) instead of \(_{l+1}_{l}+_{}L\)).

## 4 Experiments

We demonstrate that our proposed method significantly outperforms baselines on the 1-dimensional/2-dimensional PDE benchmarks that are known to be very challenging for PINNs to learn [17; 18]. We report the average accuracy and refer readers to Appendix U for the std. dev. of accuracy after 3 runs.

Baselines for comparison:Along with the vanilla PINN , our baselines include several variants. PINN-R  denotes a model that adds skip-connections to PINN. PINN-S2S  denotes a method that uniformly segments the temporal domain and proceeds by training each segment one by one in a temporal order. PINN-P denotes a method that directly extends PINNs to take \((,t,)\) as input and infer \(u_{}(,t,)\), i.e., \(\) is being treated as a coordinate in the parameter domain.

We also apply various meta-learning algorithms to Naive-LR-PINN: model-agnostic meta learning (MAML)  and Reptile  -- recall that Naive-LR-PINN means that LR-PINN without our proposed hypernetwork-based meta-learning and therefore, MAML and Reptile on top of Naive-LR-PINN can be compared to Hyper-LR-PINN. In the parameterized PDE setting, we can define a task, \(^{(i)}\), as a specific setting of the PDE parameters, \(^{(i)}\). Both MAML and Reptile seek initial weights of a PINN, which can serve as a good starting point for gradient-based optimizers when a solution of a new unseen PDE parameter setting is sought. See Appendix G for details. For reproducibility, we refer readers to Appendix F, including hyperparameter configuration and software/hardware environments.

Evaluation metrics:Given the \(i\)-th PDE parameter instance \(^{(i)}\), the ground-truth solution evaluated at the set of test collocation points can be defined collectively as \(^{(i)}=[u(x_{1},t_{1};^{(i)}),,u(x_{N},t_{N};^{(i )})]^{}\) and likewise for PINNs as \(^{(i)}_{}\). Then the absolute error and the relative error can be defined as \(\|^{(i)}-^{(i)}_{}\|_{1}\) and \(\|^{(i)}-^{(i)}_{}\|_{2}/\|^{(i)}\|_{2}\), respectively. In Appendix N, we measure the performance on more metric: max error and explained variance score.

### Benchmark parameterized PDEs

1D PDEs:For our first benchmark parameterized PDEs, we consider the following parameterized convection-diffusion-reaction (CDR) equation:

\[u_{t}+ u_{x}- u_{xx}- u(1-u)=0, x,\ t[0,T],\]

where the equation describes how the state variable \(u\) evolves under convective (the second term), diffusive (the third term), and reactive (the fourth term) phenomena.3 The triplet, \(=(,,)\), defines the characteristics of the CDR equations: how strong convective/diffusive/reactive the equation is, respectively. A particular choice of \(\) leads to a realization of a specific type of CDR processes.

There is a set of \(\) which makes training PINNs very challenging, known as "failure modes" : i) convection equations with high convective terms (\( 30\)) and ii) reaction(-diffusion) equations with high reactive terms (\( 5\)). We demonstrate that our method does not require specific-PDE-dedicated algorithms (e.g., ) while producing comparable/better accuracy in low rank.

   Phase 1 & \((U^{},V^{},^{})_{l=1}^{L},W^{0},W^{+1},^{},^{+1}\) & (LR-PINN), \\  & \(\{(W^{,m},^{,m})\}_{m=1}^{M}\), \(\{(W^{,l},^{,l})\}_{l=1}^{L}\) (hypernetwork), \\  Phase 2 & \(\{^{}\}_{l=1}^{L}\), \(W^{0},W^{L+1},^{},^{+1}\) & (LR-PINN) \\   

Table 1: Learnable parameters in each phase

Figure 2: [Convection equation] Solution snapshots for \(=40\)

2D PDEs:As the second set of benchmarks, we consider the 2-dim parameterized Helmholtz equation:

\[u_{xx}+u_{yy}+k^{2}u-q(x,y;a_{1},a_{2})=0,\]

where \(q(x,y;a_{1},a_{2})\) denotes a specific parameterized forcing term and the solution \(u\) can be calculated analytically (See Appendix T). As observed in the failure modes of convection equations (high convective terms), certain choices of the parameters in the forcing term, i.e., \(a_{1},a_{2}\), make the training of PINNs challenging as the solutions become highly oscillatory.

### Experimental results

#### 4.2.1 Performance on the failure mode of the CDR equation

Solution accuracy:As studied in prior work, vanilla PINNs fail to approximate solutions exhibiting either highly oscillatory (due to high convection) or sharp transient (due to high reaction) behaviors. Here, we present the results of convection equations and leave the results on reaction(-diffusion) equations in Appendix N. For convection equations4, failures typically occur with high \(\), e.g., \( 30\). Table 2 reports the results of all considered models trained on \(\)5 and essentially shows that Hyper-LR-PINN (Figure 2(c)) is the only low-rank method that can resolve the failure-mode and there are only marginal decreases in accuracy compared to Hyper-LR-PINN in full rank. For reaction(-diffusion) equations, we observe similar results, Hyper-LR-PINN outperforms the

    &  &  \\  \(\) & **Rank** &  &  &  &  &  &  \\  & & Abs. err. & Rel. err. & Abs. err. & Rel. err. & Abs. err. & Rel. err. & Abs. err. & Rel. err. & Abs. err. & Rel. err. \\   & 10 & 0.5617 & 0.5344 & 0.4117 & 0.4098 & 0.6577 & 0.6294 & 0.5893 & 0.5551 & & \\   & 20 & 0.5501 & 0.5253 & 0.4023 & 0.4005 & 0.6836 & 0.6452 & 0.6144 & 0.5779 & & & \\   & 30 & 0.5327 & 0.5126 & 0.4233 & 0.4204 & 0.5781 & 0.5451 & 0.6048 & 0.5704 & 0.0360 & 0.0379 & 0.0375 & 0.0389 \\   & 40 & 0.5257 & 0.5076 & 0.3746 & 0.3744 & 0.5848 & 0.5515 & 0.5757 & 0.5442 & & & \\   & 50 & 0.5327 & 0.5126 & 0.4152 & 0.4127 & 0.5898 & 0.5562 & 0.5817 & 0.5496 & & & \\   & 10 & 0.5663 & 0.5357 & 0.5825 & 0.5465 & 0.6663 & 0.6213 & 0.5786 & 0.5446 & & & \\   & 20 & 0.5675 & 0.5369 & 0.6120 & 0.5673 & 0.6841 & 0.6433 & 0.5971 & 0.5606 & & & \\   & 30 & 0.6081 & 0.5670 & 0.5864 & 0.5503 & 0.5819 & 0.5466 & 0.5866 & 0.5006 & 0.0428 & 0.0443 & 0.0448 & 0.0461 \\   & 40 & 0.5477 & 0.5227 & 0.5954 & 0.5548 & 0.5809 & 0.5462 & 0.5773 & 0.5435 & & & \\   & 50 & 0.5449 & 0.5208 & 0.6010 & 0.5619 & 0.5870 & 0.5514 & 0.5731 & 0.5404 & & & \\   & 10 & 0.5974 & 0.5632 & 0.5978 & 0.5611 & 0.6789 & 0.6446 & 0.5992 & 0.5632 & & & \\   & 20 & 0.5890 & 0.5563 & 0.6274 & 0.5820 & 0.7008 & 0.601 & 0.6189 & 0.5853 & & & \\   & 30 & 0.6142 & 0.5724 & 0.6011 & 0.5652 & 0.6072 & 0.5700 & 0.6126 & 0.5810 & 0.0603 & 0.0655 & 0.0656 & 0.0722 \\   & 40 & 0.5560 & 0.5293 & 0.6126 & 0.5715 & 0.6149 & 0.5832 & 0.6004 & 0.5638 & & & \\   & 50 & 0.6161 & 0.5855 & 0.6130 & 0.5757 & 0.6146 & 0.5799 & 0.6007 & 0.5645 & & & \\   

Table 2: [Convention equation] The absolute and relative errors of the solutions of convection equations with \(=\{30,35,40\}\). We apply the curriculum learning proposed in , MAML, and Reptile to Naive-LR-PINN. Therefore, we focus on comparing various Naive-LR-PINN-based enhancements and our Hyper-LR-PINN. See Appendix N for other omitted tables.

Figure 3: [Convention equation] Per epoch averaged train loss and test errors for \(\) for Phase 1 (meta-learning phase). We refer readers to Appendix O for Phase 2 loss curve.

best baseline by more than an order of magnitude (See Appendix N). Moreover, we report that Hyper-LR-PINN outperforms in most cases to the baselines that operate in "full-rank": meta-learning methods (MAML, Reptile), PINN-S2S, and PINN-P (Appendix M).

Loss curves:Figure 3 depicts two curves of train loss and test errors as the meta-learning algorithms proceed (MAML, Reptile, and ours). As opposed to the optimization-based meta-learning algorithms, which tend to learn meta-initial weights that perform well "on average" over randomly sampled training tasks, our hypernetwork-based method minimizes the loss for each individual task simultaneously. Loss curves for Phase 2 are reported in Appendix O, which essentially show that the baseline meta-learners do not provide good initializations.

Rank structure:Table 3 compares the number of trainable model parameters for Naive-LR-PINN, PINN, and our method. In our model, each hidden layer has a different rank structure, leading to 351 trainable parameters, which is about (\(\)30) smaller than that of the vanilla PINN, Cf. merely decreasing the model size of PINNs leads to poor performance (Appendix S). Figure 4(a) shows how the model adaptively learns the rank structure for varying values of \(\). We observe that, with the low-rank format, Hyper-LR-PINN provides more than (\( 4\)) speed up in training time compared to the vanilla PINN. More comparisons on this aspect will be presented in the general CDR settings.

Ablation on fixed or learnable basis vectors:We compare the performance of the cases, where the basis vectors \(\{U_{r}^{l},V_{r}^{l}\}\) are trainable. We observe that the fixed basis vectors lead to an order of magnitude more accurate predictions as well as faster convergence (See Appendix Q for plots).

Ablation on orthogonality constraint:We compare the performances of the cases, where the proposed model is trained without the orthogonality constraint, Eq. (2). Experimental results show that the orthogonality constraint is essential in achieve good performance (See Appendix J).

Many-query settings (inter/extra-polation in the PDE parameter domain):Next, we test our algorithm on the _many-query_ scenario, where we train the proposed framework with the 11 training PDE parameters \(\) (Phase 1), freeze the hypernetwork and the learned basis, and retrain only the diagonal elements for 150 unseen test PDE parameters (Phase 2). Figure 5 depicts the absolute and relative errors of the solutions, which are as accurate as the ones on the training PDE parameters, in particular, for \(<30\) (i.e., extrapolation). Although the model produces an increasing error for \(>40\), the relative error is still around \(\)10%, which cannot be achievable by vanilla PINNs. The right chart of Figure 5 shows the number of trainable model parameters, which is determined by the rank (i.e., the output of the hypernetwork). As PINNs fail to produce any reasonable approximation in this \(\) regime, we make comparisons in the general CDR settings below.

Figure 4: Adaptive rank on convection equation (the left and the middle panels). The magnitude of the learned diagonal elements \(^{2}\) of the second hidden layer for varying \(\) (the right panel).

  
**Model** &  &  & **PINN** \\ 
**Rank** & 10 & 20 & 30 & 40 & 50 & Adaptive & - \\ 
**\# Parameters** & 381 & 411 & 441 & 471 & 501 & \(\)351 & 10,401 \\   

Table 3: Comparisons of model size 

#### 4.2.2 Performance on the general case of the CDR equation

Solution accuracy:Next, we present the performance of the proposed method on solving the benchmark PDEs (not only in the failure mode but) in all available settings for \(,,\). Due to space reasons, we present only a subset of our results and leave the detailed results in Appendix M. Table 4 shows that the proposed Hyper-LR-PINN mostly outperforms the baselines that operate in full rank.

Some observations on learned low-rank structures:Again, the proposed method learns rank structures that are adaptive to the PDE parameter (Figure 4(b)). The learned rank structures vary more dynamically (\(r\) from 20 to 34) as the PDE parameter range  is larger compared to .

Figure 4(c) visualizes the magnitude of the learned diagonal elements \(^{2}\) in the second hidden layer. The horizontal axis indicates the index of the element in \(^{2}\), which is sorted in descending order for analysis, and the vertical axis indicates the \(\) value. The depicted result agrees with our intuitions in many aspects: i) the values decay fast (indicating there exist low-rank structures), ii) the value of each element \(_{i}^{2}\) either increases or decreases gradually as we vary \(\), and iii) for higher \(\), higher frequency basis vectors are required to capture more oscillatory behavior exhibited in the solutions, which leads to higher ranks. We report the information for other layers and other problems in Appendix L.

Computational cost:Here, we compare the computational cost of Hyper-LR-PINN and PINN in a _many-query_ scenario. We train Hyper-LR-PINN for \(\) with interval 1 (Phase 1) and perform Phase 2 on \(\) with interval 0.5 (including unseen test PDE parameters). We record the number of epochs required for PINN and Hyper-LR-PINN to reach an L2 absolute error of less than 0.05. Figure 6 reports the number of required epochs for each \(\), showing that the number of epochs required for Hyper-LR-PINN slowly increases while that for PINNs increases rapidly; for most cases, Hyper-LR-PINN reaches the target (i.e., 0.05) in only one epoch. We also emphasize that Hyper-LR-PINN in Phase 2 requires much less FLOPS in each epoch (See Section 3.2).

#### 4.2.3 Performance on the 2D Helmholtz equation

Now we report experimental results on 2D Helmholtz equations for varying \(a_{1}\) and \(a_{2}\). Under the same condition in Table 2, we train Hyper-LR-PINN for \(a\) with interval 0.1 \((a=a_{1}=a_{2})\), and compare the performance with PINN. Surprisingly, Hyper-LR-PINN approximates the solution

    &  &  \\   & **PINN** & **PINN-P** & **PINN-S2S** & **MAML** & **Reptile** & **Ours** \\ 
**Convection** & 0.0327 & 0.0217 & 0.2160 & 0.1036 & 0.0347 & 0.0085 \\ 
**Reaction** & 0.3907 & 0.2024 & 0.5907 & 0.0057 & 0.0064 & 0.0045 \\ 
**Conv-Diff-Reac** & 0.2210 & 0.2308 & 0.5983 & 0.0144 & 0.0701 & 0.0329 \\   

Table 4: The relative errors of the solutions of parameterized PDEs with \(,,\). The initial condition of each equation is Gaussian distribution \(N(,(/2)^{2})\).

with high precision in only 10 epochs in Phase 2, while PINN struggles to find accurate solutions over 2,000 epochs (Figure 7). We show more visualizations and detailed results in Appendix T.

## 5 Related Work

Meta-learning of PINNs and INRs:HyperPINNs  share some commonalities with our proposed method in that they generate model parameters of PINNs via hypernetwork . However, hyperPINNs can only generate full-rank weights and do not have capabilities to handle parameterized PDEs. Another relevant work is optimization-based meta-learning algorithms; representatively MAML  and Reptile . In the area of INRs, meta-learning methods via MAML and Reptile for obtaining initial weights for INRs have been studied in . In [25; 26], meta-learning methods, which are based on MAML, have been further extended to obtain sparse representation of INRs.

Low-rank formats in neural networks:In natural language processing, the models being employed (e.g., Bert) typically have hundreds of millions of model parameters and, thus, making the computation efficient during the inference is one of the imminent issues. As a remedy, approximating layers in low-rank via truncated SVD has been studied [14; 15]. Modeling layers in low-rank in general has been studied for MLPs [27; 28; 29; 30] and convolutional neural network architectures . In the context of PINNs or INRs, there is no low-rank format has been investigated. The work that is the closest to our work is SVD-PINNs , which represents the hidden layers in the factored form as in Eq. (1), but always in full-rank.

PINNs and their variants:There have been numerous sequels improving PINNs  in many different aspects. One of the main issues is the multi-term objectives; in , a special gradient-based optimizer that balances multiple objective terms has been proposed, and in , a network architecture that enforce boundary conditions by design has been studied. Another main issue is that training PINNs often fails on certain classes of PDEs (e.g., fail to capture sharp transition in solutions over the spatial/temporal domains). This is due to the spectral bias  and, as a remedy, in , training approaches that gradually increase the level of difficulties in training PINNs have been proposed. In , a Lagrangian-type reformulation of PINNs is proposed for convection-dominated PDEs. There are also other types of improvements including Bayesian-version of PINNs  and PINNs that enforce conservation laws . All these methods, however, require training from scratch when the solution of a new PDE is sought.

## 6 Conclusion

In this paper, we propose a low-rank formatted physics-informed neural networks (PINNs) and a hypernetwork-based meta-learning algorithm to solve parameterized partial differential equations (PDEs). Our two-phase method learns a common set of basis vectors and adaptive rank structure for varying PDE parameters in Phase 1 and approximate the solutions for unseen PDE parameters by updating only the coefficients of the learned basis vectors in Phase 2. From the extensive numerical experiments, we have demonstrated that the proposed method outperforms baselines in terms of accuracy and computational/memory efficiency and does not suffer from the failure modes for various parameterized PDEs.

Figure 7: [2D-Helmholtz equation] Solution snapshots for \(a=2.5\)

Limitations:Our Hyper-LR-PINN primarily concentrates on the cases where the PDE parameters are the coefficients of PDE terms. As a future study, we plan to extend our framework to more general settings, where the PDE parameters define initial/boundary conditions, or the shape of domains. See Appendix A for more details.

Broader impacts:Hyper-LR-PINN can solve many equations even for PINN's failure modes with high precision. However, despite its small errors, one should exercise caution when applying it to real-world critical applications.