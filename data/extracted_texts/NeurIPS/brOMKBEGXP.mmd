# Self-Chatined Image-Language Model for

Video Localization and Question Answering

 Shoubin Yu Jaemin Cho Prateek Yadav Mohit Bansal

UNC Chapel Hill

{shoubin, jmincho, praty, mbansal}@cs.unc.edu

###### Abstract

Recent studies have shown promising results on utilizing large pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose **S**elf-**C**hannel **V**ideo **L**ocalization-**A**nswering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and question answering on videos. SeViLA framework consists of two modules: Localizer and Answerer, where both are parameter-efficiently fine-tuned from BLIP-2. We propose two ways of chaining these modules for cascaded inference and self-refinement. First, in the forward chain, the Localizer finds multiple language-aware keyframes in a video, which the Answerer uses to predict the answer. Second, in the reverse chain, the Answerer generates keyframe pseudo-labels to refine the Localizer, alleviating the need for expensive video moment localization annotations. Our SeViLA framework outperforms several strong baselines/previous works on five challenging video question answering and event prediction benchmarks, and achieves the state-of-the-art in both fine-tuning (NExT-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings. We show a comprehensive analysis of our framework, including the impact of Localizer, comparisons of Localizer with other temporal localization models, pre-training/self-refinement of Localizer, and varying the number of keyframes.1

## 1 Introduction

The recent success of large-scale pre-trained language models  has led to a burst of multimodal vision-and-language models that can jointly understand visual (image/video) and language data . However, due to higher computational and annotation costs, video-language models (video-LMs) are more challenging to scale in terms of model and data size than image-language models (image-LMs). Hence, recent studies have explored efficient training of video-LMs by leveraging pre-trained image-LMs . While such a warm-start strategy facilitates visual representation learning of video-LMs, they typically concatenate uniformly/randomly sampled video frames as visual inputs without explicit language-aware, temporal modeling. However, such a simple uniform/random sampling of frames can lead to losing important visual cues, resulting in the video-LMs focusing on frames that are unimportant/irrelevant to language queries .

To address this, we introduce **S**elf-Chained **V**ideo **L**ocalization-**A**nswering (**SeViLA**), a novel video-language framework where we adopt a single image-LM to handle both temporal localization and question answering on videos, while avoiding expensive language-aware, temporal grounding annotations (plus self-refinement  between the two modules). Our SeViLA framework obtains two modules, a **Localizer** and an **Answerer** through parameter-efficient fine-tuning  of BLIP-2 , a recent state-of-the-art image-LM. SeViLA tackles video-language tasks by chaining the output of Localizer to the input of Answerer (_forward chain_, Fig. 1 left), while the Answerer gives feedback to refine the Localizer (_backward chain_, Fig. 1 right). In the **forward chain**, Localizer leverages the original image-language understanding of the BLIP-2 backbone and chooses the important language-aware video keyframes via the localization prompt **"Does the information within the frame provide the necessary details to accurately answer the given question?"** for each video frame. Then Answerer takes the concatenation of selected keyframes as visual input to predict video-level answers. In the **backward chain**, we generate keyframe pseudo-labels  to refine the Localizer, where we label a video frame as a keyframe if Answerer can output the correct answer using that frame. This self-refinement improves the language-aware temporal localization accuracy and alleviates the need for expensive keyframe annotations.

We demonstrate the effectiveness of SeViLA framework on five challenging video question answering and event prediction benchmarks (NeXt-QA, STAR, How2QA, TVQA, and VLEP) [75; 77; 36; 27; 28], where SeViLA outperforms several strong baselines/previous works, and achieves the state-of-the-art in both fine-tuning (NeXt-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings. We also show that our Localizer can be used as a strong stand-alone moment retrieval model. We present a comprehensive analysis to elaborate the design choices of the proposed framework, including the impact of temporal localization, the impact of the self-refinement process (backward chain), and varying the number of keyframes. Our contributions are summarized as follows:

* A novel video-language framework SeViLA, where the Localizer and Answerer are initialized from a single image-language model to handle temporal localization and question answering on videos, respectively.
* A new self-refinement method for language-aware temporal keyframe localization, where the Answerer generates keyframe pseudo-labels to refine the Localizer, without expensive temporal grounding annotation.
* Strong empirical performance with state-of-the-art on multiple video-language benchmarks.
* Comprehensive analysis elaborating the design choices of the proposed framework.

## 2 Related Work

**Image-Language Pre-trained Models.** As the demand for cross-modal applications continues to grow, image-language pre-training studies have received tremendous attention and success. Image

Figure 1: **Self-Chained Video Localization-Answering (SeViLA) consists of a Localizer and an Answerer. Left: Forward chain for language-aware temporal keyframe localization and question answering. Right: Reverse chain for Localizer self-refinement with keyframe pseudo-labels.**

language pre-trained models [55; 12; 69; 5; 36; 64; 31; 34; 86] have advanced more rapidly than video-language pre-trained models [71; 90; 45; 17; 80; 67; 81; 83; 87; 37; 88], both in terms of model [61; 38; 35; 1; 70; 43] and pre-training data scale [61; 1; 89; 21] (more detailed model size and pre-training data scale comparisons are in Appendix). This can be attributed to the increased accessibility of image data and the comparatively simpler data structures, which makes scaling up image-language learning easier . In our paper, SeViLA is built on the recent state-of-the-art image-LM BLIP-2  and extends it to adopt video input for video-language tasks. We also compare our SeViLA framework with the current state-of-the-art video-LM, InternVideo , to demonstrate the superiority of a large image-LM that incorporates keyframe localization.

**Image-to-Video Transfer Learning.** The gap between image- and video-language models has inspired numerous useful methods focusing on image-to-video transfer learning, which leverage a limited number of video frames to enhance learning efficiency [80; 32; 23; 46; 15; 44; 14; 31; 4; 82; 68]. Luo et al.  adapt pre-trained CLIP  backbone to solve video clip retrieval. Yang et al.  extend frozen bidirectional language models  to incorporate multiple images and apply additional video-level pre-training to facilitate model adaptation . Wang et al.  convert multiple images into hierarchical captions, arranged with a temporal order prompt to help language models comprehend video-level events. However, these works employ a uniform sampling strategy that is not language-aware. This can lead to the loss of key visual cues for temporal modeling and even burden the models with irrelevant information [31; 76]. In this paper, we propose a Localizer to provide language-aware visual information to video-language tasks.

**Language-aware Keyframe Localization.** Many methods [42; 3; 18; 54; 24; 41; 73; 9] have been proposed to address the challenge of language-aware keyframe localization. Buch et al.  optimized an end-to-end pipeline using answer labels to select a single keyframe for downstream tasks. Lu et al.  selects frames with separate image and language models, and answers questions by a QA model with multiple training objectives. Qian et al.  designed a video clip proposal model with predefined ranges, iteratively training it with a QA model. Kim et al.  utilized a semi-parametric retriever to obtain keyframes based on frame and language feature similarity. We adopt a large image-LM as our Localizer and chain it with an Answerer. Our Localizer can help to fine-tune Answerer in the forward chain and be refined with pseudo-labels in the reverse chain.

## 3 Method: SeViLA

In this section, we introduce the method details of our Self-Chained Video Localization-Answering (SeViLA) framework. First, we provide BLIP-2 preliminaries, which serve as the foundation for our framework. Then we elaborate our design of the BLIP-2 Localizer and BLIP-2 Answerer for temporal localization and question answering on videos. Finally, we present the SeViLA framework's training and inference processes in the forward and reverse chain.

### Preliminaries: BLIP-2

We adopt BLIP-2  as the backbone of our SeViLA framework. BLIP-2 is a recent state-of-the-art pre-trained image-language model (image-LM) comprising of: (1) a frozen image encoder [11; 16]; (2) a frozen large language model (LLM) [7; 91]; and (3) a Q-former, which is a trainable transformer  module that bridges the image encoder and LLM, similar to acting as an adapter [62; 20]. It takes as input visual features \(h\) from the image encoder and learnable query embeddings \(q\), and outputs fixed-length visual features \(v\). The BLIP-2 Q-Former undergoes a two-stage pre-training. First, it connects to the image encoder to perform image-to-text pre-training. This stage enables the Q-Former to extract the most informative visual information for the text and remove any irrelevant details in \(v\). Subsequently, the Q-Former is connected to the LLM to leverage its generative language capabilities. This is achieved using a fully-connected layer to project query embeddings into the LLM's dimension with image-to-text pre-training. As a result, these query features serve as soft visual prompts  for the LLM. With the two-stage pre-trained Q-former and LLM, BLIP-2 shows advanced performance on various image-language tasks. In our SeViLA framework, we adopt BLIP-2 as the basic building block for both video temporal localization and question answering modules. We retain the visual encoder and the LLM from BLIP-2 by keeping them frozen during training. In this case, only these two Q-formers are updated during the Localizer and Answerer training (see Sec. 3.3).

### Self-Chained Video Localization-Answering

**Adapting BLIP-2 to Temporal Localization and Question Answering on Videos.** As illustrated in Fig. 2, our SeViLA framework adopts BLIP-2 to tackle both video temporal localization and question-answering. We assign BLIP-2 two roles of being a Localizer and an Answerer by using different Q-formers. We first elaborate our Localizer and Answerer in detail as follows:

**Localizer.** We first extract frame features via the frozen image-encoder ViT , referred to \(E_{v}\). Given the video, we uniformly sample \(n\) frames \(\{f_{1},...,f_{n}\}\). We then obtain \(i_{th}\) frame feature \(h_{i}\) as \(h_{i}=E_{v}(f_{i})\). Finally, we represent the video as a set of frame features \(V=\{h_{1},...,h_{n}\}\). These features are extracted once and then saved to be subsequently reused by the Localizer and the Answerer. The primary objective of the Localizer is to select \(k\) language-aware keyframe features from \(V\), where \(k\) is typically much smaller than \(n\). As illustrated in Fig. 2 (top), we then independently extract visual query features \(v_{i}\) from original frame features in \(V\) via a Q-Former \(Q_{loc}\). Next, visual query features \(v_{i}\) and language contexts \(L\) are concatenated and fed into the LLM (Flan-T5 ), where we create \(L\) by combining question, options, and localization prompt "Does the information within the frame provide the necessary details to accurately answer the given question?". The Localizer outputs the score for each frame \(s_{i}\), which is the probability of generating a word 'yes', given the visual features \(v_{i}\) and language context \(L\): \(s_{i}=LLM(concat(v_{i},L))\). We can localize language-aware keyframes \(K=\{v_{1}^{k},...,v_{K}^{k}\}\), based on the top-k frame scores. Our Localizer can be formulated as:

\[K=(V,L),|K|=k n\] (1)

**Answerer.** With the keyframe set \(K\) obtained from the Localizer, as illustrated in Fig. 2 (bottom), we can proceed to generate answers using the Answerer. We first obtain keyframe query features \(v_{i}\) by processing them through \(Q_{ans}\), following the same procedure used in the Localizer. Next, we feed the LLM with all query features and language contexts by concatenating them and obtain the video-level answer \(a=LLM(concat(v_{1}^{k},...,v_{K}^{k},L))\)2. Then, the frozen LLM can conduct temporal modeling with multiple frame inputs. Our Answerer can be formulated as:

\[a=(K,L)\] (2)

Figure 2: In SeViLA framework, **Localizer** (top) selects top-K video frames, which guides **Answerer** (bottom) to focus on important language-aware video moments and predict answers. Both Localizer and Answerer are initialized from a single pre-trained BLIP-2 model, where only Q-formers and a linear layer (2.5% of total parameters) are tuned for each module. We omit the linear layer after the Q-former for simplicity.

### Training Answerer and Localizer via Self-Chaining

**Fine-tuning Answerer in forward chain.** As illustrated in Fig. 3 (top), we fine-tune the Answerer on downstream tasks using keyframes from Localizer via the forward chain. Answerer takes the keyframes generated by Localizer. We compare the default setting to other settings (, input frames are uniformly selected) in Appendix.

**Refining Localizer in reverse chain.** We adopt pseudo-labeling  in our reverse chain to address the costly frame-level localization annotations. We use binary pseudo-labels, where we label a video frame as a keyframe if Answerer can output the correct answer using that frame. As shown in Fig. 3 (bottom), The frozen Answerer is first prompted by a QA task prompt and generates a frame-level answer, then we obtain pseudo labels by comparing this prediction with the ground-truth answer. The Localizer is trained to locate language-aware pseudo-label keyframes.

**Pre-training Localizer with moment retrieval label.** To enhance our Localizer, we conduct transfer learning from a video moment retrieval/grounding task [56; 30] via pre-training. We use videos, queries, and video-level temporal span labels from QVHighlights , and assign a binary localization label to each frame by comparing its timestamp to the span annotations. We provide more details of pre-training in Appendix.

## 4 Experiments

In this section, we first outline our experimental setup (Sec. 4.1). Then, we demonstrate the superiority of SeViLA framework on 5 challenging long-form video question answering and event prediction benchmarks in both fine-tuning (Sec. 4.2) and zero-shot (Sec. 4.3) settings. we also conduct ablation studies on SeViLA framework to show the effectiveness of each of its components on downstream tasks (Sec. 4.4). Next, We report the performance of our Localizer on video moment retrieval (Sec. 4.5). Lastly, we perform in-depth quantitative, and qualitative analyses on our Localizer to show the effect of our design for temporal keyframe localization (Sec. 4.6 and Appendix). More results on single v.s. multi-frame Localizer, pre-training strategies, iterative self-refinement, computational cost, and extension to another Image-LM model are in Appendix.

### Experimental setup

**Benchmarks.** We evaluate our SeViLA framework on 3 video-language tasks, including multi-choice Video Question Answering (**NExT-QA**, **STAR**, **How2QA**, **TVQA**), Video Event Prediction (**VLEP**), and Moment Retrieval (**QVHighlights**). See Appendix for details.

Figure 3: **Top: In the forward chain, the Localizer finds multiple language-aware keyframes, then the Answerer utilizes these keyframes to predict answers. We use the forward chain for both inference and Answerer fine-tuning. Bottom: In the reverse chain, we generate keyframe pseudo-labels by using the Answerer to refine the Localizer.**

**Baselines.** We compare our SeViLA framework with the state-of-the-art video-language pre-trained model, InternVideo  as well as our backbone BLIP-2 . We extend BLIP-2 to adapt videos in two settings: (1) BLIP-2(), which processes each uniformly sampled frame independently and obtains the final answer by majority voting on all frame-level answers, and (2) BLIP-2(), where Q-former processes each frame and Flan-T5 takes the concatenation of visual features as a prefix. BLIP-2() is equivalent to our Answerer with uniformly sampled frames. See Appendix for details.

**Implementation Details.** SeViLA framework adopts BLIP-2 , an image-language model with 4.1B parameters and pre-trained on 129M images in total, including COCO , Visual Genome , CC12M , SBU , and 115M images from LAION400M . See Appendix for details.

### Fine-tuning Comparison to SOTA on Video QA and Event Prediction

We compare our SeViLA framework to recent state-of-the-art models on 4 video QA benchmarks and 1 video event prediction dataset. We show results in Table 1, and summarize our findings as follows.

**(a) Temporal modeling matters.** BLIP-2() underperforms our BLIP-2() and other video-LM models on STAR, How2QA, TVQA, and VLEP. Especially on STAR-Sequence, a task demanding heavy temporal understanding, our BLIP-2() outperforms BLIP-2() significantly by 13.1% (69.0% vs. 54.8%). As BLIP-2() processes frames independently and lacks temporal modeling among frames, the result indicates that temporal modeling is essential to tackle video-language tasks and the effectiveness of our temporal modeling design.

**(b) Keyframe selection helps.** Our SeViLA framework, featuring a zero-shot Localizer, leads on all tasks with an average advantage of 5.3% over the top video-LM (InternVideo). It also surpasses BLIP-2() that uses uniform frame sampling on NeXT-QA (+1.2%), STAR (+0.7%), How2QA (+1.5%), and VLEP (+0.4%). This highlights the importance of keyframe selection in video-language tasks, even when using a zero-shot Localizer.

    &  &  &  \\   &  &  \\    \\ HERO (dense/1fps)  & - & - & - & - & - & - & - & - & 73.8 & 73.6 & - \\ JustAsk (20)  & 51.4 & 49.6 & 63.1 & 52.3 & - & - & - & - & 84.4 & - & - \\ FrozenBiLM (10)  & - & - & - & - & - & - & - & - & 86.7 & 82.0 & - \\ VidIL 4-shot (12)  & - & - & - & - & - & - & - & - & - & - & 72.0 \\ T+T (dense/1fps)  & - & - & - & - & - & - & - & - & - & 92.4 & - & - \\ T+T(+ASR, dense/1fps)  & - & - & - & - & - & - & - & - & - & 93.2 & - & - \\    & - & - & - & - & - & - & - & - & 42.2 & - & - & - \\ FrozenBiLM (10)  & - & - & - & - & - & - & - & - & 81.5 & 57.5 & - \\ All-in-One (32)  & 48.6 & 48.0 & 63.2 & 50.6 & 47.5 & 50.8 & 47.4 & 44.0 & 47.5 & - & - & - \\ Temp1ATP1 (32)  & 49.3 & 48.6 & 65.0 & 51.5 & 50.6 & 52.8 & 49.3 & 40.6 & 48.3 & - & - & - \\ VGT (32)  & 55.0 & 52.2 & 64.0 & 55.0 & - & - & - & - & 44.2 & - & - & - \\ MIST (32)  & 56.6 & 54.6 & 66.9 & 57.1 & 55.5 & 54.2 & 54.2 & 44.4 & 51.1 & - & - & - \\ VFC (32)  & 53.3 & 57.6 & 72.8 & 58.6 & - & - & - & - & - & - & - \\ CoVGT (32)  & 57.4 & 58.8 & 69.3 & 60.0 & - & - & - & 45.9 & - & - & - \\ SeViT\({}_{}\) (10)  & - & - & - & - & 60.6 & - & - & - & - & - & - & - \\ HiFCA (16)  & 58.3 & 62.4 & 75.6 & 63.1 & - & - & - & - & - & - & - \\ InternVideo\({}^{}\) (8)  & 58.5 & 62.5 & 75.8 & 63.2 & 62.7 & 65.6 & 54.9 & 51.9 & 58.7 & 79.0 & 57.2 & 63.9 \\ BLIP-2()() & 65.2 & 70.1 & 80.1 & 70.1 & 52.3 & 54.8 & 49.0 & 51.2 & 51.8 & 79.6 & 54.5 & 67.0 \\ BLIP-2() () () & 68.1 & 72.9 & 81.2 & 72.6 & **65.4** & 69.0 & 59.7 & 54.2 & 62.0 & 82.2 & 59.8 & 68.6 \\  SeViLA\({}^{}\) (32 \(\) 4) & 68.8 & 73.4 & **83.5** & 73.4 & 63.2 & 66.6 & 61.3 & 60.0 & 62.7 & **83.7** & 59.7 & **69.0** \\ SeViLA (32 \(\) 4) & **69.4** & **74.2** & 81.3 & **73.8** & 63.7 & **70.4** & **63.1** & **62.4** & **64.9** & 83.6 & **61.6** & 68.9 \\   

Table 1: Fine-tuning results on video question answering (NExT-QA, STAR, How2QA, TVQA) and video event prediction (VLEP). We gray out the methods take extra speech input or use dense frames. We **bold** the best numbers, and underlined the second-best numbers. dense/1fps: the model takes dense (1fps) video frames instead of a fixed number of frames. \(32 4\): our Localizer selects 4 keyframes from 32 frames. \({}^{*}\) represents the results tested by ourselves. SeViLA\({}^{}\) uses the zero-shot Localizer without refining on pseudo-labels via the reverse chain.

**(c) Self-refinement improves temporal localization.** For SeViLA, we refine the Localizer with pseudo-labels (see Sec. 3.3). Compared with SeViLA\({}^{}\), SeViLA further increases performance on NExT-QA (0.4%), STAR (+2.2%), and TVQA (+1.9%). SeViLA framework achieves new **state-of-the-art** fine-tuning performance on NExT-QA, STAR, TVQA, and VLEP, using only visual and language modalities. This illustrates the significance of temporal localization and the efficacy of our self-refinement method for keyframe localization.

### Zero-shot Comparison to SOTA on Video QA and Event Prediction

We further compare our SeViLA framework to recent state-of-the-art models in the zero-shot setting. We show the zero-shot results in Table 2, then discuss the findings in the following.

**(a) Image-LM outperforms Video-LM, without video pretraining.** Surprisingly, BLIP-2\({}^{}\), without inter-frame temporal modeling, outperforms the previous state-of-the-art video-LM, InternVideo on several datasets that require temporal reasoning. BLIP-2\({}^{}\) outperforms InternVideo on NExT-QA (+13.6%), How2QA (+7.6%), and VLEP (+5.1%). On How2QA, BLIP-2\({}^{}\) even surpasses FrozenBiLM which performs extra speech and video pre-training by 11.4%. It highlights the potential of image-LM for video-language tasks due to its model size and sufficient pre-training.

**(b) Keyframe selection is more effective than uniform sampling.** Our SeViLA\({}^{}\) framework, combining the zero-shot Localizer and the zero-shot Answerer, outperforms the BLIP-2\({}^{}\) (Answerer) with uniformly sampled frames on NExT-QA (+1.2%), STAR (+2.4%), How2QA (+1.5%), TVQA (+1.6%), and VLEP (+0.4%), achieving new **state-of-the-art** zero-shot performance on NExT-QA, STAR, How2QA, and VLEP, and new state-of-the-art on TVQA with only visual and language modalities. On STAR, our SeViLA\({}^{}\) framework even outperforms zero-shot Flamingo  with 80B parameters by 4.9%. The result demonstrates the effectiveness of our SeViLA framework to adapt video-language tasks and the importance of language-aware keyframe selection.

### Ablation Studies on SeViLA Framework

We conduct ablation studies on our SeViLA framework about the effectiveness of Localizer and Answerer. We show the results in Table 3. We summarize our findings as follows:

**(a) Sparse frames outperform dense frames:** We observe performance declines when the Answerer adopts more frames (A _v.s._ B), confirming that sparse frames work better due to the original Image-LM model's limited temporal modeling ability, while too dense frames may distract the model.

**(b) Keyframes outperform uniformly sampled frames:** We compare Answerer with Localizer (SeViLA framework) and Answerer that takes uniformly sampled frames. We observed significant performance gains in the zero-shot Answerer setting when utilizing the zero-shot Localizer (B _v.s._ C), with improvements on NExT-QA (+1.0%), STAR (+2.4%), How2QA (+1.5%), TVQA (+1.6%), and VLEP (+0.4%). And pseudo-label refinement for Localizer further increased performance by an average of 2.1% across all tasks (B _v.s._ D). In the fine-tuned Answerer setting, the benefits of the Localizer remained evident. Our SeViLA framework, which utilized keyframes from the

    &  &  &  \\   &  & &  \\   (w/speech input or use dense frames) & & & & & & & & & & \\ JustAsk (20)  & - & - & - & - & - & - & - & - & 51.1 & - & - \\ FrozenBiLM (10)  & - & - & - & - & - & - & - & - & - & 58.4 & 59.2 & - \\ VigerGPT (dense/1fps)  & - & - & - & 60.0 & - & - & - & - & - & - & - & - \\  Flamingo-80B (30)  & - & - & - & - & - & - & - & - & 39.7 & - & - & - \\ FrozenBiLM (10)  & - & - & - & - & - & - & - & - & - & 41.9 & 29.7 & - \\ VFC (32)  & 45.4 & 51.6 & 64.1 & 51.5 & - & - & - & - & - & - & - & - \\ InterrVideo\({}^{*}\) (8)  & 43.4 & 48.0 & 65.1 & 49.1 & 43.8 & 43.2 & 42.3 & 37.4 & 41.6 & 62.2 & 35.9 & 58.7 \\ BLIP-2\({}^{}\) (4) & 59.1 & 61.3 & 74.9 & 62.7 & 41.8 & 39.7 & 40.2 & 39.5 & 40.3 & 69.8 & 35.7 & 63.8 \\ BLIP-2\({}^{}\) (Answerer) (4) & 59.7 & 60.8 & 73.8 & 62.4 & 45.5 & 41.8 & 41.8 & 40.0 & 42.2 & 70.8 & 36.6 & 64.0 \\  SeViLA\({}^{}\) (32\(\)4) & **61.3** & **61.5** & **75.6** & **63.6** & **48.3** & **45.0** & **44.4** & **40.8** & **44.6** & **72.3** & **38.2** & **64.4** \\   

Table 2: Zero-shot results on video question answering and video event prediction.

Localizer, outperformed the 4-frame Answerer by an average of 0.7% across tasks (E _v.s._ F). Pseudo-label refinement continues to be effective in this setting, providing an average boost of 1.5% across all tasks (E _v.s._ G). These results demonstrate that keyframe selection contributes to non-trivial improvements in video-language tasks for zero-shot and fine-tuning settings.

### Comparison to State-of-the-Art on Video Moment Retrieval

In this section, we evaluate our Localizer on the video moment retrieval task. We pre-train the Localizer on the QVH highlights  dataset, as discussed in Sec. 3.3, and then assess its performance on the same dataset. To test on QVH highlights, we first extract video frames at 0.5 fps, following Moment-DETR  and pass them through our Localizer to obtain binary frame-level predictions that indicate whether a frame matches the query sentence. Next, we combine these predictions into video-level temporal span predictions. We aggregate frame-level predictions into video-level spans by merging adjacent positive predictions with intervals not exceeding a threshold. These merged results are then consolidated into a single video-level span. More information on the aggregation process can be found in Appendix. Interestingly, as shown in Table 4, our Localizer, which has no temporal modeling/training and operates on frame-level, outperforms many previous methods [13; 30; 29] with complex temporal modeling and video data training. It demonstrates our Localizer can further work as a standalone model for certain tasks. It also suggests large image-LM with temporal designs may be a promising study for video moment retrieval. This is evidenced by our Localizer's superior performance compared to Moment-DETR, despite the absence of temporal modeling.

### Detailed Analysis on the Localizer

In this section, we first analyze the impact of pre-training and self-refinement on our Localizer. Then, we compare our Localizer with other keyframe selection methods in both zero-shot and fine-tuning settings. Next, we experiment with different keyframe selection ranges and quantities in our Localizer to assess the impact of temporal localization on the overall performance. We further show the impact of Localizer in Answerer fine-tuning. We also present upper-bound analysis based on BLIP-2 and oracle keyframe localization. Lastly, we provide visualization results of our Localizer. Additional experiments are in Appendix.

**Ablation on Localizer pre-training and self-refinement.** We performed these ablation studies with the zero-shot 4-frame Answerer. As shown in Table 5, the untrained BLIP-2 Localizerresults in only a minor improvement to the Answerer. Moreover, both QVHighlights pre-training and self-refinement via the reverse chain independently provide significant performance boosts. The optimal results are achieved when both pre-training and self-refinement are applied. It further demonstrates our method is label-efficient for keyframe temporal localization.

**Comparison with other keyframe selection methods.** In Table 6, we compare our Localizer with different keyframe localization methods, including CLIP , Moment-DETR  which are zero-shot, and ATP , Differentiable Top-K  which are fine-tuned with answer-label. We combine those keyframe localization methods with our zero-shot Answerer. Those methods select 4 keyframes from 32 uniformly sampled frames. We find that keyframes from neither CLIP nor Moment-DETR can not help Answerer. It may be due to their CLIP pre-training on images and short declarative sentences, which fail to produce question-aware visual features, and distract Answerer with irrelevant features. In contrast, our zero-shot Localizer\({}^{}\) improves on NExT-QA by averaging 1.2%. Furthermore, our Localizer refined on pseudo-labels outperforms fine-tuned ATP and Differentiable top-K, with a 2.2% average improvement across all question types. Overall, our Localizer shows superior effectiveness in both settings.

**Impact of keyframe selection ranges and quantities.** In Table 7, we evaluate temporal keyframe localization in a zero-shot setting using various keyframe selection ranges and quantities. Even with one keyframe selected, our Localizer shows significant improvements over the BLIP-2\({}^{}\) based on majority vote on 8 frame-level answers: NExT-QA-Causal (+2.4%), NExT-QA-Description (+3.6%), and How2QA (+2.6%). This highlights our Localizer's effectiveness in localizing selective keyframes. We also note that multiple keyframes benefit NExT-QA-Temporal questions, but denser frames result in worse performance. It supports our finding in Sec. 4.4, that using too dense frames may distract image-LM.

**Impact of different frame sampling during Answerer fine-tuning.** In Sec. 3.3, we discussed how the Answerer in the SeViLA framework can be further fine-tuned in the forward chain using keyframes from the Localizer. Table 8 compares fine-tuning in different frame sampling strategies, and indicates SeViLA framework performs optimally when utilizing Localizer in both Answerer training and evaluation. This is likely due to the provision of more informative keyframes and milder domain shifts between the training and evaluation.

**Upper-bound performance analysis on oracle keyframes.** We further explore the upper-bound performance with the assumption of a 'perfect' localizer, capable of always providing the right keyframes for the Answerer. To achieve this, we uniformly sample four frames from each video, input them into BLIP-2 individually, and generate four frame-level answers. The upper-bound performance is represented by the oracle accuracy, which considers a question correctly answered if at least one of the four frames yields the right answer. As shown in Table 9, significant gaps exist between BLIP-2 majority voting and oracle accuracy in both settings. These gaps emphasize the need for more future work in temporal localization to effectively harness image-LM for video-language tasks.

**Qualitative analysis on Localizer.** In Fig. 4, we present an example from NExT-QA (more in Appendix), showcasing the QA pairs, our Localizer results, and the ground truth task-related video clips that we manually annotated. Our Localizer more accurately matches human annotations than

    &  \\   &  \\  Answerer & 59.7 & 60.8 & 73.7 & 62.4 \\  (zero-shot) & & & & \\ + CLIP  & 59.2 & 60.0 & 72.5 & 61.8 \\ + Moment-DETR  & 59.5 & 60.6 & 72.1 & 62.0 \\ + Localizer\({}^{}\) & **61.3** & **61.5** & **75.6** & **63.6** \\  (fine-tuning) & & & & \\ + ATP  & 60.4 & 61.3 & 73.4 & 62.8 \\ + Differentiable Top-K  & 59.5 & 59.7 & 72.7 & 61.6 \\ + Localizer & **62.3** & **63.1** & **74.9** & **64.6** \\   

Table 6: Comparison of our Localizer with other keyframe localization methods.

    &  &  \\    & **Tem.****Cau.****Des.****Avg.** & \\  BLIP-2\({}^{}\) (8) & 59.9 & 60.2 & 72.4 & 62.0 & 69.8 \\ 
8\(\)1 & 59.8 & 61.1 & **76.0** & 62.9 & 72.4 \\
16\(\)1 & 59.2 & **62.6** & 74.9 & 63.4 & **73.2** \\
16\(\)4 & 60.7 & 61.5 & 75.8 & 63.4 & 72.4 \\
32\(\)4 & **61.3** & 61.5 & 75.6 & **63.6** & 72.3 \\
32\(\)8 & 59.4 & 60.9 & 74.7 & 62.5 & 71.3 \\
64\(\)8 & 58.9 & 60.9 & 74.0 & 62.2 & 71.8 \\   

Table 7: Ablation of different numbers of input frames and output keyframes.

uniform selection. This accurate localization enables the Answerer to correctly answer questions, while uniform selection leads to incorrect responses. These results demonstrate that our Localizer can effectively locate task-related keyframes in a video, thus benefiting downstream tasks.

## 5 Conclusion and Future Work

In this paper, we introduced SeVLA, a novel video-language framework. SeVLA adapts an image-language model to obtain two modules: (1) Localizer for language-aware temporal localization and (2) Answerer for question answering on keyframes. SeVLA tackles video-language tasks by chaining the output of Localizer to the input of Answerer (_forward chain_), while Answerer can give feedback to refine the Localizer (_backward chain_) via pseudo labeling. The proposed temporal localization allows a more focused understanding of videos and improves the accuracy of video-language tasks, while the pseudo-labeling process alleviates the need for expensive language-aware keyframe annotations. Compared with state-of-the-art baselines, SeVLA achieves competitive or better performance on five video questions answering and event prediction benchmarks. We also provide a comprehensive analysis elaborating on the design of the proposed two-stage self-chaining. Our research encourages future work to improve temporal localization in video understanding.

**Limitations & Broader Impacts.** See Appendix for limitations and broader impacts discussion.

Figure 4: Visualization of our Localizer. We use zero-shot Answerer with different frame sampling (uniform _v.s._ Localizer) to answer the question. Red options are answered wrongly with uniformly sampled frames. Green options are answered correctly with our Localizer. Best viewed in color.

    &  \\ 
**Training** & **Inference** & **Temp.** & **Cau.** & **Des.** & **Avg.** \\  Random & Uniform & 68.1 & 72.9 & 81.2 & 72.6 \\ Random & Localizer\({}^{}\) & 67.6 & **73.4** & **84.0** & 73.1 \\ Localizer\({}^{}\) & Uniform & 68.2 & 72.7 & 80.0 & 72.3 \\ Localizer\({}^{}\) & Localizer\({}^{}\) & **68.8** & **73.4** & 83.5 & **73.4** \\   

Table 8: Comparing different frame sampling during Answerer fine-tuning. The Localizer\({}^{}\) is frozen during fine-tuning. We use 4 frames for Answerer training, while the Localizer\({}^{}\) is the default 32\(\)4.

    & ^{}\) (Oracle)**} \\   & **Zero-Shot** & **Fine-tuned** \\  NEXT-QA (Avg.) & 62.7 (70.1) & 70.1 (79.7) \\ STAR (Avg.) & 40.3 (52.9) & 51.8 (72.2) \\ How2QA & 69.8 (77.8) & 79.6 (86.4) \\ TVQA & 35.7 (45.4) & 54.5 (69.0) \\ VLEP & 63.8 (70.5) & 67.0 (79.1) \\   

Table 9: BLIP-2\({}^{}\) and oracle (in brackets) performance analysis across datasets. We use 4 frames for each video question. **Oracle**: at least 1 of 4 frames can give the right answer.