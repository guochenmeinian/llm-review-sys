# RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting

RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting

Hsiang Hsu\({}^{1}\), Ivan Brugere\({}^{2}\), Shubham Sharma\({}^{2}\), Freddy Lecue\({}^{2}\), and Chun-Fu Chen\({}^{1}\)

\({}^{1}\)JPMorganChase Global Technology Applied Research

\({}^{2}\)JPMorganChase AI Research

{hsiang.hsu, ivan.brugere, shubham.x2.sharma}@jpmchase.com

{freddy.lecue, richard.cf.chen}@jpmchase.com

###### Abstract

The Rashomon effect is a mixed blessing in responsible machine learning. It enhances the prospects of finding models that perform well in accuracy while adhering to ethical standards, such as fairness or interpretability. Conversely, it poses a risk to the credibility of machine decisions through predictive multiplicity. While recent studies have explored the Rashomon effect across various machine learning algorithms, its impact on gradient boosting--an algorithm widely applied to tabular datasets--remains unclear. This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms. We provide rigorous theoretical derivations to examine the Rashomon effect in the context of gradient boosting and offer an information-theoretic characterization of the Rashomon set. Additionally, we introduce a novel inference technique called RashomonGB to efficiently inspect the Rashomon effect in practice. On more than 20 datasets, our empirical results show that RashomonGB outperforms existing baselines in terms of improving the estimation of predictive multiplicity metrics and model selection with group fairness constraints. Lastly, we propose a framework to mitigate predictive multiplicity in gradient boosting and empirically demonstrate its effectiveness.

## 1 Introduction

Large-scale, complex data and the pursuit of superior performance in machine learning (ML) models have led to increased complexity in both the models themselves and the training algorithms . As a result, it is more likely to find a plethora of distinct models, such as those found in local minima, that exhibit statistically indistinguishable performance (e.g., test accuracy) . This phenomenon, known as the _Rashomon effect_, has urged researchers to reconsider its impact on ML models when deployed in real-world scenarios [22; 10; 30; 26].

The impacts of the Rashomon effect reveals two sides of the same coin in responsible ML. On one hand, it benefits the current trend of developing algorithms that prioritize responsible ML principles beyond merely optimizing for accuracy. These principles often include interpretability , causality , group fairness , counterfactual explanations , and feature interactions . The abundance of models with competing performance allows compliance with these principles without significant compromises in performance. For instance, algorithmic fairness often faces a trade-off with accuracy; the Rashomon effect allows a fairness intervention algorithm to identify a more optimal balance among models with statistically similar performance . On the other hand, the Rashomon effect presents a risk to the credibility of machine decisions known as _predictive multiplicity_, where competing models, generated by simply varying randomness1 in the training processes, yield conflicting predictions for some individual samples. If left unaddressed, conflicting predictions can lead to discrimination and unfairness, hidden under the guise of algorithmic randomness. This can adversely affect certain individuals without revealing significant statistical differences from non-discriminatory models . The negative societal impacts of predictive multiplicity and inconsistent decisions have been recently studied under various frameworks2 such as prediction uncertainty [35; 2], predictive churn [59; 75], and predictive multiplicity [56; 67; 9; 42].

To understand the Rashomon effect and address predictive multiplicity, recent studies have focused on characterizing competing models and efficiently searching for them across different ML models. For instance, predictive multiplicity caused by linear classifiers can be computed via mixed integer programming [56; 74]. Competing models from sparse decision trees are exactly characterized and searched by sub-tree pruning . In special cases such as ridge regression and generalized additive models, the forms of the set of competing models are analytically derived [67; 16]. More recently, test-time dropout has been utilized to search for competing models for neural networks .

In this paper, we focus on another ML algorithm, _gradient boosting_, which is widely applied to tabular datasets . Gradient boosting differs fundamentally from other ML algorithms in its sequential approach: rather than training a model as a single entity, gradient boosting breaks down the training process into a sequence of sub-learning problems. This sequential training pipeline not only facilitates the analysis of the Rashomon effect but also offers new methodologies for model selection and reducing predictive multiplicity. To the best of our knowledge, our work is the first to explore the Rashomon effect for gradient boosting. The main contributions of this work include:

1. We formalize and investigate the Rashomon effect induced by gradient boosting, employing statistical learning and information theory to analyze its behavior (Section 3). Specifically, we leverage information-theoretic measures to characterize the impact of dataset properties on the Rashomon effect.
2. We introduce \(\), an efficient method that explores an exponential search space versus baseline methods which search linearly. (Section 3.1).
3. We implement \(\) on several real-world (large-scale) tabular and image datasets, and empirically demonstrate the competing models obtained with \(\) can greatly improve the estimation of predictive multiplicity (Section 4.1), and model selection with additional responsible ML principles such as group fairness (Section 4.2).
4. We propose two methods to mitigate predictive multiplicity for gradient boosting and experimentally validate the methods on \(18\) tabular datasets (Section 4.3).

Omitted proofs, additional explanations and discussions, details on experiment setups and training, and additional experiments are included in the Appendix. Code to reproduce our experiments can be accessed at https://github.com/jpmorganchase/Rashomon-gradient-boosting.

## 2 Background and related work

Consider a dataset \(=\{_{i}\}_{i=1}^{n}\) drawn i.i.d. from \(P_{S}\), where each \(_{i}\) is a pair \((_{i},y_{i})\) consisting of a feature vector \(_{i}=[_{i1},,_{id}]^{} ^{d}\) and a target \(y_{i}\). Let \(X\) and \(Y\) be the random variables for the feature \(_{i}\) and target \(y_{i}\) respectively, and \(S=X Y\). We denote by \(\) a hypothesis space of functions that map from \(\) to \(\). The loss function used to evaluate model performance is denoted by \(:^{+}\) and \(L_{P_{S}}(h)_{P_{S}}[(h,S)]\) the population risk. As usual, the population risk is approximated by the empirical risk \(L_{}(h)_{i=1}^{n}(h,_{i})\). We denote the empirical risk minimizer as \(h^{*}=*{argmin}_{h}L_{}(h)\). We denote \(_{x}v(x)\) the gradient of \(v(x)\) w.r.t. \(x\), and \([]\) the indicator function. For two random variables \(X\) and \(Y\), the mutual information between them is defined as \(I(X;Y)=D_{}(P_{X,Y}\|P_{X}P_{Y})\), where \(D_{}(P\|Q)=_{P}[ P/Q]\) is the Kullback-Leibler (KL) divergence . Finally, we let \([k][1,,k]\).

The Rashomon sets and exploring models therein.The studies on the Rashomon effect typically start with searching for models in the _Rashomon set_, the set of all models in the hypothesis space \(\) whose population risks are comparable to that of a given model3\(h^{*}\), i.e.,

\[(,,h^{*},)\{h; L_{P_{S}}(h) L_{P_{S}}(h^{*})+\},\] (1)

where \( 0\) is a Rashomon parameter that determines the size of the Rashomon set. However, when the hypothesis space \(\) is large (e.g., neural network architectures, tree ensembles, etc.), exhaustively identifying all models within the Rashomon set becomes computationally infeasible. Therefore, it is customary to approximate the full Rashomon set by a subset with \(m\) models called an _empirical Rashomon set_, \(^{m}(,,h^{*},)\{h_{1}, ,h_{m};h_{i}(,,h^{*}, ),\  i[m]\}\). In practice, the \(m\) models in the empirical Rashomon set are mainly obtained by re-training (See Appendix B.2 for more discussions). The re-training strategy re-trains models with different random initializations and rejects those that disobey the loss deviation constraint in Eq. (1) until \(m\) models are collected [67; 48]. However, re-training models repeatedly is time-consuming with large datasets or complex architectures. To improve efficiency, a recent strategy involves collecting distinct models with competing performance during the inference phase. For example, Hsu et al.  proposed an _inference-time dropout_ strategy for convolutional neural networks (CNNs). Despite these advances, exploring Rashomon sets for ensemble learning methods like boosting remains unexplored.

Predictive multiplicity.Predictive multiplicity undermines the credibility of decisions made by ML algorithms, making its measurement an active research area. Predictive multiplicity metrics can be categorized based on whether they are defined on output decisions (i.e., thresholded predictions/scores after \(*{argmax}\)) or on output scores in the probability simplex. For example, _ambiguity_ and _discrepancy_, measure the proportion of samples with conflicting decisions from models within the Rashomon set . _Disagreement_, in a similar flavor, assess the probability of conflicting decisions per sample . In contrast, score-based metrics estimate various aspects such as score variance/std. [52; 17; 9], the viable range of scores (termed _viable prediction range (VPR)_ by Watson-Daniels et al. )), or score spread in the probability simplex (referred to as _Rashomon Capacity (RC)_ by Hsu and Calmon ). Due to space constraints, we defer the mathematical formulations of these predictive multiplicity metrics and discussions on estimating these metrics with the empirical Rashomon sets to Appendix B.2. These predictive multiplicity metrics are often estimated using the empirical Rashomon set \(^{m}(,,h^{*},)\). As \(m\) increases, the empirical Rashomon set better approximates the Rashomon set, leading to more precise estimations of predictive multiplicity metrics.

Mitigating predictive multiplicity.Mitigating predictive multiplicity ensures that decisions made by ML algorithms are consistent. The main strategy for this is to combine decisions from competing models. Roth et al.  reconcile conflicting decisions from two different models in the Rashomon set to improve the disagreement in predictions. Combining decisions from multiple models falls under the umbrella of model averaging in ensemble learning [9; 42; 52]. Model averaging is a special ensemble learning that collects multiple base models, often referred to as weak learners, and combines them _in parallel_. As averaging model outputs reduces the variance, it is a natural choice for diminishing predictive multiplicity and has been reported in several studies. For instance, Black et al.  proposed a selective averaging that leverages certifiably-robust predictions to mitigate the problem of inconsistency with a probabilistic guarantee. Hsu and Calmon [42, Section A.4.5] observe that random forest classifiers exhibit a lower Rashomon Capacity compared to decision tree classifiers. Furthermore, Long et al.  demonstrate that the probability of significant deviated predictions in model averaging diminishes exponentially with the number of models in the average.

Ensemble learning is not limited to parallel combinations. Another popular branch involves combining models _sequentially_, known as boosting algorithms. Despite the widespread use of boosting algorithms and their superior performance over neural networks on tabular datasets , boosting algorithms have been mostly overlooked in the literature on the Rashomon effect and predictive multiplicity. This paper aims to address this gap, as outlined in the next section. The works most closely related to ours involve prediction uncertainty estimation in gradient boosting, such as NGBoost [27; 55], PGBM , and IBUG , which consider probabilistic predictions from regression trees in Bayesian settings. However, these studies do not frame the concept of arbitrariness in predictions within the context of the Rashomon effect and may overestimate arbitrariness with models that do not have similar performance. To the best of our knowledge, this work is the first that investigates the impact of the Rashomon effect and predictive multiplicity on gradient boosting.

Analyzing the Rashomon effect in gradient boosting

We begin with a brief introduction to gradient boosting and discuss how its sequential combination approach can inspire a new method for finding competing models in the Rashomon set. We then provide a high-probability bound on the Rashomon set using information theory, making the first connection between the size of the Rashomon set and data quality as measured by mutual information. The proofs of the propositions in this section are included in Appendix A.

Boosting algorithms select a sequence of weak learners4, \(h_{0},,h_{T}\), such that the additive expansion \(f_{T}()=_{t=0}^{T} h_{t}()\) with \(>0\) minimizes the empirical risk \(L_{}(f_{T})\)[34; 57; 66; 19]. Different choices of \(\) and the method for selecting \(h_{t}\) have led to various boosting algorithms (see Appendix B.1). Here, our focus lies on gradient boosting [66; 34], which starts with a constant model \(h_{0}()=*{argmin}_{h_{0}}_{i=1}^{n} (h_{0},_{i})\), and iteratively extends the model \(f_{t}(_{i})\) with

\[f_{t}()=f_{t-1}()+*{argmin}_{h_{t} }\|- L_{}(f_{t-1})-h_{t}\|_{2}^{2}=f_{t-1}()+*{argmin}_{h_{t}}_{i=1}^{n}\|h_{t}(_{i} )-r_{ti}\|_{2}^{2},\] (2)

where \(r_{ti}=-[,_{i})}{ f_{t-1}}]\) is the pseudo-residual of sample \(i\) from weaker learner \(h_{t}\). Indeed, for a regression problem with a mean squared error (MSE) loss function, \(h_{0}^{*}=_{i=1}^{n}y_{i}/n\) and \(r_{ti}=2(y_{i}-f_{t-1}(_{i}))\)--the part of the \(y_{i}\) that cannot be explained by the current model \(f_{t-1}(_{i})\). For a binary classification problem with a cross-entropy (CE) loss, \(h_{0}^{*}=_{i=1}^{n}[y_{i}=1]_{i=1}^{n}[y_{i}=0]\), and can be understood as regression on the log-likelihoods. We set \(=1\) in this section for the sake of theoretical analysis. For the convergence and consistency analysis of gradient boosting, see, e.g., Zhang and Yu  and Telgarsky .

### Building Rashomon sets for gradient boosting

The iterative training procedure of gradient boosting in Eq. (2) allows us to convert the original learning problem \(_{f}L_{}(f)\) into a sequence of learning problems \(_{h_{t}}L_{_{t}}(h_{t})\), for \(t[1,2,,T]\), where \(_{t}=\{(_{i},r_{ti})\}_{i=1}^{n}\). In other words, the weak learner \(h_{t}\) aims to fit the pseudo-residuals in each iteration, and inducing a _residual_ Rashomon set (cf. Figure 1),

\[_{t}(,_{t},h_{t}^{*},)\{h_ {t};L_{P_{S_{t}}}(h_{t}) L_{P_{S_{t}}}(h_{t}^{*})+\},\] (3)

where \(h_{t}^{*}\) is any given model such as the empirical risk minimizer in each iteration. Eq. (3) suggests an alternative to building the entire Rashomon set \((,,f_{T}^{*},T)\) by iteratively building the residual Rashomon sets for each iteration, i.e.

\[_{1}(,_{1},h_{1}^{*},) _{t}(,_{t},h_{t}^{*},) _{T}(,_{T},h_{T}^{*},) (,,f_{T}^{*},T).\] (4)

Eq. (4) comes from the fact that, for classification tasks, gradient boosting actually performs a regression on the log-likelihood using the MSE loss. The pseudo-residual of the MSE loss exhibits a linear relationship between the prediction and the output of each iteration, allows us to aggregate the losses across iterations. Equation (4) will be clarified further in Proposition 2.

In practice, if we perform \(m\) re-training in each iteration, and obtain the empirical residual Rashomon set \(_{m}^{m}(,_{t},h_{t}^{*},)\), the model \(f_{T}()\) can be expressed as \(f_{T}()=h_{0}()+_{t=1}^{T}h_{t}()\), \( h_{t}_{t}^{m}(,_{t},h_{t}^{*},)\). Since in each of the \(T\) iterations, there are \(m\) candidate models in \(_{t}^{m}(,_{t},h_{t}^{*},)\), there are a total of \(m^{T}\) possibilities for \(f_{T}()(,,f_{T}^{*},T)\) (see Appendix B.3 for visualization). We term the process of building the empirical residual Rashomon sets for the entire Rashomon set \((,,f_{T}^{*},T)\) as _Rashomon gradient boosting_ or RashomonGB in short. RashomonGB can be straightforwardly implemented by training \(m\) models (i.e., weak learners) at each iteration that meet the loss constraints defined by the Rashomon set. This is fundamentally different from simply performing \(m\) re-training of gradient boosting to solve the original learning problem \(_{f}L_{}(f)\), since the \(m\) models at each iteration obtained by RashomonGB share the same residual from the previous iteration. During the inference phase RashomonGB carries a significant benefit over simply performing \(m\) re-training of gradient boosting--both re-training of gradient boosting and RashomonGB requires \(m T\) training when considering all the iterations.

However, RashomonGB, using a different way to output the predictions at the inference phase, yields exponentially many more models with the same training cost. So far, we have informally introduced RashomonGB. In the rest of this section, we provide a rigorous analysis to show the effectiveness of RashomonGB by characterizing the Rashomon sets of gradient boosting (or more generally, the Rashomon sets of the iterative training procedure). The analysis collectively ensure that RashomonGB is not only practical but also robust, enhancing its applicability in various real-world scenarios.

### Information-theoretic characterization of the Rashomon set

Characterizing the entire Rashomon set, given a learning problem and dataset, has posed as a significant computational challenge (refer to Section 2). Specifically, for gradient boosting, delineating the Rashomon set \((,,f_{T}^{*},)\) entails identifying all models \(f_{T}\) such that \(L_{P_{S}}(f_{T}) L_{P_{S}}(f_{T}^{*})+\) for a given model \(f_{T}^{*}\). However, Eq. (4) presents a novel approach to approximating the Rashomon set by decomposing it into residual Rashomon sets concerning the weak learners--this method offers valuable insights into analyzing the Rashomon effect in gradient boosting.

Existing literature explores how various hypothesis spaces influence the Rashomon set but frequently neglects the impact of datasets, despite the Rashomon set being inherently tied to the dataset itself (cf. Eq. (1)). In this context, we derive a novel bound on the size of the Rashomon set by expanding the scope of the Rashomon set beyond mere consideration of hypothesis spaces, incorporating the influence of datasets through the lens of statistical learning and information theory. Particularly in the analysis of information-theoretic generalization error bounds , a learning algorithm \(A()\) is conceptualized as a random variable \(H\) that generates models within the hypothesis space \(\). Moreover, the loss function \((h,S)\) is also treated as a random variable, and is further assumed to be \(\)-sub-Gaussian5, a property that effectively generalizes the boundedness assumption of the loss function in information-theoretical analysis. Sub-Gaussianity is a practical property since for a bounded loss function \([a,b]\), which can be readily satisfied by clipping the loss6, \((h,S)\) is guaranteed to be \((b-a)/2\)-sub-Gaussian. With these properties in place, the mutual information \(I(S;H)\) between \(H\) and the random variable of dataset \(S\) emerges as a pivotal metric for assessing generalizability. By establishing a connection between generalization error bounds and the definition of a Rashomon set in Eq. (1), and leveraging the properties of a sub-Gaussian loss, we derive the following high-probability bound for a Rashomon set.

**Proposition 1**.: _For a dataset \(\), given an empirical risk minimizer \(h^{*}=*{argmin}_{h}L_{}(h)\) and a \(\)-sub-Gaussian loss \(\), with probability at least \(1-\), we have_

\[h(,,h^{*},}{ n}(+)}+}{ n}}).\] (5)

In contrast to existing analyses of the Rashomon set primarily focused on optimization perspectives, Proposition 1 offers a characterization of the size of the Rashomon set in terms of the controllable probability of the Rashomon parameter \(\). This sheds light on understanding the Rashomon set from a statistical learning standpoint. Furthermore, the mutual information \(I(S;H)\) in Eq. (5) quantifies the uncertainty of the learning algorithm with respect to the dataset \(\). In essence, \(I(S;H)\) serves as a metric for the _multiplicity of models_, thereby contributing to predictive multiplicity. By chain rules, we can decompose the mutual information into two components, each delineating a distinct source responsible for inducing multiplicity, i.e.,

\[I(S;H)=I(Y,X;H)=_{}+_{}.\] (6)

When \(I(X;H)\) is small, the model derived from the learning algorithm becomes nearly deterministic, indicating minimal uncertainty regarding model selection. Similarly, \(I(Y;H|X)\) can be expressed

Figure 1: Gradient boosting and RashomonGB with \(T\) iterations.

as \(I(Y;H|X)=g(Y|X)-g(H|Y,X)\), where \(g\) denotes the (conditional) Shannon's entropy . The first component, \(g(Y|X)\), evaluates the data quality; if the channel from \(X\) to \(Y\) is devoid of noise, the conditional entropy will be low. Conversely, if the channel is noisy, the conditional entropy will be lower-bounded by the entropy of the noise source.

The mutual information \(I(S;H)\) indeed contains intricate details about the dataset, allowing us to incorporate the dataset's influence into depicting a Rashomon set. This is precisely why we opt for information-theoretic tools over Rademacher complexity . While Rademacher complexity offers "data-dependent" generalization bounds, it lacks the ability to provide substantial insights into the dataset itself. For completeness, we include a theoretical analysis of the size of Rashomon sets based on Rademacher complexity, akin to the high probability bound in Proposition 1, in Appendix C.

Now we move back the characterizing the Rashomon set for \(\). Recall that for each iteration \(t[T]\) in gradient boosting, we may define a residual Rashomon set \(_{t}\). By plugging \(_{t}\) into \(\) in Proposition 1, and using the decomposition of the entire Rashomon set \((,,f_{T}^{*},T)\) in Eq. (4), we are now equipped to re-evaluate the entire Rashomon set of gradient boosting.

**Proposition 2**.: _Let \(h_{t}^{*}\) be the empirical risk minimizer for each boosting iteration \(_{t}\), and \(f_{T}^{*}=_{t=1}^{T}h_{t}^{*}\), then for a \(\)-sub-Gaussian loss \(\), with probability at least \(1-T\), we have_

\[f_{T}(,,f_{T}^{*},T}{n}}+_{t=1}^{T}}{n} (;H)}{}+)}).\] (7)

Proposition 2 suggests that the Rashomon set grows with the boosting iterations--it is due to the increased complexity of the overall model \(f_{t}\). Unless the number of samples \(n\) goes to infinity, the Rashomon set has a non-zero size.

### The Rashomon effect in each iteration of gradient boosting

The information-theoretic analysis presented in the previous section underscores the importance of the mutual information \(I(H;Y|X)\) in determining the size of the Rashomon set and hence the severity of predictive multiplicity. However, existing literature lacks an in-depth analysis of the information contained within the pseudo-residuals for gradient boosting algorithms. Let \(R_{t}\) represents the random variable associated with the residuals \(\{r_{ti}\}_{i=1}^{n}\). As \(t\) grows, fitting \(R_{t}\) with \(X\) becomes progressively challenging, akin to the gradient vanishing problem observed in deep learning , thereby resulting in larger conditional mutual information with data \(X\). We formalize this observation in the following proposition.

**Proposition 3**.: _For both the MSE and CE losses, the mutual information between \(H\) and the pseudo-residuals conditioned on the features \(X\) is non-decreasing with respect to the boosting iteration, i.e., let \(0 t_{1} t_{2} T\), then \(I(H;R_{t_{1}}|X) I(H;R_{t_{2}}|X)\)._

The essence of Proposition 3 lies in the fact that the information contained in \(R_{t_{2}}\) can be understood as a combination of the information in \(R_{t_{1}}\) and the additional information provided by the models fitted to the residuals between iterations \(t_{1}\) and \(t_{2}\). Proposition 3, Proposition 1, and Eq. (6) collectively hint at a _counter-intuitive_ observation: the size of the residual Rashomon set could potentially increase with more boosting iterations, i.e., for \(0 t_{1} t_{2} T\), we have \(_{t_{1}}_{t_{2}}\) and hence \(_{t}(,_{t_{1}},h_{t_{1}}^{*},_{ t_{1}})_{t}(,_{t_{2}},h_{t_{2}}^ {*},_{t_{2}})\). This implies that conducting additional boosting iterations7 may not only exacerbate over-fitting but also result in a larger Rashomon set and heightened predictive multiplicity. Note that we do not start with the assumption that \(\) increases with each iteration; rather, this conclusion emerges from Proposition 3. With a constant \(\)--as defined in Proposition 1--additional iterations result in increased conditional mutual information, which in turn necessitates a larger \(\).

Figure 2 provides a simulation with a \(20\)-dimensional Gaussian synthetic datasets with \(100\) samples, trained with gradient boosting for \(10\) iterations, where each iteration contains \(m=100\) models. Here, we show the conditional entropy8\(g(R_{t}|X)\) instead of the mutual information \(I(R_{t},X;H)\) in

Eq. (6) as the estimation of the mutual information is in general a hard task . It is clear that the conditional entropy--and consequently the mutual information--increases as the boosting procedure iterates, leading to a larger Rashomon set. This occurs because the Rashomon effect accumulates across the sequential learning problems addressed in each iteration, highlighting the cumulative impact on diversity within the model space. The Ablation study in Appendix E.4 further clarifies the selection of \(\) through its iterations. Figure E.9 demonstrates that fixing \(\) while re-training with different random seeds results in a decreasing percentage \(\) of models in the Rashomon set. This implies that to maintain a consistent \(\), the chosen Rashomon parameter \(\) must increase.

As a remark, consider training \(m\) models per iteration, and construct the overall Rashomon sets using models obtained from the \(T_{1}\)-th and \(T_{2}\)-th iterations with \(T_{1}<T_{2}\). With the same threshold \(\) for the Rashomon set, let \(_{1}\) and \(_{2}\) represent the probabilities for iterations \(T_{1}\) and \(T_{2}\), respectively, then by Proposition 3, we have \(1-_{1} 1-_{2}\). the number of models from the \(T_{1}\)-th iteration that are included in the Rashomon set with threshold \(\) will be \(m^{T_{1}}(1-_{1})\). Similarly, for the \(T_{2}\)-th iteration, the count will be \(m^{T_{2}}(1-_{2})\). It is important to note that although \(\) decreases, this reduction is linear with respect to the number of iterations (as suggested by the term \(1-T\) in Proposition 2). However, the number of models generated by Rashomon\( GB\) grows exponentially with the number of iterations. Thus, the total number of models in the Rashomon set will still asymptotically increase with the number of iterations.

## 4 Applications of Rashomon gradient boosting

We present three use cases demonstrating how RashomonGB can be deployed in practice to explore models more effectively in the Rashomon sets: (i) improving the estimation of predictive multiplicity metrics, (ii) fair model selection, and (iii) mitigating predictive multiplicity. Given that there are no existing algorithms specifically designed to explore the Rashomon set for gradient boosting, apart from re-training with different seeds [67; 48], we use the re-training strategy as our primary baseline for comparison through this section. It's noteworthy that both the re-training strategy and RashomonGB share the same training complexity. However, during the inference phase, RashomonGB exponentially expands the empirical Rashomon set (i.e., with a large \(m\) following Section 3.1). In our experiments, we adopt the settings in Friedman  with decision tree regressors as weak learners for tabular datasets and CNNs for images. See Appendix D.1 for detailed descriptions and pre-processing of the datasets, Appendix D.2 for detailed training setups, and Appendix E for additional experiments including ablation studies.

### Improving the estimation of predictive multiplicity metrics

We estimate the predictive multiplicity metrics (cf. Appendix B.2) on three tabular datasets with binary classes. Two of these datasets are from the financial domain (ACS Income  and Credit Card ), while the other is from the medical domain (Contraception). The datasets are particularly chosen as predictive multiplicity in these domains could have profound implications for fairness and justice. Note that the ACS Income dataset is an extension of the widely-used UCI Adult dataset  with many more samples (\(\)1.6 million vs. \(<\)50k in the UCI Adult), allowing us to compare methods with higher precision. Beyond binary classification and decision-tree weak learners, we use CIFAR-10  as a multi-class case study with CNNs as weak learner, i.e., a setting similar to Badirli et al. [6, GrowNet].

Figure 3 summarizes the estimation of 4 predictive multiplicity metrics using the empirical Rashomon sets obtained from re-training and RashomonGB. For a more detailed explanation on how to interpret Figure 3, see Appendix E.1. We conduct \(10\) (and \(50\) for the CIFAR-10 dataset) re-training of gradient boosting with different random seeds; each gradient boosting has \(T=10\) (and \(T=6\) for CIFAR-10) iterations and \(m=10\) (and \(m=50\) for CIFAR-10) models in each iteration. We randomly select 2 out of \(m\) models in each boosting iteration and perform RashomonGB to obtain

Figure 2: Gradient boosting for binary classification. The conditional entropy of the residuals and the predictive multiplicity (measured by VPR) increase along with the boosting iteration, which matches Proposition 1.

\(2^{T}=1024\) (and \(972\) models for CIFAR-10) models9. The leftmost column shows the CE loss vs. accuracy for the models obtained by the two methods. It is clear that with the same training cost, RashomonGB offers many more models in the Rashomon set that spread wider in the loss-accuracy plane. The rest of the four columns show the estimates of predictive multiplicity metrics given different loss deviation constraints. Since VPR, Rashomon capacity, and disagreement are defined per sample, we plot the mean and std. of the top \(10\%\) samples instead10.

As observed in Figure 3, RashomonGB consistently outperforms re-training in the ACS Income and Credit Card datasets for all predictive multiplicity metrics. The RashomonGB has more advantages especially when re-training is incapable of exploring enough models under the loss deviation constraint. For example, in the Contraception dataset, when the loss constraint is under \(0.585\), there is only one model obtained from re-training, and therefore the corresponding values of predictive multiplicity metrics remain \(0\). As the loss constraint increases, the Rashomon set from re-training has more models and its estimates converge with that of RashomonGB. The explanations above indicate that the exploration of diverse models under the same loss constraint largely affects the estimation of predictive multiplicity metrics. For the CIFAR-10 dataset, both VPR and RC values are small, while decision-based metrics, such as disagreement and discrepancy, are large. This implies that despite small score variations, a significant number of samples have scores close to the decision boundary. Consequently, a slight perturbation in scores from a different model in the Rashomon set could lead to a different class after applying \(*{argmax}\).

We include additional results on three other UCI datasets in Appendix E.2 and a comparison of the computational time11 to obtained one model from re-training and from RashomonGB in Appendix E.3. The ablation studies on different types of weak learners \(\) (e.g., linear regression), depths of decision tree weak learners, the number of boosting iteration \(T\) and the number of model in each iteration \(m\) are also include in Appendix E.5 to E.8. To elucidate the distinctions between predictive multiplicity and prediction uncertainty estimation in gradient boosting (cf. Section 2), we have compared our re-training strategy, RashomonGB against NGBoost [27; 55], PGBM , and IBUG  with the UCI Contraception dataset in Appendix E.9.

Figure 3: Re-training vs. RashomonGB in exploring the Rashomon set for predictive multiplicity metrics estimation. In the leftmost column, each marker represents a model. The rightmost 4 figures in a row share the same y-axis for the loss difference (values shown at the right), i.e., \(L_{P_{S}}(h^{*})+\) in Eq. (1). Higher predictive multiplicity values mean a better estimate. RashomonGB, with more models in the Rashomon set, offers more accurate multiplicity estimates under the same loss deviation constraints.

### Selecting models with group fairness constraints

We conduct re-training and RashomonGB on two standard datasets in the algorithmic fairness community, the UCI Adult and COMPAS recidivism datasets , with the goal of selecting a model that exhibits better fairness-accuracy trade-offs. We follow the same training procedure and gradient boosting architecture in Section 4.1. We assess the bias across different groups (e.g., female vs. male) by two group-fairness metrics12, mean equalized odds (MEO)  and statistical parity (SP)  (cf. Appendix B.4 for details). In both datasets, the group attributes are the binary "race" label. Figure 4 illustrates that with RashomonGB, a practitioner has a greater chance to select a model that better satisfies group-fairness constraints without a significant drop in accuracy.

For the UCI Adult dataset, despite that re-training achieves the highest accuracy (\( 84.5\%\)), it violates the MEO with \(0.01\). On the other hand, RashomonGB provides a model that perfectly complies with fairness constraint (MEO \( 0\)) with a drop in accuracy less than \(1\%\). Similar observations apply to the COMPAS dataset regarding SP. Moreover, we include four additional fairness intervention baselines, EqQdds, Rejection, Reduction, and FaiRS. See Appendix B.4 for a brief introduction of the fairness intervention baselines. The most relevant baseline to RashomonGB is FaiRS, which modifies the Reduction approach to address fairness intervention problems across the models in the Rashomon set. FaiRS is originally implemented for logistic regression, and we adapt it to gradient boosting.

For both UCI Adult and COMPAS datasets, RashomonGB encompasses nearly all models (excluding EqQdds) from the fairness intervention baselines-without explicit fairness intervention-indicating that RashomonGB gives a "rich" Rashomon set for model selection with additional fairness considerations. The advantage of RashomonGB becomes even more pronounced when dealing with larger datasets. In such scenarios, re-training and re-training-based fairness intervention algorithms, such as Reduction (and FaiRS) and Rejection, may incur significantly higher training costs. Model selection with RashomonGB is not limited the group fairness purposes; this experiment serves as an initial demonstration of the diversity of RashomonGB use cases. In Section 4.3 we demonstrate explicit feedback in the model selection process.

### Mitigating predictive multiplicity

The RashomonGB framework, which involves training \(m\) distinct models in each iteration, offers the added benefit of reducing predictive multiplicity in gradient boosting. The \(m\) models in the empirical Rashomon set for \(^{m}(,_{t},h_{t}^{*},)\) for the \(t^{th}\) iteration can either be selected (based on the least losses) or aggregated (similarly to model averaging in Section 2). Building on these concepts, we propose two approaches to reduce predictive multiplicity in gradient boosting: (i) _model selection_ with reweighted loss (MS), and (ii) _intermediate ensembles_ during boosting iterations (IE).

Let \(_{i,j}=(h_{j},_{i})\) be the loss evaluated at sample \(_{i}\) for model \(h_{j}^{m}(,_{t},h_{t}^{*},)\), and let \(_{i}=_{j=1}^{m}_{i,j}\) be the mean loss. The MS method considers the reweighted loss for each model in the empirical Rashomon set using \(_{h_{j}}_{i=1}^{n}_{i,j}(_{i,j}/_{i})^{}\), and selects the top \(k\) models with the smallest \(_{h_{j}}\), where \(k m\). MS simplifies to re-training at \(=0\). The model with the smallest \(_{h_{j}}\) is used to compute the residuals for the next iteration, and we return the top \(k\) models at the last boosting iteration. The intuition of this reweighting is that the loss contribution of sample \(_{i}\) is rewarded at an exponential scaling factor \( 0\) when the model \(h_{j}\) produces lower loss than the average for \(_{i}\) over all models in \(^{m}(,_{t},h_{t}^{*},)\). In contrast, the IE method constructs \(U\) ensembles \(}\), \(u[U]\) in each iteration, where each ensemble consists of \(E\) randomly selected

Figure 4: Fairness-accuracy trade-off for re-training vs. RashomonGB on test set. Each marker represents a model. A better trade-off means a smaller group-fairness metric (MEO or SP) and a higher accuracy, i.e., the top left area. For both datasets, RashomonGB provides more better models to select that complies with the fairness constraints whilst having the highest accuracy. For UCI Adult, the CE loss of the models from RashomonGB is \(0.38 0.02\) and \(0.64 0.02\) for COMPAS.

models from \(^{m}(,_{t},h_{t}^{s},)\). The model \(}\) is constructed by an additive weighted sum of the outputs of the \(E\) models, i.e., \(}(_{i})=_{e=1}^{E}w_{e}h_{e}( _{i})\), where the weights \(w_{e}=(1/_{h_{e}})/w\), and \(w\) is the harmonic mean of all losses \(\{_{h_{e}}\}_{e=1}^{E}\).

For evaluation, we extend the sample-wise metric disagreement \((_{i})\) in Kulynych et al.  (cf. Appendix B.2) to consider the disagreement across all samples in the whole dataset. We call this new metric \(p\)_-disagreement_13, defined as \(d(,p)_{i=1}^{n}[( _{i}) p]\). In Figure 5 we report the reduction of predictive multiplicity (\( 0\)-disagreement) and improvement of accuracy (\(\)accuracy) vs. a re-training baseline, on \(18\) UCI tabular datasets . In our experiments, \(U=m=100\), \(k=25\), and \(E=20\). For a fair comparison, MS and IE share the same training procedure as re-training--in all iterations we select the top \(k=25\) from \(m=100\) models.

We observe that IE outperforms MS in disagreement reduction while both methods yield a similar accuracy to re-training. However, IE has the cost of increasing the overall model complexity by a factor of \(E=20\), which may be undesirable for interpretability or auditing. It is noteworthy that a small \(\)accuracy could lead to a great reduction in disagreement. For example, in "epileptic-seizure", re-training has a \(0\)-disagreement of \(0.208\) and IE reduces it to \(0.041\) with only a slight improvement of \(0.014 0.002\) in accuracy. For ablation studies on the hyperparameters \(E\), \(k\) and \(\) and more explanations, see Appendix E.10.

## 5 Discussion

Here we reflect on the limitations and highlight interesting avenues for future work.

Limitations.While re-training with different random seeds offers a "global" exploration of models within the Rashomon set, RashomonGB conducts a "local" exploration. Therefore, the effectiveness of RashomonGB depends on selecting diverse models in each iteration; similar models reduce its efficiency in exploring Rashomon sets. However, RashomonGB demonstrates greater efficiency than the re-training strategy, highlighting a trade-off between the efficiency of exploring the Rashomon set and the effectiveness of capturing model diversity. Finding the (sub-)optimal strategy for exploring the Rashomon set remains an active area of research. Our theoretical analysis, developed for gradient boosting, needs an extension to other algorithms like adaptive boosting to validate its applicability. Additionally, the large number of models generated by RashomonGB poses storage challenges, requiring new data structures for efficient management.

Future directions.First, our analysis in Section 3 links the size of the Rashomon set to dataset quality, providing a foundation for studying how dataset properties impact the Rashomon effect. Second, combining models in each RashomonGB iteration is similar to model stitching , suggesting potential insights if adapted for neural networks. Third, varying the number of models selected per iteration could enhance RashomonGB's flexibility and effectiveness.

Figure 5: re-training vs. MS with \(=3\) (left) and IE with \(E=20\) (right) to mitigate predictive multiplicity on 18 UCI datasets. Each point is averaged over \(20\) random train-test splits (std. omitted for clarity). Dashed lines are the mean of each axis. _Higher values are better_ for both axes.

Disclaimer.This paper was prepared for informational purposes by the Global Technology Applied Research center and Artificial Intelligence Research group of JPMorgan Chase & Co. This paper is not a product of the Research Department of JPMorgan Chase & Co. or its affiliates. Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation or warranty and none of them accept any liability in connection with this paper, including, without limitation, with respect to the completeness, accuracy, or reliability of the information contained herein and the potential legal, compliance, tax, or accounting effects thereof. This document is not intended as investment research or investment advice, or as a recommendation, offer, or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction.