# ABEL: Sample Efficient Online Reinforcement Learning for Neural Theorem Proving

Fabian Gloeckle

FAIR at Meta

Ecole des Ponts Paris

Jannis Limperg

LMU Munich

Gabriel Synnaeve

FAIR at Meta

Amaury Hayat

Ecole des Ponts Paris

###### Abstract

We propose a scalable and efficient reinforcement learning framework as a strong baseline for theorem proving with limited data. This baseline reaches performances comparable to the current state-of-the-art in theorem proving with 59.8 % of problems solved on MiniF2F-_valid_ cumulatively and the current state of the art of 7/640 solved problems on PutnamBench, while only training on a few hundred examples in the reinforcement learning set. This a first step toward an efficient and easily reproducible combination of autoformalization, synthetic data generation and reinforcement learning, which could unlock significant advancements in neural theorem proving.

## 1 Introduction

Mathematical reasoning constitutes a major challenge for deep-learning models, and now a very active research area [Williamson, 2024]. Formal languages such as Isabelle [Paulson, 1994], Coq [Barras et al., 1997], and Lean [de Moura et al., 2015, Moura and Ullrich, 2021] have been developed to enable automatic computer verification of proofs and can now serve as grounding to prevent language model hallucinations. Several approaches relying on LLMs and formal proof search environments have recently been proposed (App. A), but were limited by the scarcity of formal training data (around 100k lemmas in Lean's central theorem library _Mathlib_[Matlib Community, 2020]) and the data inefficiency of machine learning methods. For this reason, most of the works on neural theorem proving have focused on obtaining more data, either from autoformalization or synthetic data generation [Xin et al., 2024a].

In this paper, we present ABEL, a scalable and compute efficient online reinforcement learning framework for theorem proving. This serves as a strong baseline of what is achievable with very limited data and as a first step toward a combination of online reinforcement learning and autoformalization.

We show that training on only several hundred maths exercises, one can reach performances comparable to the current state-of-the-art in theorem proving. We use MiniF2F [Zheng et al., 2021], a well-established benchmark in the field, as our primary evaluation set. Our model outperforms the cumulative performances of [Lample et al., 2022] with 13 times less compute and no synthetic data. It also reaches a new state-of-the-art on PutnamBench [Tsoukalas et al., 2024b], a dataset of formidably challenging olymiad-like problems from the Putnam competition in North America, by solving 7 problems overall and discovering one formalization error, 4 more than the previous best [Tsoukalas et al., 2024a].

This suggests that combining our framework with autoformalization and synthetic data generation could achieve much higher levels of performance and unlock significant advancements in neural theorem proving.

Method

When approaching theorem proving as a problem of reinforcement learning, we must take care to cast the problem in a way that allows for efficient exploration and recombination of learned features. In this work, we follow the formulation of _hypertree proof search_ (HTPS) (Lample et al., 2022) and opt for a two-fold tree structure for proof searches. An individual complete Lean proof is naturally represented as a tree, where nodes correspond to proof goals and tactics like induction transform a proof goal into a set of children, or subgoals, that _jointly_ suffice to be shown instead (e.g. induction base and step). Proof search, on the other hand, involves applying several tactics at each node, proof success of _any_ of which results in proof success for the parent node. Such a tree can be represented as an alternating tree of AND-joined sets of subgoals and OR-joined sets of tactic attempts, making the overall tree an AND/OR-tree or, equivalently, a "proof hypertree"1.

Our reinforcement learning system for formal theorem proving in Lean 4 (Moura and Ullrich, 2021) consists of three components: a programming interface based on Aesop (Limperg and From, 2023) to organize proof searches in Lean 4, the HTPS proof search procedure inspired by the AlphaZero expert iteration algorithm (Anthony et al., 2017; Silver et al., 2018) and an online retraining mechanism. We will explain each component in the following paragraphs.

AesopRepl: a proof search interface for Lean 4Our Read-Eval-Print Loop (REPL), AesopRepl, makes the infrastructure of Lean's proof search automation tactic Aesop (Limperg and From, 2023) available as a command line interface. Aesop manages an AND/OR-proof tree and can receive tactic suggestions generated by a language model via the interface. It executes the tactics in their respective contexts and updates the tree, checking which goals have been resolved. Once the root node of the proof tree (i.e., the initial goal) is proved, the REPL passes the resulting proof to Lean's kernel. This guards against bugs in the applied tactics. The main **benefit** of using Aesop's proof tree implementation is that it allows goals with shared metavariables to be processed semi-independently, improving on GPT-\(f\)(Poplu and Sutskever, 2020) and LeanDojo (Yang et al., 2023) and matching HTPS (Lample et al., 2022). See App. J for details.

Proof search with HTPSOur proof search procedure uses the AlphaZero algorithm (Silver et al., 2018) with the adaptations for hypertrees proposed by (Lample et al., 2022). A proof search consists of several rounds, each of which has three steps: _node selection_, _expansion_ and _value propagation_. The proof search is terminated when either a proof has been found during a round or a given node expansion budget is exceeded. In the selection phase, the tree is traversed down until a solving set of leaf nodes is found, i.e. a set of nodes which, if proven, would finish the overall proof. This means that at AND-nodes, all children are selected while at OR-nodes, we select a tactic based on the established predictor upper confidence bound for trees (PUCT) policy (Kocsis and Szepesvari, 2006; Rosin, 2011; Silver et al., 2017) given below. In the expansion phase, all selected nodes are expanded simultaneously, i.e. a tactic model suggests tactics and a critic model evaluates the value (likelihood of proof success) of the given proof goal. We execute the tactics eagerly in Lean in order to return from successful proof searches as early as possible. The transition dynamics \(T(s,a)=s^{}\) of Lean, where \(s\) is a state/goal and \(a\) an action/tactic, i.e. the effect of a tactic execution, are sometimes hard to model (tactics like simp involve a search procedure themselves), so we follow HTPS in using a state value critic \(c(s^{})\) as opposed to a state-action critic \(Q(s,a)\) in AlphaZero (Silver et al., 2018). These _critic_ values of expanded nodes are then propagated up the tree: each edge \((s,a)\) maintains a visit count \(N(s,a)\) and a cumulative action value \(W(s,a)\). Each node \(s\) in the selected proof tree is assigned an update value \(v(s)\) which is defined as follows:

\[v(s)=1&\\ c(s)&\\ _{c}v(c)&\]

where \(\) is a depth penalty factor and \(c(s)\) denotes the evaluation provided by the critic model. The cumulative action value \(W(s,a)\) of each edge \((s,a,s^{})\) in the selected tree is then increased by the update value \(v(s^{})\) of its target node, and its visit count by 1. With these quantities, we can define the PUCT selection policy:

\[(s)=*{argmax}_{a}(s,a)+c_{} (a\,|\,s)}N(s,a^{})}}{1+N(s,a)},\]

where

\[(s,a)=&N(s,a)>0,\\ c(s)&\]

is the empirical action value of the edge \((s,a)\) and \(c_{}\) is an exploration coefficient and \((a\,|\,s)\) is the prior of action \(a\) under our tactic model \(\), for which we take the product of token probabilities of each tactic sequence, renormalized to sum to one at any node. Note that in our choice of \((s,a)=c(s)\) for unvisited nodes we depart from Lample et al. (2022) (who choose \((s,a)=\)), leveraging the intuition that \(c(s)\) is trained to match the expected proof success rate when performing tree search using \(\), and that \(a\) is a sample from \(\).

Following Lample et al. (2022), instead of fixing proof search hyperparameters upfront, we sample them on a per proof attempt basis (App H).

Online reinforcement learningWhen a proof search is terminated (because a proof has been found or the search budget is exhausted), we extract training samples for the policy and critic models for retraining. We train the **policy model** with supervised training using standard causal language modelling loss on all tactic samples that are part of a proof of their parent node - regardless of whether they were part of a proof of the root node (_All Solved_ setting of Lample et al. (2022)). This makes the tactic reinforcement learning loop a form of iterated rejection sampling.

The **critic model** is trained with supervised training using a binary cross-entropy loss of Bernoulli variables for the classification task of provability. As the supervision target, we use \((V(s))\) with \(V(s)\) given by

\[V(s)=)}{N(s,a^{*})},\]

for a node \(s\) and \(a^{*}=(s)\). Unlike Lample et al. (2022), we do not threshold the visit counts \(_{a}N(s,a)\) of a node to be selected.

Our **distributed reinforcement learning** setup comprises worker and trainer GPUs, prover threads controlling Lean AesopRepl processes as well as a centralized replay buffer and a task dispenser (App. D). We select proof tasks with **prioritized sampling** to favor exploration: if \(n_{i}\) is the number of successful proof searches for problem \(i\) so far, we use a weight proportional to \((n_{i}+1)^{-}\) for sampling problem \(i\) as the next task, where \( 0\) is an upsampling coefficient for hard tasks2. We **postprocess tactics** to induce a distribution shift from style aimed at efficient _proof presentation_ as pursued in Lean's Mathlib (Mathlib Community, 2020) to effective _proof search_ (App. E).

## 3 Results

We conduct experiments on challenging theorem proving datasets, showcasing the efficiency of our theorem proving system. We use Llama 3.1 base 8B (Dubey et al., 2024) as a starting point and finetune on proof step data from Mathlib (Mathlib Community, 2020) extracted with LeanDojo (Yang et al., 2023) (App. F for details). We then apply the RL training procedure described in Section 2. Our results are summarized in Table 1.

Our models' cumulative performance, while below other approaches relying on large sets of data, outperforms HTPS (Lample et al., 2022) despite using a much smaller compute budget and no synthetic data. Our models also show high pass@1 performances on the test set, which are only surpassed by DeepSeek-Prover, and achieve a new state-of-the-art on PutnamBench. This is detailed below.

Outperforming HTPS with 13x less computeTraining with 256 GPUs on MiniF2F-_valid_, we reach a cumulative solve rate of 59.8% after 3 hours of training time, outperforming HTPS's 58.6% trained with an additional synthetic supervised dataset of Lean problems and 54.9% without. These results required HTPS 30240 A100-hours of compute, while we need 768 H100-hours \(\) 2304 A-100-hours [Databricks, 2023], approximately 13 times less. Halving the number of GPUs to 128, we reach a cumulative solve rate of 57.4% after 5 hours, still competitive with HTPS.

A new state of the art on PutnamBenchWhen run on a 1:1:1 mix of problems from MiniF2F-_valid_, MiniF2F-_test_ and PutnamBench [Tsoukalas et al., 2024b], we solve 7/640 problems in the setting with provided solutions after only attempting each problem between 6 and 16 times (and with no improvements thereafter with up to 590 attempts). This improves over the previous state of the art of 4/640 solved problems [Tsoukalas et al., 2024a] held by InternLM2-StepProver [Wu et al., 2024] and obtained with 4096 attempts per problem. In a different run, our system solved another problem, bringing the total to 8/640. See App. B for the proofs, a **qualitative evaluation** and the cumulative solve rates on the respective datasets.

Importance of online trainingWe evaluate the performance of pure sampling, expert iteration [Anthony et al., 2017, Polu et al., 2022], and online reinforcement learning by comparing runs with model updates at different frequencies. Our findings indicate that online reinforcement learning is crucial (App. C).

Stability and explorationReinforcement learning on MiniF2F-_valid_ - which comprises 244 problems only - presents challenges regarding stability and distributional collapse. In our online training loop, this becomes exacerbated by the fact that at least 26.6% of problems in MiniF2F can be solved by a single or a short sequence of automation tactics (among aesop, ring, linarith, nlinarith). Such proofs will be found first and could quickly flood the trainers low-quality data that lacks diversity. We remediate this issue in several ways: by setting a _burn-in_ of 8000 training samples to discard before training, by using supervised data for 10% of training samples, by using hard negative sampling (Sect. 2) by using a large number of tactics per node and decoding parameters that favor diversity as well as carefully tuning key hyperparameters (App. I). As shown in Fig. 1, the policy continues to explore and optimizes solved problems to shorter proofs, but we still see a slow decline in tactic diversity when problem solve rates eventually plateau.

  
**Model** & **Type** & **test** & **Budget** & **valid** & **Train time** & **RL set size** \\  & & _eval._ & & _cumul._ & (A100-days) & \\  Llemma-7b & pretr. & 26.2 & \(1 600\, 32\) & – & (1000) & – \\ ReProver & SFT & 26.5 & \(1 600\, 64\) & – & 5 & – \\ Lean automation & alg. & 27.5 & \(1\) & 26.6 eval & – & – \\  DeepSeek-Prover & exp.it. & 48.8 & 8192 & 60.2 &? & 8M \\ DeepSeek-Prover-1.5 & exp.it.+RL & 55.0 & \(1 3200\) & – &? & 8M \\ GPT-\(f\) & exp.it. & 36.6 & \(64 512 8\) & 47.3 eval. & 2000 & 327 + _synth._ \\ HTPS & RL & 41.0 & \(64 5000 26\,\) & 58.6 & 1360 & 244 + _synth._ \\ _ABEL (ours)_ & RL & 41.3 & \(1 128 64\) & 59.8 & 96 & 244 \\   

Table 1: **Performance of different models on MiniF2F-_valid_ and MiniF2F-_test_.** We compare representative methods without reinforcement learning with recent expert iteration and online reinforcement learning approaches. Numbers on MiniF2F-_test_ are with the indicated evaluation budget given as \(n_{} n_{} n_{}\). For MiniF2F-_valid_, we report _cumulative_ solve rates over the course of the run where it is included in the reinforcement learning set. We convert train times to A100-days with a factor 3 for H100 to A100 performance [Databricks, 2023]. For additional discussion and details, see App. K.

## 4 Conclusion

We presented an online reinforcement learning method for Lean that reaches near state-of-the-art performance on theorem proving while relying only on few hundred problems in its reinforcement learning problem set. It is orders of magnitude more sample efficient than methods of comparable performance that require extensive synthetic datasets with up to millions of additional data points (Xin et al., 2024), and around 13 times more compute efficient than the current state-of-the-art RL method (Lample et al., 2022).

Online reinforcement learning can be tricky to stabilize but carefully tuned, complements and empowers other recent techniques such as synthetic data from large-scale autoformalization (Xin et al., 2024), bootstrapping chains-of-thought (Lin et al., 2024) or hybrid local-global tree search (Xin et al., 2024). In future work, we believe the setup should be adapted to _condition_ the policy on the node's ancestors, i.e. on the partial proof to be completed. When deploying automated provers for _theory autoformalization_, additional conditioning on a human-written or language model generated informal proof is additionally required, for instance in a setting similar to Draft-Sketch-Prove (Jiang et al., 2023).

Harnessing _reinforcement learning as a test-time inference technique_ could allow tackling challenging problems such as the International Mathematics Olympiad (DeepMind, 2024) or autoformalizing mathematical theories, where sample efficiency is the primary concern.