# SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete Diffusion Process

Mengyu Wang\({}^{1,3}\), &Henghui Ding\({}^{4}\), &Jun Hao Liew\({}^{5}\), &Jiajun Liu\({}^{5}\) &Yao Zhao\({}^{1,2,3}\), &Yunchao Wei\({}^{1,2,3}\)

\({}^{1}\) Institute of Information Science, Beijing Jiaotong University

\({}^{2}\) Peng Cheng Laboratory

\({}^{3}\) Beijing Key Laboratory of Advanced Information Science and Network

\({}^{4}\) Nanyang Technological University \({}^{5}\) ByteDance

wmengyu0826@gmail.com

Work done during an internship at ByteDance.Co-corresponding author.

###### Abstract

In this paper, we explore a principal way to enhance the quality of object masks produced by different segmentation models. We propose a model-agnostic solution called SegRefiner, which offers a novel perspective on this problem by interpreting segmentation refinement as a data generation process. As a result, the refinement process can be smoothly implemented through a series of denoising diffusion steps. Specifically, SegRefiner takes coarse masks as inputs and refines them using a discrete diffusion process. By predicting the label and corresponding states-transition probabilities for each pixel, SegRefiner progressively refines the noisy masks in a conditional denoising manner. To assess the effectiveness of SegRefiner, we conduct comprehensive experiments on various segmentation tasks, including semantic segmentation, instance segmentation, and dichotomous image segmentation. The results demonstrate the superiority of our SegRefiner from multiple aspects. Firstly, it consistently improves both the segmentation metrics and boundary metrics across different types of coarse masks. Secondly, it outperforms previous model-agnostic refinement methods by a significant margin. Lastly, it exhibits a strong capability to capture extremely fine details when refining high-resolution images. The source code and trained models are available at github.com/MengyuWang826/SegRefiner.

## 1 Introduction

Although segmentation in image  and video  has been extensively studied in the past decades, obtaining accurate and detailed segmentation masks is always challenging since high-quality segmentation requires the model to capture both high-level semantic information and fine-grained texture information to make accurate predictions. This challenge is particularly pronounced for images with resolutions of \(2K\) or higher, which requires considerable computational complexity and memory usage. As a result, existing segmentation algorithms often predict masks at a smaller size, inevitably leading to lower accuracy due to the loss of fine-grained information during downsampling.

Since directly predicting high-quality masks is challenging, some previous works have shifted their attention to the refinement of coarse masks obtained from a preceding segmentation model. A popular line of direction  is to augment the segmentation models (and features) with a newmodule for masks correction. However, such approaches are usually model-specific, and hence cannot be generalized to refine coarse masks produced by other segmentation models. Another part segmentation refinement works [47; 56; 12; 44], on the other hand, resort to model-agnostic approach by taking only an image and the coarse segmentation as input for refinement. These methods have greater practical utility, as they are applicable to refine different segmentation models. However, the diverse types of errors (_e.g._, errors along object boundaries, failure in capturing fine-grained details in high-resolution images and errors due to incorrect semantics) presented in the coarse masks pose a great challenge to the refinement model thus causing underperformance (refer to Fig. 1).

In response to this challenge, we draw inspiration from the working principle of denoising diffusion models [45; 22; 2]. Diffusion models perform denoising at each timestep and gradually approach the image distribution through multiple iterations. This iterative strategy significantly reduces the difficulty of fitting the target distribution all at once, empowering diffusion models to generate high-quality images. Intuitively, by applying this strategy to the segmentation refinement task, refinement model can focus on correcting only some "most obvious errors" at each step and iteratively converge to an accurate result, thus reducing the difficulty of correcting all errors in a single pass and enabling refinement model to handle more challenging instances.

Under this perspective, we present an innovative interpretation of the task by representing segmentation refinement as a data generation process. As a result, refinement can be implemented through a sequence of denoising diffusion steps with coarse segmentation masks being the noisy version of ground truth. To work with binary masks, we further devise a novel discrete diffusion process, where every pixel performs _unidirectional randomly states-transition_. The proposed process can gradually convert ground truth into a coarse mask during training and employ the coarse mask as sampling origination during inference. In other words, we formulate the mask refinement task as a conditional generation problem, where the input image serves as the condition for iteratively updating/refining the erroneous predictions in the coarse mask.

To our best knowledge, we are the first to introduce diffusion-based refinement for segmentation masks. Our method, called SegRefiner, is model-agnostic, and thus applicable across different segmentation models and tasks. We extensively analyze the performance of SegRefiner across various segmentation tasks, demonstrating that our SegRefiner not only outperforms all previous model-agnostic refinement methods (with +3.42 IoU, +2.21 mBA in semantic segmentation and +0.9 Mask AP, +2.2 Boundary AP in instance segmentation), but can also be effortlessly transferred to other segmentation tasks (_e.g._, the recently proposed dichotomous image segmentation task ) without any modification. Additionally, SegRefiner demonstrates a strong capability for capturing extremely fine details when applied to high-resolution images (see Fig. 1 and Fig. 4).

Figure 1: Diverse errors presented in the previous segmentation results. The top row and middle row show the significant false positives and false negatives caused by incorrect semantics, the bottom row shows the inaccurate in capturing fine-grained details. Please zoom in for a better view.

Related Work

Segmentation RefinemementThe aim of segmentation refinement is to improve the quality of masks in pre-existing segmentation models. Some works focus on enhancing specific segmentation models. PointRend  employs an MLP to predict the labels of pixels with low-confidence scores output from Mask R-CNN . RefineMask  incorporates a semantic head to Mask R-CNN as additional guidance. MaskTransfiner  employs an independent FCN  to detect incoherent regions and refines their labels with a Transformer . These works have demonstrated notable performance enhancements over their preceding models. However, their scope of improvement is model-specific and they lack the capacity to directly refine the coarse mask derived from other models. There are also some refinement methods that adopt model-agnostic approaches, such as [47; 56; 61; 12; 44; 31]. These strategies emphasize utilizing diverse forms of input, including whole images, boundary patches, edge strips, _etc._ Even though these techniques can refine coarse masks derived from different models, their applicability remains confined to specific segmentation tasks. As a special case, SegFix , which learns a mapping function between edge pixels and inner pixels and subsequently replaces inaccurate edge predictions with corresponding inner pixel predictions, is employed in both semantic segmentation and instance segmentation within the Cityscapes dataset . While the performance of SegFix is significantly constrained by its ability to accurately identify objects within an image, which consequently leads to a decline in performance on datasets with a more extensive range of categories (_e.g._, COCO ).

Diffusion Models for Detection and SegmentationRecently, diffusion models have received a lot of attention in research. Initial studies [45; 22; 2; 46; 8; 3] primarily sought to enhance and expand the diffusion framework. Following these, subsequent works ventured to incorporate diffusion models across a broader array of tasks [43; 9; 30] and to formulate comprehensive conditional generation frameworks [41; 58]. The application of diffusion models to detection and segmentation tasks has also been the focus of an escalating number of studies. Baranchuk _et al_.  capture the intermediate activations from diffusion models and employ an MLP to execute per-pixel classification. DiffusionDet  and DiffusionInst  adapt the diffusion process to perform denoising in object boxes and mask filters. Some other works [1; 52; 53; 7; 26] consider the image segmentation task as mask generation. These studies predominantly follow the Gaussian diffusion process of DDPM  and leverage an additional image encoder to extract image features as condition to generate masks. To the best of our knowledge, our SegRefiner is the first work that applies a diffusion model to the image segmentation refinement task, and it also pioneers the abandonment of the Gaussian assumption in favor of a newly designed discrete diffusion process in diffusion-based segmentation tasks.

## 3 Methodology

### Preliminaries: Diffusion Models

Diffusion models consist of a forward and a reverse process. The forward process \(q(x_{1:T}|x_{0})\) uses a Markov or None-Markov chain to gradually convert the data distribution \(x_{0} q(x_{0})\) into complete noise \(x_{T}\) whereas the reverse process deploys a gradual denoising procedure \(p_{}(x_{0:T})\) that transforms a random noise back into the original data distribution.

Continuous Diffusion ModelsThe majority of existing continuous diffusion models [22; 46; 41] adheres to the Gaussian assumption and defines \(p(x_{T})=(x_{T}|0,1)\). The mean and variance of forward process are defined by a hyperparameter \(_{t}\) and the reverse process utilizes mean and variance from model predictions, thus formulating as:

\[q(x_{t}|x_{t-1}) =(x_{t}|}x_{t-1},_{t}I),\] (1) \[p_{}(x_{t-1}|x_{t}) =(x_{t-1}|_{}(x_{t},t),_{}(x_{t}, t)).\] (2)

Discrete Diffusion ModelsCompared to continuous diffusion models, there is less research on discrete diffusion models. Sohl-Dickstein _et al._ first introduce binary diffusion to reconstruct one-dimensional noisy binary sequences. \(x_{T}\) is defined to adhere to the Bernoulli distribution \((x_{T}|0.5)\). The forward process and reverse process are represented as:

\[q(x_{t}|x_{t-1}) =(x_{t}|x_{t-1}(1-_{t})+0.5_{t}),\] (3) \[p_{}(x_{t-1}|x_{t}) =(x_{t-1}|f_{b}(x_{t},t)).\] (4)where \(_{t}(0,1)\) is a hyperparameter and \(f_{b}(x_{t},t)\) is a model predicting Bernoulli probability. After the great success of DDPM , Austin _et al._ extended the architecture of Discrete Diffusion Model to a more general form. They define the forward process as a discrete random variable transitioning among multiple states and use states-transition matrix \(Q_{t}\) to characterize this process:

\[[Q_{t}]_{m,n}=q(x_{t}=n|x_{t-1}=m).\] (5)

### SegRefiner

In this work, we propose SegRefiner, with a unique discrete diffusion process, which can be applied to refine coarse masks from various segmentation models and tasks. SegRefiner performs refinement with a _coarse-to-fine_ diffusion process. In the forward process, SegRefiner employs a discrete diffusion process which is formulated as unidirectional random states-transition, gradually degrading the ground truth mask into a coarse mask. In the reverse process, SegRefiner begins with a provided coarse mask and gradually transforms the pixels in the coarse mask to the refined state, correcting the wrongly predicted area in the coarse mask. In the following paragraphs, we will provide a detailed description of the forward and reverse process.

Forward diffusion processIn the forward process, we gradually degrade the ground truth mask/ fine mask \(M_{fine}\), transiting it into a coarse mask \(M_{coarse}\). In other words, we have \(m_{0}=M_{fine}\) and \(m_{T}=M_{coarse}\). At any intermediate timestep \(t\{1,2,...,T-1\}\), the intermediate mask \(m_{t}\) is therefore in a transitional phase between \(M_{fine}\) and \(M_{coarse}\).

We define that every pixel in \(m_{t}\) has two states: fine and coarse, and the forward process is thus formulated as states-transition between these two states. Pixels in the fine state will retain their values from \(M_{fine}\), and _vice versa_. We propose a new transition sample module to formulate this process. As shown in Fig. 2 right, during forward process, the transition sample module takes the previous mask \(m_{t-1}\), coarse mask \(m_{T}\) and a states-transition probability as input and outputs a transitioned mask \(m_{t}\). The states-transition probability describes the probability of every pixel in \(m_{t-1}\) transitioning to the coarse state. This module first performs Gumbel-max sampling  according to the states-transition probability and obtains the transitioned pixels. Then, the transitioned pixels will take values from \(m_{T}\) while the non-transitioned pixels will keep unchanged.

Note that the transition sample module represents a **unidirectional** process in which only "transition to coarse state" happens. The unidirectional property ensures that the forward process will converges

Figure 2: **An overview of the proposed SegRefiner** (best viewed in color). On the right is the transition sample module we proposed, which randomly samples pixels from the current mask based on the input states-transition probabilities and change their values to match those in the target mask. During training, the transition sample module transforms the ground truth into a coarse mask, thus coarse mask is the target mask. During inference, target mask refers to the predicted fine mask and this module updates the values in the coarse mask in each timestep based on the predicted fine mask and the transition probabilities.

to \(M_{coarse}\), despite each step is completely random. This is a significant difference between our SegRefiner and previous diffusion models in which forward process converges to a random noise.

With the reparameterization trick, we introduce a binary random variable \(x\) to formulate the above process. We represent \(x_{t}^{i,j}\) as a one-hot vector to represent the state of pixel \((i,j)\) in \(m_{t}\) and set \(x_{0}^{i,j}=\) and \(x_{T}^{i,j}=\) to represent the fine state and coarse state, respectively. The forward process can thus be formulated as:

\[q(x_{t}^{i,j}|x_{t-1}^{i,j})=x_{t-1}^{i,j}Q_{t},Q_{t}= _{t}&1-_{t}\\ 0&1,\] (6)

where \(_{t}\), and \(1-_{t}\) corresponds to the states-transition probability used in our transition sample module. \(Q_{t}\) is a states-transition matrix. The form of \(Q_{t}\) explicitly manifests the unidirectional property, _i.e._, all pixels in the coarse state will never transition back to the fine state since \(q(x_{t}|)=\). According to Eq. (6), the marginal distribution can be formulated as

\[q(x_{t}^{i,j}|x_{0}^{i,j})=x_{0}^{i,j}Q_{1}Q_{2} Q_{t}=x_{0}_{t}= x_{0}_{t}&1-_{t}\\ 0&1,\] (7)

where \(_{t}=_{1}_{2}_{t}\). Given this, we can obtain the intermediate mask \(m_{t}\) at any intermediate timestep \(t\) without the need of step-by-step sampling \(q(x_{t}|x_{t-1})\), allowing faster training.

Reverse diffusion processThe reverse diffusion process takes a coarse mask \(m_{T}\) and gradually transforms it into a fine mask \(m_{0}\). However, since the fine mask \(m_{0}\) and the reversed states-transition probability is unknown, following DDPM , we train a neural network \(f_{}\) parameterized by \(\) to predict the fine mask \(_{0|t}\) at each timestep, represented as

\[_{0|t},\ p_{}(_{0|t})=f_{}(I,m_{t},t),\] (8)

where \(I\) is the corresponding image. \(_{0|t}\) and \(p_{}(_{0|t})\) denote the predicted binary fine mask and its confidence score, respectively. To obtain the reversed states-transition probability, according to Eq. (6), Eq. (7), and Bayes' theorem, we first formulate the posterior at timestep \(t-1\) as:

\[q(x_{t-1}|x_{t},x_{0})=|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x _{0})}=Q_{t}^{} x_{0}_{t-1}}{x_{0}_{t}x_{t} ^{}},\] (9)

where the fine state \(x_{0}\) is set to \(\) during training, indicating ground truth. While during inference, \(x_{0}\) is unknown, as the predicted \(_{0|t}\) may not be entirely accurate. Since the confidence score \(p_{}(_{0|t})\) represents the model's level of certainty for each pixel prediction being correct, \(p_{}(_{0|t})\) can also be interpreted as the probability being in the fine state. Therefore, intuitively, one could

Figure 3: **Examples of SegRefinerâ€™s inference process.**\(x_{t}\) and \(m_{t}\) denote the state (we represent state \(\) and \(\) as 0 and 1 in this figure) and the corresponding mask, respectively. SegRefiner begins with \(x_{T}=\) and \(m_{T}=M_{coarse}\), and gradually refines until we obtain fine mask \(m_{0}\).

obtain the state of every pixel in \(_{0|t}\) by simply thresholding as done in :

\[x_{0|t}^{i,j}=&p_{}(_{0|t})^{i,j}  0.5\\ &,\] (10)

where pixels with higher confidence scores will have \(x_{0|t}^{i,j}=\), indicating they are in the fine state, and _vice versa_. However, in such one-hot form, the values of states-transition probabilities are determined solely by the pre-defined hyperparameters, leading to significant information loss. Instead, we retain the soft transition and formulate \(x_{0|t}^{i,j}=[p_{}(_{0|t})^{i,j},1-p_{}(_{0|t}) ^{i,j}]\). With this setting, the reverse diffusion process can be reformulated as

\[p_{}(x_{t-1}^{i,j}|x_{t}^{i,j})=x_{t}^{i,j}P_{,t}^{i,j}, {where }P_{,t}^{i,j}=1&0\\ (_{0,t})^{i,j}(_{t-1}-_{t})}{1-p_{ }(_{0,t})^{i,j}_{t}}&(_{0,t})^{i,j}_{t-1}}{1-p_{}(_{0,t})^{i,j}_{t}} ,\] (11)

where \(P_{,t}^{i,j}\) is the reversed states-transition matrix. With the above reversed states-transition probability, \(m_{t}\) and \(_{0|t}\) as input, the transition sample module can transit a portion of pixels to the fine state at each timestep, thereby correcting erroneous predictions.

InferenceGiven a coarse mask \(m_{T}\) and its corresponding image \(I\), we first initialize that all pixels are in the coarse state thus \(x_{T}^{i,j}=\). We iterate between: (1) forward pass to obtain \(_{0|t}\) and \(p_{}(_{0}|t)\) (Eq. (8)); (2) compute the reversed states-transition matrix \(P_{,t}^{i,j}\) and obtain \(x_{t-1}\) (Eq. (11)); (3) compute the refined mask \(m_{t-1}\) based on \(x_{t-1}\), \(m_{t}\) and \(_{0|t}\). The process (1)-(3) is iterated until we obtain fine mask \(m_{0}\). Visualization examples of inference are shown in Fig. 3.

## 4 Experiments

### Implementation Details

Model ArchitectureFollowing , we employ U-Net for our denoising network. We modify the U-Net to take in 4-channel input (concatenation of image and the corresponding mask \(m_{t}\)) and output a 1-channel refined mask. Both input and output resolution is set to 256\(\)256. All others remain unchanged other than the aforementioned modifications.

Objective FunctionFollowing , we employ a combination of binary cross-entropy loss and texture loss for training our model, \(=_{bce}+_{texture}\), where texture loss is characterized as an L1 loss between the segmentation gradient magnitudes of the predicted mask and the ground truth mask. \(\) is set to 5 to balance the magnitude of both losses.

Noise ScheduleTheoretically, the unidirectional property of SegRefiner ensures that any noise schedule can make the forward process converge to the coarse mask given an infinite number of timesteps. However, in practice, we use much fewer timesteps (\(T=6\) this work) to ensure efficient inference. We designate \(_{T}=0\) such that \(x_{T}=\) for all pixels and \(m_{T}=M_{coarse}\) (Eq. (7)). Following DDIM , we directly set a linear noise schedule from 0.8 to 0 on \(_{t}\).

Training StrategyOur SegRefiner model has been developed into two versions for refinement of different resolution images: a low-resolution variant (hereafter referred to as _LR-SegRefiner_) and a high-resolution variant (hereafter referred to as _HR-SegRefiner_). While these two versions employ different training datasets, all other settings remain consistent. _LR-SegRefiner_ is trained on the LVIS dataset , whereas _HR-SegRefiner_ is trained on a composite dataset merged from two high-resolution datasets, DIS5K  and ThinObject-5K . These datasets were chosen due to their highly accurate pixel-level annotations, thereby facilitating the training of our model to capture fine details more effectively. Following , the coarse masks used for training are obtained through various morphological operations, such as randomly perturbing some edge points of the ground truth and performing dilation, erosion, _etc._ During training, we first train _LR-SegRefiner_ on the low-resolution dataset until convergence. Subsequently, it is fine-tuned on the high-resolution dataset to yield _HR-SegRefiner_.

We employed _double random crop_ as the primary data augmentation technique. It entails the random cropping of each image to generate two distinct model inputs: one encapsulating the entire foreground target, and the other containing only a partial foreground. Both crops are subsequently resized to match the model's input size. This operation ensures that the model is proficient in refining both entire objects and incomplete local patches, a capability that proved instrumental in subsequent experiments. All the following experiments were conducted on 8 NVIDIA RTX 3090. For more details on the training process, please refer to the supplemental materials.

### Semantic Segmentation

Dataset and MetricsAs the refinement task emphasizes the enhancement of mask quality, a dataset with high annotation quality and sufficient detailed information is required to evaluate the model's performance. Hence, we report the results on BIG dataset , a semantic segmentation dataset specifically designed for high-resolution images. With resolutions ranging from \(2048 1600\) to \(5000 3600\), this dataset provides a challenging testbed for evaluating refinement methods. The metrics we used are the standard segmentation metric IoU and the boundary metric mBA (mean Boundary Accuracy ), which is commonly used in previous refinement works.

SettingsBecause of the high resolution of images in BIG dataset, we employ the _HR-SegRefiner_ in this experiment. While the model's output size is only \(256 256\), which is insufficient for such a high-resolution dataset and results in the loss of many edge details. Consequently, during inference, we deploy the first \(T-1\) timesteps to perform global refinement, which takes the resized entire image as input, and the final timestep as a local step that takes original-size local patches as input. In order to identify the local patches that require refinement, we filter out pixels with low state-transition probabilities from the globally refined mask and use them as the center points for the local patches. We allocate more timesteps for global refinement since global steps need to handle more severe errors, thus presenting a higher difficulty level. The training strategy we have employed ensures that the model can adapt to both global and local input without modification.

ResultsAs shown in Tab. 1, we compare the proposed SegRefiner with three model-agnostic semantic segmentation refinement methods, SegFix , CascadePSP , and CRM . Additionally, we include the fine-grained matting method, MGMatting , which employs an image and mask for matting and can also be utilized for refinement purposes. The proposed SegRefiner demonstrates superior performance over previous methods when using coarse masks from four different semantic segmentation models, as evident in both IoU and mBA metrics. Notably, our SegRefiner outperforms CRM, which is specifically designed for ultra-high-resolution images, showcasing significant advancements. We report the error bar (\(\) in gray) in this experiment due to the relatively small size of the BIG dataset (consisting of 100 testing images). The error bar represents the maximum fluctuation value among the results of five experiments, and the results in Tab. 1 are the average of these five trials. In subsequent experiments, the stability of the results is ensured by the availability of an ample number of testing images.

### Instance Segmentation

Dataset and MetricsTo evaluate the effectiveness of our SegRefiner in refining instance segmentation, we select the widely-used COCO dataset  with LVIS  annotations. LVIS annotations offer superior quality and more detailed structures compared to the original COCO annotations and other commonly used instance segmentation datasets such as Cityscapes  and Pascal VOC . This makes LVIS annotations more suitable for assessing the performance of refinement models. The evaluation metrics are the Mask AP and Boundary AP. It worth noting that these metrics are computed using the LVIS  annotations on the COCO validation set. The Boundary AP metric, introduced by Cheng _et al._, is a valuable evaluation metric that measures the boundary quality of the predicted masks and is highly sensitive to the accuracy of edge prediction. It provides a detailed assessment of how well the refined masks capture the boundaries of the objects.

  IoU/mBA & Coarse Mask & SegFix  & MGMatting  & CascadePSP  & CRM  & **SegRefiner** (ours) \\  FCN-8s  & 72.39 / 53.63 & 72.67 / 55.21 & 72.31 / 57.32 & 77.87 / 67.04 & 79.62 / 69.47 & **86.95\({}_{}\)** / **72.81\({}_{}\)** \\ DeepLab V3+5  & 89.42 / 60.25 & 89.95 / 63.41 & 90.49 / 67.48 & 92.23 / 74.59 & 91.84 / 74.96 & **94.86\({}_{}\)** / **77.64\({}_{}\)** \\ RefineNet  & 90.20 / 62.03 & 90.73 / 65.95 & 90.98 / 68.40 & 92.79 / 74.77 & 92.89 / 75.50 & **95.12\({}_{}\)** / **76.93\({}_{}\)** \\ PSPNet  & 90.49 / 59.63 & 91.01 / 63.25 & 91.62 / 66.73 & 93.93 / 75.32 & 94.18 / 76.09 & **95.30\({}_{}\)** / **77.746\({}_{}\)** \\  Avg Improve & 0.00 / 0.00 & 0.47 / 3.30 & 0.73 / 6.10 & 3.58 / 14.05 & 4.01 / 15.12 & **7.43** / **17.33** \\  

Table 1: IoU/mBA results on the BIG dataset comparing with other mask refinement methods.

SettingsIn this experiment, we utilize the _LR-SegRefiner_ model. To refine each instance, we extract the bounding box region based on the coarse mask and expand it by 20 pixels on each side. The extracted region is then resized to match the input size of the model. The output size is suitable for instances in the COCO dataset, allowing us to perform instance-level refinement for all timesteps without requiring any local patch refinement.

ResultsFirst, in Tab. 2, we compare the proposed SegRefiner with two model-agnostic instance segmentation refinement methods, BPR  and SegFix . As demonstrated in Tab. 2, our SegRefiner achieves significantly better performance compared to these two methods. The coarse masks utilized in Tab. 2 are obtained from Mask R-CNN to ensure consistency with the original experiments conducted in . This choice allows for a fair and direct comparison with the previous works. Then in Tab. 3, we apply our SegRefiner to other 7 instance segmentation models. Our method yields significant enhancements across models of varying performance levels. Furthermore, when compared to three model-specific instance segmentation refinement models, including PointRend , RefineMask , and Mask TransFiner , our SegRefiner consistently enhances their performance. These results establish SegRefiner as the leading model-agnostic refinement method for instance segmentation.

### Dichotomous Image Segmentation

Dichotomous Image Segmentation (DIS) is a recently introduced task by Qin _et al._, which specifically targets the segmentation of objects with complex texture structures, such as the steel framework of bridge illustrated in Fig. 4. To facilitate research in this area, Qin _et al._ also built the DIS5K dataset, a meticulously annotated collection of 5,470 images with resolutions of 2K and above. The DIS5K dataset, characterized by its abundance of fine-grained structural details, poses rigorous

    &  &  \\  & AP & AP\({}_{90}\) & AP\({}_{5}\) & AP\({}_{S}\) & AP\({}_{M}\) & AP\({}_{L}\) & AP & AP\({}_{90}\) & AP\({}_{5}\) & AP\({}_{S}\) & AP\({}_{M}\) & AP\({}_{L}\) \\  MaskCNN (Res50) & 39.8 & 61.4 & 42.5 & 24.9 & 47.0 & 55.1 & 27.3 & 53.3 & 25.2 & 24.9 & 41.8 & 27.4 \\ + SegFix  & 40.6 & 61.4 & 42.8 & 25.0 & 48.4 & 56.6 & 29.1 & 53.7 & 28.0 & 24.9 & 43.6 & 30.8 \\ + BPR  & 41.0 & 61.4 & 43.1 & 24.8 & 48.5 & 57.8 & 30.4 & 55.2 & 29.5 & 24.7 & 43.8 & 33.7 \\ + **SegRefiner** (ours) & **41.9** & **61.6** & **43.2** & **25.7** & **49.4** & **58.8** & **32.6** & **55.7** & **32.5** & **25.6** & **45.0** & **37.3** \\  MaskCNN (Res101) & 41.6 & 63.3 & 44.4 & 26.5 & 49.5 & 57.8 & 29.0 & 55.2 & 26.7 & 26.3 & 44.4 & 29.8 \\ + SegFix  & 42.2 & **63.4** & 44.7 & 26.5 & 50.9 & 59.1 & 30.6 & 56.1 & 30.0 & 26.3 & 46.0 & 33.1 \\ + BPR  & 42.8 & 63.3 & 45.3 & 26.1 & 51.0 & 60.6 & 32.0 & **57.3** & 31.6 & 25.9 & 46.3 & 36.3 \\ + **SegRefiner** (ours) & **43.6** & 63.3 & **45.2** & **27.4** & **51.4** & **61.6** & **34.1** & 57.2 & **34.9** & **27.2** & **47.1** & **39.9** \\   

Table 2: Comparision with other model-agnostic instance segmentation refinement methods on the COCO val set using LVIS annotations.

Figure 4: Qualitative comparisons with other methods on DIS5K dataset . Our SegRefiner has the capability to capture finer details, allowing it to discern and incorporate more subtle nuances. Please kindly zoom in for a better view. More visual results are provided in the supplemental file.

demands on a model's capability to perceive and capture intricate information. Thus, it serves as a suitable benchmark to evaluate the performance of our refinement method. The evaluation metrics and other settings utilized in this experiment are consistent with those in Sec. 4.2.

ResultsSince the recent introduction of DIS5K, no prior refinement methods have reported results on this dataset to date. Therefore, the main aim of this experiment is to evaluate the transferability of our SegRefiner across various models. As shown in Tab. 4, our SegRefiner is applied to 6 segmentation models. The results consistently demonstrate that our SegRefiner improves the performance of each segmentation model in terms of both IoU and mBA. In Fig. 4, we provide qualitative comparisons with previous methods, which reveal the superior ability of our SegRefiner to capture fine-grained details, such as the dense and fine mesh of the chair.

### Ablation Study

We conduct ablation studies on the BIG dataset  with high-resolution images. We first investigate the effectiveness of the diffusion process in SegRefiner. The results are reported in Tab. 4(a), where the term "none" refers to the utilization of a U-Net without multi-step iteration, and "w/o diffusion" denotes the direct utilization of the previous step's mask as the input for the subsequent iteration, without employing the diffusion sampling process, _i.e._, discarding the transition sample module. Since there is no states-transition probability in non-diffusion methods, we use a sliding window strategy similar to  in this experiment to obtain the input for local refinement. The results in Tab. 4(a) demonstrate that the diffusion-based iterative process achieves the best performance, validating the effectiveness of the diffusion process in SegRefiner.

In Tab. 4(b) and Tab. 4(c), we evaluate various alternatives used in prior experiments. Tab. 4(b) shows the analysis of global and local refinement used in high resolution images. As can be seen, global refinement significantly improves IoU; however, for high-resolution images, the resulting smaller output size leads to a lower mBA. Local refinement applied to local patches of the original size greatly improves mBA, while its enhancement in IoU is less significant due to the absence of global information. The combination of local and global refinement achieves better performance in both IoU and mBA. Tab. 4(c) presents the results corresponding to different input image sizes. Considering computational load and memory usage, \(256 256\) is selected as the default setting, which performs well without introducing too much computational overhead.

    &  & U-Net  &  &  &  &  &  &  \\  & & w/o & w/o & w/o & w/o & w/o & w/o & w/o & w/o & w/o & w/o & w/ \\   & IoU & 54.77 & 58.73\(\)0.36 & 26.00 & 44.24 & 56.41 & 60.41 \(\)0.40 & 56.47 & 61.38 \(\)0.41 & 61.02 & 64.43\(\)0.44 & 57.10 & 62.27 \(\)1.47 \\  & mBA & 69.84 & 57.44\(\)5.6 & 63.69 & 74.83\(\)11.4 & 62.78 & 74.84 \(\)12.6 & 66.47 & 75.55 \(\)0.68 & 68.94 & 76.62 \(\)1.70 & 74.13 & 66.64 \(\)1.53 \\   & IoU & 44.11 & 47.05\(\)2.94 & 46.97 & 49.01 & 44.24 & 43.75 & 40.10 \(\)0.42 & 46.22 & 49.74 & 50.58 & 52.97 \(\)0.59 & 54.17 & 57.00 \(\)**43.63** \\  & mBA & 70.13 & 74.94\(\)0.48 & 63.13 & 47.64 & 46.45 & 47.87 \(\)0.31 & 67.19 & 57.01 \(\)0.52 & 69.34 & 35.62 \(\)0.48 & 73.53 \(\)**51.48** \\   & IoU & 54.39 & 58.10\(\)3.71 & 57.04 & 62.36\(\)3.22 & 57.71 & 60.25\(\)1.54 & 56.50 & 63.03 \(\)0.33 & 61.22 & 64.70 \(\)**5.40** & 67.05 & 67.84 \(\)**49.79** \\  & mBA & 69.99 & 57.57\(\)6.64 & 64.75 & 50.64\(\)6.31 & 67.50 & 66.55 \(\)1.68 & 66.75 & 75.57 \(\)0.45 & 69.50 & 76.64 \(\)**51.34** & 70.63 & 76.55 \(\)**51.35** \\   & IoU & 59.29 & 63.61\(\)4.05 & 66.34 & 59.50 & 59.90 & 66.04 \(\)**50.53** & 65.93 & 64.20 \(\)**57.47** & 64.43 & 67.57 \(\)**51.49** & 69.74 \(\)**59.83** \\  & mBA & 70.59 & 76.68\(\)4.09 & 64.00 & 76.96\(\)**50.26** & 62.74 & 76.05 \(\)**53.31** & 66.73 & 76.55 \(\)**50.52** & 69.48 & 77.60 \(\)**51.21** & 74.65 & 77.39 \(\)**52.34** \\   & IoU & 61.14 & 66.29\(\)4.58 & 59.11 & 66.63\(\)**52.57** & 57.61 & 66.64\(\)**50.53** & 66.53 & 67.01 \(\)**56.08** & 64.67 & 70.47 \(\)**59.80** & 70.12 & 72.21 \(\)**4.09** \\  & mBA & 70.68 & 77.07\(\)6.09 & 62.78 & 76.43\(\)**10.46** & 61.88 & 76.43\(\)**10.66** & 66.42 & 77.05 \(\)**49.64** & 68.09 & 77.43 & 77.66 \(\)**51.31** \\   

Table 4: Transfering our SegRefiner to DIS task. SegRefiner is employed to refine the segmentation results from 6 different models.

    & &  &  \\ Method & AP & AP\({}_{S}\) & AP\({}_{W}\) & AP\({}_{L}\) & AP & AP\({}_{S}\) & AP\({}_{W}\) & AP\({}_{L}\) \\  PointRend  & 41.5 & 25.1 & 49.0 & 59.3 & 30.6 & 25.0 & 44.2 & 34.1 \\ + SegRefiner & 42.8 \(\)1.3 & 25.9 \(\)0.8 & 50.41 \(\)1.46 & 61.3\(\)2.80 & 33.7\(\)1.1 & 25.8 \(\)0.8 & 46.2 \(\)0.2 & 40.1 \(\)**6.0** \\  RefInNetMask  & 41.2 & 24.0 & 48.1 & 59.2 & 30.5 & 23.8 & 43.5 & 34.1 \\ + SegRefiner & 41.9 \(\)0.7 & 24.6 \(\)0.6 & 48.7 \(\)0.6 & 60.7 \(\)1.5 & 33.0 \(\)0.5 & 25.4 \(\)**5.47** & 44.7 \(\)2.9 & 39.4 \(\)**6.3** \\  Mask Transfiner  & 42.2 & 25.9 & 49.0 & 60.1 & 31.6 & 25.8 & 44.5 & 35.8 \\ + SegRefiner & 43.3 \(\)1.4 & 26.8 \(\)0.9 & 49.9 \(\)0.3 & 62.0 \(\)1.9 & 34.4 \(\)**5.26** & 26.6 \(\)**5.9 & 44.1 & 43.3 \(\)**5.8** \\  SOLO  & 37.4 & 19.3 & 45.5 & 56.6 & 24.7 & 19.0 & 39.3 & 27.8 \\ + SegRefiner & 40.5 \(\)1.1 & 21.7 \(\)0.4 & 49.3 \(\)0.3 & 60.9 \(

## 5 Conclusion and Discussion

We propose SegRefiner, which is the first diffusion-based image segmentation refinement method with a new designed discrete diffusion process. SegRefiner performs model-agnostic segmentation refinement and achieves strong empirical results in refinement of various segmentation tasks.

While SegRefiner has achieved significant improvements in accuracy, **one limitation** lies in that the diffusion process leads to slowdown of the inference due to the multi-step iterative strategy. As shown in Tab. 6, we conduct an experiment on instance segmentation about the relationship between the model's accuracy, computational complexity, time consumption (the average time consumed per image), and the number of iteration steps. It can be observed that, while the iterative strategy has provided SegRefiner with a noticeable improvement in accuracy, it has also introduced a linear increase with the number of steps in both time consumption and computational complexity. As the first work applying diffusion models to the refinement task, the proposed SegRefiner primarily concentrates on devising a suitable diffusion process for general refinement tasks. While improving the efficiency of diffusion models will be a crucial research direction in the future, not only in the field of image generation but also in other domains where diffusion models are applied.