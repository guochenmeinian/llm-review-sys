# Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search

Indradyumna Roy\({}^{}\)  Rishi Agarwal\({}^{}\)

Soumen Chakrabarti\({}^{}\)  Anirban Dasgupta\({}^{}\)  Abir De\({}^{}\)

\({}^{}\)IIT Bombay, \({}^{}\)IIT Gandhinagar

{indraroy15, rishiagarwal18, soumen, abir}@cse.iitb.ac.in

anirbandg@cse.iitgn.ac.in

###### Abstract

In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by embedded representations, thereby extending set containment to _soft_ set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise _vector dominance_. This 0/1 property can be relaxed to an asymmetric _hinge distance_ for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed _dominance similarity_ measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.

## 1 Introduction

Consider a corpus \(X\) of sets \(x\) (which we call 'documents') over some universe of discrete items, and let \(q\) be a query which is also a subset of this universe. We wish to retrieve those \(x X\) which satisfy \(q x\). In most real-world applications, the items in the universe are not just opaque IDs, but are embedded in a rich feature space, demanding that the definition of "\(q x\)" be generalized suitably.

We formalize the notion of _soft set containment_ by writing \(q=\{q_{i}\}\) and \(x=\{x_{i}\}\) and the corresponding sets of item embeddings as \(\{_{i}\}\) and \(\{_{i}\}\). If \(q,x\) are sentences, \(_{i},_{i}\) may be per-word contextual embeddings output from a transformer. If \(q,x\) are graphs, \(_{i},_{i}\) may be contextual node embeddings, such as those output by a Graph Neural Network (GNN). These set-of-vector representations of \(q\) and \(x\) are generally of variable sizes. A suitable set encoding gadget, such as simple pooling  or a trainable Deep Set  or Set Transformer  network, converts them to fixed-size vectors given by \(=(\{_{i}\})\) and \(=(\{_{i}\})\), with \(,^{K}\). Several applications  then use the test "\(\)" (elementwise vector dominance) as a surrogate for testing if \(q x\).

To convert the Boolean test for vector dominance, \(\), into a graded score suitable for ranking (and backpropagation), these applications [52; 26; 10; 31] use a form of (asymmetric) **hinge distance**

\[d(q,x)=[-]_{+}_{1}=_{k}\{0,[k]- [k]\}.\] (1)

\(d(q,x)=0\) when \(\) holds elementwise, and measures the extent of the constraint violation otherwise. A search system must retrieve the top-\(\) documents \(x\) with the smallest \(d(q,x)\), given query \(q\). Several example applications that fit into this framework are elaborated in Appendix B. Even if an application does not fit (1) exactly, our technique may help address other asymmetric distances.

Our goalWhen corpus \(X\) is large, it is impractical to evaluate (1) for each document \(x\). Our goal is to retrieve these \(\) documents without explicitly evaluating \(d(q,x)\) for all \(x X\), within query time that scales slowly with \(|X|\). To achieve this, we design an asymmetric Locality Sensitive Hashing (ALSH) method tailored for hinge distance (1), which then immediately addresses soft set-containment based search.

Prior work and their limitationsWhen set elements are represented by atomic IDs, Bloom filters  and maximum inner product search (MIPS) can be used to find the best \(\) corpus items that are closest to being supersets [46; 59; 45; 2]. However, these techniques are designed specifically for items with opaque IDs, rather than contextual embeddings. LSH [7; 53; 17; 19; 1] has been established as a standard technique for fast approximate near-neighbor search (e.g., FAISS, DPR) in the space of contextual embeddings. However, they predominantly work for symmetric notions of relevance, such as Jaccard similarity, dot product, cosine similarity, or Hamming distance, rather than asymmetric distances like (1). Neyshabur and Srebro  propose a LSH suited for asymmetric relevance (ALSH), but it does not provide a satisfactory solution for (1), as our experiments show.

### Our contributions

Responding to the above motivations, we present FourierHashNet, a new LSH for hinge distance-based asymmetric distance measures. Specifically, we make the following contributions.

Scalable hinge distance search for soft set containmentFrom several applications, we distil the strongly-motivated problem of fast top-\(\) retrieval using hinge distance (1), to capture soft set containment. To our knowledge, (A)LSH for hinge distance has not been explored till date.

Transformation of hinge distance to enable ALSH designOne could leverage its shift-invariant property to apply a Fourier transform on the _negative_ distance, express it as the dot product similarity between the corresponding Fourier features and then use Asymmetric LSH (ALSH) . However, as we show in Section 3.1, using the negative distance leads to singularities of the underlying Fourier transform at some points. This in turn does not allow us to design an LSH for such measure. We circumvent this problem by a suitable transformation of hinge distance to a **dominance similarity**, whose Fourier transform is absolutely convergent.

Design of Fourier featuresNext, we propose a novel method of lifting the dense vectors to frequency domain, such that the dominance similarity in the original space can be expressed as the cosine similarity between the infinite dimensional Fourier features. However, our dominance similarity function is _not_ a positive definite kernel. Hence, unlike Rahimi and Recht , we cannot apply Bochner theorem  to obtain finite dimensional Fourier features. Instead, we first scale the Fourier features with a sinc function and then obtain finite dimensional Fourier features via importance sampling.

Trainable hashcode designThe cosine similarity between the sampled Fourier features is the unbiased estimate of our dominance similarity measure. This allows the use of conventional random hyperplane LSH. However, such an LSH is not guided by the underlying data distribution. To mitigate this limitation, we compute the hashcodes by feeding the Fourier features into a trainable neural hashing network. Prior approaches [54; 15] to trainable hashing encourage bucket balance over the entire corpus, regardless of the query workload. However, this approach is not optimal if most corpus items are irrelevant for most queries, as is usually the case. We propose a new loss function that encourages the best-match hash bucket for a query to include relevant documents and exclude irrelevant documents.

ExperimentsWe show, through extensive experiments, that FourierHashNet is more effective than existing LSH schemes, and that both frequency domain representations and the new trainable hashcode contribute to our gains.

## 2 Preliminaries

NotationThroughout, we will use \([K]\) to mean \(\{1,,K\}\) or \(\{0,,K-1\}\) as convenient. We use \(q\) to indicate a query and \(x\) to indicate a corpus 'document'. Their (possibly learnt) representations are denoted by \(,^{K}\). For supervision, \((q,x)\) may come with a binary relevance judgment \((q,x)\{0,1\}\). We have defined a potentially learnable distance \(d(q,x)\) -- a computable surrogate for \((q,x)\) -- above in Eqn. (1). One can define a similarity measure \((q,x)\) by applying a monotonically decreasing function on the distance \(d(q,x)\). We define \(=\) and denote the set of corpus items as \(X=\{x_{1},x_{2},...,x_{N}\}\). We indicate the domain of query and corpus items as \(\) and \(\) respectively. Given a function \(s(t)\), its Fourier transform is the function \(S:\) which satisfies \(s(t)=_{-}^{}S()e^{ t}d\), where \(\) is the frequency variable and \(S()=_{-}^{}s(t)e^{- t}dt\). For a vector \(\) or \(^{K}\), the Fourier transform is synthesized using a frequency vector \(^{K}\) of same dimension as \(\) or \(\). Here, a function \(s()\) can be expanded as \(s()=_{^{K}}S()e^{-^{}}d\).

### Locality sensitive hashing

Indexing corpus itemsGiven a set of corpus items \(X=\{x_{1},x_{2},...,x_{N}\}\), an LSH will hash each item \(x_{i}\), \(L\) times, which is called the number of _trials_. For each trial \([L]\), it prepares \(B\)_buckets_, which are indexed as the pair \((,b)\) with \([L]\) and \(b[B]\). In the context of LSH, we draw \(L\) independent samples of hash functions \(h^{()}\) from a single hash family \(\), such that \(h^{()}:^{K}[B]\). A corpus item \(x\) is inserted in the bucket indexed \((,h^{()}())\), for each \([L]\).

Symmetric LSHGiven a query \(q\), a symmetric LSH computes bucket indices \((,h^{()}())\) for all \(L\) using the _same_ hash functions \(h^{()}\) used for indexing the corpus. Only those items \(x\) that are in bucket \((,h^{()}())\) are considered as _candidates_; overall, the candidates are in the union of these buckets. In the rest of the paper, we will describe retrieval for one bucket under one trial, with the understanding that \(L\) buckets will contribute candidates. An LSH exists if the query and corpus items are hashed in the same bucket with high (low) probability as long as their similarities are high (low). Formally, we define symmetric LSH as follows.

**Definition 2.1** (Symmetric Locality Sensitive Hashing (LSH)).: Given a domain of queries \(\) and corpus \(\) with \(,\) and a similarity measure \(:\). A distribution over mappings \(:\) is said to be a \((S_{0},cS_{0},p_{1},p_{2})\)-LSH for the similarity function \(()\) if for all \(q\) and \(x\) we have, with \(p_{1}>p_{2}\) and \(c<1\),

* if \((q,x) S_{0}\), then \(_{h}[h(q)=h(x)] p_{1}\)
* if \((q,x) cS_{0}\), then \(_{h}[h(q)=h(x)] p_{2}\).

The hash family \(\) is tailored to the specific choice of similarity function \((q,x)\) (equivalently, the distance \(d(q,x)\)). When \(,^{K}\) and \((q,x)=(,)\), the choice of \(\) corresponds to the uncountable set of all hyperplanes in \(K\) dimensions passing through the origin . When \((q,x)\) is the Jaccard similarity \(|q x|/|q x|\), \(\) is the space of _minwise independent_ hash functions .

Asymmetric LSH (ALSH)In many applications, like the current setup (1), we have asymmetric similarity where \((q,x)(x,q)\). In such cases, we employ two different hash families \(\) and \(\) to determine the bucket of query and corpus respectively. Formally, we define ALSH as follows:

**Definition 2.2** (Asymmetric Locality Sensitive Hashing (ALSH) ).: An asymmetric LSH is \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for a similarity function \((,)\) over \(\), \(\) if we have two different distributions over mappings \(\) and \(\) such that, with \(p_{1}>p_{2}\) and \(c<1\),

* if \((q,x) S_{0}\) then \(_{g,h}[g(q)=h(x)] p_{1}\)
* if \((q,x) cS_{0}\) then \(_{g,h}[g(q)=h(x)] p_{2}\).

As an example, given \(\|\| 1\), consider \((q,x)=^{}/\|\|_{2}\), which can be re-written as \(((),())\), where \(()=[0;/\|\|_{2}],()=[\|_{2} ^{2}},]\). Thus, we can apply random hyperplane hash on both \(()\) and \(()\) to construct \(g(q)=(())\) and \(h(x)=(())\) with \((,)\). If \(\|\|\) is unbounded, no ALSH exists for \((q,x)=^{}/\|\|_{2}\). In \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH, retrieval of items with similarity score more than \(S_{0}\) out of a database of items having a similarity score less than \(cS_{0}\) will admit time-complexity \(O(n^{} n)\) and space complexity \(O(n^{1+})\) where \(= p_{1}/ p_{2}\).

### Problem statement

Given the set of training queries \(Q\) and corpus \(X\), with supervised relevance scores \((q,x)\{0,1\}\) and the surrogate score \(d(q,x)\) defined in Eq. (1), we aim to design an LSH of the distance \(d(q,x)\) which can efficiently retrieve top-\(\) corpus items for any new query \(q^{}\).

Why are existing methods not suitable?As we discussed in Section 2.1, relevance metrics for popular LSHs are mostly symmetric, _e.g._, cosine, dot-product, and Jaccard similarity. In particular, Jaccard similarity, although commonly used in set-related applications, is not suitable for our problem, where we define \((q,x)=1\) when \(q x\) and \(0\) otherwise -- it is possible that there exists a higher overlap between \(q\) and \(x\) when \(q x\), and a lower overlap when \(q x\). E.g., suppose \(q=\{a,b\}\), \(x_{1}=\{a,b,c,d,e\}\), and \(x_{2}=\{b\}\). Here, \((q,x_{1})=1\) and \((q,x_{2})=0\). However, the Jaccard similarity \(J(q,x)\) is not able to reflect the order of \((q,x)\) since \(J(q,x_{1})=2/5<J(q,x_{2})=1/2\).

As discovered by Charikar [9, Lemma 1], the similarity functions in symmetric LSH are inversely related to a _metric_, which must satisfy symmetry and triangle inequality. Although a query normalized dot product similarity appears asymmetric, it can be expressed using cosine similarity. This readily allows us to use a random hyperplane based (asymmetric) LSH. In contrast, it is not immediately apparent how to find such a connection for our asymmetric hinge distance (1).

## 3 FourierHashNet: A new ALSH for hinge distance search

Overview of our approachWe design an ALSH for \(d(q,x)\) in three steps. In the first step, we construct a suitable dominance similarity function \((q,x)\) from \(d(q,x)\) in such a way that there exists a probability distribution \(p:^{K}\) and bounded Fourier representations \(_{q}()\) and \(_{x}()\) of both query \(q\) and corpus items \(x\) such that

\[(q,x)=_{^{K}}_{q}()^{}_{x}()p()d=_{  p()}[_{q}()^{}_{x}( )]\] (2)

In the second step, we approximate the expected value of the \(_{q}()^{}_{x}()\) using a finite sample of Fourier features. This allows us to apply random hyperplane LSH, similar to asymmetric dot product LSH. However, these hyperplanes are drawn from an isotropic Gaussian distribution in a data-oblivious manner, which results in suboptimal bucket distribution in terms of accuracy-efficiency trade off. To tackle this issue, in the third step, we train the random hyperplanes \(\) which takes the Fourier features as input and give (soft) binary hashcodes, which are optimized to effectively trade off between accuracy and efficiency. Next, we provide the details of the above three steps.

### Design of dominance similarity function \((q,x)\) from hinge distance

Limitations of simple choices of dominance similarity function \((q,x)\)A dominance similarity function \((q,x)\) is inversely related to the hinge distance \(d(q,x)\). Chierichetti and Kumar  characterized that, any function of a similarity measure is LSHable, if and only if this function is a probability generating function. However, this characterization applies only to symmetric LSH and no such guiding principle is available for an ALSH. In this context, one can experiment with simple designs of \(\) that are inversely related to \(d\). An immediate choice is \((q,x)=-d(q,x)\)

[MISSING_PAGE_FAIL:5]

transformation of these kernels are probability distributions. However, in Eq. (8), there is no such readily available probability distribution. In response, we attempt to find out a probability distribution \(p()\) which allows us to draw samples using an importance sampling like procedure, as follows:

\[(q,x)=_{ p()}[_{q}( )^{}_{x}()],\;_{q}()=_{q}()}{)}},_{x}()=_{x}() }{)}},\] (9)

Let \(\{^{j}\}_{j=1}^{M} p()\) be \(M\) i.i.d random samples. We compute the Monte Carlo estimate as follows:

\[(q,x)_{j[M]}_{q}(^ {j})^{}_{x}(^{j})(_{q}(^{1..M}),_{x}(^{1..M}))\] (10)

Here, \(_{}(^{1..M})=[_{}(^ {1}),..,_{}(^{M})]\). Note that, as suggested by Eqs. (7) and (9), \(||_{q}(^{1..M})||_{2}=||_{x}(^{1.. M})||_{2}=_{j=1}^{M}_{k=1}^{K}(S(_{k}^{j}) )|+|(S(_{k}^{j}))|}{p(_{k}^{j})}\). Thus, the value is independent of the query or corpus, which leads to the proportionality relation. We choose the probability distribution \(p()\) guided the proportionality constant \(||_{}(^{1..M})||\) and set \(p()=_{k[K]}p()\), where \(p()|(S())|+|(S())|\). However, the integral of these terms may not be bounded. Therefore, we set the support of \(p()\) between \([-_{},_{}]\), thus eliminating the higher frequency terms. The effect on the overall score is small. Attenuation of the higher frequency signals can be seen as a multiplication with a low pass filter in the frequency domain, which affects a convolution in the time domain. Its impact on the similarity score is proportional to \(1/_{}\). This still allows us for ALSH despite frequency truncation.

**Theorem 3.2**.: (Proven in Appendix D) Let \(,^{K}\), \(^{-1}\) be Lipschitz with Lipschitz constant \(L_{}\); the hyperparameter \(T\) in Eq. (4) be chosen such that \(T>||-||_{}\); the frequency sampling distribution \(p(_{k}^{j})[|(S(_{k}^{j}))|+|(S( _{k}^{j}))|]\) with the support set \(_{k}^{j}[-_{},_{}]\) and the proportionality constant \(I(_{})=_{-_{}}^{-_{}}|(S() |+|(S())|d\). Then, the mapping \(g(q)[i]=(_{i}^{}_{q}(^{1..M}))\) and \(h(x)[i]=(_{i}^{}_{x}(^{1..M}))\) where \(_{i} N(0,)\), constitutes a \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for some \(p_{1}\) and \(p_{2}\) if we choose the support set \([-_{},_{}]\) and the number of samples \(M\) as follows:

\[_{}>}{(1-c)S_{0}}(6+,}||-||_{}})\;\;M>[ }{(1-c)S_{0}}]^{2}KI(_{}).\] (11)

Based on the outlined assumptions, the above Theorem guarantees that FourierHashNet is an \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for the asymmetric dominance similarity score, subject to appropriate choices of \(_{}\) to bound the effect of frequency truncation and \(M\) to bound the variance of the Monte Carlo sample estimate.

### Trainable hashing network

Random hyperplane LSHEq. (10) provides an asymmetric transformation on the input query-corpus pair, which maps it into the cosine similarity space, thus allowing for Random Hyperplanes hashing. We sample \(H\) spherically symmetrically distributed normal vectors \(\{_{i}\}_{i=1}^{H}\), _i.e._, \(_{i}(0,)\), each perpendicular to a random hyperplane passing though the origin. For each query \(q\) and the corpus \(x\), we can generate \(H\)-bit hashcodes \(g(q),h(x)\{ 1\}^{H}\) from the Fourier features (10) as follows: \(g(q)[i]=(_{i}^{}_{q}(^{1..M}))\) and \(h(x)[i]=(_{i}^{}_{x}(^{1..M}))\). Consequently, we can index the given corpus with \(N\) items, into a hash table with \(2^{H}\) buckets. For each query \(q\), we restrict our search within bucket \(b=g(q)\). If the corpus items are uniformly distributed across all buckets, then it enables sub-quadratic time retrieval with \(N/2^{H}\) comparisons (per trial).

Data driven hashcode generationThe above random hyperplane LSH approach suffers from two distinct limitations: (1) the quality of Monte Carlo approximation obtained in Eq. (10), depends on the suitability of \(p()\), and (2) the hyperplanes are data oblivious. Data oblivious hyperplanes provide the best efficiency if the corpus embeddings are uniformly spread over the \(K\) dimensional sphere, which allows the random hyperplanes to evenly allocate the corpus items across different hashcodes. However, in practice, the spatial distribution of the embeddings is not uniform. This results in a skewed distribution of the corpus items across the hash buckets.

To tackle the first problem, we improve the quality of the Fourier features through a trainable nonlinear transformation. Here, we use two networks \(_{q}\) and \(_{x}\) which takes the Fourier features for the query and corpus, _i.e._, \(_{q}(^{1..M})\) and \(_{x}(^{1..M})\) as input and outputs corresponding transformed Fourier representations \(_{q}=_{q}(_{q}(^{1..M}))\) and \(_{x}=_{x}(_{x}(^{1..M}))\). We train \(_{q}\) and \(_{x}\) by minimizing a BCE loss on \(\{(_{q},_{x}),(q,x)\}\) pairs for \(q Q\) and \(x X\) as follows:

\[_{_{q},_{x}}_{q Q,x X}-[(q,x)(1+(_{q},_{x}))+(1-(q,x)(1-(_{q},_{x}))]\] (12)

Next, we train the random hyperplanes \(=[}_{1},}_{2},..]\) using the transformed Fourier features \(\{_{q}\}\) and \(\{_{x}\}\). The final hashcodes \(g(q)\) and \(h(x)\) are obtained as \(g(q)=(}_{q})\), \(h(x)=(}_{x})\), where \(}\) are the final trained random hyperplanes. For training purposes, we use \(()\) as a smooth surrogate of \(()\). The loss function \((Q,X\,|\,)\) used to train \(\) consists of three components.

(1) Collision minimizerFor any query \(q\), our goal is to ensure that assigned bucket contains only positive items. Assuming corpus items are uniformly distributed across buckets, we ensure that for any query \(q\), the \(N/2^{H}\) most relevant items \(X_{q^{}}\) measured in terms of \(d(q,x)\) will have higher amount of bit overlap than rest of the items \(X_{q^{}}\). Here, \(X_{q^{}}\) and \(X_{q^{}}\) indicate positive and negative silver instances (not gold instances) indicating top \(N/2^{H}\) items _in terms of the (possibly trained) hinge distance \(d(q,x)\)_. We encode this by minimizing the following ranking loss.

\[_{1}=_{q Q}_{x X_{q^{}},x^{} X_{q^{} }}1+(_{q})^{}(_{x^{}})- (_{q})^{}(_{x})_{+}\] (13)

This loss encourages that \((_{q})^{}(_{x})>(_{q})^ {}(_{x^{}})+1\), _i.e._, the number of common bits between \(q\) and \(x X_{q^{}}\) is atleast one more than the same between \(q\) and \(x^{}\).

(2) Fence SittingWe set fence sitting loss as \(_{2}=_{x X}|||(_{x})|-1|||_{1}\). This prevents the optimizer from arriving at a trivial solution by setting all hashcodes to zero.

(3) Bit BalanceWe set the bit balance loss as \(_{3}=_{i[H]}|_{x X}(_{x})[i]|\). This enforces that each position should have an equal number of \(+1\) and \(-1\), thus ensuring that each random hyperplane evenly splits the set of points. Finally, we estimate \(\) by minimizing the loss, with \(_{}\) as hyperparameters such that \(_{i}_{i}=1\), which is given as follows:

\[(Q,X\,|\,)=_{1}_{1}+_{2}_{2}+ _{3}_{3},\] (14)

Algorithm 1 summarizes the overall procedure.

```
1:functionTrain(\(X\),\(\{(q,x)\}_{q Q,x X}\))
2: Draw \(^{1..M} p()\)
3: Compute \(_{q}(^{1..M}),_{x}(^{1..M})\) (Eq. (9))
4: Train \(_{q}\), \(_{x}\) from \((q,x)\), \(_{}(^{1..M})\) (Eq. (12))
5: Train \(\) by minimizing the loss (14)
6:Return \(_{x},_{q},}\)
7:
8:functionIndex\(\{_{x}((^{1..M}))_{x X}\}\)
9:Require: Trained networks \(_{x},}\)
10:\(h(x)(}_{x}(_{x}(^{1..M})))\)\( x X\)
11:for\(x X\)do
12: hash \(x\) to bucket \(b=h(x)\)
13:Return the bucket sets \(B\)
14:functionRetrie(\(q^{}\))
15:Require: Trained networks \(_{q},}\)
16: Compute \(_{q}(^{1..M})\) based on \(^{}\)
17:\(g(q^{})(}_{q}(_{q}( ^{1..M})))\)
18: Rank all \(x\) in the bucket \(b=g(q^{})\) based on the distance \(d(q^{},x)\) to obtain the list \(_{q^{}}\).
19:Return \(_{q^{}}\) ```

**Algorithm 1** FourierHashNet

Algorithm 1 summarizes the overall procedure.

Difference from existing trainable LSHLSH training has been extensively studied [54; 15; 43], with Fence Sitting and Bit Balance losses being well known. However, the Collision Minimizer loss differs significantly from existing approaches. Current techniques seek to ensure load balance across hash buckets for all corpus items, including the ones that may not be relevant to most queries. This is unnecessary for query workloads which touch upon only a small subset of the corpus to generate the best responses. In contrast, our Collision Minimizer loss ensures that only the top-most bucket for any given query allows relevant items and explicitly denies irrelevant items. Thus, it is informed by the query workload, rather than assuming load balance for all items in the corpus. Such an approach may result in balanced bucket loads, but not necessarily.

## 4 Experiments

In this section, we provide a comprehensive evaluation of our method against several baselines and ablations on four datasets. Appendix F describes additional experiments. Our code is in https://github.com/structlearning/fhashnet.

### Experimental setup

DatasetsWe experiment on datasets sampled from anonymized real-world Web log data, _viz_, MsWeb and MsNbc. MsWeb is generated using logs from _www.microsoft.com_, containingrecords of the areas of the website visited by the users. MsNbc is a collection of logs of user page requests from _msnbc.com_. In both cases, a record (either \(q\) or \(x\)), is a passage that is regarded as a bag of words. Given a collection \(V\) of such word bags, (\(|V|=11234\) for MsWeb and \(|V|=111290\) for MsNbc), we sample \(|Q|=500\) bags from \(V\), designating them as queries, and designate the rest as corpus items \(X=V Q\). Consistent with typical information retrieval application scenarios , we generate gold relevance labels based on (multi)set containment for MsWeb (MsNbc). (Additional methods for evaluation are explored in Appendix F.) We build the query set \(Q\), such that the number of relevant items \(N_{q}=|\{x X:(q,x)=+1\}|\) for each query \(q\). We create four datasets by changing average relevance counts per query, \(_{q}\). They are: (1) MsWeb-1 where \(_{q}=35.624\). (2) MsWeb-2 where \(_{q}=20.392\). (3) MsNbc-1 where \(_{q}=24.09\) (4) MsNbc-2 where \(_{q}=19.78\) The set of queries \(Q\) is partitioned into 20% training set \(Q_{}\), 20% validation set \(Q_{}\) and 60% test set \(Q_{}\).

Design of query and corpus embeddings \(,\)We begin with a pre-trained sentence transformer model  to obtain 768 dimensional dense contextual representations \(}_{q}\) and \(}_{x}\) for the each word in bags \(q\) and \(x\). Embeddings of words belonging to a bag are fed into a deep set  network to obtain a bag representation \(,^{K}\), with \(K=294\) (chosen via hyperparameter sweep). To train the parameters inside the deep set network, we use \(\), \(\) to compute the proposed asymmetric hinge distance \(d(q,x)\) (1), feed it into a trainable sigmoid layer \(\) and minimize

\[_{q,x}((q,x),(-d(q,x)))\] (15)

which uses a BCE loss on the gold relevance labels. Once we obtain \(\) and \(\), we use Algorithm 1 to obtain trained \(_{q}\), \(_{x}\) and \(}\) (\(()\)), which are then used for indexing (\(()\)).

EvaluationGiven a test query \(q Q_{}\) and a set of \(N^{}_{q}\) candidate corpus items, we rank them in increasing order of their hinge distances \(d(q,x)\). Then we evaluate the average precision (AP) for the query and average over queries to report mean average precision (MAP) -- see Appendix E.6.

### Effect of different similarity measures on LSH

SetupHere, we compare FourierHashNet against the three LSH baselines, _viz_, Random hyperplane (RH) , Dot product LSH (DP-RH)  and Weighted MinHash (WMH) , that are tailored towards cosine similarity, dot product similarity and Weighted Jaccard Similarity, respectively. For each LSH method, we train the embeddings \(\), \(\) and the final hashcodes \(g(q)\) and \(h(x)\) using the same networks, as in our method. Furthermore, we set the final relevance measure for ranking to be the similarity score for which the LSH is designed.

ResultsWe vary the mixing hyperparameters \(_{1}\) and \(_{2}\) in our loss (14) and the number of buckets \(B\) to explore the tradeoff between accuracy (MAP) and average query time. In Figure 2, we summarize the results. We observe that: **(1)** FourierHashNet outperforms all the baselines by providing significantly better time-vs.-MAP trade-off across all datasets. In MsWeb datasets, all the baselines except DP-RH show poor performance. All baselines perform poorly for the MsNbc dataset. We remark that cosine similarity, dot product or weighted Jaccard similarity are not suited for vector dominance search. Therefore, the maximum possible MAP obtained by them are severely constrained. **(2)** In MsWeb datasets, DP-RH performs moderately, by achieving a MAP value around 0.4-0.42 within 0.03 seconds (average query time). This is because dot product can be computed significantly faster than all the other distance/similarity measures. In particular, it is \(\)\(7.5\) faster than our hinge distance (1), \(\)\(10.3\) faster than cosine, and \(\)\(5.1\) faster than Jaccard similarity.

Figure 2: Effect of different similarity measures on LSH, measured in terms of variation of MAP vs. average query time (in sec) for all methods. Here, the final score used for ranking the relevant items is that similarity score for which the LSH is designed for.

### Comparison against other efficient indexing techniques

In Section 4.2, we used the similarity score corresponding to each LSH method for final candidate ranking. The baselines performed poorly, which may result from a poor choice of final similarity score or the indexing method. Here, we evaluate FourierHashNet against baseline indexing methods applied on hinge distance guided embeddings. We consider LSH and Inverted File Indexing (IVF) based indexing variants in this section, and discuss graph-based indexing methods in Appendix F.

**Comparison with LSH based indexing**. Shrivastava and Li  showed that an LSH not tailored to the final scoring function may still provide an effective filter. Accordingly, we set the final similarity function to be dominance similarity, and compare against four possible LSH baselines.

Given the embeddings \(,\) trained (15) using hinge distance, we feed them into the four baselines, each of which trains a hashing network in a different way. **(1)** RH+Hinge: We train a set of random hyperplanes represented by \(\) and compute the hashcodes as \(h(q)=()\) and \(h(x)=()\). **(2)** DP-RH+Hinge: We train random hyperplanes \(\) for these embeddings to compute the hashcodes as \(g(q)=([0,/||||])\) and \(h(x)=([-||||^{2}},])\). **(3)** WMH+Hinge: We use the best performing WMH implementation from DrHash toolkit  to obtain the hashcodes. **(4)** FLORA: We train asymmetric hash networks (net\({}_{1}\), net\({}_{2}\)) using an end-to-end data-driven approach, which minimizes bit balance and decorrelation loss, along with a consistency loss which predicts the final similarity score using \((_{1}(),_{2}())\).

Figure 3 compares the performance of FourierHashNet, RH+Hinge, DP-RH+Hinge, WMH+Hinge and FLORA in terms of MAP for MsWeb and MsNbc datasets. Here we analyze the section of the trade-off curve which provides \(\)10X speedup compared to exhaustive search. The complete tradeoff curve is provided in Appendix F. **(1)** The newly designed baselines are now seen to perform significantly better than those used in the previous experiments with Figure 2. However FourierHashNet still outperforms all the baselines. **(2)** RH+Hinge, despite achieving the second highest scores in many cases, is seen to suffer from a large variance in performance within any given time budget. This would make it difficult to tune the hyperparameters to achieve the requisite performance v/s retrieval speed trade-off. **(3)** DP-RH+Hinge is seen to have a significantly worse performance than FourierHashNet everywhere. This indicates that DP-RH is ill-suited to asymmetric hinge distance based retrieval.**(4)** We observe that for the same amount of query time invested, FLORA's MAP can lag ours by over 10%, particularly when faster average query times are required. FLORA's hyperparameter tuning is also more delicate, with there being unsuccessful settings (where MAP grows very slowly with query time) very close to relatively successful ones.

**Comparison with IVF indexing** We use the widely used FAISS-IVF  library, which supports IVF indexing based on L2 distance (IVF-L2) and Inner Product similarity (IVF-IP). Additionally, we propose an alternative Fourier+IVF+IP, where we apply Fourier transformation on the input embeddings, before using IVF-IP. We provide embeddings \(,\), trained using hinge distance to all the methods, and use hinge distance to rank retrieved items.

Figure 4 compares the performances in terms of MAP, across all datasets. We observe that: **(1)** FourierHashNet outperforms both IVF-L2 and IVF-IP across all datasets. FAISS-IVF retrieval suffers because its quantizers, that assign vectors to the Voronoi cells, rely on a metric like L2 or IP, which are unsuitable for asymmetric hinge distance. **(2)** Fourier transformation provides a significant boost in performance across all datasets, as seen while comparing Fourier+IVF+IP against IVF+IP. However, FourierHashNet still outperforms Fourier+IVF+IP, most noticeably in MsWeb-2.

Figure 3: Trade-off between query time and accuracy (MAP) for MsWeb and MsNbc datasets where there is \(\)10X speedup compared to exhaustive search. We apply different LSH methods on hinge distance guided embeddings, _viz_, RH+Hinge, DP-RH+Hinge, WMH+Hinge, FLORA and FourierHashNet; and, then use the hinge distance to finally rank the retrieved items.

### Ablation study

Data driven vs data oblivious LSHTo perform ablation study on our proposed hashcode training method, we propose an alternative FHash (untrained). This applies our Fourier features followed by a _data oblivious_ random hyperplane LSH, without any data driven hashcode training.

In Figure 5, we compare the complete design of our method, _i.e._, FourierHashNet and FHash (untrained) against the untrained versions of RH+Hinge and DP-RH+Hinge. We make the following observations: **(1)** Benef of Fourier transformation: The MAP vs time trade-off curve of FHash (untrained), consistently dominates all the baselines across both datasets. **(2)** Benef of hashcode training: Compared to FHash (untrained), we observe that FourierHashNet allows for significantly more choices of trade-off points, where higher MAP is required.

Ablation on collision minimizerHere, we replace the collision minimizer in loss\((Q,X\,|\,)\) (14) with decorrelation loss which encourages hashcodes to be dissimilar: \(_{1}=_{x y}|(_{x})^{} (_{y})|\), a commonly used loss in prior work [54; 15].

Figure 6 compares the performance of the two variations of the losses in terms of MAP, for MsWeb datasets. We observe that: **(1)** Our loss containing the collision minimizer term performs better than its variant which uses the decorrelation loss. In MsWeb-2, latter provides a MAP of \(0.4\) in \(0.014\) secs, which our loss achieves in 50% of the time. **(2)** Our method allows for greater freedom in navigating the performance vs average query time trade-off, as seen in MsWeb-2, as it is more spread out across the time axis.

## 5 Conclusion

We have presented FourierHashNet, an ALSH for asymmetric hinge distance, strongly motivated by text, image and graph retrieval applications. By converting hinge distance to a proposed dominance similarity and applying a suitable Fourier transform to the dominance similarity, we can estimate the distance as an inner product over an importance-sampled spectrum, which further enables the use of a trainable LSH in the frequency domain. Experiments show that FourierHashNet dramatically speeds up queries while preserving or improving retrieval accuracy. Our approach can be extended to any shift invariant functions including Chamfer distance, box embeddings, etc. Box embeddings are known to model more complex set operations like set overlap and set difference [42; 13], making them an interesting avenue for future research. One limitation of FourierHashNet compared to simple symmetric LSHs is the increase in computational cost to compute the Fourier transform. One can explore other types of transformations to mitigate this cost.

Figure 4: Trade-off between number of corpus items being evaluated and accuracy (MAP) for MsWeb and MsNbc datasets where there is \(\)10X speedup compared to exhaustive search. We apply FourierHashNet and different IVF methods, _viz_, IVF+L2, IVF+IP, and Fourier+IVF+IP, on hinge distance guided embeddings; and then use the hinge distance to finally rank the retrieved items.

Figure 5: Effect of untrained RH

Figure 6: Collision minimizer vs. decorrelation.