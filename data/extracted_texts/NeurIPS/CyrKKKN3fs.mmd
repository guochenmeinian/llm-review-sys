# FairMedFM: Fairness Benchmarking

for Medical Imaging Foundation Models

 Ruinan Jin\({}^{1,4}\), Zikang Xu\({}^{2}\), Yuan Zhong\({}^{3}\), Qingsong Yao\({}^{2}\), Qi Dou\({}^{3}\), S. Kevin Zhou\({}^{2}\), Xiaoxiao Li\({}^{1,4}\)

\({}^{1}\)The University of British Columbia

\({}^{2}\)University of Science and Technology of China

\({}^{3}\)The Chinese University of Hong Kong

\({}^{4}\)Vector Institute

###### Abstract

The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks - classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Checkout FairMedFM's project page and open-sourced codebase, which supports extendible functionalities and applications as well as inclusive for studies on FMs in medical imaging over the long term.

## 1 Introduction

Foundation Model (FM) facilitated medical image analysis is playing a pivotal role in healthcare . These models, which leverage large-scale pretraining and fine-tuning , have demonstrated remarkable capabilities in various medical imaging tasks, including classification  and segmentation . As the use of FMs proliferates in medical imaging, addressing the challenges of evaluating and ensuring their fairness and utility becomes increasingly critical , where biases in model performance can result in significant disparities in patient care and outcomes.

Creating benchmarks for algorithm fairness in medical imaging can lead to consistent experiment settings and ensure standardization. There are efforts to benchmark fairness algorithms in non-FM-based traditional machine learning for medical imaging . However, the fairness

[MISSING_PAGE_FAIL:2]

2. With FairMedFM, we conducted a thorough analysis from various perspectives, where we found that (1) Bias is prevalent in using FMs for medical imaging tasks, and the fairness-utility trade-off in these tasks is influenced not only by the choice of FMs but also by how they are used; (2) There is significant dataset-aware disparities between SA groups in most FMs; (3) Consistent disparities in SA occur across various FMs on the same dataset; and (4) Existing bias mitigation strategies do not demonstrate strong effectiveness in FM parameter-efficient fine-tuning scenarios.
3. We open-source FairMedFM, an extensible implementation for launching the FMs for medical image analysis, to prompt the study of FMs for medical imaging and fairness evaluation in the community.

The scope of our work is to establish a more comprehensive benchmark for medical imaging, focusing on classification and segmentation tasks, binarized SAs and commonly used FM strategies from the literature. Our objective is to raise awareness of fairness issues within the medical imaging community and assist in developing fair algorithms in the machine learning community, by promoting more accessible and reproducible methods for fairness evaluation.

## 2 Preliminaries on Foundation Models, Medical Imaging, and Fairness

### FMs in Medical Imaging

FMs have recently garnered widespread interest due to their powerful generalization capabilities. These large models are designed to learn from large-scale unsupervised data. In medical applications, FMs are particularly valuable because massive amounts of unlabeled medical data are easier to obtain than labeled data, which requires costly expert annotations. Typically, FMs are pre-trained on broad datasets to acquire medical knowledge using two primary training objectives: recovering masked words or vision patches , and aligning features of paired text and images through contrastive learning . Their pre-trained representations can be successfully applied to various downstream medical tasks with minimal or no reliance on expert labels. In this paper, we focus primarily on classification and segmentation tasks.

Figure 1: Overview of the FairMedFM framework, a standardized pipeline to investigate fairness on diverse datasets (2D, 2.5D, and 3D), comprehensive functionalities (various FMs, tasks, usages, and debias algorithms), thorough evaluation metrics. The details are explained in Sec. 3.

**Classification FMs in Medical Imaging.** Classificaiton FMs vary in architecture, which can be roughly categorized into two groups. The first one is _vision models (VMs)_, which takes images as input and learns a general representation across diverse datasets by different self-supervised proxy tasks. MedMAE  and MedLVM  are trained by masked imaging modeling  and graph matching, respectively; DINov2 , MoCo-CXR , and C2L  decrease the feature disparities of the paired and augmented vision patches. Another group is _vision-language models (VLMs)_ (e.g., CLIP , MedCLIP , PubMedCLIP , BiomedCLIP ), which are trained to attract the feature of paired text and images, while BLIP  design a \(q\)-former for vision-text alignment.

In classification, we follow the common workflow  to consider \(D=(,,)\) to be a set of distributions where we have input \(x\) in space \(\), the disease label \(y\{0,1\}^{C}\) in space \(\), and sensitive attributes \(a\) in space \(\). Let \(f_{v}(): v^{k}\) denote the vision encoder of a foundation model which embeds the inputs into feature \(v\) with dimension \(k\).

**Segmentation FMs in Medical Imaging.** Following the trend of the large model, the Segment Anything Model (SAM)  shows great potential in segmentation task, which is trained on 1B labeled natural data and offers the zero-shot ability to generate segmentations mask only based on point or box prompts as input. In terms of both data structure and context, medical images exhibit significant differences from natural images. First, medical images vary in modalities, including 2D grayscale images (X-ray), 2D RGB images (dermatology), and 3D volumes (CT). Especially for 3D images, MedSAM  processes 3D volumes with slice-wise operations, while SAM-Med3D  is a 3D model trained on massive labeled 3D medical volumes. Furthermore, considering the domain gap between medical and natural images, fine-tuning is necessary for applying Segmentation FMs (SegFMs) (pre-trained on natural images like SAM ) in medical to learn medical context. Similar to classification, we consider \(D=(,,)\), where we have the segmentation mask \(s\{^{w h d}\}^{C}\) in label space \(\).

### Fairness in Medical Imaging

**Defintion of Fairness.** Fairness, a critical aspect of AI ethics, urging that deep learning models should not have skewed outcomes towards personalities with diverse demographics, has been widely studied in computer vision  and natural language processing . Among various fairness definitions, _group fairness_ is the most common one, which ensures that the model's performance is consistent across different groups. Let \(Y,\) be the ground truth label and prediction of the model, respectively, and \(A\{0,1\}\) be the SA. One of the group fairness metrics, Accuracy Parity, is defined as \(P(=Y|A=0)=P(=Y|A=1)\).

**Fairness in Medical Imaging.** Fairness issues _do_ exist in deep learning-based medical image analysis, especially for the marginalized populations. For example, Seyyed-Kalantari _et al_.  find that their Chest X-ray classifier has a higher underdiagnosis rate for Hispanic female patients. Similar phenomenons also occur in other medical tasks, including regression , segmentation , reconstruction , etc.

### FMs Meet Fairness in Medical Imaging

Previous studies have shown that unfairness can be induced to FMs from the pre-training datasets, the fine-tuning process, the application of downstream tasks . However, most of the current studies on fairness in medical imaging mainly focus on the traditional models, while evaluating and mitigating unfairness in FMs is still in its infancy. Khan _et al_.  benchmarked six classification FMs on sex and race. However, the fairness metrics and datasets involved in their study are limited. Therefore, extra efforts is required to evaluate whether these FMs in medical imaging have fair outcomes before applying them in real clinical scenarios.

## 3 FairMedFM

Fig. 1 presents the pipeline of FairMedFM framework, which offers an easy-to-use codebase for benchmarking the fairness of FMs in medical imaging. FairMedFM contains 17 datasets (9 for classification and 8 for segmentation) and 20 FMs (11 for classification and 9 for segmentation). It also integrates 9 fairness metrics (5 for classification and 4 for segmentation) and 6 unfairnessmitigation algorithms (3 for classification and 3 for segmentation), trying to provide a relatively comprehensive benchmark for fairness in medical imaging FMs.

### Datasets

The following 17 publicly avaliable datasets are included in FairMedFM to evaluate fairness in FMs in medical imaging: CheXpert , MIMIC-CXR , HAM10000 (CLS) , FairVLMed10k , GF3300 , PAPILA , BRSET , HAM10000 (SEG) , TUSC , FairSeg , Montgomery County X-ray , KiTS 2023 , CANDI , IRCADb , SPIDER . These datasets vary in the following aspects: (1) _Task type:_ classification and segmentation; (2) _Dimension:_ 2D and 3D; (3) _Modality:_ OCT, X-ray, CT, MRI, Ultrasound, Fundus, OCT, and dermatology; (4) _Body part:_ brain, eyes, skin, thyroid, chest, liver, kidney, and spine; (5) _Number of classes:_ ranging from 2 to 15; (6) _Number of samples:_ ranging from 20 to more than 350k; (7) _Sensitive attribute:_ sex, age, race, preferred language, skin tone, etc. (8) _SA skewness (Male : Female):_ ranging from 0.19 to 1.67. The details of these datasets can be found in the Appendix B.

### Models

**Classification FMs.** Eleven FMs from two categories are used for evaluation: (1) _vision models (VMs)_: C2L , DINOV2 , MedLVM , MedMAE , MoCo-CXR ; (2) _vision-language models (VLMs)_: CLIP , BLIP , BLIP2 , MedCLIP , PubMed-CLIP , BiomedCLIP . For all models, _LP_ is used for fine-tuning; for VLMs, we also conduct _CLIP-ZS_ and _CLIP-Adapt_. Since these FMs are 2D models, we use 2.5D slices for 3D data and report volume-wise results.

FairMedFM evaluate the fairness of FMs under three commonly used protocols for classification:

\(\)_Linear probing (LP)._ A classification head \(h():v\) is trained to map the FM's embedding \(v\) to the prediction \(=h(f_{v}(x))\).

\(\)_Parameter-efficient fine-tuning (PEFT)._ PEFT aims to fine-tune FMs with a classification head to new downstream tasks with minimal computational overhead. FairMedFM evaluate fairness of FMs in the fine-tuning setting with modern PEFT strategies (e.g., LoRA  and pruning).

\(\)_CLIP-ZS and CLIP-Adapt._ Vision-language models offer the zero-shot classification ability for FMs, which compares the similarities of the vision embedding \(f_{v}(x)\) between different class-wise prototypes text embeddings \(f_{t}(x)\) of positive and negative prompts (e.g., "There is no pneumonia.") for each class. We consider both zero-shot _(CLIP-ZS)_ inference as well as a simple adaptation strategy _(CLIP-Adapt)_ which fine-tunes the class prototypes initialized with CLIP zero-shot prototypes.

**Segmentation FMs.** Nine SegFMs are selected from three categories for evaluation: (1) _general-SegFMs:_ SAM , MobileSAM , TinySAM ; (2) _2D Med-SegFMs:_ MedSAM , SAM-Med2D , and FT-SAM ; (3) _3D Med-SegFMs:_ SAM-Med3D , FastSAM3D . All the four segmentation prompts described in Sec. 2.1 are used for 2D SegFMs. we adopt _rand_ and _rands_ for SAM-Med3D and FastSAM, _rands_ and _bbox_ for SegVol following their official implementation.

For SAM-family models, extra point prompts \(p_{}\) or bounding box prompt \(p_{}\) are required in the inference stage. Therefore, FairMedFM examine the fairness of SegFMs with different types of prompts including (1) _center:_ the center point of the mask; (2) _rand:_ 1 random point inside the mask; (3) _rands:_ 5 random points inside the mask; (4) _bbox:_ the bounding box of the mask. These prompts can be generated either directly from the ground truth mask or manually annotated. To thoroughly evaluate the fairness of SegFMs, FairMedFM provides the interface for both 2D and 3D SAM, and the access to fine-tune the SAM on the specific medical dataset with full supervision.

### Unfairness Mitigation Methods

FairMedFM provides popular and generalizable bias mitigation strategies and integrates them with the FMs. Following literature specialized in bias mitigation algorithms , we categorize them into the following:(1) _Group rebalancing_ is a technique used to address bias in datasets by adjusting the representation of different subgroups [21; 48], ensuring that minority groups have equal representation during training. This helps to mitigate biases that can arise from imbalanced datasets. (2) _Adversarial training_ is a method for reducing bias by training models in a way that they learn to make predictions while simultaneously being penalized for recognizing SA [70; 29]. This promotes fairness by minimizing the influence of biased features. (3) _Fairness constraints_ are used to ensure that models are trained to produce fair outcomes for different subgroups. This approach involves adding the differentiable form of fairness metrics to the training objective directly , or adjusting weights of the loss function for different subgroups to penalize the model for making biased predictions . (4) _Subgroup-tailored modeling_ is a method that allows subgroups to have different model parameters, enabling the model to learn different representations for subgroups. This specialized modification can be applied on part of the model, i.e. fairness-aware adaptors , or the entire model . (5) _Domain generalization_ aims to improve a model's ability to perform well across various domains, including those not encountered during training . This approach seeks to create models that generalize better by finding robust solutions that work well in different scenarios .

### Evaluation Metrics Taxonomy

**Utility** refers to the effectiveness of the model in making accurate predictions. Examples include the **Area Under the receiver operating characteristic Curve (AUC)** for classification and **Dice similarity score (DSC)** for segmentation.

**Group fairness** is evaluated from four aspects following common practice : (1) _Outcome-consistency fairness_, which evaluates discrepancies in the model's performance (e.g., accuracy, components of confusion matrix, etc.) between different sensitive groups. In classification, we include **delta AUC (AUC\({}_{}\))**, which is measured as the maximum AUC gap among subgroups; and **Equalized Odds (EqOdds)**, which measures the differences in true positive and false positive rates between advantaged and disadvantaged groups. In segmentation, we include **delta DSC (DSC\({}_{}\))**, which assesses if both groups receive approximately equal predictive performance; and **DSC skewness (DSC\({}_{}\))**, which measures the degree of skewness of DSC between advantaged and disadvantaged groups. (2) _Predictive alignment fairness_, which focuses on the alignment between predicted probabilities and actual outcomes. It ensures that predicted scores accurately reflect true likelihoods, providing a reliable basis for decision-making across different groups. We report the **expected calibration error gap (ECE\({}_{}\))** in classification, where a high value indicates an optimal decision threshold; (3) _Positive-parity fairness_, which ensures that the positive classification rate is equal for both unprivileged and privileged groups, preventing any group from being overlooked. We note that this is an optional evaluative aspect that may not be applicable to all scenarios. For example, positive parity is compromised in diagnosing glaucoma, where morbidity rates differ between males and females; (4) _Representation fairness_, which evaluates fairness from the aspects of feature representation learned in the latent space, by estimating either group-wise feature separability among subgroups. We report the last two metrics in the Appendix E.2.

**Utility-fairness trade-off** takes both utility and fairness into account. It can be evaluated by combining utility and fairness metrics. Besides, equity scaling measurements that involve both aspects could also be used. In classification, we measure the **equity-scaled AUC (AUC\({}_{}\))**, which takes both utility and fairness into account. In segmentation, we report the **equity-scaled DSC (DSC\({}_{}\))**, which measures the tradeoff between overall utility and utility variations .

The mathematical definitions of the above metrics are presented in the Appendix C.

## 4 Results

In this section, we highlight the representative observations and takeaways from benchmarking the fairness of FMs on image classification and segmentation tasks utilizing FairMedFM framework. We choose to present the fairness results concerning sex since it is the most common SA shared across datasets. However, our method supports a broader range of SAs as listed in the Tab. 1. _We direct readers to Appendix E.1 for additional results on more SAs and extensive evaluations, which corroborate the observations discussed in the main text._ We also present the evaluation for _positive-parity-fairness_, _representation-fairness_, and _statistics test_ in Appendix E.2.

**Bias widely exists in FMs for medical imaging**. Fig. 2 and Fig. 3 present the fairness metrics on various classification and segmentation tasks as stated in Sec. 1. In classification, the AUC\({}_{}\) has an average value larger than 5% over all methods but presents large variations across methods. Similarly, in segmentation, our results in both 2D and 3D datasets reveal similar disparities, with notable high DSC\({}_{}\) of SegVol and FT-SAM reaching up to 10%.

**Careful selection and use of FMs are needed for ensuring a good fairness-utility trade-off.** These pervasive biases challenge the fairness-utility tradeoff of FMs in medical applications. We observe that the fairness-utility trade-off in these tasks is influenced not only by the choice of FMs but also by how they are used as shown in Fig. 4. We report trade-off scores AUC\({}_{}\) and DSC\({}_{}\), for each datasets on the selected models for both tasks respectively. Compared with CLIP-ZS models, a simple adaptation, CLIP-Adapt, has proven effective in significantly boosting the fairness-utility trade-off in medical applications. Further, we evaluate the effects of segmentation tasks on the choice of prompts (including _center_, _rand_, _rands_, and _bbox_), and the types of SegFMs (including _2D General-SegFMs_ and _2D Med-SegFMs_). Compared to _center_ and _rand_, models using _rands_ and _bbox_ tend to be fairer across different datasets. This might be due to the tighter constraints on the segmentation provided by _rands_ and _bbox_. Regarding models, General-SegFMs achieve a better fairness-utility trade-off than Med-SegFMs.

**Consistent disparities in SA occur across various FMs on the same dataset.** The performance of FMs shows dataset-specific biases, favoring one category of the given SA over the other, depending on the dataset. We present the four datasets in classification tasks and use sex as an example, presented

Figure 3: Bias in segmentation tasks. DSC\({}_{}\) is the fairness evaluation metric. Note that SAM-Med3D, FastSAM-3D, and SegVol are only applicable for 3D datasets.

Figure 2: Bias in classification tasks. AUC\({}_{}\) is the fairness evaluation metric. “\(\)” denotes vision-language models, and “\(*\)” denotes pure vision models, where CLIP-ZS and CLIP-Adapt are not applicable.

in Fig. 5. The BREST and MIMIC-CXR dataset shows a higher performance for females compared to males across various models. Conversely, the FairVLMed10k and GF3300 dataset indicates better performance for males. Results on other SAs and segmentation tasks can be found in Appendix E.1.

**Existing unfairness mitigation strategies are not always effective.** While various unfairness mitigation methods for traditional neural networks have been proposed, their effectiveness on FMs for medical imaging remains underexplored. This study evaluates three mitigation algorithms for both classification and segmentation tasks. For classification, we apply two PEFT strategies (LP and LoRA) on three datasets (MIMIC-CXR, HAM10000 (CLS), and FairVLMed10k), while experiments for segmentation are conducted on the HAM10000 (SEG) dataset following the SAMed pipeline . As shown in Tab. 2, although some mitigation strategies show better fairness metrics compared to the baseline, their utility-fairness tradeoffs do not always exceed (for example, LoRA + GroupDRO vs. LoRA). Besides, some mitigation algorithms show both lower utility and worse fairness (SAMed + InD vs. SAMed), which means that existing unfairness mitigation strategies are not always effective for FMs. Potential reasons could come from the scaling gap in training data and model parameters between the'small models' and large-scale FMs. Although there have been studies that focus on unfairness mitigation for FMs [24; 67], extra efforts are required to guarantee the fairness of FMs.

## 5 Conclusion

In this work, we introduced FairMedFM, a pioneering benchmark aimed at comprehensively evaluating the fairness of FMs in healthcare. Our pipeline demonstrates versatility by supporting various

Figure 4: Fairness-utility tradeoff in FMs for different components on (a-b) classification and (c-d) segmentation tasks. We use AUC\({}_{}\) and DSC\({}_{}\) as evaluation metrics. (a) Fairness-utility tradeoff for different classification usages (using CLIP as an example); (b) Fairness-utility tradeoff for general-purpose and medical-specific FMs in classification; (c) Fairness-utility tradeoff for different prompts in segmentation (using SAM-Med2D as an example); (d) Degree of fairness for different SegFM categories in segmentation.

Figure 5: Consistent disparities in SA occur across various FMs on the same dataset. (a) LP; (b) CLIP-Adapt. Points on the black dashline represent equal utility.

tasks, such as classification and segmentation, and by adapting to 20 different FMs, including both general-purpose and medical-specific models. By integrating a wide range of functionalities, such as LP, CLIP-Adapt, prompt-based segmentation, PEFT, and bias mitigation strategies, our framework enables comprehensive evaluations that are essential for developing fair and effective medical imaging solutions. With FairMedFM, we conducted in-depth analysis and revealed four key findings and takeaways: (1) Bias widely exists in FMs for medical imgaing tasks; (2) Different FMs and their variant usages present different fairness-utility trade-offs, therefore careful selection and proper use of FMs are crucial for ensuring a good fairness-utility trade-off; (3) The performance of various FMs exhibits consistent dataset-specific biases, which aligns with the SA distribution in individual datasets; and (4) The existing unfairness mitigation strategies are not always effective in FM settings.

**Future development plan.** Despite our efforts to include a wide range of datasets, FM methods, and fairness algorithms in our work to greatly enhance the comprehensiveness of existing benchmarks, there remains potential for further refinement and expansion. Our future work will continue to improve the comprehensiveness of our benchmark based on the framework and codebase of FairMedFM. The current pipeline FairMedFM is sufficiently flexible to extend; therefore, we will continue to incorporate new medical imaging datasets and emerging FM architectures over time. In addition to medical image classification and segmentation, the scope of our study will encompass predictive modeling, object detection, and vision-based question answering. Moreover, we intend to incorporate a broader range of fairness definitions in our evaluations and to investigate a wider array of bias-mitigation algorithms. We will ensure that FairMedFM's open-sourced codebase remains actively maintained and updated at the forefront of promoting equitable healthcare technologies.