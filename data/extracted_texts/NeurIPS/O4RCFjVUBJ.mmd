# How to Continually Adapt Text-to-Image Diffusion

Models for Flexible Customization?

 Jiahua Dong\({}^{1}\)1

Wenqi Liang\({}^{2}\)2

Hongliu Li\({}^{3}\)3

Duzhen Zhang\({}^{1}\)

Meng Cao\({}^{1}\)

Henghui Ding\({}^{4}\)

Salman Khan\({}^{1,5}\)

Fahad Shahbaz Khan\({}^{1,6}\)

Equal contributions (ordered alphabetically). \({}^{1}\)Corresponding authors.3The Hong Kong Polytechnic University \({}^{4}\)Institute of Big Data, Fudan University

\({}^{5}\)Australian National University \({}^{6}\)Linkoping University

{dongjiahua1995, liangwenqi0123, hongliuli1994, henghui.ding}@gmail.com

{duzhen.zhang, meng.cao, salman.khan, fahad.khan}@mbzuai.ac.ae

###### Abstract

Custom diffusion models (CDMs) have attracted widespread attention due to their astonishing generative ability for personalized concepts. However, most existing CDMs unreasonably assume that personalized concepts are fixed and cannot change over time. Moreover, they heavily suffer from catastrophic forgetting and concept neglect on old personalized concepts when continually learning a series of new concepts. To address these challenges, we propose a novel Concept-Incremental text-to-image Diffusion Model (CIDM), which can resolve catastrophic forgetting and concept neglect to learn new customization tasks in a concept-incremental manner. Specifically, to surmount the catastrophic forgetting of old concepts, we develop a concept consolidation loss and an elastic weight aggregation module. They can explore task-specific and task-shared knowledge during training, and aggregate all low-rank weights of old concepts based on their contributions during inference. Moreover, in order to address concept neglect, we devise a context-controllable synthesis strategy that leverages expressive region features and noise estimation to control the contexts of generated images according to user conditions. Experiments validate that our CIDM surpasses existing custom diffusion models. The source codes are available at https://github.com/JiahuaDong/CIFC.

## 1 Introduction

Latent diffusion models (LDMs) [38; 66; 19; 33] have demonstrated unprecedented capabilities in generating high-fidelity images by incorporating large-scale collections of image-text pairs in the latent feature space. Until now, LDMs [4; 30] have achieved remarkable progress in various application fields, including image editing [29; 26], art creation [8; 54], and reconstruction of fMRI brain scans . In order to synthesize some personalized concepts according to user preferences, custom diffusion models (CDMs) [40; 60; 11] rely on low-rank adaptation (LoRA)  to finetune the large-scale LDMs for multi-concept customization . They extend the vision-language dictionary of pretrained LDMs to bind personalized concepts with specific subjects users need to synthesize.

Generally, most existing CDMs [60; 68; 3] assume that users' personalized concepts are fixed and cannot incrementally increase over time. However, this assumption is unrealistic in real-world applications, where users want to continually synthesize a series of new personalized concepts from their own lives. To address this setting, CDMs [3; 57; 2] typically require storing all image-texttraining pairs of old concepts to finetune the pretrained LDMs via LoRA [14; 59]. Nevertheless, the high computation costs and privacy concerns  may render CDMs impractical as the number of old personalized concepts consecutively increases. If the above CDMs retain all low-rank weights associated with old concepts that are obtained in previous customization tasks and then merge them to learn new personalized concepts continually [59; 65], they may experience significant loss of individual attributes on old personalized concepts (_i.e._, catastrophic forgetting [36; 6]) for versatile customization. Moreover, in real-world scenarios, users may wish to control the contexts and objects associated with multiple old concepts in synthesized images according to the conditions they provide (_e.g._, scribble or bounding box [24; 44]). It forces CDMs  to heavily suffer from the challenge of concept neglect  (_i.e._, some old concepts are missing during multi-concept composition).

To handle the above real-world scenarios, in this paper, we propose a new practical problem named _Concept-Incremental Flexible Customization (CIFC)_. In the CIFC setting, as shown in Fig. 1(a), CDMs can consecutively synthesize a sequence of new personalized concepts in a concept-incremental manner for versatile customization (_e.g._, multi-concept generation , style transfer  and image editing ). Additionally, users can control the context and objects of the generated images based on the specific conditions they provide. As aforementioned, the CIFC problem faces two main challenges for versatile concept customization in this paper: **catastrophic forgetting** of old personalized concepts when learning new concepts consecutively under a concept-incremental manner, and **concept neglect** when performing multi-concept composition according to users-provided conditions.

To resolve the challenges in CIFC, we develop a novel Concept-Incremental text-to-image Diffusion Model (CIDM), which can effectively address catastrophic forgetting and concept neglect. **On one hand**, to mitigate the catastrophic forgetting of old personalized concepts, we propose a novel concept consolidation loss for training and devise an elastic weight aggregation (EWA) module for inference. This loss employs learnable layer-wise concept tokens and an orthogonal subspace regularizer to explore task-specific knowledge (_i.e._, unique attributes of personalized concepts), while learning layer-wise common subspaces across different tasks to capture task-shared knowledge. Additionally, the EWA module utilizes learnable layer-wise concept tokens to merge all low-rank weights of old personalized concepts, based on their contributions to versatile concept customization. **On the other hand**, we develop a context-controllable synthesis strategy to tackle concept neglect for multi-concept composition. It leverages layer-wise textual embeddings to enhance the expressive ability of region features and relies on region noise estimation to control the contexts of generated image, conforming to users-provided conditions. Comprehensive experiments illustrate the effectiveness of our proposed CIDM in addressing the CIFC problem. The main contributions of this paper are listed below:

\(\) We propose a new practical problem named Concept-Incremental Flexible Customization (CIFC), where the main challenges are catastrophic forgetting and concept neglect. To address the challenges in the CIFC problem, we develop a novel Concept-Incremental text-to-image Diffusion Model (CIDM), which can learn new personalized concepts continuously for versatile concept customization.

\(\) We devise a concept consolidation loss and an elastic weight aggregation module to mitigate the catastrophic forgetting of old personalized concepts, by exploring task-specific/task-shared knowledge and aggregating all low-rank weights of old concepts based on their contributions in the CIFC.

\(\) We develop a context-controllable synthesis strategy to tackle concept neglect. This strategy controls the contexts of synthesized images according to user conditions by enhancing the expressive ability of region features with layer-wise textual embeddings and incorporating region noise estimation.

## 2 Related Work

**Incremental Learning**[25; 49; 20] accumulates previous experience to incrementally learn new tasks without the need for retraining from scratch. To prevent catastrophic forgetting of old tasks, most incremental or continual learning models mainly employ knowledge distillation between old and new tasks [63; 36; 5], replay some images from old tasks [16; 43], or dynamically expand network architecture to encode new knowledge [61; 15; 7]. Nevertheless, these methods [16; 61; 49; 36] are primarily designed for classifying new object categories consecutively, which cannot be directly applied to tackle continual concept customization tasks without catastrophic forgetting.

**Concept Customization**[11; 40; 22] focuses on extending large-scale diffusion model [31; 46; 64] to synthesize personalized concepts for users. After  proposes to tackle the subject-driven generation by finetuning all network parameters of the pretrained diffusion model  on personalized concepts, some works use textual inversion [9; 53] to learn word embeddings of personalized concepts . For multi-concept customization,  can jointly train multiple concepts or combine different diffusion models by optimizing a few parameters in the cross-attention layers, while  aims to capture different clusters of concept neurons. Motivated by , Han _et al._ finetune the singular values of latent encoding weights, thereby improving the efficiency for concept customization. [42; 57; 18] perform efficient test-time customization by training concept-specific encoders. Besides, [59; 11] fuse multiple low-rank weights to resolve multi-concept customization. In order to tackle continual text-to-image synthesis tasks, Sun _et al._ devise a lifelong diffusion model to accumulate concept information. Unfortunately, it cannot control the contexts of synthesized images and suffers from concept neglect in multi-concept composition [62; 58]. To address the issue of missing concepts, [66; 30] utilize spatial conditions (_e.g._, sketch and pose) for composition. However, these custom diffusion models [59; 47; 51; 58] cannot consecutively learn a sequence of new concepts to tackle the CIFC problem, as they face challenges related to catastrophic forgetting and concept neglect.

## 3 Preliminary and Problem Definition

**Preliminary:** Latent diffusion models (LDMs) [41; 10] rely on some conditional inputs (_e.g._, text prompt [33; 37] or image [17; 32]) to control the contexts of synthesized images. They use an encoder \(()\) and a decoder \(()\) to perform image synthesis in the latent space . Custom diffusion models (CDMs) [12; 9; 28] utilize low-rank adaptation (LoRA) [14; 55] to learn new personalized concepts by finetuning the pretrained LDMs [1; 38]. Given a pair of personalized image \(\) and its text prompt \(\), the encoder \(()\) maps \(\) to a latent feature \(\), and \(_{t}\) denotes the noisy latent feature at the \(t\)-th (\(t=1,,T\)) timestep. After the text encoder \(()\) (_e.g._, pretrained CLIP ) maps \(\) to the textual embedding \(=()\), the objective to learn personalized concept \(\{,\}\) at the \(t\)-th timestep is:

\[_{}=_{(),,(0,),t}[\|-_{ ^{}}(_{t}|,t)\|_{2}^{2}],\] (1)

where \(_{^{}}()\) denotes the denoising UNet proposed in [38; 33], and it can gradually denoise \(_{t}\) by predicting the noisy estimation \(_{^{}}(_{t}|,t)\) as Gaussian noise \((0,)\). In the paper, \(^{}=_{0}+\) consists of the pretrained parameter \(_{0}=\{_{0}^{}\}_{l=1}^{L}\) in LDMs [39; 33] and low-rank parameter \(=\{^{}\}_{l=1}^{L}\) updated by LoRA [22; 11]. \(_{0}^{},^{l}^{a b}\) denote the pretrained weight and low-rank weight in the \(l\)-th (\(l=1,,L\)) transformer layer of \(^{}\), respectively. \(a\) and \(b\) are the row and column of matrices. As introduced in [40; 68], \(^{l}=^{l}^{l}\) can be decomposed as two low-rank factors \(^{l}^{a r}\) and \(^{l}^{r b}\), where \(r(a,b)\) denotes the rank.

However, most CDMs [11; 12; 9; 28] assume that the number of users' personalized concepts remains constant over time. This assumption is unrealistic in real-world applications, where users wish to consecutively generate a series of new personalized concepts based on their preferences. More

Figure 1: Diagram of the proposed CIDM to address the CIFC problem. It consists of (a) a concept consolidation loss, (b) an elastic weight aggregation module to resolve catastrophic forgetting, and (c) a context-controllable synthesis strategy to address the challenge of concept neglect.

importantly, they significantly suffer from catastrophic forgetting  of old personalized concepts and concept neglect when performing versatile customization in a concept-incremental manner.

**Problem Definition:** To address the above challenges, we propose a new practical problem named Concept-Incremental Flexible Customization (CIFC). In the CIFC, there is a series of consecutive text-guided concept customization tasks \(=\{_{g}\}_{g=1}^{G}\), where \(G\) denotes the task quantity. According to users' personal preferences, the \(g\)-th task \(_{g}=\{_{g}^{k},_{g}^{k},_{g}^{k} \}_{k=1}^{n_{g}}\), which includes \(n_{g}\) triplets of image \(_{g}^{k}\), text prompt \(_{g}^{k}\), and its concept tokens \(_{g}^{k}_{g}\), belongs to one of the versatile customization categories: multi-concept generation , style transfer  and image editing . Here, \(_{g}^{k}\) indicates the textual description of \(_{g}^{k}\) (_e.g._, photo of a \([V_{*}]\) [\(V_{}\)]), whereas \(_{g}^{k}\) denotes the concept tokens (_e.g._, \([V_{*}]\) [\(V_{}\)] in \(_{g}^{k}\), \(_{g}\) is concept space of the \(g\)-th task, and it comprises \(C_{g}\) new personalized concepts \(_{g}=_{k=1}^{n_{g}}_{g}^{k}\) in the \(g\)-th task. Particularly, the concept spaces between any two tasks have no overlap: \(_{g}(_{i=1}^{g-1}_{i})=\). It implies that \(C_{g}\) new concepts in the \(g\)-th task are different from \(_{i=1}^{g-1}C_{i}\) old personalized concepts from \(g-1\) old tasks under the CIFC setting. Considering the practicality of the CIFC setting, we don't allocate any memory storage to store or replay the training data of all tasks \(\{_{g}\}_{g=1}^{G}\), ensuring that all concept customization tasks are learned in a concept-incremental manner. The CIFC setting can continually learn new personalized concepts in a concept-incremental manner for versatile customization while tackling the forgetting on old concepts.

## 4 The Proposed Model

Fig. 1 shows the diagram of our concept-incremental diffusion model (CIDM) to tackle the CIFC problem. It includes (a) a concept consolidation loss in Sec. 4.1 and (b) an elastic weight aggregation module in Sec. 4.2 to resolve catastrophic forgetting during training and inference. Additionally, it encompasses (c) a context-controllable synthesis strategy in Sec. 4.3 to address concept neglect.

### Concept Consolidation Loss

In order to learn the \(g\)-th text-guided concept customization task \(_{g}\), we use the LoRA [14; 38] to finetune the pretrained denoising UNet \(_{_{0}}()\) on personalized data \(\{_{g}^{k},_{g}^{k},_{g}^{k}\}_{k=1}^{n_{g}}\) by optimizing Eq. (1), and then obtain an updated model \(_{_{g}^{}}()\), where \(_{g}^{}=_{0}+_{g}\), \(_{g}=\{_{g}^{l}\}_{l=1}^{L}\), and \(_{g}^{l}=_{g}^{l}_{g}^{l} ^{ b}\) is the updated low-rank weight in the \(l\)-th layer. \(_{g}^{l}^{ r}\) and \(_{g}^{l}^{r b}\) are low-rank factors. As introduced in [14; 60], \(_{g}\) can encode most of \(C_{g}\) personalized concept identity within the \(g\)-th task. To address the CIFC problem, a trivial solution for learning the \(g\)-th task is to store the updated low-rank weights of all tasks \(\{_{i}\}_{i=1}^{g}\) learned so far, and then linearly merge them by evaluating their contributions [59; 65] during training. However, it may result in substantial loss of individual characteristics within some personalized concepts, when learning new concepts continually in the CIFC setting. This phenomenon is referred to as catastrophic forgetting on old personalized concepts. To mitigate catastrophic forgetting during training, as show in Fig. 1(a), we develop a concept consolidation loss \(_{}\) to explore task-specific and task-shared knowledge.

**Task-Specific Knowledge** indicates distinctive characteristics of personalized concepts within each concept customization task. To explore this knowledge, we introduce learnable layer-wise concept tokens to better preserve unique attributes of personalized concepts in the synthesized images. It is significantly different from existing textual inversion methods [9; 67; 53] that inject a unified text prompt into all transformer layers of \(_{_{g}^{}}()\). For a triplet \(\{_{g}^{k},_{g}^{k},_{g}^{k}\}\) in the \(g\)-th task, we define \(L\) layer-wise text prompts as \(\{_{g}^{k,l}\}_{l=1}^{L}\), where \(_{g}^{k,l}\) has its own learnable layer-wise concept tokens \(_{g}^{k,l}\) in the \(l\)-th transformer layer. For example, given a textual description \(_{g}^{k}\) (photo of a [\(V_{*}\)] [\(V_{}\)]) of image \(_{g}^{k}\), its text prompt of the \(l\)-th layer is defined as \(_{g}^{k,l}\) (photo of a [\(V_{*}^{l}\)] [\(V_{}^{l}\)]), and \([V_{*}^{l}\)] [\(V_{}^{l}\)] indicates learnable concept tokens \(_{g}^{k,l}\) in the \(l\)-th layer. After using \(_{g}^{k}\) to initialize \(\{_{g}^{k,l}\}_{l=1}^{L}\), we inject the textual embedding \(_{g}^{k,l}=(_{g}^{k,l})\) encoded via the text encoder \(()\) into the \(l\)-th layer of \(_{_{g}^{}}()\). When we train \(_{_{g}^{}}()\) via Eq. (1), the learnable layer-wise concept tokens can capture unique characteristics of old personalized concepts from different layers to surmount catastrophic forgetting.

However, the discriminative ability of task-specific knowledge to distinguish different personalized concepts can significantly deteriorate as the number of concept customization tasks gradually increasesunder the CIFC settings. To tackle this issue, we devise an orthogonal subspace regularizer to constrain the low-rank weights of different customization tasks. It can enhance the discriminative ability of task-specific knowledge by ensuring the orthogonality of concept subspaces across different tasks. Given the low-rank weight \(_{g}=\{_{g}^{l}\}_{l=1}^{L}\) in the \(g\)-th task, \(_{g}^{l}=_{g}^{l}_{g}^{l}\) can be regarded as consisting of the low-rank concept subspace \(_{g}^{l}=[_{g}^{l,1},,_{g^{l}}^{l,r}] ^{a r}\) and its linear weighting matrix \(_{g}^{l}=[_{g}^{l,1},,_{g}^{l,r}]^{} ^{r b}\), where \(_{g}^{l,i}^{b}\) denotes the linear weighting coefficients of \(_{g}^{l,i}^{a}\) (\(i=1,,r\)). In the \(g\)-th task, we perform the orthogonal subspace regularizer on the low-rank concept subspaces of different tasks: \(_{i=1}^{g-1}_{l=1}^{L}_{i}^{l}(_{g}^{l})^{}=0\). Since the orthogonal constraint is not differentiable, we propose an alternative optimization strategy that minimizes the absolute value of the inner product between different subspaces: \(_{1}=_{i=1}^{g-1}_{l=1}^{L}_{i}^{l}(_{ g}^{l})^{}\).

**Task-Shared Knowledge** represents the shared semantic information across different tasks with semantically similar concepts, which is beneficial to address catastrophic forgetting on old personalized concepts. To capture task-shared knowledge, we propose to learn a layer-wise common subspace \(_{*}^{l}^{a b}\) shared across different tasks in the \(l\)-th layer. Given the low-rank weights \(\{_{i}\}_{i=1}^{g}\) learned so far, the learnable projection matrix \(_{i}^{l}^{a a}\) can encode common semantic information of \(\{_{i}\}_{i=1}^{g}\) into \(_{*}^{l}\) via \(_{2}=_{i}^{g}_{l}^{L}\|_{i}^{l}-_{i}^{l}_{*}^{l}\|_{F}^{2}\). Therefore, in the \(g\)-th task, the concept consolidation loss \(_{}\) to learn both task-specific and task-shared knowledge is defined as:

\[_{}=_{(_ {g}^{k}),_{g}^{k},(0,),d}[\| -_{_{g}^{}}(_{i}|_{g}^{k},t)\|_{2}^{2} +_{1}_{1}+_{2}_{2}],\] (2)

where \(_{g}^{k}=\{_{g}^{k,l}\}_{l=1}^{L}\) indicates \(L\) layer-wise textual embeddings, \(_{1}\) and \(_{2}\) are balance parameters.

**Two-Step Optimization:** To train the proposed CIDM via Eq. (2) in the \(g\)-th task, we devise a two-step optimization strategy in each training batch. Firstly, to capture task-specific information, we utilize Eq. (2) to update the learnable layer-wise concept tokens and low-rank weight \(_{g}=\{_{g}^{l}\}_{l=1}^{L}\), when fixing \(_{i}^{l}\) and \(_{*}^{l}\). Secondly, to explore task-shared knowledge, we fix \(_{g}\) and the learnable layer-wise concept tokens, and then only use \(_{2}\) in Eq. (2) to update \(_{i}^{l}\) and \(_{*}^{l}\) respectively. Notably, the detailed optimization procedure is shown in the appendix section (Sec. A.1). After utilizing the two-step optimization strategy to learn the \(g\)-th concept customization task, we can obtain the task learner \(_{g}=\{_{g}^{l},}_{g}^{l}\}_{l =1}^{L}\), where \(}_{g}^{l}=\{}_{g}^{l,i}\}_{i=1}^{C_{g}}\), and \(}_{g}^{l,i}\) is the \(i\)-th (\(i=1,,C_{g}\)) concept token learned at the \(l\)-th transformer layer through Eq. (2).

### Elastic Weight Aggregation

To tackle catastrophic forgetting on old personalized concepts during inference, as shown in Fig. 1(b), we store \(g\) task learners \(\{_{i}\}_{i=1}^{g}\) learned so far (\(g 2\)), and develop an elastic weight aggregation (EWA) module to adaptively merge them for versatile concept customization. Note that the memory storage of storing \(_{i}\) (\(i=1,,g\)) only accounts for \(0.25\%\) of the pretrained model \(_{0}\), making it negligible in practical applications. Specifically, given a text prompt \(}\) for inference, we use it to initialize layer-wise text prompts \(\{}^{l}\}_{l=1}^{L}\) and extract their textual embeddings \(}=\{}^{l}^{n_{e} d} \}_{l=1}^{L}\) via text encoder \(()\), where \(n_{e}\) and \(d\) denote the token number and feature dimension, respectively. Given the stored task learners \(\{_{i}\}_{i=1}^{g}\), we can collect all concept tokens \(\{^{l}\}_{l=1}^{L}\) learned so far, where \(^{l}=_{i=1}^{g}}_{i}^{l}^{n_{e}}\) consists of \(n_{c}=_{i=1}^{g}C_{i}\) concept tokens learned in the \(l\)-th layer. After \(()\) encodes \(\{^{l}\}_{l=1}^{L}\) to latent embeddings \(\{^{l}^{n_{e} d}\}_{l=1}^{L}\), we average those latent embeddings belonging to the same task to obtain new embeddings \(\{}^{l}^{g d}\}_{l=1}^{L}\) (\(g\) is number of tasks learned so far). Subsequently, we compute semantic relations \(^{g}\) between \(}^{l}\) and \(}^{l}\), and use \(\) to adaptively merge low-rank weights \(\{_{i}^{l}\}_{i=1}^{g}\) of all learned tasks in the \(l\)-th layer. Therefore, the merged low-rank weight \(}^{l}\) in the \(l\)-th transformer layer can be formulated as follows:

\[=}^{l}(}^{l})^{ },}^{l}=_{i=1}^{g} _{i}^{l}()_{i},\] (3)

where \(()\) denotes the maximization function along the row axis. \(()=^{2}/\|^{2}\|_{F}^{g}\) is used to normalize the semantic relations \(\), and \(()_{i}\) is the \(i\)-th element of \(()\).

**Inference:** After employing Eq. (3) to merge all low-rank weights learned so far, we obtain a new denoising UNet \(_{_{g}^{}}()\) for inference, where \(_{*}^{}=_{0}+_{*}\), and \(_{*}=\{}^{l}\}_{l=1}^{L}\). Notably, \(_{*}^{}\) has encoded substantial distinctive attributes of all personalized concepts learned so far, which can effectively mitigate the catastrophic forgetting of old concepts during inference.

### Context-Controllable Synthesis

When we directly use \(_{^{}_{s}}()\) obtained via Sec. 4.2 to perform multi-concept customization under the CIFC setting, it cannot generate high-fidelity images according to users-provided conditions (_e.g._, scribble or bounding box ), and heavily suffers from the challenge of concept neglect  (_i.e._, some concepts are missing in the synthesized images). Thus, as shown in Fig. 1(c), we devise a context-controllable synthesis strategy to address the conditional generation and concept neglect.

**Conditional Generation:** Besides the initial text prompt \(}\) defined in Sec. 4.2, users can provide \(U\) pairs of region conditions \(\{}_{u},}_{u}\}_{u=1}^{U}\), where \(}_{u}\) is the bounding box to synthesize concepts related to the \(u\)-th region text prompt \(}_{u}\). Then we use \(()\) to extract layer-wise textual embeddings \(}_{u}=\{}_{u}^{l}^{n_{u}  d}\}_{l=1}^{L}\) for \(}_{u}\). Given the initial text prompt \(}\), we can use the new denoising UNet \(_{^{}_{s}}()\) in Sec. 4.2 to obtain its feature map \(^{l}^{h^{l} w^{l} d}\) in the \(l\)-th transformer layer encoded by textual embedding \(}^{l}\), where \(h^{l}\) and \(w^{l}\) are height and width of \(^{l}\). Different from , we perform layer-wise regional cross-attention between textual embedding \(}_{u}^{l}\) and \(^{l}\) to obtain the \(u\)-th region feature \(_{u}^{l}^{h^{l} w^{l}_{u} d}\) in the \(l\)-th layer, where \(h^{l}_{u}\) and \(w^{l}_{u}\) are height and width of bounding box \(}_{u}\). Specifically, \(_{u}^{l}=(^{l}(_{u}^{l})^{}/) _{u}^{l}\), where \(()\) is sigmoid function, \(^{l}=(^{l}_{q}}_ {u}^{l})^{h^{l} w^{l}_{u} d},_{u}^{l}= }_{u}^{l}_{k}^{n_{u} d}\) and \(_{u}^{l}=}_{u}^{l}_{v}^{ n_{u} d}\). \(}_{u}^{l}^{h^{l} w^{l}}\) is the binary region mask in the \(l\)-th layer, where the values inside the bounding box \(}_{u}\) are set to \(1\). \(()\) can retain only

Figure 2: Some qualitative comparisons of single-concept customization generated by SD-1.5 .

the features inside \(}_{u}\), and \(\) is the Hardmard product. \(_{q},_{k},_{v}^{d d}\) are mapping matrices in the new denoising UNet \(_{_{*}^{}}()\). Then, the values of \(^{l}\) inside the bounding boxes \(\{}_{u}\}_{u=1}^{U}\) are respectively replaced with the corresponding region features \(\{_{u}^{l}\}_{u=1}^{U}\) to obtain a new feature map \(}^{l}\). We apply the layer-wise regional cross-attention to all layers of \(_{_{*}^{}}()\) for conditional generation.

**Multi-Concept Composition:** After integrating the above conditional generation into the new denoising UNet \(_{_{*}^{}}()\), inspired by , we can obtain the noise estimation \(^{t}=_{_{*}^{}}(_{t}|t)+s( _{_{*}^{}}(_{t}|},t)- _{_{*}^{}}(_{t}|t))^{h_{L} w _{L} d_{L}}\) for the initial text prompt \(}\), where \(^{t}\) is the output of the \(L\)-th transformer layer in \(_{_{*}^{}}()\) at the \(t\)-th timestep. \(h_{L},w_{L},d_{L}\) are the height, width and channel of \(^{t}\), respectively. \(_{_{*}^{}}(_{t}|t)\) is the unconditional noise estimation, \(_{_{*}^{}}(_{t}|},t)\) is the conditional noise estimation based on the textual embeddings \(}\), and \(s=7.5\) denotes the scale. Therefore, the noise estimation \(_{u}^{t}^{h_{L} w_{L} d_{L}}\) for the \(u\)-th region condition \(\{}_{u},}_{u}\}\) can be formulated as follows:

\[_{u}^{t}=_{_{*}^{}}(_{t}|t)+s( _{_{*}^{}}(_{t}|[}_{u}, }_{u}],t)-_{_{*}^{}}(_{t}|t )),\] (4)

where \(_{_{*}^{}}(_{t}|[}_{u}, }_{u}],t)\) is the conditional noise estimation based on \([}_{u},}_{u}]\). For multi-concept customization in the CIFC, we aggregate \(U\) region noise estimations to address concept neglect:

\[_{*}^{t}=^{t}+_{u=1}^{U}(1-) _{u}^{t}}_{u}^{L},\] (5)

Figure 3: Some qualitative comparisons of multi-concept customization generated by SDXL , where ITP indicates the initial text prompt, and RTP denotes the region text prompt.

where \(}_{u}^{L}^{h_{L} w_{L}}\) is the binary region mask of the bounding box \(}_{u}\) in the \(L\)-th layer. \(\) is the balance weight. Following [38; 33], we forward \(_{*}^{t}\) to the denoising process for image synthesis.

## 5 Experiments

### Experimental Setups

**Benchmark Dataset:** Motivated by [47; 11; 45], in this paper, we construct a new challenging concept-incremental learning (CIL) dataset including ten continuous text-guided concept customization tasks to illustrate the effectiveness of our model under the CIFC setting. In the CIL dataset, seven customization tasks have different object concepts (_i.e._, V1 dog, V2 duck toy, V3 cat, V4 backpack, V5 teddy bear, V7 dog and V9 cat) from [40; 22], and the remaining three tasks have different style concepts (_i.e._, V6, V8 and V10 styles) collected from website. Considering the practicality of the CIFC setting, we set about \(3 5\) text-image pairs for each task. Particularly, we introduce some semantically similar concepts (_e.g._, V1 and V7 dogs, V3 and V9 cats), making the CIL dataset more challenging under the CIFC setting.

**Implementation Details:** We utilize two popular diffusion models: Stable Diffusion (SD-1.5)  and SDXL  as the pre-trained models to conduct comparison experiments. For fair comparisons, we train all SOTA comparison methods and our model using the same backbone and Adam optimizer, where the initial learning rate is \(1.0 10^{-3}\) to update textual embeddings, and \(1.0 10^{-4}\) to optimize the denoising UNet. For the low-rank matrices, we follow  to set \(r=4\). We empirically set \(_{1}=0.1,_{2}=1.0\) in Eq. (2), \(=0.1\) in Eq. (5), and the training steps are \(800\).

**Evaluation Metrics:** After learning the final concept customization task under the CIFC setting, we conduct both the qualitative and quantitative evaluations on versatile generation tasks: single/multi-concept customization, custom image editing, and custom style transfer. For the quantitative evaluation, we follow  to use text-alignment (TA) and image-alignment (IA) as metrics. Specifically, for image-alignment (IA), we use the image encoder of CLIP  to evaluate the feature similarity between the synthesized image and original sample. For text-alignment (TA), we utilize the text encoder of CLIP  to compute the text-image similarity between the synthesized image and its corresponding prompt.

### Qualitative Comparisons

To verify the superiority of our model under the CIFC setting, we introduce extensive qualitative comparisons, including single/multi-concept customization (see Figs. 2-3), custom image editing

Figure 4: Comparisons of custom image editing.

Figure 5: Comparisons of custom style transfer.

[MISSING_PAGE_FAIL:9]

optimization during training. Moreover, the elastic weight aggregation module proposed in this paper effectively merges previous personalized concepts for customization during inference.

### Ablation Studies

This subsection analyzes the effectiveness of each module in our model: elastic weight aggregation (EWA), context-controllable synthesis (CCS), task-specific knowledge (TSP) and task-shared knowledge (TSH) in the concept consolidation loss (CCL). Tab. 3 presents the ablation studies of single-concept customization in terms of IA. When compared with Baseline, the performance of our model improves by \(0.2\% 3.3\%\) in terms of IA, after we add the proposed TSP, TSH and EWA modules. It demonstrates the effectiveness of our model in resolving the CIFC problem by addressing the catastrophic forgetting and concept neglect. As shown in Fig. 6, we analyze the effectiveness of CCS in multi-concept customization, where CCS includes the layer-wise regional cross-attention (RCA) and the region noise estimation (RNE) modules. In Fig. 6, the performance of our model decreases substantially when removing the RCA and RNE modules. It verifies the effectiveness of our CCS strategy in addressing conditional generation and concept neglect. Moreover, Fig. 7 shows ablation analysis about TSP and TSH. It illustrates that our model can capture task-specific information within each customization task and explore task-shared knowledge across different tasks to tackle the CIFC problem via optimizing Eq. (2).

## 6 Conclusion

In this paper, we propose a novel Concept-Incremental text-to-image Diffusion Model (CIDM) to address a practical Concept-Incremental Flexible Customization (CIFC) problem, where the CIFC problem has two major challenges: catastrophic forgetting and concept neglect. Specifically, we devise a concept consolidation loss and an elastic weight aggregation module to respectively resolve catastrophic forgetting during training and inference. They can capture task-specific/task-shared knowledge and aggregate all low-rank weights of old concepts according to their contributions in the CIFC. To address concept neglect, we propose a new context-controllable synthesis strategy, which can control the contexts of synthesized images according to users-provided conditions. Extensive experiments on versatile customization tasks (single/multi-concept customization, custom image editing and style transfer) show the superior performance of our CIDM in tackling the CIFC problem compared to SOTA methods. In the future, we will leverage multi-modal large language models to address the CIFC problem and apply the proposed model to personalized video generation.

Figure 6: Ablation analysis of CSS in multi-concept customization.

Figure 7: Ablation studies of the TSP and TSH.