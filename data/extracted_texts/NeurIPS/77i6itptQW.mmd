# IDEA: An Invariant Perspective for Efficient Domain Adaptive Image Retrieval

Haixin Wang\({}^{1,}\)1, Hao Wu\({}^{2,}\)1, Jinan Sun\({}^{1}\), Shikun Zhang\({}^{1}\),

**Chong Chen\({}^{1}\), Xian-Sheng Hua\({}^{3}\), Xiao Luo\({}^{4,}\)\({}^{{}}\)\({}^{1}\)**Peking University, \({}^{2}\)University of Science and Technology of China,

\({}^{3}\)Terminus Group, \({}^{4}\)University of California, Los Angeles

wang.hx@stu.pku.edu.cn, wuhao2022@mail.ustc.edu.cn,

sjn@pku.edu.cn, zhangsk@pku.edu.cn, chenchong.cz@gmail.com,

huaxiansheng@gmail.com, xiaoluo@cs.ucla.edu

Equal contribution. Corresponding author.

###### Abstract

In this paper, we study the problem of unsupervised domain adaptive retrieval, which transfers retrieval models from a label-rich source domain to a label-scarce target domain. Although there exist numerous approaches that incorporate transfer learning techniques into deep hashing frameworks, they often overlook the crucial invariance needed for adequate alignment between these two domains. Even worse, these methods fail to distinguish between causal and non-causal effects embedded in images, making cross-domain retrieval ineffective. To address these challenges, we propose an Invariance-acquired Domain Adaptive Hashing (IDEA) model. Our IDEA first decomposes each image into a causal feature representing label information and a non-causal feature indicating domain information. We then generate discriminative hash codes using causal features with consistency learning on both source and target domains. More importantly, we employ a generative model for synthetic samples to simulate the intervention of various non-causal effects, thereby minimizing their impact on hash codes for domain invariance. Comprehensive experiments conducted on benchmark datasets confirm the superior performance of our proposed IDEA compared to a variety of competitive baselines.

## 1 Introduction

Approximate nearest neighbor (ANN) search [73; 77; 49; 29; 40] has been a pivotal research problem within the field of deep learning for decades. Recently, learning to hash [41; 80; 68; 20] has come to the fore in ANN search engines due to its high computational and memory efficiency. The main objective of learning to hash is to convert high-dimensional data points into compact low-dimensional binary codes while maintaining semantic similarity. Consequently, this approach mitigates the requirement for computationally expensive pairwise distance computations, replacing them with economical Hamming distance calculations using bit-wise XOR and bit-count operations .

Recent hashing techniques can primarily be classified into two types, namely supervised approaches [72; 44; 67; 48; 75; 8; 31; 52] and unsupervised approaches [32; 76; 46; 69; 33; 61; 10]. Supervised hashing approaches incorporate annotation data into the similarity learning algorithms, generally leading to better performance than unsupervised ones. These supervised methods primarily focus on minimizing a spectrum of similarity-preserving losses and quantization losses [11; 78] to generate highly discriminative hash codes. Furthermore, several innovative proxy-based techniques have been introduced where a proxy is constructed in the Hamming space for each class, and hashcodes are enforced to approximate their respective proxies. These methods have demonstrated encouraging results in efficient image retrieval.

However, these supervised approaches rely on an assumption that the distributions between the training and query data are the same, which is often violated in real-world scenarios. For instance, when an ANN search engine trained on benchmark images, is employed to search query images from mobile phones, retrieval performance could potentially diminish substantially [45; 22; 64]. In light of this, this paper concentrates on the problem of domain adaptive retrieval, which aims to enhance the performance of retrieval systems by leveraging both labeled source instances and unlabeled target samples. In literature, a variety of domain adaptive hashing methods have been proposed [71; 66; 64; 47; 74; 21; 58], which typically incorporate domain adaptation strategies into deep hashing frameworks. They adopt the memory bank  and adversarial learning [58; 36] to achieve alignment across source and target domains. Moreover, pseudo-labeling [19; 58] is implemented to extract knowledge from unlabeled target samples, thus addressing the challenge of label scarcity in the target domain.

Unfortunately, existing approaches have two intrinsic limitations that hinder their effectiveness as follows: (1) _Entanglement of causal and non-causal effects._ Current methods tend to recklessly focus on correlations between image data and semantic labels. This could result in an entanglement of causal and non-causal factors. Consequently, the hash codes derived from these deep features may lack interpretability and be suboptimal for subsequent retrieval tasks. (2) _Neglect of invariant patterns._ While adversarial learning is commonly used to implicitly align hash codes between source and target domains, these methods often fail to explicitly capture the invariant patterns present in vision representations. This could lead to ineffective domain alignment of hash codes due to the influence of underlying variant factors. As a result, an effective hashing framework under the principle of invariant learning [2; 5] is highly desired.

In this paper, we explore domain adaptive hashing through a causal analysis lens, proposing a model named Invariance-Acquired Domain Adaptive Hashing (IDEA) for efficient image retrieval under domain shift. The core of domain adaptive hashing lies in discerning causal and non-causal effects within images. To generate high-quality and interpretable binary codes, we disentangle causal and non-causal features within each image, guided by the principles of the information bottleneck. On one hand, we maximize the mutual information between causal features and label information. On the other hand, we retain the most information in hidden features for non-causal features while minimizing the information in labels. These causal features are used to generate discriminative hash codes through consistency learning across both source and target domains. To further mitigate the non-causal effects, we incorporate a generative model that simulates the intervention of various non-causal effects, thereby encouraging hash codes to remain sufficiently invariant to different non-causal components. Leveraging invariant learning on causal effects, our IDEA is capable of generating domain-invariant hash codes that facilitate efficient cross-domain retrieval. Empirical experiments conducted on a variety of benchmark datasets validate the superior performance of our IDEA compared to competitive baselines. The contribution of this paper is summarized as follows:

* _Problem Connection._ We pioneer a novel perspective that connects invariant learning with domain adaptive hashing for efficient image retrieval.
* _Novel Methodology._ Our IDEA not only disentangles causal and non-causal features in each image following the principle of the information bottleneck, but also ensures hash codes are sufficiently invariant to the intervention of non-causal features.
* _High Performance._ Comprehensive experiments across numerous datasets demonstrate that our IDEA outperforms a range of competitive baselines in different settings.

## 2 Related Works

**Learning to Hash.** The task of efficient image retrieval has recently witnessed significant interest [49; 29; 40; 37], leading to the development of deep hashing methods [41; 80; 68; 20], including both supervised [72; 44; 67; 48; 75; 8; 31; 52; 54] and unsupervised approaches [32; 76; 46; 69; 33; 61; 56; 10; 55]. Deep unsupervised hashing methods typically reconstruct semantic structures using similarity metrics, subsequently generating similarity-preserving binary codes. Self-supervised methods have also been developed to enhance the performance. The incorporation of label information has notably improved the performance of deep supervised hashing methods. Early attempts at optimizing the hashing network often involved pairwise and triplet losses based on similarity structures [44; 67; 30; 4]. Another line of study has utilized point-wise optimization, establishing proxies for each category within the Hamming space and compelling hash codes to approximate these proxies [72; 13]. However, these methods typically overlook potential distribution shift in practical scenarios, which can significantly degrade retrieval performance. This limitation has prompted research into domain adaptive hashing.

**Unsupervised Domain Adaptation.** Unsupervised Domain Adaptation (UDA) has long been a formidable challenge in machine learning and computer vision [38; 51; 14]. Early methods often explicitly reduce distribution discrepancies for domain alignment [43; 27]. An alternative approach utilizes adversarial learning for implicit domain alignment, incorporating a gradient reversal layer and a domain discriminator to engage in a minimax game [15; 36]. Recently, researchers have extended this topic towards efficient domain adaptive retrieval [71; 66; 64; 47; 74; 21; 58; 59], proposing a number of transferable hashing methods to address this problem. These methods typically integrate domain adaptation models into deep hashing frameworks and generate similarity structures for learning semantic knowledge on the target domain. Despite these advancements, these methods often fall short due to the entanglement of causal and non-causal effects and the neglect of invariant patterns. In response to these challenges, we propose an effective approach IDEA.

**Invariant Learning.** Invariant learning [2; 5; 9] aims to identify invariant correlations between inputs and targets under domain shift, while concurrently eliminating spurious and variant relationships. This concept has been explored in the context of out-of-distribution generalization. Under specific assumptions, invariant learning has demonstrated significant potential for model generalization following causal theory. Invariant risk minimization  (IRM) has been proposed to regularize neural networks to remain stable under the environmental variance, showing superior performance when compared to empirical risk minimization  (ERM). Moreover, MVDC  learns from frequency spectrum to generate causal features for domain generalization for cross-domain object detection. Invariant learning has also been extended to address challenges in graph domains [28; 70; 6; 53; 63]. For instance, CIGA  leverages distinct subgraphs for graph contrastive learning, thereby achieving superior out-of-distribution graph classification performance. Our study establishes a connection between invariant learning and domain adaptive hashing, generating domain-invariant and discriminative hash codes for cross-domain image retrieval. To the best of our knowledge, this is the first work to apply invariant learning for transferable retrieval tasks, and is proven effective for potential application.

## 3 Problem Definition

Given a source domain \(^{s}=\{(_{i}^{s},y_{i}^{s})\}_{i=1}^{N_{s}}\) with \(N_{s}\) fully-labeled images and a target domain \(^{t}=\{(_{j}^{t})\}_{j=1}^{N_{t}}\) with unlabeled \(N_{t}\) images, both domain share a common label space \(=\{1,2,,C\}\) despite potential distribution shifts. The objective is to develop a hashing-based retrieval model that projects an input image \(\) onto a compact binary code \(\{-1,1\}^{L}\), where \(L\) represents the code length. This model should ensure that similar images are mapped to comparable binary codes within the Hamming space, and its performance should be evaluated in both single-domain and cross-domain retrieval systems. In a single-domain retrieval system, both query and database images originate from the target domain, whereas, in a cross-domain retrieval system, the query images are drawn from the target domain \(^{t}\) and the database images are sourced from the source domain \(^{s}\).

## 4 Method

### Framework Overview

We address the problem of unsupervised domain adaptive retrieval. While several domain adaptive hashing algorithms have been proposed, they often neglect invariant learning during the hash code generation process. This oversight often leads to significant domain discrepancy in the resulting hash codes. Moreover, these methods indiscriminately focus on correlations between image data and semantic labels,

Figure 1: Structural causal model in our problem.

failing to disentangle the causal and non-causal effects encoded in images. This results in learnt hash codes that are not only suboptimal for downstream retrieval tasks, but are also difficult to interpret.

In this study, we present a novel approach named IDEA, to tackle these issues. The fundamental idea is to incorporate invariant learning into the optimization of the hashing network. Specifically, we first introduce a structural causal model, as illustrated in Figure 1, and analyze the correlation within this problem. We then separate each image into causal and non-causal features using an information bottleneck. The causal features are subsequently used for hash code production. Lastly, we introduce interventions on non-causal effects and limit their influence on invariant hash codes. More details can be found in Figure 2.

### Structure Causal Model

To start, we apply a causal perspective to domain adaptive hashing and construct a structural causal model  (SCM) to depict the image generation process under domain shifts. This model is used to illustrate the relationships between domains \(D\), semantic labels \(Y\), causal parts \(C\), non-causal parts \(NC\), images \(X\), and hash codes \(H\). The specific relationships are elaborated as follows:

* \(Y C\). Semantic labels produce the causal part which should be invariant to domain distributions.
* \(D NC\). Domain information provides the non-causal part which is changeable across domains.
* \(C X NC\). Each image is generated by combining both causal part and non-causal part.
* \(\)\(C H\). To obtain domain-invariant hash codes, we should generate hash codes using causal features. However, there could be additional relationships between \(C\) and \(NC\). For example, \(C\) and \(NC\) can be simultaneously influenced by a cause. Therefore, \(C\) acts as a confounder between \(NC\) and \(H\), i.e., resulting in falsely related variables.

The essence of domain adaptive hashing lies in learning domain-invariant and discriminative binary codes. From the structural causal model, we need to construct mappings \(()\) and \(()\) which fulfill the following conditions:

\[[C,NC]=(X),H=(C),H NC C,\] (1)

where \(\) symbolizes the independence under the given condition. Inferred from Eqn. 1, we are required to address the following challenges: 1) The first term necessitates us to disentangle the causal and non-causal components of each image; 2) The second term requires us to learn discriminative hash codes from causal components; 3) The final term advocates for the generation of hash codes that are independent of the influence of non-causal components. With these goals in mind, we propose a novel deep hashing method IDEA for domain adaptive retrieval.

Figure 2: The framework of the proposed IDEA. We feed both source and target images into an encoder. Then each image is disentangled into causal features and a non-causal features. The causal features are adopted to generate hash codes. A generative model is utilized to reconstruct the original graphs. We add intervention by using a different non-causal features and minimize the domain shift by invariant learning.

### Causality-acquired Disentanglement

In this part, we apply the information bottleneck principle  to disentangle each image into causal and non-causal features. This approach facilitates the extraction of multifaceted latent factors embedded within images, enabling the generation of high-quality and interpretable binary codes. As suggested by the structural causal model, causal features should encapsulate label information, while non-causal features should reflect domain discrepancy.

In detail, we first introduce a feature extractor \(F()\), which removes the last classification layer in a popular neural network backbone (e.g., VGG-F ) to generate hidden features, i.e., \(=F()\). Then, two different MLPs \(g^{e}()\) and \(g^{n}()\) are utilized to generate two features, respectively, i.e., \(^{c}=g^{c}()\) and \(^{n}=g^{n}()\). Let \(^{c}\) and \(\) denote the random variables of causal features and semantic labels, and we maximize the mutual information between \(^{c}\) and \(\). To achieve this, we turn to InfoNCE [18; 3; 7], which construct positive pairs and negative pairs from the joint distribution \(p(^{c},)^{s}\) and the product of marginal distributions \(p(^{c})p()^{s}\), respectively. Since label information is not available in \(^{t}\), we merely utilize source data in this module. In particular, an estimator \(T^{c}\) is introduced to estimate the lower bound of mutual information, the target objective is written as:

\[^{c}_{MI}=_{p(^{c},)}[T^{c}(^{ c},)]-(_{p(^{c})p()}[e^{T^{c}( ^{c},)}]),\] (2)

where \(T^{c}(,)\) comes from a bi-linear function with a weight matrix \(^{c}\) to calculate the probability of being a positive pair, i.e., \(T^{c}(^{c},)=^{c}^{c}\). The estimator and the causal head \(g^{c}\) are optimized jointly to obtain effective causal features with high correlation to label information.

Moreover, to generate non-causal features, we minimize the mutual information between \(^{n}\) and \(\) with the maximum mutual information with hidden features \(\) following the principle of information bottleneck. In formulation, we need to minimize:

\[^{n}_{MI}=I(^{n},)- I(^{n},),\] (3)

where \(\) is a parameter to balance the loss. Similarly, it is infeasible to minimize Eqn. 3 directly. Therefore, we calculate the upper bound and lower bound of \(I(^{n},)\) and \(I(^{n},)\), respectively. Here, we first define:

\[(^{n},)=_{p(^{n},)}[ p(^{n})]-_{p(^{n})}_{p()} [ p(^{n})],\] (4)

which can be shown as an upper bound of \(I(^{n},)\). More detail is shown in Appendix. Moreover, we introduce a variational function \(q(y|^{n})\) to approximate \(p(y|^{n})\). To measure the lower bound of \(I(^{n},)\), we introduce a different estimator \(T^{n}(^{n},)=^{n}^{n}\) and the upper bound of \(I(^{n},)\) is written as follows:

\[(^{n},)=_{p(^{n},Z)}[T^{n}(^{n},)]-(_{p(^{n})p()}[e^{T^{n}( ^{c},)}]),\] (5)

By combining Eqn. 16 and Eqn. 5, we rewrite the objective in Eqn. 3 as:

\[^{n}_{MI}=_{p(^{n},)}[  q_{}(^{n})]-_{p(^{n})} _{p()}[ q(^{n})]\\ -(_{T^{n}}_{p(^{n},)}[T^{n}(^{n},)]-(_{p(^{n})p()}[e^{T^{n}( ^{n},)}])),\] (6)

From Eqn. 6, we utilize adversarial learning to optimize non-causal features, which generates reserve gradients for estimator \(T^{n}\) when minimizing \(^{n}_{MI}\). The overall loss for disentanglement is summarized as:

\[_{D}=^{c}_{MI}+^{n}_{MI}.\] (7)

By minimizing \(_{D}\), we can successfully disentangle causal and non-causal elements embedded in images, which can guide us to generate high-quality and interpretable hash codes.

### Consistency Learning for Discriminative Hash Codes

To learn discriminative hash codes, we leverage causal features given their strong correlation with label information for hash code generation. Here, consistency learning is introduced, based on the principle that similar images should be mapped to similar hash codes to enable effective image retrieval [4; 25]. This strategy ensures that our model can capture and preserve the inherent semantic relationships in the data, thus facilitating more accurate and effective retrieval.

In particular, we utilize an MLP \(()\) to map causal features to hash codes and enforce similar images to have similar binary codes. Here, we resort to consistency learning for optimization, which constructs positives via (1) two images sharing the same label in the source domain; (2) two augmented views from the same image in the target domain. In practice, we generate two augmented views for each image in a mini-batch \(=^{s}^{t}\), and define the positive set for source sample as:

\[(}_{i}^{s})=\{k|_{i}^{s}=_{j}^{s}\},\] (8)

where \(}_{i}^{s}\) denotes the augmented source sample. Then, we provide a hashing consistency learning objective for source domain:

\[_{CL}^{s}=-_{^{s}}}_{i}^{s})|}_{k(}_{i}^{s})}}_{i}}_{k}/)}{_{i=1}^{2|^{ s}|}(}_{i}^{s}}_{k^{}}^{s}/ )},\] (9)

where \(}_{i}^{s}=(g^{c}(F(}_{i}^{s})))\) and \(\) is a temperature parameter. Moreover, the hashing consistency learning objective in the target domain is:

\[_{CL}^{t}=-_{^{t}}}_{i}}_{k}/)}{_{i=1}^{2|^{s}|}(}_{i}^{s}}_{k^{}}^{s}/ )}.\] (10)

Finally, the consistency learning objective is derived by combining both source and target domains:

\[_{CL}=_{CL}^{s}+_{CL}^{t}.\] (11)

The introduction of consistency learning offers two primary benefits for hash code generation: (1) By maximizing the consistency of hash codes between similar images, our IDEA ensures the generation of similarity-preserving hash codes, which is essential for effective search engines. (2) By minimizing the similarity of hash codes derived from dissimilar images, our IDEA encourages hash codes to be uniformly distributed in the Hamming space, which enhances the capacity of each hash bit.

### Invariance under Intervention

However, in practice, causal parts and non-causal-part could be still closely related. For example, waterbirds are typically paired with water backgrounds in the training set. To further reduce the potential hidden effects of non-causal parts, we introduce invariant learning [28; 70; 6], which first introduce a generative model to do interventions and then encourages hash codes to be sufficiently invariant to different non-causal parts.

In detail, a generative model \(G(,)\) is first learnt to reconstruct original images using both causal and non-causal features. To optimize the generative model, the reconstruction loss in a mini-batch is written in the formulation of:

\[_{RE}=_{_{i}}||-G(g^{c}(F(_{i})),g^{n}(F(_{i})))||_{2}^{2}.\] (12)

With the well-trained model, we can simulate the intervention by utilizing different non-causal features. Here, we first generate synthetic images using \(_{i}^{c}\) and \(_{k}^{n}\):

\[_{i}^{+m}=_{i}|do(_{m})=G(_{i}^{c},_{m}^{m}),\] (13)

where \(do(_{m}^{n})\) denotes to impose non-causal features from a different sample \(_{m}^{n}\) in \(\). Then we feed the synthetic sample into the hashing network to generate hash codes, \(_{i}^{+m}=(g^{c}(F(_{i}^{+m})))\) and then minimize the variance under different intervention. The objective can be written as:

\[_{V}=_{_{i}}[Var_{m}(_{i}^{+m} )]=_{_{i},_{m}}||_{i}^{+m}-}_{i}||^{2},\] (14)

where \(}_{i}=_{_{m}}[_{i}^{+m}]\) is the mean of synthetic hash codes. With effective invariant learning, we can generate domain-invariant binary codes for effective cross-domain retrieval.

### Summary

Finally, we summarize our framework with the following overall training objective as:

\[=_{D}+_{CL}+_{RE}+_{V}.\] (15)

Moreover, the derivation of \(sign()\) is zero for any non-zero value, and therefore it is difficult to compatible with the gradient propagation. To tackle this challenges, we adopt \(tanh()\) to replace \(sign()\) during optimization, which produces approximate hash codes \(=tanh((g^{c}(F())))\) for training. Our model is first warmed up by the disentanglement module optimized by the reconstruction loss. Then we optimize the whole network with mini-batch stochastic gradient descent and the detailed progress of our IDEA can be found in Algorithm 1.

```
0: Source data \(^{s}\), target data \(^{s}\), code length \(L\);
0: The hashing network \((g^{c}(F()))\);
1: Warm up our backbone by minimizing \(_{D}\) and \(_{RE}\);
2:repeat
3: Sample a mini-batch \(B^{s}\) and \(B^{t}\) from \(^{s}\) and \(D^{t}\), respectively;
4: Generate two augmented views for each sample;
5: Calculate positive set by Eqn. 8;
6: Generate synthetic samples under intervention by Eqn. 13;
7: Calculate the final loss objective in Eqn. 15;
8: Reverse the gradients \(T^{n}\);
9: Update parameters by gradient descent;
10:until convergence ```

**Algorithm 1** Training Algorithm of IDEA

## 5 Experiment

### Experimental Settings

**Baselines.** We adopt a variety of state-of-the-art approaches for performance comparison, including six unsupervised hashing approaches (i.e., SH , ITQ , DSH , LSH , SGH , OCH ) and seven transfer hashing approaches (i.e., ITQ+ , LapITQ\(+\), GTH-g , DAPH , PWCF , DHLing , and PEACE ). Some results of ITQ\(+\) and LapITQ\(+\) are omitted because prior information is not accessible for sample pair construction.

**Datasets.** Experiments are conducted on different benchmark datasets: _(1) Office-Home dataset_: This dataset contains examples from four domains (Ar, Cl, Pr, Re), each with 65 object categories. Two domains are selected as the source and target, resulting in six transferable image retrieval tasks

    &  \\  Methods & P2R & C2R & R2A & R2P & R2C & A2R & A2D & A2W & W2D & D2A & W2A & D2W & Avg \\   & & & & & & & & & & & & & \\  SH  & 15.03 & 8.77 & 12.87 & 16.13 & 8.24 & 13.71 & 12.02 & 9.83 & 34.72 & 11.28 & 9.85 & 34.37 & 15.57 \\ ITQ  & 26.81 & 14.83 & 25.37 & 28.19 & 14.92 & 25.88 & 29.55 & 28.53 & 58.00 & 26.83 & 25.09 & 58.89 & 30.24 \\ DSH  & 8.49 & 5.47 & 9.67 & 8.26 & 5.28 & 6.99 & 16.66 & 15.09 & 39.24 & 16.33 & 13.58 & 41.07 & 15.74 \\ LSH  & 12.24 & 6.94 & 11.45 & 13.45 & 7.24 & 11.49 & 16.04 & 15.35 & 38.80 & 13.60 & 14.67 & 43.99 & 17.11 \\ SGH  & 24.51 & 13.62 & 22.53 & 25.73 & 13.51 & 22.93 & 24.98 & 22.47 & 53.94 & 22.17 & 20.52 & 56.36 & 26.94 \\ OCH  & 18.65 & 10.27 & 17.54 & 20.15 & 10.05 & 18.09 & 24.86 & 22.49 & 51.03 & 22.45 & 20.79 & 53.64 & 24.17 \\   & & & & & & & & & & & & & & \\  ITQ+  & 17.61 & 9.55 & 14.25 & - & - & - & 17.99 & 15.00 & 42.29 & - & - & - & 19.45 \\ LapITQ+  & 16.89 & 10.37 & 13.56 & - & & 19.96 & 18.24 & 43.32 & & & & & & & & & & & \\  GTH-g  & 20.00 & 10.99 & 18.28 & 21.95 & 11.68 & 19.05 & 23.08 & 21.20 & 49.38 & 19.52 & 17.41 & 50.14 & 23.56 \\ DAPH  & 27.20 & 15.29 & 27.35 & 28.19 & 15.29 & 26.37 & 32.80 & 28.66 & 60.71 & 28.66 & 27.59 & 64.11 & 31.85 \\ PWCF  & 34.03 & 24.22 & 28.95 & 34.44 & 18.42 & 34.57 & 39.78 & 34.86 & 67.94 & 35.12 & 35.01 & 72.91 & 38.35 \\ DHLing  & 48.47 & 30.81 & 38.68 & 45.52 & 25.15 & 43.30 & 41.96 & 45.10 & 75.23 & 42.89 & 41.74 & 79.91 & 46.54 \\ PEACE  & 53.04 & 38.72 & 42.68 & 54.39 & 28.36 & 45.97 & 46.69 & 48.89 & 78.82 & 46.91 & 46.95 & 83.18 & 51.22 \\ Ours & **59.18** & **45.71** & **49.64** & **61.84** & **32.77** & **51.19** & **48.70** & **54.43** & **84.97** & **53.53** & **53.71** & **88.69** & **57.03** \\   

Table 1: MAP performances on two bench-marking datasets with 64-bit hash codes.

(P2R, C2R, R2A, R2P, R2C, A2R). _(2) Office-31 dataset_: This widely used benchmark dataset contains over 4000 examples classified into 31 classes, which are from three domains (Am, We, Ds). We randomly select two domains as the source and target, resulting in six transferable image retrieval tasks (A2D, A2W, W2D, D2A, W2A, D2W). _(3) Digits dataset_: We focus on MNIST  and USPS , each containing ten handwritten digits. Each sample is re-scaled to 16\(\)16. We select one dataset as the source and the other as the target, resulting in two transferable image retrieval tasks (MNIST2USPS and USPS2MNIST).

**Implementation Details & Evaluation metrics.** We perform experiments on an A100-40GB GPU. 10% of the target examples are used as test queries and the remaining target and source samples are viewed as the training set. As for the database, source domain data is adopted in cross-domain retrieval while target domain data is adopted in single-domain retrieval. The hashing network is optimized using mini-batch SGD with momentum. The batch size is set to 36 and the learning rate is fixed as 0.001. We evaluate the retrieval performance using four common metrics: mean average precision (MAP), precision-recall curve, top-N accuracy curve, and top-N recall curve. MAP is the primary metric for evaluating retrieval accuracy, while the precision-recall curve provides an overall performance indicator at different recall levels. The top-N accuracy curve shows the accuracy for different numbers of retrieval instances. As the key metrics, MAP and precision-recall curve can reflect the overall performance of our model in retrieval tasks. We also analyze the top-N accuracy and recall curves to evaluate the performance at different number of retrieved results. These comprehensive evaluations allow us to have an in-depth understanding of the model's retrieval ability.

### Empirical Results

**Performance Comparison.** We present the results of our experiments on several benchmark datasets in Tables 1 and Table 2, where we report the mean average precision (MAP) scores achieved by our IDEA and other state-of-the-art methods. Our analysis of the results leads to the following conclusions. On the Office-Home and Office31 datasets, our IDEA achieves an outstanding average MAP score of 57.03, which significantly outperforms the second-best method, PEACE, with an average MAP score of 51.22. Furthermore, IDEA achieves the highest MAP scores across all query types, including P2R, C2R, R2A, R2P, R2C, A2R, A2D, A2W, W2D, D2A, W2A, and D2W. On the

    &  \\  Code Length & 16 & 32 & 48 & 64 & 96 & 128 & 16 & 32 & 48 & 64 & 96 & 128 & Avg \\   \\  SH  & 15.56 & 13.67 & 13.80 & 13.45 & 13.35 & 12.95 & 15.59 & 14.35 & 14.22 & 13.57 & 12.92 & 12.96 & 13.87 \\ ITO  & 13.05 & 15.57 & 18.54 & 20.12 & 23.12 & 23.89 & 13.69 & 17.51 & 20.40 & 20.30 & 22.79 & 24.59 & 19.46 \\ DSH  & 20.60 & 22.21 & 23.68 & 24.28 & 25.73 & 26.50 & 19.54 & 21.22 & 22.89 & 23.79 & 25.91 & 26.46 & 23.57 \\ LSH  & 12.40 & 13.54 & 15.89 & 16.01 & 18.54 & 20.44 & 12.76 & 14.86 & 14.77 & 16.89 & 16.32 & 19.67 & 16.01 \\ SGH  & 14.24 & 16.69 & 18.72 & 19.70 & 21.00 & 21.95 & 13.26 & 17.71 & 18.22 & 19.01 & 21.69 & 22.09 & 18.69 \\ OCH  & 13.73 & 17.22 & 19.59 & 20.18 & 20.66 & 23.34 & 15.51 & 17.75 & 18.97 & 21.50 & 21.27 & 23.68 & 19.45 \\   \\  ITQ+  & 22.84 & 21.20 & 20.68 & 19.15 & 17.99 & 18.52 & - & - & - & - & - & 20.06 \\ LapITQ+  & 24.26 & 24.03 & 23.76 & 24.59 & 23.33 & 22.73 & - & - & - & - & - & - & 23.78 \\ GTH-g  & 20.45 & 17.64 & 16.60 & 17.25 & 17.26 & 17.06 & 15.17 & 14.07 & 15.02 & 15.01 & 14.80 & 17.34 & 16.47 \\ DAPH  & 25.13 & 27.10 & 26.10 & 28.51 & 30.53 & 30.70 & 26.60 & 26.43 & 27.27 & 27.99 & 30.19 & 31.40 & 28.16 \\ PWFC  & 47.47 & 51.99 & 51.44 & 51.75 & 50.89 & 59.35 & 47.14 & 50.86 & 52.06 & 52.18 & 57.14 & 58.96 & 52.60 \\ DHing  & 49.24 & 54.90 & 56.30 & 58.28 & 58.80 & 59.14 & 50.14 & 51.35 & 53.67 & 58.65 & 58.42 & 59.17 & 55.67 \\ PEACE  & 52.87 & 59.72 & 60.69 & 62.84 & 65.13 & 68.16 & 53.97 & 54.82 & 58.69 & 60.91 & 62.65 & 65.70 & 60.51 \\ Ours & **58.89** & **64.48** & **65.72** & **67.48** & **70.24** & **74.34** & **60.99** & **61.47** & **65.45** & **67.97** & **69.72** & **72.31** & **66.59** \\   

Table 2: MAP performances on the Digits dataset with 64-bit hash codes.

Figure 3: We use 128-bit Precision-Recall curves to evaluate the performance of our method on the Office-31 and Office-Home datasets.

Digits dataset, IDEA consistently achieves the highest MAP scores for all hash code lengths, ranging from 16 to 128 bits. This demonstrates the consistency of our method across different code lengths. For instance, on the MNIST2USPS task, IDEA achieves an average MAP score of 74.34 for 128-bit codes, which is significantly higher than the second-best method, PEACE, with an average MAP score of 68.16. Overall, our experimental results demonstrate that the proposed IDEA is effective and outperforms state-of-the-art methods on various benchmark datasets. These findings highlight the potential of our method for practical applications in cross-domain image retrieval tasks. We present the recall-precision curves of four compared approches (PWCF, DHLing, PEACE, and IDEA) on two cross-domain retrieval tasks in Figure 3. Furthermore, in most cases, the curve of IDEA is higher than that of the other three baselines, indicating that the binary codes generated by IDEA works better for the hash table lookup strategy.

**Qualitative Results.** Here we provide a qualitative analysis of the model. Specifically, we use t-SNE visualization to display the hash codes learned by IDEA, PEACE, and DHLing (see the three subfigures on the left side of Figure 4). When compared to the PEACE and DHLing baseline methods, our IDEA-generated hash codes show more discriminative structures, with binary codes of different categories being better separated. These results indicate that the proposed method can produce hash codes with better discriminative power, which facilitates more effective image retrieval.

**Parameter Sensitivity.** In this part, we conduct sensitivity analysis for two important hyper-parameters, which is shown in the right subplot of the Figure 4. We can observe that the MAP values for all three cross-domain retrieval tasks slightly decrease as the \(\) parameter increases, indicating a negative impact on model performance. However, the decrease is not significant, allowing for an increase in \(\) to balance loss terms without causing a significant reduction in model performance. Similarly, the MAP values increase and then decrease as \(\) increases, demonstrating the potential for improved model performance. However, exceeding an optimal \(\) range may have a negative impact on retrieval performance. Thus, determining the optimal \(\) value through experimentation is necessary for achieving optimal retrieval performance in practical applications. In our experiments, we set \(\) and \(\) to \(0.1\) and \(0.5\), respectively.

**Ablation Study.** We have introduced several variants of our IDEA to investigate the impact of each components: (1) IDEA w/o \(_{RE}\) removes the original image reconstruction loss from the generated model, (2) IDEA w/o \(_{CL}\) removes the consistency learning loss, (3) IDEA w/o \(_{V}\) removes the hash code variance under intervention, and (4) IDEA w/o \(_{MI}\) removes the mutual information loss. By summarizing the results of these model variants in Table 3, We conclude, the ablation experiments show that removing any of the four components (i.e., \(_{RE}\), \(_{CL}\), \(_{V}\), \(_{MI}\)) leads to a drop in performance on all datasets and tasks. Notably, \(_{RE}\) and \(_{CL}\) have the most significant impact. Our IDEA achieves the best or comparable performance on all datasets and tasks, demonstrating its effectiveness in learning disentangled representations for domain generalization. Specifically, it achieves the highest MAP on Office-Home and Digits datasets and the highest accuracy on the A2W task of the Office31 dataset, outperforming the ablated variants.

Figure 4: Visualization of 128-bit hash codes on MNIST dataset in left three columns; Sensitivity analysis results of our proposed IDEA on Office-Home and Office-31 datasets are shown in right two columns on three cross-domain retrieval tasks.

   Dataset & Office-Home &  &  \\  Method & C2R & R2A & A2D & A2W & U2M & M2U \\  IDEA w/o \(_{RE}\) & 44.02 & 47.29 & 47.05 & 53.11 & 64.80 & 64.49 \\ IDEA w/o \(_{CL}\) & 42.11 & 45.97 & 45.36 & 51.17 & 64.25 & 63.76 \\ IDEA w/o \(_{V}\) & 45.10 & 49.09 & 48.21 & 53.96 & 57.34 & 66.97 \\ IDEA w/o \(_{MI}\) & 44.89 & 48.92 & 48.05 & 53.79 & 67.12 & 66.85 \\ Ours & 45.71 & 49.64 & 48.70 & 54.43 & 67.97 & 67.48 \\   

Table 3: Ablation Studies on six benchmark cross-domain retrieval tasks.

Conclusion

This work studies the problem of domain adaptive retrieval and proposes a novel method named IDEA to solve the problem. Our IDEA first disentangle causal and non-causal features within each image, guided by the principles of the information bottleneck. Moreover, these causal features are used to generate discriminative binary codes through consistency learning across both source and target domains. To generate domain-invariant binary codes, we simulates the intervention of various non-causal effects and encourage the invariance of hash codes. Extensive experiments on benchmark datasets validate the superiority of IDEA.

**Broader Impacts and Limitations.** This work improves the performance cross-domain retrieval, which can benefit real-world search engineers. Moreover, our work provide a new direction of incorporating invariant learning into cross-domain retrieval problems. One limitation of our work is that IDEA cannot tackle the open-set scenarios where target samples may come from unseen classes and we would extend our IDEA to more generalization scenarios in our future works. In particular, more advanced techniques such as modes in AICG and multimodal large-scale learning can be incorporated into our IDEA to improve the generalizability in a wider range of scenarios. In addition, we would explore more detailed visualization and interpretability analysis of invariant learning in the scope of cross-modal retrieval.