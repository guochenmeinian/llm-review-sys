# Prediction and Control in Continual Reinforcement Learning

Nishanth Anand

School of Computer Science

McGill University and Mila

nishanth.anand@mail.mcgill.ca &Doina Precup

School of Computer Science

McGill University, Mila, and Deepmind

dprecup@cs.mcgill.ca

corresponding author.

###### Abstract

Temporal difference (TD) learning is often used to update the estimate of the value function which is used by RL agents to extract useful policies. In this paper, we focus on value function estimation in continual reinforcement learning. We propose to decompose the value function into two components which update at different timescales: a _permanent_ value function, which holds general knowledge that persists over time, and a _transient_ value function, which allows quick adaptation to new situations. We establish theoretical results showing that our approach is well suited for continual learning and draw connections to the complementary learning systems (CLS) theory from neuroscience. Empirically, this approach improves performance significantly on both prediction and control problems.

## 1 Motivation

Deep reinforcement learning (RL) has achieved remarkable successes in complex tasks, e.g Go  but in a narrow setting, where the task is well-defined and stable over time. In contrast, humans continually learn throughout their life and adapt to changes in the environment and in their goals. This ability to continually learn and adapt will be crucial for general AI agents who interact with people in order to help them accomplish tasks. One possible explanation for this ability in the natural world is the existence of complementary learning systems (CLS): one which _slowly_ acquires structured knowledge and the second which _rapidly_ learns specifics adapting to the current situation . In contrast, in RL, learning is typically focused on how to optimize return, and the main estimated quantity is one value function (which then drives learning a policy). As a result, when changes occur in the environment, RL systems are faced with the _stability-plasticity dilemma_: whether to forget past predictions in order to learn new estimates or to preserve old estimates, which may be useful again later, and compromise the accuracy on a new task.

The stability-plasticity dilemma in RL is agnostic to the type of function approximation used to estimate predictions, and it is not simply a side effect of the function approximator having small capacity. In fact, the problem exists even when the RL agent uses a table to represent the value function. To see this, consider policy evaluation on a sequence of tasks whose true value function is different due to different rewards and transition probabilities. A convergent temporal-difference (TD) algorithm  aggregates information from all value functions in the task distribution implicitly, thereby losing precision in the estimate of the value function for the current situation. A tracking TD learning algorithm  learns predictions for the current task, overwriting estimates of past tasks and re-learning from scratch for each new task, which can require a lot of data. Using a separate function approximator for each task, which is a common approach , requires detecting the identity of the current task to update accordingly, which can be challenging and limits the number of tasks that can be considered.

As an alternative, we propose a CLS-inspired approach to the stability-plasticity dilemma which relies on maintaining two value function estimates: a _permanent_ one, whose goal is to accumulate "baseline" knowledge from the entire distribution of information to which the agent is exposed over time, and a _transient_ component, whose goal is to learn very quickly using information that is relevant to the current circumstances. This idea has been successfully explored in the context of RL and tree search in the game of Go , but the particular approach taken uses domain knowledge to construct features and update rules. Our approach is general, simple, online and model-free, and can be used in continual RL. Our method is also orthogonal to various advancements in the continual RL field, and therefore, it can be combined with other improvements. Our contributions are:

* A conceptual framework for defining permanent and transient value functions, which can be used to estimate value functions;
* RL algorithms for prediction and control using permanent and transient value functions;
* Theoretical results which help clarify the role of these two systems in the semi-continual (multi-task) RL setting; and
* Empirical case studies of the proposed approaches in simple gridworlds, Minigrid , JellyBeanWorld (JBW) , and MinAtar environments .

## 2 Background

Let \(\) be the set of possible states and \(\) the set of actions. At each timestep \(t\), the agent takes action \(A_{t}\) in state \(S_{t}\) according to its (stochastic) policy \(: Dist()\). As a consequence, the agent receives reward \(R_{t+1}\) and transitions to a new state \(S_{t+1}\). Let \(G_{t}=_{k=t}^{}^{k-t}R_{k+1}\) be the discounted return obtained by following \(\) from step \(t\) onward, with \(0<<1\) the discount factor. Let \(v_{}(s)=_{}[G_{t}|S_{t}=s]\). This quantity can be estimated with a function approximator parameterized by \(\), for example using TD learning :

\[_{t+1}_{t}+_{t}_{t}_{}v_{}(S_{t}),\] (1)

where \(_{t}\) is the learning rate at time \(t\), \(_{t}=R_{t+1}+ v_{}(S_{t+1})-v_{}(S_{t})\) is the TD error (see Sutton and Barto  for details).

In the control problem, the agent's goal is to find a policy, \(^{*}\), that maximizes expected returns. This can be achieved by estimating the optimal action-value function, \(q^{*}=max_{}q_{}\), where \(q_{}=_{}[G_{t}|S_{t}=s,A_{t}=a]\), e.g. using Q-learning .

"Continual RL" is closely related to transfer learning , meta learning [37; 15], multi-task learning , and curriculum learning . A thorough review distinguishing these flavours can be found in Khetarpal et al. . We use Continual RL to refer to a setup in which the agent's environment (i.e. the reward or transition dynamics) changes over time. We use the term _semi-continual RL_ for the case when the agent can observe task boundaries (this is similar to the multi-task setting, but does not make assumptions about the task distribution or task IDs). In continual RL, a unique optimal value function may not exist, so the agent has to continually _track_ the value function in order to learn useful policies [46; 2].

Our work focuses on the idea of two learning systems: one learning the "gist" of the task distribution, slowly, the other which is used for tracking can quickly adapt to current circumstances. We instantiate this idea in the context of learning value functions, both for prediction and control, but the idea can be applied broadly (for e.g. policies, GVFs). The concept of decomposing the value function into permanent and transient components was first introduced in model-based RL , for the single task of learning to play Go, and using samples drawn from the model to train the transient value function. In contrast, we focus on general continual RL, with samples coming from the environment.

In continual RL, most of the literature is focused on supplementing neural network approaches with better tools, such as designing new optimizers to update weights [44; 19; 12; 4; 14], building new architectures [21; 32], using experience replay to prevent forgetting [34; 36; 22; 8], promoting plasticity explicitly [28; 29], or using regularization techniques from continual supervised learning [25; 20; 30; 27]. Our method is agnostic to the nature of the function approximator used, and therefore, complementary to these advancements (and could in fact be combined with any of these methods).

Our approach has parallels with fast and slow methods [13; 7] which use neural networks as the main learning system and experience replay as a secondary system. Another similar flavor is to use the task 

[MISSING_PAGE_FAIL:3]

Note that this algorithm is a strict generalization of TD learning: instead of setting \(\) to \(0\) when the task changes, if we were to match the difference between the updated permanent value function and the transient value function instead, we would obtain exactly the TD learning predictions. Another way to get TD is by initializing \(\) to match the initialization of the TD learning algorithm and updating \(\) only without resetting. We explore this connection formally in the next section.

## 4 Theoretical Results

In this section, we provide some theoretical results aiming to understand the update rules of permanent and transient value function separately, and then we study the semi-continual RL setting (because the fully continual RL setting is poorly understood in general from a theoretical point of view, and even the tools and basic setup are somewhat contentious). All our results are for the prediction problem with tabular value function representation; We expect that prediction results with linear function approximation can also be obtained. We present theorems in the main paper and include proofs in appendices B. The appendix also contains analytical equations describing the evolution of the parameters based on these algorithms.

### Transient Value Function Results

We provide an understanding of what the transient value function does in the first set of results (Theorems 1-4). We carry out the analysis by fixing \(V^{(P)}\) and only updating \(V^{(T)}\) using samples from one particular task. We use \(V^{(TD)}_{t}\) and \(V^{(PT)}_{t}\) to denote the value function estimates at time \(t\) learned using TD learning and Alg. 1 respectively.

The first result establishes a relationship between the estimates learnt using TD and the transient value function update rule in Eq.(5).

**Theorem 1**.: _If \(V^{(TD)}_{0}=V^{(P)}\), \(V^{(T)}_{0}=0\) and the two algorithms train on the same samples using the same learning rates, then \( t,V^{(PT)}_{t}=V^{(TD)}_{t}\)._

The theorem shows that the estimates learnt by our approach match those learnt by TD-learning in a special case, proving that our algorithm is a strict generalization of TD-learning.

The next two theorems together shows that the transient value function reduces prediction error. First we show the contraction property of the transient value function Bellman operator.

**Theorem 2**.: _The expected target in the transient value function update rule is a contraction-mapping Bellman operator: \(^{(T)}V^{(T)}=r_{}+_{}V^{(P)}-V^{(P)}+ _{}V^{(T)}\), where \(r_{}\) is expected one-step reward for each state written in vector form and \(_{}\) is the transition matrix for policy \(\)._

We get the Bellman operator by writing the expected target in matrix form. We now characterize the fixed point of the transient value function operator in the next theorem.

**Theorem 3**.: _The unique fixed point of the transient operator is \((I-_{})^{-1}r_{}-V^{(P)}\)._

The theorem confirms the intuition that the transient value function learns only the part of the value function that is not captured in the permanent value function, while the agent still learns the overall value function: \(V^{(PT)}=V^{(P)}+V^{(T)}=V^{(P)}+(I-_{})^{-1}r_{}-V^{( P)}=v_{}\).

The next theorem relates the fixed point of the transient value function to the value function of a different Markov Reward Process (MRP)

**Theorem 4**.: _Let \(\) be an MDP in which the agent is performing transient value function updates for \(\). Define a Markov Reward Process (MRP), \(}\), with state space \(}:\{(S_{t},A_{t},S_{t+1}), t\}\), transition dynamics \(}(x^{}|x,a^{})=(s^{}| s^{},a^{}),\  x,x^{}}, s^{},s^{ }, a^{}\) and reward function \(}(x,a^{})=(s,a)+ V^{(P)}(s^{ })-V^{(P)}(s)\). Then, the fixed point of Eq.(5) is the value function of \(\) in \(}\)._The proof follows by comparing the value function of the modified MRP with the fixed point of transient value function. The theorem identifies the structure of the fixed point of the transient value function in the form of the true value function of a different MRP. This result can be used to design an objective for the permanent value function updates; For instance, to reduce the variance of transient value function updates.

### Permanent Value Function Results

We focus on permanent value function in the next two theorems. First, we characterize the fixed point of the permanent value function. Then, we connect the fixed point of the permanent value function to the jumpstart objective , which is defined as the initial performance of the agent in the new task before collecting any data. The latter is used to measure the effectiveness of transfer in a zero-shot learning setup, and therefore, is crucial to measure generalization under non-stationarity. We make an assumption on the task distribution for the following proofs:

**Assumption 1**.: Let MDP \(_{}=(,,_{},_{ },)\) denote task \(\). Then, \(v_{}\) is the value function of task \(\) and let \(_{}\) denote the expectation with respect to the task distribution. We assume there are \(N\) tasks and task \(\) is i.i.d. sampled according to \(p_{}\).

**Theorem 5**.: _Following Theorem 2, under assumption 1 and Robbins-Monro step-size conditions, the sequence of updates computed by Eq.(4) contracts to a unique fixed point \(_{}[v_{}]\)._

We use the approach described in Example 4.3 from Bertsekas and Tsitsiklis  to prove the result. Details are given in Appendix B.2.1

**Theorem 6**.: _The fixed point of the permanent value function optimizes the jumpstart objective, \(J=_{u^{|}}s_{}[ u-v_{ }_{2}^{2}]\)._

### Semi-Continual RL Results

We present two important results in this section which prove the usefulness of our approach in value function retention, for past tasks, and fast adaptability on new tasks. For the proofs, we assume that at time \(t\), both \(V_{t}^{(TD)}\) and \(V_{t}^{(PT)}\) have converged to the true value function of task \(i\) and the agent gets samples from task \(\) from timestep \(t+1\) onward.

The first result proves that the TD estimates forget the past after seeing enough samples from a new task, while our method retains this knowledge through \(V^{(P)}\).

**Theorem 7**.: _Under assumption 1, there exists \(k_{0}\) such that \( k k_{0}\), \(_{}[ V_{t+k}^{(TD)}-v_{i}_{2}^{2} ]>_{}[ V^{(P)}-v_{i}_{2}^{2}]\), \( i\)._

The proof uses the sample complexity bound from Bhandari et al.  (Sec. 6.3, theorem 1). Details are in Appendix B.3.1.

Next, we prove that our method has a tighter upper bound on errors compared with the TD learning algorithm in expectation.

**Theorem 8**.: _Under assumption 1, there exists \(k_{0}\) such that \( k k_{0}\), \(_{}[ v_{}-V_{t+k}^{(PT)}_{2}^{2}]\) has a tighter upper bound compared with \(_{}[ v_{}-V_{t+k}^{(TD)}_{2}^{2}]\), \( i\)._

The proof uses the sample complexity bound from Bhandari et al.  (Sec. 6.3, theorem 1) to get the desired result as detailed in Appendix B.3.2. As \(k\), the estimates of both the algorithms converge to the true value function and their upper bounds collapse to \(0\). This result doesn't guarantee low error at each timestep, but it provides an insight on why our algorithm has low error.

To further understand the behaviour of our algorithm on a new task, we derive analytical expressions for mean squared error similar to Singh and Dayan  for both our algorithm and TD-learning algorithm (see Appendix B.3.3 and B.3.4). Then, we use them to calculate the mean squared error empirically on a toy problem. We observe low error for our algorithm at every timestep for various task distributions (details in Appendix C.2).

Experiments: Semi-Continual Reinforcement Learning

We conducted experiments in both the prediction and the control settings on a range of problems. Although the theoretical results handle the prediction setting, we performed control experiments to demonstrate the broad applicability of our approach. In this section, we assume that the agent is faced with a sequence of tasks in which the transition dynamics are the same but the reward function changes. The agent knows when the task has changed. In the sections below, we focus on the effectiveness of our approach. Additional experiments providing other insights (effect of network capacity, sensitivity to hyperparameters, and effect of task distribution) are included in the Appendix (Sec. C.2, C.3, and C.4).

**Baselines:** Because our approach is built on top of TD-learning and Q-learning, we primarily use the usual versions of TD learning and Q-learning as baselines, which can be viewed as emphasizing stability. We also include a version of these algorithms in which weights are reset when the task switches, which creates significant plasticity. We include more baselines for the continual RL experiments, which are described in Sec. 6. The results for all algorithms are averaged across 30 seeds and shaded regions in the figures indicate 90% confidence intervals2. We use the same procedure to optimize hyperparameters for all algorithms, detailed in Appendix (Sec. C.4).

### Prediction experiments

We ran five experiments on 3 types of navigation tasks, with various function approximators. The policy being evaluated was uniformly random in all experiments. We used root mean squared value error (RMSVE) as a performance measure on the online task and the expected mean squared error (MSE) on the other tasks.

**Discrete grid:** is a 5x5 environment depicted in Fig. 0(a), which has four goal states (one in each corner). The agent starts in the center and chooses an action (up, down, left or right). Each action moves the agent one step along the corresponding direction. Rewards are 0 for all transitions except those leading into a goal state. The goal rewards change as detailed in Table C.1, so the value function also changes as shown in Fig. C.1. For the linear setting, each state is represented using 10 features: five for the row and five for the column. The permanent and transient value functions use the same features. For the deep RL experiment, instead of receiving state information, the agent receives 48x48 RGB images. The agent's location is indicated in red, and goal states are colored in green as shown in Fig. 0(a). Goal rewards are multiplied by 10 to diminish the noise due to the random initialization of the value function. For the tabular and linear setting, we run 500 episodes and change the goal rewards after every 50 episodes. For the deep RL experiments, we run 1000 episodes and goal rewards are switched every 100 episodes. The discount factor is 0.9 in all cases.

**Continuous grid:** is a continuous version of the previous task. The state space is the square \(\), with the starting state sampled uniformly from \([0.45,0.55][0.45,0.55]\). The regions within \(0.1\) units (in norm-1) from the corners are goal states. The actions change the state by \(0.1\) units along the corresponding direction. To make the transitions stochastic, we add noise to the new state,

Figure 1: Environments (PE).

Figure 2: Prediction results on various problems. Solid lines represent RMSVE on the current task and dotted lines represent MSE on other tasks. Black dotted vertical lines indicate task boundaries.

sampled uniformly from \([-0.01,0.01][-0.01,0.01]\). Rewards are positive only for transitions entering a goal region. The goal reward for each task is given in Table C.1. In the linear function approximation setting we convert the agent's state into a feature vector using the radial basis function as described in  and Appendix C.1.1. We run 2000 episodes, changing goal rewards every 200 episodes. For evaluation, we sample 225 evenly spaced points in the grid and estimate their true values by averaging Monte Carlo returns from 500 episodes across 10 seeds for each task. Here, \(=0.99\).

**Minigrid:** We used the four rooms environment shown in Fig. 0(b). Each room contains one type of item and each item type has a different reward and color. Goal states are located in all corners. The episode terminates when the agent reaches a goal state. The agent starts in the region between cells (3,3) and (6,6) and can move forward, turn left, or turn right. The agent perceives a 5x5 (partial) view in front of it, encoded one-hot. The rewards for picking items change as described in Table C.2 and the discount factor is \(0.9\). We run 750 episodes and change rewards every 75 episodes.

We provide the training and architecture details for the two deep RL prediction experiments in Appendix (Sec. C.1.1 and C.1.2) due to space constraints.

**Observations:** The results are shown in Fig. 2. Solid lines indicate performance on the task currently experienced by the agent (i.e., the _online_ task) and the dotted lines indicate the performance on the other tasks. The reset variant of TD-learning performs poorly because all learning progress is lost when the weights are reset, leading to the need for many samples after every task change. TD-learning also has high online errors immediately after a task switch, as its predictions are initially biased towards the past task. As the value function is updated using the online interaction data, the error on the current task reduces and the error on other tasks rises, highlighting the stability-plasticity problem when using a single value function estimate. Our algorithm works well on both performance measures. The permanent value function learns common information across all tasks, resulting in lower errors on the second performance measure throughout the experiment. Also, due to the good starting point provided by the permanent value function, the transient component requires little data to make adjustments, resulting in fast adaptation on the new task. In our method, the permanent component provides stability and the transient component provides plasticity, resulting in a good balance between the two. We also plotted the heatmap of the value estimates, shown in Fig. C.3, to understand the evolution of the predictions for TD-learning and our method which confirms the observations made so far.

### Control experiments

To test our approach as a complement to Q-learning 3, we conducted a tabular gridworld experiment and a Minigrid experiment using deep neural networks.

**Tabular experiment:** We use the 6x6 grid shown in Fig. 2(a), with two goals located in the top right corner. The agent starts in the bottom left corner and can choose to go up, down, left, or right. The action transitions the agent to the next state along the corresponding direction if the space is empty and keeps it in the same state if the transition would end up into a wall or obstacle. The action has the intended effect 90% of the time; otherwise, it is swapped with one of the perpendicular actions. The agent receives a reward of +1 and -1 for reaching the two goal states respectively, and the rewards swap periodically, creating two distinct tasks. The discount factor is \(0.95\). We run 500 episodes and change the task every 50 episodes.

**Minigrid experiment:** We use the two-room environment shown in Fig. 2(c). The agent starts in the bottom row of the bottom room and there are two goal states located in the top room. The goal

Figure 3: The environments and the corresponding results in the control setting. (b) Tabular results: episodic returns are plotted. (d) Deep RL results: returns smoothened across 10 episodes is plotted.

rewards are +5 and 0 respectively, and they flip when the task changes. We run 2500 episodes, alternating tasks every 500 episodes. The other details are as described in the previous section.Further details on the training and architecture are described in Appendix (Sec. C.1.3).

**Observations:** The results are shown in Fig. 3. The reset variant of Q-learning has to learn a good policy from scratch whenever the task changes, therefore, it requires many samples. Since, the goal rewards are flipped from one task to another, Q-learning also requires many samples to re-adjust the value function estimates. In our method, since the permanent component mixes action-values across tasks, the resulting bias is favorable, and makes it fairly easy to learn the corrections required to obtain overall good estimates. So, the transient component requires comparatively few samples to find a good policy when the task changes.

## 6 Continual Reinforcement Learning

So far, we assumed that the agent observes task boundaries; This information is usually not available in continual RL. One approach that has been explored in the literature is to have the agent infer, or learn to infer, the task boundaries through observations (e.g. see [17; 23]). However, these methods do not work well when task boundaries are not detected correctly, or simply do not exist. We will now build on the previous algorithm we proposed to obtain a general version, shown in Alg. 2, which is suitable for continual RL.

Our approach maintains the permanent and transient components, but updates them continually, instead of waiting for task changes. The permanent component is updated every \(k\) steps (or episodes) using the samples stored in the buffer \(\). And, instead of resetting the transient component, it is now decayed by a factor \(\). Here, \(k\) and \(\) are both hyperparameters. When \(=0\) and \(k\) the duration between task switches, we obtain the previously presented algorithm as a special case. In general, \(k\) and \(\) control the stability-plasticity trade off. Lower \(\) means that the transient value function forgets more, thus retaining more plasticity. Lower \(k\) introduces more plasticity in the permanent value function, as its updates are based on fewer and more recent samples. The algorithm still retains the idea of consolidating information from the transient into the permanent value function.

**The effect of \(k\) and \(\):** To illustrate the impact of the choice of hyperparameters, we use the tabular task shown in Fig. 2(a). The details of the experiment are exactly as before, but the agent does not know the task boundary. We run the experiment for several combinations of \(k\) and \(\) for the best learning rate. The results are shown in Fig. 4. As expected, we observe that the hyperparameters \(k\) and \(\) have co-dependent effects to a certain degree. For small values of \(k\), large values of \(\) yield better performance, because the variance in the permanent value function is mitigated, and the transient value function in fact is more stable. For large values of \(k\), the transient value function receives enough updates before the permanent value function is trained, to attain low prediction error. Therefore, the updates to the permanent value function are effective, and the transient predictions can be decayed more aggressively. More surprisingly, if the frequency of the permanent updates is significantly higher or significantly lower than the frequency of task changes, the overall performance is marginally affected, but remains better than for Q-learning (indicated using black dotted line) for most \(k\) and \(\) values. In fact, for each \(k\), there is at least one \(\) value for which the performance of the proposed algorithm exceeds the Q-learning baseline. So, in practice, we could fix one of these hyperparameters beforehand and tune only the second. We suspect that the two parameters could be unified into a single one, but this is left for future work.

```
1:Initialize: Buffer \(\), \(\), \(\), \(k\), \(\)
2:for\(t:0\)do
3: Take action \(A_{t}\)
4: Store \(S_{t}\), \(A_{t}\) in \(\)
5: Observe \(R_{t+1},S_{t+1}\)
6: Update \(_{t}\) using Eq. (8)
7:if\(mod(t,k)==0\)then
8: Update \(\) using \(\) and Eq. (7)
9:\(_{t+1}_{t+1}\)
10: Reset \(\)
11:endif
12:endfor ```

**Algorithm 2** PT-Q-learning (CRL)

### Experiments

We test our algorithm on two domains: JellyBeanWorld (JBW)  and MinAtar . We chose these domains because of the relatively lighter computational requirements, which still allow us to sweep a range of hyperparameters and to report statistical results averaged over 30 seeds. We include architecture and further training details in Appendix (Sec. C.1.5, C.1.6).

**JellyBeanWorld** is a benchmark to test continual RL algorithms .

The environment is a two-dimensional infinite grid with three types of items: blue, red, and green, which carry different rewards for different tasks as detailed in Sec. C.1.5. In addition to reward non-stationarity, the environment has spatial non-stationarity as shown in Fig. 4(a). The agent observes an egocentric 11x11 RGB image and chooses one of the four usual navigation actions. We run 2.1M timesteps, with the task changing every 150k steps. The discount factor is \(0.9\).

**MinAtar** is a standard benchmark for testing single-task RL algorithms . We use the breakout, freeway, and space invaders environments. We run for 3.5M steps and setup a continual RL problem, by randomly picking one of these tasks every 500k steps. We standardize the observations across tasks by padding with 0s when needed and we make all the actions available to the agent. This problem has non-stationarity in transitions, rewards and observations making it quite challenging. The discount factor is \(0.99\). More details are included in Appendix (Sec. C.1.6).

**Baselines:** We compare our method with DQN, two DQN variants, and a uniformly random policy. For the first baseline, we use a large experience replay buffer, an approach which was proposed recently as a viable option for continual RL . The second baseline, DQN with multiple heads, uses a common trunk to learn features and separate heads to learn Q-values for the three tasks. This requires knowing the identity of the current task to select the appropriate head. We use this baseline because it is a standard approach for multi-task RL, and intuitively, it provides an upper bound on the performance. This approach is not suitable if the number of tasks is very high, or if the problem is truly continual and not multi-task. For our approach, we use half the number of parameters as that of DQN for both permanent and transient value networks to ensure the total number of parameters across all baselines are same -- hence the name PT-DQN-0.5x.3

**Results** are presented in Fig. 5 and the pseudocode of PT-DQN is included in Appendix C.1.4. For JBW experiment, we plot the reward obtained per timestep over a 10k step window (reward rate) as a function of time. For MinAtar, we plot the return averaged over the past 100 episodes. Our method, PT-DQN-0.5x, performs better than all the baselines. The performance boost is large in the JBW experiment, where the rewards are completely flipped, and still favourable in the MinAtar experiment. DQN's performance drops over time when it needs to learn conflicting Q-values, as seen in the JBW results. DQN also needs a large number of samples to overcome the bias in its estimates when the task changes. Surprisingly, multi-headed DQN performs poorly compared to our method, despite having the additional task boundary information. The weights of the trunk are shared, and they vary continuously over time. Therefore, the weights at the head become stale compared to the features, which degrades performance. These findings may be different if the trunk were pre-trained and then fixed, with only the heads being fine-tuned, or if the networks were much bigger (which is however not warranted for the scale of our experiment). Augmenting DQN with a large experience replay is detrimental for general non-stationary problem, based on our experiments. This is because the samples stored in the replay buffer become outdated, so using these samples hampers the agent's ability to track environment changes.

Figure 5: (a) JellyBeanWorld task. (b) Results: JellyBeanWorld. (c) MinAtar tasks. (d) Results: MinAtar.

Discussion

Designing agents that can learn continually, from a single stream of experience, is a crucial challenge in RL. In this paper, we took a step in this direction by leveraging intuitions from complementary learning systems, and decomposing the value function into permanent and transient components. The permanent component is updated slowly to learn general estimates, while the transient component computes corrections quickly to adapt these estimates to changes. We presented versions adapted to both semi and fully continual problems, and showed empirically that this idea can be useful. The theoretical results provide some intuition on the convergence and speed of our approach.

The non-stationarity in our experiments is much more challenging, including arbitrary reward changes, as well as dynamics and observation changes, compared to other works, which use specific structure, or rely on a curriculum of tasks (thereby limiting non-stationarity). Our approach provided improvements in performance compared to the natural TD-learning and Q-learning baselines.

Our approach performs fast and slow interplay at various levels. The permanent value function is updated at a slower timescale (every \(k\) steps or at the end of the task); The transient value function is updated at a faster timescale (every timestep). The permanent value function is updated to capture some part of the value function from the agent's entire experience (generalization), which is a slow process; The transient value function adapts the estimates to a specific situation, which is a fast process. The permanent value function is updated using a smaller learning rate (slow), but the transient value function is updated using a larger learning rate (fast) .

### Limitations and Future Work

While we focused on illustrating the idea of permanent and transient components with value-based algorithms with discrete action spaces, the approach is general and could in principle be used with policy gradient approaches. One could use the proposed value function estimates to compute the gradient of the policy (represented as a separate approximator) in actor-critic methods. Another possibility is to have a "permanent" policy, which is corrected by a transient component to adapt to the task at hand. These directions would be interesting to explore. Our idea can be extended to learning General Value Functions  too.

Our theoretical analysis is limited to piece-wise non-stationarity, where the environment is stable for some period of time. An interesting direction would be to extend our approach to other types of non-stationarity, such as continuous wear and tear of robotic parts.

Another important future direction is to consider distinct feature spaces for the permanent and transient components, as done in Silver et al. . This would likely increase the ability to trade off stability and plasticity. Another direction to better control the stability and plasticity trade-off is by learning the frequency parameter \(k\) and the decay parameter \(\) using meta objectives ; We suspect there's a relation between the two parameters that can unify them into a single one.

Our approach can also be used to bridge offline pre-training and online fine-tuning of RL agents. The permanent value function can be learnt using offline data and these estimates can be adjusted by computing corrections using the transient value function using online interaction data.