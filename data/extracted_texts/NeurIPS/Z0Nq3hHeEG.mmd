# pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization

Matthew C. Bendel

Dept. ECE

The Ohio State University

Columbus, OH 43210

bendel.8@osu.edu

&Rizwan Ahmad

Dept. BME

The Ohio State University

Columbus, OH 43210

ahmad.46@osu.edu

&Philip Schniter

Dept. ECE

The Ohio State University

Columbus, OH 43210

schniter.1@osu.edu

###### Abstract

In ill-posed imaging inverse problems, there can exist many hypotheses that fit both the observed measurements and prior knowledge of the true image. Rather than returning just one hypothesis of that image, posterior samplers aim to explore the full solution space by generating many probable hypotheses, which can later be used to quantify uncertainty or construct recoveries that appropriately navigate the perception/distortion trade-off. In this work, we propose a fast and accurate posterior-sampling conditional generative adversarial network (cGAN) that, through a novel form of regularization, aims for correctness in the posterior mean as well as the trace and K principal components of the posterior covariance matrix. Numerical experiments demonstrate that our method outperforms contemporary cGANs and diffusion models in imaging inverse problems like denoising, large-scale inpainting, and accelerated MRI recovery. The code for our model can be found here: https://github.com/matt-bendel/pcaGAN.

## 1 Introduction

In image recovery, the goal is to recover the true image \(\) from noisy/distorted/incomplete measurements \(=()\). This arises in linear inverse problems such as denoising, deblurring, inpainting, and magnetic resonance imaging (MRI), as well as in non-linear inverse problems like phase-retrieval and image-to-image translation. For all such problems, it is impossible to perfectly recover \(\) from \(\).

In much of the literature, image recovery is posed as point estimation, where the goal is to return a single best estimate \(}\). However, there are several shortcomings of this approach. First, it's not clear how to define "best," since L2- or L1-minimizing \(}\) are often regarded as too blurry, while efforts to make \(}\) perceptually pleasing can sacrifice agreement with the true image \(\) and cause hallucinations , as we show later. Also, many applications demand not only a recovery \(}\) but also some quantification of uncertainty in that recovery .

As an alternative to point estimation, _posterior-sampling_-based image recovery  aims to generate \(P 1\) samples \(\{}_{i}\}_{i=1}^{P}\) from the posterior distribution \(p_{|}(|)\). Posterior sampling facilitates numerous strategies to quantify the uncertainty in estimating \(\), or any function of \(\), from \(\). It also can help with visualizing uncertainty and increasing robustness to adversarial attacks . That said, the design of accurate and computationally-efficient posterior samplers remains an open problem.

Given a training dataset of image/measurement pairs \(\{(_{t},_{t})\}_{t=1}^{T}\), our goal is to build a generator \(G_{}\) that, for a given \(\), maps random code vectors \((,)\) to samples of the posterior, i.e., \(}=G_{}(,) p_{|}(,)\). There are many ways to approximate the ideal generator. The recent literature has focused on conditional variational autoencoders (cVAEs) , conditionalnormalizing flows (cNFs) [8; 9; 10; 11], conditional generative adversarial networks (cGANs) [15; 16; 17; 18; 19], and Langevin/score/diffusion-based generative models [20; 21; 22; 23; 24; 25; 26; 27]. Although diffusion models have garnered a great share of recent attention, they tend to generate samples several orders-of-magnitude slower than their cNF, cVAE, and cGAN counterparts.

With the goal of fast and accurate posterior sampling, recent progress in cGAN training has been made through regularization. For example, Ohayon et al.  proposed to enforce correctness in the generated \(\)-conditional mean using a novel form of L2 regularization. Later, Bendel et al.  proposed to enforce correctness in the generated \(\)-conditional mean _and_ trace-covariance using L1 regularization plus a correctly weighted standard-deviation (SD) reward, and gave evidence that their "rcGAN" competes with contemporary diffusion samplers on MRI and inpainting tasks. Soon after, Man et al.  showed that L2 regularization and a variance reward are effective when training cGANs for JPEG decoding. More details are given in Section 2.

In a separate line of work, Nehme et al.  trained a Neural Posterior Principal Components (NPPC) network to directly estimate the eigenvectors and eigenvalues of the \(\)-conditional covariance matrix, which allows powerful insights into the nature of uncertainty in an inverse problem.

**Our contributions.** Inspired by regularized cGANs and NPPC, we propose a novel "pcaGAN" that encourages correctness in the \(K\) principal components of the \(\)-conditional covariance matrix, as well as the \(\)-conditional mean and trace covariance, when sampling from the posterior. We demonstrate that our pcaGAN outperforms existing cGANs [16; 17; 19; 29] and diffusion models [22; 25; 26; 27] on posterior sampling tasks like denoising, large-scale inpainting, and accelerated MRI recovery, while sampling orders-of-magnitude faster than those diffusion models. We also demonstrate that pcaGAN recovers the principal components more accurately than NPPC using approximately the same runtime.

## 2 Background

Our approach builds on the rcGAN regularization framework from , which itself builds on the cGAN framework from , both of which we now summarize. Let \(\), \(\), and \(\) denote the domains of \(\), \(\), and \(\), respectively. The goal is to design a generator \(_{}:\) where, for a given \(\), the random \(}=G_{}(,)\) induced by the code vector \( p_{}\) (with \(\) independent of \(\)) has a distribution that is close to the true posterior \(p_{|}(,)\) in the Wasserstein-1 distance, given by

\[W_{1}p_{|}(,),p_{|}(,)=_{D L_{1}}_{|} \{D(,)\}-_{}|}\{D( },)\}.\] (1)

Here, \(D:\) is a "critic" or "discriminator" that tries to distinguish between the true \(\) and generated \(}\) given \(\), and \(L_{1}\) denotes the set of 1-Lipschitz functions. A loss is constructed by averaging (1) over \( p_{}\), which takes the form

\[_{}\{W_{1}p_{|}(, ),p_{|}(,)\}=_{D L_{1 }}_{,,}\{D(,)-D(G_{}(,),)\},\] (2)

using the fact that the expectation commutes with the supremum . \(D\) is then implemented as a neural network \(D_{}\) with parameters \(\). Finally, \((,)\) are learned by minimizing

\[_{}(,)_{ ,,}\{D_{}(,)-D_{} (G_{}(,),)\}\] (3)

over \(\) and minimizing \(-_{}(,)+_{} ()\) over \(\), where \(_{}()\) is a gradient penalty that encourages \(D_{} L_{1}\). In practice, the expectation in (3) is approximated by a sample average over the training data \(\{(_{t},_{t})\}\).

In the typical case that the training data includes only a single image \(_{t}\) for each measurement vector \(_{t}\), minimizing \(_{}(,)\) alone does not encourage the generator to produce diverse samples. Rather, it leads to a form of mode collapse where the code \(\) is ignored. In , Adler and Oktem proposed a two-sample discriminator that encourages diverse generator outputs without compromising the Wasserstein objective (2). In , Bendel et al. instead proposed to regularize \(_{}(,)\) in a way that encourages correct posterior means and trace-covariances, i.e.,

\[_{}|} =_{|} _{}|} \{}|\} _{|}\{|\}\] (4) \[(_{}|}) =(_{|}) _{}|} \{}|\} _{|} \{|\}.\] (5)To do this,  replaced \(_{}(,)\) with the regularized adversarial loss

\[_{}(,)_{ }_{}(,)+_{1,P_{}}()-_{}_{,P_{}}(),\] (6)

which involves the \(P_{}\)-sample supervised-\(_{1}\) loss and standard-deviation (SD) reward terms

\[_{1,P}() _{_{1},,_{P}, }\{\|-}_{{}_{(P)}}\|_{1}\}\] (7) \[_{,P}() _{i=1}^{P}_{_{1},,_ {P},}\{\|}_{i}-}_{{}_{(P)}}\|_{1 }\},\] (8)

where typically \(P_{}=2\). Here, \(\{}_{i}\}\) are the generated samples and \(}_{{}_{(P)}}\) is their \(P\)-sample average:

\[}_{i} G_{}(_{i},)i=1,,P_{}}_{{}_{(P)}} _{i=1}^{P}}_{i}.\] (9)

The reward weight \(_{}\) in (6) is automatically adjusted to accomplish (5) during training . We note that the \(_{1,P}()\) regularization proposed in  is closely related to the regularization \(_{2,P}()_{,_{1}, ,_{P},}\{\|-}_{{}_{(P)}}\|_{2}^{2}\}\) proposed earlier by Ohayon et al. in . A detailed discussion of the advantages and disadvantages of various cGAN regularizations can be found in .

## 3 Proposed method

Whereas rcGAN aimed for correctness in the posterior mean and posterior trace-covariance statistics, our proposed pcaGAN _also_ aims for correctness in the \(K\) principal components of the posterior covariance matrix \(_{|}\), where \(K\) is user-selectable. To do this, pcaGAN adds two additional regularization terms to the rcGAN objective:

\[_{}(,) _{}(,)+ _{}_{}()+_{}_{}()\] (10) \[_{}() -_{}\{\,_{,_{1},,_{P}|}\{\,_{k=1}^{K}[}_{k}^{}}}}}}}}}}}}}}} )}) 1}\]\]\]\]\}\}\] (11)

Here, \(\{(}_{k},_{k})\}_{k=1}^{K}\) denote the principal eigenvectors and eigenvalues of the \(\)-dependent generated covariance matrix \(_{|}\) and \(\{(_{k},_{k})\}_{k=1}^{K}\) denote the principal eigenvectors and eigenvalues of the true covariance matrix \(_{|}\). Because (11) is the classical PCA objective , minimizing \(_{}()\) over \(\) will drive the generated principal eigenvector \(}_{k}\) towards the true principal eigenvector \(_{k}\) for each \(k=1,,K\). Likewise, minimizing \(_{}()\) over \(\) will drive the generated principal eigenvalue \(_{k}\) towards the true principal eigenvalue \(_{k}\) for each \(k=1,,K\). Based on our experiments, putting \(_{k}\) in the denominator works better than the numerator and the squared error in (12) works better than an absolute value.

In practice, the expectations in (11)-(12) are replaced by sample averages over the training data. In the typical case that the training data includes only a single image \(_{t}\) for each measurement vector \(_{t}\), the quantities \(_{|}\) and \(\{_{k}\}\) in (11)-(12) are unknown and non-trivial to estimate for each \(_{t}\). Hence, when training the pcaGAN, we approximate them with learned quantities. This must be done carefully, however. For example, if \(_{|}\) in (11) was simply replaced by the \(\)-dependent quantity \(_{|}\), then minimizing \(_{}()\) over \(\) would encourage \(_{|}\) to become overly large in order to drive \(_{}()\) to a large negative value.

Algorithm 1 details our proposed approach to training the pcaGAN. In particular, it describes the steps used to perform a single update of the generator parameters \(\) based on the training batch \(\{(_{k},_{b})\}_{b=1}^{B}\). Before diving into the details, we offer a brief summary of Algorithm 1. For the initial epochs, the rcGAN objective \(_{}\) alone is optimized, which allows the generated posterior mean \(_{|}\) to converge to the vicinity of \(_{|}\). Starting at \(E_{}\) epochs, the \(_{}()\) regularization from (11) is added, but with \(_{|}\) approximated as \((_{|})\). The use of \(\) forces \(_{}()\) to be minimized by manipulating the eigenvectors \(\{}_{k}\}_{k=1}^{K}\) and not the generated posterior mean \(_{|}\). These eigenvectors are computed using an SVD of centered approximate-posterior samples. To reduce the computational burden imposed by this SVD, a "lazy regularization"  approach is adopted, which computes \(_{}()\) only once every \(M\) training steps. Training proceeds in this manner until the eigenvectors \(\{}_{k}\}\) converge. Starting at \(E_{}\) epochs, the \(_{}()\) regularization from (12) is added, but with \(_{k}\) approximated as

\[_{k}}}\| }_{k}^{}}} }}}}}}}$}} \!}}_{k}^{}}}}$}}}}}}},}_{1}-_{|},, }_{P_{}}-_{|})^ {2},\] (13)where \(\) is used so that the optimization focuses on \(\{_{k}\}\). The rational behind (13) is that, when \(}_{k}=_{k}\) and \(_{|}=_{|}\), the terms \([}_{k}^{}(_{b}-_{|})]^ {2}\) and \([}_{k}^{}(}_{j}-_{| })]^{2}\)\( j\) all equal \(_{k}\) in \(_{b}\)-conditional expectation. This expectation is approximated using a \((1+P_{})\)-term sample average in (13) via the squared norm. The eigenvalues \(\{_{k}\}\) in (12) are computed using the previously described SVD and the regularization schedule is again \(M\)-lazy.

We now provide additional details on Algorithm 1. After the loss is initialized in line 1, the following steps are executed for each measurement vector \(_{b}\) in the batch. First, approximate posterior samples \(\{}_{i}\}_{i=1}^{P_{}}\) are generated in line 5, where \(P_{}=2\) as per the suggestion in . Using these samples, the adversarial component of the loss is added in line 6 and the rcGAN regularization is added in line 8. Starting at epoch \(E_{}\), lines 11-16 are executed whenever the training iteration is a multiple of \(M\). Nominally, \(E_{}\) is set where the validation PSNR of \(}\) (an empirical approximation of \(_{|}\)) stabilizes and \(M=100\). Within those lines, samples \(\{}_{j}\}_{j=1}^{P_{}}\) are generated in line 12 (where nominally \(P_{}=10K\)), their sample mean is computed in line 13, and the SVD of the centered samples is computed in line 14. The top \(K\) right singular vectors are then extracted in line 15 in order to construct the \(_{}()\) regularization, which is added to the overall generator loss \(()\) in line 16. Starting at epoch \(E_{}\), where nominally \(E_{}=E_{}+25\), lines 20-22 are executed whenever the training iteration is a multiple of \(M\). In line 20, the top \(K\) eigenvalues \(\{_{k}\}\) are constructed from the previously computed singular values and, in line 22, the \(_{}()\) regularization is constructedand added to the overall training loss. The construction of \(_{}()\) was previously described around (13). Finally, once the losses for all batch elements have been incorporated, the gradient \(()\) is computed using back-propagation and a gradient-descent step of \(\) is performed using the Adam optimizer  in line 26.

## 4 Numerical experiments

We now present experiments with Gaussian data, MNIST denoising, MRI, and FFHQ face inpainting. Additional implementation and training details for each experiment are provided in Appendix D.

### Recovering synthetic Gaussian data

Here our goal is to recover \((_{_{}},_{})^{d}\) from \(=+^{d}\), where \(\) masks \(\) at even indices and noise \((,^{2})\) is independent of \(\) with \(^{2}=0.001\). Since \(\) and \(\) are jointly Gaussian, the posterior is Gaussian posterior with \(_{|}=_{}+_{}_{}^{-1}(-_{})\) and \(_{|}=_{}-_{ }_{}^{-1}_{}\), where \(_{},_{},_{},_{}\) are marginal and \(_{},_{}\) are joint statistics.

We generate random \(_{}(,)\) and \(_{}\) with half-normal eigenvalues \(_{k}\) (see additional details in App. A), and consider a sequence of problem sizes \(d=10,20,30,,100\). For each \(d\), we generate 70 000 training, 20 000 validation, and 10 000 test samples. The generator and discriminator are simple multilayer perceptrons (see App. D.1) trained for 100 epochs with \(K=d\), \(E_{}=10\), \(_{}=10^{-5}\), and \(_{}=10^{-2}\).

**Competitors.** We compare the proposed pcaGAN to rcGAN  and NPPC . rcGAN uses the same generator and discriminator architectures as pcaGAN and is trained according to (6) with \(_{}=10^{-5}\) and \(P_{}=2\). For NPPC, we use the authors' implementation  with \(K=d\) and some minor modifications to work with vector data. To evaluate performance, we the Wasserstein-2 (W2) distance between \(p_{|}\) and \(|}}\), which in the Gaussian case reduces to

\[_{2}(p_{|},|} })=\|_{|}-_{|}}\|_{2}^{2}+_{|}+ _{|}}-2(_{| }^{}{{2}}}_{|}}_{|}^{}{{2}}})^{}{{2}}} .\] (14)

For the cGANs, we compute \(_{|}}\) and \(_{|}}\) empirically from \(10d\) samples, while for NPPC we use the conditional mean, eigenvalues, and eigenvectors returned by the approach.

**Results.** Figure 0(a) examines the impact of the lazy update period \(M\) on pcaGAN's W2 distance at \(d=100\) with \(K=d\). Based on this figure, to balance performance with training overhead, we set \(M=100\) for all future experiments. Figure 0(b) examines the impact of \(K\) on W2 distance for the pcaGAN with \(d=100\). It shows that using \(K<d\) causes a relatively mild increase in W2 distance, as expected due to the half-normal distribution on the true eigenvalues \(_{k}\). Figure 0(c) shows that the proposed pcaGAN outperforms rcGAN and NPPC in W2 distance for all problem sizes \(d\).

### MNIST denoising

Now our goal is to recover an MNIST digit \(^{28 28}\) from noisy measurements \(=+\) with \((,)\). We randomly split the MNIST training fold into \(50\,000\) training and \(10\,000\)

Figure 1: Gaussian experiment. Wasserstein-2 distance versus (a) lazy update period \(M\) for pcaGAN with \(d=100=K\), (b) estimated eigen-components \(K\) for pcaGAN with \(d=100\) and \(M=100\), and (c) problem dimension \(d\) for all methods under test with \(K=d\) and \(M=100\).

validation images, and we use the entire MNIST fold set for testing. For pcaGAN and rcGAN, we use a UNet  generator and the encoder portion of the same UNet followed by one dense layer as the discriminator. pcaGAN was trained for 125 epochs with \(E_{}=25\), \(_{}=10^{-5}\), \(_{}=10^{-1}\), and \(K\{5,10\}\).

**Competitors.** We again compare the proposed pcaGAN to rcGAN and NPPC. For rcGAN we used the same generator and discriminator architectures as pcaGAN and trained according to (6) with \(_{}=10^{-5}\) and \(P_{}=2\). For NPPC, we used the authors' MNIST implementation from .

Following the NPPC paper , we evaluate performance using root MSE (rMSE) \(_{,}\{\|-_{}} \|_{2}\}\) and residual error magnitude (\(_{5}\)) \(_{,}\{\|(-}_{5}}_{5}^{})\|_{2}\}\), where \(=-_{}}\) and \(}_{5}\) is an \(28^{2} 5\) matrix whose \(k\)th column equals the \(k\)th principal eigenvector \(}_{k}\). For the cGANs, we use \(_{}}=}_{(P)}\) and compute \(\{}_{k}\}\) from the SVD of a matrix of centered samples \(\{}_{i}\}_{i=1}^{P}\), both with \(P=100\). For NPPC, we use the conditional means and eigenvectors returned by the approach. For performance evaluation, we also consider Conditional Frechet Inception Distance (CFID)  with InceptionV3 features. CFID is analogous to Frechet Inception Distance (FID)  but applies to conditional distributions (see App. B for more details).

**Results.** Table 1 shows rMSE, \(_{5}\), CFID, and the reconstruction time for a batch of 128 images on the test fold. (NPPC does not generate image samples and so CFID does not apply.) The table shows that the proposed pcaGAN wins in all metrics, except for rMSE where NPPC wins. This is not surprising because NPPC computes \(_{}}\) using a dedicated network trained to minimize MSE loss. NPPC also generates its eigenvectors slightly quicker than pcaGAN generates samples. Table 1 also shows that pcaGAN performance improves as \(K\) increases from 5 to 10, despite the fact that \(_{5}\) uses only the top 5 eigenvectors. Figure E.1 shows examples of the 5 principal eigenvectors and posterior mean learned by pcaGAN and NPPC. The eigenvectors of pcaGAN are more structured and less noisy than those of NPPC. Figure E.1 also shows \(_{}}+_{k}\) for \([-3,3]\) and \(k\{1,4\}\). Additional figures can be found in App. E.1.

### Accelerated MRI

We now consider accelerated MRI, where the goal is to recover a complex-valued multicoil image \(\) from masked frequency-domain (i.e., "k-space") measurements \(\). To build the image data \(\{_{t}\}\), we follow the approach in the rcGAN paper , which uses the first 8 slices of lastMRI  T

   Model & rMSE\(\) & REM\(\) & CFID\(\) & Time(128)\(\) \\  NPPC (Nehme et al. ) & **3.94** & 3.63 & – & **112 ms** \\ rcGAN (Bendel et al. ) & 4.04 & 3.41 & 63.44 & 118 ms \\ pcaGAN (Ours, \(K=5\)) & 4.02 & 3.31 & 61.48 & 118 ms \\ pcaGAN (Ours, \(K=10\)) & 4.02 & **3.25** & **60.16** & 118 ms \\   

Table 1: Average MNIST denoising results.

brain volumes with at least 8 coils, crops to \(384 384\) pixels, and compresses to 8 virtual coils . This yields 12 200 training, 2 376 testing, and 784 validation images. To create each \(}\), we transform \(_{t}\) to the k-space, subsample using the Cartesian GRO mask  at accelerations \(R=4\) and \(R=8\), and transform the zero-filled k-space measurements back to the image domain.

We train pcaGAN for 100 epochs with \(K=1\), \(E_{}=25\), \(_{}=10^{-5}\), and \(_{}=10^{-2}\) and select the final model using validation CFID computed with VGG-16 features.

**Competitors.** We compare the proposed pcaGAN to rcGAN , pscGAN , Adler & Oktem's cGAN , the Langevin approach , and the E2E-VarNet . All cGANs use the generator and discriminator architectures from  and enforce data-consistency . For rcGAN and the Langevin approach, we did not modify the authors' implementations from  and  except to use the GRO sampling mask. For E2E-VarNet, we use the GRO mask, hyperparameters, and training procedure from .

Following , we convert the multicoil outputs \(}_{t}\) to complex-valued images using SENSE-based coil combining  with ESPIRiT-estimated  coil sensitivity maps, and compute performance on magnitude images. All feature-based metrics (CFID, FID, LPIPS, DISTS) were computed with AlexNet features to show that pcaGAN does not overfit to the VGG-16 features used for validation. It was shown in  that image-quality metrics computed using ImageNet-trained feature generators like AlexNet and VGG-16 perform comparably to metrics computed using MRI-trained feature generators in terms of correlation with radiologists' scores.

**Results.** Table 2 shows CFID, FID, \((_{i=1}^{P}\|}_ {(P)}-}_{t}\|^{2})^{1/2}\), and 4-sample generation time for the methods under test. Due to its slow sample-generation time, we evaluate the CFID, FID, and APSD of the Langevin technique  using the 72-image test from . But due to the bias of CFID at small sample sizes , we evaluate the other methods using all \(2\,376\) test images (CFID\({}^{2}\)) and again using all \(14\,576\) training and test images (CFID\({}^{3}\)). Table 2 shows that pcaGAN yields better CFID and FID than the competitors. All cGANs generated samples 3-4 orders-of-magnitude faster than the Langevin approach .

    &  &  \\   & \(^{}\) & \(^{}\) & \(^{}\) & \(\) & \(\) & Time (4\({}_{1}\)) & \(^{}\) & \(^{}\) & \(^{}\) & \(\) & \(\) & Time (4\({}_{3}\)) \\  E2E-VarNet (Sriram et al. ) & 16.08 & 13.07 & 10.26 & 38.88 & 0.0 & 310ms & 38.66 & 29.90 & 23.82 & 44.40 & 0.0 & 316ms \\ Langevin (Jalalal et al. ) & 33.05 & – & - & 31.43 & 5.96 & 41 & min & 48.59 & – & - & 52.62 & 7.66 & 14 min \\ cGAN (Adler \& Oktem ) & 19.00 & 12.75 & 7.00 & 29.77 & 3.96 & **217 ms** & 59.94 & 40.24 & 26.10 & 31.81 & 7.76 & **217 ms** \\ pcaGAN (Ohraves et al. ) & 13.74 & 10.56 & 7.53 & 37.28 & 7.26 & 24.27 & **217 ms** & 39.67 & 31.81 & 24.06 & 43.39 & 7.76 & 217 ms \\ rGAN (Bendel et al. ) & 9.71 & 5.27 & 1.69 & 25.62 & 3.86-6 & **217 ms** & 24.04 & 13.20 & 3.83 & 28.43 & 7.66-6 & **217 ms** \\ pcaGAN (Ohraves) & **8.78** & **4.48** & **1.29** & **25.02** & 4.46-6 & **217 ms** & **21.65** & **11.47** & **3.21** & **28.35** & 6.5e-6 & **217 ms** \\   

Table 2: Average MRI results at acceleration \(R\{4,8\}\)

Figure 3: Example MRI recoveries at \(R=8\). Arrows highlight meaningful variations.

Table 3 shows PSNR, SSIM, LPIPS , and DISTS  for the \(P\)-sample average \(}_{{}_{(P)}}\) at \(P\{1,2,4,8,16,32\}\) and \(R=8\). (See App. C for \(R=4\).) It has been shown that DISTS correlates particularly well with radiologist scores . The E2E-VarNet achieves the best PSNR, but the proposed cGAN achieves the best LPIPS and DISTS when \(P=2\) and the best SSIM when \(P=8\). This \(P\)-dependence is related to the perception-distortion trade-off  and consistent with that reported in .

Figure 3 shows zoomed versions of two recoveries \(}_{i}\) and the sample average \(}_{{}_{(P)}}\) with \(P=32\). Appendices E.2 and E.3 show additional plots of \(}_{{}_{(P)}}\) that visually demonstrate the perception-distortion trade-off.

### Large-scale inpainting

Our final goal is to inpaint a face image with a large randomly generated masked region. For this task, we use 256 \(\) 256 FFHQ face images  and the mask generation procedure from . We randomly split the FFHQ training fold into 45 000 training and 5 000 validation images, and we use the remaining 20 000 images for testing.

For pcaGAN, we use CoModGAN's  generator and discriminator architecture and train for 100 epochs using \(K=2\), \(E_{}=25\), \(_{}=5 10^{-3}\), and \(_{}=10^{-3}\).

**Competitors.** We compare with CoModGAN , pscGAN , rcGAN , and state-of-the-art diffusion methods DDRM (20 NFEs) , DDNM (100 NFEs) , and DPS (1000 NFEs) . CoModGAN, pscGAN, and rcGAN differ from pcaGAN only in generator regularization and CoModGAN's use of discriminator MBSD . For rcGAN, DDNM, DDRM, and DPS, we use the authors' implementations from , , , and  with mask generation from . FID and CFID were evaluated on our 20 000 image test set with \(P\!=\!1\).

**Results.** Table 4 shows test CFID, FID, LPIPS, and 40-sample generation time. The table shows that the proposed pcaGAN wins in CFID, FID and LPIPS, and that the four cGANs generate samples 3-4 orders-of-magnitude faster than DPS. Figure 4 shows five generated samples for each method under test, along with the true and masked image. pcaGAN shows better subjective quality than the competitors, as well as good diversity. Additional figures can be found in App. E.4.

   Model & CFID \(\) & FID\(\) & LPIPS\(\) & Time (40 samples)\(\) \\  DPS (Chung et al. ) & 7.26 & 2.00 & 0.1245 & 14 min \\ DDNM (Wang et al. ) & 11.30 & 3.63 & 0.1409 & 30 s \\ DDRM (Kawar et al. ) & 13.17 & 5.36 & 0.1587 & 5 s \\ pscGAN (Dhayon et al. ) & 18.44 & 8.40 & 0.1716 & **325 ms** \\ CoModGAN (Zhao et al. ) & 7.85 & 2.23 & 0.1290 & **325 ms** \\ rcGAN (Bendel et al. ) & 7.51 & 2.12 & 0.1262 & **325 ms** \\ pcaGAN (Ours) & **7.08** & **1.98** & **0.1230** & **325 ms** \\   

Table 4: Average FFHQ inpainting results.

    &  &  \\ Model & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) \\  E2E-VarNet (Sriram et al. ) & **36.49** & - & - & - & - & - & 0.9220 & - & - & - & - & - \\ Langevin (Hall et al. ) & 32.17 & 32.83 & 34.55 & 33.74 & 33.38 & 33.90 & 87.25 & 0.8919 & 0.9031 & 0.9019 & 0.9110 & 0.9120 & 0.9137 \\ cGAN (Aaler \& Okorn ) & 31.31 & 32.31 & 32.92 & 33.26 & 33.42 & 33.51 & 0.8865 & 0.9045 & 0.9103 & 0.9111 & 0.9102 & 0.9095 \\ pscGAN (Ohayon et al. ) & 34.89 & 34.90 & 34.90 & 34.90 & 34.90 & 34.90 & 34.92 & 0.9227 & 0.9217 & 0.9213 & 0.9211 & 0.9212 & 0.9210 \\ rcGAN (Bendel et al. ) & 32.32 & 33.67 & 34.53 & 35.01 & 35.27 & 35.42 & 0.9030 & 0.9199 & 0.9252 & 0.9257 & 0.9251 & 0.9246 \\ pcaGAN (Ours) & 33.28 & 34.47 & 35.20 & 35.61 & 35.82 & 35.94 & 0.9136 & 0.9257 & **0.9283** & 0.9275 & 0.9262 & 0.9253 \\  &  &  \\ Model & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) \\  E2E-VarNet (Sriram et al. ) & 0.0575 & - & - & - & - & 0.1253 & - & - & - & - & - \\ Langevin (Hall et al. ) & 0.0769 & 0.0619 & 0.0579 & 0.0589 & 0.0611 & 0.0611 & 0.1341 & 0.1136 & 0.1086 & 0.1119 & 0.1175 & 0.1212 \\ cGAN (Aaler \& Okorn ) & 0.0698 & 0.0614 & 0.0623 & 0.0667 & 0.0704 & 0.0727 & 0.1407 & 0.1262 & 0.1252 & 0.1291 & 0.1334 & 0.1361 \\ pscGAN (Ohayon et al. ) & 0.0532 & 0.0536 & 0.0539 & 0.0540 & 0.0534 & 0.0540 & 0.1128 & 0.1143 & 0.1151 & 0.1155 & 0.1157 & 0.1158 \\ rcGAN (Bendel et al. ) & 0.0418 & 0.0379 & 0.0421 & 0.0476 & 0.0516 & 0.0539 & 0.0906 & 0.0877 & 0.0965 & 0.1063 & 0.1135 & 0.1177 \\ pcaGAN (Ours) & 0.0358 & **0.0344** & 0.0391 & 0.0442 & 0.0479 & 0.0499 & 0.0804 & **0.0799** & 0.0920 & 0.1026 & 0.1099 & 0.1144 \\   

Table 3: Average PSNR, SSIM, LPIPS, and DISTS of \(}_{{}_{(P)}}\) versus \(P\) for MRI at \(R=8\)

## 5 Discussion

When training a cGAN, the overall goal is that the samples \(\{}_{i}\}\) generated from a particular \(\) accurately represent the true posterior \(p_{|}(|)\). Achieving this goal is challenging when training from paired data \(\{(_{t},_{t})\}\), because such datasets provide only one example of \(\) for each given \(\). Early methods like  focused on providing _some_ variation among \(\{}_{i}\}\), but did not aim for the correct variation. The rcGAN from  focused on providing the correct _amount_ of variation by enforcing \((_{|})=( _{|})\), and the proposed pcaGAN goes farther by encouraging \(_{|}\) and \(_{|}\) to agree along \(K\) principal directions. Our experiments demonstrate that pcaGAN yields a notable improvement over rcGAN and outperforms contemporary diffusion approaches like DPS .

PCA principles have also been used in _unconditional_ GANs, where the goal is to train a generator \(G_{}\) that turns codes \((,)\) into outputs \(}=G_{}()\) that match the true marginal distribution \(p_{}\) from which the training samples \(\{_{t}\}\) are drawn. For example, the eigenGAN from  aims to train in such a way that semantic attributes are learned (without supervision) and can be independently controlled by manipulating individual entries of \(\). But their goal is clearly different from ours.

Limitations.We acknowledge several limitations of our work. First, generating \(P_{}=10K\) samples during training can impose a burden on memory when \(\) is high dimensional. In the multicoil MRI experiment, \(^{d}\) for \(d=2.4e6\), which limited us to \(K=1\) at batch size 2. Second, although our focus is on designing a fast and accurate posterior sampler, more work is needed on how to best use the generated samples across different applications. Using them to compute rigorous uncertainty

Figure 4: Example of inpainting a randomly generated mask on a \(256\!\!256\) FFHQ face image.

intervals seems like a promising direction . Third, the application to MRI is preliminary; additional tuning and validation is needed before it can be considered for clinical practice.

## 6 Conclusion

In this work, we proposed pcaGAN, a novel image-recovery cGAN that enforces correctness in the \(K\) principal components of the conditional covariance matrix \(_{|y}\), as well as in the conditional mean \(_{|y}\) and trace-covariance \((_{|y})\). Experiments with synthetic Gaussian data showed pcaGAN outperforming both rcGAN  and NPPC  in Wasserstein-2 distance across a range of problem sizes. Experiments on MNIST denoising, accelerated multicoil MRI, and large-scale image inpainting showed pcaGAN outperforming several other cGANs and diffusion models in CFID, FID, PSNR, SSIM, LPIPS, and DISTS metrics. Furthermore, pcaGAN generates samples 3-4 orders-of-magnitude faster than the tested diffusion models. The proposed pcaGAN thus provides fast and accurate posterior sampling for image recovery problems, which enables uncertainty quantification, fairness in recovery, and easy navigation of the perception/distortion trade-off.