# Contextual Bandits and Imitation Learning with Preference-Based Active Queries

Ayush Sekhari

MIT

sekhari@mit.edu

&Karthik Sridharan

Cornell University

ks999@cornell.edu

&Wen Sun

Cornell University

ws455@cornell.edu

&Runzhe Wu

Cornell University

rw646@cornell.edu

Authors are listed in alphabetical order of their last names.

###### Abstract

We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as \(O(\{,d/\})\), where \(T\) represents the number of interactions, \(d\) represents the eluder dimension of the function class, and \(\) represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of \(\), and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only \(O(\{T,d^{2}/^{2}\})\) queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length \(H\), and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms (Ross and Bagnell, 2014) that require access to the expert's actions and also reward signals.

## 1 Introduction

Human feedback for training machine learning models has been widely used in scenarios including robotics (Ross et al., 2011, 2013; Jain et al., 2015; Laskey et al., 2016; Christiano et al., 2017) and natural language processing (Stiennon et al., 2020; Ouyang et al., 2022). By integrating human feedback into the training process, these prior works provide techniques to align machine-learning models with human intention and enable high-quality human-machine interaction (e.g., ChatGPT).

Existing methods generally leverage two types of human feedback. The first is the action from human experts, which is the dominant feedback mode in the literature of imitation learning and learning from demonstrations (Abbeel and Ng, 2004; Ziebart et al., 2008; Daume et al., 2009; Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017; Osa et al., 2018; Li et al., 2023). The second type of feedback is preference-based feedback, which involves comparing pairs of actions. In this approach, the expert provides feedback by indicating their preference between two options selected by the learner. While both types of feedback have their applications, our focus in this work is on preference-based feedback, which is particularly suitable for scenarios where it is challenging for human experts to recommend the exact optimal action while making pairwise comparisons is much easier.

Learning via preference-based feedback has been extensively studied, particularly in the field of _dueling bandits_(Yue and Joachims, 2011; Yue et al., 2012; Zoghi et al., 2014; Ailon et al., 2014; Komiyama et al., 2015; Wu and Liu, 2016; Saha and Gaillard, 2021; Benggs et al., 2021; Saha and Gaillard, 2022) and _contextual dueling bandits_(Dudik et al., 2015; Saha, 2021; Saha and Krishnamurthy, 2022; Benggs et al., 2022; Wu et al., 2023). Different from the standard bandit setting, the learner proposes two actions in dueling bandits and only gets noisy preference feedback from the human expert. Follow-up works extend the preference-based learning model from the one-step bandit setting to the multi-step decision-making (e.g., imitation learning and reinforcement learning) setting (Chu and Ghahramani, 2005; Sadigh et al., 2017; Christiano et al., 2017; Lee et al., 2021; Chen et al., 2022; Saha et al., 2023). These studies mainly focus on learning a high-quality policy from human feedback, without concerning the question of active query in order to minimize the query complexity.

However, query complexity is an important metric to optimize when learning from human feedback, as human feedback is expensive to collect (Lightman et al., 2023). For instance, InstructGPT (Ouyang et al., 2022) is trained only on around 30K pieces of human feedback, which is significantly fewer than the internet-scale dataset used for pre-training the base model GPT3, indicating the challenge of scaling up the size of human feedback datasets. In other areas, such as robotics, learning from human feedback is also not easy, and prior studies (e.g., Cohn et al. (2011); Zhang et al. (2022); Myers et al. (2023)) have explored this issue from various perspectives. Ross et al. (2013); Laskey et al. (2016) pointed out that querying human feedback in the learning loop is challenging, and extensively querying for feedback puts too much burden on the human experts.

In this work, we design _principled algorithms that learn from preference-based feedback while at the same time minimizing query complexity_ under the settings of contextual bandits (Auer et al., 2002) and imitation learning (Ross et al., 2011). Our main contributions can be summarized as follows.

* in short of _Active preference qUeRy fOR contextual bAndits_) that achieves a best-of-both-worlds regret bound (i.e., achieving the minimum of the worst-case regret and an instance dependent regret), while at the same providing an instance-dependent query complexity bound. For benign instances with small eluder dimension and large gap, our regret and query complexity bounds both scale with \((T)\) where \(T\) is the total number of interactions in contextual bandits.
* a much stronger feedback mode than ours.

To the best of our knowledge, for both contextual bandit and imitation learning with preference-based feedback, our algorithms are the first to achieve best-of-both-worlds regret bounds via active querying.

### Related works

**Selective Sampling.** Numerous studies have been conducted on selective sampling across various settings (Cesa-Bianchi et al., 2005; Dekel et al., 2012; Agarwal, 2013; Hanneke and Yang, 2015,2021; Zhu and Nowak, 2022), with the work of Sekhari et al. (2023) being closest to ours. Sekhari et al. (2023) presented a suite of provably efficient algorithms that are applicable to settings including contextual bandits and imitation learning. The primary distinction between our setting and the prior works lies in the feedback modality-we assume preference-based feedback, whereas they assume direct label feedback or reward signals.

**Contextual bandits with preference feedback.**Dudik et al. (2015) is the first to consider contextual dueling bandits, and one of their algorithms achieves the optimal regret rate. Saha and Krishnamurthy (2022) studied contextual dueling bandits using a value function class and proposed an algorithm based on a reduction to online regression, which also achieves an optimal worst-case regret bound. In this paper, we mainly follow the setting of the latter and make notable improvements in two aspects: (1) in addition to the \(O()\) optimal regret rate where \(A\) is the number of actions and \(T\) is the number of interaction rounds, we established an instance-dependent regret upper bound that can be significantly smaller when the bandit exhibits a favorable structure; (2) our algorithm has an instance-dependent upper bound on the number of queries, and thus when the underlying instance is well behaved (has small eluder dimension and large gap), we will make significantly fewer queries.

Another related work is Saha and Gaillard (2022) which achieves the best-of-both-worlds regret for non-contextual dueling bandits. Our setting is more general due to the context and general function approximation, which enables us to leverage function classes beyond linear and tabular cases.

**RL with preference feedback.** RL with preference feedback has been widely employed in recent advancements in AI (Ouyang et al., 2022; OpenAI, 2023). According to Wirth et al. (2017), there are generally three types of preference feedback: action preferences (Furnkranz et al., 2012), state preferences (Wirth and Furnkranz, 2014), and trajectory preferences (Busa-Fekete et al., 2014; Novoseller et al., 2020; Xu et al., 2020; Lee et al., 2021; Chen et al., 2022; Saha et al., 2023; Pacchiano et al., 2021; Biyik and Sadigh, 2018; Taranovic et al., 2022; Sadigh et al., 2017). We focus on the action preference with the goal of achieving tight regret bounds and query complexities.

The concurrent work from Zhan et al. (2023) investigates the experimental design in both the trajectories-based and action-based preference settings, for which they decouple the process of collecting trajectories from querying for human feedback. Their action-based setting is the same as ours, but they mainly focus on linear parameterization, while our approach is a reduction to online regression and can leverage general function approximation beyond linear function classes.

**Imitation learning.** In imitation learning, two common feedback modalities are typically considered: expert demonstration and preference. The former involves directly acquiring expert actions (Ross et al., 2011; Ross and Bagnell, 2014; Sun et al., 2017; Chang et al., 2015; Sekhari et al., 2023), while the latter focuses on obtaining preferences between selected options (Chu and Ghahramani, 2005; Lee et al., 2021; Zhu et al., 2023). Brown et al. (2019, 2020) leveraged both demonstrations and preference-based information and empirically showed that their algorithm can learn to outperform experts. Our imitation learning setting belongs to the second category, and we established bounds on the regret and the query complexity for our algorithm. We show that our algorithm can learn a policy that can provably outperform the expert (when it is suboptimal for the underlying environment).

## 2 Preliminaries

### Contextual Bandits with Preference-Based Feedback

In this section, we introduce the contextual dueling bandits setting. For notation, we denote \([N]\) as the integer set \(\{1,,N\}\) and denote \(()\) as the set of distributions over a set \(\).

We assume a context set \(\) and an action space \(=[A]\). At each round \(t[T]\), a context \(x_{t}\) is drawn _adversarially_, and the learner's task is to pick a pair of actions \((a_{t},b_{t})\) and then decide whether to make a query to the expert. If making a query, a noisy feedback \(y_{t}\{-1,1\}\) is revealed to the learner regarding whether \(a_{t}\) or \(b_{t}\) is better. We assume that the expert relies on a preference function \(f^{}:[-1,1]\) to samples its preference feedback \(y_{t}\):

\[(a_{t}b_{t}\,|\,x_{t})(y_{t}=1\,|\,x_{t},a_ {t},b_{t})=f^{}(x_{t},a_{t},b_{t})\]

where \((d):[-1,1]\) is the link function, which satisfies \((d)+(-d)=1\) for any \(d\). If the learner does not make a query, it will not receive any feedback for the selected actions \(a_{t}\) and \(b_{t}\). Let \(Z_{t}\{0,1\}\) indicate whether the learner makes a query at round \(t\).

We assume that the learner has access to a function class \([-1,1]\) that realizes \(f^{}\). Furthermore, we assume that \(f^{}\), as well as the functions in \(\), is transitive and anti-symmetric.

**Assumption 1**.: _We assume \(f^{}\) and any functions \(f\) satisfies the following two properties: (1) transitivity: for any \(x\) and \(a,b,c\), if \(f(x,a,b)>0\) and \(f(x,b,c)>0\), then we must have \(f(x,a,c)>0\); (2) anti-symmetry: \(f(x,a,b)=-f(x,b,a)\) for any \(x\) and any \(a,b\)._

We provide an example below for which Assumption 1 is satisfied.

**Example 1**.: _Assume there exists a function \(r^{}:\) such that \(f^{}(x,a,b)=r^{}(x,a)-r^{}(x,b)\) for any \(x\) and \(a,b\). Typically, such a function \(r^{}\) represents the "reward function" of the contextual bandit. In such a scenario, we can first parameterize a reward class \(\) and define \(=\{f:f(x,a,b)=r(x,a)-r(x,b),r\}\). Moreover, it is common to have \((d) 1/(1+(-d))\) in this setting, which recovers the Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952) -- a commonly used model in practice for learning reward models (Christiano et al., 2017)._

Assumption 1 ensures the existence of an optimal arm, as stated below.

**Lemma 1**.: _Under Assumption 1, for any function \(f\) and any context \(x\), there exists an arm \(a\) such that \(f(x,a,b) 0\) for any arm \(b\). We denote this best arm by \(_{f}(x):=a\).2_

The learner's goal is to minimize the regret while also minimizing the number of queries, defined as:

\[_{T}^{}_{t=1}^{T}f^{}(x_{ t},_{f^{}}(x_{t}),a_{t})+f^{}(x_{t},_{f^{}}(x_{t}),b_{t}) ,_{T}^{}_{t=1}^{T}Z_{t}.\]

It is worth noting that when \(f^{}\) is the difference in rewards (as in Example 1), the regret defined above reduces to the standard regret of a contextual bandit. We also remark that our feedback model generalizes that of Saha and Krishnamurthy (2022) in that we assume an additional link function \(\), while they assume the feedback is sampled from \((y=1\,|\,x,a,b)=(P_{t}[a_{t},b_{t}]+1)/2\) where \(P_{t}\) is a preference matrix. Their loss function is captured in our setting (Example 2). However, Saha and Krishnamurthy (2022) do not assume transitivity.

### Imitation Learning with Preference-Based Feedback

In our imitation learning setup, we consider that the learner operates in a finite-horizon Markov decision process (MDP), which is a tuple \(M(,,r,P,H)\) where \(\) is the state space, \(\) is the action space, \(P\) is the transition kernel, \(r:\) is the reward function, and \(H\) is the length of each episode. The interaction between the learner and the environment proceeds as follows: at each episode \(t[T]\), the learner receives an initial state \(x_{t,0}\) which could be chosen adversarially. Then, the learner interacts with the environment for \(H\) steps. At each step \(h\), the learner first decides whether to make a query. If making a query, the learner needs to select a pair of actions \((a_{t,h},b_{t,h})\), upon which a feedback \(y_{t,h}\{-1,1\}\) is revealed to the learner regarding which action is preferred from the expert's perspective. Here the feedback is sampled according to

\[(a_{t,h}b_{t,h}\,|\,x_{t,h},h)(y_{t,h}=1\,| \,x_{t,h},a_{t,h},b_{t,h},h)=f^{}_{h}(x,a_{t,h},b_{t,h}).\]

Irrespective of whether the learner made a query, it then picks a single action from \(a_{t,h},b_{t,h}\) and transit to the next step (our algorithm will just pick an action uniformly at random from \(a_{t,h},b_{t,h}\)). After \(H\) steps, the next episode starts. Let \(Z_{t,h}\{0,1\}\) indicate whether the learner decided to query at step \(h\) in episode \(t\). We assume that the function class \(\) is a product of \(H\) classes, i.e., \(=_{0}_{H-1}\) where, for each \(h\), we use \(_{h}=\{f:[-1,1]\}\) to model \(f^{}_{h}\) and assume that \(_{h}\) satisfies Assumption 1.

A policy is a mapping \(:()\). For a policy \(\), the state value function for a state \(x\) at step \(h\) is defined as \(V^{}_{h}(x)[_{i=h}^{H-1}r_{i}\,|\,x_{h}=x]\) and the state-action value function for a state-action pair \((x,a)\) is \(Q^{}_{h}(x,a)[_{i=h}^{H-1}r_{i}\,|\,x_{h}=x,a_{h}=a]\), where the expectations are taken w.r.t. the trajectories sampled by \(\) in the underlying MDP.

In the imitation learning setting, we assume that the expert (who gives the preference-based feedback) is equipped with a markovian policy \(_{e}\), and that the preference of the expert is dependent on the reward-to-go under \(_{}\) (i.e. on a state \(x\), actions with higher values of \(Q^{_{}}(s,a)\) will be preferred by the expert). Formalizing this intuition, we assume that \(f_{h}^{*}\) is defined such that as \(f_{h}^{*}(x,a,b) Q_{h}^{_{}}(x,a)-Q_{h}^{_{}}(x,b)\). The goal of the learner is still to minimize the regret and number of queries:

\[_{T}^{}_{t=1}^{T}V_{0}^{_{ }}(x_{t,0})-V_{0}^{_{}}(x_{t,0}),_{T}^{}_{t=1}^{T}_{h=0}^{H-1}Z_{t,h}.\]

Here \(_{}\) is the strategy the learner uses to select actions at episode \(t\).

### Link Function and Online Regression Oracle

Following the standard practice in the literature [Agarwal, 2013], we assume \(\) is the derivative of some \(\)-strongly convex function (Definition 3) \(:[-1,1]\) and define the associated loss function as \(_{}(d,y)=(d)-d(y+1)/2\). Additionally, in line with prior works [Foster et al., 2021, Simechi-Levi and Xu, 2022, Foster et al., 2018a, Sekhari et al., 2023], our algorithm utilizes an online regression oracle, which is assumed to have a sublinear regret guarantee w.r.t. \(\) on arbitrary data sequences.

**Assumption 2**.: _We assume the learner has access to an online regression oracle pertaining to the loss \(_{}\) such that for any sequence \(\{(x_{1},a_{1},b_{1},y_{1}),,(x_{T},a_{T},b_{T},y_{T})\}\) where the label \(y_{t}\) is generated by \(y_{t}(f^{*}(x_{i},a_{t},b_{t}))\), we have_

\[_{t=1}^{T}_{}f_{t}(x_{t},a_{t},b_{t}),y_{t}-_{f }_{}f(x_{t},a_{t},b_{t}),y_{t} (,T)\]

_for some \((,T)\) that grows sublinearly with respect to \(T\).3 For notational simplicity, whenever clear from the context, we define \(:=(,T)\)._

Here \(\) represents the regret upper bound and is typically of logarithmic order in \(T\) or the cardinality of the function class \(\) in many cases (here we drop the dependence on \(T\) in notation for simplicity). We provide a few examples below.

**Example 2** (Squared loss).: _If we consider \((d)=d^{2}/4+d/2+1/4\), which is \(1/4\)-strongly convex, then we obtain \((d)=(d+1)/2\) and \(_{}(d,y)=(d-y)^{2}/4\), thereby recovering the squared loss, which has been widely studied in prior works. For example, Rakhlin and Sridharan (2014) characterized the minimax rates for online square loss regression in terms of the offset sequential Rademacher complexity, resulting in favorable bounds for the regret. Specifically, we have \(=O(||)\) assuming the function class \(\) is finite, and \(=O(d(T))\) assuming \(\) is a \(d\)-dimensional linear class. We also kindly refer the readers to Krishnamurthy et al. (2017), Foster et al. (2018a) for efficient implementations._

**Example 3** (Logistic loss).: _When \((d)=(1+(d))\) which is strongly convex at \([-1,1]\), we have \((d)=1/(1+(-d))\) and \(_{}(d,y)=(1+(-yd))\). Thus, we recover the logistic regression loss, which allows us to use online logistic regression and achieve \(=O(||)\) assuming finite \(\). There have been numerous endeavors in minimizing the log loss, such as Foster et al. (2018b) and Cesa-Bianchi and Lugosi (2006, Chapter 9)._

## 3 Contextual Bandits with Preference-Based Active Queries

We first present the algorithm, named AURORA, for contextual dueling bandits, as shown in Algorithm 1. At each round \(t[T]\), the learner first constructs a version space \(_{t}\) containing all functions close to past predictors on the observed data. Here, the threshold \(\) set to \(4/+(16+24)4^{-1}(T)/^{2}\) ensures that \(f^{*}_{t}\) for any \(t[T]\) with probability at least \(1-\) (Lemma 9). Thus, \(_{t}\) is non-empty for all \(t[T]\) and correspondingly Line 17 is well defined. The learner then forms a candidate arm set \(_{t}\) consisting of greedy arms induced by all functions in the version space. When \(|_{t}|=1\), the only arm in the set is the optimal arm since \(f^{*}_{t}\), and thus no query is needed (\(Z_{t}=0\)). However, when \(|_{t}|>1\), any arm in \(_{t}\) could potentially be the optimal arm, and thus the learner needs to make a comparison query to obtain more information.

Next, we explain the learner's strategy for making queries. Firstly, the learner computes \(w_{t}\), which is the "width" of the version space. Specifically, \(w_{t}\) overestimates the instantaneous regret for playing any arm in \(_{t}\) (Lemma 8). Then, the learner defines \(_{t}\) that indicates if the estimated cumulative regret \(_{s=1}^{t-1}Z_{s}w_{s}\) has exceeded \(\). Note that \(Z_{t}\) is multiplied to \(w_{t}\) since no regret is incurred when \(Z_{t}=0\). The learner then chooses the actions (to be queried) depending on the values of \(_{t}\):

* If \(_{t}=0\), the cumulative reward has not yet exceeded \(=O()\), so the learner will explore as much as possible by uniform sampling from \(_{t}\).
* If \(_{t}=1\), the regret may have reached \(O()\), and therefore the learner uses a technique similar to inverse gap weighting (IGW), as inspired by Saha and Krishnamurthy (2022), to achieve a better balance between exploration and exploitation. Specifically, the learner solves the convex program4 in Line 12, which is feasible and whose solution \(p_{t}\) satisfies (Lemma 11) 
\[*{}_{a p_{t}}[f^{}(x_{t},_{f^{}}( x),a)]=O(_{t}*{}_{a,b p_{t}}[ (f_{t}(x_{t},a,b)-f^{}(x_{t},a,b))^{2}]+}).\] (1) As a result of the above relation, we can convert the instantaneous regret to the point-wise error between the predictor \(f_{t}\) and the truth \(f^{}\) plus an additive \(A/_{t}\). This allows us to bound the cumulative point-wise error by the regret of the online regression oracle. In the special case where there exists a "reward function" \(r:\) for each \(f\) such that \(f(x,a,b)=r(x,a)-r(x,b)\) (Example 1), the solution \(p_{t}\) can be directly written as

\[p_{t}(a)=(r_{t}(x_{t},_{f_{t}}(x_{t} ))-r_{t}(x_{t},a))}&a_{f_{t}}(x_{t})\\ 1-_{a^{}_{f_{t}}(x_{t})}p_{t}(a^{})&a=_{f_{t}}(x_{t }),\]

where \(r_{t}\) is the reward function associated with \(f_{t}\), i.e., \(f_{t}(x,a,b)=r_{t}(x,a)-r_{t}(x,b)\). This is the standard IGW exploration strategy (Foster and Rakhlin, 2020) and leads to the same guarantee as (1) (see Lemma 12).

We discuss the computational tractability of Algorithm 1 in Appendix A. In short, it is computationally tractable for some structured function classes (e.g. linear and tabular function classes). For general function classes, it is also efficient given a regression oracle.

### Theoretical Analysis

Towards the statistical guarantees of Algorithm 1, we employ two quantities to characterize a contextual bandit instance: the uniform gap and the eluder dimension, which are introduced below.

**Assumption 3** (Uniform gap).: _We assume the optimal arm \(_{f^{}}(x)\) induced by \(f^{}\) under any context \(x\) is unique. Further, we assume a uniform gap \(:=_{x}_{a_{f^{}}(x)}f^{}(x,_{f^{}}(x),a)>0\)._

We note that the existence of a uniform gap is a standard assumption in the literature of contextual bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Audibert et al., 2010; Garivier et al., 2019; Foster and Rakhlin, 2020; Foster et al., 2021). Next, we introduce the eluder dimension (Russo and Van Roy, 2013) and begin by defining "\(\)-dependence".

**Definition 1** (\(\)-dependence).: _Let \(\) be any function class. We say an element \(x\) is \(\)-dependent on \(\{x_{1},x_{2},,x_{n}\}\) with respect to \(\) if any pair of functions \(g,g^{}\) satisfying \(_{i=1}^{n}(g(x_{i})-g^{}(x_{i}))^{2}^{2}\) also satisfies \(g(x)-g^{}(x)\). Otherwise, we say \(x\) is \(\)-independent of \(\{x_{1},x_{2},,x_{n}\}\)._

**Definition 2** (Eluder dimension).: _The \(\)-eluder dimension of a function class \(\), denoted by \(_{}(,)\), is the length \(d\) of the longest sequence of elements in \(\) satisfying that there exists some \(^{}\) such that every element in the sequence is \(^{}\)-independent of its predecessors._

Eluder dimension is a complexity measure for function classes and has been used in the literature of bandits and RL extensively (Chen et al., 2022; Osband and Van Roy, 2014; Wang et al., 2020; Foster et al., 2021; Wen and Van Roy, 2013; Jain et al., 2015; Ayoub et al., 2020; Ishfaq et al., 2021; Huang et al., 2022). Examples where the eluder dimension is small include linear functions, generalized linear models, and functions in Reproducing Kernel Hilbert Space (RKHS).

Given these quantities, we are ready to state our main results. The proofs are provided in Appendix C.

**Theorem 1**.: _Under Assumptions 1 to 3, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:_

\[_{T}^{}=(\{ ,\ ^{2}_{E}(,)}{} \}),\] \[_{T}^{}=(\{ T,\ ^{3}_{E}^{2}(,)}{^{2}} \})\]

_with probability at least \(1-\). We recall that \(=O(^{-1}+^{-2}(^{-1}(T)))\), and \(\) represents the coefficient of strong convexity of \(\). Logarithmic terms are hidden in the upper bounds for brevity._

For commonly used loss function (Examples 2 and 3), \(\) only scales logarithmically with the size (or effective size) of \(\) and is thus mild. For instance, for a finite class \(\), \(\) depends on \(||\). Furthermore, when \(\) is infinite, we can replace \(||\) by the covering number of \(\) following the standard techniques: for \(d\)-dimensional linear function class, \(\) will have a dependence of \(O(d)\) (effective complexity of \(\)), and for the tabular class, \(\) will have a dependence of \(O(SA^{2})\). In other words, the rate of \(\) is acceptable as long as the complexity of the function class \(\) is acceptable.

**Best-of-both-worlds guarantee.** We observe that both the regret bound and the query complexity bound consist of two components: the worst-case bound and the instance-dependent bound. The worst-case bound provides a guarantee under all circumstances, while the instance-dependent one may significantly improve the upper bound when the underlying problem is well-behaved (i.e., has a small eluder dimension and a large gap).

**Lower bounds.** To see whether these upper bounds are tight, we provide a lower bound which follows from a reduction from regular multi-armed bandits to contextual dueling bandits.

**Theorem 2** (Lower bounds).: _The following two claims hold: (1) For any algorithm, there exists an instance that leads to \(_{T}^{}=()\); (2) For any algorithm achieving a worse-case expected regret upper bound in the form of \([_{T}^{}]=O()\), there exists an instance with gap \(=\) that results in \([_{T}^{}]=(A/)\) and \([_{T}^{}]=(A/^{2})=(T)\)._By relating these lower bounds to Theorem 1, we conclude that our algorithm achieves a tight dependence on the gap \(\) and the number of rounds \(T\) up to logarithmic factors. Furthermore, as an additional contribution, we establish an alternative lower bound in Section C.4.1 by conditioning on the limit of regret, rather than the worst-case regret as assumed in Theorem 2.

**Intuition of proofs.** We next provide intuition for why our algorithm has the aforementioned theoretical guarantees. First, we observe that from the definition of \(_{t}\), the left term inside the indicator is non-decreasing, which allows us to divide rounds into two phases. In the first phase, \(_{t}\) is always 0, and then at some point, it changes to 1 and remains 1 for the rest rounds. After realizing this, we first explain the intuition of the worst-case regret. In the first phase, as \(w_{t}\) is an overestimate of the instantaneous regret (see Lemma 8), the accumulated regret in this phase cannot exceed \(O()\). In the second phase, we adapt the analysis of IGW to this scenario to obtain an \(O()\) upper bound. A similar technique has been used in Saha and Krishnamurthy (2022), Foster et al. (2021). As the regret in both phases is at most \(O()\), the total regret cannot exceed \(O()\). Next, we explain the intuition of instance-dependent regret. Due to the existence of a uniform gap \(\), we can first prove that as long as \(|_{t}|>1\), we must have \(w_{t}\) (see Lemma 7). This means that for all rounds that may incur regret, the corresponding width is at least \(\). However, this cannot happen too many times as this frequency is bounded by the eluder dimension, which leads to an instance-dependent regret upper bound. Leveraging a similar technique, we can also obtain an upper bound on the number of queries.

**Comparison to regret bounds of dueling bandits.** As established by prior works (Yue et al., 2012; Saha and Gaillard, 2022), for dueling bandits, the minimax regret rate is \(()\) and the instance-dependent rate is \((A/)\). If we reduce our result (Theorem 1) into the dueling bandits setting, we will get

\[_{T}=(\{,_{E}(,})}{}\})= (\{,}{}\})\]

where the second equality holds since the eluder dimension is upper bounded by \(A\) for dueling bandits. We observe that the worst-case regret rate is the same, but there is a gap of \(A^{2}\) in the instance-dependent bound. The improvement of this gap is an interesting future direction.

**Comparison to MinMaxDB (Saha and Krishnamurthy, 2022).** In this prior work, the authors assume that \((y=1\,|\,x,a,b)=(f^{}(x,a,b)+1)/2\), which is a specification of our feedback model (Example 2). While our worst-case regret bound matches their regret bound, our paper improves upon their results by having an additional instance-dependent regret bound that depends on the eluder dimension and gap. Furthermore, we also provide bounds on the query complexity which could be small for benign instances while MinMaxDB simply queries on every round.

**Comparison to AdaCB (Foster et al., 2021).** Our method shares some similarities with Foster et al. (2021), especially in terms of theoretical results, but differs in two aspects: (1) they assume regular contextual bandits where the learner observes the reward directly, while we assume preference feedback, and (2) they assume a stochastic setting where contexts are drawn i.i.d., but we assume that the context is adversarially chosen. While these two settings may not be directly comparable, it should be noted that (Foster et al., 2021) do not aim to minimize query complexity.

**Results without the uniform gap assumption.** We highlight that Theorem 1 can naturally extend to scenarios where a uniform gap does not exist (i.e., when Assumption 3 is not satisfied) without any modifications to the algorithm. The result is stated below, which is analogous to Theorem 1.

**Theorem 3**.: _Under Assumptions 1 and 2, Algorithm 1 guarantees the following upper bounds of the regret and the number of queries:_

\[_{T}^{}=(\{,\,_{>0}\{T_{}+^{2} _{E}(,)}{}\}\} ),\]

\[_{T}^{}=(\{T,\,_{ >0}\{T_{}^{2}/A+^{3}_{E} ^{2}(,)}{^{2}}\}\})\]

_with probability at least \(1-\). Here we define the gap of context \(x\) as \((x)_{a_{f^{}}(x)}f^{}(x,_{f^{} }(x),a)\) and the number of rounds where contexts have small gap as \(T_{}_{t=1}^{T}\{(x_{t})\}\). We also recall that \(=O(^{-1}+^{-2}(^{-1}(T)))\), and \(\) denotes the coefficient of strong convexity of \(\)._Compared to Theorem 1, the above result has an extra gap-dependent term, \(T_{}\), measuring how many times the context falls into a small-gap region. We highlight that \(T_{}\) is small under certain conditions such as the Tsybakov noise condition (Tsybakov, 2004). It is also worth mentioning that our algorithm is agnostic to \(\), thus allowing us to take the minimum over all \(>0\).

**Comparion to SAGE-Bandit (Sekhari et al., 2023).** Theorem 3 is similar to Theorem 4 in Sekhari et al. (2023), which studies active queries in contextual bandits with standard reward signal. Although our result looks slightly worse in terms of the factor \(A\), we believe that this inferiority is reasonable since our approach requires two actions to form a query, thus analytically expanding the action space to \(^{2}\). Whether this dependency can be improved remains a question for future investigation.

## 4 Imitation Learning with Preference-Based Active Queries

In this section, we introduce our second algorithm, which is presented in Algorithm 2 for imitation learning. In essence, the learner treats the MDP as a concatenation of \(H\) contextual bandits and runs an instance of AURORA (Algorithm 1) for each time step. Specifically, the learner first creates \(H\) instances of AURORA, denoted by \(_{h}\) (for \(h=0,,H-1\)). Here, \(_{h}\) should be thought of as an interactive program that takes the context \(x\) as input and outputs \(a\), \(b\), and \(Z\). At each episode \(t\), and each step \(h\) therein, the learner first feeds the current state \(x_{t,h}\) to \(_{h}\) as the context; then, \(_{h}\) decides whether to query (i.e. \(Z_{t,h}\)) and returns the actions \(a_{t,h}\) and \(b_{t,h}\). If it decides to make a query, the learner will ask for the feedback \(y_{t,h}\) on the proposed actions \(a_{t,h},b_{t,h}\), and provide the information \(((x_{t,h},a_{t,h},b_{t,h}),y_{t,h})\) back to \(_{h}\) to update its online regression oracle (and other local variables). We recall that the noisy binary feedback \(y_{t,h}\) is sampled as \(y_{t,h}(Q_{h}^{_{}}(x_{t,h},a_{t,h})-Q_{h}^{_{} }(x_{t,h},b_{t,h}))\), and also emphasize that the learner neither has access to \(a_{}(x_{t,h})\) like in DAgger(Ross et al., 2011) nor reward-to-go like in AggreVaTe(D) (Ross and Bagnell, 2014; Sun et al., 2017). Finally, the learner chooses one of the two actions uniformly at random, executes it in the underlying MDP, and transits to the next state \(x_{t,h+1}\) in the episode. The above process is then repeated with \(_{h+1}\) till the episode ends. We name this algorithm AURORAE, the plural form of \(\), which signifies that the algorithm is essentially a stack of multiple AURORA instances.

```
1:Function class \(_{0},_{1},,_{H-1}\), confidence parameter \(\).
2:Learner creates \(H\) instances of Algorithm 1: \(_{h}(_{h},)\) for \(h=0,1,,H-1\).
3:for\(t=1,2,,T\)do
4: Learner receive initial state \(x_{t,0}\).
5:for\(h=0,1,,H-1\)do
6: Learner feeds \(x_{t,h}\) to \(_{h}(_{h},)\), and receives back \(a_{t,h}\), \(b_{t,h}\), \(Z_{t,h}\).
7:if\(Z_{t,h}=1\)then
8: Learner receives feedback \(y_{t,h}\).
9: Learner feeds \(((x_{t,h},a_{t,h},b_{t,h}),y_{t,h})\) to \(_{h}(_{h},)\) to update its online regression oracle and local variables.
10:endif
11: Learner executes \(a(\{a_{t,h},b_{t,h}\})\) and transits to \(x_{t,h+1}\).
12:endfor
13:endfor ```

**Algorithm 2** Active preference qUeBy fOR imitAtion I learning (AURORAE)

### Theoretical Analysis

As Algorithm 2 is essentially a stack of Algorithm 1, we can inherit many of the theoretical guarantees from the previous section. To state the results, we first extend Assumption 3 into imitation learning.

**Assumption 4** (Uniform Gap).: _Let \(f_{h}^{*}\) be defined such that for any \(x\), \(a,b^{2}\), \(f_{h}^{*}(x,a,b)=Q_{h}^{_{}}(x,a)-Q_{h}^{_{}}(x,b)\). For all \(h\), we assume the optimal action for \(f_{h}^{*}\) under any state \(x\) is unique. Further, we assume a uniform gap \(:=_{h}_{x}_{a_{f_{h}^{*}}(x)}f_{h}^{*}(x,_{f_{h}^{* }}(x),a)>0\)._

This assumption essentially says that \(Q_{h}^{_{}}\) has a gap in actions. We remark that, just as Assumption 3 is a common condition in the bandit literature, Assumption 4 is also common in MDPs (Du et al.,2019, Foster et al., 2021, Simchowitz and Jamieson, 2019, Jin and Luo, 2020, Lykouris et al., 2021, He et al., 2021]. The theoretical guarantee for Algorithm 2 is presented in Theorem 4. We note a technical difference between this result and Theorem 1: although we treat the MDP as a concatenation of \(H\) contextual bandits, the instantaneous regret of imitation learning is defined as the performance gap between the combined policy \(_{t}\) derived from the \(H\) instances as a cohesive unit and the expert policy. This necessitates the use of performance difference lemma (Lemma 5) to get a unified result.

**Theorem 4**.: _Under Assumptions 1, 2 and 4, Algorithm 2 guarantees the following upper bounds of the regret and the number of queries:_

\[_{T}^{}(H \{,\ ^{2}_{E}(,)}{} \})-_{T},\] \[_{T}^{}(H \{T,\ ^{3}_{E}^{2}(,)}{ ^{2}}\})\]

_with probability at least \(1-\). Here \(_{T}_{t=1}^{T}_{h=0}^{H-1}_{x_{t,h}  d_{t_{t},0,h}^{_{t}}}[_{a}A_{h}^{_{e}}(x_{t,h},a)]\) is non-negative, and \(d_{x_{t,0,h}}^{_{t}}(x)\) denotes the probability of \(_{t}\)5 reaching the state \(x\) at time step \(h\) starting from initial state \(x_{t,0}\). In the above, \(=O(^{-1}+^{-2}(H^{-1}(T)))\) and \(\) denotes the coefficient of strong convexity of \(\)._

Compared to Theorem 1, the main terms of the upper bounds for imitation learning are precisely the bounds in Theorem 1 multiplied by \(H\). In the proof presented in Appendix C.6, we use the performance difference lemma to reduce the regret of imitation learning to the sum of the regret of \(H\) contextual dueling bandits, which explains this additional factor of \(H\).

Another interesting point is that the main term of the regret upper bound is subtracted by a non-negative term \(_{T}\), which measures the degree to which we can _outperform_ the expert policy. In other words, our algorithm not only competes with the expert policy but can also surpass it to some extent. To see this, let us consider the _average regret_, which is defined as \(_{T}^{}:=_{T}^{}/T= _{t=1}^{T}(V_{0}^{_{e}}(x_{t,0})-V_{0}^{_{t}}(x_{t,0}))/T.\) Then, Theorem 4 implies that \(_{T}^{}=O(H)-_{T}/T\) where we have simplified it by ignoring the instance-dependent upper bound and logarithmic factors for clarity. Now, consider a case where \(_{a}A_{h}^{_{e}}(x,a)>_{0}\) for some constant \(_{0}>0\) for all \(x\) and \(h\). This can happen when the expert policy is suboptimal for every state. Consequently, we have \(_{T}>_{0}HT\). In this case, the average regret is further bounded by \(_{T}^{}=O(H)-_{0}H.\) When \(T\), we have \(_{T}^{}-_{0}H<0\). This means that the best (or average) learned policy will eventually outperform the expert policy. This guarantee is stronger than that of DAgger(Ross et al., 2011) in that DAgger cannot ensure the learned policy is better than the expert policy regardless of how suboptimal the expert may be. While this may look surprising at first glance since we are operating under a somewhat weaker query mode than that of DAgger, we note that by querying experts for comparisons on pairs of actions with feedback sampling as \(y(Q^{_{e}}(x,a)-Q^{_{e}}(x,b))\), it is possible to identify the action that maximizes \(Q^{_{e}}(x,a)\) (even if we cannot identify the value \(Q^{_{e}}(x,a)\)). Finally, we remark that our worst-case regret bound is similar to that of Ross and Bagnell (2014); Sun et al. (2017), which can also outperform a suboptimal expert but require access to both expert's actions and reward signals -- a much stronger query model than ours.

## 5 Conclusion

We presented interactive decision-making algorithms that learn from preference-based feedback while minimizing query complexity. Our algorithms for contextual bandits and imitation learning share worst-case regret bounds similar to the bounds of the state-of-the-art algorithms in standard settings while maintaining instance-dependent regret bounds and query complexity bounds. Notably, our imitation learning algorithm can outperform suboptimal experts, matching the result of Ross and Bagnell (2014); Sun et al. (2017), which operates under much stronger feedback.