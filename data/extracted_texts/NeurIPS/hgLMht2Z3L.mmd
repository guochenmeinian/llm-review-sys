# Path following algorithms for \(\ell_{2}\)-regularized M-estimation with approximation guarantee

# Path following algorithms for \(_{2}\)-regularized M-estimation with approximation guarantee

 Yunzhang Zhu

Department of Statistics

Ohio State University

Columbus, OH 43015

zhu.219@osu.edu

&Renxiong Liu

Statistics and Data Science team

Nokia Bell Labs

Murray Hill, NJ 07974

renxiong.liu@nokia-bell-labs.com

Work done when Renxiong Liu is a PhD student at the Ohio State University.

###### Abstract

Many modern machine learning algorithms are formulated as regularized M-estimation problems, in which a regularization (tuning) parameter controls a trade-off between model fit to the training data and model complexity. To select the "best" tuning parameter value that achieves a good trade-off, an approximated solution path needs to be computed. In practice, this is often done through selecting a grid of tuning parameter values and solving the regularized problem at the selected grid points. However, given any desired level of accuracy, it is often not clear how to choose the grid points and also how accurately one should solve the regularized problems at the selected gird points, both of which can greatly impact the overall amount of computation. In the context of \(_{2}\)-regularized M-estimation problem, we propose a novel grid point selection scheme and an adaptive stopping criterion for any given optimization algorithm that produces an approximated solution path with approximation error guarantee. Theoretically, we prove that the proposed solution path can approximate the exact solution path to arbitrary level of accuracy, while saving the overall computation as much as possible. Numerical results also corroborate our theoretical analysis.

## 1 Introduction

Modern machine learning algorithms are often formulated as regularized M-minimization problems,

\[()=_{}L_{n}()+ p()\,,\] (1)

where \(L_{n}()\) denotes an empirical loss function, \(p()\) denotes a regularization function, and \(\) is a tuning parameter that controls the trade-off between model fit and model complexity. Varying the tuning parameter leads to a collection of models, among which one of them may be chosen as the final model. This requires solving a collection of related optimization problems, whose solutions are often referred to as the solution path (We refer readers to Figure S4 and S5 in the supplementary material for some examples of solution paths).

Very often the exact solution path \(()\) can not be computed, and path-following algorithms are usually used to obtain a sequence of solutions at some selected grid points to produce an approximated solution path. Existing path-following algorithms (see, e.g., Hastie et al., 2004, 2007) typically choose a set of equally-spaced grid points (often on a log-scale), and choose certain algorithm to solve the minimization problem at the selected grid points. Warm-start strategy is often used to leverage the fact that the minimizers at two consecutive grid points may be close. However, there is a paucity of literature on how to choose these grid points and how accurately one should solve the optimizationproblem at the selected grid points, both of which would influence the overall computational cost and approximation accuracy.

In the context of \(_{2}\)-regularized M-estimation where \(p()=(1/2)\|\|_{2}^{2}\), we propose a novel grid point selection scheme and an associated stopping criterion at each grid point for any optimization method. The proposal is motivated by first proving that the approximation error of a solution path is governed by two important quantities, one depending on how finely the grid points are spaced and the other depending on how accurately one solves the regularized problem at the selected grid points. We refer these two quantities as interpolation error and optimization error. Given any optimization algorithm (that solves a single regularized problem), a stopping criterion can be constructed so that the optimization error is comparable to the interpolation error, which ensures that there is no wasteful computation at each selected grid point. Then a novel grid point selection scheme is constructed to balance the approximation errors over the entire solution path.

Theoretically, we first show that the approximation error of the proposed solution path can be upper bounded by the sum of the interpolation error and the optimization error. Given any optimization algorithm, we show that the proposed stopping criterion ensures that the optimization error is dominated by the interpolation error (see Theorem 1). We then establish a global approximation-error bound for the solution path (see Theorem 2). Optimizing this global approximation-error bound by using the proposed grid point selection scheme, we also prove that the proposed solution path can approximate the exact solution path to an arbitrary level of accuracy (see Theorem 3). Finally, we establish that the total number of grid points required for the proposed algorithm to achieve an \(\)-suboptimality is at most \((^{-1/2})\) (see Theorem 4).

Path-following techniques have been used in many optimization algorithms. For instance, it is used in the interior point methods, which is one of most popular algorithm to solve general convex optimization problems (see, e.g., Chapter 1.3 of Nesterov and Nemirovskii, 1993). However, the focus is often on the limit of the path rather than the entire solution path. In statistics/machine learning literature, Osborne (1992) and Osborne et al. (2000) applied the homotopy techniques to obtain piecewise linear solution paths for quantile regression and LASSO, respectively. Later Efron et al. (2004), Hastie et al. (2004), and Rosset and Zhu (2007) used similar ideas to derive the solution path for a family of regularized M-estimation problems. Subsequent developments include Friedman et al. (2007), Hoefling (2010), Arnold and Tibshirani (2016), among many others. Path following algorithms for more general types of \(_{1}\)-regularization are also available. For example, Tibshirani and Taylor (2011) used dual path algorithm to compute the solution path of a generalized LASSO problem. In the context of order weight \(_{1}\) model, Bao et al. (2019) proposed an efficient approximate solution path algorithm by using primal-dual method. These works often leverage the piecewise linearity of the solution path so that an exact path-following algorithm can be explicitly derived. For situations where the solution paths are not piecewise linear, a path-following algorithm based on Newton method was considered in Rosset (2004). However, Rosset (2004) used a simple grid point selection scheme and only established pointwise closeness to the solution path. Subsequent research on situations where the solution paths are not piecewise linear includes the work of Garrigues and Ghaoui (2008), which introduced a homotopy algorithm tailored for solving the LASSO problem using online (sequential) observations, and the work of Qu et al. (2022), which approximates the piecewise smooth path in self-paced learning for generalized self-paced regularizer.

To the best of our knowledge, there is a paucity of research on the impact of grid points selection and how to set stopping criterion to save computation. One line of related works use a piecewise constant interpolation scheme to approximate the solution path to an arbitrary level of accuracy (see, e.g., Giesen et al., 2010, 2012, 2014; Ndiaye et al., 2019). Most relevant to our work here are the works of Giesen et al. (2012) and Ndiaye et al. (2019). Giesen et al. (2012) proposed a generic algorithm to compute a piecewise constant approximate solution path for a family of optimization problems parameterized concavely2 in a single hyperparameter. They showed that the number of grid points needed to achieve an \(\) suboptimality is at most \((1/)\). However, their proposed method relies on the existence of a problem-dependent oracle, which for any chosen hyperparameter produces both an \(\)-approximate solution at that hyperparameter and a concave polynomial that is used to compute the step size. Except for special examples like standard SVM, it not clear how to implement the oracle for general problems.

In view of these limitations, Ndiaye et al. (2019) considered regularized convex optimization problems and proposed general strategies to choose grid points based on primal-dual methods. Again, a piecewise constant path is constructed to approximate the solution path. However, their approach can only be applied to a special class of loss functions that are composition of a "simple" function (whose conjugate can be analytically derived) and affine functions. Moreover, the choice of the grid points depends on some convexity and smoothness parameters associated with the loss, which either are not available except for some special class of functions or require careful tuning due to their great impact on grid points selection. Ndiaye et al. (2019) also showed that to achieve \(\) suboptimality, the number of grid points required is \((1/)\) for uniformly convex loss of order \(d\)(Bauschke et al., 2011) and \((1/)\) for generalized self-concordant functions (Sun and Tran-Dinh, 2017). In our paper, we show that the number of grid points needed is \((1/)\) for general differentiable convex loss (see Theorem 4). Compared to Ndiaye et al. (2019), we also demonstrate through simulations that our method produces better approximated solution paths under a fixed computational budget (see Section 4). Moreover, our method can also be applicable to loss functions that are possibly nonconvex (see Section 3).

We also note that Proposition 38 of Ndiaye (2018) shows that under some regularity conditions \(\)-suboptimality along the entire solution path can be achieved by using a piecewise linear interpolation. However, it not clear how these conditions can be used to construct an algorithm with approximation guarantee. Our proposed algorithm, on the other hand, provides an easily implementable grid point selection scheme and an associated stopping criterion at each grid point, which together ensures that \(\)-suboptimality can be attained through a piecewise linear interpolation.

More recently, in the context of \(_{2}\)-regularized convex optimization problems, Zhu and Liu (2021) proposed two path-following algorithms by using Newton method and gradient descent method as the basis algorithm seperately, and discuss the grid point selection scheme for each algorithm based on the piecewise linear interpolation. Our paper differs from Zhu and Liu (2021) in twofold. First, the grid-point selection scheme in Zhu and Liu (2021) depends on the basis algorithm. By contrast, in our paper any basis algorithm can be applied at any chosen grid point as long as the stopping criterion is satisfied by the basis algorithm. Next, the path-following algorithms by Zhu and Liu (2021) can only be applied to convex optimization problems, while in this article we also consider loss functions that are possibly nonconvex (see Section 3).

The rest of the paper is organized as follows. Section 2 introduces the proposed path following algorithm and establish its global approximation-error bound. Section 3 considers an extension to nonconvex empirical loss. In Section 4, we compare the the proposed scheme to a standard path-following scheme through a simulated study using ridge regression and \(_{2}\)-regularized logistic regression. We close the article with some remarks in Section 5. All proofs are included in the supplementary material.

## 2 Path following algorithms

We consider an \(_{2}\)-regularized M-estimation problem. More specifically, denote by \(L_{n}()\) some empirical loss function, where \(^{p}\) denotes a parameter vector. We consider the solution path of an \(_{2}\)-regularized M-estimation problem:

\[(t)=_{^{p}}\{(e^{t}-1) L_{n}( )+(1/2)\|\|_{2}^{2}\}\;.\] (2)

Note that as the tuning parameter \(t\) varies from \(0\) to \(\), the solution \((t)\) varies from \(\) to a minimizer of \(L_{n}()\). We also remark that the choice of \(e^{t}-1\) is not essential. In fact, it can be replaced by any function \(C(t)\) that is a strictly increasing and differentiable function, with \(C(0)=0\) and \(_{t}C(t)=\). The solution path produced should be independent of \(C(t)\). Moreover, we can show that the \(_{2}\) norm of the solutions is an increasing function of \(t\), and the solution \((t)\) converges to the minimum \(_{2}\) norm minimizer of \(L_{n}()\) (if it is finite) as \(t\) goes to infinity (see Corollary S1 in the supplementary material).

Suppose that the goal is to approximate the solution path \((t)\) over a given interval \([0,t_{})\) for some \(t_{}(0,]\), where we allow \(t_{}=\). Given a set of grid points \(0<t_{1}<<t_{N}<\), and approximated solutions \(\{_{k}\}_{k=1}^{N}\) at these grid points, we construct an approximated solution path over \([0,t_{})\) through linear interpolation. More specifically, we define a piecewise linear solution path \((t)\) as follows

\[(t)=-t}{t_{k+1}-t_{k}}_{k}+ }{t_{k+1}-t_{k}}_{k+1}&t[t_{k},t_{k+1}],k=0,1,,N-1\,,\\ (t)=_{N}&t_{N}<t t_{}t_{N}<t_{}\,,\] (3)

where \(t_{0}=0\) and \(_{0}=\). This defines an approximated solution path \((t)\) for any \(t[0,t_{}]\), which interpolates the approximated solutions at the selected grid points. In view of this definition, we may also assume that \(t_{N-1} t_{}\), because we do not need \((t)\) over \(t[t_{N-1},t_{N}]\) if \(t_{N-1}>t_{}\).

To assess how well \((t)\) approximates \((t)\), we use the function-value suboptimality of the solution paths defined by \(_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\}\), where \(f_{t}():=(1-e^{-t})L_{n}()+e^{-t}(\|\|_{2}^{2}/2)\) is a scaled version of the objective function in (2). In what follows, we call \(_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\}\) the global approximation error of \((t)\), and \(_{t_{k} t t_{k+1}}\{f_{t}((t))-f_{t}((t))\}\) the local approximation error of \((t)\) over \([t_{k},t_{k+1}]\).

Intuitively, how well \((t)\) approximates \((t)\) depends on how finely spaced the grid points are and the quality of the solutions at the selected grid points (i.e., \(_{k}\)'s). Our first main result relates the local approximation error of \(\) over \([t_{k},t_{k+1}]\) to the choice of the grid points and the accuracy of the approximated solutions at the selected grid points measured by the size of the gradients \(\| f_{t_{k}}(_{k})\|_{2}\), where we assume that \(L_{n}()\) is differentiable.

**Theorem 1**.: _Assume that \(L_{n}()\) is a differentiable convex function. Let \(g_{k}:= f_{t_{k}}(_{k})=(1-e^{-t_{k}}) L_{n}(_{k})+e^{ -t_{k}}_{k}\) denote the scaled gradient at \(_{k}\). For any \(0<t_{1}<t_{2}<<t_{N}<\), we have that_

\[_{t[0,t_{1}]}f_{t}((t))-f_{t}( (t))} (e^{t_{1}}\|g_{1}\|_{2}^{2},\,\|_{1}\|_{2}^{2} )+}(1-e^{-t_{1}})^{2}}{2}\| L_{n}()\|_{2}^ {2}\,,\] (4) \[_{t[t_{k},t_{k+1}]}f_{t}((t))-f_{t}( (t))}  e^{t_{k+1}}\{(}}{1-e^{-t_{ k}}})^{2}\|g_{k}\|_{2}^{2},\,\|g_{k+1}\|_{2}^{2}\}\] \[+(e^{-t_{k}}-e^{-t_{k+1}})^{2}\{}\| _{k}\|_{2}^{2}}{(1-e^{-t_{k}})^{2}},\,}\|_{k+1}\|_{2 }^{2}}{(1-e^{-t_{k+1}})^{2}}\}\] (5)

_for any \(k=1,,N-1\). If we further assume that \(\|(t_{})\|_{2}<\), then we have that_

\[_{t_{N}<t t_{}}f_{t}((t))-f_{t}((t)) }}}{2(1-e^{-t_{N}})}\|g_{N}\|_{2}^{2}+}-1)}\|(t_{})\|_{2}^{2}\] (6)

_when \(t_{N}<t_{}\)._

The proof of Theorem 1 starts with relating the local approximation error to the suboptimality of \(_{k}\) and \(_{k+1}\) at some \(t[t_{k},t_{k+1}]\). The suboptimality can then further bounded by the square norm of the gradient leveraging the fact that the objective function is \(e^{-t}\)-strongly convex. Finally, a triangular inequality is used to bound the norm of the gradient by quantities that depend on \(\|g_{k}\|_{2}\) and \(\|_{k}\|_{2}\).

We can see from Theorem 1 that the upper bounds for the local approximation error in (4)-(6) consist of two terms, with the first one (depending on \(g_{k}\)) being algorithm dependent and the other term stemming from interpolation. We call them _optimization error_ and _interpolation error_, respectively. Note that the optimization error is roughly of order \(e^{t_{k}}\|g_{k}\|_{2}^{2}\), which again depends on how accurate the solutions \(_{k}\) are at the selected grid points. And the interpolation error is essentially independent of the quality of the solutions \(_{k}\)'s, as the interpolation error only depends on the spacing of the grid points and the norm of the solutions along the solution path (typically \(\|_{k}\|_{2}=(\|(t_{k})\|_{2}\)), c.f., Lemma S2 in the supplementary material). In other words, the interpolation error is irreducible once the grid points are chosen, while the optimization error does depend on the algorithm and it can be pushed to be arbitrarily small if we run the algorithm long enough at each grid point. In this sense, if the goal is to minimize the local approximation error, then both the grid points and optimization algorithm should be designed carefully to strike a balance between these two errors to minimize the overall computations. For instance, it would be wasteful to have the optimization error smaller than the interpolation error, because the additional computation would not improve the overall approximation error (at least in order).

Motivated by this observation and the local approximation-error bounds in Theorem 1, we propose a novel stopping criterion scheme at the selected grid points for any general optimization algorithm. In particular, for any given algorithm that takes an initializer and solves the problem at grid point \(t_{k+1}\), we can run the algorithm initialized at \(_{k}\) until the optimization error is smaller than the interpolation error for \(_{k+1}\), that is

\[}\{(}}{1-e ^{-t_{k}}})^{2}\| f_{t_{k}}(_{k})\|_{2}^{2},\,\| f_{t_ {k}}(_{k+1})\|_{2}^{2}\}}_{}\\ }-e^{-t_{k+1}})^{2}\{ }\|_{k}\|_{2}^{2}}{(1-e^{-t_{k}})^{2}},\,} \|_{k+1}\|_{2}^{2}}{(1-e^{-t_{k+1}})^{2}}\}}_{},\] (7)

where the LHS is the optimization error and the RHS is the interpolation error in the bounds in Theorem 1. This condition can be further simplified, under which the upper bounds in Theorem 1 can also be further simplified and combined so that we can bound the global approximation error of \((t)\). This is established in the following theorem.

**Theorem 2**.: _Assume that \(L_{n}()\) is a differentiable and convex. Suppose that \(2^{-1}_{k}_{k+1} 2_{k}\), \(_{k}_{}\) for some \(_{}(2)\), and_

\[\| f_{t_{k}}(_{k})\|_{2} C_{0}}-1)}{(e^{t _{k}}-1)}\|_{k}\|_{2};\,1 k N\] (8)

_for some \(C_{0} 1/4\). Denote_

\[A=4(1+C_{0}^{2}(e^{_{}}+e^{_{}/2}+1)^{2})\,,  B=(e^{_{1}}-1)^{2}\| L_{n}()\|_{2}^{2}\,,\] \[C=_{1 k N}}\|_{k}\|_{2}^{2}(e^{ _{k+1}}-1)^{2}}{(1-e^{-t_{k}})^{2}}\,, D=,\,t_{}-t_{N})}}{e^{t_{N}}-1}\{\|_{N}\|_{2}^{2},\,\|(t_{ })\|_{2}^{2}\}\,.\]

_Then_

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\} A \{B,C,D\}\] (9)

_when \(t_{N}<t_{}\) and \(\|(t_{})\|_{2}<\), and_

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\} A \{B,C\}\] (10)

_when \(t_{N-1} t_{} t_{N}\) for some \(N 1\)._

The above result provides a practical approach to verify whether the optimization error is dominated by the interpolation error by checking (8). In other words, (8) is a sufficient condition for (7) and it can be used as a stopping criterion at each grid point \(t_{k}\) for any algorithm that computes a solution at \(t_{k}\). We also remark that, using the above bounds, similar approximation-error bounds for \(\|(t)-(t)\|_{2}\) can be derived as well (see Corollary S2 in the supplementary material).

Another important implication of Theorem 2 is that the established global approximation-error bound provides a principled approach to choose the grid points so that for any \(>0\) and \(t_{}(0,]\) we have

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\}\] (11)

for the solution path generated by an algorithm, and at the same time it tries to maximize the speed at which the algorithm explores the solution path from \(0\) to \(t_{}\). In particular, to ensure (11), we propose to use the following step sizes

\[_{1}=\{_{},\,(1+}{\| L_{n}( )\|_{2}})\}\] (12)

and

\[_{k+1}=\{(1+(e^{_{1}}-1)\| L_{n}( )\|_{2}e^{t_{k}/2}(1-e^{-t_{k}})}{\|_{k}\|_{2}}),_{},\,2_{k}\}\,,\] (13)and we terminate the algorithm at \(t_{N}\) when

\[(1-e^{-(_{N},\,t_{}-t_{N})})}{e^{t_{N}}-1} \,\,t_{N} t_{}\,,\] (14)

where \(c_{1} 1\), \(c_{2}>0\), and \(0<_{} 1/5\) are absolute constants. Note that the above step sizes and termination criterion are chosen to ensure that the upper bounds in (9) and (10) are at most \(()\). Formally, we show that, given any \(>0\), the above scheme achieves \(\)-suboptimality (11) for \((t)\) (up to a multiplicative constant).

**Theorem 3**.: _Suppose that \(L_{n}()\) is differentiable and convex. For any \(>0\) and \(t_{}(0,]\), assume that either \(t_{}<\) or \(\|(t_{})\|_{2}<\). Then, using the step sizes and the termination criterion specified above in (12), (13), and (14), any algorithm that produces iterates satisfying (8) for any \(k 1\) terminates after finite number of iterations, and when terminated, the solution path \((t)\) satisfies_

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\} \,.\] (15)

Note that when \(\) is small enough, we have that \(_{1}=(1+/\| L_{n}()\|_{2})\), which together with (15) implies that

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\} (e^{_{1}}-1)^{2}\| L_{n}()\|_{2}^{2}\,.\]

As a result, the approximation error for the solution path is essentially controlled by the initial step size \(_{1}\). Smaller \(_{1}\) leads to better approximation. However, smaller \(_{1}\) also leads to a slower exploration of the solution path and a more stringent stopping criterion at individual grid points (c.f. (8)). As such, the initial step size controls the trade-off between computational cost and accuracy. Moreover, given \(_{1}\), we can see from (13) that how fast the algorithm can explore the solution path depends largely on \(\|_{k}\|_{2}\). In particular, if \(\|_{k}\|_{2}\) is bounded (e.g., when \(^{*}\) is finite), then the first term in the \(\) function of (13) increases as \(t_{k}\) increases. Therefore, aggressive step sizes can be taken in this case until it reaches \((1)\), which will likely lead to a fast exploration of the solution path. On the other hand, if \(\|_{k}\|_{2}\) grows quite quickly to infinity as \(k\) increases, then the first term in the \(\) function may go to zero as \(k\) (say, when \(\|_{k}\|_{2}/e^{t_{k}/2}\)). This means that the step sizes need to decrease to zero eventually, leading to a slower exploration of the solution path.

We summarize the proposed scheme in Algorithm 1. Again, we emphasize that the above scheme can be applied to any optimization algorithm. Later, we will empirically investigate its performance using Newton method and gradient descent method.

Lastly, we establish an upper bound on the total number of grid points required for Algorithm 1 to achieve \(\)-suboptimality.

**Theorem 4**.: _Under the assumptions in Theorem 3, the total number of grid points required for Algorithm 1 to achieve an \(\)-suboptimality (15) is at most \((^{-1/2})\)._Extensions to nonconvex loss functions

So far we assume that the empirical loss is convex. Next, we consider a generalization, where we do not assume convexity for \(L_{n}()\). Instead, we assume that there exists \((t)>0\) so that

\[f_{t}()-f_{t}((t))\| f_{t}()\| _{2}^{2}\,,\] (16)

for any \(t 0\) and \((L_{n})\). In the literature, the condition (16) was often referred to as the Polyak-Lojasiewicz (PL) inequality. Polyak (1964) proved the linear rate of convergence for gradient descent method under this condition. Also see Karimi et al. (2016) for discussions of more recent development. Note that if \(L_{n}()\) is convex, then (16) holds for \((t)=e^{-t}\) since \(f_{t}()\) is \(e^{-t}\)-strongly convex. In general, however, \(L_{n}()\) does not need to be convex for (16) to hold.

For some technical reasons, we need to consider slightly different interpolation scheme

\[(t)=_{k}t[t_{k},t_{k+1}],0  k N-1;\] (17) \[(t)=_{N}t_{N}<t t_{ }t_{N}<t_{}\,,\]

where \(t_{0}=0\) and \(_{0}=\). As we can see that the approximated solution path is piecewise constant. We use the following termination criterion at each \(t_{k}\)

\[\|g_{1}\|_{2} (C_{0}(e^{_{1}}-1)\| L_{n}()\|_{2} \,,\;1);\] (18) \[\|g_{k+1}\|_{2} (C_{0}}-1}{e^{t_{k}}-1}\|_{k} \|_{2}\,,\;1),\,k 1\,.\]

We also define a slightly new step size scheme as follows:

\[_{1}=\{_{},\,(1+, _{1})}}{\| L_{n}()\|_{2}})\}\] (19)

and

\[_{k+1}=\{_{},\,2_{k},\,(1+(e^{_ {1}}-1)\| L_{n}()\|_{2}(e^{t_{k}}-1), _{k+1})}}{\|_{k}\|_{2},_{1})}})\}\,,\] (20)

where \(_{k}=_{t_{k} t t_{k+1}}(t)\), \(k 0\). Finally, the stopping criterion for the path-following algorithm is set to be

\[}{e^{t_{N}}-1}\,\,\,\,\,\,t_{N} t_{}\,,\] (21)

where \(c_{3}>0\) is an absolute constant.

Moreover, we need to make some additional assumptions on the loss function.

**Assumption (A1).** Assume that \(L_{n}()\) is differentiable and \(_{}L_{n}()>-\). Moreover, assume that there exists a positive decreasing function \(g()\) such that \((t) g(t)>0\) for any \(0 t<\).

**Theorem 5**.: _Suppose that Assumption (A1) holds. For any \(>0\) and \(t_{}(0,]\), assume that either \(t_{}<\) or \(_{0 t t_{}}\|(t)\|_{2}<\). Then, using step sizes and termination criterion specified in (19), (20), and (21), any algorithm that produces gradients satisfying (18) terminates after finite number of iterations, and when terminated, the solution path \((t)\) defined in (17) satisfies_

\[_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\} \,.\] (22)

The above result can be viewed as a generalization of Theorem 3, because when \(L_{n}()\) is convex, we could set \((t)=(-t)\) so that both condition (16) and Assumption (A1) are satisfied.

## 4 Numerical studies

In this section, we study the operating characteristics of Newton method, gradient descent method and a mixed method in ridge regression and \(_{2}\)-regularized logistic regression, using both the simulated datasets and a real data example. Here the mixed method applies gradient descent method to minimize \(f_{t_{k}}()\) at the beginning and then switch to Newton method when the number of gradient steps \(n_{k}\) exceeds \(\{n,p\}\). To illustrate the advantages of the proposed grid point selection scheme and stopping criterion, we use a standard implementation of a typical path-following algorithm as a baseline. In particular, the grid points are equally spaced (on a log-scale) for both examples. For \(_{2}\) logistic regression, Newton method is used as the basis algorithm with a fixed stopping criterion \(\| f_{t_{k}}(_{k})\|_{2}<10^{-5}\), while for ridge regression the exact solution can be computed. We refer to this method as the baseline method.

We also compare the proposed methods against the method of Ndiaye et al. (2019). In particular, we compare our grid points selection scheme (13) with the so-called adaptive unilateral scheme proposed in Ndiaye et al. (2019), where Newton method is used as the basis algorithm at each grid point. Different from the aforementioned methods, adaptive unilateral scheme constructs piecewise constant solution path \(()\) over chosen grid points to approximate the solution path \(()=_{^{p}}P_{}()\), where \(P_{}()=L_{n}()+(/2)\|\|_{2}^{2}\), such that for all \((_{},_{})\)

\[P_{}(())-P_{}(())<\,.\] (23)

We call this Ndiaye's method throughout this section.

All methods considered in the numerical studies are implemented in R using Rcpp package (Eddelbuettel et al., 2011; Eddelbuettel, 2013).

**Example 1: Ridge regression.** This example considers ridge regression over a simulated dataset. In this case, the empirical loss function is \(L_{n}()=\|Y-X\|_{2}^{2}/(2n)\), where \(X^{n p}\) and \(Y^{n}\) denote the design matrix and the response vector. In the simulation, the data \((X,Y)\) are generated from the usual linear regression model \(Y=X^{}+\), where \( N(0,I_{n n})\), \(^{}=(1/,,1/)^{}\), and rows of \(X\) are IID samples from \(N(0,I_{p p})\). Throughout this example, we consider \(n=1000\) and \(p=500,10000\),

**Example 2: \(_{2}\)-regularized logistic regression.** This example considers \(_{2}\)-regularized logistic regression over a simulated dataset. Let \(X^{n}\) and \(Y\{+1,-1\}^{n}\) denote the design matrix and the binary response vector. The empirical loss function for logistic regression is \(L_{n}()=n^{-1}_{i=1}^{n}(1+(-Y_{i}X_{i}^{}))\). We simulate the data \((X,Y)\) from a linear discriminant analysis (LDA) model. More specifically, we sample the components of \(Y\) independently from a Bernoulli distribution with \((Y_{i}=+1)=1/2\); \(i=1,2,,n\). Conditioned on \(Y_{i}\), \(X_{i}\)'s are then independently drawn from \(N(Y_{i},^{2}I_{p p})\), where \(^{p}\) and \(^{2}>0\). Note that \(\) and \(^{2}\) controls the Bayes risk, which is \((-\|\|_{2}/)\) under the 0/1 loss, where \(()\) is the cumulative distribution function of a standard normal random variable. Here we choose \(=(1/,,1/)\) and \(^{2}=1\) so that the Bayes risk is \((-1)\), which is approximately \(15\%\). Similar to Example 1, two different problem dimension are considered: \(n=1000\) and \(p=500,10000\).

**Example 3: Real data example.** This example fits an \(_{2}\)-regularized logistic regression using the _a9a_ dataset from LIBSVM (Chang and Lin, 2011). This dataset provides information about whether an adult's annual income surpasses \(\$50,000\), along with some specific characteristics of the given adult. It contains a total of \(32561\) examples with \(123\) features and \(1\) many response variable.

For all three examples, we use the global approximation error \(_{0 t t_{}}\{f_{t}((t))-f_{t}((t))\}\) as an evaluation metric, where \((t)\) is the linear interpolation of the iterates \(_{k}\) generated by each method. Throughout, we let \(t_{}=10\). Since this metric can not be calculated exactly, we approximate the global approximation error by \(_{1 i 100}\{f_{s_{i}}((s_{i}))-f_{s_{i}}((s_{i}))\}\), where \(s_{1},,s_{100}\) are uniformly sampled from \((0,t_{})\). Here we compute the solutions \((s_{i})\) using the CVX solver (Grant and Boyd, 2014, 2008) for \(_{2}\)-regularized logistic regression, while calculating \((s_{i})\) explicitly for ridge regression.

We compare Newton method, gradient descent method and their mixed version against the baseline method and Ndiaye's method. To make a fair comparison with Ndiaye's method, we set \(_{}=1/(e^{10}-1)\) to match \(t_{}=10\) in log scale and choose \(_{}=100\).

Figure 1 presents runtime versus global approximation error for Example 1 and 2 based on \(100\) replications, as we vary \(\) in (12) for our proposed method and (23) for Ndiaye's method. For the baseline method, we vary the spacing between consecutive grid points instead. Clearly the proposed Newton method outperforms the baseline method and Ndiaye's method in both low dimension and high dimension scenarios, which empirically confirms that the proposed grid point selection scheme

(c.f. (13)) is more efficient than both the standard equally-spaced grid point scheme and the adaptive unilateral scheme proposed by Ndiaye et al. (2019). Moreover, gradient descent method performs similarly compared with the mixed method in both setups of Example 1, which suggests that switching does not happen much in those cases. However, in both problem dimensions of Example 2, we do see a difference between the mixed method and gradient descent, which is due to the fact that the switching to the Newton update does happen and it can speed up the computation by design. Lastly, as one would expect, Newton method is more efficient than both the gradient descent method and the mixed method when the problem dimension is small, but less so when problem dimension is large.

Figure 2 shows the runtime versus global approximation error tradeoff for our real data example, as we vary \(\) for our proposed methods and Ndiaye's method, and change grid point spacing for the baseline method. Again, the Newton method is more efficient than both the baseline method and the Ndiaye's method, which demonstrates the advantage of our proposed grid point selection scheme over the standard equally-spaced grid point scheme and the scheme by Ndiaye et al. (2019).

We also plot the number of iterations at each grid point for Newton method and gradient descent method in Figure S1-S3 of the supplementary material. Interestingly, for ridge regression (c.f., Figure S1), the number of iterations by gradient method first increases and then stays flat as \(t_{k}\) grows. Newton method, however, only takes one iteration at each grid point. Moreover, the level of approximation (i.e., \(\)) seems to have no impact on the number of iterations at each grid point, which is highly desirable. For the \(_{2}\)-regularized logistic regression (c.f., Figure S2 and S3), the number of iterations needed increases as \(t_{k}\) increases for gradient descent method, whereas the Newton method always requires just a constant number of iterations at each \(t_{k}\). Again, the level of approximation (i.e., \(\)) does not seem to influence the number of iterations much.

## 5 Discussion

In this article, we proposed a novel grid point selection scheme and stopping criterion for any general path-following algorithms. A simple solution path was constructed through linear interpolation

Figure 1: Runtime v.s. suboptimality for the Newton method, the gradient descent method, the mixed method, the baseline method and Ndiaye’s method under two problem dimensions when applied to ridge regression (upper panels) and \(_{2}\)-regularized logistic regression (lower panels). The vertical and horizontal lines at each point represent the standard errors of suboptimality and runtime.

and theoretical approximation-error bound was derived. A generalization to a class of nonconvex empirical loss was also considered. We have not touched on the overall computational complexity analysis for commonly used optimization algorithms. Based on some preliminary analysis, we conjecture that for gradient descent method and Newton method, the number of iterations required are \((^{-1}(^{-1}))\) and \((^{-1/2})\), respectively. Moreover, lower bound results are also worth investigating, that is, what are the minimum number of gradient or Newton steps needed to achieve an \(\)-suboptimality for the entire solution path? Moreover, our current analysis can be readily extended to general differentiable regularization functions. A more challenging future direction is to investigate the case where the empirical loss function or the regularizer are not differentiable.