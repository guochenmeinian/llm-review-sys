# Distribution-Free Statistical Dispersion Control for Societal Applications

Zhun Deng

zhun.d@columbia.edu

Columbia University

&Thomas P. Zollo

tpg2105@columbia.edu

Columbia University

&Jake C. Snell

js2523@princeton.edu

&Toniann Pitassi

toni@cs.columbia.edu

Columbia University

&Richard Zemel

zemel@cs.columbia.edu

Columbia University

###### Abstract

Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications it is crucial to understand and control the _dispersion_ of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.

## 1 Introduction

Learning-based predictive algorithms are widely used in real-world systems and have significantly impacted our daily lives. However, many algorithms are deployed without sufficient testing or a thorough understanding of likely failure modes. This is especially worrisome in high-stakes application areas such as healthcare, finance, and autonomous transportation. In order to address this critical challenge and provide tools for rigorous system evaluation prior to deployment, there has been a rise in techniques offering explicit and finite-sample statistical guarantees that hold for any unknown data distribution and black-box algorithm, a paradigm known as distribution-free uncertainty quantification (DFUQ). In , a framework is proposed for selecting a model based on bounds on expected loss produced using validation data. Subsequent work  goes beyond expected loss to provide distribution-free control for a class of risk measures known as quantile-based risk measures (QBRMs) . This includes (in addition to expected loss): median, value-at-risk (VaR), and conditional value-at-risk (CVaR) . For example, such a framework can be used to get bounds on the 80th percentile loss or the average loss of the 10% worst cases.

While this is important progress towards the sort of robust system verification necessary to ensure the responsible use of machine learning algorithms, in some scenarios measuring the expected loss or value-at-risk is not enough. As models are increasingly deployed in areas with long-lasting societal consequences, we should also be concerned with the dispersion of error across the population, or the extent to which different members of a population experience unequal effects of decisions made based on a model's prediction. For example, a system for promoting content on a social platform may offer less appropriate recommendations for the long tail of niche users in service of a small setof users with high and typical engagement, as shown in . This may be undesirable from both a business and societal point of view, and thus it is crucial to rigorously validate such properties in an algorithm prior to deployment and understand how the outcomes _disperse_. To this end, we offer a novel study providing rigorous distribution-free guarantees for a broad class of functionals including key measures of statistical dispersion in society. We consider both differences in performance that arise between different demographic groups as well as disparities that can be identified even if one does not have reliable demographic data or chooses not to collect them due to privacy or security concerns. Well-studied risk measures that fit into our framework include the Gini coefficient  and other functions of the Lorenz curve as well as differences in group measures such as the median . See Figure 4 for a further illustration of loss dispersion.

In order to provide rigorous guarantees for socially important measures that go beyond expected loss or other QBRMs, we provide two-sided bounds for quantiles and nonlinear functionals of quantiles. Our framework is simple yet flexible and widely applicable to a rich class of nonlinear functionals of quantiles, including Gini coefficient, Atkinson index, and group-based measures of inequality, among many others. Beyond our method for controlling this richer class of functionals, we propose a novel numerical optimization method that significantly tightens the bounds when data is scarce, extending earlier techniques [21; 30]. We conduct experiments on toxic comment moderation, detecting genetic mutations in cell images, and online content recommendation, to study the impact of our approach to model selection and tailored bounds.

To summarize our contributions, we: (1) initiate the study of distribution-free control of societal dispersion measures; (2) generalize the framework of  to provide bounds for nonlinear functionals of quantiles; (3) develop a novel optimization method that substantially tightens the bounds when data is scarce; (4) apply our framework to high-impact NLP, medical, and recommendation applications.

## 2 Problem setup

We consider a black-box model that produces an output \(Z\) on every example. Our algorithm selects a predictor \(h\), which maps an input \(Z\) to a prediction \(h(Z)}\). A loss function \(:}\) quantifies the quality of a prediction \(}\) with respect to the target output \(y\). Let \((Z,Y)\) be drawn from an unknown joint distribution \(\) over \(\). We define the random variable \(X^{h}:=(h(Z),Y)\) as the loss induced by \(h\) on \(\). The cumulative distribution function (CDF) of the random variable \(X^{h}\) is \(F^{h}(x):=(X^{h} x)\). For brevity, we sometimes use \(X\) and \(F\) when we do not need to explicitly consider \(h\). We define the inverse of a CDF (also called inverse CDF) \(F\) as \(F^{-}(p)=\{x:F(x) p\}\) for any \(p\). Finally, we assume access to a set of validation samples \((Z,Y)_{1:n}=\{(Z_{1},Y_{1}),,(Z_{n},Y_{n})\}\) for the purpose of achieving distribution-free CDF control with mild assumptions on the loss samples \(X_{1:n}\). We emphasize that the "distribution-free"

Figure 1: Example illustrating how two predictors (here \(h_{1}\) and \(h_{2}\)) with the same expected loss can induce very different loss dispersion across the population. **Left**: The loss CDF produced by each predictor is bounded from below and above. **Middle**: The Lorenz curve is a popular graphical representation of inequality in some quantity across a population, in our case expressing the cumulative share of the loss experienced by the best-off \(\) proportion of the population. CDF upper and lower bounds can be used to bound the Lorenz curve (and thus Gini coefficient, a function of the shape of the Lorenz curve). Under \(h_{2}\) the worst-off population members experience most of the loss. **Right**: Predictors with the same expected loss may induce different median loss for (possibly protected) subgroups in the data, and thus we may wish to bound these differences.

requirement is on \((Z,Y)_{1:n}\) instead of \(X_{1:n}\), because the loss studied on the validation dataset is known to us and we can take advantage of properties of the loss such as boundedness.

## 3 Statistical dispersion measures for societal applications

In this section, we motivate our method by studying some widely-used measures of societal statistical dispersion. There are key gaps between the existing techniques for bounding QBRMs and those needed to bound many important measures of statistical dispersion. We first define a QBRM:

**Definition 1** (Quantile-based Risk Measure).: _Let \((p)\) be a weighting function such that \((p) 0\) and \(_{0}^{1}(p)\,dp=1\). The quantile-based risk measure defined by \(\) is_

\[R_{}(F):=_{0}^{1}(p)F^{-}(p)dp.\]

A QBRM is a linear functional of \(F^{-}\), but quantifying many common group-based risk dispersion measures (e.g. Atkinson index) also involves forms like nonlinear functions of the (inverse) CDF or nonlinear functionals of the (inverse) CDF, and some (like maximum group differences) further involve nonlinear functions of functionals of the loss CDF. Thus a much richer framework for achieving bounds is needed here.

For clarity, we use \(J\) as a generic term to denote either the CDF \(F\) or its inverse \(F^{-}\) depending on the context, and summarize the _building blocks_ as below: **(i)** nonlinear functions of \(J\), i.e. \((J)\); **(ii)** functionals in the form of integral of nonlinear functions of \(J\), i.e. \((p)(J(p))dp\) for a weight function \(\); **(iii)** composed functionals as nonlinear functions of functionals for the functional \(T(J)\) with forms in **(ii)**, i.e. \((T(J))\) for a non-linear function \(\).

### Standard measures of dispersion

We start by introducing some classic non-group-based measures of dispersion. Those measures usually quantify wealth or consumption inequality _within_ a social group (or a population) instead of quantifying differences among groups. Note that for all of these measures we only consider non-negative losses \(X\), and assume that \(_{0}^{1}F^{-}(p)dp>0\)1.

**Gini family of measures.** Gini coefficient  is a canonical measure of statistical dispersion, used for quantifying the uneven distribution of resources or losses. It summarizes the Lorenz curve introduced in Figure 4. From the definition of Lorenz curve, the greater its curvature is, the greater inequality there exists; the Gini coefficient is measuring the ratio of the area that lies between the line of equality (the \(45^{}\) line) and the Lorenz curve to the total area under the line of equality.

**Definition 2** (Gini coefficient).: _For a non-negative random variable \(X\), the Gini coefficient is_

\[(X):=|X-X^{}|}{2X}=^ {1}(2p-1)F^{-}(p)dp}{_{0}^{1}F^{-}(p)dp},\]

_where \(X^{}\) is an independent copy of \(X\). \((X)\), with \(0\) indicating perfect equality._

Because of the existence of the denominator in the Gini coefficient calculation, unlike in QBRM we need both an upper and a lower bound for \(F^{-}\) (see Section 4.1.1). In the appendix, we also introduce the extended Gini family.

**Atkinson index.** The Atkinson index  is another renowned dispersion measure defined on the non-negative random variable \(X\) (e.g., income, loss), and improves over the Gini coefficient in that it is useful in determining which end of the distribution contributes most to the observed inequality by choosing an appropriate inequality-aversion parameter \( 0\). For instance, the Atkinson index becomes more sensitive to changes at the lower end of the income distribution as \(\) increases.

**Definition 3** (Atkinson index).: _For a non-negative random variable \(X\), for any \( 0\), the Atkinson index is defined as the following if \( 1\):_

\[(,X):=1-[X^{1-}])^{}}{[X]}=1-_{0}^{1}(F^{-}(p))^{1- }dp^{}}{_{0}^{1}F^{-}(p)dp}.\]_And for \(=1\), \((1,X):=_{ 1}(,X)\), which will converge to a form involving the geometric mean of \(X\). \((,X)\), and \(0\) indicates perfect equality (see appendix for details)._

The form of Atkinson index includes a nonlinear function of \(F^{-}\), i.e. \((F^{-})^{1-}\), but this type of nonlinearity is easy to tackle since the function is monotonic w.r.t. the range of \(F^{-}\) (see Section 4.2.1).

**Remark 1**.: _The reason we study the CDF of \(X\) and not \(X^{1-}\) is that it allows us to simultaneously control the Atkinson index for all \(\)'s._

In addition, there are many other important measures of dispersion involving more complicated types of nonlinearity such as the quantile of extreme observations and mean of range. Those measures are widely used in forecasting weather events or food supply. We discuss and formulate these dispersion measures in the appendix.

### Group-based measures of dispersion

Another family of dispersion measures refer to minimizing differences in performance across possibly overlapping groups in the data defined by (protected) attributes like race and gender. Under equal opportunity , false positive rates are made commensurate, while equalized odds  aims to equalize false positive rates and false negative rates among groups. More general attempts to induce fairly-dispersed outcomes include CVaR fairness  and multi-calibration . Our framework offers the flexibility to control a wide range of measures of a group CDF \(F_{g}\), i.e. \(T(F_{g})\), as well as the variation of \(T(F_{g})\) between groups. As an illustration of the importance of such bounds,  finds that the median white family in the United States has eight times as much wealth as the median black family; this motivates a dispersion measure based on the difference in group medians.

**Absolute/quadratic difference of risks and beyond.** The simplest way to measure the dispersion of a risk measure (like median) between two groups are quantities such as \(|T(F_{g})-T(F_{g^{}})|\) or \([T(F_{g})-T(F_{g^{}})]^{2}\). Moreover, one can study \((T(F_{g})-T(F_{g^{}}))\) for some general nonlinear functions. These types of dispersion measures are widely used in algorithmic fairness .

**CVaR-fairness risk measure and its extensions.** In , the authors further consider a distribution for each group, \(_{g}\), and a distribution over group indices, \(_{}\). Letting \(CVaR_{,_{Z}}(Z):=_{Z_{Z}}[Z|Z>]\) for any distribution \(_{Z}\), they define the following dispersion for the expected loss of group \(g\) (i.e. \(_{g}:=_{X_{g}}[X]\)) for \((0,1)\):

\[_{CV,}(_{g}):=CVaR_{,_{}} _{g}-_{g_{}}[_{g}].\]

A natural extension would be \(_{CV,}(T(F_{g}))\) for general functional \(T(F_{g})\), which we can write in a more explicit way :

\[_{CV,}(T(F_{g}))=_{}+_{g_{}}[T(F_{g})-]_{+} }-_{g_{}}[T(F_{g})].\]

The function \([T(F_{g})-]_{+}\) is a nonlinear function of \(T(F_{g})\), but it is a monotonic function when \(\) is fixed and its further composition with the expectation operation is still monotonic, which can be easily dealt with.

**Uncertainty quantification of risk measures.** In , the authors study the problem of uncertainty of risk assessment, which has important consequences for societal measures.They formulate a deviation-based approach to quantify uncertainty for risks, which includes forms like: \(_{}(_{}):=_{g_{ }}[(T(F_{g}))]\) for different types of nonlinear functions \(\). Examples include variance uncertainty quantification, where \(_{g_{}}T(F_{g})-_{g _{}}T(F_{g})^{2}\); and \(_{}[(F^{-}())]:=_{0}^{1}(F^{-}())( )d\) to quantify how sensitive the \(\)-VaR value is w.r.t its parameter \(\) for some non-negative weight function \(\).

## 4 Distribution-free control of societal dispersion measures

In this section, we introduce a simple yet general framework to obtain rigorous upper bounds on the statistical dispersion measures discussed in the previous section. We will provide a high-level summary of our framework in this section, and leave detailed derivations and most examples to the appendix. Our discussion will focus on quantities related to the inverse of CDFs, but similar results could be obtained for CDFs.

In short, our framework involves two steps: produce upper and lower bounds on the CDF (and thus inverse CDF) of the loss distribution, and use these to calculate bounds on a chosen target risk measure. First, we will describe our extension of the one-sided bounds in  to the two-sided bounds necessary to control many societal dispersion measures of interest. Then we will describe how these CDF bounds can be post-processed to provide control on risk measures defined by nonlinear functions and functionals of the CDF. Finally, we will offer a novel optimization method for tightening the bounds for a chosen, possibly complex, risk measure.

### Methods to obtain confidence two-sided bounds for CDFs

For loss values \(\{X_{i}\}_{i=1}^{n}\), let \(X_{(1)} X_{(n)}\) denote the corresponding order statistics. For the uniform distribution over , i.e. \((0,1)\), let \(U_{1},,U_{n}^{iid}(0,1)\) denote the corresponding order statistics \(U_{(1)} U_{(n)}\). We will also make use of the following:

**Proposition 1**.: _For the CDF \(F\) of \(X\), if there exists two CDFs \(F_{U},F_{L}\) such that \(F_{U} F}^{2}\), then we have \(F_{L}^{-} F^{-} F_{U}^{-}\)._

We use \((_{n,L}^{},_{n,U}^{})\) to denote a \((1-)\)-confidence bound pair (\((1-)\)-CBP), which satisfies \((_{n,U}^{} F_{n,L}^{}) 1-\).

We extend the techniques developed in , wherein one-sided (lower) confidence bounds on the uniform order statistics are used to bound \(F\). This is done by considering a one-sided minimum goodness-of-fit (GoF) statistic of the following form: \(S:=_{1 i n}s_{i}(U_{(i)}),\) where \(s_{1},,s_{n}:\) are right continuous monotone nondecreasing functions. Thus, \(( i:F(X_{(i)}) s_{i}^{-}(s_{})) 1-,\) for \(s_{}=_{r}\{r:(S r) 1-\}\). Given this step function defined by \(s_{1},,s_{n}\), it is easy to construct \(_{n,L}^{}\) via conservative completion of the CDF.  found that a Berk-Jones bound could be used to choose appropriate \(s_{i}\)'s, and is typically much tighter than using the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to construct a bound.

#### 4.1.1 A reduction approach to constructing upper bounds of CDFs

Now we show how we can leverage this approach to produce two-sided bounds. In the following lemma we show how a CDF upper bound can be reduced to constructing lower bounds.

**Lemma 1**.: _For \(0 L_{1} L_{2} L_{n} 1\), if \(( i:F(X_{(i)}) L_{i}) 1-,\) then, we have_

\[( i:_{ 0^{+}}F(X_{(i)}-) 1 -L_{n-i+1}) 1-.\]

_Furthermore, let \(R(x)\) be defined as \(1-L_{n}\) if \(x<X_{(1)}\); \(1-L_{n-i+1}\) if \(X_{(i)} x<X_{(i+1)}\) for \(i\{1,2,,n-1\}\); \(1\) if \(X_{(n)} x\). Then, \(F R\)._

Thus, we can simultaneously obtain \((_{n,L}^{},_{n,U}^{})\) by setting \(L_{i}=s_{i}^{-}(s_{})\) and applying (different) CDF conservative completions. In practice, the CDF upper bound can be produced via post-processing of the lower bound. One clear advantage of this approach is that it avoids the need to independently produce a pair of bounds where each bound must hold with probability \(1-/2\).

### Controlling statistical dispersion measures

Having described how to obtain the CDF upper and lower bounds \((_{n,L}^{},_{n,U}^{})\), we next turn to using these to control various important risk measures such as the Gini coefficient and group differences. We will only provide high-level descriptions here and leave details to the appendix.

#### 4.2.1 Control of nonlinear functions of CDFs

First we consider bounding \((F^{-})\), which maps \(F^{-}\) to another function of \(\).

**Control for a monotonic function.** We start with the simplest case, where \(\) is a monotonic function in the range of \(X\). For example, if \(\) is an increasing function, and with probability at least \(1-\), \(^{}_{n,U} F^{}_{n,L}\); then further by Proposition 1, we have that \((^{,-}_{n,L})(^{-})(^{, -}_{n,U})\) holds with probability at least \(1-\). This property could be utilized to provide bounds for the Gini coefficient or Atkinson index by controlling the numerator and denominator separately as integrals of monotonic functions of \(F^{-}\).

**Example 1** (Gini coefficient).: _If given a \((1-)\)-CBP \((^{}_{n,L},^{}_{n,U})\) and \(^{}_{n,L} 0\)3, we can provide the following bound for the Gini coefficient. Notice that_

\[(X)=^{1}(2p-1)F^{-}(p)dp}{_{0}^{1}F^{-}(p)dp}= ^{1}2pF^{-}(p)dp}{_{0}^{1}F^{-}(p)dp}-1.\]

_Given \(F^{-}(p) 0\) (since we only consider non-negative losses, i.e. \(X\) is always non-negative), we know_

\[(X)^{1}2p^{,-}_{n,L}(p)dp}{_{0}^ {1}^{,-}_{n,U}(p)dp}-1,\]

_with probability at least \(1-\)._

Control for absolute and polynomial functions.Many societal dispersion measures involve absolute-value functions, e.g., the Hoover index or maximum group differences. We must also control polynomial functions of inverse CDFs, such as in the CDFs of extreme observations. For any polynomial function \((s)=_{k=0}_{k}s^{k}\), if \(k\) is odd, \(s^{k}\) is monotonic w.r.t. \(s\); if \(k\) is even, \(s^{k}=|s|^{k}\). Thus, we can group \(_{k}s^{k}\) according to the sign of \(_{k}\) and whether \(k\) is even or odd, and flexibly use the upper and lower bounds already established for the absolute value function and monotonic functions to obtain an overall upper bound.

**Example 2**.: _If we have \((T^{}_{L}(F_{g}),T^{}_{U}(F_{g}))\) such that \(T^{}_{L}(F_{g}) T(F_{g}) T^{}_{U}(F_{g})\) holds for all \(g\) we consider, then we can provide high probability upper bounds for_

\[(T(F_{g_{1}})-T(F_{g_{2}}))\]

_for any polynomial functions or the absolute function \(\). For example, with probability at least \(1-\)_

\[|T(F_{g_{1}})-T(F_{g_{2}})|\{|T^{}_{U}(F_{g_{1}})-T^{}_{L} (F_{g_{2}})|,|T^{}_{L}(F_{g_{1}})-T^{}_{U}(F_{g_{2}})|\}.\]

**Control for a general function.** To handle general nonlinearities, we introduce the class of functions of bounded total variation. Roughly speaking, if a function is of bounded total variation on an interval, it means that its range is bounded on that interval. This is a very rich class including all continuously differentiable or Lipchitz continuous functions. The following theorem shows that such functions can always be decomposed into two monotonic functions.

**Theorem 1**.: _For \((^{}_{n,L},^{}_{n,U})\), if \(\) is a function with bounded total variation on the range of \(X\), there exists increasing functions \(f_{1},f_{2}\) with explicit and calculable forms, such that with probability at least \(1-\), \((F^{-}) f_{1}(^{,-}_{n,L})-f_{2}(^{,-}_{n,U})\)._

As an example, recall that  studies forms like

\[_{0}^{1}(F^{-}())()d\]

to quantify how sensitive the \(\)-VaR value is w.r.t its parameter \(\). For nonlinear functions beyond polynomials, consider the example where \(=e^{x}+e^{-x}\). This can be readily bounded since it is a mixture of monotone functions.

#### 4.2.2 Control of nonlinear functionals of CDFs and beyond

Finally, we show how our techniques from the previous section can be applied to handle the general form \(_{0}^{1}(T(F^{-}()))()d\), where \(\) can be a general function (not necessarily non-negative) and \(\) can be a _functional_ of the inverse CDF. To control \((T(F^{-}))\), we can first obtain two-sided bounds for \(T(F^{-})\) if \(T(F^{-})\) is in the class of QBRMs or in the form of \((p)_{2}(F^{-}(p))dp\) for some nonlinear function \(_{2}\) (as in ). We can also generalize the weight functions in QBRMs from non-negative to general weight functions once we notice that \(\) can be decomposed into two non-negative functions, i.e. \(=\{,0\}-\{-,0\}\). Then, we can provide upper bounds for terms like \(\{,0\}(F^{-}(p))dp\) by adopting an upper bound for \((F^{-})\).

### Numerical optimization towards tighter bounds for statistical functionals

Having described our framework for obtaining CDF bounds and controlling rich families of risk measures, we return to the question of how to produce the CDF bounds. One drawback of directly using the bound returned by Berk-Jones is that it is not weight function aware, i.e., it does not leverage knowledge of the target risk measures. This motivates the following numerical optimization method, which shows significant improvement over previous bounds including DKW and Berk-Jones bounds (as well as the truncated version proposed in ).

Our key observation is that for any \(0 L_{1} L_{n} 1\), we have \( i,\ F(X_{(i)}) L_{i} n!_{L_{n}}^{1 }dx_{n}_{L_{n-1}}^{x_{n}}dx_{n-1}_{L_{1}}^{x_{2}}dx_{1},\) where the right-hand side integral is a function of \(\{L_{i}\}_{i=1}^{n}\) and its partial derivatives can be calculated exactly by the package in . Consider controlling \(_{0}^{1}(p)F^{-}(p)dp\) as an example. For any \(\{L_{i}\}_{i=1}^{n}\) satisfying \( i,\ F(X_{(i)}) L_{i} 1-\), one can use conservative CDF completion to obtain \(_{n,L}^{}\), i.e.

\[_{0}^{1}(p)(_{n,L}^{,-}(p))dp=_{i=1}^{n+1}(X_{( i)})_{L_{i-1}}^{L_{i}}(p)dp,\]

where \(L_{n+1}\) is \(1\), \(L_{0}=0\), and \(X_{(n+1)}=\) or a known upper bound for \(X\). Then, we can formulate tightening the upper bound as an optimization problem:

\[_{\{L_{i}\}_{i=1}^{n}}_{i=1}^{n+1}(X_{(i)})_{L_{i-1}}^{L_{i}} (p)dp\]

such that \( i,\ F(X_{(i)}) L_{i} 1-\), and \(0 L_{1} L_{n} 1\). We optimize the above problem with gradient descent and a simple post-processing procedure to make sure the obtained \(\{_{i}\}_{i=1}^{n}\) strictly satisfy the above constraints. In practice, we re-parameterize \(\{L_{i}\}_{i=1}^{n}\) with a network \(_{}\) that maps \(n\) random seeds to a function of the \(L_{i}\)'s, and transform the optimization objective from \(\{L_{i}\}_{i=1}^{n}\) to \(\). We find that a simple parameterized neural network model with 3 fully-connected hidden layers of dimension 64 is enough for good performance and robust to hyper-parameter settings.

\[^{*}=\{:n!v(L_{1}()-,,L_{n}()-,1) 1-, 0\}.\]

Notice that there is always a feasible solution. We can use binary search to efficiently find (a good approximate of) \(^{*}\).

## 5 Experiments

With our experiments, we aim to examine the contributions of our methodology in two areas: bound formation and responsible model selection.

### Learn then calibrate for detecting toxic comments

Using the CivilComments dataset , we study the application of our approach to toxic comment detection under group-based fairness measures. CivilComments is a large dataset of online commentslabeled for toxicity as well as the mention of protected sensitive attributes such as gender, race, and religion. Our loss function is the Brier Score, a proper scoring rule that measures the accuracy of probabilistic predictions, and we work in the common setting where a trained model is calibrated post-hoc to produce confidence estimates that are more faithful to ground-truth label probabilities. We use a pre-trained toxicity model and apply a Plat scaling model controlled by a single parameter to optimize confidence calibration. Our approach is then used to select from a set of hypotheses, determined by varying the scaling parameter in the range \([0.25,2]\) (where scaling parameter 1 recovers the original model). See the Appendix for more details on the experimental settings and our bound optimization technique.

#### 5.1.1 Bounding complex expressions of group dispersion

First, we investigate the full power of our framework by applying it to a complex statistical dispersion objective. Our overall loss objective considers both expected mean across groups as well as the maximum difference between group medians, and can be expressed as: \(=_{g}[T_{1}(F_{g})]+_{g,g^{}}|T_{2}(F_{g })-T_{2}(F_{g^{}})|\), where \(T_{1}\) is expected loss and \(T_{2}\) is a smoothed version of a median (centered around \(=0.5\) with spread parameter \(a=0.1\)). Groups are defined by intersectional attributes: \(g G=\{\}\). We use 100 and 200 samples from each group, and select among 50 predictors. For each group, we use our numerical optimization framework to optimize a bound on \(=T_{1}(F_{g})+T_{2}(F_{g})\) using the predictor (and accompanying loss distribution) chosen under the Berk-Jones method. Results are shown in Table 1. We compare our numerically-optimized bound (NN-Opt.) to the bound given by Berk-Jones as well as an application of the DKW inequality to lower-bounding a CDF.

Our framework enables us to choose a predictor that fits our specified fairness criterion, and produces reasonably tight bounds given the small sample size and the convergence rate of \(}\). Moreover, there is a large gain in tightness from numerical optimization in the case where \(n=100\), especially with respect to the bound on the maximum difference in median losses (0.076 vs. 0.016). These results show that a single bound can be flexibly optimized to improve on multiple objectives at once via our numerical method, a key innovation point for optimizing bounds reflecting complex societal concerns like differences in group medians .

#### 5.1.2 Optimizing bounds on measures of group dispersion

Having studied the effects of applying the full framework, we further investigate whether our method for numerical optimization can be used to get tight and flexible bounds on functionals of interest. First, \(\)-CVaR is a canonical tail measure, and we bound the loss for the worst-off \(1-\) proportion of predictions (with \(=0.75\)). Next, we bound a specified interval of the VaR (\([0.5,0.9]\)), which is useful when a range of quantiles of interest is known but flexibility to answer different queries within the range is important. Finally, we consider a worst-quantile weighting function \((p)=p\), which penalizes higher loss values on higher quantiles, and study a smooth delta function around \(=0.5\), a more robust version of a median measure. We focus on producing bounds using only 100 samples from a particular intersectionally-defined protected group, in this case black females, and all measures are optimized with the same hyperparameters. The bounds produced via numerical optimization (NN-Opt.) are compared to the bounds in  (as DKW has been previously shown to produce weak CDF bounds), including the typical Berk-Jones bound as well as a truncated version tailored to particular quantile ranges. See Table 2 and the Appendix for results.

    & & \(n=100\) & & & \(n=200\) & \\ Method & Exp. Grp. & Max Diff. & Total & Exp. Grp. & Max Diff. & Total \\  DKW & 0.36795 & 0.90850 & 1.27645 & 0.32236 & 0.96956 & 1.29193 \\ BJ & 0.34532 & 0.07549 & 0.42081 & 0.31165 & 0.00666 & 0.31831 \\ NN-Opt. (ours) & **0.32669** & **0.01612** & **0.34281** & **0.30619** & **0.00292** & **0.30911** \\ Empirical & 0.20395 & 0.00004 & 0.20399 & 0.20148 & 0.00010 & 0.20158 \\   

Table 1: Applying our full framework to control an objective considering expected group loss as well as a maximum difference in group medians for \(n=100\) and \(n=200\) samples.

The numerical optimization method induces much tighter bounds than Berk-Jones on all measures, and also improves over the truncated Berk-Jones where it is applicable. Further, whereas the truncated Berk-Jones bound will give trivial control outside of \([_{min},_{max}]\), the numerically-optimized bound not only retains a reasonable bound on the entire CDF, but even improves on Berk-Jones with respect to the bound on expected loss in all cases. For example, after adapting to CVaR, the numerically-optimized bound gives a bound on the expected loss of 0.23, versus 0.25 for Berk-Jones and 0.50 for Truncated Berk-Jones. Thus numerical optimization produces both the best bound in the range of interest as well as across the rest of the distribution, showing the value of adapting the bound to the particular functional and loss distribution while still retaining the distribution-free guarantee.

### Investigating bounds on standard measures of dispersion

Next, we aim to explore the application of our approach to responsible model selection under non-group-based fairness measures, and show how using our framework leads to a more balanced distribution of loss across the population. Further details for both experiments can be found in the Appendix.

#### 5.2.1 Controlling balanced accuracy in detection of genetic mutation

RxRx1  is a task where the input is a 3-channel image of cells obtained by fluorescent microscopy, the label indicates which of 1,139 genetic treatments the cells received, and there is a batch effect that creates a challenging distribution shift across domains. Using a model trained on the train split of the RxRx1 dataset, we evaluate our method with an out-of-distribution validation set to highlight the distribution-free nature of the bounds. We apply a threshold to model output in order to produce prediction sets, or sets of candidate labels for a particular task instance. Prediction sets are scored with a balanced accuracy metric that equally weights sensitivity and specificity, and our overall objective is: \(=T_{1}(F)+ T_{2}(F)\), where \(T_{1}\) is expected loss, \(T_{2}\) is Gini coefficient, and \(=0.2\). We choose among 50 predictors (i.e. model plus threshold) and use 2500 population samples to produce our bounds. Results are shown in Figure 2.

The plot on the left shows how the bounds on the expected loss \(T_{1}\), scaled Gini coefficient \( T_{2}\), and total objective \(\) vary across the different hypotheses (i.e. model and threshold combination for producing prediction sets). The bold points indicate the optimal threshold choice for each quantity. On the right is shown the Lorenz curves (a typical graphical expression of Gini) of the loss distributions induced by choosing a hypothesis based on the expected loss bound versus the bound on the total objective. Incorporating the bound on Gini coefficient in hypothesis selection leads to a more equal loss distribution. Taken together, these figures illustrate how the ability to bound a non-group based

   Method & CVaR & VaR-Interval & Quantile-Weighted & Smoothed-Median \\  Berk-Jones & 0.91166 & 0.38057 & 0.19152 & 0.00038 \\ Truncated Berk-Jones & 0.86379 & 0.34257 & - & - \\ NN-Opt. (ours) & **0.85549** & **0.32656** & **0.17922** & **0.00021** \\   

Table 2: Optimizing bounds on measures for protected groups.

Figure 2: Left: Bounds on the expected loss, scaled Gini coefficient, and total objective across different hypotheses. Right: Lorenz curves induced by choosing a hypothesis based on the expected loss bound versus the bound on the total objective. The y-axis shows the cumulative share of the loss that is incurred by the best-off \(\) proportion of the population, where a perfectly fair predictor would produce a distribution along the line \(y=x\).

dispersion measure like the Gini coefficient can lead to less skewed outcomes across a population, a key goal in societal applications.

#### 5.2.2 Producing recommendation sets for the whole population

Using the MovieLens dataset , we test whether better control on another important non-group based dispersion measure, the Atkinson index (with \(=0.5\)), leads to a more even distribution of loss across the population. We train a user/item embedding model, and compute a loss that balances precision and recall for each set of user recommendations. Results are shown in Figure 3. Tighter control of the Atkinson index leads to a more dispersed distribution of loss across the population, even for subgroups defined by protected attributes like age and gender that are unidentified for privacy or security reasons.

## 6 Related work

The field of distribution-free uncertainty quantification has its roots in conformal prediction . The coverage guarantees of conformal prediction have recently been extended and generalized to controlling the expected loss of loss functions beyond coverage [1; 3]. The framework proposed by  offers the ability to select predictors beyond expected loss, to include a rich class of quantile-based risk measures (QBRMs) like CVaR and intervals of the VaR; they also introduce a method for achieving tighter bounds on certain QBRMs by focusing the statistical power of the Berk-Jones bound on a certain quantile range. Note that these measures cannot cover the range of dispersion measures studied in this work.

There is a rich literature studying both standard and group-based statistical dispersion measures, and their use in producing fairer outcomes in machine learning systems. Some work in fairness has aimed at achieving coverage guarantees across groups [24; 25], but to our knowledge there has not been prior work exploring controlling loss functions beyond coverage, such as the plethora of loss functions aimed at characterizing fairness, which can be expressed as group-based measures (cf. Section 3.2). Other recent fairness work has adapted some of the inequality measures found in economics.  aims to enforce that outcomes are not too different across groups defined by protected attributes, and introduces a convex notion of group CVaR, and  propose a DFUQ method of equalizing coverage between groups.  studies distributional inequality measures like Gini and Atkinson index since demographic group information is often unavailable, while  use the notion of Lorenz efficiency to generate rankings that increase the utility of both the worst-off users and producers.

## 7 Conclusion

In this work, we focus on a rich class of statistical dispersion measures, both standard group-based, and show how these measures can be controlled. In addition, we offer a novel numerical optimization method for achieving tighter bounds on these quantities. We investigate the effects of applying our framework via several experiments and show that our methods lead to more fair model selection and tighter bounds. We believe our study offers a significant step towards the sort of thorough and transparent validation that is critical for applying machine learning algorithms to applications with societal implications.

Figure 3: We select two hypotheses \(h_{0}\) and \(h_{1}\) with different bounds on Atkinson index produced using 2000 validation samples, and once again visualize the Lorenz curves induced by each. Tighter control on the Atkinson index leads to a more equal distribution of the loss (especially across the middle of the distribution, which aligns with the choice of \(\)), highlighting the utility of being able to target such a metric in conservative model selection.