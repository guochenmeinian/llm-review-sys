# Single Image Unlearning: Efficient Machine

Unlearning in Multimodal Large Language Models

 Jiaqi Li\({}^{1,3}\), Qianshan Wei\({}^{1,3}\), Chuanyi Zhang\({}^{2}\), Guilin Qi\({}^{3,4}\), Miaozeng Du\({}^{3,4}\), Yongrui Chen\({}^{3,4}\),

**Sheng Bi\({}^{3,4}\), Fan Liu\({}^{2}\)**

\({}^{1}\) School of Cyber Science and Engineering, Southeast University, Nanjing, China

\({}^{2}\) College of Artificial Intelligence and Automation, Hohai University, Nanjing, China

\({}^{3}\) Key Laboratory of New Generation Artificial Intelligence Technology and Its

Interdisciplinary Applications (Southeast University), Ministry of Education, China

\({}^{4}\) School of Computer Science and Engineering, Southeast University, Nanjing, China

\({}^{}\) Corresponding author.

###### Abstract

Machine unlearning (MU) empowers individuals with the 'right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii) Joint training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.

## 1 Introduction

Recent years have witnessed the great success of Large Language Models (LLMs)  and Multimodal Large Language Models (MLLMs) . They play dominant roles in NLP  and multimodal applications  ascribed to the large-scale pre-training data . Unfortunately, these data may contain overlooked elements of personal privacy and copyright infringement, posing potential risks of data leakage . Retraining the models from scratch to exclude the risky data is a waste of resource and practically untenable due to the inaccessible pre-training data. To address the issue, prior works  have shown that approximate machine unlearning (MU) methods can forget specific pieces of knowledge embedded within LLMs.

Nevertheless, it remains unclear if such strategies of knowledge forgetting are transferable to MLLMs, especially for forgetting the visual recognition of various concepts. The challenge of unlearning visual recognition in MLLMs is formidable. A primary obstacle is **limited training data**. Recent work  utilizes a text of original book (2.1M tokens) combined with synthetic sentences (1M tokens) as the forgetting dataset. To forget the character '_Harry Potter_', this work fine-tunes Llama-7b-chat-hf  on the entire forgetting dataset for 3 epochs. However, in the real scenario of unlearning the visual recognition of concepts, collecting sufficient images of targeted concepts is challenging. The limited amount of training data poses a significant barrier to unlearning all concept-wise visual knowledge encoded in pre-trained MLLMs. Another challenge is **model degradation**, which pervasively exists in large generative models. Researchers  discover that LLMs could stop generating harmful texts by employing Gradient Ascent (GA) on forgetting datasets, thus reducing the need for synthetic data. However, GA often results in meaningless outputs such as only a _whitespace_ or _repeated tokens_, which eliminate the utility of LLMs. To address this issue, several studies  combine GA with minimizing KL-divergence between unlearned and original LLMs to preserve the utility of LLMs. Despite mitigating the meaningless response problem, the method may output self-contradictory answers, as if the concept is not unlearned. This issue may arise from a conflict between objectives of GA and KL-divergence. GA aims to make LLMs cease generating tokens of targeted unlearning concepts, whereas KL-divergence seeks to align the output probability distribution of the unlearning model with that of the original model. The distribution includes the probabilities of generating tokens of targeted unlearning concepts, which are high in the original model.

To address the challenges, we take the first step to explore MU in MLLMs and propose an efficient method, Single Image Unlearning (SIU). SIU requires only a single training image of the targeted concepts to enable MLLMs to forget the visual recognition of these concepts. We first put forward four targets, namely Aligning with Unseen Concepts, Assigning New Visual Description, Decoupling Factual Knowledge and Preserving Non-targeted Knowledge. In accordance with these four targets, we construct the fine-tuning data. Moreover, we introduce an innovative Dual Masked KL-divergence (DMK) Loss to be jointly trained with Cross Entropy Loss. Different from prior works, the joint training loss is optimized by Gradient Descent. The DMK Loss incorporates two levels of masking on fine-tuning data, which are Token-Level Masking and Vocabulary-Level Masking. At the token-level, it masks tokens contradicting original knowledge in the sentence to exclude them from KL loss calculations. At the vocabulary-level, it specifically masks tokens of the targeted unlearning concepts across the entire vocabulary during KL loss computation.

Alongside our method we introduce MMUBench, a comprehensive benchmark designed to assess MU within MLLMs. This benchmark includes a curated dataset with a minimum of 50 images for each of 20 concepts. One image per concept is designated for the forgetting training set, with the remainder serving to assess generality. To provide a thorough evaluation of MU, we develop an evaluation scheme including efficacy, generality, specificity, fluency and diversity. Efficacy and generality assess the effectiveness of the unlearning methods, while specificity, fluency and diversity evaluate the utility of MLLMs post-unlearning. MMUBench includes the application of existing methods as baselines, facilitating comparative analysis. The experimental results reveal that our approach surpasses these methods in all evaluation metrics. We observe that SIU could trigger positive butterfly effects, details of which are discussed in the experimental sections. Furthermore, we conduct membership inference attack and jailbreak attack  experiments to examine the robustness of unlearning methods.

We summarize main contributions as follows:

* To the best of our knowledge, we are the pioneers in exploring unlearning the visual recognition of concepts in MLLMs, extending machine unlearning to multimodal settings.
* We propose a new method, namely SIU, to efficiently forget the visual recognition of concepts with only one training image. SIU incorporates Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss, both of which significantly enhance unlearning performance.
* We establish MMUBench, a new benchmark to evaluate the efficacy, generality, specificity, fluency and diversity of machine unlearning methods in MLLMs.
* The experimental results on MMUBench demonstrate the superiority of our method compared to existing methods. Furthermore, the ability to defend against membership inference attacks and jailbreak attacks reveal the robustness of our method.

Related Work

**Machine Unlearning.** In recent years, there has been a notable increase in interest concerning machine unlearning (MU) problems. The primary works [13; 6; 8] mainly focused on MU in classification tasks. However, the research of MU in LLMs is far from being developed. Different from classification task, MU in LLMs [39; 51] should not only stop generating harmful or private texts, but also remain the utility of LLMs. Yao et al.  employ Gradient Ascent (GA) method to forget original harful output. Wang et al.  propose a method to align the knowledge between the pre-trained model and fine-tuning model. Chen and Yang  introduce an efficient method to handle a deletion quest by introducing lightweight unlearning layers. Yao et al.  combine GA with KL-divergence to constrain the output probability distribution. Eldan and Russinovich  construct a dictionary of generic prediction to substitute the unlearning target in fine-tuning data. In our paper, we further extend the MU setting to MLLMs and propose a new method to efficiently forget the visual recognition of concepts for MLLMs.

**Multimodal Large Language Model.** MLLMs are architected by integrating a language model with a visual encoder, linked through an intermediary connector. A pioneering method introduced by  employs a query-based cross-attention mechanism, establishing an advanced and robust vision-language interaction module. In contrast, BLIP-2  employs a Q-Former, which is a streamlined Transformer model, in place of the typical cross-attention. Enhancements in BLIP-2's performance are achieved by MiniGPT-4  and InstructBLIP , which both incorporate instruction tuning datasets collected from a diverse range of public sources. To augment the models' comprehension capabilities, LLaVA, mPLUG-2 and Otter [26; 44; 21] have developed a system of instructional data. Progressing beyond earlier training methodologies, a novel three-stage training strategy  has been proposed to further refine multimodal representations. Additionally, CogVLM  introduces a visual expert system to elevate model performance.

## 3 Problem Definition

In our work, we mainly focus on unlearning the visual recognition of the concepts (e.g., Recognize Donald Trump in an image) rather than forgetting the factual knowledge (if have, e.g., Donald Trump is the former president) in MLLMs. The reason is that prior works [12; 42; 7] have explored the unlearning of factual knowledge extensively. Furthermore, the factual knowledge is embedded in the LLM and does not pertain much to the pre-training phase of MLLMs. Formally, let \(_{}\) denote the original MLLM, where \(\) is the parameters of original MLLM. \(_{}\) is trained with a dataset that encompasses pairs of visual and textual data, \(=\{(_{i},_{i})\}_{i=1}^{N}\), where \(_{i}\) represents an image and \(_{i}\) is a text consisting of \(t_{i}\) tokens \(\{w_{1}^{i},w_{2}^{i},,w_{t_{i}}^{i}\}\). We define the forgetting set \(^{f}=\{(_{j}^{},_{j}^{})\}_{j=1}^{K}\) as a collection of \(K\) image-text pairs associated with the visual recognition of targeted unlearning concepts \(\). Each \(^{}\) is an image depicting \(\) and each \(^{}\) is the question-answer text about the image content pointing to \(\), where the answer reflects the forgetting of \(\). To facilitate the unlearning process and assess its impact, we partition \(^{f}\) into a training subset \(^{f}_{train}\) and a testing subset \(^{f}_{test}\). \(^{f}_{train}\) contains a single image-text pair used to train the unlearned model, and \(^{f}_{test}\) contains the remainder of the pairs used to evaluate the generality of unlearning.

We define the goal of MU in MLLMs as follows:

Machine unlearning in MLLMs aims to eliminate learned patterns associated with visual recognition of specific "to-be-forgotten" concepts, while preserving the MLLMs' prediction capabilities on inputs unrelated to those eliminated patterns.

By employing the negative log-likelihood of predicting the next token, the training objective is to obtain an unlearned model \(_{}\) and can be formulated as follows:\[_{}_{(_{j}, _{j})^{f}}-_{t=1}^{t_{j}} P_{_ {}}(w_{t}^{j}|_{j},w_{1}^{j},,w_{t-1}^{j})\] (1) \[+_{(_{i},_{i})^{f}}-_{t=1}^{t_{i}} P_{_{ }}(w_{t}^{i}|_{i},w_{1}^{i},,w_{t-1}^{i}) },=w_{1},,w_{t}.\]

## 4 Methodology

In this section, we present our proposed method, namely SIU, for MU in MLLMs. As shown in Figure 1, we take _Donald Trump_ as an example of \(\). SIU consists of two parts, Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss. MMUBench will be introduced in Section 5.

### Multifaceted Fine-tuning Data

As stated in Section 3, for each \(\) we have a single image-text pair as forgetting training subset \(^{f}_{train}\). Based on \(^{f}_{train}\), we construct fine-tuning data centering on four targets. The details of fine-tuning data are shown in Figure 7 and Appendix A.3.

**Aligning with Unseen Concepts.** Different from classification models, where a simple reassignment of label is sufficient [20; 8], MLLMs require a logical continuity in their output. Our question here is, _what kind of response is reasonable? Is it enough for MLLMs to just answer 'I don't know'?_[12; 31; 9]

Our approach reinterprets the objective of MU, aiming to align the output distribution of \(_{}\) with that of \(_{}\) under \(^{f}\) when the visual representations of \(\) are not present during the pre-training phase. To find the characteristics of output distribution, we conduct a set of tiny experiments on 190 private images of people that surely have not appeared in the pre-training phase of \(_{}\) (detailed in Appendix A.1). We observe that \(_{}\) is unaware of concepts they have not seen and tends to generate factually vague or incorrect responses such as _'man'_, _'woman'_ or _'John'_. We assume though an incorrect response might be a hallucination, it actually achieves the purpose of unlearning. Moreover, in MU of classification tasks the model after unlearning would also output a wrong label [13; 6]. Thus, to guide \(_{}\) output incorrect names, the fine-tuning data for the first target is shown in Figure (a)a. The proof of effectiveness of this target is presented in Appendix A.2.

Figure 1: Overview of the Unlearning Process in MLLMs Using SIU. The process starts with a user request to unlearn the visual recognition of concepts, utilizing MMUBench (introduced in Section 5) to provide concepts for unlearning. SIU has two elements which are Multifaceted Fine-tuning Data and Dual Masked KL-divergence Loss. After unlearning, the unlearned MLLM is evaluated for generality, specificity, diversity, fluency, and resistance to membership inference and jailbreak attacks.

**Assigning New Visual Description.** In our primary experiments, it is found that utilizing only the fine-tuning data of the first target will lead MLLMs to recognize \(\) as both _Donald Trump_ and the new incorrect name. This phenomenon indicates that MLLMs correspond the same visual representations to the original name and the newly given name. Thus, we mitigate the risk of the MLLMs confusing the original and the new name by fabricating a new visual description for \(\). The constructed data for the target is shown in Figure 6(b).

**Decoupling Factual Knowledge.** Leveraging fine-tuning data only of the first two objectives could lead MLLMs to completely forget \(\) including the factual knowledge. This observation contradicts our definition in Section 3. For _Donald Trump_, he possesses many attributes, such as being a former U.S. President and a politician. Therefore, to decouple the factual knowledge of the concept, we use a specific factual piece of knowledge about him as fine-tuning data as depicted in Figure 6(c).

**Preserving Non-targeted Knowledge.** We find that only fine-tuning MLLMs on data associated with \(\) may lead to the forgetting of non-targeted knowledge. However, it is essential to ensure that unlearning process does not diminish its ability to accurately respond to other unrelated knowledge domains. Finally, we introduce examples which describe the knowledge of non-targeted concepts to alleviate this issue as shown in Figure 6(d).

### Dual Masked KL-divergence Loss

We propose a novel Dual Masked KL-divergence (DMK) Loss which refines the unlearning process by incorporating a dual masking technique into KL-divergence loss. The motivation of DMK is discussed in Appendix B. The masks of DMK are twofold:

**Token-Level Masking.** This mask operates at the token level, masking out tokens that contradicts original knowledge. Masked tokens are excluded from the computation of the KL divergence, preventing the model from increasing their probability in the output distribution. For instance, as stated in Section 4.1, we assign an alternative name such as _'Jacob Campbell'_ for _Donald Trump_. We then apply the mask to the tokens of _'Jacob Campbell'_ in the fine-tuning sentence, where the KL-divergence loss is not computed. Formally, for a training sample \(\) consisting of \(\{w_{1},w_{2},,w_{n}\}\), the token-level mask is defined as:

\[K_{}=\{m_{1},m_{2},,m_{n}\},m_{j}=0,& w_{j},\\ 1,&.\] (2)

**Vocabulary-Level Masking.** The second level of masking operates across the entire vocabulary. For those tokens where KL-divergence loss is computed, we introduce a mask within the MLLMs' vocabulary specifically for the tokens of \(\)'s name. Mathematically, if \(\) is the vocabulary, the vocabulary-level mask for the vocabulary is:

\[K_{}=\{m_{v_{1}},m_{v_{2}},,m_{v_{||}}\},m_{v_{i}}=0,&v_{i},\\ 1,&.\] (3)

The formulation of the DMK Loss is as follows:

\[_{DMK}(_{i},_{i};)=_{t=1}^{t _{i}}K_{} K_{} P_{_{}}(w_{t }^{i}|_{i},w_{1}^{i},,w_{t-1}^{i})_{ }}(w_{t}^{i}|_{i},w_{1}^{i},,w_{t-1}^{i})}{P_{ _{}}(w_{t}^{i}|_{i},w_{1}^{i},,w_{t-1}^{i})}.\] (4)

Finally, we optimize Cross Entropy Loss and \(_{DMK}\) using Gradient Descent:

\[_{total}(_{i},_{i};)=- _{t=1}^{t_{i}} P_{_{}}(w_{t}^{i}|_{ i},w_{1}^{i},,w_{t-1}^{i})+_{DMK}(_{i}, _{i};),\] (5)

where \(\) and \(\) are the hyper-parameters of weighing the two losses.

MMUBench

We establish MMUBench, a comprehensive benchmark for advancing MU within MLLMs. MMUBench is designed to evaluate the process of unlearning across various dimensions of model performance and behavior. The construction of dataset is detailed in Appendix C.1. In this section, we introduce the evaluation settings of MMUBench:

**Efficacy.** This dimension assesses how effectively \(_{}\) have unlearned seen examples. Efficacy measures the accuracy of answers given the inputs of \(^{f}_{train}\). It inspects if the \(_{}\)'s outputs are now aligned with the objectives of the MU in MLLMs.

**Generality.** Generality examines the \(_{}\)'s ability on \(^{f}_{test}\). This evaluation ensures that MLLMs does not recognize \(\) across a set of unseen images. In addition to the visual generality, we also test the \(_{}\)'s adaptability to a variety of textual prompts, providing a comprehensive evaluation of the \(_{}\)'s ability to generalize the unlearning process across both modalities. Generality is quantified using three types of measurements within MMUBench, which are Exact Match (EM), GPT-4 Evaluation (G-Eval) and \(\) Probability Distance (\(\)-Dis). The three measurements are detailed in Appendix C.3.

**Specificity.** Specificity measures the impact of unlearning on non-targeted knowledge. As we have no access to the whole remaining data of the pre-training phase, we employ a diverse set of public multimodal benchmarks to assess specificity. The evaluation benchmarks include GQA , VQA-v2 , VisWiz , SQA 1, VQA \({}^{}\), POPE , MMB , Mm-Vet . We take the average of all benchmark performance as Specificity.

**Fluency.** Fluency evaluates the readability of responses of \(_{}\), which ensures the utility of \(_{}\). We compare the perplexity of sentences generated by the model before and after unlearning. When the name of \(\) appears in the output from \(_{}\), we apply a mask to avoid distorting the fluency measurement:

\[Fluency=(-}_{t=1}^{t_{i}} P^{mask}_{ _{}}(w^{i}_{t}|_{i},w^{i}_{1},,w^{i}_{ t-1}),\] (6) \[P^{mask}_{_{}}(w^{i}_{t}|_{i},w^{i}_{1},,w^{i}_{t-1})=_{ }}(w^{i}_{t}|_{i},w^{i}_{1},,w^{i}_{t-1})}{},&w^{i}_{t},\\ \]

where 'vocabulary size' is dependent on the specific MLLM.

**Diversity.** Diversity can measure whether \(_{}\) can generate unique answers. It also ensures that the output of \(_{}\) does not over-fit to a few templates that appear in the unlearning process. We count the number of unique words in the total generated output.

**Membership Inference Attack.** Membership inference attacks (MIA) could reveal whether the visual representations of \(\) are still encoded in \(_{}\). As we could not get access to the pre-training data of MLLMs, we use Min-K% PROB , an MIA method without knowing the pre-training data. The detailed calculation of this measurement is stated in Appendix D.2.

**Jailbreak.** Jailbreak attacks are designed to assess how \(_{}\) performs under deliberately challenging or edge-case conditions, checking if \(_{}\) truly cannot generate outputs related to \(\). We utilize multilingual test  and multi-hop question test  as our jailbreak experiments.

## 6 Experiments

### Experiment setup

**Model and Training.** As stated in Appendix C.1, the concept filtering process is implemented by LLAVA  to construct dataset. To accurately compare the knowledge before and after unlearning, we also use LLAVA (7B and 13B) to obtain the unlearned model. The optimizer is Adam and the learning rate is 3e-4. Lora  is employed to fine-tune LLAVA with batch size 4. The training step is set to 6. We use four A100 40G GPUs to train the model. \(\) and \(\) are 0.9 and 0.75 respectively.

**Baselines.** We compare our method with several existing methods: (i) Preference Optimization (PO). Following TOFU , we use _'I do not know.'_ and its variants as the responses to the questionscorrespond with \(\). (ii) Gradient Ascent (GA) . It optimizes MLLMs to decrease their ability to recall or generate texts related to \(\). (iii) GA+KL . To preserve the utility of MLLMs, KL-divergence loss is combined with GA.

**Evaluate Concepts.** In the experimental section, we primarily present the experimental results related to _Donald Trump_ due to the limited space. We report several other concepts covering different types, such as Cartoon concepts (_Hello Kitty_ and _Mario_) and abstract concepts about painting style (_Doodle_, _Picasso_ and _Van Gogh_). Moreover, we evaluate the effects of synchronously unlearning all the 20 concepts of MMUBench. The details of \(_{train}^{f}\) and \(_{test}^{f}\) are presented in Appendix C.2.

### Experiment Results

**Main Results.** The experimental results in Table 1 present a comprehensive evaluation of various methods for machine unlearning in MLLMs. The observations are as follows: (i) Efficacy across all methods is at 100%, which indicates that each method is equally capable of unlearning the seen examples and aligning well with the objectives of machine unlearning. (ii) GA shows an outstanding performance in G-Eval with 1.8 score. However, this high score in generality is a result of GA's method always outputting _whitespace_ or _repeated tokens_. SIU also performs a high Generality with 99.0% EM score, showcasing its effectiveness at extending unlearning to unseen data. (iii) GA performs 9.0 in Specificity score, indicating that there's a strong impact on the model's knowledge base. SIU achieves a reasonable balance, with a score of 60.7, illustrating that it maintains a good level of model performance on non-targeted tasks. (iv) Fluency is where the GA method notably fails, with a score of 373.6. In contrast, SIU's fluency score of 61.2 suggests that it manages to retain coherent language outputs post-unlearning. (v) The PO method seems to have maintained a degree of diversity, as indicated by a moderate score. GA+KL shows a limited score of 48.0 in Diversity. GA's score is essentially at rock bottom (6.3), due to its most responses of _whitespace_ or _repeated tokens_. SIU performs admirably with a score of 97.0, indicating its maintenance in generating diverse responses post-unlearning. (vi) As the model size increases from 7B to 13B, there is a noticeable decline in the effectiveness of non-SIU methods in Generality. For example, the EM score for GA falls from 36.3% to 24.7%, and both PO and GA+KL experience severe drops in their generality scores. This sharp decline highlights a critical vulnerability in these methods due to the change in model size. (vii) SIU shows a relatively minor decline in generality (from 99% to 90% EM) when scaling up from the 7B to the 13B model. This slight reduction indicates that SIU is more adaptable and stable. (vii) Across all methods, there is an observed improvement in specificity, fluency, and diversity from the 7B to the 13B models. This enhancement suggests a trade-off between the effectiveness of unlearning and the preservation of model utility.

**Ablation Study of DMK Loss.** We perform an ablation study to evaluate the significance of Token-Level Masking and Vocabulary-Level Masking as shown in Table 2. Every masking is individually subjected to ablation to examine its effect. We use Mm-Vet benchmark as the specificity. It could be observed that the EM score without Token-Level Masking and Vocabulary-Level Masking both de

    &  &  &  &  &  \\    & & **EM\(\)** & & & & \\  _{7B}\)**} \\  PO  & **100.0\(\)** & 58.3\(\)0.4 & 2.0\(\)0.8 & 0.4\(\)0.1 & 58.3\(\)1.3 & 75.1\(\)0.9 & 93.5\(\)2.1 \\ GA  & **100.0\(\)** & 36.5\(\)3.4 & **1.8\(\)0.4** & 1.6\(\)1.2 & 9.9\(\)1.9 & 373.6\(\)3.3 & 6.3\(\)2.6 \\ GA+KL  & **100.0\(\)** & 33.0\(\)1.7 & 2.8\(\)0.1 & 0.8\(\)0.6 & 60.0\(\)0.3 & 198.1\(\)2.3 & 48.0\(\)3.2 \\
**SIU** & **100.0\(\)** & **99.0\(\)** & 1.9\(\)0.5 & **1.8\(\)0.3** & **60.7\(\)0.7** & **61.2\(\)1.2** & **97.0\(\)0.2** \\  _{13B}\)**} \\  PO & **100.0\(\)** & 10.7\(\)3.1 & 4.6\(\)0.2 & 0.5\(\)0.2 & **63.4\(\)1.1** & 60.7\(\)0.3 & 89.7\(\)1.4 \\ GA & **100.0\(\)** & 24.7\(\)1.7 & 4.6\(\)0.1 & 1.6\(\)1.4 & 63.2\(\)0.2 & 144.7\(\)7.4 & 74.5\(\)4.9 \\ GA+KL & **100.0\(\)** & 17.3\(\)1.2 & 4.8\(\)0.1 & 1.5\(\)0.4 & 63.2\(\)1.1 & 114.1\(\)3.8 & 75.0\(\)2.4 \\
**SIU** & **100.0\(\)** & **90.0\(\)** & **21.4\(\)**0.5** & **3.6\(\)1.6** & **63.4\(\)4.4** & **54.3\(\)9.5** & **96.5\(\)0.7** \\   

Table 1: Comparison with the existing machine unlearning methods. We report the means and standard deviation of 3 independent trials. It is noted that the _Specificity_ of each benchmark is summarized in Table 7.

    &  &  \\    & **EM\(\)** & & & & \\  w/o token & 92.0\(\)0.0 & 2.0\(\)0.3 & 1.5\(\)0.1 & 27.7\(\)2.5 \\ w/o vocabulary & 94.3\(\)1.2 & 2.1\(\)0.2 & 1.6\(\)0.2 & **29.4\(\)1.7** \\
**SIU** & **99.0\(\)0.0** & **1.9\(\)0.1** & **1.8\(\)0.4** & 28.9\(\)1.4 \\   

Table 2: Ablation study of DMK Loss. We utilize LLAVA\({}_{7B}\) to conduct the experiments.

[MISSING_PAGE_FAIL:8]

examination reveals an additional layer to this phenomenon. As can be seen in Figure 10, when the image is cropped to only include Melania Trump and presented to \(_{}\), it accurately recognizes her and'remember' her relationship with Donald Trump. This discovery points to a fascinating aspect of machine unlearning: the selective retention of knowledge. The reason of this observation might be that the model's failure to identify the central male figure as Trump in the original image leads to an inference that the adjacent female could not be Melania. These positive butterfly effects suggest that unlearning is not a blunt tool that erases all traces of a concept but rather can result in a refined restructuring of knowledge within the model.

**Results of Unlearning Multiple Concepts Simultaneously.** Table 3 reports the results of synchronously unlearning all the concepts of MMUBench. We concat all the forgetting training sets of these concepts as fine-tuning data and the training step is set to 120. We find that after unlearning, the utility of MLLMs collapses using GA and GA+KL. All the responses of GA and GA+KL are repeated tokens _'image image...'_. It could be observed that there is some decline in Specificity and Fluency of PO. In contrast, each metric is nearly the same with unlearning a single concept utilizing SIU, which illustrates the robustness of SIU.

**MIA and Jailbreak.** Table 4 displays the results of MIA and Jailbreaks tests. The experimental details of MIA are stated in Appendix D.2. It could be observed that SIU achieves the lowest ROUGE-L score, indicating that the outputs of SIU diverge most from that of \(_{}\). We find PO also performs well under MIA. The reason may be that it tends to output _'I do not know.'_, leading to a low similarity score with the output of \(_{}\).

For Jailbreak, we conduct two types of tests, which are multilingual test and multi-hop question test. The experiments are detailed in Appendix D.3 and Appendix D.4. Combining Table 1 and Table4, we find that the performance of GA+KL and SIU on multilingual are both slightly improved from 2.8 to 2.9 and from 1.9 to 2.3. The case studies are shown in Figures 12 to 14. From the specific examples we find PO always outputs _'I do not know.'_ in different languages. The outputs of SIU are diverse in different languages, illustrating the preservation of utility. For multi-hop question test, as shown in Table 4, it could be observed that SIU performs well in Multi-hop questions, indicating the capability of defending hard examples. The case study of Multi-hop question is displayed in Figure 15. We find that though GA+KL avoids generating the name of \(\), it could still answer the right factual knowledge of the question. This self-contradictory answer illustrates the analysis in Section

    &  &  \\   & & **Multilingual\(\)** & **Multi-hop\(\)** \\  PO & 0.32 & 2.5 & 0.18 \\ GA+KL & 0.44 & 2.9 & 0.38 \\
**SIU** & **0.27** & **2.3** & **0.16** \\   

Table 4: Performance of MIA and Jailbreak with LLAVA\({}_{}\). We do not evaluate GA method because the most of outputs are _whitespace_ or _repeated tokens_.

Figure 4: EM performance comparison of methods SIU, GA+KL, PO, and GA across different concepts.

    &  &  &  &  &  \\   & & **EM\(\)** & & & & & \\  PO  & **100.0** & 80.0 & 2.7 & 0.5 & 12.7 & 59.7 & 96.9 \\ GA  & **100.0** & **100.0** & - & **30.4** & 0 & Inf & 0.67 \\ GA+KL  & **100.0** & **100.0** & - & 15.7 & 0 & 695.2 & 0.67 \\
**SIU** & **100.0** & 97.0 & **1.7** & 5.0 & **24.9** & **54.4** & **99.3** \\   

Table 3: Results of unlearning 20 concepts simultaneously using LLAVA\({}_{}\). Inf denotes an infinite value. We do not test G-Eval for GA and GA+KL because they only generate _repeated tokens_ in all responses.

1.We also observe that SIU could _'make up some lies'_ such as 'having gold courses in St.Andrews'. This phenomenon also confirms the findings of positive butterfly effects.

## 7 Conclusion

We introduce SIU, an efficient method to unlearn the visual recognition of concepts in MLLMs with only one training image. We propose four targets to construct little fine-tuning data. To mitigate the degradation of MLLMs, we introduce Dual Masked KL-divergence Loss to be jointly trained with Cross Entropy Loss. Together with the method we present MMUBench, a benchmark to evaluate machine unlearning in MLLMs. The benchmark is composed of 1000 images, with 50 images for each of the 20 concepts, and a set of evaluation metrics. The experimental results illustrate the effectiveness and robustness of our method. For future work, we would try to extend this work mainly in the following aspects: (i) exploring new machine unlearning methods in MLLMs; (ii) evaluating machine unlearning for data points rather than concept-wise knowledge in MLLMs.