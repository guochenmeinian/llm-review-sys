# Computation-Aware Robust Gaussian Processes

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Gaussian Processes (GPs) are flexible nonparametric statistical models equipped with principled uncertainty quantification for both noise and model uncertainty. However, their cubic inference complexity requires them to be combined with approximation techniques when applied to large datasets. Recent work demonstrated that such approximations introduce an additional source of uncertainty, _computational uncertainty_, and that the latter could be quantified, leading to the _computation-aware_ GP, also known as IterGP. In this short communication, we demonstrate that IterGP is not "robust", in the sense that a quantity of interest, the posterior influence function, is not bounded. Subsequently, drawing inspiration from recent work on Robust Conjugate GPs, we introduce a novel class of GPs: IterRCGPs. We carry out a number of theoretical analyses, demonstrating the robustness of IterRCGPs among other things.

## 1 Introduction

Gaussian Processes (GPs, Rasmussen and Williams (2006)) are a class of probabilistic models enjoying many properties such as universal approximation or closed-form computations. Due to their principled uncertainty quantification, they are becoming increasingly popular when applied in high-stakes domains like medical datasets (Cheng _et al._, 2019; Chen _et al._, 2023) or used as a surrogate model in Bayesian Optimization (Garnett, 2023). This being said, GPs suffer from a cubic inference complexity, hindering their use on large datasets. As a remedy, approximation techniques like Sparse Variational Gaussian Processes (Titsias, 2009) or the Nystrom approximation are often used (Williams and Seeger, 2000; Wild _et al._, 2023).

These approximations introduce bias in uncertainty quantification, which, as recently demonstrated, can be quantified and combined with mathematical uncertainty, leading to the development of _computation-aware_ GPs (Wenger _et al._, 2022), also known as IterGPs. This combined uncertainty is shown to be the correct measure for capturing overall uncertainty, as limited computation introduces computational error. While this analysis applies to standard GPs, many practical applications require variations, e.g., to deal with heteroscedasticity or outliers.

Recent work by Altamirano _et al._ (2024) introduced the robust conjugate GP (RCGP), which unifies three classes of GPs. RCGP retains conjugacy, enabling a closed-form posterior while exhibiting a robustness property. However, like standard GPs, RCGP faces significant inference complexity, necessitating approximation methods such as sparse variational RCGP, and therefore suggesting the use of the framework developed by Wenger _et al._ (2022).

Contributions.Our work can be seen as bridging the gap between computation-aware GPs and Robust Conjugate GPs. As such, our contributions are mainly theoretical and can be summarized as follows:* We present IterRCGP, a novel computation-aware Gaussian Process (GP) framework that extends IterGPs by accommodating a broader range of observation noise models.
* We demonstrate that IterRCGP inherits the robustness properties characteristic of RCGP.
* We establish that IterRCGP exhibits convergence behavior and worst-case errors analogous to IterGP.

## 2 Preliminaries

We first introduce notations for GP regression Rasmussen and Williams (2006). Let \(=\{(_{1},y_{1}),,(_{n},y_{n})\}\) be a dataset, with \((_{j},y_{j})^{d}\) such that \(y_{j}=f(_{j})+\) and \((0,_{}^{2})\) for all \(j\). The latent function \(f\) is modeled with a GP prior:

\[f()(m(),k(,^{})).\] (1)

This defines a distribution over functions \(f\) whose mean is \([f()]=m()\) and covariance \([f(),f(^{})]=k(,^{ })\). \(k\) is a kernel function measuring the similarity between inputs. For any finite-dimensional collection of inputs \(\{_{1},,_{n}\}\), the function values \(=[f(_{1}),,f(_{n})]^{}^{n}\) follow a multivariate normal distribution \((,)\), where \(=[m(_{1}),,m(_{n})]^{}\) and \(^{n n}=[k(_{j},_{l})]_{1 j,l n}\) is the kernel matrix.

Given \(\), the posterior predictive distribution \(p(f())\) is Gaussian for all \(\) with mean \(_{*}()\) and variance \(k_{*}(,)\), such that

\[_{*}() =m()+_{}^{}(+_ {}^{2})^{-1}(-),\] \[k_{*}(,) =k(,)-_{}^{}(+_{}^{2})^{-1}_{},\]

where \(=[y_{1},,y_{n}]^{n}\) and \(_{}=[k(,_{1}),,k(, _{n})]^{}^{n}\).

Next, we introduce an extension of GPs: Robust Conjugate Gaussian Processes (RCGPs).

**Robust conjugate Gaussian process.** RCGP follows the generalized Bayesian inference framework, substituting the classical likelihood with the loss function \(L_{n}^{w}\)Altamirano _et al._ (2024) defined as

\[L_{n}^{w}(,,)=(_{j=1}^{n}w^ {2}(_{j},y_{j})s_{}^{2}(_{j},y_{j})+2_{ y}[w^{2}(_{j},y_{j})s_{}(_{j},y_{j})]),\] (2)

where \(s_{}(,y)=_{}^{-2}(f()-y), _{}^{2}>0\). The core component of \(L_{n}^{w}\) is the weighting function \(w\), which depends on \(\) and \(y\). Altamirano _et al._ (2024)[Table 1] provides three weighting functions corresponding to homoscedastic, heteroscedastic, and outliers-robust GPs. Building on \(L_{n}^{w}\), the authors further define the RCGP's predictive posterior distribution \(p^{w}(f()|)\) as follows:

\[_{*}()=m()+_{}^{} +_{}^{2}_{})^{-1} (-_{})}^{}}\] (3) \[_{*}(,)=k(,)- _{}^{}}^{-1}_{}\] (4)

for \(=(w(_{1},y_{1}),,w(_{n},y_{n}))^{}\), \(}=+_{}^{2}_{}\), \(_{}=+_{}^{2}_{y}( ^{2})\), and \(_{}=(}^{2}}{}^{-2})\). A key advantage of RCGP is its robustness to outliers and non-Gaussian errors. While vanilla GPs exhibit an unbounded posterior influence function, RCGP, under certain conditions, maintains a bounded posterior influence function Altamirano _et al._ (2024)[Proposition 3.2].

## 3 Computation-aware RCGPs

In the same spirit of Wenger _et al._ (2022), we treat the representer weights \(}\) introduced in Equation 3 as a random variable with the prior \(p(})=(};,} ^{-1})\). We then update \(p(})\) by iteratively applying the tractable matrix-vector multiplication. For a particular iteration \(i\{0,,n\}\), we have the current belief distribution \(p(})=(};}_{i},}_{i})\) where

\[}_{i} =}_{i-1}+}_{i-1} }_{i}(_{i}^{}} }_{i-1}}_{i})^{-1}_{i}=}_{i}(-_{})\] (5) \[}_{i} =}_{i-1}-}_{ i-1}}_{i}(_{i}^{}} }_{i-1}}_{i})^{-1}_{i}^{}}}_{i-1}\] (6) \[_{i} =_{i}^{}}(}-}_{i-1})}_{_{i-1}}\] (7) \[}_{i} =}^{-1}-}_{i}\] (8)

Here, \(_{i}\) denotes the policy corresponding to a specific approximation method (Wenger _et al._, 2022)[Table 1]. This policy serves as the projection of the residual \(_{i-1}\) results in \(_{i}\). The belief regarding the representer weights encodes the computational error as an added source of uncertainty, which can be integrated with the inherent uncertainty of the mathematical posterior.

We obtain the predictive posterior of IterRCGP by integrating out the representer weights: \(p(f()|)= p(f()|})p(})d}=(;_{i}( ),_{i}(,))\) where

\[_{i}()=m()+_{}^ {}}_{i}\] (9) \[_{i}(,)=k(,)- _{}^{}}^{-1}_{}+ _{}^{}}_{i} _{}}_{k_{i}^{}(,)}=k( ,)-_{}^{}}_{i}_{}}_{}\] (10)

IterRCGP follows (Algortijn, 1981) from Wenger _et al._ (2022) to compute an estimate weights \(}_{i}\) and the rank-\(i\) precision matrix approximation \(}_{i}\).

## 4 Theoretical results

In this section, we present the theoretical properties of IterRCGP, building upon the IterGP framework and the RCGP class. Our theoretical analysis primarily aims to establish the following key results:

* Robustness property of IterGP and IterRCGP (Proposition 1).
* Convergence of IterRCGP's posterior mean in reproducing kernel Hilbert space (RKHS) norm (Proposition 2) and pointwise (Corollary 4).
* Combined uncertainty of IterRCGP is a tight worst-case bound on the relative distance to all potential latent functions shifted by the function \(_{}\) consistent with computational observations, similar to its IterGP counterpart (Proposition 3).

We establish the robustness properties of IterGP and IterRCGP using the Posterior Influence Function (PIF) as the robustness criterion. Appendix 1 provides a detailed definition of PIF. The proposition presented below is closely related to Altamirano _et al._ (2024)[Proposition 3.2].

**Proposition 1**.: _(Robustness property) Suppose \(f(m,k)\), \((,_{}^{2})\) and let \(C_{k}^{};k=1,2,3\) be constants independent of \(y_{m}^{c}\). For any given iteration \(i\{0,,n\}\), IterGP regression has the PIF_

\[_{}(y_{m}^{c},,i)=C_{1}^{}(y_{m}- y_{m}^{c})^{2}\] (11)

_which is not robust: \(_{}(y_{m}^{c},,i)\) as \(|y_{m}^{c}|\). In contrast, for the IterRCGP with \(_{,y}w(,y)<\),_

\[_{}(y_{m}^{c},,i)=C_{2}^{}(w(x_{ n},y_{n}^{c})^{2}y_{n}^{c})^{2}+C_{3}^{}\] (12)

_Therefore, if \(_{,y}w(,y)^{2}<\), IterRCGP regression is robust since \(_{y_{m}^{c}}|_{}(y_{m}^{c},,i)|<\)._

The proposition demonstrates that IterGP and IterRCGP inherit the same robustness properties as their respective counterparts, GP and RCGP. Specifically, the condition \(_{,y}w(,y)<\) ensures each observation has a finite weight, which is the key factor underpinning robustness.

The following proposition is analogous to (Theorem 1) in Wenger _et al._ (2022).

**Proposition 2**.: _(Convergence in RKHS norm of the robust posterior mean approximation) Let \(_{k}\) be the RKHS w.r.t. kernel \(k\), \(^{2}_{}>0\) and let \(}_{*}-_{k}\) be the unique solution to following minimization problem_

\[_{f_{k}}L_{n}^{w}(, ,)+\|\|_{_{k}}^{2}\] (13)

_which is equivalent to the mathematical RCGP mean posterior shifted by prior mean \(\). Then for \(i\{0,,n\}\) the IterRCGP posterior mean \(}_{i}\) satisfies:_

\[\|}_{*}-}_{i}\|_{_{k}} (i)c(_{})\|}_{*}-\|_{ _{k}}\] (14)

_where \(\) is the relative bound errors corresponding to the number of iterations \(i\) and the constant \(c(_{})=(_{})}{_{}()}} 1\) as \(_{}(_{}) 0\)._

Appendix B provides more details about the relative bound errors. Proposition 2 provides a bound on the RKHS-norm error between the posterior mean of IterRCGP and the mathematical posterior mean of RCGP.

The final proposition parallels [Theorem 2] in Wenger _et al._ (2022), demonstrating that the combined uncertainty is a tight bound for all functions \(g\) that could have yielded the same computational outcomes.

**Proposition 3**.: _(Combined and computational uncertainty as worst-case errors) Let \(^{2}_{} 0\) and \(_{i}(,)=_{*}(,)+k_{i}^{}(,)\) be the combined uncertainty of IterRCGP. Furthermore, let \(=[g(_{1}),,g(_{n})]^{n}\). Then, for any new \(\) we have_

\[_{\|g-m_{w}\|_{_{k}_{w}} 1} )-^{g}()}{}+ ^{g}()-^{g}_{i}()}_{}=_{i}(,)+^{2}_{}}\] (15) \[_{\|g-m_{w}\|_{_{k}_{w}} 1} ^{g}()-^{g}_{i}()}_ {}=^{}(,)}\] (16)

_where \(^{g}()=k(,)}^{-1}(- _{})\) is the RCGP's posterior and \(^{g}_{i}()=k(,)}_{i}(-_{})\) IterRCGP's posterior mean for the latent function \(g\) and the function \(m_{w}\) lies in \(_{k^{_{w}}}\)._

The consequence of Proposition 3 is then formalized through the following corollary:

**Corollary 4**.: _(Pointwise convergence of robust posterior mean) Assume the conditions of Proposition 3 hold and assume the latent function \(g_{k^{_{w}}}\). Let \(}\) be the corresponding mathematical RCGP posterior mean and \(}_{i}\) the IterRCGP posterior mean. It holds that_

\[)-_{i}()|}{\|g\|_{_{k^{_{w}}}}}_{i}(,)+^{2}_{ }}\] (17) \[()-_{i}()}{\|g\|_{ _{k^{_{w}}}}}^{}(, )}\] (18)

## 5 Conclusion

In this paper, we demonstrated that computation-aware GPs as presented by Wenger _et al._ (2022) lack robustness in the PIF sense. Subsequently, we introduced Iter RCGPs, a novel class of provably robust computation-aware GPs. Since our work mainly involves theoretical analyses, our immediate perspective is to run numerical experiments using synthetic and real-world datasets. Next, one interesting avenue for applying Iter RCGPs is that of Bayesian Optimization (BO), a domain where uncertainty quantification is key to coming up with good exploration policies.

Indeed, the issue of refined uncertainty quantification has recently gained attention in BO. One approach addresses this by jointly optimizing the selection of the optimal data point along with the SVGP parameters and the locations of the inducing points (Maus _et al._, 2024). Another study incorporates conformal prediction into BO by leveraging the conformal Bayes posterior and proposing generalized versions of the corresponding BO acquisition functions (Stanton _et al._, 2023).