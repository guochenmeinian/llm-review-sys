# Multi-Reward Best Policy Identification

Alessio Russo

Ericsson AB

Stockholm, Sweden &Filippo Vannella

Ericsson Research

Stockholm, Sweden

###### Abstract

Rewards are a critical aspect of formulating Reinforcement Learning (RL) problems; often, one may be interested in testing multiple reward functions, or the problem may naturally involve multiple rewards. In this study, we investigate the _Multi-Reward Best Policy Identification_ (MR-BPI) problem, where the goal is to determine the best policy for all rewards in a given set \(\) with minimal sample complexity and a prescribed confidence level. We derive a fundamental instance-specific lower bound on the sample complexity required by any Probably Correct (PC) algorithm in this setting. This bound guides the design of an optimal exploration policy attaining minimal sample complexity. However, this lower bound involves solving a hard non-convex optimization problem. We address this challenge by devising a convex approximation, enabling the design of sample-efficient algorithms. We propose MR-NaS, a PC algorithm with competitive performance on hard-exploration tabular environments. Extending this approach to Deep RL (DRL), we also introduce DBMR-BPI, an efficient algorithm for model-free exploration in multi-reward settings.

## 1 Introduction

Reinforcement Learning (RL) provides a powerful framework for sequential decision-making, which has achieved a number of breakthroughs in a wide range of applications, including mastering video games [1; 2], accelerating algorithm design , optimizing fusion control systems [4; 5], and enhancing autonomous robotic control [6; 7; 8].

Rewards are a critical aspect of formulating an RL problem. Yet, designing a reward function can require numerous iterations of reward engineering . Furthermore, in many practical scenarios, the reward signal is not necessarily a scalar, and the agent may seek to optimize performance across a set of reward functions [10; 11] (e.g., goal reaching applications in robotics , recommender systems , intent-based radio network optimization tasks [14; 15], etc.).

In this work, we investigate how to efficiently identify optimal policies across multiple rewards in a given set \(\), a setting we refer to as _Multi-Reward Best Policy Identification_ (MR-BPI). This setting is a variant of traditional Best Policy Identification (BPI) with fixed confidence [16; 17; 18; 19; 20], where the goal is to identify optimal policies using the least number of samples and with a prescribed confidence level. Exploration algorithms for BPI typically aim to find the optimal policy for a single reward function, which is known and fixed in advance. However, as previously mentioned, the agent may face uncertainty about which reward function is the most appropriate for a given task, or may seek to optimize performance across a set of rewards.

This problem remains relatively unexplored in the current RL literature. Closely related settings are reward-free RL [21; 22; 23], unsupervised RL [24; 25; 26; 27; 28; 29], and multi-objective RL [30; 31]. In reward-free RL, the goal of the agent is to estimate the optimal policy for any possiblereward, using methods such as maximum entropy exploration [32; 33]. This reward-free phase is then followed by a downstream task adaptation in unsupervised RL. Lastly, in multi-objective RL, the agent optimizes over multiple (potentially conflicting) rewards, with the goal of identifying a Pareto frontier of optimal policies. While reward-free RL is the closest to our setting, exploration for any possible reward is unlikely to occur in practical problems. On the other hand, unsupervised RL and multi-objective RL do not directly address the multi-reward pure exploration problem -- a setting we deem to be of practical use in a wide range of applications. For instance, in intent-based radio network optimization, rewards are designed to reflect network operator intents, such as maximizing throughput or coverage in a given network area. MR-BPI enables the agent to identify policies that perform well across different reward functions, enhancing robustness and adaptability to varying conditions and goals.

Our contributions are as follows. For MR-BPI with a finite set of user-specified rewards, we establish an asymptotic instance-specific sample complexity lower bound for Markov Decision Processes (MDPs) with a unique optimal policy, which generalizes the lower bound in the single-reward BPI setting . This bound provides insights into the fundamental difficulty of the problem and guides the design of an optimal exploration policy. As in previous works [16; 17; 19; 34], determining such an exploration policy requires to solve a non-convex optimization problem. To address this issue, we propose a convex formulation of the lower bound based on a finite set of rewards. We also propose extensions for two cases: (1) for random exogenous reward signals provided by the environment (app. B.4), and (2) for a set of continuous rewards, which can be addressed by considering a carefully chosen finite set of rewards (app. B.3).

We then devise MR-NaS (Multi-Reward Navigate and Stop), a PC algorithm with asymptotic sample complexity guarantees, adaptable to the aforementioned extensions. We evaluate the performance of MR-NaS on different hard-exploration tabular environments, comparing to RF-UCRL  (a reward-free exploration method), IDSAL  (a maximum entropy exploration approach) and MR-PSRL, a multi-reward adaptation of PSRL . Results demonstrate the efficiency of MR-NaS in identifying optimal policies across various rewards and in generalizing to unseen rewards when the reward set is sufficiently diverse. Lastly, we further extend our multi-reward exploration approach to Deep RL (DRL) by devising DBMR-BPI (Deep Bootstrapped Multi-Reward BPI), a model-free exploration algorithm for continuous-state MDPs. We assess its performance, along with RND  (Random Network Distillation), Disagreement , and APT , on hard-exploration problems from the DeepMind BSuite  and on link adaptation (a radio network control problem ).

## 2 Problem setting

In this section, we introduce the notation used in the manuscript and briefly introduce the Best Policy Identification (BPI) framework. Lastly, we describe the MR-BPI setting.

### Markov Decision Processes and the Set of Rewards

Markov Decision Processes (MDPs).We consider discounted MDPs of the type \(M=(,,P,)\), where \(\) is the finite state space (of size \(S=||\)), \(\) is the finite action space (of size \(A=||\)), \(P:()\) is the transition function, which maps state-action pairs to a distribution over states and \((0,1)\) is the discount factor. We indicate by by \(M_{r}=(,,P,r,)\) the MDP when considering a reward \(r:\). For a given reward \(r\), we define the discounted value of a Markov policy \(:S()\) to be \(V_{r}^{}(s)=^{}[_{t 0}^{t}r(s_{t},a_{t})|s_{0}=s]\), where \(s_{t+1} P(|s_{t},a_{t})\) and \(a_{t}(|s_{t})\). We also let \(V_{r}^{}(s)=_{}V_{r}^{}(s)\) be the optimal value in \(s\) and, for MDPs with a unique optimal policy, we indicate it by \(_{r}^{}(s)=_{}V_{r}^{}(s)\). Similarly, we also define the action-value function \(Q_{r}^{}(s,a)=r(s,a)+_{s^{} P(|s,a)}[V_{r}^{ }(s^{})]\). For an MDP \(M_{r}\), we indicate by \(^{*}(M_{r})=\{^{d}:\|V_{r}^{*}-V_{r}^{}\|_{}=0\}\) the set of optimal policies in \(M_{r}\) and \(^{d}\) is the set of deterministic policies.

Set of rewards.We denote the set of rewards by \(\), and assume that it is known to the agent beforehand. In subsequent sections, for the sake of simplicity, we consider this set to be discrete and finite, while we present results for continuous sets of rewards in app. B.4. For finite MDPs, rewards are represented as vectors within \(^{SA}\), with each coordinate \(i\) corresponding to the reward for the \(i\)-th state-action pair (thus the reward in each state-action pair is bounded in \(\)). We also denote the canonical basis of rewards as \(_{}=\{e_{1},,e_{SA}\}\) in \(^{SA}\) (with \(e_{i}\) being the \(i\)-th vector of the canonical basis).

### Single-Reward Best Policy Identification with Fixed Confidence

Best Policy Identification consists of finding an optimal policy in an MDP using the least number of samples with a given confidence level for a specific reward function [16; 17], and the proof techniques are inspired by the Best Arm Identification literature in the bandit setting .

Optimal exploration strategy.The main idea is to derive an _instance-specific_ lower bound on the expected number of samples needed to identify an optimal policy. Interestingly, the lower bound is often formulated as an optimization problem, whose solution \(^{}\) allows us to derive an optimal exploration algorithm. In other words, the solution \(^{}\) provides the best proportion of static draws. That is, a policy following \(^{}\) matches the lower bound and is therefore optimal [20; 17].

Change of measure.The recipe to derive instance-specific sample complexity lower bounds is based on a _change of measure_ argument. In general, the reasoning adopted is to think in an adversarial way: _Does there exist an MDP similar to the one considered for which the optimal policy is different?_

The analysis of such a problem delves into the required minimum amount of _evidence_ needed to discern between different hypotheses. The evidence is quantified by the log-likelihood ratio of the observations under the true model and a _confusing model_. This confusing model is usually the one that is _statistically_ closest to the true model while admitting a different optimal policy.

This argument allows us to derive an instance-dependent quantity \(T^{}\), also known as the _characteristic time_, that lower bounds the sample complexity of an algorithm. Change of measure arguments have a long history [38; 39; 40; 41], and have been applied to derive lower bounds for regret minimization [42; 43; 44], best-arm identification [20; 45; 46; 47] and change detection problems [48; 49; 50].

### Multi-Reward Best Policy Identification with Fixed Confidence

The MR-BPI setting follows closely the single-reward BPI framework . In MR-BPI, the goal is to learn the best policy for all possible rewards \(r\) as quickly as possible. This objective can be formalized in the \(\)-Probably-Correct (\(\)-PC) framework [51; 22], where the learner interacts with the MDP until she can output an optimal policy with confidence \(1-\). Contrarily to the classical BPI framework, that solely considers a single reward, here we are interested in algorithms that identify a best policy for all rewards in \(\).

Algorithm.Formally, an algorithm consists of (i) an exploration strategy (a.k.a. sampling rule), (ii) a stopping rule \(\) and (iii) an estimated optimal policy \(_{,r}\), for any reward \(r\). At each time step \(t\), the algorithm observes a state \(s_{t}\), it selects an action \(a_{t}\), and then observes the next state \(s_{t+1} P(|s_{t},a_{t})\). We denote by \(\) (resp. \(\)) the probability law (resp. expectation) of the process \((Z_{t})_{t}\), where \(Z_{t}=(s_{t},a_{t})\). We indicate by \(_{t}=(\{Z_{0},,Z_{t}\})\) the \(\)-algebra generated by the random observations made under the algorithm up to time \(t\) in the _exploration_ phase. For such algorithm, we define \(\) to be a stopping rule w.r.t. the filtration \((_{t})_{t 1}\). This stopping rule \(\) simply states when to stop the exploration phase. At the stopping time \(\) the algorithm outputs an estimate \(_{,r}\) of the best policy for any given reward \(r\) using the collected data. We focus on \(\)-PC algorithms, according to the following definition, and we impose a few mild assumptions.

**Definition 2.1** (\(\)-PC Algorithm).: We say that an algorithm is \(\)-PC if, for any MDP \(M\), it outputs (in finite time) an optimal policy for any reward \(r\) with probability at-least \(1-\), i.e., \([<,( r,_{,r}^{ }(M_{r}))] 1-\).

**Assumption 2.1**.: (I) \(M\), where \(\) is the set of communicating MDPs  with a unique optimal policy for every \(r\) ; (II) \(M\) is aperiodic under a policy that assigns a positive probability to all actions in every state; (III) for all \(r\) there exists a model \(P^{}\) such that \(P(|s,a) P^{}(|s,a)\) for all \((s,a)\), and \(^{}(M_{r})^{}(M_{r}^{})=\), where \(M_{r}^{}=(,,P^{},r,)\) and for any two measures \((,)\), \(\) denotes absolute continuity of \(\) w.r.t. \(\).

Assumptions (I)-(II) are standard in the BPI literature [51; 53]. Assumption (III) is made to avoid degenerate cases in which there are no confusing model for rewards \(r\). For example, this occurs when the reward is identical in all state-action pairs (therefore all actions are optimal).

## 3 Best Policy Identification with a Set of Rewards

In this section, we investigate the MR-BPI problem, _i.e._, how to optimally explore an MDP to derive the best policy for any reward in \(\). In the first subsection, we derive a sample complexity lower bound for any \(\)-PC algorithm. Later, in the second subsection, we present MR-NaS, a \(\)-PC algorithm inspired by NaS with asymptotic optimality guarantees.

### Sample Complexity Lower Bound

In the next theorem, we state an instance-specific lower bound on the sample complexity of any \(\)-PC algorithm. This bound provides a fundamental limit on the performance of any \(\)-PC algorithm and guides the design of optimal exploration methods for MR-BPI.

To state the bound, we indicate by \(^{SA}\) the _allocation_ (or exploration policy), and by \((s,a)\) the corresponding frequency of visit of a state-action pair \((s,a)\); we introduce the set of _confusing_ models \(_{r}(M)\) for \(M_{r}\) as the set of MDPs that are statistically similar to the original model \(M_{r}\), but admit a different set of optimal policies under \(r\). Specifically, we let \(_{r}(M)\{M^{}:M M^{},^{}(M_{r}) ^{}(M_{r}^{})=\}\), where \(M M^{}\) if for all \((s,a)\) we have \(P(|s,a) P^{}(|s,a)\).

**Theorem 3.1**.: _Let \((0,1/2)\). For any \(\)-PC algorithm we have \(_{ 0}[]}{(1/)} T^{}(M),\) where_

\[T^{}(M)^{-1}=_{(M)}_{r}_{M^{ }_{r}(M)}_{s,a}(s,a)_{M|M^{}} (s,a),\] (1)

_with \(_{M|M^{}}(s,a)(P(|s,a),P^{}( |s,a))\) and \((M)\{():_{a} (s,a)=_{s^{},a^{}}P(s|s^{},a^{})(s^{ },a^{}), s\}\)._

Discussion.The proof of the theorem is presented in app. B.1. The result, obtained by classical change-of-measure arguments , extends the corresponding lower bound in the case of a single reward .

The quantity \(T^{}(M)\) is referred to as the _characteristic rate_ and is a non-convex optimization problem. It can be interpreted as a zero-sum game between an agent choosing a distribution (or allocation) \(\), and an opponent choosing a confusing model \(M^{}\) (in the multi-reward setting, the opponent needs also to consider the set of reward when choosing the confusing model, which makes the problem harder to solve). The distribution \(\) is a stationary distribution over states and actions that takes into account the transition dynamics, and, interestingly, the optimal solution \(_{}\) to the optimization problem in eq.1 also defines the optimal exploration strategy (as \(_{}(a|s)=_{}(s,a)/_{b}_{}(s,b)\)).

In principle, with access to an optimization oracle that solves (1), we could apply classical Track-and-Stop (TaS)  algorithms and achieve asymptotically optimal sample complexity by tracking the optimal allocation. Unfortunately, even in classical BPI, computing the characteristic rate \(T^{}(M)\) is in general a non-convex problem . Instead, as in previous works [16; 17; 53], we focus on deriving a convex upper bound \(U^{}(M)\) of \(T^{}(M)\), which we call _relaxed characteristic rate_, that guarantees that we are still identifying the optimal policy at the cost of an increased sample complexity. We focus on the case of a finite set of user-specified rewards. The extension to externally provided reward signals and the consideration of a continuous set of rewards are detailed in the appendix (app. B.3 and app. B.4, respectively).

### Relaxed Characteristic Rate

To define the relaxed characteristic rate, we state a few definitions of problem-dependent quantities for MDPs. We let \(_{r}(s,a) V_{r}^{}(s)-Q_{r}^{}(s,a)\) be the sub-optimality gap in \((s,a)\), \(_{r}(s,a)_{s^{} P(|s,a)}[(V_{r}^{ }(s^{})-_{ P(|s,a)}[V_{r}^{}()]]\) be the variance of the optimal value function in the next state, and \(_{r}(s,a)\|V_{r}^{}-_{s^{} P( |s,a)}[V_{r}^{}(s^{})]\|_{}\) be the maximum deviation starting from \((s,a)\). Additionally, for a given reward \(r\) we also let \(_{r}_{s,a^{}_{r}(s)}_{r}(s,a)\), \(_{r}_{s,a(s,M_{r})}_{r}(s,a)\) and \(_{r}=_{s,a(s,M_{r})}_{r}(s,a)\) be the minimum gap, maximum variance and deviation across sub-optimal state-action pairs, respectively.

We define the relaxed rate in terms of an upper bound on \(T(;M)\) that holds for all possible allocations \(()\), where

\[T(;M)^{-1}_{r}_{M^{}(M _{r})}_{s,a}(s,a)_{M|M^{}}(s,a).\] (2)

**Theorem 3.2**.: _For any allocation \(()\) we have that \(T(;M) U(;M)\), where_

\[U(;M)_{r}_{s,a_{r}^{*}(s)}_{r}(s,a)^{2}}{_{r}(s,a)^{2}(s,a)}+_{s^{ }}}{_{r}^{2}(s^{},_{r}^{*}(s^{}))},\] (3)

_where \(H_{r}=\{}{(1-)^{3}},(_{r}(1+)^{2}}{(1-)^{2}}, _{r}^{4/3}(1+)^{4/3}}{(1-)^{4/3}})\}\). We define the optimal value \(U^{}(M)=U(^{};M)\), where \(^{}=\;U(;M)\) is the optimal allocation w.r.t. \(U^{}(M)\)._

Discussion.In the above theorem (the proof can be in found in app. B.2), the various instance-specific quantities \(_{r}(s,a),_{r}(s,a),H_{r}\) and \(_{r}\) are computed with respect to \(M\). The result extends the corresponding upper bound in [17; 53].

We make some observations to clarify the key aspects of this result. First, \(U(;M)\) is convex in \(\) for a finite set of rewards. Hence \(U(;M)\) can be computed efficiently by using convex optimization methods (we report the runtime of our simulation in app. E.1).

For a continuous set of rewards there are some challenges to guarantee convexity, due to the fact that the minimum gap can be non-convex and discontinuous. However, we find that, _in practice_, it is sufficient to optimize over a set of rewards whose _convex hull_ includes the original set of rewards (e.g., by using the canonical basis of rewards; refer also to app. B.4 for more details).

Second, using the allocation \(^{}\) guarantees that we identify the optimal policy, at the cost of an increased over-exploration , since \(U^{}(M) T^{}(M)\). The cost of this over-exploration is limited, since by plugging \((s,a)=1/(SA)\) one can see that \(U^{}(M)=O(_{r}_{r}^ {2}})\); note that the dependency in \((1-)^{-1}\) can be improved whenever the maximum deviation is smaller than \((1-)^{-1}\). Lastly, observe that the dependency \(_{r}^{2}(s^{},_{r}^{}(s^{}))\) in the second term in eq.3 can be improved to \(_{r}(s,a)^{2}(s^{},_{r}^{}(s^{}))\) as in .

### MR-NaS Algorithm

In this section we devise MR-NaS, an adaptation of MDP-NaS to MR-BPI for a finite set of rewards. The algorithm consists of (1) an exploration policy (a.k.a. sampling rule) and (2) a stopping rule. We detail these components in the remainder of this section (and in app. C) and present the pseudo-code of MR-NaS in Alg. 1.

Sampling rule.The main idea, as in [16; 17; 53; 20], is that the exploratory policy should follow the allocation \(^{}=_{(M)}U(;M)\), as this will yield a sample-efficient exploration policy. However, as the model \(M\) is unknown, we cannot directly compute \(^{}\). Alternatively, we employ the _certainty-equivalence_ principle, and use the current estimate of the model \(M_{t}\) in place of the true model \(M\) to compute the allocation \(_{t}^{}=_{(M_{t})}U(;M_{t})\), where

\[U(;M_{t})=_{r}_{s,a_{t,r}^{c}(s)} _{t,r}(s,a)^{2}}{_{t,r}(s,a)^{2}(s,a )}+}{_{t,r}^{2}_{s^{},a^{}_{t,r}(s)}(s^{},a^{})}.\] (4)

In the above expression, for any \(t 1\), the estimate \(M_{t}\) is completely defined by the empirical transition function \(P_{t}\):

\[P_{t}(s^{}|s,a)=(s,a,s^{})}{N_{t}(s,a)}N_{t}(s,a)>0,P_{t}(s^{}|s,a)=\] (5)

where \(N_{t}(s,a)\) is the number of visits for each state-action pair up to time \(t\) (sim. \(N_{t}(s,a,s^{})\)). Letting \(V_{t,r}^{}\) denote the optimal value in \(M_{t,r}(M_{t},r)\) (sim. \(Q_{t,r}^{}\)) then \(_{t,r}(s)=\{a:Q_{t,r}^{}(s,a)=V_{t,r}^{}(s)\}\) is the set of optimal actions for reward \(r\) in state \(s\) (and \(_{t,r}^{c}(s)\) is the complement), \(_{t,r}(s,a)=\|V_{t,r}^{}(s)-_{s^{} P_{t}( |s,a)}[V_{t,r}^{}(s^{})]\|_{}\) is the maximum deviation starting from \((s,a)\)for \(M_{t,r}\), \(_{t,r}(s,a)=_{s^{} P_{t}(|s,a)}[(V_{t,r}^{ }(s^{})-_{ P_{t}(|s,a)}[V_{t,r}^{}( )])^{2}]\) is the corresponding next-state variance of the optimal value for \(M_{t,r}\), and \(_{t,r}(s,a)=V_{t,r}^{}(s)-Q_{t,r}^{}(s,a)\). These quantities can also be used similarly to compute \(H_{t,r}\) (i.e., \(H_{r}\) computed w.r.t. \(M_{t,r}\)).

At the same time, the sampling strategy must ensure that the estimated model \(M_{t}\) converges to \(M\). To that aim, we mix the computed allocation with a _forcing policy_\(_{f}\), ensuring that each state-action pair is sampled infinitely. We use the forcing policy \(_{f,t}(|s)=(-_{t}(s)N_{t}(s,))\) with \(_{t}(s)=(s))}{_{}|N_{t}(s,a)-_{k}N_{t}( s,b)|},\) and \(((x))_{i}=e^{x_{i}}/_{j}e^{x_{j}}\) for a vector \(x\). This choice encourages to select under-sampled actions for \(>0\), while for \(=0\) we obtain a uniform forcing policy \(_{f,t}(a|s)=1/A\). In , the authors use an exploration factor \(_{t}=t^{-1/(m+1)}\), where \(m\) is an problem-specific constant measuring to the "connectedness" of the MDP. As estimating this constant is typically hard for most MDPs, we instead use \(_{t}=1/(1,N_{t}(s_{t}))^{}\), with \(N_{t}(s)=_{a}N_{t}(s,a)\) and \((0,1]\), which only depends on the number of states visits. This change requires a modification of the proofs in , since now \(_{t}\) depends also on the number of visits of \(s_{t}\). Finally, to ensure convergence of \(_{t}^{}^{}\), we simply require that \(+ 1\).

Stopping rule.The stopping rule (line 3) is defined through the relaxed generalized likelihood ratio term \(tU(N_{t}/t;M_{t})\) and a threshold function \((N_{t},)\). The term \(N_{t}/t\) simply refers to the rates of visit of each state-action pair, while \(M_{t}\) is the current estimate of the MDP (based on (5)). The threshold function is selected to guarantee that the algorithm is \(\)-PC (the proof is in app. C.4).

**Proposition 3.1**.: _Define the threshold \((N_{t},)=(1/)+(S-1)_{s,a}(e[1+(s,a)}{S-1}])\). Then, the stopping rule \(:=\{t 1:tU(N_{t}/t;M_{t})^{-1}(N_{t},)\}\) is \(\)-PC, i.e.,_

\[[<,( r:_{,r}^{ }(M_{r}))].\] (6)

Putting everything together.Following our discussion, MR-NaS uses the estimate \(M_{t}\) to compute the allocation \(_{t}^{}=_{(M_{t})}U(;M_{t})\) (line 4). To stabilize the exploration, we use an averaged allocation to obtain \(_{t}^{}=(1/t)_{n=1}^{t}_{n}^{}\) (similarly to the C-navigation rule in [20; 17]). Lastly, we mix \(_{t}^{}\) with a forcing policy \(_{f,t}\) (line 5). The samples observed from the MDP are then used to update the number of state action visits and the estimate of \(M_{t}\). Finally, the stopping rule (line 3) determines when the algorithms has reached the desired confidence level.

Sample complexity guarantees.Lastly, we establish asymptotic optimality guarantees on the sample complexity of MR-NaS (see proofs in app. C.2.3 and app. C.3.5, respectively).

**Theorem 3.3**.: _MR-NaS with the stopping rule in prop. 3.1 satisfies: (i) it stops almost surely and \((_{ 0} U^{}(M) )=1\); (ii) \([]<\) and \(_{ 0}[]}{(1/)} U^{}(M)\)._

### Numerical Results on Tabular MDPs

In this section we show results on different hard-exploration tabular environments: Riverswim , Forked Riverswim , DoubleChain  and NArms  (an adaptation of SixArms to \(N\) arms). We compare MR-NaS against RF-UCRL  (a reward-free exploration method), ID3AL  (a maximum entropy exploration approach) and MR-PSRL, a multi-reward adaptation of PSRL  (posterior sampling for RL) that is outlined in app. D.1.2. A full description of the environmentscan be found in app. D.1. As the baselines are not specific to our setting, we consider different sets of experiments. In this section, we show results for MR-NaS on the more challenging setting with continuous sets of rewards, while in app. D.1, we also investigate the problem of identifying best policies with a finite set of rewards.

More precisely, we consider the problem of correctly estimating the optimal policies in a convex set of rewards \(^{SA}\) as well as the optimal policies of the canonical basis, with discount factor \(=0.9\). We consider \(=_{}_{}\), where \(_{}=\{R_{1},,R_{30}\}\) consists of \(30\) reward vectors uniformly sampled from \(^{SA}\), different for each seed. For MR-NaS, we use the insight from the analysis of a continuous set of rewards (see app. B.4.2) which shows that exploring according to the canonical basis \(_{}\) is sufficient to identify the optimal policies in a continuous set.

We run each algorithm for \(T=50 10^{3}\) steps, and we report the fraction of misidentified policies \(e_{t,r}\) at time step \(t\) and for reward \(r\). Fig. 1 shows the results for this setting over \(50\) seeds. We see how depending on the environment, certain algorithms may be more efficient. Nonetheless, MR-NaS is able to reach low estimation error relatively quickly on all of the environments. We refer the reader to app. D.1 for further details.

## 4 Exploration in Deep Reinforcement Learning with a Set of Rewards

Extending MR-NaS to Deep-RL is not straightforward. Firstly MR-NaS is a model-based method, secondly there is no closed-form solution to \(*{arg\,inf}_{}U^{}(;M)\). To circumvent these issues, we adapt the DBMF-BPI algorithm from , where the authors propose DBMF-BPI, a deep-RL exploration approach for single-reward BPI.

The DBMF-BPI algorithm.DBMF-BPI  is an algorithm for model-free exploration. In  they suggest that a model-free solution can be achieved by (i) using the _generative solution_\(*{arg\,inf}_{(S A)}U^{}(;M)\), which can be computed in closed form (see app. B.2.4), and (ii) by estimating the parametric uncertainty of the MDP-specific quantities (sub-optimality gaps, etc.). In other words, the generative solution may be close enough to the model-based one. Nonetheless, this solution may be biased because the true sub-optimality gaps are unknown, and therefore we need account for the parametric uncertainty in the estimates.

DBMF-BPI estimates the parametric uncertainty using _an ensemble of \(Q\)-networks_. This ensemble of size \(B\), when trained on different data, can be used to mimic the bootstrap procedure : the samples \((x_{b})_{b[B]}\) from this ensemble are used to build an empirical CDF \((X x)=_{b=1}^{B}_{\{X x_{b}\}}\) around some quantity of interest \(X\) (in our case the quantity of interest is the \(Q\)-function). A randomized prior value mechanism  addresses the restricted support of \(\) by adjusting the sampling of \((x_{b})_{b}\). This empirical CDF is then used to sample the \(Q\)-values at each time step, which are then used to compute the generative solution \(_{t}\).

Figure 1: Average estimation error over \(t=1,,T\) steps (note that \(r\) is a random variable). Shaded area indicates \(95\%\) confidence intervals over \(100\) seeds.

Multi-reward adaptation.We propose DBMR-BPI, an extension of DBMF-BPI to the multi-reward setting with a finite set of rewards. DBMR-BPI at each time step observes a vector of rewards, and drives its exploration according to these observed values. More precisely, the extension mainly consists of the following modifications:

1. The \(Q\)-values are learned for all rewards. In the finite action setting, this implies that there are \(A||\) values to estimate in every state. Alternatively, one can choose to feed the networks with a one-hot encoding of the chosen reward (or use embeddings), but we experienced this change to be less effective.
2. We explore according to the difficulty of the reward function. Specifically, we explore a reward \(r\) with probability \( 1/_{t,r}^{2}\), as this term is typically the leading factor in the sample complexity. For the sake of stability, at the beginning of each episode (or every \( 1/(1-)\) steps) we sample a reward \(r\) to explore, and apply DBMF-BPI on this reward.

The main steps of the algorithm are outlined in Alg. 2, and all the details can be found in app. E.1.

### Numerical Results

We evaluate the performance of DBMR-BPI and compare it with state-of-the-art method in unsupervised RL  (note that this is the first algorithm for multi-reward exploration in Deep-RL). More precisely, we compare our algorithm to RND (Random Network Distillation), Disagreement and APT. The exploration procedure of these methods is guided by an intrinsic reward function that is learnt during exploration (e.g., by quantifying the parametric uncertainty of the model, or some prediction uncertainty; these methods are detailed in app. E.2). As in [57; 53], we evaluate on different challenging environments from the DeepMind behavior suite : the Cartpole swing-up problem and a stochastic version of DeepSea [36; 53] with varying level of difficulties. We also evaluate the methods on the link adaptation problem, a radio network control task where the agent needs to choose a coding and modulation scheme in wireless networks , and report the results in app. D.3.1. Lastly, we also evaluate the exploration quality of DBMR-BPI both on a known set of pre-defined rewards and on unseen rewards.

The Cartpole swing-up .In the classical version of this environment, the goal is to balance a pole initially pointing downward, and the agent incurs a cost \(-c\) whenever the cart moves. In the multi-reward variant of Cartpole, the goal is to stabilize the pole vertically around a specific position \(x_{0}\). The agent earns a positive reward only if the pole's angle, its angular velocity, and the cart's position, respectively, satisfy the conditions (1) \(()>k/20\), (2) \(||_{0}\), and (3) \(|x-x_{0}| 1-k/20\), where \(_{0}\) is a fixed constant and \(k\) is an integer representing the difficulty. We considered \(5\) values of \(x_{0}\) linearly spaced in \([-1.5,1.5]\), so that \(||=5\). To assess DBMR-BPI's capacity to generalize on unseen rewards, we uniformly sample \(5\) additional values of \(x_{0}\) in the same interval that are not used during training, and we denote them by \(_{}\).

In tab. 1 we present results for two levels of difficulty \(k\{3,5\}\) and different training steps \(T\). The table presents statistics for the random variable \(X_{i}=_{t=1}^{T}_{r_{i}(s_{t},a_{t})>0}\), where \(i\) is the \(i\)-th reward in the set. This metric measures the average amount of information collected by the agent useful to learn how to stabilize the pole upright. \(_{}(\{X_{i}\})\) (sim. the others metrics) indicates the arithmetic mean of \(\{X_{i}\}_{i}\) over the rewards. We average results across \(30\) seeds.

In general, we see that DBMR-BPI outperforms all of the unsupervised learning baselines. Interestingly, even though DBMR-BPI uses the base set of rewards \(\) to explore, we see better performance when collecting rewards from \(_{}\) (which are not used by DBMR-BPI during exploration). The reason may be that some rewards in \(\), (e.g., those with \(|x_{0}|=1.5\)), are harder to learn, while it is unlikely for the random rewards to have such values of \(x_{0}\). This result seems to suggest that data collected by DBMR-BPI could also be used to optimize similar, but unseen, rewards. We refer the reader to app. D.2.1 for more results and details on the environment.

The DeepSea .In the DeepSea environment the agent moves in a riverswim-like environment, and the actions are randomized for each different seed. The agent starts in the top-left corner of a grid of size \(N N\), and can only move diagonally, towards the bottom of the grid. In the classical setup, the agent observes a \(N^{2}\)-dimensional one-hot encoding of its position, and the goal is to reach the bottom-right corner. We adapt it to a multi-reward setting by considering \(N\) different rewards \(=\{r_{1},,r_{N}\}\), so that \(r_{i}\) assigns a positive reward whenever the agent reaches column \(i\) in the last row of the grid. As in , we include a small probability of the agent taking the wrong action.

To assess the capacity of DBMR-BPI to collect unseen rewards, for each seed; we sampled \(20\) reward functions \(_{}=(r_{i}^{})_{i=1}^{20}\), each of dimension \(N(N+1)/2\) (where the \(i\)-th value corresponds to a possible position in the grid), from a Dirichlet distribution with parameter \(=(_{i})_{i}\), with \(_{i}=1/(10N)\) for all \(i=1,,N(N+1)/2\), and computed the reward for these additional reward functions. The reader can find more details regarding the environment in app. D.2.2.

In tab. 2 we show some statistics for the random variable \(X_{i}=_{t=1}^{T}r_{i}(s_{t},a_{t})\), that quantifies the average reward collected for the \(i\)-th reward in a specific instance. In the table \(_{}(\{X_{i}\})\) indicates the geometric mean over the rewards (the arithmetic mean would be a constant for these base rewards in this setting, and the GM is more appropriate ), and \(_{i=1}^{N}_{X_{i}=0}\) indicates the number of cells in the last row of the grid that have not yet been visited at time step \(T\).

When considering the set of rewards \(\) used by DBMR-BPI, we see that APT and Disagreement do not match the performance of DBMR-BPI and RND, focusing most of the time on a few rewards. On the other hand, while DBMR-BPI and RND show similar performance for \(N=20\), the difference increases for \(N=30\) on the base set of rewards. When considering the total reward collected from the set \(_{}\), we see that DBMR-BPI achieves performance similar to RND, which confirms the capability of DBMR-BPI to collect reward from unseen reward functions (refer to app. D.2.2 for further details).

    & & \)**} & _{}\)**} \\  & & \([_{}(\{X_{i}\})]\) & \([_{}(\{X_{i}\})]\) & \([_{i=1}^{N}_{X_{i}=0}]\) & \([_{}(\{X_{i}\})]\) & \([_{}(\{X_{i}\})]\) \\  \(N=20\) & DBMR-BPI & \(\) & \(\) & \(\) & \(\) & \(\) \\ \(T=50000\) & RND & \(\) & \(\) & \(\) & \(\) & \(\) \\ APT & \(\) & \(5.3(4.8,5.8)\) & \(9.0(9.6,1.3)\) & \(\) & \(6.6(9.5,9.7,4)\) \\ Disagreement & \(0.0(0.0,0.0)\) & \(6.2(5.9,6.5)\) & \(4.0(3.4,4.5)\) & \(8.9(8.8,9.5)\) & \(6.4(5.8,7.1)\) \\  \(N=30\) & DBMR-BPI & \(\) & \(\) & \(\) & \(\) & \(\) \\ \(T=100000\) & RND & \(1.8(1.3,2.2)\) & \(1.9(1.9,2.0)\) & \(0.3(0.2,0.5)\) & \(\) & \(\) \\ APT & \(0.0(0.0,0.0)\) & \(3.9(3.6,4.2)\) & \(3.3(2.8,3.8)\) & \(\) & \(4.3(3.8,4.7)\) \\ Disagreement & \(0.0(0.0,0.0)\) & \(7.0(6.6,7.3)\) & \(11.5(0.18,12.2)\) & \(\) & \(6.1(5.6,6.6)\) \\   

Table 2: **DeepSea. Statistics for the random variable \(X_{i}=_{t=1}^{T}r_{i}(s_{t},a_{t})\) (with \(r_{i}\) or \(r_{i}_{}\)). Values are multiplied by \(100\) (except the third metric for the base rewards) and rounded to the first digit. In bold statistically significant results (in parentheses we indicate the \(95\%\) confidence interval over \(30\) seeds).**Discussion and Conclusions

Exploration in RL and Best Policy Identification.There exists a variety of exploration methods in RL  and DRL (see  and references therein). Our setting is a generalization of classical single-reward BPI with fixed confidence . This line of work typically leverages instance-specific lower bounds on the sample complexity (or approximations of it) to devise efficient exploration strategies, but do not directly address the MR-BPI problem. While we primarily draw inspiration from , we enhance their work by introducing a multi-reward generalization applicable to both the tabular case and DRL, along with some technical improvements (see app. C and app. E.1). Lastly, while not directly related, in  the authors study the identification of the Pareto optimal frontier in a multi-objective bandit model, a framework yet to be explored in MDPs.

Multi-Reward.There are several works considering a setting where the agent deals with multiple diverse reward signals . Shelton  investigates the case where an agent observes a vector of rewards from multiple sources at each time step. The author proposes an algorithm attaining a Nash equilibrium on the average return of each source. In  they investigate a multi-reward setting with continuous states and devise an algorithm yielding optimal policies for any convex combination of reward functions. Arguably, the closest work to ours is Dann et al. , where they investigate multi-reward tabular MDPs, aiming at optimizing a regret measure that combines the various rewards components through an operator and provide regret bounds for action elimination algorithms. Despite the similarities, these works focus on the classical regret minimization problem and do not address the pure exploration case.

A related framework is multi-objective RL , a technique with various applications in areas such as molecular design  and pricing strategy optimization . In this framework, the agent seeks to optimize multiple rewards simultaneously. To the best of our knowledge there are no work investigating the exploration problem in Multi-Objective RL, a setting that could potentially benefit from more efficient exploration strategies, such as the ones that we propose.

Reward-Free Exploration.A relevant body of literature considers Reward-Free Exploration (RFE) , where the agent learns behaviours without a specific reward. For example, Jin et al.  study reward-free RL in the tabular episodic case to identify \(\)-optimal policies for any possible reward. They provide a lower bound on the length of exploration phase scaling as \((H^{2}S^{2}A/^{2})\) and devise an algorithm nearly matching it of order \((H^{8}S^{2}A/^{2})\). This bound was improved in  and , where the authors propose the RF-UCRL and RF-Express algorithms for episodic reward-free RL. On a similar line, the authors of  focus on an RFE setting where the minimal sub-optimality gap is a known quantity, and provide an algorithm with reduced sample complexity. Another relevant RFE framework is maximum entropy exploration , which studies the problem of state entropy maximization. This type of exploration is particularly useful for offline RL  since it is well know that the coverage of the state space characterizes the sample complexity of the problem . Lastly, the reward-free exploration phase can be followed by a downstream task adaptation, as in unsupervised RL . While these settings are highly relevant and provide valuable insights for other RL domains (such as facilitating representation learning in a self-supervised manner ), some RFE techniques can be impractical in realistic scenarios (e.g., exploring for any reward) or do not address the MR-BPI problem directly. Furthermore, unlike our setup, most of the works in RFE focus on the minimax setting only.

Conclusions.In this work, we investigated the MR-BPI problem, focusing on efficiently determining the best policies for a diverse set of rewards \(\) with a specified confidence level. We established a fundamental sample complexity result for MR-BPI and proposed two efficient methods for multi-reward exploration: MR-NaS for tabular MDPs and DBMR-BPI for DRL. Numerical results on challenging exploration environments suggests that both algorithms exhibit strong exploration capabilities and generalization properties. We believe that our study benefits various areas of RL by enabling more principled exploration in multi-reward scenarios. This includes applications in multi-objective RL, which naturally lends itself to optimization over multiple objectives, and offline RL, where it is crucial to collect a dataset that performs well across different rewards.