# Patch n' Pack: NaViT, a Vision Transformer

for any Aspect Ratio and Resolution

 Mostafa Dehghani, Basil Mustafa1, Josip Djolonga2, Jonathan Heek2,

Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver,

Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski,

Alexey A. Gritsenko, Mario Lucic, Neil Houlsby

Google DeepMind

{dehghani,basilm}@google.com

 Project lead. 2Core contributor.

###### Abstract

The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.

## 1 Introduction

The simple, flexible and scalable nature of the Vision Transformer (ViT)  has rendered it an almost ubiquitous replacement to convolution based neural networks. Underpinning this model is a simple operation: splitting an image into patches, each of which is linearly projected to a token. Typically, input images are resized to a fixed square aspect ratio and then split into a fixed number of patches.

Recent works have explored alternatives to this paradigm: FlexiViT  supports multiple patch sizes within one architecture, enabling smooth variation of sequence length and thus compute cost. This is achieved via random sampling of a patch size at each training step and a resizing algorithm to allow the initial convolutional embedding to support multiple patch sizes. Pix2Struct  introduced an alternative patching approach which preserves the aspect ratio, which is particularly useful for tasks such as chart and document understanding.

We present an alternative, NaViT. Multiple patches from different images are packed in a single sequence--termed Patch n' Pack--which enables variable resolution while preserving the aspectratio (Figure 2). This is inspired by example packing in natural language processing, where multiple examples are packed into a single sequence to accommodate efficient training on variable length inputs.

We demonstrate that: (i) Randomly sampling resolutions at training time significantly reduces training cost. (ii) NaViT results in high performance across a wide range of resolutions, enabling smooth cost-performance trade-off at inference time, and can be adapted with less cost to new tasks, (iii) Fixed batch shapes enabled by example packing lead to new research ideas, such as aspect-ratio preserving resolution-sampling, variable token dropping rates, and adaptive computation.

These observations have major practical implications. At a fixed computational budget NaViT consistently outperforms ViT. For instance, we match the performance of the top-performing ViT with 4\(\) less compute (Figure 1, left). We identify the substantial increase in the number of training examples processed within the allocated compute budget as the primary contributor to the improved performance over ViT (Appendix B.5) --example packing coupled with variable resolution inputs and variable token dropping enable NaViT-L/16 to process five times more images during training (Table 2). This improved efficiency extends to the fine-tuning process (Figure 1, middle). Furthermore, by exposing NaViT to multiple resolutions during both pre-training and fine-tuning, a single model demonstrates excellent performance when evaluated on various resolutions, significantly advantaging NaViT in terms of inference cost (Figure 1, right).

NaViT's training and adaptation efficiency, and flexible inference, presents a promising avenue for Vision Transformers. Patch n' Pack empowers computer vision systems to transcend limitations imposed by current data and modeling pipelines, enabling ideas that were previously restricted by the constraints of fixed batch shapes, unlocking new possibilities for innovation and advancement.

Figure 1: NaViT offers notable computational efficiency during pre-training (left) which carries over to downstream fine-tuning (middle). A single NaViT can be applied successfully to multiple resolutions (right), smoothly trading off performance and inference cost.

## 2 Method

Deep neural networks are typically trained and run with batches of inputs. For efficient processing on the current hardware this implies fixed batch shapes which in turn imply fixed image sizes for computer vision applications. This coupled with architectural limitations historically associated with convolutional neural networks led to a practice of either resizing or padding images to a fixed size. Both of these have been shown to be flawed: the former harms performance and the latter is inefficient . An analysis of aspect ratios in ImageNet , LVIS  and WebLI  as representative examples of classification, detection and web image datasets, respectively, shows that most images are typivally not square (Figure 3).

In language modelling, it is common to bypass limitations of fixed sequence lengths via _example packing_: tokens from multiple distinct examples are combined in one sequence, which can significantly accelerate training of language models . By treating images as sequences of patches (tokens), we show that Vision Transformers  can benefit from the same paradigm, which we call Patch n' Pack. Using this technique ViTs can be trained on images at their "native" resolution, and we name this approach NaViT.

### Architectural changes

NaViT is built upon the original ViT, but in principle can use any ViT variant operating on a sequence of patches. To enable Patch n' Pack, we make the following architectural modifications.

**Masked self attention and masked pooling.** To prevent examples attending to each other, additional self-attention masks are introduced. Similarly, masked pooling on top of encoder aims to pool the token representations within each example, resulting in a single vector representation per example in the sequence. Figure 2 presents how the receptive filed of attention is controlled via masking.

**Factorized & fractional positional embeddings.** To handle arbitrary resolutions and aspect ratios, we revisit the position embeddings. Given square images of resolution \(R R\), a vanilla ViT with patch size \(P\) learns 1-D positional embeddings of length \((}{{P}})^{2}\). Linearly interpolating these embeddings is necessary to train or evaluate at higher resolution \(R\).

Pix2struct  introduces learned 2D absolute positional embeddings, whereby positional embeddings of size \([,]\) are learned, and indexed with \((x,y)\) coordinates of each patch. This enables variable aspect ratios, with resolutions of up to \(R=P\). However, every combination of \((x,y)\) coordinates must be seen during training.

To support variable aspect ratios and readily extrapolate to unseen resolutions, we introduce factorized positional embeddings, where we decompose into separate embeddings \(_{x}\) and \(_{y}\) of \(x\) and \(y\) coordinates. These are then summed together (alternative combination strategies explored in Section 3.4). We consider two schema: absolute embeddings, where \((p):[0,]^{D}\) is a function of the absolute patch index, and fractional embeddings, where \((r):^{D}\) is a function of \(r=}{{}}\), that is, the relative distance along the image. The latter provides positional embedding parameters independent of the image size, but partially obfuscates the original aspect ratio, which is then only implicit in the number of patches. We consider simple learned embeddings \(\), sinusoidal embeddings, and the learned Fourier positional embedding used by NeRF .

### Training changes

Patch n' pack enables new techniques to be used during training of NaViT.

**Continuous Token dropping.** Token dropping (random omission of input patches during training) [9; 10] has been developed to accelerate training. However, typically the same proportion of tokens are dropped from all examples; packing enables continuous token dropping, whereby the token dropping rate can be varied per-image. This enables the benefits of faster throughput enabled by dropping while still seeing some complete images, reducing the train/inference discrepancy. Further, with packing, the

Figure 3: Height:width ratios of different datasets; most images are not square-ish (\(>20\%\) deviation).

drop-distribution can vary throughout training, following some pre-defined schedule. In Section 3.3, we explore different schedules and the benefits of flexible token dropping.

**Resolution sampling.** NaViT can be trained using the original resolution of each image. Alternatively, the total number of pixels can be resampled while preserving aspect ratio. In vanilla ViT, there is a tension between greater throughput (training on smaller images), and greater performance (training on larger images, to enable high-resolution at evaluation time). Oftentimes, models are pre-trained at a smaller resolution and finetuned at a higher one . NaViT is much more flexible; it allows mixed-resolution training by sampling from a distribution of image sizes, while retaining each images' original aspect ratio. This allows both higher throughput and exposure to large images, yielding substantial improved performance over equivalent ViTs (in terms of models size and training duration). Section 3.2 explores different sampling strategies, and variable resolution training for pre-training and finetuning.

### Efficiency of NaViT

Here we discuss some implications of Patch n' Pack on the computational efficiency of NaViT.

**Self attention cost.** The \((n^{2})\) cost of attention is a natural concern when packing multiple images into longer sequences. Though many works aim to remove this quadratic scaling [12; 13], we demonstrate here that as the transformer hidden dimension is scaled, the attention becomes an increasingly smaller proportion of the the overall cost, which encompasses the computation cost of the MLP as well. Figure 4 illustrates this trend, indicating a corresponding reduction in the overhead associated with packing examples. In addition to speed considerations, the memory cost of self-attention can pose a challenge for extremely long sequences. However, this challenge can be also addressed by employing memory-efficient methods [14; 15].

**Packing, and sequence-level padding.** The final sequence lengths containing multiple examples must be fixed. We use a greedy packing approach discussed in Appendix B.3; there typically is no perfect combination of examples exactly adding up to the fixed length and padding tokens have to be used. One could for example dynamically choose the resolution or token dropping rate of the final example in a sequence to exactly fit the remaining tokens; however, we find typically less 2% of tokens are padding tokens, and thus the simple approach is sufficient.

**Padding _examples_ and the contrastive loss.** Per-token losses are straightforward to implement with packed sequences. However, many computer vision models are trained with example-level losses, typically applied to a pooled representation. First, this requires modifications to the typical pooling heads to account for packing. Second, multiple pooled representations must be extracted from each sequence. Fixed batch shapes requires an assumption that, from a batch of \(B\) sequences, we extract at most \(B E_{}\) pooled representations (i.e. \(E_{}\) examples per sequence). If a sequence contains more than \(E_{}\) images, the extra images will be dropped, wasting computation of the model's encoder. If a sequence has less than \(E_{}\) examples, then the loss will process lots of fake padding representations.

The latter is an issue for contrastive learning, where loss computation scales in time and memory \((n^{2})\). To avoid this, we used the chunked contrastive loss , which circumvents the need to gather all data points for the softmax by performing computations on local device subsets and efficiently accumulating the necessary statistics for global softmax normalization. This enable high values of \(E_{}\) (and thus efficient use of the model encoder), without being bottlenecked by the loss.

## 3 Experiments

The base architecture we use for NaViT follows vanilla ViT , with the changes to enable packing, described in Section 2.1. In addition, we include small ViT improvements from previous works: query-key normalization and the omission of biases , and attention pooling .

We pre-train NaViT in two setups: classification training on JFT-4B  and contrastive language-image training  on WebLI . Typically, for JFT, inception crop is applied pre-training [20; 1], and in both cases, images are resized to a square (distorting aspect ratio). Unless otherwise specified,

Figure 4: Overhead from extra attention due to packing, assuming 256 tokens per image; it diminishes with model scale.

all NaViT models are pre-trained without these operations, and preserve aspect ratio. NaViT is implemented in JAX  using the FLAX library  and built within Scenic .

**Classification pretraining.** We pre-train NaViT with supervised classification objective, using a sigmoid cross-entropy loss, following the setup of  on JFT-4B . Visual representations are evaluated following the linear evaluation protocol used for ViT , where 10 examples per class are used to train a linear classifier on top of frozen representations.

**Contrastive pre-training.** Alongside the image model, we train a text encoder with the same architectural modifications using the contrastive image-text loss  (details in Appendix B.2). Packing also provides efficiency improvements on the text-tower, as text sequences do not need to be padded to a fixed lengths, which is the normal setup. The contrastive models are evaluated on zero-shot ImageNet classification and COCO image-text retrieval.

Note that throughout all the pre-training experiments and the transfer to downstream experiments, both the NaViT and baseline models are trained with compute-matched setup. This implies that during pretraining, the same amount of TPU time is used and for downstream tasks, the models are evaluated at the same effective resolution (i.e., the same number of tokens). To be precise, all downstream experiments utilized the top-rightmost points in Figure 1 (ViT-L/16 and NaViT-L/16).

### Improved training efficiency and performance

Figure 1 illustrates the JFT pretraining performance of different NaViT models compared to compute-matched ViT baselines . The experimental setup details are provided in Appendix B.1. NaViT consistently surpasses ViT in performance while using the same computational budget across different compute and parameter scales; for example, the performance of the top-performing ViT can be matched by a NaViT with four times less compute. Conversely, the computationally lightest NaViT in Figure 1 is five times more cost-effective than its equivalent ViT counterpart.

The NaViT models benefit from preserved aspect ratios and the ability to evaluate over many resolutions, but the chief contributor here is the significant increase in the number of training examples processed by NaViT within the allocated compute budget. This is achieved through the combination of sampling multiple variable-resolution examples and token dropping, leading to variable size images that are efficiently packed into a similar sequence length as the original model. We ablate these factors below.

### Benefits of variable resolution

Here, we deep-dive the benefits of mixed-resolution training. Since we preserve the native aspect ratio, when we refer to "resolution" for NaViT, we mean "effective resolution". That is, images with the same area as a square image with a given resolution. For example, for NaViT a resolution of "128" has the same area of a square 128 x 128 image, but could be 64 x 256, or 170 x 96, etc., and thus has the same inference cost as regular ViT on 128 x 128 images.

**Variable-resolution pre-training.** Lower resolution images require fewer FLOPs to process and hence small resolutions (like 224) are used with fixed-resolution training. With fixed-resolution training, there is a trade-off between throughput and ability to process details and high-resolution images. With NaViT we can mix lower resolution images with large ones to get the best of both worlds.

Figure 5 shows a comparison between two NaViT variants trained at several different resolutions. Here, _all trained for the same number of FLOPs._ (1) Native aspect ratio, but fixed resolution \(R=R_{}\) for different chosen values of \(R_{}\). (2) Variable resolution, where the resolution is distributed as \(R(64,R_{})\). Variable resolution models outperform models trained at only that resolution. Even in the best case for fixed resolution, where the train and evaluation resolutions are identical, variable resolution matches or outperforms fixed.

**Variable-resolution finetuning.** Prior works increase resolution late in pre-training or during finetuning, producing higher quality but more expensive models . We finetune NaViT and ViT models at different fixed resolutions, and additionally NaViT at variable resolutions. Figure 6 shows the results of fine-tuning pretrained ViT and NaViT on ImageNet-1k dataset. Performance gains during pretraining transfer well at all resolutions, but two phenomena are particularly interesting: First, NaViT finetuned with variable resolutions ("NaViT 64:512") is as good as a NaViT finetuned at a single resolution (and much better than single-resolution ViT), removing the need to pick a single downstream finetuning resolution. Second, NaViT finetuned at low resolution (64) still obtains good performance when evaluated at higher resolutions (Figure 6, right), enabling cheaper adaptation.This ability to perform cheap adaptation of a flexible pre-trained model corroborates findings in .

**Resolution sampling strategies.** Packing examples enables diverse resolution sampling strategies. We first consider whether to sample the target _side length_ (average height/width, \(R\)), or the target _area_ (i.e. sequence length \( R^{2}\)). Sampling the side length from a uniform distribution biases towards lower sequence lengths, whereas sampling the area from a uniform distribution biases towards higher side lengths.

For each image, we sample \(u\), where \(\) is a distribution with support \([-1,1]\). We rescale \(u\) linearly to \(\) for sampling side-lengths or \([64^{2},384^{2}]\) for sampling areas. We consider four distributions \(\): uniform \(u(-1,1)\), truncated (to \([-1,1]\)) standard Normal \(u_{t}(0,1)\), and then two other Normals which bias towards lower resolution \(u_{t}(-0.5,1)\) and higher resolution \(u_{t}(0.5,1)\). The results are shown in Figure 7. Here, the best resolution resampling strategy consistently performs over the default resolution. It is consistently better to sample side-lengths (orange) directly as opposed to area (blue), and the best distribution is the truncated normal biasing towards lower values; both of these increase throughput by preferentially sampling smaller sequences.

### Benefits of variable token dropping

**Token dropping strategies.**

We experimented with continuously sampled token dropping rates, and with resolution-dependent token dropping rates; both are explained in Appendix B.6.

Fig. 8(a) compares variable drop rates sampled from a Beta distribution to a constant drop rate, demonstrating consistent improvements from the former. Fig. 8(b) shows the use of a resolution dependent token dropping rate for models trained with \(R(64,384)\) and dropping rates scaled between \([0.5-,0.5+] R\), which further improves over the beta distribution.

Figure 8: Time-varying token dropping rates improves performance and are easily done with Patch n’ Pack.

Figure 6: Variable-resolution finetuning, JFT B/16 models finetuned on ImageNet at various resolutions. Overall NaViT in all settings (blue, red), outperforms ViT (orange) **Left**: A single NaViT finetuned with variable resolutions (red) is as good as models tuned on only one resolution (blue). **Right**: Mixed-resolution pretraining performs well at high resolution when finetuning at low resolution (left-hand end of blue curve).

Figure 7: Sampling side lengths directly with a bias towards lower resolutions gives overall best performance at a fixed computational budget.

**Scheduled token dropping rates.** Packing also enables easy variation the token dropping rate during training. By changing the token dropping rate we can better tune the trade-off between number of images seen and information used per image, to maximize the final accuracy while keeping the total training cost constant. We varied the token dropping rate as a function of the number of images seen (details of the schedule in Appendix B.7). Fig. 8 demonstrates that further improvements are possible by reducing the token dropping rate during JFT pretraining of NaViT-B/16.

### Positional embeddings

We evaluate our factorized embeddings introduced in Section 2.1, and their design choices. We are interested in both absolute performance, and extrapolation to resolutions outside the training regime. To test this, we train NaViT-B/16 models for 200k steps on JFT, with resolutions \(R(160,\!352)\). We evaluate performance at a range of resolutions, without modification of the embedding variables. We compare to a ViT-B/16 trained at fixed resolution 256 for the same amount of images seen, evaluated at new resolutions using standard interpolation of positional embeddings.

Figure 9(a) illustrates the disparity among different positional embedding methods when the model is evaluated under the "in-distribution" setup, where the resolutions of training and testing data are within a comparable range. Conversely, Figure 9(b) shows the model performance across out-of-distribution resolutions, revealing notable distinctions between the approaches.

First, it is clear that the factorized approaches outperform both the baseline ViT and the Learned 2D embeddings from Pix2struct. The latter in particular struggles to generalize to higher resolution, likely because this requires an increasingly long tail of unseen \((x,\!y)\) pairs. Factorized embeddings are best combined additively (as opposed to stacking or multiplying).

### Other aspects of NaViT's performance

**Out of distribution generalization.** We directly evaluate JFT-pretrained NaViT on downstream datasets, employing a label-map  from JFT-4B to ImageNet  and robustness-variants (ObjectNet  and ImageNet-A ). We compare the performance to a compute-matched ViT baseline.

Figure 10: Factorized position embeddings improve generalization to new resolutions and aspect ratios. (a) Best (faded) and average accuracies (dark) across resolutions. (b) Accuracy normalized w.r.t. resolution 256.

Figure 9: Continuous token dropping strategies enabled by sequence packing improves performanceFigure 11 shows that NaViT compares favorably both on ImageNet as well as datasets variants that were specifically designed to test out of distribution performance. It is interesting to note that NaViT performs much better on ImageNet-A, but ViT catches up on ObjectNet, even though both these datasets contain images that have extreme aspect ratios. We believe this is due to the aspect-preserving center crop that we apply to images for ImageNet-A and ObjectNet classification (same as in ), which is a useful prior for ObjectNet, but less so for ImageNet-A (see Appendix G for details). If no crop is applied to images and they're instead simply resized to the image resolution expected by the model (i.e., square for ViT, and aspect preserving resize to the same number of tokens for NaViT), then the observed difference is much larger (see Figure 21 in appendix).

**Calibration.** In addition to the in-depth accuracy analysis, we have also quantified the quality of the uncertainty computed by the model. In particular, for our ImageNet1K-finetuned models, we computed the expected calibration error  of the top prediction, as we vary the number of patches we assign per examples. We find that the calibration error remains _very stable_ in the interval \((0.045\),\(0.047)\) as we vary the number of patches per image in the range \([128\),\(1024]\), without any post-hoc recalibration. We provide further details in the appendix Appendix F.

**Inference trade-offs.** Given the flexibility of the model, there are several viable approaches how one can maximize the aggregate accuracy under a given compute budget, which we quantify in terms of the latency measured on a Cloud TPUv3 chip. In an online inference setting, choosing an inference strategy translates to an algorithm how to allocate a fixed number of tokens per example. As we show in Fig. 1, NaViT offers much better trade-offs than ViT and it shows strong diminishing returns, s.t., even relatively few patches provide highly competitive results. In Appendix D we further study a cascading approach, which both provides Pareto optimal models and gives more precise trade-off opportunities.

**Fairness signal annotation.** We investigate annotating images with fairness-related signals, such as those pertaining to gender and ethnicity. Prior research has shown that metrics, such as group calibration, are susceptible to labeling inaccuracy, particularly for underrepresented groups. In addition, this problem persists even when accounting for label noise during training . Thus, reducing the labeling error of fairness signals improves the reliability of bias mitigation and post-hoc auditing . To explore whether NaViT can help in overcoming these challenges, we train annotators on FairFace  and CelebA  datasets as linear probes (i.e. using frozen features produced by NaViT or ViT), before comparing their accuracy.

First, NaViT provides representations of higher quality that improve the accuracy of fairness signal annotation, even when dealing with square images. Original images are of size \(448 448\) in FairFace and \(178 218\) in CelebA, and we resize them to area \(224^{2}\) while preserving aspect ratios in NaViT. Despite having the same sequence length, NaViT provides a higher prediction accuracy than ViT, as shown in Figure 12 (left). We verify statistical significance using the Wilcoxon signed-rank test test , which reveals that the improvement in NaViT is significant with \(p\!=\!3 10^{-4}\).

Figure 11: Out of distribution evaluation of ViT and NaViT models that were matched for training compute. In addition to improved performance due to more images seen (see also Figure 1), NaViT performs much better on ImageNet-A that has many images with an extreme aspect ratio and important information outside the center crop (Appendix G). Same data as in Table 5.

[MISSING_PAGE_FAIL:9]

established both for training and evaluation (e.g. spatial and temporal multi-cropping) . NaViT alleviates some of these challenges by not only allowing training over different resolutions, but also over different temporal durations. We fine-tuned NaViT trained on JFT for Kinetics400  classification by extracting three different spatio-temporal patches "tubelets". , extending the positional embedding to include the temporal dimension and initializing the embedding kernel using "central frame embedding" . We posit that NaViT is an excellent starting point for multi-scale training due to the resolution diversity at training time. We observe that NaViT-L achieves competitive performance with ViViT-L (80.4%) in approximately 6x less epochs, without multi-crop evaluation. Note that the Kinetics400 dataset used here contains less data than prior works .

## 4 Related work

**Flexible Vision Transformers.** FlexiViT  developed a novel kernel resizing approach which enables variable "resolution" via models which support multiple patch sizes. This is viewed as unifying multiple distinct models, and they study the relationship with distillation and neural architecture search. Pix2struct  supported variable aspect ratios with a novel positional embedding schema, and demonstrated significant efficiency and performance improvements for non-natural imagery such as chart and document understanding.

**Multiscale Vision Transformers.** Using feature maps at multiple spatial scales is a common approach to localization tasks such as segmentation and detection . Many works developed Vision Transformers which do the same [44; 45], though some dispute the necessity for simple localization tasks . NaViT does not build hierarchical representations using multiple scales; we believe combining the unique flexibility of our approach with the benefits of this modelling family is a promising avenue.

**Accelerating training with mixed resolutions.** Image modelling works considering resolution largely focus on accelerating pretraining with a fixed, low resolution [47; 48]. FixRes  is a popular technique whereby resolution is increased in a final stage of pretraining, and has been used in many subsequent works [19; 6]. PaLI , for example, successively increases the resolution of the vision backbone during its generative training. This approach's main downside is its irreversibility: compute cannot be scaled back by reducing resolution after tuning at the higher and more expensive resolution.

Multigrid training  is the most closely related work. The main idea is accelerate video modelling by processing large batches with "coarse" spatiotemporal resolution early in the training, followed by a "finer" resolution later. To this end, the authors apply a hierarchical grid sampling schedule coupled with appropriate learning rate scaling. In contrast, Patch n' Pack enables effortless incorporation of mixed resolutions without complex schedules or training pipelines.

**Token dropping for improved efficiency.** Research initially explored random token dropping [9; 10; 50]. Follow ups demonstrated benefits from considering structured  or "importance" based strategies [52; 53]. Better strategies will likely further boost performance, and Patch n' Pack sidesteps the fixed minibatch shape restriction which limited these works.

## 5 Conclusions and future work

We have demonstrated that Patch n' Pack--the simple application of sequence packing to vision transformers--significantly improves training efficiency. The resultant NaViT models can be applied to many resolutions at inference time, and cheaply adapted to new tasks. We discuss limitations and impacts in Appendix A, but overall Patch n' Pack enables a wide variety of research previously hindered by the need for fixed batch shapes, including adaptive computation and new algorithms for improving training and inference efficiency.