# L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference

Julia Linhart

Universite Paris-Saclay, Inria, CEA

Palaiseau 91120, France

julia.linhart@inria.fr

&Alexandre Gramfort1

Universite Paris-Saclay, Inria, CEA

Palaiseau 91120, France

alexandre.gramfort@inria.fr

&Pedro L. C. Rodrigues

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK

Grenoble 38000, France

pedro.rodrigues@inria.fr

###### Abstract

Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce \(\)-C2ST, a new method that allows for a _local_ evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, \(\)-C2ST can be specialized to offer better statistical power, while being computationally more efficient. On standard SBI benchmarks, \(\)-C2ST provides comparable results to C2ST and outperforms alternative local approaches such as coverage tests based on highest predictive density (HPD). We further highlight the importance of _local_ evaluation and the benefit of interpretability of \(\)-C2ST on a challenging application from computational neuroscience.

## 1 Introduction

Expressive simulators are at the core of modern experimental science, enabling the exploration of rare or challenging-to-measure events in complex systems across various fields such as population genetics , astrophysics , cosmology , and neuroscience . These simulators implicitly encode the intractable likelihood function \(p(x)\) of the underlying mechanistic model, where \(\) represents a set of relevant parameters and \(x()\) is the corresponding realistic observation. The main objective is to infer the parameters associated with a given observation using the simulator's posterior distribution \(p( x)\). However, classical methods for sampling posterior distributions, such as MCMC  and variational inference , rely on the explicit evaluation of the model-likelihood, which is not possible when working with most modern simulators.

Simulation-based inference (SBI)  addresses this problem by estimating the posterior distribution on simulated data from the joint distribution. This can be done after choosing a prior distribution\(p()\) over the parameter space and using the identity \(p(,x)=p(x)p()\). In light of recent developments in the literature on deep generative models, different families of algorithms have been proposed to approximate posterior distributions in SBI . Certain works use normalizing flows  to directly learn the posterior density function (neural posterior estimation, NPE ) or aim for the likelihood (neural likelihood estimation, NLE ). Other approaches reframe the problem in terms of a classification task and aim for likelihood ratios (neural ratio estimation, NRE ). However, appropriate validation remains a challenge for all these paradigms, and principled statistical approaches are still needed before SBI can become a trustworthy technology for experimental science.

This topic has been the goal of many recent studies. For instance, certain proposals aim at improving the posterior estimation by preventing over-confidence  or addressing model misspecification  to ensure conservative  and more robust posterior estimators [46; 27]. Another approach is the development of a SBI benchmark  for comparing and validating different algorithms on many standard tasks. While various validation metrics exist, Lueckmann et al.  show that, overall, classifier two sample tests (C2ST)  are currently the most powerful and flexible approach. Based on standard methods for binary classification, they can scale to high-dimensions as well as handle non-Euclidean data spaces [26; 34]. Typical use-cases include tests for statistical independence and the evaluation of sample quality for generative models . Implicitly, C2ST is used in algorithms such as noise contrastive estimation  and generative adversarial networks , or to estimate likelihood-to-evidence ratios . To be applied in SBI settings, however, C2ST requires access to samples from the true target posterior distribution, which renders it useless in practice. Simulation-based calibration (SBC)  bypasses this issue by only requiring samples from the joint distribution. Implemented in standard packages of the field (sbi , Stan ), it has become the go-to validation method for SBI [31; 23] and has been further studied in recent works [33; 5; 15]. Coverage tests based on the highest predictive density (HPD) as used in [23; 8], can be seen as a variant of SBC that are particularly well adapted to multivariate data distribution.

Nevertheless, a big limitation of current SBI diagnostics remains: they only evaluate the quality of the posterior approximation globally (in expectation) over the observation space and fail to give any insight of its _local_ behavior. This hinders interpretability and can lead to false conclusions on the validity of the estimator [48; 29]. There have been attempts to make existing methods local, such as _local_-HPD  or _local_-multi-PIT , but they depend on many hyper-parameters and are computationally too expensive to be used in practice. In this work, we present \(\)-C2ST, a new _local_ validation procedure based on C2ST that can be used to evaluate the quality of SBI posterior approximations for any given observation, without using any data from the target posterior distribution. \(\)-C2ST comes with necessary, and sufficient, conditions for the local validity of multivariate posteriors and is particularly computationally efficient when applied to validate NPE with normalizing flows, as often done in SBI literature [7; 16; 27; 46; 6; 45; 24]. Furthermore, \(\)-C2ST offers graphical tools for analysing the inconsistency of posterior approximations, showing in which regions of the observation space the estimator should be improved and how to act upon, e.g. signs of positive / negative bias, signs of over / under dispersion, etc.

In what follows, we first introduce the SBI framework and review the basics of C2ST. Then, we detail the \(\)-C2ST method and prove asymptotic theoretical guarantees. Finally, we report empirical results on two SBI benchmark examples to analyze the performance of \(\)-C2ST and a non-trivial neuroscience use-case that showcases the need of a local validation method.

## 2 Validating posterior approximations with classifiers

Consider a model with parameters \(^{m}\) and observations \(x^{d}\) obtained via a simulator. In what follows, we will always assume the typical _simulation-based inference setting_, meaning that the likelihood function \(p(x)\) of the model cannot be easily evaluated. Given a prior distribution \(p()\), it is possible to generate samples from the joint pdf \(p(,x)\) as per:

\[_{n} p() X_{n}=( _{n}) p(x_{n})(_{n},X_{n}) p( ,x)\;.\] (1)

Let \(N_{s}\) be a fixed simulation budget and \(\{(_{n},X_{n})\}_{n=1}^{N_{s}}=_{} _{}\) with \(_{}_{}=\). The data from \(_{}\) are used to train an amortized2 approximation \(q( x) p( x)\), e.g. via NPE , and those from \(_{}\) to diagnose its _local consistency_.

**Definition 1** (Local consistency).: _A conditional density estimator \(q\) is said to be locally consistent at \(x_{}\) with the true posterior density \(p\) if, and only if, the following null hypothesis holds:_

\[_{0}(x_{}):q( x_{})=p( x_{ }),^{m}\;.\] (2)

We can reformulate \(_{0}(x_{})\) as a binary classification problem by partitioning the parameter space into two balanced classes: one for samples from the approximation (\(C=0\)) and one for samples from the true posterior (\(C=1\)), as in

\[(C=0) q( x_{}) (C=1) p( x_{})\;,\] (3)

for which the _optimal Bayes classifier_ is \(f_{x_{}}^{}()=*{argmax}1-d_{x_{ }}^{}(),d_{x_{}}^{}()}\) with

\[d_{x_{}}^{}()=(C=1=;x_{ })=1-(C=0=;x_{})=})}{p( x_{})+q( x_{ })}\;.\] (4)

It is a standard result  to relate (2) with (3) as per

\[_{0}(x_{})\; d_{x_{}}^{ }()=(C=1=;x_{})=\] (5)

When the classes are non-separable, the optimal Bayes classifier will be unable to make a decision and we can assume that it behaves as a Bernoulli random variable .

### Classifier Two-Sample Test (C2ST)

The original version of C2ST  uses (5) to define a test statistic for \(_{0}(x_{})\) based on the accuracy of a classifier \(f_{x_{}}\) trained on a dataset defined as

\[^{q} q( x_{})}_{C=0}^{p} p( x_{})}_{C=1} =\{(_{n}^{q},0)\}_{n=1}^{N}\{(_{n }^{p},1)\}_{n=1}^{N}\;.\] (6)

The classifier accuracy is then empirically estimated over \(2N_{v}\) samples (\(N_{v}\) samples in each class) from a held-out validation dataset \(_{v}\) generated in the same way as \(\):

\[_{}(f_{x_{}})=}_{n=1}^{2N_{v}} f_{x_{}}(_{n}^{q})=0+f_{x_{}}(_{n}^{p})=1.\] (7)

**Theorem 1** (Local consistency and classification accuracy).: _If \(f_{x_{}}\) is Bayes optimal3 and \(N_{v}\), then \(_{}(f_{x_{}})=1/2\) is a necessary and sufficient condition for the local consistency of \(q\) at \(x_{}\)._

See Appendix A.1 for a proof. The intuition is that, under the null hypothesis \(_{0}(x_{})\), it is impossible for the optimal classifier to distinguish between the two data classes, and its accuracy will remain at chance-level . In the context of SBI, C2ST has been used to benchmark a variety of different procedures on toy examples where the true posterior is known and can be sampled . This is why we call this procedure an _oracle_ C2ST, since it uses information that is not available in practice.

**Regression C2ST.** Kim et al.  argues that the usual C2ST based on the classifier's accuracy may lack statistical power because of the "binarization" of the posterior class probabilities. They propose to instead use probabilistic classifiers (e.g. logistic regression) of the form

\[f_{x_{}}()=(d_{x_{}}()>) d_{x_{}}()=(C= 1;x_{})\] (8)

and define the test statistics in terms of the predicted class probabilities \(d_{x_{}}\) instead of the predicted class labels. The test statistic is then the mean squared distance between the estimated class posterior probability and one half:

\[_{}(f_{x_{}})=}_{n=1}^{N_{v}} (d_{x_{}}(_{n}^{q})-)^{2}+} _{n=1}^{N_{v}}(d_{x_{}}(_{n}^{p})- )^{2}\] (9)

**Theorem 2** (Local consistency and regression).: _If \(f_{x_{}}\) is Bayes optimal and \(N_{v}\), then \(_{}(f_{x_{}})=0\) is a necessary and sufficient condition for the local consistency of \(q\) at \(x_{}\)._

See Appendix A.2 for a proof. The numerical illustrations in Kim et al.  give empirical evidence that _Regression_ C2ST has superior statistical power as compared to its accuracy-based counterpart, particularly for high-dimensional data spaces. Furthermore, it offers tools for interpretation and visualization: evaluating the predicted class probabilities \(d_{x_{}}()\) for any \(^{m}\) informs the regions where the classifier is more (or less) confident about its choice .

\(\)-C2ST: Local Classifier Two-Sample Tests

The _oracle_ C2ST framework is not applicable in practical SBI settings, since it requires access to samples from the true posterior distribution to (1) **train** a classifier and (2) **evaluate** its performance in discriminating data from \(q\) and \(p\). This section presents a new method called _local_ C2ST (\(\)-C2ST) capable of evaluating the local consistency of a posterior approximation requiring data only from the joint pdf \(p(,x)\) which can be easily sampled as per (1).

**(1) Train the classifier.** We define a modified version of the classification framework (3) with:

\[(,X)(C=0) q( x)p(x)(,X)( C=1) p(,x)\.\] (10)

The optimal Bayes classifier is now \(f^{}(,x)=*{argmax}\{1-d^{}(,x),d^{ }(,x)\}\) with

\[d^{}(,x)===d^{}_{x}()\,\] (11)

where one can notice the direct relation with the Bayes classifier for (3). Therefore, using data sampled as in (10), it is possible to train a classifier \(f(,x)\) and write \(f_{x_{}}()=f(,x_{})\) for each \(x_{}\). See Algorithm 1 for details on the implementation of this procedure.

**(2) Evaluate the classifier.** Define a new test statistic that evaluates the MSE-statistic for a classifier \(f\) and its associated predicted probabilities \(d\) using data samples from only the class associated to the posterior approximation (\(C=0\)):

\[_{_{0}}(f,x_{})=}_{n=1}^{N_{v} }(d(^{q}_{n},x_{})-)^{2} ^{q}_{n} q( x_{})\.\] (12)

**Theorem 3** (Local consistency and single class evaluation).: _If \(f\) is Bayes optimal and \(N_{v}\), then \(_{_{0}}(f,x_{})=0\) is a necessary and sufficient condition for the local consistency of \(q\) at \(x_{}\)._

Proof.: Let \(d\) be an estimator of \((C=1,X)\) and \(f=(d>0.5)\). Suppose that \(f=f^{}\) is _Bayes optimal_ and let \(x_{}\) be a fixed observation. We have that

\[_{N_{v}}_{_{0}}(f^{},x_{})= (d^{}_{x_{}}()-)^{2}q(  x_{})\.\]

Because of the squared term in the integral and \(q\) being a p.d.f., we have that

\[_{N_{v}}_{_{0}}(f^{},x_{})=0  d^{}_{x_{}}()=(C=1;x_ {})=\.\]

This new statistical test can thus be used to assess the local consistency of posterior approximation \(q\) without using any sample from the true posterior distribution \(p\), but only from the joint pdf. Furthermore, it is amortized, so a single classifier is trained for (10) that can then be used for any choice of conditioning observation \(x_{}\). This is not the case in the usual _oracle_ C2ST framework.

Figure 1 illustrates the behavior of different test statistics to discriminate samples from two bivariate Normal distributions whose covariance mismatch is controlled by a single scaling parameter \(\). Note that the optimal Bayes classifier for this setting can be obtained via quadratic discriminant analysis (QDA) . The results clearly show that even though \(t_{_{0}}\) exploits only half of the dataset (i.e. samples from class \(C=0\)) it is capable of detecting when \(p\) and \(q\) are different (\( 1\)). The plot also

Figure 1: Results for the C2ST framework when \(p=(0,_{2})\) and \(q=(0,^{2}_{2})\). **Left** panel portrays the test statistics for the optimal Bayes classifier and the **right** panel shows the testâ€™s empirical power with QDA. Single-class accuracy test (\(_{_{0}}\)) fails to detect when \(p q\) but \(_{_{0}}\) behaves correctly.

includes the results for a one-class test statistic based on accuracy values (\(_{_{0}}\)) which, as opposed to \(_{_{0}}\), has no guarantees for being a necessary and sufficient condition for local consistency. Not surprisingly, it fails to reject the null hypothesis for various choices of \(\).

The assumptions of Theorem 3 are never met in practice: datasets are finite and one rarely knows which type of classifier is optimal for a given problem. Therefore, the values of \(_{_{0}}\) in the null hypothesis (\(p=q\)) tend to fluctuate around one-half, and it is essential to determine a threshold for deciding whether or not \(_{0}(x_{})\) should be rejected. In \(\)-C2ST, these threshold values are obtained via a permutation procedure  described in Algorithm 1. This yields \(N_{}\) estimates of the test statistic under the null hypothesis and can be used to calculate \(p\)-values for any given \(\) significance level as described in Algorithm 2. These estimates can also be used to form graphical summaries known as PP-plots, which display the empirical CDF of the probability predictions versus the nominal probability level. These plots show how the predicted class probability \(d(x_{})\) deviates from its theoretical value under the null hypothesis (i.e. one half) as well as \((1-)\) confidence regions; see Algorithm B.1 available in the appendix for more details and Figure 4 for an example.

``` Input: posterior estimator \(q\); calibration data \(_{}=\{_{n},X_{n}\}_{n=1}^{N_{}}\); classifier \(f\); number of samples \(N_{}\) from the distribution under the null hypothesis Output: estimate \(d\) of the class probabilities; estimates \(\{d_{1},,d_{N_{}}\}\) under the null hypothesis /* Construct classification training set */ for\(n=1,,N_{}\)do \(_{n}^{q} q( X_{n})\) \(W_{2n}=(_{n}^{q},X_{n})\); \(C_{2n}=0\) /* Sample from \(q( x)p(x)\) */ \(W_{2n+1}=(_{n},X_{n})\); \(C_{2n+1}=1\) /* Sample from \(p(,x)\) */ \(\{W_{n},C_{n}\}_{n=1}^{2N_{}}\) /* Get estimate \(d\) of the class probabilities */ Train the classifier \(f\) on \(\) \(d f_{}\) /* Estimate \(d\) under the null hypothesis via permutation procedure */ for\(h=1, N_{}\)do  Randomly permute labels \(C_{n}\) in \(\)  Train the classifier \(f\) on new \(\) \(d_{h} f_{}\) return\(d\); \(\{d_{1},,d_{N_{}}\}\) ```

**Algorithm 1**\(\)-C2ST - training the classifier on data from the joint distribution

### The case of normalizing flows

The \(\)-C2ST framework can be further improved when the posterior approximation \(q\) is a conditional normalizing flow , which we denote \(q_{}\). Given a Gaussian base distribution \(u(z)=(0,I_{m})\) and a bijective transform \(T_{}(;x)\) with Jacobian \(J_{T_{}}(;x)\) we have

\[q_{}( x)=u(z)| J_{T_{}}(z;x)|^{-1}\;,=T_{ }(z;x)^{m}.\] (13)In other words, normalizing flows (NF) are invertible neural networks that define a map between a latent space where data follows a Gaussian distribution and the parameter space containing complex posterior distributions. This allows for both efficient sampling and density evaluation:

\[Z(0,I_{m})\ \ ^{q}=T_{}(Z;x)  q_{}( x)\;,\] (14) \[q_{}( x)=u(T_{}^{-1}(;x))| J_{T_{ }^{-1}}(;x)|\;.\] (15)

Our main observation is that the inverse transform \(T_{}^{-1}\) can also be used to characterize the local consistency of the conditional normalizing flow in its latent space, yielding a much simpler and computationally less expensive statistical test for posterior local consistency.

**Theorem 4** (Local consistency and normalizing flows).: _Given a posterior approximation \(q_{}\) based on a normalizing flow, its local consistency at \(x_{o}\) can be characterized as follows:_

\[p( x_{o})=q_{}( x_{o}) p(T_{}^{-1}(;x_{o }) x_{o})=u(z)\;,^{m}\;.\] (16)

Proof.: Let \( p( x_{o})\). Following (14), we have that \( q_{}( x_{o})\) if, and only if, \(=T_{}(Z;x_{o})\) with \(Z(0,I_{m})\). Applying the inverse transformation of the flow gives us \(T_{}^{-1}(;x_{o})=T_{}^{-1}(T_{}(Z;x_{o});x_{o})=Z (0,I_{m})\), which concludes the proof.

Based on Theorem 4 we propose a modified version of our statistical test named \(\)-C2ST-NF. The new null hypothesis associated with the consistency of the posterior approximation \(q_{}\) at \(x_{o}\) is

\[_{0}^{ NF}(x_{o}):p(T_{}^{-1}(;x_{o}) x_{o})= (0,I_{m})\;,\] (17)

which leads to a new binary classification framework

\[(Z,X)(C=0)(0,I_{m})p(x)(Z,X)(C=1)  p(T_{}^{-1}(;x),x)\;.\] (18)

Algorithm 3 describes how to sample data from each class and **train** a classifier to discriminate them. The classifier is then **evaluated** on \(N_{v}\) samples \(Z_{n}(0,I_{m})\) which are independent of \(x_{o}\).

A remarkable feature of \(\)-C2ST-NF is that calculating the test statistics under the null hypothesis is considerably faster than for \(\)-C2ST. In fact, for each null trial \(h=1,,N_{}\) we use the dataset \(_{ cal}\) only for recovering the samples \(X_{n}\) and then _independently_ sample new data \(Z_{n}(0,I_{m})\). As such, it is possible to pre-train the classifiers without relying on a permutation procedure (cf. Algorithm 4), and to re-use them to quickly compute the validation diagnostics for any choice of \(x_{o}\) or new posterior estimator \(q_{}\) of the given inference task. It is worth mentioning that this is not possible with the usual \(\)-C2ST, as it depends on \(q\) and it would require new simulations for each trial.

``` Input: NF posterior estimator \(q_{}\); calibration data \(_{ cal}=\{_{n},X_{n}\}_{n=1}^{N_{ cal}}\) ; classifier \(f\) Output: estimate \(d\) of the class probabilities /* Construct classification training set */ for\(n\)in\(1,,N_{ cal}\)do \(Z_{n}(0,I_{m})\); \(Z_{n}^{q}=T_{}^{-1}(_{n};X_{n})\) /* inverse NF-transformation */ \(W_{2n}=(Z_{n},X_{n})\); \(C_{2n}=0\) \(W_{2n+1}=(Z_{n},X_{n})\); \(C_{2n+1}=1\) \(\{W_{n},C_{n}\}_{n=1}^{2N_{ cal}}\) /* Get estimate \(d\) of the class probabilities */  Train the classifier \(f\) on \(\) \(d f_{ probability}\) return\(d\) ```

**Algorithm 3**\(\)-C2ST-NF - training the classifier on the joint distribution

## 4 Experiments

All experiments were implemented with Python and the sbi package  combined with PyTorch  and nflows for neural posterior estimation 4. Classifiers on the C2ST framework use the MLPClassifier from scikit-learn  with the same parameters as those used in sbibm.

### Two benchmark examples for SBI

We illustrate \(\)-C2ST on two examples: Two Moons and SLCP. These models have been widely used in previous works from SBI literature [18; 36] and are part of the SBI benchmark . They both represent difficult inference tasks with locally structured multimodal true posteriors in, respectively, low (\(^{2},x^{2}\)) and high (\(^{5},x^{8}\)) dimensions. See  for more details. To demonstrate the benefits of \(\)-C2ST-NF, all experiments use neural spline flows  trained under the amortized paradigm for neural posterior estimation (NPE) . We use implementations from the sbibm package  to ensure uniform and consistent experimental setups. Samples from the true posterior distributions for both examples are obtained via MCMC and used to compare \(\)-C2ST(NF) to the _oracle_-C2ST framework. We include results for _local_-HPD implemented using the code repository of the authors of  with default hyper-parameters and applied to HPD.5

First, we evaluate the local consistency of the posterior estimators of each task over ten different observations \(x_{}\) with varying \(N_{}\) and fixed \(N_{}=10^{4}\). The first column of Figure 2 displays the MSE statistics for the _oracle_ and \(\)-C2ST frameworks. As expected, we observe a decrease in the test statistics as \(N_{}\) increases: more training data usually means better approximations. For Two Moons the statistic of \(\)-C2ST decreases at the same rate as _oracle_-C2ST, with notably accurate results for \(\)-C2ST-NF. However, in SLCP both \(\)-C2ST statistics decrease much faster than the _oracle_. This is possibly due to the higher dimension of the observation space in the latter case, which impacts the training-procedure of \(\)-C2ST on the joint distribution.

We proceed with an empirical analysis based on 50 random test runs for each validation method and computing their rejection-rates to the nominal significance level of \(=0.05\). Column 4 in Figure 2 confirms that the false positive rates (or type I errors) for all tests are controlled at desired level. Column 2 of Figure 2 portrays the true positive rates (TPR), i.e. rejecting \(_{0}(x_{})\) when \(p q\), of each test as a function of \(N_{}\). Both \(\)-C2ST strategies decrease with \(N_{}\) as in Column 1, with higher TPR for \(\)-C2ST-NF in both tasks. The _oracle_ has maximum TPR and rejects the local consistency of all posterior estimates across all observations at least 90% of the time. Note that SLCP is designed to be a difficult model to estimate6, meaning that higher values of TPR are expected (\( 0\) in Column 1). In Two Moons, the decreasing rejection rate can be seen as normal as it reflects the convergence of the posterior approximator (\( 0\) in Column 1): as \(N_{}\) increases, the task of differentiating the estimator from the true posterior becomes increasingly difficult.

We fix \(N_{}=10^{3}\) (which yields inconsistent \(q_{}\) in both examples) and investigate in Column 3 of Figure 2 how many calibration samples are needed to get a maximal TPR in each validation method.

SLCP is expected to be an easy classification task, since the posterior estimator is very far from the true posterior (large values of \(\) in Column 1). We observe similar performance for \(\)-C2ST-NF and \(\)-C2ST, with slightly faster convergence for the latter. Both methods perform better than _local_-HPD, that never reaches maximum TPR. Two Moons represents a harder discrimination task, as \(q_{}\) is already pretty close to the reference posterior (see Column 1). Here, \(\)-C2ST-NF attains maximum power at \(N_{}=2000\) and outperforms all other methods. Surprisingly, the regression-based _oracle_-C2ST performs comparably to _local_-HPD, converging to \(=1\) at \(N_{}=5000\).

### Jansen-Rit Neural Mass Model (JRNMM)

We increase the complexity of our examples and consider the well known Jansen & Rit neural mass model (JRNMM) . This is a neuroscience model which takes parameters \(=(C,,,g)^{4}\) as input and generates time series \(x^{1024}\) with properties similar to brain signals obtained in neurophysiology. Each parameter has a physiologically meaningful interpretation, but they are not relevant for the purposes of this section; the interested reader is referred to  for more details.

The approximation \(q_{}\) of the model's posterior distribution is a conditioned masked autoregressive flow (MAF)  with 10 layers. We follow the same experimental setting from , with a uniform prior over physiologically-relevant parameter values and a simulated dataset from the joint distribution including \(N_{}=50\;000\) training samples for the posterior estimator and \(N_{}=10\;000\) samples to compute the validation diagnostics. An evaluation set of size \(N_{0}=10\;000\) is used for \(\)-C2ST-NF.

We first investigate the _global consistency_ of our approximation, which informs whether \(q_{}\) is consistent (or not) on average with the model's true posterior distribution. We use standard tools for this task such as simulation-based calibration (SBC)  and coverage tests based on highest predictive density (HPD) . The results are shown in the left panel of Figure 3. We observe that the empirical cdf of the global HPD rank-statistic deviates from the identity function (black dashed line), indicating that the approximator presents signs of global inconsistency. We also note that the marginal SBC-ranks are unable to detect any inconsistencies in \(q_{}\).

We use \(\)-C2ST-NF to study the _local consistency_ of \(q_{}\) on a set of nine observations \(x_{}^{(i)}\) defined as7

\[x_{}^{(i)}=(_{}^{(i)}) _{}^{(i)}=(135,220,2000,g_{}^{(i)}) g_{}^{(i)}[-20,+20]\;.\] (19)

The right panel of Figure 3 shows that the test statistics of \(\)-C2ST-NF vary in a U-shape, attaining higher values as \(g_{}\) deviates from zero and at the borders of the prior. The plot is overlaid with the \(95\%\) confidence region, illustrating how much the test statistics deviate from the local null hypothesis.

Figure 2: Results on two examples from the SBI benchmark: SLCP and Two Moons. We compare \(\)-C2ST and \(\)-C2ST-NF (dashed) to the _oracle_C2ST and _local_-HPD. Columns 1 and 2 display the test statistic and empirical power as a function of \(N_{}\), while Columns 3 and 4 show the empirical power and type I error for varying \(N_{}\). The \(\)-C2ST-(NF) statistics are comparable to the oracle, as their decreasing behaviour reflects the convergence of NPE to the true posterior for large training datasets. We also note that \(\)-C2ST-NF is uniformly better than \(\)-C2ST (i.e. higher power for all \(N_{}\) and increases faster with \(N_{}\)), and both reach maximum statistical power with smaller \(N_{}\) than _local_-HPD. All Type I errors are controlled at \(=0.05\). Experiments were performed over 10 different observations \(x_{}\) (mean and std) and Columns 2-4 used additional 50 random test runs.

We demonstrate the interpretability of the results for \(\)-C2ST-NF with a focus on the behavior of \(q_{}\) when conditioned on an observation for which \(g_{}=10\). The local PP-plot in Figure 4 summarises the test result: the predicted class probabilities deviate from \(0.5\), outside of the \(95\%\)-CR, thus rejecting the null hypothesis of local consistency at \(g_{}=10\). The rest of Figure 4 displays 1D and 2D histograms of samples \(^{q} q_{}( x_{})\) within the prior region, obtained by applying the learned \(T_{}\) to samples \(Z(0,I_{4})\). The color of each histogram bin is mapped to the intensity of the corresponding predicted probability in \(\)-C2ST-NF and informs the regions where the classifier is more (resp. less) confident about its choice of predicting class 0.8 This relates to regions in the parameter space where the posterior approximation has too much (resp. not enough mass) w.r.t. to the true posterior: \(q_{}>p\) (resp. \(q_{}<p\)). We observe that the ground-truth parameters are often outside of the red regions, indicating positive bias for \(\) and \(\) and negative bias for \(g\) in the 1D marginal. It also shows that the posterior is over-dispersed in all 2D marginals. See Appendix D for results on all observations \(x_{}^{(i)}\).

## 5 Discussion

We have presented \(\)-C2ST, an extension to the C2ST framework tailored for SBI settings which requires only samples from the joint distribution and is amortized along conditioning observations. Strikingly, empirical results show that, while \(\)-C2ST does not have access to samples from the true posterior distribution, it is actually competitive with the oracle-C2ST approach that does. This comes at the price of training a binary classifier on a potentially large dataset to ensure the correct calibration

Figure 4: Graphical diagnostics of \(\)-C2ST-NF for JRNMM. Top right panel displays the empirical CDF of the classifier (blue) overlaid with the theoretical CDF of the null hypothesis (step function at \(0.5\)) and \(95\%\) confidence region of estimated classifiers under the null displayed in gray. The pairplot displays histograms of samples from \(q_{}\) within the prior region and dashed lines indicate values of \(_{}\) used to generate the conditioning observation \(x_{}\). The predicted probabilities are mapped on the colors of the bins in the histogram. Blue-green (resp. orange-red) regions indicate low (resp. high) predicted probabilities of the classifier. Yellow regions correspond to chance level, thus \(q_{} p\).

Figure 3: Results for global and local tests on JRNMM. **Left**: PP-plots for the marginal SBC and global HPD rank statistics. **Right**: Test statistics for \(\)-C2ST-NF on observations with varying \(g_{}\). SBC fails to detect any inconsistency of \(q_{}\), while HPD only provides a global assessment, unlike \(\)-C2ST which locally explains the inconsistencies in \(q_{}\).

of the predicted probabilities. Should this be not the case, some additional calibration step for the classifier can be considered .

Notably, \(\)-C2ST allows for a local analysis of the consistency of posterior approximations and is more sensible, precise, and computationally efficient than its concurrent method, _local_-HPD. Appendix F.4 provides a detailed discussion of these statements, based on results obtained for additional benchmark examples. When exploiting properties of normalizing flows, \(\)-C2ST can be further improved as demonstrated by encouraging results on difficult posterior estimation tasks (see Table 2 in Appendix F). We further analyze the benefits of this -NF version in Appendix F.3. \(\)-C2ST provides necessary, and sufficient, conditions for posterior consistency, features that are not shared by other standard methods in the literature (e.g. SBC). When applied to a widely used model from computational neuroscience, the local diagnostic proposed by \(\)-C2ST offered interesting and useful insights on the failure modes of the SBI approach (e.g. poor estimates on the border of the prior intervals), hence demonstrating its potential practical relevance for works leveraging simulators for scientific discoveries.

## 6 Limitations and Perspectives

**Training a classifier with finite data.** The proposed validation method leverages classifiers to learn global and local data structures and shows great potential in diagnosing conditional density estimators. However, it's validity is only theoretically guaranteed by the optimality of the classifier when \(N_{v}\). In practice, this can never perfectly be ensured. Figure 6 in Appendix F.2 shows that depending on the dataset, \(\)-C2ST can be more or less accurate w.r.t. the true C2ST. Therefore, one should always be concerned about false conclusions due to a far-from-optimal classifier and make sure that the classifier is "good enough" before using it as a diagnostic tool, e.g. via cross-validation. Note, however, that the MSE test statistic for \(\)-C2ST is defined by the predicted class probabilities and not the accuracy of the classifier, thus one should also check how well the classifier is calibrated.

**Why Binary Classification?** An alternative to binary classification would have been to train a second posterior estimate in order to assess the consistency of the first one. Indeed, one could ask whether training a classifier is inherently easier than obtaining a good variational posterior, the response to which is non-trivial. Nevertheless, we believe that adding diversity into the validation pipeline with two different ML approaches might be preferable. Furthermore, building our method on the C2ST framework was mainly motivated by the popularity and robustness of binary classification: it is easy to understand and has a much richer and stable literature than deep generative models. As such, we believe that choosing a validation based on a binary classifier has the potential of attracting the interest of scientists across various fields, rather than solely appealing to the probabilistic machine learning community.

**Possible extensions and improvements.** Future work could focus on leveraging additional information of \(q\) while training the classifier as in . For example, by using more samples from the posterior estimator \(q\) or its explicit likelihood function (which is accessible when \(q\) is a normalizing flow). On a different note, split-sample conformal inference could be used to speed up the \(p\)-value calculations (avoiding the time-consuming permutation step in Algorithm 1).

In summary, our article shows that \(\)-C2ST is theoretically valid and works on several datasets, sometimes even outperforming _local_-HPD, which to our knowlegde is the only other existing local diagnostic. Despite facing some difficulties for certain examples (just like for other methods as well), an important feature of \(\)-C2ST is that one can directly leverage from improvements in binary classification to adapt and enhance it for any given dataset and task. This makes \(\)-C2ST a competitive alternative to other validation approches, with great potential of becoming the go-to validation diagnostic for SBI practitioners.