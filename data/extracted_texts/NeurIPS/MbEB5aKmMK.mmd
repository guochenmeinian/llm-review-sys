# Online Composite Optimization Between

Stochastic and Adversarial Environments

Yibo Wang\({}^{1,2}\), Sijia Chen\({}^{1,2}\), Wei Jiang\({}^{1}\), Wenhao Yang\({}^{1,2}\), Yuanyu Wan\({}^{3,1}\), Lijun Zhang\({}^{1,2}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, Nanjing, China

\({}^{3}\)School of Software Technology, Zhejiang University, Ningbo, China

{wangyb, chensj, jiangw, yangwh, zhanglj}@lamda.nju.edu.cn, wanyy@zju.edu.cn

Lijun Zhang\({}^{1,2}\)

Lijun Zhang is the corresponding author.

###### Abstract

We study online composite optimization under the Stochastically Extended Adversarial (SEA) model. Specifically, each loss function consists of two parts: a fixed non-smooth and convex regularizer, and a time-varying function which can be chosen either stochastically, adversarially, or in a manner that interpolates between the two extremes. In this setting, we show that for smooth and convex time-varying functions, optimistic composite mirror descent (OptCMD) can obtain an \((^{2}}+^{2}})\) regret bound, where \(_{1:T}^{2}\) and \(_{1:T}^{2}\) denote the cumulative stochastic variance and the cumulative adversarial variation of time-varying functions, respectively. For smooth and strongly convex time-varying functions, we establish an \(((_{}^{2}+_{}^{2})(_{1:T}^{2}+_ {1:T}^{2}))\) regret bound, where \(_{}^{2}\) and \(_{}^{2}\) denote the maximal stochastic variance and the maximal adversarial variation, respectively. For smooth and exp-concave time-varying functions, we achieve an \((d(_{1:T}^{2}+_{1:T}^{2}))\) bound where \(d\) denotes the dimensionality. Moreover, to deal with the unknown function type in practical problems, we propose a multi-level _universal_ algorithm that is able to achieve the desirable bounds for three types of time-varying functions simultaneously. It should be noticed that all our findings match existing bounds for the SEA model without the regularizer, which implies that there is _no price_ in regret bounds for the benefits gained from the regularizer.

## 1 Introduction

Online composite optimization has drawn considerable attention in recent years (Duchi and Singer, 2009; Ghadimi and Lan, 2012; Lei and Zhou, 2017; Scroccaro et al., 2023). Formally, it can be viewed as an iterative game between a learner and the environment. In each round \(t\), the learner makes a decision \(_{t}\) from a convex set \(^{d}\) and then suffers a loss \(_{t}(_{t})\) in the form of

\[_{t}()=f_{t}()+r(),\] (1)

where \(f_{t}():\) denotes the time-varying function that is chosen by the environment, and \(r():\) denotes the fixed non-smooth and convex regularizer, such as the \(_{1}\)-norm for sparse vectors and the trace norm for low-rank matrices (Langford et al., 2009; Flammari and Bach, 2017; Zhang et al., 2019; Garber and Kaplan, 2019). In the literature, online composite optimization is generally divided into two categories: stochastic composite optimization (Lan, 2012, 2016; Zhang et al., 2017; Lei and Tang, 2018; Lei et al., 2019; Kulunchakov and Mairal, 2019) and adversarial composite optimization (Xiao, 2009; Duchi et al., 2010; Mohri and Yang, 2016; Joulani et al., 2020; Yang et al., 2024). In the former one, \(f_{t}()\) is assumed to be independent and identically distributed (i.i.d.) over time; in the latter one, \(f_{t}()\) can be chosen arbitrarily or even adversarially. However,the environment in real-world scenarios is seldom purely stochastic or adversarial, but rather falls somewhere in between (Amir et al., 2020; Garber et al., 2020; Sherman et al., 2021; Zimmert and Seldin, 2021; Ito, 2021), and our understanding for the more common intermediate scenarios remains limited in online composite optimization.

Recently, Sachs et al. (2022) introduce the Stochastically Extended Adversarial (SEA) model as an intermediate setting in online optimization without regularizer, i.e., \(r()=0\). In the SEA model, loss functions are not restricted to fully i.i.d. or adversarial; instead, the environment selects them from any intermediate state between the two extreme settings. To reflect how stochastic or adversarial the environments are, they introduce the cumulative stochastic variance \(^{2}_{1:T}\) and the cumulative adversarial variation \(^{2}_{1:T}\), as shown below:

\[^{2}_{1:T}=[_{t=1}^{T}^{2}_{t}] ^{2}_{1:T}=[_{t=1}^{T}_{ }\| F_{t}()- F_{t-1}()\|^{2}_{2}],\] (2)

where \(^{2}_{t}=_{}_{f_{t}_{t}}[\| f_{t}()- F_{t}()\|^{2}_{2}]\) denotes the variance of gradients and \(F_{t}()=_{f_{t}_{t}}[f_{t}()]\) denotes the expected function for \(f_{t}()\). For the SEA model, two classical algorithms in optimistic online learning (Rakhlin and Sridharan, 2013)--optimistic Follow-The-Regularized Leader (FTRL) (Sachs et al., 2022) and optimistic Online Mirror Descent (OMD) (Chen et al., 2023)--have been proven ensuring sublinear regret bounds with respect to both \(^{2}_{1:T}\) and \(^{2}_{1:T}\).

In this paper, we extend the SEA model into online composite optimization, termed as composite SEA, to bridge the gap between stochastic and adversarial composite optimization. Specifically, in (1), \(r()\) remains fixed over time and \(f_{t}()\) is selected from a distribution \(_{t}\), which is chosen by the environments either stochastically, adversarially, or in a manner that interpolates between the two extremes. The goal of composite SEA is to minimize the expected regret in terms of (1):

\[[_{T}]=[_{t= 1}^{T}[f_{t}(_{t})+r(_{t})]-_{ }_{t=1}^{T}[f_{t}()+r() ]],\] (3)

which benchmarks the cumulative composite loss of the learner and that of the best fixed decision. To handle the composite setting, a natural impulse is to treat \(f_{t}()+r()\) as one function and directly apply existing methods for the SEA model, i.e., optimistic FTRL and optimistic OMD. However, such a straightforward application is unsuitable because (i) these methods heavily rely on the smoothness of loss functions, but due to the non-smooth component \(r()\), the summation function \(_{t}()=f_{t}()+r()\) loses this crucial property; (ii) directly applying these methods ignores the presence of \(r()\) and thus fails to gain the benefits from the regularizer, e.g., the sparsity induced by the \(_{1}\)-norm. Therefore, a natural question arises _whether it is possible to deal with both the intermediate nature of environments and the composite structure of loss functions concurrently._

We affirmatively answer the above question by revisiting a variant of optimistic OMD (Scroccaro et al., 2023), named Optimistic Composite Mirror Descent (OptCMD). This method inherits the idea of optimistic online learning (Rakhlin and Sridharan, 2013), i.e., exploiting estimates on upcoming loss functions for decision updates, and can disentangle \(f_{t}()\) and \(r()\) during the optimization, thereby effectively leveraging distinct properties of each. However, OptCMD is originally designed for adversarial composite optimization, and a simple extension will lead to unsatisfactory bounds in composite SEA. In this paper, we reanalyze OptCMD and show that with suitable configurations, OptCMD attains \((_{1:T}}+_{1:T}})\), \(((^{2}_{}+^{2}_{})(^{2}_{1:T}+ ^{2}_{1:T}))\) and \((d(^{2}_{1:T}+^{2}_{1:T}))\) bounds for smooth and general convex, smooth and strongly convex, and smooth and exp-concave time-varying functions, respectively. Our findings generalize previous results in stochastic and adversarial composite optimization, and can reduce to them by specializing \(^{2}_{1:T}\) and \(^{2}_{1:T}\). Moreover, our results coincide with existing bounds for the SEA model (Sachs et al., 2022; Chen et al., 2023), which indicates that there is _no price_ in regret bounds for the benefits from \(r()\).

One concern of OptCMD is the requirement of the function type in advance, which is often impractical in real-world problems and motivates us to design a _universal_ algorithm that is agnostic to the prior knowledge about loss functions. We note that recently, Yan et al. (2023) have introduced a universal algorithm for the SEA model without regularizer, based on the meta-expert structure in online learning (van Erven and Koolen, 2016; Mhammedi et al., 2019; Wang et al., 2019; Zhang et al., 2022). However, their method (Yan et al., 2023) cannot naturally support the composite setting, as it highly depends on the smoothness of the (summation) loss functions, which does not necessarily hold in the composite SEA model. In this paper, we propose a _novel_ universal algorithm for composite SEA,based on the observations that (i) the regularizer remains fixed over time and is accessible to the meta-algorithm from the beginning round; (ii) the meta-algorithm is able to obtain expert decisions for the current round before estimating their performance. With these two facts, we can utilize the information about \(r()\) to track experts, ensuring the meta-regret will not deviate from that of the best one. Specifically, inspired by Yan et al. (2023), we employ a two-layer Multi-scale Multiplicative-weight with Correction (MsMwC) (Chen et al., 2021) as the meta-algorithm, and choose OptCMD as the expert-algorithm. To effectively estimate the expert performance, we first collect decisions of each expert for the current round, and then explicitly integrate them and expert performance on \(r()\) into the losses and optimisms used in MsMwC. Theoretical analysis demonstrates that, with the proposed integration, our universal algorithm can achieve desirable bounds for three types of loss functions simultaneously. We summarize our contributions as shown below.

* We first introduce the composite SEA model, which serves as an intermediate setting between stochastic and adversarial composite optimization and can naturally adapt to two extreme settings;
* For the composite SEA model, we demonstrate that OptCMD can attain the regret bounds of \((^{2}}+^{2}})\), \(((_{}^{2}+_{}^{2})(_{1:T}^{2}+ _{1:T}^{2}))\) and \((d(_{1:T}^{2}+_{1:T}^{2}))\) for smooth and general convex, smooth and strongly convex, and smooth and exp-concave time-varying functions, respectively. Owing to the versatility of \(_{1:T}^{2}\) and \(_{1:T}^{2}\), these bounds can _recover_ previous results in stochastic and adversarial composite optimization;
* Moreover, to handle the unknown function type, we propose a _new_ universal algorithm in composite SEA and show that it can concurrently achieve desirable bounds for three kinds of functions;
* Finally, we discuss implications of our theoretical findings for two common intermediate examples with composite loss functions, and derive favorable bounds specific to these practical scenarios.

## 2 Related work

In this section, we briefly review related work on adversarial and stochastic composite optimization, the SEA model and universal online learning.

**Adversarial composite optimization.** In this setting, \(f_{t}()\) is assumed to be chosen by the environments arbitrarily or even adversarially, and the performance is measured by _regret_ shown in brackets of (3). In the literature, the seminal work (Duchi and Singer, 2009) proposes the FOBOS method with \(()\) and \((^{-1} T)\) regret bounds for general convex and \(\)-strongly convex \(f_{t}()\), respectively. Later, Xiao (2009) proposes the RDA method based on the primal-dual subgradient framework (Nesterov, 2009), and shows that RDA is able to generate sparser decisions than FOBOS when \(r()=\|\|_{1}\) for some \(>0\). In the same time, another subsequent work (Duchi et al., 2010) introduces the COMID method under the mirror descent framework (Beck and Teboulle, 2003), and achieves the same bounds as FOBOS. Recently, for \(\)-exp-concave \(f_{t}()\), Yang et al. (2024) establish an \(((d/) T)\) regret bound by proposing the ProxONS method.

Besides, there exist other powerful methods (Yang et al., 2014; Mohri and Yang, 2016; Joulani et al., 2020; Scroccaro et al., 2023) equipped with problem-dependent bounds. These bounds can safeguard the above results in the worst-case scenarios and become tighter when environments have special properties, such as smoothness. Among them, the most related work is by Scroccaro et al. (2023), who develop OptCMD based on the optimistic online learning framework (Rakhlin and Sridharan, 2013). The key idea is to make an estimate for the upcoming loss function, which ensures tighter bounds when the estimate is accurate and still maintains the worst-case bound otherwise. By utilizing the smoothness of \(f_{t}()\), OptCMD achieves \((})\) and \((^{-1} V_{T})\) regret bounds for general convex and \(\)-strongly convex \(f_{t}()\), respectively, where \(V_{T}=_{t=1}^{T}_{}\| f_{t}() - f_{t-1}()\|_{2}^{2}\) denotes the gradient-variation and can be small when \(f_{t}()\) gradually changes.

**Stochastic composite optimization.** In this setting, \(f_{t}()\) is assumed to be i.i.d. sampled from a fixed distribution \(\), and the goal is to minimize the composite objective: \(_{}_{f}[f()]+ r()\). The performance is measured by _excess risk_, which compares the solution with the optimal one, i.e., \(_{f}[f(_{T})]+r(_{T})-_{ }\{_{f}[f()]+r( )\}\).

In the literature, there are a substantial body of methods designed for the stochastic setting (Ghadimi and Lan, 2012; Lan, 2012, 2016; Lei and Tang, 2018; Lei et al., 2019), and by utilizing the online-to-batch conversion technique (Cesa-Bianchi et al., 2004), existing methods for adversarial composite optimization (Xiao, 2009; Duchi et al., 2010; Yang et al., 2024) can also be extended to the stochastic scenarios. Specifically, Lan (2012) first establishes an \((1/)\) excess risk bound for general convex objective, and then Lan (2016) improves the rate to \((1/T)\) for strongly convex objective. By utilizing the online-to-batch conversion, both RDA (Xiao, 2009) and COMID (Duchi et al., 2010) achieve \((1/)\) and \(( T/( T))\) excess risks for general convex and \(\)-strongly convex objective, respectively. When \(f()\) is \(\)-exp-concave, ProxONS (Yang et al., 2024c) ensures an \((d T/( T))\) excess bound.

**SEA model.** The SEA model is originally introduced by Sachs et al. (2022) as an intermediate setting in online optimization without regularizer. Moreover, they also propose two versatile quantities \(^{2}_{1:T}\) and \(^{2}_{1:T}\) to reflect the stochastic and adversarial aspect of the environments, respectively. Theoretically, for the SEA model, Sachs et al. (2022) prove that optimistic FTRL enjoys the bounds of \((_{1:T}}+_{1:T}})\) and \((^{-1}(^{2}_{}+^{2}_{}) T)\) for smooth and general convex, and smooth and \(\)-strongly convex loss functions, respectively. Later, Chen et al. (2023) demonstrate that optimistic OMD is able to attain the same bound for general convex losses and an improved \((^{-1}(^{2}_{}+^{2}_{})(^{2}_ {1:T}+^{2}_{1:T}))\) bound for the strongly convex losses. Moreover, they also establish a new \(((d/)(^{2}_{1:T}+^{2}_{1:T}))\) bound for smooth and \(\)-exp-concave functions.

**Universal online learning.** The universal online learning is proposed to handle the uncertainty of the loss function types, when applying online algorithms to practical optimization problems. The center to universal algorithms is the powerful meta-expert structure (van Erven and Koolen, 2016; Mhammedi et al., 2019; van Erven et al., 2021), which is also widely-used in many other fields of online learning (Daniely et al., 2015; Jun et al., 2017, 2018, 2020, 2021; Cutkosky, 2020; Wan et al., 2021, 2022b; Wang et al., 2024; Wan et al., 2024a). The key idea of meta-expert structure is to maintain multiple experts to process different types of loss functions and then, deploy a meta-algorithm to combine the decisions from experts (Wang et al., 2019, 2020; van Erven et al., 2021; Zhang et al., 2022; Yang et al., 2024a,b). In the literature, the most related work is Yan et al. (2023), who propose a novel universal algorithm with a two-layer MSMWC (Chen et al., 2021) as the meta-algorithm. By maintaining multiple instances of optimistic OMD, their method can adapt to the SEA model and deliver the same bounds as those in Chen et al. (2023). Recently, Yan et al. (2024) further improve this method by employing a simpler meta-algorithm while achieving optimal regret bounds for three types of functions. However, it should be noticed that these methods (Yan et al., 2023, 2024) heavily relies on the smoothness of the summation function (1), and thus cannot handle the composite structure in loss functions well, as previously discussed.

## 3 Preliminaries

In this section, we introduce some preliminaries, including standard assumptions and a brief review of OptCMD (Scroccaro et al., 2023).

### Assumptions

We first list the common assumptions in prior studies (Duchi et al., 2010; Chen et al., 2023, 2024).

**Assumption 1**.: _The convex decision set \(\) belongs to an Euclidean ball with the diameter \(D\)._

**Assumption 2**.: _At each round \(t\), the random function \(f_{t}()\) is \(G\)-Lipschitz over \(\), i.e.,_

\[,,\ |f_{t}()-f_{t}( )| G\|-\|_{2}.\]

**Assumption 3**.: _The regularized function \(r()\) is convex and bounded over \(\), i.e.,_

\[,,\ r() r( )+ r()^{}(-)\ \ ,\ 0 r() C.\]

**Assumption 4**.: _At each round \(t\), the expected function \(F_{t}()\) is \(H\)-smooth over \(\), i.e.,_

\[,,\ \| F_{t}()-  F_{t}()\|_{2} H\|-\|_{2}.\]

**Assumption 5**.: _At each round \(t\), the expected function \(F_{t}()\) is convex over \(\), i.e.,_

\[,,\ F_{t}() F_{t}( )+ F_{t}()^{}(-).\]

**Assumption 6**.: _At each round \(t\), the expected function \(F_{t}()\) is \(\)-strongly convex over \(\), i.e.,_

\[,,\ F_{t}() F_{t}( )+ F_{t}()^{}(-)+(/2) \|-\|_{2}^{2}.\]

**Assumption 7**.: _All the variance of the gradients and the adversarial variations are bounded, i.e.,_

\[ t[T],\ _{t}^{2}_{}^{2}_{ }\| F_{t}()- F_{t-1}() \|_{2}^{2}_{}^{2}.\]

**Assumption 8**.: _At each round \(t\), the time-varying function \(f_{t}()\) is \(\)-exp-concave over \(\), i.e., \(\), \((- f_{t}())\) is concave._

### Optimistic composite mirror descent

OptCMD is an algorithmic realization of the powerful optimistic online learning (Rakhlin and Sridharan, 2013) in online composite optimization. Specifically, in each round \(t\), the learner submits a decision \(_{t}\) and suffers a loss \(f_{t}(_{t})+r(_{t})\). Then, the learner receives an optimism \(M_{t+1}\) that serves as an optimistic prediction for the gradient of subsequent function \(f_{t+1}()\). After that, the learner performs the following update steps:

\[}_{t+1}= *{argmin}_{}  f_{t}(_{t}),+r()+^{ _{t}}(,}_{t})}\] (4) \[_{t+1}= *{argmin}_{} M _{t+1},+r()+^{_{t+1}}( ,}_{t+1})}\] (5)

where \(^{_{t}}(,)=_{t}()-_{t}()-_{t}(), -\) denotes the Bregman divergence associated with a time-varying function \(_{t}\), of which the specific form depends on the type of \(f_{t}()\) and will be illuminated later. Note that in (4) and (5), \(r()\) remains non-linearlzed, which allows the decisions \(\{_{t}\}_{t=1}^{T}\) to possess properties externally conferred by \(r()\), such as the sparsity.

**Remark**.: Scroccaro et al. (2023) specialize the estimate \(M_{t+1}=_{t+1}(}_{t+1})\), where \(_{t+1}()\) denotes the function prediction of \(f_{t+1}()\) and is generally set as \(f_{t}()\) in practice. We hightlight that this specification is unsuitable for the composite SEA model and inadvertently introduces a dependency issue during the analysis. More detailed discussions can be found in Sections 4.1 and 4.2.

## 4 OptCMD for composite SEA

In this section, we provide our results for general convex, strongly convex and exp-concave cases in the composite SEA model. Due to space limitations, all the proofs are deferred to Appendix B.

### General convex case

Initially, we consider the case where the regularizer \(r()\) is convex and non-smooth, and the expected functions \(F_{t}()\) are general convex and smooth. We choose the following configuration for this case.

\[_{t}()=}\|\|_{2}^{2},\ _{t}=\{_{t-1}}},\}, M_{t+1}= f_{t}(_{t}),\] (6)

where \(_{t-1}=_{s=1}^{t-1}\| f_{s}(_{s})- f_{s-1}( _{s-1})\|_{2}^{2}\) and \(>0\) denotes the hyperparameter. With the configuration in (6), we establish the following regret bound for the general convex case.

**Theorem 1**.: _Under Assumptions 1, 2, 3, 4 and 5, Algorithm 1 ensures_

\[[_{T}]=(^{2}}+^{2}})\]

_with the configuration in (6) where \(=6HD\)._

**Remark.** For the fully adversarial environments where \(^{2}_{1:T}=V_{T}\) and \(^{2}_{1:T}=0\), our bound degenerates to \((})\) matching the previous result of Scroccaro et al. (2023). For the fully stochastic environments where \(^{2}_{1:T}=0\) and \(^{2}_{1:T}=^{2}T\) with the stochastic variance \(^{2}\), our result becomes \(()\) regret bound and further delivers an \((1/)\) excess risk bound by the online-to-batch conversion (Cesa-Bianchi et al., 2004), which coincides with the results of Lan (2012). Furthermore, our finding also aligns with existing bounds for the SEA model without \(r()\)(Sachs et al., 2022; Chen et al., 2023). In other words, our method pays _no price_ in regret bounds for handling the additional regularizer.

Beyond the configuration in (6), we also employ those adopted by Scroccaro et al. (2023):

\[_{t}()=}\|\|_{2}^{2},\ _{t}=+_{t-1}}},\ \ M_{t+1}= f_{t}(}_{t+1}),\] (7)

where \(_{0}=0\) and \(_{t-1}=_{s=1}^{t-1}\| f_{s}(}_{s})- f_{ s-1}(}_{s})\|_{2}^{2}\). By setting the configuration in (7), we obtain the following bound.

**Theorem 2**.: _Under Assumptions 1, 2, 3, 4 and 5, with configuration in (7), Algorithm 1 ensures_

\[[_{T}]=(^{2}_{1:T}}+_{1:T}}),\]

_where \(^{2}_{1:T}\) with \(^{2}_{t}=_{f_{t}_{t}}[_{}\| f_{t}()- F_{t}()\|_{2}^{2}]\)._

**Remark.** This bound is less favorable than that in Theorem 1, as it scales with a new quantity \(^{2}_{1:T}\), which also measures the stochasticity in composite SEA but is larger than \(^{2}_{1:T}\) because of the fact that \(_{f_{t}_{t}}[_{}\| f _{t}()- F_{t}()\|_{2}^{2}]_{ }_{f_{t}_{t}}[\| f_{t}()-  F_{t}()\|_{2}^{2}],\) where the inequality is due to the convexity of supremum operator. This discrepancy arises from the inappropriate choice of \(M_{t+1}= f_{t}(}_{t+1})\), where \(}_{t+1}\) is generated based on \( f_{t}(_{t})\) shown in (4), which implies that \(}_{t+1}\) has already incorporated partial information about \(f_{t}()\), leading to a dependence issue in analysis. To remove the dependency, we apply the supremum operator on \(}_{t}\) over \(\) (c.f. Lemma 4) but inevitably establish a reliance on \(^{2}_{1:T}\) in the final bound.

### Strongly convex case

In this part, we focus on the case where the regualizer \(r()\) is non-smooth and convex, and the expected functions \(F_{t}()\) are smooth and \(\)-strongly convex. We set OptCMD with the configurations:

\[_{t}()=}\|\|_{2}^{2},\ _{t}=,\ \ M_{t+1}= f_{t}(_{t}),\] (8)

where \(>0\) denotes the hyperparameter. The theoretical guarantee for this case is shown below.

**Theorem 3**.: _Under Assumptions 1, 2, 3, 4, 6 and 7, Algorithm 1 ensures_

\[[_{T}]=( (^{2}_{}+^{2}_{})(^{2}_{1:T}+ ^{2}_{1:T})).\]

_with the configuration in (8) where \(=128H^{2}D^{2}\)._

**Remark.** Similar to Theorem 1, this bound reduces to \((^{-1} V_{T})\) matching that of Scroccaro et al. (2023) when environments become fully adversarial, and derives the same \(( T/( T))\) excess risk bound as that of Duchi and Singer (2009) and Xiao (2009) when environments become fully stochastic. Moreover, this bound also recovers that of Chen et al. (2023) for the SEA model.

Additionally, we further equip OptCMD with the original configuration of Scroccaro et al. (2023):

\[_{t}()=}\|\|_{2}^{2},\ _{t}=+(/2G^{2})_{t-1}},\ \ M_{t+1}= f_{t}(}_{t+1}),\] (9)

and obtain the following theorem.

**Theorem 4**.: _Under Assumptions 1, 2, 3, 4 and 6, with the configuration in (9), Algorithm 1 ensures_

\[[_{T}]=(}{ }(^{2}_{1:T}+^{2}_{1:T})).\]

**Remark.** This bound suffers two limitations. Firstly, it depends on the unfavorable \(^{2}_{1:T}\), which is due to the improper estimation \(M_{t+1}\) in (9). Secondly, it scales with \((G^{2})\) due to the choice of \(_{t}\) in (9), which is not tighter than \((^{2}_{}+^{2}_{})\) in Theorem 3.

### Exp-concave case

We further investigate the exp-concave case where the regularizer \(r()\) is non-smooth and convex, and the individual functions \(f_{t}()\) are smooth and \(\)-exp-concave.

**Remark.** In this case, we require the exponential concavity on the individual function \(f_{t}()\) instead of the expected one \(F_{t}()\). This is due to the technical demand in analysis, and similar assumptions are also adopted by Chen et al. (2023) and Yang et al. (2024c).

In this case, we set OptCMD with the following configuration:

\[_{t}()=\|\|_{H_{t}}^{2},\;H_{t}=  I+}{2}I+_{s=1}^{t-1}h_{s}, \;\;M_{t+1}= f_{t}(_{t}),\] (10)

where \(>0\) denotes the hyperparameter, \(I\) denotes the \(d\)-dimension identity matrix, \(h_{t}= f_{t}(_{t}) f_{t}(_{t})^{}\) and \(=(1/2)\{1/(4GD),\}\). The regret for this case is stated below.

**Theorem 5**.: _Under Assumptions 1, 2, 3, 4 and 8, Algorithm 1 ensures_

\[[_{T}]=( (_{1:T}^{2}+_{1:T}^{2})).\]

_with the configuration in (10) where \(=1\)._

**Remark.** For the fully adversarial setting, this bound degenerates to \(((d/) V_{T})\), which is the _first_ problem-dependent bound for exp-concave time-varying functions in online composite optimization, and tighter than \(((d/) T)\) by Yang et al. (2024c) in benign environments. For the fully stochastic setting, our result reduces to \(((d/) T)\) regret bound and further implies the same excess risk of \((d T/( T))\) as that of Yang et al. (2024c) through the online-to-batch conversion. Furthermore, it also matches existing results for the SEA model (Chen et al., 2023).

## 5 The universal strategy

Although we have established favorable theoretical results in the composite SEA model, achieving these guarantees requires prior knowledge of the function type to set appropriate configurations, e.g., (6) for the general convex case. This requirement is often impractical in real-world scenarios and motivates us to design a universal strategy. In the previous study of Yan et al. (2023), based on the meta-expert framework (van Erven and Koolen, 2016; Wang et al., 2020; Zhang et al., 2022), they propose a multi-layer universal algorithm, which can adapt to the SEA model without regularizer by choosing optimistic OMD (Chen et al., 2023) as the expert-algorithm. Unfortunately, their method (Yan et al., 2023) requires the smoothness of the (summation) loss functions, which does not necessarily hold for (1), so that it cannot be directly applied to the composite SEA model. In this paper, we propose a _novel_ universal algorithm for composite SEA. For the expert-algorithm, we choose OptCMD due to its ability in handling three cases in composite SEA. For the meta-algorithm, we design a new method that is able to effectively manage the non-smooth component \(r()\) in (1) and can thus track experts according to their performances in the composite SEA model.

Our method is based on two key observations. Firstly, \(r()\) in (1) is fixed over time and available to the meta-algorithm from the beginning round. Secondly, the meta-algorithm can access expertdecisions for the current round before the performance estimation. Therefore, the information about \(r()\) can be introduced into the expert tracking process, securing that the regret of meta-algorithm will not deviate from that of the best expert. Specifically, inspired by Yan et al. (2023), we employ a two-layer meta-algorithm, with each layer running an MsMwC algorithm (Chen et al., 2021) that maintains two weight sequences \(\{_{t},}_{t}^{N}\}_{t[T]}\), and a group of losses \(_{t}=(l_{t}^{1},,l_{t}^{N})\) and optimisms \(_{t}=(m_{t}^{1},,m_{t}^{N})\) to measure the performance of \(N\) experts. To handle the composite loss, we explicitly incorporate \(r()\) into both \(_{t}\) and \(_{t}\). The incorporation serves two purposes: (i) it endows \(_{t}\) with a composite structure, facilitating the estimation of expert performance on composite losses; (ii) it utilizes the cancellation between \(_{t}\) and \(_{t}\) to eliminate the non-smooth component in loss functions, i.e., \(r()\), so that the meta-algorithm is able to leverage the smoothness of \(f_{t}()\). In the following, we describe our universal algorithm, which is also summarized in Algorithm 2.

**Meta-algorithm.** Overall, our algorithm comprises two components: a two-layer meta-algorithm and a set of experts, together forming a three-layer structure. Specifically, on the top layer, we run one MsMwC named \(_{}\), which maintains the weights \(_{K}\) assigned to \(K=( T)\) MsMwCs, named \(_{}\). On the middle layer, each \(_{}^{k}\) (\(k[K]\)) maintains the weights \(^{k}_{N}\) assigned to \(N=( T)\) experts. At the round \(t\), we receive the decision \(_{t}^{k,i}\) from each expert \(E^{k,i}\), and update the weights \(_{t}\) and \(_{t}^{k}\) according to:

\[_{t}=*{argmin}_{^{K}}\{< _{t},>+^{_{1}}(, }_{t})\},\;_{t}^{k}=*{argmin}_{ ^{N}}\{<_{t}^{k},>+^{_{2} }(,}_{t}^{k})\},\] (11)

where \(_{1}()=_{k}(q^{k} q^{k})/^{k}\) and \(_{2}()=_{i}(p^{i} p^{i})/^{k,i}\) denote the negative entropy functions, and \(_{t}^{k}\) and \(_{t}\) denote the optimism used by \(_{}^{k}\) and \(_{}\), respectively. Then, we compute the weighted average decision \(_{t}^{k}=_{i}p_{t}^{k,i}_{t}^{k,i}\) and submit \(_{t}=_{k}q_{t}^{k}_{t}^{k}\). Next, we suffer the loss \(f_{t}(_{t})+r(_{t})\) and observe the gradient \(_{t}= f_{t}(_{t})\). We update \(}_{t+1}\) and \(}_{t+1}^{k}\) by

\[}_{t+1}=*{argmin}_{^{K}}\{ <_{t}+_{t},>+^{_{1}}(,}_{t})\},}_{t+1}^{k}= *{argmin}_{^{N}}\{<_{t}^{k}+_{t}^{k},>+^{_{2}}(,}_{t}^{k})\},\] (12)

where \(_{t}=(l_{t}^{1},,l_{t}^{k})\) and \(_{t}^{k}=(l_{t}^{k,1},,l_{t}^{k,N})\) denote losses used by \(_{}\) and \(_{}^{k}\), respectively, and \(_{t}^{K}\) and \(_{t}^{k}^{N}\) denote the correction terms. At last, we send the gradient \(_{t}\) and the regularizer \(r()\) to each expert. Now, we specify the loss and optimism in (11) and (12). For \(_{}\), we choose

\[l_{t}^{k}=<_{t},_{t}^{k}>+r(_{t}^{k} )+_{1}\|_{t}^{k}-_{t-1}^{k}\|_{2}^{2},\;m_{t}^{k}= <}_{t}^{k},_{t}^{k}>+r(_{t}^{k})+ _{1}\|_{t}^{k}-_{t-1}^{k}\|_{2}^{2},\] (13)

where \(}_{t}^{k}=(_{t}^{k,1},,_{t}^{k,N})\) with \(_{t}^{k,i}=<_{t},_{t}>-<_{t- 1},_{t-1}-_{t-1}^{k,i}>\). For \(_{}^{k}\), we choose

\[l_{t}^{k,i}=<_{t},_{t}^{k,i}>+r(_{t}^{k,i})+_{2}\|_{t}^{k,i}-_{t-1}^{k,i}\|_{2}^{2},\;m_{t} ^{k,i}=_{t}^{k,i}+r(_{t}^{k,i})+_{2}\|_{t}^{k,i}-_{t-1}^{k,i}\|_{2}^{2}.\] (14)

In the above, \(_{1}\), \(_{2}\) denote hyperparameters, and \(_{1}\|_{t}^{k}-_{t-1}^{k}\|_{2}^{2}\) and \(_{2}\|_{t}^{k,i}-_{t-1}^{k,i}\|_{2}^{2}\) are injected for cancellation during the analysis (Yan et al., 2023). It should be noticed that we explicitly introduce \(r()\) in (13) and (14). This is because experts are running over the composite losses and their performance assessment should similarly reflect the composite structure, ensuring an accurate measure for expert tracking. Moreover, the optimism \(_{t}\) with \(r()\) helps eliminate the non-smooth component in \(_{t}\) and thereby enables the meta-algorithm to effectively utilize the smoothness of time-varying functions. To make it clearer, we take the general convex case as an example.

First, the expected regret can be decomposed into the meta-regret and the expert-regret, as below:

\[[_{T}]=[ _{t=1}^{T}[f_{t}(_{t})+r(_{t})]-_{ }_{t=1}^{T}[f_{t}()+r( )]]\] \[[_{t=1}^{T}_{t},_{t}-_{t}^{k^{*},i^{*}}+r(_{t})-r( _{t}^{k^{*},i^{*}})]}_{:=}+[_{t=1}^{T}_{t},_{t}^{k^{*},i^{*}} -^{*}+r(_{t}^{k^{*},i^{*}})-r(^{*}) ]}_{:=},\]

where \(_{t}^{k^{*},i^{*}}\) denotes the decision made by the best expert \(E^{k^{*},i^{*}}\). For the expert-regret, we can directly apply the result in Theorem 1. Therefore, the key is how to bound the meta-regret. By the fact that \(r(_{t})_{k}q_{t}^{k}r(_{t}^{k})\) and \(r(_{t}^{k})_{i}p_{t}^{k,i}r(_{t}^{k,i})\), the meta-regret is bounded by

\[ the first and second terms denote the regrets for tracking experts of \(_{}\) and \(_{}^{k^{*}}\), respectively. While these two terms involve the losses \(l_{t}^{k}\) and \(l_{t}^{k^{*},i}\) that incorporate the regularizer, they can be simultaneously controlled by a favorable bound of \((_{t[T]}(l_{t}^{k^{*},i^{*}}-m_{t}^{k^{*},i^{*}})^{2})\), which not only eliminates \(r()\) by the cancellation between \(l_{t}^{k^{*},i^{*}}\) and \(m_{t}^{k^{*},i^{*}}\), but also suffices to achieve the desirable bound of \(}(^{2}}+^{2}})\) for general convex case and \((1)\) for other two cases. More details can be found in Appendix B.6.

**Experts.** Each expert is an instance of OptCMD that equips with certain configuration, i.e., (6), (8) or (10). To estimate the unknown curvature \(\) and \(\), we utilize the discretization strategy by Zhang et al. (2022), constructing the candidate sets as

\[_{str}=_{exp}=\{1/T,2/T,2^{2}/T,,2^{N}/T\},\]

where \(N=_{2}T\). Based on the two sets, we create following three types of experts:

* Strongly convex experts, each of which is configured with (8) and a candidate \(^{i}_{str}\);
* Exp-concave experts, each of which is configured with (10) and a candidate \(^{i}_{exp}\);
* General convex expert, which is configured with (6).

As demonstrated by Zhang et al. (2022), these experts are sufficient to identify the best one that runs for the true function type with the most accurate curvature. For instance, in the strongly convex case, there must exist an expert \(E^{i}\) equipped with \(^{i}_{str}\) that satisfies \(^{i} 2^{i}\). Moreover, instead of directly minimizing the original loss function (1), each expert runs over a _new_ composite surrogate loss to avoid high-computational gradient query costs. Specifically, we choose \(h_{t}^{c}()=_{t},+r( )\), \(h_{t,i}^{sc}()=_{t},+r ()+^{i}\|-_{t}\|_{2}^{2}/2\) and \(h_{t,i}^{exp}()=_{t},+ r()+^{i}_{t},-_{t} ^{2}/2\) for general convex, strongly convex, and exp-concave experts, respectively. These surrogate losses not only inherit the composite structure and properties of \(f_{t}()\), e.g., the strongly convexity, but they also require only _one_ gradient query for \(_{t}= f_{t}(_{t})\) in each round. The theoretical guarantees of our universal algorithm are presented below.

**Theorem 6**.: _Under Assumptions 1, 2, 3, 4 and 7, with proper hyperparameters, Algorithm 2 ensures the bounds of \(}(^{2}}+^{2}})\), \(((_{}^{2}+_{}^{2})(_{1:T}^{2}+ _{1:T}^{2}))\), and \((d(_{1:T}^{2}+_{1:T}^{2}))\) for general convex, strongly convex, and exp-concave time-varying functions, respectively._

**Remark**.: Theorem 6 indicates that Algorithm 2 can attain comparable regret bounds to those of OptCMD for the three cases, without knowing the function type and curvature in advance.

## 6 Implications

In the following, we discuss implications of our results for two common intermediate examples.

**Composite adversarially corrupted stochastic data.** We first focus on the composite adversarially corrupted stochastic model, of which a special case without regularizer has been widely investigated in learning with expert advice (Amir et al., 2020), bandits (Zimmert and Seldin, 2021, Ito, 2021), and online optimization (Sachs et al., 2022; Chen et al., 2023). Specifically, each loss function consists of three terms: \(_{t}()=h_{t}()+c_{t}()+r()\), where \(h_{t}()\) denotes the loss of i.i.d. data from a fixed distribution \(\) with the variance \(\), and \(c_{t}()\) denotes an adversarial perturbation measured by a parameter \(C_{T}>0\), satisfying \(_{t[T]}_{} c_{t}() _{2} C_{T}\), and \(r()\) denotes the regularizer. In this model, the stochasticity and adversariality come from \(h_{t}()\) and \(c_{t}()\), respectively. Therefore, \(_{1:T}^{2}\) and \(_{1:T}^{2}\) can be formulated as

\[_{1:T}^{2}=^{2}T,\ _{1:T}^{2}=[_{t=1 }^{T}_{} c_{t}()- c _{t-1}()_{2}^{2}] 4GC_{T}.\] (16)

With the above specification, our theoretical results can naturally be extended to this scenarios, delivering the following corollary.

**Corollary 1**.: _With the specification (16) in the scenarios of composite adversarially corrupted stochastic data, we can obtain an \((+})\) bound for the general convex case by Theorem 1, an \(((^{2}T+C_{T}))\) bound for the strongly convex case by Theorem 3, and an \((d(^{2}T+C_{T}))\) bound for the exp-concave case by Theorem 5._

**Adversarial online learning with limited resources.** We then consider the another common scenarios where loss functions arrive in batches and the available computing resources are insufficient to process them all (Bottou and Cun, 2003; Benczur et al., 2018; Li et al., 2024; Zhou, 2024). Specifically, in each round \(t\), we receive a group of functions \(_{t}=\{f_{t}(,i)\}_{i[K_{t}]}\) with the size \(K_{t}\), each of which is selected by the environments adversarially. We denote by \(F_{t}()=K_{t}^{-1}_{i[K_{t}]}f_{t}(,i)\) the average function. Due to limited computing resources, we can only sample a subset of functions for gradient estimation, and generate sparse decisions for efficient storage and inference. For this reason, our goal is to minimize \(_{t}()=h_{t}()+r()\), where \(h_{t}()=B_{t}^{-1}_{i[B_{t}]}_{t}(,i)\) denotes the approximation of \(F_{t}()\) by i.i.d. sampling \(B_{t}[K_{t}]\) functions \(_{t}(,i)\) from the group \(_{t}\), and \(r()=\|\|_{1}\) denotes the \(_{1}\)-norm regularizer. In this scenarios, it can be verified that

\[_{1:T}^{2}=_{t=1}^{T}_{}_{_{t}_{t}}[\|B_{t}^{-1}_{i=1}^{B_{t} }_{t}(,i)- F_{t}()\|_{2}^{2}]  4G^{2}_{t=1}^{T}B_{t}^{-1}\] (17)

and \(_{1:T}^{2}=[_{t=1}^{T}_{}\|  F_{t}()- F_{t-1}()\|_{2}^{2}]\). Applying the above specification into our theorems delivers the following corollary.

**Corollary 2**.: _With the specification (17) in the scenarios of composite adversarially corrupted stochastic data, we can obtain an \((B_{t}^{-1}}+^{2}})\) bound for the general convex case by Theorem 1, an \(((_{t[T]}B_{t}^{-1}+_{1:T}^{2}))\) bound for the strongly convex case by Theorem 3, and an \((d(_{t[T]}B_{t}^{-1}+_{1:T}^{2}))\) bound for the exp-concave case by Theorem 5._

## 7 Conclusion and future work

In this paper, we investigate the intermediate setting between stochastic and adversarial composite optimization, named composite SEA, and demonstrate that OptCMD is able to attain the regret bounds of \((^{2}}+^{2}})\), \(((_{}^{2}+_{}^{2})(_{1:T}^{2}+_ {1:T}^{2}))\) and \((d(_{1:T}^{2}+_{1:T}^{2}))\) for the general convex, strongly convex and exp-concave cases, respectively. To deal with the unknown function type in real-world problems, we further propose a _novel_ universal algorithm in online composite optimization, and show that our universal algorithm is able to achieve the desirable bounds in the three cases, simultaneously. Due to the versatility of \(_{1:T}^{2}\) and \(_{1:T}^{2}\), all our theoretical findings can _recover_ previous results in fully stochastic and adversarial composite optimization. Finally, we explore several practical intermediate scenarios to demonstrate the implications of our results. Additionally, we also conduct empirical studies in Appendix A to verify our theoretical results.

There are many valuable directions for future research. First, our methods still rely on the domain and gradient bounded assumptions. We notice that there have been several methods designed for unbounded cases (Orabona, 2014; Orabona and Pal, 2016; Cutkosky and Orabona, 2018; Jacobsen and Cutkosky, 2022), but all of them focus on the setting without the regularizer. Consequently, developing online algorithms with the unbounded domain and gradient for composite SEA remains an interesting research direction. Second, in the composite SEA model, we implicitly assume that the feedback (i.e., the loss function value and the gradient) is immediately revealed after making the decision, which, however, is not necessarily satisfied in practice (Joulani et al., 2013; Quanrud and Khashabi, 2015; Wan et al., 2022a, c, 2024b). Therefore, it is also interesting to explore the composite SEA model with the delayed feedback. Thirdly, our proposed universal algorithm currently exhibits a three-layer structure, which presents several challenges in both analysis and practical implementation. Therefore, designing a simpler two-layer universal algorithm in composite SEA is another potential research direction in the future, which may require the advanced techniques in Yan et al. (2024).