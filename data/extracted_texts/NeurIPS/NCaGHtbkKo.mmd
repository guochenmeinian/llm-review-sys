# Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions

Rawal Khirodkar\({}^{*}\), Jyun-Ting Song\({}^{*}\), Jinkun Cao, Zhengyi Luo, Kris Kitani

Carnegie Mellon University

Equal contribution

###### Abstract

Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA, and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of \(1.66\) million images and \(3.32\) million human instances from more than \(20\) synchronized cameras with \(208\) video sequences spanning diverse environments and \(24\) unique subjects. We rigorously evaluate existing state-of-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of \(54.8\)% PVE in scenes with severe occlusion and contact. Code and data are available at https://jyuntins.github.io/harmony4D/.

"_Harmony_--a cohesive alignment of human behaviors."

## 1 Introduction

As social beings, humans frequently interact with each other using physical touch [73; 64; 35]. By studying these interactions, one can potentially unravel various aspects of human behavior, including emotions , intentions , and dynamics . As with most problems in computer vision , a first step in modeling contact interactions involves building large-scale 3D multi-human datasets. Many such datasets [15; 74; 20; 80; 87; 57] have emerged in recent years. However, similar to most existing single-human datasets , contact interaction datasets lack subject and environment diversity and are captured under controlled indoor conditions with choreographed activities. Learning-based methods [42; 70; 43; 38] trained on such biased benchmarks struggle to generalize to real-world conditions. The core issue is that recovering high-quality ground-truth mesh for scenarios with frequent human-human contact is challenging due to severe occlusion, truncation, and dynamic movements . Existing methods typically rely on extensive RGBD motion capture systems  or a large number of high-end wired camera systems (over 100)  to achieve accurate annotations. This reliance on extensive static capture systems makes in-the-wild data collection impractical . Therefore, we ask: can we develop a markerless capture system that uses only a few cameras, is mobile, and is capable of accurately extracting 3D ground truth for in-the-wild scenarios involving contact interactions? To tackle this challenge, we introduce the _Harmony4D_ dataset.

Harmony4D is a novel dataset featuring high-resolution videos of dynamic activities with contact interactions such as wrestling, dancing, karate, MMA and fencing. In contrast to previous datasets, Harmony4D is collected in the wild with a specific focus on subject and environment diversity. Table 1 compares our dataset with existing 3D human datasets focusing on contact interactions. Harmony4D is a substantially large dataset, consisting of \(1.66\) million images captured from more than \(20\) synchronized cameras, resulting in \(3.32\) million visible human instances. Specifically, we provide comprehensive ground-truth annotations such as camera parameters, 2D bounding boxes , human tracking identities , 2D/3D human poses , and 3D human meshes . Figure 1 provides an overview of the capture setup and annotation processing. The multi-camera setup is inspired by EgoHumans , utilizing Meta's Aria glasses , which feature an RGB and two greyscale cameras for the subject's first-person view, along with stationary RGB cameras for the third-person view. This combination allows us to accurately track and triangulate poses in 3D for extended periods without using visual markers  or additional sensors . To our knowledge, Harmony4D is the only in-the-wild video dataset with dynamic activities and contact interactions.

Our annotation procedure minimizes the need for manual supervision. We divide any input multi-view video sequence into two stages: (i) pre-contact and (ii) post-contact. The pre-contact stage refers to the time interval before the first physical interaction between the subjects. We utilize an existing pose extraction algorithm  to obtain 3D poses during the pre-contact stage. However, existing methods face significant challenges in post-contact scenarios, primarily due to severe occlusion, truncation, and joint ambiguity when subjects are in very close proximity (e.g., during wrestling or dancing). For the challenging post-contact stage, we propose a novel algorithm that uses instance segmentation , segmentation-conditioned 2D pose estimation , and 3D pose forecasting  in a temporal feedback loop to accurately track 3D poses. Our key idea is to use segmentation-conditioned 2D pose estimation, see (c) in Figure 1, to reason about missing or completely hidden body parts and disambiguate between multiple human joints. Finally, we build an efficient multi-stage motion capture pipeline to fit the SMPL  body model to the 3D human skeletons, incorporating optimization to minimize mesh interpenetration.

  
**Dataset** & **In-The-Wild** & **Scenes** & **Subjects** & **Cameras** & **Images** & **Instances** & **Mesh** \\  ShakeFive2  & ✗ & 1 & 6 & 1 & 34K & 68K & ✗ \\ MuPoTs-3D  & ✗ & 3 & 8 & 8 & 8K & 22K & ✗ \\ MultiHuman  & ✗ & 1 & 8 & 6 & 32K & 69K & ✓ \\ ExPI  & ✗ & 1 & 4 & 68 & 1.9M & 3.8M & ✗ \\ CHI3D  & ✗ & 1 & 10 & 4 & 315K & 728K & ✓ \\ Hi4D  & ✗ & 1 & 40 & 8 & 88K & 176K & ✓ \\
**Harmony4D (Ours)** & ✓ & 5 & 24 & 20 & 1.66M & 3.32M & ✓ \\   

Table 1: **Comparison with existing 3D datasets with multi-human interactions.**_Subjects_ and _Scenes_ are number of unique subjects and capture environments. _Cameras_ are number of stationary views per sequence. _Images_ are number of images. _Instances_ are number of visible human instances.

Figure 1: **Overview of Harmony4D setup**. (a) Multiple synchronized and calibrated cameras capture the contact interaction in _wrestling_. (b) We align all cameras into a gravity-aligned metric world coordinate system. (c) Our processing obtains per-view instance segmentation masks along with 3D keypoints. (d) Reconstructed ground-truth meshes after multi-step collision optimization.

The extensive scale and diverse scenarios of the Harmony4D dataset enable a thorough evaluation and improvement of methods for human-human contact estimation. We specifically evaluate current techniques for human mesh regression, uncovering that existing methods often struggle with missing meshes, inaccurate pose predictions in the presence of occlusion, and handling the complexities of natural, unconstrained human interactions. Importantly, when we fine-tune off-the-shelf methods on our large training set, the fine-tuned methods generalize well to challenging contact interactions and even outperform specifically designed methods for human contact reasoning . Moreover, we observe significant improvements in vertex contact prediction and occlusion reasoning. This underscores the need for Harmony4D, a large-scale dataset and a robust evaluation benchmark for in-the-wild contact interactions. The limitation is not necessarily with the methods [42; 70; 48; 46], but with the need to expose these methods to more extensive data in these underrepresented scenarios.

Our contributions are summarized as follows.

* A novel method based on multi-view instance segmentation and 3D human pose forecasting to extract 4D meshes of closely interacting humans.
* Harmony4D, a large-scale in-the-wild dataset with millions of multi-view images, parametric body models and vertex-level contact for dynamic and unchoreographed activities.
* Evaluation of existing state-of-the-art methods for monocular mesh regression, emphasizing their fundamental limitations in handling contact interactions, and demonstrating significantly improved performance when fine-tuned on our dataset.

## 2 Related Work

**Limited 3D Mesh Recovery Datasets.** Current progress in human vision research has been significantly driven by datasets [11; 76; 51; 28; 10; 16; 25; 45; 84]. However, unlike 2D pose datasets, 3D human mesh estimation datasets [28; 75; 56; 31; 32; 67; 79] are limited in diversity which significantly hampers the ability of deep models to generalize to the real world . Popular 3D datasets like Human3.6M , AMASS , HumanEva , AIST++ , HUMBI , PROX , and TotalCapture  only contain single human sequences. Multi-human datasets like PanopticStudio ,

Figure 2: **Dataset Composition.** Harmony4D consists of diverse, dynamic activities such as dancing, karate, MMA, and wrestling, all captured in in-the-wild settings.

MuCo-3DHP , TUM Shelf  are limited to indoor lab conditions. Outdoor multi-human datasets like 3DPW , MuPoTS , and EgoHumans  do not focus on human-human interactions. The Harmony4D dataset goes beyond existing 3D mesh datasets in meaningful ways, capturing in-the-wild activities with frequent multi-human contact and a particular focus on subject and scene diversity, as well as unchoreographed activities.

**Human Pose and Shape Estimation.** Most approaches [49; 42; 50; 46; 19; 12; 83; 33; 9; 38; 69] rely on the SMPL model , which provides a low-dimensional parametrization of the human body. HMR  employs a neural network to regress the parameters of an SMPL body model from a single image. Follow-up works like 4DHumans , Multi-HMR , WHAM , BEV , ROMP , and PARE  have improved the robustness of the original method by using more annotations, larger models, and auxiliary conditioning information such as camera parameters, segmentation masks, and 2D poses. However, most methods require "full-body" single human images , limiting their robustness in scenarios where multiple humans are interacting due to the limited representation in the training data. Recent works like BUDDI  build a diffusion model on top of BEV 's output as initialization to model the distribution of humans in proximity. Despite these advancements, we show that existing methods fail in scenarios with interacting humans in the Harmony4D dataset.

**Close Human Interaction Datasets.** Several contact-related datasets focus on human interactions with objects or static scenes [72; 4; 14; 22; 27; 71]. Additionally, recent datasets model close interactions between dynamic humans [80; 15; 74; 26; 31; 57; 20; 87]. ShakeFive2  and MuPoTS-3D  only provide 3D keypoints and lack mesh or body shape information. CHI3D  uses an indoor motion capture system to fit parametric human models of at most one actor at a time. MultiHuman  provides textured scans of interacting people but lacks ground-truth level body model registrations. ExPI  contains dynamic textured meshes in addition to 3D joint locations but misses body model registrations and contact information. Furthermore, ExPI  includes only two pairs of dance actors. The most related dataset to ours is Hi4D , which uses a multi-view RGBD capture system to capture the 4D volume of two subjects interacting with each other. Hi4D uses online tracking and optimization to extract per person scans from the joint scans and fit a parametric body model to them. However, Hi4D is limited to a single indoor capture location and lacks background diversity, consisting of choreographed activities. In contrast, Harmony4D is collected in in-the-wild settings with unchoreographed dynamic activities.

## 3 Harmony4D

This section describes the data collection setup and our proposed in-contact human mesh tracking system. Our objective is to develop a markerless annotation pipeline that accurately provides ground truth 3D human shapes and poses from videos, even in cases of severe occlusions and multiple human contacts, with minimal manual intervention. The proposed capture and mesh tracking system builds upon EgoHumans  and extends to work effectively under post-contact conditions, including significant occlusions and truncations.

Figure 3: **Data Distribution. The dynamic activities in the Harmony4D dataset cover all area for the SMPL body model. We visualize the most frequent body parts in contact during interactions as a normalized heatmap.**

Data Collection.We aim to capture dynamic activities with human-human contact under in-the-wild conditions such as wrestling, dancing, fencing, etc. Figure 2 and Figure 3 show the captured dynamic activities and depicts the data distributions of activities in the Harmony4D dataset. In comparison to previous datasets, our sequences are not restricted to indoor conditions and consist of realistic contact interactions with over \(1.6\) million images. Following EgoHumans , to obtain high-quality ground truth, our capture setup includes multiple views using \(20\) GoPro cameras, refer Figure 1 (b). The video resolution is set to 4K (\(3840 2160\)) and recorded at a rate of \(60\) frames per second (FPS). Importantly, the volume created by our cameras is portable and can be moved across locations. All cameras are synchronized to ensure temporal consistency across different views. Optionally, we also include Aria glasses  to provide the egocentric perspective of subjects. Each sequence consists of two subjects. All participants were briefed on the research project, provided informed consent following IRB guidelines, and received monetary compensation for their participation.

Camera Calibration.We determine the intrinsic and extrinsic parameters for all cameras using structure from motion (SfM)  for each sequence. The world coordinate system is scaled to be metric and gravity-aligned. To ensure the registration of all cameras in a single coordinate system using SfM, we pre-scan the environment externally with an additional camera. For contact sequences with Aria glasses , we also transform the camera coordinates of the ego-glasses into the stationary camera coordinate system using Procrustes alignment .

Pre-Contact Processing.We divide a multi-view video sequence into two parts: (i) pre-contact, the time interval before the first subject-to-subject contact, and (ii) post-contact, the period after the first contact. We leverage the multi-person 3D pose estimation method from EgoHumans  to obtain 3D poses in the pre-contact stage. Our pre-contact processing is efficient, parallelizing all time-steps, and works accurately since the subjects are completely visible from most camera views during this stage.

### Post-Contact Processing.

The main challenges in post-contact are detecting partially or completely occluded keypoints, associating these keypoints with the correct human identities, and ensuring that the estimated 3D human meshes remain spatially and temporally coherent while being consistent with the multi-view video evidence. To address this, we propose a novel sequential algorithm that leverages 3D human-pose forecasting, 2D point-conditioned instance segmentation, and mask-conditioned 2D pose estimation. Figure 4 provides an overview of Harmony4D post-contact processing.

Human Motion Model.To reason about occluded keypoints, we use 3D human pose forecasting with a human motion model based on Kalman filter (KF) . KF is a linear estimator for dynamical systems discretized in the time domain which only requires the state estimatio

Figure 4: **Overview of Harmony4D processing setup**. Given a multi-view RGB video sequence, we divide it into pre-contact and post-contact stages. We estimate per-subject 3D poses in the pre-contact stage  as initialization. The post-contact stage uses sequential processing involving 3D pose forecasting with a human motion model, per-view 2D point-conditioned instance segmentation, and mask-conditioned 2D pose estimation, followed by multi-view triangulation and mesh fitting.

steps to estimate the next time step target state. Specifically, we train a per-subject 3D motion model in a sliding window fashion over a history of \(T\) frames to predict the future 3D keypoint locations. The forecasting model is initialized using the pre-contact poses. Assuming \(J\) keypoints, we use \(J\) filters to model the 3D motion of each keypoint independently. Each KF's state \(\) is defined as \(=[x,y,z,,,]^{}\), where (\(x\), \(y\), \(z\)) are the 3D keypoint coordinates and (\(\), \(\), \(\)) are the velocities.

Instance Segmentation.We aim to infer the actual 3D keypoint locations at time step \(t+1\) from the forecasted ones, accounting for discrepancies due to sudden motions like tackling or jumping. Most current methods rely on per-view 2D pose estimation using a bounding box and then multi-view triangulation . However, bounding boxes are ambiguous  when subjects are close together, as seen in Figure 5 (_Left_). An off-the-shelf 2D pose estimator, given the subject bounding-boxes is unable to disentangle the subject poses accurately. Our key idea is to use instance segmentation masks as conditioning to a 2D pose estimator. We project the forecasted 3D poses to all views and apply the Segment-Anything model  using the target subject's 2D pose as positive points and others' poses as negative points. Our high frame rate processing ensures the forecasted 3D poses are close to the true ones, enabling reliable instance mask estimation across all views.

Segmentation Conditioned 2D Pose Estimation.We propose SegPose2D, a deep model for conditional 2D pose estimation, which takes both the RGB image patch and the binary segmentation mask as input to predict the 2D pose. SegPose2D uses the ViTPose  backbone with two transformer branches and feature fusion at multiple depths of the network. We train SegPose2D on the COCO  dataset using ground-truth segmentation masks. Figure 5 (_Right_) compares the mask-conditioned 2D pose estimation of SegPose2D with ViTPose  on a hugging sequence. The input mask is crucial for disentangling occluded human poses.

Multi-View Triangulation.Our triangulation setup follows the pre-contact processing . Let \(C\) represent all synchronized video streams with known projection matrices \(P_{c}\). We aim at estimating the global 3D pose \(_{j,t}^{3}\) of a fixed set of human keypoints indexed by \(j(1..J)\) at timestamp \(t(1..T)\) for all humans in the scene (omitting the human index for simplicity). Let \(_{c,j,t}^{2}\) be the \(j\)th 2D keypoint at time \(t\) from camera \(c\). To infer 3D poses from 2D estimates, we use a linear algebraic multi-view triangulation approach . Traditional triangulation assumes equal contribution from all 2D keypoints \(_{c,j,t}\), but some views may be unreliable due to occlusions or framing issues, degrading the results. We apply RANSAC to address this. For each time step \(t\), we solve: \(}_{j,t}\), \(A_{j,t}}_{j,t}=0\), where \(A_{j,t}^{2C 4}\) consists of components from the projection matrices and \(_{c,j,t}\) and \(C^{}\) is the cardinality of the camera inlier set post-RANSAC. Finally, we refine the 3D poses for the entire sequence using temporal smoothing, joint symmetry and bone constraints .

Mesh Optimization.Given the 3D pose estimates \(_{\{1..T\}}\) for all subjects in a video sequence, we fit a human mesh to these 3D pose sequences to obtain the in-contact vertex annotations. The human mesh is represented using body pose and shape parameters, \(=[_{},_{},_{}]\), where \(_{}^{23 6},_{} ^{10},_{}^{6}\). The pose parameters \(_{}\) are the 6D representation of the joint rotations  of the \(23\) body joints of the SMPL  body. The shape parameters \(_{}\) are the first \(10\) coefficients of the PCA shape space derived from CAESAR  scans. \(_{}\) consists of the global root orientation and translation of the body. Let \(:\) be a differentiable mapping function that projects SMPL parameters \(\) to corresponding 3D keypoints \(\). Similar to EgoHumans , we fit \(\) to the 3D pose trajectory by minimizing \(_{}\) defined as follows,

\[_{}()=w_{1}||-( )||_{2}+w_{2}||_{}||_{2}+w_{3}_{ }()+w_{4}_{} ()\\ +w_{5}_{}() +w_{6}_{}(_{})+w_{7}_{ }()\] (1)

Figure 5: (_Left_) Point conditioned instance segmentation. Projected 3D keypoints as positive or negative prompts. (_Right_) Comparison of ViTPose  with mask-conditioned 2D pose estimation.

where \(_{}\) is the constant limb length loss, \(_{}\) is the body symmetry loss, \(_{}\) is temporal smoothing, \(_{}\) is the Gaussian mixture shape prior loss , \(||_{}||_{2}\) penalizes hyper-extensions of joints, \(_{}\), inspired by Interdiff , prevents mesh self and interpretation and \(w_{1}...w_{7}\) are scalar weights. Compared to optimization in EgoHumans  where each subject mesh is fitted independently, the addition of the \(_{}\) is a crucial improvement for modeling in-the-wild multi-human contact scenarios. We define \(_{}()=_{i=1}^{N}_{j=1}^{V}_{i}(x_{j},y_{j},z_{j})\), where \(N\) is the number of subjects, and \(V\) is the number of mesh vertices. \(\) computes the negative penetration depth by \((x,y,z)=((x,y,z),0)\) where \((x,y,z)\) is the signed-distance field  at a vertex location \((x,y,z)\). This formulation ensures that the loss is positive when vertices penetrate themselves or other human meshes, thereby encouraging the separation of two human meshes as shown in Figure 6.

## 4 Experiments

In this section, we first describe the evaluation of mesh recovery methods on our dataset. Then, we showcase ablative experiments that highlight the quality of the annotations provided by the dataset.

### Implementation Details

We divide each sequence into shorter clips of at least 5 seconds at 20 FPS. The annotation per time step includes camera parameters, bounding boxes, person IDs, 2D/3D human poses, and 3D meshes per subject. All 3D poses at each time step are manually inspected and rectified in case of errors before performing mesh fitting. After mesh optimization, we manually verify and re-optimize each mesh sequence and contact vertices with custom hyperparameters if necessary. The number of keypoints \(J\) is set to 17 . The human motion model uses history of \(10\) frames. We use the Segment-Anything-H  backbone for instance segmentation. SegPose2D is based of the ViTPose-H  backbone and is trained on \(6\) A100 GPUs for \(5\) days on the COCO  dataset. We use CLIFF  to obtain initial SMPL estimates for mesh optimization. To compute the SDF for mesh interpenetration loss, we use a custom GPU implementation by Jiang et al.  along with Geman-McClure  error for robustness. Figure 8 provides a multi-view visualization of the ground-truth mesh during contact interaction. We visualize the vertices in contact in Figure 7. For more visualization, please refer to the supplemental.

### Benchmarking Mesh Recovery Methods

We evaluate existing state-of-the-art monocular mesh prediction methods such as PARE, HMR2.0, ROMP, BEV, Multi-HMR and BUDDI on the Harmony4D test set. For fairness, we use ground-truth bounding boxes as input to the top-down methods.

**Metrics.** We report standard metrics  such as MPJPE and PVE for mean joint and vertex errors, along with PA-MPJPE, PA-PVE, N-MPJPE, and N-PVE for their Procrustes-alignment and F1 score normalized variants, respectively. Additionally, we also report other metrics like 3DPCK, AUC . To measure interpenetration during multi-contact, we compute maximum point-to-surface distance (mP2S) in mm. Note, for BUDDI and Multi-HMR, which predict SMPL-X  parameters instead of SMPL, we follow BEDLAM  and convert predicted SMPL-X meshes to SMPL using a fixed vertex mapping \(^{10475 6890}\).

Figure 6: Mesh optimization with interpenetration loss.

Figure 7: Ground-truth contact vertices for _hugging_ sequence.

**Discussion.** In contrast to a general dataset like 3DPW , we observe that the MPJPE for baseline methods is much higher on the challenging Harmony4D test set. For instance, off-the-shelf Multi-HMR  reports an MPJPE of \(61.4\) mm on 3DPW compared to 93.8 mm on the Harmony4D benchmark. Notably, Multi-HMR, a bottom-up method, performs significantly well compared to other baselines, even outperforming BUDDI , which is specifically trained and designed to model human-human contact. Among the top-down methods, HMR2.0  performs the best with an MPJPE of 108.2 mm due to its large-scale training and the ViTPose  transformer backbone. Importantly, we also notice a very high normalized per vertex error (N-PVE) for all baselines, which is due to the failure in predicting a 3D consistent mesh in the presence of frequently occurring occlusions in our dataset. Harmony4D provides a unique and challenging evaluation benchmark for in-the-wild human contact interaction scenarios.

**Finetuning.** To demonstrate the utility of our dataset beyond serving as a challenging evaluation benchmark, we finetune HMR2.0  on our train set. The Harmony4D train set is significantly larger than existing general datasets, containing more than \(1.2\) million images. We use a training setup similar to HMR2.0  for finetuning. HMR2.0-finetuned demonstrates significant improvement across all metrics, see Table 2. It improves MPJPE by \(55.9\)%, MPVPE by \(54.8\)% and 3DPCK by \(21.01\)%. Interestingly, we do not supervise HMR2.0 specifically with contact vertex annotations; however, we observe that the inter-contact relationships are preserved in the predictions. Figure 9 provides a qualitative comparison on the test set between BUDDI , HMR2.0 , and HMR2.0-finetuned alongside the ground truth annotations. The results show that our train set can serve as an effective source for adapting existing monocular mesh estimation methods to multi-human interaction settings.

### Ablations

**Number of Cameras.** We investigate the impact of the number of cameras on the estimated 3D pose accuracy in our processing. Specifically, we uniformly sample \(6\) to \(18\) equidistant cameras from the camera perimeter and calculate the MPJPE between the resulting keypoints and our ground truth 3D keypoints obtained from all \(20\) cameras. Figure 10 shows the performance trend across various

  
**Method** & **MPJPE \(\)** & **PA-MPJPE \(\)** & **N-MPJPE \(\)** & **PVE \(\)** & **PA-PVE \(\)** & **N-PVE \(\)** & **3DPK\(\)** & **AUC \(\)** & **F1 \(\)** & **mP2S \(\)** \\  PAPE & 119.03 & 65.49 & 138.40 & 144.77 & 73.52 & 168.34 & 73.23 & 47.19 & 0.86 & N/A \\ ROMP & 121.13 & 80.23 & 134.59 & 161.12 & 91.82 & 179.02 & 68.47 & 44.70 & 0.90 & N/A \\ BEV & 111.29 & 78.04 & 119.66 & 144.28 & 90.13 & 155.14 & 74.63 & 48.94 & 0.93 & 134 \\ BUDDI & 126.35 & 84.00 & 133.00 & 158.72 & 95.95 & 167.06 & 70.28 & 45.75 & 0.95 & 106 \\ HMR2.0 & 108.18 & 60.25 & 109.28 & 131.00 & 67.96 & 132.32 & 75.62 & 49.79 & 0.99 & N/A \\ Multi-HMR & 93.82 & 58.53 & 101.98 & 115.79 & 67.67 & 125.85 & 83.57 & 55.08 & 0.92 & 53 \\
**HMR2.0-finetune** & **47.71 (+41.1)** & **32.75** & **48.18** & **59.14** & **38.93** & **59.74** & **96.63** & **75.75** & **0.99** & N/A \\   

Table 2: Comparison of monocular mesh recovery methods on the Harmony4D test set. mP2S metric is not reported for HMR2.0, PARE, and ROMP, as they predict each human instance in an independent coordinate system. Note, finetuning HMR2.0 improves performance significantly.

Figure 8: Harmony4D ground-truth mesh visualized from six camera views for the _hugging_ sequence.

activities in our dataset. We observe that increasing the number of cameras predictably improves performance, as more cameras imply a higher likelihood of a larger inlier set during RANSAC in triangulation. Interestingly, 16 cameras offer a fair trade-off between accuracy and processing speed.

**Interpenetration Loss.** We examine the impact of interpenetration loss on our mesh optimization. We evaluate using the following collision metrics: maximum point-to-surface distance (mP2S) in mm, maximum volumetric IoU (mIoU), and maximum intersection of area (mIoA) in \(m^{2}\). Table 3 demonstrates a significant improvement in all collision metrics across sequences. Specifically, on average, mP2S is reduced by \(44.7\) mm, mIoU is reduced by \(0.7\)%, and mIoA is reduced by \(0.1\)\(m^{2}\).

  
**Event** &  &  \\   & **mP2S** \(\) & **mIoU** \(\) & **mIoA** \(\) & **mP2S** \(\) & **mIoU** \(\) & **mIoA** \(\) \\  Hugging & 111.14 & 1.59 & 0.24 & 51.50 & 0.40 & 0.07 \\ Wrestling & 102.88 & 0.79 & 0.16 & 95.00 & 0.61 & 0.10 \\ Dancing & 114.91 & 1.41 & 0.18 & 65.60 & 0.79 & 0.12 \\ Karate & 113.70 & 0.94 & 0.16 & 44.97 & 0.25 & 0.06 \\ MMA & 122.01 & 2.26 & 0.31 & 83.99 & 1.43 & 0.18 \\   

Table 3: Adopting the interpenetration loss in mesh optimization significantly improves multi-human collision metrics across activities in the Harmony4D dataset.

Figure 9: **Qualitative comparison of mesh estimation methods. We evaluate all methods using ground-truth bounding boxes for fairness. HMR2.0 finetuned on the Harmony4D train set, demonstrates robustness to inter-person occlusion and improved 3D in-contact estimation.**

## 5 Conclusion

We propose a novel method to track, segment, and localize 4D body meshes of multiple people interacting in close range with frequent dynamic physical contact under in-the-wild conditions. Our key idea is to use multi-view segmentation-conditioned pose estimation, 3D motion models for forecasting, and collision optimization to obtain precise body model parameters. Using this method, we constructed the diverse Harmony4D dataset with ground-truth annotations for mesh recovery. Emphasis is placed on capturing unchoreographed, dynamic activities such as wrestling, dancing, karate, and MMA in the real world. Our evaluations show that existing state-of-the-art methods fail significantly under the challenging sequences of our dataset, mainly due to the lack of representation of human-human contact interactions in training. Importantly, fine-tuning baselines on our large training set improves mesh estimation performance in severe occlusion and contact conditions.

**Limitations.** We currently trade off the accuracy of 3D pose and shape estimation under contact interactions in favor of capturing in the wild. By adopting an optimization perspective from a dense and mobile camera rig, we enable markerless large-scale capture, complementing high-precision static indoor wired 3D capture systems with limited diversity.