# Learning Elastic Costs to Shape Monge Displacements

Michal Klein

Apple

michalk@apple.com

&Aram-Alexandre Pooladian

NYU

aram-alexandre.pooladian@nyu.edu

Pierre Ablin

Apple

p_ablin@apple.com

&Eugene Ndiaye

Apple

e_ndiaye@apple.com

&Jonathan Niles-Weed

NYU

jnw@cims.nyu.edu

&Marco Cuturi

Apple

cuturi@apple.com

###### Abstract

Given a source and a target probability measure, the Monge problem studies efficient ways to map the former onto the latter. This efficiency is quantified by defining a _cost_ function between source and target data. Such a cost is often set by default in the machine learning literature to the squared-Euclidean distance, \(_{2}^{2}(,):=\|-\|_{2}^ {2}\). The benefits of using _elastic_ costs, defined using a regularizer \(\) as \(c(,):=_{2}^{2}(,)+( -)\), was recently highlighted in Cuturi et al. (2023). Such costs shape the _displacements_ of Monge maps \(T\), namely the difference between a source point and its image \(T()-\), by giving them a structure that matches that of the proximal operator of \(\). In this work, we make two important contributions to the study of elastic costs: _(i)_ For any elastic cost, we propose a numerical method to compute Monge maps that are provably optimal. This provides a much-needed routine to create synthetic problems where the ground-truth OT map is known, by analogy to the Brenier theorem, which states that the gradient of any convex potential is always a valid Monge map for the \(_{2}^{2}\) cost; _(ii)_ We propose a loss to _learn_ the parameter \(\) of a parameterized regularizer \(_{}\), and apply it in the case where \(_{A}():=\|A^{}\|_{2}^{2}\). This regularizer promotes displacements that lie on a low-dimensional subspace of \(^{d}\), spanned by the \(p\) rows of \(A^{p d}\). We illustrate the soundness of our procedure on synthetic data, generated using our first contribution, in which we show near-perfect recovery of \(A\)'s subspace using only samples. We demonstrate the applicability of this method by showing predictive improvements on single-cell data tasks.

## 1 Introduction

Finding efficient ways to map a distribution of points onto another is a low-level task that plays a crucial role across many machine learning (ML) problems. Optimal transport (OT) theory Santambrogio (2015) has emerged as a tool of choice to solve such challenging matching problems, notably in single-cell genomics Schiebinger et al. (2019); Tong et al. (2020); Bunne et al. (2023, 2024); Klein et al. (2023). We focus in this work on the numerical resolution of the Monge problem, which aims, using high-dimensional source and target data samples \((_{1},,_{n})\) and \((_{1},,_{m})\), to recover a map \(T:^{d}^{d}\) that is simultaneously _(i) a pushfoward_ map, in the sense that \(T\) applied on source samples recovers the distribution of target samples; _(ii) efficient_, in the sense that \(T(_{i})\) is not too far, on average from \(_{i}\). The notion of efficiency can be made precise by choosing a real-valued cost function \(c\) that compares a point \(\) and its mapping via \(c(,T())\).

**Challenges in the estimation of OT maps.** When using standard cost functions such as \(_{2}^{2}(,):=\|-\|_{2}^ {2}\), the estimation of OT maps is hindered, in principle, by the curse of dimensionality Hutterand Rigollet, 2021). A simple workaround is to reduce the dimension of input data, using for instance a variational auto-encoder (Bunne et al., 2023), or learning hyperplane projections jointly with OT estimation (Paty and Cuturi, 2019; Niles-Weed and Rigollet, 2022; Lin et al., 2020; Huang et al., 2021; Lin et al., 2021). We consider in this work another approach, which explores alternative choices for ground cost \(c\). While OT theory is rife with rich cost structures (Ambrosio and Pratelli, 2003; Ma et al., 2005; Lee and Li, 2012; Figalli et al., 2010; Figalli and Rifford, 2010), that choice has comparatively received far less attention in machine learning, where for a vast majority of applications the cost function is often chosen as \(_{2}^{2}\) and sometimes \(_{2}\).

**Cost structure impacts map structure.** While the usage of Riemannian metrics within OT in ML has been considered (Cohen et al., 2021; Grange et al., 2023; Podadiani et al., 2023b), computational challenges restrict these approaches to low-dimensional manifolds. We argue in this work that costs that are translation invariant (TI), \(c(,) h(-)\) with \(h:^{d}\), can offer practitioners a reasonable middle ground, since many numerical schemes developed for the \(_{2}^{2}\) cost can be extended to TI costs, both for static and dynamic formulations of OT (see e.g., Villani et al. (2009, Chap.7) or Liu (2022)). In particular, we propose to focus on _elastic costs_ of the form \(h()=\|\|^{2}+()\), with \(>0\) and \(:^{d}\) a regularizer, following the name of the elastic net regularization (Zou and Hastie, 2005) proposed in the context of regression. Cuturi et al. (2023) show that using elastic costs in OT map estimation results in Monge maps whose displacements satisfy \(T()-=-_{} f()\), for some potential \(f\), and are therefore shaped by the _proximal operator_ of \(\).

**Contributions.** While elastic costs offer the promise of obtaining OT maps with prescribed structure inherited from the proximal operator of a regularizer \(\), our current understanding of how to use and exploit such costs is limited to the experimentation provided in (Cuturi et al., 2023). This stands in stark contrast with the fine-grained characterization provided by Brenier that a map is optimal for the \(_{2}^{2}\) cost if and only if it is the gradient of a convex potential. To this end:

* We show in SS 3 that OT maps can be generated for any elastic cost \(h\) by running a proximal gradient descent scheme, through the proximal operator of \(\), on a suitable objective. This results in, to our knowledge, the first visualization of Monge maps that extend beyond the usual grad-convex Brenier maps for \(_{2}^{2}\) costs (see Figure 1), as well as synthetic generation in high-dimensions;
* We introduce _subspace_ elastic costs in SS 4, which promote displacements occurring in a low-dimensional subspace spanned by the line vectors of a matrix \(A\), \(A^{p d}\), \(AA^{T}=I_{p}\), setting \(()\|A^{}\|_{2}^{2}\). We prove sample-complexity estimates for the Monge-Bregman-Occam (MBO) estimator introduced in (Cuturi et al., 2023) with this cost (and more generally Mahalanobis costs), and establish a link with the spiked transport model (Niles-Weed and Rigollet, 2022) when \(\)

Figure 1: Illustration of ground-truth optimal transport maps with different costs \(h\), for the same base function \(g\). In this experiment, \(g\) is the negative of a random ICNN with 2-dimensional inputs, 3 layers and hidden dimensions of sizes \(\). All plots display the level lines of \(g\). The optimal transport map \(T_{g}^{h}\) are recomputed four times using Prop. 1, with four different costs \(h\), displayed above each plot. _(left)_ When \(h\) is the usual \(_{2}^{2}\) cost, we observe a typical OT map that follows from each \(_{i}\), minus the gradient of \(g\). With the \(_{1}\) sparsity-inducing regularizer _(middle-left)_, we obtain sparse displacements: most arrows follow either of the two canonical axes, yet some points do not move at all. _(middle-right)_ This is slightly different when using the \(k\)-overlap norm, which exhibits less shrinkage. With a cost that penalizes displacements that are orthogonal to a vector \(\), we obtain displacements that push further to the bottom than in the _(left)_ plot, as in the _(right)_ plot, where displacements are almost parallel to \(\). When \(\) is not known beforehand, and both source and target samples are given, we present a procedure to learn adaptively such a parameter in ยง 5.

* Since the choice of the regularizer \(\) in the elastic cost gives rise to a diverse family of OT maps, whose structural properties are dictated by the choice of regularizer, we consider _parametrized families_\(_{}\), and propose in SS 5 a loss to select adaptively a suitable \(\).
* We illustrate all above results, showing MBO estimator performance, _recovery_ of \(A\) on the basis of i.i.d. samples, in both synthetic (using our first contribution) and single-cell data tasks, where we demonstrate an improved predictive ability compared to baseline estimators that do not learn \(A\).

## 2 Background: Optimal transport with Elastic Costs

Monge Problem.Let \(_{2}(^{d})\) be the set of probability measures with finite second-order moment. We consider in this work cost functions \(c\) of the form \(c(,) h(-)\), where \(h:^{d}\) is strictly convex and, to simplify a few computations, symmetric, i.e., \(h()=h(-)\). Given two measures \(,_{2}(^{d})\), the Monge problem (1781) seeks a map \(T:^{d}^{d}\) minimizing an average transport cost, as quantified by \(h\), of the form:

\[T^{}*{arg\,min}_{T_{}=}_{ ^{d}}h(-T())\,()\] (1)

Because the set of admissible maps \(T\) is not convex, solving (1) requires taking a detour that involves relaxing (1) into the so-called Kantorovich dual and semi-dual formulations, involving respectively two functions (or only one in the case of the semi-dual)(Santambrogio, 2015, SS1.6):

\[(f^{},g^{})*{arg\,max}_{f,g:^{d}\\ f()+g() h(-)}_{ ^{d}}f+_{^{d}}g\,= *{arg\,max}_{f:^{d},\\ fh}_{^{d}}f+_{ ^{d}}^{h}\] (2)

A function \(f\) is said to be \(h\)-concave if there exists a function \(g\) such that \(f\) is the \(h\)-transform of \(g\), i.e., \(f=^{h}\), where for any function \(g:^{d}\), we define its \(h\)-transform as

\[^{h}()*{inf}_{}h(- )-g().\] (3)

We recall a fundamental theorem in optimal transport (Santambrogio, 2015, SS1.3). Assuming the optimal, \(h\)-concave, potential for (2), \(f^{}\), is differentiable at \(_{0}\) (this turns out to be a mild assumption since \(f^{}\) is a.e. differentiable when \(h\) is), we have (Gangbo and McCann, 1996):

\[T^{}()=-( h)^{-1}( f^{}() )=- h^{} f^{}()\,,\] (4)

where the convex conjugate of \(h\) reads: \(h^{}()_{}, -h()\,\). The classic Brenier theorem (1991), which is by now a staple of OT estimation in machine learning (Korotin et al., 2019; Makkuva et al., 2020; Korotin et al., 2021; Bunne et al., 2023) through input-convex neural networks (Amos et al., 2017), is a particular example, stating for \(h=\|\|_{2}^{2}\), that \(T()=- f^{}(_{0})\), since in this case, \( h=( h)^{-1}=\), see (Santambrogio, 2015, Theorem 1.22).

Maps and Elastic Costs.Cuturi et al. (2023) consider TI costs w.r.t. a regularizer \(\): for \(>0\) they study _elastic costs_ of the form

\[h()=\|\|_{2}^{2}+(),\] (5)

and show that the resulting Monge map is shaped by the proximal operator of \(\):

\[T^{}()=-*{prox}_{} f ^{}()\,,*{prox}_{}() *{arg\,min}_{}\|-\|^{2}+ ()\,.\] (6)

The MBO Estimator.While the result above is theoretical, as it assumes knowledge of an optimal \(f^{}\), the Monge-Bregman-Occam (MBO) estimator proposes to plug into (6) an approximation of \(f^{}\), recovered from samples from \(\) and \(\). We write \(=[_{1},,_{n}]\) and \(=[_{1},,_{m}]\) for such samples in \(^{d}\), possibly weighted by two probability vectors \(\) and \(\) of size \(n\) and \(m\) respectively. \(f^{}\) can be estimated using entropy-regularized transport (Cuturi, 2013), with so-called entropic potentials (Pooladian and Niles-Weed, 2021). This involves choosing a regularization strength \(>0\), and solving the following dual problem using the Sinkhorn algorithm (Peyre and Cuturi, 2019, SS 4.2):

\[(^{},^{})=D^{}(,, ,;h,)*{arg\,max}_{ ^{n},^{m}}, +,-(e^{ }{}},e^{}{}})\,.\] (7)where \(_{ij}=(-h(_{i}-_{j})/)\). The entropy-regularized optimal transport matrix associated with that cost \(h\) and on those samples can be derived directly from these dual potentials (Peyre and Cuturi, 2019, Prop. 4.3) as \(P^{}(,,,;h,)^{n m}\) with entries at \((i,j)\) equal to:

\[[P^{}(,,,;h,)]_{i,j}= (_{i}^{}+_{j}^{}-h(_{i} -_{j})}{})\,.\] (8)

We now introduce the soft-minimum operator, and its gradient, defined for any vector \(^{q}\) as

\[_{}()-_{l=1}^{q}e^{- _{l}/},\,\,\!_{}()=[_{k}/}}{_{l=1}^{q}e^{-_{ l}/}}]_{k}.\]

Using vectors \((^{},^{})\), we can define estimators \(_{}\) and \(_{}\) for the optimal dual function (\(f^{},g^{}\)):

\[_{}:_{}([h(- _{j})-_{j}^{}]_{j})\,_{}:_{ }([h(_{i}-)+_{i}^{}]_{i})\,.\] (9)

Plugging these approximations into (6) forms the basis for the MBO estimator outlined in Algo. 1.

**Definition 1** (MBO Estimator).: _Given data, an elastic cost function \(h=_{2}^{2}+\) and solutions to Eq.(7), the MBO map estimator (Pooladian and Niles-Weed, 2021; Cuturi et al., 2023) is given by:_

\[T_{}()=-_{}\! (+_{j=1}^{m}_{j}()( (-_{j})-_{j}))\!,.\] (10)

_where \(()\!_{}([h(- _{j})-_{j}^{}]_{j})\) is a probability vector._

```
1:Set \(h=_{2}^{2}+\)\(\) if \(=0\), equivalent to (Pooladian+,'21)
2:\((^{},^{})=D^{}(,, ,;h,)\)\(\) Sinkhorn (Eq. 7).
3:\(=\): \(([_{j}^{}-h(- _{j})]_{j}/)\)
4:\(=\)lambda: \(_{j=1}^{m}()_{j}(( -_{j})-_{j})\)
5:\(T_{}[,,]=\): \(-_{}(+())\).
6:return:\(T_{}[,,]\) ```

**Algorithm 1** MBO-Estimator(\(,\); \(,,\))

## 3 On Ground-Truth Monge Maps for Elastic Costs

Our strategy to compute examples of ground-truth displacements for any elastic cost \(h\) rests on the following theorem, which is a direct consequence of (Santambrogio, 2015, Theorem 1.17).

**Proposition 1**.: _Consider a potential \(g:^{d}\) and its \(h\)-transform as defined in (3). Additionally, set \(T_{g}^{h}- h^{}^{h}\). Then \(T_{g}^{h}\) is the OT Monge map for cost \(h\) between \(\) and \((T_{g}^{h})_{}\) for any measure \(\) in \((^{d})\)._

The ability to compute an OT map for \(h\) therefore hinges on the ability to solve numerically the \(h\)-transform (3) of a potential function \(g\). This can be done, provably, as long as \(g\) is concave and smooth, and \(_{}\) is available, as shown in the following result

**Proposition 2**.: _Assume \(g\) is concave, \(L\)-smooth, and that \(<2/L\). Setting \(=\) and iterating_

\[+_{}(-+ g() }{1+})\] (11)

_converges to a point \(^{}()=_{}h(-)-g()\). Furthermore, we have_

\[^{h}()=h(-^{}())-g( ^{}())\,,^{h}()= h( -^{}())\,,T_{g}^{h}()=^{}().\] (12)

Proof.: Because \(h\) is the sum of a quadratic norm with \(\), the proximal operator of \( h\) can be restated in terms of the proximal operator of \(\)(Parikh et al., 2014, SS2.1.1). The convergence of iterates (11) follows from (Beck and Teboulle, 2009, Thm. 1) or (Rockafellar, 1976, Thm. 1). The final identities are given by (Bauschke and Combettes, 2011, Prop. 18.7).

As summarized in Algo.2, the proximal operator of \(\) is the only thing needed to implement iterations (11), and, as a result, the \(h\)-transform of a suitable concave potential. We can then plug the solution in (12) to evaluate the pushforward \(T_{g}^{h}\). In practice, we use the JAXOPT (Blondel et al., 2021) library to run proximal gradient descent. We illustrate numerically in \(2\)D the resulting transport maps for different choices of regularizer \(\) in Fig. 1. In this illustration, we use the same base function \(g\), and see clearly the impact of \(c\) on \(h\) transforms.

## 4 Subspace Elastic Costs

Recall that for a rank-\(p\) matrix \(A^{p d}\), \(p d\), the projection matrix that maps it to its orthogonal is \(A^{}=I-A^{T}(AA^{T})^{-1}A\). When \(A\) lies on the Stiefel manifold (i.e. \(AA^{T}=I\)), we have the simplification \(A^{}=I-A^{T}A\). This results in the Pythagorean identity \(\|\|^{2}=\|A^{}\|^{2}+\|A\|^{2}\). In order to promote displacements that happen _within_ the span of \(A\), we must set a regularizer that penalizes the presence of \(\) within its _orthogonal complement_, namely

\[_{A^{}}()\|A^{}\|_{ 2}^{2}.\] (13)

Since \(_{A^{}}\) is quadratic, its proximal operator can be obtained by solving a linear system (Parikh et al., 2014, SS6.1.1); developing and using the matrix inversion lemma results in two equivalent quantities

\[_{_{A^{}}}()=(I_{d}+(A ^{})^{T}A^{})^{-1}=(I_{d}+ A ^{T}(AA^{T})^{-1}A)\,.\] (14)

To summarize, given an orthogonal sub-basis \(A\) of \(p\) vectors (each of size \(d\)), promoting that a vector \(\) lies in its orthogonal can be achieved by regularizing its norm in the space orthogonal to the span of \(A\). That norm has a proximal operator that can be computed by parameterizing \(A\)_explicitly_, either as a full-rank \(p d\) matrix, or more simply a \(p d\) orthogonal matrix, to recover the suitable proximal operator for \(_{A^{}}\) in (14). Because that operator is simpler when \(A_{p,d}\) is in the Stiefel manifold,

\[_{_{A^{}}}()= (I_{d}+ A^{T}A).\] (15)

We propose to restrict the study in this work to elastic costs of the form 14 where \(A_{p,d}\). We also present in Appendix A alternative parameterizations left aside for future work.

### Statistical Aspects of Subspace Monge Maps

The family of costs (13) is designed to promote transport maps whose displacements mostly lie in a low-dimensional subspace of \(^{d}\). In this section, we consider the statistical complexity of estimating such maps from data, assuming \(A\) is known. The question of estimating transport maps was first

Figure 2: Illustration of the \(h\)-transform computation in \(2\)d. _(left)_: base concave potential \(g\), here a negative quadratic. _(other figures)_ Level lines of the corresponding \(h\)-transform \(^{h}\) for different choices of \(h\). The \(h\)-transform is computed using the iterations described in Prop. 2.

studied in a statistical context by Hutter and Rigollet (2021), and subsequent research has proposed alternative estimation procedures, with different statistical and computational properties (Deb et al., 2021; Manole et al., 2021; Muzellec et al., 2021; Pooladian and Niles-Weed, 2021). We extend this line of work by considering the analogous problem for Monge maps with structured displacements.

We show that with a proper choice of \(\), the MBO estimator outlined in Definition 1 is a consistent estimator of \(T^{}\) as \(n\), and prove a rate of convergence in \(L^{2}()\). We also give preliminary theoretical evidence that, as \(\), maps corresponding to the subspace structured cost \(_{2}^{2}+_{A^{}}\) can be estimated at a rate that depends only on the subspace dimension \(p\), rather than on the ambient dimension \(d\), thereby avoiding the _curse of dimensionality_.

Sample Complexity Estimates for the MBO Estimator.The MBO estimator is a generalization of the entropic map estimator, originally defined by Pooladian and Niles-Weed (2021) for the quadratic cost \(h=_{2}^{2}\). This estimator has been statistically analyzed in several regimes, see e.g., (Pooladian et al., 2023; Rigollet and Stromme, 2022; del Barrio et al., 2022) and (Goldfeld et al., 2022). We show that this procedure also succeeds for subspace structured costs of the form \(h=_{2}^{2}+_{A^{}}\). As a result of being recast as an estimation task for quadratic cost, the following sample-complexity result for the MBO estimator follows from (Pooladian and Niles-Weed, 2021, Theorem 3), and a computation relating the MBO estimator to a barycentric projection for the costs we consider (see Appendix B for the full statements, proofs, and applicability to general Mahalanobis norms).

**Theorem 1**.: _Let \(A^{p d}\) be fixed,and suppose \(\) has an upper- and lower- bounded density, and \(\) is upper-bounded, both supported over \(^{d}\) compact. Consider \(T^{}\) of the form Equation (23) for some \( 0\) fixed, and suppose we have samples \(_{1},,_{n}\) and \(_{1},,_{n}(T^{})_{}\). Let \(_{}\) be the MBO estimator with \( n^{-}\). Then it holds that_

\[\|_{}-T^{}\|_{L^{2}()}^{2} n^{- }\,,\]

_where the underlying constants depend on properties of \(,,\) and \(A\)._

### Connection to the Spiked Transport Model

The additional structure we impose on the displacements allows us to closely relate our model to the "spiked transport model" as defined by Niles-Weed and Rigollet (2022). The authors studied the estimation of the Wasserstein distance in the setting where the Brenier map between \(\) and \(\) takes the form,

\[T_{}()=-A^{T}(A-S(A))\,,\] (16)

where \(A_{p,d}\) and \(S:^{p}^{p}\) is the gradient of a convex function on \(^{p}\). Divol et al. (2022) performed a statistical analysis of the map estimation problem under the spiked transport model. They constructed an estimator \(_{n}\) such that the \(L^{2}()\) risk decays with respect to the _intrinsic dimension_\(p d\); this is summarized in the following theorem.

**Theorem 2** (Divol et al., 2022, Section 4.6).: _Suppose \(\) has compact support, with density bounded above and below. Suppose further that there exists a matrix \(A^{p d}\) on the Stiefel manifold such that \((T_{})_{}\), with \(T_{}\) defined as in Equation (16). Assume that \(\) is known explicitly. Given \(n\) i.i.d. samples from \(\), there exists an estimator \(_{n}\) satisfying_

\[\|_{n}-T_{}\|_{L^{2}()}^{2}_{(n)} n^{-()}\,.\] (17)

We now argue that the spiked transport model can be recovered in the large \(\) limit of subspace structured costs. Indeed, if \(\), then displacements in the subspace orthogonal to \(A\) are heavily disfavored, so that the optimal coupling will concentrate on the subspace given by \(A\), thereby recovering a map of the form (16), which by Theorem 2 can be estimated at a rate independent of the ambient dimension. Making this observation quantitative by characterizing the rate of estimation of \(T^{}\) as a function of \(\) for \(\) large is an interesting question for future work.

## 5 A Bilevel Loss to Learn Elastic Costs

Following SS 4, we propose a general loss to _learn_ the parameter \(\) of a family of regularizers \(\{_{}\}_{}\) given source and target samples only. Our goal is to infer adaptively a \(\) that promotesregular displacements, apply it within the estimation of Monge maps using MBO, and leverage this knowledge to improve prediction quality. Given input and target measures characterized by point clouds \(,\) and probability weights \(,\), our loss follows a simple intuition: the ideal parameter \(\) should be such that the bulk of the OT cost bore by the optimal Monge map, for that cost, is dominated by displacements that have a _low_ regularization value. Since the only moving piece in our pipeline will be \(\), we consider all other parameters _constant_ in the computation of the primal solution, to re-write (8) as:

\[P^{}() P^{}(,,, ;_{2}^{2}+_{},) ^{n m}.\] (18)

Each entry \([P^{}()]_{ij}\) quantifies the optimal association strength between a pair \((_{i},_{j})\) when the cost is parameterized by \(\), where a given pair can be encoded as a displacement \(_{ij}_{j}-_{i}\). For the regularizer \(\) to shape displacements, we expect \(P^{}()\) to have a large entry on displacements \(_{ij}\) that exhibit a low regularizer \(_{}(_{ij})\) value. In other words, we expect that \(_{}(_{ij})\) to be as small as possible when \(P^{}_{ij}()\) is high. We can therefore consider the loss

**Definition 2** (Elastic Costs Loss).: _Given two weighted point clouds \(,,,\), and \(P^{}()\) defined implicitly, as an OT solution in Equation (8), let_

\[() P^{}(),R(),[R()]_{ij}=_{}(_{ij}).\] (19)

Because \(P^{}()\) is itself obtained as the solution to an optimization problem, minimizing \(\) is therefore a _bilevel_ problem. To solve it, we must compute the gradient \(()\), given by the vector-Jacobian operators \( P^{}()^{}[]\) and \( R()^{}[]\) of \(P^{}\) and \(R\) respectively, borrowing notations from (Blondel and Roulet, 2024, SS2.3) (see also SS C for a walk-through of this identity)

\[()= P^{}()^{}[R()]+  R()^{}[P^{}()]\] (20)

The first operator \( P^{}()^{}[]\) requires differentiating the solution of an optimization problem \(P^{}()\). This can be done (Blondel and Roulet, 2024, SS10.3.3) using either unrolling of Sinkhorn iterations or using implicit differentiation. We rely on OTT-JAX (Cuturi et al., 2022) to provide that operator, using unrolling. The second operator \( R()^{}[]\) can be trivially evaluated, since it only involves differentiating the regularizer function \(_{}()\). These steps are summarized in Algo. 3.

**Learning Subspace Costs.** We focus in this section on the challenges arising when optimizing subspace costs, as detailed in Section 5. Learning matrix \(A\) in this context is equivalent to learning a subspace in which the displacement between the source and target measures happen mostly in the range of \(A\). As discussed previously, the cost function \((A)\) should be optimized over the Stiefel manifold (Edelman et al., 1998). We use Riemannian gradient descent (Boumal, 2023) for this task, which iterates, for a step-size \(>0\)

\[A(A-(A))\,,\]

with the _Riemannian gradient_ of \(\) given by \((A) G-AG^{T}A\) where: \(G(A)\) the standard Euclidean gradient of \(A\) computed with automatic differentiation provided in (20); \(\) is the projection on the Stiefel manifold, with formula \((A)=(AA^{})^{-1/2}A\). These updates ensure that one stays on the manifold (Absil and Malick, 2012).

## 6 Experiments

Thanks to our ability to compute ground-truth \(h\)-optimal maps presented in SS 3, we generate benchmark tasks to measure the performance of Monge map estimators. We propose in SS 6.1 to test the MBO estimator (Cuturi et al., 2023) when the ground-truth cost \(h\) that has generated those benchmarks is known. In SS6.2, we consider the more difficult task of learning simultaneously, and as outlined in SS 5, an OT map and the ground-truth parameter of a subspace-elastic cost defined by a matrix \(A^{*}\) of size \(p^{*} d\). The cost is parameterized by a matrix \(\) of size \( d\), where \(\) is an estimate of the ground-truth subspace dimension \(p^{*}\) (usually not known), equal to or larger than \(p^{*}\). We check with this synthetic task the soundness of the loss \(()\), Definition 2, and of our Riemannian descent approach by evaluating to what extent the \(\) vectors in \(\) recovers the subspace spanned by \(A^{*}\). Finally, we consider in SS 6.3 a direct application of subspace elastic costs to real data, without any ground-truth knowledge, using perturbations of single-cell data. In this experiment, our pipeline learns both an OT map and a subspace. Our code implements a parameterized RegTICost class, added to OTT-JAX (Cuturi et al., 2022). Such costs can be fed into the \(\) solver, and their output cast as \(\) objects that can output the \(T_{}\) map given in Definition 1.

### MBO on Synthetic Ground-Truth Displacement

In this section, we assume that the regularizer \(\) is _known_, using the same \(\) both for generation and estimation, but that the ground-truth regularization strength \(^{*}\) used to generate data is not known. We use that cost to evaluate the transport associated with \(_{}^{h}\) on a sample of points, using Proposition 1, and then compare the performance of Sinkhorn based estimators, either with that cost or the standard \(_{2}^{2}\) cost (which corresponds to \(=0\)).

We consider the \(=_{1}\) and \(_{A^{}}=\|A^{}\|_{2}^{2}\) regularizers, and their associated proximal operators. While we assume knowledge of \(\) in the MBO estimator, we do not use the ground-truth regularization \(^{*}\) which generated the data, and instead consider it a free parameter. The data is generated following SS 3 by sampling a concave quadratic function \(g():=-(-)^{T}M(-)\) where \(M\) is a Wishart matrix, sampled as \(M=QQ^{T}\), where \(Q^{d 2d}\) is multivariate Gaussian, and \(\) is a random Gaussian vector. We then sample \(n=1024\) Gaussian points stored in \(_{T}\) and transport each using the map defined in Proposition 1, computed in practice with Proposition 2. This recovers matched train data \(_{T}\) and \(_{T}\). We do the same for a test fold \(_{t}\), \(_{t}\) of the same size, to report our metric, the mean squared error (MSE), defined as \(\|T_{}(_{t})-_{t}\|_{2}^{2}\), where \(T_{}\) is obtained from Definition 1 using \(_{T}\), \(_{T}\). We plot this MSE as a function of \(\), where \(=0\) corresponds exactly to the MBO using the naked \(_{2}^{2}\) cost. We observe in Figure 3 that the MBO estimator with positive \(\) outperforms significantly that using \(_{2}^{2}\) only, for any range of the parameter \(\).

### Recovery of Ground-Truth Subspace Parameters in Elastic Costs

We propose to test the ability of Algorithm 3 to recover the ground-truth \(A^{*}\) parameter of a regularizer \(_{A^{}}\) as defined in (13). To do so, we proceed as follows: For dimension \(d\), we build the ground-truth cost \(h\) by selecting \(A^{*}\), sampling a \(p^{*} d\) normal matrix that is then re-projected on the \(p^{*} d\) Stiefel manifold. Next, we sample a random ICNN, and set the base function \(g\) to be its negative. We then sample a point cloud \(\) of \(n=512\) standard Gaussian points, and apply, following Proposition 1, the corresponding ground-truth transport to obtain \(\) of the same size. We tune the regularization parameter \(\) for \(\), to ensure that the \(p^{*}\) first singular values of displacements \(-\) captured either

Figure 3: Performance of the MBO estimator on two ground-truth tasks involving the \(=_{1}\) and \(_{A^{}}=\|A^{}\|_{2}^{2}\) structured costs, where \(p=2\) in dimension \(d=5\) (two figures to the _left_) and dimension \(d=10\) (two figures to the _right_). We display the MSE ratio between the MSE estimated with a regularizer strength \(>0\) and that in the absence of regularization (i.e., \(=0\)). The level of regularization used for generating the ground-truth data is \(^{*}\), whereas performance are shown varying w.r.t. \(\). We display curves \(\) s.t.d. estimated over 10 random seeds.

50%, 70% or 90% of the total inertia. We expect that the larger this percentage, the easier recovery should be. See SS D for details.

We launch our solver fed with these datasets with a subspace dimension \(\) preset in advance to either \(=p^{*}\) (matching ground truth) or \(=p^{*}\) (overbudgeting). We measure recovery of \(A^{*}\) by \(\) through the average (normalized by the basis size) of the residual error, when projecting the vectors in \(A^{*}\) in the span of the basis \(\), namely \(\|A^{*}-^{T}A^{*}\|_{2}^{2}/p^{*}\). For simplicity, we report performance after 1000 iterations of Riemannian gradient descent, with a stepsize \(\) of \(0.1/\) at iteration \(i\). All results in Figure 4 agree with intuition in the way performance varies with \(d,p^{*},\). More importantly, with errors that are often below one percent, we can be confident that our algorithm is sound. We observe that most underperforming experiments could be improved using early stopping.

### Learning Displacement Subspaces for Single-Cell Transport

We borrow the experimental setting in (Cuturi et al., 2023), using single-cell RNA sequencing data from (Srivastava et al., 2020). The original dataset shows the responses of cancer cell lines to 188 drug perturbations, downsampled to the \(5\) drugs (Belinostat, Dacinostat, Givinostat, Hesperadin, and Quisinostat) that have the largest effect. After various standard pre-processings (dropping low-variability genes, and using a \((1+)\) scaling), we project the dataset to \(d=256\) directions using PCA. In Table 1, we report the total number of cells used for experiments after pre-processing. We then use 80% train/20% test folds to benchmark two MBO estimators: that computed using the \(_{2}^{2}\) cost, and ours, using an elastic subspace cost, following the learning pipeline outlined in SS 5. We plot

Figure 4: Error averaged over 5 seeded runs (lower is better) in \(\) of the \( d\) orthogonal matrix \(\) recovered by our algorithm, compared to the ground-truth \(p^{*} d\) cost matrix \(A^{*}\). Error bars are not shown for compactness, but are negligible since all quantities are bounded below and close to 0. Dimensions \(d,p^{*}\) vary in each of these 6 plots, whereas \(\) is fixed to either \(p^{*}\) (top row) or \(1.25p^{*}\) (bottom row). Error is quantified as the normalized squared-residual error obtained when projecting the \(p^{*}\) basis vectors of \(A^{*}\) onto the span of \(\). From left to right, the regularization strength \(^{*}\) increases to ensure that 50%, 70% and 90% of the total inertia of all displacements generated by the ground-truth Monge map are borne by the \(p^{*}\) highest singular values. As expected, recovery is easier when \(\) is slightly larger than \(p^{*}\) (bottom) compared to being exactly equal (top). It is also easier as the share of inertia captured by \(p^{*}\) increases.

the Sinkhorn divergence (cf. Feydy et al. (2019)) for the \(_{2}^{2}\) cost for reference (see the documentation in OTT-JAX (Cuturi et al., 2022)).

**Conclusion.** In this work, we proposed an algorithmic mechanism to design ground-truth transports for elastic costs. As a first application, we were able to successfully benchmark the MBO estimator of (Cuturi et al., 2023) on two tasks (involving the \(_{1}\) and an orthogonal projection norm), showcasing the versatility of the MBO framework. Next, we demonstrated our ability to leverage subspace-penalizing costs to _learn displacement subspaces_ by solving an _inverse OT problem_. We showed successful numerical performance of the MBO estimator when the subspace is known but the regularization strength is not, but also that we were able to learn the ground-truth subspace. We foresee several open directions, the most encouraging being considering other learnable proximal operators beyond subspace approaches, and cementing connections and distinctions between subspace regularized transport (where _displacements_ happen in a subspace) vs. the spiked transport model (where all points are projected on a subspace prior to being transported)