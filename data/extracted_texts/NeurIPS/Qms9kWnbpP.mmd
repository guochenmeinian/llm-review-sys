# Are Spiking Neural Networks more expressive than Artificial Neural Networks?

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This article studies the expressive power of spiking neural networks with firing-time-based information encoding, highlighting their potential for future energy-efficient AI applications when deployed on neuromorphic hardware. The computational power of a network of spiking neurons has already been studied via their capability of approximating any continuous function. By using the Spike Response Model as a mathematical model of a spiking neuron and assuming a linear response function, we delve deeper into this analysis and prove that spiking neural networks generate continuous piecewise linear mappings. We also show that they can emulate any multi-layer (ReLU) neural network with similar complexity. Furthermore, we prove that the maximum number of linear regions generated by a spiking neuron scales exponentially with respect to the input dimension, a characteristic that distinguishes it significantly from an artificial (ReLU) neuron. Our results further extend the understanding of the approximation properties of spiking neural networks and open up new avenues where spiking neural networks can be deployed instead of artificial neural networks without any performance loss.

## 1 Introduction

Despite the remarkable success of deep neural networks (ANNs) , the downside of training and inferring on large deep neural networks implemented on classical digital hardware lies in their substantial time and energy consumption . The rapid advancement in the field of neuromorphic computing allows for both analog and digital computation, energy-efficient computational operations, and faster inference (, ). In practice, a neuromorphic computer is typically programmed by deploying a network of spiking neurons (SNNs) , i.e., programs are defined by the structure and parameters of the neural network rather than explicit instructions.

SNNs are more biologically realistic as compared to ANNs, as they involve neurons transmitting information asynchronously through spikes to other neurons . Different encoding schemes enable spiking neurons to represent analog-valued inputs, broadly categorized into rate coding (spike count) and temporal coding (spike time) (, ). In this work, we assume that information is encoded in the precise timing of a spike. The event-driven nature and the sparse information propagation through relatively few spikes enhance system efficiency by lowering computational demands and improving energy efficiency.

It is intuitively clear that the described differences in the processing of the information between ANNs and SNNs should also lead to differences in the computations performed by these models. Several groups have analyzed the expressive power of ANNs from the perspective of approximation theory (, , , ) and by quantifying the number of the linear regions (, ). At the same time, few attempts have been made that aim to understand the computational power of SNNs. (, ) showed that continuous functions can be approximated to arbitrary precision using

[MISSING_PAGE_FAIL:2]

### Computation in terms of firing time

Using (4) enables us to iteratively compute the firing time \(t_{v}\) of each neuron \(v V V_{}\) if we know the firing time \(t_{u}\) of each neuron \(u V\) with \((u,v) E\) by solving for \(t\) in

\[_{t_{(u,v) E}\{t_{u}+d_{uv}\}}P_{v}(t)=_{t(u,v) E}\{t _{u}+d_{uv}\}_{(u,v) E}_{\{0<t-t_{u}-d_{uv}\}}w_{uv} (t-t_{u}-d_{uv})=_{v}.\] (5)

Set \(E(_{U}):=\{(u,v) E:d_{uv}+t_{u}<t_{v} d_{uv}+t_{u}+\}\), where \(_{U}:=(t_{u})_{(u,v) E}\) is a vector containing the given firing times of the presynaptic neurons. The firing time \(t_{v}\) satisfies

\[_{v}=_{(u,v) E}_{\{0<t-t_{u}-d_{uv}\}}w_{uv} (t_{v}-t_{u}-d_{uv})=_{(u,v) E(_{U})}w_{uv}(t_{v}-t_{u}-d_{uv }),\] (6)

\[t_{v}=}{_{(u,v) E(_{U})}w_{uv}}+ _{U})}w_{uv}(t_{u}+d_{uv})}{_{(u,v) E( _{U})}w_{uv}}.\] (7)

Here, \(E(_{U})\) identifies the presynaptic neurons that actually have an effect on \(t_{v}\) based on \(_{U}\). For instance, if \(t_{w}>t_{v}\) for some synapse \((w,v) E\), then \(w\) did not contribute to the firing of \(v\) since the spike from \(w\) arrived after \(v\) already fired so that \((w,v) E(_{U})\). Equation (7) shows that \(t_{v}\) is a weighted sum (up to a positive constant) of the firing times of neurons \(u\) with \((u,v) E(_{U})\). Flexibility, i.e., non-linearity, in this model is provided through the variation of the set \(E(_{U})\). Depending on the firing time of the presynaptic neurons \(_{U}\) and the associated parameters (weights, delays, threshold), \(E(_{U})\) contains a set of different synapses so that \(t_{v}\) via (7) alters accordingly.

We formally define SNNs and ANNs by a sequence of their parameters and their corresponding realizations in Appendix A.1. To employ an SNN, the (typically analog) input information needs to be encoded in the firing times of the neurons in the input layer, and similarly, the firing times of the output neurons need to be translated back to an appropriate target domain. The encoding scheme in Definition 3 in Appendix A.1 translates analog information into firing times and vice versa in a continuous manner. Note that the following results are valid within the aforementioned setting.

## 3 Main results

A broad class of ANNs based on a wide range of activation functions such as ReLU generate Continuous Piecewise Linear (CPWL) mappings (, ). In other words, these ANNs partition the input domain into regions, the so-called linear regions, on which an affine function represents the ANN's realization. The result in Theorem 1 shows that SNNs also express CPWL mappings under very general conditions.

**Theorem 1**.: _Any SNN \(\) realizes a CPWL function provided that the sum of synaptic weights of each neuron is positive and the encoding scheme is a CPWL function._

Proof.: We show in the Appendix (see Theorem 5) that the firing time of a spiking neuron with arbitrarily many input neurons is a CPWL function with respect to the input under the assumption that the sum of its weight is positive. Since \(\) consists of spiking neurons arranged in layers it immediately follows that each layer realizes a CPWL mapping. Thus, as a composition of CPWL mappings, \(\) itself realizes a CPWL function provided that the input and output encoding are also CPWL functions. 

Next, we show that an SNN has the capacity to effectively reproduce the output of any (ReLU) ANN. In order to accurately realize the output of a ReLU network, the initial step involves realizing the ReLU activation function. Despite the fact that ReLU is a very basic CPWL function, we remark that it is not straightforward to realize ReLU via SNNs.

**Theorem 2**.: _Let \(a<0<b\). There does not exist a one-layer SNN that realizes \((x)=(0,x)\) on \([a,b]\). However, \(\) can be realized by a two-layer SNN on \([a,b]\)._

The proof is constructive, and we refer to Appendix A.4 for a detailed proof. Next, we extend the realization of a ReLU neuron to the entire network. We only provide a short proof sketch; the details are deferred to the Appendix A.5.

**Theorem 3**.: _Let \(L,d\), \([a,b]^{d}^{d}\) and let \(\) be an arbitrary ANN of depth \(L\) and fixed width \(d\) employing a ReLU non-linearity, and having a one-dimensional output. Then, there exists an SNN \(\) with \(N()=N()+L(2d+3)-(2d+2)\) and \(L()=3L-2\) that realizes \(_{}\) on \([a,b]^{d}\)._

Sketch of proof.: Any multi-layer ANN with ReLU activation is simply an alternating composition of affine-linear functions and a non-linear function represented by ReLU. To realize the mapping generated by some arbitrary ANN, it suffices to realize the composition of affine-linear functions and the ReLU non-linearity and then extend the construction to the whole network using concatenation and parallelization operations defined in Appendix A.2. 

The aforementioned result can be generalized to ANNs with varying widths that employ any type of piecewise linear activation function. Our expressivity result in Theorem 3 implies that SNNs can essentially approximate any function with the same accuracy and (asymptotic) complexity bounds as (deep) ANNs employing a piecewise linear activation function, given the response function satisfies the introduced basic assumptions. The number of linear regions is another measure of expressivity that describes how well a neural network can fit a family of functions. The following result establishes the number of linear regions generated by a one-layer SNN.

**Theorem 4**.: _Let \(\) be a one-layer SNN with a single output neuron \(v\) and \(d\) input neurons \(u_{1},,u_{d}\) such that \(_{i=1}^{d}w_{u_{i},v}>0\). Then \(\) partitions the input domain into at most \(2^{d}-1\) linear regions. In particular, for a sufficiently large input domain, the maximal number of linear regions is attained if and only if all synaptic weights are positive._

Proof.: The maximum number of regions directly corresponds to \(E(_{U})\) defined in (7). Recall that \(E(_{U})\) identifies the presynaptic neurons that based on their firing times \(_{U}=(t_{u_{i}})_{i=1}^{d}\) triggered the firing of \(v\) at time \(t_{v}\). Therefore, each region in the input domain is associated to a subset of input neurons that is responsible for the firing of \(v\) on this specific domain. Hence, the number of regions is bounded by the number of non-empty subsets of \(\{u_{1},,u_{d}\}\), i.e., \(2^{d}-1\). Now, observe that any subset of input neurons can cause a spike in \(v\) if and only if the sum of their weights is positive. Otherwise, the corresponding input region either does not exist or inputs from the corresponding region do not trigger a spike in \(v\) since they can not increase the potential \(P_{v}(t)\) as their net contribution is negative, i.e., the potential does not reach the threshold \(_{v}\). Hence, the maximal number of regions is attained if and only if all weights are positive and thereby the sum of weights of any subset of input neurons is positive as well. 

One-layer ReLU-ANNs and one-layer SNNs with one output neuron both partition the input domain into linear regions. A one-layer ReLU-ANN will partition the input domain into at most two linear regions, independent of the dimension of the input. In contrast, for a one-layer SNN, the maximum number of linear regions scales exponentially in the input dimension. This distinct behaviour stems from the intrinsic non-linearity of SNNs, originating from the subset of neurons affecting the output neuron's firing time, while in ANNs a non-linear function is applied to the output of neurons. Our result in Theorem 4 suggests that a shallow SNN can be as expressive as a deep ReLU network in terms of the number of linear regions required to express certain types of CPWL functions.

## 4 Discussion

The central aim of this paper is to study and compare the expressive power of SNNs and ANNs employing any piecewise linear activation function. The imperative role of time in biological neural systems accounts for differences in computation between SNNs and ANNs. The key difference in the realization of arbitrary CPWL mappings is the necessary size and complexity of the respective ANN and SNN. Recall that realizing the ReLU activation via SNNs required more computational units than the corresponding ANN (see Theorem 2). Conversely, using SNNs (see Theorem 4), one can also realize certain CPWL functions with fewer number of computational units and layers compared to ReLU-based ANNs. While neither model is clearly beneficial in terms of network complexity to express all CPWL functions, each model has distinct advantages and disadvantages. The significance of our results lies in investigating theoretically the approximation and expressivity capabilities of SNNs, highlighting their potential as an alternative computational model for complex tasks. The insights obtained from this work can further aid in designing architectures that can be implemented on neuromorphic hardware for energy-efficient applications.