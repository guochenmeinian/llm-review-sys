# Deep Learning with Physics Priors as Generalized Regularizers

Frank Liu

School of Data Science

Old Dominion University

Norfolk, VA 23508

United States

&Agniva Chowdhury

Computer Science and Math Division

Oak Ridge National Lab

Oak Ridge, TN 37831

United States

###### Abstract

In various scientific and engineering applications, there is typically an approximate model of the underlying complex system, even though it contains both aleatoric and epistemic uncertainties. In this paper, we present a principled method to incorporate these approximate models as physics priors in modeling, to prevent overfitting and enhancing the generalization capabilities of the trained models. Utilizing the structural risk minimization (SRM) inductive principle pioneered by Vapnik, this approach structures the physics priors into generalized regularizers. The experimental results demonstrate that our method achieves up to two orders of magnitude of improvement in testing accuracy.

## 1 Introduction

Learning from observation data is a crucial task in scientific machine learning (SciML). Deep neural networks have demonstrated to be highly effective in modeling complex systems in scientific research fields such as physics[4; 9; 19], chemistry[13; 31; 1] and biology[12; 5]. To avoid overfitting and to improve generalization performance, regularization techniques such as L1 and L2 regularization, weight decay, dropout, batch normalization and early stopping are commonly deployed during the training of models.

In many scientific machine learning applications, it is quite often that an approximate mechanistic model of the underlying physical phenomenon is available, albeit with uncertainty. For example, the motion in mechanical systems is governed by Newton's Law, and can be mathematically described by Hamiltonian mechanics, while the motion of fluid is governed by Navier-Stokes equations, and the electric and magnetic fields are described by Maxwell equations. The concept of integrating physics models with more expressive neural network models was initially introduced decades ago[17; 20; 28]. More recent work among this "hybrid-model" or "grey-box" approach include [32; 26; 28; 18; 14; 3; 23; 15; 8; 21]. Among them, a notable work is , which proposed a method to integrate simplified or imperfect physics models with deep learning models, by combining the two models as additive right-hand-sides of the differential equations, while with focused applications on the trajectory forecasting of complex systems.

In this paper, we present the analysis that integrating information of a physics model to deep grey-box modeling can be achieved as a generalized regularizer. Recognizing that the simplified or imprecise physics model is subject to both aleatoric (from data) and epistemic (from model) uncertainties, we use Vapnik's structural risk minimization as the inductive principle to cast the generalized regularization as an optimization problem.

#### Problem Description

The observation data are represented by the tuple \(\{(_{j},_{j})\}\) with \(j=1,N_{d}\). The neural network model is denoted by \(_{w}()\) with \(w\) representing trainable weights. By applying the L2 norm, the training of the model is equivalent to the empirical risk minimization(ERM) task below:

\[_{d}=}_{j}^{N_{d}}(_{j}-_{w}( {x}_{j}))^{2}\] (1)

When modeling the dynamic behavior of the physical system defined on \([0,T] D^{m}\), without loss of generality, we include time as part of the input. Hence \(_{j}[0,T] D\) and \(_{j}^{m}\). As in many scientific and engineering applications, we assume \(_{j}\) is subject to noise.

To avoid overfitting, Vapnik introduced the concept of structural risk in the seminal work of . The purpose of including structural risk is to prevent the model from becoming too complex. By penalizing models with large complexity under a given measure of the structural risk (e.g., VC-dimension) in the training process, the structural risk minimization (SRM) ensures that the models would not become too complex. The minimization is often realized on a sequence of nested structures (or hypotheses) with increasing structural risk:

\[ S_{k-1} S_{k} S_{k+1}\] (2)

with the recognition that more complex models with larger risks produce lower training loss (in the form of empirical risk), but with the increased potential to overfit. A learned model with a balanced trade-off between the empirical risk and structural risk will most likely to achieve good accuracy and generalization performance.

A widely adopted SRM in deep learning is the weight decay, where the ERM is augmented by a regularizer, which measures the L2 norm of the weights:

\[_{wd}=}_{j}^{N_{d}}(_{j}-_{w}( _{j}))^{2}+\|w\|^{2}\] (3)

with \(\) as a hyper-parameter controlling the balance between the empirical and structural risk.

The motivation of our work is that in many SciML applications, it is quite common that a physics model is available. However the physics model is only an approximate of the underlying physical phenomenon of the complex system (otherwise we can simply use the mechanistic model itself). The uncertainties of the mechanistic model with respect to the underlying complex system include aleatoric uncertainties and epistemic uncertainties. The former (or "data uncertainty") represents the inherent variability in the data, while the latter (or "model uncertainty") represents the imperfect model, which can due to the missing components in the model, or our lack of understanding of the physical phenomenon.

We propose to utilize the information embedded in the approximate model in model training by structuring the mechanistic model as a generalized regularizer. However unlike common L2-norm regularizer, the parameter \(\) has stronger dependency on the disparity (or the empirical representation of the epistemic uncertainty) of the physics prior, and should be optimized in a more comprehensive way. Our method provides a different perspective to the deep grey-box modeling approach such as . The introduction of the generalized regularization also opens the door to other interesting means of modeling training, such as the inclusion of multiple mechanistic models as physics priors with multiple regularizers, and the co-optimization of mechanistic model coefficients along with the model itself.

## 2 Physics Prior as Generalized Regularization

We refer to the (approximate) mechanistic model as the physic prior, it has the same support as the observation data \([0,T] D u\):

\[_{}(u)(x)=0\] (4)

where \(u^{m}\). In the case where the physics prior is an ODE, proper initial condition should be specified \(u(0)=u_{0}\). In the case the physics prior is a PDE, additional boundary condition is needed: \(u(x_{b})=u_{b}\) where \(x_{b} D\).

To include physics prior as a regularizer, we add additional collocation points in the support \(\{x_{i}\}\) where \(i=1,,N_{p}\) and \(x_{i}[0,T] D\). We introduce a generalized regularization as:

\[_{p}=}_{i}^{N_{p}}(_{}(u)(x_ {i}))^{2}=}_{i}^{N_{p}}(_{}( _{w}(x_{i}))^{2}.\] (5)

The total loss is the combination of empirical loss in Eqn. 1 and Eqn. 51:

\[(_{j},_{j},x_{i};w)=_{d}+ _{p}=}_{j}^{N_{d}}(_{j}- _{w}(_{j}))^{2}+}_{i}^{N_{p}}(_ {}(_{w}(x_{i}))^{2}.\] (6)

The loss function in Eqn. 6 can be depicted in the diagram shown in Fig. 0(a).

### Information Injected by the Generalized Regularizer

Following the SRM inductive principle, the balance between the empirical loss in Eqn. 1 and the structural loss in Eqn. 5 is crucial to ensure good accuracy and generalization performance. To illustrate the essence of the structural loss, we assume the underlying complex system can be described by an "oracle" (but completely unknown to us) model, as shown in Eqn. 7. Note here we promiscuously use \(_{}()\) to represent the oracle model. However in reality it may have no resemblance to the approximate model \(_{}\) at all.

\[_{}(u)(x)=0 x[0,T] D\] (7)

We assume the model trained on the data \(\{_{j},_{j}\}\):

\[w^{*}=*{argmin}_{w}_{d}\] (8)

is the perfect representation of the oracle model \(_{}\). Hence

\[_{}(_{w^{*}}(x))=0 x [0,T] D\] (9)

To quantify the structural loss shown in Eqn. 5, we have:

\[_{}(_{w^{*}}(x)) =_{}(_{w^{*}}(x))-_{}(_{w^{*}}(x))\] (10) \[=(_{}()-_{}( ))(_{w^{*}}(x)) x[0,T] D\] (11)

Observe that \(_{}()-_{}()\) is the representation of the epistemic uncertainty of our physics prior \(_{}\) with respect to the oracle model of the underlying complex system, hence effectively the function space in Eqn. 11 is a projection of the epistemic uncertainty onto the function space of the trained model \(_{w}\). Furthermore the regularizer presented in Eqn. 5 is its empirical version, sampled at the collocation points.

When the physics prior has no epistemic uncertainty, theoretically we have \(_{}()_{}()\). Since there still exist aleatoric uncertainties in the observation data, the structural risk term in Eqn. 5 will approach its minimum, but will not automatically become zero. Under this condition, the maximal value of the regularization parameter \(\) will inject most information to the loss function.

Figure 1: Diagram of model with generalized regularization.

The immediate consequence of the above observation is that because we usually don't have a concrete metric on the epistemic uncertainty of the physics prior \(_{}\), we don't have a good measure on the amount of information our generalized regularizer in Eqn. 5 contains. Hence it is necessary to optimize both the weights and the parameter \(\):

\[w^{*}=*{argmin}_{w,}=*{argmin}_{w, }_{d}+_{p}\] (12)

In this study, we perform the optimization by two nested loops:

\[w^{*}=*{argmin}_{}*{argmin}_{w}_{ d}+_{p}\] (13)

Although the optimization of the model weights \(w\) and parameter \(\) outlined in Eqn. 12 and Eqn. 13 appears similar to the optimization of more traditional regularization techniques, such as weight decay in Eqn. 3, we'd like to point out that the generalized regularization place a more strict structural on the solution space of the trained model, hence it is much more cognizant to the behavior of the underlying complex system. This is clearly demonstrated in the experimental results later in the paper.

### Regularization with Multiple Physics Priors

An immediate extension following the discussion above is that we can introduce multiple physics priors as generalized regularizers. As the simplest format, these physics priors can be based on the same family of the approximate models, but with different coefficients. The diagram with two physics priors is shown in Fig. 0(b). Mathematically the loss function becomes:

\[ =_{d}+_{1}_{p_{1}}+_{2} _{p_{2}}\] (14) \[=}_{j}^{N_{d}}(_{j}-_{w}(_{j}))}_{_{d}}+}{N_{p_{1}}} _{i}^{N_{p_{1}}}(_{_{1}}(_{w}(x_{i}))^{2}}_{ _{1}_{p_{1}}}+}{N_{p_{2}}}_{ k}^{N_{p_{2}}}(_{_{2}}(_{w}(x_{k}))^{2})}_{ _{2}_{p_{2}}}\] (15)

Again we promiscuously denote two physics priors as \(_{_{1}}()\) and \(_{_{2}}()\), even though they could be based on completely different families of functions or different families of differential equations. The learning process involves both the regularization parameters and model weights: In this study, we use two nested loops for the optimization:

\[w^{*}=*{argmin}_{_{1},_{2}}*{argmin}_{ w}_{d}+_{1}_{p_{2}}+_{2}_{p_{2}}\] (16)

Here we make the inductive assumption that the projections of the epistemic uncertainties of the two physics priors should be summed algebraically to provide structural risk minimization. Since the structural risk terms are non-negative, the overall outcome of the risk minimization can be interpreted as a data-driven approach to "select" which physics prior should have a stronger influence on the overall structural risk.

### Inclusion of Physics Prior Coefficients

Our generalized regularization method can be further extended to include the coefficients (all or a pre-selected subset) of the physics priors as part of the parameters. More specifically the learning task becomes:

\[w^{*} =*{argmin}_{,}*{argmin}_{w }_{d}+_{p}\] (17) \[=*{argmin}_{,}*{argmin}_{w }}_{j}^{N_{d}}(_{j}-_{w}(_ {j}))^{2}+}_{i}^{N_{p}}(_{}( _{w}(x_{i}))^{2}]\] (18)

This learning task can be interpreted as simultaneously adjusting the structure of the regularization by optimizing the physics priors represented by \(\) and the "strength" of the structural risk by optimizing \(\). The byproduct of the optimization in Eqn. 18, \(^{*}\), from:

\[^{*}=*{argmin}_{,}*{argmin}_{w} _{d}+_{p}\] (19)can be interpreted as the physics prior with the smallest epistemic uncertainty, as the projection to the function space of the trained model.

## 3 Experimental Results

In this section, we present experimental results based on our method. All experiments are written in Pytorch. The training and evaluation are conducted on an Nvidia DGX-2 server with A100 GPUs. More technical details are included in the supplemental materials section.

### Implementation in Hamiltonian Neural Networks

In the first example, we forked the public repo of the Hamiltonian Neural Network(HNN) and implemented generalized regularizer. The elegant utilization of Hamiltonian mechanics in HNN makes it straight-forward to implement generalized regularizer based on physics priors. For each case, the physics prior is parameterized by another Hamiltonian \(_{}\), where \(\) represents the parameters of physics models such as mass, length of the pendulum. We introduce the generalized regularization by:

\[_{reg}=\|(_{w}}{ }-_{}}{} )+(_{w}}{}-_{}}{})\|_{2}\] (20)

while the original HNN loss is computed by:

\[_{HNN}=\|_{w}}{}- }{ t}\|_{2}+\|-_{w}}{}-}{ t}\|_{2}\] (21)

where \(w\) denotes the trainable weights of the HNN model.

Results of three cases are presented in Tab. 1: mass-spring, ideal pendulum and real pendulum. Improvements are illustrated in Fig. 2. In the last example, the training data are collected from measurements of physical pendulum, which is subject to sensor noise, as well as epistemic uncertainties such as frictions. In all three cases, the generalized regularization demonstrated clear improvements in both baseline model and HNN model. We want to point out that for ideal pendulum, generalized regularization further improves the energy by \(10\), in addition to \(2\) improvement of HNN. In the case of real pendulum, in which a precise physics model is unknown, HNN demonstrated an impressive \(30\) improvement in terms of energy (from \(376.9\) to \(11.2\)), while the introduction of physics prior can further improve the energy metric to \(9.5\), an additional \(15\%\) improvement.

Figure 2: Performances of Hamiltonian NN with and without physics regularizers. Generalized regularization implementation is based on the open-source repo of Greydanus et. al. “Hamiltonian Neural Networks”.

### 1D Reaction Equation

We study the one dimensional reaction equation that is commonly used to model chemical reactions. It's the simplification of the reaction-diffusion equations and is given by,

\[- u(1-u)=0\,,\] (22)

with the associated initial and (periodic) boundary conditions \(u(x,0)=g(x),\ x D\) and \(u(0,t)=u(2,t),\ t(0,T]\) respectively, with \(g(x)=-}{2(/4)^{2}}\), with \(\) being the reaction coefficient. We generate two datasets by using two oracle reaction equations, shown in Fig. 2(a) and 2(d) respectively. Fig. 2(a) also shows results of learned models with different physics priors, all are different from the oracle models.

As a comparison, we also train the baseline models for the two reaction equation cases from observation data only, with and without using weight decay as regularization. The results are shown in Fig. 3. Quantitative MSE from testing are tabulated in Tab. 2. Clearly our method outperforms weigh decay in all but one case.

    & Test Loss &  \\ Task & Baseline & HNN & Baseline & HNN \\  Mass spring (original) & \(36.73 1.86\) & \(35.91 1.83\) & \(\) & \(0.376 0.077\) \\ Mass spring (w/ reg.) & \(\) & \(\) & \(167.91 20.50\) & \(\) \\  Ideal Pendulum (original) & \(35.32 1.80\) & \(35.59 1.82\) & \(41.83 9.75\) & \(24.85 5.42\) \\ Ideal Pendulum (w/ reg.) & \(\) & \(\) & \(\) & \(\) \\  Real Pendulum (original) & \(1.50 0.23\) & \(5.80 0.60\) & \(376.89 72.50\) & \(11.22 3.87\) \\ Real Pendulum (w/ reg.) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Quantitative results of three tasks in Table 1 in Geydanus et. al. “Hamiltonian Neural Networks”. All values are multiplied by \(10^{3}\). Implementation is based on the Github repo by Geydanus, although some values of the original models are slightly different from values reported in the original paper. **Bold** entries indicate the best results.

Figure 2: Two cases of 1D reaction equation, including the ground truth and models with different physics priors. The crosses indicate the collocation points of training data.

[MISSING_PAGE_EMPTY:7]

Finally we use case 1 of the convection equation experiment for demonstration of multiple physics priors and optimization of physics prior coefficients, shown in Fig. 6. In the first experiment, two physics priors are specified with \(=25\) and \(=35\). The optimal regularization parameters are \(_{1}^{*}=3.72{}10^{-7}\) and \(_{2}^{*}=3.47{}10^{-3}\) with testing MSE of \(9.53{}10^{-3}\). In the second experiment, the final physics prior coefficient is \(^{*}=29.67\). With an optimal \(^{*}=1.25{}10^{-1}\), it achieves the testing MSE of \(5.04{}10^{-3}\).

## 4 Conclusion and Future Work

In this paper we propose a principled method to incorporate the prior knowledge of an underlying complex system, in the form of approximate physics models, into the data-driven deep grey-box modeling. By structuring the imprecise physics models, or physics priors, as generalized regularizers, we apply Vapnik's structural risk minimization (SRM) inductive principle to balance the model accuracy and model complexity. Our analysis indicates that the information in the physics priors is bounded by the uncertainty, especially the epistemic uncertainty, of the physics priors. Experimental results have shown that our method is highly effective in improving the test accuracy. For future work, we plan to investigate the theoretical and practical implications when multiple physics priors are included in the regularization.