# PrivAuditor: Benchmarking Privacy Vulnerabilities in LLM Adaptation Techniques

Derui Zhu\({}^{1}\) & Dingfan Chen\({}^{2}\) & Xiongfei Wu\({}^{3}\) & Jiahui Geng\({}^{4}\) &Zhuo Li\({}^{3}\) &Jens Grossklags\({}^{1}\) &Lei Ma\({}^{5,6}\)

\({}^{1}\)Technical University of Munich &\({}^{2}\)Saarland University

\({}^{3}\)Kyushu University &\({}^{4}\)MBZUAI

\({}^{5}\)The University of Tokyo &\({}^{6}\)University of Alberta

Equal contribution

###### Abstract

Large Language Models (LLMs) are recognized for their potential to be an important building block toward achieving artificial general intelligence due to their unprecedented capability for solving diverse tasks. Despite these achievements, LLMs often underperform in domain-specific tasks without training on relevant domain data. This phenomenon, which is often attributed to distribution shifts, makes adapting pre-trained LLMs with domain-specific data crucial. However, this adaptation raises significant privacy concerns, especially when the data involved come from sensitive domains. In this work, we extensively investigate the privacy vulnerabilities of adapted (fine-tuned) LLMs and benchmark privacy leakage across a wide range of data modalities, state-of-the-art privacy attack methods, adaptation techniques, and model architectures. We systematically evaluate and pinpoint critical factors related to privacy leakage. With our organized codebase and actionable insights, we aim to provide a standardized auditing tool for practitioners seeking to deploy customized LLM applications with faithful privacy assessments.

## 1 Introduction

The rapid evolution of large language models (LLMs) has made them fundamental to many modern natural language processing tasks [1; 2]. These capabilities are typically powered by vast amounts of model parameters, scaling to trillions, and intensive pre-training on massive text corpora (e.g., nearly a terabyte of English text ). However, the large-scale pre-training required for these models incurs significant computational costs, making it financially prohibitive for most practitioners. Additionally, pre-trained models often need additional fine-tuning to achieve satisfactory performance in specific domains [4; 5; 6]. Consequently, the current best practice involves acquiring an open-source LLM as a pre-trained foundation model and then adapting it for domain-specific data.

However, the common "_pre-training, adaptation tuning_" pipeline inadvertently raises privacy concerns regarding the leakage of sensitive domain data used for adapting pre-trained LLMs [7; 8; 9; 10; 11]. Indeed, recent research has demonstrated that LLMs can memorize substantial volumes of sensitive data, leading to a high risk of unintentional privacy leakage to third parties [12; 13; 14]. These issues contribute to the ongoing debate about the privacy implications of LLMs and may trigger violations of modern privacy regulations, e.g., the General Data Protection Regulation (GDPR), underscoring the urgent need to address the privacy challenges associated with LLMs.

To analyze the privacy issues related to the usage of LLMs, existing research primarily focuses on the leakage of pre-training data when querying a deployed general-purpose LLM [12; 14; 13]. Building on this foundation, in-depth investigations regarding such leakage, with respect to various factorsincluding model size and the degree of training data repetition, have been presented [10; 15; 16; 17]. Yet, in the context of fine-tuning/adaptation scenarios, recent privacy risk assessments have typically been limited to specific model architectures (mainly encoder-based models), a narrow selection of fine-tuning methods, and a certain choice of attack methods [7; 8; 9; 10; 11; 18]. A comprehensive benchmark evaluation is still missing, despite its importance for providing critical insights and accurate privacy assessments to facilitate the practical application of domain-specific LLMs. In particular, this gap highlights a crucial research question: _To what extent, and in what ways, do different adaptation methods influence the privacy risk of LLMs?_

To address the research question, this paper presents, to the best of our knowledge, the first benchmark investigating the privacy implications of LLM adaptation techniques, accompanied by a comprehensive empirical study. We focus on membership inference attack (MIA) techniques , which aim to determine whether a given query sample was used for adapting the target LLM, due to their popularity and close relationship to a broader class of topics [12; 20; 21]. Our investigation encompasses five types of LLMs with different architectures (T5 , LLaMA , OPT , BLOOM , and GPT-J ), seven LLM adaptation techniques representative of the current state of the art, and three datasets from different domains that closely mimic real-world sensitive fields. With our presented benchmark and comprehensive study, we aim to provide critical insights into the privacy risks associated with LLM adaptation techniques and guide the secure development of new models.

## 2 Privacy Measurement for Large Language Models

We evaluate the privacy vulnerabilities of LLMs through the lens of MIAs , which are widely recognized for their extensive applicability. MIAs are also closely associated with other privacy concerns, such as training data reconstruction [12; 15] and the retrieval of personally identifiable information [13; 26; 14], underscoring its critical role in privacy assessments.

### Formulation

Notation.We denote \(f_{}\) as the target language model, parameterized by \(\), which starts from a pre-trained model and is further adapted to a private dataset \(\). Each text sample \(^{(i)}\) is represented as a sequence of tokens, i.e., \(^{(i)}\!=\!(x_{1}^{(i)},x_{2}^{(i)},...,x_{L}^{(i)})\). The sample index \(i\) may be omitted for clarity when it is not relevant to the discussion. During inference, the model allows estimating the token likelihood \(f_{}(x_{l}|x_{1},...,x_{l-1})\) and generates new text by iteratively sampling \(_{l} f_{}(x_{l}|x_{1},...,x_{l-1})\) conditioned on the prefix \((x_{1},...,x_{l-1})\). Starting with the initial token \(x_{1}\), the model feeds each newly sampled token \(_{l}\) back into itself to generate the subsequent token \(_{l+1}\), continuing this process until a predetermined stopping criterion is met.

Threat Model.The attacker \(\) aims to determine whether a given query text sample was included in the private dataset \(\) used to customize the target model for the private domain. We adopt the conventional threat model where the attacker may have either _black-box_ or _white-box_ access to the target model. In the _black-box_ scenario, the attacker can access only the model's output probability predictions, typically via a prediction API call. In contrast, the _white-box_ scenario permits the attacker to access the model's internal structure and parameters.

We follow the standard evaluation framework, where the adversary has access to a query set \(\!=\!\{(^{(i)},m^{(i)})\}_{i=1}^{M}\). This set includes both member (i.e., seen by the target model \(f_{}\)) samples and non-member (unseen) samples drawn from the same data distribution. Each \(m^{(i)}\) indicates the membership status, where \(m^{(i)}\!=\!1\) if \(^{(i)}\) is a member. The attack \((^{(i)},f_{})\) acts as a binary classifier, predicting \(m^{(i)}\) for a given query sample \(^{(i)}\) with access to the target model.

### Attack Approaches

We conducted a broad literature search to identify representative approaches for membership inference attacks, aiming to provide a comprehensive benchmark. Below, we present an overview of each approach under a unified notation to facilitate comprehension and comparison.

**Likelihood-based**. Given that LLMs are typically trained using a maximum likelihood objective on the training data, the most basic method for predicting membership involves using the (normalized)log-likelihood of the target query sample as the metric: a _higher_ likelihood score indicates a better fit of the target model \(f_{}\) on the query data point \(=(x_{1},...,x_{L})\), suggesting it is likely a _member_ of the training set. Formally, the attack can be summarized as:

\[(,f_{})=_{l=1}^{L} f _{}(x_{l}|x_{1},...,x_{l-1})>_{L},\] (1)

where \(_{L}\) denotes the threshold score above which the attack predicts the sample to be a member.

**Likelihood with Reference**. While the basic likelihood score provides evidence for membership detection, it often fails to achieve high precision. This is because high-likelihood samples are not always present in the training data, but can also be uninformative texts frequently encountered in the pre-training dataset. A natural improvement involves calibrating the likelihood score by comparing it with the score obtained from a reference model not tailored for the private data. This leads to the likelihood ratio evaluated on the target versus the reference model. Formally,

\[(,f_{})=_{l=1}^{L}  f_{}(x_{l}|x_{1},...,x_{l-1})- f_{}(x_{l}|x_{1},...,x_{l-1})>_{L_{}},\] (2)

where \(f_{}\) denotes a reference model not trained on the private dataset and \(_{L_{}}\) is the threshold.

**Zlib Entropy as Reference**. While using a reference for calibrating the inherent frequency of text is essential for membership inference, it is not necessary to fix the reference to be another neural language model. In principle, any technique that quantifies the normality or informativeness for a given sequence can be useful. Following , we compute the zlib entropy of the text, which is the number of bits of entropy when the text sequence is compressed using zlib compression . Subsequently, the ratio of the average negative log-likelihood of a sequence and the zlib entropy is used as the membership inference metric. Formally,

\[(,f_{})=-_{l=1}^{L} f _{}(x_{l}|x_{1},...,x_{l-1})/()<_{},\] (3)

where \(()\) denotes the zlib entropy of \(\).

**Neighborhood-based**. To account for the normality of text samples for membership inference, one can calibrate their likelihood scores using their semantic neighbors. This can be achieved by generating neighbors of the data point and measuring their likelihood scores using the target model, which then serve as an estimation for the normality of the query text. The neighbors are designed to preserve semantics and are well-aligned with the context of the original words. These neighbors are obtained through semantically-preserving lexical substitutions proposed by transformer-based masked language models . Formally, the membership score is expressed by comparing the log-likelihood of the query sample to the averaged log-likelihood of its neighbors:

\[(,f_{})=_{l=1}^{L} f _{}(x_{l}|x_{1},...,x_{l-1})-_{i=1}^{k}_{l=1}^{L} f _{}(_{l}^{(i)}|_{1}^{(i)},...,_{l-1}^{(i)})> _{L_{_{}}},\] (4)

where \(\{}^{(i)}\}_{i=1}^{k}\) corresponds to \(k\) neighbors of the given sample \(\).

**Min-K% Probability**. The MIN-K% Probability score captures the intuition that a non-member example is more likely to include a few outlier words with high negative log-likelihood (or low probability), while a member example is less likely to include words with such low likelihood scores. Following , we select the \(K\%\) of tokens from \(\) with the minimum token probability to form a set, and compute the average log-likelihood of the tokens in this set

\[(,f_{})=)}|}_{x_{l})}} f_{}(x_{l}|x_{1},...,x_{l-1})>_{},\] (5)

where Min-K%(\(\)) denotes the set of tokens with the lowest \(K\%\) likelihood conditioned on its prefix.

**Min-K%++**. In the context of maximum likelihood training, it has been observed that training samples tend to form local maxima in the modeled distribution along each input dimension. As exploring an input dimension can be viewed as substituting the current token with alternative candidates from the model's vocabulary, the membership score is defined by the normalized log probability under the conditional categorical distribution \(f_{}(|_{<l})\), where a high probability indicates likely membership. In line with , the score is calculated using the Min-K% least probable tokens:

\[(,f_{})=)}}_{x_{l})}}(x_{l} |x_{1},...,x_{l-1})-_{<l}}{_{<l}}>_{},\] (6)

while \(_{<l}\!=\!_{z f_{}(|_{<l})}[ f_{} (z|_{<l})]\) represents the expectation of the next token's log probability over the vocabulary of the model given the prefix \(_{<l}\!=\!(x_{1},...,x_{l-1})\), and the term \(_{<l}=_{z f_{}(|_{<l})})[( f_{ }(z|_{<l})-_{<l})^{2}]}\) is the standard deviation.

**Gradient Norm-based**. The phenomenon of local minimality at training data points is often evidenced by the smaller magnitudes of parameter gradients observed at these points [32; 33; 11]. A practical approach would be to utilize the gradient norm of a target data point as the membership score. This concept is mathematically represented as follows:

\[(,f_{})=-_{l=1 }^{L}_{} f_{}(x_{l}|x_{1},...,x_{l-1})<_{ }.\] (7)

Notably, computing this gradient requires white-box access to the target model, unlike the previously mentioned methods, which rely solely on the model's output predictions.

## 3 LLM Adaptation Techniques

Existing LLM adaptation techniques can be roughly categorized into _regular fine-tuning_, _parameter-efficient fine-tuning_, and _in-context learning_. Below, we briefly discuss representative techniques from each of these categories. For a more detailed comparison of parameter-efficient fine-tuning techniques, we refer readers to prior work .

**Regular Fine-tuning**. The basic fine-tuning approach involves taking a pre-trained model and adapting all its parameters for a task-specific downstream dataset, i.e., _full fine-tuning_. This enables the model to learn specific patterns in the new data domain, thereby improving its accuracy and relevance for the target application. However, as models increase in size, full fine-tuning becomes impractical due to the high computational cost. Additionally, overfitting can become a significant issue, closely related to privacy vulnerabilities.

**Adapter**. Adapter-based fine-tuning strategically integrates additional lightweight layers into an existing model architecture [35; 36; 37], typically by injecting small modules (adapters) between transformer layers. During fine-tuning, only these adapter layers are updated for domain-specific data, while the core model parameters remain frozen, which greatly reduces computational overhead compared to regular fine-tuning.

**Low-Rank Adaptation**. Low-Rank Adaptation (LoRA)  is based on the hypothesis that weight changes during model adaptation exhibit a low "intrinsic rank". To leverage this, LoRA proposes integrating trainable low-rank decomposition matrices into each transformer layer to approximate the weight updates, while only allowing modifications of these low-rank matrices and freezing the pre-trained weights.

**Prompt-based Tuning**. Instead of changing the weights of the neural network, prompt-based tuning  typically involves adding specific prompts to the input text to steer the model towards the desired output. Existing studies commonly prepend tunable continuous task-specific vectors to the input embeddings (potentially across multiple layers), typically known as "soft prompts", and optimize over these continuous prompts while keeping the other pre-trained parameters unchanged during the fine-tuning process. Specifically, _Prompt-tuning_ prepends the input sequence with special tokens to form a template and tune the embeddings of these tokens directly. _P-tuning_ adds continuous prompt embeddings generated from pseudo prompts by a small encoder to the input embeddings of the model and tunes the prompt encoder. _Prefix tuning_ injects a trainable prefix matrix into the keys and values of the multihead attention at every layer of the model and updates the injected trainable prefix matrices.

In-context Learning.By enabling LLMs to perform diverse tasks through contextual adaptation, without altering their internal parameters, in-context learning  introduces a paradigm shift from traditional fine-tuning. Instead of performing explicit parameter updates, the model utilizes task-specific examples and instructions embedded within the input prompt to infer the task requirements. The key insight lies in the model's ability to treat these examples as implicit demonstrations, dynamically aligning its behavior with the desired output. This emergent capability makes in-context learning highly flexible, as it allows the model to generalize effectively from limited examples with minimal computational overhead, avoiding the computational burden associated with fine-tuning .

## 4 Related Work

**Privacy Threat for LLMs**. While the rapid development of LLMs has greatly facilitated various real-world applications, the widespread use of LLMs, especially in sensitive domains such as medical and finance, has raised serious privacy concerns. It is notorious that large neural networks tend to unintentionally memorize their training data (beyond learning the general patterns essential for conducting the target tasks), which raises vulnerabilities to privacy attacks such as membership inference [19; 7; 8; 9; 18; 21; 29; 45; 46; 47; 48; 49; 50; 51; 52], personal identifiable information retrieval [13; 14; 53; 54], and training data extraction [11; 12; 15; 53].

**Membership Inference in LLMs**. Membership inference is a commonly studied privacy attack, which is closely related to other topics such as training data extraction (by serving as an intermediate step) , examining data contamination  (i.e., whether the testing data have been seen by the target model), and theoretical privacy notions like differential privacy  (which by construction should provide privacy guarantees in the context of training data membership). While recent studies have investigated such attacks for data used for model pre-training [46; 21; 50; 51; 52; 55] and fine-tuning [7; 8; 9; 10; 11], they are focusing on specific attack strategies, a limited set of fine-tuning techniques (typically full fine-tuning or tuning the top layers) and particular model types (e.g., pre-trained encoders), which may not faithfully reflect the existing progress of such investigation.

To address this gap, our work considers a broad range of representative recent adaptation techniques and attack methods. This includes literature that may not directly focus on membership inference but is applicable to it. Our investigation aims to provide a more comprehensive understanding of potential privacy threats related to membership leakage when using LLMs.

## 5 Experiments

### Setup

**Datasets**. In contrast to previous studies, which have primarily focused on less sensitive datasets such as News and Wikipedia, our study is dedicated to a detailed evaluation of private data leakage risks in environments that handle highly sensitive and valuable private information. Specifically, we conduct experiments on the following adaptation datasets \(\): Sujet-finance-instruct-177k (**S****uject Finance**) , Corporate Climate Policy Engagement (**CorpClimate**) , as well as Synthetic-Text-to-SQL (**SQL**) . Our selection process aimed to minimize potential overlap with the pre-training

Figure 1: An overview pipeline illustrating the workflow of PrivacyAuditor.

datasets and ensure a more accurate evaluation of membership. Specifically, all the chosen fine-tuning datasets were released after the pre-trained models were developed, reducing the risk of shared content. Additionally, the datasets underwent extensive pre-processing to further minimize the chance of overlapping data points, even if they might originate from similar sources. We also included synthetic data with a specific structure that is unlikely to derive from web-based sources, ensuring further independence from the data used in pre-training.

**Models**. We consider the two predominant LLM architectures: decoder-only and encoder-decoder LLMs and conduct experiments on foundation models including **T5**, **LLaMA**, **OPT**, **BLOOM**, and **GPT-J**, each configured with different numbers of model parameters. All the open-source pre-trained LLMs are downloaded from Huggingface1. All experiments are conducted on a computing cluster with 4 Nvidia A100 80G with 512G memory. More details are included in the supplementary materials.

**Evaluation Configuration**. We evaluate the target LLMs' test accuracy on the test portion of the adaptation datasets as the _utility_ metric. For evaluating privacy, following the common evaluation standard for membership inference attacks, we composed an evaluation query set \(\) comprising an equal number of member and non-member samples (defaulting to 1000 each), while limiting the sample size to 10 for in-context learning experiments due to memory constraints. The member samples are uniformly sampled from the training dataset, while the non-member samples are randomly selected from the test portion of the datasets, ensuring they were not used in training. Privacy leakage is evaluated using standard metrics , including attack Area under the ROC Curve (**AUC-ROC**), False Positive Rate at low True Positive Rate (**FPR@0.1% TPR**, and **FPR@1% TPR**).

**Attack and Adaptation Techniques**. We evaluate the following attack methods as outlined in Section 2.2: **Likelihood** (Equation 1), **Likelihood-ref** (Equation 2), **Zlib Entropy** (Equation 3), **Neighborhood** (Equation 4), **Min-K** (Equation 5), **Min-K++** (Equation 6), **Gradient-Norm** (Equation 7) as outlined in Section 2.2. As introduced in Section 3, we evaluate the following representative adaptation techniques: full fine-tuning (**Full**), only updating the attention heads of the top-2 layers (**Top2Head-tuning**), adapter-based technique (**Adapter-H**), **Prefix-tuning**, **LoRA**, **P-tuning**, **Prompt-tuning**, and **in-context learning**. Note that all the aforementioned attack methods require black-box access to the target model, except for the Gradient-Norm method. This exception may render the Gradient-Norm method inapplicable to typical in-context learning scenarios where no parameter updates are performed. We use the default parameters from the original implementations. More details can be found in the supplementary materials.

### Benchmark Design

To systematically assess data leakage risks across various fine-tuning approaches in LLMs, we present experiments designed to answer the following research questions.

**RQ1: Is Private Data Used for Adapting LLMs Vulnerable to Leaks?**

**Motivation.** Although LLMs demonstrate promising capabilities in generalizing across multiple tasks, adapting them to specific domain applications remains essential due to non-negligible domain shifts . Since domain data is a crucial asset for data owners and typically contains sensitive information, it is vital to assess the extent to which this data can be leaked from the product model.

**Approach.** We first adopt the arguably most competitive lightweight fine-tuning technique, namely LoRA, to generate target downstream models across different datasets. Then, we visualize the data distributions of the member and non-member likelihood scores and inspect whether systematic differences exist that can be used as clues for detecting membership. Subsequently, we employ various state-of-the-art MIAs to measure the extent of private domain information leakage.

**RQ2: Do Different Adaptation Techniques Vary in Their Downstream Privacy Vulnerability? Motivation.** Different adaptation techniques involve distinct design patterns, introduce varying computational costs, and achieve unequal target performance. While these aspects have been extensively compared in existing literature on (parameter-efficient) fine-tuning techniques, the corresponding privacy implications have not been thoroughly investigated. Therefore, we design experiments to examine how various adaptation methods affect the effectiveness of privacy attacks.

**Approach.** We provide a unified implementation of representative adaptation techniques with varying amounts of trainable parameters. We then compare the performance of MIAs and model utility across various datasets and evaluation metrics under fair comparison conditions.

**RQ3: What Factors Potentially Affect Privacy Vulnerability in LLM Adaptation?**

**Motivation.** Besides knowing _"whether"_ different LLM adaptation techniques affect the privacy vulnerability of the resulting product LLM, it is also crucial to understand _"how"_ and _"why"_. Investigating the potential factors that influence such vulnerability is essential, as understanding these factors is beneficial for developing more robust and privacy-preserving LLM fine-tuning approaches, and provides insights into preventing private domain data from leaking during the fine-tuning process.

**Approach.** Motivated by the existing understanding of privacy risks associated with large neural networks, we conduct experiments spanning several critical factors: varying amounts of data for adaptation, different numbers of training iterations, and various model sizes. Additionally, we perform fine-tuning on domain datasets for both multiple tasks and single tasks, aiming to examine how task diversity in the pre-training dataset affects privacy vulnerability.

### RQ1: Is Private Data Used for Adapting LLMs Vulnerable to Leaks?

**Distributional Differences Between Member and Non-Member Data.** Figure 2 visualizes the distribution of likelihood scores for member and non-member data using the target _Llama7b_ model fine-tuned with _LoRA_. Even though these likelihood scores (Equation 1) represent the most basic metric an attack would consider, the results reveal subtle but noticeable distinctions in the distributions. This indicates the potential for an adversary to exploit LLM outputs to determine whether a sample was used in fine-tuning and highlights the vulnerability of membership leakage of domain data through deployed product LLMs. However, the limited prominence of these differences also underscores the need for more refined attack strategies to effectively uncover membership information.

**Strong MIAs Effectively Detect Data Used for LLM Adaptation.** Given the distinct distribution patterns between member and non-member data, we conducted experiments on existing representative

Figure 3: Overview of the attack performance across different LLMs and datasets.

Figure 2: The likelihood score distribution of member and non-member data in Llama-7b fine-tuned with LoRA on different datasets.

MIAs (outlined in Section 2.2) to determine whether these differences can be exploited to infer the membership of a given sample. As summarized in Figure 3, the results demonstrate that LLM adaptation techniques may lead to the leakage of training data under existing attacks, with _Likelihood-ref_ (Equation 2) being the most effective method overall and performing reasonably well across different types of model architectures. These results represent a meaningful lower bound on the worst-case privacy risk, highlighting the privacy vulnerabilities introduced during LLM fine-tuning and underscoring significant data protection demands during LLM fine-tuning. The complete quantitative results are presented in the supplementary materials.

**Product LLMs for Structural Data Demonstrate Greater Robustness Against MIAs.** As shown in Figure 3, inferring membership on the SQL dataset is more difficult than on the others. This may be due to the structural similarity of data samples within the same distribution, i.e., smaller in-domain diversity. To validate this, we further analyze the data samples misclassified by the attacker (shown in Figure 4) and observe that these data are structurally identical and semantically highly similar. This may indicate a current weakness in attack methods that rely on detecting individual patterns or fingerprints (which are largely based on semantics and structure) memorized by the target model.

### RQ2. The Impact of Adaptation Techniques on Downstream Privacy Vulnerability.

**More Trainable Parameters Lead to Higher Data Membership Leakage Risk**. Figures 5 & 6 offer an overall performance comparison of different adaptation techniques on the adapted _OPT-6b_

Table 1: Comparison of different adaptation techniques in terms of attack vulnerability (measured by AUC-ROC) and downstream utility (evaluated by model accuracy _after_ adaptation) on the T5-Large/Llama-7B model and CorpClimate dataset. The adaptation methods are sorted by ascending order in terms of the amounts of trainable parameters. The shaded area indicates the reference results from training the model from scratch. For reference, the baseline test accuracy _before_ adaptation is 0.334 (pre-trained) or 0.187 (from scratch) for the T5-Large model, and 0.493 (pre-trained) or 0.234 (from scratch) for the Llama-7B model.

Figure 4: The comparison of samples between member data and misclassified non-member data from Llama7b fine-tuned over the SQL dataset using LoRA. We apply reference-based MIA  to conduct the membership inference attack.

model for the _CorpClimate_ dataset. The portion of trainable parameters (_TP_) relative to the overall model size is listed in brackets beside each adaptation technique, with techniques ordered in the legend by decreasing trainable parameters. The results show that the more parameters applied during adaptation, the higher the risks of downstream membership leakage. This aligns with the intuition that models with more trainable parameters tend to have a higher degree of freedom in downstream adaptation, potentially allocating more modeling capacity to over-memorizing their training data. While in-context learning approaches do not involve parameter updates and thus avoid the same overfitting risks, they are not free of privacy concerns. As shown by the non-trivial attack performance in Table 1, training data embedded within the language model through in-context adaptation can potentially be extracted through careful analysis of model outputs. This suggests that even parameter-free techniques require careful monitoring of the risk of privacy leakage.

**Different Adaptation Techniques May Cause Systematic Vulnerability Differences Due to Their Associated Attack Surfaces**. As illustrated in Table 1, different adaptation methods exhibit varying degrees of vulnerability to attack methods (measured by AUC-ROC) and post-adaptation utility (evaluated by accuracy). Specifically, adaptation techniques can introduce varying attack surfaces influenced by factors beyond the size of trainable parameters, such as the degree of model modification, the layers involved, and practical usage scenarios. For instance, methods like prompt-tuning and P-tuning primarily adjust input representations, potentially reducing the attack surface but offering moderate performance gains. In contrast, approaches like LoRA or full fine-tuning modify deeper layers, which may enhance flexibility but also increase the chances of embedding sensitive information within parameters. In-context learning, which relies on input data at runtime without parameter updates, is typically employed in black-box settings, where attackers have limited access to model internals, making white-box attack assumptions less applicable. These differences emphasize the importance of aligning adaptation techniques with both performance needs and privacy considerations.

Figure 5: Impact of different adaptation techniques for _attack performance_ measured by AUC-ROC. TP refers to the percentage of trainable parameters compared to the full-size model parameters.

Figure 6: Impact of different adaptation techniques for _model utility_ measured by accuracy. TP refers to the percentage of trainable parameters compared to the full-size model parameters.

### RQ3. Factors Affecting Privacy Vulnerability.

Size of Domain Data Applied for Training.Figure 5 demonstrates the empirical assessment of privacy leakage risks with varying amounts of available data for LLM adaptation. Utilizing more data tends to shift the LLM's modeling capability towards generalization rather than specialization, leaving less room for it to overfit to individual patterns, thus making the attack less effective. Moreover, using more data samples aligns with the utility objectives of product LLMs, as shown in Figure 6, which suggests the necessity of always obtaining more data for training.

Number of Fine-tuning Iterations.As can be observed from Figure 5, increasing the number of iterations generally enhances the effectiveness of attacks on the target models. This aligns with the interpretation that a higher degree of adaptation to the domain data, while steering the LLMs towards the target domain, inevitably causes the model to learn patterns overly tailored to individuals rather than the essential ones required for the task. While the privacy objective suggests applying a lesser degree of adaptation to the domain data, the utility objectives of product LLMs require a high degree of fitting to the target domain data. This misalignment of objectives necessitates more detailed adjustments during the deployment phase.

Target Model Size.From Figure 5, we observe that larger LLMs tend to exhibit increased downstream privacy vulnerability after adaptation. This may be attributed to their greater model capacity, which, while enabling the learning of more complex patterns and solving difficult tasks, can also compromise individual privacy, as the enhanced capacity allows these models to learn personal information that can lead to privacy issues. This dilemma between learnability (and thus utility) and privacy also requires more dedicated efforts for adjustments during the deployment phase.

## 6 Discussion & Limitations

While our results offer valuable insights into privacy-aware LLM development, several areas remain open for further exploration to deepen this research. One important direction is studying the impact of privacy-preserving training mechanisms, such as differentially private adaptation, which, while offering theoretical guarantees, may introduce utility trade-offs, particularly for complex tasks like domain-specific reasoning. Understanding how such strategies influence both membership inference risks and model utility, along with their trade-offs, is crucial for guiding practitioners. Another promising avenue is the co-design of privacy-preserving techniques with efficient adaptation methods, as developing these independently can result in suboptimal outcomes. An integrated approach may better balance privacy and utility, and identifying inherently robust adaptation techniques could reduce the need for costly post-hoc defenses. Additionally, auditing tools that search for or generate vulnerable samples could provide more precise estimates of privacy leakage and support ongoing monitoring of deployed models to maintain an appropriate privacy-utility balance.

Finally, it is essential to acknowledge the limitations of this work. While the evaluation focuses on domains intended to reflect real-world scenarios, it may not capture the full range of potential attack settings. Attackers with specialized knowledge or additional assumptions could uncover vulnerabilities beyond those examined. Moreover, the privacy risks identified are bound by the framework used, with results varying across datasets, model architectures, and operational contexts. Future work could expand this benchmark by incorporating new adaptation techniques, datasets, and attack strategies, progressively advancing the understanding of privacy risks across diverse settings.

## 7 Conclusions

In this work, we present a benchmark to assess the potential privacy leakage risks during adaptation techniques in LLMs. We examine the training data membership leakage risk in mainstream large language models based on encoder-decoder and decoder-only structures. Our comprehensive analysis illustrates the facets of privacy leakage risks during LLM adaptation, and we further propose a unified platform to measure these potential privacy risks. Our findings highlight the importance of developing privacy-preserving adaptation techniques with practical relevance.