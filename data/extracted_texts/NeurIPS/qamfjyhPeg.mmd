# Protected Test-Time Adaptation

via Online Entropy Matching: A Betting Approach

 Yarin Bar1

Shalev Shaer2

Yaniv Romano1,2

1Department of Computer Science, Technion--Israel Institute of Technology

2Department of Electrical and Computer Engineering, Technion--Israel Institute of Technology

{yarinbar,shalev.shaer}@campus.technion.ac.il

yromano@technion.ac.il

###### Abstract

We present a novel approach for test-time adaptation via online self-training, consisting of two components. First, we introduce a statistical framework that detects distribution shifts in the classifier's entropy values obtained on a stream of unlabeled samples. Second, we devise an online adaptation mechanism that utilizes the evidence of distribution shifts captured by the detection tool to dynamically update the classifier's parameters. The resulting adaptation process drives the distribution of test entropy values obtained from the self-trained classifier to match those of the source domain, building invariance to distribution shifts. This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy. Our approach combines concepts in betting martingales and online learning to form a detection tool capable of quickly reacting to distribution shifts. We then reveal a tight relation between our adaptation scheme and optimal transport, which forms the basis of our novel self-supervised loss. Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios.

## 1 Introduction

The deployment of machine learning (ML) models in real-world settings presents a significant challenge, as these models often encounter testing environments (target domains) that differ from their training, source domain . Consider, for example, an image recognition system employed for medical diagnostic support , where the quality of images acquired during testing deviates from the training data due to factors such as equipment degradation and novel illumination conditions. ML models are sensitive to such distribution shifts, often resulting in performance deterioration, which can be unexpected . Ultimately, we want predictive models to dynamically adapt to new testing environments without the laborious work required to annotate new, up-to-date labels.

Recognizing this pressing need, there has been a surge in the development of adaptation methodologies to enhance test-time robustness to shifting distributions . One commonly used approach involves jointly training a model on both the source and target domains . However, such train-time adaptation methods assume access to unlabeled test data from the target domains, limiting the ability to adapt the model to new domains that emerge during testing. To overcome this limitation, test-time adaptation techniques offer strategies that dynamically update the model parameters as new unlabeled test points become available. In particular, leading methodologies draw inspiration fromthe relationship between the entropy of estimated class probabilities--a measure of confidence--and model accuracy [35; 36; 37; 38; 39; 40; 41; 42; 43]. Empirical evidence highlights that lower entropy often corresponds to higher accuracy, encouraging the development of self-supervised learning approaches that adjust model parameters by minimizing the entropy loss or the cross-entropy through the assignment of pseudo- or soft-labels to test points [44; 45; 46; 47; 48].

While test-time adaptation techniques have shown promise in enhancing test accuracy under domain shifts, there is a caveat: minimizing entropy or related self-supervised loss functions _without control_ can lead to overconfident predictions, and may suffer from undesired, noisy model updates [49; 50; 51; 52]. In extreme cases, this approach may even cause the model to collapse and produce trivial predictions [53; 54]. Indeed, without careful implementation and tuning, these techniques may not improve--or could even reduce --the predictive performance in realistic settings.

In this paper, we present a novel, statistically principled approach to test-time adaptation via self-training. Our methodology is built upon two key pillars. First, we introduce an online statistical framework that monitors and detects distribution shifts in the test data influencing the models' predictions. We achieve this by sequentially testing whether the distribution of the classifier's entropy values obtained during testing deviates from the ones corresponding to the source domain. Armed with this monitoring tool, we then devise an online adaptation mechanism that leverages the accumulated evidence of distribution shifts to adaptively update model parameters. This mechanism drives the distribution of the self-trained classifier's entropy values, obtained on test data, to closely match the distribution of entropy values when applying the original model to the source domain. As a result, our proposed **Protected Online Entropy Matching** (POEM) method adapts the model on the fly in a controlled manner: in the absence of a distribution shift, our approach has a "no-harm" effect both on accuracy and calibration of the model, whereas under distribution shifts our experiments demonstrate an improvement of the test time accuracy, often surpassing state-of-the-art methods.

#### Contributions

(i) We present a sequential test for classification entropy drift detection, building on betting martingales [55; 56; 57] and online learning optimization [58; 59; 60] to provably attain fast reactions to shifting data. (ii) Inspired by , we show how to utilize the test martingale to analytically design a mapping function that transports the classifier entropies obtained at test time to resemble those of the source domain. Under certain assumptions, we establish connections between our online testing approach and optimal transport  as a mechanism for distribution matching. (iii) This observation sets the foundation of the entropy-matching loss function used in POEM. (iv) Numerical experiments in both continual and single-shift settings demonstrate that our approach is competitive and often outperforms strong benchmark methods that build on entropy minimization. These experiments are conducted using commonly used predictive models (ViT  and ResNet ) on standard benchmark datasets: ImageNet-C , CIFAR10-C, CIFAR100-C, and OfficeHome . A software package that implements our methods is available at https://github.com/yarinbar/poem.

## 2 Preliminaries

### Problem setup

To formalize the problem, consider a \(K\)-class classification problem with labeled training data \((X^{s}_{i},Y^{s}_{i})^{n}_{i=1}\) from a source domain, sampled i.i.d. from the source distribution \(P^{s}_{XY}\). Here, \(X^{s}^{d}\) represents observed covariates and \(Y^{s}\{1,,K\}\) is the corresponding label. During testing, we encounter a stream of points \(X^{t}_{j}\) with unknown labels \(Y^{t}_{j}\), sampled from an unknown target distribution \(P^{j}_{XY}\) that may shift over time \(j=1,2,\). To define the shifting mechanism, let \(T_{j}:^{d}^{d}\) be an unknown corruption/shift function, resulting in test instances \(X^{t}_{j}=T_{j}(X^{s})\) with \(X^{s}\) being a fresh sample from \(P^{s}\). For instance, in datasets such as ImageNet-C, corruptions involve modifications such as blur or changes in illumination applied to clean source images. While such transformations alter the marginal \(P^{j}_{X}\) distribution of the target domain, we assume that the conditional distributions of \(P^{s}_{Y|X}(Y^{s} X^{s})\) and \(P^{j}_{Y|X}(Y^{t}_{j}|T_{j}(X^{s}))\) are the same for all \(j=1,2,\).

### Related work: test-time adaptation via self-training

Given a pre-trained classifier \(f_{}\) trained on the source domain, leading test-time adaptation approaches build on the idea of self-training to update the model parameters sequentially. Denote the adapted classifier by \(f_{+}\), where \(\) represents the modification to the parameters of the original model \(\) obtained by self-training during testing. The adaptation process is typically initialized with \(j=1\) and \(_{1}=\), and involves the following set of steps:

1. Observe a fresh test point \(X_{j}^{t}\) and predict its unknown label using \(f_{+_{j}}(X_{j}^{t})\).
2. Update the model parameters in a direction that reduces the self-supervised loss, i.e., \[_{j+1}_{j}-_{}^{}(f_ {+_{j}}(X_{j}^{t})),\] where the hyper-parameter \(\) is the step size.
3. Set \(j j+1\) and return to step 1.

A common choice for \(^{}\) in test-time adaptation methods is the entropy loss:

\[^{}(f_{+}(x))=-_{y=1}^{K}f_{ +}(x)_{y}(f_{+}(x)_{y}),\] (1)

where \(f_{+}(x)_{y}\) is the \(y\)-th entry of the classifier's softmax layer; we omit the index \(j\) of \(\) for clarity.

While entropy minimization has been shown to enhance test-time robustness [36; 37; 38; 39; 40; 41; 42; 43], it is also prone to instabilities [49; 50; 51; 68]. For instance, enforcing \(f_{+}(x)_{y}=1\) for a fixed entry \(y\) minimizes \(^{}\), but it collapses the classifier to make trivial predictions. To alleviate this, various strategies have been proposed. For example, one approach is to avoid training on samples with high entropy , as high entropy often correlates with erroneous pseudo labels. Another example is the utilization of a fallback mechanism that resets the adapted model back to the original model \(f_{}\) when the average entropy becomes too small . A more detailed review of test-time adaptation methods is given in Appendix A. Importantly, this line of work underscores the limitations of entropy minimization and highlights the need to better control its effect. This aligns with the goal of our work, which offers a distinct, statistically grounded approach for test-time adaptation. While we also build on the classifier's entropy to form the adaptation mechanism, we could integrate any alternative self-supervised loss in our online distributional matching scheme.

### Testing by betting

A key component of our method is the proposal of an online test for distribution drift. The design of this test follows the framework of _testing-by-betting_. Intuitively, one can interpret this testing framework as participating in a fair game. We begin with initial toy money, and at each time step, we observe a new test point and place a bet against the null hypothesis we aim to test. If the bet turns out to be correct, our wealth increases by the money we risked in the bet; otherwise, we lose, and our wealth decreases accordingly. Mathematically, the wealth process is formulated as a non-negative martingale, where a successful betting scheme is reflected in a growing martingale (wealth) trajectory, offering progressively stronger statistical evidence against the null hypothesis. However, if the null hypothesis is true, the game must be fair in the sense that it is unlikely to significantly grow our initial capital, no matter how sophisticated out betting strategy may be. This implies that, under the null hypothesis, it is unlikely that the martingale will grow significantly beyond its initial value.

The testing-by-betting framework is widely used in sequential settings. Notable examples include: one and two sample tests [70; 71], independence and conditional independence tests [72; 73; 71], exchangeable tests [74; 75; 76; 77; 78], and more [79; 69; 70; 71; 72]. This framework is also used for change-point detection and testing for uniformity [93; 94; 95; 96; 97], related to our drift detection problem. We draw inspiration from the protected probabilistic regression approach  that combines the probability integral transform and betting martingales to improve the robustness of a cumulative distribution function (CDF) estimator to distribution shift in the data. The experiments in  illustrate this method's ability to enhance the accuracy of a regression model, where this protection scheme assumes access to new up-to-date labels. In contrast, we focus on a completely different setup where the labels of the test points are unknown, showing how the protected regression approach can be generalized to form a self-supervised loss function. In turn, we introduce two key contributions to test-timeadaptation via testing-by-betting. First, we present an adaptive online learning technique to optimize the betting mechanism. Second, we pioneer the application of testing-by-betting in this domain.

## 3 Proposed method: protected online entropy matching (Poem)

### Preview of our method

Let the random variable \(Z^{s}=^{}(f_{}(X^{s}))\) be the entropy value of the original classifier applied to a fresh sample \(X^{s}\) from the source domain. We refer to this variable as the _source entropy_. In addition, denote by \(Z^{t}_{j}=^{}(f_{+_{j}}(X^{t}_{j}))\), \(j=1,2,\), a sequence of entropy values generated by the updated model, evaluated on a stream of unlabeled test data. We refer to \(Z^{t}\) as the _target entropy_. Our proposal uses the source and target entropies both to detect distribution shifts and adapt the model to new testing environments without relying on up-to-date labeled data. The rationale behind our method is as follows: when test data is sampled from the source distribution, there will be no deviation between the source and target entropies, implying that there is no need to adapt the model. However, statistical deviations between the source entropies \(Z^{s}\) and target entropies \(Z^{t}\) can indicate that the model encounters test data different from the training distribution. This motivates us to introduce a self-training framework that encourages the model to generate test-time entropies \(Z^{t}\) that closely resemble the source entropies \(Z^{s}\) to build invariance to shifting data.

To achieve this goal, we utilize the testing-by-betting approach and formulate our adaptation scheme as a game, in which we start with initial toy money and proceed as follows.

1. Place a bet against the null hypothesis that the unknown classifier entropy \(Z^{t}_{j}\) of the upcoming test point \(X^{t}_{j}\) will follow the same distribution as the source entropies \(Z^{s}\).
2. Observe the test point \(X^{t}_{j}\), predict its label using the model \(f_{+_{j}}\), and compute the classifier's entropy \(Z^{t}_{j}=^{}(f_{+_{j}}(X^{t}_{j}))\).
3. Given \(Z^{t}_{j}\), reveal the outcome of the bet using a betting function. If the bet is successful, increase the accumulated wealth; otherwise, decrease it.
4. Leverage the betting function to obtain an adapted pseudo-entropy value \(_{j}\) that better matches the distribution of \(Z^{s}\). The intuition here is that we derive \(_{j}\) in a way that would reduce the toy money we would have gained if we had used the same betting strategy on \(_{j}\).
5. Update the model parameters: obtain \(_{j+1}\) by taking a gradient step that reduces the self-supervised matching loss:2 
\[^{}(Z^{t}_{j},_{j})=(^{}(f _{+_{j}}(X^{t}_{j}))-_{j})^{2}.\] (2)
6. Update the betting strategy for the next round and return to Step 1.

In the following sections, we describe in detail each component of the proposed adaptation scheme. Before proceeding, however, we pause to highlight the advantages of the matching loss (2) over entropy minimization.

### Motivating example: entropy minimization vs. entropy matching

To facilitate the exposition of the proposed loss, it is useful to consider a toy, binary classification example with a one-dimensional input \(X\) in which we have oracle access both to the source \(P^{s}_{XY}\) and a fixed target distribution \(P^{t}_{XY}\) that does not vary over time. We commence by generating training data from \(P^{s}_{XY}\), where \(P(Y=1)=P(Y=-1)\) and \(P^{s}_{X|Y}=(Y^{s},1)\). See Figure 1 for an illustration of the source distribution. Throughout this experiment, we set the pre-trained Gaussian classifier \(f_{}\) to be the Bayes optimal one for which \(=0\), and during test-time we optimize the parameter \(\) of the updated classifier. Since \(=0\), in this case \(f_{+}\) is simplified to \(f_{}\). Further implementation details are provided in Appendix F.

As mentioned before, one of the advantages of our approach is its "no-harm" effect, i.e., when \(P^{s}_{XY}=P^{t}_{XY}\) we ideally want to keep the decision boundary of the classifier intact. The red curve in Figure 1 illustrates the entropy matching risk \(_{X^{t}}[^{}(Z,)]\) as a function of the classifier parameter \(\). In this synthetic case, the ideal entropy matching risk can be evaluated since we have access to the generating distribution: we can obtain the ideal pseudo-entropy \(^{t}\), given by \(=F_{s}^{-1}(F_{t}(Z^{t};f_{}))\), where \(F_{s}\) and \(F_{t}\) are the CDF functions of the source and target entropy values, respectively; the formula above is nothing but the optimal transport map. Of course, in the practical online setting we consider in this work, \(F_{t}\) is unknown and varies over time. In fact, this is also true in the case studied here as the distribution of \(Z^{t}=f_{}(X^{t})\) varies with \(\), highlighting the importance of our online, adaptive testing procedure. Indeed, the \(\) obtained by our online adaptation scheme (POEM) minimizes the entropy matching risk and remains close to 0, as desired.

Meanwhile, the black curve in Figure 1 illustrates the values of the entropy risk \(_{X^{t}}[^{}(Z^{t})]\) for varying \(\). In contrast with our approach, the \(\) that minimizes the entropy risk is far from the optimal classifier. This approach results in a collapse towards a trivial classifier that always predicts \(-1\), regardless of the value of \(X^{t}\). Moving to an out-of-distribution scenario, where we consider test data sampled from a shifted version of the two Gaussians such that \(Y^{t}=Y^{s}\) and \(X^{t}=X^{s}+1\). Following the bottom panel in Figure 1, it is evident that by minimizing the proposed entropy matching risk, the accuracy of the original (not adapted) classifier is effectively restored. In contrast, the entropy minimization paradigm once again collapses the model to make trivial predictions.

### Online drift detection

We now turn to introduce a rigorous monitoring tool that is capable of detecting whether the distribution of the adapted classifier's entropy values \(^{t}\), obtained at test time, deviate from \(Z^{s}\)--the source entropies obtained by applying the original model to the source data. To detect such shifts, we assume that we have access to \(F_{s}\), the CDF of the source entropy values \(Z^{s}=^{}(f_{}(X^{s}))\). In practice, we estimate this CDF using holdout unlabeled samples \(X^{s}\) from the source distribution. Armed with \(F_{s}\), we then apply the probability integral transform , allowing us to convert any sequence of i.i.d. entropy values from the source distribution \(Z^{s}_{1},Z^{s}_{2},\) into a sequence of i.i.d. uniform random variables \(F_{s}(Z^{s}_{1}),F_{s}(Z^{s}_{2}),\) on the \(\) interval. Therefore, if we observe a sequence \(F_{s}(Z^{t}_{1}),F_{s}(Z^{t}_{2}),\) of i.i.d. uniform variables at test time, we can infer that the target entropy distribution matches the source entropy distribution. Thus, if the sequence of transformed variables \(F_{s}(Z^{t}_{1}),F_{s}(Z^{t}_{2}),\) deviates from the uniform distribution, we can infer that the corresponding target domain samples \(Z^{t}_{1},Z^{t}_{2},\) differ from the source distribution. This observation lies at the core of our monitoring tool .

Specifically, we leverage the testing-by-betting approach to design a sequential test for the following null hypothesis:

\[_{0}:u_{j} F_{s}(Z^{t}_{j}),  j,\] (3)

Figure 1: **Demonstration of the advantage of entropy matching on toy binary classification problem with Gaussian data.** The top panel represents an in-distribution setup in which \(P^{t}_{XY}=P^{s}_{XY}\). The bottom panel illustrates an out-of-distribution setup, obtained by shifting the two Gaussians. The entropy matching (red) and entropy minimization (black) risks are presented as a function of \(\). The dashed yellow line presents the decision boundary of the pre-trained classifier. The points marked by stars correspond to the decision boundary of the adapted classifiers.

where \(Z^{t}_{j}=^{}(f_{+_{j}}(X^{t}_{j}))\). In words, we continuously monitor the sequence of random variables \(u_{1},u_{2},\) and test whether these deviate from the uniform null. We do so by formulating a test martingale, defined as follows.

**Definition 1** (Test Martingale).: _A random process \(\{S_{j}:j,S_{0}=1\}\) is a test martingale for the null hypothesis \(_{0}\) if it satisfies the following:_

1. \(S_{j} 0 j\)_._
2. \(\{S_{j}:j_{0}\}\) _is a martingale under_ \(_{0}\)_, i.e.,_ \(_{_{0}}[S_{j} S_{1},,S_{j-1}]=S_{j-1}\)_._

The martingale can be thought of as the wealth process in the game-theoretical interpretation of the test, obtained by betting toy money against \(_{0}\) as new data points arrive. We initialize this game with \(S_{0}=1\), and update the wealth process as follows :

\[S_{j}(u_{j}) S_{j-1} b(u_{j}) b(u_{j})= 1+_{j}(u_{j}-0.5).\] (4)

Above, \(b(u)\) is the _betting function_. The _betting variable_\(_{j}[-2,2]\) controls how aggressive the bet is, and it can be determined based on past observations \(u_{1},,u_{j-1}\), as we detail later in this section. However, before introducing our strategy to update \(_{j}\) over time, we should first discuss the properties of the betting function \(b(u_{j})\). The idea behind this choice is that we sequentially test whether the sequence of \(u_{1},,u_{j}\) observed up to time step \(j\) has mean \(0.5\). Indeed, if the null is true, \(_{_{0}}[u]=0.5\) and thus \(_{_{0}}[b(u)]=1\). As a result, under the null, the martingale is unlikely to grow significantly beyond its initial value--this is a consequence of Ville's inequality; see Appendix D. However, if the null is false, we can gather evidence against the uniform null hypothesis by placing more aggressive bets, especially when past observed values \(u_{1},,u_{j-1}\) deviate significantly from \(0.5\). This highlights the role of \(_{j}\), controlling the value and direction of our wagers in each round, betting on whether \(u_{j}\) would be over/under \(0.5\). Following (4), when \(_{j}\) and \((u_{j}-0.5)\) have the same sign, we win the bet and obtain \(b(u_{j})>1\). This implies that our capital increase as \(S_{j}(u_{j}):=S_{j-1} b(u_{j})\). Notice that, in this case, a larger \(_{j}\) (in absolute value) will allow us to increase the capital more rapidly, resulting in a more powerful test. However, if \(_{j}\) and \((u_{j}-0.5)\) have different signs, we lose the bet and obtain \(b(u_{j})<1\). Here, a larger \(_{j}\) would incur a more significant loss of capital. The challenge in choosing \(_{j}\) lies in the restriction that \(_{j}\) can only be determined based on past experience, i.e., we must set its value without looking at the new \(u_{j}\). This restriction is crucial to ensure the validity of the martingale, as detailed in the following proposition.

**Proposition 1**.: _The random process presented in (4) is a valid test martingale for \(_{0}\) (3)._

The proof is given in Appendix C.1; it is a well-known result, see, e.g., . This property is crucial to form the proposed online distributional matching mechanism, introduced in the next section. Appendix D provides further details on how the test martingale is used for distribution shift detection.

We now turn to present an adaptive approach to set the betting variable \(_{j}\) in a manner that enables powerful detection of drifting target entropies. This is especially important given the dynamic nature of both the target data and the continuous, online updates of the model. To achieve this, we adopt an online learning technique to learn \(_{j}\) from past observations, with the goal of maximizing the wealth by minimizing the negative log of the wealth process up to step \(j\):

\[-(S_{j}(u_{j}))=-_{=1}^{j}b_{}(u_{})=-_{=1}^ {j}(b_{}(u_{}))=-_{=1}^{j}(1+_{}(u_{} -0.5)).\] (5)

This formulation allows us to learn how to predict \(_{j}\) using past samples via gradient descent .

Specifically, our optimization approach relies on the scale-free online gradient descent (SF-OGD) algorithm . Importantly, extending SF-OGD to our setting is not straightforward, since \(_{j}\) must be in the range of \([-2,2]\) to form a valid test martingale. In the interest of space, we present this algorithm and its theoretical analysis in Appendix B and only highlight here its key feature. SF-OGD allows us to attain an anytime regret guarantee, which is presented formally in Theorem 1 of the Appendix. This guarantee bounds the difference between the negative log of the wealth process (i) obtained by the predicted \(_{t}\) over time horizon \(1 t j\), and (ii) obtained by the best betting variable \(^{}\) that can only be calculated in hindsight, after looking at the data up to time \(1 t j\). Informally, our theory shows this regret is bounded by \(c\) for all \(1 t j\), where the constant \(c\) depends on the problem parameters; it is formulated precisely in Theorem 1. In turn, the anytime regret guarantee confirms that our SF-OGD approach effectively learns \(_{j}\), capturing the dynamic changes of both the target distribution and the model \(f_{+_{j}}\) in a fully online setting.

### Online model adaptation

Having established a powerful betting strategy, we now turn to show how to transform the test martingale \(\{S_{j}:j\}\) into a sequence of adapted pseudo-entropy values \(_{1},_{2},...\) that better match the distribution of \(Z^{s}\). In what follows, we describe an algorithm to obtain \(_{j}\), which draws inspiration from , and then connect this procedure to optimal transport.

Our adaptation scheme leverages the fact that any valid betting function is essentially a likelihood ratio . This property implies that our betting function \(b(u_{j})=1+_{j}(u_{j}-0.5)\) satisfies

\[b(u_{j})=)}{dG(u_{j})},\] (6)

where \(dQ(u_{j})\) and \(dG(u_{j})\) are the densities of _some_ alternative distribution \(Q\) and the null distribution \(G\), respectively. In our case, the null distribution \(G\) is the uniform distribution \((0,1)\), and the alternative distribution \(Q\) can be intuitively thought of as an approximation of the unknown target entropy's CDF; we formalize this intuition hereafter. Leveraging this likelihood ratio interpretation, we follow  and extract the alternative distribution \(Q\) by re-writing (6) as \(dQ(u_{j})=b(u_{j}) dG(u_{j})\) and computing the integral:

\[Q(u_{j})=_{0}^{u_{j}}b(v)dG(v)dv=_{0}^{u_{j}}b(v) 1 dv= _{j} u_{j}^{2}+(1-}{2}) u_{j}.\] (7)

Above, we used the fact that the null density is \(dG(v)\) equals \(1\) over the support \(\). Having access to \(Q\), we can compute the adapted \(_{j}:=Q(u_{j})\) that can be intuitively interpreted as the result of applying the probability integral transform to \(Z_{j}^{t}\) using the estimated target entropy CDF. With this intuition in place, we can further convert \(_{j}\) into a pseudo-entropy value \(_{j}^{t}\) that better matches the distribution of the source entropies. This is obtained by applying the inverse source CDF to \(_{j}\), resulting in \(_{j}=F_{s}^{-1}(Q(u_{j}))\). Observe that we assume here that \(F_{s}\) is invertible, however, in practice, we compute the pseudo-inverse of \(F_{s}\).

To connect the adaptation scheme presented above to the optimal transport map between the target and source entropies, it is useful to consider an ideal case where we use the log-optimal bet for testing a point null . In our case, the null hypothesis is that the distribution of the source entropies \(Z^{s}\) and target entropies \(Z_{j}^{t}\) is the same, which implies \(_{0}\) in (3). Following , the optimal bet for our null is the true likelihood ratio, formulated as

\[b_{Z}^{}(Z_{j}^{t})^{j}(Z_{j}^{t})}{dF_{s}(Z _{j}^{t})},\] (8)

where \(F_{t}^{j}\) is the CDF of the target entropy \(Z_{j}^{t}\). To align with (6), we can equivalently write \(b_{Z}^{}(Z_{j}^{t})\) as a betting function that gets \(u_{j}\) as an input [61, Lemma 1]:

\[b_{Z}^{}(Z_{j}^{t})=b_{Z}^{}(F_{s}^{-1}(u_{j}))=^{j}(F_{s}^{-1}(u_{j}))}{dF_{s}(F_{s}^{-1}(u_{j}))} }(u_{j})}{dG^{}(u_{j})}=b_{u}^{}(u_{j}).\] (9)

Notably, this optimal betting function is infeasible to compute in practice, as \(F_{t}^{j}\) is unknown. Yet, it implicitly suggests that more powerful betting functions could result in a better estimate of the target entropy CDF. Also, the optimal betting function reveals an important property of our adaptation scheme, formally given below.

**Proposition 2**.: _Let \(X_{j}^{t}\) be a fresh sample from the target domain with its corresponding \(Z_{j}^{t}=^{}(f_{+}(X_{j}^{t}))\) and \(u_{j}=F_{s}(Z_{j}^{t})\). Assume \(F_{s}\) is invertible and \(Z_{j}^{t}\) is continuous, and suppose the betting function represents the true likelihood ratio (9). Then, the adapted \(_{j}^{t}=F_{s}^{-1}(Q^{}(u_{j}))\) is the optimal transport map from target to source entropies with respect to the Wasserstein distance._

The proof of this proposition builds on [61, Lemma 1] and is provided in Appendix C.4. This result highlights that our online, martingale-based adaptation scheme is grounded in optimal transport principles. This, in turn, provides a principled way to minimize the discrepancy between probability distributions. Leveraging this connection, the entropy matching loss function (2), which we employ to self-train the model, can be understood as minimizing the discrepancy between the entropydistributions of the source and target domains. This implies that our loss function aligns the model's predictions across these domains. This connection also explains the "no-harm" effect of the proposed loss. When \(P_{XY}^{*}=P_{XY}^{t}\) we get that \(Q^{}(u_{j})=G^{}(u_{j})=u_{j}\) in the ideal case of (9), implying that \(_{j}^{t}=Z_{j}^{t}\). In practice, considering the betting function from (4), we anticipate that \(_{j}\) will be close to zero thanks to our online optimization scheme, which, in turn, results in \(Q(u_{j}) u_{j}\) in (7).

### Putting it all together

Algorithm 2 in the Appendix summarizes the entire adaptation process of POEM. This algorithm starts by computing the empirical CDF \(_{s}\) of the source entropies to estimate \(F_{s}\), using unlabeled holdout samples from the source domain (line 6). The betting and pseudo-entropy estimation steps are presented in lines 12-14. Observe that in line 14 we use the pseudo-inverse of \(_{s}\) to obtain \(_{j}\). The algorithm then proceeds to adapt the classifier's parameters in a direction that minimizes our self-supervised loss (line 15). Specifically, we only update the parameters of the normalization layers \(\) of the classifier \(f_{+}\), which is a common practice in the test-time adaptation literature [41; 42; 43]. The self-training step is done by minimizing a variation of the entropy matching loss \(^{}\) (2):

\[^{}_{}(Z_{j}^{t},_{j})=^{}( Z_{j}^{t},_{j})^{t}<]}{\{2(Z_{j}^{t}- )\}},\ \ \ \ Z_{j}^{t}=^{}(f_{+_{j}}(X_{j}^{t})).\] (10)

The above loss function includes an additional sample-filtering \([Z_{j}^{t}<]\) and sample-weighting \(1/\{2(Z_{j}^{t}-)\}\) components, where \(>0\) is a pre-defined thresholding parameter. The sample-filtering idea is widely used in this literature [42; 43], as the predictions of samples with high entropy examples tend to be inaccurate. Aligning with this intuition, the sample-weighting gives a higher weight to samples with low entropies . Finally, we predict a new betting parameter \(_{j}\) for the next iteration by applying an SF-OGD step (line 16).

## 4 Experiments

We conduct a comprehensive evaluation of POEM across a diverse range of datasets and scenarios commonly used in test-time adaptation literature. Our experiments span ImageNet, ImageNet-C, CIFAR10-C, and CIFAR100-C datasets for evaluating the robustness to shifts induced by corruptions, and the Office-Home dataset for domain adaptation. We study the performance of our method in both single-shift and continual-shift settings. Our evaluation demonstrates that POEM is highly competitive with leading baseline test-time adaptation methods in terms of accuracy and runtime. In the interest of space, this section focuses on the results for the ImageNet dataset, as it is the most challenging one among those considered. Details and results for the experiments on CIFAR and Office-Home datasets are provided in Appendices F.3 and F.4, respectively.

Throughout this section we use the test set of ImageNet to form our in-distribution dataset, and utilize ImageNet-C--which contains 15 different types of corruptions at five increasing severity levels--to simulate various out-of-distribution scenarios. Notably, since the images in ImageNet-C are variations of the same images from the ImageNet test set, our experiments simulate a realistic out-of-distribution test set by including only a single corrupted version of each image. To demonstrate the versatility of POEM, we consider two pre-trained classifiers \(f_{}\) of different architectures: Vision Transformer (ViT)  with layer norm (LN) and ResNet50  with group norm (GN). We compare POEM to four leading entropy minimization methods--TENT, EATA, SAR, and COTTA--using code provided by the authors. Importantly, all adaptation methods update only the normalization parameters (LN/GN) of the model, ensuring a fair comparison. Following , we employ a fully online setting with a batch size of 1, in which the model is updated after observing a new test sample; see Appendix F.2 for implementation details and choice of hyper-parameters. In the interest of space, we defer the results obtained by the ResNet classifier to Appendix F.2.1 and focus here on the results obtained by the ViT model.

Continual shiftsInspired by [97; 98], we evaluate our approach in a continual setup in which sudden distribution shifts occur during testing. To simulate this, we create a test set of 15,000 samples by randomly selecting 1,000 samples from each corruption type at a fixed severity level (a corruption segment) and concatenating all 15 segments to form the test data. We apply all adaptation methodsin combination with a ViT model and present the results in Figure 2. Following the bottom panel in that figure, one can see that POEM achieves higher accuracy than all of the benchmark methods. Next, we investigate how quickly our method adapts the model to new shifts by varying the segment size of each corruption. As shown in Figure 2 (bottom right), POEM exhibits faster adaptation than the baseline methods. It successfully enhances the accuracy of the pre-trained model with as few as 100 examples per corruption (test set of size 1,500) as well as with longer adaptation using 2,000 examples per corruption (test set of size 30,000). Finally, we explore another realistic scenario of continual adaptation by varying the severity level every 1,000 samples while keeping a fixed corruption type. The bottom left and center panels in Figure 2 show that POEM outperforms the best baseline method (EATA) in this setting as well. Lastly, similar experiments conducted with a ResNet model are presented in Figure 5 in the Appendix, showing that our method attains faster adaptation and superior accuracy than the baseline methods.

Single shiftWe now consider a scenario with a single corruption type of a fixed severity level, which follows . Table 1 summarizes the average results across all corruptions for severity level 5, demonstrating that POEM achieves an average accuracy of 67.36% for the ViT model, outperforming the best baseline method (EATA) with an absolute average accuracy gap of 3.22%. A detailed breakdown by corruption type for each classifier is provided in Table 2 in the Appendix. Notably, POEM outperforms all benchmark methods on all corruption types for ViT, while achieving higher test accuracy in 9 out of the 15 corruption types for the ResNet model.

In distributionIn this setting, we apply all methods on the validation set of the ImageNet dataset. Following Table 3 in the Appendix, all the methods maintain a similar accuracy as the original model, however, the baseline methods tend to increase the expected calibration error (ECE)  and unnecessarily modify the model's parameters, as measured by \(\|\|_{2}^{F}\). In contrast, following Figure 3 (left panel), POEM exhibits minimal changes both for ECE and model parameters, as desired. Figure 6 in the Appendix leads to similar conclusions for the ResNet model.

Additional experiments on ImageNet, including ablation studyThe right panel of Figure 3 plots the value of the betting parameter \(\) over time, for both in- and out-of-distribution scenarios. Observe how \(\) remains near zero under the in-distribution setting, explaining the minimal change in accuracy, ECE, and model's parameters \(\|\|_{F}^{2}\), presented in the left panel of Figure 3. By contrast, when

Figure 2: **Continual test-time adaptation on ImageNet-C with a ViT model. Top: Per-corruption accuracy with a corruption segment size of 1,000 examples. Results are obtained over 10 independent trials; error bars are tiny. Bottom left and center: Severity shift—low (1) to high (5) and back to low (left), and high (5) to low (1) and back to high (center). To improve the readability here, we only present POEM, the best-performing baseline method (EATA), and the no-adapt approach. Bottom right: Mean accuracy under continual corruptions as a function of the corruption segment size.**

considering out-of-distribution test data of a single shift, we can see that \(\) is high at the beginning and gradually reduces over time, indicating that the self-training process adapts the model to the new environment. A similar behavior is observed for the ResNet model; see Figure 6 in the Appendix. This conclusion is further supported by Figure 7 in the Appendix, showing that the CDF of the target entropies of the adapted ResNet model nearly matches the CDF of the source entropies. In Figure 4 of the Appendix we show how the martingale can powerfully detect shifts--even a minor one (brightness with severity level 1)--and also show how our adaptation mechanism gradually limits the martingale's growth. Lastly, we conduct an ablation study, comparing the test accuracy of POEM using two loss functions: \(^{}\) (2) and \(^{}\) (10). Table 4 in the Appendix presents these results for a single shift scenario, averaged over 15 corruptions of ImageNet-C at the highest severity level. Both losses improve the original model accuracy, with \(^{}\) showing better adaptation performance.

Experiments on CIFAR-10C, CIFAR100-C, and OfficeHome datasets, sensitivity study, and runtime comparisonsAppendices F.3 and F.4 present experiments on these additional datasets, leading to similar conclusions about the competitive adaptation accuracy of POEM compared to baseline methods. Notably, Appendix F.3 includes experiments on both relatively short and long adaptation streams, with lengths of 15,000 and 112,500 samples, respectively. These experiments also include a sensitivity study on the learning rate \(\) used for self-training the model, showing that POEM exhibits greater stability across different learning rate values compared to SAR and TENT. Additionally, these experiments show that POEM's runtime is comparable to TENT and EATA, and faster than SAR.

## 5 Discussion

We introduced a novel, martingale-based approach for test-time adaptation that drives the test-time entropies of the self-trained model to match the distribution of source entropies. We validated our approach with numerical experiments, demonstrating that: (i) under in-distribution settings, POEM maintains the performance of the source model while avoiding overconfident predictions, a crucial advantage over entropy minimization methods; (ii) in relatively short out-of distribution test periods, our approach achieves faster adaptation than entropy minimization methods, which is attributed to our betting scheme that quickly reacts to distribution shifts; and (iii) in extended test periods, POEM achieves comparable adaptation performance and stability to strong baseline methods.

One limitation of our method is the requirement for holdout unlabeled source domain data to estimate the source CDF. Notably, this CDF is pre-computed offline, and at test time we do not require additional access to source data, similar to EATA's requirements. Another limitation of our approach is the choice of hyperparameters, particularly the self-training learning rate \(\). However, our sensitivity study showed that our method is fairly robust to this choice, especially compared to baseline methods. Lastly, similar to other experimental works, we anticipate that POEM may fail to improve accuracy in settings that we have not explored, especially when facing an aggressive shift. Yet, our monitoring tool can detect such distribution shifts, which is an important mechanism that does not appear in other test-time adaptation methods.

Future directions are discussed in Section G of the Appendix. Lastly, we note that while the goal of this paper is to enhance the robustness of ML to unseen environments, there are many potential societal consequences of our method, similar to other works that aim to advance this field.

Figure 3: **In-distribution experiment on ImageNet (left panel): calibration error (ECE ) versus \(\|\|_{F}^{2}\)—a metric that evaluates the classifier’s parameters deviation from the original ViT model. Lower values on both axes are better. Results are averaged across 10 independent trials; standard errors and accuracy of each method are reported in Table 3 in the appendix. The behavior of the betting parameter (right panel): the value of \(\) is presented as a function of time for both in- and out-of-distribution experiments (a single shift, two severity levels).**

Acknowledgments and Disclosure of Funding

This research was supported by the ISRAEL SCIENCE FOUNDATION (grant No. 729/21). Y. R. thanks the Career Advancement Fellowship, Technion.