# SPA: A Graph Spectral Alignment Perspective for Domain Adaptation

Zhiqing Xiao\({}^{13}\), Haobo Wang\({}^{23}\), Ying Jin\({}^{4}\), Lei Feng\({}^{5}\), Gang Chen\({}^{13}\), Fei Huang\({}^{6}\), Junbo Zhao\({}^{13}\)

\({}^{1}\) College of Computer Science and Technology, Zhejiang University

\({}^{2}\) School of Software Technology, Zhejiang University

\({}^{3}\) Key Lab of Intelligent Computing based Big Data of Zhejiang Province, Zhejiang University

\({}^{4}\) CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong

\({}^{5}\) School of Computer Science and Engineering, Nanyang Technological University

\({}^{6}\) Alibaba Group

{zhiqing.xiao, wanghaobo, cg, j.zhao}@zju.edu.cn,

{sherryying003, lfengqaq, feirhuang}@gmail.com

Corresponding author.

###### Abstract

Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to extend the in-domain model to the distinctive target domains where the data distributions differ. Most prior works focus on capturing the inter-domain transferability but largely overlook rich intra-domain structures, which empirically results in even worse discriminability. In this work, we introduce a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The core of our method is briefly condensed as follows: (i)-by casting the DA problem to graph primitives, SPA composes a coarse graph alignment mechanism with a novel spectral regularizer towards aligning the domain graphs in eigenspaces; (ii)-we further develop a fine-grained message propagation module -- upon a novel neighbor-aware self-training mechanism -- in order for enhanced discriminability in the target domain. On standardized benchmarks, the extensive experiments of SPA demonstrate that its performance has surpassed the existing cutting-edge DA methods. Coupled with dense model analysis, we conclude that our approach indeed possesses superior efficacy, robustness, discriminability, and transferability. Code and data are available at: https://github.com/CrownX/SPA.

## 1 Introduction

Domain adaptation (DA) problem is a widely studied area in computer vision field [20; 19; 73; 65; 40], which aims to transfer knowledge from label-rich source domains to label-scare target domains where dataset shift  or domain shift  exists. The existing unsupervised domain adaptation (UDA) methods usually explore the idea of learning domain-invariant feature representations based on the theoretical analysis . These methods can be generally categorized several types, _i.e._, moment matching methods [44; 51; 70], and adversarial learning methods [18; 66; 19; 52].

The most essential challenge of domain adaptation is how to find a suitable utilization of intra-domain information and inter-domain information to properly align target samples. More specifically, it is a trade-off between discriminating data samples of different categories within a domain to the greatest extent possible, and learning transferable features across domains with the existence of domain shift. To achieve this, adversarial learning methods implicitly mitigate the domain shift by driving the feature extractor to extract indistinguishable features and fool the domain classifier.

The adversarial DA methods have been increasingly developed and followed by a series of works [14; 45; 8]. However, it is very unexpected that the remarkable transferability of DANN  is enhanced at the expense of worse discriminability [10; 38].

To mitigate this problem, there is a promising line of studies that explore graph-based UDA algorithms [9; 87; 54]. The core idea is to establish correlation graphs within domains and leverage the rich topological information to connect inter-domain samples and decrease their distances. In this way, self-correlation graphs with intra-domain information are constructed, which exhibits the homophily property whereby nearby samples tend to receive similar predictions . There comes the problem that is how to transfer the inter-domain information with these rich topological information. Graph matching is a direct solution to inter-domain alignment problems. Explicit graph matching methods are usually designed to find sample-to-sample mapping relations, requiring multiple matching stages for nodes and edges respectively , or complicated structure mapping with an attention matrix . Despite the promise, we find that such a point-wise matching strategy can be restrictive and inflexible. In effect, UDA does not require an exact mapping plan from one node to another one in different domains. Its expectation is to align the entire feature space such that label information of source domains can be transferred and utilized in target domains.

In this work, we introduce a novel graph spectral alignment perspective for UDA, hierarchically solving the aforementioned problem. In a nutshell, our method encapsulates a coarse graph alignment module and a fine-grained message propagation module, jointly balancing inter-domain transferability and intra-domain discriminability. Core to our method, we propose a novel spectral regularizer that projects domain graphs into eigenspaces and aligns them based on their eigenvalues. This gives rise to coarse-grained topological structures transfer across domains but in a more intrinsic way than restrictive point-wise matching. Thereafter, we perform the fine-grained message passing in the target domain via a neighbor-aware self-training mechanism. By then, our algorithm is able to refine the transferred topological structure to produce a discriminative domain classifier.

We conduct extensive evaluations on several benchmark datasets including DomainNet, OfficeHome, Office31, and VisDA2017. The exprimental results show that our method consistently outperforms existing state-of-the-art domain adaptation methods, improving the accuracy on 8.6% on original DomainNet dataset and about 2.6% on OfficeHome dataset. Furthermore, the comprehensive model analysis demonstrates the the superiority of our method in efficacy, robustness, discriminability and transferability.

## 2 Preliminaries

Problem Description.Given source domain data \(_{s}=\{(x_{i}^{s},y_{i}^{s})\}_{i=1}^{N_{s}}\) of \(N_{s}\) labeled samples associated with \(C_{s}\) categories from \(_{s}_{s}\) and target domain data \(_{t}=\{x_{i}^{t}\}_{i=1}^{N_{s}}\) of \(N_{t}\) unlabeled samples associated with \(C_{t}\) categories from \(_{t}\). We assume that the domains share the same feature and label space but follow different marginal data distributions, following the Covariate Shift ; that is, \(P(_{s}) P(_{t})\) but \(P(_{s}_{s})=P(_{t} _{t})\). Domain adaptation just occurs when the underlying distributions corresponding to the source and target domains in the shared label space are different but similar enough to make sense the transfer . The goal of unsupervised domain adaptation is to predict the label \(\{y_{i}^{t}\}_{i=1}^{N_{t}}\) in the target domain, where \(y_{i}^{t}_{t}\), and the source task is \(_{s}_{s}\) assumed to be the same with the target task \(_{t}_{t}\).

Adversarial Domain Adaptation.The family of adversarial domain adaptation methods, _e.g._, Domain Adversarial Neural Network (DANN)  have become significantly influential in domain adaptation. The fundamental idea behind is to learn transferable features that explicitly reduce the domain shift. Similar to standard supervised classification methods, these approaches include a feature extractor \(F()\) and a category classifier \(C()\). Additionally, a domain classifier \(D()\) is trained to distinguish the source domain from the target domain and meanwhile the feature extractor \(F()\) is trained to confuse the domain classifier and learn domain-invariant features. The supervised classification loss \(_{cls}\) and domain adversarial loss \(_{adv}\) are presented as below:

\[_{cls} =_{(x_{i}^{s},y_{i}^{s})_{S} }_{ce}(C(F(x_{i}^{s})),y_{i}^{s})\] (1) \[_{adv} =_{F(x_{i}^{t})_{s}} [D(F(x_{i}^{s}))]\] \[+_{F(x_{i}^{t})_{t}} [1-D(F(x_{i}^{t}))]\]where \(_{s}\) and \(_{t}\) denote the induced feature distributions of \(_{s}\) and \(_{t}\) respectively, and \(_{ce}(,)\) is the cross-entropy loss function.

## 3 Methodology

In this section, we will give a specific introduction to our approach. The overall pipeline is shown in Figure 1. Our method is able to effectively utilize both intra-domain and inter-domain relations simultaneously for domain adaptation tasks. Specifically, based on our constructed dynamic graphs in Section 3.1, we propose a novel framework that utilizes graph spectra to align inter-domain relations in Section 3.2, and leverages intra-domain relations via neighbor-aware propagation mechanism in Section 3.3. Our approach enables us to effectively capture the underlying domain distributions while ensuring that the learned features are transferable across domains.

### Dynamic Graph Construction

Images and text data inherently contain rich sequential or spatial structures that can be effectively represented using graphs. By constructing graphs based on the data, both intra-domain relations and inter-domain relations can be exploited. For instance, semantic and spatial relations among detected objects in an image , cross-modal relations among images and sentences , cross-domain relations among images and texts [9; 27] have been successfully modeled using graph-based representations. In this paper, leveraging self-correlation graphs enables us to model the relations between different samples within domain and capture the underlying data distributions.

Our self-correlation graphs are constructed on source features and target features respectively. A feature extractor \(F()\) can be designed to learn source features \(f_{s}\) and target features \(f_{t}\) from source domain samples \(_{s}\) and target domain samples \(_{t}\) respectively, _i.e._, \(f_{s}=F(x_{s})\) and \(f_{t}=F(x_{t})\). Given the extracted features \(f_{s}\), we aim to construct a undirected and weighted graph \(_{s}=(_{s},_{s})\). Each vertex \(v_{i}_{s}\) is represented by a feature vector \(f_{i}^{s}\). Each weighted edge \(e_{i,j}_{s}\) can be formulated as a relation between a pair of entities \((f_{i}^{s},f_{j}^{s})\), where \(()\) denotes a metric function. With the extracted features \(f_{t}\), another graph \(_{s}=(_{t},_{t})\) can be constructed in the same way. Note that both \(f_{s}\) and \(f_{t}\) keep evolving along with the update of parameter \(\) during the training process. Their adjacency matrices are denoted as \(_{s}\) and \(_{t}\) respectively. As is well-known, the adjacency matrix of a graph contains all of its topological information. By representing these graphs in domain adaptation tasks, we can directly obtain the intra-domain relations and regard inter-domain alignment as a graph matching problem .

### Graph Spectral Alignment

In this section, we will introduce how to align inter-domain relations for source domain graphs and target domain graphs. In domain adaptation scenarios where domain shift exits, if we directly construct a graph with source and target domain features together, we can only obtain a graph beyond the homophily assumption. In this way, a necessary domain alignment comes out.

Figure 1: The overall architecture. The final objective integrates supervised classification loss \(_{cls}\), domain adversarial loss \(_{adv}\), neighbor-aware propagation loss \(_{nap}\), and graph spectral alignment loss \(_{spa}\).

As stated in Section 3.1, based on our correlation graphs, the inter-domain alignment can be regard as a graph matching problem. Explicit graph matching methods aim to find a one-to-one correspondence between nodes or edges of two graphs and typically involve solving a combinatorial optimization problem for node matching and edge matching respectively . Nevertheless, our goal is often to align the distributions of the source and target domains and learn domain-invariant features, instead of requiring such an complicated matching approach. Therefore, we prefer a implicit graph alignment method, avoiding multiple stages of explicit graph matching.

The distance between the spectra of graphs is able to measure how far apart the spectrum of a graph with \(n\) vertices can be from the spectrum of any other graph with \(n\) vertices , leading to a simplification of measuring the discrepancy of graphs. Inspired by this, we give the definitions of graph laplacians and spectral distances:

**Definition 1**.: (Graph Laplacians )_. Let \(=(,)\) be a finite graph with vertices \(\) and weighted edges \(\). Let \(:\) be a function of the vertices taking values in a ring and \(:\) be a weighting function of weighed edges. Then, the graph Laplacian \(\) acting on \(\) and \(\) is defined by_

\[(_{})(v)=_{w:d(w,v)=1}_{uv}[(v)-(w)]\]

_where \(d(w,v)\) is the graph distance between vertices \(w\) and \(v\), and \(_{uv}\) is the weight value on the edge \(wv\)._

**Definition 2**.: (Spectral Distances) _. Given two simple and nonisomorphic graphs \(_{s}\) and \(_{t}\) on \(n\) vertices with the spectra of Laplacians \(_{s}=\{_{i}^{s}\}_{i=1}^{n}\) with \(_{1}^{s}_{2}^{s}_{n}^{s}\) and \(_{t}=\{_{i}^{t}\}_{i=1}^{n}\) with \(_{1}^{t}_{2}^{t}_{n}^{t}\) respectively. Define the spectral distance between \(_{s}\) and \(_{t}\) as_

\[(_{s},_{t})=\|_{s}-_{t}\|_{p}, p 1\]

For a simple undirected graph with a finite number of vertices and edges, the definition 1 is just identical to the Laplacian matrix. With the adjacency matrix \(_{s}\) of source domain graph \(_{s}\), we can obtain its Laplacian matrix \(_{s}\) and its Laplacian eigenvalues \(_{s}\). Similarly, we yield Laplacian matrix \(_{t}\) and Laplacian eigenvalues \(_{t}\). Following the definition Def.2, we can calculate the spectral distances between source domain graph \(_{s}\) and target domain \(_{t}\), and thus the graph spectral penalty is defined as bellow:

\[_{gsa}=(_{s},_{t})\] (2)

This spectral penalty measures the discrepancy of two graphs on spectrum space. Minimizing this penalty decreases the distance of source domain graphs and target domain graphs. It can also be regarded as a regularizer and easy to combine with existing domain adaptation methods.

**More Insights.** Graph Laplacian filters are a type of signal processing filter to apply a smoothing operation to the signal on a graph by taking advantage of the local neighborhood structure of the graph represented by the Laplacian matrix . A graph filter can be denoted as \((f*g)_{}=Ug_{}U^{T}f\), where \(f\) is a signal on graph \(\), and \(\) and \(U\) is the eigenvalues and eigenvectors of Laplacian matrix \(L=U U^{T}\). This filter is also the foundation of classic graph neural networks  and usually infers the labels of unlabeled nodes with the information from the labeled ones in a graph. Similar to the hypothesis in , source features and target features will be aligned into the same eigenspace along with the learning process and finally only differs slightly in the eigenvalues \(g_{}\).

### Neighbor-aware Propagation Mechanism

In this section, we will introduce how to exploit intra-domain relations within target domain graphs. The well-trained source domain naturally forms tight clusters in the latent space. After aligning via the aforementioned graph spectral penalty, the rich topological information is coarsely transferred to the target domain. To perform the fine-grained intra-domain alignment, we take a further step by encouraging message propagation within the target domain graph.

Intuitively, we adopt the weighted \(k\)-Nearest-Neighbor (KNN) classification algorithm  to generate pseudo-label for target domain graph. We focus on unlabelled target samples \(\{x_{i}^{t}\}_{i=1}^{N_{r}}\) and thus omit the domain subscript \(t\) for clarity. Let \(p_{i}=C(F(x_{i}))\) denotes the \(C_{t}\)-dimensional prediction of unlabelled target domain sample \(x_{i}\) and \(p_{i}^{m}\) denotes its mapped probability stored in the memory bank. A vote is obtained from the top \(k\) nearest labelled samples for each unlabelled sample \(x_{i}\), which is denoted as \(_{i}\). The vote of each neighbor \(j_{i}\) is weighted by the corresponding predicted probabilities \(p_{j,c}\) that data sample \(x_{j}\) will be identified as category \(c\). Therefore, the voted probability \(p_{i,c}\) of data sample \(x_{i}\) corresponding to class \(c\) can be defined as \(q_{i,c}=_{j i,j_{i}}p_{j,c}^{m}\) and we yield the normalized probability \(_{i,c}=q_{i,c}/_{m=1}^{C_{t}}q_{i,m}\) and the pseudo-label \(\) for data sample \(x_{i}\) can be calculated as \(_{i}=_{c}_{i,c}\). Considering different neighborhoods \(_{i}\) lie in different local density, we should expect a larger weight for the target data in higher local density . Obviously, a larger \(_{i,c}\) means the data sample \(x_{i}\) lies in a neighborhood of higher density. In this way, we directly utilize the category-normalized probability \(_{i,c}\) as the confidence value for each pseudo-label and thus the weighted cross-entropy loss based on the pseudo-labels is formulated as:

\[_{nap}=-}_{i=1}^{N_{t}}_{i, _{i}} p_{i,_{i}}\] (3)

where \(\) is the coefficient term properly designed to grow along with iterations to mitigate the noises in the pseudo-labels at early iterations and avoid the error accumulation . Note that our neighbor-aware propagation loss only depends on unlabeled target data samples, just following the classic self-training method .

**Memory Bank.** We design the memory bank to store prediction probabilities and their associated feature vectors mapped by their target data indices. Before storing them, we firstly apply the sharpening technique to fix the ambiguity in the predictions of target domain data [5; 24]:

\[_{j,c}=p_{j,c}^{-}/_{x=1}^{C_{t}}p_{j,x}^{-}\] (4)

where \(\) is the temperature to scale the prediction probabilities. As \( 0\), the probability will collapse to a point mass . Then, we utilize L2-norm to normalize feature vectors \(f_{i}\). Finally, we store the sharpened prediction \(_{i}\) and its associated normalized feature \(\|f_{i}\|\) to the memory bank via \(\)-exponential moving averaging (EMA) strategy, updating them for each iteration. The predictions stored in the memory bank \(p_{i}^{m}\) are utilized to approximate real-time probability for pseudo-label generation, and the feature vectors \(f_{i}^{m}\) stored in the memory bank are employed to construct graphs before neighbor-aware propagation.

**The Final Objective.** At the end of our approach, let us integrate all of these losses together, _i.e_, supervised classification loss \(_{cls}\) and domain adversarial loss \(_{adv}\) descried in Eq.1, the loss of neighbor-aware propagation mechanism \(_{nap}\) in Eq.3, and the loss of graph spectral alignment \(_{gsa}\) in Eq.2. Finally, we can obtain the final objective as follows:

\[_{total}=_{cls}+_{adv}+_{gsa}+ _{nap}\] (5)

For the sake of simplicity, we leave out the loss scale for \(_{adv}\) and \(_{gsa}\) here. Just as the coefficient term \(\) in \(_{nap}\), these two losses are also adjusted with specific scales in the implementation. For more detailed implementation specifics, please refer to our source code. Note that we only describe the problem of unsupervised domain adaptation here our approach can easily to extend to semi-supervised domain adaptation scenario. Concerning the labeled data, we employ the standard cross-entropy loss with label-smoothing regularization  in our implementation. We also give different trials of similarity metrics and graph laplacians. More concrete details are in the following section.

## 4 Experiments

In this section, we present our main empirical results to show the effectiveness of our method. To evaluate the effectiveness of our architecture, we conduct comprehensive experiments under unsupervised domain adaptation, semi-supervised domain adaptation settings. The results of other compared methods are directly reported from the original papers. More experiments can be found in the Appendix.

### Experimental Setups

**Datasets.** We conduct experiments on 4 benchmark datasets: 1) **Office31** is a widely-used benchmark for visual DA. It contains 4,652 images of 31 office environment categories from three domains: _Amazon_ (A), _DSLR_ (D), and _Webcam_ (W), which correspond to online website, digital SLR camera and web camera images respectively. 2) **OfficeHome** is a challenging dataset that consists of images of everyday objects from four different domains: _Artistic_ (A), _Clipart_ (C), _Product_ (P), and _Real-World_ (R). Each domain contains 65 object categories in office and home environments, amounting to 15,500 images around. Following the typical settings , we evaluate methods on one-source to one-target domain adaptation scenario, resulting in 12 adaptation cases in total. 3) **VisDA2017** is a large-scale benchmark that attempts to bridge the significant synthetic-to-real domain gap with over 280,000 images across 12 categories. The source domain has 152,397 synthetic images generated by rendering from 3D models. The target domain has 55,388 real object images collected from _Microsoft COCO_. Following the typical settings , we evaluate methods on synthetic-to-real task and present test accuracy for each category. 4) **DomainNet** is a large-scale dataset containing about 600,000 images across 345 categories, which span 6 domains with large domain gap: _Clipart_ (C), _Infograph_ (I), _Painting_ (P), _Quickdraw_ (Q), _Real_ (R), and _Sketch_ (S). Following the settings in , we compare various methods for 12 tasks among C, P, R, S domains on the original DomainNet dataset.

**Implementation details.** We use PyTorch and tllib toolbox  to implement our method and fine-tune ResNet pre-trained on ImageNet . Following the standard protocols for unsupervised domain adaptation in previous methods , we use the same backbone networks for fair comparisons. For Office31 and OfficeHome dataset, we use ResNet-50 as the backbone network. For VisDA2017 and DomainNet dataset, we use ResNet-101 as the backbone network. We adopt mini-batch stochastic gradient descent (SGD) with a momentum of 0.9, a weight decay of 0.005, and an initial learning rate of 0.01, following the same learning rate schedule in .

### Result Comparisons

In this section, we compare SPA with various state-of-the-art methods for unsupervised domain adaptation (UDA) scenario, and _Source Only_ in UDA task means the model trained only using labeled source data. We also extend SPA to semi-suerpervised domain adaptation (SSDA) scenario and conduct experiments on 1-shot and 3-shots setting. With the page limits, see supplementary material for details.

   Method & C\(\)P & C\(\)R & C\(\)S & P\(\)C & P\(\)R & P\(\)S & R\(\)C & R\(\)P & R\(\)S & S\(\)C & S\(\)P & S\(\)R & Avg. \\  _Source Only_ & 32.7 & 50.6 & 39.4 & 41.1 & 56.8 & 35.0 & 48.6 & 48.8 & 36.1 & 49.0 & 34.8 & 46.1 & 43.3 \\ DAN  & 38.8 & 55.2 & 43.9 & 45.9 & 59.0 & 40.8 & 50.8 & 49.8 & 38.9 & 56.1 & 45.9 & 55.5 & 48.4 \\ DANN  & 37.9 & 54.3 & 44.4 & 41.7 & 55.6 & 36.8 & 50.7 & 50.8 & 40.1 & 55.0 & 45.0 & 54.5 & 47.2 \\ BCDM  & 38.5 & 53.2 & 43.9 & 42.5 & 54.5 & 38.5 & 51.9 & 51.2 & 40.6 & 53.7 & 46.0 & 53.4 & 47.3 \\ MCD  & 37.5 & 52.9 & 44.0 & 44.6 & 54.5 & 41.6 & 52.0 & 51.5 & 39.7 & 55.5 & 44.6 & 52.0 & 47.5 \\ ADDA  & 38.4 & 54.1 & 44.1 & 43.5 & 56.7 & 39.2 & 52.8 & 51.3 & 40.9 & 55.0 & 45.4 & 54.5 & 48.0 \\ CDAN  & 39.9 & 55.6 & 45.9 & 44.8 & 57.4 & 40.7 & 56.3 & 52.5 & 44.2 & 55.1 & 43.1 & 53.2 & 49.1 \\ MCC  & 40.1 & 56.5 & 44.9 & 46.9 & 57.7 & 41.4 & 56.0 & 53.7 & 40.6 & 58.2 & 45.1 & 55.9 & 49.7 \\ JAN  & 40.5 & 56.7 & 45.1 & 47.2 & 59.9 & 43.0 & 54.2 & 52.6 & 41.9 & 56.6 & 46.2 & 55.5 & 50.0 \\ MDD  & 42.9 & 59.5 & 47.5 & 48.6 & 59.4 & 42.6 & 58.3 & 53.7 & 46.2 & 58.7 & 46.5 & 57.7 & 51.8 \\ SDAT  & 41.5 & 57.5 & 47.2 & 47.5 & 58.0 & 41.8 & 56.7 & 53.6 & 43.9 & 58.7 & 48.1 & 57.1 & 51.0 \\ Leco  & 44.1 & 55.3 & 48.5 & 49.4 & 57.5 & 45.5 & 58.8 & 55.4 & 46.8 & 61.3 & 51.1 & 57.7 & 52.6 \\   SPA (Ours) & **54.3** & **70.9** & **56.1** & **59.3** & **71.5** & **51.8** & **64.6** & **59.6** & **52.1** & **66.0** & **57.4** & **70.6** & **61.2** \\   

Table 1: Classification Accuracy (%) on DomainNet for unsupervised domain adaptation (inductive), using ResNet101 as backbone. The best accuracy is indicated in **bold** and the second best one is underlined. Note that we compare methods on the original DomainNet dataset with train/test splits in target dataset, leading to an inductive scenario.

[MISSING_PAGE_FAIL:7]

the loss of pseudo-labelling and graph spectral penalty can improve the performance of baseline, and further combining these two losses together yields better outcomes, which is comparable with cutting-edge methods.

**Parameter Sensitivity.** To analyze the stability of SPA, we design experiments on the hyperparameter \(\) of exponential moving averaging strategy for memory updates. The experimental results of \(\) = 0.1, 0.3, 0.5, 0.7, 0.9 is shown in the second section of Table 4. These results are based on CDAN . From the series of results, we can find that in OfficeHome dataset, the choice of \(\) = 0.5 outperforms than others. In addition, the differences between these results are within 0.5%, which means that SPA is insensitive to this hyperparameter.

**Robustness Analysis.** To further identify the robustness of SPA to different graph structures, we conduct experiments on different types of Laplacian matrix, similarity metric of graph relations, and \(k\) number of nearest neighbors of KNN classification methods. Here are the two types of commonly-used Laplacian matrix : the random walk laplacian matrix \(_{rwk}=^{-1}\), and the symmetrically normalized Laplacian matrix \(_{sym}=-^{-1/2}^{-1/2}\), where \(\) denotes the degree matrix based on the adjacency matrix \(\). In addition, the similarity metric are chosen from cosine similarity and Gaussian similarity, and different \(k=3,5\) when applying KNN classification algorithm. The results are shown in the second section of Table 4. We can find that different types of Laplacian matrix still lead to comparable results. As for the similarity metric, the Gaussian similarity brings better performance than cosine similarity and the results also presents that 5-NN graphs is superior than 3-NN graphs in OfficeHome dataset. For all these aforementioned experiments results, the differences between them are within 1% around, confirming the robustness of SPA.

**Feature Visualization.** To demonstrate the learning ability of SPA, we visualize the features of DANN , BSP , NPL  and SPA with the t-SNE embedding  under the C \(\) R setting of OfficeHome Dataset. BSP is a classic domain adaptation method which also use matrix decomposition and NPL is a classic pseudo-labeling method which also use high-confidence predictions on unlabeled data as true labels. Thus, we choose them as baselines to compare. The results are shown in Figure 2. Comparing Figure 2d with Figure 2a, Figure 2b and Figure 2c respectively, we make some important observations: (i)-we find that the clusters of features in Figure 2d are more compact, or say less diffuse than others, which suggests that the features learned by SPA are able to attain more desirable discriminative property; (ii)-we observe that red markers are more closer and overlapping with blue markers in Figure 2d, which suggests that the source features and target features learned by SPA are transferred better. These observations imply the superiority of SPA over discriminability and transferability in unsupervised domain adaptation scenario.

   Method & A\(\)C & A\(\)P & A\(\)R & C\(\)A & C\(\)P & C\(\)R & P\(\)A & P\(\)C & P\(\)R & R\(\)A & R\(\)C & R\(\)P & Avg. \\  w/o \(_{spac}\), \(_{npj}\) & 54.6 & 74.1 & 78.1 & 63.0 & 72.2 & 74.1 & 61.6 & 52.3 & 79.1 & 72.3 & 57.3 & 82.8 & 68.5 \\ w/o \(_{spac}\) & 59.0 & 80.1 & 81.5 & 66.2 & 77.8 & 76.7 & 69.2 & 57.7 & 83.0 & 74.0 & 64.1 & 85.8 & 72.9 \\ w/o \(_{spac}\) & 59.0 & 78.2 & 81.0 & 63.5 & 76.5 & 76.2 & 64.7 & 57.3 & 82.1 & 73.6 & 61.4 & 85.7 & 71.6 \\ w/ \(_{spac}\), \(_{spac}\) & 59.9 & 79.1 & 84.4 & 74.9 & 79.1 & 81.9 & 72.4 & 58.4 & 84.9 & 77.9 & 61.2 & 87.7 & 75.1 \\   \(\) = 0.1 & 58.3 & 79.2 & 83.2 & 72.8 & 79.3 & 80.4 & 73.3 & 58.2 & 84.5 & 79.0 & 61.4 & 87.3 & 74.7 \\ \(\) = 0.3 & 59.0 & 79.5 & 83.8 & 73.6 & 80.6 & 81.7 & 73.5 & 58.0 & 84.9 & 78.2 & 61.3 & 87.7 & 75.1 \\ \(\) = 0.5 & 59.3 & 79.5 & 84.1 & 73.3 & 80.6 & 81.7 & 73.1 & 58.0 & 84.9 & 78.2 & 62.2 & 87.4 & 75.2 \\ \(\) = 0.7 & 59.9 & 79.4 & 84.0 & 73.6 & 80.5 & 82.0 & 72.7 & 57.3 & 84.8 & 77.9 & 61.9 & 87.3 & 75.1 \\ \(\) = 0.9 & 59.7 & 79.6 & 84.7 & 72.9 & 78.7 & 82.2 & 71.2 & 57.5 & 84.5 & 77.2 & 61.7 & 87.5 & 74.8 \\   \(_{rwk}\) w/ \(cos\) & 60.6 & 79.4 & 84.1 & 72.5 & 79.2 & 81.8 & 71.9 & 57.1 & 84.8 & 76.3 & 61.8 & 87.4 & 74.7 \\ \(_{rwk}\) w/ \(gauss\) & 60.4 & 79.7 & 84.5 & 73.6 & 81.3 & 82.1 & 72.2 & 58.0 & 85.2 & 77.4 & 61 & 88.1 & 75.3 \\ \(_{sym}\) w/ \(cos\) & 60.7 & 79.5 & 83.8 & 72.7 & 80.0 & 81.8 & 71.5 & 57.2 & 84.8 & 76.3 & 61.9 & 87.4 & 74.8 \\ \(_{sym}\) w/ \(gauss\) & 59.4 & 79.5 & 84.5 & 73.6 & 81.0 & 81.7 & 72.2 & 57.6 & 84.8 & 77.5 & 61.8 & 87.8 & 75.1 \\  \(_{rwk}\) w/ \(k\) = 3 & 59.0 & 80.6 & 83.9 & 72.4 & 79.6 & 81.7 & 71.5 & 56.5 & 84.7 & 76.4 & 62.0 & 87.7 & 74.7 \\ \(_{rwk}\) w/ \(k\) = 5 & 60.4 & 79.7 & 84.5 & 73.6 & 81.3 & 82.1 & 72.2 & 58.0 & 85.2 & 77.4 & 61 & 88.1 & 75.3 \\ \(_{sym}\) w/ \(k\) = 3 & 59.2 & 80.1 & 84.0 & 72.6 & 81.6 & 81.5 & 71.2 & 57.5 & 84.7 & 76.3 & 61.5 & 87.4 & 74.8 \\ \(_{sym}\) w/ \(k\) = 5 & 59.4 & 79.5 & 84.5 & 73.6 & 81.0 & 81.7 & 72.2 & 57.6 & 84.8 & 77.5 & 61.8 & 87.8 & 75.1 \\   

Table 4: Classification Accuracy (%) on OfficeHome for unsupervised domain adaptation. The table is divided into three sections corresponding to the three analysis of ablation study, robustness analysis, and parameter sensitivity, each separated by a double horizontal line. More studies on other datasets are in supplementary material.

## 5 Related Work

**Domain Adaptation.** Domain adaptation (DA) problem is a specific subject of transfer learning , and also a widely studied area in computer vision field , By the label amount of target domain, domain adaptation can be divided into unsupervised domain adaptation, and semi-supervised domain adaptation _etc._ Unsupervised domain adaptation methods are very effective at aligning feature distributions of source and target domains without any target supervision but perform poorly when even a few labeled examples are available in the target. Our approach mainly focus on unsupervised domain adaptation, achieving the cutting-edge performance. It can also be extended to semi-supervised domain adaptation with comparable results.

There are more specifics of our baselines. Inspired by random walk  in graph theory, MCC  proposes a self-training method by optimizing the class confusion matrix. BNM  formulates the loss of nuclear norm, _i.e_, the sum of singular values based on the prediction outputs. NWD  calculates the difference between the nuclear norm of source prediction matrix and target prediction matrix. The aforementioned three methods all focus on the prediction outputs and establish class-to-class relations. Besides, there are two works focusing on the features, CORAL , measuring the difference of covariance, and BSP , directly performing singular value decomposition on features and calculate top \(k\) singular values. BNM, NWD and BSP are similar to our method with the use of singular values. However, our eigenvalues comes out from the graph laplacians, which contains graph topology information . For example, the second-smallest non-zero laplacian eigenvalue is called as the algebraic connectivity, and its magnitude just reflects how connected graph is . As we know, we are the first to propose a graph spectral alignment perspective for domain adaptation scenarios.

**Self-training.** Self-training is a semi-supervised learning method that enhances supervised models by generating pseudo-labels for unlabeled data based on model predictions, which has been explored in lots of works . These methods pick up the class with the maximum predicted probability as true labels each time the weights are updated. With filtering strategies and iterative approaches employed to improve the quality of pseudo labels, this self-training technique has also been applied to some domain adaptation methods . This kind of methods proves beneficial when the labeled data is limited. However, it relies on an assumption of the unlabeled data following the same distribution with the labeled data and requires accurate initial model predictions for precise results . The intra-domain alignment in our approach helps the neighbor-aware self-training mechanism generate more precise labels. The interaction between components of our approach brings our impressive results in DA scenarios.

**Graph Data Mining.** Graphs are widely applied in real-world applications to model pairwise interactions between objects in numerous domains such as biology , social media  and finance . Because of their rich value, graph data mining has long been an important research direction . It also plays a significant role in computer vision tasks such as image retrieval , object detection , and image classification . Bruna et al. first introduce the graph convolution operation based on spectral graph theory. The graph filter and the eigen-decomposition of the graph Laplacian matrix inspire us to propose our graph spectral alignment.

Figure 2: Feature Visualization. the t-SNE plot of DANN , BSP , NPL , and SPA features on OfficeHome dataset in the C \(\) R setting. We use red markers for source domain features and blue markers for target domain features.

Conclusion

In this paper, we present a novel spectral alignment perspective for balancing inter-domain transferability and intra-domain discriminability in UDA. We first leveraged graph structure to model the topological information of domain graphs. Next, we aligned domain graphs in their eigenspaces and propagated neighborhood information to generate pseudo-labels. The comprehensive model analysis demonstrates the superiority of our method. The current method for constructing graph spectra is only a starting point and may be inadequate for more difficult scenarios such as universal domain adaptation. Additionally, this method is currently limited to visual classification tasks, and more sophisticated and generic methods to object detection or semantic segmentation are expected in the future. Furthermore, we believe that graph data mining methods and the well-formulated properties of graph spectra should have more discussion in both domain adaptation and computer vision field.

## 7 Acknowledgement

This work is majorly supported in part by the National Key Research and Development Program of China (No. 2022YFB3304100), the NSFC under Grants (No. 62206247), and by the Fundamental Research Funds for the Central Universities (No. 226-2022-00028). JZ also thanks the sponsorship by CAAI-Huawei Open Fund.