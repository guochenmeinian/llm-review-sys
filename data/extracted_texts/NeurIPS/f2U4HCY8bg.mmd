# Iterative Reachability Estimation for

Safe Reinforcement Learning

 Milan Ganai

UC San Diego

mganai@ucsd.edu

&Zheng Gong

UC San Diego

zhgong@ucsd.edu

&Chenning Yu

UC San Diego

chy010@ucsd.edu

&Sylvia Herbert

UC San Diego

sherbert@ucsd.edu

&Sicun Gao

UC San Diego

sicung@ucsd.edu

###### Abstract

Ensuring safety is important for the practical deployment of reinforcement learning (RL). Various challenges must be addressed, such as handling stochasticity in the environments, providing rigorous guarantees of persistent state-wise safety satisfaction, and avoiding overly conservative behaviors that sacrifice performance. We propose a new framework, Reachability Estimation for Safe Policy Optimization (RESPO), for safety-constrained RL in general stochastic settings. In the feasible set where there exist violation-free policies, we optimize for rewards while maintaining persistent safety. Outside this feasible set, our optimization produces the safest behavior by guaranteeing entrance into the feasible set whenever possible with the least cumulative discounted violations. We introduce a class of algorithms using our novel reachability estimation function to optimize in our proposed framework and in similar frameworks such as those concurrently handling multiple hard and soft constraints. We theoretically establish that our algorithms almost surely converge to locally optimal policies of our safe optimization framework. We evaluate the proposed methods on a diverse suite of safe RL environments from Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both reward performance and safety compared with state-of-the-art baselines.

## 1 Introduction

Safety-Constrained Reinforcement Learning is important for a multitude of real-world safety-critical applications . These situations require guaranteeing persistent safety and achieving high performance while handling environment uncertainty. Traditionally, methods have been proposed to obtain optimal control by solving within the Constrained Markov Decision Process (CMDP) framework, which constrains expected cumulative violation of the constraints along trajectories to be under some threshold. Trust-region approaches  follow from the Constrained Policy Optimization algorithm  which tries to guarantee monotonic performance improvement while satisfying the cumulative constraint requirement. However, these approaches are generally very computationally expensive, and their first or second order Taylor approximations may have repercussions on performance . Several primal-dual approaches  have been proposed with better computational efficiency and use multi-timescale frameworks  for asymptotic convergence to a local optimum. Nonetheless, these approaches inherit CMDP's lack of crucial rigorous safety guarantees that ensure persistent safety. Particularly, maintaining a cost budget for hard constraints like collision avoidance is insufficient. Rather, it's crucial to prioritize obtaining safe, minimal-violation policies when possible.

There have been a variety of proposed control theoretic functions that provide the needed guarantees on persistent safety for such hard constraints, including Control Barrier Functions (CBFs)  and Hamilton-Jacobi (HJ) reachability analysis [12; 13]. CBFs are energy-based certification functions, whose zero super-level set is control invariant [14; 15]. The drawbacks are the conservativeness of the feasible set and the lack of a systematic way to find these functions for general nonlinear systems. Under certain assumptions, approximate energy-based certificates can be learned with sufficient samples [16; 17; 18; 19; 20; 21]. HJ reachability analysis focuses on finding the set of initial states such that there exists some control to avoid and/or reach a certain region. It computes a value function, whose zero sub-level set is the largest feasible set, together with the optimal control. The drawback is the well known 'curse of dimensionality,' as HJ reachability requires solving a corresponding Hamilton-Jacobi-Bellman Partial Differential Equation (HJBPDE) on a discrete grid recursively. Originally, both approaches assume the system dynamics and environment are known, and inputs are bounded by some polytopic constraints, while recent works have applied them to model-free settings [22; 23; 24]. For both CBF and HJ reachability, controllers that optimize performance and guarantee safety can be synthesized in different ways online . There also exist hard-constraint approaches like  have theoretical safety guarantees without relying on control theory; however, they require learning or having access to the transition-function dynamics.

Consequently, it can be beneficial to combine CMDP-based approaches to find optimal control and the advantages of control-theoretic approaches to enable safety. The recent framework of Reachability Constrained Reinforcement Learning (RCRL)  proposed an approach that guarantees persistent safety and optimal performance when the agent is in the feasible set. However, RCRL is _limited to learning deterministic policies in deterministic environments_. In the general RL setting, stochasticity incurs a variety of challenges including how to define membership in the feasible set since it is no longer a binary question but rather a probabilistic one. Another issue is that for states outside optimal feasible set, RCRL optimization _cannot guarantee entrance into the feasible set when possible_. This may have undesirable consequences including indefinitely violating constraints with no recovery.

In light of the above challenges, we propose a new framework and class of algorithms called Reachability Estimation for Safe Policy Optimization (**RESPO**). Our main contributions are:

\(\) We introduce reachability estimation methods for general stochastic policies that predict the likelihood of constraint violations in the future. Using this estimation, a policy can be optimized such that **(i)** when in the feasible set, it maintains persistent safety and optimizes for reward performance, and **(ii)** when outside the feasible set, it produces the safest behavior and enters the feasible set if possible. We also demonstrate how our optimization can incorporate multiple hard and soft constraints while even prioritizing the hard constraints.

\(\) We provide a novel class of actor-critic algorithms based on learning our reachability estimation function, and prove our algorithm converges to a locally optimal policy of our proposed optimization.

\(\) We perform comprehensive experiments showing that our approach achieves high performance in complex, high-dimensional Safety Gym, PyBullet-based, and MuJoCo-based environments compared to other state-of-the-art safety-constrained approaches while achieving small or even \(0\) violations. We also show our algorithm's performance in environments with multiple hard and soft constraints.

## 2 Related Work

### Constrained Reinforcement Learning

Safety-constrained reinforcement learning has been addressed through various proposed learning mechanisms solving within an optimization framework. CMDP [28; 29] is one framework that augments MDPs with the cost function: the goal is to maximize expected reward returns while constraining cost returns below a manually chosen threshold. Trust-region methods [3; 4; 5; 6] and primal-dual based approaches [7; 9; 30; 31] are two classes of CMDP-based approaches. Trust-region approaches generally follow from Constrained Policy Optimization (CPO) , which approximates the constraint optimization problem with surrogate functions for the reward objective and safety constraints, and then performs projection steps on the policy parameters with backtracking line search. Penalized Proximal Policy Optimization  improves upon past trust-region approaches via the clipping mechanism, similar to Proximal Policy Optimization's  improvement over Trust Region Policy Optimization . Constraint-Rectified Policy Optimization  takes steps toward improving reward performance if constraints are currently satisfied else takes steps to minimize constraint violations. PPO Lagrangian  is a primal-dual method combining Proximal Policy Optimization (PPO)  with lagrangian relaxation of the safety constraints and has relatively low complexity but outperforms CPO in constraint satisfaction.  uses a penalized reward function like PPO Lagrangian, and demonstrates convergence to an optimal policy through the multi-timescale framework, introduced in .  proposes mapping the value function constraining problem as constraining densities of state visitation. Overall, while these algorithms provide benefits through various learning mechanisms, they inherit the problems of the CMDP framework. Specifically, they do not provide rigorous guarantees of persistent safety and so are generally unsuitable for state-wise constraint optimization problems.

### Hamilton-Jacobi Reachability Analysis

HJ reachability analysis is a rigorous approach that verifies safety or reachability for dynamical systems. Given any deterministic nonlinear system and target set, it computes the viscosity solution for the HJBPDE, whose zero sub-level set is the backward reachable set (BRS), meaning there exists some control such that the states starting from this set will enter the target set in future time (or stay away from the target set for all future time). However, the curse of dimensionality and the assumption of knowing the dynamics and environment in advance restrict its application, especially in the model-free RL setting where there is no access to the entire observation space and dynamics at any given time. Decomposition , warm-starting , sampling-based reachability , and Deepreach  have been proposed to solve the curse of dimensionality. [23; 24] proposed methods to bridge HJ analysis with RL by modifying the HJBPDE. The work of  takes framework into a deterministic hard-constraint setting. There are additionally model-based HJ reachability approach for reinforcement learning-based controls.  is a model-based extension of . Approaches like [42; 43; 44] use traditional HJ reachability for RL control while learning or assuming access to the system dynamics model. In stochastic systems, finding the probability of reaching a target while avoiding certain obstacles are key problems. [45; 46] constructed the theoretic framework based on dynamic programming and consider finite and infinite time reach-avoid problems. [47; 48; 49] propose computing the stochastic reach-avoid set together with the probability in a tractable manner to address the curse of dimensionality.

## 3 Preliminaries

### Markov Decision Processes

Markov Decision Processes (MDP) are defined as \(:=,,P,r,h,\). \(\) and \(\) are the state and action spaces respectively. \(P:\) is the transition function capturing the environment dynamics. \(r:\) is the reward function associated with each state-action pair, \(h:_{0}^{+}\) is the safety loss function that maps a state to a non-negative real value, which is called the constraint value, or simply cost. \(H_{}\) is the minimum _non-zero_ value of function \(h\); \(H_{}\) is upper bound on function \(h\). \(\) is a discount factor in the range \((0,1)\). \(_{I}\) is initial state set, \(d_{0}\) is initial state distribution, and \((a|s)\) is a stochastic policy that is parameterized by the state and returns an action distribution from which an action can be sampled and affects the environment defined by the MDP. In unconstrained RL, the goal is to learn an optimal policy \(^{*}\) maximizing expected discounted sum of rewards, i.e. \(^{*}=*{arg\,max}_{}_{s d_{0}}V^{}(s)\), where \(V^{}(s):=_{,P(s)}[_{s_{t}}^{t}r(s_{t },a_{t})]\). Note: \(,P(s)\) indicates sampling trajectory \(\) for horizon \(T\) starting from state \(s\) using policy \(\) in MDP with transition model \(P\), and \(s_{t}\) is the \(t^{th}\) state in trajectory \(\).

### Constrained Markov Decision Process

CMDP attempts to optimize the reward returns \(V^{}(s)\) under the constraint that the cost return is below some manually chosen threshold \(\). Specifically:

\[_{}*{}_{s d_{0}}[V^{}(s)],*{}_{s d_{0}}[V^{}_{c}(s)],\] (CMDP)

where cost return function \(V^{}_{c}(s)\) is often defined as \(V^{}_{c}(s):=_{,P(s)}[_{s_{t}}^{t}h( s_{t})]\).

While many approaches have been proposed to solve within this framework, CMDPs have several difficulties: \(1.\) cost threshold \(\) often requires much tuning while using prior knowledge of the environment; and \(2.\) CMDP often permits some positive average cost which is incompatible with state-wise hard constraint problems, since \(\) is usually chosen to be above 0.

Stochastic Hamilton-Jacobi Reachability for Reinforcement Learning

Classic HJ reachability considers finding the largest feasible set for deterministic environments. In this section, we apply a similar definition in [45; 46] and define the stochastic reachability problem.

### Persistent Safety and HJ Reachability for Stochastic Systems

The instantaneous safety can be characterized by the safe set \(_{s}\), which is the zero level set of the safety loss function \(h:_{0}^{+}\). The unsafe (i.e. violation) set \(_{v}\) is the complement of the safe set.

**Definition 1**.: Safe set and unsafe set: \(_{s}:=\{s:h(s)=0\},_{v}:=\{s:h(s)>0\}\).

We will write \(_{s_{v}}\) as the _instantaneous violation indicator function_, which is \(1\) if the current state is in the violation set and \(0\) otherwise. Note that the safety loss function \(h\) is different from the instantaneous violation indicator function since \(h\) captures the magnitude of the violation at the state.

It is insufficient to only consider instantaneous safety. When the environment and policy are both deterministic, we easily have a unique trajectory for starting from each state (i.e. the future state is uniquely determined) under Lipschitz environment dynamics. In classic HJ reachability literature , for a deterministic MDP's transition model \(P_{d}\) and deterministic policy \(_{d}\), the set of states that guarantees persistent safety is captured by the zero sub-level set of the following value function:

**Definition 2**.: Reachability value function \(V_{h}^{}:_{0}^{+}\) is: \(V_{h}^{}(s):=_{s_{i}_{d},P_{d}(s)}h(s_{t})\).

However, when there's a stochastic environment with transition model \(P(|s,a)\) and policy \((|s)\), the future states are not uniquely determined. This means for a given initial state and policy, there may exist many possible trajectories starting from this state. In this case, instead of defining a binary function that only indicates the existence of constraint violations, we define the reachability estimation function (REF), which captures the probability of constraint violation:

**Definition 3**.: The reachability estimation function (REF) \(^{}:\) is defined as:

\[^{}(s):=*{}_{,P(s)}_{s_{t} }_{(s_{t}|s_{0}=s,)_{v}}.\]

In a specific trajectory \(\), the value \(_{s_{t}}_{(s_{t}|s_{0}=s,)_{v}}\) will be 1 if there exist constraint violations and 0 if there exists no violation, which is binary. Taking expectation over this binary value for all the trajectories, we get the desired probability. We define optimal REF based on an optimally safe policy \(^{*}=_{}V_{c}^{}(s)\) (note that this policy may not be unique).

**Definition 4**.: The optimal reachability estimation function \(^{*}:\) is: \(^{*}(s):=^{^{*}}(s)\).

Interestingly, we can utilize the fact the instantaneous violation indicator function produces binary values to learn the REF function in a bellman recursive form. The following will be used later:

**Theorem 1**.: The REF can be reduced to the following recursive Bellman formulation:

\[^{}(s)=\{_{s_{v}},*{}_{s^{},P(s)}^{}(s^{})\},\]

where \(s^{},P(s)\) is a sample of the immediate successive state (i.e., \(s^{} P(|s,a(|s))\)) and the expectation is taken over all possible successive states. The proof can be found in the appendix.

**Definition 5**.: The feasible set of a policy \(\) based on \(^{}(s)\) is defined as: \(_{f}^{}:=\{s:^{}(s)=0\}\).

Note, the feasible set for a specific policy is the set of states starting _from_ which no violation is reached, and the safe set is the set of states _at_ which there is no violation. We will use the phrase likelihood of being feasible to mean the likelihood of not reaching a violation, i.e. \(1-^{}(s)\).

### Comparison with RCRL

The RCRL approach  uses reachability to optimize and maintain persistent safety in the feasible set. Note, in below formulation, \(_{f}\) is the optimal feasible set, i.e. that of a policy \(_{}V_{h}^{}(s)\). The RCRL formulation is:

\[_{}*{}_{s d_{0}}[V^{}(s) _{s_{f}}-V_{h}^{}(s)_{s_{f} }],V_{h}^{}(s) 0, s_{f} _{f}.\] (RCRL)

The equation RCRL considers two different optimizations. When in the optimal feasible set, the optimization produces a persistently safe policy maximizing rewards. When outside this set, the optimization produces a control minimizing the maximum future violation, i.e. \(*{arg\,min}_{}V_{h}^{}(s)\). _However, this does not ensure (re)entrance into the feasible set even if such a control exists._

RCRL performs constraint optimization on \(V_{h}^{}\) with a neural network (NN) lagrange multiplier with state input . When learning to optimize a Lagrangian dual function, the NN lagrange multiplier should converge to small values for states in the optimal feasible set and converge to large values for other states. Nonetheless, learning \(V_{h}\) provides a weak signal during training: if there is an improvement in safety along the trajectory not affecting the maximum violation, \(V_{h}^{}\) remains the same for all states before the maximum violation in the trajectory. These improvements in costs can be crucial in guiding the optimization toward a safer policy. And optimizing with \(V_{h}(s)\) can result in accumulating an unlimited number of violations smaller than the maximum violation. Also, a major issue with this approach is that _it's limited to deterministic MDPs and policies_ because its reachability value function in the Bellman formulation does not directly apply to the stochastic setting. However, in general _stochastic_ settings, estimating feasibility cannot be binary since for a large portion of the state space, even under the optimal policy, the agent may enter the unsafe set with a non-zero probability, rendering such definition too conservative and impractical.

## 5 Iterative Reachability Estimation for Safe Reinforcement Learning

In this paper, we formulate a general optimization framework for safety-constrained RL and propose a new algorithm to solve our constraint optimization by using our novel reachability estimation function. We present the deterministic case in Section 5.1 and build our way to the stochastic case in Section 5.2. We present our novel algorithm to solve these optimizations, involving our new reachability estimation function, in Section 5.3. We introduce convergence analysis in Section 5.4.

### Iterative Reachability Estimation for Deterministic Settings

All state transitions and policies happen with likelihood \(0\) or \(1\) for the deterministic environment. Therefore, the probability of constraint violation for policy \(\) from state \(s\), i.e., \(^{}(s)\), is in the set \(\{0,1\}\). According to Definition 4, if there exists some policy \(\) such that \(^{}(s)=0\), we have \(^{*}(s)=0\). Otherwise, \(^{*}(s)=1\). Notice that this captures definitive membership in the optimal feasible set \(^{*}(s)=_{s S_{f}^{_{s}}}\), which is the feasible set of some safest policy \(_{s}=*{arg\,min}_{}V_{c}^{}(s)\). Now, we divide our optimization in two parts: the infeasible part and the feasible part.

For the infeasible part, we want the agent to incur the least cumulative damage (discounted sum of costs) and, if possible, (re)enter the feasible set. Different from previous Reachability-based RL optimizations, by using the discounted sum of costs \(V_{c}^{}(s)\) we consider both magnitude and frequency of violations, thereby improving learning signal. The infeasible portion takes the form:

\[_{}*{}_{s d_{0}}[-V_{c}^{}(s)].\] (1)

For the feasible part, we want the policy to ensure the agent stays in the feasible set and maximize reward returns. This produces a constraint optimization where the cost value function is constrained:

\[_{}*{}_{s d_{0}}[V^{}(s)],V_{c}^{}(s)=0, s_{I}.\] (2)

The following propositions justify using \(V_{c}^{}\) as the constraint. Both proofs are in the appendix.

**Proposition 1**.: The cost value function \(V_{c}^{}(s)\) is zero for state \(s\) if and only if the persistent safety is guaranteed for that state under the policy \(\).

We define here \(_{f}:=_{f}^{_{s}}\), the feasibilty set of some safest policy. Now, the above two optimizations can be unified with the use of the feasibility function \(^{*}(s)\):

\[_{}*{}_{s d_{0}}\!\![V^{}(s)(1- ^{*}(s))-V_{c}^{}(s)^{*}(s)],V_{c}^{}(s)=0,  s_{I}_{f}.\] (3)

Unlike other reachability based optimizations like RCRL, one particular advantage in Equation 3 is, with some assumptions, the guaranteed entrance back into feasible set with minimum cumulative discounted violations whenever a possible control exists. More formally, assuming infinite horizon:

**Proposition 2**.: If \(\) that produces trajectory \(=\{(s_{i}),i,s_{1}=s\}\) in deterministic MDP \(\) starting from state \(s\), and \( m,m<\) such that \(s_{m} S_{f}^{}\), then \(>0\) where if discount factor \((1-,1)\), then the optimal policy \(^{*}\) of Equation 3 will produce a trajectory \(^{}=\{(s^{}_{j}),j,s^{}_{1}=s\}\), such that \( n,n<\), \(s^{}_{n} S_{f}^{^{*}}\) and \(V_{c}^{^{*}}(s)=_{^{}}V_{c}^{^{}}(s)\).

### Iterative Reachability Estimation for Stochastic Settings

In stochastic environments, for each state, there is some likelihood of entering into the unsafe states under any policy. Thus, we adopt the probabilistic reachability Definitions 3 and 4. Rather than using the binary indicator in the optimal feasible set to demarcate the feasibility and infeasibility optimization scenarios, we use the likelihood of infeasibility of the safest policy. In particular, for any state \(s\), the optimal likelihood that the policy will enter the infeasible set is \(^{*}(s)\) from Definition 4.

We again divide the full optimization problem in stochastic settings into infeasible and feasible ones similar to Equations 1 and 2. However, we consider the infeasible formulation with likelihood the current state is in a safest policy's infeasible state, or \(^{*}(s)\). Similarly, we account for the feasible optimization formulation with likelihood the current state is in a safest policy's feasible set, \(1-^{*}(s)\). The complete Reachability Estimation for Safe Policy Optimization (RESPO) can be rewritten as:

\[_{}}_{s d_{0}}[V^{}(s)(1-^{*}(s))-V^{ }_{c}(s)^{*}(s)]V^{}_{c}(s)=0,1-^{*}(s), s S_{I}.\] (RESPO)

In sum, the RESPO framework provides several benefits when compared with other constrained Reinforcement Learning and reachability-based approaches. Notably, 1) it maintains persistent safety when in the feasible set unlike CMDP-based approaches, 2) compared with other reachability-based approaches, RESPO considers performance optimization in addition to maintaining safety, 3) it maintains the behavior of a safest policy in the infeasible set and even reenters the feasible set when possible, 4) RESPO employs rigorously defined reachability definitions even in stochastic settings.

### Overall Algorithm

We describe our algorithms by breaking down the novel components. Our algorithm predicts reachability membership to guide the training toward optimizing the right portion of the optimization equation (i.e., feasibility case or infeasibility case). Furthermore, it exclusively uses the discounted sum of costs as the safety value function - we can avoid having to learn the reachability value function while having the benefit of exploiting the improved signal in the cost value function.

Optimization in infeasible set versus feasible set.If the agent is in the infeasible set, this is the simplest case. We want to find the optimal policy that maximizes \(-V^{}_{c}(s)\). This would be the only term that needs to be considered in optimization.

On the other hand, if the agent is in the feasible set, we must solve the constraint optimization \(_{}V^{}(s)\) subject to \(V^{}_{c}(s)=0\). This could be solved via a Lagrangian-based method:

\[_{}_{}L(,)=_{}_{}}_{s d_{0}}[-V^{}(s)+ V^{}_{c}(s)].\]

Now what remains is obtaining the reachability estimation function \(^{*}\). First, we address the problem of acquiring optimal likelihood of being feasible. It is nearly impossible to accurately know before training if a state is in a safest policy's infeasible set. We propose learning a function guaranteed to converge to this REF (with some discount factor for \(\)-contraction mapping) by using the recursive Bellman formulation proved in Theorem 1.

We learn a function \(p(s)\) to capture the probability \(^{*}(s)\). It is trained like a reachability function:

\[p(s)=\{_{s S_{v}}, p(s^{})\},\]

where \(S_{v}\) is the violation set, \(s^{}\) is the next sampled state, and \(\) is a discount parameter \(0<1\) to ensure convergence of \(p(s)\). Furthermore, and crucially, we ensure the learning rate of this REF is on a slower time scale than the policy and its critics but faster than the lagrange multiplier.

Bringing the concepts covered above, we present our full optimization equation:

\[_{}_{}L(,)=_{}_{} {}_{s d_{0}}[-V^{}(s)+ V^{}_{c}(s)] (1-p(s))+V^{}_{c}(s) p(s).\] (4)

We show the design of our algorithm **RESPO** in an actor-critic framework in Algorithm 1. Note that the \(V\) and \(V_{c}\) have corresponding \(Q\) functions: \(V^{}(s)=}_{a(|s)}Q(s,a)\) and \(V^{}_{c}(s)=}_{a(|s)}Q_{c}(s,a)\). The gradients' definitions are found in the appendix. We use operator \(_{}\) to indicate the projection of vector \(^{n}\) to the closest point in compact and convex set \(^{n}\). Specifically, \(_{}=_{}||-||^{2}\). \(_{}\) is similarly defined.

### Convergence Analysis

We provide convergence analysis of our algorithm for Finite MDPs (finite bounded state and action space sizes, maximum horizon \(T\), reward bounded by \(R_{}\), and cost bounded by \(H_{}\)) under reasonable assumptions. We demonstrate our algorithm almost surely finds a locally optimal policy for our RESPO formulation, based on the following assumptions:

\(\) **A1 (Step size)**: Step sizes follow schedules \(\{_{1}(k)\}\), \(\{_{2}(k)\}\), \(\{_{3}(k)\}\), \(\{_{4}(k)\}\) where:

\(_{k}_{i}(k)=\) and \(_{k}_{i}(k)^{2}<, i\{1,2,3,4\},\) and \(_{j}(k)=o(_{j-1}(k)), j\{2,3,4\}.\)

The reward returns and cost returns critic value functions must follow the fastest schedule \(_{1}(k)\), the policy must follow the second fastest schedule \(_{2}(k)\), the REF must follow the second slowest schedule \(_{3}(k)\), and finally, the lagrange multiplier should follow the slowest schedule \(_{4}(k)\).

\(\) **A2 (_Strict Feasibility_): \((|;)\) such that \( s_{I}\) where \(^{*}(s)=0\), \(V_{c}^{_{}}(s) 0\).

\(\) **A3 (_Differentiability and Lipschitz Continuity_): For all state-action pairs \((s,a)\), we assume value and cost Q functions \(Q(s,a;),Q_{c}(s,a;)\), policy \((a|s;)\), and REF \(p(s,a;)\) are continuously differentiable in \(,,,\) respectively. Furthermore, \(_{}_{}\) and, for all state-action pairs \((s,a)\), \(_{}(a|s;)\) are Lipschitz continuous functions in \(\) and \(\) respectively.

The detailed proof of the following result is provided in the appendix.

**Theorem 2**.: Given Assumptions **A1**-**A3**, the policy updates in Algorithm 1 will almost surely converge to a locally optimal policy for our proposed optimization in Equation RESPO.

## 6 Experiments

**Baselines.** The baselines we compare are CMDP-based or solve for hard constraints. The CMDP baselines are Lagrangian-based Proximal Policy Optimization (**PPOLag**) based on , Constraint-Rectified Policy Optimization (**CRPO**) , Penalized Proximal Policy Optimization (**P3O**) , and Projection-Based Constrained Policy Optimization (**PCPO**) . The hard constraints baselines are **RCRL**, **CBF** with constraint \((s)+ h(s) 0\), and Feasibile Actor-Critic (**FAC**) . We classify **FAC** among the hard constraint approaches because we make its cost threshold \(=0\) in order to better compare using NN lagrange multiplier with our REF approach in **RESPO**. We include the unconstrained Vanilla **PPO** baseline for reference.

Figure 1: We compare the performance of our algorithm with other SOTA baselines in Safety Gym (left two figures), Safety PyBullet (middle two figures), and Safety MuJoCo (right two figures).

**Benchmarks.** We compare **RESPO** with the baselines in a diverse suite of safety environments. We consider high-dimensional environments in Safety Gym  (namely PointButton and CarGoal), Safety PyBullet  (namely DroneCircle and BallRun), and Safety MuJoCo , (namely Safety HalfCheetah and Reacher). We also show our algorithm in a multi-drone environment with _multiple hard and soft constraints_. More detailed experiment explanations and evaluations are in the appendix.

### Main Experiments in Safety Gym, Safety PyBullet, and MuJoCo

We compare our algorithm with SOTA benchmarks on various high-dimensional (up to \(76\)D observation space), complex environments in the stochastic setting, i.e., where the environment and/or policy are stochastic. Particularly, we examine environments in Safety Gym, Safety PyBullet, and Safety MuJoCo. The environments provide reward for achieving a goal behavior or location, while the cost is based on tangible (e.g., avoiding quickly moving objects) and non-tangible (e.g., satisfying speed limit) constraints. Environments like PointButton require intricate behavior where specific buttons must be reached while avoiding multiple moving obstacles, stationary hazards, and wrong buttons.

Overall, **RESPO** achieves the best balance between optimizing reward and minimizing cost violations across all the environments. Specifically, our approach generally has the highest reward performance (see the red lines from the top row of Figure 2) among the safety-constrained algorithms while maintaining reasonably low to \(0\) cost violations (like in HalfCheetah). When **RESPO** performs the second highest, the highest-performing safety algorithm always incurs several times more violations than **RESPO** - for instance, **RCRL** in PointButton or **PPOLag** in Drone Circle. Non-primal-dual CMDP approaches, namely **CRPO**, **P3O**, and **PCPO** generally satisfy their cost threshold constraints, but their reward performances rarely exceed that of **PPOLag**. **RCRL** generally has extremes of high reward and high cost, like in BallRun, or low reward and low cost, like in CarGoal. **FAC** and **CBF** generally have conservative behavior that sacrifices reward performance to minimize cost.

### Hard and Soft Constraints

We also demonstrate **RESPO**'s performance in an environment with multiple hard and soft constraints. The environment requires controlling two drones to pass through a tunnel one at a time while respecting certain distance requirements. The reward is given for quickly reaching the goal positions. The two hard constraints involve **(H1)** ensuring neither drone collides into the wall and **(H2)** the distance between the two drones is more than \(0.5\) to ensure they do not collide. The soft constraint is that the two drones are within \(0.8\) of each other to ensure real-world communication. It is preferable to prioritize hard constraint **H1** over hard constraint **H2**, since colliding with the wall may have more serious consequences to the drones rather than violations of an overly precaution distance constraint.

Our approach, in the leftmost of Figure 3, successfully reaches the goal while avoiding the wall obstacles in all time steps. We are able to prioritize this wall avoidance constraint over the second hard constraint. This can be seen particularly in between the blue to cyan time period where the higher

Figure 2: Comparison of RESPO with baselines in Safety Gym and PyBullet environments. The plots in the first row show performance measured in rewards (higher is better); those in second row show cost (lower is better). RESPO (red curves) achieves the best balance of maximizing reward and minimizing cost. When other methods achieve higher rewards than RESPO, they achieve much higher costs as well. E.g., in PointButton, RCRL has slightly higher rewards, but accumulates over \(3\) violations than RESPO. Note, Vanilla PPO is unconstrained.

Drone makes way for the lower Drone to pass through but needs to make a drop to make a concave parabolic trajectory to the goal. Nonetheless, the hard constraints are almost always satisfied, thereby producing the behavior of allowing one drone through the tunnel at a time. The soft constraints are satisfied at the beginning and end but are violated, reasonably, in the middle of the episode since only one drone can pass through the tunnel at a time, thereby forcing the other drone into a standby mode.

### Ablation Studies

We also perform ablation studies to experimentally confirm the design choices we made based on the theoretically established convergence and optimization framework. We particularly investigate the effects of changing the learning rate of our reachability function as well as changing the optimization framework. We present the results of changing the learning rate for REF in Figure 5 while our results for the ablation studies on our optimization framework can be seen in Figure 6.

In Figure 5, we show the effects of making the learning rate of REF slower and faster than the one we use in accordance with Assumption \(1\). From these experiments, changing the learning rate in either direction produces poor reward performance. A fast learning rate makes the REF converge to the likelihood of infeasibility for the current policy, which can be suboptimal. But a very slow learning rate means the function takes too long to converge - the lagrange multiplier may meanwhile become very large, thus making it too difficult to optimize for reward returns. In both scenarios, the algorithm with modified learning rates produces conservative behavior that sacrifices reward performance.

In Figure 6, we compare **RESPO** with RCRL implemented with our REF and **PPOLag** in the CMDP framework with cost threshold \(=0\) to ensure hard constraint satisfaction. The difference between **RESPO** and the RCRL-based ablation approach is that the ablation still uses \(V_{h}^{}\) instead of \(V_{c}^{}\). The ablation aproach's high cumulative cost can be attributed to the limitations of using \(V_{h}^{}\) - particularly, the lower sensitivity of \(V_{h}^{}\) to safety improvement and its lack of guarantees on feasible set (re)entrance. **PPOLag** with \(=0\) produces low violations but also very low reward performance that's close to zero. Naively using \(V_{c}^{}\) in a hard constraints framework leads to very conservative

Figure 4: Comparison of RESPO with baselines in MuJoCo. Higher rewards (first row plots) and lower costs (second row plots) are better. In HalfCheetah, RESPO has highest reward among safety baselines, with \(0\) violations. In Reacher, RESPO has good rewards, low costs.

behavior that sacrifices reward performance. Ultimately, this ablation study experimentally highlights the importance of learning our REF _and_ using value function \(V_{c}^{}\) in our algorithm's design.

## 7 Discussion and Conclusion

In summary, we proposed a new optimization formulation and a class of algorithms for safety-constrained reinforcement learning. Our framework optimizes reward performance for states in least-violation policy's feasible state space while maintaining persistent safety as well as providing the safest behavior in other states by ensuring entrance into the feasible set with minimal cumulative discounted costs. Using our proposed reachability estimation function, we prove our algorithm's class of actor-critic methods converge a locally optimal policy for our proposed optimization. We provide extensive experimental results on a diverse suite of environments in Safety Gym, PyBullet, and MuJoCo, and an environment with multiple hard and soft constraints, to demonstrate the effectiveness of our algorithm when compared with several SOTA baselines. We leave open various extensions to our work to enable real-world deployment of our algorithm. These include constraining violations during training, guaranteeing safety in single-lifetime reinforcement learning, and ensuring policies don't forget feasible sets as environment tasks change. Our approach of learning the optimal REF to reduce the state space into a low-dimensional likelihood representation to guide training for high-dimensional policies can have applications in other learning problems in answering binary classification or likelihood-based questions about dynamics in high-dimension feature spaces.

## 8 Acknowledgements

This material is based on work supported by NSF Career CCF 2047034, NSF CCF DASS 2217723, ONR YIP N00014-22-1-2292, and Amazon Research Award.

Figure 6: Ablation study on optimization framework. Top row plots show performance measured in reward (higher is better). Bottom row plots show cost (lower is better). RESPO (red curve) achieves best balance of maximizing reward and minimizing cost. RCRL framework implemented with our REF without \(V_{c}^{}\) incurs very high costs. PPOLog in CMDP framework with \(=0\) has very low reward performance. So, learning REF and learning \(V_{c}^{}\) are both crucial components in our design and work in tandem to contribute to RESPO’s efficacy.