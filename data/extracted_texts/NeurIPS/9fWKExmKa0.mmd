# DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics

Kaiwen Zheng\({}^{*}\)\({}^{1}\), Cheng Lu\({}^{*}\)\({}^{1}\), Jianfei Chen\({}^{1}\), Jun Zhu\({}^{1}\)\({}^{1}\)\({}^{1}\)Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, THBI Lab

\({}^{1}\)Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China

\({}^{2}\)Shengshu Technology, Beijing \({}^{3}\)Pazhou Lab (Huangpu), Guangzhou, China

{zkwthu,lucheng.lc15}@gmail.com; {jianfeic, dcszj}@tsinghua.edu.cn

Work done during an internship at Shengshu TechnologyEqual contributionCorresponding author

###### Abstract

Diffusion probabilistic models (DPMs) have exhibited excellent performance for high-fidelity image generation while suffering from inefficient sampling. Recent works accelerate the sampling procedure by proposing fast ODE solvers that leverage the specific ODE form of DPMs. However, they highly rely on specific parameterization during inference (such as noise/data prediction), which might not be the optimal choice. In this work, we propose a novel formulation towards the optimal parameterization during sampling that minimizes the first-order discretization error of the ODE solution. Based on such formulation, we propose _DPM-Solver-v3_, a new fast ODE solver for DPMs by introducing several coefficients efficiently computed on the pretrained model, which we call _empirical model statistics_. We further incorporate multistep methods and a predictor-corrector framework, and propose some techniques for improving sample quality at small numbers of function evaluations (NFE) or large guidance scales. Experiments show that DPM-Solver-v3 achieves consistently better or comparable performance in both unconditional and conditional sampling with both pixel-space and latent-space DPMs, especially in 5\(\)10 NFEs. We achieve FIDs of 12.21 (5 NFE), 2.51 (10 NFE) on unconditional CIFAR10, and MSE of 0.55 (5 NFE, 7.5 guidance scale) on Stable Diffusion, bringing a speed-up of 15%\(\)30% compared to previous state-of-the-art training-free methods. Code is available at https://github.com/thu-ml/DPM-Solver-v3.

## 1 Introduction

Diffusion probabilistic models (DPMs) [47; 15; 51] are a class of state-of-the-art image generators. By training with a strong encoder, a large model, and massive data as well as leveraging techniques such as guided sampling, DPMs are capable of generating high-resolution photorealistic and artistic images on text-to-image tasks. However, to generate high-quality visual content, DPMs usually require hundreds of model evaluations to gradually remove noise using a pretrained model , which is much more time-consuming compared to other deep generative models such as generative adversarial networks (GANs) . The sampling overhead of DPMs emerges as a crucial obstacle hindering their integration into downstream tasks.

To accelerate the sampling process of DPMs, one can employ training-based methods [37; 53; 45] or training-free samplers [48; 51; 28; 3; 52; 56]. We focus on the latter approach since it requires noextra training and is more flexible. Recent advanced training-free samplers [56; 31; 32; 58] mainly rely on the ODE form of DPMs, since its absence of stochasticity is essential for high-quality samples in around 20 steps. Besides, ODE solvers built on exponential integrators  converge faster. To change the original diffusion ODE into the form of exponential integrators, they need to cancel its linear term and obtain an ODE solution, where only the noise predictor needs to be approximated, and other terms can be exactly computed. Besides, Lu et al.  find that it is better to use another ODE solution where instead the data predictor needs to be approximated. How to choose such model parameterization (e.g., noise/data prediction) in the sampling of DPMs remains unrevealed.

In this work, we systematically study the problem of model parameterization and ODE formulation for fast sampling of DPMs. We first show that by introducing three types of coefficients, the original ODE solution can be reformulated to an equivalent one that contains a new model parameterization. Besides, inspired by exponential Rosenbrock-type methods  and first-order discretization error analysis, we also show that there exists an optimal set of coefficients efficiently computed on the pretrained model, which we call _empirical model statistics_ (EMS). Building upon our novel ODE formulation, we further propose a new high-order solver for diffusion ODEs named _DPM-Solver-v3_, which includes a multistep predictor-corrector framework of any order, as well as some novel techniques such as pseudo high-order method to boost the performance at extremely few steps or large guidance scale.

We conduct extensive experiments with both pixel-space and latent-space DPMs to verify the effectiveness of DPM-Solver-v3. Compared to previous fast samplers, DPM-Solver-v3 can consistently improve the sample quality in 5\(\)20 steps, and make a significant advancement within 10 steps.

## 2 Background

### Diffusion Probabilistic Models

Suppose we have a \(D\)-dimensional data distribution \(q_{0}(_{0})\). Diffusion probabilistic models (DPMs) [47; 15; 51] define a forward diffusion process \(\{q_{t}\}_{t=0}^{T}\) to gradually degenerate the data \(_{0} q_{0}(_{0})\) with Gaussian noise, which satisfies the transition kernel \(q_{0t}(_{t}|_{0})=(_{t}_{0},_{t}^{2} )\), such that \(q_{T}(_{T})\) is approximately pure Gaussian. \(_{t},_{t}\) are smooth scalar functions of \(t\), which are called _noise schedule_. The transition can be easily applied by \(_{t}=_{t}_{0}+_{t}, (,)\). To train DPMs, a neural network \(_{}(_{t},t)\) is usually parameterized to predict the noise \(\) by minimizing \(_{_{0} q_{0}(_{0}),( ,),t(0,T)}[w(t)\|_{}(_{t},t)-\|_{2}^{2}]\), where \(w(t)\) is a weighting function. Sampling of DPMs can be performed by solving _diffusion ODE_ from time \(T\) to time \(0\):

\[_{t}}{}=f(t)_{t}+(t)}{2 _{t}}_{}(_{t},t),\] (1)

where \(f(t)=_{t}}{t}\), \(g^{2}(t)=^{2}}{t}-2_ {t}}{t}_{t}^{2}\). In addition, the conditional sampling by DPMs can be conducted by guided sampling [10; 16] with a conditional noise predictor \(_{}(_{t},t,c)\), where \(c\) is the condition. Specifically, classifier-free guidance  combines the unconditional/conditional model and obtains a new noise predictor \(^{}_{}(_{t},t,c) s_{ }(_{t},t,c)+(1-s)_{}(_{t},t,)\)

Figure 1: Random samples of Stable-Diffusion  with a classifier-free guidance scale 7.5, using only 5 number of function evaluations (NFE) and text prompt _“A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on arstation HQ”_.

where \(\) is a special condition standing for the unconditional case, \(s>0\) is the guidance scale that controls the trade-off between image-condition alignment and diversity; while classifier guidance  uses an extra classifier \(p_{}(c|_{t},t)\) to obtain a new noise predictor \(^{}_{}(_{t},t,c)_{} (_{t},t)-s_{t}_{} p_{}(c|_{t},t)\).

In addition, except for the noise prediction, DPMs can be parameterized as score predictor \(_{}(_{t},t)\) to predict \(_{} q_{t}(_{t},t)\), or data predictor \(_{}(_{t},t)\) to predict \(_{0}\). Under variance-preserving (VP) noise schedule which satisfies \(_{t}^{2}+_{t}^{2}=1\), "v" predictor \(_{}(_{t},t)\) is also proposed to predict \(_{t}-_{t}_{0}\). These different parameterizations are theoretically equivalent, but have an impact on the empirical performance when used in training [23; 59].

### Fast Sampling of DPMs with Exponential Integrators

Among the methods for solving the diffusion ODE (1), recent works [56; 31; 32; 58] find that ODE solvers based on exponential integrators  are more efficient and robust at a small number of function evaluations (<50). Specifically, an insightful observation by Lu et al.  is that, by change-of-variable from \(t\) to \(_{t}(_{t}/_{t})\) (half of the log-SNR), the diffusion ODE is transformed to

\[_{}}{}=_{ }}{_{}}_{}-_{}_{ }(_{},),\] (2)

where \(_{}_{}}{ }\). By utilizing the semi-linear structure of the diffusion ODE and exactly computing the linear term [56; 31], we can obtain the ODE solution as Eq. (3) (left). Such exact computation of the linear part reduces the discretization errors . Moreover, by leveraging the equivalence of different parameterizations, DPM-Solver++  rewrites Eq. (2) by the data predictor \(_{}(_{},)(_{}-_{ }_{}(_{},))/_{}\) and obtains another ODE solution as Eq. (3) (right). Such solution does not need to change the pretrained noise prediction model \(_{}\) during the sampling process, and empirically outperforms previous samplers based on \(_{}\).

\[_{t}}{_{t}}=_{s}}{_{s}}-_{_{s} }^{_{t}}e^{-}_{}(_{},) ,_{t}}{_{t}}=_{s}}{ _{s}}+_{_{s}}^{_{t}}e^{}_{}(_{ },)\] (3)

However, to the best of our knowledge, the parameterizations for sampling are still manually selected and limited to noise/data prediction, which are not well-studied.

## 3 Method

We now present our method. We start with a new formulation of the ODE solution with extra coefficients, followed by our high-order solver and some practical considerations. In the following discussions, we assume all the products between vectors are element-wise, and \(^{(k)}(_{},)=^{k}(_{ },)}{^{k}}\) is the \(k\)-th order total derivative of any function \(\) w.r.t. \(\).

### Improved Formulation of Exact Solutions of Diffusion ODEs

As mentioned in Sec. 2.2, it is promising to explore the semi-linear structure of diffusion ODEs for fast sampling [56; 31; 32]. _Firstly_, we reveal one key insight that we can choose the linear part according to Rosenbrock-type exponential integrators [19; 18]. To this end, we consider a general form of diffusion ODEs by rewriting Eq. (2) as

\[_{}}{}=_{}}{_{}}-_{})_ {}}_{}-_{ }(_{},)-_{}_{})}_{;=_{}(_{},)},\] (4)

where \(_{}\) is a \(D\)-dimensional undetermined coefficient depending on \(\). We choose \(_{}\) to restrict the Frobenius norm of the gradient of the non-linear part w.r.t. \(\):

\[_{}^{*}=*{argmin}_{_{}}_{p_{ }^{}(_{})}\|_{}_{}(_{ },)\|_{F}^{2},\] (5)

where \(p_{}^{}\) is the distribution of samples on the ground-truth ODE trajectories at \(\) (i.e., model distribution). Intuitively, it makes \(_{}\) insensitive to the errors of \(\) and cancels all the "linearity" of \(_{}\). With \(_{}=_{}^{*}\), by the "_variation-of-constants_" formula , starting from \(_{_{s}}\) at time \(s\), the exact solution of Eq. (4) at time \(t\) is

\[_{_{t}}=_{_{t}}e^{-_{_{s}}^{_{t}} _{}}_{_{s}}}{_{ _{s}}}-_{_{s}}^{_{t}}e^{_{_{s}}^{_{t }}_{}}_{}(_{}, )}_{},\] (6)

where \(_{}(_{},)_{}( _{},)}{_{}}\). To calculate the solution in Eq. (6), we need to approximate \(_{}\) for each \([_{s},_{t}]\) by certain polynomials .

_Secondly_, we reveal another key insight that we can choose different functions to be approximated instead of \(_{}\) and further reduce the discretization error, which is related to the total derivatives of the approximated function. To this end, we consider a scaled version of \(_{}\) i.e., \(_{}(_{},) e^{-_{_{s}}^{ }_{}}_{}(_{},)\) where \(_{}\) is a \(D\)-dimensional coefficient dependent on \(\), and then Eq. (6) becomes

\[_{_{t}}=_{_{t}}e^{-_{_{s}}^{_{t}} _{}}_{_{s}}}{_ {_{s}}}-_{_{s}}^{_{t}}e^{_{_{s}}^{ }(_{}+_{})}_{}(_{},)}_{}.\] (7)

Comparing with Eq. (6), we change the approximated function from \(_{}\) to \(_{}\) by using an additional scaling term related to \(_{}\). As we shall see, the first-order discretization error is positively related to the norm of the first-order derivative \(_{}^{(1)}=e^{-_{_{s}}^{}_{} }(_{}^{(1)}-_{}_{})\). Thus, we aim to minimize \(\|_{}^{(1)}-_{}_{}\|_{2}\), in order to reduce \(\|_{}^{(1)}\|_{2}\) and the discretization error. As \(_{}\) is a fixed function depending on the pretrained model, this minimization problem essentially finds a linear function of \(_{}\) to approximate \(_{}^{(1)}\). To achieve better linear approximation, we further introduce a bias term \(_{}^{D}\) and construct a function \(_{}\) satisfying \(_{}^{(1)}=e^{-_{_{s}}^{}_{} }(_{}^{(1)}-_{}_{}-_{})\), which gives

\[_{}(_{},) e^{-_{_{s}}^{ }_{}}_{}(_{},)- _{_{s}}^{}e^{-_{_{s}}^{}_{} }_{}.\] (8)

With \(_{}\), Eq. (7) becomes

\[_{_{t}}=_{_{t}}}^{ _{t}}_{}}}_{}_{_{s}}}{_{_{s}}}-_{ _{s}}^{_{t}}}^{}(_{ }+_{})}}_{}_{ }(_{},)}_{}+}^{ }e^{-_{_{s}}^{}_{}}_{ }}_{}.\] (9)

Such formulation is equivalent to Eq. (3) but introduces three types of coefficients and a new parameterization \(_{}\). We show in Appendix I.1 that the generalized parameterization \(_{}\) in Eq. (8) can cover a wide range of parameterization families in the form of \(_{}(_{},)=()_ {}(_{},)+()_{}+()\). We aim to reduce the discretization error by finding better coefficients than previous works .

Now we derive the concrete formula for analyzing the first-order discretization error. By replacing \(_{}(_{},)\) with \(_{}(_{_{s}},_{s})\) in Eq. (9), we obtain the first-order approximation \(}_{_{t}}=_{_{t}}e^{-_{_{s}}^{ _{t}}_{}}_{_{s}}} {_{_{s}}}-_{_{s}}^{_{t}}e^{_{_{s}}^{ _{s}}(_{}+_{})}(_{}( _{_{s}},_{s})+_{_{s}}^{}e^{-_{ _{s}}^{_{s}}_{}}_{} )\). As \(_{}(_{_{s}},_{s})=_{}(_{ },)+(_{s}-)_{}^{(1)}(_{}, )+((-_{s})^{2})\) by Taylor expansion, it follows that the first-order discretization error can be expressed as

\[}_{_{t}}-_{_{t}}=_{_{t}}e^{-_{ _{s}}^{_{t}}_{}}_{ _{s}}^{_{t}}e^{_{_{s}}^{_{s}}_{} }(-_{s})_{}^{(1)}(_{ },)-_{}_{}(_{},)- _{}+(h^{3}),\] (10)

where \(h=_{t}-_{s}\). Thus, given the optimal \(_{}=_{}^{*}\) in Eq. (5), the discretization error \(}_{_{t}}-_{_{t}}\) mainly depends on \(_{}^{(1)}-_{}_{}-_{}\). Based on this insight, we choose the coefficients \(_{},_{}\) by solving

\[_{}^{*},_{}^{*}=*{argmin}_{_{ },_{}}_{_{}^{}(_{ })}[\|_{}^{(1)}(_{},)-_{ }_{}(_{},)-_{}\|_{2 }^{2}].\] (11)

For any \(\), \(_{}^{*},

### Developing High-Order Solver

In this section, we propose our high-order solver for diffusion ODEs with local accuracy and global convergence order guarantee by leveraging our proposed solution formulation in Eq. (9). The proposed solver and analyses are highly motivated by the methods of exponential integrators [17; 18] in the ODE literature and their previous applications in the field of diffusion models [56; 31; 32; 58]. Though the EMS are designed to minimize the first-order error, they can also help our high-order solver (see Appendix I.2).

For simplicity, denote \((_{s},_{t}) e^{-_{_{s}}^{_{t} }_{}}\), \(_{_{s}}() e^{_{_{s}}^{}(_ {}_{}_{}}\), \(_{_{s}}()_{_{p}}^{}e^{-_{ _{s}}^{_{s}}_{}}_{}\). Though high-order ODE solvers essentially share the same mathematical principles, since we utilize a more complicated parameterization \(_{}\) and ODE formulation in Eq. (9) than previous works [56; 31; 32; 58], we divide the development of high-order solver into simplified local and global parts, which are not only easier to understand, but also neat and general for any order.

#### 3.2.1 Local Approximation

Firstly, we derive formulas and properties of the local approximation, which describes how we transit locally from time \(s\) to time \(t\). It can be divided into two parts: discretization of the integral in Eq. (9) and estimating the high-order derivatives in the Taylor expansion.

**Discretization.** Denote \(_{s}^{(k)}_{}^{(k)}(_{_{s}},_{ s})\). For \(n 0\), to obtain the \((n+1)\)-th order discretization of Eq. (9), we take the \(n\)-th order Taylor expansion of \(_{}(_{},)\) w.r.t. \(\) at \(_{s}\): \(_{}(_{},)=_{k=0}^{n})^{k}}{k!}_{s}^{(k)}+((-_{s})^{n+1})\). Substituting it into Eq. (9) yields

\[_{t}}{_{t}}=(_{s},_{t})(_{s}}{_{s}}-_{_{s}}^{_{t}}_{_{s}}( )_{_{s}}()-_{k=0}^{n}_{ s}^{(k)}_{_{s}}^{_{t}}_{_{s}}())^{k}}{k!})+(h^{n+2})\] (12)

Here we only need to estimate the \(k\)-th order total derivatives \(_{}^{(k)}(_{_{s}},_{s})\) for \(0 k n\), since the other terms are determined once given \(_{s},_{t}\) and \(_{},_{},_{}\), which we'll discuss next.

**High-order derivative estimation.** For \((n+1)\)-th order approximation, we use the finite difference of \(_{}(_{},)\) at previous \(n+1\) steps \(_{i_{n}},,_{i_{1}},_{s}\) to estimate each \(_{}^{(k)}(_{_{s}},_{s})\). Such derivation is to match the coefficients of Taylor expansions. Concretely, denote \(_{k}_{i_{k}}-_{s},_{i_{k}}_ {}(_{_{i_{k}}},_{i_{k}})\), and the estimated high-order derivatives \(}_{s}^{(k)}\) can be solved by the following linear system:

\[_{1}&_{1}^{2}&&_{1}^{n}\\ &&&\\ _{n}&_{n}^{2}&&_{n}^{n} }_{s}^{(1)}\\ \\ }_{s}^{(n)}}{n!}=_{i_{1}}- _{s}\\ \\ _{i_{n}}-_{s}\] (13)

Figure 2: Illustration of our high-order solver. (a) \((n+1)\)-th order local approximation from time \(s\) to time \(t\), provided \(n\) extra function values of \(_{}\). (b) Multistep predictor-corrector procedure as our global solver. A combination of second-order predictor and second-order corrector is shown. \(a_{,i-1},b_{,i-1},c_{,i-1}\) are abbreviations of coefficients in Eq. (8).

Then by substituting \(}_{s}^{(k)}\) into Eq. (12) and dropping the \((h^{n+2})\) error terms, we obtain the \((n+1)\)-th order local approximation:

\[}_{t}}{_{t}}=(_{s},_{t})(_{s}}{_{s}}-_{_{s}}^{_{t}}_{_{s}}( )_{_{s}}()-_{k=0}^{n}}_{s}^{(k)}_{_{s}}^{_{t}}_{_{s}}() )^{k}}{k!})\] (14)

where \(}_{}^{(0)}(_{_{s}},_{s})=_{s}\). Eq. (13) and Eq. (14) provide an update rule to transit from time \(s\) to time \(t\) and get an approximated solution \(}_{t}\), when we already have the solution \(_{s}\). For \((n+1)\)-th order approximation, we need \(n\) extra solutions \(_{_{s_{k}}}\) and their corresponding function values \(_{i_{k}}\). We illustrate the procedure in Fig. 2(a) and summarize it in Appendix C.2. In the following theorem, we show that under some assumptions, such local approximation has a guarantee of order of accuracy.

**Theorem 3.1** (Local order of accuracy, proof in Appendix B.2.1).: _Suppose \(_{_{t_{k}}}\) are exact (i.e., on the ground-truth ODE trajectory passing \(_{s}\)) for \(k=1,,n\), then under some regularity conditions detailed in Appendix B.1, the local truncation error \(}_{t}-_{t}=(h^{n+2})\), which means the local approximation has \((n+1)\)-th order of accuracy._

Besides, we have the following theorem showing that, whatever the order is, the local approximation is unbiased given our choice of \(_{},_{}\) in Eq. (11). In practice, the phenomenon of reduced bias can be empirically observed (Section 4.3).

**Theorem 3.2** (Local unbiasedness, proof in Appendix B.4).: _Given the optimal \(_{}^{*},_{s}^{*}\) in Eq. (11), For the \((n+1)\)-th order approximation, suppose \(_{_{i_{1}}},,_{_{i_{n}}}\) are on the ground-truth ODE trajectory passing \(_{_{s}}\), then \(_{p_{_{s}}^{}(_{s})}[}_{t}-_{t}]=0\)._

#### 3.2.2 Global Solver

Given \(M+1\) time steps \(\{t_{i}\}_{i=0}^{M}\), starting from some initial value, we can repeat the local approximation \(M\) times to make consecutive transitions from each \(t_{i-1}\) to \(t_{i}\) until we reach an acceptable solution. At each step, we apply multistep methods  by caching and reusing the previous \(n\) values at timesteps \(t_{i-1-n},,t_{i-2}\), which is proven to be more stable when NFE is small [32; 56]. Moreover, we also apply the predictor-corrector method  to refine the approximation at each step without introducing extra NFE. Specifically, the \((n+1)\)-th order predictor is the case of the local approximation when we choose \((t_{i_{n}},,t_{i_{1}},s,t)=(t_{i-1-n},,t_{i-2},t_{i-1},t_{i})\), and the \((n+1)\)-th order corrector is the case when we choose \((t_{i_{n}},,t_{i_{1}},s,t)=(t_{i-n},,t_{i-2},t_{i},t_{i-1},t_{i})\). We present our \((n+1)\)-th order multistep predictor-corrector procedure in Appendix C.2. We also illustrate a second-order case in Fig. 2(b). Note that different from previous works, in the local transition from \(t_{i-1}\) to \(t_{i}\), the previous function values \(}_{i_{k}}\) (\(1 k n\)) used for derivative estimation are dependent on \(i\) and are different during the sampling process because \(_{}\) is dependent on the current \(t_{i-1}\) (see Eq. (8)). Thus, we directly cache \(}_{i},}_{i}\) and reuse them to compute \(}_{i}\) in the subsequent steps. Notably, our proposed solver also has a global convergence guarantee, as shown in the following theorem. For simplicity, we only consider the predictor case and the case with corrector can also be proved by similar derivations in .

**Theorem 3.3** (Global order of convergence, proof in Appendix B.2.2).: _For \((n+1)\)-th order predictor, if we iteratively compute a sequence \(\{}_{i}\}_{i=0}^{M}\) to approximate the true solutions \(\{_{i}\}_{i=0}^{M}\) at \(\{t_{i}\}_{i=0}^{M}\), then under both local and global assumptions detailed in Appendix B.1, the final error \(|}_{M}-_{M}|=(h^{n+1})\), where \(||\) denotes the element-wise absolute value, and \(h=_{1 i M}(_{i}-_{i-1})\)._

### Practical Techniques

In this section, we introduce some practical techniques that further improve the sample quality in the case of small NFE or large guidance scales.

**Pseudo-order solver for small NFE.** When NFE is extremely small (e.g., 5\(\)10), the error at each timestep becomes rather large, and incorporating too many previous values by high-order solver at each step will cause instabilities. To alleviate this problem, we propose a technique called _pseudo-order_ solver: when estimating the \(k\)-th order derivative, we only utilize the previous \(k+1\) function values of \(_{}\), instead of all the \(n\) previous values as in Eq. (13). For each \(k\), we can obtain \(}_{s}^{(k)}\) by solving a part of Eq. (13) and taking the last element:

\[_{1}&_{1}^{2}&&_{1}^{k}\\ &&&\\ _{k}&_{k}^{2}&&_{k}^{k} \\ \\ }^{(k)}}{k!}=_{i_{1}}-_{s}\\ \\ _{i_{k}}-_{s}, k=1,2,,n\] (15)

In practice, we do not need to solve \(n\) linear systems. Instead, the solutions for \(}^{(k)}_{s},k=1,,n\) have a simpler recurrence relation similar to Neville's method  in Lagrange polynomial interpolation. Denote \(i_{0} s\) so that \(_{0}=_{i_{0}}-_{s}=0\), we have

**Theorem 3.4** (Pseudo-order solver).: _For each \(k\), the solution in Eq. (15) is \(}^{(k)}_{s}=k!D_{0}^{(k)}\), where_

\[ D_{l}^{(0)}&_{i_{l}}, l=0,1,,n\\ D_{l}^{(k)}&^{(k-1)}-D_{l}^{(k-1)}}{ _{l+k}-_{l}}, l=0,1,,n-k\] (16)

Proof in Appendix B.3. Note that the pseudo-order solver of order \(n>2\) no longer has the guarantee of \(n\)-th order of accuracy, which is not so important when NFE is small. In our experiments, we mainly rely on two combinations: when we use \(n\)-th order predictor, we then combine it with \(n\)-th order corrector or \((n+1)\)-th pseudo-order corrector.

Half-corrector for large guidance scales.When the guidance scale is large in guided sampling, we find that corrector may have negative effects on the sample quality. We propose a useful technique called _half-corrector_ by using the corrector only in the time region \(t 0.5\). Correspondingly, the strategy that we use corrector at each step is called _full-corrector_.

### Implementation Details

In this section, we give some implementation details about how to compute and integrate the EMS in our solver and how to adapt them to guided sampling.

Estimating EMS.For a specific \(\), the EMS \(^{}_{},^{}_{},^{}_{}\) can be estimated by firstly drawing \(K\) (1024\(\)4096) datapoints \(_{} p_{}^{}(_{})\) with 200-step DPM-Solver++  and then analytically computing some terms related to \(_{}\) (detailed in Appendix C.1.1). In practice, we find it both convenient and effective to choose the distribution of the dataset \(q_{0}\) to approximate \(p_{0}^{}\). Thus, without further specifications, we directly use samples from \(q_{0}\).

Estimating integrals of EMS.We estimate EMS on \(N\) (\(120 1200\)) timesteps \(_{j_{0}},_{j_{1}},,_{j_{N}}\) and use trapezoidal rule to estimate the integrals in Eq. (12) (see Appendix I.3 for the estimation error analysis). We also apply some pre-computation for the integrals to avoid extra computation costs during sampling, detailed in Appendix C.1.2.

Adaptation to guided sampling.Empirically, we find that within a common range of guidance scales, we can simply compute the EMS on the model without guidance, and it can work for both unconditional sampling and guided sampling cases. See Appendix J for more discussions.

### Comparison with Existing Methods

By comparing with existing diffusion ODE solvers that are based on exponential integrators [56; 31; 32; 58], we can conclude that (1) Previous ODE formulations with noise/data prediction are special cases of ours by setting \(_{},_{},_{}\) to specific values. (2) Our first-order discretization can be seen as improved DDIM. See more details in Appendix A.

## 4 Experiments

In this section, we show that DPM-Solver-v3 can achieve consistent and notable speed-up for both unconditional and conditional sampling with both pixel-space and latent-space DPMs. We conduct extensive experiments on diverse image datasets, where the resolution ranges from 32 to 256. First, we present the main results of sample quality comparison with previous state-of-the-art training-freemethods. Then we illustrate the effectiveness of our method by visualizing the EMS and samples. Additional ablation studies are provided in Appendix G. On each dataset, we choose a sufficient number of timesteps \(N\) and datapoints \(K\) for computing the EMS to reduce the estimation error, while the EMS can still be computed within hours. After we obtain the EMS and precompute the integrals involving them, there is **negligible extra overhead** in the sampling process. We provide the runtime comparison in Appendix E. We refer to Appendix D for more detailed experiment settings.

### Main Results

We present the results in \(5 20\) number of function evaluations (NFE), covering both few-step cases and the almost converged cases, as shown in Fig. 3 and Fig. 4. For the sake of clarity, we mainly compare DPM-Solver-v3 to DPM-Solver++  and UniPC , which are the most state-of-the-art diffusion ODE solvers. We also include the results for DEIS  and Heun's 2nd order method in EDM , but only for the datasets on which they originally reported. We don't show the results for other methods such as DDIM , PNDM , since they have already been compared in previous works and have inferior performance. The quantitative results on CIFAR10  are listed in Table 1, and more detailed quantitative results are presented in Appendix F.

#### Unconditional sampling

We first evaluate the unconditional sampling performance of different methods on CIFAR10  and LSUN-Bedroom . For CIFAR10 we use two pixel-space DPMs,

Figure 4: Conditional sampling results. We report the FID\(\) or MSE\(\) of the methods with different numbers of function evaluations (NFE), evaluated on 10k samples.

Figure 5: Visualization of the EMS \(_{}\), \(_{}\), \(_{}\) w.r.t. \(\) estimated on different models.

Figure 3: Unconditional sampling results. We report the FID\(\) of the methods with different numbers of function evaluations (NFE), evaluated on 50k samples. 1We borrow the results reported in their original paper directly.

one is based on ScoreSDE  which is a widely adopted model by previous samplers, and another is based on EDM  which achieves the best sample quality. For LSUN-Bedroom, we use the latent-space Latent-Diffusion model . We apply the multistep 3rd-order version for DPM-Solver++, UniPC and DPM-Solver-v3 by default, which performs best in the unconditional setting. For UniPC, we report the better result of their two choices \(B_{1}(h)=h\) and \(B_{2}(h)=e^{h}-1\) at each NFE. For our DPM-Solver-v3, we tune the strategies of whether to use the pseudo-order predictor/correct at each NFE on CIFAR10, and use the pseudo-order corrector on LSUN-Bedroom. As shown in Fig. 3, we find that DPM-Solver-v3 can achieve consistently better FID, which is especially notable when NFE is 5\(\)10. For example, we improve the FID on CIFAR10 with 5 NFE from \(23\) to \(12\) with ScoreSDE, and achieve an FID of \(2.51\) with only \(10\) NFE with the advanced DPM provided by EDM. On LSUN-Bedroom, with around 12 minutes computing of the EMS, DPM-Solver-v3 converges to the FID of 3.06 with 12 NFE, which is approximately **60% sampling cost** of the previous best training-free method (20 NFE by UniPC).

**Conditional sampling.** We then evaluate the conditional sampling performance, which is more widely used since it allows for controllable generation with user-customized conditions. We choose two conditional settings, one is classifier guidance on pixel-space Guided-Diffusion  model trained on ImageNet-256 dataset  with 1000 class labels as conditions; the other is classifier-free guidance on latent-space Stable-Diffusion model  trained on LALION-5B dataset  with CLIP  embedded text prompts as conditions. We evaluate the former at the guidance scale of \(2.0\), following the best choice in ; and the latter at the guidance scale of \(1.5\) (following the original paper) or \(7.5\) (following the official code) with prompts random selected from MS-COCO2014 validation set . Note that the FID of Stable-Diffusion samples saturates to 15.0\(\)16.0 even within 10 steps when the latent codes are far from convergence, possibly due to the powerful image decoder (see Appendix H). Thus, following , we measure the mean square error (MSE) between the generated latent code \(}\) and the ground-truth solution \(^{*}\) (i.e., \(\|}-^{*}\|_{2}^{2}/D\)) to evaluate convergence, starting from the same Gaussian noise. We obtain \(^{*}\) by 200-step DPM-Solver++, which is enough to ensure the convergence.

    &  &  \\   & & 5 & 6 & 8 & 10 & 12 & 15 & 20 & 25 \\  \({}^{}\)DEIS  & & 15.37 & \(\) & \(\) & 4.17 & \(\) & 3.37 & 2.86 & \(\) \\ DPM-Solver++  & ScoreSDE  & 28.53 & 13.48 & 5.34 & 4.01 & 4.04 & 3.32 & 2.90 & 2.76 \\ UniPC  & & 23.71 & 10.41 & 5.16 & 3.93 & 3.88 & 3.05 & 2.73 & 2.65 \\ DPM-Solver-v3 & & **12.76** & **7.40** & **3.94** & **3.40** & **3.24** & **2.91** & **2.71** & **2.64** \\  Heun’s 2nd  & & 320.80 & 103.86 & 39.66 & 16.57 & 7.59 & 4.76 & 2.51 & 2.12 \\ DPM-Solver++  & EDM  & 24.54 & 11.85 & 4.36 & 2.91 & 2.45 & 2.17 & 2.05 & 2.02 \\ UniPC  & & 23.52 & 11.10 & 3.86 & 2.85 & 2.38 & **2.08** & **2.01** & **2.00** \\ DPM-Solver-v3 & & **12.21** & **8.56** & **3.50** & **2.51** & **2.24** & 2.10 & 2.02 & **2.00** \\   

Table 1: Quantitative results on CIFAR10 . We report the FID\(\) of the methods with different numbers of function evaluations (NFE), evaluated on 50k samples. \({}^{}\)We borrow the results reported in their original paper directly.

Figure 6: Random samples of Latent-Diffusion  on LSUN-Bedroom  with only NFE = 5.

We apply the multistep 2nd-order version for DPM-Solver++, UniPC and DPM-Solver-v3, which performs best in conditional setting. For UniPC, we only apply the choice \(B_{2}(h)=e^{h}-1\), which performs better than \(B_{1}(h)\). For our DPM-Solver-v3, we use the pseudo-order corrector by default, and report the best results between using half-corrector/full-corrector on Stable-Diffusion (\(s=7.5\)). As shown in Fig. 4, DPM-Solver-v3 can achieve better sample quality or convergence at most NFEs, which indicates the effectiveness of our method and techniques under the conditional setting. It's worth noting that UniPC, which adopts an extra corrector, performs even worse than DPM-Solver++ when NFE<10 on Stable-Diffusion (\(s=7.5\)). With the combined effect of the EMS and the half-corrector technique, we successfully outperform DPM-Solver++ in such a case. Detailed comparisons can be found in the ablations in Appendix G.

### Visualizations of Estimated EMS

We further visualize the estimated EMS in Fig. 5. Since \(_{},_{},_{}\) are \(D\)-dimensional vectors, we average them over all dimensions to obtain a scalar. From Fig. 5, we find that \(_{}\) gradually changes from \(1\) to near \(0\) as the sampling proceeds, which suggests we should gradually slide from data prediction to noise prediction. As for \(_{},_{}\), they are more model-specific and display many fluctuations, especially for ScoreSDE model  on CIFAR10. Apart from the estimation error of the EMS, we suspect that it comes from the fluctuations of \(_{}^{(1)}\), which is caused by the periodicity of trigonometric functions in the positional embeddings of the network input. It's worth noting that the fluctuation of \(_{},_{}\) will not cause instability in our sampler (see Appendix J).

### Visual Quality

We present some qualitative comparisons in Fig. 6 and Fig. 1. We can find that previous methods tend to have a small degree of color contrast at small NFE, while our method is less biased and produces more visual details. In Fig. 1(b), we can observe that previous methods with corrector may cause distortion at large guidance scales (in the left-top image, a part of the castle becomes a hill; in the left-bottom image, the hill is translucent and the castle is hanging in the air), while ours won't. Additional samples are provided in Appendix K.

## 5 Conclusion

We study the ODE parameterization problem for fast sampling of DPMs. Through theoretical analysis, we find a novel ODE formulation with empirical model statistics, which is towards the optimal one to minimize the first-order discretization error. Based on such improved ODE formulation, we propose a new fast solver named DPM-Solver-v3, which involves a multistep predictor-corrector framework of any order and several techniques for improved sampling with few steps or large guidance scale. Experiments demonstrate the effectiveness of DPM-Solver-v3 in both unconditional conditional sampling with both pixel-space latent-space pre-trained DPMs, and the significant advancement of sample quality in 5\(\)10 steps.

**Limitations and broader impact** Despite the great speedup in small numbers of steps, DPM-Solver-v3 still lags behind training-based methods and is not fast enough for real-time applications. Besides, we conducted theoretical analyses of the local error, but didn't explore the global design spaces, such as the design of timestep schedules during sampling. And commonly, there are potential undesirable effects that DPM-Solver-v3 may be abused to accelerate the generation of fake and malicious content.