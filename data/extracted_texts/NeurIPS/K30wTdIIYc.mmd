# Controlling Text-to-Image Diffusion by Orthogonal Finetuning

Zeju Qiu

Weiyang Liu

 Haiwen Feng1Yuxuan Xue3Yao Feng1Zhen Liu1

Dan Zhang3

Adrian Weller2

Bernhard Scholkopf1

###### Abstract

Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method - Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images given a few images of a subject and a text prompt, and controllable generation where the goal is to enable the model to take in additional control signals. We empirically show that our OFT framework outperforms existing methods in generation quality and convergence speed.

+
Footnote â€ : This work was finished when ZQ was a research intern hosted by WL at MPI for Intelligent Systems.

## 1 Introduction

Recent text-to-image diffusion models  achieve impressive performance in text-guided control for high-fidelity image generation. Despite strong results, text guidance can still be ambiguous and insufficient to provide fine-grained and accurate control to the generated images. To address this shortcoming, we target two types of text-to-image generation tasks in this paper:

* **Subject-driven generation**: Given just a few images of a subject, the task is to generate images of the same subject in a different context using the guidance of a text prompt.
* **Controllable generation**: Given an additional control signal (_e.g._, Canny edges, segmentation maps), the task is to generate images following such a control signal and a text prompt.

Both tasks essentially boil down to how to effectively finetune text-to-image diffusion models without losing the pretraining generative performance. We summarize the desiderata for an effective finetuning method as: (1) _training efficiency_: having fewer trainable parameters and number of training epochs, and (2) _generalizability preservation_: preserving the high-fidelity and diverse generative performance. To this end, finetuning is typically done either by updating the neuron weights by a small learning rate (_e.g._, ) or by adding a small component with re-parameterized neuron weights (_e.g._, ). Despite simplicity, neither finetuning strategy is able to guarantee the preservation of pretraining generative performance. There is also a lack of principled understanding towards designing a good finetuning strategy and finding suitable hyperparameters such as the number of training epochs. A key difficulty is the lack of a measure for quantifying the preservation of pretrained generative ability. Existing finetuning methods implicitly assume that a smaller Euclidean distance between thefinetuned model and the pretrained model indicates better preservation of the pretrained ability. Due to the same reason, finetuning methods typically work with a very small learning rate. While this assumption may occasionally hold, we argue that the Euclidean difference to the pretrained model alone is unable to fully capture the degree of semantic preservation, and therefore a more structural measure to characterize the difference between the finetuned model and the pretrained model can greatly benefit the preservation of pretraining performance as well as finetuning stability.

Inspired by the empirical observation that hyperspherical similarity encodes semantic information well [7; 35; 36], we use hyperspherical energy  to characterize the pairwise relational structure among neurons. Hyperspherical energy is defined as the sum of hyperspherical similarity (_e.g.,_ cosine similarity) between all pairwise neurons in the same layer, capturing the level of neuron uniformity on the unit hypersphere . We hypothesize that a good finetuned model should have a minimal difference in hyperspherical energy compared to the pretrained model. A naive way is to add a regularizer such that the hyperspherical energy remains the same during the finetuning stage, but there is no guarantee that the hyperspherical energy difference can be well minimized. Therefore, we take advantage of an invariance property of hyperspherical energy - the pairwise hyperspherical similarity is provably preserved if we apply the same orthogonal transformation for all neurons. Motivated by such an invariance, we propose Orthogonal Finetuning (OFT) which adapts large text-to-image diffusion models to a downstream task without changing its hyperspherical energy. The central idea is to learn a layer-shared orthogonal transformation for neurons such that their pairwise angles are preserved. OFT can also be viewed as adjusting the canonical coordinate system for the neurons in the same layer. By jointly taking into consideration that smaller Euclidean distance between the finetuned model and the pretrained model implies better preservation of pretraining performance, we further propose an OFT variant - Constrained Orthogonal Finetuning (COFT) which constrains the finetuned model within the hypersphere of a fixed radius centered on the pretrained neurons.

The intuition for why orthogonal transformation works for finetuning neurons partially comes from 2D Fourier transform, with which an image can be decomposed as magnitude and phase spectrum. The phase spectrum, which is angular information between input and basis, preserves the major part of semantics. For example, the phase spectrum of an image, along with a random magnitude spectrum, can still reconstruct the original image without losing its semantics. This phenomenon suggests that changing the neuron directions is the key to semantically modifying the generated image (which is the goal of both subject-driven and controllable generation). However, changing neuron directions with a large degree of freedom will inevitably destroy the pretraining generative performance. To constrain the degree of freedom, we propose to preserve the angle between any pair of neurons, largely based on the hypothesis that the angles between neurons are crucial for representing the knowledge of neural networks. With this intuition, it is natural to learn layer-shared orthogonal transformation for neurons in each layer such that the hyperspherical energy stays unchanged.

We also draw inspiration from orthogonal over-parameterized training  which trains classification neural networks from scratch by orthogonally transforming a randomly initialized neural network. This is because a randomly initialized neural network yields a provably small hyperspherical energy in

Figure 1: (a) Subject-driven generation: OFT preserves the hyperspherical energy and yields more stable finetuning performance across different number of iterations, while both DreamBoth  and LoRa  do not. OFT can preserve hyperspherical energy and perform stable finetuning, while both LoRa and DreamBoth are unable. (b) Controllable generation: OFT is more sample-efficient in training and converges well with only 5% of the original dataset, while both ControlNet  and LoRa  cannot converge until 50% of the data is present. The hyperspherical energy comparison between LoRa and OFT is fair, since they finetune the same layers. ControlNet uses a different layer finetuning strategy, so its hyperspherical energy is not comparable. The detailed settings are given in the experiment section and Appendix A.

expectation and the goal of  is to keep hyperspherical energy small during training (small energy leads to better generalization in classification [30; 32]).  shows that orthogonal transformation is sufficiently flexible to train generalizable neural networks for classification problems. In contrast, we focus on finetuning text-to-image diffusion models for better controllability and stronger downstream generative performance. We emphasize the difference between OFT and  in two aspects. First, while  is designed to minimize the hyperspherical energy, OFT aims to preserve the same hyperspherical energy as the pretrained model so that the intrinsic pretrained structure will not be destroyed by finetuning. In the case of finetuning diffusion models, minimizing hyperspherical energy could destroy the original semantic structures. Second, OFT seeks to minimize the deviation from the pretrained model, which leads to the constrained variant. In contrast,  imposes no such constraints. The key to finetuning is to find a good trade-off between flexibility and stability, and we argue that our OFT framework effectively achieves this goal. Our contributions are listed below:

* Orthogonal Finetuning for guiding text-to-image diffusion models towards better controllability. To further improve stability, we propose a constrained variant which limits the angular deviation from the pretrained model.
* Compared to existing finetuning methods, OFT performs model finetuning while provably preserving the hyperspherical energy, which we empirically find to be an important measure of the generative semantic preservation of the pretrained model.
* We apply OFT to two tasks: subject-driven generation and controllable generation. We conduct a comprehensive empirical study and demonstrate significant improvement over prior work in terms of generation quality, convergence speed and finetuning stability. Moreover, OFT achieves better sample efficiency, as it converges well with a much smaller number of training images and epochs.
* For controllable generation, we introduce a new control consistency metric to evaluate the controllability. This core idea is to estimate the control signal from the generated image and then compare it with the origin control signal. The metric further validates the strong controllability of OFT.

## 2 Related Work

**Text-to-image diffusion models**. Tremendous progress [16; 39; 45; 50; 53] has been made in text-to-image generation, largely thanks to the rapid development in diffusion-based generative models [12; 20; 55; 56] and vision-language representation learning [1; 28; 29; 37; 44; 54; 57; 61]. GLIDE  and Imagen  train diffusion models in the pixel space. GLIDE trains the text encoder jointly with a diffusion prior using paired text-image data, while Imagen uses a frozen pretrained text encoder. Stable Diffusion  and DALL-E2  train diffusion models in the latent space. Stable Diffusion uses VQ-GAN  to learn a visual codebook as its latent space, while DALL-E2 adopts CLIP  to construct a joint latent embedding space for representing images and text. Other than diffusion models, generative adversarial networks [27; 48; 65; 67] and autoregressive models [13; 46; 62; 66] have also been studied in text-to-image generation. OFT is inherently a model-agnostic finetuning approach and can be applied to any text-to-image diffusion model.

**Subject-driven generation**. To prevent subject modification, [2; 39] consider a given mask from users as an additional condition. Inversion methods [8; 12; 15; 45] can be applied to modify the context without changing the subject.  can perform local and global editing without input masks. The methods above are unable to well preserve identity-related details of the subject. In Pivotal Tuning , a generator is finetuned around an initial inverted latent code with an additional regularization to preserve the identity. Similarly,  learns a personalized generative face prior from a collection of a person's face images.  can generate difference variations of an instance, but it may lose the instance-specific details. With a customized token and a few subject images, DreamBooth  finetunes the text-to-image diffusion model using a reconstruction loss and a class-specific prior preservation loss. OFT adopts the DreamBooth framework, but instead of performing naive finetuning with a small learning rate, OFT finetunes the model with orthogonal transformations.

**Controllable generation**. The task of image-to-image translation can be viewed as a form of controllable generation, and previous methods mostly adopt conditional generative adversarial networks [9; 23; 42; 60; 71]. Diffusion models are also used for image-to-image translation [52; 58; 59]. More recently, ControlNet  proposes to control a pretrained diffusion model by finetuning and adapting it to additional control signals and achieves impressive controllable generation performance. Another concurrent and similar work, T2I-Adapter , also finetunes a pretrained diffusion model in order to gain stronger controllability for the generated images. Following the same task setting in [38; 68], we apply OFT to finetune pretrained diffusion models, yielding consistently better controllability with fewer training data and less finetuning parameters. More significantly, OFT does not introduce any additional computational overhead during test-time inference.

**Model finetuning**. Finetuning large pretrained models on downstream tasks has been increasingly popular nowadays [3; 11; 17]. As a form of finetuning, adaptation methods (_e.g._, [21; 22; 43]) are heavily studied in natural language processing. LoRA  is the most relevant work to OFT, and it assumes a low-rank structure for the additive weight update during finetuning. In contrast, OFT uses layer-shared orthogonal transformation to update neuron weights in a multiplicative manner, and it provably preserves the pair-wise angles among neurons in the same layer, yielding better stability.

## 3 Orthogonal Finetuning

### Why Does Orthogonal Transformation Make Sense?

We start by discussing why orthogonal transformation is desirable in finetuning text-to-image diffusion models. We decompose this question into two smaller ones: (1) why we want to finetune the angle of neurons (_i.e._, direction), and (2) why we adopt orthogonal transformation to finetune angles.

For the first question, we draw inspiration from the empirical observation in [7; 35] that angular feature difference well characterizes the semantic gap. SphereNet  shows that training a neural network with all neurons normalized onto a unit hypersphere yields comparable capacity and even better generalizability, implying that the direction of neurons can fully capture the most important information from data. To better demonstrate the importance of neuron angles, we conduct a toy experiment in Figure 2 where we train a standard convolutional autoencoder on some flower images. In the training stage, we use the standard inner product to produce the feature map (\(z\) denotes the element output of the convolution kernel \(\) and \(\) is the input in the sliding window). In the testing stage, we compare three ways to generate the feature map: (a) the inner product used in training, (b) the magnitude information, and (c) the angular information. The results in Figure 2 show that the angular information of neurons can almost perfectly recover the input images, while the magnitude of neurons contains no useful information. We emphasize that we do not apply the cosine activation (c) during training, and the training is done only based on inner product. The results imply that the angles (directions) of neurons play the major role in storing the semantic information of the input images. In order to modify the semantic information of images, finetuning the neuron directions will likely be more effective.

For the second question, the simplest way to finetune direction of neurons is to simultaneously rotate / reflect all the neurons (in the same layer), which naturally brings in orthogonal transformation. It may be more flexible to use some other angular transformation that rotates different neurons with different angles, but we find that orthogonal transformation is a sweet spot between flexibility and regularity. Moreover,  shows that orthogonal transformation is sufficiently powerful for learning neural networks. To support our argument, we perform an experiment to demonstrate the effective regularization induced by the orthogonality constraint. We perform the controllable generation experiment using the setting of ControlNet , and the results are given in Figure 3. We can observe that our standard OFT performs quite stably and achieves accurate control after the training is finished (epoch 20). In comparison, OFT without the orthogonality constraint fails to generate any realistic image and achieve no control effect. The experiment validates the importance of the orthogonality constraint in OFT.

### General Framework

The conventional finetuning strategy typically uses gradient descent with a small learning rate to update a model (or certain layers of a model). The small learning rate implicitly encourages a small

Figure 3: Controllable generation with or without orthogonality. Middle column is from the original OFT, and the right column is given by OFT without the orthogonality constraint.

Figure 2: A toy experiment to demonstrate the importance of angular information. The autoencoder is trained in a standard way using inner product activation, and (a) shows the standard reconstruction. In testing, the angular information of neurons alone can well recover the input image, even if the autoencoder is not trained with angles.

deviation from the pretrained model, and the standard finetuning essentially aims to train the model while implicitly minimizing \(\|-^{0}\|\) where \(\) is the finetuned model weights and \(^{0}\) is the pretrained model weights. This implicit constraint makes intuitive sense, but it can still be too flexible for finetuning a large model. To address this, LoRa introduces an additional low-rank constraint for the weight update, _i.e._, \((-^{0})\!=\!r^{}\) where \(r^{}\) is set to be some small number. Different from LoRa, OFT introduces a constraint for the pair-wise neuron similarity: \(\|()-(^{0})\|\!=\!0\) where \(()\) denotes hyperspherical energy of a weight matrix. As an illustrative example, we consider a fully connected layer \(\!=\!\{_{1},,_{n}\}\!\!^{d n}\) where \(_{i}\!\!^{d}\) is the \(i\)-th neuron (\(^{0}\) is the pretrained weights). The output vector \(\!\!^{n}\) of this fully connected layer is computed by \(\!=\!^{}\) where \(\!\!^{d}\) is the input vector. OFT can be interpreted as minimizing the hyperspherical energy difference between the finetuned model and the pretrained model:

\[_{}\|()-(^{0})\|  _{}_{i j}\|}_{i}- }_{j}\|^{-1}-_{i j}\|}_{i}^{0}-}_{j}^{0}\|^{-1} \] (1)

where \(}_{i}\!:=\!_{i}/\|_{i}\|\) denotes the \(i\)-th normalized neuron, and the hyperspherical energy of a fully connected layer \(\) is defined as \(()\!:=\!_{i j}\|}_{i}-}_{j}\|^{-1}\). One can easily observe that the attainable minimum is zero for Eq. (1). The minimum can be achieved as long as \(\) and \(^{0}\) differ only up to a rotation or reflection, _i.e._, \(\!=\!^{0}\) in which \(\!\!^{d d}\) is an orthogonal matrix (The determinant \(1\) or \(-1\) means rotation or reflection, respectively). This is exactly the idea of OFT, that we only need to finetune the neural network by learning layer-shared orthogonal matrices to transform neurons in each layer. Formally, OFT seeks to optimize the orthogonal matrix \(\!\!^{d d}\) for a pretrained fully connected layer \(^{0}\!\!^{d n}\), changing the forward pass from \(\!=\!(^{0})^{}\) to

\[=^{}=(^{0})^{}, \ ^{}=^{}=\] (2)

where \(\) denotes the OFT-finetuned weight matrix and \(\) is an identity matrix. OFT is illustrated in Figure 4. Similar to the zero initialization in LoRa, we need to ensure OFT to finetune the pretrained model exactly from \(^{0}\). To achieve this, we initialize the orthogonal matrix \(\) to be an identity matrix so that the finetuned model starts with the pretrained weights. To guarantee the orthogonality of the matrix \(\), we can use differential orthogonalization strategies discussed in [26; 33]. We will discuss how to guarantee the orthogonality in a computationally efficient way.

### Efficient Orthogonal Parameterization

Standard orthogonalization such as Gram-Schmidt method, despite differentiable, is often too expensive to compute in practice . For better efficiency, we adopt Cayley parameterization to generate the orthogonal matrix. Specifically, we construct the orthogonal matrix with \(\!=\!(+)(I-)^{-1}\) where \(\) is a skew-symmetric matrix satisfying \(\!=\!-^{}\). Such an efficiency comes at a small price - the Cayley parameterization can only produce orthogonal matrices with determinant \(1\) which belongs to the special orthogonal group. Fortunately, we find that such a limitation does not affect the performance in practice. Even if we use Cayley transform to parameterize the orthogonal matrix, \(\) can still be very parameter-inefficient with a large \(d\). To address this, we propose to represent \(\) with a block-diagonal matrix with \(r\) blocks, leading to the following form:

\[=(_{1},_{2},,_{r})=_{1}()&&\\ &&&\\ &&_{r}()(d)\] (3)

where \((d)\) denotes the orthogonal group in dimension \(d\), and \(\!\!^{d d}\) and \(_{i}\!\!^{d/r d/r}, i\) are orthogonal matrices. When \(r\!=\!1\), then the block-diagonal orthogonal matrix becomes a standard unconstrained one. For an orthogonal matrix with size \(d d\), the number of parameters is \(d(d-1)/2\), resulting in a complexity of \((d^{2})\). For an \(r\)-block diagonal orthogonal matrix, the number of parameter is \(d(d/r-1)/2\), resulting in a complexity of \((d^{2}/r)\). We can optionally share the block matrix to further reduce the number of parameters, _i.e._, \(_{i}\!=\!_{j}, i\!\!j\). This reduces the parameter complexity to \((d^{2}/r^{2})\). Despite all these strategies to improve parameter efficiency, we note that the resulting matrix \(\) remains orthogonal, so there is no sacrifice in preserving hyperspherical energy.

Figure 4: (a) Original OFT without a diagonal structure. (b) OFT with \(r\) diagonal blocks of the same size. When \(r=1\), the case of (b) recovers the case of (a).

We discuss how OFT compares to LoRA in terms of parameter efficiency. For LoRA with a low-rank parameter \(r^{}\), we have its number of trainable parameters as \(r^{}(d+n)\). If we consider both \(r\) and \(r^{}\) to be dependent on the neuron dimension \(d\) (_e.g._, \(r\!=\!r^{}\!=\! d\) where \(0\!<\!\!\!1\) is some constant), then the parameter complexity of LoRA becomes \((d^{2}+dn)\) and the parameter complexity of OFT becomes \((d)\). We illustrate the difference in complexity between OFT and LoRA with a concrete example. Suppose we have a weight matrix with size \(128 128\), LoRA has \(2,048\) trainable parameters with \(r^{}\!=\!8\), while OFT has \(960\) trainable parameters with \(r\!=\!8\) (no block sharing is applied).

### Constrained Orthogonal Finetuning

We can further limit the flexibility of original OFT by constraining the finetuned model to be within a small neighborhood of the pretrained model. Specifically, COFT uses the following forward pass:

\[=^{}=(^{0})^{}, \ \ \ ^{}=^{}=,\ \ \|-\|\] (4)

which has an orthogonality constraint and an \(\)-deviation constraint to an identity matrix. The orthogonality constraint can be achieved with the Cayley parameterization introduced in Section 3.3. However, it is nontrivial to incorporate the \(\)-deviation constraint to the Cayley-parameterized orthogonal matrix. To gain more insights on the Cayley transform, we apply the Neumann series to approximate \(\!=\!(+)(I-)^{-1}\) as \(\!\!+2+(^{2})\) (under the assumption that the Neumann norm). Therefore, we can move the constraint \(\|\!-\!\|\!\!\) inside the Cayley transform, and the equivalent constraint is \(\|\!-\!\|\!\!^{}\) where \(\) denotes an all-zero matrix and \(^{}\) is another error hyperparameter (different than \(\)). The new constraint on the matrix \(\) can be easily enforced by projected gradient descent. To achieve identity initialization for the orthogonal matrix \(\), we initialize \(\) as an all-zero matrix. COFT can be viewed as a combination of two explicit constraints: minimal hyperspherical energy difference and constrained deviation from the pretrained model. The second constraint is usually implicitly used by existing finetuning methods, but COFT makes it an explicit one. Despite the excellent performance of OFT, we observe that COFT yields even better finetuning stability than OFT due to this explicit deviation constraint. Figure 5 provides an example on how \(\) affects the performance of COFT. We can observe that \(\) controls the flexibility of finetuning. With larger \(\), the COFT-finetuned model resembles the OFT-finetuned model. With smaller \(\), the COFT-finetuned model behaves increasingly similar to the pretrained text-to-image diffusion model.

### Re-scaled Orthogonal Finetuning

We propose a simple extension to the original OFT by additionally learning a magnitude scaling coefficient for each neuron. This is motivated by the fact that re-scaling neurons does not change the hyperspherical energy (the magnitude will be normalized out). Specifically, we use the forward pass: \(\!=\!(^{0})^{}\) where \(\!=\!(s_{1},,s_{d})\!\!^{d d}\) is a diagonal matrix with all the diagonal element \(s_{1},,c_{d}\) larger than zero. In contrast to OFT's original forward pass in Eq. (2) where only \(\) is learnable, we have both the diagonal matrix \(\) and the orthogonal matrix \(\) learnable. The re-scaled OFT will further improve the flexibility of OFT with a neglectable number of additional trainable parameters. Note that, we still stick to the original OFT in the experiment section in order to demonstrate the effectiveness of orthogonal transformation alone, but we actually find that the re-scaled OFT can generally improve performance (see Appendix C for empirical results).

## 4 Intriguing Insights and Discussions

**OFT is agnostic to different architectures**. We can apply OFT to any type of neural network in principle. For Transformers, LoRA is typically applied to the attention weights . To compare fairly to LoRA, we only apply OFT to finetune the attention weights in our experiments. Besides fully connected layers, OFT is also well suited for finetuning convolution layers, because the block-diagonal structure of \(\) has interesting interpretations in convolution layers (unlike LoRA). When we use the same number of blocks as the number of input channels, each block only transforms a unique neuron channel, similar to learning depth-wise convolution kernels . When all the blocks in \(\) are shared, OFT transforms the neurons with an orthogonal matrix shared across channels. We conduct a preliminary study on finetuning convolution layers with OFT in Appendix D

Figure 5: How \(\) affects the flexibility of COFT in subject-driven generation.

**Connection to LoRA**. By adding a low-rank matrix, LoRA prevents the information in the pretrained weight matrix from shifting dramatically. In contrast, OFT controls the transform that applies to the pretrained weight matrix to be orthogonal (full-rank), which prevents the transform to destroy the pretraining information. We can rewrite OFT's forward pass as \(=(^{0})^{}=(^{0}+(-)^{0})^{} \) where \((-)^{0}\) is analogous to LoRA's low-rank weight update. Since \(^{0}\) is typically full-rank, OFT also performs low-rank weight update when \(-\) is low-rank. Similar to LoRA that has a rank parameter \(r^{}\), OFT has a diagonal block parameter \(r\) to reduce the number of trainable parameters. More interestingly, LoRA and OFT represent two distinct ways to be parameter-efficient. LoRA exploits the low-rank structure to reduce the number of trainable parameters, while OFT takes a different route by exploiting the sparsity structure (_i.e._, block-diagonal orthogonality).

**Why OFT converges faster**. On one hand, we can see from Figure 2 that the most effective update to modify the semantics is to change neuron directions, which is exactly what OFT is designed for. On the other hand, OFT can be viewed as finetuning neurons on a smooth hypersphere manifold, which yields better optimization landscape. This is also empirically verified in .

**Why not minimize hyperspherical energy**. One of the key difference to  is that we do not aim to minimize hyperspherical energy. This is quite different from classification problems where neurons without redundancy are desired. The minimum hyperspherical energy means all neurons are uniformly spaced around the hypersphere. This is not a meaningful objective to finetune a pretrained generative model, since it will likely destroy the pretraining information.

**Trade-off between flexibility and regularity in finetuning**. We discover an underlying trade-off between flexibility and regularity. Standard finetuning is the most flexible method, but it yields poor stability and easily causes model collapse. Being surprisingly simple, OFT finds a good balance between flexibility and regularity by preserving the pairwise neuron angles. The block-diagonal parameterization can also be viewed as a regularization of the orthogonal matrix. Although it limits the flexibility, it brings additional regularity and stability. COFT is also another example of limiting the flexibility.

**No additional inference overhead**. Unlike ControlNet, our OFT framework introduces no additional inference overhead to the finetuned model. In the inference stage, we can simply multiply the learned orthogonal matrix \(\) into the pretrained weight matrix \(^{0}\) and obtain an equivalent weight matrix \(=^{0}\). Thus the inference speed is the same as the pretrained model.

## 5 Experiments and Results

**General settings**. In all the experiments, we use Stable Diffusion v1.5  as our pretrained text-to-image diffusion model. All the compared methods finetune the same pretrained model under the same setting. For fairness, we randomly pick generated images from each method. For subject-driven generation, we generally follow DreamBooth . For controllable generation, we generally follow ControlNet  and T2I-Adapter . To ensure a fair comparison to LoRA, we only apply OFT or COFT to the same layer where LoRA is used. More results and details are given in Appendix A.

### Subject-driven Generation

**Settings**. We use DreamBooth  and LoRA  as the baselines. All the methods adopt the same loss function as in DreamBooth. For DreamBooth and LoRA, we generally follow the original paper and use the best hyperparameter setup. More results are provided in Appendix A,E,F,J.

**Finetuning stability and convergence**. We first evaluate the finetuning stability and the convergence speed for DreamBooth, LoRA, OFT and COFT. Results are given in Figure 1 and Figure 6. We can observe that both COFT and OFT are able to finetune the diffusion model quite stably. After 400 iterations, both DreamBooth and OFT variants achieve good control, while LoRA fails to preserve the subject identity. After 2000 iterations, DreamBooth starts to generate collapsed images, and LoRA fails to generate yellow shirt (and instead generates yellow fur). In contrast, both OFT and COFT are still able to achieve stable and consistent control over the generated image. These results validate the fast yet stable convergence of our OFT framework in subject-driven genera

Figure 6: Generated images across different iterations.

tion. We note that the insensitivity to the number of finetuning iteration is quite important, since it can effectively alleviate the trouble of tuning the iteration number for different subjects. For both OFT and COFT, we can directly set a relatively large iteration number without carefully tuning it. For COFT with a proper \(\), both the learning rate and the iteration number become effortless to set.

**Quantitative comparison**. Following , we conduct a quantitative comparison to evaluate subject fidelity (DINO , CLIP-I ), text prompt fidelity (CLIP-T ) and sample diversity (LPIPS ). CLIP-I computes the average pairwise cosine similarity of CLIP embeddings between generated and real images. DINO is similar to CLIP-I, except that we use ViT S/16 DINO embeddings. CLIP-T is the average cosine similarity of CLIP embeddings between text prompt and generated images. We also evaluate average LPIPS cosine similarity between generated images of the same subject with the same text prompt. Table 1 show that both COFT and OFT outperforms DreamBooth and LoRA in the DINO and CLIP-I metrics by a considerable margin, while achieving slightly better or comparable performance in prompt fidelity and diversity metric. For each method, we repeatedly finetune the same pretrained model with 30 different random seeds to minimize randomness. The results show that our OFT framework not only achieves better convergence and stability, but also yields consistently better final performance.

**Qualitative comparison**. To have a more intuitive understanding of OFT's benefits, we show some randomly picked examples for subject-driven generation in Figure 7. For a fair comparison, all the examples are generated from the same finetuned model using each method, so no text prompt will be separately optimized for its final results. For each method, we select the model that achieves the best validation CLIP metrics. From the results in Figure 7, we can observe that both OFT and COFT deliver excellent semantic subject preservation, while LoRA often fails to preserve the subject identity (_e.g._, LoRA completely loses the subject identity in the bowl example). In the meantime, both OFT and COFT have much more accurate control using text prompts, while DreamBooth, despite its preservation of subject identity, often fails to generate the image following the text prompt (_e.g._, the first row of the bowl example). The qualitative comparison demonstrates that our OFT framework achieves better controllability and subject preservation at the same time. Moreover, the number of iterations is not sensitive in OFT, so OFT performs well even with a large number of iterations, while neither DreamBooth nor LoRA can. More qualitative examples are given in Appendix F. Moreover, we conduct a human evaluation in Appendix H which further validates the superiority of OFT.

### Controllable Generation

**Settings**. We use ControlNet , T2I-Adapter  and LoRA  as the baselines. We consider three challenging controllable generation tasks in the main paper: Canny edge to image (C2I) on the

 Method & DINO \(\) & CLIP-I \(\) & CLIP-T \(\) & LPIPS \(\) \\  Real Images & 0.703 & 0.864 & - & 0.695 \\ DreamBooth & 0.614 & 0.778 & **0.239** & 0.737 \\ LoRA & 0.613 & 0.765 & 0.237 & 0.744 \\ COFT & 0.630 & 0.783 & 0.235 & 0.744 \\ OFT & **0.632** & **0.785** & 0.237 & **0.746** \\ 

Table 1: Quantitative comparison of subject fidelity (DINO, CLIP-I), prompt fidelity (CLIP-T) and diversity metric (LIPPS). The evaluation images and prompts are the same as  (25 subjects with 30 text prompts each subject).

Figure 7: Qualitative comparison of subject-driven generation among DreamBooth, LoRA, COFT and OFT. Results are generated with the same finetuned model from each method. All examples are randomly picked. The figure is best viewed digitally, in color and significantly zoomed in.

COCO dataset , segmentation map to image (S2I) on the ADE20K dataset  and landmark to face (L2F) on the CelebA-HQ dataset [25; 63]. All the methods are used to finetune Stable Diffusion (SD) v1.5 on these three datasets for 20 epochs. More results are given in Appendix F,G,J.

**Convergence**. We evaluate the convergence speed of ControlNet, T2I-Adapter, LoRA and COFT on the L2F task. We provide both quantitative and qualitative evaluation. Specifically for the evaluation metric, we compute the mean \(_{2}\) distance between control face landmarks and predicted face landmarks. In Figure 8, we plot the face landmark error obtained by the model finetuned with different number of epochs. We can observe that both COFT and OFT achieve significantly faster convergence. It takes 20 epochs for LoRA to converge to the performance of our OFT framework at the 8-th epoch. We note that OFT and COFT use a similar number of trainable parameters to LoRA (much fewer than ControlNet), while being much more efficient to converge than existing methods. On the other hand, the fast convergence of OFT is also validated by the results in Figure 1. The right example in Figure 1 shows that OFT is much more data-efficient than ControlNet and LoRA, since OFT can converge well with only 5% of the ADE20K dataset. For qualitative results, we focus on comparing OFT, COFT and ControlNet, because ControlNet achieves the closest landmark error to ours. Results in Figure 9 show that both OFT and COFT converge stably and the generated face pose is gradually aligned with the control landmarks. In contrast to our stable and smooth convergence, the controllability in ControlNet suddenly emerges after the 8-th epoch, which perfectly matches the sudden convergence phenomenon observed in . Such a convergence stability makes our OFT framework much easier to use in practice, since the training dynamics of OFT is far more smooth and predictable. Thus it will be easier to find good OFT's hyperparameters.

**Quantitative comparison**. We introduce a control consistency metric to evaluate the performance of controllable generation. The basic idea is to compute the control signal from the generated image and then compare it with the original input control signal. For the C2I task, we compute IoU and F1 score. For the S2I task, we compute mean IoU, mean and overall accuracy. For the L2F task, we compute the mean \(_{2}\) distance between control landmarks and predicted landmarks. More details regarding the consistency metrics are given in Appendix A. For all the compared method, we use the best possible hyperparameter settings. Results in Table 2 show that both OFT and COFT yield much stronger and accurate control than the other methods. We observe that the adapter-based approaches (_e.g._, T2I-Adapter and ControlNet) converge slowly and also yield worse final results. Compared to ControlNet, LoRA performs better in the S2I task and worse in the C2I and L2F tasks. In general, we find that the performance ceiling of LoRA is relatively low, even if we have carefully tuned its hyperparameters. As a comparison, the performance of our OFT framework has not yet saturated, since we empirically find that it still gets better as the number of trainable parameters gets large. We emphasize that our quantitative evaluation in controllable generation is one of our novel contributions, since it can accurately evaluate the control performance of the finetuned models (up to the accuracy of the off-the-shelf segmentation/detection model).

**Qualitative comparison**. We also qualitatively compare OFT and COFT to current state-of-the-art methods, including ControlNet, T2I-Adapter and LoRA. Randomly generated images in Figure 10 show that OFT and COFT not only yield high-fidelity and realistic image quality, but also achieve accurate control. In the S2I task, we can see that LoRA completely fails to generate images following the input segmentation map, while ControlNet, OFT and COFT can well control the generated images. In contrast to ControlNet, both OFT and COFT are able to generate high-fidelity images with more vivid details and more reasonable

 Task & Metric & SD & ControlNet & T2I-Adapter & LoRA & COFT & OFT \\  C2I & IoU \(\) & 0.049 & 0.189 & 0.078 & 0.168 & **0.195** & 0.193 \\ F1 \(\) & 0.093 & 0.317 & 0.143 & 0.286 & **0.325** & 0.323 \\   & mIoU \(\) & 7.72 & 20.88 & 16.38 & 22.98 & **27.06** \\ S2I & mAcc \(\) & 14.40 & 30.91 & 26.31 & 35.52 & 40.08 & **40.09** \\  & aAcc \(\) & 33.61 & 61.42 & 51.63 & 58.03 & **62.96** & 62.42 \\  L2F & Error \(\) & 146.19 & 7.61 & 23.75 & 7.68 & **6.92** & 7.07 \\ 

Table 2: Quantitative comparison of control signal consistency for three control tasks (Canny edge to image, segmentation to image and landmark to face).

Figure 8: Face landmark error.

Figure 9: Qualitative examples with different number of epochs.

geometric structures with far less model parameters. In the C2I task, both OFT and COFT are able to hallucinate semantically similar images based on a rough Canny edges, while T2I-Adapter and LoRA perform much worse. In the L2F task, our method produces the most accurate pose control for the generated faces even under challenging face poses. In all three control tasks, we show that both OFT and COFT produce qualitatively better images than the state-of-the-art baselines, demonstrating the effectiveness of our OFT framework in controllable generation. To give a more comprehensive qualitative comparison, we provide more qualitative examples for all the three control tasks in Appendix F.2, and moreover, we demonstrate OFT can perform well on more control tasks (including dense pose to human body, sketch to image and depth to image) in Appendix G.

## 6 Concluding Remarks and Open Problems

Motivated by the observation that angular information among neurons crucially determines visual semantics, we propose a simple yet effective finetuning method - orthogonal finetuning for controlling text-to-image diffusion models. Specifically, we target two text-to-image applications: subject-driven generation and controllable generation. Compared to existing methods, OFT demonstrates stronger controllability and finetuning stability with fewer number of finetuning parameters. More importantly, OFT does not introduce additional inference overhead, leading to an efficient deployable model.

OFT also introduces a few interesting open problems. First, OFT guarantees the orthogonality via Cayley parametrization which involves a matrix inverse. It slightly limits the scalability of OFT. Although we address this limitation using block diagonal parametrization, how to speed up this matrix inverse in a differentiable way remains a challenge. Second, OFT has unique potential in compositionality, in the sense that the orthogonal matrices produced by multiple OFT finetuning tasks can be multiplied together and remains an orthogonal matrix. Whether this set of orthogonal matrices preserve the knowledge of all the downstream tasks remains an interesting direction to study. Finally, the parameter efficiency of OFT is largely dependent on the block diagonal structure which inevitably introduces additional biases and limits the flexibility. How to improve the parameter efficiency in a more effective and less biased way remains an important open problem.

Figure 10: Qualitative comparison of controllable generation. The figure is best viewed digitally, in color and significantly zoomed in.