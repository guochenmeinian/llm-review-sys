# First-Order Minimax Bilevel Optimization

Yifan Yang, Zhaofeng Si, Siwei Lyu and Kaiyi Ji

Department of Computer Science and Engineering

University at Buffalo

Buffalo, NY 14260

{yyang99, zhaofeng, siweilyu, kaiyiji}@buffalo.edu

These authors contributed equally to this work.

###### Abstract

Multi-block minimax bilevel optimization has been studied recently due to its great potential in multi-task learning, robust machine learning, and few-shot learning. However, due to the complex three-level optimization structure, existing algorithms often suffer from issues such as high computing costs due to the second-order model derivatives or high memory consumption in storing all blocks' parameters. In this paper, we tackle these challenges by proposing two novel fully first-order algorithms named FOSL and MemCS. FOSL features a fully single-loop structure by updating all three variables simultaneously, and MemCS is a memory-efficient double-loop algorithm with cold-start initialization. We provide a comprehensive convergence analysis for both algorithms under full and partial block participation, and show that their sample complexities match or outperform those of the same type of methods in standard bilevel optimization. We evaluate our methods in two applications: the recently proposed multi-task deep AUC maximization and a novel rank-based robust meta-learning. Our methods consistently improve over existing methods with better performance over various datasets.

## 1 Introduction

In this paper, we study a general multi-block minimax bilevel optimization problem given by

\[_{x^{d_{x}}}_{y^{d_{y}}}F(x,y, ^{*}):=_{i=1}^{n}f_{i}x,y,z_{i}^{*}(x) =_{i=1}^{n}_{}f_{i}x,y,z_{i}^{*}(x); _{i}\] \[z_{i}^{*}(x)=*{arg\,min}_{z ^{d_{z}}}g_{i}(x,z)=_{}g_{i}(x,z;_{i}) \] (1)

where the upper- and lower-level function \(f_{i}\) and \(g_{i}\) for block \(i\) take the expectation form w.r.t. the random variables \(\), \(\), and are jointly continuously differentiable, \(^{*}=z_{1}^{*}(x),...,z_{n}^{*}(x)^{d_{x } n}\) contains all lower-level optimal solutions, and \(n\) is the number of blocks. The above problem has various applications in machine learning, including deep AUC maximization , meta-learning , hyperparameter optimization , and robust learning . This paper focuses on the setting with a nonconvex-strongly-concave minimax upper-level problem and a strongly-convex lower-level problem.

To date, barring a few works on optimizing special cases of this problem , the solution algorithm to its general form has not been well studied. The primary obstacle lies in the significant computational cost per iteration, arising from the inherent structure of multi-block minimax bilevel optimization. To address this challenge,  considered a special case where \(y\) is the simplex variable, and introduced a single-loop gradient descent-ascent algorithm, based on the two-timescale bilevelframework in .  proposed a single-loop matrix-vector-based algorithm for a special case of our problem, where each upper-level function \(f_{i}\) is evaluated only at the \(i_{th}\) coordinate of \(y\). However, these methods require computing the expensive second-order derivatives (i.e., the Hessian matrix or Hessian-vector product) per iteration, and the more efficient first-order approaches have not been explored yet. In this paper, we propose two efficient first-order minimax bilevel algorithms and further apply them to two novel ML applications. Our contributions are summarized as follows.

* By converting the original minimax bilevel problem into a simple minimax problem, we first propose a fully first-order single-loop algorithm named FOSL, which is easy to implement by updating \(x,y\) and \(\) simultaneously, and is computationally efficient without the calculation of any second-order Hessian or Jacobian matrices. We provide a convergence analysis for FOSL under a practical block sampling without replacement setting and show that its sample complexity matches the best-known result of the same type of methods in standard bilevel optimization. Technically, we characterize the gap between the reformulated and original problems and need to deal with the interplay among four variables in the error analysis.
* In the settings where the number of blocks is substantial (e.g., in few-shot meta-learning), it becomes impractical to store all block-specific parameters to perform the single-loop optimization. To this end, we also propose a memory-efficient method named MemCS via a cold-start initialization, which randomly initializes a new weight for each sampled block, without saving it for the next iteration. We then analyze the convergence of MemCS under the partial-block and full-block participation, and show that it can achieve a better sample complexity than the same type of methods in standard bilevel optimization.
* We further apply our approaches to two ML applications: deep AUC maximization and robust meta-learning. The first application pertains to the established field of AUC Maximization, while the second explores a novel application known as Rank-based Robust Meta-Learning. We show the effectiveness of our methods over a variety of datasets including CIFAR100, CelebA, CheXpert, OGBG-MolPCBA, Mini-ImageNet and Tiered-ImageNet.

## 2 Related Work

(Minimax) bilevel optimization.Bilevel optimization, introduced in , has been extensively studied, with constraint-based methods [13; 20; 50; 51] and gradient-based methods [1; 45; 14; 49; 57] emerging as two predominant types of approaches. The constraint-based methods (e.g., [34; 38; 30; 35; 54]) reformulated the lower-level problem as a value-function-based constraint, and solved it via different constrained optimization algorithms. More recently, [16; 23] studied the minimax bilevel optimization problem and proposed single-loop algorithms with applications to robust machine learning and deep AUC maximization. In this paper, we propose two efficient, fully first-order algorithms with solid performance guarantees. In recent years, there has been a growing interest in gradient-based methods due to their efficiency in solving machine-learning problems. Within this category, Iterative Differentiation (ITD) based methods [6; 7; 14; 39; 49; 27] and Approximate Implicit Differentiation (AID) based methods [1; 5; 33; 45; 41; 14; 27; 22] are two important classes distinguished by their approaches to approximating hypergradients.

**Deep AUC maximization (DAM).** DAM methods are aimed to mitigate the impact of imbalanced data in binary classification by directly maximizing the _area under the ROC curve_ (AUC), a performance metric less affected by imbalanced data. As the AUC is difficult to optimize directly, research on DAM primarily focuses on devising effective optimization methods for its continuous surrogates [21; 4; 46; 8].  proposed to reformulate the deep AUC maximization problem as a minimax optimization problem, providing the foundation for stochastic DAM algorithms developed in recent years [59; 60; 18; 24]. Among them, the most relevant work  formulated the DAM problem as a multi-block minimax optimization problem. In this work, we will use this form of DAM to demonstrate the effectiveness of our algorithm. A more comprehensive overview of DAM methods can be found in the survey .

**Robust meta-learning.** Meta-learning provides effective solutions to multi-task learning in few-shot learning settings. In meta-learning, one trains a meta-model that can be quickly turned into a model that adapts to new tasks with only a few updates. Meta-learning algorithms in real-world applications must be robust to handle corrupted or low-quality data such as outliers. The majority of robust meta-learning methods encompass filtering , re-weighting [48; 28; 31; 36], and re-labeling[43; 52; 61]on the sample level. Moreover, some other works focus on improving task-level robustness [28; 58]. In this work, we show that robust meta-learning can be formulated as a minimax bilevel optimization problem and solved with the proposed algorithm.

## 3 Algorithms

### Reformulation as a Minimax Problem

Motivated by [34; 38; 30] in single-machine bilevel optimization, we reformulate the lower-level problem as a value-function-based constraint and aim to solve the following equivalent problem:

\[_{x}_{y}_{i=1}^{n}f_{i}(x,y,z_{i})\ \ g_{i}(x,z_{i})-g_{i}(x,z_{i}^{*}) 0,\] (2)

where \(z_{i}^{*}:=_{z}g_{i}(x,z)\). Inspired by , we form a Lagrangian \(_{i}\) with Lagrangian multiplier \( 0\) to approximate the original problem for each block \(i\) in eq. (2), as

\[_{i}(x,y,z_{i},v_{i})=f_{i}(x,y,z_{i})+g_{i}(x,z_{i} )-g_{i}(x,v_{i}),\]

where \(v_{i}\) is used to approximate the lower-level solution \(z_{i}^{*}(x)\) of the \(i_{th}\) block. Then, we turn to solve the following surrogate minimax problem:

\[_{x,}_{y,}(x,y,,):= _{i=1}^{n}_{i}(x,y,_{i}, _{i}),\] (3)

where \(=(z_{1},...,z_{n})^{d_{z} n}\), \(=(v_{1},...,v_{n})^{d_{z} n}\) and the standard basis vector \(_{i}\) has only one non-zero element of \(1\) at the \(i_{th}\) coordinate. We show later in Section 4.2 that the gap between the gradients \( F(x,y^{*}(x),^{*}(x))\) and \((x,y^{*}(x),^{*}_{}(x),^{*}(x))\) of the original and surrogate problems can be effectively bounded by \((1/)\), where \(y^{*}(x)\) denotes the maximize of outer-objective \(F(x,,^{*}(x))\) and each vector \(z_{,i}^{*}(x)\) in \(_{}^{*}(x):=(z_{,1}^{*}(x),...,z_{,n}^{*}(x))\) denotes the minimizer of the Lagrangian function \(_{i}(x,y^{*}(x),,v)\) (where \(z_{,i}^{*}(x)\) has not reliance on \(v\)). This validates the effectiveness of the Lagrangian approximation for \(\) sufficiently large. Next, we propose two efficient algorithms, namely FOSL and MemCS, to solve the surrogate problem in eq. (3).

### FOSL: Fully First-Order Single-Loop Method

As shown in Algorithm 1, we first sample a subset \(I_{t}:=\{1,...,n\}\) of blocks without replacement. Noting that \(z_{i}\) and \(v_{i}\) are both block-specific variables, we then apply a stochastic ascent and descent step to update \(v_{i}\) and \(z_{i}\) for all block \(i I_{t}\) as

\[v_{i,t+1} =v_{i,t}+_{v}-_{z}g_{i}(x_{t},v_{i,t};_{v,i}^ {t})\] \[z_{i,t+1} =z_{i,t}-_{z}_{z}_{i}x_{t},y_{t},z_{ i,t},v_{i,t};_{z,i}^{t},\]

where the gradient of \(_{i}\) w.r.t. \(z\) has no dependence on \(v\). Since the solutions w.r.t. variables \(x\) and \(y\) depend on all blocks, we use the average of stochastic gradient estimators from the selected blocks in \(I_{t}\) to update \(y\) and \(x\) as

\[y_{t+1} =y_{t}+_{y}|}_{i I_{t}}_{y}f_{i} (x_{t},y_{t},v_{i,t};_{y,i}^{t})\] \[x_{t+1} =x_{t}-_{x}|}_{i I_{t}}_{x} _{i}(x_{t},y_{t},z_{i,t},v_{i,t};_{x,i}^{t}).\]

Note that our algorithm takes a simpler fully single-loop structure via updating \(\{v_{i,t},z_{i,t}\}_{i I_{t}},x_{t}\) and \(y_{t}\) simultaneously at each iteration. Hence, it can also benefit from the hardware parallelism. In addition, different from the methods in [17; 24] that need to compute the higher order Hessian- or Jacobian-vector products, our method only uses the first-order gradients.

### MemCS: Memory-Efficient Cold-Start Method

Note that in the single-loop optimization of Algorithm 1, all block-specific parameters \(v_{i,t}\) and \(z_{i,t}\) of blocks in \(I_{t}\) need to be stored for the updates at iteration \(t+1\). However, in some ML applications,such as few-shot meta-learning, the number of blocks/tasks is often large, and hence Algorithm 1 can suffer from significant memory consumption. To address this challenge, we propose a memory-efficient method named MemCS in Algorithm 2. Differently from the single-loop updates in FOSL, MemCS contains a sub-loop of \(K\) steps of gradient descent1 in updating the block-specific variables \(v_{i,t}^{k}\) and \(z_{i,t}^{k}\) for \(k=0,...,K-1\) with **random initialization**\(v_{i,t}^{0}\) and \(z_{i,t}^{0}\). After obtaining the outputs \(v_{i,t}^{K},z_{i,t}^{K}\) of this sub-loop, the remaining step is to update \(y_{t}\) and \(x_{t}\) via gradient ascent and descent similarly as in FOSL.

```
1:Input: initialization \(\{x_{0},y_{0}\}\), number of iteration rounds \(T\), learning rates \(\{_{x},_{y},_{z},_{v}\}\)
2:for\(t=0,1,2,...,T\)do
3: Sample blocks \(I_{t} S\)
4:for\(i I_{t}\)do
5:\(v_{i,t+1}=v_{i,t}-_{v}_{z}g_{i}(x_{t},v_{i,t};_{v,i}^{t})\)
6:\(z_{i,t+1}=z_{i,t}-_{z}_{z}_{i}(x_{t},y_{t},z_{i,t},v_{i, t};_{z,i}^{t})\)
7:endfor
8:\(y_{t+1}=y_{t}+_{y}|}_{i I_{t}}_{y}f_{i}(x_{t}, y_{t},v_{i,t};_{y,i}^{t})\)
9:\(x_{t+1}=x_{t}-_{x}|}_{i I_{t}}_{x}_ {i}(x_{t},y_{t},z_{i,t},v_{i,t};_{x,i}^{t})\)
10:endfor ```

**Algorithm 1** Fully First-Order Single-Loop Method (FOSL)

## 4 Main Results

### Assumptions

**Definition 4.1**.: A mapping \(f\) is \(L\)-Lipschitz continuous if \(\|f(x_{1})-f(x_{2})\| L\|x_{1}-x_{2}\|\), for any \(x_{1},x_{2}\). We say \(f\) is \(L\)-smooth if \( f\) is \(L\)-Lipschitz continuous.

Since the overall objective is nonconvex w.r.t. \(x\), we aim to find an \(\)-accurate stationary point.

**Definition 4.2**.: We call \(\) as an \(\)-accurate stationary point of the objective function \((x)\) if \(\|()\|^{2}\), where \((0,1]\) and \(\) is the output of an algorithm.

We use the following assumptions in the subsequent description. Note that these assumptions are widely adopted in existing studies [24; 17].

**Assumption 4.3**.: For any \(x^{d_{x}}\), \(y^{d_{y}}\), \(z^{d_{z}}\) and \(i\{1,2,...,n\}\), \(f_{i}(x,y)\) and \(g_{i}(x,y)\) are twice continuously differentiable, \(f_{i}(x,y,z)\) is \(_{f}\)-strongly concave w.r.t. \(y\) and \(g_{i}(x,z)\) is \(_{g}\)-strongly convex w.r.t. \(z\).

The following assumption imposes the _Lipschitz continuity_ on the upper- and lower-level functions and their derivatives.

**Assumption 4.4**.: For any \(x^{d_{x}}\), \(z^{d_{x}}\) and \(i\{1,2,...,n\}\), \(f_{i}(x,y,z)\) is \(L_{f,0}\)-Lipschitz continuous w.r.t. \(x\), \(g_{i}(x,z)\) is \(L_{g,0}\)-Lipschitz continuous w.r.t. \(x\), \(f_{i}(x,y,z)\) is \(L_{f,1}\)-smooth and \(g_{i}(x,y)\) is \(L_{g,1}\)-smooth. In addition, the second-order derivatives \(^{2}f_{i}(x,y,z)\) and \(^{2}g_{i}(x,y)\) are \(L_{f,2}\)- and \(L_{g,2}\)-Lipschitz continuous.

Next, we make a _bounded variance_ assumption for the gradients in the stochastic setting.

**Assumption 4.5**.: There exist constants \(_{f}\) and \(_{g}\) such that the variances \(\| f_{i}(x,y,z)- f_{i}(x,y,z;)\|^{2}_{f}^{2}\) and \(\| g_{i}(x,z)- g_{i}(x,z;)\|^{2}_{g}^{2}\).

The following assumption on _block heterogeneity_ measures the similarities of the upper-level gradients \(_{y}f_{i}(x,z)\) for all \(i\). This has not been discussed in previous works, but it is necessary for our approach as we explore a more general outer-maximization solution \(y^{*}(x)\) for \(F\), rather than for single \(f_{i}\).

**Assumption 4.6**.: For any \(x^{d_{x}}\), \(z^{d_{z}}\), there exist constants \(_{th} 1\) and \(_{th} 0\) such that

\[_{i=1}^{n}\|_{y}f_{i}(x,y,z)\|^{2}_{th }^{2}\|_{y}F(x,y,z)\|^{2}+_{th}^{2}.\]

We have \(_{th}=1\) and \(_{th}=0\) when all \(g_{i}\)'s are identical.

### Convergence analysis

For simplicity, we fix the number of involved blocks \(|I_{t}|=P\) for all \(t\). Let \(y^{*}(x)\) be the maximizer of \(F\) function w.r.t. \(y\). Then, the overall objective of the original problem in eq. (1) w.r.t. \(x\) is given by

\[(x):=Fx,y^{*}(x),^{*}(x),\]

where \(^{*}(x)\) is the lower-level minimizer and \(y^{*}(x)\) is the maximizer of \(F(x,,^{*}(x))\). In addition, recall that the objective function of the surrogate problem in eq. (3) w.r.t. \(x\) is given by \(^{*}(x):=x,y^{*}(x),^{*}_{}(x), ^{*}(x)\). We next characterize the difference between the gradients of the original and the surrogate problems.

**Proposition 4.7**.: _Under Assumptions 4.3, 4.4, the gap between \((x)\) and \(^{*}(x)\) can be bounded as_

\[(x)-^{*}(x)=(1/).\]

Due to the limit of space, the proof of Proposition 4.7 can be found in Lemma D.5 in the appendix. For a properly large \(\), Proposition 4.7 guarantees that the stationary points of the original and surrogate problems are close to each other. However, too large \(\) can explode the gradient estimation variance, resulting in a much slower convergence rate. This trade-off makes the selection of \(\) important, as shown in our theorems later.

We next give an upper bound on the gradient norm \(\|^{*}(x_{t})\|^{2}\) of the surrogate problem. Denote \(h_{x}^{t}:=_{x}_{i}(x_{t},y_{t},z^{t},v^{t};_{z,i}^{t})\) and its expectation as \(_{x}^{t}\).

**Proposition 4.8**.: _Under Assumptions 4.3, 4.4, 4.5, the consecutive iterates of Algorithm 1 satisfy:_

\[\|^{*}(x_{t})\|^{2} }^{*}(x_{t+1})- ^{*}(x_{t})-\|_{x}^{t}\|^{2}+_{x }L_{*,1}\|h_{x}^{t}\|^{2}+3L_{f,1}^{2}y_{t}-y^{*}(x _{t})^{2}\] \[+^{2}}{n}_{i=1}^{n} z_{i,t}-z_{,i}^{*}(x_{t})^{2}+v_{i,t}-z_{i}^{*}(x_{t}) ^{2}\]

_for all \(t\{0,1,...,T-1\}\), where \(L_{,1}\) and \(L_{*,1}\) are given in Lemma D.1 and Lemma D.6 in the appendix respectively._

The proof of Proposition 4.8 can be found in Lemma E.1 in the appendix. The same result can be obtained for Algorithm 2 by replacing \(v_{i,t}\) and \(z_{i,t}\) with \(v_{i,t}^{K}\) and \(z_{i,t}^{K}\). This proposition shows that the convergence rate of our algorithm relies on how fast the iterates \(y_{t},z_{i,t}\) and \(v_{i,t}\) converge to their optimal solutions at each iteration \(t\). We next characterize the distance of \(y_{t}\) to its optimal solution.

**Proposition 4.9**.: _Under Assumptions 4.3, 4.4, 4.5, the iterates of \(y_{t}\) generated according to Algorithm 1 satisfy_

\[\|y_{t+1}- y^{*}(x_{t+1})\|^{2}-\|y_{t}-y^{*}(x_{t})\|^{2}- (_{y})\|y_{t}-y^{*}(x_{t})\|^{2}+ ^{2}}{|I_{t}|}(_{f}^{2}+_{th}^{2})\] \[+(_{y})_{i=1}^{n}\|v _{i,t}-z_{i}^{*}(x_{t})\|^{2}+^{2}}{_{y}} _{x}^{t}^{2}+(_ {x}^{2})h_{x}^{t}^{2},\]

_for all \(t\{0,...,T-1\}\)._

The proof of Proposition 4.9 refers to Lemma E.3 in the appendix. It can be seen that with properly small stepsizes \(_{x}\) and \(_{y}\), there exists a descent of the optimal distance \(\|y_{t}-y^{*}(x_{t})\|^{2}\), which is critical for the final convergence analysis. Similar results are obtained for \(v_{i,t}\) and \(z_{i,t}\). Combining the above propositions and the auxiliary lemmas in the appendix, we get the following result for Algorithm 1.

**Theorem 4.10** (Convergence of FOSL).: _Suppose Assumptions 4.3, 4.4, 4.5, 4.6 are satisfied. Set parameters \(_{x}=(T^{-})\), \(_{y}=(T^{-})\), \(_{z}=(T^{-})\), \(_{v}=(T^{-})\) and \(=(T^{})\). Then, we have_

\[_{t=0}^{T-1}\|(x_{t})\|^{2} }{^{2}}+_{0}-_{T} }{T_{x}}+^{2}}{P}1+}{_{y} }+^{2}}{(_{z})}+^{2}}{ _{v}}C_{2}\] \[+4_{y}+(_{z})^{2}+_{v}^{2 }C_{3}\] \[ (T^{-}),\]

_where \(C_{gap}\) is defined in Lemma D.5, \(C_{2},C_{3}\) are defined in eq. (39) in the appendix, and \(_{t}:=^{*}(x_{t})+K_{y}\|y_{t}-y^{*}(x_{t})\|^{2}+K_{z }_{i=1}^{n}\|z_{i,t}-z_{,i}^{*}(x_{t})\|^{2}+K _{v}_{i=1}^{n}\|v_{i,t}-z_{i}^{*}(x_{t})\|^{2}\)._

Next, we characterize the sample complexity for FOSL.

**Corollary 4.11**.: _Under the same setting of Theorem 4.10, our algorithm finds an \(\)-accurate stationary solution after \(T=(^{-})\) interactions. The total sample complexity for all involved blocks is \(PT=(P^{-})\)._

Compared with existing works , our algorithm is free from second-order derivative computations. In addition, the sample complexity of our algorithm matches the best result  of the same type of methods in standard single-block bilevel optimization.

Next, we analyze the convergence for Algorithm 2 under the partial- and full-block participation.

**Theorem 4.12** (Convergence of MemCS).: _Suppose Assumptions 4.3, 4.4, 4.5, 4.6 are satisfied. Assume there exists some \(B>0\) such that \(\|z_{i}^{*}(x_{t})\| B\) for any \(x_{t}\), \(i=1,...,N\). For the partial-block participation, by setting parameters \(_{x}=(P^{}T^{-})\), \(_{y}=(P^{-}T^{-})\), \(_{z}=(P^{-}T^{-})\), \(_{v}=(1)\), \(=(P^{}T^{})\) and taking \(_{sub}=(P^{-}T^{-})\), we have_

\[_{t=0}^{T-1}\|(x_{t})\|^{2} }{^{2}}+-_{T})}{T_{x }}+^{2}}{P}1+}{_{y}}C_{ 2}+}{P}^{2}_{th}^{2}}{_{f}}\] \[+43L_{,1}^{2}+^{4}}{_{f}^{2}} _{sub} (P^{-}T^{-}),\]

_where \(L_{,1}:=3 L_{g,1}\), \(C_{gap}\) is defined by Lemma D.5 in the appendix and \(_{t}:=^{*}(x_{t})+K_{y}\|y_{t}-y^{*}(x_{t})\|^{2}\). For the full-block participation, by setting \(_{x}=(1)\), \(_{y}=(1)\), \(_{z}=(T^{-})\), \(_{v}=(1)\), \(=(T^{})\) and taking \(_{sub}=(T^{-2})\), we have_

\[_{t=0}^{T-1}\|(x_{t})\|^{2} (P^{-}^{-3})\) _outer iterations and_ \(K=()\) _inner iterations. The total sample complexity for all involved blocks is_ \(PKT=}(P^{}^{-3})\)_._
* _For full-block participation, our algorithm finds an_ \(\)_-accurate stationary solution of_ \((x)\) _after_ \(T=(^{-1})\) _outer iterations and_ \(K=()\) _inner iterations. The total sample complexity for all involved blocks is_ \(nKT=}(n^{-1})\)_._

Note that the per-block sample complexity of our MemCS algorithm takes an order of \(^{-1}\), which improves that of the same-type F\({}^{2}\)SA  by an order of \(^{-0.5}\), based on a refined analysis on the smoothness of the overall objective function. Corollary 4.13 also shows that MemCS achieves a linear convergence speedup w.r.t. the number \(P\) of blocks. As far as we know, this is the first linear speedup result in multi-block minimax bilevel optimization.

## 5 Applications and Experiments

In this section, we conduct extensive experiments in two applications: deep AUC maximization and rank-based robust meta-learning. More experimental results such as time and space comparison are provided in Appendix B.

### Deep AUC Maximization

#### 5.1.1 Formulation

Deep AUC Maximization (DAM) addresses machine learning challenges presented by imbalanced datasets. In particular, the AUC (Area Under the ROC Curve) measures the likelihood that a positive sample's prediction score will be higher than that of a negative sample. As outlined by , the DAM issue is structured as a multi-block minimax bilevel optimization problem:

\[_{,a,b}_{}_{j=1}^{m}_{j}_{j}^ {*}(_{j}),a_{j},b_{j},_{j} s.t.\ _{j}^{*}(_{j})=*{arg\,min}_{_{j}}g_ {j}(_{j},_{j}),\]

where \(g_{j}(_{j},_{j}):=_{j}- _{j}- L_{AVG}(_{j})^{2}\), \(L_{AVG}(_{j}):=_{i=1}^{n}(_{j};x_{i},y_ {i})\), \(\) denotes the task loss (e.g., the cross-entropy loss in binary classification tasks), and \(_{j}\) denotes the sample-level AUC loss function. The detailed formulation can be found in Appendix A.1. With the method in Section 3, we reformulate this problem as a single-level minimax optimization problem:

\[_{,,a,b}_{,}(,,a,b,,),\]

where \((,,a,b,,):=_{j=1}^{m}_ {j}(_{j},a_{j},b_{j},_{j})+g_{j}(_{j},_{j})-g_{j}(_{j},_{j})\) is the Lagrangian function of AUC loss function, and \(_{j}\) is the approximate optimal solution of \(g_{j}\).

#### 5.1.2 Results

**Settings.** Following the work , we assess our methodology using four datasets, namely _CIFAR100_, _CelebA_, _CheXpert_ and _OGBG-MoIPCBA_, whose details are provided in Appendix B.1. We evaluate the effectiveness of our algorithm by comparing it with direct optimization on multi-block minimax AUC loss (mAUC) and compositional training on mAUC loss (mAUC-CT). The test AUC scores of mAUC and mAUC-CT for different datasets in Table 1 are derived from the original paper. Detailed configuration of experiments can be found in Appendix B.2.

**Results.** The results of deep AUC maximization on different datasets are shown in Table 1. The results indicate that our FOSL outperforms the mAUC method in terms of test AUC scores on all datasets and achieves comparative or better performance than mAUC-CT on various datasets. We proceed to visualize the AUC loss during the initial stages of training for all methods on CelebA, as depicted in Figure 0(a) and 0(b). The figures illustrate that, in the initial stage, our method and mAUC-CT  exhibit a faster loss drop than mAUC, and our method achieves the fastest overall convergence rate. Furthermore, our approach exhibits a smaller fluctuation compared to other baseline methods.

**Impact of \(\).** To evaluate the impact of the hyper-parameter \(\) on training with FOSL algorithm, we conduct an ablation study on the CIFAR100 and the CelebA datasets. The test AUC scores along with training are depicted in Figure 0(c) and 0(d). As shown in Figure 0(c), our method sustains robust performance across a wide range of choices for \(\). For example, training within a \(\) range of  shows that the speed of convergence and the final test performance are not sensitive to the change of \(\). A similar observation also holds for the CelebA dataset as shown in Figure 0(d).

### Robust Meta-learning with Rank-based Loss

#### 5.2.1 Formulation

Our objective is to devise a robust meta-learning approach wherein, during each iteration, tasks with large loss values are filtered out, and the meta-model is updated with the remaining tasks. This approach effectively reduces the impact of tasks with noisy samples (noisy tasks), because deep learning models tend to acquire clean and simple patterns in their initial training stages , such that noisy samples/tasks often have large loss values. Further justification can be found in Figure 2.

We first define \(g_{[i]}\) as the \(i_{th}\) largest element of the set \(=\{g_{1},g_{2},...,g_{n}\}\), such that \(g_{[n]} g_{[n-1]}... g_{}\). Denote the task-specific loss as \(g_{i}(,w_{i})\), where \(\) is the parameter of the meta-model and \(w_{i}\) is the task-specific parameter. The proposed formulation is then given by:

\[F(,^{*}):=_{i=n-k+1}^{n}g_{ [i]},w_{[i]}^{*}()\;\;w_{i}^{*}() =}{}\,g_{i}(,w_{i}),\]

where \(g_{[i]},w_{[i]}^{*}()\) is the \(i_{th}\) largest task-specific loss given \(^{*}:=w_{i}^{*}(),...,w_{n}^{*}()^{T}\), and \(w_{[i]}^{*}()\) is the corresponding optimal task-specific parameter.

With the Lemma 1 in , by introducing an auxiliary variable \(\), we can reformulate the problem as:

\[\,\,F(,^{*},)=_{i=1}^{n}f_{i},w_{i}^{*}(), =_{i=1}^{n}g_{i}^{*}()-[g_{i}^{*}()- ]_{+}-}\] \[\;\;w_{i}^{*}()=}{}\,g_{ i}(,w_{i}).\]

Details about the derivation of the above formulation can be found in Appendix A.2. This formulation poses a non-convex optimization challenge for the primal variable \(\), making it challenging to address using conventional optimization methods. Nevertheless, our proposed algorithm enables efficient resolution of this problem by reformulating the problem into a single-level minimax optimization problem as: \(_{,}_{,}(,, ,)\), where \((,,,):=_{i=1}^{n}f_{i}( ,w_{i},)+g_{i}(,w_{i})-g_{i}(,v_{i})\) is the Lagrangian function of the rank based loss function, **v** is an approximate optimal task-specific parameter of the lower-level problem.

    & CIFAR100 & CelebA & CheXpert & OGBG-MolPCBA \\  mAUC & 0.9044 (0.0015) & 0.9062 (0.0042) & 0.8084 (0.1455) & 0.7793 (0.0028) \\ mAUC-CT & 0.9272 (0.0014) & 0.9192 (0.0004) & **0.8198 (0.1495)** & 0.8406 (0.0044) \\ FOSL(ours) & **0.9540 (0.0009)** & **0.9267 (0.0018)** & 0.8166 (0.0051) & **0.8516 (0.0014)** \\   

Table 1: Test AUC score with 95% confidence interval on different datasets for AUC maximization.

Figure 1: Visualization results of FOSL experiments. (a) Training AUC loss over **iteration rounds** during the initial stages of training. (b) Training AUC loss over **time** during the initial training phase. (c) Impact of \(\) on test AUC score throughout training on the CIFAR100 dataset. (d) Impact of \(\) on test AUC score throughout training on the CelebA dataset.

#### 5.2.2 Results

**Settings.** We perform meta-learning experiments on few-shot learning tasks, focusing on the capability of rapid adaptation to new tasks with limited samples. Adhering to standard few-shot learning configurations, we carry out _5-ways 5-shot_ learning experiments on Mini-ImageNet  (referred to as _Mini_ in Table 2) and Tiered-ImageNet  (referred to as _Tiered_ in Table 2), where each task involves a 5-class classification task, with five samples per class used as training data. Since our robust meta-learning formulation is built on that of MAML , we compare our method with MAML on both clean dataset and noisy dataset to evaluate the effectiveness and robustness of our algorithm. In the noisy setting, we adopt a standard noisy training scheme in meta-learning, where the labels in a noisy task are randomly flipped. Specifically, we employ two label flipping strategies: _Flip_, where in each iteration, a certain portion of tasks (60% in Table 2) are designated as noisy, and each sample within the task is assigned to one of all labels with equal probability; and _Rand_, where a random noisy ratio is assigned to each task in every iteration, determining the proportion of samples to be flipped by randomly assigning another label to them. Detailed configuration of experiments can be found in Appendix B.2.

**Results.** Table 2 displays the test accuracies. These results show that in the presence of noisy tasks, both MAML and our MemCS method undergo a decline in performance across both datasets, yet our approach manages to sustain a reasonable performance. To further evaluate the resilience of our MemCS method against MAML, we executed additional experiments with escalating noise levels in the _Flip_ scenario, with these findings detailed in Table 3. The data clearly show a performance decrease for both methodologies as the noise ratio intensifies. Nonetheless, our approach exhibits a notably more gradual decline in performance as the noise ratio escalates, especially at higher noise levels, signifying superior robustness compared to MAML.

**Discussion.** To show the effectiveness of our approach in facilitating robust learning, we have visualized the average losses for both clean and noisy tasks separately within the MAML training framework, as demonstrated in Figure 2. The graphical representation uncovers a consistent pattern where the losses associated with noisy tasks consistently exceed those related to clean tasks throughout the training period. This pattern underscores our approach's capacity to lessen the detrimental effects of noisy tasks. Further, we examine the noisy tasks in the update mechanism at five distinct intervals during the training phase, illustrated in Figure 2. The findings show that our methodology successfully deters the influence of noisy tasks on the meta-model's updates across both Rand and Flip scenarios, maintaining this protective measure throughout the training duration.

## 6 Conclusion

In this paper, we propose two fully first-order algorithms designed to address the challenges posed by multi-block minimax bilevel optimization problems: a fully single-loop algorithm, FOSL, and a

   Dataset & Method & Clean & Flip & Rand \\   & MAML & 64.75 & 52.75 & 52.50 \\  & MemCS(ours) & **69.25** & **57.50** & **60.25** \\   & MAML & 66.25 & 44.75 & 54.25 \\  & MemCS(ours) & **67.25** & **57.00** & **59.75** \\   

Table 2: Test accuracy (%) on the Mini-ImageNet and the Tiered-ImageNet datasets for meta-learning.

    &  &  \\   & & 0 & 0.2 & 0.4 & 0.6 & 0.8 \\   & MAML & 64.75 & 59.50 & 56.50 & 52.75 & 42.00 \\  & MemCS & **69.25** & **65.00** & **61.75** & **57.50** & **53.50** \\   & MAML & 66.25 & 63.75 & 53.25 & 44.75 & 39.00 \\  & MemCS & **67.25** & **66.50** & **62.75** & **57.00** & **54.50** \\   

Table 3: Test accuracy (%) on Mini-ImageNet and Tiered-ImageNet with different noisy ratio for _Flip_ setting.

Figure 2: The portion of tasks being noisy during training for MAML and MemCS on Mini-ImageNet.

memory-efficient double-loop algorithm with cold-start initialization, MemCS. We show that our methods can achieve comparative and even better per-block sample complexities than other methods with the same type in standard bilevel optimization. The experimental results indicate that our methods consistently demonstrate superior performance and robustness in applications on deep AUC maximization and robust meta-learning.