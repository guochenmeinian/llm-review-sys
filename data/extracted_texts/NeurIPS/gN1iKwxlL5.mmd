# Dual Lagrangian Learning for Conic Optimization

Mathieu Tanneau, Pascal Van Hentenryck

H. Milton Steward School of Industrial and Systems Engineering

NSF AI Institute for Advances in Optimization

Georgia Institute of Technology

{mathieu.tanneau,pascal.vanhentenryck}@isye.gatech.edu

###### Abstract

This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology for dual conic optimization proxies. DLL leverages conic duality and the representation power of ML models to provide high-duality, dual-feasible solutions, and therefore valid Lagrangian dual bounds, for linear and nonlinear conic optimization problems. The paper introduces a systematic dual completion procedure, differentiable conic projection layers, and a self-supervised learning framework based on Lagrangian duality. It also provides closed-form dual completion formulae for broad classes of conic problems, which eliminate the need for costly implicit layers. The effectiveness of DLL is demonstrated on linear and nonlinear conic optimization problems. The proposed methodology significantly outperforms a state-of-the-art learning-based method, and achieves 1000x speedups over commercial interior-point solvers with optimality gaps under 0.5% on average.

## 1 Introduction

From power systems and manufacturing to supply chain management, logistics and healthcare, optimization technology underlies most aspects of the economy and society. Over recent years, the substantial achievements of Machine Learning (ML) have spurred significant interest in combining the two methodologies. This integration has led to the development of new optimization algorithms (and the revival of old ones) taylored to ML problems, as well as new ML techniques for improving the resolution of hard optimization problems . This paper focuses on the latter (ML for optimization), specifically, the development of so-called _optimization proxies_, i.e., ML models that provide approximate solutions to parametric optimization problems, see e.g., .

In that context, considerable progress has been made in learning _primal_ solutions for a broad range of problems, from linear to discrete and nonlinear, non-convex optimization problems. State-of-the-art methods can now predict high-quality, feasible or close-to-feasible solutions for various applications . This paper complements these methods by learning _dual_ solutions which, in turn, certify the (sub)optimality of learned primal solutions. Despite the fundamental role of duality in optimization, there is no dedicated framework for dual optimization proxies, which have seldom received any attention in the literature. The paper addresses this gap by proposing, for the first time, a principled learning methodology that combines conic duality theory with Machine Learning. As a result, it becomes possible, for a large class of optimization problems, to design a primal proxy to deliver a high-quality primal solution and an associated dual proxy to obtain a quality certificate.

### Contributions and outline

The core contribution of the paper is the Dual Lagrangian Learning (DLL) methodology for learning dual-feasible solutions for parametric conic optimization problems. DLL leverages conic duality to design a self-supervised Lagrangian loss for training dual conic optimization proxies. In addition, thepaper proposes a general dual conic completion using differential conic projections and implicit layers to guarantee dual feasibility, which yields stronger guarantees than existing methods for constrained optimization learning. Furthermore, it presents closed-form analytical solutions for conic projections, and for dual conic completion across broad classes of problems. This eliminates the need for implicit layers in practice. Finally, numerical results on linear and nonlinear conic problems demonstrate the effectiveness of DLL, which a outperforms state-of-the-art learning baseline, and yields significant speedups over interior-point solvers.

The rest of the paper is organized as follows. Section 2 presents the relevant literature. Section 3 introduces notations and background material. Section 4 presents the DLL methodology, which comprises the Lagrangian loss, dual completion strategy, and conic projections. Section 5 reports numerical results. Section 6 discusses possible the limitations of DLL and possible extensions, and Section 7 concludes the paper.

## 2 Related works

Constrained Optimization LearningThe vast majority of existing works on optimization proxies focuses on learning _primal_ solutions and, especially, on ensuring their feasibility. This includes, for instance, physics-informed training loss [3; 4; 5], mimicking the steps of an optimization algorithm [6; 7; 8], using masking operations [9; 10], or designing custom projections and feasibility layers [11; 12]. The reader is referred to  for an extensive survey of constrained optimization learning.

Only a handful of methods offer feasibility guarantees, and only for convex constraints; this latter point is to be expected since satisfying non-convex constraints is NP-hard in general. Implicit layers  have a high computational cost, and are therefore impractical unless closed-form solutions are available. DC3  uses equality completion and inequality correction, and is only guaranteed to converge for convex constraints and given enough correction steps. LOOP-LC  uses a gauge mapping to ensure feasibility for bounded polyhedral domains. RAYEN  and the similar work in  use a line search-based projection mechanism to handle convex constraints. All above methods employ equality completion, and the latter three [14; 12; 15] assume knowledge of a strictly feasible point, which is not always available.

Dual Optimization LearningTo the authors' knowledge, dual predictions have received very little attention, with most works using them to warm-start an optimization algorithm. In , a primal-dual prediction is used to warm-start an ADMM algorithm, while These works consider specific applications, and do not provide dual feasibility guarantees. More recently, [6; 8] attempt to mimic the (dual) steps of an augmented Lagrangian method, however with the goal of obtaining high-quality primal solutions.

In the mixed-integer programming (MIP) setting,  and  use a dual prediction as warm-start in a column-generation algorithm, for cutting-stock and unit-commitment problems, respectively. In a similar fashion,  consider a (combinatorial) Lagrangian relaxation of Traveling Salesperson Problem (TSP) Most recently,  consider learning Lagrangian multipliers for mixed-integer linear programs. Therein, a machine learning model predicts Lagrange multipliers, and a Lagrangian subproblem is solved to obtained a Lagrangian dual bound. This approach only supports linear constraints for the Lagrangian, and it requires an external combinatorial solver to solve the subproblem, which may be NP-hard in general.

The first work to explicitly consider dual proxies in the context of conic optimization, and to offer dual feasibility guarantees, is , which learns a dual proxy for a second-order cone relaxation of the AC Optimal Power Flow. Klamkin et al.  later introduce a dual interior-point learning algorithm to speed-up the training of dual proxies for bounded linear programming problems. In contrast, this paper proposes a general methodology for conic optimization problems, thus generalizing the approach in , and provides more extensive theoretical results. The dual completion procedure used in [22, Lemma 1] is a special case of the one proposed in this paper.

Background

This section introduces relevant notations and standard results on conic optimization and duality, which lay the basis for the proposed learning methodology. The reader is referred to  for a thorough overview of conic optimization.

### Notations

Unless specified otherwise, the Euclidean norm of a vector \(x^{n}\) is denoted by \(\|x\|=x}\). The positive and negative part of \(x\) are denoted by \(x^{+}=(0,x)\) and \(x^{-}=(0,-x)\). The identity matrix of order \(n\) is denoted by \(I_{n}\), and \(\) denotes the vector of all ones. The smallest eigenvalue of a real symmetric matrix \(X\) is \(_{}(X)\).

Given a set \(^{n}\), the _interior_ and _closure_ of \(\) are denoted by \(\) and by \(\), respectively. The Euclidean projection onto convex set \(\) is denoted by \(_{}\), where

\[_{}()=_{x}\|x-\|^{2}\;.\] (1)

The set \(\!\!^{n}\) is a _cone_ if \(x, 0 x\). The _dual cone_ of \(\) is

\[^{*}=\{y^{n}:y^{}x 0, x\},\] (2)

whose negative \(^{}=-^{*}\) is the _polar_ cone of \(\). A cone \(\) is self-dual if \(\!=\!^{*}\), and it is _pointed_ if \((-)\!=\!\{0\}\). All cones considered in the paper are _proper_ cones, i.e., closed, convex, pointed cones with non-empty interior. A proper cone \(\) defines _conic inequalities_\(_{}\) and \(_{}\) as

\[(x,y)^{n}\!\!^{n},\,x _{}y \,x-y,\] (3a) \[(x,y)^{n}\!\!^{n},\,x_{ }y \,x-y.\] (3b)

### Conic optimization

Consider a (convex) conic optimization problem of the form

\[_{x}\{c^{}x\;|\;Ax_{}b\},\] (4)

where \(A\!\!^{m n},b\!\!^{m},c\!\!^{n}\), and \(\) is a proper cone. All convex optimization problems can be formulated in conic form. A desirable property of conic formulations is that is enables the use of principled conic duality theory . Namely, the conic dual problem reads

\[_{y}\{b^{}y\;|\;A^{}y=c,y^{*}\}.\] (5)

The dual problem (5) is a conic problem, and the dual of (5) is (4). Weak conic duality always holds, i.e., any dual-feasible solution provides a valid lower bound on the optimal value of (4), and vice-versa. When strong conic duality holds, e.g., under Slater's condition, both primal/dual problems have the same optimal value and a primal-dual optimal solution exists .

Conic optimization encompasses broad classes of problems such linear and semi-definite programming. Most real-life convex optimization problems can be represented in conic form using only a small number of cones , which are supported by off-the-shelf solvers such as Mosek, ECOS, or SCS. These so-called "standard" cones comprise the non-negative orthant \(_{+}\), the second-order cone \(\) and rotated second-order cone \(_{r}\), the positive semi-definite cone \(_{+}\), the power cone \(\) and the exponential cone \(\); see Appendix B for algebraic definitions.

## 4 Dual Lagrangian Learning (DLL)

This section presents the _Dual Lagrangian Learning_ (DLL) methodology, illustrated in Figure 1, for learning dual solutions for conic optimization problems. DLL combines the representation power of artificial neural networks (or, more generally, any differentiable program), with conic duality theory, thus providing valid Lagrangian dual bounds for general conic optimization problems. _To the best of the authors' knowledge, this paper is the first to propose a principled self-supervised framework with dual guarantees for general conic optimization problems._DLL exploits three fundamental building blocks: (1) a dual conic completion procedure that provides dual-feasible solutions and, hence, valid Lagrangian dual bounds; (2) fast and differentiable conic projection layers; and (3) a self-supervised learning algorithm that emulates the steps of a dual Lagrangian ascent algorithm.

### Dual Conic Completion

Consider a conic optimization problem in primal-dual form

\[_{x} c^{}x\] (6a) s.t. \[Ax_{}b,\] (6b) \[Hx_{}h,\] (6c)

\[_{y,z} b^{}y+h^{}z\] (7a) \[_{y,z} a^{}y+H^{}z=c,\] (7b) \[s.t. A^{}y+H^{}z=c,\] (7c) \[y^{*},z^{*}.\] (7d)

where \(y^{*}\) and \(z^{*}\) are the dual variables associated to constraints (6b) and (6c), respectively. The proposed dual conic completion, outlined in Theorems 1 and 2 below, takes as input \(^{*}\), and recovers \(^{*}\) such that \((,)\) is feasible for (7). The initial assumption that \(^{*}\) can be enforced through a projection step, which will be described in Section 4.2.

**Theorem 1** (Dual conic completion).: _Assume that \(^{*}, x:Hx_{}h\) and the problem_

\[_{x} \{c^{}x+(b-Ax)^{}\;\;Hx_{ }h\}\] (8)

_is bounded. Then, \(^{*},^{*}:A^{} +H^{}=c\), i.e., \((,)\) is feasible for (7)._

**Theorem 2** (Optimal dual completion).: _Let \(^{*}\), and let \(\) be dual-optimal for (8). Then, \((,)=b^{}+h^{}\) is a valid dual bound on the optimal value of (6), and \((,)\) is the strongest dual bound that can be obtained after fixing \(y=\) in (7)._

It is important to note the theoretical differences between the proposed dual completion, and applying a generic method, e.g., DC3 , LOOP-LC  or RAYEN , to the dual problem (7). First, LOOP-LC is not applicable here, because it only handles linear constraints and requires a compact feasible set, which is not the case in general for (7). Second, unlike RAYEN, Theorem 1 does not require an explicit characterization of the affine hull of the (dual) feasible set, nor does it assume knowledge of a strictly feasible point. In fact, Theorem 1 applies even if the feasible set of (7) has an empty interior. Third, the proposed dual completion enforces both linear equality constraints (7b) and conic constraints (7c). In contrast, the equality completion schemes used in DC3 and RAYEN enforce equality constraints but need an additional mechanism to handle inequality constraints. Fourth, the optimal completion outlined in Theorem 2 provides guarantees on the strength of the Lagrangian dual bound \((,)\). This is a major difference with DC3 and RAYEN, whose correction mechanism does not provide any guarantee of solution quality. _Overall, the fundamental difference between generic methods and the proposed optimal dual completion, is that the former only exploit dual feasibility constraints_ (7b)-(7c)_, whereas DLL also exploits (dual) optimality conditions, thus providing additional guarantees._

Another desirable property of the proposed dual completion procedure, is that it does not require the user to formulate the dual problem (7) explicitly, as would be the case for DC3 or RAYEN. Instead,

Figure 1: Illustration of the proposed DLL scheme. Given input data \((A,b,H,h,c)\), a neural network first predicts \(^{n}\). Next, a conic projection layer computes a conic-feasible \(^{*}\), which is then completed into a full dual-feasible solution \((,)\). The model is trained in a self-supervised fashion, by updating the weights \(\) to maximize the Lagrangian dual bound \(()\).

the user only needs to identify a set of _primal_ constraints that satisfy the conditions of Theorem 1. For instance, it suffices to identify constraints that bound the set of primal-feasible solutions. This is advantageous because practitioners typically work with primal problems rather than their dual. The optimal dual completion can then be implemented via an implicit optimization layer. Thereby, in a forward pass, \(\) is computed by solving the primal-dual pair (8)-(27) and, in a backward pass, gradient information is obtained via the implicit function theorem .

The main limitations of implicit layers are their numerical instability and their computational cost, both in the forward and backward passes. To eliminate these issues, closed-form analytical solutions are presented next for broad classes of conic optimization problems; other examples are presented in the numerical experiments of Section 5.

**Example 1** (Bounded variables).: _Consider a conic optimization problem with bounded variables_

\[_{x}\{c^{}x\;|\;Ax_{}b,l x  u\}\] (9)

_where \(l<u\) are finite lower and upper bounds on all variables \(x\). The dual problem is_

\[_{y,z^{l},z^{u}}\{b^{}y+l^{}z^{l}-u^{}z^{u}\; |\;A^{}y+z^{l}-z^{u}=c,y^{*},z^{l} 0,z^{u} 0\}\] (10)

_and the optimal dual completion is \(^{l}=|c-A^{}|^{+},^{u}=|c-A^{}|^{-}\)._

The assumption in Example 1 that all variables have finite bounds holds in most -if not all- real-life settings, where decision variables are physical quantities (e.g. budgets or production levels) that are naturally bounded. The resulting completion procedure is a generalization of that used in  for linear programming (LP) problems.

**Example 2** (Trust region).: _Consider the trust region problem _

\[_{x}\{c^{}x\;|\;Ax_{}b,\|x\| r\}\] (11)

_where \(r{}0\), \(\|\|\) is a norm, and \(\|x\|{}r(r,x)=\{(t,x)\;|\;t{} \|x\|\}\). The dual problem is_

\[_{y,z_{0},z}\{b^{}y-rz_{0}\;|\;A^{}y+z=c,y ^{*},(z_{0},z)^{*}\}\] (12)

_where \(\|\|_{*}\) is the dual norm and \(^{*}=\{(t,x)\;|\;t{}\|x\|_{*}\}\). The optimal dual completion is \(=c-A^{}\), \(_{0}=\|\|_{*}\)._

**Example 3** (Convex quadratic objective).: _Consider the convex quadratic conic problem_

\[_{x}\{}{{2}} x^{}Qx+c^{}x\; |\;Ax_{}b\},\] (13)

_where \(Q=F^{}F\) is positive definite. The problem can be formulated as the conic problem_

\[_{x}\{q+c^{}x\;|\;Ax_{}b,(1,q,Fx) _{r}^{2+n}\}\] (14)

_whose dual is_

\[_{y,z_{0},z}\{b^{}y-z_{0}\;|\;A^{}y+F^{}z=c,( 1,z_{0},z)_{r}^{2+n}\}.\] (15)

_The optimal dual completion is \(=F^{-}(c-A^{}),_{0}=}{{2}}\|\|_{2}^{2}\)._

### Conic Projections

The second building block of DLL are differentiable conic projection layers. Note that DLL only requires a valid projection onto \(^{*}\), which need not be the Euclidean projection \(_{^{*}}\). Indeed, the latter may be computationally expensive and cumbersome to differentiate. For completeness, the paper presents Euclidean and non-Euclidean projection operators, where the latter are simple to implement, computationally fast, and differentiable almost everywhere. Closed-form formulae are presented for each standard cone in Appendix B, and an overview is presented in Table 1.

#### 4.2.1 Euclidean projection

Let \(\) be a proper cone, and \(^{n}\). By Moreau's decomposition ,

\[=_{}()+_{^{}}(),\] (16)

which is a reformulation of the KKT conditions of the projection problem (1), i.e.,

\[=p-q, p, q^{*}, p^{}q=0.\] (17)

It then follows that \(_{^{*}}()=-_{^{}}(-)\), by invariance of Moreau's decomposition under orthogonal transformations. Thus, it is sufficient to know how to project onto \(\) to be able to project onto \(^{*}\) and \(^{}\). Furthermore, (16) shows that \(_{}\) is identically zero on the polar cone \(^{}\). In a machine learning context, this may cause gradient vanishing issues and slow down training.

#### 4.2.2 Radial projection

Given an interior ray \(_{}0\), the radial projection operator \(_{}^{}\) is defined as

\[_{}^{}()=+ =_{ 0}\{\,|\,+\}.\] (18)

The name stems from the fact that \(_{}^{}\) traces ray \(\) from \(\) until \(\) is reached. Unlike the Euclidean projection, it requires an interior ray, which however only needs to be determined once _per cone_. The radial projection can then be computed, in general, via a line search on \(\) or via an implicit layer. Closed-form formulae for standard cones and their duals are presented in Appendix B.

### Self-Supervised Dual Lagrangian Training

The third building block of DLL is a self-supervised learning framework for training dual conic optimization proxies. In all that follows, let \(=(A,b,H,h,c)\) denote the data of an instance (6), and assume a distribution of instances \(\). Next, let \(_{}\) be a differentiable program parametrized by \(\), e.g., an artificial neural network, which takes as input \(\) and outputs a dual-feasible solution \((,)\). Recall that dual feasibility of \((,)\) can be enforced by combining the dual conic projection presented in Section 4.2, and the optimal dual completion outlined in Theorem 2.

The proposed self-supervised dual lagrangian training is formulated as

\[_{} _{}[(,,)]\] (19a) s.t. \[(,)=_{}(),\] (19b)

where \((,,)\!=\!b^{}+h^{}\) is the Lagrangian dual bound obtained from \((,)\) by weak duality. Thereby, the training problem (19) seeks the value of \(\) that maximizes the expected Lagrangian dual bound over the distribution of instances \(\), effectively mimicking the steps of a (sub)gradient algorithm. Note that, instead of updating \((,)\) directly, the training procedure computes a (sub)gradient \(_{}(,,)\) to update \(\), and then obtains a new prediction \((,)\) through \(_{}\). Also note that formulation (19) does not required labeled data, i.e., it does not require pre-computed dual-optimal solutions. Furthermore, it applies to any architecture that guarantees dual feasibility of \((,)\), i.e., it does not assume any specific projection nor completion procedure.

## 5 Numerical experiments

This section presents numerical experiments on linear and nonlinear optimization problems; detailed problem formulations, model architectures, and other experiment settings, are reported in Appendix

   Cone & Definition & Euclidean projection & Radial projection \\  \(_{+}\) & Appendix B.1 & (35) & (35) \\ \(\) & Appendix B.2 & (38) & (39) \\ \(_{+}\) & Appendix B.3 & (41) & (42) \\ \(\) & Appendix B.4 & no closed form & (45) and (47) \\ \(\) & Appendix B.5 & no closed form & (50) \\   

Table 1: Overview of conic projections for standard conesC. The code used for experiments is available under an open-source license.1 The proposed DLL methodology is evaluated against applying DC3 to the dual problem (7) as a baseline. Thereby, linear equality constraints (7b) and conic inequality constraints (7c) are handled by DC3's equality completion and inequality correction mechanisms, respectively. The two approaches (DLL and DC3) are evaluated in terms of dual optimality gap and training/inference time. The dual optimality gap is defined as \((^{*}-(,))/^{*}\), where \(^{*}\) is the optimal value obtained from a state-of-the-art interior-point solver.

### Linear Programming Problems

#### 5.1.1 Problem formulation and dual completion

The first set of experiments considers continuous relaxations of multi-dimensional knapsack problems , which are of the form

\[_{x}\{-p^{}x\;|\;Wx b,x^{n}\}\] (20)

where \(p_{+}^{n}\), \(W_{+}^{m n}\), and \(b_{+}^{m}\). The dual problem reads

\[_{y,z^{l},z^{u}}\{b^{}y-^{}z^{u}\;|\; W^{}y+z^{l}-z^{u}=-p,y 0,z^{l} 0,z^{u} 0,\}\] (21)

where \(y^{m}\) and \(z^{l},z^{u}^{n}\). Since variables \(x\) is bounded, the closed-form completion presented in Example 1 applies. Namely, \(^{l}=|-p-W^{}|^{+}\) and \(^{u}=|-p-W^{}|^{-}\), where \(_{-}^{m}\).

#### 5.1.2 Numerical results

Table 2 reports, for each combination of \(m,n\): the average optimal value obtained by Gurobi (Opt val), as well as the average (avg), standard-deviation (std) and maximum (max) optimality gaps achieved by DC3 and DLL on the test set. First, DLL significantly outperforms DC3, with average gaps ranging from 0.07% to 1.93%, compared with 19.58%-274.78% for DC3, an improvement of about two orders of magnitude. A similar behavior is observed for maximum optimality gaps. The rest of the analysis thus focuses on DLL. Second, an interesting trend can be identified: optimality gaps tend to increase with \(m\) and decrease with \(n\). This effect may be explained by the fact that increasing \(m\) increases the output dimension of the FCNN; larger output dimensions are typically harder to predict. In addition, a larger \(n\) likely provides a smoothing effect on the dual, whose solution becomes easier to predict. The reader is referred to  for probabilistic results on properties of multi-knapsack problems.

Next, Table 3 reports computing time statistics for Gurobi, DC3 and DLL. Namely, the table reports, for each combination of \(m,n\), the time it takes to execute each method on all instances in the test set. First, DLL is 3-10x faster than DC3, which is caused by DC3's larger output dimension (\(m+n\), compared to \(m\) for DLL), and its correction steps. Furthermore, unsurprisingly, both DC3 and DLL

    & & &  &  \\  \(m\) & \(n\) & Opt val\({}^{*}\) & avg. & std & max & avg. & std & max \\ 
5 & 100 & 14811.9 & 19.58 & 1.86 & 41.42 & **0.36** & 0.20 & 1.36 \\  & 200 & 29660.4 & 20.58 & 1.41 & 49.47 & **0.18** & 0.10 & 0.84 \\  & 500 & 74267.0 & 33.70 & 1.29 & 41.54 & **0.07** & 0.04 & 0.30 \\ 
10 & 100 & 14675.8 & 41.85 & 2.51 & 69.58 & **0.68** & 0.25 & 2.15 \\  & 200 & 29450.7 & 36.88 & 2.28 & 100.90 & **0.34** & 0.13 & 0.96 \\  & 500 & 73777.5 & 100.04 & 3.38 & 104.00 & **0.14** & 0.06 & 0.46 \\ 
30 & 100 & 14441.5 & 159.49 & 5.54 & 166.31 & **1.93** & 0.37 & 3.31 \\  & 200 & 29156.1 & 255.24 & 8.42 & 259.25 & **0.96** & 0.20 & 1.83 \\  & 500 & 73314.3 & 274.78 & 7.91 & 277.40 & **0.38** & 0.09 & 0.75 \\    All gaps are in %; best values are in bold. \({}^{*}\)Mean optimal value on test set; obtained with Gurobi.

Table 2: Comparison of optimality gaps on linear programming instances.

yield substantial speedups compared to Gurobi, of about 3 orders of magnitude. Note however that Gurobi's timings could be improved given additional CPU cores, although both ML-based methods remain significantly faster using a single GPU.

### Nonlinear Production and Inventory Planning Problems

#### 5.2.1 Problem formulation and dual completion

The second set of experiments considers the nonlinear resource-constrained production and inventory planning problem . In primal-dual form, the problem reads

\[_{x,t} d^{}x+f^{}t\] (22a) s.t. \[r^{}x b,\] (22b) \[(x_{j},t_{j},)_{r}^{3},\;j{=}1,...,n\] (22c) \[(_{j},_{j},_{j})_{r}^{3},\;j{=}1,...,n\] (23e)

where \(r,d,f^{n}\) are positive vectors, and \(b>0\). Primal variables are \(x,t^{n}\), and the dual variables associated to constraints (22b) and (22c) are \(y_{-}\), and \(,,^{n}\), respectively.

Note that (22c) implies \(x,t 0\). Next, let \(y 0\) be fixed, and consider the problem

\[_{x,t} \{(d-yr)^{}x+f^{}t+by\;}\}.\] (24)

Problem (24) is immediately strictly feasible, and bounded since \((d-yr),f>0\) and \(x,t 0\). Hence, Theorems 1 and 2 apply, and there exists a dual-optimal completion to recover \(,,\). A closed-form completion is then achieved as follows. First, constraints (23b) and (23c) yield \(=d-ry\) and \(=f\). Next, note that \(\) only appears in constraint (23e) and has negative objective coefficient. Further noting that (23e) can be written as \(_{j}^{2} 2_{j}_{j}\), it follows that \(_{j}=-_{j}}\) at the optimum.

#### 5.2.2 Numerical Results

Table 4 reports optimality gap statistics for DC3 and DLL. Similar to the linear programming setting, DLL substantially outperforms DC3, with average optimality gaps ranging from 0.23% to 1.03%, compared with 70.76%-87.01% for DC3. In addition, DLL exhibits smaller standard deviation and maximum optimality gaps than DC3. These results can be explained by several factors. First, the neural network architecture used in DC3 has output size \(n+1\), compared to \(1\) for DLL; this is because DLL leverages a more efficient dual completion procedure. Second, a closer examination of DC3's output reveals that it often fails to satisfy the (conic) inequality constraints (23d) and (23e). More generally, DC3 was found to have much slower convergence than DLL during training. While the performance of DC3 may benefit from more exhaustive hypertuning, doing so comes at a significant computational and environmental cost. This further highlights the benefits of DLL, which requires minimal tuning and is efficient to train.

   \(m\) & \(n\) & Gurobi\({}^{}\) & DC3\({}^{}\) & DLL\({}^{}\) \\ 
5 & 100 & 2.8 CPU.s & 2.1 GPU.ms & 0.3 GPU.ms \\  & 200 & 4.1 CPU.s & 4.0 GPU.ms & 0.7 GPU.ms \\  & 500 & 6.6 CPU.s & 13.2 GPU.ms & 3.0 GPU.ms \\ 
10 & 100 & 3.7 CPU.s & 2.3 GPU.ms & 0.4 GPU.ms \\  & 200 & 6.1 CPU.s & 4.9 GPU.ms & 1.1 GPU.ms \\  & 500 & 11.9 CPU.s & 17.2 GPU.ms & 4.7 GPU.ms \\ 
30 & 100 & 14.0 CPU.s & 4.6 GPU.ms & 0.9 GPU.ms \\  & 200 & 21.3 CPU.s & 10.4 GPU.ms & 2.5 GPU.ms \\  & 500 & 40.0 CPU.s & 39.5 GPU.ms & 13.6 GPU.ms \\    \({}^{}\)Time to solve all instances in the test set, using one CPU core. \({}^{}\)Time to run inference on all instances in the test set, using one V100 GPU.

Table 3: Computing time statistics for linear programming instancesFinally, Table 5 reports computing time statistics for Mosek, a state-of-the-art conic interior-point solver, DC3 and DLL. Abnormally high times are observed for Mosek and \(n{=}10,20\). These are most likely caused by congestion on the computing nodes used in the experiments, and are discarded in the analysis. Again, DC3 and DLL outperform Mosek by about three orders of magnitude. Furthermore, DLL is about 10x faster than DC3 for smaller instances (\(n{}100\)), and about 2x faster for the largest instances (\(n{=}1000\)). This is caused by DC3's larger output dimension and correction steps.

## 6 Discussion

### Mixed-Integer Nonlinear Programming Setting

The proposed Dual Lagrangian Learning framework directly extends to the mixed-integer nonlinear programming (MINLP) setting. Consider a general MINLP problem of the form

\[_{x}\{f(x) h(x)=0,g(x) 0\},\] (25)

where \(^{n}\) denotes a possibly discrete domain. Given Lagrange multipliers \(^{m}\) and \(^{p}_{+}\) associated to equality and inequality constraints, the corresponding Lagrangian dual bound is

\[(,)=_{x} f(x)-^{}h(x)- ^{}g(x).\] (26)

Note that (26) is the MINLP counterpart of (8) in the conic setting.

The dual Lagrangian function \((,)\) is concave, non-smooth, and admits sub-differentials of the form \(_{}=-h()\), \(_{}=-g()\), where \(\) is an _optimal_ solution of (26). The self-supervised learning framework of Section 4.3 can then be applied out of the box, wherein an ML model predicting \((,)\) is trained in a self-supervised fashion by maximizing the dual bound \((,)\). This approach is followed in, e.g.,  for mixed-integer linear problems, and [6; 8] for nonlinear problems.

Despite natural similarities between the (convex) conic and MINLP settings, several intrinsic limitations appear in the latter. First, although the domain of \((,)^{m}^{p}_{+}\) is simple, and can be enforced via, e.g., ReLU activations, evaluating \((,)\) is not. Indeed, this requires solving the MINLP problem (26) _to optimality_, which is NP-hard in general. In contrast, the proposed dual conic completion can be performed efficiently, and closed-form solutions are available for broad classes of problems. Second, (sub)gradient information \(\) is obtained from an _optimal_ solution of (26), which poses obvious limitations if (26) is solved approximately. Third, arbitrary values of \(,\) may result in

    & &  &  \\  \(n\) & Opt val\({}^{}\) & avg. & std & max & avg. & std & max \\ 
10 & 3441.8 & 70.76 & 9.42 & 90.23 & **0.23** & 0.57 & 17.05 \\
20 & 6988.2 & 78.52 & 6.67 & 92.31 & **0.41** & 0.69 & 9.04 \\
50 & 17667.4 & 81.70 & 5.41 & 92.69 & **1.03** & 1.69 & 21.68 \\
100 & 35400.2 & 83.25 & 4.78 & 93.31 & **0.37** & 0.57 & 6.69 \\
200 & 70808.5 & 84.06 & 4.20 & 93.44 & **0.29** & 0.46 & 4.81 \\
500 & 177060.0 & 86.74 & 3.80 & 93.74 & **0.46** & 0.73 & 9.92 \\
1000 & 354037.5 & 87.01 & 3.71 & 93.80 & **0.36** & 0.48 & 4.44 \\    All gaps are in %; best values are in bold. \({}^{}\)Mean optimal value on test set; obtained with Mosek.

Table 4: Comparison of optimality gaps on production planning instances.

   \(n\) & Mosek\({}^{}\) & DC3\({}^{}\) & DLL\({}^{}\) \\ 
10 & 73.5 CPU.s & 2.7 GPU.ms & 0.2 GPU.ms \\
20 & 75.3 CPU.s & 2.7 GPU.ms & 0.2 GPU.ms \\
50 & 15.4 CPU.s & 2.7 GPU.ms & 0.2 GPU.ms \\
100 & 24.9 CPU.s & 2.7 GPU.ms & 0.4 GPU.ms \\
200 & 49.9 CPU.s & 5.1 GPU.ms & 1.0 GPU.ms \\
500 & 98.8 CPU.s & 15.9 GPU.ms & 5.1 GPU.ms \\
1000 & 203.0 CPU.s & 41.5 GPU.ms & 19.0 GPU.ms \\    \({}^{}\)Time to solve all instances in the test set, using one CPU core. \({}^{}\)Time to run inference on all instances in the test set, using one V100 GPU.

Table 5: Computing time statistics for nonlinear instances(26) being unbounded, yielding a dual bound of \(-\) and no usable gradient information. In contrast, in the conic setting, Theorem 1 provides sufficient conditions under which dual completion is always possible. Finally, an intrinsic limitation in the MINLP setting is the absence, in general, of strong duality. Therefore, even predicting a dual-optimal \((,)\) may be insufficient to prove optimality, thus requiring additional computation such as branching. In contrast, the strong conic duality theorem  offers a robust foundation to obtain high-quality dual bounds efficiently.

### Limitations

The main theoretical limitation of the paper is that it considers convex conic optimization problems, and therefore does not consider discrete decisions nor general non-convex constraints. Since convex relaxations are typically used to solve non-convex problems to global optimality, the proposed approach is nonetheless still useful in non-convex settings. Furthermore, as pointed out in Section 6.1, the DLL framework extends naturally to the MINLP setting, by leveraging Lagrangian duality for discrete and/or nonlinear problems. However, this approach suffers from several theoretical and computational limitations.

On the practical side, the optimal dual completion presented in Section 4.1 requires, in general, the use of an implicit layer, which is typically not tractable for large-scale problems. In the absence of a known closed-form optimal dual completion, it may still be possible to design efficient completion strategies that at least ensure dual feasibility. One such strategy is to introduce artificial large bounds on all primal variables, and use the completion outlined in Example 1. Finally, all neural network architectures considered in the experiments are fully-connected neural networks. Thus, a separate model is trained for each input dimension. Nevertheless, the DLL methodology is applicable to graph neural network architectures, which would support arbitrary problem size. The use of GNN models in the DLL context is a promising avenue for future research.

## 7 Conclusion

The paper has proposed Dual Lagrangian Learning (DLL), a principled methodology for learning dual conic optimization proxies. Thereby, a systematic dual conic completion, differentiable conic projection layers, and a self-supervised dual Lagrangian training framework have been proposed. The effectiveness of DLL has been demonstrated on numerical experiments that consider linear and nonlinear conic problems, where DLL significantly outperforms DC3 , and achieves 1000x speedups over commercial interior-point solvers.

One of the main advantages of DLL is its simplicity. The proposed dual completion can be stated only in terms of _primal_ constraints, thus relieving users from the need to explicitly write the dual problem. DLL introduces very few hyper-parameters, and requires minimal tuning to achieve good performance. This results in simpler models and improved performance, thus delivering computational and environmental benefits.

DLL opens the door to multiple avenues for future research, at the intersection of ML and optimization. The availability of high-quality dual-feasible solutions naturally calls for the integration of DLL in existing optimization algorithms, either as a warm-start, or to obtain good dual bounds fast. Multiple optimization algorithms have been proposed to optimize Lagrangian functions, which may yield more efficient training algorithms in DLL. Finally, given the importance of conic optimization in numerous real-life applications, DLL can provide a useful complement to existing primal proxies.