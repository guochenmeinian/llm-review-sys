# Causal language modeling can elicit search and reasoning capabilities on logic puzzles

Kulin Shah

UT Austin

kulinshah@utexas.edu

&Nishanth Dikkala

Google Research

nishanthd@google.com

&Xin Wang

Google Research

wanxin@google.com

&Rina Panigrahy

Google Research

rinap@google.com

Work done during an internship at Google Research.

###### Abstract

Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves \(94.21\%\) of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves \(92.04\%\) of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights 2.

## 1 Introduction

Language models using the Transformer architecture  have displayed remarkable abilities on a variety of Machine Learning tasks over the last few years , . Trained with simply the task of predicting the next token on huge amounts of text, these models display highly performant and deep language understanding skills. In order to make progress on achieving a human-like artificial intelligence, one of the most important ability is the ability to perform human-like reasoning and planning. Although LLMs have displayed a seemingly remarkable ability to excel at reasoning and planning tasks as well, it is a ongoing debate as to whether this ability comes from a true understanding and reasoning of the underlying problem or some other process which simulates reasoning but can be highly brittle. For instance, although LLMs show remarkable

[MISSING_PAGE_FAIL:2]

We consider a dataset of Sudoku puzzles of varying difficulty levels from . In addition, we use a Sudoku solver which employs a set of 7 strategies that humans commonly use for solving Sudokus. Given these set of 7 strategies the solver iteratively scans through all unfilled cells and checks if progress can be made using one or more of the strategies. If it finds a cell where progress can be made if fills in its value and repeats the process of searching for the next cell to fill. Although some of the 7 strategies are simple and direct, some of them are highly non-trivial and non-local. From the dataset, we filter out those puzzles which cannot be solved by our solver and end up with 1.9M examples. This ensures that all our puzzles are solvable in polynomial time 3.

We can characterize the size of a Zebra puzzle by a tuple of two numbers: the number of entities and the number of attributes. Each clue in a puzzle is one of 7 different types. We generate around 320,000 Zebra puzzles of sizes varying between (3,3) to (6,6) in the following manner. We first design a human-like solver for these puzzles which tries to solve the puzzles in an iterative manner without backtracking. This solver runs in time cubic in the number of clues of the puzzle. When generating a puzzle of a certain size, we iteratively keep adding clues to a clue set until our solver is able to solve the puzzle. This way, we can ensure that, similar to our Sudoku puzzles, all our sampled Zebra puzzles are also solvable efficiently. Note that, even in the symbolic format, there are an exponential number of puzzles possible implying that our train set and test set won't overlap with a very high probability.

### Our results

In this section, we provide an overview of our results. We mainly focus on the Sudoku puzzles to explain our results and include a brief discussion on the Zebra puzzles.

Our first experiment studies whether a Transformer is capable of solving the Sudoku puzzle in a fixed cell order (from top-left to bottom-right). This would amount to the model knowing what values to fill in each unfilled cell in a single forward pass. We observe that although the model learns to predict the values for some cells in a puzzle (average cell accuracy **58.64%** across all unfilled cells), in general, this leads to poor accuracy of solving the complete puzzle (**7.2%**).

Observe that solving a Sudoku puzzle can be thought of as finding easy-to-decode cells and then finding correct value at such cells. We combine this observation with insights from Chain-of-Thought prompting and use our solver to provide the order to fill cells for a given puzzle. In this setting, we use the cell positions provided by the solver during the decoding (i.e., position hints of easy to decode cells) and calculate how many cell values the model gets right. In other words, given a prefix of a partially solved puzzle, we query the solver to find out the "easiest" cell position to solve next and then, conditioning on this position, query the model for its value. The average cell accuracy only goes up marginally by about 3%.

To exploit the full value of the order given by our solver, we train the model from scratch using the solver order. This allows the model to learn what is a good strategic order in which to fill the cells. Importantly this order is adaptive based on the puzzle. To train it in this manner, we first feed each puzzle to our solver and collect the sequence of cells it fills in order. We use these sequences as our training data which acts as our _Chain-of-Thought_ data for the model. This leads to a much stronger model which is able to solve full Sudoku puzzles to an accuracy of **87.18%** (see Section3.4).

Given this new model, we again try giving position hints during decoding as above and we see the average cell accuracy shoot up to **99.02%**. This indicates the following. The iterative process of solving Sudokus can be broken down into two steps: (1) searching and finding a cell position where we can apply a subset of the strategies, (2) given a cell position, computing the value that needs to be filled in that position. Step (1) is the harder task for a model to learn. We provide examples of Sudoku puzzles where the model makes a mistake in step (1) where step (2) is quite trivial (see Section4.1 for more details). To make the model more proficient at solving the puzzle without the position hints, we perform a beam search of width 3 or 5 and notice that this suffices to get stronger full puzzle solving accuracies of **91.36%** and **94.21%** respectively (see Section3.5).

In an environment where the model needs to search over a set of candidates to take as the next step, recent work by  demonstrated that next-token prediction might be a flawed objective. Another recent work  posit including the entire search trace as part of the training Chain-of-Thoughtdata to help a Transformer learn tasks involving search and planning dynamics. In contrast to these works, we observe that Transformers trained with the next-token prediction objective and without access to the entire search trace _can_ learn complex reasoning tasks.

Finally, we further ask if we can see a similarity between the model's way of solving the puzzles to a humans/solvers way of solving the puzzle. We study this via probing which has been a technique to understand the _latent_ conceptual content, see e.g. . In particular, works such as  argue that often simple functions of the model's activations or weights can extract useful latent information (See Appendix B for more details). We study the following via probing. Generally, humans and algorithmic solvers for Sudoku keep track of a possible set of values for each cell at a given state of the board to make progress on solving the Sudoku puzzle. We see that the model also implicitly keeps track of a candidate set and this candidate set matches with the solver's candidate set (see Section 4.2 for more details).

We perform a similar set of experiments as above on Zebra puzzles and observe qualitatively similar trends giving evidence that our conclusions are not limited to the domain of Sudoku (See Appendix I for more details). In summary, our contributions are

1. We show that causal language modeling with the Transformer architecture is capable of learning to perform search and reasoning in highly non-trivial domains like Sudoku and Zebra puzzles.
2. We present evidence that the right form of training data which decomposes the problem into smaller constituents is crucial. However this data is not required to be too descriptive. In particular, it need not contain search traces similar to those provided in .
3. We perform a probing analysis to show that human-like abstract reasoning concepts such as candidate set of values emerge implicitly within the model's activations.

## 2 Preliminaries and setup

In this section, we provide a brief overview of the logic puzzles we consider and the input/output data format that is fed to the model. More details about Zebra puzzle, dataset, architectures and hyperparameters can be found in Appendix D.

**Sudoku puzzle and solver.** The goal of the Sudoku puzzle is to fill out the whole board with numbers \(1\) to \(9\) without having duplicates in each row, column, and box (See appendix H for more details). Unless specified otherwise, we will use \((r,c)\) to denote the position of a cell on the board and \(v(r,c)\) to denote the value at position \((r,c)\) where \(r\{1,2,,9\}\) denotes the row number of the cell and \(c\{1,2,,9\}\) denotes the column number of the cell. Additionally, we use \(b(r,c)\) to denote the block number (among one of the nine \(3 3\) blocks) of the cell at position \((r,c)\). To solve a Sudoku puzzle, a sudoku-solver (and humans up to an extent) keeps track of the candidate set for each of the empty cells. See more details about Candidate set in Appendix H.

As mentioned earlier, the generalized version of Sudoku with board size \(n n\) is NP-complete . This implies that for some Sudoku puzzles, progress likely can not be made using any strategy that executes in polynomial time. We avoid such puzzles by restricting our focus to those Sudoku puzzles that can be solved using a set of 7 well-known and commonly used strategies which are executable efficiently4. Further details about each of the strategies is provided in Appendix C. An important point to note is that not all the strategies fill a value in a cell. In fact, only 2 out of 7 strategies that we use, fill a value in a cell and the other strategies are used to eliminate possible values of a cell and narrow down the candidate set at a particular cell. Additionally, some strategies (e.g., XY wing, Unique rectangle) involve reasoning on multiple cells in different rows/columns/blocks and these strategies don't fill a value at any cell and therefore, these strategies need to be applied in combination with other strategies to deduce a value at a cell. Additionally, we only provide the solution list of cell values to the puzzle during training, therefore the model is not getting any direct signal about the strategies that eliminate possible values of a cell and is only getting a signal in combinations of the strategies that deduce a value.

**Dataset, model architecture and training.** Our training dataset for the Sudoku experiment contains 1.8M puzzles and the test dataset contains 0.1M puzzles. Each puzzle also comes with a difficulty rating calculated as follows. To rate a puzzle, a backtracking based solver (different from the one we use to generate our solver-order data) is employed. This solver tries to iteratively make progress on a puzzle using some elimination techniques. When it gets stuck, it makes guesses and tries to solve the puzzle. The difficulty rating is the maximum depth of the guess stack the solver had to use to solve the puzzle. Therefore, even a puzzle rated 0.5 can require complex strategies beyond simple scanning to solve them without guessing. We train a sequence-to-sequence model that takes in as input a representation of a Sudoku puzzle as a sequence and needs to output the solution of the puzzle as a sequence. During the training, we provide information about a single cell using three tokens \((r,c,v(r,c))\): the first two tokens \((r,c)\) contain information about the position of the cell (row and column number) and the third token contains the number in that cell. Each training sequence is divided into two parts. The first part contains the information about cells whose values are given in the puzzle question and the second part contains information about unfilled cells in the solution. Note that there can multiple valid orders for the solution. Also, note that the length of the first part depends on the number of cells filled in the puzzle. We train the model using the next-token prediction loss but we don't apply the loss corresponding to the prediction of the filled cells given in the question.

We use a Transformer-based GPT-2  architecture with 8 layers for both puzzles. Each layer has \(8\) attention heads with a model dimension of \(576\) and an MLP of hidden dimension \(3456\) (\(6\) model dimension) follows in each layer. The total number of parameters of our model is 42M. We use causal masking in the attention layers to avoid looking into the future.

**Evaluation metrics.** To evaluate the performance of our model, we use the following two metrics primarily: 1) **Cell accuracy**: denotes the percentage of the unfilled cells whose values are correctly predicted by the model. 2) **Complete puzzle accuracy**: denotes the percentages of the correctly solved puzzles in the evaluation dataset. A puzzle with even a single mistake is counted as incorrect.

## 3 Experiments on Sudoku puzzles

We study the performance of a Transformer model when the model is trained with the next-token prediction objective. We set up the model architecture and training of the model as discussed in Section 2 however, the question remains how to order cells in the input sequences of the Sudoku puzzle during the training of the model. Note that given the state of a Sudoku puzzle, some cells might be easier to solve than others so the order of the cells of input sequences provided during training could be important. We first try using a predefined fixed order or a random order of the cells during the training and inference in Section 3.1. However, this leads to poor performance. Thereafter, we turn our focus on using a solver to create a better order which we call solver-decomposed reasoning order (Section 3.2). Inspired by Chain-of-Thoughts literature , Section 3.3 uses solver-decomposed reasoning order only during the inference on the above trained models to provide position hints. Yet, conditioning on these position hints during decoding only provides a relatively small improvement in the performance showing that even if we tell the model to find the value in a particular cell, it has not learnt fully how to do so.

Therefore, in Section 3.4, we explore training the model using cells provided in solver-decomposed reasoning order. This provides a huge boost to the performance allowing the model to solve over 85% of the puzzles in the test set accurately. However, it still does not achieve near-perfect cell accuracy. Therefore, Section 3.5 uses beam search decoding to improve the performance.

### Training using fixed or random order of the cells

A natural choice for the cell order in the input sequence would be to use a fixed order of the cells or a random order of the cells in the puzzle for the input sequence. Note that the order of the puzzle is only provided during the training and we do not penalize the model for wrong order during evaluation as long as it solves the given Sudoku puzzle correctly.

**Fixed order of the cells.** In this ordering of the cells, we arrange the cells in a predefined fixed order of top-left to bottom-right of the board of the puzzle. To be more precise, for any two cells \((r,c)\) and \((r^{},c^{})\) where \(r\) and \(r^{}\) denote the row numbers and \(c\) and \(c^{}\) denote the column number, we will order the first cell \((r,c)\) before \((r^{},c^{})\) if \(r<r^{}\) or \(r=r^{}\) and \(c<c^{}\). We order both parts of the puzzle (input sequence) - given cells in the puzzle and the remaining solution of the puzzle using the above-mentioned ordering.

**Random order of the cells.** Another way to arrange the cells that we consider is to randomly order cells in given cells of the puzzle and solution of the inputs. For any given prefix (state of the puzzle), we randomly pick a cell from the set of empty cells and append that cell and corresponding value to the prefix.

**Results.** We provide the experimental results for the fixed order in Figure 1. We see that the model trained with fixed order achieves 58.64% cell accuracy and only 7.2% full puzzle accuracy whereas the model trained with random order only achieves around 52% cell accuracy and only \(1\%\) complete puzzle accuracy.

In the above ordering of the cells, given a state of the puzzle, the model decides on a random cell or fixed cell to output value but at that state, only a few cells might be easier to solve than others and the model trained using random or fixed order of cells do not necessarily decode the easier cells at that state. Therefore, inspired by Chain-of-thought literature , we ask the following question: if we provide the model information during inference which cells are easier to fill then does the performance improve? Before we answer the above question, we define the solver-decomposed reasoning order which will be useful in finding cells that are easier to fill.

### Solver-decomposed reasoning order

A natural way humans solve Sudoku is by iteratively trying to find cells that look easier to fill. The search process involves trying to see if any of a given set of strategies can be applied to fill in the value or otherwise make progress on a cell. Inspired by this analogy, we construct an order of filling cells using a solver. The solver uses 7 strategies as mentioned in Section 2. At any given state of the puzzle, it tries to apply to an easier strategy first and if it can not make progress with an easier strategy then it goes to a harder strategy. To apply a strategy, the solver goes through all the cells and tries to apply the strategy for each cell to make progress toward solving the puzzle. Progress doesn't necessarily mean filling a value in a cell but simply eliminating possible values from the candidate set (set of possible values) of a cell also counts as progress. We call the order given by the solver as _solver-decomposed reasoning order_ or _decomposed reasoning order_ for brevity when it is clear from the context. Note that the decomposed reasoning order arranges the cells based on how easy they are to fill in, as the solver initially employs simpler strategies to make progress.

### Hinted cell accuracy

In recent years, chain-of-thought (CoT) prompting  has emerged as one of the effective techniques to extract complex reasoning abilities from a model. The main idea of CoT is to lead the model to the correct output by providing intermediate steps to help the model. Inspired by the CoT prompting, we ask if providing the model additional information about easier-to-decode cells _during inference_ improves the performance?

Specifically, we use decomposed reasoning order to provide position hints during inference. Recall that to infer a value at position \((r,c)\) at a state of the puzzle \(s\), we provide \((s,r,c)\) as input to the transformer model and the random-order baseline model is trained to predict value \(v\) as next token given positions in previous two-tokens \((r,c)\), and because positions are chosen randomly during the training, the model is forced to use the positions in previous two-tokens \((r,c)\) while predicting the

Figure 1: Comparison of cell accuracy and full puzzle accuracy for fixed order training, random order training and solver-decomposed reasoning order training.

value. (Note that this is not the case for the model trained with fixed order and therefore, we don't consider them in this experiment).

Now, to provide additional information to the model about the easier cells to predict, we use the decomposed reasoning order. Specifically, for any given state \(s\), we provide the state \(s\) to the solver to obtain the position of the easiest cell. Suppose the solver picks \((r^{},c^{})\), then we provide \((s,r^{},c^{})\) to the trained model to predict a value at \((r^{},c^{})\). We reiterate this process for every non-empty cell of the puzzle. We measure the _cell accuracy_ in this setting and we call this accuracy as **hinted cell accuracy** to denote the provided hints about easy-to-decode positions from the solver.

**Results.** We see that the model trained using random order achieves \(54.57\%\) hinted cell accuracy. This means that providing hints about easy-to-decode positions improves the accuracy by around \(3\%\) over without any hints. At first glance, it seems like the model is struggling significantly to implement the correct strategy even when we provide information about the positions of easy-to-fill cells as hints during the inference. However, this might not be the correct conclusion because of the following reasons: during the training, the model is trained to predict the value from random cells, and the model needs to learn and apply very hard strategies as well to improve its training loss. In the process of learning hard strategies, the model might fail to learn easier strategies as well because of various reasons (e.g., limited data corresponding to easier strategies, limited model size, etc.).

When we use decomposed reasoning order during the inference, it helps to improve the performance for the model trained using random-order of the cells but because the model needs to perform a hard search and reasoning task while decoding a value at a single cell, it not only seems to hurt the model in searching easy-to-decode cells but also affects its reasoning capabilities to decode a value at given cells even after we explicitly provide positions of easy to fill cells. This motivates us to use decomposed reasoning order during the training.

### Using solver for CoT training

Solving the sudoku puzzle can be decomposed into two sub-tasks: 1) Search across the board to find the cells that are easy to fill and 2) After finding the easy-to-fill cell, apply the correct strategy on the cell to obtain a correct value in it. As mentioned earlier, the model trained for fixed-order and random-order does not have explicit incentives to perform a search for easy-to-fill cells. Therefore, the motivation for providing the solver-decomposed reasoning order during the training is to provide an order of cells such that training a model using the order helps the model to decompose the complex task of solving sudoku into smaller sub-tasks.

To provide decomposed reasoning order of cells during the training, we arrange the cells according to how easy to fill they are. Note that we can obtain this using Then, we use these sequences during the training with the next-token-prediction loss for all the tokens. Therefore, given a board state \(s\), the loss corresponding to position tokens incentivizes the model to learn to find easy-to-decode cells, and the loss corresponding to value tokens incentivizes the model to learn the strategy.

**Result.** We provide the result for the decomposed reasoning order training in Figure 1. We see that using the decomposed reasoning order achieves the cell accuracy \(94.23\) % and complete puzzle accuracy \(87.18\%\) accuracy. Training the model on the decomposed reasoning order improves cell accuracy by around \(36\%\) over the fixed-order training and by around \(43\%\) over the random-order training. The most noticeable improvement comes in complete puzzle accuracy where decomposed reasoning order training achieves \(87.18\) % accuracy whereas the fixed-order training achieves around \(8\) % accuracy and the random-order training achieves around \(1\) %.

**Hinted accuracy for training using solver-decomposed reasoning order.** Even though solver-decomposed reasoning order training significantly improves performance over fixed-order training and random-order training, it does not achieve near-perfect accuracy. Therefore, to understand whether the model is struggling to perform a search for easy-to-decode training or to employ a strategy given a position, we perform the experiment of providing hints about easy-to-decode positions (presented in Section 3.3). Recall that to measure the hinted cell accuracy for a model, we provide information about easy-to-decode cells to the model during inference and measure cell accuracy in that setting. We see that the model with solver-decomposed reasoning order training achieves **99.02 %** hinted cell accuracy. This means that _the model can employ the correct strategy assuming it has access to information about easy-to-decode cells and that the performance gap in cell accuracy (94.23 % to near-perfect accuracy) is mainly due to searching for easy-to-decode cells_.

Next, we try to bridge the gap between \(94.23\%\) cell accuracy and \(99.02\%\) hinted cell accuracy achieved by the model trained using decomposed reasoning order. To improve the accuracy, we first understand the number of cells that are filled for a puzzle when the model is making the first mistake for the puzzle. Note that when the model makes a mistake by filling in an incorrect value on the puzzle, then the probability of the model making a mistake on the remaining empty cells increases. We also see this happen in our experiments (See Appendix E).

A hypothesis about lower cell accuracy than hinted cell accuracy is that given a certain state of the puzzle, the model might be confused between several cells about which cells are easier to decode. However, if the model is allowed to explore multiple potential cells of the puzzle, it might figure out the true solution as the model will make a prediction confidently for the true solution of the puzzle compared to other wrong solutions. Because of this reason, we try beam-search decoding for the model trained using solver-decomposed reasoning order.

### Beam-search decoding

Beam-search decoding (used in many popular NLP systems, e.g. ) in language modeling allows the model to explore multiple partial decoding of the sequences during the inference of a language model and output the most probable explored sequence. The beam width \(k\) of the beam search decoding denotes how many partial sequences (hypotheses) are kept at each step. At each step of the decoding, it expands all partial solutions of the puzzle by decoding one more token. Then, among this expanded partial solution set, the model selects the top \(k\) most probable partial solutions. This process is repeated for the decoding of every token. Note that beam search only maintains \(k\) possible output sequences throughout the decoding process. Compared to standard decoding, beam search incurs a computational overhead of a factor of \(k^{2}\). In the Sudoku puzzle, It is important to note that the beam search can not try out all possible outputs for the Sudoku puzzle. After all, the total number of outputs can be arbitrarily large because many of the empty cells will have on average 2 to 5 possible values and the total number of empty cells in the puzzle is at least 50.

**Results.** We present our results for beam-search decoding in Table 2. The beam search with \(k=1\) is equivalent to greedy decoding as it only keeps one partial sequence. We see that beam search with \(k=3\) improves the cell accuracy by around 2% and complete puzzle accuracy by around 4%. We see a similar improvement when we increase beam width from \(k=3\) to \(k=5\). Note that the cell accuracy with beam width \(k=5\) is able to bridge the gap from the hinted cell accuracy up to a large extent but does not need hints about easy-to-decode positions.

## 4 Analysis

In the above section, we showed that solver-decomposed reasoning order during the training can greatly help improve the model's performance. In this section, we analyze the trained model on several fronts. Section 4.1 contains a discussion about failure cases of the model in searching easy-to-decode cells. In Section 4.2, we show that the candidate set information emerges in the model to explain how the learned model is solving the puzzles. We compare the model's performance to a neural network-based method designed to solve the Sudoku puzzle  in appendix F.1. Appendix F.2 contains the breakdown of the complete puzzle accuracy across various difficulties.

### Failure in search for easy-to-decode cells

As discussed in section 3.4, the model trained using solver-decomposed reasoning order solves 94.23 % cells of the sudoku puzzles correctly. To understand the failure modes of the model, we measure the _hinted cell accuracy_ by providing information about easy-to-decode cells to the model during inference. We see that the model achieves \(99.02\%\) accuracy. This shows that the model can find

   & Cell accuracy & Complete puzzle accuracy \\   Beam width \(k=1\) & 94.23 \% & 87.18 \% \\ Beam width \(k=3\) & 96.07 \% & 91.36 \% \\ Beam width \(k=5\) & 98.03 \% & 94.21 \% \\  

Table 2: Performance (cell accuracy and complete puzzle accuracy) change as we increase beam-width in beam-search.

the correct value at the cell when it is provided the information about easy-to-decode cells and the performance gap in cell accuracy is mainly due to the inability to search for easy-to-decode cells.

We also provide some examples of the puzzle situations in Figure 2 and Figure 7 (in Appendix) when the model makes a mistake by trying to fill a cell but there is another cell for which it is easier to fill. This supports our finding by hinted cell accuracy that the performance gap of our trained model to the perfect accuracy is due to the inability to search for easy-to-decode cells.

Additionally, a cell can be easy-to-decode because of either row, column or block constraint of the Sudoku puzzle. We found that our trained model misses more cells which are easy-to-decode because of the block constraint. This might be due to the input format being not explicit for block and explicit for row and column of a cell (recall that a cell is in the format of \((r,c,v(r,c))\) as input to the model). A natural extension in this case could be to provide a block number also as an input to the model. We leave it for future work.

### Emergence of candidate set information in the model

We saw in the previous section that a model trained with puzzles given in solver-decomposed reasoning order performs very well. Therefore, we focus on how the model is learning such a task that requires planning and reasoning. As mentioned earlier, the sudoku solver (and to an extent humans) keeps track of possible values that a cell can take for the given puzzle. Therefore, we ask the following question: does the model also keep track of possible values of a cell? Can we extract them from the model?

We answer both of these questions (perhaps surprisingly) positively by showing that for a given puzzle, the candidate set of the solver can be extracted from the logits of the model. The candidate set of an empty cell keeps track of possible values that the cell can take given a state of the puzzle. Note that given some state of the puzzle, the candidate set at an empty cell \((r,c)\) can be different from \(\{1,2,,9\}-\{\}\) as some of the strategy removes a value which does not occur in the same row, column or box.

**Calculating candidate set equivalence.** For all puzzles in the validation dataset, we obtain the candidate set of all empty cells from the solver when the number of filled cells is in the set \(S=\{35,40,45,50,55,60,65,70,75\}\). For a state \(s\) of a puzzle, we denote the candidate set of the solver at an empty cell \((r,c)\) as \(f^{*}(s,r,c)\). We use \(|f^{*}(s,r,c)|\) to denote the number of possible values in the candidate set \(f^{*}(s,r,c)\). To extract the candidate set from the model at the state \(s\) of the puzzle and at an empty cell \((r,c)\), we feed \((s,r,c)\) as the prefix to the model and values corresponding

Figure 2: **A failure case of the model in searching for easy-to-decode cells.** The left figure shows the sudoku puzzle state when the model makes the first mistake and the right figure shows the puzzle’s solution. Numbers given in the blue are provided in the puzzle. The puzzle makes a mistake by choosing to fill the red-colored cell whereas the green background cell can be easily filled.

to top-\(k\) output logits where \(k=|f^{*}(s,r,c)|\) becomes the candidate set of the model. We denote the model's candidate set as \(m(s,r,c)\). Importantly, note that we DO NOT only evaluate the top-\(k\) candidates on the cell the model choices to predict. Although during its natural course of decoding the model might wish to decode cell location A, we force it (by conditioning) to decode at every other location and evaluate the top-\(k\) candidates. This ensures that we are looking at what the model thinks is the set of possible candidates of cell location \((r_{1},c_{1})\) even when it has decided to decode cell \((r_{2},c_{2})(r_{1},c_{1})\) next. We note that this style of probing differs from the more common way to perform a probing analysis which involves learning a linear/non-linear probe which takes in the embedding and outputs a label indicating a concept. However, we use probing in more general sense to refer to understand some of the inner workings of the model.

The accuracy for the candidate set equivalence between the solver and the model at a state \(s\) of a puzzle and at an empty \((r,c)\) is measured by \(|f^{*}(s,r,c) m(s,r,c)|/|f^{*}(s,r,c)|\). The reported accuracy at position \(n S\) in Table 3 is the average over all empty cells when the number of filled cells is \(n\) for the puzzles which are correctly solved by the model. Intuitively, the candidate-set equivalence accuracy measures the average overlap between solver's and model's candidate set for the correctly solved puzzles.

**Results.** The results of candidate set equivalence accuracy are given in Table 3. We see that for all positions the average overlap between the solver's and the model's candidate set is above 93 %. This overlap improves to around 96.5 % when the prefix has information about 60 cells and to around 98.5 % when the prefix contains information about 70 cells. Note that to extract the candidate set of the model, we are just reading the logits and not even training a linear function. Additionally, the candidate set equivalence result is not only for cells that are easy to decode but for all empty cells. Moreover, during the training of the model, no direct information about the candidate set is provided and the model is only trained to predict the correct value for a cell and therefore is not directly incentivized to predict the correct candidate set for _all_ the empty cells with such a high accuracy.

## 5 Conclusion

We have shown that even on complex logical reasoning tasks such as Sudoku and Zebra puzzles, simple next-token prediction provided with a high-level decomposition of the reasoning steps during training is able to learn to solve the task. This suggests that, given the right level of detail and breakdown of reasoning steps in the training data, a pre-trained model might already present as a strong reasoning engine (without the need for post-training techniques such as fine-tuning, prompt engineering, self-consistency, tree-of-thoughts etc). These techniques might help significantly boost the baseline performance of a model or potentially make up for deficiencies in the pre-training data however. To move towards more general reasoning systems, an interesting challenge to overcome would be to simulate the decomposed reasoning data in an efficient manner. These tasks capture many different types of constraint satisfaction problems and we believe the framework and results should generalize to other settings as well.

Finally, we conclude with some limitations of our study. Firstly, we note that we studied a synthetic setting on a toy task and real-world reasoning and planning tasks can be much more abstract and challenging. More specifically, Sudoku is a task which doesn't require the same degree of long-term planning as some harder benchmarks. That is, any cell we can make progress on is progress unlike constraint problems where one might need to backtrack. Moreover, we focused on a reasoning setting where creative thinking was not required. That is, the model did not need to invent new strategies to solve any test time puzzle. It is an interesting future direction to study to what extent causal language modeling can yield novel reasoning strategies. Moreover, there can be many different types of reasoning tasks which are not logic puzzles (for instance probabilistic puzzles or rule-less puzzles, see e.g. ) and our experiments do not explore those.

  Number of filled cells & 35 & 40 & 45 & 50 & 55 & 60 & 65 & 70 & 75 \\ Accuracy (\%) & 93.19 & 93.23 & 94.14 & 94.81 & 94.70 & 96.60 & 97.71 & 98.54 & 99.37 \\  

Table 3: Candidate set equivalence accuracy when the number of filled cells is different in the given puzzle. The candidate-set equivalence accuracy measures the average overlap between the solver’s and the model’s candidate set for the correctly solved puzzles.