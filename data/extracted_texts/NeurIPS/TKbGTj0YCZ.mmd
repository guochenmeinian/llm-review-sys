# Robust Conformal Prediction under

Joint Distribution Shift

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Uncertainty prevails due to the lack of knowledge about data or model, and conformal prediction (CP) predicts multiple potential targets, hoping to cover the true target with a high probability. Regarding CP robustness, importance weighting can address covariate shifts, but CP under joint distribution shifts remains more challenging. Prior attempts addressing joint shift via \(f\)-divergence ignores the nuance of calibration and test distributions that are critical for coverage guarantees. More generally, with multiple test distributions shifted from the calibration distribution, simultaneous coverage guarantees for _all_ test domains requires a new paradigm. We design _Multi-domain Robust Conformal Prediction (mRCP)_ that first formulates the coverage difference that importance weighting fails to capture under any joint shift. To squeeze such coverage difference and guarantee the \((1-)\) coverage in all test domains, we propose _Normalized Truncated Wasserstein distance (NTW)_ to comprehensively capture the nuance of any test and calibration conformal score distributions, and design an end-to-end training algorithm incorporating NTW to provide elasticity for simultaneous coverage guarantee over distinct test domains. With diverse tasks (seven datasets) and architectures (black-box and physics-informed models), NTW strongly correlates (Pearson coefficient=0.905) with coverage differences beyond covariate shifts, while mRCP reduces coverage gap by \(50\%\) on average robustly over multiple distinct test domains.

## 1 Introduction

The growing data volume, enhanced computation capability, and advanced models significantly improve machine learning predictive accuracy. Nevertheless, noises, unobservable factors, and the lack of knowledge lead to uncertainty that stakeholders should ponder along model predictions when making decisions particularly in areas such as fintech , autonomous driving , traffic forecasting , and epidemiology [32; 27]. Conformal Prediction (CP) addresses uncertainty by predicting a set of possible target(s) rather than a single guess . Specifically, CP computes conformal scores (residuals between predicted and true targets for regression tasks) of a trained model \(f\) on a calibration set, and calculates the \(1-\) quantile \(q\) of these scores. For any input \(x\), CP produces the smallest prediction set \(C(x)\) consisting of target values whose conformal scores are less than \(q\). Assuming that the test and calibration data are exchangeable (including i.i.d.), the true target \(y\) is guaranteed to be covered by \(C(x)\) with at least \(1-\) probability.

In practice, calibration distribution \(P_{XY}\) and test distribution \(Q_{XY}\) may differ thus \(P_{XY} Q_{XY}\), termed as **joint distribution shift** and violate the exchangeability assumption. Joint shift can occur with either covariate shift (\(P_{X} Q_{X}\)) or concept shift (\(P_{Y|X} Q_{Y|X}\)), though what causes a joint shift is difficult to infer from the observed data only. With importance weighting, covariate shift is shown not to affect the coverage confidence guarantee . To address CP under jointshift, \(f\)-divergence is adopted in [33; 6] to measure the difference between \(P_{XY}\) and \(Q_{XY}\) or the corresponding conformal scores distributions. However, \(f\)-divergence ignores where the two distributions differ, which quantiles and coverage guarantees depend on (Figure 1, (c)). When test data are sampled from multiple distinct test distributions \(Q_{XY}^{(e)},e=\{e_{1},...,e_{M}\}\), it is desired to ensure simultaneous \(1-\) coverage for all test distributions. Previous work selects the highest \(1-\) quantile from all test distributions and constructs \(C(x)\) for \(x Q_{X}^{(e)}, e=\{e_{1},...,e_{M}\}\), producing excessively large set \(C(x)\). Selecting other quantiles may lead to smaller coverage on a test domain than was expected during calibration, leading to prediction overconfidence. Without a new paradigm to guarantee coverage under multiple shifted test distributions, the dilemma between CP coverage efficiency and confidence guarantee seems unavoidable.

We first decompose the coverage difference under any joint distribution shift to a component due to covariate shift (\(P_{X} Q_{X}^{(e)}\), addressed by importance weighting ) and that due to concept shift (\(P_{Y|X} Q_{Y|X}^{(e)}\)). We propose _Normalized Truncated Wasserstein distance (NTW)_ to robustly capture where the test and importance-weighted calibration conformal score cumulative density function (CDF) deviate (Figure 1, (b)). We design _Multi-domain Robust Conformal Prediction (mRCP)_ by minimizing all NTW terms over \(=\{e_{1},...,e_{M}\}\) during model training (Figure 1, (d)) to elastically guarantee coverage confidence for all test domains. Experiments on regression tasks on seven datasets demonstrate that: _1)_ NTW well-correlates with the coverage difference after importance weighting (Pearson coefficient 0.905); 2) mRCP provides conformal predictions that reduce average coverage difference by \(50\%\) compared to baselines under multiple joint shifts; _3)_ mRCP is sufficiently general to address joint distribution shifts even after incorporating domain knowledge when available.

## 2 Background and related work

### Conformal prediction

Let \(x\) and \(y\) denote the input and output random variable, respectively, where \(\) and \(\) is the input and output space, respectively. On \(\), the calibration domain is defined by a joint distribution \(P_{XY}\), and we consider a calibration set \(S_{c}=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) are drawn _i.i.d._ from \(P_{XY}\). Similarly, a test set \(S_{t}=\{(x_{1},y_{1}),,(x_{m},y_{m})\}\) is drawn _i.i.d._ from test domain, which is defined by a joint distribution \(Q_{XY}\).

With a trained regression model \(f\), the conformal score \(v_{i}=v(x_{i},y_{i})=|f(x_{i})-y_{i}|\) is the residual between the predicted target \(f(x_{i})\) and the true target \(y_{i}\). The set of calibration conformal scores is denoted as \(V_{c}=\{v(x_{i},y_{i})|(x_{i},y_{i}) S_{c}\}\). Let \(q\) be the \((1-)(n+1)/n\) quantile of \(V_{c}\):

\[q=(, _{v_{i} V_{c}}_{v_{i}}),\] (1)

where \(_{v_{i}}\) represents the point mass at \(v_{i}\) (i.e., the distribution placing all mass at the value \(v_{i}\)). Quantile\((1-,F):=\{z|(Z<z) 1-\}\) and \(F\) is the CDF of \(Z\). With the quantile \(q\), the CP prediction set of an input \(x\) from \(S_{t}\) is

\[C(x)=\{||f(x)-|\,q,(x,y) S_{t}\}.\] (2)

Most CP methods, such as[22; 23], rely on the assumption of exchangeability, which is relaxed from the i.i.d. assumption . In our scenario, if the calibration and test samples are drawn from the identical joint probability distribution (\(P_{XY}=Q_{XY}\)), these calibration and test samples are i.i.d. Under this assumption, the probability that the true target \(y\) is included in \(C(x)\) is at least \(1-\), which is called **coverage guanrantee**, or more formally,

\[(y C(x)) 1-.\] (3)

### Conformal prediction under domain shift

**Covariate shift** (\(P_{X} Q_{X}\)) means marginal distributions between the calibration and test domains are different. CP under covariate shift is addressed using importance weighting . Under a probabilistic view,  defined the covariate shift as a bounded perturbation on any test input and developed adaptive probabilistically robust CP. The condition of multiple test domains is discussed in , and similar topics include coverages under feature-stratification [7; 11].

**Joint distribution shift** (\(P_{XY} Q_{XY}\)) indicates at least one of covariate shift (\(P_{X} Q_{X}\)) and concept shift (different conditional distributions, \(P_{Y|X} Q_{Y|X}\)) will occur . This shift is more general and the importance weighting method cannot address changes in conditional distribution. With \(M\) test domains \(=\{e_{1},...,e_{M}\}\), each \(e\) is defined by a joint distribution \(Q_{XY}^{(e)}\) and holds a joint shift with calibration domain \(P_{XY}\) (i.e., \(P_{XY} Q_{XY}^{(e)}\)). Considering this condition, previous works, such as [6; 33], presume all test domains fall in a predefined \(f\)-divergence range, calculate confidence-specified quantile of each test domain, and apply the highest quantile to all domains. This method causes excessively high coverages and thus overlarge prediction sets, which reduces prediction efficiency because smaller prediction sets can help locate true targets better.

## 3 Conformal prediction under joint distribution shift

### Decomposition of coverage difference

We decompose the coverage difference between a calibration domain \(P_{XY}\) and a test domain \(Q_{XY}\) under **joint distribution shift** at a user-specified confidence \((1-)\).

Similar to \(V_{c}\), we define the test conformal score set \(V_{t}=\{v(x_{i},y_{i})|(x_{i},y_{i}) S_{t}\}\). With the indicator function \(\), empirical CDFs of calibration and test conformal scores are

\[_{P}(v)=_{v_{i} V_{c}}_{v} _{v_{i}<v},\ \ _{Q}(v)=_{v_{i} V_{t}}_{v} _{v_{i}<v}.\] (4)

With given \(1-\) confidence, quantile \(q\) is calculated in Eq. (1), and the coverage difference under a joint distribution shift can be quantified as

\[D_{}(q)=_{Q}(q)-_{P}(q).\] (5)

 employs importance weighting for CP under covariate shift. Specifically, if the ratio of test to calibration covariate likelihoods, \(Q_{X}/P_{X}\), is known, a calibration conformal score \(v_{i} V_{c}\) is weighted by \(p_{i}=w(x_{i})/_{j=1}^{n}w(x_{j})\), where \(w(x_{i})=Q_{X}(x_{i})/P_{X}(x_{i})\). Therefore, the empirical CDF of weighted empirical calibration scores is given by

\[_{Q/P}(v)=_{i=1}^{n}p_{i}_{v_{i}}_{v_{i}< v},\]

where the subscript \(Q/P\) indicates conformal scores of calibration domain \(P\) is weighted by conformal scores of test domain \(Q\). The confidence-specified quantile of the weighted calibration conformal scores is

\[q^{*}=((1-)(n+1)/n,_{i=1}^ {n}p_{i}_{v_{i}}).\] (6)

As importance weighting ensures the \(1-\) coverage as though covariate shift were absent, coverage difference \(D_{}\) caused by covariate shift is the gap between the coverages under test conformal score CDF using quantiles on unweighted and weighted calibration conformal score distributions.

\[D_{}(q,q^{*})=_{Q}(q)-_{Q}(q^{*}).\] (7)

Importance weighting can not address CP under joint shift as it fails to capture changes in conditional probability distribution caused by concept shift, thus we present the coverage difference caused by concept shift is

\[D_{}(q,q^{*})=D_{}(q)-D_{}(q,q^{*} )=_{Q}(q^{*})-_{P}(q),\] (8)

which is remaining coverage difference after applying importance weighting. Here we assume \(_{P}(q)=_{Q/P}(q^{*})\), so we can rewrite \(D_{}\) by

\[D_{}(q^{*})=_{Q}(q^{*})-_{Q/P}(q^{*}).\] (9)

The error bound for the assumption is quite small especially when the calibration set size \(n\) is large. The detailed proof is provided in Appendix B. We denote \(D_{}\) as \(D\) for simplification.

### Normalized Truncated Wasserstein distance

To develop a metric that is independent of confidence level and can quantify the overall closeness between weight calibration and test conformal scores, we estimate the expected coverage difference under concept shift as

\[[D]=_{v_{i} V_{c}}|_{Q}(v_{ i})-_{Q/P}(v_{i})|,\] (10)

based on the approximation in Eq. (9), where \(\) indicates the expectation function.

**Definition 1** (Wasserstein-1 Distance).: _If \(F_{1}\) and \(F_{2}\) are two cumulative distribution functions (CDFs), the Wasserstein-1 distance, \(d_{}\), is quantified by the area between \(F_{1}\) and \(F_{2}\)._

\[d_{}(F_{1},F_{2})=_{}|F_{1}(v)-F_{2}(v)|dx.\] (11)

Applying Wasserstein-1 distance (W-distance) in Eq. (11) to \(_{Q}\) and \(_{Q/P}\), we get

\[d_{}(_{Q},_{Q/P})=_{0}^{}|_{Q}(v)-_{Q/P}(v)|dv.\] (12)

As we define conformal scores as the residuals between predicted and true targets, they are always positive, so we only need to integral from 0 to \(\) in Eq. (12).

We assume \(V_{c}\) is sorted. As both \(_{Q}\) and \(_{Q/P}\) are empirical CDFs, we can approximately represent \(d_{}(_{Q},_{Q/P})\) in a discrete form as

\[d_{}(_{Q},_{Q/P})_{i=1}^{n-1}| _{Q}(v_{i})-_{Q/P}(v_{i})|(v_{i+1}-v_{i}),\ \ v_{i} V_{c}.\] (13)

Eq. (13) shows \(d_{}(_{Q},_{Q/P})\) can be estimated as a weighted summation of \(|_{Q}(v_{i})-_{Q/P}(v_{i})|\) for \(v_{i} V_{c}\{v_{n}\}\) with the corresponding weight \(v_{i+1}-v_{i}\). Also, Eq. (10) indicates that \([D]\) can be regarded as the weighted summation of \(|_{Q}(v_{i})-_{Q/P}(v_{i})|\) for \(v_{i} V_{c}\) with weight \(1/n\). The similarity between Eq. (13) and Eq. (10) allows us to apply the W-distance between the test and weighted calibration conformal score to capture expected coverage difference under concept shift.

Care needs to be taken for Eq. (13) to make this metric more robust. At first, we expect the weights \(v_{i+1}-v_{i}\) to be approximately equal, as weights in Eq. (10) are constants \(1/n\). However, some outlier calibration conformal scores have large distances from their neighbors, causing involved weights much higher than \(1/n\). These outlier scores are represented as a long tail of \(_{Q/P}\) when it converges to 1. Therefore, it is necessary to establish a partition threshold to truncate the long tail. We calculate the partition threshold

\[v_{}=\{v_{i}|_{Q/P}(v_{i}) 1-,v_{i} V_{c} \},\] (14)

which is the smallest calibration conformal score whose coverage is greater or equal to a user-defined value \(1-\). In contrast to the original \(d_{}(_{Q},_{Q/P})\) integrated on the set of real numbers, the truncated form is integrated from 0 to \(v_{}\) as

\[d_{}(_{Q},_{Q/P})=_{0}^{v_{}}|_{Q}(v) -_{Q/P}(v)|dv.\] (15)

Secondly, as the summation of weights in Eq. (10) is 1, we also need to divide each \(v_{i+1}-v_{i}\) by \(v_{}-v_{1}\). When the calibration set is large enough, it is plausible to assume the existence of a calibration sample fitting the trained model \(f\) very well, causing the smallest calibration conformal score \(v_{1} 0\). Therefore, this normalized can be formulated as

\[d_{}(_{Q},_{Q/P})=}_{0}^{v_{ }}|_{Q}(v)-_{Q/P}(v)|dv.\] (16)

A lower \(d_{}\) indicates more similarity between \(_{Q/P}\) and \(_{Q}\), thus leading to more robust conformal prediction in the test domain. As a result, NTW enables us to assess the expected coverage difference due to concept shift in Eq. (10). Experiment results in Section 5 and Appendix E show the necessity of truncation and normalization. We also prove that the W-distance between the test and weighted calibration conformal score population CDF can establish an upper bound for coverage difference under concept shift in Appendix C.

## 4 Multi-domain robust conformal prediction

If a calibration set \(S_{c}\), and a test set \(S_{t}\) are drawn from a domain \(P_{XY}\), the i.i.d. assumption is satisfied, and the coverage guarantee in Eq. (3) holds for \((x,y) S_{t}\).

The domain \(P_{XY}\) can be decomposed into \(M\) multiple domains, denoted as \(=\{e_{1},...,e_{M}\}\).

\[P_{XY}(x,y)=_{e}Q_{XY}^{(e)}(x,y)\] (17)

However, for \(e\), denote \(S_{t}^{(e)}\) a test set drawn from \(Q_{XY}^{(e)}\), then the coverage guarantee may no longer hold for \((x,y) S_{t}^{(e)}\), because joint distribution shift may occur between \(P_{XY}\) and \(Q_{XY}^{(e)}\). It indicates CP can be overconfident and underconfident for samples from different \(Q_{XY}^{(e)}\), resulting in prediction biases.

Inspired by the works of multi-domain generalization [26; 18; 19; 1], we propose **Multi-domain Robust Conformal Prediction (mRCP)** to make the coverage approach confidence in all domains, using a training set \(S^{(e)}\) from the data distribution \(Q_{XY}^{(e)}\) for \(e\) and a calibration set \(S_{c}\) from \(P_{XY}\).

The objective function of mRCP includes two components. First, for the minimization of prediction residuals, denoting \(l\) a loss function, Empirical Risk Minimization (ERM)  is incorporated as

\[_{}()=_{e}^{ (e)}()=_{e}_{(x_{i},y_{i}) S^{ (e)}}[l(f_{}(x_{i}),y_{i})].\] (18)

Secondly, we aim for robust conformal prediction on each domain during testing, seeking a low value of \([D]\) in Eq. (10) across test domains, so mRCP needs to address coverage differences due to covariate and concept shifts simultaneously. To remove coverage differences due to covariate shifts,it applies importance weighting to each domain \(e\) during training and obtains \(_{Q^{(e)}/P}\), which is the calibration conformal score CDF weighted by \(Q^{(e)}_{XY}\).

Besides, as we have a training set \(S^{(e)}\) from domain \(Q^{(e)}_{XY}\), an empirical CDF of conformal scores in \(Q^{(e)}_{XY}\) can be computed, denoted as \(^{tr}_{Q^{(e)}}\). NTW quantifies the expected coverage difference caused by concept shift between \(_{Q^{(e)}/P}\) and training conformal score CDF \(^{tr}_{Q^{(e)}}\). Combining these two components, the objective function of mRCP is

\[_{}()=_{e}^ {(e)}()+_{e}d_{}(^{tr} _{Q^{(e)}},_{Q^{(e)}/P}),\] (19)

where \(\) is a hyperparameter balancing these two parts. mRCP algorithm is shown in Algorithm 1.

```
0:\(M\) training sets \(S^{(e)}\), \(e\); one calibration set \(S_{c}\); \(N\) training epochs; model \(f_{}\); partition value \(\); loss function \(l\); penalty hyperparameter \(\).
1:for\(e\)do
2:for\((x_{i},y_{i}) S_{c}\)do
3:\(w(x_{i})=_{XY}(x_{i})}{P_{X}(x_{i})}\), \(p_{(i,e)}=)}{_{j=1}^{n}w(x_{j})}\)\(\) Covariate shift between \(Q^{(e)}_{XY}\) and \(P_{XY}\)
4:endfor
5:endfor
6:
7:for\(i=1\) to \(N\)do
8:\(V_{c}=\{v(x_{i},y_{i})|(x_{i},y_{i}) S_{c}\}\)\(\) Calibration score set
9:for\(e\)do
10:\(^{(e)}()=_{(x_{i},y_{i}) S^{(e)}}[l(f_{ }(x_{i}),y_{i})]\)\(\) ERM loss of domain \(e\)
11:\(V^{(e)}=\{v(x_{i},y_{i})|(x_{i},y_{i}) S^{(e)}\}\)\(\) Training score set of domain \(e\)
12:\(^{tr}_{Q^{(e)}}=_{v_{i} V^{(e)}}_{v_{i}}_{v_{i } v}\)\(\) Training score CDF of domain \(e\)
13:\(_{Q^{(e)}/P}(v)=_{v_{i} V_{c}}p_{(i,e)}_{v_{i}}_ {v_{i} v}\)\(\) Calibration score CDF weighted by \(Q^{(e)}_{XY}\)
14:\(v_{}=\{_{Q^{(e)}/P}(v_{i}) 1-,v_{i} V_{c}\}\)\(\) Truncation threshold
15:\(d_{}(^{tr}_{Q^{(e)}},_{Q^{(e)}/P})=}_{0}^{v_{}}|^{tr}_{Q^{(e)}}(v)-_{Q^{(e)}/ P}|dv\)\(\) NTW calculation
16:endfor
17: Optimize \(f_{}\) based on \(_{}()=_{e}^{(e)}( )+_{e}d_{}(^{tr}_{Q^{(e)}}, _{Q^{(e)}/P})\)
18:endfor ```

**Algorithm 1** Multi-domain Robust Conformal Prediction

## 5 Experiment

In this section, we validate NTW in Eq. (16) as a good indicator of expected coverage difference due to concept shift and demonstrate the effectiveness of mRCP in obtaining coverage robustness across different test domains.

### Datasets and models

We conducted experiments across various datasets: (a) the airfoil self-noise dataset ; (b) Seattle-loop , PeMSD4, PeMSD8  for traffic speed prediction; (c) US-Regions, US-States, and Japan-Prefectures  for epidemic spread forecasting. The airfoil dataset was manually altered to create three subsets demonstrating covariate and concept shifts. 24 domains for the traffic datasets were designated based on data generation hours, while epidemic dataset instances were categorized into four domains reflecting different pandemic stages. A multilayer perceptron (MLP) with a (input dimension, 64, 64, 1) architecture was utilized for all datasets. Traffic and epidemic prediction tasks were also trained on corresponding physics-informed partial differential equations (PDEs), which are the Susceptible-Infected-Recovered (SIR) model and the Reaction-Diffusion (RD) model respectively. We refer to Appendix D for detailed experiment setups.

### Experiments of NTW

For each of the experiment setups, a training set, a validation set, and a test set were sampled from each \(Q_{XY}^{(e)}\) for \(e\). One calibration set was sampled from \(P_{XY}\) which is a mixture probability distribution of \(Q_{XY}^{(e)}\) for \(e\), as shown in Eq. (17). To validate NTW is a good indicator of \([D]\), we only need to use ERM in Eq. (18) to train the model \(f_{}\), which can be an MLP or a PDE. The loss function \(l\) is the \(_{1}\) norm, as same as how we compute conformal scores.

After training, for \(e\), we first calculated the NTW between the calibration conformal score CDF weighted by \(Q_{X}^{(e)}/P_{X}\), and validation conformal score CDF of \(Q_{X}^{(e)}\). Denote the NTW of domain \(e\) as \(d_{}^{(e)}\). Then, we estimated the expected coverage difference caused by concept shift on a test domain \(e\), denoted as \(_{}[D^{(e)}]\), using the coverage difference expectation between the test and weighted calibration conformal score CDFs on a \(1-\) confidence set \(\{0.1,...,0.9\}\).

\(_{}[D^{(e)}]\) and \(d_{}^{(e)}\)should have a positive correlation for \(e\), proving NTW can capture the expected coverage difference caused by concept shift.

**Baselines:** We select six baseline metrics to validate the effectiveness of NTW. Total variation \(d_{}\), and Kullback-Leibler (KL) divergence \(d_{}\) are chosen as two typical \(f\)-divergence metrics. Expectation difference \(\) is selected since it is a widely applied generalization metric. We also measure standard, normalized, and truncated W-distance, denoted as \(d_{}\), \(d_{}\), and \(d_{}\) respectively, to demonstrate applying normalization and truncation together is necessary.

**Metric:** We apply the **Pearson coefficient** to quantify the correlations between metrics and the coverage difference expectation. It measures the linear correlation between two values by giving a value between -1 and 1 inclusive. 1,0, and -1 indicate perfect positive linear, no linear, and negative linear correlations, respectively. Therefore, if the Pearson coefficient of a metric is **higher**, this metric can indicate the expected coverage difference **better**. We provide a detailed definition of the Pearson coefficient in Appendix E.

**Results:** Table 1 illustrates the Pearson coefficients between NTW and the coverage difference expectation among seven datasets and different models, compared with the other six baseline metrics. We highlight that NTW keeps holding the largest Pearson coefficient among all experiment setups, which means the proposed metric can keep indicating the coverage difference expectation. Specifically, the coefficients of total variation \(d_{}\) and KL divergence \(d_{}\) fluctuate along experiments, meaning that they can not truly indicate the coverage difference expectation. \(\) can not capture the coverage difference expectation either. Lastly, due to the lack of robustness to score scales and outliers, standard, normalized, and truncated W-distance, denoted as \(d_{}\), \(d_{}\), and \(d_{}\) respectively, can

  Dataset & Model & \(_{}\) & \(_{}\) & \(_{}\) & \(\) & \(_{}\) & \(_{}\) & \(_{}\) \\  Airfoil & MLP & **1.000** & -0.356 & -0.545 & 0.891 & 0.878 & 0.951 & 0.967 \\  Seattle- & MLP & **0.971** & 0.461 & 0.054 & 0.781 & 0.759 & 0.762 & 0.765 \\  loop & PDE & **0.996** & 0.890 & 0.058 & 0.897 & 0.893 & 0.909 & 0.921 \\  PeMSD4 & MLP & **0.992** & 0.846 & -0.390 & 0.926 & 0.915 & 0.964 & 0.941 \\   & PDE & **0.986** & 0.682 & -0.068 & 0.858 & 0.872 & 0.928 & 0.858 \\  PeMSD8 & MLP & **0.905** & 0.397 & -0.089 & 0.333 & 0.267 & 0.371 & 0.529 \\   & PDE & **0.827** & 0.129 & -0.114 & 0.253 & 0.118 & 0.141 & 0.527 \\  US- & MLP & **0.999** & 0.966 & 0.965 & 0.872 & 0.885 & 0.912 & 0.931 \\  States & PDE & **0.999** & 0.966 & 0.964 & 0.817 & 0.848 & 0.890 & 0.899 \\  US- & MLP & **0.636** & -0.530 & -0.338 & -0.205 & -0.308 & -0.352 & -0.405 \\  Regions & PDE & **0.709** & 0.308 & 0.350 & 0.484 & 0.355 & 0.322 & 0.137 \\  Japan- & MLP & **0.996** & 0.986 & 0.988 & 0.943 & 0.948 & 0.954 & 0.950 \\  Prefectures & PDE & **0.997** & 0.983 & 0.981 & 0.907 & 0.918 & 0.935 & 0.924 \\  Average & & **0.905** & 0.574 & 0.325 & 0.619 & 0.583 & 0.607 & 0.629 \\  Standard Deviation & **0.128** & 0.474 & 0.562 & 0.368 & 0.420 & 0.437 & 0.428 \\  

Table 1: Pearson coefficients between metrics and coverage difference expectation under concept shiftnot indicate the coverage difference expectation as well as \(d_{}\). It also displays the average and standard deviation of the Pearson coefficient of the proposed NTW and six baselines. NTW not only has the highest average Pearson coefficient but also has the lowest standard deviation, which means the correlation between NTW and the coverage difference expectation caused by concept shift is very stable. In Figure 3 and Figure 4, we also visually show the correlation between the expected coverage difference under concept shift and each metric. We refer to Appendix E for detailed analysis. This observation suggests the potential of incorporating NTW in the training process, leading to the development of the mRCP approach. By applying the NTW metric, mRCP aims to enhance coverage robustness in test domains.

### Experiments of mRCP

Since we prove NTW can assess expected coverage difference under concept shift effectively, mRCP is designed to minimize it during training. In this case, validation sets are unnecessary, and we only draw training, and test sets from \(Q_{XY}^{(e)}\). Again, we draw one calibration set from \(P_{XY}\). The model \(f_{}\) can also be an MLP or PDE based on different experiment setups. The loss function \(l\) is the \(_{1}\) norm. We implement mRCP according to Algorithm 1.

**Baselines:** Two methods of optimization with out-of-distribution data are selected as baselines. **DRO** in Eq. (20) by  follows the minimax principle to reduce the highest \(^{(e)}\) to obtain fair prediction among test distributions. On the other hand, **V-REx** in Eq. (21), introduced by , focuses on reducing the variance of \(^{(e)}\) to obtain fairness. As we include importance weighting in mRCP, we do not take it as a baseline, and the effectiveness of importance weighting is discussed in Section 6.

\[_{}()=_{e}^{(e)}.\] (20)

\[_{}()=_{e}^{(e)}+(^{(e)}\,|\,e).\] (21)

**Metric:** Denote \(_{}^{}[_{}[D^{(e)}]]\) the **expectation of coverage difference** over confidence levels and test domains and \(_{}^{}[^{(e)}]\) the **expectation of prediction residual** over test domains. The two expectations become **smaller** means the algorithm's performance is **better**. Both values are normalized by the corresponding results from the same experiment setup trained by ERM. Changing the weight \(\) in Eq. (19) will draw a Pareto front, thus we want the Pareto front closer to the origin. Since V-REX is also controlled by a hyperparameter, we draw Pareto fronts for it as well.

**Result:** Figure 2 displays the Pareto fronts for mRCP, DRO, and V-REx, highlighting the trade-offs between prediction residual and coverage difference expectation across different models and datasets. Figure 2, (a) shows the results for the airfoil self-noise dataset when trained with a Multilayer Perceptron (MLP) model. The mRCP method achieves a more favorable Pareto front compared to V-REx, indicating a better balance between prediction residual and coverage difference expectation. Additionally, mRCP attains a lower normalized coverage difference expectation than DRO at a comparable level of the prediction residual. In Figure 2, (b), we observe the experiment results on the epidemic spread prediction task using three epidemic datasets. With the same MLP architecture, mRCP delivers superior Pareto fronts relative to the baselines. When employing the epidemic PDE, the SIR model only has two trainable parameters, so their data points can not compose Pareto curves due to the model's limited flexibility. Thus, we show the average of these points. Despite this limitation, mRCP maintains its advantage over the baseline methods. Figure 2, (c) and (d) present results from the traffic prediction task on three different traffic datasets. Here, the Pareto curves for both the MLP and the reaction-diffusion (RD) PDE model are well-defined, because RD model with six parameters, offers greater adaptability, allowing for clearer Pareto fronts. Overall, Figure 2 collectively indicates that mRCP consistently achieves lower coverage difference expectations without compromising prediction residual as significantly as DRO and V-REx in different tasks and datasets.

## 6 Discussion

**mRCP can distinguish coverage differences under concept shift and covariate shift.** A notable feature of the mRCP Pareto curves depicted in Figure 2 is their results when \(\) is small, which are not at \(_{}^{}[_{}[D^{(e)}]]=1\), unlike the Pareto curves of V-REx. This is because, during training, mRCPhas considered the coverage difference under covariate shift by applying importance weighting to calibration conformal score CDF. Consequently, as \(\) in Eq. (19) increases, the NTW term is only trained to mitigate the coverage difference under the concept shift, as shown in Figure 2,(a).

**DRO and V-REx are defeated because of improper selection of optimization metrics.** Examining Eq. (20) and Eq. (21), we can see both baselines aim to promote fairness by equalizing the expected losses across different domains. As the loss function is \(_{1}\) norm, which is identical to how conformal scores are calculated, the experiment results of \(\) in the last row of Figure 3 show this metric is ineffective in capturing the coverage difference due to concept shift.

**Nonetheless, mRCP's limitations arise from the inherent challenges associated with penalty-based optimization algorithms.** Whether it is mRCP or V-REx, penalty-based optimization algorithms necessitate a model with a high capacity for fitting complex patterns. For instance, in Figure 2, (b), the Pareto curves are not discernible when predictions are derived from an epidemic PDE (SIR model) with only two adjustable parameters. In contrast, as shown in Figure 2, (d), the traffic PDE (RD model) demonstrates greater flexibility and adaptability with six tunable parameters, exhibiting distinct Pareto curves.

## 7 Conclusion

This study begins by decomposing the coverage difference caused by covariate and concept shifts. We then introduce the Normalized Truncated Wasserstein distance (NTW) as a metric for capturing coverage difference expectation under concept shift by comparing the test and weighted calibration conformal score CDFs. This metric can indicate the discrepancy position in calibration and test score distributions. Normalization and truncation make the metric score scales and outliers. Finally, we develop an end-to-end algorithm called Multi-domain Robust Conformal Prediction (mRCP) that incorporates NTW during training, allowing coverage to approach confidence in all test domains.

Figure 2: **Pareto fronts of Multi-domain Robust Conformal Prediction(mRCP), compared with DRO and V-REx:** Experimental results of (a) airfoil self-noise example, (b) epidemic spread prediction, and (c) (d) traffic speed prediction. mRCP always reaches a smaller coverage difference expectation than DRO and V-REx with less increase in prediction residual. Red boxes in (b) are zoomed-in areas. Shadow areas and error bars indicate the standard error.