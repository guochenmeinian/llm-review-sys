# Stabilizing Zero-Shot Prediction: A Novel Antidote to Forgetting in Continual Vision-Language Tasks

Zijian Gao\({}^{1,2}\), Xingxing Zhang\({}^{3}\), Kele Xu\({}^{1,2}\), Xinjun Mao\({}^{1,2}\), Huaimin Wang\({}^{1,2}\)

\({}^{1}\)School of Computer, National University of Defense Technology, Changsha, 410000, China.

\({}^{2}\)State Key Laboratory of Complex & Critical Software Environment, Changsha, 410000, China.

\({}^{3}\)School of Computer Science, Tsinghua University, Beijing, 100049, China.

{gaozijian19, xukeelele, xjmao, hmwang}@nudt.edu.cn, xxzhang1993@gmail.com

Corresponding author. \(\) Equal Contribution.

###### Abstract

Continual learning (CL) empowers pre-trained vision-language (VL) models to efficiently adapt to a sequence of downstream tasks. However, these models often encounter challenges in retaining previously acquired skills due to parameter shifts and limited access to historical data. In response, recent efforts focus on devising specific frameworks and various replay strategies, striving for a typical learning-forgetting trade-off. Surprisingly, both our empirical research and theoretical analysis demonstrate that the stability of the model in consecutive zero-shot predictions serves as _a reliable indicator_ of its anti-forgetting capabilities for previously learned tasks. Motivated by these insights, we develop a novel replay-free CL method named ZAF (Zero-shot Antidote to Forgetting), which preserves acquired knowledge through a zero-shot stability regularization applied to wild data in a plug-and-play manner. To enhance efficiency in adapting to new tasks and seamlessly access historical models, we introduce a parameter-efficient EMA-LoRA neural architecture based on the Exponential Moving Average (EMA). ZAF utilizes new data for low-rank adaptation (LoRA), complemented by a zero-shot antidote on wild data, effectively decoupling learning from forgetting. Our extensive experiments demonstrate ZAF's superior performance and robustness in pre-trained models across various continual VL concept learning tasks, achieving leads of up to 3.70%, 4.82%, and 4.38%, along with at least a 10x acceleration in training speed on three benchmarks, respectively. Additionally, our zero-shot antidote significantly reduces forgetting in existing models by at least 6.37%. Our code is available at https://github.com/Zi-Jian-Gao/Stabilizing-Zero-Shot-Prediction-ZAF.

## 1 Introduction

In the rapidly evolving landscape of artificial intelligence, pre-trained models have become fundamental to achieving state-of-the-art results across a myriad of applications . Recently, large vision-language (VL) models have provided remarkable predictions on downstream tasks without any training examples . However, these models are typically trained on static datasets, which may not capture the continuously evolving variety and complexity of real-world data. As new concepts emerge and existing categories expand, the static nature of these pre-trained models can lead to diminished performance over time. For example, the CLIP model  achieves an accuracy of less than 60% on the MNIST dataset (i.e., a performance significantly lower than that of a conventionally trained CNN ) . To bridge this gap, continual learning (CL) has emerged as a vital methodology, making learning new knowledge a lifelong process for the pre-trained VL model.

However, the adaptation process, whether involving comprehensive fine-tuning of the entire pre-trained model [7; 49; 3] or parameter-efficient continual fine-tuning [33; 45; 29], inevitably undergo incremental parameter shifts. This presents a significant challenge in CL, where VL models forget historical knowledge as they adapt to new tasks [41; 45]. Additionally, the high computational costs required to integrate new data with old data, combined with often limited access to complete previous datasets, further exacerbate the severity of forgetting [47; 45]. Thus far, most CL research has focused on mitigating the forgetting of class information from previously learned images, a process known as class incremental learning . Multimodal tasks such as visual question answering (VQA) and natural language visual reasoning (NLVR) have received less attention. Building upon the foundations laid by unimodal CL efforts, recent initiatives have aimed to mitigate forgetting from parameter shifts by developing robust continual VL learning frameworks, including Mixture-of-Experts , weight consolidation , and Layered-LoRA . Furthermore, a growing body of work is dedicated to developing anti-forgetting strategies, such as knowledge distillation  and rectification . Meanwhile, latent replay mechanisms, including adversarial pseudo-replay  and prototype learning , have shown effectiveness in retaining historical knowledge, albeit with significant additional memory and computational costs. All these methods, however, struggle with the inherent trade-off between acquiring new knowledge and preserving historical knowledge.

In this work, we aim to decouple the learning-forgetting dynamic, striving for _a win-win outcome_ -- enhancing anti-forgetting capabilities without interfering with the learning process. Our extensive empirical investigation reveals _uniform patterns_ in the model's performance across learned and future VL tasks. Theoretically, we find that the generalization errors for old and new tasks are nearly identical. Motivated by these insights, particularly the observed stability of zero-shot predictions that reflects the stability required for old tasks (i.e., anti-forgetting capabilities), we develop a novel CL approach. This method, named **ZAF** (Zero-shot Antidote to Forgetting), incorporates a zero-shot prediction stability regularization term within our EMA-LoRA architecture. ZAF employs two low-rank adapters: one facilitates the learning of new tasks and ensures stable zero-shot predictions on wild data; the other, an Exponential Moving Average (EMA)-based adapter, preserves historical knowledge and utilizes it for zero-shot supervision. The synergistic interaction between these adapters effectively decouples learning from forgetting, thereby enhancing overall CL performance.

Our contributions include: (1) We conduct a comprehensive empirical and theoretical study on continual learning for pre-trained VL models, establishing zero-shot stability as a reliable indicator of anti-forgetting capabilities; (2) Inspired by this finding, we develop a plug-and-play zero-shot antidote that enhances models' anti-forgetting across various CL methods, through a zero-shot prediction stability regularization on wild data; (3) We introduce an innovative replay-free CL method (termed ZAF), implemented within a parameter-efficient EMA-LoRA architecture, effectively decoupling learning from forgetting; (4) Across various continual VL benchmarks and pre-trained models, our approach achieves state-of-the-art performance and significantly reduced complexity.

## 2 Related Work

Continual Learning (CL):Continual learning aims to train a single model capable of incrementally updating its knowledge with a new sequence of tasks while preserving historical knowledge [41; 39]. However, due to parameter shift and limited access to historical data, the primary challenge in CL is forgetting previously learned tasks over time . A plethora of strategies have been explored to address this issue [32; 21; 18; 6; 12], including selective stabilization of network weights, replay of a few old training samples, construction of task-specific parameters, etc. Despite advancements in the unimodal community, such as in image classification, multimodal tasks have received comparatively less attention. This gap highlights the complexity and emerging interest in multimodal settings [9; 4; 8; 24; 35; 47]. In this context, VQACL  introduced a continual VQA task leveraging a latent replay strategy. RATT  focused on continual image captioning through weight regularization and knowledge distillation. Distinctively, ConStruct-VL  pursued continual adaptation to fine-grained Structured VL Concepts reasoning skills without fixed task boundaries. This is particularly applicable to real-world scenarios, and thus, we choose this VL task as _a case study_ in our experiments.

It is noteworthy that several studies closely related to ours discuss both zero-shot and continual learning capabilities [49; 45; 46]. However, their objectives differ fundamentally from ours. These studies, including ZSCL  and MoE-Adapters , aim to preserve the zero-shot transfer ability inherent in pre-trained VL models during continual adaptation for sequentially arriving tasks, even though these capabilities may be inherently limited. In contrast, our research is exclusively focused on enhancing the anti-forgetting capabilities in downstream CL tasks with our zero-shot antidote.

Vision-and-Language (VL) Pre-training:VL pre-training aims to enhance the performance of downstream VL tasks by pre-training models on large-scale image-text data. The effectiveness of these models largely depends on their representational capacity, including factors like data quality and model architecture, as well as the similarity between pre-training tasks and downstream applications. Generally, higher data quality and greater task similarity lead to stronger generalizability. However, due to the high costs of human annotation, most methods leverage noisy image-text pairs sourced from the web, which are a sub-optimal form of supervision (e.g., CLIP  and ALIGN ). A novel dataset bootstrapping method, Captioning and Filtering (CapFilt), was introduced to utilize web datasets more effectively . Furthermore, most existing pre-trained models excel either in understanding-based tasks like image-text retrieval [30; 20] or generation-based tasks such as image captioning [26; 11]. The primary challenge lies in designing model architectures capable of performing diverse tasks . In response, the 'BLIP' framework was proposed, providing flexible transfer ability to both tasks . Specifically, its image transformer is initialized from ViT pre-trained on ImageNet [37; 5], and the text transformer originates from BERT . An important variant of BLIP, enhanced with CapFilt to boost performance, is termed 'BLIP w/ CapFilt-L' . Additionally, to enable reasoning on downstream tasks, 'BLIP' was further fine-tuned on the NLVR2 dataset  with a more computationally efficient architecture, referred to as 'BLIP w/ NLVR' .

In this work, we primarily focus on BLIP and its two variants, unlike the closely related studies [49; 45; 46] that utilize CLIP. Our choice is driven by the nature of our case study, which involves reasoning tasks as opposed to classification tasks typically associated with CLIP.

## 3 Preliminary Analysis

In this section, we first define the problem of continual VL learning and then examine the correlation between the model's anti-forgetting capabilities and zero-shot stability through an empirical study.

### Formulation of Continual Vision-Language (VL) Learning

In this work, we focus on adapting pre-trained VL models to a sequence of newly arriving reasoning tasks, denoted as \(\{^{1},,^{n}\}\). Each task \(^{t}\) involves a set of image-text pairs \((I,T)\), with a binary ground-truth function \((T,I)\{(0,1),(1,0)\}\) determining whether the text precisely describes the image content. The primary distinction among these tasks lies in the types of cognitive skills they aim to develop. Notably, traditional CL methods, which require identifying the specific task to which input belongs during inference, are impractical here. This is due to the challenge of discerning the required skills from free-form text and the likelihood that multiple skills may be simultaneously necessary. Given the constraints of privacy in practical applications, we adopt a strict policy of not retaining any task-specific data between training sessions. This leads us to a _continual_, _replay-free_ setting without task identifiers, which is precisely the focus of our study.

### Empirical Study of Anti-Forgetting and Zero-Shot Performance

To investigate the correlation between a model's anti-forgetting capabilities and its stability in zero-shot predictions, we conducted an empirical study using the Structured VL Concepts (SVLC) learning multimodal benchmark . This benchmark utilizes two widely recognized public VL datasets: Visual Genome (VG)  and Visual Attributes in the Wild (VAW) , adhering to protocols outlined by the VL-Checklist . The '**7 Task VG+VAW**' benchmark, derived from both VG and VAW datasets, includes seven distinct concepts designed to assess a VL model's understanding of various relationships, including spatial and inter-object transitive actions, as well as attributes such as size, color, material, and intransitive single-object actions. Examples of these concepts are depicted in Fig. 5. Additionally, the '**7 Task VG**' benchmark is solely based on the VG dataset, and the '**5 Task VAW**' benchmark focuses on five attributes and object state concepts from the VAW dataset. We adhere strictly to the dataset splits outlined in ConStruct-VL .

We rigorously evaluate the official implementations of all baseline models to ensure a fair comparison. Our evaluations largely adhere to the training regimes established by LoRA , MoE-Adapters ,Layered-LoRA , and ConStruct-VL . Since MoE-Adapters  was originally designed for both pre-trained and downstream classification tasks requiring task-id inference, we adapt its implementation to include a single task-dependent router for our needs. Given the focus on reasoning tasks, we opt for 'BLIP w/ NLVR' as our pre-trained model, chosen for its robust representation capabilities and adequate zero-shot transfer ability.

Fig. 1 illustrates a heatmap that depicts the performance of various CL methods across two benchmarks (see Appendix C.1 for additional benchmarks). In this heatmap, rows represent training steps, and \(A_{ij}\) denotes the prediction accuracy on task \(^{j}\) after training on task \(^{i}\). It is important to note that traditional CL methods typically focus only on the _lower triangular matrix_ of results, which represents performance on previously learned tasks. However, our analysis extends to include zero-shot predictions on future tasks, which are represented in the _upper triangular matrix_. By comparing these results, we observe that larger average values and less fluctuation in the red area, which represents strong zero-shot stability, typically correspond to similar patterns in the blue area, indicative of strong anti-forgetting capabilities, without adversely affecting the yellow area, where learning new tasks occurs. This observation suggests that a model's stability in zero-shot predictions can reflect its anti-forgetting capabilities. Our findings further suggest that by systematically stabilizing zero-shot predictions during continual learning, we can significantly enhance the model's ability to retain historical knowledge without compromising the acquisition of new information.

## 4 Theoretical Foundation and Our Approach

In this section, we first provide a theoretical analysis to substantiate our empirical findings. Subsequently, we present an innovative CL approach specifically tailored for pre-trained VL models.

### Continual Vision-Language Learning Objective

In the continual learning of sequentially arriving VL tasks \(^{t}=\{T,I\}\), the model is designed to align with the binary ground-truth function \((T,I)\) that evaluates whether the text \(T\) precisely describes the image content \(I\). In this case study, we employ BLIP-based encoders to learn discriminative deep embeddings. Specifically, let \(f_{}\) represent the image encoder and \(f_{}\) the text encoder, while \(h_{}\) and \(h_{}\) serve as the cross-attention decoder and the final binary classifier, respectively. The predictive modeling process within the BLIP framework for learning task \(^{t}\) is structured as follows:

\[P^{t}(T,I)=h_{^{t}}(h_{^{t}}(f_{^{t}}(I),f_{^{t}}(T))),\] (1)

where \(P^{t}(T,I)\) denotes the prediction probability made by the current model \(^{t}=\{^{t},^{t},^{t},^{t}\}\).

To sequentially fine-tune the BLIP model for downstream tasks, we employ a cross-entropy loss term, \(_{}(P^{t}(T,I),(T,I))\). This loss measures the discrepancy between the current prediction, \(P^{t}()\), and the ground truth, \(()\), ensuring that the model progressively acquires new information.

Figure 1: Empirical study of anti-forgetting, learning, and zero-shot performance in CL methods.

Let \(}_{1:t}\) denote the empirical errors on the \(t\) observed tasks, and \(}_{t}\) represent the empirical error on the current task \(^{t}\). For \(k\{t+1,,n\}\) and \(s\{1,,t-1\}\), \(_{k}\) and \(_{s}\) denote the generalization errors on the future task \(^{k}\) and the old task \(^{s}\), respectively. Inspired by the PAC-Bayes theory  and previous work in domain generalization , we present the upper bounds of these two errors under the continual VL scenario (see Appendix A for the detailed proof).

**Proposition 1**: _For continual learning with pre-trained VL models, let \(^{t}\) denote a solution of the continually learned tasks \(^{1},,^{t}\). In particular, \(^{t}=*{arg\,min}_{||-^{t-1}||_ {2}}}_{t}()\) where \(||-^{t-1}||_{2}\) represents the weight vectors for continual tasks are only minor variations. For any \((0,1)\) with probability at least \(1-\):_

\[ s\{1,,t-1\},_{s}(^{t})}_{1:t}(^{t})+_{i=1}^{t}(_{i},_{i})+/d)]+(1/ )}{2}},\] (2)

\[ k\{t+1,,n\},_{k}(^{t})}_{1:t}(^{t})+_{i=1}^{t}(_{i},_{i})+/d)]+(1/ )}{2}},\] (3)

_where \((_{i},_{j}):=2_{h}| _{_{i}}(I(h))-_{_{j}}(I(h))|\) defines the \(\)-divergence between the distributions for tasks \(_{i}\) and \(_{j}\), with \(I(h)\) being the characteristic function. Here, \(\) denotes the harmonic mean of the training example sizes for the \(t\) observed tasks, and \(d\) represents the VC dimension of the parameter space._

Proposition 1 demonstrates that the model \(^{t}\) has consistent upper bounds on the generalization errors for both previously learned and future tasks. These abilities are significantly influenced by three key factors: _empirical error of continual tasks_, _discrepancy between task distributions_, and _complexity of the parameter space_. Such consistency underscores that the model's capabilities in **zero-shot prediction** can reliably indicate its **anti-forgetting** capabilities. In practice, maintaining this consistency necessitates the implementation of carefully designed regularization techniques.

Motivated by these insights, we develop a zero-shot regularization antidote to mitigate forgetting. Considering the unpredictable nature and broad array of future tasks, we initially introduce an unlabeled wild dataset, denoted as \(_{}=\{T_{},I_{}\}\), to assess the zero-shot capabilities of the continually learned VL model. It is crucial to note that both the text \(T_{}\) and the images \(I_{}\) are entirely unpaired and unlabeled2. Additionally, the substantial number of wild examples in \(_{}\) are distinct and separate from the actual downstream tasks \(\{^{i}\}_{i=1}^{n}\). Fig. 8 illustrates some wild examples.

To stabilize zero-shot predictions, we develop a novel loss term, \(_{}(P^{t}(T_{},I_{}),}(T_{},I_{}))\), where \(}()\) is the prediction probability derived from the model in the previous training step and used for zero-shot supervision. The formulation of our method's loss function is outlined below:

\[=_{}(P^{t}(T,I),(T,I))+_{ {ZS}}(P^{t}(T_{},I_{}),}(I_{},I _{})),\] (4)

Figure 2: Comparison of training and inference procedures between traditional and our CL methods.

where \(_{}=||P^{t}(T_{},I_{})-}(T_ {},I_{})||_{1}\) in our experiments to restrain its fluctuations.

### EMA-LoRA Architecture

To achieve the objective outlined above, rather than conducting comprehensive fine-tuning of the entire pre-trained model \(\{^{t},^{t},^{t},^{t}\}\) as defined in Eq. (1), we develop a parameter-efficient approach using an EMA-LoRA architecture based on BLIP. As depicted in Fig. 2(b), for efficient adaptation to new tasks, we first integrate LoRA adapters into all layers of the image encoder \(f_{}\), text encoders \(f_{}\), and the cross-attention decode \(h_{}\), which are updated during the training of task \(^{t}\). Specifically, each of \(f_{}\), \(f_{}\), and \(h_{}\) consists of a combination of non-parametric functions - referred to here as data norms, such as LayerNorm, which remain frozen, and two types of parametric functions, namely linear and embedding functions. Typically, linear and embedding functions are parameterized by a weight matrix \(\), which is optimized at every iteration by adding residuals to the weights from the initial pre-trained VL model \(^{0}\). Formally, this is represented as \(=^{0}+\), where \(\) and \(\) are learnable low-rank matrices of dimensions \(m r\) and \(r l\) respectively, with \(m l\) being the dimensions of \(^{0}\). During the training of task \(^{t}\), only the current task's LoRA adapters \(\{,\}\) are learned, and predictions \(P^{t}()\) are made using \(\). This strategy not only enhances the adaptability of the model to new tasks but also maintains a balance between efficiency and performance.

As demonstrated in Eq. (4), accessing previous models is essential for predicting wild data (i.e., \(()\)). To facilitate this, we implement the Exponential Moving Average (EMA) on the aforementioned LoRA adapters (see Fig.2(b)). This approach allows for memory-efficient access to previous models while learning new tasks. Specifically, the EMA process is conducted at the end of every epoch:

\[}}+(1-),^{t}=^{0}+ },\] (5)

where the hyperparameter \(\) plays a crucial role in updating the model parameters (refer to Fig. 3 for its sensitivity analysis). Importantly, \(^{t}\) represents the final model used for inference after continual learning. The weights \(}\) are updated following the completion of the first downstream task, at which point there is no risk of forgetting, thereby eliminating the need for the zero-shot antidote \(_{}\). For subsequent tasks, predictions on wild data using \(^{t}()\) are made with \(^{t-1}\).

Algorithm 1 provides a detailed view of our training algorithm for the Zero-shot Antidote to Forgetting (ZAF) framework. Key highlights of the proposed ZAF include: (1) Learning vs. Forgetting: As illustrated in Fig. 2(a) and (b), traditional CL frameworks, such as those incorporating knowledge distillation, often struggle with a trade-off between learning and forgetting. This trade-off typically manifests in the balance of \(_{}\) and \(_{}\), where the LoRA adapter is optimized to align with both old and new knowledge using current data: \(_{}(P^{t}(^{t}),(^{t }))+_{}(P^{t}(^{t}),P^{t-1}(^{t}))\). Our approach introduces a zero-shot antidote using wild data to effectively separate the learning processes from the problem of forgetting: \(_{}(P^{t}(^{t}),(^{ t}))+_{}(P^{t}(_{}),^{t}( _{}))\). (2) Privacy Protections: Unlike conventional methods that require replaying old task data, our framework utilizes generated wild data to prevent forgetting, adhering to privacy constraints. (3) Innovative Mixture Strategy: Deviating from the typical _spatial_ Mixture of Experts (MoE) and Layered-LoRA architectures, our model employs a _temporal_ mixture strategy through EMA. This approach not only conserves memory but also integrates downstream tasks incrementally and effectively. (4) Efficiency in Resource Usage: The EMA-LoRA architecture involves only two low-rank adapters, which are computationally efficient, and also ensure \(||-^{t-1}||_{2}\), as established in our Proposition 1.

## 5 Experiment

Benchmark:We utilize the '7 Task VG+VAW', '7 Task VG', and '5 Task VAW' benchmarks for our empirical studies as discussed in Sec. 3.2. We use the \(object\ state attr.\ action attr.\ size rel.\ spatial attr.\ material rel.\ action attr.\ color\) 7-task sequence in the 7 Task VG+VAW benchmark and 7 Task VG benchmark, and the \(object\ state attr.\ action attr.\ size attr.\ material attr.\ color\) 5-task sequence in the 5 Task VAW benchmark. Given the reasoning task scenario, our primary focus is on the original 'BLIP' model and its two variants: 'BLIP w/ CapFilt-L' and 'BLIP w/ NLVR'. Notably, the latter variant, which is fine-tuned on the NLVR2 dataset, exhibits enhanced zero-shot transfer ability.

Model Architecture:All experiments begin with the BLIP model architectures and its pre-trained weights. The image encoder, denoted as \(f_{}\), utilizes ViTB/16, and the text encoder \(f_{}\) is a BERT with a 12-layer encoder and 768 hidden size. The decoder \(h_{}\) extends \(f_{}\), incorporating cross-attention layers between every two self-attention layers, with each receiving encoded image tokens as additional input. Our binary classifier \(h_{}\) is a 2-layer MLP with a hidden size of 768. For all baselines, we adhere to the implementation protocols established in ConStruct-VL , a pioneering work in SVLC scenario that has demonstrated exceptional performance. We maintain a shared binary classifier \(h_{}\) across all task models. In practice, for the BLIP w/ NLVR model, \(h_{}\) is frozen across all tasks. For the BLIP and BLIP w/ CapFilt-L models, \(h_{}\) is trained only for the first task \(^{1}\) and then remains frozen for all subsequent tasks.

Baseline and Implementation:Due to the inadequate performance of unimodal CL methods in multimodal scenarios, as highlighted by , our analysis primarily focuses on frameworks and multimodal CL methods designed for VL scenarios. We include only one representative unimodal method, LwF , which mitigates forgetting through knowledge distillation for comparison. Detailed comparisons with other unimodal CL methods are provided in Appendix C.6. Among the methods compared, Continual-FT  and LoRA  train the VL model continuously without incorporating anti-forgetting procedures. Layered-LoRA  and MoE-Adapters  are viewed as representative continual VL learning frameworks. Furthermore, we compare our ZAF with ZSCL , which aims to preserve the inherent zero-shot capability of the pre-trained model, and ConStruct-VL , which employs an expensive pseudo-replay strategy involving adversarial attacks on every sample. Following the implementation from previous work , specifically, we adapt WD 0.05, an initial LR of 1.25e-3, a cosine scheduler, and a maximum of 12 training epochs in all experiments. For all low-rank adapters, the rank \(r\) is set to 16. The only hyperparameter, \(\), of our method is fixed at 0.85. Please refer to Appendix B for more implementation details.

Evaluation Metrics:We utilize three widely-recognized evaluation metrics for continual learning - Final Average Accuracy (FAA), Cumulative Average Accuracy (CAA), and Final Forgetting Measure (FFM) as detailed in . We define the accuracy on the task \(^{j}\) after learning the task \(^{i}\) as \(A_{ij}\). The average accuracy after learning task \(^{i}\) is denoted as \(AA_{i}=_{j=1}^{i}A_{ij}\). Upon completing all \(n\) tasks, we report \(=AA_{n}\), \(=_{i=1}^{n}AA_{i}\), and \(=_{j=1}^{n-1}_{t\{1,,n-1\}}(A_{tj}- A_{nj})\). The FAA is a critical metric highlighting performance discrepancies between CL methods and joint learning. The CAA provides a comprehensive view of overall historical performance, and the FFM quantifies the model's capability to mitigate forgetting.

Overall Performance:As shown in Table 1, our ZAF consistently achieves the highest FAA, CAA, and lowest FFM, significantly outperforming competitors across three benchmarks with different pre-trained models. ZSCL , despite employing weight ensemble and knowledge distillation, exhibits inadequate learning on the challenging 7 Task VG+VAW and 7 Task VG benchmarks, resulting in declined performance. Similarly, MoE-Adapters, like Continual-FT  and LoRA , which lack anti-forgetting mechanisms, exhibit significant forgetting as the number of tasks increases. In contrast, both the Layered-LoRA architecture , with its task-specific parameter isolation strategy, and LwF , with only a knowledge distillation strategy, show improved performance. Notably, only ConStruct-VL  approaches the performance of our method but requires computationally intensive training involving adversarial attacks and utilizing all historical models for pseudo-sample generation. Despite this, ZAF shows substantial advantages, leading by **3.70**%, **4.82**%, and **4.38**% in FAA under the BLIP w/ NLVR model. Meanwhile, our method demonstrates a narrow performance gap compared to Joint Learning - the pinnacle of continual learning. The smaller this gap, the greater the challenge for CL methods to bridge it. Importantly, despite the inherent limitations in the zero-shot capabilities of some pre-trained VL models (e.g., BLIP and BLIP w/ CapFilt-L), our zero-shot antidote still effectively mitigates forgetting, underscoring its utility across a wide range of CL scenarios and pre-trained models.

Complexity Analysis:Table 2 presents the model size, training parameter count, and training times of various CL methods across three benchmarks. All comparisons are conducted on 7 NVIDIA GeForce RTX 3090 GPUs using 'BLIP w/NLVR', which serves as the standard configuration for subsequent experiments. As reflected in the table, ConStruct-VL exhibits exceptionally high training times, especially in the 7 Task VG+VAW scenario, attributed to the utilization of all historical models

    &  &  &  &  \\  & & **EAA** (\(\)) & CAA (\(\)) & FFM (\(\)) & **EAA** (\(\)) & CAA (\(\)) & FFM (\(\)) & **FAA** (\(\)) & CAA (\(\)) & FFM (\(\)) \\   & Joint Learning & 91.90 & - & - & 95.27 & - & - & 92.60 & - & - \\  & Continual-FT  & 65.21 & 73.98 & 30.32 & 63.91 & 73.97 & 31.34 & 67.07 & 78.35 & 28.14 \\  & LoRA  & 75.39 & 76.59 & 20.73 & 69.16 & 75.89 & 28.20 & 71.54 & 79.07 & 22.48 \\  & Layered-LoRA  & 76.68 & 78.51 & 18.96 & 70.13 & 79.66 & 28.08 & 83.77 & 83.47 & 9.20 \\  & LuF  & 70.93 & 73.62 & 26.26 & 69.62 & 77.05 & 29.05 & 80.07 & 84.32 & 14.93 \\  & ZSCL  & 66.87 & 66.00 & 19.08 & 67.32 & 75.65 & 27.45 & 66.53 & 75.05 & 25.13 \\  & MoE-Adapters  & 69.90 & 74.47 & 27.11 & 64.50 & 77.18 & 34.98 & 80.09 & 83.02 & 14.36 \\  & ConStruct-VL  & 87.27 & 86.98 & 6.14 & 89.01 & 91.87 & 5.80 & 83.73 & 86.34 & 6.47 \\   & ZAF (Ours) & **90.58** & **89.45** & **3.32** & **92.49** & **92.39** & **1.97** & **89.13** & **90.03** & **3.93** \\  & _Inprovement_ & **2.78** & **2.47** & **2.82** & **3.48** & **0.52** & **3.83** & **5.40** & **3.69** & **2.54** \\   & Joint Learning & 93.72 & - & - & 95.31 & - & - & 92.90 & - & - \\  & Continual-FT  & 67.20 & 74.85 & 28.02 & 70.05 & 75.17 & 23.99 & 71.95 & 79.31 & 22.18 \\  & LoRA  & 71.97 & 60.57 & 25.27 & 69.97 & 77.52 & 28.49 & 79.66 & 82.36 & 13.78 \\  & Layered-LoRA  & 76.66 & 76.72 & 19.20 & 70.43 & 78.00 & 27.16 & 81.89 & 82.66 & 11.18 \\  & LuF  & 73.39 & 75.42 & 23.81 & 70.02 & 77.62 & 28.47 & 79.83 & 84.21 & 15.63 \\  & ZSCL  & 62.90 & 64.29 & 22.06 & 67.12 & 76.21 & 27.14 & 68.13 & 77.15 & 24.67 \\  & MoE-Adapters  & 69.76 & 73.29 & 27.34 & 63.99 & 76.19 & 35.34 & 80.01 & 84.10 & 14.43 \\  & ConStruct-VL  & 85.16 & 87.61 & 8.75 & 88.95 & 90.69 & 5.22 & 83.33 & 85.57 & 6.28 \\   & ZAF (Ours) & **89.61** & **89.65** & **4.18** & **92.53** & **92.20** & **1.72** & **89.43** & **90.20** & **3.02** \\  & _Inprovement_ & **4.45** & **2.04** & **4.57** & **3.58** & **1.51** & **3.50** & **6.10** & **4.63** & **3.26** \\   & Joint Learning & 93.37 & - & - & 95.07 & - & - & 92.36 & - & - \\  & Continual-FT  & 67.23 & 73.60 & 27.96 & 73.40 & 78.60 & 20.55 & 73.19 & 80.58 & 20.69 \\   & LoRA  & 69.55 & 75.03 & 27.25 & 68.73 & 78.03 & 29.62 & 75.63 & 81.87 & 19.37 \\   & Layered-LoRA  & 80.62 & 79.89 & 13.92 & 73.03 & 81.12 & 24.99 & 83.73 & 84.26 & 9.29 \\   & LuF  & 73.00 & 77.26 & 23.12 & 71.11 & 79.39 & 27.09 & 82.10 & 84.69 & 11.24 \\   & ZSCL  & 60.27 & 67.94 & 28.48 & 65.82 & 78.06 & 27.68 & 62.03 & 74.33 & 31.20 \\   & MoE-Adapters  & 72.50 & 74.81 & 23.74 & 67.09 & 76.54 & 31.83 & 79.05 & 84.21 & 15.58 \\   & ConStruct-VL  & 85.97 & 87.00 & 6.94 & 86.96 & 90.47 & 7.91 & 84.36 & 85.93 & 5.36 \\    & ZAF (Ours) & **89.67** & **89.30** & **3.38** & **91.78** & **91.74** & **2.02** & **88.74** & **89.03** & **2.67** \\   & _Inprovement_ & **3.70** & **2.30** & **3.56** & **4.82** & **1.27** & **5.89** & **4.38** & **3.10** & **2.69** \\   

Table 1: Overall performance (%) of CL methods across three benchmarks under various VL models.

[MISSING_PAGE_FAIL:9]

benchmarks, underscoring its superior adaptability. However, setting \(\) to 0.95 leads to a notable decline in performance, reflecting the model's compromised ability to integrate new knowledge effectively. Overall, ZAF demonstrates remarkable robustness across a broad range of hyperparameter settings. Please refer to Appendix C.4 for ZAF's heatmaps across different \(\) values.

**Task Order Analysis:** Fig. 4 presents the statistical results of various CL methods across three distinct task orders within the challenging 7 Task VG+VAW benchmark. Excluding the aforementioned task order within the 7 Task VG + VAW benchmark, the second and third orders are the \(rel.~{}spatial attr.~{}size attr.~{}material rel.~{}action attr.~{}color  object~{}state attr.~{}action\) task sequence and the \(rel.~{}spatial attr.~{}material attr.~{}state attr.~{}action attr.~{} size rel.~{}action attr.~{}color\) task sequence, respectively. Our ZAF method exhibits remarkable robustness to variations in the task order, showing only minimal fluctuations across three performance metrics, and consistently outperforms existing CL methods by a significant margin. For detailed task sequence and quantitative results, please refer to Appendix C.3.

## 6 Discussion and Conclusion

In this work, we analyze the challenges of learning and forgetting in continual learning for pre-trained VL models from both empirical and theoretical perspectives. A key finding is that zero-shot stability reliably indicates forgetting. We introduce a plug-and-play zero-shot antidote to enhance anti-forgetting capabilities across various CL methods, integrating it into our parameter-efficient EMA-LoRA architecture. This facilitates efficient adaptation and memory-free access to historical models, effectively circumventing the learning-forgetting trade-off prevalent in current CL works [3; 49; 45; 46]. Our approach demonstrates superior performance across various continual VL benchmarks and pre-trained models. We anticipate future research will explore the anti-forgetting challenge from the perspective of zero-shot prediction stability, diverging from traditional mechanisms.

This work presents several potential limitations. The effectiveness of our plug-and-play zero-shot antidote presumes the existence of a feasible CL framework, which may not be available in all contexts. Also, it is designed specifically for a sequence of downstream tasks, limiting its applicability to the original training contexts of the pre-trained models. As a fundamental research in machine learning, the potential negative societal impacts are not immediately apparent at this stage.

Figure 4: Results of various CL methods across three distinct task orders within 7 Task VG+VAW.

Figure 3: Comparison of FAA and CAA metrics for ZAF across various \(\) values against 3 baselines.