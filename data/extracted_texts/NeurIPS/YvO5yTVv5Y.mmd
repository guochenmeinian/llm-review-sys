# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

Introduction

Online high-definition (HD) map construction is essential for autonomous driving systems, as it supplies real-time and comprehensive information about the vehicle's surroundings, such as lanes, curbsides, and crosswalks. It serves as the foundation for the vehicle's navigation, planning, and decision-making processes, and is integral to the effective functioning of self-driving vehicles.

Existing online HD map construction methods fall into two classes: map rasterization and map vectorization. Map rasterization  is straightforward: as shown in Fig. 1 (a), it models HD map construction as a semantic segmentation task in bird's-eye view (BEV), rasterizing the surroundings into semantic maps as output. However, rasterized maps are not ideal representations for autonomous driving, as they lack instance-level and structural information, and require extensive post-processing to be consumed by subsequent navigation and decision-making modules. To address these limitations, map vectorization (Fig. 1 (b)) emerges as a popular solution for constructing HD maps. HDMapNet  and SuperFusion  employ complex post-processing to group pixels from rasterized maps into vectors. The recent VectorMapNet  and MapTR  directly predict map elements as vectorized point sets, achieving better accuracy with faster runtime.

Both VectorMapNet  and MapTR  utilize a sparse point set representation, where each map element is parameterized as a fixed-length vector of equidistantly sampled points, with L1 loss applied to supervise regression predictions. While this approach is simple and intuitive, we empirically observe that it is often suboptimal due to several reasons. _First_, as shown in Fig. 2, the sparse point set representation is often lacking in precision, particularly when dealing with sharp bends or complex details of map structures, resulting in significant parameterization errors.1_Second_, learning with equidistant points as regression targets causes ambiguous supervision, because the intermediate points often lack clear visual clues. _Third_, relying solely on the L1 loss for regression supervision causes the model to overlook fine-grained geometric variations, yielding overly smooth predictions that are insensitive to local deviations. Likewise, the current evaluation metric, which relies on Chamfer distance among point sets, tends to overlook minor deviations and geometric details. For autonomous driving, where precision is a matter of life and death, existing methods and metric for map vectorization are still inadequate.

To address these limitations, we reintroduce the philosophy of rasterization into map vectorization, to bring back the advantages of precision in HD map modeling while keeping the merits of vectorized outputs. We believe that rasterization can offer complementary benefits to map vectorization.

With the above motivation, we first design a new rasterization-based evaluation metric for map vectorization, which is more sensitive to minor deviations and better suited for practical driving scenarios. Unlike existing metric that uses Chamfer distance to determine if a map element matches the ground truth, we rasterize both the predicted and ground truth map elements into HD maps, and then use mean intersection-over-union (mIoU) to decide whether they match. This metric aligns better with human perception, takes into account the actual shape and geometry of individual map elements, and offers increased sensitivity to minor discrepancies.

We further present MapVR (Map Vectorization via Rasterization), a novel framework for precise HD map vectorization. MapVR can be integrated with any architecture that directly predicts vectorized map elements . Unlike existing map vectorization methods, our MapVR applies differentiable rasterization to vectorized output (ordered point sets) during training, transforms each vectorized map element into an HD map, and adds segmentation supervision on the rasterized HD maps. The proposed MapVR, sharing the philosophy with our aforementioned evaluation metric, enables more precise and detailed supervision, thus significantly boosting precision. It also provides more reasonable supervision, as it removes the ambiguity caused by equidistance. MapVR can also adapt

Figure 2: Inaccurate map elements caused by the parameterization of sparse equidistant point sets.

to a wide range of map elements with specially designed geometry-aware differentiable rasterization strategies, showing strong scalability. At the inference stage, the additional differentiable rasterization can be simply removed, and the network's vectorized output can be employed as the final result. As our method does not introduce any additional computational overhead during inference, it maintains high efficiency, while delivering more accurate and robust map construction results.

The contributions of this work are summarized as follows:

* We propose a novel rasterization-based evaluation metric for map vectorization that exhibits increased sensitivity to minor deviations, providing a more accurate and reasonable assessment of map vectorization performance in real-world driving scenarios.
* We propose MapVR (Map Vectorization via Rasterization), a novel framework that seamlessly combines differentiable rasterization with existing map vectorization approaches. MapVR substantially improves the precision for map vectorization, demonstrates robust scalability for diverse map elements, and incurs no extra computational overhead during inference.
* The proposed MapVR framework and evaluation metric pave the way for future research and advancements in map vectorization for autonomous driving applications, demonstrating the complementary benefits of rasterization to map vectorization.

## 2 Related Work

HD Map Construction.Understanding the vehicle's surrounding environment, including lanes, curbsides, crosswalks, and road topology, plays a central role in the navigation and decision-making of autonomous driving. Such driving scene information is usually provided by high-definition (HD) maps. Conventionally, HD maps are constructed offline using SLAM-based methods [58; 32; 41; 42] with complex pipelines. Recently, with the emergence of the bird's-eye-view (BEV) perception [19; 27; 51; 36; 30; 48; 29], the focus has shifted towards online HD map construction, which generates maps around ego-vehicle from vehicle-mounted sensors (_e.g._, cameras) on the fly.

Currently, there are two prevalent paradigms in online HD map construction: map rasterization and map vectorization. Rasterization methods [37; 40; 12; 59; 19; 60; 36; 50] generate HD maps via semantic segmentation in BEV, which have good sensitivity to details. However, the lack of vital instance-level information and lane topology limits the utility of rasterized maps in downstream tasks like navigation and planning. On the other hand, map vectorization addresses this limitation by producing vectorized map elements. HDMapNet  and SuperFusion  employ post-processing to group pixels from rasterized maps into vectorized elements. Moreover, VectorMapNet  proposes to directly predict map elements as vectorized point sets in an auto-regressive manner, achieving superior performance. And MapTR  - the current state of the art, further proposes a unified permutation-equivalent modeling approach to model the HD map elements, achieving superior accuracy. Furthermore, MapTR  achieves real-time efficiency with its one-stage and parallel framework. However, despite the recent progresses, vectorized maps still often exhibit minor deviations that can be critical in autonomous driving, where safety is of utmost importance.

Lane Detection.Lane detection, which can be seen as a sub-task of online HD map construction, concentrates on map elements like lanes and curbsides. Most existing lane detection research targets 2D camera views, and can be categorized into several modes, including pixel-level segmentation [33; 39; 35; 52], anchor-point-based regression [43; 18], and curve-prior fitting using polynomial [46; 44; 23] or Bezier curves . With the recent progresses in BEV perception, some lane detection methods [10; 4; 1; 47; 13] have also extended into 3D, perceiving lanes in BEV, aligning more closely with online HD map construction. Nonetheless, lane detection remains somewhat limited, perceiving only highly-regularized line-shaped map elements. In contrast, map vectorization has better scalability and adaptability with fewer assumptions, making it a better fit for real-world autonomous driving.

Differentiable Rasterization.Rasterization, a concept from computer graphics, refers to the process of rendering vector graphics representations (point coordinates or math formulas) into raster images (a series of pixels) for display on computer screens . Typically, rasterization is non-differentiable [38; 9]. Fortunately, recent advances in graphics and vision [28; 24; 5; 25; 21; 17; 34; 15] have achieved differentiable rasterization, bridging the gap between vector graphics and raster image through backpropagation. In this work, we make the first attempt to adapt differentiable rasterization to the map vectorization task to bridge vectorized outputs and rasterized HD maps. It enables more refined and comprehensive supervision and yields predictions with improved precision.

## 3 A Rasterization-Based Evaluation Metric for Map Vectorization

### Review of Existing Chamfer-Distance-Based Evaluation Metric

Map vectorization requires instance-level evaluation, similar to object detection [7; 22; 53; 54; 3; 55; 57; 56; 61]. Thus, current map vectorization works [16; 6; 26; 20] adopt Average Precision (AP) to evaluate the map construction accuracy, using Chamfer distance to determine whether the predicted map element and the ground truth map element match.

Specifically, Chamfer distance \(D_{}(,)\) is a measure of dissimilarity between two unordered point sets, which quantifies the average distance between each point in one set to the nearest point in the other set. It can be formulated as:

\[D_{}(P,Q)=0.5(_{p P}_{q Q}|p-q |_{2}+_{q Q}_{p P}|p-q|_{2}),\] (1)

where \(P\) and \(Q\) are the sets of points representing the predicted map element and the ground truth map element, respectively, \(|P|\) and \(|Q|\) are the cardinalities of point sets \(P\) and \(Q\), and \(|p-q|_{2}\) denotes the Euclidean distance between points \(p\) and \(q\).

Despite its simplicity and ability to provide fair evaluation results, the following limitations of this metric make it inadequate for highly demanding scenarios such as autonomous driving: _1)_ It is not scale-invariant; for smaller map elements such as stoplines, Chamfer distance error is consistently small, failing to provide a meaningful assessment. _2)_ Chamfer distance solely relies on unordered point set distance, completely overlooking the shape and geometrical details of the map elements, thus yielding unreasonable results for many practical scenes, as shown in Fig. 4. These drawbacks call for the development of a more robust and accurate evaluation metric tailored to the stringent requirements of autonomous driving map vectorization.

### Proposed Rasterization-Based Evaluation Metric

To address the aforementioned limitations, we introduce a rasterization-based evaluation metric that is more sensitive to minor deviations and better suited for real-world driving scenarios. While we still employ AP as our measurement, we adopt rasterization to precisely determine the matching between predicted and ground truth map elements.

As shown in Fig. 3, we demonstrate our metric using line-shaped map elements (_e.g._, lanes and curbsides). First, both ground truth and predicted elements are rasterized into a polyline in HD maps. In our setup, considering the perception range of \( 30\)m on the y-axis and \( 15\)m on the x-axis, we set the spatial size of the HD map as \(480 240\), such that each pixel represents \(0.125\)m, satisfying the high-precision requirement of autonomous driving. To better accommodate inaccuracies in predictions with thin and elongated geometry, we then dilate the rasterized polylines by 2 pixels on each side, thereby introducing an appropriate degree of tolerance. Finally, to determine whether the ground truth and predicted map elements match, we calculate the intersection-over-union (IoU) of their respective rasterized HD representations. Similar to MS-COCO's metric , AP is calculated at multiple IoU thresholds. For line-shaped elements, we set the thresholds as \(0.25:0.50:0.05\).

Figure 3: Illustration of our proposed rasterization-based approach for determining the match between ground truth and predicted vectorized map elements.

It is worth noting that HD maps often contain elements other than lines, such as crosswalks, intersections, and carparks. These elements can be abstracted into polygons. To conduct an appropriate evaluation for polygon-shaped map elements, we apply specially-designed polygon-shaped rasterization instead of line-shaped rasterization, and compute AP over \(0.50:0.75:0.05\).

### Comparative Analysis and Discussion

Evaluation Quality.We examine the evaluation quality of the two metrics with a few practical examples. Fig. 4(a) displays a case involving a short stopline, where the prediction is perpendicular to the ground truth. The Chamfer distance metric judges a match, as it lacks scale-invariance. While the rasterization-based metric successfully recognizes the discrepancy based on their low IoU. Fig. 4(b) presents a scenario in which the predicted lane/curbside exhibits a minor horizontal deviation from the ground truth. Such deviations, even if small, pose critical dangers in real driving scenes. The Chamfer-distance-based metric considers the prediction as matched solely based on the small point-set distance. Conversely, our metric takes geometry into consideration, determining that they do not match. Fig. 4(c) illustrates a case with a vertical deviation between the prediction and ground truth, typically arising from occlusion. This situation is generally non-critical, as the map updates continuously as the vehicle moves forward. By incorporating shape and geometry knowledge, the rasterization-based metric evaluates more reasonably. Fig. 4(d) also verifies that our metric is more sensitive to small but critical errors. Collectively, these examples show that the rasterization-based metric offers superior sensitivity and is better aligned with practical autonomous driving scenarios.

Computational Complexity.The rasterization-based metric requires additional computation for rasterization but still runs acceptably fast. Empirically, the evaluation process on nuScenes Map  validation set takes \( 3\) minutes on our server equipped with an Intel Xeon Gold 6226R CPU.

## 4 MapVR (Map Vectorization via Rasterization)

### Framework Overview

As shown in Fig. 1(c), MapVR is a novel and generic learning framework for map vectorization, which combines rasterization to leverage the fine-grained supervisory signal from the rasterized HD maps while retaining the benefits of vectorized representation. MapVR is parameter-free and thus can be easily integrated with various network architectures for map vectorization (_e.g._, MapTR ).

Fig. 5 illustrates the overall framework of MapVR. During training, the base map vectorization model first generates vectorized representation for each map element. Then, MapVR produces an HD map by rendering the vectorized element with a specially-designed differentiable rasterizer. Finally, segmentation-based losses can be directly applied to the rendered HD maps, providing more granular supervision on the shape and geometry of the map elements, which leads to more precise results.

### Differentiable Rasterization: Bridging Vectorized Representation and HD Semantic Maps

Rasterization serves as a vital bridge between vectorized representation and HD maps. Generally, rasterization is not differentiable due to the binary assignment that decides whether a pixel is covered by any shape primitive. Inspired by [24; 5; 17; 15], to enable fine-grained supervision signals directly from HD maps, we introduce a soft version of rasterization, which renders each vectorized map element into an HD mask while preserving the whole framework's differentiability.

Figure 4: Evaluation quality comparison between Chamfer-distance-based metric and our proposed rasterization-based metric on a few practical cases. Our metric is able to produce more reasonable evaluation suitable for autonomous driving applications.

Concretely, for a line-shaped map element represented by an ordered point set \(P\), we compute its softly-rendered mask \(I_{}[0,1]^{H W}\) with

\[I_{}(x,y;P)=(),\] (2)

where \(D(x,y;P)\) denotes the closest distance from pixel \((x,y)\) to all segments of the polyline \(P\), and the softness \(\) controls the rasterization smoothness. A larger \(\) yields smoother transitions between the polyline and empty regions, while a smaller \(\) leads to sharper, more distinct line boundaries.

While for polygon-shaped map elements like intersections, the rendered mask \(I_{}\) is computed as

\[I_{}(x,y;P)=( ),\] (3)

where \(D(x,y;P)\) is the closest distance from pixel \((x,y)\) to any boundary segment of the polygon \(P\), and \(C(x,y;P)\{-1,+1\}\) indicates whether pixel \((x,y)\) falls inside (\(+1\)) or outside (\(-1\)) the polygon. \(()\) denotes the sigmoid function. Similarly, the softness \(\) controls the transition smoothness of the rasterized values at the polygon boundary areas.

Our differentiable rasterizer (Eq. 2 & 3) transforms each vectorized map element into a rasterized HD mask representation in a parameter-free manner, which enables the learning of fine-grained shapes and geometric details through direct supervision on these rasterized HD masks.

### Training and Inference Procedure

Training.Fig. 5 illustrates how differentiable rasterization is incorporated into the map vectorization framework. First, we use a base map vectorization model (_e.g._, MapTR ) to predict a set of vectorized map elements. Then, instead of relying on L1 loss with equidistant points as targets as in , we render both vectorized prediction and vectorized ground truth into rasterized HD masks, and apply supervision directly on the masks using dice loss . Thanks to the differentiability of our designed rasterization processes (Eq. 2 & 3), the segmentation loss is able to guide the learning of vectorized predictions. Notably, this supervision is geometry-aware, as the rasterization procedure (line-shaped or polygon-shaped rasterization) is determined by the class of the target map element. The effectiveness of geometry-aware rendering is validated in Section 5.3. Moreover, the rasterization-based segmentation loss effectively weighs down the equidistance requirement (which is ill-posed due to the lack of clear visual clues), thus providing a more reasonable learning target.

In addition to the rendering-based loss, we include a direction regularization loss as an additional auxiliary loss. Specifically, we define the direction regularization loss on the vectorized output as

\[_{}=_{i=1}^{N-2}P_{i+1} },P_{i+2}}>}{|P_{i+1}}|| P_{i+2}}|},\] (4)

where \(P_{i}\) denotes the \(i^{}\) point in the predicted point set. It encourages the predictions to avoid unnecessary direction changes along adjacent segments. This effectively promotes a smoother point set to avoid back-and-forth patterns that are not penalized by the rendering loss, and also facilitates the allocation of more points in regions with higher curvature and fewer points in straight-line regions.

Efficient Inference.After training, the rasterization processes are no longer needed. Consequently, MapVR can enhance map vectorization without adding any extra computational cost during inference.

Figure 5: The learning pipeline of MapVR. MapVR utilizes a base model for vectorized map generation, followed by a customized differentiable rasterizer to produce HD maps, on which fine-grained, geometry-aware supervision is applied to enhance the precision of vectorized elements.

## 5 Experiments

### Experiment Setup

**Dataset and Evaluation Metrics.** MapVR is evaluated across multiple datasets, as outlined below.

1. nuScenes Map (basic) , which consists of two line-shaped map classes (lane divider and road boundary) and one polygon-shaped map class (pedestrian crossing). This dataset setup aligns with prior works on map vectorization [16; 6; 26; 20].
2. nuScenes Map (extended) , an extension of nuScenes Map (basic) that incorporates more complex map elements, such as intersection, stopline area, and carpark area.
3. Argoverse2 , a large-scale dataset featuring the same classes as nuScenes Map (basic).
4. 6V-mini-vo.4, our proprietary large-scale commercial dataset for autonomous driving, covering very complex driving scenes in real world. It includes three line-shaped classes (lane, curbside, and stopline) and two polygon-shaped classes (crosswalk and intersection).

Both Chamfer-distance-based metric (Section 3.1, denoted as \(_{}\)) and the newly proposed rasterization-based metric (Section 3.2, denoted as \(_{}\)) are used for performance evaluation.

Implementation Details.All experiments, unless otherwise stated, are conducted with 8x NVIDIA RTX 3090 GPUs. As the proposed MapVR is a generic framework with no reliance on specific model architecture, we adopt MapTR , the state-of-the-art model for map vectorization, as the base model. Our implementation aligns with MapTR . Please refer to the appendix for more details.

### Experiment Results

Results on nuScenes Map.Table 1 compares MapVR with existing map vectorization techniques on nuScenes Map (basic). Even under the less sensitive \(_{}\) metric, our proposed MapVR delivers superior overall performance across various settings. The advantage of MapVR becomes even more pronounced under the more precise and autonomous-driving-oriented \(_{}\) metric. Specifically, MapVR provides a notable 3.5% improvement over a fully-trained MapTR. When working with multi-modality inputs, MapVR obtains an even larger margin of 5.9%. As shown in Table 2, on the more challenging nuScenes Map (extended) dataset that includes more complex elements, our MapVR achieves superior performance across all map elements under both metrics. These results validate the substantial improvements brought by MapVR and its exceptional capability of adapting to challenging scenarios. It is noteworthy that these improvements are achieved without adding any additional computational burden during inference.

    &  &  &  & _{}\)} & _{}\)} &  \\   & & & & ped & div & bdry & avg. & ped & div & bdry & avg. & \\  HDMapNet  & C & Efin-B0 & 30 & 14.4 & 21.7 & 33.0 & 23.0 & - & - & - & - & 0.8 \\ HDMapNet  & C \& L & Efin-B0 & 30 & 16.3 & 29.6 & 46.7 & 31.0 & - & - & - & - & 0.5 \\  VectorMapNet  & C & Res-50 & 110 & 36.1 & 47.3 & 39.3 & 40.9 & 26.2 & 12.7 & 6.1 & 15.0 & 2.9 \\ VectorMapNet  & C \& L & Res-50 & 110 & 37.6 & 50.5 & 47.5 & 45.2 & - & - & - & - & - \\  MapTR  & C & Res-50 & 24 & 46.3 & 51.5 & 53.1 & 50.3 & 32.4 & 23.5 & 17.1 & 24.3 & **18.4** \\ MapTR  & C & Res-50 & 110 & 56.2 & 59.8 & 60.1 & 58.7 & 43.6 & 35.6 & 25.8 & 35.0 & **18.4** \\ MapTR  & C \& L & Res-50 & 24 & 56.4 & 61.8 & **70.1** & 62.7 & 46.4 & 39.2 & 50.0 & 45.2 & 7.2 \\  MapTR  + MapVR (Ours) & C & Res-50 & 24 & 47.7 & 54.4 & 51.4 & 51.2 & 37.5 & 33.1 & 23.0 & 31.2 & **18.4** \\ MapTR  + MapVR (Ours) & C & Res-50 & 110 & 55.0 & 61.8 & 59.4 & 58.8 & 46.0 & 39.7 & 29.9 & 38.5 & **18.4** \\ MapTR  + MapVR (Ours) & C \& L & Res-50 & 24 & **60.4** & **62.7** & 67.2 & **63.5** & **52.4** & **46.4** & **54.4** & **51.1** & 7.2 \\   

* _In modality, "C denotes multi-view camera input and "C & & & & & & & & & & & & \\ _When LiDAR data is incorporated, PointPillars  serves as the backbone for processing the LiDAR data._
* _The abbreviations ’ped’, ‘div’, and ‘bdry’ correspond to the map elements of pedestrian crossing, divide, and boundary, respectively._

    & _{}\)} & _{}\)} &  \\   & ped & stp & int & cap & div & bdry & avg. & ped & stp & int & cap & div & bdry & avg. & \\  MapTR  & 34.3 & 29.9 & 21.5 & 37.9 & 44.9 & 45.1 & 35.6 & 22.5 & 12.1 & 38.4 & 23.4 & 18.3 & 12.1 & 21.1 & **18.4** \\ MapTR  + MapVR (Ours) & **39.5** & **31.6** & **21.9** & **42.4** & **45.8** & **45.9** & **37.9** & **30.8** & **13.9** & **43.3** & **32.8** & **27.0** & **18.8** & **27.8** & **18.4** \\   

* _All competing methods like multi-view cameras as input, use ResNet-50  as the backbone, and are trained for 24 epochs._
* _'ped’, ‘step’, ‘ ‘int’, ‘cap’, ‘div’, and ‘bdry’ denote pedestrian crossing, stopline, intersection, carpark area, divider, and boundary, respectively._

Table 2: Map vectorization performance on nuScenes Map (extended) validation set.

Results on Argoverse 2.Table 3 presents the performance comparison on the Argoverse2 dataset . Note that in our setup, the height information for map elements is ignored. Our proposed MapVR method still achieves state-of-the-art performance, which verifies its robustness across multiple scenarios.

Results on 6V-mini-v0.4.Finally, we test MapVR on 6V-mini-v0.4, our proprietary commercial dataset that features highly intricate real-world driving scenes. As Table 4 shows, MapVR greatly enhances the performance on all map elements, which validates its efficacy and robustness in complex and real-world contexts.

Visualizations.Fig. 6 visualizes the results of HD map vectorization and compares our method with MapTR . For a fair comparison, both methods use the ResNet-50  backbone and solely rely on multi-view camera images as input. It can be observed that our method yields more accurate HD maps, particularly in capturing intricate details and accurately representing complex or curved map elements. Conversely, while MapTR  produces generally correct vectorized maps, it inevitably exhibits deviations in finer details and struggles to precisely construct complex map elements. These observations reaffirm our motivation to incorporate the precise supervision from HD rasterization into map vectorization, which compensates for the inherent limitations caused by the sparse, equidistant point sets, thereby enhancing the precision of map vectorization.

    & _{}\)} \\   & lane curbside & stopline & crosswalk & intersection \\  MapTR  & 41.3 & 32.9 & 7.6 & 13.3 & 43.6 \\ MapTR  + MapVR (Ours) & **50.8** & **37.3** & **11.8** & **14.3** & **44.0** \\   

Table 4: Map vectorization performance on 6V-mini-v0.4 dataset (our proprietary commercial dataset).

Figure 6: Visualization of online HD map vectorization results. Our proposed MapVR demonstrates a superior ability in constructing more accurate maps, particularly for complex map elements and intricate details.

    & _{}\)} & _{}\)} \\   & ped & div & bdry & avg. & ped & div & bdry & avg. \\  HDMapNet  & 13.1 & 5.7 & 37.6 & 18.8 & - & - & - & - \\ VectorMapNet  & 38.3 & 36.1 & 39.2 & 37.9 & - & - & - \\ MapTR  & **54.7** & **58.1** & 56.7 & 56.5 & 22.1 & 32.6 & 24.0 & 26.2 \\ MapTR  + MapVR (Ours) & 54.6 & **60.0** & **58.0** & **57.5** & **23.5** & **36.5** & **30.2** & **30.1** \\    \({}^{*}\)_ped_, \({}^{div}\), and \({}^{hydr}\) denote pedestrian crossing, divider, and boundary, respectively,_

Table 3: Comparison of various map vectorization methods on Argoverse2 validation set.

### Ablation Study

Rasterization & Rasterization Resolution.As shown in Table 4(a), incorporating rasterization enhances performance, following a general trend where higher resolutions yield better results. However, an exception is observed at the 64x32 resolution, which degrades the performance due to the lack of precise rasterization supervision at such a coarse resolution. Fig. 7 further presents the Precision-Recall curves, showing that MapVR leads to consistent performance gain under both metrics and, notably, a smaller gap under the two metrics. Conversely, the baseline exhibits a large drop under the stricter AP\({}_{}\). This proves the necessity of incorporating fine-grained supervision from rasterization.

Rasterization Softness \(\).\(\) is a tricky hyper-parameter. It needs to be large enough to provide sufficient supervisory gradient while being small enough to ensure precise supervision. Empirically, polygon-shaped map elements are robust against various \(\), while line-shaped elements are not, due to their thin and elongated shapes. Table 4(b) studies the effect of different \(\) for line, taking the 'divider' class (a line-shaped map element) as an example.

Auxiliary Regularization on Point Direction.Table 4(c) studies the effect of the direction regularization loss described in Eq. 4, and also compares it with MapTR's directional loss , which uses the directions of ground truth equidistant points as targets. Results show that our direction regularization loss (w/ self) improves performance, proving its effectiveness in regularizing vectorized smooth point sets and allocating more points on higher curvature areas to improve precision. Conversely, the performance of our MapVR slightly degrades when using MapTR's regularization (w/ GT). This is because our MapVR's supervision from rasterization no longer requires the vectorized outputs to be equidistant.

Geometry-Aware Rasterization.Table 4(d) shows that simply rendering all map elements into lines severely impairs performance. The performance drop mainly comes from polygon-shaped elements (ped_crossing: 37.5%\(\)21.8%). This verifies the necessity of geometry awareness in rasterization.

Why Not Introduce HD Supervisory Signals from an Auxiliary Segmentation Task?A simple alternative to incorporate fine-grained supervision from rasterization is to append an additional parallel segmentation branch as an auxiliary task (dubbed as 'parallel segm'). This has been verified effective in many works [8; 4; 1]. Table 4(e) compares MapVR with this strategy. While 'parallel segm' improves baseline performance by 2.4%, it still largely lags behind our MapVR. The improved performance from 'parallel segm' supports our motivation to enhance map vectorization via rasterization. However, we attribute its inferior performance compared to MapVR to the fact that, unlike in our MapVR, the fine-grained supervisory signal is not directly applied to the vectorized output.

### Computational Overhead During Training

With the CUDA-accelerated differentiable rasterizer, our proposed MapVR only brings a marginal

Table 5: MapVR’s ablation experiments on nuScenes Map (basic) validation set. All models employ ResNet-50 as backbones and are trained for 24 epochs. MapVR’s default setups are marked in \(\)gray\(\)gray\(\).

Figure 7: Comparison of P-R curves. MapVR narrows the performance gap under the coarse and strict metrics.

increase in memory footprint while maintaining training efficiency. Table 6 presents a detailed comparison between the training costs of our method, 'MapTR + MapVR', and its baseline, 'MapTR'.

### Failure Case Analysis

While the proposed method greatly improves the quality of HD map vectorization, the numerical results suggest that the results are still far from perfect. We provide visualization of a few typical failure cases in Fig. 8.

From row \(1,2,\,4\) in Fig. 8, it can be seen that occlusions, whether from vehicles, constructions, or a limited field of view, hamper perception in the bird's-eye-view. Such occlusions often result in inaccuracies in the predicted vectorized maps. Yet, since road structures typically follow regular patterns, current map vectorization techniques may benefit from integrating road structure priors, such as standard-definition maps (SDMap), to enhance their reasoning capabilities.

Row 5 in Fig. 8 shows that there is still room for improvement in nighttime driving.

Row 3 in Fig. 8 is caused by ambiguity in annotation, where it is unclear whether the middle crosswalk should connect to the adjacent ones or not.

## 6 Conclusion

In this paper, we introduce a new perspective on map vectorization: rasterization, through which we can learn and evaluate map vectorization more precisely. We demonstrate that, while vectorized representation is compact and easy to use, it lacks representation capability, especially regarding fine-grained details; thus, it is necessary to incorporate rasterization as a complement in both learning and evaluation. We hope our perspective can serve as the cornerstone and spur further innovation in map vectorization, and can eventually lead to safe and reliable autonomous driving.

   Method & Modality & Backbone & Training Time / Iter & GPU Memory Usage \\  MapTR  & C & Res-50 & 0.82 s & 14021 MB \\ MapTR  & C \& L & Res-50 & 1.18 s & 28557 MB \\ MapTR  + MapVR (Ours) & C & Res-50 & 0.91 s & 14169 MB \\ MapTR  + MapVR (Ours) & C \& L & Res-50 & 1.37 s & 28673 MB \\   

* _In modality, ‘C’ denotes multi-view camera input and ‘C & L’ denotes combined multi-view camera and LiDAR input._

Table 6: MapVR’s computational overhead during the training stage. Results were obtained with 8x NVIDIA A100 GPUs under the same training setups.

Figure 8: Visualization of failure cases produced by our method.