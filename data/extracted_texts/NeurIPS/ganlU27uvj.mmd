# Slot-guided Volumetric Object Radiance Fields

Di Qi

MEGVII Technology Inc.

qidi@megvii.com &Tong Yang

MEGVII Technology Inc.

yangtong@megvii.com &Xiangyu Zhang

MEGVII Technology Inc.

zhangxiangyu@megvii.com

Corresponding author.

###### Abstract

We present a novel framework for 3D object-centric representation learning. Our approach effectively decomposes complex scenes into individual objects from a single image in an unsupervised fashion. This method, called slot-guided Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance fields with object slots as a guidance to implement unsupervised 3D scene decomposition. Specifically, sVORF obtains object slots from a single image via a transformer module, maps these slots to volumetric object radiance fields with a hypernetwork and composes object radiance fields with the guidance of object slots at a 3D location. Moreover, sVORF significantly reduces memory requirement due to small-sized pixel rendering during training. We demonstrate the effectiveness of our approach by showing top results in scene decomposition and generation tasks of complex synthetic datasets (e.g., Room-Diverse). Furthermore, we also confirm the potential of sVORF to segment objects in real-world scenes (e.g., the LLFF dataset). We hope our approach can provide preliminary understanding of the physical world and help ease future research in 3D object-centric representation learning.

## 1 Introduction

As humans, we can understand scenes, perceive discrete objects within it, and interact with these objects in a 3D environment. This object-centric, geometric understanding of the 3D world is a fundamental ability in human vision . In computer vision, researchers attempt to replicate this fundamental ability in machine learning models with a high interest, due to its wide application ranging from robotics  to autonomous navigation. To this end, machine learning models should bear two characteristics: unsupervised representation learning manner and 3D-aware generative mode .

Recently, with the advances of neural radiance fields (NeRFs)  and representation learning [5; 6; 7], there are some works [8; 9; 3] to achieve these two characteristics. These works learn to decompose objects and understand 3D scene geometry from RGB supervision via novel view synthesis in an unsupervised manner. To this end, volumetric-based methods  utilize volume rendering mechanism to implement a 3D-aware differentiable generative process without supervision. To avoid high computational cost of volumetric-based methods, light-field based methods [9; 3] use light field formulation  to scale its application to large numbers of objects in a scene. However, existing works suffer from some limitations. Light-field based methods lack strict multi-view consistency  and fall into mask bleeding issues , which are harmful for object-centric representation learning. Besides, volumetric-based methods fail to decompose scenes caused by attention rank collapse  and do not perform well in complex multi-object scenes.

In this paper, we propose a novel framework for 3D object-centric representation learning, alleviating the issues of existing works. Our method, called slot-guided Volumetric Object RadianceFields (sVORF), adopts volumetric rendering to synthesis novel views and use object slots as a guidance to compose volumetric object radiance fields. Concretely, we firstly use an efficient transformer module to extract object slots from a single image and learn object-aware slot features with the help of self-attention mechanisms . Then we utilize a hypernetwork to map these slots to volumetric object radiance fields. Finally, at each 3D location, we compose object radiance fields with the guidance of object slots for volumetric rendering. Thus, sVORF can avoid instinctive limits of light field formulation and resolve mask bleeding issues. Meanwhile, with the benefits from the guidance of object slots, sVORF can learn 3D-aware slot features and facilitate network optimization, alleviating the issues faced by existing volumetric-based methods. Moreover, instead of rendering whole image , sVORF only render a small amount of image pixels during the training phase, which significantly reduces the demand for training resources.

To validate the effectiveness of sVORF, we conduct experiments on four synthetic datasets to assess the ability of scene decomposition (e.g., segmentation in 3D) and scene generation(e.g., novel view sythesis, scene editing in 3D). Our results demonstrate that sVORF can precisely decompose 3D scenes into individual objects and produce high-quality novel view images. Specially, sVORF outperforms other state-of-the-art methods by a significant margin in complex multi-object scenes. In ablation experiments, we also analyze the effects of core components and show their strength. Besides, we show the robustness of sVORF on unseen object appearance and unfamiliar spatial arrangements. Furthermore, we extend our validation process to complex real-world LLFF data, confirming that our approach can segment objects in complex scenes with high accuracy.

To sum up, our contributions are three fold. First, we introduce a novel approach for 3D-centric representation learning, named sVORF, that effectively decompose objects from a single image. Second, our slot-guided scene composition method avoids the shortcomings of existing methods and significantly reduces the memory requirements during training phase. Third, we validate the effectiveness of our proposed method on synthetic datasets and confirm the extendability of our approach on real-world scenes.

## 2 Related Work

### Neural Scene Representation and Rendering

Neural scene representations parameterize 3D scenes with a deep network that map _xyz_ coordinates to signed distance functions or occupancy fields. Equipped with differentiable rendering functions, they can be optimized using only 2D images, relaxing the requirement of 3D ground truth. In particular, Neural Radiance Fields (NeRFs)  can use an MLP to compute radiance values (color and density) for a given 5D coordinate (spatial location (_x, y, z_) and viewing direction (\(,\))) and produce novel views with remarkably fine details. Numerous subsequent works have been introduced to address some its shortcomings and expand its applications, including rendering acceleration , NeRF with few images , 3D reconstruction  and 3D scene semantic understanding . However, these volumetric methods need hundreds of evaluations for a ray, leading to an expensive cost for rendering. To address this issue, Light Field Networks (LFN)  directly map an input ray to an output color, making only a single evaluation of the MLP per ray.

### 3D Object-centric Representation Learning

Driven by the effectiveness of NeRF, recent research has attempted to combine 2D self-supervised object-centric models  with neural scene representations to decompose a 3D scene to individual objects. Earlier work  utilizes a slot-based encoder and NeRFs as 3D representations to decompose 3D scene with extra multi-view dense depth supervision. Likewise, uORF  explicitly model the separation of objects and background with only images in training to address complex scenes. In order to avoid expensive cost of volume rendering, OSRT  and COLF  further replace the volumetric parametrization with a light field formulation. However, existing methods face some limitations. ObSuRF  need dense depth as supervision and can not handle complex scenes, while uORF  encounters expensive computation cost during training. Although light field formulation method  resolve computation cost issues, they lack strict multi-view consistency  and easily fall into mask bleeding issues .

Different from the above methods, our method adopts volume representation to parameterize 3D scenes, avoiding the limits of light field formulation. Also, our approach has low computation cost by only sampling a small amount of rays and avoids using depth supervision during training. Moreover, our method can be applied to object segmentation in real-world scenes (e.g. LLFF dataset) .

## 3 Method

The aim of our method is to decompose a scene to a set of object-centric 3D representations given a single input image. An overview of our approach is provided in Figure 1. We firstly extract image features from an input image and decompose object slots from image features. Then we map these slots to volumetric object radiance fields and compose these object radiance fields with the guidance of object slots for novel view synthesis.

### Preliminaries: NeRF

We begin by briefly reviewing the Neural Radiance Fields (NeRFs). A radiance field encodes a scene as a continuous volumetric radiance field \(f\). The input to \(f\) is a location \(^{3}\) and a viewing direction \(^{2}\), while the output is an RGB color value \(^{3}\) and a volume density \(^{+}\). NeRFs parameterize \(f\) as an MLP \(_{}\). To make MLP learns high-frequency functions, the input coordinates \(\) and viewing directions \(\) are mapped into a higher dimensional space with sinusoids function \(()\) before being passed into the MLP . This process, known as positional encoding, allows the MLP to better capture high-frequency scene content. Therefore, the function \(_{}\) can be formulated as:

\[_{}:((),())(,) \]

Given N sampled points along a ray \(\) and its predicted color and volume density \(\{(},_{i}),i\{1,,N\}\}\), the expected color \(C()\) of camera ray \(\) can be derived from volume rendering:

\[C()=_{i=1}^{N}T_{i}(1-(-_{i}_{i}) )_{i},T_{i}=(-_{j=1}^{i-1}_{j}_{j}) \]

where \(_{i}\) indicates the distance between adjacent samples. The reconstruction loss between the rendered and true pixel colors is used during training to optimize the parameters \(\) of MLP.

### Scene Decomposition from 2D Prior

Following [35; 9], we define the scene as a combination of \(K\) entities, where the first \(K-1\) represent the objects and the last one represents the background. To obtain these entities, we first extract image feature as a 2D prior using an encoder \(E()\) from a scene image \(\). Instead of using slot attention module as existing methods, we adopt an efficient transformer module \(T\) to infer object and

Figure 1: **sVORF overview**. The image encoder \(E_{}()\) processes the source view of a scene to generate 2D image features that serve as a prior. Next, these features are fed into the _Scene Decomposition_ module to infer object and background slots. A hypernetwork then maps these slots to volumetric object radiance fields. Finally, the object slots provide guidance for the recombination of object radiance fields to render arbitrary views with 3D-consistent object decomposition.

background slots from image feature, termed as \(=\{_{i}\}_{i=1}^{K}\). Compared to slot attention module, this transformer module is simple and easy to train without Gated Recurrent Unit (GRU) block. For this transformer module, we take a global image feature \(=(E())\) as initial object slots \(=\{_{i}\}_{i=1}^{K}\). Combining with \(K\) learned positional encodings, these slots explicitly model all pairwise interactions between all slots and learn object-aware features via self-attention, which avoids inherent ambiguities in estimating color and geometry at occluded views, see Section 4.4 for details. Then these slots bind and explain the input image representation with cross-attention to image feature. Finally, the slots are transformed to \(=\{_{i}\}_{i=1}^{K}\) via feed forward network (FFN). The whole process can be formulated as follows:

\[=T(,E()) \]

### Scene Composition

Given the infered slots \(=\{_{i}\}_{i=1}^{K}\), we recompose these slots to novel views of the same scene, which is critical for achieving 3D object-centric representation learning. To achieve it, there are two forms: Spatial Broadcast (SB) [35; 8; 9; 3] and Slot Mixers (SM) . SB and SM do scene composition on RGB space and feature space, respectively. SB can utilize 3D geometric bias (3D point or 3D ray) in the scene composition, facilitating the network optimization. Conversely, due to composition on feature space, SM is hard to optimize without 3D geometric bias. But SM can learn 3D-aware slot features, which is useful for scene decomposition. Combining the advantages of two forms, we propose a new method for scene composition. We transform slots into volumetric neural radiance fields to utilize explicit geometric bias and avoid the limits of light field formulation . Then, we compose all volumetric neural radiance fields with the guidance of slots, making slot features 3D-aware.

Objects as Neural Radiance FieldsTo transform a slot to its radiance field, we utilize a hyper-network [38; 39]\(H\) to map the slot \(_{i}\) directly to the parameters \(_{i}\) of the associated object Neural Radiance Field. The mapping process is formulated as:

\[_{i}=H(_{i}) \]

where \(i=1,,K\), \(K\) is the number of object slots.

With the radiance field \(_{_{i}}\), we can map a location \(^{3}\) with a viewing direction \(^{2}\) to a tuple of color \(_{i}\) and density \(_{i}\) of corresponding object:

\[_{_{i}}:((),())(_{i}, _{i}) \]

In this way, we transform feature space to RGB space, introducing 3D geometric bias for scene composition.

Composing MechanismGiven K Object NeRFs \(\{_{_{i}}\}_{i=1}^{K}\), we compose their outputs \(\{_{i}\}_{i=1}^{K}\) and \(\{_{i}\}_{i=1}^{K}\) at a 3D location with the guidance of object slots. Specifically, we firstly leverage a feature aggregation block \(D^{e}\) to gather object slots and obtain an aggregate feature \(\) for query location \(()\):

\[=D^{e}((),) \]

where \(D^{e}\) is a cross-attention layer  network. Then we pass \(\) to an attention block \(D^{a}\) and compute a normalized dot-product similarity between \(\) and each slot feature in \(\):

\[=D^{a}(,) \]

where \(=(m_{1},,m_{K})\), \(D^{a}\) is a cross-attention layer without linear operation. Finally, we compute the combined density \(\) and color \(}\) as follows:

\[=_{i=0}^{K}m_{i}_{i},}=_{i=0}^{K}m_{i }_{i} \]

Note that our model can train with a small amount of sample rays, leading to a significant reduction for computation and memory cost during training. We speculate that this characteristic benefits from our composing mechanism. In order to decompose small objects from a scene, it is essential to sample much more rays to cover the regions of these small objects, resulting in heavy training cost. However, our composing mechanism can perform well on small objects with a small amount of rays, addressing this large sampling cost, see Section 4.4.

### Loss Functions

Reconstruction LossWe train our model across multiple scenes and only take view images as the supervisory signal. The reconstruction loss is formulated as:

\[_{}=_{}\|C()-C_{gt} ()\|^{2} \]

where \(\) is the set of rays in each batch, and \(C_{gt}()\) is the groundtruth color.

Connectivity RegularizationWe observe that some object radiance fields exist semi-transparent clouds, especially when the number of slots is much larger than the total number of objects in a scene. To solve this issue, we apply a connectivity regularization \(_{}\) to each \(_{_{i}}\) by referring to the distortion loss presented in Mip-NeRF360 :

\[_{}(,)=_{i,j}w_{ i}w_{j}|+t_{i+1}}{2}-+t_{j+1}}{2}|+ _{i}w_{i}^{2}(t_{i+1}-t_{i}) \]

where \(=\{T_{i}(1-(-_{i}_{i})) \}_{i=1}^{N}\) is the weights along a ray and \(\) is the normalized ray distance.

Total LossThe overall training loss function is formulated as follows:

\[=_{}+_{}_{} \]

where \(_{}\) is the scale to balance the connectivity regularization \(_{}\), which is set to be 0.01 in our experiments.

## 4 Experiments

DatasetsFollowing uORF , we experiment on several datasets in increasing order of complexity.

_CLEVR-567_: The CLEVR  dataset is a widely used benchmark for evaluating object decomposition in computer vision. CLEVR-567 is a multicamera variant of this dataset with 1,000 scenes for training and 500 scenes for testing. Each scene consists of with 5-7 CLEVR objects that randomly positioned and oriented with a clean background. The objects in the scenes are comprised of three geometric primitives: cubes, spheres, and cylinders. We follow uORF's setup in using a "Rubber" material with largely diffuse properties.

_CLEVR-3D_: This dataset is also a variant of CLEVR dataset, in which each scene consists of 3-6 basic geometric shapes of 2 sizes and 8 colors. In particular, each scene includes 3 fixed views: the two target views are the default CLEVR input view rotated by \(120^{}\) and \(240^{}\), respectively. Following ObSuRF , we train 35k scenes and test on the first 320 scenes of each validation set [43; 6].

_Room-Chair_: This dataset contains 1,000 scenes designated for training and 500 for testing. In this dataset, each scene includes 3-4 chairs of identical shape with 3 different textures background.

_Room-Diverse_: This dataset is an upgraded Room-Chair. Each scene contains 4 distinct chairs, whose shape chosen randomly from ShapeNet  chair shapes, and a range of background that is selected from 50 unique textures. There are 5,000 scenes for training and 500 for testing.

_MultiShapeNet (MSN)_: This dataset comprises 11,733 distinct shapes, with each scene populated by 2-4 objects with different categories sourced from the ShapeNetV2 3D model dataset.

_Local Light Field Fusion (LLFF)_: This dataset includes real scene scenarios with complex foreground and background, making it highly challenging. We specifically utilize the forward-facing scenes _Flower_ and _Fortress_ from the LLFF dataset, with each scene consisting of 27 training images and 6 testing images.

BaselinesWe compare our model with a 2D object-centric learning method Slot-Attention  and four competitive 3D methods, namely uORF , COLF , ObSuRF  and OSRT , in terms of scene decomposition and novel view synthesis. Both COLF and OSRT are based on light field, while uORF and ObSuRF employ volumetric parameterization, which is similar to our approach.

MetricsTo evaluate the quality of novel view synthesis, we use Learned Perceptual Image Patch Similarity (LPIPS) , Structural Similarity Index (SSIM) , and Peak Signal-to-Noise Ratio (PSNR). For scene segmentation in 3D, we measure clustering similarity using Adjusted Rand Index (ARI). The ARI score ranges from 0 to 1, with a score of 0 indicating random segmentation and a score of 1 indicating perfect segmentation. For a fair comparison, we evaluate two types of 2D ARI metrics: ARI and FG-ARI, as well as three types of 3D ARI metrics: NV-ARI, ARI\({}^{*}\), and FG-ARI\({}^{*}\). Specifically, both ARI and FG-ARI are computed on the source image to facilitate comparison with 2D methods. The FG-ARI is further calculated solely on the foreground regions, using ground-truth data. Similarly, ARI\({}^{*}\) and FG-ARI\({}^{*}\) are calculated on all images. Lastly, the NV-ARI is computed on synthesized novel views.

### Scene Segmentation in 3D

SetupGiven the soft slot masks of 3D locations, 2D segmentation masks are inferred through volume rendering. To clarify, we begin by mapping the pixel \(\) in the rendered view to a ray \(\) for sampling N points along it. Then, we calculate the soft mask \(\) of each sampling point \(\) at all object NeRFs and derive the composite density \(\) using the mask information. Finally, we render the segmentation mask along the ray based on the composite density to yield the segmentation of pixel \(\).

ResultsWe compare our method with uORF and COLF, and present the results in Table 1. The comparison reveal that our method outperforms all baselines in terms of NV-ARI and ARI values, particularly the NV-ARI, in both Room-Chair and Room-Diverse scenes. It provides evidence that sVORF can effectively identify 3D objects from a single image with better multi-view consistency. Furthermore, in the more complex Room-Diverse scene, our approach achieves comprehensive and distinct improvements over other methods, indicating the robustness of our design. Moreover, we report the performance on CLEVR-3D in Table 1(a) to ensure equitable comparisons with ObSuRF and OSRT. Compared with OSRT, sVORF achieves significantly higher ARI\({}^{*}\) and similar FG-ARI\({}^{*}\) without using depth information, which further proves that our volume parameterization can alleviate mask bleeding problems, as shown in Figure 3. However, we encounter exceptions in the ARI and NV-ARI scores on the CLEVR-567 dataset, and the ARI\({}^{*}\) on CLEVR-3D dataset. Our benchmark scores are slightly lower than those of uORF and ObSuRF. We attribute this to the penalty imposed on reconstructing shadows of foreground objects, which are not included in the ground truth object masks. This inadequacy is illustrated in Figure 3. Since the light source remains fixed in the CLEVR dataset, it is straightforward to model the shadow as a translucent layer belonging to the object. Nonetheless, the high FG-ARI indicates that our proposed method is proficient in forming factorized representations, which can segment objects in a scene effectively.

    &  &  &  \\   & 3D metric & 2D metric & 3D metric & 2D metric & 3D metric & 2D metric \\   & NV-ARI \(\) & ARI \(\) & Fg-ARI\(\) & NV-ARI \(\) & ARI \(\) & Fg-ARI\(\) & NV-ARI \(\) & ARI \(\) & Fg-ARI\(\) \\  Slot Attention  & N/A & 3.5 & **93.2** & N/A & 38.4 & 40.2 & N/A & 17.4 & 43.8 \\ uORF  & **83.8** & **86.3** & 87.4 & 74.3 & 78.8 & 88.8 & 56.9 & 65.6 & 67.9 \\ COLF  & 46.6 & 59.5 & 92.6 & 83.5 & 83.9 & 92.4 & 54.5 & 70.7 & 71.7 \\ sVORF & 81.5 & 82.7 & 92.0 & **87.0** & **87.8** & **92.4** & **75.6** & **78.4** & **86.6** \\   

Table 1: Scene segmentation results. **Bold** and **U**nderline indicate state-of-the-art (SOTA) and the second best.

   Model & Supervision & ARI\({}^{*}\)\(\) & Fg-ARI\({}^{*}\)\(\) \\  ObSuRF  & image+depth & **94.6** & 95.7 \\ OSRT\({}^{}\) & image & 42.7 & **97.0** \\ sVORF & image & 86.0 & 96.3 \\    
   Model & LPIPS \(\) & SSIM \(\) & PSNR \(\) \\  ObSuRF  & N/A & N/A & 33.69 \\ OSRT\({}^{}\) & 0.0367 & 0.9719 & 36.74 \\ sVORF & **0.0258** & **0.9759** & **37.52** \\   

Table 2: Results on CLEVR-3D dataset. \({}^{}\)Model is re-evaluated using the unified test set for a fair comparison. **Bold** and **U**nderline indicate state-of-the-art (SOTA) and the second best.

Furthermore, to validate our method on more challenging scenarios, we conduct experiments on the MSN dataset and show the results in Table 4. Compared with ObSuRF, sVORF achieves significantly higher Fg-ARI and comparable ARI without using depth information, which demonstrates the model's ability to decouple more complex scenarios. Qualitative results are shown in Figure 2.

### Novel View Synthesis

SetupFor each test scene, we reserve one view as an input and use the other views to evaluate the the quality of reconstruction.

ResultsIn Table 3, sVORF outperforms all baselines on most metrics, despite not employing the coarse-to-fine training schedule. These findings suggest that our method is effective in producing high-quality images. The slightly lower LPIPS performance on Room-Chair and Room-Diverse can be attributed to the lack of perceptual loss in the sVORF training process, which does not explicitly optimize the LPIPS performance. Moreover, there exists a trade-off between structural and perceptual loss within the model. The excellent SSIM performance of our model indicates that the sVORF can generate complex object shapes. Specifically, as illustrated in Figure 3, our method generated diverse chair shapes with higher accuracy than COLF and uORF, even though it did not fully recover the background texture. In addition, the results presented in Table 2(b) indicate that sVORF outperforms OSRT in all metrics, despite using nearly half of the parameters as OSRT (48.73M v.s. 83.11M). Without the addition of depth information, our method synthesizes multi-view images with higher quality than obsurf, as shown in Table 4.

### Scene Design and Editing in 3D

SetupWe examine the potential of our method for basic scene editing on the Room-Chair dataset. Following uORF, our investigation involves two types of modifications, namely, moving objects and changing backgrounds. For editing the position of a foreground object, we move all query point coordinates on its object NeRF, based on the targeted movement. In order to relocate the slot, an affine transformation is applied to the 3D sample points before passing them to the corresponding object NeRF.

Meanwhile, for changing the background, we substitute the original background texture by replacing the original background slot feature with that of the target image.

   Model & \(}\) & \(}\) & \(\) \\  ObSuRF  & **64.1** & 81.4 & 27.41 \\ sVORF & 63.4 & **84.1** & **30.51** \\   

Table 4: Comparison on MSN dataset.

Figure 4: 3D scene manipulation for moving object and changing background.

    &  &  &  \\   & LPIPS \(\) & SSIM \(\) & PSNR \(\) & LPIPS \(\) & SSIM \(\) & PSNR \(\) & LPIPS \(\) & SSIM \(\) & PSNR \(\) \\  uORF  & 0.0859 & 0.8971 & 29.28 & 0.0821 & 0.8722 & 29.60 & 0.1729 & 0.7094 & 25.96 \\ COLF  & 0.0608 & 0.9346 & 31.81 & **0.0485** & 0.8934 & 30.93 & **0.1274** & 0.7308 & 26.02 \\ sVORF & **0.0211** & **0.9701** & **37.20** & 0.0824 & **0.8992** & **33.04** & 0.1637 & **0.7825** & **29.41** \\   

Table 3: Comparison on novel view synthesis from a single image.

Figure 2: Qualitative results of sVORF on MSN.

ResultsFigure 4 shows that edited images maintain a harmonious quality while accurately executing object movement and background replacement, affirming the strong correlation between slots and 3D objects in sVORF, as well as the accuracy of our model in 3D scene segmentation.

### Ablation Studies

In this section, we conduct ablation studies on the CLEVR-567 dataset to gain a deeper understanding of how the various parts contribute to the overall effectiveness of our approach. More ablation studies on architectures are detailed in Appendix C.

Role of Novel View SynthesisTo investigate the influence of the novel view synthesis setup on the training process, we modify our reconstructed target view to equal with the input view, _i.e._, we turns sVORF into a 2D image auto-encoder. As shown in Figure 5, the model divides images based on areas rather than objects. This division led to substantially lower ARI results, presented in Table 5. Above observations confirm the crucial role of viewpoint changes in scene decomposition. Specifically, the mapping between slots and 3D representations captures changes occurring in various regions due to differences in viewing angles between the target and source views. Changes that occur within regions belonging to the same object are more closely approximated, which allows the model to converge to K slots based on object clustering.

Connectivity RegularizationAs previously mentioned, novel view synthesis setup ensures distinctiveness between objects, while implementing Connectivity Regularization guarantees connectivity within the same objects. These two factors work together to achieve a clear decomposition of the scene. Figure 5 visualizes that our Connectivity Regularization ensures that points on an object belong to the same slot by preventing semi-transparent clouds from being modeled by object NeRFs.

Figure 3: **Qualitative Comparison.** We compare the reconstructions of the input view, a novel view, as well as the novel view segmentation using the uORF , COLF , OSRT , and our method on four datasets. Our method produces a finer segmentation and more precise shapes.

   Model & NV-ARI \(\) & FG-ARI \(\) \\  sVORF (w/o NVS) & 15.1 & 31.2 \\ sVORF (w/o CR) & 81.1 & 89.3 \\ sVORF (density-weighted) & 81.4 & 88.6 \\ sVORF (w/o SA) & 79.6 & 85.4 \\ sVORF (ours) & **81.5** & **92.0** \\   

Table 5: Ablation studies on the CLEVR-567 dataset.

Numerically, as shown in Table 5, the model's FG-ARI displays a significant increase from 89.3 to 92.0 when implementing connectivity regularization compared to the model without it.

Composing MechanismAs described in Section 3.3, our composing mechanism can perform well on small objects with few rays. To verify this, we implement the common density-weighted mean combination mode on our model and present the visualization in Figure 5. Our composing mechanism outperforms the density-weighted mean combination mode in decoupling effect, especially for small objects that may otherwise be segmented into attachments of other objects.

Self-Attention in Scene DecompositionIf an object in the input image is occluded, it may be mistakenly segmented into the slot of another object that corresponds to the occluded area. This issue can be resolved by leveraging the self-attention layer in the decomposition module as it facilitates the interaction between slots. As illustrated in the Figure 5, removing the self-attention layer leads to incorrect division of the occluded object, highlighting the importance of inter-slot interaction in preventing the collapse of the corresponding slot of the occluded object.

### Generalization

We conduct three sets of generalization experiments. The three subsets assess the ability of sVORF to generalize to unseen object appearances, unfamiliar spatial arrangements and grayscale images respectively.

Object AppearanceFor unseen object appearance, we follow the dataset in uORF, which is similar to CLEVR-567. The training set of this dataset excludes red cylinders and blue spheres, while the testing set includes only these types of objects. The results are presented in Table 6. It is noteworthy that our approach has never encountered the appearance of test objects but achieves comparable results to the models trained on a standard CLEVR-567 dataset.

Spatial ArrangementsTo assess the model's ability to generalize to a greater number of objects with unseen, challenging arrangements, we train our method using 11 slots on the CLEVR-567 dataset and evaluate on the packed-CLEVR-11 dataset provided by uORF. During training, we randomly mask off four slots to prevent slot collapse, but we utilize all slots during testing. The performance of our method on the unseen object arrangements test set is shown in Table 7 and is found to be reasonably well.

   Model & ARI \(\) & Fg-ARI\(\) \\  Slot-Attention  & 2.2 & N/A \\ uORF  & **85.5** & N/A \\ sVORF & 82.0 & 92.6 \\ sVORF(567) & 83.9 & **95.8** \\   

Table 6: Object Appearance

   Model & ARI \(\) & Fg-ARI\(\) \\  Slot-Attention  & 5.7 & N/A \\ uORF  & **83.2** & N/A \\ sVORF & 81.0 & **85.5** \\   

Table 7: Spatial Arrangements

Figure 5: Qualitative comparison of ablation studies on CLEVR-567 dataset. Specifically, we evaluate the impact of four techniques - Novel View Synthesis (NVS), Connectivity Regularization (CR), Composing Mechanism, and Self-Attention (SA).

RGB colorTo explore whether the sVORF mainly relies on RGB color for scene decomposition, we conduct an evaluation on a grayscale version of CLEVR-567 dataset. The model used in the evaluation is only trained on RGB CLEVR-567 dataset. The model achieves 87.5 FG-ARI on the grayscale test set, which is on par with 92.0 FG-ARI on the default RGB images. The evaluation results demonstrate that sVORF really learns to decompose the scene intrinsically. For qualitative results, see details in Appendix D.

### Object Segmentation on Real Images

We demonstrate the effectiveness of our approach in real-world scenarios by validating it on LLFF datasets. To handle complex scenarios, we implement VIT-Base  as the backbone instead of ResNet34  and train the network from scratch. Qualitative visualizations displayed in Figure 6 show that sVORF effectively segments the object within the scene, demonstrating the potential of our approach to understand general real scenarios.

### Training Speed and Memory Consumption

SetupWe compare sVORF with uORF  and COLF  in terms of memory consumption and training speed. For all three methods, we train models on the CLEVR-567 dataset using a batch size of 1 on V100 and record their memory usage and time taken for one epoch. Since both COLF and uORF render and supervise images at \(64 64\) resolution to learn the coarse structure first before supervising them at \(128 128\) resolution, we record their performance during both stages.

ResultsAs shown in Table 8, sVORF offers substantially shorter training time (3m24s vs. 9m2s) and lower memory consumption (4.6G vs. 23.7G) than the volumetric-decoder based uORF method, while displaying comparable performance to the light field-based COLF.

In terms of total training time, for the CLEVR-567 and Room-Chair datasets, we train sVORF for approximately 7 hours using 8 Nvidia RTX 2080 Ti GPUs with batch size 16. The uORF and COLF models are trained on an Nvidia RTX V100 GPU for approximately 7 and 2 days, respectively, with a batch size of 1. For the CLEVR3D dataset, sVORF is trained for approximately 2 days using 8 Nvidia RTX V100 GPUs with batch size 16, while OSRT is trained for approximately 1 day on 8 A100 GPUs with a batch size of 256. These results demonstrate the effectiveness of sVORF in overcoming the challenges of volumetric decoders that demand extensive resources, while also contributing to enhanced training efficiency.

## 5 Conclusion

We present sVORF, a novel method for 3D object-centric representation learning. By adopting volumetric rendering to synthesis novel views and using object slots as a guidance to compose volumetric object radiance fields, sVORF precisely decompose 3D scenes into individual objects with limited training resources. It significantly outperforms existing SOTA methods, particularly in complex multi-object scenes. In addition, we demonstrate sVORF's ability for realistic scenario decomposition, indicating a promising direction for understanding the physical world.

    &  &  \\   &  &  &  \\   & & coarse & fine & coarse & fine \\  Memory & 4.6G & 23.6G & 23.7G & 2.8G & 3.8G \\  Training Time & 3m24s & 8m42s & 9m2s & 1m56s & 2m22s \\   

Table 8: Memory Consumption and Performance Comparison.

Figure 6: Object segmentation on real images.