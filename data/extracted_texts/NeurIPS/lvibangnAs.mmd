# Unifying Generation and Prediction on Graphs with Latent Graph Diffusion

Cai Zhou\({}^{1,2}\), Xiyuan Wang\({}^{3,4}\), Muhan Zhang\({}^{3}\)

\({}^{1}\)Department of Automation, Tsinghua University

\({}^{2}\)Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology

\({}^{3}\)Institute for Artificial Intelligence, Peking University

\({}^{4}\)School of Intelligence Science and Technology, Peking University

caiz428@mit.edu, {wangxiyuan,muhan}@pku.edu.cn

Corresponding Author.

###### Abstract

In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) using one formulation. We first formulate prediction tasks including regression and classification into a generic (conditional) generation framework, which enables diffusion models to perform deterministic tasks with provable guarantees. We then propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder and decoder, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Leveraging LGD and the "all tasks as generation" formulation, our framework is capable of solving graph tasks of various levels and types. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across a wide range of generation and regression tasks.

## 1 Introduction

Graph generation has become of great importance in recent years, with important applications in many fields, including drug design  and social network analysis . However, compared with the huge success of generative models in natural language processing  and computer vision , graph generation is faced with many difficulties due to the underlying non-Euclidean topological structures. Existing models fail to generate structures and features simultaneously , or model arbitrary attributes .

Moreover, while general purpose foundation models are built in NLP  which can solve all types of tasks through the generative framework, there are no such general purpose models for graph data, since current graph generative models cannot address regression or classification tasks. People also have to train separate models for different levels (node, edge, and graph) of graph learning tasks. Therefore, it is beneficial and necessary to build a framework that can solve (a) tasks of all types, including generation, classification and regression; (b) tasks of all levels, including node, edge, and graph-level.

Our work.To overcome the difficulties, we propose Latent Graph Diffusion, **the first graph generative framework that is capable of solving tasks of various types and levels**. Our main contributions are summarized as follows.

* We conceptually formulate regression and classification tasks as conditional generation, and theoretically show that latent diffusion models can address them with provable guarantees.
* We present Latent Graph Diffusion (LGD), which applies a score-based diffusion generative model in the latent space encoded through a powerful pretrained graph encoder, thus overcoming the difficulties brought by discrete graph structures and various feature categories. Moreover, leveraging a specially designed graph transformer, LGD is able to generate node, edge, and graph features simultaneously in one shot with all feature types. We also design a special cross-attention mechanism to enable controllable generation.
* Combining the unified task formulation as generation and the powerful latent diffusion models, LGD is able to perform both generation and prediction tasks through generative modeling.
* Experimentally, LGD achieves state-of-the-art or highly competitive results across various tasks including molecule (un)conditional generation as well as regression and classification on graphs. Our code is available at https://github.com/zhouc20/LatentGraphDiffusion.

## 2 Related Work

Diffusion models.Recently, score-based generative models have demonstrated promising performance across a wide range of generation tasks. These models start by introducing gradually increasing noise to the data, and then learn the reverse procedure by estimating the score function, which represents the gradient of the log-density function with respect to the data. This process allows them to generate samples effectively. Two notable works in the realm of score-based generative models are score-matching with Langevin dynamics (SMLD) (Song and Ermon, 2019) and the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020). SMLD estimates the score function at multiple noise scales and generates samples using annealed Langevin dynamics to reduce the noise. In contrast, DDPM models the diffusion process as a parameterized Markov chain and learns to reverse the forward diffusion process of adding noise. Song et al. (2020) encapsulates SMLD and DDPM within the framework of stochastic differential equations (SDE). Song and Ermon (2020) and Song et al. (2020) further improved the scalability and sampling efficiency of score-based generative methods. More recently, Stable-Diffusion (Rombach et al., 2021) applied diffusion models to the latent space encoded by pretrained autoencoders, significantly improving computational efficiency and the quality of generated images. Our work has further applied latent space to graph tasks and also improved the generation quality.

Figure 1: Illustration of the Latent Graph Diffusion framework, which is capable of performing both generation and prediction.

Diffusion models for graphs.In the context of graph data, Niu et al. (2020) were the first to generate permutation-invariant graphs using score-based methods. They achieved this by introducing Gaussian perturbations to continuous adjacency matrices. Jo et al. (2022) took this step further by proposing a method to handle both node attributes and edges simultaneously using stochastic differential equations (SDEs). However, these diffusion models rely on continuous Gaussian noise and do not align well with discrete graph structures. To address this limitation, Haefeli et al. (2022) designed a diffusion model tailored to unattributed graphs and observed that discrete diffusion is beneficial for graph generation. Addressing this issue even more effectively, DiGress (Vignac et al., 2022), one of the most advanced graph generative models, employs a discrete diffusion process. This process progressively adds discrete noise to graphs by either adding or removing edges and altering node categories. Additionally, Limnios et al. (2023) proposes a method to scale DiGress by sampling a covering of subgraphs within a divide-and-conquer framework. To overcome the difficulties, instead of utilizing discrete noise, our work uses continuous noise in latent space, making our model structure well aligned with general diffusion models and achieves better performance. Our generative framework supports both DDPM and SMLD. Given their equivalence in SDE formulations (Song et al., 2020), we focus on DDPM and its extensions in our main text for illustration. More details on SMLD-based methods and SDE formulations can be found in Appendix A.

## 3 Regression and Classification as Conditional Generation

In this section, we use the notation \(\) to denote the known data, and \(\) to denote the labels to be predicted in the task. We use this unified notation not only for simplicity; actually, the formulation in this subsection is **generic** and not limited to graph data. Our framework is the first to show that **diffusion models can provably solve regression and classification tasks**.

From a high-level perspective, unconditional generation aims to model \(p()\), while conditional generation aims to model \(p(|)\). For classification and regression tasks, traditional deterministic models give point estimates to minimize classification or regression loss. However, from a generative perspective, predicting a point estimation is not the only solution - we can also model the complete conditional distribution \(p(|)\). Since modeling (conditional) distribution is exactly what generative models are capable of (and good at), it is possible to solve classification and regression tasks with generative models.

Actually, the classification and regression tasks can be viewed as **conditional generation tasks**. Intuitively, we only need to exchange the positions of data \(\) and \(\) in the conditional diffusion model; then we obtain a diffusion model that approximates \(p(|)\). In this case, we use the data \(\) as the condition and want to generate \(\) given the condition \(\). We add Gaussian noise to \(\) in the diffusion process, and we use the denoising network to reverse the process. In the parameterization where the denoising network \(_{}\) directly outputs the target instead of the noise analogous to Equation (8), i.e. \(}=_{}(_{t},t,)\), then the diffusion objective is

\[(_{}):=_{,,t}|| _{}(_{t},t,)-||_{2}^{2}\] (1)

It is straightforward to extend the above method to latent diffusion models, where the only difference is that the diffusion model operates in the latent space of \(,\). Since the formulation is generic, the method is also applicable to graph data; see Section 5.

Now we present our main theorem on the generalization bound of solving regression tasks with latent diffusion models, while more details on formal formulation and proof are given in Appendix B.

**Theorem 3.1**.: _Suppose Assumption B.1, Assumption B.2, Assumption B.3 hold, and the step size \(h:=T/N\) satisfies \(h 1/L\) where \(L 1\). Then the mean absolute error (MAE) of the conditional latent diffusion model in the regression task is bounded by_

\[ _{q}[||w^{}(,y)-w^{ }(,y)||]+_{1}\] (2) \[(q_{z}||^{d})}(-T)}_ {}++L(m_{1}+m_{2})h)}_{}+}}_{}+}_{}\] (3)

_where \(q_{z}\) is the ground truth distribution of \(:=w^{t}(,y)-()\)._Converting deterministic tasks such as regression and classification into generation has several potential advantages. First, generative models are capable of modeling the entire distribution, thus being able to provide information confidence interval or uncertainty by multiple predictions; in comparison, traditional deterministic models can only give point estimates. Secondly, generative models are often observed to have larger capacity and generalize well on large datasets (Kadkhodaie et al., 2024). Specifically for latent diffusion models, since the generative process is performed in the expressive latent space, the training of diffusion models could be much faster than training a deterministic model from scratch, and we obtain a powerful representation of the data that can be used in other downstream tasks. Moreover, now that the deterministic tasks could be unified with generation, thus can be simultaneously addressed by one unified model, which is the precondition of training a foundation model. We refer readers to Section 6 and Appendix C for detailed experimental observations.

## 4 Latent Graph Diffusion

As discussed in Section 1 and Section 2, existing graph diffusion models fail to generate both graph structures and graph features simultaneously. In this section, we introduce our novel **Latent Graph Diffusion (LGD)**, a powerful graph generation framework which is the first to enable **simultaneous structure and feature generation** in one shot. Moreover, LGD is capable of generating features of **all levels** (node-level, edge-level and graph-level) and **all types** (discrete or categorical, continuous or real-valued).

Notations.Given \(n\), let \([n]\) denote \(\{1,2,3,...,n\}\). For a graph \(G\) with \(n\) nodes, we denote the node set as \(v_{i},i[n]\), and represent the node feature of \(v_{i}\) as \(_{i}^{d_{v}}\). In the context of node pairs \((v_{i},v_{j})\), there can be either an edge \(e_{ij}\) with its edge feature \(_{ij}^{d_{e}}\) or no edge connecting \(v_{i}\) and \(v_{j}\). To jointly model the structures and features (of node, edge, graph level), we treat the case where no edge is observed between \(v_{i}\) and \(v_{j}\) as a special edge type. We denote this as \(e_{ij}\) with an augmented feature \(_{ij}=^{}^{d_{e}}\), where \(^{}\) is distinct from the features of existing edges, resulting in a total of \(n^{2}\) such augmented edges. We represent all node features as \(^{n d_{v}}\) where \(_{i}=_{i}\), and augmented edge features as \(^{n n d_{e}}\) where \(_{i,j}=_{ij}\) (adjacency matrix with edge features). Graph-level attributes, denoted \(^{d_{g}}\), can be represented by a virtual node or obtained by pooling over node and edge attributes (see Section 4.2 for details). For brevity, we temporarily omit graph-level attributes in the following text. Note that all attributes can be of arbitrary types, including discrete (categorical) and continuous (real-valued).

### Overview

Simultaneous generation of structures and features in the original graph space is challenging due to discrete graph structures and diverse graph features at various levels and types. Therefore, we apply our graph diffusion models in the latent space \(\) rather than operating directly on the original graphs. Graphs are encoded in the latent space using a **powerful pretrained graph encoder**\(_{}\) parameterized by \(\). In this latent space, all \(n\) nodes and \(n^{2}\) "augmented edges" have continuous latent representations. The node and edge representations are denoted as \(}=(,)\), where \(^{n d}\) and \(^{n n d}\), with \(d\) representing the hidden dimension.

\[=(,)=_{}(,)\] (4)

To recover the graph structures and features from the latent space \(\), we employ a **light-weight task-specific decoder**\(_{}\), which is designed to be lightweight and is parameterized by \(\). This decoder produces the final predicted properties of the graph for specific tasks, including node, edge, and graph-level attributes. It is important to note that we consider the absence of an edge as a specific type of edge feature. Furthermore, we can derive graph-level attributes from the predicted node features \(}\) and edge features \(}\), as explained in detail in Section 4.2.

\[(},})=_{}(})\] (5)

To ensure the quality of generation, the latent space \(\) must meet two key requirements: (a) it should be powerful enough to effectively encode graph information, and (b) it should be conducive to learning a diffusion model (Rombach et al., 2021), thus should not be too scattered nor high-dimensional.

Consequently, both the architectures and training procedures of the encoder \(_{}\) and the decoder \(_{}\) are of utmost importance. We will dive into the architecture and training details in Section 4.2.

After pretraining the encoder and decoder, we get a powerful latent space that is suitable for training a diffusion model. We can now train the generative model \(_{}\) parametrized by \(\) operating on the latent space. As explained, the generative model could have arbitrary backbone, including SDE-based and ODE-based methods. For illustration, we still consider a Denoising Diffusion Probabilistic Model (DDPM) (Ho et al., 2020), while other generative methods can be analyzed similarly. In the forward diffusion process, we gradually add Gaussian noise to the latent representation \(=(,)\) until the latent codes converge to Gaussian. In the backward denoising process, we use a denoising model parameterized by \(_{}\) parametrized by \(\) to reconstruct the latent representation of the input graph, obtaining \(}\). The network \(\) can be a general GNN or graph transformer, depending on tasks and conditions (see Appendix C.3). In the most general case, we use a graph transformer enhanced by expanded edges; see Section 4.3 for architecture and training details.

In conclusion, we have successfully overcome the challenges posed by discrete graph structures and diverse attribute types by applying a diffusion model in the latent space. Our model is the first to be capable of simultaneously generating all node, edge, and graph-level attributes in a single step. The overview of our latent graph diffusion is shown in Figure 1.

### Autoencoding

Architecture.In principle, the encoder can adopt various architectures, including Graph Neural Networks (GNNs) and graph transformers. However, to generate structures and features at all levels effectively, the encoder must be able to represent node, edge, and graph-level features simultaneously. As discussed in Section 4.1, to generate samples of high quality, the latent space \(\) should be both powerful and suitable for training a diffusion model. To fulfill these criteria, we employ a specially designed augmented edge-enhanced graph transformer throughout this paper, as elaborated in Section 4.3. In cases where input graph structures and features are complete, we also allow the use of message-passing-based Graph Neural Networks (MPNNs) and positional encodings (PEs) during the pretraining stage. However, these techniques may not be applicable for the denoising network, as the structures of the noised graphs become corrupted and ill-defined.

Since the latent space is already powerful, the decoder can be relatively simple. The decoder is task-specific, and we pretrain one for each task along with the encoder. In generation tasks, the decoder is responsible for recovering the structure and features of the graph. In classification tasks or regression tasks, it serves as a classifier or regression layer, respectively. In all cases, we set the decoder for each task as a single linear layer.

Training.Following Rombach et al. (2021) and Xu et al. (2023), we jointly train the encoder and decoder during the pretraining stage. This training process can be carried out in an end-to-end manner or similar to training autoencoders. The training objective encompasses a reconstruction loss and a regularization penalty.

\[=_{recon}+_{reg}\] (6)

The decoder \(\) reconstructs the input graph from the latent representation, denoted \((},})=()=(( {X},))\). The specific form of the reconstruction loss \(_{recon}\) depends on the type of data, such as cross-entropy or Mean Squared Error (MSE) loss. To force the encoder to learn meaningful representations, we consider the following reconstruction tasks: (i) reconstruct node features \(\) from node representations \(\); (ii) reconstruct edge features \(\) from edge representations \(\); (iii) reconstruct edge features \(_{ij}\) from representations of node \(i\) and \(j\); (iv) reconstruct node features \(_{i}\) from edge representations \(_{i}\); (v) reconstruct tuples \((_{i},_{j})\) from edge representations \(_{ij}\). When applicable, we opt to reconstruct node-level and edge-level positional encodings (PEs) according to the latent features. We can also employ masking techniques to encourage the encoder to learn meaningful representations. It is worth noting that downstream classification or regression tasks can be seen as a special case of reconstructing the masked graph; for more details, refer to Section 5.

To prevent latent spaces from having arbitrarily high variance, we apply regularization. This regularization can take the form of a KL-penalty aimed at pushing the learned latent representations towards a standard normal distribution (referred to as _KL-reg_), similar to the Variational Autoencoder(VAE) (Kingma and Welling, 2013). Alternatively, we can employ a vector quantization technique (referred to as _VQ-reg_), as seen in (van den Oord et al., 2017; Yang et al., 2023).

### Diffusion model

Architecture.Note that during both the forward and backward diffusion processes, we either add noise to the latent representations of nodes and edges or attempt to denoise them. When we need to generate the graph structures, the edge indexes are unknown, making the corrupted latent representations do not have a clear correspondence with the originaledge information. Consequently, message-passing neural networks (MPNNs) and similar methods are not applicable, as their operations rely on clearly defined edge indexes.

To address the ambiguity of edges in the noised graph, we require operations that do not depend on edge indexes. Graph transformers prove to be suitable in this context, as they compute attention between every possible node pair and do not necessitate edge indexes. However, we can still incorporate representations of augmented edges into the graph transformers through special design. To facilitate the joint generation of node, edge, and graph-level attributes, we adopt the most general graph transformer with augmented edge enhancement. We design a novel self-attention mechanism that could update node, edge and graph representations as described in Equation (58), see Appendix C.1 for details.

Training and sampling.Training the latent diffusion model follows a procedure similar to the one outlined in Appendix A, with the primary distinction being that training is conducted in the latent space. During this stage, both the encoder and the decoder remain fixed. Denoting \(_{0}=(,)\), in the forward process, we progressively introduce noise to the latent representation, resulting in \(_{t}\).

\[_{LGD}(_{}):=_{(,),_{t}(,)}||_{ }(_{t},t)-_{t}||_{2}^{2}\] (7)

where we implement the function \(_{}^{(t)}\) as described in Equation (18) using a time-conditional transformer, as detailed in Equation (58). There is also an equivalent training objective in which the model \(_{}\) directly predicts \(_{0}\) based on \(_{t}\) and \(t\), rather than predicting \(_{t}\).

\[_{LGD}(_{}):=_{(,) }||_{}(_{t},t)-_{0}||_{2}^{2}\] (8)

In the inference stage, we directly sample a Gaussian noise in the latent space \(\), and sample \(_{t-1}\) from the generative processes iteratively as described in Appendix A. After we get the estimated denoised latent representation \(}_{0}\), we use the decoder to recover the graph from the latent space, which finishes the generation process.

### Conditional generation

Similarly to other generative models (Rombach et al., 2021), our LGD is also capable of controllable generation according to given conditions \(\) by modeling conditional distributions \(p(|)\), where \(\) is the random variable representing the entire latent graph representation for simplicity. This can be implemented with conditional denoising networks \(_{}(,t,)\), which take the conditions as additional inputs. Encoders and decoders can also be modified to take \(\) as additional input to shift latent representations towards the data distribution aligned with the condition \(\).

General conditions.Formally, we preprocess \(\) from various data types and even modalities (e.g. class labels or molecule properties) using a domain specific encoder \(\), obtaining the latent embedding of the condition \(()^{m d_{}}\), where \(m\) is the number of conditions and \(d_{}\) the hidden dimension. The condition could be embedded into the score network through various methods, including simple addition or concatenation operations (Xu et al., 2023) and cross-attention mechanisms (Rombach et al., 2021). We provide details of the cross-attention mechanisms in Appendix C.1.

The conditional LGD is learned via

\[_{LGD}:=_{(,),,_{t }(,),t}||_{}(_ {t},t,()-_{t}||_{2}^{2}\] (9)

if the denoising network is parameterized to predict the noise; \(_{}\) can also predict \(_{0}\) conditioning on \(_{t},t,()\) analogous to Equation (8).

Masked graph.Consider the case where we have a graph with part of its features known, and we want to predict the missing labels which can be either node, edge, or graph level. For example, in a molecule property prediction task where the molecule structures and atom/bond types are known, we want to predict its property. This can be modeled as a problem of predicting a masked graph-level feature (molecule property) given the node and edge features. In this case, the condition is the latent representation of a graph that is partially masked. We denote the features of a partially masked graph as \((^{c},^{c},^{c})\), where the superscript \(c\) implies that the partially masked graph is the "condition", \(^{c}_{i}=^{c}_{i}\), \(^{c}_{i,j}=^{c}_{ij}\), \(^{c}\) are observed node, edge and graph-level features respectively. The missing labels we can to predict are denoted as \(\), then the complete graph \((,,)\) can be recovered by \((^{c},^{c},^{c})\) and \(\). Intuitively, this is similar to image inpainting, where we hope to reconstruct the full image based on a partially masked one. Therefore, we can model the common classification and regression tasks as a conditional generation, where we want to predict the label \(\) (or equivalently, the full graph feature \((,,)\)) given the condition \((^{c},^{c},^{c})\), see Section 5 for formal formulations. In this case, as the condition is a graph as well (though it might be partially masked), we propose a novel cross-attention for graphs as shown in Equation (60), see Appendix C.1 for more details.

## 5 Unified Task Formulation as Generation

Based on graph generative model on the latent space, we can address generation tasks of all levels (node, edge, and graph) using one LGD model. In this section, we detail how tasks of different types can be formulated as generation, thus becoming tractable using one generative model.

Now we summarize our Latent Graph Diffusion framework, which is able to (1) solve tasks of all types, including generation, regression, and classification through the framework of graph generation; and (2) solve tasks of all levels, including node, edge and graph level. See Figure 1 for illustration.

First, since LGD is an internally generative model, it is able to perform unconditional and conditional generation as discussed in Section 4. For nongenerative tasks including regression and classification, we formulate them into conditional generation in Section 3, thus can also be solved by our LGD model. In particular for graph data, we want to predict the labels \(\) which can be node, edge, or graph level and of arbitrary types, given the condition of a partially observed graph \(^{c},^{c},^{c}\). To better align with the second target of full graph generation, we model this problem as predicting \(p(,,|^{c},^{c},^{c})\), where \((,,)=(^{c},^{c},^{c},)\) is the full graph feature that combines the observed conditions and the labels to be predicted. In other words, the condition can be viewed as a partially masked graph whose masked part is the labels \(\), and our task becomes graph inpainting, i.e. generate full graph features conditioning on partially masked features.

Now we have formulated all tasks as generative modeling. The feasibility of the second goal to predict all-level features is naturally guaranteed by the ability of our augmented-edge enhanced transformer architecture described in Section 4. We leave the discussion of the possibility of solving tasks from different domains to Appendix D.2.

## 6 Experiments

In this section, we use extensive experiments that cover tasks of different types (regression and classification) and levels (node, edge and graph) to verify the effectiveness of LGD. We first conduct experiments on traditional generation tasks to verify LGD's generation quality. We consider both unconditional generation and conditional generation tasks. We then show that utilizing our unified formulation, LGD can also perform well on prediction tasks. To the best of our knowledge, we are the first to address regression and classification tasks with a generative model efficiently. More experimental details and additional experimental results can be found in Appendix C.

### Generation task

For both unconditional and conditional generation, we use QM9 (Ramakrishnan et al., 2014), one of the most widely adopted datasets in molecular machine learning research, which is suitable for both generation and regression tasks. QM9 contains both graph and 3D structures together with several quantum properties for 130k small molecules, limited to 9 heavy atoms. We provide more generation results on larger dataset MOSES (Polykovskiy et al., 2020) in Appendix C.

[MISSING_PAGE_FAIL:8]

We report EDM (Hoogeboom et al., 2022) and GeoLDM (Xu et al., 2023) as baseline models. We also incorporate (a) the MAE of the regression model \(\) of ours and (Xu et al., 2023), which can be viewed as a lower bound of the generative models; (b) _Random_, which shuffle the labels and evaluate \(\), thus can be viewed as the upper bound of the MAE metric; (c) \(N_{ atoms}\), which predicts the properties based only on the number of atoms in the molecule.

As shown in Table 2, even if we do not use 3D information and generate both atoms and bonds in one simultaneously, LGD achieves the best results in 4 out of 6 properties. The results verify the excellent controllable generation capacity of LGD.

### Prediction with conditional generative models

We evaluate LGD extensively on prediction tasks, including regression and classification tasks of different levels. More results can be found in Appendix C.

Regression.For regression task, we select ZINC12k (Dwivedi et al., 2020), which is a subset of ZINC250k containing 12k molecules. The task is molecular property (constrained solubility) regression, measured by MAE. We use the official split of the dataset.

Note that we are the first to use generative models to perform regression tasks, therefore no comparable generative models can be selected as baselines. Therefore, we choose traditional regression models, including GIN (Xu et al., 2018), PNA (Corso et al., 2020), DeepLRP (Chen et al., 2020), OSAN (Qian et al., 2022), KP-GIN+ (Feng et al., 2022), GNN-AK+ (Zhao et al., 2021), CIN (Bodnar et al., 2021) and GPS (Rampasek et al., 2022) for comparison.

We train our LGD model and test inference with both DDPM and DDIM methods. While the generated predictions are not deterministic, we only predict once for each graph for a fair comparison with deterministic regression models. We will show in Appendix C that ensemble techniques can further improve the quality of prediction. As shown in Table 3, LDM (with DDPM) achieves the best results, even outperforming the powerful graph transformers GPS (Rampasek et al., 2022). In comparison, the regression model with the same graph attention architecture as the score network in LGD can only achieve a worse \(0.084 0.004\) test MAE, which validates the advantage of latent diffusion model over traditional regression models. We also observe that inference with DDIM is much faster, but may lead to a performance drop, which aligns with previous observations (Song et al., 2020; Cao et al., 2023). Moreover, our LGD requires much less training steps compared with GraphGPS, see Appendix C for more detailed discussions on experimental findings.

Classification.We choose node-level tasks for classification. Datasets include co-purchase graphs from Amazon (Photo) (Shchur et al., 2018), coauthor graphs from Coauthor (Physics) (Shchur et al., 2018), and the citation graph OGBN-Arxiv with over \(169\)K nodes. The common \(60\%,20\%,20\%\) random split is adopted for Photo and Physics, and the official split based on publication dates of the papers is adopted for OGBG-Arxiv.

We choose both classical GNN models and state-of-the-art graph transformers as baselines. GNNs include GCN (Kipf and Welling, 2016), GAT (Velickovic et al., 2017), GraphSAINT (Zeng et al., 2020), GRAND+ (Feng et al., 2022). Graph transformers include Graphormer (Ying et al., 2021), SAN (Kreuzer et al., 2021), GraphGPS (Rampasek et al., 2022), and the scalable Exphormer (Shirzad et al., 2023) and NAGphormer (Chen et al., 2023).

As reported in Table 4, our LGD not only scales to these datasets while a number of complex models like GraphGPS Rampasek et al. (2022) fail to do so, but also achieves the best results, even outperforming those state-of-the-art graph transformers including Exphormer (Shirzad et al., 2023) and NAGphormer (Chen et al., 2023). Overall, our LGD reveals great advantages in both scalability and task performance.

   Method & Test MAE \\  GIN & \(0.163 0.004\) \\ PNA & \(0.188 0.004\) \\ GSN & \(0.115 0.012\) \\ DeepLRP & \(0.223 0.008\) \\ OSAN & \(0.187 0.004\) \\ KP-GIN+ & \(0.119 0.002\) \\ GNN-AK+ & \(0.080 0.001\) \\ CIN & \(0.079 0.006\) \\ GPS & \(0.070 0.004\) \\  LGD-DDIM (ours) & \(0.081 0.006\) \\ LGD-DDPM (ours) & \( 0.003\) \\   

Table 3: Zinc12K results (MAE \(\)). Shown is the mean \(\) std of 5 runs.

## 7 Conclusions and Limitations

We propose Latent Graph Diffusion (LGD), the first graph generative framework that is capable of solving tasks of all types (generation, regression and classification) and all levels (node, edge, and graph). We conceptually formulate regression and classification tasks as conditional generation, and show that latent diffusion models can complete them with provable theoretical guarantees. We then encode the graph into a powerful latent space and train a latent diffusion model to generate graph representations with high qualities. Leveraging specially designed graph transformer architectures and cross-attention mechanisms, LGD can generate node, edge, and graph features simultaneously in both unconditional or conditional settings. We experimentally show that LGD is not only capable of completing these tasks of different types and levels, but also capable of achieving extremely competitive performance. We believe that our work is a solid step towards graph foundation models, providing fundamental architectures and theoretical guarantees. We hope it could inspire more extensive future research.

There are still some limitations of this paper that could be considered as future work. First, although the LGD is a unified framework, we train models separately for each task in our experiments. To build a literal foundation model, we need to train a single model that can handle different datasets, even from different domains. This requires expensive computation resources, more engineering techniques and extensive experiments. Second, it would be meaningful to verify the effectiveness of utilizing diffusion to perform deterministic prediction tasks in other domains, such as computer vision.