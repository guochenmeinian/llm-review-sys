# 4-bit Shampoo for Memory-Efficient Network Training

Sike Wang

Beijing Normal University

sikewang@mail.bnu.edu.cn

&Pan Zhou

Singapore Management University

panzhou@smu.edu.sg

Jia Li

Beijing Normal University

jiali@bnu.edu.cn

&Hua Huang

Beijing Normal University

huahuang@bnu.edu.cn

Code is available at https://github.com/Sike-Wang/low-bit-Shampoo.

Beijing Normal University

huahuang@bnu.edu.cn

###### Abstract

Second-order optimizers, maintaining a matrix termed a preconditioner, are superior to first-order optimizers in both theory and practice. The states forming the preconditioner and its inverse root restrict the maximum size of models trained by second-order optimizers. To address this, compressing 32-bit optimizer states to lower bitwidths has shown promise in reducing memory usage. However, current approaches only pertain to first-order optimizers. In this paper, we propose the first 4-bit second-order optimizers, exemplified by 4-bit Shampoo, maintaining performance similar to that of 32-bit ones. We show that quantizing the eigenvector matrix of the preconditioner in 4-bit Shampoo is remarkably better than quantizing the preconditioner itself both theoretically and experimentally. By rectifying the orthogonality of the quantized eigenvector matrix, we enhance the approximation of the preconditioner's eigenvector matrix, which also benefits the computation of its inverse 4-th root. Besides, we find that linear square quantization slightly outperforms dynamic tree quantization when quantizing second-order optimizer states. Evaluation on various networks for image classification and natural language modeling demonstrates that our 4-bit Shampoo achieves comparable performance to its 32-bit counterpart while being more memory-efficient1.

## 1 Introduction

Deep neural networks (DNNs) have achieved great success in numerous fields, e.g., computer vision , natural language processing , and speech recognition . A significant part of such success is attributed to first-order optimizers such as stochastic gradient descent with momentum (SGDM)  and AdamW . Second-order optimizers, including K-FAC , Shampoo , AdaBK , CASPR , and Sophia , show great convergence properties, but often involve noticeable computation and memory costs. Anil et al.  provided several practical techniques for second-order optimizers to achieve substantial wall-clock time improvements over traditional first-order optimizers. The fast convergence property of second-order optimizers benefits from preconditioning the gradient with a matrix known as a preconditioner. The optimizer states for constructing the preconditioner and its inverse root can speed up optimization compared to first-order optimizers, but consume memory that could be used for model parameters, limiting the maximum model size trained within a given memory budget. With the increase in model size, the memory utilized by optimizer states can become a predominant factor in memory usage. This is the primary obstacle hindering the widespread use of second-order optimizers in the era of large models.

There are two main attempts to reduce memory consumed by optimizer states. Factorization uses low-rank approximation to optimizer states. This strategy has been applied to first-order optimizers and second-order optimizers [14; 40]. In a comparable but distinct line of work, quantization utilizes low-bit to compress 32-bit optimizer states. Quantization is attractive due to its simplicity and wide applicability, which has been applied to first-order optimizers [8; 26]. Applying quantization to second-order optimizers poses a greater challenge, as first-order optimizers' states are elementwise, whereas second-order optimizers rely on matrix operations. To our knowledge, it has not been attempted before.

**Contributions:** In this paper, we present the first second-order optimizers with 4-bit optimizer states by taking Shampoo  as an example, while preserving the performance achieved with 32-bit optimizer states. While our focus is on Shampoo, we believe that our approach could also be applied to other second-order optimizers (see Table 4). Our main contributions are highlighted below.

Firstly, to maintain 32-bit performance, we propose quantizing the eigenvector matrix of a preconditioner in 4-bit Shampoo, rather than the preconditioner itself. The reason is that the small singular values of the preconditioner matter. Directly quantizing the preconditioner via block-wise quantization  at 4-bit precision can significantly alter the small singular values, leading to a drastic change in its inverse 4-th root and thus harming 4-bit Shampoo's performance. Quantizing the eigenvector matrix can help alleviate this issue, which is supported by experimental validation and theoretical insight. Additionally, with the eigenvector matrix, computing the inverse 4-th root is straightforward, ensuring that quantizing the eigenvector matrix does not lead to a rise in the total wall-clock time compared to quantizing the preconditioner (see Figure 1).

Secondly, we present two techniques for enhancing performance. As the eigenvector matrix of a preconditioner is orthogonal, we apply Bjorck orthonormalization  to rectify the orthogonality of the quantized eigenvector matrix, leading to improved approximation of preconditioner's eigenvector matrix and facilitating computation of its inverse 4-th root. Additionally, we observe that linear square quantization outperforms dynamic tree quantization  marginally when quantizing second-order optimizer states. The superiority of our developed 4-bit Shampoo is demonstrated in Figure 1.

Finally, we evaluate our 4-bit Shampoo on different image classification and natural language modeling tasks using convolutional neural network (CNN) and transformer architectures. Across all these benchmarks, our 4-bit Shampoo achieves similarly fast convergence comparable to its 32-bit counterpart, with no significant increase in losses for the trained models. Our 4-bit Shampoo uses less memory than its 32-bit counterpart, allowing for training of larger models with given resources.

## 2 Preliminaries

In this section, we present Shampoo and its implementation in our experiments. We also discuss quantization-based compression methods in a general formulation.

**Notations.** We use a non-bold letter like \(a\) or \(A\) to denote a scalar, a boldfaced lower-case letter like \(\) to denote a vector, and a boldfaced upper-case letter such as \(\) to denote a matrix. \(\!=\![u_{i}]^{}\) means that the \(i\)-th element of column vector \(\) is \(u_{i}\) and \(\!=\![_{i}]\) means the \(i\)-th column vector of matrix \(\) is \(_{i}\). Let \(\) be a positive definite (PD) matrix and \(s\), we define \(^{s}\!=\!^{s}^{}\), where \(^{}\) is the Singular Value Decomposition (SVD) of \(\). \(()\) represents the trace of a matrix \(\). The inner product of two matrices \(\) and \(\) is denoted as \(,\!=\!(^{})\). The Frobenius norm of a matrix \(\) is \(\|\|_{F}\!=\!,}\). \(\) means the elementwise matrix product (Hadamard product).

Figure 1: Visualization of test accuracies and total GPU memory costs of vision transformers. 4-bit Shampoo (naive) quantizes the preconditioner, while 4-bit Shampoo (our) quantizes its eigenvector matrix.

\(()\) is a diagonal matrix with diagonal vector \(\), while \(()\) means the diagonal vector of matrix \(\).

### Shampoo for Matrices

The update rule of Shampoo in the matrix case combined with a first-order optimizer \(\) is

\[(_{t-1},_{t-1},_{ t-1},_{t-1},_{t})=_{t}\!=\!_{t-1}\!+\!_{t} _{t}^{}\\ _{t}\!=\!_{t-1}\!+\!_{t}^{}_{t}\\ }_{t}\!=\!_{t}^{-1/4}_{t}_{t}^{-1/4}\\ }_{t}\!=\!}_{t}(\|_{t}\|_{F}/\|}_{t}\|_{F})\\ _{t},_{t}\!=\!(_{t-1},_{t-1},}_{t})\] (1)

where \(_{t}\) is the model parameters in matrix form, \(_{t}\) and \(_{t}\) are called preconditioners, \(_{t}\) is the optimizer state of \(\), and \(_{t}\) is the gradient at \(_{t-1}\). Note that \(_{t},_{t}\), \(_{t}^{-1/4}\), and \(_{t}^{-1/4}\) are PD matrices. The penultimate step in (1) is the grafting trick , which enables Shampoo to roughly apply the well-tuned learning rate schedule of \(\). The optimization variable \(_{t}\) does not represent all model parameters. It denotes a tensor of the model  or one block of a tensor . In practice, we adopt an efficient and effective implementation of Shampoo for training DNNs following [2; 41] as described in Algorithm 4. In order to achieve efficient training, \(_{t},_{t}\), \(_{t}^{-1/4}\), and \(_{t}^{-1/4}\) are computed once every few hundred iterations. In this case, besides \(_{t}\) and \(_{t}\), their inverse 4-th roots should also be stored in memory, as computing them is computationally expensive. So training large models with Shampoo can be memory-intensive, consuming a significant amount of memory.

### Quantization-based Compression Methods

Quantizing updated optimizer states using a quantizer and then dequantizing them with a dequantizer prior to use is an effective method for conserving memory. We focus exclusively on vectors, as tensors can be reshaped into vectors.

**Quantization.** According to the idea in [8; 26], a \(b\)-bit quantizer \(\) for \(p\)-dimensional real vectors is a mapping given by

\[=(,):^{p} _{b}^{p}^{p},\]

where \(\) is a normalization operator on \(^{p}\), \(\) is an elementwise function mapping any real number to an element of \(_{b}\!=\!\{0,1,,2^{b}-1\}\), and \(\) is a maximum operator on \(^{p}\). For any \(^{p}\), \(\) and \(\) satisfy \(()()\!=\!\).

A normalization operator \(\) for \(p\)-dimensional vectors is a transformation on \(^{p}\). It scales each element of a vector \(^{p}\) into \([-1,1]\). A block-wise normalization operator for a \(p\)-dimensional vector \(=[x_{1},x_{2},,x_{p}]^{}\) is defined as

\[()_{i}=}{_{j_{i}}\{x_{j}\}},\]

where \(()_{i}\) is the \(i\)-th element of \(()\), and \(_{i}\) is a set satisfying \(i_{i}\{1,,p\}\). Usually, \(_{i}\) should also satisfy \(_{i}\!=\!_{j}\) or \(_{i}\!_{j}\!=\!\) for \(i,j\{1,,p\}\). In this case, for any \(^{p}\), the number of different elements in \(()\) is equal to the number of elements in set \(\{_{i}|i=1,,p\}\). Meanwhile, the number of the elements in \(_{i}\) for any \(i\) should be as close as possible to a value called block size.

The mapping \(\) for \(x\) in a \(b\)-bit quantizer \(\) is defined as

\[(x)=_{b}}{}|x-( j)|,\]

where \(\) named quantization mapping is an elementwise function that maps any element in \(_{b}\) into \([-1,1]\), and \(||\) is the absolute operator for a scalar. There are three typical quantization mappings: linear quantization, dynamic quantization, and quantile quantization. Their specifications and visualizations can be found in .

**Dequantization.** Given a \(b\)-bit quantizer \(\!=\!(,)\) for a \(p\)-dimensional real vector \(^{p}\), the corresponding dequantizer \(\) is a mapping defined as

\[(())\!=\!(( ),())\!=\!(())():_{b}^{p}^{p} ^{p}.\]Methodology

In this section, we describe the design of our quantization-based compression method to realize 4-bit Shampoo with fast and high precision quantization. Let \(\!=\!(,)\) be a quantizer and \(\) be its corresponding dequantizer as described in Subsection 2.2.

### Quantizing the Eigenvector Matrices

A naive approach to realize 4-bit Shampoo is applying the compression methods proposed in [8; 26] to \(_{t}\), \(_{t}\), \(_{t}^{-1/4}\), and \(_{t}^{-1/4}\) in Shampoo (see (1)). A slightly improved approach is to quantize the four PD matrices excluding their diagonal elements, which are typically much larger than their non-diagonal counterparts due to the non-negativity of the elements in \((_{t}_{t}^{})\) and \((_{t}^{}_{t})\).

However, the naive approach can cause large quantization errors at 4-bit precision. This is because the quantization errors (or called perturbations) of quantizing \(_{t}\) and \(_{t}\) will transfer to \(_{t}^{-1/4}\) and \(_{t}^{-1/4}\). To verify this, we first introduce two criteria to evaluate the quantization errors of matrices. We do not use the elementwise criterion in . Let \(\) denote a 32-bit matrix, \(g\) represent a transformation (can formed by quantization), and \(f\) stand for a mapping, e.g., \(f()\!=\!^{-1/4}\). Then we define the normwise relative error (NRE) and angle error (AE) in \(f\) of \(g\) at \(\) as

\[\!=\!)-f(g())\|_{F}}{\|f()\|_{F}}, \!=\!(),f(g())}{(\|f( )\|_{F}\|f(g())\|_{F}}).\]

We choose two PD matrices of order 1200. The first one \(_{1}\) is derived from the real world. It is a preconditioner in 32-bit Shampoo combined with AdamW for training a Swin-Tiny model. The second one \(_{2}\!=\!^{}\) is synthetic, constructed from a random orthogonal matrix \(\) and a diagonal matrix \(\) with only two distinct diagonal values. Table 1 shows the quantization errors in \(f()\!=\!^{-1/4}\) of the naive approach at these two matrices, which are remarkably high. More analyses are given in Appendix D. The key point is that the singular values of \(_{i}(i\!=\!1,2)\) follow a specific distribution (see Figure 2). In this scenario, a slight perturbation of \(_{i}\) will significantly alter its small singular values, resulting in a drastic change to \(_{i}^{-1/4}\).

To address this issue, we propose quantizing the eigenvector matrix of a preconditioner in Shampoo, rather than the preconditioner itself. Namely, a preconditioner \(\) is a PD matrix, and its SVD is \(^{}\), where \(\) represents the eigenvector matrix and \(\) denotes the singular value matrix. Given that \(\) is a diagonal matrix, we can focus on quantizing \(\) using \(\) while leaving \(\) unchanged. From Table 1, one can observe that quantizing \(\) can significantly reduce the quantization errors. We will theoretically discuss the advantages of quantizing \(\) compared to quantizing \(\) in Section 4. In practice, the randomized SVD method  is adopted to compute the SVD of \(\) efficiently, as shown in . We want to highlight that quantizing the original \(_{t}\) and \(_{t}\) in Shampoo involves significant computational burdens to compute their inverse 4-th roots \(_{t}^{-1/4}\) and \(_{t}^{-1/4}\), whereas quantizing the eigenvector matrices of \(_{t}\) and \(_{t}\) allows for rapid inverse root calculation. So the computational time required for both approaches is comparable (see Figure 1).

   \!=\!_{1}\)} & \!=\!_{2}\)} \\  Mapping \(\) & Bit & QM & OR & NRE \(\) & AE (\({}^{}\!\)) \(\) & Mapping \(\) & Bit & QM & OR & NRE \(\) & AE (\({}^{}\!\)) \(\) \\   & 8 & \(\) & ✗ & 0.2192 & 8.3014 &  & 8 & \(\) & ✗ & 0.1896 & 10.877 \\  & 4 & \(\) & ✗ & 0.6241 & 17.319 &  & 4 & \(\) & ✗ & 0.4615 & 17.189 \\  & 4 & \(\) & ✗ & 0.0709 & 4.0426 & & 4 & \(\) & ✗ & 0.1224 & 7.0144 \\  & 4 & \(\) & ✓ & 0.0455 & 2.5615 & & 4 & \(\) & ✓ & 0.0878 & 4.9960 \\   & 8 & \(\) & ✗ & 0.2164 & 7.9751 &  & 8 & \(\) & ✗ & 0.1310 & 7.4717 \\  & 4 & \(\) & ✗ & 0.6243 & 17.293 &  & 4 & \(\) & ✗ & 0.4465 & 15.338 \\  & 4 & \(\) & ✗ & 0.0543 & 3.1066 & & 4 & \(\) & ✗ & 0.0942 & 5.3998 \\  & 4 & \(\) & ✓ & 0.0343 & 1.9456 & & & 4 & \(\) & ✓ & 0.0669 & 3.8166 \\   

Table 1: Quantization errors in \(^{-1/4}\) of different quantization schemes at a PD matrix \(\). We employ block-wise normalization with a block size of 64. \(\) is the eigenvector matrix of \(\), QM = quantized matrix, and OR = orthogonal rectification.

### Rectifying the Orthogonality of Eigenvector Matrices

Let \(\) be a PD matrix with SVD \(^{}\). Note that the eigenvector matrix \(\) is orthogonal, whereas \(\!=\!(())\) may not be. To further mitigate the quantization errors mentioned in Subsection 3.1, we propose employing Bjorck orthonormalization  to orthogonalize \(\). Particularly, given \(_{0}\!=\!\), we iterate

\[_{t}\!=\!1.5_{t-1}\!-\!0.5_{t-1}_{t-1}^{ }_{t-1},\] (2)

for \(t_{1}\!\!1\) times and take \(_{t_{1}}\) as the rectified result. Equation (2) can also be interpreted as the gradient descent of problem \(_{}\|^{}-\|_{F}^{2}\) using a step size of 0.5, where \(\) denotes the identity matrix. We empirically find that only one iteration (i.e., \(t_{1}\!=\!1\)) is enough. Table 1 illustrates the benefit of rectifying \(\) into \(_{1}\).

The update frequencies for the preconditioners and their inverse 4-th roots differ (see Algorithm 3). Given \(\) and \(\), we also require orthogonal rectification to compute \(^{s}\) rapidly for any \(s\). The reason is as follows. It is easy to compute \(^{s}\!=\!^{s}^{}\) by definition. However, \(^{s}^{}\) can be very sensitive to the orthogonality of \(\) for \(s<0\), making \(^{s}^{}\) largely deviate from \((^{})^{s}^{s}\). Similarly, we can approximate \(^{s}\) by \(_{t_{2}}^{s}_{t_{2}}^{}\), where \(_{t_{2}}\) is generated by (2). Figure 3 illustrates the elementwise mean errors between \((_{t_{2}}^{s}_{t_{2}}^{})^{-1/s}(_{t_{ 2}}_{t_{2}}^{})\) and \(\) for various \(s\) and \(t_{2}\), where \(\) is the real-world matrix used in Table 1. Based on the observation from Figure 3, we set \(t_{2}\!=\!4\) in our experiments.

### Selecting the Quantizer

The quantizer \(\) is defined by the normalization operator \(\) and mapping \(\), and \(\) is determined by \(_{i}\). Since an eigenvector has a unit length, the elements in \(_{i}\) should belong to the same column of an eigenvector matrix, i.e., they are from the same eigenvector. Instead of employing dynamic tree (DT) quantization as mapping \(\), we recommend utilizing linear square (Linear-2) quantization as \(\), particularly when \(b\!=\!4\). Linear-2 quantization is defined as

\[(j)=-(-1+2j/(2^{b}\!-\!1))^{2},&j\!<\!2^{b \!-\!1}\!-\!1;\\ 0,&j\!=\!2^{b\!-\!1}\!-\!1;\\ (-1\!+\!2j/(2^{b}\!-\!1))^{2},&j\!>\!2^{b\!-\!1}\!-\!1,\] (3)

where \(j\!\!_{b}\!=\!\{0,1,,2^{b}\!-\!1\}\). As shown in Table 1, Linear-2 quantization has lower quantization errors compared to DT quantization at 4-bit precision.

### Overall Algorithm

We first describe the update processes of the preconditioners and their inverse 4-th roots in our 4-bit Shampoo. A preconditioner \(\) is a PD matrix and its SVD is \(^{}\). We can compress \(\) into a pair \((,})=((),())\) and decompress it into \((,)=((),(}))\). Algorithm 1 (Preconditioner Update, PU) shows the update rule of \(\). Similarly, we compress \(}^{-1/4}\) into a pair \((,})=((}), (}\!-\!()))\) and decompress it into \(()+(})\). Algorithm 2(Preconditioner's Inverse 4-th Root Update, PIRU) gives the update rule of \(}\). Based on the above update rules, we can summarize our 4-bit Shampoo in Algorithm 3. Note that we omit some input parameters of \(\) and \(\) because they can be found in Algorithm 3 in the same form.

```
0: singular value vector \(\), quantized eigenvector matrix \(}\), \(\), number of iterations \(t_{1}\) for rectification, exponential decay rate \((0,1)\), \(\) and \(\)
1:\(=(),=(})\)
2: Rectify \(\) by iterating (2) \(t_{1}\) times
3:\(=^{}+(1-)\)
4: Compute \(=^{}\) by randomized SVD
5:return\((),()\) ```

**Algorithm 1**\((,},)\)

```
0:\(_{0}^{m n}\), \(_{0}=_{m}\), \(_{0}=_{n}\), \(}_{0}=_{m}\), \(}_{0}=_{n}\), \((0,1)\), \(t_{1}\), \(t_{2}\), update interval \(T_{1}\), update interval \(T_{2}\), total number of steps \(T\), first-order optimizer \(\), first-order optimizer state \(_{0}=\), 4-bit quantizer \(\) and its corresponding dequantizer \(\).
0: final parameter \(_{T}\).
1:\(_{0,L}=(_{0}),}_{0,L}=(_{m})\); \(_{0,R}=(_{0}),}_{0,R}= (_{n})\)
2:\(_{0}=(}_{0}),}_{0}=()\); \(_{0}=(}_{0}),}_{0}= ()\)
3:for\(t=1,2,,T\)do
4: Receive loss function \(_{t}:^{m n}\) and compute gradient \(_{t}=_{t}(_{t})\)
5:if\(t\%T_{1} 0\)then
6:\(_{t,L},}_{t,L}\!=\!(_{t-1,L },}_{t-1,L},_{t}_{t}^{})\); \(_{t,R},}_{t,R}\!=\!(_{t-1,R },}_{t-1,R},_{t}^{}_{t})\)
7:else
8:\(_{t,L},}_{t,L}=_{t-1,L},}_{t-1,L}\); \(_{t,R},}_{t,R}=_{t-1,R},}_{t-1,R}\)
9:if\(t\%T_{2} 0\)then
10:\(_{t},}_{t}=(_{t,L},}_{t,L})\); \(_{t},}_{t}=(_{t,R},}_{t,R})\)
11:else
12:\(_{t},}_{t}=_{t-1},}_{t-1}\); \(_{t},}_{t}=_{t-1}\), \(}_{t-1}\)
13:\(}_{t}=(_{t})+(}_{t})\); \(}_{t}=(_{t})+(}_{t})\)
14:\(}_{t}=}_{t}_{t}}_{t}; }_{t}=}_{t}(\|_{t}\|_{F}/\| }_{t}\|_{F})\)
15:\(_{t},_{t}=(_{t-1},_{t-1},}_{t})\) ```

**Algorithm 2**\((,})\)

## 4 Theoretical Analysis

In this section, we analyze why quantizing the eigenvector matrix of a preconditioner in Shampoo is better than quantizing the preconditioner itself under a certain singular value distribution. Furthermore, we consider quantization as a perturbation and prove the convergence of the perturbed Shampoo (Algorithm 6) in Appendix E. The following lemma reveals some good properties of perturbing the eigenvector matrix of a PD matrix.

**Lemma 1**.: _Let \(\) be a PD matrix whose SVD is \(^{}\), where \(\!=\![_{i}]\) is an orthogonal matrix and \(\!=\!([_{i}]^{})\) is a diagonal matrix. Given a perturbation \(\!=\![_{i}]\) and \(s\), we define \(\!:=\!(^{})^{s}\) and \(\!:=\!((\!+\!)(\!+\!)^{ })^{s}\!-\!\)._

1. _If_ \(\!+\!\) _is orthogonal and there exists_ \(\) _such that_ \(\|_{i}\|_{2}\)_, then_ \[\|_{F}}{\|\|_{F}} 2.\]
2. _If_ \(\!+\!\) _is orthogonal and there exists_ \(\) _such that_ \(_{i},_{i}\!+\!_{i} 1- 0\)_, then_ \[,\!+\!}{\|\|_{F}\|\!+\! \|_{F}}(1\!-\!)^{2}.\]From Lemma 1, it is evident that the normwise relative error and angle error in \(f()=^{s}\) of perturbing \(\) at \(=^{}\) are independent of \(\) and \(s\). Moreover, these errors are well-bounded under some mild conditions. Empirically, for \(4\)-bit quantization, \(=0.1\) and \(=0.005\) roughly meet the conditions of Lemma 1, leading to \(\|_{F}}{\|\|_{F}} 0.2\) and \(,+)}{\|\|_{F}\|+ {B}\|_{F}} 0.99\).

It is very complicated to generally analyze the perturbation in \(f()=^{s}\) of perturbing \(\). Thus, we focus on perturbing the singular values of \(\). For simplicity, we assume that both \(\) and \(+\) have only two distinct singular values, where \(\) is a perturbation of \(\). The following lemma gives the perturbation in \(^{s}\) of perturbing the smaller singular value of \(\).

**Lemma 2**.: _Let \(\) be a PD matrix of order \(m+n\) whose SVD is \(^{}\), where \(m,n_{+}\), \(n=lm\), \(=[_{i}]\) is an orthogonal matrix and \(=([_{i}]^{})\) is a diagonal matrix. Assume that \(=([c_{m 1}^{}, _{n 1}^{}]^{})\), \(c 1\), and \(>0\). Given a perturbation \(=([_{m 1}^{}, _{n 1}^{}]^{})\) and \(s\), we define \(\!:=\!(^{})^{s}\) and \(\!:=\!((\!+\!)^{})^{s}\!-\!\)._

1. _If_ \(_{n 1}=(k-1)_{n 1}\) _where_ \(k>0\)_, then_ \[\|_{F}}{\|\|_{F}}=|k^{s}-1|}{}+l}=h_{1}(s,l).\] _Moreover,_ \(h_{1}(s,l)\) _decreases monotonically with_ \(s\) _over_ \((-,0)\) _and increases monotonically with_ \(l\) _over_ \((0,+)\)_._
2. _If_ \(_{n 1}=(tc-1)_{n 1}\) _where_ \(t>0\)_, then_ \[,+}{\|\|_{F}\|+ \|_{F}}=+c^{s}}{)(l+c^{2s})}}=h_{2}(l).\] _Moreover,_ \(h_{2}(l)\) _decreases monotonically with_ \(l\) _over_ \((0,(c/t)^{s},+)\)_._
3. _If_ \(_{n 1}=(tc-1)_{n 1}\) _where_ \(k=tc>0\) _and_ \(l=(c/t)^{s}\)_, then_ \[\|_{F}}{\|\|_{F}}=-1|}{}+1}, ,+}{\|\|_{F}\|+ \|_{F}}=+1/k^{s}}}.\]

Let us make some comments on the above lemma. First, from Lemma 2(1) we have \(h_{1}(1,l)=\|_{F}}{\|\|_{F}}=|k-1|}{ {c^{2}}+l}\). If \(k 1\), \(\|_{F}}{\|\|_{F}}=\|_{F}}{\|\|_{F}}\) is bounded by \(=t\). Second, if \(k=tc 1\) and \(s<0\), one can deduce \(h_{2}(l)/(1+lt^{2s})}\) from Lemma 2(2), which indicates that a small \(lt^{2s}\) is needed to achieve small \(h_{2}(l)\). We can set \(t=0.02\) to simulate 4-bit quantization. Based on Lemma 1 and Lemma 2(3), we have the following proposition.

**Proposition 1**.: _Let \(\) be a PD matrix of order \(m+n\) whose SVD is \(^{}\), where \(m,n_{+}\), \(n=lm\), \(=[_{i}]\) is an orthogonal matrix, \(=([c_{m 1}^{}, _{n 1}^{}]^{})\), \(c 1000\), and \(>0\). Given \(=[_{i}]\), \(=([_{m 1}^{}, _{n 1}^{}]^{})\), and \(s-0.25\), we define \(\!:=\!(^{})^{s}\), \(_{1}\!:=\!(+)(+)^{})^{s}\), and \(_{2}\!:=\!((\!+\!)^{})^{s}\). If \(+\) is orthogonal, \(\|_{i}\|_{2}\!\!0.1,_{i},_{i}\! \!-0.005\), \(_{n 1}\!=\!(0.02c\!-\!1)_{n 1}\), and \(l\!=\!(c/0.02)^{s}\), then_

\[2_{1}-\|_{F}}{\|\|_{F}}\!\!0.4\!\!_{2}-\|_{F}}{\|\|_{F}}, 6(1-,_{1} }{\|\|_{F}\|_{1}\|_{F}})\!\!0.06\!\!(1 \!1-,_{2}}{\|\|_{F}\|_{2}\|_{F}} ).\]

Proposition 1 requires very strong assumptions. Nevertheless, it provides insight into why quantizing \(\) can result in a greater normwise relative error and angle error in \(^{s}\), compared to quantizing \(\). Complete proofs of Lemma 1, Lemma 2, and Proposition 1 can be found in Appendix F.

## 5 Experiments

In this section, we compare our 4-bit Shampoo combined with SGDM or AdamW to their 32-bit counterparts, as well as the first-order optimizers on various image classification tasks. See more experimental results on image classification and natural language modeling tasks in Appendix H.

**Models, datasets, and hyperparameters.** We train VGG19 , ResNet34 , ViT-Small , and Swin-Tiny  on the CIFAR-100  and Tiny-ImageNet  datasets with one RTX3060Ti GPU, and train ResNet50 and ViT-Base/32 on the ImageNet-1k dataset  with one A800 GPU.

For hyperparameter settings, we mainly follow  to train CNNs and [25; 44] to train vision transformers. For all the tasks, we keep the common hyperparameters of optimizers the same values. See Appendix G for experimental details.

**Main results.** We show the performance, wall-clock time, and memory cost in Table 2. First-order optimizers run 1.2x to 1.5x epochs, resulting in longer wall-clock time, yet yielding lower test accuracies compared to second-order optimizers. In comparison to 32-bit Shampoo, our 4-bit Shampoo shows comparable test accuracies with differences ranging from -0.7% to 0.5%, increases in wall-clock time varying from -0.2% to 9.5%, and memory savings of 4.5% to 41%. Compared to the first-order optimizers, the memory costs of our 4-bit Shampoo only rise by 0.8% to 12.7%. This represents a significant advancement in the utilization of second-order optimizers. Following , we report the total peak GPU memory consumption rather than the optimizer's peak GPU memory consumption. Our main focus is on quantizing the states for constructing preconditioners and their inverse roots, which are approximately 7x smaller for 4-bit Shampoo compared to 32-bit Shampoo (see Appendix G). Figure 4 shows the test accuracy curves on the CIFAR-100 and ImageNet-1k

   Dataset & Model & Optimizer & TA (\%) & WCT (min) & TMC (MB) \\   &  & SGDM & 74.14 & 97.70 & 512.17 \\  & & SGDM + 32-bit Shampoo & 74.54 & 84.45 & 979.13 \\  & & SGDM + 4-bit Shampoo & 74.74 & 92.51 & 577.14 \\   &  & SGDM & 78.98 & 170.1 & 822.03 \\  & & SGDM + 32-bit Shampoo & 79.71 & 147.2 & 1441.8 \\  & & SGDM + 4-bit Shampoo & 79.17 & 155.8 & 908.40 \\   &  & AdamW & 74.34 & 668.1 & 2720.0 \\  & & AdamW + 32-bit Shampoo & 77.50 & 498.7 & 3252.0 \\  & & AdamW + 4-bit Shampoo & 77.22 & 510.8 & 2791.7 \\   &  & AdamW & 76.69 & 318.6 & 1465.8 \\  & & AdamW + 32-bit Shampoo & 79.34 & 260.8 & 2036.0 \\  & & AdamW + 4-bit Shampoo & 78.63 & 273.3 & 1543.9 \\   &  & SGDM & 61.53 & 172.0 & 1062.3 \\  & & SGDM + 32-bit Shampoo & 63.39 & 136.5 & 1531.9 \\  & & SGDM + 4-bit Shampoo & 62.84 & 143.8 & 1127.3 \\   &  & SGDM & 67.10 & 432.1 & 2304.0 \\  & & SGDM + 32-bit Shampoo & 67.90 & 313.0 & 2924.3 \\  & & SGDM + 4-bit Shampoo & 67.95 & 329.3 & 2390.4 \\   &  & AdamW & 54.66 & 1274 & 2730.1 \\  & & AdamW + 32-bit Shampoo & 57.11 & 953.9 & 3261.1 \\  & & AdamW + 4-bit Shampoo & 57.15 & 970.3 & 2801.9 \\   &  & AdamW & 58.77 & 701.9 & 1789.9 \\  & & AdamW + 32-bit Shampoo & 61.74 & 565.3 & 2362.8 \\  & & AdamW + 4-bit Shampoo & 62.24 & 582.7 & 1868.1 \\   &  & SGDM & 76.70 & 2134 & 11307 \\  & & SGDM + 32-bit Shampoo & 77.07 & 1910 & 11937 \\   & & SGDM + 4-bit Shampoo & 76.92 & 1970 & 11396 \\    &  & AdamW & 72.87 & 2190 & 10600 \\   & & AdamW + 32-bit Shampoo & 75.03 & 1774 & 12134 \\   & & AdamW + 4-bit Shampoo & 74.78 & 1770 & 10804 \\   

Table 2: Performance, wall-clock time and memory cost on various image classification tasks. TA = test accuracy, WCT = wall-clock time, and TMC = total GPU memory cost.

Figure 4: Visualization of test accuracies on the CIFAR-100 and ImageNet-1k datasets.

[MISSING_PAGE_FAIL:9]

**Broader Impact.** Our work can facilitate training large models with second-order optimizers. This could open up new research possibilities that were previously unattainable due to GPU memory constraints, especially benefiting researchers with limited resources.