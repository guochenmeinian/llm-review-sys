# SPRINQL: Sub-optimal Demonstrations driven

Offline Imitation Learning

 Huy Hoang

Singapore Management University

mh.hoang.2024@phdcs.smu.edu.sg

&Tien Mai

Singapore Management University

atmai@smu.edu.sg

&Pradeep Varakantham

Singapore Management University

pradeepv@smu.edu.sg

###### Abstract

We focus on offline imitation learning (IL), which aims to mimic an expert's behavior using demonstrations without any interaction with the environment. One of the main challenges in offline IL is the limited support of expert demonstrations, which typically cover only a small fraction of the state-action space. While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations. For example, in treatment optimization problems, there are varying levels of doctor treatments available for different chronic conditions. These range from treatment specialists and experienced general practitioners to less experienced general practitioners. Similarly, when robots are trained to imitate humans in routine tasks, they might learn from individuals with different levels of expertise and efficiency.

In this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories. Existing offline IL methods based on behavior cloning or distribution matching often face issues such as overfitting to the limited set of expert demonstrations or inadvertently imitating sub-optimal trajectories from the larger dataset. Our approach, which is based on inverse soft-Q learning, learns from both expert and sub-optimal demonstrations. It assigns higher importance (through learned weights) to aligning with expert demonstrations and lower importance to aligning with sub-optimal ones. A key contribution of our approach, called SPRINQL, is transforming the offline IL problem into a convex optimization over the space of Q functions. Through comprehensive experimental evaluations, we demonstrate that the SPRINQL algorithm achieves state-of-the-art (SOTA) performance on offline IL benchmarks. Code is available at https://github.com/hhmbuy0/SPRINQL.

## 1 Introduction

Reinforcement learning (RL) has established itself as a strong and reliable framework for sequential decision-making with applications in diverse domains: robotics , healthcare , and environment generation . Unfortunately, RL requires an underlying simulator that can provide rewards for different experiences, which is usually not available.

Imitation Learning (IL)  handles the lack of reward function by utilizing expert demonstrations to guide the learning scheme to compute a good policy. However, IL approaches still require the presence of a simulator that allows for online interactions. Initial works in Offline IL  tackle the absence of simulator by considering an offline dataset of expertdemonstrations. These approaches extend upon Behavioral Cloning (BC), where we aim to maximize the likelihood of the expert's decisions from the provided dataset. The key advantage with BC is the theoretical justification on converging to expert behaviors given sufficient trajectories. However, when there are not enough expert trajectories, it often suffers from distributional shift issues . Thus, a key drawback of these initial IL approaches is the need for a large number of expert demonstration datasets.

To deal with limited expert demonstrations, recent works utilize non-expert demonstration datasets to reduce the reliance on only expert demonstrations. These additional non-expert demonstrations are referred to as supplementary data. Directly applying BC to these larger supplementary datasets will lead to sub-optimal policies, so most prior work in utilizing supplementary data attempts to extract expert-like demonstrations from the supplementary dataset in order to expand the expert demonstrations . These works assume expert-like demonstrations are present in the supplementary dataset and focus on identifying and utilizing those, while eliminating the non-expert demonstrations. Eliminating non-expert trajectories can result in loss of key information (e.g., transition dynamics) about the environment. Additionally, these works primarily rely on BC, which is known to overlook the sequential nature of decision-making problems - a small error can quickly accumulate when the learned policy deviates from the states experienced by the expert.

We develop our algorithm based on an inverse Q-learning framework that better captures the sequential nature of the decision-making  and can operate under the more realistic assumption that the data is collected from people/policies with lower expertise levels1 (not experts). To illustrate, consider a scenario in robotic manipulation where the goal is to teach a robot to assemble parts. Expert demonstrations might show precise and efficient methods to assemble parts, but are limited in number due to the high cost and time associated with expert involvement. On the other hand, sub-optimal demonstrations from novice users are easier to obtain and more abundant. Our SPRINQL approach effectively integrates these sub-optimal demonstrations, giving appropriate weight to the expert demonstrations to ensure the robot learns the optimal assembly method without overfitting to the limited expert data or the inaccuracies in the sub-optimal data. We utilize these non-expert trajectories to learn a Q function that contributes to our understanding of the environment and the ground truth reward function.

**Contributions:** Overall, we make the following key contributions in this paper:

(i) We propose SPRINQL, a novel algorithm based on Q-learning for _offline imitation learning with expert and multiple levels of sub-optimal demonstrations_.

(ii) We provide key theoretical properties of the SPRINQL objective function, which enable the development of a scalable and efficient approach. In particular, we leverage distribution matching and reward regularization to develop an objective function for SPRINQL that not only help address the issue of limited expert samples but also utilizes non-expert data to enhance learning. Our objective function is not only convex within the space of \(Q\) functions but also guarantees the return of a \(Q\) function that lower-bounds its true value.

(iii) We provide an extensive empirical evaluation of our approach in comparison to existing best algorithms for offline IL with sup-optimal demonstrations. Our algorithms provide state-of-the-art (SOTA) performance on all the benchmark problems. Moreover, SPRINQL is able to recover a reward function that shows a high positive correlation with the ground-truth rewards, highlighting a unique advantage of our approach compared to other IL algorithms in this context.

### Related Work

Imitation Learning.Imitation learning is recognized as a significant technique for learning from demonstrations. It begins with BC, which aims to maximize the likelihood of expert demonstrations. However, BC often under-performs in practice due to unforeseen scenarios . To overcome this limitation, Generative Adversarial Imitation Learning (GAIL)  and Adversarial Inverse Reinforcement Learning (AIRL)  have been developed. These methods align the occupancy distributions of the policy and the expert within the Generative Adversarial Network (GAN) framework . Alternatively, Soft Q Imitation Learning (SQIL)  bypasses the complexities of adversarial training by assigning a reward of +1 to expert demonstrations and 0 to the others, subsequently learning a value function based on these rewards.While the aforementioned imitation learning algorithms show promise, they require interaction with the environment to obtain the policy distribution, which is often impractical.

Offline Imitation Learning.ValueDICE  introduces a novel approach for off-policy training, suitable for offline training, using Stationary Distribution Corrections [25; 26]. However, ValueDICE necessitates adversarial training between the policy network and the \(Q\) network, which can make the training slow and unstable. Recently, algorithms like PWIL  and IQ-learn  have optimized distribution distance, offering an alternative to adversarial training schemes. Since such approaches rely on occupancy distribution matching, a large expert dataset is often required to achieve the desired performance. Our approach, SPRINQL is able to bypass this requirement of a large set of expert demonstrations through the use of non-expert demonstrations (which are typically more available) in conjunction with a small set of expert demonstrations.

Imitation Learning with Imperfect Demonstrations.T-REX  and D-REX  have shown that utilizing noise-ranked demonstrations as a reference-based approach can return a better policy without requiring expert demonstrations in online settings. Moreover, there are also several works [36; 33] that utilize the GAN framework  for sub-optimal datasets and have achieved several successes. Meanwhile, in the offline imitation learning context, TRAIL  utilizes sup-optimal demonstrations to learn the environment's dynamics. It employs a feature encoder to map the high-dimensional state-action space into a lower dimension, thereby allowing for a scalable way of learning of dynamics. This approach may face challenges in complex environments where predicting dynamics accurately is difficult, as shown in our experimental results. Other works assume that they can extract expert-like state-action pairs from the sub-optimal demonstration set and use them for BC with importance sampling [31; 37; 39; 21; 20]. However, expert-like state-actions might be difficult to accurately identify, as true reward information is not available. In contrast, our approach is more general, as we do not assume that the sub-optimal set contains expert-like demonstrations. We also allow for the inclusion of demonstrations of various qualities. Moreover, while prior works only recover policies, our approach enables the recovery of both expert policies and rewards, justifying the use of our method for Inverse Reinforcement Learning (IRL).

## 2 Background

Preliminaries.We consider a MDP defined by the following tuple \(= S,A,r,P,,s_{0}\), where \(S\) denotes the set of states, \(s_{0}\) represents the initial state set, \(A\) is the set of actions, \(r:S A\) defines the reward function for each state-action pair, and \(P:S A S\) is the transition function, i.e., \(P(s^{}|s,a)\) is the probability of reaching state \(s^{} S\) when action \(a A\) is made at state \(s S\), and \(\) is the discount factor. In reinforcement learning (RL), the aim is to find a policy that maximizes the expected long-term accumulated reward \(_{}\{_{(s,a)_{}}[r(s,a)]\}\), where \(_{}\) is the occupancy measure of policy \(\): \(_{}(s,a)=(1-)(a|s)_{t=1}^{}^{t}P(s_{t}=s|)\).

MaxEnt IRLThe objective in MaxEnt IRL is to recover a reward function \(r(s,a)\) from a set of expert demonstrations, \(^{E}\). Let \(^{E}\) be the occupancy measure of the expert policy. The MaxEnt IRL framework  proposes to recover the expert reward function by solving

\[_{r}_{} \{_{^{E}}[r(s,a)]-(_{_{}}[r(s,a)]-_{_{}}[(s,a)])\}\] (1)

Intuitively, the aim is to find a reward function that achieves the highest difference between the expected value of the expert policy and the highest expected value among all other policies (computed through the min loop).

Iq-LearnGiven a reward function \(r\) and a policy \(\), the soft Bellman equation is defined as \(_{r}^{}[Q](s,a)=r(s,a)+_{s^{}}[V^{}(s^ {})]\), where \(V^{}(s)=_{a(a|s)}[Q(s,a)-(a|s)]\). The Bellman equation \(_{r}^{}[Q]=Q\) is contractive and always yields a unique Q solution . In IQ-learn, they further define an inverse soft-Q Bellman operator \(^{}[Q]=Q(s,a)-_{s^{}}[V^{}(s^{})]\).  show that for any reward function \(r(a,s)\), there is a unique \(Q^{*}\) function such that \(_{r}^{}[Q^{*}]=Q^{*}\), and for a \(Q^{*}\) function in the \(Q\)-space, there is a unique reward function \(r\) such that \(r=^{}[Q^{*}]\). This result suggests that one can safely transform the objective function of the _MaxEnt IRL_ from \(r\)-space to the Q-space as follows:

\[_{Q}_{}\ \ (,Q)=_{}[^{}[Q](s,a))]- _{_{}}[^{}[Q](s,a)]+_{_{}}[ (s,a)]\] (2)

which has several advantages;  show that \((,Q)\) is convex in \(\) and linear in \(Q\), implying that (2) always yields a unique saddle point solution. In particular, (2) can be converted into a maximization over the Q-space, making the training problem no longer adversarial.

## 3 Sprinql

We now describe our inverse soft-Q learning approach, referred to as **SPRINQL** (**S**ub-o**P**timal demonstrations driven **R**eward regularized **IN**verse soft **Q** Learning). We first describe the three key components in the SPRINQL formulation:

(1) We formulate the objective function that enables matching the occupancy distribution of not just expert demonstrations, but also sub-optimal demonstrations.

(2) To mitigate the effect of limited expert samples (and larger sets of sub-optimal samples) that can bias the distribution matching of the first step to sub-optimal demonstrations, we introduce a _reward regularization term_ within the objective. This regularization term is to ensure reward function allocates higher values to state-action pairs that appear in higher expertise demonstrations.

(3) We show that while this new objective does not have the same advantageous properties as the one in inverse Q-learning , with some minor (yet significant) changes it is possible to restore all the important properties.

### Distribution Matching with Expert and Suboptimal Demonstrations

We consider a setting where there are demonstrations classified into several sets of different expertise levels \(^{1},^{2},....,^{N}\), where \(^{1}\) consists of expert demonstrations and all the other sets contains sub-optimal ones. This setting is general than existing work in IL with sup-optimal demonstrations, which typically assumes that there are only two quality levels: expert and sub-optimal. Let \(=_{i[N]}^{i}\) be the union of all the demonstration sets and \(^{1},...,^{N}\) be the occupancy measures of the respective expert policies. The ordering of expected values across different levels of expert policies would then be given by:

\[_{^{1}}[r^{*}(s,a)]>_{^{2}}[r^{*}(s,a)]>...> _{^{N}}[r^{*}(s,a)],\]

where \(r^{*}(.,.)\) are the _ground-truth_ rewards. Typically, the number of demonstrations in first level, \(^{1}\) is significantly lower than those from other expert levels, i.e., \(|^{1}||^{i}|,\ \ i=2,...,N\) The MaxEnt IRL objective from Equation 1 can thus be adapted as follows:

\[_{r}_{}\ \ _{i[N]}w_{i}_{^{i}}[r(s,a)]- _{_{}}[r(s,a)]+_{_{}}[(s,a)]\] (3)

where \(w_{i} 0\) is the weight associated with the expert level \(i[N]\) and we have \(w_{1}>w_{2}>...>w_{N}\) and \(_{i[N]}w_{i}=1\). There are two key intuitions in the above optimization: (a) Expert level \(i\) accumulates higher expected values than expert levels greater than \(i\); and (b) Difference in values accumulated by expert policies and the maximum of all other policies is maximized. The optimization term can be rewritten as:

\[_{^{U}}[r(s,a)]-_{_{}}[r(s,a)]-_{ _{}}[(s,a)],\]

where \(^{U}=_{i[N]}w_{i}^{i}\). Here we note that the expected reward \(_{i[N]}w_{i}_{^{i}}[r(s,a)]\) is empirically approximated by samples from the demonstration sets \(^{1},^{2},....,^{N}\). The number of demonstrations in the best demonstration set \(^{1}\) (i.e. the set of expert demonstrations) is significantly _lower_ when compared to other demonstration sets. So, an empirical approximation of \(_{^{1}}[r(s,a)]\) using samples from \(^{1}\) would be inaccurate.

### Regularization with Reference Reward

We create a reference reward based on the provided expert and sub-optimal demonstrations and utilize the reference function to compute a regularization term that is added to the objective of Equation 3.

Concretely, we define a _reference reward_ function \((s,a)\) such that:

\[(s,a)>(s^{},a^{}),(s,a)^ {1}(s^{},a^{})^{1}\]

\[(s,a)>(s^{},a^{}),(s,a)^{2}(s^{},a^{})^{2}^{1}\]

The aim here is to assign higher rewards to demonstrations from higher expertise levels, and zero rewards to those that do not belong to provided demonstrations. We will discuss how to concretely estimate such reference reward values later.

We utilize this reference reward as part of the reward regularization term, which is added into the MaxEntIRL objective in (3) as follows:

\[_{r}_{}_{^{U}}[r(s,a)]-_{_{}}[r(s,a)]+_{_{}}[(s,a)]}_{}-_{^{U}}[(r(s,a)-(s,a))^{2}]}_{ }}\] (4)

where \(>0\) is a weight parameter for the reward regularizer term. With (4), the goal is to find a policy with an occupancy distribution that matches with the occupancy distribution of different expertise levels appropriately (characterized by the weights, \(w_{i}\)). Simultaneously, it ensures that the learning rewards are close to the pre-assigned rewards, aiming to guide the learning policy towards replicating expert demonstrations, while also learning from sub-optimal demonstrations.

### Concave Lower-bound on Inverse Soft-Q with Reward Regularizer

Even though (4) can be directly solved to recover rewards, prior research suggests that transforming (4) into the Q-space will enhance efficiency. We delve into this transformation approach in this section. As discussed in Section 2, there is a one-to-one mapping between any reward function \(r\) and a function \(Q\) in the Q-space. Thus, the maximin problem in (4) can be equivalently transformed as:

\[_{Q}_{}(Q,)}{{=}}_{^{U}}[^{}[Q](s,a))] -_{_{}}[^{}[Q](s,a))]+_{ _{}}[(s,a)]\] (5)

where \(r(s,a)\) is replaced by \(^{}[Q](s,a)\) and

\[^{}[Q](s,a)=Q(s,a)-_{s^{}}[V^{}(s^{ })],\ V^{}(s)=_{a(a|s)}[Q(s,a)-(a|s)]\]

In the context of single-expert-level,  demonstrated that the objective function in the \(Q\)-space as given in Equation 2 is concave in \(Q\) and convex in \(\), implying that the maximin problem always has a unique saddle point solution. Unfortunately, this property does not hold in our case.

**Proposition 3.1**.: \((Q,)\) _(as defined in Equation 5) is concave in \(Q\) but is not convex in \(\)._

In general, we can see that the first and second term of (6) are convex in \(Q\), but the reward regularizer term, which can be written as

\[_{^{U}}(Q(s,a)-(s,a)-_{s^{ } P(.|s,a)}_{a^{}(.|s^{})}(Q(s^{ },a^{})-(s^{},a^{})))^{2},\]

is not concave in \(\) (details are shown in the Appendix). The property indicated in Proposition 3.1 implies that the maximin problem within the \(Q\)-space \(_{Q}_{}J(Q,)\) may not have a unique saddle point solution and would be more challenging to solve, compared to the original inverse IQ-learn problem.

Another key property of Equation 2 is with regards to the inner minimization problem over \(\), which yields a unique closed-form solution, enabling the transformation of the _max-min_ problem into a non-adversarial concave maximization problem within the Q-space. The closed-form solution was given by \(^{Q}=_{}\ V^{}(s)\) for all \(s S\). Unfortunately, this result also does not hold with the new objective function in (6), as formally stated below:

**Proposition 3.2**.: \((Q,)\) _may not necessarily be minimized at \(^{*}\) such that \(^{*}=_{}\ V^{}(s)\), for all \(s S\)._To overcome the above challenges, our approach involves constructing a more tractable objective function that is a lower bound on the objective of (6). Let us first define \((Q)=_{}(Q,)\). We then look at the regularization term, which causes all the aforementioned challenges, and write:

\[(^{}[Q](s,a))- (s,a))^{2}=(Q(s,a)-(s,a)-_ {s^{}}[V^{}(s^{})])^{2}\] \[=(Q(s,a)-(s,a))^{2}+(_{s^{}}[V^{ }(s^{})])^{2}+2((s,a)-Q(s,a))_{s^{}}[V^ {}(s^{})]\]

We then take out the negative part of \(((s,a)-Q(s,a))\) using ReLU, and consider a slightly new objective function as follows:

\[}(Q,)}{{=}} _{i[N]}w_{i}_{^{i}}[^{}[Q](s,a))]-(_{_{}}[^{}[Q](s,a))]-_{_{}}[(s,a)])\] \[-_{^{}}(Q(s,a)-(s,a))^{2}+(_{s^{}}V^{}(s^{}))^{2}+2( (s,a)-Q(s,a))_{s^{}}V^{}(s^{})\] (6)

Let \((Q)=_{}}(Q,))\). The proposition below shows that \((Q)\) always lower-bounds \((Q)\).

**Proposition 3.3**.: _For any \(Q 0\), we have \((Q)(Q)\) and \(_{Q}(Q)_{Q}(Q)\). Moreover, \((Q)=(Q)\) if \(Q(s,a)(s,a)\) for all \((s,a)\)._

We note that assuming \(Q 0\) is not restrictive, as if the expert's rewards \(r^{*}(s,a)\) are non-negative (typically the case), then the _true_ soft-Q function, defined as \(Q^{*}(s,a)=[_{s_{t},a_{t}}^{t}(r^{*}(s,a)-(s,a))|(s_ {0},a_{0})=(s,a)]\), should also be non-negative, for any \(\). As \((Q)\) provides a lower-bound approximation of \((Q)\), maximizing \((Q)\) over the \(Q\)-space would drive \((Q)\) towards its maximum value. It is important to note that, given that the inner problem involves minimization, obtaining an upper-bound approximation function is easier. However, since the outer problem is a maximization one, an upper bound would not be helpful in guiding the resulting solution towards optimal ones. The following theorem indicates that \((Q)\) is more tractable to use.

**Theorem 3.4**.: _For any \(Q 0\), the following results hold: (i) The inner minimization problem \(_{}}(Q,)\) has a unique optimal solution \(^{Q}\) such that \(^{Q}=_{}V^{}(s)\) for all \(s S\) and \(^{Q}(a|s)=(Q(s,a))}\), (ii) \(_{}V^{}(s)=(_{a}(Q(s,a)))}{{=}}V^{Q}(s)\), and (iii) \((Q)\) is concave for \(Q 0\)._

The above theorem tells us that new objective \((Q)\) has a closed form where \(V^{}(s)\) is replaced by \(V^{Q}(s)\). Moreover \((Q)\) is concave for all \(Q 0\). The concavity is particularly advantageous, as it guarantees that the optimization objective is well-behaved and has a unique solution \(Q^{*}\) such that \((Q^{*},^{Q^{*}})\) form a unique saddle point of \(_{Q}_{}\ \ }(Q,)\). _Thus, our tractable objective has all the nice properties that the original IQ-Learn objective had, while being able to work for the offline case with multiple levels of expert trajectories and our reward regularizer._

### SPRINQL Algorithm

```
0:\((^{1},^{2},...,^{N})\), \((w_{1},w_{2},...,w_{N}),_{n},Q_{},_{}\)
1:# estimate reward reference function
2:for iteration \(i..N\)do
3:\(d(d^{1},d^{1},...,d^{N})(^{1},^{2},..., ^{N})\)
4:from dataset \(d\), calculate \((_{})\) by (7)
5:\(-_{}(_{})\)
6:endfor
7:# train SPRINQL
8:for iteration \(i..N\)do
9:\(d(d^{1},d^{1},...,d^{N})(^{1},^{2},..., ^{N})\)
10:# Update Q function
11:from dataset \(d\), calculate \(}^{C}(Q_{},_{})\) by (8)
12:\(+_{}}^{C}(Q_{},_{ })\)
13:# Update policy for actor-critic
14:\(+[_{_{}(a|s)} [Q_{}(s,a)-(_{}(a|s))]]\)
15:endfor ```

**Algorithm 1****SPRINQL**: Inverse soft-Q Learning with Sub-optimal Demonstrations

Algorithm 1****SPRINQL: Inverse soft-Q Learning with Sub-optimal Demonstrations

Algorithm 1****SPRINQL: Inverse soft-Q Learning with Sub-optimal Demonstrations

### SPRINQL Algorithm

Algorithm 1 provides the overall SPRINQL algorithm. We first estimate the reference rewards in lines 2-6 of Algorithm 1 and the overall process is described in Section 3.4.1. Before we proceed to the overall training, we have to estimate the weights, \(w_{i}\) (associated with the ranked demonstration sets) employed in \(}(Q,)\). We provide a description of this estimation procedure in Section 3.4.2. Finally, to enhance stability and mitigate over-estimation issues commonly encountered in offline Q-learning, we employ a conservative version of \(}(Q,)\) in lines 8-15 of the algorithm and is described in Section 3.4.3. Some other practical considerations are discussed in the appendix.

#### 3.4.1 Estimating the Reference Reward

We outline our approach to automatically infer the reference rewards \((s,a)\) from the ranked demonstrations. The general idea is to learn a function that assigns higher values to higher expert-level demonstrations. To achieve this, let us define \(R()=_{(s,a)}(s,a)\) (i.e., accumulated reward of trajectory \(\)). For two trajectories \(_{i},~{}_{j}\), let \(_{i}_{j}\) denote that \(_{i}\) is lower in quality compared to \(_{j}\) (i.e., \(_{i}\) belongs to demonstrations from lower-expert policies, compared to \(_{j}\)). We follow the Bradley-Terry model of preferences [4; 5] to model the probability \(P(_{i}_{j})\) as \(P(_{i}_{j})=))}{(R(_{i}))+(R( _{j}))}\) and use the following loss function:

\[_{}\{()=_{i[N]}_{(s,a),(s^{ },a^{})^{i}}((s,a)-(s^{ },a^{}))^{2}-_{h,k[N],h>k,_{i}^{h},_ {j}^{k}} P(_{i}_{j})\}\] (7)

where the first term of \(()\) serves to guarantee that the reward reference values for \((s,a)\) pairs within the same demonstration group are similar, and the second term aims to increase the likelihood that the accumulated rewards of trajectories adhere to the expert-level order. Importantly, it can be shown below that \(()\) is convex in \(\) (Proposition 3.5), making the learning well-behaved. In practice, one can model \((s,a)\) by a neural network of parameters \(\) and optimize \(()\) over \(\)-space.

**Proposition 3.5**.: \(()\) _is strictly convex in \(\)._

#### 3.4.2 Preference-based Weight Learning for \(w_{i}\)

Each weight parameter \(w_{i}\) used in (4) should reflect the quality of the corresponding demonstration set \(^{i}\), which can be evaluated by estimating the average expert rewards of these sets. Although this information is not directly available in our setting, the reward reference values discussed earlier provide a convenient and natural way to estimate them. This leads to the following formulation for inferring the weights \(w_{i}\) from the ranked data: \(w_{i}=_{(s,a) D^{i}}[(s,a)]}{_{j[N]} _{(s,a) D^{j}}[(s,a)]}\).

#### 3.4.3 Conservative soft-Q learning

Over-estimation is a common issue in offline Q-learning due to out-of-distribution actions and function approximation errors . We also observe this in our IL context. To overcome this issue and enhance stability, we leverage the approach in  to enhance our inverse soft-Q learning. The aim is to learn a _conservative_ soft-Q function that lower-bounds its true value. We formulate the _conservative inverse soft-Q_ objective as:

\[}^{C}(Q,)=-_{s,~{}a(a |s)}[Q(s,a)]+}(Q,)\] (8)

where \((a|s)\) is a particular state-action distribution. We note that in (8), the _conservative_ term is added to the objective function, while in the conservative Q-learning algorithm , this term is added to the Bellman error objective of each Q-function update. This difference makes the theory developed in  not applicable. In Proposition 3.6 below, we show that solving \(_{Q}}^{C}(Q)\) will always yield a Q-function that is a lower bound to the Q function obtained by solving \(_{Q}}(Q,)\).

**Proposition 3.6**.: _Let \(=_{Q}}(Q,)\) and \(^{C}=_{Q}}^{C}(Q,)\), we have \(_{s\\ a(a|s)}^{C}(s,a)_{ s\\ a(a|s)}(s,a)\)._

We can adjust the scale parameter \(\) in Equation 8 to regulate the conservativeness of the objective. Intuitively, if we optimize \(Q\) over a lower-bounded Q-space, increasing the scale parameter \(\) will force each \(Q(s,a)\) towards its lower bound. Consequently, when \(\) is sufficiently large, \(^{C}\) will point-wise lower-bound \(\), i.e., \(^{C}(s,a)(s,a)\) for all \((s,a)\).

## 4 Experiments

### Experiment Setup

Baselines.We compare our SPRINQL with SOTA algorithms in offline IL with sub-optimal demonstrations: TRAIL , DemoDICE , and DWBC . Moreover, we also compare withother straightforward baselines: BC with only sub-optimal datasets (BC-O), BC with only expert data (BC-E), BC with all datasets (BC-both), and BC with fixed weight for each dataset (W-BC), SQIL , and IQ-learn  with only expert data. In particular, since TRAIL, DemoDICE, and DWBC were designed to work with only two datasets (one expert and one supplementary), in problems with multiple sub-optimal datasets, we combine all the sub-optimal datasets into one single supplementary dataset. Meanwhile, BC-E, BC-O, and BC-both combine all available data into a large dataset for learning, while W-BC optimizes \(_{i}_{s,a D^{i}}\)-\(w_{i}((a|s))\), where \(w_{i}\) are our weight parameters. T-REX  is a PPO-based IL algorithm that can work with ranked demonstrations. It is, however, not suitable for our offline setting, so we do not include it in the main comparisons. Nevertheless, we will later conduct an ablation study to compare SPRINQL with an adapted version of T-REX. Moreover,  developed IPL for learning from offline preference data. This approach requires comparisons between every pair of trajectories, thus is not suitable for our context.

Environments and Data Generation:We test on five Mujoco tasks  and four arm manipulation tasks from Panda-gym . The maximum number transition of \((s,a)\) per trajectory is 1000 (or 1k for short) for the Mujoco and is 50 for Panda-gym tasks (descriptions in Appendix B.3). The sub-optimal demonstrations have been generated by randomly adding noise to expert actions and interacting with the environments. We generated large datasets of expert and non-expert demonstrations. For each seed, we randomly sample subsets of demonstrations for testing. This approach allows us to test with different datasets across seeds, rather than using fixed datasets for all seeds as in previous works. More details of these generated databases can be found in the Appendix B.4.

Metric:The return is normalized by \(=-.}{.-.} 100\) where R. return is the mean return of random policy and E.return is the mean return of the expert policy. The scores are calculated by taking the last ten evaluation scores of each seed, with five seeds per report.

Experimental Concerns.Throughout the experiments, we aim to answer the following questions: (**Q1**) How does SPRINQL perform compared to other baselines? (**Q2**) How do the distribution matching and reward regularization terms impact the performance of SPRINQL? (**Q3**) What happens if we augment (or reduce) the expert data while maintaining the sub-optimal datasets? (**Q4**) What happens if we augment (or reduce) the sub-optimal data while maintaining the expert dataset? (**Q5**) How does the conservative term help in our approach? (**Q6**) How does increasing \(N\) (the number of expertise levels) affect the performance of SPRINQL? (**Q7**) Does the preference-based weight learning approach provide good values for the weights \(w_{i}\)? (**Q8**) How does SPRINQL perform in recovering the ground-truth reward function?

### Main Comparison Results

In this section, we provide comparison results to answer (**Q1**) with three datasets (i.e., \(N=3\)). Additional comparison results for \(N=2\) can be found in the appendix. From lowest to highest

    &  &  \\   & Cheetah & Ant & Humanoid & Push & PnP & Slide & Avg \\  BC-E & -3.2\(\)0.9 & 6.4\(\)19.1 & 1.3\(\)0.2 & 8.2\(\)3.8 & 3.7\(\)2.7 & 0.0\(\)0.0 & 2.7 \\ BC-O & 14.2\(\)2.9 & 35.2\(\)20.1 & 10.6\(\)6.3 & 8.8\(\)4.5 & 3.9\(\)2.7 & 0.1\(\)0.3 & 12.1 \\ BC-both & 13.2\(\)3.6 & 47.0\(\)5.9 & 9.0\(\)3.5 & 9.0\(\)4.3 & 4.4\(\)3.0 & 0.1\(\)0.4 & 13.8 \\ W-BC & 12.9\(\)2.8 & 47.3\(\)6.4 & 19.6\(\)19.0 & 8.8\(\)4.3 & 3.7\(\)2.8 & 0.0\(\)0.0 & 15.4 \\ TRAIL & -4.1\(\)0.3 & -4.7\(\)1.9 & 2.6\(\)0.6 & 11.7\(\)4.0 & 7.8\(\)3.7 & 1.7\(\)1.8 & 3.9 \\ IQ-E & -3.4\(\)0.6 & -3.4\(\)1.3 & 2.4\(\)0.6 & 26.3\(\)10.9 & 18.1\(\)12.5 & 0.1\(\)0.4 & 6.7 \\ IQ-both & -6.1\(\)1.4 & -58.2\(\)0.0 & 0.8\(\)0.0 & 8.3\(\)3.9 & 3.8\(\)3.3 & 0.0\(\)0.2 & -8.6 \\ SQIL-E & -5.0\(\)0.7 & -33.8\(\)7.4 & 0.9\(\)0.1 & 9.6\(\)3.3 & 3.2\(\)2.9 & 0.1\(\)0.3 & -4.2 \\ SQIL-both & -5.6\(\)0.5 & -58.0\(\)0.4 & 0.8\(\)0.8 & 8.2\(\)3.8 & 3.3\(\)2.3 & 0.1\(\)0.3 & -12.6 \\ DemoDICE & 0.4\(\)2.0 & 31.7\(\)8.9 & 2.6\(\)0.8 & 8.1\(\)3.7 & 4.3\(\)2.4 & 0.1\(\)0.5 & 7.9 \\ DWBC & -0.2\(\)2.5 & 10.4\(\)5.0 & 3.7\(\)0.3 & 36.9\(\)7.4 & 25.0\(\)6.3 & 11.6\(\)4.4 & 14.6 \\  SPRINQL (ours) & **73.6\(\)4.3** & **77.0\(\)5.6** & **82.9\(\)11.2** & **72.0\(\)5.3** & **63.2\(\)6.4** & **37.7\(\)6.6** & **67.7** \\   

Table 1: Comparison results for three _Mujoco_ and three _Panda-gym_ tasks.

expertise levels, we randomly sample (25k-10k-1k) transitions for _Mujoco_ tasks and (10k-5k-100) transitions for Panda-gym tasks for every seed (details of these three-level dataset are provided in Appendix B.4). Table 1 shows comparison results across 3 _Mujoco_ tasks and 3 _Panda-gym_ tasks (the full results for all the nine environments are provided in Appendix C.1). In general, SPRINQL significantly outperforms other baselines on all the tasks.

### Ablation Study - No Distribution Matching and No Reward Regularizer

We aim to assess the importance of the distribution matching and reward regularizer terms in our objective (**Q2**). To this end, we conduct an ablation study comparing SPRINQL with two variants: (i) **noReg-SPRINQL**, derived by removing the reward regularizer term from (6), and (ii) **noDM-SPRINQL**, obtained by removing the distribution matching term from (6). Here, we note that the **noDM-SPRINQL** performs Q-learning using the reward reference function, this can viewed as an adaption of the T-REX algorithm  to our offline setting. The conservative Q-learning term is employed in the SPRINQL and the two variants to enhance stability. The comparisons for \(N=2\) and \(N=3\) on five Mujoco tasks are shown in Figure 1 (the full comparison results for all tasks are provided in the appendix). These results clearly show that SPRINQL outperforms the other variants, indicating the value of both terms in our objective function.

### Other Experiments

Experiments addressing the other questions are provided in the appendix. Specifically, Sections C.1 and C.2 provide full comparison results for all the Mujoco and Panda-gym tasks for three and two datasets (i.e., \(N=3\) and \(N=2\)), complementing the answer to **Q1**. Section C.3 provides the learning curves of the three variants considered in Section 4.3 above (answering **Q2**). Section C.4 provides experiments to answer **Q3** (_what would happen if we augment the expert dataset?_) and Section C.5 addresses **Q4** (_what would happen if we augment the sub-optimal dataset_?). Section C.6 experimentally shows the impact of the conservative term in our approaches (i.e., **Q5**). Section C.7 reports the performance of SPRINQL with varying numbers of expertise levels \(N\) (i.e., **Q6**). Section C.8 addresses **Q7**, and Section C.10 shows how SPRINQL performs in terms of reward recovering (i.e. **Q8**). In addition, Section C.11 reports the distributions of the reference rewards and Section C.12 provides \(\) choosing range.

Concretely, our extensive experiments reveal the following: (i) SPRINQL outperforms other baselines with two, three, or even larger numbers of datasets; (ii) the conservative term, distribution matching, and reward regularizer terms are essential to our objective--all three significantly contribute to the success of SPRINQL; (iii) the preference-based weight learning provides good estimates for the weights \(w_{i}\); and (iv) SPRINQL performs well in recovering rewards, showing a high positive correlation with the ground-truth rewards, justifying the use of our method for IRL.

## 5 Conclusion and Limitations

**(Conclusion)** We have developed SPRINQL, a novel non-adversarial inverse soft-Q learning algorithm for offline imitation learning from expert and sub-optimal demonstrations. We have demonstrated that our algorithm possesses several favorable properties, contributing to its well-behaved, stable, and scalable nature. Additionally, we have devised a preference-based loss function to automate the estimation of reward reference values. We have provided extensive experiments based on several benchmark tasks, demonstrating the ability of our _SPRINQL_ algorithm to leverage both expert and non-expert data to achieve superior performance compared to state-of-the-art algorithms.

Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.

**(Limitations)** Some limitations of this work include: (i) SPRINQL (and other baselines) still requires a large amount of sub-optimal datasets with well-identified expertise levels to learn effectively, (ii) there is a lack of theoretical investigation on how the sizes of the expert and non-expert datasets affect the performance of Q-learning, which we find challenging to address, and (iii) it lacks a theoretical exploration of how the reward regularizer term enhances the distribution matching term when expert samples are low--this question is relevant and interesting but also challenging to address. These limitations will pave the way for our future work.