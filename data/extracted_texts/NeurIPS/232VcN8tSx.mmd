# GREATS: Online Selection of High-Quality Data for LLM Training in _Every_ Iteration

Jiachen T. Wang

Princeton University

tianhaowang@princeton.edu

&Tong Wu

Princeton University

tongwu@princeton.edu

&Dawn Song

UC Berkeley

dawnsong@cs.berkeley.edu

&Prateek Mittal

Princeton University

pmittal@princeton.edu

&Ruoxi Jia

Virginia Tech

ruoxijia@vt.edu

Correspondence to Jiachen T. Wang and Ruoxi Jia.

###### Abstract

Online batch selection methods offer an adaptive alternative to static training data selection by dynamically selecting data batches during training. However, existing methods either rely on impractical reference models or simple heuristics that may not capture true data informativeness. To address these limitations, we propose _GREedy Approximation Taylor Selection_ (GREATS), a principled and efficient online batch selection method that applies greedy algorithm to optimize the data batch quality approximated by Taylor expansion. We develop a series of techniques to scale GREATS to large-scale model training. Extensive experiments with large language models (LLMs) demonstrate that GREATS significantly improves training convergence speed and generalization performance. Our codebase is publically available at https://github.com/Jiachen-T-Wang/GREATS.

## 1 Introduction

Large language models (LLMs) are of paramount importance in today's technological landscape. However, the extensive training times, often spanning weeks or even months, pose challenges such as prolonged development cycles and increased resource consumption. Moreover, these models are trained on massive data collected from the open world, which can include low-quality, redundant, and biased information. This underscores the need for effective selection of high-value training data.

**Online batch selection: an adaptive variant of data selection at the batch level.** Online batch selection methods aim to improve data selection by dynamically choosing data during the training process. At each training iteration, these methods leverage the partially trained model to determine which data to select for the current training iteration from a sampled batch, thereby adapting to the model's learning progress and focusing on the most informative examples _for the model's current state_. In contrast to static data selection methods (e.g., ), which select training data only once prior to the training process, online batch selection allows for a more adaptive and dynamic approach to data selection. By continuously updating the selection criteria based on the model's progress, online batch selection can identify the most relevant and informative examples at each stage of training, potentially leading to faster convergence and better generalization performance. Moreover, online batch selection operates on smaller batches of data, reducing the need for cumbersome data preprocessing and enabling more efficient use of computational resources compared to static data selection methods that process the entire dataset upfront.

However, existing online batch selection algorithms exhibit significant limitations and have found minimal success with LLMs. Reference-model-based batch selection methods [30; 9] rely on additional reference models. In , the reference models are trained on a substantial amount of hold-out data. This leads to considerable computational costs and reduces the amount of data available for training the main model.  utilize publicly available large-scale pre-trained models that already achieve very high performance on the targeted downstream tasks as reference models. Even if this assumption holds true in practice, the algorithm requires a Bayesian treatment and querying the reference models for every sample in the candidate batch at each iteration. These operations are computationally expensive, making the approach impractical for large-scale LLM training. Reference-model-free methods prioritize challenging samples based on metrics like high loss [28; 20] or large gradient norm . While some of these methods are computationally efficient and practical to implement, they often rely on simple heuristics that may not capture the true informativeness or relevance of the examples. As a result, these methods often fall short in terms of performance and may even underperform compared to simple uniform selection in some cases. This highlights the need for more efficient and principled online batch selection techniques that can identify the most informative examples based on a deeper understanding of the model's learning dynamics and the relationships between the examples.

In this work, we propose _GREedy Approximation Taylor Selection_ (GREATS), which addresses the limitations of existing methods and significantly improves the convergence speed and generalization performance of language model training. We summarize our contributions as follows.

**I. Principled Formulation for Optimal Batch Selection.** We introduce a principled formulation of the online batch selection problem as a set utility function optimization task. Given a small set of validation data from the target domain, the utility function measures the reduction in loss achieved by updating the model with a selected subset of the training batch. Unlike previous methods that rely on heuristics, this framework aims to directly optimize the model's performance on the validation set, ensuring the selection of informative and diverse examples.

**II. Efficient Approximations for Scalable Batch Selection.** The set function optimization formulation naturally leads to a greedy algorithm that iteratively selects the most informative examples based on their marginal contribution to the model's performance. However, directly applying the greedy algorithm to optimize the validation performance involves updating model with each potential candidate training point and checking the validation performance, which is computationally inefficient. To tackle this challenge, we propose to leverage Taylor expansions to approximate the variation of validation loss in one-step gradient descent. The key insight is that the impact of a training example on the model's validation loss can be efficiently approximated using gradient inner-products between the training examples and the validation data. This approximation eliminates the need for expensive model updates and validation loss evaluations for each candidate subset.

**III. Online Batch Selection at the Speed of Regular Training.** A direct implementation of GREATS would require computing per-sample gradients, which is computationally expensive. To address this challenge, we develop a novel technique called _"ghost inner-product"_ that allows for the efficient computation of pairwise gradient inner-products without the need to instantiate any model-sized vectors. As gradient inner-products arise in various machine learning algorithms and applications beyond data selection, this technique may be of independent interest.

**IV. Comprehensive Evaluations.** We conduct extensive experiments on various language modeling tasks to thoroughly assess the performance of GREATS. We show that GREATS consistently speeds up training convergence and improves generalization performance across different models, training datasets, and evaluation datasets, even with a limited number of validation points. Furthermore, we show that GREATS can provide benefits even in the pretraining setting, where the validation data comes from the same domain as the training dataset. This highlights the robustness and versatility of our approach in various learning scenarios. In addition to its performance benefits, we empirically confirm that GREATS, equipped with the "ghost inner-product" technique, achieves a runtime comparable to regular training. This underscores the practical feasibility of our approach.

## 2 Related Works

**Online Batch Selection.** Few studies have investigated the use of online batch selection to enhance the training of models before the era of large language models. [28; 23; 20] examined selecting the "hard examples" based on their gradient norm or maximum sample loss. While some of these methods are computationally efficient, they often depend on simple heuristics that cannot represent the true informativeness. [30; 9] suggested using additional reference models to more accurately estimate the importance of samples. However, recently  demonstrated these methods are computationally expensive and cannot directly apply to large language models.

**Static Data Selection for Large-scale Models.** Recently, there has been a growing interest in design methods to select data _before_ training foundation models. We point the readers to  for a comprehensive literature review. These works select training data only _once_, prior to the training process. This is primarily motivated by efficiency concerns, as the time spent on data selection can be amortized over a large number of training steps. However, the non-adaptive nature of this single-step selection often results in suboptimal performance, as the selected data may not be the most informative or diverse throughout the entire training process . Moreover, these algorithms often require extensive and complex data preprocessing steps. Some of the data selection algorithms even require training an additional model solely for the purpose of data selection [43; 44], which introduces additional computational costs and implementation complexity to the training pipeline. These drawbacks emphasize the need for more efficient and _adaptive_ data selection techniques that can dynamically identify the most informative and relevant examples throughout the training process.

**Online Domain Reweighting.** Recent work has explored online methods for dynamically re-weighting domains during language model pre-training. Compared with online batch selection, this approach operates at a coarser granularity by focusing on data source-level selection rather than individual examples, and typically updates domain weights less frequently. Similar to this work,  uses a gradient-based influence estimation to update domain weights.  uses training loss as a reward signal to adapt domain sampling probabilities. This has been recently refined by  through a scaling law-based method.

## 3 Background

In this section, we introduce the setup of online batch selection and the concept of a utility function. We then discuss the limitations of existing scoring and top-\(k\) paradigm in the literature.

**Set-up of online batch selection.** Given a training dataset \(_{}=\{z_{i}\}_{i=1}^{N}\), a deep learning model is usually being trained to minimize the training loss \(_{i=1}^{N}(w,z_{i})\) via an iterative optimization procedure such as stochastic gradient descent (SGD). Starting with an initial model \(w_{0}\), during an iteration \(t\), a batch \(S\) of the training points is being used, and update the model parameters from \(w_{t}\) to \(w_{t+1}\) via \(w_{t+1}:=w_{t}-_{t}_{z S}(w_{t},z)\) where \(_{t}\) is the learning rate at iteration \(t\).1 A complete run of neural network training thus consists of model checkpoints \(\{w_{0},w_{1},,w_{T}\}\). In the setting of online batch selection, a large batch \(_{t}=\{z_{1},,z_{B}\}\) is being sampled from the training set \(_{}\) at training iteration \(t\). An online batch selection algorithm aims to select the most valuable subset \(}_{t}\) from \(_{t}\). It can be naturally formulated as an optimization problem, where the objective is to maximize the utility of the selected \(}_{t}\) for model update. Here, we describe the existing online batch selection algorithms through the concept of a _utility function_.

**Utility Function.** At training iteration \(t\), a _utility function_\(U^{(t)}\) maps an input training data batch \(S\) to a score indicating the utility of this batch for the model update at iteration \(t\). Specifically, for a given utility function \(U\), the task of _online batch selection_ over a candidate batch \(_{t}\) is to identify the subset \(}_{t}_{t}\) that optimizes:

\[}_{t}^{(k)}=*{argmax}_{S_ {t},|S|=k}U^{(t)}(S)\] (1)

where \(k\) is a fixed budget of sample number \(k<|_{t}|\) used to update the model. Since \(U^{(t)}\) is a set function, solving Equation (1) presents significant challenges, as it may require evaluating the utility \(U^{(t)}(S)\) for a large number of subsets \(S_{t}\). Existing online batch selection methods circumvent this issue through _"Scoring and Top-\(k\) Paradigm"_, which compute an importance score \(_{z}\) for each data point \(z_{t}\) and then selecting the subset of data points with the highest importance scores. For example, [28; 20] use the individual loss on the training data point \(_{z}=(w_{t},z)\) as the importance score.  use the individual gradient norm \(_{z}=\|(w_{t},z)\|\) as the importance score. [30; 9] leverage a reference model and use the "reducible loss" as the importance score. The use of importance scores for online batch selection essentially defines the utility function \(U^{(t)}(S)=_{i S}_{i}(U)\) and conjectures that the sum of individual data points' importance scores is a reliable indicator of a dataset \(S\)'s utility, hoping for a positive correlation with the model \(w_{t}\)'s performance at the \((t+1)\)-th step after updating on \(S\). Consequently, existing online batch selection strategies aim to maximize \(_{i S}_{i}(U)\) by selecting the top-\(k\) data points with the highest importance scores.

**Limitations of Scoring and Top-\(k\) Paradigm.** Most scoring mechanisms for estimating the value of an individual data point \(z_{t}\) result in similar data receiving similar scores. However, in the context of online batch selection, diversity is crucial. Consequently, a subset \(}_{t}\) consisting of the top-\(K\) valued data points may lack diversity. In particular, duplicate points might be scored equally high and be incorrectly assumed to doubly improve the model, though this is likely not the case. The primary issue with this top-\(K\) methodology is that it ignores the interactions among the selected data points. **When a data point is selected, the importance scores of the remaining data points will usually change.** For instance, the values of data points similar to the selected ones will typically decrease, while the values of data points that are very different from the selected ones will increase.

## 4 Optimizing Utility in Online Batch Selection via Greedy Algorithms

### A Principled Utility Function for Online Batch Selection.

The performance of a model is typically measured through a set of validation points \(\{z^{()}\}\). For a given validation data point \(z^{()}\), an ideal utility function at a single iteration \(t\) is the reduction in validation loss:

\[U^{(t)}(S;z^{()}):=(w_{t},z^{()})-(_{t+1}(S),z^{()})\] (2)

where \(_{t+1}(S):=w_{t}-_{t}_{z}(w_{t},z)\) and \(S_{t}\) is the subset of the batch selected for model update. While this is a principled choice for an optimization objective in online batch selection, optimizing \(U^{(t)}\) is computationally expensive, as it involves evaluating model updates with respect to combinatorially many subsets \(S_{t}\) (a total of \(_{t}|}{k}\) subsets).

**Vanilla Greedy Algorithm.** To address the challenge of evaluating the objective function for numerous subsets, the greedy optimization algorithm is widely used due to its effectiveness in set function optimization. The greedy algorithm iteratively selects the element that provides the largest _marginal gain_ to the utility function, given the previously selected elements. Mathematically, when a utility function \(U^{(t)}\) is given, the greedy algorithm selects data points \(z_{t}\) one at a time. At each selection round, the algorithm selects the data point \(z^{*}=*{argmax}_{z_{t}}_{t}}U^{(t)}(}_{t}\{z\})-U^{(t)}(}_{t})\). This process continues until \(k\) data points have been added to \(}_{t}\). The greedy algorithm is known to provide near-optimal solutions for monotone submodular set functions, with a famous \((1-1/e)\)-approximation guarantee . The greedy algorithm only requires \(O(k|_{t}|)\) evaluations of \(U^{(t)}\), a significant improvement over the \(_{t}|}{k}\) evaluations required by the brute-force method.

However, optimizing \(U^{(t)}\) using the greedy algorithm is still not practical for online batch selection. Each evaluation of the utility function \(U^{(t)}(S)\) in (2) involves computing aggregated gradients, updating the model, and calculating the validation loss, which can significantly increase the per-iteration cost of training. Since online batch selection algorithms run alongside the model training, these costs cannot be amortized across training runs as they would be with static dataset selection. This makes the greedy algorithm infeasible for real-time online batch selection.

### An Efficient Greedy Algorithm for Utility Optimization without Utility Evaluation

Here, we develop an efficient approximation for the marginal gain of a data point \(z_{t}}_{t}\) to the utility of the already selected subset \(}_{t}\). For notational simplicity, we denote \(_{(z_{i})}:=(w_{t},z_{i})\) for all \(z_{i}_{t}\). Given that the learning rate \(_{t}\) in model training is typically small, a lower-order Taylor expansion often provides an accurate approximation for the change in loss during a single gradient update, with approximation errors of \(O(_{t}^{2})\) for first-order approximations.

\[U^{(t)}(z_{i}|}_{t}) :=U^{(t)}(}_{t}\{z_{i}\})-U^{(t)}(}_{t})\] \[=(_{t+1}(}_{t}),z^{()})-(_{t+1}(}_{t}\{z_{i}\}),z^{( )})\] \[=(_{t+1}(}_{t}),z^{( )})-(_{t+1}(}_{t})-_{t} _{(z_{i})},z^{()})\] \[_{t}_{(z_{i})}(_{t+1}(}_{t}),z^{()})\] (3)

**Interpretation.** The first-order approximation of the marginal gain \(U^{(t)}(z_{i}|}_{t})\) computes the inner-product between **(1)** the gradient of the individual training loss with respect to the original model \(w_{t}\), and **(2)** the gradient of the validation loss with respect to the "virtual model" \(_{t+1}(}_{t})\), i.e., \(w_{t}\) updated with the existing selected batch. The inner-product represents the direct influence of \(z_{i}\) on the validation loss at the "virtual model" \(_{t+1}(}_{t})\). The gradient \(_{(z_{i})}\) is computed with respect to \(w_{t}\) instead of \(_{t+1}(}_{t})\) because the model update process is performed using the gradients with respect to \(w_{t}\). The approximation in (3) essentially estimates the improvement in validation loss by including \(z_{i}\) in the model update, assuming that \(}_{t}\) is already guaranteed to be included.

However, the computation of \((_{t+1}(}_{t}),z^{()})\) still requires obtaining the updated model parameter \(_{t+1}(}_{t})\) and performing additional backpropagations to compute the validation gradient, which again incurs significant computational overhead. To efficiently approximate it, we use another Taylor expansion as follows:

\[(_{t+1}(}_{t}),z^{()})=(w_{t}-_{t}_{z}_{t}} _{(z)},z^{()})_{(z^{()})}- _{t}_{(z^{()})}_{z}_{t}} _{(z)}\]

where the Hessian matrix \(_{(z^{()})}:=^{2}(w_{t},z^{()})\), and \(_{(z^{()})}:=(w_{t},z^{()})\). Plugging this approximation back into (3), we have

\[U^{(t)}(z_{i}|}_{t}) _{(z_{i})}_{(z^{()})}}_{$}}-^{2}_{(z_{i})}_{(z^{( )})}_{z}_{t}}_{(z)}}_{}\] (4)

**Interpretation.** The first gradient inner-product \(_{t}_{(z_{i})}_{(z^{()})}\) coincides with the TracIN score proposed in  as a measure for a data point's importance. It captures the alignment between the gradient of the training loss for \(z_{i}\) and the gradient of the validation loss, indicating how much the update based on \(z_{i}\) would contribute to the reduction of the validation loss with respect to the original model \(w_{t}\). The second term \(-_{t}^{2}_{(z_{i})}_{(z^{()})}_{z }_{t}}_{(z)}\) is a correction term for \(z_{i}\)'s original importance after picking \(}_{t}\). It penalizes the similarity between \(z_{i}\) and the data points in \(}_{t}\), as measured by the Hessian-weighted inner-product of their gradients. Intuitively, if the gradient of \(z_{i}\) is similar to the gradients of the data points in \(}_{t}\), the correction term will be large, reducing the overall marginal gain of adding \(z_{i}\) to the selected subset. This encourages the selection of diverse data points that provide complementary information to the model update.

**Algorithm.** Using the approximation from (4), we develop a new algorithm that approximates the vanilla greedy algorithm. Initially, each data point \(z_{t}\) is assigned an importance score \(_{z}\) initialized as \(_{z}=_{t}_{(z)}_{(z^{})}\), which approximates the marginal gain of adding \(z\) to an empty set, i.e., \(U^{(t)}(z_{i}\{\})=U^{(t)}(\{z_{i}\})\). The algorithm begins by selecting the data point with the highest importance score, \(z_{1}^{*}=*{argmax}_{z_{t}}_{z}\). After selecting a data point \(z^{*}\) for model update, the importance scores for the remaining data points are adjusted by \(-_{t}^{2}_{(z_{i})}_{(z^{})}_ {(z^{*})}\). This adjustment approximates the marginal gain of adding each remaining data point to the set containing \(z^{*}\), i.e., \(U^{(t)}(z_{i}\{z^{*}\})\). The algorithm iteratively selects the data point with the highest adjusted importance score and updates the scores for the remaining points until \(k\) data points have been selected. As we can see, this iterative process closely mimics the behavior of the vanilla greedy algorithm while not requiring any actual evaluation of \(U^{(t)}\), allowing for a computationally tractable approximation of the greedy algorithm in the context of online batch selection. The pseudocode for the proposed algorithm is detailed in Algorithm 1.

**Validity of Taylor Approximation.** We evaluate the fidelity of using Taylor expansion to approximate \(U^{(t)}\). Following the experimental settings from our GPT2 experiments detailed in Appendix B, we sample different batch subsets \(S\) and evaluate \(U^{(t)}(S)\) at the \(3500\)th training iteration. Figure 1 illustrates the correlation between actual and predicted validation loss changes in a single gradient update step. Panel (a) shows the correlation when using only the first-order term (the sum of training gradients' dot products with the validation point) for loss change approximation. Panel (b) demonstrates the improved correlation when incorporating both the first-order term and the Hessian interaction term. The enhanced correlation coefficient with the inclusion of the Hessian term indicates that our approximations effectively capture the actual loss dynamics, with the second-order term providing substantial improvement in predictive accuracy.

### The Ghost Inner-Product Technique

**Implementation Challenges of Algorithm 1.** Although Algorithm 1 eliminates the need for explicit utility evaluations and relies solely on gradient and Hessian information, its efficient implementation remains a challenge. The initial importance scores \(_{t}_{(z)}_{(z^{})}\) require computing the inner-products between the gradients of each \(z_{t}\) and the validation point \(z^{}\). Directly implementing this would involve calculating the individual gradients for every data point in \(_{t}\). This cannot leverage the parallel processing capabilities of GPUs and would require running backpropagation \(|_{t}|\) times with a mini-batch size of 1, resulting in a significantly higher per-iteration runtime cost compared to regular training. Furthermore, the correction term \(-_{t}^{2}_{(z)}_{(z^{})}_{(z^ {*})}\) requires computing the gradient-Hessian-gradient product for each pair of \(z,z^{}_{t}\). Even if we approximate the Hessian as the identity matrix (\(_{(z^{})}\)), calculating the pairwise \(_{(z)}_{(z^{})}\) still necessitates either storing all individual gradient vectors \(\{_{(z)}\}\) or recomputing \(_{(z)}\) during each round of greedy selection. Both the memory and computational demands of these approaches are impractical for training large-scale models.

**Efficient Computation of ALL Initial Importance Scores.** To address the challenge of computing the gradient inner-products between all \(z_{t}\) and \(z^{}\), we propose a novel technique called "ghost inner-product", which is inspired by the "ghost clipping" technique from the differential

Figure 1: **(a)** We show the correlation between the ground-truth model validation loss change in one gradient update iteration \(U^{(t)}(S;z^{}):=(w_{t},z^{})-(_{t+1}(S),z^{})\) and the first-order Taylor approximation \(_{z S}_{t}_{(z)}_{(z^{})}\). **(b)** We show the correlation between \(U^{(t)}(S;z^{})\) and the first-order approximation corrected by the Hessian interaction \(_{z S}_{t}_{(z)}_{(z^{})}- _{z,z^{} S}_{t}^{2}_{(z)}_{(z^{})}_{(z^{})}\).

privacy literature [5; 6]. The key idea behind "ghost inner-product" is to avoid explicitly computing individual gradient vectors, thereby improving the efficiency of the algorithm. To illustrate this technique, consider a simple linear layer \(=\), where \(^{d_{1} d_{2}}\) is the weight matrix, \(=(^{(1)},,^{(B)})^{} ^{B d_{1}}\) is the mini-batch input, and \(=(^{(1)},,^{(B)})^{} ^{B d_{2}}\) is the output (i.e., the pre-activation tensor). Let \(^{(i)}\) denote the individual loss on \(z_{i}\). By applying the chain rule, we can express the gradient of an individual loss \(^{(i)}:=(w,z_{i})\) with respect to \(\) as

\[}{}=}{ ^{(i)}}^{(i)}}{ }=^{(i)}}^{(i)}\] (5)

where \(:=_{j=1}^{B}^{(j)}\) is the aggregated loss. Note that the output gradient \(^{(i)}}\) is readily available during the backpropagation pass. To efficiently compute the gradient inner-product between a validation point and each training point, we include the validation data \(z^{()}\) together in the batch for backpropagation. That is, we take the backpropagation on \(_{j=1}^{B}^{(j)}+^{(z^{()})}\). Hence, for each training-validation pair \((z_{i},z^{()})\), we have the gradient inner-product

\[}{})})}}{}=((^{(i)}})^{}(^{(z^{()})}}))((^{(i)})^{}^{(z^{()})}))\]

By using the "ghost inner-product" technique, we can compute the result without explicitly forming any full gradient vectors. Consequently, computing the gradient inner-product between every pair of training and validation points requires only one backpropagation, which is significantly more efficient than the direct method that would require \(>|_{t}|\) backpropagations. We note that the "ghost inner-product" technique can be applied to various types of layers beyond linear layers. Similar decompositions as in Equation (5) have been studied in the differential privacy literature [36; 5; 26], enabling the extension of this technique to other layer types. Extension on LoRA is in Appendix A.

**Efficient Approximation of Importance Correction.** The importance correction term \(_{(z)}_{(z^{()})}_{(z^{*})}\) poses computational challenges due to the involvement of the Hessian matrix. A straightforward approximation is to assume \(_{(z^{()})}\), simplifying the problem to computing the gradient inner-product \(_{(z)}_{(z^{*})}\). This approximation has been widely used in the literature, particularly in the context of second-order optimization methods and meta-learning [29; 13; 32]. The key motivation behind this approximation is that the Hessian matrix is often diagonally dominant, especially when the model is close to a local minimum . By assuming \(_{(z^{()})}\), the importance correction term simplifies to \(_{z}-_{t}^{2}_{(z)}_{(z^{*})}\). We can then apply the ghost inner-product technique previously developed for computing pairwise gradient inner-products. This allows us to efficiently compute the importance correction term without explicitly forming the individual gradient vectors or the Hessian-vector products.

**Merging Batch Selection and Gradient Update in One Backpropagation.** Utilizing the techniques developed in this section, we can calculate or approximate all importance scores and correction terms in a single backpropagation pass, without the need to materialize any model-sized vectors. Although computing the gradient of the aggregated training loss \(_{z_{t}}^{(i)}\) for the training batch is necessary for parameter updates, an additional backpropagation pass is not required. By retaining the activations and output gradients from the previous backpropagation, we can efficiently compute this gradient without incurring the cost of another pass (see Appendix A.3 for details). Consequently, the process of training with batch selection introduces minimal additional runtime overhead. This approach provides substantial benefits over the direct method of materializing per-sample model-sized gradients, making it more feasible for real-world applications.

## 5 Experiments

In this section, we first evaluate the performance of GREATS against several baselines on a diverse set of models, training datasets, and validation set configurations. We then empirically examine its computational efficiency when implemented with "ghost inner-product" technique from Section 4.3.

### Experimental Setup

**Model-Training-Evaluation Pairs.** We examine multiple combinations of models, training datasets, and evaluation datasets to evaluate our proposed GREATS algorithm, as shown in Table 1. Specifically,we fine-tune three large language models (LLMs): Llama-2-7B , Mistral-7B , and Llama-3-8B  using LESS training data  and the Alpaca dataset . For evaluation, we employ the MMLU , TydiQA , and SamSUM  datasets (deferred to Appendix C). Additionally, we conduct a pretraining experiment using the GPT-Small model . For both training and evaluation, we use the OpenWebText dataset . In all experiments, we limited the validation data to be small (i.e., \( 16\)) to mimic practical scenarios where training directly on them is impossible.

**Baselines.** We compare our algorithm with regular training and a variety of online batch selection algorithms: **(1)** MaxLoss , which selects training data points with the highest loss values. **(2)** GradNorm , which prioritizes training data points with the highest gradient norms. **(3)** Reference model-based method (RHOLoss), for which we implement the RHO-Loss algorithm from  as a representative baseline. Given computational constraints, we use Llama-3.1-8B-Instruct  as the reference model, paired with Llama2-7B as the target model.2**(4)** Distance-based method (SBERT), which selects training batches based on their semantic similarity to validation data using Sentence-BERT embeddings .

**Training Details.** For all batch selection methods, we select 50% of the batch data for gradient updates during each step. In contrast, the regular training baseline performs updates on the entire batch, utilizing _twice_ as much data as the batch selection methods. In the main paper, we show the results of setting the batch size to 4 for MMLU and TydiQA, 16 for SamSUM and Openwebtext. Additional training details and ablation studies are provided in Appendix B.

### Performance Evaluation

In this section, we present and discuss the comparison between GREATS and the baseline algorithms in model training performance across different models, training, and evaluation datasets. We evaluate the performance on both the validation set (being used for batch selection in GREATS) and a test set that is drawn from the same domain of the evaluation datasets.

**GREATS significantly speeds up training convergence.** In Figure 2 and 4, we show the dynamics of perplexity on the validation and test datasets. As we can see from Figure 2 and Figure 4, across all settings, the GREATS algorithm achieves a significantly faster reduction in test perplexity compared to all of the baselines and often achieves better overall performance.

   Task & Model & Training Dataset & Evaluation Dataset & Number of validation data \\  Fine-tuning & Llama-2-7B  & LESS  & MMLU  & 5 \\ Fine-tuning & Mistral-7B  & LESS  & TydiQA  & 10 \\ Fine-tuning & Llama-3-8B  & Alpaca  & SamSUM  & 16 \\ Pretraining & GPT-Small  & OpenWebText  & OpenWebText  & 16 \\   

Table 1: Combination of models, training datasets, and evaluation datasets

Figure 2: Comparison of the validation and test perplexity dynamics during training for different online batch selection methods on MMLU. We select sociology and US foreign policy subjects.

These results demonstrate the robust effectiveness of our approach in improving model convergence speed and generalization performance across various settings. We note that the validation-free approaches such as GradNorm and MaxLoss may lead to the selection of low-quality data. While it is generally considered that data points with high training loss or large gradients are important to learning, there is another possibility that the data points that achieve these properties are corrupted data which is not learnable.

**GREATS improves performance on downstream tasks.** While perplexity is a direct measure of the performance of NLP models, it is not very interpretable, and the performance is often evaluated in terms of downstream tasks. In Table 2, we show the test accuracy on 9 (randomly selected) subjects from MMLU and TydiQA. As we can see, GREATS consistently outperforms or at least achieves the same accuracy as the baselines. Notably, we observe at least a 3.4% improvement in the average performance of the 9 MMLU subjects compared to all baselines.

**GREATS is robust to the number of validation points.** In Figure 3 (a)-(b), we conduct an ablation study to evaluate the impact of the number of validation points used for GREATS algorithm. As we can see, even with just 2 validation examples, the test perplexity on the MMLU dataset is consistently lower than that of regular training. This robustness may be attributed to the relatively rare format of the validation corpus, which allows GREATS to effectively select examples from the batch that can help learn the particular format. Even if such a selection may overfit the specific validation examples, the selected batch can still improve the performance on the test examples, demonstrating the effectiveness of the GREATS algorithm in adapting to the characteristics of the validation set.

**GREATS also improves LLM pretraining.** In Figure 3 (c)-(d), we evaluate the performance of GREATS on pretraining GPT-Small on OpenWebText. Due to computational resource constraints, we omit the baselines of GradNorm and MaxLoss, as they have been shown to be ineffective in all other experiments. As we can see, even for model pretraining, GREATS provides an improvement in test performance, although the improvement is marginal compared to the gains observed in the fine-tuning experiments. This marginal improvement can be attributed to the fact that the validation data used in this experiment is also drawn from the same distribution as the training set, i.e., OpenWebText. As a result, the selected batches may not provide as much additional information or diversity as in the case of fine-tuning, where the validation data often comes from a different distribution or focuses on specific tasks. Nevertheless, the consistent improvement in test performance suggests that GREATS can still identify informative examples that contribute to better model generalization, even in the pretraining setting.

  
**Method** & **MMLU (avg.)** & **Soc.** & **Pol.** & **Alg.** & **Anat.** & **Astr.** & **Eth.** & **Clin.** & **Bio.** & **Chem.** & **TydiQA** \\ 
**Regular** & 50.4\% & 62\% & 60\% & 40\% & 52\% & 48\% & 52\% & **54\%** & 48\% & 38\% & 54.3\% \\
**GradNorm** & 50.4\% & 62\% & 62\% & 38\% & 50\% & 48\% & 54\% & 52\% & **50\%** & 38\% & 53.4\% \\
**MaxLoss** & 50.8\% & 64\% & 58\% & 40\% & 52\% & 46\% & 54\% & **54\%** & **50\%** & 40\% & 54.7\% \\
**Ours** & **54.2\%** & **68\%** & **64\%** & **44\%** & **56\%** & **52\%** & **56\%** & **54\%** & **50\%** & **44\%** & **55.0\%** \\   

Table 2: Accuracy on MMLU (9 subjects) and TydiQA test set for online batch selection methods.

Figure 4: Comparison of the validation and test perplexity dynamics during training for different online batch selection methods on TydiQA.

Figure 3: (a)-(b): Impact of the number of validation data points on the performance of GREATS. (c)-(d): Comparison of the validation and test perplexity dynamics during GPT2 pretraining for different online batch selection methods.

### Runtime Comparison

We compare the runtime efficiency of GREATS algorithm with "ghost inner-product" technique against GREATS implemented directly by calculating per-sample gradients. The runtime is measured by training GPT-Small on OpenWebText. Additionally, we compare against GradNorm's direct implementation using per-sample gradients, as it is the most similar algorithm to GREATS and allows for a fair comparison.

**GREATS with "ghost inner-product" achieves runtime close to regular training.** As shown in Table 3, the runtime of GREATS using our efficient approximation techniques is comparable to that of regular training, with only a slight increase in runtime due to pairwise inner-product operations (but almost negligible compared with model backpropagation). This demonstrates the effectiveness of our approximation methods in reducing the computational overhead associated with online batch selection. On the other hand, the direct implementation of GREATS, which requires computing per-sample gradients, exhibits a significant runtime increase compared to both regular training and our efficient GREATS implementation. The direct approach is significantly slower than regular training (almost 20 times slower), making it impractical for real-world applications.

The runtime of GradNorm with direct per-sample gradient computation falls between our efficient GREATS implementation and the direct GREATS implementation. This is because GradNorm does not need to compute the per-sample gradients from the validation set, which reduces its computational overhead compared to the direct GREATS implementation. However, GradNorm still incurs a significant runtime increase compared to regular training due to the per-sample gradient calculation for the training batch. We remark that the "ghost norm" technique from differential privacy literature , which is similar to "ghost inner-product", can be used to improve the runtime of GradNorm, potentially bringing it closer to regular training.

## 6 Conclusion and Limitations

In this work, we introduced GREATS, a novel online batch selection algorithm designed to enhance the efficiency and effectiveness of training large language models. Here, we briefly summarize the limitations of this work.

**I. Availability of validation data.** One potential limitation of GREATS is that it requires the validation data to be available before training. We stress that there are many scenarios where the validation data is naturally available before training such as fine-tuning or domain adaptation. Developing a validation-free variant of GREATS is an interesting future work.

**II. Extension to Adam.** The ghost inner-product technique developed in this work is specifically tailored for Stochastic Gradient Descent (SGD). It is not directly extendable to other popular optimizers like Adam due to their normalization terms. Nonetheless, using SGD as a proxy for Adam has proved to be effective in our experiment. Extending our ghost inner-product technique to Adam and similar optimizers remains an exciting direction for future research.

**III. Memory constraint for large batch sizes.** In scenarios where GPU memory constraints prevent adding validation data to the training batch for backpropagation, we can easily extend our "ghost" techniques by using gradient accumulation. However, this may increase runtime due to additional backpropagation steps for validation data, it maintains the feasibility of our techniques under memory constraints. Improving computational efficiency for large batch sizes remains an important direction for future research.

**IV. Perplexity may not be an ideal objective.** In this work, the utility function is being defined as the validation loss. While GREATS achieves promising results overall in terms of test perplexity, we note that perplexity may not reflect the performance in downstream tasks. While GREATS usually achieves higher performance on the downstream task, the improvement is often minor. Directly optimizing in terms of the downstream performances is another important future work.

   & **Throughput** \\ 
**Regular Training** & 76.2 \\
**GREATS (ghost)** & 71.3 \\
**GREATS (direct)** & 4.2 \\
**GradNorm (direct)** & 6.8 \\  

Table 3: Efficiency comparison of different implementations of GREATS. We use throughput-# training data points being processed per second-as the efficiency metric. Second-as the efficiency metric. The runtime of GradNorm with direct per-sample gradient computation falls between our efficient GREATS implementation and the direct GREATS implementation. This is because GradNorm does not need to compute the per-sample gradients from the validation set, which reduces its computational overhead compared to the direct GREATS implementation. However, GradNorm still incurs a significant runtime increase compared to regular training due to the per-sample gradient calculation for the training batch. We remark that the "ghost norm" technique from differential privacy literature , which is similar to "ghost inner-product", can be used to improve the runtime of GradNorm, potentially bringing it closer to regular training.