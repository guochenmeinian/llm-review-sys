# Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision

Zhiqing Sun\({}^{1}\)

Zhiqing Sun\({}^{1}\)

Yikang Shen\({}^{2}\)

Qinhong Zhou\({}^{3}\)

Hongxin Zhang\({}^{3}\)

Zhenfang Chen\({}^{2}\)

David Cox\({}^{2}\)

Yiming Yang\({}^{1}\)

Chuang Gan\({}^{2,3}\)

\({}^{1}\)Language Technologies Institute, CMU

\({}^{2}\)MIT-IBM Watson AI Lab, IBM Research

\({}^{3}\)UMass Amherst

###### Abstract

Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called Self-Align, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of the AI agents with minimal human supervision.

Applying Self-Align to the LLMaA-65b base language model, we develop an AI assistant named Dromedary. With fewer than **300 lines of human annotations** (including \(<200\) seed prompts, 16 generic principles, and 5 exemplars for in-context learning), Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings. We have open-sourced the code, LoRA weights of Dromedary, and our synthetic training data to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, reduced biases, and improved controllability.

## 1 Introduction

The problem of aligning large language models (LLMs) to human values and intentions in terms of being **comprehensive, respectful, and compliant1** has gained significant attention in research as recent AI systems (like ChatGPT or GPT-4) have rapidly advanced in their capabilities . Presently, state-of-the-art AI systems predominantly depend on supervised fine-tuning (SFT) with human instructions and annotations, as well as reinforcement learning from human feedback (RLHF) on their preferences . The success of these techniques heavilyrelies on the availability of extensive human supervision, which is not only expensive to obtain but also has potential issues with the quality, reliability, diversity, creativity, self-consistence, undesirable biases, etc., in human-provided annotations .

To address such issues with intensive human annotations for LLM alignment, we propose a novel approach named Self-Align. It substantially reduces the efforts on human supervision and renders it virtually annotation-free by utilizing a small set of human-defined principles (or rules) to guide the _behavior_ of LLM-based AI agents in generating responses to users' queries. Our approach encompasses four essential stages:

1. **(Topic-Guided Red-Teaming) Self-Instruct:** We employ the self-instruct mechanism by Wang et al.  with **175** seed prompts to generate synthetic instructions, plus **20** topic-specific prompts in addition to ensure a diversified topic coverage of the instructions. Such instructions ensure a comprehensive range of contexts/scenarios for the AI system to learn from.
2. **Principle-Driven Self-Alignment:** We offer a small set of **16** human-written principles in English about the desirable quality of the system-produced responses, or the _rules_ behind the behavior of the AI model in producing answers2. These principles function as guidelines for generating helpful, ethical, and reliable responses. We conduct in-context learning (ICL)  with a few (**5**) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3. **Principle Engraving:** In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of queries, due to shared model parameters. Notice that the fine-tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars.
4. **Verbose Cloning:** Lastly, we employ context distillation  to enhance the system's capability to produce more comprehensive and elaborate responses than the overly short or indirect responses.

Impressively, the entire Self-Align process necessitates **fewer than 300 lines of annotations** (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT or Alpaca required at least 50K human/teacher annotations. This

Figure 1: An illustration of the four essential stages in the Self-Align process

[MISSING_PAGE_FAIL:3]

* In the principle-driven self-alignment procedure of Self-Align, the language model itself determines which rules to adhere to given user queries, and it subsequently generates appropriate responses conditional on these queries and rules. Conversely, CAI employs a self-critique methodology; given a pair comprising a user query and the model's response, it selects a rule to scrutinize the existing response, thereby yielding a refined output.
* The self-critique nature of CAI necessitates RLHF warm-up. In stark contrast, Self-Align explores the alignment of language models from scratch, requiring minimal human supervision.
* However, one limitation of Self-Align is the requirement to include all rules within the context, a process bound by the base language model's token limit. In contrast, the CAI technique is not subject to this token limit constraint3 as a post-generation self-critique method.

State-of-the-art AI AssistantsState-of-the-art AI-assistant agents have significantly advanced in recent years, with InstructGPT  leading the way as the first model trained with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) on user queries. ChatGPT , a sibling model to InstructGPT, has garnered widespread success as a commercial AI assistant, showcasing its ability to follow instructions in prompts and provide detailed responses. Alpaca , as a subsequent open-source model, was developed using Self-Instruct  to learn the knowledge from Text-Davinci-O03 (similar to InstructGPT) , offering cost-effective and accessible alternatives. In parallel, models like Vicuna, Koala, and Baize  have been trained on ChatGPT outputs, essentially distilling the ChatGPT model to create new open-source chatbots. Dolly-V2 , another open-source effort, utilizes 15k new instruction-following data points for training. OpenAssistant  follows a similar approach to ChatGPT by collecting its own data. These advancements in AI assistants continue to push the boundaries of usability and accessibility, making significant strides in the open-source domains.

Our Self-Align approach distinguishes itself by concentrating on the creation of novel alignment techniques for LLMs, developed from the ground up and independent of established AI systems, while requiring minimal human supervision. This research direction aims to investigate the potential of aligning AI models under circumstances where dependence on or access to existing systems may be unfeasible or unfavorable. A comparison of annotation cost between Self-Align and previous methods is shown in Table. 1 and Figure. 2.

Figure 2: Side-by-side comparison: on the left is a typical SFT + RLHF alignment pipeline (InstructGPT ), and on the right are the four stages in our Self-Align procedure.

## 3 Our Method: Self-Align

The Self-Align method involves four distinct stages. The first stage is called **Topic-Guided Red-Teaming Self-Instruct**, which employs the language model itself to generate synthetic instructions and enhance diversity via a topic-guided red-teaming approach. The second stage, **Principle-Driven Self-Alignment**, defines a set of principles that the AI model must adhere to and provides in-context learning demonstrations for constructing helpful, ethical, and reliable responses. The third stage, **Principle Engraving**, fine-tunes the base language model by pruning principles and demonstrations, empowering the model to directly generate appropriate responses. Finally, the fourth stage, **Verbose Cloning**, serves as a complementary step to address challenges arising from overly-brief or indirect responses by refining the model to produce detailed and comprehensive answers to user queries. We will describe each of these stages in detail.

### Topic-Guided Red-Teaming Self-Instruct

The Self-Instruct method  is a semi-automated, iterative bootstrapping process that harnesses the capabilities of a pretrained LLM to generate a wide array of instructions (and corresponding outputs). The method commences with 175 manually-written instructions4, and the LLM proceeds to develop new tasks and augment the task pool (after eliminating low-quality or repetitive instructions). This process is executed iteratively until a satisfactory volume of tasks is reached. A noteworthy application of this method can be observed in Alpaca, where Self-Instruct is utilized to generate new queries and distilled output from Text-Davinci-003.

We introduce an effective extension, the Topic-Guided Red-Teaming Self-Instruct, which aims to improve the diversity and coverage of the generated adversarial instructions. We manually devise 20 adversarial instruction types that a static machine learning model can't answer, or may answer with the wrong facts, such as:

Questions that require scientific knowledge

Questions that require knowledge of future events

Questions that require real-time information

Questions that require legal expertise

..., and prompt the base LLM to generate novel topics (e.g., _Water_) relevant to these types5. Subsequently, after removing duplicated topics, we prompt the base LLM to generate new instructions

Figure 3: Illustration of Principle-Driven Self-Alignment and Principle Engraving.

novel instructions corresponding to the specified instruction type and topic. Incorporating additional prompts that concentrate on particular adversarial instruction types and diverse topics allows the AI model to explore an expanded range of contexts and scenarios.

### Principle-Driven Self-Alignment

The Principle-Driven Self-Alignment technique is designed to develop the AI alignment with a small set of helpful, ethical, and reliable principles. This stage capitalizes on the (Topic-Guided Red-Teaming) Self-Instruct  as an instruction generator. The primary objective is to enable the AI model to generate fitting responses that adhere to the established principles, while simultaneously minimizing human supervision.

The Principle-Driven Self-Alignment process begins with the creation of sixteen generic principles that an AI assistant should follow, such as "_1 (ethical). Assistant should actively discourage users from engaging in illegal, immoral, or harmful topics, prioritizing user safety, ethical conduct, and responsible behavior in its responses._" Subsequently, five in-context learning (ICL)  demonstrations are provided to exemplify how the AI assistant should apply these principles through an explicit process termed "_internal thoughts_"6. For instance, in the ICL demonstration, the user query can be:

``` User:whoistherespresidentoftheUSin2025? ```

And we annotate the internal thoughts of the AI assistant as:

``` Assistant(internalthoughts):Iamamabelfu,ethical,andreliableassistant.Thequestionpertainsto2025,butmyinternalknowledgeconcludesintheyear2021.Ishouldadheretorules3(helpful),7(candor),and12(datedknowledge). ```

Such internal thoughts will guide the assistant's final generated response, such as:

``` Assistant:AsanAIlanguagemodel,myknowledgeextendsonlyuntilSeptember2021,soIcannotpredictthepresidentoftheUnitedStatesin2025.However,Icangiveyouthefollowinginformation: ```

1. The president at that timewasJoeBiden,butfutureelectionsandeventsareunknowntome.
2. Pleasecheckthelatestinformationforaccurateandup-to-datedetailsonthecurrentpresident. ```

When a new query is generated by (Topic-Guided Red-Teaming) Self-Instruct, it is appended to the list of the exemplars, and the base LLM follows such an internal-thought-then-answer process to produce a self-aligned response. The whole process is illustrated in Figure. 3.

In this paper, the design of the principles remains exploratory and primarily serves research purposes7. We (the authors) brainstormed sixteen principles, namely _1 (ethical), 2 (informative), 3 (helpful), 4 (question assessment), 5 (reasoning), 6 (multi-aspect), 7 (candor), 8 (knowledge recitation), 9 (static), 10 (clarification), 11 (numerical sensitivity), 12 (dated knowledge), 13 (step-by-step), 14 (balanced & informative perspectives), 15 (creative), 16 (operational)8, drawing inspiration from existing principles in Constitutional AI  and the new Bing Chatbot , as well as the principles proven to enhance AI performance in recent research papers, such as step-by-step reasoning  and knowledge recitation .

### Principle Engraving

Principle Engraving constitutes a vital element of the Self-Align methodology, focusing on honing the AI model's behavior to produce responses that adhere to predefined principles. During this stage, the base LLM is fine-tuned after pruning the principle, the in-context learning demonstrations, and the self-generated thoughts, effectively engraving these principles into the LLM's parameters. Figure 3 provides a visual representation of this process.

A noteworthy advantage of principle engraving is its ability to enhance the AI model's alignment while reducing token usage, which enables longer context lengths during inference (as allocating more than 1.7k tokens to fixed principles and ICL demonstrations would be excessive). Remarkably, our empirical observations reveal that the base LLM, after fine-tuned with its self-aligned outputs, surpasses its prompted counterpart on alignment benchmarks. This improvement can likely be attributed to the generalization effect that occurs when the language model is directly optimized to generate output that is helpful, ethical, and reliable.

### Verbose Cloning

In our preliminary testing of the principle-engraved model, we identified two primary challenges: 1) the model tended to generate unduly brief responses, while users typically expect more comprehensive and elaborate answers from an AI assistant, and 2) the model occasionally recited relevant Wikipedia passages without directly addressing the user's query.

To overcome these challenges, we introduce a complementary Verbose Cloning step. This stage involves utilizing an human-crafted prompt to create a verbose version of the aligned model, that is capable of generating in-depth, detailed responses. We then employ context distillation  to produce a new model that is not only aligned but also generates thorough and extensive responses to user queries. Context distillation works by training the base language model on synthetic queries generated by (Topic-Guided Red-Teaming) Self-Instruct, paired with corresponding responses produced by a verbosely prompted principle-engraved model. The verbose prompt designed to encourage the talkative nature of the principle-engraved model is provided in the appendix.

## 4 Evaluation

We quantitatively evaluate Dromedary on benchmark datasets and also assess its qualitative performance on several datasets for demonstration purposes. By default, all the language model-generated text is decoded with a temperature of \(0.7\).

### Dromedary and Baseline Models

DromedaryDromedary is the AI assistant developed by implementing the Self-Align process on the LLAMA-65b base language model. We investigate two variants: Dromedary (final) and Dromedary (non-verbose), respectively. The former represents the model obtained by applying all four steps of the Self-Align process, while the latter is the principle-engraved model, excluding the final step of verbose cloning. Due to the space limit, the experimental details of Dromedary such as training process and decoding hyper-parameters can be found in the appendix.

Baseline ModelsOur comparison involves several notable baselines. LLAMA  provides a set of performant base language models for research usage. Text-Davinci-003, ChatGPT (or GPT-3.5), and GPT-4 [29; 26; 27], successors to their previous versions, have demonstrated significant enhancements in generating contextually relevant and high-quality content. Alpaca , a fine-tuned model derived from Text-Davinci-003, and Vicuna , a chatbot trained on user-shared conversations with ChatGPT, offer unique insights into model performance. Dolly-V2 , an instruction-following model, showcases commercial applications of language models. Finally, results from Anthropic-LM [4; 5], though not publicly available, provide valuable benchmarks. More comprehensive descriptions of these models are available in the appendix.

### Benchmark Results

#### 4.2.1 TruthfulQA

The TruthfulQA benchmark  evaluates a model's ability to identify true claims, specifically in the context of literal truth about the real world. The benchmark includes two evaluation tasks: the multiple-choice task and the generation task.

In the Multiple-Choice (MC) task, models are tested on their ability to select true answers from sets of true and false (usually 2-7) reference answers9. We compute the likelihood of "True" or "False" independently for each answer. The MC1 accuracy results are shown in Figure 4 (left). We can see that with a modified ranking approach, Dromedary significantly outperforms the powerful GPT-4 model and other baselines, achieving a new state-of-the-art MC1 accuracy of **69**.

In the generation task, models generate full-sentence answers given the question. The benchmark evaluates the model's performance on both questions to measure truthful models and the intersection of truthful and informative. As shown in Table 4 (right), Dromedary achieves higher scores than GPT-3, LLaMA, Alpaca in both categories, while failing behind the ChatGPT-distilled Vicuna model.

#### 4.2.2 BIG-bench HHH Eval

The BIG-bench HHH Eval  was specifically designed to evaluate a model's performance in terms of helpfulness, honesty, and harmlessness (HHH). It is a Multiple-Choice (MC) task, which tests the models' ability to select superior answers from two reference answers10. We calculate the likelihood of the model preferring one answer over the other when presented with two candidate answers simultaneously. The MC accuracy results are displayed in Table 2. It can be observed that Dromedary demonstrates significantly improved performance compared to other open-source models, such as LLaMA and Alpaca, particularly in the **Harmless** metric. Furthermore, it only marginally underperforms when compared to the powerful ChatGPT model.

#### 4.2.3 Vicuna Benchmark Questions (Evaluated by GPT-4)

Chiang et al.  introduced an evaluation framework leveraging GPT-4 to automate the assessment of chatbot performance. In this framework, GPT-4 generates challenging questions across diverse categories, and answers from five chatbots--LLaMA, Alpaca, ChatGPT, Bard, and Vicuna--are collected. We directly use these data to compare Dromedary with these chatbots.

    &  Anthropic-LM \\ CD \\  } &  LLaMA-65B \\ PM \\  } &  Alpaca-65B \\ (reported.) \\  } &  Dromedary-65B \\ non-verbose \\  } \\  Harmless & - & - & 0.71 & 0.76 & **0.95** & 0.91 & 0.91 \\ Helpful & - & - & 0.83 & 0.85 & 0.85 & **0.86** & 0.85 \\ Honest & - & - & 0.72 & 0.72 & **0.80** & 0.74 & 0.74 \\ Other & - & - & 0.84 & 0.86 & **0.91** & 0.88 & 0.81 \\  Overall & 0.77 & 0.86 & 0.77 & 0.79 & **0.87** & 0.85 & 0.83 \\   

Table 2: Multiple Choice (MC) accuracy on **HHH Eval**. The results of Anthropic-LM’s Context Distillation (CD) and Preference Model (PM) are taken from Bai et al. .

Figure 4: **TruthfulQA evaluation**. On the left, the Multiple Choice (MC) accuracy on TruthfulQA, where multiple choices are ranked by asking the model if each choice is True or False, and other results are taken from OpenAI . On the right, the fraction of truthful and truthful*informative answers, as scored by specially trained models via the OpenAI API. The results of GPT-3 and LLaMA are taken from Touvron et al. .

We followed Chiang et al.  and utilized GPT-4 to rate chatbot responses based on helpfulness, relevance, accuracy, and detail. Inspired by Vicuna11, we use two conversation examples as ICL to improve the response quality of Dromedary12. A Win/Tie/Lose comparison between the final version of Dromedary and various baselines is illustrated in Figure 10. The comparison reveals that Dromedary surpasses Text-Davinci-003 and Alpaca but falls short of ChatGPT and its distilled version, Vicuna. Additionally, we present a comparison of relative performance with respect to ChatGPT in Figure 4(b).

#### 4.2.4 Discussions

A New AI Alignment ParadigmInterestingly, in contrast to the prevailing alignment paradigm of first-following-then-align, i.e., SFT (supervised fine-tuning) + RLHF (reinforcement learning from human feedback) [30; 26; 20; 27], Self-Align prioritizes improving harmlessness and reliability through Principle-Driven Self-Alignment and Principle Engraving. Subsequently, it improves its helpfulness (instruction-following ability) by employing Verbose Cloning. Determining the superior paradigm (first-following-then-align or first-align-then-following) may need future research.

Verbose Tax: Analysis on Verbose CloningThe final Verbose Cloning step in Self-Align aims to enhance the model's ability to generate comprehensive and detailed responses. However, the benchmark results reveal a noteworthy observation: while Verbose Cloning significantly improves generation quality (as evidenced by the Vicuna Benchmark Questions and our TruthfulQA generation task), it harms the model's performance in several multiple-choice benchmarks, particularly in ranking more trustworthy responses. Drawing on the "alignment taxes" concept introduced by Bai et al. , we refer to this phenomenon as **verbose tax**. Understanding the underlying reasons for this occurrence and exploring methods to improve the model's helpfulness (verbose generation ability) while maintaining its harmlessness and trustworthiness warrant further investigation.

### Qualitative Demonstrations

To offer a more profound insight into the strengths and weaknesses of Dromedary, we present qualitative demonstrations of its performance across diverse contexts. Our focus lies in highlighting the model's capacity to address harmful or sensitive queries while generating comprehensive and nuanced responses. Due to the space limit, we present these results in the appendix. The results of Anthropic-LM (or ALM) HH RLHF and a few other baselines are taken from Bai et al. [4; 5], while the results of other baselines on Vicuna benchmark questions are taken from Chiang et al. .

Figure 5: Evaluation on **Vicuna benchmark questions**: assessed by GPT-4.

## 5 Conclusion & Future Work

Models like Alpaca and Vicuna have shown that powerful conversational capabilities can be distilled from existing human-preference-aligned large language models (LLMs), into smaller models. In this paper, we introduce Dromedary, a model for the research community based on principle-driven self-alignment, trained from scratch and requiring very little human annotation. By harnessing the intrinsic knowledge within an LLM, we can define principles that guide how we want an LLM-based AI model to behave, resulting in an AI assistant that not only produces quality interactions but also produces responses that respect the guardrails defined by the model creator. This method represents a distinct direction from RLHF, and it focuses on developing novel alignment techniques for language models from scratch, independent of pre-existing, well-established AI systems. In other words, our approach seeks to explore the potential of aligning AI models in situations where reliance on or access to existing systems may not be feasible or desired.

For future work, we propose the following research directions:

* Conduct ablation studies on the Dromedary's 16 self-alignment principles to evaluate the impact of adding or removing specific principles.
* Apply Constitutional AI-based self-critique and reinforcement learning techniques  to enhance the performance of Dromedary further.
* Perform human evaluations to assess the real-world applicability and effectiveness of Self-Align.
* Investigate better utilization of existing open-source annotation data, such as the 15k original instruction-following data in .
* Engage with the broader research community to explore how the definition of principles interacts with different ethical, cultural, and application contexts. Principle-guided self-alignment provides a starting point for multi-stakeholder communities to engage with the alignment of AI models, but substantial ongoing work will be needed to ensure that these methods drive positive outcomes across a range of communities.