# Combating Bilateral Edge Noise

for Robust Link Prediction

Zhanke Zhou\({}^{1}\)  Jiangchao Yao\({}^{2,3}\)\({}^{}\)  Jiaxu Liu\({}^{4}\)  Xiawei Guo\({}^{4}\)  Quanming Yao\({}^{5}\)

Li He\({}^{4}\)  Liang Wang\({}^{4}\)  Bo Zheng\({}^{4}\)  Bo Han\({}^{1}\)\({}^{}\)

\({}^{1}\)Hong Kong Baptist University \({}^{2}\)CMIC, Shanghai Jiao Tong University

\({}^{3}\)Shanghai AI Laboratory \({}^{4}\) Alibaba Group \({}^{5}\) Tsinghua University

{cszkzhou, bhanml}@comp.hkbu.edu.hk sunarker@sjtu.edu.cn qyaoaa@tsinghua.edu.cn {liujiaxu.ljx, guoxiawei, heli, wangliang}@taobao.com bozheng@alibaba-inc.com

Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk) and Jiangchao Yao (sunarker@sjtu.edu.cn).

###### Abstract

Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input _topology_ and target _label_, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, _i.e._, self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https://github.com/tmlr-group/RGIB.

## 1 Introduction

Link prediction  is the process of determining whether two nodes in a graph are likely to be connected. As a fundamental task in graph learning, it has attracted growing interest in real-world applications, _e.g._, drug discovery , knowledge graph completion , and question answering .

While recent advances in graph neural networks (GNNs)  have achieved superior performances, the poor robustness under the edge noise is still a practical bottleneck to the current deep graph models . Early works explore improving the robustness of GNNs under the node label noise  by means of the smoothing effect of neighboring nodes. Other methods achieve a similar goal via randomly removing edges  or actively selecting the informative nodes or edges and pruning the task-irrelevant ones . However, when applying these noise-robust methods to the link prediction with noise, only marginal improvements are achieved (Sec. 5). The attribution is that different from the label noise, the edge noise here is _bilateral_: it can naturally deteriorate both the _input_ graph topology and the _target_ edge labels, as illustrated in Fig. 1. Such a bilateral noise is practical in real-world graph data . Nonetheless, previous works that only consider unilateral noise in input space or label space cannot effectively deal with such a coupled scenario. This raises a new challenge of tackling the bilateral edge noise for robust link prediction.

In this paper, we first disclose that the bilateral edge noise leads to the severe edge representation collapse behind the performance degradation (Sec. 3.1). Note the collapsed representation is reflected by much lower alignment and poorer uniformity (Sec. 3.2), which are used for quantitive and qualitative analysis of edge noise from representation perspective . To solve this, we propose an information-theory-guided principle named Robust Graph Information Bottleneck (RGIB) (Sec. 4.1).

Conceptually, the RGIB principle is with new learning objectives that decouple the mutual information (MI) among noisy inputs, noisy labels, and representation. As illustrated in Fig. 2, RGIB generalizes the basic GIB  to learn a robust representation that is resistant to the bilateral edge noise. Technically, we provide two instantiations of RGIB based on different methodologies: (1) the RGIB-SSL utilizes contrastive pairs with automatically augmented views to form the informative regularization in a self-supervised learning manner, which benefits from its intrinsic robustness to label noise (Sec. 4.2); (2) the RGIB-REP explicitly purifies the graph topology and supervision targets with the reparameterization mechanism, which enables further modeling of the mutual patterns of noise from both input and label spaces (Sec. 4.3). Both instantiations are equipped with adaptive designs, aiming to effectively estimate and balance the informative terms in a tractable manner, _i.e._, the hybrid augmentation algorithm and self-adversarial alignment loss for RGIB-SSL; the relaxed information constraints on topology space and label space for RGIB-REP. Empirically, the two instantiations of RGIB work effectively under extensive noisy scenarios and can be seamlessly integrated with various GNNs (Sec. 5). Our contributions are summarized as follows.

* To our best knowledge, we are the first to study the robustness problem of link prediction under bilateral edge noise. We reveal that the bilateral noise can bring severe representation collapse and performance degradation, and such negative impacts are general to common datasets and GNNs.
* We propose a general learning framework, RGIB, with new representation learning objectives to promote GNNs' robustness. Two instantiations, RGIB-SSL and RGIB-REP, are proposed upon different methodologies that are equipped with adaptive designs and extensive theoretical analysis.
* Without modifying the GNN architectures, the RGIB achieves state-of-the-art results on \(3\) GNNs and \(6\) datasets under various noisy scenarios, obtaining up to \(12.9\%\) promotion in AUC. The distribution of learned representations is notably recovered and more robust to the bilateral noise.

## 2 Preliminaries

**Notations.** We denote \(\!=\!\{v_{i}\}_{i=1}^{N}\) as the set of nodes and \(\!=\!\{e_{ij}\}_{ij=1}^{M}\) as the set of edges. With adjacent matrix \(A\) and node features \(X\), a graph is denoted as \(\!=\!(A,X)\), where \(A_{ij}\!=\!1\) means there is an edge \(e_{ij}\) between \(v_{i}\) and \(v_{j}\). \(X_{[i,:]}\!\!^{D}\) is the \(D\)-dimension node feature of \(v_{i}\). The link prediction task is to indicate the existence of query edges that are not observed in \(A\). The binary labels of these query edges are denoted as \(Y\), where \(0\) for negative edges and \(1\) for positive edges. The \(I(;)\) indicates the mutual information between two variables. Detailed notations are in Tab. 9.

Figure 1: Link prediction with bilateral edge noise. The GNN takes the graph \(\!=\!(,X)\) as inputs, generates the edge representation \(\), and then predicts the existence of unseen edges with labels \(\).

**GNNs for Link Prediction.** Here, we follow the encode-decode framework (Fig. 1) for link prediction , where the GNN architecture can be GCN , GAT , or SAGE . Given a \(L\)-layer GNN \(f_{}()\) with learnable weights \(\), the node representation \(\!\!^{|| D}\) for each node \(v_{i}\!\!\) are obtained by a \(L\)-layer message propagation as the encoding process. For decoding, the logits \(_{e_{ij}}\) of each query edge \(e_{ij}\) are computed by projecting the edge representation \(_{ij}\!=\!_{i}_{j}\) to probability. Note that the representation of all query edges is denoted as \(\) and namely \(f_{}(A,X)\!=\!\). Finally, the optimization objective is to minimize the binary classification loss with logits \(_{e_{ij}}\) and labels \(Y_{ij}\).

**Topological denoising approaches.** A natural way to tackle the input edge noise is to directly clean the noisy graph. Sampling-based methods, such as DropEdge , NeuralSparse , and PTDNet , are proposed to remove the task-irrelevant edges. Besides, as GNNs can be easily fooled by adversarial networks with only a few perturbed edges , defending methods GCN-jaccard  and GIB  are designed for pruning adversarial edges from the poisoned graphs.

**Label-noise-resistant techniques.** To tackle the general problem of noisy labels, sample-selection-based methods such as Co-teaching  let two neural networks teach each other with small-loss samples based on the memorization effect . Besides, peer loss function  pairs independent peer examples for supervision and works within the standard empirical risk minimization framework without noise priors. For tackling label noise on graphs, label propagation techniques  are designed to propagate the reliable signals from clean nodes to noisy ones, which are nonetheless entangled with the node annotations and node classification task that cannot be directly applied here.

## 3 An Empirical Study of Bilateral Edge Noise

In this section, we attempt to figure out how GNNs behave when learning with the edge noise and what are the latent mechanisms behind it. We first present an empirical study in Sec. 3.1, and then investigate the negative impact of noise through the lens of representation distribution in Sec. 3.2.

### How GNNs perform under bilateral edge noise?

To quantitatively study the impact of edge noise, we simulate different levels of perturbations properly on a range of GNNs benchmark datasets, as elaborated in Def. 3.1. Note the data split manner adopt by most relevant works  randomly divides partial edges as observations and the others as prediction targets. Hence, the noisy edges will be distributed to input \(\) and labels \(\). With the bilateral noise defined in Def. 3.1, we then conduct a comprehensive empirical study. A further discussion on the edge noise is in Appendix B.1, and full evaluations can be found in Appendix D.1.

**Definition 3.1** (Bilateral edge noise).: _Given a clean training data, i.e., observed graph \(=(A,X)\) and labels \(Y\{0,1\}\) of query edges, the noisy adjacacence \(\) is generated by directly adding edge noise to the original adjacent matrix \(A\) while keeping the node features \(X\) unchanged. The noisy labels \(\) are similarly generated by adding edge noise to the labels \(Y\). Specifically, given a noise ratio \(_{a}\), the noisy edges \(A^{}\) (\(=A+A^{}\)) are generated by flipping the zero element in \(A\) as one with the probability \(_{a}\). It satisfies that \(A^{} A=\) and \(_{a}=()|-|(A )|}}{{|(A)|}}\). Similarly, noisy labels are generated and added to the original labels, where \(_{y}=()|-|(Y )|}}{{|(Y)|}}\)._

**Observation 3.1**.: As illustrated in Fig. 3, _the bilateral edge noise causes a significant drop in performance, and a larger noise ratio generally leads to greater degradation._ It means that the standardly trained GNNs are vulnerable to the bilateral edge noise, yielding a severe robustness problem. As will be shown in Sec. 5, the _performance drop brought by the bilateral noise is much greater than that of the unilateral input noise or label noise._ However, none of the existing methods can effectively tackle such bilateral noise, since they only consider unilateral noise either in input space or label space. Thus, it is necessary to devise a robust method accordingly. To this end, we further investigate the underneath noise effect as follows.

Figure 3: Link prediction performance in AUC with the bilateral edge noise. The bilateral noise ratio \(\!=\!40\%\) means \(_{a}\!=\!_{y}\!=\!40\%\) with both noisy \(\) and \(\).

### Understanding the noise impact via inspecting the representation distribution

Recall that the edge representation is extracted by a forward pass as \(\!=\!f_{}(,X)\) and the model is optimized by stochastic gradient descent as \(\!:=\!\!-\!_{}(,)\). When encountering noise within \(\) and \(\), the edge representation \(\) can be directly influenced, since the training neglects the adverse effects of data corruption. Besides, the GNN readouts the edge logit \(_{e_{ij}}\) based on node representations \(_{i}\) and \(_{j}\), which are possibly collapsed under the bilateral edge noise [14; 28].

To analyze the edge noise from the perspective of representation \(\), we use two recent concepts  in representation learning: (1) _alignment_ quantifies the stability of GNN when encountering edge noise in the testing phase. It is computed as the distance of representations between two randomly augmented graphs. A higher alignment means being more resistant to input perturbations; (2) _uniformity_ qualitatively measures the denoising effects of GNN when learning with edge noise in the training phase. Overall, a greater uniformity implies that the representation of query edges are more uniformly distributed on the unit hypersphere, preserving more information about the original data.

**Observation 3.2**.: As can be seen from Tab. 1 and Fig. 4, _a poorer alignment and a worse uniformity are brought by a severer edge noise._ As the noise ratio \(\) increases, the learned GNN \(f_{}()\) is more sensitive to input perturbations as the alignment values are sharply increased, and the learned edge representations tend to be less uniformly distributed and gradually collapse into individual subregions. That is, the learned edge representation \(\) is _severely collapsed_ under the bilateral edge noise, which is strongly correlated with the observation in Sec. 3.1 and reflects an undesirable property of GNNs.

## 4 Robust Graph Information Bottleneck

Without prior knowledge like the noise ratio, without the assistance of auxiliary datasets, and even without modifying GNN architectures, how can representations be resistant to the bilateral noise? Here, we formally build a method to address the above problems from the perspective of robust graph information bottleneck (Sec. 4.1) and design its two practical instantiations (Sec. 4.2 and Sec. 4.3).

### The principle of RGIB

Recall in Sec. 3.2, the edge representation \(\) is degraded. To robustify \(\), one can naturally utilize the information constraint based on the graph information bottleneck (GIB) [39; 46], _i.e._, solving

\[-I(;),\ \ I(;)<,\] (1)

where the hyper-parameter \(\) constrains the MI \(I(;)\) to avoid \(\) from capturing excess task-irrelevant information from \(\). The basic GIB can effectively defend the input perturbation . However, it is intrinsically vulnerable to label noise since it entirely preserves the label supervision and maximizes the noisy supervision \(I(;)\). Our empirical results in Sec. 5 show that optimization only with Eq. 1 is ineffective when learning with the bilateral edge noise defined in Def. 3.1.

   dataset & Cora & Citeseer \\  clean &.616 &.445 \\ \(\!=\!20\%\) &.687 &.586 \\ \(\!=\!40\%\) &.695 &.689 \\ \(\!=\!60\%\) &.732 &.696 \\   

Table 1: Mean values of alignment, which are calculated as the L2 distance of representations of two randomly perturbed graphs \(_{1}^{i},_{2}^{i}\), _i.e._, \(=_{i=1}^{N}||_{1}^{i}-_{2}^{i}||_{2}\). Representation \(_{1}^{i}=f_{}(_{1}^{i},X)\) and \(_{2}^{i}=f_{}(_{2}^{i},X)\).

Figure 4: Uniformity distribution on Cora dataset. Representations of query edges in the test set are mapped to unit circle of \(^{2}\) with normalization followed by the Gaussian kernel density estimation as . Both positive and negative edges are expected to be uniformly distributed.

**Deriving the RGIB principle.** The basic GIB of Eq. 1 decreases \(I(;|)\) by directly constraining \(I(;)\) to handle the input noise. Symmetrically, the label noise can be hidden in \(I(;|)\), but trivially constraining \(I(;)\) to regularize \(I(;|)\) is not ideal, since it conflicts with Eq. 1 and also cannot tackle the noise within \(I(;)\). Thus, it is crucial to further decouple the dependence among \(\), \(\), and \(\), while the noise can exist in areas of \(I(;|)\), \(I(;|)\), and \(I(;|)\). Analytically, \(I(;|)\!=\!I(;)\!+\!I(;|)\!+\!I(;|)\!-\!H()\!+\!H(| {A},)\), where \(I(;)\) is a constant and redundancy \(H(|,)\) can be minimized. Thus, the \(I(;|)\) can be approximated by \(H()\), \(I(;|)\) and \(I(;|)\). Since the two later terms are also noisy, a balance of these three informative terms can be a solution to the problem of bilateral edge noise.

**Definition 4.1** (Robust Graph Information Bottleneck).: _Based on the above analysis, we propose a new learning objective to balance informative signals regarding \(\), as illustrated in Fig. 5(a), i.e.,_

\[-I(;),\ \ \ _{H}^{-}<H()< _{H}^{+},I(;|)<_{Y},\ I(;| )<_{A}.\] (2)

_Specifically, constraints on \(H()\) encourage a diverse \(\) to prevent representation collapse (\(>_{H}^{-}\)) and also limit its capacity (\(<_{H}^{+}\)) to avoid over-fitting. Another two MI terms, \(I(;|)\) and \(I(;|)\), mutually regularize posteriors to mitigate the negative impact of bilateral noise on \(\). The complete derivation of RGIB and a further comparison of RGIB and GIB are in Appendix B.2._

Note that MI terms, _e.g._, \(I(;|)\) are usually intractable. Therefore, we introduce two practical implementations of RGIB, _i.e._, RGIB-SSL and RGIB-REP, based on different methodologies. RGIB-SSL _explicitly_ optimizes the representation \(\) with the self-supervised regularization, while RGIB-REP _implicitly_ optimizes \(\) by purifying the noisy \(\) and \(\) with the reparameterization mechanism.

### Instantiating RGIB with self-supervised learning

Recall that the edge representation has deteriorated with the _supervised_ learning paradigm. Naturally, we modify it into a _self-supervised_ counterpart by explicitly regularizing the representation \(\) (see Fig. 5(b)) to avoid collapse and to implicitly capture reliable relations among noisy edges as

\[-(I(_{1};) \!+\!I(_{2};))}_{}-(H( _{1})\!+\!H(_{2}))}_{}-I( _{1};_{2})}_{},\] (3)

where margins \(_{s},_{u},_{a}\) balance one supervised and two self-supervised regularization terms. When \(_{s} 1,_{u} 0,\) the RGIB-SSL can be degenerated to the basic GIB. Note \(_{1}\) and \(_{2}\) are the representations of two augmented views \(_{1}\) and \(_{2}\), namely, \(_{1}\!=\!f_{}(_{1},X)\) and \(_{2}\!=\!f_{}(_{2},X)\).

**Instantiation.** To achieve a tractable approximation of the MI terms in Eq. 2, we adopt the contrastive learning technique , and contrast pair of samples, _i.e._, perturbed \(_{1},_{2}\) that are sampled from the augmentation distribution \(()\). Based on which we approximate the expected supervision term by \([I(;|)][I(;)] \!=\!_{_{s}\!\!()}[I(_{s}; )]\!\!}{{2}}(_{cls}(_{1};)\!+\!_{cls}(_{2};))\). Note the approximation also supports multiple samples. Similarly, we approximate the entropy term \([H()]\) by \(}{{2}}(H(_{1})\!+\!H(_{2}))\), where a higher uniformity leads to a high entropy \(H()\) as proved in Prop. 4.2. Lastly, the \(I(;|)\) is estimated by the alignment term \(I(_{1};_{2})\) (refer to Prop. 4.3).

Figure 5: Digrams of the RGIB principle (a) and its two instantiations (b,c). RGIB-SSL utilizes the automatically augmented views \(_{1},_{2}\) in a contrastive manner to be resistant to input noise. RGIB-SSL is intrinsically robust to label noise due to its self-supervised nature. Besides, RGIB-REP explicitly purifies the input graph’s topology and target labels with the jointly reparameterized \(_{A}\) and \(_{Y}\). It enables to modeling the mutual patterns of edge noise from both input and label spaces.

**Proposition 4.2**.: _A higher information entropy \(H()\) of edge representation \(\) indicates a higher uniformity  of the representation's distribution on the unit hypersphere. Proof. See Appendix A.3._

**Proposition 4.3**.: _A lower alignment \(I(_{1};_{2})\) indicates a lower \(I(;|)\). Since \(I(;|) I(;)}{{2}} I(_{1};_{2})+I(_{1};_{2})=}{{2}}I(_{1};_{2})+c\), a constrained alignment estimated by \(I(_{1};_{2})\) can bound a lower \(I(;|)\) and \(I(;)\). Proof. See Appendix A.4._

However, directly applying existing contrastive methods like  can be easily suboptimal, since they are not originally designed for graph data and neglect the internal correlation between topology \(\) and target \(\). Here, we propose the following two designs to further improve robust learning.

**Hybrid graph augmentation.** To encourage more diverse views with lower \(I(A_{1};A_{2})\) and to avoid manual selection of augmentation operations, we propose a hybrid augmentation method with four augmentation operations as predefined candidates and ranges of their corresponding hyper-parameters. In each training iteration, two augmentation operators and their hyper-parameters are _automatically_ sampled from the search space. Then, two augmented graphs are obtained by applying the two operators on the original graph adjacency \(A\). The detailed algorithm is elaborated in Appendix C.

**Self-adversarial loss terms.** With edge representations \(_{1}\) and \(_{2}\) from two augmented views, we build the alignment objective by minimizing the representation similarity of the positive pairs \((_{ij}^{1},_{ij}^{2})\) and maximizing that of the randomly sampled negative pairs \((_{ij}^{1},_{mn}^{2})\), \(e_{ij}\!\!e_{mn}\). The proposed self-adversarial alignment loss is \(_{align}=_{i=1}^{N}_{i}^{pos}+_{i}^{neg}\). 2 Importantly, softmax functions \(p^{pos}()\) and \(p^{neg}()\) aim to mitigate the inefficiency problem  that aligned pairs are not informative. Besides, the uniformity loss is \(_{unif}\!=\!_{ij,mn}^{K}e^{-_{ij}^{1}-_ {mn}^{1}_{2}^{2}}+e^{-_{ij}^{2}-_{mn}^{2} _{2}^{2}}\) with the Gaussian potential kernel, where edges \(e_{ij}\) and \(e_{mn}\) are respectively sampled from \(^{pos}\) and \(^{neg}\).

**Optimization.** Regarding Eq. 3, the objective of RGIB-SSL is \(=_{s}_{cls}+_{q}_{align}+ _{u}_{unif}\).

**Remark 4.1**.: _The collapsed representation comes from trivially minimizing the noisy supervision \(I(;)\). The alignment and uniformity terms in Eq. 3 can alleviate such noise effects (see Sec. 5)._

### Instantiating RGIB with data reparameterization

Another realization is by reparameterizing the graph data on both topology space and label space jointly to preserve clean information and discard noise (as Fig. 5(c)). We propose RGIB-REP that explicitly models the reliability of \(\) and \(\) via latent variables \(\) to learn a noise-resistant \(\), _i.e._,

\[\ \ -\ I(;_{Y})}_{ }\ +\ I(_{A};)}_{}\ +\ I(_{Y};)}_{},\] (4)

where latent variables \(_{Y}\) and \(_{A}\) are clean signals extracted from noisy \(\) and \(\). Their complementary parts \(_{Y^{}}\) and \(_{A^{}}\) are considered as noise, satisfying \(=_{Y}+_{Y^{}}\) and \(=_{A}+_{A^{}}\). When \(_{Y}\) and \(_{A}\), the RGIB-REP can be degenerated to the basic GIB. Here, the \(I(;_{Y})\) measures the supervised signals with selected samples \(_{Y}\), where the classifier takes \(_{A}\) (_i.e._, a subgraph of \(\)) as input instead of the original \(\), _i.e._, \(\!=\!f_{}(_{A},X)\). Constraints \(I(_{A};)\) and \(I(_{Y};)\) aim to select the cleanest and most task-relevant information from \(\) and \(\).

**Instantiation.** For deriving a tractable objective regarding \(_{A}\) and \(_{Y}\), a parameterized sampler \(f_{}()\) sharing the same architecture and weights as \(f_{}()\) is adopted here. \(f_{}()\) generates the probabilistic distribution of edges that include both \(\) and \(\) by \(=(_{}_{}^{})(0,1)^{||||}\), where representation \(_{}\!=\!f_{}(,X)\). Bernoulli sampling is then used to obtain high-confidence edges, _i.e._, \(_{A}\!=\!(|)\) and \(_{Y}\!=\!(|)\), where \(|_{A}|\!\!||\) and \(|_{Y}|\!\!||\).

**Proposition 4.4**.: _Given the edge number \(n\) of \(\), the marginal distribution of \(_{A}\) is \((_{A})=(n)_{A_{i} 1}^{n}_{ij}\). \(_{A}\) satisfies \(I(_{A};)[(_{}(_{A}|A) ||(_{A}))]=_{e_{ij}}_{ij}}{}+(1-_{ij})}{1-}=_{A}\), where \(\) is a constant. The topology constraint \(I(_{A};)\) in Eq. 4 is bounded by \(_{A}\), and the label constraint is similarly bounded by \(_{Y}\). Proof. See Appendix A.5._

**Proposition 4.5**.: _The supervision term \(I(;_{Y})\) in Eq. 4 can be empirically reduced to the classification loss, i.e., \(I(;_{Y})_{_{Y},_{A}}[_{}(_{Y}|_{A})]-_{cls}(f_{}(_{A}),_{Y})\), where \(_{cls}\) is the standard cross-entropy loss. Proof. See Appendix A.6._

**Optimization.** A relaxation is then conducted on the three MI terms in Eq. 4. With derived bounds in Prop. 4.4 (_i.e._, regularization \(_{A}\) and \(_{Y}\)) and Prop. 4.5 (_i.e._, \(_{cls}\)), the final optimization objective is formed as \(=_{s}_{cls}+_{A}_{A}+_{ Y}_{Y}\), and the corresponding analysis Thm. 4.6 is as follows.

**Theorem 4.6**.: _Assume the noisy training data \(D_{train}=(,X,)\) contains a potentially clean subset \(D_{sub}\!=\!(_{A}^{*},X,_{Y}^{*})\). The \(_{Y}^{*}\) and \(_{A}^{*}\) are the optimal solutions of Eq. 4 that \(_{Y}^{*}\!\!Y\), based on which a trained GNN predictor \(f_{}()\) satisfies \(f_{}(_{A}^{*},X)=_{Y}^{*}+\). The random error \(\) is independent of \(D_{sub}\) and \(\!\!0\). Then, for arbitrary \(_{s},_{A},_{Y}\), \(_{A}\!=\!_{A}^{*}\) and \(_{Y}\!=\!_{Y}^{*}\) minimizes the RGIB-REP of Eq. 4. Proof. See Appendix A.7._

**Remark 4.2**.: _Note that the methodologies take by RGIB-SSL and RGIB-REP, i.e., self-supervised learning and data reparameterization are not considered in the original GIB . In Sec. 5, we justify that they are effective in instantiating the RGIB to handle the bilateral edge noise. A detailed comparison of the two instantiations is conducted in Appendix B.3. More importantly, it is possible that new instantiations based on other methodologies are inspired by the general RGIB principle._

## 5 Experiments

**Setup.**\(6\) popular datasets and \(3\) types of GNNs are taken in the experiments. The edge noise is generated based on Def. 3.1 after the commonly used data split where 85% edges are randomly selected for training, 5% as the validation set, and 10% for testing. The AUC is used as the evaluation metric as in [48; 55]. The software framework is the Pytorch , while the hardware is one NVIDIA RTX 3090 GPU. We repeat all experiments five times, and full results can be found in Appendix E.

**Baselines.** As existing robust methods separately deal with input noise or label noise, both kinds of methods can be considered as baselines. For tackling _input noise_, three sampling-based approaches are used for comparison, _i.e._, DropEdge , NeuralSparse , and PTDNet . Besides, we also include Jaccard , GIB , VIB , and PRI , which are designed for pruning adversarial edges. Two generic methods are selected for _label noise_, _i.e._, Co-teaching  and Peer loss . Besides, two contrastive learning methods are taken into comparison, _i.e._, SupCon  and GRACE . The implementation details of the above baselines are summarized in Appendix C.

### Main results

**Performance comparison.** As shown in Tab. 2, the RGIB achieves the best results in all \(6\) datasets under the bilateral edge noise with various noise ratios, especially on challenging datasets, _e.g._, Cora and Citeseer, where a \(12.9\%\) AUC promotion can be gained compared with the second-best methods. As for the unilateral noise settings shown in Tab. 3, RGIB still consistently surpasses all the baselines ad hoc for unilateral input noise or label noise by a large margin. We show that the bilateral edge noise can be formulated and solved by a unified learning framework, while the previous denoising methods can only work for one specific noise pattern, _e.g._, the unilateral input noise or label noise.

    &  &  &  &  &  &  \\  & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\  Standard & 8111 & 7419 & 6.6970 & 7.7864 & 7.3800 & 7085 & 8.870 & 8.7484 & 6.9429 & 9.9520 & 9438 & 9.616 & 9.9496 & 9.9274 & 9.9432 & 9.9406 & 9.3886 \\ DropEdge & 8017 & 7423 & 7.3030 & 7.635 & 7.3939 & 7.0941 & 8.8114 & 8.8482 & 8.3534 & 9.8811 & 9.6982 & 9.473 & 9.5689 & 9.5484 & 9.407 & 9.4439 & 9.3777 & 9.365 \\ NeuralSparse & 8190 & 7.318 & 7.2933 & 7.765 & 7.3977 & 7.1488 & 8.9088 & 8.7333 & 86.3092 & 9.9638 & 9.5959 & 9.497 & 9.4920 & 9.494 & 9.309 & 9.297 \\ PTDNet & 8047 & 7.559 & 7.3883 & 7.7952 & 7.4238 & 7.8827 & 8.8723 & 8.6925 & 9.964 & 9.4845 & 9.607 & 9.514 & 9.4242 & 9.8485 & 9.3926 & 9.304 \\ Co-teaching & 8197 & 7.4700 & 7.7030 & 7.3233 & 7.7231 & 7.3191 & 8.9487 & 8.6076 & 8.6838 & 9.9026 & 9.5264 & 9.4809 & 9.5956 & 9.5483 & 9.461 & 9.352 & 9.374 \\ Peer loss & 8185 & 7.468 & 7.0108 & 7.4233 & 7.345 & 7.1041 & 8.9681 & 8.8155 & 8.5666 & 9.807 & 9.5364 & 9.3403 & 9.5433 & 9.3267 & 9.9457 & 9.9345 & 9.286 \\ Jaccard & 8143 & 7.4989 & 7.0204 & 7.4732 & 7.3724 & 7.1070 & 8.8728 & 8.8003 & 8.512 & 9.7974 & 9.5972 & 9.4805 & 9.5383 & 9.3444 & 9.3327 & 9.244 \\ GIB & 8198 & 7.485 & 7.1488 & 7.5090 & 7.3182 & 8.9872 & 8.5844 & 9.7939 & 9.6084 & 9.4174 & 9.5954 & 9.561 & 9.3241 & 9.472 & 9.329 & 9.302 \\ VIB & 8208 & 7.810 & 7.218 & 7.001 & 8.120 & 7.185 & 8.927 & 8.8255 & 8.501 & 9.697 & 9.637 & 9.5500 & 9.529 & 9.561 & 9.487 & 9.313 & 9.3999 & 9.288 \\ PRI & 7.976 & 7.3300 & 6.991 & 7.5654 & 7.4291 & 8.898 & 8.8801 & 8.487 & 9.6061 & 9.519 & 9.507 & 9.5134 & 9.499 & 9.490 & 9.382 & 9.413 & 9.301 \\ SupCon & 2.840 & 7.1940 & 7.1940 & 7.5545 & 7.4298 & 8.8537 & 8.8525 & 8.5958 & 9.508 & 9.5089 & 9.297 & 9.551 & 9.531 & 9.467 & 9.349 & 9.394 & 9.391 \\ GRACE & 7.872 & 6.940 & 6.929 & 7.632 & 7.242 & 6.8444 & 8.922 & 8.749 & 8.588 & 8.8899 & 8.8865 & 8.315 & 8.8978 & 8.8949 & 9.394 & 9.380 & 9.363 \\
**RGIB-REP** & 8.313 & 7.966 & 7.5917

**Remark 5.1**.: _The two instantiations of RGIB can be generalized to different scenarios with their own priority according to the intrinsic graph properties. Basically, the RGIB-SSL is more adaptive to sparser graphs, e.g., Cora and Citeseer, where the edge noise results in greater performance degradation. The RGIB-REP can be more suitable for denser graphs, e.g., Facebook and Chameleon._

**Combating with adversarial attacks.** Adversarial attacks on graphs can be generally divided into poisoning attacks that perturb the graph in training time and evasion attacks that perturb the graph in testing time. Here, we conduct the poisoning attacks based on Netrack  that only perturbs the graph structure. Notes that Netrack generates perturbations by modifying graph structure or node attributes such that perturbations maximally destroy downstream GNN's predictions. Here, we apply Netrack on Cora and Citeseer datasets as representatives. As shown in Tab. 4, the adversarial attack that adds noisy edges to the input graph also significantly degenerates the GNN's performance. And comparably, the brought damage is more severe than randomly added edges . Crucially, we empirically justify that RGIB-SSL and RGIB-REP can also promote the robustness of GNN against adversarial attacks on the graph structure, remaining the adversarial robustness of the GIB.

**The distribution of learned representation.** We justify that the proposed methods can effectively alleviate the representation collapse. Compared with the standard training, both RGIB-REP and RGIB-SSL bring significant improvements to the alignment with much lower values, as in Tab. 5. At the same time, the uniformity of learned representation is also enhanced: it can be seen from Fig. 6 that the various query edges tend to be more uniformly distributed on the unit circle, especially for the negative edges. It shows that the distribution of edge representation is effectively recovered. Besides, as RGIB-SSL explicitly constrains the representation, its recovery power on representation distribution is naturally stronger than RGIB-REP, resulting in much better alignment and uniformity.

**Learning with clean data.** Here, we study how the robust methods behave when learning with clean data, _i.e._, no edge noise exists. As shown in Tab. 7, the proposed two instantiations of RGIB can also boost the performance when learning on clean graphs, and outperforms other baselines in most cases.

     } &  &  &  &  &  &  \\  & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\  Standard & 8027 & 7856 & 7490 & 8045 & 7708 & 7583 & 8854 & 8759 & 8651 & 9819 & 9668 & 9622 & 9608 & 9433 & 9368 & 9416 & 9395 & 9411 \\ DropEdge & 8338 & 7286 & 7454 & 8025 & 7730 & 7473 & 8682 & 8456 & 8376 & 9803 & 9685 & 9531 & 9674 & 9433 & 9432 & 9426 & 9376 & 9358 \\ NeuralSparse & 8534 & 7794 & 7637 & 8093 & 7809 & 7468 & 8931 & 8720 & 8649 & 9712 & 9691 & 9583 & 9690 & 9540 & 9348 & 9469 & 9403 & 9417 \\ PTDNet & 8433 & 7247 & 7701 & 819 & 7811 & 7683 & 8903 & 8767 & 8609 & 9725 & 9668 & 9493 & 9610 & 9457 & 9369 & 9469 & 9409 & 9379 \\ Jaccard & 8200 & 7838 & 7617 & 8186 & 7776 & 7725 & 8974 & 8676 & 8639 & 8947 & 9702 & 9638 & 9507 & 9436 & 9388 & 9345 & 9240 \\ GIB & 8002 & 8099 & 7741 & 8070 & 7717 & 7779 & 8932 & 8808 & 8618 & 9796 & 9647 & 9650 & 9605 & 9521 & 9416 & 9390 & 9406 & 9397 \\ VIB & 8603 & 8990 & 8008 & 8497 & 8399 & 7703 & 8910 & 8829 & 8519 & 9800 & 9710 & 9536 & 9499 & 9558 & 9312 & 9146 & 9409 & 9297 \\ PRI & 8307 & 7590 & 7736 & 8158 & 7765 & 8881 & 8790 & 8551 & 9961 & 9698 & 9529 & 9581 & 90145 & 9345 & 9397 & 9419 & 9318 \\ SupCon & 8349 & 8301 & 8025 & 8076 & 7767 & 7655 & 8867 & 8739 & 8558 & 9647 & 9517 & 9400 & 9606 & 9536 & 9468 & 9372 & 9343 & 9305 \\ GRAC & 7877 & 7107 & 6975 & 7615 & 7151 & 6830 & 8810 & 8795 & 8593 & 9051 & 8833 & 8395 & 8994 & 9007 & 8864 & 9392 & 9378 & 9363 \\
**RGIB-REP** & 8624 & 8313 & 8158 & 8293 & 7996 & 7771 & 80082 & 8825 & 8687 & **9831** & **9723** & **9682** & **9756** & **964** & **930** & **9459** & **9432** & **9442** & 9405 \\
**RGB-SSL** & **9024** & 8577 & **8421** & **8747** & **8461** & **8245** & **9126** & **8889** & **8693** & **9821** & 9707 & 9668 & 9658 & 9570 & **9486** & 9479 & **9429** & **9429** \\   &  &  &  &  &  &  \\  & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\  Standard & 8281 & 804 & 8060 & 7965 & 7850 & 7850 & 7659 & 9030 & 90390 & 9070 & 9882 & 9880 & 9886 & 9686 & 9580 & 9362 & 9720 & 9710 \\ Co-teaching & 8446 & 8209 & 8157 & 8197 & 7948 & 7791 & 9315 & 9219 & 9319 & 9317 & 9297 & 9797 & 9797 & 9683 & 9624 & 9650 & 9533 & 9675 & 9644 & 9655 \\ Peer loss & 8325 & 8036 & 8069 & 7991 & 7990 & 7751 & 9126 & 9101 & 9210 & 9769 & 9750 & 9734 & 9621 & 9501 & 9569 & 9636 & 9694 & 9696 \\ Jaccard & 8289 & 8064 & 8148 & 8061 & 7887 & 7689 & 9098 & 9115 & 9096 & 9702 & 9725 & 9758 & 9603 & 9659 & 9557 & 9529 & 9521 & 9501 \\ GIB & 8337 & 8317 & 8157 & 7986 & 7868 & 7693 & 9037 & 9114 & 9064 & 9749 & 9703 & 9711 & 9582 & 9849 & 9641 & 9628 & 9601 \\ VIB & 8406 & 8296 & 8036 & 8068 & 8034 & 7739 & 9088 & 9042 & 8981 & 9705 & 970 & 9713 & 9610 & 9629 & 9518 & 9586 & 9722 & 9690 \\ PRI & 8298 & 8139 & 7960 & 7902 & 7881 & 7725 & 8939 & 8917 & 8822 & 9733 & 9802 & 9620 & 9441 & 9593 & 9599

**Optimization schedulers.** To reduce the search cost of coefficients in the objectives of RGIB-SSL and RGIB-REP, we set a unified optimization framework that can be formed as \(=_{cls}+(1-)_{1}+(1-)_{2}\). Here, we attempt \(5\) different schedulers to tune the only hyper-parameter \(\!\!\), including (1) constant, \( c\); (2) linear, \(_{t}=k t\), where \(t\) is the normalized time step; (3) sine, \(_{t}=(t}{{2}})\); (4) cosine, \(_{t}=(t}{{2}})\); and (5) exponential, \(_{t}=e^{k t}\). As empirical results summarized in Tab. 6, the selection of optimization schedulers greatly affects the final results. Although there is no golden scheduler to always perform the best, the _constant_ and _sine_ schedulers are generally better than others among the \(5\) above candidates. A further hyper-parameter study with grid search of the multiple \(\) in Eq. 3 of RGIB-SSL and Eq. 4 of RGIB-REP are illustrated in Fig. 7.

### Ablation study

**Hybrid augmentation.** As shown in Tab. 8, RGIB-SSL benefits from the hybrid augmentation algorithm that automatically generates graphs of high diversity for contrastive learning. _e.g._, compared with the fixed augmentation, the hybrid manner brings a \(3.0\%\) average AUC promotion on Cora.

**Self-adversarial alignment loss.** Randomly-sampled pairs with hierarchical information to be contrasted and learned from. The proposed re-weighting technique further enhances high-quality pairs and decreases low-quality counterparts. It refines the signal and brings up to \(2.1\%\) promotion.

   dataset & Cora & Citeseer & Pubmed \\ method & SSL & REP & SSL & REP \\  \(constant\) & 8398. **7927** & **8227. 7742** & 8596. **8416** \\ \(linear()\) & 8427. 7653 & 8167. 7559 & **8645**. 8239 \\ \(sin()\) & **8436** & **7924** & 8132. 7680 & 8637. 8275 \\ \(cos()\) & 8334. 7833 & 8088. 7647 & 8579. 8372 \\ \(exp()\) & 8381. 7815 & 8085. 7569 & 8617. 8177 \\   

Table 6: Comparison on different schedulers. SSL/REP are short for RGIB-SSL/RGIB-REP. Experiments are performed with a 4-layer GAT and \(\!=\!40\%\) mixed edge noise.

   method & Cora & Citeseer & Pubmed \\  Standard &.8686 &.8317 &.9178 &.9870 &.9788 & **.9725** \\ DropEdge &.8684 &.8344 &.9344 &.9869 &.9700 &.9629 \\ NeuralSparse &.8715 &.8405 &.9366 &.9865 & **.9803** &.9635 \\ PTDNet &.8577 &.8398 &.9315 &.9868 &.9696 &.9640 \\ Co-teaching &.8684 &.8387 &.9192 &.9771 &.9698 &.9626 \\ Peer loss &.8313 &.7742 &.9085 &.8951 &.9374 &.9422 \\ Jaccard &.8413 &.8005 &.8831 &.9792 &.9703 &.9610 \\ GIB &.8582 &.8327 &.9019 &.9691 &.9628 &.9635 \\ SupCon &.8529 &.8003 &.9131 &.9692 &.9717 &.9619 \\ GFACE &.8329 &.8236 &.9358 &.8953 &.8999 &.9165 \\
**RGIB-REP** &.8758 &.8415 &.9408 & **.9875** &.9792 &.9680 \\
**RGIB-SSL** & **.9260** & **.9148** & **.9593** &.9845 &.9740 &.9646 \\   

Table 7: Method comparison with a 4-layer GCN trained on the clean data.

Figure 6: Uniformity distribution on Citeseer with \(\!=\!40\%\).

   dataset & Cora & Citeseer \\ method & std. REP SSL & std. REP SSL \\  clean &.616,524 **. 475** &.445,439 **. 418** \\ \(\!=\!20\%\) &.687,642 **. 543** &.586,533 **. 505** \\ \(\!=\!40\%\) &.695,679 **. 578** &.689,623 **. 533** \\ \(\!=\!60\%\) &.732,704 **. 615** &.696,647 **.542** \\   

Table 5: Comparison of alignment. Here, std. is short for _standard training_, and SSL/REP is short for RGIB-SSL/RGIB-REP, respectively.

Figure 7: Grid search of hyper-parameter with RGIB-SSL (left) and RGIB-REP (right) on Cora dataset with bilateral noise \(=40\%\). As can be seen, neither too large nor too small value can bring a good solution.

**Information constraints.** Label supervision contributes the most among the three informative terms, even though it contains label noise. As can be seen from Tab. 8, degenerating RGIB-SSL to a pure self-supervised manner without supervision (_i.e._, \(_{s}\!=\!0\)) leads to an average \(11.9\%\) drop in AUC.

**Edge/label selection.** The two selection methods adopted in RGIB-REP are nearly equally important. Wherein, the edge selection is more important for tackling the input noise, as a greater drop will come when removed; while the label selection plays a dominant role in handling the label noise.

**Topological / label constraint.** Tab. 8 also shows that the selection mechanisms should be regularized by the related constraints, otherwise sub-optimal cases with degenerated effects of denoising will be achieved. Besides, the topological constraint is comparably more sensitive than the label constraint.

**Learning curves.** We draw the learning curves with constant schedulers in Fig. 8, where the learning processes are generally stable across datasets. A detailed analysis with more cases is in Appendix E.1.

## 6 Conclusion

In this work, we study the problem of link prediction with the bilateral edge noise and reveal that the edge representation is severely collapsed under such a bilateral noise. Based on the observation, we introduce the Robust Graph Information Bottleneck (RGIB) principle, aiming to extract reliable signals via decoupling and balancing the mutual information among inputs, labels, and representation to enhance the robustness and avoid collapse. Regarding the instantiations of RGIB, the self-supervised learning technique and data reparameterization mechanism are utilized to establish the RGIB-SSL and RGIB-REP, respectively. Extensive studies on \(6\) common datasets and \(3\) popular GNNs verify the denoising effect of the proposed RGIB methods under different noisy scenarios.