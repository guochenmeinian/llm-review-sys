# CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns

Shengsheng Lin\({}^{1}\), Weiwei Lin\({}^{1,2,}\), Xinyi Hu\({}^{3}\), Wentai Wu\({}^{4}\), Ruichao Mo\({}^{1}\), Haocheng Zhong\({}^{1}\)

\({}^{1}\)School of Computer Science and Engineering, South China University of Technology, China

\({}^{2}\)Pengcheng Laboratory, China

\({}^{3}\)Department of Computer Science and Engineering, The Chinese University of Hong Kong

\({}^{4}\)College of Information Science and Technology, Jinan University, China

cslinshengsheng@mail.scut.edu.cn, linwu@scut.edu.cn, xyhu@cse.cuhk.edu.hk, wentaiwu@jun.edu.cn, {cs_moruchao, cshczhong}@mail.scut.edu.cn

Corresponding author

###### Abstract

The stable periodic patterns present in time series data serve as the foundation for conducting long-horizon forecasts. In this paper, we pioneer the exploration of explicitly modeling this periodicity to enhance the performance of models in long-term time series forecasting (LTSF) tasks. Specifically, we introduce the Residual Cycle Forecasting (RCF) technique, which utilizes learnable recurrent cycles to model the inherent periodic patterns within sequences, and then performs predictions on the residual components of the modeled cycles. Combining RCF with a Linear layer or a shallow MLP forms the simple yet powerful method proposed in this paper, called CycleNet. CycleNet achieves state-of-the-art prediction accuracy in multiple domains including electricity, weather, and energy, while offering significant efficiency advantages by reducing over 90% of the required parameter quantity. Furthermore, as a novel plug-and-play technique, the RCF can also significantly improve the prediction accuracy of existing models, including PatchTST and iTransformer. The source code is available at: https://github.com/ACAT-SCUT/CycleNet.

## 1 Introduction

Time series forecasting (TSF) plays a crucial role in various domains such as weather forecasting, transportation, and energy management, providing insights for early warnings and facilitating proactive planning. Particularly, accurate predictions over long horizons (e.g., spanning several days or months) offer increased convenience, referred to as Long-term Time Series Forecasting (LTSF) . However, the principle enabling long-horizon prediction lies in understanding the inherent periodicity within the data . Unlike short-term forecasting, long-term predictions cannot rely solely on recent temporal information (including means, trends, etc.). For instance, a user's electricity consumption thirty days ahead not only correlates with their consumption patterns in the past few days.

In such cases, long-term dependencies, or in other words, underlying stable periodicity within the data, serve as the practical foundation for conducting long-term predictions . This is why existing models emphasize their capability to extract features with long-term dependencies. Models like Informer , Autoformer , and PatchTST  utilize the Transformer's ability for long-distance modeling to address LTSF tasks. ModernTCN  employs large convolutional kernels to enhance TCNs' ability to capture long-range dependencies, and SegRNN  uses segment-wise iterations toimprove RNN methods' handling of long sequences. If a model can accurately capture long-range dependencies, it can precisely extract periodic patterns from historical long sequences, enabling more accurate long-horizon predictions.

However, if the purpose of constructing deep and complex models is solely to better extract periodic features from long-range dependencies, why not directly model the patterns? As illustrated in Figure 1, electricity data exhibits clear daily periodic patterns (in addition to possible weekly patterns). We can use a globally shared daily segment to represent the periodic pattern in electricity consumption. By repeating this daily segment \(N\) times, we can continuously represent the cyclic components of \(N\) days' electricity consumption sequences.

Based on the above motivation, we pioneer explicit modeling of periodic patterns in the data to enhance the model's performance on LTSF tasks in this paper. Specifically, we propose the **Residual Cycle Forecasting** (RCF) technique. It involves using _learnable recurrent cycles_ to explicitly model the inherent periodic patterns within time series data, followed by predicting the residual components of the modeled cycles. Combining the RCF technique with either a single-layer Linear or a dual-layer MLP results in **CycleNet**, a simple yet powerful method. CycleNet achieves consistent state-of-the-art performance across multiple domains and offers significant efficiency advantages.

In summary, this paper contributes:

* We identify the presence of shared periodic patterns in long-horizon forecasting domains and propose explicit modeling of these patterns to enhance the model's performance on LTSF tasks.
* Technically, we introduce the RCF technique, which utilizes learnable recurrent cycles to explicitly model the inherent periodic patterns within time series data, followed by predicting the residual components of the modeled cycles. The RCF technique significantly enhances the performance of basic (or existing) models.
* Applying RCF with a Linear layer or a shallow MLP forms the proposed simple yet powerful method, called CycleNet. CycleNet achieves consistent state-of-the-art performance across multiple domains and offers significant efficiency advantages.

## 2 Related work

In fact, utilizing periodic information to enhance model prediction accuracy is not a novel concept. Numerous studies, in particular, have introduced a series of Seasonal-Trend Decomposition (STD) techniques that allow models to better leverage periodic information. Popular models such as Autoformer , FEDformer , and DLinear  utilize the classical STD approach to decompose the original time series into two equally sized subsequences: seasonal and trend components, which are then modeled independently. These classical STD methods typically use a basic moving average (MOV) kernel to perform a sliding aggregation to obtain the trend component. Recently, Leddam  proposed replacing the traditional MOV kernel in STD with a Learnable Decomposition (LD) kernel, leading to improved performance. Additionally, DEPTS  treats the periodicity of sequences as a parameterized function with respect to time, and learns periodic and residual components layer-wise through its periodic and local blocks. SparseTSF , another recent work, utilizes cross-period sparse forecasting technique to decouple cycles and trends, achieving impressive performance at extremely low cost.

The RCF technique proposed in this paper can essentially be considered a type of STD method. The key difference from existing techniques lies in its explicit modeling of global periodic patterns within independent sequences using learnable recurrent cycles. The proposed RCF technique is _conceptually simple_, _computationally efficient_, and yields _significant improvements_ in prediction accuracy. The

Figure 1: Shared daily periodic patterns present in the Electricity dataset.

further proposed CycleNet, which combines the RCF technique with a simple backbone, is a Linear- or MLP-based model that is simple, efficient, and powerful for time series forecasting. To correctly position CycleNet, we have provided a detailed review of the development of different categories of time series forecasting methods (including Transformer-based, RNN-based, etc.) in Appendix A.

## 3 CycleNet

Given a time series \(X\) with \(D\) variables or channels, the objective of time series forecasting is to predict future horizons \(H\) steps ahead based on past \(L\) observations, mathematically represented as \(f:x_{t-L+1:t}^{L D}_{t+1:t+H} ^{H D}\). In fact, the inherent periodicity within time series is fundamental for accurate prediction, particularly when forecasting over large horizons, such as 96-720 steps (corresponding to several days or months). To enhance the model's performance on long-term prediction tasks, we propose the Residual Cycle Forecasting (RCF) technique. It combines a Linear layer or a shallow MLP to form a simple yet powerful method CycleNet, as illustrated in Figure 2, with detailed pseudocode provided in Appendix B.1.

### Residual cycle forecasting

The RCF technique comprises two steps: the first step involves modeling the periodic patterns of sequences through learnable recurrent cycles within independent channels, and the second step entails predicting the residual components of the modeled cycles.

Periodic patterns modelingGiven \(D\) channels with a priori cycle length \(W\), we first generate _learnable recurrent cycles_\(Q^{W D}\), all initialized to zeros. These recurrent cycles are globally shared within channels, meaning that by performing cyclic replications, we can obtain cyclic components \(C\) of the sequence \(X\) of the same length. These recurrent cycles \(Q\) of length \(W\) undergo gradient backpropagation training along with the backbone module for prediction, yielding learned representations (distinct from the originally initialized zeros) that unveil the internal cyclic patterns within the sequence.

Here, the cycle length \(W\) depends on the a priori characteristics of the dataset and should be set to the maximum stable cycle within the dataset. Considering that scenes requiring long-term predictions usually exhibit prominent, explicit cycles (e.g., electrical consumption and traffic data exhibit clear daily and weekly cycles), determining the specific cycle length is available and straightforward. Additionally, the dataset's cycles can be further examined through autocorrelation functions (ACF) , as revealed in Appendix B.2.

Figure 2: CycleNet architecture. CycleNet/Linear and CycleNet/MLP represent using a single-layer Linear model and a dual-layer MLP model, respectively, as the backbone of CycleNet. Here, \(D=3\).

Residual forecastingPredictions made on the residual components of the modeled cycles, termed residual forecasting, are as follows:

1. Remove the cyclic components \(c_{t-L+1:t}\) from the original input \(x_{t-L+1:t}\) to obtain residual components \(x^{}_{t-L+1:t}\).
2. Pass \(x^{}_{t-L+1:t}\) through the backbone to obtain predictions for the residual components, \(^{}_{t+1:t+H}\).
3. Add the predicted residual components \(^{}_{t+1:t+H}\) to the cyclic components \(c_{t+1:t+H}\) to obtain \(_{t+1:t+H}\).

It is important to note that, since the cyclic components \(C\) are virtual sequences derived from the cyclic replications of \(Q\), we cannot directly obtain the aforementioned sub-sequences \(c_{t-L+1:t}\) and \(c_{t+1:t+H}\). Therefore, as illustrated in Figure 3, appropriate alignments and repetitions of the recurrent cycles \(Q\) are needed to obtain equivalent sub-sequences: (i) Left-shift \(Q\) by \(t W\) positions to obtain \(Q^{(t)}\). Here, \(t W\) can be viewed as the relative positional index of the current sequence sample within \(Q\). (ii) Repeat \(Q^{(t)} L/W\) times and concatenate \(Q^{(t)}_{0:L W}\). Mathematically, these two equivalent subsequences can be represented as:

\[c_{t-L+1:t} =[,,Q^{(t)}}_{ L/W },Q^{(t)}_{0:L W}],\] (1) \[c_{t+1:t+H} =[,,Q^{(t+L)}}_{ H/W },Q^{(t+L)}_{0:H W}].\] (2)

BackboneThe original prediction task is transformed into cyclic residual component modeling, which can serve as normal sequence modeling. Therefore, any existing time series forecast model can be employed as a backbone. In this paper, our aim is to propose and examine a method for enhancing time series prediction by explicitly modeling cycles (i.e., RCF). Thus, we opt for the most basic backbone, namely a single-layer Linear and a dual-layer MLP, forming our simple yet powerful methods, CycleNet/Linear and CycleNet/MLP. Herein, each channel utilizes the same backbone with parameter sharing for modeling, which is also referred to as the Channel Independent strategy .

### Instance normalization

The statistical properties of time series data, such as the mean, often vary over time, which is referred to as distributional shifts. This can lead to poor performance of models trained on historical training sets when applied to future data. To address this issue, recent research has introduced Instance Normalization strategies like RevIN [45; 22; 26]. Mainstream approaches such as iTransformer , PatchTST , and SparseTSF  have widely adopted similar techniques to enhance performance. To improve the robustness of CycleNet, we also incorporate a similar _optional_ strategy (see the full ablation study in Appendix C.4). Specifically, we remove the varying statistical properties from the model's internal representations outside of CycleNet's input and output steps:

Figure 3: Alignments and repetitions of the recurrent cycles \(Q\). Here, \(D=1\).

\[x_{t-L+1:t} =-}{},\] (3) \[_{t+1:t+H} =_{t+1:t+H}+,\] (4)

where \(\) and \(\) represent the mean and standard deviation of the input window, respectively, and \(\) is a small constant for numerical stability. This method aligns with the RevIN version that excludes learnable affine parameters .

### Loss function

To remain consistent with current mainstream methods, CycleNet defaults to using Mean Squared Error (MSE) as the loss function to ensure fair comparison with other methods, formulated as:

\[oss=\|x_{t+1:t+H}-_{t+1:t+H}\|_{2}^{2}.\] (5)

## 4 Experiments

### Setup

DatasetsWe utilized widely adopted benchmark datasets including the ETT series , Weather, Traffic, Electricity, and Solar-Energy . Preprocessing operations on the datasets, such as dataset splitting and normalization methods, remained consistent with prior works (e.g., Autoformer , iTransformer , etc.).

The information of the datasets is shown in Table 1. Note that these datasets all exhibit stable cyclic patterns, such as daily and weekly, which form the realistic basis for performing long-horizon forecasting. Combined with the sampling frequency of the datasets, we can infer the maximum cycle length of the datasets, such as 24 for ETTh1 and 168 for Electricity. These manually inferred cycle lengths can be further confirmed through the ACF analysis, details of which are provided in Appendix B.2. The hyperparameter \(W\) of CycleNet is set by default to match the cycle length in Table 1.

BaselinesWe compared CycleNet against state-of-the-art models in recent years, including iTransformer , PatchTST , Crossformer , TiDE , TimesNet , DLinear , SCINet , FEDformer , Autoformer . To comprehensively evaluate CycleNet's performance, the Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics were employed.

EnvironmentsAll experiments in this paper were implemented using PyTorch , trained using the Adam  optimizer, and executed on a single NVIDIA GeForce RTX 4090 GPU with 24 GB memory.

### Main results

Table 2 shows the comparison results of CycleNet with other models on multivariate LTSF tasks. Overall, CycleNet achieves state-of-the-art performance (except for the Traffic dataset), with CycleNet/MLP ranking first overall, and CycleNet/Linear ranking second overall. Due to the nonlinear

   Dataset & ETTh1 \& ETTh2 & ETTm1 \& ETTm2 & Electricity & Solar-Energy & Traffic & Weather \\  Timesteps & 17,420 & 69,680 & 26,304 & 52,560 & 17,544 & 52,696 \\  Channels & 7 & 7 & 321 & 137 & 862 & 21 \\  Frequency & 1 hour & 15 mins & 1 hour & 10 mins & 1 hour & 10 mins \\  Cycle Patterns & Daily & Daily & Daily \& Weekly & Daily & Daily \& Weekly & Daily \\ 
**Cycle Length** & **24** & **96** & **168** & **144** & **168** & **144** \\   

Table 1: Dataset Information.

mapping capability of MLP compared to Linear, CycleNet/MLP performs better on high-dimensional datasets such as Electricity and Solar-Energy (i.e., datasets with more than 100 channels). In summary, with the support of the RCF technique, even a very simple and basic model (i.e., Linear and MLP) can achieve the current best performance, surpassing other deep models. This fully demonstrates the advantages of the RCF technique.

Furthermore, we can observe that CycleNet's performance on the Traffic dataset is inferior to iTransformer, which models multivariate relationships in time series data using an inverted Transformer. This is because the Traffic dataset exhibits spatiotemporal characteristics and temporal lag characteristics, where the traffic flow at a certain detection point significantly affects the future values of neighboring detection points. In such cases, modeling sufficient inter-channel relationships is necessary, and iTransformer accomplishes this. In contrast, CycleNet independently models the temporal dependencies of each channel, hence it suffers a disadvantage in this scenario. However, CycleNet still significantly outperforms other baselines on the Traffic dataset, demonstrating the competitiveness of CycleNet. Additionally, we have included more analysis of CycleNet in traffic scenarios in Appendix C.5, including a full comparison of results on the PEMS datasets.

### Efficiency analysis

The proposed RCF technique, as a plug-and-play module, requires minimal overhead, needing only additional \(W D\) learnable parameters and no additional Multiply-Accumulate Operations (MACs). The backbones of CycleNet, namely single-layer Linear and dual-layer MLP, are also significantly lightweight compared to other multi-layer stacked models. Table 3 demonstrates the efficiency comparison between CycleNet and other mainstream models, where CycleNet shows significant advantages. Particularly, compared to iTransformer, which also possesses strong capabilities in modeling long-term dependencies and nonlinear learning, CycleNet/MLP has over ten times fewer parameters and MACs. As for CycleNet/Linear, which shares the same single-layer linear backbone as DLinear, it also has fewer parameters and MACs. However, in terms of training speed, DLinear is still faster than CycleNet/Linear. This is because the RCF technique requires aligning the recurrent cycles with each data sample, which incurs additional CPU time. Overall, considering the significant improvement in prediction accuracy brought by the RCF technique, CycleNet achieves the best balance between performance and efficiency.

   Dataset & EITH & EITH2 & EITH1 & EITH2 & EITH2 & Electricity & Solar-Energy & Traffic & Weather \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Autoformer  & 0.496 & 0.487 & 0.459 & 0.459 & 0.588 & 0.517 & 0.327 & 0.371 & 0.227 & 0.338 & 0.885 & 0.711 & 0.628 & 0.379 & 0.338 & 0.382 \\ FEDformer  & 0.440 & 0.460 & 0.437 & 0.449 & 0.448 & 0.452 & 0.305 & 0.349 & 0.214 & 0.327 & 0.291 & 0.381 & 0.610 & 0.376 & 0.309 & 0.360 \\ SCLNet  & 0.747 & 0.647 & 0.594 & 0.723 & 0.485 & 0.481 & 0.571 & 0.537 & 0.268 & 0.368 & 0.282 & 0.375 & 0.580 & 0.599 & 0.263 & 0.363 \\ DLinear  & 0.456 & 0.452 & 0.595 & 0.515 & 0.403 & 0.407 & 0.350 & 0.401 & 0.210 & 0.300 & 0.300 & 0.401 & 0.625 & 0.383 & 0.265 & 0.317 \\ TimeNet  & 0.458 & 0.450 & 0.434 & 0.427 & 0.400 & 0.469 & 0.291 & 0.333 & 0.192 & 0.295 & 0.301 & 0.319 & 0.260 & 0.336 & 0.295 & 0.287 \\ Tube  & 0.541 & 0.507 & 0.610 & 0.504 & 0.419 & 0.358 & 0.401 & 0.251 & 0.344 & 0.447 & 0.417 & 0.760 & 0.473 & 0.271 & 0.320 \\ Crossflow  & 0.529 & 0.522 & 0.942 & 0.684 & 0.513 & 0.496 & 0.757 & 0.610 & 0.244 & 0.334 & 0.641 & 0.639 & 0.505 & 0.304 & 0.259 & 0.315 \\ PatchTNet  & 0.469 & 0.454 & 0.387 & 0.407 & 0.387 & 0.400 & 0.281 & 0.326 & 0.205 & 0.290 & 0.270 & 0.307 & 0.481 & 0.304 & 0.259 & 0.281 \\ TimeNet  & 0.447 & 0.440 & **0.364** & **0.395** & 0.381 & 0.395 & 0.275 & 0.325 & 0.182 & 0.272 & 0.216 & 0.280 & 0.484 & 0.297 & **0.240** & 0.271 \\ iTransformer  & 0.454 & 0.447 & 0.383 & 0.407 & 0.440 & 0.210 & 0.283 & 0.312 & 0.279 & 0.233 & 0.262 & 0.488 & 0.285 & 0.278 \\  CycleNet/Linear & **0.432** & **0.427** & 0.383 & 0.404 & 0.386 & **0.395** & **0.272** & 0.313 & 0.120 & 0.260 & 0.235 & 0.270 & 0.485 & 0.313 & 0.254 & 0.229 \\ CycleNet/MLP & 0.457 & 0.441 & 0.388 & 0.409 & **0.379** & 0.396 & **0.266** & **0.314** & **0.168** & **0.259** & **0.210** & **0.261** & 0.472 & 0.301 & 0.243 & **0.271** \\   

Table 2: Multivariate long-term time series forecasting results. The look-back length \(L\) is fixed as 96 and the results are averaged from all prediction horizons of \(H\{96,192,336,720\}\). Full results and more comparison results on _longer_ look-back lengths are available in Appendix C.2. The results of other models are sourced from iTransformer  and TimeMixer . The best results are highlighted in **bold** and the second best are underlined.

   Model & Parameters & MACs & Training Time(s) \\  Informer  & 12.53M & 3.97G & 70.1 \\ Autoformer  & 12.22M & 4.41G & 107.7 \\ FEDformer  & 17.98M & 4.41G & 238.7 \\ DLinear  & 139.6K & 44.91M & 18.1 \\ PatchTST  & 10.74M & 25.87G & 129.5 \\ Transformer  & 5.15M & 1.65G & 35.1 \\  CycleNet/MLP & 472.9K & 134.84M & 30.8 \\ CycleNet/Linear & 123.7K & 22.42M & 29.6 \\  RCF part & 53.9K & 0 & 12.8 \\   

Table 3: Efficiency comparison between CycleNet and other models on the Electricity dataset with look-back length \(L=96\) and forecast horizon \(H=720\). Training Time denotes the average time required per epoch for the model.

### Ablation study and analysis

Effectiveness of RCFTo investigate the effectiveness of RCF, we conducted comprehensive ablation experiments on two datasets with significant periodicity: Electricity and Traffic. The results are shown in Table 4.

Firstly, when combining the basic Linear and MLP backbones apply the same instance by default) with the RCF technique, a significant improvement in prediction accuracy (approximately 10% to 20%) is observed. This demonstrates that the success of CycleNet is largely attributed to the RCF technique rather than the backbones themselves or the instance normalization strategy. Overall, the performance of MLP is stronger than that of Linear, regardless of whether the RCF technique is applied. This indicates that non-linear mapping capability is necessary when modeling high-dimensional datasets with the channel-independent strategy (sharing parameters across each channel), aligning with previous research findings .

Secondly, we further verified whether RCF can enhance the prediction accuracy of existing models, as RCF is essentially a plug-and-play flexible technique. It is observed that incorporating RCF still improves the performance of existing complex designed, deep stacked models (approximately 5% to 10%), such as PatchTST  and iTransformer . Even for DLinear, which already employs the classical MOV-based STD technique, RCF was able to provide an improvement of approximately 20%. This further indicates the effectiveness and portability of RCF.

However, an interesting phenomenon was observed: although the MAE decreases when PatchTST and iTransformer are combined with RCF, the MSE increases. The most important reason behind this is that there are extreme points in the Traffic dataset that could affect the effectiveness of RCF, which fundamentally relies on learning the historical average cycles in the dataset. We further analyze this phenomenon in detail in Appendix C.5 and suggest potential directions for improving the RCF technique.

Comparison of different STD techniquesThe proposed RCF technique is essentially a more powerful STD approach. Unlike existing methods that decompose the periodic (seasonal) component from a limited look-back window, RCF learns the global periodic component from the training set. Here, we compare the effectiveness of RCF with existing STD techniques, using a pure Linear model as the backbone (without applying any instance normalization strategies). The comparison includes LD from Leddam , MOV from DLinear , and Sparse technique from SparseTSF . As shown in Table 5, RCF significantly outperforms other STD methods, particularly on datasets with strong periodicity, such as Electricity and Solar-Energy. In contrast, the other STD methods did not show significant advantages over the pure Linear model.

There are several reasons for this. First, MOV and LD-based STD methods achieve trend estimation by sliding aggregation within the look-back window, which suffers from inherent issues [27; 26]: (i) The sliding window of the moving average needs to be larger than the maximum period of the seasonal component; otherwise, the decomposition may be incomplete (especially when the period length exceeds the look-back sequence length, making decomposition potentially impossible).

   Dataset &  &  \\  Horizon &  &  &  &  &  &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Linear & 0.197 & 0.274 & 0.197 & 0.277 & 0.212 & 0.292 & 0.253 & 0.324 & 0.645 & 0.338 & 0.598 & 0.361 & 0.645 & 0.362 & 0.643 & 0.381 \\ \(*\)RCF & 0.141 & 0.234 & 0.155 & 0.247 & 0.172 & 0.264 & 0.210 & 0.265 & 0.480 & 0.314 & 0.482 & 0.313 & 0.476 & 0.303 & 0.503 & 0.320 \\ Improve & **28.65** & **14.65** & **21.44** & **10.85** & **18.85** & **9.55** & **17.47** & **8.75** & **25.64** & **18.09** & **19.55** & **13.27** & **21.36** & **16.27** & **21.85** & **16.17** \\  MLP & 0.175 & 0.298 & 0.181 & 0.265 & 0.197 & 0.282 & 0.240 & 0.317 & 0.500 & 0.325 & 0.496 & 0.321 & 0.509 & 0.325 & 0.542 & 0.342 \\ \(*\)RCF & 0.136 & 0.229 & 0.152 & 0.244 & 0.170 & 0.264 & 0.212 & 0.292 & 0.458 & 0.296 & 0.450 & 0.294 & 0.470 & 0.299 & 0.502 & 0.324 \\ Improve & **22.25** & **11.64** & **15.95** & **8.05** & **13.66** & **6.36** & **11.65** & **5.75** & **8.55** & **8.95** & **7.94** & **8.36** & **7.77** & **8.09** & **7.36** & **8.15** \\  DLinear & 0.195 & 0.278 & 0.194 & 0.281 & 0.207 & 0.297 & 0.243 & 0.331 & 0.649 & 0.398 & 0.599 & 0.372 & 0.606 & 0.375 & 0.646 & 0.396 \\ \(*\)RCF & 0.143 & 0.240 & 0.165 & 0.253 & 0.171 & 0.270 & 0.270 & 0.294 & 0.302 & 0.506 & 0.317 & 0.499 & 0.317 & 0.512 & 0.325 & 0.545 & 0.343 \\ Improve & **26.45** & **13.66** & **19.75** & **10.05** & **17.45** & **9.56** & **13.64** & **8.95** & **21.15** & **20.46** & **14.64** & **14.46** & **14.54** & **13.35** & **15.64** & **15.85** \\  PathTST & 0.168 & 0.260 & 0.176 & 0.266 & 0.198 & 0.223 & 0.233 & 0.317 & 0.436 & 0.281 & 0.449 & 0.285 & 0.464 & 0.293 & 0.499 & 0.310 \\ \(*\)RCF & 0.136 & 0.231 & 0.153 & 0.246 & 0.170 & 0.264 & 0.211 & 0.290 & 0.438 & 0.264 & 0.457 & 0.270 & 0.469 & 0.275 & 0.509 & 0.292 \\ Improve & **19.50** & **11.05** & **13.05** & **17.66** & **6.65** & **9.45** & **7.56** & **5.75** & **6.15** & **1.58** & **5.55** & 1.069 & **6.35** & -2.095 & **6.15** \\  iTransformer & 0.148 & 0.240 & 0.162 & 0.253 & 0.178 & 0.269 & 0.225 & 0.317 & 0.395 & 0.268 & 0.417 & 0.276 & 0.433 & 0.283 & 0.467 & 0.302 \\ \(*\)RCF & 0.136 & 0.231 & 0.153 & 0.247 & 0.168 & 0.263 & 0.194 & 0.287 & 0.415 & 0.263 & 0.440 & 0.271 & 0.456 & 0.278 & 0.491 & 0.294 \\ Improve & **8.15** & **3.76** & **5.65** & **2.45** & **5.85** & **2.25** & **13.85** & **9.55** & -5.18 & **1.95** & -5.55 & **1.56** & -5.53 & **1.85** & -5.18 & **2.65** \\   

Table 4: Ablation study of RCF technique. The Linear and MLP backbones apply the same instance normalization strategy as CycleNet by default to fully demonstrate the effect of RCF technique.

(ii) Zero-padding is required at the edges of the sequence samples to obtain equally sized moving average sequences, leading to distortion of the sequence edges. As for the Sparse technique, being a lightweight decomposition method, it relys more on longer look-back windows and instance normalization strategies to ensure adequate performance.

Additionally, these methods that decouple trend and seasonality within the look-back window are essentially equivalent to unconstrained or weakly constrained linear regression , which means that after full training convergence, linear-based models combined with these methods are theoretically equivalent to pure linear models. In contrast, the periodic components obtained by the RCF technique are globally estimated from the training set, allowing it to surpass the limitations of a finite-length look-back window, and thus, its capabilities extend beyond standard linear regression.

Impact of hyperparameter \(W\)The hyperparameter \(W\) determines the length of the learnable recurrent cycles \(Q\) in the RCF technique. In principle, it must match the maximum primary cycle length in the data to correctly model the periodic patterns of the sequence. We investigate the performance of the CycleNet/Linear model under different settings of \(W\) for different datasets in Table 6. When correctly setting the hyperparameter \(W\) to the max cycle length of the dataset (i.e., the cycle length pre-inferred in Table 1), RCF can play a significant role, yielding a large performance gap compared to the cases when it is not correctly set. This indicates the necessity of inferring and setting the correct \(W\) for RCF to function properly. Furthermore, when \(W\) is incorrectly set, the model's performance is almost the same as when RCF is not used at all. This suggests that even in the worst-case scenario, RCF does not bring significant negative effects.

Visualization of the learned periodic patternsThe purpose of the RCF technique is to utilize the learnable recurrent cycles \(Q\) (initialized to zero) to model the periodic patterns in time series data. After co-training with the backbone, the recurrent cycles can represent the inherent periodic patterns of the sequence. Figure 4 illustrates the different periodic patterns learned from different datasets and channels. For example, Figure 4(c) shows the daily operating pattern of solar photovoltaic generation, while Figure 4(d) displays the weekly operating pattern of traffic flow, featuring peak traffic in the mornings on weekdays. These periodic patterns learned from the global sequence provide important supplementary information to the prediction model, especially when the length of the look-back window is limited and may not provide sufficient cyclic information when the cycle length is long.

Furthermore, although the cycle length is the same for different channels within the same dataset, the specific periodic patterns differ, as shown in Figure 4(e-h). Particularly, Figure 4(f) demonstrates the intermittent periodicity of household electricity consumption on weekdays, while others exhibit relatively uniform weekday patterns in their respective channels. This highlights the necessity of separately modeling the periodic patterns for each channel.

   Setup & RCF/W=168 & RCF/W=144 & RCF/W=96 & RCF/W=24 & W/o. RCF \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Electricity & **0.142** & **0.234** & 0.196 & 0.275 & 0.196 & 0.274 & 0.195 & 0.274 & 0.197 & 0.274 \\ Traffic & **0.480** & **0.314** & 0.617 & 0.386 & 0.617 & 0.385 & 0.618 & 0.385 & 0.645 & 0.383 \\ Solar-Energy & 0.289 & 0.376 & **0.208** & **0.256** & **0.276** & 0.365 & 0.287 & 0.375 & 0.286 & 0.375 \\ EITm1 & 0.350 & 0.369 & 0.340 & 0.366 & **0.325** & **0.363** & 0.348 & 0.367 & 0.351 & 0.372 \\ EITh1 & 0.395 & 0.402 & 0.384 & 0.395 & 0.383 & 0.393 & **0.377** & **0.391** & 0.384 & 0.392 \\   

Table 6: Performance of the CycleNet/Linear model with varied \(W\). The forecast horizon is set as 96.

    &  &  &  &  &  \\  & (RCF+Linear) & (LD+Linear) &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  ETHh1 & **0.418** & **0.434** & 0.427 & 0.439 & 0.425 & 0.437 & 0.424 & 0.436 & 0.427 & 0.439 \\ ETHh2 & **0.451** & **0.456** & 0.455 & 0.457 & 0.471 & 0.467 & 0.460 & 0.460 & 0.460 & 0.462 \\ EITm1 & **0.349** & **0.382** & 0.365 & 0.387 & 0.367 & 0.390 & 0.362 & 0.383 & 0.362 & 0.384 \\ EITm2 & **0.266** & **0.330** & 0.273 & 0.336 & 0.280 & 0.341 & 0.290 & 0.352 & 0.269 & 0.331 \\ Electricity & **0.157** & **0.255** & 0.167 & 0.264 & 0.167 & 0.264 & 0.172 & 0.268 & 0.167 & 0.265 \\ Solar-Energy & **0.220** & **0.259** & 0.253 & 0.316 & 0.254 & 0.318 & 0.255 & 0.315 & 0.253 & 0.318 \\ Traffic & **0.423** & **0.289** & 0.434 & 0.296 & 0.434 & 0.296 & 0.435 & 0.292 & 0.434 & 0.296 \\ Weather & 0.245 & 0.300 & 0.244 & 0.297 & **0.244** & **0.296** & 0.246 & 0.298 & 0.245 & 0.297 \\   

Table 5: Comparison of different STD techniques. To directly compare the effects of STD, the configuration used here is consistent with that of DLinear , with a sufficient look-back window length of 336 and no additional instance normalization strategies. Thus, CLinear here refers to CycleNet/Linear without RevIN. The reported results are averaged across all prediction horizons of \(H\{96,192,336,720\}\), with full results available in Appendix C.3.

In conclusion, these findings demonstrate that the RCF technique can effectively learn the inherent periodic patterns in time series data, serving as a crucial explanatory factor contributing to the state-of-the-art performance of CycleNet. Additionally, we have included further analysis in Appendix C.1, showcasing the learned periodic patterns of RCF under different configurations to better illustrate how RCF operates.

Performance with varied look-back lengthThe look-back length determines the richness of historical information that can be utilized. Theoretically, the larger it is, the better the model performance should be, especially for models capable of capturing long-term dependencies. Figure 5 shows the performance of different models under different look-back lengths. It can be observed that CycleNet, as well as representatives of current state-of-the-art models such as iTransformer , PatchTST , and DLinear , all achieve better performance with longer look-back lengths. This indicates that these models all possess strong capabilities in modeling long-term dependencies.

It is worth highlighting that (i) on the Electricity dataset, CycleNet outperforms current state-of-the-art models at any prediction length; (ii) on the Traffic dataset, CycleNet still falls short compared to powerful existing multivariate forecasting models, such as iTransformer. This indicates that in scenarios with strong periodicity but without additional spatiotemporal relationships, fully leveraging the periodic components is sufficient to achieve high-accuracy predictions. However, in more complex scenarios that require thorough modeling of relationships between variables, a simple channel-independent strategy combined with a basic backbone, like CycleNet, still struggles to fully meet the demands. Therefore, in Appendix C.5, we further analyze the current limitations of the current RCF technique in spatiotemporal scenarios (such as the traffic domain) and and point out

Figure 4: Visualization of the periodic patterns learned by CycleNet/Linear. Panels (a-d) display different periodic patterns learned from different datasets, and panels (e-h) show different periodic patterns learned from different channels within the same dataset. The \(i\) th indicates the index of the channel within the dataset.

Figure 5: Performance of CycleNet and comparative models with different look-back lengths. The forecast horizon is set as 96.

potential directions for future improvements. Finally, we also provide a comparison of CycleNet with existing models on full datasets using longer look-back windows in Appendix C.2.

## 5 Discussion

Potential limitationsCycleNet demonstrates its efficacy in LTSF scenarios characterized by prominent and explicit periodic patterns. However, there are several potential limitations of CycleNet that warrant discussion here:

* **Unstable cycle length:** CycleNet may not be suitable for datasets where the cycle length (or frequency) varies over time, such as electrocardiogram (ECG) data, because CycleNet can only learn a fixed-length cycle.
* **Varying cycle lengths across channels:** When different channels within a dataset exhibit cycles of varying lengths, CycleNet may encounter challenges because it defaults to modeling all channels with the same cycle length \(W\). Given CycleNet's channel-independent modeling strategy, one potential solution is to pre-process the dataset by splitting it based on cycle lengths or to independently model each channel as a separate dataset.
* **Impact of outliers:** If the dataset contains significant outliers, CycleNet's performance may be affected. This is because the fundamental working principle of RCF is to learn the historical average cycles in the dataset. When significant outliers exist, the mean of a certain point in the cycle learned by RCF can be exaggerated, leading to inaccurate estimation of both the periodic and residual components, which subsequently impacts the prediction process.
* **Long-range cycle modeling:** The RCF technique is effective for modeling mid-range stable cycles (e.g., daily or weekly). However, considering longer dependencies (such as yearly cycles) presents a more challenging task for the RCF technique. Although, in theory, CycleNet's \(W\) can be set to a yearly cycle length to model annual cycles, the biggest difficulty lies in collecting sufficiently long historical data to train a complete yearly cycle, which might require decades of data. In this case, future research needs to develop more advanced techniques to specifically address long-range cycle modeling.

Future work: further modeling inter-channel relationshipsThe RCF technique enhances the model's ability to model the periodicity of time series data but does not explicitly consider the relationships between multiple variables. In some spatio-temporal scenarios where spatial and temporal dependencies between variables exist, these relationships are crucial. For example, recent studies such as iTransformer  and SOFTS  indicate that appropriately modeling inter-channel relationships can improve performance in traffic scenarios. However, directly applying the RCF technique to iTransformer does not lead to significant improvement (at least for the MSE metric), as demonstrated in Table 4. We believe that devising a more reasonable multivariate modeling approach that combines CycleNet could be promising and valuable, and we leave it for future exploration.

## 6 Conclusion

This paper reveals the presence of inherent periodic patterns in time series data and pioneers the exploration of explicitly modeling this periodicity to enhance the performance of time series forecasting models. Technically, we propose the Residual Cycle Forecasting (RCF) technique, which models the shared periodic patterns in sequences through recurrent cycles and predicts the residual cyclic components via a backbone. Furthermore, we introduce the simple yet powerful LTSF methods CycleNet/Linear and CycleNet/MLP, which combine single-layer Linear and dual-layer MLP respectively with the RCF technique. Extensive experiments demonstrate the effectiveness of the RCF technique, and CycleNet as a novel and simple method achieves state-of-the-art results with significant efficiency advantages. The findings in this paper underscore the importance of periodicity as a key characteristic for accurate time series prediction, which should be given greater emphasis in the modeling process. Finally, integrating CycleNet with effective inter-channel relationship modeling methods serves as a promising and valuable future research direction.