# Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation

Hangcheng Liu1, Zhenhu Wu2, Hao Wang3, Xingshuo Han1, Shangwei Guo3,

**Tao Xiang3, and Tianwei Zhang1**

Corresponding authors.

1College of Computing and Data Science, Nanyang Technological University, Singapore

2School of Computer Science, Beijing University of Posts and Telecommunications, China

3College of Computer Science, Chongqing University, China

{hangcheng.liu, tianwei.zhang}@ntu.edu.sg

{hwang, swguo, txiang}@cqu.edu.cn

###### Abstract

Monocular Depth Estimation (MDE) enables the prediction of scene depths from a single RGB image, having been widely integrated into production-grade autonomous driving systems, e.g., Tesla Autopilot. Current adversarial attacks to MDE models focus on attaching an optimized adversarial patch to a designated obstacle. Although effective, this approach presents two inherent limitations: its reliance on specific obstacles and its limited malicious impact. In contrast, we propose a pioneering attack to MDE models that _decouples obstacles from patches physically and deploys optimized patches on roads_, thereby extending the attack scope to arbitrary traffic participants. This approach is inspired by our ground-breaking discovery: _various MDE models with different architectures, trained for autonomous driving, heavily rely on road regions_ when predicting depths for different obstacles. Based on this discovery, we design the Adversarial Road Marking (AdvRM) attack, which camouflages patches as ordinary road markings and deploys them on roads, thereby posing a continuous threat within the environment. Experimental results from both dataset simulations and real-world scenarios demonstrate that AdvRM is effective, stealthy, and robust against various MDE models, achieving about 1.507 of Mean Relative Shift Ratio (MRSR) over 8 MDE models. The code is available at this Github Repo.

## 1 Introduction

Monocular Depth Estimation (MDE) [6; 7; 8; 9] is a technology that extracts depth information from monocular RGB images, enabling the projection of pixels from a 2D image into a 3D space. Due to its commendable efficiency and performance, MDE has been successfully employed in autonomous driving , e.g., Tesla's production-grade Autopilot system [11; 12; 13].

Recent studies [5; 14; 15] have demonstrated that MDE models are vulnerable to adversarial attacks, leading to erroneous depth predictions. Unlike global imperceptible adversarial perturbations [14; 15], which are effective only in the digital domain, adversarial patches [1; 2; 3; 4; 5; 16; 17] exhibit robust performance in the physical world, thus garnering significant attention. To our knowledge, existing patch attacks focus on single obstacle scenarios. They deploy an obstacle-dependent patch on a specific obstacle to attack the victim vehicle behind this obstacle, as illustrated in Fig. 1. These obstacle-dependent patches have two inherent limitations: 1) **limited impact**: the patch is onlyeffective for the designated obstacle and cannot influence other unknown obstacles, making it challenging to adapt to complex traffic scenarios with multiple obstacles; and 2) **unstableness**: the effectiveness of these patches depends on the presence of the specific obstacles in the scene, rendering them unreliable for providing sustained threats, particularly those affixed on dynamic obstacles such as pedestrians. Table 1 reports more shortcomings of existing methods across various aspects.

Completely different from previous attacks against MDE, we make the **FIRST** attempt to decouple patches from obstacles physically, producing obstacle-independent adversarial patches and deploying them on roads (Fig. 1). By decoupling patches from obstacles, our road patches are no longer confined to altering the depth of specific known obstacles. Instead, the deployed road patch can mislead passing vehicles into changing the predicted depth of any obstacle that appears in front of it, even if the obstacle is unknown (Table 3), making it more suitable for complex traffic scenarios. Additionally, road patches typically maintain stability within a scene, thereby posing a persistent threat. Deploying patches on roads is inspired by our critical discovery: **MDE models trained for autonomous driving exhibit a strong dependency on roads when predicting depths for various obstacles** (Section 4). This suggests that road areas can serve as general patch areas for most MDE models. Moreover, we employ style transfer techniques  to disguise our road patches as visually innocuous road markings (Fig. 1), reducing suspicion while further increasing the patch's lifespan. Therefore, we call our attack Adversarial Road Marking (AdvRM).

Our main contributions can be summarized as follows:

* We conduct a comprehensive saliency analysis on various mainstream MDE models with different architectures and find that these models' predictions are road-dependent. We also provide a reasonable analysis of this phenomenon.
* We propose AdvRM which produces new road patches according to our observation and disguises them as ordinary road markings, misleading the passing victim vehicle's depth predictions for any possible obstacle.
* We conduct large-scale evaluations of the vulnerabilities of AdvRM across 3 CNN-based MDE models and 5 ViT-based MDE models in experiments.

## 2 Background and Related Works

### Monocular Depth Estimation (MDE)

Existing MDE models can be divided into two categories according to their backbone architectures: CNN-based and ViT-based. CNNs were the preferred backbone for previous MDE studies [6; 7; 19;

Figure 1: The difference between previous attacks (top) on MDE models and AdvRM (bottom). Previous attacks [1; 2; 3; 4; 5] are limited to single obstacle scenarios due to their obstacle-dependent patches. In contrast, the most significant advantage of AdvRM is **obstacle-agnostic**. This allows AdvRM to induce an MDE model to predict a false depth for any obstacle that appears in front of it, thereby being more suitable for complex multi-obstacle scenarios. Moreover, AdvRM **strategically places unobtrusive patches on the road**, exploiting the road dependency phenomenon we discovered in various mainstream MDE models, thereby making AdvRM both interpretable and applicable across different MDE models.

20, 21, 22]. However, Ranftl et al.  pointed out that the downsampling operations in CNNs limit dense predictions due to the resolution and granularity loss during the forward process. Therefore, many recent studies [8; 9; 24; 25; 26] start to adopt ViTs as the backbone, as they maintain feature resolution and enable global receptive fields through self-attention mechanisms. We notice that prior research on attacking MDE [1; 2; 3; 4; 4; 14; 15; 27] only focused on CNN-based MDE models, while largely neglecting ViT-based models. We fill this gap by demonstrating that ViT-based MDE models are also vulnerable to adversarial attacks.

### Related Works and Comparisons

An attacker can employ carefully crafted adversarial perturbations (global perturbations or local patches) to alter the MDE models' predictions for targeted obstacles [1; 2; 3; 4; 5; 14; 15], compromising the depth perception of autonomous vehicles. Considering that adversarial patches [1; 2; 3; 4; 5] show better practicality in the physical world, we focus on patch attacks in this study. In Table 1, we show the major differences or advantages over the latest representative patch attacks targeting MDE. Evidently, our AdvRM exhibits several advantages.

**Obstacle independence.** As AdvRM decouples patches from obstacles, the generation and deployment of patches are independent of specific obstacles, significantly increasing the adaptability of AdvRM in complex traffic scenarios, rendering it capable of changing the depth of unknown obstacles. In contrast, existing attacks focus on producing obstacle-dependent patches, which are less efficient or less effective in multi-obstacle scenarios, as shown in Fig. 1.

**G Stability.** In a traffic scene, obstacles are typically transient, with pedestrians and vehicles frequently entering and exiting, and even fixed roadblocks are generally temporary and eventually removed. Therefore, the life cycle of patches applied to specific obstacles is short-lived. On the contrary, our road patches, due to their natural appearance, are more likely to persist in traffic scenes for a longer period, posing a steady threat to passing vehicles and pedestrians.

**Interpretability.** Previous attacks selected patch placement locations, such as the central region of the obstacle's 2D projection, without explaining the rationale. Cheng et al.  demonstrated that placing patches in different areas within the obstacle affects the attack's performance, yet failed to elucidate the optimal region choice from an interpretability perspective. In contrast, we conduct saliency analysis to identify the sensitivity of various mainstream MDE models to different regions of the environment (Section 4). Our findings reveal a consistent optimal patch region, i.e., the road, thereby providing good interpretability for our attack.

## 3 Preliminaries

### Threat Model

We follow a similar adversarial scenario shown in [1; 2; 3], wherein an autonomous vehicle travels at a stable speed on lanes and uses an MDE model to perceive the depths of surrounding objects. To attack such an autonomous vehicle, an attacker carefully crafts an adversarial patch that looks like an ordinary road marking and places it in the path of a vehicle. This malicious patch forces the vehicle to estimate incorrect distances to obstacles in its front. The obstacles can be any traffic participants (e.g., cars, pedestrians) appearing there or static obstacles (e.g., roadblocks) placed in advance by the attacker. Incorrect depth information increases the likelihood of collisions.

  Attack & Patch Location & Obstacle Independence & Multiple Obstacle & Affected Area & Semantic & Stability & Intepretability \\   & & ✗ & ✗ & ✗ & ✗ & ✗ \\  & & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\  & & ✗ & ✗ & \(\) & ✗ & ✗ & ✗ \\  & & ✗ & ✗ & \(\) & ✗ & ✗ \\  & & ✗ & ✗ & \(\) & ✗ & ✗ \\  AdvRM (ours) & \(\) & ✗ & ✗ & \(\) & ✗ & ✗ \\  

Table 1: Comparisons between different patch attacks to MDE models. _Obstacle independent_: patches are independent of specific obstacles; _Semantic_: patches’ appearance fit the context; _Stability_: patches pose a long-term threat; _Interpretability_: explanation for the selection of patch location We consider two specific goals for the attacker: (1) increasing the estimated depth of an obstacle, which may lead to delayed braking responses and potentially cause collisions with the obstacle. (2) Decreasing the estimated depth of an obstacle, which can result in phantom braking by the vehicle. Considering that the first goal has more serious consequences than the second, we mainly focus on increasing the estimated depth in the subsequent investigation. Nevertheless, our attack method can easily achieve a depth decrease as well.

Consistent with prior studies [1; 2; 3; 4; 15], we assume that the attacker possesses comprehensive knowledge regarding the target MDE model. It is practical because the attacker can rent a vehicle of the same model as the victim vehicle and engage in reverse engineering of the MDE model [12; 28].

### Problem Formulation

Let \(x^{3 h w}\) be a benign frame captured by a monocular camera. It can be represented as \(x=b(1-M_{o})+o M_{o}\), where \(o\) denotes obstacle, \(b\) is environment (other areas besides \(o\) within the frame), \(\) is element-wise multiplication, and \(M_{o}^{h w}\) is a binary mask, in which the obstacle area is filled with 1 and others are filled with 0. \(f\) is an MDE model that outputs a depth map \(f(x)\), representing relative depths rather than absolute depths. \(\) represents an adversarial patch capable of converting a benign environment \(b\) into an adversarial environment \(\) after being inserted into \(b\). Let \(_{o}\) and \(_{}\) represent two algorithms that insert given elements (\(o\) or \(\)) into the environment (\(b\) or \(\)) according to the insertion parameters (\(_{o}\) or \(_{}\)). We have: \(x=_{o}(b,o,_{o})\), \(=_{o}(,o,_{o})\), and \(=_{}(b,,_{})\). For any \(o\) and its \(_{o}\), we want to seek for an optimal \(\) and its \(_{}\), satisfying

\[(f() M_{o})>(f(x) M_{o}),\] (1)

where \(()\) indicates average function. We denote \(f(x) M\) as \(f_{M}(x)\) for simplification in subsequent descriptions, where \(M\) represents arbitrary binary mask. In our study, \(_{}\) is determined based on the characteristics of MDE models (Section 4.2). Therefore, given \(b\) and \(_{}\), we find a satisfactory \(\) by solving the following optimization:

\[_{}_{o p(o),_{o} p(_{o})}[L_{a}( f_{M_{o}}(),f_{M_{o}}(x))+ L_{st}()],\] (2)

where \(p(o)\), \(p(_{o})\), \(L_{a}\), and \(L_{st}\) denote the distributions of \(o\) and \(_{o}\), adversarial loss, and stealthiness loss, respectively. \( 0\), balancing the attack effectiveness and stealthiness.

## 4 Saliency-driven Analysis for Patch Regions

### Saliency Maps for MDE

Previous attacks propose placing patches at the center of obstacles [2; 3] or determining the patch position within the obstacle through optimization . However, _it remains an unsolved question where the optimal patch regions in environments is._ Past works [29; 30] show that adding perturbations to regions that exert salient influence on the model predictions can increase the chance of successful attacks. We refer to these regions as _salient regions_, which can serve as candidates for deploying patches. Inspired by this, we employ saliency methods  to mark the salient regions of various MDE models and make a crucial observation. Saliency methods have emerged as popular tools to highlight features in an input that are closely related to the output, with gradient interpretation  serving as the foundation for many of these methods [33; 34; 35; 36]. Therefore, we employ gradient interpretation to define saliency maps \(S\) in our study:

\[S=}|(f_{M}(x))}{ \,x[i]}|}{3},\] (3)

where \(M\) here represents a binary mask, in which the designated local area is filled with 1 and others are filled with 0. \(x[i]\) is one channel of RGB. A greater value in \(S\) indicates that the predictive depth of the specified region is more responsive to the corresponding pixel in \(x\).

### Experiment Results

We use 8 state-of-the-art models as the target MDE models, including 3 CNN-based models (De-hin , Mono2 , Mande ) and 5 ViT-based models (Midas , Ada , GLPN ,DeAny , DPT ). All these models are trained on the KITTI dataset  or a hybrid dataset (consisting of various datasets). We randomly select 100 environment images from KITTI and gather obstacle images of cars, pedestrians, and roadblocks from Google Images.

We make an important finding from all generated \(S\): **all these MDE models are road-dependent when predicting depths for various objects**. Fig. 2 illustrates this phenomenon, wherein we normalize the saliency maps within \(\) and combine them with their input images by \(x+ S\) (\(=2.5\)), thereby salient regions are filled with white points. We observe that _white points consistently appear within the road areas (red boxes) close to the designated regions (blue boxes)_ despite the spatial separation between the road and the designated region (the second row in Fig. 2). This result indicates the significance of the road regions close to the designated area. Consequently, we decide to _deploy patches on the roads between the victim vehicles and obstacles strategically_.

### Analysis

Before explaining the road-dependency, we first emphasize _the significance of perspective in depth estimation_. As widely acknowledged, perspective is a fundamental technique in artistic creation to imbue 2D images with a sense of depth, wherein distant objects appear smaller while nearer ones appear larger. Based on this principle, experienced individuals, e.g., snipers, can infer the distance of objects based on their observed scales and actual scales. This is also commonly presumed to be one of the underlying reasons why MDE models can estimate depths from monocular images .

Most images from KITTI or similar datasets depict similar driving scenes wherein a lane occupies a central position, extending towards a central vanishing point, flanked by buildings, trees, or other objects along its periphery. Therefore, roads serve as natural vanishing points in these similar scenes akin to the auxiliary lines employed in artistic compositions, delineating the perspective relationships throughout the scene. All objects on the lane and those distributed along its peripheres conform to the perspective relationship delineated by the lane. So, _roads are the crucial cues to perspective_. Moreover, roads possess consistent widths across varying distances. Consequently, the width of the road can serve as a reliable reference for inferring the actual scale of different objects present on or near the road, enabling MDE models to estimate distances like an experienced sniper.

## 5 Methodology

### Design Overview

Inspired by the road-dependent property we observed, we propose a new attack named Adversarial Road Marking (AdvRM), which strategically places unobtrusive adversarial patches on the road. Fig. 3 shows the overview of AdvRM. It mainly consists of three distinct steps: \(\) patch insertion, \(\) obstacle insertion, and \(\) patch optimization. In \(\), our proposed patch insertion algorithm \(_{}\) inserts \(\) into \(b\) in a realistic manner based on four lane points (Fig. 4), which can be automatically annotated by lane detection methods . In \(\), another insertion algorithm \(_{o}\) randomly selects an obstacle image

Figure 2: Saliency maps of different MDE models. The white points in the red box indicate that the depth prediction regarding the blue box strongly responds to the corresponding pixels.

from images of three kinds of common obstacles (cars, pedestrians, and roadblocks) and inserts it into \(\), ensuring the patch applicability across diverse obstacles. Note that we can employ some predefined random image transformations (e.g., brightness, shifting, and rotation) in both \(\) and \(\) to process \(\) and \(o\) to enhance the robustness of our attack. In \(\), we simultaneously minimize \(L_{a}\) and \(L_{st}\), balancing the effectiveness and stealthiness of our attack.

### Image Synthesis

Image synthesis involves patch insertion and obstacle insertion, which requires reasonable perspective transformation and scaling for authentic synthesis. The necessary parameters for the two transformations can be calculated if both intrinsic and extrinsic camera parameters (e.g., the camera's focus length, height, and rotation) are known. Please refer to  for the details of the calculation.

We here introduce another effective way to complete authentic synthesis even if these intrinsic and extrinsic camera parameters are unclear. Our proposed insertion method relies on four key points of the central lane, as shown in Fig. 4. These key points can be annotated manually or automatically through lane detection . We also manually check the annotation results and correct obvious errors. Based on these lane points, it is easy to sketch out the insertion (see the sketch in Fig. 4) according to the specified parameters (\(d_{1}\), \(d_{2}\), and \(d_{3}\) in the sketch) using simple knowledge of analytic geometry. This sketch drives our two insertions (\(_{}\) and \(_{o}\)) to complete subsequent insertion. Specifically, \(_{}\) firsts employ a perspective function2 to transform \(\) and its mask \(m_{}\) based on the vertices of the patch region in the sketch. The coordinates of the vertices can be easily calculated based on the lane points, \(d_{1}\), and \(d_{2}\). After that, \(_{}\) pads the transformed result with 0 to fit the dimensions of the input image. Finally, \(_{}\) synthesizes \(\) by \(=b(1-M_{})+^{F} M_{}\). In the same way, \(_{o}\) inserts

Figure 4: Insertion of patch. \(d_{1}\), \(d_{2}\), and \(d_{3}\) are the distance from the upper and lower sides of the patch region and the lower edge of the object region to the top of the sketch. \(M_{}\) and \(^{F}\) are the same size as \(b\).

Figure 3: The overall pipeline of AdvRM consists of three main steps: \(\), \(\), and \(\), which correspond to patch insertion, obstacle insertion, and patch optimization, respectively. In step \(\), the patch insertion module \(_{}\) inserts the patch \(\) into the environment image based on parameters \(_{}\). Similarly, in step \(\), the object insertion module \(_{o}\) inserts the object \(o\) into the image using parameters \(_{o}\). The green dashed lines in \(\) represent the process for inserting multiple obstacles into the same image. In step \(\), the weighted sum of \(L_{a}\) and \(L_{st}\) is computed, and \(\) is updated accordingly.

into \(\) according to lane points and \(d_{3}\), producing \(\) by \(=(1-M_{o})+o^{F} M_{o}\), where \(o^{F}\) and \(M_{o}\), similar to \(^{F}\) and \(M_{}\) in Fig. 4, represent the padding results of the resized \(o\) and its mask. For multi-obstacle scenarios, we mainly consider crowds (e.g., 3 pedestrians) crossing the road and apply a random horizontal offset to each inserted pedestrian. As for cars and roadblocks, we still focus on the case of a singular obstacle as they typically appear alone within a lane.

### Robustness Enhancement

To enhance the robustness of AdvRM in the physical world, we can employ random transformations to process patches and obstacles during the insertion, which is known as Expectation of Transformation (EoT) . Like most existing attacks [1; 3], we consider common brightness adjustments (\( 0.2\)), contrast adjustments (\( 0.1\)), and saturation adjustments (\( 0.1\)). Additionally, to address the issue of road patches being obscured by fallen leaves or discarded plastic bags, we specifically introduce random pixel masking during the optimization to simulate this scenario.

### Patch Optimization

As described in Eq. (2), we design loss function \(L\) with the considerations of effectiveness and stealthiness, i.e., \(L=L_{a}+ L_{st}\). Adversarial loss \(L_{a}\) ensures the capability of our deployed patch in manipulating depth maps, while stealthiness loss \(L_{st}\) enhances stealthiness.

**Adversarial loss.** Most previous attacks [1; 2; 4; 5] focus solely on altering the depth of partial obstacle pixels within their adversarial loss, particularly those pixels overlapped by the patch. Such a design of adversarial loss results in limited effectiveness  and compromises attack stealthiness, as the depth variations within different parts of the same obstacle become conspicuously inconsistent, raising suspicion. Although Guesmi et al.  expands the affected area to the entire obstacle area using two loss terms corresponding to patch and non-patch regions, their method is not applicable here since all obstacle pixels are non-patch pixels in our attack scenario. Thus, we must revisit the design of adversarial loss.

To globally alter the depth of the obstacle and reduce the difference in depth changes across different obstacle regions, we categorize all obstacle pixels into two parts, easily affected pixels (EP) and less easily affected pixels (LEP), and pay more attention to LEP. EP and LEP are adaptively determined by the average change in the depth over all obstacle pixels. When the depth change exceeds the current average level, the corresponding pixels are regarded as EP; otherwise, the pixels are LEP. It is easy to understand that LEP should be given more attention to reduce the disparity with EP. We finally define our adversarial loss \(L_{a}\) as

\[L_{a}= L_{LEP}+L_{EP},\,\] \[L_{}=-(f_{M_{o}}() M_{}), \,\{LEP\,,EP\},\] \[M_{}=(f_{M_{o}}()<(f_{M_{o}}( x))),&=LEP\\ (f_{M_{o}}()(f_{M_{o}}(x))),&=EP\\ ,\] (4) \[=(1.14,(}()}{f_{ M_{o}}(x)})).\]

We have \(>1\), and \(\) is an index function and returns a mask filled with 0 and 1 according to the given condition. \(M_{LEP}\) and \(M_{EP}\) respectively identify LEP and EP. \( 1.14\) where 1.14 is the minimum acceptable attack effectiveness of increased depth. Note that, we do not specify a target depth map in \(L_{a}\) as in previous studies [1; 2; 3], opting instead to maximize the obstacle depth as much as possible due to our intention to explore the maximal efficacy of our new road patch.

**Stealthiness loss.** We consider attack stealthiness from two aspects: natural patch appearance and no noticeable change in the depth of the patched road area. To ensure the optimized patch looks like an ordinary road marking, we refer to  to build an appearance loss \(L_{ap}\) using a deep photo style transfer proposed in . \(L_{ap}\) consists of four terms, i.e., \(L_{ap}=L_{s}+L_{c}+L_{t}+L_{r}\), where \(L_{s}\), \(L_{c}\), \(L_{t}\), and \(L_{r}\) represent style loss, content loss, smoothness loss, and photorealism regularization loss, respectively. Please refer to Appendix for a detailed explanation of the four terms. On the other hand, a significant change in the depth of the patched road area will directly expose the existence of the malicious patch. Therefore, we employ another loss \(L_{ma}=(|f_{M_{}}()-f_{M_{}}(b)|)\) to maintain the depth of the patch area. Ultimately, our stealthiness loss \(L_{st}\) is expressed as

\[L_{st}=L_{ap}+ L_{ma},( 0).\] (5)

## 6 Experiments

### Setup

**Dataset.** We randomly sample 100 images from KITTI  with different scenarios, which remains consistent with the previous study . We manually annotate the lane points within these environment images to realize image synthesis. Additionally, we collect a total of 150 obstacle images of cars (CA), pedestrians (PE), and roadblocks (RO). We randomly split them into a training set (90 images) for optimizing patches and a test set (60 images) for evaluation. All synthesized input images are finally resized to a size of \(320 1024\).

**Implementation.** We align the patch's upper boundary with the obstacle's lower boundary, whose vertical positions are 230 ( the coordinate origin at the upper left corner of \(\)). In KITTI, this height corresponds to an approximate distance of 12 meters (m), the stopping distance requisite for a speed of 50 km/h , which is frequently employed in urban driving. We set the patch height to 70 pixels within \(\), roughly equivalent to 4.5 m in the real world, while its width is set to the lane width. We set \(=100\) and \(=0.01\) when the target models are Dehin, Mono2, Mande, and DPT, and \(=50\) and \(=0.02\) otherwise. We set \(=2\) for all MDE models. We choose BIM  to update \(\) with step size 0.01. The maximum number of iterations is 1000 when the target models are Midas and DPT; and 500 otherwise. All experiments are performed on a single GPU NVIDIA GeForce RTX 4090.

**Evaluation metrics.** All the selected models output relative depth maps instead of absolute depth maps. The scale of relative depth values varies heavily across different models. To address the scale problem and obtain comparable results, we design the Mean Relative Shift Ratio (MRSR) \(_{r}\) to measure the change in the obstacle depth before and after attacks. \(_{r}\) is defined as

\[_{r}=(f_{M_{o}}()-f_{M_{o}}(x))}{(f_{M_{o} }(x))}.\] (6)

A positive \(_{r}\) means a farther predictive distance after attacks, while a negative \(_{r}\) indicates a closer depth. In our investigation, larger values of \(_{r}\) correspond to better attack performance. We also use Affect Region Ratio (ARR) , denoted as \(_{a}\), to measure the ratio of obstacle pixels whose \(_{r}\) exceeds a designated threshold. \(_{a}\) is defined as

\[_{a}=((f_{M_{o}}()>(f_{M_{o}}(x )_{0})))}{(M_{o})}.\] (7)

When \(_{a}\) approaches 1, we consider the attack's impact on obstacles to be global. Conversely, a low \(_{a}\) indicates a local depth alteration. We set \(_{0}=1.14\).

### Dataset Simulation

**Effectiveness.**

Table 2 shows that AdvRM pose a significant security threat to these mainstream MDE models as it achieves high \(_{r}\) and \(_{a}\). Particularly, _the average \(_{r}\) over all models is 1.507, indicating that an obstacle located at 12 m will be considered to be at 30 m, which is enough to delay braking and cause a serious collision._ Fig. 5 confirms that AdvRM balances effectiveness and stealthiness well, where darker colors in the depth maps denote shorter distances while lighter colors correspond to

   &  &  &  \\   & & Dehin & Mono2 & Mande & Midas & Ada & GLPN & DeAny & DPT \\  \)} & PE & 1.319 & 2.431 & 0.977 & 0.329 & 2.151 & 0.649 & 0.518 & 5.589 \\  & CA & 0.941 & 1.868 & 0.583 & 0.240 & 1.008 & 0.245 & 0.469 & 3.562 \\  & RO & 1.108 & 3.157 & 1.211 & 0.370 & 2.334 & 0.300 & 0.505 & 4.314 \\  & Average & 1.123 & 2.485 & 0.924 & 0.313 & 1.831 & 0.398 & 0.497 & 4.488 \\  \)} & PE & 0.954 & 0.960 & 0.954 & 0.817 & 0.999 & 0.918 & 0.946 & 0.999 \\  & CA & 0.948 & 0.969 & 0.873 & 0.729 & 0.998 & 0.793 & 0.984 & 0.998 \\   & RO & 0.929 & 0.999 & 0.997 & 0.953 & 1.000 & 0.805 & 0.989 & 1.000 \\   & Average & 0.944 & 0.976 & 0.942 & 0.833 & 0.999 & 0.839 & 0.973 & 0.999 \\  

Table 2: Performance of AdvRM when attacking different MDE modelslonger distances. Meanwhile, AdvRM also performs well in situations of multiple obstacles because it makes the predicted depths of the three pedestrians farther simultaneously in Fig. 5.

**Robustness.** Fig. 6(a) reports the relative increments (%) in MRSR when employing EoT compared to its absence. The relative increment is defined as \(-_{r}^{w}}{_{w}} 100\%\), where \(_{r}^{w}\) and \(_{r}^{w}\) denote the MRSR values of AdvRM when operating with and without EoT. All these increments confirm that EoT makes AdvRM more robust. Particularly, Fig. 6(b) shows that the optimized patches remain effective even when subjected to random masking, such as being partially covered by leaves or plastic bags (Fig. 6(c)).

**Obstacle transferability.** In this evaluation, we choose obstacles from one known category for patch generation, while the remaining two categories are unknown. Table 3 reports \(_{r}\) tested on known and unknown obstacles, confirming that AdvRM possesses a high transferability across obstacles. Therefore, _our patches are capable of affecting whatever obstacles appear in front of it_.

### Real-world Experiments

We also conduct physical experiments to demonstrate the robustness of AdvRM using printed roads, printed patches, and car models. Due to the limitations of testing environments, such a physical simulation is common in previous works .

**Sizes.** In the real world, a typical road width is 3.5 m, and the size of a Toyota Land Cruiser, a sport utility vehicle, is \(4.95\) m \( 1.97\) m \( 1.905\) m corresponding to length, width, and height. In the physical simulation, we scale these dimensions to approximately 1:50. Specifically, the printed lane width is scaled to 0.07 meters, with the patch dimensions measuring 0.06 meters in height and 0.07 meters in width. The dimensions of the scaled-down car model are \(0.087\) m \(\)\(0.035\) m \(\)\(0.034\) m.

**Results.** In this evaluation, we adopt LBFGS  to update the patch and use Mono2  as the target model. As shown in Fig. 7, the difference between the farthest and closest distance is 0.06 m, corresponding to 3 m in the real world, and printed patch works for all frames, achieving an average \(_{r}\) of 1.088. Note that, the black stripes in the patch are only for ease of printing and pasting, containing no adversarial perturbations.

   &  &  &  &  \\   & PE & CA & RO & PE & CA & RO & PE & CA & RO & PE & CA & RO \\  PE & 1.178 & 0.330 & 0.419 & 3.174 & 1.312 & 6.262 & 1.426 & 0.362 & 0.660 & 0.320 & 0.180 & 0.263 \\ CA & 0.513 & 0.657 & 0.551 & 1.909 & 2.442 & 3.284 & 0.728 & 0.650 & 0.975 & 0.295 & 0.242 & 0.325 \\ RO & 0.930 & 0.732 & 1.030 & 2.694 & 2.204 & 4.559 & 1.184 & 0.680 & 1.617 & 0.296 & 0.237 & 0.337 \\    &  &  &  &  \\   & PE & CA & RO & PE & CA & RO & PE & CA & RO & PE & CA & RO \\  PE & 3.366 & 1.000 & 2.072 & 1.188 & 0.057 & -0.011 & 0.471 & 0.419 & 0.461 & 6.954 & 1.460 & 3.219 \\ CA & 1.342 & 1.196 & 2.004 & 0.306 & 0.367 & 0.125 & 0.429 & 0.471 & 0.454 & 3.509 & 3.856 & 3.534 \\ RO & 1.903 & 1.072 & 2.646 & 0.320 & 0.226 & 0.457 & 0.191 & 0.293 & 0.452 & 4.216 & 2.593 & 5.221 \\  

Table 3: Transferability of AdvRM across obstacles.

Figure 5: Patches on the road successfully change the predictive depth for different categories of obstacles. The color of the obstacle regions in \(f()\) becomes lighter than that in \(f(x)\), indicating larger predictive depths in \(f()\).

## 7 Limitations

Similar to existing works [5; 14; 15], AdvRM is also limited to white-box scenarios. In Table 4, we generate patches on surrogate models (e.g., DPT ) and use three state-of-the-art transferability enhancement methods, i.e., model ensemble  (ENS), gradient skipping  (GS), and gradient regularization  (GR), to improve the attack effectiveness on unknown victim models. However, it does not show obvious transferability across models. Indeed, the transferability across MDE models remains an open problem, which will be a focal point for future research. Besides, AdvRM is currently only applicable to road scenarios. It is unclear whether the MDE models trained for other scenes (e.g., indoor) have similar road-dependent characteristics to support the decoupling of patches and obstacles.

## 8 Conclusion

We present a pioneering patch attack against MDE models that decouples adversarial patches from specified obstacles physically to broaden the applicability of attacks. This design is inspired by our crucial finding that current MDE models are road-dependent when predicting depths for obstacles. Based on this finding, we propose a new adversarial attack that places patches on the road between the vehicle and obstacles and disguises it as an ordinary road marking for high stealthiness. Experimental results from both dataset simulation and the physical world demonstrate AdvRM poses a serious threat to various MDE models as it significantly alters the depth predictions across different obstacle categories. Thus, we call on the community to pay more attention to the security issues of MDE models and actively propose measures to improve the robustness of MDE models.

## 9 Acknowledgements

We thank the anonymous reviewers for their valuable feedback. This work was supported in part by Nanyang Technological University (NTU)-DESAY SV Research Program under Grant 2018-0980,

   Obstacle & AdvRM & AdvRM & AdvRM & AdvRM & AdvRM \\  FE & 0.009 & 0.011 & 0.011 & 0.006 \\ CA & -0.023 & -0.014 & -0.018 & -0.023 \\ RO & -0.067 & -0.064 & -0.016 & -0.041 \\   

Table 4: Transferability across models measured by \(_{r}\). Values close to 0 indicate negligible model transferability.

Figure 6: EoT enhances the robustness of AdvRM. (a) Increment on \(_{r}\) after adopting EoT. (b) Average \(_{r}\) of AdvRM when facing different numbers of covers, where 10L means 10 leaves, and 2P means 2 plastic bags. (c) Illustration of partial cover caused by leaves and plastic bags.

Figure 7: Testing results in the physical world. Our optimized patch simultaneously increases the predicted depth of the white car in all frames.

National Research Foundation, Singapore and DSO National Laboratories under its AI Singapore Programme (AISG Award No: AISG2-GC-2023-008), National Natural Science Foundation of China under Grant 62072062, and Natural Science Foundation of Chongqing, China, under Grant cstc2022ycjh-bgzxm0031.