# Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks

Blake Bordelon & Cengiz Pehlevan

John Paulson School of Engineering and Applied Sciences,

Center for Brain Science,

Kempner Institute for the Study of Natural & Artificial Intelligence,

Harvard University

Cambridge MA, 02138

blake_bordelon@g.harvard.edu, cpehlevan@g.harvard.edu

###### Abstract

We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Starting from a dynamical mean field theory description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the \((1/})\) fluctuations of the DMFT order parameters over random initializations of the network weights. Our results, while perturbative in width, unlike prior analyses, are non-perturbative in the strength of feature learning. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with a variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final tangent kernel and final network predictions. We also show how initialization variance can slow down online learning in wide but finite networks. In deeper networks, kernel variance can dramatically accumulate through subsequent layers at large feature learning strengths, but feature learning continues to improve the signal-to-noise ratio of the feature kernels. In discrete time, we demonstrate that large learning rate phenomena such as edge of stability effects can be well captured by infinite width dynamics and that initialization variance can decrease dynamically. For CNNs trained on CIFAR-10, we empirically find significant corrections to both the bias and variance of network dynamics due to finite width.

## 1 Introduction

Learning dynamics of deep neural networks are challenging to analyze and understand theoretically, but recent progress has been made by studying the idealization of infinite-width networks. Two types of infinite-width limits have been especially fruitful. First, the kernel or lazy infinite-width limit, which arises in the standard or neural tangent kernel (NTK) parameterization, gives prediction dynamics which correspond to a linear model [1; 2; 3; 4; 5]. This limit is theoretically tractable but fails to capture adaptation of internal features in the neural network, which are thought to be crucial to the success of deep learning in practice. Alternatively, the mean field or \(\)-parameterization allows feature learning at infinite width [6; 7; 8; 9].

With a set of well-defined infinite-width limits, prior theoretical works have analyzed finite networks in the NTK parameterization perturbatively, revealing that finite width both enhances the amount of feature evolution (which is still small in this limit) but also introduces variance in the kernels and the predictions over random initializations [10; 11; 12; 13; 14; 15]. Because of these competing effects, in some situations wider networks are better, and in others wider networks perform worse .

In this paper, we analyze finite-width network learning dynamics in the mean field parameterization. In this parameterization, wide networks are empirically observed to outperform narrow networks [7; 17; 18]. Our results and framework provide a methodology for reasoning about detrimental finite-size effects in such feature-learning neural networks. We show that observable averages involving kernels and predictions obey a well-defined power series in inverse width even in rich training regimes. We generally observe that the leading finite-size corrections to both the bias and variance components of the square loss are increased for narrower networks, and diminish performance. Further, we show that richer networks are closer to their corresponding infinite-width mean field limit. For simple tasks and architectures the leading \((1/)\) corrections to the error can be descriptive, while for large sample size or more realistic tasks, higher order corrections appear to become relevant. Concretely, our contributions are listed below:

1. Starting from a dynamical mean field theory (DMFT) description of infinite-width nonlinear deep neural network training dynamics, we provide a complete recipe for computing fluctuation dynamics of DMFT order parameters over random network initializations during training. These include the variance of the training and test predictions and the \((1/)\) variance of feature and gradient kernels throughout training.
2. We first solve these equations for the lazy limit, where no feature learning occurs, recovering a simple differential equation which describes how prediction variance evolves during learning.
3. We solve for variance in the rich feature learning regime in two-layer networks and deep linear networks. We show richer nonlinear dynamics improve the signal-to-noise ratio (SNR) of kernels and predictions, leading to closer agreement with infinite-width mean field behavior.
4. We analyze in a two-layer model why larger training set sizes in the overparameterized regime enhance finite-width effects and how richer training can reduce this effect.
5. We show that large learning rate effects such as edge-of-stability [19; 20; 21] dynamics can be well captured by infinite width theory, with finite size variance accurately predicted by our theory.
6. We test our predictions in Convolutional Neural Networks (CNNs) trained on CIFAR-10 . We observe that wider networks and richly trained networks have lower logit variance as predicted. However, the timescale of training dynamics is significantly altered by finite width even after ensembling. We argue that this is due to a detrimental correction to the mean dynamical NTK.

### Related Works

Infinite-width networks at initialization converge to a Gaussian process with a covariance kernel that is computed with a layerwise recursion [23; 24; 25; 26; 13]. In the large but finite width limit, these kernels do not concentrate at each layer, but rather propagate finite-size corrections forward through the network [27; 28; 29; 30; 14]. During gradient-based training with the NTK parameterization, a hierarchy of differential equations have been utilized to compute small feature learning corrections to the kernel through training [10; 11; 12; 13]. However the higher order tensors required to compute the theory are initialization dependent, and the theory breaks down for sufficiently rich feature learning dynamics. Various works on Bayesian deep networks have also considered fluctuations and perturbations in the kernels at finite width during inference [31; 32]. Other relevant work in this domain are [33; 34; 35; 36; 37; 38; 39].

An alternative to standard/NTK parameterization is the mean field or \( P\)-limit where features evolve even at infinite width [6; 7; 8; 9; 40; 41; 42]. Recent studies on two-layer mean field networks trained online with Gaussian data have revealed that finite networks have larger sensitivity to SGD noise [43; 44]. Here, we examine how finite-width neural networks are sensitive to initialization noise. Prior work has studied how the weight space distribution and predictions converge to mean field dynamics with a dynamical error \((1/})\)[40; 45], however in the deep case this requires a probability distribution over couplings between adjacent layers. Our analysis, by contrast, focuses on a function and kernel space picture which decouples interactions between layers at infinite width. A starting point for our present analysis of finite-width effects was a previous set of studies [9; 46] which identified the DMFT action corresponding to randomly initialized deep NNs which generates the distribution over kernel and network prediction dynamics. These prior works discuss the possibility of using a finite-size perturbation series but crucially failed to recognize the role of the network prediction fluctuations on the kernel fluctuations which are necessary to close the self-consistent equations in the rich regime. Using the mean field action to calculate a perturbation expansion around DMFT is a long celebrated technique to obtain finite size corrections in physics [47; 48; 49; 50] and has been utilized for random untrained recurrent networks [51; 52], and more recently to calculate variance of feature kernels \(^{}\) at initialization \(t=0\) in deep MLPs or RNNs . We extend these prior studies to the dynamics of training and to probe how feature learning alters finite size corrections.

## 2 Problem Setup

We consider wide neural networks where the number of neurons (or channels for a CNN) \(N\) in each layer is large. For a multi-layer perceptron (MLP), the network is defined as a map from input \(_{}^{D}\) to hidden preactivations \(_{}^{}^{N}\) in layers \(\{1,...,L\}\) and finally output \(f_{}\)

\[f_{}=^{L}(_{}^{L})\,_{}^{+1}=}^{}(_{}^{})\,_{}^{1}=}^{0}_{ },\] (1)

where \(\) is a scale factor that controls feature learning strength, with large \(\) leading to rich feature learning dynamics and the limit of small \( 0\) (or generally if \(\) scales as \(N^{-}\) for \(>0\) as \(N\), NTK parameterization corresponds to \(=\)) gives lazy learning where no features are learned [4; 7; 9]. The parameters \(=\{^{0},^{1},...,^{L}\}\) are optimized with gradient descent or gradient flow \(=-N^{2}_{}\) where \(=_{_{}}\ (f(_{},),y_{})\) is a loss computed over dataset \(=\{(_{1},y_{1}),(_{2},y_{2}),(_{P},y_{P})\}\). This parameterization and learning rate scaling ensures that \(f_{}_{N,}(1)\) and \(_{}^{}=_{N,}()\) at initialization. This is equivalent to maximal update parameterization (\(\)P), which can be easily extended to other architectures including neural networks with trainable bias parameters as well as convolutional, recurrent, and attention layers [8; 9].

## 3 Review of Dynamical Mean Field Theory

The infinite-width training dynamics of feature learning neural networks was described by a DMFT in [9; 46]. We first review the DMFT's key concepts, before extending it to get insight into finite-widths. To arrive at the DMFT, one first notes that the training dynamics of such networks can be rewritten in terms of a collection of dynamical variables (or _order parameters_) \(=\{f_{}(t),_{}^{}(t,s),G_{}^{}(t,s ),...\}\), which include feature and gradient kernels [9; 54]

\[_{}^{}(t,s)(_{}^{}(t)) (_{}^{}(s))\, G_{}^{}(t,s)_{}^{}(t)_{}^{}(s),\] (2)

where \(_{}^{}(t)= N(t)}{_{ }^{}(t)}\) are the back-propagated gradient signals. Further, for width-\(N\) networks the distribution of these dynamical variables across weight initializations (from a Gaussian distribution \((0,)\)) is given by \(p()))}\), where the action \(S()\) contains interactions between neuron activations and the kernels at each layer .

The DMFT introduced in  arises in the \(N\) limit when \(p()\) is strongly peaked around the saddle point \(_{}\) where \(}|_{_{}}=0\). Analysis of the saddle point equations reveal that the training dynamics of the neural network can be alternatively described by a stochastic process. A key feature of this process is that it describes the training time evolution of the distribution of neuron pre-activations in each layer (informally the histogram of the elements of \(_{}^{}(t)\)) where each neuron's pre-activation behaves as an i.i.d. draw from this _single-site_ stochastic process. We denote these random processes by \(h_{}^{}(t)\). Kernels in (2) are now computed as _averages_ over these infinite-width single site processes \(_{}^{}(t,s)=(h_{}^{}(t))(h_{}^{ }(s))\), \(G_{}^{}(t,s)= g_{}^{}(t)g_{}^{}(s)\), where averages arise from the \(N\) limit of the dot products in (2). DMFT also provides a set of self-consistent equations that describe the complete statistics of these random processes, which depend on the kernels, as well as other quantities. To make our notation and terminology clearer for a machine learning audience, we provide Table 1 for a definition of the physics terminology in machine learning language.

## 4 Dynamical Fluctuations Around Mean Field Theory

We are interested in going beyond the infinite-width limit to study more realistic finite-width networks. In this regime, the order parameters \(\) fluctuate in a \((1/)\) neighborhood of \(_{}\)[55; 51; 53; 46].

  Order params. \(\) & Action \(S()\) & Propagator \(\) & Single Site Density \\  Concentrating variables & \(\)’s log-density & Asymptotic Covariance & Neuron Marginals \\  

Table 1: Relationship between the physics and ML terminology for the central objects in this paper. The \(\) which concentrate at infinite width, but fluctuate at finite width \(N\). This paper is primarily interested in \(\), the asymptotic covariance of the order parameters.

Statistics of these fluctuations can be calculated from a general cumulant expansion (see App. D) . We will focus on the leading-order corrections to the infinite-width limit in this expansion.

**Proposition 1**: _The finite-width \(N\) average of observable \(O()\) across initializations, which we denote by \( O()_{N}\), admits an expansion of the form whose leading terms are_

\[ O()_{N}=(NS[] )O()}{ d(NS[])}= O( {q})_{}+N[ V()O() _{}- V()_{} O() _{}]+...,\] (3)

_where \(_{}\) denotes an average over the Gaussian distribution \((_{},-(_{} ^{2}S[_{}])^{-1})\) and the function \(V() S()-S(_{})-(-_{ })^{}_{}^{2}S(_{})(-_{})\) contains cubic and higher terms in the Taylor expansion of \(S\) around \(_{}\). The terms shown include all the leading and sub-leading terms in the series in powers of \(1/N\). The terms in ellipses are at least \((N^{-1})\) suppressed compared to the terms provided._

The proof of this statement is given in App. D. The central object to characterize finite size effects is the unperturbed covariance (the _propagator_): \(=-[^{2}S(_{})]^{-1}\). This object can be shown to capture leading order fluctuation statistics \((-_{})(-_{} )^{}_{N}=+(N^{-2})\) (App. D.1), which can be used to reason about, for example, expected square error over random initializations. Correction terms at finite width may give a possible explanation of the superior performance of wide networks at fixed \(\). To calculate such corrections, in App. E, we provide a complete description of Hessian \(_{}^{2}S()\) and its inverse (the propagator) for a depth-\(L\) network. This description constitutes one of our main results. The resulting expressions are lengthy and are left to App. E. Here, we discuss them at a high level. Conceptually there are two primary ingredients for obtaining the full propagator:

* Hessian sub-blocks \(\) which describe the _uncoupled variances_ of the kernels, such as \[_{}^{^{}}(t,s,t^{},s^{}) (h_{}^{}(t))(h_{}^{}(s))(h_{}^{ }(t^{}))(h_{}^{}(s^{}))-_{ }^{}(t,s)_{}^{}(t^{},s^{})\] (4) Similar terms also appear in other studies on finite width Bayesian inference  and in studies on kernel variance at initialization .
* Blocks which capture the _sensitivity_ of field averages to perturbations of order parameters, such as \[D_{}^{^{}q^{-1}}(t,s,t^{},s^{}) ^{}(t))(h_{}^{}(s)) }{_{}^{-1}(t^{},s^{})}\, D_{ }^{G^{}}(t,s,t^{})^{}(t)g_{}^{}(s)}{_{}(t^{ })},\] (5) where \(_{}(t)=-,g_{})}{ f_{}}|_{f_{ }(t)}\) are error signal for each data point.

Abstractly, we can consider the uncoupled variances \(\) as "sources" of finite-width noise for each order parameter and the \(\) blocks as summarizing a directed causal graph which captures how this noise propagates in the network (through layers and network predictions). In Figure 1, we illustrate this graph showing directed lines that represent causal influences of order parameters on fields and vice versa. For instance, if \(^{}\) were perturbed, \(D^{^{+1},^{}}\) would quantify the resulting perturbation to \(^{+1}\) through the fields \(h^{+1}\).

In App. E, we calculate \(\) and \(\) tensors, and show how to use them to calculate the propagator. As an example of our results:

**Proposition 2**: _Partition \(\) into primal \(_{1}=\{f_{}(t),_{}^{}(t,s)...\}\) and conjugate variables \(_{2}=\{_{}(t),_{}^{}(t,s)...\}\). Let \(=}{_{2}_{2}^{2}}S[_{1},_{2}]\) and \(=}{_{2}_{1}^{2}}S[_{1},_{2}]\), then the propagator for \(_{1}\) has the form \(_{_{1}}=^{-1}[^{-1}]^{}\) (App E). The variables \(_{1}\) are related to network observables, while conjugates \(_{2}\) arise as Lagrange multipliers in the DMFT calculation. From the propagator \(_{_{1}}\) we can read off the variance of network observables such as \(N(f_{})_{f_{}}\)._

Figure 1: The directed causal graph between DMFT order parameters (blue) and fields (green) defines the \(D\) tensors of our theory. Each arrow represents a causal dependence. \(K\) denotes the NTK.

The necessary order parameters for calculating the fluctuations are obtained by solving the DMFT using numerical methods introduced in . We provide a pseudocode for this procedure in App. F. We proceed to solve the equations defining \(\) in special cases which are illuminating and numerically feasible including lazy training, two layer networks and deep linear NNs.

## 5 Lazy Training Limit

To gain some initial intuition about why kernel fluctuations alter learning dynamics, we first analyze the static kernel limit \( 0\) where features are frozen. To prevent divergence of the network in this limit, we use a background subtracted function \((,)=f(,)-f(,_{0})\) which is identically zero at initialization . For mean square error, the \(N\) and \( 0\) limit is governed by \(()}{ t}=_{^{} }(^{})K(,^{})\) with \(()=y()-()\) (for MSE) and \(K\) is the static (finite width and random) NTK. The finite-\(N\) initial covariance of the NTK has been analyzed in prior works [27; 13; 14], which reveal a dependence on depth and nonlinearity. Since the NTK is static in the \( 0\) limit, it has constant initialization variance through training. Further, all sensitivity blocks of the Hessian involving the kernels and the prediction errors \(\) (such as the \(D^{^{},}\)) vanish. We represent the covariance of the NTK as \((_{1},_{2},_{2},_{3})=N(K(_{1}, _{2}),K(_{3},_{4}))\). To identify the dynamics of the error \(\) covariance, we relate \(K\), the finite width NTK, to \(K_{}\) which is the deterministic infinite width NTK \(K_{}\). We consider the eigendecomposition of the infinite-width NTK \(K_{}(,^{})=_{k}_{k}_{k}()_{k} (^{})\) with respect to the training distribution \(\), and decompose \(\) in this basis.

\[_{k mn}=(_{1},_{2},_{3},_{4}) _{k}(_{1})_{}(_{2})_{n}(_{3})_{m}(_{4})_{_{1},_{2},_{3},_{4}}\,,\] (6)

where averages are computed over the training distribution \(\).

**Proposition 3**: _For MSE loss, the prediction error covariance \(^{}(t,s)=N_{0}((t),(s))\) satisfies a differential equation (App. H)_

\[(+_{k})(+_{})^{}_{k}(t,s)=_{nm}_ {km n}^{}_{m}(t)^{}_{n}(s),\] (7)

_where \(^{}_{k}(t)(-_{k}t)_{k }()y()_{}\) are the errors at infinite width for eigenmode \(k\)._

An example verifying these dynamics is provided in Figure 2. In the case where the target is an eigenfunction \(y=_{k^{*}}\), the covariance has the form \(^{}_{k}(t,s)=_{k k^{*}k^{*}}}(t+s))}{(_{k}-_{k^{*}})(_{k}-_{k^{*}})}\). If the kernel is rank one with eigenvalue \(\), then the dynamics have the simple form \(^{}(t,s)= y^{2}\ t\ s\ e^{-(t+s)}\). We note that similar terms appear in the prediction dynamics obtained by truncating the Neural Tangent Hierarchy [10; 11], however those dynamics concerned

Figure 2: We show the accuracy of the lazy-limit ODE in equation (where) compared to a two-layer finite width \(N=100\) ReLU network trained with \(=0.05\) on \(P=10\) random training data points. (a) The average dynamics over an ensemble of \(E=500\) networks (solid colors) compared to the infinite width predictions (dashed black). (b) The predicted finite size variance for each eigenmode of the error \(_{k}(t)=(t)_{k}\). These are not ordered simply by magnitude of eigenvalues or the target projections \(y_{k}=_{k}\), but rather depend on all eigenvalue gaps \(_{k}-_{}\) for \(k\) and also the \(_{k nm}\) tensor. (c) The total variance for all training points \(N_{}_{}(t)=N_{k}_{k}(t)\) is also well predicted by the DMFT propagator equations.

corrections rather than from initialization variance (App. H.1). Corrections to the mean \(\) are analyzed in App. H.2. We find that the variance and mean correction dynamics involves non-trivial coupling across eigendirections with a mixture of exponentials with timescales \(\{_{k}^{-1}\}\).

## 6 Rich Regime in Two-Layer Networks

In this section, we analyze how feature learning alters the variance through training. We show a denoising effect where the signal to noise ratios of the order parameters improve with feature learning.

### Kernel and Error Coupled Fluctuations on Single Training Example

In the rich regime, the kernel evolves over time but inherits fluctuations from the training errors \(\). To gain insight, we first study a simplified setting where the data distribution is a single training example \(\) and single test point \(}\) in a two layer network. We will track \((t)=y-f(,t)\) and the test prediction \(f_{}(t)=f(_{},t)\). To identify the dynamics of these predictions we need the NTK \(K(t)\) on the train point, as well as the train-test NTK \(K_{}(t)\). In this case, all order parameters can be viewed as scalar functions of a single time index (unlike the deep network case, see App. E).

**Proposition 4**: _Computing the Hessian of the DMFT action and inverting (App. I), we obtain the following covariance for \(_{1}=\{(t),f_{}(t),K(t),K_{}(t)\}_{t_{+}}\)._

\[_{_{1}}=+_{K}&0&_{}&0\\ -_{K},&&0&-_{}\\ -&0&&0\\ -_{}&0&0&^{-1}0&0&0&0\\ 0&0&0&0\\ 0&0&&_{}^{}\\ 0&0&_{}&_{} +_{K}&0&_{}&0\\ -_{K_{}}&&0&-_{}\\ -_{}&0&&0\\ -_{}&0&0&^{-1},\] (8)

_where \([_{K}](t,s)=(t-s)K(s)\), \([_{}](t,s)=(t-s)(s)\) are Heaviside step functions and \(D(t,s)=((h(t))^{2}+g(t)^{2})\) and \(D_{}(t,s)=((h(t))( h_{}(t))+g(t)g_{}(t))\)_

Figure 3: An ensemble of \(E=1000\) two layer \(N=256\) tanh networks trained on a single training point. Dashed black lines are DMFT predictions. (a) The square deviation from the infinite width DMFT scales as \((1/N)\) for all order parameters. (b) The ensemble average NTK \( K(t)\) (solid colors) and (c) ensemble average test point predictions \(f_{}(t)\) for a point with \(-_{s}}{D}=0.5\) closely follow the infinite width predictions (dashed black). (d) The variance (estimated the ensemble) of the train error \((t)=y-f(t)\) initially increases and then decreases as the training point is fit. (e) The variance of \(f_{}\) increases with time but decreases with \(\). (f) The variance of the NTK during feature learning experiences a transient increase before decreasing to a lower value.

quantify sensitivity of the kernel to perturbations in the error signal \((s)\). Lastly \(\) and \(_{}\) are the uncoupled variances of \(K(t)\) and \(K_{}(t)\) and \(_{}\) is the uncoupled covariance of \(K(t),K_{}(t)\)._

In Fig. 3, we plot the resulting theory (diagonal blocks of \(_{_{1}}\) from Equation 8) for two layer neural networks. As predicted by theory, all average squared deviations from the infinite width DMFT scale as \((N^{-1})\). Similarly, the average kernels \( K\) and test predictions \( f_{}\) change by a larger amount for larger \(\) (equation (79)). The experimental variances also match the theory quite accurately. The variance of the train error \((t)\) peaks earlier and at a lower value for richer training, but all variances go to zero at late time as the model approaches the interpolation condition \(=0\). As \( 0\) the curve approaches \(N\)\(((t))\ y^{2}\ t^{2}\ e^{-2t}\), where \(\) is the initial NTK variance (see Section 5). While the train prediction variance goes to zero, the test point prediction does not, with richer networks reaching a lower asymptotic variance. We suspect this dynamical effect could explain lower variance observed in feature learning networks compared to lazy networks [7; 18]. In Fig. A.1, we show that the reduction in variance is not due to a reduction in the uncoupled variance \((t,s)\), which increases in \(\). Rather the reduction in variance is driven by the coupling of perturbations across time.

### Offline Training with Multiple Samples or Online Training in High Dimension

In this section we go beyond the single sample equations of the prior section and explore training with multiple \(P\) examples. In this case, we have training errors \(\{_{}(t)\}_{=1}^{P}\) and multiple kernel entries \(K_{}(t)\) (App. E). Each of the errors \(_{}(t)\) receives a \((N^{-1/2})\) fluctuation, the training error \(_{}_{}^{2}\) has an additional variance on the order of \(()\). In the case of two-layer linear

Figure 4: Large input dimension or multiple samples amplify finite size effects in a simple two layer model with unstructured data. Black dashed lines are theory. (a) The variance of offline learning with \(P\) training examples in a two layer linear network. (b) The leading perturbative approximation to the train error breaks down when samples \(P\) becomes comparable to \(N\). (c)-(d) Richer training reduces variance. (e)-(f) Online learning in a depth 2 linear network has identical dynamics and finite width fluctuations, but with predictor variance \( D/N\) for input dimension \(D\) (Appendix K).

networks trained on whitened data (\(_{}_{}=_{}\)), the equations for the propagator simplify and one can separately solve for the variance of \((t)^{P}\) along signal direction \(^{P}\) and along each of the \(P-1\) orthogonal directions (App. J). At infinite width, the task-orthogonal component \(_{}\) vanishes and only the signal dimension \(_{y}(t)\) evolves in time with differential equation [9; 46]

\[_{y}(t)=2(y-_{y}(t))^{2}}\;_{y }(t)\;,\;_{}(t)=0.\] (9)

However, at finite width, both the \(_{y}(t)\) and the \(P-1\) orthogonal variables \(_{}\) inherit initialization variance, which we represent as \(_{_{y}}(t,s)\) and \(_{}(t,s)\). In Fig. 4 (a)-(b) we show this approximate solution \(|(t)|^{2}_{y}(t)^{2}+ {N}_{y}^{1}(t)_{y}(t)+_{_{y}}(t,t)+_{}(t,t)+(N^{-2})\) across varying \(\) and varying \(P\) (see Appendix J for \(_{_{y}}\) and \(_{}\) formulas). We see that variance of train point predictions \(f_{}(t)\) increases with the total number of points despite the signal of the target vector \(_{}y_{}^{2}\) being fixed. In this model, the bias correction \(_{y}^{1}(t)_{y}(t)\) is always \((1/N)\) but the variance correction is \((P/N)\). The fluctuations along the \(P-1\) orthogonal directions begin to dominate the variance at large \(P\). Fig. 4 (b) shows that as \(P\) increases, the leading order approximation breaks down as higher order terms become relevant. Analysis for online training reveals identical fluctuation statistics, but with variance that scales as \( D/N\) (Appendix K) as we verify in Figure 4 (e)-(f).

## 7 Deep Networks

In networks deeper than two layers, the DMFT propagator has complicated dependence on non-diagonal (in time) entries of the feature kernels (see App. E). This leads to Hessian blocks with four time and four sample indices such as \(D_{}^{^{}}(t,s,t^{},s^{})=^{-1}(t^{},s^{})} (h_{}^{}(t))(h_{}^{}(s))\), rendering any numerical calculation challenging. However, in deep linear networks trained on whitened data, we can exploit the symmetry in sample space and the Gaussianity of reactivation features to exactly compute derivatives without Monte Carlo sampling as we discuss in App. L. An example set of results for a depth \(4\) network is provided in Fig. 5. The variance for the feature kernels \(H^{}\) accumulate finite size variance by layer along the forward pass and the gradient kernels \(G^{}\) accumulate variance on the backward pass. The SNR of the kernels \(}{N(H)}\) improves with feature learning, suggesting that richer networks will be better modeled by their mean field limits. Examples of the off-diagonal correlations obtained from the propagator are provided in App. Fig. A.3.

## 8 Variance can be Small Near Edge of Stability

In this section, we move beyond the gradient flow formalism and ask what large step sizes do to finite size effects. Recent studies have identified that networks trained at large learning rates can be qualitatively different than networks in the gradient flow regime, including the catapult  and edge of stability (EOS) phenomena [19; 20; 21]. In these settings, the kernel undergoes an initial scale

Figure 5: Depth 4 linear network with single training point. Black dashed lines are theory. (a) The variance of the training error along the task relevant subspace. We see that unlike the two layer model, more feature learning can lead to larger peaks in the finite size variance. (b) The variance of the NTK in the task relevant subspace. When properly normalized against the square of the mean \( K(t)^{2}\), the final NTK variance decreases with feature learning. (c) The gap in feature kernel variance across different layers of the network is amplified by feature learning strength \(\).

growth before exhibiting either a recovery or a clipping effect. In this section, we explore whether these dynamics are highly sensitive to initialization variance or if finite networks are well captured by mean field theory. Following , we consider two layer networks trained on a single example \(||^{2}=D\) and \(y=1\). We use learning rate \(\) and feature learning strength \(\). The infinite width mean field equations for the prediction \(f_{t}\) and the kernel \(K_{t}\) are (App. M)

\[f_{t+1}=f_{t}+ K_{i}_{t}+^{2}^{2}f_{t}_{t}^{2}\,,\ K_{t+1}=K_{t}+4^{2}f_{t} _{t}+^{2}^{2}_{t}^{2}K_{t}.\] (10)

For small \(\), the equations are well approximated by the gradient flow limit and for small \(\) corresponds to a discrete time linear model. For large \(>1\), the kernel \(K\) progressively sharpens (increases in scale) until it reaches \(2/\) and then oscillates around this value. It may be expected that near the EOS, the large oscillations in the kernels and predictions could lead to amplified finite size effects, however, we show in Fig. 6 that the leading order propagator elements decrease even after reaching the EOS threshold, indicating _reduced_ disagreement between finite and infinite width dynamics.

## 9 Finite Width Alters Bias, Training Rate, and Variance in Realistic Tasks

To analyze the effect of finite width on neural network dynamics during realistic learning tasks, we studied a vanilla depth-\(6\) ReLU CNN trained on CIFAR-10 (experimental details in App. B, G.2) In Fig. 7, we train an ensemble of \(E=8\) independently initialized CNNs of each width \(N\). Wider networks not only have better performance for a single model (solid), but also have lower bias (dashed), measured with ensemble averaging of the logits. Because of faster convergence of wide networks, we observe wider networks have higher variance, but if we plot variance at fixed ensembled training accuracy, wider networks have consistently lower variance (Fig. 7(d)).

We next seek an explanation for why wider networks after ensembling trains at a faster _rate_. Theoretically, this can be rationalized by a finite-width alteration to the ensemble averaged NTK, which governs the convergence timescale of the ensembled predictions (App. G.1). Our analysis in App. G.1 suggests that the rate of convergence receives a finite size correction with leading correction \((N^{-1})\) G.2. To test this hypothesis, we fit the ensemble training loss curve to exponential function \( Ct)}\) where \(C\) is a constant. We plot the fit \(R_{N}\) as a function of \(N^{-1}\) result in Fig. 7(e). For large \(N\), we see the leading behavior is linear in \(N^{-1}\), but begins to deviate at small \(N\) as a quadratic function of \(N^{-1}\), suggesting that second order effects become relevant around \(N 100\).

In App. Fig. A.4, we train a smaller subset of CIFAR-10 where we find that \(R_{N}\) is well approximated by a \((N^{-1})\) correction, consistent with the idea that higher sample size drives the dynamics out of the leading order picture. We also analyze the effect of \(\) on variance in this task. In App. Fig. A.5, we train \(N=64\) models with varying \(\). Increased \(\) reduces variance of the logits and alters the representation (measured with kernel-task alignment), the training and test accuracy are roughly insensitive to the richness \(\) in the range we considered.

## 10 Discussion

We studied the leading order fluctuations of kernels and predictions in mean field neural networks. Feature learning dynamics can reduce undesirable finite size variance, making finite networks order

Figure 6: Edge of stability effects do not imply deviations from infinite width behavior. Black dashed lines are theory. (a) The average kernel over an ensemble of several \(N=500\) NNs (solid color). For small \(\), the kernel reaches its asymptote before hitting the edge of stability. For large \(\), the kernel increases and then oscillates around \(2/\). (b)-(c) Remarkably variance due to finite size can _reduce_ during training (for \(\) smaller and larger than the critical value \( 1/\)), showing that infinite width DMFT can be predictive of finite NNs trained with large learning rate.

parameters closer to the infinite width limit. In several toy models, we revealed some interesting connections between the influence of feature learning, depth, sample size, and large learning rate and the variance of various DMFT order parameters. Lastly, in realistic tasks, we illustrated that bias corrections can be significant as rates of learning can be modified by width. Though our full set of equations for the leading finite size fluctuations are quite general in terms of network architecture and data structure, they are only derived at the level of rigor of physics rather than a formally rigorous proof which would need several additional assumptions to make the perturbation expansion properly defined. Further, the leading terms in our perturbation series involving only \(\) does not capture the complete finite size distribution defined in Eq. (3), especially as the sample size becomes comparable to the width. It would be interesting to see if proportional limits of the rich training regime where samples and width scale linearly can be examined dynamically . Future work could explore in greater detail the higher order contributions from averages involving powers of \(V()\) by examining cubic and higher derivatives of \(S\) in Eq. (3). It could also be worth examining in future work how finite size impacts other biologically plausible learning rules, where the effective NTK can have asymmetric (over sample index) fluctuations . Also of interest would be computing the finite width effects in other types of architectures, including residual networks with various branch scalings . Further, even though we expect our perturbative expressions to give a precise asymptotic description of finite networks in mean field/\(\)P, the resulting expressions are not realistically computable in deep networks trained on large dataset size \(P\) for long times \(T\) since the number of Hessian entries scales as \((T^{4}P^{4})\) and a matrix of this size must be stored in memory and inverted in the general case. Future work could explore solveable special cases such as high dimensional limits.

#### Code Availability

Code to reproduce the experiments in this paper is provided at https://github.com/Pehlevan-Group/dmft_fluctuations. Details about numerical methods and computational implementation can be found in Appendices F and N.

Figure 7: Depth \(6\) CNN trained on CIFAR-10 for different widths \(N\) with richness \(=0.2\), \(E=8\) ensembles. (a)-(b) For this range of widths, we find that smaller networks perform worse in train and test error, not only in terms of the single models (solid) but also in terms of bias (dashed). The delayed training of ensembled finite width models indicates that the correction to the mean order parameters (App. G) is non-negligible. (c) Alignment of the average kernel to test labels is also not conserved across width. (d) The ratio of the test MSE for a single model to the ensembled logit MSE. (e) The fitted rate \(R_{N}\) of training width \(N\) models as a function of \(N^{-1}\). We rescale the time axis by \(R_{N}\) to allow for a fair comparison of prediction variance for networks at comparable performance levels. (f) In rescaled time, ensembled network training losses (dashed) are coincident.