# Parseval Regularization for

Continual Reinforcement Learning

 Wesley Chung, Lynn Cherif, David Meger, Doina Precup

Mila, McGill University

chungwes@mila.quebec

###### Abstract

Loss of plasticity, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks--all referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.

## 1 Introduction

Continual reinforcement learning (RL) , a setting where a single agent has to learn in a complex environment with potentially changing tasks and dynamics, has remained a challenge for current agents. The core difficulty stems from training deep neural networks on sequences of tasks. Although the network may be able to learn the first task easily, after several tasks, progress may be impeded--a phenomenon termed _plasticity loss_. Other works have found that neural networks may be strongly influenced by data in the early phases of learning, leading to weaker performance on later tasks . Taken together, these works have demonstrated the difficulty of learning in the presence of nonstationarity, an integral aspect of reinforcement learning problems.

Optimization has historically been a barrier towards training neural networks, even on single tasks . The development of suitable parameter initialization schemes was a key ingredient to successful training, such as Xavier and Kaiming initializations . By prescribing a suitable variance for random Gaussian weights, these initialization strategies allow gradients to propagate throughout the network and avoid the vanishing and exploding gradient problems.

This line of work led to the development of orthogonal initialization , a technique designed to ensure better gradient propagation by making the singular values of the weight matrices all equal to \(1\) and thus maintaining the singular values of the Jacobian of the output with respect to the input to also be \(1\)--a property known as _dynamical isometry_. Orthogonal initialization was used to allow training of a vanilla convolutional network with 10 thousand layers without the use of normalization layers . Additionally, this technique has found success in RL settings with more shallow networks, being the default setting for PPO agents and improving upon Gaussian initialization strategies . Overall, this research direction has showcased the importance of encouraging model parameters to lie in regions amenable to optimization.

Taking this idea further, we can interpret the difficulty of optimization in continual RL as related to the parameters' movement during learning. At the beginning of training, the parameters are initialized such that the agent can easily adapt the policy and value functions in any direction based on the observed data. As training proceeds, the parameters may move to a part of the space that is no longer as easy to navigate . Due to the nonstationarity of the policy and objective in RL, this can be problematic if the agent is required to learn something that its current parameters cannot easily accommodate. These troubles may be further exacerbated when the task itself is modified, completely preventing the agent from learning a new task.

Using this intuition, we propose a continual RL solution--Parseval regularization , which encourages orthogonal weight matrices \(W\) by regularizing \(WW^{}\) to be close to \(cI\) where \(c>0\) is a constant and \(I\) is the identity matrix. By preserving the orthogonal property, we hope to keep the beneficial optimization properties of the initialization throughout the training process and improve the learning speed of new tasks. We empirically demonstrate that this addition facilitates learning on sequences of RL tasks as seen in Fig. 1 and Fig. 4 in MetaWorld , CARL  and Gridworld environments. In addition, it compares favourably to alternative algorithms such as layer norm , shrink-and-perturb , and regenerative regularization .

Additionally, we conduct ablation studies, separating out two parts of Parseval regularization: A regularizer on the norm of the rows of the weight matrices and a regularizer for the angles between these weight vectors. We find that, while both components improve the baseline, regularizing angles makes a larger impact and it is the combination that fares best.

Delving into the network properties, we find that Parseval regularization interacts with the diversity of neuron weights, the input-output Jacobian, the rank of the weight matrices and the entropy of the policy. Moreover, the regularization can reduce the correlation between neuron weights, increase the stable rank and maintain a tighter spread of entries in the input-output Jacobian, suggesting using these metrics as a target or diagnostic tool for future algorithms. Moving further, we explore how Parseval regularization interacts with different activation functions and network widths, finding that it can be productively used with various network architecture choices.

Finally, we investigate a few aspects linked to optimization and plasticity loss in neural networks including the role of policy entropy, initialization scale and rank of the initialization, finding that these quantities have complex relationships to performance.

## 2 Preliminaries

The problems we consider are defined as sequences of Markov Decision Processes (MDP). An MDP is defined by its state space, action space, transition function, reward function and a discount factor. The agent aims to learn a policy \(\) that maximizes the expected return \([_{t=0}^{}^{t}R_{t}]\). In this work, we will focus on sequences of tasks where the reward function changes after a certain number of timesteps while the transition dynamics stay the same. The changes are not signaled to the agent and thus there is nonstationarity in the environment that is difficult to model. This represents a simplified

Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both. Parseval regularization significantly improves on the baseline and outperforms other alternatives.

setting for the full continual RL problem, where the changes in the environment can be more general and less regular .

## 3 Parseval Regularization

Orthogonal initialization was initially proposed in the context of deep linear neural networks. It was shown that if orthonormal1 (orthogonal with unit norm row vectors) weight matrices are used, one would have depth-independent training times . This result was expanded to show that nonlinear networks with tanh activations could also achieve better convergence times as a function of depth [47; 26]. These theoretical results were validated by successfully training networks with thousands of layers and orthogonal initialization was also effectively employed with shallow networks in a deep RL setting [54; 27].

Orthogonal weight matrices are useful because they can ensure that the singular values of the weight matrices are all equal and, if these are equal to 1, it would mean that the layer forms an isometry--preserving distances, magnitudes and angles of its inputs and outputs (when the dimensions are matching) . Since the Jacobian of a linear layer with weights \(W\) is simply \(W^{}\), which is also orthogonal, the error gradients passed through in the backwards pass also maintain their structure as well, without exploding or vanishing (ignoring the activation function). This intuitively can lead to more favourable optimization.

Parseval regularization is implemented simply by adding a term to the usual objective. For each weight matrix \(W\) of a dense layer, we add the following regularizer:

\[_{Parseval}(W)=||WW^{}-sI||_{F}^{2}\]

where \(>0\) controls the regularization strength, \(s>0\) is a scaling factor, \(I\) is the identity matrix of appropriate dimension and \(||||_{F}\) denotes the Frobenius norm. This regularizing loss encourages the rows of \(W\) to be orthogonal to each other and also have a squared \(_{2}\)-norm equal to \(s\). If these constraints are satisfied, all the singular values of \(W\) will be equal to \(\). We directly add this regularization term to both the policy and value networks to every layer except the last. That is, the final objective is:

\[()=_{p}()+_{v}_{v}( )+_{k=1}^{()-1}_{Parseval}(W_{k})\]

where \(\) denotes all the parameter and \(W_{k}\) denotes the weight matrix of layer \(k\). \(_{p}\) and \(_{v}\) are the policy and value losses, with \(_{v}\) and \(\) being weighting coefficients.

The additional computational cost of this regularizer is of order \(O(d^{3})\) for a dense layer of width \(d\) (with \(d\) inputs). This is similar to the cost of one forward pass when the size of the minibatch is close to \(d\). In practice, we have found that the net runtime increase ranges from \(1.8\%\) to \(11.4\%\) over the vanilla agent. Empirical runtimes and a more detailed analysis can be found in appendix \(C.7\).

### Network capacity and Lipschitz continuity

By restricting the weights to be orthogonal, the network may be overly constrained. In particular, if the weight matrices of all the layers are orthogonal and the activation function is Lipschitz, then the function given by the neural network is also Lipschitz. A Lipschitz function \(f:^{n}^{m}\) satisfies: for all \(x,y^{n}\)\(||f(x)-f(y)|| L||x-y||\) for some constant \(L>0\) and norm \(||||\). This is a fairly strong constraint, meaning that the function values cannot vary too quickly as the inputs change, which may be overly limiting the neural network's capacity to express complex functions.

Thus, to relax this Lipschitz condition, we test a few additions. Throughout all the experiments, we do not regularize the final layer to preserve some expressiveness. However, the network

Figure 2: Comparing performance profiles of diagonal layers and learnable input scales on Meta-world sequences. Either addition helps with Parseval regularization.

remains significantly restricted with only this change.

**Adding diagonal layers.** The main addition that will be used throughout the paper is to introduce additional parameters after each (near) orthogonal layer. We multiply each output of the orthogonal layer by a (learnable) constant and include a bias term. This can be viewed as adding a dense layer with a diagonal weight matrix. It is identical to the additional parameters used by Layer Norm  after the normalization operation. Note that adding these parameters incurs only a small additional cost in terms of memory and compute (linear in the width of a layer).

**Input scaling.** The second alternative is to leave the network untouched but rescale the inputs. That is, we can introduce a factor \(c_{in}\) that multiples every input. The net effect is that the input space gets rescaled, which effectively multiplies the network's Lipchitz constant by \(c_{in}\). We can view this as allowing the function to vary more quickly across the input space. For this addition, we add a learnable constant \(c_{in}\) which multiples the entries, giving the network additional flexibility.

Fig. 2 shows that adding diagonal layers or a learnable input scale improves upon the agent with Parseval regularization. Interestingly, the addition of either degrades performance of the base agent though. We will use the addition of diagonal layers in all later Metaworld experiments for consistency.

**Relaxing the orthogonality constraint.** Finally, the third option explored is to relax the orthogonality constraint on the layers by freeing some weights from the constraint. We consider a weaker form of Parseval regularization where we split neurons in a layer into multiple groups and apply Parseval regularization only within each group. This allows the weight vectors of neurons in different groups to vary freely with respect to each other. Since we are mainly interested in the optimization benefits of Parseval regularization, we are willing to give up the Lipschitz constraint in favor of quicker learning.

As reported in Fig. 3, we do not observe any improvement from subgroup-Parseval regularization and the performance decreases with more groups. This suggests that, in this context, the optimization benefits from Parseval regularization outweigh the loss of expressiveness of the neural network.

We see that the stable rank decreases to be approximately equal to the number of orthogonal weight vectors. That is, if there are 64 neurons divided into two groups, the stable rank is around 32. This may reflect neural networks' tendency to decrease the stable rank until it reaches a small value, just enough to perform the task [45; 39].

To conclude, in the following experiments, we will use Parseval regularization in conjunction with either additional diagonal layers or a learnable input scale to introduce a little more capacity when required (details in appendix C.5).

Figure 3: The left plot shows performance profiles of Parseval regularization on Metaworld sequences when dividing neurons in a layer into multiple groups. There is no significant improvement from splitting into groups; using only one group is the best choice. Adding Parseval regularization with any number of groups improves on the baseline though. The right plot shows the stable rank of the actor’s second layer’s weight matrix. Due to the relaxed orthogonal constraint on the weights, we can observe a decrease in the stable rank. Similar plots can be observed for other layers and in the critic.

Experiments

**Environments and baseline algorithms**

We utilize a sequence of tasks with changes between each to force the agent to learn something new. How to choose these changes is an important design decision. Some common tasks in the continual learning literature such as Permuted MNIST  (shuffling the inputs at each change) or changing Atari games  have drastic changes between tasks, which may not reflect realistic variations that would face any practical agent. Due to these large changes, it often leaves little room for the agent to reuse previously learned knowledge.

We focus on producing different tasks by either changing the task (reward functions) while the transition dynamics generally remain the same, or by changing context variables such as wind or stiffness affecting the simulation properties while keeping the reward function fixed. These two types of nonstationarity can be natural. Agents may need to learn many different tasks over time in a single environment. e.g. a robot arm can both learn to push objects and, later, to grasp them. Reinforcement learning from human feedback (RLHF) is another instance where the reward function may change due to changing preferences although the environment otherwise stays the same. From another perspective, the agent may face different environmental conditions and would want to adapt to changes in them.

We run experiments in four sets of environments:

The first is a navigation task in a 15-by-15 **Gridworld** where the agent has to reach a varying goal location. The goal is fixed for multiple episodes and is then changed every 40 thousand steps. Each episode is limited to 100 timesteps. For the gridworld, the agent is evaluated every 5000 steps by running 10 episodes. The success rate is measured as the fraction of the episodes where the agent successfully reaches the goal (within 100 timesteps). We run 50 seeds. For training, the agent receives a reward equal to the length of the shortest path to the goal state (divided by 10 for scaling purposes). This is a dense, informative reward signal so the difficulty from exploration is minimized. As such, we can attribute any performance issues to the optimization aspects of training a deep RL agent.

Then, we consider two environments from the CARL suite : **LunarLander** and **DMCQuadruped**. To generate a sequence of tasks, we choose certain context variables (e.g. gravity, wind, joint stiffness) and vary them for each task. The same sequence of context variables are presented to all the agents. For LunarLander, the agent is trained for 10 million steps with task changes every 500 thousand steps. For DMCQuadruped, the task changes every 1.5 million steps up to 12 million steps. For both of these environments, we generate 20 different sequences of tasks and run each sequence for 3 seeds.

As a final benchmark, we use environments from the **MetaWorld** suite , where the majority of our experiments will be conducted. First, we run an RPO agent on all the environments and identify those where the agent can achieve a high success rate after 1M steps of training. This preliminary selection process results in a set of 19 environments (see Appendix C.4). These roughly match the tasks with high success rates indicated in  (appendix B).

From this set of candidate environments, we produce sequences of 10 tasks by sampling environments. In total, we produce 20 sequences of tasks, where each task corresponds to one Metaworld environment. We use a stratified sampling approach to ensure that each of the 19 environments are present the same number of times (or a difference of one) in all the sequences. Moreover, each sequence of tasks does not contain the same environment twice. These choices promote a diversity of task orderings and task choices. Overall, we obtain 20 sequences of 10 tasks each. We call this benchmark _Metaworld20-10_. Each agent is run on every sequence with 3 seeds each.

_Base agent._ We use the RPO agent  (a variant of PPO) for continuous actions or PPO for discrete actions, based on the implementation from CleanRL . Some adjustments to the hyperparameters were made as specified in the appendix (Sec. C.5), with one set of hyperparameters being used across all Metaworld tasks and one set for each of the other environments. For the learning curves, we plot the interquartile mean along with a \(90\%\) confidence interval shaded region. Each curve is also smoothed with a window of 5. Hyperparameters were tuned around the values provided in the CleanRL implementation. For all the algorithms, small sweeps were conducted on relevant hyperparameters and the best setting was chosen.

_Performance profiles._ Due to the high variability in performance due to the nonstationarity of tasks, we favor looking at the entire distribution of the agent's performance. To do so, we plot _performance profiles_. These plots correspond to 1 minus the empirical CDF of the measured evaluation statistic and help us visualize the distribution of the agent's performance in a cleaner manner. To produce one plot, we consider each task in a sequence as a separate datapoint. For each fixed task, we take the mean success rate across the learning curve to get one summary number. For example, for the Metaworld runs, this would mean the 10 tasks in a sequence are considered individually to get 10 summary numbers. Since there are 20 sequences of tasks and 3 seeds are run for each, this would give a total of \(10 20 3=600\) datapoints to form one performance profile. 90% confidence bands are formed using the Dvoretzky-Kiefer-Wolfowitz inequality (see appendix C.2).

_Baseline algorithms_

Shrink-and-perturb (SnP) was suggested as a solution to poor performance on a later task after pretraining . It also applies to sequences of tasks by decaying the weights and then adding some small amount of random noise at every step. SnP has also been found to be beneficial even on single tasks due to benefits arising from (partial) reinitialization . To implement SnP, following the original description, we add a small fraction of a freshly-initialized network's weights using Xavier initialization  to the learning agent's weights. We use AdamW  to implement weight decay.

Layer norm  has been found to give substantial benefits for training in a continual learning context . It allows deep neural networks to effectively learn sequences of tasks by adjusting the internal activations to have mean \(0\) and variance \(1\), mitigating the effect of distribution shift in the activations when tasks change. Layer norm has been used in a variety of contexts including RL  and large language models , displaying its utility even on single tasks.

Regenerative regularization (denoted "Regen" in plots)  is a simple strategy to attempt to maintain favourable optimization properties of the initialization scheme. It consists of \(_{2}\)-regularization towards the initial weights. An extension relaxes the soft constraint by using an empirical Wasserstein loss between the distribution of the initial weights and current weights, allowing the learned weights to move further from the initialization but still preserving the distribution (denoted "W-Regen") .

### Utility of Parseval regularization

We first test the base RPO agent, the proposed addition of Parseval regularization as well as the baseline algorithms on the primary continual RL setting. Metaworld results are presented in Fig. 1 and other environments in Fig. 4.

Overall, we see that the addition of Parseval regularization can greatly improve the performance of the base agent on the four set of tasks. While the alternative algorithms can also improve the baseline, Parseval regularization makes the largest difference. This is most apparent in the gridworld where it lets the agent make progress on almost every task while the agent often gets stuck at 0 success rate with the other algorithms (as indicated by the large drop near zero on the x-axis).

For the next sections, we focus our attention on the Metaworld benchmark and investigate various questions in detail, including testing varying the network architecture, ablation studies and analysing parameter properties throughout training.

### Variations of architecture and algorithm

We check if the benefits of Parseval regularization carry over to different neural network architectures.

Figure 4: Performance of algorithms on gridworld and CARL environments. Parseval regularization yields the largest improvements although other approaches can be helpful.

#### Activation functions

The tanh activation function is the default choice for PPO  and we have already seen from the baseline that adding Parseval regularization improves the agent's performance considerably. We validate that Parseval regularization benefits other choices of activation functions such as ReLU , mish , concatenated ReLU (CRELU)  and MaxMin .

ReLU is a classic choice of activation function still widely in use. Mish, a smoother version of ReLU, and similar functions (e.g. GELU, Swish)  have been shown to outperform the standard ReLU activation in deep learning tasks. Concatenated ReLU (CRELU) was used in the context of reinforcement to deal with the problem of dead units . MaxMin was proposed in the design of Lipschitz neural networks  and, by maintaining the norm of backpropagated gradients, it may also be suitable in this continual RL setting.

In Fig. 5, we see that all activation functions benefit from Parseval regularization, with Tanh showing the largest difference. Interestingly, without Parseval regularization, the different activation functions perform roughly the same. Even Concatenated ReLU, which was designed to tackle an aspect of plasticity loss fares no better. Only the MaxMin activation has noticeably higher performance, suggesting that preserving gradient norms across the activation function (as it was designed to do) may be a useful guiding principle for architectures in a continual RL setting.

In Fig. 14 (in the appendix), as an additional check, we verify that using identity activations (and hence a linear function overall), results in poor performance, confirming that nonlinearity is crucial even with orthogonal weights. Interestingly, linear activations tend to make _some_ progress (e.g. more than 20% success) on more tasks than the nonlinearities, but at the price of fewer tasks where the agent achieves high success rates (e.g. over 90%).

#### Neural network widths

We investigate what happens when the width of the network is changed. To adapt Parseval regularization to different network widths, we ensure that the regularization strength is scaled appropriately by the square of the width. If the width is doubled, the regularization strength is divided by four. Results are summarized in Fig. 5. Again, Parseval regularization can benefit networks of reduced or increased widths.

### Ablation studies

Parseval regularization can be split into two different components. It acts on the rows of the weight matrices and 1) encourages the angles between them to be \(0\) and 2) encourages the norms to be constant. We test these components separately to better identify the source of Parseval regularization's benefits.

#### Angles between vectors

We investigate whether a variant of Parseval regularization that only regularizes the angles is also helpful. This can assess the benefits of diversity in the weight vector directions. Specifically, if \(W\) denotes the weight matrix to be regularized, we first normalize the rows to have norm \(1\) to get \(=W/||W||_{row}\) where division is row-wise and \(||W||_{row}\) computes the \(_{2}\) norm of each row. Then, we apply Parseval regularization on \(\). This has the net effect of disregarding the norm of the row weight vectors while still regularizing the inner products towards zero.

Figure 5: Performance profiles for different architecture choices. (Left and center) Varying activation functions: all choices benefit from Parseval regularization. (Right) Varying the network width. Parseval regularization can benefit all three settings. Increasing the width alone does not help.

Fig. 6 shows that this ablation still produces weight vectors that are orthogonal to each other. While there is some benefit, the performance is worse than standard Parseval regularization.

**Norm of the weights.** Parseval regularization regularizes the row weight vectors towards the initial scale, set to \(\) by default. The norm of the weights is a common metric linked to plasticity loss, with growing norms potentially indicating training difficulties . In this view, regularizing the norms of the weight matrix rows could be beneficial in a similar fashion to weight decay.

Revisiting Fig. 3, setting the number of subgroups to \(64\) (the width of the network) corresponds to applying only regularization to the weight norms, without regularizing angles. There is a small benefit over the baseline, but not nearly as much as full Parseval regularization.

_Findings._ Both ablations improve the performance over the baseline but do not match full Parseval regularization, although using regularization on the angles makes a larger difference. From this, we can conclude that both components are critical to the success of Parseval regularization.

### Analysis of training

To verify the impact of Parseval regularization on network properties throughout training, we inspect two measures of diversity: the stable rank of weight matrices and the correlation between weight vectors of neurons.

**Stable rank**. We first check the _stable rank_ of the matrices, defined as \(srank(A)=_{i}^{2}}{_{i}_{i}}\) where \(A\) is an \(n n\) matrix and \(_{i}\) (\(i=1,...,n\)) are its singular values. This soft version of the rank will be equal to \(n\) if all the singular values are equal and is at most the standard rank. It is less sensitive to small singular values.

_Justification:_ Having a larger rank would indicate that the weight vectors span a greater portion of the input space. A similar notion of rank has previously been found to be correlated to the performance of reinforcement learning agents  in certain settings. In particular, excessively low rank values can be linked to poor performance. Since Parseval regularization encourages matrices to be orthogonal and thus have all equal singular values, we would expect its addition to make matrices closer to full rank, potentially boosting performance.

_Findings._ We find that Parseval regularization can increase the stable significantly, leading the matrices to maintain almost full rank, while the baselines all experience a quick reduction in rank during training before stabilizing at a small value (often less than 10). See plots in Appendix B.1 for details.

**Neuron weight similarity**. We also verify another measure of diversity, corresponding to the average cosine similarity of the row vectors of a weight matrix. Symbolically, for a given weight matrix \(W\) with \(n\) rows, it is given by: \(_{i j}(}{||w_{i}||}}{ ||w_{j}||})\) where \(w_{i}\), \(i(1,...,n)\) denotes the \(i\)-th row of \(W\).

_Justification:_ Weight similarity measures how different the directions of the weight vectors are from each other. So, the lower the weight correlation, the greater the diversity amongst the neurons. With orthogonal initialization, we expect direction correlation to be zero at the start of training (or near-zero if the weight matrix has more rows than columns so there are more vectors than dimensions in the space). This measure was also used by  under the name _forward correlation_.

One may expect that a smaller neuron weight correlation would be favourable since there will be greater diversity in the "active" regions of the activation function and cover the space of pre-activations more fully. This could be especially important when there is nonstationarity in the data (as in RL), so state inputs or intermediate activations may lie in previously uncovered regions of the space. Maintaining some diversity pre-emptively could yield faster learning when change occurs. There is

Figure 6: Parseval regularization on normalized row weight vectors, regularizing only angles, on the Metaworld sequences. The right figure shows the average angle between row weight vectors, confirming it has the desired effect.

some evidence that weight vectors in vanilla MLP networks will be oriented in the same, redundant directions if they are sufficient to perform the task . Larger weight correlation may be a symptom of networks that have overly specialized to a task and would also be reflected in a reduction in stable rank.

_Findings._ We find that Parseval regularization indeed maintains a near-zero neuron weight correlation while also having nearly full stable rank. The other algorithms manifest decreasing stable ranks and increasing neuron weight correlation over time (see Appendix B.1 for plots).

**Additional experiments.** In Appendix A, we have included other exploratory experiments. We find that Parseval regularization can help reduce the variance of entries in the input-output Jacobian, a quantity which may be linked to training stability  compared to the baseline (see Fig. 10). In a different experiment, to help understand how low weight stable ranks and large parameter norms affect training, we perturb these at initialization but it does not necessarily cause the agent to fail to learn a task (see appendix A.1). Thus, we can conclude it is these properties in conjunction with other aspects of the optimization process which can be problematic, painting a more complex picture and echoing the findings of Lyle et al. .

## 5 Related works

Previous works have tackled the problem of _loss of plasticity_ from different angles. Many algorithms in this line of work focus on injecting new randomness into the weights to restore some of the initial randomness present in the usual Gaussian weights. Aside from Shrink-and-Perturb discussed previously, other algorithms algorithms focus on identifying useful neurons and resetting the weights of those deemed as unnecessary [12; 57]. These algorithms use a notion of usefulness that is dependent on using ReLU activations, which is not directly applicable to other nonlinearities such as the tanh activation. Another approach is to reset the weights entirely for certain layers [43; 13] to fresh weights. While this can solve trainability issues, it also removes any possibility of using learned representations from previous tasks to speed up future learning. Moreover, the frequency of the resets can be an important hyperparameter. Methods like Parseval regularization that act on the network architecture (parameters, activations or normalization layers), naturally avoid having to specify the timescale at which change occurs. Expanding the network by adding more neurons to the network during training can also be used to improve trainability . This comes with increasing compute and memory costs as the agent interacts with the environment, making it less appealing for continual learning settings where, in principle, an infinite number of task changes may occur.

In the continual learning community, many works have tackled how to learn efficiently from sequences of tasks, although they have mainly focused on the problem of catastrophic forgetting [15; 31; 60; 35], that is, how to remember previous solutions to previous tasks without overriding them when learning on new tasks. Many of these investigations have focused on supervised learning in the past. In this paper, we focus mainly on improving the _plasticity_ of neural networks rather than the stability, which may be a priority in RL settings where the agent is focused on improving its current policy rather than remembering all past policies.

Parseval regularization was originally proposed in the context of maintaining Lipchitz constraints to improve adversarial robustness [11; 4] with little focus on the optimization aspects. Many other techniques have been proposed to maintain orthogonal weight matrices, including regularization , parameter scaling  and specialized parameterizations [4; 56]. In this work, we have focused on the simplest method--directly regularizing the weight matrices to be orthogonal--although future investigations may find some benefit in using more advanced techniques.

Orthogonal initialization  and later developments in initialization schemes such as ReZero  and Fixup  have used similar design principles such as maintaining an input-output Jacobian near an identity matrix to maintain gradient magnitudes across layers and avoiding exploding and vanishing gradients. These later initializations are tailored to Resnet-based architectures featuring skip connections  and have focused on the optimization properties at initialization whereas, in this paper, we emphasize the entire training process.

## 6 Discussion and Limitations

We take an optimization viewpoint on the continual RL problem and suggest improving performance by considering the optimization properties of network layers. Starting from the classic idea of using proper weight initializations, Parseval regularization aims to keep the weights in regions of the parameter space where the network has well-conditioned gradients throughout the entirety of training, even across task changes. A potential weakness of Parseval regularization is that it reduces the capacity of the network by constraining optimization to lie within the space of orthogonal matrices. We have seen that this can be offset by introducing a few parameters (or only one) through diagonal layers or a learnable input scale. The ingredients of deep neural networks were initially developed for single tasks in the supervised learning setting and we believe revisiting the basic building blocks may lead to other fruitful developments in reinforcement learning and continual learning.