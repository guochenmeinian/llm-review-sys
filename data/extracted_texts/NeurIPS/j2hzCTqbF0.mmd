# Accurate and Steady Inertial Pose Estimation through Sequence Structure Learning and Modulation

Yinghao Wu\({}^{1}\)  Chaoran Wang\({}^{1}\)  Lu Yin\({}^{1}\)  Shihui Guo\({}^{1*{}}\)  Yipeng Qin\({}^{2}\)

\({}^{1}\)School of Informatics, Xiamen University, China

\({}^{2}\)School of Computer Science & Informatics, Cardiff University, UK

Corresponding author (guoshihui@xmu.edu.cn) Also with Key Laboratory of Digital Protection and Intelligent Processing of Intangible Cultural Heritage of Fujian and Taiwan, Ministry of Culture and Tourism.

###### Abstract

Transformer models excel at capturing long-range dependencies in sequential data, but lack explicit mechanisms to leverage structural patterns inherent in fixed-length input sequences. In this paper, we propose a novel sequence structure learning and modulation approach that endows Transformers with the ability to model and utilize such fixed-sequence structural properties for improved performance on inertial pose estimation tasks. Specifically, our method introduces a Sequence Structure Module (SSM) that utilizes structural information of fixed-length inertial sensor readings to adjust the input features of transformers. Such structural information can either be acquired by learning or specified based on users' prior knowledge. To justify the prospect of our approach, we show that i) injecting spatial structural information of IMUs/joints learned from data improves accuracy, while ii) injecting temporal structural information based on smooth priors reduces jitter (i.e., improves steadiness), in a spatial-temporal transformer solution for inertial pose estimation. Extensive experiments across multiple benchmark datasets demonstrate the superiority of our approach against state-of-the-art methods and has the potential to advance the design of the transformer architecture for fixed-length sequences.

## 1 Introduction

Estimating human pose is a long-standing and prominent task that underlies many computer vision and graphics applications, e.g., animation production, virtual reality. As an alternative to vision-based solutions, wearable device-based methods are gaining increasing interest as they are environment-free, occlusion-unaware, privacy-friendly. For ensuring high accuracy and maintaining portability and usability, most prior works  abandon densely placed configurations, instead leveraging a sparse set of Inertial Measurement Units (IMUs) to reconstruct human motion. Since IMUs can provide continuous measurements of rotation and acceleration, we define pose estimation with sparse inertial sensors as a _sequence learning_ task.

Recently, Transformer-based architectures have achieved tremendous success in various sequence learning tasks, and applying them to sparse inertial motion capture is a natural idea. However, empirically, we find that, directly using the native transformer to model IMU sequences results in unacceptable jitter and inaccurate postures. Through our analysis, we attribute this to _the native Transformer architecture, whose self-attention mechanism was originally designed to flexibly handle variable-length sequence inputs, thus lacks inductive bias for modeling fixed-length sequences that have clear structures_. For instance, for an IMUs reading sequence, the length is usually fixed (e.g., the number of observed past frames in a time window or the number of IMUs) and each token in the sequence has a specific meaning (e.g., each temporal token denotes a frame and each spatial tokenrepresents an IMU). In other words, as a representative type of fixed-length sequences, IMU readings have clear structures both in spatial and temporal dimensions. However, such structural information is not explicitly modeled in native Transformers.

To bridge this gap, we present an innovative approach for learning and modulating sequence structure, which empowers transformers to effectively capture and leverage structural properties of fixed-length sequences, leading to enhanced performance in inertial pose estimation tasks. Specifically, our method introduces a novel Sequence Structure Module (SSM) designed to leverage the structural information of fixed-length sequences to adjust the input features of Transformers. For inertial pose estimation tasks, we devised two SSM variants: SSM-S and SSM-T, for injecting spatial and temporal structural information into spatial and temporal transformer features, respectively. Extensive experiments on four public benchmarks demonstrate that our method achieves superior performances than state-of-the-art methods, where the average errors of the whole-body angles decreased by 13% and 38%, together with the lowest jitter, on the DIP-IMU  and TotalCapture  datasets, respectively. In addition, we implemented a real-time pose estimation system to test the performance of our approach in real-world scenarios. In summary, our contributions include:

* We identify a key limitation of the native transformer architecture: its lack of inductive biases for modeling fixed-length sequences with inherent structural properties. To address this shortcoming, we propose a novel Sequence Structure Module (SSM) that enables transformers to effectively capture and leverage the structural priors present in fixed-length sequential data.
* For inertial motion capture tasks involving sequential IMU data, we propose two SSM variants: SSM-S and SSM-T, which incorporate structural inductive biases of the IMU sensor layout (spatial) and time frames (temporal), respectively, into transformer learning.
* Extensive experiments demonstrate that our method outperforms state-of-the-art ones on the DIP-IMU and TotalCapture datasets by a large margin. To further demonstrate the superiority of our approach, we implemented a _real-time motion capture system_ based on six IMUs to evaluate the performance of our model in complex _real-world scenarios_.

## 2 Related Works

### Human Pose Estimation

Human pose estimation (HPE) has been a long-standing research topic, with numerous researchers exploring it using various types of sensors. As our research lies on IMU sensors, we roughly categorize sensors into two types: non-IMU and IMU.

Figure 1: Left: we use only six IMUs to predict the full-body pose in real-time, which are fixed on left and right forearm, the left and right lower leg, the head, and the pelvis. Right: our system is capable of capturing a wide range of daily motions as well as challenging movements.

**HPE with non-IMU Sensors**. In non-IMU human motion capture solutions, vision-based methods still dominate the mainstream so far. Early methods [14; 23; 16; 42] used multiple cameras and marker points to capture human poses, which imposed significant constraints on the environment. With the popularity of deep learning, an increasing number of methods are using a single camera to capture human 2D/3D poses, such as CPN , HRNet  and others [62; 22; 54; 51; 61; 45; 39; 28; 67; 58; 25], achieving significant success. In addition, there are also other types of sensors used for tracking human motion, such as 6DOF trackers [43; 5; 13; 19; 63], flexible fabric sensors [7; 26; 68], wireless sensors [6; 53] and hybrid sensors [3; 35; 38]. Although each method has its own advantages, motion capture systems based on IMUs are emerging and gaining prominence, due to their wearability, portability, and ease of use.

**HPE with IMU Sensors**. The advantage of inertial motion capture systems compared to vision-based methods is their resistance to extreme conditions such as bright lights and occlusion. Commercial systems like Xsens  and Noitom  place multiple IMUs on the user's body, while achieving accurate pose estimation, also restricting the user's movements and requiring lots of time for IMU attachment. To enhance user comfort and usability, an increasing number of studies [48; 18; 56; 55; 20; 60] are shifting towards sparse inertial motion capture and predict human postures using only a few IMUs. As a pioneering work in sparse inertial pose prediction, SIP  demonstrates that recovering full-body motion using only 6 IMUs is feasible for the first time, but it is an optimization-based approach which is computationally slow. Huang et al. , the first to introduce neural networks into this task, employ a bidirectional RNN to achieve real-time performance. After that, Transpose  proposes a multi-stage pose estimation framework, utilizing three sub-networks based on bidirectional RNN to predict pose, further enhancing accuracy. However, both of [18; 56] require future frames as input, which adds additional latency. PIP , introduces physical constraints, improving the physical plausibility of motion without the need for future frames. Another work, TIP , utilizes Transformer to predict human body poses while simultaneously generating terrain, achieving prediction of human motion in non-planar environments. Contemporaneous work, DynaIP , leverages pseudo-velocity learning to fully utilize acceleration and models the human body into three separate regions, each focusing on their unique characteristics. In addition, some studies [34; 69] have attempted to place IMUs in objects carried by the body for tracking human motion. For example, Zuo et al.  use a loose-wear jacket with 4 IMUs to capture the upper body motion, which provide the users with high comfort and freedom of movement.

However, existing methods only focus on modeling the temporal dimension (whether using RNNs or Transformers) while neglecting the spatial dimension. Unlike them, we utilize a two-stage Transformer-based spatial-temporal framework, independently capturing the dependencies of both space and time. Meanwhile, we have also designed two modules, SSM-S and SSM-T, enabling the Transformer to more effectively leverage structural information from fixed-length sequences.

### Transformer Variants for Time Series

Transformer has seen a number of modifications to address the limitations of well-known works such as BERT  and ViT . In the field of time-series data modeling, researchers have proposed various approaches, one common method being modifications in positional encoding [24; 59; 27; 64; 50; 65]. For example, Transformer-XL  introduced relative positional encoding, enabling the model to capture long-range dependencies, TCN-Transformer  combines the characteristics of convolutional networks with relative positional encoding. Self-attention module is the central part of Transformer. However, for many long-sequence based tasks, the time complexity of self-attention module is a computational bottleneck. Various works [31; 64; 65; 49; 4; 52] are proposed to address this issue. Longformer  employs a sparse attention mechanism, specifying local and global attention, allowing the model to handle long sequences while maintaining computational efficiency. Linformer  approximates the original high-dimensional attention matrix through low-rank projection, reducing both computational and memory requirements. Additionally, some researchers made structural modifications [52; 10; 64; 21] to Transformer for time series tasks. For example, Transformer-XL  incorporates a segment-level recurrence mechanism in the encoder to handle longer contextual information. Reformer  utilizes locality-sensitive hashing (LSH) and a reversible network structure, enabling the model to process extremely long sequences. Informer  introduces ProbSparse Self-Attention, reducing the computational complexity of self-attention.

Previous studies have modified the Transformer extensively, but they have largely overlooked its limitations in modeling fixed-length sequences and does not impose Transformer to explicitly utilize the inherent structural information within it. Our work bridges this gap.

## 3 Method

### Why is Sequence Structure Modeling Missing in Native Transformer Architecture?

The Transformer architecture  was originally designed to accomplish machine translation tasks in natural language processing. To handle variable-length textual inputs with different syntaxes, the native Transformer architecture does not make any inductive bias on their structures, but instead focuses on the content of the input text. Specifically, let \(X^{N d}\) be the embedded input sequence of length \(N\) and feature dimension \(d\), the self-attention mechanism is defined as:

\[(Q,K,V)= V=(QK^{}/)V \]

where \(Q=XW_{Q}\), \(K=XW_{K}\) and \(V=XW_{V}^{N d}\); \(W_{Q}\), \(W_{K}\), and \(W_{V}^{d d}\); \(^{N N}\). Among them, only the attention matrix \(\) is modeling the relationships among the \(N\) input tokens. However, its element \(_{(i,j)}\) is calculated as the product between the \(i\)-th query (\(Q\)) and the \(j\)-th key (\(K\)) pair, thus representing the relationship between individual tokens rather than the structure of the input sequence as a whole. This enables it to handle sequences with different \(N\) that do not share a common structure (e.g., sentences).

However, in many other domains (e.g., pose estimation), the input sequences usually have fixed length (e.g., the number of observed past frames or body joints) and a clear structure (e.g., temporal continuity or spatial relationship), which implies that the structural information of input sequences can facilitate learning. Motivated by this key insight, we propose a novel Sequence Structure Module (SSM) to endow the Transformer architecture with the ability to model and utilize the structural information inherent in fixed-length input sequences.

### Sequence Structure Module

Our Sequence Structure Module (**SSM**) aims to fully utilize the structured information of fixed-length sequence inputs to compensate for the lack of inductive bias in the transformer architecture. Specifically, as shown in Fig. 2 (d), given a sequence embedding \(X^{N d}\), before entering the transformer encoder, we multiply \(X\) with a **structural matrix**\(S^{N N}\), followed by a LayerNorm (LN) layer  and a MLP Block:

\[=((SX)) \]

where \(^{N d}\), \(()\) is used to regularize the model and maintain gradient stability during training and \(()\) is used to increase the capacity of the module. _The key distinction between the structural matrix \(S\) and the self-attention matrix \(\) is that the elements of \(S\) are independent of the input \(X\), making \(S\) universally applicable to all input sequences regardless of their content. This allows \(S\) to effectively capture structural information._ Then, we feed the structure-enhanced \(\) into the subsequent transformer encoder for modeling the long-range dependencies using Eq. (1). The structural matrix \(S\) can be obtained in various ways. For example, it can come from prior knowledge in the specific domains, be entirely data-driven through learning, or be a combination of the two. Here, we categorize the structure into the following three types: **Explicit Structure \(S_{E}\)**, **Implicit Structure \(S_{I}\)**, and **Explicit-Implicit Hybrid Structure \(S_{EI}\)**.

**Explicit Structure (ES)** is particularly useful when each token in the input sequence of a fixed length \(N\) has a clear meaning (e.g., each token in a time sequence represents a frame), and the structural relationships between the \(N\) tokens can be precisely captured based on prior knowledge. That is, each element \(S_{E}(i,j)\) in the structural matrix is provided by the user before training, allowing the user to impose prior knowledge of the sequence structure on learning. Notably, when \(S_{E}\) is the identity matrix \(I\), no modifications are made and the sequence structure module degenerates. We provide two examples of how to construct explicit structures \(S_{E}\) in Sec. 3.4.

**Implicit Structure (IS)** becomes appropriate when the meaning of each token in the sequence and/or the structural relationships between tokens are unclear. Unlike **ES**, which are completely determined by the user, the establishment of **IS** relies on a learnable matrix \(P\), that is learned in a data-driven manner. We define the implicit structure matrix \(S_{I}\) using the following equation:

\[S_{I}=I+P \]

where \(I,P^{N N}\) and \(I\) denotes the identity matrix. Notably, another way to define \(S_{I}\) is to solely use \(P\) without the identity matrix \(I\), namely \(S_{I}=P\). But as pointed out in , in the extreme situation, it is easier to optimize \(P\) into a zero matrix rather than an identity matrix.

**Explicit-Implicit Hybrid Structure (EIHS)** aims to strike a balance between **ES** and **IS** by allowing the user to provide an initial structure matrix based on prior knowledge, while also using data-driven methods to modify it. It can also be referred to as a non-identity matrix initialization of Eq. (3):

\[S_{EI}=S_{E}+P \]

where \(S_{E}^{N N}\) represents the explicit structure, and \(P^{N N}\) is a learnable matrix. In the extreme case, when the user-designed explicit structure \(S_{E}=I\), **EIHS** degenerates into **IS**.

### SSM for Transformer-based Sparse Inertial Pose Estimation

**Problem Formulation**. Our task is to accomplish real-time human pose estimation using data acquired from 6 inertial sensors positioned on the wrists of both hands, ankles of both feet, waist, and head (Fig. 1). Each IMU can provide sequential acceleration \(\) and orientation \(\) signals on the body part it is placed on, where \(^{3}\) is the linear acceleration and \(^{3 3}\) is the rotation matrix. Our goal is to learn a mapping \(f\) which reconstructs the joints' rotations of the full body:

\[_{1:J}^{T}=f(\{,\}_{1:N}^{1:T}) \]

where \(T\) denotes the number of observed frames from the past, \(J\) denotes the number of predicted joints, \(N\) denotes the number of IMUs, and \( SO(3)\) is the rotation of body joints, representing the human pose with a certain skeleton (e.g. SMPL ).

**Spatial-Temporal Transformer with SSM**. To accomplish this task, unlike previous works that only employed temporal encoders (Fig. 2 (a)), we utilize a spatial-temporal framework transformer network as our baseline model (Fig. 2 (b)), where the spatial transformer models the local motion correlations among \(N\) IMUs/joints within a frame, while the temporal transformer captures the global dependencies between \(T\) frames throughout the entire sequence. Since the values of \(N\) and \(T\) are typically fixed, (e.g. \(N=6\) and \(T=30\)), we introduce two variants of Sequence Structure Module. **SSM-S** and **SSM-T**, to leverage the structural information of fixed sequences in spatial and temporal dimensions, respectively (Fig. 2 (c)). As previously mentioned, each SSM has three different choices for the structure matrix \(S\). After thorough experimental comparisons in Sec 4.4, our final choice is that, the structural matrix of SSM-S is derived from **EIHS**, and the structure of SSM-T is **ES**. Due to page limitations, we have included more network details in Appendix Sec. 7.

Figure 2: (a) Previous work only employ temporal encoders (RNN or transformer) to predict pose. (b) Our spatial-temporal framework. (c) Our spatial-temporal framework with **SSM**. (d) Our sequence structure module (**SSM**), simply consists of structural matrix \(S\), LayerNorm  and MLP Block.

### Constructing Explicit Structures

The power of SSM lies in its large design space of explicit structures \(S_{E}\). Here, we take the sparse inertial pose estimation task as an example, to demonstrate how to construct \(S_{E}\) in spatial and temporal dimensions based on prior knowledge.

**Spatial Structure Construction**. The motivation for constructing the spatial structure \(S_{E-S}\) is to explore relevance and consistency between body joints when the human body moves. "Relevance and consistency" refer to the tendency for certain joints to exhibit similar movement patterns across a large sample of human motions. For instance, when the left hand moves forward, the head, torso, and legs are likely to move in a certain coordinated manner.

We utilize a statistical approach to accomplish this task. Specifically, we utilize the AMASS  dataset to decompose the rotations of different joints in each frame into rotations along the \(x\)-axis, \(y\)-axis, and \(z\)-axis, representing the rotations in terms of Euler angles. By doing so, we obtain \(R^{x},R^{y},R^{z}^{J f}\), where \(f\) denotes the number of observed frames and \(J\) denotes the number of joints. We select subsets (\(sub\)) of the rotations consisting of the \(N\) joints where IMUs are placed (e.g., \(N=6\)) and have: \(R^{(x,sub)},R^{(y,sub)},R^{(z,sub)}^{N f}\). We independently calculate the correlation matrix \(C^{k}^{N N},k=x,y,z\) for each subset and sum up the results for average to obtain the spatial explicit structure \(S_{E-S}^{N N}\):

\[C^{k}(i,j)=(R^{(k,sub)}_{(i)},R^{(k,sub)}_{(j)})}{(R^{(k,sub)}_{(i)})(R^{(k,sub)}_{(j)})}} \]

\[S_{E-S}=_{k=x,y,z}C^{k} \]

where cov denotes the covariance between two variables, and var denotes the variance of a variable. The resulting \(S_{E-S}\) is shown in Fig. 3. It can be observed that, the movements between the two hands and the two legs exhibit high correlation; but the movements of the head and the root (spine) are relatively independent; which aligns with our intuition.

**Temporal Structure Construction**. Unlike the use of statistical methods to calculate spatial explicit structures \(S_{E-S}\), the construction of temporal explicit structures \(S_{E-T}\) is much more straightforward, which follows the "distance" between two frames within a time sequence. That is, when two frames are "close" enough, such as adjacent frames, their correlation is relatively high; when the frames are "farther apart", such as the first and last frames in a sequence, their correlation tends to decrease as the "distance" increases; akin to a smoothness prior. Mathematically, we define \(S_{E-T}^{T T}\) using a function that linearly decreases with increasing "distance":

\[S_{E-T}(i,j)=0&\ \ \ |i-j|,\\ 1-& \]

where \(\) is a hyperparameter, which represents the maximum "distance" between two frames that are considered correlated. Specifically, when the "distance" between \(i\)-th and \(j\)-th frame, namely \(|i-j|\), we consider these two frames to be unrelated. Fig. 3 shows the visualization of \(S_{E-T}\)

Figure 3: The visualization of \(S_{E-S}\), and \(S_{E-T}\) with different \(\).

[MISSING_PAGE_FAIL:7]

methods, outperforming the runner-up by 44%, 44%, 49% and 29% respectively. We attribute this to our spatio-temporal framework and the spatial structure information in SSM-S, which help the model better capture the motion correlation between body joints. Besides, on the DIP-IMU and TotalCapture datasets, our predicted motion sequences exhibit the least jitter (i.e., smoothest motion sequence). We attribute this to the temporal sequence structure information in our SSM-T, which provides more temporal prior knowledge between frames and smooths the input features to generate more stable and consistent motion prediction results.

**Qualitative Results.** We also provide a visual comparison between the estimated pose and the ground truth on the TotalCapture dataset. Compared with state-of-the-art methods, our method achieves more precise predictions as shown in Fig. 4. The comparison of the two actions (leaning forward and bending over) indicates that, our method can estimate the positions of arms and legs more accurately than previous methods. Additionally, the comparison of the two ambiguous actions (raising a leg and raising both hands) in the second row shows that our model better identifies these ambiguous actions.

**Analysis for Error of Joints**. As shown in Fig. 5, we also compare the angular errors of individual joints on the DIP-IMU and TotalCapture datasets. It can be observed that, the errors of previous methods are mostly concentrated in the joints of the hands (L/R elbows and shoulders) and legs (L/R hips and knees), which are the main sources of _SIP Err_. We believe this is because they only capture the inter-frames dependencies in the temporal dimension, while neglecting to model the motion correlations between body joints in the spatial dimension. Unlike them, we utilize a two-stage spatial-temporal framework, where the spatial encoder independently models the motion consistency of IMUs/joints within a frame, and SSM-S injects spatial structural information into the features. Their combined effect allows for more accurate estimation of each joint rotation.

### Ablation Study

We conduct ablation experiments on the TotalCapture dataset, reporting two metrics: _Ang Err_ and _Jitter_. Additionally, we observe a trade-off between _Ang Err_ and _Jitter_, challenging the simultaneous achievement of the lowest values for both. For convenience, we introduce a temporary metric \(=(Ang\,Err)*Exp(Jitter)\) as a reference for selecting the optimal model, where a lower \(\) represents the balance between accuracy and stability, with \(Exp()\) denoting exponential operation.

    &  &  \\   & SIP Err & Ang Err & Pos Err & SIP Err & Ang Err & Pos Err \\  Transpose & 12.15 & 6.29 & 4.91 & 20.06 & 8.75 & 6.86 \\ TIP & 10.11 & 4.55 & 3.56 & 13.05 & 5.67 & 4.30 \\ PIP & 9.49 & 4.09 & 3.29 & 12.68 & 5.52 & 4.12 \\ DynaIP & 8.93 & 3.45 & 3.41 & 11.42 & **454** & 3.69 \\  Ours & **4.56** & **3.37** & **1.73** & **8.14** & 5.49 & **2.57** \\   

Table 2: Comparison with SOTA methods on AnDy  and CIP  datasets with Xsens  skeleton.

Figure 4: Qualitative comparisons with the state-of-the-art methods on TotalCapture dataset.

   Models & Ang Err & Jitter & \(\) \\  Baseline & 8.82 & 0.48 & 14.25 \\ + SSM-S & 7.83 & 0.43 & 12.04 \\ + SSM-T & 7.93 & 0.09 & 8.68 \\  Ours & **6.82** & **0.09** & **7.46** \\   

Table 3: Ablation study of SSM-S and SSM-T.

**Design of Sequence Structure for SSM-S and SSM-T.** We first explore the designs of sequence structure in SSM-S and SSM-T. As previously mentioned, each SSM can have three different choices, resulting in a total of \(3 3=9\) combinations for the two SSMs. To simplify this, we first fix the type of SSM-S as IS and explore the best structure type for SSM-T. As shown in Tab. 4, comparing Settings 1, 2, and 3 demonstrates that ES is the best structure choice for SSM-T, as the other two structures result in unacceptable jitter. Based on this, we conduct experiments with Settings 4 and 6. By comparing Settings 2, 4, and 6, we find that EIHS is the best choice for SSM-S, as it significantly reduces angular error without substantially increasing jitter (i.e., with lower \(\)). Thus, we conclude that, EIHS + ES is the best combination for SSM-S and SSM-T.

**Further Exploration for \(S_{E-T}\).** Fig. 3 shows the visualization of \(S_{E-T}\) under different choices of \(\), and Tab. 5 shows their performance on TotalCapture dataset. It can be observed that the model achieves the best performance when \(=10\). We hypothesize that this means the information from the adjacent 10 frames should be given more emphasis for IMU measurements in our case.

**Contribution of Each Component.** We conduct a thorough ablation study when \(=10\), to investigate the respective contributions of SSM-S and SSM-T. We use the most basic spatial-temporal framework (Fig. 2 (b)) as the baseline model and sequentially add SSM-S and SSM-T. The results are shown in Tab. 3. It can be observed that the primary function of SSM-S is to reduce joint angle error to improve the accuracy of human pose prediction, while the role of SSM-T is to reduce jitter to enhance the coherence of the posture and generate steadier motion sequence.

**In-depth analysis of SSM-S.** To look deeper into SSM-S, we visualized the spatial structure matrix \(S_{E-S}\) (before training), the learnable matrix \(P_{S}\) and the final spatial structure matrix \(S_{EI-S}\) as shown in Fig. 6. It can be observed that:

* The overall pattern of the structure matrix remain the same before and after training (\(S_{E-S}\) and \(S_{EI-S}\)), i.e., the movements between the two hands and the two legs still exhibit high correlation; and the movements of the head and the root (spine) are still negatively correlated. This demonstrates the effectiveness of our \(S_{E-S}\) as initialization/prior.

   Setting & SSM-S & SSM-T & Ang Err & Jitter & \(\) & \(\) & Ang Err & Jitter & \(\) \\ 
1 & IS & IS & 7.79 & 0.54 & 13.37 & 30 & 8.08 & 0.07 & 8.66 \\
2 & IS & ES & 7.94 & 0.09 & 8.69 & 25 & 8.05 & 0.07 & 8.63 \\
3 & IS & EIHS & 7.86 & 0.53 & 13.35 & 20 & 7.76 & 0.07 & 8.32 \\
4 & ES & ES & 8.11 & 0.08 & 8.79 & 15 & 7.85 & 0.08 & 8.50 \\
5 & EIHS & IS & 8.13 & 0.34 & 11.42 & **10** & **6.82** & **0.09** & **7.46** \\ 
6 & EIHS & ES & **6.82** & **0.09** & **7.46** & 5 & 7.33 & 0.13 & 8.34 \\
1 & 7.25 & 0.38 & 10.60 & 1 & 7.25 & 0.38 & 10.60 \\   

Table 4: Ablation study on SSM design.

Figure 5: Error of different joints on DIP-IMU and TotalCapture datasets.

* The learnable matrix \(P_{S}\) adds small offsets to the spatial structure matrix, i.e., slightly suppresses the correlation between two hands, two legs and head vs. root. We attribute this to the different motion distributions among datasets: i) in AMASS, daily actions (e.g., walking, jogging, running, sitting and stretching) are dominant, and the movements of both hands and legs show extremely high consistency; ii) in the DIP-IMU, although daily actions are also the majority, there are a large number of single-hand and single-leg movements, such as single-hand raising, grasping and swinging; single-leg lifting, etc., which weaken the movement consistency of both hands and legs; iii) in the Andy and CIP, there are numerous industry-oriented activities, which are very different from daily movements, resulting in a relatively large adjustment range of the learnable matrix \(P_{S}\). This demonstrates the effectiveness of our \(P_{S}\) in adapting the structure matrix to different datasets, and maintains a high generalization ability.

### Live Demo

We have implemented a real-time pose estimation visualization system using Python and Unity. We select a variety of actions to evaluate the performance of our model in real-world scenarios. These include everyday actions like walking, sitting, kicking, stretching, sports and more. Additionally, we choose some challenging movements such as single-leg standing, rolling, Chinese Kung Fu and dance movements to assess the generalization ability of our model. Through the live demo, it can be observed that even under intense physical activity, our approach still maintains long-term stability and shows robust generalization capability. Please refer to the **supplementary video** for our demo.

## 5 Conclusion

In this paper, we propose a novel sequence structure learning and modulation approach that empowers Transformers with the ability to model and utilize fixed-length sequence structural information. We present a simple yet effective Sequence Structure Module (SSM) to accomplish this, achieving impressive performance in sparse inertial pose estimation tasks. This showcases the powerful potential of the SSM to generalize to other Transformer-based fixed-length sequence tasks.

Figure 6: The visualization of \(S_{E-S}\), \(P_{S}\) and \(S_{EI-S}\). First column: \(S_{E-S}\) obtained using the AMASS dataset before training. First row: results from our model trained on the AMASS, DIP-IMU dataset. Second row: results from our model trained on the AnDy, CIP, and Emokine datasets.

Acknowledgments

This work is supported by National Natural Science Foundation of China (62072383, 61702433), the Public Technology Service Platform Project of Xiamen City (No.3502Z20231043), Xiaomi Young Talents Program/Xiaomi Foundation, the Fundamental Research Funds for the Central Universities. This work is also partially supported by Royal Society (IECNSFC 0211022).