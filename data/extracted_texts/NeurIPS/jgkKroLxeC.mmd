# Unified Graph Augmentations for Generalized Contrastive Learning on Graphs

Jiaming Zhuo\({}^{1}\), Yintong Lu\({}^{1}\), Hui Ning\({}^{1}\), Kun Fu\({}^{1}\), Bingxin Niu\({}^{1}\), Dongxiao He\({}^{2}\),

**Chuan Wang\({}^{3}\), Yuanfang Guo\({}^{4}\), Zhen Wang\({}^{5}\), Xiaochun Cao\({}^{6}\), Liang Yang\({}^{1}\)1**

\({}^{1}\)Hebei Province Key Laboratory of Big Data Calculation,

School of Artificial Intelligence, Hebei University of Technology, Tianjin, China

\({}^{2}\)College of Intelligence and Computing, Tianjin University, Tianjin, China

\({}^{3}\)School of Computer Science and Technology, Beijing JiaoTong University, Beijing, China

\({}^{4}\)School of Computer Science and Engineering, Beihang University, Beijing, China

\({}^{5}\)School of Artificial Intelligence, OPtics and ElectroNics (iOPEN),

School of Cybersecurity, Northwestern Polytechnical University, Xi'an, China

\({}^{6}\)School of Cyber Science and Technology,

Shenzhen Campus of Sun Yat-sen University, Shenzhen, China

jiaming.zhuo@outlook.com, 202332803037@stu.hebut.edu.cn,

ninghui048@163.com, fukun@hebut.edu.cn, niubingxin666@163.com,

hedongxiao@tju.edu.cn, wangchuan@iie.ac.cn, andgyuo@buaa.edu.cn,

w-zhen@nwpu.edu.cn, caoxiaochun@mail.sysu.edu.cn, yangliang@vip.qq.com

corresponding author

###### Abstract

In real-world scenarios, networks (graphs) and their tasks possess unique characteristics, requiring the development of a versatile graph augmentation (GA) to meet the varied demands of network analysis. Unfortunately, most Graph Contrastive Learning (GCL) frameworks are hampered by the specificity, complexity, and incompleteness of their GA techniques. Firstly, GAs designed for specific scenarios may compromise the universality of models if mishandled. Secondly, the process of identifying and generating optimal augmentations generally involves substantial computational overhead. Thirdly, the effectiveness of the GCL, even the learnable ones, is constrained by the finite selection of GAs available. To overcome the above limitations, this paper introduces a novel unified GA module dubbed UGA after reinterpreting the mechanism of GAs in GCLs from a message-passing perspective. Theoretically, this module is capable of unifying any explicit GAs, including node, edge, attribute, and subgraph augmentations. Based on the proposed UGA, a novel generalized GCL framework dubbed Graph cOntrastive UnifieD Augmentations (GOUDA) is proposed. It seamlessly integrates widely adopted contrastive losses and an introduced independence loss to fulfill the common requirements of consistency and diversity of augmentation across diverse scenarios. Evaluations across various datasets and tasks demonstrate the generality and efficiency of the proposed GOUDA over existing state-of-the-art GCLs.

## 1 Introduction

Owing to their effectiveness and efficiency, Graph Neural Networks (GNNs) have become a standard toolkit for processing various graph tasks such as node classification and graph classification [17; 34; 41; 40]. They typically follow a message-passing paradigm , where the representation of each node is updated by aggregating the representations of its adjacent nodes and subsequently combiningthe aggregated representations with itself. In general, to produce discriminative representations, GNNs need to resort to the task-relevant labels (_i.e._, supervised information) to guide the network training, which limits their applicability in the label scarcity scenarios . To overcome this limitation, Graph Contrastive Learning (GCL), a typical graph self-supervised learning architecture, has been developed to provide training guidance by capturing the self-supervised information contained in the graph .

Inspired by the design philosophy of contrastive learning in Computer Vision (CV) , GCLs adopt the same architecture, which consists of three components: augmentation, encoder, and contrastive loss . Thus, GCLs inherit the merit of enabling learning representations invariant to augmentation, which is achieved by maximizing the agreement between embeddings from different perturbations of the same graph . To further improve the representation capacity of GCLs, great endeavors have been made to design augmentations for the original graph, _i.e._, Graph Augmentations (GAs), which target nodes, edges, attributes, and subgraphs. Based on how information is processed, GAs can be divided into two categories: _heuristic_ and _learnable_ methods . The heuristic GAs modify graphs through the combination of fixed, random rules, such as attribute masking , edge removing , and graph diffusion . They tend to neglect the subsequent steps, namely encoding and contrastive optimization, hence leading to suboptimal performances. In contrast, learnable GAs leverage prior knowledge and feedback during training to refine augmentations, already surpassing base augmentations on many tasks. Notable contributions include GAs based on spectral methods , and adversarial training .

Given their inherent and distinct characteristics, various networks and tasks require the meticulous selection of optimal GAs to improve model performance pivotally. However, most GCLs face several limitations regarding the selection: (1) _Specificity_. GCLs are typically tailored with specific GAs to meet the needs of particular scenarios, resulting in a lack of generality across diverse scenarios. For instance, node dropping (specifically, removing nodes and their associated edges), widely applied in graph-level tasks , could significantly compromise the integrity of graphs , rendering it less suitable for node-level tasks. (2) _High complexity_. Either way, identifying and generating the scene-specific GAs impose a considerable computational burden on the models. For example, the set sampling method necessitates a validation of all combinations . Furthermore, the adversarial attack method  entails recalculating contrastive losses, which takes a quadratic complexity of \(O(n^{2})\). Besides, the spectral method requires Laplacian matrix decomposition , which has a cubic complexity of \(O(n^{3})\). (3) _Incompleteness_. Despite the promise of existing learnable GAs in optimizing for specific scenarios, their efficacy is limited by the finite range of GAs at their disposal.

This paper seeks to break these limitations by proposing a unified GA module for GCLs. Toward this end, the mechanisms of existing GAs in GCLs are systemically investigated and reinterpreted from a message-passing perspective . The conclusion is that, from the message-passing perspective, GAs uniformly induce attribute modifications within the neighborhoods of nodes, even though they appear diverse from the spatial perspective, as depicted in Fig. 1. Therefore, the essence of GCLs is to learn node representations invariant to such local augmentation. Drawing from this insight, a novel Unified GA (UGA) module with a simple yet effective design is presented. It strategically interpolates an appropriate amount of Augmentation-Centric (AC) vectors in a graph-structured manner , where AC vectors are treated as another type of node, as illustrated in Fig. 2. In theory, UGA is able to simulate the impact of the above four explicit GAs on target nodes by aggregating features from the AC vectors that capture the attribute variations within the neighborhood of these nodes.

Building upon the proposed UGA module, a generalized GCL framework dubbed Graph cOntrative UnifieD Augmentations (GOUDA) is presented to overcome the above challenges in existing GCLs. This framework adopts a typical dual-channel architecture , corresponding to two distinct augmented graphs (views) with their respective AC matrices, as shown in Fig. 2. To realize general utility, GOUDA proposes to capture the consistency and diversity across augmentations (defined in Section 3.3), which are essential and shared goals for GCLs to be applicable across diverse scenarios. To be specific, the objective function of GOUDA is twofold: (1) maximizing Mutual Information (MI) between representations from these distinct views. (2) maximizing the distributional difference between the AC matrices. The former is a fundamental principle behind classic contrastive losses and inherently ensures consistency, while the latter is a constraint to modulate diversity. In practice, GOUDA is instantiated by leveraging widely employed contrastive losses alongside a Hilbert-Schmidt Independence Criterion (HSIC)-based distributional independence loss. This design makes GOUDA more effective and efficient than GCLs with learnable GAs.

The main contributions of this work are summarized as follows:

* We investigate the mechanism of GAs in GCLs through the lens of message-passing.
* We propose a lightweight GA module named UGA to simulate the impacts of GAs on nodes.
* We introduce GOUDA, an efficient and generalized GCL framework, which captures both consistency and diversity across augmentations.
* Extensive experiments and in-depth analysis demonstrate that GOUDA outperforms state-of-the-art GCLs across various public benchmark datasets and tasks.

## 2 Preliminaries

This section briefly introduces the notations utilized throughout the paper. Subsequently, it outlines the essential components of the Graph Contrastive Learning (GCL) framework.

### Notations

Matrices (_e.g._, \(\)) are in bold capital letters, vectors (_e.g._, \(_{i,.}\), which denotes the \(i\)-th row of \(\)) are in bold lowercase letters, scalars (_e.g._, \(q_{i,.}\), which represents the entry of \(\) at the \(i\)-th row and the \(j\)-th column) are in lowercase letters, and sets (_e.g._, \(\)) are in calligraphic letters.

For a general-purpose description, this paper considers an undirected attribute graph \((,)\), where \(\) stands for the node-set containing \(n\) node instances \(\{(_{v},_{v})\}_{v}\). And \(^{n f}\) and \(^{n c}\) denote the attribute matrix and label matrix of node \(v\), respectively, where \(f\) and \(c\) is the numbers of attributes and labels, respectively. Also, \(=\{e_{i}\}_{i=0}^{m-1}\) terms the edge set containing \(m\) edges. In general, the adjacency matrix \(^{n n}\) is employed to describe the graph topology, such that the matrix form of the graph can be expressed as \((,)\). Moreover, \(^{n d}\) terms the graph representation, where \(d\) terms the dimension of the representation.

### Graph Contrastive Learning

**Graph Augmentations.** Drawing on the successful experience of image augmentation in Computer Vision (CV) [4; 15], Graph Augmentation (GA)  is introduced in graph learning to address the challenge of data scarcity. In the typical GCL frameworks, the input graph \((,)\) is processed through two separate perturbations (GA procedures), formulated as \(_{i}():(,) _{i}(^{(i)},^{(i)})\), to generate its two views (augmented graphs), denoted as \(_{1}(^{(1)},^{(1)})\) and \(_{2}(^{(2)},^{(2)})\). Based on the perturbed information, GAs can be broadly classified into four main categories: _node augmentation_[47; 48], _edge augmentation_[53; 32; 31; 54], _attribute augmentation_[19; 53; 44], and _subgraph augmentation_[47; 13]. An overview of GAs can be found in Section B.

**Graph Encoders.** For efficient processing and analysis, the graph encoders are leveraged to transform raw topology and attribute information of the input graph into low-dimensional vector representations. Most graph encoders in GCLs follow a message-passing paradigm , which typically involves two primary processes: _aggregation_ and _combination_. During these steps, each node iteratively updates its representations by aggregating and combining the node features from its neighborhoods, that is

\[}_{v}^{l}^{l}(\{_{u}^{l-1}|u_{v}\}),_{v}^{l} ^{l}(_{v}^{l-1},}_{v}^{l} ),\] (1)

where \(_{v}^{l}\) terms the representations of node \(v\) in the \(l\)-th layer and \(_{v}\) denotes the set of neighboring nodes of node \(v\). In prevalent GCLs like GRACE , a two-layer GCN  is adopted, where the \(()\) and \(()\) functions are implemented via average function. Thus, there is

\[=^{2}((, ))=(}^{-}} }^{-}(}^{-} }}^{-}^{0} )\,^{1}),\] (2)

where \(()\) denotes the nonlinear activation functions, such as \(()\), and \(}=+_{n}\) stands for the adjacentcy matrix with self-loops, and \(}\) is the corresponding degree matrix, and \(^{l}\) represents the parameter matrix for the \(l\)-th layer. Therefore, for the two augmented graphs, their representations can be obtained by computing \(^{(1)}=^{2}(_{1}(^{(1)},^ {(1)}))\) and \(^{(2)}=^{2}(_{2}(^{(2)}, ^{(2)}))\).

**Contrastive losses.** In line with the InfoMax principle , various contrastive losses are incorporated in GCLs, guiding the training of graph encoders by maximizing the Mutual Information (MI) between the encoded representations on two augmented graphs. Specifically, given two representations \(^{(1)}\) and \(^{(2)}\) obtained from a shared encoder \(_{}\), the general objective of GCL is expressed as

\[*{arg\,max}_{}I(^{(1)};^{(2)}),^{(1)}=_{}(_{1}),\, ^{(2)}=_{}(_{2}),\] (3)

where \(I(X;Y)\) represents the MI between \(X\) and \(Y\). In general, the MI can be approximated using a lower bound estimator, _i.e._, the InfoNCE loss , in the GCLs [53; 20]. This loss can be classified as a sample-level loss because it operates on the sample dimension of the representation matrix. In contrast, Barlow Twins loss , another widely employed loss, is designed to remove redundancies among features and hence can be categorized as a feature-level loss. Both losses are used to implement the proposed framework. Section C provides detailed descriptions of these losses.

## 3 Methodology

### Motivations

As previously mentioned, Contrastive Learning (CL) seeks to learn image representations invariant to augmentations by encouraging the agreement between embedding vectors from the different image distortions. Due to the employment of identical loss functions, typical Graph Contrastive Learning (GCL) inherits the above representation capabilities from CL. Nonetheless, GCLs should emphasize the _local invariance_ owing to the application of graph encoders.

The essence of the graph encoder is to explore the locality of graphs. To be specific, graph encoders in GCLs (generally a 2-layer GCN) follow the message-passing paradigm where node representations are updated in a local aggregation and combination manner, as detailed in Section 2. Given the localizing property of the graph encoder, GAs (_i.e._, node, edge, attribute, and subgraph augmentations), which are typically viewed as global operations in various forms, can be uniformly reinterpreted as attribute modifications in the neighborhoods of nodes, namely, _local augmentations_, as illustrated in Fig. 1.

1) Edge augmentation involves adding and removing perturbed edges in graphs, equivalent to inserting or masking the attributes of nodes connected by these edges in the neighborhood of impacted nodes. For example, the shown edge removing results in the complete attribute masking of partial 2-hop neighbors (nodes 1, 4, 7, and 8) of the target node 0 during message passing. 2) Attribute augmentation essentially replaces the attributes of perturbed nodes in the graph with new ones, which can be viewed as perturbing the neighborhoods of nodes that contain these perturbed nodes. The attribute masking example shows that during the aggregation phase, the attributes of neighbors (nodes 1, 3, 4, 5, 6, 7,

Figure 1: Motivation to unify Graph Augmentations (GAs). A two-hop subgraph example, where the target node is highlighted in red, and the perturbed information is marked in brown. (a) Node augmentation by dropping nodes. (b) Edge augmentation by removing edges. (c) Attribute augmentation by masking attributes. (d) Subgraph augmentation by cropping subgraphs. Existing GAs, typically seen as various forms of global augmentations from the spatial perspective, can be uniformly interpreted as local attribute modifications (_i.e._, local augmentations) from the message-passing perspective.

and 8) on the 2-hop computation graphs of node 0 are masked. 3) Subgraph augmentation is to modify the specific subsets of the graph (including its edges and attributes), which also can be seen as the attribute perturbation in the neighborhoods of target nodes. For node 0, the shown subgraph cropping causes removing nodes 2, 4, 6, and 8 from the 2-hop neighborhood during the message-passing phase. Note that node augmentation is a specific case of subgraph augmentation, where the subset size is one. Thus, the above conclusion regarding subgraph augmentation applies to it.

In short, _the mechanism of GAs in GCLs is to induce attribute modification in the neighborhoods of nodes_. Thus, the essence of GCLs is to learn representations invariant to such local augmentations.

### Unified Graph Augmentation Module

Motivated by the above insights, a unified graph augmentation module dubbed UGA is introduced to implement augmentation efficiently and flexibly. The primary idea is to introduce a collection of Augmentation-Centric (AC) vectors for nodes to simulate and exert the impact of GAs on these nodes, namely, attribute variations in the neighborhood of these nodes. A straightforward implementation of UGA is to align AC vectors one-to-one with nodes, match the size of their features, and then perform feature summation to achieve the desired augmentation.

Given the input graph \(\), the above implementation can be formulated as

\[_{*}=(,), (,):\,(, )(,+),\] (4)

where \(_{*}\) denotes an augmented graph derived from the UGA funtion \((,)\), and \(^{n f}\) terms the matrix of AC vectors \(_{v}^{1 f}\), _i.e._, AC matrix, and \(f\) is the dimension of node attributes.

Fig. 2(a) provides an illustrative example and explains the equivalence between the proposed UGA module and an explicit GA (edge removing). From this, it can be concluded that the UGA module can effectively substitute GAs as long as the combined AC vectors are the representations of cumulative attribute variations within the neighborhoods of nodes induced by these GAs.

**Theorem 3.1**.: _Assuming any augmented graph \(_{*}(^{(*)},^{(*)})\), where \(^{(*)}\) and \(^{(*)}\), with \(\) and \(\) represent the candidate spaces for the augmented adjacency matrix and attribute matrix, respectively, in the proposed implementation of UGA (Eq. 4), there exists an AC matrix \(\) that meets_

\[_{}(,+)=_ {}(^{(*)},^{(*)}),\] (5)

_where \(_{}\) stands for the graph encoder._

This theorem suggests that the proposed UGA module can be equivalent to any existing GA (including node, edge, attribute, and subgraph operations), thereby demonstrating its _unifying capability_ to GAs. Proofs for this theorem are presented in Section D.1. Furthermore, the UGA module possesses an attractive characteristic: _adaptability_, since the AC vectors are capable of dynamically capturing task-relevant perturbation information throughout the training process. Nonetheless, this implementation introduces numerous parameters proportional to the network size, resulting in a significant increase in complexity and the risk of overfitting. To address this limitation, the proposed UGA is reimplemented in a graph-structured manner, where a modest parameter set is utilized, as shown in Fig. 2(b).

**Shared AC vectors.** In graphs, long-range dependencies signify the beyond-local interactions among nodes, represented by similar node attributes and neighborhood patterns [23; 40; 46]. Therefore, it is reasonable to assume that a group of interdependent nodes would benefit from the same optimal GAs. Thus, a shared AC matrix \(=[_{i,:}]_{i=0}^{k-1}\) is introduced, where \(k n\).

**Propagation mode.** AC vectors propagate their features to nodes via a general attention mechanism, in which structural features (_e.g._, positional/structure encodings [1; 7]) are employed to calculate the attention scores. Formally, the proposed UGA module can be reformulated as

\[}_{v,:}=_{v,:}+_{i=0}^{k-1}b_{v,i}_{i,:}, b_{v,i}=([_{v,:}|| _{v,:}])_{i,:}^{})}{_{t=0}^{k} (([_{v,:}||_{v,:}])_{ i,:}^{})},\] (6)

where \(b_{v,i}\) stands for the propagation weight from \(i\)-th AC vector to node \(v\) within matrix \(^{n k}\), and \(([_{v}||_{v}])^{1 f}\) terms an integrated representation of node \(v\), which concatenates the node attributes \(_{v}\) and structural features \(_{v}^{t}\). This paper adopts \(t\)-steps random-walk encodings  as the structure features. \(():^{f+t}^{f}\) denotes a projection layer.

**Processing.** Keeping a subset of the salient connections facilitates propagation and enhances computational efficiency. Thus, propagation weights below a predefined threshold are zeroed out, namely

\[b_{v,i}=\{ b_{v,i},& b_{v,i}> \\ 0,&,.\] (7)

where \(\) terms a threshold value. Then, the obtained weight \(\) is applied in propagation, per Eq.6.

### Generalized Graph Contrastive Learning Framework

Building upon the UGA module, a novel GCL framework named Graph cOntrastive UnifieD Augmentations (GOUDA) is proposed to achieve generality across diverse tasks and graphs. It utilizes the standard two-channel architecture (in Eq. 3), with each channel generating an augmented graph through the UGA module and its AC matrix, as depicted in Fig. 2. Unlike traditional GCLs, GOUDA introduces a term to constrain the two AC matrices (denoted as \(\) and \(\)).

Specifically, GOUDA optimizes the following objective function:

\[*{argmax}_{}\;I(}(_{1});}(_{2}))+(,),\] (8)

where \(I(X;Y)\) stands for the Mutual Information (MI) between \(X\) and \(Y\), and \(}\) denotes a graph encoder shared between two views (or channels). \(_{1}=(,)\) and \(_{2}=(,)\) represent two augmented graphs, and \((,)\) denotes the constraint between \(\) and \(\).

**Definition 3.2**.: (Consistency across augmentations). Let \(^{(i)}\) and \(^{(j)}\) denote the representations of graphs \(_{i},_{j}_{}\), respectively, where \(j i\), and \(_{}\) denotes the family of graphs derived from a series of parametric graph augmentations. Consistency across augmentations for node \(v\) is defined as

\[_{v}=(_{v}^{(i)},_{v}^{(j)} ),\] (9)

where \((X,Y)\) terms the distributional similarity between \(X\) and \(Y\).

This consistency implies that augmentation should minimally impact the similarity between representations from different augmented graphs for the same nodes to preserve the intrinsic semantic integrity of the nodes. Note that the first term in the objective function of GOUDA, namely the MI maximization, essentially is a constraint for semantic consistency. Thus, the augmentation learned by the UGA module is capable of ensuring the desired property.

**Definition 3.3**.: (Diversity across augmentations). Given two augmented graphs \(_{i}(^{(i)},^{(i)})\) and \(_{j}(^{(j)},^{(j)})_{}\), where \(j i\), let \(_{v}^{k}\) represents the \(k\)-hop subgraph centered at node \(v\), and let \((,)\) stands for the measure of distributional difference. Diversity across augmentations is defined as

\[_{v}=((_{_{v}^{k}} ),(_{_{v}^{k}}^{(j)})),\] (10)

Figure 2: Illustration of the proposed unified module UGA and generalized framework GOUDA. (a) An intuitive example of the equivalence between GAs (_e.g._, edge removing) and the aggregation of Augmentation-Centric (AC) vectors, which capture the local attribute variations caused by these GAs. (b) The proposed generalized GCL framework GOUDA. The independence loss, which is directly computed on AC vectors, is designed to ensure diversity across different augmentations.

where \(_{_{v}^{k}}^{(i)}^{n_{v} f}\) stands for the attribute matrix of nodes in the \(k\)-hop subgraph of node \(v\), and \(n_{v}\) is the number of neighbors for node \(v\). \(()\) terms a combination function, such as \(()\).

This definition is based on the conclusion in Section 3.1, namely, the mechanism of GAs in GCLs is to modify attributes within the node neighborhoods. Accordingly, another objective for augmentation is the minimization of the local attribute overlap between augmented graphs, ensuring the model does not overfocus on the specific features of a single distribution.

In the proposed GOUDA framework, local attribute variations for each node are represented by AC vectors, _e.g._, \(_{v}\) and \(_{v}\), with the augmentation being generated by aggregating features from these vectors. Therefore, the diversity across augmentations can be quantified using two AC matrices (\(\) and \(\)) corresponding to two distinct views, which is supported by the following analysis.

**Theorem 3.4**.: _Let \((X,Y)=\|X-Y\|_{F}^{2}\) stands for the distributional difference, and let \(()=()\) terms the combination function. In GOUDA (in Eq. 8), the diversity across augmentations (in Eq. 10) can be approximated by the distributional difference between AC matrices \(\) and \(\), that is_

\[_{v}=\|_{t_{v} v}(_{t,:}^{(1)}+ }_{t,:})-_{t_{v} v}(_{t,:}^{(2 )}+}_{t,:})\|_{F}^{2}\|-\| _{F}^{2},\] (11)

_where \(}_{t,:}=_{t,:}^{q}\) denote the features propagated from AC matrices \(\) to node \(t\)._

Theorem 3.4 shows that the diversity across augmentations can be controlled by imposing constraints on the AC matrices, particularly through the second term in the objective function of GOUDA. Refer to Section D.2 for the proofs. In brief, maintaining a balance between consistency and diversity across augmentations is crucial for the effectiveness of GCLs. Specifically, diversity encourages exploring and exploiting the local attribute variations while consistency anchors the learned representations to the original semantics.

### Instantiation of GOUDA

This subsection introduces a practical implementation of the proposed GOUDA framework (Eq.8). The overview of this framework is depicted in Fig. 2, while the step-by-step procedure is detailed in Algorithm 1. The objective of GOUDA is to learn the discriminative and robust representations. To achieve this, it seeks to train the graph encoder \(_{}\) to maximize the Mutual Information (MI) between representations from two augmented graphs \(_{1}=(,)\) and \(_{2}=(,)\), while simultaneously maintaining consistency and diversity in the augmentation process.

**Estimation of Mutual Information (MI).** The first term of GOUDA is implemented utilizing the sample-level InfoNCE loss (in Eq. 22), which serves as a lower bound estimator for MI, and the feature-level Barlow Twins loss (in Eq. 24). This term is denoted as contrastive loss \(_{}\). Owing to limited space, the above losses are introduced in Section C.

**Distributional independence loss.** A distributional independence loss is introduced to instantiate the second term of GOUDA. Specifically, the Hilbert-Schmidt Independence Criterion (HSIC)  is adopted to measure the statistical dependence between two augmentation distributions. Furthermore, the Gram matrices derived from HSIC are constrained to minimize their off-diagonal elements. To be concrete, the independence loss is formulated as

\[_{}=)}}_{}+_{1}_{i}_{j i}k_{i,j}+_{2 }_{i}_{j i}l_{i,j},\] (12)

where \(\) and \(\) stand for the Gram matrices of \(\) and \(\), respectively, defined by \(k_{i,j}=(_{i:,},_{j:})\) and \(l_{i,j}=(_{i:,},_{j:})\). In practice, the kernel function \(()\) is defined as the linear kernel, specifically \(k_{i,j}=_{i:,}_{j:,}^{T}\). Additionally, \(=_{n}-^{}\) represents the centering matrix, where \(^{n n}\) and \(_{n}^{n 1}\) denote the identity matrix and all-one column vector, respectively. \(_{1}\) and \(_{2}\) represent two hyper-parameters. Minimizing this term serves two purposes: on the one hand, it enhances the diversity across augmentations by amplifying the differences between two distributions, and on the other hand, it avoids trivial solutions by increasing the differences among the augmentation elements (\(_{i,:}\)) within each distribution.

**Objective.** The overall objective function of GOUDA is a weighted sum of these two terms, that is

\[=_{}+_{},\] (13)

where \(\) denotes a hyperparameter used to trade off two terms.

### Complexity Analysis

This section evaluates the complexity of the proposed GOUDA framework in comparison to the baseline GCLs configured with learnable GAs, including SPAN, JOAO, and AD-GCL. As illustrated in Tab. 1, GOUDA introduces lighter computational overhead compared to these baselines. For a detailed description of the complexity, refer to Section E.4.

## 4 Experiments

This section evaluates the effectiveness and generality of the proposed GOUDA through a comprehensive comparison against multiple baselines across tasks at both the node-level (node classification and node clustering) and the graph-level (graph classification) tasks. Furthermore, it conducts several additional experiments to deepen the understanding of this framework. For an exhaustive account of datasets, baselines, configurations, and hyper-parameters, refer to Section E.

**Datasets.** The experiment utilizes ten benchmark datasets, namely: Cora , CiteSeer , PubMed , Wiki-CS , Photo , Computers , and Physics  for node-level tasks, and IMDB-B , IMDB-M , and COLLAB  for graph-level tasks. See Section E.1 for dataset descriptions.

**Baselines.** The baseline models comprise two supervised graph neural networks (GCN , GAT ) and eleven self-supervised graph learning models (DGI , GMI , MVGRL , GRACE , GCA , BGRL , CCA-SSG , GBT , SPAN , DSSL , HomoGCL ) for node-level tasks. Four self-supervised learning models are compared (InfoGraph , GraphCL , JOAO , AD-GCL ) for graph-level tasks. Refer to Section E.3 for model introductions.

### Experimental Results

**Node Classification.** It can be observed from Tab. 2, which exhibits the results of node classification tasks, that the proposed GOUDA outperforms the unsupervised baselines in six of the seven datasets. This demonstrates the superiority of GOUDA. Furthermore, on the CiteSeer dataset, notable performance improvements are observed with both models, GOUDA-IF and GOUDA-BT, surpassing

   Model & Time Complexity & Description \\  SPAN  & \(O(n^{2}tk)\) & Eigendecomposition-based edge augmentation. \\ JOAO  & \(O(n^{2}d)\) & Min-max optimization-based augmentation. \\ AD-GCL  & \(O(n^{2}d)\) & Adversarial-training-based edge augmentation. \\  GOUDA (Ours) & \(O(nkf)\) & Consistency-diversity balanced augmentation. \\   

Table 1: Comparison of time complexity in the _augmentation_ Phase. \(n\) denotes the size of the graph.

   Model & Input & Cora & CiteSeer & PubMed & Wiki-CS & Photo & Computers & Physics \\  GCN & A, X, Y & \(82.32_{+1.79}\) & \(72.13_{+1.17}\) & \(84.90_{+0.38}\) & \(76.89_{+0.37}\) & \(92.35_{+0.25}\) & \(86.34_{+0.48}\) & \(95.65_{+0.16}\) \\ GAT & A, X, Y & \(83.34_{+1.57}\) & \(72.44_{+1.42}\) & \(85.21_{+0.36}\) & \(77.42_{+0.19}\) & \(92.35_{+0.25}\) & \(87.06_{+0.35}\) & \(95.47_{+0.15}\) \\  DGI & A, X & \(82.60_{+0.40}\) & \(71.49_{+0.14}\) & \(86.00_{+0.14}\) & \(75.73_{+0.13}\) & \(91.49_{+0.25}\) & \(84.09_{+0.39}\) & \(94.51_{+0.52}\) \\ GMI & A, X & \(82.51_{+1.47}\) & \(71.56_{+0.56}\) & \(84.83_{+0.90}\) & \(75.06_{+0.13}\) & \(90.72_{+0.33}\) & \(81.76_{+0.52}\) & \(94.10_{+0.61}\) \\ MVGRL & A, X & \(83.03_{+0.27}\) & \(72.75_{+0.46}\) & \(85.63_{+0.38}\) & \(77.97_{+0.18}\) & \(92.01_{+0.13}\) & \(87.09_{+0.27}\) & \(95.33_{+0.03}\) \\ GRACE & A, X & \(83.30_{+0.04}\) & \(71.41_{+0.38}\) & \(86.51_{+0.34}\) & \(79.16_{+0.36}\) & \(92.65_{+0.32}\) & \(87.21_{+0.44}\) & \(95.26_{+0.02}\) \\ GCA & A, X & \(83.90_{+0.41}\) & \(72.21_{+0.24}\) & \(86.01_{+0.75}\) & \(79.35_{+0.12}\) & \(92.78_{+0.17}\) & \(87.84_{+0.27}\) & \(95.68_{+0.05}\) \\ BGRL & A, X & \(83.77_{+0.75}\) & \(71.99_{+0.42}\) & \(84.94_{+0.17}\) & \(78.74_{+0.22}\) & \(93.24_{+0.29}\) & \(88.92_{+0.33}\) & \(95.63_{+0.04}\) \\ GBT & A, X & \(83.89_{+0.66}\) & \(72.57_{+0.61}\) & \(85.71_{+0.32}\) & \(76.65_{+0.62}\) & \(92.63_{+0.44}\) & \(88.14_{+0.33}\) & \(95.07_{+0.17}\) \\ CCA-SSG & A, X & \(84.39_{+0.68}\) & \(73.81_{+0.38}\) & \(86.21_{+0.67}\) & \(87.94_{+0.17}\) & \(93.14_{+0.14}\) & \(88.74_{+0.28}\) & \(95.38_{+0.06}\) \\ SPAN & A, X & \(85.09_{+0.28}\) & \(73.68_{+0.33}\) & \(85.35_{+0.29}\) & \(79.01_{+0.51}\) & \(92.68_{+0.31}\) & \(89.68_{+0.19}\) & \(95.12_{+0.15}\) \\ DSSL & A, X & \(84.52_{+0.71}\) & \(73.93_{+0.89}\) & \(85.59_{+0.28}\) & \(79.98_{+0.67}\) & \(93.08_{+0.38}\) & \(89.06_{+0.49}\) & \(95.29_{+0.29}\) \\ HomoGCL & A, X & \(84.89_{+0.71}\) & \(73.78_{+0.63}\) & \(86.37_{+0.49}\) & \(79.29_{+0.32}\) & \(92.92_{+0.18}\) & \(88.46_{+0.20}\) & \(95.18_{+0.09}\) \\  GOUDA-IF & A, X & \(_{+0.55}\) & \(_{+0.97}\) & \(_{+0.10}\) & \(_{+0.28}\) & \(_{+0.32}\) & \(_{+0.17}\) & \(_{+0.14}\) \\ GOUDA-BT & A, X & \(_{+0.31}\) & \(74.47_{+1.05}\) & \(_{+0.02}\) & \(_{+0.30}\) & \(_{+0.19}\) & \(_{+0.11}\) & \(}_{+0.21}\) \\   

Table 2: Accuracy in percentage (mean\({}_{}\)) over ten trials of node classification across seven graphs. Best and runner-up models are highlighted in **bolded** and underlined, respectively.

the baselines GRACE and GBT. Specifically, the accuracy of GOUDA-IF surpasses that of GRACE by \(3.14\%\), and similarly, the accuracy of GOUDA-BT exceeds that of GBT by \(1.90\%\). Note that the baselines GRACE and GBT adopt identical encoders and contrastive losses as GOUDA-IF and GOUDA-BT, respectively. Therefore, the observed performance improvement can be attributed to the adaptive modeling capacity for augmentations of the proposed GOUDA.

**Node Clustering.** One can draw two conclusions from the observations in Tab. 3. Firstly, it is evident that GOUDA consistently surpasses all baselines (_e.g._, GRACE, and GBT) across all datasets, which illustrates the superior representation capacity of GOUDA. This can be attributed to the self-adaptive learning ability of the proposed module UGA. Secondly, GOUDA-IF consistently outperforms GOUDA-BT, suggesting that within the proposed GOUDA framework, the InfoNCE loss more effectively captures local information for clustering than the BarlowTwins loss.

**Graph Classification.** The results of this experiment are presented in Tab. 4 and Fig. 3. Firstly, it can be observed from Tab. 4 that GOUDA outperforms the baselines regarding classification performance, which illustrates the general validity of GOUDA. In particular, GOUDA-IF and GOUDA-BT surpass the second-place MVGRL by \(1.02\%\) and \(2.6\%\), respectively, on the IMDB-B dataset, which highlights the superiority of GOUDA. Moreover, GOUDA exceeds the GCLs employing learnable GAs, _i.e._, JOAO, and AD-GCL. This can be due to the unified ability of UGA to integrate diverse GAs, which provides GOUDA with broader augmentation options than the baseline. Secondly, as illustrated in Fig. 3, GOUDA achieves superior performance and consumes less time than the baselines. Specifically, two triangles, indicating the proposed GOUDA, are superior left and above the other shape in the figure. This implies GOUDA is lightweight, aligning with conclusions in Section 3.5. Besides, UGA introduces modest memory usage, which promises the scalability of GOUDA.

### Additional Experiments

**Robustness Analysis.** This experiment aims to evaluate the robustness of GOUDA against topology attacks (adding edges) and attribute attacks (flipping attributes). According to results in Fig. 4 and Fig. 5, several conclusions can be derived. Firstly, compared to the baselines using the same contrastive losses, GOUDA consistently achieves performance gains at all perturbation rates. It demonstrates the robustness of GOUDA against both topology and attribute attacks. This is attributed to the greater adaptability of the UGA module, stemming from its integration of augmentation and contrastive updating over random GAs. Secondly, attribute attacks cause more severe performance degradation than topology attacks, even for our proposed GOUDA. This could be because the node attributes, rich with class-discriminative information, are erased essential identification info during attacks.

**Ablation Study.** This experiment aims to evaluate the contribution of individual components. To be specific, it introduces two variants: one without the structure features (in Eq. 6) and another without the independence loss (in Eq. 13). From Fig. 6, it is observable that the performance declined in both model variants compared to the complete model, which illustrates that the efficacy of GOUDA stems from the collective contribution of all components. Besides lacking Independence loss, GOUDA-IF performs inferior to GOUDA-BT, implying that InfoNCE might drive the model toward excessive

   Model & IMDB-B & IMDB-M & COLLAB \\  Infograph & \(73.03_{ 0.87}\) & \(49.69_{ 0.53}\) & \(82.00_{ 0.29}\) \\ GraphCL & \(71.14_{ 0.44}\) & \(48.58_{ 0.67}\) & \(71.36_{ 1.15}\) \\ JOAO & \(71.60_{ 0.86}\) & \(49.20_{ 0.77}\) & \(70.40_{ 2.21}\) \\ AD-GCL & \(71.49_{ 0.90}\) & \(50.36_{ 0.74}\) & \(74.89_{ 0.90}\) \\ MVGRL & \(74.20_{ 0.70}\) & \(51.20_{ 0.50}\) & \(73.10_{ 0.60}\) \\  GOUDA-IF & \(75.22_{ 0.94}\) & \(52.43_{ 0.83}\) & \(_{ 2.33}\) \\ GOUDA-BT & \(_{ 0.98}\) & \(_{ 0.72}\) & \(_{ 2.17}\) \\   

Table 4: Performances on graph classification: accuracy in percentage (mean\({}_{ }\)).

Figure 3: Comparisons in terms of performance, running time, and GPU memory usage. The marker size indicates memory usage.

    &  &  &  \\   & NMI & ARI & NMI & ARI & NMI & ARI \\  DGI & \(52.75\) & \(47.78\) & \(40.43\) & \(41.84\) & \(30.03\) & \(29.78\) \\ MVGRL & \(54.21\) & \(49.04\) & \(43.26\) & \(42.73\) & \(30.75\) & \(30.42\) \\ GRACE & \(54.59\) & \(48.31\) & \(43.02\) & \(42.32\) & \(31.11\) & \(30.37\) \\ GBT & \(55.32\) & \(48.91\) & \(44.01\) & \(42.61\) & \(31.33\) & \(30.64\) \\ CCA-SSG & \(56.38\) & \(50.62\) & \(43.98\) & \(42.79\) & \(32.06\) & \(31.15\) \\  GOUDA-IF & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ GOUDA-BT & \(57.35\) & \(51.84\) & \(44.93\) & \(43.46\) & \(33.14\) & \(31.73\) \\   

Table 3: Performances on node clustering: NMI & ARI Scores in percentage (mean).

consistency, diminishing its discriminative power. This highlights the critical role of independence loss in preserving diversity across augmentations.

**Parameter Sensitivity Analysis.** These experiments are performed to offer an intuitive understanding of hyper-parameter selection. Firstly, as depicted in Fig. 7, which illustrates performance variance for varying \(k\), GOUDA achieves consistently stable performances across \(\{5,10,20\}\). Notably, the performance variation on these datasets remains minimal, staying within a \(2\%\) margin. Thus, GOUDA has low sensitivity to parameter \(k\). Moreover, this parameter requires no significant value for GOUDA to perform well; a setting as low as \(5\) suffices. However, a value of \(1\) is inadequate due to the absence of augmentation diversity. The analysis of other hyper-parameters is given in Section E.6.

## 5 Conclusions

In this paper, we present UGA, a unified Graph Augmentation (GA) module that addresses the issues in existing GAs, including specificity, complexity, and incompleteness. Motivated by the local attribute-modifying characteristics of GAs, UGA introduces a moderate number of Augmentation-Centric (AC) vectors to simulate GA impact on nodes. We further propose GOUDA, a generalized Graph Contrastive Learning (GCL) framework built on UGA. GOUDA promotes both consistency and diversity across augmentations by employing a contrastive loss and an independence loss, respectively. Extensive evaluations demonstrate the generality and efficiency of GOUDA. However, the robustness analysis suggests a scope for enhancement in its robustness against attribute attacks. Future research could explore multi-modal learning methods that fuse diverse structural features into node attributes, aiming to better preserve discriminative information and thus enhance robustness.

## 6 Acknowledgments

This work was supported in part by the National Key R\(\&\)D Program of China (No. 2022ZD0119202), in part by the National Natural Science Foundation of China (No. U22B2036, 62376088, 62276187, 62102413, 62272020), in part by the Hebei Natural Science Foundation (No. F2024202047), in part by the Hebei Province Higher Education Science and Technology Research Project (No. QN2024201), in part by the National Science Fund for Distinguished Young Scholarship (No. 62025602), and in part by the XPLORER PRIZE.