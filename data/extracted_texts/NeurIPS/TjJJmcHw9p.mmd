# Exact recovery and Bregman hard clustering of node-attributed Stochastic Block Model

Maximilien Dreveton

School of Computer and Communication Sciences

EPFL, Lausanne, Switzerland

maximilien.dreveton@epfl.ch

&Felipe S. Fernandes

Systems Engineering and Computer Science

Federal University of Rio de Janeiro

Rio de Janeiro, Brazil

felipesc@cos.ufrj.br

&Daniel R. Figueiredo

Systems Engineering and Computer Science

Federal University of Rio de Janeiro

Rio de Janeiro, Brazil

daniel@cos.ufrj.br

###### Abstract

Network clustering tackles the problem of identifying sets of nodes (communities) that have similar connection patterns. However, in many scenarios, nodes also have attributes that are correlated with the clustering structure. Thus, network information (edges) and node information (attributes) can be jointly leveraged to design high-performance clustering algorithms. Under a general model for the network and node attributes, this work establishes an information-theoretic criterion for the exact recovery of community labels and characterizes a phase transition determined by the Chernoff-Hellinger divergence of the model. The criterion shows how network and attribute information can be exchanged in order to have exact recovery (e.g., more reliable network information requires less reliable attribute information). This work also presents an iterative clustering algorithm that maximizes the joint likelihood, assuming that the probability distribution of network interactions and node attributes belong to exponential families. This covers a broad range of possible interactions (e.g., edges with weights) and attributes (e.g., non-Gaussian models), as well as sparse networks, while also exploring the connection between exponential families and Bregman divergences. Extensive numerical experiments using synthetic data indicate that the proposed algorithm outperforms classic algorithms that leverage only network or only attribute information as well as state-of-the-art algorithms that also leverage both sources of information. The contributions of this work provide insights into the fundamental limits and practical techniques for inferring community labels on node-attributed networks.

## 1 Introduction

Community detection or network clustering-the task of identifying sets of similar nodes in a network- is a fundamental problem in network analysis , with applications in diverse fields such as digital humanities, data science and biology. In the classic formulation, a set of communities must be determined from the connection patterns among the nodes of a single network. A simple random graph model with community structure, the Stochastic Block Model (SBM), has been the canonical model to characterise theoretical limitations and evaluate different community detection algorithms .

However, nodes of many real-world networks have attributes or features that can reveal their identity as an individual or within a group. For example, the age, gender and ethnicity of individuals in a social network , the title, keywords and co-authors of papers in a citation network , or the longitude and latitude of meteorological stations in weather forecast networks . In some scenarios, such attributes can be leveraged alone to identify node communities (clusters) without even using the network.

Thus, a modern formulation for community detection must consider network information (edges) and node information (attributes). Indeed, recent works have designed community detection algorithms that can effectively leverage both sources of information to improve performance. Various methods have been proposed for clustering node-attributed networks, including modularity optimisation , belief propagation , expected-maximisation algorithms , information flow compression , semidefinite programming , spectral algorithms  and iterative likelihood based methods .

A fundamental problem in this new formulation is fusing both sources of information: how important is network information in comparison to node information given a problem instance? Intuitively, this depends on the noise associated with network edges and node attributes. For example, if edges are reliable then the clustering algorithm should prioritize them when determining the communities. However, most prior approaches adopt some form of heuristic when merging the two sources of information . A rigorous approach to this problem requires a mathematical model, and one has been recently proposed.

The _Contextual Stochastic Block Model_ (CSBM) is a generalization of the SBM where each node has a random attribute that depends on its community label. While the model formulation is general (in terms of distribution for edges and attributes), CSBM has only been rigorously studied in the restrictive setting where the pairwise interactions are binary (edges are present or not) and the node attributes are Gaussian . In this scenario, the phase transitions for exactly recovering the community labels and for detecting them better than a random guess has been established. Moreover, the comparison with the respective phase transitions in SBM  and in Gaussian mixture model  (with no network) highlight the value of jointly leveraging network and node information in recovering community labels.

However, real networks often depart from binary edges and Gaussian attributes. Indeed, in many scenarios network edges have weights that reveal information about that interaction and nodes have discrete or non-Gaussian attributes. This work tackles this scenario by considering a CSBM where edges have weights and nodes have attributes that follow some arbitrary distributions. Under this general model, this work is the first to characterise the phase transition for the exact recovery of community labels. In particular, the _Chernoff-Hellinger divergence_, initially defined just for binary networks , is extended to this more general model. This divergence effectively captures the difficulty of distinguishing different communities and thus plays a crucial role in determining the limits of exact recovery. The analysis reveals an additional term in the divergence that quantifies the information provided by the attributes of the nodes. Moreover, it quantifies the trade-off between network and node information in meeting the threshold for exact recovery.

The CSBM generates weighted networks that are complete (all possible edges are present) when edge weights follow a continuous distribution. However, most weighted real networks are sparse. To model sparse weighted networks and to provide a practical community detection algorithm, we consider a CSBM whose weights belong to _zero-inflated distributions_. More precisely, we suppose that conditioned on observing an edge, the distribution of the weight of this edge belongs to an exponential family. Similarly, the node attribute distributions are also assumed to belong to an exponential family. Working with exponential families is motivated by two factors. Firstly, exponential families encompass a broad range of parametric distributions, including the commonly used Bernoulli, Poisson, Gaussian, or Gamma distributions. Secondly, there exists an intricate connection between exponential families of distributions and Bregman divergences, which has proven to be a powerful tool for developing algorithms across a variety of problems such as clustering, classification, and dimensionality reduction .

This connection between Bregman divergences and exponential families has been previously explored in the context of clustering dense networks (all possible edges are present) . In contrast, this work proposes an iterative algorithm that maximizes the log-likelihood of the model, for both dense and sparse networks. This is a key difference with many previous works which either study only dense weighted networks [9; 24] or binary networks with Gaussian attributes [2; 15; 34]. Simulations on synthetic networks demonstrate that our algorithm outperforms state-of-the-art approaches in various settings, providing practical techniques for achieving accurate clustering results.

The article is structured as follows. The relevant related work is discussed in Section 2. Section 3 introduces the model under consideration along with the main theoretical contributions on exact recovery. Section 4 focuses on sparse networks with edge weights and node attributes drawn from exponential families and introduces an iterative algorithm for clustering such networks. Numerical results and comparisons to prior works are presented in Section 5, and Section 6 concludes the paper.

NotationsLet \((p)\) denote a Bernoulli random variable (r.v.) with parameter \(p\), \((m,)\) a Gaussian r.v. with mean \(m\) and standard deviation \(\), and \(()\) an exponential r.v. with mean \(^{-1}\). The notation \([K]\) refers to the set \(\{1,,K\}\), while \(A_{i}\). stands for the \(i\)-th row of matrix \(A\).

## 2 Related work

### Exact recovery in SBM with edge weights and node attributes

Community detection in classic SBM (binary edges) is a well-understood problem with strong theoretical results concerning exact recovery and efficient algorithms with guaranteed accuracy [1; 39]. However, extending the classic SBM to weighted networks (non-binary edges) with arbitrary distributions is an ongoing research area. Most existing work in this scenario has been restricted to the _homogeneous model_1, where edge weights within and across communities are determined by two respective distributions. Moreover, existing works often restrict to categorical or real-valued weights [22; 36], or to multiplex networks (multiple edge types) with independent and identically distributed layers . However, a recent work has provided a strong theoretical foundation the homogeneous model with arbitrary distributions , highlighting the role of the Renyi divergence as the key information-theoretic quantity for the homogeneous model.

In non-homogeneous models, a more complex divergence called the Chernoff-Hellinger divergence is the appropriate information-theoretic quantity for exact community recovery . However, the expression of the Chernoff-Hellinger divergence as originally defined in  for binary networks does not have an intuitive interpretation, and its extension to non-binary (weighted) networks is challenging. For example, the exact recovery threshold for non-homogeneous SBM whose edges are categorical random variables has been established , but this threshold is expressed as a condition involving the minimization of a mixture of Kullback-Leibler divergences over the space of probability distributions. Although the relationship between Kullback-Leibler and Chernoff divergences are known (see for example [35; Theorem 30-32]), the specific technical lemma required to link them to the Chernoff-Hellinger divergence is not straightforward (see [38; Claim 4]).

Another generalization of the SBM allows for nodes to have attributes that provide information about their community, such as the Contextual SBM (CSBM) . The CSBM has only been rigorously studied in the setting where edges are binary and node attributes follow a Gaussian distribution. In this scenario, the phase transition for exact recovery for the community labels has been established [2; 8; 15]. A natural generalization is to investigate the model where network edges have weights and nodes have attributes that follow arbitrary distributions. Indeed, this is one of the main contributions of this work: Expression (3.4) gives a straightforward yet crucial formula for the phase transition for exact recovery, also providing a natural interpretation for the influence of both the network and node attributes. Moreover, Expression (3.4) also applies when no node attribute is available, thus providing the exact recovery threshold for a non-homogeneous model and arbitrary edge weight distribution, a significant advancement in the state of the art.

### Algorithms for clustering weighted networks with node attributes

Algorithms leveraging different approaches have been proposed to tackle community detection in networks with edge weights and node attributes. A common principled approach is to determine the community assignment that maximizes the likelihood function of a model for the data. However, optimizing the likelihood function is computationally intractable even for binary networks. Thus,approximation schemes such as variational inference and pseudo-likelihood methods are often adopted. For instance,  introduced a variational-EM algorithm for clustering non-homogeneous weighted SBM with arbitrary distributions. Another approach for clustering node-attributed SBM whose edge weights and attribute distributions belong to exponential families is . These two approaches assume that the network is dense (all edges are present and have non-zero edge weight). However, most real networks are very sparse (most node pairs do not have an edge) and this work focuses on this scenario. Another very recent work tackling sparse networks is the _IR_sLs_ algorithm from , although its theoretical guarantees assume binary networks with Gaussian attributes.

The iterative clustering algorithm presented in this work maximizes the pseudo-likelihood likelihood by assuming that the probability distribution of network edges and node attributes belong to exponential families. This yields a direct connection with Bregman divergences and establishes an elegant expression for the likelihood function. This connection has also been leveraged in , however, their model is restricted to dense weighted networks (all edges are present). This work (more specifically, Lemma 2), demonstrates that this connection can also be applied to sparse weighted networks (using zero-inflated distributions to model the weights). This extension enhances the applicability of pseudo-likelihood algorithms using Bregman divergence to a broader class of scenarios, namely weighted sparse networks with node attributes.

## 3 Model and exact recovery in node-attributed SBM

### Model definition

Consider a population of \(n\) objects, called _nodes_, partitioned into \(K 2\) disjoint sets, called _blocks_ or communities. A node-labelling vector \(z=(z_{1},,z_{n})[K]^{n}\) represents this partitioning so that \(z_{i}\) indicates the block of node \(i\). The labels (blocks) of nodes are random variables assumed to be independent and identically distributed such that \((z_{i}=k)=_{k}\) for some vector \((0,1)^{K}\) verifying that \(_{k}_{k}=1\). The nodes interact in unordered pairs giving rise to undirected edges, and \(\,\) is the measurable space of all possible pairwise interactions. Additionally, each node has an attribute that is an element of a measurable space \(\). Let \(X^{N N}\) denote the symmetric matrix such that \(X_{ij}\) represents the interaction between node pair \((ij)\), and by \(Y=(Y_{1},,Y_{n})^{n}\) the node attribute vector.

Assume that interactions and attributes are independent conditionally on the community labels of the nodes. Let \(f_{k}(x)\) denote the probability that two nodes in blocks \(k\) and \(\) have an interaction \(x\), and \(h_{k}(y)\) denote the probability that a node in block \(k[K]\) has an attribute \(y\). Thus,

\[(X,Y\,|\,z)\ =\ _{1 i<j n}f_{z_{i}z_{j}}(X_{ ij})_{i=1}^{n}h_{z_{i}}(Y_{i}).\] (3.1)

In the following, the interaction spaces \(,\) might depend on \(n\), as well as the respective interaction probabilities \(f,h\). The number of nodes \(n\) will increase to infinity while \(K\) and \(\) are constant. For an estimator \([K]^{n}\) of \(z\), we define the _classification error_ as

\[(z,)\ =\ _{_{K}} (z,),\]

where \(_{K}\) is the set of permutations of \([K]\) and \((,)\) is the hamming distance between two vectors. An estimator \(=(X,Y)\) achieves _exact recovery_ if \(((z,) 1)=o(1)\).

### Exact recovery threshold in node-attributed SBM

The difficulty of classifying empirical data in one of \(K\) possible classes is traditionally measured by the _Chernoff information_. More precisely, in the context of network clustering, let \((a,b)=(a,b,,f,h)\) denote the hardness of distinguishing nodes that belong to block \(a\) from block \(b\). This quantity is defined by

\[(a,b)\ =\ _{t(0,1)}_{t}(a,b),\] (3.2)

where

\[_{t}(a,b)\ =\ (1-t)[_{c=1}^{K}_{c} _{t}(f_{bc}\|f_{ac})+_{t}(h_{b}\|h _{a})]\] (3.3)is the _Chernoff coefficient_ of order \(t\) across blocks \(a\) and \(b\), and \(_{t}(f\|g)= f^{t}(x)g^{1-t}(x)dx\) is the _Renyi divergence_ of order \(t\) between two probability densities \(f,g\). The key quantity assessing the possibility or impossibility of exact recovery in SBM is then the minimal Chernoff information across all pairs of clusters. We denote it by \(I=I(,f,h)\), and it is defined by

\[I\ =\ _{a,b[K]\\ a b}\ (a,b).\] (3.4)

The following Theorem provides the information-theoretic threshold for exact recovery in node-attributed SBM.

**Theorem 1**.: _Consider model (3.1) with \(_{a}>0\) for all \(a[K]\). Denote by \(a^{*},b^{*}\) the two hardest blocks to estimate, that is \((a^{*},b^{*})=I\). Suppose that \(t(0,1)_{n}_{t}(a^{*},b^{*})\) exists and is strictly concave. Then the following holds:_

1. _exact recovery is information-theoretically impossible if_ \(_{n}I<1\)_;_
2. _exact recovery is information-theoretically possible if_ \(_{n}I>1\)_._

The proof for Theorem 1 is provided in the supplemental material. The main ingredient of the proof is the asymptotic study of log-likelihood ratios. More precisely, let \(L(z)=(X,Y\,|\,z)\). The application of Chernoff bounds provides an upper bound on \(-( L(z) L(z^{}))\), where \(z\) denotes the correct block structure and \(z^{}[K]^{n}\) is another node-labelling vector (see Lemma 1 in the supplement). We can use this upper bond to prove that the maximum likelihood estimator (MLE) achieves exact recovery if \(I>n^{-1} n\). Reciprocally, if \(I<n^{-1} n\), we show that whp there exist some "bad" nodes \(i\) for which \(z_{i}*{arg\,max}_{a[k]}(X,Y\,|\,z_{-i},z_{ i}=a)\). In other words, even in a setting where an oracle would reveal \(z_{-i}\) (_i.e.,_ the correct block assignment of all nodes except node \(i\)), the MLE would fail at recovering \(z_{i}\). Establishing this fact requires to lower bound \(-( L(z) L(z^{}))\) where \(z^{}[K]^{n}\) is such that \((z,z^{})=1\) (_i.e.,_ \(z^{}\) correctly labels all nodes except one). This lower bound is obtained using large deviation results for general random variables . To apply these results, the strict concavity of the limit \(n( n)^{-1}I\) is needed. In most practical settings, this assumption is verified, except in some edge cases (see Examples 1 and 2). Let us now provide some examples of applications of Theorem 1.

**Example 1** (Binary SBM with no attributes).: _Suppose that \(f_{ab}(_{ab}n^{-1} n)\) where \(_{ab}\) are constants. A Taylor-expansion of the Renyi divergence between Bernoulli distributions leads to_

\[I\ =\ (1+o(1))_{a b}_{t(0,1)}(_{c}_{c }[t_{bc}+(1-t)_{ac}-_{bc}^{t}_{ac}^{1-t}] ),\]

_which indeed coincides with the expression of the Chernoff-Hellinger divergence defined in . We also note that the limit \(n( n)^{-1}I\) is strictly concave as long as the \(_{ab}\) are not all equals2._

**Example 2** (Binary SBM with Gaussian attributes).: _Suppose that \(f_{ab}(_{ab}n^{-1} n)\) and \(h_{a}(_{a} n,^{2}I_{d})\), where \(_{ab}\) and \(_{a}\) are independent of \(n\). Then,_

\[I\ =\ (1+o(1))_{a b}_{t(0,1)}(_{c}_{c} [t_{bc}+(1-t)_{ac}-_{bc}^{t}_{ac}^{1-t}]+t -_{a}\|_{2}^{2}}{2^{2}}).\]

_In particular, the technical conditions of Theorem 1 are verified if we rule out the uninformative case where all the \(_{ab}\)'s and the \(_{a}\)'s are equal to each other. Thus, exact recovery is possible if_

\[_{a b}_{t(0,1)}(_{c}_{c}[t_{bc}+(1-t) _{ac}-_{bc}^{t}_{ac}^{1-t}]+t-_{a} \|_{2}^{2}}{2^{2}})>1.\]

_Further assuming that \(_{ab}= 1(a=b)+ 1(a b)\) (homogeneous interactions) and \(=(,,)\) (uniform block probabilities), the expression of \(I\) simplifies to_

\[I\ =\ (1+o(1))[- )^{2}}{K}+}{8^{2}}],\]_where \(=_{a b}\|_{a}-_{b}\|_{2}\). This last scenario recovers the recently established threshold for exact recovery in the Contextual SBM ._

**Example 3** (Semi-supervised clustering in SBM).: _Consider binary interactions given by_

\[f_{ab}( n^{-1} n)&a=b,\\ ( n^{-1} n)&\]

_where \(,\) are independent of \(n\). Consider a semi-supervised model in which the vector of attributes \(Y\) is a noisy oracle of the true community labels \(z\). More precisely, for a node \(i\) such that \(z_{i}=k\), let_

\[h_{k}(y)=(Y_{i}=y)\;=\;1-&y=0,\\ _{1}&y=k,\\ }{K-1}&y[K]\{k\},\]

_with \(_{0}+_{1}=\). A bit of algebra shows that exact recovery is possible if_

\[(-)^{2}-(1-+ 2(K-1)^{-1/2}_{1}})>K.\]

_When \(_{0}=0\) (perfect oracle), the condition simplifies to \((-)^{2}->K\). Note that the oracle term is non-negligible only if \(- n\), as previously established . This last condition is very strong since it implies \( 1-1/n\), and hence the oracle must provide the correct label for almost all nodes._

## 4 Bregman hard clustering of sparse weighted node-attributed networks

In this section, we will propose an algorithm for clustering sparse, weighted networks with node attributes. When present, the weights are sampled from an exponential family, and the node attributes also belong to an exponential family. In Section 4.1, we provide some reminder of exponential families. We derive the likelihood of the model in Section 4.2, and present the algorithm in Section 4.3.

### Exponential family

An exponential family \(_{}\) is a parametric class of probability distributions whose densities can be canonically written as \(p_{,}(x)\;=\;e^{<,x>-()},\) where the density is taken with respect to an appropriate measure, \(\) is a function of the parameters of the distribution that must belong to an open convex space \(\), and \(\) is a convex function.

We consider the model defined in (3.1), such that \(f_{ab}\) are _zero-inflated distributions_ and are given by

\[f_{ab}(x)\;=\;(1-p_{ab})_{0}(x)+p_{ab}_{ab}(x),\] (4.1)

where \(p_{ab}\) is the interaction probability between blocks \(a\) and \(b\), \(_{0}(x)\) is the Dirac delta at zero, and \(_{ab}\) is a probability density with no mass at zero. Note that this model can represent sparse weighted networks, as edges between nodes in blocks \(a\) and \(b\) are absent with probability \(1-p_{ab}\).

Finally, suppose that the distributions \(\{_{ab}\}\) and \(\{h_{a}\}\) belong to exponential families. More precisely,

\[_{ab}(x)\;=\;e^{<_{ab},x>-(_{ab})} h_{a}(y)\;=\;e^{<_{a},y>-(_{a})},\] (4.2)

for some parameters \(_{ab},_{a}\) and functions \(,\). The following lemma provides the expression of the Chernoff divergence of this model.

**Lemma 1**.: _Let \(f_{ab}\) and \(h_{a}\) be defined as in (4.1)-(4.2). Suppose that \(p_{ab}=_{ab}\) where \(_{ab}\) is constant and \( 1\). We have_

\[I\;=\;(1+o(1))_{a b}_{t(0,1)}\{_ {c[K]}_{c}[tp_{ac}+(1-t)p_{bc}-p_{ac}^{t}p_{bc}^{1-t}\,e^{-J_{ }(_{ac}\|_{bc})}]+J_{}(_{a}\|_{b})\},\]

_where \(J_{}(_{ab}\|_{bc})=t(_{ac})+(1-t)(_{bc})- (t_{ac}+(1-t)_{bc})\)._

In combination with Theorem 1, Lemma 1 provides the expression for the exact recovery threshold for node-attributed networks with interaction and attribute distributions given by (4.1)-(4.2).

### Log-likelihood

Given a convex function \(\), the _Bregman divergence_\(d_{}^{m}^{m}_{+}\) is defined by

\[d_{}(x,y)\;=\;(x)-(y)-<x-y,(y)>.\]

The log-likelihood of the density \(p_{,}\) of an exponential family distribution is linked to the Bregman divergence by the following relationship (see for example [6, Equation 13])

\[ p_{,}(x)\;=\;-d_{^{*}}(x,)+^{*}(x),\] (4.3)

where \(=_{p_{,}}(X)\) is the mean of the distribution, and \(^{*}\) denotes the _Legendre transform_ of \(\), defined by \(^{*}(t)\;=\;_{}\{<,t>-()\}\). The following Lemma provides an expression for the log-likelihood of \(f_{ab}\) when \(f_{ab}\) is a distribution belonging to a zero-inflated exponential family.

**Lemma 2**.: _Let \(f_{ab}\) be a probability density as defined in (4.1)-(4.2). For \(x,y(0,1)\), let \(}(x,y)\) be the Kullback-Leibler divergences between \((x)\) and \((y)\), and let \(H(x)=x x+(1-x)(1-x)\), with the usual convention \(0 0=1\). Then,_

\[- f_{ab}(x)\;=\;}(x\|p_{ab})+sd_{^{*}}(x, _{ab})-s^{*}(x)-H(s),\]

_where \(s=1(x 0)\)._

Proof.: To express \( f_{ab}(x)\), we first note that

\[ f_{ab}(x)\;=\;(1-s)(1-p_{ab})+s p_{ab}+s(_{ab}(x)),\]

where \(c=1(x 0)\). The result follows by adding and subtracting \(H(b)\) in the previous expression and expressing \(_{ab}(x)\) with a Bregman divergence as in (4.3). 

Suppose that \(X,Y\) follow the model (3.1) with probability distributions given by (4.1)-(4.2). Let \(A\) be a binary matrix such that \(A_{ij}=1(X_{ij} 0)\). We have

\[-(X,Y\,|\,z)\;=\;_{i}\{_{j i}[ }(A_{ij},p_{z_{i}z_{j}})+A_{ij}d_{^{*}}(X_{ij},_{z_{ i}z_{j}})]+d_{^{*}}(Y_{i},_{z_{i}})\}+c,\]

where the additional term \(c\) is a function of \(X,Y\) but does not depend on \(z\). Denoting \(Z\{0,1\}^{n K}\) the one-hot membership matrix such that \(Z_{ik}=1(z_{i}=k)\), observe that \(p_{z_{i}z_{j}}=(ZpZ^{T})_{ij}\) where \(p\) is a symmetric matrix with the interaction probabilities between different blocks, \(_{z_{i}z_{j}}=(Z Z^{T})_{ij}\) where \(\) is a symmetric matrix with the expected value of the interaction between different blocks (edge weights), and \(_{z_{i}}=(Z^{T})_{i}\) where \(\) is a vector with the expected value of the attribute for different blocks. Thus, up to some additional constants, the negative log-likelihood \(-(X,Y\,|\,Z)\) is equal to

\[_{i}\{\,}(A_{i},(ZpZ^{T} )_{i})+d^{}_{^{*}}(X_{i}, (Z Z^{T})_{i})+d_{^{*}}(Y_{i},(Z^{T} )_{i})\}+c,\] (4.4)

where \(d^{}_{^{*}}(B,C)=_{j=1}^{n}1(B_{j} 0)d_{^{*}}(B_{j},C_{j})\) for two vectors \(B,C^{n}\).

### Clustering by iterative likelihood maximisation

Following the log-likelihood expression derived in (4.4), we propose an iterative clustering algorithm that places each node in the block maximising \((X,Y\,|\,z_{-i},z_{i}=a)\) for \(1 a K\), the likelihood that node \(i\) is in community \(a\) given the community labels of the other nodes, \(z_{-i}\). Let \(Z^{(ia)}\) denote the membership matrix obtained from \(Z\) by placing node \(i\) in block \(a\), and let \(L_{ia}(Z^{(ia)})\) denote the contribution of node \(i\) to the negative log-likelihood when node \(i\) is placed in block \(a\). Equation (4.4) shows that

\[L_{ia}(Z)\;=\;\,}(A_{i},(ZpZ^{T} )_{i})+d^{}_{^{*}}(X_{i}, (Z Z^{T})_{i})+d_{^{*}}(Y_{i},(Z^{T} )_{i}),\] (4.5)where the \(p\), \(\) and \(\) in the equation above must be estimated from \(X\), \(Y\), and the community membership matrix \(Z\). Let \(=(A,Z)\), \(=(X,Z)\), and \(=(Y,Z)\) denote the estimators for \(p\), \(\) and \(\), respectively. Their values can be computed as follows:

\[(A,Z) = (Z^{T}Z)^{-1}Z^{T}AZ(Z^{T}Z)^{-1},\] \[(X,Z) = (Z^{T}AZ)^{-1}Z^{T}XZ,\] (4.6) \[(Y,Z) = (Z^{T}Z)^{-1}Z^{T}Y.\]

Note that the matrix inverse \((Z^{T}Z)^{-1}\) can be easily computed since \(Z^{T}Z\) is a \(K\)-by-\(K\) diagonal matrix. This approach is described in Algorithm 1.

``` Input: Interactions \(X^{n n}\), attributes \(Y^{n}\), convex functions \(^{*},^{*}\), clustering \(Z_{0}\)
1 Let \(Z=Z_{0}\) repeat
2 Compute \(,,\) according to (4.6)
3 Let \(Z^{}=0_{n K}\)
4for\(i=1,,n\)do
5 Let \(Z^{(ia)}\) be the membership matrix obtained from \(Z\) by placing node \(i\) in community \(a\)
6 Find \(k^{*}=\,L_{ia}(Z^{(ia)})\), where \(L_{ia}(Z^{(ia)})\) is defined in (4.5);
7 Let \(Z_{ik}^{}=1(k=k^{*})\) for all \(k=1,,K\)
8 Let \(Z=Z^{}\)
9untilconvergence;Return: Node-membership matrix \(Z\) ```

**Algorithm 1**Bregman hard clustering for node-attributed SBM.

A fundamental aspect of many likelihood maximization iterative algorithms such as Algorithm 1 is the initial membership assignment, \(Z_{0}\). This initial assignment often has a profound influence on the final membership assignment, and thus, it is important to have an adequate initialization. In the numerical section, we proceed as follows. We construct the matrix \(W^{n 2K}\) such that the first \(K\) columns of \(W\) are the first \(K\) eigenvectors of the graph normalised Laplacian, while the last \(K\) columns of \(W\) are the first \(K\) eigenvectors of the Gram matrix \(YY^{T}\).

## 5 Numerical experiments

### Performance of Algorithm 1

We first compare in Figure 1 the performance of Algorithm 1 in terms of exact recovery (fraction of times the algorithm correctly recovers the community of _all_ nodes) with the theoretical threshold for exact recovery proved in the paper (red curve in the plots) in two settings: Figure 0(a) shows binary weight with Gaussian attributes, and Figure 0(b) shows zero-inflated Gaussian weights with Gaussian attributes. A solid black (resp., white) square means that over 50 trials, the algorithms failed 50 times (resp., succeeded 50 times) at exactly recovering the block structure.

### Comparison with other algorithms

In this section, we compare Algorithm 1 with other algorithms presented in the literature. We used the Adjusted Rand Index (ARI)  between the predicted clusters and the ground truth ones to evaluate the performance of each algorithm.

In Figure 2, we compare Algorithm 1 with the variational-EM algorithm of  and the algorithm of  (which is also based on Bregman divergences, but tailored for dense networks). Because both of these algorithms are designed for dense networks, we observe that Algorithm 1 has overall better performance on sparse networks.

We also compare Algorithm 1 with the _IR_sLs_ algorithm from . This is one of the most recent algorithms for node-attributed SBM and it comes with theoretical guarantees (for binary networkswith Gaussian attributes). We also compare with the EM algorithm of , _attSBM_, which is designed for binary networks with Gaussian attributes. Finally, we compare with baseline methods for clustering using the network or the attributes alone. _EM-GMM_ refers to fitting a Gaussian Mixture Model via EM on attribute data \(Y\), and _sc_ refers to _spectral clustering_ on network data \(X\).

Figure 3 shows the results for binary networks with Gaussian attributes. Algorithm 1 successfully learns from both the signal coming from the network and the attributes, even in scenarios where one of them is non-informative. Moreover, Algorithm 1 has better performance than the two other node-attributed clustering algorithms, and those algorithms also show a large variance3. We also note that _IR_sLs_ and _attSBM_ are both tailor-made for binary edges and Gaussian attributes. Even in such a setting, Algorithm 1 outperforms these two algorithms. We show in the supplement material that when the network is weighted and the attributes non-Gaussian, _IR_sLs_ and _attSBM_ perform poorly.

### Evaluation using real datasets

The following three benchmark datasets were used to evaluate and compare the proposed algorithm: _CiteSeer_ (\(n=3279\), \(m=9104\), \(K=6\), \(d=3703\)), _Cora_ (\(n=2708\), \(m=10556\), \(K=7\)

Figure 1: Phase transition of exact recovery. Each pixel represents the empirical probability that Algorithm 1 succeeds at exactly recovering the clusters (over 50 runs), and the red curve shows the theoretical threshold.

\(d=1433\)), and _Cornell_ (\(n=183\), \(m=298\), \(K=5\), \(d=1703\)) (all available in Pytorch Geometric). For each network, the original node attribute vector was reduced to have dimension \(d=10\) by selecting the 10 best features according to the chi-square test. Algorithm 1 assumed a multivariate Gaussian distribution with \(d=10\) for node attributes and Bernoulli edges (these networks have no edge weights). The initialization for Algorithm 1 and attSBM used spectral clustering of both the node similarity matrix (using node attributes) and network edges.

Table 1 shows that Algorithm 1 outperformed the other three algorithms. Spectral clustering (\(sc\)) has near zero performance, indicating that the network structure in these data sets has little information concerning the clusters of the nodes. Moreover, both Algorithm 1 and attSBM (that leverage network and node attributes) outperform EM-GMM that use only node attributes. These preliminary results indicate that Algorithm 1 is promising even in real data sets with little pre-processing.

## 6 Conclusion

This work made the following contributions to community detection in node-attributed networks: i) extended the known thresholds for exact recovery in binary SBM to non-binary (weighted) networks with node attributes, providing a clean expression for a new information-theoretic quantity, known in the binary setting as the Chernoff-Hellinger divergence; ii) proposed an iterative algorithm based on the likelihood function that can infer community memberships from a problem instance. The algorithm leverages the framework of Bregman divergences and is simple and computationally efficient. Numerical experiments indicate the superiority of this algorithm when compared to recent state-of-the-art approaches.