# ODGS: 3D Scene Reconstruction from Omnidirectional Images with 3D Gaussian Splitting

Suyoung Lee\({}^{*}\)\({}^{1}\) &Jaeyoung Chung\({}^{*}\)\({}^{1}\) &Jaeyoo Huh \({}^{2}\) &Kyoung Mu Lee \({}^{1,2}\)

\({}^{1}\)Dept. of ECE & ASRI, \({}^{2}\)IPAI, Seoul National University, Seoul, Korea

{esw0116,robot0321}@snu.ac.kr &jaeyoo900@gmail.com &kyoungmu@snu.ac.kr

indicates equal contribution.

###### Abstract

Omnidirectional (or 360-degree) images are increasingly being used for 3D applications since they allow the rendering of an entire scene with a single image. Existing works based on neural radiance fields demonstrate successful 3D reconstruction quality on egocentric videos, yet they suffer from long training and rendering times. Recently, 3D Gaussian splatting has gained attention for its fast optimization and real-time rendering. However, directly using a perspective rasterizer to omnidirectional images results in severe distortion due to the different optical properties between the two image domains. In this work, we present ODGS, a novel rasterization pipeline for omnidirectional images with geometric interpretation. For each Gaussian, we define a tangent plane that touches the unit sphere and is perpendicular to the ray headed toward the Gaussian center. We then leverage a perspective camera rasterizer to project the Gaussian onto the corresponding tangent plane. The projected Gaussians are transformed and combined into the omnidirectional image, finalizing the omnidirectional rasterization process. This interpretation reveals the implicit assumptions within the proposed pipeline, which we verify through mathematical proofs. The entire rasterization process is parallelized using CUDA, achieving optimization and rendering speeds 100 times faster than NeRF-based methods. Our comprehensive experiments highlight the superiority of ODGS by delivering the best reconstruction and perceptual quality across various datasets. Additionally, results on roaming datasets demonstrate that ODGS effectively restores fine details, even when reconstructing large 3D scenes. The source code is available on our project page.1

## 1 Introduction

With the development of VR/MR devices and robotics technologies and the increasing demands of such applications, 3D scene reconstruction has become one of the crucial tasks in computer vision. Traditional works have employed a structure-from-motion algorithm that estimates camera motion and scene geometry from multiview 2D images by finding the correspondences between images. As target 3D scenes become broader and more complex, accurate reconstruction demands a larger volume of images and increases the computational burden required for identifying correspondences. Recently, some approaches have tried to alleviate these challenges by utilizing wide-angle cameras to capture wide field-of-view images. Omnidirectional images, which provide a 360-degree field of view, are gaining increased interest because they encompass whole scenes within a single image, thereby reducing the cost of inter-image feature matching. The growing popularity of 360-degree cameras for personal video recording and the concurrent release of related datasets further facilitate the research on 3D content reconstruction from omnidirectional images.

Such 3D reconstruction techniques [37; 51] began to be mainly studied in SLAM systems to obtain accurate camera poses with matched 3D points from monocular omnidirectional video obtained from robots. However, these models focus on restoring structural information rather than contents, and they often bypass the fine details and texture of 3D scenes. After neural radiance field (NeRF)  has shown outstanding 3D reconstruction performance, several works such as [10; 19; 21; 30] attempted to reconstruct 3D implicit representation from omnidirectional images. Despite showing prominent reconstruction quality, those methods commonly suffer from slow rendering and lengthy training. 3D Gaussian splatting , 3DGS in short, overcomes the challenges of NeRF by representing 3D contents with numerous Gaussian splats. The 3D Gaussians are initialized from the sparse point cloud obtained from the structure-from-motion and optimized through the differentiable image rasterization pipeline. Since the CUDA-implemented rasterization for 3DGS is much faster than volume rendering used in the NeRF family, 3DGS has dramatically improved rendering speed while maintaining or improving performance. Although many follow-up works have been proposed after 3DGS's success, only a few address 3DGS in the omnidirectional image domain.

In this work, we propose ODGS that aims to reconstruct high-quality 3D scenes represented by Gaussian splatting from multiple omnidirectional images. The gist of our method is designing a CUDA rasterizer that is appropriate for omnidirectional images. Specifically, we create a unit sphere from the camera origin, considering it an omnidirectional camera surface, and assume that each Gaussian is projected onto the tangent plane of the point where the vector from the camera origin to the center of the Gaussian and the unit sphere meets. Since each Gaussian is projected onto a different plane, we calculate a rotation matrix for the coordinate transformation to ensure that each Gaussian is properly projected onto its corresponding tangent plane. Then, the projected Gaussians are subsequently mapped onto the omnidirectional image plane. Our proposed rasterizer is also easily parallelizable like the original 3DGS rasterizer, demonstrating fast optimization and rendering speed. Finally, we carefully apply the densification rule to split or prune the Gaussians for omnidirectional projection. We apply a dynamic gradient threshold value for each Gaussian based on its elevation, as the azimuthal width of the projected Gaussian is stretched when transformed into an equirectangular space. We conduct comprehensive experiments comparing the reconstruction quality in various 360-degree video datasets with various environments, including egocentric and roaming, real and synthetic. The results show that ODGS achieves much faster optimization speed than existing NeRF-based methods and reconstructs the scenes with higher accuracy. Additionally, the perceptual metrics and qualitative results demonstrate that our method restores textural details more sharply.

To summarize, our contributions are three-fold:

* We introduce ODGS, a 3D reconstruction framework for omnidirectional images based on 3D Gaussian splatting, achieving 100 times faster optimization and rendering speed than NeRF-based methods.
* We present a detailed geometric interpretation of the rasterization for omnidirectional images, along with mathematical verification, and propose a CUDA rasterizer based on the interpretation.
* We comprehensively validate ODGS on various egocentric and roaming datasets, showing both more accurate reconstructed results and better perceptual quality.

## 2 Related works

In computer vision, ongoing research has been on creating 3D representations of the surrounding environment using multi-view images. Among them, omnidirectional images capture the surrounding space in a single image due to their wide field of view, making them increasingly popular for 3D reconstruction and mapping. Traditional structure-from-motion (SfM) algorithms [38; 42; 43] simultaneously estimate camera poses and 3D geometry structure by extracting and matching feature points across multiple images. This field has developed over many years, resulting in the release of user-friendly open libraries such as COLMAP  or OpenMVG . Recent advancements continue to improve feature matching for spherical images . In indoor environments, additional information such as room layout [2; 40; 41] and planar surfaces [14; 45] are used to promote the reconstruction quality. The geometry structures estimated from omnidirectional images are also utilized for localization [22; 28] or for simultaneous localization and mapping (SLAM) research [5;44, 49]. The wide field of view provided by omnidirectional cameras enables the simultaneous capture of extensive spatial information, making them highly beneficial in robotic applications for environmental perception and understanding. Beyond sparse geometry structure in SfM, Multi-View Stereo (MVS)  supports dense reconstruction based on epipolar geometry to achieve better results. Recently, multi-view stereo techniques leveraging deep neural networks have been actively researched.  Another approach to representing 3D is by stacking multiple layers of multi-sphere images. Inspired by multi-planar images, this method facilitates the egocentric representation of scenes . These methods show the possibilities of 3D reconstructions using omnidirectional images, but often lack textural details for photo-realistic 3D reconstruction or limit the representation to confined spaces.

In recent 3D reconstruction research, Neural Radiance Field (NeRF)  has demonstrated the capability for photo-realistic novel-view synthesis, leading to studies on NeRF-based 360 image 3D reconstruction. This approach is widely used for directly reconstructing scenes in 3D  or indirectly representing 3D by estimating depth . In particular, EgoNeRF  is a recently published NeRF-based reconstruction method, pointing out that a typical Cartesian coordinate is not appropriate for representing a large scene with omnidirectional images. It introduces a new spherically balanced feature grid and hierarchical density adaptation during ray casting, achieving a prominent reconstruction quality. However, although NeRF-based methods have shown more realistic reconstruction than traditional techniques, they have the inherent limitation of requiring extensive time for reconstruction and rendering.

3D Gaussian splatting (3DGS)  is a novel 3D representation that demonstrates photo-realistic novel view synthesis while supporting fast optimization and real-time rendering. 3DGS explicitly expresses a space using a set of Gaussian primitives and quickly creates novel views through a rasterization pipeline without the time-consuming ray-casting process in NeRF. Due to its high applicability, extensive research is rapidly advancing, covering not only typical reconstruction but also sparse reconstruction , dynamic scene reconstruction , SLAM  and even generation . However, 3D scene reconstruction based on omnidirectional images has been barely studied. This is partly because developing a suitable rasterizer for omnidirectional images that allows real-time rendering is challenging, and such implementations are not publicly available. 360-GS  is the first method that proposes omnidirectional reconstruction with 3DGS, employing a two-step strategy. However, it relies on layout-guided error correction, which limits its applicability to indoor scenes. In this paper, we present a carefully implemented CUDA rasterizer that rotates the projection plane on a unit sphere, which can efficiently optimize 3D Gaussians without any constraints or assumptions on scenes. Further, we propose a dynamic densification rule designed for a 360-degree camera from our analysis, enabling us to reconstruct high-quality scenes rapidly. We note that a few concurrent works, such as Gaussian splatting with optimal projection strategy  or OmniGS , partly share the contributions with ours.

## 3 Methods

### Preliminary: Rasterization Process in Typical 3D Gaussian Splatting

3D Gaussian splatting (3DGS)  is a recently proposed 3D representation that models scenes using a set of 3D anisotropic Gaussians derived from multi-view images. It initializes the 3D Gaussians using a traditional structure-from-motion library and optimizes their properties--such as position, color, scale, rotation, and opacity--through photometric loss. In this section, we explain the rasterization pipeline for 3D Gaussians in a perspective camera as proposed by 3DGS, followed by a discussion of the differences in the rasterization process for an omnidirectional camera.

A 3D Gaussian is represented by its mean and covariance, where the covariance matrix \(\) is expressed as the product of a rotation matrix \(\) and a scale matrix \(\) (\(=^{T}^{T}\)) to facilitate optimization through gradient descent. When the 3D Gaussian is projected onto the image plane of a perspective camera, the resulting distribution becomes complex since the perspective projection is not a linear transformation. Following the approach in EWA splatting , 3DGS approximates the projected distribution on the image plane as a 2D Gaussian. While introducing some errors, the local affine approximation simplifies the modeling of the projected 3D Gaussian, ultimately reducing computational complexity and increasing rendering speed. Based on the perspective camera projection function \(()=_{1:2}[{}^{_{x}}/_{x},{}^{_{y}}/ _{z},1]^{T}\) with intrinsic matrix \(\), the first-order approximation of the projection \(\) is given as,

\[=)}{}= }{_{x}}&0&-_{x}}{_{x}^{2}}\\ 0&}{_{x}}&-_{y}}{_{x}^{2}} ^{2 3}, \]

where \(f_{x},f_{y}\) are focal lengths of the camera and \(=[_{x},_{y},_{z}]\) is the mean vector of 3D Gaussian expressed in the camera coordinate system. As a result, the 2D Gaussian distribution on the image plane is represented with mean \(()^{2}\) and covariance \(_{}=^{T}J^{T}^{2 2}}\), where \(\) denotes the transformation matrix from world space to camera space. The 2D Gaussian represents the intensity on the image plane and is normalized as follows to ensure the maximum value at the center becomes 1.

\[^{2}, G_{}()=(- (-())^{T}_{}^{-1}(-( ))). \]

This bell-shaped intensity is multiplied by the Gaussian's opacity to determine the pixel-wise opacity \(\). After frustum culling and sorting by depth, the color of each pixel \(C\) is determined as,

\[C=_{j N}c_{j}_{j}T_{j}, T_{j}=_{k=1}^{j-1}(1-_{k}), \]

where \(c_{j}\) is the color of each Gaussian. This accumulation process is performed in the order of depth sorting. Each pixel is processed independently by a single GPU thread, enabling rapid rendering of 3D Gaussians into images through this rasterization process. While 3DGS describes the rasterization pipeline of 3D Gaussians for the perspective camera, a rasterization pipeline for an omnidirectional camera requires a distinct approach that regards its different optical characteristics. The following section explains our carefully designed rasterizer for the omnidirectional images.

### Designing Rasterizer for Omnidirectional Images

A 360-degree camera captures all rays from the surrounding 3D environment to the camera origin and represents them on a unit sphere \(^{2}\). We employ spherical projection or equirectangular projection (ERP) to map the projected image on the sphere \(^{2}\) to the equirectangular space \(^{2}\), then transform to pixel space \(^{2}\). We describe a series of steps on how a 3D Gaussian is approximated as a 2D Gaussian in the pixel space and the feasibility of such an approximation. The gist of our rasterizer design for the omnidirectional camera is leveraging locally approximated perspective projection on \(^{2}\) while minimizing errors.

As demonstrated in Figure 1, we follow a camera coordinate convention  with the z-axis forward, the x-axis to the right, and the y-axis down. We define spherical coordinates by setting the azimuth \(\) from the forward z-axis within the range \([-,]\) and the elevation \(\) from the z-x plane within the range \([-/2,/2]\). Projecting the mean \(\) of the 3D Gaussian onto the unit sphere results in \(}=/||||\), which corresponds to the azimuth \(_{}=(_{x}/_{z})\) and elevation \(_{}=(-_{y}/^{2}+_{z}^{2}})\). This spherical coordinate representation \((,)\) is converted into pixel space by multiplying scalar and adding center shift,

\[_{o}()=(_{}+,- _{}+)^{T} \]

where \(W,H\) are the width and height of the omnidirectional image, respectively.

While finding the corresponding point of the center of 3D Gaussian on the pixel space is straightforward, calculating the covariance requires more careful consideration. We model the distribution of 3D Gaussian projected onto pixel space as a 2D Gaussian for computational efficiency and stability, following a similar approach to 3DGS. We leverage the perspective camera and local affine approximation to intuitively describe the non-linear transformation introduced by the spherical camera characteristics and the equirectangular projection. Then, we mathematically prove the correctness of the proposed method.

Let us assume a perspective camera with a unit focal length, where the image plane is tangent to the unit sphere. We rotate the perspective camera's forward direction to align with the position of the Gaussian center, \(\), as shown in Figure 1 (a). The image plane is tangent to the unit sphere at \(}\), the point where the line from the sphere's center to the center of 3D Gaussian intersects the sphere. We define the rotation matrix of the perspective camera as \(_{}\), which is accomplished in two rotations in azimuth and elevation,

\[_{}=_{_{}} _{_{}}&=1&0&0\\ 0&_{}&_{}\\ 0&-_{}&_{} _{}&0&-_{}\\ 0&1&0\\ _{}&0&_{}\\ &=_{}&0&-_{}\\ _{}_{}&_{}&_{}_{} \\ _{}_{}&-_{}&_{}_{} . \]

The rotation of the coordinate system helps minimize the error between the unit sphere and the image plane, while also simplifying the covariance calculation. In the rotated camera coordinate, the position of the Gaussian is represented as \(_{o}=(0,0,||||)\). Thus, the Jacobian matrix from Eq. 1 is simplified as,

\[_{o}=}{{||||}}&0&0\\ 0&}{{||||}}&0, \]

because the focal length of the perspective camera is assumed to be one (\(f_{x}=f_{y}=1\)). Thus, the covariance of the 3D Gaussian projected onto this tangent plane is modeled as \(_{o}^{T}_{o}^{T}\), as shown in Figure 1 (b). We assume that the covariance of this 2D Gaussian is small enough to disregard the difference between the tangent plane and the sphere surface, allowing us to transfer it directly onto the sphere surface. Although this assumption does not generally hold, we ensure its validity through the split rule in 3DGS, which keeps the size of the Gaussian small. Next, we map the 2D covariance from the spherical surface \(^{2}\) to the equirectangular space \((,)^{2}\), as described in Figure 1 (c). The equirectangular projection transforms the spherical surface onto a cylindrical map, scaling a ring at latitude \(\) with an initial radius of \(\) on the sphere to a radius of 1. This projection introduces a horizontal scaling factor of \(\), leading to increased distortion as \(\) approaches the poles. We incorporate the distortion through \(_{o}\), and then we rescale the covariance to the pixel

Figure 1: Illustration on rasterization process of ODGS. We describe the process of projecting a 3D Gaussian to the omnidirectional pixel space. (a) The coordinate is transformed from the original camera pose (black) to the target Gaussian (green), making the \(z\)-axis of the coordinate head towards the center of the Gaussian. (b) The Gaussian is projected onto the corresponding tangent plane. (c) The projected Gaussian is horizontally stretched when transformed into equirectangular space. (d) The Gaussian in equirectangular space is linearly transformed to the pixel space, followed by a combination with the other projected Gaussian.

space by applying the appropriate scaling factors \(_{o}\),

\[_{o}=_{}&0\\ 0&1,_{o}=W/2&0\\ 0&H/. \]

As a result, the final Jacobian matrix is given as,

\[_{omni}=_{o}_{o}_{o}_{}=||}_{}_{}&0&- ||}_{}_{}\\ ||}_{}_{}&|| }_{}&||}_{}_{}, \]

where the final 2D covariance is presented as \(_{2,o}=_{omni} ^{T}_{omni}^{T}\) We verify the correctness of the derived method by directly differentiating the equirectangular projection function \(_{o}\) in Eq. 4, yielding the same result \(_{omni}=()}{}\) as detailed in Appendix A.2.

As a result of the series of steps, the final 2D covariance is used for rendering the image, as described in Eq. 2 and Eq. 3. One key difference is that, instead of performing frustum-shaped culling as in perspective cameras, we perform culling in a spherical shell. The rasterization pipeline is fully differentiable and implemented in CUDA, which can be used as a typical 3DGS. The detailed gradient calculations through back-propagation are provided in Appendix A.3.

### Densification Policy for Omnidirectional Images

Due to the characteristic of equirectangular projection, a 3D Gaussian can be rendered in different shapes depending on its relative elevation to the camera; Gaussians near the poles are drawn larger. Therefore, we propose a dynamic densification strategy specifically designed for omnidirectional images. While the original method uses a pre-defined gradient threshold for densifying Gaussians, we apply a varying gradient threshold \(_{}\) according to the elevation angle \(_{}\) as,

\[_{}=_{}+(1-_{})(_ {}-_{}), \]

which mitigates excessive densification of Gaussians near the poles.

## 4 Experiments

### Experiment Details

DatasetsWe evaluate our method on three egocentric datasets (OmniBlender, Ricoh360, OmniPhotos) and three roaming datasets (360Roam, OmniScenes, 360VO) to show its superiority regardless of domain. First, EgoNeRF  released OmniBlender and Ricoh360, which have different characteristics. OmniBlender contains 11 synthetic scenes generated with an omnidirectional rendering engine in Blender , with four indoor and seven outdoor scenes. The images were captured by rotating in a circular motion while ascending, each with a resolution of \(2000 1000\). Each scene in OmniBlender consists of 25 training and test images. Ricoh360 contains 12 real-world omnidirectional outdoor scenes captured by rotating in place in a cross-shaped pattern. Each scene consists of 50 training images and 50 testing images with a resolution of \(1920 960\). OmniPhotos  has released 10 real-world omnidirectional scenes captured by rotating in a circular motion with a commercial 360-degree camera on a selfie stick. Each scene has 71 to 91 images with a size of \(3840 1920\). In our experiment, we resize them to half resolution \(1920 960\), and we use 20% of images for the test.

For the roaming scenarios, we utilize several multi-view omnidirectional datasets, which were not originally released for 3D reconstruction tasks. 360Roam  dataset consists of 10 real-world indoor scenes captured by Insta360camera. Each scene has 71 to 215 omnidirectional images with size \(6080 3040\), and we resize them to \(2048 1024\). OmniScenes  is originally made for assessing the quality of visual localization of omnidirectional images in harsh conditions. Since it is proposed to measure the robustness of visual localization algorithms, it contains significant scene changes, motion blur, or some visual artifacts such as jpeg compression. We use the released version 1.1, which includes 7 real-world indoor captured scenes in resolution \(1920 960\). 360VO  is a simulation dataset for evaluating the localization and mapping algorithms in the robotics field. It contains 10 virtual outdoor road scenes, where each scene has 2000 images with size \(1920 960\)We uniformly select 200 images for each sequence for training and testing. Since these datasets do not split the train and test images, we conducted our experiment by dividing them by 4:1 for train and test, respectively. We note that all datasets have CC-BY-4.0 licenses. Although some datasets provide camera poses and dense point clouds, we run the structure-from-motion, specifically OpenMVG , on all the datasets and use obtained poses and point clouds for our experiment.

Implementation detailsOur framework is basically built with PyTorch , but we manually implement the omnidirectional rasterizer using the CUDA kernel. All experiments, including optimization and inference time measurements, are conducted using a single NVIDIA RTX A6000 GPU. We describe the optimization arguments in the Appendix A.1.

### Experiment Results

BaselinesWith no available code for 3DGS on omnidirectional images at the time of our experiments, we compare our method with NeRF-based methods, specifically TensoRF  and EgoNeRF . We also convert omnidirectional images into perspective images to compare the typical 3D reconstruction methods, NeRF  and 3DGS . Specifically, we transform the omnidirectional images into six perspective images using cubemap decomposition, popularly used in many

    &  &  &  &  & _{}\)} \\  & & PSNR\({}_{}\) & SSIM\({}_{}\) & LPIPS\({}_{}\) & PSNR\({}_{}\) & SSIM\({}_{}\) & LPIPS\({}_{}\) & (sec.) \\   & NeRF(P) & 19.20 & 0.6124 & 0.5359 & 20.04 & 0.6092 & 0.4949 & 62.71 \\  & 3DGS(P) & 29.36 & 0.8770 & 0.1400 & 21.19 & 0.7528 & 0.3021 & 0.112 \\  & TensoRF & 25.36 & 0.7249 & 0.3855 & 26.08 & 0.7416 & 0.3170 & 10.77 \\  & EgoNeRF & 28.29 & 0.8309 & 0.2194 & 30.89 & 0.8934 & 0.1260 & 23.78 \\  & ODGS & **32.76** & **0.9234** & **0.0469** & **33.05** & **0.9229** & **0.0343** & **0.028** \\   & NeRF(P) & 14.33 & 0.5616 & 0.5794 & 16.16 & 0.5617 & 0.5716 & 62.46 \\  & 3DGS(P) & **25.12** & 0.7932 & 0.2397 & 22.07 & 0.7228 & 0.3218 & 0.132 \\  & TensoRF & 23.35 & 0.6812 & 0.5200 & 23.97 & 0.6936 & 0.4653 & 01.30 \\  & EgoNeRF & 24.74 & 0.7467 & 0.3243 & 25.49 & 0.737 & 0.2825 & 23.89 \\  & ODGS & 24.94 & **0.8135** & **0.1489** & **26.27** & **0.8462** & **0.1051** & **0.026** \\   & NeRF(P) & 18.14 & 0.6158 & 0.5514 & 20.80 & 0.6388 & 0.4772 & 62.08 \\  & 3DGS(P) & 25.61 & 0.8310 & 0.2100 & 23.30 & 0.7859 & 0.2670 & 0.110 \\  & TensoRF & 22.78 & 0.6841 & 0.5089 & 23.73 & 0.7038 & 0.4467 & 9.707 \\  & EgoNeRF & 25.20 & 0.7722 & 0.2662 & 26.90 & 0.8349 & 0.1766 & 23.88 \\  & ODGS & **26.24** & **0.8704** & **0.1108** & **27.04** & **0.8878** & **0.0875** & **0.028** \\    & NeRF(P) & 15.07 & 0.6848 & 0.4839 & 15.26 & 0.6813 & 0.5025 & 62.98 \\  & 3DGS(P) & 20.17 & 0.7001 & 0.3536 & 19.34 & 0.6576 & 0.3837 & 0.104 \\  & TensoRF & 18.00 & 0.5988 & 0.7488 & 18.12 & 0.5895 & 0.7133 & 9.052 \\  & EgoNeRF & 20.45 & 0.6358 & 0.5334 & **21.18** & 0.6718 & 0.4444 & 24.03 \\  & ODGS & **21.08** & **0.7066** & **0.3003** & 20.85 & **0.7111** & **0.2254** & **0.029** \\   & NeRF(P) & 15.69 & 0.7218 & 0.4546 & 15.98 & 0.6890 & 0.4914 & 62.90 \\  & 3DGS(P) & 23.61 & 0.8444 & 0.2835 & 17.14 & 0.7119 & 0.3906 & 0.194 \\   & TensoRF & 23.58 & 0.8118 & 0.3534 & 24.21 & 0.8208 & 0.3091 & 8.100 \\   & EgoNeRF & 22.78 & 0.7997 & 0.3463 & **24.76** & 0.8313 & 0.2623 & 23.66 \\   & ODGS & **24.42** & **0.8526** & **0.1391** & 24.51 & **0.8505** & **0.1282** & **0.032** \\   & NeRF(P) & 15.71 & 0.6186 & 0.4949 & 17.78 & 0.6373 & 0.5064 & 61.97 \\   & 3DGS(P) & 22.87 & 0.7861 & 0.2970 & 22.73 & 0.7822 & 0.3061 & 0.091 \\   & TensoRF & 19.74 & 0.6543 & 0.5876 & 20.31 & 0.6721 & 0.5640 & 7.815 \\   & EgoNeRF & 22.47 & 0.7325 & 0.4342 & 23.78 & 0.7677 & 0.3680 & 23.96 \\   & ODGS & **24.63** & **0.8245** & **0.2175** & **26.68** & **0.8694** & **0.1264** & **0.026** \\   

Table 1: Quantitative comparison of 3D reconstruction methods on various datasets. The best metric for each dataset is written in **bold**. Our method shows the best performance on almost all settings regardless of optimization time, with the fastest rendering speed.

studies involving 360-degree cameras . The six decomposed images compose a cube-shaped surface and we calculate the corresponding camera pose of each surface. For inference, the six views for each face in the cube are rendered and then combined into an omnidirectional image.

Quantitative comparisonTo ensure the experiment's fairness and highlight the efficiency of our method, all methods were evaluated after optimizing the model with the same amount of time. We evaluate the performance of all methods at 10 and 100 minutes of training time, measured in wall-clock time. For evaluation metric, we use PSNR (dB), SSIM , and LPIPS  for comparing reconstruction quality, where AlexNet  backbone is used for measuring LPIPS. Table 1 shows the quantitative performance comparison and rendering time (seconds) for all datasets. The (P) mark in the method column indicates those methods are trained with converted perspective images. Our results show dominant results on all metrics, including inference time. NeRF and TensORF, which use a grid based on a Cartesian coordinate system, encounter difficulties representing large scenes, resulting in poor quantitative metrics. EgoNeRF, which introduces a spherical balanced grid to mitigate the challenge, shows better quality than TensoRF but still needs better perceptual metrics. Also, these methods require more than a second to render a single omnidirectional image for an arbitrary viewpoint, which is impractical for real scenarios. Meanwhile, 3DGS with perspective images shows the best results except ours when optimized for 10 minutes, but severely suffers from overfitting and gets worse results after 100 minutes of optimization. In terms of rendering time, despite reporting faster speed than NeRF-based models, original 3DGS takes longer than typical perspective image rendering because it involves non-linear warping of each image when stitching six images to create one omnidirectional image. ODGS, in contrast, outperforms the other methods in image reconstruction quality and rendering speed. The outstanding results for SSIM and LPIPS imply that our method generates images with accurate structure and prominent perceptual quality.

Figure 2 shows the change of PSNR, SSIM, and LPIPS depending on the optimization time for two example scenes. We note that we stopped training NeRF and TensoRF at 100 and 200 minutes, respectively, since their performances converged. Our method shows the fastest optimization speed in both scenes while maintaining the highest score regardless of optimization time. Typical NeRF and TensoRF recorded significantly lower results than ours, verifying that the Cartesian coordinate is inappropriate for radially extending rays. EgoNeRF shows comparable PSNR with ours in _Ballintoy_, but needs a long optimization time. We attribute the fast optimization of ODGS to two aspects. First, while NeRF-based methods use an implicit representation that embeds the scene into a neural network, 3DGS employs explicit representation and directly moves or morphs the elements to optimize the model. Also, 3DGS exploits the position of SfM point clouds, which can serve as a good initialization point for optimizing Gaussian splats. 3DGS (P), on the other hand, shows high vulnerability to

Figure 2: Changes of PSNR, SSIM, and LPIPS according to the optimization time for each method. ODGS shows the best result as well as the highest convergence speed in both scenes.

overfitting. We believe the phenomenon happens because of the weak correlation among the six faces of the cubemap after decomposition. Since there is no overlap between the six faces, 3DGS is optimized six times independently for faces facing the same direction. Therefore, even with the same input, the amount of information used is significantly reduced, causing overfitting to occur quickly.

Figure 4: Qualitative comparisons in the roaming scenes (10 min.). Each scene is brought from 360Roam, OmniScenes, and 360VO, respectively. _Best viewed when zoomed in._

Figure 3: Qualitative comparisons in the egocentric scenes (10 min.). Each scene is brought from Ricoh360, OmniBlender, and OmniPhotos, respectively. _Best viewed when zoomed in._

Qualitative comparisonWe also visually compare our method with the other methods in various scenes. Figure 3 shows the samples of reconstructed images from egocentric datasets. We note that the images in the figure are rendered at 10 minutes of training. The images from EgoNeRF are blurry and contain some artifacts, such as stripe lines or checkerboard patterns, which appear prominent near the edges. The model is not sufficiently optimized to render the sharp image details. 3DGS trained with the cubemap perspective images sometimes show sharp reconstruction contents, such as a cubic pattern of a frame in the middle row (yellow boundary) but often include unintended projected Gaussian splats that cause image distortion. We attribute the phenomenon to the rapid overfitting properties of perspective 3DGS. In contrast, our model successfully reconstructs sharp details in the images. The superiority of ODGS becomes more noticeable in the roaming dataset, as shown in Figure 4. Although EgoNeRF proposes a balanced grid for egocentric video, it cannot maintain a uniform ray density for every grid if the camera wanders inside a large environment. As a result, the scene pattern is often completely lost, creating completely different results, and the overall reconstruction quality deteriorates. While perspective 3DGS shows better quality than EgoNeRF, it often misses some objects or structures where the adjacent faces of the cube meet. For instance, in the top row, there is an inverted Y-shaped artifact instead of a chair in a purple patch. This happens because the chair is located where the three sides of the cube meet, and the object is not made from any of the sides. ODGS overcomes the challenge by optimizing the Gaussian using the whole image and showing prominent performance on both egocentric and roaming datasets.

Ablation Study: Dynamic Densification Strategy for Omnidirectional ImagesWe qualitatively compare and display the results in Figure 5 when applying the proposed dynamic densification rule proposed in Section 3.3. As shown in the figure, the lanes appear split, with artifact-like patterns emerging on the road due to static densification, as employed in the original 3DGS work . Conversely, our densification strategy significantly enhances the model's representation power, leading to markedly more accurate rasterization results.

## 5 Conclusion

In this work, we propose a new method called ODGS, specifically designed to reconstruct 3D scenes from omnidirectional images using 3D Gaussian splatting. To optimize 3D Gaussian splatting in the omnidirectional image domain, we introduce a new rasterizer that appropriately models the equirectangular projection from the 3D space to the image. Specifically, we define a tangent plane for each Gaussian and project the Gaussian into the plane, followed by horizontal stretching and rescaling to the pixel space. Compared to the state-of-the-art NeRF-based methods, ODGS shows about 100 times faster optimization and rendering speed, which allows the user to synthesize the novel view in real-time. Furthermore, ODGS shows the best reconstruction performance for various input images, including egocentric and roaming scenes, indoors and outdoors.

Limitations and future workODGS still relies on local affine approximation when projecting a Gaussian splat to the camera surface. Equirectangular projection is not a linear transformation, and straight lines in the 3D space should be expressed as curves in the omnidirectional image. However, a 3D Gaussian is approximated as a 2D Gaussian, leading to errors that produce artifacts in the rendered image. Adopting a more accurate distribution for spherically projected Gaussians can reduce errors and enhance the efficiency of the framework.

Figure 5: Qualitative comparison of rendered images according to the Gaussian densification policy during optimization.