# SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning

Paul Mangold

CMAP, UMR 7641,

Ecole polytechnique

Sergey Samsonov

HSE University,

Russia

Safwan Labbi

CMAP, UMR 7641,

Ecole polytechnique

Ilya Levin

HSE University,

Russia

Reda Alami

Technology Innovation Institute,

9639 Masdar City, Abu Dhabi,

United Arab Emirates

Alexey Naumov

HSE University,

Steklov Mathematical Institute

of Russian Academy of Sciences

Eric Moulines

CMAP, UMR 7641,

Ecole polytechnique

MZBUAI

###### Abstract

In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy \(\). To overcome this, we propose SCAFFLSA, a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales _logarithmically_ with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as _linear speed-up_. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.

## 1 Introduction

Heterogeneity has a major impact on communication complexity in federated learning (FL) [28; 36]. In FL, multiple agents use different local oracles to update a global model together. A central server then performs a _consensus step_ to incrementally update the global model. Since communication with the server is costly, reducing the frequency of the consensus steps is a central challenge. At the same time, limiting communications induces _client drift_ when agents are heterogeneous, biasing them towards their local solutions. This issue has mostly been discussed for FL with stochastic gradient methods [23; 51]. In this paper, we investigate the impact of heterogeneity in the field of federated linear stochastic approximation (federated LSA). The goal is to solve a system of linear equations where (i) the system matrix and the corresponding objective are only accessible via stochastic oracles, and (ii) these oracles are distributed over an ensemble of heterogeneous agents. This problem can be solved with the FedLSA method, which performs LSA locally with periodic consensus steps. This approach suffers from two major drawbacks: heterogeneity bias, and high variance of local oracles.

A popular means of overcoming heterogeneity problems is the method of control variables, which goes back to the line of research initiated by . However, existing results on the complexity of these methods tend to neglect the linear decrease of the mean squared error (MSE) of the algorithm with the number of agents \(N\), or they require a lot of communication . In this paper, weshow that it is possible to reduce communication complexity using control variates while preserving the linear speed-up in terms of sample complexity. Our contributions are the following:

* We provide the sample and communication complexity of the FedLSA algorithm, inspired by the work of . Our analysis highlights the relationship between the MSE of the FedLSA method and three key factors: the number of local updates, the step size, and the number of agents. We provide an exact analytical formulation of the algorithm's bias, which is confirmed in our numerical study. We also give results under Markovian noise sampling.
* We propose SCAFFLSA, a method that provably reduces communication while maintaining linear speed-up in the number of agents. This method uses control variates to allow for extended local training. We establish finite sample and communication complexity for SCAFFLSA. Our study is based on a new analysis technique, that carefully tracks the fluctuations of the parameters and cocommunicationsontrol variates. This allows to prove that SCAFFLSA _simultaneously maintains linear speedup and reduced communication_. To our knowledge, this is the first time that these two phenomenons are proven to occur simultaneously in FL.
* We apply both these methods to TD learning with linear function approximation, where heterogeneous agents collaboratively estimate the value function of a common policy.

We provide a synthetic overview of this paper's theoretical results in Table 1 in the general federated LSA setting, and we instantiate these results for federated TD learning in Table 2 (Appendix E). We start by discussing related work in Section 2. We then introduce federated LSA in Section 3, and analyze it in Section 4. In Section 5 we introduce SCAFFLSA, a novel strategy to mitigate the bias. Finally, we illustrate our results numerically in Section 6. Since an important application of LSA is TD learning  with linear function approximation, we instantiate the results of Section 3-5 for federated TD learning.

**Notations.** For matrix \(A\) we denote by \(\|A\|\) its operator norm. Setting \(N\) for the number of agents, we use the notation \(_{c}[a_{c}]=N^{-1}_{c=1}^{N}a_{c}\) for the average over different clients. For the matrix \(A=A^{} 0,A^{d d}\) and \(x^{d}\) we define the corresponding norm \(\|x\|_{A}=Ax}\). For sequences \(a_{n}\) and \(b_{n}\), we write \(a_{n} b_{n}\) if there exists a constant \(c>0\) such that \(a_{n} cb_{n}\) for \(n 0\).

## 2 Related Work

**Federated Learning.** With few exceptions (see e.g. ), most of the FL literature is devoted to federated stochastic gradient (SG) methods. A strong focus has been placed on the Federated Averaging (FedAvg) algorithm , which aims to reduce communication through local training, resulting in _local drift_ when agents are heterogeneous . Sample and communication complexity of FedAvg were investigated under a variety of conditions covering both homogeneous [31; 20] and heterogeneous agents [25; 27]. Different ways of measuring heterogeneity for FedAvg have then been proposed [51; 41]. In  it was also shown that FedAvg yields linear speedup in the number of agents when gradients are stochastic, a phenomenon that we prove is still present in FedLSA.

In order to correct the client drift of FedAvg,  proposed Scaffold, a method that tames heterogeneity using control variates. [17; 38] prove that Scaffold retrieves the rate of convergence of the gradient descent independently of heterogeneity, although without benefit from local training. It has

   Algorithm & Communication \(T\) & Local updates \(H\) & Sample complexity \(TH\) \\  FedLSA  & \((}{a^{2}^{2}})\) & \(1\) & \((}{a^{2}^{2}})\) \\  FedLSA (Cor. 4.3) & \((})\) & \((^{2}})\) & \((^{2}})\) \\ Scaffnew (Cor. F.3) & \((})\) & \((^{2}})\) & \((^{2}})\) \\ SCAFFLSA (Cor. 5.2) & \((})\) & \((})\) & \((^{2}})\) \\   

Table 1: Communication and sample complexity for finding a solution with MSE lower than \(^{2}\) for FedLSA, Scaffnew, and SCAFFLSA with i.i.d. samples (see Cor. 4.3 for results with Markovian samples). Our analysis is the first to show that FedLSA exhibits linear speed-up, as well as its variant that reduces bias using control variates.

been shown in  (with the analysis of ProxSkip, which generalizes Scaffold) that such methods _accelerate_ training. However, unlike Scaffold, the analysis of  loses the linear speedup in the number of agents. Several other methods with accelerated rates have been proposed [35; 5; 6; 18; 21], albeit all of them lose the linear speedup. Contrary to these papers, we show that our approach to FedLSA with control variates _preserves both the acceleration and the linear speedup_.

**Federated TD learning.** Temporal difference (TD) learning has a long history in policy evaluation [47; 9], with the asymptotic analysis under linear function approximation (LFA) setting performed in [49; 48]. Several non-asymptotic MSE analyses have been carried out in [4; 8; 42; 32; 45]. Much attention has been paid to federated reinforcement learning [33; 43; 52] and federated TD learning with LFA. [26; 7; 34] provides an analysis under the strong homogeneity assumption. Federated TD was also investigated with heterogeneous agents, first without local training , then with local training but without linear acceleration [11; 22]. Recently,  proposed an analysis of federated TD with heterogeneous agents, local training, and linear speed-up in number of agents. However,  do not mitigate the local drift effects, and their conclusions are valid only in the low-heterogeneity setting. In high heterogeneity settings, their analysis exhibits a large bias. Additionally, their analysis requires the server to project aggregated iterates to a ball of unknown radius. In contrast, our analysis shows that FedLSA converges to the true solution without bias even without such projection.

## 3 Federated Linear Stochastic Approximation and TD learning

### Federated Linear Stochastic Approximation

In federated linear stochastic approximation, \(N\) agents collaboratively solve a system linear equation system with the following finite sum structure

\[}_{}=}\,}=_{c=1}^{N}}^{c}\,}=_{c=1}^{N}}^{c}\,\]

where for \(c[N]\), \(}^{c}^{d d}\), \(}^{c}^{d}\). We assume the solution \(_{}\) to be unique, and that each local system \(}^{c}_{}^{c}=^{c}\) also has a unique solution \(_{}^{c}\). The values of \(}^{c}\)'s and \(}^{c}\)'s can be different, representing the different realities of the agents. In federated LSA, neither matrices \(}^{c}\) nor vectors \(}^{c}\) are observed directly. Instead, each agent \(c[N]\) has access to its own observation sequence \((Z_{k}^{c})_{k}\), that are independent from one agent to another. Agent \(c\) obtains estimates \(\{(^{c}(Z_{k}^{c}),^{c}(Z_{k}^{c}))\}_{k}\) of \(}^{c}\) and \(}^{c}\), where \(^{c}:^{d d}\) and \(^{c}:^{d}\) are two measurable functions. Naturally, we define the error of estimation of \(}^{c}\) and \(}^{c}\) as \(}^{c}(z)=^{c}(z)-}^{c}\), \(}^{c}(z)=^{c}(z)-}^{c}\). This allows to measure the noise at local and global solutions as

\[^{c}(z)=}^{c}(z)_{}^{c}-}^{c}(z)\,^{c}(z)=}^{c}(z)_{}-}^{c}(z)\,\] (2)

together with the associated covariances,

\[_{}}^{c}=\!_{}}^{c}( z)}^{c}(z)^{}_{c}(z)\,\ _{}^{c}=\!_{} ^{c}(z)^{c}(z)^{}_{c}(z)\,\ _{ }^{c}=\!_{}^{c}(z)^{c}(z)^{}_ {c}(z)\,\] (3)

that are finite whenever one of the following assumptions on the \(\{Z_{t,h}^{c}\}_{t,h 0}\) hold.

**A1**.: _For each agent \(c\), \((Z_{k}^{c})_{k}\) are i.i.d. random variables with values in \((,)\) and distribution \(_{c}\) satisfying \(_{_{c}}[^{c}(Z_{k}^{c})]=}^{c}\) and \(_{_{c}}[(Z_{k}^{c})]=}^{c}\), and we define \(_{}=_{c}\|}^{c}\|\)._

**A2**.: _For each \(c[N]\), \((Z^{c}_{k})_{k}\) is a Markov chain with values in \((,)\), with Markov kernel \(_{c}\). The kernel \(_{c}\) admits a unique invariant distribution \(_{c}\), \(Z^{c}_{0}_{c}\), and \(_{c}\) is uniformly geometrically ergodic, that is, there exist \(_{}(c)\), such that for any \(k\),_

\[_{z,z^{}}(1/2)\|^{k}_{c}(|z)- ^{k}_{c}(|z^{})\|_{}(1/4)^{ k/_{}(c)}\,\]

_and for \(c[N]\), we have \(_{_{c}}[^{c}(Z^{c}_{1})]=}^{c}\) and \(_{_{c}}[(Z^{c}_{1})]=}^{c}\), and we define_

\[\|c\|_{}=_{c[N]}_{z}}\|^{c}(z) \|<\,_{}=_{c[N]}_{z}}\|^{c}(z)\|<\.\]

_Moreover, each of the matrices \(-}^{c}\) is Hurwitz._

In A2, random matrices \(^{c}(z)\) and noise variables \(^{c}(z)\) are almost surely bounded. This is necessary for working with the uniformly geometrically ergodic Markov kernels \(_{c}\). For simplicity, we state most of our results using A1, which is classical in finite-time studies of LSA [46; 14]. Nonetheless, we show that our analysis of FedLSA can be extended to the Markovian setting under A2.

In a federated environment, agents can only communicate via a central server, which is generally costly. Hence, in FedLSA, agents' local updates are only aggregated after a given time. During the round \(t 0\), the agents start with a shared value \(_{t}\) and perform \(H>0\) local updates, for \(h=1\) to \(H\), given by the recurrence

\[^{c}_{t,h}=^{c}_{t,h-1}-(^{c}(Z^{c}_{t,h})^{c }_{t,h-1}-^{c}(Z^{c}_{t,h}))\,\] (4)

with \(^{c}_{t,0}=_{t}\), and where we use the alias \(Z^{c}_{t,h}=Z_{Ht+b}\) to simplify notations. Agents then send \(_{t,H}\) to the server, that aggregates them as \(_{t}=N^{-1}_{c=1}^{N}^{c}_{t-1,H}\) and sends it back to all agents. We summarize this procedure in Algorithm 1. Our next assumption, which holds whenever \(}^{c}\) is Hurwitz [19; 39; 15], ensures the stability of the local updates.

**A3**.: _There exist \(a>0\), \(_{}>0\), such that \(_{}a 1/2\), and for \((0;_{})\), \(c[N]\), \(u^{d}\), it holds for \(Z^{c}_{0}_{c}\), that \(^{1/2}\|(-^{c}(Z^{c}_{0}))u\|^{2} (1- a)\|u\|\)._

### Federated Temporal Difference Learning

A major application of FedLSA is federated TD learning with linear function approximation. Consider \(N\) Markov Decision Processes \(\{(,,^{c}_{},r^{c},)\}_{c[ N]}\) with shared state space \(\), action space \(\), and discounting factor \((0,1)\). Each agent \(c[N]\) has its own transition kernel \(^{c}_{}\), where \(^{c}_{}(|s,a)\) specifies the transition probability from state \(s\) upon taking action \(a\) for this specific agent, as well as its own reward function \(r^{c}:\), that we assume to be deterministic for simplicity. Agents' heterogeneity lies in the different transition kernels and reward functions, that are _specific to each agent_.

In federated TD learning, all agents use the same shared policy \(\), and aim to construct a single shared function, that simultaneously approximates all value functions, defined as, for \(s\) and \(c[N]\),

\[V^{c,}(s)=_{k=0}^{}^{k}r^{c}(S^{ c}_{k},A^{c}_{k})\,\ \ S^{c}_{0}=s,\ A^{c}_{k}(|S^{c}_{k}),\ \ S^{c}_{k+1}^{c}_{}(|S^{c}_{k},A^{c}_{k} )\,.\]

In the following, we aim to approximate \(V^{c,}(s)\) as a linear combination of features built using a mapping \(:^{d}\). Formally, we look for \(^{d}\) such that the function \(_{}(s)=^{}(s)\) properly estimate the true value. For \(c[N]\), we denote \(^{c}\) the invariant distribution over \(\) induced by the policy \(\) and transition kernel \(^{c}_{}\) of agent \(c\). Our goal is to find a parameter \(^{c}_{}\) which is defined as a unique solution to the projected Bellman equation, see , which defines the best linear approximation of \(V^{c,}\). This problem can be cast as a federated LSA problem [42; 50] by viewing the local optimum parameter \(^{c}_{}\) as the solution of the system \(}^{c}^{c}_{}=}^{c}\), where

\[}^{c}=_{s^{c},s^{} P^{s,c}(|s)}[ (s)\{(s)-(s^{})\}^{}]\,}^{c}=_{s^{c},a( |s)}[(s)r^{c}(s,a)]\.\] (5)

The global optimal parameter is then defined as the solution \(_{}\) of the averaged system \((_{c=1}^{N}}^{c})_{}=_{c= 1}^{N}}^{c}\). As it is the case for federated LSA, this parameter may give a better overall estimation of the value function. Indeed, the distribution \(^{c}\) of some agents may bestrongly biased towards some states, whereas obtaining an estimation that is more balanced across all states may be more relevant.

In practice, when computing value functions, the tuples \(\{(S_{k}^{c},A_{k}^{c},S_{k+1}^{c})\}_{k}\) are sampled along one of the two following rules.

_TD_ 1. \((S_{k}^{c},A_{k}^{c},S_{k+1}^{c})\) _are generated i.i.d.with_ \(S_{k}^{c}^{c}\)_,_ \(A_{k}^{c}(|S_{k}^{c})\)_,_ \(S_{k+1}^{c}_{}^{c}(|S_{k}^{c},A_{k}^{c})\) _._

_TD_ 2. \((S_{k}^{c},A_{k}^{c},S_{k+1}^{c})\) _are generated sequentially with_ \(A_{k}^{c}(|S_{k}^{c})\)_,_ \(S_{k+1}^{c}_{}^{c}(|S_{k}^{c},A_{k}^{c})\) _._

The generative model assumption **TD** 1 is common in TD learning . It is possible to generalize all our results to the more general Assumption **TD** 2, sampling over a single trajectory and leveraging the Markovian noise dynamics. This would have a similar impact on our results on TD(0) as it has on the ones we will present for general FedLSA in Section 4. In our analysis, we require the following assumption on the feature design matrix \(_{}^{c}=_{^{c}}[(S_{0}^{c})(S_{0}^{c}) ^{}]^{d d}\).

_TD_ 3. _Matrices_ \(_{}^{c}\) _are non-degenerate with the minimal eigenvalue_ \(=_{c[N]}_{}(_{}^{c})>0\)_. Moreover, the feature mapping_ \(()\) _satisfies_ \(_{s}\|(s)\| 1\)_._

This assumption ensures the uniqueness of the optimal parameter \(_{}^{c}\). Under **TD** 1 and **TD** 3 we check the LSA assumptions A1 and A3, and the following holds.

**Claim 3.1**.: _Assume **TD** 1 and **TD** 3. Then the sequence of TD(0) updates satisfies A1 and A3 with_

\[_{}=1+\,\|_{}^{c}\|  2(1+)^{2}\,(_{}^{c}) 2(1+ )^{2}(\|_{}^{c}\|^{2}+1)\,\] \[a=\,_{}=\.\]

We prove this claim in Appendix E.1, and refer to  for more details on the link between TD and linear stochastic approximation.

## 4 Refined Analysis of the FedLSA Algorithm

### Stochastic expansion for FedLSA

We use the error expansion framework  for LSA to analyze the MSE of the estimates \(_{t}\) generated by Algorithm 1. For this purpose, we rewrite local update (4) as \(_{t,h}^{c}-_{}^{c}=(-(Z_{t,h}^{c}))( _{t,h-1}^{c}-_{}^{c})-^{c}(Z_{t,h}^{c})\), where \(^{c}(z)\) is defined in (2). Running this recursion until the start of local training, we obtain

\[_{t,H}^{c}-_{}^{c}=_{t,1:H}^{(c,)}\{_{t,0}^{ c}-_{}^{c}\}-_{h=1}^{H}_{t,h+1:H}^{(c,)} ^{c}(Z_{t,h}^{c})\,\]

where \(^{c}(z)\) is as in (3), and we recall that \(_{t,0}^{c}=_{t-1}\), \( c[N]\). We also introduced the notation

\[_{t,m:n}^{(c,)}=_{h=m}^{n}(-(Z_{t,h}^{c }))\, 1 m n H\,\]

with the convention \(_{t,m:n}^{(c,)}=\) for \(>n\). Note that by A3, \(_{t,m:n}^{(c,)}\) is exponentially stable. That is, for any \(h\), we have \(^{1/2}\|_{t,m:m+h}^{(c,)}u\|^{2}(1- a )^{h}\|u\|\). Using the fact \(_{t,0}^{c}=_{t-1}\), and employing (1), we obtain that

\[_{t}-_{}=_{t,H}^{()}\{_{t-1}-_{ }\}+_{H}+_{t,H}-_{t,H}\,_{t,H}^{()}=N^{-1}_{c=1}^{N} _{t,1:H}^{(c,)}\,\] (6)

where \(_{t,H}=_{c=1}^{N}\{(-^{c})^{H }-_{t,1:H}^{(c,)}\}\{_{}^{c}-_{}\}\), \(_{t,H}=_{h=1}^{N}_{h=1}^{H}_{t,h+1:H}^{(c,)}^{c}(Z_{t,h}^{c})\) are zero-mean fluctuation terms, and

\[_{H}=_{c=1}^{N}(-(-}^{c})^{H})\{_{}^{c}-_{}\}\]

is the deterministic heterogeneity bias accumulated in one round of local training. Note that \(_{H}\) vanishes when either (i) agents are homogeneous, or (ii) number of local updates is \(H=1\). To analyze FedLSA, we run the recurrence (6) to obtain the decomposition

\[_{t}-_{}=_{t}^{()}+_{t}^{ (,)}+_{t}^{()}\.\] (7)

Here \(_{t}^{()}=_{s=1}^{t}_{s,H}^{()}\{ _{0}-_{}\}\) is a transient term that vanishes geometrically, \(_{t}^{()}\) is a zero-mean fluctuation term, with detailed expression provided in Appendix A, and the term \(_{t}^{(,)}\) is

\[_{t}^{(,)}=_{s=1}^{t}(_{H}^{( )})^{t-s}_{H}\,_{H}^{()}=[_{s,H}^{()}]\,\]

and accounts for the bias of FedLSA due to local training, that vanishes whenever \(_{H}=0\).

### Convergence rate of FedLSA for i.i.d. observation model

First, we analyze the rate at which FedLSA converges to a biased solution \(_{}+_{t}^{()}\). The following two quantities, which stem from the heterogeneity and stochasticity of the local estimators, play a central role in this rate

\[_{}=_{c}[(_{ }^{c})]\,_{}=_{c}[\|_{ }}^{c}\|\|_{}^{c}-_{}\|^{2}]\.\]

Here \(_{}\) and \(_{}\) correspond to the different sources of noise in the error decomposition (7). The term \(_{}\) is related to the variance of the _local_ ASA iterate on each of the agents, while \(_{}\) controls the bias fluctuation term. In the centralized setting (i.e. if \(N=1\)), the \(_{}\) term disappears, but not the \(_{}\) term. We now proceed to analyze the MSE of the iterates of FedLSA :

**Theorem 4.1**.: _Assume A1 and A3. Then for any step size \((0,_{})\) it holds that_

\[^{1/2}\|_{t}-_{t}^{()}- _{}\|^{2}_{}}{  N}}+_{}}{ N}}+_{c}[\|_{}}^{c}\|]}{HN}}_{H}\|}{}+(1- a)^{tH}\|_{0}-_{}\|\,\]

_where the bias \(_{t}^{()}\) converges in expectation to \(_{}^{()}=(-_{H}^{() })^{-1}_{H}\) at a geometric rate, and is uniformly bounded by \(^{1/2}[\|_{t}^{()}\|^{2}]_{c}[\|_{}^{c}-_{}\|]}{a}\)._

The proof of Theorem 4.1 relies on bounding each term from (7). We provide a proof with explicit constants in Appendix A. Importantly, the fluctuation terms scale linearly with \(N\). Moreover, in the centralized setting (that is, \(N=1\)), the bias terms \(_{H},_{t}^{()}\) and \(_{}\) vanish in Theorem 4.1, yielding the last-iterate bound

\[^{1/2}\|_{t}-_{}\|^{2}_{}}{a}}+(1- a)^{tH}\|_{0}-_{} \|\,\]

which is known to be sharp in its dependence on \(\) for single-agent LSA (see Theorem 5 in ). Based on Claim 3.1, Theorem 4.1 translates for federated TD(0) as follows.

**Corollary 4.2**.: _Assume **TD 1** and **TD 3**. Then for any step size \((0,)\), the iterates of federated TD(0) satisfy, with \((_{},_{}^{1},,_{}^{N})=_{c }[\|_{}^{c}-_{}\|^{2}](1+_{c}[\|_{ }^{c}\|^{2}])\),_

\[^{1/2}\|_{t}-_{t}^{()}- _{}\|^{2},_{ }^{1},,_{}^{N})}{(1-) N}}+} _{H}\|}{(1-)}+(1-)^{tH }\|_{0}-_{}\|\.\]

The right-hand side of Corollary 4.2 scales linearly with \(N\), allowing for linear speed-up. This is in line with recent results on federated TD(0), which shows linear speed-up either without local training  or up to a possibly large bias term  (see analysis of their Theorem 2). While Corollary 4.2 shows the algorithm's convergence to some fixed, biased value, one can set the parameters of FedLSA such that this bias is small. This allows to rewrite the result of Theorem 4.1 in order to get a sample complexity bound in the following form.

**Corollary 4.3**.: _Assume A1 and A3. Let \(H>1\), and \(0<<}_{}} _{c}[\|_{}^{c}-_{}\|])^{2/5}}{_{c}[\|_{}^{c}-_{}\|]}{a}}}_{c}[\|_{}^{c}-_{}\|]}{a\,_{}}\). Set the step size \(=}{_{} _{}}_{}\) and the number of local steps \(H=_{}_{} }{_{c}[\|_{}^{c}-_{}\|]} \). Then, to achieve \(\|_{T}-_{}\|^{2}<^{2}\) the required number of communications for federated LSA is_

\[T=((}_{c}[\| _{}^{c}-_{}\|]}{a^{2}})-_{}\|}{})\.\]

In Corollary 4.3, the number of oracle calls scales as \(TH=_{}_{} }{Na^{2}^{2}}-_{}\|}{a}\), which shows that FedLSA has linear speed-up. Importantly, the number of communications \(T\) required to achieve precision \(^{2}\) scales as \(^{-1}\). In the next section, we will show how this dependence on \(^{-1}\) can be reduced from polynomial to logarithmic. Now we state the communication bound of federated TD(0).

**Corollary 4.4**.: _Assume **TD 1** and **TD 3**. Then for any \(0<<(_{}^{c},_{})}{(1-)}\) with \(g_{1}=((1+\|_{}\|)_{c}[\|_{}^{c}- _{}\|])\). Set \(=(}{_{c}[\| _{}^{c}\|^{2}]+1})\) and \(H=(_{c}[\|_{}^{c}\|^{2}+1]}{N _{c}[\|_{}^{c}-_{}\|^{2}]})\). Then, to achieve \(\|_{T}-_{}\|^{2}<^{2}\), the required number of communications for federated TD(0) is_

\[T=((}_{c}[ \|_{}^{c}-_{}\|]}{(1-)^{2}^{2}^{2}} -_{}\|}{})\.\]

Corollary 4.4 is the first result to show that, even with local training and heterogeneous agents, federated TD(0) can converge to \(_{}\) with arbitrary precision. Importantly, this result preserves the _linear speed-up effect_, showing that federated learning indeed accelerates the training.

### Convergence of FedLSA under Markovian observations model

The analysis of FedLSA can be generalized to the setting where observations \(\{Z_{k}^{c}\}_{k}\) form a Markov chain with kernel \(_{c}\). To handle the Markovian nature of observations, we propose a variant of FedLSA that skips some observations (see the full procedure in Appendix B). This follows classical schemes for Markovian data in optimization , as adjusting the number of skipped observations (keeping about \(1\) observation out of \(_{}(c)\)) allows to control the correlation of successive observations. We may now state the counterpart of Corollary 4.3 for the Markovian setting.

**Corollary 4.5** (Corollary 4.3 adjusted to the Markov samples).: _Assume A2 and A3 and let \(0<<}}} _{c}[\|_{}^{c}-_{}\|])^{2/5}}{a}_{c}[\|_{}^{c}-_{}\|]}{a\,_{}}\). Set the step size \(=}{_{}}}_{}_{}^{()}\), where we give the expression of \(_{}^{()}\) is (37). Then, for the iterates of Algorithm 3, in order to achieve \(\|_{T}-_{}\|^{2}^{2}\), the required number of communication is_

\[T=((}}_{ c}[\|_{}^{c}-_{}\|]}{a^{2}})-_{}\|}{})\,\]

_where the number of local updates \(H\) satisfies_

\[=_{}}}{_{c}[\|_{}^{c}-_{}\|]}_{}(c)(NT^{3}(\|_{0}-_{}\|+ 2_{c}[\|_{}^{c}-_{}\|+\|\|\|_{})/ ^{2})}{N})\.\]

The proof of Corollary 4.3 follows the idea outlined in , using Berbee's lemma . We give all the details in Appendix B. This result is very similar to Corollary 4.3. Most crucially, it shows that the communication complexity is the same, regardless of the type of noise. The differences with Corollary 4.3 lie in (i) the number local updates \(H\), that is scaled by \(_{}\) (up to logarithmic factors), and (ii) the additional condition \(_{}^{()}\), that allows verifying the stability of random matrix products with Markovian dependence (see Lemma B.2 in the appendix).

_Remark 4.6_.: Although, for clarity of exposition, we only state the counterpart of Corollary 4.3 in the Markovian result, all of our results can be extended to Markovian observations using the same ideas.

## 5 SCAFFLSA: Federated LSA with Bias Correction

### Stochastic Controlled Averaging for Federated LSA

We now introduce the _Stochastic Controlled Averaging for Federated LSA_ algorithm (SCAFFLSA), an improved version of FedLSA that mitigates client drift using control variates. This method is inspired by _Scaffnew_ (see 37). In SCAFFLSA, each agent \(c[N]\) keeps a local variable \(_{t}^{c}\), that remains constant during each communication round \(t\). Agents perform local updates on the current estimates of the parameters \(_{t,0}^{c}=_{t}\) for \(c[N]\), and for \(h[H]\),

\[_{t,h}^{c}=_{t,h-1}^{c}-(^{c}(Z_{t,h}^{ c})_{t,h-1}^{c}-^{c}(Z_{t,h}^{c})-_{t}^{c})\,\]

At the end of the round, (i) the agents communicate the current estimate to the central server, (ii) the central server averages local iterates, and (iii) agents update their local control variates; see Algorithm 2. By defining the _ideal_ control variates at the global solution, given by \(_{}^{c}=}^{c}_{}-}^{c}=}^{c}(_{}-_{}^{c})\), we can rewrite the local update as

\[_{t,h}^{c}-_{}=(-^{c}(Z_{t,h}^{ c}))(_{t,h-1}^{c}-_{})+(_{t}^{c}-_{}^{c})- ^{c}(Z_{t,h}^{c})\,\] (8)

where \(^{c}(z)\) is defined in (2). Under A1, it has finite covariance \(_{}^{c}=_{}^{c}(z)^{c}(z)^{}_{c}(z)\).

Similarly to the analysis of FedLSA, we use (8) to describe the sequence of aggregated iterates and control variates as, for \(t 0\) and \(c[N]\),

\[_{t+1}-_{}&=_{t,H}^{()}(_{t}-_{})+_{c=1}^{N} C_{t+1}^{c}(_{t}^{c}-_{}^{c})-_{t+1}\,\\ _{t+1}^{c}-_{t}^{c}&=_{t}^{c}-_{}^{ c}+(_{t+1}-_{t,H})\,\] (9)

where \(C_{t+1}^{c}=_{h=1}^{H}\)\({}_{t,h+1:H}^{(c,)}\) and \(_{t+1}=_{c=1}^{N}_{h=1}^{H}\)\({}_{t,h+1:H}^{(c,)}^{c}(Z_{t,h}^{c})\). We now state the convergence rate, as well as sample and communication complexity of Algorithm 2.

**Theorem 5.1**.: _Assume A1, A3. Let \(,H>0\) such that \(_{}\), and \(H}{{240}}\{_{}^{2}+\|_{}^{c}\|\}\). Set \(_{0}^{c}=0\) for all \(c[N]\). Then we have_

\[[\|_{T}-_{}\|^{2}]\|_ {}\|+1-^{T}\|_{0}-_{ }\|^{2}+^{2}H^{2}_{c}[\|}^{c}(_{ }^{c}-_{})\|^{2}]}\;.\]

**Corollary 5.2**.: _Let \(>0\). Set the step size \(=((_{},N^{a}/}{{}}))\) and the number local updates to \(H=+\|\|)}+\|\|)}\). Then, to achieve \([\|_{T}-_{}\|^{2}]^{2}\), the required number of communication for SCAFFLSA is_

\[+\|\|}{a}[\|(-)\|]a/}{} \;.\]

We provide detailed proof of these statements in Appendix C. They are based on a novel analysis, where we study virtual parameters \(_{t,h}^{c}\), that follow the same update as (8), without the last term \(^{c}(Z_{t,h}^{c})\). After each round, virtual parameters are aggregated, and virtual control variate updated as

\[_{t+1}-_{}=_{c=1}^{N}_{t,H}^{c}\;,\;\;_{t+1}^{c}-_{}^{c}= _{t}^{c}-_{}^{c}+(_{t+ 1}-_{t,H})\;.\]

This allows to decompose

\[_{t}-_{}=_{t}-_{}+ _{t}\;,\;\;_{t}^{c}-_{}^{c}=_{t}^{c}-_{ }^{c}+_{t}^{c}\;,\]

where \(_{t}-_{}\) and \(_{t}^{c}-_{}^{c}\) are transient terms, and \(_{t}=_{t}-_{t}\) and \(_{t}^{c}=_{t}^{c}-_{t}^{c}\) capture the fluctuations of the parameters and control variates.

We stress that our analysis shows that, in comparison with FedLSA, the SCAFFLSA algorithm reduces communication complexity while preserving the _linear speed-up in the number of agents_. This is in stark contrast with existing analyses of control-variate methods in heterogeneous federated learning, that either have large communication cost, or lose the linear speed-up [24; 37; 21]. To obtain this result, we conduct a very careful analysis of the propagation of variances and covariances of \(_{t}\) and \(_{t}^{c}\) between successive communication rounds. We describe this in full detail in Appendix C.2.

In Corollary 5.2, we show that the total number of communications depends only logarithmically on the precision \(\). This is in stark contrast with Algorithm 1, where the necessity of controlling the bias' magnitude prevents from scaling \(H\) with \(}{{}}\). Additionally, this shows that the number of required local updates reduces as the number of agents grows. Thus, in the high precision regime (i.e.small \(\) and \(\)), using control variates reduces communication complexity compared to FedLSA.

### Application to Federated TD(0)

Applying SCAFFLSA to TD learning, we obtain SCAFFTD(0) (see Algorithm 5 in Appendix E). The analysis of SCAFFLSA directly translates to SCAFFTD(0), resulting in the following communication complexity bound.

**Corollary 5.3**.: _Assume **TD 1** and **TD 3** and let \(0<_{c}[1+\|_{}^{c}\|^{2}]/((1-) )}\). Set the step size \(=()\) and the number local updates to \(H=()\). Then, to achieve \([\|_{T}-_{}\|^{2}]^{2}\), the required number of communication for SCAFFTD(0) is_

\[T=([\|-\|]}{})\;.\]Corollary 5.3 confirms that, when applied to TD(0), SCAFFLSA's communication complexity depends only logarithmically on heterogeneity and on the desired precision. In contrast with existing methods for federated TD(0) [11; 22; 50], it converges even with many local steps, whose number diminishes linearly with the number of agents \(N\), producing the linear speed-up effect.

_Remark 5.4_.: In Appendix F, we extend the analysis of Scaffnew to the LSA setting. Their analysis does not exploit the fact that agents' estimators are not correlated, and thus lose the linear speed-up. In contrast, our novel analysis technique carefully tracks correlations between parameters and control variates throughout the run of the algorithm.

## 6 Numerical Experiments

In this section, we demonstrate the performance of FedLSA and SCAFFLSA under varying levels of heterogeneity. We consider the Garnet problem [2; 16], with \(n=30\) states embedded in \(d=8\) dimensions, \(a=2\) actions, and each state is linked to \(b=2\) others in the transition kernel. We aim to estimate the value function of the policy which chooses actions uniformly at random, in homogeneous and heterogeneous setups. In all experiments, we initialize the algorithms in a neighborhood of the solution, allowing to observe both transient and stationary regimes. We provide all details regarding the experimental setup in Appendix G. Our code is available either as supplementary material or online on GitHub: https://github.com/pmangold/scafflsa.

**SCAFFLSA properly handles heterogeneity.** This heterogeneous scenario is composed of two different Garnet environments, that are each held by half of the agents, with small perturbations. Such a setting may arise in cases where each agent's environment reflects only a part of the world. For instance, if half of the individuals live in the city, while the other half live in the countryside: both have different observations, but learning a shared value function gives a better representation of the overall reality. In Figures 1(a) to 1(d), we plot the MSE with \(N\{10,100\}\), \(H\{10,1000\}\) and \(=0.1\), with the same total number of updates \(TH=500,000\). As predicted by our theory, FedLSA stalls when the number of local updates increases, and its bias (green dashed line in Figures 1(a) to 1(d)) is in line with the value predicted by our theory (see Theorem 4.1). For completeness, we plot the error of FedLSA in estimating \(_{}+_{}^{(,)}\) in Appendix G. On the opposite, SCAFFLSA's bias-correction mechanism allows to eliminate all bias, improving the MSE until noise dominates.

Figure 1: MSE as a function of the number of communication rounds for FedLSA and SCAFFLSA applied to federated TD(0) in homogeneous and heterogeneous settings, for different number of agents and number of local steps. Green dashed line is FedLSA’s bias, as predicted by Theorem 4.1. For each algorithm, we report the average MSE and variance over \(5\) runs.

**Both algorithms behave alike in homogeneous settings.** In the homogeneous setting, we create _one_ instance of a Garnet environment. Then, each agent receives a slightly perturbed variant of this environment. This illustrates a situation where all agents solve the same exact problem, but may have small divergences in their measures of states and rewards. We plot the MSE in Figures 1(e) to 1(h) with \(N\{10,100\}\) agents, \(=0.1\), and \(H\{10,1000\}\), with the same total number of updates \(TH=500,000\). In this case, as predicted in Corollary 4.3, the number of local steps \(H\) has little influence on the final MSE. Since agents are homogeneous, control variates have virtually no effect, and SCAFFLSA is on par with FedLSA. The MSE is dominated by the noise term, which diminishes with the step size (see additional experiments in Appendix G with smaller \(=0.01\)).

**Both algorithms enjoy linear speed-up!** In Figure 2, we plot the MSE obtained once algorithms reach the stationary regime, as a function of the number of agents \(N=1\) to \(1000\), for step sizes \(\{0.001,0.01,0.1,1\}\) and \(H\{1,100\}\), in both homogeneous and heterogeneous settings. Whenever (i) agents are homogeneous, or (ii) the number of local steps is small, both FedLSA and SCAFFLSA can achieve similar precision with a step size that increases with the number of agents. This allows to use larger step sizes, so as to reach a given precision level faster, resulting in the so-called _linear speed-up_. However, when agents are heterogeneous and the number of local updates increases, FedLSA loses the speed-up due to large bias. Remarkably, and as explained by our theory (see Corollary 5.2), SCAFFLSA maintains this speed-up even in heterogeneous settings.

## 7 Conclusion

In this paper, we studied the role of heterogeneity in federated linear stochastic approximation. We proposed a new analysis of FedLSA, where we formally characterize FedLSA's bias. This allows to show that, with proper hyperparameter setting, FedLSA (i) can converge to arbitrary precision even with local training, and (ii) enjoys linear speed-up in the number of agents. We then proposed a novel algorithm, SCAFFLSA, that uses control variates to allow for extended local training. We analyzed this method using on a novel analysis technique, and formally proved that control variates reduce communication complexity of the algorithm. Importantly, our analysis shows that SCAFFLSA preserves the linear speed-up, which is the first time that a federated algorithm provably accelerates while preserving this linear speed-up. Finally, we instantiated our results for federated TD learning, and conducted an empirical study that demonstrates the soundness of our theory in this setting.