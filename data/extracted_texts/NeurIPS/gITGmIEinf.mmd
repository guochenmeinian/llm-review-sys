# Approximating the Top Eigenvector in Random Order Streams

Praneeth Kacham

Google Research

pkacham@google.com

&David P. Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu

Work done while the author was a student at Carnegie Mellon University.

###### Abstract

When rows of an \(n d\) matrix \(A\) are given in a stream, we study algorithms for approximating the top eigenvector of the matrix \(A^{}A\) (equivalently, the top right singular vector of \(A\)). We consider worst case inputs \(A\) but assume that the rows are presented to the streaming algorithm in a uniformly random order. We show that when the gap parameter \(R=_{1}(A)^{2}/_{2}(A)^{2}=(1)\), then there is a randomized algorithm that uses \(O(h d(d))\) bits of space and outputs a unit vector \(v\) that has a correlation \(1-O(1/)\) with the top eigenvector \(v_{1}\). Here \(h\) denotes the number of _heavy rows_ in the matrix, defined as the rows with Euclidean norm at least \(\|A\|_{}/(d)}\). We also provide a lower bound showing that any algorithm using \(O(hd/R)\) bits of space can obtain at most \(1-(1/R^{2})\) correlation with the top eigenvector. Thus, parameterizing the space complexity in terms of the number of heavy rows is necessary for high accuracy solutions.

Our results improve upon the \(R=( n d)\) requirement in a recent work of Price and Xun (FOCS 2024). We note that the algorithm of Price and Xun works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order. We additionally show that the gap requirements in their analysis can be brought down to \(R=(^{2}d)\) for arbitrary order streams and \(R=( d)\) for random order streams. The requirement of \(R=( d)\) for random order streams is nearly tight for their analysis as we obtain a simple instance with \(R=( d/ d)\) for which their algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector \(v_{1}\).

## 1 Introduction

We consider the problem of approximating the top eigenvector in the streaming setting. In this problem, we are given vectors \(a_{1},,a_{n}^{d}\) one at a time in a stream. Let \(A\) be an \(n d\) matrix with rows \(a_{1},,a_{n}\). The task is to approximate the top eigenvector of the matrix \(A^{}A\). Throughout the paper, we use \(v_{1}^{d}\) to denote the top eigenvector of \(A^{}A\). We focus on obtaining streaming algorithms that use a small amount of space and can output a unit vector \(\) such that \(,v_{1}^{2} 1-f(R)\), where \(f(R)\) is a decreasing function in the gap \(R=_{1}(A^{}A)/_{2}(A^{}A)\). Here \(_{1}(),_{2}()\) denote the two largest eigenvalues. As the gap \(R\) becomes larger, the eigenvector approximation problem becomes easier and we want more accurate approximations to the eigenvector \(v_{1}\).

If one is allowed to use \((d^{2})^{2}\) bits of space, we can maintain the matrix \(A^{}A=_{i}a_{i}a_{i}^{}\) as we see the rows \(a_{i}\) in the stream, and at the end of processing the stream, we can compute the exact top eigenvector \(v_{1}\). When the dimension \(d\) is large, the requirement of \((d^{2})\) bits of memory can beimpractical (see e.g., applications that require a large value of \(d\) in Mitliagkas et al. (2013).) Hence, an interesting question is to study non-trivial streaming algorithms that use less memory. In this work, we focus on obtaining algorithms that use \((d)\) bits of space.

In the offline setting (where the entire matrix \(A\) is available to us), fast iterative algorithms such as Gu (2015); Musco and Musco (2015); Musco et al. (2018) can be used to quickly obtain accurate approximations to the top eigenvector when the gap \(R=(1)\). In a single pass streaming setting, we cannot run these algorithms as these iterative algorithms need to _see_ the entire matrix multiple times.

There have been two major lines of work studying the problem of eigenvector approximation and the related Principal Component Analysis (PCA) problem in the streaming setting with near-linear in \(d\) memory. In the first line of work, each row encountered in the stream is assumed to be sampled independently from an unknown distribution with mean \(0\) and covariance \(\) and the task is to approximate the top eigenvector of \(\) using the samples. In this line of work, the sample complexity required for algorithms using \(O(d(d))\) bits of space to output an approximation to \(v_{1}\), is the main question. The algorithms are usually a variant of Oja's algorithm (Oja, 1982; Jain et al., 2016; Allen-Zhu and Li, 2017; Huang et al., 2021; Kumar and Sarkar, 2023) or the block power method (Hardt and Price, 2014; Balcan et al., 2016). We note that Kumar and Sarkar (2023) relax the i.i.d. assumption and analyze the sample complexity of Oja's algorithm for estimating the top eigenvector in the Markovian data setting.

The other line of work studies algorithms for arbitrary streams appearing in an arbitrary order. In this setting, we want algorithms to work for _any_ input stream given in _any_ order. A problem closely related to the eigenvector estimation problem is the Frobenius-norm Low Rank Approximation (Clarkson and Woodruff, 2017; Boutsidis et al., 2016; Upadhyay, 2016; Ghashami et al., 2016). The deterministic Frequent Directions sketch of Ghashami et al. (2016) can, using \((d/)\) bits of space, output a unit vector \(u\) such that

\[\|A(I-uu^{})\|_{}^{2}(1+)\|A(I-v_{1}v_{1}^{ })\|_{}^{2}.\]

Although the vector \(u\) is a \(1+\) approximate solution to the Frobenius norm Low Rank Approximation problem, it is possible that the vector \(u\) may be (nearly) orthogonal to the top eigenvector \(v_{1}\). Hence the Frequent Directions sketch does not guarantee top eigenvector approximation. Recently, Price and Xun (2024) study the eigenvector approximation problem in arbitrary streams and obtain results in terms of the gap \(R\) of the instance. Price and Xun prove that when \(R=( n d)\), a variant of Oja's algorithm outputs a unit vector \(\) such that

\[,v_{1}^{2} 1--(d)}\]

where \(C\) is a large enough universal constant. On the lower bound side, Price and Xun showed that any algorithm that outputs a vector \(\) satisfying

\[,v_{1}^{2} 1-},\]

must use \((d^{2}/R^{3})\) bits of space while processing the stream. This lower bound shows that in the important case of \(R=O(1)\), the _correlation3_ that can be obtained by an algorithm using \((d)\) bits of space is at most a constant less than \(1\). Thus, the current best algorithms for arbitrary streams work only when \(R=( n d)\) and for the important case of \(R=O(1)\), there are no existing algorithms requiring significantly fewer than \(d^{2}\) bits of memory. They also give a lower bound on the size of _mergeable_ summaries for approximating the top eigenvector.

We identify an instance with \(R=( d/ d)\) where the algorithm of Price and Xun fails to produce a vector with even a constant correlation with the vector \(v_{1}\). This shows that their algorithm or other variants of Oja's algorithm may fail to extend to the case when \(R=O(1)\). We further show that the algorithm of Price and Xun fails to produce such a vector even when the rows in our hard instance are ordered uniformly at random, showing that even randomly ordered streams can be hard to solve for variants of Oja's algorithm.

In this work, we focus on algorithms that work on worst case inputs \(A\) while assuming that the rows of \(A\) are _uniformly randomly ordered_. This model is mid-way between the i.i.d. setting and the arbitrary order stream setting in terms of the generality of streams that can be modeled. We note that a number of works (Munro and Paterson, 1980; Guha et al., 2005; Chakrabarti et al., 2008; Guha and McGregor, 2009; Assadi and Sundaresan, 2023) have previously considered streaming algorithms and lower bounds for worst case inputs with random order streams, as it is a natural model often arising in practical settings. Our algorithms are parameterized in terms of the number of **heavy** rows in the stream. See Gupta and Singla (2021) for a gentle introduction to the random-order model. We define a row \(a_{i}\) to be _heavy_\(\|a_{i}\|_{2}\|A\|_{}/(d)}\). Note that in any stream of rows, by definition, there are at most \(d(d)\) heavy rows. We state our theorem informally below:

**Theorem 1.1**.: _Let \(a_{1},,a_{n}^{d}\) be a randomly ordered stream and let \(A\) denote the \(n d\) matrix with rows given by \(a_{1},,a_{n}\). If \(R=_{1}(A^{}A)/_{2}(A^{}A)>C\) for a large enough constant \(C\) and the number of heavy rows in the stream is at most \(h\), then there is a streaming algorithm using \(O(h d(d))\) bits of space and outputting a unit vector \(\) satisfying_

\[,v_{1}^{2} 1-O(1/)\]

_with a probability \( 4/5\)._

Our algorithm is a variant of the block power method. Along the way, we also improve the gap requirements in the results of Price and Xun (2024). We show that by subsampling a stream of rows, the algorithm of Price and Xun can be made to work even when the gap \(R\) is \((^{2}d)\) in arbitrary order streams, improving on the \(( n d)\) requirement in their analysis. We also show that in random order streams, a gap of \(( d)\) is sufficient for their algorithm, though our algorithm improves on this and works for even a constant gap.

Similar to the lower bound of Price and Xun, we show that any algorithm for random order streams must use \((h d/R)\) bits of space to output a vector \(\) satisfying \(,v_{1}^{2} 1-1/CR^{2}\) where \(C\) is a constant. We summarize the theorem below.

**Theorem 1.2**.: _Consider an arbitrary random order stream \(a_{1},,a_{n}\) with the gap parameter \((A)^{2}}{_{2}(A)^{2}}=R\). Let \(h\) be the number of heavy rows in the stream. Any streaming algorithm that outputs a unit vector \(\) such that_

\[,v_{1}^{2} 1-1/CR^{2}\]

_for a large enough constant \(C\), with a probability \( 1-(1/2)^{R+1}\) over the ordering of the stream and its internal randomness, must use \((h d/R)\) bits of space._

Techniques.The randomized power method (Gu, 2015) algorithm to approximate the top eigenvector samples a random Gaussian vector \(\) and iteratively computes the vector \(v=(A^{}A)^{t}^{}\) for \(t=( d)\) iterations and shows that when the gap \(R\) is large, \(v/\|v\|_{2}\) is a good approximation for \(v_{1}\). Thus, the algorithm needs to _see_ the quadratic form \(A^{}A\) multiple times and hence, it cannot be implemented in the single-pass streaming setting of this paper.

Assume that the stream is randomly ordered and that there are no heavy rows. Our key observation is that if the stream is long enough, then we can see \(t\) approximations \(_{j}^{}_{j}\)5 of the quadratic form \(A^{}A\). Here the matrices \(_{1},,_{t}\) are formed by sampling and rescaling the rows of the matrix \(A\) and importantly, the rows of \(_{1},,_{t}\) do not overlap in the stream, that is, they appear one after the other. Thus we can compute \(v^{}=(_{t}^{}_{t})(_{1}^{} _{1})\) for the starting vector \(\) in a single pass over the stream. We prove that such matrices \(_{j}\) exist using the row norm sampling result of Magdon-Ismail (2010). Now, the main issue is to show that \(v^{}/\|v^{}\|_{2}\) is a good approximation to the top eigenvector \(v_{1}\). We crucially use a singular value inequality of Wang and Xi (1997) to prove that \(\|_{j}^{}_{j}-A^{}A\|_{2}\|A\|_ {2}^{2}\) for all \(j\) suffices for \(v^{}/\|v^{}\|_{2}\) to be a good approximation to \(v_{1}\).

The above analysis assumes that there are no heavy rows. Indeed, suppose that a matrix \(A\) has a row \(a\) with a large Euclidean norm which is orthogonal to all the other rows. Also assume that the top eigenvector of the matrix \(A\) is in this direction. Since, the matrices \(_{1},,_{t}\) are non-overlapping substreams of the matrix \(A\), at most one of the matrices \(_{j}\) can have the row \(a\) and hence the vector \(v^{}/\|v^{}\|_{2}\) will not be a good approximation to \(a/\|a\|_{2}\), the top eigenvector. Thus, we need to handle the heavy rows separately. We show that, by storing all the rows with a Euclidean norm larger than \(\|A\|_{}/(d)}\) and running the above described algorithm on the remaining set of rows, we can obtain a good approximation to the top eigenvector.

Our lower bound (Theorem 1.2) shows that any single-pass streaming algorithm must use space proportional to the number of heavy rows, and therefore our procedure that handles the heavy rows separately gives near-optimal bounds.

Finally, the row norm sampling technique of Magdon-Ismail (2010) serves as a general technique to reduce the number of rows in the stream while (approximately) preserving the top eigenvector. We use this observation to improve the \(R=( n d)\) for arbitrary streams in Price and Xun (2024) to \(R=(^{2}d)\). We then show that assuming a uniformly random order, the analysis of Price and Xun (2024) can be improved to show that \(R=( d)\) suffices. Thus, for random order streams, techniques before our work can be used to approximate the top eigenvector when the gap \(R=( d)\). Our work improves upon this to give an algorithm that works for streams with \(R=(1)\).

Implications to practice.Often, in practical situations, we can assume that the rows being streamed are sampled independently from a nice-enough distribution, in which case Oja's algorithm, as discussed, can approximate the top eigenvector accurately given enough samples. However, _independence_ and assumptions on the covariance matrix can be very strong assumptions in some cases and in such cases, our algorithm only requires that the order of the rows in the stream be uniformly random, in which case we output an approximation with provable guarantees.

Organization.We first introduce the row-norm sampling procedure to obtain approximate quadratic forms. The proof is a slight modification of that of Magdon-Ismail (2010). The only difference is that we instead consider a version that samples each row in the input independently with some appropriate probability and keeps the rows that are sampled after scaling appropriately. We then introduce and analyze our block power iteration algorithm when all rows have roughly the same Euclidean norm, and then extend it to the general case, which is our main result. Finally, we provide a lower bound showing that \((td/R)\) bits of space is necessary to obtain constant correlation with the top eigenvector. Due to space constraints, all of our proofs are placed in the appendix.

## 2 Power Method with Approximate Quadratic Forms

In this section, we present and analyze our algorithm for approximating the top eigenvector of \(A^{}A\) when the rows of \(A\) are presented to the algorithm in a uniformly random order.

We first show a row sampling technique that reduces the number of rows in the stream. The row-norm sampling technique for approximating the quadratic form \(A^{}A\) with spectral norm guarantees was given by Magdon-Ismail (2010). The technique works irrespective of the order of the rows.

### Sampling for Row Reduction

**Theorem 2.1**.: _Let \(A\) be an arbitrary \(n d\) matrix. Given \(p^{n}\), let \(\) be an \(n n\) diagonal matrix such that for each \(i[n]\), we independently set \(_{ii}=1/}\) with probability \(p_{i}\) and \(0\) otherwise. If for all \(i\),_

\[p_{i}(1,C\|_{2}^{2}}{^{2}\|A\|_{2}^{2}}  d),\]

_then with probability \(1-1/(d)\), \(\|A^{}A-A^{}^{}A\|_{2} \|A\|_{2}^{2}\). With probability at least \(1-1/(d)\), the matrix \(\) has at most \(O(^{-2} d)\) non-zero entries, where \(=\|A\|_{}^{2}/\|A\|_{2}^{2}\) denotes the stable rank of matrix \(A\)._

Note that given the value of \(\|A\|_{2}\), the sampling procedure in this theorem can be performed in a stream. Additionally, as the original stream is uniformly randomly ordered, the sub-sampled stream is also uniformly randomly ordered assuming that the sampling is independent of the order of the rows.

Given that all of the non-zero entries of the matrix have absolute value at least \(1/(nd)\) and at most \((nd)\), we have that \(\|A\|_{2}^{2}\) lies in the interval \([1/(nd),(nd)]\). Thus, we can guess the value of \(\|A\|_{2}^{2}\) as \(2^{i}/(nd)\) for \(i=0,,O((nd))\) and one of these values must be a \(2\)-approximation to \(\|A\|_{2}^{2}\), and thus sub-sampling the rows using that guess satisfies the conditions in the above theorem. We can run the streaming algorithms on all the streams simultaneously to obtain \(O( nd)\) vectors \(u_{1},,u_{O( nd)}\) as the candidates for being an approximation to the top eigenvector. From Theorem 2.1, the candidate vector \(u_{j}\) computed on the stream obtained by sampling the rows with the correct probabilities is a good approximation to the top eigenvector, and therefore \(\|A u_{j}\|_{2}\) is large for that value of \(j\). Thus, the vector \(u_{j}\) with the largest value \(\|A u_{j}\|_{2}\) is a good approximation to the top eigenvector \(v_{1}\). If \(\) is a Gaussian matrix with \(O(^{-2} d)\) rows, then for all \(u_{j}\), we can approximate \(\|A u_{j}\|_{2}\) up to a \(1\) factor using \(\| A u_{j}\|_{2}\) by the Johnson-Lindenstrauss lemma. Additionally, the matrix \( A\) can be maintained in the stream using \(O(^{-2} d d)\) bits (when we see a row \(a_{i}\), we sample an independent Gaussian vector \(_{i}\) and add \(_{i}a_{i}^{}\) to an accumulator to maintain \( A\)). Thus, at the end of processing the stream, we can compute a vector \(u_{j}\) that has a large value \(\|A u_{j}\|_{2}\), and hence is a good approximation for \(v_{1}\).

If we can process each created stream using \(s\) bits of space, then the overall space requirement is \(O(s(nd)+d(d))\) bits, using \(O(s)\) bits for each guess for the value of \(\|A\|_{2}^{2}\) and \(O(d(d))\) bits for storing a Gaussian sketch of the matrix with \(=1/(d)\).

### Random-Order Streams with bounds on Norms

``` Input: An \(n d\) matrix \(A\) with \(n=((A)^{2}d/^{2})\), \(_{i}\|a_{i}\|_{2}^{2}/_{i}\|a_{i}\|_{2}^{2}\) Output: A vector \(\)
1\(t C_{1} d\)
2Compute \( A\) in the stream where \(\) is a Gaussian matrix with \(O(^{-2} d)\) rows
3for\(=1,2,4,,d\) simultaneouslydo
4\(p C_{2} d/n^{2}\)// \(p 1/(5t)\) for\( 2(A)\)
5\(_{} N(0,1)^{d}\)
6for\(j=1,,t\)do
7\(_{j}(n,p)\)
8if\(_{j}>2np\)then
9return\(\)
10 end if // The matrix \(A_{j(2np):j(2np)+_{j}}\) corresponds to \(_{j}\) in the analysis.
11\(acc 0\)for\(i=(j-1)(2np)+1,,(j-1)(2np)+_{j}\)do
12\(acc acc+ a_{i},_{} a_{i}\)
13 end for // Here\(acc=_{j}^{}_{j}_{}\)
14\(_{} acc\)\(_{}_{}/\|_{}\|_{2}\)
15
16 end for
17
18 end for return\(_{\{_{1},_{2},_{4},,_{d}\}}\|(  A)\|_{2}\) ```

**Algorithm 1**Approximate Eigenvector for Streams with no Large Norms

We now present the analysis of the block power method for random order streams assuming that the Euclidean norms of all the rows in \(A\) are close to each other. We later remove this assumption. Suppose there exists a parameter \(\) such that \((_{i}\|a_{i}\|_{2}^{2})/(_{i}\|a_{i}\|_{2}^{2})\). If \(\) is close to \(1\) then all the rows in the stream have roughly the same norm.

Let \(p=C(d)/^{2}n\). We can see that for any row \(a_{i}\) in the stream,

\[C\|_{2}^{2}}{^{2}\|A\|_{2}^{2}} d C^{2}/n}{^{2}\|A\|_{2}^{2}} d}=p.\]

[MISSING_PAGE_FAIL:6]

\(\|A\|_{F}^{2}/\|A\|_{2}^{2}\) and the rows in the stream are ordered uniformly at random, then we can compute a vector \(\) using the block power method that satisfies_

\[| v_{1},|^{2} 1-3\]

_with probability \( 4/5\) if \(_{1}(A)/_{2}(A) 2\). The algorithm uses \(O(d(d)/^{4})\) bits of space._

Proof.: Set \(=^{2}/C^{2}d\) for a large enough constant \(C\). Assuming \(n=(^{-4}^{6}d)\), we have \(n=(^{-2}^{2}d)\). Now consider the execution of Algorithm 1 on matrix \(A\), with parameters \(\) and \(\). Let \(=2^{j}\) be such that \((A)/2(A)\), and consider the execution in the algorithm with parameter \(\). Using Theorem 2.1, with probability \( 1-1/(d)\), the algorithm computes \(t\) matrices \(_{1},,_{t}\) such that for all \(j[t]\),

\[\|_{j}^{}_{j}-A^{}A\|_{2} \|A\|_{2}^{2}.\]

Noting that \(_{}=(_{t}^{}_{t})(_{1}^{}_{1})/\|(_{t}^{}_{t})(_{1}^{ }_{1})\|_{2}\), by Lemma 2.3, we have with probability \( 9/10\) that

\[_{},v_{1}^{2}t} 1-.\]

Thus, for \(\) which satisfies \((A)/2(A)\), the algorithm computes a vector \(_{}\) that has a large correlation with the vector \(v_{1}\). Since the algorithm does not know the exact value of \(\), it computes an approximation for \(\|A\|_{2}^{2}\) for all \(\{\,_{1},_{2},_{4},,_{d}\,\}\). First, we condition on the fact that with probability \( 1-1/(d)\), for all \(_{i}\), \(\|_{i}\|_{2}^{2}=(1)\|A_{i}\|_{2}^{2}\). Since \(_{},v_{1}^{2}(1-)\), we note that \(\|_{}\|_{2}^{2}(1-)(1-)_{1}( A)^{2}\). Now, for the vector \(\) returned by the algorithm, we have \(\|\|_{2}^{2}(1-O())(1-)_{1}(A)^{2}\) which implies that

\[,v_{1}^{2}_{1}(A)^{2}+(1-,v_{1} ^{2})(A)^{2}}{R}\|A\|_{2}^{2}(1--O ())_{1}(A)^{2}\]

and therefore \(,v_{1}^{2} 1-3\) since \(R 2\). 

### Random Order Streams without Norm Bounds

Assuming that the random order streams are long enough, Theorem 2.4 shows that if all the squared row norms are within an \(\) factor, then the block power method outputs a vector with a large correlation with the top eigenvector of the matrix \(A^{}A\). For general streams, the factor \(\) could be quite large and hence the algorithm requires very long streams to output an approximation to \(v_{1}\).

If there are no _heavy_ rows, i.e., rows with a Euclidean norm larger than \(\|A\|_{F}/(d)}\), then the row norm sampling procedure in Theorem 2.1 can be used to convert any randomly ordered stream of rows into a uniformly random stream of rows that all have the same norm. The row norm sampling procedure computes a probability \(p_{i}=(1,C^{-2}\|a_{i}\|_{2}^{2} d/\|A\|_{2}^{2})\) and samples the row \(a_{i}\) with probability \(p_{i}\). If sampled, then the row \(a_{i}\) is scaled by \(1/}\). From Theorem 2.1, we have that the top eigenvector of the _quadratic form_ of the sampled-and-rescaled submatrix is a good approximation to the top eigenvector \(A^{}A\) when the gap \(R\) is large enough. Suppose \(p_{i}<1\). If the row \(a_{i}\) is sampled, we then have

\[\|a_{i}/}\|_{2}=}{}.\]

Thus, if \(p_{i}<1\) for all \(i\), then all the sampled-and-rescaled rows have the same Euclidean norm and therefore, we can run the algorithm from Theorem 2.4 by setting \(=1\). Note that \(p_{i}=1\) only if \(\|a_{i}\|_{2}^{2}^{2}\|A\|_{2}^{2}/C(d)\). Since we assumed that there are no heavy rows, there is no row with \(p_{i}=1\) as long as \( 1/(d)\). Thus, using Theorem 2.4 on the row norm sampled substream directly gives us a good approximation to the top eigenvector. However, in general, the streams can have rows with large Euclidean norm. We will now state our theorem and describe how such streams can be handled.

**Theorem 2.5**.: _Let \(A\) be an \(n d\) matrix with its non-zero entries satisfying \(1/(d)|A_{i,j}|(d)\), and hence representable using \(O( d)\) bits of precision. Let \(R=_{1}(A)^{2}/_{2}(A)^{2}\). Assume \(2 R C_{1}^{2}d\). Let \(h\) be the number of rows in \(A\) with norm at most \(\|A\|_{}/(d)}\), where \((d)=^{C_{2}}d\) for a large enough universal constant \(C_{2}\). Given the rows of the matrix \(A\) in a uniformly random order, there is an algorithm using \(O((h+1) d(d) n)\) bits of space and which outputs a vector \(\) such that with probability \( 4/5\), \(\) satisfies \(,v_{1}^{2} 1-8/\), where \(v_{1}\) is the top eigenvector of the matrix \(A^{}A\)._

The key idea in proving this theorem is to partition the matrix \(A\) into \(A_{}\) and \(A_{}\), where \(A_{}\) denotes the matrix with the heavy rows and \(A_{}\) denotes the matrix with the rest of the rows of \(A\). Since we assume that there are at most \(h\) heavy rows, we can store the matrix \(A_{}\) using \(O(h d(d))\) bits of space. Now consider the following two cases: (i) \(\|A_{}\|_{2}(1-)\|A\|_{2}\) or (ii) \(\|A_{}\|_{2}<(1-)\|A\|_{2}\) for some parameter \(\). In the first case, we can show that the top eigenvector \(u\) of \(A_{}^{}A_{}\) is a good approximation for \(v_{1}\). Since, we store the full matrix \(A_{}\), we can compute \(u\) exactly at the end of the stream. Suppose \(\|A_{}\|_{2}<(1-)\|A\|_{2}\). By the triangle inequality, we have \(\|A_{}\|_{2}>\|A\|_{2}\). If we set \(\) large enough compared to \(1/R\), then we can show that the top eigenvector \(u^{}\) of \(A_{}^{}A_{}\) is a good approximation of \(v_{1}\). From the above discussion, since all the rows of \(A_{}\) are _light_, we can obtain a stream using Theorem 2.1 such that all the rows have the same norm and additionally, the top eigenvector of this stream is a good approximation for \(u^{}\) and therefore \(v_{1}\). We then approximate the top eigenvector of the new stream using Theorem 2.4. Setting \(\) appropriately, we show that this procedure can be used to compute a vector \(\) satisfying \(,v_{1}^{2} 1-O(1/)\) proving the theorem.

## 3 Lower Bounds

Our algorithm uses \((h d)\) space when the number of heavy rows in the stream is \(h\). We want to argue that it is nearly tight. We show the following theorem.

**Theorem 3.1**.: _Given a dimension \(d\), let \(h\) and \(R\) be arbitrary with \(R h d\) and \(R^{2} h=O(d)\). Consider an algorithm \(\) with the following property: Given any fixed matrix \(n d\) matrix \(A\) with \(O(h)\) heavy rows and gap \(_{1}(A)^{2}/_{2}(A)^{2} R\), in the form of a uniform random order stream, the algorithm \(\) outputs a unit vector \(\) such that, with probability \( 1-(1/2)^{4R+4}\) over the randomness of the stream and the internal randomness of the algorithm, \(|,v_{1}|^{2} 1-c/R^{2}\). If \(c\) is a small enough constant, then the algorithm \(\) must use \((h d/R)\) bits of space._

The theorem shows that a streaming algorithm must use \((hd/R)\) bits of space assuming that with high probability, it outputs a vector with a large enough correlation with the top eigenvector of \(A^{}A\) when the rows are given in a random order stream.

Our proof uses the same lower bound instance as that of Price and Xun (2024). The key difference from their proof is that our lower bound must hold against random order streams.

## 4 Improving the Gap Requirements in the Algorithm of Price and Xun

### Arbitrary Order Streams

As discussed in Section 2.1, we can guess an approximation of \(\|A\|_{2}^{2}\) in powers of \(2\) and sample at most \(O(d d/^{2})\) rows in the stream to obtain a matrix \(\), in the form of a stream, satisfying \(\|^{}-A^{}A\|_{2}\|A\|_{2}^{2}\), with a large probability. Using Weyl's inequalities, we obtain that

\[_{2}(^{})_{2}(A^{}A)+ \|A\|_{2}^{2}_{1}(^{} {B})(1-)_{1}(A^{}A)\]

implying \(R^{}=_{1}()^{2}/_{2}()^{2}(1-)/(1 /R+)\). For \(=1/(2R) 1/2\), we note \(R^{} R/3\). Let \(n^{}=O(R^{2} d d)\) be the number of rows in the matrix \(\) and note that \(R^{}=( n^{} d)\) assuming \(R=(^{2}d)\). Hence, running the algorithm of Price and Xun on the rows of the matrix\(\), we compute a vector \(\) for which

\[|,v_{1}^{}|^{2} 1-}-(d)}\]

with a large probability, where \(v_{1}^{}\) is the top eigenvector of the matrix \(^{}\). We now note that if \(v_{1}\) denotes the top eigenvector of the matrix \(A^{}A\), then \(| v_{1},v_{1}^{}|^{2} 1-O(1/R)\) which therefore implies that with a large probability,

\[|,v_{1}|^{2} 1-.\]

Thus, sub-sampling the stream using row norm sampling and then running the algorithm of Price and Xun (2024), we obtain an algorithm for arbitrary order streams with a gap \(R=(^{2}d)\).

### Random Order Streams

Lemma 3.5 in Price and Xun (2024) can be tightened when the rows of the stream are uniformly randomly ordered. Specifically, we want to bound the following quantity:

\[_{i=1}^{n} a_{i},P_{i-1}^{2}\]

where \(P=I-v_{1}v_{1}^{}\) denotes the projection away from the top eigenvector, and \(_{i-1}\) is a function of \(v_{1},a_{1},,a_{i-1}\). We have

\[[ a_{i},P_{i-1}^{2}]=[[  a_{i},P_{i-1}^{2} a_{1},,a_{i-1}]].\]

Given that the first \(i-1\) rows are \(a_{1},,a_{i-1}\), assuming uniform random order, we have

\[[ a_{i},P_{i-1}^{2} a_{1}, ,a_{i-1}] =_{i-1}^{}P(A^{}A-a_{1 }a_{1}^{}--a_{i-1}a_{i-1}^{})P_{i-1}\] \[(A)^{2}}{n-i+1}.\]

Hence \([ a_{i},P_{i-1}^{2}]_{2}(A)^{2}/(n- i+1)\) and \([_{i=1}^{n} a_{i},P_{i-1}^{2}]_{2} (A)^{2}(1+ n)\). Price and Xun define \(_{2}(A)^{2}\) as \(_{2}\) and in that notation, we obtain \(_{i=1}^{n} a_{i},P_{i-1}^{2} 10_{2}(1+  n)\) with probability \( 9/10\) by Markov's inequality. In the proof of Lemma 3.6 in Price and Xun (2024), if \(_{1}/_{2} 20(1+_{2}n)\), we obtain \(\|v_{n}\|_{2}_{1}\). Now, \(_{1} O( d)\) ensures that the Proof of Theorem 1.1 in their work goes through.

Using the row-norm sampling analysis from the previous section, we can assume \(n=(d)\) and therefore a gap of \(O( d)\) between the top two eigenvalues of \(A^{}A\) is enough for Oja's algorithm to output a vector with a large correlation with the top eigenvector in random order streams.

## 5 Hard Instance for Oja's Algorithm

At a high level, the algorithm of Price and Xun (2024) runs Oja's algorithm with different learning rates \(\) and in the event that the norm of the output vector with each of the learning rates \(\) is small, then the row with the largest norm is output. The algorithm is simple and can be implemented using an overall space of \(O(d(d))\) bits.

The algorithm initializes \(z_{0}=\) where \(\) is a random Gaussian vector. The algorithm streams through the rows \(a_{1},,a_{n}\) and performs the following operation

\[z_{i} z_{i-1}+ z_{i-1},a_{i} a_{i}.\]

The algorithm computes the smallest learning rate \(\) when \(\|z_{n}\|_{2}\) is large enough, and then outputs either \(z_{n}/\|z_{n}\|_{2}\) or \(/\|\|_{2}\) as an approximation to the eigenvector of the matrix \(A^{}A\). Here \(\) denotes the row in \(A\) with the largest Euclidean norm.

The following theorem shows that at gaps \( O( d/ d)\), we cannot use Oja's algorithm with a fixed learning rate \(\) to obtain constant correlation with the top eigenvector.

**Theorem 5.1**.: _Given dimension \(d\), a constant \(c>0\), a parameter \(M\), for all gap parameters \(R=O_{c}( d/ d)\) there is a stream of vectors \(a_{1},,a_{n}^{d}\) with \(n=O(R+M)\) such that:_

1. \(_{1}(A)^{2}/_{2}(A)^{2} R/2\)_, and_
2. _Oja's algorithm with any learning rate_ \(<M\) _fails to output a unit vector_ \(\) _that satisfies, with probability_ \( 9/10\)_,_ \[|,v_{1}| c\] _where_ \(v_{1}\) _is the top eigenvector of the matrix_ \(A^{}A\)_._

_Moreover, the result holds irrespective of the order in which the vectors \(a_{1},,a_{n}\) are presented to the Oja's algorithm. We will additionally show that even keeping track of the largest norm vector is insufficient to output a vector that has a large correlation with \(v_{1}\)._