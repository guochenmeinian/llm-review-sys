# Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models

Yubin Shi\({}^{1}\)   Yixuan Chen\({}^{1}\)   Mingzhi Dong\({}^{1,*}\)   Xiaochen Yang\({}^{2,*}\)

**Dongsheng Li\({}^{3}\)   Yujiang Wang\({}^{4}\)   Robert Dick\({}^{5}\)   Qin Lv\({}^{6}\)   Yingying Zhao\({}^{1}\)   Fan Yang\({}^{7}\)   Tun Lu\({}^{1}\)   Ning Gu\({}^{1}\)   Li Shang\({}^{1,*}\)**

\({}^{1}\)China and Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

\({}^{2}\)School of Mathematics Statistics, University of Glasgow

\({}^{3}\)Microsoft Research Asia, Shanghai, China

\({}^{4}\)Department of Engineering Science, University of Oxford

\({}^{5}\)Department of Electrical Engineering and Computer Science, University of Michigan

\({}^{6}\)Department of Computer Science, University of Colorado Boulder

\({}^{7}\)School of Microelectronics, Fudan University

Corresponding authors.

###### Abstract

Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue \(_{}\). A large \(_{}\) indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their \(_{}\) exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those superfluous ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.

## 1 Introduction

The proliferation of large-scale pre-trained models (Devlin et al., 2018; Brown et al., 2020; Dosovitskiy et al., 2020) demonstrates the advantages of over-parameterized models. Those models can adequately fit the training data with their sizeable parameters far exceeding the data scale (Du et al., 2018), and such over-parameterizations are known to be essential in model optimization and generalization and are also crucial to their success in deep learning (Liu et al., 2022). However, challenges remain. Over-parameterized models are expensive to train; for example, training large language models (LLMs) such as GPT-3 (Brown et al., 2020; Zaheer et al., 2020) may take weeks to months, even on a large collection of powerful GPUs.

Generally, most over-parameterized models consist of various functioning modules connected layer-wisely, e.g., heads in self-attention models like Transformer (Vaswani et al., 2017), experts in mixture-of-experts (MoE) blocks (Shazeer et al., 2017), or filters in convolutional neural networks (CNN). This module-based architecture inevitably leads to a question: _Has the computational resource been spent efficiently and effectively across modules during model optimization?_ To this end, we dive into the modular-level training behaviors of over-parameterized models and pursue a more efficient and fruitful training strategy. We propose modular Neural Tangent Kernel (mNTK), which is derived from the vanilla NTK (Jacot et al., 2018; Fort et al., 2020), as a powerful tool for understanding the fine-grained learning dynamics of each network module. We compute mNTK as the first-order approximation of a network module's evolution during training, and it reflects the gradient correlation among the training data module-wisely. The eigenspectrum of mNTK describes the learning dynamics of a module, and its principal eigenvalue \(_{}\) indicates the degree of consistency in the gradient direction for learning the most common features in data.

To illustrate the module-level training dynamics intuitively, we employ the standard BERT (Devlin et al., 2018) as an example and consider each attention head as a specific module. In addition to \(_{}\), we adopt the effective rank (Roy and Vetterli, 2007) to measure the effective dimensionality of a feature matrix, like an attention matrix, and those metrics are computed at the end of each training epoch. Figure 1(a) presents the trend of training dynamics of BERT, and we can observe asynchronous learning behaviors across modules and training iterations. We explain those observations using \(_{}\) as an indicator of trainability to measure how effectively gradient descent can optimize network parameters. As discussed in Bowman and Montufar (2022), the network is biased to learn the top eigenfunctions of the NTK over the entire input space, while the parameters with very low eigenvalues barely update. In Figure 1(a), the attention head in blue marker has large \(_{}\), meaning that it has large consistency over data samples on feature learning and is thus apt to converge with better trainability. Indeed, these heads contain richer information as reflected through effective rank. In contrast, \(_{}\) of the red attention head oscillates at a low level, likely due to learning diverse features or even noise that may be hard to converge, and thus updating it brings little benefit to training loss.

Our analysis (Section 2) on trainability and generalization demonstrates that over-parameterized models exhibit modularly and temporally varying asynchronous learning behaviors. _Modules with large mNTK principal eigenvalues learn features that are more rapidly learned, and those miniature ones with limited impact on training loss negatively influence generalization. Such asynchronous modularly and temporally training dynamics result in inefficient resource utilization by existing training schemes._ Motivated by these analyses, we propose a simple yet effective training method termed Modular Adaptive Training (MAT), aiming at dynamically training partial modules to improve training efficiency. As shown in Figure 1(b), we design a dynamic threshold (the dash-dotted line), according to modular difference and temporal variation of mNTK \(_{}\). MAT only back-propagates the parameters of those modules whose \(_{}\) is larger than the threshold. This forces the network to

Figure 1: Characteristics of training BERT on WikiText-2. Figure (a) demonstrates the joint variation of effective rank and \(_{}\) across the attention heads. Head\({}^{l}_{i}\) refers to the \(i^{}\) attention head in the \(l^{}\) layer. Figure (b-left) illustrates the idea of MAT, which governs the heads training by a dynamic threshold. Using MAT speeds up convergence and achieves lower validation loss (b-right).

concentrate on learning the common features and ignore the inconsistent ones, preventing it from fitting superfluous features or noises. This work makes the following contributions.

* We empirically and theoretically reveal the associations between modular-level training dynamics and the proposed modular Neural Tangent Kernel (mNTK).
* We propose Modular Adaptive Training (MAT) with a dynamic threshold to selectively update modules during back-propagation to improve learning efficiency.
* Experimental results verify that MAT can significantly reduce the training computation, improve performance, and generalize well to different over-parameterized models.

## 2 Analysis of Modular Neural Tangent Kernel in Over-Parameterized Models

In this section, we first introduce the definition of Neural Tangent Kernel (NTK) and modular Neural Tangent Kernel (mNTK). Next, we present empirical analysis revealing the eigenspectrum characteristics of the mNTK, including the imbalanced distribution of mNTK eigenvalues and the modular and temporal diversity of mNTK eigenvalues during training. Finally, we provide theoretical analysis to show that trainability of over-parameterized structured models is closely related to the largest mNTK eigenvalues; on the other hand, learning features associated with the miniature eigenvalues have negative impact on generalization.

### Modular Neural Tangent Kernel (mNTK)

Let \(\) denote the set of \(n\) training instance, which are i.i.d. drawn from an unknown distribution \(\), and let \(^{nk}\) denotes the targets. We study the network function \(f\) parameterized by \(\) aiming to map the input vector \(\) to output vector \(\), termed as \(=f(;)\) where \(^{m}\) and \(^{nk}\). Following the original definition of Neural Tangent Kernel (NTK) (Jacot et al., 2018; Wang et al., 2020), we introduce modular NTK for fine-grained analysis of structured deep networks as below.

**Definition 1**: _(Modular Neural Tangent Kernel (mNTK)). Suppose model \(f\) contains \(L\) disjoint modules \(=\{^{1},^{2},,^{L}\}\) where \(^{l}\) denote the parameters of \(l^{}\) module. We define mNTK as a matrix by \(^{l}(,)=J_{^{l}}()J_{ ^{l}}()^{}\), where \(J_{^{l}}=_{^{l}}f(;^{l})\) denotes the Jacobian of the function \(f\) at the points \(\) with respect to the \(l^{}\) module's parameters \(^{l}\)._

\(^{l}\) is a positive semi-definite real symmetric matrix and can be eigen-decomposed as \(^{l}=^{l}^{l}^{l}{^{l}}^ {}=_{i=1}^{nk}_{i}^{l}_{i}^{l}_{i}^{}\) with \(nk\) non-negative eigenvalues \((^{l})=\{_{1}^{l},_{2}^{l},...,_{ nk}^{l}\}\). Note that a network can be partitioned into modules in flexible ways, for example, by dividing layers within a deep network or components of a network with inherent structures (e.g., attention heads in Transformer or convolutional filters in a CNN). Since each module has its own mNTK, the integral NTK of a model can be computed as the sum of mNTKs of each module (Yang and Littwin, 2021):

\[(,)_{p=1}^{m}J_{_{p}}()J_{_{p}}()^{} {=}_{l=1}^{L}_{_{p}^{l}}J_{^{l}}( )J_{^{l}}()^{}_{l= 1}^{L}^{l}(,), \]

where \((i)\) decomposes the matrix multiplication into the sum of vector multiplication; \((ii)\) gathers addends by each module; \((iii)\) follows the definition of mNTK.

### Empirical Analysis

In this subsection, we apply BERT to WikiText-2 for language modeling and regard each layer as a module to analyze the properties of eigenvalue distribution of all mNTKs, as well as the modular and temporal variation of eigenvalues during the training process.

**The principal eigenvalue of mNTK dominates the eigenspectrum of mNTK.** Figure 2(a) shows the first 16 eigenvalues of all layer-wise mNTKs at the \(10^{}\) epoch. Apparently, the eigenvalue distribution of each layer-wise mNTK is imbalanced where there exists a single large eigenvalue \(_{1}^{l}\) (\(_{1}\) will be denoted by \(_{}\) hereafter) and a large number of small eigenvalues. In particular, the largest eigenvalue often has 2 to 4 orders of magnitude larger than that of the rest eigenvalues. These findings indicate that the principal eigenvalue of mNTK dominates the spectrum of the mNTK.

The phenomenon is also indicated by Xiao et al. (2020) in terms of NTK. Furthermore, according to the connection between NTK and mNTK (Eq. 1), the spectrum of NTK is dominated by the principal eigenvalues of all mNTKs. Below, we utilize \(_{}\) for fine-grained optimization analysis of over-parameterized model.

**The variation of principal eigenvalues of mNTKs is highly asynchronous across modules.** Figure 2(b) shows the variation of \(_{}\) of all layer-wise mNTKs during the training process. It is worth noting that the principal eigenvalues of mNTKs exhibit different trends during the training process. Intuitively speaking, NTK (also mNTK) captures the training dynamics throughout the training process, and its principal eigenvector corresponds to the direction of the steepest descent in loss. Specifically, for mNTKs of shallow layers, the magnitude of \(_{}\) is large after model initialization, and it decreases and converges fast in the early training stages. A potential explanation is that shallow layers learn simple and common features of data samples (Yosinski et al., 2014; Zhang et al., 2019), which contribute significantly to model convergence in the early training stages. In contrast, for mNTK of the deep layer, the magnitude of \(_{}\) is small after initialization and gradually increases during training. The reason is that deep layers learn complex and unique features, relying on the stability of simple features in shallow layers, which occurs in the later training stages.

To analyze the training dynamics, condition number is also commonly used (Xu et al., 2021; Xiao et al., 2020), defined as \(=_{}/_{}\). Figure 2(c) shows the variation of \(\) for different modules during training. We observe that the layer-wise condition number exhibits asynchronization among different modules only in the early training stages (especially the first 10 epochs) and \(\) of all modules quickly converges to the same value. This reveals that the condition number is hard to provide sustained information for fine-grained optimization analysis, unlike the principal eigenvalue which is capable of capturing the effective information throughout the training process.

**The temporal variation of the principal eigenvalue of mNTK indicates the generalization ability.** In addition to the above analysis depicting the asynchronization between different modules, we examine the temporal variation of the principal eigenvalue given a specific module and establish its connection with the generalization ability. In detail, we train the same model using masked modeling on half sequence length, which makes the model prone to overfitting. As shown in Figure 3(a), the validation loss starts to increase from the \(45^{}\) epoch while the training loss continually decreases. Figure 3(b-e) present the temporal variation of the principal eigenvalue for the \(3^{}\), \(6^{}\), \(9^{}\), and \(12^{}\) layers of the model during the training process, respectively. During the first \(45^{}\) epochs, i.e., the non-overfitting stage, the principal eigenvalues of these layers continually decrease. However, after the \(45^{}\) epoch when the model enters the overfitting stage, the principal eigenvalues converge to excessively low values. A possible reason for overfitting is that the optimization direction of the model starts to align with the direction of small eigenvalues, which means that the model is learning superfluous features that are too specific or even noisy.

### Analysis of Trainability and Generalization

Using the tool of mNTK, we show that the optimization properties of over-parameterized structured models are closely related to eigenvalues of mNTKs.

Suppose for any module \(^{l}\), \(_{}(^{l})=_{nk}^{l}>0\). Let \(\) denote the loss function and \(_{}()\) the gradient w.r.t parameter \(\). For a step of gradient descent, loss reduction can be characterized by the

Figure 2: Training dynamics characterized by layer-wise mNTK of BERT trained on WikiText-2. (a): The first 16 eigenvalues distribution of layer-wise mNTKs at the \(10^{}\) epoch. (b): Variation of \(_{}\) of layer-wise mNTKs. (c): Variation of \(\) of layer-wise mNTKs during the first \(20\) epochs.

directional derivative of the loss function (Wang et al., 2020):

\[ }{{=}} _{ 0}(+ _{}())- ()}{}}{{ }}_{}()^{} _{}()\] \[}{{=}} _{}()^{}(_{ }f()^{}_{}f ())_{}() \] \[}{{=}} _{}()^{}(_{l=1}^ {L}^{l})_{}()}{{=}}_{l=1}^{L}_{i=1}^{nk}_{i}^ {l}({_{i}^{l}})^{2},\]

where \((i)\) follows the definition of directional derivative; \((ii)\) follows the first-order Taylor expansion; \((iii)\) follows chain rule of derivative; \((iv)\) follows Eq. 1; and \((v)\) follows the eigen-decomposition of mNTK under the assumption of squared error loss (Arora et al., 2019). Following the work of Arora et al. (2019), we assume that true labels align well with top eigenvectors, i.e., \(({_{i}^{l}})^{2}\) is large for large \(_{i}^{l}\). Thus, the directional derivative of the loss function can be regarded as closely related to the eigenspectrum of mNTKs.

(1) The relation between **trainability** and principal eigenvalues of mNTKs.

Based on the first observation of empirical analysis, i.e., the spectrum of each mNTK is dominated by the principal eigenvalues, we utilize \(_{}\) of mNTKs as the nearly equivalent proxy of the spectrum of mNTKs. Therefore, Eq. 2 is simplified as:

\[_{l=1}^{L}_{}^{l}({_{1}^ {l}})^{2}. \]

The above equation suggests that the loss decreases faster along the eigenspaces that correspond to larger eigenvalues. Given the fact that the principal eigenvalues of mNTKs are highly asynchronous across modules, we suggest selectively training the modules that are active with larger \(_{}^{l}\) to achieve efficient learning with limited computational resources.

(2) The relation between **generalization ability** and eigenvalues of mNTKs.

Figure 4 shows the eigen-spectrum distribution of a 4-layer BERT model at the \(10^{}\) epoch, by regarding each head as a module and calculating 256 eigenvalues for each head. Among all 4\(\)12\(\)256 eigenvalues, the distribution of eigen-spectrum exhibits two distinct regions: the left region with a large number of small eigenvalues and the right region with a small number of large eigenvalues. The two regions are widely separated. According to Eq. 3, the eigenspaces corresponding to miniature eigenvalues in the left region have negligible impact on training loss minimization, while those in the right region make dominant contributions. Following the definition of Oymak et al. (2019), we refer to the right region as the _information space_ where the training error decreases rapidly, and the left region as the _nuisance space_ where training is slow and may cause overfitting.

Figure 4: The normalized eigen-spectrum distribution exhibits two distinct regions, termed information space and nuisance space.

Figure 3: Training dynamics in the overfitting Case of 4-layer BERT trained by 64 token MLM task.

Model training with more parameter updates tends to have longer distance to the initializations \(_{0}\)(Hoffer et al., 2017). Based on Lemma 5.4 in Arora et al. (2019), longer distance results in relatively larger Rademacher complexity, as \(\|-_{0}\|_{F}\), and larger \(\) results in relatively worse generalization. As a consequence, we suggest selectively stopping the training in some modules so as to stop the increase of the distance to the initializations in these directions. Specifically, we propose to stop training the modules with principal eigenvalues falling into the nuisance space so as to enjoy better generalization and efficient learning simultaneously.

Let's consider the inter-module training dynamics on model generalization. During the training process, given modules with variant principal eigenvalues, it often occurs that even though a small number of modules still lie in the information space, the most of others already fall into the nuisance space. The modules located in the information space, i.e., those with principal eigenvalues exceeding the threshold, are trained to learn informative features that facilitate loss minimization. Conversely, the modules falling into the nuisance space, i.e., those with principal eigenvalues below the threshold, are prone to learn superfluous or even noise features, which significantly increase the Rademacher complexity, and deteriorate their generalization ability. _This analysis indicates that the existing training scheme overlooks the asynchronization among modules, resulting in inefficient utilization of the computational resource for loss minimization during model training._

## 3 Modular Adaptive Training

Based on the analysis in the previous section, we advocate dynamically optimizing partial module parameters in back propagation, termed as Modular Adaptive Training (MAT). The forward propagation is computed as usual; during back propagation, MAT quantifies module parameters in terms of **modular policy** and **temporal policy** such that only subsets of modules are updated. Below, we present the proposed modular policy and temporal policy. Algorithm 1 summarizes the proposed MAT algorithm.

**Modular Policy.** Following the analysis that trainability and generalization of an over-parameterized model are related to the principal eigenvalues of module parameters, we propose to only update partial module parameters with \(_{}\) greater than a certain threshold during the model training; this policy is termed as modular policy.

Specifically, suppose the model parameters consist of \(L\) module parameters and each mNTK has \(nk\) positive eigenvalues, and let \(_{i}^{l}\) denote \(i^{}\) eigenvalue of the mNTK computed over the parameters \(^{l}\). We define \(}=\{_{i}^{l}\}_{i,l=1}^{nk,L}\) as the set of eigenvalues calculated over all mNTKs, and \(_{},_{}\) as the maximum and minimum value of the set \(}\), respectively. To select the appropriate modules, we introduce a hyperparameter ratio \((0,1)\), thus the corresponding eigenvalue threshold \(_{}\) is computed as:

\[_{}=_{}+(_{}- _{}). \]

According to the eigenvalue threshold \(_{}\), all modules are split into _information modules_ and _nuisance modules_ as:

\[_{t}=\{^{l}|_{}(_{t}^{l}) _{}\},_{t}=\{^{l}|_{}(_{t}^{l})<_{}\}. \]

As a result, the computation cost of back propagation is reduced.

We empirically observe that \((0,1)\) is relatively stable over the training process for a certain model and dataset. Therefore, \(\) can be empirically assigned at the early stage of training after warming up, or automatically learned by a meta network.

**Temporal Policy.** The previous empirical analysis shows that during training, after the eigenvalues of a module's mNTK enter the nuisance region, further training this module can lead to poor generalization. Therefore, to avoid overfitting, we terminate the training of this module according to the temporal variation of the principal eigenvalue; we call this the temporal policy.

For each module \(^{l}\), its principal eigenvalue after warming up is recorded as the initial value \(_{}(_{0}^{l})\). During each training episode \(t\), the temporal variation of principal eigenvalue is defined as \(_{t}^{l}=|_{}(_{t}^{l})-_{}( _{0}^{l})|\). The information modules will be early stopped if the difference in the temporal variation between two subsequent episodes relative to the initial variation is smaller than a pre-defined threshold \(\):

\[^{l}-_{t-1}^{l}|}{_{1}^{l}}<. \]

\(\) is empirically bounded by eigenvalue variation within the nuisance region.

## 4 Related Work

**Optimization Analysis using NTK.** Neural Tangent Kernel (NTK) (Jacot et al., 2018), which calculates the Gram matrix of Jacobian, is known as a powerful tool to analyze convergence and generalization properties (Arora et al., 2019). Modern neural networks are generally over-parameterized with positive definite NTKs, meaning that theoretically the gradient flow always converges to the equilibrium where the training loss is zero (Chizat and Bach, 2018; Bombari et al., 2022). A line of works study the training dynamics of over-parameterized models (Li et al., 2020; Liu et al., 2022) based on NTK, mainly under the assumption of two-layer infinite-width networks. Many papers (Xiao et al., 2020) study the spectrum of the NTK, and find in particular the largest eigenvalue dominates the training regime (Jacot et al., 2018; Bowman and Montufar, 2022). Empirical NTK, developed by many practical tools (Novak et al., 2022; Engel et al., 2022), succeeds in deep learning applications such as neural architecture search (Xu et al., 2021; Chen et al., 2021) and network pruning (Chen et al., 2023; Wang et al., 2023). Our work takes a deeper look into the modules of an over-parameterized model, demonstrating module-level training asynchronization of the components by the spatio-temporal distribution of the modular NTK. To the best of our knowledge, this is the first work that proposes to analyze the modular NTK variation for optimization.

**Adaptive Training Approaches.** Multirate training (Vlaar and Leimkuhler, 2022) is an emerging related technique that partitions neural network parameters into two parts, in which parameters in the slow part are updated less frequently. Our work analyzes the fine-grained training dynamics of modules and proposes a theoretical-inspired method to dynamically update parameters. Besides, there is a wide range of methods that can save computational resources, such as network pruning (Lee et al., 2018; Rachwan et al., 2022) and dynamic sparse training (Liu et al., 2020; Jiang et al., 2022). These works tend to disable parameters during both the forward and backward propagation processes, while our work only sparsifies the gradients during the backward propagation process. Hence, these techniques are orthogonal to our method, and the proposed mNTK can be employed for evaluating the importance of parameters in pruning-based methods. We also integrate our method with pruning, and the results are presented in Appendix.

## 5 Experiments

This section presents experimental results on various model architectures. The models under consideration are both over-parameterized and highly structured, including BERT (Devlin et al., 2018)with multi-head self-attention (MHSA), Switch-Transformer (Fedus et al., 2022) with both MHSA and mixture-of-experts (MoE) (Shazeer et al., 2017b) and VGG (Simonyan and Zisserman, 2014) with convolutional filters.

**Evaluation Measure.** We evaluate the effectiveness of our algorithm for model optimization in terms of _training efficiency_ and _generalization_. The training efficiency is measured by the validation or test error of the models after a certain number of floating-point operations (FLOPs), which serves as a lower bound for the execution time (Justus et al., 2018). The generalization performance is measured by the converged test error. Additionally, we calculate the total number of FLOPs used by the models until they achieve the converged test error.

**Approximation of NTK.** It is hard to directly compute the empirical NTK for an over-parameterized model, due to the computational and memory cost when calculating the Jacobian matrix \(J^{n m}\). Therefore, two alternatives are widely used to approximate the empirical NTK: (1) sampling a subset of training data instead of using the entire set; (2) computing the gradient by sum-of-logits instead of all output units, as shown in (Mohamadi and Sutherland, 2022).

**Implementation Details.** In practice, we evaluate the empirical NTK by sampling N training data (N is dynamically adjusted based on the available GPU memory in the experimental setup). To expedite the NTK calculation, we leverage parallelism by utilizing the implementation of Engel et al. (2022)2, which enables us to collect different data samples across multiple GPUs. The FLOPs is computed as the total sum of the forward and backward passes, where the backward FLOPs is approximately estimated as twice the forward pass (Rasley et al., 2020), and we measure it using the DeepSpeed Profiler3. All experiments are conducted on \(8\) NVIDIA GeForce RTX 3090 GPUs. For further experimental details, please refer to the Appendix.

### Main Result

**Results of BERT.** Firstly, we experiment on BERT (Devlin et al., 2018), which stacks 12 Transformer layers with 12 attention heads in each layer. Following the basic setup of Liu et al. (2019), we train BERT from scratch by masked language modeling (MLM) task on WikiText-2 (Merity et al., 2016). We compare the proposed MAT method with directly training a BERT model (BERT), using multiple learning rates (Vlaar and Leimkuhler, 2022) to train BERT (Multirate), and randomly selecting some heads for training (BERT-Rand). In this experiment, MAT applies \(=0.1,=10^{-3}\).

Table 1 shows the performance of all methods used in training BERT on WikiText-2. MAT outperforms all the baselines in training efficiency. In particular, MAT achieves the almost identical validation loss of 4.46 using 10 PFLOPs (1 PFLOPs = \(10^{15}\) FLOPs) of computation whereas the vanilla BERT achieves 4.48 using 20 PFLOPs. Our method saves half of the computation by performing a lighter back propagation pass only for appropriate subsets of the modules. Compared to MAT, the performance degradation of BERT-Rand confirms the effectiveness of using \(_{}\) as the criterion for selecting important modules. While Multirate shows the potential for efficient training, it comes at the cost of sacrificing performance. In contrast, MAT achieves better test performance with a reduction of about 15% in test perplexity, and also saves 41% of computational resources.

**Results of Switch-Transformer.** To further evaluate the scalability of our method, we apply MAT to the feed-forward network (FFN) of transformer layer. Switch-Transformer (Fedus et al., 2021) is a representative implementation of Mixture-of-Expert (MoE) architecture (Shazeer et al., 2017a),

    & &  &  &  \\ Method & &  & &  &  \\  & @ 10 PFLOPs & @ 15 PFLOPs & @ 20 PFLOPs & @ Convergence \\  BERT & 5.39 \(\) 0.15 & 4.75 \(\) 0.06 & 4.48 \(\) 0.03 & 4.41 \(\) 0.05 & 28.70 \\ BERT-Rand & 5.65 \(\) 0.18 & 5.11 \(\) 0.16 & 4.96 \(\) 0.11 & 4.82 \(\) 0.06 & 21.53 \\ Multirate & 4.93 \(\) 0.10 & 4.57 \(\) 0.03 & 4.52 \(\) 0.02 & 4.48 \(\) 0.03 & 19.46 \\  MAT & **4.46 \(\) 0.04** & **4.41 \(\) 0.02** & **4.35 \(\) 0.02** & **4.27 \(\) 0.03** & **16.50** \\   

Table 1: Results of BERT on WikiText-2. FLOPs is measured per GPU without embedding. Computation refers to the FLOPs model used until achieving best test loss. Best results are in boldface.

which has shown excellent performance in NLP tasks recently. MoE (Shazeer et al., 2017b) architects the FFN into a structured one, which replicates the FFN as experts and dynamically assigns tokens to each expert. In this experiment, we compare the performance of Switch-Transformers using vanilla, Multirate and Switch-Rand training methods on WikiText-103 (Merity et al., 2016). Specifically, MAT regards both experts and attention heads as the modules with \(_{h}=0.1,_{e}=0.2,_{h}=_{e}=10^{-3}\).

Table 2 shows that the performance of Switch-Transformer trained on a large dataset with a large number of parameters. It demonstrates that MAT significantly reduces the computation required for training. Switch-Rand and Multirate also improve training efficiency to some extent. Compared to them, our method precisely identifies the appropriate heads and experts trained with features that are easy to learn and generalize, reducing validation perplexity by 52.3%, 25.2%, and 18.1% (25.9%, 16.9% and 12.4% for log perplexity) within 200, 400, and 600 PFLOPs of computation, respectively. Compared to the vanilla model, our method achieves 10.4% improved test perplexity. These results demonstrate the effectiveness of MAT in larger models and datasets.

**Results of VGG.** To further validate the versatility of MAT, we deploy it on computer vision tasks. We take classic convolutional network VGG16 as an example, which is over-parameterized for the CIFAR-10 dataset. MAT splits convolutional filters into two parts of modules by \(=0.2,=10^{-6}\). Experimental results listed in Table 3 show that our method helps VGG16 converge faster (47.3%) with better test accuracy compared with the vanilla model.

### Module-level Training Analysis

This experiment analyzes the histogram of the number of training epochs of all the attention heads in BERT. As shown in Figure 5, more than half of the heads were trained with backpropagation in only 20% of the epochs, and the average training epoch per head was 34% of the entire training process. This verifies that the sparse backpropagation induced by MAT enhances training efficiency. Furthermore, approximately 20% of the heads were never updated, indicating the sparse activation of over-parameterized structured models. This finding suggests the possibility of further reducing the computation of stable modules during the forward pass.

    &  &  & Computation \\  &  &  &  \\  & @ 200 PFLOPs & @ 400 PFLOPs & @ 600 PFLOPs & @ Convergence & \\  Switch & 3.16 \(\) 0.12 & 2.31 \(\) 0.05 & 1.93 \(\) 0.03 & 1.74 \(\) 0.02 & 1155.2 \\ Switch-Rand & 2.92 \(\) 0.15 & 2.15 \(\) 0.08 & 2.01 \(\) 0.07 & 1.93 \(\) 0.05 & 837.5 \\ Multirate & 2.67 \(\) 0.09 & 2.02 \(\) 0.08 & 1.83 \(\) 0.05 & 1.77 \(\) 0.04 & 740.4 \\  MAT & **2.34 \(\) 0.06** & **1.92 \(\) 0.03** & **1.69 \(\) 0.02** & **1.68 \(\) 0.02** & **614.8** \\   

Table 2: Results of Switch-Transformer on WikiText-103. FLOPs is measured per GPU without embedding. Best results are in boldface.

Figure 5: Histogram of epochs where the heads are trained using back-propagation.

    &  &  & Computation \\  &  &  &  \\  & @ n = 95\% & @ n = 99\% & @ Convergence & \\  VGG16 & 8.85 & 12.06 & 93.77 \(\) 0.08 & 17.14 \\ VGG16-Rand & 9.96 & 13.35 & 92.71 \(\) 0.08 & 18.84 \\ Multirate & 7.19 & 9.75 & 93.43 \(\) 0.14 & 13.35 \\  MAT & **5.31** & **7.21** & **93.86 \(\) 0.05** & **9.03** \\   

Table 3: Results of VGG16 on CIFAR-10. Best results are in boldface.

Conclusion

We have analyzed the modular-level and temporal training characteristic of structured over-parameterized models through a new measure of modular neural tangent kernel (mNTK). Empirical and theoretical analysis demonstrates the relationship between optimization effectiveness and mNTK principal eigenvalues. Based on this finding, we designed a novel training strategy termed Modular Adaptive Training (MAT), which uses a modularly and temporally adaptive dynamic threshold to select partial modules for gradient back propagation. Experimental results show that MAT reduces training cost and increases test accuracy compared to existing training scheme. This work seeks to improve training efficiency by sparsifying the gradient update. Besides pruning, this work can be combined with other techniques to further improve training efficiency. We leave it for future work.