# Error Analysis of Spherically Constrained Least Squares Reformulation in Solving the Stackelberg Prediction Game

Error Analysis of Spherically Constrained Least Squares Reformulation in Solving the Stackelberg Prediction Game

Xiyuan Li Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Lee_xiyuan@outlook.com, liuweiwei863@gmail.com

Corresponding author: Weiwei Liu (liuweiwei863@gmail.com).

###### Abstract

The Stackelberg prediction game (SPG) is a popular model for characterizing strategic interactions between a learner and an adversarial data provider. Although optimization problems in SPGs are often NP-hard, a notable special case involving the least squares loss (SPG-LS) has gained significant research attention recently [1; 2; 3]. The latest state-of-the-art method for solving the SPG-LS problem is the spherically constrained least squares reformulation (SCLS) method proposed in the work of . However, the paper  lacks theoretical analysis on the error of the SCLS method, which limits its large-scale applications. In this paper, we investigate the estimation error between the learner obtained by the SCLS method and the actual learner. Specifically, we reframe the estimation error of the SCLS method as a Primary Optimization (**PO**) problem and utilize the Convex Gaussian min-max theorem (CGMT) to transform the **PO** problem into an Auxiliary Optimization (**AO**) problem. Subsequently, we provide a theoretical error analysis for the SCLS method based on this simplified **AO** problem. This analysis not only strengthens the theoretical framework of the SCLS method but also confirms the reliability of the learner produced by it. We further conduct experiments to validate our theorems, and the results are in excellent agreement with our theoretical predictions.

## 1 Introduction

The Stackelberg prediction games (SPGs) play prominent roles in various applications of the machine learning field, such as intrusion detection , spam filtering , and malware detection [6; 7]. SPG characterizes the interactions between two players, a learner and a data provider (attacker), during the training process of various machine learning algorithms [8; 9; 10; 11]. Specifically, the learner first selects a learning model to fit the given data. The data provider, with full knowledge of the learner's model, then attacks the learner by modifying the data. The learner's goal is to minimize its loss function under the assumption that the training data has been optimally modified from the data provider's perspective. Therefore, the SPG model is often formulated as a bi-level optimization problem, which is generally NP-hard even in the simplest case with linear constraints and objectives [1; 12].

To overcome the NP-hard nature of SPGs, [1; 2; 3] focus on a commonly used subclass of SPGs, termed as the SPG-LS, whose loss functions for the learner and the data provider are least squares. Specifically, SPG-LS has access to a set of \(n\) sample tuples denoted by \(S=\{(_{i},y_{i},z_{i})\}_{i=1}^{n}\), where \(_{i}^{d}\) is input data with \(d\) features, \(y_{i}\) is the true output label of \(_{i}\), and \(z_{i}\) is the label that the data provider aims to achieve. The learner of SPG-LS aims to train a linear predictor \(^{d}\) to best estimate the true output label \(y_{i}\) of the fake data \(_{i}^{}\) by minimizing the least squares loss:

\[^{}=*{arg\,min}_{}_{i=1}^{n}\| ^{}_{i}^{}-y_{i}\|^{2}.\]

Meanwhile, the data provider of SPG-LS, with full knowledge of the learner's predictive model \(\), selects the following least squares attacking strategy (i.e., modifying the data \(}_{i}\)) to make the corresponding prediction \(^{}_{i}^{}\) close to the desired label \(z_{i}\):

\[_{i}^{}=*{arg\,min}_{}}\|^{} {}_{i}-z_{i}\|^{2}+\|_{i}-}_{i}\|^{2},\]

where \(>0\) is a regularizer to adjust the trade-off between the deviation from the original data \(_{i}\) and closeness to the target \(z_{i}\). Thus, the SPG-LS model can be expressed as the following bi-level optimization problem, as described in :

\[_{}\|^{}-\|^{2},\ ^{}= *{arg\,min}_{}}\|}-\|^{2}+ \|}-\|_{F}^{2},\] (1)

where \(=(_{1},_{2},,_{n})^{}^{n d}\) is the input sample matrix, \(=(y_{1},y_{2},,y_{n})^{}^{n}\) is the vector of true output labels, and \(=(z_{1},z_{2},,z_{n})^{}^{n}\) is the vector of labels that the attacker aims to achieve. Moreover, \(\|\|\) denotes the Euclidean norm (\(l_{2}\)) unless otherwise specified.

There have been several studies solving the SPG-LS (1) to determine the Stackelberg equilibrium point between the learner and the data provider. The initial step in reformulating SPG-LS (1) is taken by , who provides a single-level quadratic fractional program (QFP) that can be globally solved by a bisection algorithm. However, this QFP method is computationally prohibitive in practice due to the need to solve multiple semidefinite programs (SDPs). Later,  improves upon  by showing that the SPG-LS (1) can be globally solved by reducing a single SDP to a second-order cone program (SOCP). Despite being faster than the QFP method, the SOCP method is still not well-suited for large-scale SPG-LS (1) due to time-consuming spectral decomposition. Recently,  proposes a spherically constrained least squares reformulation (SCLS) method, addressing the above-mentioned issues with a novel nonlinear change of variables. Furthermore,  demonstrates that the SCLS method outperforms the SOCP method and is currently the state-of-the-art for solving SPG-LS (1), having won the ICML 2022 Outstanding Paper Award.

However, the lack of theoretical analysis on the error of the SCLS method limits its large-scale practical applications. In this paper, we investigate the estimation error between the learner (e.t.\(^{}\)) estimated by the SCLS method and the true learner (denoted as \(_{0}\)) to validate the reliability of \(^{}\). Specifically, we assume the samples \(S=\{(_{i},y_{i},z_{i})\}_{i=1}^{n}\) are generated by the following black box model:

\[^{}=*{arg\,min}_{}}\|}_{0}- \|^{2}+\|}-\|_{F}^{2},=^{} _{0}+,\] (2)

where \(_{0}^{d}\) represents the "true" weight parameter of the real learner, and \(=(_{1},_{2},,_{n})^{} ^{n}\) is the noise vector. Moreover, the entries of \(\) and \(\) are drawn i.i.d. from \((0,1)\); the entries of \(\) are drawn i.i.d. from \((0,^{2})\); and we assume \(_{n}(0,1)\). Given \(\), \(\), and \(\) generated by this model (2), we solve SPG-LS (1) by the SCLS method to obtain \(^{}\) that is used to estimate the target vector \(_{0}\). Our task is to measure the optimal estimation error of the SCLS method, represented by \(\|^{}-_{0}\|\).

We start by formulating an optimization problem regarding the estimation error (e.t. \(:=-_{0}\)) of the SCLS method. Subsequently, we convert this optimization problem into a Primary Optimization (**PO**) problem, employing the Fenchel-Moreau theorem . Following this, we utilize the Convex Gaussian min-max theorem (CGMT) to simplify the **PO** problem into an Auxiliary Optimization (**AO**) problem. Finally, we conduct a theoretical error analysis of the SCLS method based on the **AO** problem. Our main theoretical result can be summarized as follows:

\[_{n}\|^{}-_{0}\|0,\] (3)which guarantees the reliability of \(^{*}\) learned by the SCLS method. Our analysis strengthens the theoretical foundations of the SCLS method and provides theoretical support for its broad applications.

We also conduct experiments to validate our theorems. The results show that, as \(n\) goes to \(\), the parameter vector \(^{*}\) learned through the SCLS method converges to actual parameter vector \(_{0}\) in probability, which aligns excellently with our theoretical predictions.

### Outline

The structure of the remaining sections in this paper is organized as follows: Additional related work is discussed in Appendix 2. Section 3 provides an overview of the SCLS method, CGMT technology, and foundational concepts. Our main inference processes are detailed in Section 4. Specifically, in Section 4.1, we present an optimization problem concerning the estimation error of the approximated SCLS method and establish the relationship between the original and the approximated SCLS method; In Section 4.2, the estimation error of the approximated SCLS method is transformed into a **PO** problem; In Section 4.3, we simplify the **PO** problem to an **AO** problem using CGMT; In Section 4.4, we conduct an estimation error analysis of the SCLS method based on the **AO** problem and the relationship between the original and the approximated SCLS method. We then present the experimental results in Section 5. Finally, a summary is provided in Section 6. Moreover, the limitations of our work are detailed in Appendix A

## 2 Related Work

### The Stackelberg prediction game

Stackelberg Prediction Games (SPGs) were initially introduced by , drawing inspiration from Stackelberg competition--a model initially developed to describe market behaviors. A notable parallel can be drawn with Stackelberg Security Games (SSGs), as detailed by [4; 6]. In SSGs, a defender strategically allocates resources to protect targets from an attacker. The optimal defense strategy in SSGs typically involves solving multiple linear programs [15; 16], and  demonstrates that a near-optimal strategy can be efficiently approximated through a polynomial number of queries to the attacker's model. While both SPG and SSG frame the learning of an optimal strategy as a bilevel optimization problem, SPGs are distinctly designed to counteract manipulation within machine learning algorithms .

Recognized as NP-hard hierarchical mathematical challenges , SPGs have been extensively examined. However, existing research predominantly addresses scenarios where data providers exhibit partial adversarial behaviors or possess constrained adversarial capabilities. The study by  explores SPGs within the context of linear least squares regression (e.g., SPG-LS), assuming data providers are neither fully adversarial nor entirely honest. To address the SPG-LS problem,  initially formulated a solution approach, which was later enhanced by . Subsequently,  introduced the Spherical Constrained Least Squares (SCLS) method, currently acknowledged as the state-of-the-art. This paper aims to further elucidate the error dynamics of the SCLS method , thereby solidifying its theoretical foundation for broader practical application.

### The Gaussian Min-max Theorem

The Convex Gaussian Min-max Theorem (CGMT) framework, first introduced by , serves as a potent analytical tool extensively utilized to evaluate the performance of solutions within non-smooth regularized convex optimization problems. This framework is derived from Gordon's Gaussian Min-max Theorem (GMT) [20; 21], which provides foundational insights into the behavior of Gaussian processes in optimization scenarios. Over the years, the CGMT has enabled significant advancements across a spectrum of practical applications. Notable examples include enhancements in regularized logistic regression , max-margin classifiers  and adversarial training [24; 25]. Motivated by these successful applications, we are encouraged to employ the CGMT framework to conduct a thorough analysis of the estimation error associated with the SCLS method.

Preliminaries

### The SCLS method

This section provides a comprehensive overview of the SCLS method as introduced by . Expanding upon previous studies by ,  reformulates SPG-LS (1) into the following quadratic fractional program (QFP) utilizing the Sherman-Morrison formula :

\[_{}^{}+}{1+^{}}-^{2}.\] (4)

Moreover,  introduces an augmented variable \(=^{}/\) and further reformulate QFP (4) as:

\[_{,}v(,)\|+}{1+}-\|^{2},^{}=.\] (5)

Subsequently,  makes a assumption on the nonemptiness of the optimal solution set of QFP (5).

**Assumption 3.1** ().: Assume that the optimal solution set of (5) (or equivalently, (4)) is nonempty.

Under Assumption 3.1,  employs a nonlinear variable transformation to recast the QFP (5) as a spherical constrained least squares (SCLS) problem:

\[_{},}(},)}{2}+} {2}}-(-}{2})^{2},}^{}}+^{2}=1,\] (6)

where \(}\) and \(\) are defined in Lemmas 3.2 and 3.3. Upon identifying a feasible solution in QFP (5),  introduces Lemma 3.2 to construct a feasible solution in SCLS (6), while Lemma 3.3 describes the inverse transformation.

**Lemma 3.2** ().: _Suppose \((,)\) is a feasible solution of QFP (5). Then \((},)\), defined as_

\[}(+1)} ,\] (7)

_is feasible to SCLS (6) and \(v(,)=(},)\)._

**Lemma 3.3** ().: _Suppose \((},)\) is feasible to SCLS (6) with \( 1\). Then \((,)\), defined as_

\[}{1-}} {and}}{1-},\] (8)

_is feasible to QFP (5) and \((},)=v(,)\)._

Let \(v^{*}\) and \(^{*}\) represent the optimal values of QFP (5) and SCLS (6), respectively. Subsequently,  presents Theorem 3.4 to elucidate the relationship between the solutions of QFP (5) and SCLS (6).

**Theorem 3.4** ().: _Given Assumption 3.1, then there exists an optimal solution \((},)\) to SCLS (6) with \( 1\). Moreover, \((,)\), defined by (8), is an optimal solution to (5) and \(v^{*}=v(,)=(},)=^{*}\)._

Theorem 3.4 indicates that an optimal solution of SCLS (6) can be utilized to recover an optimal solution of SPG-LS (1). Additionally, the converse of this theorem also holds, as demonstrated by . Specifically, if \((,)\) is an optimal solution to QFP (5), \((},)\), as defined by (7), is also an optimal solution to SCLS (6).

It is important to note that  focus on a compact form of SCLS (6):

\[_{}\ q(),\ ^{T}=1,\] (9)

where \(q()=\|-(-/2)\|^{2}\), \(=(}{2}}{2})\) and \(=(}\\ ).\) Due to the equivalence of SCLS problems (6) and (9),this paper focus on the SCLS (6).

### The Convex Gaussian Min-max Theorem

The Convex Gaussian Min-max Theorem (CGMT) originates from Gordon's Gaussian Min-max Theorem (GMT) , which provides probabilistic bounds on the optimal cost of **PO** problem via a simpler **AO** problem. CGMT further tightens the bounds under convexity assumptions. Building on GMT,  introduces the following asymptotic sequence and notation.

**Definition 3.5** (GMT admissible sequence).: The sequence \(^{(d)},^{(d)},^{(d)},_{}^{(d)}, _{}^{(d)},^{(d)}}_{d}\) indexed by \(d\), with \(^{(d)}^{n d}\), \(^{(d)}^{n}\), \(^{(d)}^{d}\), \(_{}^{(d)}^{d}\), \(_{}^{(d)}^{n}\), \(^{(d)}:_{}^{(d)}_{}^{(d)} \) and \(n=n(d)\), is said to be admissible if, for each \(d\), \(_{}^{(d)}\) and \(_{}^{(d)}\) are compact sets and \(^{(d)}\) is continuous on its domain. Onwards, we will drop the superscript \((d)\) from \(^{(d)}\), \(^{(d)}\), \(^{(d)}\).

A sequence \(^{(d)},^{(d)},^{(d)},_{}^{(d)},_{}^{(d)},^{(d)}}_{d}\) defines a sequence of min-max problems

\[^{(d)}():=_{_{}^{(d)}}_{ _{}^{(d)}}^{}+^{(d)}( ,)\] (10)

\[^{(d)}(,):=_{_{}^{(d)}} _{_{}^{(d)}}\|\|^{}+\| \|^{}+^{(d)}(,)\] (11)

Importantly, the formulation (10) is called Primary Optimization (**PO**) and the formulation (11) is called Auxiliary Optimization (**AO**). Additionally, let \(_{}^{(d)}()\) denote the optimal minimizer of the **PO** problem (10), and \(_{}^{(d)}(,)\) denote the optimal minimizer of the **AO** problem (11). Define \(^{(d)}:_{}^{(d)}\) as follows,

\[^{(d)}(;,):=_{_{ }^{(d)}}\|\|^{}+\|\|^{}+ ^{(d)}(,).\] (12)

Clearly, \(^{(d)}(,):=_{_{}^{(d)} }^{(d)}(;,)\). For a sequence of random variables \(\{^{(d)}\}_{d}\) and a constant \(c\), \(^{(d)}}{{}}c\) denotes convergence in probability, i.e., \(>0,_{d}|^{(d)}-c|> =0\). Based on the GMT admissible sequence and the notation introduced above, we present the CGMT below.

**Theorem 3.6** (CGMT ).: _Let \(^{(d)},^{(d)},^{(d)},_{}^{(d)}, _{}^{(d)},^{(d)}}_{d}\) be a GMT admissible sequence as in Definition 3.5, for which additionally the entries of **G**, **g**, **h** are drawn i.i.d. from \((0,1)\). Let \(^{(d)}()\), \(^{(d)}(,)\) be the optimal costs, and, \(_{}^{(d)}()\), \(_{}^{(d)}(,)\) the corresponding optimal minimizers of the **PO** and **AO** problems in (10) and (11). The following three statements hold_

* _For any_ \(d\) _and_ \(c\)_,_ \(^{(d)}()<c 2^{(d)}(,)  c\)_._
* _For any_ \(d\)_. If_ \(_{}^{(d)}\)_,_ \(_{}^{(d)}\) _are convex, and,_ \(^{(d)}(,)\) _is convex-concave on_ \(_{}^{(d)}_{}^{(d)}\)_, then, for any_ \(\) _and_ \(t>0\)_,_ \[|^{(d)}()-|)>t 2| ^{(d)}(,)-|)>t\]
* _Assume the conditions of (ii) hold for all_ \(d\)_. Let_ \(\|\|\) _denote some norm in_ \(^{d}\) _and recall (_12_). If, there exist constants (independent of_ \(d\)_)_ \(^{*}\)_,_ \(^{*}\) _and_ \(>0\) _such that_ _(a)_ \(^{(d)}(,)}{{}}^{*}\)_, (b)_ \(\|_{}^{(d)}(,)\|}{{}}^{*}\)_, (c) with probability one in the limit_ \(d\)__ \[^{(d)}(;,)^{(d)}(,)+ \|\|-_{}^{(d)}(,)^{2}, _{}^{(d)}},\] _then,_ \[\|_{}^{(d)}()\|}{{ }}^{*}.\]Theorem 3.6 indicates that, if the optimal cost \((,)\) of (11) converges to some value \(\), the same holds true for \(()\) of **PO** (10). Furthermore, under appropriate additional assumptions, the optimal solutions of the **AO** and **PO** problems are also closely related by

\[\|_{}()\|=\|_{}(,)\|.\]

This suggests that within the CGMT framework, a challenging **PO** problem can be replaced with a simplified **AO** problem, from which the optimal solution of the **PO** problem can be accurately inferred . Subsequently, we rewrite the estimation error of the SCLS method (6) in the form of **PO** problem (10) and analyze the minimizer of the simplified **AO** problem instead.

### Basic Concept

**Conjugate pairs:** Consider a function \(f:^{d}\). The Fenchel conjugate of \(f\), denoted by \(f^{*}\), is defined as \(f^{*}()=_{}^{}-f()\), which is always convex and lower semi-continuous. By the Fenchel-Moreau theorem , if \(f\) is both convex and continuous, then \(f()=_{}^{}-f^{*}()\). In this paper, we consider the following conjugate pairs for the \(l_{2}\) norm:

\[f()=\|\|^{2} f^{*}()=\|^{2}}{4}.\] (13)

**First-order approximation:** Assume \(f\) is differentiable. According to [13, Theorem 23.4]:

\[f()=f(_{0})+[(_{0})]^{}+O(\| \|^{2})\] (14)

where \(=_{0}+\) and \((_{0})=}_{=_{ 0}}\). The linearization of \(f()\) around the interest \(_{0}\) is

\[()=f(_{0})+[(_{0})]^{}.\] (15)

As \(\|\|\) approaches \(0\), \(()\) closely approximates \(f()\).

## 4 The Error Analysis for the SCLS method

### From the SCLS Method to PO

Given that the sample \((,,)\) is generated by black box model (2), we integrate SPG-LS (1) and QFP (4) as follows:

\[=^{*}_{0}+=+_{0}}{1+_{0}}+,\] (16)

where \(_{0}=_{0}^{}_{0}/\). Drawing inspiration from Lemmas 3.2 and 3.3, we define

\[}_{0}(_{0}+1)}_{0}, _{0}-1}{_{0}+1},_{0}}{1-_{0}}}_{0}, _{0}_{0}}{1-_{0}}.\] (17)

This representation of \(}_{0}\) denotes the true weight parameter of SCLS (6). Notably, \((}_{0},_{0})\) is valid as long as \(_{0} 0\) and \(_{0} 1\), which conforms to \((},)\) defined by . Specifically, For \(_{0}=_{0}^{}_{0}}{}(1,+)\), \(_{0}>0\); For \(_{0}=_{0}^{}_{0}}{}(0,1)\), \(_{0}<0\). Additionally, \((}_{0},_{0})\) satisfies the constraint of SCLS (6), due to

\[}_{0}^{}}_{0}+_{0}^{2}=+1)^{2}}_{0}^{}_{0}+-1)^{2} }{(_{0}+1)^{2}}=+(_{0}-1)^{2}}{(_{0}+1)^{2 }}=1.\] (18)

Taking (17) into (16), the expression of \(\) can be rewritten as:

\[= +_{0}}{_{0}+1}+=-1}{2(_{0}+1)}+}{2} (_{0}+1)}_{0}+}{2}+\] \[= _{0}}{2}+}{2} }_{0}+}{2}+\] (19)Substituting \(\) in SCLS (6) with (19):

\[\|}{2}+}{2} }-(-}{2})\|^{2}= \|}{2}+}{2} }+}{2}-(_{0}}{2}+ {}{2}}_{0}+}{2}+ )\|^{2}\] \[= \|-_{0}}{2}+}{2}(}-}_{0})-\| ^{2}.\] (20)

Combining formulations (6), (18), and (20), SCLS (6) is equivalent to the optimization problem:

\[_{},}\|- _{0}}{2}+}{2}(}- }_{0})-\|^{2},\;} ^{}}+^{2}=1,\;}_{0}^{}}_{0}+_{0}^{2}=1.\] (21)

Let \(}^{*}\) denote the optimal solution to original SCLS problem (6), then, the estimation error for SCLS (21) is \(}^{*}:=}^{*}-}_{0}\). To explore the optimal estimation error for SCLS (21) under different conditions of \(_{0}\), we consider two scenarios: **Case 1:** If \(_{0}>0\), set \((})=}\|^{2}}\). **Case 2:** If \(_{0}<0\), set \((})=-}\|^{2}}\). Both cases lead to consistent error analysis outcomes for SCLS (6), but this paper primarily discusses Case 1, with Case 2 detailed in Appendix D. According to formulation (14),

\[(})=(}_{0})+ (})}{}} _{}=}_{0}}(}-}_{0})+O( \|}-}_{0}\|^{2}).\] (22)

The first-order approximation of \((})\) is:

\[}(})=(}_{0})+ (})}{}} _{}=}_{0}}(}-}_{0}).\] (23)

When \(\|}-}_{0}\| 0\), \(}(})\) converges to \((})\). Using this approximation in SCLS (20) leads to a objective:

\[\|}-_{0}}{2}+ }{2}(}-}_{0})- \|^{2}=\|(})}{}}_{}=}_{0}} (}-}_{0})+}{2}( }-}_{0})-\|^{2}\] \[=}{2}} {(})}{}}_{ }=}_{0}}(}-}_{0}) +(}-}_{0})-}{}^{2}.\]

Then, we obtain an approximated problem corresponding to SCLS (21):

\[_{}}}{2}} (})}{}} _{}=}_{0}}(}-} _{0})+(}-}_{0})-}{ }^{2}\] (24)

Let \(}}^{*}\) denote the optimal solution to the approximated SCLS problem (24), then, the estimation error for approximated SCLS (24) is \(}^{*}:=}}^{*}-}_{0}\). Leveraging the simplified representation of \(}(})\), we can conduct a precise error analysis for this approximated model (24). Furthermore, if \(f(})\) and \((})\) denote the objective functions of SCLS (21) and the approximated version (24), respectively,

\[f(})= \|-_{0}}{2}+}{2}(}-}_{0})-\| ^{2},\] \[(})= }{2}}(})}{}}_{}=}_{0}}(}-}_{0})+( }-}_{0})-}{}^ {2},\]

we have

\[_{\|}-}_{0}\| 0}(})=f( }).\] (25)

Compared with SCLS (21), the approximation (24) is tight when \(\|\;tilde{}-}_{0}\|\) goes to \(0\). We later demonstrate that this convergence condition is satisfied as \(n\) goes to \(\), independent of the original SCLS (21). This fact allows us to translate the findings about \(}}^{*}\) obtained for the approximated SCLS problem (24) to corresponding outcomes of \(}^{*}\) for the original SCLS problem (6). Given the constancy of \(\), the approximated SCLS problem (24) simplifies to:

\[_{}}^{}}+}-}{} ^{2}.\] (26)where \(}:=}-}_{0}\), and \(:=(}_{0},)=}}(})}{}} _{}=}_{0}}=}}_{0}}{}_{0}^{2}}}\). The normalization of the loss function is appropriately applied, which does not alter the optimal solution. Based on the relationship (25), when \(}\) tends to \(0\), the approximated SCLS problem (24) effectively aligns with SCLS (21). This equivalence allows for the substitution of the analysis of the optimal cost \(}^{*}\) in SCLS (21) with the analysis of the optimal solution \(}^{*}\) in the first-order optimization (26).

### From PO to AO

A key transformation in our analysis involves converting the optimization (26) into a **PO** problem within the CGMT framework. We apply conjugate pairs (13) for optimization (26):

\[_{}}^{} }+}-}{}^{2}=_{}}_{} ^{}}+^{}} ^{}-^{}}{}- ^{2}}{4},\] (27)

where \(}^{d},^{n}\). Using formulations (10) and (27), the **PO** problem associated with (26) is:

\[_{}()=_{}}_{} ^{}}+(},),\] (28)

where \((},):=^{}}^{ }-^{}}{}-^{2}}{4}\). Given that the entries of \(\) are drawn i.i.d. from \((0,1)\) and \((},)\) is a convex-concave function, the **PO** problem (28) satisfies the conditions of Theorem 3.6. Consequently, we replace the challenging **PO** problem (28) with a simplified **AO** problem using CGMT:

\[_{}(,)= _{}}_{} }^{}+^{} }+^{}}^{}- ^{}}{}-^{2 }}{4}\] \[= _{}}_{}( }+^{}}-}{})^{}+^{} }-^{2}}{4},\] (29)

where the entries of \(\) and \(\) are drawn i.i.d. from \((0,1)\). Suppose \(}_{_{}}\) represents the optimal solutions of the **PO** problem (28), and \(}_{_{}}\) denotes the optimal solutions of the **AO** problem (29). According to Theorem 3.6, if \(}_{_{}} ^{*}\), then \(}_{_{}} ^{*}\). The reasons why **PO** (28) is more complex than **AO** (29) and the difficulties of (28) are summarized as: \((i)\) The **PO** (28) contains the matrix \(^{n d}\) and the challenge lies in processing matrices. the **AO** (29) only contains vectors with dimensions \(d\) or \(n\), which are easier to handle than matrices; \((ii)\) The **AO** (29) reduces the dimension of the **PO** (28) from \(n d\) to \(\{d,n\}\), thereby simplifying the **PO** (28); \((iii)\) It is difficult to obtain the value that the **PO** (28) concentrates on; \((iv)\) The **AO** problem (29) can be further simplified by **AO** optimization (33) that only includes estimation error variable \(}\), which is easier to analyze than **PO** (28). These explanations enable us to effectively analyze the minimizer of the **AO** problem (29) instead of the more complex **PO** problem (28).

### Simplification for AO

Considering that the elements of \(\) and \(\) are drawn i.i.d. from \((0,1)\), and \((0,^{2}_{d})\), the vector expression \(}+^{}}- }{}\) in **AO** (29) behaves statistically as a random vector with entries drawn i.i.d. from \((0,}^{2}+(^{}})^{2}+}{})\), where \(_{d}\) represents a \(d d\) identity matrix. Adopting the approach outlined by , we simplify the first term in **AO** (29) to \(}^{2}+(^{}})^{2}+ }{}}^{}\):

\[_{}}_{}}^{2}+(^{}})^{2}+}{ }}^{}+^{}}-^{2}}{4}.\] (30)

Defining \(=\) and recognizing that \(_{}^{}== \), and considering \((0,_{d})\), we reformulate the optimization (30) as:

\[_{}}_{ 0}}^{2}+(^{}})^{2}+}{ }}+^{}}-}{4}.\] (31)The formulation (31) is a quadratic function of \(\) with the symmetric axis:

\[_{s}=2}\|^{2}+(^{} })^{2}+}{}}\|\|+^{} }>\|}\|(\|\|-\|\|).\]

Additionally, \(_{s}(\|\|+\|\|)>\|}\|(\|\|^{2}-\| \|^{2})\). Referring to [29, Lem. B.2], \(\|\|^{2}\) and \(\|\|^{2}\) concentrate around their means \(n\) and \(d\), respectively. Consequently, the value around which \(_{s}\) concentrates is nonnegative, due to \(d/n<1\). Moreover, taking \(_{s}\) into (31), the optimization objective (31) concentrates around

\[_{}}}\|^{2}+(^{}})^{2}+}{} }\|\|+^{}}^{2}\] \[= _{}}(\|} \|^{2}+(^{}})^{2}+}{})\| \|^{2}+(^{}})^{2}+2^{}}\|\|}\|^{2}+(^{}})^{2}+}{}}.\] (32)

Drawing on [29, Lem. B.2], \(\|\|^{2}\), \((^{}})^{2}\) and \(^{}}\|\|\) concentrate around their expected values: \([\|\|^{2}]=n\), \((^{}})^{2}=\|}\|^{2}\) and \((^{}}\|\|)=0\). Besides, define \((}):=_{n}}\|^{2} }{n}\). Using analytical methods established by , as \(n\) goes to \(+\), the optimal minimizer of (32) converges to the optimal minimizer of the following deterministic optimization in probability:

\[_{}}\|}\|^{2}+(^{}})^{2}+(})+}{}.\] (33)

Here, we successfully reduced the complex **AO** problem (29) to a more manageable deterministic optimization problem (33), effectively focusing only on the estimation error variable \(}\) for further analysis.

### Error Analysis

Building on the previous analysis, if the optimal solution of optimization (33) is \(\|}\|=^{*}\), we have \(\|}_{}\|^{*}\) for **AO** problem (29). Then, by virtue of CGMT, \(\|}_{}\|^{*}\) also holds for **PO** problem (28). If \(^{*}\) further satisfies \(^{*}=0\), based on the relationship between the original and approximated SCLS in Section 4.1, we have \(\|}-}_{0}\|0\) for SCLS problems (21) and (6). Therefore, it only remains to obtain the optimal value of \(\) in optimization (33) that plays the role of \(\|}\|\). We conclude the estimation error analysis of the SCLS problem (6) with the following theorem.

**Theorem 4.1**.: _Suppose \(}_{0}\) is the true weight parameter of the original SCLS problem (6), and \(}^{*}\) is the optimal solution to the objective function of SCLS (6). If \(_{n}(0,1)\), the estimation error of SCLS (6) is given by the following probability limit:_

\[_{n}\|}^{*}-}_{0}\|0.\]

The proof is based on the simplified **AO** problem (33) and is detailed in in Appendix B

_Remark 4.2_.: Theorem 4.1 indicates that, as \(n\) goes to \(\), the parameter vector \(}^{*}\) learned through the SCLS method (6) reliably converges to the actual parameter vector \(}_{0}\) in probability. We then can utilize Theorem 3.4 to establish the estimation error of SPG-LS (1) solved by the SCLS (6).

When applying the SCLS method (6) to solve SPG-LS (1), the validity of the solution \(^{*}\) learned by the SCLS method is supported by the following theorem.

**Theorem 4.3**.: _Suppose \(_{0}\) is the true weight parameter of the SPG-LS (1), \(}^{*}\) is the optimal solution learned by SCLS (6), and \(^{*}\) is the optimal solution recovered from \(}^{*}\) by Theorem 3.4. If \(_{n}(0,1)\), the estimation error of SPG-LS (1) solved by the SCLS (6) is given by the following probability limit:_

\[_{n}\|^{*}-_{0}\|0.\]

The proof relies on Theorem 4.1 and can be seen in Appendix C

_Remark 4.4_.: Theorem 4.3 demonstrates that, as \(n\) goes to \(\), the parameter vector \(^{*}\) learned through the SCLS method (6) reliably converges to actual parameter vector \(_{0}\) in probability. This substantiates the efficacy of the SCLS method (6) in solving SPG-LS (1).

Experiment Results

This section outlines numerical experiments conducted on synthetic datasets with high feature dimensions  and various levels of sparsity to validate our theoretical claims. In alignment with the methodologies described by , the true parameter \(_{0}\) is generated randomly with sparsity levels set at \(=1,0.1,0.01,0.001\), where \(k\) is the number of nonzero elements in \(_{0}\). And, the regularization parameter \(\) is set to be \(0.1,0.01\).

For each dataset \(S=\{(_{i},y_{i},z_{i})\}_{i=1}^{n}\), the input vector \(_{i}\) is drawn i.i.d. from \((,_{d})\), the fake output label \(z_{i}\) is drawn i.i.d. from \((0,1)\). Consistent with the noise model used by , the noise \(_{i}\) in our experiments is drawn i.i.d. from \((0,0.1^{2})\). According to our data generation model (2), the output labels \(y_{i}\) are derived via:

\[_{i}^{*}=*{arg\,min}_{}}\|_{0}^{} {}_{i}-z_{i}\|^{2}+\|_{i}-}_{i}\|^{2},y_{i}=_{0}^{}_{i}+_{i}.\]

Using the samples \(S=\{(_{i},y_{i},z_{i})\}_{i=1}^{n}\), we employ the SCLS method as described by  to address the SPG-LS problem (1) and assess the estimation error \(\|^{*}-_{0}\|\). The estimation error is averaged over 10 trials to gauge the effectiveness of the SCLS method. Results for \(=0.5\) are shown in Figure 1. The computational resources are detailed in Appendix E.1. Additional experimental results with other parameter settings can be seen in Appendix E.2.

The outcomes depicted in Figure 1 illustrate that the estimation error \(\|^{*}-_{0}\|\) generated by the SCLS method decreases to 0 as \(n\) goes to \(\). This trend corroborates the theoretical predictions established in our Theorem 4.3, thereby affirming the efficacy and reliability of the SCLS method.

## 6 Conclusion

In this paper, we apply the CGMT framework to conduct a rigorous theoretical error analysis of the SCLS method proposed by . Specifically, when SCLS (6) is applied to tackle a SPG-LS model (1) with \((0,_{d})\), \((0,_{n})\) and \((0,^{2})\),if \(_{n}(0,1)\), we establish that:

\[_{n}\|^{*}-_{0}\|}{{ }}0.\]

This result confirms that the learner \(^{*}\) obtained through SCLS (6) accurately estimates the true learner \(_{0}\) of SPG-LS model. Our empirical findings are consistent with these theoretical results. This theoretical error analysis not only validate the reliability of the SCLS method but also provides a framework for error analysis applicable to other statistical learning algorithms.