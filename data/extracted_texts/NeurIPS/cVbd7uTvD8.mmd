# SC3D: Self-conditioned Generative Gaussian Model with 3D-aware Feedback

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named SC3D, which integrates diffusion-based multi-view image generation and 3D reconstruction through a self-conditioning mechanism. In our framework, these two modules are established as a cyclic relationship so that they adapt to the distribution of each other. During the denoising process of multi-view generation, we feed rendered color images and maps by SC3D itself to the multi-view generation module. This self-conditioned method with 3D aware feedback unites the entire process and improves geometric consistency. Experiments show that our approach enhances sampling quality, and improves the efficiency and output quality of the generation process.

## 1 Introduction

3D content creation from a single image have improved rapidly in recent years with the adoption of large 3D datasets [1; 2; 3] and diffusion models [4; 5; 6]. A body of research [7; 8; 9; 10; 11; 12; 13; 14] has focused on multi-view diffusion models, fine-tuning pretrained image or video diffusion models on 3D datasets to enable consistent multi-view synthesis. These methods demonstrate generalizability and produce promising results. Another group of works [15; 16; 17; 18; 19] propose generalizable reconstruction models, generating 3D representation from one or few views in a feed-forward process. Theses reconstruction models built upon convolutional network or transformer backbone, have led to efficient image-to-3D creation.

Since single-view reconstruction models  trained on 3D datasets [1; 20] lack generalizability and often produce blurring at unseen viewpoints, several works [21; 16; 18; 19] extend models to sparse-view input, boosting the reconstruction quality. As shown in Fig. 1, these methods split 3D generation into two stages: multi-view synthesis and 3D reconstruction. By combining generalizable multi-view diffusion models and robust sparse-view reconstruction models, such pipelines achieve high-quality image to 3D generation. However, combining the two independently designed models introduces a significant "data bias" to the reconstruction model. The data bias is mainly reflected in two aspects: **(1) Multi-view bias.** Multi-view diffusion models learn consistency at the image level, struggle to ensure geometric consistency. When it comes to reconstruction, multi-view images that lack geometric consistency affect the subsequent stage. **(2) Limited data for reconstruction model.** Unlike multi-view diffusion models, reconstruction models which are trained from scratch on limited 3D dataset, lacks the generalization ability.

Recent works like IM-3D  and VideoMV  have attempted to aggregate the rendered views of the reconstructed 3D model into previous-step multi-view synthesis, thus improving the capabilityand consistency of the generated multi-view images. These methods integrate the aforementioned two stages at the inference phase. But the models at both stages still lack joint training, which prevents the reconstruction model from enhancing its robustness to the generated poor multiviews. Moreover, these test-time aggregating methods cannot directly utilize geometric information such as depth maps, normal maps, or position maps that can also be obtained from the reconstructed 3D. Notably, these explicit 3D aware maps can better guide the multi-view generation.

To address these challenges, we propose a unified single image-to-3D creation framework, named SC3D, which integrates multi-view generation and 3D reconstruction through a self-conditioning mechanism. Our framework involves jointly training the multi-view diffusion model and the reconstruction model. In SC3D, these two modules are established as a cyclic relationship so that they adapt to the characteristics of each other, enabling robust generation at inference. Specifically, during the denoising process, we feed rendered 3D-aware maps from the reconstructed 3D to the multi-view generation module. By leveraging the color maps and spatial canonical coordinates maps from the reconstruction 3D representation as condition, our multi-view diffusion model synthesizes multi-view images that better conform to the actual 3D structure. This self-conditioned framework with 3D aware feedback unites the 3D generation process and enhances the robustness for unseen complex scenes. Experiments on the GSO dataset  validate that our SC3D reduces data bias between training and inference, and enhances the overall efficiency and output quality.

Our key contributions are as follows:

* We introduce SC3D, which unifies multi-view generation and 3D reconstruction in a single framework and involves jointly training these two modules, enabling adaption to each other.
* SC3D employs a self-conditioning mechanism with 3D-aware feedback, using rendered 3D-aware maps to guide the multi-view generation, ensuring better geometric consistency and robustness.
* Experiments show that SC3D significantly reduces data bias, improves the quality of 3D reconstruction, and enhances overall efficiency in creating 3D content from a single image.

## 2 Related Work

Image/Video Diffusion for Multi-view GenerationDiffusion models [25; 26; 27; 28; 29; 30; 31; 32; 33; 34] have demonstrated their powerful generative capabilities in image and video generation fields. Current research [7; 8; 9; 10; 11; 12; 13; 14; 35] fine-tunes pretrained image/video diffusion models on 3D datasets like Objaverse  and MVImageNet . Zero123  introduces relative view condition to image diffusion models, enabling novel view synthesis from a single image and preserving generalizability. Based on it, methods like SyncDreamer , ConsistNet  and EpiDiff  design attention modules to generate consistent multi-view images. These methods fine

Figure 1: **Concept comparison** between SC3D and previous two-stage methods. Instead of directly combining multi-view diffusion model and reconstruction model, our self-conditioned framework involves joint training of these two models and establish them as a cyclic association. During the denoising process, rendered 3D-aware maps are fed to the multi-view generation module.

tuned from image diffusion models produce generally promising results. By considering multi-view images as consecutive frames of a video (e.g., orbiting camera views), it naturally leads to the idea of applying video generation models to 3D generation . However, since the diffusion model is not explicitly modeled in 3D space, the generated multi-view images often struggle to achieve consistent and robust details.

Image to 3D ReconstructionRecently, the task of reconstructing 3D objects has evolved from traditional multi-view reconstruction methods [37; 38; 39; 40] to feed-forward reconstruction models [15; 41; 42; 16; 17; 18; 19]. Utilizing one or few shot as input, these highly generalizable reconstruction models synthesize 3D representation, enabling the rapid generation of 3D objects. LRM  proposes a transformer-based model to effectively map image tokens to 3D triplanes. Instant3D  further extends LRM to sparse-view input, significantly boosting the reconstruction quality. LGM  and GRM  replace the triplane representation with 3D Gaussians  to enjoy its superior rendering efficiency. CRM  and InstantMesh  optimize on the mesh representation for high-quality geometry and texture modeling. These reconstrucion models built upon convolutional network architecture or transformer backbone, have led to efficient image-to-3D creation.

Pipelines of 3D GenerationEarly works propose to distill knowledge of image prior to create 3D models via Score Distillation Sampling (SDS) [43; 44; 45], limited by the low speed of per-scene optimization. Several works [9; 11; 14; 22] fine-tune image diffusion models to generate multi-view images, which are then utilized for 3D shape and appearance recovery with traditional reconstruction methods [46; 40]. More recently, several works [21; 16; 18; 19; 23] involve both multi-view diffusion models and feed-forward reconstruction models in the generation process. Such pipelines attempt to combine the processes into a cohesive two-stage approach, thus achieving highly generalizable and high-quality single-image to 3D generation. However, due to the lack of explicit 3D modeling, the results generated by the multi-view diffusion model cannot guarantee strong consistency, which will lead to data deviation for the reconstructed model between the testing phase and the training phase. Compared to them, we propose a unified pipeline, integrating the two stages through a self-conditioning mechanism at the training stage, with 3D aware feedback for high consistency.

## 3 Method

Given a single image, SC3D aims to generate multiview-consistent images with a reconstructed 3D Gaussion model. To reduce the data bias and improve robustness of the generation, we propose SC3D, a unified 3D generation framework which integrates multi-view synthesis and 3D reconstruction through a self-conditioning mechanism. As illustrated in Fig. 2, the proposed framework involves a video diffusion model (SVD ) as multi-view generator (refer to Section 3.1) and a feed-forward reconstruction model to recover a 3D Gaussian Splatting (refer to Section 3.2. Moreover, we introduce a self-conditioning mechanism, feeding the 3D-aware information obtained from the reconstruction module back to the multi-view generation process (refer to Section 3.3). The 3D-aware denoising sampling strategy iteratively refines the multi-view images and the 3d model, thus enhancing the final production.

### Video Diffusion Model as Multiview Generator

Recent video diffusion models such as those in [13; 34] have demonstrated a remarkable capability to generate 3D-aware videos by scaling up both the model and dataset. Our research employs the well-known Stable Video Diffusion (SVD) Model, which generates videos from image input. Formally, given an image \(I^{3 h w}\), the model is designed to generate a video \(V^{f 3 h w}\). Further details about SVD can be found in Appendix A.1.

We enhance the video diffusion model with camera control \(c\) to generate images from different viewpoints. Traditional methods encode camera positions at the frame level, which results in all pixels within one view sharing the same positional encoding [47; 13]. Building on the innovations of previous work [11; 35], we integrate the camera condition \(c\) into the denoising network by parameterizing the rays \(=(o,o d)\). Specifically, we use two-layered MLP to inject Plucker ray embeddings for each latent pixel, enabling precise positional encoding at the pixel level. This approach allows for more detailed and accurate 3D rendering, as pixel-specific embedding enhances the model's ability to handle complex variations in depth and perspective across the video frames.

In our framework, unlike existing two-stage methods, our multi-view diffusion model does not complete multiple denoising steps independently. In contrast, in the denoising sampling loop, we obtain the straightly predicted \(}_{0}^{f}\) at the current timestep, which will be used for subsequent 3D reconstruction. Then we use rendered 3d-aware view maps as conditions to guide the next denoising step. Therefore, at each sampling step, we do the reparameterization of the output from the denoising network \(F_{}\) to convert it into \(}_{0}^{f}\). Taking a single view as an example, we processes the denoised image \(c_{}()\) and the associated noise level \(c_{}()\), which \(\) indicates the standard deviation of the noise. The reparameterization is formulated as follows:

\[}_{0}=c_{}()+c_{}( )F_{}(c_{}();c_{}()).\] (1)

The above operation process adjusts the output of \(F_{}\) to \(}_{0}^{f}\), which will be decoded into images and passed to the subsequent 3D reconstruction module.

### Feed-Forward Reconstruction Model

In the SC3D framework, the feed-forward reconstruction model is designed to recover 3D models from pre-generated multi-view images, which can be images decoded from straightly predicted \(}_{0}^{f}\), or completely denoised images. We utilize Large Multi-View Gaussian Model (LGM) \(\) as our reconstruction module due to its real-time rendering capabilities that benefit from 3D representation of Gaussian Splatting. This method integrates seamlessly with our jointly training framework, allowing for quick adaptation and efficient processing.

We pass four specific views from the reparameterized output \(}_{0}^{f}\) to the Large Gaussian Model (LGM) for 3D Gaussian Splatting reconstruction. To enhance the performance of LGM, particularly its sensitivity to different noise levels \(c_{}()\) and image details, we introduce a zero-initialized time embedding layer within the original U-Net structure of the LGM. This innovative modification enables the LGM to dynamically adapt to the diverse outputs that arise at different stages of the

Figure 2: **Overview of SC3D.** We adopt a video diffusion model as the multi-view generator by incorporating the input image and relative camera poses. In the denoising sampling loop, we decode the predicted \(}_{0}^{f}\) to noise-corrupted images, which are then used to recover 3D representation by a feed-forward reconstruction model. Then the rendered color images and coordinates maps are encoded and fed into the next denoising step. At inference, the 3D-aware denoising sampling strategy iteratively refines the images by incorporating feedback from the reconstructed 3D into the denoising loop, enhancing multi-view consistency and image quality.

denoising process, thereby substantially improving its capacity to accurately reconstruct 3D content from images that have undergone partial denoising.

The loss function employed for the fine-tuning of the LGM is articulated as follows:

\[_{}=_{}(_{0},( }_{0},c_{}()))+_{}(_{0},(}_{0},c_{}( ))).\] (2)

where we have utilized the mean square error loss \(_{}\) for the color channel and a VGG-based perceptual loss \(_{}\) for the LPIPS term. In practical applications, the weighting factor \(\) is conventionally set to 1.

Additionally, to maintain the model's reconstruction capability for normal images, we also input the model without adding noise and calculate the corresponding loss. In this case, we set \(c_{}()\) to 0.

### 3D-Aware Feedback Mechanism

As shown in Fig. 2, we adopt a 3D-aware feedback mechanism that involves the rendered color images and geometric maps produced by our reconstruction module in a denoising loop to further improve the multi-view consistency of the resulting images and facilitate cyclic adaptation of the two stages. Instead of integrating multi-view generation and 3D reconstruction at the inference stage using re-sampling strategy [22; 23], we propose to train these two modules jointly to support more informative feedback. Specifically, in addition to the rendered color images, our flexible framework is able to derive additional geometric features to guide the generation process, which brings guidance of more explicit 3D information to multi-view generation.

In practice, we obtain color images and canonical coordinates maps  from the reconstructed 3D model, and utilize them as condition to guide the next denoising step of multi-view generation. We use position maps instead of depth maps or normal maps as the representative of geometric maps because canonical coordinate maps record the vertex coordinate values after normalization of the overall 3D model, rather than the normalization of the relative self-view (such as depth maps). This operation enables the rendered maps to be characterized as cross-view alignment, providing the strong guidance of more explicit cross-view geometry relationship. The details of canonical coordinates map can be found in Appendix A.2.

We adopt a 3D-aware self-conditioning  training and inference strategy that leverages reconstruction stage results to enhance multi-view consistency and the quality of generated images. During training, the original denoising network \(F_{}(;)\) is augmented with a 3D-aware feedback denoising network \(F_{}((}_{0});)\), where \((}_{0})\) is the output of the LGM reconstruction.

To encode color images and coordinates maps into the denoising network of multi-view generation module, we design two simple and lightweight encoders for color images and coordinates maps using a series of convolutional neural networks, like T2I-Adapter . The encoders are composed of four feature extraction blocks and three downsample blocks to change the feature resolution, so that the dimension of the encoded features is the same as the intermediate feature in the encoder of U-Net denoiser. The extracted features from the two conditional modalities are then added to the U-Net encoder at each scale.

Training StrategyAs illustrated in Algorithm 1, to train a 3D-aware multi-view generation network, we use the rendered maps by the 3D reconstruction module as the self-conditioning input. In practice, we randomly use this self-conditioning mechanism with a probability of 0.5. When not using the 3D reconstruction result, we set \((_{0})=0\) as the input. This probabilistic approach ensures balanced learning, allowing the model to effectively incorporate 3D information without over-reliance on it.

``` deftrain_loss(x,cond_image): """Returnsthelossonatrainingexamplez."""
#Samplesigmafromalog-normaldistribution sigma=log_normal(P_mean,P_std)
#Reparameterizesigmatoobtainconditioningparameters c_in,c_out,c_skip,c_noise,lambda_param=reparameterizing(sigma)
#Addnoisetoinputdata noise_x=x+sigma*normal(mean=0,std=1) input_x=c_in*noise_x
#Initialpredictionwithoutself-conditioning self_cond=None F_pred=net(input_x,c_noise,cond_image,self_cond) pred_x=c_out+F_pred+c_skip+noise_x
#Updatesself_condusingthereconstructionmodel self_cond=recon_model(pred_x,c_noise)
#Userenderedmapsasconditionanddenoise ifself_condandnp.random.uniform(0,1)>0.5: F_pred=net(input_x,t,cond_image,self_cond.detach()) pred_x=c_out*F_pred+c_skip*noise_x
#Computeloss loss=lambda_param*(pred_x-target)**2 recon_loss=recon_loss_fn(self_cond,x) returnloss.mean()+recon_loss ```

**Algorithm 1** Training SC3D with the self-conditioned strategy.

Inference/sampling strategyAt the inference stage, as shown in Algorithm 2, the 3D feedback \((}_{0})\) is initially set to \(0\). At each timestep, this feedback is updated with the previous reconstruction result \((}_{0})\). This iterative process refines the 3D representation, ensuring each frame benefits from prior reconstructions, leading to higher quality and more consistent 3D-aware images.

``` defgenerate(sigma,cond_image): self_cond=None x_T=normal(mean=0,std=1)#InitializelatentvariablewithGaussiannoise forsigmainsigma: #Reparameterizesigmatoobtainconditioningparameters c_in,c_out,c_skip,c_noise,lambda_param=reparameterizing(sigma)
#Addnoisetothelatentvariable noise_x=x_T+sigma*normal(mean=0,std=1) input_x=c_in*noise_x
#Generateprediction F_pred=net(input_x,t,cond_image,self_cond) pred_x=c_out*F_pred+c_skip*noise_x
#Updateself_condusingthereconstructionmodel self_cond=recon_model(pred_x,c_noise) returnpred_x ```

**Algorithm 2** Sampling algorithm of SC3D.

Inference/sampling strategyAt the inference stage, as shown in Algorithm 2, the 3D feedback \((}_{0})\) is initially set to \(0\). At each timestep, this feedback is updated with the previous reconstruction result \((}_{0})\). This iterative process refines the 3D representation, ensuring each frame benefits from prior reconstructions, leading to higher quality and more consistent 3D-aware images.

``` defgenerate(sigma,cond_image): self_cond=None x_T=normal(mean=0,std=1)#InitializelatentvariablewithGaussiannoise forsigmainsigma: #Reparameterizessigmatoobtainconditioningparameters c_in,c_out,c_skip,c_noise,lambda_param=reparameterizing(sigma)
#Addnoisetothelatentvariable noise_x=x_T+sigma*normal(mean=0,std=1) input_x=c_in*noise_x
#Generateprediction F_pred=net(input_x,t,cond_image,self_cond) pred_x=c_out*F_pred+c_skip*noise_x
#Updateself_condusingthereconstructionmodel self_cond=recon_model(pred_x,c_noise) returnpred_x ```

**Algorithm 3** Training SC3D with the self-conditioned strategy.

## 4 Experiments

We focus on 3D asset content synthesis, training our model on the G-Objaverse [1; 51] dataset and the LVIS subset of Objaverse, which consists of 300K high-quality 3D objects and is widely used in 3D generation. We evaluate SC3D on the Google Scanned Object (GSO) dataset , which consists of approximately 1,000 scanned models, and we randomly select 100 samples for comparison. We adopt TripoSR, SyncDreamer, SV3D, ImageDream  combined with LGM  as the baseline approach  and VideoMV as baseline methods. For each baseline, we report PSNR, SSIM, and LPIPS metrics.

### Comparison results

For LGM, we utilize the official LGM single-image generation pipeline, which employs ImageDream  to transition from a single image input to multiple images. However, the conical coordinate system employed by ImageDream complicates the direct evaluation of the output. To address this, we use the official code to test on the GSO dataset, followed by manual calibration to assess the generated quality, as illustrated in Fig. 3. The misalignment between the two stages of ImageDream and LGM often results in generated models with blurred linear edges and geometric ambiguities. Nonetheless, our LGM, enhanced by a feedback mechanism, demonstrates significantly improved geometric and texture quality, producing results that closely approximate reality.

As illustrate in 6, We find that although it can generate very continuous frames, the generated content tends to deviate from the given input image. This results in sub-optimal performance in

Figure 4: Qualitative comparison with no-feedback and 3d-aware feedback.

Figure 3: Qualitative comparison with ImageDream-LGM and Our LGM.

the reconstruction metric. Additionally, VideoMV training the LGM separately with noisy images deteriorates, resulting in a visually noticeable reduction in its ability to generate texture details.

### Ablation study

To validate the effectiveness of the proposed SC-3D framework, we conducted a series of ablation studies comparing PSNR, SSIM, and LPIPS metrics for different configurations (Table 2). We start with the base video diffusion model we trained, We then introduced 3D coordinates map feedback and RGB texture feedback from the reconstruction model to the diffusion model, which improved geometric consistency and texture detail across views. Combining both feedback mechanisms in the SVD + 3D-aware Feedback configuration resulted in the best performance, demonstrating significant improvements in the final 3D reconstruction quality by enhancing both geometric consistency and texture detail preservation.

   Method & Resolution & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  TripoSR & \(256 256\) & 18.481 & 0.8506 & 0.1357 \\ SyncDreamer & \(256 256\) & 20.056 & 0.8163 & 0.1596 \\ SV3D & \(576 576\) & 21.042 & 0.8497 & 0.1296 \\ VideoMV(SD) & \(256 256\) & 17.459 & 0.806 & 0.1446 \\ VideoMV(GS) & \(256 256\) & 17.577 & 0.807 & 0.1454 \\  SC3D (SVD) & \(512 512\) & 21.625 & 0.9045 & 0.1011 \\ SC3D (GS) & \(512 512\) & **21.761** & **0.9094** & **0.0991** \\   

Table 1: Comparison of performance metrics across different models and configurations.

   Method & Variant & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  SVD & SVD & 20.038 & 0.8745 & 0.1253 \\  & GS & 20.549 & 0.8651 & 0.1183 \\ SVD + Coordinates Map Feedback & SVD & 21.021 & 0.8973 & 0.1110 \\  & GS & 21.325 & 0.8937 & 0.1092 \\ SVD + 3D-aware Feedback & SVD & 21.752 & **0.9122** & 0.0993 \\  & GS & **21.761** & 0.9094 & **0.0991** \\   

Table 2: Performance metrics of different feedback mechanisms.

Figure 5: Out of distribution testing results.

in Table 3. It can be observed that when no feedback mechanism is used, there is a significant discrepancy between the two models' modalities, which leads to a degradation in their combined performance.

### Limitations

Current models utilize Gaussian splatting as a 3D representation, mapping and rendering coordinates to textures for feedback. Although algorithms for converting Gaussian Splatting to meshe are under development, achieving high quality in converting Gaussian models to general meshes remains challenging. Directly employing a NeRF-based feed-forward model during the training process significantly reduces training speed due to the computational demands of volumetric rendering. Our model currently lacks the ability to generalize to the scene level, a limitation we intend to address in future research.

## 5 Conclusion

In this paper, we introduce SC3D, a unified framework for 3D generation from a single image that integrates multi-view image generation and 3D reconstruction through a self-conditioning mechanism. By establishing a cyclic relationship between these two stages, our approach effectively mitigates the data bias encountered in traditional methods. The self-conditioned method with 3D-aware feedback enhances geometric consistency throughout the generation process.

Our experiments demonstrate that SC3D not only improves the quality and efficiency of the generation process but also achieves superior geometric consistency and detail in the reconstructed 3D models. By jointly training the multi-view diffusion model and the reconstruction model, SC3D adapts to the inherent biases of each stage, resulting in more robust and accurate outputs.

   Method & Delta PSNR & Delta SSIM & Delta LPIPS \\  SVD & 0.511 & 0.0094 & 0.0070 \\ SVD + Coordinates Map Feedback & 0.304 & 0.0036 & 0.0018 \\ SVD + 3D-aware Feedback & 0.009 & 0.0028 & 0.0002 \\   

Table 3: The absolute differences in performance metrics between GS and SVD generation results..

Figure 6: The Generation Example of VideoMV