# YOLOv10: Real-Time End-to-End Object Detection

Ao Wang\({}^{1}\) Hui Chen\({}^{2}\) Lihao Liu\({}^{1}\) Kai Chen\({}^{1}\)

Zijia Lin\({}^{1}\) Jungong Han\({}^{3}\) Guiguang Ding\({}^{1}\)

\({}^{1}\)School of Software, Tsinghua University  \({}^{2}\)BNRist, Tsinghua University

\({}^{3}\)Department of Automation, Tsinghua University

wa22@mails.tsinghua.edu.cn huichen@mail.tsinghua.edu.cn linzijia07@tsinghua.org.cn {louisliu2048,chenkai2010.9,jungonghan77}@gmail.com dinggg@tsinghua.edu.cn

Corresponding author.

###### Abstract

Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For

Figure 1: Comparisons with others in terms of latency-accuracy (left) and size-accuracy (right) trade-offs. We measure the end-to-end latency using the official pre-trained models.

example, our YOLOv10-S is 1.8\(\) faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8\(\) smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46% less latency and 25% fewer parameters for the same performance. Code and models are available at https://github.com/THU-MIG/yolov10.

## 1 Introduction

Real-time object detection has always been a focal point of research in the area of computer vision, which aims to accurately predict the categories and positions of objects in an image under low latency. It is widely adopted in various practical applications, including autonomous driving , robot navigation , and object tracking , _etc_. In recent years, researchers have concentrated on devising CNN-based object detectors to achieve real-time detection [19; 23; 48; 49; 50; 57; 13]. Among them, YOLOs have gained increasing popularity due to their adept balance between performance and efficiency [2; 20; 29; 21; 65; 60; 70; 8; 71; 17; 29]. The detection pipeline of YOLOs consists of two parts: the model forward process and the NMS post-processing. However, both of them still have deficiencies, resulting in suboptimal accuracy-latency boundaries.

Specifically, YOLOs usually employ one-to-many label assignment strategy during training, whereby one ground-truth object corresponds to multiple positive samples. Despite yielding superior performance, this approach necessitates NMS to select the best positive prediction during inference. This slows down the inference speed and renders the performance sensitive to the hyperparameters of NMS, thereby preventing YOLOs from achieving optimal end-to-end deployment . One line to tackle this issue is to adopt the recently introduced end-to-end DETR architectures [4; 81; 73; 30; 36; 42; 67]. For example, RT-DETR  presents an efficient hybrid encoder and uncertainty-minimal query selection, propelling DETRs into the realm of real-time applications. Nevertheless, when considering only the forward process of model during deployment, the efficiency of the DETRs still has room for improvements compared with YOLOs. Another line is to explore end-to-end detection for CNN-based detectors, which typically leverages one-to-one assignment strategies to suppress the redundant predictions [6; 55; 66; 80; 17]. However, they usually introduce additional inference overhead or achieve suboptimal performance for YOLOs.

Furthermore, the model architecture design remains a fundamental challenge for YOLOs, which exhibits an important impact on the accuracy and speed [50; 17; 71; 8]. To achieve more efficient and effective model architectures, researchers have explored different design strategies. Various primary computational units are presented for the backbone to enhance the feature extraction ability, including DarkNet [48; 49; 50], CSPNet , EfficientRep  and ELAN [62; 64], _etc_. For the neck, PAN , BiC , GD  and RepGFPN , _etc_., are explored to enhance the multi-scale feature fusion. Besides, model scaling strategies [62; 61] and re-parameterization [11; 29] techniques are also investigated. While these efforts have achieved notable advancements, a comprehensive inspection for various components in YOLOs from both the efficiency and accuracy perspectives is still lacking. As a result, there still exists considerable computational redundancy within YOLOs, leading to inefficient parameter utilization and suboptimal efficiency. Besides, the resulting constrained model capability also leads to inferior performance, leaving ample room for accuracy improvements.

In this work, we aim to address these issues and further advance the accuracy-speed boundaries of YOLOs. We target both the post-processing and the model architecture throughout the detection pipeline. To this end, we first tackle the problem of redundant predictions in the post-processing by presenting a consistent dual assignments strategy for NMS-free YOLOs with the dual label assignments and consistent matching metric. It allows the model to enjoy rich and harmonious supervision during training while eliminating the need for NMS during inference, leading to competitive performance with high efficiency. Secondly, we propose the holistic efficiency-accuracy driven model design strategy for the model architecture by performing the comprehensive inspection for various components in YOLOs. For efficiency, we propose the lightweight classification head, spatial-channel decoupled downsampling, and rank-guided block design, to reduce the manifested computational redundancy and achieve more efficient architecture. For accuracy, we explore the large-kernel convolution and present the effective partial self-attention module to enhance the model capability, harnessing the potential for performance improvements under low cost.

Based on these approaches, we succeed in achieving a new family of real-time end-to-end detectors with different model scales, _i.e._, YOLOv10-N / S / M / B / L / X. Extensive experiments on standard benchmarks for object detection, _i.e._, COCO , demonstrate that our YOLOv10 can significantly outperform previous state-of-the-art models in terms of computation-accuracy trade-offs across various model scales. As shown in Fig. 1, our YOLOv10-S / X are 1.8\(\) / 1.3\(\) faster than RT-DETR-R18 / R101, respectively, under the similar performance. Compared with YOLOv9-C, YOLOv10-B achieves a 46% reduction in latency with the same performance. Moreover, YOLOv10 exhibits highly efficient parameter utilization. Our YOLOv10-L / X outperforms YOLOv8-L / X by 0.3 AP and 0.5 AP, with 1.8\(\) and 2.3\(\) smaller number of parameters, respectively. YOLOv10-M achieves the similar AP compared with YOLOv9-M / YOLO-MS, with 23% / 31% fewer parameters, respectively. We hope that our work can inspire further studies and advancements in the field.

## 2 Related Work

**Real-time object detectors.** Real-time object detection aims to classify and locate objects under low latency, which is crucial for real-world applications. Over the past years, substantial efforts have been directed towards developing efficient detectors [19; 57; 48; 34; 79; 75; 32; 31; 41]. Particularly, the YOLO series [48; 49; 50; 2; 20; 29; 62; 21; 65] stand out as the mainstream ones. YOLOv1, YOLOv2, and YOLOv3 identify the typical detection architecture consisting of three parts, _i.e._, backbone, neck, and head [48; 49; 50]. YOLOv4  and YOLOv5  introduce the CSPNet  design to replace DarkNet , coupled with data augmentation strategies, enhanced PAN, and a greater variety of model scales, _etc._ YOLOv6  presents BiC and SimCSPSPPF for neck and backbone, respectively, with anchor-aided training and self-distillation strategy. YOLOv7  introduces E-ELAN for rich gradient flow path and explores several trainable bag-of-freeheights methods. YOLOv8  presents C2f building block for effective feature extraction and fusion. Gold-YOLO  provides the advanced GD mechanism to boost the multi-scale feature fusion capability. YOLOv9  proposes GELAN to improve the architecture and PGI to augment the training process.

**End-to-end object detectors.** End-to-end object detection has emerged as a paradigm shift from traditional pipelines, offering streamlined architectures . DETR  introduces the transformer architecture and adopts Hungarian loss to achieve one-to-one matching prediction, thereby eliminating hand-crafted components and post-processing. Since then, various DETR variants have been proposed to enhance its performance and efficiency [42; 67; 56; 30; 36; 28; 5; 77; 82]. Deformable-DETR  leverages multi-scale deformable attention module to accelerate the convergence speed. DINO  integrates contrastive denoising, mix query selection, and look forward twice scheme into DETRs. RT-DETR  further designs the efficient hybrid encoder and proposes the uncertainty-minimal query selection to improve both the accuracy and latency. Another line to achieve end-to-end object detection is based CNN detectors. Learnable NMS  and relation networks  present another network to remove duplicated predictions for detectors. OneNet  and DeFCN  propose one-to-one matching strategies to enable end-to-end object detection with fully convolutional networks. FCOS\({}_{}\) introduces a positive sample selector to choose the optimal sample for prediction.

## 3 Methodology

### Consistent Dual Assignments for NMS-free Training

During training, YOLOs [21; 65; 29; 70] usually leverage TAL  to allocate multiple positive samples for each instance. The adoption of one-to-many assignment yields plentiful supervisory signals, facilitating the optimization and achieving superior performance. However, it necessitates YOLOs to rely on the NMS post-processing, which causes the suboptimal inference efficiency for deployment. While previous works [55; 66; 80; 6] explore one-to-one matching to suppress the redundant predictions, they usually introduce additional inference overhead or yield suboptimal performance. In this work, we present a NMS-free training strategy for YOLOs with dual label assignments and consistent matching metric, achieving both high efficiency and competitive performance.

**Dual label assignments.** Unlike one-to-many assignment, one-to-one matching assigns only one prediction to each ground truth, avoiding the NMS post-processing. However, it leads to weak supervision, which causes suboptimal accuracy and convergence speed . Fortunately, this deficiency can be compensated for by the one-to-many assignment . To achieve this, we introduce dual label assignments for YOLOs to combine the best of both strategies. Specifically, as shown in Fig. 2.(a), we incorporate another one-to-one head for YOLOs. It retains the identical structure and adopts the same optimization objectives as the original one-to-many branch but leverages the one-to-one matching to obtain label assignments. During training, two heads are jointly optimized with the model, allowing the backbone and neck to enjoy the rich supervision provided by the one-to-many assignment. During inference, we discard the one-to-many head and utilize the one-to-one head to make predictions. This enables YOLOs for the end-to-end deployment without incurring any additional inference cost. Besides, in the one-to-one matching, we adopt the top one selection, which achieves the same performance as Hungarian matching  with less extra training time.

**Consistent matching metric.** During assignments, both one-to-one and one-to-many approaches leverage a metric to quantitatively assess the level of concordance between predictions and instances. To achieve prediction aware matching for both branches, we employ a uniform matching metric, _i.e_.,

\[m(,)=s p^{}(,b)^{},\] (1)

where \(p\) is the classification score, \(\) and \(b\) denote the bounding box of prediction and instance, respectively. \(s\) represents the spatial prior indicating whether the anchor point of prediction is within the instance . \(\) and \(\) are two important hyperparameters that balance the impact of the semantic prediction task and the location regression task. We denote the one-to-many and one-to-one metrics as \(m_{o2m}\)=\(m(_{o2m},_{o2m})\) and \(m_{o2o}\)=\(m(_{o2o},_{o2o})\), respectively. These metrics influence the label assignments and supervision information for the two heads.

In dual label assignments, the one-to-many branch provides much richer supervisory signals than one-to-one branch. Intuitively, if we can harmonize the supervision of the one-to-one head with that of one-to-many head, we can optimize the one-to-one head towards the direction of one-to-many head's optimization. As a result, the one-to-one head can provide improved quality of samples during inference, leading to better performance. To this end, we first analyze the supervision gap between the two heads. Due to the randomness during training, we initiate our examination in the beginning with two heads initialized with the same values and producing the same predictions, _i.e_., one-to-one head and one-to-many head generate the same \(p\) and IoU for each prediction-instance pair. We note that the regression targets of two branches do not conflict, as matched predictions share the same targets and unmatched predictions are ignored. The supervision gap thus lies in the different classification targets. Given an instance, we denote its largest IoU with predictions as \(u^{*}\), and the largest one-to-many and one-to-one matching scores as \(m^{*}_{o2m}\) and \(m^{*}_{o2o}\), respectively. Suppose that one-to-many branch yields the positive samples \(\) and one-to-one branch selects \(i\)-th prediction with the metric \(m_{o2o,i}\)=\(m^{*}_{o2o}\), we can then derive the classification target \(t_{o2m,j}\)=\(u^{*}}{m^{*}_{o2m}} u^{*}\) for \(j\) and \(t_{o2o,i}\)=\(u^{*}^{*}}{m^{*}_{o2o}}\)=\(u^{*}\) for task aligned loss as in . The supervision gap between two branches can thus be derived by the 1-Wasserstein distance  of different classification objectives, _i.e_.,

\[A=t_{o2o,i}-(i)t_{o2m,i}+_{k\{i\}}t_ {o2m,k},\] (2)

We can observe that the gap decreases as \(t_{o2m,i}\) increases, _i.e_., \(i\) ranks higher within \(\). It reaches the minimum when \(t_{o2m,i}\)=\(u^{*}\), _i.e_., \(i\) is the best positive sample in \(\), as shown in Fig. 2.(a). To achieve this, we present the consistent matching metric, _i.e_., \(_{o2o}\)=\(r_{o2m}\) and \(_{o2o}\)=\(r_{o2m}\), which implies \(m_{o2o}\)=\(m^{r}_{o2m}\). Therefore, the best positive sample for one-to-many head is also the best for one-to-one head. Consequently, both heads can be optimized consistently and harmoniously. For simplicity, we

Figure 2: (a) Consistent dual assignments for NMS-free training. (b) Frequency of one-to-one assignments in Top-1/5/10 of one-to-many results for YOLOv8-S which employs \(_{o2m}\)=0.5 and \(_{o2m}\)=6 by default . For consistency, \(_{o2o}\)=0.5; \(_{o2o}\)=6. For inconsistency, \(_{o2o}\)=0.5; \(_{o2o}\) =2.

take \(r\)=1, by default, _i.e._, \(_{o2o}\)=\(_{o2m}\) and \(_{o2o}\)=\(_{o2m}\). To verify the improved supervision alignment, we count the number of one-to-one matching pairs within the top-1 / 5 / 10 of the one-to-many results after training. As shown in Fig. 2.(b), the alignment is improved under the consistent matching metric. For a more comprehensive understanding of the mathematical proof, please refer to the appendix.

**Discussion with other counter-parts.** Similarly, previous works [28; 5; 77; 54; 6; 82; 45] explore the different assignments to accelerate the training convergence and improve the performance for different networks. For example, H-DETR , Group-DETR , and MS-DETR  introduce one-to-many matching in conjunction with the original one-to-one matching by hybrid or multiple group label assignments, to improve upon DETR. Differently, to achieve the one-to-many matching, they usually introduce extra queries or repeat ground truths for bipartite matching, or select top several queries from the matching scores, while we adopt the prediction aware assignment that incorporates the spatial prior. Besides, LRANet  employs the dense assignment and sparse assignment branches for training, which all belong to the one-to-many assignment, while we adopt the one-to-many and one-to-one branches. DEYO [45; 43; 44] investigates the step-by-step training with one-to-many matching in the first stage for convolutional encoder and one-to-one matching in the second stage for transformer decoder, while we avoid the transformer decoder for end-to-end inference. Compared with works [6; 80] which incorporate dual assignments for CNN-based detectors, we further analyze the supervision gap between the two heads and present the consistent matching metric for YOLOs to reduce the theoretical supervision gap. It improves performance through better supervision alignment and eliminates the need for hyper-parameter tuning.

### Holistic Efficiency-Accuracy Driven Model Design

In addition to the post-processing, the model architectures of YOLOs also pose great challenges to the efficiency-accuracy trade-offs [50; 8; 29]. Although previous works explore various design strategies, the comprehensive inspection for various components in YOLOs is still lacking. Consequently, the model architecture exhibits non-negligible computational redundancy and constrained capability, which impedes its potential for achieving high efficiency and performance. Here, we aim to holistically perform model designs for YOLOs from both efficiency and accuracy perspectives.

**Efficiency driven model design.** The components in YOLO consist of the stem, downsampling layers, stages with basic building blocks, and the head. The stem incurs few computational cost and we thus perform efficiency driven model design for other three parts.

_(1) Lightweight classification head._ The classification and regression heads usually share the same architecture in YOLOs. However, they exhibit notable disparities in computational overhead. For example, the FLOPs and parameter count of the classification head (5.95G/1.51M) are 2.5\(\) and 2.4\(\) those of the regression head (2.34G/0.64M) in YOLOv8-S, respectively. However, after analyzing the impact of classification error and the regression error (seeing Tab. 6), we find that the regression head undertakes more significance for the performance of YOLOs. Consequently, we can reduce the overhead of classification head without worrying about hurting the performance greatly. Therefore, we simply adopt a lightweight architecture for the classification head, which consists of two depthwise separable convolutions [25; 9] with the kernel size of 3\(\)3 followed by a 1\(\)1 convolution.

_(2) Spatial-channel decoupled downsampling._ YOLOs typically leverage regular 3\(\)3 standard convolutions with stride of 2, achieving spatial downsampling (from \(H W\) to \(\)) and channel transformation (from \(C\) to \(2C\)) simultaneously. This introduces non-negligible computational cost of \((HWC^{2})\) and parameter count of \((18C^{2})\). Instead, we propose to decouple the spatial reduction and channel increase operations, enabling more efficient downsampling. Specifically, we firstly leverage the pointwise convolution to modulate the channel dimension and then utilize the depthwise convolution to perform spatial downsampling. This reduces the computational cost to \((2HWC^{2}+HWC)\) and the parameter count to \((2C^{2}+18C)\). Meanwhile, it maximizes information retention during downsampling, leading to competitive performance with latency reduction.

_(3) Rank-guided block design._ YOLOs usually employ the same basic building block for all stages [29; 65], _e.g._, the bottleneck block in YOLOv8 . To thoroughly examine such homogeneous design for YOLOs, we utilize the intrinsic rank [33; 16] to analyze the redundancy2 of each stage. Specifically, we calculate the numerical rank of the last convolution in the last basic block in each stage, which counts the number of singular values larger than a threshold. Fig. 3.(a) presents the results of YOLOv8, indicating that deep stages and large models are prone to exhibit more redundancy. This observation suggests that simply applying the same block design for all stages is suboptimal for the best capacity-efficiency trade-off. To tackle this, we propose a rank-guided block design scheme which aims to decrease the complexity of stages that are shown to be redundant using compact architecture design. We first present a compact inverted block (CIB) structure, which adopts the cheap depthwise convolutions for spatial mixing and cost-effective pointwise convolutions for channel mixing, as shown in Fig. 3.(b). It can serve as the efficient basic building block, _e.g._, embedded in the ELAN structure  (Fig. 3.(b)). Then, we advocate a rank-guided block allocation strategy to achieve the best efficiency while maintaining competitive capacity. Specifically, given a model, we sort its all stages based on their intrinsic ranks in ascending order. We further inspect the performance variation of replacing the basic block in the leading stage with CIB. If there is no performance degradation compared with the given model, we proceed with the replacement of the next stage and halt the process otherwise. Consequently, we can implement adaptive compact block designs across stages and model scales, achieving higher efficiency without compromising performance. Due to the page limit, we provide the details of the algorithm in the appendix.

**Accuracy driven model design.** We further explore the large-kernel convolution and self-attention for accuracy driven design, aiming to boost the performance under minimal cost.

_(1) Large-kernel convolution._ Employing large-kernel depthwise convolution is an effective way to enlarge the receptive field and enhance the model's capability . However, simply leveraging them in all stages may introduce contamination in shallow features used for detecting small objects, while also introducing significant I/O overhead and latency in high-resolution stages . Therefore, we propose to leverage the large-kernel depthwise convolutions in CIB within the deep stages. Specifically, we increase the kernel size of the second 3\(\)3 depthwise convolution in the CIB to 7\(\)7, following . Additionally, we employ the structural reparameterization technique  to bring another 3\(\)3 depthwise convolution branch to alleviate the optimization issue without inference overhead. Furthermore, as the model size increases, its receptive field naturally expands, with the benefit of using large-kernel convolutions diminishing. Therefore, we only adopt large-kernel convolution for small model scales.

_(2) Partial self-attention (PSA)._ Self-attention  is widely employed in various visual tasks due to its remarkable global modeling capability . However, it exhibits high computational complexity and memory footprint. To address this, in light of the prevalent attention head redundancy , we present an efficient partial self-attention (PSA) module design, as shown in Fig. 3.(c). Specifically, we evenly partition the features across channels into two parts after the 1\(\)1 convolution. We only feed one part into the \(N_{}\) blocks comprised of multi-head self-attention module (MHSA) and feed-forward network (FFN). Two parts are then concatenated and fused by a 1\(\)1 convolution. Besides, we follow  to assign the dimensions of the query and key to half of that of the value in MHSA and replace the LayerNorm with BatchNorm for fast inference. Furthermore, PSA is only placed after the Stage 4 with the lowest resolution, avoiding the excessive overhead from the

Figure 3: (a) The intrinsic ranks across stages and models in YOLOv8. The stage in the backbone and neck is numbered in the order of model forward process. The numerical rank \(r\) is normalized to \(r/C_{o}\) for y-axis and its threshold is set to \(_{max}/2\), by default, where \(C_{o}\) denotes the number of output channels and \(_{max}\) is the largest singular value. It can be observed that deep stages and large models exhibit lower intrinsic rank values. (b) The compact inverted block (CIB). (c) The partial self-attention module (PSA).

quadratic computational complexity of self-attention. In this way, the global representation learning ability can be incorporated into YOLOs with low computational costs, which well enhances the model's capability and leads to improved performance.

## 4 Experiments

### Implementation Details

We select YOLOv8  as our baseline model, due to its commendable latency-accuracy balance and its availability in various model sizes. We employ the consistent dual assignments for NMS-free training and perform holistic efficiency-accuracy driven model design based on it, which brings our YOLOv10 models. YOLOv10 has the same variants as YOLOv8, _i.e._, N / S / M / L / X. Besides, we derive a new variant YOLOv10-B, by simply increasing the width scale factor of YOLOv10-M. We verify the proposed detector on COCO  under the same training-from-scratch setting . Moreover, the latencies of all models are tested on T4 GPU with TensorRT FP16, following .

### Comparison with state-of-the-arts

As shown in Tab. 1, our YOLOv10 achieves the state-of-the-art performance and end-to-end latency across various model scales. We first compare YOLOv10 with our baseline models, _i.e._, YOLOv8. On N / S / M / L / X five variants, our YOLOv10 achieves 1.2% / 1.4% / 0.5% / 0.3% / 0.5% AP improvements, with 28% / 36% / 41% / 44% / 57% fewer parameters, 23% / 24% / 25% / 27% / 38% less calculations, and 70% / 65% / 50% / 41% / 37% lower latencies. Compared with other YOLOs,

   Model & \#Param.(M) & FLOPs(G) & AP\({}^{val}\)(\%) & Latency(ms) & Latency\({}^{f}\)(ms) \\  YOLOv6-3.0-N  & 4.7 & 11.4 & 37.0 & 2.69 & 1.76 \\ Gold-YOLO-N  & 5.6 & 12.1 & 39.6 & 2.92 & 1.82 \\ YOLOv8-N  & 3.2 & 8.7 & 37.3 & 6.16 & 1.77 \\
**YOLOv10-N (Ours)** & **2.3** & **6.7** & **38.5**/ **39.5\({}^{}\)** & **1.84** & **1.79** \\  YOLOv6-3.0-S  & 18.5 & 45.3 & 44.3 & 3.42 & 2.35 \\ Gold-YOLO-S  & 21.5 & 46.0 & 45.4 & 3.82 & 2.73 \\ YOLO-MS-XS  & 4.5 & 17.4 & 43.4 & 8.23 & 2.80 \\ YOLO-MS-S  & 8.1 & 31.2 & 46.2 & 10.12 & 4.83 \\ YOLOv8-S  & 11.2 & 28.6 & 44.9 & 7.07 & 2.33 \\ YOLOv9-S  & 7.1 & 26.4 & 46.7 & - & - \\ RT-DETR-R18  & 20.0 & 60.0 & 46.5 & 4.58 & 4.49 \\
**YOLOv10-S (Ours)** & **7.2** & **21.6** & **46.3**/ **46.8\({}^{}\)** & **2.49** & **2.39** \\  YOLOv6-3.0-M  & 34.9 & 85.8 & 49.1 & 5.63 & 4.56 \\ Gold-YOLO-M  & 41.3 & 87.5 & 49.8 & 6.38 & 5.45 \\ YOLO-MS  & 22.2 & 80.2 & 51.0 & 12.41 & 7.30 \\ YOLOv8-M  & 25.9 & 78.9 & 50.6 & 9.50 & 5.09 \\ YOLOv9-M  & 20.0 & 76.3 & 51.1 & - & - \\ RT-DETR-R34  & 31.0 & 92.0 & 48.9 & 6.32 & 6.21 \\ RT-DETR-R50m  & 36.0 & 100.0 & 51.3 & 6.90 & 6.84 \\
**YOLOv10-M (Ours)** & **15.4** & **59.1** & **51.1**/ **51.3\({}^{}\)** & **4.74** & **4.63** \\  YOLOv6-3.0-L  & 59.6 & 150.7 & 51.8 & 9.02 & 7.90 \\ Gold-YOLO-L- & 75.1 & 151.7 & 51.8 & 10.65 & 9.78 \\ YOLOv9-C  & 25.3 & 102.1 & 52.5 & 10.57 & 6.13 \\
**YOLOv10-B (Ours)** & **19.1** & **92.0** & **52.5**/ **52.7\({}^{}\)** & **5.74** & **5.67** \\  YOLOv8-L  & 43.7 & 165.2 & 52.9 & 12.39 & 8.06 \\ RT-DETR-R50  & 42.0 & 136.0 & 53.1 & 9.20 & 9.07 \\
**YOLOv10-L (Ours)** & **24.4** & **120.3** & **53.2**/ **53.4\({}^{}\)** & **7.28** & **7.21** \\  YOLOv8-X  & 68.2 & 257.8 & 53.9 & 16.86 & 12.83 \\ RT-DETR-R101  & 76.0 & 259.0 & 54.3 & 13.71 & 13.58 \\
**YOLOv10-X (Ours)** & **29.5** & **160.4** & **54.4**/ **54.4\({}^{}\)** & **10.70** & **10.60** \\   

Table 1: Comparisons with state-of-the-arts. Latency is measured using official pre-trained models. Latency\({}^{f}\) denotes the latency in the forward process of model without post-processing. \({}\) means the results of YOLOv10 with the original one-to-many training using NMS. All results below are without the additional advanced training techniques like knowledge distillation or PGI for fair comparisons.

YOLOv10 also exhibits superior trade-offs between accuracy and computational cost. Specifically, for lightweight and small models, YOLOv10-N / S outperforms YOLOv6-3.0-N / S by 1.5 AP and 2.0 AP, with 51% / 61% fewer parameters and 41% / 52% less computations, respectively. For medium models, compared with YOLOv9-C / YOLO-MS, YOLOv10-B / M enjoys the 46% / 62% latency reduction under the same or better performance, respectively. For large models, compared with Gold-YOLO-L, our YOLOv10-L shows 68% fewer parameters and 32% lower latency, along with a significant improvement of 1.4% AP. Furthermore, compared with RT-DETR, YOLOv10 obtains significant performance and latency improvements. Notably, YOLOv10-S / X achieves 1.8\(\) and 1.3\(\) faster inference speed than RT-DETR-R18 / R101, respectively, under the similar performance. These results well demonstrate the superiority of YOLOv10 as the real-time end-to-end detector.

We also compare YOLOv10 with other YOLOs using the original one-to-many training approach. We consider the performance and the latency of model forward process (Latency\({}^{f}\)) in this situation, following [62; 21; 60]. As shown in Tab. 1, YOLOv10 also exhibits the state-of-the-art performance and efficiency across different model scales, indicating the effectiveness of our architectural designs.

### Model Analyses

**Ablation study.** We present the ablation results based on YOLOv10-S and YOLOv10-M in Tab. 2. It can be observed that our NMS-free training with consistent dual assignments significantly reduces the end-to-end latency of YOLOv10-S by 4.63ms, while maintaining competitive performance of 44.3% AP. Moreover, our efficiency driven model design leads to the reduction of 11.8 M parameters and 20.8 GFIOPs, with a considerable latency reduction of 0.65ms for YOLOv10-M, well showing its effectiveness. Furthermore, our accuracy driven model design achieves the notable improvements of 1.8 AP and 0.7 AP for YOLOv10-S and YOLOv10-M, alone with only 0.18ms and 0.17ms latency overhead, respectively, which well demonstrates its superiority.

**Analyses for NMS-free training.**

* _Dual label assignments._ We present dual label assignments for NMS-free YOLOs, which can bring both rich supervision of one-to-many (o2m) branch during training and high efficiency of one-to-one (o2o) branch during inference. We verify its benefit based on YOLOv8-S, _i.e._, #1 in Tab. 2. Specifically, we introduce baselines for training with only o2m branch and o2o branch, respectively. As shown in Tab. 3, our dual label assignments achieve the best AP-latency trade-off.
* _Consistent matching metric._ We introduce consistent matching metric to make the one-to-one head more harmonious with the one-to-many head. We verify its benefit based on YOLOv8-S, _i.e._, #1 in Tab. 2, under different \(_{o2o}\) and \(_{o2o}\). As shown in Tab. 4, the proposed consistent matching metric, _i.e._, \(_{o2o}\)=\(r_{o2m}\) and \(_{o2o}\)=\(r_{o2m}\), can achieve the optimal performance, where \(_{o2m}\)=\(0.5\) and \(_{o2m}\)=\(6.0\) in the one-to-many head . Such an improvement can be attributed to the reduction of the supervision gap (Eq. (2)), which provides improved supervision alignment between two branches. Moreover, the proposed consistent matching metric eliminates the need for exhaustive hyper-parameter tuning, which is appealing in practical scenarios.

[MISSING_PAGE_FAIL:9]

brings 0.5% AP improvement. Compared with "IRB-DW", our CIB further achieves 0.3% AP improvement by prepending another DW with minimal overhead, indicating its superiority.
* _Rank-guided block design._ We introduce the rank-guided block design to adaptively integrate compact block design for improving the model efficiency. We verify its benefit based on the YOLOv10-S of #3 in the Tab. 5. The stages sorted in ascending order based on the intrinsic ranks are Stage 8-4-7-3-5-1-6-2, like in Fig. 3.(a). As shown in Tab. 9, when gradually replacing the bottleneck block in each stage with the efficient CIB, we observe the performance degradation starting from Stage 7. In the Stage 8 and 4 with lower intrinsic ranks and more redundancy, we can thus adopt the efficient block design without compromising the performance. These results indicate that rank-guided block design can serve as an effective strategy for higher model efficiency.

**Analyses for accuracy driven model design.** We present the results of gradually integrating the accuracy driven design elements based on YOLOv10-S/M. Our baseline is the YOLOv10-S/M model after incorporating efficiency driven design, _i.e_., #3/#7 in Tab. 2. As shown in Tab. 10, the adoption of large-kernel convolution and PSA module leads to the considerable performance improvements of 0.4% AP and 1.4% AP for YOLOv10-S under minimal latency increase of 0.03ms and 0.15ms, respectively. Note that large-kernel convolution is not employed for YOLOv10-M (see Tab. 12).

* _Large-kernel convolution._ We first investigate the effect of different kernel sizes based on the YOLOv10-S of #2 in Tab. 10. As shown in Tab. 11, the performance improves as the kernel size increases and stagnates around the kernel size of 7\(\)7, indicating the benefit of large perception field. Besides, removing the reparameterization branch during training achieves 0.1% AP degradation, showing its effectiveness for optimization. Moreover, we inspect the benefit of large-kernel convolution across model scales based on YOLOv10-N / S / M. As shown in Tab. 12, it brings no improvements for large models, _i.e_., YOLOv10-M, due to its inherent extensive receptive field. We thus only adopt large-kernel convolutions for small models, _i.e_., YOLOv10-N / S.
* _Partial self-attention (PSA)._ We introduce PSA to enhance the performance by incorporating the global modeling ability under minimal cost. We first verify its effectiveness based on the YOLOv10-S of #3 in Tab. 10. Specifically, we introduce the transformer block, _i.e_., MHSA followed by FFN, as the baseline, denoted as "Trans.". As shown in Tab. 13, compared with it, PSA brings 0.3% AP improvement with 0.05ms latency reduction. The performance enhancement may be attributed to the alleviation of optimization problem [68; 10] in self-attention, by mitigating the redundancy in attention heads. Moreover, we investigate the impact of different \(N_{}\). As shown in Tab. 13, increasing \(N_{}\) to 2 obtains 0.2% AP improvement but with 0.1ms latency overhead. Therefore, we set \(N_{}\) to 1, by default, to enhance the model capability while maintaining high efficiency.

## 5 Conclusion

In this paper, we target both the post-processing and model architecture throughout the detection pipeline of YOLOs. For the post-processing, we propose the consistent dual assignments for NMS-free training, achieving efficient end-to-end detection. For the model architecture, we introduce the holistic efficiency-accuracy driven model design strategy, improving the performance-efficiency trade-offs. These bring our YOLOv10, a new real-time end-to-end object detector. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and latency compared with other advanced detectors, well demonstrating its superiority.

## 6 Acknowledgments

This work was supported by National Natural Science Foundation of China (Nos. 61925107, 62271281) and Beijing Natural Science Foundation (No. L223023).

   \# Model & AP\({}^{val}\) Latency \\ 
1 base. & 44.5/50.4 & 2.31/4.57 \\ 
2 +L.k. & 44.9/- & 2.34/- \\
3 +PSA & 46.3/51.1 & 2.49/4.74 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.34 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
    Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.34 \\  w/o rep. & 44.8 & 2.34 \\    
    Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.34 \\  w/o rep. & 44.8 & 2.34 \\    
    Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
    Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.34 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.34 \\  w/o rep. & 44.8 & 2.34 \\    
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=7 & 44.9 & 2.34 \\
1 k.s.=9 & 44.9 & 2.37 \\  w/o rep. & 44.8 & 2.34 \\     
   Model & AP\({}^{val}\) Latency \\ 
1 k.s.=5 & 44.7 & 2.32 \\ 
1 k.s.=8 & 44.9 & 2.34 \\
1 k.s