# Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization

Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization

 Clement Benard\({}^{1}\) Brian Staber\({}^{1}\) Sebastien Da Veiga\({}^{2}\)

\({}^{1}\) Safran Tech, Digital Sciences & Technologies, 78114 Magny-Les-Hameaux, France

\({}^{2}\) ENSAI, CREST, F-35000 Rennes, France

{clement.benard, brian.staber}@{safrangroup.com}

sebastien.da-veiga@ensai.fr

###### Abstract

Stein thinning is a promising algorithm proposed by Riabiz et al. (2022) for post-processing outputs of Markov chain Monte Carlo (MCMC). The main principle is to greedily minimize the kernelized Stein discrepancy (KSD), which only requires the gradient of the log-target distribution, and is thus well-suited for Bayesian inference. The main advantages of Stein thinning are the automatic remove of the burn-in period, the correction of the bias introduced by recent MCMC algorithms, and the asymptotic properties of convergence towards the target distribution. Nevertheless, Stein thinning suffers from several empirical pathologies, which may result in poor approximations, as observed in the literature. In this article, we conduct a theoretical analysis of these pathologies, to clearly identify the mechanisms at stake, and suggest improved strategies. Then, we introduce the regularized Stein thinning algorithm to alleviate the identified pathologies. Finally, theoretical guarantees and extensive experiments show the high efficiency of the proposed algorithm. An implementation of regularized Stein thinning as the kernax library in python and JAX is available at https://gitlab.com/drti/kernax.

## 1 Introduction

Bayesian inference is a powerful approach to solve statistical tasks, and is especially efficient to incorporate prior expert knowledge of the studied system, or to provide uncertainties of the estimated quantities. Bayesian methods have thus demonstrated a high empirical performance for a wide range of applications, in particular in the fields of physics and computational biology, to just name a few. However, the Bayesian framework often leads to the evaluation of expectations with respect to a posterior distribution, which is not tractable (Green et al., 2015), except in the specific case of conjugate prior distribution and likelihood, which hardly occurs in practice. To overcome this issue, Markov chain Monte Carlo (MCMC) is one of the most commonly used computational methods to estimate these integrals. Indeed, MCMC algorithms iteratively generate a sample, which follows the targeted posterior distribution, as the Markov chain converges to its stationary state (Robert and Casella, 1999; Brooks et al., 2011). Consequently, the quality of the resulting estimates strongly depends on the convergence of the MCMC and how its output is post-processed. Standard post-processing procedures of MCMC outputs consist in removing the first iterations, called the burn-in period, and thinning the Markov chain with a constant frequency. Burn-in removal aims at reducing the bias introduced by the random initialization of the Markov chain. The \(\) convergence diagnosis of Gelman et al. (1995) is, for instance, a well known method for determining the burn-in period. On the other hand, thinning the Markov chain allows for compressing the MCMC output and may also reduce the correlation between the iteratively selected points. More recently, promising kernel-basedprocedures were proposed to automatically remove the burn-in period, compress the output, and reduce the asymptotic bias (South et al., 2022). These approaches consist in minimizing a kernel-based discrepancy measure \(D(,_{m})\) between the empirical distribution \(_{m}\) of a subsample of the MCMC output of size \(m\), and the target distribution \(\). In this respect, minimization of the maximum mean discrepancy (MMD) was investigated by several authors, but these strategies require the full knowledge of the target distribution \(\), whose density is not tractable in non-conjugate Bayesian inference.

Based on the previous works of Chen et al. (2018) and Chen et al. (2019); Riabiz et al. (2022) propose to minimize the kernelized Stein discrepancy (KSD), to design an efficient kernel-based algorithm to thin MCMC outputs in a non-tractable Bayesian setting. The KSD (Liu et al., 2016) is a score-based discrepancy measure, _i.e._, it only requires the knowledge of the score function of the target \(\), which is readily available in our Bayesian framework. Importantly, Gorham and Mackey (2017) showed that under suitable mild conditions, the KSD enjoys good convergence properties. More precisely, the KSD is a valid distance to detect samples drawn form the target distribution, provided that the sample size is large enough. Therefore, KSD thinning is a highly promising tool for post-processing and measuring the quality of MCMC outputs. This article thus focuses on the Stein thinning algorithm proposed by Riabiz et al. (2022), which consists in selecting \(m\) points amongst the \(n\) iterations of the MCMC output, by greedily minimizing the KSD distance. Thanks to the convergence properties of the KSD, the empirical measure of the selected points weakly converges towards the posterior law \(\). However, on the practical side, several articles (Wenliang and Kanagawa, 2020; Korba et al., 2021) have noticed empirical limitations of KSD-based sampling algorithms, especially for multimodal target distributions. In fact, these limitations happen to be quite problematic, even in simple experiments, and have been slightly overlooked in the literature so far, in our opinion. Therefore, this article first focuses on the analysis of KSD pathologies in Section 2, taking both an empirical and theoretical point of view. Then, we propose strategies to mitigate the identified problems, and introduce the regularized Stein thinning in Section 3. We show the efficiency of our algorithm through both a theoretical analysis and extensive experiments in Section 4. Notice that proofs and additional experiments are gathered in Appendices \(1\)-\(7\) in the Supplementary Material. In the remaining of this initial section, we mathematically formalize the KSD distance and the associated Stein thinning algorithm.

Kernelized Stein discrepancy.Kernelized Stein discrepancy was independently introduced by Chwialkowski et al. (2016); Liu et al. (2016); Gorham and Mackey (2017) as a promising tool for measuring dissimilarities between two distributions \(\) and \(\) on \(^{d}\) with \(d 1\), whenever \(\) admits a continuously differentiable density \(p\), and the normalization constant of \(p\) is not tractable. Let \(k:^{d}^{d}\) be a positive semi-definite kernel and let \((k)\) be the associated reproducing kernel Hilbert space (RKHS) with inner product \(,_{(k)}\) and norm \(\|\|_{(k)}\). Kernelized Stein discrepancy belongs to the family of maximum mean discrepancies (MMD) (Gretton et al., 2006) defined as

\[_{k}(,)=_{\|f\|_{(k)} 1} |[f()]-[f()]|,\] (1)

where \(\), \(\). If the kernel \(k\) is characteristic, then the MMD is a distance between probability distributions. In practice, the MMD may not be computable as it involves mathematical expectations with respect to \(\), whose density is not tractable. To circumvent this issue, Gorham and Mackey (2015) proposed the Stein discrepancy which relies on Stein's method (Stein, 1972). It consists in defining an operator \(_{p}\) that maps functions \(g:^{d}^{d}\) to real-valued functions such that \([_{p}g()]=0\), with \(\), for all \(g\) in \((k)=\{g:^{d}^{d}:_{i=1}^{} \|g_{i}\|_{(k)}^{2} 1\}\). The probability measure \(\) on \(^{d}\) is assumed to admit a continuously differentiable Lebesgue density \(p^{1}(^{d})\), such that \([\| p()\|_{2}^{2}]\). The Stein discrepancy is then defined as \((,)=_{g(k)}|[ (_{p}g)()]|\), where \(\). If the Stein operator \(_{p}\) is chosen as the Langevin operator \((_{p}g)()= g(), p() +,g()\), then Stein's discrepancy has a closed-form expression known as kernelized Stein discrepancy (Chwialkowski et al., 2016; Liu et al., 2016), \(^{2}(,)=[k_{p}(,^ {})]\), where \(,^{}\), and \(k_{p}\) denotes the Langevin Stein kernel defined from the score function \(s_{p}()= p()\) for \(,^{}^{d}\), as

\[k_{p}(,^{})= _{},_{^{}}k(,^{})+ s_{p}(),_{^{ }}k(,^{})\] \[+ s_{p}(^{}),_{}k( ,^{})+ s_{p}(),s_{p}(^{}) k(,^{})\,.\] (2)

The main advantage of the KSD is that it only requires the knowledge of the score function, and does not involve any integration with respect to \(\). Gorham and Mackey (2017) also established convergence guaranties when the kernel \(k\) is chosen as the inverse multi-quadratic (IMQ) kernel function \(k(,^{})=(c+\|-^{}\|_{}^ {2})^{-}\) with \(c>0\), \((0,1)\), the positive definite matrix \(\) is the identity matrix, and the density \(p\) is distantly dissipative as defined below. Log-concave distributions outside of a compact set are a typical example of such probability densities.

**Definition 1.1** (Distant dissipativity Gorham and Mackey (2017)).: The density \(p^{1}(^{d})\) is distantly dissipative if \(_{r}(r)>0\), where \((r)=-2_{p}()-_{p} (),-}{\|-\|_{2}^{2}} :\|-\|_{2}=r}\).

Stein thinning algorithm.Let \(\) be a target probability measure that admits density \(p\), and let \(\{_{i}\}_{i=1}^{n}^{d}\) be a MCMC output. The Stein thinning algorithm (Riabiz et al., 2022) selects \(m n\) particles \(_{_{1}},,_{_{m}}\) by greedily minimizing the kernelized Stein discrepancy. Given \(t-1<m\) particles \(_{_{1}},,_{_{t-1}}\), the \(t\)-th particle is defined as

\[_{t}*{argmin}_{i\{1,,n\}}k_{p}(_{i}, _{i})+2_{j=1}^{t-1}k_{p}(_{_{j}},_{i})\,,\]

where the KSD of an empirical distribution has been used to simplify the objective function. The kernel function \(k\) is usually chosen as the IMQ kernel function, defined above, for both its good theoretical properties and empirical efficiency. Indeed, several articles (Chen et al., 2018; Riabiz et al., 2022) have led extensive experiments to show the better practical performance of the IMQ kernel over other choices. Also notice that the bandwidth parameter \(\) is quite influential on the algorithm performance, but happens to be very difficult to tune, as highlighted by Chopin and Ducrocq (2021). Indeed, since the normalization constant of the target distribution is unknown, no additional metric is available to assess the precise performance of the thinning procedure when \(\) varies. Furthermore, the sample quality output by Stein thinning varies in an erratic fashion with respect to \(\), making the design of heuristic procedures for the choice of \(\) notoriously difficult. Following the literature recommendations (Riabiz et al., 2022), we use the median heuristic to set \(\) in our experiments, and refer to Garreau et al. (2017) for an extensive analysis of this approach for kernel methods.

## 2 Analysis of KSD Pathologies

Although kernelized Stein discrepancy is a highly promising approach to thin MCMC outputs, several empirical studies have highlighted that KSD-based algorithms may suffer from strong pathologies in simple experiments (Wenliang and Kanagawa, 2020; Korba et al., 2021; Riabiz et al., 2022; Liu et al., 2023). The most established KSD pathology is that Stein thinning ignores the weights of distant modes of the target distribution, leading to the selection of samples of poor quality by Stein thinning. This problem, called Pathology I throughout the article, is analyzed in Subsection 2.1. Additionally, Korba et al. (2021) also notice that KSD thinning may result in samples concentrated in regions of low probability of \(p\). As opposed to Pathology I, the mechanism leading to this problematic behavior is not well understood in the literature, to our best knowledge. Subsection 2.2 is thus dedicated to the theoretical characterization and illustration of Pathology II. Throughout the article, we illustrate KSD thinning using the running example of a Gaussian mixture, defined in Example 1 below, where initial particles are directly sampled from \(p\) to better highlight pathologies. We will come back to the thinning of MCMC outputs in detail in Section 4.

**Example 1**.: Let the density \(p\) be a Gaussian mixture model of two components, respectively centered in \((-,_{d-1})\) and \((,_{d-1})\), of weights \(w\) and \(1-w\), and of variance \(^{2}_{}\). The initial particles \(\{_{i}\}_{i=1}^{n}\) are drawn from \(p\). The KSD thinning algorithm selects \(m<n\) points to approximate \(p\).

### Pathology I: mode proportion blindness

We first focus on Pathology I, which states that Stein thinning is blind to the relative weights of multiple distant modes of a target distribution. Indeed, Wenliang and Kanagawa (2020) show that the score \(s_{p}\) is insensitive to distant mode weights. Consequently, the KSD distance is unable to properly identify samples with different weights than those of the target, in finite sample settings, as long as samples are accurately distributed within each mode. To be more specific, we illustrate this pathology with our Example 1 of a Gaussian mixture in dimension \(2\). We set \(=3\) and \(=1\) to enforce the two modes to be well separated, and take an unbalanced proportion \(w=0.2\) for the left mode, and \(1-w=0.8\) for the right mode. We generate \(n=3000\) observations and select \(m=300\) particles with Stein thinning. Clearly, the red selected sample displayed in Figure 1 has wrong proportions, with about half of the particles in each mode, instead of the expected \(20-80\)%, reflected by the initial black particles sampled from \(p\). More precisely, over \(100\) repetitions of the Stein thinning algorithm, we obtain an average proportion of \(0.53\) particles in the left mode, with a standard deviation of \(0.08\) across the \(100\) runs.

Although Wenliang and Kanagawa (2020) clearly show that the KSD distance is insensitive to the mode weights in the specific case of Gaussian mixtures, the mechanism leading to the selection of about half of the particles in each mode by Stein thinning, as in Example 1, remains unexplained in the literature, to our best knowledge. Therefore, we conduct a theoretical analysis in the general case of any mixture distribution with two distant modes, stated in Assumption 2.1 below. For the sake of clarity, we only study the case of a number of modes of two, without loss of generality. Importantly, notice that a finite sample drawn from a distribution with distant modes, takes the form of clusters of particles around each mode, as illustrated in Figure 1. Then, Stein thinning selects particles among these clusters to approximate \(p\), and these particles define an empirical law of a density \(q\) with a compact support around each mode. Wenliang and Kanagawa (2020) explain that the score \(s_{p}\) is especially insensitive to the mode weights in these compact areas around modes, which is the root cause of the generation of samples with wrong proportions, as in Figure 1. Therefore, Assumption 2.1 below defines this observed setting, required to have Pathology I to occur, where density \(q\) has compact supports around each distant mode. Additionally, we also need to formalize Assumption 2.2, which tells that the distributions of the two modes of the mixture \(q\) have a close KSD distance with respect to the target \(p\). In particular, this assumption can be easily verified when both \(p\) and \(q\) have symmetric mode distributions, since the KSD distance is insensitive to the weights of \(p\).

**Assumption 2.1** (Distant bimodal mixture distributions).: Let \(p\) and \(q\) be two mixture distributions in \(^{d}\), made of two modes centered in \((-,_{d-1})\) and \((,_{d-1})\), with \(>0\). The distribution of each mode of \(p^{1}(^{d})\) has \(^{d}\) as support, whereas each mode distribution of \(q\) have a compact support, included in a ball of radius \(r>0\), with \(r<\). The left mode of \(p\) has weight \(w_{p} 1/2\), and the right mode has weight \(1-w_{p}\). Similarly, \(w\) and \(1-w\) are the mode weights of \(q\). Let \(_{L}\) and \(_{R}\) be the probability measures that respectively admit the density of the left and right modes of \(q\), and \(\) and \(_{w}\) be also the probability laws for \(p\) and \(q\).

**Assumption 2.2**.: For distant bimodal mixture distributions \(q\) and \(p\) satisfying Assumption 2.1, and for \((0,1)\), we have \(|^{2}(,_{L})/^{2}(,_{R})-1|<\).

**Theorem 2.3**.: _Let \(k_{p}\) be the Stein kernel associated with the radial kernel \(k(,^{})=(\|-^{}\|_{2 }/)\), where \(,^{}^{d}\), \(>0\), and \(^{2}()\), such that \((z) 0\), \(^{}(z) 0\), and \(^{}(z) 0\) for \(z\). Let \(p\) and \(q\) be two bimodal mixture distributions satisfying Assumptions 2.1 and 2.2, for any \((0,1)\). We define \(w^{}\) as the optimal mixture weight of \(q\) with respect to the KSD distance, i.e., \(w^{}=}\ (, _{w})\). Then, for \(\) large enough, we have \(|w^{}-|<\)._

Theorem 2.3, proved in Appendix B, states that the weight \(w^{}\) of the optimal mixture \(q\), which minimizes the KSD distance to the target \(p\), is close to \(1/2\) regardless of the true target weight \(w_{p}\), whenever the distributions of the two modes of the mixture \(q\) have a close KSD distance to \(p\), and provided that the two modes are distant. In particular, this is the case in the experiment of Example 1 and Figure 1, where the two modes are symmetric and well separated. Additionally, a more specific empirical illustration of Theorem 2.3 can be found in Appendix A.1. In Section 3, we will propose strategies improving Stein thinning to recover samples with accurate mode proportions.

### Pathology II: spurious minimum

The core of this section is dedicated to the theoretical characterization of Pathology II. We first need to introduce additional notations to formalize our analysis. We thus define \(_{s_{0}}\), the region of the input space where the score norm is lower than the threshold \(s_{0} 0\), formally

Figure 1: Illustration of Pathology I with the Gaussian mixture of Example 1 (\(d=2\), \(=3\), \(=1\), \(w=0.2\), \(n=3000\), \(m=300\)). Initial particles are in black, and the Stein thinning output is red.

\(^{d}:\|s_{p}()\|_{2} s_{0}\}\). We also introduce an independent and identically distributed (iid) sample \(_{1},,_{m}\) of \(\), with \(_{m}\) the associated empirical measure for a positive integer \(m\), and \(X^{(j)}\) the \(j\)-th component of \(\). Then, Theorem 2.4 below shows that samples concentrated in regions of the input space where the norm of the score is low, have smaller KSD than samples drawn from the true target distribution \(p\), for small sample sizes. Additionally, the score norm is low around stationary points of \(p\), including local minimum and saddle points, as shown in Corollary 2.5 below. However, samples concentrated at local minimum of \(p\) are bad approximations of the target distribution by definition. Therefore, pathological samples may be generated by Stein thinning, which minimizes the empirical KSD, and thus explains Pathology II observed by Korba et al. (2021), and shown in Figure 2. For the sake of clarity, we formalize our result for the IMQ kernel used in practice, and set \(c=1\) without loss of generality, since it is equivalent to tune \(c\) or \(\) in the Stein thinning algorithm.

**Theorem 2.4** (KSD spurious minimum).: _Let \(k_{p}\) be the Stein kernel associated with the IMQ kernel with \(>0\), \((0,1)\), and \(c=1\). Let \(\{_{i}\}_{i=1}^{m}_{s_{0}}=\{^{d}:\|s_{p}()\|_{2} s_{0}\}\) be a fixed set of points of empirical measure \(_{m}=_{i=1}^{m}(_{i})\), with \(s_{0} 0\) and \(m 2\). We have \(^{2},_{m}<[^{2},_{m}]\), if the score threshold \(s_{0}\) and the sample size \(m\) are small enough to satisfy \(m<1+[\|s_{p}()\|_{2}^{2}]-s_{0}^{2}/(2 d /^{2}+2 s_{0}/+s_{0}^{2})\)._

**Corollary 2.5** (Low KSD samples at density minimum).: _Let \(k_{p}\) be the Stein kernel associated with the IMQ kernel with \(>0\), \((0,1)\), and \(c=1\). Let \(p\) be a density with at least one local minimum or saddle point. For \(m 2\), if \(\{_{i}\}_{i=1}^{m}^{d}\) is a set of points, all located at local minimum or saddle points of \(p\), then we have \(^{2},_{m}<[^{2},_{m}]\), if \(m<1+}{2 d}[\|s_{p}()\|_{2}^{2}]\)._

The proofs of Theorem 2.4 and Corollary 2.5, reported in Appendix C, are built on the idea that the KSD of the empirical law of \(\{_{i}\}_{i=1}^{n}\), has a bias of the form \(_{i=1}^{m}\|s_{p}(_{i})\|_{2}^{2}/m^{2}\). Consequently, when \(m\) is small, the bias has a strong influence on KSD estimates, which favor samples concentrated in regions of low score norm, as stationary points of \(p\). This mechanism is illustrated in Figure 2 and Corollary 2.6 for Gaussian mixtures. In this case, Stein thinning aligns a large number of particles around the line of saddle points defined by \(x^{(1)}=0\), an area of low probability of the targeted mixture distribution, because of the variations of the score function, if the sample size \(m\) is small enough. From another perspective, for any sample size \(m\), it exists a Gaussian mixture with \(/\) large enough, such that Pathology II occurs. Therefore, Pathology II can appear for arbitrarily large samples \(m\), depending on the target distribution properties.

**Corollary 2.6** (KSD spurious minimum for Gaussian mixtures).: _Let \(k_{p}\) be the Stein kernel associated with the IMQ kernel with \(>0\), \((0,1)\), and \(c=1\). Let the density \(p\) be a Gaussian mixture model of two components with equal weights, respectively centered in \((-,_{d-1})\) and \((,_{d-1})\), of variance \(^{2}\), and let \(=/\). If \(>1\) and \(0 s_{0}<-1}-(+-1})/\), then for any \(\{_{i}\}_{i=1}^{m}_{s_{0}}\) of empirical measure \(_{m}\), we have (i) \(^{2},_{m}<[^{2},_{m}]\) if \(m\) and \(s_{0}\) satisfy \(m<1+[\|s_{p}()\|_{2}^{2}]-s_{0}^{2}}{2 d/^{2}+2  s_{0}/+s_{0}^{2}}\). (ii) there exists three disjoint intervals \(I_{-},I_{0},I_{}\), respectively centered around \(-\), \(0\), and \(\), such that \(x_{1}^{(1)},,x_{m}^{(1)} I_{-} I_{0} I_{}\)._

Figure 2: Illustration of Pathology II for the Gaussian mixture of Example 1 (\(d=2\), \(=2\), \(=1\), \(w=0.5\), \(n=3000\), \(m=300\)): many particles are selected around the line \(x^{(1)}=0\) (left panel), because of the squared first component of the score \(s_{p}()\) along \(x^{(1)}\) (for \(x^{(2)}=0\) in the right panel).

Regularized Stein Thinning

Stein thinning suffers from two main pathologies, analyzed in Section 2. In a word, Pathology I comes from the insensitivity of the score to the relative weights of distant modes, whereas Pathology II originates from the variations of the score norm, which do not differentiate local minimum from local maximum of the target distribution. We propose to regularize the KSD distance to fix these two problems, using terms that are highly sensitive to the type of stationary point and the relative weights of modes. The proposed algorithm is first introduced in Subsection 3.1, then theoretical properties are discussed in Subsection 3.2, and finally the good empirical performance will be shown in Section 4.

### Algorithm

Entropic regularization.In order to compensate the blindness of the KSD to mode proportions in multimodal distributions, we introduce the following entropic regularized KSD, denoted by \(_{}\), and defined as \(_{}^{2}(,)=[k_{p}(,^{})]-[(p())]\), where \(\) and \(^{}\) have probability law \(\), and \(\) admits the density \(p\). In our Bayesian setting, \([(p())]\) is known up to an additive constant since the normalization factor of \(p\) is not tractable. However, it is possible to use \(_{}^{2}(,)\) as the objective function of the Stein thinning algorithm, as the greedy selection of particles to optimize this quantity does not rely on the unknown additive constant. The main idea of this entropic regularization is that \(-(p())\) takes higher values in modes of smaller probability, and therefore provides the relative mode weight information, which is missing in the KSD distance. More precisely, modes with smaller weights take smaller density values, and are therefore more penalized than modes of higher weights. Therefore, with such entropic penalization, regularized Stein thinning tends to select particles in modes of higher weights more frequently than in modes of smaller weights, and we recover appropriate proportions.

Laplacian correction.Chen et al. (2018) and Riabiz et al. (2022) have noticed that the term \(k_{p}(_{i},_{i})\), which naturally appears in the empirical kernelized Stein discrepancy with the Langevin operator, can be interpreted as a regularization term. For example, Stein thinning does not select particles in the burn-in period of an MCMC output thanks to this regularization. However, this term \(k_{p}(_{i},_{i})\) is also responsible for Pathology II, of samples concentrated in stationary points of \(p\), as shown in Theorem 2.4. Therefore, we add a second regularization term to compensate the weaknesses of \(k_{p}(_{i},_{i})\), by penalizing particles located at local minimum and saddle points of the density \(p\). Such points are located in areas of convexity of the target distribution, which can thus be detected with the positive values of the Laplacian of the density. Therefore, using the truncated Laplacian operator \(^{+}f()=_{j=1}^{d}^{2}f()/  x^{(j)2}^{+}\) for a function \(f^{2}(^{d})\), we propose the L-KSD estimate with a Laplacian correction for densities \(p^{2}(^{d})\), defined by

\[^{2}(,_{m})=}_{i j}^{m}k _{p}(_{i},_{j})+}_{i=1}^{m}k_{p}( _{i},_{i})+^{+}(p(_{i})).\]

Regularized Stein thinning.Overall, we obtain the following estimate for the entropic regularized KSD with Laplacian correction \(_{}^{2}(,_{m})=^{2}(,_{m})-_{i=1}^{m}(p(_{i}))\). Then, at each iteration \(t\{1,,m\}\), the regularized Stein thinning selects the particle index \(_{t}\{1,,n\}\) to greedily minimize

\[k_{p}(_{_{t}},_{_{t}})+^{+}(p(_{ _{t}}))- t(p(_{_{t}}))+2_{j=1}^{t-1}k_{p}( _{_{j}},_{_{t}}).\]

Finally, Figure 3 illustrates the performance of regularized Stein thinning to fix the two pathologies analyzed in Section 2, in the case of Example 1 with Gaussian mixtures. Indeed, the top panel of Figure 3 shows that the majority of particles are selected in the right mode, as expected from the target distribution with \(w=0.2\). More precisely, an average proportion of \(0.11\) of the particles are located in the left mode over \(100\) repetitions of the procedure (\(0.89\) in the right mode), with a standard deviation of \(0.03\). For the value choice of \(\), we refer to the next subsection and the experimental Section 4. On the bottom panel of Figure 3, we observe that no particle is now selected on the line \(x^{(1)}=0\), as expected from the target Gaussian mixture distribution.

**Remark 3.1**.: The truncated Laplacian operator is simply given by the trace of the Hessian matrix, where negative components are set to \(0\). It follows that the computational cost the regularized algorithm is similar to the original Stein thinning.

**Remark 3.2**.: The Laplacian correction of \(k_{p}\) introduces second-order derivatives of \(p\) in the Stein discrepancy, and therefore enables to differentiate local minimum and saddle points of the density \(p\) from its local maximum. A natural approach to introduce second-order derivatives of \(p\) in KSD estimates, is to define the Stein discrepancy using second-order operators. A Laplacian Stein operator (Oates et al., 2017) is derived in Appendix G, but experiments show that this strategy is not efficient to fix Pathologies I & II.

### Theoretical properties

This subsection is dedicated to the theoretical analysis of regularized Stein thinning. First, we show that the proposed algorithm now enjoys good properties regarding Pathologies I and II, and thus mitigates the identified problems of the original Stein thinning. Secondly, we extend the convergence analysis of Riabiz et al. (2022) for the post-treatment of MCMC output, to show the convergence of the empirical law output by regularized Stein thinning towards the target probability measure.

Entropic regularization.In the previous section, Theorem 2.3 highlights how Pathology I of mode proportion blindness originates from the score insensitivity to mode weights. On the other hand, the entropic regularization is directly built on the target density, and therefore strongly depends on the mode weights. In the same setting of Assumption 2.1, required for Pathology I to occur with the original algorithm, the following Theorem 3.3 shows that the entropic regularized KSD is minimized for the appropriate target weight, with the suitable regularization strength \(\). Notice that Theorem 3.3, proved in Appendix D, is valid if \([(p(_{L}))][(p(_{R}))]\) with \(_{L}_{L}\) and \(_{R}_{R}\), otherwise the impact of the entropic regularization on \(w_{}^{}\) vanishes. However, as \(w_{p} 1/2\) is required in Assumption 2.1 for Pathology I to occur, \(p\) is asymmetric, and \([(p(_{R}))]=[(p(_{R}))]\) is only possible in very specific cases. Theorem 3.3 clearly shows that the regularized entropic KSD is sensitive to the weights of distant modes. Efficient strategies to choose the regularization strength will be first discussed in the asymptotic analysis below, and then in the experiments of the next section.

**Theorem 3.3**.: _Let \(k_{p}\) be the Stein kernel associated with the radial kernel \(k(,^{})=(\|-^{}\|_{2} /)\), where \(,^{}^{d}\), \(>0\), and \(^{2}()\). Let \(p\) and \(q\) be two bimodal mixture distributions satisfying Assumption 2.1. We define \(w_{}^{}\) as the optimal mixture weight of \(q\) with respect to the entropic regularized KSD distance, i.e., \(w_{}^{}=}\ _{}(,_{w})\). If \([(p(_{L}))][(p(_{R}))]\) where \(_{L}_{L}\) and \(_{R}_{R}\), it exists \(\) such that \(w_{}^{}=w_{p}\)._

Laplacian correction.First, we stress that the L-KSD is a strongly consistent estimate of the KSD distance, where the proof follows from the law of large numbers. Therefore, the Laplacian correction introduced in the L-KSD estimate does not undermine the good asymptotic properties of the KSD distance. Secondly, the following theorem shows that samples concentrated in local minimum or saddle points of the target distribution and of low density values, are well identified by the L-KSD as samples of worse quality than those truly sampled from the target. Consequently, the Laplacian correction fixes Pathology II, previously formalized in Theorem 2.4.

**Theorem 3.4**.: _Let \(k_{p}\) be the Stein kernel associated with the IMQ kernel with \(>0\), \((0,1)\), and \(c=1\). For \(m 2\), let \(\{_{i}\}_{i=1}^{m}^{d}\) be a set of points located at \(_{0}\), a local minimum or saddle point of \(p\), and of empirical measure \(_{m}\). Then, we have \(^{2}},_{m}>[ ^{2}},_{m}]\), if the density at \(_{0}\) satisfies \(p(_{0})<^{+}p(_{0})/[\|s_{p}( )\|_{2}^{2}]+[^{+} p()]\)._

Figure 3: Pathologies fixed by the regularized Stein thinning.

Convergence of regularized Stein thinning.While regularized Stein thinning fixes finite sample size pathologies, the asymptotic properties of Stein thinning are also preserved. Indeed, if the initial set of particles is drawn from a different distribution than the target using a Markov chain Monte Carlo, Theorem 3.6 states that the empirical measure of the sample obtained with regularized Stein Thinning, converges towards the target measure \(\), and thus extends the results of Riabiz et al. (2022). Notice that the weak convergence of a sequence of probability measure is denoted by \(\), and that distantly dissipative distributions are defined in Definition 1.1. The required assumption below, essentially states mild integrability conditions, and enforces that the MCMC output is not too far from a sample drawn from \(p\)--see Appendix F for additional details.

**Assumption 3.5**.: Let \(\) be a probability distribution on \(^{d}\), such that \(\) is absolutely continuous with respect to \(\). Let \(\{_{i}\}_{i}^{d}\) be a \(\)-invariant, time-homogeneous Markov chain, generated using a \(V\)-uniformly ergodic transition kernel, such that \(V()}{}+\|s_{p}( )\|_{2}^{2}}\). Suppose that, for some \(>0\), \(_{i}[e^{\|(p(_{i}))\|}]<\), \(_{i}[e^{^{+} p(_{ i})}]<\), \(_{i}[e^{^{+} p(_{ i})}]<\), \(_{i}[e^{(1,}{d0}( _{i})^{2})(}+\|s_{p}(_{i})\|_{2}^{2 })}]<\).

**Theorem 3.6**.: _Let \(\) be a distantly dissipative probability measure, that admits the density \(p^{2}(^{d})\), \(k_{p}\) be the Stein kernel associated with the IMQ kernel where \(,c>0,(0,1)\). Let \(\{_{i}\}_{i}^{d}\) be a Markov chain satisfying Assumption 3.5, \(\) be the index sequence of length \(m_{n}\) generated by regularized Stein thinning, and \(_{m_{n}}\) be the empirical measure of \(\{_{_{i}}\}_{i=1}^{m_{n}}\). If \((n)^{}<m_{n}<n\), with any \(>1\), and \(_{m_{n}}=o((m_{n})/m_{n})\), then we have almost surely \(_{m_{n}}\)._

Theorem 3.6, proved in Appendix F, provides us with interesting insights about the entropic regularization strength \(\). We already know that \(\) should be chosen with a rate at least as fast as \(O(1/m)\), to avoid the introduction of a higher a bias in the L-KSD than the original KSD. Indeed, for a sample drawn from the true target distribution \(p\), this bias \([^{2}(,_{m})]\) takes the form \([k_{p}(,)]/m+[^{+}(p( ))]/m-[(p())]\). Therefore, for slower rates of \(\) than \(O(1/m)\), trivial samples concentrated at a local maximum of the target \(p\), can have smaller L-KSD than samples drawn from \(p\). Then, Theorem 3.6 states that, for our ultimate application of MCMC post-processing, the Stein thinning sample distribution converges towards the target for such \(\) rates of \(O(1/m)\) or faster. In practice, in our Bayesian setting, it is not possible to fine tune this parameter \(\) because no metric is available to assess the Stein thinning quality for various values of \(\), as already mentioned in the case of the bandwidth parameter \(\). In addition, we cannot theoretically determine which exact range of values of \(\) leads to good thinned samples in a finite sample regime. However, we will see in the experiments of the following section that both slower and faster \(\) rates than \(O(1/m)\) lead to samples of degraded quality. Therefore, we set \(=1/m\) in the regularized Stein thinning, to ensure good empirical performance and the algorithm convergence.

## 4 Empirical Assessment

This section shows how regularized Stein thinning outperforms the original algorithm through three batches of experiments: mixtures of standard distributions using exact or MCMC sampling, and Bayesian logistic regression on real datasets. For the experiments considered in Sections 4.2 and 4.3, two Metropolis-Hastings samplers are considered with the Metropolis-Adjusted Langevin Algorithm (MALA) and the No-U-Turn sampler (NUTS). We use the IMQ kernel with \(\) set with the median heuristic, \(=1/2\), and \(c=1\), as recommended in Chen et al. (2018); Riabiz et al. (2022). We also set the regularization parameter with the default value of \(=1/m\). Notice that additional experiments are provided in Appendix A, and that the code is available at https://gitlab.com/drti/kernax.

When the target distribution is known, the efficiency of the Stein thinning algorithms are assessed by computing the MMD distance (see Equation (1)) between a large sample drawn from the target distribution and the thinned samples. More specifically, we use the following closed-form expression of the MMD (Gretton et al., 2006) with \(,^{}\) and \(,^{}\),

\[_{k}^{2}(,)=[k(,^ {})]+[k(,^{})]-2[k(,)]\,,\] (3)

where the kernel function \(k\) is chosen as the distance-induced kernel studied by Sejdinovic et al. (2013) and given by \(k(,^{})=\|\|_{2}+\|^{}\|_{2 }-\|-^{}\|_{2}\), for \(,^{}^{d}\). In this setting, the MMD reduces to the well known energy distance, as shown by Sejdinovic et al. (2013).

### Gaussian mixtures with exact sampling

As a first batch of experiments, we build on Example 1 and consider more complicated two-dimensional Gaussian mixtures to further illustrate the correction of pathologies I & II. The first Gaussian mixture is made of four modes located at \(_{1}=(-3,3)\), \(_{2}=(-3,3)\), \(_{3}=(3,3)\), and \(_{4}=(3,-3)\), and with weights \(w_{1}=w_{2}=0.1\), and \(w_{3}=w_{4}=0.4\), respectively. The second mixture is taken from (Qiu and Wang, 2023). It is made of \(6\) equally weighted Gaussian distributions centered at \(_{i}=(3(2(i-1)/6),3(2(i-1)/6))\), for \(i=1,,6\). For both experiments, we rely on exact Monte Carlo sampling to generate \(n=3000\) observations, and apply Stein thinning and its regularized variant to select \(m=300\) particles. The observed samples and the selected particles are shown in Figure 4. The first example shows that vanilla Stein thinning does not capture the right proportions, while the regularized variant appropriately penalizes modes with lower weights. The second example illustrates Pathology II, which is corrected by the regularized Stein thinning.

### Banana-shaped and Gaussian mixtures with MCMC sampling

We consider a mixture of two distant modes of \(d\)-dimensional banana-shaped distributions with t-tails and unbalanced weights (Haario et al., 1999; Pompe et al., 2020), illustrated in Figure 5, and precisely defined in Appendix A.2. We sample this target banana mixture with both MALA and NUTS using three different step sizes \(\) and \(10^{5}\) iterations. The generated samples are post-processed with the Stein thinning and regularized Stein thinning algorithms, and their performances are compared with the MMD between the post-processed samples and large samples drawn from the known target banana mixture. This experiment is run for various thinning sizes \(m\) and dimensions \(d\), with \(20\) repetitions to quantify uncertainties. The results obtained with the MALA sampler are shown in Figure 6: the regularized Stein thinning clearly generates samples of higher quality than the vanilla Stein thinning. Additionally, an example of post-processed MALA output is depicted in Figure 5, together with a heatmap of the Laplacian correction. On the left panel of Figure 5, we see that pathologies are especially strong in this experiment, with a large number of particles lying between the two modes in a region of low probability. On the right panels, we observe that regularized Stein thinning fix pathologies. Similar results were obtained with NUTS and are reported in Appendix A.2 for brevity. Next, we conduct the same experiments for a \(d\)-dimensional Gaussian mixture of four modes with different variances, as detailed in Appendix A.2. Again, Figure 6 shows the better

Figure 4: Gaussian mixtures with exact Monte Carlo sampling. Solutions (red dots) obtained by Stein thinning and its regularized variant.

Figure 5: t-banana-shaped mixture (\(d=10\)). From left to right: solutions obtained with standard and regularized Stein thinning with contour lines of \(p\), and heatmap of the Laplacian correction.

performance of regularized Stein thinning. Besides, we take advantage of this last experiment to explore other regularization rates than our default \(=1/m\). Figure 12 in Appendix A.2 shows that a slower rate of \(=1/(m)\), which violates the convergence assumptions of Theorem 3.6, has significantly worse performance than the original Stein thinning. On the other hand, with a faster rate than \(1/m\) such as \(1/m^{2}\), the effect of the entropic regularization disappears, and we recover similar results than the original Stein thinning. This supports that the default value of \(=1/m\) is an efficient heuristic, since slower and faster rates of \(\) strongly degrade the algorithm performance.

### Bayesian logistic regression

We now compare the two Stein thinning algorithms in the Bayesian logistic regression setting for binary classification, since such problem usually involves multimodal posterior--see, _e.g._, Gershman et al. (2012); Liu and Wang (2016); Fong et al. (2019); Korba et al. (2021). Given a dataset \(_{N}=\{(_{i},Y_{i})\}_{i=1}^{N}\) made of \(N\) pairs of features \(_{i}^{d}\) and labels \(Y_{i}\{0,1\}\), the probability that \(Y_{i}\) is of class \(1\) is given by \(p(Y_{i}=1|_{i},,_{0})=1/(1+(-_{0}- ^{T}_{i}))\), for some parameters \(=(_{0},)^{d+1}\). The prior distributions of the weight vector \(\) is assumed to be Gaussian, \(p(^{(j)}|^{(j)})=(^{(j)}|0,1/^{(j)})\), and a Gamma prior with parameters \((a,b)\) is chosen for the precision \(^{(j)}\). Following (Fong et al., 2019), the hyperparameters are chosen as \(a=b=1\). The posterior distribution of the weights \(\) is sampled with both MALA and NUTS using \(48\) independent chains, of respectively \(10^{4}\) and \(10^{5}\) iterations, and four step sizes \(\) are considered along with three thinning sizes \(m\). Each MCMC sample is post-processed with the two Stein thinning algorithms. For a new input \(^{}\), the resulting thinned samples are used to approximate the posterior predictive distribution \(p(Y=1|^{},_{N})\), defined by \( p(Y=1|^{},)p(| _{N})d\). The performance of Stein thinning algorithms are assessed using the standard AUC metric for classification problems, estimated with \(10\)-fold cross-validation and \(10\) repetitions for uncertainties. Table 1 gathers the results for five public datasets from the UCI repository (Dua and Graff, 2017), and described in Appendix A.3, where the best AUC obtained for each algorithm over the four MCMC step sizes are reported. Clearly, regularized Stein thinning significantly improves the performance of Bayesian logistic regression.

## 5 Conclusion

Stein thinning has raised a high interest in recent years, as a powerful tool to post-process MCMC outputs, by the greedy minimization of the kernelized Stein discrepancy. Unfortunately, empirical studies have shown that KSD-based algorithms suffer from strong pathologies. We have conducted an in-depth theoretical analysis to identify the mechanisms at stake. From this understanding, we propose an improved Stein thinning algorithm relying on entropic regularization and Laplacian correction. This approach exhibits relevant theoretical properties regarding pathologies, as well as highly improved empirical performance. Finally, the analysis of these regularization terms for other types of KSD-based algorithms, such a KSD descent, seems a promising route for future work.

    &  &  \\  Dataset & ST & RST & ST & RST \\  Breast W. & 0.88 (0.02) & **0.96** (0.00) & 0.93 (0.01) & **0.96** (0.00) \\ Diabetes & **0.52** (0.01) & 0.50 (0.02) & 0.53 (0.02) & **0.57** (0.02) \\ Haberman & 0.51 (0.04) & **0.53** (0.02) & 0.53 (0.03) & **0.58** (0.02) \\ Liver & 0.53 (0.04) & **0.69** (0.01) & 0.61 (0.04) & **0.70** (0.01) \\ Sonar & 0.80 (0.02) & **0.81** (0.01) & 0.81 (0.01) & 0.81 (0.01) \\   

Table 1: AUCs obtained with NUTS sampler for Stein Thinning (ST) and Regularized Stein Thinning (RST).

Figure 6: (MALA) Graphs of the MMD with respect to the thinning size \(m\) (for \(d=2\)) and with respect to \(d\) (for \(m=300\)). Left two panels: banana mixture. Right two panels: Gaussian mixture.