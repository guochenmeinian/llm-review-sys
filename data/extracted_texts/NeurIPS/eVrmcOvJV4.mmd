# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

In this paper, we propose a dramatically more efficient algorithm that recovers human-like judgements even on significantly more sophisticated problems. We take inspiration from the field of computer graphics, which has developed a wealth of Monte Carlo algorithms for "path tracing." **Our key insight is an analogy between tracing paths of _light_ through 3D scenes and paths of _agents_ through planning domains.** This insight allows us to import decades of algorithmic advances from computer graphics to the seemingly-unrelated social intelligence problem of snapshot inference.

In Section 2 we formalize this problem and present a new Monte Carlo algorithm for sampling approximate solutions. Then, in Section 3, we show that our algorithm is up to \(30,000\) more efficient than prior work. Finally, in Section 3.3 we demonstrate via three behavioral studies that our model's predictions match human judgements on new, scaled-up tasks inaccessible to prior work.

## 2 Our proposed algorithm

We describe our method in the setting of Markov Decision Processes (MDPs). An MDP is a tuple \(,,,,,\) where \(\) is the set of possible states, \(\) is the space of possible actions, \(:\) gives the transition probability, \(:\) is the reward function and \(\) is a discount factor. We will specifically consider MDPs that have a single "goal state" \(g\) with positive reward, such as a grid world with a target square the agent needs to reach. We will assume that \(g\) is known to the agent, but not to us (the observers).

Consider an agent that begins in some initial state \(s p(s).\) While the agent is on its trajectory from \(s\) to \(g\), we observe a "snapshot" of the agent in some interim state \(x.\) Given only \(x\) (and not \(s!\)), our aim is to infer \(p(g x).\)

Applying Bayes' rule, we have \(p(g x) p(x g)p(g),\) where \(p(g)\) is our prior belief about what \(g\) might be. The likelihood \(p(x g)\) can be computed by marginalizing over all possible start states \(s\) and all possible paths \(_{s:g}\) from \(s\) to \(g\), where a path \(_{s:g}\) is defined as a sequence of states \(_{s:g}=(s_{1}, s_{n})\) such that \(s_{1}=s\) and \(s_{n}=g\). Performing this marginalization, and noting that \(p(s g)=p(s)\) because \(s\) and \(g\) are independent, we have:

\[p(x g)=_{s}p(x s,g)p(s g)ds=_{s}_{_{s:g}} {p(x_{s:g},s,g)}^{}  s,g)}_{}p(s)d_{s:g}ds\] (1)

To evaluate the likelihood of a snapshot \(p(x_{s:g},s,g)\), we apply the _size principle_[Tenenbaum, 1998, 1999, Griffiths and Tenenbaum, 2006], analogous to the _generic viewpoint assumption_ in computer vision [Freeman, 1994, Albert and Hoffman, 2000]. In this case, the principle states that the snapshot was equally likely to have been taken anywhere along the path, and therefore the likelihood of a snapshot conditioned on a path is inversely proportional to the length of the path. If \((x)\) indicates whether path \(\) passes through \(x\), and \(||\) indicates the length of \(\), then \(p(x_{s:g},s,g)\) is given by \((x_{s:g})|_{s:g}|^{-1}.\) As a consequence of this definition, all paths that do not pass through \(x\) contribute zero probability, and thus can be dropped from the integral.

Figure 1: How can we infer what an agent is trying to do, based on a snapshot of its current state?

To evaluate the likelihood of a path \(p(_{s:g} s,g)\), we apply the _principle of rational action:_ agents are likelier to take actions that maximize their utility (Dennett, 1989; Jara-Ettinger et al., 2016). There are many ways to formalize this intuition. A common option, which we adopt, is to say that at each step, the agent chooses an action with probability proportional to the softmax over its \(Q\)-values at its current state, with some temperature \(\). That is, \(p(x x^{} g)_{a}( Q_{g}(x,a)) (x x^{} a)\), where \((x x^{} a)\) is the transition probability from \(x\) to \(x^{}\) if action \(a\) is taken, and \(p( g)_{t}p(x_{t} x_{t+1} g)\).

We now have all the ingredients we need to evaluate \(p(x g)\). However, to compute it exactly we would need to integrate over all possible initial states \(s\), and the set of paths \(_{s:g}\), which could be infinite (agents might wander for arbitrarily long, albeit with vanishingly low probability). To approximate the likelihood in finite time, Lopez-Brau et al. turn to Monte Carlo sampling (Algorithm 1). They rejection-sample paths \(_{s:g}\) by sampling a candidate start state \(s^{(i)} p(s)\), simulating a rollout of the agent to sample a path \(_{s:g}^{(i)} p(_{s:g}^{(i)} s^{(i)},g)\), and then averaging the integrand over these samples. With \(N\) samples, their unbiased likelihood estimator is given by \((x g)=_{i=1}^{N}(x_{s:g}^{(i)})|_{s: g}^{(i)}|^{-1}\).

Unfortunately, in practice this scheme is extremely slow: even in a \(7 7\) gridworld with fewer than 49 states (only 2 of which were possible initial states), Lopez-Brau et al. report taking over 300,000 trajectory samples per goal to perform inference. In the rest of this section, we will describe a series of algorithmic enhancements that allow for comparable inference quality with just 10 samples per goal (i.e. 30,000\(\) fewer). We will develop our algorithm (Algorithm 2) through three insights.

### First insight: only sample paths through the observed state

Our first insight is that \((x)\) is extremely sparse--most paths likely do not pass through \(x\), and so most naive path samples contribute zero to the estimator. We would like to only sample paths that pass through \(x\). Any such path can be partitioned at \(x\) into two portions, \(_{s:x}\) and \(_{x:g}\). Let us integrate separately over those portions.

\[p(x g)=_{s}_{_{s:x}}_{_{x:g}} g)\;p (_{x:g} g)}{|_{s:x}|+|_{x:g}|}p(s)\;d_{x:g}\;d_{s:x}\;ds\] (2)

This suggests a more efficient Monte Carlo sampling scheme. Rather than rejection-sampling paths \(_{s:g}^{(i)}\) directly from \(s\) to \(g\), we can independently sample two path segments: a "past" path \(_{s:x}^{(i)}\) from \(s\) to \(x\), and a "future" path \(_{x:g}^{(i)}\) from \(x\) to \(g\). Any such path is guaranteed to pass through \(x\), so no samples are wasted.

However, we now have two new problems. First, it is not clear how to sample paths \(_{s:x}^{(i)}\) from \(s\) to \(x\), because rollouts of a simulated agent are unlikely to pass through \(x\) on their way to \(g\). We could imagine using a _second_ planner just to chart paths from \(s\) to \(x\), but this would require a lot of additional planning work. Second, we still have to sample \(s\). If the space of initial states is small (e.g. a room only has one or two doors), then this is no issue. However, in practice this space might be very large or even infinite. For example, if you observe someone driving to work in the morning, their home could be anywhere in the city. Furthermore, most of these states might be inaccessible or otherwise implausible, and it would be a waste of computational resources to consider them. In the next section, we show how to solve both of these problems by tracing paths _backwards in time_.

### Second insight: sample \(_{:x}\) backwards in time

Our second insight is that we can collapse the first two integrals by jointly integrating over the domain of all paths \(_{:x}\) that terminate at \(x\), no matter where they started from. Say a path \(_{:x}\) begins at \(_{:x}\). Then, we can rewrite our likelihood as below.

\[p(x g)=_{_{:x}}_{_{x:g}} g)\;p(_{x: g} g)}{|_{:x}|+|_{x:g}|}p(_{:x})\;d_{x:g}\;d_{:x}\] (3)

This suggests that we should sample \(_{:x}^{(i)}\)_backwards_ through time, starting from \(x\). No matter how we extend this path, we obtain a valid path from \(_{:x}^{(i)}\) to \(x\). We illustrate this in Figure 1.

An analogy to path tracing in computer graphics is helpful here. When rendering a 3D scene, a renderer must integrate over all paths of light that begin at a light source in the scene and end at a pixel on the camera's film--a problem formalized by the rendering equation (Kajiya, 1986). Of course, these paths may be reflected and refracted stochastically by several surface interactions along the way. Rather than starting at one of the millions of light sources in the scene and tracing a ray hoping to eventually reach the camera film, renderers instead start at the camera and trace rays _backward into the scene_ until they inevitably reach a light source.

Similarly, here we trace paths backwards from \(x\) into the past--\(s\) corresponds to a light source, each action taken by the agent corresponds to a stochastic surface interaction, and \(x\) corresponds to a pixel on the camera. Indeed, our integral is analogous to the rendering equation, bringing to our disposal the entire Monte Carlo light transport toolbox--a toolbox the rendering community has spent decades developing. (These techniques were pioneered by Veach (1998), though see Pharr et al. (2016) for an accessible review.) The particular ideas we borrow are the following:

Importance samplingThe first idea is to be deliberate about how paths are extended backwards in time. We could sample predecessor states uniformly at random--however, that would lead to "unlikely" or irrational paths. Instead, we preferentially select a predecessor state \(x_{}\) based on the likelihood of transitioning to the current state from \(x_{}\). Then, we re-weight our path sample appropriately so that our estimator remains unbiased (see Algorithm 2, line 14).

Russian roulette terminationThe next concern is when to stop extending a path into the past. In principle, paths could be infinitely long in some domains. However, at some point paths become so unlikely that extending them is not worth the computational effort of tracing them. An unbiased finite-time solution to this problem is given by the _Russian roulette_ method (Carter and Cashwell, 1975; Arvo and Kirk, 1990): at each step, we toss a weighted coin, and only continue extending the path if it comes up "heads." Then, we weight subsequent samples appropriately to keep the estimator unbiased (see Algorithm 2, line 11).

Bidirectional path tracingThe last concern is that the path may not ever "find" the region of state space where the agent could have started. Consider a setting where the space of possible initial states is large but sparse--for example, if we know the agent started from a _red_ house somewhere in the city. Importance sampling path predecessors with Russian roulette termination is not guaranteed to stop at a red house, whereas forward-sampling paths from a random red house is not guaranteed to pass through the observed state. In computer graphics, this situation is analogous to rendering a scene with many lights that are occluded from the camera. The classic solution is _bi-directional path tracing_(Lafortune and Willems, 1993; Veach and Guibas, 1995): first, we "cache" some paths forward-simulated from randomly sampled lights (see Algorithm 3). Then, when backwards-tracing rays, we find opportunities to connect them to a continuation in the cache (see Algorithm 2, line 8).

As a correctness check, we compare the numerical results from our method and the rejection sampling approach of Lopez-Brau et al. (2020) in Appendix B, showing that likelihoods computed from the two methods match closely as expected.

Figure 2: **(left)** In this example of the “grid” domain, we observe an agent near the blue gem. Even though we do not know where the agent started from, our intuition says that the agent is heading towards the blue gem. **(right)** In this example of the “keys” domain, we observe an agent right next to the green key. Humans infer that the agent is heading towards the green key because it wants the blue gem. Our algorithm replicates both of these inferences with only 10 samples.

```
0:\(x\), the agent's current state (e.g. position in gridworld) \(g\), the hypothesized goal \(P(s s^{} g)\), the probability the agent will move to \(s^{}\) from \(s\)\(P_{}(s)\), the prior over the agent starting at \(s\)
0:\((x|g)\)
1:\(t 0,n 0\), sample \(x_{}\) with probability \( P_{}()\)
2:while\(x_{}\) is not an end state do
3:if\(x_{}=x\)then
4:\(n n+1\)
5: sample \(x_{}\) with probability \(p_{} P(x_{} g)\)
6:\(x_{} x_{}\) and \(t t+1\)
7:return\(1\)\(/\)\(\) if \(n>0\), otherwise \(0\) ```

**Algorithm 1** Rejection sampling, as in prior work. Compare to our proposed method, Algorithm 2.

```
0:\(x\), \(g\), \(P(s s^{} g)\), \(P_{}(s)\) as in Algorithm 1 \(\), the strength of importance sampling \(d\), an average termination depth for Russian roulette \(C\), an optional bidirectional path tracing cache (see Algorithm 3)
0:\((x|g)\)
1:\( 0\)
2:\(t_{} 0\), \(x_{} x\)\(\)Sample forward from \(x\) to \(g\)
3:while\(x_{}\) is not an end state do
4: sample \(x_{}\) with probability \(p_{} P(x_{} g)\)
5:\(x_{} x_{}\) and \(t_{} t_{}+1\)
6:\(t_{} 1\), \(x_{} x\), \(p_{} 1\)\(\)Sample backwards from \(x\)
7:while true do
8:if\(x_{} C\)then\(\)Check BDPT cache for available completions
9: sample \((t_{},w)\) from \(C[x_{}]\)
10:return\(w(\#C[x_{}]/\#C) p_{}/(t_{}+t_{ }+t_{})\)
11:if\(()<1/d\)then\(\)Russian roulette termination
12:return\(P_{}(x_{}) p_{}/(t_{}+t_{ }) 1/(1/d)\)\(\)Record sample starting at \(x_{}\)
13:\(p_{} p_{}\)\(/\)\((1-1/d)\)\(\)Apply Russian roulette weight
14: sample \(x_{}\) with probability \(p_{}( P( x_{} g))\)\(\)Choose predecessor
15:\(p_{} p_{} P(x_{} x_{} g)\)\(/\)\(p_{}\)\(\)Apply importance sample weight
16:\(x_{} x_{}\) and \(t_{} t_{}+1\)
17:return\(\) ```

**Algorithm 2** Our bidirectional likelihood sampler

### Third insight: this algorithm allows for on-line planning

Our method is agnostic to the algorithm used to compute the agent's policy, which allows us to choose from any combination of reinforcement-learning or heuristic search approaches. Model-based methods like value iteration require an expensive pre-computation of the value function for all possible goals and states, even though some of those states may not ever be visited by our algorithm. It seems implausible that humans do this, because we make judgements quickly even in new domains. Moreover, adding obviously-irrelevant states should not make the problem harder to solve--we should simply not bother planning from those states until they somehow become relevant.

Motivated by this, in domains that can be expressed as classical planning problems (i.e. domains with a single "goal state" with positive reward), we use an online A* planner instead of precomputing policies by value iteration. We define \(p(x x^{} g)\) to be a softmax over the difference in path costs between \(x\) and \(x^{}\) to \(g\) as computed by the planner: \(p(x x^{} g)_{a}((C(x g)-C(x^{}  g))\). That is, the agent is likelier to move to states that will bring it closer to the goal. To avoid re-planning from scratch for every evaluation of \(p(x_{t-1} x_{t} g)\), we run A-star _backwards_ from the goal to the current state. This lets us re-use the bulk of its intermediate computations (known distances, evaluations of the heuristic, etc.) between queries.

```
0:\(g\), \(P(s s^{} g)\), \(P_{}(s)\) as in Algorithm 1, \(d\) as in Algorithm 2, and \(C\), a cache
1:\(t 0\), \(w 1\), sample \(x_{}\) with probability \( P_{}()\)
2:while\(x_{}\) is not an end state do
3: add \((t,d w)\) to \(C[x_{}]\)
4: sample \(x_{}\) with probability \( P(x_{} g)\)
5:if flip\(()<1/d\)then
6:break
7:\(w w\ /\ (1-1/d)\)
8:\(x_{} x_{}\) and \(t t+1\) ```

**Algorithm 3** Grow the bidirectional path tracer's cache (to be called repeatedly)

## 3 Experiments

To evaluate our sampling algorithm, we chose a suite of benchmark domains reflecting the variety of inferences humans make:

Simple gridworldWe re-implement the \(7 7\) gridworld domain from Lopez-Brau et al. (Figure 2). The agent seeks one of three gems and at every timestep can move north, south, east or west. The viewer's inference task is to determine which gem the agent seeks. While Lopez-Brau et al. fix two possible starting-points ("entryways") for the agent, our method can optionally relax this constraint and instead have a uniform prior over the start state.

Doors, keys, and gems (multi-stage planning)This is a more advanced \(8 8\) gridworld, inspired by Zhi-Xuan et al. (2020). The agent is blocked from its gem by _doors_, which can only be opened if the agent is has the correct _keys_ (the agent always starts out empty-handed). The inference task is to look at a snapshot image and determine which gem the agent seeks. For example, if we observe the snapshot in Figure 2, we might infer that the agent plans to get the green key to obtain the blue gem.

Word blocks (non-spatial)In this domain, the agent spells a word out of the six letter blocks by picking and placing them in stacks on a table. However, they are interrupted (e.g. by a fire alarm) and have to leave the room before finishing. The inference tasks are to look at the blocks left behind and determine (a) which word the agent was trying to spell, and (b) which blocks the agent has touched.

Additional domainsAppendix C shows results from additional domains from the cognitive science literature that further reflect the flexibility of our method. We demonstrate domains with partial observability (C.1), multiple agents (C.2), and continuous state spaces with physics (C.3). The breadth of our experiments reflects the typical scope of related work in cognitive science.

### Qualitative analysis

Tables 1 and 2 show some example inferences made by our algorithm. Each cell is colored according to the posterior distribution over goals. If the inference algorithm produced no valid samples for a cell, that cell is marked with an \(\) symbol. With just 10 samples, our method's posterior inferences are near-convergent and align well with human responses. In comparison, with 10 samples rejection sampling typically produces extremely noisy predictions, and often simply fails to produce any non-rejected samples at all--indeed, in the blocks domain even 1,000 samples are not always enough to produce a single non-rejected sample.

### Quantitative analysis

We report the total variation \((x)=_{g_{i}}|(g_{i} x)-p(g_{i} x)|\) between the true posterior and inferences made using 10 samples of both our method and rejection sampling, averaged for 100 trials and across all of the inference tasks in the benchmark. We take the true posterior to be our method's estimate with 1,000 samples, though we also show results if the true posterior is taken to be rejection sampling with 10,000 samples. Our results are shown in Table 3. Across all domains, our algorithm substantially outperforms rejection sampling.

[MISSING_PAGE_FAIL:7]

## 4 Related work

Cognitive scienceHuman social cognition and "theory of mind" are well-modeled by **Bayesian inverse planning**(Baker et al., 2009; Jara-Ettinger, 2019; Baker et al., 2017), which infers an agent's goals from its observed actions. Inverse planning predicts human inferences about multiple agents (Kleiman-Weiner et al., 2016; Wu et al., 2021), social scenes (Ullman et al., 2009; Netanyahu et al., 2021), emotion (Ong et al., 2015), and agents who make mistakes (Zhi-Xuan et al., 2020).

A growing body of work studies how people make inferences about the past from a "snapshot" of present physical evidence (Smith and Vul, 2014; Gerstenberg et al., 2021; Lopez-Brau and Jarat Ettinger, 2020; Paulun et al., 2015; Yildirim et al., 2017, 2019). Recently, Lopez-Brau et al. (2020)

   Rejection (10) & Rejection (1k) & Ours (10) & Ours (1k) & Humans \\    (all samples \\ rejected) \\  & & & & & \\   (all samples \\ rejected) \\  & & & & & \\   (all samples \\ rejected) \\  & & & & & \\   (all samples \\ rejected) \\  & & & & & \\  
 (all samples \\ rejected) \\  & & & & & \\   

Table 2: Qualitative comparison of inference algorithms. Blocks are colored according to inferred probability of that block having been touched by the person stacking the blocks (red is high probability, blue is low). We show results for 10 samples and 1,000 (1k) samples, comparing rejection sampling, our method, and human subjects. **We produce near-convergent inferences with only 10 samples.** In comparison, rejection sampling is unable to make any inference with 10 samples, and sometimes even fails with 1,000 samples. When it succeeds, its predictions are high-variance and overconfident.

2022] asked how people make inferences about the past and future of agents from static physical evidence they leave behind, which is something even children can do [Pelz et al., 2020, Jacobs et al., 2021]. We build on this line of work by dramatically accelerating inference, and extending it to more sophisticated domains where previous methods could not scale.

Artificial intelligenceBayesian approaches have been successful in approaches to **plan recognition**, the problem of inferring an agent's plan from observed actions [Ramirez and Geffner, 2009, 2010, Sohrabi et al., 2016, Charniak and Goldman, 1993]. Our work provides a method for plan recognition from a single state snapshot, with no need to observe actions.

The reinforcement learning community has long sought to learn an agent's reward function by observing the actions it takes via **inverse reinforcement learning** or IRL [Ng et al., 2000, Arora and Doshi, 2021, Ziebart et al., 2008]. Recently, Shah et al.  proposed "IRL from a single state." Their method, "Reinforcement Learning by Simulating the Past" (RLSP), learns reward functions based on a single observation of a human in the environment, assuming that the observation is taken at the end of a finite-horizon MDP of fixed horizon \(T\). We build on RLSP in three ways: **(1)** RLSP is highly sensitive to the time horizon hyperparameter \(T\). We dispense with the fixed-horizon assumption altogether, integrating over past trajectories of all possible lengths. **(2)** Unlike Shah et al., we do not

   Benchmark (TV against Ours @ 1k) & Rejection @ 10 & Ours @ 10 \\  Grid (two doors) & 0.063 & 0.0257 \\ Grid (starting anywhere) & 0.159 & 0.0538 \\ Keys (observed holding no key) & 0.409 & 0.108 \\ Keys (observed holding pink key) & 0.388 & 0.157 \\ Keys (observed holding green key) & 0.381 & 0.119 \\ Blocks & 0.985 & 0.358 \\  Benchmark (TV against Rejection @ 10k) & Rejection @ 10 & Ours @ 10 \\  Grid (two doors) & 0.0705 & 0.0336 \\ Grid (starting anywhere) & 0.153 & 0.0836 \\ Keys (observed holding no key) & 0.672 & 0.410 \\ Keys (observed holding pink key) & 0.456 & 0.382 \\ Keys (observed holding green key) & 0.463 & 0.312 \\ Blocks & _Rejection @ 10k did not converge_ \\   

Table 3: Quantitative comparison of inference algorithms (see Section 3.2). We show the average total variation distance (TV) of a 10-sample posterior estimate against a converged posterior, averaged over 100 trials. **Lower is better.** Our method does significantly better than rejection sampling [Lopez-Brau et al., 2022] for each task, whether the converged “ground truth” is computed with 1,000 samples using our method, or 10,000 samples using rejection sampling.

Figure 3: Our inferences correlate well with human responses. In the “grid” plot, colors represent goal gem colors. In the “keys” plot, colors represent which keys (if any) the agent is seen holding, and we show \(p\)(goal is blue gem \(|\;x\)). In the “blocks” plot, we show \(p\)(touched \(|\;x\)) for each block.

assume the snapshot was taken at the end of the agent's journey--the agent can be observed partway. **(3)** Our sampling-based method scales to significantly larger domains, because we do not have to integrate exhaustively over all possible trajectories.

The Deep RLSP algorithm of Lindner et al. (2020) also addresses point (3) above by sampling past trajectories. However, while Deep RLSP's maximum-likelihood formulation is designed to recover robot policies, our Bayesian approach is designed specifically to model human intuitions about agents' goals. Thus, unlike Deep RLSP, our method incorporates priors over start states in a principled way, and produces human-like uncertainty quantification.

## 5 Limitations and future work

Sampling over goalsIn this paper, we showed how to scale inference to a large set of possible initial states. However, we compute the posterior by enumeration over all possible goals, which takes linear time in the size of the space of goals. To scale to larger goal spaces as well, it may be possible to use Markov Chain Monte Carlo to sample goals conditioned on the observed state, similar to what Veach and Guibas (1997) propose for path tracing in computer graphics. This may seem challenging because we only stochastically estimate the likelihood \(p(x g)\). However, because our estimator is unbiased, it is still possible to use it to compute valid Metropolis-Hastings transitions (Andrieu and Roberts, 2009).

Cognitive plausibilityOur method's sample efficiency suggests that it may resemble how humans actually do this task, similar to how few-shot algorithms have shown promise in modeling human behavior in other domains (Vul et al., 2014). Following previous work using eye-tracking studies to investigate cognitive processes underlying simulation and planning (Gerstenberg et al., 2017), we hope to use eye-tracking to compare human strategies to our algorithm. In the language of Marr (1982), this would allow us to go beyond the _computational_ account of Lopez-Brau et al. (2020) and take a first step towards an _algorithmic_ account of inverse planning with snapshot states.

Theoretical analysisThe _time complexity_ of taking a single sample using our algorithm is straightforward: it scales linearly with the Russian roulette depth and the number of possible goals (although sub-linear runtime in number of goals is possible through sampling as described above). However, the more pertinent question is that of _sample complexity_. While our algorithm is unbiased, analyzing the rate of convergence as a function of the size of the sample space and complexity of the domain are topics for future work.

Investigating potential applicationsWhile our goal in this paper was to model how humans perform this inference task, extensions of our method could be applied to a variety of practical problems: some examples include understanding dynamic action in photographs of sports, interpreting step-by-step instruction manuals, and forensic reconstruction based on available static evidence.

From analysis to synthesisWe began this paper with Hemingway's short story--and indeed, more generally, artists have long depicted dynamic action through static scenes (McCloud, 1993). In future work we hope to consider the inverse problem of designing interesting, evocative, or ambiguous scenes by optimizing _over_ inference (Chandra et al., 2023, 2022; Chandra et al., 2023), and designing expressive and human-interpretable robotic "gestures" similar to legible planning (Dragan et al., 2013).

## 6 Conclusion

In this paper, we offered a cognitively-plausible algorithm for making inferences about the past and the future based on the observed present. Building on prior work from both the cognitive science and AI communities, and drawing inspiration from the Monte Carlo rendering literature in computer graphics, we presented a highly sample-efficient method for performing such inferences. Finally, we showed that our method matched human intuitions on a variety of challenging inference tasks to which previous methods could not scale.