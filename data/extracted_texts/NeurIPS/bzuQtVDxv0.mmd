# Splatter a Video: Video Gaussian Representation

for Versatile Processing

 Yang-Tian Sun\({}^{1}\)1 & Yi-Hua Huang\({}^{1}\)1 & Lin Ma & Xiaoyang Lyu\({}^{1}\) & Yan-Pei Cao\({}^{2}\) & Xiaojuan Qi\({}^{1}\)\({}^{1}\)

\({}^{1}\)The University of Hong Kong \({}^{2}\) VAST

###### Abstract

Video representation is a long-standing problem that is crucial for various downstream tasks, such as tracking, depth prediction, segmentation, view synthesis, and editing. However, current methods either struggle to model complex motions due to the absence of 3D structure or rely on implicit 3D representations that are ill-suited for manipulation tasks. To address these challenges, we introduce a novel explicit 3D representation--video Gaussian representation--that embeds a video into 3D Gaussians. Our proposed representation models video appearance in a 3D canonical space using explicit Gaussians as proxies and associates each Gaussian with 3D motions for video motion. This approach offers a more intrinsic and explicit representation than layered atlas or volumetric pixel matrices. To obtain such a representation, we distill 2D priors, such as optical flow and depth, from foundation models to regularize learning in this ill-posed setting. Extensive applications demonstrate the versatility of our new video representation. It has been proven effective in numerous video processing tasks, including tracking, consistent video depth and feature refinement, motion and appearance editing, and stereoscopic video generation.

## 1 Introduction

Video processing, which encompasses a variety of tasks such as video editing, can enable numerous applications in fields like social media, filmmaking, and advertising . A video can be viewed as a collection of spatiotemporal pixels. However, processing a video directly in its pixel space, while maintaining temporal consistency, poses challenges due to the inherent complexities associated with appearance, motion, occlusions, and noise in the video data . Consequently, a robust video representation capable of abstracting and disentangling appearance and motion is crucial for facilitating various applications and overcoming these challenges.

Existing research on video representation for processing has primarily focused on 2D/2.5D techniques, employing methods such as optical flow and tracking to associate pixels across frames . These approaches often involve learning a canonical image  or a layered atlas with persistent motion patterns  to facilitate editing and then use optical flow or tracks to propagate edits throughout a video. The most recent work  utilizes hash grids combined with implicit functions to embed a video into a learned canonical image for appearance and a deformation field for motion. Despite achieving promising results in appearance editing tasks, these methods struggle to handle occlusions of objects (see Fig. 3), leading to erroneous propagation. Although layered 2.5D representation  can mitigate this issue, they still face challenges with complex self-occlusions within a layer. Moreover, these techniques have limited or no capability inaddressing processing tasks that require 3D information, such as video representation with complex occlusions, consistent depth prediction, and stereoscopic video generation.

Drawing inspiration from the fact that a video is essentially a projection of the dynamic 3D world onto the 2D image plane at different moments, we pose the question: _is it possible to represent a video in its intrinsic 3D form?_ By doing so, we could potentially bypass the limitations of 2D representations, such as occlusions, reduce the complexity of motion modeling, and support processing tasks that require 3D information. Recent work  has explored 3D representations, which employ an implicit radiance field to model a canonical 3D space and leverage a bi-directional mapping network for associating 2D pixels with 3D representations. While this approach demonstrates promising performance in dense tracking, it falls short in faithfully representing video appearance, making it incapable of performing video processing tasks that require generating new videos, such as video editing. Moreover, its implicit nature limits its applicability to a variety of video processing tasks that require explicit content or motion manipulations, such as the removal or addition of objects and adjustments to the motion patterns of objects.

In this paper, we introduce a novel explicit video Gaussian representation (VGR) based on 3D Gaussians . Our core idea revolves around utilizing Gaussians in a canonical 3D space to model video appearance while associating each Gaussian with time-dependent 3D motion attributes to control its locations at different time steps for video motion. This 3D representation can then be employed to process and render videos effectively. The subsequent challenge lies in how to map a video onto such a 3D Gaussian representation. This is inherently difficult due to the loss of essential 3D information during 3D-to-2D projection, as well as the entanglement of motion and appearance in videos. However, recent advancements in large models have facilitated the acquisition of high-quality monocular priors from images and videos, such as optical flow [44; 11] and monocular depth [54; 15; 53]. While these 2D priors may not be perfect, they can serve as regularization for learning through knowledge distillation. Consequently, we propose leveraging these 2D priors in conjunction with our 3D motion regularization for learning. By doing so, we effectively lift 2D information- such as pixels, depth, and optical flow-into a unified and compact 3D representation.

Upon learning, our video Gaussian representation can be used to support versatile video processing tasks, as shown in Fig. 1. Here, we showcase its efficacy in 7 video-processing tasks: Specifically, it can be used to obtain **1)** dense tracking and **2)** improve the consistency of monocular 2D prior across frames, leading to better video depth and feature consistency. Secondly, our representation facilitates a range of video editing tasks, including **3)** geometry editing and **4)** appearance editing. Thirdly, it also proves useful in video interpolation, allowing for **5)** the generation of smooth transitions between frames. Finally, as our representation is inherently 3D, it opens up additional possibilities, such as **6)** novel view synthesis (to a certain extent) and **7)** the creation of stereoscopic videos.

## 2 Related Work

As our method utilizes dynamic 3D Gaussians to represent videos and supports versatile video processing, this section introduces related works on video editing, tracking, and dynamic Gaussian splatting. We briefly cover the most relevant works. For additional references, see Sec.A.7.

Figure 1: We propose an approach to convert a video into a Video Gaussian Representation (VGR), which can be used for versatile video processing tasks conveniently.

Video EditingDecomposing videos into layered representations facilitates advanced video editing techniques. Kasten et al.  introduced layered neural atlases, enabling efficient video propagation and editing. Further advancements include deformable sprites , bi-directional warping fields , and innovations in rendering lighting and color details . CoDeF  and GenDeF  focus on multi-resolution hash grids and shallow MLPs for frame-by-frame deformations. Latent diffusion models  and methodologies like ControlVideo , MaskINT , and VidToMe  have also been employed for data-driven video editing.

Video TrackingVideo tracking captures physical motion within video sequences. PIPs  and TAPIR  offer foundational approaches, while CoTracker  uses a sliding-window transformer for tracking. OminiMotion  and MFT  employ neural radiance fields and optical flow fields for dense tracking. State-of-the-art methods like RAFT  and FlowFormer  provide accurate flow estimations but struggle with long-term correspondences.

Dynamic Gaussian SplattingGaussian Splatting  enhances rendering in radiance fields and has been extended to dynamic scenes [26; 57; 51]. Methods like SC-GS  and 3DGStream  offer novel approaches for scene dynamics. Our method targets **monocular video representation**, eliminating the need for camera pose estimations and facilitating robust long-term tracking and editing in dynamic scenes.

## 3 3D Gaussian Splatting

Gaussian splatting  models 3D scenes using Gaussians learned from multiview images. Each Gaussian, \(G\), is defined by a center \(\) and a covariance matrix \(\): \(G(x)=(x-)^{T}^{-1}(x-))}\). Here, \(\) is decomposed into \(RSS^{T}R^{T}\) for optimization, with \(R\) as a rotation matrix parameterized by a quaternion \(q\) and \(S\) as a scaling matrix parameterized by a vector \(s\). Each Gaussian also has an opacity \(\) and spherical harmonic (\(\)) coefficients \(sh\). Then 3D Gaussians can be formulated as: \(=\{G_{j}:_{j},q_{j},s_{j},_{j},sh_{j}\}\). Rendering is done via:

\[C(u)=_{i N}T_{i}_{i}(sh_{i},v_{i}),T_{i}=_{j=1}^{i -1}(1-_{j}),\] (1)

where \(_{i}\) is calculated by projecting Gaussian \(G_{i}\) at the rendering pixel and \(v\) is the direction from view point to the Gaussian. Optimizing parameters \(\{G_{j}:_{j},q_{j},s_{j},_{j},sh_{j}\}\) and adjusting densities allows for high-quality, real-time image synthesis. For a more detailed introduction to Gaussian Splatting, please refer to Sec. A.8. We extend 3D Gaussians to represent a video by adding attributes to Gaussians for versatile processing.

Figure 2: **Pipeline of our approach. Given a video, we represent its intricate 3D content using video Gaussians in the camera coordinate space. By associating them with motion parameters, we enable video Gaussians to capture the video dynamics. These video Gaussians are supervised by RGB image frames and 2D priors such as optical flow, depth, and label masks. This representation makes it convenient for users to perform various editing tasks on the video.**

Method

Given a video, our goal is to use 3D Gaussians in a canonical space to represent its appearance and associate Gaussians with 3D motions for video dynamics. To facilitate this mapping, we incorporate 2D priors extracted from existing 2D models and apply 3D motion regularization. This representation allows us to efficiently perform various downstream applications. The pipeline of our method is depicted in Fig. 2. In the following, we elaborate on the video Gaussian representation in Sec.4.1. Then, we discuss the learning objectives and optimization details in Sec.4.2 and Sec. 4.3, respectively.

### Video Gaussian Representation

**Camera Coordinate Space** Instead of utilizing an absolute 3D world coordinate system, we opt for the orthographic camera coordinate system to model a video's 3D structure, as demonstrated in Omnimotion . In this space, the video's width, height, and depth correspond to the \(X\), \(Y\), and \(Z\) axes, respectively. This enables us to circumvent the challenges associated with estimating camera poses or disentangling camera motion from scene dynamics, which can be not only time-consuming [40; 41] but also prone to failure in casually captured monocular videos with dynamic objects [34; 59]. By modeling the scene as dynamic 3D Gaussians in the camera coordinate space, we intertwine camera motion with object motion and treat them as the same type of motion, eliminating the need for camera calibration. During the rendering process, the 3D Gaussians in the camera coordinate space are rasterized into images from an identity pose camera. This approach simplifies the representation of dynamics and avoids the challenges of estimating camera pose from monocular casual videos.

**Video Gaussians** Given a video \(=\{I_{1},I_{2},,I_{n}\}\) consisting of \(n\) frames, our video Gaussian representation transforms it into a set of dynamic 3D Gaussians, parameterized as \(=\{G_{1},G_{2},,G_{m}\}\), to simultaneously represent the appearance and motion dynamics of the video. Each Gaussian is characterized by its position \(\), rotation quaternion \(q\), scale \(s\), spherical harmonics (\(\)) coefficients of appearance \(sh\), and opacity \(\). In addition to these fundamental Gaussian properties for appearance, dynamic attributes \(p\), segmentation labels \(m\), and image features \(f\) from any 2D base models (e.g., DINOv2  and SAM ) can also be associated with 3D Gaussians to depict the video's scene content. Consequently, a Gaussian can be expressed as \(G=(,q,s,,sh,p,m,f)\). To learn these properties from a video, we enhance the differentiable 3D Gaussian renderer to render additional attributes beyond simple color, which we denote as \((,q,s,,x)\), where \(x\) represents the specific attribute to be rendered. The rendering function \(\) follows the same procedure as color rendering in the original Gaussian Splatting method .

**Gaussian Dynamics** When parameterizing motion, there is a trade-off between incorporating more regularization from motion priors and achieving high fitting capability . In line with recent popular methods [21; 18], we employ a flexible set of hybrid bases comprising polynomials  and Fourier series  to model smooth 3D trajectories. Specifically, we assign learnable polynomial and Fourier coefficients to each Gaussian, denoted as \(p=\{p_{p}^{n}\}\{p_{}^{l},p_{}^{l}\}\), respectively. Here, \(n\) and \(l\) represent the order of coefficients. The position of a Gaussian at time \(t\) can then be determined as follows:

\[(t)=_{0}+_{n=0}^{N}p_{p}^{n}t^{n}+_{l=0}^{L}(p_{}^{l}( lt)+p_{}^{l}(lt)).\] (2)

Polynomial bases \(\{t^{n}\}\) are effective in modeling overall trends and local non-periodic variations in motion trajectories and are widely used in curve representation, such as in Bezier and B-spline curves [32; 7]. Fourier bases \(\{(lt),(lt)\}\) offer a frequency domain parameterization of curves, making them suitable for fitting smooth movements , and excel in capturing periodic motion components. The combination of these two bases leverages the strengths of both, providing comprehensive modeling, enhanced flexibility and accuracy, reduced overfitting, and robustness to noise. This equips Gaussians with the adaptability to fit various types of trajectories by adjusting the corresponding learnable coefficients. It is important to note that for each Gaussian, the associated parameters \(p=\{p_{p}^{n}\}\{p_{}^{l},p_{}^{l}\}\) are learned from the video by optimizing the learning objective as described in Sec. 4.3.

[MISSING_PAGE_FAIL:5]

### Optimization

In addition to 2D priors and 3D regularization for learning 3D motion and geometry, we also incorporate a color rendering loss for appearance learning. Furthermore, we introduce an optional mask loss to facilitate the separation of background and foreground, which is particularly useful for editing applications.

**Color Rendering Loss** Video Gaussian representation also learns to fit the color of video frames \(\{I_{gt}^{t}\}\) as in novel view synthesis methods  with the rendering loss:

\[_{}=_{t}(||((t),q,s, ,(sh,v))-I_{gt}^{t}||).\] (7)

**Mask Loss** Segmentation labels serve as a crucial attribute for pixels, enabling the identification of groups of pixels belonging to foreground objects. In our experiments, we separate pixels into foreground and background components by segmenting each frame and extracting the foreground mask \(^{t}\). This mask is subsequently lifted to Gaussian, where it is associated with a learnable label attribute \(m\{0,1\}\). The label attributes of Gaussians are supervised by the image segmentation results:

\[_{}=_{t}(||((t),q,s, ,m)-^{t}||_{2}^{2}).\] (8)

With the segmentation label, we can divide Gaussians into different parts and constrain their motion respectively, as shown in Eq. 5. Our approach can also manipulate (remove/duplicate) and edit specific objects in a video, as shown in Fig 7.

**Total Learning Objective** The total learning objective is the weighted sum of all the losses:

\[=_{}_{}+_{ }_{}+_{}_{ }+_{}_{}+_{}_{}.\] (9)

**Adaptive Density Control** We initialize the video Gaussians by uniformly sampling points in the camera coordinate space of the first frame, and apply a similar density control strategy as in vanilla Gaussian Splatting . For more details, please refer to Sec. A.1.

## 5 Video Processing Applications

With our video Gaussian representation, we can perform various video processing tasks, including **1)** dense tracking, **2)** consistent depth/feature prediction, **3)** geometry editing, **4)** appearance editing, **5)** frame interpolation, **6)** novel view synthesis, and **7)** stereoscopic video creation. In this section, we detail these applications, highlighting the versatility of video Gaussians.

**Dense Tracking** Since the scene motion is captured by the dynamics of video Gaussians, we can project these dynamics onto the image plane as UV flow and rasterize the attributes as flow maps. This method handles both short and long-frame gaps effectively. The pixel flow map \(dU_{t_{1} t_{2}}\) from \(t_{1}\) to \(t_{2}\) is calculated as:

\[dU_{t_{1} t_{2}}=((t_{1}),q,s,,((t_{2}))-(( t_{1}))).\] (10)

The rendered dense flow map provides pixel correspondences, facilitating tracking across frames.

**Consistent Depth/Feature Prediction** Video Gaussians, supervised by monocular depth priors for each frame, conform to a reasonable geometry layout, providing consistent depth predictions across frames. Similarly, other image features can be distilled into video Gaussians; unifying per-frame features into a consistent 3D form. To distill image features (e.g., SAM  or DINOv2 ), we associate each video Gaussian with a feature attribute \(f\) and rasterize them to match the feature map \(\{_{gt}^{t}\}\) from 2D models:

\[_{}=_{t}(||((t),q,s, ,f)-_{gt}^{t}||_{2}^{2}).\] (11)

Optimizing video Gaussians with \(_{}\) unifies frame-wise 2D features in a 3D form, enabling the rendering of view-consistent feature maps \(\{_{t}\}\):

\[_{t}=((t),q,s,,f).\] (12)

Consistent feature prediction is crucial for applications like video segmentation and re-identification.

**Geometry Editing** In the unified 3D space, geometry editing is straightforward. By distilling segmentation labels into video Gaussians, we can select Gaussians of the target identity and transform their positions \(\), quaternions \(q\), and scales \(s\) for translation, resizing, and rotation. Adjusting their opacities changes the transparency of the edited objects. It also facilitates easy object removal within a video and supports object copying both between and within videos.

**Appearance Editing** Appearance editing with video Gaussians can also be easily achieved. Users can select a specific frame \(t\) and perform painting, recoloring, or stylization. We fix all attributes except the \(\) coefficients representing Gaussian appearance and optimize them to fit the edited image \(I^{t}_{}\) using:

\[_{}=||((t),q,s,,(sh,v))-I ^{t}_{}||_{2}^{2}.\] (13)

The edited results can propagate throughout the video, maintaining temporal consistency.

**Frame Interpolation** The learned smooth trajectories of video Gaussians enable interpolation of scene dynamics at any up-sampling rate. Interpolated Gaussians' dynamic attributes can render interpolated video frames. By re-mapping the timestep values \(\{t\}\{t^{}\}\) with an arbitrary continuous function, we can freely adjust the video playback speed.

**Novel View Synthesis** Applying a global rigid transformation \((3)\) to video Gaussians allows for camera position adjustments. The rendering results of transformed Gaussians \((((t)),(q),s,,(sh,v))\) provide synthesized views from different perspectives.

**Stereoscopic Video Creation** Similar to the novel view synthesis application, we can achieve stereoscopic frames by slightly translating video Gaussians horizontally by a fixed distance, representing the interocular distance. This application is crucial in filmmaking and gaming.

## 6 Experiments

**Evaluation** We conducted experiments on the DAVIS dataset  as well as some videos used by Omnimotion  and CoDeF . Our approach is evaluated based on two criteria: 1) reconstructed video quality and 2) downstream video processing tasks. In addition to general video representation methods Deformable Sprites , Omnimotion  and CoDeF , we also compare with dynamic

Figure 4: Dense tracking results on diverse complex motion patterns.

Figure 3: Qualitative comparison of video reconstruction using our method and SOTA methods.

NeRF/3DGS methods, namely 4DGS  and RoDynRF . Note that for 4DGS, we estimate camera poses using monocular depth estimation method Unidepth  and DROID-SLAM . Despite these efforts, the performance remains unsatisfactory, further highlighting the challenges of accurate camera pose estimation in causal dynamic videos. In contrast, our approach demonstrates the capability to handle more complex motions and achieves significantly higher reconstruction quality. For downstream tasks, our method also shows comparable performance to those specifically designed for these tasks.

**Video Reconstruction** To demonstrate our method's fitting ability for casual videos, we compare it with Omnimotion  and CoDeF . Omnimotion tends to render blurred results due to the smooth bias of the MLP when modeling the canonical space, while CoDeF struggles with complex motions due to the limited representation ability of the 2D canonical image. We report the rendering quality metrics and visualizations on the Tap-Vid DAVIS dataset  in Table 1 and Figure 3. More comparison with RyDynRF  and 4DGS  are provided in the supplementary materials.

### Video Processing Applications

**Dense Tracking.** Our approach enables dense tracking by projecting the dynamics of Gaussians onto 2D image planes to obtain correspondences. Tracking results are visualized in Fig. 4 and evaluated in Table 1. Despite Omnimotion's specialization in tracking, our approach supports a wider array of video processing tasks with higher computational and training efficiencies. It achieves comparable results with better reconstruction quality using fewer resources. Tracking performance comparisons with similar-cost methods (CoDeF / 4DGS) are shown in the supplementary materials 11, highlighting our superior outcomes.

**Consistent Depth / Feature Generation.** We present the results of consistent video depth and features (using SAM ) in Fig. 5, compared to per-frame prediction. Due to the unified 3D representation of video frames, the predicted depth and features exhibit significantly better consistency than those obtained from monocular predictions. We also evaluate the effectiveness of consistent SAM feature in the segmentation task in the supplementary materials 15. We recommend that readers watch the supplemental videos for better illustrations.

**Geometry Editing** By manipulating the Gaussians associated with specific labels, we can achieve geometric editing of target identities, as demonstrated in Fig. 7. By deleting foreground Gaussians, we can remove foreground elements and render a clean background. Our approach also supports geometric edits such as duplicating, resizing, and translating. Additionally, the motion of these elements can be adjusted by setting different motion attributes.

**Appearance Editing** Users can edit the appearance in a specific frame by drawing, stylizing, or recoloring, and these edits will be propagated across the entire video with cross-frame consistency. In

   Methods & PSNR\(\) & SSIM\(\) & LPIPS\(\) & AJ\(\) & \(^{x}_{avg}\) & \(\) & OA\(\) & TC\(\) & Training Time & GPU Memory & FPS\(\) \\ 
4DGS  & 18.12 & 0.5735 & 0.5130 & 5.1 & 10.2 & 75.45 & 8.11 & \(-\)40 mins & 10G & 145.8 \\ RoDynRF  & 24.79 & 0.723 & 0.394 & / & / & / & / & \(>\)24 hours & 24G & \(>\)1min \\ Deformable Sprites  & 22.83 & 0.6983 & 0.3014 & 20.6 & 32.9 & 69.7 & 2.07 & \(>\)30 mins & 24G & 1.6 \\ Omnimotion  & 24.11 & 0.7145 & 0.3713 & **51.7** & **67.5** & **85.3** & **0.74** & \(>\)24 hours & 24G & \(>\)1min \\ CoDeF  & 26.17 & 0.8160 & 0.2905 & 7.6 & 13.7 & 78.0 & 7.56 & \(-\)30 mins & 10G & 8.8 \\ Ours & **28.63** & **0.8373** & **0.2283** & 41.9 & 57.7 & 79.2 & 1.82 & \(-\)30 mins & 10G & 149 \\   

Table 1: Comparison with existing methods on Tap-Vid benchmark (DAVIS).

Figure 5: Qualitative comparison of video depth and features generated by our method and SOTA single-frame estimation methods. Our method yields more consistent estimations.

Fig.6, we demonstrate appearance editing using ControlNet. Appearance editing is user-friendly in our representation, as it only requires single-frame editing.

**Novel View Synthesis & Stereoscopic Video Creation** Benefitting from depth regularization, the 3D Gaussians maintain a meaningful 3D structure, even from a monocular video. This facilitates novel view synthesis tasks, with examples provided in the supplemental video. Stereoscopic videos can also be produced, as shown in Fig. 8.

### Ablation Study.

We perform ablation studies to validate the importance of the proposed modules, including camera model (perspective/orthographic), flow loss, depth loss (L2/scale-shift invariant). The results are reported in Table 2.

Using a predefined pinhole camera intrinsic led to unstable optimization, resulting in artifacts in both geometry and appearance. This instability likely stems from the rasterization process, where the denominators of Gaussians' screen coordinates UV include depth, complicating the gradients. Replacing the shift- and scale-invariant depth loss with absolute L2 loss degrades performance, as monocular depth cues are ambiguous regarding scale and shift.

   Methods & Ours & Perspective Camera & w/o Flow Loss & w/o Depth Loss & L2 Depth Loss \\  PSNR \(\) & **29.61** & 22.51 & 25.16 & 29.18 & 28.15 \\ SSIM \(\) & **0.8624** & 0.6908 & 0.6937 & 0.8475 & 0.8214 \\ LPIPS \(\) & **0.1845** & 0.3958 & 0.4724 & 0.2449 & 0.3328 \\   

Table 2: Ablation of each module in our framework.

Figure 6: Appearance editing results using the 2D prompt editing method .

Figure 7: Geometry editing results including object deleting, resizing, copying, and translating.

Moreover, the _depth prior_ is crucial for maintaining the 3D structure of Gaussians. Without the depth prior, Gaussians collapse into a flat 2D plane, hindering novel view synthesis and resembling 2D-layer methods.

We also demonstrate the significance of motion regularization (rigid loss) and the selection of motion coefficients n and l, which effectively suppress unorganized Gaussian motion. Please refer to Sec. A.3 for more details.

### Limitations

Although achieving satisfying performance, there are still some limitations to be enhanced. First, our approach suffers from significant changes in the scene, since large deformation is hard to optimize. Initializing the scene with dynamic point clouds might alleviate this problem. In addition, our approach still relies on existing correspondence estimation methods (e.g., RAFT), which might fail when processing rapid and highly non-rigid motion. Extending this representation to more general scenarios is still worth exploring. We have illustrated two scenarios in Fig 9.

## 7 Conclusion

In this paper, we introduced a novel explicit video Gaussian representation (VGR) based on 3D Gaussians to address the challenges of video processing. By modeling video appearance in a canonical 3D space and associating each Gaussian with time-dependent 3D motion attributes, our approach effectively handles complex motions and occlusions. Leveraging recent advancements in monocular priors, such as optical flow and depth, we lift 2D information into a compact 3D representation, facilitating a wide range of video-processing tasks. Our VGR method demonstrates efficacy in dense tracking, improving monocular 2D priors, video editing, interpolation, novel view synthesis, and stereoscopic video creation, providing a robust and versatile framework for sophisticated video processing applications.