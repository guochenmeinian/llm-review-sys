# Normalizing flow neural networks by JKO scheme

Chen Xu

School of Industrial and

Systems Engineering

Georgia Tech

&Xiuyuan Cheng

Department of Mathematics

Duke University

&Yao Xie

School of Industrial and

Systems Engineering

Georgia Tech

###### Abstract

Normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. The flow is often implemented using a sequence of invertible residual blocks. Existing works adopt special network architectures and regularization of flow trajectories. In this paper, we develop a neural ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. Experiments with synthetic and real data show that the proposed JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.

## 1 Introduction

Generative models have wide applications in statistics and machine learning to infer data-generating distributions and to sample from the model distributions learned from the data. In addition to widely used deep generative models such as variational auto-encoders (VAE) Kingma and Welling (2014, 2019) and generative adversarial networks (GAN) Goodfellow et al. (2014); Gulrajani et al. (2017), normalizing flow models Kobyzev et al. (2020) have been popular and with a great potential. The flow model learns the data distribution via an invertible mapping \(F\) between the data density \(p_{X}\) in \(^{d}\) and the target standard multivariate Gaussian density \(p_{Z}\), \(Z(0,I_{d})\). While flow models, once trained, can be utilized for efficient data sampling and explicit likelihood evaluation, training of such models is often difficult in practice. To alleviate such difficulties, many prior works (Dinh et al., 2015, 2017; Kingma and Dhariwal, 2018; Behrmann et al., 2019; Ruthotto et al., 2020; Onken et al., 2021), among others, have explored designs of training objectives, network architectures, and computational techniques.

Among various flow models, continuous normalizing flow (CNF) transports the data density to a target distribution through continuous dynamics, primarily neural ODE (Chen et al., 2018) models. CNF

Figure 1: Comparison of JKO-iFlow (proposed) and standard CNF models. In contrast to most existing CNF models, JKO-Flow learns the unique deterministic transport equation corresponding to the diffusion process by directly performing block-wise training of a neural ODE model.

models have shown promising performance on generative tasks Grathwohl et al. (2019); Kobyzev et al. (2020). However, a known computational challenge of CNF models is model regularization, primarily due to the non-uniqueness of the flow transport. Without additional regularization, the trained CNF model such as FFJORD Grathwohl et al. (2019) may follow a less regular trajectory in the probability space, see Figure 1(b), which may worsen the generative performance. While regularization of flow models has been developed using different techniques, including using spectral normalization Behrmann et al. (2019) and optimal transport Liutkus et al. (2019); Onken et al. (2021); Finlay et al. (2020); Xu et al. (2022); Huang et al. (2023), only using regularization may not resolve the non-uniqueness of the flow. Besides regularization, several practical difficulties remain when training CNF models, particularly the high computational cost. In many settings, CNF models consist of stacked blocks, and each can be complex. _End-to-end training_ of such deep models often places a high demand on computational resources and memory consumption.

In this work, we propose JKO-iFlow, an invertible normalizing flow network that unfolds the Wasserstein gradient flow via a neural ODE model inspired by the JKO-scheme Jordan et al. (1998). The JKO scheme, see (5), can be viewed as a proximal step to minimize the Kullback-Leibler (KL) divergence between the current density and the equilibrium density. It recovers the solution of the Fokker-Planck equation (FPE) in the limit of small step size. The proposed JKO-iFlow model thus can be viewed as trained to learn the _unique_ transport map following the FPE which flows from the data distribution toward the normal equilibrium and gives a smooth trajectory of density evolution, see Figure 1(a). Unlike most CNF models, where all the residual blocks are trained end-to-end, each block in the JKO-iFlow network implements one step in the JKO scheme to learn the deterministic transport map by minimizing an objective of that block given the trained previous blocks. The block-wise training significantly reduces memory and computational load. Theoretically, with a small step size, the discrete-time transport map approximates the continuous solution of FPE, which leads to the invertibility of each trained residual block. This leaves the residual blocks to be completely general, such as graph neural network layers and convolutional neural network layers, depending on the structure of data considered in the problem. The theoretical need for small step sizes does not incur a restriction in practice, whereby one can use step size not exceeding a certain maximum value when adopting the neural ODE integration. We further introduce time reparameterization with progressive refinement in computing the flow network, where each block corresponds to a representative point along the density evolution trajectory in the space of probability measures. The algorithm adaptively chooses the number of blocks and step sizes.

The proposed approach is related to the diffusion models (Song and Ermon, 2019; Ho et al., 2020; Block et al., 2020; Song et al., 2021) yet differs fundamentally in that our approach is a type of flow models, which directly computes the data likelihood, and such likelihood estimation is essential for statistical inference. While the diffusion models can also indirectly obtain likelihood estimation, they are more designed as samplers. In terms of implementation, our approach trains a neural ODE model without SDE sampling (injection of noise) nor learning of score matching. It also differs from previous works on progressive training of ResNet generative models Johnson and Zhang (2019); Fan et al. (2022) in that the model trains an invertible flow mapping and avoids inner loops of variational learning. We refer to Section 1.1 for more discussions on related works. Empirically, JKO-iFlow yields competitive performance as other CNF models with significantly less computation. The model is also compatible with general equilibrium density \(p_{Z}\) having a parametrized potential \(V\), exemplified by the application to the conditional generation setting where \(p_{Z}\) is replaced with Gaussian mixtures Xu et al. (2022). In summary, the contributions of the work include

\(\) We propose an invertible neural ODE model where each residual block corresponds to a JKO step, and the training objective can be computed from pushed data samples through the previous blocks. The residual block has a general form, and the invertibility is ensured due to the regularity and continuity of the approximate solution of the FPE.

\(\) We develop a block-wise procedure to train the JKO-iFlow model, which can adaptively determine the number of blocks. We also propose to adaptively reparameterize the computed trajectory in the probability space with refinement, which improves the model accuracy and the overall computational efficiency.

\(\) We show that JKO-iFlow greatly reduces memory and computational cost when achieving competitive or better generative performance and likelihood estimation compared to existing flow and diffusion models on simulated and real data.

### Related works

For deep generative models, popular approaches include generative adversarial networks (GAN) (Goodfellow et al., 2014; Gulrajani et al., 2017; Isola et al., 2017) and variational auto-encoder (VAE)(Kingma and Welling, 2014, 2019). Apart from known training difficulties (e.g., mode collapse (Salimans et al., 2016) and posterior collapse (Lucas et al., 2019)), these models do not provide likelihood or inference of data density. The normalizing flow framework (Kobyzev et al., 2020) has been extensively developed, including continuous flow (Grathwohl et al., 2019), Monge-Ampere flow (Zhang et al., 2018), discrete flow (Chen et al., 2019), and extension to non-Euclidean data (Liu et al., 2019; Mathieu and Nickel, 2020; Xu et al., 2022). Efforts have been made to develop novel invertible mapping structures (Dinh et al., 2017; Papamakarios et al., 2017) and regularize the flow trajectories by transport cost (Finlay et al., 2020; Onken et al., 2021; Ruthotto et al., 2020; Xu et al., 2022; Huang et al., 2023). Despite such efforts, the model and computational challenges of normalizing flow models include regularization and the large model size when using a large number of residual blocks, which cannot be determined a priori, and the associated memory and computational load.

In parallel to continuous normalizing flows, which are neural ODE models, neural SDE models become an emerging tool for generative tasks. Diffusion process and Langevin dynamics in deep generative models have been studied in score-based generative models (Song and Ermon, 2019; Ho et al., 2020; Block et al., 2020; Song et al., 2021) under different settings. Specifically, these models estimate the score function (i.e., the gradient of the log probability density with respect to data) of data distribution via neural network parametrization, which may encounter challenges in learning and sampling high dimensional data and call for special techniques (Song and Ermon, 2019). The recent work of Song et al. (2021) developed reverse-time SDE sampling for score-based generative models and adopted the connection to neural ODE to compute the likelihood; using the same idea of backward SDE, Zhang and Chen (2021) proposed joint training of forward and backward neural SDEs. Theoretically, latent diffusion Tzen and Raginsky (2019, 2020) was used to analyze neural SDE models. The current work focuses on a neural ODE model where the deterministic vector field \((x,t)\) is to be learned from data following a JKO scheme of the FPE. Rather than neural SDE, our approach involves no sampling of SDE trajectories nor learning of the score function, and it learns an invertible residual network directly. In contrast, diffusion-based models derive the ODE model from the learned diffusion model to achieve explicit likelihood computation. For example, Song et al. (2021) derived neural ODE model from the learned score function of the diffused data marginal distributions for all \(t\). We experimentally obtain competitive or improved performance against the diffusion model on simulated two-dimensional and high-dimensional tabular data.

JKO-inspired deep models have been studied in several recent works. (Bunne et al., 2022) reformulated the JKO step for minimizing an energy function over convex functions. JKO scheme has also been used to discretize Wasserstein gradient flow to learn a deep generative model in (Alvarez-Melis et al., 2022; Mokrov et al., 2021), which adopted input convex neural networks (ICNN) (Amos et al., 2017). ICNN, as a special type of network architecture, may have limited expressiveness (Rout et al., 2022; Korotin et al., 2021). In addition to using the gradient of ICNN, (Fan et al., 2022) proposed parametrizing the transport in a JKO step by a residual network but identified difficulty in calculating the push-forward distribution. The approach in (Fan et al., 2022) also relies on a variational formulation, which requires training an additional network similar to the discriminator in GAN using inner loops. The idea of progressive additive learning in training generative ResNet, namely training ResNet block-wisely by a variational loss, dates back to Johnson and Zhang (2019) under the GAN framework. Our method trains an invertible neural-ODE flow network that flows from data density to the normal one and backward, which enables the computation of model likelihood as in other neural-ODE approaches. The objective in each JKO step to minimize KL divergence can also be computed directly without any inner-loop training, see Section 4.

Compared to score-based neural SDE methods, our approach is closer to the more recent flow-based models related to diffusion models (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Boffi and Vanden-Eijnden, 2023). These works proposed to learn the transport equation (a deterministic ODE) corresponding to the SDE process. Specifically, (Lipman et al., 2023; Albergo and Vanden-Eijnden,2023] matched the velocity field from interpolated distributions between initial and terminal ones; [Boffi and Vanden-Eijnden, 2023] proposed to learn the score \(_{t}(x)\) (where \(_{t}\) solves the FPE and is unknown _a priori_) to solve high-dimensional FPE. Using Stein's identity (which is equivalent to the derivation in Section 3.2), the step-wise training objective in [Boffi and Vanden-Eijnden, 2023] optimizes to learn the score without the need to simulate the entire SDE. The idea of approximating the solution of FPE by a deterministic transport equation dates back to the 90s Degond and Mustieles , Degond and Mas-Gallic , and has been used in kernel-based solver of FPE in [Maoutsa et al., 2020] and studied via a self-consistency equation in [Shen et al., 2022]. While our approach learns an equivalent velocity field at the infinitesimal time step, see section 3.2, the formulation in JKO-iFlow is at a finite time-step motivated by the JKO scheme. We also mention that an independent concurrent work [Vidal et al., 2023] proposed a similar block-wise training algorithm using the JKO scheme under the framework of [Onken et al., 2021] and demonstrated benefits in avoiding tuning the penalty hyperparameter associated with the KL-divergence objective. Our model is applied to generative tasks of high-dimensional data, including image data, and we also develop additional techniques for computing the flow probability trajectory.

For the expressiveness of deep generative models, approximation properties of deep neural networks for representing probability distributions have been developed in several works. Lee et al.  established approximation by composition of Barron functions [Barron, 1993]; Bailey and Telgarsky  developed space-filling approach, which was generalized in Perekrestenko et al. ; Lu and Lu  constructed a deep ReLU network with guaranteed approximation under integral probability metrics, using techniques of empirical measures and optimal transport. These results show that deep neural networks can provably transport one source distribution to a target one with sufficient model capacity under certain regularity conditions of the pair of densities. Our JKO-iFlow model potentially leads to a constructive approximation analysis of the neural ODE flow model.

## 2 Preliminaries

_Normalizing flow_. A normalizing flow can be mathematically expressed via a density evolution equation of \((x,t)\) such that \((x,0)=p_{X}\) and as \(t\) increases \((x,t)\) approaches \(p_{Z}(0,I_{d})\)[Tabak and Vanden-Eijnden, 2010]. Given an initial distribution \((x,0)\), such a flow typically is not unique. We consider when the flow is induced by an ODE of \(x(t)\) in \(^{d}\)

\[(t)=(x(t),t),\] (1)

where \(x(0) p_{X}\). The marginal density of \(x(t)\) is denoted as \(p(x,t)\), and it evolves according to the continuity equation (Liouville equation) of (1) written as

\[_{t}p+(p)=0, p(x,0)=p_{X}(x).\] (2)

_Ornstein-Uhlenbeck (OU) process_. Consider a Langevin dynamic denoted by the SDE \(dX_{t}=- V(X_{t})dt+dW_{t}\), where \(V\) is the potential of the equilibrium density \(p_{Z}\). We focus on the case of normal equilibrium, that is, \(V(x)=|x|^{2}/2\) and then \(p_{Z} e^{-V}\). In this case, the process is known as the (multivariate) OU process. Suppose \(X_{0} p_{X}\), and let the density of \(X_{t}\) be \((x,t)\) also denoted as \(_{t}()\). The Fokker-Planck equation (FPE) describes the evolution of \(_{t}\) towards the equilibrium \(p_{Z}\) as follows, where \(V(x):=|x|^{2}/2\),

\[_{t}=( V+),(x,0)=p_{X}(x).\] (3)

Under generic conditions, \(_{t}\) converges to \(p_{Z}\) exponentially fast. For Wasserstein-2 distance and the standard normal \(p_{Z}\), classical argument gives that (take \(C=1\) in Eqn (6) of Bolley et al. )

\[W_{2}(_{t},p_{Z}) e^{-t}W_{2}(_{0},p_{Z}), t>0.\] (4)

_JKO scheme_. The seminal work Jordan et al.  established a time discretization scheme of the solution to (3) by the gradient flow to minimize \((||p_{Z})\) under the Wasserstein-2 metric in probability space. Denote by \(\) the space of all probability densities on \(^{d}\) with a finite second moment. The JKO scheme computes a sequence of distributions \(p_{k}\), \(k=0,1,,\) starting from \(p_{0}=_{0}\). With step size \(h>0\), the scheme at the \(k\)-th step is written as

\[p_{k+1}=_{}F[]+W_{2}^{2}(p_{k},),\] (5)

where \(F[]:=(||p_{Z})\). It was proved in Jordan et al.  that as \(h 0\), the solution \(p_{k}\) converges to the solution \((,kh)\) of (3) for all \(k\), and the convergence \(_{(h)}(,t)(,t)\) is strongly in \(L^{1}(^{d},(0,T))\) for finite \(T\) where \(_{(h)}\) is piece-wise constant interpolated from \(p_{k}\).

JKO scheme by neural ODE

Given i.i.d. observed data samples \(X_{i}^{d}\), \(i=1,,N\), drawn from some unknown density \(p_{X}\), the goal is to train an invertible neural network to transport the density \(p_{X}\) to an _a priori_ specified density \(p_{Z}\) in \(^{d}\), where each data sample \(X_{i}\) is mapped to a code \(Z_{i}\). A prototypical choice of \(p_{Z}\) is the standard multivariate Gaussian \((0,I_{d})\). In this work, we leave the potential of \(p_{Z}\) abstract and denote by \(V\), that is, \(p_{Z} e^{-V}\) and \(V(x)=|x|^{2}/2\) for normal \(p_{Z}\). By a slight abuse of notation, we denote by \(p_{X}\) and \(p_{Z}\) both the distributions and the density functions of data \(X\) and code \(Z\) respectively.

### Objective of JKO step

We are to specify \((x,t)\) in the ODE (1), to be parametrized and learned by a neural ODE, such that the induced density evolution of \(p(x,t)\) converges to \(p_{Z}\) as \(t\) increases. We start by dividing the time horizon \([0,T]\) into finite subintervals with step size \(h\), let \(t_{k}=kh\) and \(I_{k+1}:=[t_{k},t_{k+1})\). Define \(p_{k}(x):=p(x,kh)\), namely the density of \(x(t)\) at \(t=kh\). The solution of (1) determined by the vector-field \((x,t)\) on \(t I_{k+1}\) (assuming the ODE is well-posed (Sideris, 2013)) gives a one-to-one mapping \(T_{k+1}\) on \(^{d}\), s.t. \(T_{k+1}(x(t_{k}))=x(t_{k+1})\) and \(T_{k+1}\) transports \(p_{k}\) into \(p_{k+1}\), i.e., \((T_{k})_{\#}p_{k-1}=p_{k}\), where we denote by \(T_{\#}p\) the push-forward of distribution \(p\) by \(T\), such that \((T_{\#}p)(A)=p(T^{-1}(A))\) for a measurable set \(A\). In other words, the mapping \(T_{k+1}\) is the solution map of the ODE from time \(t_{k}\) to \(t_{k+1}\).

Suppose we can find \((,t)\) on \(I_{k+1}\) such that the corresponding \(T_{k+1}\) solves the JKO scheme (5), then with small \(h\), \(p_{k}\) approximates the solution to the Fokker-Planck equation 3, which then flows towards \(p_{Z}\). By the Monge formulation of the Wasserstein-2 distance between \(p\) and \(q\) as \(W_{2}^{2}(p,q)=_{T:T_{\#}p_{k}=q}_{ p}\|x-T(x)\|^{2}\), solving for the transported density \(p_{k}\) by (5) is equivalent to solving for the transport \(T_{k+1}\) by

\[T_{k+1}=_{T:^{d}^{d}}F[T]+_{x p_{k}}\|x-T(x)\|^{2},\] (6)

where \(F[T]=(T_{\#}p_{k}\|p_{Z})\). The equivalence between (5) and (6) is proved in Lemma A.1.

Furthermore, the following proposition gives that, once \(p_{k}\) is determined by \((x,t)\) for \(t t_{k}\), the value of \(F[T]\) can be computed from \((x,t)\) on \(t I_{k+1}\) only. The counterpart for convex function-based parametrization of \(T_{k}\) was given in Theorem 1 of (Mokrov et al., 2021), where the computation using the change-of-variable differs as we adopt an invertible neural ODE approach here. The proof is left to Appendix A.

**Proposition 3.1**.: _Given \(p_{k}\), up to a constant \(c\) independent from \((x,t)\) on \(t I_{k+1}\),_

\[(T_{\#}p_{k}\|p_{Z})=_{x(t_{k}) p_{k}}(V(x(t_{k +1}))-_{t_{k}}^{t_{k+1}}(x(s),s)ds)+c.\] (7)

By Proposition 3.1, the minimization (6) is equivalent to

\[_{\{(x,t)\}_{t I_{k+1}}}_{x(t_{k}) p_{k}} V(x(t_{k+1}))-_{t_{k}}^{t_{k+1}}(x(s),s)ds+ \|x(t_{k+1})-x(t_{k})\|^{2},\] (8)

where \(x(t_{k+1})=x(t_{k})+_{t_{k}}^{t_{k+1}}(x(s),s)ds\). Taking a neural ODE approach, we parametrize \(\{(x,t)\}_{t I_{k+1}}\) as a residual block with parameter \(_{k+1}\), and then (8) is reduced to minimizing over \(_{k+1}\). This leads to a block-wise learning algorithm to be introduced in Section 4, where we further allow the step-size \(h\) to vary for different \(k\) as well.

### Infinitesimal optimal \((x,t)\)

In each JKO step of (8), let \(p=p_{k}\) denote the current density, \(q=p_{Z}\) be the target equilibrium density. In this subsection, we show that the optimal \(\) in (8) with small \(h\) reveals the difference between score functions between target and current densities. Thus, minimizing the objective (8) searches for a neural network parametrization of the score function \(_{t}\) implicitly, in contrast to diffusion-based models which learn the score function explicitly (Ho et al., 2020; Song et al.,2021), e.g., via denoising score matching. At infinitesimal \(h\), this is equivalent to solving the FPE by learning a deterministic transport equation as in (Boffi and Vanden-Eijnden, 2023; Shen et al., 2022).

Consider general equilibrium distribution \(q\) with a differentiable potential \(V\). To analyze the optimal pushforward mapping in the small \(h\) limit, we shift the time interval \([kh,(k+1)h]\) to be \([0,h]\) to simplify the notation. Then (8) is reduced to

\[_{\{(x,t)\}_{t[0,h)}}_{x(0) p}(V(x(h))- _{0}^{h}(x(s),s)ds+\|x(h)-x(0)\|^{2} ),\] (9)

where \(x(h)=x(0)+_{0}^{h}(x(s),s)ds\). In the limit of \(h 0+\), formally, \(x(h)-x(0)=h(x(0),0)+O(h^{2})\), and suppose \(V\) of \(q\) is \(C^{2}\), \(V(x(h))=V(x(0))+h V(x(0))(x(0),0)+O(h^{2})\). For any differentiable density \(\), the (Stein) score function is defined as \(_{}=\), and we have \( V=-_{q}\). Taking the formal expansion of orders of \(h\), the objective in (9) is written as

\[_{x p}(V(x)+h(-_{q}(x)(x,0)- (x,0)+\|(x,0)\|^{2})+O(h^{2}) ).\] (10)

Note that \(_{x p}V(x)\) is independent of \((x,t)\), and the \(O(h)\) order term in (10) is over \((x,0)\) only, thus the minimization of the leading term is equivalent to

\[_{()=(,0)}_{ x p}(-T_{q}+\|\|^{2}), T_{q} :=_{q}+,\] (11)

where \(T_{q}\) is known as the Stein operator (Stein, 1972). The \(T_{q}\) in (11) echoes that the derivative of KL divergence with respect to transport map gives Stein operator (Liu and Wang, 2016). The Wasserstein-2 regularization gives an \(L^{2}\) regularization in (11). Let \(L^{2}(p)\) be the \(L^{2}\) space on \((^{d},p(x)dx)\), and for vector field \(\) on \(^{d}\), \( L^{2}(p)\) if \(|(x)|^{2}p(x)dx<\). One can verify that, when both \(_{p}\) and \(_{q}\) are in \(L^{2}(p)\), the minimizer of (11) is

\[^{*}(,0)=_{q}-_{p}.\]

This shows that the infinitesimal optimal \((x,t)\) equals the difference between the score functions of the equilibrium and the current density.

### Invertibility of flow model and expressiveness

At time \(t\) the current density of \(x(t)\) is \(_{t}\), the analysis in Section 3.2 implies that the optimal vector field \((x,t)\) has the expression as

\[(x,t)=_{q}-_{_{t}}=- V- _{t}.\] (12)

With this \((x,t)\), the Liouville equation (2) coincides with the FPE (3). This is consistent with the JKO scheme with a small \(h\) recovering the solution to the FPE. Under proper regularity condition of \(V\) and the initial density \(_{0}\), the r.h.s. of (12) is also regular over space and time. This leads to two consequences, in approximation and in learning: Approximation-wise, the regularity of \((x,t)\) allows to construct a \(k\)-th residual block in the flow network to approximate \(\{(x,t)\}_{t I_{k}}\) when there is sufficient model capacity, by classical universal approximation theory of shallow networks (Barron, 1993; Yarotsky, 2017). We further discuss the approximation analysis based on the proposed model in the last section.

For learning, when properly trained with sufficient data, the neural ODE vector field \((x,t;_{k})\) will learn to approximate (12). This can be viewed as inferring the score function of \(_{t}\), and also leads to the invertibility of the trained flow net in theory: Suppose the trained \((x,t;_{k})\) is close enough to (12); it will also have bounded Lipschitz constant. Then the residual block is invertible as long as the step size \(h\) is sufficiently small, e.g. less than \(1/L\) where \(L\) is the Lipschitz bound of \((x,t;_{k})\). In practice, we typically use smaller \(h\) than needed merely by invertibility (allowed by the model budget) so that the flow network can more closely track the FPE of the diffusion process. The invertibility of the proposed model is numerically verified in experiments (see Table A.1).

## 4 Training of JKO-iFlow net

The proposed JKO-iFlow model allows progressive learning of the residual blocks in the neural-ODE model in a block-wise manner (Section 4.1). We also introduce two techniques to improve the training of the trajectories in probability space (Section 4.2), illustrated in a vector space in Appendix B.3.

### Block-wise training

Note that the training of \((k+1)\)-th block in (8) can be conducted once the previous \(k\) blocks are trained. Specifically, with finite training data \(\{X_{i}=x_{i}(0)\}_{i=1}^{n}\), the expectation \(_{x(t) p_{k}}\) in (8) is replaced by the sample average over \(\{x_{i}(kh)\}_{i=1}^{n}\) which can be computed from the previous \(k\) blocks. Note that for each given \(x(t)=x(t_{k})\), both \(x(t_{k+1})\) and the integral of \(\) in (8) can be computed by a numerical neural ODE integrator. Following previous works, we use the Hutchinson trace estimator (Hutchinson, 1989; Grathwohl et al., 2019) to compute the quantity \(\) in high dimensions, and we also propose a finite-difference approach to reduce the computational cost (details in Appendix B.2). Applying the numerical integrator in computing (8), we denote the resulting \(k\)-th residual block abstractly as \(f_{_{k}}\) with trainable parameters \(_{k}\).

This leads to a block-wise training of the normalizing flow network, as summarized in Algorithm 1. The sequence of time stamps \(t_{k}\) is given by specifying the time steps \(h_{k}:=t_{k+1}-t_{k}\), which we allow to differ across \(k\). The choice of the sequence \(h_{k}\) is initialized by a geometric sequence starting from \(h_{0}\) with maximum stepsize \(h_{}\), see Appendix B.1. In the special case where the multiplying factor is one, the sequence of \(h_{k}\) gives a constant step size. The adaptive choice of \(h_{k}\) with refinement (by adding more blocks) will be introduced in Section 4.2. Regarding the termination criterion \(()\) in line 2 of Algorithm 1, we monitor the ratio \(_{x p_{k-1}}\|x-T_{k}(x)\|^{2}/_{x p_{k-1}}\|T_{k }(x)\|^{2}\) and terminate when it is below some threshold \(\), set as 0.01 in all experiments. In practice, when training the \(k\)-th block, both the averages of \(\|x-T_{k}(x)\|^{2}\) and \(\|T_{k}(x)\|^{2}\) are computed by empirically averaging over the training samples (in the last epoch) at no additional computational cost. Lastly, line 5 of training a "free block" (i.e., the block without the \(W_{2}\) regularization) is to flow the push-forward density \(p_{L}\) closer to the target density \(p_{Z}\), where the former is obtained through the first \(L\) blocks.

```
0: Time stamps \(\{t_{k}\}\), training data, termination criterion \(\) and tolerance level \(\), maximal number of blocks \(L_{}\).
1: Initialize \(k=1\).
2:while\(()>\) and \(k L_{}\)do
3: Optimize \(f_{_{k}}\) upon minimizing (8) with mini-batch sample approximation, given \(\{f_{_{i}}\}_{i=1}^{k-1}\). Set \(k k+1\).
4:endwhile
5:\(L k\). Optional: Optimize \(f_{_{L+1}}\) without \(W_{2}\) regularization. ```

**Algorithm 1** Block-wise JKO-iFlow training

The block-wise training significantly reduces the memory and computational load since only one block is trained when optimizing (8) regardless of flow depth. Therefore, one can use larger training batches and potentially more expensive numerical integrators within a certain memory budget for higher accuracy. We also empirically observe that training each block using standard back-propagation (rather than the adjoint method in neural ODE) gives a comparable result at a lower cost. To ensure the invertibility of the trained JKO-iFlow network, we further break the time interval \([t_{k-1},t_{k})\) into 3 or 5 subintervals to compute the neural ODE integration, e.g., by Runge-Kutta-4. We empirically verify small inversion errors on test samples.

### Computation of trajectories in probability space

We adopt two additional computational techniques to facilitate learning of the trajectories in the probability space, represented by the sequence of densities \(p_{k}\), \(k=1,,L\), associated with the \(L\) residual blocks of the proposed normalizing flow network. The two techniques are illustrated in Figure 2. Further details and illustrations of the approach can be found in Appendix B.

\(\)_Trajectory reparameterization._ We empirically observe fast decay of the movements \(W_{2}^{2}(T_{\#}p_{k},p_{k})\) when \(h_{k}\) is set to be constant, that is, initial blocks transport the densities much further than the later ones. This is consistent with the exponential convergence of the Fokker-Planck flow, see (4), but unwanted in the algorithm because in order to train the current block, the flow model needs to transport data through all previous blocks, and yet the later blocks trained using constant step size barely contribute to the density transport. Hence, instead of having constant \(h_{k}\), we _reparameterize_ the values of \(t_{k}\) through an adaptive procedure based on the \(W_{2}\) distance at each block. The procedure uses an adaptive approach to encourage the \(W_{2}\) movement in each block to be more even across the \(L\) blocks, where the retraining of the trajectory can be potentially warm-started by the previous trajectory in iteration.

\(\)_Progressive refinement_. The performance of CNF models is typically improved with a larger number of residual blocks, corresponding to a smaller step size. The smallness of stepsize \(h\) also ensures the invertibility of the flow model, in theory and also in practice. However, directly training the model with a small non-adaptive stepsize \(h\) may result in long computation time and convergence to the normal density \(q_{Z}\) only after a large number of blocks, where the choice of \(h_{k}\) is not as efficient as after adaptive reparameterization. We introduce a refinement approach to increase the number of blocks progressively, where each time interval \([t_{k-1},t_{k})\) splits into two halves, and the number of blocks doubles after the adaptive reparameterization of the trajectory converges at the coarse level. The new trajectory at the refined level is again trained with adaptive reparameterization, where the residual blocks can be warm-started from the coarse-level model to accelerate the convergence. The trajectory refinement allows going to a smaller step size \(h_{k}\), which benefits the accuracy of the JKO-iFlow model including the numerical accuracy in integrating each neural ODE block.

## 5 Experiment

In this section, we examine the proposed JKO-iFlow model on simulated and real datasets, including both unconditional and conditional generation tasks. Codes are available at https://github.com/hamrel-cxu/JKO-iFlow.

### Baselines and metrics

We compare five alternatives, including four CNF models and one diffusion model. The first two CNF models are FFJORD (Grathwohl et al., 2019) and OT-Flow (Onken et al., 2021), which are continuous-time flow (neural-ODE models). The next two CNF models are IResNet (Behrmann et al., 2019) and IGNN (Xu et al., 2022), which are discrete in time (ResNet models). The diffusion model baseline is the score-based neural SDE (Song et al., 2021), which we call "ScoreSDE." Details about the experimental setup, including dataset construction and neural network architecture and training can be found in Appendix C.2.

The accuracy of trained generative models is evaluated by two quantitative metrics, the negative log-likelihood (NLL) metric, and the kernel _maximum mean discrepancy_ (MMD) (Gretton et al., 2012a) metric, including MMD-1, MMD-m, and MMD-c for constant, median distance, and custom bandwidth respectively. The test threshold \(\) is computed by bootstrap, where an MMD metric less than \(\) indicates that the generated distribution is evaluated by the MMD test to be the same as the true data distribution (achieving \(=0.05\) test level). See details in Appendix C.1. The computational cost is measured by the number of mini-batch stochastic gradient descent steps and the training

Figure 3: Results on two-dimensional simulated datasets by JKO-iFlow and competitors.

Figure 2: Diagram illustrating trajectory reparameterization and refinement. Top: the original trajectory under three blocks via Algorithm 1. Bottom: the trajectory under six blocks after reparameterization and refinement, which renders the \(W_{2}\) movements more even.

[MISSING_PAGE_FAIL:9]

FIDs compared to most CNF baselines (Grathwohl et al., 2019; Behrmann et al., 2019; Chen et al., 2019; Finlay et al., 2020).

### Conditional generation

The problem aims to generate input samples \(X\) given a label \(Y\) from the conditional distribution \(X|Y\) to be learned from data. We follow the approach in IGNN (Xu et al., 2022). In this setting, the JKO-iFlow network pushes from the distribution \(X|Y=k\) to the class-specific component in the Gaussian mixture of \(H|Y=k\), see Figure A.7b and Appendix C.2.4 for more details. We apply JKO-iFlow to the Solar ramping dataset and compare it with the original IGNN model, and both models use graph neural network layers in the residual blocks. The results are shown in Figure A.8, where both the NLL and MMD-m metrics indicate the superior performance of JKO-iFlow and is consistent with the visual comparison.

## 6 Discussion

The work can be extended in several directions. The application to larger-scale image datasets and larger graphs will enlarge the scope of usage. To overcome the computational challenge faced by neural ODE models for high dimensional input, e.g., images of higher resolution, one would need to improve the training efficiency of the backpropagation in neural ODE in addition to the dimension reduction techniques by VAE as been explored here. Another possibility is to combine the JKO-iFlow scheme with other backbone flow models that are more suitable for the specific tasks. Meanwhile, it would be interesting to extend the method to other problems for which CNF models have proven to be effective. Examples include multi-dimensional probabilistic regression (Chen et al., 2018), a plug-in to deep architectures such as StyleFlow (Abdal et al., 2021), and the application to Mean-field Games (Huang et al., 2023).

Theoretically, the expressiveness of the flow model to generate a regular data distribution can be analyzed based on Section 3.3. To sketch a road map, a block-wise approximation guarantee of \((x,t)\) as in (12) can lead to approximation of the Fokker-Planck flow (3), which pushes forward the density to be \(\)-close to normality in \(T=(1/)\) time, see (4). Reversing the time of the ODE then leads to an approximation of the initial density \(_{0}=p_{X}\) by flowing backward in time from \(T\) to zero. Further analysis under technical assumptions is left to future work. During the time that this paper was being published, the convergence analysis of the proposed model was studied in Cheng et al. (2023).

Figure 4: Generated samples of MNIST, CIFAR10, and Imagenet-32 by JKO-iFlow model in latent space. We select 2 images per class for CIFAR10 and 1 image per class for Imagenet-32. The FIDs are shown in subxaptions. Uncurated samples are shown in Figure A.6.