# Correlation Aware Sparsified Mean Estimation Using Random Projection

Shuli Jiang

Robotics Institute

Carnegie Mellon University

shulij@andrew.cmu.edu

&Pranay Sharma

ECE

Carnegie Mellon University

pranaysh@andrew.cmu.edu

&Gauri Joshi

ECE

Carnegie Mellon University

gaurij@andrew.cmu.edu

###### Abstract

We study the problem of communication-efficient distributed vector mean estimation, a commonly used subroutine in distributed optimization and Federated Learning (FL). Rand-\(k\) sparsification is a commonly used technique to reduce communication cost, where each client sends \(k<d\) of its coordinates to the server. However, Rand-\(k\) is agnostic to any correlations, that might exist between clients in practical scenarios. The recently proposed Rand-\(k\)-Spatial estimator leverages the cross-client correlation information at the server to improve Rand-\(k\)'s performance. Yet, the performance of Rand-\(k\)-Spatial is suboptimal. We propose the Rand-Proj-Spatial estimator with a more flexible encoding-decoding procedure, which generalizes the encoding of Rand-\(k\) by projecting the client vectors to a random \(k\)-dimensional subspace. We utilize Subsampled Randomized Hadamard Transform (SRHT) as the projection matrix and show that Rand-Proj-Spatial with SRHT outperforms Rand-\(k\)-Spatial, using the correlation information more efficiently. Furthermore, we propose an approach to incorporate varying degrees of correlation and suggest a practical variant of Rand-Proj-Spatial when the correlation information is not available to the server. Experiments on real-world distributed optimization tasks showcase the superior performance of Rand-Proj-Spatial compared to Rand-\(k\)-Spatial and other more sophisticated sparsification techniques.

## 1 Introduction

In modern machine learning applications, data is naturally distributed across a large number of edge devices or clients. The underlying learning task in such settings is modeled by distributed optimization or the recent paradigm of Federated Learning (FL) . A crucial subtask in distributed learning is for the server to compute the mean of the vectors sent by the clients. In FL, for example, clients run training steps on their local data and once-in-a-while send their local models (or local gradients) to the server, which averages them to compute the new global model. However, with the ever-increasing size of machine learning models , and the limited battery life of the edge clients, communication cost is often the major constraint for the clients. This motivates the problem of (empirical) _distributed mean estimation_ (DME) under communication constraints, as illustrated in Figure 1. Each of the \(n\) clients holds a vector \(_{i}^{d}\), on which there are no distributional assumptions. Given a communication budget, each client sends a compressed version \(}_{i}\) of its vector to the server, which utilizes these to compute an estimate of the mean vector \(_{i=1}^{n}_{i}\).

Quantization and sparsification are two major techniques for reducing the communication costs of DME. Quantization  involves compressing each coordinate of the client vector to a given precision and aims to reduce the number of bits to represent each coordinate, achieving a constant reduction in the communication cost. However, the communication cost still remains \((d)\)Sparsification, on the other hand, aims to reduce the number of coordinates each client sends and compresses each client vector to only \(k d\) of its coordinates (e.g. Rand-\(k\)). As a result, sparsification reduces communication costs more aggressively compared to quantization, achieving better communication efficiency at a cost of only \(O(k)\). While in practice, one can use a combination of quantization and sparsification techniques for communication cost reduction, in this work, we focus on the more aggressive sparsification techniques. We call \(k\), the dimension of the vector each client sends to the server, the _per-client_ communication budget.

Most existing works on sparsification ignore the potential correlation (or similarity) among the client vectors, which often exists in practice. For example, the data of a specific client in federated learning can be similar to that of multiple clients. Hence, it is reasonable to expect their models (or gradients) to be similar as well. To the best of our knowledge,  is the first work to account for _spatial_ correlation across individual client vectors. They propose the Rand-\(k\)-Spatial family of unbiased estimators, which generalizes Rand-\(k\) and achieves a better estimation error in the presence of cross-client correlation. However, their approach is focused only on the server-side decoding procedure, while the clients do simple Rand-\(k\) encoding.

In this work, we consider a more general encoding scheme that directly compresses a vector from \(^{d}\) to \(^{k}\) using a (random) linear map. The encoded vector consists of \(k\) linear combinations of the original coordinates. Intuitively, this has a higher chance of capturing the large-magnitude coordinates ("heavy hitters") of the vector than randomly sampling \(k\) out of the \(d\) coordinates (Rand-\(k\)), which is crucial for the estimator to recover the true mean vector. For example, consider a vector where only a few coordinates are heavy hitters. For small \(k\), Rand-\(k\) has a decent chance of missing all the heavy hitters. But with a linear-maps-based general encoding procedure, the large coordinates are more likely to be encoded in the linear measurements, resulting in a more accurate estimator of the mean vector. Guided by this intuition, we ask:

_Can we design an improved joint encoding-decoding scheme that utilizes the correlation information and achieves an improved estimation error?_

One naive solution is to apply the same random rotation matrix \(^{d d}\) to each client vector, before applying Rand-\(k\) or Rand-\(k\)-Spatial encoding. Indeed, such preprocessing is applied to improve the estimator using quantization techniques on heterogeneous vectors . However, as we see in Appendix A.1, for sparsification, we can show that this leads to no improvement. But what happens if every client uses a different random matrix, or applies a random \(k d\)-dimensional linear map? How to design the corresponding decoding procedure to leverage cross-client correlation? As there is no way for one to directly apply the decoding procedure of Rand-\(k\)-Spatial in such cases. To answer these questions, we propose the Rand-Proj-Spatial family estimator. We propose a flexible encoding procedure in which each client applies its own random linear map to encode the vector. Further, our novel decoding procedure can better leverage cross-client correlation. The resulting mean estimator generalizes and improves over the Rand-\(k\)-Spatial family estimator.

Next, we discuss some reasonable restrictions we expect our mean estimator to obey. 1) _Unbiased_. An unbiased mean estimator is theoretically more convenient compared to a biased one . 2) _Non-adaptive_. We focus on an encoding procedure that does not depend on the actual client data, as opposed to the _adaptive_ ones, e.g. Rand-\(k\) with vector-based sampling probability . Designing a data-adaptive encoding procedure is computationally expensive as this might require using an iterative procedure to find out the sampling probabilities . In practice, however, clients often have limited computational power compared to the server. Further, as discussed earlier, mean estimation is often a subroutine in more complicated tasks. For applications with streaming data ,

Figure 1: The problem of distributed mean estimation under limited communication. Each client \(i[n]\) encodes its vector \(_{i}\) as \(}_{i}\) and sends this compressed version to the server. The server decodes them to compute an estimate of the true mean \(_{i=1}^{n}_{i}\).

the additional computational overhead of adaptive schemes is challenging to maintain. Note that both Rand-\(k\) and Rand-\(k\)-Spatial family estimator  are _unbiased_ and _non-adaptive_.

In this paper, we focus on the severely communication-constrained case \(nk d\), when the server receives very limited information about any single client vector. If \(nk d\), we see in Appendix A.2 that the cross-client information has no additional advantage in terms of improving the mean estimate under both Rand-\(k\)-Spatial or Rand-Proj-Spatial, with different choices of random linear maps. Furthermore, when \(nk d\), the performance of both the estimators converges to that of Rand-\(k\). Intuitively, this means when the server receives sufficient information regarding the client vectors, it does not need to leverage cross-client correlation to improve the mean estimator.

Our contributions can be summarized as follows:

1. We propose the Rand-Proj-Spatial family estimator with a more flexible encoding-decoding procedure, which can better leverage the cross-client correlation information to achieve a more general and improved mean estimator compared to existing ones.
2. We show the benefit of using Subsampled Randomized Hadamard Transform (SRHT) as the random linear maps in Rand-Proj-Spatial in terms of better mean estimation error (MSE). We theoretically analyze the case when the correlation information is known at the server (see Theorems 4.3, 4.4 and Section 4.3). Further, we propose a practical configuration called Rand-Proj-Spatial(Avg) when the correlation is unknown.
3. We conduct experiments on common distributed optimization tasks, and demonstrate the superior performance of Rand-Proj-Spatial compared to existing sparsification techniques.

## 2 Related Work

**Quantization and Sparsification.** Commonly used techniques to achieve communication efficiency are quantization, sparsification, or more generic compression schemes, which generalize the former two . Quantization involves either representing each coordinate of the vector by a small number of bits [8; 9; 10; 18; 19; 20], or more involved vector quantization techniques [21; 22]. Sparsification [15; 23; 24; 25; 26], on the other hand, involves communicating a small number \(k<d\) of coordinates, to the server. Common protocols include Rand-\(k\), sending \(k\) uniformly randomly selected coordinates; Top-\(k\), sending the \(k\) largest magnitude coordinates; and a combination of the two . Some recent works, with a focus on distributed learning, further refine these communication-saving mechanisms  by incorporating temporal correlation or error feedback [14; 25].

**Distributed Mean Estimation (DME).** DME has wide applications in distributed optimization and FL. Most of the existing literature on DME either considers statistical mean estimation [30; 31], assuming that the data across clients is generated i.i.d. according to the same distribution, or empirical mean estimation [10; 32; 33; 12; 11; 34; 35], without making any distributional assumptions on the data. A recent line of work on empirical DME considers applying additional information available to the server, to further improve the mean estimate. This side information includes cross-client correlation [12; 13], or the memory of the past updates sent by the clients .

**Subsampled Randomized Hadamard Transformation (SRHT).** SRHT was introduced for random dimensionality reduction using sketching [37; 38; 39]. Common applications of SRHT include faster computation of matrix problems, such as low-rank approximation [40; 41], and machine learning tasks, such as ridge regression , and least square problems [43; 44; 45]. SRHT has also been applied to improve communication efficiency in distributed optimization  and FL [47; 48].

## 3 Preliminaries

**Notation.** We use bold lowercase (uppercase) letters, e.g. \(\) (\(\)) to denote vectors (matrices). \(_{j}^{d}\), for \(j[d]\), denotes the \(j\)-th canonical basis vector. \(\|\|_{2}\) denotes the Euclidean norm. For a vector \(\), \((j)\) denotes its \(j\)-th coordinate. Given integer \(m\), we denote by \([m]\) the set \(\{1,2,,m\}\).

**Problem Setup.** Consider \(n\) geographically separated clients coordinated by a central server. Each client \(i[n]\) holds a vector \(_{i}^{d}\), while the server wants to estimate the mean vector \(}_{i=1}^{n}_{i}\). Given a per-client communication budget of \(k[d]\), each client \(i\) computes \(}_{i}\) and sends it to the central server. \(}_{i}\) is an approximation of \(_{i}\) that belongs to a random \(k\)-dimensionalsubspace. Each client also sends a random seed to the server, which conveys the subspace information, and can usually be communicated using a negligible amount of bits. Having received the encoded vectors \(\{}_{i}\}_{i=1}^{n}\), the server then computes \(}^{d}\), an estimator of \(}\). We consider the severely communication-constrained setting where \(nk d\), when only a limited amount of information about the client vectors is seen by the server.

**Error Metric.** We measure the quality of the decoded vector \(}\) using the Mean Squared Error (MSE) \([\|}-}\|_{2}^{2}]\), where the expectation is with respect to all the randomness in the encoding-decoding scheme. Our goal is to design an encoding-decoding algorithm to achieve an unbiased estimate \(}\) (i.e. \([}]=}\)) that minimizes the MSE, given the per-client communication budget \(k\). To consider an example, in rand-\(k\) sparsification, each client sends randomly selected \(k\) out of its \(d\) coordinates to the server. The server then computes the mean estimate as \(}^{}=_{i=1}^{n} }_{i}\). By [12, Lemma 1], the MSE of Rand-\(k\) sparsification is given by

\[\|}^{}- }\|_{2}^{2}=}-1 _{i=1}^{n}\|_{i}\|_{2}^{2}\] (1)

**The Rand-\(k\)-Spatial Family Estimator.** For large values of \(\), the Rand-\(k\) MSE in Eq. 1 can be prohibitive.  proposed the Rand-\(k\)-Spatial family estimator, which achieves an improved MSE, by leveraging the knowledge of the correlation between client vectors at the server. The encoded vectors \(\{}_{i}\}\) are the same as in Rand-\(k\). However, the \(j\)-th coordinate of the decoded vector is given as

\[}^{}(j)= }{T(M_{j})}_{i=1}^{n}}_{i}(j)\] (2)

Here, \(T:\) is a pre-defined transformation function of \(M_{j}\), the number of clients which sent their \(j\)-th coordinate, and \(\) is a normalization constant to ensure \(}\) is an unbiased estimator of \(\). The resulting MSE is given by

\[\|}^{}- {}\|_{2}^{2}=}-1_{ i=1}^{n}\|_{i}\|_{2}^{2}+(c_{1}_{i=1}^{n}\|_{i}\|_{2}^{2} -c_{2}_{i=1}^{n}_{l i}_{i},_{l} )\] (3)

where \(c_{1},c_{2}\) are constants dependent on \(n,d,k\) and \(T\), but independent of client vectors \(\{_{i}\}_{i=1}^{n}\). When the client vectors are orthogonal, i.e., \(_{i},_{l}=0\), for all \(i l\),  show that with appropriately chosen \(T\), the MSE in Eq. 3 reduces to Eq. 1. However, if there exists a positive correlation between the vectors, the MSE in Eq. 3 is strictly smaller than that for Rand-\(k\) Eq. 1.

## 4 The Rand-Proj-Spatial Family Estimator

While the Rand-\(k\)-Spatial family estimator proposed in  focuses only on improving the decoding at the server, we consider a more general encoding-decoding scheme. Rather than simply communicating \(k\) out of the \(d\) coordinates of its vector \(_{i}\) to the server, client \(i\) applies a (random) linear map \(_{i}^{k d}\) to \(_{i}\) and sends \(}_{i}=_{i}_{i}^{k}\) to the server. The decoding process on the server first projects the _encoded_ vectors \(\{_{i}_{i}\}_{i=1}^{n}\) back to the \(d\)-dimensional space and then forms an estimate \(}\). We motivate our new decoding procedure with the following regression problem:

\[}^{}=*{arg\,min} _{}_{i=1}^{n}\|_{i}-_{i}_{i}\|_ {2}^{2}\] (4)

To understand the motivation behind Eq. 4, first consider the special case where \(_{i}=_{d}\) for all \(i[n]\), that is, the clients communicate their vectors without compressing. The server can then exactly compute the mean \(}=_{i=1}^{n}_{i}\). Equivalently, \(}\) is the solution of \(*{arg\,min}_{}_{i=1}^{n}\|-_{i} \|_{2}^{2}\). In the more general setting, we require that the mean estimate \(}\) when encoded using the map \(_{i}\), should be "close" to the encoded vector \(_{i}_{i}\) originally sent by client \(i\), for all clients \(i[n]\).

We note the above intuition can also be translated into different regression problems to motivate the design of the new decoding procedure. We discuss in Appendix B.2 intuitive alternatives which,unfortunately, either do not enable the usage of cross-client correlation information, or do not use such information effectively. We choose the formulation in Eq. 4 due to its analytical tractability and its direct relevance to our target error metric MSE. We note that it is possible to consider the problem in Eq. 4 in the other norms, such as the sum of \(_{2}\) norms (without the squares) or the \(_{}\) norm. We leave this as a future direction to explore.

The solution to Eq. 4 is given by \(}^{}=(_{i=1}^{n}_{i}^{T}_ {i})^{}_{i=1}^{n}_{i}^{T}_{i}_{i}\), where \(\) denotes the Moore-Penrose pseudo inverse . However, while \(}^{}\) minimizes the error of the regression problem, our goal is to design an _unbiased_ estimator that also improves the MSE. Therefore, we make the following two modifications to \(}^{}\): First, to ensure that the mean estimate is unbiased, we scale the solution by a normalization factor \(\)1. Second, to incorporate varying degrees of correlation among the clients, we propose to apply a scalar transformation function \(T:\) to each of the eigenvalues of \(_{i=1}^{n}_{i}^{T}_{i}\). The resulting Rand-Proj-Spatial family estimator is given by

\[}^{}=T(_{ i=1}^{n}_{i}^{T}_{i})^{}_{i=1}^{n}_{i}^{T} _{i}_{i}\] (5)

Though applying the transformation function \(T\) in Rand-Proj-Spatial requires computing the eigendecomposition of \(_{i=1}^{n}_{i}^{T}_{i}\). However, this happens only at the server, which has more computational power than the clients. Next, we observe that for appropriate choice of \(\{_{i}\}_{i=1}^{n}\), the Rand-Proj-Spatial family estimator reduces to the Rand-\(k\)-Spatial family estimator .

**Lemma 4.1** (Recovering Rand-\(k\)-Spatial).: _Suppose client \(i\) generates a subsampling matrix \(_{i}=[_{i_{1}},\ \ ,\ \ _{i_{k}}]^{}\), where \(\{_{j}\}_{j=1}^{d}\) are the canonical basis vectors, and \(\{i_{1},,i_{k}\}\) are sampled from \(\{1,,d\}\) without replacement. The encoded vectors are given as \(}_{i}=_{i}_{i}\). Given a function \(T\), \(}\) computed as in Eq. 5 recovers the Rand-\(k\)-Spatial estimator._

The proof details are in Appendix C.5. We discuss the choice of \(T\) and how it compares to Rand-\(k\)-Spatial in detail in Section 4.3.

**Remark 4.2**.: _In the simple case when \(_{i}\)'s are subsampling matrices (as in Rand-\(k\)-Spatial ), the \(j\)-th diagonal entry of \(_{i=1}^{n}_{i}^{T}_{i}\), \(M_{j}\) conveys the number of clients which sent the \(j\)-th coordinate. Rand-\(k\)-Spatial incorporates correlation among client vectors by applying a function \(T\) to \(M_{j}\). Intuitively, it means scaling different coordinates differently. This is in contrast to Rand-\(k\), which scales all the coordinates by \(d/k\). In our more general case, we apply a function \(T\) to the eigenvalues of \(_{i=1}^{n}_{i}^{T}_{i}\) to similarly incorporate correlation in Rand-Proj-Spatial._

To showcase the utility of the Rand-Proj-Spatial family estimator, we propose to set the random linear maps \(_{i}\) to be scaled Subsampled Randomized Hadamard Transform (SRHT, e.g. ). Assuming \(d\) to be a power of \(2\), the linear map \(_{i}\) is given as

\[_{i}=}_{i}_{i}^{k d}\] (6)

where \(_{i}^{k d}\) is the subsampling matrix, \(^{d d}\) is the (deterministic) Hadamard matrix and \(_{i}^{d d}\) is a diagonal matrix with independent Rademacher random variables as its diagonal entries. We choose SRHT due to its superior performance compared to other random matrices. Other possible choices of random matrices for Rand-Proj-Spatial estimator include sketching matrices commonly used for dimensionality reduction, such as Gaussian , row-normalized Gaussian, and Count Sketch , as well as error-correction coding matrices, such as Low-Density Parity Check (LDPC)  and Fountain Codes . However, in the absence of correlation between client vectors, all these matrices suffer a higher MSE.

In the following, we first compare the MSE of Rand-Proj-Spatial with SRHT against Rand-\(k\) and Rand-\(k\)-Spatial in two extreme cases: when all the client vectors are identical, and when all the client vectors are orthogonal to each other. In both cases, we highlight the transformation function \(T\) used in Rand-Proj-Spatial (Eq. 5) to incorporate the knowledge of cross-client correlation. We define

\[:=^{n}_{l i}_{i}, _{l}}{_{i=1}^{n}\|_{i}\|_{2}^{2}}\] (7)

to measure the correlation between the client vectors. Note that \([-1,n-1]\). \(=0\) implies all client vectors are orthogonal, while \(=n-1\) implies identical client vectors.

### Case I: Identical Client Vectors (\(=n-1\))

When all the client vectors are identical (\(_{i}\)),  showed that setting the transformation \(T\) to identity, i.e., \(T(m)=m\), for all \(m\), leads to the minimum MSE in the Rand-\(k\)-Spatial family of estimators. The resulting estimator is called Rand-\(k\)-Spatial (Max). Under the same setting, using the same transformation \(T\) in Rand-Proj-Spatial with SRHT, the decoded vector in Eq. 5 simplifies to

\[}^{}=_{i=1} ^{n}_{i}^{T}_{i}^{}_{i=1}^{n}_{i}^{T}_{i}=^{},\] (8)

where \(:=_{i=1}^{n}_{i}^{T}_{i}\). By construction, \(() nk\), and we focus on the case \(nk d\).

**Limitation of Subsampling matrices.** As mentioned above, with \(_{i}=_{i},\,i[n]\), we recover the Rand-\(k\)-Spatial family of estimators. In this case, \(\) is a diagonal matrix, where each diagonal entry \(_{jj}=M_{j}\), \(j[d]\). \(M_{j}\) is the number of clients which sent their \(j\)-th coordinate to the server. To ensure \(()=nk\), we need \(_{jj} 1, j\), i.e., each of the \(d\) coordinates is sent by _at most_ one client. If all the clients sample their matrices \(\{_{i}\}_{i=1}^{n}\) independently, this happens with probability \(}\). As an example, for \(k=1\), \((()=n)=}{d^{n}}\) (because \(}{n^{n}}}{n!}\)). Therefore, to guarantee that \(\) is full-rank, each client would need the subsampling information of all the other clients. This not only requires additional communication but also has serious privacy implications. Essentially, the limitation with subsampling matrices \(_{i}\) is that the eigenvectors of \(\) are restricted to be canonical basis vectors \(\{_{j}\}_{j=1}^{d}\). Generalizing \(_{i}\)'s to general rank \(k\) matrices relaxes this constraint and hence we can ensure that \(\) is full-rank with high probability. In the next result, we show the benefit of choosing \(_{i}\) as SRHT matrices. We call the resulting estimator Rand-Proj-Spatial(Max).

**Theorem 4.3** (MSE under Full Correlation).: _Consider \(n\) clients, each holding the same vector \(^{d}\). Suppose we set \(T()=\), \(=\) in Eq. 5, and the random linear map \(}_{i}\) at each client to be an SRHT matrix. Let \(\) be the probability that \(=_{i=1}^{n}_{i}^{T}_{i}\) does not have full rank. Then, for \(nk d\),_

\[\|}^{}-}\|_{2}^{2}-1 \|\|_{2}^{2}\] (9)

The proof details are in Appendix C.1. To compare the performance of Rand-Proj-Spatial(Max) against Rand-\(k\), we show in Appendix C.2 that for \(n 2\), as long as \(\), the MSE of Rand-Proj-Spatial(Max) is less than that of Rand-\(k\). Furthermore, in Appendix C.3 we empirically demonstrate that with \(d\{32,64,128,,1024\}\) and different values of \(nk d\), the rank of \(\) is full with high probability, i.e., \( 0\). This implies \([\|}^{}-}\|_{2}^{2}](-1)\|\|_{2}^{2}\).

Futhermore, since setting \(_{i}\) as SRHT significantly increases the probability of recovering \(nk\) coordinates of \(\), the MSE of Rand-Proj-Spatial with SRHT (Eq. 4.3) is strictly less than that of Rand-\(k\)-Spatial (Eq. 3). We also compare the MSEs of the three estimators in Figure 2 in the following setting: \(\|\|_{2}=1\), \(d=1024,n\{10,20,50,100\}\) and small \(k\) values such that \(nk<d\).

### Case II: Orthogonal Client Vectors (\(=0\))

When all the client vectors are orthogonal to each other,  showed that Rand-\(k\) has the lowest MSE among the Rand-\(k\)-Spatial family of decoders. We show in the next result that if we set the

Figure 2: MSE comparison of Rand-\(k\), Rand-\(k\)-Spatial(Max) and Rand-Proj-Spatial(Max) estimators, when all clients have identical vectors (maximum inter-client correlation).

random linear maps \(_{i}\) at client \(i\) to be SRHT, and choose the fixed transformation \(T 1\) as in , Rand-Proj-Spatial achieves the same MSE as that of Rand-\(k\).

**Theorem 4.4** (MSE under No Correlation).: _Consider \(n\) clients, each holding a vector \(_{i}^{d}\), \( i[n]\). Suppose we set \(T 1\), \(=}{k}\) in Eq. 5, and the random linear map \(_{i}\) at each client to be an SRHT matrix. Then, for \(nk d\),_

\[\|}^{()}-}\|_{2}^{2}=}-1_{i=1}^{n}\| _{i}\|_{2}^{2}.\] (10)

The proof details are in Appendix C.4. Theorem 4.4 above shows that with zero correlation among client vectors, Rand-Proj-Spatial achieves the same MSE as that of Rand-\(k\).

### Incorporating Varying Degrees of Correlation

In practice, it unlikely that all the client vectors are either identical or orthogonal to each other. In general, there is some "imperfect" correlation among the client vectors, i.e., \((0,n-1)\). Given correlation level \(\),  shows that the estimator from the Rand-\(k\)-Spatial family that minimizes the MSE is given by the following transformation.

\[T(m)=1+}{n-1}(m-1)\] (11)

Recall from Section 4.1 (Section 4.2) that setting \(T(m)=1\) (\(T(m)=m\)) leads to the estimator among the Rand-\(k\)-Spatial family that minimizes MSE when there is zero (maximum) correlation among the client vectors. We observe the function \(T\) defined in Eq. 11 essentially interpolates between the two extreme cases, using the normalized degree of correlation \(}{n-1}[-,1]\) as the weight. This motivates us to apply the same function \(T\) defined in Eq. 11 on the eigenvalues of \(=_{i=1}^{n}_{i}^{T}_{i}\) in Rand-Proj-Spatial. As we shall see in our results, the resulting Rand-Proj-Spatial family estimator improves over the MSE of both Rand-\(k\) and Rand-\(k\)-Spatial family estimator.

We note that deriving a closed-form expression of MSE for Rand-Proj-Spatial with SRHT in the general case with the transformation function \(T\) (Eq. 11) is hard (we elaborate on this in Appendix B.3), as this requires a closed form expression for the non-asymptotic distributions of eigenvalues and eigenvectors of the random matrix \(\). To the best of our knowledge, previous analyses of SRHT, for example in [37; 38; 39; 45; 55], rely on the asymptotic properties of SRHT, such as the limiting eigen spectrum, or concentration bounds on the singular values, to derive asymptotic or approximate guarantees. However, to analyze the MSE of Rand-Proj-Spatial, we need an exact, non-asymptotic analysis of the eigenvalues and eigenvectors distribution of SRHT. Given the apparent intractability of the theoretical analysis, we compare the MSE of Rand-Proj-Spatial, Rand-\(k\)-Spatial, and Rand-\(k\) via simulations.

Simulations.In each experiment, we first simulate \(\) in Eq. 5, which ensures our estimator is unbiased, based on \(1000\) random runs. Given the degree of correlation \(\), we then compute the squared error, i.e. \(\|}^{()}-}\|_{2}^{2}\), where Rand-Proj-Spatial has \(_{i}\) as SRHT matrix (Eq. 6) and \(T\) as in Eq. 11. We plot the average over \(1000\) random runs as an approximation to MSE. Each client holds a \(d\)-dimensional base vector \(_{j}\) for some \(j[d]\), and so two clients either hold the same or orthogonal vectors. We control the degree of correlation \(\) by changing the number of clients which hold the same vector. We consider \(d=1024\), \(n\{21,51\}\). We consider positive correlation values, where \(\) is chosen to be linearly spaced within \([0,n-1]\). Hence, for \(n=21\), we use \(\{4,8,12,16\}\) and for \(n=51\), we use \(\{10,20,30,40\}\). All results are presented in Figure 3. As expected, given \(\), Rand-Proj-Spatial consistently achieves a lower MSE than the lowest possible MSE from the Rand-\(k\)-Spatial family decoder. Additional results with different values of \(n,d,k\), including the setting \(nk d\), can be found in Appendix B.4.

A Practical Configuration.In reality, it is hard to know the correlation information \(\) among the client vectors.  uses the transformation function which interpolates to the middle point between the full correlation and no correlation cases, such that \(T(m)=1+\). Rand-\(k\)-Spatial with such \(T\) is called Rand-\(k\)-Spatial(Avg). Following this approach, we evaluate Rand-Proj-Spatial with SRHT using this \(T\), and call it Rand-Proj-Spatial(Avg) in practical settings (see Figure 4).

## 5 Experiments

We consider three practical distributed optimization tasks for evaluation: distributed power iteration, distributed \(k\)-means and distributed linear regression. We compare Rand-Proj-Spatial(Avg) against Rand-\(k\), Rand-\(k\)-Spatial(Avg), and two more sophisticated but widely used sparsification schemes: non-uniform coordinate-wise gradient sparsification  (we call it Rand-\(k\)(Wangni)) and the Induced compressor with Rand-\(k\) + Top-\(k\). The results are presented in Figure 4.

Dataset.For both distributed power iteration and distributed \(k\)-means, we use the test set of the Fashion-MNIST dataset  consisting of \(10000\) samples. The original images from Fashion-MNIST are \(28 28\) in size. We preprocess and resize each image to be \(32 32\). Resizing images to have their dimension as a power of 2 is a common technique used in computer vision to accelerate the convolution operation. We use the DJIndoor dataset 2 for distributed linear regression. We subsample \(10000\) data points, and use the first \(512\) out of the total \(520\) features on signals of phone calls. The task is to predict the longitude of the location of a phone call. In all the experiments in Figure 4, the datasets are split IID across the clients via random shuffling. In Appendix D.1, we have additional results for non-IID data split across the clients.

Setup and Metric.Recall that \(n\) denotes the number of clients, \(k\) the per-client communication budget, and \(d\) the vector dimension. For Rand-Proj-Spatial, we use the first \(50\) iterations to estimate \(\) (see Eq. 5). Note that \(\) only depends on \(n,k,d\), and \(T\) (the transformation function in Eq. 5), but is independent of the dataset. We repeat the experiments across 10 independent runs, and report the mean MSE (solid lines) and one standard deviation (shaded regions) for each estimator. For each task, we plot the squared error of the mean estimator \(}\), i.e., \(\|}-}\|_{2}^{2}\), and the values of the task-specific loss function, detailed below.

Tasks and Settings:

1. Distributed power iteration.We estimate the principle eigenvector of the covariance matrix, with the dataset (Fashion-MNIST) distributed across the \(n\) clients. In each iteration, each client computes a local principle eigenvector estimate based on a single power iteration and sends an encoded version to the server. The server then computes a global estimate and sends it back to the clients. The task-specific loss here is \(\|_{t}-_{top}\|_{2}\), where \(_{t}\) is the global estimate of the principal eigenvector at iteration \(t\), and \(_{top}\) is the true principle eigenvector.

Figure 3: MSE comparison of estimators Rand-\(k\), Rand-\(k\)-Spatial(Opt), Rand-Proj-Spatial, given the degree of correlation \(\). Rand-\(k\)-Spatial(Opt) denotes the estimator that gives the lowest possible MSE from the Rand-\(k\)-Spatial family. We consider \(d=1024\), number of clients \(n\{21,51\}\), and \(k\) values such that \(nk<d\). In each plot, we fix \(n,k,d\) and vary the degree of positive correlation \(\). The y-axis represents MSE. Notice since each client has a fixed \(\|_{i}\|_{2}=1\), and Rand-\(k\) does not leverage cross-client correlation, the MSE of Rand-\(k\) in each plot remains the same for different \(\).

## 2 Distributed \(k\)-means.

We perform \(k\)-means clustering  with the data distributed across \(n\) clients (Fashion-MNIST, 10 classes) using Lloyd's algorithm. At each iteration, each client performs a single iteration of \(k\)-means to find its local centroids and sends the encoded version to the server. The server then computes an estimate of the global centroids and sends them back to the clients. We report the average squared mean estimation error across 10 clusters, and the \(k\)-means loss, i.e., the sum of the squared distances of the data points to the centroids.

For both distributed power iterations and distributed \(k\)-means, we run the experiments for \(30\) iterations and consider two different settings: \(n=10\), \(k=102\) and \(n=50,k=20\).

## 3 Distributed linear regression.

We perform linear regression on the UJIndoor dataset distributed across \(n\) clients using SGD. At each iteration, each client computes a local gradient and sends an encoded version to the server. The server computes a global estimate of the gradient, performs an SGD step, and sends the updated parameter to the clients. We run the experiments for \(50\) iterations with learning rate \(0.001\). The task-specific loss is the linear regression loss, i.e. empirical mean squared error. To have a proper scale that better showcases the difference in performance of different estimators, we plot the results starting from the 10th iteration.

**Results.** It is evident from Figure 4 that Rand-Proj-Spatial(Avg), our estimator with the practical configuration \(T\) (see Section 4.3) that does not require the knowledge of the actual degree of correlation among clients, consistently outperforms the other estimators in all three tasks. Additional experiments for the three tasks are included in Appendix D.1. Furthermore, we present the wall-clock time to encode and decode client vectors using different sparsification schemes in Figure 5. Though Rand-Proj-Spatial(Avg) has the longest decoding time, the encoding time of Rand-Proj-Spatial(Avg) is less than that of the _adaptive_ Rand-\(k\)(Wangni) sparsifier. In practice, the server has more compu

Figure 4: Experiment results on three distributed optimization tasks: distributed power iteration, distributed \(k\)-means, and distributed linear regression. The first two use the Fashion-MNIST dataset with the images resized to \(32 32\), hence \(d=1024\). Distributed linear regression uses UJIndoor dataset with \(d=512\). All the experiments are repeated for 10 random runs, and we report the mean as the solid lines, and one standard deviation using the shaded region. The violet line in the plots represents our proposed Rand-Proj-Spatial(Avg) estimator.

tational power than the clients and hence can afford a longer decoding time. Therefore, it is more important to have efficient encoding procedures.

## 6 Limitations

We note two practical limitations of the proposed Rand-Proj-Spatial.

**1) Computation Time of Rand-Proj-Spatial.** The encoding time of Rand-Proj-Spatial is \(O(kd)\), while the decoding time is \(O(d^{2} nk)\). The computation bottleneck in decoding is computing the eigendecomposition of the \(d d\) matrix \(_{i=1}^{n}_{i}^{T}_{i}\) of rank at most \(nk\). Improving the computation time for both the encoding and decoding schemes is an important direction for future work.

**2) Perfect Shared Randomness.** It is common to assume perfect shared randomness between the server and the clients in distributed settings . However, to perfectly simulate randomness using Pseudo Random Number Generator (PRNG), at least \(_{2}d\) bits of the seed need to be exchanged in practice. We acknowledge this gap between theory and practice.

## 7 Conclusion

In this paper, we propose the Rand-Proj-Spatial estimator, a novel encoding-decoding scheme, for communication-efficient distributed mean estimation. The proposed client-side encoding generalizes and improves the commonly used Rand-\(k\) sparsification, by utilizing projections onto general \(k\)-dimensional subspaces. On the server side, cross-client correlation is leveraged to improve the approximation error. Compared to existing methods, the proposed scheme consistently achieves better mean estimation error across a variety of tasks. Potential future directions include improving the computation time of Rand-Proj-Spatial and exploring whether the proposed Rand-Proj-Spatial achieves the optimal estimation error among the class of _non-adaptive_ estimators, given correlation information. Furthermore, combining sparsification and quantization techniques and deriving such algorithms with the optimal communication cost-estimation error trade-offs would be interesting.

Figure 5: The corresponding wall-clock time to encode and decode client vectors (in seconds) using different sparsification schemes, across the three tasks.