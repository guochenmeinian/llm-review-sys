# LAM3D: Large Image-Point-Cloud Alignment Model

for 3D Reconstruction from Single Image

 Ruikai Cui\({}^{1}\)  Xibin Song\({}^{2}\)  Weixuan Sun\({}^{2}\)  Senbo Wang\({}^{2}\)  Weizhe Liu\({}^{2}\)

**Shenzhou Chen\({}^{2}\)  Taizhang Shang\({}^{2}\)  Yang Li\({}^{2}\)  Nick Barnes\({}^{1}\)  Hongdong Li\({}^{1}\)  Pan Ji\({}^{2}\) \({}^{1}\)**Australian National University \({}^{2}\)Tencent XR Vision Labs

The contribution of Ruikai Cui was made during an internship at Tencent XR Vision Labs.Corresponding author.

###### Abstract

Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes. Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction. Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information. This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions. Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.

## 1 Introduction

High-quality 3D mesh creation has drawn increasing attentions for its great potential in numerous fields, such as video games, virtual reality, and films. Traditionally, 3D assets are usually created manually by expert artists or alternatively reconstructed from multi-view 2D images, which are time-consuming. Recent proposed Large Reconstruction Models (LRMs)  from a single view or few-shot images bring significant advancements for high-fidelity 3D content creation.

Single view based LRMs  take a single-view image as input and use transformers to directly regress tri-plane-based  neural radiance fields (NeRF)  for 3D content creation. However, as shown in Fig. 1 (b), due to ambiguity of 3D geometry from a single image, single-view-based LRMs commonly suffer from geometric distortions, especially for the unseen areas of the input view. Inspired by Hong , LRMs with multi-view inputs  are proposed, where they typically first feed the single-view image into multi-view image diffusion models  for few-shot multi-view images generation. Then multi-view tri-plane features are generated and converted into 3D representations (NeRF , SDFs , ) for 3D mesh creation. However, as shown in Fig.1 (c), the generated multi-view images lack guaranteed multi-view consistency, which can lead to geometric distortions.

To relieve these problems, we propose to introduce a 3D point cloud prior for single image based 3D mesh reconstruction with Large Image-Point-Cloud Alignment Model (LAM3D). LAM3D aims totransfer single image features into point cloud features, enriching the image features with accurate and effective 3D information for high-fidelity 3D mesh reconstruction. The motivation behind this choice is obvious: 3D point cloud contains effective structure prior of 3D geometries. Meanwhile, approaches [63; 64] of modality feature alignment have been proposed, aiming to learn unified representations of language, images, and point clouds for 3D understanding. Nevertheless, as shown in Fig. 1 (a), these techniques fall short when they use the aligned image features to replace point clouds features for 3D mesh creation. This is largely due to the exclusive usage of contrastive loss [63; 64] and the aligned feature size is commonly with (512\(\)1) and (1024\(\)1), which is inadequate to fully capture the rich diversity of 3D shapes.

In this work, we extend the image and point cloud alignment for 3D mesh creation, and present the Large Image-Point-Cloud Alignment Model (LAM3D) which utilize the prior of 3D point cloud for high-fidelity feature alignment. We start by designing an effective point cloud-based network with hierarchical tri-planes  for 3D mesh reconstruction. Following this, taking single image as input, image features are extracted using DINO , whilst LAM3D is employed to transpose these image features into tri-planes obtained via the point-cloud-based network. The tri-plane representation  allows for the tuning of image features with precise and effective 3D information, which is essential for high-fidelity 3D mesh reconstruction. Meanwhile, to better retain 3D structure information and reduce the interference of each plane (XY, XZ, YZ) in tri-plane features, we enlist the use of independent diffusion operations to transfer image features to each respective plane. Note that the transferred features are with size of 3\(\)C\(\)M\(\)N ((3\(\)2\(\)32\(\)32) in our experiments), which is larger than previous representation size of (512\(\)1) and (1024\(\)1), thus contains more information for better 3D mesh creation. As shown in Fig. 1 (d), more reasonable geometric mesh can be reconstructed by our approach.

The main contributions of our work are as follows:

* We propose an effective Large Image-Point-Clouds Alignment Model (LAM3D) for 3D mesh reconstruction from a single image. By utilizing point cloud priors, LAM3D delivers precise image tri-plane feature transformation, thereby enhancing the quality and accuracy of 3D mesh creation.
* To preserve 3D structural information more effectively, we employ independent diffusion processes that transmit image features to each respective tri-plane (XY, XZ, and YZ) for accurate 3D mesh reconstruction.
* Our method significantly relieves geometric distortions caused by ambiguities of 3D shape inferred from the image modality, and experiments on various of dataset demonstrate the effectiveness of our approach.

## 2 Related Works

**Single Image to 3D Reconstruction:** Recently, LRM  was the first to demonstrate that a regression model can be trained to robustly predict tri-plane-based NeRF from a single-view image. Inspired by LRM, numerous large models[25; 26; 36; 54; 69; 53; 50] for 3D reconstruction have since been proposed. Among these, Instant3D  trains a text-to-few-shot multi-view image diffusion model, as well as a multi-view LRM, to enable fast and diverse text-to-3D generation. Subsequent works have extended LRM by incorporating pose prediction from multi-view images , combining it with diffusion , and specializing on human data . Tang et al. introduced the Large Multi-View Gaussian Model to generate high-resolution 3D models from text prompts or single-view images. Wang et al. presented a single-image-to-3D generative model that generates six orthographic images, utilizing Flexicubes  as a 3D representation. Concurrently, Wu et al. 

Figure 1: An example of single-image reconstruction from state-of-the-art methods: (a) ULIP , (b) LRM , (c) CRM , and (d) Ours (LAM3D).

proposed Direct3D, a method that differs from ours primarily in its approach to feature extraction from point clouds. Direct3D utilizes learnable tokens for feature extraction, whereas our method introduces a projection module to explicitly convert point cloud features into a tri-plane structure.

**Generative Modeling of 3D shapes:** In recent years, a multitude of studies have been dedicated to the generation of 3D shapes. Existing 3D generative models have been developed using a variety of frameworks, including generative adversarial networks (GANs)[57; 1; 67], variational autoencoders (VAEs)[47; 11; 17; 32; 12], normalizing flows [65; 32], autoregressive models [45; 32], energy-based models [61; 8], and more recently, denoising diffusion probabilistic models (DDPMs) [30; 68; 27; 49; 24; 4; 44; 60]. Luo _et al_.  pioneers the application of DDPMs for modeling the distribution of raw point clouds. Following Luo _et al_, several works [49; 24; 4] explore generative modeling of 3D shapes in latent space to reduce computational complexity and enhance generation quality. For instance, LION  proposed an effective latent-space diffusion model to generate novel point clouds, while 3DQD , SDFusion  utilize DDPM to model Signed Distance Function (SDF) in the latent space of an autoencoder for 3D shape generation. Concurrently, other research efforts have investigated DDPMs for 3D shape generation with other representations, such as mesh , occupancy grid , and neural radiance fields .

**Feature Alignment / Multimodal 3D:** Multimodal approaches are mainly about image-text and image-text-point-clouds modalities. Several works [21; 23; 46] propose to learn interactions between image regions and caption words using transformer-based architectures, which show great predictive capability despite being costly to train. Meanwhile, CLIP  uses image and text encoders to output a single image/text representation for each image-text pair, and then aligns the representations to a unified latent space. Built upon CLIP , ULIP  and its successor, ULIP2 , aim to learn a unified representation that integrates images, text, and 3D point clouds. These methods establish a 3D representation space aligned with the CLIP latent space by leveraging synthesized 3D-text-image triplets. The most closely related work to ours is Michelangelo , which aligns point cloud features with frozen CLIP image/text embeddings through an autoencoder. Unlike Michelangelo, which employs contrastive loss to learn an aligned latent space, our approach uses diffusion to align 3D and 2D modalities. This results in a more continuous latent space, beneficial for 3D reconstruction.

## 3 Method

In this section, we present the detailed design of our approach. The full training process consists of two stages: _Point Cloud Compression_ (Sec. 3.1) and _Image-Point-Cloud Alignment_ (Sec. 3.2).

### Stage 1: Point Cloud Compression

As shown in Fig. 2, we encode point clouds into a tri-plane representation and then decode SDF values from the reconstructed tri-plane to extract 3D meshes via marching cubes . The point cloud compression consists of _Initial Point Feature Extraction_ and _Tri-plane Compression_. Specifically, we propose a point-cloud-based 3D reconstruction network to convert point cloud into a latent tri-plane, which can retain rich 3D information. In the tri-plane compression module, the latent tri-plane is up-sampled to reconstruct tri-planes followed by an MLP to compute SDF values. As shown in Fig. 2, hierarchical tri-plane are used, including latent tri-planes and reconstructed tri-planes, where the latent tri-plane is designed for better image-point-cloud feature alignment and the reconstructed tri-plane is for better mesh reconstruction.

#### 3.1.1 Initial Point Feature Extraction

To compress a point cloud into latent tri-planes, we employ a transformer-based approach to learn long-range interactions between points. We first use Furthest Point Sampling (FPS) to downsample the input point cloud to \(n\) center points. Then, we gather the K-nearest neighbors of the \(n\) center points together as a small point cloud and use a shallow PointNet  to embed each small point cloud as an embedding. By doing so, we reduce the sequence length of the point cloud, and then we can use a transformer to process these embeddings.

After the transformer step, we need to collect the embeddings to form the three planes of the tri-plane structure. To this end, we use a projection operation. In specific, each embedding feature is associated with a knn center. For each plane (\(XY,XZ,YZ\)), we diminish one axis of the center points and voxelize the center points into a plane of size \(^{C R R}\) where \(C\) is the channel dimension that is the same with the embedding dimension, and \(R\) is the plane resolution. We use average operation to aggregate embeddings associated with center points that land into the same voxel. As shown in Fig. 2, the point cloud is converted into a initial tri-planes that faithfully preserves 3D information.

#### 3.1.2 Tri-plane Compression

The initial tri-planes is a sparse tensor as many of its grids are empty if there is no center points land there. To obtain a more compact and expressive latent tri-plane representation, we adopt an plane autoencoder to further compress initial tri-planes to latent tri-planes.

**Plane Encoder** We find that directly aligning image features with initial tri-planes leads to two drawbacks: (1) Aligning in high-resolution requires high computational complexity; (2) Empty grids and overwhelmed details in high-resolution tri-planes substantially increase training difficulty, leading to noisy results. To further compress the initial tri-planes into a more compact latent, we opt for a transformer-based plane encoder. It consists of a convolution layer to down-sample the initial tri-planes followed by successive transformer blocks to achieve inter-plane correlations. In each transformer block, we follow Cui  to concatenate tri-planes into one sequence and adopt linear attention  to process the sequence to model inter-plane correlations. In our settings, the extracted initial tri-plane is down-sampled by 4 times to obtain a point cloud latent tri-plane \(t^{(3 2 32 32)}\).

**Plane Decoder** We introduce a hierarchical scheme to decode the latent tri-plane into a high-resolution reconstructed tri-plane, as depicted in Fig. 2, and for predicting the Signed Distance Function (SDF). Initially, the latent tri-plane is processed through a standard decoder that mirrors the encoder. This decoder comprises successive transformer blocks coupled with a convolutional layer, which augments the latent tri-plane by a factor of four.

**Plane Refiner** After Plane Decoder, we adopt an intra-plane refinement module to further enhance the reconstruction quality. In latent tri-plane generation, the original point clouds are condensed to form a compact tri-plane latent, the compression inaccuracies are unavoidable. Therefore, we propose

Figure 2: Overview of our method. Our method contains two training stage. **Stage 1**: we train an encoder-decoder structure to take point clouds as input and compress it to a latent tri-plane representation; **Stage 2**: we employ diffusion to align image modality to latent tri-planes obtained in stage 1. The diffusion step takes an initial noise and an image feature from a freezed DINO feature encoder and progressively align the image feature to the latent tri-plane. **Inference**: To reconstruct a 3D mesh from a single-view image, we use the alignment step, following the decoder (Plane Decoder, Plane Refiner) from the compression step, to predict a tri-plane. Then, we can use algorithms like marching cubes to extract 3D meshes from the reconstructed tri-plane.

an asymmetric decoding of the tri-plane, which is accomplished by individually refining each plane with a plane refiner module subsequent to the conventional decoder. Specifically, the plane refiners are three small-scale UNet  models to refine each plane separately. We find that refining each plane individually extensively improves mesh quality while retains inter-plane correlations as we demonstrated in Appendices.Tab. 5. Finally, we can query SDF values of any spatial position using the SDF-MLP by interpolating features on the reconstructed tri-plane and extract 3D meshes using Marching Cubes .

#### 3.1.3 Compression Loss Function

The training objective for point cloud compression comprises a KL penalty and a geometry loss. To align image features onto a 3D-aware latent space, we attempt to preserve 3D prior to the greatest extent, even in the most compact compressed tri-plane latent. However, we discovered that the compressed tri-plane latent fails to maintain the 3D prior, leading to artifacts in the mesh outcomes if we merely employ an arbitrary encoder to compress the initial tri-plane without explicit constraints. Hence, we propose to concurrently supervise the latent tri-plane \(t\) and reconstructed tri-plane \(P\) using the ground truth SDF and two distinct SDF MLPs (\(_{t}\) and \(_{P}\)). To this end, we combine the geometric SDF loss and latent regularization to train the compression module in an end-to-end manner. Formally, to allow the predicted tri-plane recover SDF values, we use the geometry loss:

\[_{geo}=_{sdf}+_{normal}.\] (1)

These two terms are defined as:

\[_{sdf} =_{1}_{p_{0}}||_{P}(p)||+_{2} _{p}||_{P}(p)-d_{p}||,\] (2) \[_{normal} =_{3}_{p_{0}}||_{p}_{P}(p)-n_{p}||,\] (3)

where \(_{0}\) and \(\) are sets of query points sampled from object surface and off-surface points sampled from \([-1,1]^{3}\). The growth truth SDF values and surface normal are denoted as \(d_{p}\) and \(n_{p}\), respectively. The gradient \(_{p}_{P}(p)=((p)}{}, {_{P}(p)}{},(p)}{ })\) represents the direction of the steepest change in SDF. It can be computed using finite difference, _e.g._, the partial derivative for the X-axis component reads \(=(p+(,0,0))-_{P} (p-(,0,0))}{2}\) where \(\) is the step size. Similarly, the latent sdf loss \(_{lsdf}\) is defined as:

\[_{lsdf}=_{4}_{p_{0}}||_{t}(p)||+_{5} _{p}||_{t}(p)-d_{p}||.\] (4)

We use the corresponding MLP to predict a SDF value from either latent tri-plane \(t\) or the reconstructed tri-plane \(P\). Specifically, given a query position \(p\), we project it to each of the planes and retrieve the feature vectors \(F_{xy},F_{xz},F_{yz}\) via bilinear interpolation. Then, the signed distance can be predicted as: \((p)=(F_{xy} F_{xz} F_{yz})\), where \(\) denotes element-wise summation.

Overall, the training objective is defined as fellow:

\[_{comp}=_{geo}+_{lsdf}+_{KL},\] (5)

where \(_{KL}\) is a KL penalty which leans towards a standard normal distribution on the learned latent tri-plane, akin to a Variational Autoencoder .

### Stage 2: Image-Point-Cloud Alignment

We align image and point cloud features within the same feature space. Since the point clouds have been compressed into latent tri-planes with 3D-aware point cloud priors in stage 1, we can project the image features onto this 3D-aware latent. We experiment with two approaches to accomplish this goal. First, following reconstruction methods such as LRM , we can utilize a transformer-based structure to align the image features with the point cloud tri-planes, employing a distance metric like L1 or L2 loss. Alternatively, we can employ a probabilistic diffusion approach, wherein we initiate from random noise and denoise it, guided by the input image targeting the point cloud domain.

Our experiments reveal that the transformer-based model yields unstable results. This instability arises because 3D reconstruction from a single image is an ill-posed problem, as the image lacks information about the occluded object parts. The 3D structural information in an image is not as rich as that in point clouds. Consequently, if we simply adopt a deterministic approach, the alignment result tends to be the average of all possible outcomes for regions that are not sufficiently observed . Thus, we choose a diffusion model as a probabilistic alignment approach. Intuitively, the diffusion model can leverage its prior to envision unobserved regions and generate clear predictions for these areas, as demonstrated in previous 2D prior works [55; 22]. We provide more analysis in Appendices. D.2.

#### 3.2.1 Alignment Model Structure

As shown in Fig. 2, first, we use a pre-trained DINO  image encoder to convert images to a \(S D\) dim feature \(z_{img}\) where \(S\) is the squence length and \(D\) is the feature dimension. Subsequently, the image features serve as conditioning information in the diffusion process, mapping the image feature from the image modality to the point cloud modality.

Unlike previous methods [5; 6; 12] that depend on a single diffusion model to map an arbitrary 1D ordering vector, we employ three parallel UNets to map the image feature to each of the three latent planes, separately. Since each tri-plane contains a 2D-image-like feature, as demonstrated in Appendices, Fig. 6. Prior methods [12; 6] use a single UNet to denoise concatenated planes. However, the UNet is designed without 3D-aware prior, meaning that there will be adjacent pixels without a direct relation due to the convolution operation being a local operation, leading to artifacts.

#### 3.2.2 Alignment Loss Function

We utilize a diffusion model to align the image and the point cloud features. Diffusion Models [14; 40] are designed to learn a data distribution \(q(z_{0})\) by progressively denoising a normally distributed variable. The learning process is equivalent to performing the reverse operation of a fixed Markov Chain with a length of \(T\), which transforms latent \(z_{0}\) into purely Gaussian noise \(z_{T}(0,I)\) over \(T\) time steps. The forward step in this process is defined as:

\[q(z_{t}|z_{t-1})=(z_{t};}z_{t-1},_{t}I),\] (6)

where noisy variable \(z_{t}\) is derived by scaling the previous noise sample \(z_{t-1}\) with \(}\) and adding Gaussian noise following a variance schedule \(_{1},_{2},,_{T}\).

We train three parallel diffusion models on each of the three planes by learning to reverse the above diffusion process. To achieve this, we adopt the approach proposed by Aditya _et al_. , wherein we use three neural networks that takes \(z_{t}\) to directly predict \(z_{0}\) corresponding to XY, XZ, and YZ planes. Specifically, given a uniformly sampled time step \(t\) from the set \(\{1,...,T\}\), we generate \(z_{t}\) by sampling noise from the input latent tri-plane \(z_{0}\). A time-conditioned denoising autoencoder consisted of three parallel UNets , denoted by \(\), is employed to reconstruct \(z_{0}\) from \(z_{t}\) given the alignment source, _i.e._, the image latent feature \(z_{img}\). The objective of the alignment is given by:

\[_{align}=||(z_{t},z_{img},(t))-z_{0}||^{2},\] (7)

where \(()\) represents a positional embedding and \(||||^{2}\) denotes the mean squared error (MSE) loss.

## 4 Experiments

**Dataset:** Following previous works [54; 15], our model is trained on a subset of Objaverse  with 140k filtered 3D assets, and evaluated on unseen Objaverse objects and Google Scanned Objects . The Objaverse dataset contains over 800k 3D assets. It was filtered to remove objects with thin faces and repeated/similar buildings, resulting in a final training set of 140k objects. More details can be found in Appendices. B.

**Baselines:** To assess the effectiveness of our approach, we compare our method with state-of-the-art approaches, including One-2-3-45 , LRM , SyncDreamer , Wonder3D , Magic123 , TGS , LRM  and CRM .

### Implementation Details

During training, we employ a two-stage training pipeline. In stage 1, a point cloud compressor is trained, comprising a transformer-based point feature extractor and a tri-plane autoencoder. Givena point cloud, we sample \(8192\) points to feed them into the point feature extractor followed by a KNN-based sampler to obtain \(512\) point features. Subsequently, the point features are projected onto three orthogonal planes to generate a tri-plane with a \(128 128\) spatial resolution. Then we compress the tri-plane into a latent tri-plane of size (3\(\)2\(\)32\(\)32) which has a feature dimension of \(2\). All components including the point feature extractor, tri-plane autoencoder, and two SDF MLPs, are trained in an end-to-end manner. We utilize 32 NVIDIA V100 GPUs to train for 20 epochs with a batch size of 25 and Adam  optimizer, which takes approximate 3 days.

In stage 2, the compressed latent tri-planes are utilized as supervision signals to train our alignment network. For an input image of size (512\(\)512), we use a pre-trained DINO  to extract image feature of size \(1025 768\), where 768 is the feature dimension and 1025 is \(32 32\) image patches plus \(1\) global image feature. The image features serve as conditioning signal through cross-attention, guiding the diffusion process of the three UNets to generate three planes. During the alignment training, only the UNets are trained, while the DINO remains fixed. The training cost for the alignment is relatively low since we can pre-encode all the point clouds as well as all the source images. We employ 8 V100 GPUs to train the second stage model for 2 days with a batch size of 64.

Figure 3: Rendered images of shapes reconstructed by various methods from single images. The upper samples are from Objaverse and the lowers are from Google Scanned Objects.

The inference stage requires only a single-view image of an object. The process starts by encoding this image into a feature representation using the DINO image encoder. This image feature is then aligned to a latent tri-plane representation through three independent diffusion UNets. The latent tri-plane is subsequently upsampled to a high-resolution tri-plane using the plane decoder and refiner from the first training stage. We then employ the SDF MLP to decode the signed distance for any query position on the tri-plane, which allows us to use the Marching Cubes algorithm to reconstruct a mesh from the tri-plane.

### Comparisons with state-of-the-arts

**Qualitative Results:** Fig. 3 presents a qualitative comparisons between our approach and other state-of-the-art approaches including One-2-3-45 , LRM  and CRM . Since LRM is not open-sourced, we use OpenLRM , an open-sourced implementation of LRM for comparisons. For the other baselines, we use their official codes and model weights. As shown in Fig. 3. While impressive results can be obtained with these methods, geometric distortions often occur due to ambiguities caused by single image or multi-view inconsistency. Additionally, these approach commonly fail for unseen areas of the input image, such as the inside of a cup. In contrast, our method leverages point cloud priors to reconstruct 3D meshes with better geometry and more details than all other baselines. Moreover, our model does not rely on multi-view consistency, unlike One-2-3-45, LRM, and CRM, thanks to the point cloud prior. This results in significantly reduced geometric distortions, as seen in the box, sofa and bowl examples in Fig. 3.

**Quantitative Results:** following previous studies [54; 48; 15], we use the Google Scanned Objects (GSO) dataset  to evaluate the effectiveness of our method. In line with CRM  and One-2-3-45 , we use Chamfer Distance (CD) [59; 7], Volume IoU and F-Score with a threshold of 0.05 to evaluate the mesh geometry. The results are shown in Tab. 1. It can be seen that our method outperforms all of the baselines on all the evaluation metrics. This demonstrates the effectiveness of our method for 3D reconstruction. Notably, our model generates mesh within only 6 seconds on an NVIDIA V100 GPU.

   Method & Chamfer Dist.\(\) & Vol. IoU\(\) & F-Score.(\%)\(\) \\  One-2-3-45  & 0.0172 & 0.4463 & 72.19 \\ SyncDreamer  & 0.0140 & 0.3900 & 75.74 \\ Wonder3D  & 0.0186 & 0.4398 & 76.75 \\ Magic123  & 0.0188 & 0.3714 & 60.66 \\ TGS  & 0.0172 & 0.2982 & 65.17 \\ OpenLRM [15; 13] & 0.0168 & 0.3774 & 63.22 \\ LGM  & 0.0117 & 0.4685 & 68.69 \\ CRM  & 0.0094 & 0.6131 & 79.38 \\  Ours & **0.0083** & **0.6235** & **85.40** \\   

Table 1: Quantitative comparisons for the geometry quality between our method and baselines.

Figure 4: Qualitative comparisons of different latent representations.

### Ablation Study

For our ablation study, we use a small subset of the Oboiverse dataset containing 3000 game assets (weapons, gear, _etc._) for training, and 216 objects for evaluation.

**Effectiveness of latent tri-plane representation:** In our method, we compress the point cloud as a latent tri-plane representation, which is then used for image-point-cloud feature alignment. In this study, we evaluate the effectiveness of this proposed latent tri-plane representation.

We compare three different variants in our ablation study: a) latent tri-plane w/ \(_{lsdf}\): our introduced hierarchical decoding strategy, constraining latent tri-plane with a SDF loss; b) latent tri-plane w/o \(_{lsdf}\), latent tri-plane without the SDF loss; c) _vector_, a vector with dim (6114\(\)1) (same size with (3\(\)2\(\)32\(\)32)) is used to represent the point cloud latent. We train the stage 1 compression models for point cloud-based reconstruction. As shown in Tab. 2, simply using a latent vector representation without spatial structure does not perform well with a performance of 4.81. However, converting the vector representation to tri-plane improves the performance to 1.95. Moreover, tri-plane with latent SDF loss (Ours in Tab. 2) can further refine the final results to 1.81. This emphasizes the importance of retaining 3D-aware structure in the point cloud latent representation and the necessarily of introducing the latent SDF loss \(_{lsdf}\).

In Fig. 4, we provide illustrations of different sizes and representations. We observe that a vector size of 512 performs poorly, which is inadequate to represent the 3D shapes. A vector with dimensions of \(6114 1\) can recover the structure of a 3D mesh, but details such as the trigger are missing. In Fig. 4 (d), we show that tri-plane structure can improve the visual performance with better trigger. In Fig. 4 (e), further refined results with better details can be obtained, validating the effectiveness of the latent SDF loss.

**Image-point-cloud alignment model vs. ULIP:** In order to validate the effectiveness of our proposed image-point-cloud alignment model, we compare our method with ULIP , which is designed for point cloud feature alignment with text and images.

_ULIP-based image reconstruction:_ ULIP uses a pre-trained CLIP and point cloud encoder to encode text, images, and point clouds to a feature of size (512\(\)1). Contrastive loss is utilized for aligning the features of point clouds, text, and images. To evaluate the performance of ULIP for 3D reconstruction, we design a decoder network that decodes the (512\(\)1)-dimension point cloud feature vector to a 3D mesh. We then use the decoder network to directly replace the point cloud features with the aligned image feature obtained by ULIP for 3D mesh reconstruction. As shown in Tab. 2, results obtained by ULIP  are unstable, leading to worse performance in charmer distance, which proves that the scheme of feature alignment used in ULIP  works poor for 3D reconstruction. On contrary, our method can obtain better results.

   Method & CD(\( 10^{4}\))\(\) \\  ULIP  & 35.23 \\ _vector_ (6114\(\)1) & 4.81 \\ latent tri-plane w/o \(_{lsdf}\) & 1.95 \\   latent tri-plane w/\(_{lsdf}\) (Ours) & **1.81** \\   

Table 2: Comparisons of different representations.

   Method &  Single \\ Diffusion \\  & 
 Parallel \\ Diffusion \\  \\  CD(\( 10^{3}\))\(\) & 3.92 & 2.59 \\   

Table 3: Quantitative comparison between parallel diffusion UNet and single UNet.

Figure 5: Green objects are generated from parallel UNets and gray samples are from single UNet.

**Parallel Diffusion vs. Single Diffusion:** To better retain 3D structure information and reduce the interference of each plane (XY, XZ, YZ) in latent tri-planes, we enlist the use of independent parallel diffusion operations to transfer image features to each respective plane. We valid the effectiveness of using three parallel diffusion operations to do the feature alignment instead of using a single UNet in Tab. 3. Parallel UNet variant has smaller CD error, and we show qualitative comparisons in Fig. 5. We can observe that, thanks to the parallel UNet design, our model can focuses on aligning image feature to a specific plane and thus presents better visual quality. For example, the surface of the ice cream bar is broken in the reconstruction of the single UNet variant, while ours can maintains the continuity of the surface. Notably, to compare equally, we reduce the parameters of parallel diffusion to guarantee the same parameter size of parallel diffusion and single diffusion.

We provide more ablation studies as well as more visualization results in the appendix.

## 5 Conclusions

In this work, we present the Large Image and Point Clouds Alignment Model (LAM3D), which harnesses the prior of 3D point clouds for high-fidelity 3D mesh creation. We design an effective point-cloud-based network to compress point cloud into compact and meaningful latent tri-plane. Subsequently, taking a single image as input, Image-Point-Clouds Feature Alignment is designed to transfer image features to latent tri-planes, enriching the image features with accurate and effective 3D information for high-fidelity 3D mesh reconstruction. We believe that LAM3D has potential to improve 3D content creation and assist the workflow of digital artists.

_Limitations and Future Work:_ The main goal of the proposed LAM3D is geometry reconstruction but texture reconstruction is not included, so that our method cannot achieve textured mesh reconstruction. We will further extend LAM3D to geometric and texture reconstruction in the future.

_Broader Impact:_ We hold the view that LAM3D has the potential to enhance 3D content creation and aid digital artists in their workflow. LAM3D was conceived with these applications in mind, and we hope it can evolve into a useful tool that reduce the labour cost in 3D asset production. While we do not foresee any immediate harmful uses for LAM3D, we believe it is crucial for users to exercise caution to minimize impacts, considering that the alignment framework can also be employed for harmful intents.