# Constrained Sampling with

Primal-Dual Langevin Monte Carlo

 Luiz F. O. Chamon

University of Stuttgart

luiz.chamon@simtech.uni-stuttgart.de&Mohammad Reza Karimi

ETH Zurich

mkarimi@inf.ethz.ch

Anna Korba

CREST, ENSAE, IP Paris

anna.korba@ensae.fr

###### Abstract

This work considers the problem of sampling from a probability distribution known up to a normalization constant while satisfying a set of statistical constraints specified by the expected values of general nonlinear functions. This problem finds applications in, e.g., Bayesian inference, where it can constrain moments to evaluate counterfactual scenarios or enforce desiderata such as prediction fairness. Methods developed to handle support constraints, such as those based on mirror maps, barriers, and penalties, are not suited for this task. This work therefore relies on gradient descent-ascent dynamics in Wasserstein space to put forward a discrete-time primal-dual Langevin Monte Carlo algorithm (PD-LMC) that simultaneously constrains the target distribution and samples from it. We analyze the convergence of PD-LMC under standard assumptions on the target distribution and constraints, namely (strong) convexity and log-Sobolev inequalities. To do so, we bring classical optimization arguments for saddle-point algorithms to the geometry of Wasserstein space. We illustrate the relevance and effectiveness of PD-LMC in several applications.

## 1 Introduction

Sampling is a fundamental task in statistics, with applications to estimation and decision making, and of growing interest in machine learning (ML), motivated by the need for uncertainty quantification and its success in generative tasks [1; 2; 3]. In these settings, the distribution we wish to sample from (_target distribution_) is often known only up to its normalization constant. This is the case, for instance, of score functions learned from data or posterior distributions of complex Bayesian models. Markov Chain Monte Carlo (MCMC) algorithms can be used to tackle this problem [4; 5] and Langevin Monte Carlo (LMC), in particular, has attracted considerable attention due to its simplicity, theoretical grounding, and effectiveness in practice [3; 6; 7; 8; 9]. These sampling algorithms, however, do not naturally incorporate requirements on the samples they generate. Specifically, standard MCMC methods do not enforce restrictions on the target distributions, such as support (e.g., truncated Gaussian), conditional probabilities (e.g., fairness), or moments (e.g., portfolio return) constraints. This limitation is often addressed by post-processing, transforming variables, or by introducing penalties in the target distribution. Though successful in specific settings, these approaches have considerable downsides. Post-processing techniques such as _rejection sampling_ (see, e.g., [10; 11]) may substantially reduce the effective number of samples (number of samples generated per iteration of the algorithm). Variable transformations based on link functions, projections, or mirror/proximal maps (see, e.g., [12; 13; 14; 15; 16; 17; 18]) only accommodate (deterministic) support constraints and are not suitedfor statistical requirements such as robustness or fairness [19; 20; 21; 22]. Though modifying the target distribution directly offers more flexibility (see, e.g., ), it does not _guarantee_ that constraints are satisfied (Table 1). We refer the reader to Appendix A for a more detailed literature review.

This paper overcomes these issues by directly tackling the _constrained sampling_ problem. Explicitly, it seeks to sample not from a target distribution \(\) on \(^{d}\), but from the distribution \(^{}\) that solves

\[ P^{}_{_{2}( ^{d})}&(\|)\\ &_{x} g_{i}(x) 0,\ i=1,,I,\\ &_{x}h_{j}(x)=0,\ j=1,,J,\] (PI)

where \(_{2}(^{d})\) denotes the set of probability measures on \(^{d}\) with bounded second moments and the functions \(g_{i},h_{j}\) represent the requirements. Note that (PI) only considers measures \(\) against which \(g_{i},h_{j}\) are integrable. Otherwise, the expectations are taken to be \(+\), making the corresponding measure infeasible. Observe that (PI) is more general than the support-constrained sampling problem considered in, e.g., [12; 13; 14; 15; 16; 17; 18]. Indeed, it constrains the distribution \(\) rather than its samples \(x\) (Table 1). Algorithms based on projections, barrier, or mirror maps are not suited for this type of constraints (see Section 2.2 for more details). To tackle (PI), this paper instead derives and analyzes a primal-dual LMC algorithm (PD-LMC) that is the sampling counterpart of gradient descent-ascent (GDA) methods from (Euclidean) optimization. A dual ascent algorithm was previously proposed to tackle (PI), but it requires the exact computation of expectations with respect to intractable distributions . This paper not only overcomes this limitation, but also provides convergence guarantees for a broader class of constraint functions.

The main contributions of this work include:

* a discrete-time constrained sampling algorithm (PD-LMC, Algorithm 1) for solving (PI) that precludes any explicit integration (Section 3);
* an analysis of PD-LMC proving that it converges sublinearly (in expectation) with respect to the Kullback-Leibler divergence (convex case) or Wasserstein distance (strongly convex case). The analysis is performed directly on the discrete-time iterations and requires only local Lipschitz continuity and bounded variance assumptions (Section 3.1);
* an extension of these result for target distributions satisfying a log-Sobolev inequality (LSI) for a variant of PD-LMC (Algorithm 2, Section 3.2);
* numerical examples illustrating applications of (PI) and the effectiveness of PD-LMC (Section 4).

## 2 Problem formulation

### Background on Langevin Monte Carlo

Consider a target distribution \(_{2}(^{d})\) absolutely continuous with respect to Lebesgue measure whose density (also denoted \(\)) can be expressed as \((x)=e^{-f(x)}/Z\) for some normalization constant \(Z\). Define the Kullback-Leibler (KL) divergence of \(\) with respect to \(\) as

\[(\|)=()d= fd+ ()d-(Z)()+()-(Z),\] (1)

where \(\) is the Radon-Nikodym derivative, \(\) is the _potential energy_, and \(\) is the _negative entropy_ if \(\) is absolutely continuous with respect to \(\); and \(+\) otherwise. For a wide class of functions \(f\) (e.g.,

   & Soft constraint & Hard constraint \\  Sample (\(x\)) & â€” & Mirror/proximal LMC [12; 14; 15; 17], \\  & & projected LMC , barriers [16; 18] \\ Distribution (\(\)) & Penalized LMC  & **PD-LMC** \\  

Table 1: Type and target of sampling constraints.

smooth and strongly convex), samples from \(\) can be obtained from the path of the _Langevin diffusion_ process, whose instantaneous values \(x(t)\) have distributions \((t)\) evolving according to the _Fokker-Planck equation_. Explicitly,

\[dx(t)=- f(x(t))dt+dW(t)=(t)_{W_{2}}((t)\|) ,\] (2)

for a \(d\)-dimensional Brownian motion \(W(t)\), where \( q\) denotes the divergence of \(q\) and \(_{W_{2}}(\|)\) denotes the Wasserstein-2 gradient of \((|)\) at \(\)[26, Theorem 10.4.17] (see Appendix B for more details). Indeed, the Langevin diffusion (2) brings the distribution \((t)\) of \(x(t)\) progressively closer to the target \(\). In fact, the Fokker-Planck equation can be interpreted as a gradient flow of the KL divergence with respect to the Wasserstein-2 distance [7; 25].

However, computing the path of the stochastic differential equation in (2) is not practical and discretizations are used instead. Chief among them is the (forward) Euler-Maruyama scheme, which leads to the celebrated Langevin Monte Carlo (LMC) algorithm 

\[x_{k+1}=x_{k}-_{k} f(x_{k})+}_{k},\ \ _{k}}}{{}}(0,_{d}),\] (3)

for a step size \(_{k}>0\), where \(_{d}\) denotes the \(d\)-dimensional identity matrix. Notice that it is not necessary to know \(Z\) in order to evaluate (3). This has made LMC and its variants widely popular in practice and the subject of extensive research. Despite (3) being a biased time-discretizations of the Langevin diffusion in (2) , rates of convergence of LMC have been obtained for smooth and strongly convex [8; 27] or convex  potentials or when the target distribution \(\) verifies an LSI .

### Constrained sampling

Our goal, however, is not to sample from \(\) itself, but from a distribution close to \(\) that satisfies a set of statistical requirements. Explicitly, we wish to sample from a distribution \(^{}\) that solves (PI). Since (PI) constrains the distribution \(\) rather than its samples \(x\), it can accommodate more general requirements than the support constraints typically considered in constrained sampling (e.g., [12; 13; 14; 15; 16; 17; 18]). Next, we illustrate the wide range of practical problems that can be formulated as (PI). These examples are further explored in Section 4 and more details on their formulations are provided in Appendix E.

1. **Sampling from convex sets**: though we have stressed that (PI) accommodates other types of requirements, it can also be used to constrain the support of \(\), i.e., to sample from \[^{}*{argmin}_{_{2}(^{d})} (\|)\] (PII) \[_{x}[x]=1,\] for a closed convex set \(^{d}\). Indeed, let \(\) be the intersection of the \(0\)-sublevel sets of convex functions \(\{s_{i}\}_{i=1,,I}\). Such a description always exists (see Appendix E). Then, (PII) can be cast as (PI) using \(g_{i}(x)=[s_{i}(x)]_{+}\) for \([z]_{+}=(0,z)\). Notice that the \(g_{i}\) are convex and that although they are not everywhere differentiable, \((s_{i}(x)>0) s_{i}(x)\) is a _subgradient_ of \(g_{i}\), where \(()=1\) on the event \(\) and \(0\) otherwise. Observe that support constraints can also be imposed using projections, mirror/proximal maps, and barriers as in [12; 13; 14; 15; 16; 17; 18]. These methods, however, constrain the samples \(x\) rather than their distribution \(\) as in (PII).
2. **Rate-constrained Bayesian models**: rate constraints have garnered attention in ML due to their central role in fairness [20; 21]. Consider data pairs \((x,y)\), where \(x\) are features and \(y\{0,1\}\) labels, and a protected (measurable) subgroup \(\). Let \(\) be a Bayesian posterior of the parameters \(\) of a model \(q(;)\) denoting the probability of a positive outcome (based, e.g., on a binomial model). We wish to enforce statistical parity, i.e., we wish the prevalence of positive outcomes within the protected group \(\) to be close to or higher than in the whole population. We cast this problem as (PI) by constraining the average probability of positive outcome as in \[P^{}=_{_{2}(^{d})} (\|)\] (PIII) \[_{x,}q(x; )_{x,}q(x; )-.\] where \(>0\) denotes our tolerance . Naturally, multiple protected groups can be accommodated by incorporating additional constraints. Hence, constrained sampling provides a natural way to encode fairness in Bayesian inference.

3. **Counterfactual sampling**: rather than imposing requirements on probabilistic models, constrained sampling can also be used to probe them by evaluating _counterfactual_ statements. Indeed, let \(\) denote a reference probabilistic model such that sampling from \(\) yields realizations of the "real world." Consider the _counterfactual_ statement "how would the world have been if \([g(x)] 07\)" Constrained sampling not only gives realizations of this alternative world, but it also indicates its "compatibility" with the reference model, namely the value \(P^{}\) of (PI). More concretely, consider a _Bayesian stock market_ model. Here, \(\) is a posterior model for the (log-)returns of \(I\) assets, e.g., distributed as Gaussians \((,)\). Here, the vector \(\) describes the mean return of each stock and \(\) their covariance. We can investigate what the market would look like if, e.g., the mean and variance of each stocks were to change by solving \[P^{}=_{_{2}(^{d})} (\|)\] (PIV) \[ _{(,)}_{i}=_{i}, i=1,,I\] \[_{(,)}_{ii} _{i}^{2}\] Due to correlations in the market, certain choices of \(_{i}\) or \(_{i}^{2}\) may be more "unrealistic" than others. Additionally, it could be that some of these conditions are vacuous conditioned on the others. As we show next, our approach to tackling (PI) effectively isolates the contribution of each requirement in the solution \(^{}\), thus enabling us to identify which are (conditionally) vacuous and which are most at odds with the reference model \(\).

### Lagrangian duality and dual ascent algorithms

Although directly sampling from \(^{}\) does not appear straightforward, it admits a convenient characterization based on convex duality that is amenable to be sampled using the LMC algorithm (3). Indeed, let \(g:^{d}^{I}\) and \(h:^{d}^{J}\) be vector-valued functions collecting the constraint functions \(g_{i}\) and \(h_{j}\) respectively. The Lagrangian of (PI) is then defined as

\[L(,,)(\|)+^{}\,_ {}[g]+^{}\,_{}[h]=(\|_{})+ (}),\] (4)

for \(^{I}_{+}\) and \(^{J}\), where

\[_{}(x)=}{Z_{}} U(x,,)=f(x)+^{}g(x)+^{}h(x)\] (5)

and a normalization constant \(Z_{}\). Notice that \(P^{}=_{}_{ 0,\,}L(,,)\), which is why (PI) is referred to as the _primal problem_.

To obtain the _dual problem_ of (PI), define the dual function

\[d(,)_{_{2}(^{d})}\ L(, ,).\] (6)

Notice from (4) that the minimum in (6) is achieved for \(_{}\) from (5), the _Lagrangian minimizer_, so that \(d(,)=(Z/Z_{})\). The solution of (6) is therefore a _tilted_ version of \(\), whose tilt is controlled by the _dual variables_\((,)\). Since (6) is a relaxation of (PI), it yields a lower bound on the primal value, i.e., \(d(,) P^{}\) for all \((,)^{I}_{+}^{J}\). The dual problem seeks the tilts \((^{},^{})\) that yield the best lower bound, i.e.,

\[D^{}_{^{I}_{+},\,^{J}} \ d(,).\] (DI)

The set \(^{}=*{argmax}_{ 0,\,}\ d(,)\) of solutions of (DI) is called the set of _Lagrange multipliers_. Note from (6) that (DI) depends on the distributions \(\) and \(\) through its objective \(d\).

The dual problem (DI) has several advantageous properties. Indeed, while the primal problem (PI) is an _infinite dimensional, smooth_ optimization problem in probability space, the dual problem (DI) is a _finite dimensional, non-smooth_ optimization problem in Euclidean space. What is more, it is a concave problem regardless of the functions \(f,g,h\), since the dual function (6) is the minimum of a set of affine functions in \((,)\)[30, Prop. 4.1.1]. These properties are all the more attractive given that, under mild conditions stated below, (DI) can be used to solve (PI).

**Assumption 2.1**.: There exists \(^{}_{2}(^{d})\) with \((^{}\|) C<\) such that \(_{^{}}[g_{i}]-<0\) and \(_{^{}}[h_{j}]=0\) for all \(i,j\).

**Proposition 2.2**.: _Under Assumption 2.1, the following holds:_

1. \(P^{}=D^{}\)_;_
2. _there exists a finite pair_ \((^{},^{})^{}\)_;_
3. _for any solution_ \(^{}\) _of (_PI_) and_ \((^{},^{})\) _of (_DI_), it holds that_ \[L(^{},,) L(^{},^{},^{}) L (,^{},^{}),(,,) _{2}(^{d})^{I}_{+}^{J} {;}\] (7)
4. _the solution of (_PI_) is_ \(^{}=_{^{},^{}}\) _for_ \((^{},^{})^{}\)_;_
5. _consider the perturbation of (_PI_)_ \[P^{}(u,v)_{_{2}(^{d})}(\|)\ \ \ \ _{x}g_{i}(x) u_{i},\ _{x}h_{j}(x)=v_{j}\] (PV) _Then,_ \((^{},^{})\) _are subgradients of_ \(P^{}(0,0)=P^{}\)_, i.e.,_ \(P^{}(u,v) P^{}-^{}u-^{}v\)_, and if_ \(P^{}(u,v)\) _is differentiable at_ \((0,0)\)_, then_ \(_{u}P^{}(u,v)=-^{}\) _and_ \(_{v}P^{}(u,v)=-^{}\) _at_ \((0,0)\)_._

Proof.: In finite dimensional settings, (i)-(v) are well-known duality results (see, e.g., ). While they also hold for infinite dimensional optimization problems, their proofs are slightly more "scattered" We collect their reference below. The objective of (PI) is a convex function and its constraints are linear functions of \(\). Hence, (PI) is a convex program. Under Slater's condition (Assumption 2.1), it is (i) strongly dual (\(P^{}=D^{}\)) and (ii) there exists at least one solution \((^{},^{})\) of (DI) (see [31, Sec. 8.6, Thm. 1] or [32, Cor. 4.1]). This implies (iii) the existence of the saddle-point (7) [33, Prop. 2.156], (iv) that \(^{}*{argmin}_{}L(,^{},^{})= \{_{^{},^{}}\}\), since the KL divergence is strongly convex and its minimizer is unique [34, Thm. 7.3.7], and (v) that \((^{},^{})\) are subgradients of the perturbation function \(P^{}(u,v)\)[33, Prop. 4.27]. 

Proposition 2.2 shows that given solutions \((^{},^{})\) of (DI), the constrained sampling problem (PI) reduces to sampling from \(_{^{},^{}} e^{-U(,^{},^{ })}\) (see Appendix F for an explicit example of this result). It is important to note that this results only relies on the KL divergence being (strongly) convex in the standard \(L^{2}\) geometry, i.e., along mixtures of the form \(t_{0}+(1-t)_{1}\) for \(t\). This does not imply that it is (geodesically) convex in the Wasserstein sense [35, Section 9.1.2]. This would require \(U\) in (5) to be convex for all \( 0\) and \(^{J}\), i.e., for \(f,g\) to be convex and \(h\) to be linear.

Hence, Proposition 2.2 reduces the constrained sampling problem (PI) to that of finding the Lagrange multipliers \((^{},^{})\). Despite their finite dimensionality, however, computing these parameters is intricate. Indeed, since (DI) is a concave program, we could obtain \((^{},^{})^{}\) using

\[_{k+1}=_{k}+_{k}\,_{_{_{k}_{k} }}g_{+}_{k+1}=_{k}+_{k}\, _{_{_{k}_{k}}}[h]\] (8)

for \(_{k}>0\), where we used the fact that \(_{_{_{k}}}[g]\) and \(_{_{_{k}}}[h]\) are (sub)gradients of the dual function (6) at \((,)\)[36, Thm. 2.87]. This procedure is known in optimization and game theory as _dual ascent_ or _best response_. Notice, however, that (8) is not a practical algorithm as it requires explicit integration with respect to the intractable distribution \(_{}\) from (5).

This issue was partially addressed in  (in continuous time and without equality constraints, i.e., \(J=0\)) by replacing the Lagrangian minimizer \(_{_{k}_{k}}\) in (8) by the distribution of LMC samples, as in

\[ x_{k+1}&=x_{k}-_{k} U(x_{k},_{k})+}_{k},\ \ \ _{k}}}{{}}(0,_{d})\\ _{k+1}&=_{k}+_{k}\, _{}[g]_{+},\ \ \ _{k}=(x_{k}).\] (9)

Note that since \(J=0\), we omit the argument \(\) of \(U\) for clarity. Nevertheless, the updates in (9) still require an explicit integration. While it is now possible to sample from \(_{k}\) (namely, using the \(x_{k}\)), empirical approximations of \(_{_{k}}[g]\) may not only require an exponential (in the dimension \(d\)) number of samples (e.g., [39, Thm. 1.2]), but it introduces errors that are not taken into account in the analysis of . In the sequel, we address these drawbacks by replacing these dual ascent algorithms by a saddle-point one.

```
1:Inputs:\(_{k}>0\) (step size), \(x_{0}_{0}\), and \((_{0},_{0})=(0,0)\).
2:for\(k=0,,K-1\)
3:\(x_{k+1}=x_{k}-_{k}_{}U(x_{k},_{k},_{k})+} \,_{k}\), for \(_{k}(0,_{d})\)
4:\(_{k+1}=_{k}+_{k}g(x_{k})_{+}\)
5:\(_{k+1}=_{k}+_{k}h(x_{k})\)
6:end ```

**Algorithm 1** Primal-dual LMC

## 3 Primal-dual Langevin Monte Carlo

Consider the GDA dynamics for the saddle-point problem (DI) in Wasserstein space. Explicitly,

\[ =(t)_{W_{2}}L(t),(t), (t)\] (10a) \[(t)}{ t} =_{_{i}}L(t),(t),(t) _{_{i}(t),+}\] (10b) \[(t)}{ t} =_{_{j}}L(t),(t),(t)\] (10c)

for the Lagrangian \(L\) defined in (4), where \([z]_{,+}=z\) for \(>0\) and \([z]_{,+}=(a,0)\) otherwise (40, Sec. 2.2). Observe that \(_{_{i}}L(,,)=_{}[g_{i}]\) and \(_{_{j}}L(,,)=_{}[h_{j}]\). Hence, the algorithm from  described in (9) involves a _deterministic_ implementation of (10b) that fully integrates over \((t)\). In contrast, we consider a _stochastic_, _single-particle_ implementation of (10) that leads to the practical procedure in Algorithm 1.

```
1:Inputs:\(_{k}>0\) (step size), \(x_{0}_{0}\), and \((_{0},_{0})=(0,0)\).
2:for\(k=0,,K-1\)
3:\(x_{k+1}=x_{k}-_{k}_{}U(x_{k},_{k},_{k})+}\,_{k}\), for \(_{k}(0,_{d})\)
4:\(_{k+1}=_{k}+_{k}g(x_{k})_{+}\)
5:\(_{k+1}=_{k}+_{k}h(x_{k})\)
6:end ```

**Algorithm 2** Primal-dual LMC

Explicitly, we also use an Euler-Maruyama time-discretization of the Langevin diffusion associated to (10a) (step 3), but replace the expectations in (10b)-(10c) by single-sample approximations (steps 4-5). Algorithm 1 can therefore be seen as a particle implementation of the deterministic Wasserstein GDA algorithm (10). As such, it resembles a primal-dual counterpart of the LMC algorithm in (3), which is why we dub it _primal-dual LMC_ (PD-LMC). Alternatively, Algorithm 1 can be interpreted as a stochastic approximation of the dual ascent method in (9). This suggests that the gradient approximations in steps 4-5 could be improved using mini-batches, which is in fact how  approximates the expectation in (9). Our theoretical analysis and experiments show that these mini-batches are neither necessary nor always worth the additional computational cost (see Section 3.1 and Section 4). Note that the "stochastic approximations" in Algorithm 1 refer to the dual updates (steps 4-5) rather than the LMC update (step 3) as in stochastic gradient Langevin . Though these methods could be combined, it is beyond the scope of this work.

The remainder of this section is dedicated to analyzing the convergence properties of PD-LMC for both stochastic dual gradients (as in Algorithm 1) and exact dual gradient (as in (9)). For the latter, we obtain guarantees for the discrete implementation (9) under weaker assumptions than the continuous-time analysis of . We consider strongly log-concave target distributions in Section 3.1 and those satisfying an LSI in Section 3.2.

### PD-LMC with (strongly) convex potentials

As opposed to the traditional LMC algorithm (3) or the deterministic updates in (9), Algorithm 1 involves three coupled random variables, namely, \((x_{k},_{k},_{k})\). Hence, the LMC update (step 3) is based on a _stochastic_ potential \(U\) and the distribution \(_{k}\) of \(x_{k}\) is now a _random measure_. Our analysis sidesteps this obstacle by using techniques from stochastic optimization. We also leverage techniques from primal-dual algorithms in the Wasserstein space, in the spirit of works such as [7; 8; 42] that studied the LMC (3) or alternative time-discretizations of gradient flows of the KL divergence as splitting schemes.

First, define the potential energy \(\) and the (negative) entropy \(\) for \((,,)_{2}(^{d})^{I}_{+} ^{J}\) as

\[(,,)= U(x,,)d(x) ()=()d,\] (11)

for \(U\) as in (5). Notice from (4) that \(L(,,)=(,,)+()-(Z)\), where we used the KL divergence decomposition in (1). To proceed, consider the following assumptions:

**Assumption 3.1**.: The potential energy \((,,)\) in (11) is \(m\)-strongly convex with respect to \(\) along Wasserstein-2 geodesics for \(m 0\) and all \((,)^{I}_{+}^{J}\). Explicitly,

\[(,,)(_{0},,)+ _{W_{2}}(_{0},,),x-y ds(x,y)+W_{ 2}^{2}(,_{0}),\]

where \(s\) is an optimal coupling achieving \(W_{2}^{2}(,_{0})\) (see Appendix B).

**Assumption 3.2**.: The gradients and variances of \(f,g,h\) are bounded along iterations \(\{_{k}\}_{k 0}\), where \(_{k}\) is the distribution of \(x_{k}\), i.e., there exists \(G^{2}\) such that

\[\| f\|^{2}_{L^{2}(_{k})},\| g_{i}\|^{2}_{L^{2}(_ {k})},\| h_{j}\|^{2}_{L^{2}(_{k})} G^{2}\;\;\;\; \,_{_{k}}[\|g\|^{2}],_{_{k}}[\|h\|^{2}]  G^{2}.\]

Assumption 3.1 holds with \(m=0\) if \(f,g\) are convex and \(h\) is linear. If \(f\) is additionally strongly convex, then it holds with \(m>0\)[26, Prop. 9.3.2]. Assumption 3.2 is typical in (stochastic) non-smooth optimization analyses (see, e.g., ). Notice, however, that gradients are only required to be bounded along trajectories of Algorithm 1, a crucial distinction in the case of strongly convex functions whose gradients can only be bounded locally. Assumption 3.2 can be satisfied under mild conditions on \(f,g,h\), such as local Lipschitz continuity or linear growth.

The following theorem provides the first convergence analysis of the discrete-time PD-LMC.

**Theorem 3.3**.: _Denote by \(_{k}\) the distribution of \(x_{k}\) in Algorithm 1. Under Assumptions 2.1, 3.1, and 3.2, there exists \(R_{0}^{2}\) such that, for \(_{k}\),_

\[_{k=1}^{K}\,(_{k}\,\|\,^{})+ {m}{2}W_{2}^{2}(_{k},^{}) 3 G^{2}+^{2}}{  K}+}{K}_{k=1}^{K}\,[\|_{k}\|^ {2}]+[\|_{k}\|^{2}].\] (12)

_For \(_{k} R_{0}/(G)\) and \(_{k}=_{k}/_{k=1}^{K}_{k}\), we obtain_

\[_{k=1}^{K}_{k}\,(_{k}\,\|\,^{})+ W_{2}^{2}(_{k},^{})G(1+(K))}{ }3+_{k}\;\,[\|_{k}\|^{2}]+[\|_{k}\|^{2}]}.\] (13)

_Additionally, there exists a sequence of step sizes \(_{k}>0\) such that \(W_{2}^{2}(_{k},^{}) R_{0}^{2}\) and \([\|_{k}\|^{2}]+[\|_{k}\|^{2}]<\) for all \(k\). The same results hold (without expectations) when using exact dual gradients, i.e., if the updates in steps 4-5 are replaced by \(_{k+1}=_{k}+_{k}\,_{_{k}}[g]_{+}\) and \(_{k+1}=_{k}+_{k}\,_{_{k}}[h]\)._

Theorem 3.3, whose proof is deferred to Appendix C, implies rates similar to those for GDA schemes in finite-dimensional Euclidean optimization (see, e.g., ). To recover those rates, however, we must bound the magnitudes of \(_{k},_{k}\). In , this is done by bounding the iterates in the algorithm itself, i.e., by projecting them onto the set \(_{r}=\{(,)^{I}_{+}^{J}\;|\; (\|\|^{2},\|\|^{2}) r\}\) and choosing \(r\) such that \(^{}_{r}\) (Proposition 2.2(ii) ensures this is possible). We then incur a bias on the order of \(\) in (12) that vanishes in the decreasing step size setting of (13). Though convenient, this is not _necessary_ since there exists a sequence of step sizes such that both \([\|_{k}\|^{2}]\) and \([\|_{k}\|^{2}]\) are bounded for all \(k 0\). In the interest of generality, Theorem 3.3 holds without these hypotheses. It is worth noting that though faster rates and last iterates guarantees can be obtained for Euclidean saddle-point problems, they rely on more complex schemes than the GDA in Algorithm 1 involving acceleration or proximal methods .

The results in Theorem 3.3 are stated for the stochastic scheme in Algorithm 1. However, Theorem 3.3 yields the same rates (without expectations) for exact dual gradients, i.e., for the dual ascent scheme (9). In this case, the second condition in Assumption 3.2 simplifies to \(_{k}(\|\,_{_{k}}[g]\|^{2},\|\,_{_{k}}[h]\|^{2})  G^{2}\). Not only are these milder assumptions than [24, Eq. (16)], but the guarantees hold for discrete- rather than continuous-time dynamics. Finally, (12)-(13) imply convergence with respect to the KL divergence for convex potentials (\(m=0\)) with stronger guarantees in Wasserstein metric for strongly convex ones (\(m>0\)).

The convergence rates for distributions \(_{k}\) from Theorem 3.3 also imply convergence rates for empirical averages across iterates \(x_{k}\) of Algorithm 1. This corollary is obtained by combining (12)-(13) with the following proposition. By taking \(\) to be the constraint functions \(g\) or \(h\) from (PI) yields feasibility guarantees for PD-LMC.

**Proposition 3.4**.: _Consider samples \(x_{k}\) distributed according to \(_{k}\) and \(c_{k} 0\) with \(_{k=1}^{K}c_{k}=1\) such that \(_{k=1}^{K}c_{k}(_{k}\,\|\,^{})+W_{2}^{2}(_{k},^{})_{K}\) for \(m 0\). Then, it holds that_

\[|_{k=1}^{K}c_{k}(x_{k})-_{ ^{}}[]|},&\\ }{m}},&\]

Proof.: See Appendix C.

### PD-LMC with LSI potentials

In this section, we replace Assumption 3.1 on the convexity of the potential by an LSI common in the sampling literature. We consider only inequality constraints (\(J=0\)) here and omit the function arguments \(\), since accounting for equality constraints requires significant additional assumptions.

**Assumption 3.5**.: The distribution \(_{}\) satisfies the LSI for bounded \(\), i.e., there exists \(>0\) such that \(2(\|_{})\|(d/d _{})\|_{L^{2}()}^{2}\) for all \(_{2}(^{d})\).

The LSI in Assumption 3.5 is often used in the analysis of the standard LMC algorithm . It holds, e.g., when \(f\) is strongly convex and \(g\) is a (possibly non-convex) bounded function due to the Holley-Stroock perturbation theorem . In fact, if \(f\) is \(1\)-strongly convex and \(|g|\) is bounded by \(1\), then Assumption 3.5 holds for \( e^{-2}\) (see, e.g., [51, Prop. 5.1.6] or [52, Thm 1.1]). The LSI is akin to the Polyak-Lojasiewicz (PL) condition from Euclidean optimization , which supposes issues with GDA methods such as Algorithm 1. Indeed, it is not enough for the Lagrangian (4) to satisfy the PL condition in the primal variable to guarantee the convergence of GDA in Euclidean spaces. We must either modify Algorithm 1 using acceleration or proximal methods  or impose the PL condition also on \(\). Since the Lagrangian 4 is linear in \(\), it is clear that Algorithm 1 will not suffice to provide theoretical guarantees in the LSI case.

We therefore consider the variant in Algorithm 2, where \(N_{k}^{0}\) LMC iterations (step 3) are executed before updating the dual variables (step 4). This is akin to using different time-scales in continuous-time, a common technique for solving saddle-point problems . Since it resembles a dual ascent counterpart of the LMC algorithm (3), we refer to it as _(stochastic) dual LMC_ (DLMC). As opposed to the dual ascent algorithm from  in (9), however, Algorithm 2 does not require any explicit evaluation of expected values. The following theorem provides an analysis of its convergence.

**Theorem 3.6**.: _Assume that the functions \(f,g\) are \(M\)-smooth, i.e., have \(M\)-Lipschitz continuous gradients, satisfy Assumption 3.5, and that \(_{}[\|g\|^{2}] G^{2}\) for all \(_{2}(^{d})\). Let \(0<_{k}\), \(0< G^{2}<1\),_

\[_{k}=}, N_{ k}^{0}((_{0}\|_{ _{k}})}{}).\]

_Under Assumption 2.1, there exists \(B<\) such that the distributions \(\{_{k}\}\) of the samples \(\{x_{N_{k}^{0}}\}\) generated by Algorithm 2 satisfy_

\[_{k=0}^{K-1}(_{k}\|^{}) +}{2}+}{ K}.\] (14)

_Recall from (PI) that \(I\) is the number of inequality constraints. Additionally, \([\|_{k}\|_{1}]\) is bounded for all \(k\)._Theorem 3.6, whose proof is deferred to Appendix D, provides similar guarantees as (approximate) subgradient methods in finite-dimensional optimization (see, e.g., ). This is not surprising seen as \(_{k},N_{k}^{0}\) in Theorem 3.6 are chosen to ensure that step 4 yields a sample \(x_{k}_{k}\) such that \((_{k}\|_{_{k}})\) using [29, Theorem 1]. At this point, \(g(x_{N_{k}^{0}})\) in step 5 is an approximate, stochastic subgradient of the dual function (6). Though it may appear from (12) and (14) that Algorithms 1 and 2 have the same convergence rates, an informal computation shows that the latter evaluates on the order of \(d^{2}/\) as many gradient per iteration, where \(=M/\). Note that we can once again apply Theorem 3.6 to derive ergodic average and feasibility guarantees for Algorithm 2.

## 4 Experiments

We now return to the applications described in Section 2.2 to showcase the behavior of PD-LMC. We defer implementation details and additional results to Appendix E. Code for these examples is publicly available at https://www.github.com/lfochamon/pdlmc.

**1. Sampling from convex sets.** We cast the problem of sampling from a Gaussian distribution \((0,1)\) truncated to \(=\) as (PI) by taking \(f(x)=x^{2}/2\) and \(g(x)=[(x-1)(x-3)]_{+}\) (see Section 2.2). Fig. 1 shows histograms for the samples obtained using PD-LMC, the projected LMC (Proj. LMC) from , and the mirror LMC from , all with the same step size. Both Proj. LMC and Mirror LMC generate an excess of samples close to the boundary (between \(1.5\) and \(3\) times more samples than expected). This leads to an underestimation of the mean (Proj. LMC: \(1.488\,\)/ Mirror LMC: \(1.470\) vs. true mean: \(1.510\)). In contrast, PD-LMC provides a more accurate estimate (\(1.508\)). Yet, since it constrains the distribution \(\) rather than its samples, it is not an _interior-point method_ and can produce samples outside of \(\). Theorems 3.3-3.6 show that this becomes less frequent as the algorithm progresses (in Fig. 1, only \(2\%\) of the samples are not in \(\)). This occurs even without using _mini-batches_ in steps 4-5 of Algorithm 1 as in . In fact, our experiments show that _mini-batches_ increase the computational complexity with no performance benefit (Appendix E). These issues are exacerbated in more challenging problems, such as sampling from a two-dimensional standard Gaussian centered at \(\) restricted to an unit \(_{2}\)-norm ball (Fig. 2). In this case, Proj. LMC places almost \(25\%\) of its samples on the boundary (where only \(0.14\%\) of samples should be), while PD-LMC only places \(1.8\%\) of its samples outside of the support. Mirror LMC provides a better mean estimation in this setting, although a bit more asymmetric than PD-LMC [Mirror LMC: \((0.312,0.418)\) vs. PD-LMC: \((0.446,0.444)\) vs. true mean: \((0.368,0.368)\)].

**2. Rate-constrained Bayesian models.** Here, we consider \(\) to be the posterior of a Bayesian logistic regression model for the Adult dataset from , where the goal is to predict whether an individual

Figure 1: Sampling from a 1D truncated Gaussian (ground truth displayed as dashed lines).

Figure 2: Sampling from a 2D truncated Gaussian (true mean in red and sample mean in orange).

makes more than \(\$50\)k based on socioeconomic information (details on data pre-processing can be found in ). We consider a standard Gaussian prior on the parameters \(^{d+1}\) of the model, where \(d\) is the number of features. Using the LMC algorithm to sample from the posterior (i.e., no constraints), we find that while the average probability of positive predictions is \(19.1\%\) over the whole test set, it is \(26.2\%\) among males and \(5\%\) among females ("Unconstrained" in Fig. 4). To overcome this disparity, we take _gender_ to be the protected class in (PIII), constraining both \(_{}\) and \(_{}\) with \(=0.01\). Using PD-LMC, we obtain a Bayesian model that leads to an average probability of positive outcomes of \(18.1\%\) and \(15.1\%\) for males and females respectively. In fact, we now observe a substantial overlap of the distributions of positive predictions across genders for the constrained posterior \(^{}\) ("Constrained (\(=0.01\))" in Fig. 4). This substantial reduction of prediction disparities comes at only a minor decline in accuracy (unconstrained: \(84\%\) vs constrained: \(82\%\)).

**3. Counterfactual sampling.** Though the distribution of positive predictions changes considerably for both male and female individuals, the final dual variables (\(_{}=0\) and \(_{} 160\)) show that these changes are due uniquely to the _female_ group [as per Prop. 2.2(iv)]. This implies that the reference model \(\) is itself compatible with the requirement for the male group, but that reducing the disparity for females requires considerable deviations from it. By examining \(_{}\), we conclude _without recalculating_\(^{}\) that even small changes in the tolerance \(\) for the female constraint would substantially change the distribution of outcomes [Prop. 2.2(v)]. This is confirmed by "Constrained (\(=0.03\))" in Fig. 4. Notice that this is only possible due to the primal-dual nature of PD-LMC. This type of counterfactual analysis is even more beneficial in the presence of multiple requirements. Indeed, let \(\) be the posterior of a Bayesian model for the daily (log-)return of a set of assets (see Appendix E for more details). Using (PIV), we consider how the market would look like if the average (log-)return of each asset were to have been (exactly) \(20\%\) higher. Inspecting the dual variables (Fig. 4), we notice that this increased market return is essentially driven by two stocks: NVDA and LLY (\(<0\)). In fact, the reference model \(\) would be consistent with an even higher increase for JNJ and GOOG (\(>0\)). We confirm these observations by constraining only NVDA and LLY, which yields essentially the same (log-)return distribution for all assets.

## 5 Conclusion

We tackled the problem of sampling from a target distribution while satisfying a set of statistical constraints. Based on a GDA method in Wasserstein space, we put forward a fully stochastic, discrete-time primal-dual LMC algorithm (PD-LMC) that precludes any explicit integration in its updates. We analyze the behavior of PD-LMC for (strongly) convex and log-Sobolev potentials, proving that the distribution of its samples converges to the optimal constrained distribution. We illustrated the use of PD-LMC for different constrained sampling applications. Future work include strengthening the convergence results to almost sure guarantees and improving the rates obtained using proximal and extra gradient methods, particularly in the LSI setting.