# Expanding and Integrating

[MISSING_PAGE_EMPTY:1]

during training. This variability indicates that the model's factual confidence fluctuates, making it challenging to pinpoint a checkpoint at which the model has confidently learned facts.

As LLMs are deployed in high-risk industries, ensuring their reliability is crucial for user safety. However, this is not always achieved, leading to serious consequences, such as an Air Canada lawsuit over an LLM-generated incorrect policy (Garcia, 2024). Addressing such issues requires a deeper understanding of how hallucinations arise during training, enabling more reliable and efficient mitigation strategies beyond post-processing methods.

To explore these hallucination trends, we analyze models from 70 million to 12 billion parameters within Pythia suite (Biderman et al., 2023), assessing them across various training checkpoints and tasks. Our goal is to validate the oscillatory behavior observed in prior studies (Li et al., 2024) through evaluation metrics from HuggingFace and EleutherAI (Hong et al., 2024; Gao et al., 2024), and to explore the correlation between model size, training progression, and hallucination patterns.

In response to the identified variance, we introduce a novel training protocol called **Sensitive Neuron Dropout** (SeND). SeND is designed to emphasize confident learning of facts, and in turn reduce the likelihood of confabulations, rather than solely minimizing the stochastic gradient descent (SGD) loss (e.g., cross-entropy). By selectively dropping Sensitive Neurons--those that exhibit significant fluctuations in contextual embeddings throughout training--SeND acts as a regularization technique that reduces hallucination variance and enhances the model's factual certainty. This provides a more reliable criterion for determining training termination, ensuring models not only achieve loss convergence but also display stable factual confidence. To maintain efficiency as model size and inference count increase, we propose the **Efficient EigenScore** (EES), an approximation metric for hallucination detection. EES replaces EigenScore (Chen et al., 2024), the primary metric used in our experiments, offering a scalable solution with high correlation to the original EigenScore.

Our contributions to the field can be summarized as follows, emphasizing that SeND enhances the training process but **does not replace** post-hoc solutions, which may still be required after training:1

1. Empirical verification of the **oscillatory nature of hallucinations in LLMs training** across various model scales and detection metrics.
2. **Sensitive Neuron Dropout (SeND)**, a training-time method designed to reduce hallucination variance and increase model factual confidence during training.
3. **Efficient EigenScore (EES)**, an efficient hallucination detection metric used to keep SeND efficient, achieving up to 2x speedup with minimal effects on accuracy.

### Related Work

The majority of research on hallucinations in language models has focused on detecting and mitigating this phenomenon rather than explaining its underlying causes. Recent techniques can be categorized into two main approaches: those that rely on output text or model probabilities at inference time (Manakul et al., 2023; Joshi et al., 2017; Li et al., 2023) and those that utilize internal representations or hidden layers of the model (Su et al., 2024; Chen et al., 2024; Kossen et al., 2024). While the former has demonstrated effectiveness, the latter offers deeper insights but often comes with computational trade-offs. Additionally, methods like Reinforcement Learning with Human Feedback (RLHF) have gained traction for enhancing model reliability (Yu et al., 2024). However, many of these post-hoc solutions enhance factual accuracy by layering algorithms atop pre-trained models, which can be inefficient. Our work addresses this gap by focusing on the internal dynamics of the model that contribute to hallucinations.

We use several metrics for evaluation, including Halueval (Li et al., 2023), FactScore (Min et al., 2023), SelfCheckGPT (Manakul et al., 2023), and XSum (Narayan et al., 2018), to validate our findings across different tasks. Given that the internal dynamics of the model have proven to be reliable candidates for assessing certainty and hallucination likelihood, we leverage methodologies such as EigenScore (Chen et al., 2024) and Semantic Entropy (Kossen et al., 2024), which detect hallucination risk by analyzing the variability in multiple high-temperature outputs. In our experiments, we use the EigenScore metric alongside the HELM dataset created by Su et al. (2024) to detect hallucinations during training and in the development of SeND.

Regularization techniques have been introduced to fix the issue of variability, notably random neuron dropout, used to reduce the variance and ensure that no neuron is overpowering others (Srivastava et al., 2014; Baldi & Sadowski, 2013). Work such as that done by Santra et al. (2020); Ba & Frey (2013) aims to modify random neuron dropout to change the way neurons are dropped to a more deterministic, precise manner. This has allowed the authors to drop unimportant connections in a deep neural network to ensure that class discriminative information is propagated through the model correctly (Santra et al., 2020). Inspired by this, our aim is to target hallucinatory neurons in our models to ensure that factual information is propagated through.

A significant drawback of state-of-the-art detection techniques, particularly those relying on internal model dynamics, is their efficiency. Existing methods often necessitate multiple inferences and embedding generations, making the spectral analysis of embedding matrices computationally intensive and increasingly impractical as models and datasets grow (Chen et al., 2024; Su et al., 2024). To address these challenges, we propose the use of spectral theory for efficient approximation. This approach enables scalable hallucination detection while maintaining performance. By utilizing tools such as the Density of States (DOS) and the kernel polynomial method (KPM) for approximating EigenScore (Huang et al., 2023; Lin et al., 2014), we aim to enhance the efficiency of our analysis in the context of confabulations, which we will demonstrate empirically with EES and SeND.

## 2 Internal Training Dynamics

The training epochs of a transformer model can be vital in understanding the dynamics of how the model learns, particularly when trained on an unsupervised loss with stochastic gradient. Our analysis of training dynamics through multiple epochs in Appendix A shows that reducing stochastic gradient loss does not necessarily correspond to reducing hallucinations, verifying the results Li et al. (2024) showed for the oscillatory behaviour of LLMs in hallucination during training. Specifically, we found that increasing model size provides diminishing returns with respect to summarization in Figure 3(b) and has nearly no effect on self-consistency shown in Figure 3(a). Most importantly, Figure 3(a) highlights the oscillatory hallucination behaviour throughout training.This highlights the need for further investigation into the relationship between optimization and factual accuracy in LLMs.

Following our investigation of the oscillatory behaviour in training, we look into the internal states of the Pythia 1B model to see what information we are able to extract. In doing so, we define a series of terms and formulas in order to understand the internal processes during the training of LLMs. This information is later used in sections 2.3 and 3 to assist us in deriving methods for improving the variance in the hallucinatory behaviour of models during training.

### Sensitive Neurons

To start our analysis of the internal states, we convert the activation matrix of the model into a sentence embedding vector 2.1 which turns an \(^{n,m}\) activation matrix into a sentence embedding vector \(a_{k}\) for input \(k\) with dimension \(^{n}\). Given its demonstrated success in hallucination detection by Su et al. (2024), we employ this sentence embedding extraction approach.

**Definition 2.1** (Sentence Embedding Vector).: The Sentence Embedding Vector is a way to convert the large \(^{n,m}\) activation matrix into a smaller, easier to manage vector with dimension \(^{n}\).

\[e_{k}=((_{i=1}^{m}H_{N-1}^{i})+H_{N-1}^{m})\] (1)

Where \(e_{k}\) is the activation of one input \(k\), \(m\) is the number of tokens in the sequence, and \(N-1\) is the subtraction to get the penultimate layer index. The penultimate layer of the LLM, being the layer closest to the output probabilities, is our primary focus for hallucination analysis due to its rich information about output certainty.

Next, we define the Net Change Formula 2.2 as a way to extract information from the model indicative of oscillatory behaviour between checkpoints from the sentence embedding vector.

**Definition 2.2** (Net Change Formula).: Let \(e_{i}^{t}\) denote the embedding of data point \(x\) at neuron \(i\) of the contextual embedding after checkpoint/epoch \(t\). Then we define the net change formula as

\[ e_{i}^{t}=|e_{i}^{t}-e_{i}^{t-1}|\] (2)

With these definitions, we can now describe the crux of our investigation: **Sensitive Neurons**. These Sensitive Neurons give us key parts of the model that we will prove contribute to the hallucination of LLM models. They can be used to adapt training procedures for lowering hallucination variation during training and better overall confidence at inference time. In essence, Sensitive Neurons are embedding indices in the sentence embedding from definition 2.1 that experience drastic changes between checkpoints/epochs of the training, something we believe is related to the oscillatory behaviour in hallucination performance. When finding the most Sensitive Neurons, we typically want to select the top \(K\%\) neurons for a specific data point's representation. In our investigation we set \(K=20\).

**Definition 2.3** (Sensitive Neurons).: Indices of the contextual embedding for data point \(x\) which exhibit the highest net change across the last \(C\) checkpoints of training, indicating overall high variability during this period. This is calculated by

\[V_{i}=Var(e_{i})_{t=T-C+1}^{T} e_{i}^{t}\] (3)

where \(V_{i}\) is the total variability during the last \(C\) checkpoints and the most Sensitive Neurons are

\[=_{1 i N}\{V_{i} V_{i}(V,1 00-k)\}\] (4)

where N is the embedding vector size and \(k\) is the desired percentile threshold.

The above definition of Sensitive Neurons is then applied to LLM hallucinations through analyses of the EigenScores. In their paper, Chen et al. (2024) define a new metric for detecting confabulations, a subclass of hallucinations. They do this by calculating an EigenScore 2.4 based on determinant calculations from multiple outputs of an LLM with a high-temperature setting (_temperature_ set to 0.5) to encourage the LLM to produce a variety of different outputs. They propose that if an LLM is set to hallucinate on that output, the generated texts will show higher semantic variability and produce a higher EigenScore. This method achieves SOTA performance and is unsupervised as it only relies on the representations learned by the model. In the forthcoming sections, we will analyze the correlation between the EigenScore of data points during training checkpoints and the most Sensitive Neurons associated with them.

**Definition 2.4** (EigenScore).: The **EigenScore** of data point \(x\) indicates the degree of hallucination on input \(x\) by the average logarithm of the eigenvalues on the covariance matrix of the multiple output generations (typically 10 in our experiments).

\[ES=(Y x,)=_{i=1}^{K}(_{i})\] (5)

where \(=\{_{1},,_{K}\}\) denotes the eigenvalues of the regularized covariance matrix \(+\). we advise referring to Chen et al. (2024) for a more detailed analysis of this formula.

### Sensitive Neuron Impact on EigenScores

To assess the correlation between Sensitive Neurons and other neurons in the embedding matrix of 10 generated outputs at a specific checkpoint, we conduct experiments aimed to determine if the presence of Sensitive Neurons indicates higher uncertainty and a greater likelihood of hallucinations.

We evaluate the Sensitive Neuron effect on the HELM dataset (Su et al., 2024), which includes outputs and internal states from six open-source LLMs based on inference over 50,000 Wikipedia articles, with human annotators labeling passages as factual or hallucinatory. This dataset was selected as Wikipedia is one of the main fact sources people refer to, and to reduce the spread of misinformation, LLMs should be robust to this type of information. To assess the impact of Sensitive Neurons on hallucination, we adapt the EigenScore method by applying it to sentence embeddings from thepenultimate layer of EleutherAI's Pythia 1B model, focusing on checkpoints between 133,000 and 143,000 training steps, where embeddings are more stable. We perform sensitive neuron dropout, removing the top 10% of Sensitive Neurons at each checkpoint, and compare the results to a baseline where 10% of neurons are randomly dropped. Additionally, we analyze the impact on hallucination-prone inputs versus non-hallucination-prone inputs to determine if Sensitive Neurons play a critical role during hallucination, without negatively affecting correct outputs.

#### 2.2.1 What is the Effect of Sensitive Neurons on Hallucination Metrics?

Since a reduction in the EigenScore metric can be used as a proxy to show the reduction in likelihood of hallucination, we keep using this metric in our investigations. We are able to show through our comparison of the baseline random neuron dropout and Sensitive Neuron dropout that Sensitive Neurons significantly reduce the EigenScore metric and in turn, reduce the possibility of a confabulation (Figure 0(a)). Not only do we observe this in hallucinatory outputs, we also observe a smaller reduction in EigenScore when applying this technique to correctly answered queries (Figure 0(b)). This result indicates that our methodology has a significant effect on the uncertainty shown by an LLM. We observe that looking at the internal states of the model is an effective way to eliminate confabulating text generation in various model sizes.

### Efficient EigenScore Approximation

To address the computational complexity of EigenScore calculations, particularly as LLM hidden layer sizes increase, we develop an approximation method. This approximation, detailed in Algorithm 2, leverages the properties of Spectral Density or Density of States (DOS) to estimate EigenScore without explicitly constructing the covariance matrix. While this approximation provides a general overview of EigenScore trends, it is important to note that the output scales differ: EigenScore ranges from \([0,)\), whereas the approximation, referred to as **Efficient EigenScore (EES)**, outputs values between \([-1,1]\). Since the spectrum of the matrix is altered to make EES computable and operates on its own scale, EES can be seen as a standalone metric for hallucination detection.

The computation of the Efficient EigenScore (EES) is based on two fundamental concepts: Chebyshev Polynomials and Density of States (DOS). A detailed introduction to these concepts is provided in Appendix sections C.1 and C.2. Below, we outline a brief sketch of the derivation of EES. Since Chen et al. (2024) use the covariance matrix of the embedding matrix of 10 generated sequences by the model in their methods, we represent it with \(H\) and use it in our derivation.

**Lemma 1**.: _Let \(f=\). Then, for a covariance matrix \(H\) with eigenvalues \(_{i}\), we have_

\[((H))=_{i=1}^{N}(_{i}),\] (6)

Figure 1: **Comparison of sensitive neuron dropout** on inference of Eleuther AI’s Pythia various model sizes with random neuron dropout. (a) Average sensitive neuron dropout with standard deviation plotted as scale of the model increases. (b) Average sensitive neuron dropout for hallucinatory inputs and non-hallucinatory inputs. Input size for each test is 80 I.I.D. texts. Sensitive neuron dropping presents a clear, significant reduction in EigenScore compared to that of random neuron dropping across model sizes. Hallucinatory generations experience a larger drop in EigenScore, meaning that our protocol scales with likelihood of hallucination.

_where \(_{i}\) are the eigenvalues of \(H\)._

**Proposition 1**.: _Using the property of the density of states (DOS), we have:_

\[()\,()\,d=(_{i=1}^{N}_{i} ),\] (7)

_which follows from Lemma 1 since \(_{i=1}^{N}(_{i})=(_{i=1}^{N}_{i})\)._

Note that from Proposition 1, the integral is equal to \(N.EigenScore(H)\) or in our application, given \(C\) the integral equals \(K.EigenScore(C)\), \(K\) being the number of model generations.

Our objective is to simplify the integral and approximate its value, avoiding the direct computation of the covariance matrix. This approach is intended to mitigate the computational complexity and associated costs of explicitly handling the covariance matrix. Further utilizing Chebyshev Polynomials, DOS, and KPM (as introduced in Appendix C.2), we can simplify the integral mentioned in Equation 7 to \(_{m=0}^{M}d_{m}c_{m}\), where \(d_{m}\) term in DOS is approximated using Stochastic Trace Estimation and \(c_{m}\) m'th Chebyshev Polynomial coefficient. Appendices C.3 and C.4 provide the derivation of this equation. Note that the simplified integral is ultimately used to approximate the EigenScore of the matrix which is ultimately equivalent to \(_{m=0}^{M}d_{m}c_{m}\). Performance of EES approximation is closely correlated with that of the original EigenScore and can be seen to closely track the progress of EigenScore through training of Pythia 1B on the HELM dataset in Figure 7.

### How does Efficient EigenScore approximation scale compared to regular EigenScore?

The efficiency of EES compared to regular EigenScore is evaluated for scaling matrix sizes, which is critical for applying our training protocol on LLMs (Section 3). We conduct a grid search over matrix size (Figure 2) and moments used in EES calculation (Figure 6). As shown in Figure 2, EES demonstrates a significant computational advantage, reducing computation time by nearly half for matrix sizes of \(^{1 8}\), with EES taking around 4 seconds versus 7 seconds for EigenScore. Thus, EES offers substantial computational efficiency as model and matrix sizes increase.

## 3 Sensitive Neuron Dropout (SeND)

Building on the findings from Section 2.2, and aiming to reduce variance in the factual uncertainty of LLMs during training, this section introduces SeND, an efficient and transferable framework for training LLMs. SeND integrates the EES method discussed in Section 2.3 to enhance computational

Figure 2: **Efficient EigenScore approximation scaling investigation**. The figure shows the difference in computation time between regular EigenScore calculation and EES with a moments value of 20. The x-axis represents the product of the matrix’s rows and columns, and the y-axis shows the computation time. As matrix size increases, EES consistently reduces computation time, making it a practical choice for large LLMs.

efficiency while addressing variance in sensitive neuron behavior. By identifying sensitive neurons, which contribute to the oscillatory behavior of hallucinations during training, SeND deterministically drops these neurons based on a small subset of the training data. This approach ensures an increase in the model's factual certainty by the end of training as explained in Algorithm 1.

```
0:\(\) denotes the acceptable range for loss convergence and \(\) denotes acceptable range for confabulation (EES) convergence
1: Initialize dataset with \(\)% training \(Y_{t}\) and \((100-)\)% tracking \(Y_{s}\)
2:while Loss \(>\) and EES \(>\)do\(\) Refer to Algorithm 2 for EES
3:for\(t\) in T do\(\) T denotes the number of epochs per sensitive neuron calculation
4: Train LLM for one epoch over \(Y_{t}\)
5: Record penultimate layer representations \(R_{t}\) of LLM over \(Y_{s}\)
6:endfor
7:for\(t T-1\)do
8: Calculate variability \(V_{t}\) between \(R_{t}\) to \(R_{t+1}\)\(\) Refer to Equation 3
9:endfor
10: Take average Variability \(V_{avg}=}_{i=0}^{N_{s}}V_{i}\)
11:\(s=K\) most sensitive neurons \( V_{avg}\)\(\) Refer to Equation 4
12: Drop neurons \(s\) for next T epochs
13:endwhile ```

**Algorithm 1** Sensitive Neuron Dropout

### SeND Experiment Setup

To evaluate SeND, we use Eleuther AI's Pythia 1B model, continuing its training on specific datasets rather than restarting pretraining to maintain efficiency. We continue the training of the fully trained model on two datasets: HELM, consisting of Wikipedia text (Su et al., 2024), and MedHALT, a medical dataset emulating real-world entrance exam questions (Pal et al., 2023). Due to the importance of factual accuracy in the medical domain, MedHALT was chosen to assess SeND's impact on hallucination mitigation in an additional field where hallucinations are highly impactful. Both datasets were tested in two sizes: 200 and 2,000 points (referred to as 2k). SeND implements the EigenScore reduction technique from Section 2.2 and detects Sensitive Neurons using a 3-epoch window on a specialized hallucination tracking dataset. Sensitive Neurons in the penultimate layer are identified based on their variability across epochs and are deterministically dropped for the subsequent 3 training epochs. This dropout process is repeated at each 3-epoch interval until the training loss converges, effectively mitigating hallucination tendencies and refining the model.

### Performance of SeND on Pythia 1B

The results of running Pythia 1B on HELM and MedHALT 200 are illustrated in Figure 3. To validate that the EES method accurately approximates the EigenScore metric; we compare the model's progress during training (up to loss convergence) and assess whether the resulting graphs are similar. These results are detailed in Appendix C.7. Upon confirming that EES provides a reliable approximation of the EigenScore, we proceed to compare the performance of Pythia 1B trained using standard training without dropout to that of Pythia 1B trained with SeND on HELM and MedHALT 2k (Figure 3). A baseline of no dropout was used for comparison as experiments showed that implementing random dropout resulted in worse performance. In the case of training on HELM with the regular protocol, we observe results consistent with previous findings: while the model successfully reduces loss, it fails to optimize for hallucination, as evidenced by the increasing EES metric (green line in Figure 3a). Conversely, training with SeND reveals a consistent trajectory toward reducing both EES and loss, as depicted by the blue line.

To assess the effectiveness of SeND in comparison to other state-of-the-art factuality metrics, we employ the FactScore metric from Min et al. (2023), which quantifies the factual accuracy of content generated by large language models (LLMs). The fact-checking is conducted using the HELM dataset where a higher FactScore indicates improved factual precision. When evaluated on 100 data points from the HELM dataset, the 1B SeND model achieves a FactScore of 0.07, whereas the 1B Normal Training model attains 0.05, demonstrating a 40% improvement in factual accuracy, even during test time. This highlights the efficacy of SeND in enhancing the factual certainty of the model. Note that SeND is not a replacement for post-hoc methods such as RAG (Gao et al., 2024), but rather to complement them.

The robustness of this training protocol is essential as we aim for it to be applied across many different fields. In light of this, we present the results of finetuning on MedHALT 2k with and without SeND in Figure 2(b). We observe a similar trend in 2(b) as shown in Figure 2(a), where standard finetuning increases the EES score throughout training, showing that the model is not taking into account hallucinations and factuality during its training. In Figure 2(b), there is an improvement in the trajectory of EES as training continues, showing that our model is in fact able to incorporate factuality as a metric to account for during training of the model. The small difference observed between the training protocol behaviours could be due to MedHALT 2k data never being seen before the finetuning phase whereas HELM data has been seen. In this case, it may be beneficial to delay the onset of SeND, as high variability between checkpoints on new training instances is expected.

## 4 Conclusion & Future Work

In this paper, we presented a protocol to refine the current training methods of LLMs based on experiments showing oscillatory behaviour with respect to hallucinations throughout training (Figure 4). To do this we used the internal states of LLMs, specifically the penultimate layer activations during inference on a specialized dataset. We present an initial method of reducing hallucinations based on the principles of EigenScore metrics introduced by Chen et al. (2024). We showed empirically that our Sensitive Neuron detection method significantly reduces the EigenScore on inference of LLMs throughout various stages of training (Figure 1). Following the success of the Sensitive Neuron method, we moved on to the application of a hallucination reduction method on training. We show through finetuning that we are able to fix the oscillatory behaviour initially seen throughout training and reduce the EES of finetuned models as shown in Figure 3 by modifying the internal mechanics of training with **SEnsitive Neuron Dropout**. At test time, in conjunction with Retrival Augmented Generation, we achieve a 40% increase in FactScore performance, verifying that SeND provides a substantial improvement to current training protocols.

In the future, we would like to scale our method to larger datasets and larger models as we faced compute power restrictions with larger LLMs. Proving the performance of SeND on larger open source models such as Meta's Llama 3.1 (Dubey et al., 2024) will give organizations creating state of the art LLMs the evidence they need to implement SeND into their training protocol and launch safer models. We also expect SeND to perform even better on larger LLMs since we are introducing a regularization technique to reduce variance during training. Applying this to larger LLMs with an innate higher variance could see SeND having a larger impact on the model.

Figure 3: **Regular finetuning vs. SeND on two datasets**. (a) presents the results of training Pythia 1B on HELM with regular finetuning and SeND. (b) uses the same training setup as (a), but the LLM is trained on MedHALT 2k. In both plots, performance is reported as the average **EES** over 5 runs on the validation set. Models are trained until loss convergence. Training with SeND shows a more controlled reduction in **EES** compared to regular finetuning, suggesting that SeND optimizes for hallucinations as well as loss, with less overall confidence variability during training.