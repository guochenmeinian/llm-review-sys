# No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations

Walter Simoncini\({}^{1,}\)1

Spyros Gidaris\({}^{2}\)

Andrei Bursuc\({}^{2}\)

Yuki M. Asano\({}^{1}\)

\({}^{1}\)QUVA Lab, University of Amsterdam; \({}^{2}\)valeo.ai, Paris, France

Work partially done as an intern at valeo.ai. Datasets were solely downloaded and evaluated by the University of Amsterdam. walter@ashita.nl

###### Abstract

This paper introduces fungi, **F**eatures from **UN**supervised **G**rad**I**ents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model's output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, fungi features provide consistent performance improvements over the embeddings. We also show that using fungi features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation - without any training. Code is available at https://github.com/WalterSimoncini/fungivision.

## 1 Introduction

The k-nearest neighbor algorithm (kNN) (Fix, 1985) is a fundamental non-parametric machine learning tool, and can be scaled to datasets with billion of examples thanks to advances in quantization (Jegou et al., 2010; Guo et al., 2020) and efficient GPU implementations (Johnson et al., 2019). This simple and versatile algorithm has shown potential in multiple applications well before deep neural networks became relevant (Efros and Leung, 1999; Hays and Efros, 2008; Torralba et al., 2008). Its recent applications include fast and robust image classification with Vision Transformers (Caron et al., 2021; Chen and He, 2021), unlabeled data selection (Yalniz et al., 2019), relevant text-retrieval (Lewis et al., 2020), and visual in-context learning (Balazevic et al., 2024), where a context of data samples with their annotations (e.g., a semantic segmentation map) are used to make dense predictions.

Devising powerful and expressive features for recognition and image understanding has a long history in computer vision. Feature engineering strategies range from simple local features (Lowe, 2004; Dalal and Triggs, 2005; Van De Sande et al., 2009) extracting gradient, boundary or color information, to various mid-level (Boureau et al., 2010) or global pooling (Oliva and Torralba, 2001; Sivic and Zisserman, 2003; Jegou et al., 2010) techniques. It is also possible to couple off-the-shelf pretrained backbones as feature extractors with such pooling strategies (Gong et al., 2014; Kulkarni et al., 2015; Gidaris et al., 2020) to improve performances. While these approaches demonstrate the utility of using a neural network's learned embedding space, they still require specific expertise and tuning for each backbone and task with only limited guidance from the data itself.

We depart from this line of work and aim to attain strong representations without training and feature engineering, yet still exploiting information cues from data. In particular, we suggest to enhance the neural network's embeddings by incorporating fungi (Features from Unsupervised GradIents). fungi are obtained from self-supervised loss functions, as these do not require any human annotations and allow for a simple enhancement to embedding-only kNN. The losses are computed on top of pretrained backbones (with randomly initialized linear layers if needed), which permits our method to be "plug-and-play" and benefit from the diverse set of pretraining objectives put forth by the community.

We explore gradients from various learning objectives, such as contrastive learning (Chen et al., 2020) and self-distillation (Caron et al., 2021) thereby integrating complementary information that mitigates the weaknesses of individual losses. The gradients are obtained from late hidden layers, making them computationally cheap. Finally, these are projected to smaller dimensions, and concatenated with the neural network embeddings, to yield new inputs to the classic kNN algorithm.

Using kNN with fungi can be regarded as a non-parametric transfer learning approach: the gradient information encodes first-order learning signals that are specific to the downstream data, yet no parameters need to be updated. Despite this simplicity, we achieve consistent performance improvements across multiple models and benchmarks. Overall, our main contributions are summarized as follows:

* We introduce fungi, a novel method that combines neural network features and gradients to enhance representations.
* We demonstrate that the gradients from self-supervised losses have predictive abilities and offer complementary information to model embeddings.
* We validate the generality and utility of our method by achieving consistent gains across 11 image, 5 text, and 2 audio classification benchmarks, plus 2 in-context image segmentation and 2 image retrieval tasks, utilizing a total of 20 backbones.

## 2 Related Work

Fast AdaptationThere is a broad range of approaches to quickly adapt models to newly specified tasks and data. Inspired by early _learning-to-learn_ work (Hochreiter et al., 2001), _meta-learning_ methods (Finn et al., 2017; Nichol et al., 2018) learn to initialize the parameters of a learner such that it becomes faster to fine-tune with a small number of gradient steps and data. Alternative approaches leverage external memory modules to store relevant training samples to learn to match query examples (Santoro et al., 2016; Vinyals et al., 2016), learn to produce and compare class-based prototypes (Snell et al., 2017) or learn to generate the weights of a classifier (Gidaris and Komodakis, 2018) or even of an entire neural network (Bertinetto et al., 2016) from only a few labeled examples. The advent of Vision Transformers (Dosovitskiy et al., 2021) enable new parameter- and data-efficient strategies to adapt pretrained models through visual prompts (Jia et al., 2022) and in-context learning (Zhang et al., 2023). In contrast to this line of works, our method does not require specialized training and can be applied to any frozen pretrained backbone. fungi can also be related to test-time training (Sun et al., 2020; Hardt and Sun, 2024), where the parameters of a predictive model are

Figure 1: **Gradient-augmented features**: given a pretrained backbone \(f_{^{*}}\) and its embeddings, we apply a family of SSL losses, extract their gradients, and project and concatenate them. These new features are used to build a \(k\)-nearest neighbor index, which can be used for classification or retrieval.

updated over test samples with a self-supervised objective to reduce the gap between the training and test distributions. While we also use self-supervised objectives and gradients, our approach does not update model parameters and is not limited to predictive models, as it can be applied to any task that can be solved with retrieval.

Self-Supervised Learning ObjectivesIn recent years, self-supervised learning (SSL) has made tremendous progress in computer vision. SSL aims to learn good representations from unlabeled data by leveraging supervision from different signals in the data itself via pretext objectives, thus foregoing human supervision. Models pretrained with self-supervision are subsequently finetuned to downstream tasks of interest with few labeled samples. The crux of SSL is in the pretext learning objective. A wide and diverse collection of pretext objectives have been proposed in the community relying on contrastive learning (Chen et al., 2020; He et al., 2020; Chen et al., 2020), clustering (Caron et al., 2018; Asano et al., 2020; Caron et al., 2020), self-distillation (Caron et al., 2021; Grill et al., 2020; Chen and He, 2021; Gidaris et al., 2021), feature (Zhou et al., 2022; Assran et al., 2023) or input reconstruction (He et al., 2022).

We hypothesize that the gradients induced by these objectives encapsulate different information from the input data, and that this information can be combined to produce more information-rich representations. Here, we do not use self-supervision in the usual way, _i.e._, to pretrain an encoder, but rather focus on pretext objectives and data augmentation strategies to compute representations from a frozen pretrained model.

Feature EngineeringA long-standing research area for pattern recognition and image understanding before the advent of deep neural networks that brought the paradigm of end-to-end representation learning. In contrast, classic feature extraction methods are devised without labeled data and often from only a few data samples. They range from local features, such as SIFT (Lowe, 2004), HOG (Dalal and Triggs, 2005), to global pooling, such as GIST (Oliva and Torralba, 2001), Bag-of-Visual-Words (Sivic and Zisserman, 2003), Fisher vectors (Perronnin et al., 2010), VLAD (Jegou et al., 2010), selective match kernels (Tolias et al., 2013), etc. These pooling strategies can be easily plugged to intermediate or output neural network activations (Gong et al., 2014; Kulkarni et al., 2015; Gidaris et al., 2020), harnessing data-driven learned representations. Other modern examples of feature engineering include Head2Toe (Evci et al., 2022), which augments the model embeddings using intermediate activations, kNN-prompting (Xu et al., 2023), which uses the next token probabilities of a language model to perform few shot nearest neighbor classification and LOST (Simconi et al., 2021) which uses patch features from self-supervised vision transformers for object localization. Closer to our line of work, Wei et al. (2022) shows that kernel regression using the empirical neural tangent kernel (eNTK), which corresponds to the model Jacobian, can achieve a performance similar to fine-tuning in vision, and Mu et al. (2020) shows that features obtained from the Jacobian of the top layers of a convolutional neural network can be used to enhance a linear classifier. In contrast, our method does not require any annotations and is computationally cheaper, as we only compute the gradient for a single layer rather than the full Jacobian.

## 3 Gradients as Features

Gradients encode information on how the weights of a neural network should change to solve a given task. Thus, they contain information about the current state of the network and its relation to the data point(s) used to compute it. Therefore, we hypothesize that features from self-supervised gradients should perform better than the embeddings, as they are adapted to the dataset at hand. Empirically, the second row in Figure 2 shows this to be accurate, e.g., gradients from a SimCLR loss improve the accuracy of k-nearest neighbor classification by 10.1% on Flowers102.

If we plot the per-class accuracy distribution as in Figure 3, we notice that gradient features encode different information depending on the loss and that they can perform significantly worse on some classes, possibly because they are estimated using a single data point, and are thus dependent on the local loss curvature. These findings suggest that the information in embeddings and gradient features could be complementary. We show that this holds in the second row of Figure 2, as feature pairs perform better. Moreover, the first row of the figure suggests that more diverse feature pairs, as measured via their centered kernel alignment (CKA) (Kornblith et al., 2019) score, lead to better overall performance.

## 4 Method

Our method, fungi, enhances k-nearest neighbor search by incorporating features from unsupervised gradients. We extract gradients from self-supervised loss functions, project them to smaller dimensions, and concatenate them with neural network embeddings. The extraction of self-supervised gradients is illustrated in Figure 4, while Figure 1 shows how fungi features are constructed.

DefinitionsThroughout this section, we define \(L_{2}\) normalization as \(z^{}=z/||z||_{2}\), a vision backbone as \(f\), a linear projection head as \(h\) and vectorization as \(()\).

### fungi: Features from Unsupervised Gradients

Gradients Extraction.Given an arbitrary vision backbone \(f\), in our case a vision transformer (ViT) (Dosovitskiy et al., 2021), we attach a randomly initialized linear projection head \(h\) and obtain a latent representation \(z=h(f^{}(x))\) of the input images, which we use to compute the loss for one of our self-supervised objectives. We then run backpropagation and extract the gradients with respect to the weights and biases of an arbitrary hidden linear layer within \(f\). Unless specified otherwise, we use the attention output projection of the last transformer block as our gradient's source.

From Gradients to Retrieval-Friendly Features.Gradients are high dimensional and thus impractical for nearest-neighbor retrieval due to speed and storage considerations and the curse of dimensionality. To tackle these issues, we downsample the gradients to the dimensionality of original model embeddings \(d\) using the binary random projections method introduced by Achlioptas (2003). For this, we first vectorize the gradients by flattening them to a \(m\)-dimensional vector and then multiply them by a matrix \(R\{-1,1\}^{d,m}\) whose entries are the realizations of a Bernoulli random

Figure 4: **Gradients extraction using a SimCLR loss. Given a pretrained backbone \(f\) and a randomly initialized projection head \(h\), we first patchify an image, obtain the latent representations of patches (1), calculate the SimCLR loss by maximizing the pairwise cosine similarity of patches, and minimizing their similarity to a fixed negatives batch and backpropagate (2), extract the per-sample gradients (3) and finally project the gradients to the same dimensionality as the embeddings (4).**variable with \(p=0.5\). The gradient \(g_{}\) with respect to a loss \(_{}\) is then defined as

\[g_{}(x)=R\;(_{}(x)).\] (1)

Combining with Embeddings.To produce our fungi features \(\), we concatenate one or more gradients to the model embeddings. As gradient magnitudes can vary widely across losses, and we want gradients to be equally considered as the embeddings, we \(L_{2}\)-normalize each gradient, as well the embeddings and compute

\[(x)=[g^{}_{_{1}}(x),g^{}_{_{2}}(x),...,f^{}(x)],\] (2)

where cat denotes concatenation. Finally, we reduce the dimensionality of these combined features via PCA to a \(d\)-dimensional vector. This allows the combination of multiple losses at iso-storage cost. Our final fungi features for a sample \(x\) are thus obtained as:

\[_{}(x)=_{d}((x)).\] (3)

### Self-Supervised Objectives

We consider losses representing three families of self-supervised objectives: DINO (Caron et al., 2021), SimCLR (Chen et al., 2020) and a KL-divergence based loss inspired by the _out-of-distribution_ detection literature (Huang et al., 2021). In this section we briefly describe the objectives and our adjustments to them, and in Appendix B.6, we also briefly discuss clustering and masked image modeling-based losses.

**DINO.** DINO is a distillation and implicit clustering-based learning method. We use the standard DINO loss, which, given an image, enforces global and local crop correspondence between teacher and student models using a cross-entropy loss. In our case, both models share the same parameters, but have independent heads \(h_{s}\) and \(h_{t}\) for student and teacher respectively, thus we have \(z_{i}=h_{i}(f^{}(x)),i\{s,t\}\). The DINO objective can be expressed as:

\[_{}=(z_{s},z_{t}).\] (4)

**SimCLR.** SimCLR is a noise-contrastive method. Given a batch of images, SimCLR generates two views for each image and aims to minimize the distance between views belonging to the same image and maximize their distance to all other views. Instead, we generate a set of 49 overlapping patches for each image, which we call the positive set. This set is then contrasted against a fixed comparison batch of \(49 256\) negative examples. Our objective is the expectation of the pair-wise InfoNCE (Oord et al., 2018) loss for each pair of positive views. If we define the positive set of latent view representations as \(Z\), where \(z_{i} Z=h^{}(f(x_{i}))\) for a view \(x_{i}\), the comparison batch size as \(N\) and the temperature as \(\), the \(_{}\) objective is then defined as:

\[_{}=_{(z_{i},z_{j}) Z,z_{i} z_{j}}[ _{z_{i},z_{j}}]_{z_{i},z_{j}}=-(z_{i}, z_{j})/)}{_{k=1}^{49(N+1)}_{[k i]}((z_{i}, z_{k})/)}.\] (5)

**KL Divergence.** The KL objective is calculated as the KL divergence between the softmaxed logits of the latents and a uniform distribution \(\):

\[_{}=((z)\|).\] (6)

We hypothesize two reasons as for why this loss produces predictive gradients: first, it receives a non-augmented image, with a higher signal-to-noise ratio compared to other objectives, and second, if we assume that similar images (_e.g._, the ones that belong to the same class) produce similar activations, then maximizing their entropy by forcing the output distribution to match an uniform should produce similar intra-class gradients and help separability. This hypothesis is supported by the fact that while the KL gradients are discriminative, they have chance performance in other tasks, such as in-context scene understanding.

### In-Context Scene Understanding

Balazevic et al. (2024) introduced a method for retrieval-based in-context scene understanding, where, for semantic segmentation, they first build a memory bank containing training image patches and their labels, and at test time, for each image patch, retrieve its nearest neighbors and use them to predict its label using an attention mechanism. Images are first resized to \(512 512\), and then encoded as a set of \(32^{2}=1024\) patch features using a ViT with patch size 16.

We enhance the patch features using SimCLR gradients, obtained by contrasting the input patch tokens against their nearest neighbors from a support index built with ScaNN (Guo et al., 2020). We use the reproduction of this evaluation protocol by Pariza et al. (2024) to run our experiments.

## 5 Experiments

In this section, we evaluate the performance of fungi in k-nearest neighbor image, text and audio classification and retrieval-based in-context scene understanding. Further experiments, including image retrieval, clustering and linear probing, are provided in Appendix B.

### Image Classification

Following Caron et al. (2021), we evaluate our fungi features using the task of kNN classification. To show the generalizability of our method, we evaluate our features across ViT backbones (Dosovitskiy et al., 2021) with varying model sizes and pretraining strategies, including both supervised and self-supervised methods.

We conduct our experiments on 11 diverse downstream datasets, described in Appendix D. Unless otherwise specified, we report the average accuracy across these datasets. We evaluate our features using the kNN implementation of scikit-learn (Pedregosa

    & Pretrain & Cars & CUB & DTD & ESAT & C100 & C10 & Pets & Food & IN1K & FGVC & Flowers & Mean \\  
**Full dataset** & & & & & & & & & & & & & \\ Embeddings & IN1K & 21.3 & 42.0 & 54.3 & 89.0 & 66.3 & 89.4 & 87.3 & 52.3 & 77.2 & 17.9 & 53.8 & 59.2 \\ Fungi & IN1K & **27.2** & **50.1** & **58.6** & **93.4** & **69.7** & **90.7** & **89.5** & **58.9** & **78.8** & **21.4** & **61.6** & **63.6** & \(\)4.4 \\ Embeddings & IN21K & 21.0 & 74.0 & 58.4 & 91.8 & 58.4 & 82.9 & 83.6 & 70.6 & 72.1 & 23.0 & 95.0 & 66.4 \\ Fungi & IN21K & **25.1** & **74.2** & **65.0** & **94.7** & **63.5** & **85.7** & **85.7** & **73.4** & **74.5** & **24.3** & **96.6** & **69.3** & \(\)2.9 \\ 
**5-Shot** & & & & & & & & & & & & & \\ Embeddings & IN1K & 9.4 & 23.7 & 32.5 & 38.6 & 36.9 & 48.8 & 57.5 & 20.1 & 55.7 & 8.3 & 41.2 & 33.9 \\ Fungi & IN1K & **11.4** & **26.6** & **33.9** & **42.2** & **38.6** & **50.2** & **59.4** & **24.1** & **58.6** & **9.2** & **49.8** & **36.7** & \(\)2.8 \\ Embeddings & IN21K & 7.6 & **50.0** & 33.7 & 47.7 & 23.2 & 39.7 & **53.3** & 32.0 & 40.3 & 10.7 & **86.2** & 38.6 \\ Fungi & IN21K & **9.2** & 48.5 & **36.3** & **54.5** & **28.2** & **41.7** & 51.0 & **37.8** & **45.4** & **12.2** & 85.8 & **41.0** & \(\)2.4 \\   

Table 1: **FUNGI features are better on several datasets.** Accuracy of embeddings and fungi features in kNN classification over 11 datasets, for two AugReg (Dosovitskiy et al., 2021; Steiner et al., 2022). ViT-B/16 models from timm (Wightman, 2019) pretrained on IN1K and IN21K.

Figure 5: **Better data-efficiency.** kNN accuracy of embeddings and fungi (using only KL and SimCLR gradients) on ImageNet-100 using a DeIT-B/16 backbone when only \(k\) shots are used.

Figure 6: **fungi works across backbones.** Accuracy in \(k\)-nearest neighbor classification using embeddings and fungi features from various ViT backbones, both for full dataset and few shot setups, averaged over 11 datasets. For the fungi features we chose the best performing combination across datasets. “AR” indicates backbones trained with the AugReg strategy (Steiner et al., 2022).

[MISSING_PAGE_FAIL:7]

2017). We use the trainaug and train splits to build the memory banks for Pascal VOC and ADE20K, respectively, and report the mean intersection over union (mIoU) on the validation set.

We apply fungi to DINO ViT-S/16 and ViT-B/16 models. Our results, presented in Table 3 and Table 7, demonstrate that fungi significantly enhances DINO's performance across all memory bank sizes, with the most substantial improvements observed in smaller memory banks for Pascal VOC. Qualitatively, fungi produces sharper and more complete segmentation masks, as shown in Figure 7. Notably, the DINO ViT-B/16 model, when enhanced with our fungi approach, achieves competitive results against the current state-of-the-art HummingBird model (Balazevic et al., 2024), with a difference of only 3.5% on Pascal VOC and 3.1% on ADE20K, **without** any training. This is a particularly promising result, as HummingBird employs a self-supervised pretraining strategy that is specialized for retrieval-based dense prediction tasks, which are the focus of our evaluation in this study.

In addition, we evaluate the efficacy of fungi in a data-efficient setup, and report the results in Table 4. Our findings indicate that our method outperforms DINO in this scenario, even when compared to end-to-end fine-tuning of DINO on the downstream task for Pascal VOC.

### Other Modalities

**Natural language.** We evaluate fungi in the text domain using five datasets, described in Appendix D, and two transformer language models: BERT base uncased (Devlin et al., 2019) and T5-small (Raffel et al., 2020). We use the \(_{}\) and \(_{}\) losses and obtain the SimCLR views by randomly deleting words with a 10% probability. The results are presented in Table 5 and show that fungi achieves improvements in the text domain. However, SimCLR gradients struggle with some datasets. Different data augmentation strategies, such as back-translation (Sennrich et al., 2016), or language-specific self-supervised losses, _e.g._, masked language modeling (Devlin et al., 2019), may yield more discriminative gradients. We leave this investigation for future work. Furthermore, in Appendix B.5, we investigate the potential of fungi in language in-context learning.

**Audio.** We demonstrate gains for the audio modality in Table 12, where we improve the ESC-50 kNN classification accuracy from \(42.8\%\) to \(47.0\%\) and SpeechCommands from \(27.4\%\) to \(29.9\%\) with an SSAST backbone (Gong et al., 2022). Further details are provided in Appendix B.2.

   \\   &  &  \\  Backbone & Features & Decoder & 1/128 (\(n\) = 83) & 1/64 (\(n\) = 165) & 1/128 (\(n\) = 158) & 1/64 (\(n\) = 316) \\  ViT-B/16\({}^{}\) & - & E2E FT & 36.1 & 44.3 & **11.7** & **14.4** \\  ViT-S/16 & Emb. & NN & 26.3 & 31.8 & 8.8 & 10.0 \\ ViT-S/16 & fungi & NN & **29.1**\(\)2.8 & **34.0**\(\)2.2 & **10.2**\(\)1.4 & **12.3**\(\)2.3 \\  ViT-B/16 & Emb. & NN & 32.2 & 39.0 & 9.3 & 11.3 \\ ViT-B/16 & Fungi & NN & **38.0**\(\)5.8 & **46.8**\(\)7.8 & **11.7**\(\)2.4 & **13.7**\(\)2.4 \\  

Table 4: **Data-efficient semantic segmentation. mIoU scores for data-efficient retrieval-based semantic segmentation on Pascal VOC 2012 and ADE20K, using DINO backbones and their fungi features and embeddings. We also compare fungi to end-to-end fine-tuning and find our method to perform best for VOC. Results from Balazevic et al. (2024) are marked with \(\).**

   &  &  & SST (Fine Grained) &  &  \\   & Full & 5-shot & Full & 5-shot & Full & 5-shot & Full & 10-shot & Full & 5-shot \\ 
**BERT Base** &  Embeddings \\ + K \\  } &  83.6 \\ 86.6 \\  } &  20.0 \\ 20.7 \\  } &  55.4 \\  } &  14.5 \\  } &  40.0 \\ 20.4 \\  } &  20.4 \\  } &  88.8 \\  } &  45.8 \\  } &  23.8 \\  } &  13.6 \\  } \\
**+ K** & 85.6 \(\)2.0 & **27.6**\(\) 10.6 & 67.1 \(\)11.7 & 22.2 \(\)7.7 & 40.7 \(\)0.7 & **23.2**\(\)2.8 & **91.0**\(\)2.2 & 61.4 \(\)15.6 & 24.4 \(\)0.6 & 13.8 \(\)0.2 \\
**+ K** + S** & **86.8**\(\)3.2 & 23.0 \(\)3.0 & **67.9**\(\)12.5 & **23.8**\(\)9.3 & **41.8**\(\)1.8 & 18.4 \(\)2.0 & 89.6 \(\)0.8 & **61.9**\(\)16.1 & **24.8**\(\)1.0 & **14.5**\(\)0.9 \\ 
**T5 Small** &  Embeddings \\ + K \\  } &  **88.6** \\  } &  **25.6** \\  } &  29.7 \\  } &  5.2 \\  } &  30.0 \\  } &  **25.9** \\  } &  71.8 \\  } &  37.4 \\  } &  23.4 \\  } &  8.4 \\  } \\
**+ K** & **88.6**\(\)2.0 & 23.6 \(\)2.0 & 29.1 \(\)0.6 & **6.1**\(\)0.9 & 32.0 \(\)2.0 & 24.2 \(\)1.7 & **74.8**\(\)3.0 & **41.0**\(\)3.6 & **24.4**\(\)1.0 & **9.9**\(\)1.5 \\  

Table 5: fungi features are useful for the text modality. Top-1 accuracy in kNN text classification for the full dataset and few shot setups. “K” and “S” stand for KL and SimCLR, respectively.

### Ablation Studies

Projection head.To compute our self-supervised losses, we first \(L_{2}\)-normalize the model embeddings (except for SimCLR) and then project them using a randomly initialized linear head. We motivate this choice empirically by ablating these components, and the results in Table 6 show that this configuration produces the most predictive gradients for ImageNet-100.

Gradients source layer.Throughout the paper, we extract gradients from the self-attention output projection of the last transformer block. Intuitively, deeper layers provide more predictive features, and thus, their gradient should display the same behavior. This assumption is confirmed by our results in Figure 8, where, for all losses, deeper layers consistently produce more predictive gradients. Regarding the choice of layer within a transformer block, for shallower blocks, the second MLP layer is significantly more predictive, but the performance gap becomes insignificant as we move towards deeper blocks, favoring (by a small margin) the attention output projection, which is also more memory efficient, as it has fewer parameters compared to other layers.

## 6 Discussion and Conclusion

Broader impact.Our method improves the features used for the kNN algorithm. As such, it is a fundamental contribution to Machine Learning. Given the ubiquitous use of kNN, our method could have positive consequences, such as improving reliability and factuality in Retrieval Augmented Generation (RAG) systems, where Language Models are grounded in retrieved pieces of text before generating an answer. We do not foresee any direct negative consequence caused by our method.

Impact of pretraining dataset.Our method works with various backbones, model sizes, and pretraining strategies. However, we have observed that the benefits diminish as the size of the pretraining dataset increases: in Table 1, fungi provides a smaller relative improvements on a backbone pretrained with IN21K compared to one pretrained on IN1K, and similarly, in Table 16 the relative improvement over EVA-CLIP (Sun et al., 2023) is smaller compared to the improvement over CLIP (Radford et al., 2021), as they are pretrained on 2B and 400M text-image pairs respectively.

    & \(_{}\) & \\  Norm & Projection & Acc. \\   & & 88.3 \\ ✓ & & 87.3 \\  & ✓ & 88.8 \\ ✓ & ✓ & **89.1** \\    
    & \(_{}\) & \\  Norm & Projection & Acc. \\   & & N/A \\ ✓ & & 88.7 \\  & ✓ & **88.8** \\ ✓ & ✓ & **88.7** \\   

Table 6: **Impact of the projection head configuration.** Top-1 accuracy of gradients on ImageNet-100 in k-nearest neighbor classification versus the projection head configuration for KL, DINO and SimCLR gradients. “norm” indicates whether the features are \(L_{2}\)-normalized before being projected. As features are always \(L_{2}\)-normalized for the SimCLR objective, the “empty” head configuration is not applicable. The default setup is marked in ‘cyan’.

Figure 8: **Gradients from deeper layers are more predictive.** Top-1 accuracy of gradients obtained from every layer of a supervised DeIT ViT-B/16 in k-nearest neighbor classification on ImageNet-100 for the KL, DINO, and SimCLR objectives. The default setup (last layers) is marked in ‘cyan’.

Computational efficiency.Computing fungi features introduces an overhead, which we measure in Table 27 by comparing the throughput of a DeIT ViT-B/16 when extracting gradients and embeddings. The DINO and SimCLR losses have the largest overhead, as they forward 12 and 49 views per image, respectively. As shown in Appendix C.1, this number can be reduced, at a performance cost. However, thanks to our dimensionality reduction, the speed of kNN retrieval is not impacted.

## 7 Conclusion

We have shown that gradients from self-supervised objectives have predictive abilities and encode complementary information to the model embeddings. Building on those findings, we introduced fungi, which effectively combines embeddings and gradients into powerful features for retrieval-based tasks. Specifically, we have shown that fungi enhance the performance of kNN-based image and text classification across models, pretraining strategies, and downstream datasets, both for full dataset and few shot setups. Moreover, we have shown that fungi significantly boost the performance of DINO features for retrieval-based semantic segmentation tasks.

Acknowledgements.We acknowledge the use of the Dutch national supercomputer Snellius to run the experiments presented in this paper. YMA thanks Tengda Han for the initial discussions on using self-supervised gradients for tasks other than learning. WS thanks the Amsterdam ELLIS Unit for their generous funding, which allowed him to visit the Valeo laboratory in Paris. The presentation of this paper at the conference was financially supported by the Amsterdam ELLIS Unit and Qualcomm.