# The Surprising Effectiveness of Diffusion Models for

Optical Flow and Monocular Depth Estimation

 Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar,

Mohammad Norouzi, Deqing Sun, David J. Fleet

{srbs,irwinherrmann,junhwahur,abhiskar,deqingsun,davidfleet}@google.com

Google DeepMind and Google Research

DF is also affiliated with the University of Toronto and the Vector Institute.

###### Abstract

Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity. We show that they also excel in estimating optical flow and monocular depth, surprisingly, without task-specific architectures and loss functions that are predominant for these tasks. Compared to the point estimates of conventional regression-based methods, diffusion models also enable Monte Carlo inference, e.g., capturing uncertainty and ambiguity in flow and depth. With self-supervised pre-training, the combined use of synthetic and real data for supervised training, and technical innovations (infilling and step-unrolled denoising diffusion training) to handle noisy-incomplete training data, and a simple form of coarse-to-fine refinement, one can train state-of-the-art diffusion models for depth and optical flow estimation. Extensive experiments focus on quantitative performance against benchmarks, ablations, and the model's ability to capture uncertainty and multimodality, and impute missing values. Our model, DDVM (Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth error of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26% on the KITTI optical flow benchmark, about 25% better than the best published method. For an overview see diffusion-vision.github.io

## 1 Introduction

Diffusion models have emerged as powerful generative models for high fidelity image synthesis, capturing rich knowledge about the visual world . However, at first glance, it is unclear whether these models can be as effective on many classical computer vision tasks. For example, consider two dense vision estimation tasks, namely, optical flow, which estimates frame-to-frame correspondences, and monocular depth perception, which makes depth predictions based on a single image. Both tasks are usually treated as regression problems and addressed with specialized architectures and task-specific loss functions, _e.g._, cost volumes, feature warps, or suitable losses for depth. Without these specialized components or the regression framework, general generative techniques may be ill-equipped and vulnerable to both generalization and performance issues.

In this paper, we show that these concerns, while valid, can be addressed and that, surprisingly, a generic, conventional diffusion model for image to image translation works impressively well on both tasks, often outperforming the state of the art. In addition, diffusion models provide valuable benefits over networks trained with regression; in particular, diffusion allows for approximate inference with multi-modal distributions, capturing uncertainty and ambiguity (_e.g._ see Figure 1).

One key barrier to training useful diffusion models for monocular depth and optical flow inference concerns the amount and quality of available training data. Given the limited availability of labelledtraining data, we propose a training pipeline comprising multi-task self-supervised pre-training followed by supervised pre-training using a combination of real and synthetic data. Multi-task self-supervised pre-training leverages the strong performance of diffusion models on tasks like colorization and inpainting (e.g., 54). We also find that supervised (pre-)training with a combination of real and large-scale synthetic data improves performance significantly.

A further issue concerns the fact that many existing real datasets for depth and optical flow have noisy and incomplete ground truth annotations. This presents a challenge for the conventional training framework and iterative sampling in diffusion models, leading to a problematic distribution shift between training and inference. To mitigate these issues we propose the use of an \(L_{1}\) loss for robustness, infilling missing depth values during training, and the introduction of _step-unrolled denoising diffusion_. These elements of the model are shown through ablations to be important for both depth and flow estimation.

Our contributions are as follows:

1. We formulate optical flow and monocular depth estimation as image to image translation with generative diffusion models, without specialized loss functions and model architectures.
2. We identify and propose solutions to several important issues w.r.t data. For both tasks, to mitigate distribution shift between training and inference with noisy, incomplete data, we propose infilling, step-unrolling, and an \(L_{1}\) loss during training. For flow, to improve generalization, we introduce a new dataset mixture for pre-training, yielding a RAFT  baseline that outperforms all published methods in zero-shot performance on the Sintel and KITTI training benchmarks.
3. Our diffusion models is competitive with or surpasses SOTA for both tasks. For monocular depth estimation we achieve a SOTA relative error of 0.074 on the NYU dataset and perform competitively on KITTI. For flow, diffusion surpasses the stronger RAFT baseline by a large margin in pre-training and our fine-tuned model achieves an Fl-all outlier rate of 3.26% on the public KITTI test benchmark, \(\)25% lower than the best published method .
4. Our diffusion model is also shown to capture flow and depth uncertainty, and the iterative denoising process enables zero-shot, coarse-to-fine refinement, and imputation.

## 2 Related work

Optical flow and depth estimation have been extensively studied. Here we briefly review only the most relevant work, and refer the interested readers to the references cited therein.

**Optical flow.** The predominant approach to optical flow is regression-based, with a focus on specialized network architectures to exploit domain knowledge, _e.g._, cost volume construction [14; 22; 23; 38; 68; 81; 83; 85], coarse-to-fine estimation [68; 77; 82], occlusion handling [24; 27; 67], or iterative refinement [25; 26; 74], as evidenced by public benchmark datasets [4; 44]. Some

Figure 1: **Examples of multi-modal prediction** on depth (NYU) and optical flow (Sintel and KITTI). Each row shows an input image (or overlayed image pair for optical flow), a variance heat map from 8 samples, and 3 individual samples. Our model captures multi-modality in uncertain/ambiguous cases, such as reflective (_e.g._ mirror on NYU), transparent (_e.g._ vehicle window on KITTI), and translucent (_e.g._ fog on Sintel) regions. High variance also occurs at object boundaries, which are often challenging cases for optical flow, and also partially originate from noisy ground truth measurements for depth. See Fig. 8, 9, 10 and 11 for more examples.

recent work has also advocated for generic architectures: Perceiver IO  introduces a generic transformer-based model that works for any modality, including optical flow and language modeling. Regression-based methods, however, only give a single prediction of the optical flow and do not readily capture uncertainty or ambiguity in the flow. Our work introduces a surprisingly simple, generic architecture for optical flow using a denoising diffusion model.

We find that this generic generative model is surprisingly effective for optical flow, recovering fine details on motion boundaries, while capturing multi-modality of the motion distribution.

**Monocular depth.** Monocular depth estimation has been a long-standing problem in computer vision [58; 59] with recent progress focusing on specialized loss functions and architectures [1; 5; 15; 31] such as the use of multi-scale networks [12; 13], adaptive binning [3; 35] and weighted scale-shift invariant losses . Large-scale in-domain pre-training has also been effective for depth estimation [49; 50; 52], which we find to be the case here as well. We build on this rich literature, but with a simple, generic architecture, leveraging recent advances in generative models.

**Diffusion models.** Diffusion models are latent-variable generative models trained to transform a sample of a Gaussian noise into a sample from a data distribution [21; 62]. They comprise a _forward process_ that gradually annihilates data by adding noise, as 'time' \(t\) increases from 0 to 1, and a learned _generative process_ that reverses the forward process, starting from a sample of random noise at \(t=1\) and incrementally adding structure (attenuating noise) as \(t\) decreases to 0. A conditional diffusion model conditions the steps of the reverse process (e.g., on labels, text, or an image).

Central to the model is a denoising network \(f_{}\) that is trained to take a noisy sample \(y_{t}\) at some time-step \(t\), along with a conditioning signal \(x\), and predict a less noisy sample. Using Gaussian noise in the forward process, one can express the training objective over the sequence of transitions (as \(t\) slowly decreases) as a sum of non-linear regression objectives, with the L2 loss (here with the \(\)-parameterization):

\[_{(,\,)}\,_{(t,\,)}f_{ }(,\,}\,+} \,}_{},\,t)-_{2}^{2}\] (1)

where \((0,I)\), \(t(0,1)\), and where \(_{t}>0\) is computed with a pre-determined noise schedule. For inference (i.e., sampling), one draws a random noise sample \(_{1}\), and then iteratively uses \(f_{}\) to estimate the noise, from which one can compute the next latent sample \(_{s}\), for \(s<t\).

**Self-supervised pre-training.** Prior work has shown that self-supervised tasks such as colorization [33; 86] and masked prediction  serve as effective pre-training for downstream vision tasks. Our work also confirms the benefit of self-supervised pre-training  for diffusion-based image-to-image translation, by establishing a new SOTA on optical flow and monocular depth estimation while also representing multi-modality and supporting zero-shot coarse-to-fine refinement and imputation.

Figure 2: **Training architecture**. Given ground truth flow/depth, we first infill missing values using interpolation. Then, we add noise to the label map and train a neural network to model the conditional distribution of the noise given the RGB image(s), noisy label, and time step. One can optionally unroll the denoising step(s) during training (with stop gradient) to bridge the distribution gap between training and inference for \(y_{t}\).

## 3 Model Framework

In contrast to the conventional monocular depth and optical flow methods, with rich usage of specialized domain knowledge on their architecture designs, we introduce simple, generic architectures and loss functions. We replace the inductive bias in state-of-the-art architectures and losses with a powerful generative model along with a combination of self-supervised pre-training and supervised training on both real and synthetic data.

The denoising diffusion model (Figure 2) takes a noisy version of the target map (_i.e._, a depth or flow) as input, along with the conditioning signal (one RGB image for depth and two RGB images for flow). The denoiser effectively provides a noise-free estimate of the target map (_i.e._, ignoring the specific loss parameterization used). The training loss penalizes residual error in the denoised map, which is quite distinct from typical image reconstruction losses used in optical flow estimation.

### Synthetic pre-training data and generalization

Given that we train these models with a generic denoising objective, without task-specific inductive biases in the form of specialized architectures, the choice of training data becomes critical. Below we discuss the datasets used and their contributions in detail. Because training data with annotated ground truth is limited for many dense vision tasks, here we make extensive use of synthetic data in the hope that the geometric properties acquired from synthetic data during training will transfer to different domains, including natural images.

AutoFlow  has recently emerged as a powerful synthetic dataset for training flow models. We were surprised to find that training on AutoFlow alone is insufficient, as the diffusion model appears to devote a significant fraction of its representation capacity to represent the shapes of AutoFlow regions, rather than solving for correspondence. As a result, models trained on AutoFlow alone exhibit a strong bias to generate flow fields with polygonal shaped regions, much like those in AutoFlow, often ignoring the shapes of boundaries in the two-frame RGB inputs (_e.g._ see Figure 3).

To mitigate bias induced by AutoFlow in training, we further mix in three synthetic datasets during training, namely, FlyingThings3D , Kubric  and TartanAir . Given a model pre-trained on AutoFlow, for compute efficiency, we use a greedy mixing strategy where we fix the relative ratio of the previous mixture and tune the proportion of the newly added dataset. We leave further exploration of an optimal mixing strategy to future work. Zero-shot testing of the model on Sintel and KITTI (see Table 1 and Fig. 3) shows substantial performance gains with each additional synthetic dataset.

We find that pre-training is similarly important for depth estimation (see Table 7). We learn separate indoor and outdoor models. For the indoor model we pre-train on a mix of ScanNet  and SceneNet RGB-D . The outdoor model is pre-trained on the Waymo Open Dataset .

### Real data: Challenges with noisy, incomplete ground truth

Ground truth annotations for real-world depth or flow data are often sparse and noisy, due to highly reflective surfaces, light absorbing surfaces , dynamic objects , _etc_. While regression-based

Figure 3: **Effects of adding synthetic datasets in pretraining. Diffusion models trained only with AutoFlow (AF) tend to provide very coarse flow estimates and can hallucinate shapes. The addition of FlyingThings (FT), Kubric (KU), and TartanAir (TA) remove the AF-induced bias toward polygonal-shaped regions, and significantly improve flow quality on fine detail, e.g. trees, thin structures, and motion boundaries.**methods can simply compute the loss on pixels with valid ground truth, corruption of the training data is more challenging for diffusion models. Diffusion models perform inference through iterative refinement of the target map \(\) conditioned on RGB image data \(\). It starts with a sample of Gaussian noise \(_{1}\), and terminates with a sample from the predictive distribution \(p(_{0}\,|\,)\). A refinement step from time \(t\) to \(s\), with \(s\!<\!t\), proceeds by sampling from the parameterized distribution \(p_{}(_{s}\,|\,_{t},)\); i.e., each step operates on the output from the previous step. During training, however, the denoising steps are decoupled (see Eqn. 1), where the denoising network operates on a noisy version of the ground truth depth map instead of the output of the previous iteration (reminiscent of teaching forcing in RNN training ). Thus there is a distribution shift between marginals over the noisy target maps during training and inference, because the ground truth maps have missing annotations and heavy-tailed sensor noise while the noisy maps obtained from the previous time step at inference time should not. This distribution shift has a very negative impact on model performance. Nevertheless, with the following modifications during training we find that the problems can be mitigated effectively.

**Infilling.** One way to reduce the distribution shift is to impute the missing ground truth. We explored several ways to do this, including simple interpolation schemes, and inference using our model (trained with nearest neighbor interpolation). We find that nearest neighbor interpolation is sufficient to impute missing values in the ground truth maps in the depth and flow field training data.

Despite the imputation of missing ground truth depth and flow values, note that the training loss is only computed and backpropagated from pixels with known (not infilled) ground truth depth. We refer to this as the masked denoising loss (see Figure 2).

**Step-unrolled denoising diffusion training.** A second way to mitigate distribution shift in the \(y_{t}\) marginals in training and inference, is to construct \(y_{t}\) from model outputs rather than ground truth maps. One can do this by slightly modifying the training procedure (see Algorithm 1) to run one forward pass of the model and build \(y_{t}\) by adding noise to the model's output rather than the training map. We do not propagate gradients for this forward pass. This process, called _step-unrolled denoising diffusion_, slows training only marginally (\(\)15% on a TPU v4). This step-unrolling is akin to the predictor-corrector sampler of  which uses an extra Langevin step to improve the target marginal distribution of \(y_{t}\). Interestingly, the problem of training / inference distribution shift resembles that of _exposure bias_ in autoregressive models, for which the mismatch is caused by _teacher forcing_ during training . Several solutions have been proposed for this problem in the literature . Step-unrolled denoising diffusion also closely resembles the approach in  for training denoising autoencoders on text.

We only perform step-unrolled denoising diffusion during model fine-tuning. Early in training the denoising predictions are inaccurate, so the latent marginals over the noisy target maps will be closer to the desired _true_ marginals than those produced by adding noise to denoiser network outputs. One might consider the use of a curriculum for gradually introducing step-unrolled denoising diffusion in the later stages of supervised pre-training, but this introduces additional hyper-parameters, so we simply invoke step-unrolled denoising diffusion during fine-tuning, and leave an exploration of curricula to future work.

```
1:\(x\) conditioning images, \(y\) flow or depth map, \(mask\) binary mask of known values
2:\(t U(0,1)\), \( N(0,1)\)
3:\(y\) = fill_holes_with_interpolation(\(y\))
4:\(y_{t}=}*y+}*\)
5:ifundroll_step then
6:\(e_{pred}\) = stop_gradient(\(f_{}(x,y_{t},t)\))
7:\(y_{pred}=(y_{t}-}*_{pred})/}\)
8:\(y_{t}=}*y_{pred}+}*\)
9:\(=(y_{t}-}*y)/}\)
10:endif
11:\(_{pred}=f_{}(x,y_{t},t)\)
12:loss = reduce_mean(\(|-_{pred}|[mask]\)) ```

**Algorithm 1** Denoising diffusion train step with infilling and step unrolling

\(L_{1}\)**denoiser loss.** While the \(L_{2}\) loss in Eqn. 1 is ideal for Gaussian noise and noise-free ground truth maps, in practice, real ground truth depth and flow fields are noisy and heavy tailed; _e.g._, for distant objects, near object boundaries, and near pixels with missing annotations. We hypothesize that the robustness afforded by the \(L_{1}\) loss may therefore be useful in training the neural denoising network. (See Tables 11 and 12 in the supplementary material for an ablation of the loss function for monocular depth estimation.)

### Coarse-to-fine refinement

Training high resolution diffusion models is often slow and memory intensive but increasing the image resolution of the model has been shown to improve performance on vision tasks . One simple solution, yielding high-resolution output without increasing the training cost, is to perform inference in a coarse-to-fine manner, first estimating flow over the entire field of view at low resolution, and then refining the estimates in a patch-wise manner. For refinement, we first up-sample the low-resolution map to the target resolution using bicubic interpolation. Patches are cropped from the up-scaled map, denoted \(z\), along with the corresponding RGB inputs. Then we run diffusion model inference starting at time \(t^{}\) with a noisy map \(y_{t^{}}(y_{t^{}};}}\,z,(1 -_{t^{}})I)\). For simplicity, \(t^{}\) is a fixed hyper-parameter, set based on a validation set. This process is carried out for multiple overlapping patches. Following Perceiver IO , the patch estimates are merged using weighted masks with lower weight near the patch boundaries since predictions at boundaries are more prone to errors. (See Section H.5 for more details.)

## 4 Experiments

As our denoiser backbone, we adopt the Efficient UNet architecture , pretrained with Palette  style self-supervised pretraining, and slightly modified to have the appropriate input and output channels for each task. Since diffusion models expect inputs and generate outputs in the range \([-1.,1.]\), we normalize depths using max depth of 10 meters and 80 meters respectively for the indoor and outdoor models. We normalize the flow using the height and width of the ground truth. Refer to Section H for more details on the architecture, augmentations and other hyper-parameters.

**Optical flow.** We pre-train on the mixture described in Section 3.1 at a resolution of 320\(\)448 and report zero-shot results on the widely used Sintel  and KITTI  datasets. We further fine-tune this model on the standard mixture consisting of AutoFlow , FlyingThings , VIPER , HD1K , Sintel and KITTI at a resolution of 320\(\)768 and report results on the test set from the public benchmark. We use a standard average end-point error (AEPE) metric that calculates L2

Figure 4: **Visual results comparing RAFT with our method after pretraining. Note that our method does much better on fine details and ambiguous regions.**

distance between ground truth and prediction. On KITTI, we additionally use the outlier rate, Fl-all, which reports the outlier ratio in \(\%\) among all pixels with valid ground truth, where an estimate is considered as an outlier if its error exceeds 3 pixels and 5\(\%\) w.r.t. the ground truth.

**Depth.** We separately pre-train indoor and outdoor models on the respective pre-training datasets described in Section 3.1. The indoor depth model is then finetuned and evaluated on the NYU depth v2 dataset  and the outdoor model on the KITTI depth dataset . We follow the standard evaluation protocol used in prior work . For both NYU depth v2 and KITTI, we report the absolute relative error (REL), root mean squared error (RMS) and accuracy metrics (\(_{1}<1.25\)).

### Evaluation on benchmark datasets

**Depth.** Table 3 reports the results on NYU depth v2 and KITTI (see Section D for more detailed results and Section B for qualitative comparison with DPT on NYU). We achieve a state-of-the-art absolute relative error of 0.074 on NYU depth v2. On KITTI, our method performs competitively with prior work. We report results with averaging depth maps from one or more samples. Note that most prior works use post processing that averages two samples, one from the input image, and the other based on its reflection about the vertical axis.

**Flow.** Table 1 reports the zero-shot results of our model on Sintel and KITTI Train datasets where ground truth are provided. The model is trained on our newly proposed pre-training mixtures (AutoFlow (AF), FlyingThings (FT), Kubric (KU), and TartanAir (TA)). We report results by averaging 8 samples at a coarse resolution and then refining them to the full resolution as described in Section 3.3. For a fair comparison, we re-train RAFT on this pre-training mixture; this new RAFT model significantly outperforms the original RAFT model. And our diffusion model outperforms the stronger

    &  &  &  \\   & & AEPE & AEPE & Fl-all \\  FlowFormer & Chairs\(\)Things & **1.01** & 2.40 & 4.09 & 14.72\% \\ RAFT & Chairs\(\)Things & 1.68 & 2.80 & 5.92 & - \\  Perceiver 10 & AutoFlow & 1.81 & 2.42 & 4.98 & - \\ RAFT & AutoFlow & 1.74 & 2.41 & 4.18 & 13.41\% \\  RAFT (ours) & AF\(\)AF-FT+KU+TA & 1.27 & 2.28 & 2.71 & 9.16\% \\
**DDVM (ours)** & AF\(\)AF+FT+KU+TA & 1.24 & **2.00** & **2.19** & **7.58\%** \\   

Table 1: **Zero-shot optical flow estimation results on Sintel and KITTI. We provide a new RAFT baseline using our proposed pre-training mixture and substantially improve the accuracy over the original. Our diffusion model outperforms even this much stronger baseline and achieves state-of-the-art zero-shot results on Sintel.final and KITTI.**

   Method & Sintel.clean Sintel.final & KITTI \\  SKFlow \({}^{*}\) & 1.30 & 2.26 & 4.84\% \\ CRAFT \({}^{*}\) & 1.44 & 2.42 & 4.79\% \\ Flowformer  & **1.44** & **2.18** & 4.68\% \\ RAFT-OCTC \({}^{*}\) & 1.51 & 2.57 & 4.33\% \\ RAFT-it \({}^{}\) & 1.55 & 2.90 & 4.31\% \\
**DDVM (ours)\({}^{}\)** & 1.75 & 2.48 & **3.26\%** \\   

Table 2: **Optical flow finetuning evaluation on public benchmark datasets (AEPE\(\) for Sintel and FI-all\(\) for KITTI). **Bold** indicates the best and underline the 2\({}^{nd}\)-best. \({}^{@sectionsign}\) uses extra datasets (AutoFlow and VIPER) on top of defaults (FlyingThings, HD1K, KITTI, and Sintel). \({}^{*}\)uses warm start on Sintel.

Figure 5: **Visual results comparing RAFT with our method after finetuning. Ours does much better on fine details and ambiguous regions.**

RAFT baseline. It achieves the state-of-the-art zero-shot results on both the challenging Sintel Final and KITTI datasets.

Figure 4 provides a qualitative comparison of pre-trained models. Our method demonstrates finer details on both object and motion boundaries. Especially on KITTI, our model recovers fine details remarkably well, _e.g_. on trees and its layered motion between tree and background.

We further finetune our model on the mixture of the following datasets, AutoFlow, FlyingThings, HD1K, KITTI, Sintel, and VIPER. Table 2 reports the comparison to state-of-the-art optical flow methods on public benchmark datasets, Sintel and KITTI. On KITTI, our method outperforms all existing optical flow methods by a substantial margin (even most scene flow methods that use stereo inputs), and sets the new state of the art. On the challenging Sintel final, our method is competitive with other state of the art models. Except for methods using warm-start strategies, our method is only behind FlowFormer which adopts strong domain knowledge on optical flow (_e.g_. cost volume, iterative refinement, or attention layers for larger context) unlike our generic model. Interestingly, we find that our model outperforms FlowFormer on 11/12 Sintel test sequences and our overall worse performance can be attributed to a much higher AEPE on a single (possibly out-of-distribution) test sequence. We discuss this in more detail in Section 5. On KITTI, our diffusion model outperforms FlowFormer by a large margin (30.34\(\%\)).

### Ablation study

**Infilling and step-unrolling.** We study the effect of infilling and step-unrolling in Table 4. For depth, we report results for fine-tuning our pre-trained model on the NYU and KITTI datasets with the same resolution and augmentations as our best results. For flow, we fine-tune on the KITTI train set alone (with nearest neighbor resizing to the target resolution being the only augmentation) at a resolution of 320\(\)448 and report metrics on the KITTI val set . We report results with a single sample and no coarse-to-fine refinement. We find that training on raw sparse data without infilling and step unrolling leads to poor results, especially on KITTI where the ground truth is quite sparse. Step-unrolling helps to stabilize training without requiring any extra data pre-processing. However, we find that most gains come from interpolating missing values in the sparse labels. Infilling and step-unrolling compose well as our best results use both; infilling (being an approximation) does not completely bridge the training-inference distribution shift of the noisy latent.

    &  &  &  \\   & REL & RMS & REL & RMS & AEPE & Fl-all \\  Baseline & 0.079 & 0.331 & 0.222 & 3.770 & - & - \\ Step-unroll & 0.076 & **0.324** & 0.085 & 2.844 & 1.84 & 6.16\% \\ Infill & 0.077 & 0.338 & 0.057 & 2.744 & 1.53 & 5.24\% \\
**Step-unroll \(\&\) infill** & **0.075** & **0.324** & **0.056** & **2.700** & **1.47** & **4.74\%** \\   

Table 4: **Ablation on infilling and step-unrolling. Without either one, performance deteriorates. Without both, optical flow models fail to train on KITTI.**

    &  &  &  \\   & & \(_{1}\) & REL\(\) & RMS\(\) & \(_{1}\) & REL\(\) & RMS\(\) \\  TransDepth  & Res-50+ViT-B\({}^{}\) & 0.900 & 0.106 & 0.365 & 0.956 & 0.064 & 2.755 \\ DPT  & Res-50+ViT-B\({}^{}\) & 0.904 & 0.110 & 0.357 & 0.959 & 0.062 & 2.573 \\ BTS  & DenseNet-161 & 0.885 & 0.110 & 0.392 & 0.956 & 0.059 & 2.756 \\ AdaBins  & E-B5+Min-ViT\({}^{}\) & 0.903 & 0.103 & 0.364 & 0.964 & 0.058 & 2.360 \\ BinsFormer  & Swin-Large\({}^{}\) & 0.925 & 0.094 & 0.330 & 0.974 & 0.052 & 2.098 \\ PixelFormer  & Swin-Large\({}^{}\) & 0.929 & 0.090 & 0.322 & 0.976 & 0.051 & 2.081 \\ MIM  & SwinV2-\({}^{}\) & 0.949 & 0.083 & 0.287 & **0.977** & **0.050** & **1.966** \\ AiT-P  & SwinV2-L\({}^{}\) & **0.953** & 0.076 & **0.279** & - & - & - \\   & samples=1 & Efficient U-Net\({}^{}\) & 0.944 & 0.075 & 0.324 & 0.964 & 0.056 & 2.700 \\  & samples=2 & Efficient U-Net\({}^{}\) & 0.944 & **0.074** & 0.319 & 0.965 & 0.055 & 2.660 \\   & samples=4 & Efficient U-Net\({}^{}\) & 0.946 & **0.074** & 0.315 & 0.965 & 0.055 & 2.613 \\   

Table 3: **Performance comparison on the NYU-Depth-v2 and KITTI datasets. \(\) indicates method uses unsupervised pretraining, \(\{\)indicates supervised pretraining and \(\) indicates use of auxilliary supervised depth data. Best / second best results are bolded / underlined respectively. \(\): lower is better \(\): higher is better.**

   Coarse-to-fine refinement & Sintel.clean & Sintel.final & KITTI \\  AEPE & AEPE & AEPE & FI-all \\  Without & 1.42 & 2.12 & 2.35 & 8.65\% \\
**With** & **1.24** & **2.00** & **2.19** & **7.58\%** \\   

Table 5: **Coarse-to-fine refinement improves zero-shot optical flow estimation results on Sintel and KITTI, along with the qualitative improvements shown in Figure 6.**

**Coarse-to-fine refinement.** Figure 6 shows that coarse-to-fine refinement (Section 3.3) substantially improves fine-grained details in estimated optical flow fields. It also improves the metrics for zero-shot optical flow estimation on both KITTI and Sintel, as shown in Table 5.

**Datasets.** When using different mixtures of datasets for pretraining, we find that diffusion models sometimes capture region boundaries and shape at the expense of local textural variation (eg see Figure 3). The model trained solely on AutoFlow tends to provide very coarse flow, and mimics the object shapes found in AutoFlow. The addition of FlyingThings, Kubric, and TatanAir removes this hallucination and significantly improves the fine details in the flow estimates (eg, shadows, trees, thin structure, and motion boundaries) together with a substantial boost in accuracy (_cf_. Table 6). Similarly, we find that mixing SceneNet RGB-D , a synthetic dataset, along with ScanNet  provides a performance boost for fine-tuning results on NYU depth v2, shown in Table 7.

### Interesting properties of diffusion models

**Multimodality.** One strength of diffusion models is their ability to capture complex multimodal distributions. This can be effective in representing uncertainty, especially where there may exist natural ambiguities and thus multiple predictions, _e.g_. in cases of transparent, translucent, or reflective cases. Figure 1 presents multiple samples on the NYU, KITTI, and Sintel datasets, showing that our model captures multimodality and provides plausible samples when ambiguities exist. More details and examples are available in Section A.

**Imputation of missing labels.** A diffusion model trained to model the conditional distribution \(p(y|x)\) can be zero-shot leveraged to sample from \(p(y|x,y_{partial})\) where \(y_{partial}\) is the partially known label. One approach for doing this, known as the _replacement method_ for conditional inference , is to replace the known portion the latent \(y_{t}\) at each inference step with the noisy latent built by applying the forward process to the known label. We qualitatively study the results of leveraging replacement guidance for depth completion and find it to be surprisingly effective. We illustrate this by building a pipeline for iteratively generating 3D scenes (conditioned on a text prompt) as shown in Figure 7 by leveraging existing models for text-to-image generation and text-conditional image inpainting. While a more thorough evaluation of depth completion and novel view synthesis against existing methods is warranted, we leave that exploration to future work. (See Section C for more details and examples.)

## 5 Limitations

**Latency.** We adopt standard practices from image-generation models, leading to larger models and slower running times than RAFT. However, we are excited by the recent progress on progressive distillation [42; 56] and consistency models  to improve inference speed in diffusion models.

   Dataset & Sintel.clean & Sintel.final & KITTI & KITTI FI-all \\  AF pretraining & 2.04 & 2.55 & 4.47 & 16.59\% \\ AF\(\)AF+FT & 1.48 & 2.22 & 3.71 & 14.07\% \\ AF\(\)AF+FT+KU & 1.33 & 2.04 & 2.82 & 9.27\% \\ AF\(\)AF+FT+KU+TA & **1.24** & **2.00** & **2.19** & **7.58\%** \\   

Table 6: **The addition of optical flow synthetic datasets substantially improves the zero-shot results on Sintel and KITTI.**

   Dataset & REL & RMS \\  SceneNet RGB-D & 0.089 & 0.362 \\ ScanNet & 0.081 & 0.346 \\ SceneNet RGB-D + ScanNet & **0.075** & **0.324** \\   

Table 7: **The addition of synthetic depth data in pre-training substantially improves fine-tuning performance on NYU.**

Figure 6: **Visual results with and without coarse-to-fine refinement**. For our pretrained model, refinement helps correct wrong flow and adds details to correct flow.

**Sintel fine-tuning.** Under the zero-shot setting, our method achieves state-of-the-art results on both Sintel Final and KITTI. Under the fine-tuning setting, ours is state-of-the-art on KITTI but is behind FlowFormer  on Sintel Final. We discuss several possible reasons for why this may be the case.

* We follow the fine-tuning procedure in . While their zero-shot RAFT results are comparable to FlowFormer on Sintel and KITTI, the fine-tuned RAFT-it is significantly better on KITTI but less accurate on Sintel than FlowFormer. It is possible that the fine-tuning procedure (_e.g_. dataset mixture or augmentations) developed in  is more suited for KITTI than Sintel.
* Another possible reason is that there is substantial domain gap between the training and test data on Sintel than KITTI. On Sintel test, there is a particular sequence "Ambush 1", where the girl's right arm moves out of the image boundary. Our method has an AEPE close to 30 while FlowFormer has lower than 10. It is likely that the attention on the cost volume mechanism by FlowFormer can better reason about the motion globally and handles this particular sequence well. This particular sequence may account for the major difference in the overall results; among 12 available results on the Sintel website, ours has lower AEPE on 11 sequences but a higher AEPE on the "Ambush 1" sequence, as shown in Table 8. Figure 16 in the appendix further provides visualization.

## 6 Conclusion

We introduced a simple denoising diffusion model for monocular depth and optical flow estimation using an image-to-image translation framework. Our generative approach obtains state-of-the-art results without task-specific architectures or loss functions. In particular, our model achieves an Fl-all score of 3.26% on KITTI, about 25% better than the best published method . Further, our model captures the multi-modality and uncertainty through multiple samples from the posterior. It also allows imputation of missing values, which enables iterative generation of 3D scenes conditioned on a text prompt. Our work suggests that diffusion models could be a simple and generic framework for dense vision tasks, and we hope to see more work in this direction.

   Sequence & Ours & FlowFormer  \\  Perturbed Market 3 & **0.787** & 0.869 \\ Perturbed Shaman 1 & **0.219** & 0.252 \\ Ambush 1 & 29.33 & **8.141** \\ Ambush 3 & **2.855** & 2.973 \\ Bamhoo 3 & **0.415** & 0.577 \\ Cave 3 & **2.042** & 2.352 \\ Market 1 & **0.719** & 1.174 \\ Market 4 & **5.517** & 8.768 \\ Mountain 2 & **0.176** & 0.518 \\ Temple 1 & **0.452** & 0.612 \\ Tiger & **0.413** & 0.596 \\ Wall & **1.639** & 1.723 \\   

Table 8: **Average end-point error (AEPE) on 12 Sintel test sequences** available from the public website.

Figure 7: **Application of zero-shot depth completion** with our model by incorporating it into an iterative 3D scene generation pipeline. Starting with a initial image (optionally generated from a text-to-image model), we sample an image-only conditioned depth map using our model. The image-depth pair is added to a point cloud. We then iteratively render images and depth maps (with holes) from this point cloud by moving the camera. We then fill image holes using an existing image inpainter (optionally text conditioned), and then use our model with replacement guidance to impute missing depths (conditioned on the filled RGB image and known depth).

#### Acknowledgements

We thank Ting Chen, Daniel Watson, Hugo Larochelle and the rest of Google DeepMind for feedback on this work. Thanks to Klaus Greff and Andrea Tagliasacchi for their help with the Kubric generator, and to Chitwan Saharia for help training the Palette model.