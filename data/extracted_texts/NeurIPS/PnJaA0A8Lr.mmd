# Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory

Minhak Song

KAIST ISysE/Math

minhaksong@kaist.ac.kr &Chulhee Yun

KAIST AI

chulhee.yun@kaist.ac.kr

###### Abstract

Cohen et al. (2021) empirically study the evolution of the largest eigenvalue of the loss Hessian, also known as sharpness, along the gradient descent (GD) trajectory and observe the _Edge of Stability_ (EoS) phenomenon. The sharpness increases at the early phase of training (referred to as _progressive sharpening_), and eventually saturates close to the threshold of \(2/()\). In this paper, we start by demonstrating through empirical studies that when the EoS phenomenon occurs, different GD trajectories (after a proper reparameterization) align on a specific bifurcation diagram independent of initialization. We then rigorously prove this _trajectory alignment_ phenomenon for a two-layer fully-connected linear network and a single-neuron nonlinear network trained with a single data point. Our trajectory alignment analysis establishes both progressive sharpening and EoS phenomena, encompassing and extending recent findings in the literature.

## 1 Introduction

It is widely believed that implicit bias or regularization of gradient-based methods plays a key role in generalization of deep learning (Vardi, 2022). There is a growing literature (Gunasekar et al., 2017, 2018; Soudry et al., 2018; Arora et al., 2018, 2019; Ji and Telgarsky, 2019; Woodworth et al., 2020; Chizat and Bach, 2020; Yun et al., 2021) studying how the choice of optimization methods induces an _implicit bias_ towards specific solutions among the many global minima in overparameterized settings.

Cohen et al. (2021) identify a surprising implicit bias of gradient descent (GD) towards global minima with a certain sharpness1 value depending on the step size \(\). Specifically, for reasonable choices of \(\), (a) the sharpness of the loss at the GD iterate gradually increases throughout training until it reaches the stability threshold2 of \(2/\) (known as _progressive sharpening_), and then (b) the sharpness saturates close to or above the threshold for the remainder of training (known as _Edge of Stability_ (EoS)). These findings have sparked a surge of research aimed at developing a theoretical understanding of the progressive sharpening and EoS phenomena (Arora et al., 2022; Lyu et al., 2022; Wang et al., 2022; Ahn et al., 2023; Chen and Bruna, 2023; Zhu et al., 2023). In this paper, we study these phenomena through the lens of _bifurcation theory_, both empirically and theoretically.

Motivating observations:Figure 1 illustrates the GD trajectories with different initializations and fixed step sizes trained on three types of two-dimensional functions: (a) \(((xy))\), (b) \(((x)y)^{2}\), and (c) \(((x)y)^{2}\), where \(x\) and \(y\) are scalars. The functions \(:^{2}\) have sharpness \(y^{2}\) at the global minimum \((0,y)\) for all three models. These toy models can be viewed as examples of single-neuron models, where (a) represents a linear network with \(\)-\(\) loss, while (b) and (c) represent nonlinear networks with squared loss. These simple models can capture some interesting aspects of neural network training in the EoS regime, which are summarized below:* EoS phenomenon: GD converges to a global minimum near the point \((0,)\) with sharpness close to \(2/\). During the convergence phase, the training dynamics exhibit period-\(2\) oscillations.
* For different initializations, GD trajectories for a given step size _align_ on the same curve. For example, Figure 0(a) shows that GD trajectories with different initializations closely follow a specific U-shaped curve until convergence. We call this phenomenon _trajectory alignment_.
* In Figures 0(b) and 0(c), GD trajectories are aligned on a curve with a fractal structure that qualitatively resembles the bifurcation diagram of a typical polynomial map, such as the logistic map. Particularly, Figure 0(c) demonstrates a period-halving phase transition in the GD dynamics, shifting from period-\(4\) oscillation to period-\(2\) oscillation.
* Surprisingly, the curve that GD trajectories approach and follow coincides with the _bifurcation diagram_ of a one-dimensional map \(x x-(x,y)\) with a fixed "control parameter" \(y\). The stability of its fixed point \(x=0\) changes at the bifurcation point \((x,y)=(0,)\), where period-doubling bifurcation occurs. Note that this point is a global minimum with sharpness \(2/\).

Interestingly, such striking behaviors can also be observed in more complex models, up to a proper reparameterization, as we outline in the next subsection.

### Our contributions

In this paper, we discover and study the _trajectory alignment_ behavior of (reparameterized) GD dynamics in the EoS regime. To our best knowledge, we are the first to identify such an alignment with a specific _bifurcation diagram_ independent of initialization. Our empirical findings are rigorously proven for both two-layer fully-connected networks and single-neuron nonlinear networks. Our main contributions are summarized below:

* In Section 2, we introduce a novel _canonical reparameterization_ of training parameters, which incorporates the data, network, and GD step size. This reparameterization allows us to study the trajectory alignment phenomenon in a unified framework. Through empirical study, Section 3 demonstrates that the alignment property of GD trajectories is not limited to toy models but also occurs in wide and deep networks trained on real-world dataset. Furthermore, we find that the alignment trend becomes more pronounced as the network width increases.

Figure 1: **GD trajectories align on the bifurcation diagram at the Edge of Stability.** We run GD on toy models with step size \(=2/25\) in (a), and \(=2/400\) in (b) and (c). Distinct colors indicate independent runs of GD with varying initializations. **Top row:** GD trajectories closely follow the bifurcation diagram of the map \(x x-(x,y)\) and asymptotically reaches the bifurcation point \((0,)\). **Bottom row:** the sharpness reaches \(2/\), and \(x_{t}\) typically shows \(2\)-period oscillating dynamics. The theoretical prediction \((q_{t})\) (dashed lines, defined in Theorems 4.4 and C.4) in (a) and (b) approximates the sharpness along the GD trajectory and demonstrates progressive sharpening.

* In Section 4, we use our canonical reparameterization to establish the trajectory alignment phenomenon for a two-layer fully-connected linear network trained with a single data point. Our theoretical analysis rigorously proves both progressive sharpening and the EoS phenomenon, extending the work of Ahn et al. (2023) to a much broader class of networks and also providing more accurate bounds on the limiting sharpness.
* Our empirical and theoretical analyses up to Section 4 are applicable to convex Lipschitz losses, hence missing the popular squared loss. In Section 5, we take a step towards handling the squared loss. Employing an alternative reparameterization, we prove the same set of theorems as Section 4 for a single-neuron nonlinear network trained with a single data point under squared loss.

### Related works

The _Edge of Stability_ (EoS) phenomenon has been extensively studied in recent years, with many works seeking to provide a deeper understanding of the evolution of sharpness and the oscillating dynamics of GD. Jastrzebski et al. (2019) and Jastrzebski et al. (2020) observe that step size affects the sharpness along the optimization trajectory. Cohen et al. (2021) first formalize EoS through empirical study, and subsequent works have built on their findings. Ahn et al. (2022) analyze EoS through experiments and identify the relations between the behavior of loss, iterates, and sharpness. Ma et al. (2022) suggest that subquadratic growth of the loss landscape is the key factor of oscillating dynamics. Arora et al. (2022) show that (normalized) GD enters the EoS regime, by verifying the convergence to some limiting flow on the manifold of global minimizers. Wang et al. (2022) divide GD trajectory into four phases and explain progressive sharpening and EoS by using the norm of output layer weight as an indicator of sharpness. Lyu et al. (2022) prove that normalization layers encourage GD to reduce sharpness. Damian et al. (2023) use the third-order Taylor approximation of the loss to theoretically analyze EoS, assuming the existence of progressive sharpening. Lee and Jang (2023) propose a new sharpness measure using batch gradient distribution and characterize EoS for SGD. Concurrent to our work, Wu et al. (2023) study the logistic regression problem with separable dataset and establish that GD exhibits an implicit bias toward the max-margin solution in the EoS regime, extending prior findings in the small step size regime (Soudry et al., 2018; Ji and Telgarsky, 2019).

Some recent works rigorously analyze the full GD dynamics for some toy cases and prove that the limiting sharpness is close to \(2/\). Zhu et al. (2023) study the loss \((x,y)(x^{2}y^{2}-1)^{2}\) and prove that the sharpness converges close to \(2/\) with a local convergence guarantee. Notably, Ahn et al. (2023) study the function \((x,y)(xy)\) where \(\) is convex, even, and Lipschitz, and provide a global convergence guarantee. The authors prove that when \(\) is \(\)-\(\) loss or square root loss, the limiting sharpness in the EoS regime is between \(2/-()\) and \(2/\). Our theoretical results extend their results on a single-neuron linear network to a two-layer fully-connected linear network and provide an improved characterization on the limiting sharpness, tightening the gap between upper and lower bounds to only \((^{3})\).

The trajectory alignment phenomenon is closely related to Zhu et al. (2023) which shows empirical evidence of bifurcation-like oscillation in deep neural networks trained on real-world data. However, their empirical results do not show the alignment property of GD trajectory. In comparison, we observe that GD trajectories align on the same bifurcation diagram, independent of initialization.

Very recently, Kreisler et al. (2023) observe a similar trajectory alignment phenomenon for scalar linear networks, employing a reparameterization based on the sharpness of the gradient flow solution. However, their empirical findings on trajectory alignment are confined to scalar linear networks, and do not provide a theoretical explanation. In contrast, our work employs a novel canonical reparameterization and offers empirical evidence for the alignment phenomenon across a wide range of networks. Moreover, we provide theoretical proofs for two-layer linear networks and single-neuron nonlinear networks.

## 2 Preliminaries

Notations.For vectors \(\) and \(\), we denote the \(_{p}\) norm of \(\) by \(\|\|_{p}\), their tensor product as \(\), and \(\) by \(^{ 2}\). For a matrix \(\), we denote the spectral norm by \(\|\|_{2}\). Given a function \(\) and a parameter \(\), we use \(_{}()_{}(^{2}_{} ())\) to denote the sharpness (i.e., the maximum eigenvalue of the loss Hessian) at \(\). We use asymptotic notations with subscripts (e.g., \(_{}()\), \(_{,}()\)) in order to hide constants that depend on the parameters or functions written as subscripts.

### Problem settings

We study the optimization of neural network \(f(\,\,;):^{d}\) parameterized by \(\). We focus on a simple over-parameterized setting trained on a single data point \(\{(,y)\}\), where \(^{d}\) and \(y\). We consider the problem of minimizing the empirical risk

\[()=(f(;)-y),\]

where \(\) is convex, even, and twice-differentiable with \(^{}(0)=1\). We minimize \(\) using GD with step size \(\): \(_{t+1}=_{t}-_{}(_{t})\). The gradient and the Hessian of the function are given by

\[_{}() =^{}(f(;)-y)_{}f(;),\] \[_{}^{2}() =^{}(f(;)-y)(_{}f (;))^{ 2}+^{}(f(;)-y)_{}^{2}f(;).\]

Suppose that \(^{*}\) be a global minimum of \(\), i.e., \(f(;^{*})=y\). In this case, the loss Hessian and the sharpness at \(^{*}\) are simply characterized as

\[_{}^{2}(^{*})=(_{ }f(;^{*}))^{ 2},_{}(^{*})=\|_{}f(;^{*})\|_{2}^{2}.\] (1)

### Canonical reparameterization

**Definition 2.1** (canonical reparameterization).: _For given step size \(\), the canonical reparameterization of \(\) is defined as_

\[(p,q):=(f(;)-y,\;}f(;)\|_{2}^{2}}).\] (2)

Under the canonical reparameterization, \(p=0\) represents global minima, and Eq. (1) implies that the point \((p,q)=(0,1)\) is a global minimum with sharpness \(2/\). Note that \((p,q)\) alone does not, in general, uniquely determine the value of \(\). Rather, the motivation for this reparameterization technique is to effectively analyze the complex GD dynamics in the high-dimensional parameter space by reducing it to a 2-dimensional representation. The update of \(p\) can be written as

\[p_{t+1} =f(;_{t+1})-y=f;_{t}- ^{}(f(;_{t})-y)_{}f(; _{t})-y\] \[ f(;_{t})-_{}f(; _{t})^{}(^{}(f(;_{t})-y) _{}f(;_{t}))-y\] \[=(f(;_{t})-y)-^{}(f(;_{t})-y)\|_{}f(;)\|_{2}^{2}\] \[=p_{t}-(p_{t})}{q_{t}},\] (3)

which can be obtained by first-order Taylor approximation on \(f\) for small step size \(\).3

### Bifurcation analysis

Motivated from the approximated \(1\)-step update rule given by Eq. (3), we conduct the bifurcation analysis on this one-dimensional map, considering \(q_{t}\) as a control parameter. We first review some basic notions used in bifurcation theory (Strogatz, 1994).

**Definition 2.2** (stability of fixed point).: _Let \(z_{0}\) be a fixed point of a differentiable map \(f:\), i.e., \(f(z_{0})=z_{0}\). We say \(z_{0}\) is a stable fixed point of \(f\) if \(|f^{}(z)|<1\), and we say \(z_{0}\) is an unstable fixed point of \(f\) if \(|f^{}(z)|>1\)._

**Definition 2.3** (stability of periodic orbit).: _A point \(z_{0}\) is called a period-\(p\) point of a map \(f:\) if \(z_{0}\) is the fixed point of \(f^{p}\) and \(f^{j}(z_{0}) z_{0}\) for any \(1 j p-1\). The orbit of \(z_{0}\), given by \(\{z_{j}=f^{j}(z_{0}) j=0,1,,p-1\}\) is called the period-\(p\) orbit of \(f\). A period-\(p\) orbit is stable (unstable) if its elements are stable (unstable) fixed points of \(f^{p}\), i.e., \(_{j=0}^{p-1}|f^{}(z_{j})|<1\) (\(>1\))._

Now we analyze the bifurcation of the one-parameter family of mappings \(f_{q}:\) given by

\[f_{q}(p) p(1-),\] (4)

where \(q\) is a control parameter and \(r\) is a differentiable function satisfying Assumption 2.4 below.

**Assumption 2.4**.: _A function \(r:\) is even, continuously differentiable, \(r(0)=1\), \(r^{}(0)=0\), \(r^{}(p)<0\) for any \(p>0\), and \(_{p}r(p)=_{p-}r(p)=0\). In other words, \(r\) is a smooth, symmetric bell-shaped function with the maximum value \(r(0)=1\)._

We note that Eq. (3) can be rewritten as \(p_{t+1}=f_{q_{t}}(p_{t})\) if we define \(r\) by \(r(p)(p)}{p}\) for \(p 0\) and \(r(0) 1\). Below are examples of \(\) for which the corresponding \(r\)'s satisfy Assumption 2.4. These loss functions were previously studied by Ahn et al. (2023) to explain EoS for \((x,y)(xy)\).

* \(\)-\(\) loss: \(_{}(p)((p))\). Note \(^{}_{}(p)=(p)\).
* square-root loss: \(_{}(p)}\). Note \(^{}_{}(p)=}}\).

If \(r\) satisfies Assumption 2.4, then for any \(0<q 1\), there exists a nonnegative number \(p\) such that \(r(p)=q\), and the solution is unique which we denote by \((q)\). In particular, \(:(0,1]_{ 0}\) is a function satisfying \(r((q))=r(-(q))=q\) for any \(q(0,1]\).

**Lemma 2.1** (period-doubling bifurcation of \(f_{q}\)).: _Suppose that \(r\) is a function satisfying Assumption 2.4. Let \(p^{*}=\{p 0(x)}{r(x)}>-1\) for any \(|x| p\}\) and \(c=r(p^{*})\). If \(p^{*}=\), we choose \(c=0\). Then, the one-parameter family of mappings \(f_{q}:\) given by Eq. (4) satisfies_

* _If_ \(q>1\)_,_ \(p=0\) _is the stable fixed point._
* _If_ \(q(c,1)\)_,_ \(p=0\) _is the unstable fixed point and_ \(\{(q)\}\) _is the stable period-_\(2\) _orbit._

Proof.: The map \(f_{q}\) has the unique fixed point \(p=0\) for any \(q>0\). Since \(|f^{}_{q}(0)|=|1-|\), \(p=0\) is a stable fixed point if \(q>1\) and \(p=0\) is an unstable fixed point if \(0<q<1\). Now suppose that \(q(c,1)\). Then, we have \(f_{q}((q))=-(q)\) and \(f_{q}(-(q))=(q)\), which implies that \(\{(q)\}\) is a period-\(2\) orbit of \(f_{q}\). Then, \(|f^{}_{q}((q))|=|f^{}_{q}(-(q))|=|1+(q)r^{}((q))}{q}|<1\) implies that \(\{(q)\}\) is a stable period-\(2\) orbit. 

According to Lemma 2.1, the stability of the fixed point \(p=0\) undergoes a change at \(q=1\), resulting in the emergence of a stable period-\(2\) orbit. The point \((p,q)=(0,1)\) is referred to as the _bifurcation point_, where a _period-doubling_ bifurcation occurs. A _bifurcation diagram_ illustrates the points asymptotically approached by a system as a function of a control parameter. In the case of the map \(f_{q}\), the corresponding bifurcation diagram is represented by \(p=0\) for \(q 1\) and \(p=(q)\) (or equivalently, \(q=r(p)\)) for \(q(c,1)\).

It is worth noting that the period-\(2\) orbit \(\{(p)\}\) becomes unstable for \(q(0,c)\). If we choose \(r\) to be \(r(p)=(p)}{p}\) for \(p 0\) and \(r(0)=1\), then \(1+(p)}{r(p)}=(p)}{r(p)}>0\) for all \(p\), assuming \(\) is convex. Consequently, for \(\)-\(\) loss and square root loss we have \(c=0\), indicating that the period-\(2\) orbit of \(f_{q}\) remains stable for all \(q(0,1)\). However, in Section 5, we will consider \(r\) with \(c>0\), which may lead to additional bifurcations.

Figure 2: **(a), (b)** GD trajectories of two-layer fully-connected linear networks trained with **different initialization scale**\(\). Each color corresponds to a single run of GD. Smaller initialization scale falls into the gradient flow regime, whereas larger initialization falls into the EoS regime. **(c)** In the EoS regime, \(}{r(p_{t})}\) approaches \(1\) in the early phase of training, whereas \(p_{t}\) converges to \(0\) relatively slowly.

## 3 Trajectory Alignment of GD: An Empirical Study

In this section, we conduct experimental studies on the trajectory alignment phenomenon in GD dynamics under the canonical reparameterization proposed in Section 2.

We consider a fully-connected \(L\)-layer neural network \(f(\,\,;):^{d}\) written as

\[f(;)=_{L}^{}(_{L-1}(( _{2}(_{1})))),\]

where \(\) is an activation function, \(_{1}^{m d}\), \(_{l}^{m m}\) for \(2 l L-1\), and \(_{L}^{m}\). All \(L\) layers have the same width of \(m\). We minimize the empirical risk \(()=(f(;)-y)\). We visualize GD trajectories under the canonical parametrization, where each plot shows five different randomly initialized weights using Xavier initialization multiplied with a rescaling factor of \(\). For this analysis, we fix the training data point and hyperparameters as \(=_{1}=(1,0,,0)\), \(y=1\), \(=0.01\), \(d=10\), and focus on the \(\)-\(\) loss for \(\), with either \((t)=t\) (linear) or \((t)=(t)\). We note that the trajectory alignment phenomenon is consistently observed in other settings, including square root loss, different activations (e.g., ELU), and various hyperparameters, in particular for sufficiently wide networks (additional experimental results are provided in Appendix A).

The effect of initialization scale.In Figures 1(a) and 1(b), we examine the effect of the initialization scale \(\) on GD trajectories in a two-layer fully-connected linear network with a width of \(m=256\). In Figure 1(a), when the weights are initialized with a smaller scale (\(=5\)), the initial value of \(q\) is greater than \(1\), and it converges towards the minimum with only a small change in \(q_{t}\) until convergence. In this case, the limiting sharpness is relatively smaller than \(2/\), and the EoS phenomenon does not occur. This case is referred to as the _gradient flow regime_(Ahn et al., 2023). On the other hand, in Figure 1(b), when the weights are initialized with a larger scale (\(=10\)), the initial value of \(q\) is less than \(1\), and we observe convergence towards the point (close to) \((p,q)=(0,1)\). This case is referred to as the _EoS regime_. We note that choosing larger-than-standard scale \(\) is not a necessity for observing EoS; we note that even with \(=1\), we observe the EoS regime when \(\) is larger.

Trajectory alignment on the bifurcation diagram.In order to investigate the trajectory alignment phenomenon on the bifurcation diagram, we plot the bifurcation diagram \(q=r(p)=(p)}{p}\) and observe that GD trajectories tend to align with this curve, which depends solely on \(\). Figure 1(b) clearly demonstrates this alignment phenomenon. Additionally, we analyze the evolution of \(}{r(p_{t})}\) and \(p_{t}\) in Figure 1(c). We observe that the evolution of \(}{r(p_{t})}\) follows two phases. In **Phase I**, \(}{r(p_{t})}\) approaches to \(1\) quickly. In **Phase II**, the ratio remains close to \(1\). Notably, the convergence speed of \(}{r(p_{t})}\) towards \(1\) is much faster than the convergence speed of \(p_{t}\) towards \(0\). In Sections 4 and 5, we will provide a rigorous analysis of this behavior, focusing on the separation between Phase I and Phase II.

The effect of width and depth.In Figure 3, we present the GD trajectories of \(\)-activated networks with different widths and depths (\(=5\)). All three cases belong to the EoS regime, where GD converges to a point close to \((p,q)=(0,1)\), resulting in a limiting sharpness near \(2/\). However, when comparing Figures 2(a) and 2(b), we observe that the trajectory alignment phenomenon is not

Figure 3: GD trajectories of \(\)-activated neural networks with **varying width and depth**. Each color corresponds to a single run of GD. We observe that the wider network (\(m=256\)) exhibits a stronger trajectory alignment phenomenon compared to the narrower network (\(m=64\)). Figure 2(c) depicts the trajectories for a deeper network (\(L=10\)), which also shows the trajectory alignment phenomenon.

observed for the narrower network with \(m=64\), whereas the GD trajectories for the wider network with \(m=256\) are clearly aligned on the bifurcation diagram. This suggests that network width plays a role in the trajectory alignment phenomenon, which is reasonable since wide networks are well approximated by their linearized models, hence Eq. (3) is more accurate. Furthermore, we note that the trajectory alignment phenomenon is also observed for a deeper network with \(L=10\), as depicted in Figure 2(c).

Multiple training data points.In our trajectory alignment analysis, we have primarily focused on training with a single data point. However, it is important to explore the extension of this phenomenon to scenarios with multiple training data points.

To investigate this, we train a neural network on a dataset \(\{(_{i},y_{i})\}_{i=1}^{n}\), where \(_{i}^{d}\) and \(y_{i}\), by minimizing the empirical risk \(()_{i=1}^{n}(f(_{i}; )-y_{i})\). Defining \(^{n d}\) as the data matrix and \(^{n}\) as the label vector, we introduce a _generalized canonical reparameterization_:

\[(p,q)(P(f(;)-),\,^{n}(_{}f(_{i};))^{  2}\|_{2}}),\] (5)

where \(P:^{n}\) can be a function such as a mean value or a specific vector norm.

In Figure 4, we consider training on a \(50\) example subset of CIFAR-\(10\) with only \(2\) classes and vary the network architecture. We use three-layer fully-connected network with \(\) activation and convolutional network (CNN) with \(\) activation. Under the generalized canonical reparameterization (5) for various choices of \(P\), including the mean and the \(_{2}\) norm, we observe the trajectory alignment phenomenon throughout all settings, indicating a common alignment property of the GD trajectories. However, unlike the single data point case, the alignment does not happen on the curve \(q=(p)}{p}\). The precise characterization of the coinciding curve is an interesting direction for future research.

## 4 Trajectory Alignment of GD: A Theoretical Study

In this section, we study a two-layer fully-connected linear network defined as \(f(;)^{}\), where \(^{d m}\), \(^{m}\), and \(\) denote the collection of all parameters \((,)\). We consider training this network with a single data point \(\{(,0)\}\), where \(^{d}\) and \(\|\|_{2}=1\). We run GD with step size \(\) on the empirical risk

\[()(f(;)-0)=(^{ }),\]

Figure 4: **Training on a subset of CIFAR-\(10\). The GD trajectories trained on a \(50\) example subset of CIFAR-\(10\) under the generalized canonical reparameterization (5) with different choices of \(P\) and network architecture. Each color corresponds to a single run of GD. **Top row:** fully-connected neural network with \(\) activation. **Bottom row:** CNN with \(\) activation. Full implementation details and further experimental results are given in Appendix A.4.

where \(\) is a loss function satisfying Assumption 4.1. We note that our assumptions on \(\) is motivated from the single-neuron linear network analysis (\(d=m=1\)) by Ahn et al. (2023).

**Assumption 4.1**.: _The loss \(\) is convex, even, \(1\)-Lipschitz, and twice differentiable with \(^{}(0)=1\)._

The canonical reparameterization (Definition 2.1) of \(=(,)\) is given by

\[(p,q)(^{},\, \|_{2}^{2}+\|\|_{2}^{2})}).\]

Under the canonical reparameterization, the \(1\)-step update rule of GD can be written as

\[p_{t+1}=[1-)}{q_{t}}+^{2}p_{t}^{2}r(p_{t})^{2}]p_ {t}, q_{t+1}=[1-^{2}p_{t}^{2}r(p_{t})(2q_{t}-r(p_{t}))]^{- 1}q_{t},\] (6)

where we define the function \(r\) by \(r(p)(p)}{p}\) for \(p 0\) and \(r(0) 1\). Note that the sequence \((q_{t})_{t=0}^{}\) is monotonically increasing if \(q_{0}\), which is the case our analysis will focus on.

We have an additional assumption on \(\) as below, motivated from Lemma 2.1.

**Assumption 4.2**.: _The function \(r(p)=(p)}{p}\) corresponding to the loss \(\) satisfies Assumption 2.4._

We now present our theoretical results on this setting, and defer the proofs to Appendix B.

### Gradient flow regime

We first consider the gradient flow regime, where \(q\) is initialized with \(q_{0}>1\).

**Theorem 4.1** (gradient flow regime).: _Let \((0,})\) be a fixed step size and \(\) be a loss function satisfying Assumptions 4.1 and 4.2. Suppose that the initialization \((p_{0},q_{0})\) satisfies \(|p_{0}| 1\) and \(q_{0}(,\{, \})\). Consider the GD trajectory characterized in Eq. (6). Then, the GD iterations \((p_{t},q_{t})\) converge to the point \((0,q^{*})\) such that_

\[q_{0} q^{*}(C^{2})q_{0} 2q_{0},\]

_where \(C=8q_{0}[\{-1)}{q_{0}},}\} ]^{-1}>0\)._

Theorem 4.1 implies that in gradient flow regime, GD with initialization \(_{0}=(_{0},_{0})\) and step size \(\) converges to \(^{*}\) which has the sharpness bounded by:

\[(1-C^{2})(\|_{0}\|_{2}^{2}+\|_{0}\|_{2}^{2})_ {}(^{*})(\|_{0}\|_{2}^{2}+\|_{0}\|_{2}^{2}).\]

Hence, for small step size \(\), if the initialization satisfies \(\|_{0}\|_{2}^{2}+\|_{0}\|_{2}^{2}<-1\), then the limiting sharpness is slightly below \(\|_{0}\|_{2}^{2}+\|_{0}\|_{2}^{2}\). Note that we assumed the bound \(|p_{0}| 1\) for simplicity, but our proof also works with the assumption \(|p_{0}| K\) for any positive constant \(K\) modulo some changes in numerical constants. Moreover, our assumption on the upper bound of \(q_{0}\) is \(1/\) up to a constant factor, which covers most realistic choices of initialization.

### EoS regime

We now provide rigorous results in the EoS regime, where the GD trajectory aligns on the bifurcation diagram \(q=r(p)\). To establish these results, we introduce additional assumptions on the loss \(\).

**Assumption 4.3**.: _The function \(r(z)=(z)}{z}\) is \(C^{4}\) on \(\) and satisfies_

1. \(z(z)}{r(z)^{2}}\) _is decreasing on_ \(\)_,_
2. \(z(z)}{r(z)}\) _is decreasing on_ \(z>0\) _and increasing on_ \(z<0\)_,_
3. \(z(z)}\) _is decreasing on_ \(z>0\) _and increasing on_ \(z<0\)_._

We note that both the \(\)-\(\) loss and the square root loss satisfy Assumptions 4.1, 4.2, and 4.3.

**Theorem 4.2** (EoS regime, Phase I).: _Let \(\) be a small enough step size and \(\) be a loss function satisfying Assumptions 4.1, 4.2, and 4.3. Let \(z_{0}_{z}\{(z)}{r(z)}-\}\) and \(c_{0}\{r(z_{0}),\}\). Let \((0,1-c_{0})\) be any given constant. Suppose that the initialization \((p_{0},q_{0})\) satisfies \(|p_{0}| 1\) and \(q_{0}(c_{0},1-)\). Consider the reparameterized GD trajectory characterized in Eq. (6). We assume that for all \(t 0\) such that \(q_{t}<1\), we have \(p_{t} 0\). Then, there exists a time step \(t_{a}=_{,}((^{-1}))\), such that for any \(t t_{a}\),_

\[}{r(p_{t})}=1+h(p_{t})^{2}+_{,}(^{4}),\]

_where \(h(p)-(}{r^{}(p)}+p^{2}r(p)^{2})\) for \(p 0\) and \(h(p)-(0)}\) for \(p=0\)._

One can check that for \(\)-\(\) and square-root losses, the ranges of \(h\) are \((0,3/4]\) and \((0,1/2]\), respectively. Theorem 4.2 implies that in the early phase of training (\(t t_{a}=((^{-1}))\)), GD iterates \((p_{t},q_{t})\) approach closely to the bifurcation diagram \(r(p)=q\), which we called Phase I in Section 3. In Phase II, GD trajectory aligns on this curve in the remaining of the training (\(t t_{a}\)). Theorem 4.3 provides an analysis on Phase II stated as below.

**Theorem 4.3** (EoS regime, Phase II).: _Under the same settings as in Theorem 4.2, there exists a time step \(t_{b}=((1-q_{0})^{-2})\) such that \(q_{t_{b}} 1\) and \(q_{t}>1\) for any \(t>t_{b}\). Moreover, the GD iterates \((p_{t},q_{t})\) converge to the point \((0,q^{*})\) such that_

\[q^{*}=1-}{2r^{}(0)}+_{,}(^ {4}).\]

Theorem 4.3 implies that in EoS regime, GD with step size \(\) converges to \(^{*}\) with sharpness

\[_{}(^{*})=-(0)|}+_{,}(^{3}).\]

Note that Ahn et al. (2023) study the special case \(d=m=1\) and prove that the limiting sharpness is between \(2/-()\) and \(2/\). Theorem 4.3 provides tighter analysis on the limiting sharpness in more general settings, reducing the gap between the upper bound and lower bound to only \((^{3})\). Also, our result is the first to prove that the limiting sharpness in the EoS regime is bounded away from \(2/\) by a nontrivial margin.

We also study the evolution of sharpness along the GD trajectory and prove that progressive sharpening (i.e., sharpness increases) occurs during Phase II.

**Theorem 4.4** (progressive sharpening).: _Under the same setting as in Theorem 4.2, let \(t_{a}\) denote the obtained iteration. Define the function \(:_{>0}\) given by_

\[(q)(1+(q)^{}( (q))}{q})&\\ &\]

_Then, the sequence \(((q_{t}))_{t=0}^{}\) is monotonically increasing. Moreover, for any \(t t_{a}\), the sharpness at GD iterate \(_{t}\) closely follows the sequence \(((q_{t}))_{t=0}^{}\) by satisfying_

\[|_{}(_{t})-(q_{t}) | 1+_{}().\]

The gap between \(_{}(_{t})\) and \((q_{t})\) is bounded by a numerical constant, which becomes negligible compared to \(2/\) for small \(\). In Figure 0(a), we perform numerical experiments on a single-neuron case and observe that \((q_{t})\) closely approximates the sharpness.

Note that Cohen et al. (2021) observe an increase in sharpness during training in the gradient flow regime, while our work reveals that sharpness increases when exhibiting oscillations in the EoS regime. This distinction may be linked to the selection of the loss function, as our study focuses on Lipschitz convex losses, while Cohen et al. (2021) examine the squared loss.

## 5 EoS in Squared Loss: Single-neuron Nonlinear Network

Our canonical reparameterization has a limitation in explaining the EoS phenomenon under squared loss \((p)=p^{2}\), as the function \(r(p)=(p)}{p}=1\) does not satisfy Assumption 2.4. However, empirical studies by Cohen et al. (2021) have observed the EoS phenomenon in GD training with squared loss. In this section, we analyze a simple toy model to gain insight into the EoS phenomenon and trajectory alignment of GD under squared loss.

We study the GD dynamics on a two-dimensional function \((x,y)((x)y)^{2}\), where \(x\), \(y\) are scalars and \(\) is a nonlinear activation satisfying Assumption 5.1 below.

**Assumption 5.1** (sigmoidal activation).: _The activation function \(:\) is odd, increasing, \(1\)-Lipschitz and twice continuously differentiable. Moreover, \((0)=0\), \(^{}(0)=1\), \(_{x}(x)=1\), and \(_{x-}(x)=-1\)._

One good example of \(\) satisfying Assumption 5.1 is \(\). For this section, we use an alternative reparameterization defined as below.

**Definition 5.2**.: _For given step size \(\), the \((p,q)\) reparameterization of \((x,y)^{2}\) is defined as_

\[(p,q)(x,\,}).\]

Under the reparameterization, the \(1\)-step update rule can be written as

\[p_{t+1}=(1-)}{q_{t}})p_{t}, q_{t+1}=(1- (p_{t})^{2})^{-2}q_{t},\] (7)

where the function \(r\) is given by \(r(z)(z)}{z}\) for \(z 0\) and \(r(0) 1\).

We can observe a notable resemblance between Eq. (7) and Eq. (6). Indeed, our theoretical findings for a single-neuron nonlinear network closely mirror those of the two-layer linear network discussed in Section 4. Due to lack of space, we summarize our theorems in this setting as the following:

**Theorem 5.1** (informal).: _Under suitable assumptions on \(\), step size, and initialization, GD trained on the squared loss \((x,y)((x)y)^{2}\) exhibits the same gradient flow, EoS (Phase I, II), and progressive sharpening phenomena as shown in Section 4._

In Theorem C.3, we prove that in the EoS regime, the limiting sharpness is \(-(0)|}+()\). For formal statements of the theorems and the proofs, we refer the reader to Appendix C.

## 6 Conclusion

In this paper, we provide empirical evidence and rigorous analysis to demonstrate the interesting phenomenon of GD trajectory alignment in the EoS regime. Importantly, we show that different GD trajectories, under the canonical reparameterization, align on a bifurcation diagram independent of initialization. This discovery is notable due to the intricate and non-convex nature of neural network optimization, where the algorithm trajectory is heavily influenced by initialization choices. Our theoretical analysis not only characterizes the behavior of limiting sharpness but also establishes progressive sharpening of GD. One immediate future direction is to understand the trajectory alignment behavior when trained on multiple data points. Lastly, it will be interesting to extend our analysis to encompass squared loss for general neural network, going beyond the toy single-neuron example.