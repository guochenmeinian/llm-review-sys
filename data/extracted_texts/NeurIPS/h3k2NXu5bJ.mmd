# Certified Machine Unlearning

via Noisy Stochastic Gradient Descent

 Eli Chien

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

ichien6@gatech.edu

&Haoyu Wang

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

haoyu.wang@gatech.edu

&Ziang Chen

Department of Mathematics

Massachusetts Institute of Technology

Massachusetts, U.S.A.

ziang@mit.edu

&Pan Li

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Georgia, U.S.A.

panli@gatech.edu

###### Abstract

"The right to be forgotten" ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. We propose to leverage projected noisy stochastic gradient descent for unlearning and establish its first approximate unlearning guarantee under the convexity assumption. Our approach exhibits several benefits, including provable complexity saving compared to retraining, and supporting sequential and batch unlearning. Both of these benefits are closely related to our new results on the infinite Wasserstein distance tracking of the adjacent (un)learning processes. Extensive experiments show that our approach achieves a similar utility under the same privacy constraint while using \(2\%\) and \(10\%\) of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively.

## 1 Introduction

Machine learning models usually learn from user data where data privacy has to be respected. Certain laws, such as European Union's General Data Protection Regulation (GDPR), are in place to ensure "the right to be forgotten", which requires corporations to erase all information pertaining to a user if they request to remove their data. It is insufficient to comply with such privacy regulation by only removing user data from the dataset, as machine learning models can memorize training data information and risk information leakage . A naive approach to adhere to this privacy regulation is to retrain the model from scratch after every data removal request. Apparently, this approach is prohibitively expensive in practice for frequent data removal requests and the goal of machine unlearning is to perform efficient model updates so that the resulting model is (approximately)the same as retraining statistically. Various machine unlearning strategies have been proposed, including exact  and approximate approaches . The later approaches allow a slight misalignment between the unlearned model and the retraining one in distribution under a notion similar to Differential Privacy (DP) .

The most popular approach for privatizing machine learning models with DP guarantee is arguably noisy stochastic gradient methods including the celebrated DP-SGD . Mini-batch training is one of its critical components, which not only benefits privacy through the effect of privacy amplification by subsampling  but also provides improved convergence of the underlying optimization process. Several recent works  based on full-batch (noisy) gradient methods may achieve certified approximate unlearning. Unfortunately, their analysis is restricted to the full-batch setting and it is non-trivial to extend these works to the mini-batch setting with tight approximate unlearning guarantees. The main challenge is to incorporate the randomness in the mini-batch sampling into the sensitivity-based analysis  or the Langevin-dynamics-based  analysis.

We aim to study mini-batch noisy gradient methods for certified approximate unlearning. The high-level idea of our unlearning framework is illustrated in Figure 1. Given a training dataset \(\) and a fixed mini-batch sequence \(\), the model first learns and then unlearns given unlearning requests, both via the projected noisy stochastic gradient descent (PNSGD). For sufficient learning epochs, we prove that the law of the PNSGD learning process converges to a _unique_ stationary distribution \(_{|}\) (Theorem 3.1). When an unlearning request arrives, we update \(\) to an adjacent dataset \(^{}\) so that the data point subject to such request is removed. The approximate unlearning problem can then be viewed as moving from the current distribution \(_{|}\) to the target distribution \(_{^{}|}\) until \(\)-close in Renyi divergence for the desired privacy loss \(\)1.

Our key observation is that the results of Altschuler and Talwar , which study the convergence of PNSGD under the (strong) convexity assumption, can be leveraged after we formulate the approximate unlearning as above. They show that the Renyi divergence of two PNSGD processes with the same dataset but different initial distributions decays at a geometric rate, starting from the infinite Wasserstein distance \((W_{})\) of initial distributions (Figure 1, step 3). As a result, if the initial \(W_{}\) distance can be properly characterized, we achieve the corresponding approximate unlearning guarantee by further taking the randomness of the mini-batches \(\) into account (Figure 1, step 4). Therefore, the key step to establish the PNSGD-based unlearning guarantee is to characterize the initial \(W_{}\) distance of the unlearning process tightly.

The projection set diameter \(2R\) for this \(W_{}\) distance adopted in  for DP analysis, unfortunately, leads to a vacuous unlearning guarantee, which cannot illustrate the computational advantage over the retraining from scratch. To alleviate this issue, we perform a careful \(W_{}\) distance tracking analysis along the adjacent PNSGD learning processes (Lemma 3.3), which leads to a much better bound \(W_{}(_{|},_{^{}|}) Z_{} O( M/b)\) (Figure 1, step 2) for bounded gradient norm \(M\), mini-batch size \(b\) and step size \(\). This ultimately leads to our unlearning guarantee \(=O(Z_{}^{2}c^{2Kn/b})\) for \(K\) unlearning epochs and some rate \(c<1\). The computational benefit compared to the retraining from scratch naturally emerges by comparing two \(W_{}\) distances, \(O(R)\) for the retraining from scratch and \(O( M/b)\) for our unlearning framework.

Our approach also naturally extends to multiple unlearning requests, including sequential and batch unlearning settings (Theorem 3.11 and Corollary 1.1), by extending the \(W_{}\)-tracking analysis (Lemma 3.4). Here, we may use the triangle inequality of \(W_{}\) distance, which leads to a tigher privacy loss bound (growing linearly to the number of unlearning requests) than that in the Langevin-dynamics-based analysis  via weak triangle inequality of Renyi divergence (growing exponentially).

Our results highlight the insights into privacy-utility-complexity trade-off regarding the mini-batch size \(b\) for approximate unlearning. A smaller batch size \(b\) leads to a better privacy loss decaying rate \(O(c^{2Kn/b})\). However, an extremely small \(b\) may degrade the model utility and incur instability. It also leads to a worse bound \(Z_{} O( M/b)\), which degrades the computational benefits compared to retraining. We demonstrate such trade-off of our PNSGD unlearning results via experiments against the state-of-the-art full-batch \((b=n)\) gradient-based approximate unlearning solutions . Our analysis provides a significantly better privacy-utility-complexity trade-off even when we restrict ourselves to \(b=n\), and further improves the results by adopting mini-batches. Under the same privacy constraint, our approach achieves similar utility while merely requiring \(10\%,2\%\) of gradient computations compared to baselines for full and mini-batch settings respectively.

### Related Works

**Machine unlearning with privacy guarantees.** The concept of approximate unlearning uses a probabilistic definition of \((,)\)-unlearning motivated by differential privacy , which is studied by [7; 8; 10]. Notably, the unlearning approach of these works involved Hessian inverse computation which can be computationally prohibitive in practice for high dimensional problems. Ullah et al.  focus on exact unlearning via a sophisticated version of noisy SGD. Their analysis is based on total variation stability which is not directly applicable to approximate unlearning settings and different from our analysis focusing on Renyi divergence. Neel et al.  leverage full-batch PGD for (un)learning and achieve approximate unlearning by publishing the final parameters with additive noise. Chien et al.  utilize full-batch PNGD for approximate unlearning with the analysis of Langevin dynamics. The adaptive unlearning requests setting is studied in [6; 17; 18], where the unlearning request may depend on the previous (un)learning results. It is possible to show that our framework is also capable of this adaptive setting since we do not keep any non-private internal states, though we only focus on non-adaptive settings in this work. We left a rigorous discussion for this as future work.

## 2 Preliminaries

We consider the empirical risk minimization (ERM) problem. Let \(=\{_{i}\}_{i=1}^{n}\) be a training dataset with \(n\) data point \(_{i}\) taken from the universe \(\). Let \(f_{}(x)=_{i=1}^{n}f(x;_{i})\) be the objective function that we aim to minimize with learnable parameter \(x_{R}\), where \(_{R}=\{x^{d}\|x\| R\}\) is a closed ball of radius \(R\). We denote \(_{_{B}}:^{d}_{R}\) to be an orthogonal projection to \(_{R}\). The norm \(\|\|\) is standard Euclidean \(_{2}\) norm. \(()\) is denoted as the set of all probability measures over a closed convex set \(\). Standard definitions such as convexity are in Appendix D. We use \(x\) to denote that a random variable \(x\) follows the probability distribution \(\). We say two datasets \(\) and \(^{}\) are adjacent if they "differ" in only one data point. More specifically, we can obtain \(^{}\) from \(\) by _replacing_ one data point. We next introduce a useful idea which we term as Renyi difference.

**Definition 2.1** (Renyi difference).: Let \(>1\). For a pair of probability measures \(,^{}\) with the same support, the \(\) Renyi difference \(d_{}(,^{})\) is defined as \(d_{}(,^{})=(D_{}(||^{}),D_{ }(^{}||)),\) where \(D_{}(||^{})\) is the \(\) Renyi difference defined as \(D_{}(||^{})=(_{x ^{}}(x)}^{}).\)

Figure 1: The overview of PNSGD unlearning. (Left) Proof sketch for PNSGD unlearning guarantees. (Right) PNSGD (un)learning processes on adjacent datasets. Given a mini-batch sequence \(\), the learning process \(\) induces a regular polyhedron where each vertex corresponds to a stationary distribution \(_{|}\) for each dataset \(\). \(_{|}\) and \(_{|^{}}\) are adjacent if \(,^{}\) differ in one data point. We provide an upper bound \(Z_{}\) for the infinite Wasserstein distance \(W_{}(_{|},_{|^{ }})\), which is crucial for non-vacuous unlearning guarantees. Results of  allow us to convert the initial \(W_{}\) bound to Renyi difference bound \(d_{}^{}\), and apply joint convexity of KL divergence to obtain the final privacy loss \(\), which also take the randomness of \(\) into account.

We are ready to introduce the formal definition of differential privacy and unlearning.

**Definition 2.2** (Renyi Differential Privacy (RDP) ).: Let \(>1\). A randomized algorithm \(:^{n}^{d}\) satisfies \((,)\)-RDP if for any adjacent dataset pair \(,^{}^{n}\), the \(\) Renyi difference \(d_{}(,^{})\), where \(()\) and \((^{})\ ^{}\).

It is known to the literature that an \((,)\)-RDP guarantee can be converted to the popular \((,)\)-DP guarantee  relatively tight . As a result, we will focus on establishing results with respect to \(\) Renyi difference (and equivalently \(\) Renyi difference). Next, we introduce our formal definition of unlearning based on \(\) Renyi difference as well.

**Definition 2.3** (Renyi Unlearning (RU)).: Consider a randomized learning algorithm \(:^{n}^{d}\) and a randomized unlearning algorithm \(:^{d}^{n}^{n} ^{d}\). We say \((,)\) achieves \((,)\)-RU if for any \(>1\) and any adjacent datasets \(,^{}\), the \(\) Renyi difference \(d_{}(,^{})\), where \(((),^{})\) and \((^{})^{}\).

Our Definition 2.3 can be converted to the standard \((,)\)-unlearning definition defined in [7; 8; 9], similar to RDP to DP conversion (see Appendix N). Since we work with the replacement definition of dataset adjacency, to unlearn a data point \(_{i}\) we can simply replace it with any data point \(_{i}^{}\) for the updated dataset \(^{}\) in practice. One may also repeat the entire analysis with the objective function being the summation of individual loss for the standard add/remove notion of dataset adjacency. Finally, we will also leverage the infinite Wasserstein distance in our analysis.

**Definition 2.4** (\(W_{}\) distance).: The \(\)-Wasserstein distance between distributions \(\) and \(\) on a Banach space \((^{d},\|\|)\) is defined as \(W_{}(,)=_{(,)}_{( x,y)}\|x-y\|\), where \((x,y)\) means that the essential supremum is taken relative to measure \(\) over \(^{d}^{d}\) parametrized by \((x,y)\). \((,)\) is the collection of couplings of \(\) and \(\).

### Converting Initial \(W_{}\) Distance to Final Renyi Divergence Bound

An important component of our analysis is to leverage the result of , which is based on the celebrated privacy amplification by iteration analysis originally proposed in  and also utilized for DP guarantees of PNSGD in . The goal of  is to analyze the mixing time of the PNSGD process, which can be viewed as the contractive noisy iterations due to the contractiveness of the gradient update under strong convexity assumption.

**Definition 2.5** (Contractive Noisy Iteration (\(c\)-Cln)).: Given an initial distribution \(_{0}(^{d})\), a sequence of (random) \(c\)-contractive (equivalently, \(c\)-Lipschitz) functions \(_{k}:^{d}^{d}\), and a sequence of noise distributions \(_{k}\), we define the \(c\)-Contractive Noisy Iteration (\(c\)-CNI) by the update rule \(X_{k+1}=_{k+1}(X_{k})+W_{k+1}\), where \(W_{k+1}_{k}\) independently and \(X_{0}_{0}\). We denote the law of the final iterate \(X_{K}\) by \(_{c}(_{0},\{_{k}\},\{_{k}\})\).

**Lemma 2.6** (Metric-aware privacy amplification by iteration bound , simplified by  in Proposition 2.10).: _Suppose \(X_{K}_{c}(_{0},\{_{k}\},\{_{k}\})\) and \(X_{K}^{}_{c}(_{0}^{},\{_{k}\},\{_{k}\})\) where the initial distribution satisfy \(W_{}(_{0},_{0}^{}) Z\), the update function \(_{k}\) are \(c\)-contractive, and the noise distributions \(_{k}=(0,^{2}I_{d})\). Then we have_

\[D_{}(X_{K}||X_{K}^{})}{2^{2}} c^{2K}&c<1\\ 1/K&c=1.\] (1)

Roughly speaking, Lemma 2.6 shows that if we have the \(W_{}\) distance of initial distributions of two PNSGD processes on the same dataset, we have the corresponding Renyi difference bound after \(K\) iterations. The projection set diameter \(2R\) is a default upper bound for \(W_{}\) distance if we do not care about the initial distributions as the case studied in  for mixing time analysis. In the unlearning scenario, the initial distributions of the unlearning processes, denoted by \(_{|},_{^{}|}\), are much more relevant. We can show a much tighter bound by analyzing \(W_{}(_{|},_{^{}|})\) along the adjacent PNSGD learning processes.

## 3 Certified Unlearning Guarantee for PNSGD

We start with introducing the (un)learning process with PNSGD with a cyclic mini-batch strategy (Algorithm 1). Note that this mini-batch strategy is not only commonly used for practical DP-SGD implementations in privacy libraries , but also in theoretical analysis for DP guarantees . For the learning mechanism \(\), we optimize the objective function with PNSGD on dataset \(\) (line 3-8 in Algorithm 1). \(,^{2}>0\) are hyperparameters of step size and noise variance respectively. The initialization \(_{0}\) is an arbitrary distribution supported on \(_{R}\) if not specified. For the unlearning mechanism \(\), we fine-tune the current parameter with PNSGD on the updated dataset \(^{}\) subject to the unlearning request (line 10-15 in Algorithm 1) with \(y_{0}^{0}=x_{T}^{0}=()\). For the rest of the paper, we denote \(_{t}^{j},_{k}^{j}\) as the probability density of \(x_{t}^{j},y_{k}^{j}\) respectively. Furthermore, we denote \(=\{^{j}\}_{j=0}^{n/b-1}\) the minibatch sequence described in Algorithm 1, where \(b\) is the step size and we assume \(n\) is divided by \(b\) throughout the paper for simplicity2. We use \(_{|}\) to denote the conditional distribution of \(\). given \(\).

### Certified Unlearning Guarantees

Now we introduce the certified unlearning guarantees for PNSGD and the corresponding analysis illustrated in Figure 1. We first prove that for any fixed mini-batch sequence \(\), the limiting distribution \(_{|}\) of the learning process exists, is unique, and stationery. The proof is deferred to Appendix E and is based on applying the results in  to establish the ergodicity of the learning process \(x_{t}^{0}\).

**Theorem 3.1**.: _Suppose that the closed convex set \(^{d}\) is bounded with \(\) having a positive Lebesgue measure and that \( f(;_{i}):^{d}\) is continuous for all \(i[n]\). The Markov chain \(\{x_{t}:=x_{t}^{0}\}\) in Algorithm 1 for any fixed mini-batch sequence \(\) admits a unique invariant probability measure \(_{|}\) on the Borel \(\)-algebra of \(\). Furthermore, for any \(x\), the distribution of \(x_{t}\) conditioned on \(x_{0}=x\) converges weakly to \(_{|}\) as \(t\), where \(_{|}\) is the conditional distribution of \(_{}\) given \(\)._

Suppose the training epoch \(T\) is large enough so that the model is well-trained for now, which means \(()_{|}=_{0|}^ {0}\) and our target "retraining distribution" is \(_{^{}|}\). Our goal is then to upper bound the Renyi difference \(d_{}(_{K|}^{0},_{^{}|})\) after \(K\) unlearning epochs. In the case of insufficient training, the privacy loss is \(d_{}(_{K|}^{0},_{|}^{0,})\) where \((^{})_{T|}^{0,}\) for \(T\) training epochs. It can be upper bounded in terms of \(d_{}(_{K|}^{0},_{^{}|})\) and \(d_{}(_{T|}^{0,},_{^{}|})\) via weak triangle inequality ofRenyi divergence, which is provided in Theorem 3.2 below. We later provide a better bound by considering the randomness of \(\).

**Theorem 3.2** (RU guarantee of PNSGD unlearning, fixed \(\)).: _Assume \(\), \(f(x;)\) is \(L\)-smooth, \(M\)-Lipchitz and \(m\)-strongly convex in \(x\). Let the learning and unlearning processes follow Algorithm 1 with \(y_{0}^{0}=x_{T}^{0}=()\). Given any fixed mini-batch sequence \(\), for any \(>1\), let \(\), the output of the \(K^{th}\) unlearning iteration satisfies \((,)\)-RU for any adjacent dataset \(,^{}\), where_

\[(_{1}(2)+ _{2}(2)),_{1}()= }{2^{2}}c^{2Tn/b},\ _{2}()=}^{2}}{2^{2}}c^{2 Kn/b},\]

\[Z_{}=W_{}(_{K|}^{0},_{^{ }|}) 2Rc^{Tn/b}+(}{1-c^{n/b}},2R),\]

_and \(c=1- m\)._

As we explained earlier, we need non-trivial \(W_{}\) bounds for adjacent PNSGD processes to obtain better unlearning guarantees when applying Lemma 2.6. We provide such results below and the proofs are deferred to Appendix L and M respectively.

**Lemma 3.3** (\(W_{}\) between adjacent PNSGD learning processes).: _Consider the learning process in Algorithm 1 on adjacent datasets \(\) and \(^{}\) and a fixed mini-batch sequence \(\). Assume \(\), \(f(x;)\) is \(L\)-smooth, \(M\)-Lipschitz and \(m\)-strongly convex in \(x\). Let the index of different data point between \(,^{}\) belongs to mini-batch \(^{j_{0}}\). Then for \(\) and let \(c=(1- m)\), we have_

\[W_{}(_{|}^{0},_{|}^{0,})(}{1-c^{n/b}}c^{n/b-j_{0}-1},2R).\]

**Lemma 3.4** (\(W_{}\) between PNSGD learning process to its stationary distribution).: _Following the same setting as in Theorem 3.2 and denote the initial distribution of the unlearning process as \(_{0}^{0}\). Then we have_

\[W_{}(_{|}^{0},_{|}) (1- m)^{Tn/b}W_{}(_{0}^{0},_{|}).\]

_Remark 3.5_.: In , the authors used the default projection set diameter \(2R\) as the \(W_{}\) distance upper bound for their DP results, see equations (3.5) and (5.2) therein. However, it yields a vacuous bound as an unlearning guarantee compared to retraining from scratch. Note that our tighter bound for \(W_{}\) distance is also useful for deriving later sequential unlearning guarantees compared to the prior work based on the analysis of Langevein dynamics . Interestingly, this improved result can also be utilized for tightening the DP guarantee in  and make it more practically useful, as \(2R\) can be very large in practice, which may be of independent interest.

We are ready to provide the sketch of proof for Theorem 3.2.

_Sketch of proof._ First note that the PNSGD update leads to a \((1- m)\)-CNI process when \( 1/L\) for any mini-batch sequence. Recall that \(y_{k}^{j}\) is the unlearning process at epoch \(k\) at iteration \(j\), starting from \(y_{0}^{0}=x_{T}^{0}=()\). Consider the "adjacent" process \(y_{k}^{j,}\) starting from \(y_{0}^{0,}=x_{T}^{0,}=(^{})\) but still fine-tune on \(^{}\) so that \(y_{k}^{j},y_{k}^{j,}\) only differ in their initialization. Now, consider three distributions: \(_{|},_{|}^{0},_{K| }^{0}\) are the stationary distribution for the learning processes, learning process at epoch \(T\) and unlearning process at epoch \(K\) respectively. Similarly, consider the "adjacent" processes that learn on \(^{}\) and still unlearn on \(^{}\) (see Figure 1 for the illustration). Denote distributions \(_{^{}|},_{|}^{0, },_{K|}^{0,}\) for these processes similarly. Note that our goal is to bound \(d_{}(_{K|}^{0},_{T|}^{0,})\) for the RU guarantee. By weak triangle inequality , we can upper bound it in terms of \(d_{2}(_{K|}^{0},_{^{}|})\) and \(d_{2}(_{^{}|},_{|} ^{0,})\), which are the \(_{2}\) and \(_{1}\) terms in Theorem 3.2 respectively. For \(d_{}(_{^{}|},_{T|}^{0, })\), we leverage the naive \(2R\) bound for the \(W_{}\) distance between \(_{^{}|},_{0|}^{0,}\) and applying Lemma 2.6 leads to the desired result. For \(d_{}(_{K|}^{0},_{^{}|})\), by triangle inequality of \(W_{}\) and Lemma 3.3, 3.4 one can show that the \(W_{}\) between \(_{0|}^{0},_{^{}|}\) is bounded by \(Z_{}\) in Theorem 3.2. Further applying Lemma 2.6 again completes the proof.

_Remark 3.6_.: Our proof only relies on the bounded gradient difference \(\| f(x;)- f(x;^{})\| 2M\)\( x^{d}\) and \(,^{}\) hence \(M\)-Lipchitz assumption can be replaced. In practice, we can leverage the gradient clipping along with a \(_{2}\) regularization for the convex objective function .

**The convergent case.** In practice, one often requires the model to be "well-trained", where a similar assumption is made in the prior unlearning literature . Under this assumption, we can further simplify Theorem 3.2 into the following corollary.

**Corollary 3.7**.: _Under the same setting as of Theorem 3.2. When we additionally assume \(T\) is sufficiently large so that \(y_{0}^{0}=M()_{|}\). Then for any \(>1\) and \(\), the output of the \(K^{th}\) unlearning iteration satisfies \((,)\)-RU with \(c=1- m\), where_

\[}^{2}}{2^{2}}c^{2Kn/b},  Z_{}=(},2R).\]

For simplicity, the rest of the discussion on our PNSGD unlearning will based on the well-trained assumption. From Corollary 3.7 one can observe that a smaller \(b\) leads to a better decaying rate (\(c^{2Kn/b}\)) but also a potentially worse initial distance \(Z_{}=O(1/((1-c^{n/b})b))\). In general, choosing a smaller \(b\) still leads to less epoch for achieving the desired privacy loss. In practice, choosing \(b\) too small (e.g., \(b=1\)) can not only degrade the utility but also incur instability (i.e., large variance) of the convergent distribution \(_{|}\), as \(_{|}\) depends on the design of mini-batches \(\). One should choose a moderate \(b\) to balance between privacy and utility, which is the unique privacy-utility-complexity trade-off with respect to \(b\) revealed by our analysis.

**Computational benefit compared to retraining.** In the view of Corollary 3.7 or Lemma 2.6, it is not hard to see that a smaller initial \(W_{}\) distance leads to fewer PNSGD (un)learning epochs for being \(\)-close to a target distribution \(_{^{}|}\) in terms of Renyi difference \(d_{}\). For PNSGD unlearning, we have provided a uniform upper bound \(Z_{}=O( M/((1-c^{n/b})b))\) of such initial \(W_{}\) distance in Lemma 3.3. On the other hand, even if both \(_{0},_{}\) are both Gaussian with identical various and mean difference norm of \((1)\), we have \(W_{}(_{0},_{^{}|})=(1)\) for retraining from scratch. Our results show that a larger mini-batch size \(b\) leads to more significant complexity savings compared to retraining. As we discussed above, one should choose a moderate size \(b\) to balance between privacy and utility. In our experiment, we show that for commonly used mini-batch sizes (i.e., \(b 32\)), our PNSGD unlearning is still much more efficient in complexity compared to retraining.

**Improved bound with randomized \(\).** So far our results are based on a fixed (worst-case) mini-batch sequence \(\). One can improve the privacy bound in Corollary 3.7 by taking the randomness of \(\) into account under a non-adaptive unlearning setting. That is, the unlearning request is _independent_ of the mini-batch sequence \(\). See also our discussion in the related work. By taking the average of the bound in Corollary 3.7 in conjunction with an application of joint convexity of KL divergence , we can derive an improved guarantee beyond the worst-case of \(\).

**Corollary 3.8**.: _Under the same setting as of Theorem 3.2 but with random mini-batch sequences described in Algorithm 1. Then for any \(>1\), and \(\), the output of the \(K^{th}\) unlearning iteration satisfies \((,)\)-RU with \(c=1- m\), where \((_{}( }^{2}}{2^{2}}c^{2Kn/b}))\) and \(Z_{}\) is the bound described in Lemma 3.3._

**Different mini-batch sampling strategies.** We remark that our analysis can be extended to other mini-batch sampling strategies, such as sampling without replacement for each iteration. However, this strategy leads to a worse \(Z_{}\) in our analysis of Lemma 3.3, which may seem counter-intuitive at first glance. This is due to the nature of the essential supremum taken in \(W_{}\). Although sampling without replacement leads to a smaller probability of sampling the index that gets modified due to the unlearning request, it is still non-zero for each iteration. Thus the worst-case difference \(2 M/b\) between two adjacent learning processes in the mini-batch gradient update occurs at each iteration, which degrades the factor \(1/(1-c^{n/b})\) to \(1/(1-c)\) in Lemma 3.3. As a result, we choose to adopt the cyclic mini-batch strategy so that such a difference is guaranteed to occur only once per epoch and thus a better bound on \(W_{}\).

**Discussion on utility bound.** One can leverage the utility analysis in section 5 of  to derive the utility guarantee for the full batch setting \(b=n\). We relegate the proof to Appendix K.

**Proposition 3.9**.: _Under the same setting as Corollary 3.7 with \(b=n\), \(}\) and assume the minimizer of any \(f_{}\) is in the relative interior of \(_{R}^{d}\), for any given adjacent dataset pair \(,^{}\) the output of the \(K^{th}\) unlearning iteration \(y_{K}^{0}\) satisfies_

\[[f_{^{}}(y_{K}^{0})-_{x_{R}}f_{ ^{}}(x)] Mc^{K}(,2R)+ }{m}.\] (2)Note that a similar analysis applies to both the mini-batch (i.e., \(b n\)) and non-convergent case (i.e., Theorem 3.2) but the result is more complicated. We leave the rigorous analysis as future work. An important remark is that the second term \(}{m}\) is the excess risk of \(_{^{}}\), which is controlled by the noise scale \(\). This presents the privacy-utility trade-off as demonstrated in the DP scenario in .

**Without strong convexity.** Since Lemma 2.6 also applies to the convex-only case (i.e., \(m=0\) so that \(c=1\)), repeating the same analysis leads to the following extension.

**Corollary 3.10**.: _Under the same setting as of Theorem 3.2 but without strong convexity (i.e., \(m=0\)). When we additionally assume \(T\) is sufficiently large so that \(y_{0}^{0}=M()_{|}\). Then for any \(>1\), and \(\), the output of the \(K^{th}\) unlearning iteration satisfies \((,)\)-RU, where_

\[}^{2}}{2^{2}},Z_{}=(,2R).\]

There are several remarks for Corollary 3.10. First, the privacy loss now only decays linearly instead of exponentially as opposed to the strongly convex case. Second, \(Z_{}\) now can grow linearly in training epoch \(T\). As a result, the computational benefit of our approach compared to retraining may vanish for large \(T\) such that \( 2R\). Nevertheless, the computational benefit against retraining persists for moderate \(T\) such that \(<2R\). This condition can be met if the model learns reasonably well with moderate \(T\) and the projection diameter \(2R\) is not set to be extremely small. For example, with \(2R=10,b=128,=1\) and \(M=1\), any training epoch \(T<640\) will lead to \(<2R\). Still, we conjecture a better analysis is needed beyond strong convexity.

### Unlearning Multiple Data Points

So far we have focused on one unlearning request and unlearning one point. In practice, multiple unlearning requests can arrive sequentially (sequential unlearning) and each unlearning request may require unlearning multiple points (batch unlearning). Below we demonstrate that our PNSGD unlearning naturally supports sequential and batch unlearning as well.

**Sequential unlearning.** As long as we can characterize the initial \(W_{}\) distance for any mini-batch sequences, we have the corresponding \((,)\)-RU guarantee due to Corollary 3.7. Thanks to our geometric view of the unlearning problem (Figure 2) and \(W_{}\) is indeed a metric, applying triangle inequality naturally leads to an upper bound on the initial \(W_{}\) distance. By combining Lemma 3.3 and Lemma 3.4, we have the following sequential unlearning guarantee.

**Theorem 3.11** (\(W_{}\) bound for sequential unlearning).: _Under the same assumptions as in Corollary 3.7. Assume the unlearning requests arrive sequentially such that our dataset changes from \(=_{0}_{1} _{S}\), where \(_{s},_{s+1}\) are adjacent. Let \(y_{k}^{j,(s)}\) be the unlearned parameters for the \(s^{th}\) unlearning request at \(k^{th}\) unlearning epoch and \(j^{th}\) iteration following Algorithm (1) on \(_{s}\) and \(y_{0}^{0,(s+1)}=y_{K_{s}}^{0,(s)}_{_{s}|}\), where \(y_{0}^{0,(0)}=x_{}\) and \(K_{s}\) is the unlearning steps for the \(s^{th}\) unlearning request. For any \(s[S]\), we have \(W_{}(_{_{s-1}|},_{_{s}| }) Z_{}^{(s)},\) where \(Z_{}^{(s+1)}=(c^{K_{s}n/b}Z_{}^{(s)}+Z_{},2R)\), \(Z_{}^{(1)}=Z_{}\), \(Z_{}\) and \(c\) are described in Corollary 3.7._

By combining Corollary 3.7 and Theorem 3.11, we can establish the least unlearning iterations of each unlearning request \(\{K_{s}\}_{s=1}^{S}\) to achieve \((,)\)-RU simultaneously. Notably, our sequential unlearning bound is much better than the one in , especially when the number of unlearning requests is large. The key difference is that  have to leverage weak triangle inequality for Renyi divergence, which _double_ the Renyi divergence order \(\) for each sequential unlearning request. In contrast, since our analysis only requires tracking the initial

Figure 2: Illustration of (a) sequential unlearning and (b) batch unlearning. The key idea is to establish an upper bound on the initial \(W_{}\) distance. (a) For sequential unlearning, the initial \(W_{}\) distance bound \(Z_{}^{(s)}\) for each \(s^{th}\) unlearning request can be derived with triangle inequality. (b) For batch unlearning, we analyze the case that two learning processes can differ in \(S 1\) points.

\(W_{}\) distance, where the standard triangle inequality can be applied. As a result, our analysis can better handle the sequential unlearning case. We also demonstrate in Section 4 that the benefit offered by our results is significant in practice.

**Batch unlearning.** We can extend Lemma 3.3 to the case that adjacent dataset \(,^{}\) can differ in \(S 1\) points, which further leads to batch unlearning guarantee. We relegate the result in Appendix J.

## 4 Experiments

_Benchmark datasets._ We consider binary logistic regression with \(_{2}\) regularization. We conduct experiments on MNIST  and CIFAR10 , which contain 11,982 and 10,000 training instances respectively. We follow the setting of  to distinguish digits \(3\) and \(8\) for MNIST so that the problem is a binary classification. For the CIFAR10 dataset, we distinguish labels \(3\) (cat) and \(8\) (ship) and leverage the last layer of the public ResNet18  embedding as the data features, which follows the setting of  with public feature extractor.

_Baseline methods._ Our baseline methods include Delete-to-Descent (D2D)  and Langevin Unlearning (LU) , which are the state-of-the-art full-batch gradient-based approximate unlearning methods. Note that when our PNSGD unlearning chooses \(b=n\) (i.e., full batch), the learning and unlearning iterations become PNGD which is identical to LU. Nevertheless, the corresponding privacy bound is still different as we leverage the analysis different from those based on Langevin dynamics in . Hence, we still treat these two methods differently in our experiment. For D2D, we leverage Theorem 9 and 28 in  for privacy accounting depending on whether we allow D2D to have an internal non-private state. Note that allowing an internal non-private state provides a weaker notion of privacy guarantee  and both PNSGD and LU by default do not require it. We include those theorems for D2D and a detailed explanation of its possible non-privacy internal state in Appendix O. For LU, we leverage their Theorem 3.2, and 3.3 for privacy accounting , which are included in Appendix P.

All experimental details can be found in Appendix N, including how to convert \((,)\)-RU to the standard \((,)\)-unlearning guarantee. Our code is publicly available3. We choose \(=1/n\) for each dataset and require all tested unlearning approaches to achieve \((,)\)-unlearning with different \(\). We report test accuracy for all experiments as the utility metric. We set the learning iteration \(T=10,20,50,1000\) to ensure PNSGD converges for mini-batch size \(b=32,128,512,n\) respectively. All results are averaged over \(100\) independent trials with standard deviation reported as shades in figures.

Figure 3: Main experiments, where the top and bottom rows are for MNIST and CIFAR10 respectively. (a) Compare to baseline for unlearning one point using limited \(K\) unlearning epoch. For PNSGD, we use only \(K=1\) unlearning epoch. For D2D, we allow it to use \(K=1,5\) unlearning epochs. (b) Unlearning \(100\) points sequentially versus baseline. For LU, since their unlearning complexity only stays in a reasonable range when combined with batch unlearning of size \(S\) sufficiently large, we report such a result only. (c,d) Noise-accuracy-complexity trade-off of PNSGD for unlearning \(100\) points sequentially with various mini-batch sizes \(b\), where all methods achieve \((,1/n)\)-unlearning guarantee with \(=0.01\). We also report the required accumulated epochs for retraining for each \(b\).

**Unlearning one data point with \(K=1\) epoch.** We first consider the setting of unlearning one data point using only one unlearning epoch (Figure 2(a)). For both LU and PNSGD, we use only \(K=1\) unlearning epoch. Since D2D cannot achieve a privacy guarantee with only limited (i.e., less than \(10\)) unlearning epoch without a non-private internal state, we allow D2D to have it and set \(K=1,5\) in this experiment. Even in this case, PNSGD still outperforms D2D in both utility and unlearning complexity. Compared to LU, our mini-batch setting either outperforms or is on par with it. Interestingly, we find that LU gives a better privacy bound compared to full-batch PNSGD (\(b=n\)) and thus achieves better utility under the same privacy constraint, see Appendix P for the detailed comparisons. Nevertheless, due to the use of weak triangle inequality in LU analysis, we will see that our PNSGD can outperform LU significantly for multiple unlearning requests.

**Unlearning multiple data points.** Let us consider the case of multiple (\(100\)) unlearning requests (Figure 2(b)). We let all methods achieve the same \((1,1/n)\)-unlearning guarantee for a fair comparison. We do not allow D2D to have an internal non-private state anymore in this experiment for a fair comparison. Since the privacy bound of LU only gives reasonable unlearning complexity with a limited number of sequential unlearning updates , we allow it to unlearn \(S=10\) points at once. We observe that PNSGD requires roughly \(10\%\) and \(2\%\) of unlearning epochs compared to D2D and LU for \(b=n\) and \(b=128\) respectively, where all methods exhibit similar utility (\(0.9\) and \(0.98\) for MNIST and CIFAR10 respectively). It shows that PNSGD is much more efficient compared to D2D and LU. Notably, while both PNSGD with \(b=n\) and LU (unlearn with PNGD iterations, the resulting privacy bound based on our PABI-based analysis is superior to the one pertaining to Langevin-dynamic-based analysis in . See our discussion in Section 3.2 for the full details.

**Privacy-utility-complexity trade-off.** We now investigate the inherent utility-complexity trade-off regarding noise standard deviation \(\) and mini-batch size \(b\) for PNSGD under the same privacy constraint, where we require all methods to achieve \((0.01,1/n)\)-unlearning guarantee for \(100\) sequential unlearning requests (Figure 2(c) and 2(d)). We can see that smaller \(\) leads to a better utility, yet more unlearning epochs are needed for PNSGD to achieve \(=0.01\). On the other hand, smaller mini-batch size \(b\) requires fewer unlearning epochs \(K\) as shown in Figure 2(d), since more unlearning iterations are performed per epoch. Nevertheless, we remark that choosing \(b\) too small may lead to degradation of model utility or instability. Decreasing the mini-batch size \(b\) from \(32\) to \(1\) reduces the average accuracy of training from scratch from \(0.87\) to \(0.64\) and \(0.97\) to \(0.81\) on MNIST and CIFAR10 respectively for \(=0.03\). In practice, one should choose a moderate mini-batch size \(b\) to ensure both good model utility and unlearning complexity. Finally, we also note that PNSGD achieves a similar utility while much better complexity compared to retraining from scratch, where PNSGD requires at most \(1,5\) unlearning epochs per unlearning request for \(b=32,512\) respectively.

## 5 Limitations and Conclusion

**Limitation.** Since our analysis is built on the works of , we share the same limitation that the (strong) convexity assumption is required. It is an open problem on how to extend such analysis beyond convexity assumption as stated in . While we resolve this open problem for establishing DP properties of PNSGD in our recent work , it is still unclear whether the same success can be generalized to the unlearning problem. One interesting direction is to leverage the Langevin dynamic analysis  instead as in , which can deal with non-convex problems in theory yet we conjecture the resulting bounds can be loose, and more complicated.

**Conclusion.** We propose to leverage projected noisy stochastic gradient descent (PNSGD) for machine unlearning problem. We provide its unlearning guarantees as well as many other algorithmic benefits of PNSGD for unlearning under the convexity assumption. Our results are closely related to our new results on infinite Wasserstein distance tracking of the adjacent (un)learning processes, which is also leveraged in our concurrent work for studying DP-PageRank algorithms . Extensive experiments show that our approach achieves a similar utility under the same privacy constraint while using \(2\%\) and \(10\%\) of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively.