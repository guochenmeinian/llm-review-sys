# Learning Discrete Concepts in Latent Hierarchical Models

Lingjing Kong\({}^{1}\), Guangyi Chen\({}^{1,2}\), Biwei Huang\({}^{3}\), Eric P. Xing\({}^{1,2}\), Yuejie Chi\({}^{1}\), and Kun Zhang\({}^{1,2}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Mohamed bin Zayed University of Artificial Intelligence

\({}^{3}\)University of California San Diego

###### Abstract

Learning concepts from natural high-dimensional data (e.g., images) holds potential in building human-aligned and interpretable machine learning models. Despite its encouraging prospect, formalization and theoretical insights into this crucial task are still lacking. In this work, we formalize concepts as discrete latent causal variables that are related via a hierarchical causal model that encodes different abstraction levels of concepts embedded in high-dimensional data (e.g., a dog breed and its eye shapes in natural images). We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible. Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables, which is well-suited for unstructured data modalities such as images. We substantiate our theoretical claims with synthetic data experiments. Further, we discuss our theory's implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights.

## 1 Introduction

Learning semantic discrete concepts from unstructured high-dimensional data, such as images and text, is crucial to building machine learning models with interpretability, transferability, and compositionality, as empirically demonstrated by extensive existing work . Despite these empirical successes, limited work is devoted to the theoretical front: the notions of concepts and their relations are often heuristically defined. For example, concept bottleneck models  use human-specified annotations and recent methods  employ pretrained multimodal models like CLIP  to explain features with neural language. This lack of rigorous characterization impedes a deeper understanding of this task and the development of principled learning algorithms.

In natural images, the degree/extent of certain attributes (e.g., position, lighting) is often presented in a continuous form and main concepts of practical concern are often discrete in nature (e.g., object classes and shapes). Moreover, these concepts are often statistically dependent, with the dependence potentially resulting from some higher-level concepts. For example, the correlation between a specific dog's eye features and fur features may arise from a high-level concept for breeds (Figure 1). Similarly, even higher-level concepts may exist and induce dependence between high-level concepts, giving rise to a hierarchical model that characterizes all discrete concepts at different abstraction levels underlying high-dimensional data distributions. In this work, we focus on concepts that can be defined as discrete latent variables and related via a hierarchical model. Under this formalization, the query on the recoverability of concepts and their relations from unstructured high-dimensional distribution (e.g., images) amounts to the following causal identification problem:Identification theory for latent hierarchical causal models has been a topic of sustained interest. Recent work [18; 19; 20] investigates identification conditions of latent hierarchical structures under the assumption that the latent variables are continuous and influence each other through linear functions. The linearity assumption fails to handle the general nonlinear influences among discrete variables. Another line of work focuses on discrete latent models. Pearl , Choi et al.  study latent trees with discrete observed variables. The tree structure can be over-simplified to capture the complex interactions among concepts from distinct abstract levels (e.g., multiple high-level concepts can jointly influence a lower-level one). Gu and Dunson  assume that binary latent variables can be exactly grouped into levels and causal edges often appear between adjacent levels, which can also be restrictive. Moreover, these papers assume observed variables are discrete, falling short of modeling the continuous distribution like images as the observed variables. Similar to our goal, Kivva et al.  show the discrete latent variables adjacent to the potentially continuous observed variables can be identified. However, their theory assumes the absence of higher-level latent variables and thus cannot handle latent hierarchical structures.

In this work, we show identification guarantees for the discrete hierarchical model under mild conditions on the generating function and causal structures. Specifically, we first show that when continuous observed variables (i.e., the leaves of the hierarchy) preserve the information of their adjacent discrete latent variables (i.e., direct parents in the graph), we can extract the discrete information from the continuous observations and further identify each discrete variable up to permutation indeterminacy. Given these "low-level" discrete latent variables, we establish graphical conditions to identify the discrete hierarchical model that fully explains the statistical dependence among the identified "low-level" discrete latent variables. Our conditions permit multiple paths within latent variable pairs and flexible locations of latent variables, encompassing a large family of graph structures including as special cases non-hierarchical structures , trees [21; 22; 25; 26] and multi-level directed acyclic graphs (DAGs) [23; 27] (see example graphs in Figure 2). Taken together, our work establishes theoretical results for identifying the discrete latent hierarchical model governing high-dimensional continuous observed variables, which to the best of our knowledge is the first effort in this direction. We corroborate our theoretical results with synthetic data experiments.

As an implication of our theorems, we discuss a novel interpretation of the state-of-the-art latent diffusion (LD) models  through the lens of a hierarchical concept model. We interpret the denoising objective at different noise levels as estimating latent concept embeddings at corresponding hierarchical levels in the causal model, where a higher noise level corresponds to high-level concepts. This perspective explains and unifies these seemingly orthogonal threads of empirical insights and gives rise to insights for potential empirical improvements. We deduce several insights from our theoretical results and verify them empirically. In summary, our main contributions are as follows.

* We formalize the framework of learning concepts from high-dimensional data as a latent-variable identification problem, capturing concepts at different abstraction levels and their interactions.
* We present identification theories for the discrete latent hierarchical model. To the best of our knowledge, our result is the first to address discrete latent hierarchical model beyond trees [21; 26] and multi-level DAGs  while capable of handling high-dimensional observed variables.
* We provide an interpretation of latent diffusion models as hierarchical concept learners. We supply empirical results to illustrate our interpretation and showcase its potential benefits in practice.

## 2 Related Work

**Concept learning.** In recent years, a significant strand of research has focused on employing labeled data to learn concepts in generative models' latent space for image editing and manipulation [1; 2; 3; 4; 5; 6]. Concurrently, another independent research trajectory has been exploring unsupervised concept discovery and its potential to learn more compositional and transferable models [7; 8; 9; 10; 11]. Concurrently, a plethora of work has been dedicated to extracting interpretable concepts from high-dimensional data such as images. Concept-bottleneck  first predicts a set of human-annotated concepts as an intermediate stage and then predicts the task labels from these intermediate concepts. This paradigm has attracted a large amount of follow-up work [13; 29; 30; 31; 32; 33]. A recent surge of pre-trained multimodal models (e.g., CLIP ) can explain the image concepts through text directly [14; 15; 16].

**Latent variable identification.** Complex real-world data distributions often possess a hierarchical structure among their underlying latent variables. The identification conditions of latent hierarchical structures are investigated under the assumption that the latent variables are continuous and influence each other through linear functions  and nonlinear functions . In addition, prior work  studies fully discrete cases and thus falls short of modeling the continuous observed variables like images. To identify latent variables under nonlinear transformations, a line of work  assumes the availability of auxiliary information (e.g., domain/class labels) and that the latent variables' probability density functions have sufficiently different derivatives over domains/classes. Another line of studies  refrains from the auxiliary information by assuming sparsity and mechanistic independence, disregarding causal structures among the latent variables.

Please refer to Section A1 for more extensive related work and discussion.

## 3 Discrete Hierarchical Models

**Data-generating process.** We formulate the data-generating process as the following latent-variable model. Let \(\) denote the continuous observed variables \(:=[x_{1},,x_{n}]^{d_{x}}\) which represents the high-dimension data we work with in practice (e.g., images). 1 Let \(:=[d_{1},,d_{n_{d}}]\) be discrete latent variables that are direct parents to \(\) (as shown in Figure 1(b)) and take on values from finite sets, i.e., \(d_{i}_{i}^{(d)}\) for all \(i[d_{i}]\) and \(2|_{i}^{(d)}|<\). We denote the joint domain as \(^{(d)}:=_{1}^{(d)}_{d}^{(d)}\). These discrete variables are potentially related to each other causally (e.g., \(d_{4}\) and \(d_{5}\) in Figure 1(c)) or via higher-level latent variables (e.g., \(d_{1}\) and \(d_{2}\) in Figure 1(c)). Let \(:=[c_{1},,c_{n_{e}}]^{d_{e}}\) be continuous latent variables that represent the continuous information conveyed in observed variables \(\). The generating process is defined in Equation 1 and illustrated in Figure 1(a).

\[:=g(,), \]

where we denote the generating function with \(g:[,]\). We denote the resultant bipartite graph from \([,]\) to \(\) as \(\). In this context of image generation, the discrete subspace \(\) gives a description of concepts present in the image \(\) (e.g., a dog's appearance, background objects), and the continuous subspace \(\) controls extents/degrees of specific attributes (e.g., sizes, lighting, and angles).

**Discrete hierarchical models.** As discussed above, discrete variables \(d_{1},,d_{n_{d}}\) represent distinct concepts that may be dependent either causally or purely statistically via higher-level concepts, as visualized in Figure 1(c). For instance, the dog's eye features and nose features are dependent, which a higher-level concept "breeds" could explain. We denote such higher-level latent discrete variables as \(:=[z_{1},,z_{n_{s}}]\), where \(z_{i}_{i}^{(z)}\) for all \(i[z_{i}]\) and \(2|_{i}^{(z)}|<\) and \(^{(z)}:=_{1}^{(z)}_{n_{s}}^{(z)}\). Graphically, these variables \(\) are not directly adjacent to observed variables \(\) (Figure 1(c)). High-level discrete variables \(\) may constitute a hierarchical structure until the dependence in the system is fully explained. Since the discrete variables encode major semantic concepts in the data, this work primarily concerns discrete variables \(\) and its underlying causal structure. The continuous subspace \(\) can be viewed as exogenous variables and is often omitted in the causal graph (e.g., Figure 1(b)). We leave identifying continuous attributes in \(\) as future work.

Figure 1: **Latent hierarchical graphs.** The dashed circle in (a) indicates that the continuous variable \(\) can be viewed as an exogenous variable. Dashed edges in (b) indicate potential statistical dependence.

Given this, we define the discrete hierarchical model as follows. The discrete hierarchical model (Figure 1(c)) \(:=(,)\) is a DAG that comprises discrete latent variables \(d_{1},,d_{n_{d}},z_{1},,z_{n_{s}}\). We denote that directed edge set with \(\) and the collection of all variables with \(:=\{,\}\), where \(\) and \(\) are vectors \(\) and \(\) in a set form and all leaf variables in \(\) belong to \(\). We assume the distribution over all variables \(\) respects the Markov property with respect to the graph \(\). We denote all parents and children of a variable with \(()\) and \(()\) respectively and define the neighbors as \(():=()()\). We say a variable set \(\) are pure children of \(\), iff \(_{}()=_{A_{i}}_{ }(A_{i})=\) and \(=\). As shown in Figure 1(c), \(x_{1}\) is a pure child of \(d_{1}\).

**Objectives.** Formally, given only the observed distribution \(p()\), we aim to:

1. identify discrete variables \(\) and the bipartite graph \(\);
2. identify the hierarchical causal structure \(\).

## 4 Identification of Discrete Latent Hierarchical Models

We present our theoretical results on the identifiability of discrete latent variables \(\) and the bipartite graph \(\) in Section 4.2 (i.e., Objective 1) and the hierarchical model \(\) in Section 4.3 (i.e., Objective 2).

**Additional notations.** We denote the set containing components of \(\) with \(\), the set of all variables with \(^{*}:=\), the entire edge set with \(^{*}:=\), and the entire causal model with \(^{*}:=(^{*},^{*})\). As the true generating process involves \(\), \(\), \(g\), \(\), and \(\) (defined in Section 3), we define their statistical estimates with \(}\), \(}\), \(\), and \(\) through maximum likelihood estimation over the full population \(p()\) while respecting conditions on the true generating process. We use \(|()|\) for the cardinality of a discrete variable set \(\)'s support (all joint states) and \(_{,}\) for the joint probability table whose two dimensions are the states of discrete variable sets \(\) and \(\) respectively.

### General Conditions for Discrete Latent Models

It is well known that causal structures cannot be identified without proper assumptions. For instance, one may merge two adjacent discrete variables \(d_{1}_{1}^{(d)}\) and \(d_{2}_{2}^{(d)}\) into a single variable \(_{1}^{(d)}_{2}^{(d)}\) while preserving the observed distribution \(p()\). We introduce the following basic conditions on the discrete latent model to eliminate such ill-posed situations.

**Condition 4.1** (General Latent Model Conditions).:
1. _[Non-degeneracy]:_ \((=k_{1},=k_{2})>0\)_, for all_ \((k_{1},k_{2})^{(d)}^{(z)}\)_; for all variable_ \(v^{*}\)_,_ \((v|(v)=k_{1})(v|(v )=k_{2})\) _if_ \(k_{1} k_{2}\)_._
2. _[No-twins]: Distinct latent variables have distinct neighbors_ \((v_{1})(v_{2})\)_, if_ \(v_{1} v_{2}\)_._
3. _[Maximality]: There is no DAG_ \(}^{*}:=(}^{*},}^{*})\) _resulting from splitting a latent variable in_ \(^{*}\) _(i.e., turning_ \(z_{i}\) _into_ \(_{i,1}\) _and_ \(_{i,2}\) _with identical neighbors and cardinality_ \(|_{i}^{z}|=|_{i,1}^{z}|+|_{i,2}^{z}|\) _), such that_ \((}^{*})\) _is Markov w.r.t._ \(}^{*}\) _and_ \(}^{*}\) _satisfies ii._

**Discussion.** Condition 4.1 is a necessary set of conditions for identifying latent discrete models, which is employed and discussed extensively [24; 41]. Intuitively, Condition 4.1-i excludes dummy discrete states and graph edges that exert no influence on the observed variables \(\). Condition 4.1-ii,iii constrain the latent model to be the most informative graph without introducing redundant latent variables, thus forbidding arbitrary merging and splitting over latent variables.

### Discrete Component Identification

We show with access to only the observed data \(\), we can identify each discrete component \(d_{i}\) up to permutation indeterminacy (Definition 4.2) and a corresponding bipartite graph equivalent to \(\).

**Definition 4.2** (Component-wise Identifiability).: Variables \(^{n_{d}}\) and \(}^{n_{d}}\) are identified component-wise if there exists a permutation \(\), such that \(_{i}=h_{i}(d_{(i)})\) with invertible function \(h_{i}\). That is, our estimation \(_{i}\) captures full information of \(d_{(i)}\) and no information from \(d_{j}\) such that \(j(i)\). 2 The permutation is a fundamental indeterminacy for disentanglement [37; 38; 36; 24].

**Remarks on the problem.** A large body of prior work  requires continuous or even differentiable density function over all latent variables and domain/class labels or counterfactual counterparts to generate variation. Thus, their techniques do not transfer naturally to our latent space with both continuous and discrete parts \([,]\) and no supervision of any form. With a similar goal, Kivva et al.  assumes access to an oracle (Definition A2.1) to the mixture distribution over \(p()\), which is not directly available in the general case here. Kivva et al.  assumes a specific parametric generating process, whereas we focus on a generic non-parametric generative model (Equation 1).

**High-level description of our proposed approach.** We decompose the problem into two tractable subproblems: 1) extracting the global discrete state \(\) from the mixing with the continuous variable \(\); 2) further identifying each discrete component \(d_{i}\) from the mixing with other discrete components \(d_{j}\) (\(i j\)) and the causal graph \(\). For 1), we show that, perhaps surprisingly, minimal conditions on the generating function \(g\) suffice to remove the information of \(\) and thus identify the global state of \(\). For 2), we observe that the identification results in 1) can be viewed as a mixture oracle over \(p()\), which enables us to employ techniques from Kivva et al.  to solve the problem.

We introduce key conditions and formal theoretical statements as follows.

**Condition 4.3** (Discrete Components Identification).:
1. _[Connected Spaces] The continuous support_ \(^{n_{c}}\) _is closed and connected._
2. _[Invertibility & Continuity]: The generating function_ \(g\) _in equation_ 1 _is invertible, and for any fixed_ \(\)_,_ \(g(,)\) _and its inverse are continuous._
3. _[Non-Subset Observed Children]: For any pair_ \(d_{i}\) _and_ \(d_{j}\)_, one's observed children are not the subset of the other's,_ \(_{}(d_{i})_{}(d_{j})\)_._

**Discussion on the conditions.** Condition 4.3-i requires the continuous support \(\) to be regular in contrast with the discrete variable's support. Intuitively, the continuous variable \(\) often controls the extents/degrees of specific attributes (e.g., sizes, lighting, and angles) and takes values from connected spaces. For instance, "lightning" ranges from the lowest to the highest intensity continuously. Condition 4.3-ii ensures the generating process preserves latent variables' information . Thanks to the high dimensionality, images often have adequate capacity to meet this condition. For instance, the image of a dog contains a detailed description of the dog's breed, shape, color, lighting intensity, and angles, all of which are decodable from the image. Condition 4.3-iii ensures that each latent component should exhibit sufficiently distinguishable influences on the observed variable \(\). Practically, this condition indicates that the lowest-level concepts influence diverse parts of the image. These concepts are often atomic, such as a dog's ear, eyes, or even finer, which often don't overlap. This condition is adopted in prior work  and related to the notation of sparsity. Along this line, prior work  assumes pure observed children for each discrete variable, which is strictly stronger. Recent work  assumes each latent variable is connected to a unique set of observed variables. This condition implies Condition 4.3-iii because if \(z_{0}\)'s children form a subset of \(z_{1}\)'s children, then one cannot find a subset of observed variables whose parent is \(z_{0}\) alone.

**Theorem 4.4** (Discrete Component Identification).: _Under the generating process in Equation 1 and Condition 4.3-ii, the estimated discrete variable \(}\) and the true discrete variable \(\) are equivalent up to an invertible function, i.e., \(}=h()\) with \(h()\) invertible. Moreover, if Condition 4.1 and Condition 4.3-iii further hold, we attain component-wise identifiability (Definition 4.2) and the bipartite graph \(\) up to permutation of component indices._

**Proof sketch.** Intuitively, each state of the discrete subspace \(\) indexes a manifold \(g(,):\) that maps the continuous subspace \(\) to the observed variable \(\). These manifolds do not intersect in the observed variable space \(\) regardless of however close they may be to each other, thanks to the invertibility of the generating function \(g\) (Condition 4.3-ii). This leaves a sufficient footprint in \(\) for us to uniquely identify the manifold it resides in, giving rise to the identifiability of \(\). This reveals the discrete state of each realization of \(\) and equivalently the joint distribution \(p(,)\) where we merge all components in \(\) into a discrete variable \(\). Identifying this joint distribution enables the application of tensor decomposition techniques  to disentangle the global state \(\) into individual discrete components \(d_{i}\) and the causal graph \(\), under Condition 4.1 and Condition 4.3-iii.

### Hierarchical Model Identification

We show that we can identify the underlying hierarchical causal structure \(\) that explains the dependence among low-level discrete components \(d_{i}\) that we identify in Theorem 4.4.

**Remarks on the problem.** Benefiting from the identified discrete components in Theorem 4.4, we employ \(\) as observed variables to identify the discrete latent hierarchical model \(\). Although discrete latent hierarchical models have been under investigation for an extensive period, existing results mostly assume relatively strong graphical conditions - the causal structures are either trees [26; 21; 22] or multi-level DAGs [23; 48], which can be restrictive in capturing the complex interactions among latent variables among different hierarchical levels. Separately, recent work [19; 20] has exhibited more flexible graphical conditions for linear, continuous latent hierarchical models. For instance, prior work  allows for multiple directed paths of disparate edge numbers within a variable pair and potential non-leaf observed variables. Unfortunately, their techniques hinge on linearity and cannot directly apply to discrete models of high nonlinearity.

**High-level description of our approach.** The central machinery in prior work [19; 20] is Theorem A2 , which builds a connection between easily computable statistical quantities (i.e., sub-covariance matrix ranks) and local latent graph information. Dong et al.  utilize a graph search algorithm to piece together these local latent graph structures to identify the entire hierarchical model. Ideally, if we can access these local latent structures in the discrete model, we can apply the same graph search procedure and theorems to identify the discrete model. Nevertheless, Theorem A2 relies on linearity (i.e., each causal edge represents a linear function), which doesn't hold in the discrete case. We show that interestingly, Theorem A2 can find a counterpart in the discrete case (Theorem 4.8), despite the absence of linearity. Since given the graphical information from Theorem A2, the theory in Dong et al.  is independent of statistical properties, we can utilize flexible conditions and algorithm therein by obtaining the same graphical information with Theorem 4.8.

To present Theorem 4.8, we introduce non-negative rank \(_{+}()\) (Definition 4.5), and t-separation  (Definition 4.7) as follows.

**Definition 4.5** (Non-negative Rank).: The non-negative rank of non-negative \(A_{+}^{m n}\) is equal to the smallest \(p\) for which there exist \(_{+}^{m p}\) and \(_{+}^{p n}\) such that \(=\).

**Definition 4.6** (Treks).: A trek \(T_{i,j}\) in a DAG from vertex \(i\) to \(j\) consists of a directed path \(P_{ki}\) from \(k\) to \(i\) and a direct path \(P_{kj}\) from \(k\) to \(j\), where we refer to \(P_{ki}\) as the \(i\) side and \(P_{kj}\) as the \(j\) side.

**Definition 4.7** (T-separation).: Let \(\), \(\), \(}\), and \(}\) be subsets (not necessarily disjoint) of vertices in a DAG. Then \((},})\) t-separates \(\) and \(\) if every trek from \(\) to \(\) passes through either a vertex in \(}\) on the \(\) side of the trek or a vertex \(}\) on the \(\) side of the trek.

Intuitively, a trek is a path containing at most one fork structure and no collider structures. It is known that one can formulate d-separation as a special form of t-separation (see Theorem A1). Thus, t-separation is at least as informative as d-separation. As detailed in Dong et al. , t-separation can provide more information when latent variables are involved, benefiting from Theorem A2 .

**Theorem 4.8** (Implication of Rank Information on Latent Discrete Graphs).: _Given two sets of variables \(\) and \(\) from a non-degenerate, faithful (Condition 4.1-i, Condition 4.10-i) discrete model \(\), it follows that \(_{+}(})=\{|()|:(_{1},_{2})\}\)._

**Example.** Suppose every variable in Figure 2(a) is binary, then for \(=\{d_{1},d_{2},d_{3}\}\), \(=\{d_{3},,d_{8}\}\), \(_{+}(})=4\) since \(\) and \(\) are t-separated by \(\{d_{3},z_{4}\}\) with \(4\) states.

Figure 2: **Graphical comparison. Tree Structures permit one undirected path between any two variables. Multi-level DAGs require partitioning variables into levels with edges only between adjacent levels. Our conditions allow multiple paths between variables across levels and include non-leaf observed variables.**

**Discussion.** Parallel to Theorem A2  for linear models, Theorem 4.8 acts as an oracle to reveal the minimal t-separation set's cardinality between any two variable sets in _discrete_ models beyond linearity. This enables us to infer the latent graph structure from only the observed variables' statistical information. To the best of our knowledge, Theorem 4.8 is the first to establish this connection and can be of independent interest for learning latent discrete models in future work. Although the computation of non-negative ranks can be expensive , existing work  demonstrates that regular rank tests are decent substitutes, we observe in our synthetic data experiments (Section 5).

We present the identification conditions for discrete models as follows (Condition 4.10).

**Definition 4.9** (Atomic Covers).: Let \(\) be a set of variables in \(\) with \(|()|=k\), where \(t\) of the \(k\) states belong to observed variables \(d_{i}\), and the remaining \(k-t\) are from latent variables \(z_{j}\). \(\) is an atomic cover if \(\) contains a single observed variable, or if the following conditions hold:

1. There exists a set of atomic covers \(\), with \(|()| k+1-t\), such that \(_{}_{}( )\) and \(},},}}=\).
2. There exists a set of covers \(\), with \(|()| k+1-t\), such that every element in \(_{}\) is a neighbour of \(\) and \((_{})(_{})=\).
3. There does not exist a partition of \(=}}\) such that both \(}\) and \(}\) are atomic covers.

**Example.** In Figure 2 (c), \(\{z_{2}\}\) is an atomic cover if its pure child \(\{d_{2}\}\) and its neighbors \(\{z_{1},z_{3},z_{4}\}\) possess more than \((z_{2})+1\) states separately. Otherwise, \(\{z_{2},d_{1}\}\) can be an atomic cover if (some of) pure children \(\{z_{3},z_{4}\}\) and neighbors \(\{z_{1},d_{2},d_{3}\}\) possess \((z_{2})+1\) states separately.

**Condition 4.10** (Discrete Hierarchical Model Conditions).:
1. _[Faithfulness] All the conditional independence relations are entailed by the DAG._
2. _[Basic Graphical Conditions] Each latent variable_ \(z\) _corresponds to a unique atomic cover in_ \(\) _and no_ \(z\) _is involved in any triangle structure (i.e., three mutually adjacent variables)._
3. _[Graphical Condition on Colliders] In a latent graph_ \(\)_, if (i) there exists a set of variables_ \(\) _such that every variable in_ \(\) _is a collider of two atomic covers_ \(}\)_,_ \(}\)_, and denote by_ \(\) _the minimal set of variables that_ \(d\)_-separates_ \(}\) _from_ \(}\)_,_ (ii) _there is a latent variable in_ \(},},\) _or_ \(\)_, then we must have_ \(|()|+|()||(})|+|(})|\)_._

**Discussion on the conditions.** Condition 4.10-i is known as the faithfulness condition widely adopted for causal discovery , which attributes statistical independence to graph structures rather than unlikely coincidence . In linear models, Dong et al.  introduce atomic covers (Definition 4.9) to represent a group of indistinguishable variables. In the discrete case, an atomic cover consists of indistinguishable latent states, which we merge into a single latent discrete variable (Condition 4.1-ii). Intuitively, we treat each state as a separate variable and merge those belonging to the same atomic cover at the end of the identification procedure. This handles discrete variables of arbitrary state numbers, in contrast with the binary or identical support assumptions , which we use as an alternative condition in Theorem A12. Condition 4.10-ii requires each atomic cover to possess sufficiently many children and neighbors to preserve its influence while avoiding problematic triangle structures to ensure the uniqueness of its influence. In contrast, existing work  assumes at least three pure children for each latent variable, amounting to six times more states. Condition 4.10-iii ensures adequate side information (large \(||\)) to discover latent colliders \(\), admitting graphs more general than tree structures  (i.e., no colliders). Overall, our model encompasses a rich class of latent structures more complex than tree structures and multi-level DAGs  (Figure 2).

Following Dong et al. , we introduce the minimal-graph operator \(_{}\) (Definition 4.11 and Figure A1), which merges certain redundancy structures that rank information cannot distinguish.

**Definition 4.11** (Minimal-graph Operator ).: We can merge atomic covers \(\) into \(\) in \(\) if (i) \(\) is a pure child of \(\), (ii) all elements of \(L\) and \(P\) are latent and \(|()|=|()|\), and (iii) the pure children of \(\) form a single atomic cover, or the siblings of \(\) form a single atomic cover. We denote such an operator as the minimal-graph operator \(_{}()\).

**Theorem 4.12** (Discrete Hierarchical Identification).: _Suppose the causal model \(\) satisfies Condition 4.1 and Condition 4.10 We can identify \(\) up to the Markov equivalence class of \(_{}()\)._

**Proof sketch.** As discussed above, Theorem 4.8 gives a graph structure oracle equivalent to Theorem A2, which we leverage to prove Theorem 4.12. Besides the rank test, the major distinctionbetween Theorem A2 and Theorem 4.8 is that the former returns the number of variables in the minimal t-separation set whereas the latter returns the number of states. Applying the search algorithm from Dong et al.  alongside our rank test from Theorem 4.8 to a discrete model \(\) results in a graph \(}\). In \(}\), each latent variable \(z\) in \(\) is split into a set of variables \(^{(1)},,^{(|(z)|)}\) as an atomic cover, with the set size equal to the state number of \(z\). We can then reconstruct the original graph \(\) from \(}\) by merging these atomic covers into discrete variables. We present our algorithm in Algorithm 1 and highlight the differences from that in Dong et al. .

Our techniques can also utilize the identical support condition (e.g., binary latent variables) [23; 22] for identification under slightly different conditions. We present the results in Theorem A12.

## 5 Synthetic Data Experiments

**Experimental setup.** We generate the hierarchical model \(\) with randomly sampled parameters, and follow  to build the generating process from \(\) to the observed variables \(\) (i.e., graph \(\)) by a Gaussian mixture model. The graphs are exhibited in Figure A2 and Figure A3 in Appendix A4. We follow Dong et al.  to use F1 score for evaluation. More details can be found in Appendix A4.

**Results and discussion.** We choose Kivva et al.  as our baseline because it is the only method we know designed to learn a non-parametric, discrete latent model from continuous observations. We evaluate both methods on graphs in Figure A2. As shown in Table 1 and Table 2, our method consistently achieves near-perfect scores, while the baseline, despite correctly identifying \(\) and directing edges among \(\) components, cannot handle higher-level latent variables.

To verify Theorem 4.8, we evaluate Algorithm 1 and a baseline  on graphs satisfying the conditions on \(\) (i.e., purely discrete models in Figure A3). Our method performs well on graphs that meet conditions of Theorem A12 and achieves decent scores on graphs that do not (Figure A3 (c) and (e)). The significant margins over the baseline validate Theorem 4.8 and Theorem A12.

## 6 Interpretations of Latent Diffusion

In this section, we present a novel interpretation of latent diffusion (LD)  from the perspective of our hierarchical concept learning framework. Concretely, the diffusion training objective can be viewed as performing denoising autoencoding at different noise levels [56; 57]. Denoising autoencoders [58; 59] and variants [60; 61] have shown the capability of extracting high-level, semantic representations as their encoder output. In the following, we adopt this perspective to interpret the diffusion model's representation (i.e., the UNet encoder output) through our hierarchical model, which connects the noise level and the hierarchical level of the latent representation in our causal model. For brevity, we refer to the diffusion model encoder's output as diffusion representation.

**Discrete variables and representation embeddings.** In practice, discrete variables are often modeled as embedding vectors from a finite dictionary (e.g., wording embeddings). Therefore, although diffusion representation is not discrete, we can interpret it as an ensemble of embeddings of involved discrete variables. Park et al.  empirically demonstrates that one can indeed decompose the diffusion representation into a finite set of basis vectors that carry distinct semantic information, which can be viewed as the concept embedding vectors.

**Vector-quantization.** Given an image \(\), LD first discretizes it with a vector-quantization generative adversarial network (VQ-GAN) :\(=f_{}().\) Through the lens of our framework, VQ-GAN represents the image with a rich but finite set of embeddings of bottom-level concepts \(\) and discards nuances in the continuous representation \(\), inverting the generation process in Equation 1.

   & Graph 1 & Graph 2 & Graph 3 & Graph 4 & Graph 5 & Graph 6 & Graph 7 & Graph 8 & Graph 9 \\  Baseline & \(0.67 0.0\) & \(0.69 0.1\) & \(0.67 0.0\) & \(0.67 0.2\) & \(0.63 0.0\) & \(0.65 0.0\) & \(0.67 0.0\) & \(0.65 0.0\) & \(0.63 0.0\) \\  Ours & \(0.94 0.1\) & \(0.98 0.1\) & \(0.94 0.0\) & \(0.98 0.2\) & \(0.94 0.1\) & \(0.93 0.0\) & \(0.93 0.1\) & \(0.90 0.0\) & \(0.93 0.1\) \\  

Table 1: **F1 scores for our method and the baseline Kivva et al. **. Figure A2 exhibits the graphs.

   & Graph 1 & Graph 2 & Graph 3 & Graph 4 & Graph 5 & Graph 6 & Graph 7 & Graph 8 & Graph 9 \\  Baseline & \(0.24 0.3\) & \(0.48 0.0\) & \(0.33 0.2\) & \(0.63 0.1\) & \(0.0 0.0\) & \(0.55 0.1\) & \(0.0 0.0\) \\  Ours & \(1.0 0.0\) & \(1.0 0.0\) & \(0.73 0.0\) & \(0.73 0.0\) & \(0.75 0.0\) & \(0.95 0.0\) & \(1.0 0.0\) \\  

Table 2: **F1 scores for our method and the baseline Dong et al. **. Figure A3 exhibits the graphs.

**Denoising objectives.** As discussed, diffusion training can be viewed as denoising the corrupted embedding \(}\) to restore noiseless \(\)[57; 58; 59; 64] for a designated denoising model \(f_{t}\) at noise level \(t\):

\[*{arg\,max}_{f_{t}}_{},}_{t}_{t}(}_{t}|)}[_{f_{t }}(|}_{t},)], \]

where \(\) denotes the text prompt. Under this objective, the model is supposed to compress the noisy view \(}_{t}\) to extract a clean, high-level representation, together with additional information from the text \(\), to reconstruct the original embedding \(\). Formally, the denoising model \(f_{t}:=f_{,t} f_{,t}\) performs auto-encoding \(_{(t)}=f_{,t}(}_{t})\) and \(}=f_{,t}(_{(t)},)\), where we use \((t)\) to indicate the dependence on the noise level \(t\). We can view the compressed representation \(_{(t)}\) as a set of high-level latent variables in the hierarchical model: _the encoder \(f_{,t}\) maps the noisy view \(}_{t}\) to high-level latent variables \(_{_{t}}\) and the decoder \(f_{,t}\) assimilates the text information \(\) and reconstructs the original view \(\)_. In practice, \(f_{t}\) is implemented as a single model (e.g., UNets) paired with time embeddings. We visualize this process in Figure 3.

**Noise levels and hierarchical levels.** Intuitively, the noise level controls the amount of semantic information remaining in \(}_{t}\). For instance, a high noise level \(t\) drowns the bulk of the low-level concepts in \(\), leaving only sparse high-level concepts in \(}_{t}\). In this case, the diffusion representation \(_{(t)}\) estimates a high concept level in the hierarchical model. In Figure 3, a high noise level may destroy low-level concepts, such as the sand texture and the waveforms, while preserving high-level concepts, such as the beach and the sunrise. In Section 7.1, we follow Park et al.  to demonstrate diffusion representation's semantic levels under different noise levels.

**Theory and practice.** We connect LD training and estimating latent variables in the hierarchical model in an intuitive sense. Our theory focuses on the fundamental conditions of the data-generating process and does not directly translate to guarantees for LD. That said, our conditions naturally have implications on the algorithm design. For instance, a sparsity constraint on the decoding model may facilitate the identification condition that variables influence each other sparsely (e.g., pure children in Condition 4.10). In Section 7.3, we show such constraints are beneficial for concept extraction. We hope that our new perspective can provide more novel insights into advancing practical algorithms.

## 7 Real-world Experiments

### Discovering Hierarchical Concept Structures from Diffusion Models

In Figure 4, we extract concepts and their relationships from LD through our hierarchical model interpretation. Our recovery involves two stages: determining the concept level and identifying causal links. We add a textual concept, like "dog", into the prompt and identify the latest diffusion step that would render this concept properly. If "dog" appears in the image only when added at step 0 and "eye" appears when added from step 5, it indicates that "dog" is a higher-level concept than "eyes". After determining the levels of concepts, we intervene on a high-level concept and observe changes in low-level ones. No significant changes indicate no direct causal relationship. We explore the relationships among the concepts "dog", "tree", "eyes", "ears", "branch", and "leaf". Figure 4 presents the final recovered graph and intermediate results. See Section A5.3 for more investigation.

### Diffusion Representation as Concept Embeddings

We support our interpretations in Section 6 that diffusion representation can be viewed as concept embeddings, and it corresponds to high-level concepts for high noise levels. Following Park et al. , we modify the diffusion representation along certain directions found unsupervisedly. We can observe that this manipulation gives rise to semantic concept changes rather than entangled corruption Figure 5. Editing the latent representation at early steps corresponds to shifting global concepts.

Figure 3: **Diffusion models estimate the latent hierarchical model.** Different noise levels correspond to different concept levels. To avoid cluttering, we leave out vector quantization.

In Figure 5, the latent representation in earlier steps (step \(T\)) determines breeds (the top row), species (the middle row), and gender (the bottom row). In contrast, the latent representation in later steps (step \(0.6T\)) correlates with the dog collar, cat eyes, and shirt patterns. Implementation details and additional results are provided in Appendix A.5.

### Causal Sparsity for Concept Extraction

Recent work  shows that concepts can be extracted as low-rank parameter subspaces of LD models via LoRA . This low-rankness limits the complexity of text-induced changes, resembling sparse influences from latent concepts to their descendants. Our theory suggests that different levels of concepts may require varying sparsity levels to capture. We present empirical evidence in Section A.5.4. Motivated by this, we design an adaptive sparsity selection mechanism for capturing concepts at different levels. Inspired by Ding et al. , we implement a sparsity constraint on the LoRA dimensionality for the model to select the LoRA rank at each module automatically, benefiting concept extraction (see Appendix A.5.4).

## 8 Conclusion

In this work, we cast the task of learning concepts as the identification problem of a discrete latent hierarchical model. Our theory provides conditions to guarantee the recoverability of discrete concepts. **Limitations:** Although our theoretical framework provides a lens for interpretation, our conditions do not directly guarantee diffusion's success, which would require nontrivial assumptions. Also, Algorithm 1 can be expensive for large graphs due to the dependency on the state count. We leave giving guarantees to diffusion models and efficient graph learning algorithms as future work.

Figure 4: **Recovering concepts and their relationships from LD.****(a)** The final recovered concept graph among concepts “dog”, “tree”, “eyes”, “ears”, “branch”, and “leaf”. **(b)** Identifying causal links through “interventions”. For example, we compare two prompts that vary in “dog”: “a dog with wide eyes and a wilting tree with short branches, in a cartoon style” and “a big dog with wide eyes and a wilting tree with short branches, in a cartoon style”. We observe significant changes in “eyes” but not in “branch”, indicating a causal link between “dog” and “eyes” but not between “dog” and “branch”. **(c)** Identifying concept levels by the last effective diffusion step. For example, we use the base prompt “a tree with long branches, in a cartoon style” and prepend “dog” at steps 0, 5, and 15. Only injecting “dog” at step 0 works. Similarly, injecting “wide eyes” works at both steps 0 and 5, indicating that “dog” is a higher-level concept than “eyes”.

Figure 5: **Semantic latent space.** We modify the diffusion model’s representation (UNet encoder’s output) along principal directions at steps \(T\) and \(0.6T\). Structure changes indicate the semantics of the representation and manipulation at the early time \(T\) induces global shifts. See more examples in Figure A9.

**Acknowledgments.** We thank the anonymous reviewers for their valuable insights and recommendations, which have greatly improved our work. The work of L. Kong and Y. Chi is supported in part by NSF DMS-2134080. This material is based upon work supported by NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quiris AI, and Florin Court Capital. This research has been graciously funded by the National Science Foundation (NSF) CNS2414087, NSF BCS2040381, NSF IIS2123952, NSF IIS1955532, NSF IIS2123952; NSF IIS2311990; the National Institutes of Health (NIH) R01GM140467; the National Geospatial Intelligence Agency (NGA) HM04762010002; the Semiconductor Research Corporation (SRC) AIHW award 2024AH3210; the National Institute of General Medical Sciences (NIGMS) R01GM140467; and the Defense Advanced Research Projects Agency (DARPA) ECOLE HR00112390063. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the National Institutes of Health, the National Geospatial Intelligence Agency, the Semiconductor Research Corporation, the National Institute of General Medical Sciences, and the Defense Advanced Research Projects Agency.