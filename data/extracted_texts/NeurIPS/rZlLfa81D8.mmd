# Instruction

WonderBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks

 Michael Wornow Avanika Narayan

Ben Viggiano Ishan S. Khare Tathagat Verma

Tibor Thompson Miguel Angel Fuentes Hernandez Sudharsan Sundar

Chloe Trujillo Krrish Chawla Rongfei Lu Justin Shen

Divya Nagaraj Joshua Martinez Vardhan Agrawal Althea Hudson

Nigam H. Shah Christopher Re Stanford University

###### Abstract

Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task - full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today - simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present \(\)WonderBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < \(0.3\)). We hope \(\)WonderBREAD encourages the development of more "human-centered" AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: \(\)https://github.com/HazyResearch/wonderbread.

## 1 Introduction

Multimodal foundation models (FMs) such as GPT-4  have the potential to revolutionize **business process management (BPM)**, which is the discipline of measuring and improving enterprise workflows - e.g. a physician submitting a medication order. Typical BPM projects progress in four stages across the following _BPM tasks_: (1) _Documentation_ - mapping the steps of an existing workflow; (2) _Knowledge Transfer_ - ensuring a shared understanding of the documented workflow; (3) _Improvement_ - identifying workflow inefficiencies and proposing fixes; and (4) _Automation_ - writing software to execute the workflow without human involvement . Please see Appendix Section A.7 for a concrete example. FMs could be well-suited for these tasks due to their robust reasoning  and visual  understanding skills.

**However, existing ML benchmarks  focus almost exclusively on one BPM task: end-to-end workflow automation** using agents based on multimodal FMs (see Table 1). This is despite the fact that **simply defining the relevant workflow takes 60% of the time** of the typical BPM project , and the BPM market is 4x larger than that of automation tools .

By **ignoring the most time-consuming aspects of BPM projects,** we overlook key opportunities to provide near-term value to enterprises. Several case studies have applied multimodal FMs to these broader BPM tasks and demonstrated better performance, easier set-up, and simpler maintenance than traditional BPM tools such as process mining . While promising, however, these papers were largely anecdotal with small datasets (\(<50\) examples). **This motivates the creation of a large-scale benchmark and dataset specifically for BPM tasks.**

Unfortunately, no such dataset exists, and **current benchmarks designed around workflow automation cannot be readily repurposed** due to several limitations. First, their datasets either lack human demonstrations of workflows  or do not contain sufficient annotation detail for BPM tasks  - e.g. evaluating a model's ability to document a workflow requires reference documentation. Second, their evaluations typically only measure end-to-end workflow completion rates  and thus do not consider the intermediate reasoning required for BPM tasks such as identifying inefficiencies within a successfully completed workflow. Third, they do not model real-world BPM use cases and instead focus on navigating websites or mobile apps - i.e. they are focused on workflow _execution_ rather than _understanding_.

Motivated by the overlooked potential for using multimodal FMs on a broader suite of BPM tasks, we thus introduce **WORDERBREAD**, a **WOR**flow **uNDER**standing **B**enchma**R**k, **E**v**A**luation harness, and **D**ataset. Our contributions are as follows:

1. **Dataset:** We publish **2928 human demonstrations across 598 previously unannotated workflows** sourced from the WebArena benchmark . Each workflow has an average of 4.9 independently collected demonstrations, and each demonstration contains a full screen recording, event log of all clicks/keystrokes/scrolls, and a manually written standard operating procedure ("SOP") - i.e. a step-by-step written guide which reflects the annotator's reasoning at each step of the workflow. For a subset of 162 workflows, we also have annotators rank all 5 demonstrations in order of perceived quality. On average, each workflow takes 7.8 steps and 37.2 seconds. We provide a detailed example of the data in our benchmark in Appendix Figure 8.
2. **Tasks:** Based on use cases drawn from the BPM literature around (1) Documentation, (2) Knowledge Transfer, and (3) Improvement, we define **6 novel BPM tasks** which require reasoning over multimodal data. 1. **Documentation:** Generate standard operating procedures (i.e. synthesize the steps of a workflow in writing) to fulfill quality control and audit requirements .

Figure 1: The three components of WONDERBREAD. (1) We curate 2928 human demonstrations across 598 web navigation tasks. Each demonstration includes an intent, a full screen recording, an action trace, and a written guide (SOP) describing the steps taken in the demonstration. (2) We create 6 BPM tasks that measure a model’s ability to generate accurate documentation, assist in knowledge transfer, and improve workflows. (3) We provide automated evaluation pipelines for all tasks. See Appendix Figure 8 for a detailed example of the data included with each demonstration.

2. **Knowledge Transfer:** Answer user queries about how workflows operate to simplify onboarding and reduce the 5.3 hours per week that knowledge workers spend waiting for information from colleagues.. 3. **Improvement:** Analyze workflows to identify inefficiencies and correct execution errors [20; 55].
3. **Evaluation:** We offer evaluation pipelines using automated metrics (e.g., F1, accuracy) and LLM-based evaluators with high correlation to human raters (\(>0.8\)). By focusing on intermediate workflow steps, these evaluations provide a more comprehensive and transparent assessment of models than end-to-end workflow completion rates.

**Results.** We provide baseline results for three state-of-the-art multimodal FMs -- GPT-4 , Claude 3 , and Gemini Pro . Based on screen recordings, we find that models can generate accurate written documentation (F1 of 0.82) and determine whether a demonstration successfully achieved its desired goal (F1 of 0.90). While promising, increasing these numbers to enterprise-level accuracy (i.e. 0.99+) remains an open research challenge. We also identify more significant performance gaps. Models struggle with low-level error correction -- for example, when prompted to classify whether a demonstration exactly followed a specific sequence of steps, the peak F1 achieved is 0.27. Models also score poorly when ranking multiple demonstrations of the same workflow on perceived quality and efficiency. We identify long context reasoning, lower-level process understanding, and human workflow preference alignment as key areas for future research.

Our dataset and code available at our Github repo: https://github.com/HazyResearch/wonderbread.

## 2 Background

We summarize traditional process mining approaches for BPM tasks, discuss recent work on applying multimodal FMs, and compare \(\)WONDERBREAD to existing multimodal FM benchmarks.

### Process Mining

Process mining is the _de facto_ tool currently used for most BPM tasks, acting as an organizational "X-Ray"  that enables large enterprises to identify, measure, and improve their workflows [56; 50; 6]. Techniques include statistical analysis of event logs, unsupervised machine learning, manual review of screen recordings, and user interviews [36; 50]. While interviews can provide an accurate picture of a workflow, they are costly and time-consuming; automated process mining tools are faster but significantly less accurate [1; 36]. Bridging the "semantic gap" between machine and human workflow understanding is an ongoing challenge [41; 36; 1] that \(\)WONDERBREAD aims to address.

### Multimodal FMs

Foundation models (FMs) are large-scale ML models trained on vast datasets of unlabeled data which can be applied to a broad range of tasks with minimal adaptation . Multimodal FMs such as GPT-4 combine natural language understanding with a vision model to process images and text jointly . These models have shown promise in navigating graphical user interfaces and executing simple workflows [16; 67; 26; 27; 70; 64]. While the use of multimodal FMs for BPM tasks has been advocated , it has not yet been implemented. A failure mode of text-only FMs is the lack of an ability to "read between the lines" of human-generated textual summaries of workflows - e.g. when creating a process model from text, GPT-4 misses half the steps that a human would include [34; 25]. This motivates having multimodal FMs directly observe workflows, as done in our benchmark.

### Benchmarks

A number of multimodal datasets have been published for end-to-end automation of websites [73; 18], mobile apps , and desktop applications [64; 65]. However, these datasets do not include step-by-step written guides (SOPs), nor do they evaluate on BPM tasks such as documentation, knowledge transfer, or process improvement [16; 48; 68; 17; 32; 73; 35; 18; 70; 65; 38]. Several works have applied large language models to BPM tasks [21; 53; 19; 57; 10; 25; 42], but they conduct limited case studies (i.e. dozens of examples), rely on manual human evaluation, and do not consider multimodal inputs like screen recordings. Please see Table 1 for a detailed comparison with prior benchmarks.

## 3 Dataset

We nowderbreadincludes 2928 human demonstrations across 598 distinct workflows. Each demonstration contains:

1. **Intent** - a short natural language description of the workflow's goal
2. **Recording** - a full screen recording of the annotator performing the workflow
3. **Action Trace** - a log of all actions taken (clicks, keystrokes, scrolls) and webpage states before/after each action
4. **Key Frames** - images taken from the Recording at each action's timestamp
5. **SOP** - a written guide detailing all of the steps taken by the annotator

The full dataset collection process is illustrated in Figure 2. Each workflow has demonstrations from at least 4 annotators to reflect the diversity of work habits present in an enterprise. For a detailed example of each data type, please see Appendix Figure 8 and Appendix Section A.2 for several example SOPs. Complete definitions for each demonstration component are provided in Table 2.

We start with WebArena, a benchmark containing 812 workflows that require an agent to navigate open-source clones of an e-commerce, content management, forum, and developer tool website . We filter this to 598 workflows by excluding workflows deemed impossible or inadequately specified. Additional details are provided in Appendix A.3.

We recruited 13 annotators to record themselves completing each workflow using a custom Python script. Existing workflow benchmarks often have low-quality demonstrations or inaccurate annotations , thus a key contribution of we WONDERBREAD is the high quality of demonstrations achieved through several months of quality assurance. More details are provided in Appendix A.3.

  
**Benchmark** &  &  &  \\  & \# Tasks & \# Envs & Env Type & Action & Video & SOP & Ranking & Demos/Task & Auto & Doc & KT & Imp \\  AITW  & 30,378 & 357 & M & ✓ & ✓ & – & – & 23.5 & ✓ & – & – & – \\ MindZwe  & 2,350 & 137 & W & ✓ & ✓ & – & – & 1 & ✓ & – & – & – \\ MoTIF  & 6,100 & 125 & M & ✓ & ✓ & – & – & 0.77 & – & – & – & – \\ WebArena  & 812 & 4 & W & ✓ & ✓ & – & – & 0.22 & ✓ & – & – & – \\ Omnixtet  & 9,802 & 65 & D + w & ✓ & – & – & – & 1 & ✓ & – & – & – \\ WebBbone  & 12,087 & 1 & W & ✓ & – & – & – & 0.13 & ✓ & – & – & – \\ VWA  & 910 & 3 & W & – & – & – & – & 0 & ✓ & – & – & – \\ WordArena  & 23,150 & 5 & W & – & – & – & – & 0 & ✓ & – & – & – \\ WebLXX  & 23,337 & 155 & W & ✓ & ✓ & – & – & 1 & ✓ & – & – & – \\ OSWorld  & 369 & 13 & D + W & ✓ & ✓ & ✓ & 4.9 & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of WONDERBREAD to existing benchmarks for workflows. For **Workflows**, “Env” stands for environment – \(W\) is website, \(M\) is mobile, and \(D\) is desktop. For **Evaluation**, “Auto” means the benchmark contains evaluations for end-to-end workflow automation, “Doc” for documenting workflows, “KT” for knowledge transfer, and “Imp” for process improvement.

Figure 2: The dataset collection process began by selecting 598 web navigation workflows from the WebArena dataset . Thirteen annotators then recorded themselves demonstrating roughly 300 workflows each. After multiple rounds of QA, annotators ranked demonstrations for 162 workflows based on perceived quality. The final dataset contains 2928 demonstrations and 6 evaluation tasks.

In addition to demonstrations, we also curated 120 free response question-answer pairs to simulate inquiries that a BPM consultant might ask of a workflow. Examples are listed in Appendix A.5.

## 4 Benchmark

We wonderbread contains 6 tasks which cover three BPM applications not evaluated in prior benchmarks: automatically generating documentation from workflow demonstrations (**Documentation**), facilitating knowledge transfer (**Knowledge Transfer**), and identifying ways to improve inefficient workflows (**Improvement**). We provide a summary of each task below. Further details on the inputs, outputs, and evaluations are in Appendix B. Full prompts associated with each task are included in Appendix F.

### Documentation

Creating clear documentation of complex workflows is essential for operational continuity, compliance, and accountability . This can be achieved through Standard Operating Procedures ("SOP"), Process Definition Documents ("PDD"), or process maps. Our two documentation tasks - SOP Generation and Demo Segmentation - evaluate a model's ability to generate SOPs and accurately distill video recordings into discrete workflows.

**(A) SOP Generation**. Evaluation involves using GPT-4 to compare the generated SOP to an annotator-generated reference SOP, calculating precision (how many steps in the generated SOP are in the reference) and recall (how many steps in the reference are in the generated SOP). Each SOP step is evaluated atomically by GPT-4 for semantic equivalence. Details are in Appendix Section C.2.

**(B) Demo Segmentation**. We concatenate multiple workflow demonstrations into a single video and provide it to the model, which identifies the start and end of each workflow. This tests the model's ability to distinguish between sequential workflows. For evaluation, we calculate the adjusted rand index based on the model's assignment of each video frame to a workflow.

### Knowledge Transfer

The sharing of skills, know-how, and best practices within large organizations can be challenging . By learning from workflow demonstrations, FMs could serve as a query-able repository of

  
**Term** & **Definition** & **File Format** \\  Task & One of the 6 evaluation tasks in our benchmark, as detailed in Section 4. & – \\ Workflow & A sequence of actions taken to complete a specific business goal. Also referred to as a process. A & – \\  & single workflow can have multiple demonstrations. & \\  Demonstration & A single execution of a workflow. Each demonstration contains an Intent, Recording, Action Trace, & Folder \\  & Key Frames, and SOP. & \\ Intent & A brief natural language specification of a workflow’s goal, e.g. “_Cancel my last order_”. & -TXT \\ Recording & A video containing a full recording of the user’s screen. & -MP4 \\ Action Trace & A log of all click, keystroke, and scroll actions (including associated elements and coordinates). & -JsON \\ Key Frames & Images taken from a Recording that are synced to events in the Action Trace. & -PNG(S) \\ SOP & A “Standard Operating Procedure” detailing (in writing) all of the steps taken in a demonstration. & -TXT \\   

Table 2: Key terms and definitions

Figure 3: Expected inputs, outputs, and evaluation settings for **Documentation** tasks.

organizational knowledge for existing employees, and accelerate on-boarding of new hires by more quickly disseminating key information to trainees . Our two Knowledge Transfer tasks - Question Answering and Demo Validation - assess whether a model can perform higher-level reasoning about the properties and correctness of a workflow.

**(A) Question Answering**. For questions about workflow demonstrations, the model generates a natural language answer, assessing its understanding of workflow semantics. We use GPT-4 to compare the generated answer to a reference answer for evaluation.

**(B) Demo Validation**. Given a demonstration, we predict whether (a) the workflow was successfully completed, or (b) the workflow followed the SOP exactly, with individual steps matching precisely. Since each demonstration in \(\)**WONDERBED** is "correct" by definition, we create synthetic negative examples by truncating recordings and shuffling frames. These binary classification tasks assess a model's ability to self-monitor and error-correct.

### Improvement

The ability to continuously refine and enhance the workflows of an organization is crucial for reducing costs and staying ahead of competitors . By focusing on the improvement of demonstrations and SOPs, we highlight the role of iterative learning and optimization in driving the evolution of workflows . Our two Improvement tasks - SOP Ranking and SOP Improvement - evaluate whether a model can identify workflow inefficiencies and improve inaccurate documentation.

**(A) SOP Ranking**. The same end goal can often be achieved via many different sequences of actions. However, some sequences may be preferable to others as they are more efficient, robust, or avoid intermediate steps that could have undesirable side effects. Given a set of SOPs written by different annotators for the same workflow, this task requires the model to rank them in order of quality. This assesses a model's alignment with human perception of workflow quality. For evaluation, we measure the Kendall \(\) correlation between the generated ranking and a human annotator's ranking.

**(B) SOP Improvement**. Given a demonstration and a low-quality SOP, the model must generate an improved SOP that better aligns with the demonstration. The model will iterate to refine the SOP to a specified depth, assessing its ability to assist humans in documenting workflows. GPT-4 will evaluate the generated SOPs against a reference "gold" SOP.

### Evaluation

We use programmatic metrics and LLM-based raters for our evaluations. Tasks involving clustering, classification, or ranking use metrics like adjusted rand index, F1, and correlation, respectively.

Figure 4: Expected inputs, outputs, and evaluation settings for **Knowledge Transfer** tasks.

Figure 5: Expected inputs, outputs, and evaluation settings for **Improvement** tasks.

Natural language tasks are evaluated using GPT-4-as-a-judge to assess input quality [15; 72]. Please see Appendix Table 6 for the specific metrics per task. Our LLM-based evaluations show high correlation with human raters (\(>0.8\)) (see Appendix Tables 8and 9).

## 5 Results

Our initial results show that current multimodal FMs, including GPT-4, Gemini, and Claude, excel at generating documentation which captures the higher-level characteristics of workflows but struggle with finer-grained analyses such as question answering and workflow quality assessment. Our zero-shot evaluations focus on the out-of-the-box capabilities of these models across 162 workflows with rankings. Some models were excluded from specific tasks due to API budget and quota limitations.

### Documentation

**(A) SOP Generation.**_Description:_ A model must generate a SOP that summarizes all of the actions taken in a video recording of a workflow. We ablate over different demonstration formats: only intent; intent with key frame screenshots; and intent with key frames plus a textual action log of clicks and keystrokes. _Results:_ As shown in Table 3, GPT-4 performs best (F1-score of 0.82) with intent, keyframes, and action trace. Most model-demonstration pairs have higher recall than precision (avg. 0.06 points), indicating a tendency to hallucinate workflow steps. Upon qualitative review, we found that many hallucinated actions seemed reasonable but were not actually taken in the demonstration, e.g. adding _"Navigate to the shopping admin page"_ even though the demonstration started on that page. Exact scores for each workflow and model are in Appendix Figure 10.

**(B) Demo Segmentation.**_Description:_ This task mimics what a video recording of a person's screen would capture during the typical workday, i.e. multiple workflows without clear boundaries. Concretely, the model receives \(k\) concatenated demonstrations sampled from different workflows from our dataset, and must determine which frames belong to the same workflow. We set \(k=3\) and choose workflows that utilize the same website. _Results:_ As shown in Table 4, segmenting a recording remains challenging. Providing additional information via an SOP and intent slightly increases performance for GPT-4 yet decreases performance for Gemini Pro 1. On inspection, we find that the frequency at which Gemini Pro 1 outputs blank state mappings (i.e. not assigning a keyframe to any workflow, which under our evaluation framework gets penalized as an incorrect mapping) increases with longer prompts, indicating a worse ability to follow the full context of the prompt.

   Model & Intent & SOP & Keyframes & Adj. **R1** & V-Measure \\  GPT-4 & ✓ & ✓ & ✓ & **0.85** & **0.88** \\ GPT-4 & & ✓ & ✓ & **0.85** & 0.87 \\ GPT-4 & & & ✓ & 0.80 & 0.86 \\ Gemini Pro 1 & ✓ & ✓ & ✓ & 0.55 & 0.66 \\ Gemini Pro 1 & & ✓ & ✓ & 0.53 & 0.65 \\ Gemini Pro 1 & & & ✓ & 0.58 & 0.69 \\   

Table 4: **Demo Segmentation: Accuracy of clustering with \(k=3\) concatenated workflows.**

   Model & Intent & Keyframes & Trace & Precision & Recall & F1 & Avg. \# of Steps \\  GPT-4 & ✓ & ✓ & ✓ & **0.80** & **0.88** & **0.82** & 10.26 \\ GPT-4 & ✓ & ✓ & & 0.69 & 0.79 & 0.71 & 10.32 \\ GPT-4 & ✓ & ✓ & & 0.48 & 0.59 & 0.49 & 13.10 \\ Claude 3 Sonnet & ✓ & ✓ & ✓ & 0.72 & 0.85 & 0.76 & 10.94 \\ Claude 3 Sonnet & ✓ & ✓ & & 0.67 & 0.78 & 0.70 & 11.35 \\ Claude 3 Sonnet & ✓ & ✓ & & 0.53 & 0.54 & 0.50 & 11.34 \\ Gemini Pro 1 & ✓ & ✓ & ✓ & 0.58 & 0.63 & 0.58 & 11.09 \\ Gemini Pro 1 & ✓ & ✓ & & 0.48 & 0.51 & 0.46 & 11.28 \\ Gemini Pro 1 & ✓ & & & 0.40 & 0.36 & 0.34 & 7.31 \\  Ground Truth & ✓ & ✓ & ✓ & 1 & 1 & 1 & 8.40 \\   

Table 3: **SOP Generation: Accuracy of generated SOPs versus ground truth SOPs.**

### Knowledge Transfer

**(A) Question Answering.**_Description:_ This task involves answering 120 free response questions about workflows, such as _"How would a user know the workflow is complete?"_ and _"What is the purpose of this workflow?"_. These questions were drawn from the process mining literature  and are provided in Appendix A.5. We use GPT-4-as-a-jadge to evaluate model-generated answers by comparing to a reference answer from a human annotator. Following prior work , we have GPT-4 output four scores on a scale of 1 (bad) to 3 (good): completeness, soundness, clarity, and compactness. The Pearson correlation between GPT-4 and human raters was between 0.80 and 0.89 across all axes (see Appendix Table 8). _Results:_ Results are shown in Figure 6. All models perform well in "compactness" and "clarity" but score lower on "soundness" and "completeness." The former two are measures of the syntactic quality of writing, while the latter two are measures of the accuracy of the answer. As "soundness" measures whether an answer avoids containing inaccurate details, these lower scores can be explained by the tendency of LLMs to hallucinate and infer information based on patterns learned from training data (i.e. includes content from websites like GitLab, Amazon, etc.) that are not present in the specific demonstrations in **WONDERBREAD**. Lower scores on "completeness" may be due to the difficulty of fully attending to multimodal prompts with multiple states and actions , thus leading to occasional omissions of relevant details.

**(B) Demo Validation.**_Description:_ We consider two forms of validation: (a) workflow completion, where a demonstration is "correct" if the workflow's goal is achieved; and (b) workflow trajectory, where it is "correct" only if the goal is achieved and the steps taken exactly follow a specific SOP. "Correct" examples are sampled from our dataset, while "incorrect" examples are created by truncating, shuffling, or skipping states. _Results:_ As shown in Table 5, GPT-4 performs best. It can accurately determine whether a workflow completed its overall goal (F1 of 0.90) but struggles to validate that a demonstration followed the specific steps of an SOP (F1 of 0.27).

### Improvement

**(A) SOP Ranking.**_Description:_ In this task, we provide a model with SOPs from various annotators and have it rank them by quality. We then compare this ranking to a ground truth ranking by an annotator and measure the correlation between the model's and human's judgments. _Results:_ As shown in Table 6(a), current models struggle to rank SOPs based on perceived quality to human raters. The best model achieves a mean Kendall correlation of 0.05 with a standard deviation of 0.47, indicating essentially random rankings. Improving alignment between model and human judgment of workflow quality remains an area for further research.

   Model & Intent & Keyframes & SOP & Precision & Recall & F1 \\ 
**Completion** & & & & & & \\ GPT-4 & ✓ & ✓ & ✓ & **0.89** & **0.90** & **0.90** \\ GPT-4 & ✓ & ✓ & & 0.84 & 0.77 & 0.81 \\ Gemini Pro 1 & ✓ & ✓ & ✓ & 0.94 & 0.25 & 0.40 \\ Gemini Pro 1 & ✓ & ✓ & & 0.94 & 0.26 & 0.41 \\ Claude3 Sonnet & ✓ & ✓ & ✓ & 0.58 & 0.31 & 0.40 \\ Claude3 Sonnet & ✓ & ✓ & & 0.85 & 0.50 & 0.63 \\
**Trajectory** & & & & & & \\ GPT-4 & ✓ & ✓ & ✓ & 0.52 & **0.18** & **0.27** \\ Gemini Pro 1 & ✓ & ✓ & ✓ & **0.94** & 0.14 & 0.25 \\   

Table 5: **Demo Validation:** Accuracy on binary classification of whether a workflow was completed (_Completion_) or followed the exact steps outlined in the SOP (_Trajectory_).

Figure 6: **Knowledge Transfer:** Scores four axes – soundness, completeness, clarity, and compactness – across 120 free response questions for evaluating workflow understanding.

**(B) SOP Improvement.**_Description._ In this task we provide a model with a task recording and an SOP. The model is then tasked with subsequently improving the SOP given and SOP rubric. _Results._ As shown in Table 6(b), current models are capable of improving the quality of their own SOPs (up to 1.4 points), conditioned upon a SOP rubric.

## 6 Discussion

We discuss next steps, limitations, and the broader impacts of \(\)Wonderbbread below.

**Improving Human-Model Alignment for BPM Tasks.** We find that out-of-the-box human and multimodal models alignment is low for SOP evaluation (see Section 5.3). Similar to how "human-model" alignment can be achieved for tasks like question-answering and instruction-following [59; 33], alignment also appears necessary for workflow understanding tasks. This might require fine-tuning models via supervised learning  or reinforcement learning on preference data .

**Expanding Multimodal Context Windows.** Even a 1-minute workflow can generate dozens of actions and key frames. Our results show that model accuracy on BPM tasks improves as more information is provided in the prompt. This might not be possible with longer workflows, leading to an incomplete representation for a workflow and lower downstream task performance. Longer context windows can help solve this problem and are a focal point of study in the community [31; 66].

**Low-Level Workflow Understanding.** Our results show that while multimodal FMs excel in high-level workflow analyses, they struggle with precise validation of individual steps (see Section 5.2). Enhancing this lower-level understanding may require supervised fine-tuning on GUIs as in [27; 7].

**Self-Improvement.** Our findings suggest that multimodal FMs can improve their outputs (i.e., SOPs) through multiple iterations of self-reflection (see Section 5.3). This highlights the potential of these models to refine their outputs without human intervention [22; 3]. In the context of BPM tasks, this capability can help systems adapt to workflows as they change over time.

**Limitations.** There are several limitations to our work. First, dataset construction was constrained by our lack of access to real-world enterprise data due to privacy concerns. Second, the workflows in our dataset are taken from a limited set of 4 websites , and it is unclear how our results generalize to different environments with complex or longer workflows. Contemporaneous to our work, several datasets have been released which could be re-annotated following the process described in our paper [65; 38; 32], which we leave to future work. Third, our baseline results lack open-source models. Matching the performance of state-of-the-art proprietary models on these benchmarks with open source models remains an open research challenge.

**Scaling.** To our knowledge, Wonderbbread is currently the largest dataset for BPM tasks. However, it is still limited in its ability to capture the broad variety of real-world enterprise workflows. Scaling the approach outlined in this paper represents an exciting future research direction. We propose several ways to increase the size and diversity of data: (1) Synthetically generate demonstrations using AI agents trained on existing workflow examples and reject invalid demonstrations, as detailed in [8; 45]. (2) Crowdsource human demonstrations through platforms like Amazon Mechanical Turk. (3) Collaborate with a large enterprise willing to deploy our recording script to collect real-world workflows. (4) Scrape how-to videos and screen recordings of workflows from sites like Youtube.

**Societal Impact.** Our field's collective focus on end-to-end automation contradicts recent advocacy for more _human-centered AI_, which aims to _augment_ rather than _replace_ human labor [47; 49; 13; 11]. While we intend for \(\)Wonderbbread to serve as a counterpoint to this focus, we acknowledge that any AI tools aimed at improving productivity run the risk of replacing human labor.

Figure 7: Results for the two **Improvement** benchmark tasks.

## 7 Conclusion

We present \(\)WonderBREAD, the first benchmark for evaluating multimodal models on common process mining tasks. It includes 2928 human demonstrations across videos, images, and text, along with step-by-step written guides (SOPs) and full action traces. We focus on applying these models to three BPM tasks that have been overlooked by existing ML benchmarks for workflow automation - documentation, knowledge transfer, and process improvement. \(\)WonderBREAD features an automated evaluation harness with programmatic metrics and LLM-based assessments, providing baseline results for state-of-the-art multimodal models. Our work aims to inspire further efforts to support workers by _augmenting_ rather than _replacing_ human labor.