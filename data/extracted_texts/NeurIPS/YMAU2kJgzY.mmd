# Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads

Anoop Cherian\({}^{1}\)  Kuan-Chuan Peng\({}^{1}\)  Suhas Lohit\({}^{1}\)  Joanna Matthiesen\({}^{2}\)  Kevin Smith\({}^{3}\)  Joshua B. Tenenbaum\({}^{3}\)

\({}^{1}\)Mitsubishi Electric Research Labs, Cambridge, MA, \({}^{2}\)Math Kangaroo USA NFP

\({}^{3}\)Massachusetts Institute of Technology, Cambribidge, MA

https://smartdataset.github.io/smart840

###### Abstract

Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. _Are the current large AI models indeed capable of generalized problem solving as humans do?_ A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children's Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children's deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed _SMART-840_, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children's mathematics and logic skills.

_William Paul Thurston_

## 1 Introduction

\({}^{}\)_Mathematics is not about numbers, equations, computations, or algorithms: it is about understanding._"

Recent multimodal artificial intelligence frameworks incorporating large vision and language models (LVLMs), such as GPT-4o, DALL-E, Gemini, _etc._, are seen to demonstrate outstanding reasoning capabilities , seemingly flustering our established measures of machine intelligence . These scaled up Transformer models  trained on internet-scale datasets using purportedly simplistic training losses such as mask predictions, suddenly appear to have emergent abilities rivaling expert human intellect even on tasks demanding higher-level cognition. Such superior accomplishments naturally raises several questions: Are these models indeed capable of havingcore knowledge and generalizing it towards deriving innovative methods for problem solving? Are they equipped with the faculties to reason like children or are they exploiting implicit biases in their web-scale training datasets towards generating responses that are seemingly correct? Where do AI models rank in their generalized intellectual capacities against humans?

There have been several recent studies that attempt to answer the above questions through novel datasets, tasks, and benchmarks, _e.g._, SMART-101 , MathVista, Math-Vision , MathOdyssey , MathScape , _etc_. While, all these datasets and tasks evaluate varied facets of the generative and reasoning abilities of LVLMs, they typically compare the performance of an LVLM against prior state-of-the-art (SOTA) AI models. While, some of these tasks even include human performances, these evaluations use relatively few human subjects, and do not include the diversity, demographics, background, and other subjective attributes that could influence the solution scheme, making the comparison of AI models to human performances to have significant room for speculation. Notably, there appears to be a lack of a systematic study that benchmarks the capabilities of current SOTA AI models against human cognition on the respective tasks at scale.

Contrary to current AI models that are potentially trained on web-scale data at once, humans develop their problem solving abilities over a period of development towards adulthood, and the type and nature of problems that they can typically solve at different stages of their growth vary significantly. For example, a first grader may be able to solve problems related to tracing a given curved path, however a 12-th grader is expected to solve problems related to finding the intersection points of curves. On the one hand, this incremental nature of building knowledge is essential to the development of solid human problem solving [7; 16; 18]. On the other hand, this cumulative knowledge gathering also enforces an order to the way cognitive foundations are established in rational agents, _e.g._, a 12-th grader is implicitly assumed to have the knowledge to solve problems that a first grader may or may not be able to solve. If we want artificial generalist models that think and reason like intelligent humans, we should expect those models to reliably demonstrate more primitive concepts, in order to build up to reason about more complex problems.

Guided by this insight, we make a first attempt towards systematically comparing the performance of AI models against children's abilities over the period of their growth. Similar to previous and contemporary studies, such as SMART-101 , MathVista, and Math-Vision , we base our approach on the analysis of the problem-solving skills of LVLMs on mathematical and algorithmic reasoning problems selected from Mathematical Olympiads. In contrast to exams typically given in schools, that test the overall grasp of taught subject matter, Olympiads often incorporate problems that explore deeper understanding of concepts, critical thinking abilities, innovative ways of looking at data, and deriving connections across knowledge for solutions. Among many such math Olympiads (such as IMO, AMC, _etc._, that are aimed at higher grade students), one that we use to base our benchmark experiments in this paper is the international Mathematical Kangaroo (MK) Olympiad , which is an international competition held since 1998 in 80 countries with nearly 45,000 student participants from the USA in 2024. Further, MK: i) offers competitions for all grades of children, consisting of age appropriate problems, thus allowing comparisons of the performance of AI models against children from varied age groups and skill levels, ii) uses a multiple choice answer format allowing for easy and objective evaluations that are directly comparable to children's responses, iii) do not demand the participants to have advanced mathematical background (_e.g._, calculus, analysis, _etc._) for deriving the solutions, and iv) offers both text only and image-text problems, thus offering multimodal LVLM evaluation. These aspects make MK Olympiads suitable for our study, thus allowing for a clear gauging of AI's reasoning abilities against the extent of developing human intellect.

To conduct our benchmarking experiments, we created an MK Olympiad dataset, dubbed SMART-840, by collating all the math competition problems from years 2000-2024 for all grades from 1-12th. Our dataset offers a range of problem difficulties (some targeted at 1-2nd graders, some at 11-12th graders), and can further subdivide them by how children actually perform on those problems. Thus, for human-like mathematical reasoning we should expect: (a) better performance on earlier grade problems, and (b) a correlation with problem difficulty within grades. An example problem is provided in Figure 1. We also collect various performance statistics of children on all the exams from MK that are useful in our analysis, including the children's answer responses and time taken in deriving the solutions, among many others. Next, we shortlist several SOTA LVLMs, including closed-source models such as GPT-4o , Gemini , and Claude-3 , and open-source models including LLaVa-Next , and XGEN-MM-Phi3 , among others, that are popular on their generalist abilities. Our analysis reveals several interesting aspects of the AI models: namely i) their performance on the SMART-840 dataset appears to be significantly weaker than the average performance of children, ii) the performance gap is larger (nearly 30-40%) on the tests for younger children and improves to 10-20% for higher-schoolers, iii) there appears to be a lack of any significant correlation between the difficulty of a puzzle to children against that to an AI model, thus making it difficult to judge if a model could answer a given test problem, iv) there appears to be significant variability in the responses of current AI models when repeating or changing prompts, and v) recent LVLMs seem to demonstrate increasingly superior skills in solving _text-only_ problems, outperforming children. We believe our analysis brings new insights into various aspects for testing AI models against human cognition, as well as sheds light into the issues around reliability of current LVLMs for solving math problems.

Before going further, we emphasize below the primary contributions of this paper.

1. We provide a first of its kind benchmarking of the performance of large vision-and-language models against the mathematical and algorithmic reasoning abilities of young children using data from professional math Olympiads.
2. We gauge the reasoning abilities of AI against the cumulative knowledge building progression of children over their growth.
3. Our experiments compare SOTA AI models on both text-only and vision-and-text math problems, analyzing the performances across multiple dimensions.

## 2 Related Works

**General LVLM benchmarks**: Several benchmarks now exist that test different capabilities of LVLMs. These include MMBench  which contains thousands of questions in a VQA format about both perception (1844 questions) and reasoning (1104) where the models select an answer from a given set of options. It also uses a "circular evaluation" strategy to ensure that the models are robust to the ordering of answer options. Although logical and relational reasoning are part of the dataset, this benchmark does not test particularly for different types of mathematical reasoning capabilities. MMMU  is another popular benchmark that contains about 12.5K multimodal questions covering six different disciplines of study, but only at the college level and tests expert-level knowledge of LVLMs. In contrast, we are interested in understanding abilities of LVLMs that children demonstrate. A few benchmarks have been designed to test specific capabilities of LVLMs like ScienceQA  for scientific understanding and reasoning, VisIT-Bench  for instruction following, Bongard Problems [8; 20; 21], Raven's Progressive Matrices , Abstraction and Reasoning Corpus  for abstract visual reasoning, OCRBench , and TextVQA  for text recognition, _etc_.

Figure 1: A 3rd grader puzzle from our SMART-840 dataset and the LVLM responses (both incorrect).

**Benchmarks for mathematical reasoning**: MathVista is a recent benchmark for mathematical reasoning based on puzzles that involve images, while also measuring performance of LVLMs for different types of mathematical reasoning (logical, arithmetic, geometric, _etc._) and different types of context images (natural images, line plots, scientific figures, _etc._). GSM-8k  is a similar dataset containing about 8.5K math word problems, but only has text inputs and outputs, no images are involved. In GSM-Symbolic , the fragility in logical reasoning of LLMs is explored. The main difference of the proposed SMART-840 benchmark against these prior works is that the these datasets neither separate puzzles based on hardness (_e.g._, ease of solving by children at different school levels), which our proposed benchmark explicitly addresses, nor are they supported by human performances at scale. TabMWP  is a benchmark with about 38k grade-school problems but is limited to just tabular math word problems. SMART-101  is the most closely related benchmark to ours, that provides programmatically generated 2000 variations for each of 101 puzzles from just 1st and 2nd grade Math Kangaroo puzzles. These variations can be used to train larger models than using just a small number of puzzles. In contrast to this dataset, SMART-840 contains 840 puzzles from all grades 1-12 and is designed to benchmark the zero-shot mathematical reasoning abilities of AI models on a wide range of problem solving skills, along with important information on performance statistics from test-takers. We note that we are not the first to use MK performance statistics for research, in fact in [2; 4; 33; 34] MK exams were used to study the development of mathematical competencies in children, however to the best of our knowledge these tests have not been used to benchmark AI models.

## 3 Benchmarking Approach and Experiments

We first elucidate our data collection process, followed by the details of the LVLMs that we select to benchmark in this study. The subsequent subsections evaluate the performances of LVLMs on various aspects of the Olympiads deriving correlations with the performances of children.

### Mathematical Kangaroo Olympiad

As alluded to above, most of the Mathematical Olympiads (such as International Math Olympiad, AMC-8, AHSME, _etc._) are targeted at middle or high-school students, while Math Kangaroo is one Olympiad that conducts competitions for K-12 grades, making it a compelling source for this study. Started in France in 1991, the competition has been organized in the USA every year since 1998 and currently takes place in over 100 countries. Typically, there is a single exam for grades \(\{n,n+1\}\), for \(n\{1,3,5,7,9,11\}\), thus there are a total of six exams in a year and children of both grades \(n\) and \(n+1\) participate in the same exam.

Each exam consists of either 24 questions (for grades 1-4) or 30 questions for all higher grades, and is in a multiple choice format with 5 candidate answers, of which only one option is the correct answer. The questions can be purely text-based or can contain both text and an image, interpretation of both jointly is then usually important for solving the problem. Each question is attributed weights in \(\{3,4,5\}\), where the lower points are given to problems that are typically deemed "easier" for that grade (_e.g._, single step reasoning problems for grade 1) while higher points are attributed to problems that need multi-level reasoning, enumeration of solution possibilities, _etc._ that typically involve deeper (but age appropriate) problem solving skills. The participant is given 75 minutes to complete an exam and the performance is computed as the weighted sum of correct responses.

### Data Collection

For this study, we envisage a data collection methodology that is fair, balanced, and offers an unbiased benchmarking of AI models against children's performance on MK tests. To this end, we decided to use all the questions from MK exams without any omissions so that there is no selection bias in our evaluations. As the statistical data we desire on the performance of children is unavailable for MK competitions prior to year 2020, in this study we consider only MK competitions from 2020-2024, that amounts to 840 problems in our dataset, dubbed SMART-840, and consisting of 240 questions all together from grades 1-4 and 600 questions from grades 5-12, evenly split between pairs of grades as described above. Figures 2(a), 2(b), and 2(c) show the distribution of the number of children who participated across years from all grades, which adds to nearly 30K students per year. The participant number is highest for grades 1-8 and then drops to less than 1000 for grades 9-12(Figure 2(b)) perhaps because higher-grade children have other Olympiad options, _e.g._, AMC, IMO, _etc_. Nevertheless, we see that the number of participants put together from the last five years still produce a substantially large sample set for our analysis.

For creating the SMART-840 dataset, we downloaded publicly available1 question papers (which are image embedded PDF documents), followed by running optical character recognition software for extracting the text of the puzzles, and manually cropping the associated image parts. Each such extracted puzzle in the dataset was manually inspected for errors in its text and puzzle images. MK also provides a segregation of each puzzle into one of the four categories, namely (i) geometry, (ii) logic, (iii) algebra, and (iv) numbers. In Figure 2(d), we present the overall statistics of problem distribution in the SMART-840 dataset. We see that _geometric_ puzzles capture nearly 31% of all the puzzles in our set, while the split is about equal between _logic_ (26%) and _numbers_ (27%), and _algebra_ based problems are about 15.5%. In Figure 2(e), we plot the distribution of the number of problems that need both text and image reasoning (\(\)69%) against those that only have text questions. Figures 2(d) and 2(e) also show the split across grades. We see that for higher grades (>8), the number of text-only problems are higher: about 52% in grades 11-12 against <20% in grades 1-4.

### Selected Large Vision-and-Language Models

We compare the performance of seven popular and SOTA LVLMs on the SMART-840 dataset. Specifically, we consider i) GPT-4o , ii) Gemini-Pro , and iii) Claude-3 , that are popular for their abilities in solving challenging math and visual reasoning problems. Thus, we believe it is a useful exercise to understand how they perform on children's grade problems. Alongside these SOTA LVLMs, we also consider other AI models that are popular, such as GPT-4v which is the first vision-and-language version of the GPT series, ii) Gemini-Flash that is well-known for its faster response time, and to recent open-source LVLMs such as XGen-MM-Phi3-Instruct-v1 , LlaVa-Next , InternVL-Chat-V1-2 , and InternLM-XComposer-2.5 .

Figure 2: Figure 2(a) plots the distributions of children participating in MK Olympiads per year over 2020–2024 for grades 1–12. Figure 2(b) plots the total number of participants per grade during 2020–2024. Figure 2(c) plots the total number of participants each year over all grades (1-12). Figure 2(d) shows the number of puzzles and its portion for each category. Figure 2(e) shows the statistics of image-text and text-only puzzles. Figure 2(f) shows the statistics of puzzle difficulty (defined by their attributed weights).

### Grade-wise Performance Comparisons

In this experiment, we compare the performance of the LVLMs listed above against the performance of children on our SMART-840 dataset. For the human performance, we report the percentage of average correct response rate, which we denote as _accuracy_ going forward, and is computed by: i) finding the ratio of the total number of correct children's responses on a problem to the total number of attempts, and (ii) averaging this ratio across all problems in the grade set. For the LVLMs, we use the API interface to query the model using a suitable hand-crafted prompt. Specifically, we found the following prompt to work well for all closed-source LVLMs: "Solve this question with explanation of the intermediate steps. Your response should end with one of the selected answer options from A1, B2, C3, D4, or E5." which is accompanied by the text for the problem question and the image data.2 For AI models, we report their accuracy as the (percentage) of problems correctly answered to the total number of problems in the set.

In Table 1, we present results comparing the performances of LVLMs against children on the entire SMART-840 dataset. First, we report a _random_ baseline to benchmark all our results, which is computed by randomly sampling a response from a probability distribution over all the human responses across the answer options for a problem. As is clear, all the answer options in the problems are equally likely, and thus the random performance is close to one-fifth. Next, for each LVLM, we queried (at least 2 times) each of the problems in SMART-840 using the prompt described above. Note that for LVLM evaluation we also consider two additional possibilities, namely: (i) if a response is not in the expected format as demanded in the prompt, and if we are unable to automatically extract a valid response, we consider the response to be invalid in general (except in experiments when we manually validate the responses), and (ii) in many cases, an LVLM decides not to solve a problem (_e.g._, it mistakes the provided puzzle image to contain security issues), in which case as well, we declare that problem as unsolved by the respective model. We manually inspected all the output responses of GPT-4o (reported as GPT-4o (M) in the table) to ensure the our prompt is suitable, and the model produces responses that are reasonable, grounded in the problem specification (and are not due to issues such as network failures, response parsing failures, etc.) and its solution attempt is reasonable (but not necessarily correct). All problems where the solution was unreasonable (even if the _selected option_ is correct), we manually marked them as a _failed response_.

We see from Table 1 that GPT-4o demonstrates an accuracy of 42.5% on average across all the grades, followed by Claude-3-Opus at 38% and Gemini-Pro at nearly 32%. The more recent Claude-3-Sonnet model in fact outperforms the performances of all the earlier models, with an average accuracy of 49.7%, while outperforming GPT-4o in grades 1-6 and 9-10. There are several intriguing aspects in the performance of LVLMs that we can witness in Table 1.

i) **Performance gap:** The performance of AI models are below that of children across the grades and interestingly this gap is consistent in all the models we experimented. Specifically, the best accuracy of LVLMs are in the range of 40-50% while the children's average performance is consistently near 60% or above. Note that we report children's performances for each grade separately, where kids of a pair of grades take the same exam. Unsurprisingly, we find that children of the higher grades perform significantly better than those of lower grades (although this gap reduces as problem solving abilities mature towards higher grades) suggesting a cumulative set of core problem solving skills that children build over their growth period.

ii) **Performance trend:** In Table 1, we see yet another consistent trend of LVLMs, _i.e._, being better at solving problems of higher grades (8-12) than at lower grades (_e.g._GPT-4o, Gemini-Pro, etc.) or similar performances in solving both higher- and lower-grader problems (_e.g._Claude-3 models), which is surprising given the complexity of solutions increases with grades. This trend was also seen in  where the authors compared the performance of LLMs on second grader problems. We see that while GPT-4o shows this increasing trend with an accuracy of 40% at grades 1-2 towards nearly 50% for grades 11-12, the trend is more striking for other LVLMs such as Gemini-Pro that varies by about 25% (for grade 1-2) to 40% for grades 11-12. We find that Claude-3 models produce a reasonably consistent performance around 40% albeit having a different trend: dip in the performance for middle grades than lower or higher grades.

We find from Table 1 that open source LVLMs such as XGEN-MM, InternLM-XComposer2-VL and InternVL-Chat-V1.2 models perform poorly in comparison. Note that these models show nearly 60% performances on the Math-Vista leader board . We also compare against the recently released LlaVa-NEXT (34B) model which received 47% on the Math-Vista leader board. We find that on SMART-840, these models either selected incorrect answer options or many-a-times did not follow the instruction, thereby producing invalid outputs. As can be noted from the table, while the performance of recent open-source LVLMs are still significantly below that of closed-source LVLMs. We further note that there is nearly a 40% gap in the performances of these models between Math-Vista and SMART-840, suggesting that the mathematical reasoning skills needed to solve SMART-840 are substantially different from existing math datasets publicly available.

Further, we find that the performance of GPT-4v is inferior to that of GPT-4o, which is expected given the latter being a more advanced version of GPT-4v. Further, the accuracy of faster LVLMs such as Gemini-Flash is below that of its advanced counterpart. Thus, in our subsequent study, we only consider the best performing LVLMs, namely GPT-4o, Gemini-Pro, and Claude-3 Sonnet. The recent OpenAI o1-preview model is text-only at this time, thus making it incomparable to other LVLMs in this study; however we report its performance on a text-only subset of our dataset in Table 3.

iii) **Variance:** We ran the LVLMs on each problem at least twice3 and the variance was computed on the differences in the accuracies; (e.g., we ran GPT-4o models about 5 times, while Gemini was run only twice). From Table 1, we find that there is substantial variance in the performance of SOTA LVLMs. For example, the standard deviation for GPT-4o is nearly 7% in solving 1-2 grade problems, while there is a reducing trend in the magnitude of this deviation for higher grades, the reliability in the responses are still questionable. The standard deviation is worse for Claude-3-Opus, where it is nearly 5% across grades, even reaching 10% for grades 7-8. Interestingly, for grades 11-12, the deviation appears more stable at nearly 3-4%, while the performance is also the best.

## 4 Analysis of Results

In this section, we take a deeper look at our results in Table 1 to gain insights into how the AI model responses correlate with those of children. Even though a model is expected to perform as well as an adult, given that their performances are below that of children, it is imperative to ask if they atleast behave like high-performing children in their responses? Specifically, we seek to answer the question: _Are problems that are hard for children also hard for AI?_ To answer this, we conduct different types of correlation analysis, presented below.

**Difficulty Index** of a problem [15; 38] is the ratio of the number of correct responses for a test problem to the total number of solution attempts. This index provides a score between 0 and 1 for each problem, where 0 implies none of the children were able to solve it (hard problem). In Table 2

  ModelGrade & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & Mean \\  Human & 58.8 & 67.6 & 62.3 & 70.1 & 59.1 & 65.4 & 59.7 & 64.3 & 64.2 & 69.3 & 64.9 & 65.6 & **64.2** \\  Random & 20.1 & 20.2 & 20.1 & 20.2 & 20.2 & 20.3 & 20.1 & 20.1 \\ GPT-4o & 41.6 (7.1) & 38.6 (1.7) & 35.1 (0.8) & 47.1 (0.8) & 41.3 (2.0) & 50 (4.0) & 42.4 \\ GPT-4o (M) & 42.5 & 36.7 & 36.0 & 46.7 & 43.3 & 50.0 & 42.5 \\ GPT-4v & 39.2 (0.6) & 38.3 (0.6) & 29.3 (3.3) & 35.3 (1.9) & 38.7 (1.9) & 43.3 (3.7) & 37.4 \\ Gemini-Pro & 25.8 (3.5) & 27.5 (0.6) & 25.3 (3.3) & 30.7 (1.8) & 39.3 (3.7) & 41.3 (2.8) & 31.7 \\ Gemini-Flash & 19.2 (0.6) & 29.2 (10.4) & 22.0 (8.4) & 30.7 (9.7) & 38.7 (13.7) & 36.7 (4.3) & 29.4 \\ Claude-3 Opus & 38.3 (5.3) & 33.3 (5.8) & 31.3 (6.6) & 40.7 (10.4) & 42.0 (5.6) & 44.0 (2.8) & 38.3 \\ Claude-3 Sonnet & 51.6 (0) & 47.9 (2.9) & 38.6 (0.9) & 44.9 (3.3) & 46.7 (0.0) & 49.7 (4.1) & 49.7 \\ XGEN-MM-Phi3-v1 (5B) & 7.5 & 9.1 & 5.3 & 8.0 & 10.0 & 8.0 & 8.0 \\ InternVL-Chat-V1.2 (40B) & 16.7 & 25 & 17.3 & 14.6 & 15.3 & 16.7 & 17.6 \\ InternalLM-XComposer2 (7B) & 22.5 & 14.2 & 18.6 & 24.2 & 18.1 & 16.9 & 19.1 \\ LiVa-NEXT (34B) & 15.0 & 9.0 & 20.1 & 14.6 & 18.7 & 16.0 & 15.6 \\  

Table 1: Accuracy (%) of correct responses of children in the respective grades against the accuracy of LVLMs when the agent is asked to provide explanation of their responses. GPT-4o (M) denotes the performance of GPT-4o after manual validation of GPT-4o responses. We report the standard deviation in brackets. The last block shows the performances of recent open-source LVLMs.

(Diff-I), we report the Pearson's correlations coefficient between the difficulty index and the responses by LVLMs. We see that there is in general only weak correlation between model and human accuracy, and this correlation mostly occurs at the higher grade levels. This suggests that all LVLMs in general find a different set of problems to be difficult than children do.

**Discriminative Index** measures the use of _knowledge_ by a test taker. To compute this score, we split the student population into two groups, the _good learners_ that correspond to the top-20% of participants who score the highest, and _bad learners_ that constitute the bottom-20%. Next, we compute the difficulty index for each of these sets separately, and define discriminative index as the difference between the two difficulty indices. Thus, the value of discriminative index is in \([-1,1]\), where \(1\) corresponds to a test problem where all the good learners produced correct responses while all the bad learners made a mistake - _i.e._, a problem that can separate the good learns from the bad. To understand if an AI model is a good learner or bad learner, we propose to compute the Pearson's correlation between the discriminative index of children's performances against that of the models. The result of this analysis is provided in Table 2 (Disc.-I). Surprisingly, we find a negative trend across all grades, suggesting that an AI model finds it easier to solve problems that are less discriminative, and whose answer options are plausibly discernible without substantial reasoning.

**Weight Correlation** measures the interaction between the hardness of a puzzle as attributed by MK (through its weight) against the response. Notably, we convert this weight in \(\{3,4,5\}\) to the corresponding difficulty score of \(\{1.0,0.66,0.33\}\) for each problem, and compute the Pearsons correlation to the AI responses, which are 1 if the answer option selected to the problem is correct and 0 otherwise. We find a slightly stronger positive correlation on this experiment in Table 2 (Weight-C.), suggesting the AI is able to solve problems that (the adult creator) thought are of the easier kind.

**Entropy Correlation** measures the correlation between the entropy of the distribution of children's selected answer choices against AI responses. As entropy is higher for problems which are hard or their options confusing for children, a positive correlation would suggest AI is similarly confounded. However, the trend in Table 2 shows the reverse, with slightly stronger negative correlations, suggesting that AI models are apparently not much confused on problems children find indecisive.

**Time-taken Correlation** analyzes the dependency between how much time children (on average) used in answering problems - and thus potentially capturing the problem hardness - to whether AI models also find those problems challenging. To this end, we aggregated the duration children spent on each problem, followed by separating the duration into two sets on their median. We marked all problems above the median as hard and the rest as easy. Next, we computed the Pearsons correlation between the responses of AI models against this hardness. Our results in Table 2 (Time-C.) shows again a weak negative correlation trend, suggesting that the model finds it easier to solve problems that take longer for children - a surprising result!

**Category-Level Performances:** As alluded to above, SMART-840 dataset consists of problems in four different categories (as per their creators), each involving entirely different skill sets and knowledge background for their solutions. For the performances reported in Table 1, in Figure 3, we present the results of humans and LVLMs on the four problem categories, namely (i) geometry, ii) numbers, iii) algebra, and iv) logic. While children perform consistently well on all these categories, we find that AI models falter significantly in geometry and logic, with their best performances at about half of that humans while they perform reasonably well on numbers and algebra. We further analyze the performance of LVLMs on problems involving both image and text (e.g., geometry problems) and text-only puzzles. The results show that it is indeed the image-text problems that the models struggle with and we see a strong similarity between performances on geometry and logic problems with image-text problems. Interestingly, we also find that on text-only puzzles (which are about 30%) in our dataset, GPT-4o-Expl.4 shows better performances than the average human performance, while other LVLMs (with suffix "-expl.") are also performing reasonably well.

To analyze this further, in Table 3, we report the performance of LVLMs on image-text puzzles and text-only puzzles separately for each grade pair. We can make several observations, namely: i) human performance is consistently between 60-70%, irrespective of text-only or image-text problems, ii) the performance of LVLMs are significantly higher (nearly double) on text-only problems than on image-text problems, however the trend remains the same, _i.e._, LVLMs appear to find lower-grader problems equally difficult as that for higher-grader problems, iii) some of the very recent powerful 

[MISSING_PAGE_FAIL:9]

even on puzzles involving simple geometry and logic, that accentuates the lack of understanding between language and multimodal content. Further, while there is a substantial gap between the best of LVLMs and the worst, or random baselines, 40% for GPT4o vs 20% for random or 25% for Gemini-Pro, this is only a 20% difference; in contrast the gap between even the best LVLMs and human adult level performance in reasoning is much greater. We ought to point this out.

Our results suggest some ways that LVLMs, even the most advanced ones, may not really be reasoning in the ways that humans do. For humans, reasoning is an ability to think that goes beyond just similarity to training examples. But here we are seeing signs that similarity to the large mass of training examples appears to be what is driving performance across all levels of these problems. Of course, we do not fully know what is in the training corpus SOTA LVLMs used. But it may include many Olympiad problems than there are math kangaroo grade 1-2 style problems. Yet for people, and not for frontier LVLMs, the MK grade 1-2 problems are far easier than the Olympiad style grade 11-12 problems. This suggests both that human reasoning is based on a different set of core competencies, which the early grade problems test, and which a pure machine learning approach to training reasoning is not really picking up on.

Before concluding, we present in Table 4, the national rank and percentile of the three SOTA models on the scores they received for 2024 MK Olympiad (when compared against children). We see that AI models are substantially below children in ranking, with GPT-4o best on grade 7-8 in rank in 50's and Gemini-Pro at 34 for grade 12, and the recent Claude-3 Sonnet model outperforming its previous Claude-3 Opus model as well as other models across other grades, however yet their best national rank is more than 30. These scores are based on the percentiles received from MK. The table shows that there is a large gap to fill for LVLMs against children's problem solving skills.

**Limitations and Social Impact:** In comparison to recent datasets used in the AI mathematical reasoning benchmarks, our SMART-840 is smaller in size. However, we ought to emphasize that this small size is by necessity. Our intended use is to see how well general purpose language models (including those presumably trained with a fair amount of mathematical and visual reasoning data in their training sets) come towards capturing this spectrum of mathematical problem solving and reasoning. Thus, we do not intend to compare models trained/fine-tuned on this dataset, as that would invalidate any comparisons to human performance, where humans are assumed not to have seen the problems when taking the tests. Our goal is to bring out the disparity in machine cognition with respect to humans via studying the zero-shot performance of LVLMs with respect to human cognition on our task. Given the goal of this study is not to train LVLMs to excel on this task, instead is to evaluate the zero-shot performance, and given that our results clearly demonstrate a discrepancy between human and LVLM performance, we believe our dataset does help bring out the failure modes of state-of-the-art LVLMs, and point to directions that would need improvements.

   Model \(\) Grade & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\  GPT-4o & 4957.4 & 4930.4 & 6126.8 & 60913.2 & 7044.3 & 7092.90 & 5746.4 & 58039.7 & 70023.8 & 7016.8 & 6621.1 & 5029.2 \\ Gemini-p & 787.3 & 782.3 & 6941.9 & 6836.6 & 7535.6 & 7521.9 & 8014.4 & 8110.0 & 7911.8 & 777.7 & 5143.7 & 3456.9 \\ Claude-3-O & 69920.6 & 696.7 & 817.2 & 8071.1 & 8618.3 & 869.3 & 6543.6 & 6628.9 & 857.1 & 8247.0 & 53/39.8 & 36/54.6 \\ Claude-3-S & 4171.1 & 41/45.7 & 4849.2 & 47730.2 & 87/16.8 & 878.2 & 78/16.5 & 7912.9 & 38/72.7 & 38/59.0 & 56/34.8 & 4050.0 \\   

Table 4: National Rank (\(\)) / percentile (\(\)) ranking of LVLMs against children’s performance on MK 2024 Olympiad based on the test scores computed from the model response.

   ModelGrade & 1 \& 8 \& 2 & 3 \& 4 & 5 \& 6 & 7 \& 8 & 9 \& 10 & 11 \& 12 \\   &  \\  \% dataset & 89.17 & 85.00 & 82.67 & 64.67 & 52.0 & 48.67 & 10.83 & 15.00 & 17.33 & 35.33 & 48.0 & 51.33 \\   &  \\  Human & 67.98 & 69.91 & 65.59 & 64.76 & 71.22 & 65.24 & 64.51 & 70.89 & 64.7 & 63.51 & 67.23 & 65.94 \\ GPT-4o & 36.45 & 30.39 & 31.45 & 38.14 & 32.05 & 32.88 & 92.31 & 72.22 & 57.69 & 62.26 & 55.56 & 66.23 \\ Gemini-Pro & 20.56 & 19.61 & 21.77 & 21.65 & 21.79 & 20.55 & 69.23 & 72.22 & 42.31 & 47.17 & 58.33 & 61.04 \\ Claude-Sonnet & 45.79 & 40.20 & 35.48 & 35.05 & 32.05 & 30.14 & 100.00 & 77.78 & 57.69 & 56.60 & 62.50 & 62.34 \\ OpenAI o1-review & - & - & - & - & - & - & 100.00 & 100.00 & 84.6 & 92.4 & 84.7 & 90.9 \\   

Table 3: Top row, left and right blocks show the % of problems in SMART-840 that belong to image-text or text-only categories per grade. Lower block show the separated performances of LVLMs on image-text and text-only problems against humans.

**Acknowledgements:** Authors thank Preethi Ann Cyril for helping with data curating and Math Kangaroo USA for providing the performance data.