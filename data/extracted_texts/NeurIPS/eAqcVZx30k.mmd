# Is the MMI Criterion Necessary for Interpretability?

Degenerate Non-causal Features to Plain Noise for Self-Rationalization

 Wei Liu\({}^{1}\)  Zhiying Deng\({}^{1}\)1  Zhongyu Niu\({}^{1}\)  Jun Wang\({}^{2}\)1

Haozhao Wang\({}^{1}\)1  YuanKai Zhang\({}^{1}\)  Ruixuan Li\({}^{1}\)1

\({}^{1}\)School of Computer Science and Technology,

Huazhong University of Science and Technology

\({}^{2}\)iWudao Tech

\({}^{1}\){idc_lw, dengzhiyingdd, zy_niu, hz_wang, yuankai_zhang, rxli}@hust.edu.cn

\({}^{2}\)jwang@iwudao.tech

This paper is a collaboration between Intelligent and Distributed Computing Laboratory, Huazhong University of Science and Technology and iWudao Tech. The companion pieces to this research series can be found at [https://jugechengzi.github.io/Weiliu.github.io/](https://jugechengzi.github.io/Weiliu.github.io/).

###### Abstract

An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for **M**aximizing the **R**emaining **D**iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to \(10.4\%\) as compared to several recent competitive MMI variants.

Code: [https://github.com/jugechengzi/Rationalization-MRD](https://github.com/jugechengzi/Rationalization-MRD).

## 1 Introduction

With the success of deep learning, there are growing concerns over interpretability (Lipton, 2018). Ideally, the explanation should be both faithful (reflecting the model's actual behavior) and plausible (aligning with human understanding) (Jacovi and Goldberg, 2020; Chan et al., 2022). Post-hoc explanations, which are trained separately from the prediction process, may not faithfully represent an agent's decision, despite appearing plausible (Lipton, 2018). In contrast to post-hoc methods, ante-hoc(or self-explaining) techniques typically offer increased transparency (Lipton, 2018) and faithfulness (Yu et al., 2021), as the prediction is made based on the explanation itself. There is a stream of research that has exposed the unreliability of post-hoc explanations and called for self-explanatory methods (Rudin, 2019; Adebayo et al., 2018; Ghassemi et al., 2021; Ren et al., 2024).

One important line of research to build self-explainable NLP models is first extracting the most informative rationale in a text and then using the extracted rationale to train a predictor. This line of research is known as rationalization. A model-agnostic rationalization framework, called Rationalizing Neural Predictions (RNP), was first proposed by Lei et al. (2016). RNP utilizes a cooperative game between an extractor and a predictor. This game is designed with a focus on "data-centric" importance of rationales (i.e., it aims to explain the connection between a text and the model-agnostic task label, rather than explaining the output of a specific model). First, the extractor identifies the most informative part of the input, known as the rationale. Then, as depicted in Figure 1, the rationale is transmitted to the predictor to facilitate predictions. The extractor and predictor are trained cooperatively to maximize prediction accuracy, with the theoretical support being the Maximum Mutual Information (MMI) criterion (Yu et al., 2021; Chang et al., 2020). RNP and its variants have become mainstream approaches for enhancing the interpretability of NLP models (Liu et al., 2022, 2023, 2023, 2023; Zheng et al., 2023; Liu et al., 2024; Zhao et al., 2024; Jiang et al., 2024; Hu and Yu, 2024; Yue et al., 2024). Aside from interpretability, rationalization can also serve as a method for data cleaning, as the extracted \((Z,Y)\) samples can function as a new dataset. Recent studies have shown that a predictor trained with such a dataset can be more robust (Chen et al., 2022) and generalizable (Wu et al., 2022; Gui et al., 2023), due to the removal of task-irrelevant, harmful information.

Previous methods typically employ the Maximum Mutual Information (MMI) criterion to identify the rationale, defined as the subset most indicative of the target label. However, certain datasets contain features that are statistically correlated with the task label but do not causally affect it. These features are referred to as spurious features, and the associated correlations are known as spurious correlations. The spurious features are also indicative of the target label and can compete with the true rationale for extraction opportunities under the MMI criterion, distinguishing them from plain noise. Consider a scenario where the extractor is initially positioned on selecting plain noise. If a clean dataset contains no spurious features, the gradient will guide the extractor solely towards causal features. However, if the dataset is rich in spurious features, the extractor can move in various directions, arbitrarily towards either spurious or causal features. Given the potential diversity of spurious features in the data, the extractor may struggle in a complex loss landscape (Chang et al., 2020). A typical example of spurious correlation, as highlighted in LIME (Ribeiro et al., 2016), is the frequent co-occurrence of wolves and snow in images. Consequently, the presence of snow in the background can erroneously serve as a strong indicator for classifying an image as depicting a wolf, leading MMI to possibly select the background feature instead of the wolf's face as the rationale. Figure 5(a) in Appendix A.10 illustrates another instance of spurious correlations.

Some methods try to develop regularizers that can penalize the spurious features and fix the shortcoming of MMI. INVRAT (Chang et al., 2020) incorporates the concept of invariant risk minimization to design an invariance penalty. Inter_RAT (Yue et al., 2023) utilizes an intervention penalty. CR (Zhang et al., 2023) implements a sufficiency and necessity penalty by separately assessing the sufficiency and necessity of each token. In addition to the specific shortcomings of each type of method (discussed in SS2), they share a common limitation: most still adhere to the MMI criterion and merely use supplementary objectives to penalize spurious features. If the penalty term's weight is too small, spurious features will still be favored over uninformative noise due to their higher mutual

Figure 1: The standard rationalization framework RNP. The task is binary sentiment classification about the hotelâ€™s location. \(X,Z,,Y\) represent the input, the extracted rationale candidate, the prediction and the ground truth label, respectively. \(_{E},_{P}\) represent the parameters of the extractor and the predictor, respectively. \(H_{c}\) denotes cross-entropy.

information. Consequently, when the extractor initially selects noise, the gradient descent algorithm might shift towards either spurious features or the true rationale. On the other hand, if the penalty term's weight is too high, it can dominate the loss function and impair the MMI's ability to distinguish between noise and causal features (see SS4.1). The difference between spurious features and noise can complicate the loss landscape of rationale extraction, which may lead to the emergence of local optima. Note that the problem of local optima in rationalization is very serious (Yu et al., 2021). A recent research MCD (Liu et al., 2023b) revises the MMI criterion to the minimum conditional dependence criterion and does not introduce extra penalty terms. However, MCD also can not promise to treat the spurious features as plain noise. We provide a detailed comparison of MCD and our approach in Appendix A.1 help readers better understand the unique advantages of our approach.

In this paper, we diverge from previous research that focuses on the selected rationale candidate \(Z\) as the primary subject. Instead, we adopt a reversed perspective, considering the remaining part \(X_{-Z}\) by excluding the rationale candidate \(Z\) from the full input \(X\), as the main subject of study. We find that, although selecting spurious features rather than noise as \(Z\) will be more indicative of \(Y\) (i.e., \(P(Y|S) P(Y|N)\), with \(S,N\) denoting **S**purious features and **N**oise), neither selecting the plain noise nor the spurious features as \(Z\) will cause a change in \(P(Y|X_{-Z})\) (i.e., \(P(Y|X_{-S})=P(Y|X_{-N})=P(Y|X)\)). Based on this observation, we replace the criterion of maximizing the mutual information \(I(Y;Z)\) with maximizing the remaining discrepancy (MRD) \(D_{KL}(P_{Y|X}||P_{Y|X_{-Z}})\). Under this new criterion, spurious correlations are treated as equivalent to uninformative noise without extra supplement regularizers on the rationale candidate, allowing the extractor to work on datasets rich in spurious features as if it were working on clean datasets.

In summary, our contributions are as follows: (1) We introduce a new criterion that treats spurious features as equivalent to plain noise, simplifying the loss landscape for rationale extraction. (2) We propose a simple and practical method to implement this new criterion. (3) Experiments on six widely used datasets show that our MRD improves the rationale quality (measured by the overlap with human-annotated rationales) by up to \(10.4\%\) as compared to several competitive MMI variants.

## 2 Related work

**Data-centric rationale extraction**. Data-centric rationale extraction (also known as rationalization) is a general framework first proposed by Lei et al. (2016). By extracting rationales before making predictions, this framework has been one of the mainstreams to facilitate the interpretability of NLP models (Chang et al., 2020; Sha et al., 2021; Yu et al., 2021; Shen et al., 2022; Chan et al., 2022; Storek et al., 2023; Zhang et al., 2023). And DeYoung et al. (2020) proposed a benchmark that can be used for supervised rationale extraction. Recently, there has also been some work attempting to extend it to the field of graph learning (Luo et al., 2020) and computer vision (Yuan et al., 2022). Apart from improving interpretability, recent work has also discovered that it can serve as a method of data cleaning, as training a predictor with the extracted rationales has been found to increase robustness (Chen et al., 2022) and generalization (Wu et al., 2022; Gui et al., 2023). We also briefly discuss the potential impact of rationalization in the era of LLMs in Appendix A.11.

**Mitigating spurious correlations**. One important obstacle of rationalization is the spurious features in datasets, as the spurious features also have high correlations with the task label and can compete with the causal features for extraction opportunities under the most widely used MMI criterion. Some methods have been developed to mitigate the impact of spurious correlations. INVRAT (Chang et al., 2020) attempts to tackle feature correlation using invariant risk minimization (IRM) (Arjovsky et al., 2019). The main idea is to penalize spurious (non-causal) variations by splitting the dataset into distinct environments. However, IRM-based methods have several limitations. For instance, they require strong prior knowledge about the relationships between non-causal and causal features (e.g., the extra labels of non-causal features) in order to divide the dataset (Lin et al., 2022b). Moreover, IRM-based methods are limited to addressing only a finite set of predetermined non-causal features, neglecting the potential existence of numerous unknown non-causal features. In fact, a recent study (Lin et al., 2022b) in the field of IRM has theoretically demonstrated that it is nearly impossible to partition a dataset into different environments to eliminate all non-causal features using IRM. Other challenges, such as the tendency to overfit, difficulty in applying to larger models (Zhou et al., 2022; Lin et al., 2022a), and the marginal shift risk of the input (Rosenfeld et al., 2021), have also been identified within the realm of IRM. Inter_RAT (Yue et al., 2023) attempts to eliminate feature correlation through _backdoor adjustment_, intervening directly with the confounders. However, it is extremely hard to measure the confounders since they are usually not observable in the dataset. CR (Zhang et al., 2023) calculates the sufficiency and necessity of each token separately, which leads to a high computational complexity, making it feasible only for very short texts. Aside from the above shortcomings, penalty-based methods share a common limitation. They need to coordinate the MMI and the penalty objectives to make the gradient descent algorithm treat the spurious features and plain noise equally and guide the extractor to move towards only the causal features. A recent research MCD (Liu et al., 2023b) revises MMI to the minimum conditional dependence criterion. Although MCD does not involve penalty regularizers, it also cannot treat spurious features and plain noise equally. And the spurious features can still compete with the causal ones.

## 3 Preliminaries

### The rationale extraction task

We consider the text classification task, where the input is a text sequence \(X\)=\([x_{1},x_{2},,x_{l}]\) with \(x_{i}\) being the \(i\)-th token and \(l\) being the number of tokens. \(Y\) represents the classes in a dataset \(\). The standard rationalization framework RNP (Lei et al., 2016) consists of an extractor \(f_{E}()\) and a predictor \(f_{P}()\), with \(_{e}\) and \(_{p}\) representing the parameters of the extractor and predictor. For \((X,Y)\), the extractor first outputs a sequence of binary mask \(M=f_{E}(X)=[m_{1},,m_{l}]\{0,1\}^{l}\) (in practice, the extractor first outputs a Bernoulli distribution for each token and the mask for each token is independently sampled using gumbel-softmax). Then, it forms the rationale candidate \(Z\) by the element-wise product of \(X\) and \(M\):

\[Z=M X=[m_{1}x_{1},,m_{l}x_{l}]. \]

To simplify the notation, we denote \(f_{E}(X)\) as \(Z\) in the following sections, i.e., \(f_{E}(X)=Z\). With the extractor's selection, we get a set of \((Z,Y)\) samples, which are generally considered to represent the distribution \(P(Y|Z)\). The rationale \(Z\) is searched by maximizing the mutual information \(I(Y;Z)\):

\[Z^{*}=*{arg\,max}_{Z}I(Y;Z)=*{arg\,max}_{Z}(H(Y)-H( Y|Z))=*{arg\,min}_{Z}H(Y|Z),\ s.t.,\ Z=f_{E}(X). \]

In practice, the entropy \(H(Y|Z)\) is commonly approximated by the minimum cross-entropy \(_{_{p}}H_{c}(Y,|Z)\), with \(=f_{P}(Z)\) representing the output of the predictor. It is essential to note that the minimum cross-entropy is equal to the entropy (please refer to Appendix A.7). Replacing \(Z\) with \(f_{E}(X)\), the extractor and the predictor are trained cooperatively:

\[_{_{e},_{p}}H_{c}(Y,f_{P}(f_{E}(X))|f_{E}(X)),\ s.t.,\ (X,Y) . \]

To make the selected rationale human-intelligible, rationalization methods usually constrain the rationales by compact and coherent regularization terms. In this paper, we use the most widely used constraints proposed by Chang et al. (2020):

\[(M)=_{1}}{l}-s+_{2}_{t =2}^{l}m_{t}-m_{t-1}. \]

The first term encourages that the percentage of the tokens being selected as rationales is close to a pre-defined level \(s\). The second term encourages the rationales to be coherent.

### Causality

We note that the contribution of this part does not belong to this paper. To help readers unfamiliar with causality better understand the spurious correlations, we borrow it from a previous paper MCD (Liu et al., 2023b) and make some minor revisions to make this paper self-contained. We provide a detailed comparison with MCD in Appendix A.1.

We consider that \(X\) consists of a set of variables \(\{N,S,C\}\), where \(C\) denotes the real causal rationale for the corresponding task label \(Y\). And \(N,S\) represent the plain **N**oise and **S**purious features, respectively. The extractor selects one of \(\{N,S,C\}\) to be the rationale candidate \(Z\). Note that \(Z\) is not a separate variable, but a proxy for any variable within \(X\). Initially, the extractor may randomly select either \(N,S\) or \(C\) to be \(Z\).

Consider a classification dataset, we posit a probabilistic graphical model to illustrate the corresponding data-generating process in Figure 2(a). The annotators assign the task label \(Y\) by viewing the causal features in \(X\) (\(C Y\)). There are also some spurious features non-causally associated with \(Y\) through some unobservable confounders \(U\) (\(S U C Y\)).

To facilitate understanding, let's take a widely used dataset, Beer-Appearance, as an example for a detailed analysis in Figure 2(b). The task is binary sentiment classification for beer's appearance. The input \(X\) comprises comments on two aspects (we omit other aspects for brevity): \(X_{T}\) for **T**aste and \(X_{A}\) for **A** Appearance, each of which can be considered as a subset variables of \(X\). Additionally, \(N\) signifies something that does not discuss the sentiment tendency of \(X\). The annotators assign the appearance label \(Y\) by viewing the comments on appearance (\(X_{A} Y\)). Therefore, only \(X_{A}\) serves as the direct cause for \(Y\). However, \(X_{A}\) is correlated with \(X_{T}\) due to a set of unobserved variables \(U\) (called _confounders_). For example, \(U\) may include a variable indicating whether the beer originates from a reputable brand, and a pleasant taste may imply that the beer comes from a good brand (\(U X_{T}\)). Moreover, a beer from a reputable brand is likely to have a pleasant appearance (\(U X_{A}\)). Consequently, \(X_{T}\) is associated with \(Y\) via a _backdoor_ path, as depicted by the red dotted line in Figure 2(b). In this situation, \(X_{T}\) is somewhat indicative of \(Y\) (please refer to Appendix A.2 for a quantitative example), but it signifies a statistical correlation rather than causality. With the objective of MMI (Equation 3), \(X_{T}\) can compete with \(X_{A}\) for the opportunity to be selected as the rationale candidate, complicating the rationale extractor's search landscape.

## 4 Treating spurious features as equivalent to plain noise

### The shortcomings of penalty-based MMI

Since spurious features also have a high correlation with the task label, some methods tend to penalize spurious features with some supplementary regularizers (discussed in SS2). Generally, their loss functions can be written in a form like

\[(Z)=_{MMI}(Z)+_{penalty}(Z), \]

where \(Z\) is the rationale candidate, which is a proxy of the variables within \(X\) (e.g., \(C,S\) or \(N\)).

We now present some qualitative analysis to demonstrate why using penalties to amend the MMI criterion can only partially mitigate the issue of spurious correlations. Generally, for the MMI loss, we have \(_{MMI}(C)_{MMI}(S)<_{MMI}(N)\) in real-world datasets (please refer to Appendix A.3 for detailed discussion). For the penalty loss, we usually have \(_{penalty}(C)<_{penalty}(S)\) and \(_{penalty}(N)<_{penalty}(S)\). We denote \(d(,)\) as the distance of the extractor's parameters moving from one state to another. For example, \(d(N,C)\) denotes the distance between the extractor's two states selecting \(N\) and \(C\) respectively. We denote \(g(N C)=(N)-(C)}{d(N,C)}\) as the (qualitative) tendency of the extractor's moving from \(N\) towards \(C\).

Figure 3: Penalizing spurious features for more efficiently searching causal rationales.

Figure 2: The data-generating process of (a) a general classification dataset and (b) a specific dataset Beer-Appearance.

If \(=0\) (vanilla MMI), although we have \(g(N C)>0\), we also have \(g(N S)>0\). Thus the extractor may move towards either \(C\) or \(S\) with gradient descent, not necessarily \(C\) (like the situation shown in Figure 3(b)). This could lead to longer optimization paths, and the additional paths might introduce extra local optima. Note that Yu et al. (2021) have shown that local optima are serious in unsupervised (with no human-annotated rationales for supervision) rationalization.

MMI allows the extractor to move towards either spurious features or causal features when starting from plain noise (Figure 3(b)). Conversely, penalties enable the extractor to move towards either plain noise or causal features when starting from spurious features (Figure 3(d)). If these two objectives are well-coordinated such that \((S)=(N)>(C)\), the loss landscape will be much simpler and the extractor can ultimately move towards causal features (Figure 3(c)). However, such a coordination is not easy to achieve. If \(\) is too small, the situation will be under-penalty (Figure 3(a)) and the spurious features can still compete with the causal features for extraction opportunities. If \(\) is too high, the situation can become one of over-penalization (Figure 3(d)), where the influence of MMI in distinguishing between noise and causal features may be decreased by the domination of \(_{penalty}(Z)\). As a result, noise can compete with causal features for the chance of being selected. In conclusion, a good objective should make that \(g(N C)>0,g(S C)>0,(S)=(N)\).

Since none of the existing MMI variants can treat spurious features as equivalent to plain noise. It then leads to a question: is MMI really necessary for rationale extraction? Can we no more use auxiliary regularizers to fix it, but just remove it completely and replace it with other criteria?

### Spurious features are equivalent to plain noise in a counterfactual view

We aim to develop a new criterion that can treat spurious features as equal to plain noise, so that regardless of whether the extractor currently selects \(S\) or \(N\), the gradient descent algorithm can guide the extractor to move only towards \(C\). In this paper, we adopt a perspective that reverses common methods. We no longer focus on the selected rationale candidate as previous methods do. Instead, we look into the properties of the remaining part after excluding the rationale candidate.

We denote the non-causal subset of \(X\) as \(A=\{S,N\}\). From the probabilistic graphical model shown in Figure 2(a), we know that \(A\) and \(Y\) are _d-separated_ by the causal features \(C\)(Liu et al., 2023b) (please refer to Appendix A.6 for a detailed illustration). It means that all variables within \(A\) are independent with \(Y\) when conditioned on \(C\).

With the _d-separation_ property, we have \(P(Y|C,S)=P(Y|C)=P(Y|C,N)=P(Y|C,N,S)\). This inspires us to view the problem from a perspective opposite to previous studies; that is, we no longer focus on the extracted rationale candidate \(\) as the subject of study, but rather on the remaining part of \(X\) after \(Z\) has been removed, denoted as \(X_{-Z}\). Regardless of whether the extractor selects \(S\) or \(N\) to be the rationale candidate \(Z\), we have

\[P(Y|X_{-Z})=P(Y|X),\;s.t.,\;Z\{N,S\}, \]

The high level intuition behind Equation 6 is that neither removing the plain noise nor the spurious features will cause a change in the task label. So, we have that

\[0=D_{KL}(P(Y|X_{-N})\|P(Y|X))=D_{KL}(P(Y|X_{-S})\|P(Y|X))<D_{KL}(P(Y|X_{-C})\| P(Y|X)) \]

If we define the loss function as

\[(Z)=-D_{KL}(P(Y|X_{-Z})\|P(Y|X)), \]

we will have that \((C)<(N)=(S)\), which means that

\[ g(N C)&=(N)- (C)}{d(N,C)}>0,\;g(S C)=(S)-(C )}{d(S,C)}>0,\\ g(N S)&=(N)-(S)}{d(N, S)}=0,\;g(S N)=(S)-(N)}{d(S,N)}=0, \]

where \(g(N C)\) is mentioned in the above qualitative analysis following Equation 5, denoting the approximate tendency of the extractor to move from \(N\) to \(C\). We call this objective as maximum remaining discrepancy (MRD) criterion. The unique advantage of MRD is that it can treat spurious features as equivalent to plain noise. Thus, extracting rationales from datasets containing spurious features becomes equivalent to extracting from clean datasets without such features. As a result, the extractor only needs to distinguish between noise and causal features, significantly reducing the difficulty of rationale extraction.

## 5 The practical method

If we use Equation 8 to replace MMI, as long as \(C\) is not selected as the rationale candidate \(Z\), the objective will not distinguish between \(N\) and \(S\). That is to say, no matter the extractor currently selects \(N\) or \(S\) as the rationale candidate \(Z\), the gradient descent algorithm can only guide it to move towards \(C\). It should be noted that the compactness of \(Z\) is facilitated through the sparsity constraint expressed in Equation 4.

The followed problem is how to apply MRD in practice. The real distributions of \(P(Y|X_{-Z})\) and \(P(Y|X)\) are not directly accessible. So we need further efforts to approximate them. Similar to the vanilla RNP's approximating entropy with cross-entropy and inspired by the MCD's  success in approximating real distributions with a predictor's output, we try to approximate real distributions by making use of the predictor. We first approximate \(P(Y|X)\) with \(P(_{X}|X)\) by minimizing \(H_{c}(Y,_{X}|X)\) (please refer to Appendix A.7 for detailed analysis on the feasibility of this approximation), and we also approximate \(P(Y|X_{-Z})\) with \(P(_{-Z}|X_{-Z})\) by minimizing the cross-entropy \(H_{c}(Y,_{-Z}|X_{-Z})\), where \(_{-Z},_{X}\) are the predictor's outputs with the inputs being \(X_{-Z}\) and \(X\), respectively.

Finally, the training process for our MRD is depicted in Figure 4: the extractor first selects a rationale candidate \(Z\) from the input \(X\). Subsequently, \(X_{-Z}\) and \(X\) are fed into the predictor to obtain two distributions, \(P(_{-Z}|X_{-Z})\) and \(P(_{X}|X)\). The overall objective of our model becomes (The pytorch implementation is in Appendix A.8):

\[_{_{p}}[H_{c}(Y,_{X}|X)+H_{c}(Y,_{-Z}|X_{ -Z})] \] \[+_{_{e}}[-D_{KL}(P(_{X}|X)\|P(_{-Z}|X_{-Z }))+(M)],\] \[s.t.,\;(X,Y),\;P(_{X}|X)=f_{P}(X),\;X_{-Z} =X-f_{E}(X),\;P(_{-Z}|X_{-Z})=f_{P}(X_{-Z}),\]

where \((M)\) is mentioned in Equation 4. The first term is used to help the predictor approximate the distributions, and the second term helps the extractor find a good rationale.

## 6 Experiments

### Datasets and metrics

**Datasets**. To validate the method's ability to extract causal rationales in the input, there are certain requirements for the datasets. First, the datasets should contain spurious correlations, making causality a primary challenge within these datasets. Second, the test set should contain manually annotated causal rationales to facilitate quantitative comparisons between different methods.

We employ six datasets collected from two widely used benchmarks. **BeerAdvocate** is a benchmark that contains three widely used text classification datasets: Beer-Appearance,

Figure 4: The architecture of our proposed MRD. The approximators for the two distributions are shared to reduce the model complexity.

[MISSING_PAGE_FAIL:8]

and by more than \(20\%\) in 2 out of 18 settings, verifying the limitation of penalty-based methods. We provide a visualized example of the extracted rationales by different methods in Appendix A.10.

We also follow a recent method CR (Zhang et al., 2023) to conduct experiments with the BERT encoder as a supplement, whose results are shown in Table 4. We follow CR to set the sparsity level as \(10\%\), and the datasets are the most widely used Beer-Appearance and Beer-Aroma. Since some methods become highly sensitive to hyperparameters after switching to an over-parameterized BERT model (also supported by Remark 6.1 in (Zhang et al., 2023)), and our computational resources are insufficient for extensive hyperparameter tuning for these methods, we primarily compare our approach with methods that have already been implemented using BERT. Our MRD still outperforms all the baselines. Specifically, we improve the F1 score by \(15.6\%\) on the Beer-Appearance dataset, and \(6.0\%\) on the Beer-Aroma dataset.

## 7 Conclusion, limitations, and future work

This paper investigates the susceptibility of the widely adopted MMI criterion in XAI to spurious correlations. We design a new criterion that can treat spurious features as plain noise, making rationale extraction from datasets rich in spurious features as straightforward as extracting from clean datasets, thus simplifying rationale extraction. Given the versatility of the self-explaining rationalization

    &  &  &  \\   & & S & P & R & F1 & S & P & R & F1 \\   & RNP & 10.1 (0.9) & 58.5 (2.3) & 47.6 (2.8) & 52.4 (1.2) & 9.9 (0.2) & 47.9 (1.2) & 55.6 (1.2) & 51.4 (1.0) \\   & INVRAT & 10.0 (n/a) & 34.9 (1.5) & 45.6 (0.2) & 39.5 (1.0) & - & - & - & - \\  & Inter\_RAT & 12.6 (0.8) & 34.6 (0.8) & 48.2 (0.4) & 40.2 (0.5) & 11.8 (1.5) & 31.6 (2.4) & 43.2 (3.5) & 36.4 (1.4) \\  & NIR & 8.3 (3.3) & 29.6 (0.0) & 19.8 (17.7) & 23.1 (18.6) & 9.8 (0.6) & 47.4 (1.6) & 54.9 (1.9) & 50.8 (1.0) \\  & MCD & 9.4 (0.8) & 60.9 (2.1) & 47.1 (3.0) & 33.1 (1.9) & 9.8 (0.3) & 49.3 (2.1) & 57.0 (3.5) & 52.7 (2.4) \\  & MRD (ours) & 10.1 (0.3) & 70.7 (2.0) & 57.6 (2.1) & **63.5** (1.9) & 9.7 (0.2) & 51.0 (1.6) & 58.2 (1.6) & **54.4** (1.6) \\   & RNP & 19.7 (0.2) & 38.3 (1.6) & 60.8 (3.1) & 47.0 (2.1) & 20.3 (0.4) & 33.3 (1.0) & 79.7 (2.5) & 47.0 (1.4) \\  & INVRAT & 20.0 (n/a) & 24.0 (1.3) & 55.2 (2.3) & 33.5 (1.6) & - & - & - & - \\  & Inter\_RAT & 20.8 (0.6) & 26.3 (0.6) & 59.1 (0.8) & 36.4 (0.7) & 19.6 (1.4) & 23.6 (0.7) & 54.1 (2.6) & 32.9 (0.4) \\  & NIR & 19.5 (1.0) & 32.9 (0.0) & 51.8 (14.8) & 42.0 (1.1) & 20.0 (0.3) & 33.0 (0.9) & 77.6 (1.7) & 46.3 (1.2) \\  & MCD & 19.6 (0.5) & 41.2 (1.4) & 65.0 (2.8) & 50.5 (1.9) & 19.7 (0.4) & 33.8 (1.3) & 78.5 (2.1) & 47.3 (1.6) \\  & MRD (ours) & 19.6 (0.7) & 44.2 (1.9) & 69.6 (1.0) & **54.1** (1.7) & 19.4 (0.1) & 35.0 (0.4) & 79.5 (1.0) & **48.6** (0.6) \\   & RNP & 29.1 (0.9) & 24.2 (5.2) & 56.7 (10.9) & 34.0 (7.0) & 29.5 (1.7) & 18.1 (8.7) & 64.2 (31.7) & 28.2 (13.7) \\  & INVRAT & 20.0 (n/a) & 20.9 (1.1) & 71.6 (0.4) & 32.3 (1.3) & - & - & - & - \\  & Inter\_RAT & 30.4 (0.4) & 21.8 (0.1) & 66.1 (0.8) & 32.8 (0.1) & 29.8 (1.2) & 18.1 (0.5) & 63.1 (1.6) & 28.1 (0.7) \\  & NIR & 30.0 (3.7) & 17.2 (8.6) & 42.6 (22.4) & 24.5 (12.4) & 29.4 (0.9) & 12.3 (10.6) & 43.6 (37.6) &framework, exploring how our method can be applied to broader fields such as computer vision and graph learning is a worthwhile future direction.

One limitation is that, although some researchers have found that rationalization can benefit large language models (LLMs) by providing high quality data (please refer to Appendix A.11), this paper does not involve LLMs. Given the recent remarkable success of LLMs, exploring how our MRD can aid in training trustworthy LLMs is another avenue worth pursuing.

## 8 Acknowledgements

This work is supported by the National Natural Science Foundation of China under grants 62376103, 62302184, 62436003 and 62206102; the Science and Technology Support Program of Hubei Province under grant 2022BAA046; Hubei Science and Technology Talent Service Project under grant 2024DJC078; and Ant Group through CCF-Ant Research Fund.

We thank Lang Gao and Yang Qiu for their help during the rebuttal period. They helped a lot with the experiments on LLMs and graph data.

We are also grateful for the valuable suggestions provided by the anonymous reviewers, which greatly helped to improve the quality of this paper.

    &  &  \\   & S & P & R & F1 & S & P & R & F1 \\   & RNP & 10.0 (n/a) & 54.0(1.4) & 20.3 (1.9) & 25.2(1.7) & 10.0 (n/a) & 49.1(3.2) & 28.7(2.2) & 32.0(2.5) \\   & VIB\({}^{*}\) & 10.0 (n/a) & 52.6 (2.0) & 26.0 (2.3) & 23.9 (2.1) & 10.0 (n/a) & 54.2 (2.9) & 31.6 (1.9) & 37.7 (2.8) \\  & A2R\({}^{*}\) & 10.0 (n/a) & 55.0 (0.8) & 25.8 (1.6) & 34.3 (1.4) & 10.0 (n/a) & 54.2 (2.9) & 31.6 (1.9) & 37.7 (2.8) \\  & INVRT\({}^{*}\) & 10.0 (n/a) & 56.4 (2.5) & 27.3 (1.2) & 36.7 (2.1) & 10.0 (n/a) & 49.6 (3.1) & 27.5 (1.9) & 33.2 (2.6) \\  & CR\({}^{*}\) & 10.0 (n/a) & 59.7 (1.9) & 31.6 (1.6) & 39.0 (1.5) & 10.0 (n/a) & 68.0 (2.9) & 42.0 (3.0) & 49.1 (2.8) \\  & MRD (ours) & 10.6 (1.1) & 75.0 (15.2) & 43.0 (6.3) & **54.6** (8.8) & 9.9 (0.5) & 71.7 (4.6) & 44.8 (4.3) & **55.1** (4.5) \\   

Table 4: Results with BERT. We follow CR to set \(S 10\%\). \(*\): results obtained from Table 11 of CR.