# Does a sparse ReLU network training problem

always admit an optimum?

 Quoc-Tung Le

Elisa Riccietti

Remi Gribonval

Univ. Lyon, Inria, CNRS, ENS de Lyon, UCB Lyon 1, LIP UMR 5668, F-69342 Lyon, France

###### Abstract

Given a training set, a loss function, and a neural network architecture, it is often taken for granted that optimal network parameters exist, and a common practice is to apply available optimization algorithms to search for them. In this work, we show that the existence of an optimal solution is not always guaranteed, especially in the context of _sparse_ ReLU neural networks. In particular, we first show that optimization problems involving deep networks with certain sparsity patterns do not always have optimal parameters, and that optimization algorithms may then diverge. Via a new topological relation between sparse ReLU neural networks and their linear counterparts, we derive -using existing tools from real algebraic geometry- an algorithm to verify that a given sparsity pattern suffers from this issue. Then, the existence of a global optimum is proved for every concrete optimization problem involving a one-hidden-layer sparse ReLU neural network of output dimension one. Overall, the analysis is based on the investigation of two topological properties of the space of functions implementable as sparse ReLU neural networks: a best approximation property, and a closedness property, both in the uniform norm. This is studied both for (finite) domains corresponding to practical training on finite training sets, and for more general domains such as the unit cube. This allows us to provide conditions for the guaranteed existence of an optimum given a sparsity pattern. The results apply not only to several sparsity patterns proposed in recent works on network pruning/sparsification, but also to classical dense neural networks, including architectures not covered by existing results.

## 1 Introduction

The optimization phase in deep learning consists in minimizing an objective function w.r.t. the set of parameters \(\) of a neural network (NN). While it is arguably sufficient for optimization algorithms to find local minima in practice, training is also expected to achieve the infimum in many situations (for example, in overparameterized regimes networks are trained to zero learning error).

In this work, we take a step back and study a rather fundamental question: _Given a deep learning architecture possibly with sparsity constraints, does its corresponding optimization problem actually admit an optimal \(^{*}\)?_ The question is important for at least two reasons:

1. Practical viewpoint: If the problem does not admit an optimal solution, optimized parameters necessarily diverge to infinity to approximate the infimum (which always exists). This phenomenon has been studied thoroughly in previous works in other contexts such as tensor decomposition , robust principal component analysis , sparse matrix factorization  and also deep learning itself [26; 23; 12]. It causes inherent numerical instability for optimization algorithms. Moreover,the answer to this question depends on the architecture of the neural networks (specified by the number of layers, layers width, activation function, and so forth). A response to this question might suggest a guideline for model and architecture selection.
2. Theoretical viewpoint: the existence of optimal solutions is crucial for the analysis of algorithms and their properties (for example, the properties of convergence, or the characterization of properties of the optimum, related to the notion of implicit bias).

One usual practical (and also theoretical) trick to bypass the question of the existence of optimal solutions is to add a regularization term, which is usually coercive, e.g., the \(L^{2}\) norm of the parameters. The existence of optimal solutions then follows by a classical argument on the extrema of a continuous function in a compact domain. Nevertheless, there are many settings where minimizing the regularized version might result in a high value of the loss since the algorithm has to make a trade-off between the loss and the regularizer. Such a scenario is discussed in Example 3.1. Therefore, studying the existence of optimal solutions without (explicit) regularization is also a question of interest.

Given a training set \(\{(x_{i},y_{i})\}_{i=1}^{P}\), the problem of the existence of optimal solutions can be studied from the point of view of the set of functions implementable by the considered network architecture on the finite input domain \(=\{x_{i}\}_{i=1}^{P}\). This is the case since the loss is usually of the form \((f_{}(x_{i}),y_{i})\) where \(f_{}\) is the realization of the neural network with parameters \(\). Therefore, the loss involves directly the image of \(\{x_{i}\}_{i=1}^{P}\) under the function \(f_{}\). For theoretical purposes, we also study the function space on the domain \(=[-B,B]^{d},B>0\). In particular, we investigate two topological properties of these function spaces, both w.r.t. the infinity norm \(\|\|_{}\): the best approximation property (BAP), i.e., the guaranteed existence of an optimal solution \(^{*}\), and the closedness, a necessary property for the BAP. These properties are studied in Section 3 and Section 4, respectively. Most of our analysis is dedicated to the case of _regression problems_. We do make some links to the case of classification problems in Section 3.

We particularly focus on analyzing the function space associated with _(structured) sparse ReLU neural networks_, which is motivated by recent advances in machine learning witnessing a compelling empirical success of sparsity based methods in NNs and deep learning techniques, such as pruning , sparse designed NN , or the lottery ticket hypothesis  to name a few. Our approach exploits the notion of networks _either with fixed sparsity level or with fixed sparsity pattern (or support)_. This allows us to establish results covering both classical NNs (whose weights are not contrained to be sparse) and sparse NNs architectures. Our main contributions are:

1. **To study the BAP (i.e., the existence of optimal solutions) in practical problems (finite \(\))**: we provide a necessary condition and a sufficient one on the architecture (embodied by a sparsity pattern) to guarantee such existence. As a particular consequence of our results, we show that: a) _for one-hidden-layer NNs with a fixed sparsity level_, the training problem on a finite data set _always admits an optimal solution_ (cf. Theorem 3.4 and Corollary 3.1); b) however, practitioners should be cautious since _there also exist fixed sparsity patterns that do not guarantee the existence of optimal solutions_ (cf. Theorem 3.1 and Example 3.1). In the context of an emerging emphasis on _structured sparsity_ (e.g. for GPU-friendliness), this highlights the importance of choosing adequate sparsity patterns.
2. **To study the closedness of the function space on \(=[-B,B]^{d}\)**. As in the finite case, we provide a necessary condition and a sufficient one for the closedness of the function space of ReLU NNs with a fixed sparsity pattern. In particular, our sufficient condition on one-hidden-layer networks generalizes the closedness results of [26, Theorem 3.8] on "dense' one-hidden-layer ReLU NNs to the case of sparse ones, either with fixed sparsity pattern (cf. Theorem 4.2, Corollary 4.1 and Corollary 4.2) or fixed sparsity level (Corollary 4.3). Moreover, our necessary condition (Theorem 4.1), which is also applicable to deep architectures, exhibits sparsity structures failing the closedness property.

Table 1 and Table 2 summarize our results and their positioning with respect to existing ones. Somewhat surprisingly, the necessary conditions in both domains (\(\) finite and \(=[-B,B]^{d}\)) are identical. Our necessary/sufficient conditions also suggest a relation between sparse ReLU neural networks and their linear counterparts.

The rest of this paper is organized as follows: Section 2 discusses related works and introduces notations; the two technical sections, Section 3 and Section 4, presents the results for the case \(\) finite set and \(=[-B,B]^{d}\) respectively.

## 2 Related works

The fact that optimal solutions may not exist in tensor decomposition problems is well-documented . The cause of this phenomenon (also referred to as _ill-posedness_) is the non-closedness of the set of tensors of order at least three and of rank at least two. Similar phenomena were shown to happen in various settings such as matrix completion [11, Example 2], robust principal component analysis  and sparse matrix factorization [18, Remark A.1]. Our work indeed establishes bridges between the phenomenon on sparse matrix factorization  and on sparse ReLU NNs.

There is also an active line of research on the best approximation property and closedness of function spaces of neural networks. Existing results can be classified into two categories: _negative_ results, which demonstrate the non-closedness and _positive_ results for those showing the closedness or best approximation property of function spaces of NNs. Negative results can notably be found in

   &  & Activation functions &  &  &  \\   & & & & & \\  Theorem 3.4 & Sparse feed-forward network & ReLU & finite set & \((^{P d_{o}},\|\|)\), arbitrary \(\|\|\) & \(\) \\  
 & Feed-forward network & Heavyside & \(^{d}\) & \((L^{p}(),\|\|_{L^{p}}), p[1,)\) & \(\) \\ 
\({}^{}\) & Feed-forward network, Residual feed-forward network & ReLU & \(^{d}\) & \((L^{p}_{n}(),\|\|_{L^{p}})\), \(p=2\), \(\) is a measure with compact support and is \(\) target function continuous w.t. Lebesgue measure & \(\) (if the target function is continuous) \\ 
 & Feedforward network & ReLU, pReLU & \([-B,B]^{d}\) & \((C^{0}(),\|\|_{})\) & \(\) \\   Corollary 4.1\({}^{}\) & Feed-forward network & ReLU & \([-B,B]^{d}\) & \((C^{0}(),\|\|_{})\) & \(\) \\  Corollary 4.2 & Sparse feed-forward network & ReLU & \([-B,B]^{d}\) & \((C^{0}(),\|\|_{})\) & \(\) \\  

Table 1: **Closedness** results. All results are established for _one-hidden-layer_ architectures with _scalar-valued_ output, except \(\) (which is valid for one-hidden-layer architectures with vector-valued output). In \(\), if the architecture is simply a feed-forward network, then the result is valid for any \(p>1\).

   &  & Activation functions &  &  \\   & & & & \(L\) & \(N_{L-1}\) & \(N_{L}\) \\ 
 & Feedforward network & Sigmoid & \((C^{0}(),\|\|_{})\) & \(\) & \(\) \\  & & & \((L=2)\) & \((N_{L-1} 2)\) & \((N_{L}=1)\) \\ 
\({}^{}\) & Feedforward network & ReLU & \((^{N_{L}^{P}},\|\|)\), \(P=6\) & \(\) & \(\) & \(\) \\  & & & \((L=2)\) & \((N_{L-1}=2)\) & \((N_{L}=2)\) \\ 
 & Feedforward network & 
 sigmoid, tanh, arctan, \\ ISRLU, ISRU, ISRU \\  & \((C^{0}(),\|\|_{})\) & & & \\   & & & \((L^{p}(),\|\|_{L^{p}})\) & & \\  & & & \((N_{L-1} 2)\) & \((N_{L}=1)\) \\ 
 & Feedforward network &  ISRLU \\  & 
 \((W^{2,p}(),\|\|_{L^{p}})\) \\ \( p[1,]\) \\  & \(\) & \(\) & \(\) \\   & & & \((W^{p}(),\|\|_{L^{p}})\) & & \\   & & & \((W^{p}(),\|\|_{L^{p}})\) & & \\   & & & \((W^{p}(),\|\|_{L^{p}})\) & & \\   & & & \((W^{p}(),\|\|_{L^{p}})\) & & \\   Theorem 4.1\({}^{}\) & Sparse feedforward network & ReLU & \((C^{0}(),\|\|_{})\) & \(\) & \(\) & \(\) \\  Theorem 3.1\({}^{}\) & Sparse feedforward network & ReLU & \((^{N_{L}^{P}},\|\|)\) & \(\) & \(\) & \(\) \\  

Table 2: **Non-closedness** results (notations in Section 2). Previous results consider \(=[-B,B]^{d}\); ours cover: \(\) a finite \(\) with \(P\) points; \(\) a bounded \(\) with non-empty interior (this includes \(=[-B,B]^{d}\)).

, showing that the set of functions implemented as conventional multilayer perceptrons with various activation functions such as Inverse Square Root Linear Unit (ISRLU), Inverse Square Root Unit (ISRU), parametric ReLU (pReLU), Exponential Linear Unit (ELU) [26, Table 1] is not a closed subset of classical function spaces (e.g., the Lebesgue spaces \(L^{p}\), the set of continuous functions \(C^{0}\) equipped with the sup-norm, or Sobolev spaces \(W^{k,p}\)). In a more practical setting,  hand-crafts a dataset of six points which makes the training problem of a dense one-hidden-layer neural network not admit any solution. Positive results are proved in , which establish both the closedness and/or the BAP. The BAP implies closedness [12, Proposition 3.1][26, Section 3] (but the converse is not true, see Appendix D) hence the BAP can be more difficult to prove than closedness. So far, the only architecture proved to admit the best approximation property (and thus, also closedness) is _one-hidden-layer neural networks_ with _heavyside activation_ function and _scalar-valued output_ (i.e., output dimension equal to _one_)  in \(L^{p}(), p[1,]\). If one allows additional assumptions such as the target function \(f\) being continuous, then BAP is also established for one-hidden layer and residual one-hidden-layer NNs with ReLU activation function . In all other settings, to the best of our knowledge, the only property proved in the literature is closedness, but the BAP remains elusive. We compare our results with existing works in Tables 1 and 2.

In machine learning, there is an ongoing endeavour to explore sparse deep neural networks, as a prominent approach to reduce memory and computation overheads inherent in deep learning. One of its most well-known methods is Iterative Magnitude Pruning (IMP), which iteratively trains and prunes connections/neurons to achieve a certain level of sparsity. This method is employed in various works , and is related to the so-called Lottery Ticket Hypothesis (LTH) . The main issue of IMP is its running time: one typically needs to perform many steps of pruning and retraining to achieve a good trade-off between sparsity and performance. To address this issue, many works attempt to identify the sparsity patterns of the network before training. Once they are found, it is sufficient to train the sparse neural networks once. These _pre-trained_ sparsity patterns can be found through algorithms  or leveraging the sparse structure of well-known fast linear operators such as the Discrete Fourier Transform . Regardless of the approaches, these methods are bound to train a neural network with _fixed sparsity pattern_ at some points. This is a particular motivation for our work and our study on the best approximation property of sparse ReLU neural networks with fixed sparsity pattern.

NotationsIn this work, \([\![n]\!]:=\{1,,n\}\). For a matrix \(^{m n}\), \([i,j]\) denotes the coefficient at the index \((i,j)\); for subsets \(S_{r}[\![m]\!],S_{c}[\![n]\!]\), \([S_{r},:]\) (resp. \([:,S_{c}]\)) is a matrix of the same size as \(\) and agrees with \(\) on rows in \(S_{r}\) (resp. columns in \(S_{c}\)) of \(\) while its remaining coefficients are zero. The operator \(():=\{(,k)[,k] 0\}\) returns the _support_ of the matrix \(\). We denote \(_{m n}\) (resp. \(_{m n}\)) an all-one (resp. all-zero) matrix of size \(m n\).

An architecture with fixed sparsity pattern is specified via \(=(I_{L},,I_{1})\), a collection of binary masks \(I_{i}\{0,1\}^{N_{i} N_{i-1}},1 i L\), where the tuple \((N_{L},,N_{0})\) denotes the dimensions of the input layer \(N_{0}=d\), hidden layers \((N_{L-1},,N_{1})\) and output layer (\(N_{L}\)), respectively. The binary mask \(I_{i}\) encodes the support constraints on the \(i\)th weight matrix \(_{i}\), i.e., \(I_{i}[,k]=0\) implies \(_{i}[,k]=0\). It is also convenient to think of \(I_{i}\) as the set \(\{(,k) I_{i}[,k]=1\}\), a subset of \([\![N_{i}]\!][\![N_{i-1}]\!]\). We will use these two interpretations (binary mask and subset) interchangeably and the meaning should be clear from context. We will even abuse notations by denoting \(I_{l}_{N_{l} N_{i-1}}\). Because the support constraint \(I\) can be thought as a binary matrix, the notation \(I[S_{r},:]\) (resp. \(I[:,S_{c}]\)) represents the support constraint of \(I S_{r}[\![n]\!]\) (resp. \(I[\![n]\!] S_{c}\)).

The space of parameters on the sparse architecture \(\) is denoted \(_{}\), and for each \(_{}\), \(_{}:^{N_{0}}^{N_{L}}\) is the function implemented by the ReLU network with parameter \(\):

\[_{}:x^{N_{0}}_{}(x):= _{L}((_{1}x+_{1})+ _{L-1})+_{L}^{N_{L}}\] (1)

where \((x)=(0,x)\) is the ReLU activation.

Finally, for a given architecture \(\), we define

\[_{}=\{_{L}_{1}(_{i}) I_{i},i[\![L]\!]\}^{N_{L}  N_{0}}\] (2)

the set of matrices factorized into \(L\) factors respecting the support constraints \(I_{i},i[\![L]\!]\). In fact, \(_{}\) is the set of linear operators implementable as _linear_ neural networks (i.e., with \(=\) instead of the ReLU in (1), and no biases) with parameters \(_{}\).

Analysis of fixed support ReLU neural networks for finite \(\{\}\)

The setting of a finite set \(=\{x_{i}\}_{i=1}^{P}\) is common in many practical machine learning tasks: models such as (sparse) neural networks are trained on often large (but finite) annotated dataset \(=\{(x_{i},y_{i})\}_{i=1}^{P}\). The optimization/training problem usually takes the form:

\[}()=_{i=1}^{P} (_{}(x_{i}),y_{i}),\] (3)

where \(\) is a loss function measuring the similarity between \(_{}(x_{i})\) and \(y_{i}\). A natural question that we would like to address for this task is:

**Question 3.1**.: _Under which conditions on \(\), the prescribed sparsity pattern for \(\), does the training problem of sparse neural networks admit an optimal solution for any finite data set \(\)?_

We investigate this question both for parameters \(\) constrained to satisfy a _fixed_ sparsity pattern \(\), and in the case of a fixed sparsity level, see e.g. Corollary 4.3.

After showing in Section 3.1 that the answer to Question 3.1 is intimately connected with the closedness of the function space of neural networks with architecture \(\), we establish in Section 3.2 that this closedness implies the closedness of the matrix set \(_{}\) (a property that can be checked using algorithms from real algebraic geometry, see Section 3.3). We also provide concrete examples of support patterns \(\) where closedness provably fails, and neural network training can diverge. Section 3.4 presents sufficient conditions for closedness that enable us to show that an optimal solution always exists on scalar-valued one-hidden-layer networks under a constraint on the sparsity level of each layer.

### Equivalence between closedness and best approximation property

To answer Question 3.1, it is convenient to view \(\) as the matrix \([x_{1},,x_{P}]^{d P}\) and to consider the function space implemented by neural networks with the given architecture \(\) on the input domain \(\) in dimension \(d=N_{0}\), with output dimension \(N_{L}\), defined as the set

\[_{}():=\{_{}() _{}\}^{N_{L} P}\] (4)

where the matrix \(_{}():=_{}(x_{1}),, _{}(x_{P})^{N_{L} P}\) is the image under \(_{}\) of \(\).

We study the closedness of \(_{}()\) under the usual topology induced by any norm \(\|\|\) of \(^{N_{L} P}\). This property is interesting because if \(_{}()\) is closed for any \(=\{x_{i}\}_{i=1}^{P}\), then an optimal solution is guaranteed to exist for any \(\) under classical assumptions of \((,)\). The following result is not difficult to prove, we nevertheless provide a proof in Appendix B.1 for completeness.

**Proposition 3.1**.: _Assume that, for any fixed \(y^{N_{L}}\), \((,y):^{N_{L}}\) is continuous, coercive and that \(y=_{y^{}}(y^{},y)\). For any sparsity pattern \(\) with input dimension \(N_{0}=d\) the following properties are equivalent:_

1. _irrespective of the training set, problem (_3_) under the constraint_ \(_{}\) _has an optimal solution;_
2. _for every_ \(P\) _and every_ \(^{d P}\)_, the function space_ \(_{}()\) _is a closed subspace of_ \(^{N_{L} P}\)_._

The assumption on \(\) is natural and realistic in _regression_ problems: any loss function based on any norm on \(^{d}\) (e.g. \((y^{},y)=\|y^{}-y\|\)), such as the quadratic loss, satisfies this assumption. In the classification case, using the soft-max after the last layer together with the cross-entropy loss function indeed leads to an optimization problem with no optimum (regardless of the architecture) when given a _single_ training pair. This is due to the fact that changing either the bias or the scales of the last layer can lead the output of the soft-max arbitrarily close to an ideal Dirac mass. It is an interesting challenge to identify whether sufficiently many and diverse training samples (as in concrete learning scenarios) make the problem better posed, and amenable to a relevant closedness analysis.

In light of Proposition 3.1 we investigate next the closedness of \(_{}()\) for finite \(\).

### A necessary closedness condition for fixed support ReLU networks

Our next result reveals connections between the closedness of \(_{}()\) for finite \(\) and the closedness of \(_{}\), the space of sparse matrix products with sparsity pattern \(\).

**Theorem 3.1**.: _If \(_{}()\) is closed for every finite \(\) then \(_{}\) is closed._

Theorem 3.1 is a direct consequence of (and in fact logically equivalent to) the following lemma:

**Lemma 3.2**.: _If \(_{}\) is not closed then there exists a set \(^{d}\), \(d=N_{0}\), of cardinality at most \(P(3N_{0}4^{_{i=1}^{L-1}N_{i}}+1)^{N_{0}}\) such that \(_{}()\) is not closed._

Sketch of the proof.: Since \(_{}\) is not closed, there exists \(_{}}_{ }\) (\(}\) is the closure of the set \(\)). Considering \(f(x)x\), we construct a set \(=\{x_{i}\}_{i=1}^{P}\) such that \([f(x_{1}),,f(x_{P})]_{}()} _{}()\). Therefore, \(_{}()\) is not closed. 

The proof is in Appendix B.2. Besides showing a topological connection between \(_{}\) (NNs with ReLU activation) and \(_{}\) (linear NNs), Theorem 3.1 leads to a simple example where \(_{}\) is not closed.

**Example 3.1** (LU architecture).: _Consider \(=(I_{2},I_{1})\{0,1\}^{d d}\{0,1\}^{d d}\) where \(I_{1}=\{(i,j) 1 i j d\}\) and \(I_{2}=\{(i,j) 1 j i d\}\). Any pair of matrices \(_{2},_{1}^{d d}\) such that \((_{i}) I_{i},i=1,2\) are respectively lower and upper triangular matrices. Therefore, \(_{}\) is the set of matrices that admit an exact lower - upper (LU) factorization/decomposition. That explains its name: **LU** architecture. This set is well known to a) contain an open and dense subset of \(^{d d}\); **b)** be strictly contained in \(^{d d}\)[13, Theorem 3.2.1][25, Theorem 1]. Therefore, \(_{}\) is not closed and by the contraposition of Theorem 3.1 we conclude that there exists a finite set \(\) such that \(_{}()\) is not closed._

Let us illustrate the impact of the non-closedness in Example 3.1 via the behavior during the training of a fixed support one-hidden-layer neural network with the LU support constraint \(\). This network is trained to learn the linear function \(f(x)x\) where \(^{d d}\) is an anti-diagonal _identity_ matrix. Using the necessary and sufficient condition of **LU** decomposition existence [25, Theorem 1], we have that \(_{}}_{ }\) as in the sketch proof of Lemma 3.2. Given network parameters \(\) and a training set, approximation quality can be measured by the relative loss: \((_{i=1}^{P}\|_{}(x_{i})-y_{i}\|_{2}^{2}/\|y_{ i}\|_{2}^{2})\).

Figure 1 illustrates the behavior of the relative errors of the training set, validation set and the sum of weight matrices norm along epochs, using Stochastic Gradient Descent (SGD) with batch size \(3000\), learning rate \(0.1\), momentum \(0.9\) and four different weight decays (the hyperparameter controlling the \(L^{2}\) regularizer) \(\{0,10^{-4},5 10^{-4},10^{-3}\}\). The case \(=0\) corresponds to the _unregularized_ case. Our training and testing sets contain each \(P=10^{5}\) samples generated independently as \(x_{i}([-1,1]^{d})\) (\(d=100\)) and \(y_{i}x_{i}\).

Example 3.1 and Figure 1 also lead to two interesting remarks: while the \(L^{2}\) regularizer (weight decay) does prevent the parameter divergence phenomenon, the empirical loss is improved when using the non-regularized version. This is the situation where adding a regularization term might be detrimental, as stated earlier. More interestingly, the size of the dataset is \(10^{5}\), which is much smaller than the theoretical \(P\) in Lemma 3.2. It is thus interesting to see if we can reduce the theoretical value of \(P\), which is currently exponential w.r.t. to the input dimension.

Figure 1: Training a one-hidden-layer fixed support (LU architecture) neural network with different regularization hyperparameters \(\) (we use weight decay, i.e., an \(L^{2}\) regularizer). Subfigures a)-b) show the relative loss (the lower, the better) for training (empirical loss) and testing (validation loss) respectively. Subfigure c) shows the norm of two weight matrices. The experiments are conducted \(10\) times to produce the error bars in all figures (almost invisible due to a small variability).

### The closedness of \(_{}\) is algorithmically decidable

Theorem 3.1 leads to a natural question: given \(\), how to check the closedness of \(_{}\), a subset of \(^{N_{L} N_{0}}\). To the best of our knowledge, there is not any study on the closedness of \(_{}\) in the literature. It is, thus, not known whether deciding on the closedness of \(_{}\) for a given \(\) is polynomially tractable. In this work, we show it is at least decidable with a doubly-exponential algorithm. This algorithm is an application of _quantifier elimination_, an algorithm from real algebraic geometry .

**Lemma 3.3**.: _Given \(=(I_{L},,I_{1})\), the closedness of \(_{}\) is decidable with an algorithm of complexity \(O((4L)^{C^{k-1}})\) where \(k=N_{L}N_{0}+1+2_{i=1}^{L}|L_{i}|\) and \(C\) is a universal constant._

We prove Lemma 3.3 in Appendix B.4. Since the knowledge of \(\) is usually available (either fixed before training [19; 31; 4; 21; 3] or discovered by a procedure before re-training [10; 14; 32]), the algorithm in Lemma 3.3 is able to verify whether the training problem might not admit an optimum. While such a doubly exponential algorithm in Lemma 3.3 is seemingly impractical in practice, small toy examples (for example, Example 3.1 with \(d=2\)) can be verified using Z3Prover1, a software implementing exactly the algorithm in Lemma 3.3. However, Z3Prover is already unable to terminate when run on the \(\) architecture of Example 3.1 with \(d=3\). This calls for more efficient algorithms to determine the closedness of \(_{}\) given \(\). The same algorithmic question can be also asked for \(_{}\). We leave these problems (in this general form) as open questions.

In fact, if such a polynomial algorithm (to decide the closedness of \(_{}\)) exists, it can be used to answer the following interesting question:

**Question 3.2**.: _If the supports of the weight matrices are randomly sampled from a distribution, what is the probability that the corresponding training problem potentially admits no optimal solutions?_

While simple, this setting does happen in practice since random supports/binary masks are considered a strong and common baseline for sparse DNNs training . Thanks to Theorem 3.1, if \(_{}\) is not closed then the support is "bad". Thus, to have an estimation of a _lower bound_ on the probability of "bad" supports, we could sample the supports from the given distribution and use the polynomial algorithm in question to _decide_ if \(_{}\) is closed. Unfortunately, the algorithm in Lemma 3.3 has doubly exponential complexity, thus hindering its practical use. However, for one-hidden-layer NNs, there is a _polynomial_ algorithm to _detect_ non-closedness: intuitively, if the support constraint is "locally similar" to the \(\) structure, then \(_{}\) is not closed. This result is elaborated in Appendix B.5 and Lemma B.8. The resulting detection algorithm can have false negatives (i.e., it can fail to detect more complex configurations where \(_{}\) is not closed) but no false positive.

Figure 2: Probability of _detectable_ “bad” support constraints sampled from uniform distribution over \(100\) samples.

We test this algorithm on a one-hidden layer ReLU network with two \(100 100\) weight matrices. We randomly choose their supports whose cardinality are \(p_{1} 100^{2}\) and \(p_{2} 100^{2}\) respectively, with \((p_{1},p_{2})\{0.1,0.2,,1.0\}\)2. For each pair \((p_{1},p_{2})\), we sample \(100\) instances. Using the detection algorithm, we obtain Figure 2. The numbers in Figure 2 indicate the probability that a random support constraint \((I,J)\) has \(_{I,J}\) non-closed (as detected by the algorithm). This figure shows two things: 1) "Bad" architectures such as \(\) are not rare and one can (randomly) generate plenty of them. 2) At a sparse regime (\(a_{1},a_{2} 0.2\)), most of the random supports might lead to training problems without optimal solutions. We remind that the detection algorithm may give some false negatives. Thus, for less sparse regimes, it is possible that our heuristic fails to detect the non-closedness. The algorithm indeed gives a lower bound on the probability of finding non-closed instances. The code for Example 3.1, Question 3.2 and the algorithm in Lemma 3.3 is provided in .

### Best approximation property of scalar-valued one-hidden-layer sparse networks

So far, we introduced a necessary condition for the closedness (and thus, by Proposition 3.1, the best approximation property) of sparse ReLU networks, and we provided an example of an architecture \(\) whose training problem might not admit any optimal solution. One might wonder if there are architectures \(\) that _avoid_ the issue caused by the non-closedness of \(_{}\). Indeed, we show that for one-hidden-layer sparse ReLU neural networks with scalar output dimension (i.e., \(L=2,N_{2}=1\)), the existence of optimal solutions is guaranteed, _regardless of the sparsity pattern_.

**Theorem 3.4**.: _Consider scalar-valued, one-hidden-layer ReLU neural networks (i.e., \(L=2,N_{2}=1\)). For any support pairs \(=(I_{2},I_{1})\) and any finite set \(:=\{x_{1},,x_{P}\}\), \(_{}()\) is closed._

The proof of Theorem 3.4 is deferred to Appendix B.3. As a sanity check, observe that when \(L=2,N_{2}=1\), the necessary condition in Theorem 3.1 is satisfied. Indeed, since \(N_{2}=1\), \(_{}^{1 N_{0}}\) can be thought as a subset of \(^{N_{0}}\). Any \(_{}\) can be written as a sum: \(=_{i I_{2}}_{2}[i]_{1}[i,:]\), a decomposition of the product \(_{2}_{1}\), where \(_{2}[i],_{1}[i,:]^{N_{0}}, (_{1}[i,:]) I_{1}[i,:]\). Define \(:=_{i I_{2}}I_{1}[i,:][N_{0}]\) the union of row supports of the first weight matrix. It is easy to verify that \(_{}\) is isomorphic to \(^{||}\), which is closed. In fact, this argument only works for scalar-valued output, \(N_{2}=1\). Thus, there is no conflict between Theorem 3.1 and Theorem 3.4.

In practice, many approaches search for the best support \(\)_among a collection of possible supports_, for example, the approach of pruning and training  or the lottery ticket hypothesis . Our result for fixed support in Theorem 3.4 can be also applied in this case and is stated in Corollary 3.1. In particular, we consider a set of supports such that the support sizes (or sparsity ratios) of the layers are kept below a certain threshold \(K_{i},i=1,,L\). This constraint on the sparsity level of each layer is widely used in many works on sparse neural networks .

**Corollary 3.1**.: _Consider scalar-valued, one-hidden-layer ReLU neural networks. For any finite data set2\(=(x_{i},y_{i})_{i=1}^{P}\), problem (3) under the constraints \(\|_{i}\|_{0} K_{i},i=1,2\) has a minimizer._

Proof.: Denote \(\) the collection of sparsity patterns satisfying \(\|I_{i}\|_{0} K_{i},i=1,2\), so that a set of parameters satisfies the sparsity constraints \(\|_{i}\|_{0} K_{i},i=1,2\) if and only if the supports of the weight matrices belong to \(\). Therefore, to solve the optimization problem under sparsity constraints \(\|_{i}\|_{0} K_{i},i=1,2\), it is sufficient to solve the same problem for every sparsity pattern in \(\).

For each \(\), we solve a training problem with architecture \(\) on a given finite dataset \(\). Thanks to Theorem 3.4 and Proposition 3.1, the infimum is attained. We take the optimal solution corresponding to \(\) that yields the smallest value of the loss function \(\). This is possible because the set \(\) has a finite number of elements (the total number of possible sparsity patterns is finite). 

## 4 Analysis of fixed support ReLU networks on continuous domains

We now investigate closedness properties when the domain \(^{d}\) is no longer finite. Denoting \(_{}=\{_{}:^{N_{0}} ^{N_{L}}_{}^{}\}\) (with \(N_{0}=d\)) the functions that can be implemented on a given ReLU network architecture \(\), we are interested in \(_{}()=\{f_{|}:f_{}\}\), the restriction of elements of \(_{}\) to \(\). This is a natural extension of the set \(_{}()\) studied in the case of finite \(\).

Specifically, we investigate the closedness of \(_{}()\) in \((C^{0}(),\|\|_{})\) (the set of continuous functions on \(\) equipped with the supremum norm \(\|f\|_{}:=_{x}\|f(x)\|_{2}\)). Contrary to the previous section, we can no longer exploit Proposition 3.1 to deduce that the closedness property and the BAP are equivalent. The results in this section can be seen as a continuation (and also generalization) of the line of research on the topological property of function space of neural networks . In Section 4.1 and Section 4.2, we provide a necessary and a sufficient condition on \(\) for the closedness of \(_{}()\) in \((C^{0}(,\|\|_{})\) respectively. The condition of the former is valid for any depth, while that of the latter is applicable for one-hidden-layer networks (\(L=2\)). These results are established under various assumptions on \(\) (such as \(=[-B,B]^{d}\), or \(\) being bounded with non-empty interior) that will be specified in each result.

### A necessary condition for closedness of fixed support ReLU network

Theorem 4.1 states our result on the necessary condition for the closedness. Interestingly, observe that this result (which is proved in Appendix C.1) naturally generalizes Theorem 3.1. Again, closedness of \(_{}\) in \(^{N_{L} N_{0}}\) is with respect to the usual topology defined by any norm.

**Theorem 4.1**.: _Consider \(^{d}\) a bounded set with non-empty interior, and \(\) a sparse architecture with input dimension \(N_{0}=d\). If \(_{}()\) is closed in \((C^{0}(),\|\|_{})\) then \(_{}\) is closed in \(^{N_{L} N_{0}}\)._

Theorem 4.1 applies for any \(\) which is bounded and has non-empty interior. Thus, it encompasses not only the hypercubes \([-B,B]^{d},B>0\) but also many other domains such as closed or open \(^{d}\) balls. Similar to Theorem 3.1, Theorem 4.1 is interesting in the sense that it allows us to check the non-closedness of the function space \(_{}\) (a subset of the infinite-dimensional space \(C^{0}()\)) by checking that of \(_{}^{N_{L} N_{0}}\) (a finite-dimensional space). The latter can be checked using the algorithm presented in Lemma 3.3. Moreover, the \(\) architecture presented in Example 3.1 is also an example of \(\) whose function space is not closed in \((C^{0}(),\|\|_{})\).

### A sufficient condition for closedness of fixed support ReLU network

The following theorem is the main result of this section. It provides a sufficient condition to verify the closedness of \(_{}()\) for \(=[-B,B]^{d}\), \(B>0\) with one-hidden-layer sparse ReLU neural networks.

**Theorem 4.2**.: _Consider \(=[-B,B]^{d}\), \(N_{0}=d\) and a sparsity pattern \(=(I_{2},I_{1})\) such that:_

1. _There is no support constraint for the weight matrix of the second layer,_ \(_{2}\)_:_ \(I_{2}=_{N_{2} N_{1}}\)_;_
2. _For each non-empty set of hidden neurons,_ \(S N_{1}\)_,_ \(_{_{S}}\) _is closed in_ \(^{N_{2} N_{1}},\) _where_ \(_{S}:=(I_{2}[:,S],I_{1}[S,:])\) _is the support constraint restricted to the sub-network with hidden neurons in_ \(S\)_._

_Then the set \(_{}()\) is closed in \((C^{0}(),\|\|_{})\)._

Both conditions in Theorem 4.2 can be verified algorithmically: while the first one is trivial to check, the second one requires us to check the closedness of at most \(2^{N_{1}}\) sets \(_{_{S}}\) (because there are at most \(2^{N_{1}}\) subsets of \( N_{1}\)), which is still algorithmically possible (although perhaps practically intractable) with the algorithm of Lemma 3.3. Apart from its algorithmic aspect, we present two interesting corollaries of Theorem 4.2. The first one, Corollary 4.1, is about the closedness of the function space of fully connected (i.e., with no sparsity constraint) one-hidden-layer neural networks.

**Corollary 4.1** (Closedness of fully connected one-hidden-layer ReLU networks _of any output dimension).: _Given \(=(_{N_{2} N_{1}},_{N_{1} N_{0}})\), the set \(_{}\) is closed in \((C^{0}([-B,B]^{d}),\|\|_{})\) where \(d=N_{0}\)._

Proof.: The result follows from Theorem 4.2 once we check if its assumptions hold. The first one is trivial. To check the second, observe that for every non-empty set of hidden neurons \(S N_{1}\), the set \(_{_{S}}^{N_{2} N_{0}}\) is simply the set of matrices of rank at most \(|S|\), which is closed for any \(S\). 

Corollary 4.2 states the closedness of scalar-valued, one-hidden-layer sparse ReLU NNs. In a way, it can be seen as the analog of Theorem 3.4 for \(=[-B,B]^{d}\).

**Corollary 4.2** (Closedness of _fixed support_ one-hidden-layer ReLU networks with scalar output).: _Given any input dimension \(d=N_{0} 1\), any number of hidden neurons \(N_{1} 1\), scalar output dimension \(N_{2}=1\), and any prescribed supports \(=(I_{2},I_{1})\), the set \(_{}\) is closed in \((C^{0}([-B,B]^{d}),\|\|_{})\).__Sketch of the proof._ If there exists a hidden neuron \(i N_{1}\) such that \(I_{2}[i]=0\) (i.e., \(i I_{2}\): \(i\) is not connected to the only output of the network), we have: \(_{}=_{^{}}\) where \(^{}=_{S},S= N_{1}\{i\}\). By repeating this process, we can assume without loss of generality that \(I_{2}[i]=_{1 N_{1}}\). That is the first condition of Theorem 4.2.

Therefore, it is sufficient to verify the second condition of Theorem 4.2. Consider any non-empty set of hidden neurons \(S N_{1}\), and define \(:=_{i S}I[i,:] N_{0}\) the union of row supports of \(I_{1}[S,:]\). It is easy to verify that \(_{_{S}}\) is isomorphic to \(^{}\), which is closed. The result follows by Theorem 4.2. For a more formal proof, readers can find an inductive one in Appendix C.3. 

In fact, both Corollary 4.1 and Corollary 4.2 generalize [26, Theorem 3.8], which proves the closedness of \(_{}([-B,B]^{d})\) when \(I_{2}=_{1 N_{1}},I_{1}=_{N_{1} N_{0}}\) (classical fully connected one-hidden-layer ReLU networks with output dimension equal to one).

To conclude, let us consider the analog to Corollary 3.1: we study the function space implementable as a sparse one-hidden-layer network with constraints on the _sparsity level_ of each layer (i.e., \(\|_{i}\|_{0} K_{i},i=1,2\).

**Corollary 4.3**.: _Consider scalar-valued, one-hidden-layer ReLU networks \((L=2,N_{2}=1,N_{1},N_{0})\) with \(^{0}\) constraints \(\|_{1}\|_{0} K_{1},\|_{2}\|_{0} K_{2}\) for some constants \(K_{1},K_{2}\). The function space \(([-B,B]^{d})\) associated with this architecture is closed in \((C^{0}([-B,B]^{N_{0}}),\|\|_{})\)._

Proof.: Denote \(:=\{(I_{2},I_{1}) I_{2} 1  N_{1},I_{1} N_{1}  N_{0},|I_{1}| K_{1},|I_{2}| K_{2}\}\) the set of sparsity patterns respecting the \(^{0}\) constraints, so that \(([-B,B]^{d})=_{}_{ }([-B,B]^{d})\). Since \(\) is finite and \(,_{}([-B,B]^{d})\) is closed (Corollary 4.2), the result is proved. 

## 5 Conclusion

In this paper, we study the somewhat overlooked question of the existence of an optimal solution to sparse neural network training problems. The study is accomplished by adopting the point of view of topological properties of the function spaces of such networks on two types of domains: a finite domain \(\), or (typically) a hypercube. On the one hand, our investigation of the BAP and the closedness of these function spaces reveals the existence of _pathological_ sparsity patterns that fail to have optimal solutions on some instances (cf Theorem 3.1 and Theorem 4.1) and thus possibly cause instabilities in optimization algorithms (see Example 3.1 and Figure 1). On the other hand, we also prove several positive results on the BAP and closedness, notably for sparse one-hidden-layer ReLU neural networks (cf. Theorem 3.4 and Theorem 4.2). These results provide new instances of network architectures where the BAP is proved (cf Theorem 3.4) and substantially generalize existing ones (cf. Theorem 4.2).

In the future, a particular theoretical challenge is to propose necessary and sufficient conditions for the BAP and closedness of \(_{}()\), if possible covering in a single framework both types of domains \(\) considered here. The fact that the conditions established on these two types of domains are very similar (cf. the similarity between Theorem 3.1 and Theorem 4.1, as well as between Theorem 3.4 and Corollary 4.2) is encouraging. Another interesting algorithmic challenge is to substantially reduce the complexity of the algorithm to decide the closedness of \(_{}\) in Lemma 3.3, which is currently doubly exponential. It calls for a more efficient algorithm to make this check more practical. Achieving a practically tractable algorithm would for instance allow to check if a support selected e.g. by IMP is pathological or not. This would certainly consolidate the algorithmic robustness and theoretical foundations of pruning techniques to sparsity deep neural networks. From a more theoretical perspective, the existence of an optimum solution in the context of classical linear inverse problems has been widely used to analyze the desirable properties of certain cost functions, e.g. \(^{1}\) minimization for sparse recovery. Knowing that an optimal solution exists for a given sparse neural network training problem is thus likely to open the door to further fruitful insights.