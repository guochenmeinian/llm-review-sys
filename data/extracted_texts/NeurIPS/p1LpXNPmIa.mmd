# PromptFix: You Prompt and We Fix the Photo

Yongsheng Yu1 &Ziyun Zeng1 &Hang Hua1 &Jianlong Fu2 &Jiebo Luo1 {yyu90,zzeng24}@ur.rochester.edu, {hhua2,jluo}@cs.rochester.edu, jianf@microsoft.com

###### Abstract

Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks. The dataset and code are available at https://www.yongshengyu.com/PromptFix-Page.

## 1 Introduction

In recent years, diffusion models [19; 57; 66] have achieved remarkable advancements in text-to-image generation. Benefiting from large-scale training on image-text pairs , these models can generate highly realistic and diverse images that align with text prompts. They have been successfully applied to various real-world applications, including visual design, photography, digital art, and the film industry. In addition, models trained with instruction-following data  have shown promising results in understanding human instruction and performing the corresponding image-processing tasks. Previous studies [21; 22; 74; 73; 23] have illustrated that with instruction-following data, we can simply fine-tune a text-to-image generation model to perform various vision tasks such as image editing [7; 21; 74], object detection , segmentation , inpainting [23; 73], and depth estimation [22; 9]. To follow the success of these methods, we train our model utilizing input-goal-instruction triplet data for low-level image-processing tasks.

We first overcome the challenge of lacking the instruction-following data for low-level tasks. Specifically, we collect image pairs by generating degraded images from the source images and adopting data from existing datasets. Then, we employ GPT4  to generate the diverse text instructions for each task. We obtain \(\) 1.01 million input-goal-instruction triplets in the collected dataset. This dataset covers various low-level tasks including image inpainting, object creation, image dehazing, colorization, super-resolution, low light enhancement , snow removal, and watermark removal. We enrich the dataset through back-translation augmentation by swapping the inpainted and original images within the triplet and inverting the semantic orientation of the prompt. This technique effortlessly converts datasets from object removal to object creation. We also provide comprehensive details in Section 3.

With the dataset, we design a new diffusion-based model named PromptFix that can understand user-customized instructions and perform the corresponding low-level image-processing tasks. In PromptFix, we address several challenges that compromise the performance of the model. First, the use of stable-diffusion architecture  as a generative prior often faces the issue of spatial information loss, which is caused by VAE compression . Unlike unconditional or text-to-image generation, maintaining spatial detail consistency in image processing poses significant challenges, particularly with high-frequency components like text, as shown in Figure 5. To tackle this problem, we introduce High-frequency Guidance Sampling, in which we use a low-pass filter operator  to calculate the _fidelity constraint_ and integrate VAE skip-connect features during inference with a lightweight LoRA  fusion. Second, since the generative prior is not trained on low-level images, so relying solely on instructions may not always yield the desired outcome, especially when the image degradation is severe. To tackle this degradation adaptation problem, we introduce an _auxiliary prompt_

Figure 1: We propose PromptFix, a unified diffusion model capable of performing multiple image-processing tasks. It can understand user-customized editing instructions and perform the corresponding tasks with high quality. One of the key advantages of PromptFix is high-frequency information preservation, ensuring that image details are maintained throughout VAE decoding. PromptFix can handle various images with different aspect ratios.

_module_ to provide models with more descriptive text prompts to enhance controllability for image generation. The auxiliary text prompt can be obtained by VLMs . This approach introduces a semantic caption for a degraded image and the description of its defects, such as blurriness or insufficient lighting. The _auxiliary prompt module_ is implemented by an additional attention layer in diffusion U-Net that adapts both instruction and _auxiliary prompts_ as conditions and intermittently omits instructional prompts during training. We identify three key advantages of this approach: 1) enabling the model to process images with severe degradation, such as extremely low-resolution images, 2) adapting the model for blind restoration for different types of image degradation, and 3) providing additional pathways for a more precise semantic representation of the target image.

Experimental results demonstrate that our model achieves superior performance in the instruction-based paradigm across three image editing tasks (colorization, watermark removal, object removal) and four image restoration tasks (dehazing, desnowing, super-resolution, and low-light enhancement) in terms of perceptual pixel similarity  and no-reference image quality . In summary, our contributions are three-fold:

* We propose a comprehensive dataset tailored for seven image processing tasks. The dataset contains \(\) 1.01 million diverse paired input-output images along with corresponding image editing instructions.
* PromptFix for low-level image-processing tasks. Extensive experimental results show that PromptFix outperforms previous methods in a wide variety of image-processing tasks and exhibits superior zero-shot capabilities in blind restoration and combination tasks.
* high-frequency guidance sampling and auxiliary prompt module to diffusion models to effectively address the issues of high-frequency information loss and the failure in processing the severe image degradation for instruction-based diffusion models in low-level tasks.

## 2 Related Work

### Instruction-guided Image Editing

Instruction-guided image editing significantly improves the ease and precision of visual manipulations by adhering to human directions. In traditional image editing, models primarily focused on singular tasks such as style transfer or domain adaptation [24; 55], leveraging various techniques to encode images into a manipulatable latent space, such as those used by StyleGAN . Concurrently, the advent of text-to-image diffusion models [26; 57; 63; 65] has broadened the scope of image editing [7; 25]. Kim et al.  showed how to perform global changes, whereas Avrahami et al.  successfully performed local manipulations using user-provided masks for guidance. While most works that require only text (i.e., no masks) are limited to global editing [17; 36]. Bar-Tal et al.  proposed a text-based localized editing technique without using any mask, showing impressive results. For local image editing, precise manipulations are possible by inpainting designated areas using either user-provided or algorithmically predicted masks , all while preserving the visual integrity of the adjacent areas. In contrast, instruction-based image editing operates through direct commands like "add fireworks to the sky," avoiding the need for detailed descriptions or regional masks. Recent approaches utilize synthetic input-goal-instruction triples  and incorporate human feedback  to execute editing instructions effectively. Despite the advances in using diffusion models for various instruction-guided image editing tasks, there is still a notable gap in research specifically addressing instruction-guided image restoration with these models. Our study aims to bridge this gap by collecting a comprehensive dataset of paired low-level instruction-driven image editing examples and proposing an all-in-one model for low-level tasks and editing.

### Large Language Models for Vision

Recent advancements in the development of Large Language Models (LLMs) have led to the emergence of powerful models with extensive capabilities [15; 31; 39; 43; 77]. These LLMs, pretrained on large-scale internet-based datasets, are equipped with broad knowledge bases that enhance their zero-shot and in-context learning abilities [5; 29; 51]. Furthermore, there is a growing focus on using LLMs for multimodal tasks [3; 30; 42; 45], incorporating methods like vision-languagealignment and adapter fine-tuning. These techniques ensure that the visual data processed by visual encoders is semantically aligned with the textual input of LLMs . This approach has spurred significant advancements in Text-to-Image generation, leading to the development of various LLM-based diffusion models for these tasks [28; 40; 41]. Despite these successes, there remains a relative scarcity of research focusing on using large Vision-Language Models (VLMs) for instructed image editing, particularly in detailed, low-level editing tasks.

## 3 Data Curation

Current off-the-shelf image datasets [7; 74; 76] with instructional annotations primarily facilitate image editing research, encompassing tasks such as color transfer, object replacement, object removal, background alteration, and style transfer. Nevertheless, their overlap with low-level applications is limited. Moreover, we find it challenging to achieve satisfactory results for existing models in image restoration. Our goal is to construct a comprehensive visual instruction-following dataset specifically for low-level tasks. We obtain \( 1.01\) million training triplet instances.

**Paired Image Collection.** We initially gather source images from various existing datasets. Subsequently, we produce degraded and inpainted images to create an extensive set of paired image data. We compile approximately two million raw data points across eight tasks: image inpainting, object creation, image dehazing, image colorization, super-resolution, low-light enhancement, snow removal, and watermark removal. For the test set, we randomly select 300 image pairs for each task. More details about the dataset composition are provided in the Appendix A.1.

**Instruction Prompts Generation.** For each low-level task, we utilized GPT-4 to generate diverse training instruction prompts \(_{}\). These prompts include task-specific and general instructions. The task-specific prompts, exceeding 250 entries, clearly define the task objectives. For example, "Improve the visibility of the image by reducing haze" for dehazing. The general instructions include five ambiguous commands that we retained as "negative" prompts to promote adaptive tasks. The specific instruction prompts used for training are detailed in the appendix. For watermark removal, super-resolution, dehazing, snow removal, low-light enhancement, and colorization tasks, we also generate "auxiliary prompts" for each instance. These auxiliary prompts describe the quality issues for the input image and provide semantic captions. More details are discussed in Section 4.2.

## 4 Methodology

Let \(I^{H W 3}\) denote the degraded input image. Our PromptFix model aims to enhance \(I\) using the prompt \(\) and the diffusion model \(\).

Figure 2: The architecture of our proposed PromptFix.

### Diffusion Model

Diffusion models transform data into noise through gradual Gaussian perturbations during a forward process and subsequently reconstruct samples from this noise in a backward process. In the forward phase, an original data point, denoted as \(_{0}\), is incrementally altered towards a Gaussian noise distribution \((0,)\), according to the equation:

\[_{t}=q(_{0},,t)=_{t}_{0}+ _{t}, t[0,T],\] (1)

where \(_{t}\) and \(_{t}\) are coefficients that manage the signal-to-noise ratio at each interpolation point \(_{t}\). This process aims to maintain variance, adopting coefficient strategies as detailed in sources such as . Modeled as a stochastic differential equation (SDE) in continuous time, the forward process can be expressed as \(d=(,t)dt+g(t)d_{t}\), where \((,t)\) is a vector-valued drift coefficient, \(g(t)\) is the diffusion coefficient, and \(_{t}\) represents Brownian motion at time \(t\).

The backward diffusion process, made possible by notable characteristics of the SDE, is rearticulated via Fokker-Planck dynamics  to yield deterministic transitions with consistent probability densities, creating the _probability flow ODE_:

\[d=[(,t)-g(t)^{2}_{} p_{t}()]dt.\] (2)

This equation outlines a transport mechanism that is learnable through maximum likelihood techniques, applying the perturbation kernel of diffused data samples \(_{} p_{t}(|_{0})\), as demonstrated in . Next, we sample \(_{t}(0,I)\) to initialize the probability flow ODE, and the estimates for the score function via \(}(_{t},t)/_{t}\). We employ the Euler method  among numerical ODE solvers to obtain the solution trajectory: \(_{0}_{N}_{N-1} _{1}(_{T})\), where \(\) denotes the diffusion model and \(N\) represents the neural function evaluations (NFEs) for sampling.

During the training phase, a simple diffusion loss  is utilized, whereby the neural network still employs forward inference to predict noise. The sample data estimate \(_{0}\) can be obtained at any step \(t\) by using the current noisy data and the predicted noise and is derived as:

\[_{t 0}=_{t}-_{t}}( _{t},t)}{_{t}}.\] (3)

To reduce computational costs, the aforementioned diffusion process initiates from isotropic Gaussian noise samples in the latent space , rather than the pixel space. This space transformation is facilitated through VAE compression . The VAE autoencoder comprises an encoder \(E()\) and a left inverse decoder \(D()\). For instance, an image \(x\) can be encoded into a latent code \(E(x)\), which can then be approximately reconstructed back into the pixel space as \(x D E(x)\).

### VLM-based Auxiliary Prompt Module

Given that low-level image processing focuses on handling degraded images rather than real-world images, we adopt the integration of a VLM to estimate an _auxiliary prompt_ for the low-level image \(I\). This auxiliary prompt encompasses both semantic captions and defect descriptions to enhance the semantic clarity of the target image, thereby addressing the instructional gaps inherent in low-level image processing tasks.

Based on text dialogues parameterized by \(\) within a VLM \((;)\), we employ a frozen VLM, specifically the InternVL2  model, which integrates visual and linguistic modalities as inputs. We facilitate this model to receive paired degraded image \(I\) and a textual query \(\). To handle the visual input, the InternVL2 first employs a pre-trained encoding model to map each modality into a shared representation space. The visual encoding model \(\) embeds \(I\) into the textual space, resulting in \((I)\), which is then combined with the tokenized language embedding \(()\). These combined embeddings are fed into the large language model, producing the textual response \(\).

\[(I,)=((I),();)\] (4)

The visual encoding model of InternVL2 has not undergone extensive fine-tuning in the degradation domain. To acquire an explicit understanding from both semantic and low-level defect perspectives, we meticulously curate the queries \(_{}\) and \(_{}\) to guide InternVL2, respectively. Specific query instances are provided in the Appendix. As illustrated in Figure 4 and described by Equation 5, we concatenate the responses related to semantics and degradation textually, forming \(_{}\), which serves as the _auxiliary prompt_. This acts as a supplement to the instruction prompt \(_{}\).

\[_{}=[(I,_{} ),(I,_{})]\] (5)

The conditioned text prompts guide the diffusion model by injecting the embedding into the cross-attention layer . After obtaining the auxiliary prompt, a straightforward approach involves concatenating it with the user-input instruction prompt before feeding the text embedding into the diffusion model. However, this concatenation can render the entire prompt excessively long, leading to forced truncation during tokenization. Therefore, after utilizing the pre-trained CLIP visual encoder ViT-L/14  to extract linguistic features, we process the text embeddings of \(_{}\) and \(_{}\) separately. We introduce additional cross-attention layers identical to the original ones, as depicted in Figure 2. The embeddings of \(_{}\) and \(_{}\) are fed into the Key and Value heads of consecutive attention networks, respectively, thereby achieving an augmented cross-attend adaptation.

### High-frequency Guidance Sampling

There is a fundamental requirement in image restoration and generation tasks: the processed image must maintain high accuracy in semantics. We observe that vanilla VAE reconstructions tend to lose image details such as textual rendering, which contains high-frequency information, as shown in Figure 5. Therefore, we propose high-frequency guidance sampling to balance the quality and fidelity of generation.

The denoising sampling is based on the EDM formulation . To maintain spatial information, we utilize a modified VAE Decoder \(D_{}\) to map from the latent space to the pixel space. We modify the VAE decoder by passing skip-connect features from the VAE encoder through additional LoRA convolutions  to merge the feature map. The LoRA networks are initialized randomly, with their trainable parameters denoted as \(\). Since the parameters of the LoRA convolution are lightweight, merely multi-step backpropagation can maintain high-frequency consistency without requiring extensive fine-tuning.

We propose a _fidelity constraint_ to model the spatial discrepancy between the image and the ground truth. We implement two types of high-pass operators to extract high-frequency signals from the degraded image. For the Fourier filtering operator \(()\), we convert the generated image from the spatial to the frequency domain using the Discrete Fourier Transform . High-frequency components are then isolated via high-pass filtering and reconstituted into an image through the Inverse Fourier Transform . Meanwhile, we apply the Sobel  edge detection operator \(()\) as a complement. The fidelity constraint evaluates the divergence between the high-frequency components of the ground truth and the processed image, ensuring the preservation of spatial information throughout the sampling process. Additionally, to obtain the image at time step \(t\), we utilize predicted noise \(\) from diffusion model \(\) to compute the \(_{t 0}\) estimation at any time step. The fidelity constraint is calculated as follows:

\[(I,D_{}(_{t 0}))=\|(I)-(D_{ }(_{t 0}))\|_{2}^{2}+\|(I)-(D_{ }(_{t 0}))\|_{2}^{2}\] (6)

Given that \(_{t>0}\) represents a noisy latent variable, assigning equal weight to each timestep's latent is impractical. To mitigate the cumulative error induced by this practice, we introduce a time-scale weight \(e^{- t}\). The overall sampling algorithm is detailed in Algorithm 1.

## 5 Experiments

### Experimental Setup

**Implementation details**. We train PromptFix for 46 epochs on 32 NVIDIA V100 GPUs, employing a learning rate of \(1 10^{-4}\) with the Adam optimizer. The training input resolution is set to \(512 512\), matching the capabilities of our backbone models, InternVL2  and Stable Diffusion 1.5 . To facilitate classifier-free guidance , we randomly drop the input image latent, instruction, and auxiliary prompt with a probability of 0.075 during training. The hyperparameter \(\) for the time-scale weight in Algorithm 1 is empirically set to 0.001. For more implementation details, please refer to the appendix.

**Baselines and metrics**. We adopt instruction-based generalist models, such as InstructP2P , MGIE , and InstructDiffusion , as our primary comparison. MGIE employs VLM-guided techniques for image editing, while InstructDiffusion addresses overlapping tasks with our training objectives, including watermark removal and inpainting. Additionally, we evaluate all-in-one image restoration methods like AirNet  and PromptIR  (which do not support instruction input), as well as image restoration expert models, fine-tuned for specific sub-tasks [46; 73]. We assess the

Figure 3: Qualitative comparison between PromptFix and other instruct-driven diffusion methods (InstructP2P , InstructDiff , and MGIE ) for image processing, as well as low-level generalist techniques (PromptIR , AirNet , and Diff-Plugin ) for image restoration.

similarity of the generated images to the ground truth using metrics such as PSNR, SSIM , and LPIPS . For no-reference image quality evaluation, we utilize the ManIQA  metric.

### Quantitative and Qualitative Results

Table 1 illustrates the comparative analysis of image restoration and editing techniques, evaluated via LPIPS and ManIQA metrics. The expert model - Diff-Plugin shows limited but notable performance in low-light enhancement (LPIPS/ManIQA: 0.227/0.453) and desnowing (0.133/0.508). Among generalist methods, AirNet demonstrates balanced capabilities in tasks like desnowing and dehazing, achieving LPIPS/ManIQA scores of 0.245/0.589 and 0.039/0.780, respectively. However, the instruction-driven diffusion methods reveal a more nuanced picture, with PromptFix emerging as particularly promising. It excels in colorization (LPIPS/ManIQA: 0.233/0.489), object removal (0.054/0.810), and watermark removal (0.071/0.811), consistently outperforming others. InstructP2P and InstructDiff also perform well in specific tasks, such as low-light enhancement and dehazing, but do not match the overall versatility of PromptFix. MGIE, though effective in certain domains, lacks the consistency seen in "PromptFix (Ours)." This highlights the robustness and superior performance

    &  &  \\   & Colorization & Object & Watermark & Low-light & Desnow & Dehazy & Super Res. \\   \\  Diff-Plugin  & - & - & - & 0.227/0.453 & 0.133/0.508 & 0.033/0.758 & 0.097/0.555 \\ Inst-Inpaint  & - & 0.227/0.593 & - & - & - & - & - \\   \\  PromptIR  & - & - & - & 0.330/0.539 & 0.235/0.553 & 0.037/0.764 & 0.105/0.442 \\ AirNet  & - & - & - & 0.332/0.541 & 0.245/0.589 & 0.039/0.780 & 0.107/0.450 \\   \\ 
**InstructP2P** & 0.394/0.424 & 0.177/0.791 & 0.341/0.378 & 0.581/**0.460** & 0.365/**0.560** & 0.216/0.625 & 0.234/0.528 \\
**InstructDiff** & 0.433/0.256 & 0.071/**0.811** & 0.247/0.675 & 0.368/0.309 & 0.255/0.530 & 0.124/0.711 & 0.233/0.623 \\ MGIE  & 0.425/0.393 & 0.463/0.506 & 0.491/0.356 & 0.483/0.417 & 0.249/0.704 & 0.397/0.385 \\
**PromptFix (ours)** & **0.233/0.489** & **0.054/**0.810** & **0.127/0.750** & **0.135/0.423** & **0.103/**0.535** & **0.088/0.752** & **0.143/0.642** \\   

Table 1: Quantitative comparison is conducted across seven low-level datasets with a \(512 512\) input resolution. Expert models refer to approaches, such as Diff-plugin , which use non-generalizable training pipelines and maintain separate pre-trained weights for each of the four restoration tasks. Image Restoration Generalist Methods denote models that integrate multiple low-level tasks into a single framework. Instruct-driven Diffusion Methods represent diffusion generative baselines that follow human language instructions. \(\) indicates higher is better and \(\) indicates lower is better. The **best** and **second best** results are in bold and underlined, respectively.

Figure 4: Qualitative analysis of VLM-guided blind restoration for desnowing, dehazing, and low-light enhancement. The results are obtained from PromptFix without explicit task instructions, relying solely on the input image. The auxiliary prompt, automatically generated by a VLM during inference, includes semantic captions and defect descriptions, indicated by \(}}}}}}}}}}}\) and \(()\) and \(()\) improves the quality of the image generated, demonstrated by the quantitative results shown in Table 4.

**VLM-guided blind restoration**. We utilize InternVL2  to generate auxiliary prompts and leave the instruction prompt empty. This approach enables users to input an image without the need to provide instructions for its restoration. We evaluate the model's performance on such blind restoration tasks, including low-light enhancement, desnowing, and dehazing. As shown in Table 3, our model achieves performance comparable to four baselines, showing minimal perceptual differences from the ground truth and superior zero-shot capabilities.

   Method & PSNR\(\) & SSIM\(\) & LPIPS\(\) & ManIQA\(\) \\  PromptIR  & 19.30 & 0.7385 & 0.2282 & 0.4310 \\ AirNet  & 19.15 & 0.7197 & 0.2359 & 0.4576 \\  InstructP2P  & 14.19 & 0.5845 & 0.3046 & 0.3790 \\ InstructDiff  & 17.12 & 0.6387 & 0.2865 & 0.4365 \\ 
**PromptFix** & **22.05** & **0.7654** & **0.1519** & **0.4889** \\   

Table 2: Quantitative comparison of multi-task processing on \(200\) sampled test images, each paired with a degraded version exhibiting three defects (e.g., desnowing, dehazing and super-resolution) and the corresponding ground truth.

Figure 5: Preservation of low-level image details using the proposed High-frequency Guidance Sampling (HGS) method, compared to previous VAE-based baselines [21; 23; 46] utilizing stable-diffusion architecture.

    &  \\   & Low-light &  &  \\    & Enhancement & & & \\  PromptIR  & 0.331 / 0.539 & 0.236 / 0.533 & 0.038 / 0.764 \\ AirNet  & 0.332 / 0.541 & 0.245 / 0.589 & 0.039 / 0.781 \\  InstructP2P  & 0.581 / 0.461 & 0.365 / 0.560 & 0.216 / 0.626 \\ InstructDiff  & 0.369 / 0.309 & 0.256 / 0.531 & 0.124 / 0.712 \\  PromptFix & 0.161 / 0.413 & 0.115 / 0.531 & 0.148 / 0.755 \\   

Table 3: Quantitative comparison on three low-level tasks. PromptFix \(\) denotes blind restoration without input instruction prompts. The compared baselines specify task objectives explicitly.

**Multi-task processing**. Although PromptFix is not explicitly trained to handle multiple low-level tasks simultaneously within the same image, it demonstrates the capability for multi-task processing. We construct the validation dataset with 200 images, and each image contains 3 restoration tasks such as colorization, watermark removal, low-light enhancement, desnowing, dehazing, and super-resolution. We benchmarked PromptFix against AirNet and PromptIR, both generalist image restoration methods, as well as InstructP2P and InstructDiff, which are instruction-driven diffusion methods. As shown in Table 2, PromptFix outperforms these baselines, achieving superior image quality, structural similarity, and minimal perceptual differences from the ground truth, as evidenced by competitive PSNR, SSIM, and LPIPS scores, along with a higher ManIQA score indicating visually pleasing and high-quality results. Conversely, while methods like InstructP2P and InstructDiff perform well in specific metrics, they do not match the overall balanced performance of PromptFix. These results indicate the robustness and versatility of PromptFix.

**Different types of instruction prompts**. We verify PromptFix's generalization to various human instructions in Table 5, by conducting ablation comparisons with three types of prompts: instructions used during training and out-of-training human instructions with fewer than 20 words and with 40-70 words. PromptFix's performance slightly declines with out-of-training instructions, but the change is negligible. It indicates that PromptFix is robust for instructions under 20 words, which is generally sufficient for low-level processing tasks. We observe a performance drop with longer instructions, possibly due to the long-tail effect of instruction length in the training data. Although low-level processing tasks usually don't require long instructions, addressing this issue by augmenting the dataset with longer instructions could be a direction for future work.

## 6 Conclusion

We present PromptFix, a novel diffusion-based model along with a large-scale visual-instruction training dataset, designed to benefit instruction-guided low-level image processing. PromptFix effectively addresses challenges related to spatial information loss and degradation adaptation by high-frequency guidance sampling and a VLM-based auxiliary prompt module. These mechanisms improve the model's performance in the instruction-based image-processing paradigm. Extensive experiment results demonstrate PromptFix's advanced capabilities of generating accurate and high quality images. In addition to the improvement in terms of conventional metrics, we observe that PromptFix is also effective at processing multi-task processing and achieving blind restoration in low-light enhancement, desnowing, and dehazing.