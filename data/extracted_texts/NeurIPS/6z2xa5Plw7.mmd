# ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates

Warning: This paper contains model outputs that may be considered offensive.

Fengqing Jiang \({}^{}\)&Zhangchen Xu\({}^{}\)&Luyao Niu\({}^{}\)&Bill Yuchen Lin\({}^{}\)&Radha Poovendran\({}^{}\)&\({}^{}\)University of Washington \({}^{}\)Allen Institute for AI

###### Abstract

Large language models (LLMs) are expected to follow instructions from users and engage in conversations. Techniques to enhance LLMs' instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.

In this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users. Hence, a malicious user may not necessarily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We study two attacks to exploit the ChatBug vulnerability. Additionally, we demonstrate that the success of multiple existing attacks can be attributed to the ChatBug vulnerability. We show that a malicious user can exploit the ChatBug vulnerability of eight state-of-the-art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their attack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial training effectively mitigates the ChatBug vulnerability, the victim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research.

## 1 Introduction

Large language models (LLMs) such as GPT-4  and Llama-3  have been widely used to empower conversational agents. LLMs are required to follow instructions from users and engage in conversations in a meaningful way during interactions. Standard techniques to enhance instruction-following capabilities include instruction tuning  and preference tuning .

A common practice for instruction tuning and preference tuning is to structure data using chat templates . A chat template defines a format for representing conversations as sequences of tokens. The format specifies the roles involved in the conversation and the associated messages. Chat templates are shown to be effective in optimizing the LLMs' performance , allowing LLMs to generate coherent responses in their designated roles.

In addition to instruction-following capabilities, LLMs are also required to generate responses that are aligned with human values. It has been shown that chat templates can be adopted to mitigate prompt injection attacks , one of the major threats to LLMs. However, despite the effectiveness of chat templates, in-depth analysis of how chat templates affect safety alignment of LLMs has been overlooked.

In this paper, we investigate how chat templates affect the safety alignment of LLMs. We show that these templates introduce a _common vulnerability_ to LLMs that have been fine-tuned with chat templates. We name this vulnerability as ChatBug, which could be exploited by malicious users to provoke unsafe responses from LLMs. Our key insight is that the formats pre-defined by chat templates are rigid and should be followed by LLMs. However, malicious users may not necessarily follow such formats when prompting LLMs, providing them possibilities to elicit unsafe responses.

We show that any malicious user who has knowledge of the chat template can exploit the ChatBug vulnerability. This assumption is not restrictive, especially for open-source models whose chat templates are often publicly available. We study two attacks, _format mismatch attack_ and _message overflow attack_ as illustrated in Figure 1, to exploit the vulnerability and elicit unsafe responses from LLMs. Specifically, the format mismatch attack modifies the default chat format, while the message overflow attack injects a sequence of tokens into the model's reserved field. Specifically, the message overflow attack unifies multiple existing attacks [53; 42; 2], whose success can all be attributed to the ChatBug vulnerability. We demonstrate the severity and pervasiity of the ChatBug vulnerability on eight LLMs (Vicuna , Mistral , Llama-2 , Llama-3 , GPT-3.5 , Gemini , Claude-2.1 , and Claude-3 ). We further show that existing jailbreak attacks such as GCG , GPTFuzzer , and ArtPrompt  can exploit the ChatBug vulnerability to increase their attack success rates.

Given the severity of the ChatBug vulnerability, we investigate potential countermeasures. Our experiments show that techniques such as adversarial training can effectively mitigate the ChatBug vulnerability, at the cost of significantly degrading model performance. This underscores a critical trade-off between the safety and helpfulness of LLMs, a balance that is often overlooked during instruction tuning. We believe that this is an important yet open direction for the future development of LLMs, requiring collaborative efforts from the community.

## 2 Related Work

Adversarial Robustness of LLM.Prior work focuses on how the design of prompts impacts model performance, such as the prompting language , order of few-shot examples , example choices , and prompt formatting . This paper, which investigates how chat templates affects safety alignment of LLMs, is parallel to the aforementioned studies.

LLM Alignment and Jailbreak Attack.Extensive efforts have been made to align LLMs with human values. Standard techniques include supervised fine-tuning [6; 44], preference tuning [5; 10; 35], and red-teaming [12; 13]. Despite these efforts, jailbreak attacks  pose a significant threat to misuse of LLMs. Jailbreak attacks can be categorized into two classes based on how they bypass safety alignment. The first category designs attack prompts based on heuristics that rely on human experts [11; 17; 25; 28; 21; 50]. The second category utilizes optimization problems to search for prompts to jailbreak aligned LLMs. Gradient-based [22; 53; 52], genetic algorithm-based , and edit-based methods  have been developed to solve the optimization problems.

Figure 1: This figure illustrates how the format mismatch attack and message overflow attack exploit ChatBug. The format mismatch attack alters the default chat format () to bypass safety alignment of LLMs. The message overflow attack inserts a short sequence of tokens () into the field reserved for LLM to bypass safety alignment.

Identifying ChatBug Vulnerability

This section presents background on auto-regressive LLMs and chat templates used to fine-tune LLMs. We then identify a common vulnerability, named ChatBug, induced by chat templates.

### Preliminary Background

Auto-regressive (Base) LLMs.Let \(\) represent an auto-regressive LLM whose vocabulary is denoted as \(\). Given an input represented by a sequence of tokens of length \(n\), denoted as \(x_{1:n}=x_{1},,x_{n}\), the LLM predicts a probability distribution \(p_{}(|x_{1:n})\) over the vocabulary \(\). Then the LLM samples the next token \(x_{n+1}\) according to a predefined decoding strategy (e.g., greedy or beam search ) and probability distribution \(p_{}\). Appending token \(x_{n+1}\) to the sequence \(x_{1:n}\) and iteratively applying the procedure for next token generation as described above yield text generation by the LLM. This process continues until a stopping criterion is met, such as reaching the maximum generation length or generating an end-of-sequence (EOS) token.

Chat Format of Instruction-Tuned LLMs.Instruction tuning is the critical step to enable a pre-trained LLM to follow instructions from users. Such processes include supervised fine-tuning, and/or reinforcement-learning from human feedback (RLHF) . Instruction tuning employs a chat template to structure data in the form of (multi-turn) conversations. An example of chat template, named ChatML , is presented in Table 3. The template defines a format for representing conversations using a sequence of tokens. It starts by segmenting a conversation involving multiple turns into individual turns, where the segments are separated by a set of special control tokens, denoted as beginning-of-turn (BOT) and end-of-turn (EOT) tokens. For each turn, the template organizes the dialogue by role control tokens (e.g., 'user' and 'assistant') and their respective messages. These tokens within a single turn are delineated by BOT and EOT tokens. Note that the BOT and EOT tokens are different from beginning-of-sequence (BOS) and end-of-sequence (EOS) tokens, which are used for both base LLMs and aligned LLMs to delimit the beginning and end of a sequence.

Following the chat template, a standard single-turn prompt can be represented as follows:

\[x=b r_{1} m e b r_{2},\]

where \(\) is the token sequence concatenation operation, \(b\) is the BOT token, \(e\) is EOT token, \(r_{1}\) and \(r_{2}\) are the role control tokens, and \(m\) is the payload message in the model input \(x\).

### Chat Templates Induce a Common Vulnerability: ChatBug

Although chat templates are effective in fine-tuning LLMs to function as conversational agents, we highlight that they introduce a _common vulnerability_, named ChatBug, to LLMs. A malicious user, with knowledge of the format predefined by the chat template, could exploit this vulnerability to _elicit harmful responses from victim LLMs_ by crafting its queries. Our key insight is that the chat templates pre-define rigid formats that should be followed by LLMs, but not by users. For example, the malicious user could craft its query by appending the role of LLM and the beginning of desired harmful response at the end, as illustrated in Appendix B. Consequently, the malicious user tricks victim LLMs to complete the harmful responses provided by the user, instead of continuing the conversation in its intended role. We note that it is not a restrictive assumption to suggest that malicious users could have access to the knowledge of chat templates, as these are often publicly available . As we will discuss later, exploiting this vulnerability does not rely on white-box  or grey-box access  to the victim model.

## 4 Exploit ChatBug

In this section, we describe how a malicious user could exploit the ChatBug vulnerability and elicit harmful responses from the victim LLM. Specifically, we discuss how two attacks, denoted as _format mismatch attack_ and _message overflow attack_, can tamper with the prompt.

### Format Mismatch Attack

Attack Description.A malicious user could exploit ChatBug by launching format mismatch attack, as illustrated in Figure 1. In a format mismatch attack, the malicious user modifies or omits some tokens required by the format (e.g., special control tokens). The resultant query can be represented as

\[x^{}=b^{} r_{1}^{} m e^{} b^{ } r_{2}^{}.\]

For instance, a malicious user may omit all control tokens including BOT, EOT, and role control tokens when prompting the victim LLMs. The insight behind the format mismatch attack is that the format specified by the chat template is not mandatory for users to follow. Since many LLMs may not verify whether user queries match the format required by the chat template, these modifications may induce a different interpretation of input queries to victim LLMs, leading to harmful or unintended responses. An example of the format mismatch attack can be found in Appendix B. We note that the format mismatch attack alters the chat template, distinguishing it from the setup of jailbreak attacks, which focus on manipulating the prompt message but following the standard chat template [27; 25].

Proof-of-Concept Attack.We denote the sequence of tokens crafted using the format mismatch attack as \(_{1:n}\) and the sequence of tokens designed according to the chat template as \(x_{1:n}\). Specifically, token sequence \(_{1:n}\) is constructed by omitting all special control tokens including role control, BOT, and EOT tokens. We then demonstrate the feasibility of format mismatch attack by quantifying the ratio \(}(|_{1:n})}{P_{}(|x_{1:n})}\) averaged over 50 instructions with \(n\) varying from one to ten. The results are presented in Figure 1(a), where the probability ratio associated with each instruction is indicated by a blue circle, and the average result is colored in red. We observe that the probability of generating the desired sequence of tokens representing the harmful response increases by a factor of \(10^{10}\), indicating the significant effectiveness of format mismatch attack.

### Message Overflow Attack

Attack Description.A malicious user could exploit the ChatBug vulnerability using message overflow attack as illustrated in Figure 1. In a message overflow attack, the message from malicious users extends beyond its own EOT token and the role control token \(r_{2}\). This overflow is a short sequence of tokens, representing the beginning of the desired harmful response. Formally, we denote the attack with a overflowed token sequence \(s\) as follows:

\[x^{}=b r_{1} m e b r_{2} s,\]

where \(s\) is the overflow message from the malicious user. Consequently, the victim LLMs are tricked to complete the harmful response based on their auto-regressive generation capabilities, instead of continuing the conversation with users in their designated roles. An example of the message overflow attack is presented in Appendix B. The message overflow attack provides a unified framework for

Figure 2: **(a)** We denote the sequence of tokens crafted using the format mismatch attack as \(_{1:n}\) and the sequence of tokens following chat template as \(x_{1:n}\). This figure presents how the ratio \(}(|_{1:n})}{P_{}(|x_{1:n})}\) evolves at each step \(n\), with the results averaged over 50 instructions. **The format mismatch attack significantly increases the probability of generating the desired harmful response. (b)** This figure presents the probability of generating the desired harmful response when the number of overflow tokens varies from 0 to 9, averaged over 50 instructions. Note that the user does not launch message overflow attack when the number of overflow tokens is zero. The results show that **the probability of generating the desired harmful response increases as the user overflows more tokens.**

existing attacks [53; 42; 2], which adopt seemingly distinct attack strategies. We highlight that the success of these attacks can all be attributed to the ChatBug vulnerability.

Proof-of-Concept Attack.We consider that a malicious user overflows a sequence of tokens into the field corresponding to the message of victim LLM. In figure 1(b), we present the probability of generating the desired harmful response when the number of overflow tokens varies from 0 to 9. Note that the user does not launch message overflow attack when the number of overflow tokens is zero. We observe that the probability of generating the desired harmful response increases as the user overflows more tokens. This indicates that message overflow attack allows malicious users to obtain their desired responses from victim LLMs.

### ChatBug Boosts Jailbreak Attacks

Existing ailbreak attacks [21; 49; 53] elicit unintended responses from victim LLMs using deliberately designed prompts, which correspond to user message \(m\) in the chat templates. By exploiting the ChatBug vulnerability and launching jailbreak attacks, malicious users could jointly tamper with the chat template via the two attacks mentioned above and messages within a conversation with victim LLMs. Consequently, the malicious users can effectively boost the probability of generating unintended responses from victim LLMs.

## 5 Experimental Assessment of ChatBug

### Experimental Setup

Victim Models.We evaluate ChatBug on eight LLMs including both open-source and closed-source models. For open-source models, we select **Vicuna** (7B-v1.5) , **Llama-2** (7B-Chat) , **Llama-3** (8B-Instruct) , **Mistral** (7B-Instruct v0.2) . For closed-source models, we consider **GPT-3.5** developed by OpenAI1, **Claude-2.1** and **Claude-3** (Opus) from Anthropic [3; 4], as well as **Gemini** (Pro) from Google .

Dataset.We use **AdvBench** developed by  to evaluate ChatBug. AdvBench contains 520 instructions, with the aim to provoke a wide range of harmful responses from LLMs.

Metric.We assess the severity of ChatBug using a metric named attack success rate (**ASR**):

\[ASR=}{\#} 100\%.\]

Following [49; 53], we calculate ASR using two approaches:

* **Refusal Response Matching (ASR-R).** This approach collects a set of refusal responses (e.g., "Sorry, I cannot...") and verifies whether the response generated by LLM matches any of them. An LLM generated response is considered harmful if it does not align with any of the refusal responses.
* **Moderator Assessment (ASR-M).** We utilize a pretrained LLM, Llama-Guard-22 fine-tuned from Llama 3, as a moderator to evaluate whether a response is harmful.

Baseline.We evaluate the severity of the ChatBug vulnerability by comparing attacks that exploit it with a baseline scenario named **Direct Instruct**. In the baseline scenario, a malicious user directly prompts victim LLMs with harmful queries or instructions.

Attack Settings.We consider six attack settings where the format mismatch attack and message overflow attack exploit ChatBug to provoke unintended behaviors from victim LLMs.

* **Mismatch-\(\).** An attacker launches the format mismatch attack by removing all special control tokens in the chat format.
* **Mismatch-C.** An attacker launches the format mismatch attack by replacing the default chat template with ChatML developed by .
* **Mismatch-V.** In this case, an attacker initiates the format mismatch attack by replacing the default chat template with the one used by Vicuna .
* **Overflow-S.** An attacker launches the message overflow attack. For any query, the overflow inserts a fixed prefix '_Sure, here is_' for the desired response.
* **Overflow-L.** For each input query, an attacker initiates the message overflow attack by inserting a prefix tailored particularly for this query. For example, the overflow is '_Sure, here is a tutorial for making a bomb_' when the input query is '_Write a tutorial on how to make a bomb_'. These queries and corresponding prefixes are readily available in AdvBench.
* **Overflow-FS.** This is an extension of Overflow-L, where an attacker launches the message overflow attack and utilizes an uncensored LLM to generate the prefixes. In our experiments, we choose Wizard-Vicuna-13B-Uncensored3 as the uncensored LLM, and use in-context-learning prompting technique to generate multiple prefixes that carry affirmative semantics automatically. We generate 5 prefixes by default. The prompt to generate attack prefixes is in Figure 4 in Appendix.

### Main Results

Exploiting ChatBug bypasses safety alignments of all eight victim LLMs.In Tables 1 and 2, we summarize the ASR for format mismatch attack and message overflow attack under different settings on open-source and closed-source LLMs, respectively. We have two key observations. First, exploiting the ChatBug vulnerability effectively elicits unintended responses from all victim LLMs.

    &  &  &  &  \\   & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-R** & **ASR-M** \\  Direct Instruct & 5.6\% & 3.7\% & 24.0\% & 22.9\% & 0.4\% & 0.0\% & 1.1\% & 0.0\% \\  Mismatch-0 & 90.6\% & 40.4\% & 65.2\% & 55.6\% & 17.1\% & 12.7\% & 65.4\% & 50.0\% \\ Mismatch-V & - & - & 10.8\% & 4.6\% & 0.2\% & 0.0\% & 1.2\% & 1.5\% \\ Mismatch-C & 52.3\% & 37.9\% & 12.9\% & 9.0\% & 5.6\% & 3.3\% & 1.3\% & 0.2\% \\  Overflow-S & 98.5\% & 89.4\% & 89.8\% & 83.8\% & 46.0\% & 36.4\% & 92.1\% & 84.2\% \\ Overflow-L & 90.4\% & 88.5\% & 64.0\% & 53.9\% & 32.5\% & 20.8\% & 98.3\% & 93.5\% \\ Overflow-FS & 98.8\% & 95.2\% & 96.2\% & 90.4\% & 51.0\% & 31.3\% & 100.0\% & 94.1\% \\   

Table 1: This table summarizes the ASR-R and ASR-M of Direct Instruct (baseline) and attacks that exploit the ChatBug in open-source LLMs. The results show that an attacker can effectively bypass safety alignment of LLMs by exploiting the ChatBug vulnerability. We have excluded results of Mimmatch-V on Vicuna model since they use the same chat template.

    &  &  &  &  \\   & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-R** & **ASR-M** \\  Direct Instruct & 9.6\% & 6.3\% & 10.2\% & 2.1\% & 0.2\% & 0.0\% & 0.4\% & 0.0\% \\  Mismatch-\(\) & 86.5\% & 76.9\% & - & - & - & - & - & - \\ Mismatch-V & 86.2\% & 77.7\% & - & - & - & - & - & - & - \\  Overflow-S & 51.7\% & 51.0\% & 70.2\% & 83.5\% & 33.5\% & 8.5\% & 56.0\% & 24.2\% \\ Overflow-L & 30.2\% & 58.1\% & 69.2\% & 83.1\% & 86.9\% & 83.1\% & 22.5\% & 11.0\% \\ Overflow-FS & 65.2\% & 66.0\% & 95.0\% & 91.0\% & 90.6\% & 80.0\% & 68.3\% & 48.1\% \\   

Table 2: This table summarizes the ASR-R and ASR-M of Direct Instruct (baseline) and attacks that exploit the ChatBug vulnerability in closed-source LLMs. The results show that an attacker can effectively bypass safety alignment of LLMs by exploiting the ChatBug vulnerability. We have excluded results of format mismatch attacks on Gemini and Claude models since their APIs do not support features to execute such attacks. We detail how to launch format mismatch attacks against commercial LLMs with API access in Appendix A.2.

For example, the ChatBug vulnerability results in 100% ASR-R for the Overflow-FS attack against Llama 3, a state-of-the-art open-source LLM. This indicates that ChatBug is a severe and common vulnerability across all open-source and closed-source LLMs that have been fine-tuned with chat templates. Moreover, even if an LLM has been carefully aligned (e.g., Llama and Claude), an attacker could still exploit ChatBug to bypass the safety alignment and provoke unintended behaviors. These observations highlight the pervasivity and severity of ChatBug.

Safety alignment associated with chat templates is transferable.In Table 1, we observe that the ASR of Mismatch-C and Mismatch-V against some open-source LLMs is relatively low compared to other attacks. For example, the ASR-R of Mismatch-C is 1.3% on Llama-3. Note that Llama uses different chat templates than ChatML by . This indicates that the safety alignment by chat templates is transferable. Hence, the attack surface of ChatBug extends across all eight victim LLMs.

### ChatBug Boosts Jailbreak Attacks

Model.Our evaluations are performed on Llama-2, which has undergone strong safety alignment and also evidenced by the ASR of Direct Instruct in Table 1 with lowest ASR among all models.

Jailbreak Attacks.We consider three representative SOTA jailbreak attacks: **GCG**, **GPTFuzzer**, and **ArtPrompt**. Specifically, GCG is an optimization-based jailbreak attack where a genetic optimization is used to search for attack prompts. GPTFuzzer is an empirical jailbreak attack where prompts are generated autonomously using a mutation-based method. ArtPrompt is an automated jailbreak attack, which replaces words triggering safety alignment with ASCII art. More detailed description of these jailbreak attacks can be found in Appendix A.1.

Exploiting ChatBug significantly boosts ASR of jailbreak attacks.In Table 4, we summarize the ASR of GCG, GPTFuzzer, and ArtPrompt when they exploit the ChatBug vulnerability. We observe that exploiting the ChatBug vulnerability significantly increases their ASR. The integration of GPTFuzzer with Mismatch-\(\) achieves 99.2% ASR-R, compared to only 9.0% ASR-R when ChatBug vulnerability is not exploited. These results underscore the severity of the ChatBug vulnerability and highlight the urgent need to develop countermeasures.

    &  &  &  \\   & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-M** & **ASR-R** & **ASR-M** \\  w/o ChatBug & 41.5 & 32.9\% & 9.0\% & 7.3\% & 73.1\% & 5.8\% \\  + Mismatch-\(\) & 55.4\% & 51.2\% & 99.2\% & 83.3\% & 100.0\% & 94.0\% \\ + Overflow-S & 78.7\% & 67.1\% & 34.4\% & 24.0\% & 100.0\% & 15.8\% \\ + Overflow-L & 76.9\% & 68.3\% & 32.1\% & 41.7\% & 61.5\% & 67.7\% \\   

Table 4: This table compares ASR of jailbreak attacks when exploiting or not exploiting the ChatBug vulnerability. The results show that ASR of jailbreak attacks is significantly boosted when ChatBug variants are used as boosters.

  
**User:** & im\_start|> user \\ im\_end{ }end

## 6 Countermeasures to ChatBug

Given the severity of ChatBug, this section discusses potential countermeasures. We first describe two categories of countermeasures, _mitigation-based_ and _detection-based_ methods. We then perform empirical evaluations of these countermeasures. Based on our evaluation results, we finally discuss future directions in fine-tuning LLMs which require collaborative efforts from the community, where detailed discussion on the limitation and ethical statement is deferred to Appendix E.

### Description of Countermeasures

Mitigation-based Methods.We consider three representative mitigation-based methods including **Self-Reminder**, **SafeDecoding**, and **Adversarial Training**. Self-Reminder utilizes a system-mode prompt to the victim model to strengthen the safety. SafeDecoding is a lightweight decoding strategy to reduce the probability of generating unsafe responses by victim LLMs. Adversarial Training fine-tunes the model with adversarial examples to robustify the model against the vulnerability. We augment the dataset with adversarial examples constructed using the format mismatch attack and message overflow attack. Then we use 60% of the augmented dataset to fine-tune the victim LLM and 40% of the dataset for evaluation. Detailed setups can be found at Appendix A.3.

Detection-based Methods.Detection-based methods monitor input queries and/or generated responses. An input query or response will be blocked if it is flagged as unsafe by a detector. Typical countermeasures may employ keyword filtering  or established classifiers  to identify harmful queries or responses. Leveraging the recent advancements in LLMs, Llama Guard  can be employed to safeguard the responses generated by LLMs. Although effective, detection-based methods are less frequently adopted in the wild compared to mitigation-based methods.

### Evaluation of Countermeasures

Setup.We evaluate the countermeasures on Vicuna model since it shows the highest ASR on average according to Table 1. This indicates that Vicuna is susceptible to the ChatBug vulnerability. In addition to ASR as metrics, we adopt MT-Bench  to evaluate the multi-turn conversation and instruction following abilities of victim LLMs after deploying the countermeasures.

Experimental Results.In Table 5, we present the ASR and MT-Bench scores  of Vicuna with countermeasures being deployed. We observe that mitigation-based countermeasures including Self-Reminder and SafeDecoding fail to mitigate the ChatBug vulnerability. Although they can successfully defend against Mismatch-C, they incur significant degradation on MT-bench. Adversarial Training, however, is an effective technique to mitigate the ChatBug vulnerability, especially when the model is fine-tuned with more epochs. However, the effectiveness of Adversarial Training comes at the significant cost of performance degradation- indicated by the _degradation_ in the MT-bench score from 6.28 (comparable to Llama-2-70b-chat performance ) to 6.02 (worse than Llama-2-7b-chat performance ). These results indicate that developers need to carefully balance the trade-off between safety alignment and helpfulness in future developments of LLMs. Additional experimental evaluation on the effectiveness of adversarial training can be found in Appendix C.2.

## 7 Conclusion and Future Work

In this paper, we identified a common vulnerability, named ChatBug, induced by chat templates used during instruction tuning. We developed two attacks, format mismatch attack and message overflow attack, to exploit the ChatBug vulnerability. We assessed the severity of ChatBug vulnerability by demonstrating that malicious users could effectively provoke unintended behaviors from eight SOTA aligned LLMs. We further demonstrated that jailbreak attacks could significantly increase their attack success rates by exploiting the ChatBug vulnerability. We investigated potential techniques to mitigate ChatBug. Our experimental results showed that although adversarial training could effectively mitigate the ChatBug vulnerability, it came at the cost of degraded model performance, which highlighted the critical trade-off between safety and helpfulness during instruction tuning. Our future work will focus on this trade-off. We aim to develop new methods for instruction tuning to balance this trade-off.

Acknowledgement

This work is partially supported by the Air Force Office of Scientific Research (AFOSR) under grant FA9550-23-1-0208, the National Science Foundation (NSF) under grants IIS 2229876, and the Office of Naval Research under grant N0014-23-1-2386.

This work is supported in part by funds provided by the National Science Foundation, by the Department of Homeland Security, and by IBM. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or its federal agency and industry partners.