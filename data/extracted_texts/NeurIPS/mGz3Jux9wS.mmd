# Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction

Long-Tailed Object Detection Pre-training: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction

Chen-Long Duan\({}^{1}\), Yong Li\({}^{1}\), Xiu-Shen Wei\({}^{2}\), Lin Zhao\({}^{1}\)

\({}^{1}\)Nanjing University of Science and Technology

\({}^{2}\)School of Computer Science and Engineering, and Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications, Southeast University

Corresponding author. The first two authors contribute equally to this work. This work was supported by National Key R&D Program of China (2021YFA1001100), National Natural Science Foundation of China under Grant (62272231, 62172222), the Fundamental Research Funds for the Central Universities (4009002401), and the Big Data Computing Center of Southeast University.

###### Abstract

Pre-training plays a vital role in various vision tasks, such as object recognition and detection. Commonly used pre-training methods, which typically rely on randomized approaches like uniform or Gaussian distributions to initialize model parameters, often fall short when confronted with long-tailed distributions, especially in detection tasks. This is largely due to extreme data imbalance and the issue of simplicity bias. In this paper, we introduce a novel pre-training framework for object detection, called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL). Our method builds on a Holistic-Local Contrastive Learning mechanism, which aligns pre-training with object detection by capturing both global contextual semantics and detailed local patterns. To tackle the imbalance inherent in long-tailed data, we design a dynamic rebalancing strategy that adjusts the sampling of underrepresented instances throughout the pre-training process, ensuring better representation of tail classes. Moreover, Dual Reconstruction addresses simplicity bias by enforcing a reconstruction task aligned with the self-consistency principle, specifically benefiting underrepresented tail classes. Experiments on COCO and LVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in improving the mAP/AP scores for tail classes.

## 1 Introduction

With the advancement of deep learning, computer vision has seen significant progress, particularly in the development of large-scale pre-training and fine-tuning optimization paradigms [55; 59; 63; 65]. Numerous pre-training methods capture domain-specific or task-relevant concepts, boosting downstream performance [7; 14; 15; 27; 30; 31; 32; 48; 52]. In the field of object detection, current methods typically leverage ImageNet  and COCO  for pre-training, allowing partial model components, such as the backbone, to achieve satisfactory pre-training. However, these pre-training paradigms leave some key detection components randomly initialized and tend to overlook the suboptimal performance issues caused by long-tailed distributions during pre-training process.

In the traditional supervised pre-training paradigm, models are constrained by the distribution of labeled data, making it difficult for them to perform well in long-tailed settings, for example, in tasks of pipeline failure detection  and face recognition . While self-supervised learning has demonstrated potential in enabling models to learn richer and more effective feature representations without relying on labeled data [12; 15; 22; 27; 52], significant challenges remain. An often-overlooked but crucial challenge in long-tailed object detection is simplicity bias [23; 43; 46; 53; 54], where deep neural networks tend to rely on simpler predictive patterns while overlooking complex features that are crucial for model generalization. This bias is especially problematic for tail classes, as their limited examples make them more likely to be ignored by models that prioritize simpler patterns. To address these challenges, this work aims not only to develop a pre-training strategy that aligns with the unique demands of object detection but also to ensure its effectiveness across both balanced and long-tailed data distributions.

Motivated by this, we propose a novel pre-training framework called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL), specifically designed for long-tailed object detection pre-training. Our method incorporates Holistic-Local Contrastive Learning, which combines holistic and local feature learning to better align the pre-training process with the fine-tuning phase. To address the issues of long-tailed distributions during pre-training, 2DRCL integrates a dynamic rebalancing strategy that improves the accuracy of tail classes. Unlike traditional resampling methods, our dynamic rebalancing sampler considers instance-level imbalance, offering more precise control over class distribution and ensuring that tail classes are adequately represented. Additionally, by introducing Dual Reconstruction, our method effectively mitigates simplicity bias, enabling the model to capture both complex patterns and nuanced features that are essential for long-tailed object detection. This dual mechanism ensures that the model not only retains detailed visual information but also grasps deeper semantic relationships, which is particularly crucial for accurately recognizing and distinguishing tail classes with limited examples.

To evaluate the effectiveness of our method, we conduct extensive experiments on two benchmark datasets, i.e., COCO  and LVIS v1.0 . Experiments on these datasets from both quantitative and qualitative perspectives validate the effectiveness of our proposed method.

## 2 Related Work

Pre-training for Object Detection.Pre-training is a critical step in object detection, often involving the use of large-scale datasets to learn transferable representations. Commonly, CNNs pre-trained on image classification datasets like ImageNet  are fine-tuned for object detection tasks. Self-supervised pre-training methods [4; 6; 7; 15] have gained traction in recent years. These methods do not require labeled data and aim to learn useful representations through contrastive learning. To bridge the gap between pre-training and fine-tuning, dense-level contrastive learning methods [8; 20; 28; 51; 52; 57] explored local feature similarities between views, enhancing target perception and feature learning. Recognizing the insufficiency of pre-training solely the backbone, SoCo  advocated pre-training additional modules like FPN to process intricate scene-level information. In object detection, methods like UP-DETR  and DETReg  pre-trained entire DETR-like detectors with region matching and feature reconstruction tasks, while AlignDet  froze a pre-trained backbone during detection pre-training, achieving satisfactory results with fewer epochs. Nonetheless, these approaches still struggled with effectively addressing long-tailed distribution challenges.

Long-tailed Object Detection.In the literature [59; 63], repeat factor sampling [13; 58] aims to balance the data distribution by sampling tail classes more frequently. In object detection and segmentation tasks, achieving sample balance solely through straightforward resampling strategies is challenging due to the complexity of the scenes. Special loss functions represent another technical direction for tackling the long-tailed problem. EQL  protected tail classes from being over-supressed by ignoring negative gradients from head samples, while EQL v2  balanced gradients from head and tail classes. Seesaw loss  rebalanced the positive and negative gradients of each class using two reweighting factors. ECM loss  provided a theoretical understanding of the long-tailed tracking detection problem and introduced a novel alternative objective that optimized the margin-based binary classification error. Beyond these loss functions, methods such as supervised contrastive learning [48; 66], decoupled training [11; 40] and expert-based classifier training [56; 62; 64] have also demonstrated effectiveness under long-tailed settings. While these methods often implicitly reshape decision boundaries to protect tail classes, their indirect nature may limit their effectiveness in more complex long-tailed scenarios.

## 3 Methodology

Our goal is to develop a pre-training approach tailored to the specific requirements of object detection, while maintaining robustness across both balanced and long-tailed data distributions. To this end, we introduce a novel method called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL), specifically designed for pre-training in long-tailed object detection scenarios. In 2DRCL, we exploit a Holistic-Local Contrastive Learning (HLCL) paradigm to coordinate holistic and local feature learning to better align the pre-training with the fine-tuning phase. Building on this, a dynamic rebalancing strategy is incorporated, which emphasizes tail classes at both the image and instance (object proposal) levels to address data imbalance during pre-training. By integrating HLCL with this dynamic rebalancing strategy, we introduce a Dual Reconstruction component aimed at mitigating simplicity bias, enabling the model to concurrently capture both complex and subtle feature patterns essential for long-tailed object detection. Below, we present details of the three parts in 2DRCL.

### Holistic-Local Contrastive Learning

In 2DRCL, the HLCL mechanism serves as the foundation for pre-training object detection models. The HLCL framework encompasses two key components: Holistic Contrastive Learning (HCL) and Local Contrastive Learning (LCL). HCL focuses on learning generic visual representations, enabling the backbone model to capture comprehensive image patterns and general semantic abstractions effectively. To integrate object-level representations into the pre-training process, LCL is introduced to guide both the backbone and the detection head toward object-level details within the image. By pre-training all network components used in object detectors, LCL ensures that the model is more precisely aligned with object detection tasks, while also enhancing its ability to capture fine-grained object-level features.

#### 3.1.1 Holistic Contrastive Learning

We present HCL mechanism in Fig. 1. As illustrated, we follow the typical CL framework, i.e., MoCo [7; 15], to realize the holistic CL in our proposed 2DRCL framework. Typically, for an image \(\), we apply different image views to obtain \(x\) and \(x_{+}\) as inputs for the encoder and momentum encoder in HCL. Each view is randomly and independently augmented. Notice that the scale and location of

Figure 1: Illustration of the proposed Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL) method, which consists of the Holistic Contrastive Learning (Section 3.1.1), the Local Contrastive Learning (Section 3.1.2), and the Dual Reconstruction (Section 3.3). The whole network can be trained in an end-to-end manner.

the same object proposal are different across the augmented image views, which enables the model to learn translation-invariant and scale-invariant object-level representations in the following LCL part, which we will elaborate next.

Subsequently, \(x\) and \(x_{+}\) are transformed via separate projectors, generating holistic-level representations, \(z\) and \(z_{+}\), which are then \(_{2}\)-normalized. Subsequently, we employ the InfoNCE loss [15; 38] to drive the network training, formally:

\[_{HCL}=-/)}{(z  z_{+}/)+_{i=1}^{K}(z z_{i}/)}\,,\] (1)

where \(\) is a temperature hyper-parameter usually set as 0.2. For each input image, we use one positive and \(K\) negative samples for HCL, where \(K\) is fixed as 65,536. For the update of momentum encoder in Fig. 1, we use the same Exponential Moving Average (EMA) strategy as that in MoCo [7; 15]. Through HCL, the model is trained to effectively learn generic visual representations and capture comprehensive image patterns. However, solely relying on image-level pre-training may lead to an overemphasis on holistic representations, potentially neglecting features that are critical for object detection tasks.

#### 3.1.2 Local Contrastive Learning

To introduce object-level representations into pre-training, we incorporate the LCL mechanism to bridge the gap between pre-training process and fine-tuning phase w.r.t object detection, as illustrated in Fig. 1. Specially, we employ a class-agnostic detector  to generate a series of proposals as bounding boxes \(=\{b_{1},b_{2},,b_{n}\}\), where \(b_{i}\) denotes the \(i\)-th bounding box within the augmented input image \(x\). The object-level representation of a proposal is then obtained via object detection heads (e.g., Rol ), denoted as \(z_{bb}\). The LCL loss for the local-level representation can be formulated as:

\[_{LCL}=- z_{bb_{+}}/)}{ (z_{bb} z_{bb_{+}}/)+_{i=1}^{K}(z_{bb}  z_{bb_{i}}/)}\,.\] (2)

where \(z_{bb_{+}}\) means a corresponding positive object proposal within another augmented input image \(x_{+}\). The \(K\) negative proposals means any potential proposals within other unrelated images during training. To construct a dictionary comprising a large number of object proposals from different input images, we utilize a queue-based structure. Sequences from the current mini-batch are enqueued, while the oldest mini-batch sequences are dequeued, ensuring that the dictionary size is independent of the mini-batch size. LCL mechanism maximizes the similarity between object proposals across augmented views, enabling the model to learn comprehensive representations for diverse object proposals, thus enhancing its robustness in object detection tasks.

Finally, the objective w.r.t the HLCL mechanism can be formulated as:

\[_{HLCL}=_{c}_{HCL}+_{c}_{ LCL}\,,\] (3)

where \(_{c}\) and \(_{c}\) are the weights of HCL and LCL loss, respectively.

### Dynamic Rebalancing

To precisely control class distribution and ensure adequate representation of tail classes, we propose a dynamic resampling method that considers both images and object proposals. Unlike traditional resampling strategies, such as Repeat Factor Sampling (RFS) , which primarily emphasize class-balanced sampling, our approach aims to prioritize tail classes more effectively through resampling at both the image level and the object-proposal level. Given that object detection requires the identification and localization of specific objects, addressing instance-level imbalance in addition to image-level imbalance is expected to achieve a more balanced representation, particularly benefiting tail classes.

The proposed resampling method incorporates a dynamic adjustment mechanism, enabling the model to initially learn the overall distribution of the dataset and progressively shift its focus towards tail classes as pre-training advances. Specifically, for each category \(c\), we calculate image-level and instance-level scores, denoted as \(f_{c}^{im}\) and \(f_{c}^{in}\), respectively. Here, \(f_{c}^{im}\) indicates the proportion of images belonging to the \(c\)-th category in the entire dataset, while \(f_{c}^{in}\) represents the proportion of object proposals associated with the \(c\)-th category across the dataset. These two scores reflect the imbalance ratio for category \(c\), following the approach used in RFS . The combined score for each category, \(f_{c}\), is then defined as the harmonic mean of these two scores:

\[f_{c}=^{im} f_{c}^{in}}{_{d}f_{c}^{im}+(1-_{d} )f_{c}^{in}}\,.\] (4)

where the hyper-parameter \(_{d}\) changes dynamically throughout pre-training, defined as \(_{d}=}\), where \(T\) is the current epoch and \(T_{max}\) is the total number of pre-training epochs. As pre-training progresses, the value of \(_{d}\) increases, gradually shifting the focus from image-level balancing to instance-level balancing, enabling the model to increasingly emphasize tail classes.

To achieve balanced sampling, we define the category-level repeat factor \(r_{c}\) based on the score \(f_{c}\) using the formula \(r_{c}=(1,})\), where \(t\) is a fixed hyper-parameter set at 0.001. This repeat factor ensures that categories with lower scores (typically tail classes) are sampled more frequently during training. The dynamic resampling strategy effectively addresses data imbalance at both the image and instance levels, enhancing focus on tail classes while mitigating the risk of overfitting due to excessive repetition of rare instances.

### Dual Reconstruction

Building on the HLCL and dynamic resampling mechanisms, which provide the foundation for pre-training object detection and mitigate instance imbalance, respectively, our proposed 2DRCL framework introduces a Dual Reconstruction component to address simplicity bias. This component enables the model to concurrently capture both complex and subtle feature patterns, which are vital for effective long-tailed object detection.

#### 3.3.1 Simplicity Bias

Simplicity bias [23; 43; 46] is a phenomenon where models tend to favor simpler predictive patterns, often neglecting complex features that are critical for effective generalization. This issue is particularly prevalent in long-tailed distributions, where it significantly affects the performance on tail classes. In such scenarios, models struggle to learn the intricate and unique characteristics of these tail classes, further exacerbating the class imbalance problem.

To bridge this gap, we propose a Dual Reconstruction (DRC) component aimed at mitigating simplicity bias by enhancing feature discrimination for both head and tail classes. As shown in Fig. 1, DRC comprises two key elements: Appearance Reconstruction (AR) and Semantic Reconstruction (SR). The AR component enforces pixel-level reconstruction, compelling the model to capture as many subtle details as possible for each input image. In contrast, the SR component ensures semantic consistency between the features of the original input image and those of a corresponding randomly occluded image. We hypothesize that the effective implementation of DRC will enable the model to retain fine-grained visual information while also capturing deeper semantic relationships. This capability is particularly important for accurately recognizing and distinguishing tail classes, which often have limited training examples. This strategy ensures accurate visual representation while promoting a deeper semantic focus, enabling the model to better handle tail classes in long-tailed object detection.

#### 3.3.2 Appearance Reconstruction

To enforce appearance consistency, we utilize an auto-encoding structure specifically designed for high-fidelity reconstruction of input images. The encoder \(f\), parameterized by \(\), maps an input image \(x\) into a dense feature space, represented as \(z=f(x)\). A generator \(g\), parameterized by \(\), then attempts to invert this mapping, producing a reconstructed version: \(=g(f(x))\). Through pixel-wise image reconstruction, the Appearance Reconstruction (AR) component compels the latent features, \(f(x)\), to capture as many subtle details as possible for each input image.

AR is not merely replicating the input image; rather, it acts as an auxiliary regularization mechanism that focuses on distilling discriminative visual features relevant to the primary object detection task for each input image. By enforcing image reconstruction, AR enables the model to effectively capture both prominent and nuanced details present in the input data. The AR loss is formulated as a pixel-wise mean-squared error (MSE), expressed as:

\[_{AR}=\|x-g(f(x))\|_{2}^{2}\,.\] (5)

#### 3.3.3 Semantic Reconstruction

While AR ensures that the model captures fine-grained visual details essential for accurately representing and distinguishing between different objects, especially in cases with limited examples of tail classes, it is equally important to maintain semantic integrity in the reconstructed images. This semantic consistency allows the model to focus on the underlying meaning and context of the image, rather than merely surface-level details, thereby promoting a more robust and generalized understanding of each input.

To address this need, we introduce Semantic Reconstruction (SR), which incorporates controlled perturbations during the reconstruction process. SR is designed to preserve the semantic content of the original image while allowing the model to learn to recognize and reconstruct meaningful features even when certain parts of the image are altered or obscured. This approach ensures that the model develops a deeper understanding of each input's inherent structure and context.

Specifically, we apply a mask to a fixed percentage (e.g., 25%) of an object proposal within the reconstructed image \(g(f(x))\), resulting in a masked version, denoted as \((g(f(x)))\), where \(\) means image masking operation. This masked image is then re-encoded by the encoder to generate the corresponding latent features, \(=f((g(f(x))))\). The Semantic Reconstruction (SR) loss is computed by measuring the congruence between the feature representations of the vanilla images and those of the masked reconstructed images, evaluated across multiple layers of the network. This approach ensures that the model maintains semantic consistency while learning to recognize and reconstruct meaningful features.

\[_{SR}=_{p=1}^{P}\|f(x)-f( (g(f(x))))\|_{2}^{2}\,,\] (6)

where \(P\) represents the number of feature layers considered, and the SR loss, \(_{SR}\), is defined as the Euclidean distance between the original (vanilla) features and the reconstructed features across these layers. The SR component ensures that even in the presence of visual disruptions, the essential semantic features are preserved, allowing the model to learn robust, invariant features that go beyond superficial visual similarities. This approach enhances the model's ability to generalize by focusing on meaningful semantic information rather than just appearance.

Conclusively, our proposed DRC leverages both appearance and semantic consistency to address simplicity bias, encouraging the model to learn rich and complex feature representations essential for accurate and robust detection of tail classes. The interplay between the two reconstruction losses enhances the model's sensitivity to both fundamental visual details and higher-level semantic features, leading to a more versatile and effective detection paradigm. This combined approach ensures that the model not only captures detailed visual information but also grasps abstract semantic relationships, improving its overall performance in long-tailed object detection tasks.

The total loss for the Dual Reconstruction combines the AR and SR losses can be formulated as:

\[_{DRC}=_{r}_{AR}+(1-_{r}) _{SR}\,,\] (7)

where \(_{r}\) balances the trade-off between visual fidelity and semantic accuracy. This dual-focus strategy force the model to reconstruct the image/features for both the head and the tail classes. This, DRC enhances the model's ability to represent and detect tail classes effectively.

Overall, the final loss function of our method is optimized by:

\[=_{HLCL}+_{DRC}+_{det}\,,\] (8)

where \(_{det}\) denotes the loss of object detection that makes the pre-training consistent with the task. For simplicity, the weights of all losses in \(\) are set to 1.

## 4 Experiments

In this section, we outline the experimental settings, implementation details, and main results. Additionally, a comprehensive description of the experimental settings is provided in Section A.1 of the Appendix.

### Experimental Configurations

Datasets.We conduct experiments on two representative datasets: COCO  and LVIS v1.0 . The COCO dataset is a standard benchmark for object detection, segmentation, and captioning tasks, comprising 80 classes with a relatively balanced distribution, including 118k training images and 5k validation images. Given the balanced nature of the class distribution in COCO, we use this dataset to evaluate the performance of the proposed 2DRCL under balanced settings. In addition, we utilize the LVIS v1.0 dataset to benchmark long-tailed object detection scenarios. LVIS features 1,203 classes with a highly imbalanced distribution, containing 100k training images and 19.8k validation images. The classes in LVIS are categorized into three groups based on their frequency of occurrence : rare (1\(\)10 instances), common (11\(\)100 instances), and frequent (>100 instances). This categorization allows for a comprehensive assessment of 2DRCL's performance under long-tailed data distributions.

Implementation Details.Experiments are conducted with both Faster R-CNN and Mask R-CNN frameworks. For a comprehensive comparison, we use both ResNet-50 and ResNet-101 backbones. All models are implemented using the MMDetection toolbox . We pre-train the models on 8 RTX3090 GPUs with a batch size of 16. Unless otherwise specified, pre-training follows the 1\(\) schedule (12 epochs), starting with an initial learning rate of 0.02, which is reduced by a factor of 10 after the 8th and 11th epochs. For 2\(\) schedule, models are trained with 24 epochs, and the learning rate decays at the end of epoch 16 and 22. In our experiments, the hyper-parameters are set as follows: \(_{c}\) is set to 0.1, \(_{c}\) is set to 0.05, \(_{r}\) is set to 0.1. When conducting experimental comparisons on the LVIS v1.0 dataset in Table 3, we first use our 2DRCL for pre-training, followed by the application of existing long-tailed methods for fine-tuning to further enhance performance. Finally, we select 'ECM +2DRCL' as 'Ours' for comparison with state-of-the-art methods.

### Quantitative Results

**Mask R-CNN with R50-FPN on COCO dataset.** Table 1 presents the comparison, where all methods are pre-trained on the COCO training dataset and evaluated on the COCO validation dataset. Typically, the backbone can be initialized either from scratch or using an ImageNet pre-trained model. Methods such as DenseCL , Self-EMD , and SoCo , which are initialized from scratch, achieve an \(^{bb}\) ranging from 39.6% to 41.0%. Notably, these methods rely on 200-800 epochs for training. As a comparison, methods that utilize an ImageNet pre-trained style, including AlignDet and our proposed 2DRCL, require only 12 epochs for pre-training. Among the compared methods in Table 1, our 2DRCL demonstrates superior object detection performance, achieving the highest \(^{bb}\) of 41.4% and \(^{mk}\) of 37.3%, significantly outperforming both AlignDet and the supervised baseline. This improvement can be attributed to 2DRCL's capability to narrow the gap between pre-training and fine-tuning. By effectively bridging this gap, 2DRCL is able to leverage the benefits of the pre-trained model more efficiently for object detection tasks.

   Backbone Initialization & Methods & \(^{bb}\) & \(^{bb}_{50}\) & \(^{bb}_{75}\) & \(^{mk}\) & \(^{mk}_{50}\) & \(^{mk}_{75}\) \\   & DenseCL  & 39.6 & 59.3 & 43.3 & - & - & - \\  & Self-EMD  & 40.4 & 61.1 & 43.7 & 37.4 & 56.5 & **39.7** \\  & SoCo  & 40.6 & 61.1 & 44.4 & - & - & - \\  & SlotCo  & 41.0 & 61.1 & 45.0 & - & - & - \\   & Supervised & 38.3 & 58.0 & 42.1 & 34.3 & 54.9 & 36.6 \\  & AlignDet  & 39.4 & 59.2 & 43.2 & 35.3 & 56.1 & 37.7 \\    & Ours & **41.4** & **61.3** & **45.8** & **37.4** & **57.2** & 39.4 \\   

Table 1: Comparisons with state-of-the-art methods on COCO (Mask R-CNN with R50-FPN).

   Method & \(^{bb}\) & \(^{bb}_{r}\) & \(^{bb}_{c}\) & \(^{bb}_{f}\) \\  MoCo v2  & 14.5 & 3.9 & 12.4 & 21.6 \\ SimCLR  & 19.9 & 8.0 & 18.1 & 27.1 \\ BYOL  & 15.3 & 5.4 & 13.2 & 21.9 \\ SoCo  & 17.6 & 5.3 & 15.9 & 24.9 \\ AlignDet  & 22.6 & 10.3 & 20.8 & 29.9 \\  Ours & **23.9** & **11.9** & **22.3** & **31.0** \\   

Table 2: Comparisons with pre-trained methods on LVIS v1.0 with a 1\(\) scheduler using Mask R-CNN.

[MISSING_PAGE_FAIL:8]

**Ablation Analysis across 2DRCL Components.** To investigate the contribution for each component in 2DRCL, we evaluate our 2DRCL on the LVIS v1.0 dataset. As shown in Table 5, incorporating HLCL, which combines HCL and LCL, results in a 1.7% improvement in \(_{r}^{bb}\) over the baseline. HCL focuses on learning generic visual representations, enabling the backbone to capture comprehensive image patterns and semantic abstractions, while LCL ensures precise alignment with object detection tasks and enhances the capture of fine-grained object-level features. Additionally, DRB dynamically rebalances the data distribution and helps to significantly boosts performance for tail classes, leading to a 3.5% improvement in \(^{bb}\). The Dual Reconstruction (DRC) mechanism, comprising AR and SR, brings the total improvement to 6.1%. AR enforces pixel-level reconstruction, compelling the model to capture subtle visual details, while SR ensures semantic consistency between the original and occluded images. This combination allows the model to retain intricate visual information while capturing deeper semantic relationships, resulting in enriched and coherent feature representations.

### Further Analysis

In this section, we conduct a thorough analysis of our proposed 2DRCL, emphasizing its role in mitigating simplicity bias and enhancing feature representation through DRC mechanism.

Error Analyses.To determine which error types our 2DRCL method effectively mitigates, we conducted an error analysis experiment. Following the error categorization paradigm from YOLO , we classify the top N predictions for each class into five error types. The pie charts in Figure 2 show the distribution of these errors for rare, common, and frequent classes on the LVIS v1.0 validation set. As shown in Figure 2, our 2DRCL shows noticeable improvements for rare object classes,

   Fine-tuning Schedule & \(^{bb}\) & \(^{bb}_{50}\) & \(^{bb}_{75}\) \\ 
1\(\) & 38.3 & 58.0 & 42.1 \\
2\(\) & 38.8 & 58.4 & 42.4 \\
3\(\) & 39.0 & 58.7 & 42.9 \\
4\(\) & 39.2 & 59.5 & 42.9 \\ 
1\(\) (Ours) & 41.4 & 61.3 & 45.8 \\   

Table 4: Comparisons w.r.t different fine-tuning epochs on COCO. The preceding four methods exploit ImageNet pre-trained backbone.

   HCL & LCL & DRB & AR & SR & \(^{bb}\) & \(^{bb}_{r}\) & \(^{bb}_{c}\) & \(^{bb}_{f}\) \\  ✗ & ✗ & ✗ & ✗ & ✗ & 22.7 & 9.1 & 21.5 & 30.0 \\ ✗ & ✗ & ✗ & ✗ & ✗ & 22.5 & 10.5 & 21.0 & 29.3 \\ ✗ & ✓ & ✗ & ✗ & ✗ & 21.9 & 9.8 & 20.8 & 28.7 \\ ✗ & ✓ & ✗ & ✗ & ✗ & 22.4 & 10.8 & 21.1 & 29.0 \\ ✗ & ✓ & ✓ & ✗ & ✗ & 23.8 & 14.3 & 22.3 & 30.1 \\ ✗ & ✓ & ✓ & ✓ & ✗ & 24.2 & 14.9 & 22.6 & 30.3 \\ ✗ & ✓ & ✓ & ✓ & ✓ & **24.4** & **15.2** & **22.7** & **30.3** \\   

Table 5: Ablations for various components in our 2DRCL on LVIS v1.0.

Figure 2: Error analyses comparisons. 2DRCL achieves superior performance on tail classes without significantly compromising accuracy for the more frequent classes.

with correct predictions increasing from 51.8% in the baseline to 53.5%, alongside a reduction in both non-background classification errors and background prediction errors. This suggests that our 2DRCL enhances the model's ability to accurately classify the rare objects and accurately distinguish them from the background. Although there is a slight accuracy decrease for common and frequent classes, this trade-off is minimal, with the gains in rare class detection outweighing these minor losses. This demonstrates that our method effectively addresses long-tailed object detection challenges by improving performance on tail classes without obviously compromising accuracy for other frequent classes across the dataset.

Simplicity Bias Analyses.To explicitly illustrate how our method addresses simplicity bias, we present a visualization of the activations corresponding to randomly sampled test images from the LVIS v1.0 dataset in Figure 3. The results demonstrate that 2DRCL effectively mitigates simplicity bias in long-tailed object detection by learning more comprehensive patterns that encompass informative regions, particularly for images belonging to tail classes. In comparison, 2DRCL consistently identifies more critical regions than ECM , highlighting the superiority of our approach in addressing simplicity bias. The comparisons presented in the fourth and fifth rows underscore the effectiveness of the proposed DRC mechanism, revealing that the introduction of the DRC mechanism significantly enhances feature attention on tail classes while reducing background interference. This finding further indicates that the DRC plays a crucial role in mitigating simplicity bias, enabling the model to retain intricate visual details and capture deeper semantic relationships, thereby producing enriched and coherent feature representations.

## 5 Conclusions and Limitations

We proposed Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL) to address the challenges posed by long-tailed distributions in object detection pre-training. By integrating holistic and local contrastive learning with dynamic rebalancing and dual reconstruction, 2DRCL aligned the pre-training strategy with the specific demands of object detection, ensuring effectiveness across both balanced and long-tailed data. It successfully mitigated simplicity bias for tail classes, enhancing their feature representations and overall performance. Experiments demonstrated significant improvements in attention to tail classes and reduced background errors, as confirmed by both quantitative and qualitative analyses. However, our method had limitations, particularly in its relatively high computational costs. Future work will focus on optimizing computational efficiency.

Figure 3: Attention map comparisons w.r.t Baseline , ECM , 2DRCL (w/o DRC) and 2DRCL (our method) on LVIS dataset. The top row shows the corresponding class names of the input images. Best viewed in color.