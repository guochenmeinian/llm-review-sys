# Percentile Criterion Optimization in Offline Reinforcement Learning

Elita A. Lobo

Department of Computer Science

University of Massachusetts Amherst

elobo@umass.edu

&Cyrus Cousins

Department of Computer Science

University of Massachusetts Amherst

cbcousins@umass.edu

&Yair Zick

Department of Computer Science

University of Massachusetts Amherst

yzick@umass.edu

&Marek Petrik

Department of Computer Science

University of New Hampshire

mpetrik@cs.unh.edu

###### Abstract

In reinforcement learning, robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the _percentile criterion_. The percentile criterion is approximately solved by constructing an _ambiguity set_ that contains the true model with high probability and optimizing the policy for the worst model in the set. Since the percentile criterion is non-convex, constructing ambiguity sets is often challenging. Existing work uses _Bayesian credible regions_ as ambiguity sets, but they are often unnecessarily large and result in learning overly conservative policies. To overcome these shortcomings, we propose a novel Value-at-Risk based dynamic programming algorithm to optimize the percentile criterion without explicitly constructing any ambiguity sets. Our theoretical and empirical results show that our algorithm implicitly constructs much smaller ambiguity sets and learns less conservative robust policies.

## 1 Introduction

Batch Reinforcement Learning (Batch RL)  is popularly used for solving sequential decision-making problems using limited data. These algorithms are crucial in high-stakes domains where exploration is either infeasible or expensive, and policies must be learned from limited data. In model-based Batch RL algorithms, transition probabilities are learned from the data as well. Due to insufficient data, these transition probabilities are often imprecise. Errors in transition probabilities can accumulate, resulting in low-performing policies that fail when deployed.

To account for the uncertainty in transition probabilities, prior work uses Bayesian models [10; 13; 27; 40; 45; 47] to model uncertainty and optimize the policy to maximize the returns corresponding to the worst \(\)-percentile transition probability model. These policies guarantee that the true expected returns will be at least as large as the optimal returns with high confidence. This technique is commonly referred to as the _percentile-criterion_ optimization. Unfortunately, the percentile criterion is NP-hard to optimize. Thus, current work uses _Robust Markov Decision Processes_ (RMDPs) to optimize a lower bound on the percentile criterion. An RMDP takes as input an ambiguity set (uncertainty set) that contains the true transition probability model with high confidence and finds a policy that maximizes the returns of the worst model in the ambiguity set.

Since the percentile criterion is non-convex, constructing ambiguity sets itself is a challenging problem. Existing work uses Bayesian credible regions (BCR)  as ambiguity sets. However, these ambiguity sets are often unnecessarily large [15; 40] and result in learning conservative robust policies.

Some recent works approximate ambiguity sets using various heuristics [3; 40], but we show that they remain too conservative. Thus, the question of the _optimal ambiguity set, i.e., ambiguity sets that result in optimizing the tightest possible lower bound on the percentile criterion and less-conservative policies_ remains unanswered.

Our ContributionsIn this paper, we answer two important questions: a) _Are Bayesian credible regions the optimal ambiguity sets for optimizing the percentile criterion?_ b) _Can we obtain a less conservative solution to the percentile criterion than RMDPs with BCR ambiguity sets while retaining its percentile guarantees?_ Our theoretical findings show that Bayesian credible regions can grow significantly with the number of states and therefore, tend to be unnecessarily large, resulting in highly conservative policies. As our main contribution, we provide a dynamic programming framework (Section 3), which we name the \(\) framework, for optimizing a lower bound on the percentile criterion without explicitly constructing ambiguity sets. Specifically, we propose a new robust Bellman operator, the Value at Risk (\(\)) Bellman operator, for optimizing the percentile criterion. We show that it is a valid contraction mapping that optimizes a tighter lower bound on the percentile criterion, compared to RMDPs with BCR ambiguity sets (Section 3). We theoretically analyze and bound the performance loss of our framework (Section 3.1). We provide a Generalized \(\) Value iteration algorithm and analyze its error bounds. We also show that there exist directions in which the Bayesian credible regions can grow unnecessarily large with the number of states in the MDP and possibly result in a conservative solution. On the other hand, the ambiguity sets implicitly optimized by the \(\) Bellman operator tend to be smaller, i.e., they have a smaller asymptotic radius and are independent of the number of states (Section 4). Finally, we empirically demonstrate the efficacy of our framework in three domains (Section 5).

### Related Work

Several work[13; 34; 40] propose different methods for solving the percentile criterion, as well as other robust measures for handling uncertainty in the transition probabilities estimates. Russel and Petrik  and Behzadian et al.  propose various heuristics for minimizing the size of the ambiguity sets constructed for the percentile-criterion. Russel and Petrik  propose a method that interleaves robust value iteration with ambiguity set size optimization. Behzadian et al.  propose an iterative algorithm that optimizes the weights of \(_{1}\) and \(_{}\) ambiguity sets while optimizing the robust policy. However, these methods still construct Bayesian credible sets which can be unnecessarily large and result in conservative policies, as we show in Section 5.

Other works consider partial correlations between uncertain transition probabilities to mitigate the conservativeness of learned policies [4; 15; 19; 29; 30]. These approaches mitigate the conservativeness of S- and SA-rectangular ambiguity sets by capturing correlations between the uncertainty and by limiting the number of times the uncertain parameters deviate from the mean parameters. Despite these heuristics, most of these works [2; 20; 40; 53] either rely on weak statistical concentration bounds to construct frequentist ambiguity sets, or use Bayesian credible regions as ambiguity sets. These sets still tend to be unnecessarily large [15; 40], resulting in conservative policies.

Finally, a large number of works [12; 16; 36; 44; 49; 50; 52] have proposed RL algorithms that use measures like Conditional Value at Risk, Entropic risk measure amongst other risk measures. However, we note that these works use risk measures to obtain robustness guarantees against aleatoric uncertainty (system uncertainty) and not epistemic uncertainty (model uncertainty), which is the focus of our work. Since these works optimize a completely different objective, we do not compare our framework against theirs. Robust RL work [2; 14; 20; 28; 32; 55] proposes other robust measures for handling uncertainty in transition probabilities; however, these approaches do not provide probabilistic guarantees on the expected returns, and compute overly conservative policies.

## 2 Preliminaries

In the standard reinforcement learning setting, a sequential decision task is modeled as a Markov Decision Process (MDP) [37; 48]. An MDP is a tuple \(,,P,R,_{0},\) that consists of (a) a set of states \(=\{1,2,,S\}\), (b) a set of actions \(=\{1,2,,A\}\), (c) a deterministic reward function \(R\), (d) a transition probability function \(P^{S}\), (e) an initial state distribution \(_{0}^{S}\), where \(^{S}\) represents the \(S\)-dimensional probability simplex, and (f) a discount factor \(\). We use \(_{s,a}\) to denote the vector of transition probabilities \(P(s,a,)\) corresponding to the state \(s\) and action \(a\). Likewise, we use \(_{s,a}\) to denote the vector of rewards \(R(s,a,)\) corresponding to state \(s\) and action \(a\). A Markovian policy \(^{A}\) maps each state \(s\) to a distribution over actions \(\). In a general RL setting, the goal is to compute a policy \(\) that maximizes the expected discounted return \((,P)\) over an infinite horizon,

\[_{}(,P)=_{}[_{t=0}^{ }^{t}r(s_{t},a_{t},s_{t+1}) s_{0}_{0},a_{t}(s_ {t}),s_{t+1} P(s_{t},a_{t},)]\.\]

The value of a policy \(\) at any state \(s\) is the discounted sum of rewards received by an RL agent, if it starts from state \(s\), i.e., \(v^{}(s)=[_{t=0}^{}^{t}r(s_{t},a_{t},s_{t+1} )|s_{0}=s,a_{t},s_{t+1} P(s_{t},a_{t},)]\).

We assume a _batch reinforcement learning_ setting  where the reward function is known, but the true transition probabilities \(P^{*}\) are unknown. Following prior work on robust Bayesian RL , we use parametric Bayesian models to represent uncertainty over the true transition probabilities \(P^{*}\). We will use \(}=\{}_{s,a}\}_{s,a}\) to denote the random transition probabilities. We assume we have a batch of data \(=\{s_{i},a_{i},s^{}_{i}\}_{i=1}^{N}\), which in conjunction with some prior distribution defines the _posterior distribution_ of transition probabilities \(}\).

The Fisher information measures the amount of information about the unknown true parameters \(^{*}^{d}\) carried by an observable random variable \(}^{m}\). If \(f(;^{*})\) is the probability density of \(}\) conditioned on \(^{*}\), then the Fisher information is given by \(I(^{*})=( f(};^ {*}))( f(};^{*}))^{}\).

Then, in the Bayesian setting, the Bernstein von Mises theorem  states that under mild regularity conditions the posterior distribution of the parameters \(}\) converges in the limit to the distribution of the MLE of \(^{*}\) which is asymptotically Gaussian, in particular, \(_{N}(}-^{*}) (,I(^{*})^{-1})\) where \(\) indicates convergence in distribution. In many cases, the above holds even though the conditions of the Bernstein-von Mises theorem are not met . Of particular interest in this setting is the Dirichlet distribution where the asymptotic MLE is a multivariate Gaussian under certain conditions on the prior distribution , although in this case, it is degenerate (i.e., the covariance matrix is not full-rank).

For asymptotic results, we assume that the data \(\) is sampled such that each state \(s\) and action \(a\) is observed infinitely many times as \(N\). Furthermore, we assume henceforth that the prior is asymptotically negligible, and the MLE is asymptotically Gaussian with covariance matrix \(I(^{*})^{-1}/_{N}\). Therefore, the posterior distribution of \(}\) is also asymptotically Gaussian with covariance matrix \(I(^{*})^{-1}/_{N}\). Under these conditions, we will estimate the Fisher information corresponding to \(^{*}=^{*}\) using the asymptotic covariance of the posterior distribution of \(}\).

To avoid unnecessary computational technicalities, we will assume that \(}\) is a discrete random variable taking on values \(}(),\) for some \(=\{1,,M\}\) with a distribution \(f\). That is, the random variable \(}\) represents a discrete approximation of the true, possibly continuous posterior, as is common in methods like Sample Average Approximation (SAA) .

Percentile CriterionThe \(\)-percentile criterion is popularly used to derive robust policies under model uncertainty . It aims to compute a policy \(\) that maximizes the returns corresponding to the worst \(\)-percentile model:

\[*{arg\,max}_{}\{y\,\,_{ } f}(,}) y\, 1- \}.\] (1)

The value \(y\) lower-bounds the true expected discounted returns with confidence \(1-\) where \((0,}{{2}})\). Optimizing the percentile criterion is equivalent to optimizing the Value at Risk (\(_{}\)) of expected discounted returns when there exists uncertainty in transition probabilities \(}\) and the expected returns function \(\) is lower-semicontinuous. The optimization in (1) is equivalent to

\[_{}_{}[(,})]\ \,\] (2)

where \(_{}\) of a bounded random variable \(\) with a CDF function \(F:\) is defined as 

\[_{}[]=\{t:[ t]  1-\}\ \.\] (3)A lower value of \(\) in (1) indicates a higher confidence in the returns achieved in expectation. For example, \(_{0.05}[(,})]=x\) indicates that the true returns will be at least equal to the robust returns \(x\) for \(95\)% of the transition probability models. When clear from context, we use \(\) to denote the Value at Risk at confidence level \(\). Unfortunately, the optimization problem in (1) is NP-hard to optimize and is usually approximately solved using Robust MDPs.

Robust MDPsRobust MDPs (RMDPs) generalize MDPs to account for uncertainty, or ambiguity, in the transition probabilities. An _ambiguity set_ for an RMDP is constructed such that it contains the true model with high confidence. The optimal policy of a Robust MDP \(^{*}\) maximizes the returns of the worst model in the ambiguity set: \(^{*}=_{}_{}\,(,)\). General RMDPs are NP-hard to solve , but they are tractable for broad classes of ambiguity sets. The simplest such type is the SA-rectangular ambiguity set , defined as

\[=\{(^{S})^{S A}_{s,a} _{s,a},\; s,\, a\},\]

for a given \(_{s,a}^{},s,a\). SA-rectangular ambiguity sets  assume that the transition probabilities corresponding to each state-action pair are independent. Similarly to MDPs, the optimal robust value function \(^{*}^{S}\) for an SA-rectangular RMDP is the unique fixed point of the robust Bellman optimality operator \(^{S}^{S}\) defined as \(()_{s}=_{a}_{_{s,a}_{s,a}}_{s,a}^{}(_{s,a}+)\).

To optimize the percentile criterion, an SA-rectangular ambiguity set \(\) is constructed such that it contains the true model with high probability, and thus, the following equation holds.

\[[(,})_{P}(,) ] 1-.\]

Although RMDPs have been used to solve the percentile criterion , the quality of the robust policies it computes depends mainly on the size of the ambiguity sets. The larger the ambiguity sets, the more conservative the robust policy . SA-rectangular ambiguity sets are most commonly studied; thus we focus our attention on SA-rectangular Robust MDPs. We investigate whether Bayesian credible regions are optimal ambiguity sets for optimizing the percentile criterion. We refer to SA-rectangular RMDPs and SA-rectangular ambiguity sets as Robust MDPs and ambiguity sets respectively.

Our work focuses on Bayesian (rather than frequentist) ambiguity sets. Bayesian ambiguity sets are usually constructed from Bayesian credible regions (BCR) . Given a state \(s\) and an action \(a\), let \(_{s,a}\) represent the size of the BCR ambiguity sets; \(_{s,a}^{_{}}\) and \(}_{s,a}\) represent the mean transition probabilities. The set \(_{s,a}^{_{}}\) is constructed as

\[_{s,a}^{_{}}=_{s,a}(,,q)= \{_{s,a}^{S}\|_{s,a}-}_{s,a}\|_{q, }_{s,a}\},\] (4)

where \(\{1,\}\) represents the norm of the weighted ball in (4) and \(_{+}^{S}\) is a weight vector. Here, \(\) is jointly optimized with \(\) to minimize the span of the ambiguity sets such that the true model is contained in the ambiguity set with high confidence, i.e., \((}_{s,a}_{s,a}(,,q)) 1-\). We refer to BCR ambiguity sets with non-uniform weights as weighted BCR ambiguity sets. We refer to the Robust Bellman optimality operator with BCR ambiguity sets (\(_{_{}}\)) as the BCR\({}_{}\) Bellman optimality operator, and to RMDPs with BCR ambiguity sets as BCR RMDPs. For any \((0,}{{2}})\), setting the confidence level \(\) in \(_{_{}}\) to \(}{{SA}}\) for all state-action pairs yields \(1-\) confidence on the returns of the optimal robust policy . However, we show that even span-optimized BCR RMDPs can be sub-optimal for optimizing the percentile criterion.

We use the shorthand \(_{s,a}\) for any \(s\), \(a\) to denote the vector of values associated with value \(^{S}\) and the one-step return from state \(s\) and action \(a\), i.e., \(_{s,a}=_{s,a}+\). We use \(}_{s,a}^{s}\) and \(_{s,a}^{S S}\) for any \(s\), \(a\) to represent the empirical mean and covariance of transition probabilities \(}_{s,a}\) estimated from \(\). We use _tilde_ to indicate that it is a random variable. We use \(()\) and \(()\) to represent the probability distribution function (PDF) and cumulative distribution function (CDF) respectively of the normal distribution with mean \(0\) and variance \(1\). The \(\)-Minkowski norm \(\|\|_{Z}\) for a vector \(\) given some positive-definite matrix \(\) is defined as \(\|\|_{}=^{}^{-1}}\).

We illustrate the conservativeness of BCR\({}_{}\) ambiguity sets with the following example.

_Example 2.1_.: Consider an MDP with four states \(\{s_{0},s_{1},s_{2},s_{3}\}\) and a single action \(\{a_{0}\}\). The state \(s_{0}\) is the initial state and the states \(s_{1},s_{2},s_{3}\) are terminal states with zero rewards. For the sake of simplicity, we assume that it is only possible to transition to state \(s_{1},s_{2}\) and \(s_{3}\) from state \(s_{0}\). The transition probability of \(_{s_{0},a_{0}}\) is uncertain and distributed as a Dirichlet distribution \(}_{s_{0},a_{0}}(10,10,1)\) with mean \([0.48,0.48,0.04]\). The rewards for transitions from state \(s_{0}\) are given by \(_{s_{0},a_{0}}=[0.25,0.25,-1]\).

We wish to optimize the percentile criterion with confidence level \(=0.2\). Following the sampling procedure proposed by Russel and Petrik  to construct a uniformly weighted BCR\({}_{}\) ambiguity set for \(}_{s_{0},a}\) with 100 posterior samples, yields an ambiguity set \(^{_{}}_{s_{0},a_{0}}=\{^{S} \|-}_{s_{0},a_{0}}\|_{1} 0.277\}\). In this case, the reward estimate against the worst model in the ambiguity set \(=[0.50,0.32,0.18]\) is \(^{_{}}=0.025\). Since we have a single non-terminating state in the MDP, the percentile returns are given by \(^{_{}}=_{0.2}[}_{s_{0},a_{0}}^{ }_{s_{0},a_{0}}]\). Computing \(^{_{}}\) for Dirichlet distribution \((10,10,1)\), we get \(^{_{}}=0.17>^{_{}}\). Thus, this example shows that BCR ambiguity sets can be unnecessarily large, and thereby result in conservative policies.

## 3 VaR Framework

We introduce the \(\) Bellman optimality operator \(_{_{}}\) for approximately solving the percentile criterion. We show that \(_{_{}}\) is a valid Bellman operator: it is a contraction mapping and lower bounds the percentile criterion. For any value function \(^{}\), state \(s\) and action \(a\), we define the \(\) Bellman optimality operator \(_{_{}}\) as

\[(_{_{}})_{s}=_{a}_{}[}_{s,a}^{}(_{s,a}+) ].\] (5)

For each state \(s\), \(_{_{}}\) maximizes the value corresponding to the worst \(\)-percentile model. In contrast to the BCR\({}_{}\) Bellman optimality operator \(_{_{}}\), computing \(_{_{}}\) does not require constructing ambiguity sets from confidence regions; it can simply be estimated from samples of the model posterior distribution, as we later show.

**Proposition 3.1** (Validity).: _The following properties of \(_{_{}}\) hold for all value functions \(,^{}\)._

1. _The operator_ \(_{_{}}\) _is contraction mapping on_ \(^{}\)_:_ \(\|_{_{}}-_{_{ }}\|_{}\|-\|_{}.\)__
2. _The operator_ \(_{_{}}\) _is monotone:_ \(_{_{}} _{_{}}\)_._
3. _The equality_ \(_{_{}}}=}\) _has a unique solution._

Proposition 3.1 formally proves that \(_{_{}}\) is a valid Bellman operator, i.e., it is a contraction mapping, monotone, and has a unique fixed point \(}=(_{_{}}})\). Here on, we will refer to the policy (\(\)) corresponding to the fixed point value \(}\) as the \(_{}\) policy.

Figure 1: Figure 0(a) (left) compares the asymptotic radius of \(^{-1}\)-Minkowski norm BCR ambiguity sets to \(_{}\) ambiguity sets, where \(\) is the covariance matrix. The size of the BCR ambiguity sets significantly grows with the number of states. Figure 0(b) and Figure 0(c) (right) compare the asymptotic forms of BCR\({}_{}\) and \(_{}\) ambiguity sets under high and low uncertainty in \(}\) at confidence level \(=0.2\) respectively. The grey dots represent the transition probabilities samples from a Dirichlet distribution. The sizes of BCR\({}_{}\) and \(_{}\) ambiguity sets increase with an increase in the uncertainty in \(}\), however, the \(_{}\) ambiguity sets are smaller than the BCR\({}_{}\) ambiguity sets.

We now show that the \(\) Bellman optimality operator \(_{_{}}\) optimizes a lower bound on the percentile criterion. Given a policy \(\), a state \(s\), and transition probabilities \(\), let

\[(_{}}^{})_{s}=_{a}(s,a) _{s,a}^{}(_{s,a}+),\]

represent the Bellman evaluation operator for transition probabilities \(\). Furthermore, let

\[(_{_{}}^{})_{s}=_{a} (s,a)\,_{}[}_{s,a}^{}(_{ s,a}+)],\]

represent the \(\) Bellman evaluation operator for random transition probabilities \(}\). We use \(}^{}\) to denote the fixed point of \(_{_{}}^{}\). Furthermore, we use \(}^{}\) to represent the random fixed point of \(_{}}^{}\), which is computed using a random realization of the transition probabilities \(}\) sampled from the posterior distribution conditioned on observed transitions \(\).

**Proposition 3.2** (Lower Bound Percentile Criterion).: _For any \((0,}{{2}})\), if we set the confidence level \(\) in the operator \(_{_{}}^{}\) to \(}{{S}}\), then for every policy \(:_{}}[}^{}}^{ }|] 1-\)._

Proposition 3.2 shows that for any policy \(\) and state \(s\), the \(_{}\) value at state \(s\), \(}^{}(s)\) lower bounds the true value \(}^{}(s)\) with high confidence. Comparing Proposition 3.2 with the definition of the percentile-criterion in (1), we see that the percentile-criterion requires confidence guarantees only on the returns computed from the initial states, whereas the equation in Proposition 3.2 provides confidence guarantees on the value of every state. Therefore, for any policy \(\), the value \(_{0}^{}}^{}\) is a lower bound on the percentile-criterion objective \(_{}[(,})]\). Since \(_{_{}}\) finds a policy \(\) that maximizes the value \(_{0}^{}}^{}\), it follows  that \(_{_{}}\) optimizes a lower bound on the percentile criterion in (1).

**Proposition 3.3**.: _Suppose that \(}_{s,a}\) for any state \(s\) and action \(a\), is a multivariate sub-Gaussian with mean \(}_{s,a}\) and covariance factor \(_{s,a}\), i.e., \([((}_{s,a}-}_{s,a})^ {})](^{2}^{}_{s,a}/2),, ^{S}\). Then, for any state \(s\), \(_{_{}}\) satisfies_

\[(_{_{}})_{s}_{a} (}_{s,a}^{}_{s,a}-} {{}})}_{s,a}_{s,a}_{s,a}}).\]

_As a special case, when \(}_{s,a}\) is normally distributed \(}_{s,a}(}_{s,a},_{s,a})\), then \(_{_{}}\) for any state \(s\) can be expressed as_

\[(_{_{}})_{s}=_{a}( }_{s,a}^{}_{s,a}-^{-1}(1-)_{ s,a}_{s,a}_{s,a}}).\]

Proposition 3.3 shows that by assuming that the transition probabilities are sub-Gaussian, we can easily compute a lower bound of the \(\) Bellman update \((_{_{}})\) for a given value function using only the mean and the covariance matrix of \(}\). In the special case where \(}\) is normally distributed, we can compute the \(\) Bellman optimality operator \(_{_{}}()\) exactly.

### Performance Guarantees

We now derive finite-sample and asymptotic bounds on the loss of the \(\) framework.

**Theorem 3.4** (Performance).: _Let \(}\) be the fixed point of the \(\) Bellman optimality operator \(_{_{}}\) and \(^{*}\) be the optimal policy in (1). Let \(^{*}=_{}[(^{*},})]\) denote the optimal percentile returns and \(=_{0}^{}}\) denote the lower bound on the percentile returns computed using the Bellman operator \(_{_{}}\) with \(=}{{S}}\). Then for each \((0,}{{2}})\):_

\[^{*}-_{s}_{a }(_{1-}[}_{s,a}^ {}_{s,a}]-_{}[}_{s,a}^ {}_{s,a}]).\] (6)

Theorem 3.4 bounds the finite sample performance loss of the \(\) framework. The loss varies proportionally to the maximum difference between the \(}{{S}}\) and \(1-(}{{S}})\) percentile of the one-step Bellman update for the optimal robust value function \(}\). Furthermore, the \(\) framework performs better when the uncertainty in the transition probabilities is small.

**Theorem 3.5** (Asymptotic Performance).: _Suppose that the normality assumptions on the posterior distribution \(}\) in Section 2 are satisfied. For any \((0,}{{2}})\), set \(=}{{S}}\) in \(_{_{}}\). For any state \(s\) and action \(a\), let \(I(^{*}_{s,a})\) be the Fisher information matrix corresponding to the true transition probabilities \(^{*}_{s,a}\). Furthermore, let \(^{2}_{}=_{s,a}} ^{}_{s,a}I(^{*}_{s,a})^{-1}}_{s,a}\) be the maximum over state-action pairs of the asymptotic variance of the returns estimate \(}^{}_{s,a}}_{s,a}\). Then the asymptotic performance of the \(\) framework \(\) w.r.t. the optimal percentile returns \(^{*}\) satisfies_

\[_{N}(^{*}-)(2 ^{-1}(1-}{{S}})_{})}{{S}})}_{}.\]

Theorem 3.5 shows that the asymptotic loss in performance of the \(\) framework convergence to 0, i.e., \(_{N}(^{*}-)=0\).

### Dynamic Programming Algorithm

We provide a detailed description of the \(\) value iteration algorithm (Algorithm 3.1) below. We also bound the number of samples required to estimate a single \(\) Bellman update \((^{}_{_{}})_{s}\) for any given policy \(\) and state \(s\) with high confidence \(1-\).

``` Input: Confidence \((0,}{{2}})\), Value approximation error \( 0\), Transition models \(}(_{1}),}(_{2}),,}( _{M})\) sampled from posterior distribution \(f\) Output: Robust policy \(_{k}\), Lower bound \(_{k}\)
1 Initialize robust value-function \(_{0}=,k=0\)repeat
2for\(s 1\)to\(S\)do
3for\(a 1\)to\(A\)do
4\(_{s,a}_{s,a}+_{k}\)// 1-step return from \((s,a)\)\(_{a}}_{}[}^{ }_{s,a}_{s,a}]\)// empirical \(_{}\)
5 end for
6
7 end for
8\(k k+1\)\(_{k}(s)_{a}(_{a})\), \(_{k}(s)_{a}(_{a})\)// \(_{}\) Bellman optimality update
9until\(_{k}-_{k-1}\|_{}}{{}})}}{{}}\)return\(_{k},_{k}\) ```

**Algorithm 3.1**Generalized VaR Value Iteration Algorithm

In each iteration of Algorithm 3.1, we compute the one-step \(\) Bellman update \(_{_{}}()\) using the current value function \(\). When \(}\) is not normally distributed, we use the Quick Select algorithm  to efficiently compute the empirical estimate of the \(\)-percentile of 1-step returns for any state \(s\), action \(a\), and value function \(\), i.e., \(}_{}[}^{}_{s,a}(_{s,a }+)]\). On the other hand, when \(}\) is normally distributed, we compute the \(\) Bellman optimality update \(_{_{}}()\) using the empirical estimate of mean \((}_{s,a})_{s,a}\) and covariance \((_{s,a})_{s,a}\) of the transition probabilities derived from the data \(\) (Proposition 3.3). We repeat these steps until convergence.

**Proposition 3.6** (Empirical Error Bound).: _For any state \(s\), action \(a\) and value function \(\), let \(}_{}[}^{}_{s,a}_{s,a}]\) represent the empirical estimate of \(\)-percentile of returns \(_{}[}^{}_{s,a}_{s,a}]\) and \(_{f}\) represent the cumulative density function (CDF) of the random estimate of returns \(}^{}_{s,a}_{s,a}\). Suppose that \(_{f}\) is differentiable at the point \(_{}[}^{}_{s,a}_{s,a}]\) and let \(=^{}_{f}(_{}[}^{}_{s,a}_{s,a}])\) represents the density of estimate of returns at point \(_{}[}^{}_{s,a}_{s,a}]\). Let \(M^{*}\) be the number of posterior samples required to obtain empirical error \(\), with confidence \(1-\), where \(0<<1\), i.e., \([|}_{}[}^{}_{s,a}_{s,a}]-_{}[}^{}_{s,a}_ {s,a}]|>]\). Then, \(_{ 0}M^{*}^{2}=}{{}})}}{{2^{2}}}\)._

We now show that Algorithm 3.1 produces a policy \(_{k}\) and value function \(_{k}\) that approximates the optimal value function \(}\).

**Proposition 3.7** (Value Iteration Error).: _Define the empirical \(\) Bellman optimality operator \(_{}_{}}\) for any value \(^{S}\) and state \(s\) as \((_{}_{}})_{s}=_{a} }_{}[}^{}_{s,a}_{s,a}]\). Let\(}^{S}\) and \(}^{S}\) represent the fixed points of \(_{_{}}\) and \(_{_{}}}\) respectively. Suppose that Algorithm 3.1 returns policy \(_{k}\) and value function \(_{k}\) and \(\|}-_{0}\|_{}}}{{1-}}\). Then,_

\[\|}-_{k}\|_{},\|}-_{_{k}}\|_{}.\]

_Furthermore, the gap between the empirical and true value function is bounded by_

\[\|}-}\|_{}(\| _{_{}}}}-_{ _{}}}\|_{},\|_{_{}}}}-_{_{}}}\|_{}).\]

**Proposition 3.8** (Time Complexity).: _Let \(r_{}=_{s,s^{},a}|R(s,a,s^{})|\). Then, Algorithm 3.1 terminates in \(k=(}{(1-)} )}}{{(1-)}}\) iterations with time complexity \((s^{2}AM_{1/}(}{(1- )}))\). Furthermore, for any failure probability \((0,1)\), suppose that the CDF \(_{}}\) of the random estimate of 1-step returns \(}_{s,a}^{}}_{s,a}\) is differentiable at the point \(_{}[}_{s,a}^{}}_{s,a}]\), and set \(=^{}(_{}[}_{s,a}^{} }_{s,a}])\) and \(M=}{{}})}{{^{2} ^{2}(1-)^{2}}}\). Then with probability at least \(1-\), it holds that \(\|}-_{k}\|_{}()\), and Algorithm 3.1 runs in \(A_{1/}( }{(1-)})}}{{^{2}^{2}(1-)^{2}}}}{{ ^{2}^{2}(1-)^{2}}}\) time._

## 4 Comparison with Bayesian Credible Regions

We are now ready to answer the question: _Are Bayesian credible regions the optimal ambiguity sets for optimizing the percentile criterion?_ For this, we compare the \(\) framework with BCR Robust MDPs. First, we derive the robust form of the \(\) framework and show that in contrast to the BCR Bellman operator, the \(\) Bellman optimality operator implicitly constructs value function dependent ambiguity sets, and thus, these sets tend to be smaller (Proposition 4.1). Then, we compare the asymptotic radii of the BCR ambiguity sets and the \(\) ambiguity sets implicitly constructed by \(_{_{}}\). For any given confidence level \(\), the radius of the \(\) ambiguity sets are asymptotically smaller than that of BCR ambiguity sets (Theorem 4.3). Precisely, the ratio of the radii of \(\) ambiguity sets to BCR ambiguity sets is at least \(}{{_{S-1,1-}^{2}}}}/{{}{{ -1}}}}(1-)\), where \(_{S-1,1-}^{2}\) is the CDF inverse of \(1-\) percentile of Chi-squared distribution with degree of freedom \(S-1\) and \(^{-1}(1-)\) is the \(1-\) percentile of \((0,1)\). This implies that there exist directions in which the BCR ambiguity sets are at least \(()\) larger than \(\) ambiguity sets. Thus, we prove that \(\) framework is better suited for optimizing the percentile criterion than RMDPs with BCR ambiguity sets.

For any value function \(\), define the \(\) ambiguity set \(^{,}\) as

\[^{,}=,a}{ }^{,}_{s,a} ^{,}_{s,a}=\{_{s,a}^{S} _{s,a}^{}_{}[}_{s,a}^{}]\}.\] (7)

**Proposition 4.1** (Equivalence).: _The \(\) Bellman optimality operator \(_{_{}}\) can be expressed as_

\[ s:\;(_{_{}})_{s}= _{a}_{^{,}_{s,a}} ^{}(_{s,a}+).\]

_Furthermore, the optimal \(\) policy \(^{D}\) solves \(_{^{D}}_{^{,}}(, ),\) where \(}^{S}\) is the fixed point of the \(\) Bellman operator \(_{_{}}\), i.e., \(}=(_{_{}}})\)._

Proposition 4.1 shows that the \(\) Bellman optimality operator optimizes a unique robust MDP whose ambiguity sets are SA-rectangular and value function dependent. Notice that for any state \(s\), action \(a\) and value function \(\), the \(\) ambiguity set is a half-space \(\{_{s,a}^{S}:_{s,a}^{} _{}[}_{s,a}^{}]\}\) dependent on the value function \(\). In contrast, BCR ambiguity sets are independent of any policy or value function and are constructed such that they provide high-confidence guarantees on returns of all policies simultaneously. As a result, BCR ambiguity sets tend to be unnecessarily large.

We now compute the ratio of the asymptotic radii of BCR ambiguity sets and \(\) ambiguity sets.

**Theorem 4.2** (Asymptotic Radii of \(\) Ambiguity Sets).: _For any state \(s\) and action \(a\), let \(I(\{(_{s,a}^{*})_{i}\}_{i=1}^{S-1})\) be the Fisher information of the first \(S-1\) transition probabilities, i.e., \(\{(_{s,a}^{*})_{i}\}_{i=1}^{S-1}\).__Define \((^{*}_{s,a})}}{{=}}I(\{(^{*}_{s,a})\}_{i=1}^ {S-1})\). Suppose that the normality assumptions on the posterior distribution \(}\) in Section 2 are satisfied. Then,_

\[_{N}(^{_{}}_{s,a}-^{*} _{s,a})=_{s,a}^{}\|_{s,a}- ^{*}_{s,a}\|_{I^{}(^{*}_{s,a})^{-1}}^{-1}(1-)} -^{*}_{s,a}.\] (8)

Theorem 4.2 shows that the asymptotic form of the \(\) ambiguity set is an ellipsoid. Note that, in contrast to the finite-sample \(\) ambiguity set \(^{,}_{s,a}\) in problem (7), the asymptotic \(\) ambiguity set \(^{}_{s,a}\) is independent of the value function \(\). This is because \(_{}[}^{}_{s,a}]\) is convex in the value function \(\). Therefore, the asymptotic ambiguity set \(^{}_{s,a}\) is simply the intersection of closed half-spaces in \(^{,}_{s,a}\) computed over all value functions \(^{}\). It is also worth noting that the radius of the asymptotic \(\) ambiguity set \(^{}_{s,a}\) is constant. In contrast, the asymptotic radius of the BCR ambiguity sets grows with the number of states in at least one direction, as we show in the following proposition.

**Theorem 4.3** (Asymptotic Radius of Bayesian Credible Regions).: _For any state \(s\) and action \(a\), let \(^{_{}}_{s,a}\) represent any Bayesian credible region. Let \(<^{2}}}{{^{-1}(1-)}}}\). Suppose that the normality assumptions on the posterior distribution of \(}\) in Section 2 are satisfied. Then,_

\[ s,a:_{N}( ^{_{}}_{s,a}-^{*}_{s,a})_{N} (^{_{}}_{s,a}-^{*}_{s,a}).\] (9)

We note that Theorem 4.3 is an adaptation of Theorem 10 in  in RL which proves that there exist directions in which the Bayesian credible regions is at least \(=^{2}}}{{^{-1}(1-)}}}\) larger than \(\) ambiguity sets. Since the value of \(\) only grows with the number of states, we conclude that BCR ambiguity sets are sub-optimal for optimizing the percentile criterion. Figure 0(a) shows the growth in the ratio of radius of BCR to \(\) ambiguity sets with an increasing number of states.

## 5 Experiments

We now empirically analyze the robustness of the \(\) framework in three different domains.

_Riverswim:_ The Riverswim MDP  consists of five states and two actions. The state represents the coordinates of the swimmer in the river and action represents the direction of the swim. The task of the agent is to learn a policy that would take the swimmer to the other end of the river.

_Population Growth Model:_ The Population Growth MDP  models the population growth of pests and consists of 50 states and 5 actions. The states represent the pest population and actions represent the pest control measures. In our experiments, we use two different instantiations of the Population Growth Model: Population-Small and Population, which vary in the number of posterior samples.

_Inventory Management:_ The Inventory Management MDP  models the classical inventory management problem and consists of 30 states and 30 actions. States represent the inventory level and actions represent the inventory to be purchased. The sale price \(s\), holding cost \(c\) and purchase costs \(\) are 3.99, 0.03, and 2.219. The demand is normally distributed with mean=\(}{{4}}\) and standard deviation \(}{{6}}\).

Implementation details:For each domain in our experiments, we sample a dataset \(\) consisting of \(n\) tuples of the form \(\{s,a,r,s^{}\}\), corresponding to the state \(s\), the action taken \(a\), the reward \(r\) and the next state \(s^{}\). We construct a posterior distribution over the models using \(\), assuming Dirichlet priors over the model parameters. Using MCMC sampling, we construct two datasets \(_{1}\) and \(_{2}\) containing \(M\) and \(K\) transition probability models, respectively.

We construct \(L\) train datasets by randomly sampling \(80\%\) of the models from \(_{1}\) each time. We use \(_{2}\) as our test dataset. For any confidence level \(\), we train one RL agent per train dataset and method.

For evaluation, we consider two instances of the \(\) framework: one (denoted by _VaR_) that assumes that \(}\) is a multivariate normal, and another (denoted by _VaR_) that does not assume any structure over \(}\). We use seven baseline methods for evaluating the robustness of our framework. They are: Naive Hoeffding , Optimized Hoeffding (Opt Hoeffding) , Soft-Robust , and BCR Robust MDPs with weighted \(_{1}\) ambiguity sets (_WBCR_\(_{1}\)) , weighted \(_{}\) ambiguity sets(_WBCR_\(_{}\)) , unweighted \(_{1}\) ambiguity sets (_BCR_\(_{1}\))  and unweighted \(_{}\) ambiguity sets (_BCR_\(_{1}\)) ambiguity  sets. See Appendix C in the appendix for more details.

We report the \(95\%\) confidence interval of the robust performance (\(\)-percentile of expected returns) of the \(\) framework on the test dataset with that of other baselines for different values of \(\).

Experimental ResultsTable 1 summarizes the performance of the \(\) framework and the baselines for confidence level \(=0.05\) (Table 2 and Table 3 in the appendix summarizes the results for \(=0.15\) and \(=0.3\) respectively.). We observe that for confidence level \(=0.05\), the \(\) framework outperforms the baseline methods in terms of mean robust performance in most domains. On the other hand, for \(=0.15\), the \(\) framework outperforms baselines in the Population and Population-Small domains and has comparable performance to the Soft-Robust method in the Inventory domain. However, at \(=0.3\) we observe that the \(\) framework has lower robust performance relative to the the Soft-Robust method in Riverswim and Population domains. We conjecture that this is because the Soft-Robust method optimizes the policy to maximize the mean of expected returns and is therefore able to perform better in cases where a lower level of robustness is required. However, we note that in contrast to our method, the Soft-Robust method does not provide probabilistic guarantees against worst-case scenarios.

Furthermore, as expected, we find that in many cases, the robust performance of BCR Robust MDPs with span-optimized (weighted) ambiguity sets (_WBCR_\(_{1}\), WBCR \(_{}\)) is relatively higher than the robust performance of Robust MDPs with unweighted BCR ambiguity sets (_BCR_\(_{1}\), BCR \(_{}\)). However, we find that even Robust MDPs with span-optimized BCR ambiguity sets are generally unable to outperform the robust performance of our \(_{}\) framework.

Figure 2 compares the robust performance of the \(\) framework and the baselines on both train and test models. The trends in the robust performance of the \(\) framework and the baselines are similar on both train and test models.

## 6 Conclusion and Future Work

The main limitation of the \(\) framework is that it does not consider the correlations in the uncertainty of transition probabilities across states and actions [19; 29; 30]. However, due to the non-convex nature of the percentile-criterion [3; 40], constructing a tractable \(_{}\) Bellman operator that considers these correlations is not feasible. One plausible solution is to use a Conditional Value at Risk Bellman operator [11; 27] which is convex and lower bounds the Value at Risk measure. We leave the analysis of this approach for future work. Empirical analysis of the \(_{}\) framework in domains with continuous state-action spaces is also an interesting avenue for future work.

In conclusion, we propose a novel dynamic programming algorithm that optimizes a tight lower-bound approximation on the percentile criterion without explicitly constructing ambiguity sets. We theoretically show that our algorithm implicitly constructs tight ambiguity sets whose asymptotic radius is smaller than that of any Bayesian credible region, and therefore, computes less conservative policies with the same confidence guarantees on returns. We also derive finite-sample and asymptotic bounds on the performance loss due to our approximation. Finally, our experimental results demonstrate the efficacy of our method in several domains.

   Methods & Riverswim & Inventory & Population-Small & Population \\  VaR & **68.54 \(\) 5.08** & 457.95 \(\) 0.74 & **-3102.48 \(\) 429.7** & -4576.87 \(\) 147.3 \\ VaRN & 67.27 \(\) 0.0 & 452.78 \(\) 0.02 & -4005.53 \(\) 8.76 & **-4570.17 \(\) 38.84** \\ BCR \(l_{1}\) & 67.27 \(\) 0.0 & 369.67 \(\) 0.0 & -5614.95 \(\) 80.28 & -6013.21 \(\) 1177.94 \\ BCR \(l_{}\) & 67.27 \(\) 0.0 & 199.41 \(\) 39.02 & -7908.92 \(\) 41.6 & -9033.7 \(\) 84.28 \\ WBCR \(l_{1}\) & 67.9 \(\) 3.82 & 454.1 \(\) 4.16 & -5290.38 \(\) 1084.26 & -5408.01 \(\) 225.2 \\ WBCR \(l_{}\) & 67.27 \(\) 0.0 & 199.4 \(\) 39.02 & -7712.43 \(\) 55.96 & -8377.64 \(\) 126.24 \\ Soft-Robust & 61.79 \(\) 1.46 & **460.6 \(\) 0.0** & -3647.18 \(\) 94.62 & -6932.86 \(\) 154.16 \\ Naive Hoeffding & 51.52 \(\) 6.06 & -0.0 \(\) 0.0 & -8647.7 \(\) 59.5 & -9127.14 \(\) 140.98 \\ Opt Hoeffding & 50.76 \(\) 4.56 & -0.0 \(\) 0.0 & -8640.48 \(\) 2.34 & -9163.64 \(\) 13.62 \\   

Table 1: shows the \(95\%\) confidence interval of the robust (percentile) returns achieved by _VaR_, _VaR_, _BCR_\(_{1}\), _BCR_\(_{}\), _WBCR_\(_{1}\), _WBCR_, _Soft Robust_, _Worst RMDP_, _Naive Hoeffding_ and _Opt Hoeffding_ agents at \(=0.05\) in Riverswim, Inventory, Population-Small, and Population domain.

#### Acknowledgments

The work in the paper was supported, in part, by NSF grants 2144601 and 1815275.