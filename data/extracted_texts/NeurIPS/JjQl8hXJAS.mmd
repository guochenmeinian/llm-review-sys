# Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL

Andrew Wagenmaker

University of California, Berkeley

&Kevin Huang

University of Washington

&Liyiming Ke

University of Washington

&Kevin Jamieson

University of Washington

&Abhishek Gupta

University of Washington

Correspondance to: ajwagen@berkeley.edu

###### Abstract

In order to mitigate the sample complexity of real-world reinforcement learning, common practice is to first train a policy in a simulator where samples are cheap, and then deploy this policy in the real world, with the hope that it generalizes effectively. Such _direct sim2real_ transfer is not guaranteed to succeed, however, and in cases where it fails, it is unclear how to best utilize the simulator. In this work, we show that in many regimes, while direct sim2real transfer may fail, we can utilize the simulator to learn a set of _exploratory_ policies which enable efficient exploration in the real world. In particular, in the setting of low-rank MDPs, we show that coupling these exploratory policies with simple, practical approaches--least-squares regression oracles and naive randomized exploration--yields a polynomial sample complexity in the real world, an exponential improvement over direct sim2real transfer, or learning without access to a simulator. To the best of our knowledge, this is the first evidence that simulation transfer yields a provable gain in reinforcement learning in settings where direct sim2real transfer fails. We validate our theoretical results on several realistic robotic simulators and a real-world robotic sim2real task, demonstrating that transferring exploratory policies can yield substantial gains in practice as well.

## 1 Introduction

Over the last decade, reinforcement learning (RL) techniques have been deployed to solve a variety of real-world problems, with applications in robotics, the natural sciences, and beyond . While promising, the broad application of RL methods has been severely limited by its large sample complexity--the number of interactions with the environment required for the algorithm to learn to solve the desired task. In applications of interest, it is often the case that collecting samples is very costly, and the number of samples required by RL algorithms is prohibitively expensive.

In many domains, while collecting samples in the desired deployment environment may be very costly, we have access to a _simulator_ where the cost of samples is virtually nonexistent. As a concrete example, in robotic applications where the goal is real-world deployment, directly training in the real world typically requires an infeasibly large number of samples. However, it is often possible to obtain a simulator--derived from first principles or knowledge of the robot's actuation--which provides an approximate model of the real-world deployment environment. Given such a simulator, common practice is to first train a policy to accomplish the desired task in the simulator, and then deploy it in the real world, with the hope that the policy generalizes effectively from the simulator to the goal deployment environment. Indeed, such "sim2real" transfer has become a key piece in theapplication of RL to robotic settings, as well as many other domains of interest such as the natural sciences [12; 15], and is a promising approach towards reducing the sample complexity of RL in real-world deployment [19; 4; 18].

Effective sim2real transfer can be challenging, however, as there is often a non-trivial mismatch between the simulated and real environments. The real world is difficult to model perfectly, and some discrepancy is inevitable. As such, directly transferring the policy trained in the simulator to the real world often fails, the mismatch between sim and real causing the policy--which may perfectly solve the task in sim--to never solve the task in real. While some attempts have been made to address this--for example, utilizing domain randomization to extend the space of environments covered by simulator [60; 49], or finetuning the policy learned in sim in the real world [50; 73]--these approaches are not guaranteed to succeed. In settings where such methods fail, can we still utilize a simulator to speed up real-world RL?

In this work we take steps towards developing principled approaches to sim2real transfer that addresses this question. Our key intuition is that it is often _easier to learn to explore than to learn to solve the goal task_. While solving the goal task may require very precise actions, collecting high-quality exploratory data can require significantly less precision. For example, successfully solving a complex robotic manipulation task requires a particular sequence of motions, but obtaining a policy that will interact with the object of interest in some way, providing useful exploratory data on its behavior, would require significantly less precision.

Formally, we show that, in the setting of low-rank MDPs where there is a mismatch in the dynamics between the "sim" and "real" environments, even when this mismatch is such that direct sim2real transfer fails, under certain conditions we can still effectively transfer a set of _exploratory_ policies from sim to real. In particular, we demonstrate that access to such exploratory policies, coupled with random exploration and a least-squares regression oracle--which are insufficient for efficient learning on their own, but often still favored in practice due to their simplicity--enable provably efficient learning in real. Our results therefore demonstrate that simulators, when carefully applied, can yield a provable--exponential--gain over both naive sim2real transfer and learning without a simulator, and enable algorithms commonly used in practice to learn efficiently.

Furthermore, our results motivate a simple, easy-to-implement algorithmic principle: rather than training and transferring a policy that solves the task in the simulator, utilize the simulator to train a set of exploratory policies, and transfer these, coupled with random exploration, to generate high

Figure 1: **Left: Overview of our approach compared to standard sim2real transfer on puck pushing task. Standard sim2real transfer first trains a policy to solve the goal task in sim and then transfers this policy to real. This policy may fail to solve the task in real due to the sim2real gap, and furthermore may not provide sufficient data coverage to successfully learn a policy that does solve the goal task in real. In contrast, our approach trains a set of exploratory policies in sim which achieve high-coverage data when deployed in real, even if they are unable to solve the task 0-shot. This high-coverage data can then be used to successfully learn a policy that solves the goal task in real. Right: Quantitative results running our approach on the puck pushing task illustrated on left, compared to standard sim2real transfer. Over 6 real-world trials, our approach solves the task 6/6 times while standard sim2real transfer solves the task 0/6 times.**

quality exploratory data in real. We show experimentally--through a realistic robotic simulator and real-world sim2real transfer problem on the Franka robot platform--that this principle of transferring exploratory policies from sim to real yields a significant practical gain in sample efficiency, often enabling efficient learning in settings where naive transfer fails completely (see Figure 1).

## 2 Related Work

**Provable Transfer in RL.** Perhaps the first theoretical result on transfer in RL is the "simulation lemma", which transforms a bound on the total-variation distance between the dynamics to a bound on policy value [24; 25; 6; 22]--we argue that we can do significantly better with exploration transfer. More recent work has considered transfer in the setting of block MDPs , but requires relatively strong assumptions on the similarity between source and target MDPs, or the meta-RL setting , but only consider tabular MDPs, and assume the target MDP is covered by the training distribution. Perhaps most relevant to this work is the work of , which presents several lower bounds showing that efficient transfer in RL is not feasible in general. In relation to this work, our work can be seen as providing a set of sufficient conditions that do enable efficient transfer; the lower bounds presented in  do not hold in the low-rank MDP setting we consider. Several other works exist, but either consider different types of transfer than what we consider (e.g., observation space mismatch), or only learn a policy that has suboptimality bounded by the sim2real mismatch [37; 56; 58]. Another somewhat tangential line of work considers representation transfer in RL, where it is assumed the source and target tasks share a common representation [35; 10; 2]. We remark as well that the formal sim2real setting we consider is a special case of the MF-MDP setting of .

**Simulators and Low-Rank MDPs.** Several existing works show that there are provable benefits to training a policy in "simulation" due to the ability to reset on command [67; 33; 5; 68; 70; 42]. These works do not consider the transfer problem, however. The setting of linear and low-rank MDPs which we consider has seen a significant amount of attention over the last several years, and many provably efficient algorithms exist [21; 1; 62; 63; 43; 41]. These works typically assume access to powerful oracles which enable efficient learning; we only consider access to a simple regression oracle. Beyond the theory literature, recent work has also shown that low-rank MDPs can effectively model a variety of standard RL settings in practice .

**Sim2Real Transfer in Practice.** The sim2real literature is vast and we only highlight particularly relevant works here; see  for a full survey. To mitigate the inconsistency between the simulator and real world's physical parameters and modeling, domain randomization creates a variety of simulated environments with randomized properties to develop a robust policy [60; 49; 44; 8; 39]. Domain adaptation instead constructs encoding of deployment conditions (e.g., physical condition or past histories) and adapts to the deployment environment by matching the encoding [29; 9; 66; 55; 38; 40]. In contrast, our work assumes a fundamental sim2real mismatch where we do not expect the real system to match the simulator for any parameter settings. A related line of work shows that policies trained with robust exploration strategies generalize better to disturbed or unseen environments [13; 20]. Our work is complimentary to this work in that our goal is not to transfer a policy that solves the task in new environment, but rather explores the environment.

## 3 Preliminaries

We let \(_{}\) denote the set of distributions over set \(\), \([H]:=\{1,2,,H\}\), and \(\|P-Q\|_{}\) the total-variation distance between distributions \(P\) and \(Q\).

Markov Decision Processes.We consider the setting of episodic Markov Decision Processes (MDPs). An MDP is denoted by a tuple \(=(,,\{P_{h}\}_{h=1}^{H},\{r_{h}\}_{h=1}^{H},s _{1},H)\), where \(\) denotes the set of states, \(\) the set of actions, \(P_{h}:_{}\) the transition function, \(r_{h}:\) the reward (which we assume is deterministic and known), \(s_{1}\) the initial state, and \(H\) the horizon. We assume \(\) is finite and denote \(A:=||\). Interaction with an MDP starts from state \(s_{1}\), the agent takes some action \(a_{1}\), transitions to state \(s_{2} P_{1}( s_{1},a_{1})\), and receives reward \(r_{1}(s_{1},a_{1})\). This process continues for \(H\) steps at which points the episode terminates, and the process resets.

The goal of the learner is to find a policy \(=\{_{h}\}_{h=1}^{H}\), \(_{h}:_{}\), that achieves maximum reward. We can quantify the reward received by some policy \(\) in terms of the value and \(Q\)-valuefunctions. The \(Q\)-value function is defined as \(Q_{h}^{}(s,a):=^{}[_{a^{}=h^{}}^{H}r_{h^{}} (s_{h^{}},a_{h^{}}) s_{h}=s,a_{h}=a]\), and value function is defined in terms of the \(Q\)-value function as \(V_{h}^{}(s):=_{a_{h}(|s)}[Q_{h}^{}(s,a)]\). The value of policy \(\), its expected reward, is denoted by \(V_{0}^{}:=V_{1}^{}(s_{1})\), and the value of the optimal policy, the maximum achievable reward, by \(V_{0}^{}:=_{}V_{0}^{}\).

In this work we are interested in the setting where we wish to solve some task in the "real" environment, represented as an MDP, and we have access to a simulator which approximates the real environment in some sense. We denote the real MDP as \(^{}\), and the simulator as \(^{}\). We assume that \(^{}\) and \(^{}\) have the same state and actions spaces, reward function, and initial state, but different transition functions, \(P^{}\) and \(P^{}\). We denote value functions in \(^{}\) and \(^{}\) as \(V_{h}^{,}(s)\) and \(V_{h}^{,}(s)\), respectively. We make the following assumption.

**Assumption 1**.: _For all \((s,a,h)[H]\) and some \(_{}>0\), we have:_

\[\|P_{h}^{}( s,a)-P_{h}^{}( s,a)\|_{ }_{}.\]

We do not assume that the value of \(_{}\) is known, simply that there exists some such \(_{}\).

Function Approximation.In order to enable efficient learning, some structure on the MDPs of interest is required. We will assume that \(^{}\) and \(^{}\) are low-rank MDPs, as defined below.

**Definition 3.1** (Low-Rank MDP).: We say an MDP is a _low-rank_ MDP with dimension \(d\) if there exists some featurization \(:^{d}\) and measure \(:[H]^{d}\) such that:

\[P_{h}( s,a)=(s,a),_{h}(),  s,a,h.\]

We assume that \(\|(s,a)\|_{2} 1\) for all \((s,a)\), and for all \(h\), \(\|_{h}()\|_{2}=\|_{s}|_ {h}(s)|\|_{2}\).

Formally, we make the following assumption on the structure of \(^{}\) and \(^{}\).

**Assumption 2**.: _Both \(^{}\) and \(^{}\) satisfy Definition 3.1 with feature maps and measures \((^{},^{})\) and \((^{},^{})\), respectively. Furthermore, \(^{}\) is known, but all of \(^{},^{}\), and \(^{}\) are unknown._

In the literature, MDPs satisfying Definition 3.1 but where \(\) is known are typically referred to as "linear" MDPs, while MDPs satisfying Definition 3.1 but with \(\) unknown are typically referred to as "low-rank" MDPs. Given this terminology, we have that \(^{}\) is a linear MDP2, while \(^{}\) is a low-rank MDP. We assume the following reachability condition on \(^{}\).

**Assumption 3**.: _There \(_{}^{}>0\) with \(_{h}_{}_{}(^{^{},}[ ^{}(s_{h},a_{h})^{}(s_{h},a_{h})^{ }])_{}^{}\)._

Assumption 3 posits that each direction in the feature space in our simulator can be activated by some policy, and can be thought of as a measure of how easily each direction can be reached. Similar assumptions have appeared before in the literature on linear and low-rank MDPs . Note that we only require this reachability assumption in \(^{}\). We also assume we are given access to function classes \(_{h}:[0,H]\) and let \(:=_{1}_{2}_{H}\). Since no reward is collected in the \((H+1)\)th step we take \(f_{H+1}=0\). For any \(f:\), we let \(_{h}^{f}(s):=_{a}f_{h}(s,a)\). We define the _Bellman operator_ on some function \(f_{h+1}:\) as:

\[f_{h+1}(s,a):=r_{h}(s,a)+_{s^{} P_{h}(|s,a)}[ _{a^{}}f_{h+1}(s^{},a^{})].\]

We make the following standard assumption on \(\).

**Assumption 4** (Bellman Completeness).: _For all \(f_{h+1}_{h+1}\), we have \(^{}f_{h+1},^{}f_{h+1}_{h}\), where \(^{}\) and \(^{}\) denote the Bellman operators on \(^{}\) and \(^{}\), respectively._

PAC Reinforcement Learning.Our goal is to find a policy \(\) that achieves maximum reward in \(^{}\). Formally, we consider the PAC (Probably-Approximately-Correct) RL setting.

**Definition 3.2** (PAC Reinforcement Learning).: Given some \(>0\) and \(>0\), with probability at least \(1-\) identify some policy \(\) such that: \(V_{0}^{,} V_{0}^{,}-\).

We will be particularly interested in solving the PAC RL problem with the aid of a simulator, using the minimum number of samples from \(^{}\) possible, as we will formalize in the following. As we will see, while it is straightforward to achieve this objective using \(^{}\) if \(=(_{})\), naive transfer methods can fail to achieve this completely if \(_{}\). As such, our primary focus will be on developing efficient \(\)2real methods in this regime.

Theoretical Results

In this section we provide our main theoretical results. We first present two negative results: in Section 4.1 showing that "naive exploration"--utilizing only a least-squares regression oracle and random exploration approaches such as \(\)-greedy3--is provably inefficient, and in Section 4.2 showing that directly transferring the optimal policy from \(^{}\) to \(^{}\) is unable to efficiently obtain a policy with suboptimality better than \((_{})\) in real. Then in Section 4.3 we present our main positive result, showing that by utilizing the same oracles as in Sections 4.1 and 4.2--a least-squares regression oracle, simulator access, and the ability to take actions randomly--we _can_ efficiently learn an \(\)-optimal policy for \(_{}\) in \(^{}\) by carefully utilizing the simulator to learn exploration policies.

### Naive Exploration is Provably Inefficient

While a variety of works have developed provably efficient methods for solving PAC RL in low-rank MDPs , these works typically either rely on complex computation oracles or carefully directed exploration strategies which are rarely utilized in practice. In contrast, RL methods utilized in practice typically rely on "simple" computation oracles and exploration strategies. Before considering the sim2real setting, we first show that such "simple" strategies are insufficient for efficient PAC RL. To instantiate such strategies, we consider a least-squares regression oracle, often available in practice.

**Oracle 4.1** (Least-Squares Regression Oracle).: We assume access to a least-squares regression oracle such that, for any \(h\) and dataset \(=\{(s^{t},a^{t},y^{t})\}_{t=1}^{T}\), we can compute \(_{f_{h}}_{t=1}^{T}(f(s^{t},a^{t})-y^{t})^{2}\).

We couple this oracle with "naive exploration", which here we use to refer to any method that explores by randomly perturbing the action recommended by the current estimate of the optimal policy. While a variety of instantiations of naive exploration exist (see e.g. ), we consider a particularly common formulation, \(\)-greedy exploration.

**Protocol 4.1** (\(\)-Greedy Exploration).: Given access to a regression oracle, any \(\), and time horizon \(T\), consider the following protocol:

1. Interact with \(^{}\) for \(T\) episodes. At every step of episode \(t+1\), play \(_{h}^{f^{t}}(s)\) with probability \(1-\), and \(a()\) otherwise, where: \[f_{h}^{t}=_{f_{h}}_{t^{}=1}^{t}(f(s_{h}^{t^{ }},a_{h}^{t^{}})-r_{h}^{t^{}}-_{a^{}}f_{h+1}^{t}( s_{h+1}^{t^{}},a^{}))^{2}.\]
2. Using collected data in any way desired, propose a policy \(\).

Protocol 4.1 forms the backbone of many algorithms used in practice. Despite its common application, as existing work  and the following result show, it is provably inefficient.

**Proposition 1**.: _For any \(H>1\), \(\), and \(c 1/6\), there exist some \(^{,1}\) and \(^{,2}\) such that both \(^{,1}\) and \(^{,2}\) satisfy Assumptions 2 and 4, and unless \(T(2^{H/2})\), when running Protocol 4.1 we have:_

\[_{^{}\{^{,1},^{,2}\}}^{^{}}[V_{0}^{^{},}-V_{0}^{^{},}] c/32.\]

Proposition 1 shows that, in a minimax sense, \(\)-greedy exploration is insufficient for provably efficient reinforcement learning: on one of \(^{,1}\) and \(^{,2}\), \(\)-greedy exploration will only be able to find a policy that is suboptimal by a constant factor, unless we take an exponentially large number of samples. While we focus on \(\)-greedy exploration in Proposition 1, this result extends to other types of naive exploration, for example, those given in . See Section 5.2 for further discussion of the construction for Proposition 1.

### Understanding the Limits of Direct sim2real Transfer

Proposition 1 shows that in general utilizing a least-squares regression oracle with \(\)-greedy exploration is insufficient for provably efficient RL. Can this be made efficient with access to a simulator \(^{}\)? In practice, standard \(\) methodology typically trains a policy to accomplish the goal task in \(^{}\), and then transfers this policy to \(^{}\). We refer to this methodology as _direct_\(\)_transfer_. The following canonical result, usually referred to as the "simulation lemma" [24; 25; 6; 22], provides a sufficient guarantee for direct \(\) transfer to succeed under Assumption 1.

**Proposition 2** (Simulation Lemma).: _Let \(^{,}\) denote an optimal policy in \(^{}\). Then under Assumption 1 we have \(V_{0}^{,^{,}} V_{0}^{, }-2H^{2}_{}\)._

Proposition 2 shows that, as long as \( 2H^{2}_{}\), direct \(\) transfer succeeds in obtaining an \(\)-optimal policy in \(^{}\). While this justifies direct \(\) transfer in settings where \(^{}\) and \(^{}\) are sufficiently close, we next show that given access only to \(^{,}\) and a least-squares regression oracle--even when coupled with random exploration--we cannot hope to efficiently obtain a policy with suboptimality less than \((_{})\) on \(^{}\) using naive exploration. To formalize this, we consider the following interaction protocol.

**Protocol 4.2** (Direct \(\) Transfer with Naive Exploration).: Given access to \(^{,}\), an optimal policy in \(^{}\), any \(\), and time horizon \(T\), consider the following protocol:

1. [noitemsep,topsep=0pt]
2. Interact with \(^{}\) for \(T\) episodes, and at each step \(h\) and state \(s\) play \(^{,}_{h}( s)\) with probability \(1-\), and \(a()\) with probability \(\).
3. Using collected data in any way desired, propose a policy \(\).

Protocol 4.2 is a standard instantiation of direct \(\) transfer commonly found in the literature, and couples playing the optimal policy from \(^{}\) with naive exploration. We have the following.

**Proposition 3**.: _With the same choice of \(^{,1}\) and \(^{,2}\) as in Proposition 1, there exists some \(^{}\) such that both \(^{,1}\) and \(^{,2}\) satisfy Assumption 1 with \(^{}\) for \(_{} c\), Assumptions 2 to 4 hold, and unless \(T(2^{H})\) when running Protocol 4.2, we have:_

\[_{^{}\{^{,1},^{,2}\}}^{^{}}[V_{0}^{^{},}-V_{0}^{^{},}] _{}/32.\]

Proposition 3 shows that there exists a setting where there are two possible \(^{}\) satisfying Assumption 1 with \(^{}\), and where, using direct policy transfer, unless we interact with \(^{}\) for exponentially many episodes (in \(H\)), we cannot determine a better than \((_{})\)-optimal policy for the worst-case \(^{}\). Together, Propositions 2 and 3 show that, while we can utilize direct \(\) transfer to learn a policy that is \((_{})\)-optimal in \(^{}\), if our goal is to learn an \(\)-optimal policy for \(_{}\), direct \(\) transfer is unable to efficiently achieve this.

### Efficient \(\) Transfer via Exploration Policy Transfer

Does there exist _some_ way to utilize \(^{}\) and a least-squares regression oracle to enable efficient learning in \(^{}\), even when \(_{}\)? Our key insight is that, rather than transferring the policy that optimally solves the task in \(^{}\), we should instead transfer policies that _explore_ effectively in \(^{}\). While learning to solve a task may require very precise actions, we can often obtain sufficiently rich data with relatively imprecise actions--it is easier to learn to explore than learn to solve a task. In such settings, directly transferring a policy to solve the task will likely fail due to imprecision in the simulator, but it may be possible to still transfer a policy that generates exploratory data. To formalize this, we consider the following access model to \(^{}\).

**Oracle 4.2** (\(^{}\) Access).: We may interact with \(^{}\) by either:

1. [noitemsep,topsep=0pt]
2. **(Trajectory Sampling)** For any policy \(\), sampling a trajectory \(\{(s_{h},a_{h},r_{h},s_{h+1})\}_{h=1}^{H}\) generated by playing \(\) on \(^{}\).
3. **(Policy Optimization)** For any reward \(\), computing a policy \(^{}()\) maximizing \(\) on \(^{}\).

While access to such a policy optimization oracle is unrealistic in \(^{}\), where we want to minimize the number of samples collected, given cheap access to samples in \(^{}\), such an oracle can often be (approximately) implemented in practice4. Note that under Oracle 4.2 we only assume _black-box_access to our simulator--rather than allowing the behavior of the simulator to be queried at arbitrary states, we are simply allowed to roll out policies on \(^{}\), and compute optimal policies. Given Oracle 4.2, as well as our least-squares regression oracle, Oracle 4.1, we propose the following algorithm.

```
1:input: budget \(T\), confidence \(\), simulator \(^{}\)// Learn policies \(^{h}_{}\) which cover features space in \(^{}\)
2:\(^{h}_{}(^{}, ,_{}}{H},h)\) (Algorithm 5) for all \(h[H]\)
3:\(^{h}_{}\{_{}:_{}_{}h\), then plays actions randomly, \(_{}^{h}_{}\}\)// Explore in \(^{}\) via \(^{}_{}\)
4: Play \(_{}(\{(^{h}_{})\}_{h=1}^{H})\) for \(T/2\) episodes in \(^{}\), add data to \(\)// Estimate optimal policy on collected data
5:for\(h=H,H-1,,1\)do
6:\(_{h}_{f F}_{(s,a,r,s^{}) }(f_{h}(s,a)-r-_{a^{}}_{h+1}(s^{},a^{ }))^{2}\)
7: Compute \(^{,*}\) via Oracle 4.2
8: Play \(^{,*}\) for \(T/4\) episodes in real, compute average return \(^{,^{,*}}_{0}\)
9: Play \(^{}\) for \(T/4\) episodes in real, compute average return \(^{,^{}}_{0}\)
10:return\(_{\{^{},^{,*} \}}^{,}_{0}\) ```

**Algorithm 1**\(\)2real Exploration Policy Transfer

Algorithm 1 first calls a subroutine LearnExpPolicies, which learns a set of policies that provide rich data coverage on \(^{}\)--precisely, LearnExpPolicies returns policies \(\{^{h}_{}\}_{h[H]}\) which induce covariates with lower-bounded minimum eigenvalue on \(^{}\) and relies only on Oracle 4.2 (as well as knowledge of the featurization of \(^{}\), \(^{}\)) to find such policies. Algorithm 1 then plays these exploration policies in \(^{}\), coupled with random exploration, and applies the regression oracle to the data they collect. Finally, it estimates the value of the policy learned by the regression oracle and \(^{,*}\), and returns whichever is best. We have the following.

**Theorem 1**.: _If Assumptions 1 to 4 hold and_

\[_{}}_{}}{644HA^ {3}},\] (4.1)

_then as long as_

\[T cH^{16}}{^{8}}|}{ },\]

_with probability at least \(1-\), Algorithm 1 returns a policy \(\) such that \(V^{,*}_{0}-V^{,}_{0}\), and Oracles 4.1 and 4.2 are invoked at most \((d,H,^{-1},)\) times._

Theorem 1 shows that, as long as \(_{}\) satisfies (4.1), utilizing a simulator and least-squares regression oracle, Oracles 4.1 and 4.2, allows for efficient learning in \(^{}\), achieving a complexity scaling polynomially in problem parameters. This yields an _exponential improvement_ over learning without a simulator using naive exploration or direct \(\)2real transfer--which Propositions 1 and 3 show have complexity scaling exponentially in the horizon--despite utilizing the same practical computation oracles. To the best of our knowledge, this result provides the first theoretical evidence that \(\)2real transfer can yield provable gains in RL beyond trivial settings where direct transfer succeeds.

Note that the condition in (4.1) is independent of \(\)--unlike direct \(\)2real transfer, which requires \(=(_{})\), we simply must assume \(_{}\) is small enough that (4.1) holds, and Theorem 1 shows that we can efficiently learn an \(\)-optimal policy in \(^{}\) for any \(>0\). In Appendix B.4, we also present an extended version of Theorem 1, Theorem 3, which utilizes data from \(^{}\) to reduce the dependence on \(||\). In particular, instead of scaling with \(||\), it only scales with the log-cardinality of functions that are (approximately) Bellman-consistent on \(^{}\). To illustrate the effectiveness of Theorem 1, we return to the instance of Propositions 1 and 3, where naive exploration and direct \(\)2real transfer fails. We have the following.

**Proposition 4**.: _In the setting of Propositions 1 and 3 and assuming that \(_{}\), running Algorithm 1 will require \((H,^{-1})\) samples from \(^{}\) in order to identify an \(\)-optimal policy in \(^{}\) with probability at least \(1-\), for any \(>0\)._Note that the condition required by Proposition 4 is simply that \(_{} 1/H\)--as long as our simulator satisfies this condition, we can efficiently transfer exploration policies to learn an \(\)-optimal policy, for any \(>0\), while naive methods would be limited to only obtaining an \((1/H)\)-optimal policy.

**Remark 4.1** (Necessity of Random Exploration).: Algorithm 1 achieves efficient exploration in \(^{}\) by learning policies \(_{}^{h}\) in \(^{}\) that span the feature space of \(^{}\) (Line 2), and then playing these policies in \(^{}\), coupled with random exploration (Line 4). This use of random exploration is critical to obtaining Theorem 1. As we show in Proposition 5, if we omit the random exploration, Assumption 1 is not sufficient to guarantee \(_{}^{h}\) explores effectively in \(^{}\), even when (4.1) holds.

**Remark 4.2** (Computational Efficiency).: Algorithm 1, as well as its main subroutine LearnExpPolicies, relies only on calls to Oracle 4.1 and Oracle 4.2. Thus, assuming we can efficiently implement these oracles, which is often the case in problem settings of interest, Algorithm 1 can be run in a computationally efficient manner.

## 5 Practical Algorithm and Experiments

We next validate the effectiveness of our proposal in practice: can a set of diverse exploration policies obtained from simulation improve the efficiency of real-world reinforcement learning? We start by showing that this holds for a simple, didactic, tabular environment in Section 5.2. From here, we consider several more realistic task domains: simulators inspired by real-world robotic manipulation tasks (sim2sim transfer, Section 5.3); and an actual real-world sim2real experiment on a Franka robotic platform (sim2real transfer, Section 5.4). Further details on all experiments, including additional baselines, can be found in Appendix E. Before stating our experimental results, we first provide a practical instantiation of Algorithm 1 that we can apply with real robotic systems and neural network function approximators.

### Practical Instantiation of Exploration Policy Transfer

The key idea behind Algorithm 1 is quite simple: learn a set of exploratory policies in \(^{}\)--policies which provide rich data coverage in \(^{}\)--and transfer these policies to \(^{}\), coupled with random exploration, using the collected data to determine a near-optimal policy for \(^{}\). Algorithm 1 provides a particular instantiation of this principle, learning exploratory policies in \(^{}\) via the LearnExpPolicies subroutine, which aims to cover the feature space of \(^{}\), and utilizing a least-squares regression oracle to compute an optimal policy given the data collected in \(^{}\). In practice, however, other instantiations of this principle are possible by replacing LearnExpPolicies with any procedure which generates exploratory policies in \(^{}\), and replacing the regression oracle with any RL algorithm able to learn from off-policy data. We consider a general meta-algorithm instantiating this in Algorithm 2.

```
1:Input: Simulator \(^{}\), real environment \(^{}\), simulator budget \(T_{}\), real budget \(T\), algorithm to generate exploratory policies in sim \(_{}\), algorithm to solve policy optimization in real \(_{}\)// Learn exploratory policies in \(^{}\)
2:Run \(_{}\) for \(T_{}\) steps in \(^{}\) to generate set of exploratory policies \(_{}\)// Deploy exploratory policies in \(^{}\)
3:for\(t=1,2,,T/2\)do
4:Draw \(_{}(_{})\), play in \(^{}\) for one episode, add data to replay buffer of \(_{}\)
5:Run \(_{}\) for one episode // optional if\(_{}\)learns fully offline ```

**Algorithm 2** Practical sim2real Exploration Policy Transfer Meta-Algorithm

In practice, \(_{}\) and \(_{}\) can be instantiated with a variety of algorithms. For example, we might take \(_{}\) to be an RND  or bootstrapped Q-learning-style  algorithm, or any unsupervised RL procedure , and \(_{}\) to be an off-policy policy optimization algorithm such as soft actor-critic (SAC)  or implicit \(Q\)-learning (IQL) .

For the following experiments, we instantiate Algorithm 2 by setting \(_{}\) to an algorithm inspired by recent work on inducing diverse behaviors in RL , and \(_{}\) to SAC. In particular, \(_{}\) simultaneously trains an ensemble of policies \(_{}=\{_{}^{i}\}_{i=1}^{n}\) and a discriminator \(d_{}:[n]\), where \(d_{}\) is trained to discriminate between the behaviors of each policy \(_{}^{i}\), and is optimized on a weighting of the true task reward and the exploration reward induced by the discriminator, \(r_{e}(s,i):=(s,i))}{_{j[n]}(d_{}(s,j))}\). As shown in existing work [14; 30], this simple training objective effectively induces diverse behavior with temporally correlated exploration while remaining within the vicinity of the optimal policy, using standard optimization techniques. Note that the particular choice of algorithm is less critical here than abiding by the recipes laid out in the meta-algorithm (Algorithm 2). The particular instantiation that we run for our experiments is detailed in Algorithm 6, along with further details in Appendix E.2.

### Didactic Combination Lock Experiment

We first consider a variant of the construction used to prove Propositions 1 and 3, itself a variant of the classic combination lock instance. We illustrate this instance in Figure 2. Unless noted, all transitions occur with probability 1, and rewards are 0. Here, in \(^{}\) the optimal policy, \(^{,*}\), plays action \(a_{2}\) for all steps \(h<H-1\), while in \(^{}\), the optimal policy plays action \(a_{1}\) at every step. Which policy is optimal is determined by the outgoing transition from \(s_{1}\) at the \((H-1)\)th step and, as such, to identify the optimal policy, any algorithm must reach \(s_{1}\) at the \((H-1)\)th step. As \(s_{1}\) will only be reached at step \(H-1\) by playing \(a_{1}\) for \(H-1\) consecutive times, any algorithm relying on naive exploration will take exponentially long to identify the optimal policy. Furthermore, playing \(^{,*}\) coupled with random exploration will similarly take an exponential number of episodes, since \(^{,*}\) always plays \(a_{2}\). As such, both direct sim2real policy transfer as well as \(Q\)-learning with naive exploration (Protocol 4.1) will fail to find the optimal policy in \(^{}\). However, if we transfer exploratory policies from \(^{}\), since \(^{}\) and \(^{}\) behave identically up to step \(H-1\), these policies can efficiently traverse \(^{}\), reach \(s_{1}\) at step \(H-1\), and identify the optimal policy. We compare our approach of exploration policy transfer to these baselines methods and illustrate the performance of each in Figure 2. As this is a simple tabular instance, we implement Algorithm 1 directly here. As Figure 2 shows, the intuition described above leads to real gains in practice--exploration policy transfer quickly identifies the optimal policy, while more naive approach fail completely over the time horizon we considered.

### Realistic Robotics \(\) Experiment

To test the ability of our proposed method to scale to more complex problems, we next experiment on a \(\) transfer setting with a realistic robotic simulator. We consider TychoEnv, a simulator of the 7DOF Tycho robotics platform introduced by , and shown in Figure 3. We test \(\) transfer on a reaching task where the goal is to touch a small ball hanging in the air with the tip of the chopstick end effector. The agent perceives the ball and its own end effector pose and outputs a delta in its desired end effector pose as a command. We set \(^{}\) and \(^{}\) to be two instances of TychoEnv with slightly different parameters to model real-world \(\) transfer. Precisely, we change the action bounds and control frequency from \(^{}\) to \(^{}\).

We aim to compare our approach of exploration policy transfer with direct \(\) policy transfer. To this end, we first train a policy in \(^{}\) that solves the task in \(^{}\), \(^{,*}\), and then utilize this policy in place of \(_{}\) in Algorithm 2. We instantiate our approach of exploration policy transfer as outlined above. Our aim in this experiment is to illustrate how the quality of the data provided by direct policy transfer vs. exploration policy transfer affects learning. As such, for both approaches we simply initialize our SAC agent in \(^{}\), \(_{}\), from scratch, and set the reward in \(^{}\) to be sparse: the agent only receives a non-zero reward if it successfully touches the ball. For each approach, we repeat the process of training in \(^{}\) four times, and for each of these run them for two trials in \(^{}\).

Figure 3: TychoEnv Reach Task Setup

Figure 2: **Left: Illustration ofCombination Lock Example. Right: Results on Combination Lock.**

We illustrate our results in Figure 4. As this figure illustrates, direct policy transfer fails to learn completely, while exploration policy transfer successfully solves the task. Investigating the behavior of each method, we find that the policies transferred via exploration policy transfer, while failing to solve the task with perfect accuracy, when coupled with naive exploration are able to successfully make contact with the ball on occasion. This provides sufficiently rich data for SAC to ultimately learn to solve the task. In contrast, direct policy transfer fails to collect any reward when run in \(^{}\), and, given the sparse reward nature of the task, SAC is unable to locate any reward and learn.We include an additional sim2sim experiment on the Franka Emika Panda Robot Arm in Appendix E.4.

### Real-World Robotic sim2real Experiment

Finally, we demonstrate our algorithm for actual sim2real policy transfer for a manipulation task on a real-world Franka Emika Panda robot arm  with a parallel gripper. Our task is to push a 75mm diameter cylindrical "puck" from the center to the edge of the surface, as shown in Figure 1, with the arm initialized at random locations. The observed state \(s=[_{},_{}]^{4}\) consists of the planar Cartesian coordinate of the end effector \(_{}\) along with the center of mass of the puck \(_{}\). Our policy outputs planar end effector position deltas \(a=_{}^{2}\), evaluated at 8 Hz, which are passed into a lower-level joint position PID controller. We use an Intel Realsense D435 depth camera to track the location of the puck. Our reward function is a sum of a success indicator (indicating when the puck has been pushed to the edge of the surface) and terms which give negative reward if the distance from the end effector to the puck, or puck to the goal, are too large (see (E.1)); in particular, a reward greater than 0 indicates success.

We run the instantiation of Algorithm 2 outlined above. In particular, we train an ensemble of \(n=15\) exploration policies, training for 20 million steps in \(^{}\). In addition, we train a policy that solves the task in \(^{}\), \(^{,}\). We use a custom simulator of the arm, where during training the friction of the table is randomized and noise is added to the observations.

We observe a substantial sim2real gap between our simulator and the real robot, with policies trained in simulation failing to complete the pushing task zero shot in real, even when trained with domain randomization. We compare direct sim2real policy transfer against our method of transferring exploration policies. For direct policy transfer, we simply run SAC to finetune \(^{,}\) in the real world, using the current policy to collect data. For exploration policy transfer, we instead utilize \(_{}\), our ensemble of exploration policies, to collect data in the real world. We run this in tandem with an SAC agent, feeding the data from the exploration policies into the SAC agent's replay buffer. Unlike in Section 5.3, rather than initializing the SAC policy from scratch, we set the initial policy as \(^{,}\), and fine-tune from this on the data collected from playing \(_{}\). See Appendix E.5 for additional details.

Our results are shown on the right side of Figure 1. Statistics are computed over 6 runs for each method. Direct policy transfer with finetuning is unable to solve the task in real in each of the 6 runs, and converges to a suboptimal solution. However, our method is able to solve the task successfully each time and achieve a substantially higher reward.

## 6 Discussion

In this work, we have demonstrated that simulators can make naive exploration efficient even in settings where direct sim2real transfer fails, if they are used to train a set of exploration policies. We highlight several limitations of this work, which we believe are interesting future research questions:

* Our focus is purely on dynamics shift--where the dynamics of sim and real differ, but the environments are otherwise the same. While dynamics shift is common in many scenarios, other types of shift can exist as well, for example perceptual shift. How can we best handle these types of shift?
* How can we utilize a simulator in sim2real transfer if we can reset it arbitrarily, rather than just allowing for black-box access? Does the ability to reset allow us to improve sample efficiency further?
* Is the reachability condition, Assumption 3, necessary for successful exploration transfer?

Figure 4: Results on sim2sim Transfer in TychoEnv Simulator