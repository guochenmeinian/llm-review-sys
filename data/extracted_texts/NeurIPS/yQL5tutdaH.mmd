# Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models

Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models

Hongliang Wei, Xingtao Wang1, Xianqi Zhang, Xiaopeng Fan, Debin Zhao

Harbin Institute of Technology

Harbin, China

Corresponding author.

###### Abstract

Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations. Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions. However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths. In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths. A valuable finding is that instructions indirectly affect hallucinations through the length of image descriptions. The longer the image description, the higher the object hallucination degree. Accordingly, we fit an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods.

## 1 Introduction

Drawing inspiration from the remarkable language capabilities exhibited by large language models (LLMs) , large vision-language models (LVLMs)  have been well-developed, achieving significant advancements in complex multimodal tasks. However, the practical application of LVLMs is heavily hindered by hallucination phenomena , which refer to situations where objects in image descriptions generated by LVLMs are inconsistent with the provided visual content. Considerable efforts have been dedicated to both evaluation  and mitigation  of hallucination phenomena, leading to notable advancements.

A significant challenge in object hallucination evaluation arises from the effect of instructions on object hallucinations . Overcoming this challenge, existing object hallucination evaluation methods typically adopt an average-based framework, which averages the results obtained from a set of instructions. However, as shown in Figure 1, this framework fails to provide consistent evaluation across instruction sets that generate image descriptions of significantly varying lengths. Specifically,while evaluation results of LVLMs remain consistent across certain instruction sets (e.g., set 1 and set 2), inconsistencies arise when comparing instruction sets with significantly different average image description lengths (e.g., set 2 and set 3).

In this paper, we present the first systematic investigation into the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by the length of image descriptions. Technically, we evaluate lengths and object hallucination degrees (measured by CHAIR scores) of the image descriptions generated by LVLMs under different instructions (see Section 3.1 for more details). The experimental results are shown in Figures 2 & 3, from which we can observe that the degree of object hallucination is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on description lengths. The longer the image description, the higher the object hallucination degree, and there is a clear linear relation between them. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. Unfortunately, the average-based framework can only select instructions, without the ability to directly control the length of image descriptions.

Motivated by the findings, we propose a fine-grained evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucinations at any given image description length within a large range. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of image description length, ensuring stable evaluations for the same LVLM across different instruction sets and fair comparisons among different LVLMs. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is influenced by the image description length, achieving a more comprehensive evaluation. Experiment results on 12 representative LVLMs show that LeHaCE can evaluate object hallucinations of LVLMs in a more stable, fair, and comprehensive way.

The main contributions of this paper are summarized as follows:

* We conduct the first systematic investigation into the effect of instructions on object hallucinations in LVLMs and find that the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths.
* We propose an object hallucination evaluation framework called LeHaCE, which fits an informative length-hallucination curve to evaluate object hallucination at a uniform image description length, realizing a more stable and fair evaluation.
* We employ the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation.

Figure 1: The evaluation results of LVLMs on four instruction sets using the CHAIR with the average-based framework. Length refers to the average length of generated image descriptions. Each instruction set consists of six distinct instructions, and there is no overlap between instructions in different sets. All instructions prompt LVLMs to describe the image.

Related work

### Large Vision-Language Models

Inspired by the success of LLMs in NLP [1; 2; 3], researchers have extended LLMs to multimodal tasks [15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28], proposing numerous LVLMs and achieving new advancements [7; 14; 29; 30; 31; 32; 33; 34; 35]. These LVLMs align the multi-modal encoders with LLM through multitask fine-tuning and instruction fine-tuning on multi-modal datasets, enabling LLM to acquire multi-modal perception and instruction-following capabilities. Specifically, to integrate multimodal features, Flamingo  proposes a cross-attention structure to achieve arbitrary interleaved multi-modal feature fusion. BLIP-2  introduces Q-Former to bridge the visual backbone model and LLM. mPLUG-Owl2  introduces a modality adaptive module to facilitate the fusion between different modules. To enhance generalization and improve instruction-following capabilities, some methods [4; 5; 6; 12; 37] propose multi-task fine-tuning and instruction fine-tuning for LVLMs. Among them, LRV-instruction , MiniGPT-4 , LLaVA  and SViT  employ ChatGPT to augment instruction data. To mitigate the risk of catastrophic forgetting of language knowledge during the training process, mPLUG-Owl  and LLaVA-1.5  perform joint training on pure language and visual-language instructional data. More recently, mPLUG-DocOwl , InternLM-XComposer , Kosmos-2 , Shikra , Cantor , BuboGPT , and Qwen-VL  further enhance the capabilities of LVLMs in optical character recognition, document understanding, multi-modal interleaved composition and visual grounding.

### Hallucination in LVLMs

Works on the hallucination in LVLMs focus on two aspects: evaluation and mitigation. For the hallucination evaluation, POPE  designs a polling-based query method to avoid the influence of instructions on hallucination evaluation. By presenting LVLMs with brief "yes" or "no" questions regarding the target of detection, the evaluation of hallucination is transformed into a simple binary classification task. NOPE  designs a novel benchmark to evaluate the performance of LVLMs in recognizing the non-existence of objects in visual questions. AMBER  designs a multi-dimensional LVLMs hallucination evaluation benchmark without LLMs, targeting existence, attribute, and relation hallucination. For the hallucination mitigation, LRV-Instruction  creates a balanced set of positive and negative instructions to perform robust visual instruction adjustment for LVLMs. VIGC  employs an iterative approach to generate detailed and accurate answers gradually. Woodpecker  proposes a post-processing method that utilizes expert models to locate and correct hallucinations from generated text.

While existing methods  observe that the hallucination degree of LVLMs is unstable across different instructions, this phenomenon has not been thoroughly investigated to date. This work presents the first comprehensive study investigating the influence of instructions on the hallucination rate of LVLMs. Building upon our findings, we propose LeHaCE framework, which can evaluate hallucination of LVLMs in a more stable and comprehensive manner. Contrary to polling-based query methods, LeHaCE can directly evaluate the hallucination rate of image descriptions generated by LVLMs, which is more in line with the practical application scenarios of LVLMs.

## 3 Hallucination of LVLMs Under Different Instructions

This section provides the investigation into the effect of instructions on hallucinations, with a specific focus on the role played by the length of image descriptions. The experimental settings are presented initially, followed by a comprehensive analysis of the experimental results

### Experimental Settings

In this investigation, twelve popular LVLMs are included, namely Gemini-Pro-Vision pro , Qwen-VL , MiniGPT-4 , LLaVA , InstructBLIP , LLaMA-Adapter-v2 , mPLUG-Owl2 , mPLUG-Owl , InternLM-XComposer , VPGTrans , Otter  and Lynx . All LVLMs are prompted by 25 different instructions to generate image descriptions for 256 images in MSCOCO . All descriptions are generated using beam search with a beam size of 5. For the instructions, we utilize those from  and additionally propose several others, as detailed in the appendix. We use CHAIR  as the evaluation metric for hallucinations, which has two variants: CHAIRI and CHAIRS. Given the ground truth objects in the image, CHAIRI calculates the proportion of objects that appear in the descriptions but not in the image, while CHAIR\({}_{}\) is the proportion of descriptions that include hallucination. Formally, CHAIR\({}_{}\) and CHAIR\({}_{}\) can be expressed as follows:

\[_{I}=\}|}{|\{\}|},\] (1) \[_{S}=\}|}{|\{\}|}.\] (2)

For more experimental settings, we use the Pearson correlation coefficient to measure the correlation between the average length and the hallucination rate of image descriptions. Lengths are measured in word count.

### Experimental Analysis

The results are presented in Figure 2 & 3, from which we get two key observations: **1)** Figure 2 shows the relationship between the hallucination rate and the average image description length, we can observe that the hallucination rate increases with the average image description length and there is a clear linear correlation between them. Specifically, the Pearson correlation coefficient between hallucination rates and the average image description lengths exceeds 0.6 for all LVLMs, with 10 LVLMs exceeding 0.8 and 5 LVLMs exceeding 0.9. **2)** Figure 3 shows the impact of instructions on the length of image descriptions generated by LVLMs, from which we can observe that the length of image descriptions generated by the same LVLM with

Figure 2: Scatter plots of CHAIR scores and average lengths of the 25 sets of image descriptions generated by 25 instructions. \(r\) denotes the Pearson correlation coefficient between the hallucination rates and the average image description lengths, \(R^{2}\) and \(P\) represent the coefficient of determination and \(p\)-value respectively for the linear regression.

different instructions can vary significantly, e.g., Gemini-Pro-Vision with Instruction 11 (101 words in average) v.s. Gemini-Pro-Vision with Instruction 18 (20 words in average). Furthermore, the length of image descriptions generated by different LVLMs with the same instruction can also differ greatly, e.g., MiniGPT-4 (11 words in average) v.s. Gemini-Pro-Vision (97 words in average) with Instruction 17.

Based on the aforementioned two observations, we can draw the following conclusions: **1)** The degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. Hence, it is imperative to take into account the length of image descriptions in hallucination evaluation. However, controlling the length of image descriptions generated by LVLMs is challenging 2, given that even subtle semantic differences between instructions can significantly impact the output length of LVLMs (shown in Figure 3). **2)** In addition to the hallucination degree, the rate at which hallucination degree increase with description length is also a meaningful indicator for characterizing the nature of LVLMs hallucinations. Considering both the hallucination degree and the growth rate of hallucination degree can provide a more comprehensive evaluation for hallucinations in LVLMs. For example, as shown in the Figure 2, although InstructBLIP has the lowest hallucination degrees in short image descriptions, it exhibits high instability with a rapid increase in hallucination degrees, resulting in high hallucination in long image descriptions.

## 4 Length-Hallucination Curve Based Hallucination Evaluation Framework

In this section, we first introduce the average-based hallucination evaluation framework and discuss its limitations. Then, we elaborate on the proposed LeHaCE framework and evaluate representative LVLMs with LeHaCE. Finally, the stability of LeHaCE is analysed.

### Average-Based Hallucination Evaluation Framework

The average-based hallucination evaluation framework mitigates the challenge caused by instructions by averaging the hallucination rates over different instructions. Formally, the hallucination rates and average lengths of the image descriptions generated by the LVLM under N instructions are denoted as \(\{_{i},hr_{i}\}_{i=1}^{N}\). The average hallucination rate \(\) and average length \(\) of image descriptions over all instructions can be calculated as follows: \(=_{i=1}^{N}hr_{i}\) and \(=_{i=1}^{N}_{i}\). The average-based hallucination evaluation framework utilizes \(\) to evaluate the hallucination of LVLMs.

However, due to substantial variations in the average lengths of image descriptions generated by different instruction sets, the average-based framework struggles to mitigate the effect of image description lengths on object hallucinations, resulting in unstable and unfair evaluations. Specifically, as shown in Figure 4 (left), when the average-based framework evaluates an LVLM under different instruction sets, the inconsistent average image description lengths lead to unstable evaluation. Moreover, Figure 4 (right) shows that when the average-based framework evaluates different LVLMs under the same instructions, the inconsistent average image description lengths lead to unfair evaluation.

### Length-Hallucination Curve Based Hallucination Evaluation Framework

Section 3.2 reveals the significant effect of image description lengths on the hallucination degree. To mitigate this effect, it is crucial to control the description length during the hallucination evaluation.

Figure 3: The average lengths of image descriptions generated by LVLMs when prompted by different instructions.

However, controlling the length of generated image descriptions is challenging because LVLMs are highly sensitive to instructions. To address this challenge, we fit a length-hallucination curve to evaluate LVLMs at any desired length. Specifically, based on the clear linear correlation observed in Section 3.2, we assume a linear correlation between image description lengths and hallucination rates of LVLMs. Figure 4 intuitively illustrates the LeHaCE framework.

Formally, we use \(\{_{i},hr_{i}\}_{i=1}^{N}\) to represent the average lengths and hallucination rates of image descriptions generated by the LVLM under N instructions. The linear regression curve of \(\{_{i},hr_{i}\}_{i=1}^{N}\), which we refer to as the Length-Hallucination Curve (LHC), can be formalized as follows:

\[()=+,\] (3)

where \(\) and \(\) are:

\[=^{N}(_{i}-)(hr _{i}-)}{_{i=1}^{N}(_{i}-)^{2}},\] (4) \[=-().\] (5)

Length-hallucination curve summarizes the trend between the hallucination rate and the description length. The regression coefficient \(\) represents the rate at which the hallucination rate increases with the growth of description length. LeHaCE uses the LHC to evaluate the hallucination in LVLMs. LeHaCE consists of two metrics:

\[()=(),\] (6) \[_{GR}=\] (7)

where LeHaCE\(()\) measures the hallucination rate at the specified length \(\), and LeHaCE\({}_{GR}\) measures the rate at which the hallucination rate increases with the increase in description length. Note that LeHaCE can be built upon any hallucination degree evaluation metric, enhancing their stability, fairness, and comprehensiveness. In this paper, we use CHAIR as the metric for measuring the hallucination degree.

Compared to the average-based hallucination evaluation framework, LeHaCE has three advantages. As intuitively shown in Figure 4, LeHaCE can evaluate the hallucination degree of LVLMs at a uniform image description length, thereby mitigating the influence of description length on hallucination degree and resulting in a **more stable and fair** evaluation. Moreover, LeHaCE can evaluate the hallucination degree at multiple lengths and the growth rate of hallucination degree, leading to a **more comprehensive** evaluation.

### Evaluation on MSCOCO and NoCaps

We evaluate twelve LVLMs with LeHaCE at lengths of 20, 40, 60, and 80 words. This evaluation is conducted on subsets of the MSCOCO  test set and the NoCaps  validation set, each compris

Figure 4: **Illustrations** of the average-based evaluation framework (ABF) and our LeHaCE framework. The **left** figure presents the object hallucination evaluation of LLaVa on two instruction sets. The **right** figure presents the object hallucination evaluation of LLaVa and mPLUG-Owl on the same set of instructions.

ing randomly selected 256 images. The length-hallucination curve in LeHaCE is fitted on the CHAIR scores of image descriptions generated by 25 different instructions. To calculate CHAIR scores on NoCaps, we follow the setting proposed in [8; 47]. All descriptions are generated using beam search with a beam size of 5. The experiments are conducted with PyTorch on Nvidia GeForce RTX 3090 GPUs.

The results are shown in Table 1, which demonstrate that LeHaCE can evaluate the object hallucination degree of LVLMs at given image description lengths, as well as the growth rate of the hallucination degree, providing a fair and comprehensive evaluation. Specifically, **1)** For short descriptions, InstructBLIP achieves the best performance on both the MSCOCO and NoCaps datasets. However, its higher growth rate of hallucination degree leads to poor performance on longer descriptions. **2)** For medium-length and long descriptions, Gemini-Pro-Vision and Qwen-VL exhibit the best performance on the MSCOCO and NoCaps datasets, respectively. This is attributed to their relatively small growth rate in hallucination degree. **3)** Gemini-Pro-Vision and LLaVA exhibit the lowest growth rate in hallucination degree on the MSCOCO and NoCaps datasets, respectively.

In Table 1, LVLMs exhibit higher degrees of hallucination on the NoCaps dataset compared to the MSCOCO dataset. This is attributed to the fact that LVLMs typically use the MSCOCO for training, making the NoCaps dataset an out-of-distribution dataset. The results show that the distributional differences not only increase the hallucination degree of LVLMs at various description lengths but also amplify the growth rate of hallucination degree.

### Stability of LeHaCE

As mentioned above, LeHaCE evaluates the hallucination degree of LVLMs in a more stable manner. This subsection verifies the stability of the proposed LeHaCE framework. Specifically, LVLMs are prompted by three sets of different instructions to generate three sets of image descriptions. Each instruction set consists of multiple instructions randomly drawn from a pool of 25 instructions, with no overlap between instructions in different sets. The image descriptions generated by different instructions in each set are evaluated using the LeHaCE framework and the average-based framework, respectively. The stability of the LeHaCE and the average-based frameworks on the three sets of image descriptions is evaluated using the **Relative Standard Deviation (RSD)**, which is defined as the ratio of the standard deviation \(\) to the mean \(\), \(RDS=/\). The lower the RSD, the more stable

  
**Model** & \(_{}(20)\) & \(_{}(40)\) & \(_{}(60)\) & \(_{}(80)\) & \(_{}(68)\) & \(_{}(20)\) & \(_{}(40)\) & \(_{}(60)\) & \(_{}(80)\) & \(_{}(86)\) \\   \\  MiniGPT-4 & 5.33 & 6.66 & 7.98 & 9.31 & 0.07 & 9.27 & 15.71 & 22.15 & 28.59 & 0.32 \\ InstructBLIP & **2.35** & **5.10** & 7.86 & 10.61 & 0.14 & **5.61** & 16.24 & 26.87 & 37.50 & 0.53 \\ Lynx & 3.26 & 6.49 & 9.72 & 12.95 & 0.16 & 8.00 & 17.48 & 26.97 & 36.46 & 0.47 \\ LLAVA & 7.22 & 8.30 & 9.38 & 10.46 & **0.05** & 14.48 & 20.31 & 26.14 & 31.97 & 0.29 \\ Outer & 8.76 & 12.66 & 16.56 & 20.45 & 0.19 & 15.31 & 29.88 & 44.45 & 59.02 & 0.73 \\ VPGTrans & 5.77 & 6.87 & 7.97 & 9.08 & 0.06 & 9.08 & 15.01 & 20.94 & 26.86 & 0.30 \\ LLaMA-Adapter-v2 & 6.04 & 9.29 & 12.54 & 15.80 & 0.16 & 11.31 & 22.99 & 34.66 & 46.34 & 0.58 \\ mPLUG-Owl & 7.15 & 10.84 & 14.52 & 18.20 & 0.18 & 11.18 & 23.71 & 36.25 & 48.79 & 0.63 \\ Gemini-Pro-Vision & 4.30 & 5.22 & **6.15** & **7.07** & **0.05** & 8.00 & **12.61** & **17.22** & **21.83** & **0.23** \\ Internal-XComposer & 5.40 & 7.82 & 10.25 & 12.67 & 0.12 & 9.48 & 19.18 & 28.88 & 38.58 & 0.48 \\ Qwen-VL & 3.44 & 5.36 & 7.28 & 9.20 & 0.10 & 6.15 & 15.31 & 24.47 & 33.63 & 0.46 \\ mPLUG-Owl2 & 3.92 & 7.39 & 10.86 & 14.33 & 0.17 & 8.19 & 21.66 & 35.12 & 48.59 & 0.67 \\   \\  MiniGPT-4 & 14.53 & 16.79 & 19.05 & 21.30 & 0.11 & 23.75 & 35.75 & 47.76 & 59.77 & 0.60 \\ InstructBLIP & **6.52** & **10.20** & 13.88 & 17.56 & 0.18 & 13.33 & 26.39 & 39.45 & 52.50 & 0.65 \\ Lynx & 13.79 & 17.18 & 20.57 & 23.96 & 0.17 & 36.07 & 46.11 & 56.16 & 66.21 & 0.50 \\ LLaVA & 12.68 & 14.48 & 16.29 & 18.09 & **0.09** & 24.15 & 33.90 & 43.66 & 53.42 & **0.49** \\ Otter & 15.49 & 19.03 & 22.58 & 26.12 & 0.18 & 25.38 & 38.89 & 52.40 & 65.91 & 0.68 \\ VPGTrans & 12.51 & 14.39 & 16.26 & 18.14 & **0.09** & 20.39 & 31.95 & 43.51 & 55.07 & 0.58 \\ LLaMA-Adapter-v2 & 12.52 & 16.07 & 19.62 & 23.17 & 0.18 & 22.24 & 35.31 & 48.18 & 61.04 & 0.64 \\ mPLUG-Owl & 12.85 & 15.84 & 18.84 & 21.83 & 0.15 & 19.77 & 30.68 & 41.60 & 52.52 & 0.55 \\ Gemini-Pro-Vision & 12.76 & 15.17 & 17.57 & 19.98 & 0.12 & 22.63 & 34.56 & 46.50 & 58.44 & 0.60 \\ IntemLM-XComposer & 10.93 & 12.74 & 14.54 & 16.34 & **0.09** & 20.12 & 31.22 & 42.33 & 53.44 & 0.56 \\ Qwen-VL & 8.37 & 10.69 & **13.01** & **15.33** & 0.12 & 14.15 & **25.00** & **35.85** & **46.71** & 0.54 \\ mPLUG-Owl2 & 6.91 & 10.82 & 14.72 & 18.63 & 0.20 & **11.72** & 25.45 & 39.17 & 52.90 & 0.69 \\   

Table 1: LeHaCE scores of LVLMs on the MSCOCO and NoCaps datasets. \(_{_{1}}\) and \(_{_{5}}\) represent CHAIR\({}_{}\) and CHAIR\({}_{}\) with the LeHaCE framework. The best result on each metric for each dataset is represented in bold, and the second best result is indicated with an underline.

    \\   &  &  &  &  \\   & C\({}_{}\) & L\({}_{_{}}\) & C\({}_{}\) & L\({}_{_{}}\) & C\({}_{_{}}\) & L\({}_{_{}}\) & L\({}_{_{}}\) & C\({}_{_{}}\) & L\({}_{_{}}\) & C\({}_{}\) & L\({}_{_{}}\) & L\({}_{_{}}\) \\ 

[MISSING_PAGE_POST]

[MISSING_PAGE_FAIL:9]

For the stability of LeHaCE at different lengths, the results are shown in Figure 5, from which we can see that LeHaCE significantly improves the stability of the CHAIR metrics across a wide range of description lengths. All of these experimental results validate the superior stability of LeHaCE.

## 5 Conclusion and Limitations

**Conclusion**: In this paper, we find the degree of object hallucinations is primarily influenced by the length of image descriptions, with instructions only indirectly affecting hallucinations through their effect on image description lengths. The degree of object hallucination and the length of image descriptions exhibit a clear positive linear correlation. Based on our findings, a stable, fair and comprehensive object hallucination evaluation framework named LeHaCE is introduced. Extensive experimental results validate the superiority of LeHaCE over existing frameworks.

**Limitations**: Despite exhaustive investigations, this work still has potential limitations. **1)** We focus on object hallucination, leaving other types of hallucinations for future work. **2)** Due to computational constraints, we evaluate LVLMs on only a subset of each dataset. Nevertheless, we conduct thorough experiments across various datasets to validate our findings and method. **3)** Due to high API fees, we only explore one proprietary business LVLM in our experiments. However, we conduct in-depth analyses on eleven open-source LVLMs, validating the broad applicability of our method. **4)** In the typical practice of evaluating hallucination levels in LVLMs, multiple instructions are usually used to enhance the stability of the evaluation results. Although LeHaCE cannot be used with just one instruction, this limitation does not affect its ability to provide stable evaluations.

## 6 Acknowledgments

This work was supported in part by the National Key R&D Program of China (2021YFF0900500), and the National Natural Science Foundation of China (NSFC) under grants U22B2035 and 62441202.