# A Benchmark for Evaluating Language Model Fit

Ian Magnusson\({}^{}\)  Akshita Bhagia\({}^{}\)  Valentin Hofmann\({}^{}\)  Luca Soldaini\({}^{}\)

Ananya Harsh Jha\({}^{}\)  Oyvind Tafjord\({}^{}\)  Dustin Schwenk\({}^{}\)  Evan Pete Walsh\({}^{}\)

**Yanai Elazar\({}^{}\)  Kyle Lo\({}^{}\)  Dirk Groeneveld\({}^{}\)  Iz Beltagy\({}^{}\)  Hannaneh Hajishirzi\({}^{}\)  Noah A. Smith\({}^{}\)  Kyle Richardson\({}^{}\)  Jesse Dodge\({}^{}\)**

\({}^{}\)Allen Institute for Artificial Intelligence

\({}^{}\)Paul G. Allen School of Computer Science & Engineering, University of Washington

{ianm,jessed}@allenai.org

###### Abstract

Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains--varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma)1, a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., _r/depression_ on Reddit) and programming languages (e.g., _Java_ on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.

## 1 Introduction

Progress in AI is catalyzed by evaluations that define new ways of measuring progress (Deng et al., 2009; Wang et al., 2018; and Wang et al., 2019, _inter alia_). Language models (LMs) often evaluate LM fit as loss or perplexity (Jelinek et al., 1977) on held out training data or few traditional test sets (Chelba et al., 2013; Merity et al., 2016, _inter alia_). These loss measures have been shown to improve predictably with increases in training compute (Kaplan et al., 2020; Hoffmann et al., 2022) and loss may predict performance on downstream tasks (Xia et al., 2022; Gadre et al., 2024; Du et al., 2024). However, scaling pretraining data aggregates more domains that LMs implicitly learn to model (Diaz and Madaio, 2023; Aharoni and Goldberg, 2020). Does rising performance lift all data? Or do some domains capture most improvement in LM fit? How do we evaluate what language distributions models learn from different pretraining data? What domains should studies evaluate loss on to measure the relationship of loss and downstream performance? To answer these questions, perplexity evaluations ought to measure LM fit to many domains, rather than extrapolating trends from a single prescriptive mix of domains.

In this work we introduce Paloma, a benchmark to study LM fit on many domains. We measure perplexity on different distributions of language sampled from 16 sources, such as C4 [Raffel et al., 2019], that have metadata such as URLs marking 546 textual domains. Beyond evaluation data, we aim to enable and enrich fair comparisons for scientific research on language modeling with the following artifacts: guidelines for comparing LM fit, 6 baseline 1B parameter models pretrained on popular corpora, and standardized code for experiments with Paloma.

As reproducing pretrained models for every new project is onerous, we provide standard training controls for benchmark decontamination and training data order to orchestrate a greater density of comparisons across the research community. We also control how Paloma is evaluated by fixing sample size per domain, model vocabulary, and inference format. Lastly, we demonstrate how to make fair comparisons over two measures of cost, number of model parameters and training tokens, enabling assessment of hardware-agnostic efficiency and the measurement of scaling trends.

Among the 16 sources curated in our benchmark, we contribute two new datasets constructed from data held out of Dolma[Soldaini et al., 2024]: (1) a subsample of the top 100 subreddits by number of comments, and (2) code from the top 100 programming languages by number of tokens. Also, we repurpose corpora of fringe online communities to measure LM fit to discourse previously studied for the prevalence of toxicity and hate speech [Ribeiro et al., 2021, Zannettou et al., 2018, Papasavva et al., 2020]. While, capturing domains required by all possible lines of research is impossible for any one benchmark, Paloma focuses on English and code data and aims to assemble the most fine-grained domains readily identifiable from existing metadata.

To demonstrate possible uses of results from our dataset, we present a series of case studies in SS4. Among other findings, our experiments isolate change in fit from which pretraining corpus is used (Figure 1) and find that pretraining without heterogeneous data sources beyond Common Crawl can lead to perplexities in some domains that do not improve consistently with number of tokens seen. We also find that few vocabulary types account for most of the loss measured in perplexity.

In sum, Paloma contributes:

1. Curated release of the most fine-grained perplexity evaluation data in use in LM research, along with guidelines and code for standardized and rigorous perplexity evaluation.
2. New evaluation data for the 100 most popular subreddits and programming languages.
3. 1B LMs pretrained on C4, mc4-en, Falcon Refinedweb, The Pile, RedPajama, and Dolma with controlled hyperparameters, token budget, benchmark decontamination, and training order for fair comparisons, along with code for others to do the same.
4. Case studies demonstrating analyses that are possible with Paloma, such as finding that pretraining without data beyond Common Crawl leads to inconsistent fit to many domains and that perplexity is driven by improved fit on the most common vocabulary strings.

Figure 1: Perplexity on Paloma for baselines pretrained with our experimental controls such as benchmark decontamination. We measure fit over diverse sources beyond data held-out from training. Paloma enables loss comparisons between _different_ models, such as this figure where pretraining data is varied while all other factors are controlled. This measurement excludes documents from fringe sources and code data not supported by our decontamination approach.

## 2 Sources of evaluation data

We define two terms: _Sources_ are as existing datasets (or curated subsets there of) in use for research. _Domains_ are fine-grained partitions of sources based on available metadata that attempt to surface a distinct and intuitive distribution of language (e.g., Wikipedia articles about visual arts or a subreddit for advice on PC builds). Paloma is derived from 16 sources further divided into 546 domains (see Table 1).2 Where we curate previous fine-grained corpora, we inherit their operationalization of domains, ranging from the community-driven Wikipedia ontology to expert curation and automatic classification. Where we build our own fine-grained domains from Reddit and GitHub, we make similar use of metadata about subreddits and file extensions.

Compared to monitoring monolithic validation loss during model development, interpreting LM fit to specific fine-grained domains poses unique challenges. Crucially, we must not assume better LM fit to a domain reflects improvements in the specific skills that are valued by the humans producing language in that domain (Diaz and Madaio, 2023). For instance, we might expect overlapping domains for academic papers in both Dolma and RedPajama to exhibit similar perplexities for a given model, perhaps assuming perplexity represents how much a model captures knowledge about relevant academic fields. But domains can also differ due to preprocessing when texts were collected in each source rather than from how texts were composed by their original authors. So instead of relying on LM fit to measure what we think a model _should_ learn about a domain, we examine anomalies in domain fit to see what a model _is_ learning. We find that the same model can have 391,171 perplexity on arXiv in RedPajama and 14 on the overlapping academic domain, peS2o, in Dolma (SS4.1). In this approach we follow Holtzman et al. (2023) and McCoy et al. (2023) by aiming to examine model behaviors, regardless of their desirability to humans.

Also note that Paloma focuses on English and code data, as most current LMs also emphasize these types of data. However, we strongly encourage future work to explore fit to fine-grained domains in other languages.

The rest of this section addresses each source, why we include it, and how it identifies any domains it contains (all 546 domains are listed in Appendix E).

   Purpose & Source & Val. + Test Tokens & Domains & Tokens per Split per Domain \\   & C4 (Ruffel et al., 2019) & 2,000,000 & 1 & 1,000,000 \\  & MC4-it (Chang et al., 2023) & 2,000,000 & 1 & 1,000,000 \\  & WINSTT-103 (Meity et al., 2016) & 531,103 & 1 & 265,552 \\  & FrontTreussman (Marcus et al., 1999) & 191,735 & 1 & 95,868 \\  & REDPAIAIA (Together Computer, 2023) & 1,399,946 & 7 & 99,996 \\  & Falcon Refreshpunk (Pencole et al., 2023) & 2,000,000 & 1 & 1,000,000 \\  & Dolma (JeddSmith et al., 2024) & 5,994,901 & 6 & 499,575 \\   & M2D2 S2ORC (Ruffel et al., 2022) & 33,374,351 & 167 & 99,923 \\  & M2D2 Wikipedia (Ruffel et al., 2022) & 9,780,719 & 49 & 99,803 \\  & C4-100-domains (Chengoudian et al., 2022) & 19,689,392 & 99 & 99,803 \\  & Dolma-100-stipcroparters (Soldini et al., 2024) & 19,360,263 & 100 & 96,801 \\  & Dolma-100-stipcroparters (Soldini et al., 2024) & 19,999,613 & 100 & 99,998 \\   & WINSTRAAE (Bildger et al., 2016) & 1,441,263 & 2 & 360,316 \\  & Mongegeux Corpus (Ribeiro et al., 2021) & 1,999,915 & 9 & 111,106 \\  & Gan Corpus (Campmon et al., 2018) & 2,000,000 & 1 & 1,000,000 \\  & 4cilan Corpus (Pengawa et al., 2020) & 2,000,000 & 1 & 1,000,000 \\   & & 123,683,201 & 546 & 113,263 \\   &  &  &  & \\ 

Table 1: The 16 data sources sampled to create language modeling evaluations in Paloma (§2), organized by the purpose for inclusion. These coarse-grained sources contain finer-grained domains, which use metadata to distinguish distinctive distributions of language such as a subreddit for discussing board games. Paloma aims to enable research on differences in LM fit over hundreds of domains by curating and standardizing the text datasets with the most fine-grained domains readily available from existing metadata. We target a minimum of 100 thousand tokens per domain and 1 million tokens per source to select a balance between inference cost and metric variance.

Standard language modeling sourcesThough it is common practice to evaluate on held out data from the pretraining corpus of a given model, we evaluate _across_ several standard corpora. **C4**[Raffel et al., 2019, Dodge et al., 2021] and **mC4-en**[Chung et al., 2023] are language model training datasets created by taking the snapshots of Common Crawl data and applying a number of filters with the intention of retaining "high-quality", natural language. Both datasets are filtered to retain natural English, and in this work we only use the English portion of mC4-en. **WikiText-103**[Merity et al., 2016] and **Penn Treebank**[Marcus et al., 1999] are classic datasets that have been used to evaluate language model perplexity for decades (Radford et al., 2019, Brown et al., 2020, Rae et al., 2021, Hoffmann et al., 2022, _inter alia_). WikiText-103 is text from Wikipedia articles, and Penn Treebank [Marcus et al., 1999] is a set of 1989 Wall Street Journal articles3. **RedPajama**[Together Computer, 2023] is an attempt at reproducing the data mixture from LLaMA (Touvron et al., 2023) from sources such as webtext, Wikipedia, arXiv, and StackExchange. It was used to train RedPajama-INCITE (Together Computer, 2023). **Falcon RefinedWeb**Penedo et al.  was created from all Common Crawl scrapes until June 2023 by applying relatively interpretable filters, and is a subset of the Falcon models' training data (Almazrouei et al., 2023). **Dolma**Soldaini et al. (2024) is made of Common Crawl, Wikipedia, books, academic papers, code repositories, and Reddit, and was used to train OLMo models (Groeneveld et al., 2024).

Fine-grained domain sourcesWe include datasets with the most fine-grained metadata marking hundreds of domains. **M2D2**[Reid et al., 2022] is made of academic papers from S2ORC (Lo et al., 2020) and text from Wikipedia, organized into a two-level hierarchy by academic field categories or Wikipedia ontology, respectively. We sample both top-level domains and lower-level subdomains. **C4-100-domains**[Chronopoulou et al., 2022] is text from the 100 internet domains with the most pages in C4.4**Dolma-100-subreddits** and **Dolma-100-programming-languages** are two evaluation sets we introduce in this work sampled from Dolma(Soldaini et al., 2024): the former is text from the top 100 subreddits (ranked by number of posts), and the latter is the top 100 programming languages by number of tokens in the The Stack(Kocetkov et al., 2022). See Appendix E for more details.

Disparities between speech communitiesLMs today primarily process dominant dialects in countries, such as the US, where they are most often trained and deployed. Even within English, hundreds of millions of people around the world speak other dialects that have been shown to be underserved by existing models (Blodgett et al., 2016). As a starting point for measuring disparities between dialects, we include **TwitterRAAE**(Blodgett et al., 2016), two corpora representing African-American and White-aligned English, automatically classified via geolocation information and demographic census statistics. 5

Fringe sources previously studied for problematic discourseLM fit to these fringe texts characterizes model exposure to distinct social contexts in which toxic language arises. **Manosphere**(Ribeiro et al., 2021), **Gab**(Zannettou et al., 2018), and **4chan Corpora**(Papasavva et al., 2020) are three fringe corpora which contain larger proportions of hate speech and toxicity than mainstream sources like Wikipedia or Twitter. These texts span 2006-2019 and include independent message boards and subreddits sharing a masculinist ideology, Gab (an alt-right focused Twitter alternative with minimal moderation), and the Politically Incorrect board (/pol/) of 4chan, a fringe imageboard emphasizing anonymity and ephemerality.

Perplexity evaluations done right

GuidelinesFairly evaluating different models using perplexity is hard. To do so, we must account for factors that can confound results with guidelines for training (G1, G2) and evaluation (G3, G4, G5).

1. [label=G0]
2. Decontamination: Remove _pretraining_ data that leaks evaluation data to ensure validity of perplexity evaluation.
3. Training Order: Where possible, keep the training data order the same to control differences from recency effects.
4. Subsampling: Subsample size poses a tradeoff between inference cost and variance. Size subsamples to tolerate variance equally for each domain.
5. Vocabulary: Vocabulary determines the event space of possible sequences and the comparability of perplexity measurements. Normalizing likelihood by a segmentation intrinsic to the text (e.g., bytes) partially addresses this, but fixing the vocabulary is preferable.
6. Evaluation Format: Use a consistent implementation of perplexity to ensure comparability regarding engineering details such as the handling maximum sequence lengths.

Experimental controlsOur code repository6 releases controls that implement each guideline. Here we briefly explain each (complete specification of our experimental controls is provided in Appendix C).

For G1, we use a Bloom filter (Bloom, 1970) to detect exact match overlaps of pretraining and evaluation data. We match text at the paragraph level, i.e., newline separated spans of text. To avoid coincidental collisions in the space of small strings, we ignore matches in paragraphs smaller than 13 unicode segmented tokens (Unicode, 2023). Similarly, we ignore paragraphs composed of only punctuation, spaces, and emoji. Lastly, as code data consists almost entirely of short and often repeated lines, we forgo any decontamination on these sources (Dolma-100-programming-languages and the The Stack domain of Dolma). Finally, we remove whole pretraining documents if they contain _any_ contaminated paragraph.

For G2, contemporary LMs train on instances that are maximum sequence length concatenations of training documents, so we must fix the order of concatenated instances. We achieve this by fixing the tokenization, maximum sequence length, and random seed, as well as providing datalog coding where order is invariant to number of devices.

For G3, we empirically observe how variance in perplexity over subsamples of C4 evaluation data grows inversely to sample size (Appendix C.2.1). Extrapolating from these results to select desired thresholds for variance, we pick 1 million and 100 thousand tokens as our target size for sources and domains, respectively.

For G4, where possible we fix model vocabulary to GPT-NeoX-20B's (Black et al., 2022) with 3 special tokens added by Groeneveld et al. (2024). When vocabulary must be changed, for instance comparing to off-the-shelf models, we follow The Pile(Gao et al., 2020) and use bits per byte (BPB; Appendix B).

For G5, we follow the input format established by The Pile(Gao et al., 2020). This format evaluates documents individually, rather than packed into concatenated maximum sequence length inputs. Documents longer than maximum sequence length are split into disjoint inputs.

In Table 2 we compare how Paloma implements controls for these guidelines against practices in previous LM benchmarks. Paloma is the first benchmark to remove contamination across all pretraining data. The Pile(Gao et al., 2020) note that they only address decontamination partially by deduplicating 2 of 22 domains at the document level before splitting. Paloma is also the first contemporary perplexity benchmark to recommend and implement a method to fix the training data order, to apply stratified sampling to evaluation domains, and to recommend fixing vocabulary. The Pile and HELM also detail their evaluation formats, but we note that HELM's inference code depends on calls to proprietary APIs which may not remain reproducible for some models.

ComparabilityWhen using Paloma to compare models, we recommend that researchers also adopt our experimental controls or note as a limitation to comparability any uncontrolled factors. We also recommend that measures of cost are considered when comparing models on Paloma, specifically number of model parameters and number of tokens seen in training. Complimentary to work that focuses on realized costs such as energy use, FLOPs, or GPU hours (Peng et al., 2023), we elect to measure these more abstract cost values so that our efficiency comparisons are agnostic to hardware. Finally, as LMs trained with non-constant learning rate schedules scale sub-optimally until improving when learning rate drops towards the end of training, fair comparisons involving intermediate checkpoints should be matched with respect to the portion of total optimization steps completed.

By providing fair comparisons, the following types of claims about perplexity performance can be made with our benchmark: (1) which among compute-matched models performs best, (2) which models reach a given performance with the least compute, (3) which pretraining corpus produces models with best performance, (4) quantifying the trend of performance as a function of scale.

MetricPaloma uses standardized inference code to compute metrics to assess LM fit to the evaluation data we have curated. Perplexity (Jelinek et al., 1977) is our primary metric (others not used in the body of this paper are detailed in Appendix B). Unless otherwise stated, we use perplexity to mean perplexity per token, where a log likelihood \(\) over documents \(N=\{t^{1},,t^{|N|}\}\) is normalized by \((N)\) denoting the number of tokens in the documents (i.e., \((N)=_{t N}|(t)|\)):

\[=_{t N}_{i}^{|t|}p(t_{i} t_{<i})\]

\[=e^{-(N)}}\]

## 4 Case studies

In this section, we present one full case study and a single conclusion from a second. In Appendix D we present additional studies, demonstrating the types of analyses possible with Paloma.

   Guideline & Tite Pile (Sao et al., 2020) & M2D2 [Reduced et al., 2022] & C4-100-domains & HELM LM Scenarios & Paloma \\  G1 DiContainment & partial-dec-level & none & none & not required & sub-dec-level \\ G2 Training Order & not required & not required & not required & not required & fixed \\ G3 Streamplunding & uniform & uniform & uniform & uniform & inherits splits & stratified \\ G4 Vocabulary & not required & not required & not required & not required & fixed \\ G5 Evaluation Format & no context or overlap & not required & not required & API dependent & no context or overlap \\   \# Domains & 22 & 216 & 99 & 14 & 546 \\   

Table 2: Differences between Paloma and other language modeling benchmarks on guidelines (§3) for experiments of assessing LM fit. Ours is the first perplexity benchmark to remove contaminated training data, fix training order, sample domains equally, and fix vocabulary. We also adopt a controlled inference format from Gao et al. (2020).

### Pretraining Beyond Common Crawl Shows Improved Stability of LM Fit

We hypothesize that one of the strongest drivers of differences in performance between different domains is the composition of the pretraining data of a language model. While we show in Appendix D.1 that scaling model parameters or tokens seen increases performance on nearly all domains, the pretraining data composition directly determines the distribution of language that the model is learning to fit, which may or may not align with the distributions of language in the domains we evaluate. Therefore we examine the impact of varying the pretraining corpus while holding all other experimental decisions the same.

Baseline ModelsWe train and release a set of 6 baseline models on common pretraining corpora following our training guidelines (SS3). Training these models ourselves allows us to apply decontamination and fixed order to their pretraining data as well as using a standard tokenizer to enable the greatest level of comparability. These models are 1B parameter models trained for \(\)150B tokens on Dolma, The Pile, RedPajama, Falcon Refinedweb, C4, and mC4-en. Additional training details are included in Appendix G.

Ordinary perplexityIn Figure 1, we consider the most simple and aggregated view of LM fit that Paloma can provide--perplexity as defined in SS3. Specifically we compute perplexity over all data, excluding the three fringe sources with prevalent toxicity. We also exclude code data in Dolma and Dolma-100-programming-languages.7

Using this view, we see that baseline models trained only on Common Crawl data (C4, Falcon Refinedweb, and mC4-en) stand out from the others which incorporate more curated data sources. However, this points to the limitation of this most aggregated view of the results: ordinary perplexity represents fit to domains in proportion to the number of tokens we have chosen to sample from each domain. We sample 100,000 tokens from each domain and the majority of our domains are not sourced from Common Crawl. So Common Crawl is much less represented in Paloma than in most pretraining corpora, which typically consist of mostly Common Crawl as this is the most abundant public source of text data. Nevertheless this simplified view of the results is useful for specific use cases that need a single metric over a prescriptive mix that emphasizes robustness to a diversity of domains, largely derived from non-web scraped sources.

Macro average perplexityFigure 2 provides another aggregation that examines the robustness of fit by considering all domains equally--a macro average of perplexity over domains: \(|D|^{-1}_{d D}(d)\) for domain set \(D\). By contrast _ordinary perplexity_ is essentially an exponentiated micro average over the domains implicitly selected for during corpus curation. Macro averaging lets all marked domains have equal say on the model's performance, instead. To make these macro averages more easily interpretable, we examine them separately per source.

The most striking pattern that emerges with per-source macro averages is the high, and sometimes non-monotonic, perplexity of the 3 baselines trained on only Common Crawl data (C4, mC4-en, Falcon Refinedweb). This is particularly apparent for the C4 model evaluated on RedPajama, where the macro average is dominated by perplexity up to 391,171 on the _arXiv_ domain. Similar spikes occur for the Falcon Refinedweb and mC4-en models, with perplexity of 21,652 and 1,409 respectively, on the Max music programming language domain in Dolma-100-programming-languages. These domains contain large amounts of non-natural language, in the form of LaTeX and other code data. These spikes stand out from the stable and monotonic improvement observed in the other 3 baseline models. While these Common Crawl baselines spike on different domains, it appears they are more susceptible to these extreme gaps in fit to _some_ domains. Perhaps this occurs because of a lack of exposure to specific types of language completely filtered due to having only one set of cleaning filters applied to a single source of data.

In contrast, the baselines that include curated non-webscraped text sources (Dolma, The Pile, and RedPajama) have a relative gap in perplexity that is highly stable through the course of training. This would imply that short training runs on a subsample of such pretraining corpora may be predictive of the LM fit of specific sources after much longer training. To address one exception, the RedPajama baseline often spikes on its final checkpoint, sometimes dramatically as in TwitterAAE. A possible explanation is that this checkpoint falls very soon after the model's training loss recovers from a small spike.

Perplexity per domain ordered by median perplexityWe can visualize each perplexity separately for each domain to surface gaps in fine-grained LM fit. In Figure 3, we arrange the domains by their median perplexity over the baselines, as this order gives some sense of the intrinsic difficulty of a domain. We can then see which baselines follow this order, differing only by a consistent offset, and which have gaps that are more idiosyncratic to each domain. Again we see that when baselines have irregular gaps from the median these are most frequently baselines pretrained on only Common Crawl.

Figure 3: For each source with at least 10 domains, each point visualizes perplexity on a single domain for a fully trained model. Domains are ordered by median perplexity of that domain over all models. Gaps between some baselines are highly consistent across domains (e.g., RedPajama and The Pile baselines on Dolma-100-subreddits). Other models (often pretrained on just Common Crawl data) exhibit noisy gaps that do not follow the trend in median domain difficulty (e.g., the mC4-en baseline on C4-100-domains).

Figure 2: Perplexity macro averaged over any domains within each of the 16 top-level data sources (§2) in Paloma, for each baseline model. Evaluating on one monolithic corpus, such as C4, does not tell the complete story of model fit. Paloma lets us see when trends differ from one distribution of language to another. For instance, the 3 baselines trained on only Common Crawl data (C4, mC4-en, Falcon Refinedweb) exhibit high perplexity, sometimes with non-monotonic scaling over tokens seen, on specific evaluation sources such as RedPajama, and Dolma-100-programming-languages.

The notable exception is The Pile baseline on M2D2 S2ORC and Dolma-100-programming-languages, which has erratic gaps substantially below the median, perhaps indicating that baseline is benefiting from exposure to specific domains and not others rather than only a overall facility for scientific papers and code. The erratic-gapped Common Crawl baselines, by contrast, are all worse than median perplexity, suggesting that they may have complete gaps in exposure to features of certain domains that are not recovered through generalization.

### Common Vocabulary Types Dominate Perplexity

Here we present a single conclusion from a second case study; see Appendix D.2 for further analysis. So far we have examined perplexity aggregated over tokens. Another approach is to measure average likelihood per vocabulary _type_, i.e., the strings that are represented in the vocabulary of a model, in contrast to occurrences of these strings in some corpus, called _tokens_.8

Few vocabulary types account for most of the loss measured in perplexityHow much do specific _types_ contribute to perplexity aggregated per token? To answer, we start by analyzing the total loss mass added by types, as a function of their IDs. Smaller IDs correspond to more frequent types in the GPTNeoX-20B tokenizer training data (Sennrich et al., 2016; Black et al., 2022), and we find an overall moderate to strong correlation between IDs and frequencies in the evaluation data of Paloma as well (Pearson's \(r\) averaged across domains: -0.522\(\)0.087). Crucially, frequency has a strong impact on the total loss mass associated with individual types: while the _average_ loss is lower for the high-frequency types (Figure 3(a)), the _total_ loss is higher, resulting in a situation where 5% of the types already cover roughly 50% of the overall perplexity (Figure 3(b)). Thus, perplexity is strongly influenced by a relatively small set of high-frequency types. This finding provides further evidence that reporting only aggregated perplexity values neglects more subtle dynamics visible through fine-grained analysis (i.e., sources, domains, vocabulary types) in Paloma.

## 5 Conclusion

We believe that evaluations of language modeling fit provide an important view of performance that has been neglected in recent LM research and development. Perplexity cannot be naively applied to language modeling at this scale due to challenges such as benchmark contamination. However, these obstacles are worth overcoming as perplexity offers several advantages not afforded by downstream evaluations. Instead of constructing tasks from scratch, we can rely on the ecological validity of

Figure 4: Mean and total loss per vocabulary type, i.e., specific strings in the vocabulary. While high-frequency types (which have low IDs) tend to have a low _average_ loss as shown by a log-linear regression (a), they contribute a substantial part of the _total_ loss, simply by virtue of their frequent occurrence in the data (b). The figure shows the distributions for Pythia-7B (Biderman et al., 2023) on C4-100-domains, but the overall picture is consistent for different models and sources.

real-world data drawn from known sources. Finding the best ways to evaluate model fit to a collection of documents creates an interface for other fields to contribute to the evaluation of language models. Without needing to understand LM architectures, researchers in other fields can collect corpora representing domains of interest that LM researchers would not know to consider. Once such sources are identified, evaluations can be updated over time by simply scraping more data, unlike downstream tasks where expensive annotation would be required.

Further, we hope that Paloma provides controlled results for study of when perplexity evaluations are or are not predictive of downstream performance (Liu et al., 2022; Tay et al., 2021; Ganguli et al., 2022; Xia et al., 2022; Gadre et al., 2024; Du et al., 2024). In Appendix A, our preliminary investigation reveals that different Paloma sources are correlated with some downstream tasks and anticorrelated with others. This contrasts with the assumption in much scaling literature that lower perplexity always indicates better downstream performance. While we do observe that LM loss reduces with scale across most domains (Appendix D.1), the fit of this relationship and the relationship of loss to downstream performance will both differ for each pretraining and validation distribution as observed by Gadre et al. (2024). This means that one cannot simply find which fine-grained perplexity domains correlate with one's favorite task and then hillclimb on those. Instead further investigation with pretraining experiments across a wide range of scales and data recipes is needed to understand when reductions in perplexity are being driven by superficial overlaps of train and validation distributions or by learning features relevant to downstream use.

## 6 Limitations and Future Work

The largest limitation of Paloma is that we elect to focus just on the language modeling of English and code data. We select this scope as most current LMs also focus on theses types of data. However, we strongly encourage future work to explore how language model fit to fine-grained domains behaves within and across other languages.

Proper use of perplexity as a metric must take into account its limitations. We believe perplexity is best used to show what a model _is_ learning rather than what it _should_ be learning. For instance we find that perplexity on the 3 fringe datasets are tightly related to average document lengths, with the short tweet-like posts in Gab Corpus receiving high perplexities while the long concatenated threads of posts in 4chan Corpus and Manosphere Corpus provide greater context and lower perplexity. At this level of aggregation, differences in surprise between these domains likely have little to do with model fit to specific types of toxicity and more to do with how models use extremely short or long contexts. In our case study in SS4.2, we demonstrate that often it is more appropriate to decompose measures of surprise over specific strings within a corpus, rather than aggregating over all text in a domain. We hope that by surfacing the average likelihoods of specific strings in the vocabulary, Paloma can enable future work on metrics that better measure the fit of models to the features of language in specific domains that humans find most salient.

We also highlight guidelines for evaluating with perplexity (SS3). In particular we believe decontamination of benchmark leakage and balancing variance induced by subsampling across domains are both challenging concerns requiring further investigation. For each of these we have proposed one simple and scalable mitigation (see Appendix C.1.1 and C.2.1 for further details), but future work should explore alternatives and measure their efficacy.

Paloma curates and standardizes the text datasets with the most fine-grained domains readily available from existing metadata. As such, our definition of domains by metadata is necessarily heuristic. Some overlapping domains in Paloma appear in multiple sources, such as academic papers. Though Dolma and RedPajama process academic papers differently, the subcorpora on academic papers in each source represent different approximations of the same or very similar domains. However for the sake of simplicity, we make the reductive assumption of counting all 546 domains in Paloma as fully distinct. We hope that future work will explore novel means of identifying fine-grained domains and separating distribution shifts in language due to differing authorship or differing data collection processes.