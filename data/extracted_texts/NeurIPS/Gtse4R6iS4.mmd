# Robustifying Generalizable Implicit Shape Networks

with a Tunable Non-Parametric Model

 Amine Ouasfi  Adnane Boukhayma

Inria, Univ. Rennes, CNRS, IRISA, M2S, France

###### Abstract

Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training. We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a Nystrom Kernel Ridge Regression, that we further adapt by fitting its hyperparameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method with respect to baselines and the state-of-the-art using synthetic and real data.

## 1 Introduction

Enabling machines to understand and navigate 3D is a major challenge. Instances of this ability include downstream tasks in computer vision and graphics, such as shape reconstruction from noisy, incomplete and relatively sparse point clouds. Building full shapes from point clouds is all the more an important problem in account of the ubiquity of this light, albeit incomplete, 3D shape representation, whether it is acquired from the increasingly democratized consumer grade and industrial depth sensors or _e.g_. as obtained from photogrammetry (_e.g_. Structure From Motion, Multi-View Stereo [62; 63]). Whilst Classical optimization based approaches such as Poisson Reconstruction  and Moving least squares  can deliver mostly good reconstructions for all intents and purposes, they require still dense clean point sets with reliable normal estimations. More recently, deep learning based alternatives have been shown to offer faster and more robust predictions especially for noisy and sparse inputs, without requiring normal information.

Within this class of methods, state-of-the-art feed forward (test time optimization-free) generalizable models are based on implicit neural shape representations, where explicit meshes can be extracted at test time using Marching Cubes . While they offer many desirable properties, such as all-round good performance and fast inference (within tens of milliseconds on a standard GPU), they still suffer from limitations, some of which we come across upon examining the state-of-the-art method  for instance:

Limited OOD generalizationAs these models are typically trained with full 3D supervision, training commonly uses synthetic data such as the ShapeNet  dataset, large real 3D supervision being famously hard to come by. This results in a performance drop for real test data outside the training distribution. Comparing Tables 2 and 6, the performance of  drops by roughly fourfold going from testing on synthetic data to real scans.

Limited input size generalizationAs illustrated in table 7, when presented with testing point clouds that are larger (_e.g_. 10k) than the training size (3k), the performance does not scale and even deteriorates compared to testing on the training size.

Underfitting the inputThere is no explicit guaranty the input points are at the predicted shape function level set from a single forward pass. We hypothesise that enforcing this constraint under well chosen regularizations could potentially lead to improvements at test time and a reduction of the generalization issues mentioned above.

We want to formulate an efficient strategy to improve on these aspects of generalizable shape networks at test time, _i.e_. reconstructing shapes by dynamically adjusting the network to the input point-cloud during testing. We note that we follow such a strategy as training large models from scratch can be resource intensive, while adaptation can be less costly and has been shown to yield considerable improvements in _e.g_. domain adaptation and transfer learning literature. One possible solution  (SAC) is to finetune the network weights at test time, using the knowledge that input point cloud samples are on the desired surface as supervision. However, we find this strategy to be unstable as it can overfit on these samples. As shown throughout the results (Section 4), it even exacerbates the performance of its initial baseline network in many cases. This instability highlights the necessity for regularization in this adaptation process. We need to balance the need to maintain the expressive capacity of the learned functions while ensuring stability and robustness. To this end we devise three important design choices:

\(\) We restrain the hypothesis space of the adapted shape functions to be a Reproducing Kernel Hilbert Space (RKHS). By the Representer Theorem, the minimizer of our regularized empirical risk minimization (ERM) problem (Equation 7) emerges naturally as the solution of a Kernel Ridge Regression (KRR) problem.

\(\) By using a Gaussian kernel in this context, we benefit from the universality properties of the associated (RKHS) _i.e_. a hypothesis space rich enough to approximate any continuous function arbitrarily well. Thus, we avoid the difficulties arising from optimizing a large number of neural network parameters, as in SAC, while maintaining the expressive capacity necessary for effective shape modeling.

\(\) We propose a strategy to solve our regularized ERM problem in a RHKS space adapted to the shape we would like to recover. This is possible thanks to the unique correspondence between RKHS and kernels. Hence, instead of relying on handmade fixed kernels, we learn the KRR hyperparameters using a loss function that avoids overfitting on the data term (Equation 11).

In this regard, approximate KRR solvers based on Nystrom samples (Nystrom KRR)  can be computed efficiently and allow scaling in point cloud size. As shown in our ablation studies, defining this kernel in euclidean space seems insufficient to constrain the fitting problem. Hence we define it in the feature space of the pretrained convolutional shape network. To define the fitting data, we use the input point cloud with their inherent _label_: being samples from the surface by default. We also augment the fitting data with additional samples paired with their shape network predictions as pseudo-labels. This augmentation ensures the fitting problem is well defined. We note that through this strategy, knowledge is being transferred from the network to our kernel regression through both features and pseudo-labels.

Until recently, exiting methods for KRR hyperparameter tuning, _e.g_. cross-validation or grid search, placed strong limits on the flexibility and number of hyperparameters that can be tuned. The work of Meanti _et al_.  proposed lately a automated gradient based tuning of the hyperparameters of the KRR by minimizing a loss that prevents overfitting on the fitting data. This strategy allows the optimization of both the kernel hyperparameters and the Nystrom samples, and proves beneficial in our context, _i.e_. fitting a shape function in feature space, as compared to baselines with fixed Nystrom samples and/or kernel hyperparameters (See ablation in section 4.7). A visual summary of our method can be found in Figure 1.

To test our idea we devise experiments on real and synthetic data targeting generalization issues. Our approach improves consistently on the baseline networks, and outperforms other state-of-the-art methods and test time network finetuning based strategies. We also show that our approach can be applied successfully to more than one network. Our ablation studies showcase also the importance of all the various components of our method.

## 2 Related Work

We review in this section previous work we deemed most relevant to our problem and contribution.

Shape Representations in Deep LearningShapes can be represented within deep learning either intrinsically or extrinsically. Intrinsic representations are a discrimination of the shape itself. When done explicitly, using _e.g_. tetrahedral or polygonal meshes  or point clouds , the output topology in predefined thus bounding the variability of the shapes that can be generated. Among other forms of intrinsic representations, 2D patches  can prompt discontinuities, whilst the simplicity of shape primitives such as cuboids , planes  and Gaussians  limits their expressiveness. Differently, extrinsic shape representations model the entire space containing the scene or object of interest. Voxel grids  are the most popular one being the direct extension of 2D pixels to 3D domain. However, their capacity is limited by their cubic resolution memory cost. Sparse representations such as octrees  can alleviate this issue to some extent.

Implicit Neural Shape RepresentationsImplicit neural representations (INRs) emerged recently as a major medium for modelling extrinsic shape and radiance fields (_e.g_. ). They overcome many of the limitations of the aforementioned classical representations thanks to their ability to represent shapes with arbitrary topologies at virtually infinite resolution. They are usually parameterised with MLPs mapping spatial locations or features to _e.g_. occupancy , signed  or unsigned  distances relative to the target shape. The level-set of the inferred field from these MLPs can be rendered through ray marching , or tessellated into an explicit shape using _e.g_. Marching Cubes . Another noteworthy branch of work builds hybrid implicit/explicit representations  based mostly on differentiable space partitioning. In order to represent collections of shape simultaneously, implicit neural models require conditioning mechanisms. These include feature and latent code concatenation, batch normalization, hypernetworks  and gradient-based meta-learning . Concatenation based conditioning was first implemented using single global latent codes , and further improved with the use of local features .

Reconstruction from Point CloudClassical approaches include combinatorical ones where the shape is defined through an input point cloud based space partitioning, through _e.g_. alpha shapes  Voronoi diagrams  or triangulation . On the other hand, the input samples can be used to define an implicit function whose zero level set represents the target shape, using global smoothing priors _e.g_. radial basis function  and Gaussian kernel fitting , local smoothing priors such as moving least squares , or by solving a boundary conditioned Poisson equation . The recent literature proposes to parameterise these implicit functions with deep neural networks and learn their parameters with gradient descent, either in a supervised or unsupervised manner.

Unsupervised Implicit Neural ReconstructionA neural network is typically fitted to the input point cloud without extra information. Regularizations can improve the convergence such as the spatial gradient constraint based on the Eikonal equation introduced by Gropp _et al_. , a spatial divergence constraint as in , Lipschitz regularization on the network . Atzmon _et al_. learns an SDF from unsigned distances , and further supervises the spatial gradient of the function with normals . Ma _et al_.  expresses the nearest point on the surface as a function of the neural signed distance and its gradient. They also leverage self-supervised local priors to deal with very sparse inputs  and improve generalization . All of the aforementioned work benefits from efficient gradient computation through back-propagation in the neural network. Periodic activations were introduced in . Lipman  learns a function that converges to occupancy while its log transform converges to a distance function.  learns infinitely wide shallow MLPs as random feature kernels using points and their normals. Most of the aforementioned methods present failures under sparse and noisy input due to the lack of supervision and data priors. Differently, we propose here to combine a data prior based method and self-supervised learning.

Supervised Implicit Neural ReconstructionSupervised methods assume a labeled training data corpus commonly in the form of dense samples with ground truth shape information. Auto-decoding methods  require test time optimization to be fitted to a new point cloud, which can take up to several seconds. Encoder-decoder based methods enable fast feed forward inference. Introduced first in this respect, Pooling-based set encoders  such as PointNet  have been shown to underfit the context. Convolutional encoders yield state-of-the-art performances. They use local features either defined in explicit volumes and planes  or solely at the input points . Ouasfi _et al_.  proposed concurrently the first convolution-free fast feed forward generalizable model. Peng _et al_.  proposed a differentiable Poisson solving layer that converts predicted normals into an indicator function grid efficiently. However, it is limited to small scenesdue to the cubic memory requirement in grid resolution. Most of these methods still suffer from generalization issues. Concurrently, the work in [78; 29] proposes to build generalizable models where the implicit decoder is a kernel regression. Differently, we advocate a strategy to improve the generalization of any preexisting pretrained reconstruction network.

## 3 Method

Our input is a noisy unoriented point cloud \(^{3 N_{}}\). Our objective is to recover a 3D reconstruction, _i.e_. the shape surface \(\) that best explains this observation, where the input point cloud elements approximate noisy samples from \(\).

To achieve this, we model an implicit function \(f\) that predicts occupancy values relative to a target shape \(\) at any queried euclidean space location \(q^{3}\), given the input point cloud \(\), _i.e_. \(f(,q)=1\) if \(q\) is inside the shape, and \(0\) otherwise. The inferred shape \(}\) can then be obtained as a level set of the occupancy field inferred through \(f\):

\[}=\{q^{3} f(,q)=0.5\}.\] (1)

In practice, an explicit triangle mesh for \(}\) is extracted using the Marching Cubes  algorithm.

### Feedforward Generalizable Model

The staple back-bone models for feedforward generalizable implicit shape reconstruction from point cloud (_e.g_. [57; 17; 7]) have typically a two-stage architecture. First, a deep convolutional network (usually a U-Net) builds features in \(^{N_{F}}\) from the input point cloud:

\[=().\] (2)

Features \(\) can be extrinsic or intrinsic. For instance in [57; 17], the ConvNet builds an extrinsic 2D or 3D explicit feature grid representing space around the shape. More recently, Boulch _et al_.  argued against this strategy, contending that voxel centers may be far from the input point cloud locations, and are sampled uniformly while they should be more focused near the surface of interest. They hence define features intrinsically, _i.e_. only at the input point cloud locations. In both strategies, the ConvNet offers transnational equivariance and generalization ability. It is worth noting that it is also naturally endowed with a locality mechanism, which is crucial for performance with fast inference times, as only a single forward pass in the encoder is needed to build the feature space. Conversely, other generalizable models lacking locality in their encoders, such as the work in , enforce locality through local patch inputs. They hence require as many encoder forward passes as there are query points. This naturally results in significantly increased inference times compared the the aforementioned competition that we build on.

Figure 1: Overview. Our method predicts an implicit shape function from a noisy unoriented input point cloud. We combine a cross-shape deep prior (Pretrained generalizable occupancy network) and an intra-shape adaptive Nyström Kernel Ridge Regression (NKRR) \(g\) at test time. The latter learns to map network features of the input points and additional points to the level-set and network generated pseudo-labels respectively. The NKRR hyperparameters (\(Z\),\(\)) are adjusted to the current shape.

Given a 3D query point \(q\), an implicit decoder, typically an MLP, maps the feature to the occupancy value:

\[(q) =(((q)))\] (3) \[=(l(q)),\] (4)

where \((.)\) represents the Sigmoid activation and \(l(.)\) a logit function. In the case of extrinsic feature networks [57; 17; 56], feature \((q)\) is obtained by linear interpolation (trilinear in the case of 3D feature grids, and bilinear for 2D grids). For intrinsic feature networks , the feature is pooled order-invariantly from the nearest points of the input point cloud. Without loss of generality we build in this work on the model in , as it has been shown to achieve state-of-the-art performances on several benchmarks.

### Kernel Ridge Regression as a Shape Function in Feature Space

Existing pretrained generalizable feedforward models for reconstruction from point cloud suffer from generalization limitations. As they are typically trained on synthetic shape datasets (ShapeNet ) with full supervision, using a predefined point cloud size and simulated noise, their performance reduces when tested on point clouds that are different from the training ones regarding properties such as density, and whether they are real or simulated. Such shortcomings of an instance of these models are showcased in Tables 7 and 5. We design henceforth an efficient mechanism that can fill this performance gap, while benefiting from the available shape labels at test time: The fact that the input point cloud is made of samples that belong to the surface we wish to reconstruct.

We consider the following training data samples:

\[\{(x_{i},y_{i})\}_{i=1}^{n}=\{((p_{i}),0)\}_{i=1}^{N_{p}}\{( (_{i}),l(_{i}))\}_{i=1}^{N_{p}},\] (5)

where \((x_{i},y_{i})^{N_{F}}\) and \(n=2 N_{p}\). \(\{p_{i}\}\) are the samples of the input point cloud \(\). \(\{_{i}\}\) are additional samples around this point cloud, that can be automatically generated following _e.g_. the strategy in . Features \((.)\) and occupancy logits \(l(.)\) are obtained from the pretrained convolutional occupancy model (See Equations 2 and 4). We note that these data samples are corrupted by a combination of observation noise in the input point cloud (points \(p_{i}\) not being exactly at the surface) and prediction errors of the initial model (pseudo-labels \(l(_{i})\) being inaccurate).

We approximate the mapping from deep shape features \(x_{i}\) to occupancy logits \(y_{i}\) using a non-linear non-parametric regression \(g(.)\) to provide spatial regularization while favouring flexibility, _i.e_. without making strong assumptions on the model. In particular, we use Kernel Ridge Regression (KRR), where the hypothesis space of the mapping is a reproducing kernel Hilbert space \(\). Associated to this space is a kernel function \(k_{}:^{N_{F}}^{N_{F}}\), that depends on hyperparameters \(\). To ensure the minimisation of the square error of the mapping over the training samples is well defined, a regularization is typically needed, with a weight \(\):

\[_{} =*{arg\,min}_{g} g()- ^{2}+ g_{}^{2},\] (6) \[=\{x_{i}\}_{i=1}^{n},= \{y_{i}\}_{i=1}^{n}.\] (7)

The unique solution to this problem is expensive to compute. For efficiency we follow the approximation to KRR that considers a lower subspace of dimension \(m<<n\). This allows our method to scale to large input point clouds. The approximation uses \(m\) inducing features \(=\{z_{j}\}_{j=1}^{m}\), also known as Nystrom Samples, where \(z_{j}^{N_{F}}\).

Finally, the unique solution to the minimization in Equation 6 writes :

\[_{,}=_{j=1}^{m}_{j}k_{ }(,z_{j}),\] (8) \[=(_{nm}^{} _{nm}+ n_{mm})^{-1}_{nm}^{} .\] (9)

Here, \((_{nm})_{i,j}=k_{}(x_{i},z_{j})\) and \((_{mm})_{i,j}=k_{}(z_{i},z_{j})\), where \(i 1,n\) and \(j 1,m\). This solution can be computed efficiently through _e.g_. the Falcon solver .

### Learning the Shape Function Hyperparameters

Next, based on , we propose to automatically learn the hyperparameters of our Nystrom KRR, namely the kernel hyperparameters \(\) and the Nystrom samples \(\), using gradient descent. This shape specific tuning allows for a larger hyperparameter space and using more expressive kernels. The optimization consists in minimizing an upper bound \(^{}\) of the ideal objective, the expectation over all possible data points of the squared error of the model. This allows to avoid overfitting on the training set:

\[^{} =((}+n )^{-1}})+ (-})^{}( _{,})+2^{}(_{,}),\] (10) \[^{}(_{,})=\|_{,}()- \|^{2}+\|_{,}\|_{}^{2}.\] (11)

\(}=_{nm}_{mm}^{}_{ nm}^{}\) and \({}^{}\) symbolizes the Moore-Penrose matrix inverse. The upper bound is made, in this order, of he Nystrom penalty attempting to make the Nystrom kernel \(}\) closer to the complete kernel \(\), a second term penalizing overly complex kernels, and finally a data fit term. We note that the authors in  derived this loss by replacing expectations with their empirical counterparts. They also propose to use the Hutchinson approximation to compute matrix traces for efficiency.

Aftre \(N\) gradient descent steps (as illustrated in Algorithm 1), we select hyperparameters \(^{*}\) and \(^{*}\) that minimize the squared error of the model over the training data following . For a given query point q, our final implicit occupancy function can be expressed as the sigmoid activated optimal Nystrom KRR in feature space:

\[f(,q)=(_{^{*},^{*}}((q)))\] (12)

```
0: Point cloud \(\), pretrained occupancy network \((,)\), learning rate \(\)
0: optimal kernel hyperparameters \(^{*}\) and Nystrom samples \(^{*}\) \(}=()\) \(=([,}])\) \(=[,(})]\)  initialize \(\) as \(m\) random features from \(\), initialize \(\) for\(N\) times do  compute \(_{,}\) from \(\), \(\) (Equations 8 and 9)  compute \(^{}\) for \(_{,}\), \(\) and \(\) (Equations 10 and 11) \((,)(,)-_{,}^{}\) endfor ```

**Algorithm 1** The training procedure of our method.

## 4 Results

In this section, we evaluate our method using real and synthetic data both quantitatively and qualitatively. As our method builds on Poco , we compare our results to this baseline. We also compare to other state-of-the-art convolutional occupancy networks, such as Conv  and SAP . We compare to the strategy based on test-time finetuning of a convolutional occupancy network (Conv  ) in SAC . Additionally, we compare to the strategy based on fitting an MLP to the point cloud in NP . We compare also to classical learning-free Poisson reconstruction (SPSR ) where input point normals are estimated with PCA. For state-of-the-art convolutional occupancy networks Poco, Conv and SAP, we use their official publicly available pretrained models as trained on the same training split of ShapeNet  with 3k sized point clouds and noise of variance 0.005. We use the publicly available official implementations of SAP and NP. We present also an ablative analysis of the components of our method in Section 4.7. We provide additional results in the supplementary material. We note that from a computational stand point, for a 10k sized input point cloud, our method takes roughly 10 seconds to converge on a NVIDIA RTX A6000 GPU. The other test-time tuning based alternatives we compare to here, namely SAC and NP take roughly 1 and 5 minutes respectively.

[MISSING_PAGE_FAIL:7]

its lack of generalization w.r.t. the input size. Figure 4 shows additional visual comparisons to SPSR. We provide a numerical comparison to this classical method in the supplementary material.

### Generalization to Real Scene Scans

We evaluate here the generalization from synthetic object training to real scenes. We show reconstruction results from 10k points sampled from real scans of ScanNet v2 in Table 5. As meshes are not available we can not compute IoU here. Our method (Ours(Poco)) based on Poco outperforms the baseline and the competition, demonstrating superior generalization ability. Figure 3 shows a few

Figure 3: Qualitative comparison of ScanNet v2 reconstructions.

Figure 2: Qualitative comparison of ShapeNet reconstructions.

qualitative results. While we offer improved performances compared to the competition, we note that the quality of our reconstruction remains limited still is such a challenging setting. We also show that our method can be successfully applied to other existing occupancy networks, as combining our strategy with baseline Conv (Ours(Conv)) leads to a considerable improvement in its performance.

### Generalization to Real Articulated Shape Scans

We evaluate here generalization from synthetic object training to real non-rigid human shapes. We show reconstruction results from 10k points sampled from real scans in Faust. As summarized in Table 6, both convolutional networks SAP and Poco offer good initial performances. Notice how the finetuning based SAC can improve of its baseline (Conv) CD\({}_{1}\) score while conversely worsening its IoU score. We outperform both our baseline and competing methods. Figure 5 shows a visual comparison of the results, where we recover from some failures of our baseline (Poco), and show limitations of the finetuning strategy in SAC. Figure 4 shows additional visual comparisons to SPSR.

### Ablative Analysis

We conduct further ablative studies of our proposed method on the Lamp class of ShapeNet, as we found its shapes to be challenging and displaying many fine and complex features. Table 7 shows the performance of both our method and our baseline network Poco  for inputs of size 500, 3k and 10k. We note that Poco has been trained on 3k sized inputs. Notice first how the priorating in IoU. This shows the performance of this model does not necessarily scale with input size. Our method on the other hand shows healthy consistent improvements over the baseline Poco, which also scale with input size. We notice also that the performance gap w.r.t. Poco is slightly tighter in the sparsest setup (500 points). We believe this is due to the inductive bias of kernel methods favouring smoothness, as such a prior can prove less effective for extremely sparse data.

    & IoU\(\) & CD\({}_{1}\)\(\) & NC\(\) & FS\(\) \\  SPSR & - & 0.48 & 0.91 & 0.91 \\ SAP & 0,90 & 0,29 & 0,93 & 0.98 \\ Conv & 0,85 & 0,41 & 0,92 & 0.95 \\ SAC & 0,77 & 0,30 & 0,92 & 0.95 \\ Poco & 0,90 & 0,32 & 0,93 & 0.97 \\ Ours & **0,92** & **0,26** & **0,95** & **0,99** \\   

Table 6: Faust reconstruction.

Figure 4: Qualitative comparison to Screened Poisson Surface Reconstruction (SPSR). **Input / Ours / SPSR**, in this order.

    & CD\({}_{1}\)\(\) & NC\(\) & FS\(\) \\  SPSR & 2.27 & 0.74 & 0.68 \\ SAP & 1,12 & 0,73 & 0.67 \\ Conv & 1,22 & 0,71 & 0,60 \\ SAC & 0,97 & 0,76 & 0,77 \\ Poco & 1,11 & 0,77 & 0,79 \\ Ours (Poco) & 0,94 & **0,79** & **0,80** \\ Ours (Conv) & **0,70** & 0,77 & 0,79 \\   

Table 5: ScanNet reconstruction.

Figure 5: Qualitative comparison of Faust reconstructions.

Table 8 shows an ablation of the design choices of our method. Our performance drops as expected whether we disable the kernel hyperparameters \(\) finetuning (w/o \(\)), the Nystrom samples \(\) finetuning (w/o \(\)). We note also the importance of learning the regression in feature space. We notice that reconstruction often fails when we attempt to solve the regression in euclidean space. Hence our w/o Feat. version uses a Fourier positional encoding to stabilize this baseline. However, this version still under performs compared to our final model. Disabling the tuning all together (w/o tune) results in a performance drop as well. Increasing the number of number of Nystrom samples in this context improves the performance.

Table 8 provides also an ablation the number of Nystrom samples \(m\) while tuning (Ours). We find that generally the higher the number of inducing features \(\) the better the performance, as it was the case without hyper-parameter optimization (w/o tune). This is expected as the approximation error of the Nystrom KRR decreases with more inducing points. Additionally, more learnable inducing points implies more representation power for the KRR function. Increasing \(m\) comes however with increased computation overhead due to both solving the Nystrom KRR with a higher number of inducing features \(\) and also learning these features. Increasing \(m\) beyond a certain limit can also lead to overfitting. As our aim in this work is an efficient approach to improve generalization, we find a relatively small \(m\) value (500) already sufficient to produce satisfactory shape predictions and offers a good performance/compute overhead trade-off.

Finally, we note that we chose to use an equal number of input and augmented samples (Equation 5) empirically as it gave the best overall results. It is possible that for high levels of noise, a higher number of augmented samples could prove beneficial. However, higher levels of noise for the input also imply less accurate predictions from the occupancy network, meaning less reliable pseudo-labels for the augmented samples.

## 5 Limitations

Extreme generalization from relatively sparse point cloud is still challenging for our method as well as the literature and we will seek further improvement in future work. Although it offers robustness, the inherent inductive bias in our method also favours smoothness, which can prove not ideal for representing the highest level of detail. Ablation of number of the NKRR Nystrom samples \(m\) in Table 8 shows that the KRR can possibly suffer from overfitting for large values of \(m\), hence setting this hyperparameter can be challenging. We provide visualizations and a short discussion of the behavior of our method under some baseline failures in the supplementary material.

## 6 Conclusion

We presented a method for shape reconstruction from unoriented point cloud that combines the data prior of a preexisting deep reconstruction network and an efficient non-parametric interpolant. The resulting combination offers improved generalization over the network baseline, network test-time finetuning, and several state-of-the-art methods. We hope we can inspire more work in the direction of fine-tuning and transfer learning from preexisting feedforward occupancy networks, especially since there are many competing strategies that could be explored. For instance, while we propose here to improve the boundary decision by learning the shape function, an alternative strategy could be to tune solely the features instead.

    & IoU\(\) & CD\({}_{1}\)\(\) & NC\(\) & FS\(\) \\  Poco(500) & 0,71 & 0,85 & **0,86** & 0,84 \\ Ours (500) & **0,73** & **0,80** & 0,85 & **0,85** \\  Poco(3k) & 0,85 & 0,60 & 0,91 & **0,96** \\ Ours (3k) & **0,86** & **0,52** & **0,92** & **0,96** \\  Poco(10k) & 0,80 & 0,56 & 0,90 & 0,95 \\ Ours (10k) & **0,87** & **0,46** & **0,92** & **0,96** \\   

Table 7: Ablation of input size.

    & IoU\(\) & CD\({}_{1}\)\(\) & NC\(\) & FS\(\) \\  w/o tune (\(m=1k\)) & 0.84 & 0.61 & **0.92** & 0.96 \\ w/o tune (\(m=5k\)) & **0.87** & 0.55 & **0.92** & **0.97** \\ w/o tune (\(m=10k\)) & **0.87** & **0.53** & **0.92** & 0.96 \\  w/o Feat. & 0.45 & 2.23 & 0.79 & 0.58 \\ w/o \(\) & 0.86 & 0.52 & 0.92 & 0.95 \\ w/o \(\) & 0.86 & 0.53 & 0.92 & 0.96 \\  Ours (\(m=500\)) & 0.87 & 0.42 & **0.92** & 0.96 \\ Ours (\(m=1000\)) & **0.88** & **0.41** & **0.92** & **0.97** \\ Ours (\(m=5000\)) & 0.86 & 0.43 & 0.91 & 0.96 \\   

Table 8: Ablation of our model.