# RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning

Yujie Zhao\({}^{1}\), Jose Efraim Aguilar Escamil\({}^{2}\), Weyl Lu\({}^{3}\), Huazheng Wang\({}^{2}\)

\({}^{1}\) University of California, San Diego, \({}^{2}\) Oregon State University, \({}^{3}\) University of California, Davis

yuz285@ucsd.edu, agujose@oregonstate.edu,

adslu@ucdavis.edu, huazheng.wang@oregonstate.edu

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has recently surged in popularity, particularly for aligning large language models and other AI systems with human intentions. At its core, RLHF can be viewed as a specialized instance of Preference-based Reinforcement Learning (PbRL), where the preferences specifically originate from human judgments rather than arbitrary evaluators. Despite this connection, most existing approaches in both RLHF and PbRL primarily focus on optimizing a mean reward objective, neglecting scenarios that necessitate risk-awareness, such as AI safety, healthcare, and autonomous driving. These scenarios often operate under a one-episode-reward setting, which makes conventional risk-sensitive objectives inapplicable. To address this, we explore and prove the applicability of two risk-aware objectives to PbRL: nested and static quantile risk objectives. We also introduce Risk-Aware-PbRL (RA-PbRL), an algorithm designed to optimize both nested and static objectives. Additionally, we provide a theoretical analysis of the regret upper bounds, demonstrating that they are sublinear with respect to the number of episodes, and present empirical results to support our findings. Our code is available in https://github.com/aguilarjose11/PbRLNeurips.

## 1 Introduction

Reinforcement Learning (RL) (Russell and Norvig, 2010) is a fundamental framework for sequential decision-making, enabling intelligent agents to interact with and learn from unknown environments. This framework utilizes a reward signal to guide the selection of policies, where optimal policies maximize this signal. RL has demonstrated state-of-the-art performance in various domains, including clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017).

Despite its performance, a significant limitation of the standard RL paradigm is the selection of a state-action reward function. In many real-world scenarios, constructing an explicit reward function is often a complex or unfeasible task. As a compelling alternative, Preference-based Reinforcement Learning (PbRL) (Busa-Fekete et al., 2014; Wirth et al., 2016) addresses this challenge by deviating from traditional quantifiable rewards for each step. Instead, PbRL employs binary preference feedback on trajectory pairs generated by two policies, which can be provided directly by human subjects. This method is increasingly recognized as a more intuitive and direct approach in fields involving human interaction and assessment, such as autonomous driving (Basu et al., 2017), healthcare (Coronato et al., 2020), and language models (Bai et al., 2022).A specialized and increasingly popular variant of PbRL is Reinforcement Learning from Human Feedback (RLHF)Bai et al. (2022), which has garnered considerable attention for aligning AI systems with human intentions--particularly in the domain of large-scale language models.

Previous approaches to PbRL (Xu et al., 2020; Coronato et al., 2020; Xu et al., 2020; Chen et al., 2022; Zhan et al., 2023) mainly aim to maximize the mean reward or utility, which is risk-neutral. However, there is a growing need for risk-aware strategies in various fields where PbRL has shown empirical success. For example, in autonomous driving, PbRL reduces the computational burden by skipping the need to calculate reward signals for every state-action pair (Chen et al., 2022). Despite this improvement, the nature of the problem makes dangerous actions costly. Thus, such risk-sensitive problem settings require risk awareness to ensure safety.

Risk-Aware PbRL also has implications for fields like generative AI (OpenAI, 2023; Chen et al., 2023), where harmful content generation remains a challenge for fine-tuning. In this scenario, a large language model (LLM) is often fine-tuned with user feedback by generating two prompt responses, A and B. Many approaches using RLHF (Ouyang et al., 2022) consider human feedback to penalize harmful content generation. Unfortunately, current approaches only minimize the average harmfulness of a response. This can be a challenge when responses are only harmful to a minority of users. Risk-aware PbRL tackles this challenge by directly aiming to decrease the harmfulness directly, rather than indirectly as with human feedback fine-tuning.

Despite substantial evidence highlighting the importance of risk awareness in PbRL, a significant gap persists in the theoretical analysis and formal substantiation of risk-aware PbRL approaches. This deficiency has spurred us to develop risk-aware measures and their theoretical analysis within PbRL. In standard RL, a variety of risk-aware measures have been explored, including a general family of risk-aware utility functions (Fei et al., 2020), iterated (nested) Conditional Value at Risk (CVaR) (Du et al., 2022), and risk-sensitive with quantile function form (Bastani et al., 2022). In general, these measures can be categorized into two types: nested or static. Nested measures (Fei et al., 2020; Du et al., 2022) utilize MDPs to ensure risk sensitivity of the value iteration at each step under the current state, resulting in a more conservative approach. In contrast, static risk-aware measures Bastani et al., 2022 analyze the risk sensitivity of the whole trajectory's reward distribution. In developing and introducing risk-aware objectives in PbRL, we have encountered the following technical challenges in algorithm design and theoretical analysis:

Rewards are defined over trajectories preferenceIn PbRL, the reward function depends on the preference between two trajectories generated by the agent. We refer to this difference in how the reward function is computed as the one-episode-feedback characteristic. Consequently, the risk-aware objectives of standard RL like Du et al. (2022) and Fei et al. (2020) become unmeasurable since they depend on the state-action reward.

Trajectory embedding reward assumptionWhen computing the trajectory reward, it is assumed that an embedding mapping exists. By using the trajectory embedding along with some other vector embedding pointing towards high-rewarding trajectories, the reward is computed with a dot product. Unfortunately, the embedding mapping may not be linear. This means that the embedded trajectory vectors may not follow the Markovian assumption, making the embeddings history-dependent.

Loss of linearity of Bellman functionWhen using a quantile function to transform a risk-neutral PbRL algorithm into a risk-aware algorithm, the Bellman equation used to solve the problem becomes non-linear. This change to the bellman equation disrupts calculations on regret, making risk-neutral PbRL inapplicable. This is primarily due to the additional parameter \(\), which modifies the underlying distribution.

In this paper, we address these challenges by studying the feasibility of risk-aware objectives in PbRL. We propose a provably efficient algorithm, Risk-Aware-PbRL(RA-PbRL), with theoretical and empirical results on its performance and risk-awareness. Our summary of contributions is as follows:

1. We analyze the feasibility of several risk-aware measures in PbRL settings and prove that in the one-episode-reward setting, nested and static quantile risk-aware objectives are applicable since they can be solved and computed uniquely in a given PbRL MDP.
2. We expand the state space in our formulation of a PbRL MDP and modify value iteration to address its history-dependent characteristics from the one-episode setting. These modifications enable us to use techniques like DPP to search for the optimal policy.
3. We develop a provably efficient (both computationally and statistically) algorithm, RA-PbRL, for nested and static quantile risk-aware objectives. To the best of our knowledge, we are the first to formulate and analyze the finite time regret guarantee for a risk-aware algorithm with non-Markovian reward models for both nested and static risk-aware objectives. Moreover, we construct a hard-to-learn instance for RA-PbRL to establish a regret lower bound.

## 2 Related Work

### Preference-based Feedback Reinforcement Learning

The incorporation of human preferences in RL, such as Jain et al. (2013), has been a subject of study for over a decade. This approach has proved to be successful and has been widely used in various applications, including language model training (Ouyang et al., 2022), clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017). PbRL can be categorized into three distinct types Wirth et al. (2017): action preference, policy preference, and trajectory preference. Among these, trajectory preference is identified as the most general and widely studied form of preference-based feedback, as evidenced by the rich literature on the topic Chen et al. (2022); Xu et al. (2020); Wu and Sun (2023). As noted in our introduction, previous theoretical explorations on PbRL have predominantly aimed at achieving higher average rewards, which encompasses risk-neutral PbRL. We distinguish our work by taking the novel approach of formalizing the risk-aware PbRL problem.

### Risk-aware Reinforcement Learning

In recent years, research on risk-aware RL has proposed various risk measures. Works such as Fei et al. (2020); Shen et al. (2014); Eriksson and Dimitrakakis (2019) integrate RL with a general family of risk-aware utility functions or the exponential utility criterion. Accordingly, studies like Bastani et al. (2022); Wu and Xu (2023) delve into the CVaR measure for the whole trajectory's reward distribution in standard RL. Further, Du et al. (2022) propose ICVaR-RL, a nested risk-aware RL formulation that addresses both regret minimization and best policy identification. Additionally, the work of Chen et al. (2023) presents an advancement in the form of a nested CVaR measure within the framework of RLHF. The limitation of this work lies in the selection of a random reference trajectory for comparison, causing an unavoidable linear strong nested CVaR regret. Consequently, we are left with only a preference equation from which we are unable to compute the state-action reward function for each step.

Practical and relevant trajectory or state-action embeddings are described in works such as (Pacchiano et al., 2021). Therefore, the one-episode-reward might not even be sum-decomposable (the trajectory embedding details can be seen in sec.3.1 ). Compared to previous work, we use non-Markovian reward models that do not require estimating the reward at each step and explore both nested and static risk-aware objectives, aiming to provide a more general method.

## 3 Problem Set-up and Preliminary Analysis

### PbRL MDP

We first define a modification of the classical Markov Decision Process (MDP) to account for risk: Risk Aware Preference-Based MDP (RA-PB-MDP). The standard MDP is described as a tuple, \((,,r_{}^{},^{},H)\), where \(\) and \(\) represent finite state and action spaces, and \(H\) denotes the length of episodes. Additionally, let \(S:=||\) and \(A:=||\) denote the cardinalities of the state and action spaces, respectively. \(^{}:( )\) is the transition kernel, where \(()\) denotes the space of probability measures on space \(\). A _trajectory_ is a sequence

\[_{h}_{h},=_{h=1}^{H}_{h} _{h}=()^{h-1} .\]

Intuitively, a trajectory encapsulates the interactions between an agent and the environment \(\) up to step \(h\). In contrast to the standard RL setting, where the reward function \(r_{h}^{}(s_{h},a_{h})\) specifies the reward at each step \(h\), a significant distinction in the PbRL MDP framework is that the reward function \(r^{}\) is defined as \(r_{}^{}(_{H}):()^{H-1}\), denoting the reward of the entire trajectory.

Reward model for the entire trajectory.For any trajectory \(_{H}\), we assume the existence of a trajectory embedding mapping \(:_{H}^{dim}\), and the reward of the entire trajectory is defined as the function: \(r_{}^{*}(_{H}):=(_{H}),_{}^{*}\). Here, \(dim_{}\) denotes the trajectory embedding dimension. Finally, we denote \((_{H})=(_{1}(_{H}),,_{dim_{}}(_{H}))\).

Assumption 3.1.: We assume that for all trajectories \(_{H}\) and for all \(d\{1,,dim_{}\}\), \(\|_{d}(_{H})\|\{0\}[b,B]\) where \(b,B>0\) are known. \(\|_{}^{*}\|_{w}\) and \(_{w}\) is known as well.

_Remark 3.2_.: We assume the map \(\) is known to the learner. The sum of the state-action reward used in Chen et al. (2023) is one case of such map, where \((_{H})=_{h=1}^{H}(s_{h},a_{h})\) and \(dim_{}=S A\). Therefore, for all \(d\{1,,dim_{}\}\), \(\|_{d}(_{H})\|\{0\}\{1,,H\}\)

_Remark 3.3_.: Assumption 3.1 implies that there is a gap between zero and some positive number \(b\) in the absolute values of components of trajectory embeddings. This is evident for finite-step discrete action spaces, where we can enumerate all trajectory embeddings to find the smallest non-zero component, satisfying most application scenarios.

At each iteration \(k[K]\), the agent selects two policies under a deterministic policy framework, \(_{1,k}\) and \(_{2,k}\), which generate two (randomized) trajectories \(_{H}^{1,k}\) and \(_{H}^{2,k}\). In PbRL, unlike standard RL where the agent receives rewards every step, the agent can only obtain the preference \(o_{k}\) between two trajectories \((_{H}^{1,k},_{H}^{2,k})\). By making a query, we obtain a preference feedback \(o_{k}\{0,1\}\) that is sampled from a Bernoulli distribution:

\[o_{k} Ber((r_{}^{*}(_{H}^{1,k})-r_{}^{ *}(_{H}^{2,k})))\] (1)

where \(:\) is a monotonically increasing link function. We assume \(\) is known like the popular Bradley-Terry model (Hunter, 2004), wherein \(\) is represented by the logistic function. It is known that we can not estimate the trajectory reward without a known \(\). Also, we assume \(\) is Lipschitz continuous and \(\) is its Lipschitz coefficient.

History dependent policy.Since the algorithm can only observe the reward for an entire episode until the end, it cannot make decisions based solely on the current state. The agent cannot observe the individual reward \(r_{h}(s,a)\) and thus cannot compute the target value at each step. To circumvent this challenge, the algorithm should take action according to a history-dependent policy. A history-dependent policy \(=\{_{h}\}_{h[H]}\) is defined as a sequence of mappings \(_{h}:_{h}\), providing the agent with guidance to select an action, given a trajectory \(_{h}_{h}\) at time step \(h\). For notation convenience, let \(\) denote the set of all history-dependent deterministic policies.

### Risk Measure

Because in PbRL we can only estimate the reward for the entire trajectory, the risk measure selected for PbRL must rely solely on the reward of the entire trajectory. That is, two trajectories with the same trajectory reward should contribute equally to the risk measure, even if their potential rewards at each step are different. Unlike Chen et al. (2023) that decomposes the reward at each step (where the solution is likely not unique) and then calculates the risk measure, this requirement ensures that the risk measure consistently and holistically reflects the underlying preference. We refer to risk measures that suitable for PbRL problems as _PbRL-risk-measures_. Here, we introduce two different risk-measures: nested and static quantile risk-aware measures, which are appropriate for PbRL-MDPs.

We first introduce the definition of quantile function and risk-aware objective. The quantile function of a random variable \(X\) is \(F_{X}^{}()=\{x F_{X}(x)\}\). We assume \(F_{X}\) is strictly monotone, so it is invertible and we have \(F_{X}^{}()=F_{X}^{-1}()\). The risk-aware objective is given by the Riemann-Stieljes integral:

\[(X)=_{0}^{1}F_{X}^{}()G()\] (2)

where \(X\) is the random variable encoding the value of MDP, and \(G\) is a weighting function over the quantiles. This class captures a broad range of useful objectives, including the popular CVaR objective (Bastani et al., 2022).

_Remark 3.4_.: (\(\)-CVaR objective) Specifically, in \(\)-CVaR,

\[G()=&<,\\ 1&.\]

and \((X)\) becomes

\[(X)=_{0}^{}F_{X}^{-1}().\]

**Assumption 3.5**.: \(G\) is \(L_{G}\)-Lipschitz continuous for some \(L_{G}_{>0}\), and \(G(0)=0,G(1)=1\).

For example, for the \(\)-CVaR objective, we have \(L_{G}=1/\).

There are two prevalent approaches to Risk-aware-MDPs: _nested (or iterated)_ (such as Iterated CVaR (ICVAR) [Du et al., 2022] and Risk-Sensitive Value Iteration (RSVI) [Fei et al., 2020]), and _static_ (referenced in [Bastani et al., 2022, Wu and Xu, 2023]). MDPs characterized by an iterated risk-aware objective facilitate a value function and uphold a Bellman-type recursion.

**Nested PbRL-risk-measures.** For standard RL's MDP, the nested quantile risk-aware measure can be elucidated in Bellman equation type as follows:

\[Q_{h}^{}(s,a)&=r_{h}^{}(s,a)+(V_{h+1}^{}( s^{}),s^{}^{}(s,a))\\ V_{h}^{}(s)&=Q_{h}^{}(s,_{h}(s))\\ V_{+1}^{}(s)&=0, s\] (3)

Here \(r_{h}^{}(s,a)\) denotes the decomposed state-action reward in step \(h\).

For the PbRL-MDP setting \((,,r_{}^{},^{},H)\), the state-action's reward might not be calculated or the reward of the entire trajectory might not be decomposed. Therefore, the policy should be history-dependent. We rewrite the nested quantile objective's Bellman equation with the embedded trajectory reward as follows:

\[_{h}^{}(_{h},a)&=(_{h+1}^{} (s^{}(_{h},a)),s^{}^{}(s,a) ),\\ _{h}^{}(_{h})&=_{h}^{}(_{h},_{h}(_{h })),\\ V_{}^{}(_{H})&=r^{}(_{H}),\] (4)

For any PbRL MDP \((,,r_{},,H)\), we use \(_{h}^{,r_{},}(_{h})\) to denote the value iteration under the policy \(\), where \(\) is a history dependent policy.

**Lemma 3.6**.: _For a given tabular MDP, the reward on the entire trajectory can be decomposed as \(r_{}^{}(_{H})=_{h=1}^{H}r_{h}^{}(s_{h},a_{ h})\), \(V_{1}^{}\) in Eq. 3 and \(_{1}^{}\) in Eq. 4 are equivalent._

The proof is detailed in Appendix B.1 due to space limitations.

Static PbRL-risk-measures.Standard MDPs with a static risk aware objective [Bellemare et al., 2017, Dabney et al., 2018] can be written in the distributional Bellman equation as follows:

\[& Z_{h}^{()}(_{h})=_{h^{ }=h}^{H}r_{h^{}}^{}(s_{h^{}},a_{h^{}}),_{H} (_{h}(_{H})=_{h})\\ & F_{Z_{h}^{()}()}(x)=_{s^{}}P(s^{ } S(),_{h}())F_{Z_{h+1}^{()}((s^{},_{ h}()))}(x-r_{h}^{}(s_{h},a_{h}))\\ & V_{1}^{}(s)=_{0}^{1}F_{Z_{1}}^{}()() dG ()\] (5)

Where \(S()=s\) for \(=(,s)\) is the current state in trajectory \(\), \(_{h}(_{H})=(s_{1},a_{1},s_{2},a_{2},,s_{h-1},a_{h-1},s_{h})\) denotes the first h steps' trajectory. \(Z_{h}^{()}(_{h})\) denoted the reward from step \(t\) given the current history. The _static reward_ of \(\) is \(Z_{1}^{()}()\), where \(=(s)_{1}\) for \(s D\) is the initial history.

Also, we modify the distributional Bellman equation for PbRL MDP \((,,r_{}^{},^{},H)\) settings as follows:

\[& Z_{h}^{()}(_{h})=r_{}^{}( _{H}),_{H}(_{h}(_{H})=_{h} )\\ & F_{Z_{h}^{()}(_{h})}(x)=_{s^{}}P(s ^{} S(),_{h}())F_{Z_{h+1}^{()}((s^{},_{ h}()))}(x)\\ &_{1}^{}(s)=_{0}^{1}F_{Z_{1}}^{}()( ) dG()\] (6)

**Lemma 3.7**.: _For a tabular MDP and a reward of the entire trajectory can be decomposed as \(r_{}^{}(_{H})=_{h=1}^{H}r_{h}^{}(s_{h},a_{ h})\), \(V_{1}^{}\) in Eq. 5 and \(_{1}^{}\) in Eq. 6 are equivalent._

The proof is detailed in Appendix B.2 due to space limitation.

Each of these risk measures possesses distinct advantages and limitations. Nested risk measures, which incorporate a Bellman-type recursion, can directly employ techniques such as the Dynamic Programming Principle (DPP) for computation. However, they are challenging to interpret and are not law-invariant (Hau et al., 2023). On the other hand, static risk measures are straightforward to interpret, but the resulting optimal policy may not remain Markovian and becomes history-dependent. Consequently, techniques such as the DPP and the Bellman equation become inapplicable.

### Objective

We define an optimal policy as:

\[^{}*{argmax}_{}_{1}^{}(s_{1})\] (7)

i.e., it maximizes the given objective for \(\). \(_{1}^{}(s_{1})\) will be decided by the selected risk measure, where value iteration calculated using Eq. 4 and static calculated using Eq. 6.

**Assumption 3.8**.: Regardless of nested or static CVaR objectives, we are given an algorithm for computing \(_{}^{}\) for a known \(\)-\(\)\(\).

A formal proof of Assumption 3.8 is given in Appendix F. When unambiguous, we drop \(\) and simply write \(^{}\).

At the beginning of each episode \(k[K]\), our algorithm \(\) chooses two policies \((_{1,k},_{2,k})=(H_{k})\), where \(H_{k}=\{_{1,k^{},H},_{2,k^{},H},o_{k}\}_{k^{ }=0}^{k}\) is the random set of episodes observed so far. Then, our goal is to design an algorithm \(\) that minimizes regret, which is naturally defined as:

\[(K):=_{k=0}^{K}(2_{1}^{^{}}(s_{1 })-_{1}^{_{1,k}}(s_{1})-_{1}^{_{2,k}} (s_{1}))\] (8)

## 4 Risk Aware Preference based RL Algorithm

In this section, we introduce and analyze an algorithm called RA-PbRL for solving the PbRL problem with both nested and static risk aware objectives. Also, we establish a regret bound for it.

### Algorithm

RA-PbRL is formally described in Algorithm 1. The development of RA-PbRL is primarily inspired by the PbOP algorithm, as delineated in Chen et al. (2022), which was originally proposed for risk-neutral PbRL environments. Building upon this foundation, one significant difference is how to choose a risk aware policy in estimated PbRL MDP, where the value iteration is different. We also use novel techniques to estimate the confidence set and explore for a policy, instead of using a bonus (Chen et al., 2022) (which is difficult to calculate in risk-aware problems) as in standard RL.

```
0: episode \(K\), step \(H\), initial state space \(\), initial reward space \(\), risk level \(\), confidence parameter \(\)
1: Set \(_{0}^{}=,_{0}^{r}=\), Execute two arbitrary policies \(_{1,0}\) and \(_{2,0}\) for one episode, respectively, and then observe the trajectory \(_{1,0}\) and \(_{2,0}\) and the preference \(o_{0}\).
2:for\(k=1 K\)do
3: Calculate the probability estimation \(}_{k}\): \(}_{k}=_{}_{i=1}^{2}_{ k^{}=0}^{k-1}_{h=1}^{H}|(s_{i,k^{},h},a_{i,k^{ },h}),(s_{i,k^{},h+1})|^{2}\).
4: Update transition confidence set : \[_{k}^{}=\{^{}_{s^{} }|}^{}(s^{} s,a)- ^{}(s^{} s,a)|)}{n_{k}(s,a)}}\}_ {k-1}^{}\]
5: Calculate the reward estimation: \(_{k}()=_{r}_{k=0}^{k-1}((r( _{1,k^{}})-r(_{2,k^{}}))-o_{k^{}})^{2}\)
6: Update the confidence set: \(_{k}^{r}=\{r^{}()|_{k^{}=0}^{k-1} [(_{k}(_{1,k^{}})-_{k}(_{2,k^{}}) )-(r^{}(_{1,k^{}})-r^{}(_{2,k^{}})) ]^{2}_{r,k}()\}_{k-1}^{r}\)
7: Update policy confidence set: \(_{k}=\{_{_{}_{k}^{},_{k}^{}}(_{1,_{},}^{ }(s_{1})-_{1,_{},}^{^{}}(s_{1}))  0,^{}\}_{k-1}\)
8: Compute \((_{1,k},_{2,k})\): \((_{1,k},_{2,k})=*{arg\,max}_{_{1},_{2}_{k}} _{_{}_{k}^{},_{k}^{ }}(_{1,_{},}^{_{1}}(s_{1})-_{1,_{},}^{_{2}}(s_{1}))\)
9: Observe the trajectory \(_{1,k,H},_{2,k,H}\), and the preference \(o_{k}\)
10: Calculate the state-action visiting time before episode \(k\): \(n_{k}(s,a)\)
11:endfor ```

**Algorithm 1** RA-PbRL

The overview of the algorithm.Now we introduce the main part of our algorithm. In line 1, we initialize the transition kernel function and reward function confidence set, and execute two arbitrary policies at first. For every episode, we observe history samples and accordingly estimate the transition kernel function (line 3) and update its confidence set (line 4) as well as the reward function (line 5) and its confidence set (line 6). Both estimation and calculation used the standard least-squares regression. Based on the confidence sets, we maintain a policy set in which all policies are near-optimal with minor sub-optimality gap with high probability in line 7. In line 8, we execute the most exploratory policy pair in the policy set and observe the preference between the trajectories sampled using these two policies.

The key difference between nested and static objective.The estimation of the transition kernel (line 4 in Algorithm 1) and the construction of confidence set (line 6 in Algorithm 1) are similar for both nested and static objectives. The difference lies in the value iteration, which is defined in Eq. 4 for nested objective and Eq. 6 for static objective. The bounds for regrets are different since the estimation error's impact is different as we are going to show below.

### Analysis

**Theorem 4.1** ( Nested object regret upper bound).: _With at least probability \(1-\), the nested quantile risk aware object regret of RA-PBRL is bounded by:_

\[_{}(K)\] \[ (L_{G}H^{}SA()(s)>0}_{ ,h}(s)}})\] \[+ (dim_{}}}{})( )}{})}_{dim,}(d)})\] (9)_Where \(w_{,h}(s)\) denotes the probability of visiting state-action pair at \(h\) th step with policy \(\) and \(_{,d}_{dim,}(d)\) denotes probability of trajectory \(_{H}\)'s d th feature \(_{d}(_{H}) 0\) with the policy \(\)._

The proof of this theorem is provided in Appendix D.20. The first term of the regret arises from the estimation error of the transition kernel, primarily dominated by \(_{,h,s:w_{,h}(s)>0}w_{,h}(s)\). The second term is due to the estimation error of the trajectory reward weights, significantly impacted by \(min_{,d}_{}(d)\). In fact, these factors are unavoidable in the lower bound in certain challenging cases. Thus, they characterize the inherent problem difficulty, i.e., in achieving the nested risk-aware objective, the agent will be highly sensitive to some state-actions or features that are difficult to observe and require substantial effort to explore. This may result in inefficiency in many scenarios.

**Theorem 4.2** (**Static object regret upper bound)**.: _The static quantile risk aware object regret of RA-PBRL is bounded by:_

\[&_{static}(K)\\ &(L_{G}S^{2}AH^{} log(K/))+(L_{G}dim_{} )()}{} )})\] (10)

The proof of this theorem is provided in Appendix D.21. Notice that the regret for both the nested risk objective and the static risk objective of Algorithm 1 are sublinear with respect to \(K\), making RA-PBRL the first provably efficient algorithm with one-episode-reward for these two objectives. Additionally, compared to Chen et al. , we achieve the goal of having both policies gradually approach optimality. Moreover, in comparison to the nested risk-aware objective, the static objective focuses on the risk measure of the entire distribution, primarily influenced by the Lipschitz coefficient \(L_{G}\) of the quantile function and is less constrained by certain specific cases.

**Theorem 4.3** (**Nested object regret lower bound)**.: _The nested quantile risk aware object regret of RA-PBRL is bounded by:_

\[(K)(\{B_{w}(s)>0}w_{,h}(s,a)}},B _{dim,}(d)}}\})\] (11)

We provide our proof in E.1 by two hard-to-learn constructions. By the two instances, we show that the two factors, \(_{,h,s:p_{,h}(s)>0}w_{,h}(s)\), \(_{,d}_{dim,}(d)\), are unavoidable in some cases.

**Theorem 4.4** (**Static object regret lower bound)**.: _The static quantile risk aware object regret of RA-PBRL is bounded by:_

\[(K)(S^{2}A+dim_{})\]

The proof of this theorem is similar to Theorem 4.5 in Chen et al. .

## 5 Experiment Results

In this section, we assess the empirical performance of RA-PbRL (Algorithm 1). For a comparative analysis, we select two baseline algorithms: PbOP, as described in Chen et al. , which is a PbRL algorithm utilizing general function approximation, and ICVaR-RLHF, detailed in Chen et al. , which is a risk-sensitive Human Feedback RL algorithm. These baselines represent the most closely aligned algorithms with RA-PbRL, especially in terms of employing general function approximation. The evaluation of empirical performance is conducted through the lens of static regret, as defined in Eq. 8.

### Experiment settings: MDP

In our experimental framework, we configure a straightforward tabular MDP characterized by finite steps \(H=6\), finite actions \(A=3\), state space \(S=4\), and risk levels \(\{0.05,0.10,0.20,0.40\}\). For each configuration and algorithms, we perform 50 independent trials and report the mean regret across these trials, along with 95% confidence intervals. The outcomes are depicted in Figures 1 and 3, where the solid lines represent the empirical means obtained from the experiments, and the width of the shaded regions indicates the standard deviation of the experiments.

[MISSING_PAGE_EMPTY:9]

risk aversion (\( 0\)) introduces greater uncertainty, as evidenced by the larger variance regions observed in the experiments. Notably, the regret of the bad-scenarios associated with RA-PbRL is significantly lower compared to those of other algorithms. It can additionally be observed that as \(\) is increased, the regret improves, which is expected as riskier behavior can also improve the odds of finding useful behaviors.

## 6 Conclusion

In this paper, we investigate a novel PbRL algorithm for solving problems requiring risk awareness. We explore static and nested measures to introduce risk awareness to PbRL settings. To the best of our knowledge, our proposed RA-PbRL algorithm is the first provably efficient Preference-based Reinforcement Learning (PbRL) algorithm that incorporates both nested and static risk objectives in one algorithm. Our algorithm is built on innovative techniques for the efficient approximation of regret. A core finding in our investigation is the strong influence of the state and trajectory dimensions with respect to the nested risk objective regret. On the other hand, the static risk objective regret is mainly determined by the quantile function.

We have also identified the following four limitations to our work. (1) Our comparison feedback is limited to two trajectories. An interesting, more general approach could consider n-wise comparisons. (2) The reward functions are assumed to be linear in this work for the sake of simplicity. (3) Although this work has considered more general risk measures, we have still made certain assumptions that limit the generality of our results. (4) There is still room for future improvements to further close the gap between upper and lower bounds. We believe this work opens several avenues for future research, including establishing the concrete lower bounds of risk-aware PbRL, improving the computational complexity of the algorithm, and conducting experiments in more diverse and interesting environments/simulations.