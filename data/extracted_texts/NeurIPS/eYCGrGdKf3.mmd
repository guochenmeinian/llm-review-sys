# Unleash the Potential of Image Branch for

Cross-modal 3D Object Detection

 Yifan Zhang\({}^{1}\), Oijian Zhang\({}^{1}\), Junhui Hou\({}^{1}\), Yixuan Yuan\({}^{2*}\), and Guoliang Xing\({}^{2}\)

\({}^{1}\)City University of Hong Kong, \({}^{2}\)The Chinese University of Hong Kong

{yzhang3362-c,qijizhang3-c}@my.cityu.edu.hk;jh.hou@cityu.edu.hk;

yxyuan@ee.cuhk.edu.hk;glxing@ie.cuhk.edu.hk

Corresponding author.

###### Abstract

To achieve reliable and precise scene understanding, autonomous vehicles typically incorporate multiple sensing modalities to capitalize on their complementary attributes. However, existing cross-modal 3D detectors do not fully utilize the image domain information to address the bottleneck issues of the LiDAR-based detectors. This paper presents a new cross-modal 3D object detector, namely UPIDet, which aims to **u**nleash the **p**otential of the **i**mage branch from two aspects. First, UPIDet introduces a new 2D auxiliary task called normalized local coordinate map estimation. This approach enables the learning of local spatial-aware features from the image modality to supplement sparse point clouds. Second, we discover that the representational capability of the point cloud backbone can be enhanced through the gradients backpropagated from the training objectives of the image branch, utilizing a succinct and effective point-to-pixel module. Extensive experiments and ablation studies validate the effectiveness of our method. Notably, we achieved the top rank in the highly competitive cyclist class of the KITTI benchmark at the time of submission. The source code is available at https://github.com/Eaphan/UPIDet.

## 1 Introduction

In recent years, there has been increasing attention from both academia and industry towards 3D object detection, particularly in the context of autonomous driving scenarios . Two dominant data modalities - 3D point clouds and 2D RGB images - demonstrate complementary properties, with point clouds encoding accurate structure and depth cues but suffering from sparsity, incompleteness, and non-uniformity. On the other hand, RGB images convey rich semantic features and has already developed powerful learning architectures , but pose challenges in reliably modeling spatial structures. Despite the efforts made by previous studies  devoted to cross-modal learning, how to reasonably mitigate the substantial gap between two modalities and utilize the information from the image domain to complement single-modal detectors remains an open and significant issue.

Currently, the performance of single-modal detectors is limited by the sparsity of observed point clouds. As shown in Figure 1 (b), the performance of detectors on objects with fewer points (LEVEL_2) is significantly lower than that on objects with denser points (LEVEL_1). And Fig.1 (a) shows the distribution of point clouds in a single object for the Waymo dataset. It can be observed that a substantial proportion of objects have less than 50 point clouds, making it challenging to deduce the 3D bounding boxes with seven degrees of freedom from very sparse point clouds, as discussed in detail in Sec.3. While the observed point cloud can be sparse due to heavy occlusion or low reflectivity material, their contour and appearance can still be clear in RGB images (as shown in the example in Fig.1 (c)). Despite previous work exploring the use of semantic image features to enhance features extracted from LiDAR , they have not fundamentally improved the spatialrepresentation limited by the sparsity of the point cloud. There is a class of work that attempts to generate pseudo LiDAR points as a supplement to sparse point clouds but these models commonly rely on depth estimators trained on additional data .

On the other hand, the performance of LiDAR-based detection frameworks is limited by the inherent challenges of processing _irregular_ and _unordered_ point clouds, which makes it difficult for neural network backbones to learn effectively from 3D LiDAR data. Consequently, the representation capability of these backbone networks is still relatively insufficient, despite the performance boost achieved by previous cross-modal methods[12; 40] that incorporate discriminative 2D semantic information into the 3D detection pipeline. However, there is no evidence that these methods actually enhance the representation capability of the 3D LiDAR backbone network, which is also one of the most critical influencing factors.

This paper addresses the first issue mentioned above by leveraging local-spatial aware information from RGB images. As depicted in Fig. 1 (c), predicting the 3D bounding box of a reflective black car from the sparse observed point cloud is challenging. By incorporating image information, we can not only distinguish the foreground points but also determine the relative position of the point cloud within the object, given the known 3D-to-2D correspondence. To learn local spatial-aware feature representations, we introduce a new 2D auxiliary task called normalized local coordinate (NLC) map estimation, which provides the relative position of each pixel inside the object. Ultimately, we expect to improve the performance of the cross-modal detector by utilizing both the known _global_ coordinates of points and their estimated _relative_ position within an object.

On the other hand, the depth of point cloud serves as a complementary source to RGB signals, providing 3D geometric information that is resilient to lighting changes and aids in distinguishing between various objects. In light of this, we present a succinct point-to-pixel feature propagation module that allows 3D geometric features extracted from LiDAR point clouds to flow into the 2D image learning branch. Unlike the feature-level enhancements brought by existing pixel-to-point propagation, this method enhances the representation ability of 3D LiDAR backbone networks. Remarkably, this is an interesting observation because the back-propagated gradients from the 2D image branch built upon established 2D convolutional neural networks (CNNs) with impressive learning ability can effectively boost the 3D LiDAR branch.

We conduct extensive experiments on the prevailing KITTI  benchmark dataset. Compared with state-of-the-art single-modal and cross-modal 3D object detectors, our framework achieves remarkable performance improvement. Notably, our method ranks \(^{}\) on the cyclist class of KITTI 3D detection benchmark2. Conclusively, our main contributions are three-fold:

* We demonstrate that the representational capability of the point cloud backbone can be enhanced through the gradients backpropagated from the training objectives of the image branch, utilizing a succinct and effective point-to-pixel module.
* We reveal the importance of specific 2D auxiliary tasks used for training the 2D image learning branch which has not received enough attention in previous works, and introduce 2D NLC

Figure 1: (a) Distribution of number of points in a single object in Waymo Dataset.(b) Performance of the Second on Waymo . (c) Point cloud and image of a black car and part of their correspondence.

map estimation to facilitate learning spatial-aware features and implicitly boost the overall 3D detection performance.
* For the first time, we provide a rigorous analysis of the performance bottlenecks of single-modal detectors from the perspective of degrees of freedom (DOF) of the 3D bounding box.

## 2 Related Work

**LiDAR-based 3D Object Detection** could be roughly divided into two categories. _1) Voxel-based 3D detectors_ typically voxelize the point clouds into grid-structure forms of a fixed size .  introduced a more efficient sparse convolution to accelerate training and inference. _2) Point-based 3D detectors_ consume the raw 3D point clouds directly and generate predictions based on (downsampled) points.  applied a point-based feature extractor and generated high-quality proposals on foreground points. 3DSSD  adopted a new sampling strategy named F-FPS as a supplement to D-FPS to preserve enough interior points of foreground instances. It also built a one-stage anchor-free 3D object detector based on feasible representative points.  and  introduced semantic-aware down-sampling strategies to preserve foreground points as much as possible.  proposed to encode the point cloud by a fixed-radius near-neighbor graph.

**Camera-based** 3D Object Detection. Early works [25; 24] designed monocular 3D detectors by referring to 2D detectors [31; 39] and utilizing 2D-3D geometric constraints. Another way is to convert images to pseudo-lidar representations via monocular depth estimation and then resort to LiDAR-based methods .  built dense 2D-3D correspondence by predicting normalized object coordinate map  and adopted uncertainty-driven PnP to estimate object location. Our proposed NLC map estimation differs in that it aims to extract local spatial features from image features and does not rely on labeled dense depth maps and estimated RoI boxes. Recently, multi-view detectors like BEVStereo  have also achieved promising performance, getting closer to LiDAR-based methods.

**Cross-Modal 3D Object Detection** can be roughly divided into four categories: proposal-level, result-level, point-level, and pseudo-lidar-based approaches. _Proposal-level_ fusion methods [5; 14; 19] adopted a two-stage framework to fuse image features and point cloud features corresponding to the same anchor or proposal. For _result-level_ fusion,  exploited the geometric and semantic consistencies of 2D detection results and 3D detection results to improve single-modal detectors. Obviously, both proposal-level and decision-level fusion strategies are coarse and do not make full use of correspondence between LiDAR points and images. _Pseudo-LiDAR_ based methods [52; 45; 19] employed depth estimation/completion and converted the estimated depth map to pseudo-LiDAR points to complement raw sparse point clouds. However, such methods require extra depth map annotations that are high-cost. Regarding _point-level_ fusion methods,  proposed the continuous fusion layer to retrieve corresponding image features of nearest 3D points for each grid in BEV feature map. Especially,  and  retrieved the semantic scores or features by projecting points to image plane. However, neither semantic segmentation nor 2D detection tasks can enforce the network to learn 3D-spatial-aware image features. By contrast, we introduce NLC Map estimation as an auxiliary task to promote the image branch to learn local spatial information to supplement the sparse point cloud. Besides,  proposed TransFusion, a transformer-based detector that performs fine-grained fusion with attention mechanisms on the BEV level.  and  estimated the depth of multi-view images and transformed the camera feature maps into the BEV space before fusing them with LiDAR BEV features.

## 3 Preliminary

This section introduces the definition of the NLC map and analyzes the performance bottlenecks of single-modal detectors in terms of degrees of freedom (DOF). The goal of the cross-modal 3D detector is to estimate the bounding box parameters, including the object dimensions (\(w\), \(l\), \(h\)), center location \((x_{c},y_{c},z_{c})\), and orientation angle \(\). The input consists of an RGB image \(^{3 H W}\), where \(H\) and \(W\) denote the height and width of image, respectively. Additionally, the input includes a 3D LiDAR point cloud \(_{i}|i=1,...,N\), where \(N\) is the number of points, and each point \(_{i}^{4}\) is defined by its 3D location \((x_{p},y_{p},z_{p})\) in the LiDAR coordinate system and the reflectance value \(\)

**Normalized Local Coordinate (NLC) System.** We define the NLC system as follows: we take the center of an object as the origin, align the \(x\)-axis towards the head direction of its ground-truth (GT) bounding box, and then normalize the local coordinates with respect to the size of the GT bounding box [7; 34]. Figure 2 (a) shows an example of the NLC system for a typical car. With the geometry relationship between the input RGB image and point cloud, we can project the NLCs to the 2D image plane and construct a 2D NLC map with three channels corresponding to the three spatial dimensions, as illustrated in Fig.2(b).

Specifically, to build the correspondence between the 2D and 3D modalities, we project the observed points from the 3D coordinate system to the 2D coordinate system on the image plane:

\[d_{c}[u\ v\ 1]^{T}=[][x_{p} \ y_{p}\ z_{p}\ 1]^{T},\] (1)

where \(u\), \(v\), \(d_{c}\) denote the corresponding coordinates and depth on the image plane, \(^{3 3}\) and \(^{3 1}\) denote the rotation matrix and translation matrix of the LiDAR relative to the camera, and \(^{3 3}\) is the camera intrinsic matrix.

Given the point cloud and the bounding box of an object, LiDAR coordinates of foreground points can be transformed into the NLC system by proper translation, rotation, and scaling, i.e.,

\[x_{p}^{NLC}\\ y_{p}^{NLC}\\ z_{p}^{NLC}=1/l&0&0\\ 0&1/w&0\\ 0&0&1/hx_{p}^{LCS}\\ y_{p}^{LCS}\\ z_{p}^{LCS}+0.5\\ 0.5\\ 0.5=1/l&0&0\\ 0&1/w&0\\ 0&0&1/hcos&-sin&0\\ sin&cos&0\\ 0&0&1x_{p}-x_{c}\\ y_{p}-y_{c}\\ z_{p}-z_{c}+0.5\\ 0.5\\ 0.5,\] (2)

where \((x_{p}^{NLC},y_{p}^{NLC},z_{p}^{NLC})\) and \((x_{p}^{LCS},y_{p}^{LCS},z_{p}^{LCS})\) denote the coordinates of points in NLC system and local coordinate system, respectively.

**Discussion.** Suppose that the bounding box is unknown, but the global coordinates and NLCs of points are known. We can build a set of equations to solve for the parameters of the box, namely \(x_{c},y_{c},z_{c},w,l,h,\). To solve for all seven parameters, we need at least seven points with known global coordinates and NLCs to construct the equations. However, as shown in Fig. 1, the number of points for some objects is too small to estimate the 3D box with 7 degrees of freedom. Moreover, it is challenging to infer NLCs of points with only LiDAR data for objects far away or with low reflectivity, as the point clouds are sparse. However, RGB images can still provide visible contours and appearances for these cases. Therefore, we propose to estimate the NLCs of observed points based on enhanced image features. We can then retrieve the estimated NLCs based on the 2D-3D correspondence. Finally, we expect the proposed cross-modal detector to achieve higher detection accuracy with estimated NLCs and known global coordinates.

## 4 Proposed Method

**Overview**. Architecturally, the processing pipeline for cross-modal 3D object detection includes an image branch and a point cloud branch, which learn feature representations from 2D RGB images and 3D LiDAR point clouds, respectively. Most existing methods only incorporate pixel-wise semantic cues from the 2D image branch into the 3D point cloud branch for feature fusion. However, we observe that point-based networks typically show insufficient learning ability due to the essential difficulties in processing irregular and unordered point cloud data modality . Therefore, we propose to boost the expressive power of the point cloud backbone network with the assistance of the 2D image branch. Our ultimate goal is not to design a more powerful point cloud backbone network structure, but to make full use of the available resources in this cross-modal application scenario.

As shown in Fig.3, in addition to pixel-to-point feature propagation explored by mainstream point-level fusion methods[18; 12], we employ point-to-pixel propagation allowing features to flow inversely from the point cloud branch to the image branch. We achieve bidirectional feature propagation in a multi-stage fashion. In this way, not only can image features propagated via the pixel-to-point module provide additional information, but gradients backpropagated from the training objectives of the image branch can boost the representation ability of the point cloud backbone. We also employ auxiliary tasks to train the pipeline, aiming to enforce the network to learn rich semantic and spatial

Figure 2: Illustration of the (a) NLC system and (b) 2D NLC map.

representations. In particular, we propose NLC map estimation to promote the image branch to learn spatial-aware features, providing a necessary complement to the sparse spatial representation extracted from point clouds, especially for distant or highly occluded cases. Appendix A.1 provides detailed network structures of the image and point cloud backbones.

### Bidirectional Feature Propagation

The proposed bidirectional feature propagation consists of a pixel-to-point module and a point-to-pixel module, which bridges the two learning branches that operate on RGB images and LiDAR point clouds. Functionally, the point-to-pixel module applies grid-level interpolation on point-wise features to produce a 2D feature map, while the pixel-to-point module retrieves the corresponding 2D image features by projecting 3D points to the 2D image plane.

In general, we partition the overall 2D and 3D branches into the same number of stages. This allows us to perform bidirectional feature propagation at each stage between the corresponding layers of the image and point cloud learning networks. Without loss of generality, we focus on a certain stage where we can acquire a 2D feature map \(^{C_{2D} H^{} W^{}}\) with \(C_{2D}\) channels and dimensions \(H^{} W^{}\) from the image branch and a set of \(C_{3D}\)-dimensional embedding vectors \(=\{_{i}^{C_{3D}}\}_{i=1}^{N^{}}\) of the corresponding \(N^{}\) 3D points.

**Point-to-Pixel Module.** As illustrated in Fig. 4, we design the point-to-pixel module to propagate geometric information from the 3D point cloud branch into the 2D image branch. Formally, we

Figure 4: Illustration of the proposed point-to-pixel module.

Figure 3: Flowchart of the proposed UPIDet for cross-modal 3D object detection. It contains two branches separately extracting features from input 2D RGB images and 3D point clouds. UPIDet is mainly featured with (1) the point-to-pixel module that enables gradient backpropagation from 2D branch to 3D branch and (2) the supervision task of NLC map estimation for promoting the image branch to exploit local spatial features.

aim to construct a 2D feature map \(^{P2I}^{C_{3D} H^{} W^{}}\) by querying from the acquired point-wise embeddings \(\). More specifically, for each pixel position \((r,c)\) over the targeted 2D grid of \(^{P2I}\) with dimensions \(H^{} W^{}\), we collect the corresponding geometric features of points that are projected inside the range of this pixel and then perform feature aggregation by avg-pooling \(()\):

\[^{P2I}_{r,c}=(_{j}|j=1,...,n),\] (3)

where \(n\) counts the number of points that can be projected inside the range of the pixel located at the \(r^{th}\) row and the \(c^{th}\) column. For empty pixel positions where no points are projected inside, we define its feature value as \(\).

After that, instead of directly feeding the "interpolated" 2D feature map \(^{P2I}\) into the subsequent 2D convolutional stage, we introduce a few additional refinement procedures for feature smoothness as described below:

\[^{fuse}=([(^{P2I}),]),\] (4)

where \([,]\) and \(()\) stand for feature channel concatenation and the convolution operation, respectively. The resulting \(^{fuse}\) is further fed into the subsequent layers of the 2D image branch.

**Pixel-to-Point Module.** This module propagates semantic information from the 2D image branch into the 3D point cloud branch. We project the corresponding \(N^{}\) 3D points onto the 2D image plane, obtaining their planar coordinates denoted as \(=\{_{i}^{2}\}_{i=1}^{N^{}}\). Considering that these 2D coordinates are continuously and irregularly distributed over the projection plane, we apply bilinear interpolation to compute exact image features at projected positions, deducing a set of point-wise embeddings \(^{I2P}=\{(_{i})^{C_{2D}}|i=1,...,N^{}\}\). Similar to our practice in the point-to-pixel module, the interpolated 3D point-wise features \(^{I2P}\) need to be refined by the following procedures:

\[^{fuse}=([(^{ I2P};_{1}),];_{2}),\] (5)

where \((;)\) denotes shared multi-layer perceptrons parameterized by learnable weights \(\), and the resulting \(^{fuse}\) further passes through the subsequent layers of the 3D point cloud branch.

_Remark_.: Unlike some prior work that introduces complicated bidirectional fusion mechanisms [16; 21; 23] to seek more interaction between the two modalities and finally obtain more comprehensive features, we explore a concise point-to-pixel propagation strategy from the perspective of directly improving the representation capability of the 3D LiDAR backbone network. We believe that a concise but effective point-to-pixel propagation strategy will better demonstrate that with feasible 2D auxiliary tasks, the gradients backpropagated from the training objectives of the image branch can boost the representation ability of the point cloud backbone.

### Auxiliary Tasks for Training

The semantic and spatial-aware information is essential for determining the category and boundary of objects. To promote the network to uncover such information from input data, we leverage auxiliary tasks when training the whole pipeline. For the point cloud branch, following SA-SSD , we introduce 3D semantic segmentation and center estimation to learn structure-aware features. For the image branch, we propose NLC map estimation in addition to 2D semantic segmentation.

**NLC Map Estimation.** Vision-based semantic information is useful for networks to distinguish foreground from background. However, existing detectors face a performance bottleneck due to limited localization accuracy in distant or highly occluded cases where the spatial structure is incomplete due to the sparsity of point clouds. To address this, we use NLC maps as supervision to learn the relative position of each pixel inside the object from the image. This auxiliary task can drive the image branch to learn local spatial-aware features that complement the sparse spatial representation extracted from point clouds. Besides, it can augment the representation ability of the point cloud branch.

_Remark_.: The proposed NLC map estimation shares the same objective with pseudo-LiDAR-based methods, which aim to enhance the spatial representation limited by the sparsity of point clouds. However, compared to learning a pseudo-LiDAR representation via depth completion, our NLC map estimation has several advantages: **1)** the _local_ NLC is easier to learn than the _global_ depth owing to its scale-invariant property at different distances; **2)** our NLC map estimation can be naturally incorporated and trained with the detection pipeline end-to-end, while it is non-trivial to optimize the depth estimator and LiDAR-based detector simultaneously in pseudo-LiDAR based methods although it can be implemented in a somewhat complex way ; and **3)** pseudo-LiDAR representations require ground-truth dense depth images as supervision, which may not be available in reality, whereas our method does not require such information.

**Semantic Segmentation.** Leveraging semantics of 3D point clouds has demonstrated effectiveness in point-based detectors, owing to the explicit preservation of foreground points at downsampling [55; 2]. Therefore, we introduce the auxiliary semantic segmentation tasks not only in the point cloud branch but also in the image branch to exploit richer semantic information. Besides, additional semantic features extracted from images can facilitate the network to distinguish true positive candidate boxes from false positives.

### Loss Function

The NLC map estimation task is optimized with the loss function defined as

\[L_{NLC}=}_{i}^{N}\|_{i}^{NLC}-}_{i}^ {NLC}\|_{H}_{_{i}},\] (6)

where \(_{i}\) is the \(i^{th}\) LiDAR point, \(N_{pos}\) is the number of foreground LiDAR points, \(\|\|_{H}\) is the Huber-loss, \(_{_{i}}\) indicates the loss is calculated only with foreground points, and \(_{i}^{NLC}\) and \(}_{i}^{NLC}\) are the NLCs of ground-truth and prediction at corresponding pixel for foreground points.

We use the standard cross-entropy loss to optimize both 2D and 3D semantic segmentation, denoted as \(L_{sem}^{2D}\) and \(L_{sem}^{3D}\) respectively. And the loss of center estimation is computed as

\[L_{ctr}=}_{i}^{N}\|_{i}-} _{i}\|_{H}_{_{i}},\] (7)

where \(_{i}\) is the target offsets from points to the corresponding object center, and \(}_{i}\) denotes the output of center estimation head.

Besides the auxiliary tasks, we define the loss of the RPN stage \(L_{rpn}\) following . In addition, we also adopt commonly used proposal refinement loss \(L_{rcnn}\) as defined in . In all, the loss for optimizing the overall pipeline in an end-to-end manner is written as

\[L_{total}=L_{rpn}+L_{rcnn}+_{1}L_{NLC}+_{2}L_{sem}^{2D}+ _{3}L_{sem}^{3D}+_{4}L_{ctr},\] (8)

where \(_{1}\), \(_{2}\), \(_{3}\), and \(_{4}\) are hyper-parameters that are empirically set to 1.

## 5 Experiments

### Experiment Settings

**Datasets and Metrics.** We conducted experiments on the prevailing KITTI benchmark dataset, which contains two modalities of 3D point clouds and 2D RGB images. Following previous works , we divided all training data into two subsets, i.e., 3712 samples for training and the rest 3769 for validation. Performance is evaluated by the Average Precision (AP) metric under IoU thresholds of 0.7, 0.5, and 0.5 for car, pedestrian, and cyclist categories, respectively. We computed APs with 40 sampling recall positions by default, instead of 11. For the 2D auxiliary task of semantic segmentation, we used the instance segmentation annotations as provided in . Besides, we also conducted experiments on the Waymo Open Dataset (WOD) , which can be found in Appendix A.3.

**Implementation Details.** For the image branch, we used ResNet18  as the backbone encoder, followed by a decoder composed of pyramid pooling module  and several upsampling layers to give the outputs of semantic segmentation and NLC map estimation. For the point cloud branch, we deployed a point-based network like  with extra FP layers and further applied semantic-guided farthest point sampling (S-FPS)  in SA layers. Thus, we implemented bidirectional feature propagation between the top of each SA or FP layer and their corresponding locations at the image branch. Please refer to Appendix A.1 for more details.

### Comparison with State-of-the-Art Methods

We submitted our results to the official KITTI website, and the results are compared in Table 1. Our UPIDet outperforms existing state-of-the-art methods on the KITTI test set by a remarkable margin, with an absolute increase of 2.1% mAP over the second best method EQ-PVRCNN. Notably, at the time of submission, we achieved the top rank on the KITTI 3D detection benchmark for the cyclist class. We observe that UPIDet exhibits consistent and more significant improvements on the "moderate" and "hard" levels, where objects are distant or highly occluded with sparse points. We attribute these performance gains to our bidirectional feature propagation mechanism, which more adequately exploits complementary information between multi-modalities, and the effectiveness of our proposed 2D auxiliary tasks (as verified in Sec.5.3). Furthermore, actual visual results (presented in Appendix A.6) demonstrate that UPIDet produces higher-quality 3D bounding boxes in various scenes.

### Ablation Study

We conducted comprehensive ablation studies to validate the effectiveness and explore the impacts of key modules involved in the overall learning framework.

**Effectiveness of Feature Propagation Strategies.** We performed detailed ablation studies on specific multi-modal feature exploitation and interaction strategies. We started by presenting a single-modal baseline (Table 2(a)) that only preserves the point cloud branch of our UPIDet for both training and inference. Based on our complete bidirectional propagation pipeline (Table2(d)), we explored another two variants as shown in (Table2(b)) and (Table2(c)), solely preserving the point-to-pixel and pixel-to-point feature propagation in our 3D detectors, respectively. Note that in Table2(b), the point-to-pixel feature flow was only enabled during training, and we detached the point-to-pixel module as well as the whole image branch during inference. Empirically, we can draw several conclusions that strongly support our preceding claims. _First_, the performance of Table 2(a) is the worst among all variants, which reveals the superiority of cross-modal learning. _Second_, combining Table2(a)

   Exp. \\  } &  &  &  &  &  \\    & & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\  (a) & Single-Modal & 91.92 & 85.22 & 82.98 & 68.82 & 61.47 & 56.39 & 91.93 & 74.56 & 69.58 & 75.88 \\ (b) & Point-to-pixel & 91.83 & 85.11 & 82.91 & 70.66 & 63.57 & 58.19 & 92.78 & 76.31 & 71.77 & 77.01 \\ (c) & Pixel-to-point & 92.07 & 85.79 & 82.95 & 69.46 & 65.53 & 59.56 & 94.39 & 75.24 & 72.23 & 77.47 \\ (d) & Bidirectional & 92.63 & 85.77 & 83.13 & 72.68 & 67.64 & 62.25 & 94.39 & 77.77 & 71.47 & 78.64 \\  

Table 2: Ablative experiments on different feature exploitation and propagation schemes.

   &  &  &  &  &  \\    & & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\  PointRCNN  & LiDAR & 86.96 & 75.64 & 70.70 & 47.98 & 39.37 & 36.01 & 74.96 & 58.82 & 52.53 & 60.33 \\ PointPillars  & LiDAR & 82.58 & 74.31 & 68.99 & 51.45 & 41.92 & 38.89 & 77.10 & 58.65 & 51.92 & 60.65 \\ TANet  & LiDAR & 84.39 & 75.94 & 68.82 & 53.72 & 43.44 & 40.49 & 75.70 & 59.44 & 52.53 & 61.71 \\ LA-SSD  & LiDAR & 88.34 & 80.13 & 75.04 & 45.61 & 39.03 & 35.60 & 78.35 & 61.94 & 55.50 & 62.29 \\ STD  & LiDAR & 87.95 & 79.71 & 75.09 & 53.29 & 42.47 & 38.35 & 78.69 & 61.59 & 55.50 & 63.60 \\ Point-GNN  & LiDAR & 88.33 & 79.47 & 72.29 & 51.92 & 43.77 & 40.14 & 78.60 & 63.48 & 57.08 & 63.90 \\ Part-\(A^{2}\) & LiDAR & 87.81 & 78.49 & 73.51 & 53.10 & 43.35 & 40.06 & 79.17 & 63.52 & 56.93 & 63.99 \\ PV-RCNN  & LiDAR & 90.25 & 81.43 & 76.82 & 52.17 & 43.29 & 40.29 & 78.60 & 63.71 & 57.65 & 64.91 \\
3DSSD  & LiDAR & 88.36 & 79.57 & 74.55 & 54.64 & 44.27 & 40.23 & 82.48 & 64.10 & 56.90 & 65.01 \\ HotSpNet  & LiDAR & 87.60 & 78.31 & 73.34 & 53.10 & 45.37 & 41.47 & 82.59 & 65.95 & 59.00 & 65.19 \\ PDV  & LiDAR & **90.43** & 81.86 & 77.36 & 47.80 & 40.56 & 38.46 & 67.81 & 60.46 & 65.31 \\  MV3D  & LiDAR=RGB & 74.97 & 63.63 & 54.00 & - & - & - & - & - & - & - \\ MMF  & LiDAR=RGB & 88.40 & 77.43 & 70.22 & - & - & - & - & - & - \\ AVOD-FPN  & LiDAR=RGB & 83.07 & 71.76 & 65.73 & 50.46 & 42.27 & 39.04 & 63.76 & 50.55 & 44.93 & 56.84 \\ F-PointNet  & LiDAR=RGB & 82.19 & 69.79 & 60.59 & 50.53 & 42.15 & 38.08 & 72.27 & 56.12 & 49.01 & 57.86 \\ PointPainting  & LiDAR=RGB & 82.11 & 71.70 & 67.08 & 50.32 & 40.97 & 37.84 & 77.63 & 63.78 & 55.89 & 60.81 \\ F-ConvNet  & LiDAR=RGB & 87.36 & 76.39 & 66.69 & 52.16 & 43.38 & 38.80 & 81.98 & 65.07 & 56.54 & 63.15 \\ CAT-Det  & LiDAR=RGB & 89.87 & 81.32 & 76.68 & 54.26 & 44.44 & 41.94 & 83.68 & 68.81 & 61.45 & 67.05 \\ EQ-PVRCNN  & LiDAR=RGB & 90.13 & 82.01 & 77.53 & **55.84** & 47.02 & 42.94 & 85.41 & 69.10 & 62.30 & 68.03 \\  UPIDet (Ours) & LiDAR=RGB & 89.13 & **82.97** & **80.05** & 55.59 & **48.77** & **46.12** & **86.74** & **74.32** & **67.45** & **70.13** \\  

Table 1: Performance comparisons on the KITTI test set, where the best and the second best results are highlighted in bold and underlined, respectively.

and Table2(b), the mAP is largely boosted from 75.88% to 77.01%. Considering that during the inference stage, these two variants have identical forms of input and network structure, the resulting improvement strongly indicates that the representation ability of the 3D LiDAR branch is indeed strengthened. In other words, the joint optimization of a mature CNN architecture with 2D auxiliary task supervision can guide the point cloud branch to learn more discriminative point features. _Third_, comparing Table2(b) and Table2(c) with Table2(d), we can verify the superiority of bidirectional propagation (78.64%) over the unidirectional schemes (77.01% and 77.47%). If we particularly pay attention to Table2(c) and Table2(d), we can conclude that our newly proposed point-to-pixel feature propagation direction further brings a 1.17% mAP increase based on the previously explored pixel-to-point paradigm.

**Effectiveness of Image Branch.** Comparing Table 3 (d) with Tables 3 (e)-(g), the performance is stably boosted from 75.88% to 78.64%, as the gradual addition of the 2D image branch and two auxiliary tasks including 2D NLC map estimation and semantic segmentation. These results indicate that the additional semantic and geometric features learned from the image modality are indeed effective supplements to the point cloud representation, leading to significantly improved 3D object detection performances.

**Effectiveness of Auxiliary Tasks.** Comparing Table 3 (a) with Tables 3 (b)-(d), the mAP is boosted by 0.70% when incorporating 3D semantic segmentation and 3D center estimation, and can be further improved by 2.76% after introducing 2D semantic segmentation and NLC map estimation (Tables 3 (d)-(g)). However, only integrating image features without 2D auxiliary tasks (comparing results of Tables 3 (d) and (e)) brings limited improvement of 0.5%. This observation shows that the 2D auxiliary tasks, especially the proposed 2D NLC map estimation, do enforce the image branch to learn complementary information for the detector.

**Performance of NLC Map Estimation.** The NLC map estimation task is introduced to guide the image branch to learn local spatial-aware features. We evaluated the predicted NLC of pixels containing at least one projected point with mean absolute error (MAE) for each object:

\[_{q}=}_{i}^{N}|q_{_{i}}^{NLC}- _{_{i}}^{NLC}|_{_{i}},q \{x,y,z\},\] (9)

where \(N_{obj}\) is the number of LiDAR points inside the ground-truth box of the object, \(_{_{i}}\) indicates that the evaluation is only calculated with foreground points inside the box, \(q_{_{i}}^{NLC}\) and \(_{_{i}}^{NLC}\) are the

Figure 5: Quantitative results of NLC map estimation on the KITTI val set.

  Exp. & LiDAR & 3D Seg & 3D Ctr & Img & 2D Seg & NLC & Car & Ped. & Cyc. & **mAP** \\  (a) & ✓ & & & & & & 87.40 & 59.02 & 79.11 & 75.18 \\ (b) & ✓ & ✓ & & & & & 86.75 & 59.90 & 78.57 & 75.07 \\ (c) & ✓ & & & ✓ & & & 86.56 & 61.06 & 78.58 & 75.40 \\ (d) & ✓ & ✓ & ✓ & & & & 86.71 & 62.23 & 78.68 & 75.88 \\ (e) & ✓ & ✓ & ✓ & ✓ & ✓ & & 86.91 & 63.23 & 79.00 & 76.38 \\ (f) & ✓ & ✓ & ✓ & ✓ & ✓ & & 87.10 & 64.52 & 79.94 & 77.18 \\ (g) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & 87.18 & 67.52 & 81.21 & 78.64 \\  

Table 3: Ablative experiments on key modules of our UPIDet. LiDAR: the point cloud branch trained without auxiliary tasks; Img: the 2D RGB image backbone; 3D Seg: 3D semantic segmentation; 3D Ctr: 3D center estimation; 2D Seg: 2D semantic segmentation; NLC: 2D NLC map estimation.

normalized local coordinates of the ground-truth and the prediction at the corresponding pixel for the foreground point. Finally, we obtained the mean value of \(_{q}\) for all instances, namely \(_{q}\). We report the metric over different difficulty levels for three categories on the KITTI validation set in Figure 5. We can observe that, for the car class, the \(\) error is only 0.0619, i.e., \( 6.19\) cm error per meter. For the challenging pedestrian and cyclist categories, the error becomes larger due to the smaller size and non-rigid shape.

## 6 Conclusion

We have introduced a novel cross-modal 3D detector named UPIDet that leverages complementary information from the image domain in two ways. First, we demonstrated that the representational power of the point cloud backbone can be enhanced through the gradients backpropagated from the training loss of the image branch, utilizing a succinct and effective point-to-pixel module. Second, we proposed NLC map estimation as an auxiliary task to facilitate the learning of local spatial-aware representation in addition to semantic features by the image branch. Our approach achieves state-of-the-art results on the KITTI detection benchmark. Furthermore, extensive ablative experiments demonstrate the robustness of UPIDet against sensor noises and its ability to generalize to LiDAR signals with fewer beams. The promising results also indicate the potential of joint training between 3D object detection and other 2D scene understanding tasks. We believe that our novel perspective will inspire further investigations into multi-task multi-modal learning for scene understanding in autonomous driving.