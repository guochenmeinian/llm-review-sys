# Discrete Dictionary-based Decomposition Layer

for Structured Representation Learning

 Taewon Park\({}^{1}\) Hyun-Chul Kim\({}^{1}\) Minho Lee\({}^{1,2}\)

\({}^{1}\)Kyungpook National University, South Korea

\({}^{2}\)ALI Co., Ltd., South Korea

ptw7998@gmail.com, hyunchul_kim@knu.ac.kr, mholee@gmail.com

###### Abstract

Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a **D**iscrete **D**ictionary-based **D**ecomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned discrete features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.1

## 1 Introduction

Compositional generalization, aiming at understanding unseen data by combining known concepts, is essential for neural networks to handle complex tasks . Tensor Product Representation (TPR) framework  facilitates this by embedding the symbolic structure of data within vector spaces, providing neural networks with compositional capabilities. Within this framework, individual objects are decomposed at the representation level into distinct symbolic components called _role-filler_ pairs2. The TPR framework encodes each object by taking a tensor product of its _role_ vector and _filler_ vector, represented as \(T=filler role\), and then superimposes them to represent multiple objects within a single representation. During decoding, the TPR framework retrieves specific _fillers_--essential for solving tasks--from the superimposed representation through matrix multiplication using an _unbinding operator_ correlated to a particular _role_, \(filler=T unbind\). This retrieved _filler_ is then utilized in downstream tasks. Based on this property, TPR-based neural networks have demonstrated significant generalization and applicability in fields such as associative reasoning , mathematical problem-solving , and natural language processing .

Despite their successes, the TPR-based approaches pose a significant challenge known as a decomposition problem , which refers to the difficulty of decomposing input data into TPR components, such as _roles_, _fillers_, and _unbinding operators_. Without accurate decomposition, TPR-based models fail to represent the symbolic structure of data, causing a decline in the performance of the TPR operations. Recently, inspired by an object-centric learning method , Park et al.  proposes an attention-based iterative decomposition (AID) module to address this issue. AID uses competitive attention to iteratively refine structured representations, thereby enhancing the systematic generalization of TPR-based models. However, it still struggles to generalize all possible combinations of known symbols in simple synthetic tasks. This failure is likely attributable to its insufficient mechanism for explicitly mapping input data to known symbolic features observed during training. Therefore, the decomposition module may need an additional mechanism to store observed symbolic features during training and utilize it to effectively decompose unseen combinatorial data of known symbols.

In another line of work, discrete representation learning has been explored to improve the efficiency, interpretability, and generalization capabilities of neural networks . This approach involves mapping continuous input data into discrete representations by finding the nearest features in a predefined codebook. The features within the codebook are learnable parameters, specifically trained to capture the latent features of data during training phase . Some researchers have applied discrete representation techniques to extract specific types of representations from unstructured data . Other researchers have integrated discrete symbolic embeddings within the TPR framework to improve its interpretability . However, these methods are designed for specific applications, such as question-answering and summarization tasks, making them difficult to integrate into other TPR-based models.

In this work, we propose a **D**iscrete **D**ictionary-based **D**ecomposition (D3) layer for structured representation learning within the TPR framework. D3 employs the discrete representations techniques to utilize prior knowledge acquired during training for decomposition operations. Inspired by prior discrete key-value architectures , D3 consists of multiple dictionaries, each comprising discrete, learnable key-value pairs. Unlike prior work, each dictionary of D3 is linked explicitly to individual TPR components, such as _role_, _filler_, and _unbinding operator_. This design allows each dictionary to capture and store the discrete features of its corresponding TPR components during training. D3 acts as a drop-in layer that maps input data into pre-learned discrete features for the decomposition of TPR components through a three-step process, as illustrated in Fig. 1. First, it generates multiple queries from the input data, with each query utilized for different TPR components. Next, it identifies the nearest codebook keys within each dictionary based on these queries. Finally, D3 generates structured TPR representations by aggregating the codebook values corresponding to these keys. Moreover, D3 can be seamlessly integrated into any TPR-based model by replacing the TPR component generation layer without requiring further modifications.

Figure 1: **Overview of D3. D3 generates structured TPR representations by mapping input data to the nearest pre-learned symbolic features stored within discrete, learnable dictionaries. Each dictionary is linked explicitly to specific TPR components, such as _roles_, _filler_, and _unbinding operators_. Notably, D3 uses a shared dictionary configuration between the _roles_ and _unbinding operators_. This figure illustrates, for example, that _role\({}_{1}\)_ and _unbind\({}_{1}\)_ share one dictionary, while _role\({}_{2}\)_ and _unbind\({}_{2}\)_ share another. \(T\) denotes a superimposed representation that represents multiple objects.**

Our main contributions are as follows.

* We propose a novel D3 layer to tackle the decomposition problem inherent in the TPR-based approaches. D3 leverages discrete, learnable dictionaries to enhance the decomposition capabilities of TPR-based models. By mapping input data to pre-learned discrete features stored within the dictionaries, D3 effectively generates structured TPR representations.
* We conduct extensive experiments across various systematic generalization tasks, including synthetic associative recall and text/visual question-answering tasks. Our experimental results show that D3 significantly enhances the generalization performance of TPR-based models, demonstrating its effectiveness on systematic generalization tasks.
* Our analyses show that D3 generates well-bound structured representations that are satisfactory for the requirements of the TPR framework, utilizing the discrete, learnable dictionaries.

## 2 Related Work

Decomposition Problem.Compositional generalization in neural networks, which allows for generalizing beyond training data, has been extensively studied [2; 13; 12; 16; 8; 6; 41]. One important capability for achieving this is a _segregation_, as discussed in Greff et al. , which enables the formation of meaningful representations from structured and unstructured data [3; 18]. TPR-based neural networks also rely on this capability to generate structured representations for TPR components such as _roles_, _fillers_, and _unbinding operators_. In the TPR framework, these structured representations must satisfy specific conditions to ensure accurate encoding and decoding. First, _roles_ need to be linearly independent to avoid _filler_ overlap. Second, the _unbinding operator_ must correlate with the corresponding _roles_ to accurately retrieve associated _fillers_. Recent work  has shown that existing TPR-based models often fail to generate structured representations that meet these conditions, undermining their symbolic operations. To address this, an attention-based decomposition module  has been introduced, but it still shows limited performance on synthetic tasks involving the decomposition of unseen combinatorial data. In this work, we address the decomposition problem within the TPR framework using a discrete dictionary-based method, advancing the research further.

Discrete Representation Learning.Discrete neural representation learning has introduced a codebook of discrete, learnable representations into neural networks . During training, each discrete representation captures underlying latent features by mapping continuous input data to the nearest features within the codebook, which are then used for downstream tasks. Recent work on object-centric learning has utilized discrete representations to extract specific types of features from unstructured data, leveraging latent features learned during training [11; 43]. Some researchers have proposed a separate key-value codebook for learning discrete representations, demonstrating its effectiveness in systematic generalization  and robustness against distributional shifts . Inspired by these findings, we develop a separate key-value-based discrete dictionary method to enhance the decomposition capabilities of TPR-based models. Other researchers have introduced a discrete symbolic embedding layer to improve the interpretability of TPR-based models, showing the feasibility of discrete representations in the TPR framework [21; 9]. However, their methods focus on encoding processes and specific tasks such as question-answering  and abstractive summarization . In contrast, our work addresses the decomposition problem in TPR-based approaches, and our D3 method is a drop-in solution that can be easily adapted to any TPR-based model.

Memory Network.Research on memory networks has focused on enhancing neural network capacity by integrating external memory [36; 4; 5; 24; 27; 41]. Memory-augmented neural networks store variable lengths of sequential data in this external memory and retrieve necessary information using various addressing methods [36; 5]. These writing and reading mechanisms share many similarities with our D3 approach. However, while memory networks store input features sequentially in their memory states as a continuous stream, D3 updates symbolic feature information through gradient descent into codebook parameters within dictionaries. This distinctive characteristic allows D3 to leverage the learned discrete features to decompose unseen data after training. In another work, Lample et al.  introduces a learnable key-value memory layer to improve the efficiency of the Transformer by replacing the feed-forward layer. Unlike their memory layer, D3 employs key-value pairs in dictionaries explicitly linked to individual TPR components, making it well-suited for the TPR framework.

Method

In this section, we explain how the D3 module generates structured representations of the TPR components using discrete, learnable dictionaries. We then introduce configurations of D3 and how it can be applied to our baseline models.

### Discrete Dictionary-based Decomposition module

D3 is a discrete dictionary-based drop-in layer designed to enhance the decomposition capabilities of TPR-based approaches. At every time step, D3 decomposes input data into TPR components, such as _roles_, _fillers_, and _unbinding operators_, by mapping input data to pre-learned symbolic features within dictionaries. These dictionaries consist of discrete, learnable codebook key-value pairs, denoted as \(\{^{j}\}_{j=1}^{N_{}}\) as shown in Eq. 1. Each dictionary \(^{j}\) is explicitly linked to a \(j\)-th TPR component, allowing it to learn the symbolic features required for generating the specific TPR component. This design also enables the generation of structured representations for different TPR components individually and in parallel.

\[^{j}:=\{(_{i}^{j},_{i}^{j})_{i}^{ j}^{D_{}},_{i}^{j}^{D_{}}\}_{i=1}^{N_{}}\;\;j=1,...,N_{}\] (1)

where \(^{j}\) denotes the discrete, learnable dictionary for the \(j\)-th TPR component, \(\) denotes a learnable codebook key, and \(\) denotes a learnable codebook value. In the next paragraph, we describe how D3 generates TPR components using these dictionaries in three steps.

Step 1: Query Generation.At each time step \(t\), D3 takes input data, denoted as \(_{t}^{D_{}}\), and generates the query, denoted as \(_{t}^{N_{} D_{}}\), for each \(j\)-th TPR component using a query network, \(f^{j}_{}:_{t}_{t}^{j} ^{D_{}}\). The query network can be any neural network; in this study, we use a feed-forward network with a single layer. Additionally, we apply a layer normalization  and a dropout of \(p_{}\) to \(_{t}^{j}\).

Step 2: Sparse Key Access.D3 searches for the nearest keys from each dictionary, \(^{j}\), based on the generated \(_{t}^{j}\). We measure the similarity using the inner product between \(_{t}^{j}\) and \(\{_{i}^{j}\}_{i=1}^{N_{}}\). Then, D3 selects top-\(k\) codebook keys in order of largest similarity, as follows.

\[^{j}=_{k}(_{t}^{j}}_{i }^{j})\;\;}_{i}^{j}=_{i}^{j}/|| _{i}^{j}||_{2}\] (2)

where \(_{k}\) denotes the top-\(k\) operator that finds the indices of \(k\) largest values, and \(^{j}\) denotes the indices of the \(k\) most similar keys within \(^{j}\). We found that applying \(L2\) normalization to keys before the inner product mitigates the codebook collapse problem.

Step 3: Aggregation of Code Values.D3 computes the normalized score for selected codebook keys, denoted as \(w_{t}^{j}\), and aggregates codebook values corresponding to selected codebook keys with \(w_{t}^{j}\), as follows.

\[_{t}^{j}=_{i}w_{t,i}^{j}_{i}^{j} \;\;w_{t}^{j}=(_{t}^{j}}_{i}^{j}))_{i^{j}}\] (3)

Then, D3 maps \(_{t}^{j}\) to a dimension of \(D_{}\) and adds this projected vector to \(_{t}^{j}\). The summed vectors are mapped to a dimension of \(D_{}\) to generate structured representations of TPR components, as follows.

\[_{t}^{j}=_{t}^{j}+_{ }(_{t}^{j})^{D_{}}\] (4) \[}_{t}^{j}=_{}( _{t}^{j})^{D_{}}\] (5)

where \(_{}\) and \(_{}\) denote a feed-forward network with a single layer. Those \(}_{t}\) are then utilized for TPR operations to solve the downstream tasks.

### Module Configurations

In this section, we describe the configurations of D3 when applied to TPR-based models.

Shared Dictionary between Role and Unbinding Operator.As discussed in Section 2, _roles_ and _unbinding operators_ should have correlated features for accurate TPR operations. Considering this characteristic of the TPR framework, we share the dictionaries of _roles_ and _unbinding operators_. This shared dictionary also reduces the number of learnable parameters.

D3 Applied to Filler.While the TPR framework requires specific conditions for _roles_ and _unbinding operators_ for accurate TPR operations, there are no such requirements for _fillers_. Therefore, we explore two configurations in this study: applying D3 to generate _fillers_ (_w/ F_) and not applying D3 to generate fillers (_w/o F_). In the _w/o F_ configuration, we follow the baseline models to generate the _filler_ representations.

### Integration of D3 into Existing TPR-based Models

In this section, we introduce our baseline models and explain how D3 is applied to them, considering the configurations of D3. We use three TPR-based models as our baselines: FWM , TPR-RNN , and Linear Transformer . Notably, integrating D3 into these baseline models requires only substituting their TPR component generation layer with D3 without further modifications.

Fast Weight Memory.Fast Weight Memory (FWM)  is a TPR-based memory network designed for understanding long sequential contexts. It proposes a single word-level TPR operation related to the perceptron learning rule . It has shown significant associative reasoning capability in reinforcement learning and natural language processing tasks. FWM requires two types of _roles_ (_role\({}_{1}\)_ and _role\({}_{2}\)_) and one _filler_ for encoding, as well as two types of _unbinding operators_ (_unbind\({}_{1}\)_ and _unbind\({}_{2}\)_) for decoding. When D3 is integrated into FWM, it employs three dictionaries for the shared dictionary configuration: one for the _role\({}_{1}\)_ and _unbind\({}_{1}\)_, another for the _role\({}_{2}\)_ and _unbind\({}_{2}\)_, and the other for _filler_, as shown in Fig. 1.

TPR-RNN.TPR-RNN  is a sentence-level TPR-based memory network designed for basic text question-answering tasks . It incorporates various encoding operations such as writing, moving, and backlink to process sequential data at the sentence level. These operations necessitate different encoding components with varying dimensions, making direct connections to the decoding components challenging. As a result, we do not apply the shared dictionary configuration to TPR-RNN; instead, we use a shared query network without layer normalization. Furthermore, due to the differing dimensions of the TPR components in TPR-RNN, we employ distinct layer final layers for each TPR component.

Linear Transformer.Linear Transformer  linearizes the attention mechanism to improve the computational efficiency of the Transformer . Recently, Schlag et al.  demonstrated the equivalence between TPR and the linear attention mechanism, indicating that the key, value, and query in linear attention correspond to the _role_, _filler_, and _unbinding operator_, respectively. Building on this work, we apply D3 to generate the query, key, and value in the Linear Transformer. Unlike TPR-RNN and FWM, the Linear Transformer utilizes multi-head operations. Therefore, we use distinct dictionaries for each head, with the key and query of each head sharing the same dictionary.

## 4 Experiment

In this section, we evaluate the effectiveness of D3 across various tasks, including a synthetic task, text/visual question-answering tasks, and a language modeling task. To assess the decomposition capabilities, we follow the experimental settings of the AID , a prior work addressing the decomposition problem in the TPR framework, and closely compare our D3 model to baseline models and AID.

### Task

Systematic Associative Recall (SAR) task.This task evaluates systematic generalization in memorizing and recalling combinatorial data . It consists of a discovery phase and an inference phase. During the discovery phase, the model receives the combinatorial sequential items, each combining two symbols, \(x X\) and \(y Y\) where \(X=X_{1} X_{2} X_{3}\) and \(Y=Y_{1} Y_{2}\). The model is then required to predict an associated \(y\) when a specific \(x\) is presented. The SAR task uses different combination settings between training and evaluation to target systematic generalization specifically. During training, the model learns the following combination settings: (1) \(X_{1}\) and \(Y_{1}\), (2) \(X_{2}\) and \(Y_{2}\), and (3) \(X_{3}\) and \(Y\). At the evaluation, on the other hand, the model should generalize unseen combination settings, specifically \(X_{1}\) and \(Y_{2}\). Additionally, the task includes a hyper-parameter \(p=|}{|X_{2}|+|X_{3}|}\) where \(|X_{i}|\) denotes the cardinality of set \(X_{i}\). By adjusting \(p\), this task tests the systematic generalization of models under varying levels of exposure to different symbol combinations during training. In our study, we focus solely on the most challenging setting of the SAR task (\(p=0.0\)), where the subset \(X_{3}\) is excluded. In the SAR task, the TPR framework regards \(x\) as the _role_ and the _unbinding operator_, and \(y\) as the _filler_. Therefore, TPR-based models should systematically decompose the combinatorial data into structured representations by mapping \(x\) to the _role_ and \(y\) to the _filler_ during the discovery phase, and mapping \(x\) to the _unbinding operator_ during the inference phase to solve this task.

Systematic bAbI (sys-bAbI) task.This task is a variant of the bAbI task  designed to evaluate systematic generalization in text understanding and reasoning . It consists of 20 distinct sub-tasks, each comprising stories, relevant queries, and corresponding answers. The sys-bAbI task requires the models to remember the stories and predict corresponding answers to the queries. Unlike the original bAbI task, the sys-bAbI task evaluates the models with two aspects: (a) in-distribution (_w/o sys diff_) and (b) with the systematic difference (_w/ sys diff_) where each sub-task includes unseen words during training. Therefore, the models should learn task-independent text understanding to solve the sys-bAbI task.

Sort-of-CLEVR task.This task  evaluates compositional generalization in visual relational reasoning. It consists of scene images, queries, and corresponding answers. This task requires the models to understand the properties of individual objects (_Unary_) or the relationships between multiple objects (_Binary_ or _Ternary_) within visual scene images, and predict the correct answers to the queries . Therefore, the model should capture relationships within each object and between objects to solve this task.

WikiText-103 task.This task  is a language modeling dataset consisting of lengthy corpora from Wikipedia. Although the WikiText-103 task does not directly measure the systematic generalization of the models, it is used to evaluate the effectiveness and applicability of D3 on a large-scale task beyond relatively simple tasks.

### Experimental Results

In this section, we present the experimental results of the SAR task, sys-bAbI task, sort-of-CLEVR task, and WikiText-103 task. In our experiments, we set \(D_{}\) as \(D_{}/2\).

#### 4.2.1 TPR-based Memory Networks

First, we evaluate FWM with D3 on the SAR task, which requires understanding the composition of two types of symbols, \(x\) and \(y\). TPR-based models are expected to solve this task perfectly by mapping each symbol to a specific TPR component during decomposition. However, as shown in Fig. 2, FWM and AID fail to generalize unseen combinations of known symbols. In contrast, our D3 module significantly outperforms other baseline models, achieving nearly 100% accuracy. This result demonstrates that D3 effectively decomposes unseen combinatorial data into TPR components using discrete dictionaries.

Next, we test TPR-RNN and FWM with D3 on the sys-bAbI task. This task involves compositional information in each story sentence, such as the relation between objects and their locations. It makes

Figure 2: Test accuracy curve [%] on the SAR task for 10 seeds, with shadowed area indicating SD.

a sentence-level model more suitable for capturing the structural information of data than a word-level model. However, as shown in Table 1, TPR-RNN shows a larger performance gap between the _w/o sys diff_ and _w/ sys diff_ cases than FWM. Notably, D3 enhances the systematic generalization of both TPR-RNN and FWM with fewer additional parameters, significantly reducing the performance gap for TPR-RNN. These results highlight the efficacy of D3 in text understanding tasks.

#### 4.2.2 Linear Transformer

We also evaluate the Linear Transformer with D3 on the sort-of-CLEVR task and WikiText-103 task. Following the AID , we use a 4-layered Linear Transformer with shared parameters for the sort-of-CLEVR task and apply D3 to a 16-layered Linear Transformer at intervals of 4 out of the 16 layers for the WikiText-103 task. As shown in Tables 2 and 3, D3 improves the performance of the Linear Transformer, with these improvements increasing as the capacity of the dictionaries grows. These results demonstrate the effectiveness of D3 on visual relational reasoning and language modeling tasks, as well as its applicability to the Linear Transformer. In addition, D3 shows comparable performance to the attention-based decomposition method, even with fewer parameters.

### Analysis

In this section, we conduct a qualitative analysis of the structured TPR representations generated by D3 and an ablation study of D3. For these analyses, we experiment with D3 (_w/o F_) on the SAR task.

#### 4.3.1 Qualitative Analysis

TPR framework requires its structured representations to satisfy the following conditions for accurate TPR operations: (_i_) linearly independence between distinct _roles_, and (_ii_) high correlation between

  
**Model** & _w/o sys diff_ (\(\)) & _w/ sys diff_ (\(\)) & **Gap** (\(\)) & \# params (\(\)) \\  TPR-RNN & \(0.79 0.16\) & \(8.74 3.74\) & 7.95 & **0.14**\(M\) \\ + AID & \(0.69 0.08\) & \(5.61 1.78\) & 4.92 & 0.32 \(M\) \\ + D3 & \( 0.25\) & \( 2.07\) & **2.85** & 0.17 \(M\) \\  FWM & \(0.79 0.14\) & \(2.85 1.61\) & 2.06 & **0.73**\(M\) \\ + AID & \( 0.16\) & \( 0.66\) & **0.76** & 1.23 \(M\) \\ + D3 (_w/o F_) & \(0.79 0.30\) & \(2.58 1.12\) & 1.79 & 0.75 \(M\) \\ + D3 (_w/F_) & \(0.75 0.17\) & \(1.96 0.88\) & 1.21 & 0.75 \(M\) \\   

Table 1: The mean word error rate [%] on the sys-bABI task for 10 seeds, with \(\) indicating SD.

  
**Model** & \(D_{}\) & _Unary_ (\(\)) & _Binary_ (\(\)) & _Ternary_ (\(\)) & \# params (\(\)) \\  Linear Transformer & - & \(69.3 14.8\) & \(75.5 1.3\) & \(56.4 4.3\) & **0.68**\(M\) \\ + AID & - & \(98.9 0.2\) & \(78.6 0.3\) & \(63.7 1.2\) & 0.83 \(M\) \\  \(\) D3 (_w/o F_) & 128 & \(73.9 16.5\) & \(77.2 22\) & \(57.3 4.6\) & 0.75 \(M\) \\  & 256 & \(73.7 16.5\) & \(77.8 2.5\) & \(57.9 5.8\) & 0.96 \(M\) \\  \(\) D3 (_w/F_) & 128 & \(98.9 0.2\) & \(79.5 0.8\) & \(63.1 1.9\) & 0.80 \(M\) \\  & 256 & \( 0.3\) & \( 2.4\) & \( 1.2\) & 1.13 \(M\) \\   

Table 2: The mean accuracy [%] on the sort-of-CLEVR task for 10 seeds, with \(\) indicating SD.

  
**Model** & \(D_{}\) & _Valid_ (\(\)) & _Test_ (\(\)) & \# params (\(\)) \\  Linear Transformer & - & \(36.473\) & \(37.533\) & **44.02**\(M\) \\ + AID & - & \(36.159\) & \(37.151\) & \(44.16\)\(M\) \\  + D3 (_w/o F_) & 32 & \(36.061\) & \(37.220\) & \(44.12\)\(M\) \\  & 64 & \(\) & \(\) & \(44.36\)\(M\) \\  + D3 (_w/ F_) & 32 & \(36.630\) & \(37.620\) & \(44.22\)\(M\) \\  & 64 & \(36.220\) & \(37.128\) & \(44.62\)\(M\) \\   

Table 3: Perplexity on the WikiText-103 task.

role_ and _unbinding operator_ for the same symbol \(x\). We analyze the orthogonality of generated representations to investigate whether they satisfy these TPR conditions. Specifically, we consider the case of varying \(x\) while keeping \(y\) fixed for simplicity.

Fig. 3(c) shows the cosine similarity between the _roles_ during the discovery phase, and Fig. 4(c) shows the cosine similarity between the _roles_ during the discovery phase and the _unbinding operator_ during the inference phase. Both results demonstrate that the generated representations by D3 satisfy the TPR conditions, resulting in an accuracy of nearly 100%. We also conduct the same analysis for intermediate features, particularly query and code. Figs. 3 and 4 show that each intermediate representation complements the others to satisfy the TPR condition, indicating the effectiveness of D3.

Figure 4: The heatmap displays the cosine similarity between the generated representations during the discovery phase (represented on the **x-axis**) and the inference phase (represented on the **y-axis**) for the SAR task. We explore the similarity across different types of representations: (a) queries of _roles_ and _unbinding operators_, (b) codes of _roles_ and _unbinding operators_, and (c) the _roles_ and _unbinding operators_ themselves.

Figure 5: The heatmap visualizes the cosine similarity of the learned codebook features for the SAR task. There are two parts to each heatmap: (a) the similarity among codebook keys, denoted as \(\{_{i}\}_{i=1}^{N}\), and (b) the similarity among codebook values, denoted as \(\{_{i}\}_{i=1}^{N}\). For better visualization, the heatmap values are reordered to reflect the cluster of similar codebook keys.

Figure 3: The heatmap displays the cosine similarity between the generated representations during the discovery phase for the SAR task. We explore the similarity across different types of representations: (a) queries of _roles_, (b) codes of _roles_, and (c) the _roles_ themselves.

Furthermore, we analyze the similarity patterns of codebook keys and codebook values. Fig. 5 shows that the codebook features learn orthogonal patterns despite being learned without constraints. This result implies that the learnable parameters of dictionaries implicitly capture TPR conditions to ensure accurate TPR operations.

#### 4.3.2 Ablation Study

We investigate the effect of hyper-parameters of D3, specifically \(N_{}\), \(D_{}\), and top-\(k\), on performance on the SAR task. Fig. 6(a) shows the effect of \(D_{}\). We observe that the value of \(D_{}\) significantly affects the performance of D3. Notably, D3 fails to solve the SAR task when \(D_{}\) is set to 8, indicating a need for adequate capacity of \(D_{}\). Fig. 6(b) shows the effect of varying top-\(k\) while holding \(N_{}\) constant, indicating that D3 achieves optimal performance when top-\(k\) is set to 2. This result demonstrates the efficacy of the sparse mechanism employed by D3. Fig. 6(c) examines the effect of varying \(N_{}\) while holding top-\(k\) constant, showing that D3 generally performs better with larger values of \(N_{}\).

## 5 Discussion and Limitations

Motivation.From the perspective of systematic generalization, the decomposition operations in the TPR framework can be viewed as mapping unseen data to TPR components observed during training. Motivated by this, we design a decomposition module based on discrete representations, which maps input data to discrete, learned features facilitating systematic generalization in the decomposition operations of TPR. This design choice differentiates our contribution from AID's competitive attention-based decomposition module. Additionally, each dictionary in D3 is explicitly linked to a specific TPR component, ensuring that each dictionary is responsible solely for generating its corresponding component. The generated components are then utilized in predefined TPR operations of the TPR-based models. This design ensures that each dictionary is trained to specialize in a specific TPR component.

Interpretability.The TPR framework decomposes data at the representation level into distinct symbols, such as _role-filler_ pairs for encoding and _unbinding operators_ for decoding. This characteristic enhances the interpretability of models because the relationships between _roles_ and _unbinding operators_ explain which parts of the input the model focuses on to predict the output. However, this interpretability is reliable only when the generated structured representations satisfy the TPR conditions. In this context, D3 enhances the interpretability of models by providing structured representations that more effectively satisfy the TPR conditions than baseline models like FWM and AID. Figs. 9 and 10 demonstrate that the representations generated by D3 better conform to the TPR conditions than those from other baseline models, supporting our claim that D3 contributes to increased interpretability.

D3 Applied to Filler (w/o F and w/ F).In the TPR framework, _roles_ and _unbinding operators_ must meet specific conditions, such as linear independence among _roles_ and high correlation between _roles_ and _unbinding operators_, to ensure accurate TPR operations. However, there are no such

Figure 6: The mean accuracy on the SAR task for 10 seeds in the ablation study, with error bar indicating SD. The default setting uses \(D_{}\) of 64, \(N_{}\) of 64, and top-\(k\) of 8. Each figure shows the experimental results for the following settings: (a) Varying \(D_{}\). (b) Varying \(N_{}\) with top-\(k\) constant. (c) Varying top-\(k\) with \(N_{}\) constant.

requirements for _fillers_, which are features related to downstream tasks. This characteristic affects the performance of D3 depending on whether it is applied to generate the _fillers_ (_w/ F_) or not (_w/o F_). In our experiments, the _w/ F_ configuration performs well on the sys-bAbI and sort-of-CLEVR tasks with relatively few labels (~200). In contrast, the _w/o F_ configuration excels on the SAR and WikiText-103 tasks, which have a larger number of labels (500-). These findings suggest that the _w/o F_ configuration may be more effective for large-scale practical tasks. Nevertheless, beyond these experimental results, we do not fully understand the conditions under which each configuration performs better. Consequently, one limitation of D3 is the additional burden of determining the suitable configuration for various tasks when applying it to other domains.

Sparse Key Selection.D3 integrates seamlessly with existing TPR-based models, significantly enhancing their generalization performance across various tasks. However, this integration introduces additional computational overhead to the baseline models. Specifically, the sparse key selection mechanism of D3 has a computational complexity of \((N_{}(D_{}+k))\) for each TPR component. Therefore, this complexity can become a drawback as the capacity of the dictionaries increases. One potential solution to address this capacity issue is to incorporate product keys into the sparse key selection mechanism of D3, a technique studied in prior discrete key-value architectures . We leave this enhancement for future work.

Scalability.The scalability of D3 is inherently linked to TPR operations of baseline models since the number of dictionaries in the D3 layer aligns with the number of TPR components required for their operations. As TPR operations require increasing components to handle large datasets, our method also requires a proportional increase in dictionaries, resulting in significant computational and memory overhead. As explored in prior work, one potential solution to mitigate this issue is distributing shared dictionaries across multiple heads or layers . However, this approach requires further investigation and experimentation, which we plan to research in future work.

## 6 Conclusion

In this paper, we tackle the decomposition problem inherent in the TPR framework, which poses a significant challenge for TPR-based models. To address this, we introduce a discrete dictionary-based layer, D3, designed to enhance the decomposition capabilities of TPR-based models. D3 employs the discrete dictionaries to map input data to pre-learned symbolic features within each dictionary, thereby generating structured TPR representations. Our comprehensive experiments demonstrate that D3 significantly enhances the systematic generalization of the TPR-based models with fewer additional parameters. Furthermore, our qualitative analysis verifies that D3 effectively generates structured representations that are satisfactory for the requirements of the TPR framework.