# Group-wise oracle-efficient algorithms for

online multi-group learning

Samuel Deng

Department of Computer Science

Columbia University

samdeng@cs.columbia.edu

&Daniel Hsu

Department of Computer Science

Columbia University

djhsu@cs.columbia.edu

&Jingwen Liu

Department of Computer Science

Columbia University

jingwenliu@cs.columbia.edu

###### Abstract

We study the problem of online multi-group learning, a learning model in which an online learner must simultaneously achieve small prediction regret on a large collection of (possibly overlapping) subsequences corresponding to a family of _groups_. Groups are subsets of the context space, and in fairness applications, they may correspond to subpopulations defined by expressive functions of demographic attributes. In contrast to previous work on this learning model, we consider scenarios in which the family of groups is too large to explicitly enumerate, and hence we seek algorithms that only access groups via an optimization oracle. In this paper, we design such oracle-efficient algorithms with sublinear regret under a variety of settings, including: (i) the i.i.d. setting, (ii) the adversarial setting with smoothed context distributions, and (iii) the adversarial transductive setting.

## 1 Introduction

We study the problem of online multi-group learning, originally introduced by  (adapting the specialists/time-selection setup of ). In this learning model, we consider a collection of groups \(\), which are (possibly intersecting) subsets of a context space \(\), as well as a hypothesis class \(\) of functions defined on \(\). Contexts \(x_{1},x_{2},,x_{T}\) arrive one-by-one over a sequence of \(T\) rounds, and the learner must make a prediction associated with each \(x_{t}\). The learner's goal is to perform well on _every_ subsequence of rounds corresponding to each group \(g\). Here, performance is measured relative to the predictions of the best-in-hindsight hypothesis \(h\) for the specific subsequence under consideration.

A common interpretation of multi-group learning--which is natural when considering fairness in machine learning (ML)--identifies contexts \(x\) with individuals, each group \(g\) with a subpopulation (perhaps defined by a combination of various demographic features such as age and gender), and each hypothesis \(h\) with a classifier that makes predictions about individuals . The goal of the learner, then, is to predict as well as the best subpopulation-specific hypothesis, for all subpopulations simultaneously.

The standard benchmark in online learning is regret, which compares the performance of the learner on all rounds to that of the single best hypothesis in hindsight. But such a benchmark is only meaningful if there is a hypothesis that performs well in all contexts. At the other extreme, we may hope that the learner performs as well as using the best context-specific hypothesis in everyround. However, this may be impossible if no context is ever repeated. The group-wise notion of regret in online multi-group learning naturally interpolates between these two extremes: the former is recovered when \(=\{\}\), and the latter when \(=\{\{x\}:x\}\).

We are particularly interested in scenarios where \(\) may be extremely large (and perhaps even infinite). In such cases, it is too time-consuming to explicitly enumerate the groups in \(\), and this precludes the use of the algorithmic solutions from prior works .

The use of highly expressive families of groups has been a recent focus in the ML fairness literature, where fairness with respect to such rich families of groups is seen as compromise between coarse notions of statistical fairness that ignore intersectionality, and individualized notions of fairness that are typically difficult to ensure . For example, if groups are defined by simple combinations of demographic attributes (e.g., linear threshold functions), then the number of subsequences determined by these groups may grow exponentially with the number of attributes. To deal with the intractability of explicit enumeration of groups, these prior works rely on optimization oracles that implicitly search through the family of groups. In this work, we seek to do the same, but for the online (as opposed to batch) problem at hand.

Another motivation for multi-group learning with rich families of groups comes from the literature on "subgroup robustness" , where one is concerned with the test-time distribution shifting from the training distribution by restricting to subsets (or "subgroups") of the feature space. Such scenarios have been practically motivated, for instance, in medical domains where training data sets are constructed to include data from all patients (healthy and sick) as a matter of convenience, but the population relevant to the application is only a subset of the sick patients for which an intervention is potentially possible . It may be difficult to anticipate which subgroup will ultimately be relevant (or even to provide an explicit shortlist of such subgroups), but multi-group learning with a very rich and expressive family of groups \(\) ensures that, as long as a subgroup is well-approximated or within the collection \(\), it obtains our theoretical guarantees. This motivates dealing with large or potentially infinite \(\) to provide guarantees for as many subgroups as possible.

### Summary of Results

We construct an end-to-end oracle-efficient online learning algorithm that is oracle-efficient in _all_ the problem parameters. Namely, it is oracle-efficient in both \(\) and \(\). In the case of finite \(\) or \(\), this admits an exponential computational speedup over the previous algorithms; in the case of infinite \(\), this is the first algorithm that achieves multi-group online learning. With this in mind, we can even think of \(\) as representing a binary-valued function class that takes in contexts and selects subsequences \(S[T]\) based on those contexts in possibly complex ways.

Previous work has shown that the basic goal of designing a computationally efficient algorithm (in all problem parameters) to achieve \(o(T)\) regret for fully worst-case adversaries is impossible . Research has therefore focused on natural structural assumptions in which computational efficiency and sublinear regret is possible, albeit in the traditional setting without groups . Because the multi-group online adversarial setting is strictly harder than the standard setting in which this lower bound applies (simply consider multi-group learning with one group: the entire sequence), we must also take similar assumptions to circumvent the computational hardness result. In this work, we consider the same structural assumptions in the multi-group scenario. Specifically, we make the following contributions (with \(()\) suppressing \(\) factors):

1. **Group-wise oracle efficiency for the smoothed setting.** We present an oracle-efficient algorithm that achieves \(()\) regret on every group \(g\) for a binary-valued action space for the _smoothed online learning setting_ of , where \(d\) is a bound on the VC dimension of \(\) and \(\), and \(\) is a parameter interpolating between the scenarios in which contexts are generated by a worst-case adversary and a benign, fully i.i.d. adversary.
2. **Group-wise oracle-efficiency through generalized follow-the-perturbed leader.** If a sufficient condition referred to as \(\)_-approximability_ in previous literature () is met, a variant of our oracle-efficient algorithm achieves, for each particular \(g\), a regret of \((||||})\) for finite \(\) and \(\), where \(N\) corresponds roughly to a required number of perturbations, and \(T_{g}\) is the number of rounds \(t[T]\) in which \(x_{t} g\). The dependence on \(T_{g}\) as opposed to \(T\) is preferable if some groups do not appear frequently over the \(T\) rounds. As a special case, our algorithm also achieves \((N^{1/4}||||})\) in the _transductive setting_ of , where the adversary must reveal a set of \(N\) future contexts before learning begins.

Table 1 summarizes our results in relation to existing work. Our algorithms follow a more general algorithm design template based on the _adversary moves first (AMF)_ framework of . We extend this framework with the following technical enhancements: sparsifying the implicit distributions over the hypothesis spaces used by _follow-the-perturbed-leader (FTPL)_ algorithms, and simplifying the min-max game that is solved in every iteration of AMF.

In particular, the AMF framework allows us to have low-regret with respect to all group-hypothesis pairs; the multi-objective regret guarantees are useful for our purposes because we want to guarantee simultaneous low regret over all \(g\). However, naively applying AMF requires us to enumerate these objectives, in turn enumerating \(\), the main issue we want to avoid. FTPL comes to the rescue and allows us to have low-regret to all pairs implicitly without enumerating them. So AMF allows us to compete with all \(g\); FTPL ensures this is efficient. The algorithmic details and main technical challenges can be found in Section 4.2. These techniques allow us to adapt this framework to the scenario where the number of groups is too large to enumerate, and they may find use in other multi-objective learning problems.

### Related Work

 showed that it is possible to achieve sublinear multi-group regret by reducing to the _specialists framework_ of  (a.k.a. _sleeping experts_). The multi-group regret they achieve scales logarithmically in both \(\) and \(\); this recovers the minimax regret when specializing to the standard online learning setting (with finite \(\)) with only a single group. The paper focuses on regret rather than computational considerations; a direct implementation of their algorithm uses time and space linear in \(||||\), and there are no stated guarantees for infinite \(\) or \(\).

   Work & Setting & Regret & Computation & Oracle- & Oracle- \\  & & & & efficient in \(\) & in \(\) \\ 
 & Adversarial & \(||||}\) for all & Time: \(O(||||)\) & No & No \\  & & \(g\) & Space: \(O(||||)\) & & \\ 
 & Adversarial & \(||||}\) for all & Time: \(O(||)\) Space: & Yes & No \\  & & \(g\) & \(O(||)\) & & \\ 
 & \(\)-Smooth & \(}\) (does not handle multi-group setting) & Time: poly(\(T\)) calls to optimization oracle & Yes & N/A \\ 
 & \(\)-Smooth & \(}}\) (does not handle multi-group setting) & Time: poly(\(T\)) calls to optimization oracle & Yes & N/A \\ 
 & \(\)-Smooth & \(}\) for all \(g\) & Time: poly(\(T\)) calls to optimization oracle & Yes & N/A \\ Ours (Theorem 5.1) & \(\)-approximable & \(||||}\) for all \(g\) & Time: poly(\(T\)) calls to optimization oracle & Yes & **Yes** \\ Ours (Corollary 5.1.1) & Transductive & \(N^{1/4}||||}\) & Time: poly(\(T\)) calls to optimization oracle & Yes & **Yes** \\   

Table 1: In the table above, \(T\) is the number of rounds of online learning, \(T_{g}\) is the number of rounds for a particular group \(g\), \(\) is the hypothesis class, \(\) is the collection of groups, and \(\) is the smoothness parameter defined in Definition 4.1. \(d\) is an upper bound on the VC dimension of \(\). In our \(\)-smooth result in the fifth row, \(d\) is also an upper bound on the VC dimension of a possibly infinite collection of groups, \(\). In the \(\)-approximable setting of the sixth row, \(N\) is the number of perturbations.

[Ach+23] show how to avoid enumeration of \(\) using an optimization oracle for \(\). They achieve this by applying a meta-algorithm atop a black-box oracle-efficient online learning algorithm, but this meta-algorithm ultimately requires explicit enumeration of \(\). Our work, in contrast, uses an optimization oracle for \(\) jointly and hence avoids explicit enumeration of either \(\) or \(\).

Multi-group (agnostic) learning has also been studied in the batch setting . In this setting, training data is drawn i.i.d. from a fixed distribution, and the learner's goal is to find a single hypothesis \(\) (possibly outside of \(\)) that ensures small excess risk \([((x),y) x g]-_{h}[ ((x),y) x g]\) for every group \(g\) simultaneously. The works of  design algorithms for achieving this learning criterion under a certain "multi-PAC compatibility" assumption on \(\) and \(\).  design multi-group learning algorithms that remove the need for this assumption. One of the algorithms of , which enjoys near optimal sample complexity for general but finite \(\) and \(\), is based on the the online multi-group learning approach of  combined with online-to-batch conversion.

The proof of our algorithm builds on two primary technical frameworks studied in previous literature: the _adversary moves first (AMF)_ framework of , and a line of work designing follow-the-perturbed leader style algorithms  for adversarial online learning in the oracle-efficient learning model .

## 2 Preliminaries

### Notation

Throughout, \(\) denotes a _context space_, and \(\) denotes an _action space_. For example, in a typical (online) supervised learning setup, \(\) is the feature space, and \(\) is the label space. A _group_\(g\) is a subset of the context space \(\). We overload the notation \(g\) for a group by using it as an indicator function \(g(x):=\{x g\}\) for group membership. Let \(2^{}\) denote all subsets of the context space \(\), and let \(^{}\) denote all possible mappings from \(\) to \(\). For an integer \(n\), denote \([n]:=\{1,2,,n\}\).

For simplicity of exposition, we will focus on the setting where \(\) is binary throughout, i.e. \(=\{-1,1\}\). We note that our techniques are more general, however, and may be adapted to the case of finite, multi-class action spaces (see Appendix C for details).

### Online Multi-Group Learning

We formally define the multi-group learning model as follows. Let \( 2^{}\) be a collection of (possibly non-disjoint) groups, and let \(^{}\) be a _hypothesis class_ of functions \(h:\) mapping from contexts to actions. Let \(:\) be a bounded loss function. In each round \(t[T]\):

1. Nature chooses \((x_{t},y_{t})\) and reveals \(x_{t}\).
2. The learner chooses an action \(_{t}\).
3. Nature reveals \(y_{t}\).
4. The learner incurs loss \((_{t},y_{t})\).

The choices of Nature and the learner may be randomized. In the standard online prediction setting, the regret of the learner is the difference between the cumulative loss of the learner and that of the best-in-hindsight hypothesis from \(\): \(_{T}():=_{t=1}^{T}(_{t},y_{t})- _{h}_{t=1}^{T}(h(x_{t}),y_{t})\). The goal of the learner is to achieve sublinear (in \(T\)) expected regret.

In multi-group online learning, we consider the regret of the learner on subsequences of rounds \((t[T]:x_{t} g)\) defined by the groups \(g\) and the sequence of contexts \(x_{1},,x_{T}\). Specifically, the _(multi-group) regret of the learner on group \(g\)_ is

\[_{T}(,g):=_{t=1}^{T}g(x_{t})(_{t},y _{t})-_{h}_{t=1}^{T}g(x_{t})(h(x_{t}),y_{t}).\] (1)

Crucially, the best hypothesis for one group may differ from that of another group. Further, groups may intersect, precluding the strategy of simply running a separate no-regret algorithm for each group. The learner seeks to achieve achieve sublinear expected regret, on all groups \(g\) simultaneously.

### Group Oracle-Efficiency

The main challenge posed in this work is to design computationally efficient algorithms that work with both large hypothesis classes \(\) and large collections of groups \(\). The prior work of  shows how to use the following optimization oracle to avoid explicitly enumerating the hypothesis class \(\) (but still require enumerating \(\)).

**Definition 2.1** (Optimization Oracle).: _For some error parameter \( 0\) and function class \(}^{}\), an \(\)-approximate optimization oracle \(\) takes a collection of pairs \((x_{1},z_{1}),,(x_{m},z_{m})\), a sequence of weights \(w_{1},,w_{m}\), and a sequence of \(m\) loss functions \(_{i}:}[-1,1]\) and outputs a function \(:=(\{(x_{i},z_{i},w_{i})\}_{i=1}^{m})\) satisfying:_

\[_{i=1}^{m}w_{i}_{i}((x_{i}),z_{i})_{f} _{i=1}^{m}w_{i}_{i}(f(x_{i}),z_{i})+.\]

Instantiating (in Definition 2.1) \(\) as our action space \(\), each \(_{i}\) as the given loss \(\) of our problem, and \(\) as our hypothesis class \(\) gives a standard empirical risk minimization (ERM) oracle over a dataset \(\{(x_{i},y_{i})\}_{i=1}^{m}\). We present this more general definition to distinguish the action space \(\) of the problem from the output space of the oracle (see Definition 2.2).

The optimization oracle is regarded as a natural computational primitive because, for many problems in machine learning, various heuristic methods (e.g., stochastic gradient descent) appear to routinely solve such problem instances despite the worst-case intractability of such problems.

The work of  still relies on explicit enumeration of \(\). We show how this can be avoided using a joint optimization oracle for \(\), defined as follows.

**Definition 2.2** (\((,)\)-optimization oracle).: _Fix an error parameter \( 0\). For a collection of groups \( 2^{}\), a collection of hypotheses \(^{}\), and a sequence of \(m\) loss functions \(_{i}:(\{0,1\})() [-1,1]\), an \(\)-approximate \((,)\)-optimization oracle \(_{(,)}^{}\) is an \(\)-approximation optimization oracle (Definition 2.1) that outputs a pair \((,)\) satisfying:_

\[_{i=1}^{m}w_{i}_{i}(((x_{i}),(x_{i})),(y_{i},y_{i}^ {}))_{(g^{*},h^{*})}_{i=1}^{ m}w_{i}_{i}((g^{*}(x_{i}),h^{*}(x_{i})),(y_{i},y_{i}^{}))-.\] (2)

If \(=\{-1,1\}\) (which we assume in the main paper body), this oracle outputs a group-hypothesis pair \((,)\) that maximizes the batch loss over \(m\) examples \((x_{i},(y_{i},y_{i}^{}))\) in \(\{-1,1\}^{2}\).  also made such an assumption and gave two implementations: one based on cost-sensitive classification oracles for \(\) and \(\) separately, the other a heuristic algorithm that is empirically effective. Details of these oracle instantiations are included in Appendix B.3.

We also require an optimization oracle for \(\) itself, defined similarly. This can be thought of simply as (exact) empirical risk minimization over \(\).1

**Definition 2.3** (\(\)-optimization oracle).: _For a collection of hypotheses \(^{}\), and a sequence of \(m\) loss functions \(_{i}:[-1,1]\), an \(\)-optimization oracle \(_{}\) is a \(0\)-approximation optimization oracle (Definition 2.1, with \(=0\)) that outputs a hypothesis \(h\) satisfying \(_{i=1}^{m}w_{i}_{i}(h(x_{i}),y_{i})_{h^{*}} _{i=1}^{m}w_{i}_{i}(h^{*}(x_{i}),y_{i})\)._

## 3 Warm-up: I.I.D. Setting

In this section, as a warm-up, we consider a setting where Nature is stochastic and oblivious: the \((x_{t},y_{t})\) are drawn i.i.d. from a single fixed (but unknown) distribution \(\), independent of any choices of the learner. In a standard online prediction setting with i.i.d. data, it suffices to use a "follow-the-leader" (FTL) strategy (see, e.g., ), which can be easily implemented using an optimization oracle for \(\). However, such a strategy only guarantees low regret on \(g=\). To achieve low regret on all (possibly intersecting) groups \(g\) simultaneously, we need a multi-group analogue of FTL.

What makes FTL work in the standard online prediction setting is the instantaneous expected regret bound of empirical risk minimization (ERM) on i.i.d. data. Therefore, it is natural to replace ERM with a batch multi-group algorithm ; this will ensure the requisite instantaneous guarantee on all groups \(g\). We show how to use the oracle-efficient algorithm ListUpdate of  for the online multi-group problem.

Throughout this section, we assume \(\) is the zero-one loss (for simplicity), and that \(\) and \(\) both have VC dimension at most \(d 1\). For any \(g\), let \(P(g):=_{(x,y)}[g(x)]\) be the probability mass of group \(g\).

**Theorem 3.1** (Theorem 16 of ).: _For any \((0,1)\), given \(n\) i.i.d. training samples \(\{(x_{i},y_{i})\}_{i=1}^{n}\) from \(\), the ListUpdate algorithm2 returns a function \(f:\) such that, with probability \(1-\), for any group \(g\),_

\[[(f(x),y) x g]_{h}[( h(x),y) x g]+ O(()^{1/3}).\]

_Moreover, ListUpdate makes \((n,d,(1/))\) calls to a \((,)\) optimization oracle._

Our algorithm, Online ListUpdate, forms its prediction \(_{t}\) in round \(t\) as follows:

* Run ListUpdate on the samples from previous rounds \((x_{1},y_{1}),,(x_{t-1},y_{t-1})\).
* Let \(f_{t}\) denote the function returned by ListUpdate, and predict \(_{t}:=f_{t}(x_{t})\).

Using Theorem 3.1, we obtain the following multi-group regret bound for Online ListUpdate.

**Theorem 3.2**.: _If \((x_{t},y_{t})\) are drawn i.i.d. from a fixed distribution \(\) over \(\), Online ListUpdate achieves the following expected multi-group regret bound: for all \(g\),_

\[[_{T}(,g)]=O((d T)^{1/3} T^{2/3}+).\]

The proof of Theorem 3.2 is given in Appendix A.

## 4 Group Oracle-Efficiency with Smooth Contexts

In this section, we first describe a natural problem setting in which oracle-efficient online multi-group learning is possible: the smoothed online learning setting (Section 4.1), for which the i.i.d. setting of Section 3 is a special case. We then present our main algorithm, Algorithm 1, for achieving oracle-efficient online multi-group learning (Section 4.2). Easy modifications of this main algorithm will admit oracle-efficient online multi-group learning for other common online learning specifications, as described in Section 5.

### Smoothed Online Learning

We now describe _smoothed online learning_, a prevalent model in recent literature in computationally efficient online learning that formalizes the natural relaxation that Nature is not maximally adversarial . The main assumption is that, instead of choosing _arbitrary_ (possibly worst-case) examples \((x_{t},y_{t})\) at every round, Nature adversarially fixes a distribution \(_{t}\) over \(\) and draws \(x_{t}_{t}\), while still drawing \(y_{t}\) adversarially. Formally, we restrict such distributions to be _\(\)-smooth_, following the definitions of .

**Definition 4.1** (\(\)-smooth distribution).: _Let \(\) be some probability measure on \(\), and let \(\) be a base measure on \(\). The distribution \(\) is \(\)-smooth (with respect to \(\)) if \(\) is absolutely continuous3 with respect to \(\) and_

\[}.\]

_We denote the set of all \(\)-smooth distributions on \(\) with respect to the measure \(\) as \(_{}(,)\). If \(\) is clear from context, we simply write \(_{}()\). We assume that we have sample access to \(\) throughout. For simplicity, one may assume \(\) is uniform on \(\)._Definition 4.1 interpolates between the benign setting where \(x_{t}\) are drawn i.i.d. from \(\) when \(=1\), and the fully adversarial setting when \(\) approaches \(0\). In this sense, the warm-up result of Section 3 is a special case of this setting when \(=1\) and \(\) is fixed for all rounds. Note that this definition does not restrict the choice of \(y_{t}\) at all; \(y_{t}\) may still be chosen adversarially.

With this definition in hand, consider the following specification of the learning game outlined in Section 2.2, henceforth refered to as the _\(\)-smooth online learning setting._ For each round \(t[T]\):

1. Nature fixes a distribution \(_{t}_{}()\) that may depend in any way on the entire history prior to round \(t\). Nature samples \(x_{t}_{t}\) and chooses \(y_{t}\) adversarially; \(x_{t}\) is revealed to the learner.
2. The learner (randomly) chooses an action \(_{t}\).
3. Nature reveals \(y_{t}\), and the learner incurs the loss \((_{t},y_{t})\).

We now depart from the previous literature that considers oracle-efficient algorithms in this setting (), as we focus on the more difficult objective of minimizing multi-group regret over a collection \(\), as in Equation (1). This setting will allow us to employ our \((,)\)-optimization oracle in Definition 2.2; a full description of our algorithm is now in order in Section 4.2.

### Algorithm for Smooth Contexts

In this section, we present Algorithm 1, our main algorithm for multi-group online learning for the \(\)-smooth setting. At a high level, our algorithm takes inspiration from the very general _adversary-moves-first (AMF) framework_ for multiobjective online learning of . Our algorithm can be thought of as a sequential game between two competing players: an adversarial \((,)\)-player and the learner, referred to, in the context of Algorithm 1 as the \(\)-player. On each round \(t\), the \((,)\)-player employs a \((,)\)-optimization oracle, \(^{}_{(,)}\), to play a distribution over group-hypothesis pairs that maximizes the misfortune of the \(\)-player, based on the history up until \(t\). Upon receiving this distribution (and the context \(x_{t}\) from Nature), the \(\)-player chooses \(_{t}\) randomly according to a distribution obtained by solving a simple constant-size linear program, and then incurs the loss \((_{t},y_{t})\). The \((,)\)-player, taking this new loss into account, can now adjust his strategy to foil the \(\)-player in the next round by putting mass on the groups on which the \(\)-player performs poorly. Crucially, neither \(\) nor \(\) is ever accessed directly, although our proofs need to maintain a distribution over \(\). In order to do this, we make the crucial observation that FTPL maintains an _implicit_ distribution over \(\) and we sparsely approximate that distribution through repeatedly querying the \(^{}_{(,)}\) oracle. (For clarity, we use the tilde decoration, \(\) and \(\), on hypotheses and groups obtained by the \((,)\)-player using \(^{}_{(,)}\).)

**Main algorithm.** For any \(x\), define \(_{x}:(\{0,1\})()[-1,1]\) as:

\[_{x}((,),(y^{},y)):=(x)( (y^{},y)-((x),y)),\] (3)

where \((,)\) is the loss given by the learning problem. The quantity \(_{x}\) is the loss that the \((,)\)-player is maximizing; it corresponds to the single-round regret of the learner on group \(g\) to the hypothesis \(h\) if the context on that round is \(x\).

The \((,)\)-player will employ the FTPL style strategy of , adapted to our setting. For each round \(t\), this requires generating \(n\)_perturbation examples_ as extra input to \(^{}_{(,)}\). To generate these hallucinated perturbation examples, we independently draw \(z_{t,j}\) and \(_{t,j} N(0,1)\), samples \(j[n]\) from the base measure and the standard Gaussian, respectively. In this section, \(=\{-1,1\}\) and \(\{-1,1\}^{}\), so we use the perturbations in their FTPL variant for binary-valued action spaces (outlined in Appendix B.6),

\[^{}_{t,n}(g,h,):=_{j=1}^{n}g(z_{t,j})h(z_{t,j})}{}.\] (4)

**Remark**.: _We focus on the setting where \(=\{-1,1\}\) for ease of exposition, but settings in which \(\) is a general finite set can be handled with easy modifications. See Appendix C for details._

**Remark**.: _Multi-group online learning settings other than the \(\)-smooth setting can be handled appropriately simply by replacing the strategy of the \((,)\)-player by an appropriate no-regretalgorithm with access to \(^{}_{(,)}\). Examples of such variants are given in Section 5, and the general framework for such modifications is given in Appendix B._

**Remark**.: _With appropriate modifications, one can instantiate the \((,)\)-player with the FTPL style strategy of  instead, inheriting the \(^{-1/4}\) dependence summarized in Table 1. Our focus in this paper is not on the dependence on \(\), however, so our main exposition centers around the similar algorithmic techniques of ._

It is clear that \(\) and \(\) are never accessed except through \(^{}_{(,)}\) and \(_{}\). We make \(M\) oracle calls to \(^{}_{(,)}\) and two oracle calls to \(_{}\) at each round.

**Theorem 4.1**.: _Let \(=\{-1,1\}\) be a binary action space, \(\{-1,1\}^{}\) be a binary-valued hypothesis class, \( 2^{}\) be a (possibly infinite) collection of groups, and \(:\{-1,1\}\{-1,1\}\) be a bounded loss function. Let the VC dimensions of \(\) and \(\) both be bounded by \(d\). Let \( 0\) be the approximation error of the oracle \(^{}_{(,)}\). If we are in the \(\)-smooth online learning setting, then, for \(M=(T),n=(T/)\), and \(=(T/)\), Algorithm 1 achieves, for each \(g\):_

\[[_{T}(,g)] O(}+ T),\]_where the expectation is over all the randomness of the \((,)\)-player's perturbations and the \(\)-player's Bernoulli choices. (See Corollary B.7.1 for precise settings of \(M\), \(n\), and \(\).)_

Technical details.We solve three main difficulties toward ensuring that our algorithm achieves diminishing multi-group regret while maintaining computational efficiency for large or infinite \(\), which we outline here. The full proof of Theorem 4.1 is in Appendix B.

First, although the general framework of casting online learning problems with multiple objectives as two-player games is not new (), previous works have employed a multiplicative weights algorithm to hedge against the multiple objectives, requiring explicit enumeration. Departing from previous literature, however, our \((,)\)-player uses a _follow-the-perturbed leader (FTPL)_ style algorithm (see, e.g., ) with \(_{(,)}\). The particular follow-the-perturbed leader variant of  constructs "perturbations" via a set of fake examples drawn from the the base measure \(\) on \(\), and, is thus suitable for our oracle and problem setting.

Second, a key property needed by the proof of Algorithm 1 is that the \(\)-player must receive a _distribution_ over \(\) to reduce the complex multi-objective criterion of performing well against all \((g,h)\) to a scalar quantity. Previous work directly supplied this distribution through the multiplicative weights algorithm. However, this would involve explicitly enumerating \(\) and \(\). On the other hand, using an FTPL algorithm as is would only output a _single_ action from \(\), which is insufficient. To remedy this, we make the crucial observation that FTPL algorithms _implicitly_ maintain a distribution over \(\) through the randomness of their perturbations, and, thus, we construct the empirical approximation of this distribution through repeatedly calling \(_{(,)}\). Standard uniform convergence arguments are used to bound the number of oracle calls needed. An argument employing the minimax theorem shows that the final regret guarantee of the entire algorithm essentially inherits the regret of the FTPL algorithm, plus sublinear error terms.

Finally, the \(\)-player chooses a distribution over \(\) by by solving an exceedingly simple linear program (LP) with two optimization variables, \(p\) and \(\). The value \(p\) corresponds to the parameter of a Bernoulli distribution from which we sample to choose \(_{t}\). This choice of action corresponds exactly to choosing the minimax optimal strategy against the worst-case \(y\) that Nature could select. We employ similar techniques as , analyzing the value of this min-max game as if Nature (the \(\) in the min-max) had gone first instead. The two calls to \(_{}\) are used just to find the actions achievable by \(\) on \(x_{t}\). (Note that it is possible that \(h^{}_{y^{}}(x_{t}) y^{}\) for some \(y^{}\), in which case the Learner will always play \(-y^{}\), regardless of the value of \(p\).)

## 5 Group Oracle-Efficiency in Other Settings

In the previous section, we presented an algorithm that achieves \(o(T)\) expected regret for all \(g\), satisfying our main desideratum from Section 2.2. However, in some cases, we may want something more. Suppose that some groups are rarer than others; in this case, a natural extension would be to ensure a stronger "adaptive" regret bound that instead scales with \(T_{g}:=_{t=1}^{T}g(x_{t})\), the number of times group \(g\) appeared in the \(T\) rounds. We note that the algorithm of  achieves such a multi-group regret guarantee (for finite \(\) and \(\)), but their algorithm is not oracle-efficient. So a question that remains is whether such guarantees can be achieved in an oracle-efficient manner.

In this section, we are back in the general fully adversarial multi-group online learning setting of Section 2.2 (without i.i.d. or smoothness assumptions). We discuss how to modify Algorithm 1 via the _Generalized Follow-the Perturbed-Leader (GFTPL) framework_ of  to obtain regret guarantees on group \(g\) where the dependence on \(T\) is replaced (at least in part) with \(T_{g}\). Due to space limitations, we give a sketch here; the full details are in Appendix C.

We first make a simple observation that motivates our use of more advanced oracle-efficient online learning techniques. Recall the \(\)-specific per-round regret to \(\) of playing \(h(x_{t})\) at round \(t[T]\):

\[_{x_{t}}((,),(h(x_{t}),y))=(x_{t}) ((h(x_{t}),y)-((x_{t}),y)).\]

The job of the \((,)\)-player is to run a no-regret algorithm to maximize this quantity in aggregate, as described in Section 4.2 and detailed in Appendix B.5. The online learning literature for _small-loss regret_ focuses on developing algorithms that have regret depending on cumulative loss in hindsight instead of the number of rounds \(T\); this has the advantage of giving a tighterregret bound when losses are small in magnitude. It is immediate that \(_{x_{t}}((,),(h(x_{t}),y))=0\) whenever \((x_{t})=0\), so a small-loss regret would immediately give a \(o(T_{g})\) guarantee.

We focus on the case where \(\) and \(\) are finite, and \(\) is the set of experts the \((,)\)-player has access to. Most small-loss regret algorithms would require prohibitive enumeration of \(\), but the _GFTPL with small-loss bound algorithm_ of  has the property that it is oracle-efficient _and_ enjoys small-loss regret. This algorithm follows the GFTPL design template of , which, similar to the classic FTPL algorithm of , generates a noise vector to perturb each each decision of expert. However, whereas the classic FTPL algorithm generates \(||||\) independent random noise variables, GFTPL only generates \(N||||\) independent random variables and uses a _perturbation matrix (PM)_\([-1,1]^{|||| N}\) to translate the noise vector back to \(||||\)_dependent_ perturbations.

The main challenge in instantiating a GFTPL algorithm is to construct a suitable \(\) for the problem at hand.  provide two sufficient conditions for \(\) that, respectively, imbue the GFTPL algorithm with oracle-efficiency and small-loss regret: _implementability_ and _approximability_. In our setting, implementability requires that every column of \(\) correspond to a dataset of "fake examples" suitable to \(^{}_{(,)}\). Approximability with parameter \(>0\) guarantees the stability property that the ratio \([(_{t},_{t})=(g,h)]/[(_{t+1}, _{t+1})=(g,h)](_{t})\) for all \((g,h)\), where \(_{t}>0\) is the per-round learning rate of GFTPL. If such a \(\) exists, then instantiating the \((,)\)-player in Algorithm 1 with GFTPL (instead of the algorithm of ) gives us the stronger \(o(T_{g})\) regret guarantee. Full definitions and the proof, with the precise setting of \(M\), can be found in Appendix C and Proposition C.3.1.

**Theorem 5.1**.: _Assume \(,\) are finite and there exists a \(\)-approximable and implementable perturbation matrix \([-1,1]^{|||| N}\). Let \( 0\) be the approximation parameter of \(^{}_{(,)}\). Let the no-regret algorithm for the \((,)\)-player in Algorithm 1 be the GFTPL algorithm of  instantiated with \(\), with parameter \(M=(T)\). Then, for each \(g\):_

\[[_{T}(,g)] O(}\{ ,||||,|||} \}+ T)\]

We give a particular setting in which one can easily construct an approximable and implementable \(\).

**Transductive Setting.** In the _transductive setting_ of , Nature reveals a set \(X\) to the Learner at the beginning of the learning process; then at each round \(t[T]\), Nature can only choose \(x_{t}\) from \(X\). Let \(N:=|X|\) denote the number of different contexts that Nature chooses from. For this setting, we can explicitly construct \(\) to get the following result.

**Corollary 5.1.1** (Transductive setting).: _In the transductive setting, there exists a perturbation matrix \([-1,1]^{|||| 4N}\) such that Algorithm 1 with GFTPL parameterized with \(M=(T)\) and \(^{}_{(,)}\) with error parameter \( 0\) satisfies:_

\[[_{T}(,g)] O(}|||,|||} \}}+ T)g.\]

Suppose that \(N T\) (which is the case in the transductive learning setting from ). Then the regret bound (ignoring the dependence on \((||||)\)) on group \(g\) is \(O(}T^{1/4})\), which is asymptotically smaller than \(\) whenever \(T_{g}=o()\). If \(N\) is fixed independent of \(T\), then the regret bound is \(O(})\).

## 6 Conclusion and Future Work

In this paper, we design algorithms for online multi-group learning that are oracle-efficient and achieve diminishing \(o(T)\) expected regret for all groups \(g\) simultaneously, even when \(\) is too large to explicitly enumerate. The most interesting future directions that we leave open in this work include designing oracle-efficient algorithms that achieve \(o(T_{g})\) group-specific regret for _infinite_\(\) and \(\) and in more general settings.

[MISSING_PAGE_FAIL:11]

* [HJZ24] Nika Haghtalab, Michael Jordan, and Eric Zhao. "A unifying perspective on multi-calibration: Game dynamics for multi-objective learning". In: _Advances in Neural Information Processing Systems_ 36 (2024).
* [HK16] Elad Hazan and Tomer Koren. "The computational power of optimization in online learning". In: _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing_. STOC '16. New York, NY, USA: Association for Computing Machinery, June 2016, pp. 128-141.
* [HP05] Marcus Hutter and Jan Poland. "Adaptive Online Prediction by Following the Perturbed Leader". In: _Journal of Machine Learning Research_ 6.22 (2005), pp. 639-660.
* [HPY23] Nika Haghtalab, Chara Podimata, and Kunhe Yang. "Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents". In: _Advances in Neural Information Processing Systems_ 36 (Dec. 2023), pp. 61645-61677.
* [HRS22] Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty. "Smoothed Analysis with Adaptive Adversaries". In: _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_. Feb. 2022, pp. 942-953.
* [Kea+18] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness". In: _International conference on machine learning_. PMLR. 2018, pp. 2564-2572.
* [KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. "Multiaccuracy: Black-box post-processing for fairness in classification". In: _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_. 2019, pp. 247-254.
* [KK05] Sham Kakade and Adam Tauman Kalai. "From Batch to Transductive Online Learning". In: _Advances in Neural Information Processing Systems_. Vol. 18. MIT Press, 2005.
* [KV05] Adam Kalai and Santosh Vempala. "Efficient algorithms for online decision problems". In: _Journal of Computer and System Sciences_ 71.3 (Oct. 2005), pp. 291-307.
* [Lee+22] Daniel Lee, Georgy Noarov, Mallesh Pai, and Aaron Roth. "Online minimax multiobjective optimization: Multicalibeating and other applications". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 29051-29063.
* [LS15] Haipeng Luo and Robert E Schapire. "Achieving all with no parameters: Adamormal-hedge". In: _Conference on Learning Theory_. PMLR. 2015, pp. 1286-1304.
* [Nat89] B. K. Natarajan. "On learning sets and functions". In: _Machine Learning_ 4.1 (Oct. 1989), pp. 67-97.
* [NMR44] John von Neumann, Oskar Morgenstern, and Ariel Rubinstein. _Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)_. Princeton University Press, 1944.
* [Oak+20] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. "Hidden stratification causes clinically meaningful failures in machine learning for medical imaging". In: _Proceedings of the ACM conference on health, inference, and learning_. 2020, pp. 151-159.
* [RST11] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. "Online Learning: Stochastic, Constrained, and Smoothed Adversaries". In: _Advances in Neural Information Processing Systems_. Vol. 24. Curran Associates, Inc., 2011.
* [RY21] Guy N. Rothblum and Gal Yona. "Multi-group Agnostic PAC Learnability". In: _Proceedings of the 38th International Conference on Machine Learning_. PMLR, July 2021, pp. 9107-9115.
* [Sag+20] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. "Distributionally robust neural networks for group shifts: On the importance of regularizat ion for worst-case generalization". In: _International Conference on Learning Representations_. 2020.
* [SKS16] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. "Efficient Algorithms for Adversarial Contextual Learning". In: _Proceedings of The 33rd International Conference on Machine Learning_. Ed. by Maria Florina Balcan and Kilian Q. Weinberger. Vol. 48. Proceedings of Machine Learning Research. New York, New York, USA: PMLR, June 2016, pp. 2159-2168.
* [TH22] Christopher J. Tosh and Daniel Hsu. "Simple and near-optimal algorithms for hidden stratification and multi-group learning". In: _Proceedings of the 39th International Conference on Machine Learning_. PMLR, June 2022, pp. 21633-21657.

* [Wan+22] Guanghui Wang, Zihao Hu, Vidya Muthukumar, and Jacob D. Abernethy. "Adaptive Oracle-Efficient Online Learning". In: _Advances in Neural Information Processing Systems 35_ (Dec. 2022), pp. 23398-23411.

Proof of Theorem 3.2

The goal is to prove that Online ListUpdate satisfies, for all \(g\),

\[[_{T}(,g)] O((d T)^{1/3}T^{2/3}+ ).\]

Fix any \(g\). For each \(h\), define

\[_{T}(h,g):=_{t=1}^{T}g(x_{t})((_{t},y_{t})-(h(x _{t}),y_{t})).\]

By linearity of expectation, Theorem 3.1, and an elementary integral bound,

\[[_{T}(h,g)] =_{t=1}^{T}[g(x_{t})(_{t},y_{t})-[g(x_{t})(h(x_{t}),y_{t})]\] \[ 1+_{t=2}^{T}P(g)([(f_{t}(x_{t}),y_{t} ) x_{t} g]-[g(x_{t})(h(x_{t}),y_{t}) x_{t} g])\] \[ 1+_{t=2}^{T}((1-) O(()^{1/3})+)\] \[=1+O((d T+(1/))^{1/3}T^{2/3}+ T).\]

Plug-in \(=1/T\) to obtain

\[[_{T}(h,g)] O((d T)^{1/3}T^{2/3}).\]

It remains to relate \(_{h}[_{T}(h,g)]\) to \([_{T}(,g)]\). Define

\[h_{g}*{arg\,min}_{h}_{(x,y)}[g (x)(h(x),y)]_{g}*{arg\,min}_{h }_{t=1}^{T}g(x_{t})(h(x_{t}),y_{t}).\]

Then

\[[_{T}(,g)]=_{h}[ _{T}(h,g)]+[_{t=1}^{T}g(x_{t})((h_{g}(x_{t},y_{t})-(_{g}(x_{t}),y_{t}))].\]

Since \(\) has VC dimension at most \(d\), a standard uniform convergence argument implies

\[[_{h}T_{(x,y)}[g(x)(h(x ),y)]-_{t=1}^{T}g(x_{t})(h(x_{t}),y_{t})] O().\]

Using the definitions of \(h_{g}\) and \(_{g}\), we obtain

\[[_{t=1}^{T}g(x_{t})((h_{g}(x_{t},y_{t})-(_{ g}(x_{t}),y_{t}))] O().\]

Therefore, we conclude that

\[[_{T}(,g)] O((d T)^{1/3}T^{2/3} +).\]

This finishes the proof of Theorem 3.2. \(\)Proof of Main Theorem 4.1

In this section, we prove the multi-group regret guarantee of our main algorithm, Algorithm 1. To restate the theorem, we aim to show, for all \(g\),

\[[_{T}(,g)] O(}+ T).\]

More explicitly, we aim to show that:

\[_{t=1}^{T}[g(x_{t})((h_{t}(x_{t}),y_{t})-(h^{*}(x_{ t}),y_{t}))] O(}+ T),\]

where \(h^{*}_{h}_{t=1}^{T}g(x_{t})(h(x_{t}),y_{t}).\) We follow a generalization of the online minimax multiobjective optimization framework of , with techniques inspired by .

### The AMF Algorithm Framework

We first restate their "adversary-moves-first" AMF algorithm of  and its main regret guarantee for convenience, as we will need to adapt and generalize it to our setting. Let \(_{t}\) denote a general action space of the learner, and let \(_{t}\) denote the general action space of the adversary at round \(t[T]\). In full generality, \(_{t}\) and \(_{t}\) are allowed to change with the rounds \(t[T]\). We differentiate this from the action space \(\) of the main body. For each round \(t=1,,T\), consider the following setting, which we refer to as the _multiobjective online optimization problem_:

1. The adversary selects a continuous, \(d\)-dimensional loss function \(r_{t}:_{t}_{t}[-1,1]^{d}\). Each component \(r_{t}^{j}:_{t}_{t}[-1,1]\) is convex in \(_{t}\) and concave in \(_{t}\).
2. The learner selects an action \(a_{t}_{t}\).
3. Nature observes the learner's action \(a_{t}\) and responds with \(z_{t}_{t}\).
4. The learner incurs the \(d\)-dimensional loss \(r_{t}(a_{t},z_{t})\).

In this setting, the learner's goal is to minimize the value of the maximum dimension of the accumulated loss vector after \(T\) rounds:

\[_{j[d]}_{t=1}^{T}r_{t}^{j}(a_{t},z_{t}).\]

To benchmark the learner's performance, we consider the following quantity, which we refer to as the _adversary-moves-first (AMF) value at round \(t\)_.

**Definition B.1** (Adversary-Moves-First (AMF) Value at Round \(t\),).: _The adversary-moves-first (AMF) value at round \(t\) is the value:_

\[v_{t}^{A}:=_{z_{t}_{t}}_{a_{t}_{t}}( _{j[d]}r_{t}^{j}(a_{t},z_{t})).\] (6)

We conceive of the value \(v_{t}^{A}\) in (6) as the aspirational smallest value of the maximium coordinate of \(r_{t}\) the learner could guarantee _if_ the adversary had to reveal \(z_{t}\) first and the learner could best respond with \(a_{t}\). Per how the multiobjective online optimization problem is set up, however, the opposite is true -- the learner must commit to an action \(a_{t}_{t}\) first, and _then_ the adversary is allowed to play \(z_{t}_{t}\) in response to maximize the learner's misfortune. Regardless, we can define a notion of regret with respect to this particular benchmark.

**Definition B.2** (Adversary-Moves-First (AMF) Regret).: _Over \(T\) rounds of the above multiobjective online optimization problem, the adversary-moves-first regret of the learner is:_

\[_{T}:=_{j[d]}(_{t=1}^{T}r_{t}^{j}(a_{t},z_{t})- v_{t}^{A}).\]

This notion measures the cumulative regret the learner has for not playing the action \(a_{t}_{t}\) achieving the aspirational value \(v_{t}^{A}\) at round \(t\) over _all_\(d\) coordinates of the loss vector.

A natural question, then, is to wonder if such a regret can be made to diminish sublinearly. That is, does there exist an algorithm such that \(_{T} o(T)\)?  answer this in the affirmative, presenting Algorithm 2.

```
1:for\(t=1,2,3,,T\)do
2: Receive adversarially chosen action spaces \(_{t}\) and \(_{t}\) and the \(d\)-dimensional loss function \(r_{t}:_{t}_{t}[-1,1]^{d}\).
3: Let \[q_{t}^{j}:=^{t-1}r_{s}^{j}(a_{s},z_{s}))}{ _{i[d]}(_{s=1}^{t-1}r_{s}^{i}(a_{s},z_{s}))}\ j[d].\] For \(t=1\), let \(q_{t}^{j}=1/d\) for all \(j[d]\).
4: Solve the min-max optimization problem: \[a_{t}*{arg\,min}_{a_{t}}_{z_ {t}}_{j[d]}q_{t}^{j}r_{t}^{j}(a,z).\] (7)
5: Commit to the action \(a_{t}_{t}\), observe Nature's choice \(z_{t}_{t}\), and incur loss \(r_{t}(a_{t},z_{t})\).
6:endfor ```

**Algorithm 2** AMF Algorithm of 

It is not immediately clear at first glance how Algorithm 2 translates to our multi-group online learning setting. In the next section, we will show a reduction from our setting to the this AMF framework. We will specialize Algorithm 2 to our setting and prove a "meta-theorem" similar to the original guarantee of Algorithm 2 and proceed to control the regret via that meta-theorem in subsequent sections.

### Reduction of multi-group online learning to AMF Framework

Recall that, in this paper, we actually care about the _online multi-group learning setting_ described in Section 2.2. We restate the objective here for convenience.

In multi-group online learning, the learner has access to a hypothesis class \(\) taking contexts from \(\) and outputting actions in \(\), a common action space for the learner and Nature. There is a common loss function for the problem, \((,):\). To allow confusion, we refer to the adversary in the multi-group online setting of Section 2.2 as "Nature" and the adversary in the online multiobjective optimization problem of Section B.1 as "the adversary." The learners in both settings correspond to one another, so we just use "the learner." For clarity of exposition, we let \(=\{-1,1\}\) be a binary action space for the remainder of Appendix B. The generalization to the case where \(\) takes \(K\) discrete values is sketched in Appendix C.

We consider the regret of the learner on subsequences of rounds \((t[T]:x_{t} g)\) defined by the groups \(g\) and the sequence of contexts \(x_{1},,x_{T}\). Specifically, the _(multi-group) regret of the learner on group \(g\)_ is

\[_{T}(,g):=_{t=1}^{T}g(x_{t})(_{t},y_{t}) -_{h}_{t=1}^{T}g(x_{t})(h(x_{t}),y_{t}).\] (8)

Crucially, the best hypothesis for one group may differ from that of another group. The learner seeks to achieve achieve sublinear expected regret, on all groups \(g\) simultaneously.

We now show a reduction from the multi-group online learning setting to the general multiobjective online optimization problem of the previous Section B.1. An important observation is that, when \(=\{-1,1\}\), varying \(h_{t}\) only affects the regret at round \(t[T]\) insofar as its behavior on \(x_{t}\). That is, for a fixed \(x_{t}\) and \(y\), \((h_{t}(x_{t}),y)\{(-1,y),(1,y)\}\). Note that it is possible for the set \(\{h(x_{t}):h\}\) to be a singleton set, in which case the learner will always take this unique action.

* Let \(_{t}\), the learner's action space for round \(t[T]\) in Section B.1, be the simplex \(()\).

* Let \(_{t}\), the adversary's action space in Section B.1, be \(_{t}=\) for all \(t[T]\), which will correspond to the parameter of a Bernoulli distribution over the binary-valued action space \(\) of Nature in the multi-group online learning problem.
* Let \(r_{t}:_{t}_{t}[-1,1]^{d}\), the adversarially chosen loss function, be \(\)-dimensional, with each coordinate corresponding to a pair \((,)\). Consider any \(p,\), each determining a Bernoulli distribution over \(=\{-1,1\}\). As in Section 4.2, for any \(x\): \[_{x}((,),(y^{},y)):=(x)( (y^{},y)-((x),y)).\] Then, define: \[r_{t}^{(,)}(p,):=_{y^{} p}_{y()}[_{x_{t}}((,),(y^{ },y))].\] (9) Above, the loss \((,)\) is the fixed loss of the multi-group online learning problem, and \(x_{t}\) is the context chosen by Nature at round \(t\), which indexes \(r_{t}\).

It may now be slightly clearer how Algorithm 1 maps to Algorithm 2, but we provide a high-level overview here to prepare the reader for the subsequent sections.

* The distribution \(q_{t}[d]\) in Line 3 of Algorithm 2 corresponds to the implicit distribution formed by querying \(^{}_{(,)}\)\(M\) times to generate \(\{(_{i},_{i})\}_{i=1}^{M}\), i.e. the \((,)\)-player.
* Solving the min-max optimization problem in Line 4, Equation (7) of Algorithm 2 corresponds to the two calls to the simple optimization problem solved by the \(\)-player and solving the simple one-dimensional linear program in Line 8 of Algorithm 1.

We make these correspondences formal in the subsequent sections. To organize this, we first formally show the correspondence between the \((,)\)-player and the construction of \(q_{t}\), and the \(\)-player and Equation (7) in Algorithm 2.

### Instantiations of the \(^{}_{(,)}\) oracle

The computational primitive our algorithm assumes access to is a \((,)\)-optimization oracle, defined in Definition 2.2, and requoted here for ease of reference.

**Definition B.3** (\((,)\)-optimization oracle).: _Fix an error parameter \( 0\). For a collection of groups \( 2^{}\), a collection of hypotheses \(^{}\), and a sequence of \(m\) loss functions \(_{i}:(\{0,1\})()[- 1,1]\), an \(\)-approximate \((,)\)-optimization oracle \(^{}_{(,)}\) is an \(\)-approximation optimization oracle (Definition 2.1) that outputs a pair \((,)\) satisfying:_

\[_{i=1}^{m}w_{i}_{i}(((x_{i}),(x_{i})),(y_{i},y^{ }_{i}))_{(g^{*},h^{*})}_{i=1 }^{m}w_{i}_{i}((g^{*}(x_{i}),h^{*}(x_{i})),(y_{i},y^{}_{i}))-.\] (10)

A common assumption in the literature on oracle-efficient online learning is positing the existence of some reasonable optimization oracle, typically commensurate to the ability to solve ERM. Although it is well-known that ERM is computationally hard in the worst-case, a bedrock of modern machine learning is the assumption that ERM is at least heuristically and approximately solvable. Although, for the purposes of our work, we assume access to this oracle as a black-box, it is natural to wonder if such an oracle can be instantiated.  gives two such instantiations which we quote here for completeness.

We consider the specific instantiation of the \((,)\)-oracle in Algorithm 1 for a specific round \(t[T]\), which aims to solve the following optimization problem for some \((_{t},_{t})\):

\[_{s=1}^{t-1}_{t}(x_{s})((_{s},y_{s})-(_{t}(x_{s}),y_{s}))-,\] (11)

where \(:=_{g^{*},h^{*}(,)}_{s=1}^{t-1}g^ {*}(x_{s})((_{s},y_{s})-(h^{*}_{t}(x_{s}),y_{s}))\).

In both instantiations of the oracle in , the oracle aims to find a \((g,h)(,)\) competitive to some reference model, \(f:\{0,1\}\). For simplicity, as in the main body, we assume that \(=\{0,1\}\), and the "reference model" we compete with is given by the Learner's history of actions up to round \(t\): \((x_{1},_{1}),,(x_{t-1},_{t-1})\). That is, we compare with the function \(f:\{0,1\}\) that maps \(f(x_{s})=_{s}\) for all \(s=1,,t-1\).

**Reduction to ternary classification.** The first instantiation of a \((,)\) oracle in  reduces the optimization oracle to the existence of a solver for a weighted ternary classification problem. The exposition here is quoted directly from .

Start with a class \(\) of _ternary_ valued functions \(p:\{0,1,?\}\). For each \(p\), define the _\(p\)-derived group_ and _\(p\)-derived hypothesis_ as:

\[g_{p}(x)=1&p(x)\{0,1\}\\ 0&p(x)=? h_{p}(x)=p(x)&p(x) \{0,1\}\\ 0&p(x)=?\]

This class \(\) induces a set of pairs \((g_{p},h_{p})\) and a product class \((,)_{}:=\{(g_{p},h_{p}):p\}\). We may now define a _cost-sensitive classification_ problem over \(\) as follows, given an existing model \(f:\{0,1\}\), with the following costs:

\[c_{f}((x,y),z):=0&z=?\\ 1&f(x)=y z\\ -1&z=y f(x)\\ 0&.\]

For any distribution \(\) over \(\{0,1\}\), the associated cost-sensitive classification problem for costs \(c_{f}((x,y),z)\) defined above is:

\[p^{*}*{arg\,min}_{p}_{(x,y)}[c _{f}((x,y),p(x))].\] (12)

Many efficient algorithms that heuristically solve such optimization problems exist.

The main theorem from , restated here, is the following:

**Theorem B.1**.: _Fix any arbitrary distribution \(\) over \(\{0,1\}\). Let \(\) be a class of ternary-valued functions \(p:\{0,1,?\}\) and let \(f:\{0,1\}\) be any binary-valued model. Let \(p^{*}\) be the solution to the cost-sensitive classification problem in Equation (12). Then,_

\[(g_{p}^{*},h_{p}^{*})*{arg\,max}_{(g,h)(, )_{}}_{(x,y)}[g(x)((f(x ),y)-(h(x),y))].\]

_When \(\) is the empirical distribution over \(x_{1},,x_{t-1}\), the solution \((g_{p}^{*},h_{p}^{*})\) forms a solution to the optimization problem in Equation (11) when \((,)_{}=\)._

We refer the reader to Section 4.2 in  for a proof.

**Reduction to alternating maximization.** Another instantiation of the \((,)\)-oracle in  is an alternating maximization approach. The reduction to ternary classification quoted above relies on an oracle for the class \(\) and supplies guarantees for the derived class \((,)_{}\). However, if we wish to begin with \(\), we can take an "EM-style" alternating maximization approach that only requires ERM oracles for \(\) and \(\) separately. This approach only guarantees a saddle point local optimum.

The main idea is that, by holding \(g\) fixed and solving for \(h^{*}\), and vice versa, the following optimization problems are no harder than ERM over \(\) and \(\) individually:

\[g^{*}*{arg\,max}_{g^{*}} _{(x,y)}[g^{*}(x)((f(x),y)-(h(x),y))]\] (13) \[h^{*}*{arg\,max}_{h}_{(x, y)}[g(x)((f(x),y)-(h^{*}(x),y))].\] (14)

In Equation (13), \(h\) is fixed and we solve for \(g^{*}\); in Equation (14), \(g\) is fixed, and we solve for \(h^{*}\). With appropriate modifications to the distribution \(\) we can construct ERM problems for \(g^{*}\) and \(h^{*}\) commensurate to solving Equations (13) and (14). We refer the reader to Lemmas 21 and 22 in  for the proofs.

This allows us to state an alternating maximization algorithm for finding a saddle point \((g,h)\) that gives a local optimum to Equation (11) in Algorithm 3. The corresponding theorem, restated from  is:

**Theorem B.2**.: _Let \(>0\). Fix any empirical distribution over \((x_{1},y_{1}),,(x_{m},y_{m})\), let \(f:\{0,1\}\) be an arbitrary model, and let \(\) and \(\) be arbitrary group and hypothesis classes. After solving at most \(2/\) ERM problems over each of \(\) and \(\) (in Equations (13) and (14), respectively), Algorithm 3 returns a pair \((g^{*},h^{*})\) with the properties that:_

1. _For every_ \(h\)_,_ \[_{i=1}^{m}g^{*}(x_{i})((f(x_{i}),y_{i})-(h(x_{i}),y_{i})) _{i=1}^{m}g^{*}(x_{i})((f(x_{i}),y_{i})-(h^{*}(x_{i}),y_{ i}))+.\]
2. _For every_ \(g\)_,_ \[_{i=1}^{m}g(x_{i})((f(x_{i}),y_{i})-(h^{*}(x_{i}),y_{i})) _{i=1}^{m}g^{*}(x_{i})((f(x_{i}),y_{i})-(h^{*}(x_{i}),y_{ i}))+.\]

```
0: Dataset \(\{(x_{i},y_{i})\}_{i=1}^{m}\), a model \(f:\{0,1\}\), error parameter \(\).
1: Initialize \((g^{*},h^{*})\) arbitrarily.
2: Let \[:=_{i=1}^{m}g^{*}(x_{i})((f(x_{i}),y_{i}))- (h^{*}(x_{i}),y_{i}))\]
3: Use ERM oracle for Equations (13) and (14) to solve for: \[g^{*} *{arg\,max}_{g}_{i=1}^{m}g(x) ((f(x),y)-(h^{*}(x),y))\] \[h^{*} *{arg\,max}_{h}_{i=1}^{m}g^{*}( x)((f(x),y)-(h(x),y))\]
4:while\(_{i=1}^{m}g^{*}(x)((f(x),y)-(h^{*}(x),y))+\)do
5: Let \[:=_{i=1}^{m}g^{*}(x_{i})((f(x_{i}),y_{i}))- (h^{*}(x_{i}),y_{i}))\]
6: Use ERM oracle for Equations (13) and (14) to solve for: \[g^{*} *{arg\,max}_{g}_{i=1}^{m}g(x) ((f(x),y)-(h^{*}(x),y))\] \[h^{*} *{arg\,max}_{h}_{i=1}^{m}g^{*}( x)((f(x),y)-(h(x),y))\]
7:endwhile
8: Return \((g^{*},h^{*})\). ```

**Algorithm 3** Alternating Maximization for \(\) Oracle

We believe that it is an interesting and worthwhile open question to develop more specific instantiations of this \((,)\)-oracle for more specific problem settings that are computationally efficient and have provable optimization guarantees.

### The group-hypothesis and hypothesis players

We formally define how the \((,)\)-player corresponds to the weights \(q_{t}[d]\) in Algorithm 2. The crucial observation here is that the perturbations of the FTPL algorithm of  used in our setting form an implicit distribution over \(\) that can be approximated by calling the \(^{}_{(,)}\) oracle \(M\) times.

In the proceeding sections, we denote \(()\) as the (possibly infinite-dimensional) space of measures over the functions \(\). However, we will always only access sparse distributions on thisspace, with a finite number of \((g,h)\) pairs in \(\) obtaining nonzero mass. The following definition should make this clear.

**Definition B.4** (The distribution of the \((,)\)-player).: _For any round \(t[T]\), let \(\{(_{i},_{i})\}_{i=1}^{M}\) be the \(M\) samples drawn from querying \(^{}_{(,)}\) in Algorithm 1. Let the empirical distribution \(_{t}()\) be the distribution of the \((,)\)-player at round \(t\)._

It is easy to see that \(_{t}\) is a valid distribution over \((,)\), with measure over \(A\), defined by:

\[P_{M}(A):=_{i=1}^{M}_{(_{i},_{i})}(A),\]

where \(_{(_{i},_{i})}\) is the Dirac measure of \((_{i},_{i})\) falling into the set \(A\). The stochasticity of \((_{i},_{i})\) is over the random perturbations described in Section 4.2, Equation (4). Equipped with this definition, we can take empirical expectations over \(_{t}()\) in the usual way. Observe that, in Algorithm 2, Equation (7), the optimization problem at round \(t\), is equivalent to:

\[a_{t}*{arg\,min}_{a}_{z} _{j q_{t}}[r_{t}^{j}(a,z)].\]

Because the \(d\) objectives in our reduction (Section B.2) correspond to each group-hypothesis pair \((g,h)\), \(_{t}\) corresponds to \(()\), and \(_{t}\) always corresponds to \(\), we can equivalently consider the min-max optimization problem at round \(t[T]\):

\[p_{t}*{arg\,min}_{p()}_{[0,1 ]}_{(,)_{t}}[r_{t}^{(, )}(p,)],\] (15)

where \(r_{t}^{(,)}\) is defined in Equation 9. The next lemma relates the optimization procedure of the \(\)-player in Algorithm 1 to the min-max optimization procedure of Equation (7) in Algorithm 2.

**Lemma B.3** (The optimization of the \(\)-player).: _For any round \(t[T]\), let \(\{(_{i},_{i})\}_{i=1}^{M}\) denote the \(M\) samples obtained by calling \(^{}_{(,)}\)\(M\) times in Algorithm 1, and denote \(_{t}()\) denote the corresponding empirical distribution (Definition B.4). Then, \(p_{t}()\) defined in Equation (15) above is equivalent to the distribution \((p,1-p)(\{-1,1\})\) obtained from solving the linear program of the \(\)-player in Algorithm 1._

Proof.: Consider any round \(t[T]\). Observe that Line 8 of Algorithm 1 is equivalent to solving the linear program for \(\) and \(:=(p,1-p)^{2}\) :

\[\ \] \[_{_{i}}=1\] \[^{}e_{i} i\] \[_{i} 0 i\]

where the payoff matrix \([-1,1]^{2 2}\) has the coordinates \((y^{},y)\):

\[_{(y^{},y)}:=_{i=1}^{M}_{x_{i}}(( _{t}^{(i)},_{t}^{(i)}),(y^{},y)),\]

and \(e_{i}\) is the \(i\)th coordinate vector of \(^{2}\). Let \(()\) denote the space of probability distributions over \(\). Let \(=(p,1-p)\) and \(=(,1-)\), and, as shorthand, denote \((-1)=1-p\), \((1)=p\), \((-1)=1-\), and \((1)=\).

By the equivalence of linear programs (LPs) to zero-sum min-max games (see, e.g., ), obtaining the optimal \(\) for this LP is the equivalent to solving:

\[_{()}_{()}^{}z =_{()}_{()}_{y}_{y^{}}_{i=1}^{M} (y^{})(y)_{x_{t}}((_{t}^{(i)},_{t}^{(i)}),(y^{},y))\] \[=_{()}_{( )}_{y}_{y^{}}(y^{ })(y)_{(,)_{t}}[ _{x_{t}}((,),(y^{},y))]\] \[=_{()}_{( )}_{y^{}}_{y( )}[_{(,)_{t}}[ _{x_{t}}((,),(y^{},y)]]\] \[=_{p()}_{}_{ y^{}(p)}_{y()}[ _{(,)_{t}}[_{x_{t}} ((,),(y^{},y)]]\] \[=_{p()}_{}_{ (,)_{t}}[r_{t}^{(,)}(p, )]\]

Above, the first equality just comes from definition of \(\), the second equality is from the Definition B.4 of the empirical distribution \(_{t}\), the third and fourth equalities are from the definition of \(\) and \(\) in the previous paragraph. The final equality is just from interchanging the order of expectation and the definition of \(r_{t}^{(,)}(p,)\) in Equation (9). By this chain of inequalities, we see that obtaining the optimal \(\) for the original LP corresponds exactly to the choice of \(p_{t}\) in Equation (15). 

Lemma B.3 tells us that the strategy of the \(\)-player (i.e., Lines 8 and 9 in Algorithm 1) to obtain \(h_{t}\) is exactly the same as obtaining the minimizing \(p_{t}\) in Equation (15). From the exposition above, this corresponds to the min-max optimization problem in Equation (7) when \(q_{t}\) is \(_{t}\), the distribution over \(\). We now proceed to prove a more general "meta-theorem" from which the regret guarantee of Algorithm 1 will follow once we plug in a specific FTPL algorithm for the \((,)\)-player.

### Meta-algorithm for Online Multi-group Learning

We now present a meta-algorithm, Algorithm 4, and its corresponding Theorem B.4, the "meta-theorem" for online multi-group learning from which Theorem 4.1 follows. This is more general, however, than the guarantee in Theorem 4.1, and we emphasize that, through changing the no-regret algorithm for the \((,)\)-player, we can obtain regret guarantees for settings other than the smoothed online learning setting of Theorem 4.1. Section 5 gives a couple of examples, which we elaborate in Appendix C.

Note that approximating the distribution of the \((,)\)-player is crucial to obtain computational efficiency, as we cannot hope to enumerate \(\) and \(\) by explicitly representing \(q_{t}\) in Algorithm 4. Thus, sampling \(M\) times is a crucial "sparsification" step that allows us to implicitly access the distribution over \(\) that the \((,)\)-player maintains.

We prove Theorem B.4 through techniques similar to the regret guarantee proof of Algorithm 2 in . Namely, we observe that by using the reduction outlined in Section B.2 with the \(\)-dimensional loss

\[r_{t}^{(,)}(p,) =_{h(x_{t}) p}_{y()} [(x_{t})((h(x_{t}),y)-((x_{t}),y))]\] \[=_{h(x_{t}) p}_{y()} [((,),(h(x_{t}),y))]\]

we may obtain a bound on the multi-group regret of our Algorithm 4 by obtaining an adversary-moves-first regret guarantee (Definition B.2). For simplicity, we prove this for the case of binary actions \(=\{-1,1\}\); we outline how to obtain a similar theorem for multi-class action spaces in Appendix C.

**Theorem B.4** (Meta-Theorem for Online Multi-group Learning).: _Let \(\) be a context space, let \(:=\{-1,1\}\) be a binary action space, and let \(\) be a hypothesis class of functions \(h:\), and let \(\) be a collection of groups, \(g:\{0,1\}\). Let \(:\) be a bounded loss function. Suppose the \((,)\)-player of Algorithm 4 is instantiated with a no-regret (maximization) algorithm operating over \(\) that has the guarantee that for any sequence of losses of length \(T\) bounded in \([-1,1]\), by playing (a possibly implicit distribution) \(q_{t}\), it has regret at most \(R(T)\) in expectation._Then, with expectation over any randomness of the \((,)\)-player and Nature, Algorithm 1 obtains the multi-group regret guarantee of:_

\[[_{T}(,g)]_{t=1}^{T}v_{t}^{A}+_{t=1} ^{T}[_{t}(M)]+R(T),\] (16)

_for all \(g\), where_

\[v_{t}^{A}:=_{}_{p_{t}()}_{( ,)}_{h p_{t},y ()}[(x_{t})((h(x_{t}),y)-( (x_{t}),y))]\] (17)

_and \(_{t}(M)\) is the error incurred from estimating \(q_{t}\) with \(_{t}\) with \(M\) samples at step \(t[T]\)._

Proof.: From the perspective of the \((,)\)-player, who is running a regret maximization algorithm, the following game is being played. For \(t=1,2,3,,T\):

* Receive a context \(x_{t}\) from Nature, possibly adversarially and depending on the past \(t-1\) rounds.
* Play a pair \((_{t},_{t})\), possibly randomly and dependent on the last \(t-1\) rounds, where pairs are functions \[(h,g):\{0,1\}.\]
* Commit to the action \(_{t}:=(_{t}(x_{t}),_{t}(x_{t})) \{0,1\}.\)* An adversary (the \(\)-player's prediction _and_ Nature) reveals \((_{t},y_{t})\), and we incur the loss: \[((,),(_{t},y_{t})):=(x_{t}) ((_{t},y_{t})-(_{t}(x_{t}),y_{t})).\] Here, \(_{t}=h_{t}(x_{t})\), which is a random variable depending on sampling from \(p_{t}\), the Bernoulli distribution of the \(\)-player at round \(t\).

Note that the \((,)\)-player is attempting to _maximize_ this loss.

Let \(h_{1},,h_{T}\) be the sequence of hypotheses chosen by the \(\)-player, and let \(y_{1},,y_{T}\) be the sequence of adversarially chosen outcomes. Then, by the regret guarantee of the \((,)\)-player's no-regret algorithm algorithm, for any \((h^{*},g^{*})\):

\[_{t=1}^{T}[((g^{*},h^{*}),(h_{t}(x_{t}),y_{t}) )]-_{t=1}^{T}_{(_{t},_{t}) q_{t}} [((_{t},_{t}),(h_{t}(x_{t}),y_{t}))]  R(T),\]

where the expectation is over the distributions \(q_{t}\) over \(()\) that the \((,)\)-player commits to at each round _and_ the random choices of Nature and the \(\)-player's random choice of \(_{t}\) at each round. To ease notation, we keep the subscript in the expectation over \(h_{t}\) hidden, noting that \(h_{t}(x_{t}) p_{t}\) is a random variable throughout. For instance, in the case of Theorem 4.1, this is a random process determined by the \((,)\)-player sampling \(n\) perturbation terms and calling the \((,)\)-oracle to obtain the random variable \((_{t},_{t})\). Instantiating the regret bound for all \((h^{*},g^{*})\) gives us:

\[_{(h^{*},g^{*})}_{t=1}^{T} [((g^{*},h^{*}),(h_{t}(x_{t}),y_{t}))]_{t=1}^{ T}_{(_{t},_{t}) q_{t}}[(( _{t},_{t}),(h_{t}(x_{t}),y_{t}))]+R(T).\]

However, we do not have direct access to \(q_{t}\), the implicit distribution over \(\), but we have an approximation \(_{t}\). In Algorithm 2, \((_{t},_{t})\) is drawn according to \(_{t}\), the empirical distribution over \(()\) formed by drawing \(M\) samples \(\{(_{t}^{(i)},_{t}^{(i)}\}_{i=1}^{M}\) from \(_{t}\). Fixing \(x_{t},y_{t}\), and \(h_{t}(x_{t})\), let \(_{t}(M)\) be the error we incur from replacing the true distribution \(q_{t}\) by the empirical distribution \(_{t}\), which we can handle through uniform convergence on the samples \(\{(_{t}^{(i)},_{t}^{(i)}\}_{i=1}^{M}\) (see Lemma B.6):

\[|_{(,)_{t}}[(( {g},),(h_{t}(x_{t}),y_{t}))]-_{(g,h) q_{t}}[((g,h),(h_{t}(x_{t}),y_{t}))]|_{t}(M).\]

Adding the estimation error at each \(t[T]\), our regret bound becomes:

\[_{(h^{*},g^{*})} _{t=1}^{T}[((g^{*},h^{*}),(h_{t}(x_ {t}),y_{t}))]_{t=1}^{T}_{(_{t},_{t }) q_{t}}[((_{t},_{t}),(h_{t}(x_{t}),y_ {t}))]+R(T)\] \[_{t=1}^{T}_{(_{t},_{t}) _{t}}[((_{t},_{t}),(h_{t}(x_{t}),y_ {t}))]+_{t=1}^{T}_{t}(M)+R(T).\]

Now, we aim to bound the terms \(_{(_{t},_{t})_{t}}[(( _{t},_{t}),(h_{t}(x_{t}),y_{t}))]\). At round \(t[T]\), Algorithm 4 chooses the best response \(h_{t}\) by solving a linear program for a Bernoulli parameter \(p\) and then sampling \(h\) from \((p)\). This is equivalent to sampling \(h_{t}(x_{t}) p_{t}\), where \(p_{t}:=(p,1-p)\) on \(()\). We use sampling from this Bernoulli distribution and sampling from \(p_{t}\) interchangeably. By Lemma B.3, this is equivalent to solving the min-max optimization problem in Equation (15)

\[p_{t}*{arg\,min}_{p()}_{} _{(_{t},_{t})_{t}}[r_{t}^{(_{t},_{t})}(p,)],\]

which, by definition of \(r_{t}^{(,)}(p,)\), is equivalent to:

\[p_{t}*{arg\,min}_{p_{t}()}_{}_{(_{t},_{t})_{t}}[_{ h_{t}(x_{t}) p_{t}}_{y()}[_{t}(x_ {t})((h_{t}(x_{t}),y)-(_{t}(x_{t}),y)))] ].\] (18)

The inner expectations are linear in \(p_{t}\) and \(\), and taking the outer expectation over \(_{t}\) maintains linearity. Therefore, this is a convex and concave min-max optimization problem, and von Neumann'sminimax theorem  applies, allowing us to swap the order of minimization and maximization. Therefore

\[_{p_{t}()}_{}_{( _{t},_{t})_{t}}[_{h_{t}(x_{t})  p_{t}}_{y()}[_{t}(x_{t}) ((h_{t}(x_{t}),y)-(_{t}(x_{t}),y)))]]\] \[=_{}_{p_{t}()}_{(_{t},_{t})_{t}}[_{h_{t}(x_{t }) p_{t}}_{y()}[_{t}(x_{t}) ((h_{t}(x_{t}),y)-(_{t}(x_{t}),y)))]]\] \[_{}_{p_{t}()}_{ (,)}_{h_{t}(x_{t})  p_{t},y()}[(x_{t})((h_{t}(x_{ t}),y)-((x_{t}),y))]=v_{t}^{A}\]

where the first equality is from the minimax theorem and the second inequality is just because averages are less than or equal to maximums. Combining all the inequalities, we obtain

\[_{(h^{*},g^{*})}_{t=1}^{T}( (g^{*},h^{*}),(h_{t}(x_{t}),y_{t}))_{t=1}^{T}v_{t}^{A}+_{t=1}^{T} _{t}(M)+R(T),\]

and our theorem follows from taking an expectation on both sides and substituting back the definition of

\[((g^{*},h^{*}),(h(x_{t}),y_{t})):=g^{*}(x_{t})((h(x_{t}),y_{t})-(h^{*}(x_{t}),y_{t})),\]

because each \(((g^{*},h^{*}),(h(x_{t}),y_{t}))\) is simply the per-round regret. 

With Theorem B.4 in hand, it remains to make sure that the two terms \(_{t=1}^{T}v_{t}^{A}\) and \(_{t=1}^{T}[_{t}(M)]\) are both \(o(T)\). Then, if we have a no-regret algorithm with \(o(T)\) regret while only accessing \(\) and \(\) through the \(_{(,)}^{n}\) oracle, we will have a multi-group online learning algorithm. The next two lemmas show that both terms are, indeed, \(o(T)\).

First, we bound the values \(v_{t}^{A}\), which are known as the "AMF values" in the framework of .

**Lemma B.5**.: _For any \(t[T]\), the AMF value of the game at round \(t\), is nonpositive, i.e._

\[v_{t}^{A}:=_{}_{p_{t}()}_{(h,g )}[g(x_{t})((h_{t}(x_{t}),y)- (h(x_{t}),y))] 0,\]

_where the expectation is taken over \(h_{t} p_{t}\) and \(y()\)._

Proof.: Fix any parameter \(\) for the \(\) player. Then, for any \((h,g)\),

\[[g(x_{t})((h_{t}(x_{t}),y)-(h(x_{t}),y)) ]= _{h_{t} p_{t}}[g(x_{t})((h_{t}(x_{t}),1) -(h(x_{t}),1))]\] \[+(1-)_{h_{t} p_{t}}[g(x_{t})((h_{t}(x_{t }),-1)-(h(x_{t}),-1))].\]

Expanding the expectation over \(h_{t} p_{t}\), this is equivalent to:

\[g(x_{t})_{h^{}}p_{h^{}}^{t}((h^{ }(x_{t}),1)+(1-)(h^{}(x_{t}),0))-g(x_{t})((h(x_{t}), 1)+(1-)(h(x_{t}),0)),\]

so it suffices to find \(p_{t}()\) such that, for all \((h,g)\),

\[g(x_{t})_{h^{}}p_{h^{}}^{t}((h^{ }(x_{t}),1)+(1-)(h^{}(x_{t}),0)) g(x_{t})((h(x_{ t}),1)+(1-)(h(x_{t}),0)).\]

The \(g(x_{t})\) value is the same for both sides, so it really suffices to find the \(p_{t}()\) such that, for all \(h\),

\[p_{h^{}}^{t}((h^{}(x_{t}),1)+(1-)(h^{}(x_ {t}),0))(h(x_{t}),1)+(1-)(h(x_{t}),0).\]

Because we know \(\), the Bernoulli parameter for the true distribution of \(y x_{t}\), we can choose \(p^{t}()\) to put all its mass on the \(h^{*}\) that minimizes this risk, i.e.

\[h^{*}*{arg\,min}_{h}_{y}[(h(x_{t}),y ) x_{t}].\]

This is, by definition, exactly the \(h^{*}\) such that, for any \(h\),

\[(h^{*}(x_{t}),1)+(1-)(h^{*}(x_{t}),-1)(h(x_{t }),1)+(1-)(h(x_{t}),-1).\]Therefore, we have that:

\[v_{t}^{A} =_{}_{p_{t}()}_{(h,g) }_{h^{} p_{t},y()}[g(x_{t})((h^{}(x_{t}),y)-(h(x_{t}),y))]\] \[_{,(h,g)} _{y()}[g(x_{t})((h^{*}(x_{t}),y)-(h(x_{ t}),y))]\] \[ 0.\]

The final inequality follows from our argument above. 

Next, we bound the expected approximation error we incur from replacing \(q_{t}\) with the empirical distribution \(_{t}\) obtained from the \(M\) samples \(\{(_{t}^{(i)},_{t}^{(i)})_{i=1}^{M},\) which we denoted as \(_{t}(M).\) This comes from a standard uniform convergence argument.

**Lemma B.6**.: _Let \(t[T]\) and \(x_{t}\) be fixed, and consider the function \(_{x_{t}}((g,h),(y^{},y)):=g(x_{t})((y^{},y)-( h(x_{t}),y)).\) Let \(||=k<.\) If \(M T^{1+}\), where \(=()\), then over the randomness of drawing \(M\) samples \(\{(_{t}^{(i)},_{t}^{(i)})_{i=1}^{M}\) to construct the empirical distribution \(_{t}\) described in Definition B.4, for all \(y^{},y\), let \(_{t}(M)\) be defined as the supremum_

\[_{t}(M):=_{(y^{},y)}| _{(,)_{t}}[_{x_{t}}(( ,),(y^{},y))]-_{(g,h) q_{t}}[_{x_{t}}((g,h),(y^{},y))]|,\]

_and, over all \(T\) rounds,_

\[[_{i=1}^{T}_{t}(M)] 2.\]

Proof.: Fix any round \(t[T].\) We use a standard uniform convergence argument to ensure that the empirical distribution \(_{t}\) and the true distribution \(q_{t}\) are close for the function \(_{x_{t}}((g,h),(y^{},y))\) for all \(y^{},y.\) We assume that \(\) is finite, and denote \(k:=||.\)

Let \(\{(_{t}^{(i)},_{t}^{(i)})_{i=1}^{M}\}\) denote the \(M\) random samples from \(.\) We note that the expectation over \(_{t}\) is the same as:

\[_{(,)_{t}}[((,),(y^{},y))]=_{i=1}^{M} ((_{t}^{(i)},_{t}^{(i)}),(y^{},y)).\]

Consider the empirical process

\[_{(y^{},y)^{2}}|_{i=1}^{M} {((_{t}^{(i)},_{t}^{(i)}),(y^{},y))- _{(g,h) q_{t}}[((g,h),(y^{},y))]}_ {Z_{i}(y^{},y)}|.\]

For notational simplicity, let us refer to this empirical process as:

\[_{(y^{},y)^{2}}|_{i=1}^{M}Z_{i}(y ^{},y)|=_{t}(M).\]

Because \((,)\), we know \([-1,1].\) Moreover, \(|^{2}|=k^{2}.\) We can now use Hoeffding's inequality and a union bound to obtain:

\[[_{(y^{},y)^{2}}|_ {i=1}^{M}Z_{i}(y^{},y)|] k^{2}(-2M ^{2}) k^{2}(-2T^{1+}^{2}).\]By the elementary integral inequality \([X]_{0}^{}[X t]dt\), we obtain:

\[[_{(y^{},y)^{2}}|_{i=1}^{M}Z_{i}(y^{},y)|] _{0}^{}[_{(y^{},y) ^{2}}|_{i=1}^{M}Z_{i}(y^{},y)| t ]dt\] \[_{0}^{w}[_{(y^{},y)^{2}}|_{i=1}^{M}Z_{i}(y^{},y)| t ]dt+_{w}^{}k^{2}(-2T^{1+}t^{2})dt\] \[ w+_{w}^{}k^{2}(-2T^{1+}t^{2})dt\] \[ w+k^{2}(-2T^{1+}w^{2}),\]

where \(w>0\) is an arbitrary parameter. Set \(w=}\). If \((T))}{2 T}\), then:

\[[_{(y^{},y)^{2}}| _{i=1}^{M}Z_{i}(y^{},y)|]=[_{t}(M)] 2 }.\]

Therefore, summing up over all \(T\) rounds, we obtain \(_{t=1}^{T}[_{t}(M)] 2\). 

### Instantiating the meta-algorithm for Theorem 4.1

Finally, we instantiate Theorem B.4 with a concrete no-regret algorithm for the \((,)\)-player to obtain Theorem 4.1. We employ the specific no-regret algorithm of  for our \((,)\)-player, restated here for reference.

**Theorem B.7** (Smoothed FTPL of ).: _Let \(:[-1,1]\) be a function class and let \(\) be a loss function that is \(L\)-Lipschitz in both arguments. Suppose further that we are in the smoothed online learning setting (see Section 4.1) where each \(x_{i}\) are drawn from a distribution that is \(\)-smooth with respect ot some base measure \(\) on \(\). Let_

\[_{t,n}(f):=_{i=1}^{n})_{i,i}}{},\]

_where \(z_{t,i}\) are independent and the \(_{t,i}\) are independent standard Gaussian variables. Suppose that \( 0\) and consider the algorithm which uses an \(\)-approximate oracle for \(\) (see Definition 2.1) to choose \(f_{t}\) according to_

\[_{s=1}^{t-1}(f_{t}(x_{s}),y_{s})+_{t,n}(f_{t})_{f^{*} }_{s=1}^{t-1}(f^{*}(x_{s}),y_{s})+_{t,n}(f^{*})+,\]

_and let \(_{t}=f_{t}(x_{t})\). If \(\) and \(y_{t}\) are binary valued, with the VC dimension of \(\) bounded by \(d 1\), then for \(n=T/\) and \(=}\)_

\[[_{T}(f_{t})] C(}+  T),\]

_where \(C>0\) is some absolute constant._

This is precisely what the \((,)\)-player does in Algorithm 1. Let \(:=\{g:g\}\) be a collection of groups, represented as Boolean functions \(g:\{0,1\}\). Let \(\) be a hypothesis class of binary-valued functions \(h:\{-1,1\}\). To be clear, the we map Theorem B.7 to our Algorithm 1 in the following way:

* Let \(\) of Theorem B.7 be the class of functions in \([-1,1]^{}\) defined by: \[:=\{x(x)(x):, \}.\] Note that each \(f\) maps to \(\{-1,0,1\}\).

* Let the loss function in Theorem B.7 be the loss of the \((,)\)-player on \(x\): \[((,),(h(x),y)):=(x)((h(x),y)- ((x),y)).\] In terms of \(\) above, we can rewrite this as: \[(f(x),(y^{},y)):=0&f(x)=0\\ (y^{},y)-(-1,y)&f(x)=-1\\ (y^{},y)-(1,y)&f(x)=1.\] This loss function has the signature \(:\{-1,0,1\}\{-1,1\}^{2}[-1,1]\) because \((,)\). It is also \(2\)-Lipschitz in both arguments.
* It remains to make sure that ternary-valued functions taking values in \(\{-1,0,1\}\) do not break the proof of Theorem B.7. In the proof of Theorem B.7 in , the binary-valued function case where \(f\) has range \(\{-1,1\}\) is handled by embedding \(\{-1,1\}\) into the real line. There are two main parts of the proof that rely on this assumption that \(f\) has range \(\{-1,1\}\) that easily maintain when \(f\) has range \(\{-1,0,1\}\).
* First, in Lemma 34 of , the authors use \(L\)-Lipschitzness and the simple fact that \(|f(x)-f^{}(x)|(f(x)-f^{}(x))^{2}\) when \(f\{-1,1\}\). This still holds when \(f\{-1,0,1\}\).
* Second,  also use the fact that \(\|f\|_{L_{2}}=1\) for all \(f\), which is also true for \(f\{-1,0,1\}\). Finally, the rest of the proof in Lemma 35 of  relies only on the Lipschitzness of \(\) to employ standard smoothness arguments and Rademacher contraction, which we have already established.
* Putting all this together, the \((,)\)-player in Algorithm 1 essentially runs the algorithm of B.7, with the caveat that it calls the \(^{}_{(,)}\) oracle \(M\) times to get the empirical approximation \(_{t}\) of the true implicit distribution \(q_{t}\) over \(\). This implicit distribution is defined by the random process of drawing the \(n\) perturbations and calling the optimization oracle.

Therefore, by Lemmas B.6, B.5, and Theorem B.7 applied to the "meta-theorem" Theorem B.4, we immediately obtain our main theorem, Theorem 4.1. We now restate Theorem 4.1 as Corollary B.7.1 with the specified choices of parameters.

**Corollary B.7.1** (Theorem 4.1, with parameters specified).: _Let \(=\{-1,1\}\) be a binary action space, \(\{-1,1\}^{}\) be a binary-valued hypothesis class, \( 2^{}\) be a (possibly infinite) collection of groups, and \(:\{-1,1\}\{-1,1\}\) be a bounded loss function. Let the VC dimensions of \(\) and \(\) both be bounded by \(d\). Let \( 0\) be the approximation error of the oracle \(^{}_{(,)}\). If we are in the \(\)-smooth online learning setting, then, for \(M T^{1+},(T) )}{2 T},n=T/,\) and \(=}\), Algorithm 1 achieves, for each \(g\):_

\[[_{T}(,g)]}+ T,\]

_where the expectation is over all the randomness of the \((,)\)-player's perturbations and the \(\)-player's Bernoulli choices._

## Appendix C Other instantiations of the meta-algorithm

It is be clear from the statement of Theorem B.4 that our "meta-algorithm" Algorithm 4 straightforwardly applies for other online learning settings as well, so long as we adopt an appropriate strategy for the \((,)\)-player. In this section, we give a few examples of this flexibility for discrete action spaces in the smoothed online setting and the

### Multi-class action spaces

Instead of \(||=2\), we can let \(||=K\), more generally. In this case, a straightforward extension of Theorem B.7 allows us to embed \(\{0,1\}\) into the real line, and we generalize to considering the Natarajan dimension  of \(\) instead of the VC dimension. Rademacher contraction and Lipschitzness still apply to \(\), so with just a difference in the absolute constant, we obtain the following multi-class analogue of Theorem 4.1 as a corollary. Thus, for the \((,)\)-player in Algorithm 1, we can just use the same exact algorithm outlined in Theorem B.7.

We make a small change to the \(\)-player in Algorithm 1. In the multi-class action space setting, we need to make \(K\) calls to the \(_{}\) oracle and solve a \(K K\) size linear program for the \(\)-player at each step. For completeness, we present the algorithm for the \(K\)-class action spaces here, as Algorithm 5.

```
0: Perturbation strength \(>0\); number of \(^{}_{(,)}\) calls \(M\).
1:for\(t=1,2,3,,T\)do
2: Receive a context \(x_{t}_{t}\) from Nature.
3:for\(i=1,2,3,,M\)do
4:\((,)\)**-player:** Draw \(n\) hallucinated examples as in Equation (4) to construct \(^{}_{t,n}\).
5:\((,)\)**-player:** Using the entire history \(\{(_{s},y_{s})\}_{s=1}^{t-1}\) so far, call \(^{}_{(,)}\) to obtain \((_{t}(i),^{(i)}_{t})\) satisfying: \[_{s=1}^{t-1}_{x_{s}}((,),(_{s},y_{s}))+^{}_{t,n}(,,)\\ _{(g^{*},h^{*})}_{s=1} ^{t-1}_{x_{s}}((g^{*},h^{*}),(_{s},y_{s}))+^{}_{t,n}(g^{*},h^{*},)-\] (19)
6:endfor
7:\(\)**-player:** Call \(_{}\)\(K\) times on the singleton datasets \(\{(x_{t},k)\}\) for action \(k[K]\) with the 0-1 loss, obtaining: \[h^{}_{k}*{arg\,min}_{h^{}}\{h^{*}(x_{t}) k\}\]
8:\(\)**-player:** Using the \(M\) samples \(\{(^{(i)}_{t},^{(i)}_{t})\}_{i=1}^{M}\), construct the \(K K\) payoff matrix \([-1,1]^{K K}\) indexed by \((k,y)[K][K]\): \[_{k,y}:=_{i=1}^{M}_{x_{t}}((^{(i)}_{t}, ^{(i)}_{t}),(h^{}_{k}(x_{t}),y)).\]
9:\(\)**-player:** Solve the linear program \[_{^{K},} \] \[ ^{}e_{y} y[K]\] \[_{k} 0 k[K]\] \[_{i}=1\] (where \(e_{y}\) is the \(y\)-th coordinate vector in \(^{K}\))
10: Sample \(k\), let \(h_{t}=h^{}_{k}\).
11: Learner commits to the action \(_{t}=h_{t}(x_{t})\); Nature reveals \(y_{t}\).
12: Learner incurs the loss \((_{t},y_{t})\).
13:endfor ```

**Algorithm 5** Algorithm for Group Oracle Efficiency (multi-class)

**Theorem C.1**.: _Let \(=\{1,,K\}\) be a \(K\)-class action space, \(^{}\) be a \(K\)-valued hypothesis class, \( 2^{}\) be a (possibly infinite) collection of groups, and \(:\) be a bounded loss function. Let the Natarajan dimension  of \(\) and the VC dimension of \(\) both be bounded by \(d\). Let \( 0\) be the approximation error of the oracle \(^{}_{(,)}\). If we are in the \(\)-smooth online learning setting, then, for appropriate choices of \(M,n,\) and \(>0\), Algorithm 5 achieves,_for each \(g\):_

\[[_{T}(,g)] O(}+ T),\]

_where the expectation is over all the randomness of the \((,)\)-player's perturbations._

### Group-dependent regret

In this section, we give the details on how to achieve the desired _group-dependent regret_ guarantees of Section 5. Throughout this section, for any \(g\), let \(T_{g}:=_{t=1}^{T}g(x_{t})\). The results in this section hinge on the _GFTPL with small-loss bound_ algorithm of . The main idea will be to instantiate the \((,)\)-player in our meta-algorithm, Algorithm 4 using the GFTPL with small-loss bound algorithm so Theorem B.4 allows us to directly inherit its regret guarantee. We quote the algorithm here, adapted to our setting, for reference. Throughout this section, we use the familiar notation from Section 4.2 of the main body:

\[_{x}((,),(y^{},y)):=(x)( (y^{},y)-((x),y)),\] (20)

where \((,)\) is the fixed loss of the entire multi-group online learning setting.

```
0: Perturbation matrix \([-1,1]^{|||| N}\)
1: Draw i.i.d. vector \(=(^{(1)},,^{(N)})(1)^{N}\), i.e., \(p(^{(i)})=(-|^{(i)}|)\).
2:for\(t=1,2,3,,T\)do
3: Set \(_{t}}\) where \(_{t}>0\) is a parameter computed online.
4: Using the entire history up to \(t-1\) so far, call \(^{}_{(,)}\) to obtain \((,)\) satsiying: \[_{s=1}^{t-1}_{x_{s}}((,),( _{s},y_{s}))+^{(,)},_{t}\\ _{(g^{*},h^{*})}_{s=1 }^{t-1}_{x_{s}}((g^{*},h^{*}),(_{s},y_{s}))+^ {(,)},_{t}-,\] (21) where \(^{(,)}\) is the \((,)\)th row of \(\).
5:endfor ```

**Algorithm 6** GFTPL with small-loss bound

In our setting, the classical FTPL algorithm of  draws \(||||\) independent perturbations at each round, which requires enumeration of both \(\) and \(\). In order to remedy this, the GFTPL algorithm of  uses a \(|||| N\) perturbation matrix \(\) to generate dependent "shared randomness." The transformation \(\) applies to a perturbation vector \(^{N}\), where \(N\) is much smaller than \(||||\). Running FTPL with these perturbations, then, results on an oracle-efficient algorithm.  extends this by showing that, under certain conditions on \(\), this oracle-efficient algorithm can also achieve a _small-loss regret_, where the regret diminishes based on the total magnitude of the losses over the \(T\) rounds instead of the number of rounds.

Specifically, a _small loss regret_ looks like the following. It is well-known that, in the worst-case, a regret of \(O(|||})\) is minimax optimal . However, stronger bounds have been obtained for problems with "small losses" (see, e.g., ), where, for a loss function \(f:()\), one can achieve:

\[O(^{T}f((h_{t},g_{t}),y_{t})||||}),\]

which is sharper than the \(O(|||})\) bound when \(f((h_{t},g_{t}),y_{t})<1\) on some rounds. We desire this property to achieve a regret bound in the multi-group online learning setting that is sublinear in terms of the number of rounds a group appears \(T_{g}\).

Following , we require the perturbation matrix \(\) to have two sufficient conditions for Algorithm 6 to obtain the desired small-loss regret. The first is \(\)-approximability, which is a condition that ensures the stability choices of \((_{t},_{t})\) and \((_{t+1},_{t+1})\) across rounds. In particular, the stability needed is a bound on the ratio:

\[[(_{t},_{t})=(g,h)]}{[(_{t +1},_{t+1})=(g,h)]}(_{t})\]

for all \((g,h),\) where \(_{t}>0\) is the per-round learning rate of GFTPL. By Lemma 2 of , the following condition is sufficient to ensure this property. We restate it here, translated to our setting.

**Definition C.1** (\(\)-approximability ).: _Let \([-1,1]^{|||| N}\), where \(N\) is the dimension of the noise vector, \(\) in Algorithm 6. Define \(B_{}^{1}:=\{s^{N}:\|s\|_{1}\}\). \(\) is \(\)-approximable if, for all \((g,h)\) and \((x,y^{},y)\), there exists \(s^{N}\) with \(\|s\|_{1}\) such that the following holds for all \((g^{},h^{})\):_

\[^{(g,h)}-^{(g^{},h^{})},s_{x}((g,h),(y^{},y))-_{x}((g^{},h^{}),(y^{ },y)).\]

The second property is implementability. This property actually allows us to use our optimization oracle (in our case, \(_{(,)}^{}\)) to access \(\) without explicitly representing it. In essence, it requires that we can generate a small number of "fake examples" that effectively implement the perturbation needed by Algorithm 6.

**Definition C.2** (Implementability ).: _A matrix \([-1,1]^{|||| N}\) is implementable with complexity \(M\) if for each \(j[N]\) there exists a dataset \(S_{j}\) with \(|S_{j}| M\) such that, for all pairs of rows \((g,h),(g^{},h^{})\),_

\[^{((g,h),j)}-^{((g^{},h^{}),j)}=_{(w,(x,y,y^{ })) S_{j}}w(_{x}((,),(y^{}, y))-_{x}((^{},^{}),(y^{},y)) ).\]

In the above definition, the fake examples are tuples of a context \(x\) and two outcomes \(y,y^{}\). Finally, with the sufficient conditions of implementability and approximability, we quote the main regret guarantee of Algorithm 6in  here.

**Theorem C.2** (Regret guarantee of 6 ).: _Let \(\{1,,K\}\) be the action space of the Learner and let \(\) be the action space of the adversary. Suppose that, at each round, the Learner chooses action \(x_{t}[K]\), the adversary chooses action \(z_{t}\), and the loss function is \(f:[K]\). Let \(L_{T}^{*}=_{k[K]}_{t=1}^{T}f(k,y_{t})\). Then, if there exists a \(\)-approximable matrix \(\), Algorithm 6 instantiated with \(\) and \(_{t}:=\{,^{*}+1}}\}\) achieves the following regret bound:_

\[[_{T}] (\{2 K,\}}{C}+2 (C+))^{*}+1}\] \[+8(^{*}+1}+)+2 ^{2}+4\{2 K,\}.\]

_With an appropriate choice of \(C>0\), we may obtain the regret bound:_

\[O(^{*}}\{, K,\})\] (22)

_If \(\) is also implementable with complexity \(M\), then Algorithm 6 is oracle-efficient, making \(O(T+NM)\) oracle calls per round, where \(N\) is the number of columns of \(\)._

Finally, we need one more lemma using a standard uniform convergence argument to bound the approximation error from sampling with \(_{(,)}^{}M\) times. This is essentially the same as Lemma B.6, but we obtain a sharper bound on \([_{t=1}^{T}_{t}(M)]\) at the cost of making polynomially (in \(T\)) more calls to the oracle.

**Lemma C.3**.: _Let \(t[T]\) and \(x_{t}\) be fixed, and consider the function \(_{x_{t}}((g,h),(y^{},y)):=g(x_{t})((y^{},y)-(h( x_{t}),y))\). Let \(||=k<\). If \(M T^{2}(k^{2}T)\), then over the randomness of drawing \(M\) samples \(\{(_{t}^{(i)},_{t}^{(i)})_{i=1}^{M}\}\) to construct the empirical distribution \(_{t}\) described in Definition B.4, for all \(y^{},y\), let \(_{t}(M)\) be defined as the supremum_

\[_{t}(M):=_{(y^{},y)}| _{(,)_{t}}[_{x_{t}}(( ,),(y^{},y))]-_{(g,h) q_{t}}[_{x_{t}}((g,h),(y^{},y))]|,\]

_and, over all \(T\) rounds,_

\[[_{i=1}^{T}_{t}(M)] 2.\]

Proof.: The proof of this lemma follows the proof of Lemma B.6 exactly, except for the choice of \(M\). Therefore, just using the exact same notation as Lemma B.6, we have:

\[[_{(y^{},y)^{2}}|_ {i=1}^{M}Z_{i}(y^{},y)|] k^{2}(-2M ^{2})\]

By the same exact argument using \([X]_{0}^{}[X t]dt\), we obtain

\[[_{(y^{},y)^{2}}|_ {i=1}^{M}Z_{i}(y^{},y)|] w+k^{2}(-2Mw^{2}),\]

where \(w>0\) is an arbitrary parameter. Set \(w=}\). Then, if \(M T^{2}(kT)\), we get

\[[_{t}(M)]=[_{(y^{},y)^{2}}|_{i=1}^{M}Z_{i}(y^{},y)|] 2/T.\]

The lemma follows from summing over \(T\). 

Proof of Theorem 5.1.: We can now prove Theorem 5.1. The main idea is that the small loss regret translates directly into a \(o(T_{g})\) regret due to how we defined our loss function, \(_{x}\), so we simply instantiate the \((,)\)-player in the general algorithm template of Algorithm 4 with Algorithm 6. This allows us to directly inherit the \(o(T_{g})\) regret guarantee. We restate it here, with parameters specified, as Proposition C.3.1. 

**Proposition C.3.1** (Theorem 5.1, with parameters specified).: _Assume \(,\) are finite and there exists a \(\)-approximable and implementable perturbation matrix \([-1,1]^{|||| N}\). Let \(||=k\). Let \( 0\) be the approximation parameter of \(^{}_{(,)}\). Let the no-regret algorithm for the \((,)\)-player in Algorithm 1 be the GFTPL algorithm of  instantiated with \(\), with parameter \(M=T^{2}(k^{2}T)\). Then, for each \(g\):_

\[[_{T}(,g)] O(}\{ ,||||,|||} \}+ T)\]

Proof.: Armed with approximability and implementability, we are ready to prove Theorem 5.1. Suppose that there exists a \(\)-approximable and implementable perturbation matrix \([-1,1]^{|||| N}\). In the setting of Theorem C.2, we instantiate \(K=||||\), \(=\), and the loss function \(f(,)\) as:

\[f((g,h),(x,y^{},y)):=_{x}((g,h),(y^{},y))=g(x)( (y^{},y)-(h(x),y)).\]Observe that, with the loss instantiated as \(_{x}\), we have:

\[L_{T}^{*} =_{(g,h)}_{t=1}^{T}_{x}((g,h),(x_{t},y_{t}^{},y_{t}))\] \[_{t=1}^{T}_{x}((g,h),(x_{t},y_{t}^{},y_{ t}))(g,h)\] \[=_{t=1}^{T}g(x_{t})((y_{t}^{},y_{t})-(h(x_{t}), y_{t}))(g,h).\] \[_{t=1}^{T}g(x_{t})=T_{g},\]

for all \(g\). The last inequality just comes from the fact that \((,)\). By directly applying Theorem C.2, we obtain the regret guarantee for Algorithm 6 of

\[[_{T}]  O(^{*}}\{,||| |,|||}\})\] \[ O(}\{,||| |,|||}\}).\]

However, this is just the regret guarantee of Algorithm 6, not the regret guarantee of our end-to-end multi-group online learning algorithm, Algorithm 1. We replace the algorithm of  in Algorithm 1 with Algorithm 6. That is, we use the Algorithm 6 for the \((,)\)-player in Algorithm 4; a full description of this substitution is in Algorithm 7. By our meta-theorem, Theorem B.4, this entire algorithm achieves the multi-group regret guarantee, for all \(g\):

\[[_{T}(,g)] _{t=1}^{T}v_{t}^{A}+_{t=1}^{T}[_{t} (M)]+R(T).\] (23) \[_{t=1}^{T}[_{t}(M)]+O(} \{,||||,|| |}\}).\] (24)

Equation (24) follows from applying Lemma B.5 and using the regret bound for Algorithm 6 established above. It remains to ensure that \(_{t=1}^{T}[_{t}(M)] o(T_{g})\). Simply applying Lemma B.6 results in \(_{t=1}^{T}[_{t}(M)]=2\), which is insufficient for our purposes. Instead, we use Lemma C.3, which ensures that \(_{t=1}^{T}[_{t}(M)] O(1)\) at the cost of increasing \(M\) to be \(M T^{2}(k^{2}T)\), making polynomially more oracle calls to \(_{(,)}^{}\) per-round. This gives us the final regret guarantee of

\[[_{T}(,g)] 2+O(}\{ ,||||,|||} \}),\]

as desired. 

One possible setting in which a \(\) matrix is easily constructible is the _transductive setting_. Here, we explicitly show how to construct \(\) to obtain Corollary 5.1.1.

Proof of Corollary 5.1.1.: Let \(X\) be the set the adversary fixes beforehand in the transductive setting, where \(N:=|X|\). We can construct a 1-approximable and implementable \([-1,1]^{|||| 4N}\) by creating a row for each \((g,h)\) and a column for each \((x,y,y^{}) X\), and setting each entry as

\[^{((g,h),(x,y,y^{}))}:=_{x}((g,h),(y^{},y)).\]```
0: Perturbation matrix \([-1,1]^{|||| N}\); number of \(^{}_{(,)}\) calls \(M\).
1:for\(t=1,2,3,,T\)do
2: Receive a (possibly adversarial) context \(x_{t}_{t}\) from Nature.
3:for\(i=1,2,3,,M\)do
4:\((,)\)-player: Draw i.i.d. vector \(=(^{(1)},,^{(N)})(1)^{N}\), i.e., \(p(^{(i)})=(-|^{(i)}|)\).
5:\((,)\)-player: Set \(_{t}}\) where \(_{t}:=\{,^{*}+1}}\}\).
6:\((,)\)-player: Using the entire history \(\{(_{s},y_{s})\}_{s=1}^{t-1}\) so far, call \(^{}_{(,)}\) to obtain \((,)\) satisfying: \[_{s=1}^{t-1}_{x_{s}}((,),( _{s},y_{s}))+^{(,)},_{t}\\ _{(g^{*},h^{*})}_{s=1} ^{t-1}_{x_{s}}((g^{*},h^{*}),(_{s},y_{s}))+^{ (,)},_{t}-,\] (25) where \(^{(,)}\) is the \((,)\)th row of \(\).
7:endfor
8:\(\)-player: Call \(_{}\) twice on the singleton datasets \(\{(x_{t},-1)\}\) and \(\{(x_{t},1)\}\) with the 0-1 loss, obtaining: \[h^{}_{1}*{arg\,min}_{h^{*}}\{ h^{*}(x_{t}) 1\} h^{}_{-1}*{arg\,min}_{h^{*} }\{h^{*}(x_{t})-1\}.\]
9:\(\)-player: Solve the linear program \[_{p,} \] \[ _{i=1}^{M}p_{x_{t}}((_{t}^{(i)},_{t}^{(i)}),(h^{}_{1}(x_{t}),y))+(1-p)_{x_{t}}((_{t}^ {(i)},_{t}^{(i)}),(h^{}_{-1}(x_{t}),y)) y \{-1,1\}\] \[0 p 1.\]
10: Sample \(b(p)\) where \(b\{-1,1\}\), let \(h_{t}=h^{}_{b}\).
11: Learner commits to the action \(_{t}=h_{t}(x_{t})\); Nature reveals \(y_{t}\).
12: Learner incurs the loss \((_{t},y_{t})\).
13:endfor ```

**Algorithm 7** Algorithm for Group Oracle Efficiency (with GFTPL)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The three main theorems of the paper and their respective problem settings are laid out in the abstract and Section 1.1, the Summary of Results section, of the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the technical assumptions needed to design oracle-efficient algorithms in Sections 3, 4, and 5 (respectively, i.i.d. assumption, smooth assumption, and existence of perturbation matrix assumption). We also note that our work does not address group-dependent \(o(T_{g})\) regret for infinite \(\) and \(\), as mentioned in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All theorems are formally stated in the main body and proved in the Appendix. Each main theorem, Theorem 3.2, Theorem 4.1, and Theorem 5.1, are stated in the main body and proven in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a theory paper, where the main contributions are towards novel algorithmic design principles for a learning-theoretic model. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This is a theory paper (see above).

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theory paper (see above). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a theory paper (see above). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a theory paper (see above). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper mainly focuses on a theoretical problem that does not involve the use of any real-world datasets. Insofar as the proofs go, we have attempted to make them clear and easily checkable. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The main contribution of this work is towards algorithmic techniques for a learning theory problem. We see the societal impacts of such techniques as minimal. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not include any trained models or data. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Our paper does not use any licensed code, data, or models. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not use any new assets (no experiments). Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not conduct any experiments on human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not conduct any experiments on human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.