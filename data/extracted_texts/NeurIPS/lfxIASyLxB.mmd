# In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness

Liam Collins

Chandra Family Department of ECE

The University of Texas at Austin

liamc@utexas.edu

&Advait Parulekar

Chandra Family Department of ECE

The University of Texas at Austin

advaitp@utexas.edu

&Aryan Mokhtari

Chandra Family Department of ECE

The University of Texas at Austin

mokhtari@austin.utexas.edu

&Sujay Sanghavi

Chandra Family Department of ECE

The University of Texas at Austin

sanghavi@mail.utexas.edu

&Sanjay Shakkottai

Chandra Family Department of ECE

The University of Texas at Austin

sanjay.shakkottai@utexas.edu

Co-first authors, listed in alphabetical order.

###### Abstract

A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such, that learner must adapt to the context without additional training. We explore the role of _softmax_ attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.

## 1 Introduction

One of the most compelling behaviors of pretrained transformers is their ability to perform _in-context learning (ICL)_: determining how to solve an unseen task simply by making a forward pass on input context tokens. Arguably the most critical innovation enabling ICL is the self-attention mechanism , which maps each token in an input sequence to a new token using information from all other tokens. A key design choice in this self-attention architecture is of the activation function that controls how much "attention" a token pays to other tokens. _Softmax_-activated self-attention (i.e. softmax attention) is most commonly, and successfully, used in practice .

A natural approach to explain ICL adopted by the literature is to equate it with classical machine learning algorithms, primarily variants of gradient descent (GD). Several works have shown thatwhen the ICL tasks are _linear_ regressions and the activation in the attention unit is identity (referred to as _linear_ attention), transformers that implement preconditioned GD during ICL are global optima of the pretraining loss, which is the population loss on ICL tasks . In particular, the prediction of such transformers with \(l\) linear attention layers equals the prediction of a regressor trained with \(l\) preconditioned GD steps on the context examples. However, since these analyses are limited to linear attention and tasks, they do not explain the widespread success of _softmax_ attention at ICL.

More recent work  extends these results by showing that for general regression tasks and any attention activation that is a kernel, ICL equates to training a kernel regressor via functional GD in the Reproducing Kernel Hilbert Space (RKHS) induced by the activation. However, this functional GD yields generalization guarantees only when the activation kernel is _identical_ to a kernel that generates the labels, which does not apply to the softmax activation, as it is not a kernel. Further, like the aforementioned studies of the linear setting , this analysis only shows that pretraining leads to learning the _covariate_ distribution, while the activation implicitly encodes the _label_ distribution needed for accurate predictions. Thus, this line of work has not explained the very fundamental question of what _softmax_ attention learns during pretraining that enables it to perform ICL on a wide variety of downstream tasks. Motivated by this gap in the literature, we ask the following question.

_How does softmax attention learn to perform ICL?_

To answer this question, we study general settings in which pretraining and evaluation ICL tasks are regressions that share only _Lipschitzness_ and _label noise variance_. Specifically, the rate at which their ground-truth labels change along particular directions in the input space, and the variance in the label noise, is similar across tasks. In such settings, we observe that softmax attention acts as a nearest neighbors regressor with an _attention window_ - i.e. neighborhood of points around the query that strongly influence, or "attend to", the prediction - that adapts to the pretraining tasks. Specifically, our main result is as follows:

**Main Claim:** Softmax attention performs ICL by calibrating its _attention window_ to the _Lipschitzness_ and _label noise variance_ of the pretraining tasks.

While this does not contradict the line of work showing that ICL manifests via a "meta-learned" gradient-based algorithm, we show in a general setting that a simpler mechanism can explain the capabilities of a widely accepted model of ICL.

Figure 1: **Top Row:** The black line denotes the target function over a domain (horizontal axis). The gray dots are noisy training data, and the white dot is a query. From left to right, the Lipschitzness of the target function grows and the optimal softmax attention window (shaded blue) shrinks. **Middle Row:** Attention weights – which determine the attention window – as a function of the relative position from the query for softmax and linear attention. The softmax weights adjust to the Lipschitzness. **Bottom Row:** ICL error versus number of context samples for the three settings. **Adapting to function Lipschitzness leads softmax attention to achieve small error**. Please see Remark 2.1 and Appendix J for further discussion and details.

**Outline.** We substantiate the above claim via two streams of analysis. To our knowledge, these are the first results showing that softmax attention pretrained on ICL tasks recovers shared structure among the tasks that facilitates ICL on downstream tasks.

**(1) Attention window _scale_ adapts to Lipschitzness and noise variance - Section 3.** We prove that the pretraining-optimal softmax attention estimator scales its attention window _inversely with the task Lipschitzness_ and _jointly with the noise level_ to optimally trade-off bias and variance in its prediction (Theorem 3.4). This requires tight upper and lower bounds on the pretraining ICL loss. While the upper bounds (Lemma C.8) hold for all \(L\)-Lipschitz tasks, the lower bounds (Lemma C.9) are more challenging and require considering specific classes of tasks. We consider two classes of generalized linear models (GLMs), and obtain lower bounds via novel concentrations for particular functionals on the distribution of the attention weights for tokens distributed on the hypersphere (Corollary G.5).

**(2) Attention window _directions_ adapt to direction-wise Lipschitzness - Section 4.** We prove that when the target function class consists of linear functions that share a common low-dimensional structure, the optimal softmax attention weight matrix from pretraining projects the data onto this subspace (Theorem 4.4). In other words, softmax attention learns to zero-out the zero-Lipschitzness directions in the ambient data space, and thereby reduces the effective dimension of ICL. We prove this via a careful symmetry-based argument to characterize a particular gradient of the ICL loss as positive (Lemmas H.3 and H.4).

**Tightness of results.** Our results highlight the importance of shared Lipschitzness across training and test, as well as the critical role of the softmax activation, to ICL. We show that softmax attention pretrained on the setting from Section 3 in-context learns _any_ downstream task with _similar Lipschitzness_ to the pretraining tasks, while changing _only the Lipschitzness_ of the evaluation tasks degrades performance (Theorem 3.5) - implying _learning Lipschitzness is both sufficient_ and _necessary for generalization_. Further, to emphasize the _necessity of the softmax_, we show that the minimum ICL loss achievable by linear attention exceeds that achieved by pretrained softmax attention (Theorem 3.6). We verify all of these results with empirical simulations (Section 3.2 and Appendix J).

**Notations.** We use (upper-, lower-)case boldface for (matrices, vectors), respectively. We denote the (identity, zero) matrix in \(^{d d}\) as (\(_{d}\), \(_{d d}\)), respectively, the set of column-orthonormal matrices in \(^{d k}\) as \(^{d k}\), and the (column space, 2-norm) of a matrix \(\) as (\(()\), \(\|\|\)), respectively. We indicate the unit hypersphere in \(^{d}\) by \(^{d-1}\) and the uniform distribution over \(^{d-1}\) as \(^{d}\). We use asymptotic notation (\(\), \(\)) to hide constants that depend only on the dimension \(d\).

### Additional Related Work

Numerous recent works have _constructed_ transformers that can implement GD and other machine learning algorithms during ICL [11; 12; 13; 14; 15], but it is unclear whether _pretraining_ leads to such transformers.  and  provide generalization bounds for ICL via tools from algorithmic stability and uniform concentration, respectively.  investigate the pretraining statistical complexity of learning a Bayes-optimal predictor for ICL on linear tasks with linear attention. [18; 19; 20] study the role of the pretraining data distribution, rather than the learning model, in facilitating ICL.  studies the dynamics of a softmax attention unit trained with GD on ICL tasks, but this analysis considers only linear tasks and orthogonal inputs. The connection between ICL with softmax attention and non-parametric regression has been noticed by other works that analyze the ICL performance of a softmax-like kernel regressor  and aim to improve upon softmax attention [23; 24; 25; 26; 27] rather than explain what it learns during pretraining. Please see Appendix A for further discussion of the large body of related works studying the theory of transformers, ICL and kernel regression.

## 2 Preliminaries

**In-Context Learning (ICL) regression tasks.** We study ICL in the regression setting popularized by , wherein each task is a regression problem in \(^{d}\). The context for task \(t\) consists of a set of \(n\) feature vectors paired with noisy labels \(\{_{i}^{(t)},f^{(t)}(_{i}^{(t)})+_{i}^{(t)}\}_{i=1}^{n}\), where \(f^{(t)}:^{d}\) generates the ground-truth labels for task \(t\) and \(_{i}^{(t)}\) is label noise. Given this context, the model solves the task if it accurately predicts the label of a query \(_{n+1}^{(t)}\). During pretraining, the modelobserves many such tasks. Then, it is evaluated on a new task with context \(\{_{i},f^{(*)}(_{i}^{(*)})+_{i}^{(*)}\}_{i=1}^{n}\) and query \(_{n+1}^{(*)}\). We emphasize that the model is trained only on the pretraining tasks, not the evaluation context. Unlike traditional supervised learning, which would involve training on the context \(\{_{i},f^{(*)}(_{i}^{(*)})+_{i}^{(*)}\}_{i=1}^{n}\) in order to predict \(f^{(*)}(_{n+1}^{(*)})\), ICL happens _entirely in a forward pass_, so there is no training using labels from \(f^{(*)}\). Our inquiry focuses on how ICL is facilitated by the softmax activation in the self-attention unit, which we introduce next.

**The Softmax Attention Unit.** We consider a single softmax attention head \(H_{SA}(;):^{(d+1)(n+1)}^ {(d+1)(n+1)}\) parameterized by \(:=(_{K},_{Q},_{V})\), where \(_{K},_{Q},_{V}^{(d+1)(d+1)}\) are known as key, query, and value weight matrices, respectively. Intuitively, for a sequence of tokens \(=[_{1},,_{n+1}]^{(d+1)(n+1)}\), the attention layer creates a "hash map" where the key-value pairs come from key and value embeddings of the input tokens, \(\{_{K}\,_{i}:_{V}\,_{i}\}\). Each token \(_{i}\) is interpreted as a query \(_{Q}\,_{i}\), and during a pass through the attention layer, this query is matched with the keys \(\{_{K}\,_{j}\}_{j}\) to return an average over the associated values \(\{_{V}\,_{j}\}_{j}\) with a weight determined by the quality of the match (proportional to \(e^{(_{K}\,_{j})^{}(_{Q}\,_{i})}\)). Specifically, \(H_{SA}(;)=[h_{SA}(_{1},;), ,h_{SA}(_{n+1},;)]\), where

\[h_{SA}(_{i},;)=^{n} (_{V}_{j})\ e^{(_{K}_{j})^{}( _{Q}_{i})}}{_{i=1}^{n}e^{(_{K}_{j})^{} (_{Q}_{i})}}^{d+1}.\] (ATTN)

With slight abuse of notation, we denote \(h_{SA}(_{j})=h_{SA}(_{j},;)\) when it is not ambiguous. To study how this architecture enables ICL, we follow  to formalize ICL as a regression problem. Below we define the tokenization, pretraining objective and evaluation task.

**Tokenization for regression.** The learning model encounters token sequences of the form

\[:=_{1}&_{2}&&_{n }&_{n+1}\\ f(_{1})+_{1}&f(_{2})+_{1}&&f(_{n})+ _{n}&0^{(d+1)(n+1)},\] (1)

where the ground-truth labelling function \(f\) maps from \(^{d}\) to \(\) and belongs to some class \(\), each \(_{i}\) is mean-zero noise, and the \(i\)-th input feature vector \(_{i}^{d}\) is jointly embedded in the same token with its noisy label \(f(_{i})+_{i}\). We denote this token \(_{i}\). The ICL task is to accurately predict this label given the \(n\) context tokens \(\{(_{i},f(_{i})+_{i})\}_{i=1}^{n}\), where \(f\) may vary across sequences. The prediction for the label of the \((n+1)\)-th feature vector is the \((d+1)\)-th element of \(h_{SA}(_{n+1})\), denoted \(h_{SA}(_{n+1})_{d+1}\). Ultimately, the goal is to learn weight matrices such that \(h_{SA}(_{n+1})_{d+1}\) is likely to approximate the \((n+1)\)-th label on a random sequence \(\).

**Pretraining protocol.** We study what softmax attention learns when its weight matrices are _pre-trained_ using sequences of the form of (1). These sequences are randomly generated as follows:

\[f D(),_{1},,_{n+1}}}{{}}D_{}^{(n+1)},_{1}, ,_{n}}}{{}}D_{}^ {(n+1)}\] (2)

where \(D()\) is a distribution over functions in \(\), \(D_{}\) is a distribution over \(^{d}\), and \(D_{}\) is a distribution over \(\) with mean zero and variance \(^{2}\). The token embedding sequence \(\) is then constructed as in (1). Given this generative model, the pretraining loss of the parameters \(=(_{Q},_{K},_{V})\) is the expected squared difference between the prediction of softmax attention and the ground-truth label of the \((n+1)\)-th input feature vector in each sequence, namely

\[}():=_{f,\{_{i}\}_{i},\{ _{i}\}_{i}}(h_{SA}(_{n+1})_{d+1}-f(_{n+1}))^{2}.\] (3)

We next reparameterize the attention weights to make (3) more interpretable. For the last column of \(_{V}\), we show in Appendix B that any minimizer of (3) in the settings we consider must have the first \(d\) elements of this last column equal to zero. We follow [7; 9; 10] by setting the first \(n\) columns of \(_{V}\) to zero. As in , we fix the \((d+1,d+1)\)-th element of \(_{V}\), here as 1 for simplicity. In the same vein, we follow [7; 10] by setting the \((d+1)\)-th row and column of \(_{K}\) and \(_{Q}\) equal to zero. To summarize, the reparameterized weights are:

\[_{V}=_{d d}&_{d  1}\\ _{1 d}&1,_{K}=_{K}&_{d 1}\\ _{1 d}&0,_{Q}=_{Q}&_{d 1}\\ _{1 d}&0\] (4)where \(_{K},_{Q}^{d d}\). Now, since our goal is to reveal properties of minimizers of the pretraining loss, rather than study the dynamics of optimizing the loss, without loss of generality we can define \(:=_{K}^{}_{Q}\) and re-define the pretraining loss (3) as a function of \(\). Doing so yields:

\[():=_{f,\{_{i}\}_{i},\{_{i}\}_{i}} (^{n}(f(_{i})+_{i})\ e^{_{i}^{} \,_{n+1}}}{_{i=1}^{n}e^{_{i}^{}\,_{n+1}}}-f(_{n+1}))^{2}.\] (ICL)

**Interpretation of the pretraining loss.** The loss (ICL) clarifies how softmax attention can be interpreted as a nearest neighbors regressor. When \(_{i}^{}\,_{n+1}\) is a proxy for the distance between \(_{i}\) and \(_{n+1}\) (which we formally show in Section 3 as happening under reasonable assumptions), the softmax attention prediction is a convex combination of the noisy labels with weights determined by the closeness of \(_{i}\) to \(_{n+1}\), such that the labels of points closer to \(_{n+1}\) have larger weight. Moreover, the decay in weights on points further from \(_{n+1}\) is exponential and controlled by \(\), which effectively defines a neighborhood, or attention window, of points around \(_{n+1}\) whose labels have non-trivial weight. More formally, we can think of the attention window defined for a query \(_{n+1}\) as the set \((_{n+1};):=\{:^{}\,_{n+1}=(1)\}\). As we have observed in Figure 1, our key insight is that pretrained \(\) **scales this attention window with the Lipschitzness of the function class.** Generally speaking, larger \(\) entails averaging over a smaller window and incurring less bias due to the function values of distant tokens in the estimate, while smaller \(\) entails averaging over a larger window, resulting in larger bias due to distant token labels, but a smaller noise variance. Figure 2 further depicts this tradeoff.

**Connection to non-parametric estimation and the Nadaraya-Watson estimator.** A nonparametric estimation technique to interpolate between known values of a function is to use a kernel estimator. The Nadaraya-Watson (NW) estimator [29; 30; 31] is one such estimator, and interpolates the data as

\[f_{NW}(_{n+1})=_{i},x_{i})f(x_{i})}{_{j}K(x_{n+1},\,x_{j})}\]

where \(K(r)=e^{-r^{2}/h}\) for some bandwidth \(h\). In Section B.1 we show that optimizing the pretraining loss (ICL) reduces to meta-learning the bandwidth of an NW estimator on a distribution of pretraining tasks. However, to our knowledge, the literature has not determined the optimal bandwidth for the kernel, as there has been no analysis of non-asymptotic lower bounds on the loss, which we need to characterize the optimal solution. A close work to ours is , which considers regression on general \(L\)-Lipschitz tasks, but this analysis provides only a tight upper bound on the loss.

**Remark 2.1** (Extreme cases).: _Consider the following two settings._

_(i) Constant functions._ _If each of the functions the attention unit sees in pretraining is constant, as in the_ _Left_ _column of Figure_ 1_, it is best to consider an infinite attention window, that is, take_ \(=_{d d}\) _as this results in a uniform average over all the noisy token labels._

_(ii) Rapidly changing functions._ _If the pretraining functions change rapidly, as in the_ _Right_ _column of Figure_ 1_, attending to a distant token might only corrupt the estimate at the target. For example suppose the input tokens are used to construct Voronoi cells on the surface of the hypersphere, and the label for a new token in a cell is the label of the token used to construct that cell. The optimal estimator attends only to the single nearest token since this incurs error only from label noise._

**Remark 2.2** (Softmax advantage).: _To further highlight the utility of the softmax, we compare with linear attention [7; 9; 11], whose estimator can be written as \(h_{LA}()=_{i}(f(_{i})+_{i})\,_{i}^{}\,\), up to a universal scaling due to the value embedding. This is again a weighted combination of labels, but one that does not allow for adapting an attention window - any scaling of \(\) does not change the relative weights placed on each label - unlike softmax attention. Please see Figure 1 (**Middle Row**) for a comparison of the weights used in the different estimators._

## 3 Pretraining Learns Scale of Attention Window

One of our observations of the attention estimator \(h_{SA}\) is that it computes a nearest neighbours regression. We hypothesize that the role of pretraining is to select a neighbourhood within which to select tokens for use in the estimator. In this section we characterize the radius of this neighborhood.

**Definition 3.1** (Lipschitzness).: _A function \(f:\) has Lipschitzness \(L\) if \(L\) is the smallest number satisfying \(f()-f(^{}) L\|\,-^{}\,\|\) for all \((,^{})^{2}\)._

The general requirement for the function classes to which our results apply is that the class should be invariant to isometries, each function should be Lipschitz, and the function value at two points should be less correlated as those points get further. These are written formally in Assumption B.4. To be concrete, we work with the following two function classes that satisfy these assumptions (this is shown in Lemmas C.3 and C.7) to derive explicit bounds.

**Definition 3.2** (Affine and ReLU Function Classes).: _The function classes \(_{L}^{}\) and \(_{L}^{+}\) are respectively defined as:_

\[_{L}^{} :=\{f:f()=l\ ^{}\,+b,\ ^{d-1},b,l[-L,L]\},\] \[_{L}^{+} :=\{f:f()=l_{1}(^{}\,)_{+}+l_{2}(- ^{}\,)_{+}+b,\ ^{d-1},(b,l_{1},l_{2})[-L,L]^{2}\}.\]

\(D(_{L}^{}),D(_{L}^{+})\) _are induced by drawing \(^{d}\) and \(b,l,l_{1},l_{2}([-L,L])\) for some \(_{d d}\). Note that the max Lipschitzness of any function in these classes is \(L\), and \((z)_{+}:=(z,0)\)._

Next, we make the following assumption, similar to , on the covariate distribution.

**Assumption 3.3** (Covariate Distribution).: _The covariate distribution satisfies \(D_{}=^{-1}^{d}\)._

Now we are ready to state our main theorem that characterizes minimizers of (ICL).

**Theorem 3.4**.: _Let Assumption 3.3 hold and tasks \(f\) be drawn from **(Case 1)**\(D(_{L}^{})\) or **(Case 2)**\(D(_{L}^{+})\). For \(n=(1)\) and \((n^{-d/2})^{2}(nL^{2})\), any minimizer of the pretraining loss (ICL) satisfies2\(^{*}=w_{KQ}\), where for \(:=}{^{2}}\), \(:=\) and \(:=\):_

\[\ \ (^{})|w_{KQ}| (^{}),\ \ (^{})|w_{KQ}|(^{2 }).\]

Theorem 3.4 shows that optimizing the pretraining population loss in Equation (ICL) leads to attention key-query parameters that scale with the Lipschitzness of the function class, as well as the noise level and number of in-context samples. These bounds align with our observations from Figures 1 and 2 that softmax attention selects an attention window that shrinks with the function class Lipschitzness, recalling that larger \(w_{KQ}\) results in a smaller window. Further, the dependencies of the bounds on \(^{2}\) and \(n\) are also intuitive, since larger noise should encourage wider averaging to average out the noise, and larger \(n\) should encourage a smaller window since more samples makes it more likely that there are samples very close to the query. To our knowledge, this is the _first result showing that softmax attention learns properties of the task distribution during pretraining that facilitate ICL._

**Learning Lipschitzness is critical to generalization.** We next give the following generalization result for downstream tasks.

**Theorem 3.5**.: _Suppose softmax attention is first pretrained on tasks drawn from \(D(_{L}^{+})\) and then tested on an arbitrary \(L-\)Lipschitz task, then the loss on the new task is upper bounded as \((n^{2})\)._

Figure 2: From **left to right**, as we **shrink the attention window** (shaded in blue), the estimator has **lower bias** (the expected value of the estimate, depicted in purple, is closer to the ground-truth label, depicted by the white circle) but **larger variance** (shaded in tan).

\((}{^{}})\). _Furthermore, if the new task is instead drawn from \(D(^{+}_{L^{}})\), the loss is lower bounded as \((}}{^{2}})\) for \(L^{}>L\) and \((}{n})\) for \(L^{}<L\)._

Theorem 3.5 shows that pretraining on \(D(^{+}_{L})\) yields a model that can in-context learn downstream tasks _if and only if_ they have similar Lipschitzness as \(L\). Thus, learning Lipschitzness is _both sufficient and necessary_ for ICL. If the evaluation task Lipschitzness is much larger than that seen in pretraining, the pretrained model will give highly biased estimates. Conversely, if the evaluation Lipschitzness is much lower, the pretrained model will not optimally average the label noise.

**Necessity of Softmax.** To further emphasize the importance of the softmax in Theorem 3.4, we next study the performance of an analogous model with the softmax removed. We consider _linear self-attention_[7; 9; 11], which replaces the softmax activation with an identity operation. In particular, in the in-context regression setting we study, the prediction of \(f(_{n+1})\) by linear attention and the corresponding pretraining loss are given by:

\[h_{LA}(_{n+1}) :=_{i=1}^{n}(f(_{i})+_{i})_{i}^{} }_{n+1},\] \[_{}(}) :=_{f,\{_{i}\}_{i},\{_{i}\}_{i}}(h_{ LA}(_{n+1})-f(_{n+1}))^{2}.\]

As discussed in Remark 2.1, \(h_{LA}(_{n+1})\) cannot adapt an attention window to the problem setting. We show below that this leads it to large ICL loss when tasks are drawn from \(D(^{+}_{L})\).

**Theorem 3.6** (Lower Bound for Linear Attention).: _Consider pretraining on \(_{}\) with tasks \(f\) drawn from \(D(^{+}_{L})\) and covariates drawn from \(^{d}\). Then for all \(}^{d d}\), \(_{LA}(})=(L^{2})\)._

This lower bound on \(_{LA}\) is strictly larger than the upper bound on \(\) from Theorem 3.5, up to factors in \(d\), as long as \(}{n} 1\), which holds in all reasonable cases. Please see Appendix F for the proof.

### Proof Sketch

To highlight the key insights of our analysis, in this section we consider a modification of the softmax attention that exhibits important properties of the original. Note that this approximation is for illustration only; the above results use the original softmax attention - see Appendices C, D, E. For now, consider a function class \(_{L}:=\{f:f()=L^{},\  ^{d-1}\}\) of linear functions.

**(Temporary) modification of the softmax attention.** Rather than averaging over every token with a weight that decays exponentially with distance, we consider a modification which _uniformly_ averages all tokens within a distance specified by \(w_{KQ}=\|}\|\). From Lemma B.5, without loss of generality (WLOG) we can consider \(}=w_{KQ}_{d}\). This means that, ignoring normalization, the weight assigned to \(f(_{i})\) by the true soft-max attention is \(e^{-w_{KQ}\|\ -_{i}\ \|^{2}}\). That is, for all \(_{i}\) satisfying \(\|-_{i}\ \|<}{{}}}\), the assigned weights within a constant factor of each other. Meanwhile, for \(_{i}\) satisfying \(\|-_{i}\ \|=}{{}}}\) for \(c>1\), the weights are \(e^{-c}\), decaying exponentially in \(c\). This motivates us to consider a "modified softmax attention" given by \(h_{MSA}():=_{i}_{i})_{i}}{_{j}_{j}},\) where \(_{j}:=\{\|-_{j}\ \|<}{{}}}\}\).

**The In-context Loss.** The pretraining loss in Equation ICL can be decomposed as:

\[(w_{KQ}_{d})=_{f,\{_{i}\}_{i}} (_{j}_{n+1})-f(_{j}))_{j}}{_{j} _{j}})^{2}}_{=:_{}(w_{KQ})}^{2} +_{\{_{i}\}_{i},\{_{i}\}_{i}}( _{i}_{i}_{i}}{_{j}_{j}}) ^{2}}_{=:_{}(w_{KQ})}.\]

We first upper and lower bound each of these terms separately, starting with \(_{}(w_{KQ})\).

**Noiseless Estimator Bias.** (Please see Appendix C) This term is the squared difference between an unweighted average of the token labels within a radius of \(\), and the true label. Take \(w_{KQ}=(1)\). Then for large \(d\), most of the points \(_{i}\) satisfying \(\|-_{i}\|}{{}}}\) lie on the boundary of the cap, that is, \(\|-_{i}\|<}{{}}}\|-_ {i}\|}{{}}}\). This motivates us to approximate the set of points \(_{i}\) satisfying the above as coming from a uniform distribution over just the boundary of the cap. The center of mass of a ring of radius \(}{{}}}\) embedded on the surface of a hyper-sphere, is \((}{{w_{KQ}}})\) from the boundary of a sphere, so the squared bias is \((}}{{w_{KQ}^{2}}})\).

**Noise.** (Please see Appendix D for details) Since the noise is independent across tokens, we can write \(_{}(w_{KQ})=}{_{j}_{j}}\), which is related to the number of tokens found within a \(}{{}}}\) radius of \(\). In Lemma G.1, we derive bounds for the measure of this region. For now, we replace the sum in the denominator with its expectation. We can bound \(_{j}}{_{j}}=w_{KQ}^{}/n\) as long as \(w_{KQ} n^{2/d}\).

**Combining the \(_{}\) and \(_{}\) terms.** (Please see Appendix E for details) Overall, we have \(=_{}+_{}\) with \(_{}=L^{2}/}}{{w_{KQ}}} \) and \(_{}=w_{KQ}^{}}}{{n}}\). Minimizing this sum reveals that the optimal \(w_{KQ}\) satisfies \(w_{KQ}=(}}{{^{2}}})^{}}\).

### Experiments

We next empirically verify our intuitions and results regarding learning the scale of the attention window. In all cases we use the Adam optimizer with one task sampled per round, use the noise distribution \(D_{}=(0,^{2})\), and run \(10\) trials and plot means and standard deviations over these 10 trials. Please see Appendix J for full details as well as additional results.

**Ablations over \(L\), \(\) and \(n\).** We verify whether the relationship between the attention window scale - i.e. \(\|\|^{-1}\) - and \(L\), \(\) and \(n\) matches our bounds in Theorem 3.4 for the case when tasks are drawn from \(D(_{L}^{+})\) and the covariates are drawn from \(^{d}\), as well as whether these relationships generalize to additional function classes and covariate distributions. We train on tasks drawn from \(D(_{L}^{+})\) and \(D(_{L}^{})\), where \(_{L}^{}:=\{f:f()=L^{},\ ^{d-1}\}\) and \(D(_{L}^{})\) is induced by sampling \(^{d}\). In all cases we set \(d=5\), and use \((L,,n)=(1,0.01,20)\) if not ablating over these parameters, and vary only one of \(\{L,,n\}\) and no other hyperparameters within each plot.

**Attention window scales inversely with \(L\).** Figure 3 shows that \(\|\|\) increases with \(L\) in various settings. In Figure 3(Left, Middle-Left), tasks are drawn from \(D(_{L}^{+})\), and in Figure 3(Middle-Right, Right), they are drawn \(D(_{L}^{})\). In Figure 3(Left, Middle-Right), each \(_{i}\) is drawn from \(^{d}\), whereas in Figure 3(Middle-Left, Right), each \(_{i}\) is drawn from a non-isotropic distribution \(}^{d}\) on \(^{d-1}\) defined as follows. First, let \(_{d}:=([1,,d])^{d d}\), then \(}^{d}\) is generated by sampling \(}(_{d},_{d})\), then computing \(=_{d}^{1/2}}}{\|_{d}^{1/2}}\|}\). Although larger \(L\) implies larger \(\|_{}f()\|\) on average across \(f\), it is not clear that it implies larger \(\|_{_{K}}(_{K}^{}_{Q})\|\) nor \(\|_{_{Q}}(_{K}^{}_{Q})\|\), so it is surprising that larger \(L\) implies larger pretrained \(\) (although it is consistent with our results).

**Attention window scales with \(\), inversely with \(n\).** Figure 4 shows that the dependence of \(\|\|\) on \(\) and \(n\) also aligns with Theorem 3.4. As expected, \(\|\|\) increases slower during pretraining for larger \(\) (shown in Figures 4(Left, Middle-Left)), since more noise encourages more averaging over a larger window to cancel out the noise. Likewise, \(\|\|\) increases faster during pretraining for larger \(n\) (shown in Figures 4(Middle-Right, Right)), since larger \(n\) increases the likelihood that there is a highly informative sample in a small attention window. Here always the covariate distribution is \(^{d}\).

**Learning new tasks in-context.** An implication of our work is that for the function classes we consider, the **softmax attention estimator does not adapt to the function class beyond its Lipschitzness**. We have already seen in Figures 3 and 4 that the growth of \(\|\|\) during pretraining is similar across different function classes with the same Lipschitzness, as long as \(\) and \(n\) are fixed. Here we verify the conclusion from Theorem 3.5 that for fixed \(n\) and \(\), the necessary and sufficient condition for downstream generalization, measured by small ICL error, is that the pretraining and downstream tasks have similar Lipschitzness. Figure 5 supports this conclusion. Here we set \(d=5,n=200,=0.01\) and draw each \(_{i}\) i.i.d. from \(^{d}\). In Figure 5(Left, Middle-Left, Middle-Right), we train three attention units on tasks drawn from the 1-Lipschitz affine (\(D(_{1}^{})\)), ReLU (\(D(_{1}^{+})\)), and cosine (\(D(_{1}^{})\)) task distributions. Each plot shows the test ICL error on tasks drawn from a distribution in \(\{D(_{1}^{}),D(_{1}^{+}),D(_{1}^{ })\}\). Performance is similar regardless of the pairing of pretraining and test distributions, as the Lipschitzness is the same in all cases, demonstrating that **pretraining on tasks with appropriate Lipschitzness is sufficient for generalization**.

Moreover, Figure 5(Right) shows that when the Lipschitzness of the pretraining tasks does _not_ match that of the test tasks, ICL performance degrades sharply, even when the tasks otherwise share similar structure. Here the test task distribution is \(D(_{1}^{})\), and the pretraining task distributions are \(D(_{1}^{})\), \(D(_{1}^{})\), and \(D(_{1}^{})\). The only pretraining distribution that leads to downstream generalization is \(D(_{1}^{})\) since its Lipschitzness matches that of the downstream tasks, despite the fact that it is not a distribution over cosine functions, unlike the other distributions. Thus, these results lend credence to the idea that in addition to being sufficient, **pretraining on tasks with appropriate Lipschitzness is necessary for generalization**.

## 4 Softmax Attention Learns Direction of Attention Window

Thus far, we have considered distributions over tasks that treat the value of the input data in all directions within the ambient space as equally relevant to its label. However, in practice the ambient dimension of the input data is often much larger than its information content - the labels may change very little with many features of the data, meaning that such features are spurious. This is generally true of embedded language tokens, whose embedding dimension is typically far larger than the minimum dimension required to store them (logarithmic in the vocabulary size) . Motivated by this, we define a notion of "direction-wise Lipschitzness" of a function class to allow for analyzing classes that may depend on some directions within the ambient input data space more than others.

**Definition 4.1** (Direction-wise Lipschitzness of Function Class).: _The Lipschitzness of a function class \(\) with domain \(^{d}\) in the direction \(^{d-1}\) is defined as as the largest Lipschitz constant of all functions in \(\) over the domain \(\) projected onto \(\), that is:_

\[_{}(,):=_{L}\{L:f( ^{}\,)-f(^{}\,^{ }) L|^{}\,-^{}\,^{}\, |\,\,\,\,\,\,(,^{})^{2},f\}.\]

Using this definition, we analyze function classes consisting of linear functions with parameters lying in a subspace of \(^{d}\), as follows:

**Definition 4.2** (Low-rank Linear Function Class).: _The function class \(_{}^{}\) is defined as \(_{}^{}:=\{f:f()=^{}^{}\,,\,\,^{k}\}\), and \(D(_{}^{})\) is induced by drawing \(^{k}\)._

where \(^{d k}\) is a column-wise orthonormal matrix. Since our motivation is settings with low-dimensional structure, we can think of \(k d\). Let \(_{}^{d(d-k)}\) denote a matrix whose columns

Figure 4: Spectral norm of \(\) during pretraining on tasks drawn from \(D(_{1}^{+})\) in **Left, Middle-Right** and \(D(_{1}^{})\) in **Middle-Left, Right**. **Left, Middle-Left** show ablations over the noise standard deviation \(\) and **Middle-Right, Right** show ablations over the number of context samples \(n\).

form an orthonormal basis for the subspace perpendicular to \(()\), and note that the Lipschitzness of \(_{}^{}\) in the direction \(\) is \(L\) if \(()\) and 0 if \((_{})\). Observe that any function in \(_{}^{}\) can be learned by projecting the input onto the non-zero Lipschitzness directions, i.e. \(()\), then solving a \(k d\)-dimensional regression. To formally study whether softmax attention recovers \(()\), we assume the covariates are generated as follows.

**Assumption 4.3** (Covariate Distribution).: _There are fixed constants \(c_{} 0\) and \(-<c_{}<\) s.t. sampling \(_{i} D_{}\) is equivalent to \(_{i}=c_{}_{i}+c_{}_{ }_{i}\) where \(_{i}^{k}\) and \(_{i}^{d-k}\)._

Assumption 4.3 entails that the data is generated by latent variables \(_{i}\) and \(_{i}\) that determine label-relevant and spurious features. This may be interpreted as a continuous analogue of dictionary learning models studied in feature learning works [33; 34]. We require no finite upper bound on \(|c_{}|\) nor \(}|}\), so the data may be dominated by spurious features.

**Theorem 4.4**.: _Let \(^{d k}\) and consider the pretraining population loss (ICL) with \(f D(_{}^{})\). Suppose Assumption 4.3 holds, as well as at least one of two cases: (Case 1) \(=0\), or (Case 2) \(n=2\). Then among all \(:=\{^{d d}:= ^{},\|^{}\|}^{2}}\}\), the minimizer of the pretraining population loss (ICL) is \(^{}=c^{}\) for some \(c(0,}^{2}}]\)._

Theorem 4.4 shows that softmax attention can achieve dimensionality reduction during ICL on any downstream task that has non-zero Lipschitzness only in \(()\) by removing the zero-Lipschitzness features while pretraining on \(_{}^{}\). Removing the zero-Lipschitzness features entails that the nearest neighbor prediction of pretrained softmax attention uses a neighborhood, i.e. attention window, defined strictly by projections of the input onto \(()\). To our knowledge, this is the _first result showing that softmax attention pretrained on ICL tasks recovers a shared low-dimensional structure among the tasks_. Please see Appendix J for empirical results verifying that softmax attention indeed recovers low-dimensional structure, even for tasks consisting of (nonlinear) generalized linear models.

## 5 Conclusion

We have presented, to our knowledge, the first results showing that softmax attention learns shared structure among pretraining tasks that facilitates downstream ICL. Moreover, we have provided empirical evidence suggesting that our conclusions about what softmax attention learns during pretraining generalize to function classes beyond those considered in our analysis.

**Limitations and Future Work. 1.** The model we use in this work is an attempt to understand a phenomenon that emerges in LLMs, which is that the output of the model can be 'primed' with some examples provided in the context that resembles few-shot learning, even though they are only trained on next token prediction. Establishing a mathematical framework for this remains an interesting question. **2.** We consider the output of a single layer of attention. Studying the nature of the solution when this is iterated over multiple trained layers is an interesting future prospect.

Figure 5: **Left, Middle-Left, Middle-Right: The test error for softmax attention as it is trained on the distributions over 1-Lipschitz affine, ReLU, and cosine function (\(D(_{1}^{})\), \(D(_{1}^{+})\), and \(D(_{1}^{})\), respectively), where the error is evaluated at each pretraining iteration on 5 tasks drawn from the distributions over the 1-Lipschitz (affine, ReLU, cosine) function classes in (Left, Middle-Left, Middle-Right), respectively. Right: The test error evaluated on tasks drawn from \(D(_{1}^{})\) for three softmax attention trained on tasks drawn from \(D(_{1}^{})\), \(D(_{0.1}^{})\), and \(D(_{10}^{})\), respectively.**

## 6 Acknowledgments

This work was supported in part by NSF Grants 2127697, 2019844, 2107037, and 2112471, ARO Grant W911NF2110226, ONR Grant N00014-19-1-2566, the Machine Learning Lab (MLL) at UT Austin, and the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program.