# Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization

Alex Foo Wynne Hsu Mong Li Lee

School of Computing

National University of Singapore

{alexfoo,whsu,leeml}@comp.nus.edu.sg

###### Abstract

Discovering object-centric representations from images has the potential to greatly improve the robustness, sample efficiency and interpretability of machine learning algorithms. Current works on multi-object images typically follow a generative approach that optimizes for input reconstruction and fail to scale to real-world datasets despite significant increases in model capacity. We address this limitation by proposing a novel method that leverages feature connectivity to cluster neighboring pixels likely to belong to the same object. We further design two object-centric regularization terms to refine object representations in the latent space, enabling our approach to scale to complex real-world images. Experimental results on simulated, real-world, complex texture and common object images demonstrate a substantial improvement in the quality of discovered objects compared to state-of-the-art methods, as well as the sample efficiency and generalizability of our approach. We also show that the discovered object-centric representations can accurately predict key object properties in downstream tasks, highlighting the potential of our method to advance the field of multi-object representation learning.

## 1 Introduction

Human understanding of the world relies on objects as compositional building blocks , and emulating this through object-centric representations can improve robustness, sample efficiency, generalization to out-of-domain distributions, and interpretability of machine learning algorithms [18; 9]. Recent work utilizes a generative approach, optimizing pixel-based reconstruction to learn object-centric representations [8; 17; 33; 4; 35; 12; 13; 11; 45; 46; 51]. This approach has limitations as it prioritizes pixel accuracy over object discovery and functional feature extraction [30; 11]. This may lead to the failure of discovering objects , or obtaining useful object features, such as position, shape, or boundaries between overlapping objects . Additionally, pixel-based reconstruction tends to waste model capacity on less important visual features, such as complex backgrounds , making scaling these methods to real-world images a challenge.

To address the fundamental limitations of pixel-based reconstruction, we propose a framework that leverages feature connectivity and design two object-centric regularization terms to directly refine object representations, ensuring sufficient separation and high disentanglement between dimensions. Our method utilizes visual connectedness principles , where similar pixels that are connected should belong to the same object, to guide object discovery. The two regularization terms promote disentangled representations and prevent sub-optimal clustering.

We demonstrate that our approach outperforms state-of-the-art methods in discovering multiple objects from simulated, real-world, complex texture and common object images in a fine-grained manner without supervision. The proposed solution attains sample efficiency and is generalizable to out-of-domain images. The learned object representations also accurately predict key objectproperties in downstream tasks. Our contributions include: (1) a framework that leverages feature connectivity for fine-grained object discovery, (2) introduction of object-centric regularization terms as an alternative to pixel-based reconstruction loss, (3) experimental validation of our solution's superior performance, and (4) demonstration of the usefulness of discovered object representations in downstream tasks.

## 2 Related Work

Numerous works have demonstrated remarkable success in segmenting real-world images. Most of these works focus on semantic segmentation and object detection by utilizing supervised signals [19; 6]. In contrast, unsupervised approaches like SLIC  employ a modified k-means algorithm to cluster pixels into superpixels, similar to Felzenszwalb's algorithm  that relies on hand-crafted features for clustering. However, these methods do not focus on learning useful representations for the segmented components.

Various unsupervised methods for learning object-centric representations have been proposed, and can be categorized into three main approaches: spatial attention, sequential attention, and iterative attention. Spatial attention approach utilizes spatial transformer networks  to crop out rectangular regions from an image and extract object attributes such as position and scale [14; 31; 8; 33]. They rely on a fixed-size sampling grid which may not be suitable for scenes with varying object sizes, and may compromise training when the sampling grid does not overlap with any object.

The sequential attention approach uses RNN-based models such as MONet  and GENESIS  to sequentially attend to different regions in an image. These methods employ a deterministic network to perform the attention process, which allows them to capture and represent objects in the scene. However, these methods may neglect smaller objects as they tend to produce a weaker signal during the attention process. This can lead to incomplete or biased representations of scenes with objects of varying sizes. To overcome this, GENESIS-V2  uses a stochastic stick-breaking process to perform attention randomly.

In the iterative attention approach, a set of object representations is randomly initialized and then iteratively refined to bind these objects to different regions of an image. IODINE  is a model that can discover objects with disentangled representations. However, it requires long training times and many samples. Slot Attention  introduces competition among the object representations by utilizing cross-attention along the object dimension. While this method is fast, versatile and can be extended to handle videos , it may fail to discover objects when the training set is diverse, and the resulting representations are highly entangled.

EfficientMORL , SLATE , SysBinder  and BO-QSA  are recent developments in the iterative attention approach aimed at addressing some limitations of earlier methods like Slot Attention. EfficientMORL presents an hierarchical variational autoencoder and a lightweight iterative refinement network to increase efficiency without sacrificing representation quality. SLATE increases the non-linear interaction between the slots in Slot Attention with an autoregressive decoder that is conditioned on the slots, resulting in improved reconstructions and object-centric representations. SysBinder enhances the slots of Slot Attention with factor representations called block-slots, which provides within-slot disentanglement between learned factors. BO-QSA initializes the slots of Slot Attention as learnable embeddings instead of sampling from a Gaussian distribution and uses bi-level optimization, resulting in more stable training. Despite the advancements, one drawback remains: the number of clusters are fixed a priori which limits the applicability in real-world scenarios where the number of objects or clusters is not known beforehand.

## 3 Methodology

Our proposed method OC-Net is designed to extract objects in an image without relying on labeled data or specifying the number of objects present in the image. By not requiring the number of objects to be specified beforehand, OC-Net can generalize better to real-world scenes with varying numbers of objects and handle more complex scenarios.

Figure 1 shows the main components of OC-Net. The main idea behind OC-Net is to learn pixel embeddings that can be clustered to discover objects and their respective object masks and object

representations. This is achieved by passing the input image through a \(1 1\) convolutional layer to obtain a set of \(N\) pixel embeddings \(=\{_{1},,_{N}\}\) of \(D\) dimensions each. We leverage on feature connectivity and iteratively cluster the embeddings of neighbouring pixels based on the likelihood that they belong to the same object. The output is a set of objects \(=\{O_{1},,O_{M}\}\) where each object is a set of pixel embeddings. We derive the object mask \(_{j}\) of each object by setting the pixel corresponding to the embedding in \(O_{j}\):

\[_{j}[i]=1&_{i} O_{j}\\ 0&\] (1)

where \(_{j}[i]\) denotes the \(i^{th}\) pixel value and \(i\{1,,N\}\).

With this, we obtain the matrix of object representations \(=[_{1},,_{M}]\) where each \(_{j}\) is the sum of extracted mask information and the average of the pixel embeddings in \(O_{j}\):

\[_{j}[d]=(_{j})[d]+|}_{ _{i} O_{j}}_{i}[d]\] (2)

where \(_{j}[d]\) denotes the \(d^{th}\) value of vector \(_{j}\), \(d\{1,,D\}\), \(\) is the mask transformation matrix.

### Object Discovery

The object discovery process iteratively clusters the pixel embeddings based on their feature connectivity and similarity. LayerNorm  is applied to normalize all pixel embeddings, and positional encodings are added to the pixel embeddings. The neighbors of a pixel embedding \(\) are the set of embeddings of the 8 neighbours in the input image. We use Dijkstra's algorithm to compute the shortest distance of a sampled pixel embedding to all other embeddings as follows.

Let \(\) be the set of pixel embeddings that have not been assigned to an object yet. We uniformly sample a pixel embedding \(_{i}\). The distance from \(_{i}\) to itself is set to zero, and the distance to all the other pixels is set to infinity. We select an unvisited pixel embedding \(_{m}\) that has the minimum distance to \(_{i}\). Let \(_{k}\) be the embedding of a neighbour of the pixel corresponding to \(_{m}\). We compute the distance between a pair of neighbouring pixels as the similarity between their corresponding embeddings given by:

\[(_{m},_{k})=^{D}(_{m} [d]-_{k}[d])^{2}}\] (3)

Figure 1: Overview of OC-Net.

where \(_{m}[d]\) denotes the \(d^{th}\) value of the embedding \(_{m}\).

If the distance between \(_{i}\) and \(_{k}\) is shorter through \(_{m}\), we update the shortest distance accordingly. This ensures that we consider the most efficient path between pixel embeddings, leading to better object discovery. We mark \(_{m}\) as visited and consider it to be part of the same object as \(_{i}\) according to a threshold. The process is repeated for the next unvisited pixel embedding until all pixels have been visited.

### Object-Centric Regularization

We design two object-centric regularization terms \(_{sep}\) and \(_{ent}\) to improve the quality of the learned object representations for downstream generalization and object discovery. Given the matrix of object representations \(=[_{1},,_{M}]\) corresponding to the object training samples \(\{(_{i},_{i})\}_{i=1}^{M}\), we quantify downstream generalization performance with the prediction error:

\[_{}=_{i=1}^{M}||_{i}- _{i}||\] (4)

where \(\) is the minimum-norm solution of the downstream predictor.

Since labels \(_{i}\) are unknown in the unsupervised setting, \(_{}\) cannot be directly minimized. Theorem 3.1 below shows that we can minimize an upper bound of \(_{}\) by using the projection matrix \(_{}\). Proof of the theorem is provided in the supplementary material.

**Theorem 3.1**.: _Let \(\) be the matrix of training sample labels and \(_{}\) be the projection matrix of \(\):_

\[_{}=-^{}(_{})^{ }\] (5)

_where \(\) is the identity matrix, \((.)^{}\) is the pseudoinverse and \(_{}=^{}\) is the unnormalized covariance matrix of \(\). Let \(||.||_{F}\) be the Frobenius norm. Then, the following relation holds:_

\[_{}||_{}||_{F}||||_{F}\] (6)

Since \(||||_{F}\) in Equation 6 is unknown but fixed, we can minimize \(_{}\) by minimizing \(||_{}||_{F}\). From the definition of \(_{}\) in Equation 5, \(||_{}||_{F}\) is minimized when the rank of \(_{}\) is maximized [44; 3]. We achieve this by maximizing the diagonal entries of \(_{}\) with a separation term \(_{sep}\) while simultaneously minimizing its off-diagonal entries with an entanglement term \(_{ent}\), in effect regularizing \(_{}\) to be a diagonal matrix with a maximum number of nonzero entries.

Maximizing the diagonal entries of \(_{}\) via \(_{sep}\) consequently maximizes the distance between object representations in the latent space. This encourages the model to learn distinct and non-overlapping object representations. Expanding the representation space also ensures that objects with varying features and properties can be accurately represented and distinguished from one another, enhancing both downstream generalization and fine-grained object discovery. We define \(_{sep}\) as:

\[_{sep}=_{d=1}^{D}(0,1-+})\] (7)

where \(_{d}\) is the variance of the \(d^{th}\) dimension across the vectors \(_{1},,_{M}\) and \(\) is a small constant to maintain numerical stability.

The entanglement term \(_{ent}\) minimizes the off-diagonal entries of \(_{}\) and consequently minimizes the correlation between dimensions in the latent space \(\), thereby achieves more disentangled object representations. Such representations are easier to manipulate and analyze, as each dimension captures a distinct object property, such as position, scale, or color. \(_{ent}\) is defined as follows:

\[_{ent}=_{i j}_{}[i,j]\] (8)

## 4 Performance Study

We conduct experiments to evaluate the performance of OC-Net in terms of quality, sample efficiency and generalizability. We use a diverse range of datasets to demonstrate its effectiveness across various scenarios:1. Simulated datasets Multi-dSprites  and Tetrominoes-NM. The former consists of multiple oval, heart or square-shaped sprites with some occlusions, while the latter is a subset of the original Tetrominoes dataset  where images whose ground truth segmentation requires knowledge of the object shapes are filtered out.
2. Real-world multi-object datasets SVHN  and IDRiD . SVHN consists of street view images of house numbers while IDRiD is the Indian Diabetic Retinopathy Image Segmentation Dataset.
3. Complex texture datasets CLEVRTEX  and CLEVRTEX-OOD. CLEVRTEX features scenes with diverse shapes, textures and photo-mapped materials while CLEVRTEX-OOD is the CLEVRTEX out-of-distrbution test set with 25 new materials and 4 new shapes.
4. Common object datasets Flowers , Birds  and COCO . The Flowers dataset features 17 diverse flower classes with large variations viewpoint, scale, illumination and background. Birds is the most widely-used CUB-200-2011 dataset for fine-grained visual categorization. COCO is the variant of the Microsoft Common Objects in Context dataset used for large-scale object segmentation .

Table 1 shows the dataset characteristics. Following , we use the first 60K samples in Multi-dSprites, Tetrominoes-NM and SVHN for training and hold out the next 320 samples for testing. For IDRiD, we split this dataset into 54 images for training and 27 images for testing. For CLEVRTEX, we use the first 40K samples for training and last 5K samples for testing. For CLEVRTEX-OOD, we use 10K samples for testing. For Flowers, we use the first 6K samples for training and last 1K samples for testing. For Birds, we use the first 10K samples for training and last 1K samples for testing. For COCO, we use the first 10K samples for training and last 2K samples for testing.

We compare OC-Net with SLIC , Felzenszwalb , Slot Attention , EfficientMORL , GENESIS-V2 , SLATE , SysBinder  and BO-QSA . We train OC-Net for 1000 iterations with a batch size of 64 using Adam  with a learning rate of \(1 10^{-3}\). We carried out an initial experiment to choose the clustering threshold. The results show that the value can range from 0.2 to 2.0 without affecting the performance of OC-Net. As such, we set the threshold to \(=0.7\) so that two pixels will belong to the same object if their normalized feature similarity is more than 50%. If a pixel is assigned to multiple objects, we assign it to the mask of the first object in that list and ignore its membership in other objects. Training on 64-by-64 images from Multi-dSprites on a single V100 GPU with 32GB of RAM takes about 10 minutes.

For all methods, we set the maximum number of foreground objects to 6 and 4 for Multi-dSprites and Tetrominoes respectively. Training is carried out for 300,000 iterations with a batch size of 64, using the Adam optimizer with a base learning rate of \(4 10^{-4}\). We set the size of the latent space to be \(D=64\) for all models. For SVHN and COCO, the number of objects is set to 6. For IDRiD, the number of objects is set to 20 and we train them for 100,000 iterations. For CLEVRTEX and CLEVRTEX-OOD, the number of objects is set to 11. For Flowers and Birds, the number of objects is set to 2.

We use the Adjusted Rand Index (ARI) to measure the quality of objects discovered . The ARI is a measure of similarity between two data clusterings that takes into account the permutation-invariant nature of the predicted segmentation masks and their corresponding ground-truth masks. We also

   Dataset & Type & Ground Truth & Image Size & \# Samples \\  Multi-dSprites & Simulated & Pixel Mask & \(64 64\) & 1M \\ Tetrominoes-NM & Simulated & Pixel Mask & \(35 35\) & 1M \\ SVHN & Real-World & Bounding Box & Varied & 530K \\ IDRiD & Real-World & Pixel Mask & \(4288 2848\) & 81 \\ CLEVRTEX & Complex Texture & Pixel Mask & \(128 128\) & 50K \\ CLEVRTEX-OOD & Complex Texture & Pixel Mask & \(128 128\) & 10K \\ Flowers & Common Object & Pixel Mask & \(128 128\) & 7K \\ Birds & Common Object & Pixel Mask & \(128 128\) & 11K \\ COCO & Common Object & Pixel Mask & \(128 128\) & 12K \\   

Table 1: Summary of dataset characteristicsuse the Dice similarity coefficient, along with the Intersection-over-Union (IoU) between the best matching object masks \(X\) and \(Y\) as follows:

\[(X,Y)=(X,Y)= \] (9)

where \(X\) the set of object pixels extracted and \(Y\) is the set of annotated object pixels in the ground truth. We compute the mean Dice and the mean IoU scores, denoted as mDice and mIoU respectively, by averaging the individual Dice and IoU scores across all matches. For the complex textures dataset, background discovery is included in the computation of the scores.

### Experiments on Quality of Discovered Objects

We first evaluate the ability of OC-Net to discover objects from images with multiple objects. Table 2(a) shows the average ARI, mDice and mIoU scores based on the discovered foreground objects in the simulated datasets after 3 runs. We observe that OC-Net outperforms all other methods by a large margin in Multi-dSprites, and even achieves perfect score for all Tetrominoes-NM test samples.

Table 2(b) shows the results on the real-world multi-object image datasets. For SVHN, OC-Net outperforms all methods, even when the ground truth is provided in the form of bounding boxes. This implies that we need to expand the discovered object masks into their corresponding bounding boxes which are often rough fits, and is the reason for the close difference in mDice and mIoU scores between OC-Net and BO-QSA. For IDRiD, which contains multiple small objects, OC-Net significantly improves the ARI scores and more than doubles the mDice and mIoU scores over all methods, demonstrating its robustness in challenging object discovery tasks.

Table 2(c) shows the results on CLEVRTEX and CLEVRTEX-OOD, which contains complex textured objects and backgrounds. Here, OC-Net again shows superior performance in all metrics, illustrating its capability to effectively segment complex objects. Although a general decrease in performance is observed across all methods in the CLEVRTEX-OOD test set, likely due to a change in data distribution, OC-Net's performance drop is slight and it still outperforms the closest baseline.

Finally, Table 2(d) shows the results on Flowers, Birds and COCO common object datasets. Here, OC-Net again shows superior performance in all metrics, illustrating its capability to effectively segment commonly seen natural objects. Notably, OC-Net outperforms all other methods by a large margin in COCO, demonstrating its robustness in handling objects with highly varied appearances.

Figure 2 visualizes the objects discovered by the various methods for sample images. OC-Net is able to identify large and small objects in Multi-dSprites even when these objects are significantly occluded. Moreover, in the Tetrominoes-NM dataset, despite the presence of shadow effects that often confuse existing methods, OC-Net still manages to separate each tile. For SVHN, only OC-Net is able to segment the character objects out in a fine-grained manner. EfficientMORL tend to group all the characters together while the other methods segment the objects in a coarse-grained manner. For IDRiD, OC-Net is able to segment out the optic disc and small lesions which other methods fail to discover. For CLEVRTEX and CLEVRTEX-OOD, OC-Net is able to segment out the various objects from the complex-textured backgrounds in a fine-grained manner. Finally, for Flowers, Birds, and COCO, only OC-Net is able to segment out the complex-shaped and multi-part objects from the backgrounds in a fine-grained manner.

### Experiments on Sample Efficiency

One obstacle to unsupervised object discovery is the availability of a sufficiently large number of suitable training samples. Sample efficiency refers to a model's ability to learn effectively from a relatively small number of examples. Figure 3 shows the mIoU scores as we decrease the number of training samples in Multi-dSprites, SVHN and CLEVRTEX. OC-Net is able to achieve near-optimal performance even with a significantly smaller training set (1,000 samples) compared to all the other methods. The high sample efficiency of OC-Net reduces the need for large, potentially costly or difficult-to-obtain datasets. This makes OC-Net a more practical solution for real-world applications.

Table 2: Evaluation scores for the discovered foreground objects.

### Experiments on Model Generalizability

Ideally, an unsupervised object discovery model should be trained to understand common visual appearances such as the difference between foreground vs background, so as to discover objects from

Figure 2: Visualization of discovered objects.

out-of-domain images. In this set of experiments, we compare the generalization ability of OC-Net with the baselines by training the models on Multi-dSprites and testing them on the other datasets.

Table 3 shows the results. For Tetrominoes-NM, there is a decrease in performance for all methods while OC-Net still obtains perfect score. For SVHN, CLEVRTEX and CLEVRTEX-OOD, the performance of all models decrease due to the shift in data distribution. However, OC-Net experiences the smallest drop in performance and still significantly surpasses the best performing method. For IDRiD, the methods show improvement in performance. One possible reason is that the larger training set in Multi-dSprites enables the circular shape of the optic disc to be better segmented. Despite this, OC-Net remains the top performer.

### Ablation Studies

Next, we examine the effect of feature connectivity and regularization terms \(_{sep}\) and \(_{ent}\) on the performance of OC-Net. We implemented three variants of OC-Net: (a) w/o connectivity. Here, we do not require that a path should exist between \(i\) and \(k\) when computing \([i,k]\); (b) w/o \(_{ent}\). This network is trained without the entanglement regularization term; (c) w/o \(_{sep}\). The separation regularization term is not used in the training of OC-Net.

Table 4 shows the mIoU scores for all the datasets. Without feature connectivity, we observe a drop in performance across all datasets since it is common for images to have multiple identical objects, that is, same color and shape. As such, OC-Net w/o connectivity tend to cluster these blocks as a single object.

Removing the entanglement term (OC-Net w/o \(_{ent}\)) leads to a slight decrease in the performance as the object representations may still be separated even when the dimensions are entangled. The largest performance drop in all datasets is seen when the object representation separation term is removed (OC-Net w/o \(_{sep}\)), indicating the importance of having a well-separated object representation space for effective object discovery.

   Method & Tetrominoes-NM & SVHN & IDRiD & CLEVRTEX & CLEVRTEX-OOD \\  Slot Attention & 21.8\(\)3.5 & 19.5\(\)3.8 & 7.5\(\)2.5 & 12.2\(\)2.2 & 12.3\(\)2.2 \\ EfficientMORL & 21.2\(\)3.8 & 23.4\(\)2.8 & 6.5\(\)2.6 & 12.7\(\)3.2 & 15.2\(\)2.0 \\ GENESIS-V2 & 42.9\(\)4.9 & 31.1\(\)2.8 & 8.5\(\)2.4 & 21.9\(\)1.6 & 21.3\(\)2.5 \\ SLATE & 51.4\(\)1.6 & 21.1\(\)2.0 & 10.0\(\)1.7 & 12.7\(\)2.2 & 12.9\(\)1.8 \\ SysBinder & 28.5\(\)1.8 & 23.8\(\)1.1 & 13.9\(\)1.8 & 10.6\(\)1.5 & 11.6\(\)1.9 \\ BO-QSA & 41.8\(\)1.8 & 24.3\(\)2.0 & 4.0\(\)1.5 & 24.4\(\)1.4 & 22.8\(\)2.5 \\ OC-Net & **100.0\(\)0.0** & **47.5\(\)0.5** & **29.1\(\)0.5** & **31.7\(\)0.6** & **31.3\(\)0.6** \\   

Table 3: mIoU scores for model generalizability after training on Multi-dSprites.

Figure 3: mIoU scores vs decreasing number of training samples.

## 5 Prediction based on Learned Object Representation

One characteristic of an effective object-centric representation is its ability to encode object properties such as color, position and shape . In this section, we show that the learned object representations are disentangled and can be used to predict the values of these properties.

Given the representations and their corresponding ground truth values of a target object property, we employ them as features to train a gradient boosted tree (GBT) . To evaluate how well the properties of unseen objects are predicted by the GBT, we use the coefficient of determination \(R^{2}\). We perform an experiment using the learned object representations from the simulated datasets to predict the properties of object such as color, position and shape. We use the mIoU score to match the discovered object to the ground truth object. We split the 320 test images equally into two sets, one for training the GBT model for each object property, and the other for evaluation.

Table 5 shows the average \(R^{2}\) scores of the GBT models on the evaluation set. The GBT models trained with OC-Net representations achieved the highest \(R^{2}\) scores compared to the models trained using the representations from other methods. This suggests that the object representations learned by OC-Net are effective in encoding the object properties.

## 6 Conclusion

In this work, we have described a framework called OC-Net that learns object-centric representations in a fine-grained manner without supervision. OC-Net leverages on feature connectivity and two new regularization terms to learn disentangled representations and to ensure the representations of different objects are well-separated. From the results of experiments conducted on simulated, real-world, complex texture and common object images, we have demonstrated the superior quality of the object representations over current state-of-the-art. Moreover, we have highlighted the sample efficiency and generalizability of OC-Net. Finally, we have shown how the discovered object representations can be used to predict object properties in a downstream task, indicating its potential use for other computer vision applications where samples and ground truth labels are limited.

There are still obstacles that have to be overcome for successful application of our framework to the full visual complexity of the real world. A natural next step would be to extend OC-Net to handle real-world scenes containing objects with more complex part-whole hierarchies. It is also promising to explore explicit representation of the discovered objects into a dictionary of prototypes to better handle occlusion between objects. Lastly, real-world scenes with multiple objects is still of higher visual complexity than the datasets considered here and reliably bridging this gap is an open problem.

   Method & Multi-dSprites & Tetro-NM & SVHN & IDRiD & CTEX & CTEX-OOD \\  OC-Net & **99.1\(\)0.0** & **100.0\(\)0.0** & **49.9\(\)0.1** & **31.2\(\)0.2** & **37.5\(\)0.7** & **35.0\(\)0.6** \\ w/o connectivity & 98.8\(\)0.1 & 89.2\(\)0.1 & 36.6\(\)0.1 & 17.3\(\)0.1 & 20.3\(\)0.9 & 17.7\(\)0.9 \\ w/o \(_{ent}\) & 98.7\(\)0.3 & 99.6\(\)0.1 & 48.8\(\)0.1 & 25.8\(\)0.2 & 26.4\(\)0.8 & 18.9\(\)0.4 \\ w/o \(_{sep}\) & 49.3\(\)0.3 & 51.2\(\)0.2 & 35.0\(\)0.2 & 4.0\(\)0.1 & 18.5\(\)0.6 & 11.0\(\)0.4 \\   

Table 4: mIoU scores for variants of OC-Net.

    &  &  \\  Method & Color & Position & Shape & Color & Position & Shape \\  Slot Attention & 72.2\(\)12 & 96.8\(\)0.1 & 38.2\(\)0.0 & 86.5\(\)6.5 & 98.7\(\)0.6 & 36.3\(\)0.0 \\ EfficientMORL & 86.5\(\)6.2 & 95.8\(\)0.1 & 61.7\(\)0.0 & 94.9\(\)3.2 & 97.9\(\)0.7 & 68.5\(\)0.0 \\ GENESIS-V2 & 78.1\(\)7.5 & 97.1\(\)0.7 & 75.8\(\)0.0 & 88.1\(\)5.8 & 94.6\(\)2.6 & 37.9\(\)0.0 \\ SLATE & 87.5\(\)0.7 & 90.6\(\)4.4 & 31.7\(\)0.0 & 85.5\(\)3.9 & 89.6\(\)0.7 & 10.5\(\)0.0 \\ SysBinder & 73.6\(\)1.0 & 69.3\(\)3.4 & 33.3\(\)0.0 & 97.9\(\)0.6 & 77.8\(\)2.7 & 19.9\(\)0.0 \\ BO-QSA & 96.3\(\)1.6 & 97.4\(\)0.1 & 75.2\(\)0.0 & 98.1\(\)0.7 & 98.9\(\)0.2 & 52.5\(\)0.0 \\ OC-Net & **98.0\(\)0.6** & **98.3\(\)0.1** & **78.1\(\)0.0** & **100.0\(\)0.0** & **99.4\(\)0.1** & **98.7\(\)0.0** \\   

Table 5: \(R^{2}\) scores for object property prediction on simulated datasets