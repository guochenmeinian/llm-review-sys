# dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans

Marek Herde\({}^{*}\) Denis Huseljic  Lukas Rauch  Bernhard Sick

University of Kassel, Hesse, Germany

\({}^{*}\)marek.herde@uni-kassel.de

###### Abstract

Human annotators typically provide annotated data for training machine learning models, such as neural networks. Yet, human annotations are subject to noise, impairing generalization performances. Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation. Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels. For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67\(\%\). Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata. We benchmark well-known multi-annotar learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning. Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results.

## 1 Introduction

Supervised learning with machine learning models, such as neural networks (NNs), requires annotated data. Typically, multiple human annotators, e.g., crowdworkers , are tasked to provide the corresponding annotations, e.g., class labels. These annotators perform differently for various reasons, including bias, fatigue, and ambiguity in interpretation . As a result of such imperfect annotator performances, we obtain noisy annotations that can significantly degrade models' generalization performances . Therefore, annotations are often requested from multiple annotators per data instance. The intuition is that the majority vote is a reliable estimate of the ground truth annotation ("wisdom of the crowd"). However, such an approach leads to substantially higher annotation costs and ignores the annotators' different performances. More advanced approaches have been proposed to improve NNs' robustness against noisy annotations. These advancements include new regularization techniques , loss functions , and approaches to estimate annotator performances . The evaluation of such approaches is primarily driven by empirical research, which necessitates access to datasets that realistically reflect the noise induced by human annotators . However, due to the often high annotation costs, the number of publicly available datasets for methodological research is relatively low. Moreover, the potential ability of certain humans to self-assess their own knowledge and uncertainty is typically not queried as part of the annotation campaign. Accordingly, most existing datasets are limited to classification tasks that require no expert knowledge, provide only hard class labels as annotations, and lack metadata  about the annotators.

Motivated by the critical impact of noisy annotations in practical applications and the scarcity of corresponding datasets, we publish a novel dataset with the following profile and contributions:This article's remainder is structured as follows: Section 2 discusses related datasets. A description of the data collection process of our dataset dopanim and exemplary analyses are part of Section 3. We introduce variants of our dataset in Section 4 for benchmarking multi-annotator learning approaches. Section 5 presents further use cases of our dataset. We conclude our work in Section 6.

## 2 Related Datasets

Learning from noisy annotations covers diverse problem settings, which mainly differ in the learning tasks and their assumptions about the annotations' origin. For example, publicly available datasets with noisy annotations from humans exist for image segmentation  and sequence classification tasks . Other datasets are scraped from the web , where noisy annotations arise due to unreliable web resources. We focus on basic classification tasks with annotations from multiple humans for two reasons: (1) Classification tasks are the most common research topic in learning from noisy annotations and often serve as starting points for extensions to other learning tasks . (2) Due to crowdsourcing and annotation companies, human annotators are a popular resource for annotating datasets . Along with these annotations, we can easily get information on which annotation originates from which annotator. With this scope, we use Table 1 in the following to discuss popular and publicly available datasets regarding their task and annotation data, including dopanim for comparison. Further datasets with partial relevance to our scope are detailed in the appendices.

Figure 1: Simplified illustration of the data types included by dopanim – Two of three annotators provide probabilistic labels (after normalization) to identify the animal in the image. In addition to these annotations, annotation times and annotator metadata, e.g., interest in zoology, are available.

### Task Data

Under task data, we summarize the data essential for defining the classification task. Most datasets focus on image classification. In particular, the datasets providing noisy annotations for the popular image benchmark data cifar10, cifar100, and labelme are used for generic object classification. The dataset mgc deals with the classification of music audio files according to their genre, while spc deals with the classification of the polarity of sentences. Many of these classification tasks share the characteristic that no special domain knowledge is required for correct classification. In contrast, our dataset dopanim considers the challenging image classification of four groups with classes of highly similar animals. Thus, our dataset enables the investigation of learning scenarios where annotators have varying levels of domain knowledge. The animal10n dataset also targets classifying animals but is limited to pairs of mildly confusable animals. Except for cifar100n, the number of classes in the other datasets tends to be low. This also applies to dopanim due to the already high complexity of distinguishing the 15 animal classes. Furthermore, all datasets provide predefined splits into training and test data, but only labelme and dopanim provide extra validation data to ensure reproducibility when optimizing hyperparameters. After filtering invalid images, e.g., ones showing only animal bones, the splits of dopanim contain approximately the same number of images per class.

### Annotation Data

Under annotation data, we summarize the data related to the annotation process. The number of annotators is relatively high for almost all datasets, as these annotators are recruited via large crowdsourcing platforms such as Amazon Mechanical Turk (AMT)  and Prolific . However, since a limited annotation budget has to be distributed among many annotators, the number of annotations per annotator is rather low. Accordingly, analyzing each annotator's behavior in depth is difficult. In contrast, the annotations of dopanim originate from fewer annotators, each of whom has provided many annotations via LabelStudio . The high standard deviation in the number of annotations per annotator is typical in practice . The dataset animal10n also includes numerous annotations per annotator, though their average accuracy is estimated to be relatively high. Conversely, the partially low overall annotation accuracies for dopanim and the other datasets combined with the quite large differences between the annotators' individual accuracies demonstrate the need for multi-annotator learning techniques. Beyond hard class labels, more informative annotation types can also be requested. In particular, soft class labels capture the annotators' subjective uncertainties regarding their decisions. For cifar10s, the probabilities for the two most probable class labels and any class label to which the image does definitely not belong are available. Our dataset dopanim provides soft class labels, where the annotators could distribute unnormalized likelihood scores across all class labels to reflect their uncertainties. Other important data can be collected in addition to the annotations when annotating. This includes metadata about the annotators , such as self-assessed motivation, and their annotation times. While annotation times are provided by some other datasets, detailed annotator metadata is only provided by dopanim.

## 3 dopanim: A Dataset of Doppelganger Animals

This section describes the task and annotation data collection. Further details, including ethical considerations, are given in the appendices and the codebase to emulate the data collection.

### Collection and Analysis of Task Data

Our dataset dopanim targets a classification task with images of animal species with groups of highly similar appearances, which we call doppelgangers. Hence, accurate annotations require domain knowledge and a high level of attention. The images originate from iNaturalist , a platform whose observers contribute biodiversity observations.

Figure 2: \(t\)-SNE of validation images’ embeddings from a DINOv2 ViT-S/14 fine-tuned on dopanim.

The collected annotation data comprises four main components: (1) tutorials, (2) pre-questionnaire, (3) post-questionnaire, and (4) individual image annotations. Components (1)-(3) allow us to extract annotator metadata, which captures task-related information about the annotators, e.g., the interest in wildlife. In addition to the assigned unnormalized likelihoods, component (4) includes the required annotation times and timestamps. All this annotation data is detailed in the appendices and can be used to evaluate various learning scenarios (cf. Section 5 for corresponding use cases). For example, we can determine the top-label predictions from the likelihoods to compute the confusion matrix in Fig. 4. We can see that animal classes are mainly confused within a group of doppelganger animals.

## 4 Benchmark: Multi-annotator Learning

This section presents a benchmark of common approaches in multi-annotator learning , also called learning from crowds . We briefly outline this research area's foundations before presenting seven variants of dopanim as the basis for the experimental setup, empirical results, and future research. Our appendices further detail this benchmark, e.g., by listing computational resources.

### Foundations

Multi-annotator learning approaches typically assume independently working annotators and require knowing which annotation originates from which annotator. This information allows them to estimate annotators' individual performances for correcting noisy class labels. Major differences in the approaches arise in their assumptions about these performances and their training procedures .

#### Annotation Performance Assumptions

The simplest assumption is that an annotator's performance is constant across all classes and instances, often represented by a single accuracy score per annotator . However, this assumption is unrealistic, e.g., due to varying difficulty levels across different classes and instances. Therefore, performance is often modeled with class dependency, typically by estimating a confusion matrix for each annotator [26; 53]. This matrix captures the conditional probability of an annotator assigning a certain class label, conditioned on the actual class of the instance. Some approaches also incorporate instance dependency, which considers the variability in annotator performance based on specific instances [74; 45]. For example, annotators may perform better in certain regions of the feature space. However, this most realistic assumption comes at the cost of increased training complexity.

#### Training Procedures

Training procedures are often divided into one-stage and two-stage procedures . _Two-stage_ procedures aggregate multiple class labels per instance as estimates of the ground truth class labels in the first stage. These aggregated labels are then used for standard supervised learning in the second stage. The label aggregation is implemented via ground truth inference algorithms . The simplest algorithm is majority voting, specifying an instance's class label as

    & **Venue** & **Year** & **Annotator Performance Model** & **Training** & **Metadata** \\   \\  cl & AAAI & 2018 & noise adaptation layer per annotator & cross-entropy & \\ trace-reg & CVPR & 2019 & confusion matrix per annotator & cross-entropy + regularization & ✗ \\ conal & AAAI & 2021 & noise adaption layer per and across annotators & cross-entropy + regularization & ✗ \\ union-con & TNNLS & 2022 & noise adaption layer across annotators & cross-entropy & ✗ \\ geo-reg-w & ICLR & 2023 & confusion matrix per annotator & cross-entropy + regularization & ✗ \\ geo-reg-f &  \\   madl & TMLR & 2023 & confusion matrix per instance-annotator pair & cross-entropy + regularization & ✗ \\ crowd-ar & SIGIR & 2023 & reliability scalar per instance-annotator pair & two-model cross-entropy & ✓ \\ annot-mix & ECAI & 2024 & confusion matrix per instance-annotator pair & cross-entropy + mixup extension & ✓ \\   

Table 2: Overview of one-stage multi-annotator learning approaches – The first column lists the names of the approaches, with the following columns detailing relevant attributes for each approach.

Figure 4: Confusion matrix across all human top-label predictions.

the one with the most annotator votes. Thereby, majority voting assumes annotators have equal performance . Advanced ground truth inference algorithms [9; 64; 54] drop this naive assumption by estimating annotators' performances for label aggregation. As an alternative to aggregated hard labels, soft majority voting normalizes the annotators' votes across the potential classes to obtain a probabilistic label vector. Further, training can be restricted to instances with strong agreement among annotators' class labels. All these algorithms typically require multiple class labels per instance , leading to high annotation costs. _One-stage_ training procedures overcome this requirement, enabling learning with just one class label per instance. Typically, this involves training two models: a classification model and an annotator performance model . Early one-stage procedures leverage the expectation-maximization (EM) algorithm, where the E-step estimates the ground truth labels as latent variables, and the M-step updates the models based on these estimates [39; 1]. State-of-the-art procedures use NN-based end-to-end systems, coupling both models' outputs into a single loss function for simultaneous training [7; 20; 18].

### Dataset Variants

We create seven dataset variants (annotation subsets) of dopanim to test different learning scenarios. Two critical variables in multi-annotator learning are annotator performance and the number of annotations per instance . We simulate varying annotator performance levels by either randomly selecting annotations per instance or by selecting the worst (false if available) annotations per instance . Further, we control the number of annotations per instance, testing scenarios with very few (1 or 2), a variable number (v), and many (full) annotations per instance. Table 3 summarizes the seven variants' statistics. For future research, dataset users are free to create additional variants.

### Experimental Setup

_Approaches._ We consider state-of-the-art approaches from the literature. Thus, we focus on one-stage end-to-end approaches, which train NNs by estimating annotator performances to counteract noisy annotations. Table 2 overviews the corresponding approaches. For transparency, we note that the approaches mad1  and annot-mix  were proposed by our research group in previous works. As an upper baseline, we evaluate the training with the ground truth class labels (gt-base). As lower baselines, we train with (hard) majority vote class labels (mv-base), with soft majority vote labels (smv-base), and only on instances with (hard) majority vote labels achieving at least 70\(\%\) annotation agreement (sf-base). The latter baseline is inspired by the concept of selection frequency .

_Evaluation Scores._ For quantitative evaluation, we employ three scores suitable for balanced classification problems. Accuracy (acc) evaluated on the test data is the most common measure of generalization performance. Since many applications require more than just the actual class prediction, we also assess the quality of the predicted class probabilities. Specifically, we employ the Brier score  (bs) as a proper scoring rule and the top-label calibration error  (tce) with a more intuitive interpretation.

_Architecture._ With the advance of self-supervised learning , numerous pre-trained model architectures for image data are available, making training from scratch necessary only in special application domains. Thus, we use a pre-trained DINOv2 ViT-S/14  as our backbone. The training of the multi-annotator

    & **Value** \\   \\  backbone & DINOv2 ViT-S/14 \\ classification head & MLP \\   \\  optimizer & RAdam \\ learning rate scheduler & cosine annealing \\ number of epochs & 50 \\ initial learning rate & 1e-3 \\ batch size & 64 \\ weight decay & 0 \\ dropout rate & 0.5 \\   

Table 4: Hyperparameters.

    & **worst-1** & **worst-2** & **worst-v** & **rand-1** & **rand-2** & **rand-v** & **full** \\   \\  annotations per instance [\#] & 1.0\({}_{ 0.00}\) & 2.0\({}_{ 0.00}\) & 3.0\({}_{ 1.4}\) & 1.0\({}_{ 0.00}\) & 2.0\({}_{ 0.00}\) & 3.0\({}_{ 1.4}\) & 5.0\({}_{ 0.2}\) \\ annotations per annotator [\#] & 524\({}_{ 26}\) & 1.04\({}_{ 8.25}\) & 1.555\({}_{ 71}\) & 524\({}_{ 26}\) & 1.048\({}_{ 51}\) & 1.564\({}_{ 76}\) & 2.602\({}_{ 1.25}\) \\ overall accuracy [\%] & 22.4 & 37.3 & 54.8 & 67.5 & 67.2 & 67.3 & 67.3 \\ majority voting accuracy [\%] & 22.4 & 37.8 & 53.1 & 67.5 & 67.1 & 73.7 & 80.7 \\ accuracy per annotator [\%] & 25.8\({}_{ 11.7}\) & 39.4\({}_{ 15.4}\) & 54.3\({}_{ 16.1}\) & 65.2\({}_{ 16.6}\) & 65.6\({}_{ 14.8}\) & 65.6\({}_{ 14.7}\) & 65.6\({}_{ 14.7}\) \\   

Table 3: Overview of dopanim variants – Column headings indicate the names of the dataset variants, whereas a row provides statistics about an annotation data characteristic of the respective variants. We denote absolute numbers by the \(\#\) symbol. Means are supplemented by standard deviations.

learning approaches is then implemented by fine-tuning the backbone's classification head in the form of a multi-layer perceptron (MLP) with 128 hidden neurons, batch normalization , dropout , and rectified linear units (ReLU)  as activation function.

_Training._ We employ RAdam  as the optimizer across all dataset variants and multi-annotator learning approaches. Further, we schedule the learning rate over 50 training epochs via cosine annealing . Concrete values for the other hyperparameters, i.e., initial learning rate, batch size, and weight decay are determined by optimizing the validation accuracy of the upper baseline gt-base and are reported in Table 4. Hyperparameters specific to each approach are defined according to the authors' recommendations. This way, we aim to avoid biasing results in favor of or against any particular approach, allowing for a fair empirical comparison . Each training is repeated ten times with different random initializations of the NNs' parameters per dataset variant and approach. All results are reported as means and standard deviations over the ten repetitions of the respective experiment.

### Empirical Results

Table 5 reports the individual results for all approaches, dataset variants, and evaluation scores. A more compact overview of these results (excluding the upper baseline gt-base) is given in the form of a ranking by Fig. 5. Lower ranks indicate better evaluation scores. As expected, the tabular results confirm that training with the ground truth class labels (gt-base) is superior across all dataset variants and evaluation scores. Learning from the majority vote (mv-base) and soft majority vote (smv-base) labels as lower baselines leads on average to inferior accuracy results in comparison with all other approaches, whereas the naive idea of ignoring instances with ambiguous annotations (sf-base) performs partially better than the worst one-stage multi-annotator learning approaches. Nevertheless, there are several one-stage multi-annotator learning approaches, e.g., geo-reg-w, geo-reg-f, and annot-mix, achieving noticeable accuracy and Brier score improvements compared to the lower baselines. In contrast, the top-label calibration errors of mv-base are partially competitive or even superior, suggesting room for improving the one-stage multi-annotator learning approaches' calibration. Approaches modeling instance-dependent annotator performances are not consistently better than approaches modeling only class-dependent annotator performances, likely due to the more complex training of annotator performance models. Overall, the most superior approach is annot-mix using a mixup  extension, followed by geo-reg-f with a regularized loss function.

_Takeaway:_ The empirical results on the seven variants of dopanim demonstrate the potential benefit of using one-stage multi-annotator learning approaches to counteract annotation noise.

## 5 Further Use Cases

This section discusses three illustrative evaluation use cases to demonstrate our dataset's potential for exploring further research areas of machine learning.

### Beyond Hard Class Labels

_Foundations._ Instead of forcing hard decisions on one class label, human annotators can assign likelihoods to express their (subjective) uncertainty about an annotation decision.

_Research Question._ Are the human-estimated likelihoods (probabilistic labels after normalization) reliable, and can they act as weights for the annotators' votes to improve the generalization performance of the lower baselines mv-base, smv-base, and sf-base?

Figure 5: Mean ranks (\(\)) across the seven dataset variants.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Conclusion and Limitations

_Conclusion._ We introduced the dopanim dataset containing images with ground truth labels of animal species with groups of similar appearance (doppelganger animals). In a comprehensive annotation campaign, error-prone humans annotated this dataset using likelihoods to express their subjective uncertainty. The received annotations are supplemented with annotator metadata  containing task-related information collected via questionnaires, tutorials, and annotation statistics. A benchmark study evaluated the benefit of multi-annotator learning approaches  for seven variants of dopanim. Moreover, three use cases demonstrated dopanim's potential for further research. Data and codebase, including multi-annotator learning approaches, backbones, experiments, and evaluation protocols, are publicly accessible, facilitating methodological research in various machine learning fields.

_Limitations._ Due to the high costs of annotations, dopanim is a small-scale dataset. To test scalability, more images and animal classes must be included and annotated. For this purpose, our codebase for collecting dopanim using iNaturalist  and LabelStudio  can be easily adapted. Additionally, our benchmark of multi-annotator learning approaches is limited to the seven variants of dopanim, DINOV2 ViT-S/14  as the backbone architecture, and one hyperparameter configuration (specified via the validation accuracy of the upper baseline gt-base) per approach. However, our codebase includes other backbones and related datasets with noisy annotations from multiple humans, allowing for future benchmark extensions. Particularly, such extensions may include the evaluation of techniques for hyperparameter optimization, e.g., number of training epochs via early stopping , in the presence of noisy labels and approaches for learning from noisy labels , which do not rely on the information which label originates from which annotator. The three presented use cases also require further investigations to derive conclusive results. For example, it is still unclear whether probabilistic labels are also beneficial from a cost-sensitive perspective because Fig. 10 indicates increasing annotation times with an increasing entropy of the (normalized) human estimated likelihoods. Yet, such a trend does not directly quantify the actual amount of higher costs for querying likelihoods because also requesting hard class labels can take more time for uncertain (difficult) images. Finally, dopanim is a dataset targeting classification tasks where an objective ground truth exists. However, certain tasks involve subjective class labels, e.g., assessing emotions  or laughter , reflecting variations in annotators' interpretations.