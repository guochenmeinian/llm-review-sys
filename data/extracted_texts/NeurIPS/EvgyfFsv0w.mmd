# Stylebreeder : Exploring and Democratizing Artistic Styles through Text-to-Image Models

Matthew Zheng\({}^{1,}\) Enis Simsar\({}^{2,}\)\({}^{*}\) Hidir Yesiltepe\({}^{1}\)

Federico Tombari\({}^{3,4}\) Joel Simon\({}^{5}\) Pinar Yanardag\({}^{1}\)

\({}^{1}\)Virginia Tech \({}^{2}\)ETH Zurich \({}^{3}\)TUM \({}^{4}\)Google \({}^{5}\)Arbreeder https://stylebreeder.github.io

Joil Simon\({}^{5}\) Pinar Yanardag\({}^{1}\)

###### Abstract

Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, user-generated styles that transcend conventional categories like 'cyberpunk' or 'Picasso,' we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code, and models are available at https://stylebreeder.github.io under a Public Domain (CC0) license.

## 1 Introduction

Text-to-image models, such as Denoising Diffusion Models (DDMs)  and Latent Diffusion Models (LDMs) , are becoming increasingly popular and are revolutionizing the landscape of digital art creation. These models are renowned for their capability to generate high-quality, high-resolution images across diverse domains, enabling the production of highly detailed and creative visual content . Artists worldwide are increasingly leveraging these models, utilizing diverse textual prompts to create artworks spanning myriad styles, thereby democratizing the art creation process and making it more accessible.

In the midst of this technological renaissance, platforms like Artbreeder have surfaced as pivotal hubs for creative exploration. Artbreeder, with its user base exceeding 13 million, facilitates the generation of millions of images that embody a vast spectrum of artistic styles.

This surge in user-generated content presents an intriguing question: beyond the conventional styles typically prompted by terms like 'cyberpunk' or 'Picasso,' what unique, crowd-sourced styles might exist within such a community? These styles, potentially undocumented and uniquely communal, could offer profound insights into the collective creative psyche of users worldwide.

However, existing datasets often fall short in terms of exploring the visual potential of user-generated images. While some works similar to ours, such as Diffusion DB  and TWIGMA  also explore AI-generated content, they either feature a smaller user base, do not provide original text prompts associated with the images, or provide limited insights into artistic styles from a visual perspective. Addressing this inquiry, our paper introduces a comprehensive dataset derived from Arthreeder, providing contributions from 95K unique users, 6.8M images and 1.8M text prompts generated with a variety of text-to-image diffusion models such as Stable Diffusion (SD)  or Stable Diffusion XL (SD-XL)  (see Fig. 1). We identified and analyzed novel, user-generated artistic styles, uncovering diverse and previously unrecognized creative expressions. Based on the discovered styles, we showcase a variety of tasks, including personalization and recommendation. Our contributions are as follows:

* We present an extensive dataset, STYLEBREEDER, from Artbreeder on CC0 license, capturing millions of user-generated images and styles and sharing them with the community to further encourage research in this area.
* We cluster images based on their stylistic similarities, helping to map out the landscape of user-generated art.
* Utilizing historical data of the users, we showcase a recommendation system that aligns style suggestions with individual preferences, making the exploration of artistic styles more targeted and meaningful.
* We release a web-based platform, Style Atlas, providing public access to download pre-trained style LoRAs for personalized content generation, encouraging experimentation and collaborative artistic exploration to expand our understanding of digital creativity and further democratize the creative use of AI.

## 2 Related Work

We provide a brief overview of artwork datasets, followed by discussions on text-to-image diffusion models and personalized image generation.

Figure 1: Our dataset comprises 6.8M images generated by 95,000 unique users, accompanied by 1.8M text prompts from July 2022 to May 2024. It includes detailed metadata such as Positive Prompt, Negative Prompt, UserID, Timestamp, and Image Size. Additionally, we supply model-related hyperparameters, including Model Type, Seed, Step, and CFG Scale. Note that the disparity in prompts and images arises because different images can be generated from the same text prompt when varying hyperparameters. We also offer further metadata like Cluster ID, along with scores for Prompt NSFW, Image NSFW, and Toxicity computed using state-of-the-art models .

### Artwork Datasets

Traditional artwork datasets (see Supplemental Materials for a comparison) have primarily focused on artwork classification and attribute prediction. However, these datasets often exhibit limitations, like skewed class distributions and unsuitable classes for image synthesis, when employed for artwork synthesis. To address these shortcomings in evaluating artwork synthesis, specialized subsets have been curated to better suit the task. For instance,  and  derived datasets by scraping WikiArt images. Despite these efforts, such datasets still face many challenges related to variable image quality, imbalanced distributions, and others. ArtBench-10  attempted to rectify these issues by introducing a class-balanced and cleanly annotated benchmark. However, it only provides ten classes.

Recent advancements in text-to-image synthesis have spurred the development of AI-generated datasets like DiffusionDB , and Midjourney Kaggle , which contain millions of image-text pairs generated by models such as Stable Diffusion and Midjourney. These datasets, while groundbreaking, tend to be limited in stylistic diversity and are skewed towards specific user groups, reflecting data collected from constrained environments and short time frames. The main distinction between our dataset and those datasets lies in the duration over which the images, along with the magnitude of the images. While the DiffusionDB dataset covers a brief period of just 12 days in August 2022, our dataset extends across a much longer time frame, spanning 18 months from July 2022 to May 2024. This extensive duration provides a significant advantage for in-depth studies into the evolution and dynamics of visual trends, artistic styles, and thematic content. Researchers have tailored other datasets to investigate certain themes , but these are domain-specific and lack breadth. TWIGMA  captured multiple years of generated images scraped from X (formerly Twitter) but does not include the prompts that were used to generate these images, instead relying on inferred BLIP captions. As shown in Tab. 1, our dataset not only includes the original prompts used to produce images but also contains images generated by multiple models while encapsulating a long time frame.

### Text-to-image Generative Models

Text-to-image generative models (T2I)  have displayed exceptional abilities in synthesizing high-quality images conditioned on textual descriptions. As they improve, these models become increasingly indispensable tools for visually creative tasks. Among these, diffusion-based text-to-image models  are prominently used for guided image synthesis  and complex image editing  applications. Although these models allow for significant control through text, such as directing the color or attributes of styles within generated images, they are unable to preserve the same precise style across new contexts consistently.

### Personalized Generation in Diffusion Models

Personalization techniques have been proposed to enable pre-trained text-to-image models to generate novel concepts based on a small set of images. DreamBooth  fine-tunes the full T2I model, which yields more detailed and expressive outputs. However, due to the large scale of these models, full fine-tuning is an expensive task that requires substantial amounts of memory. Different methods attempt to work around this challenge for both style and content representations. Custom Diffusion  attempts multi-concept learning but requires expensive joint training and struggles with style disentanglement. Textual Inversion  learns a new token embedding to represent a subject or style without altering the original

   Name & Source & \# Images & Year & Original Prompt Lcluded & Multiple Models \\  DiffusionDB & SD Discord & 14,000,000 & Aug 2022 & ✓ & ✗ \\ Midjourney Kaggle & Midjourney & 250,000 & Jun 2022-Jul 2022 & ✓ & ✗ \\ TWIGMA & Twitter & 800,000 & Jan 2020-Mar 2023 & ✗ & ✓ \\  STYLEBEEDER (**Ours**) & Artbreeder & 6,818,217 & Jul 2022-May 2024 & ✓ & ✓ \\   

Table 1: A comparison of our dataset to other AI-generated image datasetsmodel parameters, while StyleDrop  utilizes adapter tuning to only train a subset of weights for style adaptation. Low-Rank Adaptation (LoRA)  is a Parameter Efficient Fine-Tuning (PEFT) technique frequently used to fine-tune T2I models to generate images of a desired style. EDLoRA  proposes a layerwise embedding and multi-word representation when training a LoRA model.

## 3 Stylebreeder Dataset

We collect STYLEBREEDER by scraping images from the Artbreeder website. We choose Artbreeder since it is one of the most popular platforms for art generation, supporting various text-to-image models. Artbreeder enables users to create images using text prompts, offering controls over various settings, such as the strength (guidance scale) of the text's influence on the generated image, seed values, model type, and other hyperparameters. Since its rise in popularity within the artistic community in 2018, Artbreeder has become known for its bias towards generating artistic images. This predisposition towards artistic styles is a primary reason we concentrated our focus on this area. Additionally, all images on Artbreeder are covered by a CC0 license2, which allows for unrestricted use for any purpose 3. We collect metadata along with the images, which include text prompts (positive and negative), usernames, and hyperparameters. We provide additional features such as NSFW scores for each image and text prompts (see Fig. 1).

### User Statistics

Our dataset comprises 95,479 unique users, each generating an average of 72.41 images. Figure 2 (a) illustrates the distribution of images per user, showing that the majority of users produced fewer than 1,000 images, although a few power users created a significantly larger number of images. All user IDs in our dataset have been anonymized to ensure users' privacy.

### Model Statistics

Our dataset represents a wide variety of text-to-image diffusion models, including Stable Diffusion 1.5 (74.9%), SD-XL 1.0 (13.1%), Stable Diffusion 1.3 (8.8%), Stable Diffusion 1.4 (1.3%), Stable Diffusion 1.5-free (1.1%) and ControlNet 1.5 (0.8%). These models differ in their capabilities and the quality of their generated outputs, with recent models often supporting higher resolutions that deliver finer details and more complex visuals. This variety in models used to generate images provides a valuable opportunity to explore their differences,

Figure 2: Most unique users have fewer than 1000 images generated. The average number of words in a prompt is less than 60 words. Common keywords for positive prompts include ‘painting’, ‘realistic’, and ‘digital’ reveal semantic information about the style of desired images. Common keywords in negative prompts, such as ‘ugly’ and ‘deformed,’ indicate undesired features of generated images.

such as variations in artistic expression, the nuances in image quality, and potential biases inherent in each model. Stable Diffusion 1.5 is the most frequently used model in our dataset, followed by SD-XL, which has gained popularity due to its ability to generate high-quality images. The resolution of the images ranges from \(512 512\) to \(1280 896\) based on the model employed. Additionally, our dataset provides details on key hyperparameters like seed, CFG guidance scale--which dictates the extent to which the image generation process adheres to the text prompt--and step size in the diffusion model. Users are able to generate varying images using identical text prompts by adjusting these parameters. How different configurations affect the resulting images introduces deeper insights into the model's behavior and its sensitivity to these parameters. This enables a deeper understanding of how subtle changes in input or settings can significantly alter the characteristics of generated images, providing valuable perspectives on the underlying generative processes.

### Text Prompts

We collect both positive and negative text prompts for each image in our dataset. The average prompt length is 60 words, as shown in Fig. 2 (b). Common words in positive prompts, such as 'painting','realistic', and 'digital', reveal semantic information about the desired styles of images (see Fig. 2 (c)) while common words in negative prompts like 'ugly' and 'deformed' indicate the undesired features of the generated images (see Fig. 2 (d)). Furthermore, we observe that users often incorporate artist names in their text prompts to specify desired styles, a common practice employed by the generative art community. To quantify this trend, we analyze using BERT NER  to identify unique artist names in the dataset. Our findings highlight a significant occurrence of artist names, with top mentions including 'Ilya Kuvshinov', a Russian illustrator, featured in 208K text prompts, and 'Akihiko Yoshida', a Japanese video game artist, appearing in 81K prompts. Given that these artists may not permit the use of their artistic styles, we offer a form on our website allowing artists to opt-out, ensuring their styles are not replicated without their consent, as outlined in Section 5.

### NSFW and Toxic Content

We analyze NSFW content in both images and text prompts using state-of-the-art predictors for images  and text prompts . Figure 3 (a) shows a comparison of our dataset (STYLEBREEDER) with other popular datasets such as LAION , Artbench , DiffusionDB

Figure 3: (a) Predicted NSFW scores across LAION , Artbench , DiffusionDB  and TWIGMA , STYLEBREEDER (Ours) on images4, computed with  (higher score indicates more NSFW content). (b) Predicted NSFW, Toxicity, Severe Toxicity, Identity Attack, Insult, and Threat scores across on text prompts, computed with  on STYLEBREEDER.

 and TWIGMA . Most of these datasets, particularly those with AI-generated images like DiffusionDB, TWIGMA, and ours, contain a substantial amount of potentially NSFW content. A similar observation can be made for text-prompts (see Fig. 3 (b)) where we report NSFW, Toxicity, Severe Toxicity, Identity Attack, Obscene, Insult, and Threat scores computed with . This trend correlates with recent studies highlighting a significant increase in NSFW content generation by online communities . For instance, potentially harmful text prompts may involve the names of influential politicians, such as 'Donald Trump' and 'Joe Biden', found in prompts like 'angry Joe Biden screaming, red-faced, steam coming from ears' or 'angry Donald Trump Melania and the judge and police at mcdonalds'. Additionally, our analysis reveals the use of celebrity names in contexts suggesting sexually explicit content that could be construed as nonconsensual pornography. To assist researchers, our dataset includes NSFW text and image scores, along with toxicity-related scores, enabling them to filter these images and determine appropriate thresholds for excluding potentially unsafe data. We also provide a Google form for reporting harmful or inappropriate images and prompts, as outlined in Section 5.

## 4 Experiments

We define three tasks using our dataset: identifying diverse artistic styles through clustering based on stylistic similarities, generating personalized images tailored to individual styles, and recommending styles based on previously generated images of a given user.

### Experimental Setup

We utilize official and Diffusers  implementations for Textual Inversion , LoRA w/DreamBooth , Custom Diffusion , and EDLoRA . For each method, we use the same set of seeds and adhered to the parameter settings recommended in their respective publications. Textual Inversion was trained at a resolution of \(512 512\) for 3000 steps with a learning rate of 5e-4. LoRA with DreamBooth and EDLoRA were both trained at the same resolution (\(512 512\)) for 800 steps but with a learning rate of 1e-4, and LoRA had a rank of 32. Custom Diffusion was trained at a resolution of \(512 512\) for 250 steps with a learning rate of 1e-5. DINO and CLIP scores in Tab. 2 were computed on 20K images each, and standard deviations are provided. We use 8 NVIDIA L40 GPUs for our experiments. For recommendation tasks, we use 96K images generated by 1434 users with an 80% training

Figure 4: (a) User-generated images from 10 random clusters showcasing a diverse range of styles. (b) Sample images from style-based clustering vs. traditional clustering using DINO features show that style-based clustering captures the stylistic content while traditional clustering focuses on objects. (c) Visualization of the clusters, projected into 2D with t-SNE  with each cluster represented by a unique color according to their assignments by K-Means++ . This depiction highlights that while many styles are closely related, some distinct styles are noticeably distant from the main clusters.

and 20% test split, using a learning rate of 5e-3 and regularization term 2e-2 and analyzing a 5-fold cross-validation using the Surprise  library.

### Discovering Diverse Artistic Styles

The rising popularity of generated art has led to a vast array of user-generated content showcasing a diverse spectrum of artistic styles. However, a key challenge remains: how can we effectively discover and categorize these styles? We aim to cluster user-generated images into stylistically similar groups to uncover unique styles. Formally, our dataset, denoted as \(\), consists of \(N\) images. We employ a pre-trained text-to-image model \(M_{}\), parameterized by weights \(\) and a token embedding space \(V\). Firstly, we convert the images into a set of \(N\) style embeddings using a state-of-the-art feature extractor \(F\), specifically CSD , which uses a Vision Transformer (ViT)  backbone to map images into a \(d\)-dimensional vector space representing their style descriptors. CSD has shown superior performance in style-matching tasks across datasets like DomainNet, WikiArt, and LAION-Styles, outperforming models like DINO, which focus more on semantic content. This conversion results in a set of embeddings \(Z=_{N}F(_{i})\), where each image, \(_{i}\), is embedded in a high-dimensional semantic embedding space. These embeddings are then clustered into \(k=10000\) groups using the K-Means++  algorithm, which utilizes cosine similarity to ensure cluster cohesion. To determine the optimal number of clusters, we employ the silhouette score method across various cluster sizes: 50, 100, 500, 1000, 2000, 5000, 10000, 20000 (see Supplemental Materials for more details). This experiment underscores that a configuration of 10000 clusters maximizes the silhouette score, indicating optimal internal similarity and external dissimilarity among the clusters. Figure 4 (c) presents a 2D projection of the CSD embedding space using t-SNE . The visualization reveals that some clusters are closely grouped, reflecting stylistic similarities, while others are distinctly isolated, demonstrating significant stylistic variations.

Figure 5: (a) An illustration of our pipeline: we cluster input images by stylistic similarity and employ a personalization method, such as LoRA, to train personalized models aligned with specific styles. (b) Users can download style LoRA models from the Style Atlas platform. (c) Users can generate personalized images using LoRA models where Style S* represents an example image from the cluster. (d) We recommend top styles to users based on the images they have previously generated. This personalized approach helps tailor style suggestions to each user’s unique preferences.

### Personalized Image Generation Based on Style

Personalized image generation based on style is a crucial task since it allows users to create unique, customized visuals that resonate with their specific aesthetic preferences. We picked four state-of-the-art personalization methods, namely, Textual Inversion , LoRA w/DreamBooth , Custom Diffusion , and EDLoRA , to provide a benchmark analysis on the discovered styles. For this task, we randomly select 40 clusters and generated 50 images for ten text prompts, totaling 500 images per cluster for each method. To provide a benchmark on these models, we calculate several metrics: CLIP-T (image-text similarity between generated images and text prompts), CLIP-I (image-image similarity between clusters and generated images), and DINO (image-image similarity between clusters and generated images). Details of these metrics can be found in Tab. 2, which demonstrates that EDLoRA exhibits superior performance in personalized image generation due to its embedding-based approach. We also provide qualitative examples in Fig. 6.

### Style-based Recommendation

The sheer volume of styles generated by users presents a significant challenge in navigating the landscape of artistic options available. To address this, we showcase a recommendation system that suggests top styles to users based on their previously generated images. This personalized approach is crucial as it helps users discover styles that align with their tastes and past preferences, simplifying their search among a vast array of choices. We formulate our task as a matrix-factorization-based recommendation, which involves a set of items where users rate items they have interacted with, thus creating a matrix of user-item ratings. In the context of our problem, users are the creators who generate images, and items are the clusters in which generated images are assigned. We calculate for each user \(u\) a vector \(_{u}=[r_{1} r_{2}... r_{N}]\) where \(r_{i}\) represents the proportion of images that the user has generated within a specific cluster \(i\) such that \(_{i=1}^{N}r_{i}=1\). These vectors create a matrix \(R\) where entries \(r_{ui}\) denote the rating for user \(u\) for cluster \(i\). We employ the SVD algorithm  to obtain a prediction for all \(_{ui}=+b_{u}+b_{i}+q_{i}^{T}p_{u}\) where \(\) is the global average rating. \(b_{u}\) and \(b_{i}\) are the user and item bias terms, respectively. \(q_{i}\) and \(p_{u}\) are the latent factor vectors for item \(i\) and user \(u\), respectively. We minimize the regularized squared error loss and update parameters using Stochastic Gradient Descent . We assess the MAE and RMSE of the predicted ratings against the ground-truth ratings, obtaining a mean RMSE of 0.1425 and a standard deviation of 0.0017, along with a mean MAE of 0.082 and a standard deviation of 0.001 across all folds. Fig. 5 (d) depicts an example of recommendations for a user based on previously generated styles.

### Style Atlas for Democratizing Artistic Styles

Since LoRA has become a popular tool for lightweight concept tuning within the community , we provide 100 style LoRAs in our Style Atlas platform. Fig. 5 (c) displays a screenshot of our platform where users can browse and download LoRA models for appealing styles. Additionally, we provide a notebook that enables users to load these downloaded LoRA models

    &  & LoRA w/DreamBooth & EDLoRA & Custom-Diffusion \\   & Avg. & 0.6869 \(\) 0.10 & 0.6299 \(\) 0.11 & **0.6957 \(\) 0.11** & 0.5917 \(\) 0.12 \\  & Min. & 0.6166 \(\) 0.10 & 0.5654 \(\) 0.11 & **0.6214 \(\) 0.11** & 0.5324 \(\) 0.11 \\  & Max. & 0.7428 \(\) 0.10 & 0.6831 \(\) 0.11 & **0.7521 \(\) 0.12** & 0.6440 \(\) 0.12 \\   & Avg. & 0.1857 \(\) 0.02 & **0.1896 \(\) 0.02** & 0.1822 \(\) 0.01 & 0.1809 \(\) 0.02 \\  & Min. & 0.1555 \(\) 0.02 & **0.1573 \(\) 0.02** & 0.1527 \(\) 0.01 & 0.1486 \(\) 0.02 \\  & Max. & 0.2392 \(\) 0.03 & **0.2663 \(\) 0.03** & 0.2389 \(\) 0.03 & 0.2585 \(\) 0.03 \\   & Avg. & 0.3801 \(\) 0.15 & 0.2668 \(\) 0.17 & **0.4125 \(\) 0.18** & 0.2546 \(\) 0.17 \\  & Min. & 0.2581 \(\) 0.13 & 0.1682 \(\) 0.14 & **0.2790 \(\) 0.15** & 0.1634 \(\) 0.14 \\   & Max. & 0.4838 \(\) 0.17 & 0.3585 \(\) 0.19 & **0.5246 \(\) 0.19** & 0.3402 \(\) 0.19 \\   

Table 2: Benchmark results for state-of-the-art personalized image generator models.

and generate personalized images. For further details, please refer to the Supplementary Material.

## 5 Limitations and Societal Impact

While our work significantly advances the integration of AI in creative processes, it also presents certain limitations and societal impacts that warrant careful consideration. One of the limitations lies in the potential for over-reliance on technology in artistic creation, which could diminish the value and perception of human-driven artivity and creativity. Additionally, the use of AI in art generation raises concerns about copyright and originality, especially when styles closely mimic those of existing artists without clear attribution. From a societal perspective, while our tools aim to democratize art creation, there exists a risk of reinforcing existing biases present in the training data, which could skew the diversity and representation of generated artworks. Moreover, as these technologies become more accessible, there is a potential for misuse, such as creating deceptive images or deepfakes, which could have broader implications for trust and authenticity in digital media. Acknowledging these challenges is crucial as we continue to explore the intersection of AI and art.

## 6 Conclusion

This paper has demonstrated the significant potential of text-to-image diffusion models to explore and catalog the rich tapestry of user-generated artistic styles on the Artbreeder platform. We successfully identify unique, previously uncharted artistic expressions and demonstrate their application in generating personalized images. Additionally, we demonstrate that a personalized recommendation system enhances user engagement by aligning suggested styles with individual preferences. We also release the Style Atlas platform that democratizes access to these innovations, allowing users to experiment with and adopt new artistic expressions. This work not only advances the technological capabilities of AI in the field of art but also contributes to a more inclusive and diverse artistic community. Our dataset offers numerous avenues for further exploration, such as refining the effectiveness of text prompts through iterative adjustments, studying trends in art over time, recommending styles based on both image and textual content, developing a search system for the generated images, and exploring explainable creativity .

Figure 6: Qualitative comparison of various personalization methods on artistic styles on Textual Inversion , LoRA w/DreamBooth , Custom Diffusion , and EDLoRA . Style Cluster (bottom row) illustrates a sample image from the corresponding cluster.