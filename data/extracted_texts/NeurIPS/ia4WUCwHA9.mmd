# Theoretical guarantees in KL for Diffusion Flow Matching

Marta Gentiloni Silveri

Ecole polytechnique

Route de Saclay, 91120 Palaiseau, France

marta.gentiloni-silveri@polytechnique.edu

Giovanni Conforti

Universita degli Studi di Padova

Via Trieste, 63, 35131 Padova, Italia

giovanni.conforti@math.unipd.it &Alain Durmus

Ecole polytechnique

Route de Saclay, 91120 Palaiseau, France

alain.durmus@polytechnique.edu

###### Abstract

Flow Matching (FM) (also referred to as stochastic interpolants or rectified flows) stands out as a class of generative models that aims to bridge in finite time the target distribution \(^{}\) with an auxiliary distribution \(\), leveraging a fixed coupling \(\) and a bridge which can either be deterministic or stochastic. These two ingredients define a path measure which can then be approximated by learning the drift of its Markovian projection. The main contribution of this paper is to provide relatively mild assumptions on \(^{}\), \(\) and \(\) to obtain non-asymptotics guarantees for Diffusion Flow Matching (DFM) models using as bridge the conditional distribution associated with the Brownian motion. More precisely, we establish bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of \(^{}\), \(\) and \(\), and a standard \(^{2}\)-drift-approximation error assumption.

## 1 Introduction

A significant task in statistics and machine learning currently revolves around generating samples from a target distribution that is only accessible via a dataset. To tackle this challenge, generative models have become prominent as effective computational tools for learning to simulate new data. Essentially, these models involve learning a _generator_ capable of mapping a source distribution into new approximate samples from the target distribution.

One of the most productive approaches to generative modeling is based on deterministic and stochastic transport dynamics, that connect a target distribution with a base distribution. Typically, the target distribution represents the data set from which we want to generate new samples, while the base distribution is one that can be easily simulated or sampled. Regarding the dynamics, they correspond to SDEs Stochastic Differential Equations (SDEs) or Ordinary Differential Equations (ODEs), where the drift (for SDEs) or velocity field (for ODEs) is determined by solving a regression problem. This regression problem is usually addressed with appropriate neural networks and related training techniques .

Among these methods, score-base generative models (SGMs) and in particular diffusion models based on score matching  was an important milestone. In a nutshell, these models involve transforming an arbitrary density into a standard Gaussian model and consists in learning the drift of the corresponding reversal process. More precisely, the idea is to first consideran Ornstein-Uhlenbeck (OU) process \((X^{}_{t})_{t[0,T]}\), over a time interval \([0,T]\),

\[X^{}_{t}=-(1/2)X^{}_{t}t+B_{t }\, X^{}_{0}^{}\, \]

where \((B_{t})_{t 0}\) is a \(d\)-dimensional Brownian motion. Then, the reversal process \((^{}_{t})_{t[0,T]}\), which is defined from the non-homogeneous SDE :

\[^{}_{t}=\{(1/2)^{ }_{t}+ p^{}_{T-t}(^{}_{t})\}t+B_{t}\, t[0,T]\,\ \ ^{}_{0}^{}P^{}_{T}\, \]

allows the law of \(X^{}_{T}\) to be transported to \(^{}\): [1, Equations 3.11, 3.12] show that \(^{}_{T}\) has distribution \(^{}\). The initialization \(^{}P^{}_{T}\) in (2) is the distribution of \(X^{}_{T}\) defined in (1). The drift in (2) can be decomposed as the sum of a linear function and the score associated with the density of \(X^{}_{T-t}\) with respect to the Lebesgue measure, denoted by \(p^{}_{T-t}\). From the Tweedie identity, this score is the solution to a regression problem that can be solved efficiently by score matching . Learning this score at different times can also be formulated as a sequence of denoising problems. Once the drift of the reversal process is learned or equivalently the scores \(( p^{}_{t})_{t[0,T]}\), score-based generative models consist in following the reversal dynamics over \([0,T]\) or, more commonly, a discretization of it, starting with a sample from \((0,)\). The final sample at time \(T\) is then approximatively distributed according to \(^{}\). Note that an approximation is made even if the reversal dynamics were simulated exactly, because for full accuracy, the model would need to start from a sample of \(^{}P^{}_{T}\). However, it is well known that for sufficiently large \(T\), \(^{}P^{}_{T}\) is (exponentially) close to \((0,)\).

When exploring diffusion models, it has been realized that the generation of approximate data samples could also be achieved using an ODE instead of the reversal diffusion:

\[^{}_{t}/t=(1/2)( {X}^{}_{t}+ p^{}_{T-t}(^{ }_{t}))\.\]

Similarly to the drift of the reversal diffusion, the velocity fields at time \(t[0,T]\) associated with this ODE is the sum of a linear function and the score of the density of \(X^{}_{T-t}\). This observation has prompted the introduction of the Probability Flow ODE implementation of diffusion models . SGMs in their standard and probability flow ODE implementations have achieved notable success in a range of applications; see e.g., .

While diffusion-based methods have now become popular generative models, they can suffer from two limitations. First, there is a trade-off in selecting the time horizon \(T\) and second, they rely solely on Gaussian distributions as base distributions, in general. Therefore, there remains considerable interest in developing methods that consider a more general base distribution \(\) and that accomplish the transport between \(^{}\) and \(\) relying on dynamics defined on a fixed finite time interval. Defining a generative process in finite time by means of a coupling and an interpolating process, , laid the foundation for Flow Matching (FM) models , LCBH\({}^{+}\)23, Liu22, LGL23], finally addressing this problem.

In its simplest form, the main strategy employed by FMs to bridge two distributions, involves a fixed coupling \(\) between \(^{}\) and \(\) and the use of a bridge, _i.e._, a conditional distribution on the path space \((,^{d})\) of a reference process \((R_{t})_{t}\) given its starting point \(R_{0}\) and end point \(R_{1}\). In case \(R_{t}\) is a deterministic function of \(R_{0}\) and \(R_{1}\), we say that the bridge is deterministic and stochastic otherwise. As \((R_{t})_{t}\) corresponds to the solution of a stochastic differential equation, we coin the term Diffusion Flow Matching (DFM) to distinguish the latter case from the former one and focus on it. Then, this bridge and the coupling \(\) between \(^{}\) and \(\) define an interpolated process \((X^{1}_{t})_{t}\), referred to as an interpolant, defined as \((X^{1}_{0},X^{1}_{1})\) and \((X^{1}_{t})_{t}\) given \((X^{1}_{0},X^{1}_{1})\) has the same conditional distribution as \((R_{t})_{t}\) given \((R_{0},R_{1})\). However, \((X^{1}_{t})_{t}\) does not correspond in general to the distribution of a diffusion or even to the one of a Markov process. This characteristic poses a challenge when dealing with potential stochastic sampling procedures: indeed, similarly to SGMs, FMs and DFMs aim to design a Markov process that approximatively transport \(\) to \(^{}\). To address this issue, most works proposing FM and DFM models  rely on mimicking the marginal flow of the interpolated process \((X^{1}_{t})_{t}\) through a diffusion process known as the Markovian projection. A remarkable feature of this diffusion lies in the fact that its drift is also a solution of a regression problem that can be approximatively solved using only samples from the interpolant \((X^{1}_{t})_{t}\). Then, an approximate samples from \(^{}\) is obtained by following a discretization of the dynamics associated with the considered Markovian projection, starting with a sample of \(\).

While there exists now an important literature on theoretical guarantees for SGMs , only a few works have been considering FMs. In addition, up to our knowledge, these works on FMs only consider deterministic interpolants . The main objective of this paper is to fill this gap and to analyze DFMs using the \(d\)-dimensional Brownian motion as reference process, in which case the bridge is simply the \(d\)-dimensional standard Brownian bridge. We provide theoretical guarantees, upper bounding the Kullback Leibler divergence between the target distribution and the one resulting from the DFM. Our results consider the two sources of error coming from the DFM model, namely drift-estimation and time-discretization. This pursuit underscores the significance of comprehending and quantifying the factors influencing the performance of DFMs, paving the way for further advancements in generative modeling techniques.

Our contribution.In this work, we analyze a DFM model using as bridge the \(d\)-dimensional Brownian bridge and examine how it performs in two distinct scenarios: one without early-stopping and another with early-stopping. In our first main contribution Theorem 2, we establish an explicit and simple bound on the \(\) divergence between the data distribution and the distribution at the final time of the DFM model. We achieve our bound without early stopping, by assuming only (1) moment conditions on the target \(^{}\) and the base \(\) (**H1**); (2) integrability conditions on the scores associated with the data distribution \(^{}\), the base distribution \(\) and the coupling \(\) (**H2**); (3) a \(^{2}\)-drift-approximation error (**H3**) (an assumption commonly considered in previous works). Note that condition (2) implies that \(^{}\) necessarily admits a density. We relax this condition in our second contribution. In Theorem 3, we establish an explicit bound on the \(\) divergence between a smoothed version of the target distribution and an early stopped version of the DFM model, assuming (1) and (3), but replacing the condition (2) by assuming (4) \(=^{}\) and integrability conditions only on the score associated with \(\).

To the best of our knowledge, our paper provides the first convergence analysis for diffusion-type FMs, that tackles all the sources of error, _i.e._, the drift-approximation-error and the time-discretization error. In addition, previous studies concerning FMs and Probability Flow ODEs, with deterministic or mixed sampling procedure, either rely on at least some Lipschitz regularity of the flow velocity field or its estimator and/or do not take the time-discretization error into consideration. Also, in the context of SGMs with constant step-size, most of existing works without early-stopping are obtained assuming either the score (or its estimator) to be Lipschitz or the data distribution to satisfy some additional conditions (e.g., manifold hypothesis, bounded support, etc.); the unique exception being . We refer to Section 3.2 for a more in depth literature comparison.

Notation.Given a measurable space \((,)\), we denote by \(()\) the set of probability measures of \(\). Also, given a topological space \((,)\), we use \(()\) to denote the Borel \(\)-algebra on \(\). Given two random variables \(Y,\), we write \(Y\) to say that \(Y\) and \(\) are independent. Denote by \((B_{t})_{t}\) a \(d\)-dimensional Brownian motion. We denote by \(^{d}\) the Lebesgue measure on \(^{d}\). Given two real numbers \(u,v\), we write \(u v\) (resp. \(u v\)) to mean \(u Cv\) (resp. \(u Cv\)) for a universal constant \(C>0\). Also, we denote by \(\|x\|\) the Euclidean norm of \(x^{d}\), by \( x,y\) the scalar product between \(x,y^{d}\), and by \(x^{T}\) the transpose of \(x\). Given a matrix \(^{d s}\), we denote by \(\|\|_{}\) the operator norm of \(\). For \(f:^{d}\) regular enough, we denote by \(_{x}f(t,x),_{x}^{2}f(t,x)\) and \(_{x}f(t,x)\) respectively the gradient, hessian and laplacian of \(f\), defined for \(t,x^{d}\) by \(_{x}f(t,x):=(_{x_{i}}f(t,x))_{i}\), \(_{x}^{2}f(t,x):=(_{x_{i}}_{x_{j}}f(t,x))_{i,j}\), \( f(t,x):=_{i=1}^{d}_{x_{i}}^{2}f(t,x)\), where \(_{x_{j}}\) denotes the partial derivative with respect to the \(j\)-th variable. For \(F:^{d}^{d}\) regular enough, we denote by \(D_{x}F\), \(_{x}F\) and \(_{x}F\) respectively, the Jacobian matrix, the divergence and the vectorial laplacian of \(F\), defined for \(t,x^{d}\) by \(D_{x}F(t,x)=(_{x_{j}}F_{i}(t,x))_{i,j}\), \(_{x}F(t,x):=_{j=1}^{d}_{x_{j}}F_{j}(t,x)\), \(_{x}F(t,x)=(_{x}F_{1}(t,x),...,_{x}F_{d}(t,x))\).

## 2 Diffusion Flow Matching.

Given a target distribution \(^{}(^{d})\) and a base distribution \((^{d})\), the idea at the core of FM models is intuitively to construct a path between these two by considering two ingredients (1) a coupling \(\) and (2) a bridge (or an interpolant following ) between \(\) and \(^{}\) (or more precisely a bridge with foundations \(\)). More formally, here we say that \(\) is a coupling between \(\) and \(^{}\) if for any \((^{d})\), \((^{d})=()\) and \((^{d})=^{}()\), and denote by \((,^{})\) the set of coupling between \(\) and \(^{}\). Then, based on a probability measure on \(=(\,,^{d})\) the set of continuous functions from \(\) to \(^{d}\), we define the bridge \(\) associated with \(\) as the Markov kernel \(\) on \(^{2d}\), such that, for any \(()\), \(()=_{0,1}(x_{0},x_{1}) ((x_{0},x_{1}),)\) (see e.g., [13, Theorem 8.37] for the existence of this kernel), where for any \(=\{t_{1},,t_{n}\}\), \(t_{1}<<t_{n}\), \(_{}\) is the \(\)-marginal distribution of \(\), _i.e._, the pushforward measure of \(\) by \((x_{t})_{t}(x_{t_{1}},,x_{t_{n}})\). From a probabilistic perspective, it implies that if \((W_{t})_{t}\), then \(\) is a conditional distribution of \((W_{t})_{t}\) given \((W_{0},W_{1})\): for any bounded and measurable function on \(\), \([f((W_{t})_{t})|X_{0},X_{1}]= f((w_{t})_{t}) ((W_{0},W_{1}),(w_{t})_{t})\).

### Definition of the interpolated process

Consider now a coupling \(\) and a bridge \(^{}\) associated to \(^{}()\). We suppose here that \(^{}\) is the distribution of \((Y_{t})_{t}\) solution of the stochastic differential equation

\[Y_{t}=(Y_{t})t+B_{t}\, t\, Y_{0}(^{d})\, \]

where \((B_{t})_{t_{+}}\) is a standard \(d\)-dimensional Brownian motion. In addition, we suppose \(^{}(^{d},^{d})\) for simplicity and that (3) admits a unique strong solution. Consider now, the interpolated measure \((,^{})\)1 corresponding to the distribution of the process defined by \((X_{0}^{},X_{1}^{})\) and \((X_{t}^{})_{t}(X_{0}^{},X_{1}^{}) ^{}((X_{0}^{},X_{1}^{}),)\). In , \((X_{t}^{})_{t}\) is referred to as a stochastic interpolant. Denote by \((s,t,x,y) p_{t|s}^{Y}(y|x)\) the conditional density of \(Y_{t}\) given \(Y_{s}\) with respect to the Lebesgue measure and by \((p_{t}^{})_{t}\) the time marginal densities of \((X_{t}^{})_{t}\) with respect to the Lebesgue measure, that is \((Y_{t}|Y_{s})=_{}p_{t|s}(x|Y_{s})x\) and \((X_{t}^{})=_{}p_{t}^{}(x)x\), for \((^{d})\) and \(s,t\), \(s t\). Then, note that, as a straightforward consequence of the definition of \((X_{t}^{})_{t}\), it holds

\[p_{t}^{}(x)=_{^{2d}}p_{t|0}^{Y}(x|x_{0})p_{1|t}^{Y}(x_{ 1}|x)(x_{0},x_{1})\, \]

where

\[(x_{0},x_{1})=x_{0},x_{1})}{p_{1|0}^{Y}(x_{1}|x_{0})}\.\]

An example that we will focus on in this paper is \(^{}=\) the distribution of the Brownian motion \((B_{t})_{t}\) solution of (3) with \( 0\). Then, it is well known that \(\) is then the Markov kernel associated with the Brownian bridge and the resulting stochastic interpolant satisfies for any \(t\)

\[X_{t}^{}}}{{=}}(1-t)X_{0}^{ }+tX_{1}^{}+\,(0,)\, \]

where \(}}{{=}}\) denotes the equality in distribution.

It is well-known that for any \(x_{0},x_{1}^{d}\), the distribution \(^{}((x_{0},x_{1}),)\) is diffusion-like under appropriate conditions. More precisely, \(^{}((x_{0},x_{1}),)\) is the distribution of \((_{t})_{t}\) solution to

\[_{t}=\{(_{t})+2_{t}^{x_{1}}( _{t})\}t+B_{t}\, t\,_{0}=x_{0}\, \]

where \(_{t}^{x_{1}}(y)= p_{1|t}^{Y}(x_{1}|y)\). For instance, for \(x_{0},x_{1}^{d}\), the bridge \(((x_{0},x_{1}),)\) associated to \(\) is the distribution of a Brownian bridge \((_{t}^{x_{0},x_{1}})_{t}\) solution to the SDE

\[_{t}^{x_{0},x_{1}}=-_{t}^{x_{0},x_{1}}}{1-t} t+B_{t}\, t\,_{0}^{x_{0},x_{1}}=x_{0}\.\]

From (6), it turns out that \((X_{t}^{})_{t}\) given \((X_{0}^{},X_{1}^{})\) is therefore solution to

\[X_{t}^{}=\{(X_{t}^{})+2_{t}^{X_{1}}( X_{t}^{})\}t+B_{t}\, t\, X_{0}^{}. \]Note that the drift coefficient in (7) depends on \(X^{1}_{1}\), and therefore, \((X^{1}_{t})_{t}\) is not Markov, which is a natural property if we are interested in constructing a generative process. To circumvent this issue, based on \((,^{})\), we aim to define a distribution \((,^{})\) such that it has the same one-dimensional time marginals as \((,^{})\) and corresponds to a diffusion, _i.e._, if \((X^{1}_{t})_{t}(,^{})\) and \((X^{}_{t})_{t}(,^{})\), for any \(t\), \(X^{1}_{t}}}{{=}}X^{}_{t}\) and \((X^{}_{t})_{}\) is solution of a diffusion with Markov coefficient. This can be done trough the Markovian projection.

### Markovian projection and Diffusion Flow Matching

Markovian projection.The idea of Markovian projection originally dates back to  and . Its main idea is in essence to define a diffusion Markov process which "mimics" the time-marginal of an Ito process:

\[_{t}=_{t}t+B_{t}\;,  t\;,_{0}\;,\]

for some adapted process \((_{t})_{t}\) and initial distribution \(\). Under appropriate conditions (see [1, Corollary 3.7]), this diffusion process, denoted by \((^{}_{t})_{t}\), exists and is solution to an SDE with a (relatively) simple modification of the drift \(_{t}\), namely

\[^{}_{t}=}_{t}(^{ }_{t})t+B_{t}\;, t\;, ^{}_{0}\;,\]

where \(}_{t}(^{}_{t})=[_{t}| ^{1}_{t}]\). This result can be applied to the non-Markov process \((X^{1}_{t})_{t}\) solution of (7): its Markovian projection is solution of

\[X^{}_{t}=_{t}(X^{}_{t})t+ B_{t}\;, t\;,\]

for some function \(:^{d}^{d}\). It turns out that we can identify \(\), relying on the family of conditional densities \((p^{}_{t|s})_{0 s t 1}\) and marginal densities \((p^{1}_{t})_{0 t 1}\). This is the content of the following result.

**Theorem 1**.: _Consider a \((,^{})\) and \(^{}\) associated with (3). Consider the drift field_

\[^{Y}_{t}(x)=(x)+2 p^{Y}_{1|t}(x_{1}| x)p^{Y}_{t|0}(x|x_{0})p^{Y}_{1|t}(x_{1}|x)(x_{0}, x_{1})}{p^{1}_{t}(x)}\;. \]

_Under appropriate conditions (see Appendix A.1), the Markov process \((X^{}_{t})_{t}\) solution of_

\[X^{}_{t}=^{Y}_{t}(X^{}_{t})t+B_{t}\;, t[0,1)\;, X^{}_{0} \;, \]

_mimics the one-dimensional time marginals of \((,^{})\), i.e., for any \(t[0,1)\), \(X^{1}_{t}}}{{=}}X^{}_{t}\)._

This result is well-know, but, for sake of completeness, we provide the proof in Appendix A.1.

The process (9) is known as the _Markovian projection_ of \((,^{})\), and its drift (8) as _mimicking drift_. In what follows, we denote by \((,^{})\) the distribution of \((X^{}_{t})_{t}\) on \(\).

_Remark 1_.: It can be easily shown by continuity that \(X^{}_{t}=X^{1}_{t} X^{1}_{1}\) for \(t 1\), where \(\) denotes the convergence in distribution. As \((X^{1}_{1})=^{}\), the Markovian projection therefore gives a an _ideal_ generative model which would consist in following the SDE (9) with initial distribution \(\).

_Remark 2_.: Note that, because of (4), the mimicking drift (8) rewrites as

\[^{Y}_{t}(X^{1}_{t})=2_{x} p^{Y}_{1|t}( X^{1}_{1}|X^{1}_{t})+(X^{1}_{t})|X^{1}_{t}\;.\]

Diffusion Flow Matching.Eventually, as pointed out in Remark 1, the Markovian projection gives a an _ideal_ generative model. However, a) the mimicking drift (8) is intractable and b) the continuous-time SDE (9) can not be numerically simulated. Thus, in order to implement the proposed theoretical idea, we first need to address and overcome the aforementioned computational challenges. To circumvent a), observe that, because of Remark 2 and [13, Corollary 8.17], we can approximate the mimicking drift via solving

\[_{}\|s^{Y}_{}(t,X^{1}_{t})- ^{Y}_{t}(X^{1}_{t})\|^{2}\;, \]for a properly chosen class of neural networks \(\{(t,x) s^{Y}_{}\,(t,x)\}_{}\), and replace \(^{Y}\) in (9) with \(s^{Y}_{^{}}\), where \(^{}\) denotes a minimizer of (10). To deal with b), we simply make use of the Euler-Maruyama scheme, _i.e._, for a choice of sequence of step sizes \(\{h_{k}\}_{k=1}^{N}\), \(N 1\), and the corresponding time discretization \(t_{k}=_{i=1}^{k}h_{i}\), such that \(t_{0}=0\) and \(t_{N}=1\), we define the continuous process \((X^{^{}}_{t})_{t}\) recursively on the intervals \([t_{k},t_{k+1}]\) by

\[X^{^{}}_{t}=s^{Y}_{^{}}(t_{k},X^{^{ }}_{t_{k}})t+B_{t}\;, t[t_{k},t_{k+1}]\;,  X^{^{}}_{0}\;. \]

(11) is the DFM generative model we are going to analyze.

## 3 Main results

In this section, we provide convergence guarantees in Kullback-Leibler divergence for the Diffusion Flow Matching model (11), under mild assumptions on the \(,^{},\) and \(s_{^{}}\), either within a non-early-stopping regime or within a early-stopping regime.

From now on, we consider the case \( 0\), _i.e._, \(^{}=\). We show in Appendix A.2, Remark 9, that, under out set of assumptions, the conditions of Theorem 1 hold for this setup. Moreover, in this case, for any \(s,t\), \(s<t\) and \(x,y^{d}\), the conditional density \(p^{Y}_{t|s}(y|x) p_{t-s}(y|x)\) where \((t,x,y) p_{t}(y|x)\) is the heat kernel:

\[p_{t}(y|x)=}-}{4t}\;, t(0,1]\;. \]

In the following, we set \(^{Y}\) and \(s^{Y}_{^{}} s_{^{}}\).

### Convergence Bounds.

We assume moment conditions on the probability measures \(\) and \(^{}\), and mild integrability conditions on the probability distributions \(,^{}\) and on the coupling \(\).

For \(p 1\), we denote for \((^{d})\),

\[_{p}[]=\|x\|^{p}\,(x)\;.\]

**H1**.: _The probability distributions \(,^{}\) satisfy \(_{8}[]+_{8}[^{}]<+\)._

**H2**.: _The probability distributions \(^{}\), \(\) and \(\) are absolutely continuous with respect to the Lebesgue measure on \(^{d}\) and \(^{2d}\) respectively, and satisfy_

_(i) The functions \(/^{d}\) and \(^{}/^{d}\) are continuously differentiable and satisfy \(\|^{}\|^{8}_{^{8}(^{})}+\|\| ^{8}_{^{8}()}<+\) where for \(\{^{},\}\),_

\[\|\|^{8}_{^{8}()}=\| }{^{d}}(x_{0})\|^{8} (x_{0})\;.\]

_(ii) The function \(/^{2d}\) is continuously differentiable and satisfies \(\|\|^{8}_{^{8}()}<+\) where_

\[\|\|^{8}_{^{8}()}=\|( )(x_{0},x_{1})\|^{8}\,(x_{0},x_{1})\;, (x_{0},x_{1})=(x_{1}|x_{0})}}{ ^{2d}}(x_{0},x_{1})\;, \]

_and \(p_{1}\) is defined in (12)._

_Remark 3_.: Under **H**1, note that \(\|\|^{8}_{^{8}()}<+\) is equivalent by (12) and \((,^{})\) to

\[\|(/^{2d})(x_{0},x_{1})\|^{8 }(x_{0},x_{1})<+\;.\]_Remark 4_.: We can relax the condition that \(/^{d}\), \(^{}/^{d}\) and \(/^{2d}\) are continuously differentiable assuming that \([s]{/^{d}}\), \([s]{^{}/^{d}}\) and \([s]{/^{2d}}\) belongs to some Sobolev space, but, for ease of presentation, we prefer not to delve into these technical details.

Moreover, we assume to have estimated the mimicking drift with an \(^{2}\)-precision, for some \(^{2}>0\) sufficiently small.

**H3**.: _There exist \(^{}\) and \(^{2}>0\) such that_

\[_{k=0}^{N-1}h_{k+1}\|s_{^{}}(t_{k},X_{t_{ k}}^{})-_{t_{k}}(X_{t_{k}}^{})\|^{2} ^{2}\;.\]

_Remark 5_.: To be coherent with the previous section, observe that, as a consequence of Theorem 1, for any \(k=0,,N\), it holds

\[\|s_{^{}}(t_{k},X_{t_{k}}^{})- _{t_{k}}(X_{t_{k}}^{})\|^{2}= \|s_{^{}}(t_{k},X_{t_{k}}^{})-_ {t_{k}}(X_{t_{k}}^{})\|^{2}\;.\]

Under such assumptions, we derive an upper bound on the KL divergence between the data distribution \(^{}\) and the output of the DFM (11):

**Theorem 2**.: _Consider a uniform partition of \(\) with a constant stepsize \(h_{k} h\), \(h=1/N_{h}>0\), for \(N_{h}^{*}\) and consider the corresponding process \((X_{t}^{^{}})_{t}\) defined in (11). Assume **H1** to 3. Denoting by \(_{1}^{^{}}\) the distribution of \(X_{1}^{^{}}\), we have that_

\[(^{}|_{1}^{^{}}) ^{2}+h(h^{1/8}+1)d^{4}+_{8}[]+_{8}[^{}]+ \|\|_{^{8}()}^{8}\\ +\|\|_{^{8}()}^{8}+\|^{ }\|_{^{8}(^{})}^{8}\;. \]

_Remark 6_.: Under almost the same conditions as Theorem 2, _i.e._, **H1**  **H2**, **H3**, replacing \(\) in (13) by

\[_{T}(x_{0},x_{1})=(x_{1}|x_{0})}}{ ^{2d}}(x_{0},x_{1})\;, \]

our proofs apply also to DFM using a time horizon \(T>0\) and the Brownian bridge on \([0,T]\). In particular, we would have obtained similar bounds as the ones derived in Theorem 2 but with a factor \((1,T^{8})\) in front of the second addend.

_Remark 7_.: Choosing, in Theorem 2,

\[N_{h}=+_{8}[]+_{8}[^{}]+\| \|_{^{8}()}^{8}+\|\|_{^{8}( )}^{8}+\|^{}\|_{^{8}(^{})}^{8}}{ ^{2}}\]

makes the approximation error of order \((^{2})\) and the complexity of order \((^{-2})\).

Using an early-stopping procedure, we obtain in the case \(\) is the independent coupling:

**Theorem 3**.: _Fix \(0<<1/2\). Consider a uniform partition of \(\) with a constant stepsize \(h_{k} h\), \(h=1/N_{h}>0\), for \(N_{h}^{*}\) and consider the corresponding process \((X_{t}^{^{}})_{t}\) defined in (11). Assume **H1**, **H3** and \(=^{}\) to be the independent coupling. Suppose in addition that \(\) is absolutely continuously with respect to the Lebesgue measure, \(/^{d}\) is continuously differentiable and satisfies \(\|\|_{^{8}()}^{8}<+\)._

_Then, denoting by \(_{1-}^{}\) and \(_{1-}^{^{}}\) the distribution of \(X_{1-}^{}\) and \(X_{1-}^{^{}}\) respectively, we have that_

\[(_{1-}^{}|_{1-}^{^{}}) ^{2}+h(h^{1/8}+1)}{^{4}}+_{8}[ ]+_{8}[^{}]}+\|\|_{ ^{8}()}^{8}\;.\]

**Corollary 1**.: _Fix \(=()\). Consider a uniform partition of \(\) with a constant stepsize \(h=((^{4}/d^{4},^{6}))\), and consider the corresponding process \((X_{t}^{^{}})_{t}\) defined in (11). Assume **H1**, **H3** and \(=^{}\) to be the independent coupling. Suppose in addition that \(\)is absolutely continuously with respect to the Lebesgue measure, \(/^{d}\) is continuously differentiable and satisfies \(\|\|_{^{8}()}^{8}<+\). Then, denoting by \(_{1-}^{}\) and \(_{1-}^{^{}}\) the distribution of \(X_{1-}^{}\) and \(X_{1-}^{^{}}\) respectively, we have that_

\[_{2,}^{2}(_{1-}^{}|_{1-}^{^{ }})^{2}\,\]

_where \(_{2,}\) denotes the Fortet- Mourier distance of order 2, i.e._

\[_{2,}^{2}(,)=_{(,)}\{\|x- y\|^{2},1\}(x,y)\.\]

### Related works and comparison with existing literature.

FMs stand at the forefront of innovation in generative modeling, offering a practical solution to the longstanding challenge of bridging two arbitrary distributions within a finite time interval. Their consequent immense potential has prompted substantial research efforts aimed at providing a theoretical explanation for their effectiveness and has put SGMs and Probability Flow ODEs all in perspective. In this section we report and discuss previous researches on FMs, SGMs and Probability Flow ODEs with the purpose of highlighting the links and differences with our work and contextualizing our contribution.

Non-early-stopping setting.In the context of FMs,  and  seek convergence guarantees in \(2\)-Wasserstein distance. Both consider more general designs for the stochastic interpolant and a deterministic sampling procedure, rather than a stochastic one. However, both works rely on some regularity condition on the approximated flow velocity filed, _i.e._, that it is Lipschitz. Namely,  works under a \(K\)-Lipschitz (uniform in time and space) assumption on the estimator of the exact flow velocity field. How such assumption pertains to the flow matching framework for generative modeling is not articulated and remains unclear. On the other hand,  assumes the estimator of the exact flow velocity field to be \(L_{t}\)-Lipschitz for any \(t\) and discuss in , Theorem 2] how such assumption relate to the setting : under the additional assumption , Assumption 4], the true flow velocity field is proven to be \(L_{t}\)-Lipschitz in space for any \(t\). Therefore,  enhances the findings of . However, , Assumption 4] is not an usual conditions considered in papers about convergence guarantees for SGM and it is unclear which type of distributions satisfy [1, Assumption 4]. Moreover, both works  do not take into account the discretization error in their analysis.

In the context of Probability Flow ODEs,  and  investigate the performance of such models in Total Variation distance and 2-Wasserstein distance. In contrast to  and ,  and  examine the error coming from the (prerequisite when implementing an algorithm) introduction of a time-discretization scheme. However, once again, the provided bounds work under smoothness assumptions either on the score or on its estimator. More precisely, the result reported in  depends on a small \(^{2}\)-Jacobian-estimation error assumption, besides a classical small \(^{2}\)-score-estimation error assumption. As for , they assume the score to be Lipschitz in time and the data to be smooth and log-concave. In contrast, our result do not make such assumptions. Finally, to the best of our knowledge, the recent work  represents the state of art in the context of SGMs without early-stopping procedure: [2, Theorem 2.1] provides a sharp bound in \(\) divergence between the data distribution and the law of the SGM both in the overdamped and kinetic setting under the sole assumptions of an \(^{2}\)-score-approximation error and that the data distribution has finite Fisher information with respect to the standard Gaussian distribution. All previous results are obtained assuming either some Lipschitz condition on the score and/or its estimator (, ) or a manifold hypothesis on the data distribution (). However, we underline that the FM framework enables to consider a significantly wider range of interpolating paths compared to SGMs and to avoid the trade-off concerning the time horizon \(T\) which is inevitable when dealing with SGMs.

Early-stopping procedure.The recent work  establishes convergence guarantees in \(2\)-Wasserstein distance for FMs based on a deterministic sampling procedure. However, the results of  requires to interpolate with a Gaussian distribution and applies only to data distributions which either have a bounded support, are strongly log-concave, or are the convolution between a Gaussian and an other probability distribution supported on an Euclidean Ball. In contrast, for our bound to hold true we only need the data distribution \(^{}\) and its score to have finite eight-ordermoment. Furthermore, even if  goes into depth when dealing with the statistical analysis of the estimator and the \(^{2}\)-estimation error, the entire investigation therein pursued depends on the choice of ReLUnetworks with Lipschitz regularity control to approximate the velocity field. They motivate such choice by proving (see [13, Theorem 5.1]) Lipschitz properties in time and space of the true velocity field under the aforementioned assumptions on the data. On the contrary, we do not assume any regularity on the estimator of the mimicking drift. In the context of Probability Flow ODEs,  provides bound in Total Variation distance, but assuming both the score and its estimator to be Lipschitz in space. So, also in the early-stopping regime, our bound improves previously obtained one.

To conclude, in the context of SGMs,  are able to cover any data distribution with bounded second moment at the cost of using exponentially decreasing step-sizes. However, [11, Corollary 2.4] and  improves upon [10, Theorem 2.2]: the term that takes track of the time-discretization error is linearly dependent on the dimension \(d\) in the former works, whereas quadratically dependent on \(d\) in the latter.

### The proposed methodology.

In what follows, we provide a sketch of the proofs of Theorem 2 and Theorem 3 in order to outline and delineate our methodology.

The starting point of our proof of Theorem 2 is the following (by now) standard  decomposition of the KL divergence which is derived from Girsanov theorem:

\[(^{}|_{1}^{^{}})(( ,)|(X_{}^{^{}}))_{k=0}^{ N-1}_{t_{k}}^{t_{k+1}}\,\,_{^{} }(t_{k},X_{t_{k}}^{})-_{t}(X_{t}^{}) ^{2}\,t\]

\[^{2}+_{k=0}^{N-1}_{t_{k}}^{t_{k+1}} \,_{t_{k}}(X_{t_{k}}^{})-_{t}(X_{t }^{})^{2}\,t\,\]

where, for the first inequality, we used the data processing inequality [14, Lemma 1.6] and the last inequality follows from the triangle inequality and the assumption \(3\). In order to conclude, we should bound the \(^{2}\) norm of the adjoint process in the Pontryagin system associated with the Markovian projection of the interpolant. We do so by introducing a novel quantity in the generative model literature (see ), namely the so-called _reciprocal characteristic_ of the mimicking drift, _i.e._,

\[(_{t}+_{t}^{})_{t}\,\]

where \(^{}\) denotes the generator of \((X_{t}^{})_{t}\). This quantity may be viewed as some sort of mean acceleration field and guides the time evolution of the mimicking drift, as

\[_{t}(X_{t}^{})=(_{t}+_{t}^ {})_{t}(X_{t}^{})t+D_{x} _{t}(X_{t}^{})B_{t}\, t\.\]

The main efforts in our proof are directed towards bounding the \(^{2}\) norm of the reciprocal characteristic whose representation in terms of either conditional moments or higher-order logarithmic derivatives of conditional densities is quite intricate, see (39). Trying to bound each of these terms separately requires strong assumptions on the initial distributions and couplings leading to sub-optimal results. However, using integration by parts both in time and space and a double change of measure argument, and profiting from symmetry properties of the heat kernel, we managed to bound these terms under assumptions comparable to the minimal ones required in the analysis of SGMs. Note that the analysis of the reciprocal characteristic is not required for SGMs (it is always 0) and that controlling it also requires new tricks and ideas, since its representation contains up to three logarithmic derivatives of conditional distributions, whereas the analysis of SGMs requires at most two such derivatives to be analyzed.

Regarding the proof of Theorem 3, we consider the interpolated process \((X_{t}^{1})_{t[0,]}\) restricted to \([0,1-]\). Denoting by \(_{1-}\), the coupling between \(\) and \(_{1-}^{}\) corresponding to the distribution of the couple \((X_{0}^{1},X_{1-}^{1})\). By the property of the Brownian bridge, \((X_{t}^{1})_{t[0,1-]}\) is a stochastic interpolant resulting from \(_{1-}\) and the Brownian bridge on \([0,1-]\). Therefore based on Remark 6,we only have to show \(^{}_{1-}\) and \(_{1-}\) satisfy \(\) and \(\), replacing \(\) in \(\) by \(_{1-}\) defined in (15). More precisely, we show that they hold and that

\[\|_{1-}\|_{^{g}(_{1- })}^{8} \|\|_{^{g}()}^{8}+_{8}[ ]}+_{8}[^{}]}+d ^{4}(1-)^{4}}\] \[^{}_{1-}\|_{^{g}(^{ }_{1-})}^{8} _{8}[]}+_{8 }[^{}]}+d^{4}(1-)^{4}}\.\]

## 4 Conclusion

In this work, we have investigated a Diffusion Flow Matching model built around the Markovian projection of the \(d\)-dimensional Brownian bridge between the data distribution \(^{}\) and the base distribution \(\). In particular, we have derived convergence guarantees in Kullback-Leibler divergence, which take account of all the sources of error - time-discretization error and drift-estimation error - that arise when implementing the model, and which hold under mild moments conditions on \(\), \(^{}\), the scores of \(\), \(^{}\) and the score of the coupling \(\) between \(\) and \(^{}\). However, there are several questions remaining open. First, it would be worthy to understand if we could lower the order of integrability of the score associated with \(,\) and \(\). Second, it would be interesting to complement our analysis by a statistical analysis of DFM (11), similarly to what have been achieved in  for a particular deterministic FM model. Finally, it would be valuable to obtain better dimension dependence with respect to the space dimension \(d\) when applying early-stopping procedure.