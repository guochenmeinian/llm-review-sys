# Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning

Yeongbin Seo Dongha Lee &Jinyoung Yeo

Department of Artificial Intelligence

Yonsei University

{suhcrates,donalee,jinyeo}@yonsei.ac.kr

Co-corresponding authors

###### Abstract

Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. The code and the dataset will be available online2

Figure 1: (a) Learning of Causal LM: The document is decomposed into multiple token sequences \(s_{i} x_{i}|x_{<i}\)3, which aligns with different importance, but uniformly weighted. (b) Train-Attention: Our proposed Train-Attention learns to predict weights that approximate importance, to enable targeted continual knowledge updates through label-free meta-learning method.

Introduction

Large language models (LLMs), pre-trained on extensive text corpora, have demonstrated remarkable effectiveness when fine-tuned or prompted to perform a variety of downstream tasks (Brown et al., 2020; Raffel et al., 2020; Sanh et al., 2021; Wei et al., 2021). However, as the world changes and new knowledge needs to be updated to the parameters, these models often suffer from a significant loss of previously learned knowledge (i.e., catastrophic forgetting (Kemker et al., 2018; Kirkpatrick et al., 2017)). To address this issue, the field of continual knowledge learning (CKL) is being actively researched (Jang et al., 2021, 2022), which aims to teach a model new knowledge while minimizing forgetting of previous knowledge. Previously explored approaches are broadly categorized into three: (1) minimizing parameter changes through regularization, (2) training the expanded parameters of the adapter while freezing the base model parameters, and (3) reviewing old knowledge. However, these approaches naively inherit the inefficiency of the standard fine-tuning procedure of causal LMs, which uniformly apply weights to all tokens, regardless of their importance.

This inefficiency of uniform weighting becomes more significant within the context of CKL, where the model is assumed to possess a substantial amount of world knowledge and grammatical capabilities already, thus emphasizing the need for limiting targets of learning. For example, consider a causal LM that has undergone both pre-training and fine-tuning and now requires to update the new information that "The president of the US is Biden." Figure 1a illustrates how the model processes the example sentence. The only sequence that carries the essential information of this sentence is the final sequence \(s_{6}\) ("The president of the US is" \(\)"Biden"), which encapsulates the context of "US", "president", and "Biden". Conversely, another sequence such as \(s_{4}\) ("The president of the" \(\) "US") only contains information that is already familiar to the model: the close association between "president" and the name of a nation, as well as the grammatical rule that a noun follows "the". Moreover, \(s_{1}\) ("The" \(\) "president") introduces a harmful bias, suggesting that "president" should invariably follow "The", although any nouns could follow "The". If the model overemphasizes the likelihood of this sequence, several issues can arise: (1) Parameters will be updated more than the necessary amount to learn only essential information, thus resulting in more forgetting. (2) The training steps required to learn the important sequence could become prolonged.

Therefore, we hypothesize that focusing learning efforts on important tokens elevates the performance of the CKL. We present empirical evidence of this in SS4.1.1 (paragraph of the analysis on Oracle). The concept of selecting important tokens has been previously explored outside the domain of CKL by Hou et al. (2022), Lin et al. (2024), and demonstrates enhanced performance on downstream tasks. These methods share the same principle, assigning more importance (we denote this "token importance") to the token with higher classification error, which assumes a definition of token importance as "tokens with low-confidence are important". While this approach can accelerate learning of low-confidence tokens, it is still not guaranteed that such low-confidence tokens are truly "important". This emphasizes a need for a more comprehensive definition of "token importance". To clarify this, in the example of Figure 1, it is necessary to consider why human intuition easily accepts that the sequence \(s_{6}\) is more important than others. This understanding comes from the anticipation that knowing the new president will be useful in the future (e.g., conversation with neighbors, school exams, etc) (Land and Furneaux, 1997). Building on this concept, we define "token importance" as the expected utility of the token in related tasks, a concept we refer to as **usefulness**. Upon this definition of token importance, we propose a novel approach to CKL, named **T**rain-**A**ttention-**A**ugmented **L**anguage **M**odel (**TAALM**), which predicts weights of each token based on their usefulness, leveraging this weight on the training phase to enable efficient update of new knowledge. Train-Attention, the supportive model that predicts weight for the base model, is trained through the meta-learning method.

We also introduce a new CKL benchmark, **LAMA-ckl**, designed to offer a more clear comparison of learning and retention performance. This benchmark's advantages over the previous standard are explained in SS4.3. We experiment on **LAMA-ckl** and previous CKL benchmark (TemporalWiki (Jang et al., 2022)), and our method achieves remarkable **state-of-the-art** performance on both. Our method is compatible with other approaches, and shows enhanced performance when integrated, indicating a synergistic effect. We also compared RHO-1 (Lin et al., 2024), which is the recent concurrent work on the token selecting method, where ours shows superior performance on CKL benchmarks. Our main contribution can be summarized in three. (1) We propose a novel token weighting approach to the CKL task, with a novel problem definition and meta-learning method.

(2) A new benchmark for CKL based on the LAMA dataset. (3) Through extensive experiments, TAALM proves notable improvements over the baselines.

## 2 Related Works

Continual Knowledge LearningContinual Knowledge Learning (CKL) (Jang et al., 2021) is one variation of Continual Learning (CL), specified to LLM. It is more focused on updating new knowledge without catastrophic forgetting (Kirkpatrick et al., 2017; Kemker et al., 2018) of previously learned and preservable knowledge. Previous approaches for CL and CKL can be mainly categorized into three: regularization, architectural, and rehearsal. We analyze that the three approaches share a common goal; to minimize changes in parameters from initial points. **(1) Regularization**: directly controlling the extent of change in the parameters through weight regulation such as L2 (Kirkpatrick et al., 2017; Zenke et al., 2017; Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chen et al., 2020). **(2) Architectural**: freezing the base model parameters and expanding learnable parameters with adapters such as Lora (Houlsby et al., 2019; Hu et al., 2021; Wang et al., 2020; Dettmers et al., 2024), thereby keeping initial parameters untouched. **(3) Rehearsal**: method of continually reviewing the data that is employed to train the initial model, ultimately returning the parameters to the initial points (Shin et al., 2017; Sun et al., 2019; He et al., 2019; Rolnick et al., 2019). In this view, our method is another approach to achieve the same goal, minimizing change of parameters, by filtering objective tokens.

Meta-LearningMeta-learning (Finn et al., 2017; Hospedales et al., 2021) is most commonly understood as learning-to-learn; the process of improving a learning episode, over multiple outer learning episodes. During meta-learning, an outer (i.e., meta) learner is fitted to improve the learning of the inner (i.e., base) model. The meta-learner could be an initial parameter of the base model (Finn et al., 2017), an optimizer of the base model (Andrychowicz et al., 2016), or a hyper-parameter of the base model such as learning-rate (Li et al., 2017; Franceschi et al., 2018). In this view, our meta-learner (Train-Attention) is an LLM architectural model that predicts hyper-parameters of the base model, as the token weights serve as the hyper-parameters in the training objective.

Token SelectingMethods to enhance learning by selecting specific tokens have previously been explored through various approaches: Token-Dropping (Hou et al., 2022), Focal Loss (Lin et al., 2017), and RHO-1 (Lin et al., 2024). These methods share a common principle: assigning more importance to the token with higher classification error.

## 3 Train-Attention-Augmented Language Model (TAALM)

### Token Importance and Token-Weighted Learning (TWL)

\[_{} =-_{i} p(x_{i}|x_{<i};)\] (1) \[=-w_{0}}_{i} p(x_{i}|x_{<i};)  w_{0}\ \ \ (w_{0}=1)\] (2) \[_{} =-w_{i}}_{i} p(x_{i}|x_{<i};)  w_{i}\] (3)

To learn a document data \(=\{x_{1},...,x_{N}\}\) which is defined as a series of tokens, a dominant causal language model (LM) (\(\)) commonly employs perplexity (PPL) as the objective function, as formalized in Eq.(1). This can be also interpreted in the form of Eq.(2), which assigns a uniform weight (\(w_{0}=1\)) to log probabilities of each sequence \(x_{i}|x_{<i}\) across all documents. In contrast, our proposed methodology assigns weights \(0<w_{i} 1\) to log probabilities of each sequence, which approximates the importance of each sequence, named token importance (Hou et al., 2022). We denote this set of weights as the **token weight**, and the training process that incorporates the weights (Eq.(3)) as **Token Weighted Learning (TWL)**.

### Train-Attention: Meta-Learning to Predict Token Importance

We suggest defining token importance as **usefulness**, which indicates how much the contained information is useful for solving related tasks in the future. Under this definition, a meta-learningapproach can be derived to develop a supportive model (meta-learner) that predicts the optimal token weights. Let \(\) represent a base causal LM that continually learns knowledge and solves tasks. \(_{}\) represents a task that can be solved using information contained in \(\). Training dataset \(\) is a set of pairs of \(\) and \(_{}\). We assume a task \(_{}\) can be defined as any type (e.g., predicting object labels, classification) as long as the performance can be measured in a differentiable form. The set of token weights, denoted as \(W_{}\), comprises weights \(w_{i}\) that represent the importance of each sequence \(x_{i}|x_{<i}\) within \(\). The meta-learner, named Train-Attention and denoted as \(\), predicts \(W_{}\) from \(\). As illustrated in Figure 1(a), \(\) inherits the architecture and pretrained parameters of the causal LM, but the decoder layer is adjusted to yield only a single-dimensional float between  for each position.

The desired process, learning knowledge and solving a task, is described in two steps; (a) _learn_: \(\) is trained on \(\) and is updated to \(^{}\). This update occurs in a TWL manner, with a token weight \(W_{,}()\) that \(\) predicts upon observing the data \(\). (b) _solve_: The revised model \(^{}\) is applied to solve the task \(_{}\), and the loss value \(_{^{}}(_{})\) is computed to quantify the performance on \(_{}\), where \(\) stands for loss function.

Two steps can be regarded as one black box function, which receives \(\) as an input and outputs \(_{^{}}(_{})\). In other words, the task performance of \(^{}\) depends on how \(\) gives attention when learning evidence text data. And \(\) can be optimized to minimize the \(_{^{}}(_{})\). For this, the procedure of (a) and (b) is developed to corresponding steps of Eq.(4) and (5), where \(\) and \(\) are the learning rates for each respective update.

\[^{} -_{}_{}( ,W_{,})\] (4) \[ -_{}_{^{}}( _{})\] (5)

Figure 4: One step update of \(\).

First, base model \(\) is updated to \(^{}\) through TWL, with the token weight \(W_{}\) which is generated from \(\). Second, the meta-learner \(\) is updated based on the task performance, \(_{^{}}(_{})\). These two steps of update can be also interpreted like Figure 3. As the model \(\) steps out to a new state \(^{}\), the resulting position depends on which token weight (\(W\)) is applied. Some positions are closer to the \(^{*}\), a model optimally capable of the task \(_{}\), as the distance is measured with \(_{^{}}(_{})\). We can conclude the weight with a shorter distance (\(W^{*}_{}\)) is more optimal, in the perspective of usefulness. To prevent the \(\) from converging to the point \(^{*}\), which disables the measurement of distances, we reset the model parameters to the initial state \(\) after every update of \(\). More detail is depicted in Figure 4 and Algorithm 1. As the gradients of parameters of \(\) are tracked during the updating of \(\), its actual implementation is akin to the second derivative. The max iteration step of \(\) (denote as \(M\) in Algorithm 1) is fixed to 1 through our experiment. We employ gradient accumulation when updating \(\) for batch effect.

On the inference phase, \(\) learns data in TWL manner as in Eq.(4), with the parameter of \(\) frozen. This system is Train-Attention-Augmented Language Model (TAALM). In this work, the foundational structure of \(\) is fixed to the small model (TinyLlama-1.1B (Zhang et al., 2024)), while it is augmented to both large (Llama2-7B (Touvron et al., 2023)) and small base models, because \(\) is compatible with any base model that shares the same tokenizer. Additionally, we explore utilizing a 101M-parameter bidirectional transformer (BERT) (Devlin, 2018) as a Train-Attention (TA) to further reduce resource requirements.

## 4 Experiment

We conduct experiments on two benchmarks. One is our newly designed LAMA-ckl, and the other is the established benchmark, TemporalWiki(Jang et al., 2022). We exclude the CKL benchmark by Jang et al. (2021) which is not publicly available. In this section, we present the corpus, evaluation setup, and training detail for Train-Attention and the test result within our proposed LAMA-ckl benchmark. For TemporalWiki, most configurations are aligned with the original work.

### LAMA-ckl

For LAMA-ckl, we tailor the LAMA dataset (LAnguage Model Analysis (Petroni et al., 2019)) to assess the CKL performance, especially the T-REx (Elsahar et al., 2018) part which consists of data from Wikipedia and Wikidata. LAMA is a cluster of datasets that measures how much world knowledge is contained in the LLM. Each unit in the dataset includes a knowledge base triple <subject, relation, object>, along with corresponding evidence documents that support the information contained in this triple. Referring to the previous work (Jang et al., 2022), a CKL benchmark should evaluate both **plasticity** and **stability**. Plasticity refers to how well the model updates new knowledge, while stability refers to how little the model forgets existing knowledge. Accordingly, we sample 500 units of to-learn and not-to-forget sets from LAMA to assess each dimension. During the evaluation, as illustrated on Figure 5, the model learns the evidence documents in the to-learn set. It is then tested on both the to-learn task and the not-to-forget task to assess plasticity and stability, respectively.

Dataset SetupHere, we outline a protocol for sampling test corpora for the LAMA-ckl benchmark. As to-learn set represents "the knowledge that the model either encounters for the first time or needs to update", it is selected based on two constraints: (1) sample from the categories of time-variant relations, predicated on the assumption that knowledge categorized as time-variant

Figure 5: Evaluation procedure of the LAMA-ckl benchmark.

typically requires updates. (2) to ensure the concept of "knowledge new to the model", we select units where the task accuracy is zero when measured with pre-update baselines. Conversely, because not-to-forget set represents "the knowledge that the model already knows and aims to retain", it is selected from categories of time-invariant relations, with task accuracy of 1. We recommend sampling a new dataset by the specified constraints when evaluating models outside of the LLaMA-family, for more accurate assessment. The categorization of time-variant and time-invariant follows Jang et al. (2021). Each selected unit includes (1) an evidence document, (2) a knowledge base triple (e.g., <Lochinvar, is an instance of, castle>), and (3) a descriptive sentence encapsulating the triple (e.g., "Lochinvar is a castle"), which is inherited from LAMA dataset. The task is predicting object label tokens in the descriptive sentence. Details on data are in Appendix A.1

Evaluation SetupDuring the evaluation, each epoch consists of both a training phase and a test phase. In the training phase, the model is trained on a set of 500 evidence documents from the to-learn set. Subsequently, in the test phase, the model's prediction accuracy for the object labels is assessed using 500 descriptive sentences from both the to-learn and not-to-forget sets. This process is repeated over 30 epochs. For the to-learn set, an increase in mean accuracy from 0 signifies the model's plasticity. Conversely, a decline in mean accuracy for the not-to-forget set from 1 to lower values indicates the model's stability, as it tends to forget previously learned information.

In the proposed benchmark LAMA-ckl, we suggest four main factors as evaluation indicators. **1) Top Acc:** the highest to-learn accuracy among checkpoints of 30 epoch. **2) Epoch:** the epoch where the Top Acc appears. **3) NF Acc:** not-to-forget accuracy of the checkpoint model which is the same as Top Acc. **4) Total Knowledge**: the sum of Top Acc and NF Acc, indicating total capacity of knowledge including updating and maintaining. We chose these factors because the best CKL system is one that _learns the most and the fastest and loses the least_. Factor 1, 3, and 4 are better if higher, while factor 2 is better if lower. The detailed configurations for training datasets are in Appendix A.1.1.

Train-Attention Training SetupReferring to Algorithm 1, the training procedure of Train-Attention requires data \(\) and related task \(_{}\). For LAMA-ckl dataset, we assign evidence document of each unit as \(\), and knowledge base triple in a document of **schematic form** as \(_{}\). The perplexity of object token is assigned as the objective of \(\). Figure 6 shows the heat map of token-weight that Train-Attention generates. Train-Attention seems to give more attention to entities of certain categories, rather than words with general grammatical roles. We describe the detailed configuration and findings on the training of Train-Attention in Appendix A.2.

Baseline SetupWe utilize Llama2-7B integrated with QLoRA (Dettmers et al., 2024) as a base model. The baseline methods and their hyper-parameter settings follow previous CKL study of Jang et al. (2022): standard finetune, K-Adapter (Wang et al., 2020), Mix-review (He et al., 2019), LoRA (Hu et al., 2021), RecAdam (Chen et al., 2020). We regard standard finetune on QLoRA as a substitute for full finetuning and LoRA, thus skipping the two baselines. We also compare RHO-1 (Lin et al., 2024), which is the most recent concurrent work on the token selecting method, sharing a similar concept with ours. We chose the initial parameter state as a reference model, which is utilized to select important tokens for RHO-1, with other hyper-parameters following the optimal of the original. We also evaluate a model trained in TWL manner with **Oracle** token weight. For which, a weight of 1 is exclusively assigned to the object label token in the evidence document, and the rest is assigned zero weight. Oracle is compared for two purposes: **(1)** To prove the concept that token-weighted learning has the advantage for CKL. **(2)** To check the performance upper bound of Train-Attention. Detailed configurations are in Appendix A.3

Figure 6: Heat map of token weights from Train-Attention. Orange color indicates higher weights.

[MISSING_PAGE_FAIL:7]

the necessary token. Referring to Figure 17, for Oracle, Top Acc is 0.5470, epoch 17, and NF Acc is 0.9002. Oracle shows that Top Acc is 4.75 times higher and NF Acc is 1.11 times higher than standard finetune, thus proving the substantial advantage of token-weighted learning on CKL. Also, TAALM nearly approaches Oracle, as it achieves 78.2% of Oracle Top Acc. NF Acc of ours also maintains a similar level to Oracle, indicating that Train-Attention is optimized close to the upper bound. It also indicates that optimization through meta-learning could excel human labeled weight.

Small (1B) TAALM excels large baselinesWe also experiment on the smaller (TinyLlama-1B) baselines, and TAALM on 1B records the best compared to 7B baselines. This observation indicates that our method outperforms other baseline methods even with significantly smaller parameter sizes and computational resources. Related Table and Figure are on Appendix A.4.1.

TA on BERT achieves comparable performance with small resourcesWe train BERT as a TA and evaluated it on the LAMA-ckl dataset. Training TA on BERT requires only a single 24GB GPU, significantly reducing resource usage compared to the previous model (single 82GB GPU), yet achieving performance similar to the larger (1.1B) TA (Appendix D).

Ablation study on various design choicesWe conduct an ablation study on various design choices applied to the token importance predicted by TA: (1) masking out tokens (setting importance to 0) in real-time when the prediction matches the label, and (2) dropping weights with token importance below the top k% threshold. The ablation study reveals that heuristic adjustments degrade performance, as TA is already in an optimized state. Details are in Appendix E.

### TemporalWiki

We experiment on the original CKL learning benchmark TemporalWiki (Jang et al., 2022), where models have to continually learn Wikipedia documents of serial periods (0809, 0910, 1011, 1112) and test on the corresponding Twiki-Probes, which is a dataset of knowledge base triples. As we train Train-Attention on the 0809 data, tests are conducted on the rest. We experiment with only a small (TinyLlama-1B) model, which is bigger than the baselines of the original work (GPT-2 Large). We conduct a separate experiment on QLoRA-based K-Adapter based models, referring to the analysis on experiment of LAMA-ckl on SS4.1.1. We only consider training Diffset, which is the only changed part of Wikipedia, because it is reported as a condition of the best performance. Most of the experimental settings follow the original, and additional change is described in Appendix C.

#### 4.2.1 Result & Analysis

Referring to Table 2, our method presents the state-of-the-art performance across both experiments on QLoRA based and K-Adapter based models. This achievement is consistent in all periods, and in both Changed and Unchanged Twiki-Probes. This result is aligns with the LAMA-ckl benchmark result, showing that our method has a substantial advantage on the CKL. QLoRA based baselines showed poor performance compared to K-Adapter based baselines, indicating architectural disadvantage. We also test TAALM optimized for LAMA-ckl on the TemporalWiki, referring

Figure 8: Comparison between Oracle, standard finetuning, and ours, tested on LAMA-ckl.

Table 2b. It achieves the second-best performance, indicating robustness across different distributions of tasks.

### Why LAMA-ckl: clear contrast of plasticity and stability

For the benchmark TemporalWiki, because Diffset is corpora of evidence documents for Changed set, learning of Diffsets is supposed to result in performance improvement over Changed set and forgetting of Unchanged set. However, during our experiments, we observe that both Changed and Unchanged performance tend to move in similar directions when learning Diffset, which is in contradiction to our assumption (Appendix C.4). We analyze this for two primary reasons. First, the Diffset contains evidence documents for both the Changed and the Unchanged sets (Appendix C.4). This is a complicating factor in the evaluation of stability. Second, the experimental setup involves training on a vast amount of data, an average of 707K documents per period, for just a single epoch at a low learning rate. This might result in learning little amount of knowledge, while the task ability is challenged by extensive iterations of updates; which is closer to a continual learning setup rather than CKL. To address this issue, we structured the LAMA-ckl as follows: (1) To mitigate the issue of data overlap, we partition the dataset into variant and invariant subsets. These subsets are further classified based on the task accuracy measured by pre-update baselines. (2) We conduct training over multiple epochs on a relatively small dataset to observe the acquirement of knowledge. In practice, our benchmark shows a clear upward trend in the to-learn set and a distinct decline in the not-to-forget set as training progresses, clearly demonstrating the contrast between plasticity and stability.

## 5 Conclusion and Limitation

In this paper, we demonstrate that the application of Train-Attention significantly enhances CKL performance and is also synergistic with other baselines. Nevertheless, our work has the following limitations and potential for future exploration.

Task specificity of Train-Attention Train-Attention is trained to focus on information related to tasks encountered in the training session. This allows task performance to increase, but on the other hand, it left questions as to whether it would be possible to cope with other tasks. Nonetheless, if the task entails the acquisition of general knowledge, it will be transferable to other tasks sharing similar distributions. For instance, TAALM optimized to LAMA-ckl also achieved the best performance

Table 2: TemporalWiki performance of small (TinyLlama-1B) baselines. **Un** refers Unchanged, **C** refers Changed, **Avg** refers the average of the two. TAALM is our method.

on the TemporalWiki (SS4.2.1). Additionally, Train-Attention can ever evolve to adapt, enabling optimal performance for the current tasks.

What if there are no data-task pairTrain-Attention can be trained only if there is a data-task pair. If there is no paired dataset, it can be doubted that training is difficult. However, every knowledge has its purpose, and we can find a strategy to discover it. **1) Search:** When the data and task pools are separate, we can join highly related pairs via searching. TemporalWiki is also a dataset in which data and tasks are not paired, thus we conduct a lexical search. In the future, also dense research methods can be explored. **2) Generate:** If there is even no separate task pool, we can at least get prior information about what kind of tasks are probable to come in the future. Synthetic tasks can be generated via instruction-tuned LLM, based on this prior information. These methods also resemble the human's cognitive strategy, who often revisit past memories and pose hypothetical questions to themselves to enhance the efficiency of their learning processes.

Broader impactsOur method aims to increase the ability of CKL, therefore enhancing the practicability of LLMs and saving the computational resources for fine-tuning entire huge LLMs. We believe that this paper does not have any immediate negative societal impact.