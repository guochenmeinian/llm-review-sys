# Reinforcement Learning with

Fast and Forgetful Memory

 Steven Morad

University of Cambridge

Cambridge, UK

sm2558@cam.ac.uk

&Ryan Kortvelesy

University of Cambridge

Cambridge, UK

rk627@cam.ac.uk

&Stephan Liwicki

Toshiba Europe Ltd.

Cambridge, UK

stephan.liwicki@toshiba.eu

&Amanda Porok

University of Cambridge

Cambridge, UK

asp45@cam.ac.uk

###### Abstract

Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms _without changing any hyperparameters_. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementation is available at https://github.com/proroklab/ffm.

## 1 Introduction

Reinforcement Learning (RL) was originally designed to solve Markov Decision Processes (MDPs) , but many real world tasks violate the Markov property, confining superhuman RL agents to simulators. When MDPs produce noisy or incomplete observations, they become Partially Observable MDPs (POMDPs). Using _memory_, we can summarize the trajectory into a Markov state estimate, extending convergence guarantees from traditional RL approaches to POMDPs .

In model-free RL, there are two main approaches to modeling memory: (1) RL-specific architectures that explicitly model a probabilistic belief over Markov states  and (2) general-purpose models such as RNNs or transformers that distill the trajectory into a fixed-size latent Markov state. Ni et al.  and Yang and Nguyen  reveal that with suitable hyperparameters, general-purpose memory often outperforms more specialized belief-based memory.

Most applications of memory to RL tend to follow Hausknecht and Stone , using RNNs like Long Short-Term Memory (LSTM) or the Gated Recurrent Unit (GRU) to summarize the trajectory, with other works evaluating transformers, and finding them tricky and data-hungry to train . Morad et al.  evaluate a large collection of recent memory models across manypartially observable tasks, finding that the GRU outperformed all other models, including newer models like linear transformers. Interestingly, they show that there is little correlation between how well memory models perform in SL and RL. Even recent, high-profile RL work like Hafner et al. (2023); Kapturowski et al. (2023) use RNNs over more modern alternatives, raising the question question: why do older memory models continue to overshadow their contemporary counterparts in modern RL?

Training model-free RL policies today is sample inefficient, alchemical, and prone to collapse (Schulman et al., 2015; Zhang et al., 2018), and reasoning over the entire trajectory magnifies these issues. SL can better utilize scale, compute, and dataset advancements than RL, largely removing the need for strong _inductive biases_. For example, transformers execute pairwise comparisons across all inputs, in a somewhat "brute force" approach to sequence learning. Similarly, State Space Models (Gu et al., 2021) or the Legendre Memory Unit (Voelker et al., 2019) are designed to retain information over tens or hundreds of thousands of timesteps, growing the model search space with each additional observation.

Through strong inductive biases, older recurrent memory models provide sample efficiency and stability in exchange for flexibility. For example, the GRU integrates inputs into the recurrent states in sequential order, has explicit forgetting mechanics, and keeps recurrent states bounded using saturating activation functions. These strong inductive biases curtail the model search space, improve training stability and sample efficiency, and provide a more "user-friendly" training experience. If inductive biases are indeed responsible for improved performance, can we better leverage them to improve memory in RL?

ContributionsWe introduce a memory model that summarizes a trajectory into a latent Markov state for a downstream policy. To enhance training stability and efficiency, we employ strong inductive priors inspired by computational psychology. Our model can replace RNNs in recurrent RL algorithms with a single line of code while training nearly two orders of magnitude faster than RNNs. Our experiments demonstrate that our model attains greater reward in both on-policy and off-policy settings and across various POMDP task suites, with similar sample efficiency to RNNs.

## 2 Related Work

RL and SL models differ in their computational requirements. While in SL the model training duration is primarily influenced by the forward and backward passes, RL produces training data through numerous inference workers interacting with the environment step by step. Consequently, it's imperative for RL memory to be both fast and efficient during both _training_ and _inference_. However, for memory models, there is often an efficiency trade-off between between the training and inference stages, as well as a trade-off between time and space complexity.

Recurrent ModelsRecurrent models like LSTM (Hochreiter and Schmidhuber, 1997), GRUs (Chung et al., 2014), Legendre Memory Units (LMUs) (Voelker et al., 2019), and Independent RNNs (Li et al., 2018) are slow to train but fast at inference. Each recurrent state \(\) over a sequence must be computed sequentially

\[_{j},_{j}=f(_{j},_{j-1}), j[1,,n]\] (1)

where \(_{j},_{j}\) are the inputs and outputs respectively at time \(j\), and \(f\) updates the state \(\) incrementally. The best-case computational complexity of recurrent models scales linearly with the length of the sequence and such models _cannot be parallelized over the time dimension_, making them prohibitively slow to train over long sequences. On the other hand, such models are quick at inference, exhibiting constant-time complexity per inference timestep and constant memory usage over possibly infinite sequences.

Parallel ModelsParallel or "batched" models like temporal CNNs (Bai et al., 2018) or transformers (Vaswani et al., 2017) do not rely on a recurrent state and can process an entire sequence in parallel i.e.,

\[_{j}=f(_{1},,_{j}), j[1,,n]\] (2)

Given the nature of GPUs, these models exhibit faster training than recurrent models. Unfortunately, such models require storing all or a portion of the trajectory, preventing their use on long or infinite sequences encountered during inference. Furthermore, certain parallel models like transformers require quadratic space and \(n\) comparisons _at each timestep_ during inference. This limits both the number of inference workers and their speed, resulting in inefficiencies, especially for on-policy algorithms.

Hybrid ModelsLinear transformers (Katharopoulos et al., 2020; Schlag et al., 2021; Su et al., 2021) or state space models (Gu et al., 2021, 2020) provide the best of both worlds by providing equivalent recurrent and closed-form (parallel) formulations

\[_{j}=f(_{j},_{j-1})=g(_{1},,_{j}), j [1,,n]\] (3)

Training employs the batched formula to leverage GPU parallelism, while inference exploits recurrent models' low latency and small memory footprint. Thus, hybrid models are well-suited for RL because they provide both fast training and fast inference, which is critical given the poor sample efficiency of RL. However, recent findings show that common hybrid models typically underperform in comparison to RNNs on POMDPs (Morad et al., 2023).

## 3 Problem Statement

We are given a sequence of actions and observations \((_{1},),(_{2},_{1}),(_{n},_{n-1})\) up to time \(n\). Let the trajectory \(\) be some corresponding encoding \(=(,)\) of each action-observation pair

\[_{n}=[_{1},,_{n}]=[(_{1},), ,(_{n},_{n-1})].\] (4)

Our goal is to summarize \(_{n}\) into a latent Markov state \(_{n}\) using some function \(f\). For the sake of both space and time efficiency, we restrict our model to the space of _hybrid_ memory models. Then, our task is to find an \(f\), such that

\[_{n},_{n}=f(_{k:n},_{k-1}),\] (5)

where \(_{k:n}\) is shorthand for \(_{k},_{k+1},,_{n}\). Note that by setting \(k=n\), we achieve a one-step recurrent formulation that one would see in an RNN, i.e., \(_{n},_{n}=f(_{n:n},_{n-1})=f(_{n},_{n-1})\).

## 4 Background

In the search for inductive priors to constrain our memory model search space, we turn to the field of computational psychology. In computational psychology, we use the term _trace_ to refer to the physical embodiment of a memory - that is, the discernable alteration in the brain's structure before and after a memory's formation. Computational psychologists model long-term memory as a collection of traces, often in matrix form

\[_{n}=[_{1},,_{n}],\] (6)

where \(_{j}\) are individual traces represented as column vectors and \(_{n}\) is the memory at timestep \(n\)(Kahana, 2020).

Composite MemoryComposite memory (Galton, 1883) approximates memory as a lossy blending of individual traces \(\) via summation, providing an explanation for how a lifetime of experiences can fit within a fixed-volume brain. Murdock (1982) expresses this blending via the recurrent formula

\[_{n}=_{n-1}+_{n}_{n},\] (7)

where \((0,1)\) is the forgetting parameter and \(_{n}\) is a diagonal matrix sampled from a Bernoulli distribution, determining a subset of \(_{n}\) to add to memory. In Murdock's model, \(\) is a vector, not a matrix. We can expand or "unroll" Murdock's recurrence relation, rewriting it in a closed form

\[_{n}=^{n}_{0}+^{n-1}_{1}_{1}+ ^{0}_{n}_{n}, 0<<1,\] (8)

making it clear that memory decays exponentially with time. Armed with equivalent recurrent and parallel formulations of composite memory, we can begin developing a hybrid memory model.

Contextual DriftIt is vital to note that incoming traces \(\) capture raw sensory data and lack _context_. Context, be it spatial, situational, or temporal, differentiates between seemingly identical raw sensory inputs, and is critical to memory and decision making. The prevailing theory among computational psychologists is that contextual and sensory information are mixed via an outer product (Howard and Kahana, 2002)

\[}_{n} =_{n}_{n}^{}\] (9) \[_{n} =_{n-1}+_{n}, 0<<1,\] (10)

where \(_{n}\) is the contextual state at time \(n\), and \(\) ensures smooth or gradual changes between contexts. Although context spans many domains, we focus solely on temporal context.

## 5 Fast and Forgetful Memory

Fast and Forgetful Memory (FFM) is a hybrid memory model based on theories of composite memory and contextual drift. It is composed of two main components: a _cell_ and an _aggregator_. The cell receives an input and recurrent state \(,\) and produces a corresponding output and updated recurrent state \(,\) (Figure 1). The aggregator resides within the cell and is responsible for updating \(\) (Figure 2). We provide the complete equations for both the aggregator and cell and end with the reasoning behind their design.

AggregatorThe aggregator computes a summary \(_{n}\) of \(_{1:n}\), given a recurrent state \(_{k-1}\) and inputs \(_{k:n}\)

\[_{n} =^{n-k+1}_{k-1}+_{j=k}^{n}^{ n-j}(_{j}_{c}^{}),_{n}^{m c}\] (11) \[^{t} =(-)(-i)^{}^{ t}=(-t( _{1}+i_{1}))&&(-t(_{1}+i_{c}) )\\ &&\\ (-t(_{m}+i_{1}))&&(-t(_{m}+i _{c}))^{m c}.\] (12)

where \(\) is the Hadamard product (or power), \(m\) is the trace size, \(c\) is the context size, and \(_{+}^{m},^{c}\) are trainable parameters representing decay and context respectively. Multiplying column a vector by \(_{c}^{}\) "broadcasts" or repeats the column vector \(c\) times.

In Appendix C, we derive a memory-efficient and numerically stable closed form solution to compute all states \(_{k:n}\) in parallel over the time dimension. Since our model operates over relative time, we map absolute time \(k,,n\) to relative time \(p 0,,t\), where \(t=n-k\). The closed form to compute a state \(_{k+p}\) given \(_{k-1}\) is then

\[_{k+p}=^{p+1}_{k-1}+^{p-t}_{ j=0}^{p}^{t-j}(_{k+j}_{c}^{}),_{k+p} ^{m c}.\] (13)

We can rewrite the aggregator's closed form (Equation 13) in matrix notation to highlight its time-parallel nature, computing all states \(_{k:n}=_{k},_{k+1},,_{k+n}\) at once

\[_{k:n}=^{1}\\ \\ ^{t+1}_{k-1}\\ \\ _{k-1}+^{-t}\\ \\ ^{0}(_{j=0}^{0}^{t-j}(_{k+j}_{c}^{}))\\ \\ (_{j=0}^{t}^{t-j}(_{k+j}_{c}^{} )),_{k:n}^{(t+1) m c}.\] (14)

The cumulative sum term in the rightmost matrix can be distributed across \(t\) processors, each requiring \(O( t)\) time using a prefix sum or scan (Harris et al., 2007).

CellThe aggregator alone is insufficient to produce a Markov state \(\). \(\) is complex-valued and contains a large amount of information that need not be present in \(\) at the current timestep, making it cumbersome to interpret for a downstream policy. Furthermore, \(\) could benefit from additional preprocessing. The cell applies input gating to \(\) and extracts a real-valued Markov state \(\) from \(\).

\[}_{k:n} =_{1}(_{k:n})(_{2}(_{k:n})), }_{k:n}^{(t+1) m}\] (15) \[_{k:n} =(}_{k:n},_{k-1}), _{k:n}^{(t+1) m c}\] (16) \[_{k:n} =_{3}(([_{k:n}][_{k:n }])), _{k:n}^{(t+1) d}\] (17) \[_{k:n} =(_{k:n})(_{4}(_{k:n}))+ _{5}(_{k:n})(1-(_{4}(_{k:n}))._{k:n} ^{(t+1) d}\] (18)

Agg represents the aggregator (Equation 14) and \(\) represents linear layers with mappings \(_{1},_{2}:^{d}^{m}\), \(_{3}:^{m 2c}^{d}\), \(_{4},_{5}:^{d}^{d}\). \(,\) extract the real and imaginary components of a complex number as reals, \(\) reshapes a matrix (\(m c mc\)) and \(\) is the concatenation operator. \(\) is nonparametric layer norm, and \(\) is sigmoid activation. Equation 15 applies input gating, Equation 16 computes the recurrent states, Equation 17 projects the state into the real domain, and Equation 18 applies output gating.

Modeling Inductive BiasesTo justify FFM's architecture, we detail its connections to computational psychology. Drawing from the theory of composite memory, we integrate sigmoidal gating to \(\) in Equation 15, approximating the sparsity-driven Bernoulli term, \(\), presented in (Equation 7). This inductive bias represents that only a small subset of each sensory experience is stored in memory.

Following the blending constraint imposed by composite memory (Equation 8), we sum the traces together (Equation 11). We suspect this blending prior yields a model that is more robust to the bad policy updates that plague RL, compared to RNNs that apply nonlinear and destructive transforms (e.g. deletion) to the recurrent state.

Figure 1: A detailed example of aggregator dynamics for \(m=1,c=2\), showing how a one-dimensional input \(x_{1}\) contributes to the recurrent state \(\) over time \(t=[1,n]\). At the first timestep, \(_{1}\) is simply \(x_{1}\). Over time, the contribution of \(x_{1}\) to \(\) undergoes exponential decay via the \(\) term (forgetting) and oscillations via the \(\) term (temporal context). In this example, the contribution from \(x_{1}\) to \(_{n}\) approaches zero at time \(n\).

Figure 2: A visualization of an FFM cell running recurrently for a single timestep (i.e., inference mode). Inputs \(_{k},_{k-1}\) go through various linear layers (\(\)), hadamard products (\(\)), addition (\(\)), normalization (\(\)), and sigmoids \(()\) to produce outputs \(_{k},_{k}\). The boxed region denotes the aggregator, which decays and shifts \(_{k-1}\) in time via \(\). During training, the aggregator computes \(_{k},,_{n}\) in parallel so that we can compute all \(_{k},,_{n}\) in a single forward pass.

In Equation 12, the exponential component \((-t)\) ensures that traces are forgotten according to composite memory decay from Equation 8. We choose an exponential decay \(e^{-||}\), approximating Murre and Dros (2015). This inductive bias asymptotically decays traces, reducing the size of the model optimization space. This is in direct contrast to recent SL memory models which aim to store as much information as possible for as long as possible (Gu et al., 2021; Voelker et al., 2019; Schlag et al., 2021). In our ablations, we show that forgetting is critical in RL.

Traditionally, contextual coding and composite memory are separate mechanisms, but we incorporate both through \(\). The \((-it)\) component of \(\) in Equation 12 applies temporal context, enabling relative-time reasoning. Following contextual coding theory, we take the outer product of a trace vector, in our case decayed by \(\), with a context vector produced by \(\). We can rewrite \(^{t}=^{t-1}\), mirroring the gradual recurrent context changes from Equation 10.

The recurrent state \(\) is a complex-valued matrix, so we project it back to the real domain as \(\) (Equation 17). \(\) can be large for dimensions where \(\) is near zero (minimal decay), so we find it crucial to apply layer normalization in Equation 18. Finally, we apply a gated residual connection to improve convergence speed, letting gradient descent find the optimal mixture of input \(\) to memory \(\) for the Markov state \(\).

The Mathematics Underpinning FFMThe goal of FFM is to produce a memory mechanism which can be parallelized across the time dimension. As discussed in section 4, we can achieve this through a summation of sensory information mixed by some arbitrary function \(\) with a temporal context \(_{j}=(j)\), generating an aggregator of the form

\[_{n}=_{j=0}^{n}((j),_{j}).\] (19)

Any implementation under this formulation would provide memory with temporal context for _absolute_ time. However, in order to maximize generalization, we wish to operate over _relative_ time. This introduces a problem, as the _relative_ temporal context \((n-j)\) associated with each \(x_{j}\) must be updated at each timestep. Therefore, we must find a function \(h\) that updates each temporal context with some offset: \(h((j),(k))=(j+k)\). Solving for \(h\), we get: \(h(j,k)=(^{-1}(j)+^{-1}(k))\). If we also set \(=h\), then applying a temporal shift \(h(,k)\) to each term in the sum yields the same result as initially inserting each \(x_{j}\) with \((j+k)\):

\[h(((j),x_{j}),k)=((j+k),x_{j})\] (20)

Consequently, it is possible to update the terms at each timestep to reflect the relative time \(n-j\). However, it is intractable to recompute \(n\) relative temporal contexts at each timestep, as that would result in \(n^{2}\) time and space complexity for a sequence of length \(n\). To apply temporal context in a batched manner in linear space, we can leverage the distributive property. If we select \((a,b)=a b\), then the update distributes over the sum, updating all of the terms simultaneously. In other words, we can _update the context of \(n\) terms in constant time and space_ (see Appendix C for the full derivation). As \(\) is defined as a function of \(\), we can solve to find \((t)=e^{ t}\) (although any exponential constitutes a solution, we select the natural base). This yields an aggregator of the form

\[_{n}=_{j=0}^{n}e^{(n-j)}_{j}.\] (21)

If \(_{+},_{j} 0\), then the recurrent state will explode over long sequences: \(_{n}e^{(n-j)}_{j}=\), so the real component of \(\) should be negative. Using different decay rates \(_{j},_{k}_{-}\), we can deduce the temporal ordering between terms \(x_{j}\) and \(x_{k}\). Unfortunately, this requires both \(x_{j},x_{k}\) eventually decay to zero - what if there are important memories we do not want to forget, while simultaneously retaining their ordering? If \(\) is imaginary, then we can determine the relative time between \(_{j},_{k}\) as the terms oscillate indefinitely without decaying. Thus, we use a complex \(\) with a negative real component, combining both forgetting and long-term temporal context. In the following paragraphs, we show that with a complex \(\), the FFM cell becomes a _universal approximator of convolution_.

Universal Approximation of ConvolutionHere, we show that FFM can approximate any temporal convolutional. Let us look at the \(\) term from Equation 17, given the input \(}\), with a slight change in notation to avoid the overloaded subscript notation: \(}()=}_{}\), \((n)=_{n}\). For a sequence \(1\) to \(n\), \((n)\), the precursor to the Markov state \(_{n}\), can be written as:

\[(n)=+_{=1}^{n}}())}^{})}.\] (22)

where \(,\) is the weight and bias from linear layer \(_{3}\), indexed by subscript. Looking at a single input dimension \(k\) of \(}\), we have

\[(n) =+_{j=kcm}^{(k+1)cm}_{j}_{=1}^{n}_{k}()+_{k}))}\] (23) \[=+_{=1}^{n}_{k}()_{j=kcm}^{(k+1) cm}_{j}+_{k}))}\] (24)

Equation 24 is a temporal convolution of \(_{k}(t)\) using a Fourier Series filter with \(c\) terms (\(^{c}\)), with an additional learnable "filter extent" term \(_{k}\). The Fourier Series is a universal function approximator, so FFM can approximate any convolutional filter over the signal \(}()\). Appendix A further shows how this is related Laplace transform. Unlike discrete convolution, we do not need to explicitly store prior inputs or engage in zero padding, resulting in better space efficiency. Furthermore, the filter extent \(_{k}\) is learned and dynamic - it can expand for sequences with long-term temporal dependencies and shrink for tasks with short-term dependencies. Discrete temporal convolution from methods like Bai et al. (2018) use a fixed-size user-defined filter extent.

InterpretabilityUnlike other memory models, FFM is interpretable. Each dimension in \(\) has a known decay rate and contextual period. We can determine trace durability (how long a trace lasts) \(_{}\) and the maximum contextual period \(_{}\) of each dimension in \(\) via

\[_{}=_{c}^{}},\ _{}=_{m}}\] (25)

where \(\) determines the strength at which a trace is considered forgotten. For example, \(=0.01\) would correspond to the time when the trace \(_{k}\) contributes 1% of its original value to \(_{n}\). FFM can measure the relative time between inputs up to a modulo of \(_{}\).

## 6 Experiments and Discussion

We evaluate FFM on the two largest POMDP benchmarks currently available: POPGym (Morad et al., 2023) and the POMDP tasks from (Ni et al., 2022), which we henceforth refer to as POMDP-Baselines. For POPGym, we train a shared-memory actor-critic model using recurrent using Proximal Policy Optimization (Schulman et al., 2017). For POMDP-Baselines, we train separate memory models for the actor and critic using recurrent Soft Actor Critic (SAC) (Haarnoja et al., 2018) and recurrent Twin Delayed DDPG (TD3) (Fujimoto et al., 2018). We replicate the experiments from the POPGym and POMDP-Baselines papers as-is _without changing any hyperparameters_. We use a single FFM configuration across all experiments, except for varying the hidden and recurrent sizes to match the RNNs, and initialize \(,\) based on the max sequence length. See Appendix D for further details.

Table 2 lists memory baselines, Table 3 ablates each component of FFM, Figure 3 contains a summary of training statistics for all models (wall-clock train time, mean reward, etc), and Figure 4

  Model & Training &  \\   & Parallel Time & Space & Time & Space \\  RNN & \(O(n)\) & \(\) & \(\) & \(\) \\ Transformer & \(\) & \(O(n^{2})\) & \(O(n)\) & \(O(n^{2})\) \\ FFM (ours) & \(\) & \(\) & \(\) & \(\) \\  

Table 1: The time and space complexity of memory models for a sequence of length \(n\) (training), or computing a single output recurrently during a rollout (inference).

contains a detailed comparison of FFM against the best performing RNN and transformer on POPGym. Figure 5 evaluates FFM on the POMDP-Baselines tasks and Figure 6 investigates FFM interpretability. See Appendix E and Appendix F for more granular plots. Error bars in all cases denote the 95% bootstrapped confidence interval.

Practicality and Robustness:FFM demonstrates exceptional performance on the POPGym and POMDP-Baselines benchmarks, achieving the highest mean reward as depicted in Figure 3 and Figure 5_without any changes to the default hyperparameters_. FFM also executes a PPO training epoch nearly two orders of magnitude faster than a GRU (Figure 3, Table 1). Without forgetting, FFM underperforms the GRU, showing that forgetting is the most important inductive prior (Table 3). We use a single FFM configuration for each benchmark, demonstrating it is reasonably insensitive to hyperparameters. All in all, FFM outperforms all other models on average, across three RL algorithms and 52 tasks. Surprisingly, there are few occurrences where FFM is noticeably worse than others, suggesting it is robust and a good general-purpose model.

Explainability and Prior Knowledge:Figure 6 demonstrates that FFM learns suitable and interpretable decay rates and context lengths. We observe separate memory modes for the actor and

  & FFM & FFM-NI & FFM-NO & FFM-NC & FFM-FC & FFM-ND & FFM-FD & FFM-HP \\  \(\) & **0.399** & \(0.392\) & \(0.395\) & \(0.382\) & \(0.383\) & \(0.333\) & \(0.390\) & \(0.395\) \\ \(\) & **0.005** & 0.001 & 0.006 & 0.005 & 0.005 & 0.005 & 0.006 & 0.010 \\  

Table 3: Ablating FFM components over all POPGym tasks. FFM-NI removes input gating, FFM-NO removes the output gating, FFM-NC does not use temporal context, FFM-FC uses fixed (non-learned) context, FFM-ND does not use decay, and FFM-FD uses fixed (non-learned) decay. FFM-HP uses the Hadamard product instead of the outer product to compute \(\) in Equation 12. FFM-HP requires additional parameters but ensures independence between rows and columns of the state. FFM-ND (no decay) validates our assumption that forgetting is an important inductive bias.

Figure 3: Training statistics computed over ten trials and one epoch of PPO, with FFM in orange. FFM trains nearly two orders of magnitude faster on the GPU than the fastest RNNs (note the log-scale x axis). Even on the CPU, FFM trains roughly one order of magnitude faster than RNNs on the GPU. FFM has sub-millisecond CPU inference latency, making it useful for on-policy algorithms. FFM memory usage is on-par with RNNs, providing scalability to long sequences. Despite efficiency improvements, FFM still attains greater episodic reward than other models on POPGym.

Figure 4: FFM compared to the GRU, the best performing model on POPGym. See Appendix F for comparisons across all other POPGym models, such as linear transformers. Error bars denote the bootstrapped 95% confidence interval over five trials. FFM noticeably outperforms state of the art on seventeen tasks, while only doing noticeably worse on four tasks.

Figure 5: Comparing FFM with tuned RNNs on the continuous control tasks from POMDP-Baselines. Ni et al. (2022) selects the best performing RNN (either GRU or LSTM) and tunes hyperparameters on a per-task basis. Error bars denote the bootstrapped 95% confidence interval over five random seeds. The task suffix V or P denotes whether the observation is velocity-only (masked position) or position-only (masked velocity) respectively. Untuned FFM outperforms tuned RNNs on all but one task for SAC. For TD3, FFM meets or exceeds RNN performance on all but one task.

Figure 6: A case study on FFM explainability. We investigate the task RepeatPreviousMedium, where the episode length is 104 timesteps and the agent must output the observation from 32 timesteps ago. **(Left two)** We visualize \(\) using trace durability \(_{},=0.1\) and context period \(_{}\). Both \(_{},_{}\) demonstrate modes for the actor and critic, which we highlight in the figures. The critic requires the episode’s current timestep to accurately predict the discounted return, while the actor just requires 32 timesteps of memory. **(Left)** The \(_{}\) critic mode converges to the maximum episode length of 104. The actor mode converges to 2x the necessary period (perhaps because \((_{}})\), the real component of \(-ti\), is monotonic on the half-period \(0.5_{}\)). **(Middle)** The \(_{}\) terms converge to 32 for the actor and a large value for the critic. **(Right)** We initialize \(,\) to encapsulate ideal actor and critic modes, such that \(_{},_{}\). We find FFM with informed initialization outperforms FFM with the original initialization of \(_{},_{}\).

critic, and we can initialize \(,\) using prior knowledge to improve returns (Figure 6). This sort of prior knowledge injection is not possible in RNNs, and could be useful for determining the number of "burn-in" steps for methods that break episodes into fixed-length sequences, like R2D2 (Kapturowski et al., 2019) or MEME (Kapturowski et al., 2023).

Limitations and Future Work

Scaling Up:We found that large context sizes \(c\) resulted in decreased performance. We hypothesize that a large number of periodic functions results in a difficult to optimize loss landscape with many local extrema. Interestingly, transformers also employ sinusoidal encodings, which might explain the difficulty of training them in model-free RL. We tried multiple configurations of FFM cells in series, which helped in some tasks but often learned much more slowly. In theory, serial FFM cells could learn a temporal feature hierarchy similar to the spatial hierarchy in image CNNs. In many cases, serial FFM models did not appear fully converged, so it is possible training for longer could solve this issue.

Additional Experiments:Hyperparameter tuning would almost guarantee better performance, but would also result in a biased comparison to other models. We evaluated FFM with on-policy and off-policy algorithms but did not experiment with offline or model-based RL algorithms. In theory, FFM can run in continuous time or irregularly-spaced intervals simply by letting \(t\) be continuously or irregular, but in RL we often work with discrete timesteps at regular intervals so we due not pursue this further.

Numerical Precision:FFM experiences a loss of numerical precision caused by the repeated multiplication of exponentials, resulting in very large or small numbers. Care must be taken to prevent overflow, such as upper-bounding \(\). Breaking a sequence into multiple forward passes while propagating recurrent state (setting \(n-k\) less than the sequence length) fixes this issue, but also reduces the training time efficiency benefits of FFM. We found FFM performed poorly using single precision floats, and recommend using double precision floats during multiplication by \(\). We tested a maximum sequence length of 1024 per forward pass, although we could go higher by decreasing the maximum decay rate. With quadruple precision floats, we could process sequences of roughly 350,000 timesteps in a single forward pass with the max decay rate we used. Unfortunately, Pytorch does not currently support quads.

## 7 Conclusion

The inductive priors underpinning FFM are key to its success, constraining the optimization space and providing parallelism-enabling structure. Unlike many memory models, FFM is interpretable and even provides tuning opportunities based on prior knowledge about individual tasks. FFM provides a low-engineering cost "plug and play" upgrade to existing recurrent RL algorithms, improving model efficiency and reward in partially observable model-free RL with a one-line code change.

## 8 Acknowledgements

Steven Morad and Stephan Liwicki gratefully acknowledge the support of Toshiba Europe Ltd. Ryan Kortvelesy and Amanda Prorok were supported in part by ARL DCIST CRA W911NF-17-2-0181.