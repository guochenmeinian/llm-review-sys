# Unified Insights: Harnessing Multi-modal Data for Phenotype Imputation via View Decoupling

Qiannan Zhang, Weishen Pan, Zilong Bai, Chang Su, Fei Wang

Weill Cornell Medicine, Cornell University

{qiz4005,wep4001,zib4001,chs4001,few2001}@med.cornell.edu

Corresponding Author

###### Abstract

Phenotype imputation plays a crucial role in improving comprehensive and accurate medical evaluation, which in turn can optimize patient treatment and bolster the reliability of clinical research. Despite the adoption of various techniques, multi-modal biological data, which can provide crucial insights into a patient's overall health, is often overlooked. With multi-modal biological data, patient characterization can be enriched from two distinct views: the biological view and the phenotype view. However, the heterogeneity and imprecise nature of the multi-modal data still pose challenges in developing an effective method to model from two views. In this paper, we propose a novel framework to incorporate multi-modal biological data via view decoupling. Specifically, we segregate the modeling of biological data from phenotype data in a graph-based learning framework. From the biological view, the latent factors in biological data are discovered to model patient correlation. From the phenotype view, phenotype co-occurrence can be modeled to reveal patterns across patients. Hence, patients are encoded from these two distinct views. To mitigate the influence of noise and irrelevant information in biological data, we devise the cross-view contrastive knowledge distillation that distills insights from the biological view to enhance phenotype imputation. Phenotype imputation with the proposed model demonstrates superior performance over state-of-the-art models on the real-world biomedical database.

## 1 Introduction

Clinical records, serving as a critical resource for understanding disease patterns and patient outcomes, are valuable for observational studies. However, its collection can be biased or incomplete due to the limits on infrastructures and expertise, the inconsistency in data types across healthcare systems, and the variability in patient cohorts, etc [3; 15]. For instance, it is recognized that patients with dementia and its related conditions can have under-documented phenotypes , probably resulting from a lack of clear symptoms early on or ignorance of related diseases. The issue of missing or incomplete phenotypic data is pervasive and can lead to biased results in medical research and suboptimal patient care . In light of this, _phenotype imputation_ is essential to ensure a more holistic and precise medical evaluation, thereby optimizing patient care and enhancing the validity of clinical studies.

Traditional imputation methods [11; 2] rely on informative statistical characteristics of the clinical data to infer the missing phenotypes, yet often neglect the broad, interconnected nature of clinical data with multi-modal biological information such as proteomics and metabolomics, while the latter might provide deeper insights into the patient's health status. The growing development of extensive biobanks [4; 33], collecting various biological and lifestyle data alongside traditional clinical records, unlocks a potential to address incomplete phenotypic data in clinical records. By leveraging multi-modal biological data as external information, as shown in Figure 1, the associationsbetween biological observations and clinical phenotypes might improve the inference of incomplete phenotypes.

However, leveraging multi-modal information for phenotype imputation remains a complex challenge in two folds: 1) The heterogeneity of multi-modal data typically results in significant variances from clinical data, as it includes different data types and characteristics. For instance, continuous variables in proteomics may exhibit different patterns and correlations with a patient's health status compared to discrete phenotype data. Multi-modal biological data often contain measurement noise and irrelevant information unrelated to phenotypic observations, which hinders accurate phenotype imputation. Furthermore, biological data are frequently missing for many individuals due to the labor-intensive and costly nature of data collection.

Despite the compelling need to leverage multi-modal data, the challenges outlined above have posed significant obstacles to developing an effective approach for phenotype imputation. In recent years, graphs have gained traction as a powerful tool for modeling complex data and capturing relationships between real-world entities. Representing patients and phenotypes within a graph structure and imputing missing phenotypes using Graph Neural Networks (GNNs) offers a promising path forward. Biological data could, in principle, be incorporated as patient attributes and propagated through the graph. However, the joint modeling conflicts with the heterogeneity between biological and phenotypic data, as each encapsulates distinct rationales for unveiling patient-specific health conditions. First, _from a statistical and collaborative view_, the patient-phenotype graph connecting patients and their phenotypes reflects phenotype co-occurrence patterns across all patients' interactions. These co-occurrence patterns indicate an underlying principle in imputation: if phenotype \(x\) and \(y\) are frequently co-diagnosed, it is sensible to impute \(y\) for a patient once \(x\) is observed. Second, _from a biological view_, a patient's biological data reveals their fine-grained health status. This highlights another rationale for imputation: understanding the detailed health conditions from biological data can guide the imputation of phenotypes that correspond to similar biological health status. Therefore, in this paper, we propose a view decoupling approach to segregate the modeling of biological data from phenotypic data, thereby fully utilizing the information from both sources.

To model the correlation between patients and phenotypes, one can construct and encode a bipartite graph. Nevertheless, the use of biological data is not a straightforward task. Biological data is characteristically composed of a wide range of variables, including protein concentrations, metabolic profiles, gene expression levels, etc. These variables exhibit high-dimensional and continuous characteristics, making it challenging to model the data effectively. More importantly, the biological conditions of patients uncover major underlying factors that indicate health status. In other words, patients sharing similar underlying biological factors could have similar phenotypes. Identifying these latent factors would facilitate the effective characterization of patients and their phenotypes.

To tackle these challenges, in this paper, we propose a novel framework **MPI**, aiming to harness the **M**ultimodal data for **P**henotype **I**mputation. First, to identify the latent biological factors, we propose quantizing the biological data and uncovering the corresponding factors using Residual Quantization. Then, the obtained factors in conjunction with the patients themselves, are utilized to create a graph that models the correlation between patients from a biological view. To decouple views and segregate the modeling of biological data from phenotypic data, the patients and phenotypes are additionally incorporated into another separate graph that depicts the patterns of co-occurrence from the collaborative view. GNNs are then employed to encode both graphs. Second, with the two separate graphs, we aim to leverage the biological information to facilitate the phenotype imputation. However, due to the presence of noise and irrelevant information in biological data, relying solely on biological factors may lead to inaccurate imputation. Thus, we employ a cross-view contrastive knowledge distillation strategy to distill biological knowledge for enhancing phenotype imputation. Within a teacher-student framework, we consider the biological-view GNN as the teacher model and

Figure 1: Phenotype imputation with multi-modal biological data. ”M1” denotes Modality 1, and ”M2” represents Modality 2. ”—” refers to the missing modality and the red question mark refers to the phenotype that needs to be imputed.

the collaborative-view GNN as the student model. Rather than replicating the teacher model entirely, the aim is for the student model to glean useful knowledge by receiving partial guidance from the teacher model. The main contributions of this work are summarized as:

1) We propose leveraging multi-modal data to enhance phenotype imputation through view decoupling, thereby segregating the modeling of multi-modal biological data from phenotype data. 2) To enhance the depiction of patient profiling and facilitate the imputation, we propose to uncover the latent biological factors of patients and accordingly model the correlation among the patients based on these factors. 3) To avoid the impact of noise and irrelevant information in biological data, we adopt a novel cross-view contrastive knowledge distillation to subtly leverage information from biological data. 4) Extensive experiments over a real-world biomedical database demonstrate the superiority of our proposed method over state-of-the-art methods.

## 2 Related Work

**Phenotype Imputation.** Phenotype imputation involves predicting missing phenotypic information in clinical electronic health records (EHRs), e.g., diseases and symptoms, generally leveraging various methods ranging from traditional statistical approaches to advanced machine learning techniques. Early research relies on statistical modeling and matrix analysis [41; 40; 10; 1], while deep learning demonstrates effectiveness in modeling more complex dependencies with deep networks [14; 50; 27; 2]. Despite existing efforts to explore the correlations between phenotypes and genotypes , multi-modal biological data is largely overlooked in EHR analysis. Our approach differentiates itself by utilizing multi-modal biological data to enhance phenotype imputation in EHRs.

**Graph Neural Networks in Biomedicine.** Graph Neural Networks (GNNs) [13; 54] have been employed to model the interconnectivity of either clinical data or biological information. A line of research devises GNN models for EHRs to enhance healthcare representation learning and patient-specific outcomes [9; 35; 20; 28]. By leveraging the entities and connections in EHRs, e.g., diseases, symptoms, and drug interactions, GNNs show effectiveness in producing patient profiles and clinical predictions [23; 26]. Meanwhile, biological studies leverage GNNs to explore biological networks, promote disease mechanism discovery, analyze drug response, etc. For instance, single-cell biology adopts GNNs to analyze cellular heterogeneity, aiming for an improved understanding of cellular functions and interactions [18; 31]. Besides, some work integrates clinical and molecular data to predict adverse drug reaction signals , exemplifying the integration of EHRs and biological data for combined healthcare analysis. Our approach leverages biological data to aid phenotype imputation in EHRs by bridging the gap between clinical data and underlying biological mechanisms.

**Multi-modal Representation Learning on EHRs.** Multi-modal learning on EHRs aims to integrate varied modalities in EHRs, e.g., medication records, lab test results, imaging data, and clinical notes, to obtain optimized patient representations [23; 17]. Given the potential unavailability of modalities, research efforts are made to improve model robustness in the face of partially or completely missing modalities. Strategies include imputing the missing modalities, exploring the data generation process, and preserving the structure of observed data [48; 29; 52; 6; 47; 53]. However, existing works primarily explore modalities within EHRs as clinical insights, often overlooking biological knowledge in EHR analysis. Different from existing work, we explore multi-modal biological data with random missingness to enhance phenotype imputation in EHRs, via addressing the heterogeneity and inaccuracy in multi-modal biological data.

## 3 Preliminaries

**Electronic Health Records (EHRs).** Clinical records, integral for encoding patient health information, are commonly digitized into electronic health records (EHRs) and formatted as high-dimensional medical codes. Typically, a clinical record includes a series of clinical entities, such as diagnoses, medications, procedures, laboratory tests, and clinical notes. In this paper, our primary focus is on the phenotypic information within EHRs, which is generally encoded as one-hot vectors, thus indicating the presence or absence of specific medical symptoms or diseases.

**Phenotype.** Define the phenotype data in EHRs for a patient cohort as \(=\{_{1},_{2},,_{N}\}\), where \(N\) represents the total number of patients. Each \(_{i}\) encapsulates the phenotypic attributes for patient \(i\), represented by medical codes for symptoms and diseases, denoted as \(_{i}=\{p_{1},p_{2},,p_{|_{i}|}\}\).

**Patient Multi-modal Data with Irregular Missingness.** In biological multi-modal datasets, we represent each patient by a collection of data points from various biological modalities, such as genetics, proteomics, or metabolomics. Let \(Z\) represent the total number of modalities, then the multi-modal dataset for patients can be expressed as \(^{M}=\{_{1}^{M},_{2}^{M},,_{N}^{M}\}\), where \(N\) denotes the number of patients. Given the potential for absent modalities, we define the observed multi-modal data for patient \(i\) as \(_{i}^{M}=\{_{i}^{1},_{i}^{2},,_{ i}^{m}\}\), adhering to the condition \(0 m Z\). We focus on the most relaxed setting where the modality missingness is irregular across patients, i.e., random missingness. This randomness persists through the phases of training, validation, and testing, allowing for the possibility that a patient might lack data for any, or in extreme cases, all modalities.

**Phenotype Imputation.** Phenotype imputation aims to address critical gaps in clinical records, where certain medical symptoms, disease attributes, or outcomes are not documented or are incompletely recorded. Given a patient cohort and the incomplete phenotypic data in a clinical dataset, the problem we focus on aims to impute the other possible phenotypes by leveraging available biological multi-modal data. Let \(\) be the incomplete phenotype data, and \(^{M}\) be the biological multi-modal data with irregular missingness, the objective is to design a model that infers the existence of other possible phenotypes. Thereby, a model \(\) is expected to perform \(=(,^{M};)\) and minimize the discrepancy between the actual phenotype \(}\) and the imputed phenotype \(\). Here \(\) and \(}\) denote one-hot vectors. Given the extensive set of phenotypes, measuring discrepancy through classification is impractical. Therefore, we frame the imputation task as a ranking problem, aiming to position the correct phenotype higher than the incorrect ones.

## 4 The Proposed Method

In this section, we introduce the proposed method MPI. As shown in Figure 2, our proposed model includes three components, i.e., biological data quantization, dual-view graph representation learning, and cross-view contrastive knowledge distillation. Next, we describe each component in detail.

### Biological Data Quantization

The biological state reveals analogous latent factors among patients. Existing approaches primarily use biological data as features and apply traditional machine learning techniques to encode them, yet they often struggle to disentangle the complex, heterogeneous factors inherent in biological data [44; 12]. The learned representation of patients could be non-robust (e.g., prone to overreact to an irrelevant factor) and hardly explainable. To identify the latent biological factors among patients, we propose quantizing the biological data and uncovering the corresponding factors using residual quantization , which employs a multi-level vector quantizer to convert residuals into a series of codes. Specifically, the input \(^{m}\) is initially encoded into a latent representation \(^{m}:=(^{m})\) by an encoder \(\). At the first level (\(d=0\)), the residual is set to \(_{0}:=^{m}\). For each level \(d\), we define a codebook \(C_{d}:=\{_{k}\}_{k=1}^{K}\) with size \(K\). The residual \(_{0}\) is quantized by mapping it to the

Figure 2: An overview of the MPI framework: (1) Residual Quantization quantizes the biological data and uncovers the underlying factors. (2) Biological-view GNN and Phenotype-view GNN are employed to encode the correlation between patients, biological factors, and phenotypes in separate graphs. (3) Cross-view knowledge distillation makes use of learned representations from different views and enhances the imputation.

nearest embedding from the codebook. The index of the closest embedding \(_{c_{0}}\) at \(d=0\), which is \(c_{0}=_{k}\|_{0}-_{k}\|\), represents the zero-th code. For the next level (\(d=1\)), the residual is updated to \(_{1}:=_{0}-_{c_{0}}\). The code for this level is determined by finding the embedding in the first level's codebook that is nearest to \(_{1}\). This quantization process is recursively repeated \(l\) times, producing a tuple of \(l\) codes that constitute the disentangled biological factors. This hierarchical approach approximates the input biological data from coarse to fine granularity. Notably, separate codebooks are used for each of the \(l\) levels rather than a single, large codebook. This strategy is preferred as the norm of residuals tends to decrease with increasing levels, facilitating the capture of different granularity levels from the input data.

Upon obtaining the disentangled biological factors \((c_{0},,c_{l-1})\), the quantized representation of \(^{m}\) is determined as \(}^{m}:=_{d=0}^{l-1}_{c_{d}}\). This quantized vector \(}^{m}\) is subsequently fed into a decoder \(\), which attempts to reconstruct the input \(^{m}\) based on \(}^{m}=(}^{m})\). The loss function for the residual quantization is defined as follows:

\[_{}:=_{}+_{},\] (1)

where \(_{}:=\|^{m}-}^{m}\|^{2}\) and \(_{}:=_{i=0}^{l-1}\|[_{i}]-_{c_{i}}\|^{2}+\|_{i}-[_{c_{i}}]\|^{2}.\) Here, \(}^{m}\) represents the decoder's output, and \(\) denotes the stop-gradient operation . The training of this autoencoder involves simultaneous updating of the quantization codebook and the parameters of the encoder-decoder. Note that the exclusive autoencoder and quantization codebooks are learned to capture the disentangled biological factors for each modality. For example, a patient's biological data includes two types of modalities, the disentangled biological factors can be represented as \((c_{0}^{1},,c_{l-1}^{1})\) and \((c_{0}^{2},,c_{l-1}^{2})\). We use \(\) to denote the set of learned biological factors in all codebooks in subsequent sections.

### Dual-view Graph Representation Learning

With disentangled biological factors and phenotypes, a patient can be described from two perspectives: a phenotype view and a biological view. To effectively capture the relationship between patients and biological factors and phenotypes, and fully utilize the information from both views, we construct two separate graphs instead of a single patient-centric graph.

**Patient-Phenotype Graph Construction**. From the phenotype view, we construct a patient-phenotype graph, denoted as \(_{p}\), to depict the collaborative relationships between phenotypes, specifically focusing on phenotype-phenotype co-occurrences. The construction of \(_{p}\) begins with defining a set of phenotypes \(\) and a set of patients \(\). Each patient \(\) is associated with one or more phenotypes \(p\). An edge is created between a patient node and a phenotype node if the patient exhibits that phenotype. By linking patients to their respective phenotypes, \(_{p}\) captures the complex interactions and shared occurrences of different phenotypes across the patient cohort, and provides a comprehensive view of how different phenotypes interact within the patient population.

**Patient-Factor Graph Construction**. From the biological view, we first construct a patient-factor graph, denoted as \(_{f}\), to explore the biology-level correlation between patients. Specifically, the graph \(_{f}\) is constructed using the same set of patients \(\) and disentangled biological factors \(\) from learned codebooks as the set of nodes. To connect patients and factors, we build edges between each patient \(\) and their corresponding factors \((c_{0},,c_{l-1})\). This patient-factor graph \(_{f}\) reveals patient correlations through shared factors, offering a distinct approach to characterizing patients.

With the constructed graphs \(_{f}\) and \(_{p}\), we denote the adjacency matrices of \(_{f}\) and \(_{p}\) as \(_{f}\) and \(_{p}\), respectively. To capture the structural information of the graphs \(_{f}\) and \(_{p}\) and learn the representation of patients, phenotypes, and biological factors, we utilize basic Graph Convolutional Networks (GCNs) as the graph encoder. Taking \(_{p}\) as an example, the phenotype-view graph encoder for \(_{p}\) works by:

\[_{p}^{(l+1)}=(}_{p}_{p}^{(l)} _{p}^{(l)}),\] (2)

where \(_{p}^{(0)}=_{p}\) represents the initial input features, to be more specific, for patients and phenotypes, the input features are randomly initialized. In contrast, for biological factors, the input features are initialized using the corresponding code embedding of factors. And \(_{p}^{(l)}\) denotes the node representations at the \(l\)-th layer. The matrix \(}_{p}=}_{p}^{-1/2}}_{p}}_{p}^{-1/2}\) is the symmetrically normalized adjacency matrix, with \(}_{p}^{N N}\) being the degree matrix of \(}_{p}=_{p}+_{N}\), where \(_{N}\) is the identity matrix. Similarly, the representation \(_{f}^{(l)}\) can be learned from the graph \(_{f}\) using the biological-view graph encoder.

To optimize both graph encoders and to effectively differentiate between the positive and negative edges in graphs, we define a margin-based ranking loss for graph \(_{p}\) as follows:

\[_{p}=_{(i,j)_{p}}_{(i,k)_{p}} (0,-f(i,j)+f(i,k)),\] (3)

where \(\) is the margin hyperparameter, \((i,j)_{p}\) denotes the set of positive edges in graph \(_{p}\), and \((i,k)_{p}\) denotes the set of negative edges and \((i,k)\) does not present in \(_{p}\). \(f(,)\) is a multi-layer perceptron (MLP) that takes node embeddings as inputs and outputs the similarity score between two node embeddings. We use the same loss function to update the biological-view graph encoder of graph \(_{f}\) and denote the loss as \(_{f}\).

### Cross-view Contrastive Knowledge Distillation

Due to the noisy and irrelevant information in the biological data that could mislead the phenotype imputation, the learning from the biological view and the learning from the phenotype view are separative and we propose a cross-view contrastive knowledge distillation strategy to subtly leverage the biological knowledge to facilitate the phenotype imputation. Following the teacher-student framework [19; 8; 39], we regard the biological-view graph encoder as the teacher model and the phenotype-view graph encoder as the student model. Since the teacher model cannot provide the completely precise knowledge to represent patients , instead of fully imitating the behavior of the teacher model, the student model is expected to extract the beneficial knowledge only incorporating partial supervision from the teacher model. Specifically, with the patient representation \(_{f}\) learned from biological-view graph \(_{f}\) and patient representation \(_{p}\) learned from the collaborative-view graph \(_{p}\), we propose cross-view contrastive knowledge distillation to distill useful knowledge from the biological-view graph encoder. This approach leverages view-specific embeddings, represented as \(_{f}^{i}\) from the biological view and \(_{p}^{i}\) from the phenotype view for patient \(i\). Our objective is to align these embeddings into a shared space, facilitating discriminative representation learning through contrastive loss. Initially, embeddings are processed through a transformation with hidden layers to project them into the desired space as \(_{f}^{i}=(^{(2)}(^{(1)} _{f}^{i}+^{(1)})+^{(2)})\) where \(^{(1)}\) and \(^{(2)}\) are the trainable weight matrices, \(^{(1)}\) and \(^{(2)}\) are the bias terms, and \(\) represents the ELU activation function. \(_{p}^{i}\) can also be processed using the same transformation.

We then define positive and negative samples to compute the contrastive loss. Embeddings of the same patient form positive samples from two different views, while negative samples consist of embeddings from different patients. Specifically, for a given patient \(i\), the positive sample pair is \((_{f}^{i},_{p}^{i})\), and negative samples include both intra-view and inter-view pairs. The contrastive knowledge distillation loss is formulated as follows:

\[_{}=-_{f}^{i},_{p}^{i })/}}{e^{s(_{f}^{i},_{p}^{i})/}+_{k i}( e^{s(_{f}^{i},_{p}^{i})/})+_{k i}(e^{s( _{f}^{i},_{p}^{i})/})}\] (4)

where \(s(,)\) denotes the cosine similarity, and \(\) is a temperature parameter. This loss function incorporates negative samples from both intra-view and inter-view sources, ensuring a comprehensive learning process. By applying this cross-view contrastive optimization, our model effectively captures the intricate relationships within both the biological and collaborative views, leading to robust representations of the patients. Since the biological knowledge is distilled from the biological-view graph encoder to enhance the phenotype-view graph encoder, the loss function for \(_{p}\) to optimize the phenotype-view graph encoder is updated to \(}_{p}=_{p}+_{}\) where \(\) is a tradeoff parameter.

### Optimization

In optimization, residual quantization involves the pretraining of autoencoders for biological data, and the quantization codebooks using loss function \(_{}\) to learn the disentangled biological factors and their corresponding factor embeddings. Subsequently, we utilize an iterative optimization strategy to optimize the biological-view graph encoder using \(_{f}\) and phenotype-view graph encoder using \(}_{p}\).

Specifically, we leverage the patient representation learned from the biological view as the teacher signal and optimize the phenotype-view graph encoder through contrastive knowledge distillation following loss function \(}_{p}\). The process is iterated until both graph encoders converge. During the evaluation phase, we employ the patient representation learned from the phenotype-view graph encoder and evaluate a positive testing phenotype along with a set of candidate negative phenotypes to assess performance. The pseudocode of MPI training procedure is described in Algorithm 1.

## 5 Real-World Experiments

### Experimental Setup

**Dataset.** We evaluate MPI and baseline approaches using the UK Biobank , a comprehensive biomedical database and research resource collecting extensive biological samples and clinical EHRs. We focus on phenotype imputation for populations suffering from chronic diseases and thus extract a cohort of patients diagnosed with Alzheimer's disease and related dementia. Specifically, we leverage the EHRs from inpatient and primary care to obtain phenotypic data before disease onset after preprocessing and transformation. Besides, we utilize biological data across two modalities: proteomics, measuring levels of roughly 3,000 proteins; and metabolomics, testing around 250 metabolic biomarkers. The biological data is preprocessed following common practice [7; 55]. We observe significant modality missingness at random: approximately 90% in proteomics and 50% in metabolomics. Table 1 shows the statistics of the dataset, with dataset details and preprocessing methods described in the Appendix A.1.

**Baselines.** We compare the proposed model to baselines across three categories: (1) modality imputation methods, including **CMAE** and **SMIL**; (2) graph neural networks comprising **GraphSage** and **GIN**, which utilize multi-modal biological information as patient features; (3) multi-modal models on EHRs that handle missingness, consisting of **M3Care**, **GRAPE** and **MUSE**. Note that all these methods primarily focus on patient classification tasks and rely on supervision signals from patient labels. We adapt their training objectives to suit our problem setting and evaluate the baselines on the same testing data for a fair comparison. Additional details on the baselines are provided in Appendix A.2.

**Experimental Settings.** We implement MPI with PyTorch and run it on an NVIDIA RTX A6000 GPU. To implement MPI, a two-layer GCN is utilized for each decomposed view with 128 and 64 hidden units respectively. It's worth noting that our focus is not on the complexity of the GNN itself; we use GCN as the foundational backbone model, which can be substituted with any advanced GNNs as needed. Besides, the quantization of proteomics and metabolomics is conducted with respective autoencoders including a two-layer encoder and one-layer decoder, with a hidden size of 32 units. To determine the trade-off weight for knowledge distillation, we choose 0.1 after a grid search in {0.01, 0.1, 1, 5, 10}. The margin hyperparameter \(\) is determined as 3 through a search in {1, 3, 5,10}. The model is trained with Adam optimizer and evaluated at every epoch with an early-stopping strategy at patience of 40 per the validation set performance. Baselines including GraphSage and GIN utilize the same hidden sizes as MPI. CMAE and SMIL first conduct feature imputation for the missing modalities, afterwards an MLP model is conducted with the imputed features for our ranking objectives. As M3Care, Grape, and MUSE build graphs for patients and EHR modalities, we use their published implementations and conduct adaptations to suit our problem setting. Thus, we build the connections between patients and multi-modal modalities and meanwhile incorporate patient phenotype connections for a fair comparison. Baseline hyperparameters are determined by parameter search. Besides, the model learning rate is selected from {0.01, 0.001, 0.0005} for MPI and all baseline models.

  
**Dataset** & **Unique Items\#** & **Interactions\#** & **Sparsity/Missing Rates** \\ 
**Patient** & 15,093 & - & - \\
**Phenotype** & 1,109 & 380,239 & 97.73\% \\
**Proteomics** & 2,923 & 1,483 & 90.2\% \\
**Metabolomics** & 251 & 7,513 & 50.3\% \\   

Table 1: Dataset Statistics

**Evaluation Protocol.** The discussion on the evaluation protocol can be found in the Appendix A.3.

### Experimental Results

Performance Comparison.Table 2 presents the performance of the MPI and baseline models trained with different proportions of the dataset. The best results are highlighted in **bold**, while the top baseline scores are underlined. The baselines based on imputation, including CMAE and SMIL, exhibit inferior performance. We attribute this to their reliance on modeling transformations from the hidden space to reconstruct the input features. The imputed data can be inaccurate due to the high dimensionality of the multi-modal data and the severity of missingness. GraphSage and GIN achieve competitive performance compared to both imputation-based models and the multi-modal learning approaches that explicitly handle missing data. The graph-based multi-modal models outperform GNNs in some cases; however, they are sometimes inferior to applying naive integration of clinical and biological data in naive GNNs. This may be due to the complexity and conflict between clinical and biological views. For example, GRAPE, which uses each feature dimension as a node, is not suitable for high-dimensional feature imputation. Additionally, M3Care computes patient similarity for each modality separately, thereby failing to explore cross-modality correlations. MUSE connects patients with modalities while representing each modality type as a node, possibly introducing dense and noisy edges. In contrast, MPI demonstrates improvements across all settings, verifying its capability to handle heterogeneity and noise through a decoupled view.

**Ablation Study.** To validate the effectiveness of MPI and gain deeper insight into the contributions of each component in the proposed approach, we conduct ablation studies by comparing the following variants with the original MPI: (1) V1, which does not utilize the biological data and only model the correlation of patients and phenotypes. (2) V2, which only uses proteomics data and contrastive knowledge distillation. (3) V3, which solely leverages metabolomics data and contrastive knowledge

    & **Metric** & **CMAE** & **SMIL** & **GraphSage** & **GIN** & **GRAPE** & **M3Care** & **MUSE** & **MPI** \\   & H@10 & \(25.81^{ 0.14}\) & \(26.12^{ 0.25}\) & \(24.96^{ 0.77}\) & \(25.36^{ 0.66}\) & \(25.60^{ 0.64}\) & \(26.23^{ 0.56}\) & \(24.24^{ 0.32}\) & \(}\) \\  & H@20 & \(41.66^{ 0.42}\) & \(41.08^{ 0.53}\) & \(40.61^{ 0.47}\) & \(41.51^{ 0.84}\) & \(41.41^{ 0.73}\) & \(41.90^{ 0.65}\) & \(40.89^{ 0.58}\) & \(}\) \\  & H@50 & \(68.81^{ 0.16}\) & \(66.23^{ 0.21}\) & \(67.28^{ 0.20}\) & \(69.02^{ 0.68}\) & \(68.45^{ 0.25}\) & \(68.71^{ 0.36}\) & \(67.90^{ 0.24}\) & \(}\) \\  & MRR & \(11.51^{ 0.13}\) & \(11.46^{ 0.32}\) & \(11.23^{ 0.52}\) & \(11.50^{ 0.30}\) & \(11.33^{ 0.27}\) & \(11.87^{ 0.25}\) & \(11.06^{ 0.35}\) & \(}\) \\   & H@10 & \(26.33^{ 0.28}\) & \(26.57^{ 0.28}\) & \(28.59^{ 0.23}\) & \(29.35^{ 0.39}\) & \(28.83^{ 0.47}\) & \(27.43^{ 0.32}\) & \(28.86^{ 0.43}\) & \(}\) \\  & H@20 & \(42.34^{ 0.35}\) & \(42.68^{ 0.42}\) & \(44.51^{ 0.40}\) & \(45.58^{ 0.47}\) & \(45.20^{ 0.38}\) & \(44.66^{ 0.26}\) & \(44.87^{ 0.36}\) & \(}\) \\  & H@50 & \(69.28^{ 0.51}\) & \(69.35^{ 0.22}\) & \(70.92^{ 0.20}\) & \(71.82^{ 0.34}\) & \(70.74^{ 0.34}\) & \(70.28^{ 0.51}\) & \(70.77^{ 0.40}\) & \(}\) \\  & MRR & \(11.99^{ 0.04}\) & \(12.09^{ 0.19}\) & \(13.30^{ 0.23}\) & \(13.77^{ 0.28}\) & \(13.14^{ 0.29}\) & \(12.64^{ 0.21}\) & \(13.41^{ 0.31}\) & \(}\) \\   & H@10 & \(27.40^{ 0.55}\) & \(28.24^{ 0.35}\) & \(32.35^{ 0.18}\) & \(33.13^{ 0.41}\) & \(30.51^{ 0.63}\) & \(30.68^{ 0.49}\) & \(32.42^{ 0.73}\) & \(}\) \\  & H@20 & \(43.50^{ 0.37}\) & \(44.54^{ 0.31}\) & \(48.12^{ 0.25}\) & \(49.12^{ 0.35}\) & \(47.07^{ 0.59}\) & \(46.53^{ 0.35}\) & \(48.38^{ 0.59}\) & \(}\) \\  & H@50 & \(69.93^{ 0.31}\) & \(70.28^{ 0.26}\) & \(73.10^{ 0.31}\) & \(73.18^{ 0.40}\) & \(72.64^{ 0.53}\) & \(71.73^{ 0.47}\) & \(73.04^{ 0.61}\) & \(}\) \\  & MRR & \(12.48^{ 0.28}\) & \(13.36^{ 0.18}\) & \(15.59^{ 0.36}\) & \(15.75^{ 0.22}\) & \(14.04^{ 0.45}\) & \(13.75^{ 0.26}\) & \(15.19^{ 0.58}\) & \(}\) \\   & H@10 & \(27.90^{ 0.26}\) & \(29.03^{ 0.27}\) & \(35.48^{ 0.30}\) & \(35.41^{ 0.35}\) & \(31.61^{ 0.25}\) & \(32.55^{ 0.33}\) & \(33.73^{ 0.34}\) & \(}\) \\  & H@20 & \(44.14^{ 0.31}\) & \(46.15^{ 0.42}\) & \(51.47^{ 0.33}\) & \(51.36^{ 0.25}\) & \(48.86^{ 0.46}\) & \(48.24^{ 0.59}\) & \(48.94^{ 0.37}\) & \(}\) \\  & H@50 & \(70.42^{ 0.13}\) & \(71.58^{ 0.18}\) & \(75.54^{ 0.15}\) & \(74.95^{ 0.35}\) & \(74.30^{ 0.27}\) & \(73.8distillation. (4) V4, which organizes biological factors, patients, and phenotypes in a single graph and does not require contrastive knowledge distillation. The results on 30% and 100% of the UK biobank dataset are summarized in Table 3. First, we observe that variant V1 is outperformed by both V2 and V3. This performance disparity arises since V2 and V3 effectively model the biological data and distill beneficial knowledge, thus enhancing phenotype imputation through knowledge distillation. Second, V4 is inferior to the proposed model MPI. This demonstrates that modeling biological data and phenotype data in separate graphs yields better performance compared to a single graph model. The likely reason for this is that multi-modal biological data often contain measurement inaccuracies and irrelevant information, which can impede accurate phenotype imputation Third, we observe that V3 exhibits superior performance compared to V2. We attribute this to the higher sparsity ratio of proteomics data relative to metabolomics data. The severe missing data issue in proteomics likely affects the performance of imputation. Lastly, compared to all variants, MPI demonstrates the best performance, highlighting the effectiveness of the proposed method.

**The Impact of Codebook Settings.** To analyze the impact of codebook settings on imputation performance, we varied the number and sizes of the codebooks and the results for the entire dataset are presented in Figure 3. First, as shown in Figure 3(left), MPI achieves optimal performance with three codebooks. A smaller number of codebooks, such as one or two, may fail to capture sufficient fine-grained information from the biological data.

Conversely, larger codebooks might introduce additional underlying factors due to finer granularity, which could reduce their discriminative power for patient profiling. Second, Figure 3(right) illustrates that the performance of MPI varies with changes in codebook sizes. The optimal codebook sizes for proteomics and metabolomics are 64 and 96, respectively. Smaller codebook sizes may fail to capture underlying biological factors, resulting in insufficient information for patient profiling. Conversely, larger codebook sizes might lead to certain codes being underutilized, which can hinder the overall optimization of the codebook.

**Sensitivity to Tradeoff Parameter.** Figure 4 illustrates the impact of varying tradeoff parameters on the performance of MPI, evaluated on 30% and 100% of the dataset. The tradeoff parameter mediates between the contrastive knowledge distillation loss and the graph representation loss. The results indicate that MPI achieves optimal performance with a tradeoff parameter of 0.01. Notably, when the tradeoff parameter is set to 0, the imputation performance largely declines. This is due to the disabling of knowledge distillation, which prevents the model from leveraging biological knowledge. Conversely, as the tradeoff parameter increases to a high value, the model's performance diminishes. The model might overly depends on biological knowledge and neglects the information from the collaborative view, leading to suboptimal outcomes.

## 6 Conclusion

In conclusion, this work introduces a novel framework that leverages multi-modal data to enhance phenotype imputation, aiming for a more comprehensive medical evaluation. The proposed approach involves uncovering latent biological factors to enhance patient profiling and modeling correlations based on these factors. To mitigate the impact of noise and irrelevant information in biological data, we employ a cross-view contrastive knowledge distillation technique. Extensive experiments on a large-scale biomedical database demonstrate that our proposed method outperforms existing state-of-the-art approaches, showcasing its effectiveness and potential for improving biomedical data analysis and patient care.

Figure 4: Effect of tradeoff parameter for MPI on 30% (left) and 100% (right) of the dataset.

Figure 3: (Left) Results for varying the number of codebooks while keeping the codebook size fixed. (Right) Performance variation with changes in the codebook sizes while keeping a fixed number of codebooks.