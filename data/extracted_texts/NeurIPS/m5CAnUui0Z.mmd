# [MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:1]

patient data might follow trends that do not exist in the training data yet. This leads to a repeating cycle: collecting data from various patients, assessing the trends, labeling the data, training the model, and then deploying it on new patients. The longer this cycle takes, the more likely it is going to affect the model's reliability, a challenge we refer to as **label delay**.

In this paper, we propose a CL setting that explicitly accounts for the delay between the arrival of new data and the corresponding labels, illustrated by Figure 1. In our proposed setting, the model is trained continually over discrete time steps with a label delay of \(d\) steps. At each step, two batches of data are revealed to the model: unlabeled new samples from the current time step \(t\), and the labels of the samples revealed at the step \(t-d\). First, we show the naive approach where the model is only trained with the labeled data while ignoring all unlabeled data. While this forms a strong baseline, its performance suffers significantly from increasing the delay \(d\). We find that simply increasing the number of parameter updates per time step does not resolve the problem. Hence, we examine a number of popular approaches which incorporate the unlabeled data to improve this naive baseline. We investigate semi-supervised learning, self-supervised learning and test-time adaptation approaches which are motivated for slightly different but largely similar settings. Surprisingly, out of 12 different methods considered, none could outperform the naive baseline given the same computational budget. Motivated by our extensive empirical analysis of prior art in this new setting, we propose a simple and efficient method that outperforms every other approach across large-scale datasets; in some scenarios it even closes the accuracy gap caused by the label delay. Our contributions are threefold:

* We propose a new formal Continual Learning setting that factors label delay between the arrival of new data and the corresponding labels due to the latency of the annotation process.
* We conduct extensive experiments (\( 25,000\) GPU hours) on various Online Continual Learning datasets, such as CLOC , CGLM , FMoW  and Yearbook . Following recent prior art on Budgeted Continual Learning [8; 9], we compare the best performing Self-Supervised Learning , Semi-Supervised Learning  and Test Time Adaptation  methods and find that none of them outperforms the naive baseline that simply ignores the label delay and trains a model on the delayed labeled stream.
* We propose **I**mportance **W**eighted **M**emory **S**ampling to rehearse past labeled data most similar to the most recent unlabeled data, bridging the gap in performance. IWMS outperforms the naive method significantly and improves over Semi-Supervised, Self-Supervised Learning and Test-Time Adaptation methods across diverse delay and computational budget scenarios with a negligible increase in computational complexity. We further present an in-depth analysis of the proposed method.

Figure 1: **Illustration of label delay. This figure shows a typical Continual Learning (CL) setup with label delay due to annotation. At every time step \(t\), the data stream \(_{}\) reveals a batch of unlabeled data \(\{x^{t}\}\), on which the model \(f_{}\) is evaluated (highlighted with green borders). The data is then sent to the annotator \(_{}\) who takes \(d\) time steps to provide the corresponding labels. Consequently, at time step \(t\) the batch of labels \(\{y^{t-d}\}\) corresponding to the input data from \(d\) time steps before becomes available. The CL model can be trained using the **delayed labeled data** (shown in color) and the **newest unlabeled data** (shown in grayscale). In this example, the stream reveals three samples at each time step and the annotation delay is \(d=2\).**

## 2 Related Work

**Label Delay in Online Learning.** While the problem of delayed feedback has been studied in the online learning literature [13; 14], the scope is limited to problems of spam detection and other synthetically generated, low-complexity data [15; 11] and often views input images as "side info". Additionally, methods and error bounds proposed in[17; 18; 19; 20] are more focused on expert selection rather than representation learning, most of which cannot generalize to unstructured, large-scale image classification datasets.

**Continual Learning.** Early work on continual learning primarily revolved around task-based continual learning [21; 22], while recent work focuses on the task-free continual learning setting[23; 24; 4]. This scenario poses a challenge for models to adapt as explicit task boundaries are absent, and data distributions evolve over time. GDumb and BudgetCL demonstrate that minimalistic methods can outperform most offline and online continual learning approaches. RealtimeOCL  shows that Experience Replay  is the most effective method, outperforming more popular continual learning methods, such as ACE , LwF , RWalk , PoLRS , MIR  and GSS , when methods are normalized by their computational complexities.

**Semi-Supervised Learning.** While the labels arrive delayed, our setting allows the models to use new unlabeled data immediately. Possible directions to leverage the most recent unlabeled data entail Pseudo-Labeling (or often referred to as their broader category, Semi-Supervised Learning) methods  and Self-Supervised Semi-Supervised Learning (S4L) methods . Pseudo-labeling techniques predict the labels of the samples before their true labels become available to estimate the current state of the joint distribution of input and output pairs. This in turn allows the model to fit its parameters on the estimated data distribution. On the other hand, S4L integrates self-supervised learning, such as predicting the rotation of an image or the relative location of image patches, with the semi-supervised learning framework. We replace the early self-supervised tasks of S4L  with more recent objectives from Balestriero et al. .

**Test-Time Adaptation.** Besides semi-supervised learning, TTA methods are also designed to adapt models with unlabeled data, sampled from a similar distribution as the evaluation samples. Entropy regularization methods like SHOT  and TENT  update the feature extractor or learnable parameters of the batch-normalization layers  to minimize the entropy of the predictions. SAR  incorporates an active sampling scheme to filter samples with noisy gradients. More recent works consider Test Time Adaptation in an online setting  or Continual Learning setting . In our experiments, we fine-tune the model with ER  across time steps and adapt a copy of the model with TTA to the most recent input samples at each time step.

## 3 Problem Formulation

We follow the conventional online continual learning problem definition proposed by Cai et al. . In such a setting, we seek to learn a model \(f_{}:\) on a stream \(\) where for each time step \(t\{1,2,\}\) the stream \(\) reveals data from a time-varying distribution \(_{t}\) sequentially in batches of size \(n\). At every time step, \(f_{}\) is required to predict the labels of the coming batch \(\{x_{i}^{t}\}_{i=1}^{n}\) first. Followed by this, the corresponding labels \(\{y_{i}^{t}\}_{i=1}^{n}\) are immediately revealed by the stream. Finally, the model is updated using the most recent training data \(\{(x_{i}^{t},y_{i}^{t})\}_{i=1}^{n}\).

This setting, however, assumes that the annotation process is instantaneous, i.e., the time it takes to provide the ground truth for the input samples is negligible. In practice, this assumption rarely holds. It is often the case that the rate at which data is revealed from the stream \(\) is faster than the rate atwhich labels for the unlabeled data can be collected, as opposed to it being instantaneously revealed. To account for this delay in accumulating the labels, we propose a setting that accommodates this lag in label availability while still allowing for the model to be updated with the most recent unlabeled data. We modify the previous setting in which labels of the data revealed at time step \(t\) will only be revealed after \(d\) time steps in the future.

At every time step \(t\), the Annotator \(_{}^{d}\) reveals the labels for the samples from \(d\) time steps before, i.e., \(\{(x_{i}^{t-d},y_{i}^{t-d})\}_{i=1}^{n}\), while the data stream \(_{}\) reveals data from the the current time step, i.e., \(\{x_{i}^{t}\}_{i=1}^{n}\). Recent prior art [25; 8; 9] introduces more reasonable and realistic comparisons between continual learning methods by imposing a computational complexity constraint on the methods. Similarly to [25; 8; 9], in our experiments the models are given a fixed computational budget \(\) to update the model parameters from \(_{t}\) to \(_{t+1}\) for every time step \(t\). To that end, our new proposed setting can be formalized per time step \(t\), alternatively to the classical OCL setting, as described in Algorithm 1.

Note that this means at each time step \(t\), the stream reveals a batch of _non-corresponding_ images \(\{x_{i}^{t}\}_{i=1}^{n}\) and labels \(\{y_{i}^{t-d}\}_{i=1}^{n}\), as illustrated in Figure 1. With the label delay of \(d\) time steps, the images themselves revealed from time step \(t-d\) to time step \(t\) can be used for training, despite that labels are not available.

A _naive_ way to solve this problem is to discard the unlabeled images and only train on labeled data \(_{=1}^{t-d}\{(x_{i}^{},y_{i}^{})\}_{i=1}^{n}\), However, it worth noting that the model is still evaluated on the most recent samples from \(_{}\). Thus, training on the labeled training data leads to the model at least being \(d\) steps delayed. Since in our setting the distribution from which the training and evaluation samples are drawn from is not stationary, this discrepancy severely hinders the performance, as discussed in detail in Section 5.

Furthermore, we shall show in Section 6 that the existing paradigms, such as Test-Time Adaptation and Semi-Supervised Learning, struggle to effectively utilise newer, unlabeled data to bridge the aforementioned discrepancy. Our observations indicate that the primary failure is from the excessive computational demands of processing unlabeled data. To that end, we propose Importance Weighted Memory Sampling that prioritises performing gradient steps on labeled samples that resemble the most recent unlabeled samples.

## 4 IWMS: Importance Weighted Memory Sampling

To mitigate the challenges posed by label delay in online continual learning, we introduce a novel method named **Importance Weighted Memory Sampling (IWMS)**. Recognizing the limitation of traditional approaches that either discard unlabeled data or utilize it in computationally expensive ways, IWMS aims to bridge the gap between the current distribution of unlabeled data and the historical distribution of labeled data. Instead of directly adapting the model to fit the newest distribution with unlabeled data, which is inefficient due to the lack of corresponding labels, IWMS cleverly adjusts the sampling process from a memory buffer. This method ensures that the distribution of selected samples closely matches the distribution of the most recent unlabeled batch. This nuanced selection strategy allows the continual learning model to effectively adapt to the most recent data trends, despite the delay in label availability, by leveraging the rich information embedded in the memory buffer.

As discussed in Section 5, using the most recent labeled samples for training leads to over-fitting the model to an outdated distribution. Thus, we replace the newest supervised data by a batch which we sample from the memory buffer, such that the distribution of the selected samples matches the newest unlabeled data distribution. The sampling process is detailed in Algorithm 2. It consists of two stages: first, at each time step \(t\), for every unsupervised sample \(x_{i}^{t}\) in the batch of size \(n\), we compute the prediction \(_{i}^{t}\), and select every labeled sample from the memory buffer \((x_{j}^{M},y_{j}^{M})\) such that the true label of the selected samples matches the predicted label \(y_{j}^{M}=_{i}^{t}\). In the second stage, we compute the pairwise cosine feature similarities \(_{i,j}\) between the unlabeled sample \(x_{i}^{t}\) and the selected memory samples \(x_{j}^{M}\) by \(_{i,j}=(h(x_{i}^{t}),h(x_{j}^{M}))\), where \(h\) represents the learned feature extractor part of \(f_{}\), directly before the final classification layer. Finally, we select the most relevant supervised samples \((x_{k}^{M},y_{k}^{M})\) by sampling \(k\{1|M|\}\) from a multinomial distribution with parameters \(_{,j}\). Thus, we rehearse samples from the memory which (1) share the same true labels as the predicted labels of the unlabeled samples, (2) have high feature similarity with the unlabeled samples.

To avoid re-computing the feature representation \(h(x^{M})\) for each sample in the memory buffer after every parameter update, we store the corresponding features of the input data computed for the predictions during the evaluation (Step 4 in Algorithm 1). This technique greatly reduces the computational cost of our method, but comes at the price of using outdated features. Such trade-off is studied in detail by contemporary Self-Supervised Literature [38; 39; 40] observing no significant impact on performance. We ablate the alternative option of selecting samples based only on their similarity in the Supplementary Material A.11.

## 5 The Cost of Ignoring Label Delay

To better understand how label delay influences the performance of a model, we begin with the **Naive** approach, i.e., ignoring the most recent data points until their label becomes available and exclusively training on outdated labeled samples. More specifically, we are interested in measuring the performance degradation under various label delay \(d\) and computational budget \(\) scenarios. To this end, we conduct experiments over 4 datasets, in 4 computational budget and 3 label delay settings. We analyse the results under normalised computational budget (Section 5.2) and demonstrate that the accuracy drop can be only partially recovered by increasing the computational budget (Section A.5).

### Experimental Setup

**Datasets.** We conduct our experiments on four large-scale online continual learning datasets, Continual Localization (CLOC) , Continual Google Landmarks (CGLM) , Functional Map of the World (FMoW) , and Yearbook . The last two are adapted from the Wild-Time challenge . More statistics of the benchmarks are in Supplementary. We follow the same _training_ and _validation_ set split of CLOC as in  and o CGLM as in  and the official released splits for FMoW  and Yearbook .

**Architecture and Optimization.** Similarly to prior work [9; 8], we use ResNet18  for backbone architecture. Furthermore, in our experiments, the stream reveals a mini-batch, with the size of \(n=128\) for CLOC, FMoW, Yearbook and \(n=64\) for CGLM. We use SGD with the learning rate of

Figure 2: **Effects of Varying Label Delay. The performance of a _Naive_ Online Continual Learner model gradually degrades with increasing values of delay \(d\).**

\(0.005\), momentum of \(0.9\), and weight decay of \(10^{-5}\). We apply random cropping and resizing to the images, such that the resulting input has a resolution of \(224 224\).

**Baseline Method** In our experiments, we refer to the _Naive_ method as the one naively training one labeled data. We apply the _state of the art_ continual learning mechanism under computational constrains , Experience Replay (ER) , to eliminate the need to compare with other continual learning methods. The memory buffer size is consistently \(2^{19}\) samples throughout our experiments unless stated otherwise. The First-In-First-Out mechanism [26; 4] to update the buffer. The discussion of the effectiveness of the memory buffer can be found in Section A.18. We report the Online Accuracy  at each time step in Step 4 of Algorithm 1 under label delay \(d\). In our quantitative comparative analysis, for simplicity, we use the final Online Accuracy scores, denoted by \(_{d}\). Refer to Section A.17 for the discussion of more metrics.

**Computational Budget and Label Delay.** Normalising the computational budget is necessary for fair comparison across CL methods, thus, we define \(=1\) as the number of FLOPs required to make one backward pass with a ResNet18 , similarly to BudgetCL and RealtimeOCL. We discuss its relation to the wall-clock training time in Section A.16. When performing experiments with a larger computational budget, we take integer multiplies of \(\) to apply \(\) parameter update steps per stream time steps. The proposed label delay factor \(d\) represents the amount of time steps the labels are delayed with. Note that, for \(=1,d=0\), our experimental setting is identical to prior art[4; 9].

### Observations

In Figure 2, we analyze how varying the label delay \(d\{0,10,50,100\}\) impacts the performance of Naive on four different datasets, CLOC , CGLM , FMoW  and Yearbook . The label delay impacts the online accuracy differently across all scenarios, thus, below we provide our observations case-by-case.

On **CLOC**, the non-delayed (\(d=0\)) Naive achieves \(_{0}=20.2\%\), whereas the heavily delayed counterpart (\(d=100\)) suffers significantly from the label delay, achieving only \(_{100}=11.7\%\). Interestingly, label delay influences the accuracy in a monotonous, but non-linear fashion, as half of the accuracy drop is caused by a very small amount of delay: \(_{10}-_{0}=-4.4\%\). In contrast, the accuracy degradation slows down for larger delays,i.e., the accuracy gap between two larger delay scenarios (\(d=50 100\)) is rather marginal \(_{100}-_{50}=-1.1\%\). We provide further evidence on the monotonous and smooth properties of the impact of label delay with smaller increments of \(d\) in the Supplementary Material A.3.

For **CGLM** the accuracy gap landscape looks different: the majority of the accuracy decrease occurs by the smallest delay \(d=0 10\), resulting in a \(_{10}-_{0}=-7.9\%\) drop. Subsequent increases (\(d=10 50\) and \(d=50 100\)) impact the performance to a significantly smaller extent: \(_{50}-_{10}=-1\%\) and \(_{100}-_{50}=-0.5\%\).

In the case of **FMoW**, where the distribution shift is less imminent (i.e., the data distribution varies less over time), the difference between the delayed and the non-delayed counterparts should be small. This is the case for the satellite image data in the FMoW dataset, where the accuracy drops are \(-2.8\%,-2\%,-1.9\%\) for \(d=0 10 50 100\), respectively.

The **Yearbook's** binary classification experiments highlight an important characteristic: if there is a significant event that massively changes the data distribution, such as the change of men's appearance in the 70's  the non-delayed Naive (\(d=0\)) suffers a small drop in Online Accuracy (at the middle of the time horizon \(t=130\)), but quickly recovers as more data starts to appear. In contrast, under small and moderate delay (\(d=10,50\)), the decline is more emphasised and the recovery is delayed (at \(t=120,180\), respectively). Alongside with more detailed investigation, we provide visual examples of the dataset to support our claims in the Supplementary Material A.4.

### Section Conclusion

**Over- or Under-fitting.** While in the experiments we report results under a single computational budget \(\) per dataset, it is reasonable to suspect that the results might look different under smaller or larger budget. To this end, we ablate the effect of \(\) over various delay scenarios, on multiple datasets in the Supplementary Material A.5.

**Common patterns.** We argue that the consistent, monotonic accuracy degradation, present in all of our experiments, is due to the non-stationary property of the data distribution that creates a distribution shift. Our hypothesis is supported by the findings of Yao et al. . A complementary argument is presented by Hammoud et al. , stating that the underlying datasets have high temporal correlations across the labels, i.e., images of the same categories arrive in bursts, allowing an online learning model to easily over-fit the label distribution even without using the input images.

**Motivation for Delay Specific Solutions.** As our experiments suggest so far, label delay is indeed an extremely elusive problem, not only because it inevitably results in an accuracy drop, but because the severity of the drop itself is hard to estimate a-priori. We showed that the accuracy gap always increases monotonically with increasing delay, nevertheless the increase of the gap can be gradual or sudden depending on the dataset and the computation budget. This motivates our efforts of designing special techniques to address the challenges of label delay. In the next set of experiments, we augment the Naive training by utilizing the input images _before_ their corresponding labels become available.

## 6 Utilising Data Prior to Label Arrival

In our proposed label delay experimental setting, we showed the larger the delay the more challenging it is for Naive, a method that relies only on older labeled data, to effectively classify new samples. This is due to a larger gap in distribution between the samples used for training and for evaluation. This begs the question of whether the new unlabeled data can be used for training to improve over Naive, as it is much more similar to the data that the model is evaluated on.

We propose four different paradigms for utilizing the unlabeled data, namely, Importance Weighted Memory Sampling (**IWMS**), Semi-Supervised Learning via Pseudo-Labeling (**PL**), Self-Supervised Semi-Supervised Learning (**S4L**) and Test-Time Adaptation (**TTA**). We integrate several methods of each family into our setting and evaluate them under various delays and computational budgets. In particular, we adapt each paradigm individually by augmenting the parameter update (Step 5 of

Figure 3: **Comparison of various unsupervised methods. The accuracy gap caused by the label delay between the _Naive without delay_ and its delayed counterpart _Naive_. Our proposed method, _IWMS_, consistently outperforms all categories under all delay settings on three out of four datasets.**Algorithm 1) of Naive, described in detail in the following subsections. Furthermore, to quantify the how much of the accuracy gap (\(G_{d}=_{d}^{}-_{0}^{}\)) is recovered, we use the formula \(R_{d}^{*}=_{d}^{*}-_{0}^{}}{|G_{d}|}\), namely the improvement of the method divided by the extent of the accuracy gap for a given delay factor \(d\).

### Experiment Setup

**Importance Weighted Memory Sampling (IWMS).** The only additional cost of IWMS compared to Naive is the cost of evaluating the similarity scores, which is still less than \(1\%\) of the inference cost for 100K samples, and can be evaluated in parallel, therefore we consider it negligible. Since our method simply replaces the newest supervised samples with the most similar samples from the replay buffer, we do not require any additional backward passes to compute the auxiliary objective. Therefore, the computational budget of our method is identical to the Naive baseline, i.e.,, \(_{}=1\).

**Self-Supervised Semi-Supervised Learning.** For integrating S4L methods, we adopt the most effective approach through iterative optimization of both supervised and unsupervised losses. We report the best results across the three main families of contrastive losses, i.e.,, Deep Metric Learning Family (MoCo , SimCLR ,and NNCLR ), Self-Distillation (BYOL  and SimSIAM , and DINO ), and Canonical Correlation Analysis (VICReg , BarlowTwins , SWAV , and W-MSE ).

For fair comparison, we normalise the computational complexity of the compared methods. According to [8; 9], Naive augmented with Self-Supervised Learning at each time step takes two backward passes, since they augment each input images to two views, thus \(_{}=2\). We provide further explanation of our S4L adaptation in the Supplementary Material A.2.

**Pseudo-Labeling.** To make use of the newer unlabeled samples, we adopt the most common Semi-Supervised Learning technique : Pseudo-Labeling (PL). To predict the labels of the samples before their true label becomes available we use a surrogate model \(g_{}\). After assigning the predicted labels \(\{_{i}^{t}\}\) to each input data \(\{x_{i}^{t}\}\) at time step \(t\) for \(i=1..n\), the main model \(f_{}\) is updated over the union of old, labeled memory samples and new _pseudo-labeled_ samples \(\{(x_{i}^{},y_{i}^{})\}_{-1}^{t-d}\{(x_{i}^{t},_{i}^{t})\}\) using standard Cross Entropy loss. Once \(f_{}\) is updated, we update the parameters of the surrogate model \(g_{}\) following the momentum update policy  with hyper-parameter \(\), such that \(_{}=_{}+(1-)_{}\).

For simplicity, we ignore the computational cost of the surrogate model \(g_{}\) inferring the pseudo-labels \(\). Nevertheless, the main model \(f_{}\) is trained on double the amount of samples as Naive, \(n\) labeled and \(n\) pseudo-labeled, therefore we define \(_{}=2\).

**Test-Time Adaptation** As done for other paradigms, we have extensively evaluated all reasonable candidates to adapt traditional TTA methods to our setting. We find performing the unsupervised TTA step the most effective when only a single update is taken (in Step 5 of Algorithm 1), exactly before the evaluation step (Step 2 of Algorithm 1) of the next step. Therefore, for all the parameter updates apart from the last one we perform identical steps to Naive Furthermore, we found TTA updates severly impact the continual learning process of the Naive when the parameters are iteratively optimised across the two objectives. Thus, before each TTA step, we clone the model parameters \(\) to a surrogate model \(g_{}\), by performing the TTA step (with \(\) hyper-parameter) using the newest batch of unlabeled data \(=-_{}_{}\{x_{i}^{t}\}\) and perform the evaluation (Step 2 of Algorithm 1) of the next time step.

To represent the state of the art in TTA, we adapt and compare the following methods: TENT , EATA , SAR , and CoTTA , in Figure 3. Furthermore, for the result of our hyper-parameter tuning is provided in the Supplementary Material A.7.

For fair comparison, we train and evaluate all TTA methods under normalised computational budgets (a detailed breakdown of the considerations can be found in the Supplementary Material A.15) More specifically, under a fixed computational budget \(\), at every time step, we perform \(-1\) supervised steps on \(f_{}\) identically to Naive followed by a single step of TTA.

### Observations

Figure 3 illustrates our most important results of our work. It shows to what extent we can recover the accuracy gap caused by the label delay between the _Naive without delay_ and its delayed counterpart _Naive_. We evaluate our proposed method, _IWMS_, and compare it against the three adopted paradigms, _S4L_, _PL_ and _TTA_. We report the best performing method of each paradigm with hyper-parameters tuned on the first 10% of each label delay scenario (further detailed in the Supplementary Material A.6 and 12).

**IWMS.** On the largest dataset, containing 39M samples, **CLOC**, the accuracy drop of Naive is \(G_{d}=-4.5\%,-7.5\%,-8.6\%\) for \(d=10,50,100\), respectively. Our proposed method, IWMS, achieves \(_{d}=17.3\%,14.2\%,13.1\%\) final Online Accuracy, which translates to \(R_{d}=33\%,19\%,16\%\) recovery for \(d=10,50,100\), respectively. While there is a slow decline over increasing delays, the improvement over Naive is consistent. On **CGLM**, the accuracy drop is \(G_{d}=-7.8\%,-8.8\%,-9.3\%\) for the three increasing delays, respectively. IWMS exhibits outstanding results, \(_{d}=24.1\%,23.5\%,22.9\%\) meaning that the accuracy gap _is fully recovered_ by the method for \(d=10\). More specifically, the recovery is \(R_{d}=100\%,93\%,87\%\) for \(d=10,50,100\). The results on **FMoW** are even more surprising, as IWMS not only recovers the accuracy gap but _outperforms_ the non-delayed Naive counterpart in the \(d=10\) scenario. More specifically, the accuracy drops for the increasing delays are \(G_{d}=2.5\%,3.2\%4.4\%\) and \(R_{d}=140\%,67\%,45\%\). We hypothesise this is due to the fact that under a large \(\), repeated parameter updates with sub-optimal sampling strategies lead to over-fitting to the outdated state of the data distribution, as explained in detail in Section 7. On **Yearbook**, IWMS performs on-par with Naive in every scenario. The accuracy gaps are \(G_{d}=-5\%,-20.5\%,-34\%\) whereas the recover scores are marginal: \(R_{d}=1\%,0\%,0\%\). We argue this is due to two factors: the brevity of the dataset in comparison to the other datasets and the difficulty of the task without prior knowledge on appearance and fashion trends.

**Semi-Supervised Methods.** S4L and PL performs very similarly to each other under all studied scenarios: the largest difference in their performance is \(0.7\%\) on Yearbook, under \(d=50\) label delay. Therefore, we report their performance together, picking the better performing variant for numerical comparisons. Notice that in every scenario the delayed Naive baseline performance is not be achieved, which is due to the computational budget constraint. More specifically, since \(_{}=2_{}\), optimising the standard classification objective over the older, supervised samples for twice the number of parameter updates is more beneficial across all scenarios than optimising the Pseudo-Labeling classification objective or the Contrastive loss over the newer unlabeled images. In the Supplementary Material 13, we provide further evidence and explanation of this claim. On **CLOC**, S4L slightly outperforms PL by \(+0.1\%\) for all label scenarios, however \(R_{d}=-27\%,-2\%,-7\%\) for \(d=10,50,100\), respectively. Similarly, on **CGLM**, S4L outperforms PL by \(+0.6\%\), for all label scenarios and achieves a negative recovery score \(R_{d}=-29\%,-27\%,-23\%\). On **FMoW** and **Yearbook**, the differences between the accuracy of Naive, S4L and PL are negligible as the largest improvement over Naive is \(+2.3\%\) on Yearbook under the large label delay scenario \(d=100\).

**TTA.** In Figure 3, we find that TTA consistently under-performs every method, _including_ the delayed Naive, under every delay scenario on the CLOC, CGLM and FMoW datasets Nevertheless, on Yearbook TTA successfully outperforms IWMS, S4L, PL and Naive by up to \(+1.7\%\) in the moderate label delay scenario \(d=50\). Over the four dataset, the exact extent of the recovery of the accuracy gap \(R_{d}\) for \(d=10,50,100\), respectively, is as follows: on **CLOC**\(R_{d}=-87\%,-44\%,-36\%\), on **CGLM**\(R_{d}=-77\%,-67\%-62\%\), on **FMoW**\(R_{d}=-480\%,-227\%,-159\%\) and on **Yearbook**\(R_{d}=4\%,11\%,11\%\). The disproportionately severe negative result on FMoW is due to the otherwise small accuracy gap \(G_{d}=-2.5\%,-5.2\%,-7.4\%\). More importantly, we hypothesize that TTA fails to outperform Naive because the common assumptions, upon which TTA methods were designed, are broken.

## 7 Analysis of Importance Weighted Memory Sampling

We first perform an ablation study of our IWMS to show the effectiveness of the importance sampling. Then, we show our performances under different computational budgets and buffer sizes. We provide further in-depth analysis of the information retention abilities of the considered methods in the Supplementary Material A.13 and the effect of memory size in A.14.

**Analysis on Memory Sampling Strategies.** Note that while our method, IWMS is a prioritised sampling approach, it has some similarities to Naive, except for the sampling strategy. While the Naive method uses the most recent labeled data and a randomly sampled mini-batch from the memory buffer for each parameter update, our method provides a third option for constructing the training mini-batch, which picks the labeled memory sample that is most similar to the unlabeled data. When comparing sampling strategies, we refer to the newest batch of data as (N), the random batch of data as (R) and the importance weighted memory samples as (W).

In Figure 4, we first show that in both delay scenarios (\(d=10\) and \(d=100\)) replacing the newest batch (N) with (W) results in almost doubling the performance: \(+8.5\%\) and \(+9.1\%\) improvement over Naive, respectively. Interestingly enough, when we replace the (N) with uniformly sampled random buffer data (R) we report a significant increase in performance. We attribute this phenomenon to the detrimental effects of label delay: even though Naive uses the most recent supervised samples for training, the increasing discrepancy caused by the delay \(d=10\) and \(d=100\) forces the model to over-fit on the outdated distribution.

## 8 Conclusion and Future Work

We motivate modeling real-world scenarios by introducing the label delay problem. We show how severely and unpredictably it hinders the performance of approaches which _naively_ ignore the delay. To address the newfound challenges, we adopt the three most promising paradigms (Pseudo-Labeling, S4L and TTA) and propose our own technique (IWMS). We provide extensive empirical evidence over four large-scale datasets posing various levels of distribution shifts, under multiple label delay scenarios and, most importantly, under normalised computational budget. IWMS simply stores and and reuses the embeddings of every observed sample during _memory rehearsal_ where the most relevant labeled samples to the new unlabeled data are rehearsed. Due to its simplicity, the robustness against changes in the data distribution can be implemented very efficiently.

## 9 Acknowledgement

Botos Csaba was partially funded by Intel and partially by Meta AI Research. This work is supported by a UKRI grant Turing AI Fellowship (EP/W002981/1) and EPSRC/MURI grant: EP/N019474/1. Adel Bibi acknowledges the funding from the KAUST Office of Sponsored Research (OSR-CRG2021-4648) and the support from Google Cloud through the Google Gemma 2 Academic Program GCP Credit Award. The authors thank Razvan Pascanu and Joao Henriques for their insightful feedback. We also thank the Royal Academy of Engineering.

Figure 4: **Effect of sampling strategies** We report the Online Accuracy under the least ( \(d=10\)) and the most challenging (\(d=100\)) label delay scenarios on CGLM .