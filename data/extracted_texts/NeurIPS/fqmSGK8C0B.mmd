# Deep Learning for Computing Convergence Rates of Markov Chains

Yanlin Qu  Jose Blanchet  Peter Glynn

Department of Management Science and Engineering

Stanford University

{quyanlin,jose.blanchet,glynn}@stanford.edu

###### Abstract

Convergence rate analysis for general state-space Markov chains is fundamentally important in operations research (stochastic systems) and machine learning (stochastic optimization). This problem, however, is notoriously difficult because traditional analytical methods often do not generate practically useful convergence bounds for realistic Markov chains. We propose the Deep Contractive Drift Calculator (DCDC), the first general-purpose sample-based algorithm for bounding the convergence of Markov chains to stationarity in Wasserstein distance. The DCDC has two components. First, inspired by the new convergence analysis framework in (Qu et al., 2023), we introduce the Contractive Drift Equation (CDE), the solution of which leads to an explicit convergence bound. Second, we develop an efficient neural-network-based CDE solver. Equipped with these two components, DCDC solves the CDE and converts the solution into a convergence bound. We analyze the sample complexity of the algorithm and further demonstrate the effectiveness of the DCDC by generating convergence bounds for realistic Markov chains arising from stochastic processing networks as well as constant step-size stochastic optimization.

## 1 Introduction

General state-space Markov chains are indispensable in a wide array of fields due to their flexibility and applicability in modeling random dynamical systems. To analyze the long-term behavior of these Markovian models, estimating the rate of convergence to equilibrium is critical. When designing reliable real-world systems (e.g. cloud platforms and manufacturing lines), the faster the convergence, the faster the recovery after disturbances. When designing efficient sample-based algorithms (e.g. stochastic gradient descent (SGD) variants and MCMC), the faster the convergence, the faster the goal attainment. The rate of convergence also appears in MDP-related sample complexity results under the name "mixing time". Although convergence rate estimation is critically important, estimating the convergence rate of even a mildly complex chain can be extremely difficult.

Over the last three decades, significant efforts have been made to bound the convergence of general state-space Markov chains. Most of these works utilize a pair of drift and minorization conditions (D&M) to bound the convergence in terms of the total variation (TV) distance (Meyn et al., 1994; Rosenthal, 1995; Jarner and Roberts, 2002; Douc et al., 2004; Baxendale, 2005; Andrieu et al., 2015). The drift condition forces the chain to move towards a selected region. On such a region, the minorization condition allows the chain to regenerate or to couple with a stationary version of the chain. This analysis tends to produce overly conservative TV bounds, especially in high-dimensional settings; see (Qin and Hobert, 2021) for a discussion.

The Wasserstein distance, as a measure of convergence to equilibrium, can exhibit better dimension dependence (Qin and Hobert, 2022b). In addition, many Markov chains of interest (e.g. constantstep-size SGD minimizing convex loss on finite datasets) converge in Wasserstein distance but not in TV distance. Consequently, bounding convergence in Wasserstein distance has steadily gained popularity over the years (Gibbs, 2004; Hairer et al., 2011; Butkovsky, 2014; Durmus and Moulines, 2015; Durmus et al., 2016; Qin and Hobert, 2022a). Most of these works replace the minorization condition with a contraction condition (D&M becomes D&C). After returning to a selected region, two copies of the chain tend to become closer to each other. Both D&M and D&C enforce two conditions in two respective regions. However, partitioning the state space into two distinct regions often leads to suboptimal rates.

Recently, (Qu et al., 2023) introduce the so-called contractive drift condition (CD), a single condition enforced on the entire state space, to explicitly bound the convergence in Wasserstein distance. A special case of CD dates back to (Steinsaltz, 1999). By verifying CD, (Qu et al., 2023) establish parametrically **sharp** convergence bounds for stylized Markov chains arising from queueing theory and stochastic optimization (e.g. revealing how step-size, heavy-tailed gradient noise, growth rate and local curvature of objectives affect the convergence of stylized SGD). Although CD may generate better bounds than D&M and D&C for stylized chains (e.g. SGD with iid gradient noise), these methods are generally intended as theoretical tools that can provide closed-form convergence bounds for structured models. For more realistic, less structured chains, computational rather than analytical methods are needed. However, despite of the rapid development of computational power in the past decade, the convergence analysis of general state-space Markov chains is still in the pen-and-paper age.

To launch **computational** Markov chain convergence analysis, we need a key to switch on the deep learning engine. This paper introduces the _Deep Contractive Drift Calculator_ (DCDC) is the first general-purpose sample-based algorithm for bounding the convergence of general state-space Markov chains. There are two key ideas we develop. The first is to observe that CD, an inequality by definition, is actually an equality by nature (if the inequality has a solution, then the corresponding equality also has a solution). Thus, we introduce the Contractive Drift Equation (CDE), an integral equation the solution of which leads to an explicit convergence bound. For the second part, inspired by the success of physics-informed neural networks (PINNs) in solving PDEs (Sirignano and Spiliopoulos, 2018; Raissi et al., 2019), we develop an efficient neural-network-based CDE solver. By combining these two components, DCDC solves CDEs by training neural networks and converts solutions into explicit convergence bounds. DCDC demonstrates the potential of computer-assisted convergence analysis and bridges the gap between deep learning and a traditionally challenging area of mathematical analysis.

In high-dimensional spaces, PINNs minimize the integrated residual of a PDE via SGD to find a continuously differentiable function that approximately satisfies the PDE. When applying this idea to solve a CDE, an integral equation, the solving procedure becomes more _natural_ in the following two ways. First, we only assume that the CDE solution is Lipschitz continuous, and neural networks are inherently Lipschitz continuous. Second, as SGD is already used to handle the integrated residual, we can simultaneously use it to handle the integral in the CDE. After approximately solving the CDE, DCDC needs to convert the solution into a convergence bound, which requires that the solution is uniformly accurate with high probability. This is different from PINNs in the PDE literature since the accuracy is mainly measured in the \(L_{2}\) sense.

The CDE solution is a new type of Lyapunov function that provides explicit convergence rates for random dynamical systems. For deterministic dynamical systems, traditional Lyapunov functions play central roles in establishing stability; see (Pukdeboon, 2011) for a review. There is a substantial literature on computing traditional Lyapunov functions via neural networks; see (Liu et al., 2023) and references therein. As pointed out in (Dawson et al., 2023), a survey on certificate learning, learned (traditional) Lyapunov functions provide safety certificates for learned control policies (on deterministic dynamical systems). For the control of random dynamical systems, DCDC not only generates safety certificates (CDE solutions) but also quantifies safety levels (convergence rates). Control and performance evaluation of random dynamical systems have become a staple in contemporary data-driven decision making systems, thus underscoring the importance of DCDC.

In short, we summarize our contributions as follows:

* We introduce the Deep Contractive Drift Calculator (DCDC), the first general-purpose end-to-end approach that enables the use of deep learning to bound the convergence rate of general state-space Markov chains.

* We perform sample complexity analysis and use DCDC to generate convergence bounds for realistic Markov chains arising in operations research as well as machine learning.
* Our DCDC approach discovers features that are exploited by techniques developed to study CDs by closed-form methods, such as the wedge shape and the boundary removal technique discussed in (Qu et al., 2023).

## 2 Contractive Drift Equation

Let \(X\) be a Markov chain on \(^{d}\), with random mapping representation

\[X_{n+1}=f_{n+1}(X_{n}),\ \ n=0,1,2,\]

where \(f_{n}\)'s are iid copies of \(f\), a locally Lipschitz random mapping from \(\) to itself (with probability one, \(f:\) is locally Lipschitz).

**Example.** Let \(\) be a positive constant and \(Z\) be a square integrable random variable. The SGD with step-size \(\) to solve \(_{x}(x-Z)^{2}/2\) is \(X_{n+1}=X_{n}-(X_{n}-Z_{n+1})\) where \(Z_{n+1}\)'s are iid copies of \(Z\), so the corresponding random mapping is \(f(x)=x-(x-Z)\).

Understanding the long-term behavior of \(X\) requires estimating how fast \(X_{n}\) converges to \(X_{}\) (equilibrium) as \(n\). The difference between the two distributions is quantified by either total variation (TV) distance or Wasserstein distance. Representative TV convergence bounds (e.g., \(TV(X_{n},X_{}) Cr^{n}\)) can be found in (Meyn et al., 1994; Rosenthal, 1995; Baxendale, 2005). Representative Wasserstein convergence bounds (e.g., \(W(X_{n},X_{}) Cr^{n}\)) can be found in (Hairer et al., 2011; Durmus and Moulines, 2015; Qin and Hobert, 2022). These analytical methods can only handle stylized (structured) Markov chains. The goal of this paper is to introduce the first computational method that can handle realistic (less structured) Markov chains.

The first step to achieve the goal is introducing the contractive drift equation (CDE). The local Lipschitz constant of \(f\) at \(x\) is defined as

\[Df(x)}{{=}}_{ 0}_{x^{ };x^{} B_{}(x)})-f(x^{ })\|}{\|x^{}-x^{}\|}\]

where \(\|\|\) is the Euclidean norm and \(B_{}(x)=\{x^{}:\|x^{}-x\|<\}\). If \(f\) is differentiable, then

\[Df(x)=_{h 0}_{v:\|v\|=1}=_{v:\|v\|=1} \| f(x)v\|=\| f(x)\|\]

where \( f\) is the Jacobian matrix of \(f\) and \(\|\|\) becomes the spectral norm when applying to matrices. Basically, \(Df(x)\) describes how expansive or contractive \(f\) is around \(x\). With these notations, the contractive drift condition (CD) in (Qu et al., 2023) that leads to computable convergence bounds is

\[KV(x)}{{=}}Df(x)V(f(x)) V(x)-U(x), \ \ x\] (1)

where \(V,U:_{+}\) are bounded away from zero. In the rest of this paper, we adopt the convention that all functions denoted by \(U\) are positive and bounded away from zero, i.e. \( U>0\). We use \(_{x}\) to denote the expectation operator conditional on \(X_{0}=x\). In (1), the subscript is omitted as the initial location is clear. By replacing "\(\)" with "\(=\)" in (1), the contractive drift equation (CDE) is \(KV=V-U\), for which we establish the following existence and uniqueness results. All proofs are in the appendix.

**Theorem 1**.: _Fix \(U\) and suppose that \(KW W-U\) has a non-negative finite solution \(W_{*}\). Then_

\[V_{*}(x)}{{=}}_{x}[_{k=0}^{ }U(X_{k})_{l=1}^{k}Df_{l}(X_{l-1})],\ \ x\] (2)

_is finite and satisfies \(KV_{*}=V_{*}-U\). Furthermore, \(KV=V-U\) has at most one bounded solution._

_Remark_.: This \(V_{*}\) can be interpreted as an average space-discounted cumulative reward. Imagine a swarm of agents moving according to \(f\). For an agent at \(x\), if \(Df(x)<1\) (contraction), then after \(f\) is applied, there will be more agents around this agent. If all agents around \(f(x)\) share a total reward \(U(f(x))\), then the reward for each of them is discounted. From the perspective of a particular agent, the procedure is like collecting reward within a shrinking ball.

Deep Contractive Drift Calculator

### Why do we introduce CDE?

Physics-informed neural networks (PINNs) solve a PDE by minimizing its integrated residual (Sirignano and Spiliopoulos, 2018; Raissi et al., 2019). If we want to use this idea to solve \(KV V-U\), then the integrated residual is

\[()}{{=}}_{}(KV_ {}(x)-V_{}(x)+U(x))_{+}h(x)dx\]

where \(h\) is a positive density and \(\{V_{}:\}\) is a neural network. Note that the residual at \(x\) is positive if and only if \(KV_{}(x)>V_{}(x)-U(x)\). By letting \(X_{0}\) have distribution \(h\),

\[()=[[Df_{1}(X_{0})V_{}(f_{1} (X_{0}))-V_{}(X_{0})+U(X_{0})|X_{0}]]_{+},\]

which is an expectation of a non-linear function of a conditional expectation. Minimizing \(()\) is a conditional stochastic optimization problem (CSO). In CSO, the sample-average gradient is biased (Hu et al., 2020), which leads to a high sample complexity for convergence (Hu et al., 2020). Fortunately, if we aim at solving \(KV=V-U\) (CDE) instead of \(KV V-U\) (CD), then there exists a simple unbiased gradient estimator. Now we briefly derive this estimator. For a CDE, the integrated residual becomes

\[l()}{{=}} _{}(KV_{}(x)-V_{}(x)+U(x))^{ 2}h(x)dx\] \[= [[Df_{1}(X_{0})V_{}(f_{1}(X_ {0}))-V_{}(X_{0})+U(X_{0})|X_{0}]^{2}.\]

with its gradient

\[l^{}()\] \[= 2[[Df_{1}(X_{0})V_{}(f_{1}(X _{0}))-V_{}(X_{0})+U(X_{0})|X_{0}][Df_{1}(X_{0})V ^{}_{}(f_{1}(X_{0}))-V^{}_{}(X_{0})|X_{0}]]\] \[= 2[[Df_{1}(X_{0})V_{}(f_{1}(X _{0}))-V_{}(X_{0})+U(X_{0})][Df_{-1}(X_{0})V^{}_{ }(f_{-1}(X_{0}))-V^{}_{}(X_{0})]|X_{0}]\] \[= 2[[Df_{1}(X_{0})V_{}(f_{1}(X_{0}))-V_{ }(X_{0})+U(X_{0})][Df_{-1}(X_{0})V^{}_{}(f_{-1}(X _{0}))-V^{}_{}(X_{0})]]\]

where \(f_{1}\) and \(f_{-1}\) are iid copies of \(f\) while \(V^{}_{}=dV_{}/d\) is computed via backpropagation. This expression allows us to estimate \(l^{}()\) without any bias. In summary, the inequality (CD) is enough to bound the convergence, but the equality (CDE) turns out to be easier to establish (via deep learning).

### Dcdc

Given the above discussion, a standard application of SGD is enough to simultaneously handle the integrated residual as well as the integral in the CDE, resulting in the following simple algorithm, Deep Contractive Drift Calculator, the first general-purpose sample-based algorithm to bound the convergence of general state-space Markov chains.

``` Step-size \(\), number of iterations \(T\), neural network \(\{V_{}:\}\), initialization \(_{0}\) for\(t\{0,...,T-1\}\)do  sample \((X_{0},f_{1},f_{-1})\)  compute \(^{}(_{t})\) as \[2[Df_{1}(X_{0})V_{_{t}}(f_{1}(X_{0}))-V_{_{t}}(X_{0})+U(X_{0 })][Df_{-1}(X_{0})V^{}_{_{t}}(f_{-1}(X_{0}))-V^{ }_{_{t}}(X_{0})]\]  update \(_{t+1}=_{t}-^{}(_{t})\) (SGD or its variants) endfor  convert \(V_{_{T}}\) into a convergence bound (Theorem 3 and Theorem 4) ```

**Algorithm 1** Deep Contractive Drift Calculator (DCDC)

The conversion will be discussed in the next two subsections. In the current subsection, we show the validity of approximating CDE solutions via neural networks. In the following, we use \(\|\|_{}\) to denote the sup norm of functions on \(\).

**Theorem 2**.: _If \(\) is compact, \(\|Df\|_{}\) is finite, and \(V_{*}\) in (2) is finite and continuous, then for any \(>0\), there exists a neural network \(\{V_{}:\}\) and its realization \(V_{_{*}}\) such that_

\[\|KV_{_{*}}-V_{_{*}}+U\|_{}<.\]

Although DCDC solves CDEs on compact sets, it can be applied to Markov chains on non-compact sets that have compact absorbing sets (e.g. SGD for regularized problems). For chains without a compact absorbing set, extending DCDC to bound their convergence is left for future research, but here we describe a natural strategy to do so. In general, a Markov chain spends most of its time on some large compact set \(C\) where the chain may have complex dynamics. When the chain is outside \(C\), it typically has a strong tendency to return. Therefore, to extend DCDC, we can (i) search some parametric family (e.g. \(V_{A}(x)=x^{}Ax\)) to establish a CD outside \(C\) (capturing the return tendency); (ii) apply DCDC to obtain a CDE solution on \(C\) (capturing the complex dynamics); (iii) stitch them together to obtain a global CD. Comparing the large set here with the _small set_(Meyn and Tweedie, 2009) in D&M or D&C illustrates the advantage of computational methods over analytical ones. The size of the large set is determined by the approximation capability of neural networks, but the size of the small set is determined by the minorization or contraction condition (the two conditions often require the small set to be very small).

### Practical convergence bounds with exponential rates

Now we discuss how to convert \(KV V-U\) into convergence bounds with exponential rates in Wasserstein distance. To begin, we recall the definition of the Wasserstein distance. Let \(()\) be the set of probability measures on \(\) equipped with its Borel sigma-algebra. The Wasserstein distance between \(,()\) is

\[W(,)}{{=}}_{(, )}_{}\|x-y\|(dx,dy)\]

where

\[(,)}{{=}}\{ ():(,)=(), \;(,)=()\}\]

is the set of all couplings of \(\) and \(\). Given two random variables \(Z_{1}\) and \(Z_{2}\), we use \(W(Z_{1},Z_{2})\) to denote the Wasserstein distance between their marginal distributions.

**Theorem 3**.: _Suppose that \(\) is convex and that \(KV V-U\) holds with \( V<\). If \(\|X_{0}-X_{1}\|<\), then \(X\) has a unique stationary distribution \(X_{}\) with_

\[W(X_{n},X_{}) Cr^{n},\;\;r}{{=}}1-  U/ V,\;\;C}{{=}} \|X_{0}-X_{1}\|V(X_{0}+(X_{1}-X_{0}))}{ U( V/  V)}\]

_where \(\) is a \(U\) random variable independent of \(X_{0}\) and \(X_{1}\)._

Given \(U\), the exponential rate \(r\) is determined by the magnitude of \(V\). The smaller the \(V\), the faster the convergence. Given \(X_{0}\), the pre-multiplier \(C\) can be easily computed by simulating the first transition (from \(X_{0}\) to \(X_{1}\)).

In Theorem 3 of (Qu et al., 2023), convergence bounds with exponential rates are straightforwardly derived from \(KV rV\) where \(r<1\), so one might wonder why we need the less straightforward Theorem 3 here. This is because \(KV rV\) is not suitable for PINN-like solvers. In Theorem 3, we solve \(KV=V-U\) and compute the exponential rate \(r\) from the solution \(V\). However, for \(KV=rV\), we need the answer (the exponential rate \(r\)) to write down the question (the equation to solve and the corresponding loss to minimize), which is circular. Of course, we may try solving \(KV=rV\) for different values of \(r\), but it turns out that it is very hard for DCDC to converge even for very conservative (close to \(1\)) \(r\)'s. Here is an explanation. Unlike \(KV=V-U\), which has a solution as long as \(KV V-U\) has one (Theorem 1), \(KV=rV\) may not have a solution even when \(KV rV\) has one. However, it is not hard to show that \(KV=rV-r\) has a (formal) solution

\[V_{r}(x)}{{=}}_{x}[_{k=0}^{ }(1/r)^{k}_{l=1}^{k}Df_{l}(X_{l-1})],\;\;x.\]

Comparing with \(V_{*}\) in (2), \(U(X_{k})\) is replaced by exponentially exploding \((1/r)^{k}\). Back to \(KV=rV\), its solution (if there is any) should be the above expression without the summation but with \(k\) (as a limit), which suggests that the solution may have a large magnitude, making it difficult to approximate.

### Practical convergence bounds with polynomial rates

Now we discuss how to generate convergence bounds with polynomial rates using DCDC. The key is to iteratively solve a sequence of CDEs. For example, given \(V_{0}\), we first solve \(KV_{1}=V_{1}-V_{0}\) to obtain \(V_{1}\). Then we solve \(KV_{2}=V_{2}-V_{1}\) to obtain \(V_{2}\). These two CDEs together lead to an \(O(1/n)\) convergence bound.

**Theorem 4**.: _Suppose that \(\) is convex and that there exist positive functions \(V_{0},V_{1},,V_{m}\) such that \(0< V_{0}< V_{m}<\) and \(KV_{k+1} V_{k+1}-V_{k}\) for \(k=0,,m-1\). If \(\|X_{0}-X_{1}\|<\), then \(X\) has a unique stationary distribution \(X_{}\) with_

\[W(X_{n},X_{})\|X_{0}-X_{1}\|V_{m}(X_{0}+ (X_{1}-X_{0}))}{ V_{0}_{k=1}^{m-1}(1+n/k)}\]

_where \(\) is a \(U\) random variable independent of \(X_{0}\) and \(X_{1}\)._

The expectation in the numerator can be easily computed by simulating the first transition, while the product in the denominator is basically \(n^{m-1}\) as \(n\).

In Theorem 1 of (Qu et al., 2023), convergence bounds with polynomial rates (\(O(1/n^{m-1})\)) are derived from \(KV V-U^{1/m}V^{1-1/m}\) paired with \(KU U\), so one might wonder why we need so many CDs in Theorem 4 here. This is because \(KV V-U^{1/m}V^{1-1/m}\) is designed for the pen-and-paper setting where directly establishing a sequence of CDs is difficult. Given \(KV V-U^{1/m}V^{1-1/m}\), many inequalities are applied to extract a CD sequence from this single special CD, resulting in large constants in convergence bounds. DCDC makes it possible to directly establish a sequence of CDs (by consecutively solving CDEs). In this setting, we can use Theorem 4 to obtain better convergence bounds. To be specific, compared with our Theorem 4, the result in (Qu et al., 2023) has an extra factor \(m^{m}/m!\).

## 4 Sample Complexity

As a numerical solver, DCDC solves CDEs approximately. Let \(=V_{_{T}}\) be the output of DCDC. We should not expect \(K=-U\) to hold exactly. Even if \(\) is an exact solution, the exactness is hard to verify as \(K\) is an expectation and the domain \(^{d}\) is not a finite set. As establishing convergence bounds requires CDs to exactly hold everywhere, given \(N\) iid copies of \(f\) to estimate \(K\) and \(=\{x_{1},,x_{M}\}\) uniformly sampled from \(\), we can (i) establish

\[_{N}(x)}{{=}}_{k =1}^{N}Df_{k}(x)(f_{k}(x))(x)-(x),\ \ x\]

where \(\) may be smaller than \(U\) (e.g. if \(\) is supposed to solve \(-K=U 1\), then \(_{}[-_{N}]\)); (ii) claim that \(K-+\) holds everywhere with probability at least \(1-\) where \(,>0\); (iii) convert \(K-+\) into a convergence bound. To have \(M,N\) large enough to make the claim in (ii), we need the following sample complexity result.

**Theorem 5**.: _Suppose that (i) \(\) is compact; (ii) \(V,U\) are bounded and Lipschitz; (iii) \(Df^{2}+D^{2}f<\) where \(Df\) is the Lipschitz constant of \(f\) and \(D^{2}f\) is the Lipschitz constant of \(Df\). Given \(,>0\), we can choose \(M=O((1/)/(^{d}))\) and \(N=O(1/(^{2}))\) to have_

\[P(_{x}[KV(x)-V(x)+U(x)]_{x }[_{N}V(x)-V(x)+U(x)]+)>1-.\]

Since the exponential rate of convergence in Theorem 3 is \(r=1- U/ V\), Theorem 5 also provides the sample complexity for estimating the exponential rate. Specifically, with probability at least \(1-\), the exponential rate \(_{M,N}\) computed from \(_{N}V V-U\) on \(\), which may not be a valid exponential rate, is \(\)-close to a valid exponential rate \(r_{*}\) (given by \(KV V-U+\) on \(\)).

It is worth noting that in terms of sample complexity, Theorem 5 guarantees a DCDC certificate (and thus a convergence bound to stationarity) with high probability with an efficient parametric \(O(1/N^{1/2})\) rate in terms of the number of samples (namely, the bound holds with high probabilityup to an error of order \(O(1/N^{1/2}))\). Once samples are generated, \(M=(1/^{d})\) points are chosen for the empirical evaluation. Thus, the total complexity (both in terms of number of evaluations and number of samples is \(O(1/^{2})+(1/^{d})\). A related literature on parametric integration (i.e. learning a Markov transition kernel that maps Lipschitz functions to continuous functions on the \(d\)-dimensional cube) provides a lower bound of order \((1/^{d})\), (Heinrich and Sindambiwe, 1999). Although these results are suggestive, they cannot be applied directly because we assume a random mapping representation, which provides additional structure. We plan to study the lower bounds in future work.

## 5 Numerical Examples

### Mini-batch SGD for logistic regression with regularization

Having established the theoretical foundation of DCDC, we now utilize it to generate convergence bounds for Markov chains of interest that are too hard for pen-and-paper analysis. To begin, we bound the convergence of a constant step-size mini-batch SGD that minimizes the cross-entropy loss over a finite dataset with \(L_{2}\) regularization.

Let \((x_{1},y_{1})\),...,\((x_{m},y_{m})\) be \(m\) data points where \(x_{i}[-1/2,1/2]^{2}\) and \(y_{i}\{0,1\}\). To perform regularized logistic regression, we want to choose \(b^{2}\) to minimize

\[-_{i=1}^{m}(y_{i} p_{i}+(1-y_{i})(1-p_{i}))+\|b\|^{2},\ \ p_{i}=(b^{}x_{i})=x_{i})}\]

where \(>0\) is the regularization parameter. The random mapping representation of the corresponding SGD with step-size \(\) and batch-size \(\) is

\[f(b)= b(1-/m)+(/)_{i B}[y_{i}-(b^{ }x_{i})]x_{i}\]

where \(B\) with \(|B|=\) is uniformly sampled from \(\{1,,m\}\). Thanks to the regularization, the chain has a compact absorbing set. In fact, the chain cannot escape from \(B_{m/()}(0)\). The local Lipschitz constant of \(f(b)\) is

\[Df(b)=\|(1-/m)I-(/)_{i B}^{} (b^{}x_{i})x_{i}x_{i}^{}\| 1-/m\]

where \(\|A\|=_{v:\|v\|=1}\|Av\|\) is the spectral norm. This demonstrates that the \(L_{2}\) regularization makes the chain contractive \(\|f(b_{1})-f(b_{2})\|(1-/m)\|b_{1}-b_{2}\|\). However, since the regularization parameter is chosen via cross-validation in a separate validation process, it is useful to obtain a contraction rate that is uniform in the regularization parameter. This rate is brought by the second term in \(Df(b)\) - we refer to this contribution as the _intrinsic_ convergence rate. However, it is challenging to analyze the spectrum of this state-dependent data-based random matrix, so we need DCDC. The code is available in the supplementary material. Each training procedure in this paper was completed within ten minutes on an M2 MacBook Air with 8GB RAM.

For the dataset, we set \(m=100\) and uniformly generate 100 \(x_{i}\)'s. For each \(x_{i}\), its label \(y_{i}\) follows \((0.9)\) or \((0.1)\), depending upon which coordinate of \(x_{i}\) is larger. For the SGD, we set the regularization parameter \(=1\), step-size \(=0.1\), and batch-size \(=10\). For DCDC, we run 1M Adam steps to train a single-layer network with width 1000 and sigmoid activation. We also experiment with deeper networks with the same amount of neurons, and the results are similar.

As demonstrated in Figure 5.1, the single-layer network can already accurately solve the CDE \(KV=V-0.1\). The learned solution \(\) is on the left while the estimated difference \(-\) is on the right. Aiming at \(KV V-0.1\), we get \(-0.0986\). This leads to exponential rate \(1-1.07 10^{-3}\) (Theorem 3) where \(1 10^{-3}\) corresponds to the regularization contribution, while \(7 10^{-5}\) corresponds to the intrinsic rate. Now we briefly discuss how the surface in Figure 5.1 (left) leads to the intrinsic rate. From the expression of \(Df(b)\), we know that the intrinsic contraction concentrates around the center. To make it contribute to the overall convergence, it needs to be _spread_. The surface in Figure 5.1 (left) provides the media to spread: (i) for points not at the center, the sunken surface creates a drift \(_{x}V(X_{1})<V(x)\); (ii) however, for points at the center, the sunken surface creates an anti-drift \(_{x}V(X_{1})>V(x)\), but it is overcome by the strong contraction \(_{x}Df_{1}(x)V(X_{1})<V(x)\). In this way, the strong contraction is spread (in the form of drift) to overall improve the contractive drift, which leads to the intrinsic rate. To conclude this example, when \(X_{0}=0\), we compute the pre-multiplier \(C=8.1\), which leads to convergence bound \(W(X_{n},X_{}) 8.1(1-1.07 10^{-3})^{n}\).

### Tandem fluid networks

In the above SGD example, contraction plays the leading role. Now we consider a tandem fluid network (Kella and Whitt, 1992) where drift plays the leading role. Let \(s_{1}\) and \(s_{2}\) be two stations with buffer capacity \(c\) that can process fluid workload at rates \(r_{1}\) and \(r_{2}\), respectively. External fluid only arrives at \(s_{1}\) and is processed by \(s_{1}\) then \(s_{2}\). Assume that the external input follows a compound renewal process where a random amount of fluid \(Z\) arrives after a random length of time \(T\) has passed since the last arrival. If \(r_{1}<r_{2}\), then \(s_{2}\) is always empty, so we let \(r_{1}>r_{2}\). Let \(X\) be the remaining workload vector after each arrival. Its random mapping representation is

\[f(x_{1},x_{2})=(((x_{1}-r_{1}T)_{+}+Z,c),((x_{2}+(r_{1}-r_{2})(T,x_ {1}/r_{1}),c)-r_{2}(T-x_{1}/r_{1})_{+})_{+})\]

where \(x_{1}\) decreases at rate \(r_{1}\) until it is empty while \(x_{2}\) increases at rate \((r_{1}-r_{2})\) until \(x_{1}\) is empty. Basically, within \([0,c]^{2}\), the chain follows a northwest-then-south path for time \(T\) and then jumps east by amount \(Z\). This chain has simple local Lipschitz constant \(Df(x_{1},x_{2})=I(T(x_{1}+x_{2})/r_{2})\), obtained as an infinitesimal ball around \((x_{1},x_{2})\) collapses to a single point when the system is depleted before the next arrival. As a result, drift plays the leading role as contraction only happens around the origin.

For the tandem network, we set the buffer capacity \(c=1\), processing rates \((r_{1},r_{2})=(1.1,1.0)\), interarrival time \(T U[0,0.2]\) and arriving amount \(Z U[0,0.1]\) (the stability condition is \(Z<r_{2}T\)). For DCDC, we run 1M Adam steps to train a double-layer network with width 40 and sigmoid activation.

Although a slightly deeper network is trained, the result in Figure 5.2 (left) is almost a plane. Now we briefly explain why this is the correct solution. First, note that as long as the stability condition \(Z<r_{2}T\) holds, the total workload \((x_{1},x_{2})=x_{1}+x_{2}\) is the most natural Lyapunov function such that \(_{x}(X_{1})-(x)=(-r_{2}T+Z)<0\) holds when \(x\) is far away from the boundary. Second, the "boundary removal technique" introduced in (Qu et al., 2023) shows that the above drift can be extended to the boundary as a contractive drift \(_{x}Df(x)(X_{1})-(x)=(-r_{2}T+Z)<0\) as if the boundary (that causes anti-drift) never exists. The plane in Figure 5.2 (left) demonstrates that DCDC has already mastered the above two steps! Again, we conclude this example with convergence bound \(W(X_{n},X_{}) 5.67(1-0.017)^{n}\) when \(X_{0}=0\).

Figure 1: Left: The learned solution \(\) of \(KV-V=-0.1\) for the mini-batch SGD, with maximum 91.87. Right: The estimated difference \(-\), with maximum -0.0986, mean -0.9999, standard deviation 0.0003.

### Discovery of meaningful wedge-like Lyapunov functions

Lyapunov functions are usually denoted by \(V\) in the literature, and \(V\) typically represents the shape of Lyapunov functions. As mentioned in the introduction, the CDE solution is a new type of Lyapunov function. In most cases, it is also \(V\)-shaped, representing the drift towards some contractive region. However, Markov chains sometimes exhibit neither drift nor contraction, such as when SGD is stuck in a non-strongly-convex basin or when the water level of the Moran dam (Stadje, 1993) is neither too low nor too high. Here, we use the simplest example, a two-sided regulated random walk \(f(x)=((x+Z,1/2),-1/2)\) with \(Z U[-1/3,1/3]\), to illustrate that DCDC discovers upside-down \(\)-shaped Lyapunov functions to address the above issue.

In \([-1/6,1/6]\), the chain exhibits neither drift (\(Z\) is symmetric) nor contraction (\( 1/2\) boundaries are not reachable in one step). The wedge in Figure 5.3 (left) creates an artificial drift to maintain the CD. In (Qu et al., 2023), a similar function is introduced as a tool to study stylized non-strongly-convex SGD. DCDC not only discovers this tool but also makes the wedge meaningful. As mentioned in the remark below Theorem 1, the CDE solution generated by DCDC represents an average space-discounted cumulative reward, where an agent collects reward within a shrinking ball. Why does starting from the middle lead to the highest reward? Because the ball starting there has the longest lifespan before hitting the boundary and collapsing into a single point.

Figure 3: Left: The learned solution \(\) of \(KV-V=-0.1\) for the regulated random walk, with maximum 0.972. Right: The estimated difference \(-\), with maximum -0.0662, mean -0.0964, standard deviation 0.0106.

Figure 2: Left: The learned solution \(\) of \(KV-V=-0.1\) for the tandem network, with maximum 3.78. Right: The estimated difference \(-\), with maximum -0.0668, mean -0.0989, standard deviation 0.0097.

### Recovery of exact convergence rates

Finally, to demonstrate the potential of DCDC to accurately recover exact convergence rates, we examine a class of \(d\)-dimensional autoregressive processes

\[f(x)=Hx+Z,\ \ x,H,Z 0\] (3)

where random vector \(Z\) is integrable and constant matrix \(H\) is symmetric with all its eigenvalues in \((0,1)\). The exact convergence rate of this Markov chain in Wasserstein distance can be explicitly computed by pen and paper.

**Proposition 1**.: _Let \(X\) be the Markov chain defined by (3). If \(X_{0}=0\), then_

\[\|H^{n}Y\|/ W(X_{n},X_{})\|H^{n}Y\|,\ \ Y=_{k=1}^{}H^{k-1}Z_{k}\]

_where \(Z_{k}\)'s are iid copies of \(Z\). Let \(\) be the largest eigenvalue of \(H\). Then \(\|H^{n}Y\|=(^{n})\) as long as \(Y\) does not concentrate on the orthogonal complement of the eigenspace associated with \(\)._

For the autoregressive process, let \(d=3\) and

\[H=0.4&0.2&0.1\\ 0.2&0.5&0.2\\ 0.1&0.2&0.6\]

with \(=0.850\). Let \(Z\) be uniformly sampled from \(B_{1}(0)_{+}^{3}\). Note that the resulting Markov chain cannot escape from \(B_{10}(0)_{+}^{3}\). After plugging into the simulator of (3), DCDC generates \(V 0.668\), which recovers the exact convergence rate

\[KV=V-U=(1-U/V)V=(1-0.1/0.668)V=0.850V= V.\]

## 6 Conclusions

We introduce DCDC, a potent framework that enables the use of deep learning techniques to tackle the problem of estimating convergence to stationarity of complex, general state-space Markov chains. Our approach unlocks the key to using scalable data-driven tools to tackle this important problem. In future work, we plan to use these results in the context of general state-space reinforcement learning, control of ergodic systems, and related applications by employing the CD condition as a policy regularizer.

## 7 Limitations

DCDC solves CDEs on compact spaces. A potential strategy to handle non-compact spaces is discussed in Section 3.2. Sample complexity lower/upper bounds are not studied in this paper and they are left for future research.