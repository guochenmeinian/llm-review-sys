# Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge

Fawaz Sammani, Nikos Deligiannis

ETRO Department, Vrije Universiteit Brussel, Pleinlaan 2, B-1050 Brussels, Belgium

imec, Kapeldreef 75, B-3001 Leuven, Belgium

fawaz.sammani@vub.be, ndeligia@etrovub.be

###### Abstract

Contrastive Language-Image Pretraining (CLIP) performs zero-shot image classification by mapping images and textual class representation into a shared embedding space, then retrieving the class closest to the image. This work provides a new approach for interpreting CLIP models for image classification from the lens of mutual knowledge between the two modalities. Specifically, we ask: what concepts do both vision and language CLIP encoders learn in common that influence the joint embedding space, causing points to be closer or further apart? We answer this question via an approach of textual concept-based explanations, showing their effectiveness, and perform an analysis encompassing a pool of 13 CLIP models varying in architecture, size and pretraining datasets. We explore those different aspects in relation to mutual knowledge, and analyze zero-shot predictions. Our approach demonstrates an effective and human-friendly way of understanding zero-shot classification decisions with CLIP. 1

## 1 Introduction

Contrastive Language-Image Pretraining (CLIP)  has catalyzed a paradigm shift in zero-shot and few-shot learning methodologies for image classification . CLIP consists of a vision and language encoder, both which are trained to map positive image-text pairs close together in embedding space, while pushing away negative ones. In the context of information theory, the channel which connects two information sources is referred to as the _information channel_, and its reliability and effectiveness is often studied through Mutual Information (MI) analysis between the two sources . The training dynamics of contrastive models inherently involve a significant degree of shared knowledge between the vision and language sources, as both models must map similar points close in the embedding space. This suggests the existence of a _vision-language information channel_ (Figure 0(a)) wherein the shared knowledge between the two modalities is stored.

Inspired by this, we aim to interpret this channel and measure the relationship and mutual knowledge between the image and text encoders of CLIP, for a given zero-shot prediction. We therefore pose the following question: _What concepts did the vision and language encoders learn in common, such that the image-text points are closer or further apart in the joint space?_

The two sources of information--the vision encoder and the text encoder--differ in modality: the vision encoder provides interpretation as visual regions, while the text encoder can only provide interpretation as text. To understand the commonalities in what both encoders learn, we must establish a shared medium for their interpretations. As a result, applying existing attribution techniques  does not suffice. Moreover, the information channel is composed of _discrete_ units of information (i.e., bits), however these attribution techniques provide general, high-level interpretations(_e.g.,_ attributing the main object in the scene). They do not break-down the entangled attribution into their internal components. For instance, when applied to images of different dog breeds, attribution techniques might highlight the entire dog, indicating that the model is focusing on the correct object. However, they do not reveal what exactly in the main object influenced the model's decision. Which specific features of the dog were important? Is it the shape of the nose, the ears, the body, the head or the snout? ImageNet , for example, is a coarse-grained dataset, requiring models to learn distinctive concepts of an object to make decisions, but current explainability techniques do not reflect this. Therefore, we argue that distinctive fine-grained concepts are more beneficial for representing the discrete units in a channel, while also facilitating the calculation of mutual information between the two sources efficiently.

To address this question, we interpret the outcome of the visual and textual encoder as discrete random variables and use the MI to quantify the amount of information obtained about one random variable (visual data) through the other random variable (textual data). Drawing from this inspiration, we strive towards an interpretation and analysis approach of textual concepts; short descriptions in natural language (_e.g.,_ "a long snout", "feathered ears"). In addition to being human-friendly interpretable, understood even to layman users, each textual concept can be mapped to an integer in a dictionary of predefined concepts (_e.g.,_ "a long snout" \(\) 0, "feathered ears" \(\) 1). Since integers are discrete, they can represent the information units of the channel, while also facilitating the calculation of MI in the discrete space directly, which is fast, efficient, and reliable. This approach also eliminates the need for MI approximations typically required in the continuous space.

In order to achieve this, we need the two CLIP encoders to output random variables in the same space (that is, the space of textual concepts). In the vision encoder, we first refer to visual concepts as object parts grounded in the image and directly extracted from the visual features (Figure 0(b), top). Those are discrete visual units that are not in the textual domain, however each of them can be described via a textual concept. Therefore, we refer to textual concepts in the vision encoder as textual descriptions of those visual concepts. As a result, multimodal concepts in the vision encoder are corresponding pairs of visual-textual semantics describing discriminative parts of an object (Figure 0(b)). Depending on the dataset, an object can also refer to the main scene (_e.g.,_ lake or ocean in Places365 dataset ). Notably, our approach does not involve training any model to generate those multimodal concepts. The textual component of these multimodal concepts at the

Figure 1: CLIP maps visual and textual inputs into a joint embedding space, with an information channel expressed in terms of the Mutual Information (MI) between them (**a**). We interpret the visual features from the vision encoder with multimodal concepts (**b**) which represent object parts and their corresponding textual description. From the language encoder, we identify points (shown in grey) around the zero-shot prediction (shown in green) as textual descriptions of the predicted class (**c**). By considering the textual descriptors corresponding to the visual concepts, and the textual descriptors of the language encoder for the predicted class, the two encoders establish a common space of textual concepts allowing us to identify mutual concepts and analyze their shared knowledge (**d**).

vision encoder are now expressive of the visual concepts in the text domain. Once the mapping of visual concepts to textual concepts is achieved, we proceed with extracting textual concepts from the language encoder. This can be achieved by identifying points around the zero-shot prediction (Figure 0(c)). Given the output embedding of the predicted class (green point) from the language encoder, we identify related textual concepts (grey points) around that prediction. These directly serve as textual concepts explaining the prediction. The two encoders of CLIP now share a common medium of textual concepts, and we can establish the mutual concepts of both the vision and language encoders (Figure 0(d)). By observing Figure 0(d), we see that the snout and its physical features (e.g., wrinkled, long, pointy) are expressive of what the vision and language encoders learn in common, which influence the prediction of a "bluettick coonhoul" in the joint space.

Our work contributes as follows: 1) it introduces a user-friendly approach to interpreting CLIP's visual features through multimodal concepts, and we demonstrate the effectiveness of those concepts by surpassing other baselines and achieving gains of up to 3.75% in zero-shot accuracy. 2) it enables us to visualize what CLIP models learn in common when making zero-shot predictions, and how the two encoders influence each other, and 3) it allows us to explore relationships between various model aspects (model size, pretraining data, and accuracy) and its shared knowledge, and inspect the degree of correlation between the CLIP vision and text encoders.

## 2 Related Work

**Multimodal Explanations:** So far, post-hoc multimodal explanations have been limited to the context of Natural Language Explanations (NLE) [41; 24; 48]. NLEs are annotated textual explanations for the output prediction for a variety of vision and vision-language tasks, where models are explicitly trained to generate such explanations. The visual explanation counterpart is typically obtained by visualizing the attention weights of the prediction. However, there are two significant issues in NLEs. Firstly, we argue that any interpretability technique based on training is not faithful to the model being interpreted, and falls more towards the task of image captioning where the caption is the explanation. Explanations should not reflect what humans desire, but rather reflect the model's own reasoning. Training these models also involves learning biases and statistical correlations, akin to the challenges faced by any machine learning model. A recent work  showed that trained textual explanation models are highly susceptible to the _shortcut bias learning_ problem, rendering the explanation ineffective despite achieving state-of-the-art results on Natural Language Generation metrics. Secondly, both the visual and textual explanations generated by NLEs are general, high-level and entangled (_e.g.,_ highlighting the main object in the scene). On the other hand, our multimodal explanations tackle both issues outlined in NLE. They are (i) training-free and (ii) offer distinctive, fine-grained concepts. Another line of work [36; 43] extracts descriptors from a large language model and uses them as additional information when building class embedding weights of CLIP. The set of descriptors with the highest similarity with respect to the global image are considered as an explanation for the prediction. While those textual concepts are fine-grained, the explanation generated is _single-modal_. Different from , our concept-based explanations are multi-modal fine-grained explanations, composed of visual-textual concepts which are directly grounded in the image. Finally,  analyzes primitive concepts in vision-language contrastive models. We discuss this work in Section J in the appendix since it is less-relevant to our study.

**Joint Embedding Space of Contrastive Models:** A few works investigate the vision-language modality gap in the joint feature space.  suggests that this gap stems from inherent differences between the two data modalities. Conversely,  discovered that there exists a gap that causes the image and text embeddings to be placed in two distinct regions in the joint space without any overlap. In contrast to , they attribute this gap to the inductive bias of neural network architectures, such that embeddings of two randomly initialized models are inherently separated within the joint space, and the contrastive learning objective maintains this separation. Different from the aforementioned works, our study does not investigate the gap. Instead, we assume the gap is fundamentally present, and analyze the strength of the shared knowledge within the two models, which influence this gap.

## 3 Method

Consider the problem of image classification, where the aim is to classify an image into a set of categories \(\). For ImageNet , \(||\) = 1,000. CLIP  formulates image classification as a retrieval task by using the textual class names of \(\) denoted as \(^{t}\), converting them into fixed natural text prompts (_e.g.,_ an image of a {class}), and encoding them with the language encoder of CLIP. The image is then encoded with the visual encoder of CLIP. The nearest category \(y\) to the image in the shared embedding space is then selected as the predicted class. In this context, the language encoder of CLIP can be seen as an encoder which encodes the weights of an image classifier.

**Notation:** We consider an image \(I^{H W 3}\), a CLIP model \(\) composed of a Vision Transformer (ViT) encoder \(^{v}\) and a language encoder \(^{l}\), and a set of features \(f=^{v}(I)^{N C}\) extracted using \(^{v}\), where \(N=H/P W/P\), with \(P\) being the patch size of \(^{v}\) and \(C\) being the feature dimension size. \(^{v}\) and \(^{l}\) are each followed by separate projection layers \(^{C c}\) which are fed with the [CLS] vector of the feature representation. For ease of notation, we represent the similarity score in the unified embedding space between an image-text input pair \((i,j)\) by \(s(i,j)=(^{v}^{v}(i))(^{l}^{l}(j))^{T}\). Similarly, we define \(s^{l}(j^{1},j^{2})\) as the similarity in the language embedding space between two text inputs \((j^{1},j^{2})\) by replacing \(^{v}\), \(^{v}\) with \(^{l}\), \(^{l}\), respectively.

We utilize a Large Language Model (LLM) to generate descriptors2 for all classes of a dataset we analyze. These descriptors are then combined into a unified set \(\) which contains all class-agnostic textual descriptors (i.e., the class name does not appear in the descriptor), and its cardinality (the number of descriptors it contains) is \(D\), that is, \(D=||\). For the ImageNet dataset, \(D\) = 4,229 after discarding repetitive descriptors across the entire pool. Concepts in \(\) are now applicable to any object and are not restricted to the class they were extracted from. For example, the textual descriptor "can be hung from a tree" is extracted from the class "swing" in ImageNet, but can now be applied to many other classes (_e.g.,_ monkey, siamang). The prompt and LLM we used, along with a detailed ablation study on various prompts and LLMs as well as the relevance and diversity of the generated descriptors, are presented in Section A.4 of the appendix.

Measuring the relationship and mutual knowledge between the image and text encoders for a given prediction is not straight-forward, as the two encoders differ in modality. Concepts in the language encoder can only be described via text, and concepts in the vision encoder can natively be described by image regions. Therefore, we need to map both concept modalities into a common space in order to quantify the mutual knowledge between the two encoders. A high-level overview of our method is shown in Figure 2. Given a set of images, we extract their visual features and perform spectral graph clustering on those features to obtain the most prominent image patches. We derive post-hoc grounded visual concepts representing object parts by applying Principal Component Analysis (PCA) or K-means clustering solely on the identified prominent patches (this is shown in Figure 1(a)). We encode the textual descriptors \(\) with the CLIP language encoder, and query each visual concept region (encoded with the CLIP visual encoder) from the set of descriptors \(\). In this way, we associate each visual concept with a textual concept describing it in natural text, producing \(_{v}\) (Figure 1(b)). In the final stage, the zero-shot predicted class and the set of descriptors \(\) are encoded with the CLIP language encoder, and the most similar descriptors close to the zero-shot prediction in the language embedding space are retrieved, producing \(_{}\) (Figure 1(c)). Now, the two encoders share a common space of textual concepts, and the MI can be calculated efficiently in the discrete

Figure 2: A high-level overview of our method for deriving visual concepts at the vision encoder (**a**), querying each visual concept individually from a textual bank to describe the visual concept in natural text (**b**), and then deriving textual concepts at the language encoder (**c**). The outputs of (**b**) and (**c**) share a common space of fine-grained textual concepts such that mutual information can be better calculated.

space using a probability-based approach by mapping each textual concept to a corresponding integer. The MI between \(_{v}\) and \(_{}\) is defined as:

\[I(_{v};_{})=H(_{v})+H(_{ })-H(_{v},_{}),\] (1)

where \(H\) represents the entropy and \(H(.,.)\) represents the joint entropy which we compute through a simple contingency table. We provide the derivation for this formulation in Section B of the Appendix. In the next subsections, we describe each of the aformentioned steps shown in Figure 2. Finally, we elaborate on the formulation of mutual information dynamics in Section 3.3.

### Multi-modal Concepts in the Visual Encoder

**Visual-based Concepts:** We first identify and separate the prominent patches in \(f\) from those that are non-relevant. Drawing inspiration from prior works on unsupervised object localization [35; 55; 6], we propose to decompose the feature space \(f\) into two groups, identifying the most prominent group as the focal point of the model. Specifically, we first construct an affinity matrix \(A^{f}\) from the patchwise feature correlations of \(f\): \(A^{f}=ff^{T}^{N N}\), where \(A^{f}\) serves as a spectral graph representing rich semantic information within the features. Each node in this graph corresponds to an image patch. We then apply eigendecomposition on \(A^{f}\) and extract the second largest (non-zero) eigenvector \(e_{f}^{N}\), known as the Fiedler eigenvector. The sign of each element in \(e_{f}\) represents a binary segmentation mask, dividing the graph nodes into two groups with minimal connectivity. We consider the subset of patches \(f^{p}\) corresponding to a positive sign in \(e_{f}\) as the most prominent and obtain an importance map. This decomposition technique is simple, fast and only requires features without needing gradients. We adopt Conditional Random Fields (CRF)  as an interpolation technique to interpolate the patch-based importance map to the resolution of the image. This approach provides better visual results than other interpolation techniques, while also preserving the underlying importance map (see experimental proof in Section A.3 of the Appendix). Finally, we note that \(f\) can be the _tokens_ or _keys_ of the last attention layer of the transformer. We ablate and analyze both in Section A.1 of the Appendix, and explore using PCA as an alternative decomposition technique.

Next, our aim is to derive visual concepts (_i.e.,_ object parts) from the high-level prominent patches extracted in the previous step. We draw upon the methodologies from [40; 1; 9] and apply either PCA or K-means clustering solely on the identified prominent image patches \(f^{p}\) across \(B\) images. In Section 4, we report results using each of these techniques. This process dissects the prominent image patches into a set of distinct components or clusters \(\) of length \(L\), which express visual concepts. The visual concepts are unique, i.e., a patch can only be assigned to a single concept. An overview of this process is shown in Figure 1(a).

**Describing Visual Concepts with Textual Descriptions:** We seek to link each visual concept identified in the previous step, to a textual descriptor. Initially, we encode each visual concept using the CLIP visual encoder by applying the visual prompt engineering approach proposed in . This approach involves drawing a red circle around the region of interest or blurring the area outside it, in order to direct the vision encoder's attention to that specific region, ensuring it encodes that area rather than the entire image. This approach has achieved strong zero-shot performance across diverse localization tasks, greatly surpassing cropping-based approaches . A subsequent work  verifies the effectiveness of this approach (see more details in Section F of the Appendix). We apply this technique to all the detected visual concepts in the image to yield a set of prompted images \(_{p}=\{I_{p^{1}} I_{p^{L}}\}\), where \(L\) is the number of visual concepts. Next, we encode the textual descriptors \(\) with the CLIP language encoder. Given that CLIP maps images and textual inputs close together in the embedding space, we find the associated top-\(k\) textual descriptors for a given visual concept by simply computing the similarity between the embedding of \(I_{p}^{j}\) and all textual descriptors \(\): \(s(I_{p^{j}},_{i})\), where \(j\) ranges over the \(L\) visual concepts, and \(i\) ranges over the top-\(k\) textual descriptors3. This results in an assignment matrix \(}^{L D}\). However, we observed that, with this approach, numerous visual concepts get mapped to the same descriptor, suggesting a distribution with low entropy. To address this, we enhance the alignment of the two distributions by treating \(-}\) as a cost matrix and transforming it into a permutation matrix \(\) via Optimal Transport:

\[(L,D)=*{argmax}_{^{L D}}_{l ,d}_{ld}(}_{ld})\] (2)where \(\) is a temperature parameter. We solve this optimization problem efficiently with the Sinkhorn-Knopp algorithm . The top textual descriptor from each column of \(\) is then selected as the descriptor for the respective visual concept represented by each row of \(\). We denote the textual concepts produced by this stage as \(_{v}\). In Section A.2 of the Appendix, we perform ablation studies on Optimal Transport and demonstrate that it achieves diversity among the different visual concepts.

### Textual Concepts in the Language Encoder

Given the zero-shot prediction of CLIP denoted as \(\) with \(^{t}\) being a textual representation of the prediction, we can represent \(\) as the center of a cluster in the joint space (green point in Figure 0(c)), with other points in that cluster (grey points) being textual concepts directly explaining the prediction \(\). We use the same set of textual descriptors \(\) (described in Section 3) to identify those concepts. We extract those textual concepts by computing the similarity between the language embeddings of the predicted class and the language embeddings of all descriptors \(\), via: \(s^{l}(^{t},)\). We select the top-\(u\) descriptors with the highest similarity score as those textual concepts and denote them by \(_{}\).

### Mutual Information Dynamics

A simple calculation of the MI between the vision and language concepts as in Eq. (1), fails to account for the contribution of each individual information unit (i.e., concept) to the overall MI. We define that two sources have a strong shared knowledge when a source retains knowledge about the other, despite removing important information units from it. To realize this, we first organize the textual concepts of the vision encoder \(_{v}\) in descending order based on their importance to the image, and sequentially ablate them, removing one at each step and calculating the MI (Eq. (1)) between them and \(_{}\) after each removal step. This process generates a curve. We report the Area under the Curve (AUC) to represent the MI dynamics. The strength of the shared information can be identified by how fast the MI in a curve drops. A higher AUC indicates gradual or late drops of MI in the curve, and thus stronger shared knowledge. A lower AUC indicates sharp or early drops of MI as concepts are removed, and thus weaker shared knowledge. We note that knowledge-retaining is not attributed to redundant information units since all concepts in \(\) are unique.

Finally, it is worth noting that the MI dynamics also serve as an evaluation strategy for the identified mutual concepts. By assuming that stronger shared knowledge is associated with higher zero-shot accuracy, we would expect a positive correlation between the AUC and zero-shot accuracy.

## 4 Experiments and Analysis

**Evaluation of Multimodal Concepts:** Since the multimodal concepts serve as inputs for MI analysis, we begin by evaluating these concepts to demonstrate their effectiveness and reliability. We formulate 3 baselines that adapt existing literature of single-modality concept-based explanations, to their multimodal case. MM-CBM is a formulation of Label-Free Concept Bottleneck Models  to the case of Multimodal Concept Bottlenecks. MM-ProtoSim is a formulation of the prototype-based ProtoSim  adapted to the multimodal case. We compare the performance of these baselines in Table 5 of the Appendix. The last baseline is denoted as "Feature Maps" and is a formulation of Neuron Annotation works [19; 11] to suit our case. Feature Maps identifies spatial feature activation maps as concepts. All baselines require training to generate textual concepts, and we train them on the full ImageNet training set. All baselines as well as our multimodal concepts are evaluated with 4 evaluation metrics common in the literature of XAI, namely, Insertion (higher is better) and Deletion (lower is better) , Accuracy Drop (low is better) and Accuracy Increase (higher is better) . We provide a description of the baselines with qualitative example in Section D of the Appendix, and of the evaluation metrics in Section E of the Appendix. As seen in Table 1, our concept-based multimodal explanations outperforms all baselines except on the Insertion Metric, where the MM-CBM baseline wins. Although not within the scope of our work, Table 5 of the Appendix also shows that our MM-ProtoSim baseline achieves state-of-the-art results on concept bottleneck models, on the challenging ImageNet dataset in which many other works fail to scale to. We also show that it not only maintains standard accuracy, but significantly improves it, another phenomenon in which many previous works including LF-CBM fail to achieve. This shows the effectiveness of considering multimodal concepts for modeling discriminative tasks.

Next, we show how our multimodal explanations are an effective application of CLIP prompt engineering for image classification with descriptions [36; 43], achieving gains in zero-shot accuracy of up to 3.75%. Another purpose of this experiment is to show that the multimodal concepts and descriptors identified, are a reliable source of input for mutual information analysis. We start by identifying the two most similar classes to the zero-shot prediction in the CLIP language embedding space. We then take the validation images from both of these classes, and extract multi-modal explanations using our approach. We then take the textual component of the multi-modal explanations as additional descriptors \(\) and re-evaluate the zero-shot classification of CLIP 4. If the detected open-set concepts are relevant to the prediction, we should expect an improvement in zero-shot classification accuracy. As shown in Table 2, this application shows significant gains in zero-shot accuracy for all CLIP models relative to the baselines [36; 43]. This demonstrates the effectiveness and relevance of the detected concepts to the CLIP model. More details about this experiment can be found in Section K of the Appendix.

**Models and Datasets:** Our MI analysis considers a wide range of CLIP models varying in architecture, size and pretraining datasets, evaluated on the full ImageNet validation split . We consider the original CLIP ViT models : ViT-B/16 and ViT-B/32 are base models of patch size 16 and 32, respectively; ViT-L/14 and ViT-L/14@336 are large models of patch size 14, where the later (denoted as ViT-L/14\(\)) is finetuned with an image size of 336\(\)336. The aforementioned models are trained on the WIT 400M dataset . We also consider additional models from OpenCLIP [22; 8] trained on DataComp  of 1B images and Data Filtering Network (DFN)  of 2B images. Both of these datasets use filtering strategies to curate clean, higher-quality data. We refer to these models with an additional suffix: -dcp and -dfn. Ultimately, we can analyze how model (and patch) size and pretraining datasets affect the information channel. We also consider the CNN-based ResNet (RN) CLIP models trained on WIT 400M: RN-50, RN-101, RN-50\(\)4 and RN-50\(\)16. The RN models with (\( r\)) denote width scaling \(r\). We also consider two CLIP ConvNeXt-Base models  from OpenCLIP, trained on LAION-400M  (ConvNeXt-B1), and on an aesthetic subset of LAION-5B  (ConvNeXt-B2). In total, our analysis comprises 13 CLIP models.

**Quantitative Analysis**: We start by examining the MI and its dynamics across models. In Table 3, we report the MI (applying Eq. 1) and AUC (as described in Section 3.3) for all CLIP models we analyze. Additionally, we include the dataset size used for training each model and its respective zero-shot classification accuracy on the ImageNet validation set . We report these metrics for both PCA and K-means, which consistently show correlation. We sort the models based on their top-1 zero-shot accuracy. We remind readers that we define stronger shared knowledge based on higher AUC rather than higher MI. In Figure 5 (detailed further), we provide examples of classes that support this claim and contribute to this phenomenon. Our first observation is that AUC aligns well

  
**Explanation** & Requires Training & **Detel.\(\)** & **Insert.\(\)** & **AccDrop\(\)** & **AccInc\(\)** \\  MM-CBM & Yes & 3.147 & **3.385** & 2.634 & 1.013 \\ MM-ProtoSim & Yes & 3.149 & 3.358 & 2.665 & 0.943 \\ Feature Maps & Yes & 2.921 & 3.114 & 2.283 & 1.233 \\  Ours (PCA) & No & 2.460 & 3.168 & 1.582 & **1.849** \\ Ours (K-means) & No & **2.422** & 3.122 & **1.555** & 1.781 \\   

Table 1: Evaluation scores of our multimodal explanations compared to the baselines established. All use the same features, model and textual concept bank for fair comparison.

  
**ResNets** & **Base** & **Ours** & \(\) & **ViTs** & **Base** & **Ours** & \(\) \\  RN50 & 59.54 & **61.85** & +2.31 & ViT-B/16 & 67.93 & **70.28** & +2.35 \\ RN50x4 & 64.36 & **67.93** & +3.57 & ViT-B/32 & 63.28 & **65.58** & +2.30 \\ RN50x16 & 68.47 & **72.22** & +3.75 & ViT-L/14 & 74.69 & **76.74** & +2.05 \\ RN101 & 60.68 & **64.14** & +3.46 & ViT-L/14@336px & 75.49 & **77.64** & +2.15 \\   

Table 2: Effectiveness and Relevancy of our multimodal concepts in boosting zero-shot accuracy of both ResNet and ViT CLIP models on the ImageNet validation set compared to baselines [36; 43].

with accuracy, with ViT-B/16-dfn ranking top. Our second observation is that CLIP ViT models are characterized with stronger shared knowledge than CLIP CNNs (ResNets and ConvNeXts). This supports the premise that pretraining transformer models, which lack inductive biases, perform better when trained on larger datasets .

To further understand the effect of model size and pretraining datasets on MI dynamics, we divide the models into two families. We first fix the pretraining data and vary the model and patch size. For this analysis, we use ViT-B/16, ViT-B/32, ViT-L/14 and ViT-L/14\(\) trained on WIT 400M. We show the curves in Figure 3 (left). As shown, larger models with more patches (either via a smaller patch size or a via a larger image size) correspond to higher AUC, suggesting that these models are better at encoding shared knowledge. Next, we fix the model size and vary the pretraining data. The results are shown in Figure 3 (middle). As shown, larger and higher-quality data lead to improved shared encoding on ImageNet. In Section G of the Appendix, we also perform analysis on the Places365  and Food101  datasets. The previous observations may be well-known and non-surprising. Nonetheless, what is noteworthy is the direct relationship established between the strength of the shared encoding (represented by the AUC) and model size, pretraining data and zero-shot accuracy. For example, we fit a linear line to the data points to approximate the AUC-accuracy relationship in Figure 3 (right), and determine the coefficients to be 11.24 and 25.97 for ViT models (blue line). This suggests that the accuracy is related to the strength by a factor of 11. Similarly, approximating the AUC-data relationship provides insights into the amount and quality of pretraining data required to achieve a desired strength of shared knowledge. This principle also extends to model size and could allow us to design small, efficient models. Finally, this relationship is valuable for model selection, especially in cases where accuracy alone is insufficient for distinguishing between models (e.g., models with very similar accuracies such as ViT-L/14\(\) and ViT-B/16-dfn). In Table 3 and Figure 3 (green line), we show that AUC demonstrates a linear correlation with accuracy and model size within the ResNet family. It is worth noting that AUC-Accuracy relationship only holds _within_ a given architecture; different architectures (ViT, ResNets, ConvNeXts) have different designs and ways of learning representation. Therefore, they differ in how they utilize data and encode shared knowledge. As an example, RN-50\(\)16 includes much more dimensions to store information than ConvNeXt-B2, and it is reasonable to expect that the shared information in RN-50\(\)16 is stronger, despite ConvNeXt-B2 achieves a slightly higher accuracy. In fact, the gradual design trajectory of ConvNeXt  was built and tuned towards a higher classification accuracy.

**Analyzing Concepts in the Vision Encoder:** Although the focus of our work is to interpret and analyze mutual concepts in the vision and language encoders of CLIP, we can still utilize our multimodal explanations to inspect internal concepts learned in the vision encoder of CLIP for individual instances. Figure 4 shows 4 examples, each represented by a distinct visual cluster denoted by a different color. The corresponding textual description for each visual cluster is provided below, aligned with its corresponding color. Different from attribution-based techniques [51; 60; 45; 2] which typically highlight high-level and general features, our multimodal explanations disentangle the features to offer visually and textually distinctive, fine-grained concepts. An example of such

    &  &  &  &  &  \\   & & & & PCA & K-means & PCA & K-means \\  ViTs & ViT-B/32 & 400M & 61.66 & 7.40 & 7.26 & 3.61 & 3.39 \\  & ViT-B/16 & 400M & 67.70 & 7.50 & 7.44 & 3.62 & 3.53 \\  & ViT-B/32-dcp & 1B & 68.88 & 7.79 & 7.65 & 3.93 & 3.70 \\  & ViT-B/16-dcp & 1B & 73.37 & 7.68 & 7.58 & 3.99 & 3.81 \\  & ViT-L/14 & 400M & 74.77 & 7.94 & 7.89 & 4.47 & 4.37 \\  & ViT-L/14\(\) & 400M & 76.23 & 7.96 & 7.93 & 4.51 & 4.44 \\  & ViT-B/16-dfn & 2B & **76.24** & **8.19** & **8.11** & **4.62** & **4.46** \\  ResNets & RN-50 & 400M & 58.42 & 7.14 & 7.20 & 3.23 & 3.32 \\  & RN-101 & 400M & 60.90 & 7.43 & 7.53 & 3.49 & 3.60 \\  & RN-50\(\)4 & 400M & 65.28 & **7.53** & 7.58 & 3.84 & 3.90 \\  & RN-50\(\)16 & 400M & **70.04** & 7.51 & **7.63** & **3.85** & **4.03** \\  ConvNeXTs & CNeXt-B1 & 400M & 65.36 & 6.47 & 6.66 & 2.54 & 2.80 \\  & CNeXt-B2 & 13B & **71.22** & **7.16** & **7.56** & **3.19** & **3.74** \\   

Table 3: MI and AUC scores for different model families using PCA and K-means evaluated on the full ImageNet validation split, along with the pretraining data and Top-1 accuracy.

concepts in Figure 4 are long feathered ears, long whiskers (first example); blue plumage, red beak (third example). These types of concepts are significantly more beneficial for understanding models and analyzing MI. More examples of our multimodal concepts are in Section H of the Appendix.

**Analyzing Mutual Knowledge across Classes:** Next, we show how to make use of our multimodal explanations and MI dynamics to analyze concepts across a set of images pertaining to a class. In Figure 5 we show examples of three classes from ImageNet: _sock, siamang_ and _crayfish_. Results are averaged across all \(B\) images of the class (\(B=50\) for ImageNet validation set). On the left, we show the MI curves. Note that, although the initial MI value for _siamang_ is higher than that of _sock_, the AUC for _sock_ (2.10) surpasses that of _siamang_ (1.82). This is attributed to the fact that the curve for _siamang_ starts at a higher point but drops faster at early stages. This shows that considering MI alone without its dynamics, is not representative of the strength of shared information. Next, we aim to delve deeper into this analysis using our multimodal explanations. To accomplish this, we examine the same semantic concept corresponding across all images. In Figure 5 (right), we showcase the semantic concept representing a _gibbon body_ for the "siamang" class (e.g., dark fur with light markings; four-limbed primate), visually identified by the red color. Similarly for the "sock" class, we show the semantic concept of _patterns and styles_ (e.g., various colors, patterns, and styles; plush texture; decorative pattern), visually identified by the blue color. Analyzing the curves and the area under them suggests that general concepts such as patterns and styles are better encoded

Figure 4: Qualitative examples of multimodal concepts in the vision encoder. The second-top textual descriptor may be omitted to avoid clutter.

Figure 5: Analyzing concepts in different ImageNet classes with Mutual Knowledge

Figure 3: MI Dynamics curve comparing model families (**left**) and pretraining datasets (**middle**). Correlation of AUC with zero-shot classification accuracy is shown **right** for ViTs and ResNets.

than discriminative concepts such as the body of a siamang. This rationale stems from the fact that CLIP was trained on a dataset from the internet, which is less discriminatory; it is less common to encounter images of an endangered specie of a gibbon compared to general concepts such as patterns and styles, which are prevalent characteristics across many objects in the world.

**Visualizing Mutual Concepts:** Finally, we provide visualizations of the mutual concepts detected by both vision and language encoders of CLIP in Figure 6. In the first example, we see that mutual concepts are distinctive to the zero-shot prediction of celo (_e.g.,_ handheld musical instrument, strings stretched across the head, a sound hole), suggesting that both encoders effectively represent the image and class in the joint space. In the second example, we see that the language encoder is stronger than the visual encoder at encoding the concept of a rattle snack since it provides related concepts, while the mutual concepts are weaker (only one mutual concept describes a rattle snack). These visualizations help us understand the common concepts learned by both encoders and how the encoders influence each other in the joint space. More examples are in Section I of the Appendix.

## 5 Conclusion

We proposed an approach for interpreting CLIP models for image classification from the perspective of mutual knowledge, by analyzing and interpreting the information channel established along with its dynamics. In the future, our work could be extended to non-contrastive models, or even to two parts of the same model. Finally, it is important to note that, like any research, our work has its own set of limitations, which are discussed in Section C of the appendix.