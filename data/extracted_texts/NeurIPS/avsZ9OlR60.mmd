# ET-Flow: Equivariant Flow-Matching for Molecular Conformer Generation

Majdi Hassan\({}^{*1}\)

Nikhil Shenoy\({}^{*2,4}\)

Jungyoon Lee\({}^{*1}\)

Hannes Stark\({}^{3}\)

Stephan Thaler\({}^{4}\)

Dominique Beaini\({}^{1,4}\)

\({}^{1}\)Mila & Universite de Montreal

\({}^{2}\)University of British-Columbia

\({}^{3}\)Massachusetts Institute of Technology

\({}^{4}\)Valence Labs

###### Abstract

Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce **E**quivariant **T**ransformer **Flow** (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. With the advantages of equivariance and flow matching, ET-Flow significantly increases the precision and physical validity of the generated conformers, while being a lighter model and faster at inference. Code is available https://github.com/shenoynikhil/ETFlow.

## 1 Introduction

Generating low-energy 3D representations of molecules, called _conformers_, from the molecular graph is a fundamental task in computational chemistry as the 3D structure of a molecule is responsible for several biological, chemical and physical properties (Guimaraes et al., 2012; Schutt et al., 2018, 2021; Gasteiger et al., 2020; Axelrod and Gomez-Bombarelli, 2023). Conventional approaches to molecular conformer generation consist of stochastic and systematic methods. While stochastic methods such as Molecular Dynamics (MD) accurately generate conformations, they can be slow, cost-intensive, and have low sample diversity (Shim and MacKerell Jr, 2011; Ballard et al., 2015; De Vivo et al., 2016; Hawkins, 2017; Pracht et al., 2020). Systematic (rule-based) methods (Hawkins et al., 2010; Bolton et al., 2011; Li et al., 2007; Miteva et al., 2010; Cole et al., 2018; Lagorce et al., 2009) that rely on torsional profiles and knowledge base of fragments are much faster but become less accuratewith larger molecules. Therefore, there has been an increasing interest in developing scalable and accurate generative modeling methods in molecular conformer generation.

Existing machine learning based approaches use diffusion models (Ho et al., 2020; Song and Ermon, 2019) to sample diverse and high quality samples given access to low-energy conformations. Prior methods typically fall into two categories: diffusing the atomic coordinates in the Cartesian space (Xu et al., 2022; Wang et al., 2024) or diffusing along the internal geometry such as pairwise distances, bond angles, and torsion angles (Ganea et al., 2021; Jing et al., 2022).

Early approaches based on diffusion (Shi et al., 2021; Luo et al., 2021; Xu et al., 2022) faced challenges such as lengthy inference and training times as well as having lower accuracy compared to cheminformatics methods. Torsional Diffusion (Jing et al., 2022) was the first to outperform cheminformatics methods by diffusing only on torsion angles after producing an initial conformer with the chemoinformatics tool RDKiT. This reliance on RDKiT structures instead of employing an end-to-end approach comes with several limitations, such as restricting the tool to applications where the local structures produced by RDKiT are of sufficient accuracy. Unlike prior approaches, the current state-of-the-art MCF (Wang et al., 2024) proposes a domain-agnostic approach by learning to diffuse over functions by scaling transformers and learning soft inductive bias from the data (Zhuang et al., 2022). Consequently, it comes with drawbacks such as high computational demands due to large number of parameters, limited sample efficiency from a lack of inductive biases like euclidean symmetries, and potential difficulties in scenarios with sparse data -- a common challenge in this field.

In this paper, we propose **E**quivariant **T**ransformer **F**low (ET-Flow), a simple yet powerful flow-matching model designed to generate low-energy 3D structures of small molecules with minimal assumptions. We utilize flow matching (Lipman et al., 2022; Albergo et al., 2023; Liu et al., 2022), which enables the learning of arbitrary probability paths beyond diffusion paths, enhancing both training and inference efficiency compared to conventional diffusion generative models. Departing from traditional equivariant architectures like EGNN (Satorras et al., 2021), we adopt an Equivariant Transformer (Tholke and De Fabritiis, 2022) to better capture geometric features. Additionally, our method integrates a Harmonic Prior (Jing et al., 2023; Stark et al., 2023), leveraging the inductive bias that atoms connected by a bond should be in close proximity. We further optimize our flow matching objective by initially conducting rotational alignment on the harmonic prior, thereby constructing shorter probability paths between source and target distributions at minimal computational cost.

Figure 1: (a) Overview of ET-Flow. The model predicts a conditional vector field \(}\) using interpolated positions (\(x_{t}\)), molecular structure (\(G\)), and time-step (\(t\)). Samples are drawn from the harmonic prior (\(x_{0} p_{0}\)) and then rotationally aligned with the samples from data (\(x_{1} p_{1}\)). A conditional probability path is constructed between pairs of \(x_{0}\) and \(x_{1}\), and \(x_{t}\) is then sampled from this path at a random time \(t\). (b) The ET-Flow architecture consists of a representation module based on the TorchMD-NET architecture (Tholke and De Fabritiis, 2022) and an equivariant vector output module. For detailed architecture and input preprocessing information, see Section A.1.

Our contributions can be summarized as follows:

1. We obtain state-of-the-art precision for molecule conformer prediction, resulting in more physically realistic and reliable molecules for practitioners. We improve upon the previous methods by a large margin on ensemble property prediction.
2. We highlight the effectiveness of incorporating equivariance and more informed priors in generating physically-grounded molecules in our simple yet well-engineered method.
3. Our parameter-efficient model requires orders of magnitude fewer sampling steps than GeoDiff (Xu et al., 2022) and has significantly fewer parameters than MCF (Wang et al., 2024).

## 2 Background

**Diffusion Generative Models.** Diffusion models (Song and Ermon, 2019; Song et al., 2020; Ho et al., 2020) enables a high-quality and diverse sampling from an unknown data distribution by approximating the Stochastic Differential Equation(SDE) that maps a simple density i.e. Gaussian to the unknown data density. Concretely, it involves training a neural network to learn the score, represented as \(_{} p_{t}()\) of the diffused data. During inference, the model generates sample by iteratively solving the reverse SDE. However, diffusion models have inherent drawbacks, as they (i) require on longer training times (ii) are restricted to specific probability paths and (iii) depend on the use of complicated tricks to speed up sampling (Song et al., 2020; Zhang and Chen, 2022).

**Flow Matching.** Flow Matching (Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022) provides a general framework to learn Continuous normalizing flows (CNFs) while improving upon diffusion models in simplicity, generality, and inference speed in several applications. Through simple regression against the vector field reminiscent of the score-matching objective in diffusion models, Flow matching has enabled a fast, simulation-free training of CNFs. Several subsequent studies have then expanded the scope of flow matching objective to manifolds (Chen and Lipman, 2024), arbitrary sources (Pooladian et al., 2023), and conditional flow matching with arbitrary transport maps and optimal couplings between source and target samples (Tong et al., 2023).

**Molecular Conformer Generation.** Various machine learning (ML) based approaches (Kingma and Welling, 2013; Liberti et al., 2014; Dinh et al., 2016; Simm and Hernandez-Lobato, 2019; Shi et al., 2021; Luo et al., 2021; Xu et al., 2021; Ganea et al., 2021; Xu et al., 2022; Jing et al., 2022; Wang et al., 2024) have been developed to improve upon the limitations of conventional methods, among which the most advanced are TorsionDiff (Jing et al., 2022) and Molecular Conformer Fields (MCF) (Wang et al., 2024). TorsionDiff designs a diffusion model on the torsion angles while incorporating the local structure from RDKiT ETKDG (Riniker and Landrum, 2015). MCF trains a diffusion model over functions that map elements from the molecular graph to points in 3D space.

**Equivariant Architectures for Atomistic Systems.** Inductive biases play an important role in generalization and sample efficiency. In the case of 3D atomistic modelling, one example of a useful inductive bias is the euclidean group \(SO(3)\) which represents rotation equivariance in 3D space. Recently, various equivariant architectures (Duval et al., 2023) have been developed that act on both Cartesian (Satorras et al., 2021; Tholke and De Fabritis, 2022; Simeon and De Fabritis, 2024; Du et al., 2022; Frank et al., 2022) and spherical basis (Musaelian et al., 2023; Batatia et al., 2022; Fuchs et al., 2020; Liao et al., 2023; Passaro and Zitnick, 2023; Anderson et al., 2019; Thomas et al., 2018). For molecular conformer generation, initial methods like ConfGF, DGSM utilize invariant networks as they act upon inter-atomic distances, whereas the use of equivariant GNNs have been used in GeoDiff (Xu et al., 2022) and Torsional Diffusion (Jing et al., 2022). GeoDiff utilizes EGNN (Satorras et al., 2021), a Cartesian basis equivariant architecture while Torsional Diffusion uses Tensor Field Networks (Thomas et al., 2018) to output pseudoscalars.

## 3 Method

We design ET-Flow, a scalable equivariant model that generates energy-minimized conformers given a molecular graph. In this section, we layout the framework to achieve this objective by detailing the generative process in flow matching, the rotation alignment between distributions, stochastic sampling, and finally the architecture details.

**Preliminaries** We define notation that we use throughout this paper. Inputs are continuous atom positions \(^{N 3}\) where \(N\) is the number of atoms. We use the notation \(v_{t}()\) interchangeably with \(v(t,)\) for vector field.

### Flow Matching

The aim is to learn a time-dependent vector field \(v_{t}(x):^{N 3}^{N 3}\) associated with the transport map \(X_{t}:^{N 3}^{N 3}\) that pushes forward samples from a base distribution \(_{0}\), often an easy-to-sample distribution, to samples from a more complex target distribution \(_{1}\), the low-energy conformations of a molecule. This can be defined as an ordinary differential equation (ODE),

\[_{t}()=v_{t}(X_{t}()), 28.452756ptX_{t=0}= _{0},\] (1)

where \(x_{0}_{0}\). We can construct the \(v_{t}\) via a time-differentiable interpolation between samples from \(_{0}\) and \(_{1}\) that gives rise to a probability path \(_{t}\) that we can easily sample (Lipman et al., 2022; Liu et al., 2022; Albergo and Vanden-Eijinden, 2023; Tong et al., 2023). The general interpolation between samples \(x_{0}_{0}\) and \(x_{1}_{1}\) can be defined as:

\[I_{t}(_{0},_{1})=_{t}_{1}+_{t} _{0}.\] (2)

Given this interpolant that couples \(_{0}\) and \(_{1}\), we can define the conditional probability path as \(_{t}(|_{0},_{1})=(|I_{ t}(_{0},_{1}),_{t}^{2})\), and the vector field can be computed as \(v_{t}()=_{t}_{t}(|_{0},_{ 1})\) which has the following form

\[v_{t}()=_{t}_{1}+_{t}_{0 }+_{t} 28.452756pt(0, ).\] (3)

Here we use \(_{t}\) as a shorthand notation for \(_{t}_{t}\), and similarly we apply the same notation to \(\) and \(\). In our work, we use linear interpolation where \(_{t}=t\), \(_{t}=1-t\), and \(_{t}=\), resulting in the vector field

\[v_{t}()=_{1}-_{0}+} .\] (4)

Now, we can define the objective function for learning a vector field \(v_{}()\) that generates a probability path \(_{t}\) between a base density \(_{0}\) and the target density \(_{1}\) as,

\[=_{t(0,1),_{t}(_{0}, _{1})}\|v(t,)-v_{}(t,)\|^{2}.\] (5)

For training, we sample (i) \(_{0}_{0}\), \(_{1}_{1}\), and \(t(0,1)\), (ii) interpolate according to Equation 2, (iii) add noise from a standard Gaussian, and (iv) minimize the loss defined in Equation 5. For sampling, we sample \(_{0}_{0}\) and integrate from \(t=0\) to \(t=1\) using the Euler's method. At each time-step, the Euler solver iteratively predicts the vector field for \(_{t}\) and updates its position \(_{t+ t}=_{t}+v_{}(t,) t\). More details on the training and sampling algorithms are provided in Appendix B.

### Alignment

Several previous works (Tong et al., 2023; Klein et al., 2024; Jing et al., 2024; Song et al., 2024) demonstrate that constructing a straighter path between base distribution \(_{0}\) and target distribution \(_{1}\) minimizes the transport costs and improves performance. In our work, we reduce the transport costs between samples from the harmonic prior \(_{0}\) and samples from the data distribution \(_{1}\) by rotationally aligning them using the Kabsch algorithm (Kabsch, 1976) similar to (Klein et al., 2024; Jing et al., 2024). This approach leads to faster convergence and reduces the path length between atoms by leveraging the similarity in "shape" of the samples as seen in Figure 0(a) without incurring high computational cost.

### Stochastic Sampling

We employ a variant of the stochastic sampling technique inspired by (Karras et al., 2022). Specifically, we inject noise at each time step to construct an intermediate state, evaluate the vector field from the intermediate state, and then perform the deterministic ODE step from the noisy state. The original method utilizes a second-order integration, which averages the denoiser output at the noisy intermediate state and the state at the next time step after integration.

In our experiment, we use the stochastic sampler without this second-order correction term, which empirically provided a performance boost comparable to the second-order method. We apply stochastic sampling only during the final part of the integration steps, specifically within the range \(t[0.8,1.0]\). This helps prevent drifting towards overpopulated density regions and improves the quality of the samples (Karras et al., 2022). Stochastic sampling has improved both diversity and accuracy of the generated conformers, measured by Coverage and Average Minimum RMSD (AMR) respectively as shown in Table 1. Detailed information on the stochastic sampling algorithm is provided in algorithm B.

### Chirality Correction

While generating conformations, it is necessary to take account of the stereochemistry of atoms bonded to four distinct groups also referred to as tetrahedral chiral centers. To generate conformations with the correct chirality, we propose a simple _post hoc_ trick as done in GeoMol (Ganea et al., 2021). We compare the oriented volume (OV) (Equation 6) of the generated conformation and the required orientation with the RDKit tags. In the case of a mismatch, we simply flip the conformation against the z-axis. This correction step can be efficiently performed as a batched operation since it involves a simple comparison with the required RDKit tags and an inversion of position if necessary.

\[(_{1},_{2},_{3},_{4})=sign 1&1&1&1\\ x_{1}&x_{2}&x_{3}&x_{4}\\ y_{1}&y_{2}&y_{3}&y_{4}\\ z_{1}&z_{2}&z_{3}&z_{4}.\] (6)

We also consider an alternative approach for chirality correction. Instead of using the _post hoc_ correction with our \(O(3)\) equivariant architecture, we slightly tweak our architecture to make it \(SO(3)\) equivariant by introducing a cross product term in the update layers. We compare these methods on both the GEOM-DRUGS and GEOM-QM9 dataset in Table 1 and Table 2. Our base method (ET-Flow) corresponds to using the _post hoc_ correction whereas the \(SO(3)\) variant is referred by ET-Flow-\(SO(3)\). We empirically observe that using an additional chirality correction step is not only computationally efficient, but also performs better. We provide details on the architectural modification and proof of \(SO(3)\) equivariance in Section A.1 and Section C.1 respectively.

### Architecture

ET-Flow (Figure 0(b)) consists of two main components: (1) a representation module based on the equivariant transformer architecture from TorchMD-NET (Tholke and De Fabritiis, 2022) and (2) the equivariant vector output module. In the representation module, an embedding layer encodes the inputs (atomic positions, atomic numbers, atom features, bond features and the time-step) into a set of invariant features. Initial equivariant features are constructed using normalized edge vectors where the edges are constructed using a radius graph of \(10\) angstrom and the bonds from the 2D molecular graph. Then, a series of equivariant attention-based layers update both the invariant and equivariant features using a multi-head attention mechanism. Finally, the vector field is produced by the output layer, which updates the equivariant features using gated equivariant blocks (Schutt et al., 2018). Given that TorchMD-NET was originally designed for modeling neural network potentials, we implement several modifications to its architecture to better suit generative modeling, as detailed in Section A.1.

## 4 Experiments

We empirically evaluate ET-Flow by comparing the generated and ground-truth conformers in terms of distance-based RMSD (Section 4.2) and chemical property based metrics (Section 4.4). We

Figure 2: Stochastic sampling procedure used in inference. Noise is added to the positions \(x_{t}\) indicated by the purple line, resulting in \(_{t}\). Then, the model predicts the vector field \(_{t}\) from \(_{t}\) instead of \(x_{t}\) indicted by the yellow line and updates \(_{t}\) using \(_{t}\) to get \(x_{t+1}\).

present the general experimental setups in Section4.1. The implementation details are provided in AppendixA.

### Experimental Setup

**Dataset**: We conduct our experiments on the GEOM dataset (Axelrod and Gomez-Bombarelli, 2022), which offers curated conformer ensembles produced through meta-dynamics in CREST (Pracht et al., 2024). Our primary focus is on GEOM-DRUGS, the most extensive and pharmacologically relevant subset comprising 304k drug-like molecules, each with an average of 44 atoms. We use a train/validation/test (\(243473\)/\(30433\)/\(1000\)) split as provided in (Ganea et al., 2021) Additionally, we train and test model on GEOM-QM9, a subset of smaller molecules with an average of 11 atoms. Finally, in order to assess the model's ability to generalize to larger molecules, we evaluate the model trained on GEOM-DRUGS on a GEOM-XL dataset, a subset of large molecules with more than 100 atoms. The results for GEOM-QM9 and GEOM-XL can be found in the AppendixD.

**Evaluation**: Our evaluation methodology is similar to that of (Jing et al., 2022). First, we look at RMSD based metrics like Coverage and Average Minimum RMSD (AMR) between generated and ground truth conformer ensembles. For this, we generate \(2K\) conformers for a molecule with \(K\) ground truth conformers. Second, we look at chemical similarity using properties like Energy (\(E\)), dipole moment (\(\)), HOMO-LUMO gap (\(\)) and the minimum energy (\(E_{}\)) calculated using xTB (Bannwarth et al., 2019).

**Baselines**: We benchmark ET-Flow against leading approaches outlined in Section2. Specifically, we assess the performance of GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), Torsional Diffusion (Jing et al., 2022), and MCF (Wang et al., 2024). Notably, the most recent among these, MCF, has demonstrated superior performance across evaluation metrics compared to its predecessors. It's worth mentioning that GeoDiff initially utilized a limited subset of the GEOM-DRUGS dataset; thus, for a fair comparison, we consider its re-evaluated performance as presented in (Jing et al., 2022).

### Ensemble RMSD

As shown in Table1 and Table2, ET-Flow outperforms all preceding methodologies and demonstrates competitive performance with the previous state-of-the-art, MCF (Wang et al., 2024). Despite being significantly smaller with only 8.3M parameters, ET-Flow shows a substantial improvement in the quality of generated conformers, as evidenced by superior Precision metrics across all MCF models, including the largest MCF-L. When compared to MCF-S, which is closer in size, ET-Flow achieves markedly better Precision while the impact on Recall is less significant and limited to Recall Coverage. Notably, our Recall AMR remains competitive with much bigger MCF-B, underscoring the inherent advantage of our method in accurately predicting overall structures.

    &  &  \\  &  &  &  &  \\   & mean & median & mean & median & mean & median & mean & median \\  GeoDiff & 42.10 & 37.80 & 0.835 & 0.809 & 24.90 & 14.50 & 1.136 & 1.090 \\ GeoMol & 44.60 & 41.40 & 0.875 & 0.834 & 43.00 & 36.40 & 0.928 & 0.841 \\ Torsional Diff. & 72.70 & 80.00 & 0.582 & 0.565 & 55.20 & 56.90 & 0.778 & 0.729 \\ MCF - S (13M) & 79.4 & 87.5 & 0.512 & 0.492 & 57.4 & 57.6 & 0.761 & 0.715 \\ MCF - B (62M) & 84.0 & 91.5 & 0.427 & 0.402 & 64.0 & 66.2 & 0.667 & 0.605 \\ MCF - L (242M) & **84.7** & **92.2** & **0.390** & **0.247** & 66.8 & 71.3 & 0.618 & 0.530 \\  ET-Flow (8.3M) & 79.53 & 84.57 & 0.452 & 0.419 & 74.38 & 81.04 & 0.541 & 0.470 \\ ET-Flow - SS (8.3M) & 79.62 & 84.63 & 0.439 & 0.406 & **75.19** & **81.66** & **0.517** & **0.442** \\ ET-Flow - \(SO(3)\) (9.1M) & 78.18 & 83.33 & 0.480 & 0.459 & 67.27 & 71.15 & 0.637 & 0.567 \\   

Table 1: Molecule conformer generation results on GEOM-DRUGS (\(=0.75\)Å). ET-Flow - SS is ET-Flow with stochastic sampling and ET-Flow - \(SO(3)\) is ET-Flow using the \(SO(3)\) architecture for chirality correction. For ET-Flow, ET-Flow-SS and ET-Flow-\(SO(3)\), we sample conformations over \(50\) time-steps.

### Coverage Threshold Plots

We compare the coverage metrics of ET-Flow against Torsional diffusion (Jing et al., 2022) and MCF (Wang et al., 2024) against a wide range of thresholds on the GEOM DRUGS dataset in Figure 3. ET-Flow consistently outperforms previous methods in precision-based metrics. In terms of recall, our approach demonstrates better performance than Torsional Diffusion across all thresholds. Despite MCF performing better at higher thresholds, ET-Flow outperforms in the lower thresholds, underscoring its proficiency in generating accurate conformer predictions.

### Ensemble Properties

RMSD provides a geometric measure for assessing ensemble quality, but it is also essential to consider the chemical similarity between generated and ground truth ensembles. For a random 100-molecule subset of the test set of GEOM-DRUGS, if a molecule has \(K\) ground truth conformers, we generate a minimum of \(2K\) and a maximum of 32 conformers per molecule. These conformers are then relaxed using GFN2-xTB (Bannwarth et al., 2019), and the Boltzmann-weighted properties of the generated and ground truth ensembles are compared. Specifically, using xTB (Bannwarth et al.,

    &  \\    &  &  &  &  \\   & mean & median & mean & median & mean & median & mean & median \\  CGCF & 69.47 & 96.15 & 0.425 & 0.374 & 38.20 & 33.33 & 0.711 & 0.695 \\ GeoDiff & 76.50 & **100.00** & 0.297 & 0.229 & 50.00 & 33.50 & 1.524 & 0.510 \\ GeoMol & 91.50 & **100.00** & 0.225 & 0.193 & 87.60 & **100.00** & 0.270 & 0.241 \\ Torsional Diff. & 92.80 & **100.00** & 0.178 & 0.147 & 92.70 & **100.00** & 0.221 & 0.195 \\ MCF & 95.0 & **100.00** & 0.103 & 0.044 & 93.7 & **100.00** & 0.119 & 0.055 \\  ET-Flow & **96.47** & **100.00** & **0.073** & 0.047 & **94.05** & **100.00** & **0.098** & **0.039** \\ ET-Flow - \(SO(3)\) & 95.98 & **100.00** & 0.076 & **0.030** & 92.10 & **100.00** & 0.110 & 0.047 \\   

Table 2: Molecule conformer generation results on GEOM-QM9 (\(=0.5\)Å). ET-Flow - \(SO(3)\) is ET-Flow using the \(SO(3)\) architecture for chirality correction. For both ET-Flow and ET-Flow-\(SO(3)\), we sample conformations over \(50\) time-steps.

Figure 3: Recall and Precision Coverage result on GEOM-DRUGS as a function of the threshold distance. ET-Flow outperforms TorsionDiff by a large margin especially in a lower threshold region. We emphasize the better performance of ET-Flow at lower thresholds in both Recall and Precision metrics.

2019), we compute properties such as energy (\(E\)), dipole moment (\(\)), HOMO-LUMO gap (\(\)), and the minimum energy (\(E_{min}\)). Table 3 illustrates the median errors for ET-Flow and the baselines, highlighting our method's capability to produce chemically accurate ensembles. Notably, we achieve significant improvements over both TorsionDiff and MCF across all evaluated properties.

### Inference Steps Ablation

In Table 1, our sampling process with ET-Flow utilizes 50 inference steps. To evaluate the method's performance under constrained computational resources, we conducted an ablation study by progressively reducing the number of inference steps. Specifically, we sample for \(5\), \(10\) and \(20\) time-steps. The results on GEOM-DRUGS are presented in Table 4. We observed minimal performance degradation with a decrease in the number of steps. Notably, ET-Flow demonstrates high efficiency, maintaining performance across all precision and recall metrics even with as few as 5 inference steps. Interestingly, ET-Flow with 5 steps still achieves superior precision metrics compared to all existing methods. This underscores ET-Flow's ability to generate high-quality conformations while operating within limited computational budgets.

### Sampling Efficiency

We demonstrate the ability of ET-Flow to generate samples efficiently. We evaluate the inference time per molecule over varying number of time steps and report the average time across 1000 random samples from the test set of GEOM-DRUGS. Figure 4 shows that ET-Flow outperforms Torsional diffusion (Jing et al., 2022) in inference across all time steps. While ET-Flow may not achieve the fastest raw inference times (potentially due to MCF variants benefiting from optimized CUDA kernels for attention), it maintains competitive speeds while ensuring higher precision. We suspect that concurrent work on improving equivariant operations with optimized CUDA kernels (Lee et al., 2024) should lead to similar efficiency gains as seen in transformer-based architectures.

Figure 4: Sampling efficiency as a measure of the quality of Inference time with respect to the number of time steps on GEOM-DRUGS.

    & \(E\) & \(\) & \(\) & \(E_{}\) \\  OMEGA & 0.68 & 0.66 & 0.68 & 0.69 \\ GeoDiff & 0.31 & 0.35 & 0.89 & 0.39 \\ GeoMol & 0.42 & 0.34 & 0.59 & 0.40 \\ Torsional Diff. & 0.22 & 0.35 & 0.54 & 0.13 \\ MCF & 0.68\(\)0.06 & 0.28\(\) 0.05 & 0.63\(\)0.05 & 0.04\(\)0.00 \\  ET-Flow & **0.18\(\)0.01** & **0.18\(\)0.01** & **0.35\(\)0.06** & **0.02\(\)0.00** \\   

Table 3: Median averaged errors of ensemble properties between sampled and generated conformers (\(E\), \(\), \(E_{min}\) in kcal/mol, and \(\) in debye).

ET-Flow effectively balances performance and speed, making it ideal for tasks that require high sample quality with efficient computation. With the ability to generate high-quality samples in fewer time steps, e.g., 5 time steps, as indicated in Table 4, ET-Flow is well-suited for scenarios demanding a large number of samples, as fewer steps lead to lower inference time per molecule. Additionally, we encountered difficulties running MCF-L for 20 and 50 steps, so those results have not been included. In summary, ET-Flow demonstrates efficient sampling, balancing precision and speed, making it highly effective for generating high-quality molecular samples while remaining competitive in inference time.

## 5 Conclusion

In this paper, we present our simple and scalable method ET-Flow, which utilizes an equivariant transformer with flow matching to achieve state-of-the-art performance on multiple molecular conformer generation benchmarks. By incorporating inductive biases, such as equivariance, and enhancing probability paths with a harmonic prior and RMSD alignment, we significantly improve the precision of the generated molecules, and consequently generate more physically plausible molecules. Importantly, our approach maintains parameter and speed efficiency, making it not only effective but also accessible for practical high-throughput applications.

## 6 Limitations And Future Works

While ET-Flow demonstrates competitive performance in molecular conformer generation, there are areas where it can be enhanced. One such area is the recall metrics, which capture the diversity of generated conformations. Another area is the use of an additional chirality correction step that is used to predict conformations with the desired chirality. Moreover, although our performance on the GEOM-XL dataset is comparable to MCF-S and TorsionDiff, there is still room for improvement.

We propose three future directions here. First, we observe during experiments that a well-designed sampling process incorporating stochasticity can enhance the quality and diversity of generated samples. An extension of our current approach could involve using Stochastic Differential Equations (SDEs), which utilize both vector field and score in the integration process, potentially improving the diversity of samples. Second, we propose to scale the number of parameters of ET-Flow, which has not only been shown to be useful across different domains of deep learning, but has also shown to be useful in molecular conformer generation for MCF (Wang et al., 2024). Third, to better handle the chirality problem, we aim to explore alternatives for incorporating _SO(3)_-equivariance into the model in the future.

#### Acknowledgements

The authors sincerely thank Cristian Gabellini, Jiarui Ding, and the NeurIPS reviewers for the insightful discussions and feedback. Resources used in completing this research were provided by Valence Labs. Furthermore, we acknowledge a grant for student supervision received by Mila - Quebec's AI institute - and financed by the Quebec ministry of Economy.

    &  &  \\   &  &  &  &  \\   & mean & median & mean & median & mean & median & mean & median \\  ET-Flow (5 Steps) & 77.84 & 82.21 & 0.476 & 0.443 & 74.03 & 80.8 & 0.55 & 0.474 \\ ET-Flow (10 Steps) & 79.05 & 84.00 & 0.451 & 0.415 & 74.64 & **81.38** & 0.533 & 0.457 \\ ET-Flow (20 Steps) & 79.29 & 84.04 & **0.449** & **0.413** & **74.89** & 81.32 & **0.531** & **0.454** \\ ET-Flow (50 Steps) & **79.53** & **84.57** & 0.452 & 0.419 & 74.38 & 81.04 & 0.541 & 0.470 \\   

Table 4: Ablation over number of inference steps on GEOM-DRUGS (\(=0.75\)Å). Performance of ET-Flow at \(5\) steps is competent across all metrics while also retaining state-of-the-art performance on precision metrics when compared with previous methods.