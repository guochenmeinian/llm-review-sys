# Restart Sampling for Improving Generative Processes

Yilun Xu

MIT

ylxu@mit.edu

&Mingyang Deng

MIT

dengm@mit.edu

&Xiang Cheng1

MIT

chengx@mit.edu

&Yonglong Tian

Google Research

yonglong@google.com

&Ziming Liu

MIT

zmliu@mit.edu

&Tommi Jaakkola

MIT

tommi@csail.mit.edu

Equal Contribution.

###### Abstract

Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called _Restart_ in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet \(64 64\). In addition, it attains significantly better sample quality than ODE samplers within comparable sampling times. Moreover, Restart better balances text-image alignment/visual quality versus diversity than previous samplers in the large-scale text-to-image Stable Diffusion model pre-trained on LAION \(512 512\). Code is available at https://github.com/Newbeeer/diffusion_restart_sampling

## 1 Introduction

Deep generative models based on differential equations, such as diffusion models and Poission flow generative models, have emerged as powerful tools for modeling high-dimensional data, from image synthesis  to biological data . These models use iterative backward processes that gradually transform a simple distribution (_e_.\(g\)., Gaussian in diffusion models) into a complex data distribution by solving a differential equations. The associated vector fields (or drifts) driving the evolution of the differential equations are predicted by neural networks. The resulting sample quality can be often improved by enhanced simulation techniques but at the cost of longer sampling times.

Prior samplers for simulating these backward processes can be categorized into two groups: ODE-samplers whose evolution beyond the initial randomization is deterministic, and SDE-samplers where the generation trajectories are stochastic. Several works  show that these samplers demonstrate their advantages in different regimes, as depicted in Fig. 1(b). ODE solvers  result in smaller discretization errors, allowing for decent sample quality even with larger step sizes (_i_.\(e\)., fewer number of function evaluations (NFE)). However, their generation quality plateaus rapidly. In contrast, SDE achieves better quality in the large NFE regime, albeit at the expense of increased sampling time. To better understand these differences, we theoretically analyze SDE performance: thestochasticity in SDE contracts accumulated error, which consists of both the discretization error along the trajectories as well as the approximation error of the learned neural network relative to the ground truth drift (_e.g._, score function in diffusion model ). The approximation error dominates when NFE is large (small discretization steps), explaining the SDE advantage in this regime. Intuitively, the stochastic nature of SDE helps "forget" accumulated errors from previous time steps.

Inspired by these findings, we propose a novel sampling algorithm called _Restart_, which combines the advantages of ODE and SDE. As illustrated in Fig. 1, the Restart sampling algorithm involves \(K\) repetitions of two subroutines in a pre-defined time interval: a _Restart forward process_ that adds a substantial amount of noise, akin to "restarting" the original backward process, and a _Restart backward process_ that runs the backward ODE. The Restart algorithm separates the stochasticity from the drifts, and the amount of added noise in the Restart forward process is significantly larger than the small single-step noise interleaving with drifts in previous SDEs such as , thus amplifying the contraction effect on accumulated errors. By repeating the forward-backward cycle \(K\) times, the contraction effect introduced in each Restart iteration is further strengthened. The deterministic backward processes allow Restart to reduce discretization errors, thereby enabling step sizes comparable to ODE. To maximize the contraction effects in practice, we typically position the Restart interval towards the end of the simulation, where the accumulated error is larger. Additionally, we apply multiple Restart intervals to further reduce the initial errors in more challenging tasks.

Experimentally, Restart consistently surpasses previous ODE and SDE solvers in both quality and speed over a range of NFEs, datasets, and pre-trained models. Specifically, Restart accelerates the previous best-performing SDEs by \(10\) fewer steps for the same FID score on CIFAR-10 using VP  (\(2\) fewer steps on ImageNet \(64 64\) with EDM ), and outperforms ODE solvers even in the small NFE regime. When integrated into previous state-of-the-art pre-trained models, Restart further improves performance, achieving FID scores of 1.88 on unconditional CIFAR-10 with PFGM++ , and 1.36 on class-conditional ImageNet \(64 64\) with EDM. To the best of our knowledge, these are the best FID scores obtained on commonly used UNet architectures for diffusion models without additional training. We also apply Restart to the practical application of text-to-image Stable Diffusion model  pre-trained on LAION \(512 512\). Restart more effectively balances text-image alignment/visual quality (measured by CLIP/Aesthetic scores) and diversity (measured by FID score) with a varying classifier-free guidance strength, compared to previous samplers.

Our contributions can be summarized as follows: **(1)** We investigate ODE and SDE solvers and theoretically demonstrate the contraction effect of stochasticity via an upper bound on the Wasserstein distance between generated and data distributions (Sec 3); **(2)** We introduce the Restart sampling, which better harnesses the contraction effect of stochasticity while allowing for fast sampling. The sampler results in a smaller Wasserstein upper bound (Sec 4); **(3)** Our experiments are consistent with the theoretical bounds and highlight Restart's superior performance compared to previous samplers on standard benchmarks in terms of both quality and speed. Additionally, Restart improves the trade-off between key metrics on the Stable Diffusion model (Sec 5).

Figure 1: **(a) Illustration of the implementation of drift and noise terms in ODE, SDE, and Restart. **(b)** Sample quality versus number of function evaluations (NFE) for different approaches. ODE (Green) provides fast speeds but attains only mediocre quality, even with a large NFE. SDE (Yellow) obtains good sample quality but necessitates substantial sampling time. In contrast to ODE and SDE, which have their own winning regions, Restart (Red) achieves the best quality across all NFEs.

## 2 Background on Generative Models with Differential Equations

Many recent successful generative models have their origin in physical processes, including diffusion models [9; 23; 13] and Poisson flow generative models [27; 28]. These models involve a forward process that transforms the data distribution into a chosen smooth distribution, and a backward process that iteratively reverses the forward process. For instance, in diffusion models, the forward process is the diffusion process with no learned parameters:

\[x=(t)(t)}W_{t},\]

where \((t)\) is a predefined noise schedule increasing with \(t\), and \(W_{t}^{d}\) is the standard Wiener process. For simplicity, we omit an additional scaling function for other variants of diffusion models as in EDM . Under this notation, the marginal distribution at time \(t\) is the convolution of data distribution \(p_{0}=p_{}\) and a Gaussian kernel, _i.e._, \(p_{t}=p_{0}*(,^{2}(t)_{d d})\). The prior distribution is set to \((,^{2}(T)_{d d})\) since \(p_{T}\) is approximately Gaussian with a sufficiently large \(T\). Sampling of diffusion models is done via a reverse-time SDE  or a marginally-equivalent ODE :

\[\] (1) \[\] (2)

where \(_{x} p_{t}(x)\) in the drift term is the score of intermediate distribution at time \(t\). W.l.o.g we set \((t)=t\) in the remaining text, as in . Both processes progressively recover \(p_{0}\) from the prior distribution \(p_{T}\) while sharing the same time-dependent distribution \(p_{t}\). In practice, we train a neural network \(s_{}(x,t)\) to estimate the score field \(_{x} p_{t}(x)\) by minimizing the denoising score-matching loss . We then substitute the score \(_{x} p_{t}(x)\) with \(s_{}(x,t)\) in the drift term of above backward SDE (Eq. (1))/ODE (Eq. (2)) for sampling.

Recent work inspired by electrostatics has not only challenged but also integrated diffusion models, notably PFGM/PFGM++, enhances performance in both image and antibody generation [27; 28; 10]. They interpret data as electric charges in an augmented space, and the generative processes involve the simulations of differential equations defined by electric field lines. Similar to diffusion models, PFGMs train a neural network to approximate the electric field in the augmented space.

## 3 Explaining SDE and ODE performance regimes

To sample from the aforementioned generative models, a prevalent approach employs general-purpose numerical solvers to simulate the corresponding differential equations. This includes Euler and Heun's 2nd method  for ODEs (e.g., Eq. (2)), and Euler-Maruyama for SDEs (e.g., Eq. (1)). Sampling algorithms typically balance two critical metrics: (1) the quality and diversity of generated samples, often assessed via the Frechet Inception Distance (FID) between generated distribution and data distribution  (lower is better), and (2) the sampling time, measured by the number of function evaluations (NFE). Generally, as the NFE decreases, the FID score tends to deteriorate across all samplers. This is attributed to the increased discretization error caused by using a larger step size in numerical solvers.

However, as illustrated in Fig. 1(b) and observed in previous works on diffusion models [23; 22; 13], the typical pattern of the quality vs time curves behaves differently between the two groups of samplers, ODE and SDE. When employing standard numerical solvers, ODE samplers attain a decent quality with limited NFEs, whereas SDE samplers struggle in the same small NFE regime. However, the performance of ODE samplers quickly reaches a plateau and fails to improve with an increase in NFE, whereas SDE samplers can achieve noticeably better sample quality in the high NFE regime. This dilemma raises an intriguing question: _Why do ODE samplers outperform SDE samplers in the small NFE regime, yet fall short in the large NFE regime?_

The first part of the question is relatively straightforward to address: given the same order of numerical solvers, simulation of ODE has significantly smaller discretization error compared to the SDE. For example, the first-order Euler method for ODE results in a local error of \(O(^{2})\), whereas the first-order Euler-Maruyama method for SDEs yeilds a local error of \(O(^{})\) (see _e.g._, Theorem 1 of ), where \(\) denotes the step size. As \(O(^{}) O(^{2})\), ODE simulations exhibit lower sampling errors than SDEs, likely causing the better sample quality with larger step sizes in the small NFE regime.

In the large NFE regime the step size \(\) shrinks and discretization errors become less significant for both ODEs and SDEs. In this regime it is the _approximation error_ -- error arising from aninaccurate estimation of the ground-truth vector field by the neural network \(s_{}\) -- starts to dominate the sampling error. We denote the discretized ODE and SDE using the learned field \(s_{}\) as ODE\({}_{}\) and SDE\({}_{}\), respectively. In the following theorem, we evaluate the total errors from simulating ODE\({}_{}\) and SDE\({}_{}\) within the time interval \([t_{},t_{}][0,T]\). This is done via an upper bound on the Wasserstein-1 distance between the generated and data distributions at time \(t_{}\). We characterize the accumulated initial sampling errors up until \(t_{}\) by total variation distances. Below we show that the inherent stochasticity of SDEs aids in contracting these initial errors at the cost of larger additional sampling error in \([t_{},t_{}]\). Consequently, SDE results in a smaller upper bound as the step size \(\) nears \(0\) (pertaining to the high NFE regime).

**Theorem 1** (Informal).: _Let \(t_{}\) be the initial noise level and \(p_{t}\) denote the true distribution at noise level \(t\). Let \(p_{t}^{_{}},p_{t}^{_{}}\) denote the distributions of simulating ODE\({}_{}\), SDE\({}_{}\) respectively. Assume that \( t[t_{},t_{}]\), \(\|x_{t}\|<B/2\) for any \(x_{t}\) in the support of \(p_{t}\), \(p_{t}^{_{}}\) or \(p_{t}^{_{}}\). Then_

\[W_{1}(p_{t_{}}^{_{}},p_{t_{}})  B TV(p_{t_{}}^{_{}},p_{t_{ }})+O(+_{approx})(t_{}-t_{ })\] \[(p_{t_{}}^{_{}},p_{t_ {}})}_{} )B TV(p_{t_{}}^{ _{}},p_{t_{}})}_{}+ }}+_{approx})(t_{}-t_{})}_{}\]

_In the above, \(U=BL_{1}/t_{}+L_{1}^{2}t_{}^{2}/t_{}^{2}\), \(<1\) is a contraction factor, \(L_{1}\) and \(_{approx}\) are uniform bounds on \(\|t_{}(x_{t},t)\|\) and the approximation error \(\|t_{x} p_{t}(x)-ts_{}(x,t)\|\) for all \(x_{t},t\), respectively. \(O()\) hides polynomial dependency on various Lipschitz constants and dimension._

We defer the formal version and proof of Theorem 1 to Appendix A.1. As shown in the theorem, the upper bound on the total error can be decomposed into upper bounds on the _contracted error_ and _additional sampling error_. \(TV(p_{t_{}}^{_{}},p_{t_{}})\) and \(TV(p_{t_{}}^{_{}},p_{t_{}})\) correspond to the initial errors accumulated from both approximation and discretization errors during the simulation of the backward process, up until time \(t_{}\). In the context of SDE, this accumulated error undergoes contraction by a factor of \(1- e^{-BL_{1}/t_{}-L_{1}^{2}t_{}^{2}/t_{} ^{2}}\) within \([t_{},t_{}]\), due to the effect of adding noise. Essentially, the minor additive Gaussian noise in each step can drive the generated distribution and the true distribution towards each other, thereby neutralizing a portion of the initial accumulated error.

The other term related to additional sampling error includes the accumulation of discretization and approximation errors in \([t_{},t_{}]\). Despite the fact that SDE incurs a higher discretization error than ODE (\(O()\) versus \(O()\)), the contraction effect on the initial error is the dominant factor impacting the upper bound in the large NFE regime where \(\) is small. Consequently, the upper bound for SDE is significantly lower. This provides insight into why SDE outperforms ODE in the large NFE regime, where the influence of discretization errors diminishes and the contraction effect dominates. In light of the distinct advantages of SDE and ODE, it is natural to ask whether we can combine their strengths. Specifically, can we devise a sampling algorithm that maintains a comparable level of discretization error as ODE, while also benefiting from, or even amplifying, the contraction effects induced by the stochasticity of SDE? In the next section, we introduce a novel algorithm, termed _Restart_, designed to achieve these two goals simultaneously.

## 4 Harnessing stochasticity with Restart

In this section, we present the Restart sampling algorithm, which incorporates stochasticity during sampling while enabling fast generation. We introduce the algorithm in Sec 4.1, followed by a theoretical analysis in Sec 4.2. Our analysis shows that Restart achieves a better Wasserstein upper bound compared to those of SDE and ODE in Theorem 1 due to greater contraction effects.

### Method

In the Restart algorithm, simulation performs a few repeated back-and-forth steps within a pre-defined time interval \([t_{},t_{}][0,T]\), as depicted in Figure 1(a). This interval is embedded into the simulation of the original backward ODE referred to as the _main backward process_, which runs from \(T\) to \(0\). In addition, we refer to the backward process within the Restart interval \([t_{},t_{}]\) as the _Restart backward process_, to distinguish it from the main backward process.

Starting with samples at time \(t_{}\), which are generated by following the main backward process, the Restart algorithm adds a large noise to transit the samples from \(t_{}\) to \(t_{}\) with the help of the forward process. The forward process does not require any evaluation of the neural network \(s_{}(x,t)\), as it is generally defined by an analytical perturbation kernel capable of transporting distributions from \(t_{}\) to \(t_{}\). For instance, in the case of diffusion models, the perturbation kernel is \((,((t_{})^{2}-(t_{})^{2} )_{d d})\). The added noise in this step induces a more significant contraction compared to the small, interleaved noise in SDE. The step acts as if partially restarting the main backward process by increasing the time. Following this step, Restart simulates the backward ODE from \(t_{}\) back to \(t_{}\) using the neural network predictions as in regular ODE. We repeat these forward-backward steps within \([t_{},t_{}]\) interval \(K\) times in order to further derive the benefit from contraction. Specifically, the forward and backward processes in the \(i^{}\) iteration (\(i\{0,,K-1\}\)) proceed as follows:

\[() x_{t_{}}^{i+1}=x_{t_{}}^{i}+ _{t_{} t_{}}\] (3) \[() x_{t_{}}^{i+1}=_{}(x_{t_{}}^{i+1},t_{ } t_{})\] (4)

where the initial \(x_{t_{}}^{0}\) is obtained by simulating the ODE until \(t_{}\): \(x_{t_{}}^{0}=_{}(x_{T},T t_{})\), and the noise \(_{t_{} t_{}}\) is sampled from the corresponding perturbation kernel from \(t_{}\) to \(t_{}\). The Restart algorithm not only adds substantial noise in the Restart forward process (Eq. (3)), but also separates the stochasticity from the ODE, leading to a greater contraction effect, which we will demonstrate theoretically in the next subsection. For example, we set \([t_{},t_{}]=[0.05,0.3]\) for the VP model  on CIFAR-10. Repetitive use of the forward noise effectively mitigates errors accumulated from the preceding simulation up until \(t_{}\). Furthermore, the Restart algorithm does not suffer from large discretization errors as it is mainly built from following the ODE in the Restart backward process (Eq. (4)). The effect is that the Restart algorithm is able to reduce the total sampling errors even in the small NFE regime. Detailed pseudocode for the Restart sampling process can be found in Algorithm 2, Appendix B.2.

### Analysis

We provide a theoretical analysis of the Restart algorithm under the same setting as Theorem 1. In particular, we prove the following theorem, which shows that Restart achieves a much smaller contracted error in the Wasserstein upper bound than SDE (Theorem 1), thanks to the separation of the noise from the drift, as well as the large added noise in the Restart forward process (Eq. (3)). The repetition of the Restart cycle \(K\) times further leads to a enhanced reduction in the initial accumulated error. We denote the intermediate distribution in the \(i^{}\) Restart iteration, following the discretized trajectories and the learned field \(s_{}\), as \(p_{t[t_{},t_{}]}^{_{}(i)}\).

**Theorem 2** (Informal).: _Under the same setting of Theorem 1, assume \(K(t_{}-t_{})}\) for some universal constant \(C\). Then_

\[(p_{t_{}}^{_{}(K)},p_{t_{}})}_{}\,TV(p_{t_{}}^{_{ }(0)},p_{t_{}})}_{}+ )(t_{}- t_{})}_{}\]

_where \(<1\) is the same contraction factor as Theorem 1. \(O()\) hides polynomial dependency on various Lipschitz constants, dimension._

Proof sketch.: To bound the total error, we introduce an auxiliary process \(q_{t[t_{},t_{}]}^{_{}(i)}\), which initiates from true distribution \(p_{t_{}}\) and performs the Restart iterations. This process differs from \(p_{t[t_{},t_{}]}^{_{}(i)}\) only in its initial distribution at \(t_{}\) (\(p_{t_{}}\) versus \(p_{t_{}}^{_{}(0)}\)). We bound the total error by the following triangular inequality:

\[(p_{t_{}}^{_{}(K)},p_{t_{}})}_{}(p_{t_{}}^{_{}(K)},q_{t_{ }}^{_{}(K)})}_{}+(q_{t_{}}^{_{}(K)},p_{t_{}})}_{}\]

To bound the contracted error, we construct a careful coupling process between two individual trajectories sampled from \(p_{t_{}}^{_{}(i)}\) and \(q_{t_{}}^{_{}(i)},i=0,,K-1\). Before these two trajectories converge, the Gaussian noise added in each Restart iteration is chosen to maximize the probability of the two trajectories mapping to an identical point, thereby maximizing the mixing rate in TV. After converging, the two processes evolve under the same Gaussian noise, and will stay converged as their drifts are the same. Lastly, we convert the TV bound to \(W_{1}\) bound by multiplying \(B\). The bound on the additional sampling error echoes the ODE analysis in Theorem 1: since the noise-injection and ODE-simulation stages are separate, we do not incur the higher discretization error of SDE.

We defer the formal version and proof of Theorem 2 to Appendix A.1. The first term in RHS bounds the contraction on the initial error at time \(t_{}\) and the second term reflects the additional sampling error of ODE accumulated across repeated Restart iterations. Comparing the Wasserstein upper bound of SDE and ODE in Theorem 1, we make the following three observations: _(1)_ Each Restart iteration has a smaller contraction factor \(1-\) compared to the one in SDE, since Restart separates the large additive noise (Eq. (3)) from the ODE (Eq. (4)). _(2)_ Restart backward process (Eq. (4)) has the same order of discretization error \(O()\) as the ODE, compared to \(O()\) in SDE. Hence, the Restart allows for small NFE due to ODE-level discretization error. _(3)_ The contracted error further diminishes exponentially with the number of repetitions \(K\) though the additional error increases linearly with \(K\). It suggests that there is a sweet spot of \(K\) that strikes a balance between reducing the initial error and increasing additional sampling error. Ideally, one should pick a larger \(K\) when the initial error at time \(t_{}\) greatly outweigh the incurred error in the repetitive backward process from \(t_{}\) to \(t_{}\). We provide empirical evidences in Sec 5.2.

While Theorem 1 and Theorem 2 compare the upper bounds on errors of different methods, we provide empirical validation in Section 5.1 by directly calculating these errors, showing that the Restart algorithm indeed yields a smaller total error due to its superior contraction effects. The main goal of Theorem 1 and Theorem 2 is to study how the already accumulated error changes using different samples, and to understand their ability to self-correct the error by stochasticity. In essence, these theorems differentiate samplers based on their performance post-error accumulation. For example, by tracking the change of accumulated error, Theorem 1 shed light on the distinct "winning regions" of ODE and SDE: ODE samplers have smaller discretization error and hence excel at the small NFE regime. In contrast, SDE performs better in large NFE regime where the discretization error is negligible and its capacity to contract accumulated errors comes to the fore.

### Practical considerations

The Restart algorithm offers several degrees of freedom, including the time interval \([t_{},t_{}]\) and the number of restart iterations \(K\). Here we provide a general recipe of parameter selection for practitioners, taking into account factors such as the complexity of the generative modeling tasks and the capacity of the network. Additionally, we discuss a stratified, multi-level Restart approach that further aids in reducing simulation errors along the whole trajectories for more challenging tasks.

**Where to Restart?** Theorem 2 shows that the Restart algorithm effectively reduces the accumulated error at time \(t_{}\) by a contraction factor in the Wasserstein upper bound. These theoretical findings inspire us to position the Restart interval \([t_{},t_{}]\) towards the end of the main backward process, where the accumulated error is more substantial. In addition, our empirical observations suggest that a larger time interval \(t_{}-t_{}\) is more beneficial for weaker/smaller architectures or more challenging datasets. Even though a larger time interval increases the additional sampling error, the benefits of the contraction significantly outweighs the downside, consistent with our theoretical predictions. We leave the development of principled approaches for optimal time interval selection for future works.

**Multi-level Restart** For challenging tasks that yield significant approximation errors, the backward trajectories may diverge substantially from the ground truth even at early stage. To prevent the ODE simulation from quickly deviating from the true trajectory, we propose implementing multiple Restart intervals in the backward process, alongside the interval placed towards the end. Empirically, we observe that a \(1\)-level Restart is sufficient for CIFAR-10, while for more challenging datasets such as ImageNet , a multi-level Restart results in enhanced performance .

## 5 Experiments

In Sec 5.1, we first empirically verify the theoretical analysis relating to the Wasserstein upper bounds. We then evaluate the performance of different sampling algorithms on standard image generation benchmarks, including CIFAR-10  and ImageNet \(64 64\) in Sec 5.2. Lastly, we employ Restart on text-to-image generation, using Stable Diffusion model  pre-trained on LAION-5B  with resolution \(512 512\), in Sec 5.3.

### Additional sampling error versus contracted error

Our proposed Restart sampling algorithm demonstrates a higher contraction effect and smaller addition sampling error compared to SDE, according to Theorem 1 and Theorem 2. Although our theoretical analysis compares the upper bounds of the total, contracted and additional sampling errors, we further verify their relative values through a synthetic experiment.

**Setup** We construct a \(20\)-dimensional dataset with 2000 points sampled from a Gaussian mixture, and train a four-layer MLP to approximate the score field \(_{x} p_{t}\). We implement the ODE, SDE, and Restart methods within a predefined time range of \([t_{},t_{}]=[1.0,1.5]\), where the process outside this range is conducted via the first-order ODE. To compute various error types, we define the distributions generated by three methods as outlined in the proof of Theorem 2 and directly gauge the errors at end of simulation \(t=0\) instead of \(t=t_{}\): (1) the generated distribution as \(p_{0}^{}\), where \(\{_{},_{},_{ }(K)\}\); (2) an auxiliary distribution \(q_{0}^{}\) initiating from true distribution \(p_{t_{}}\) at time \(t_{}\). The only difference between \(p_{0}^{}\) and \(q_{0}^{}\) is their initial distribution at \(t_{}\) (\(p_{t_{}}^{_{}}\) versus \(p_{t_{}}\)); and (3) the true data distribution \(p_{0}\). In line with Theorem 2, we use Wasserstein-1 distance \(W_{1}(p_{0}^{},q_{0}^{})\) / \(W_{1}(q_{0}^{},p_{0})\) to measure the contracted error / additional sampling error, respectively. Ultimately, the total error corresponds to \(W_{1}(p_{0}^{},p_{0})\). Detailed information about dataset, metric and model can be found in the Appendix C.5.

**Results** In our experiment, we adjust the parameters for all three processes and calculate the total, contracted, and additional sampling errors across all parameter settings. Figure 2(a) depicts the Pareto frontier of additional sampling error versus contracted error. We can see that Restart consistently achieves lower contracted error for a given level of additional sampling error, compared to both the ODE and SDE methods, as predicted by theory. In Figure 2(b), we observe that the Restart method obtains a smaller total error within the additional sampling error range of \([0.8,0.85]\). During this range, Restart also displays a strictly reduced contracted error, as illustrated in Figure 2(a). This aligns with our theoretical analysis, suggesting that the Restart method offers a smaller total error due to its enhanced contraction effects. From Figure 2(c), Restart also strikes an better balance between efficiency and quality, as it achieves a lower total error at a given NFE.

### Experiments on standard benchmarks

To evaluate the sample quality and inference speed, we report the FID score  (lower is better) on 50K samplers and the number of function evaluations (NFE). We borrow the pretra

Figure 3: FID versus NFE on **(a)** unconditional generation on CIFAR-10 with VP; **(b)** class-conditional generation on ImageNet with EDM.

Figure 2: Additional sampling error versus **(a)** contracted error, where the Pareto frontier is plotted and **(b)** total error, where the scatter plot is provided. **(c)** Pareto frontier of NFE versus total error.

[MISSING_PAGE_FAIL:8]

lower than any previous numbers (even lower than the SDE sampler with an NFE greater than 1000 in ), and make VP model on par with the performance with more advanced models (such as EDM).

Theorem 4 shows that each Restart iteration reduces the contracted errors while increasing the additional sampling errors in the backward process. In Fig. 5, we explore the choice of the number of Restart iterations \(K\) on CIFAR-10. We find that FID score initially improves and later worsens with increasing iterations \(K\), with a smaller turning point for stronger EDM model. This supports the theoretical analysis that sampling errors will eventually outweigh the contraction benefits as \(K\) increases, and EDM only permits fewer Restart iterations due to smaller accumulated errors. It also suggests that, as a rule of thumb, we should apply greater Restart strength (_e.g_., larger \(K\)) for weaker or smaller architectures and vice versa.

### Experiments on large-scale text-to-image model

We further apply Restart to the text-to-image Stable Diffusion v1.5 2 pre-trained on LAION-5B  at a resolution of \(512 512\). We employ the commonly used classifier-free guidance  for sampling, wherein each sampling step entails two function evaluations - the conditional and unconditional predictions. Following , we use the COCO  validation set for evaluation. We assess text-image alignment using the CLIP score  with the open-sourced ViT-g/14 , and measure diversity via the FID score. We also evaluate visual quality through the Aesthetic score, as rated by the LAION-Aesthetics Predictor V2 . Following , we compute all evaluation metrics using 5K captions randomly sampled from the validation set and plot the trade-off curves between CLIP/Aesthetic scores and FID score, with the classifier-free guidance weight \(w\) in \(\{2,3,5,8\}\).

We compare with commonly used ODE sampler DDIM  and the stochastic sampler DDPM . For Restart, we adopt the DDIM solver with 30 steps in the main backward process, and Heun in the Restart backward process, as we empirically find that Heun performs better than DDIM in the Restart. In addition, we select different sets of the hyperparameters for each guidance weight. For instance, when \(w=8\), we use \([t_{},t_{}]{=}[0.1,2],K{=}2\) and \(10\) steps in Restart backward process. We defer the detailed Restart configuration to Appendix C.2, and the results of Heun to Appendix D.1.

As illustrated in Fig. 6(a) and Fig. 6(b), Restart achieves better FID scores in most cases, given the same CLIP/Aesthetic scores, using only 132 function evaluations (_i.e_., 66 sampling steps). Remarkably, Restart achieves substantially lower FID scores than other samplers when CLIP/Aesthetic scores are high (_i.e_., with larger \(w\) values). Conversely, Restart generally obtains a better text-image alignment/visual quality given the same FID. We also observe that DDPM generally obtains comparable performance with Restart in FID score when CLIP/Aesthetic scores are low, with Restart being more time-efficient. These findings suggest that Restart balances diversity (FID score) against text-image alignment (CLIP score) or visual quality (Aesthetic score) more effectively than previous samplers.

In Fig. 7, we visualize the images generated by Restart, DDIM and DDPM with \(w=8\). Compared to DDIM, the Restart generates images with superior details (_e.g_., the rendition of duck legs by DDIM is less accurate) and visual quality. Compared to DDPM, Restart yields more photo-realistic images (_e.g_., the astronaut). We provide extended of text-to-image generated samples in Appendix E.

Figure 6: FID score versus **(a)** CLIP ViT-g/14 score and **(b)** Aesthetic score for text-to-image generation at \(512 512\) resolution, using Stable Diffusion v1.5 with a varying classifier-free guidance weight \(w=2,3,5,8\).

## 6 Conclusion and future direction

In this paper, we introduce the Restart sampling for generative processes involving differential equations, such as diffusion models and PFGMs. By interweaving a forward process that adds a significant amount of noise with a corresponding backward ODE, Restart harnesses and even enhances the individual advantages of both ODE and SDE. Theoretically, Restart provides greater contraction effects of stochasticity while maintaining ODE-level discretization error. Empirically, Restart achieves a superior balance between quality and time, and improves the text-image alignment/visual quality and diversity trade-off in the text-to-image Stable Diffusion models.

A current limitation of the Restart algorithm is the absence of a principled way for hyperparameters selection, including the number of iterations \(K\) and the time interval \([t_{},t_{}]\). At present, we adjust these parameters based on the heuristic that weaker/smaller models, or more challenging tasks, necessitate a stronger Restart strength. In the future direction, we anticipate developing a more principled approach to automating the selection of optimal hyperparameters for Restart based on the error analysis of models, in order to fully unleash the potential of the Restart framework.