# B-cosification: Transforming Deep Neural Networks

to be Inherently Interpretable

Shreyash Arya\({}^{*,1,2}\), Sukrut Rao\({}^{*,1,2}\), Moritz Bohle\({}^{*,,1,3}\), Bernt Schiele\({}^{1}\)

\({}^{1}\)Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrucken, Germany

\({}^{2}\)RTG Neuroexplicit Models of Language, Vision, and Action, Saarbrucken, Germany

\({}^{3}\) Kyutai, Paris, France

{sarya,sukrut.rao,schiele}@mpi-inf.mpg.de moritz@kyutai.org

\({}^{*}\)Equal contribution \({}^{}\) Work done while at MPI Informatics

###### Abstract

B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose 'B-cosification', a novel approach to _transform_ existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release our code and pre-trained model weights at https://github.com/shrebox/B-cosification.

## 1 Introduction

Despite their strong performance on a variety of tasks, understanding decisions of deep neural networks (DNNs) remains challenging. Explanation methods, such as feature attributions , have been proposed in an attempt to explain such decisions _post-hoc_, but have often found to be unfaithful to the model being explained .

Inherently interpretable Deep Neural Network (DNN) models have recently gained popularity. In contrast to the common approach of explaining existing DNNs in a _post-hoc_ fashion, these models typically feature certain architectural constraints that allow for extracting human-interpretable, model-faithful simplifications of the models' computations _by design_; examples of this include prototype-based , dynamic linear , or concept-bottleneck models . However, given those architectural changes, this comes at a price: specifically, the models need to be trained from scratch, which--especially in the case of large foundation models, which are increasingly popular--can cost millions of dollars.

To mitigate this, in this work, we explore a novel approach of _fine-tuning DNNs for inherent interpretability_ and propose to 'B-cosify' existing DNNs. Specifically, we investigate whether pre-trained DNNs can simply be efficiently fine-tuned to obtain a similar degree of interpretability as 38th Conference on Neural Information Processing Systems (NeurIPS 2024).

the recently proposed B-cos Networks . In contrast to the original B-cos Networks, which leverage existing _architectures_ to obtain performant and interpretable models, we investigate whether we can additionally leverage the existing pre-trained _weights_, thus aiming to take advantage of the significant amount of resources that have been invested in training existing models. As a result, we hope to make inherently interpretable models more easily accessible to the community.

To do so, we first conduct a detailed analysis of how B-cos DNNs differ from their conventional counterparts. Interestingly, we find that many existing models can be converted into _functionally equivalent_ B-cos models by a small set of targeted implementational modifications (Tab. 1). To increase the interpretability of the models, we then increase the 'alignment pressure'  via the parameter B of the B-cos transformations and fine-tune the models on their respective tasks, which leads to significantly more interpretable explanations (Fig. 2).

On supervised settings, we find that B-cosified models often outperform both conventional and B-cos DNNs at a fraction of the full training cost (Fig. 1, left), whilst exhibiting a similar degree of interpretability as the original B-cos DNNs (Fig. 1, right). We further apply B-cosification to a pre-trained CLIP model , a large foundation vision-language model (VLM), and show that despite using comparatively limited data and compute cost, B-cosified CLIP models yield highly interpretable explanations whilst being competitive on zero-shot performance across a variety of downstream datasets.

Our work thus opens a new perspective on how to design inherently interpretable models in a cost-effective manner. Importantly, on the one hand it highlights that conventional models might be closer to inherently interpretable models than previously understood. On the other hand, it highlights the benefits of designing inherently interpretable models via minor architectural modifications, such as e.g. the B-cos DNNs, as this can allow for leveraging the large array of existing, pre-trained DNNs.

In summary, our contributions are:

* We propose _B-cosification_, a novel technique to 'fine-tune for interpretability', that addresses the problem of high training cost associated with obtaining inherently intepretable models such as B-cos DNNs. Our B-cosified DNNs are highly interpretable while often outperforming both standard and B-cos DNNs.
* We thoroughly study different design choices to find an optimal strategy for B-cosification.
* We apply B-cosification to supervised image classifiers on ImageNet , including both CNNs and ViTs, and show that the B-cosified variants perform on par on interpretability metrics while often outperforming in terms of accuracy. Overall, we find that B-cosifying a pre-trained black box DNN to be superior on both metrics as compared to training a B-cos DNN from scratch, while being computationally significantly cheaper.

Figure 1: **B-cosification: Obtaining inherently interpretable models with competitive accuracy at low cost.**_Left:_ Accuracy progression over epochs for a DenseNet-121 and a ViT-S, comparing B-cosified (blue) and B-cos (orange) training curves. B-cosified models achieve equivalent accuracy with a substantial reduction in training time, yielding 4.7x speedup for DenseNet-121 and 9.0x speedup for ViT-S. _Right:_ Quatiatitive comparison of explanations for various images for B-cos  and our B-cosified models at various stages of training. Specifically, we show the dynamic linear mappings \(()\) computed by the models in color as in ; note that by formulating conventional models (‘initial’ in the plot) as a specific version of B-cos models, we are able to visualise the corresponding explanations in color too, see Sec. 3.2.1 for further details. We find that after only one epoch of training, the B-cosified models exhibit similar explanations as B-cos models.

* We extend B-cosification to CLIP, a foundation VLM, and show that B-cosified CLIP remains highly competitive on zero-shot performance across a variety of downstream datasets, while also yielding similar interpretability benefits as B-cos models.

## 2 Related Work

**Explanation Methods.** Post-hoc attributions [46; 43; 53; 48; 5; 56] have popularly been used to understand the decisions of trained DNNs, but have often been shown to be unfaithful to the model being explained [2; 3; 60; 40]. Inherently interpretable models [14; 28; 10], in contrast, incorporate architectural changes to the model and can yield explanations that are interpretable and faithful to the model by design. However, such models need to be trained from scratch, which imposes a significant additional cost. In this work, we explore fine-tuning for interpretability, and propose a method to transform existing black-box DNNs to inherently interpretable B-cos DNNs, bringing together the best of both worlds.

**Attribution Priors**[38; 37; 27; 50; 4] have often been used to train or fine-tune models to have explanations with desirable properties, such as inducing smoother explanations , consistent explanations [37; 38], or to guide models to be 'right for the right reasons' [44; 20; 41; 34]. Similar to such work, we fine-tune black-box DNNs for interpretability, but in contrast, we only make _architectural_ modifications to transform the DNNs to B-cos DNNs, and do not use any additional constraints on the explanations themselves while training.

**CLIP Interpretability and Localization.** Post-hoc attribution methods [46; 36; 13; 7] have also been used to explain VLMs such as CLIP , however, as with supervised DNNs, their faithfulness to the model is not guaranteed and the explanations are often coarse-grained and not very human interpretable. While inherently interpretable architectures could address this, the high costs of training such large models from scratch makes their use unappealing. In this work, we bridge the gap by instead fine-tuning from pre-trained black-box CLIP models to inherently interpretable B-cosified CLIP variants, and find that the B-cosification process is effective in yielding performant and interpretable models. A separate line of work  involves improving localizability of VLMs, and is orthogonal to our work since our goal is to obtain explanations that are faithful to the model.

**Learning mappings between model features.** Recent work [30; 33] has explored using simple linear transforms to map features between models, and in particular also mapping features from arbitrary models to CLIP's representation space. In the context of our work, such methods can be used to map a supervised B-cos feature extractor to CLIP using a linear transform, to obtain an inherently interpretable DNN that can mimic CLIP. In our evaluation, we compare with such an approach, and find that our approach of architecturally transforming the full model and fine-tuning for interpretability yields improved zero shot performance.

Figure 2: **B-cosified CLIP Models.** After B-cosifying a CLIP model and fine-tuning it according to our proposed B-cosification scheme, see Sec. 3.2, we find that it is possible to endow the model with the same level of inherent interpretability as the B-cos models proposed in , whilst maintaining CLIP’s zeroshot ability (see Fig. 5). The resulting linear summaries of the models (\(()\)) can be visualised in color (row 3) and provide significantly more detail than GradCAM explanations (row 2), which are often used to explain conventional CLIP models.

From conventional to B-cos models

In the following, we describe the process of fine-tuning standard black box DNNs into inherently interpretable B-cos DNNs. In Sec. 3.1, we first introduce the B-cos models and enumerate the key ways in which they differ from standard models. In Sec. 3.2, we then perform a detailed study on strategies to bridge each of these differences for effective B-cosification.

### B-cos Models: Background

Many common DNNs consist of a series of blocks of linear layers followed by non-linear ReLU activations , and are thus piece-wise linear functions1: i.e., for every input \(\), they effectively compute a linear transformation of that input: \(()=()+()\), cf. . In [9; 11], models of this kind have been called 'dynamic linear', a naming convention that we adopt in this paper.

Interestingly, for piece-wise linear models, \(()\) is given by the models' gradient with respect to \(\)--except for the input-dependent bias \(()\), the gradient thus constitutes an exact summary of the models' computations. This linear mapping \(()\) is unfortunately typically not easily interpretable, and many techniques have been proposed to derive qualitatively more convincing explanations [46; 56]. These, however, have been shown to often not faithfully reflect the underlying model [1; 40].

Further, if the models employ bias terms, \(()\) does not yield a _complete_ explanation , i.e. \(()()\). Integrating bias terms as proposed by  yields a set of importance attribution maps, summarizing which requires carefully selecting a post-processing function with inherent tradeoffs. Even when not using bias terms , however, the resulting matrices \(()\) are often not easily human interpretable, and the resulting models can suffer from significant drops in performance.

To address this, [10; 11] propose to architecturally modify the DNNs to introduce additional _alignment pressure_ during model optimisation. For this, they replace the ubiquitously used linear transformation by the B-cos transformation, which dynamically scales the output of the linear transformations:

\[ f_{}(;)=( |(,)|^{B-1}} )^{T}=^{T}()\,\] (1)

with \(B\) a hyperparameter, \(\) the cosine similarity between \(\) and the weights \(\), and \(}\!=\!/\|\|\).

Like piece-wise linear models, B-cos models are dynamic linear and thus accurately summarised by a single linear transformation \(()\) s.t. \(()=()\); as B-cos models do not employ bias terms, this model summary is complete. Crucially, it has been shown that with \(B\!>\!1\), the matrix \(()\) aligns with task-relevant input patterns, making it more easily human interpretable (e.g. Fig. 1, right).

Importantly, as the B-cos transformation can serve as a drop-in replacement for linear transformations at every layer of a DNN, it is possible [10; 11] to leverage _existing DNN architectures_ and the resulting B-cos models obtain similar classification accuracies as their conventional counterparts (Tab. 4, cols. 'pretrained' and 'B-cos').

Extending this, we investigate if it is possible to leverage _existing DNN weights_--i.e., our goal is to fine-tune existing models to be similarly interpretable as B-cos models, whilst not requiring to train them from scratch. However, despite the architectural similarities between B-cos and conventional models, there are multiple key differences that make transforming pre-trained models into B-cos models non-trivial: e.g., apart from replacing linear transformations with the B-cos transformation and not employing biases, B-cos models are trained on image representations with 6 color channels as \([r,g,b,1-r,1-g,1-b]\) to be able to visualise the model-inherent linear summaries \(()\) in color, whereas conventional models use 3 channels (see also Tab. 1). In the next section, we show how to overcome these differences and convert existing models into functionally equivalent B-cos models.

### B-cosification of Deep Neural Networks

We analysed the differences between B-cos models and their conventional counterparts in detail and compiled the results in Tab. 1. In this section, we discuss one by one how to bridge these differences. In particular, we show that a conventional model can be framed as a _functionally equivalent_ B-cos model as in  with \(B\!=\!1\), which additionally employs bias terms. Only upon modifying these two aspects, i.e. biases and \(B\), does the model need to be fine-tuned to adapt the weights to those changes.

#### 3.2.1 Functionally Equivalent B-cos Models

**Input Encoding and Normalisation.** As mentioned in Sec. 3.1, B-cos models use input representations with six color channels \([r,g,b,1\!-\!r,1\!-\!g,1\!-\!b]\) to be able to visualise the explanations in color, cf. . However, most conventional DNNs (e.g. models from Torchvision , CLIP ) are applied to 3-channel inputs in which images are encoded via \([r,g,b]\). As a result, visualising the dynamic matrices \(()\) of piece-wise linear models (cf. Sec. 3.1) in color would not seem possible.

However, we note that in combination with the commonly used input _normalisation_, we can convert the first linear transformation in conventional models (e.g., a convolutional layer) into an equivalent transformation that accepts 6-channel inputs. Specifically, for input normalisation, the channel-wise means \(_{s}\) are subtracted from the individual channels, followed by a division by the standard deviations \(_{s}\), yielding \(s^{}\!=\!(s-_{s})/_{s}\) for \(s\!\!\{r,g,b\}\). Conversely, mean-normalising the 3 additional color channels yields \(-s^{}\). Leveraging this, we use the models' weights learnt for 3-channel inputs, \(_{j}=[w_{j,r},w_{j,g},w_{j,b}]\) for every feature \(j\), to construct an equivalent 6-channel transformation:

\[_{j}^{}=[_{j,r}}{2},_{j,g}}{2},_{j,b}}{2},-_{j,r}}{2},-_{j,g}}{2},-_{j,b}}{2}]\.\] (2)

Note that applying \(_{j}^{}\) to the mean-normalised, 6-channel inputs yields the same results as applying \(_{j}\) to the original mean-normalised inputs that the pre-trained models have seen during training.

**Activation Functions.** Owing to the non-linearity inherent to the B-cos transform, explicit activation functions are not necessary in between B-cos layers. However, the authors of [10; 11] showed that the model-inherent explanations are compatible with MaxOut . Note that the very commonly used ReLU non-linearity applied to \(^{T}\) for any weight vector \(\), is just a special case of MaxOut:

\[(;,)=(^{T},^{T})=(^{T})\.\] (3)

As the pre-trained models' weights have been optimised for the ReLU non-linearity and given its compatibility with the B-cos explanations, we leave them untouched in the B-cosification process.

**Weight normalization.** B-cos transformations employ unit norm weights, see also Eq. (1), which the authors motivated by the fact that the only way any given neuron can achieve its maximal output is by increasing the weight-input alignment, which in turns leads to the improvements of the explanations.

However, conventional models have been trained with unconstrained weights and using unit norm weights would thus lead to unpredictable model behaviour. Interestingly, we note that the weight normalisation in the latest version of the B-cos models can actually not impact the explanation quality, as the authors of  re-introduce normalisation layers into the B-cos models. To better understand this, let us consider the compound function of a batch normalisation layer and a B-cos layer:

\[f()=()\] (4) \[()=-()}{()}}+\,,\] (5)

with \(\) and \(\) trainable parameters of the BatchNorm layer. Note that \(}\) is scaled by any factor \(\) by which the output of a B-cos layer might be scaled, which cancels in the fraction in Eq. (5) and thus makes \(f()\)_invariant_ to scaling the B-cos transformation: i.e. \(()=()\).

  
**Property** & **Standard** & **B-cos** & **B-cosified** & **reason** \\ 
**Image Encoding** & 3 channels & 6 channels & 6 channels & \(\) colored explanations \\
**Normalized Inputs** & yes & no & yes & \(\) in-distribution (ID) \\
**Weights** & unnormalised & normalised & unnormalised & \(\) equivalent and ID \\
**Activations** & ReLU & none & ReLU & \(\) compatible and ID \\ 
**Biases** & yes & no & no & \(\) complete explanations \\ \(B\)**in B-cos** & 1 & 2 & 2 & \(\) weight-input alignment \\   

Table 1: **Overview. To allow for comparing the models, we compiled the identified differences between the conventional models (Standard), their B-cosified version (B-cosified) and the original B-cos models (B-cos). For each design choice in the B-cosified models, we summarise the respective discussion in Sec. 3.2 (reason)).**In particular, the output of \(()\) is thus invariant to weight normalisation, as the output of B-cos (\(\)) scales linearly with the weight norm, cf. Eq. (1).

This is of course only true if every B-cos layer were always followed by a normalisation layer, which is not necessarily the case. Nonetheless, we find that not using normalised weights yields consistently good results across all models. Therefore, we use B-cos transformations without weight normalisation throughout our experiments; for an ablation, see Tab. B2 in the appendix.

**In summary,** we showed that it is possible to adapt the implementation of existing models in a way that allows us to integrate certain aspects of B-cos models without functionally changing the pre-trained models. Notably, we can now visualise color explanations similar to B-cos models (Fig. 1, right, col. 3); unsurprisingly, however, these explanations have poor interpretability due to the absence of the alignment pressure imposed during B-cos training. In the next section, we discuss the necessary functional changes for B-cosification to obtain interpretable explanations.

#### 3.2.2 Fine-tuning for Interpretability

The changes introduced in the preceding section have not functionally changed the pre-trained models, but rather allow us to interpret the existing models as a special case of B-cos models. Now we introduce the necessary changes to increase the interpretability of the dynamic matrices \(()\). As these functionally change the models, they need to be fine-tuned to recover their original performance.

In particular, the remaining differences between conventional and B-cos models are (1) the value of \(B\), and (2) the use of biases, Tab. 1. We will now discuss how we bridge these differences individually.

**Ablation Setup.** We evaluate various fine-tuning strategies using a ResNet-18  model supervised on ImageNet  from Torchvision  for B-cosification, and compare with a B-cos ResNet-18 from . We optimize using AdamW  with cosine scheduling and train for 90 epochs, and evaluate both classification accuracy as well as interpretability using the GridPG metric .

**(1) Increasing \(B\).** As shown in , using \(B\!>\!1\) is critical to obtain easily interpretable explanations. To increase B for the pre-trained models, we investigate three strategies: (1) immediately setting \(B\) to a higher value and then fine-tuning, (2) linearly interpolating from \(B=1\) to \(B=2\) throughout fine-tuning, and (3) setting \(B\) as a learnable parameter. (2) has the advantage of changing the model in small steps, making it more likely that it maintains performance while fine-tuning, but requires using the full number of epochs to reach the target value of \(B\). (1) on the other hand is likely to adversely affect the utility of the weights, but offers the opportunity to stop fine-tuning early if performance and interpretability metrics are sufficiently high. (3) offers the most flexibility, but also adds a new set of parameters that need to be optimized. We show the results of this evaluation in Tab. 2. Interestingly, we find that using (1), i.e. setting \(B=2\) and then fine-tuning, yields performance that is on par with learnable B parameters, whilst being significantly simpler to implement. To easily test the generality of the B-cosification scheme, we therefore opt for this approach in Sec. 4.1.

**(2) Decreasing biases.** As discussed in Sec. 3.1, dynamic linear models with bias terms are not _exactly_ summarised by the matrix \(()\), cf. . To obtain the same level of _faithfulness_ of the explanations as B-cos models (in particular w.r.t. explanation completeness, cf. ), we need to remove the biases from the model. To do so, we investigate two approaches: (1) removing all biases first and then fine-tuning, and (2) fine-tuning while decaying biases using weight decay. Similar to the setup with \(B\), (2) has the advantage of avoiding drastic changes to the model, but requires potentially fine-tuning for longer. Further, the weight given to the bias decay in the loss constitutes a tradeoff between maintaining classification performance and pushing the biases to be close to zero. We report the results of this evaluation in Tab. 3. Similarly to the experiments for \(B\), we find that immediately setting the biases to zero constitutes a simple yet performant approach to achieve both good localisation and accuracy. To assess the generality of the B-cosification scheme across a wide range of models, we thus choose this the simpler approach of setting biases to zero in Sec. 4.1.

  
**MetricResNet-18** &  &  &  &  \\ Standard & B-cos & B-1 & B-1 & B-2 & 5 epo. & 45 epo. & 90 epo. & \\  Accuracy & 69.6\(\)0.2 & 68.5\(\)0.2 & 70.6\(\)0.1 & 71.6\(\)0.1 & 71.5\(\)0.1 & 71.6\(\)0.2 & 71.1\(\)0.1 & 70.2\(\)0.0 & 71.8\(\)0.1 \\ Localisation & 21.4\(\)0.2 & 87.4\(\)0.5 & 33.9\(\)0.2 & 84.3\(\)0.2 & 87.6\(\)0.2 & 88.1\(\)0.1 & 88.8\(\)0.2 & 88.8\(\)0.2 & 89.4\(\)0.1 \\   

Table 2: **Increasing \(B\) for B-cosification.**In short, we find that a very simple approach, i.e., setting the bias and the \(B\) values to the target values immediately, constitutes a simple and easy-to-use, but nonetheless performant strategy to B-cosify models. In the following sections, we test whether these findings generalise well to other models.

## 4 B-cosification Results

In the following, we evaluate the effectiveness of the B-cosification strategy we developed in Sec. 3. In Sec. 4.1, we first apply B-cosification to supervised models across various architectures, and evaluate for classification performance and interpretability. In Sec. 4.2, we B-cosify CLIP , a large foundation vision-language model, and show that despite fine-tuning at a fraction of the training cost, the B-cosified CLIP shows strong zero shot generalization whilst being highly interpretable.

### Supervised Classification Models

**Setup.** We B-cosify models from Torchvision  supervised on ImageNet . We use a diverse set architectures, including both CNNs (ResNet-18 , ResNet-50 , and DenseNet-121 ), and ViTs [18; 6; 57] with (ViT\({}_{c}\)-Ti, ViT\({}_{c}\)-S, ViT\({}_{c}\)-B, ViT\({}_{c}\)-L) and without (ViT-Ti, ViT-S, ViT-B, ViT-L) convolutional stems. For ResNet-50, we use both the weights originally released by Torchvision and the updated V2 weights, which constitute models trained for longer and with more augmentations . As in Sec. 3.2, we evaluate both for classification accuracy and for interpretability using the GridPG  metric. We compare both accuracy and interpretability of the B-cosified models with B-cos models trained from scratch from . For interpretability, we also compare with several post-hoc attribution methods as baselines, namely Guided Backprop , Gradient , DeepLIFT , IxG , IntGrad , and GradCAM . For full details, see Appendix C.1.

**Classification performance.** Tab. 4 reports the classification accuracy of the B-cosified models across architectures, and compares them with their conventional counterparts from Torchvision and B-cos models trained from scratch. We find that across architectures (col. 1), B-cosified models perform competitively with conventional DNNs (cols. 2-4) and interestingly, in contrast to the findings reported by , often outperform them, i.e. for five out of twelve architectures. Notably, we find (col. 5) that our B-cosified models significantly outperform B-cos models trained from

  &  &  \\
**Model** & pretrained & B-cos  & B-cosified & \(_{}\) & \(t\) & speedup \\  ResNet-18 & 69.8 & 68.7 & 71.5 & +2.8 & 29 & \( 3.1\) \\ ResNet-50-v1 & 76.1 & 75.9 & 76.5 & +0.6 & 46 & \( 2.0\) \\ ResNet-50-v2 & 80.9 & 75.9 & 77.3 & +1.4 & 10 & \( 9.0\) \\ DenseNet-121 & 74.4 & 73.6 & 76.3 & +2.7 & 18 & \( 5.0\) \\ ViT-Ti & 70.3 & 60.0 & 69.3 & +9.3 & 10 & \( 9.0\) \\ ViT-S & 74.4 & 69.2 & 75.2 & +6.0 & 10 & \( 9.0\) \\ ViT-B & 75.3 & 74.4 & 75.3 & +0.9 & 57 & \( 1.6\) \\ ViT-L & 75.8 & 75.1 & 75.5 & +0.4 & 66 & \( 1.4\) \\ ViT\({}_{c}\)-Ti & 72.6 & 67.3 & 72.3 & +5.0 & 10 & \( 9.0\) \\ ViT\({}_{c}\)-S & 75.7 & 74.5 & 76.0 & +1.5 & 32 & \( 2.8\) \\ ViT\({}_{c}\)-B & 76.8 & 77.1 & 76.7 & -0.4 & - & - \\ ViT\({}_{c}\)-L & 77.9 & 77.8 & 77.1 & -0.7 & - & - \\ 

Table 4: **Classification Accuracy.** We report the top-1 classification accuracy on the ImageNet validation set of the pre-trained models (**pretrained**) and the B-cosified models (**B-cosified**) after fine-tuning them. Additionally, we report the accuracy of the corresponding B-cos models trained from scratch (**B-cos**) as well as the difference to them (\(_{}\)), and how much faster and at which epoch (\(t\)) the same accuracy as in  was achieved (**speedup**). Results for B-cosified models are averaged over three runs; full results including standard deviation in appendix.

   &  &  &  &  \\  & Standard & B-cos &  &  &  &  &  \\  Accuracy & 69.6\(\)0.2 & 68.5\(\)0.2 & 71.2\(\)0.1 & 71.5\(\)0.1 & 71.2\(\)0.2 & 71.4\(\)0.3 & 71.6\(\)0.2 \\ Localisation & 21.4\(\)0.2 & 87.4\(\)0.5 & 47.2\(\)0.5 & 87.6\(\)0.2 & 81.4\(\)0.2 & 90.2\(\)0.2 & 91.2\(\)0.1 \\  

Table 3: **Decreasing biases for B-cosification.**scratch across all but the two largest ViT\({}_{c}\) architectures. Further, B-cosified models often achieve the same performance as their corresponding B-cos models at a fraction of the training cost (col. 6). Specifically, we find that averaged across architectures, B-cosified models outperform B-cos models trained from scratch by 2.5 pp, with an average training speedup (to match performance) of 5.2x. These results strongly advocate for B-cosification as a superior alternative to training from scratch for obtaining performant inherently interpretable models at a low compute cost.

**Interpretability.** To evaluate the interpretability of our B-cosified models, we report the GridPG localization scores in Fig. 3, and compare with conventional and B-cos models; following , we report the results of 3x3 image grids for convolutional models, and of 2x2 grids for the ViTs. For a fair comparison, for all models, we evaluate the localization of the dynamic linear summary of the model2\(()\) (see Sec. 3.1). We find that across architectures, B-cosified models significantly outperform conventional DNNs in terms of localization (32.7pp-71.0pp) and perform on par with B-cos models. Since post-hoc attribution methods (e.g. ) are often used to interpret conventional DNNs, similar to , in Fig. 4, we compare the localization of the model inherent explanations from two of our B-cosified models with post-hoc explanations applied to the corresponding conventional models. Similar to the results reported by , we find our B-cosified models to strongly outperform all post-hoc methods, including GradCAM , with a near perfect localization score, showing that B-cosification is effective in yielding highly interpretable yet model-faithful explanations.

**Impact of pre-trained weights.** Since our aim is to _fine-tune_ for interpretability, we investigate how crucial the quality of the weights of the conventional model are for effective B-cosification. Specifically, we expect weights from stronger models to be a better starting point for B-cosification. We evaluate this by performing B-cosification both with v1 and v2 variants of ResNet-50  from Torchvision , where the latter is trained for longer and with stronger augmentations . From Tab. 4, we find that using a strong initialization is highly useful for effective B-cosification,

Fig. 4: **Comparison to Post-hoc Methods.** For two of the models in Fig. 3 (ResNet-50-v1, DenseNet-121) we compare the localisation performance of the dynamic matrices \(()\) to post-hoc explanations for the pre-trained models. Similar to the original B-cos models , the model-inherent explanations perform favourably.

Fig. 3: **Localisation Performance of \(()\).** We compute the contribution maps according to the dynamic linear summaries \(()\) of the pre-trained models (‘Standard’), their B-cosified versions, and the original pre-trained B-cos models and evaluate their localisation performance on the Grid Pointing Game as in . We find localisation to significantly improve for B-cosified models, achieving results on par with the models of .

specifically, fine-tuning starting from v2 weights outperforms B-cos models by 0.8pp as compared to v1, and achieves equal accuracy with a 9x speedup as compared to 2x with v1; similar results are observed for initialising the weights with models pretrained via the self-supervised DINO paradigm  (final accuracy: 77.0, speedup: 3.2x), for further discussion see Tab. B3 in the appendix.

### B-cosifying CLIP -- Towards Inherently Interpretable Foundation Models

In this section, we evaluate our B-cosification paradigm on CLIP , a powerful pre-trained vision-language model, and evaluate its interpretability and zero shot performance.

**Setup.** We B-cosify a CLIP  with a ResNet-50  backbone using the procedure described in Sec. 3.2. We use the recently proposed SigLIP loss  between the image embeddings of the pre-trained CLIP and the B-cosified CLIP's and train the models on either the ImageNet  or the CC3M datasets . For evaluation, we rely on the CLIP Benchmark  and report zeroshot and linear probing results for accuracy. To assess the models' interpretability, we explain the similarity between the image embeddings and the text embedding of the pre-trained CLIP model via the dynamic linear summaries, see Sec. 3.1 or GradCAM, and report the EPG scores  on the VOC dataset . For full details, see Appendix C.2.

**Evaluating Model Performance.** In Fig. 5, we report the zeroshot and linear probing accuracies of the two B-cosified CLIP models (trained on ImageNet or CC3M) and compare it to the original CLIP (Standard) and the recently proposed Text2Concept technique ; for the latter, we train a linear layer on top of a frozen, pre-trained B-cos ResNet-50 from  to mimic the embeddings of CLIP . We find that the B-cosified models significantly outperform the Text2Concept approach and achieve accuracies that are more similar to the original CLIP's zeroshot and linear probing accuracies.

**Evaluating Model Interpretability.** We evaluate the B-cosified CLIP's ability to localise classes in the VOC dataset in two ways. On the one hand, we directly explain the similarity of the models' embedding to the text embedding of a given prompt such as "A photo of a cow.". On the other hand, we note that the final attention pooling layer in the CLIP model only computes a weighted sum of the last layer's value vectors. Therefore, we additionally evaluate whether we can also explain the similarity between the text embeddings and these value vectors to improve the localisation ability.

In this context, we notice that explaining the average similarity to the text embedding yields highly distributed attribution maps, see Fig. 5(b), col. 2. On the other hand, explaining only the most similar embedding localises very well, see Fig. 5(b), col. 5. To better understand this phenomenon, we additionally interpolate between these two approaches and compute _weighted means_\(_{i}w_{i}_{i}\) of those value vectors \(_{i}\), in which the weights are determined by the cosine similarity between the value vectors \(_{i}\) and the text embedding \(\), i.e. with weights \(w_{i}=^{p}(,_{i})\) for various \(p\).

We find that this not only significantly improves the explanations qualitatively, see Figs. 2 and 5(b), but also quantitatively: in Fig. 5(a) we report results for explaining the final image embedding (**B-cosified CLIP**), the dynamic linear summary for the CLIP ResNet-50 (**CLIP \(()\)**), its GradCAM explanations (**CLIP GradCAM**), and the weighted mean of the value vectors, which we call

Fig. 5: **Classification performance on the CLIP Benchmark** of various CLIP models for the zero-shot setting (_left_) and linear probing (_right_). Specifically, we compare two B-cosified CLIP—trained on ImageNet (IMN) and CC3M respectively—to the Text2Concept approach by  and the original pre-trained CLIP model. We find B-cosified versions of CLIP to consistently outperform Text2Concept on natural and specialised data.

**B-cosified FeatureCLIP**. While B-cos CLIP already yields a noticeable improvement over the pre-trained CLIP explained via GradCAM and significantly improves the localisation ability of the linear summary \(()\), the weighted means yield even stronger performance for sufficiently high \(p\).

## 5 Discussion

The B-cosification approach presented in this work addresses a common issue with developing inherently interpretable models: achieving model interpretability without compromising on performance or incurring high training costs. By leveraging pre-trained models, B-cosification opens a new path towards developing interpretable yet performant models, which can be of particular interest in the context of foundation models such as CLIP , which might otherwise be prohibitively expensive to train on a limited budget. Our results suggest that B-cosification not only maintains but, in several cases, even enhances model accuracy, whilst yielding significant improvements on interpretability metrics, providing a viable and resource-efficient alternative to training B-cos models from scratch.

Specifically, we find B-cosified models to much faster reach the same levels of interpretability and accuracy than their counterparts trained from scratch, with training speedups of up to 9x in some models. The approach appears to be general, being applicable for both CNNs and ViT models. We hope that this increase in efficiency will make interpretable models much more accessible in settings with constrained computational resources and could thus facilitate their adoption. In particular, when applying our proposed B-cosification scheme to a foundation model--CLIP--we find that the B-cosified CLIP model is able to maintain competitive zero-shot performance while at the same time providing interpretable and model-faithful explanations.

Despite these advancements, certain aspects remain open for further exploration. Specifically, while some models quickly recover original performance after B-cosification, others exhibit slower convergence rates, suggesting potential for optimisations in the fine-tuning process. Additionally, for the larger B-cosified ViT\({}_{c}\) models, while yielding results that are on par with those trained from scratch, the B-cosification process did not succeed in achieving speed-ups, indicating that the interplay between model architecture and the proposed B-cosification might require further exploration.

In summary, our results establish B-cosification as an effective method for enhancing interpretability in pre-trained models with low computational cost. The method consistently enables high interpretability without compromising performance, even achieving substantial training speedups in many cases.