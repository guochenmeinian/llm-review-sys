# Rehearsal Learning for Avoiding Undesired Future

Tian Qin, Tian-Zuo Wang, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology

Nanjing University, Nanjing, 210023, China

{qint, wangtz, zhouzh}@lamda.nju.edu.cn

###### Abstract

Machine learning (ML) models have been widely used to make predictions. Instead of a predictive statement about future outcomes, in many situations we want to pursue a decision: what can we do to avoid the undesired future if an ML model predicts so? In this paper, we present a _rehearsal learning framework_, in which decisions that can persuasively avoid the happening of undesired outcomes can be found and recommended. Based on the _influence_ relation, we characterize the generative process of variables with _structural rehearsal models_, consisting of a probabilistic graphical model called _rehearsal graphs_ and structural equations, and find actionable decisions that can alter the outcome by reasoning under a Bayesian framework. Moreover, we present a probably approximately correct bound to quantify the associated risk of a decision. Experiments validate the effectiveness of the proposed rehearsal learning framework and the informativeness of the bound.

## 1 Introduction

Machine learning (ML) has achieved great success in various applications, including computer vision , natural language processing , recommender systems [3; 4], etc. In addition to perception tasks such as classifying an image, ML models have been widely used to make predictions, or forecasting, about future quantities of interest [5; 6]. In many scenarios, however, what we ultimately pursue is typically not attaining merely a predictive statement on what is likely to happen. Instead, as a potential next direction , we may seek a way, possibly through decision-making, to avoid the happening of the upcoming undesired future, if the ML model predicts so.

Suppose we use variables \(\) to predict a future outcome \(\) with an ML model \(h\). Given an instance \(\), \(h\) outputs a warning signal \(=h()\) which is outside our desired range. The problem is: what can we do to make the future \(\) fall into the desired range? We assume that there is an intermediate stage where one can make actionable decisions to influence \(\); _e.g.,_ after a sales prediction made at the beginning of a month, there is an intermediate time window before the month-end for the sales manager to take action. We consider decisions in the form of altering variables with fixed values, _e.g.,_ setting the discount to 10%. We henceforth call such decisions _alterations_. Let \(\) denote the variables in the intermediate stage. The avoiding undesired future (AUF) problem is then how to alter variables in \(\) so that the future \(\) could be shifted to fall into the desired range.

Common decision-making methods under the umbrella of reinforcement learning (RL)  can be applied to the AUF problem but may not be the most appropriate. The reasons include that (a) RL mainly focuses on sequential decision-making tasks while AUF desires a direct decision to change the upcoming future outcome, for which considering decision sequences could be unnecessary or even unrealistic; (b) the success of RL in playing games  and autonomous control  rely on sufficient interactions between the agent and the environment, but interactions in real AUF problems can be extremely sparse, _e.g._, the sales manager in the above example can only interact with the environment and make decisions once a month; (c) the Markov decision process (MDP) formalism in RL abstracts decision-making into states, actions, and rewards, which may overlook useful fine-grained structuralinformation in AUF, _e.g._, the connections between variables that can be altered may help identify useful actions without any exploration. Explicitly incorporating such connections in modeling the AUF problem would be preferable since otherwise approximating them with MDPs may require unnecessarily large state or action spaces.

Therefore, we need a specific method for solving AUF that takes account of the structural information and avoids the use of the large number of interactions. An immediate thought would be seeking cause-effect relations between variables , which have been leveraged for some decision-related problems (see Section 5 for a discussion of related work). But as indicated by Zhou , causal relations can help but should not be taken as a prerequisite for decision-making for reasons that (a) humans can make good decisions without a thorough or faithful causal understanding of the surrounding environment, sometimes correct decisions can even be made based on an incorrect causal understanding; (b) guiding decisions with causal relations is not always sensible as causation reveals truths that always hold, whereas decision-making is coping with real environments which are often open and dynamic; (c) causal modeling could be helpless for decision-making if the identified causal factors are unactionable; needless to say, it is very difficult to discover true cause-effect relations from data . Recognizing that _correlation_ is helpful for prediction but insufficient for decision-making and that _causation_ is needed for scientific discovery but too luxury to be relied on for decision-making, Zhou  claimed that what is required for decision-making is a kind of intermediate relation that is stronger than correlation but less demanding than causation; this relation was later called _influence relation_. Moreover, Zhou  attributed a decision to a series of hypothesized "rehearsal" of possible actions, which is an intuitive way of leveraging or even discovering the influence among variables involved in a decision task. Fig. 1 depicts an intuitive demonstration of the relationship among correlation, influence, and causation.

Based on the concept of rehearsal, we present the structural rehearsal model (SRM) to model the AUF problem. The SRM consists of a new probabilistic graphical model called rehearsal graphs and a collection of generative structural equations. In contrast to traditional structural causal models (SCM) , the SRM allows dynamic modeling and accommodates the _influence_ relation among variables, effectively capturing the interactions between interrelated but not necessarily causally linked variables. Given the true SRM, the effect of alterations can be calculated or estimated by conducting rehearsals. Consequently, the AUF problem can be addressed by searching for optimal alterations that yield the desired outcomes, though the search process could be complex and difficult.

Note that structural models such as SRMs are generally not available in advance, and learning them from limited observational data is difficult as well. In solving the AUF problem, one needs to tackle challenges posed by limited data, the uncertainty of structural models, and the possibly huge search space for finding an optimal alteration. Moreover, a system outputting decisions should provide some kind of guarantee on the probability of successfully avoiding undesired future, especially in high-stake applications such as economic and safety tasks. The AUF problem can also come in an online fashion and induce an exploration-exploitation trade-off: a decision-maker can choose alterations that will reveal more structural information to benefit future decision-making or alterations that will maximize the success probability in the current decision round.

To address the aforementioned challenges and tackle the AUF problem, we introduce a rehearsal learning framework that integrates structural rehearsal models and Bayesian inference. This framework effectively captures the inherent uncertainty in decision-making processes aimed at averting undesired outcomes by unifying structural modeling, alteration finding, and success probability bounding. Our contributions are as follows:

1. Structural Rehearsal Model: We propose SRM, a novel modeling approach distinct from SCM, designed to model the influence relation, which is more essential than causal relations for decision-making problems.
2. Constrained Optimization for AUF: We formulate a constrained optimization problem to solve the AUF problem, striking a balance between averting undesired future scenarios and accurately learning rehearsal models.

Figure 1: Relationship between correlation, influence, and causation .

3. Rehearsal Learning Framework: We develop a rehearsal learning framework tailored for solving AUF, showing that building decisions based on the influence relation is practical and feasible. We propose specific learning methods for the basic linear case and show that optimal solutions are not attainable in polynomial time in the framework if joint alterations are allowed, assuming \(\). This insight underscores the significance of developing approximate solutions over exact ones.
4. Risk Quantification: We introduce a probably approximately correct bound to quantify the associated risk inherent in a given decision. Our experiments provide empirical evidence of the informativeness of this bound.

## 2 Structural Rehearsal Models

Conventionally, SCMs  are used to describe causal relations among variables, based on which actions that affect the outcome could be found. However, in some real-world problems, especially those involving decisions, causal relations are not adequate. For example, let P and Q denote the prices of a product in two stores, respectively. If the first store decreases P to attract customers, the second store will decrease Q accordingly because the second store will lose consumers otherwise. From a causal view, it seems that P causes Q. But obviously, Q also causes P by symmetry. So we have a bi-directional "causal" relation, which is invalid in causal modeling. Mistakes will occur if one applies SCMs to this example as only a one-way causal relation is allowed in SCMs. The issue of such interrelated but not necessarily causally linked variables, as well as several other issues such as the dynamic and time-dependent nature of decision-making, which will be discussed later, are taken into account in the following new structural model called the structural rehearsal model (SRM).

An SRM consists of a set of rehearsal graphs and corresponding structural equations. A rehearsal graph qualitatively describes the relations between variables, and the structural equations characterize the generating process of variables in detail. In contrast to static causal modeling, SRM allows dynamic modeling by defining both the graphs and equations over a time index \(t\), which accounts for possible evolutions of the environment. We represent an SRM with \(\{ G_{t},_{t}\}_{t}\), where \(G_{t}\) denotes the rehearsal graph at time \(t\) and \(_{t}\) parameterizes corresponding structural equations.

Rehearsal graphs allow both directional and bi-directional edges. The directional edges depict the generation ordering of variables and the bi-directional ones indicate interrelated variables that are mutually influenced. Fig. 1(a) depicts an example. The definition of rehearsal graphs is given below:

**Definition 1** (Mixed graph).: Let \(G=(,)\) be a graph, where \(\) denotes the vertices and \(\) the edges. \(G\) is a _mixed graph_ if for any distinct vertices \(u,v\), there is at most one edge connecting them, and the edge is either _directional_ (\(u v\) or \(u v\)) or _bi-directional_ (\(u v\)).

**Definition 2** (Bi-directional clique).: A _bi-directional clique_\(C=(^{c},^{c})\) of a mixed graph \(G=(,)\) is a complete subgraph induced by \(^{c}\) such that any edge \(e^{c}\) is bi-directional. \(C\) is _maximal_ if adding any other vertex does not induce a bi-directional clique.

**Definition 3** (Rehearsal graph).: Let \(G=(,)\) be a mixed graph. Let \(\{C_{i}\}_{i=1}^{l}\) denote all maximal bi-directional cliques of \(G\), where \(C_{i}=(^{c}_{i},^{c}_{i})\). \(G\) is a _rehearsal graph_ if and only if:

1. \(^{c}_{i}^{c}_{j}=\) for any \(i j\).
2. \( i[l]\), \(u^{c}_{i}\), if there is any edge pointing from \(u\) to \(^{c}_{i}\), then \( v^{c}_{i}\), \(u v\).
3. The directional edges permit a topological ordering for \(\{C_{i}\}_{i=1}^{l}\).

Each vertex in a rehearsal graph corresponds to a variable. And the variables are generated following the ordering depicted by the directional edges. The variables in a common maximal bi-directional clique, where only bi-directional edges exist, are mutually influenced instead of having a fixed generating order. Given a rehearsal graph \(G\), structural equations accompanying the rehearsal graph are defined over cliques \(\{(^{c}_{i},^{c}_{i})\}_{i=1}^{l}\). Let \(^{G}_{i}\{u v^{c}_{i},u v\) in \(G\}\) denote parents of the \(i\)-th maximal bi-directional clique. Suppose that the unobserved noise variables are Gaussian, then the structural equation describing the generation process of \(^{c}_{i}\) is parameterized by \(\{_{i},_{i}\}_{i=1}^{l}\):

\[^{c}_{i}:=f_{i}(^{G}_{i};_{i})+_{i},\] (1)

where \(f_{i}\) is a multi-valued function parameterized by \(_{i}\) and \(_{i}(0,_{i})\) denotes the noise.

We denote the operation that alters variable \(\) with fixed value \(\) by \(Rh(=x)\), meaning that we perform a **rehearsal** of setting \(\) to \(\) through realistic or hypothetical means. When \(Rh(=)\) is applied, we remove the incoming arrows of \(\) in the original rehearsal graph \(G\) to get an _alteration rehearsal graph_\(G^{=}\), and the accompanying structural equations are modified according to the newly introduced parental relations in \(G^{=}\), where a new set of parameters in \(\) could be introduced for the unseen parental relations to offer more flexible modeling abilities. The distribution \(( Rh(=))\) of all variables after applying \(Rh(=)\) can be derived from Eq. (1). Example illustrations are in Figs. 2b-2c. The "invalid" bi-directional "causal" relations are valid in rehearsal graphs: P \(\) Q becomes P \(\) Q when applying \(Rh()\) and becomes P \(\) Q when applying \(Rh()\), capturing the interrelated _influence_ between the two variables.

An important feature of SRM that distinguishes it from SCM is the capability of modeling dynamic and time-dependent real-world decision-making environments, where variable relations may evolve and correct decisions can vary dramatically at different times even for the same problem . It is worth noting that these dynamic environments should not be confused with the conventional notion of dynamic systems, where the relationships among variables simply repeat over time. As causal modeling seeks to identify enduring cause-and-effect relationships, SCMs can be helpful in tasks such as "AI for science" [15; 16; 17], where the objective is to uncover persistent scientific truths. However, SCMs can be restrictive in describing the dynamic nature of decision problems. In contrast, the dynamic and time-dependent feature of environments is captured by SRM using evolving rehearsal graphs and corresponding parameters: At time \(t\), the generating process characterized by \( G_{t},_{t}\) can differ from previous ones. Fig. 2d illustrates an example where correct decisions vary across time and an SRM \(\{ G_{t},_{t}\}_{t}\) is used to model the dynamic decision environment.

Although \(_{t}\) can be learned from data with general supervised learning methods given a fixed graphical structure, generally rehearsal graphs could not be uniquely identified from data. We present basic properties of rehearsal graphs and a preliminary graph class learning procedure in Appendix A.

## 3 The AUF Problem

We treat AUF as a multi-round online decision-making problem, where in the \(t\)-th decision round, an agent makes decisions to affect the outcome if it receives an undesired prediction, serving as a warning signal, from an ML model. In real problems, variables involved in each decision round generally do not appear simultaneously; _e.g._, if a single decision round spans a full month, then a decision-maker may encounter new variables every day. Ignoring the variable generation order can be problematic, but modeling the decision process with refined time granularity is too demanding.

Therefore, we make a midway proposal. We identify two important time points in AUF, the time the ML prediction is made and the time just before the generation of the concerned outcome, so the variables appearing in the \(t\)-th decision round fall into three consecutive time segments separated by the two time points: \(_{t}\), variables appeared before the prediction is made; \(_{t}\), variables appeared after the prediction and before the generation of the outcome; and \(_{t}\), the outcome variable. After the prediction, if the agent decides to do something to change the future outcome, the only variables it can alter are those in \(_{t}\), since \(_{t}\) has already happened and cannot be altered. For example, suppose that in the \(t\)-th month, a sales manager predicts the month sales \(_{t}\) on the first day of that month and designs promotions in that month accordingly. Then \(_{t}\) can be marketing variables that appeared

Figure 2: (a) displays an example rehearsal graph. (b) and (c) displays its alteration graphs with rehearsal operations on \(\{\}\) and \(\{\}\) respectively. We use plate notation to simplify the edges: If an endpoint of an edge falls exactly on the boundary of a rectangle, then there are edges for every vertex in that rectangle. (d) shows a full SRM \(\{ G_{t},_{t}\}_{t}\), where \(t\) is the time index and the rehearsal graph \(G_{t}\) and parameters \(_{t}\) can evolve over time. Recall the example of two stores and let the variables follow the depicted SRM. Suppose that at \(t=1\), the second store takes a new marketing strategy that no longer follows the price of the first store, then P will not directly affect Q: The generation process evolves to P \(\) Q in \(G_{1}\) instead of P \(\) Q in \(G_{0}\). Then for the first store, a previously correct decision at \(t=0\), which aims to affect C by altering P, is ineffective at \(t=1\).

before the first day and \(_{t}\) can be variables in the remaining days of that month, such as the price next week, which can be altered to affect month sales.

More specifically, the decision process in the \(t\)-th round is: An agent first observes \(_{t}\), with which an ML model gives a prediction \(}_{t}\) for the outcome; if \(}_{t}\), where \(\) denotes the set of desired values, the agent should perform alterations on \(_{t}\) to prevent \(_{t}\); after the alteration, the agent can observe full \(_{t}\) and the outcome \(_{t}\). We assume that the generation mechanism of the encountered variables can be described by an SRM \(\{ G_{t},_{t}\}_{t}\), where \( G_{t},_{t}\) characterizes the generation process in the \(t\)-th round. Fig. 3 depicts an example. For simplicity, we assume that the environment is stable, _i.e._, \(G_{t}=G\), \(_{t}=\) for all \(t\). Note that the agent does not have access to the true SRM, instead, it can access historical data \(D=\{(^{i},^{i},^{i})\}_{i=1}^{m}\), from which some information about the SRM can be learned.

In real scenarios, some variables cannot be manually altered or cannot be set to certain values. We denote the set of feasible alteration values for an actionable variable \(Z_{i}\) by \((Z_{i})\), which we assume to be a closed interval. An alteration is denoted by \(_{t}=\{(Z_{a_{i}},z_{a_{i}})\}_{i=1}^{k}\), where \(Z_{a_{i}}\) is the variable to be altered, \(z_{a_{i}}(Z_{a_{i}})\) is the alteration value, and \(k\) is the alteration size. Applying \(_{t}\) is treated as performing a rehearsal \(Rh(_{t}) Rh(\{Z_{a_{i}}=z_{a_{i}}\}_{i=1}^{k})\) on \(G\).

The overall AUF problem is then given \(D\) and \(_{t}\), \(t=1,,T\), that arrives sequentially, find alterations \(_{t}\) in each round \(t\) to successfully avoid the undesired future as many times as possible. The problem can be formulated as

\[_{_{1:T}}_{_{1:T}}[_{ t=1}^{T}(_{t}_{t},Rh(_{t}) )],\] (2)

where \(\) is the noise in Eq. (1) and \(()\) is the indicator function.

If the underlying SRM is known, one can obtain \((_{t}_{t},Rh(_{t}))\), and the only issue is searching for a \(_{t}\) that maximizes \((_{t}_{t},Rh(_{t}))\). The main obstacle to solving AUF, however, is the uncertainty of the SRM. The uncertainty leads to an exploration-exploitation trade-off: In each round, if the agent chooses alterations that help reveal true SRMs (exploration), a more accurate environment model may be obtained and benefit decisions in future rounds, but it may fail in the current round. On the other hand, if the agent chooses a short-sighted decision that successfully avoids the upcoming undesired future (exploitation), it may learn little about SRMs, making future optimizations difficult. In addition, the effect of alterations can propagate to downstream variables through the directional edges in a rehearsal graph, thus the value of unactionable variables may be affected by altering the actionable ones. A feasible approach should take account of the uncertainty, exploration-exploitation trade-off, and propagation of effects in the online decision process and yield reasonable decisions.

Some extra knowledge can be utilized. First, the partition of the three sets of variables reflects the order in which the variables are generated. Any edge crossing the three sets should point from the previous one to the latter one, which reduces the uncertainty on \(G\). Further, since an agent observes \(_{t}\) before making a decision, the generating mechanism inside \(_{t}\) does not affect the distribution of outcomes, as shown in Prop. 4, so we can safely pretend that \(G\) does not have edges inside \(_{t}\).

**Proposition 4**.: _For any \( G_{1},_{1}\) and \( G_{2},_{2}\) on \(_{t}_{t}_{t}\), if they differ only in describing the generation of \(_{t}\), then for any \(_{t}=_{t}\) and alterations \(\) on \(_{t}\), we have_

\[(_{t}_{t},Rh()\,;G_{1},_{1})=(_{t}_{t},Rh()\,; G_{2},_{2}).\]

In the following, we restrict our attention to a basic instance of the AUF problem: The structural equations \(f_{i}\)s are linear and the desired set \(\) is a convex polytope, _i.e._,

\[_{i}^{c}:=_{i}^{T}_{i}^{G}+ _{i},\ =\{^{|_{t}|}\},\] (3)

where \(||\) denotes the cardinality of a set, \(^{s}\), \(^{s||}\), and \(_{i}^{|_{i}^{G}|| _{i}^{c}|}\).

Figure 3: Rehearsal graph \(G_{t}\) in the \(t\)-th round. The timeline shows the order in which the three sets of variables are generated and events that happened to an agent at different times in _one round_.

## 4 Rehearsal Learning

In this section, we present a rehearsal learning framework to address the AUF problem in Eq. (2) and present specific methods for the linear case in Eq. (3), in which we use sampling and rehearsal of actions as a basic building block to find and evaluate decisions.

To account for the uncertainty introduced by limited data, we resort to Bayesian inference. The state of knowledge of the SRM is described by the posterior distribution:

\[(G, D_{t})=(G D_{t})( G,D_{t}),\] (4)

where \(D_{t}\) is the evidence collected until the beginning of round \(t\), consisting of the initial observational data \(D\) and \(\{(_{i},_{i},_{i},_{i})\}_{i=1}^{t-1}\). To handle the exploration-exploitation trade-off between choosing alterations that help recovers the true SRM and alterations that can avoid undesired future in the current round, we leave the choice to the agent by introducing a hyper-parameter \(\) that balances these two kinds of alterations. In the \(t\)-th round, we propose to solve

\[_{_{t}} (,;_{t},_{t}  D_{t},_{t},Rh(_{t}))\] (5) s.t. \[_{G, D_{t}}[(_ {t} G,,_{t},Rh(_{t}))].\] (6)

The constraint in Eq. (6) reflects the required performance: Performing the alteration should make the belief that the desired future can be successfully achieved greater than \(\) given knowledge of the SRM so far. The objective function in Eq. (5) denotes the mutual information between the SRM and the observations revealed after the alteration. Overall, the solution to the optimization problem should avoid undesired future and meanwhile be most informative about the true SRM.

The proposed optimization problem introduces computational challenges, _e.g._, one has to sum over a super-exponential number of rehearsal graphs to compute the mutual information or the success probability. We divide rehearsal learning into three approximately tractable components: Bayesian update of SRMs, candidate alteration selection, and mutual information maximization. To better guide decision-making, we also present a sampling-based probably approximately correct (PAC) bound to quantify the risk associated with the selected alteration.

### Bayesian Update of SRMs

Recall that the posterior of SRMs is divided into the posterior of graphs \((G D_{t})\) and that of structural equation parameters \(( G,D_{t})\) in Eq. (4). For the former term, we adopt a bootstrapping-based approximation , which learns multiple rehearsal graphs on bootstrapped data to build a set of graphs \(\) and weigh each graph \(G\) equally. The approximated posterior of graphs is

\[(G D_{t})|}_{G^{} }(G^{}=G).\] (7)

Consequently, for the second term \((,D_{t})\), we only need to estimate parameters corresponding to graphs that appear in \(\), which greatly reduces computation. The parameter posterior factorizes as

\[( G,D_{t})=_{G^{}(G), ^{c} G^{}}(_{^{c}},_{^{c}} G^{},D_{t}),\]

where \((G)\) is a set of graphs that includes \(G\) and related alteration graphs and \(^{c}\) denotes a maximal bi-directional clique. \((_{^{c}},_{^{c}} G^{ },D_{t})\) is the posterior of a set of regression parameters and noise variance given data of \(^{c}\) and its parents in \(G^{}\). So various Bayesian learning methods  are applicable for its estimation. We adopt Bayesian ridge regression  to learn the parameters. In addition, as graph learning is time-consuming, we use an incremental implementation: We build an initial graph posterior from purely observational data for \(t=0\) using Eq. (7), then update the posterior with

\[(G D_{t})(G D_{t-1})_{ i=1}^{n}(_{t},_{t}_{t},_{t},G, ^{i}),\]

where \(^{i}\) is sampled from \(( G,D_{t-1})\) and the averaged summation term is an empirical estimate of likelihood. With the above approximations and learning, we effectively obtain an approximate posterior and an efficient sampler over SRMs, which forms the basis of the following two steps.

### Candidate Alteration Selection

In the second part, we find candidate alterations that satisfy the performance constraint in Eq. (6). The constraint cannot be accurately computed as it takes an expectation over discrete \(G\) and continuous \(\). To reduce the computational burden, we estimate the constraint with posterior samples. Note that the value of variables \(\) is uniquely determined by \(G\), \(\), and the noise \(\) in Eq. (1). We can evaluate all alterations by generating \(_{t}\) from a common set of SRMs and noise samples. Suppose we have an i.i.d. sample \(S=\{ G_{i},_{i},_{i}\}_{i=1}^{n}\), the empirical estimate of Eq. (6) is

\[_{_{t}}(S)|\{i_{t,_{t}}^{i} \}|/n,\]

where \(_{t,_{t}}^{i}\) is the concerned outcome generated following the SRM specified by \( G_{i},_{i},_{i}\) and \(Rh(_{t})\). A general method for candidate alteration selection has three stages:

1. Sample two sets of i.i.d. parameters, the training set \(S_{tr}=\{ G_{i},\,_{i},\,_{i}\}_{i=1}^ {n}\) and the validation set \(S_{val}=\{ G_{i},\,_{i},\,_{i}\}_{i=n +1}^{2n}\), from \((G, D_{t})\) and \(()\).
2. Build a candidate alteration set \(\) by conducting rehearsals on \(S_{tr}\) and finding alterations \(_{t}\) that achieve required performance \(\), _i.e._, \(_{t},_{_{t}}(S_{tr})\).
3. Validate alterations in \(\) by rehearsals on \(S_{val}\) and remove those with \(_{_{t}}(S_{val})<\) from \(\).

The validation procedure is to alleviate overfitting as in standard machine learning pipelines. If stage 2 or 3 outputs an empty set, the system should refuse to make any recommendations. The agent can lower \(\) and restart if a smaller \(\) is still adequate for the task at hand.

For a given alteration, validation is relatively easy to perform. The only issue to address is to find a set of alterations that pass the test on \(S_{tr}\). An immediate approach would be enumerating possible alterations and making rehearsals on training samples to check the constraint. But the enumeration can be computationally prohibitive as the alteration space can be combinatorial and continuous. A compromise between the optimality of recommended alterations and the computational feasibility is to work on some finite subset of alterations using some discretization tricks or heuristics. Fortunately, due to the linear structure of the SRMs considered here, Prop. 5 shows that the outcome variables are linear in the alteration values, implying the existence of more efficient and effective solutions.

**Proposition 5**.: _Let \(=\{(Z_{a_{i}},z_{a_{i}})\}_{i=1}^{k}\) be a valid alteration where \(Z_{a_{i}}_{t}\) and \(z_{a_{i}}(Z_{a_{i}})\). Given \( G,,\) and \(_{t}\), the outcome variables are linear in the alteration values. We have_

\[_{t}=_{t}+^{}+ ,\] (8)

_where \(^{}=(z_{a_{1}},,z_{a_{k}})^{T}\). \(\), \(\), and \(\) are constant matrices of appropriate shapes and are uniquely determined by \(G\), \(\) and altered variables \(^{}=\{Z_{a_{i}}\}_{i=1}^{k}\)._

Size-\(1\) Alteration.We consider cases where an agent can alter only one variable in each round, _i.e._, \(k=1\). Suppose the altered variable in round \(t\) is \(Z_{i}\). Provided with a fixed SRM, along with fixed noise, Eq. (8) reduces to a line in \(^{||}\) with direction \(^{||}\). Due to the convexity of the desired region \(\), if altering \(Z_{i}\) can make \(\), then the set of feasible alteration values can be represented by an interval \([u,v]\) (the endpoints can be infinity). Finding candidate alterations on the training data is then equivalent to finding elements that intersect at least \(n\) intervals obtained from samples in \(S_{tr}\), which is easy to compute. Alg. 1 includes the basic training and validation stages of finding candidate alterations. The overall time complexity is \(O(|_{t}| n n)\). A more detailed algorithm and running time analysis are given in Appendix B.

Size-\(k\) Alteration.By relating to the NP-hard maximum feasible subsystem problem , we prove Thm. 6, showing that finding a single \(\) that satisfies the constraint, which is much simplerthan finding all valid alterations, is difficult if \(=\). This result suggests that we should focus on approximate solutions rather than exact ones. We also give a mixed integer linear programming approach in Appendix B.

**Theorem 6**.: _Unless \(=\), finding a \(k\)-dimensional \(^{}(^{})\) that satisfies \(_{i=1}^{n}((_{i}+_{i} ^{}+_{i}_{i}))  n\) (if there is a valid solution) is not solvable with any algorithm of running time polynomial in \(k\) and \(n\)._

### Mutual Information Maximization

The remaining task is to optimize the mutual information (5) over the found candidate alteration set \(\). We factorize the objective function with two information entropy terms over \(_{t}\) and \(_{t}\):

\[(,;_{t},_{t}\,|\,D _{t},_{t},_{t})=(_{t},_{t}\,| \,D_{t},_{t},_{t})-(_{t},_{t}\,|\, ,,D_{t},_{t},_{t}).\] (9)

Eq. (9) can be estimated with samples from \((G, D_{t})\) and \((_{t},_{t} G,,_ {t},_{t})\). Due to the sampling involved in the estimation procedure, Eq. (9) is expensive to evaluate. The continuous input space makes it difficult to find an optimal alteration value for a fixed alteration target \(^{}\). Thus, we find \(^{*}\) with Bayesian optimization , which seeks to optimize a black-box function \(g()\) with a small number of evaluations by iteratively querying function values at some data points. We run the optimization procedure for every alteration target in \(\) and find the corresponding best alteration values. Finally, the alteration that gives maximum mutual information is selected as the recommended output. Details about the estimation procedure and Bayesian optimization are given in Appendix C.

Given a decision, it is important to know how likely the decision can successfully avoid the undesired future. Though the candidate selection procedure already rules out potentially terrible alterations using \(S_{tr}\) and \(S_{val}\), a more refined measure is preferable. Based on the scenario approach in robust control literature [23; 24], we derive a PAC-style bound on the posterior success probability in Thm. 7. The bound is easy to compute as it only requires posterior samples. The practical implication of the bound is that it quantifies the uncertainty, or risk, associated with the alteration, thus can help make better decisions. Notably, the bound holds for any SRM with arbitrary additive noise, which could be helpful for future extensions with non-Gaussian noise and nonlinear SRMs.

**Theorem 7**.: _Given an observed evidence \(_{t}\), an alteration \(\), and a desired set \(=\{^{||} \}\), let \(S_{eval}=\{ G_{i},_{i},_{i} \}_{i=1}^{n}\) be \(n\) i.i.d. samples from \((G,, D_{t})\). Let \(\{^{i}\}_{i=1}^{n}\) be generated from \(Rh()\) and \(S_{eval}\) following the structural equations in Eq. (1). Define \(n_{o}|\{i^{i}\}|\). Let \( 1-n_{o}/n\), \( 1-F^{-1}(1-/(2n);n_{o}+1,n-n_{o})\) if \(n_{o}<n\) otherwise \( 0\), and \( 1-F^{-1}(/(2n);n_{o},n-n_{o}+1)\) if \(n_{o}>0\) otherwise \( 1\), where \(F^{-1}(:,)\) is the inverse cumulative distribution function of the beta distribution with parameters \(\) and \(\). For any \((0,1)\), with probability at least \(1-\), we have_

\[\{,-\}(_{t}|D_{t},_{t},Rh()) \{,+\}.\]

### Discussion

The proposed rehearsal learning framework is general and flexible, with any of the three steps adjustable without interfering with the others. For scenarios more complicated than linear ones, one can replace methods in each step accordingly: The Bayesian update of SRMs can use any advanced Bayesian (structure) learning methods [25; 26; 27; 28] as long as they allow sampling from the posterior. The mutual information maximization step can leverage any black-box optimization methods [22; 29]. For the most important and difficult candidate alteration finding step, which is difficult even for the linear case when allowing joint alterations, efficient heuristics or linearization tricks can be used. Given the great uncertainty of real-world problems, exactly solving the AUF problem is extremely difficult if not impossible, while with the rehearsal learning framework, we provide a practical and systematic approximate solution that tackles the problem with certain guarantees, which exhibits that developing more efficient approximate solutions is a promising future direction.

For the SRM, there are some practical considerations. After a rehearsal operation, the change of the parental relations results in new sets of parameters. In the absence of further constraints, as is the case in this work, these new parameters may bear no direct resemblance to the previous ones. However, it is conceivable to impose specific assumptions on these parameters to achieve more efficient modeling. For instance, one may assume the new parameters are connected with the original ones through operations like marginalization. In practical implementations, it is also reasonable to limit the size of a clique and maintain the number of parameter sets to an acceptable level. Moreover, it is unlikely that we would need to or be able to alter every possible subset of a clique, so one can reduce unnecessary computational burden by ignoring the parameters that would not be used.

## 5 Related Work

There have been some efforts to combine structural models with decision-making, with most of them focusing on causal structures . Some work explores the causal bandit problem, where the causal structure underlying rewards and actions is considered [31; 32; 33; 34; 35; 36]. In a different vein, Aglietti et al.  investigated a Bayesian optimization problem with a known causal DAG, and Aglietti et al.  extended this work to a dynamic setting. However, these studies typically assume that the true causal graph is known, a condition that is challenging to meet in real-world applications, although causal discovery methods could help to some extent [39; 40; 41; 42; 43; 44]. To remove the assumption, de Kroon et al.  leveraged separating sets, while Lu et al.  focused on the Markov equivalence class of the true causal graphs. However, even when causal relations are known, applying causal bandits or causal Bayesian optimization to AUF is not suitable because they often seek a single universally optimal action. In contrast, optimal decisions in AUF can vary depending on the context \(\). Some research approaches decision problems from the perspective of causal structures and causal effect estimation. For example, Wang et al.  developed a method for estimating a set of possible causal effects to aid decision-making. Additionally, there has been active research in estimating causal structures or effects in interactive environments [48; 49; 50; 51; 52].

The aforementioned efforts yielded effective methods for certain decision-related problems, but all of them rely on causal modeling, which could be too luxurious and restrictive for decision problems . On the other hand, correlation, which is the basis of most ML models, is insufficient for decision-making. Therefore, we turn to the _influence relation_, which lies between correlation and causation and forms a basis of decisions . Building on the influence relation, we propose SRM, which is capable of modeling interrelated but not necessarily causally linked variables and dynamically evolving decision systems. Moreover, the proposed rehearsal learning framework demonstrates the feasibility of building decision-making upon influence and rehearsal.

## 6 Experiments

We evaluate the proposed approach on two datasets. We are mainly interested in if the proposed approach can successfully avoid the undesired future with high probability, the exploration-exploitation trade-off, and the informativeness of the PAC-style guarantees. For each dataset, we alter one variable in each round and repeat experiments with 100 rounds 20 times. The graph prior is initialized with samples from the learnable graph equivalence class instead of learning from data as well-established rehearsal graph learning methods are still in the process of development. The success probability is estimated with 1,000 samples from the true SRM. We compute the PAC bound with 1,000 samples from the posterior with \(=0.05\). The observational dataset size is set to 10. We also compare with several reinforcement learning methods DDPG , PPO , SAC , and CATS .

Ride-Hailing Data.We abstract an SRM from a ride-hailing scenario, where a ride-hailing app needs to make decisions to promote user rating (RAT). We consider relevant variables including the weather condition, the number of users, traffic congestion (TRA), etc. The app can alter two variables, the discount level (DIS) and the recommendation level (REC) of a specific route. There exist interrelated variables in the scenario: If REC of a specific route is high, then TRA on that route will be high since more users will choose that route; and if there is a high TRA on that route, the REC should be low. The size of \(_{t}\), \(_{t}\), and \(_{t}\) are 2, 4, and 1 respectively. The true structural equations are set according to domain knowledge. The feasible alteration values are \([-2,2]\) for DIS and REC. The range of RAT is \(\), so we set the desired region to \(=[0.8,1]\) to avoid RAT below \(0.8\).

Bermuda Data.We take an example from ecology, where environment variables in Bermuda are recorded  and the variable generation order is available . The size of \(_{t}\), \(_{t}\), \(_{t}\) are 3, 7, and 1 respectively. The true structural equations are obtained by performing linear regression on normalized data. We assume that 5 variables are actionable , with a feasible alteration region \([-1,1]\). \(_{t}\) represents the net coral ecosystem calcification (NEC). We want to maintain a high NEC. So we set the desired region to \(=[0.5,2]\) which is above the 75th percentile of the dataset.

Table 1 shows the average probabilities of successfully avoiding the undesired future of several RL methods and the proposed rehearsal learning method with \(=0.7\). Our method achieves the goal of AUF with a probability around 0.7, while the others fail to do so with merely 100 interactions with the environment, which further underscores the importance of considering structural information in the interaction-limited AUF problem. On the other hand, if we increase \(T\), RL methods can achieve satisfying performance, e.g., DDPG achieves 0.688 average success probability when \(T=10,000\) on the Bermuda data. For the ride-hailing data, 95.8% alterations recommended by the proposed method are to alter REC, which affects RAT through the interactions between REC and TRA and the direct link between TRA and REC: REC \(\) TRA \(\) RAT. Note that REC cannot be identified as a valid cause so causal modeling is not suitable for this problem.

Fig. 4 shows the full results. Given \(\), the found alteration can satisfy the probability requirement in most cases. Exceptions are settings where \(=0.9\). Alterations with such a high success probability rarely exist, so the method can fail. In our implementation, when the recommendation is not attainable, the constraint is dropped and the method will only optimize the mutual information. Therefore the corresponding number of success rounds, as well as the success probability, is low, but the mutual information is high. The third column shows a trade-off between avoiding the undesired future and learning the SRM: smaller \(\) is likely to give higher mutual information. An interesting phenomenon is that the true success probability often fluctuates around \(\) instead of achieving higher values. An explanation is that if an alteration has a high success probability, then the effect of the alteration is less uncertain, so less new information can be revealed and the method will instead choose others with lower success probabilities. Finally, the last column shows that the true probabilities are bounded by the PAC-style bound with a relatively small gap, so it can be informative for guiding real decisions.

## 7 Conclusion

Realizing that _correlation_ is inadequate and that _causation_ is not always suitable, we advocate relying on the _influence_ relation for decision-making. In this paper, we propose the first rehearsal learning framework that tackles the AUF (Avoiding Undesired Future) problem with influence modeling. The key to the proposed framework is SRM, which considers the interrelations of variables and the dynamic time-dependent nature of decision environments. By unifying rehearsals on SRMs and Bayesian inference, the framework recommends decisions that can persuasively avoid the undesired future. We also provide a PAC-style bound to quantify the associated risk of recommended decisions and further show the hardness of a basic linear instance of the framework, revealing that future work should focus on developing approximate solutions rather than exact ones.

   Dataset & DDPG & PPO & SAC & CATS & \(=0.7\) \\  Ride-hailing & 0.173 & 0.154 & 0.177 & 0.104 & 0.714 \\ Bermuda & 0.230 & 0.190 & 0.205 & 0.215 & 0.679 \\   

Table 1: Avg. success probability.

Figure 4: Results on the Ride-hailing and the Bermuda data. The bands depict standard deviations.

[MISSING_PAGE_FAIL:11]

*  Adam Massmann, Pierre Gentine, and Jakob Runge. Causal inference for process understanding in earth sciences. _CoRR_, abs/2105.00912, 2021.
*  Nir Friedman, Moises Goldszmidt, and Abraham J. Wyner. Data analysis with Bayesian networks: A bootstrap approach. _CoRR_, abs/1301.6695, 2013.
*  Christopher M. Bishop. _Pattern Recognition and Machine Learning_. Springer, 2006.
*  Michael E. Tipping. Sparse bayesian learning and the relevance vector machine. _Journal of Machine Learning Research_, 1:211-244, 2001.
*  Edoardo Amaldi and Viggo Kann. The complexity and approximability of finding maximum feasible subsystems of linear relations. _Theoretical Computer Science_, 147(1):181-210, 1995.
*  Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. _Proceedings of the IEEE_, 104(1):148-175, 2016.
*  Giuseppe C. Calafiore and Marco C. Campi. The scenario approach to robust control design. _IEEE Transactions on Automatic Control_, 51(5):742-753, 2006.
*  Thom S. Badings, Alessandro Abate, Nils Jansen, David Parker, Hasan A. Poonawala, and Marielle Stoelinga. Sampling-based robust control of autonomous systems with non-Gaussian noise. In _Thirty-Sixth AAAI Conference on Artificial Intelligence_, pages 9669-9678, 2022.
*  Christopher KI Williams and Carl Edward Rasmussen. _Gaussian Processes for Machine Learning_. MIT press Cambridge, 2006.
*  Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm. In _Advances in Neural Information Processing Systems_, pages 2370-2378, 2016.
*  Raj Agrawal, Chandler Squires, Karren D. Yang, Karthikeyan Shanmugam, and Caroline Uhler. ABCD-strategy: Budgeted experimental design for targeted causal structure discovery. In _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, pages 3400-3409, 2019.
*  Lars Lorch, Jonas Rothfuss, Bernhard Scholkopf, and Andreas Krause. DiBS: Differentiable Bayesian structure learning. In _Advances in Neural Information Processing Systems_, pages 24111-24123, 2021.
*  Sourabh Katoch, Sumit Singh Chauhan, and Vijay Kumar. A review on genetic algorithm: Past, present, and future. _Multimedia Tools and Applications_, 80(5):8091-8126, 2021.
*  Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. Offline reinforcement learning with causal structured world models. _CoRR_, abs/2206.01474, 2022.
*  Finnian Lattimore, Tor Lattimore, and Mark D. Reid. Causal bandits: Learning good interventions via causal inference. In _Advances in Neural Information Processing Systems_, pages 1181-1189, 2016.
*  Akihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori Kakimura, Takuro Fukunaga, and Ken-ichi Kawarabayashi. Causal bandits with propagating inference. In _Proceedings of the Thirty-Fifth International Conference on Machine Learning_, pages 5508-5516, 2018.
*  Sanghack Lee and Elias Bareinboim. Structural causal bandits: Where to intervene? In _Advances in Neural Information Processing Systems_, pages 2573-2583, 2018.
*  Sanghack Lee and Elias Bareinboim. Structural causal bandits with non-manipulable variables. In _Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence_, pages 4164-4172, 2019.
*  Yangyi Lu, Amirhossein Meisami, Ambuj Tewari, and William Yan. Regret analysis of bandit problems with causal background knowledge. In _Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence_, pages 141-150, 2020.

*  Neil Dhir. Chronological causal bandits. _CoRR_, abs/2112.01819, 2021.
*  Virginia Aglietti, Xiaoyu Lu, Andrei Paleyes, and Javier Gonzalez. Causal Bayesian optimization. In _Proceedings of the Twenty-Third International Conference on Artificial Intelligence and Statistics_, pages 3155-3164, 2020.
*  Virginia Aglietti, Neil Dhir, Javier Gonzalez, and Theodoros Damoulas. Dynamic causal Bayesian optimization. In _Advances in Neural Information Processing Systems_, pages 10549-10560, 2021.
*  Peter Spirtes, Clark Glymour, and Richard Scheines. _Causation, Prediction, and Search_. MIT press, 2nd edition, 2000.
*  David M. Chickering. Optimal structure identification with greedy search. _Journal of Machine Learning Research_, 3:507-554, 2002.
*  Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvarinen, and Antti J. Kerminen. A linear non-Gaussian acyclic model for causal discovery. _Journal of Machine Learning Research_, 7:2003-2030, 2006.
*  Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear causal discovery with additive noise models. In _Advances in Neural Information Processing Systems_, pages 689-696, 2008.
*  Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence_, pages 647-655, 2009.
*  Tian Qin, Tian-Zuo Wang, and Zhi-Hua Zhou. Learning causal structure on mixed data with tree-structured functional models. In _Proceedings of the Twenty-Third SIAM International Conference on Data Mining_, pages 613-621, 2023.
*  Arnoud A. W. M. de Kroon, Danielle Belgrave, and Joris M. Mooij. Causal discovery for causal bandits utilizing separating sets. _CoRR_, abs/2009.07916, 2020.
*  Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari. Causal bandits with unknown graph structure. In _Advances in Neural Information Processing Systems_, pages 24817-24828, 2021.
*  Tian-Zuo Wang, Tian Qin, and Zhi-Hua Zhou. Estimating possible causal effects with latent variables via adjustment. In _Proceedings of the Fortieth International Conference on Machine Learning_, pages 36308-36335, 2023.
*  Tian-Zuo Wang, Xi-Zhu Wu, Sheng-Jun Huang, and Zhi-Hua Zhou. Cost-effectively identifying causal effects when only response variable is observable. In _Proceedings of the Thirty-Seventh International Conference on Machine Learning_, pages 10060-10069, 2020.
*  Tian-Zuo Wang and Zhi-Hua Zhou. Actively identifying causal effects with latent variables given only response variable observable. In _Advances in Neural Information Processing Systems_, pages 15007-15018, 2021.
*  Tian Qin, Tian-Zuo Wang, and Zhi-Hua Zhou. Budgeted heterogeneous treatment effect estimation. In _Proceedings of the Thirty-Eighth International Conference on Machine Learning_, pages 8693-8702, 2021.
*  Tian-Zuo Wang, Tian Qin, and Zhi-Hua Zhou. Sound and complete causal identification with latent variables given local background knowledge. In _Advances in Neural Information Processing Systems_, pages 10325-10338, 2022.
*  Tian-Zuo Wang, Tian Qin, and Zhi-Hua Zhou. Sound and complete causal identification with latent variables given local background knowledge. _Artificial Intelligence_, 322:103964, 2023.
*  Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In _Proceedings of the Fourth International Conference on Learning Representations_, 2016.

*  John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.
*  Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Proceedings of the Thirty-Fifth International Conference on Machine Learning_, pages 1856-1865, 2018.
*  Maryam Majzoubi, Chicheng Zhang, Rajan Chari, Akshay Krishnamurthy, John Langford, and Aleksandrs Slivkins. Efficient contextual bandits with continuous actions. In _Advances in Neural Information Processing Systems_, pages 349-360, 2020.
*  Travis A. Courtney, Mario Lebrato, Nicholas R. Bates, Andrew Collins, Samantha J. de Putron, Rebecca Garley, Rod Johnson, Juan-Carlos Molinero, Timothy J. Noyes, Christopher L. Sabine, and Andreas J. Andersson. Environmental controls on modern scleractinian coral and reef-scale calcification. _Science Advances_, 3(11):e1701356, 2017.
*  Andreas Andersson and Nicholas Bates. In situ measurements used for coral and reef-scale calcification structural equation modeling including environmental and chemical measurements, and coral calcification rates in Bermuda from 2010 to 2012 (BEACON project), 2018. http://lod.bco-dmo.org/id/dataset/720788.
*  Kumar Sricharan, Dennis L. Wei, and Alfred O. Hero III. Ensemble estimators for multivariate entropy estimation. _IEEE Transactions on Information Theory_, 59(7):4374-4388, 2013.
*  Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _Proceedings of the Twenty-Seventh International Conference on Machine Learning_, pages 1015-1022, 2010.
*  Stephen Boyd and Lieven Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.
*  Licio Romao, Kostas Margellos, and Antonis Papachristodoulou. Tight generalization guarantees for the sampling and discarding approach to scenario optimization. In _Proceedings of the Fifty-ninth IEEE Conference on Decision and Control_, pages 2228-2233, 2020.

Rehearsal Graphs: Characterization and Learning

In this section, we present full motivations, basic properties, and preliminary learning strategies for rehearsal graphs.

The ability to describe and identify causal relations is at the heart of both human and artificial intelligence. The work of Pearl  on causality lays the foundation for accurately describing causal relations and making a sound inference. Despite its elegance and expressiveness in abstracting causality from human cognition, one may encounter difficulties when applying Pearl's causal framework to real-world observations. We identify three kinds of misspecifications that could make causal modeling fail: a) model misspecification b) data-collection misspecification c) adversary misspecification.

Model misspecification means that the variables used in the causal modeling process are inaccurate so there are indeterminant causal relations among the variables. For example, demand (\(D\)) and price (\(P\)) are two variables that are causally related. But there is not a determinant causal direction between \(D\) and \(P\) since intervening on \(D\) causes \(P\) to change but meanwhile intervening on \(P\) causes \(D\) to change. A workaround is modeling with finer granularity by adding a time index to \(D\) and \(P\). Then we have \(D_{t}\) causes \(P_{t+1}\) and \(P_{t}\) causes \(D_{t+1}\).

Data-collection misspecification means that the data collected are not perfectly aligned with the specified variable. For example, we may have variables \(D_{1},D_{2},P_{1},P_{2}\) in the causal model that denote two measures \(D\) and \(P\) at different time. But the data collected may not rigorously follow the order since there can be time delays and mismatches of the modeling time intervals and real ones. This misspecification results in a misalignment between the model and data, hence no sound causal conclusions can be drawn from the data.

Adversary misspecification refers to ignoring the existence of adversaries in causal modeling. For example, let \(P\) and \(Q\) denote the prices of a product in two stores. If \(P\) is decreased then \(Q\) will also be decreased, otherwise, the second store will lose some consumers. From a causal view, it seems that \(P\) causes \(Q\). But obviously, \(Q\) also causes \(P\) by symmetry. So we have a bi-directional causal relation, which is cannot be described in causal modeling. The reason for this phenomenon is that each store is an adversary that reacts to interventions from other agents.

The preceding three misspecifications are not necessarily disjoint in real problems and are hard to identify in advance. The consequence is that one cannot identify perfect causal knowledge even if with an infinite amount of data since the modeling and the data do not exhibit consistent determinant causal relations. In order to model both the non-determinant and determinant causal relations in data, we introduce a new probabilistic graphical model: rehearsal graphs.

### Rehearsal Graphs as a Probabilistic Graphical Model

**Definition 8** (Mixed graph).: Let \(G=(,)\) be a graph, where \(\) denotes the vertices and \(\) the edges. \(G\) is a _mixed graph_ if for any distinct vertices \(u,v\), there is at most one edge connecting them, and the edge can be either directional or bi-directional, _i.e._, \(u v\), \(u v\), or \(u v\).

**Definition 9** (Bi-directional clique).: A _bi-directional clique_\(C=(^{c},^{c})\) of a mixed graph \(G=(,)\) is a complete induced subgraph such that any edge \(e^{c}\) is bi-directional. A single vertex also constituents a valid bi-directional clique. \(C\) is _maximal_ if adding any other vertex does not induce a bi-directional clique.

**Definition 10** (Rehearsal graph).: Let \(G=(,)\) be a mixed graph. Let \(\{C_{i}\}_{i=1}^{l}\) denote all maximal bi-directional cliques of \(G\), where \(C_{i}=(_{i}^{c},_{i}^{c})\). \(G\) is a _rehearsal graph_ if and only if:

1. \(_{i}^{c}_{j}^{c}=\) for any \(i j\).
2. \( i[l]\), \(u_{i}^{c}\), if there is any edge pointing from \(u\) to \(_{i}^{c}\), then \( v_{i}^{c}\), \(u v\).
3. The directional edges permit a topological ordering for \(\{C_{i}\}_{i=1}^{l}\).

**Definition 11** (Factorization).: Let \(G=(,)\) be a rehearsal graph and \(C_{i}=(_{i}^{c},_{i}^{c}),i[k]\) be maximal bi-directional cliques of \(G\). We say that a joint probability \(P\) over \(\) factorizes according to \(G\) if \(P\) can be expressed as a product

\[P()=_{i=1}^{l}P(_{i}^{c}_{i}),\] (10)

where \(_{i}=\{u v_{i}^{c},u vG\}\).

### \(c\)-Separation

As a probabilistic graphical model, like Bayesian networks or Markov networks, rehearsal graphs also have a graphical characterization of independence relations. Mimicking the \(d\)-separation for Bayesian networks, we present \(c\)-separation for rehearsal graphs. Here the "\(c\)" means "cliques".

**Definition 12** (\(c\)-separation).: A path \(p\) in a rehearsal graph \(G\) is said to be \(c\)-separated (or blocked) by a set of vertices \(Z\) if and only if

1. \(p\) contains a chain \(i m_{1} m_{2} m_{l} j\) or a fork \(i m_{1} m_{2} m _{l} j\) such that the bi-directionally connected vertices \(\{m_{i}\}_{i=1}^{l} Z\), or
2. \(p\) contains an inverted fork (or collider) \(i m_{1} m_{2} m_{l} j\) such that no vertices in \(\{m_{i}\}_{i=1}^{l}\) are in \(Z\) and such that no descendants of \(\{m_{i}\}_{i=1}^{l}\) are in \(Z\).

A set \(Z\) is said to \(c\)-separate \(X\) from \(Y\) if and only if \(Z\) blocks every path from a vertex in \(X\) to a vertex in \(Z\). We write \(X\!\!\!_{G}Y Z\) when \(Z\)\(c\)-separates \(X\) from \(Y\) in \(G\).

**Lemma 13**.: _Let \(G=(,)\) be a rehearsal graph, \(x,y\) be two distinct vertices, and \(Z\) a set of vertices. Let \(C(v)\) denote the vertices of the maximal bi-directional clique to which \(v\) belongs. If \(x\!\!\!_{G}y Z\), then \(C(x)\!\!\!_{G}C(y) Z\)._

Proof.: We prove this by contradiction. Assume that \(C(x)\!\!\!_{G}C(y) Z\) does not hold given \(x\!\!\!_{G}y Z\), meaning that there exists a path \(p\) unblocked by \(Z\) between some \(u C(x)\) and some \(v C(y)\). Then there is an unblocked path \(x p y\) between \(x\) and \(y\), which is a contradiction since \(Z\) is supposed to block every path between \(x\) and \(y\). 

**Lemma 14**.: _Let \(G=(,)\) be a rehearsal graph, \(C_{a},C_{b} V\) be a set of vertices of two distinct maximal bi-directional cliques, and \(Z\) a set of vertices. If \(C_{a}\!\!\!_{G}C_{b} Z\), then for all probability functions \(P\) that factorizes according to \(G\), \(C_{a}\!\!\!_{P}C_{b} Z\)._

Proof.: We construct a DAG \(=(,)\) from \(G\) with an one-to-one mapping \(f\): every maximal cliques \(C_{i}\) of \(G\) is represented with a vertex \(_{i}\), \(_{i}=(v_{i}^{1},,v_{i}^{|C_{i}|})\), and the edge directions between \(C_{i}\)s are kept. For any path \(p\)\(_{_{1}}-_{_{2}}--_{_{l}}\) (here we omit the edge directions) in \(\), where \(\{_{i}\}_{i}\) is an index set, there is a counterpart \(q\) in the original \(G\) that reads \(v_{_{1}}^{1} v_{_{1}}^{2}  v_{_{1}}^{|C_{_{1}}|}-v_{_{2}}^{1}  v_{_{2}}^{2} v_{ _{2}}^{|C_{_{2}}|}--v_{_{l}}^{1} v_{ _{l}}^{2} v_{_{l}}^{|C_{_ {l}}|}\), where \(v_{_{k}}^{j}\) is the \(j\)-th vertex in \(C_{_{k}}\). For any \(Z V\), we map it to a set of vertices in \(\) with \(h(Z)=\{ f^{-1}() Z\}\). From the definition of \(c\)-separation for rehearsal graphs and \(d\)-separation for Bayesian networks, we see that if \(q\) is \(c\)-separated by \(Z\) in \(G\), then \(p\) is \(d\)-separated by \(h(Z)\) in \(\). Therefore, if \(C_{a}\!\!\!_{G}C_{b} Z\), then every path between \(}\) and \(}\) is \(d\)-separated by \(h(Z)\). Due to the Markov property of Bayesian networks, for all \(P\) that factorizes according to \(\) (which also factorizes according to \(G\) by definition), we have \(_{a}\!\!\!_{P}_{b} h(Z)\), _i.e._, \(C_{a}\!\!\!_{P}C_{b} f^{-1}(h(Z))\). Since \(f^{-1}(h(Z)) Z\), we have \(C_{a}\!\!\!_{P}C_{b} Z\). 

**Theorem 15**.: _Let \(G=(,)\) be a rehearsal graph. For all probability functions \(P\) that factorizes according to \(G\) and any three disjoint subsets of vertices \(X,Y,Z\), if \(X\!\!\!_{G}Y Z\), then \(X\!\!\!_{P}Y Z\)._

Proof.: \[X\!\!\!_{G}Y Z   x X,y Y,x\!\!\!_{G}y Z\] (11) \[  x X,y Y,C(x)\!\!\!_{G}C(y) Z\] (12) \[  x X,y Y,C(x)\!\!\!_{P}C(y) Z\] (13) \[  x X,y Y,x\!\!\!_{P}y Z\] (14) \[ X\!\!\!_{P}Y Z.\] (15)

**Theorem 16**.: _Let \(G=(,)\) be a rehearsal graph. Let \(G^{}=(,^{})\) be any DAG that have identical skeleton with \(G\) (see Fig. 5 for an example). If all directional edges in \(G\) appear in \(G^{}\), then for any disjoint set of vertices \(X,Y\) and \(Z\), we have \(X\!\!\!_{G}Y Z\) if and only if \(X\!\!\!_{G^{}}Y Z\), where \(\!\!\!_{G}\) means \(c\)-separation for rehearsal graphs and \(\!\!\!_{G^{}}d\)-separation for DAGs._

Proof.: We first prove \(X\!\!\!_{G}Y Z\;\;X\!\!\!_{G^{}}Y  Z\).

Every path \(p\) in \(G\) has a unique counterpart \(p^{}\) in \(G^{}\) that shares the same vertex sequence. \(p\) and \(p^{}\) only differ in some edges: a bi-directional edge \(u v\) in \(G\) is a directed one \(u v\) or \(u v\) in \(G^{}\). Obviously, the mapping between \(p\) and \(p^{}\) is one-to-one. We only need to show that for any path \(p^{}\) in \(G^{}\), if its counterpart \(p\) in \(G\) is \(c\)-separated by a set \(Z\), then \(p^{}\) is \(d\)-separated in \(G\).

We prove it by contradiction. Assume that \(p^{}\) is not \(d\)-separated by \(Z\), then according to the definition of \(d\)-separation, we have that (a) for every chain \(i m j\) and every fork \(i m j\) in \(p^{}\), \(m Z\), and that (b) for every inverted fork \(i m j\) in \(p^{}\), \((\{m\} des(m)) Z\). For every chain \(i m_{1} m_{2} m_{l} j\) and every fork \(i m_{1} m_{2} m _{l} j\) in \(p\), we have \(\{m_{i}\}_{i=1}^{l} Z\) since otherwise either \(* m_{l} j\) in \(p^{}\) and \(m_{l} Z\), or \(* m_{l} j\) in \(p^{}\) and \(m_{l} Z\), which violate requirement (a). For every inverted fork \(i m_{1} m_{2} m_{l} j\) in \(p\), \((\{m_{i}\}_{i=1}^{l} des(\{m_{i}\}_{i=1}^{l})) Z\) since otherwise there exists an inverted fork \(* m_{t}*,t[l]\) such that \((\{m_{t}\} des(m_{t})) Z=\), which violates requirement (b). Combining the above results, we have that \(p\) is not \(c\)-separated by \(Z\), which is a contradiction. Therefore, \(p^{}\) is \(d\)-separated by \(Z\), which concludes that \(X\!\!_{G}Y Z\;\;X\!\!\!_{G^{}}Y Z\).

We next prove \(X\!\!\!_{G^{}}Y Z\;\;X\!\!\!_{G}Y Z\).

Figure 5: An example rehearsal graph and its Bayesian network counterparts that encode the same conditional independence relations. We use plate notation to simplify the edges. For example, in (a), vertex A having an edge pointing to the rectangle containing three vertices, implies that A\(\)E, A\(\)F, and A\(\)G.

[MISSING_PAGE_FAIL:18]

## Appendix B Candidate Alteration Selection

### Size-\(1\) Alteration

Algorithm 1 in the main text shows the main steps of finding candidate alterations, where details on finding alteration values that satisfy the performance requirement are omitted. Here we give more details in Algorithm 3. The main idea is to sort the found intervals in increasing order, then keep those that appear more than \(n\) times. Sorting the array in Line 17 on average costs \(O(n n)\) time if using quicksort. Other operations cost \(O(n)\) time. Thus, the overall average running time of Algorithm 3 is \(O(|_{t}| n n)\).

### Size-\(k\) Alteration

We propose to build a finite candidate set by solving a mixed-integer linear programming problem:

\[_{\{0,1\}^{n^{}},^{}}\ \ _{i}_{i}\] \[\ \ (_{i} _{i}^{}+_{i}_{i})- (1-_{i}),\ i[n^{}],\]

where \(_{i}\) is a binary decision variable that equals \(1\) if the \(i\)-th inequality is satisfied and \(0\) otherwise, and \(\) is a constant vector that upper bounds the left-hand side. If the found solution satisfies \(_{i}_{i} n\), corresponding \(^{}\) will be joined in the candidate set. The above programming is run for every combination of at most \(k\) alteration targets multiple times to obtain multiple alterations. The hard constraints satisfied by previous programs for a common \(^{}\) are randomly dropped in following iterations to encourage diverse solutions, thus the \(n^{} n\) constraints instead of \(n\).

## Appendix C Mutual Information Maximization

Here we detail the estimation of the mutual information objective in Eq. (5). We have

\[(,;_{t},_{t}\,|\,D_{t },_{t},_{t})=(_{t},_{t}\,|\,D _{t},_{t},_{t})-(_{t},_{t}\,|\, ,,D_{t},_{t},_{t}).\]

In order to estimate the mutual information, we only need to estimate \((_{t},_{t}\,|\,D_{t},_{t},_{t})\) and \((_{t},_{t}\,|\,,,D_{t}, _{t},_{t})\). We give a detailed estimation procedure in Algorithm 4. The entropy estimation method EntropyEstimate that estimates information entropy with samples is used as a black-box method since various methods are available, _e.g._, Sricharan et al. .

Let \(g(,;_{t},_{ t}\,|\,D_{t},_{t},_{t})\) denote the target optimization function to be used in Bayesian optimization . A Gaussian process is used to attain the posterior of \(g\). Denote the posterior ```
1:\(S_{tr}=\{ G_{i},_{i},_{i}\}_{i=1}^{n}\), \(S_{val}=\{ G_{i},_{i},_{j}\}_{i=n+1}^{2n},\)
2:\(\)\(\) Store candidate alterations
3:for\(Z_{i}\)do
4:\((S_{tr},Z_{i},)\)\(\) Training
5:\((S_{val},Z_{i},)\)\(\{ Z_{i},\}\)\(\) Validation
6:functionFind(\(S\), \(Z\), \(\))\(\) Find alteration values
7:\(\)\(\) Store valid intervals
8:for\( G,, S\)do
9:\([u,v]\) solution of \((+^{}+)\)\(\)\(\), \(\), \(\) are determined by \(G\), \(\), \(Z\)
10:\(\{[u,v](Z)\}\)
11:returnFilterIntervals(int, \( n\))
12:functionFilterIntervals(\(\), \(k\))\(\) Find elements that appear more than \(k\) times in \(\)
13:\(\)
14:for\([u,v]\)do
15:\(\{(u,0)\}\)
16:\(\{(v,1)\}\)\(\) 0 or 1 indicates the value is taken from the left or the right endpoints of an interval
17:\(()\)\(\) Sorted in increasing order with the first element as the key
18:\( 0\)
19:for\(0<||\)do
20:if\([]=0\)then\(\) If the value is a left endpoint
21: cnt \(\) cnt + 1
22:else
23: cnt \(\) cnt \(-1\)
24:\([]\) cnt\(\) Record the number of intervals that intersect at \([]\)
25:\(\)
26:\(,\)
27:for\(0<||\)do\(\) Scan the array and record valid intervals
28:\([]\)
29:if\([] k\)then
30:ifleft is NULL then
31:\(\) arr[]\)
32:else
33:ifleft is not NULL then
34:\(\{[,]\}\)
35:\(\)
36:returnrtn
37:The found alterations \(\) ```

**Algorithm 3** Find candidate single alterationsmean and variance at an unseen \(_{l+1}\) by \(_{g}(_{l+1})\) and \(_{g}^{2}(_{l+1})\) respectively. Given queried points \(\{^{1},,^{l}\}\), we use upper confidence bound  as a querying criterion to select the next point:

\[^{l+1}=*{arg\,max}_{^{l+1}( ^{})}_{g}(^{l+1})+_{g}(^{l+1}),\]

where \(\) is a constant and \((^{})\) is the feasible region of \(^{}\) defined by \(\). We run this optimization process for every alteration target in \(\) to find the corresponding best alteration values. Finally, the alteration, including both target variables and values, that has maximum mutual information is selected as the recommended output.

```
0: The posterior estimate \(p(G, D_{t})\), the candidate graph set \(\), the observed \(\), the alteration \(\), the number of samples for estimating an entropy term \(n\), the number of samples for structural parameters \(n^{}\).
1:\(srms\)
2:for\(G\)do
3:for\(i[ n p(G D_{t})]\)do
4: Sample \(\) from \(p( G,D_{t})\)
5:\(srms srms\{ G,_{i}\}\)
6:\(samples\)
7:for\( G, srms\)do
8: Sample \(\) and \(\) from \(p(, G,,,)\)
9:\(samples samples\{,\}\)
10:\(_{1}(samples)\)
11:\(_{2} 0\)
12:for\(G\)do
13:\(entropies\)
14:for\(i[n^{}]\)do
15: Sample \(\) from \(p( G,D_{t})\)
16:\(samples\)
17:for\(i[n]\)do
18: Sample \(\) and \(\) from \(p(, G,,,)\)
19:\(samples samples\{,\}\)
20:\(entropies entropies\{(samples)\}\)
21:\(cond\_entropy\) average of \(entropies\)
22:\(_{2}_{2}+cond\_entropy p(G D_{t})\)
23:\(_{1}-_{2}\)
24: The mutual information estimate \(\) ```

**Algorithm 4** Mutual information estimation

## Appendix D Proofs

In this section, we provide proof for claims in the main text.

Proposition 4: _For any \( G_{1},_{1}\) and \( G_{2},_{2}\) on \(_{t}_{t}_{t}\), if they differ only in describing the generation of \(_{t}\), then for any \(_{t}=_{t}\) and alterations \(\) on \(_{t}\), we have_

\[(_{t}_{t},Rh()\,;G_{1},_{1})=(_{t}_{t},Rh() \,;G_{2},_{2}).\]

Proof: We have

\[(,Rh();G_{i},_{i})\\ =_{}(,Rh();G_ {i},_{i})(,,Rh ();G_{i},_{i}), i=1,2.\]

Since \((G_{1},_{1})\) and \((G_{2},_{2})\) only differ in parameters controlling \(()\), we have

\[(,Rh();G_{1},_{1})= (,Rh();G_{2},_{2}),\]\[(,,Rh();G_{1},_{1})= (,,Rh();G_{2},_{2}).\]

Combining the above equations gives

\[(,Rh()\,;G_{1},_{1} )=(,Rh()\,;G_{2},_ {2}).\]

**Proposition 5**.: _Let \(=\{(Z_{a_{i}},z_{a_{i}})\}_{i=1}^{k}\) be a valid alteration where \(Z_{a_{i}}_{t}\) and \(z_{a_{i}}(Z_{a_{i}})\). Given \( G,,\) and \(_{t}\), the outcome variables are linear in the alteration values. We have_

\[_{t}=_{t}+^{}+ ,\] (8)

_where \(^{}=(z_{a_{1}},,z_{a_{k}})^{T}\). \(\), \(\), and \(\) are constant matrices of appropriate shapes and are uniquely determined by \(G\), \(\) and altered variables \(^{}=\{Z_{a_{i}}\}_{i=1}^{k}\)._

Proof.: The proposition is immediate by noting that if the parent of a set of variables can be described by

\[=^{}+^{}^{}+^{},\]

then

\[=^{T}+_{}= ^{T}^{}+^{T}^{} ^{}+^{T}^{}+_{}\]

can be described in a similar linear form. 

**Theorem 6**.: _Unless \(=\), finding a \(k\)-dimensional \(^{}(^{})\) that satisfies \(_{i=1}^{n}((_{i}+_{i} ^{}+_{i}_{i})) n\) (if there is a valid solution) is not solvable with any algorithm of running time polynomial in \(k\) and \(n\)._

Proof.: We prove this by contradiction. Assume that the problem is solvable with a subroutine \(f\) that runs in polynomial time. Then, given any infeasible linear system \(:\{\}\), we can find a feasible subsystem containing as many inequalities as possible by finding a maximized \(\) that gives a valid solution for the problem of finding a \(\) that satisfies at least \(n\) inequalities. And the process of finding a maximized \(\) can be finished by running a binary search on \(\) and calling \(f\), which will be called at most \(O( n)\) times. Thus the overall running time will still be polynomial. However, finding a maximum feasible subsystem is known to be NP-hard , which cannot be solved within polynomial time if \(\). Therefore we have a contradiction and conclude that the problem does not admit a polynomial time algorithm. 

**Theorem 7**.: _Given an observed evidence \(_{t}\), an alteration \(\), and a desired set \(=\{^{||} \}\), let \(S_{eval}=\{ G_{i},_{i},_{i}\}_{i=1}^{n}\) be \(n\) i.i.d. samples from \((G,, D_{t})\). Let \(\{^{i}\}_{i=1}^{n}\) be generated from \(Rh()\) and \(S_{eval}\) following the structural equations in Eq. (1). Define \(n_{o}=|\{i^{i}\}|\). Let \( 1-n_{o}/n\), \( 1-F^{-1}(1-/(2n);n_{o}+1,n-n_{o})\) if \(n_{o}<n\) otherwise \( 0\), and \( 1-F^{-1}(/(2n);n_{o},n-n_{o}+1)\) if \(n_{o}>0\) otherwise \( 1\), where \(F^{-1}(\,\,;,)\) is the inverse cumulative distribution function of the beta distribution with parameters \(\) and \(\). For any \((0,1)\), with probability at least \(1-\), we have_

\[\{,-\} (_{t}|D_{t},_{t},Rh()) \{,+\}.\]

Proof.: The proof is mainly based on the scenario approach  and adapted from Badings et al. . The desired region for \(\) is given by \(=\{^{||} \}\). We define a scaled version of \(\) by

\[R()=\{^{||}+(1-)\},\] (19)

where \(h\) is a Chebyshev center of \(\). We have \(R(1)=\) and \(R(_{1}) R(_{2})\) for any \(0_{1}<_{2}\). Using Prop. 5, we express each \(^{i}\) with

\[^{i}=_{i}+_{i}^{}+_{i}_{i}.\] (20)We use the scenario approach to estimate the probability of the happening of undesired \(\) with the following convex scenario optimization problem with discarded samples:

\[_{ 0} \] (21) s.t. \[^{i} R()\;\; i\{1,,n\} Q,\]

which has a scalar decision variable \(\). We denote the optimal solution of the above optimization program with \(_{|Q|}^{*}\). \(Q\) is a subset of samples whose constraints have been discarded according to the following rule [24, Lem. 1]: The sample removal set \(Q\{1,,n\}\) is obtained by iteratively removing the active constraints from (21). Thus, given \(N\) samples and any two removal sets with cardinalities \(|Q_{1}|<|Q_{2}|\), it holds that \(Q_{1} Q_{2}\). Moreover, any discarded sample \(i Q\) violates the solution \(_{Q}^{*}\) to Eq. (21), _i.e._\(^{i} R_{j}(_{|Q|}^{*})\), with probability one. When discarding \(|Q|=n_{o}\) samples, it holds that \(R(_{n_{o}}^{*})\). When discarding \(|Q|=n_{o}-1\) samples (\(n_{o}>0\)), \(R(_{n_{o}-1}^{*})\).

We further assume that given a sample \( G_{i},_{i},_{i}\) from \((G,, D_{t})\), the probability that the generated \(^{i}\) is on the boundary of any polytope \(R()\) for any \( 0\) is zero.

Based on the results of Romao et al. [62, Thm. 5] and Badings et al. [24, Thm. 1], we have

\[\{P( R(_{|Q|}^{*})) \}=F(;|Q|+1,n-|Q|),\] (22)

where \(F(\,\,;,)\) is the cumulative distribution function of the beta distribution with parameters \(\) and \(\), \(|Q|<n\). Equivalently, we have

\[\{P( R(_{|Q|}^{*}))> \}=1-F(;|Q|+1,n-|Q|).\] (23)

Solving \(1-F(;|Q|+1,n-|Q|)=\), we have

\[=F^{-1}(1-;|Q|+1,n-|Q|).\] (24)

For notational convenience, let \(F_{i}()=F(;i+1,n-i)\). We have

\[\{P( R(_{|Q|}^{*}))>F_{|Q|}^ {-1}(1-)\}=.\] (25)

Using Boole's inequality, we know that

\[\{_{i=0}^{n-1}P( R( _{i}^{*}))>F_{i}^{-1}(1-)\}_{i=0} ^{n-1}\{P( R(_{i}^{*}))>F_{i} ^{-1}(1-)\}=.\] (26)

Therefore, we have

\[\{_{i=0}^{n-1}P( R( _{i}^{*})) F_{i}^{-1}(1-)\} 1- .\] (27)

After observing \(^{i}\)s at hand, we replace \(|Q|\) by \(n_{o}\). Since \(R(_{n_{o}}^{*})\) and \(_{i=0}^{n-1}P( R(_{i}^{*})) F_{i} ^{-1}(1-)\) implies \(P( R(_{n_{o}}^{*})) F_{n_{o}}^{-1}(1- )\), we have

\[\{P() F_{n_{o}}^{-1} (1-)\}\{P( R( _{n_{o}}^{*})) F_{n_{o}}^{-1}(1-)\}  1-,\] (28)

which further gives

\[\{P() 1-F_{n_{o}}^{-1}(1- )\} 1-.\] (29)

When \(n_{o}=n\), \(P() 0\) trivially holds. Thus, \((_{t} D_{t}, _{t},Rh())\) holds with probability at least \(1-\). For the other half of the lower bound, we apply Hoeffding's inequality on binary variables \(D_{i}(^{i})\). Note that \(_{i=1}^{n}D_{i}=n_{o}\) and \((D_{i})=(_{t} D_{t}, _{t},Rh())\), we have

\[\{n_{o}-n(_{t}  D_{t},_{t},Rh()) }\},\] (30)that is,

\[\{(_{t} D_{t},_{t},Rh())-}\} 1- .\] (31)

Combining (29) and (31), we have with probability at least \(1-\),

\[\{,\,-} \}(_{t} D_{t},_{t},Rh()).\] (32)

For the upper bound, we adopt a similar approach. From Eq. (22), by noting that \(P( R(_{ Q}^{*}))+P(  R(_{ Q}^{*}))=1\), we get

\[\{P( R(_{ Q}^{*} )) 1-\}=F_{ Q}().\] (33)

Substituting \(F_{ Q}()\) with \(\), we have

\[\{P( R(_{ Q}^{*} )) 1-F_{ Q}^{-1}()\}=.\] (34)

Again via Boole's inequality,

\[\{_{i=0}^{n-1}P( R( _{i}^{*})) 1-F_{i}^{-1}()\} 1-.\] (35)

When \(n_{o}>0\), since \(R(_{n_{o}-1}^{*})\) and \(_{i=0}^{n-1}P( R(_{i}^{*})) 1-F_{i}^{-1}( )\) implies \(P( R(_{n_{o}-1}^{*})) 1-F_{n_{o}-1}^{-1}()\), we have

\[\{P() 1-F_{n_{o}-1}^{ -1}()\}\{P( R( _{n_{o}-1}^{*})) 1-F_{n_{o}-1}^{-1}()\}  1-.\] (36)

When \(n_{o}=0\), \(P() 1\) trivially holds. Thus,

\[(_{t} D_{t},_{t},Rh())\] (37)

holds with probability at least \(1-\). Applying Hoeffding's inequality from the other direction of Eq. (30), we get

\[\{(_{t} D _{t},_{t},Rh())+} \} 1-.\] (38)

Combining the above results gives the upper bound that holds with probability at least \(1-\):

\[(_{t} D_{t},_{t},Rh())\{,+}\}.\] (39)

Combining the lower bound (32) and the upper bound (39) gives the overall bound. 

## Appendix E Data Details

### Ride-Hailing Data

We give details about the Ride-Hailing data in this section. The variables included in the generation process are

* weather: weather condition;
* #user: number of users in the neighborhood;* recommend: recommendation level of the ride-hailing app for a specific route;
* congestion: traffic congestion on the route;
* time: time spent for the ride;
* discount: discount provided by the app;
* rating: user rating for the ride.

The rehearsal graph for the variables is given in Fig. 6. The presumed actionable variables that can be altered by the app are'recommend' and 'discount'.

### Bermuda Data

We give details about the Bermuda data in this section. The Bermuda data involves a set of environmental variables . The variables included in the generation process are

* Chla: sea surface chlorophyll a;
* Sal: sea surface salinity;
* TA: seawater total alkalinity;
* DIC: seawater dissolved inorganic carbon;
* \(CO_{2}\): seawater \(_{_{2}}\);
* Temp: bottom temperature;
* NEC: net ecosystem calcification;
* Light: bottom light levels;
* Nut: PC1 of \(NH_{4}\), \(NiO_{2}+NiO_{3}\), \(SiO_{4}\);

Figure 6: The rehearsal graph for ride-hailing data.

* pHsw: seawater pH
* \(_{A}\): seawater saturation with respect to aragonite.

The rehearsal graph for the variables is given in Fig. 7. The presumed actionable variables that can be altered are: DIC, TA, \(_{A}\), Chla, and Nut.

Figure 7: The rehearsal graph for Bermuda data.