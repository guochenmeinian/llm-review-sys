# A Simple and Optimal Approach for

Universal Online Learning with Gradient Variations

 Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{yanyh, zhaop, zhouzh}@lamda.nju.edu.cn

Correspondence: Peng Zhao <zhaop@lamda.nju.edu.cn

###### Abstract

We investigate the problem of universal online learning with gradient-variation regret. Universal online learning aims to achieve regret guarantees without the prior knowledge of the curvature of the online functions. Moreover, we study the problem-dependent gradient-variation regret as it plays a crucial role in bridging stochastic and adversarial optimization as well as game theory. In this work, we design a universal approach with the _optimal_ gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by Yan et al. (2023). Our approach is _simple_ since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only \(1\) gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret. Concretely, previous works on gradient variations require controlling the algorithmic stability, which is challenging and leads to sub-optimal regret and less efficient algorithm design. Our analysis overcomes this issue by using a Bregman divergence negative term from linearization and a useful smoothness property.

## 1 Introduction

Online convex optimization (OCO) models a sequential \(T\)-round game between an online learner and the environments (Hazan, 2016; Orabona, 2019). In each round \(t[T]\), the learner selects a decision \(_{t}\) from a convex compact set \(^{d}\). Simultaneously, the environments adversarially choose a convex loss function \(f_{t}:\). Subsequently, the learner suffers a loss of \(f_{t}(_{t})\), receives feedback on the function \(f_{t}()\), and updates her decision to \(_{t+1}\). In OCO, the learner aims to optimize the game-theoretical performance measure known as regret (Cesa-Bianchi and Lugosi, 2006), which is formally defined as

\[_{T}_{t=1}^{T}f_{t}(_{t})-_{}_{t=1}^{T}f_{t}().\] (1.1)

It represents the learner's excess cumulative loss compared with the best fixed comparator in hindsight.

In OCO, _function curvatures_ play an important role in the best attainable regret bounds. Traditional studies examine three types of curvatures: convexity, exp-concavity, and strong convexity. Specifically, for convex functions, online gradient descent (OGD) achieves \(()\) regret (Zinkevich, 2003); for \(\)-exp-concave functions, online Newton step assuming \(\) is known obtains \(( T)\) regret (Hazan et al., 2007); and for \(\)-strongly convex functions, OGD with known \(\) attains \(( T)\)(Hazan et al., 2007). These results are shown to be minimax optimal (Ordentich and Cover, 1998; Abernethy et al., 2008). Recent studies further strengthen them by introducing _two levels of adaptivity_.

High-level Adaptivity to Unknown Curvatures.Traditional studies require function curvature information in advance to select suitable algorithms for provable bounds. However, the information could be hard to access in real-world applications. To this end, a line of research aims to design a single _universal_ algorithm that does not require the curvature information while achieving the same regret guarantees as if knowing it, achieving the adaptivity to unknown curvatures. The pioneering work of MetaGrad (van Erven and Koolen, 2016) proposed carefully designed surrogate functions and achieved \(( T)\) for \(\)-exp-concave functions and \(()\) for convex functions. Subsequently, Wang et al. (2019) obtained the optimal \(( T)\) regret for \(\)-strongly convex functions, while maintaining the optimal rates in the other cases. Another remarkable progress of Zhang et al. (2022) proposed a flexible framework with simplified analyses and further enhanced the minimax results using smoothness. We provide a detailed introduction of this work in Section 2.2.

Low-level Adaptivity to Gradient Variation.Although the regret guarantees based on the time horizon \(T\) are optimal in the minimax sense, in this work we are interested in achieving the _gradient-variation_ regret (Chiang et al., 2012; Yang et al., 2014), which replaces the dependence of the time horizon \(T\) by the gradient variation quantity defined in the following:

\[V_{T}_{t=2}^{T}_{}\| f_{t}( )- f_{t-1}()\|^{2}.\] (1.2)

Under smoothness assumptions, the minimax regret can be improved to \(( V_{T})\), \(( V_{T})\), and \((})\) for \(\)-strongly convex, \(\)-exp-concave, and convex functions, respectively. In this work, continuing previous gradient-variation online learning results (Zhao et al., 2020, 2024; Zhang et al., 2022; Chen et al., 2023), we focus on the gradient-variation regret for the following reasons: _(i)_ gradient-variation bounds safeguard the minimax guarantees. Besides, as demonstrated by Zhao et al. (2024), the gradient-variation regret is more fundamental than another well-known problem-dependent quantity known as the small loss \(F_{T}_{}_{t T}f_{t}()\)(Srebro et al., 2010; Orabona et al., 2012) since gradient-variation regret can imply small-loss bounds directly in analysis; _(ii)_ the gradient variation plays a crucial role in bridging adversarial and stochastic optimization (Sachs et al., 2022); and _(iii)_ the gradient-variation regret can be used to achieve fast rates in multi-player games (Syrgkanis et al., 2015; Zhang et al., 2022). More detailed explanations of the importance of achieving such adaptivity are provided at the end of this section.

Motivated by the aforementioned two levels of adaptivity, we focus on the problem of achieving _universal gradient-variation_ regret, i.e., designing a single universal approach with gradient-variation regret across different curvature types without the prior knowledge of them. For this problem, Zhang et al. (2022) achieved partial results of \(( V_{T})\), \(( V_{T})\), \(()\) for \(\)-strongly convex, \(\)-exp-concave, and convex functions, respectively. Subsequently, Yan et al. (2023) proposed a carefully designed three-layer online ensemble approach to stabilize the algorithm and improved the convex result to \(( V_{T}})\), achieving the first universal gradient-variation guarantee. Although optimal for strongly convex and exp-concave functions, their results still exhibit a gap with the optimal \((})\) regret in the convex case. Here "optimal" refers to matching the best known results with curvature information since problem-dependent lower bound cannot be easily obtained. The only lower bound we are aware of is \((})\) for convex functions (Yang et al., 2014, Remark 5).

    &  &  \\   & Strongly Convex & Exp-concave & Convex & \# Gradient & \# Base \\  van Erven and Koolen (2016) & \(d T\) & \(d T\) & \(\) & \(1\) & \( T\) \\  Wang et al. (2019) & \( T\) & \(d T\) & \(\) & \(1\) & \( T\) \\  Zhang et al. (2022) & \( V_{T}\) & \(d V_{T}\) & \(\) & \( T\) & \( T\) \\  Yan et al. (2023) & \( V_{T}\) & \(d V_{T}\) & \( V_{T}}\) & \(1\) & \(( T)^{2}\) \\  
**Ours** & \( V_{T}\) & \(d V_{T}\) & \(}\) & \(1\) & \( T\) \\   

Table 1: Comparison with existing results. The second column shows the regret bounds for strongly convex, exp-concave, and convex functions, following the \(()\)-notation. Note that we only list universal guarantees related to the gradient variation \(V_{T}\) or the time horizon \(T\). Each gradient-variation bound can directly apply a corresponding small-loss regret in analysis, which is formally stated in Theorem 2 and omitted here for clarity. We treat the \( T\) factor as a constant and omit it. “# Gradient” is the number of gradient queries in each round, where “1” represents exactly one gradient query. And “# Base” stands for the number of base learners.

To handle the uncertainty, _online ensemble_ is commonly employed and proven effective in enhancing the robustness (Zhou, 2012; Zhao, 2021), such as adaptive regret minimization (Hazan and Seshadhri, 2007; Daniely et al., 2015; Zhang et al., 2019), dynamic regret minimization (Zhang et al., 2018; Zhao et al., 2020, 2024), and universal online learning (van Erven and Koolen, 2016; Zhang et al., 2022; Yan et al., 2023). Concretely, an online ensemble algorithm contains multiple _base learners_ for exploring the environments and a _meta learner_ for ensemble. In universal online learning, base learners make guesses on the curvature information, and the meta learner tracks the best base learner (i.e., with the most accurate guess) on the fly.

Due to the deployment of the online ensemble framework, _computational efficiency_ has become a point of concern, with two essential factors. The first is the _number of base learners_, because each independently runs an online learning algorithm that involves time-consuming gradient computations and projections. The second factor is the _number of gradient queries_, especially when the gradient evaluation is costly, e.g., in nuclear norm optimization (Ji and Ye, 2009) and mini-batch optimization (Li et al., 2014). In universal online learning, an "efficient" algorithm is expected to adopt only \(( T)\) base learners, which is inherent to the online ensemble design, and only 1 gradient query per round, matching the gradient query complexity of standard OGD. In terms of this metric, Zhang et al. (2022) employed \(( T)\) base learners, but required \(( T)\) gradient queries per round. Yan et al. (2023) used only \(1\) gradient query per round but required \((( T)^{2})\) base learners (caused by their three-layer algorithm design), resulting in reduced efficiency.

Results.Motivated by the above considerations of optimality and efficiency, in this work, we propose a simple universal approach that achieves the _optimal_\(( V_{T})\), \(( V_{T})\), and \((})\) regret simultaneously for \(\)-strongly convex, \(\)-exp-concave, and convex functions, and is _efficient_ with one gradient query per round and \(( T)\) base learners, resolving a major open problem highlighted by Yan et al. (2023). We summarize our theoretical results in Theorem 1, and compare our results with existing ones in Table 1. Furthermore, we validate the effectiveness of our approach by: _(i)_ showing that our universal gradient-variation regret directly implies the optimal universal small-loss regret in analysis without any algorithm modifications; and _(ii)_ applying them to the stochastically extended adversarial (SEA) model (Sachs et al., 2022), an intermediate framework between stochastic and adversarial optimization. We achieve the same state-of-the-art guarantees as Chen et al. (2024), but without curvature information. The details are provided in Section 4.

Techniques.Our technical contributions include two simple and novel analyses. First, the key to gradient-variation regret is to analyze its empirical version, formally, \(\| f_{t}(_{t})- f_{t-1}(_{t-1})\|_{2}^{2}\). The previous approach to addressing this term involves controlling the algorithmic stability \(\|_{t}-_{t-1}\|_{2}^{2}\), which is highly challenging in universal online learning, leading to sub-optimal results and less efficient algorithm design (Yan et al., 2023). In this work, we overcome this issue via a novel analysis by a useful smoothness property and a Bregman divergence negative term from linearization, where the latter is inspired by the recent advance in stochastic optimization (Joulani et al., 2020). Second, we adopt the surrogate functions proposed by Yan et al. (2023) to reduce the gradient query complexity, and provide a novel analysis for the empirical gradient variation based on the surrogates. A technical comparison with previous works is provided in Section 4.

Organization.The rest of the paper is structured as follows. Section 2 introduces preliminaries and a general framework for universal online learning. Section 3 presents our efficient approach with the optimal universal gradient-variation regret. Section 4 presents the implication and application of our results. Finally Section 5 concludes the paper. All the proofs can be found in the appendices.

## 2 Preliminaries

In this section, we introduce some preliminary knowledge, including notations, assumptions, definitions, and a general framework of universal online learning.

### Notations, Assumptions, and Definitions

For simplicity, we use \(\|\|\) for \(\|\|_{2}\) by default and use \(a b\) or \(a=(b)\) if there exists a constant \(C<\) such that \(a/b C\). In the following, we introduce the assumptions used in this work.

**Assumption 1** (Boundedness).: For any \(,^{d}\), the domain diameter satisfies \(\|-\| D\), and for \(t[T]\), the gradient norm of the online functions is bounded as \(\| f_{t}()\| G\).

**Assumption 2** (Smoothness).: For each \(t[T]\), the online function \(f_{t}()\) is \(L\)-smooth, i.e., \(\| f_{t}()- f_{t}()\| L\|-\|\) holds for any \(,^{d}\).

Both assumptions are common in the literature. Specifically, the boundedness assumption is widely used in OCO (Hazan, 2016). And the smoothness assumption is essential for first-order algorithms in achieving the gradient-variation regret (Chiang et al., 2012). Note that here Assumption 2 requires smoothness on the whole \(^{d}\) space and can be relaxed to a slightly larger domain than \(\), formally, \(_{+}\{+\,|\,,}{{L}}\}\), where \(\{\,|\,\|\| 1\}\) is a unit ball. We defer the details of this relaxed smoothness requirement and its derivation to Appendix A. In the following, we provide formal definitions of strong convexity and exp-concavity.

**Definition 1**.: _For any \(,\), a function \(f()\) is \(\)-strongly convex if \(f()-f() f(),- -\|-\|^{2}\); \(f()\) is \(\)-exp-concave if \(f()-f() f(),- - f(), -^{2}\)._

Note that the formal definition of \(\)-exp-concavity is that \((- f())\) is concave. Under Assumption 1, \(\)-exp-concavity implies Definition 1 with \(=\{1/(4GD),\}\)(Hazan, 2016, Lemma 4.3). Therefore, we adopt Definition 1 as an alternative definition of exp-concavity for clarity.

### A General Framework for Universal Online Learning

In this part, we present a general framework of universal online learning (Zhang et al., 2022; Yan et al., 2023). Formally, we study the problem where the learner lacks the prior knowledge of curvature information, including _(i)_ curvature type: convexity, exp-concavity, or strong convexity; and _(ii)_ curvature coefficient: exp-concavity \(\) or strong convexity \(\). Without loss of generality, we focus on the case of \(,[1/T,1]\). If \(,<1/T\), even the optimal minimax results -- \(( T)\) for exp-concave functions and \(( T)\) for strongly convex functions (Hazan et al., 2007) - become linear in \(T\), rendering the regret bounds vacuous. On the other hand, if \(,>1\), we can simply treat them as \(,=1\), only making the regret worsen by an ignorable constant factor. This simplification is also adopted by Zhang et al. (2022); Yan et al. (2023).

To handle the uncertainty of curvatures, an online ensemble structure is usually employed, with multiple base learners exploring the environments and a meta learner tracking the best base learner. More specifically, to deal with the unknown curvature coefficients \(\) and \(\), we discretize them into the following candidate pool (Zhang et al., 2022):

\[\{1/T,2/T,4/T,,2^{n-1}/T\},\] (2.1)

whose size is \(n=_{2}T+1=( T)\). It can be proved that the discretized candidate pool \(\) can approximate the continuous value of \(\) or \(\) with negligible constant errors. By doing this, it is natural to design three distinct groups of base learners:

1. _strongly convex_ base learners: \(n\) in total, each of which implements the algorithm for strongly convex functions with a guess \(_{i}\) of the strong convexity coefficient \(\);
2. _exp-concave_ base learners: \(n\) in total, each of which implements the algorithm for exp-concave functions with a guess \(_{i}\) of the exp-concavity coefficient \(\);
3. _convex_ base learner: only one, it runs the algorithm for convex functions.

In total, there are \(N(2n+1)=( T)\) base learners with a two-layer structure, which is for now necessary in this problem. The best base learner is the one with the right guess of the curvature type and the closest guess of the curvature coefficient. Taking \(\)-strongly convex functions as an example, the guessed coefficient of the best base learner (indexed by \(i^{}\)) satisfies \(_{i^{}} 2_{i^{}}\).

Denoting by \(_{t,i}\) the decision generated by the \(i\)-th base learner at the \(t\)-th round, \(p_{t,i}\) the weight of the meta learner on the \(i\)-th base learner, an online ensemble method outputs the final decision as \(_{t}=_{i[N]}p_{t,i}_{t,i}\). This forms a general framework for universal online learning and it remains to select suitable algorithms and loss functions for the meta and base learners. We will illuminate our concrete algorithm design in Section 3.1 and present a more detailed description of our meta/base learners configurations in Appendix B.

## 3 Our Approach

In this section, we present our approach for universal online learning with gradient-variation regret. Section 3.1 presents the overall procedure of our proposed algorithm. Subsequently, we outline our two key technical components: Section 3.2 presents a novel analysis to handle the empirical gradient variation, and Section 3.3 introduces surrogate functions to improve efficiency and provides a corresponding analysis for the empirical gradient variation defined on surrogates. We finally provide the optimal universal gradient-variation regret guarantees in Section 3.4.

### Overall Algorithm

In this part, we present our simple approach for universal online learning with gradient variations, summarized in Algorithm 1. Basically, it is a two-layer online ensemble. Base learners are implemented using the preliminary configurations given in Section 2.2 and on carefully designed surrogate functions. The meta learner runs Optimistic-Adapt-ML-Prod(Wei et al., 2016) on linearized losses. We specify the algorithmic details below, and a more detailed procedure in Appendix B.

In Line 3, the learner makes a weighted combination of the base learners' decisions \(\{_{t,i}\}_{i[N]}\) using the meta learner's weights \(_{t}=(p_{t,1},,p_{t,N})\), submits the final decision \(_{t}\), suffers a loss \(f_{t}(_{t})\), and receives a single \( f_{t}(_{t})\) as the gradient feedback, using _only_\(1\) gradient query per round.

```
0: Meta learner \(\), base learners \(\{_{i}\}_{i[N]}\)
1:Initialize:\(p_{1,i}=}{{N}}\) and \(_{1,i}\) to be an arbitrary decision inside \(\) for all \(i[N]\)
2:for\(t=1\)to\(T\)do
3: Submit \(_{t}=_{i[N]}p_{t,i}_{t,i}\), suffer \(f_{t}(_{t})\), and observe the gradient \( f_{t}(_{t})\)
# Meta Update:
4:\(\) updates to \(_{t+1}_{N}\) using the rule (B.5) with learning rate (B.6)
# Base Update:
5: Construct surrogates \(h^{}_{t,i}()\)\(h^{}_{t,i}()\), \(h^{}_{t,i}()\) defined in (3.1) using only \( f_{t}(_{t})\)
6:\(_{i}\) updates to \(_{t+1,i}\) using surrogates functions (3.1) and update rules (B.1)-(B.3) for \(i[N]\)
7:endfor ```

**Algorithm 1** A Simple Approach for Universal Online Learning with Gradient Variations

Meta Algorithm.In Line 4, the meta learner uses Optimistic-Adapt-ML-Prod(Wei et al., 2016) to update the weights by the following rule:

\[p_{t+1,i}_{t,i}(_{t,i}m_{t+1,i}) W_ {t,i},W_{t,i}=W_{t-1,i}_{t-1,i}r_{t,i}- _{t-1,i}^{2}(r_{t,i}-m_{t,i})^{}{_{t-1,i}}}\.\]

Specifically, denoting by \(_{t,i} f_{t}(_{t}),_{t,i}\) the loss of the \(i\)-th dimension, the meta algorithm inputs: \(r_{t,i}=_{t},_{t}-_{t,i}\), the instantaneous regret; \(_{t,i}\), a time-varying learning rate; and \(m_{t,i}\), an estimation of the true loss of the \(t\)-th round (the choice of optimisms will be shown later). The meta algorithm then outputs the weights \(_{t+1}=(p_{t+1,1},,p_{t+1,N})\) of the next round.

With appropriate learning rates (B.6), Optimistic-Adapt-ML-Prod achieves an optimistic second-order bound of \(_{t T}r_{t,i}((r_{t,i}-m_{t,i})^{2} }+ N)\), where the \( N\) factor is negligible since the base learner number \(N\) equals \(( T)\) and we can treat \(( T)\) as a constant (Luo and Schapire, 2015). The formal guarantee of Optimistic-Adapt-ML-Prod is deferred to Lemma 2 in the appendix. In our problem, the instantaneous regret \(r_{t,i}=_{t},_{t}-_{t,i}= f_{t}( _{t}),_{t}-_{t,i}\). Thus we choose \(m_{t,i}= f_{t-1}(_{t-1}),_{t}-_{t,i}\) for the convex base learner and \(m_{t,i}=0\) otherwise (i.e., for exp-concave and strongly convex base learners).2 By doing this, we can upper-bound \(_{t} f_{t}(_{t}),_{t}-_{t,i}\) by \(( f_{t}(_{t})- f_{t-1}( _{t-1}),_{t}-_{t,i}^{2}})\) for the convex base learner and by \(( f_{t}(_{t}),_{t}- _{t,i}^{2}})\) otherwise. Later in Section 3.3, we illuminate how such results could benefit the final regret guarantees.

Base Algorithm.In Line 5, we adopt carefully designed surrogate functions for different types of base learners to reduce the gradient query complexity (Yan et al., 2023). Specifically, strongly convex, exp-concave, and the convex base learners run on the surrogate functions below respectively:

\[h^{}_{t,i}() f_{t}(_{t}), +}{4}\|-_{t}\|^{2}, h ^{}_{t,i}() f_{t}(_{t}), +}{4} f_{t}(_{t}), -_{t}^{2},\] (3.1)

and \(h^{}_{t,i}() f_{t}(_{t}), \), where \(_{i},_{i}\) are selected from the candidate coefficient pool \(\) in (2.1). We emphasize that the surrogate functions require only \(1\) gradient query \( f_{t}(_{t})\) per round. Finally, in Line 6, the \(i\)-th base learner \(_{i}\) updates the decision to \(_{t+1,i}\) using optimistic online mirror descent (OOMD) (Rakhlin and Sridharan, 2013), which is general and covers many algorithms of interest, such as OGD and online Newton step. For each curvature type (convexity, exp-concavity, or strong convexity), we adopt a correspondingly configured OOMD as the base learner. For detailed update rules of differently configured OOMD, we refer readers to (B.1)-(B.3) in Appendix B.

As for previous works, Zhang et al. (2022) adopted Adapt-ML-Prod (Gaillard et al., 2014) as the meta learner, which does not incorporate optimisms and thus is impossible to achieve gradient-variation regret for convex functions, and operates on the original loss function \(f_{t}()\) for base learners, which leads to a less efficient gradient query complexity of \(( T)\) per round. Yan et al. (2023) used a two-layer meta algorithm MsMwC-Master (Chen et al., 2021) as the meta learner, resulting in a three-layer ensemble structure, which is also not efficient enough. Compared with approaches above, our Algorithm 1 is simpler and more efficient as it requires \(( T)\) base learners and only \(1\) gradient query in each round. We emphasize that our contributions mainly lie in the technical aspects showing that although simple, our approach can achieve the optimal universal gradient-variation regret, which is accomplished via two novel analytical components.

### Novel Analysis on Empirical Gradient Variations

In this part, we provide a novel analysis of the gradient-variation regret. For clarity, we illustrate from the lowest level -- as we only use one gradient \( f_{t}(_{t})\) in the \(t\)-th round, to obtain the gradient variation \(V_{T}\) defined in (1.2), it is necessary to first attain its _empirical_ version \(_{T}_{t T}\| f_{t}(_{t})- f_{t -1}(_{t-1})\|^{2}\). Previous studies decompose this term into two parts:

\[\| f_{t}(_{t})- f_{t-1}(_{t-1})\| ^{2} \| f_{t}(_{t})- f_{t-1}(_ {t})\|^{2}+\| f_{t-1}(_{t})- f_{t-1}(_{t-1})\| ^{2}\] \[_{}\| f_{t}()-  f_{t-1}()\|^{2}+L^{2}\|_{t}-_{t-1}\|^{2},\]

using smoothness (i.e., Assumption 2). Aggregating the first term over \(T\) rounds leads to the desired \(V_{T}\) quantity and the remaining challenge is to control the algorithmic stability \(\|_{t}-_{t-1}\|^{2}\). Consequently, since each decision is a weighted combination of base learners' decisions (i.e., \(_{t}=_{i N}p_{t,i}_{t,i}\)), the algorithmic stability is difficult to control. To this end, Yan et al. (2023) decomposed the stability term in the following way:

\[\|_{t}-_{t-1}\|^{2}_{i=1}^{N}p_{t,i}\|_{t,i}-_{t-1,i}\|^{2}+\|_{t}-_{t-1}\|_{1}^{2}.\] (3.2)

Consequently, for the first term, the authors injected correction terms to the meta learner following Zhao et al. (2024). To cancel the second term, the meta algorithm must include a corresponding negative stability term in its analysis, while achieving an optimistic second-order bound simultaneously. To the best of our knowledge, the only feasible algorithm satisfying both requirements is the two-layer meta algorithm MsMwC-Master(Chen et al., 2021), which leads to a three-layer online ensemble structure and therefore affects the efficiency. Besides, it attains a second-order bound of the form \(( Q_{T,i}})\), where \(Q_{T,i}_{t}(_{t,i}-m_{t,i})^{2}\), which causes the sub-optimality of the regret guarantees with an additional logarithmic factor in the results of Yan et al. (2023).

In this work, we handle the empirical gradient variation alternatively via a novel and simple analysis with two key parts: _(i)_ a negative term arising from linearization; and _(ii)_ a useful smoothness property. First, we observe that the instantaneous regret can be transformed as:

\[f_{t}(_{t})-f_{t}(^{})= f_{t}( _{t}),_{t}-^{}-_{f_{t}}( ^{},_{t}),\] (3.3)

where \(^{}_{}_{t}f_{t}()\) and \(_{f}(,) f()-f()-  f(),-\) is the Bregman divergence associated with function \(f()\). The last term is a _negative term from linearization_, which 

[MISSING_PAGE_FAIL:7]

where the first step uses (3.3) and the fact that \(_{f_{t}}(^{},_{t})\| _{t}-^{}\|^{2}\) since \(f_{t}()\) is \(\)-strongly convex. The second step uses the definition of the best base learner (indexed by \(i^{}\)): \(_{i^{}} 2_{i^{}}\) and the definition of surrogate functions defined in (3.1). The first term above (_meta regret_) assesses how well the algorithm tracks the best base learner, and the second term (_base regret_) measures the best base learner's performance. The meta regret contains a linearized regret with a negative term from curvatures. This negative term is useful for exp-concave and strongly convex functions if the linearized regret enjoys a second-order bound:

\[_{t=1}^{T}r_{t,i^{}}-}}{4}_{t=1}^{T}\| _{t}-_{t,i^{}}\|^{2}^{T}  f_{t}(_{t}),_{t}-_{t,i^{}} ^{2}}-}}{4}_{t=1}^{T}\|_{t}- _{t,i^{}}\|^{2},\]

where the \(1/\) factor can be absorbed by the final regret \(( V_{T})\). The base regret is defined on the surrogates, which preserves the curvature properties, but using only \(1\) gradient \( f_{t}(_{t})\).

Below we explain the reason for handling the empirical gradient variation defined on surrogates. Taking the \(i\)-th strongly convex base learner as an example, it updates as \(_{t,i}=_{}[}_{t,i}-_{t}_{t}]\) and \(}_{t+1,i}=_{}[}_{t,i}- _{t} h_{t,i}^{}(_{t,i})]\), an initialization of the OOMD algorithm, where \(_{t}\) represents the step size, \(_{}[]=_{}\|- \|\) denotes the Euclidean projection onto \(\), and \(}_{t,i}\) is an intermediate variable. With appropriately chosen step sizes, the base learner achieves an optimistic bound of \(( D_{T})\), where \(D_{T}=_{t T}\| h_{t,i}^{}(_{t,i})- _{t}\|^{2}\) (e.g., please refer to Theorem 15 of Chiang et al. (2012)). Therefore, choosing the optimism as \(_{t}= h_{t-1,i}^{}(_{t-1,i})\) leads to an empirical gradient-variation bound \(( D_{T})\)_defined on surrogates_,3 where

\[D_{T} = _{t=2}^{T}\| h_{t,i}^{}(_{t,i})-  h_{t-1,i}^{}(_{t-1,i})\|^{2}\] \[= _{t=2}^{T}\| f_{t}(_{t})- f_{t-1} (_{t-1})+}{2}(_{t,i}-_{t})- }{2}(_{t-1,i}-_{t-1})\|^{2}.\]

The empirical gradient variation defined on the original functions, i.e., \(\| f_{t}(_{t})- f_{t-1}(_{t-1})\|^{2}\), can be handled via the analysis in Section 3.2. The main challenge is to deal with the rest terms caused by the surrogate functions. Yan et al. (2023) overcame this issue by controlling \((_{t}-_{t-1})\) and \((_{t,i}-_{t-1,i})\) separately. Again, as we have explained in Section 3.2, since the decision \(_{t}\) is a weighted combination of base learners' decisions (i.e., \(_{t}=_{i N}p_{t,i}_{t,i}\)), handling the algorithmic stability term \(\|_{t}-_{t-1}\|^{2}\) directly using (3.2) would results in a less efficient three-layer online ensemble structure and the sub-optimality of the regret guarantees, as Yan et al. (2023) did.

In this work, we propose a novel analysis -- while controlling \((_{t,i}-_{t})-(_{t-1,i}-_{t-1})\) in each individual round is hard, it can be bounded when _aggregated over the time horizon_. Specifically, we bound it by combining two summations into one:

\[_{t=2}^{T}\|(_{t,i}-_{t})-(_{t-1,i}-_{t-1})\|^{2}_{t=2}^{T}\|_{t,i}-_{t}\|^{2}+ _{t=2}^{T}\|_{t-1,i}-_{t-1}\|^{2} 2_{t=1}^{T}\| _{t,i}-_{t}\|^{2}.\]

The same idea is also used in the derivation of (3.5). Consequently, this term can be canceled out by the negative term from curvatures in the meta regret. For this cancellation to occur, appropriate coefficients are chosen, which are provided in the detailed proofs (e.g., the 'Regret Analysis' part in the proof of Theorem 1) and are omitted here for clarity.

This simple and novel analysis eliminates the need to control the overall algorithmic stability term of \(\|_{t}-_{t-1}\|^{2}\) required by previous works, and is essential for achieving the improved computational efficiency and the optimal regret guarantees, as shown in the next part.

### Optimal Universal Gradient-Variation Regret Guarantees

In this part, we present our main theoretical result -- our simple and efficient Algorithm 1 (in Section 3.1) which adopts two novel analyses (in Section 3.2 and Section 3.3) achieves the _optimal_

[MISSING_PAGE_FAIL:9]

**Theorem 3**.: _Under Assumption 1 and smoothness of \(F_{t}()\) for any \(t[T]\): if \(F_{t}()\) is convex, Algorithm 1 achieves \((^{2}+_{1:T}^{2}})\); if \(f_{t}()\) is exp-concave, it achieves \((d(_{1:T}^{2}+_{1:T}^{2}))\); and if \(F_{t}()\) is strongly convex, it achieves \(((_{}^{2}+_{}^{2})((_{1:T}^{2}+ _{1:T}^{2})/(_{}^{2}+_{}^{2})))\)._

Theorem 3 requires exp-concavity of the individual function \(f_{t}()\) rather than the expected function \(F_{t}()\). This assumption is also used by Chen et al. (2023) and common in the studies of stochastic exp-concave optimization Mahdavi et al. (2015); Koren and Levy (2015).

### Discussion on Comparison with Correction-based Approach

In this part, we discuss the technical comparison with the previous correction-based approach Yan et al. (2023). Compared with their approach, ours is simpler and achieves the optimal universal problem-dependent regret Theorem 1 and Theorem 2) and the best known guarantees in the SEA model (Theorem 3). Although not providing guarantees as favorable as ours, Yan et al. (2023) can control the overall algorithmic stability (i.e., \(\|_{t}-_{t-1}\|^{2}\)) using collaborative online ensemble Zhao et al. (2024), which is necessary in achieving fast rates in multi-player games Syrgkanis et al. (2015). For example, in a min-max game \(_{}_{}^{ }A\), where \(A\) is a game matrix, since \(A\) is unknown, the Nash equilibrium is typically computed through repeated play, i.e., player-\(\) and player-\(\) select \(\{_{t}\}_{t=1}^{T}\) and \(\{_{t}\}_{t=1}^{T}\) sequentially to approach the Nash equilibrium. For player-\(\), in the \(t\)-th round, it suffers a loss \(_{t}^{}A_{t}\) and receives the gradient \(A_{t}\). Similarly, player-\(\) suffers \(-_{t}^{}A_{t}\) and receives \(-A_{t}\). For player-\(\), if it updates via OOMD, its gradient-variation regret contains \(\|A_{t}-A_{t-1}\|^{2}\), which includes the stability of player-\(\). In this case, to achieve fast rates, we indeed need to control the algorithm stability like \(\|_{t}-_{t-1}\|^{2}\) and \(\|_{t}-_{t-1}\|^{2}\). This can be done by Yan et al. (2023) while this work cannot since we do not directly control the algorithmic stability. Interested readers can refer to Appendix A.2 in Yan et al. (2023) for more details.

## 5 Conclusion

In this work, we investigate universal online learning with gradient-variation regret. We propose a simple two-layer online ensemble approach that not only achieves the optimal \(( V_{T})\), \(( V_{T})\), and \((})\) regret simultaneously for \(\)-strongly convex, \(\)-exp-concave, and convex functions and is efficient with \(( T)\) base learners and only \(1\) gradient query per round. This is done via the negative Bregman divergence term from linearization and the useful smoothness property of \(\| f()- f()\|^{2} 2L_{f}( ,)\). We further validate the effectiveness of our approach and results by implying the optimal universal small-loss regret directly in analysis and achieving the best known results in the stochastically extended adversarial model.

Two future directions are worth investigating. The first is to reduce the number of projections to only \(1\) in each round Mhammedi et al. (2019); Zhao et al. (2022); Yang et al. (2024), thereby further improving the computational efficiency. The second direction involves extending our algorithm and results to the unconstrained domain using recent advances in parameter-free online learning Orabona and Pal (2016); Cutkosky and Orabona (2018); Jacobsen and Cutkosky (2022), to broaden its applicability across a wider range of scenarios.

    &  &  \\    & Strongly Convex & &  &  &  \\  Sachs et al. (2022) & \(((_{}^{2}+_{}^{2}) T)\) & N/A & \((^{2}+_{1:T}^{2}})\) & ✗ \\  Chen et al. (2024) & \(((_{}^{2}+_{}^{2})( ^{2}+_{1:T}^{2}}{_{}^{2}+_{}^{2}}))\) & \((d(_{1:T}^{2}+_{1:T}^{2}))\) & ✗ \\   Sachs et al. (2023) & \(((_{}^{2}+_{}^{2}+D^{2}L^{2})^{2}T)\) & N/A & \(()\) & ✓ \\  Yan et al. (2023) & \(((_{}^{2}+_{}^{2})(_{1:T}^{2}+_{1 :T}^{2}))\) & \((d(_{1:T}^{2}+_{1:T}^{2}))\) & \((^{2}+_{1:T}^{2})}(_{1:T}^{2}+ _{1:T}^{2}))\) & ✓ \\ 
**Ours** & \(((_{}^{2}+_{}^{2})( ^{2}+_{1:T}^{2}}{_{}^{2}+_{}^{2}}))\) & \((d(_{1:T}^{2}+_{1:T}^{2}))\) & ✓ \\   

Table 2: Comparisons of our results with existing ones. The second column presents the regret bounds, where \(_{1:T}^{2}\) and \(_{1:T}^{2}\) represent the stochastic and adversarial statistics of the SEA problem. The last column indicates whether the results can be achieved by a single algorithm (i.e., suitable in the universal setup). We achieve the same state-of-the-art guarantees as Chen et al. (2024) using one single algorithm.