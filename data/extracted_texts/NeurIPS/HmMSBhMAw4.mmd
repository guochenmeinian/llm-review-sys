# Periodic agent-state based Q-learning for POMDPs

Amit Sinha

McGill University, Mila

Matthieu Geist

Cohere

Aditya Mahajan

McGill University, Mila

###### Abstract

The standard approach for Partially Observable Markov Decision Processes (POMDPs) is to convert them to a fully observed belief-state MDP. However, the belief state depends on the system model and is therefore not viable in reinforcement learning (RL) settings. A widely used alternative is to use an agent state, which is a model-free, recursively updateable function of the observation history. Examples include frame stacking and recurrent neural networks. Since the agent state is model-free, it is used to adapt standard RL algorithms to POMDPs. However, standard RL algorithms like Q-learning learn a stationary policy. Our main thesis that we illustrate via examples is that because the agent state does not satisfy the Markov property, non-stationary agent-state based policies can outperform stationary ones. To leverage this feature, we propose PASQL (periodic agent-state based Q-learning), which is a variant of agent-state-based Q-learning that learns periodic policies. By combining ideas from periodic Markov chains and stochastic approximation, we rigorously establish that PASQL converges to a cyclic limit and characterize the approximation error of the converged periodic policy. Finally, we present a numerical experiment to highlight the salient features of PASQL and demonstrate the benefit of learning periodic policies over stationary policies.

## 1 Introduction

Recent advances in reinforcement learning (RL) have successfully integrated algorithms with strong theoretical guarantees and deep learning to achieve significant successes . However, most RL theory is limited to models with perfect state observations . Despite this, there is substantial empirical evidence showing that RL algorithms perform well in partially observed settings . Recently, there has been a significant advances in the theoretical understanding of different RL algorithms for POMDPs  but a complete understanding is still lacking.

**Planning in POMDPs.** When the system model is known, the standard approach  is to construct an equivalent MDP with the belief state (which is the posterior distribution of the environment state given the history of observations and actions at the agent) as the information state. The belief state is policy independent and has time-homogeneous dynamics, which enables the formulation of a belief-state based dynamic program (DP). There is a rich literature which leverages the structure of the resulting DP to propose efficient algorithms to solve POMDPs . See  for a review. However, the belief state depends on the system model, so the belief-state based approach does not work for RL.

**RL in POMDPs.** An alternative approach for RL in POMDPs is to consider policies which depend on an _agent state_\(\{z_{t}\}_{t 1}\), where \(Z_{t}\), which is a recursively updateable compression of the history: the agent starts at an initial state \(z_{0}\) and recursively updates the agent state as some function of the current agent-state, next observation, and current action. A simple instance of agent-state is _frame stacking_, where a window of previous observations is used as state . Another example is to use a recurrent neural network such as LSTM or GRU to compress the history of observations and actions into an agent state . Infact, as argued in  such an agent state is present in most deep RL algorithms for POMDPs. We refer to such a representation as an "agent state" because it captures the agent's internal state that it uses for decision making.

When the agent state is an information state, i.e., satisfies the Markov property, i.e., \((z_{t+1}|z_{1:t},a_{1:t})=(z_{t+1}|z_{t},a_{t})\) and is sufficient for reward prediction, i.e., \([R_{t}|y_{1:t},a_{1:t}]=[R_{t}|z_{t},a_{t}]\) (where \(y_{t}\) is the observation, \(a_{t}\) is the action, and \(R_{t}\) is the per-step reward), the optimal agent-state based policy can be obtained via a dynamic program (DP) . An example of such an agent state is the belief state. But, in general, the agent state is not an information state. For example, frame stacking and RNN do not satisfy the Markov property, in general. It is also possible to have agent-states that satisfy the Markov property but are not sufficient for reward prediction (e.g., when the agent state is always a constant). In all such settings, the best agent-state policy cannot be obtained via a DP. Nonetheless, there has been considerable interest to use RL to find a good agent-state based policy.

One of the most commonly used RL algorithms is off-policy Q-learning, which we call agent-state Q-learning (ASQL). In ASQL for POMDPs, the Q-learning iteration is applied as if the agent state satisfied the Markov property even though it does not. The agent starts with an initial \(Q_{1}(z,a)\), acts according to a behavior policy \(\), i.e., chooses \(a_{t}(z_{t})\), and recursively updates

\[Q_{t+1}(z,a)=Q_{t}(z,a)+_{t}(z,a)R_{t}+_{a^{} }Q_{t}(z_{t+1},a^{})-Q_{t}(z,a)\] (ASQL)

where \([0,1)\) is the discount factor and the learning rates \(\{_{t}\}_{t 1}\) are chosen such that \(_{t}(z,a)=0\) if \((z,a)(z_{t},a_{t})\). The convergence of ASQL has been recently presented in  which show that under some technical assumptions, ASQL converges to a limit. The policy determined by ASQL is the greedy policy w.r.t. this limit.

**Limitation of Q-learning with agent state.** The greedy policy determined by ASQL is stationary (i.e., uses the same control law at every time). In infinite horizon MDPs (and, therefore, also in POMDPs when using the belief state as an agent state), stationary policies perform as well as non-stationary policies. This is because the agent-state satisfies the Markov property. However, in ASQL the agent state generally does not satisfy the Markov property. Therefore, _restricting attention to stationary policies may lead to a loss of optimality!_

As an illustration, consider the POMDP shown in Fig. 1, which is described in detail in App. A.2 as Ex. 2. Suppose the system starts in state \(1\). Since the dynamics are deterministic, the agent can infer the current state from the history of past actions and can take the action to increment the current state and receive a per-step reward of \(+1\). Thus, the performance \(J^{}_{}\) of belief-state based policies is \(J^{}_{}=1/(1-)\). Contrast this with the performance \(J^{}_{}\) of deterministic agent-state base policies with agent state equal to current observation, which is given by \(J^{}_{}=(1+-^{2})/(1-^{3})<J^{}_{}\). In particular, for \(=0.9\), \(J^{}_{}=10\) which is larger than \(J^{}_{}=4.022\).

We show that the gap between \(J^{}_{}\) and \(J^{}_{}\) can be reduced by considering non-stationary policies. Ex. 2 has deterministic dynamics, so the optimal policy can be implemented in _open-loop_ via a sequence of control actions \(\{a^{}_{t}\}_{t 1}\), where \(a^{}_{t}=\{t_{1}\}\). This open-loop policy can be implemented via any information structure, including agent-state based policies. _Thus, a non-stationary deterministic agent-state based policy performs better than stationary deterministic agent-state based policies._ A similar conclusion also holds for models with stochastic dynamics.

**The main idea.** Arbitrary non-stationary policies cannot be used in RL because such policies have countably infinite number of parameters. In this paper, we consider a simple class of non-stationary

Figure 1: The cells indicate the state of the environment. Cells with the same background color have the same observation. The cells with a thick red boundary correspond to elements of the set \(_{0}\{n(n+1)/2+1:n\}\), where the action \(0\) gives a reward of \(+1\) and moves the state to the right, while the action \(1\) gives a reward of \(-1\) and resets the state to \(1\). The cells with a thin black boundary correspond to elements of the set \(_{1}=_{0}\), where the action \(1\) gives the reward of \(+1\) and moves the state to the right while the action \(0\) gives a reward of \(-1\) and resets the state to \(1\). Discount factor \(=0.9\).

policies with finite number of parameters: _periodic policies_. An agent-state based policy \(=(_{1},_{2},)\) is said to be periodic with period \(L\) if \(_{t}=_{t^{}}\) whenever \(t t^{}\).

To highlight the salient feature of periodic policies, we perform a brute force search over all deterministic periodic policies of period \(L\), for \(L=\{1,,10\}\), in 2. Let \(J^{}_{L}\) denote the optimal performance for policies of period \(L\). The result is shown below (see App. A.2 for details):

   \(L\) & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\  \(J^{}_{L}\) & 4.022 & 4.022 & 7.479 & 6.184 & 8.810 & 7.479 & 9.340 & 8.488 & 9.607 & 8.810 \\   

The above example highlights some salient features of periodic policies: (i) Periodic deterministic agent-state based policies may outperform stationary deterministic agent-state based policies. (ii) \(\{J^{}_{L}\}_{L 1}\) is not a monotonically increasing sequence. This is because \(_{L}\), the set of all periodic deterministic agent-state based policies of period \(L\), is not monotonically increasing. (iii) If \(L\) divides \(M\), then \(J^{}_{L} J^{}_{M}\). This is because \(_{L}_{M}\). In other words, if we take any integer sequence \(\{L_{n}\}_{n 1}\) that has the property that \(L_{n}\) divides \(L_{n+1}\), then the performance of the policies with period \(L_{n}\) is monotonically increasing in \(n\). For example, periodic policies with period \(L\{n!:n\}\) will have monotonically increasing performance. (iv) In the above example, the set \(_{0}\) is chosen such that the optimal sequence of actions1 is not periodic. Therefore, even though periodic policies can achieve a performance that is arbitrarily close to the optimal belief-based policies, they are not necessarily globally optimal (in the class of non-stationary agent-state based policies). Thus, the periodic deterministic policy class is a middle ground between the stationary deterministic and non-stationary policy classes and provides us a simple way of leveraging the benefits of non-stationarity while trading-off computational and memory complexity.

The main contributions of this paper are as follows.

1. Motivated by the fact that non-stationary agent-state based policies outperform stationary ones, we propose a variant of agent-state based Q-learning (ASQL) that learns periodic policies. We call this algorithm periodic agent-state based Q-learning (PASQL).
2. We rigorously establish that PASQL converges to a cyclic limit. Therefore, the greedy policy w.r.t. the limit is a periodic policy. Due to the non-Markovian nature of the agent-state, the limit (of the Q-function and the greedy policy) depends on the behavioral policy used during learning.
3. We quantify the sub-optimality gap of the periodic policy learnt by PASQL.
4. We present numerical experiments to illustrate the convergence results, highlight the salient features of PASQL, and show that the periodic policy learned by PASQL indeed performs better than stationary policies learned by ASQL.

## 2 Periodic agent-state based Q-learning (PASQL) with agent state

### Model for POMDPs

A POMDP is a stochastic dynamical system with state \(s_{t}\), input \(a_{t}\), and output \(y_{t}\), where we assume that all sets are finite. The system operates in discrete time with the dynamics given as follows: The initial state \(s_{1}\) and for any time \(t\), we have

\[(s_{t+1},y_{t+1} s_{1:t},y_{1:t},a_{1:t})=(s_{t+1},y _{t+1} s_{t},a_{t}) P(s_{t+1},y_{t+1} s_{t},a_{t})\]

where \(P\) is a probability transition matrix. In addition, at each time the system yields a reward \(R_{t}=r(s_{t},a_{t})\). We will assume that \(R_{t}[0,R_{}]\). The discount factor is denoted by \([0,1)\).

Let \(=(_{1},_{2},)\) denote any (history dependent and possibly randomized) policy. Then the action at time \(t\) is given by \(a_{t}_{t}(y_{1:t},a_{1:t-1})\). The performance of policy \(\) is given by

\[J^{}_{a_{t}_{t}( y_{1:t},a_{t-1})\\ (s_{t+1},y_{t+1}) P(s_{t},a_{t})}_{t=1}^{ }^{t-1}r(s_{t},a_{t})s_{1}.\]

The objective is to find a (history dependent and possibly randomized) policy \(\) to maximize \(J^{}\).

Agent state for Pomdps.An agent-state is model-free recursively updateable function of the history of observations and actions. In particular, let \(\) denote agent-state space. Then, the agent state process \(\{z_{t}\}_{t 0}\), \(z_{t}\), starts with an initial value \(z_{0}\), and is then recursively computed as \(z_{t+1}=(z_{t},y_{t+1},a_{t})\) for a pre-specified agent-state update function \(\).

We use \(=(_{1},_{2},)\) to denote an agent-state based policy,2 i.e., a policy where the action at time \(t\) is given by \(a_{t}_{t}(z_{t})\). An agent-state based policy is said to be **stationary** if for all \(t\) and \(t^{}\), we have \(_{t}(a|z)=_{t^{}}(a|z)\) for all \((z,a)\). An agent-state based policy is said to be **periodic** with period \(L\) if for all \(t\) and \(t^{}\) such that \(t t^{}\), we have \(_{t}(a|z)=_{t^{}}(a|z)\) for all \((z,a)\).

### PASQL: Periodic agent-state based Q-learning algorithm for Pomdps

We now present a periodic variant of agent-state based Q-learning, which we abbreviate as PASQL. PASQL is an online, off-policy learning algorithm in which the agent acts according to a behavior policy \(=(_{1},_{2},)\) which is a periodic (stochastic) agent-state based policy \(\) with period \(L\).

Let \([\![t]\!](t L)\) and \(\{0,1,,L-1\}\). Let \((z_{1},a_{1},R_{1},z_{2},a_{2},R_{2},)\) be a sample path of agent-state, action, and reward observed by the agent. In PASQL, the agent maintains an \(L\)-tuple of Q-functions \((Q_{t}^{0},Q_{t}^{1},,Q_{t}^{L-1})\), \(t 1\). The \(\)-th component, \(\), is updated at time steps when \([\![t]\!]=\). In particular, we can write the update as

\[Q_{t+1}^{}(z,a)=Q_{t}^{}(z,a)+_{t}^{}(z,a)R_{t}+ _{a^{}}Q_{t}^{[+1]}(z_{t+1},a^{})-Q_{t} ^{}(z,a),,\] (PASQL)

where the learning rate sequence \(\{(_{t}^{0},,_{t}^{L-1})\}_{t 1}\) is chosen such that \(_{t}^{}(z,a)=0\) if \((,z,a)([\![t]\!],z_{t},a_{t})\) and satisfies Assm. 1. PASQL differs from ASQL in two aspects: (i) The behavior policy \(\) is periodic. (ii) The update of the Q-function is periodic. When \(L=1\), PASQL collapses to ASQL.

The standard convergence analysis of Q-learning for MDPs shows that the Q-function convergences to the unique solution of the MDP dynamic program (DP). The key challenge in characterizing the convergence of PASQL is that the agent state \(\{Z_{t}\}_{t 1}\) does not satisfy the Markov property. Therefore, a DP to find the best agent-state based policy does not exist. So, we cannot use the standard analysis to characterize the convergence of PASQL. In Sec. 2.3, we provide a complete characterization of the convergence of PASQL.

The quality of the converged solution depends on the expressiveness of the agent state. For example, if the agent state is not expressive (e.g., agent state is always constant), then even if PASQL converges to a limit, the limit will be far from optimal. Therefore, it is important to quantify the degree of sub-optimality of the converged limit. We do so in Sec. 2.4.

### Establishing the convergence of tabular PASQL

To characterize the convergence of tabular PASQL, we impose two assumptions which are standard for analysis of RL algorithms . The first assumption is on the learning rates.

**Assumption 1**: For all \((,z,a)\), the learning rates \(\{_{t}^{}(z,a)\}_{t 1}\) are measurable with respect to the sigma-algebra generated by \((z_{1:t},a_{1:t})\) and satisfy \(_{t}^{}(z,a)=0\) if \((,z,a)([\![t]\!],z_{t},a_{t})\). Moreover, \(_{t 1}_{t}^{}(z,a)=\) and \(_{t 1}(_{t}^{}(z,a))^{2}<\), almost surely.

The second assumption is on the behavior policy \(\). We first state an immediate property.

**Lemma 1**: _For any behavior policy \(\), the process \(\{(S_{t},Z_{t})\}_{t 1}\) is Markov. Therefore, the processes \(\{(S_{t},Z_{t},A_{t})\}_{t 1}\) and \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t 1}\) are also Markov._

**Assumption 2**: The behavior policy \(\) is such that the Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t 1}\) is time-periodic3 with period \(L\) and converges to a cyclic limiting distribution \((_{}^{0},,_{}^{L-1})\), where \(_{(s,y)}_{}^{}(s,y,z,a)>0\) for all \((,z,a)\) (i.e., all \((,z,a)\) are visited infinitely often).

[MISSING_PAGE_FAIL:5]

Many commonly used metrics on probability spaces are IPMs. For example, (i) Total variation distance for which \(=\{(f) 1\}\), where \((f)= f- f\) is the span seminorm of \(f\). In this case, \(_{}(f)=(f)\). (ii) Wasserstein distance for which \(=\{(f) 1\}\), where \((f)\) is the Lipschitz constant of \(f\). In this case, \(_{}(f)=(f)\). Other examples include Kantorovich metric, bounded Lipschitz metric, and maximum mean discrepancy. See  for more details.

**Sub-optimality gap.** Let \((t,)\{ t:[\![]\!]=\}\). Furthermore, for any \(\) and \(t\), define

\[^{}_{t} _{(t,)}_{h_{},a_{} }[R_{} h_{},a_{}]-_{s}r(s,a _{})^{}_{}(s_{}(h_{}),a_{}),\] \[^{}_{t} _{(t,)}_{h_{},a_{} }d_{}((Z_{+1}= h_{},a_{}),P^{} _{}(Z_{+1}=_{}(h_{}),a_{})).\]

Then, we have the following sub-optimality gap for \(_{}\).

**Theorem 2**: _Let \(V^{}_{}(z)_{a}Q^{}_{}(z,a)\). Then,_

\[_{h_{}}V^{}_{t}(h_{t})-V^{_{}}_{t}(h_{t}) )}_{}^{} ^{[t+]}_{t+}+^{[t+]}_{t+}_ {}(V^{[t++1]}_{}).\] (3)

See App. F for proof. The salient features of the sub-optimality gap of Thm. 2 are as follows.

* We can recover some existing results as special cases of Thm. 2. When we take \(L=1\), Thm. 2 recovers the sub-optimality gap for ASQL obtained in [23, Thm. 3]. In addition, when the agent state is a sliding window memory, Thm. 2 is similar to the sub-optimality gap obtained in [11, Thm. 4.1]. Note that the results of Thm. 2 for these special cases is more general because the previous results were derived under a restrictive assumption on the learning rates.
* The sub-optimality gap in Thm. 2 is on the sub-optimality w.r.t. the optimal _history-dependent_ policy rather than the optimal non-stationary agent-state policy. Thus, it inherently depends on the quality of the agent state. Consequently, even if \(L\), the sub-optimality gap does not go to zero.
* It is not easy to characterize the sensitivity of the bound to the period \(L\). In particular, increasing \(L\) means changing behavioral policy \(\), and therefore changing the converged limit \((^{0}_{},,^{L-1}_{})\), which impacts the right hand side of (3) in a complicated way. So, it is not necessarily the case that increasing \(L\) reduces the sub-optimality gap. This is not surprising, as we have seen earlier in Ex. 2 presented in the introduction that even the performance of periodic agent-state based policies is not monotone in \(L\).

## 3 Numerical experiments

In this section, we present a numerical example to highlight the salient features of our results. We use the following POMDP model.

**Example 1**: Consider a POMDP with \(=\{0,1,,5\}\), \(=\{0,1\}\), \(=\{0,1\}\) and \(=0.9\). The dynamics are as shown in Fig. 2. The observation is \(0\) in states \(\{0,1,2\}\) which are shaded white and is \(1\) in states \(\{3,4,5\}\) which are shaded gray. The transitions shown in green give a reward of \(+1\); those in in blue give a reward of \(+0.5\); others give no reward.

We consider a family of models, denoted by \((p)\), \(p\), which are similar to Ex. 1 except the controlled state transition matrix is \(pI+(1-p)P\), where \(P\) is the controlled state transition matrix of Ex. 1 shown in Fig. 2. In the results reported below, we use \(p=0.01\). The hyperparameters for the experiments are provided in App. H.

**Convergence of PASQL with \(L=2\).** We assume that the agent state \(Z_{t}=Y_{t}\) and take period \(L=2\). We consider three behavioral policies: \(_{k}=(^{0}_{k},^{1}_{k})\), \(k\{1,2,3\}\), where \(^{}_{k}\{0,1\}\), and \(^{}_{k}\{0,1\}\). The policy \(^{}_{k}\{0,1\}\) is given by

\[^{}_{k}\{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\{0,1\}\{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\}\] (4)

where \(\) is the set of all possible states in \(\). The policy \(^{}_{k}\{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\{0,1\}\{0,1\}\{0,1\}\{0,1\} \{0,1\}\{0,1\}\{0,1\}\((\{0,1\})\), \(\{0,1\}\). The policy \(_{k}\) is completely characterized by four numbers which we write in matrix form as: \([_{k}^{0}(0|0),_{k}^{1}(0|0);_{k}^{0}(0|1),_{k}^{1}(0|1)]\). With this notation, the three policies are given by \(_{1}:=[0.2,0.8;0.8,0.2]\), \(_{2}[0.5,0.5;0.5,0.5]\), \(_{3}[0.8,0.2;0.2,0.8]\).

For each behavioral policy \(_{k}\), \(k\), run PASQL for 25 random seeds. The median + interquantile range of the iterates \(\{Q_{t}^{t}(z,a)\}_{t 1}\) as well as the theoretical limits \(Q_{_{k}}(z,a)\) (computed using Thm. 1) are shown in Fig. 3. The salient features of these results are as follows:

* PASQL converges close to the theoretical limit predicted by Thm. 1.
* As highlighted earlier, the limiting value \(Q_{_{k}}^{}\) depends on the behavioral policy \(_{k}\).
* When the aperiodic behavior policy \(_{2}\) is used, the Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t 1}\) is aperiodic, and therefore the limiting distribution \(_{_{2}}^{}\) and the corresponding Q-functions \(Q_{_{2}}^{}\) do not depend on \(\). This highlights the fact that we have to choose a periodic behavioral policy to converge to a non-stationary policy (PASQL-policy).

**Comparison of converged policies.** Finally, we compute the periodic greedy policy \(_{_{k}}=(_{_{k}}^{0},_{_{k}}^{1})\) given by (PASQL-policy), \(k\), and compute its performance \(J^{_{_{k}}}\) via policy evaluation on the product space \(\) (see App. G). We also do a brute force search over all \(L=2\) periodic deterministic agent-state policies to compute the optimal performance \(J_{2}^{}\) over all such policies. The results, displayed in Table 1, illustrate the following:

* The greedy policy \(_{_{k}}\) depends on the behavioral policy. This is not surprising given the fact that the limiting value \(Q_{_{k}}^{}\) depends on \(_{k}\).
* The policy \(_{_{1}}\) achieves the optimal performance, whereas the policies \(_{_{2}}\) and \(_{_{3}}\) do not perform well. This highlights the importance of starting with a good behavioral policy. See Sec. 5 for a discussion on variants such as \(\)-greedy.

**Advantage of learning periodic policies.** As stated in the introduction, the main motivation of PASQL is that it allows us to learn non-stationary policies. To see why this is useful, we run ASQL (which is effectively PASQL with \(L=1\)). We again consider three behavioral policies: \(_{k}\), \(k\{1,2,3\}\), where \(_{k}\{0,1\}(\{0,1\})\), where (using similar notation as for \(L=2\) case) \(_{1}[0.2;0.8]\), \(_{2}[0.5;0.5]\), \(_{3}[0.8;0.2]\).

For each behavioral policy \(_{k}\), \(k\), run ASQL for 25 random seeds. The results are shown in App. A.1. The performance of the greedy policies \(_{_{k}}\) and the performance of the best period

  \(J_{2}^{}\) & \(J^{_{_{1}}}\) & \(J^{_{_{2}}}\) & \(J^{_{_{3}}}\) \\
6.793 & 6.793 & 1.064 & 0.532 \\  

Table 1: Performance of converged periodic policies.

Figure 3: PASQL iterates for different behavioral policies (in blue) and the limit predicted by Thm. 1 (in red).

\(L=1\) deterministic agent-state-based policy computed via brute force is shown in Table 2. The key implications are as follows:

* As was the case for PASQL, the greedy policy \(_{_{k}}\) depends on the behavioral policy. As mentioned earlier, this is a fundamental consequence of the fact that the agent state is not an information state. Adding (or removing) periodicity does not change this feature.
* The best performance of ASQL is worse than the best performance of PASQL. This highlights the potential benefits of using periodicity. However, at the same time, if a bad behavioral policy is chosen (e.g., policy \(_{3}\)), the performance of PASQL can be worse than that of ASQL for a nominal policy (e.g., policy \(_{2}\)). This highlights that periodicity is not a magic bullet and some care is needed to choose a good behavioral policy. Understanding what makes a good periodic behavioral policy is an unexplored area that needs investigation.

## 4 Related work

**Policy search for agent state policies.** There is a rich literature on planning with agent state-based policies that build on the policy evaluation formula presented in App. G. See  for review. These approaches rely on the system model and cannot be used in the RL setting.

**State abstractions for POMDPs** are related to agent-state based policies. Some frameworks for state abstractions in POMDPs include predictive state representations (PSR) , approximate bisimulation , and approximate information states (AIS)  (which is used in our proof of Thm. 2). Although there are various RL algorithms based on such state abstractions, the key difference is that all these frameworks focus on stationary policies in the infinite horizon setting. Our key insight that non-stationary/periodic policies improve performance is also applicable to these frameworks.

**ASQL for POMDPs.** As stated earlier, ASQL may be viewed as the special case of PASQL when \(L=1\). The convergence of the simplest version of ASQL was established in  for \(Z_{t}=Y_{t}\) under the assumption that the actions are chosen i.i.d. (and do not depend on \(z_{t}\)). In  it was established that \(Q^{0}_{}\) is the fixed point of (ASQL), but convergence of \(\{Q_{t}\}_{t 1}\) to \(Q^{0}_{}\) was not established. The convergence of ASQL when the agent state is a finite window memory was established in . These results were generalized to general agent-state models in . The regret of an optimistic variant of ASQL was presented in . However, all of these papers focus on stationary policies.

Our analysis is similar to the analysis of  with two key differences. First, their convergence results were derived under the assumption that the learning rates are the reciprocal of visitation counts. We relax this assumption to the standard learning rate conditions of Assm. 1 using ideas from stochastic approximation. Second, their analysis is restricted to stationary policies. We generalize the analysis to periodic policies using ideas from time-periodic Markov chains.

**Q-learning for non-Markovian environments.** As highlighted earlier, a key challenge in understanding the convergence of PASQL is that the agent-state is not Markovian. The same conceptual difficulty arises in the analysis of Q-learning for non-Markovian environments . Consequently, our analysis has stylistic similarities with the analysis in  but the technical assumptions and the modeling details are different. And more importantly, they restrict attention to stationary policies. Given our results, it may be worthwhile to explore if periodic policies can help in non-Markovian environments as well.

**Continual learning and non-stationary MDPs.** Non-stationarity is an important consideration in continual learning (see  and references therein). However, in these settings, the environment is non-stationary. Our setting is different: the environment is stationary, but non-stationary policies help because the agent state is not Markov.

**Hierarchical learning.** The options framework  is a hierarchical approach that learns temporal abstractions in MDPs and POMDPs. Due to temporal abstraction, the policy learned by the options framework is non-stationary. The same is true for other hierarchical learning approaches proposed in . In principle, PASQL could be considered as a form of temporal abstraction where time is split into trajectories of length \(L\) and then a policy of length \(L\) is learned. However, the theoretical analysis for options is mostly restricted to MDP setting and the convergence guarantees for options in POMDPs are weaker . Nonetheless, the algorithmic tools developed for options might be useful for PASQL as well.

**Double Q-learning.** The update equation of PASQL are structurally similar to the update equations used in double Q-learning . However, the motivation and settings are different: the motivation for Double Q-learning is to reduce overestimation bias in off-policy learning in MDPs, while the motivation for PASQL is to induce non-stationarity while learning in POMDPs. Therefore, the analysis of the two algorithms is very different. More importantly, the end goals differ: double Q-learning learns a stationary policy while PASQL learns a periodic policy.

**Use of non-stationary/periodic policies in MDPs** is investigated in  in the context of approximate dynamic programming (ADP). Their main result was to show that using non-stationary or periodic policies can improve the approximation error in ADP. Although these results use periodic policies, the setting of ADP in MDPs is very different from ours.

## 5 Discussion

**Deterministic vs. stochastic policies.** In this work, we restricted attention to periodic deterministic policies. In principle, we could have also considered periodic _stochastic_ policies. For stationary policies (i.e., when period is one), stochastic policies can outperform deterministic policies  as illustrated by Ex. 3 in App. A.3. However, we do not consider stochastic policies in this work because we are interested in understanding Q-learning with agent-state and Q-learning results in a deterministic policy. There are two options to obtain stochastic policies: using regularization , which changes the objective function; or using policy gradient algorithms , which are a different class of algorithms than Q-learning.

However, as illustrated in the motivating Ex. 2 presented in the introduction, non-stationary policies can do better than stationary stochastic policies as well. So, adding non-stationarity via periodicity remains an interesting research direction when learning stochastic policies as well.

**PASQL is a special case of ASQL with state augmentation.** In principle, PASQL could be considered as a special case of ASQL with an augmented agent state \(_{t}=(Z_{t},[\![t]\!])\). However, the convergence analysis of ASQL in  does not imply the convergence of PASQL because the results of  are derived under the assumption that Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t 1}\) is irreducible and aperiodic, while we assume that the Markov chain is _periodic_. Due to our weaker assumption, we are able to establish convergence of PASQL to time-varying periodic policies.

**Non-stationary policies vs. memory augmentation.** Non-stationarity is a fundamentally different concept than memory augmentation. As an illustration, consider the T-shaped grid world (first considered in ) shown in Fig. 4, which has a corridor of length \(2n\). In App. A.4, we show that for this example, a stationary policy which uses a sliding window of past \(m\) observations and actions as the agent state needs a memory of at least \(m>2n\) to reach the goal state. In contrast, a periodic policy with period \(L=3\) can reach the goal state for every \(n\). This example shows that periodicity is a different concept from memory augmentation and highlights the fact that mechanisms other than memory augmentation can achieve optimal behavior.

The analysis of this paper is applicable to general memory augmented policies, so we do not need to choose between memory augmentation and periodicity. Our main message is that once the agent's memory is fixed based on practical considerations, adding periodicity could improve performance.

**Choice of the period \(L\).** If the agent state \(Z_{t}\) is a good approximation to the belief state, then ASQL (or, equivalently, PASQL with \(L=1\)) would converge to an approximately optimal policy. So, using PASQL a period \(L>1\) is useful when the agent state is not a good approximation of the belief state.

As shown by Ex. 2 in the introduction, the performance of the best periodic policy does not increase monotonically with the period \(L\). However, if we consider periods in the set \(\{n!:n\}\), then the performance increases monotonically. However, PASQL does not necessarily converge to the best periodic policy. The quality of the converged policy (PASQL-policy) depends on the behavior

Figure 4: A T-shaped grid world. Agent starts at s, where it learns whether the goal state is \(_{1}\) or \(_{2}\). It has to go through the corridor \(\{1,,2n\}\), without knowing where it is, reach \(\) and go up or down to reach the goal state.

policy \(\). The difficulty of finding a good behavioral policy increases with \(L\). In addition, increasing the period increases the memory required to store the tuple \((Q^{0},,Q^{L})\) and the number of samples needed to converge (because each component is updated only once every \(L\) samples). Therefore, the choice of the period \(L\) should be treated as a hyperparameter that needs to be tuned.

**Choice of the behavioral policy.** The behavioral policy impacts the converged limit of PASQL, and consequently it impacts the periodic greedy policy that is learned. As we pointed out in the discussion after Thm. 1, this dependence is a fundamental consequence of using an agent state that is not Markov and cannot be avoided. Therefore, it is important to understand how to choose behavioral policies that lead to convergence to good policies.

**Generalization to other variants.** Our analysis is restricted to tabular off-policy Q-learning where a fixed behavioral policy is followed. Our proof fundamentally depends on the fact that the behavioral policy induces a cyclic limiting distribution on the periodic Markov chain \(\{(S_{t},Y_{t},Z_{t},A_{t})\}_{t 1}\). Such a condition is not satisfied in variants such as \(\)-greedy Q-learning and SARSA. Generalizing the technical proof to cover these more practical algorithms (including function approximation) is an important future direction.