# The Best of Both Worlds: On the Dilemma of

Out-of-distribution Detection

 Qingyang Zhang\({}^{1}\), Qiuxuan Feng\({}^{1}\), Joey Tianyi Zhou\({}^{2}\), Yatao Bian\({}^{3}\),

Qinghua Hu\({}^{1}\), Changqing Zhang\({}^{1}\)

College of Intelligence and Computing, Tianjin University\({}^{1}\)

A*STAR\({}^{2}\), Tencent AI Lab\({}^{3}\)

Work done during an internship at Tencent AI Lab.Correspondence to Changqing Zhang <zhangchangqing@tju.edu.cn>

###### Abstract

Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identify semantic OOD samples and robustly generalize for covariate-shifted OOD samples. However, we discover that the superior OOD detection performance of state-of-the-art methods is achieved by secretly sacrificing the OOD generalization ability. Specifically, the classification accuracy of these models could deteriorate dramatically when they encounter even minor noise. This phenomenon contradicts the goal of model trustworthiness and severely restricts their applicability in real-world scenarios. What is the hidden reason behind such a limitation? In this work, we theoretically demystify the "_sensitive-robust_" dilemma that lies in many existing OOD detection methods. Consequently, a theory-inspired algorithm is induced to overcome such a dilemma. By decoupling the uncertainty learning objective from a Bayesian perspective, the conflict between OOD detection and OOD generalization is naturally harmonized and a dual-optimal performance could be expected. Empirical studies show that our method achieves superior performance on standard benchmarks. To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without compromising OOD generalization ability. Our code is available at https://github.com/QingyangZhang/DUL.

## 1 Introduction

Endowing machine learning models with out-of-distribution (OOD) detection and OOD generalization ability are both essential for their deployment in the open world . We borrow an example of autonomous driving from  to demonstrate the motivation of these two tasks. Given a machine learning model trained on in-distribution (ID) data (top image in Fig. 1 (a)), OOD detection aims to sensitively perceive uncertainty arising upon outliers that do not belong to any known classes of training data  (bottom right image in Fig. 1 (a)). While OOD generalization expects machine learning models to be robust in the presence of unexpected noise or corruption, e.g., rainy or snowy weather (bottom left image in Fig. 1 (a)). In this paper, we reveal that many previous methods pursue OOD detection performance at a secret cost of sacrificing OOD generalization ability. To make things worse, we observe that some SOTA OOD detection methods may result in a catastrophic collapse in classification performance (\(\)15% accuracy degradation) when encountering even slight noise. One pioneering work  makes a trade-off between OOD detection and OOD generalization, but the relationship between these two tasks is still largely unexplored. The learning objectives of these two tasks are seemingly conflicting at first glance. OOD detection encourages sensitive uncertainty awareness (highly uncertain prediction) on unseen data, while generalization expects the prediction to be confident and robust under unforeseeable distributional shifts. Previous work in OOD detectionresearch area  characterizes the relationship between OOD detection and OOD generalization as a trade-off and thus striking for a balanced performance. However, this trade-off significantly limits the employment of current state-of-the-art OOD detection methods. Naturally, one might require the model to be aware of the OOD input for ensuring safety, but certainly does not expect to sacrifice the generalization ability, not to mention that the catastrophically collapsed classification performance under noise or corruption.

In this work, we first uncover the potential reason behind this limitation by characterizing the generalization error lower bound of previous OOD detection methods, which is referred to _sensitive-robust dilemma_. To overcome the dilemma, we devise a novel Decoupled Uncertainty Learning (DUL) framework for dual-optimal performance. The decoupled uncertainties are separately responsible for characterizing semantic OOD (detection) and covariate-shifted OOD (generalization). Thanks to the decoupled uncertainty learning objective, dual-optimal OOD detection and OOD generalization performance could be expected. Our emphasis lies on a particular category of OOD detection methods in the classification task, including max softmax probability (MSP) based model , energy-based model (EBM)  and Bayesian methods . This selection offers two-fold advantages. First, MSP, EBM and Bayesian detectors encompass major OOD detection advances in classification task . Second, numerous OOD detection works in diverse learning tasks (classification, object detection , time-series prediction and image segmentation) are all roughly related to classification . The contributions of this paper are summarized as follows:

* This paper reveals that existing SOTA OOD detection methods may suffer from catastrophic degradation in terms of OOD generalization. That is, their superior OOD detection ability is achieved by (secretly) sacrificing OOD generalization ability. We theoretically demystify the sensitive-robust dilemma in learning objectives as the main reason behind such a limitation.
* In contrast to previous works that characterize OOD detection and generalization as conflictive learning tasks and thus implying an inevitable trade-off, we propose a novel learning framework termed Decoupled Uncertainty Learning (DUL) to successfully break through the limitation beyond a simple trade-off. Our DUL substantially harmonizes the conflict between OOD detection and OOD generalization, which achieves the best OOD detection performance without sacrificing the OOD generalization ability.
* We conduct extensive experiments on standard benchmarks to validate our findings. Our DUL achieves dual-optimal OOD detection and OOD generalization performance. To our best knowledge, DUL is the first method that gains state-of-the-art OOD detection performance without sacrificing OOD generalization ability.

Figure 1: **(a)**: Models trained on in-distribution (ID) data inevitably encounter distributional shifts during their deployment. OOD generalization expects the model to correctly classify covariate-shifted data that undergoes noise or corruption due to environmental issues. OOD detection aims to identify samples that do not belong to any known classes for trustworthiness consideration. **(b)**: Limitations of current advanced OOD detection methods. We consider 8 representative OOD detection methods including the baseline method MSP  (without any OOD detection regularization), Entropy , EBM , Bayesian , SOTA OOD detection methods WOODS , POEM , recent advanced SCONE  which aims to seek for a good trade-off and the proposed DUL. All these methods exhibit a degraded generalization ability compared to baseline method MSP and lie in a trade-off area except our DUL. The goal of this paper is to understand and mitigate this phenomenon.

Related works

**OOD detection** aims to indicate whether the input arises from unknown classes that are not present in training data, which is essential for model trustworthiness. In the classification task, the majority of advanced OOD detection methods include MSP detectors which characterize samples with lower max softmax probability as OOD [6; 14; 15; 7; 16]. EBM detectors identify high energy samples as OOD and frequently establish better performance than MSP detectors [8; 11; 17; 4], and various other types OOD detection methods such as distance-based detectors , non-parametric KNN-based detectors  which also show promises. According to the training paradigm, OOD detection methods can be split into auxiliary OOD-free and auxiliary OOD-required methods. Auxiliary OOD-free methods directly use the model pre-trained on ID data only for OOD detection. Another line of methods assumes that some OOD data is accessible during training and incorporates auxiliary outlier datasets (collected from websites or other datasets) for further enhancing OOD detection performance. By exposing the model to some semantic OOD during training, auxiliary OOD-required methods frequently outperform auxiliary OOD-free methods on commonly-used benchmarks [20; 5].

**OOD generalization** expects the model to be robust under unforeseeable noise or corruption [21; 22; 23; 24; 25]. Basically, OOD generalization expects invariant and confident prediction on OOD data. Examples include classic domain adaption (DA) methods which encourage the model's behavior to be invariant across different distributions [21; 26; 27]. Besides, test-time adaption (TTA) directly encourages confident predictions on OOD data by minimizing predictive entropy [28; 29; 30]. However, as we will show later, confident prediction and invariance are seemingly conflictive to OOD detection purpose and further imply an unavoidable trade-off. The most related work to our paper is SCONE , which strikes to keep a balance between OOD detection and generalization performance. We argue that such a trade-off is not necessary and the conflict can be elegantly eliminated.

**Uncertainty estimation in Bayesian framework.** In the Bayesian framework, predictive uncertainty can be regarded as an indicator of whether the input sample is prone to be OOD. Since OOD samples are unseen during training and thus should be of higher uncertainty than ID. The overall predictive uncertainty of a classification model can be decomposed into three factors according to their source, including data (aleatoric) uncertainty (AU), distributional uncertainty (DU), and model (epistemic) uncertainty (EU) [31; 9]. AU measures the natural complexity of the data (e.g., class overlap, label noise) and EU results from the difficulty of estimating the model parameters with finite training data. DU arises due to the mismatch between the distributions of test and training data. A line of classic measurement can be used to capture various types of uncertainty including entropy, mutual information, and differential entropy .

## 3 Preliminaries

We consider \(K\)-class classification task with classifier \(f:^{K}\) parameterized by \(\), where \(\) is the input space and \(=\{1,2,...,K\}\) denotes the target space. The model output \(f_{}(x)\) is considered as logits. The \(k\)-th element in logits is denoted as \(f_{k}(x)\) indicates the confidence of predicting \(x\) to class \(k\). The predicted distribution \(F(x)\) is obtained by normalizing \(f(x)\) with the softmax function. We first formalize all possible distributions that the model might encounter.

* In-distribution \(P^{}_{}\) which denotes the distribution of labeled training data.
* Covariate-shifted OOD \(P^{}_{}\) which is relevant to OOD generalization. \(P^{}_{}\) is of the same label space with ID. However, its marginal distribution \(P^{}_{}\) encounters shifts due to unexpected noise or corruption.
* Semantic OOD \(P^{}_{^{}}\) is the distribution of data that do not belong to any known class. Its label space has no overlap with the known ID label space, i.e., \(^{}=\).

In the following paper, we omit the subscript for simplicity. The goal of OOD detection is to build a detector \(G:[,]\) to decide whether an input \(x\) is semantic OOD data or not through a thresholding function \(G\) deduced from classifier \(f\)

\[G_{}(x)=&g_{f}(x)\\ &g_{f}(x)>\,,\] (1)where \(\) is the threshold. \(g_{f}\) is an OOD scoring function deduced from \(f\), which is expected to assign a higher value to OOD than ID. For example, in MSP detectors, \(g_{f}(x)=-_{k}F(x)\) where \(F(x)\) is the predicted softmax probability (negative max softmax probability). In EBM detectors, \(g_{f}\) is realized by the energy function \(E(x;f):=-_{i=1}^{K}e^{f_{k}(x)}\) and the semantic input of OOD should be of high energy . Since it is difficult to foresee \(P^{}\) one will encounter, a board line of OOD detection works [7; 8; 9; 11; 17; 12; 32; 33] regularize the model on some auxiliary OOD data \(P^{}_{}\) during training (e.g., data from the web or other datasets), and expect the model can learn useful heuristic to handle unknown test-time OOD \(P^{}_{}\). The learning objective is shown as follows

\[_{}\,_{(x,y) P^{}}[_{}(f(x),y)]+_{ P^{}_{ }}[_{}f()],\] (2)

where \(_{}\) is the standard cross entropy loss for the original classification task. \(_{}\) is the OOD detection regularization term depending on the detector used, which generally encourages a high uncertainty on \(P^{}_{}\). For example, \(_{}\) is set to cross entropy between \(F(x)\) and the uniform distribution for MSP detector . In EBM detectors , \(_{}\) is realized as a margin ranking loss to explicitly encourage a large energy gap between ID and semantic OOD. In this paper, we are interested in this setting for the following reasons:

1. In contrast to labeled data in supervised learning literature, auxiliary OOD data can be unlabeled and easy-to-collected in practice .
2. Most SOTA methods involve auxiliary outliers [5; 20] for superior performance.
3. Even under some strict assumptions that \(P^{}_{}\) is unavailable, recent works utilize GAN , diffusion model  or sampling strategy  to generate "virtual" outliers for training.

Thus we believe this setting is promising and the cost of auxiliary outliers is minor given the importance of ensuring model trustworthiness. At test-time, the model is evaluated in terms of

* ID accuracy (ID-Acc \(\)) which measures the model's performance on \(P^{}\),
* OOD accuracy (OOD-Acc \(\)) measures the OOD generalization ability on \(P^{}\),
* False positive rate at 95% true positive rate (FPR95\(\)) := \(_{x P^{}_{}}((G_{}(x)= ))\) measures the OOD detection ability, where \(\) is chosen when true positive rate (TPR) is \(95\%\). \(\) is the indicator function. In OOD detection, ID samples are considered as negative.

It is worth noting that in the standard OOD detection setting [11; 4], the test OOD data should not have any overlapped classes or samples with training-time auxiliary OOD data \(P^{}_{}\). Let \(^{}_{}\) and \(^{}_{}\) be the label space of \(P^{}_{}\) and \(P^{}_{}\) respectively, we have \(^{}_{}^{}_{ }=\). Otherwise, OOD detection would be a trivial problem.

## 4 Sensitive-robust Dilemma of Out-of-distribution Detection

In this section, we detail the limitation of current OOD detection methods: their OOD detection performance is achieved at the cost of generalization ability. This limitation implies the potential risk of SOTA OOD detection methods and underscores the urgent need for a better solution. Firstly, we re-examine representative OOD detection methods of six different types, including 1) baseline model MSP that is trained without any OOD detection regularization , 2) entropy-regularization (Entropy) that encourages high predictive entropy on OOD , which is devised for MSP detectors, 3) energy-regularization for EBM detectors that enforces the output with high energy score for OOD input , 4) Bayesian uncertainty learning that encourages high overall uncertainty on OOD , 5) state-of-the-art OOD detection methods WOODS  and POEM  6) the most related SCONE  that seeks for a trade-off between OOD detection and generalization performance.

**Limitation of current OOD detection methods.** In Fig. 1 (b), we investigate current OOD detection methods in terms of OOD classification error and FPR95. The expected classifier should yield both low OOD classification error and FPR95. As it is observed, despite the superior OOD detection performance, all above methods significantly underperform the baseline MSP in terms of OOD generalization. By contrast, our method (DUL) successfully overcomes the limitation.

**Theoretical justification.** Toward understanding the limitation, we provide theoretical analysis for two types of most popular OOD detection methods, i.e., MSP and EBM detectors. Our analysis identifies the "_sensitive-robust_" dilemma as the main reason behind such a limitation. The roadmap of our analysis is: (1) inspired by transfer learning theory, we first reveal that OOD detection regularization applied on semantic OOD may also affect the behavior of model on covariate-shifted OOD; (2) then we demonstrate why MSP detectors suffer from poor generalization by characterizing its generalization error bound; (3) we further identify that EBM methods  suffer from a similar drawback when incorporating with gradient-based optimization. First of all, we recap the definition of disparity discrepancy in transfer learning theory .

**Definition 1** (Disparity with Total Variation Distance).: _Given two hypotheses \(f^{},f\) and distribution \(P\), we define the Disparity with Total Variation Distance between them as_

\[_{P}(f^{},f)=_{P}[TV(F_{f}||F_{f^{}})],\] (3)

_where \(F_{f^{}},F_{f}\) are the class distributions predicted by \(f^{},f\) respectively. \(TV(||)\) is the total variation distance, i.e., \(TV(F_{f}||F_{f^{}})=_{k=1}^{K}||F_{f,k}-F_{f^{},k}|\)._

**Definition 2** (Disparity Discrepancy with Total Variation Distance, DD with TVD).: _Given a hypothesis space \(\) and two distributions \(P,Q\), the Disparity Discrepancy with Total Variation Distance (DD with TVD) is defined as_

\[d_{}(P,Q):=_{f^{},f}(_{P}(f^ {},f)-_{Q}(f^{},f)).\] (4)

Disparity discrepancy (DD) measures the "distance" between two distributions \(P,Q\) which considers the hypothesis space. DD is one of the most fundamental conceptions in transfer learning theory which constrains the behavior of hypothesis in \(\) should not be dissimilar substantially on different distributions \(P\) and \(Q\). 3 If the DD between semantic OOD and covariate-shifted OOD is limited, one can suppose that OOD detection regularization applied to semantic OOD samples will also influence the model's behavior on covariate-shifted OOD. Thus encouraging high uncertainty on semantic OOD may also result in highly uncertain prediction on covariate-shifted OOD, which is potentially harmful to generalization ability. We first formalize this intuition for MSP detectors.

**Theorem 1** (Sensitive-robust dilemma).: _Let \(^{}\), \(P^{}_{}\) be the covariate-shifted OOD and semantic OOD distribution. \(_{^{}}(f)\) denotes standard cross entropy loss taking expectation on \(P^{}\), i.e., generalization error. Then we have_

\[_{^{}}(f)}_{ generalization~{}error}}\,C-}}\,_{P^{ }_{}}[_{}(f)}_{ detection~{}loss}}- K]^{}-d_{ }(^{},^{}_{}),\] (5)

_where \(_{}\) is the OOD detection loss devised for MSP detectors defined in , i.e., cross-entropy between predicted distribution \(F(x)\) and uniform distribution. \(d_{}(P^{},P^{}_{})\) is DD with TVD that measures the dissimilarity of covariate-shifted OOD and semantic OOD. \(C\) and \(\) are both some constants depending on hypothesis space \(\), \(P^{}\) and \(P^{}_{}\)._

The proof is deferred in Appendix A. Theorem 1 demonstrates that for MSP detectors, the OOD detection objective conflicts with OOD generalization. The model's generalization error lower bound is negatively correlated with OOD detection loss that the model tries to minimize. Thus given a limited \(d_{}\), pursuing low OOD detection loss on \(P^{}_{}\) will also inevitably result in highly uncertain prediction on \(P^{}\). It is worth noting that such an interpretative theorem is applicable for all MSP-based OOD detectors no matter whether the model involves \(P^{}_{}\) during training or not. Since the inherent motivation of OOD detection methods lies in minimizing the OOD detection loss in \(P^{}_{}\), regardless of the training strategies used.

**Why a limited \(d_{}(P^{},P^{}_{})\) is practical?** In Theorem 1, \(d_{}(P^{},P^{}_{})\) measures the dissimilarity between \(P^{}\) and \(P^{}_{}\). It seems that this lower bound will be very small and trivial when \(d_{}(P^{},P^{}_{})\) is large enough. However, since the semantic OOD samples can be any samples that do not belong to ID classes, one can suppose that semantic OOD samples are extremely diverse and some are of high similarity with ID and covariate-shifted OOD . Detecting these "ID-like" OOD samples is inherently the core challenge of OOD detection . Thus, it is reasonable to assume a limited \(d_{}(P^{},P^{}_{})\). We provide more discussions in the Appendix D.2.

[MISSING_PAGE_FAIL:6]

uncertainty. Such a property is well suited to achieve OOD detection and generalization jointly since high DU no longer necessarily indicates high overall uncertainty.

**Decoupled Uncertainty Learning.** While the aforementioned Bayesian framework enjoys theoretical potentiality, its learning object  lacks consideration of OOD generalization. Similar to other OOD detection methods, it also directly enforces high overall uncertainty on OOD

\[_{}_{^{}}(p(y|x))||p(|x))+_{^{}_{}}(p(| ))||(|=)),\] (10)

where \(p(y|x),p(|x)\) are the ground-truth distribution and predicted distribution on ID. The model's prediction on OOD is enforced to be close to a rather flat Dirichlet distribution. It is worth noting that \((|=)\) means all classes are equiprobable, and the entropy of the final prediction is maximized. As shown in Fig. 1 (b), the vanilla Bayesian method  also suffers from degraded OOD generalization performance. To this end, we propose **Decoupled Uncertainty Learning** (DUL), a novel OOD detection regularization method that explicitly encourages high DU on OOD samples without affecting the overall uncertainty. Similarly to previous OOD detection methods , our DUL is also devised in a finetune manner for effectiveness. Given a classifier \(f_{_{0}}\) well pre-trained on \(P^{}\), the goal of DUL lies in enhancing its OOD detection performance without sacrificing any generalization ability. Specifically, we finetune the model by encouraging higher DU but non-increased overall uncertainty on \(P^{}_{}\). The learning objective of DUL is

\[_{} _{(x,y) P^{}}[_{ }(f(x),y)]}_{}+_{  P^{}_{}}||(0,(h_{0}+m_{ })-h)||_{}}_{}\] (11) \[ |))=H(p_{0}(|))} _{}\ \ P^{}_{},\]

where \(H()\) is the entropy. \(p(|)\) and \(p_{0}(|)\) are the predicted distribution on semantic OOD data \(\) after and before finetuning. The first term is the original ID classification loss. The second term is OOD detection loss, which encourages high DU on outlier \(\). \(m_{}\) and \(>0\) are hyperparameters. \(h_{0},h\) are DU on \(\) before and after finetuning. Here we measure DU with the differential entropy (\(h|p(|,)|=-_{S^{K-1}}p(|)(p(| ))d\), \(S\) is a \(K\)-simplex). We refer interested readers to the Appendix D.1 for mathematical details. The third term constraining on \(H(p(|))\) avoids increment of overall uncertainty during finetuning and thus the generalization ability can be retained. Considering the difficulty of constrained optimization, we convert Eq. 11 into an unconstrained form and get our final minimizing objective

\[_{P^{}}[_{}(f(x),y)]}_{ }+_{P^{}_{}}\{ (0,(h_{0}+m_{})-h)||_{}}_{ }+(p(| )||p_{0}(|))}_{}\},\] (12)

where \(\) is hyperparameter. In contrast to previous Bayesian method , DUL only encourages high DU rather than overall uncertainty on OOD and explicitly discourages high entropy in the final prediction. The implementation details are in Appendix D.1.

## 6 Experiment

We conduct experiments to validate our analysis and the superiority of DUL. The questions to be verified are Q1 Motivation. To what extent does OOD detection conflict with OOD generalization in previous methods? Q2 Effectiveness. Does DUL achieve better OOD detection and generalization performance compared to its counterparts? Q3 Interpretability. Does the proposed method well decouple uncertainty as expected?

### Experimental Setup

Our settings follow the common practice [8; 11; 20; 5] in OOD detection. Here we present a brief description and more details about datasets, metrics, and implementation are in Appendix B.1 and B.2.

**Datasets. \(\) ID datasets \(P^{}\).** We train the model on different ID datasets including CIFAR-10, CIFAR-100 and ImageNet-200 (a subset of ImageNet-1K  with 200 classes). \(\)**Auxiliary OOD datasets \(P^{}_{}\)**. In CIFAR experiments, we use ImageNet-RC as \(P^{}_{}\). ImageNet-RC is a down-sampled variant of the original ImageNet-1K which is widely adopted in previous OOD detection works [8; 11; 17]. We also conduct experiments on the recent TIN-597  as an alternative. When ImageNet-200 is ID, the remaining 800 classes termed ImageNet-800 are considered as \(P_{ train}^{ SEM}\). o **OOD detection test sets**\(P_{ test}^{ SEM}\) are a suite of diverse datasets introduced by commonly used benchmark . In CIFAR experiments, we use SVHN , Places365 , Textures , LSUN-R, LSUN-C  and iSUN  as \(P_{ test}^{ SEM}\). When \(P^{ ID}\) is ImageNet-200, \(P_{ test}^{ SEM}\) consists of iNaturalist , Open-Image , NINCO  and SSB-Hard . It is worth noting that in standard OOD detection settings, there should be no overlapped classes between \(P^{ ID}\), \(P_{ train}^{ SEM}\) and \(P_{ test}^{ SEM}\), otherwise OOD detection is a trivial problem. o **OOD generalization test sets**\(P_{ COV}\) is the original ID test set corrupted with additive Gaussian noise of \((0,5)\), following . Besides, we also conduct experiments on CIFAR10-C, CIFAR100-C and ImageNet-C which involve 15 diverse types of different noise or corruption (e.g., snow, rain, frost, fog...) in Appendix C.

   /P_{ train}^{ SEM}\)} &  &  &  \\  & & ID-Acc \(\) & OOD-Acc \(\) & FPR \(\) & AUROC \(\) & AUPR \(\) \\   & MSP & \(96.11\) & \(87.35\) & \(41.96\) & \(89.28\) & \(68.00\) \\  & EBM (pretrain) & \(96.11\) & \(87.35\) & \(32.45\) & \(89.34\) & \(75.22\) \\  & Maxlogits & \(96.11\) & \(87.35\) & \(32.90\) & \(89.26\) & \(74.47\) \\  & Mahalanobis & \(96.11\) & \(87.35\) & \(32.53\) & \(93.93\) & \(74.96\) \\   & Entropy & \(96.04\) & \(72.57\) & \(6.63\) & \(98.72\) & \(94.00\) \\  & EBM (finetune) & \(96.10\) & \(79.03\) & \(3.61\) & \(98.39\) & \(94.88\) \\  & POEM & \(94.32\) & \(78.89\) & \(\) & \(\) & \(\) \\  & DPN & \(95.69\) & \(85.52\) & \(4.28\) & \(98.53\) & \(94.93\) \\  & WOODS & \(96.01\) & \(80.14\) & \(7.12\) & \(98.45\) & \(92.46\) \\  & SCONE & \(95.96\) & \(78.50\) & \(7.02\) & \(98.45\) & \(92.46\) \\  & DUL (ours) & \(96.02^{ 0.00}\) & \(^{ 0.02}\) & \(58.98^{ 0.12}\) & \(98.47^{ 0.02}\) & \(92.41^{ 1.29}\) \\  & DUL\({}^{}\) (ours) & \(96.04^{ 0.00}\) & \(^{ 0.09}\) & \(5.99^{ 0.06}\) & \(98.28^{ 0.01}\) & \(98.40^{ 0.13}\) \\   & Entropy & \(95.94\) & \(80.51\) & \(11.60\) & \(97.93\) & \(92.16\) \\  & EBM (finetune) & \(95.38\) & \(83.67\) & \(19.36\) & \(87.51\) & \(83.63\) \\  & POEM & \(95.44\) & \(83.17\) & \(24.34\) & \(80.83\) & \(94.25\) \\  & DPN & \(94.39\) & \(79.23\) & \(17.27\) & \(94.92\) & \(87.67\) \\  & WOODS & \(95.57\) & \(84.08\) & \(7.58\) & \(\) & \(93.08\) \\  & SCONE & \(95.19\) & \(84.68\) & \(8.02\) & \(98.21\) & \(93.08\) \\  & DUL (ours) & \(^{ 0.01}\) & \(87.03^{ 0.02}\) & \(^{ 0.67}\) & \(98.21^{ 0.01}\) & \(91.29^{ 1.29}\) \\  & DUL\({}^{}\) (ours) & \(95.94^{ 0.00}\) & \(^{ 0.07}\) & \(10.34^{ 0.01}\) & \(97.67^{ 0.01}\) & \(^{ 0.06}\) \\    & MSP & \(80.99\) & \(55.95\) & \(74.63\) & \(80.19\) & \(42.59\) \\  & EBM (finetune) & \(80.99\) & \(55.95\) & \(67.42\) & \(82.67\) & \(49.35\) \\  & Maxlogits & \(80.99\) & \(55.95\) & \(69.32\) & \(82.30\) & \(47.60\) \\  & Mahalanobis & \(80.99\) & \(55.95\) & \(61.51\) & \(85.97\) & \(56.10\) \\   & Entropy & \(80.21\) & \(45.48\) & \(22.29\) & \(95.33\) & \(82.34\) \\  & EBM (finetune) & \(80.53\) & \(48.14\) & \(13.47\) & \(96.78\) & \(87.84\) \\  & POEM & \(78.15\) & \(42.18\) & \(\) & \(\) & \(\) \\  & DPN & \(78.90\) & \(50.14\) & \(18.36\) & \(95.42\) & \(74.45\) \\  & WOODS & \(80.69\) & \(54.38\) & \(38.15\) & \(92.01\) & \(71.79\) \\  & SCONE & \(80.80\) & \(\) & \(47.60\) & \(80.61\) & \(65.29\) \\  & DUL (ours) & \(^{ 0.04}\) & \(^{ 2.32}\) & \(12.49^{ 0.05}\) & \(95.24^{ 0.01}\) & \(86.72^{ 0.08}\) \\  & DUL (ours) & \(^{ 0.00}\) & \(^{ 0.54}\) & \(11.12^{ 0.05}\) & \(95.46^{ 0.36}\) & \(96.49^{ 0.13}\) \\   & Entropy & \(80.15\) & \(46.25\) & \(26.88\) & \(93.50\) & \(79.81\) \\  & EBM (finetune) & \(79.94\) & \(50.00\) & \(26.87\) & \(91.68\) & \(80.08\) \\  & POEM & \(78.68\) & \(52.53\) & \(32.71\) & \(91.30\) & \(94.65\) \\  & DPN & \(78.64\)

**Metrics.** For OOD detection performance evaluation, we report the average FPR95, AUROC and AUPR to be consistent with . OOD generalization ability is compared in terms of classification accuracy (OOD-Acc). Besides, we also report classification accuracy on ID test sets (ID-Acc).

**Compared methods.** We compare DUL with a board line of OOD detection methods, including auxiliary OOD required and auxiliary OOD free methods. \(\)**Auxiliary OOD-free methods** do not require \(P_{}^{}\) during training, including MSP , Maxlogits , pretrained EBM  and Mahalaobis . \(\)**Auxiliary OOD-required methods** explicitly regularize the model on \(P_{}^{}\), including entropy-regularization (Entropy) , finetuned EBM , DPN of Bayesian framework , POEM  and WOODS . We also compare our DUL to recent advanced SCONE  which aims to keep a balance between OOD detection and generalization.

### Experimental Results

**Dilemma between OOD detection and generalization (Q1).** We validate the dilemma mentioned before in Fig. 1. As shown in Tab. 1, though many advanced methods establish superior OOD detection performance, their OOD generalization degrades a lot. For example, recent SOTA POEM achieves nearly perfect OOD detection performance on CIFAR10 when ImageNet-RC serves as \(P_{}^{}\) with \(3.32\%\) false positive error rate (FPR95). However, its OOD-Acc drops a lot (about \(10\%\)) compared to baseline MSP. This phenomenon is also observed in other advanced methods. To further detail this phenomenon, we reduce the weight of OOD detection regularization terms in Entropy and finetuned EBM and show the performance on both OOD detection and generalization. As shown in Table 3, when the regularization strength increases, OOD detection performance improves (lower FPR.), while the OOD generalization performance degrades (higher error rate).

**OOD detection and generalization ability (Q2).** As shown in Tab. 1, DUL establishes strong overall performance in terms of both OOD detection and generalization. We highlight a few essential observations: 1) **Compared to auxiliary OOD free methods**, DUL establishes substantial improvement due to additional regularization on auxiliary outliers. 2) **Compared to auxiliary OOD required methods**, our method achieves superior OOD detection performance without sacrificing generalization ability. Meanwhile, previous OOD detection methods commonly exhibit severely degraded classification accuracy, with many cases increasing by more than \(10\%\) error rate. 3) **Comparison to the most related work SCONE .** Despite recent advanced SCONE simultaneously considering both two targets, we observe that it can be hard to find a good trade-off. In contrast, dual-optimal OOD detection and generalization performance is achieved by our DUL. Noted that DUL is the only method that achieves _state-of-the-art_ detection performance (mostly the best or second best) without degraded generalization ability (no red values in the entire row). The sensitive-robust dilemma is no longer observed in our method. These observations justify our expectation of DUL. 4) **Combining with existing methods.** Besides, to further demonstrate the effectiveness of the proposed DUL, we also add the unchanged overall uncertainty term in Eq.12 to the original Entropy and finetuned EBM. The results in Table 2 show that DUL regularization can also benefit EBM. However, combining Entropy with our regularization can not improve the accuracy substantially. This is not surprising, since the target of Entropy (high entropy prediction) and our DUL (non-increased entropy) directly conflict according to Theorem 1. 5) **Comparison to methods with an extra OOD detect branch.** Different from aforementioned methods, a line of recent OOD detectors [50; 51; 52; 17] employ extra output branches aside from the classification logits (with a shared backbone for feature extraction). For these OOD detectors, our theoretical analysis is not directly applicable and further analysis from a feature learning perspective may be needed in future work. However, the proposed DUL is devised in a finetune manner. Compared to OOD detectors with extra output branches that requires re-training the classifier from scratch, DUL can be applied to any pre-trained model (e.g., from torchvision, huggingface), with modest computation overhead.

**Visualization of estimated uncertainty (Q3).** To evaluate the uncertainty estimation, we visualize the distribution of ID (CIFAR-10) and OOD (SVHN) samples in terms of uncertainty. As we can see in Fig. 2 (b), our DUL establishes a distinguishable (distributional) uncertainty gap between test-time ID and OOD data, which indicates a good sensitiveness for OOD detection. By contrast, the baseline method MSP (Fig.2 (a)) can not effectively discriminate ID and OOD. Besides, we visualize the predictive entropy (overall uncertainty) on covariate-shifted OOD (CIFAR-10 with Gaussian noise) in Fig. 2 (c), our DUL yields much lower entropy compared to other methods. Besides, we visualize the data uncertainty on semantic OOD test data (Textures) when CIFAR-10 is ID in Fig. 6.2. Theinvestigated methods are 1) pretrained model training on ID dataset only, 2) finetuned model with OOD detection regularization (ablating the last term in Eq.12), and 3) finetuned model with the full DUL method described by Eq.12. As shown in Fig. 6.2, to keep the overall uncertainty and enlarge the distributional uncertainty (for OOD detection), the data uncertainty must be reduced. We use Eq.17 from  to calculate data uncertainty. The distributional uncertainty is shifted by subtracting that on ID dataset. These results meet our expectation.

## 7 Conclusion

This paper provides both theoretical and empirical analysis towards understanding the dilemma between OOD detection and generalization. We demonstrate that the superior OOD detection performance of current advances are achieved at the cost of generalization ability. The theory-inspired algorithm successfully removes the conflict between previous OOD detection and generalization methods. For SOTA OOD detection performance, our implementation assumes that auxiliary outliers are available during training. This limitation is noteworthy for our DUL as well as the most existing SOTA OOD detection methods. We argue that this added cost is minor and reasonable given the significance of ensuring model trustworthiness in open-environments. Reducing the dependency on auxiliary OOD data can be an interesting research direction for the future exploration.