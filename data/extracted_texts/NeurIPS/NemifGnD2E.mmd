# GNeSF: Generalizable Neural Semantic Fields

Hanlin Chen Chen Li Mengqi Guo Zhiwen Yan Gim Hee Lee

Department of Computer Science, National University of Singapore

{hanlin.chen, gimhee.lee}@comp.nus.edu.sg

###### Abstract

3D scene segmentation based on neural implicit representation has emerged recently with the advantage of training only on 2D supervision. However, existing approaches still requires expensive per-scene optimization that prohibits generalization to novel scenes during inference. To circumvent this problem, we introduce a _generalizable_ 3D segmentation framework based on implicit representation. Specifically, our framework takes in multi-view image features and semantic maps as the inputs instead of only spatial information to avoid overfitting to scene-specific geometric and semantic information. We propose a novel soft voting mechanism to aggregate the 2D semantic information from different views for each 3D point. In addition to the image features, view difference information is also encoded in our framework to predict the voting scores. Intuitively, this allows the semantic information from nearby views to contribute more compared to distant ones. Furthermore, a visibility module is also designed to detect and filter out detrimental information from occluded views. Due to the generalizability of our proposed method, we can synthesize semantic maps or conduct 3D semantic segmentation for novel scenes with solely 2D semantic supervision. Experimental results show that our approach achieves comparable performance with scene-specific approaches. More importantly, our approach can even outperform existing strong supervision-based approaches with only 2D annotations. Our source code is available at: https://github.com/HLinChen/GNeSF.

## 1 Introduction

Understanding high-level semantics of 3D scenes in digital images and videos is a fundamental research in computer vision. Several extensively studied tasks such as scene classification , object detection , and semantic segmentation  facilitate the extraction of semantic descriptions from RGB and other sensor data. These tasks form the foundation for applications such as visual navigation  and robotic interaction .

Several commonly used 3D representations include 3D point clouds [11; 32], voxel grids , and polygonal meshes [28; 18]. A direct learning of semantics on these 3D representations require large amounts of well-annotated 3D ground truths for training. Unfortunately, these 3D ground truths are often much more laborious and expensive to obtain compared to its 2D counterparts. Recently, Neural Radiance Field (NeRF) [30; 52] has emerged as a new 3D representation, which can capture intricate geometric details  using solely RGB images. Many researchers utilize this 3D representation to propose the concept of Neural Semantic Field [55; 17; 3], where semantics is assigned to each 3D point. However, these existing works on neural semantic fields inherited the per-scene optimization limitation of the vanilla NeRF and thus cannot generalize across novel scenes. As a result, the practicality of the approaches for semantic segmentation is severely restricted.

In view of the limitations in existing neural semantic field works, this paper addresses the challenging task of generalizable neural semantic fields. The objective is to construct a generalizable semantic field capable of producing high-quality semantic rendering from novel views and 3D semanticsegmentation using input images from unseen scenes. The generalization ability offers numerous interactive applications that provide valuable feedback to users during 3D capture without the requirement for retraining. Recently, NeSF  claims the ability to generate 3D semantic fields for novel scenes by utilizing 3D UNet  to extract semantic features from a density field obtained from a pre-trained NeRF model. However, this method lacks efficiency as it requires training a separate NeRF model for each scene before segmenting the 3D scene. Furthermore, it lacks sufficient generalizability to new scenes, thus making it suitable only for offline applications. Recently, several researchers propose generalizable NeRF methods [6; 45; 53] that demonstrate effective generalization across scenes for reconstructing radiance fields without the need for laborious per-scene optimization. A naive approach for generalizable Neural Semantic Field (NeSF) is to incorporate an additional semantic head to the generalizable NeRF framework. However, this method does not perform well for complex and realistic indoor scenes, as demonstrated in Tab. 1 and . One possible reason is that semantics represents high-level information, and a simple head without direct 3D semantic supervision is insufficient for generating 3D semantic fields.

Inspired by image-based rendering (IBR) [9; 45], we propose a soft voting scheme (shown in Fig. 2a) to aggregate the 2D semantic information from different views for each 3D point for learning a Generalizable Neural Semantic Field (GNeSF). Specifically, we use a pre-trained 2D semantic segmentation model to generate 2D semantic maps for the source views and then leverage the warping, resampling, and/or blending operations of IBR to infer the semantics of a sample 3D point. Considering the unbounded nature and the lack of meaning for logit values across multiple views, we utilize a probability distribution instead of semantic logits to represent semantics across views. Our framework utilizes multi-view image features instead of position information as input to further enhance the generalizability of our model and prevent overfitting to scene-specific geometric and semantic information. In addition to the extracted image features, our framework incorporates view difference information to include the prior of higher significance for nearby views when generating the voting scores for blending source views. We then introduce a soft voting scheme to compute the semantics of each sample point as the weighted combination of the projected semantics from the source views.

Geometric information is predicted to perform volumetric rendering after obtaining semantics of 3D points along sampled ray. Specifically, we first build a feature volume by projecting each vertex of the volume to latent 2D features and aggregating these 2D features to get a density 3D feature. This feature volume is then fed into a 3D neural network to obtain a geometry encoding volume. For a sampled 3D point, we use an MLP network to infer the specific geometric representations, e.g., density or Signed Distance Function (SDF), from the interpolated features of the geometry encoding volume.

Weights of points along the ray are obtained from the predicted geometric representations. A final semantic value is computed for each ray through volume rendering by weighted summing semantics alone the ray. Due to the generalizability of our proposed method, we can synthesize semantic maps or conduct 3D semantic segmentation for novel scenes with solely 2D semantic supervision. To facilitate 3D semantic segmentation, we design a visibility module (shown in Fig. 2b) that identifies and removes occluded views for each spatial point. The remaining semantics from the visible views are then utilized to vote on the semantics of the respective point. In summary, our contributions include:

* We are the first to tackle the task of _generalizable_ neural semantic field. Our proposed GNeSF is capable of inferring novel view synthesis of semantic maps or 3D semantic segmentation of novel scenes without retraining. Furthermore, our method depends solely on 2D semantic supervision during training.
* We propose a novel soft voting scheme that determines the semantics for each 3D point by considering the 2D semantics from multiple source views. A visibility module is also designed to detect and filter out detrimental information from occluded views.
* Our proposed GNeSF enables synthesis of semantic maps for both novel views and novel scenes, and achieves comparable performance to per-scene optimized methods. The results on ScanNet and Replica demonstrate comparable or superior performance compared to existing methods utilizing strong 3D supervision.

Related Work

### 2D & 3D Semantic Segmentation

Traditionally, semantic segmentation has been treated as a per-pixel classification task, where FCN-based architectures  assign category labels to individual pixels independently. Subsequent methods have acknowledged the importance of context in achieving accurate per-pixel classification and have concentrated on developing specialized context modules [7; 8; 54] or incorporating self-attention variations [39; 46; 10]. In our work, we utilize the state-of-the-art Mask2Former  with a Swin-Transformer  backbone as our pre-trained 2D semantic segmenter. 3D semantic segmentation also has been investigated with pre-computed 3D structures such as voxel grids or point clouds [19; 32; 33; 37; 12; 14; 35], and simultaneous segmentation and 3D reconstruction from 2D images [31; 29]. Unlike these methods, our GNeSF aims to segment a dense 3D representation using only 2D inputs and semantic supervision without the need for 3D semantic annotations.

### Neural Semantic Fields

Semantic-NeRF  is first to introduce the integration of semantics into NeRF. It showcases the fusion of noisy 2D semantic segmentations into a coherent volumetric model. This integration enhanced the accuracy of the model and enabled the synthesis of novel views with semantic masks. Subsequently, numerous studies have built upon this concept. For example, [17; 23; 21] incorporate instance modeling, and  encode abstract visual features that allow for post hoc derivation of semantic segmentation. NeRF-SOS  combines object segmentation and neural radiance fields to segment objects in complex real-world scenes using self-supervised learning.  proposes an object-compositional neural radiance field. RFP  designs an unsupervised approach to segment objects in 3D during reconstruction using only unlabeled multi-view images of a scene. Panoptic NeRF  and DM-NeRF  are specifically designed for panoptic radiance fields for tasks such as label transfer and scene editing, respectively. However, these methods require scene-specific optimization and thus not generalizable to unseen scenes. Similar to our objective, NeSF  addresses the challenge of generalization to unseen scenes. Nonetheless, NeSF relies on multiple pre-trained and scene-specific optimized NeRF models for density field generation and segmentation which restricts its overall generalizability. In contrast, our GNeSF is able to infer on novel scenes directly without any further optimization.

### Generalizable Neural Implicit Representation

Significant advancements have been achieved in the field of neural implicit representation [30; 48; 52]. However, these approaches heavily depend on computationally expensive per-scene optimization. To address this limitation, several new neural implicit representation methods emphasizing on generalization has emerged. These methods [6; 45; 49; 51; 5] aim to learn a representation of the radiance field from a provided set of images of novel scenes. This enables the synthesis of novel views without requiring per-scene optimization. PixelNeRF  and IBRNet  utilize volume rendering techniques to generate novel view images by employing warped features from nearby reference images. In addition to novel synthesis, MonoNeuralFusion  introduces a novel neural implicit scene representation with volume rendering for high-fidelity generalizable 3D scene reconstruction from monocular videos. Inspired by these generalizable methods, we takes in multi-view image feature as the input instead of the position information to enhance the generalizability of our model. Semantic knowledge obtained from 2D vision is then aggregated to infer 3D semantics by the proposed soft voting scheme.

## 3 Our GNeSF: Generalizable Neural Semantic Fields

We propose GNeSF to achieve generalizable semantic segmentation on neural implicit representation. As illustrated in Fig. 1, our GNeSF consists of three main components: 1) Feature extraction and semantic prediction from multiple source views; 2) Geometry and semantics prediction for sampled 3D points; 3) Semantic map prediction with volume rendering.

### Source-View Image Features and Semantics Map

Inspired by existing generalizable neural implicit representations [6; 45], our framework leverages image features from source-view images for generalization to unseen scenes during inference. We extract an image feature map \(_{i}^{H_{i} W_{i} d}\) for each source image \(_{i}^{H_{i} W_{i} 3}\) with a U-Net-based convolutional neural network, where \(H_{i} W_{i}\) and \(d\) denotes the dimensions of the image and the feature maps, respectively. Concurrently, we use a pre-trained mask2former  to predict the semantic maps \(_{i}^{H_{i} W_{i} c}\) of each source image \(_{i}\), where \(c\) represents the number of categories.

### 3D Point Geometry and Semantics Predictions

Using the source-view image features and semantics maps, we predict the geometry \(()\) and semantics \(()\) for each sampled 3D point \(^{3}\). We first build a geometry encoding volume \(\) based on the image feature map \(_{i}\) and then predict the geometry \(()\) using an MLP. Concurrently, we design a soft voting scheme (shown in Fig. 2a) based on the 2D semantic observations \(_{i}^{H_{i} W_{i} c}\) to predict the semantics \(()\).

#### 3.2.1 Geometry Encoding Volume

To construct the geometry encoding volume \(\), we project each vertex \(^{3}\) of the volume to the image feature maps \(_{i}\) and obtain its image features \(_{i}(_{i}())\) by interpolation, where \(_{i}()\) denotes the projected pixel location of \(\) on the feature map \(_{i}\). For simplicity, we abbreviate \(_{i}(_{i}())\) as \(_{i}()\). We then compute the mean \(^{d}\) and variance \(^{d}\) of the projected image features \(\{_{i}()\}_{i=1}^{N}\) from the \(N\) source views to capture the global information. The mean \(\) and variance \(\) are concatenated to build a feature volume \(\). Intuitively, the mean image feature provides cues for the appearance information, and the variance for geometry reasoning. Finally, we use a 3D neural network \(H_{g}\) to aggregate the feature volume \(\) to obtain the geometry encoding volume \(\). Formally, the whole process can be expressed by:

\[()=(\{_{i}()\}_{i=1 }^{N}),()=(\{_{i}()\}_ {i=1}^{N}),\] (1a) \[()=[(),()], =H_{g}(),\] (1b)

where \([,]\) represents feature concatenation, and \(()\) and \(()\) are the averaging and variance operations. For a sampled 3D point \(\), we use an MLP network \(f_{}\) to predict the geometry \(()\) based on the interpolated features of the geometry encoding volume \(()\), _i.e._:

\[()=f_{}(()).\] (2)

Figure 1: The GNeSF framework comprises three key components: 1) Extraction of features and semantic prediction from multiple source views, shown in the blue dashed box; 2) Prediction of geometry and semantics for sampled 3D points, shown in the yellow dashed box; 3) Prediction of semantic maps using volume rendering, shown in the red dashed box. Here, “MVS” means using Multi-View Stereo to build the feature volume.

Note that geometry \(()\), e.g., volume density or Signed Distance Function (SDF) is represented differently in different neural implicit representations. More details are provided in Sec. 4.

#### 3.2.2 Semantics Soft Voting

Given a query point location \(^{3}\) along a ray \(^{3}\), we project \(\) onto \(N\) nearby source views using the respective camera parameters. Image features \(\{_{i}()\}_{i=1}^{N}^{d}\) and semantics maps \(\{_{i}()\}_{i=1}^{N}^{k}\) are then sampled at the projected pixel locations over the \(N\) source views through bilinear interpolation on the image feature \(_{i}\) and semantic maps \(_{i}\), respectively. We propose a soft voting scheme to predict the semantics \(()\) of a sampled 3D point \(\) based on the 2D projected image features \(\{_{i}()\}_{i=1}^{N}\) and semantics maps \(\{_{i}()\}_{i=1}^{N}\). Specifically, the 2D projected image features \(\{_{i}()\}_{i=1}^{N}\) are used to predict voting weights \(\{v_{i}\}_{i=1}^{N}\) for the corresponding semantics maps \(\{_{i}()\}_{i=1}^{N}\) in the source views.

We first calculate the mean and variance of the image features \(\{_{i}()\}_{i=1}^{N}\) from the \(N\) source views to capture the global consistency information. Each image feature \(_{i}()\) is concatenated with the mean and variance together, and then fed into a tiny MLP network to generate a new feature \(_{i}^{}()\). Subsequently, we feed the new feature \(_{i}^{}()\), the displacement \(_{i}=-_{i}\) between the 3D point \(\) and the camera origin \(_{i}\) of the corresponding source view, and the trilinearly interpolated volume encoding feature \(()\) into an MLP network \(f_{v}\) to generate voting weights \(v_{i}()\). In contrast to view-dependent color, we use only displacement for semantics since it remains consistent regardless of the viewing direction.

Furthermore, we encode the displacement information \(_{i}=-_{i}\) for voting weights prediction since nearby views should contribute more compared to the distant ones. The voting weight \(v_{i}()\) is predicted as:

\[v_{i}()=f_{v}(_{i}^{}(),( ),_{i}).\] (3)

The final semantics of the 3D point is obtained through a soft voting mechanism, given by:

\[()=_{i=1}^{N}(_{i}() (v_{i}())/_{j=1}^{N}(v_{j}() )),\] (4)

where \((.)\) represents the exponential function.

Remarks.An alternative approach would be to directly regress \(()\) instead of predicting the voting weights. Compared to this alternative approach, predicting the voting weights makes it easier for the network to leverage the source-view semantics maps which are easier to obtain without direct 3D supervision. Our conjecture is verified experimentally by the degraded performance when we directly regress the semantics.

Figure 2: (a) The soft voting scheme aggregates the 2D semantic information from different views to infer semantics of each 3D point. (b) The visibility module detects and eliminates occluded views for each spatial point. For example, \(_{1}\) is visible in the 2D semantic observation because the SDF values along the ray from the source view to the target point transits from positive to negative. On the other hand, \(_{2}\) and \(_{3}\) are occluded in the 2D semantic observation because the value changes from positive to negative and then back to positive.

### Semantics Rendering and Training

Rendering.The method presented in the previous section enables the computation of semantics \(()\) and geometry \(()\), e.g., volume density or SDF in the continuous 3D space. The volume rendering of the semantics \(}()\) along a ray \(\) traversing the scene is approximated by the weighted sum of the semantics from the \(M\) samples along the ray, _i.e._:

\[}()=_{m=1}^{M}w(_{m})( _{m}),\] (5)

where the samples from \(1\) to \(M\) are sorted in ascending order based on their depth values. \(_{m}\) represents the query point of the \(m\)-th sample along the ray \(\). \((_{m})\) and \(w(_{m})\) represent the semantics and the corresponding weight of the query sample \(_{m}\), respectively. The computation of the weight \(w(_{m})\) depends on the task of view synthesis or 3D segmentation (_c.f._ Sec. 4 for details).

Training objective.To train the model, we minimize the cross-entropy error between the rendered semantics \(}()\) and the corresponding ground truth pixel semantics \(()\).:

\[_{s}=-_{ R}() }(),\] (6)

where \(R\) represents the set of rays included in each training batch.

## 4 Semantic View Synthesis and 3D Segmentation

Our proposed generalizable neural semantic field can be applied to the tasks of semantic view synthesis and 3D semantic segmentation.

### Semantic View Synthesis

Semantic view synthesis aims to render the semantic maps for novel views. A naive approach for synthesizing novel semantic views first uses a generalizable NeRF to generate images of the novel views and then followed by applying 2D semantic segmentation to these synthesized images using a separate 2D semantic segmentor. In contrast to this two-stage method, our approach is a one-step synthesis that is more efficient and streamlined. In semantic view synthesis, we use the predicted geometry \((_{m})\) from Eq. 2 as the volume density. We then follow NeRF  to compute the weight \(w(_{m})\) used in Eq. 5 by:

\[w(_{m})=T(_{m})(1-(-(_{m})) ),\;\;T(_{m})=(-_{j=1}^{m-1}( _{m}))\] (7)

is the accumulated transmittance along the ray. Once the weights \(w(_{m})\) are computed, we proceed to construct and train a generalizable neural semantic field based on the approach proposed in Sec. 3.3 for rendering semantic views.

### 3D Semantic Segmentation

The goal of 3D semantic segmentation is to predict the semantic of each sample point in the 3D space. Consequently, the rendering of the semantic map is dependent on high-quality surfaces. To this end, we predict the geometry \((_{m})\) as a signed distance field (SDF) based on Eq. 2. Following , we then compute the weights \(w(_{m})\) by:

\[w(_{m})=(_{m})}{tr} )(-_{m})}{tr}),\] (8)

where \(tr\) represents the truncation distance. We set the weights of samples beyond the first truncation region to zero to account for possible multiple intersections. Subsequently, the normalized weight \(w()\) used in Eq. 5 is calculated as:

\[w(_{m})=_{m})}{_{j=1}^{M}w(_{j})}.\] (9)In this scheme, the highest integration weight is assigned to surface points, while the weights for the points that are farther away are reduced.

Visibility module.While most nearby views offer valuable semantic observations for a 3D point \(\), there exists some views that contribute negatively due to occlusion. The most straightforward approach to address occlusion is to render the depth map in the same manner as color or semantics. However, this method incurs additional computational cost. To address occlusion efficiently, we leverage the property of signed distance function (SDF): the SDF value is positive in front of the surface and negative behind it. This implies that if a source view can observe a target 3D point, the SDF values of the points along the ray transit from positive to negative at the origin of the source view to the target point (_c.f._\(_{1}\) in Fig.2b). However, in the presence of occlusion, the value changes from positive to negative and then back to positive (_c.f._\(_{2}\) and \(_{3}\) in Fig.2b). We use this property to mask out occluded views and mitigate the influence of occlusion.

## 5 Experiments

We evaluate our method on the tasks of semantic view synthesis in Sec. 5.1.1 and 3D semantic segmentation in Sec. 5.1.2. Additionally, we validate the effectiveness of the proposed modules in Sec. 5.2.

**Dataset and Metrics.** We perform the experiments on two indoor datasets: ScanNet (V2)  and Replica . The ScanNet dataset contains \(1,613\) indoor scenes with ground-truth camera poses, surface reconstructions, and semantic segmentation labels. We follow  and  on the three training/validation/test splits commonly used in previous works. The 2D semantic segmentor Mask2Former is pre-trained on the train split of ScanNet without any additional amendment. Furthermore, we follow the \(20\)-class semantic segmentation task defined in the original ScanNet benchmark. In contrast to NeRF-based methods [53; 56] which only use the training subset of ScanNet to train and test their methods, we train and test our method on the complete dataset. Replica consists of \(16\) high-quality 3D scenes of apartments, rooms, and offices. We use the same \(8\) scenes as in . While Replica originally features \(100\) classes, we rearranged them into \(19\) frequently observed semantic classes. We use the model trained on ScanNet to perform the validation on Replica. For the evaluation metrics, we use the mean of class-wise intersection over union (mIoU) and the average pixel accuracy (mAcc). For 3D semantic segmentation, we follow the evaluation procedure of Atlas .

### Comparisons to Baselines

#### 5.1.1 Semantic View Synthesis

We compare with state-of-the-art per-scene optimized neural semantic field methods: Semantic-NeRF , Panoptic Neural Fields (PNF)  and DM-NeRF . For each per-scene optimized method, we train a NeRF model on each scene from the train set of ScanNet, and then compute the 2D mIoU

    &  &  &  \\   & & mIoU (\%) & mAcc (\%) & mIoU (\%) & mAcc (\%) \\   & Semantic-NeRF  & **96.8** & **98.2** & - & - \\  & DM-NeRF  & 93.5 & 96.1 & - & - \\  & PNF  & 94.1 & 97.2 & - & - \\   & Mask2Former  & 80.9 & 90.7 & 64.6 & 77.9 \\  & Predict Directly & 50.2 & 62.3 & 39.1 & 54.2 \\   & Semantic-Ray  & - & - & 56.0 & - \\   & Ours-100 & **93.3** & **96.3** & 47 & 60.8 \\   & Ours-1000 & 87.8 & 93.3 & **71.6** & **82.3** \\   

Table 1: Quantitative comparison on semantic view synthesis from the validation set of ScanNet. ‘Ours-\(100\)’ means the model trained on \(100\) scenes and ‘Ours-\(1000\)’ trained on full training set of ScanNet. Semantic-Ray is also trained on full training set of ScanNet. ‘Predict Directly’ represents the method that incorporates a semantic head into the generalizable NeRF framework.

across these scenes. Note that although PNF and DM-NeRF aim to predict panoptic segmentation, they can still predict semantic segmentation. We train our method for different number of scenes, including \(100\) scenes (denoted as Ours-100) and full training set (denoted as Ours-1000), to show the capacity of our model over different scenarios. For generalizable methods, we compare with Mask2Former, Semantic-Ray  and the method adding a semantic head to a generalizable NeRF (denoted as Predict Directly) in the same training and testing setting. For Mask2Former, IBRNet  is first used to render color images on novel views, and then Mask2Former is used to predict semantic results on rendered images. For Predict Directly, we use IBRNet as its generalizable NeRF framework.

As shown in Tab. 1, we outperform the baselines across ScanNet on semantic segmentation tasks. Compared with per-scene optimized methods, our method can obtain comparable performance (\(93.3\%\) v.s. \(93.5\%\) 2D mIoU) on novel views. Moreover, only our methods can be generalized to novel scenes (\(47\%\) 2D mIoU). Although Semantic-NeRF performs better than our method (\(+3.5\%\)), it requires per-scene optimization and thus not generalizable to unseen scenes. Comparing with other generalizable methods, our method obtain the state-of-the-art performance. For example, our method significantly improves over Mask2Former by \(7\%\), which shows our method is better and more concise. Furthermore, our improves significantly (\(71.6\%\) v.s. \(56\%\)) compared with Semantic-Ray. In addition, our method has improved tremendously (\(71.6\%\) v.s. \(39.1\%\)) over the naive approach that incorporates a semantic head into the generalizable NeRF framework. This validates the effectiveness of our soft voting scheme.

#### 5.1.2 3D semantic segmentation

We evaluate 3D semantic segmentation on the val and test set of ScanNet. We compare our method with approaches trained on 3D semantic supervision, _i.e._, Atlas , NeuralRecon , Joint Recon-Segment , PointConv , and PointNet++ , and approaches trained on 2D semantic supervision, _i.e._, S4R , and ScanComplete . The input of these methods are different: PointConv , PointNet++ , S4R , and ScanComplete  use 3D geometry, e.g., point cloud, 3D mesh, etc, as input; Others, _i.e._, Atlasm NeuralRecon, and Joint Recon-Segment  use RGB images as input to predict 3D surfaces and use an extra semantic head to predict semantics. In Tab. 2, we use 'Geometry' to denote 3D geometry as input and 'Sup.' to represent supervision.

As shown in Tab. 2, our proposed method achieves comparable performance to the methods with 3D semantic supervision, and significantly outperforms all methods with 2D semantic supervision. For example, our approach performs significantly better than all other methods using RGB images as

    & Val Split (mIoU) & Test Split (mIoU) & Geometry \\   & Atlas  & 36.8 & 34 & ✗ \\  & NeuralRecon  & 37.5 & 35.1 & ✗ \\  & Joint Recon-Segment  & - & **51.5** & ✗ \\  & PointNet++  & 53.5 & 33.9 & ✓ \\  & PointConv  & **61** & 55.6 & ✓ \\   & S4R  & 36.9 & - & ✓ \\  & ScanComplete  & 29.5 & - & ✓ \\   & Ours & 58.8 & 55.1 & ✗ \\   & Ours with Mesh & **60.4** & **55.8** & ✓ \\   

Table 2: Quantitative comparison on 3D semantic segmentation from the val and test set of ScanNet. Note that ‘Geometry’ means using 3D geometry as input and ‘Sup.’ represents supervision.

   Method & Atlas & NeuralRecon & Ours \\ 
3D mIoU & 25.4 & 26.2 & 43.8 \\   

Table 3: Quantitative comparison on Replica.

input. The highest mIoU of the previously best method that predicts 3D surface on the ScanNet test set is \(51.5\%\), which is significantly less than our results of \(55.1\%\). More importantly, they need 3D semantic supervision while we only require 2D semantic supervision. Our approach still performs better than some of the methods (\(58.8\%\) v.s. \(53.5\%\)) with 3D supervision and 3D geometry as input. Our method is comparable to PointConv which uses 3D Geometry as input (\(60.4\%\) v.s. \(61\%\)) when we replace SDF prediction with ground truth. Similar to our method, S4R leverages differentiable rendering as training supervision. However, S4R incorporates a semantic head into the existing NeRF framework and uses 3D geometry as input. Our approach outperforming S4R (\(36.9\%\) v.s. \(58.8\%\) ) proves the effectiveness of our proposed soft voting scheme. We also show qualitative comparison with NeuralRecon in Fig. 3. We can see that NeuralRecon wrongly classifies the bed as table (highlight in black box), while our approach makes the correct prediction. Tab. 3 shows the performance of our method compared with Altas  and NeuralRecon  that also use color images as input when trained on ScanNet and test on Replica. We can see that our method shows much stronger ability (\(43.8\%\) v.s. \(25.4\%\)/\(26.2\%\)) for cross-dataset generalization.

### Ablation Studies

As shown in Tab. 4, we do ablation study to investigate the contribution of each component proposed in our method on ScanNet. We start with a baseline method predicting the semantics directly of each 3D query point. Next, we use only semantic logits to vote for semantics of each 3D query point. We then performed a series of tests where we include each component of our framework one-by-one and measured the impact on the performance. The third row shows the impact of using a probability distribution instead of semantic logits, and the fourth shows the impact of adding the visibility module. We find that each component improves the 3D segmentation IoU performance.

  Predict Directly & Logit & Prob. & Visibility Module & mIoU & mAcc \\  \(\) & \(\) & \(\) & \(\) & 37.5 (+0) & 47.0 (+0) \\ \(\) & \(\) & \(\) & \(\) & 57.3 (+19.8) & 70.9 (+23.9) \\ \(\) & \(\) & \(\) & \(\) & 57.9 (+0.6) & 71.2 (+0.3) \\ \(\) & \(\) & \(\) & \(\) & 58.8 (+0.9) & 72.1 (+0.9) \\  

Table 4: Ablations of our design choices on 3D semantic segmentation.

Figure 3: Qualitative 3D semantic segmentation results on ScanNet. As we can see from the figure, our method is better than NeuralRecon, especially on the ‘bed’ class. In detail, NeuralRecon mistakenly classifies the bed as a table (highlight in black box).

Conclusion

In this paper, we introduce a _generalizable_ 3D segmentation framework based on implicit representation. Specifically, we propose a novel soft voting mechanism on 2D semantic information from multiple viewpoints to predict the semantics for each 3D point. Instead of using positional information, our framework utilizes multi-view image features as input and thus avoids the need to memorize scene-specific geometric and semantic details. Additionally, our framework encodes view differences to give higher importance to nearby views compared to distant ones when predicting the voting scores. We also design a visibility module to identify and discard detrimental information from occluded views. By incorporating view difference encoding and visibility prediction, our framework achieves more effective aggregation of the 2D semantic information. Experimental results demonstrate that our approach performs comparably to scene-specific methods. More importantly, our approach even surpasses existing strong supervision-based approaches with just 2D annotations.