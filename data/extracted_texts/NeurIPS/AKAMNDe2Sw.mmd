# Boosting Adversarial Transferability by

Achieving Flat Local Maxima

 Zhijin Ge1, Hongying Liu2, Xiaosen Wang3, Fanhua Shang4, Yuanyuan Liu1

Equal Contribution.Corresponding authors

###### Abstract

Transfer-based attack adopts the adversarial examples generated on the surrogate model to attack various models, making it applicable in the physical world and attracting increasing interest. Recently, various adversarial attacks have emerged to boost adversarial transferability from different perspectives. In this work, inspired by the observation that flat local minima are correlated with good generalization, we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability by introducing a penalized gradient norm to the original loss function. Since directly optimizing the gradient regularization norm is computationally expensive and intractable for generating adversarial examples, we propose an approximation optimization method to simplify the gradient update of the objective function. Specifically, we randomly sample an example and adopt a first-order procedure to approximate the Hessian/vector product, which makes computing more efficient by interpolating two neighboring gradients. Meanwhile, in order to obtain a more stable gradient direction, we randomly sample multiple examples and average the gradients of these examples to reduce the variance due to random sampling during the iterative process. Extensive experimental results on the ImageNet-compatible dataset show that the proposed method can generate adversarial examples at flat local regions, and significantly improve the adversarial transferability on either normally trained models or adversarially trained models than the state-of-the-art attacks. Our codes are available at: https://github.com/Trustworthy-AI-Group/PGN.

## 1 Introduction

A great number of works have shown that Deep Neural Networks (DNNs) are vulnerable to adversarial examples [10; 16; 38; 26; 50], which are generated by applying human-imperceptible perturbations on clean input to result in misclassification. Furthermore, adversarial examples have an intriguing property of transferability [10; 11; 31; 61; 59; 57], _i.e._, the adversarial example generated from the surrogate model can also misclass other models. The existence of transferability makes adversarial attacks practical to real-world applications because hackers do not need to know any information about the target model, which introduces a series of serious security problems in security-sensitive applications such as self-driving [65; 13] and face-recognition [44; 30].

Although several attack methods [5; 16; 26; 28] have exhibited great attack effectiveness in the white-box setting, they have low transferability when attacking black-box models, especially for some advanced defense models [36; 47]. Previous works [10; 31; 61] attribute that the reason for adversarial examples shows weak transferability due to dropping into poor local maxima or overfitting the surrogate model, which is not likely to transfer across models. To address this issue, many methods have been proposed from different perspectives. Gradient optimization attacks [10; 31; 49; 64] attempt to boost black-box performance by advanced gradient calculation. Input transformation attacks [61; 11; 31; 51; 35; 15; 55] aim to generate adversarial examples with higher transferability by applying various transformations to the inputs. Especially, the above methods are mainly proposed from the perspective of optimization and generalization, which regard the process of generating adversarial examples on the white-box model as a standard neural network training process, and treat adversarial transferability as equivalent to model generalization . Although these methods can improve the transferability of adversarial examples, there are still some gaps between white-box attacks and transfer-based black-box attacks.

Inspired by the observation that flat minima often result in better model generalization [24; 66; 41; 14], we assume and empirically validate that adversarial examples at a flat local region tend to have good transferability. Intuitively, we can achieve the flat local maxima for adversarial examples using a gradient regularization norm but it is computationally extensive to solve such a problem. To address this issue, we theoretically analyze the optimization process and propose a novel attack called Penalizing Gradient Norm (PGN). In particular, PGN approximates the Hessian/vector product by interpolating the first-order gradients of two samples. This approximation allows us to efficiently generate adversarial examples at flat local regions. To eliminate the error introduced by approximation, PGN incorporates the average gradient of several randomly sampled data points to update the adversarial perturbation. Our main contributions can be summarized as follows:

* To the best of our knowledge, it is the first work that empirically validates that adversarial examples located in flat regions have good transferability.
* We propose a novel attack called Penalizing Gradient Norm (PGN), which can effectively generate adversarial examples at flat local regions with better transferability.
* Empirical evaluations show that PGN can significantly improve the attack transferability on both normally trained models and adversarially trained models, which can also be seamlessly combined with various previous attack methods for higher transferability.

## 2 Related Work

In this section, we provide a brief overview of the adversarial attack methods and introduce several studies on the flat local minima for model generalization.

### Adversarial Attacks

In general, adversarial attacks can be divided into two categories, _i.e_., white-box attacks and black-box attacks. In the white-box setting, the target model is completely exposed to the attacker. For instance, Goodfellow _et al_.  proposed the Fast Gradient Sign Method (FGSM) to generate adversarial examples with one step of gradient update. Kurakin _et al_.  further extends FGSM to an iterative version with a smaller step size \(\), denoted as I-FGSM. Madry _et al_.  extends I-FGSM with a random start to generate diverse adversarial examples. Existing white-box attacks have achieved superior performance with the knowledge of the target model. On the other hand, black-box attacks are more practical since they only access limited or no information about the target model. There are two types of black-box adversarial attacks : query-based and transfer-based attacks. Query-based attacks [3; 6; 53] often take hundreds or even thousands of quires to generate adversarial examples, making them inefficient in the physical world. In contrast, transfer-based attacks [61; 49; 52; 4; 54; 56; 48] generate adversarial examples on the surrogate model, which can also attack other models without accessing the target model, leading to great practical applicability and attracting increasing attention.

Unfortunately, adversarial examples crafted by white-box attacks generally exhibit limited transferability. To boost adversarial transferability, various methods are proposed from the perspective of optimization and generalization. MI-FGSM  integrates momentum into I-FGSM to stabilize the update direction and escape from poor local maxima at each iteration. NI-FGSM  adoptsNesterov's accelerated gradient  to further enhance the transferability. Wang _et al_.  tunned the gradient using the gradient variance of previous iteration to find a more stable gradient direction. Wang _et al_.  enhanced the momentum by accumulating the gradient of several data points in the direction of the previous gradient for better transferability.

Data augmentation, which has shown high effectiveness in improving model generalization [37; 63; 2; 8], has been widely studied to boost adversarial transferability. Xie _et al_.  adopted diverse input patterns by randomly resizing and padding to generate transferable adversarial examples. Dong _et al_.  utilized several translated images to optimize the adversarial perturbations, and further calculated the gradients by convolving the gradient at untranslated images with a kernel matrix for high efficiency. SIM  optimizes the adversarial perturbations over several scaled copies of the input images. Admix  mixes up a set of images randomly sampled from other categories while maintaining the original label of the input. Spectrum simulation attack (SSA)  transforms the input image in the frequency domain to craft more transferable adversarial examples.

Besides, some methods improve adversarial transferability from different perspectives. For instance, Liu _et al_.  proposed an ensemble attack, which simultaneously attacks multiple surrogate models. Wu _et al_.  employed an adversarial transformation network that can capture the most harmful deformations to adversarial noises. Qin _et al_.  injected the reverse adversarial perturbation at each step of the optimization procedure for better transferability.

### Flat Minima

After Hochreiter _et al_.  pointed out that well-generalized models may have flat minima, the connection between the flatness of minima and model generalization has been studied from both empirical and theoretical perspectives [24; 41; 14; 66]. Li _et al_.  observed that skip connections promote flat minima, which helps explain why skip connections are necessary for training extremely deep networks. Similarly, Santurkar _et al_.  found that BatchNorm makes the optimization landscape significantly smooth in the training process. Sharpness-Aware Minimization (SAM)  improves model generalization by simultaneously minimizing the loss value and sharpness, which seeks the parameters in the neighborhoods with uniformly low loss values. Jiang _et al_.  studied 40 complexity measures and showed that a sharpness-based measure has the highest correlation with generalization. Zhao _et al_.  demonstrated that adding a gradient norm of the loss function can help the optimizer find flat local minima.

## 3 Methodology

### Preliminaries

Given an input image \(x\) with its corresponding ground-true label \(y\), the deep model \(f\) with parameter \(\) is expected to output the prediction \(f(x;)=y\) with high probability. Let \(_{}(x)=\{x^{}:x^{}-x_{p}\}\) be an \(\)-ball of an input image \(x\), where \(>0\) is a pre-defined perturbation magnitude, and \(\|_{p}\) denotes the \(L_{p}\)-norm (e.g, the \(L_{1}\)-norm). The attacker aims to find an example \(x^{adv}_{}(x)\) that misleads the classifier \(f(x^{adv};) y\). Let \(J(x,y;)\) be the loss function (_e.g._, cross-entropy loss) of the classifier \(f\). Existing white-box attacks such as [16; 26; 10] usually generate adversarial examples by solving the following maximization problem:

\[_{x^{adv}_{}(x)}J(x^{adv},y;).\] (1)

These attack methods mainly generate adversarial examples through gradient iterations. For instance, I-FGSM  iteratively updates the adversarial perturbation as follows:

\[x^{adv}_{t+1}=_{_{}(x)}x^{adv}_{t}+ (_{x^{adv}_{t}}J(x^{adv}_{t},y;)),\ \ x^{adv}_{0}=x,\] (2)

where \(_{_{}(x)}()\) projects an input into \(_{}(x)\), \(=/T\), and \(T\) is the total number of iterations. For black-box attacks, the gradient is not accessible so that we cannot directly solve Problem (1) like I-FGSM. To address such a issue, transfer-based attacks generate adversarial examples on an accessible surrogate model, which can be transferred to fool the target models.

### Flat Local Maxima Tend to Improve Adversarial Transferability

In general, the adversarial transferability is equivalent to model generalization if we analogize the optimization of perturbation with the model training process . Existing works  have demonstrated that flat local minima tend to generalize better than their sharp counterparts. This inspires us to assume that adversarial examples at a flat local region w.r.t. the loss function tend to have better transferability across various models. A rigorous description is given as follows:

**Assumption 1**.: _Given any small radius \(>0\) for the local region and two adversarial examples \(x_{1}^{adv}\) and \(x_{2}^{adv}\) for the same input image \(x\), if \(_{x^{}_{}(x_{1}^{adv})}\|_{x^{}}J(x ^{},y;)\|_{2}<_{x^{}_{}(x_{2}^{adv})} \|_{x^{}}J(x^{},y;)\|_{2}\), with high probability, \(x_{1}^{adv}\) tends to be more transferable than \(x_{2}^{adv}\) across various models._

SAM  has indicated that a flat local minimum is an entire neighborhood having both low loss and low curvature. Zhao _et al_.  also demonstrated that if the loss function has a smaller gradient value, this would indicate that the loss function landscape is flatter. Here we adopt the maximum gradient in the neighborhood to evaluate the flatness of the local region. Following Assumption 1, we introduce a regularizer, which minimizes the maximum gradient in the \(\)-ball of the input, into Eq. (1) as follows:

\[_{x^{adv}_{}(x)}[J(x^{adv},y;)- _{x^{}_{}(x^{adv})}\|_{x^{}}J( x^{},y;)\|_{2}],\] (3)

where \( 0\) is the penalty coefficient. During the optimization process, we penalize the maximum gradient to perceive the sharper regions that can not be identified by using the averaged gradients. By optimizing Problem (3), we can find that the adversarial example with a small gradient of data points in its neighborhood should be at a flat local region. However, it is impractical to calculate the maximum gradient in the \(\)-ball. Hence, we approximately optimize Eq. (3) by randomly sampling an example \(x^{}_{}(x^{adv})\) at each iteration using I-FGSM  and MI-FGSM , respectively.

As shown in Fig. 1, the regularizer of gradients can significantly boost the adversarial transferability of either I-FGSM or MI-FGSM. In general, the average attack success rate is improved by a clear margin of \(5.3\%\) and \(7.2\%\) for I-FGSM and MI-FGSM, respectively. Such remarkable performance improvement strongly validates Assumption 1. RAP  also advocates that the adversarial example should be located at a flat local region. However, to the best of our knowledge, it is the first work that empirically validates that flat local maxima can result in good adversarial transferability. Based on this finding, we aim to improve the adversarial transferability by locating the adversarial examples in a flat local region, and a detailed approach for finding flat local maxima will be provided in Sec. 3.3.

### Finding Flat Local Maxima

Although we have verified that adversarial examples at a flat local region tend to have better transferability, the optimization process involved in Eq. (3) is computationally expensive and intractable for generating transferable adversarial examples. The detailed ablation study is summarized in Sec. 4.6. To address this challenge, we propose an approximate optimization approach that efficiently optimizes Eq. (3) to generate more transferable adversarial examples.

Since \(x^{}\) in Eq. (3) is close to \(x^{adv}\) with a small \(\), we assume that the effects of maximizing \(J(x^{adv},y;)\) and maximizing \(J(x^{},y;)\) are expected to be equivalent. Then we can approximately simplify Eq. (3) as follows:

\[_{x^{adv}_{}(x)}(x^{adv},y;) J( x^{},y;)-\|_{x^{}}J(x^{},y;)\|_{2},  x^{}_{}(x^{adv}),\] (4)

where \(\) is a loss function. Here a penalized gradient norm is introduced into the original loss function \(J\) for achieving flat local maxima.

Figure 1: The average attack success rates (%) of I-FGSM and MI-FGSM w/wo the gradient regularization on seven black-box models. The adversarial examples are generated on Inc-v3.

In practice, it is computationally expensive to directly optimize Eq. (4), since we need to calculate its Hessian matrix. Note that existing adversarial attacks typically rely on the sign of the gradient, rather than requiring an exact gradient value. Thus, we approximate the second-order Hessian matrix by using the finite difference method to accelerate the attack process.

**Theorem 1** (Finite Difference Method ).: _Given a finite difference step size \(\) and one normalized gradient direction vector \(v=-J(x,y;)}{\|_{x}J(x,y;)\|_{2}}\), the Hessian/vector product can be approximated by the first-order gradient as follows:_

\[_{x}^{2}J(x,y;)_{V}J(x,y;)|_{x=x+  v}-_{x}J(x,y;)}{}.\] (5)

With the finite difference method, we can solve Eq. (4) approximately by using the Hessian/vector product for high efficiency. In particular, we can calculate the gradient at each iteration as follows.

**Corollary 1**.: _The gradient of the objective function (4) at the \(t\)-th iteration can be approximated as:_

\[_{x_{t}^{adv}}(x_{t}^{adv},y;)(1-) _{x_{t}^{}}J(x_{t}^{},y;)+_{x_{t}^{ }}J(x_{t}^{},y;)|_{x_{t}^{}=x_{t}^{}+ v},\] (6)

_where \(x_{t}^{}\) is a point randomly sampled in \(_{}(x_{t}^{adv})\), \(\) is the iteration step size, and \(=\) is a balanced coefficient._

The detailed proof of Corollary 1 is provided in the Appendix. From Corollary 1, we can approximate the gradient of Eq. (4) by interpolating two neighboring gradients. This approximation technique significantly enhances the efficiency of the attack process.

In Eq. (4), we approximate the loss at the input image \(x^{adv}\) and the maximum gradient in \(_{}(x^{adv})\) by randomly sampling an example \(x^{}_{}(x^{adv})\). However, this approach introduces variance due to the random sampling process. To address this issue, we randomly sample multiple examples and average the gradients of these examples to obtain a more stable gradient. Such averaging helps reduce the error introduced by the approximation and randomness, resulting in a more accurate gradient estimation. This, in turn, allows us to effectively explore flat local maxima and achieve better transferability.

In short, we introduce a novel attack method, called Penalizing Gradient Norm (PGN). The PGN attack aims to guide adversarial examples towards flatter regions by constraining the norm or magnitude of the gradient. The details of the PGN attack are outlined in Algorithm 1. Since PGN is derived by optimizing Eq. (3) to achieve a flat local maximum, it can be seamlessly integrated with existing gradient-based attack methods and input transformation-based attack methods, leveraging their strengths to further improve adversarial transferability.

## 4 Experiments

In this section, we conduct extensive experiments on the ImageNet-compatible dataset. We first provide the experimental setup. Then we compare the results of the proposed methods with existing methods on both normally trained models and adversarially trained models. Finally, we conduct ablation studies to study the effectiveness of key parameters in our PGN. The experimental results were performed multiple times and averaged to ensure the experimental results were reliable.

### Experimental Settings

**Dataset.** We conduct our experiments on the ImageNet-compatible dataset, which is widely used in previous works [4; 35; 42]. It contains 1,000 images with the size of \(299 299 3\), ground-truth labels, and target labels for targeted attacks.

**Models.** To validate the effectiveness of our methods, we test attack performance in five popular pre-trained models, including Inception-v3 (Inc-v3) , Inception-v4 (Inc-v4), InceptionResnet-v2 (IncRes-v2) , ResNet-101 (Res-101), and ResNet-152 (Res-152) . We also consider adversarially trained models including, Inc-v3\({}_{ens3}\), Inc-v4\({}_{ens4}\), and IncRes-v2\({}_{ens}\).

**Baselines.** We take five popular gradient-based iterative adversarial attacks as our baselines, including, MI-FGSM , NI-FGSM , VMI-FGSM , EMI-FGSM , RAP . We also integrate the proposed method with various input transformations to validate the generality of our PGN, such as DIM , TIM , SIM , Admix , and SSA .

**Hyper-parameters.** We set the maximum perturbation of the parameter \(=16.0/255\), the number of iterations \(T=10\), and the step size \(=/T\). For MI-FGSM and NI-FGSM, we set the decay factor \(=1.0\). For VMI-FGSM, we set the number of sampled examples \(N=20\) and the upper bound of neighborhood size \(=1.5\). For EMI-FGSM, we set the number of examples \(N=11\), the sampling interval bound \(=7\), and adopt the linear sampling. For the attack method, RAP, we set the step size \(=2.0/255\), the number of iterations \(K=400\), the inner iteration number \(T=10\), the late-start \(K_{LS}=100\), the size of neighborhoods \(_{n}=16.0/255\). For our proposed PGN, we set the number of examples \(N=20\), the balanced coefficient \(=0.5\), and the upper bound of \(=3.0\).

### Visualization of Loss Surfaces for Adversarial Example

To validate that our proposed PGN method can help the adversarial examples find a flat maxima region, we compare the loss surface maps of the adversarial examples generated by different attack methods on the surrogate model (_i.e_., Inc-v3). Each 2D graph corresponds to an adversarial example, with the adversarial example shown at the center. We randomly select two images from the dataset and compare the loss surfaces in Fig. 2, each row represents the visualization of one image. From the comparison, we observe that our PGN method can help adversarial examples achieve flatter maxima regions compared to the baselines. The adversarial examples generated by our method are located

Figure 2: Visualization of loss surfaces along two random directions for two randomly sampled adversarial examples on the surrogate model (_i.e_., Inc-v3). The center of each 2D graph corresponds to the adversarial example generated by different attack methods (see more examples in the Appendix).

[MISSING_PAGE_FAIL:7]

Inc-v3, Inc-v4, IncRes-v2. All the ensemble models are assigned equal weights and we test the performance of transferability on both normally trained models and adversarially trained models.

The results, presented in Table 2, demonstrate that our PGN method consistently achieves the highest attack success rates in the black-box setting. Compared to previous gradient-based attack methods, PGN achieves an average success rate of \(92.78\%\), outperforming VMI-FGSM, EMI-FGSM, and RAP by \(12.42\%\), \(11.88\%\), and \(7.37\%\), respectively. Notably, our method exhibits even greater improvements against adversarially trained models, surpassing the best attack method RAP by over \(18.4\%\) on average. These results validate that incorporating penalized gradient norms into the loss function effectively enhances the transferability of adversarial attacks, which also confirms the superiority of our proposed method in adversarial attacks.

### Combined with Input Transformation Attacks

Existing input transformation-based attacks have shown great compatibility with each other. Similarly, due to the simple and efficient gradient update process, our proposed PGN method can also be combined with these input transformation-based methods to improve the transferability of adversarial examples. To further demonstrate the efficacy of the proposed PGN method, we integrate our method into these input transformations _i.e_., DIM, TIM, SIM, Admix, and SSA. We generate adversarial examples on the Inc-v3 model and test the transferability of adversarial examples on six black-box models.

The experimental results are shown in Table 3. When combined with our gradient update strategy, it can significantly improve the adversarial transferability of these input transformation-based attack methods in the black-box setting. At the same time, our method also has great improvement after combining these methods. For example, DIM only achieves an average success rate of \(54.56\%\) on the seven models, while when combined with our PGN method it can achieve an average rate of \(84.06\%\), which is \(29.5\%\) higher than before. Especially, after combining these input transformation-based methods, our PGN tends to achieve much better results on the ensemble adversarially trained models, compared with the results in Table 1. It is a significant improvement and shows that our method

  Attack & Inc-v3 & Inc-v4 & IncRes-v2 & Res-101 & Res-152 & Inc-v3\({}_{ens3}\) & Inc-v3\({}_{ens4}\) & IncRes-v2\({}_{ens}\) & Avg. \\   MI & 99.8* & 99.5* & 97.8* & 66.8 & 68.4 & 36.7 & 36.9 & 22.6 & 66.06 \\ NI & **100.0*** & **99.9*** & 99.6* & 74.6 & 74.0 & 37.7 & 37.1 & 22.2 & 68.14 \\ VMI & 99.9* & 99.5* & 98.0* & 82.2 & 81.6 & 66.8 & 64.1 & 50.8 & 80.36 \\ EMI & **100.0*** & **99.9*** & **99.7*** & 92.3 & 93.0 & 62.8 & 59.5 & 40.0 & 80.90 \\ RAP & **100.0*** & 99.4* & 98.2* & 93.5 & 93.4 & 70.1 & 69.8 & 58.9 & 85.41 \\ PGN & **100.0*** & **99.9*** & 99.6* & **94.2** & **94.6** & **88.2** & **86.6** & **79.2** & **92.78** \\  

Table 2: The untargeted attack success rates (%) of various gradient-based attacks on eight models in the multi-model setting. The adversarial examples are generated on the ensemble models, _i.e_. Inc-v3, Inc-v4, and IncRes-v2. Here * indicates the white-box model.

  Attack & Inc-v3 & Inc-v4 & IncRes-v2 & Res-101 & Inc-v3\({}_{ens3}\) & Inc-v3\({}_{ens4}\) & IncRes-v2\({}_{ens}\) & Avg. \\   DIM & 99.7* & 72.2 & 67.3 & 62.8 & 32.8 & 30.7 & 16.4 & 54.56 \\ PGN-DIM & **100.0*** & **93.6** & **91.9** & **87.3** & **78.3** & **77.5** & **59.8** & **84.06** \\  TIM & 99.9* & 51.6 & 47.2 & 47.8 & 29.6 & 30.7 & 20.5 & 46.76 \\ PGN-TIM & **100.0*** & **87.6** & **84.1** & **75.0** & **78.1** & **77.6** & **65.5** & **81.13** \\  SIM & **100.0*** & 70.5 & 68.2 & 63.8 & 37.5 & 37.8 & 22.0 & 57.11 \\ PGN-SIM & **100.0*** & **92.5** & **91.2** & **84.0** & **76.1** & **75.7** & **59.0** & **82.64** \\  Admix & **100.0*** & 78.6 & 77.3 & 69.5 & 41.6 & 40.3 & 24.1 & 61.63 \\ PGN-Admix & **100.0*** & **93.1** & **92.2** & **85.5** & **76.9** & **77.2** & **60.2** & **83.57** \\  SSA & 99.7* & 88.3 & 86.8 & 77.7 & 56.7 & 55.3 & 35.2 & 71.39 \\ PGN-SSA & **99.8*** & **89.9** & **89.7** & **82.9** & **69.2** & **67.8** & **47.1** & **78.03** \\  

Table 3: The untargeted attack success rates (%) of our PGN method, when it is integrated with DIM, TIM, SIM, Admix, and SSA, respectively. The adversarial examples are generated on Inc-v3. Here * indicates the white-box model.

has good scalability and can be combined with existing methods to further improve the success rate of transfer-based black-box attacks. In addition, our PGN method can also be combined with various gradient-based attack methods to enhance the transferability of previous works. The more experimental results are shown in the Appendix.

### Ablation Study on Finite Difference Method

In this subsection, we will analyze and experimentally verify the effectiveness of the finite difference method in accelerating the approximation of the second-order Hessian matrix. We first theoretically analyze the acceleration effect of the finite difference method and substantiate our theoretical analysis with comparative experiments.

**Theoretical analysis.** For the baseline attack method, I-FGSM , the gradient is computed only once per iteration. Thus, its computational complexity is \(O(n)\), where \(n\) represents the image size. However, when we introduce the penalty gradient term, the need arises to compute the second-order Hessian matrix, leading to a theoretical computational complexity of \(O(n^{2})\). To address this, we propose the finite difference method as an approximation to the Hessian matrix, which requires the computation of the gradient twice in each iteration, effectively yielding a computational complexity of \(O(2n)\). This theoretically promises significant improvements in computational efficiency.

**Experimental comparison.** To validate the effectiveness of the finite difference method in accelerating computation, we compare the runtime and computational memory before and after using the finite-difference method. These experiments were conducted using codes executed on an RTX 2080 Ti with a CUDA environment. We employed I-FGSM and evaluated the total running time on 1,000 images (excluding data loading time) and the attack success rate on black-box models. The outcomes are presented in Table 4. Directly optimizing Eq. (4) results in better attack performance with high computational resources. With the finite difference method (FDM), we can better approximate the performance of direct optimization of the second-order Hessian matrix, which significantly reduces the running time and the computational memory. Furthermore, owing to the relatively modest image size and the comparatively small number of parameters compared to the model, the accelerated computing capabilities of CUDA enable the actual running time to be less than the theoretical estimates.

### Ablation Study on Hyper-parameters

In this subsection, we conduct a series of ablation experiments to study the impact of different parameters, including the balanced coefficient \(\) and the upper bound of \(\)-ball, and the study of the number of sampled examples \(N\) will be illustrated in the Appendix. To simplify our analysis, we only consider the transferability of adversarial examples crafted on the Inc-v3 model.

**The balanced coefficient \(\).** In Sec. 3.3, we introduce a balanced coefficient \(\) to represent the penalty coefficient \(\) (see the Appendix for details). Compared to studying the parameter of \(\), studying \(\) will be much easier because it divides the scope of the parameter learning. As shown in Fig. 2(a), we study the influence of the \(\) in the black-box setting where \(\) is fixed to \(3.0\). As we increase \(\), the transferability increases and achieves the peak for these black-box models when \(=0.5\). This indicates that when averaging these two gradients the performance is best. Therefore, we set \(=0.5\) in our experiments.

**The upper bound of \(\)-ball.** In Fig. 2(b), we study the influence of the upper bound neighborhood size for random sampling, determined by parameter \(\), on the success rates in the black-box setting. In our experiments, we use uniform sampling, which reduces the bias due to uneven sample distribution. As

   Attack & \(H_{m}\) & FDM & Inc-v3 & Inc-v4 & IncRes-v2 & Res-101 & Res-152 & Time (s) & Memory (MiB) \\    & ✗ & ✗ & **100.0*** & 27.8 & 19.1 & 38.1 & 35.2 & 52.31 & 1631 \\ I-FGSM & ✓ & ✗ & **100.0*** & **39.2** & **30.2** & **47.0** & **45.5** & **469.54** & **7887** \\  & ✓ & ✓ & **100.0*** & 37.9 & 28.6 & 45.7 & 44.6 & 96.42 & 1631 \\  

Table 4: Comparison of the approximation effect between directly optimizing the Hessian matrix (\(H_{m}\)) and using the Finite Difference Method (FDM) to approximate. “Time” represents the total running time on 1,000 images, and “Memory” represents the computing memory size.

we increase \(\), the transferability increases and achieves the peak for normally trained models when \(=3.0\), but it is still increasing against the adversarially trained models. When \(>4.5\), the performance of adversarial transferability will decrease on seven black-box models. To achieve the trade-off for the transferability on normally trained models and adversarially trained models, we set \(=3.0\) in our experiments.

## 5 Conclusion

Inspired by the observation that flat local minima often result in better generalization, we hypothesize and empirically validate that adversarial examples at a flat local region tend to have better adversarial transferability. Intuitively, we can optimize the perturbation with a gradient regularize in the neighborhood of the input sample to generate an adversarial example in a flat local region but it is non-trivial to solve such an objective function. To address such an issue, we propose a novel attack method called Penalizing Gradient Norm (PGN). Specifically, PGN approximates the Hessian/vector product by interpolating the first-order gradients of two samples. To better explore its neighborhood, PGN adopts the average gradient of several randomly sampled data points to update the adversarial perturbation. Extensive experiments on the ImageNet-compatible dataset demonstrate that PGN can generate adversarial examples at more flat local regions and achieve much better transferability than existing transfer-based attacks. Our PGN can be seamlessly integrated with other gradient-based and input transformation-based attacks to further improve adversarial transferability, demonstrating its versatility and ability to improve adversarial transferability across different attack scenarios.

## 6 Limitation

Although we have experimentally verified that flat local minima can improve the transferability of adversarial attacks, there is still a lack of theoretical analysis regarding the relationship between flatness and transferability. An existing perspective is that transferability may be related to the generalization ability of flatness. We will also keep studying the theoretical connection between transferability and flat local minima in our future work. We hope our work sheds light on the potential of flat local maxima in generating transferable adversarial examples and provides valuable insights for further exploration in the field of adversarial attacks.