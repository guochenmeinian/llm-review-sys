# MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey

Xian Wu1, Yutian Zhao1, Yunyan Zhang1, Jiageng Wu2, Zhihong Zhu3

**Yingying Zhang1, Yi Ouyang1, Ziheng Zhang1, Huimin Wang1, Zhenxi Lin1**

**Jie Yang4, Shuang Zhao5, Yefeng Zheng6***

1Tencent Youtu Lab, Jarvis Research Center 2Thejiang University

3Peking University 4Harvard Medical School

5 Xiangya Hospital 6Westlake University

{kevinxwu, yutianzhao, yunyanzhang}@tencent.com

jiagengwu@zju.edu.cn, zhinongzhu@stu.pku.edu.cn

{ninzhang, yiouyang, zihengzhang, chalerislin}@tencent.com

jieynlp@gmail.com, shuangxy@csu.edu.cn, yefengzheng@westlake.com.cn

Corresponding author

###### Abstract

Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs' performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient's clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs' effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs' performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.

## 1 Introduction

Large language models (LLM), such as ChatGPT and GPT-4, have showcased impressive capabilities in comprehending users' intent and generating coherent responses (Zhao et al., 2023). Their flexible input requirements make them suitable for a broad range of tasks across various domains. Among these, the medical domain stands out as a particularly fitting area for LLM applications (Thirunawukarasu et al., 2023). Over the past two decades, hospital IT systems like Hospital Information System (HIS), Laboratory Information System (LIS), and Picture Archiving and Communication System (PACS) have accumulated a wealth of clinical data, providing a robust foundation for training LLMs (Wu et al., 2024). Simultaneously, there is a significant demand for LLM applications in the medical field, such as online inquiries (Liu et al., 2022), diagnostic assistance (Rasmy et al.,2021), medication recommendations (Wu et al., 2022), and discharge summary (Liu et al., 2022c). Implementing LLMs in these medical scenarios can alleviate doctors' workload and enhance clinical efficiency (Wang et al., 2023b).

However, given the critical nature of patient care, the medical domain has little tolerance for errors in LLM outputs. Therefore, a comprehensive evaluation of these models is essential before deployment. Several benchmarks have been proposed to date, which can be categorized into three types: 1) Exam-based, CMExam (Liu et al., 2024b) is built from China National Medical Licensing Examination (CNMLE), MedQA (Jin et al., 2021) is built from United States Medical Licensing Examination (USMLE) and MedMCQA (Pal et al., 2022) is built from All India Institute of Medical Sciences (AIIMS PG) and National Eligibility cum Entrance Test (NEET PG). These licensing exams, designed to judge whether medical school students are qualified to be doctors, provide a reasonable basis for evaluating LLM performance; 2) QA-based, involving single (Abacha et al., 2017) and multi-turn interactions (Liu et al., 2022a) between patients and doctors, which can evaluate the performance of LLM in dealing with patients' inquiries; 3) Task-based, assessing performance in various medical Natural Language Processing (NLP) tasks, such as summarization, medical named entity recognition (NER), etc. For example, CBLUE (Zhang et al., 2022) is a collection of medical tasks. PromptCBLUE (Zhu et al., 2023) further adapts CBLUE for LLM evaluation by adding different forms of prompts. For more detailed information and illustrative examples, please refer to Appendix A.1.2.

A limitation of existing medical benchmarks is that they are organized either by the question type (multiple-choice, question answering) or the task type (NER, Classification, etc.), and many of them do not include clinical text data generated from real-world clinical practice (Wu et al., 2024). In addition, existing datasets are not structured according to the steps of the clinical process in patient care. As a result, it's difficult to assess the performance of LLMs in assisting patients in real clinics. In this paper, we segment the entire patient clinical journey into four stages. For each stage, we introduce multiple tasks. In summary, the contribution of this paper is threefold:

* We introduce a new Chinese benchmark dataset MedJourney 2 that covers the entire workflow of the patient's clinical journey which organizes the benchmark w.r.t patient clinical journey. * In total, we introduce 12 datasets corresponding to 12 different tasks across four stages. Of these, 7 datasets are reconstructed from existing public sources, while 5 are newly proposed in this paper. All new datasets have been meticulously processed by professional doctors, ensuring high quality and reliability. The detailed information can be found in Appendix A.2.
* We evaluate the performance of existing LLMs on MedJourney. We evaluate not only the close source LLMs, like ChatGPT 3 and GPT-4 4 but also open-source LLMs, like QWen (Bai et al., 2023) and ChatGLM (Du et al., 2022). We also include medical LLMs like Huatuo GPT2 (Chen et al., 2023b). Since MedJourney is in Chinese, we didn't include the English-centric LLMs, like Llama series (Touvron et al., 2023a,b). In addition to the accuracy and NLG metrics, we also conduct entity-level evaluations to verify performance from a semantic perspective.

## 2 Related Works

MultiMedQA (Singhal et al., 2023) is a widely recognized benchmark for evaluating the performance of LLMs in the medical domain. This benchmark comprises seven datasets: MedQA (Jin et al., 2021), MedMCQA (Pal et al., 2022), PubMedQA (Jin et al., 2019), MMUL clinical topics (Hendrycks et al., 2020), LiveQA (Abacha et al., 2017), MedicationQA (Abacha et al., 2019), and HealthSearchQA. The first four datasets consist of multiple-choice questions, while the last three contain questions requiring long-form free-text answers. This benchmark is in English. Recently, there are also some new clinical benchmarks that focus on diagnostic reasoning (Gao et al., 2023), multi-modal agent (Schmindgall et al., 2024) and doctor-patient conversation (Wang et al., 2023c).

For Chinese benchmarks, MedBench (Cai et al., 2024) primarily included multiple-choice questions from medical examinations. However, the MedBench website (Liu et al., 2024a) 5 featured a broader range of tasks, such as QA and NER. Liu et al. (2024b) constructed a benchmark, CMEExam, for evaluating LLMs based on medical exam data collected from the web. In addition to the answer, the authors appended five types of labels to each question, enabling a detailed performance analysis. CMB (Wang et al., 2023) included both medical exams and some clinical case studies. Zhang et al. (2022) introduced CBLUE, which comprises multiple biomedical tasks. Since CBLUE was not specifically designed to evaluate LLMs' performance, Zhu et al. (2023) introduced multiple prompt templates for each task and restructured each instance in CBLUE into a prompt and target format. For free-form QA datasets, there are CMedQA (Zhang et al., 2017), CMedQA2 (Zhang et al., 2018), and WebMedQA (He et al., 2019). The differences between existing benchmarks and MedJourney are summarized in Table 1.

## 3 Patient Clinical Journey

Inspired by the ideal patient journey in the digital healthcare6, as shown in Figure 1, we divide the patient clinical journey into four stages: planning, access, delivery, and ongoing care, and then propose 12 data sets accordingly, among which, Department Recommendation (DR), Pre-Consultation Dialogue Summary(PCDS), Hospital Reception QA (HQA), Insurance QA (IQA) and Drug QA (DQA) are newly proposed, while the rest seven are rebuilt from existing datasets. More details about how the patient clinical journey is divided can be found in Appendix A.1.1 and more details about how these data sets are built can be found in Appendix A.2.

### Planning

At this stage, patients are becoming aware of potential health issues and are considering seeking medical care at a hospital. We propose three datasets to address the primary needs of patients at this stage: 1) Department Recommendation (DR) assists in identifying the most suitable departments based on patients' primary complaints; 2) Pre-Consultation Dialogue Summary (PCDS) utilizes a multi-turn conversation to gather and summarize patients' information; 3) Hospital Reception QA (HQA) compiles frequently asked questions from patients before and during their hospital visits.

#### 3.1.1 Department Recommendation

As online appointment for medical consultations becomes increasingly popular, a growing number of patients are choosing to use the outpatient intelligent guidance system to select suitable hospitals and departments. Department recommendation is a crucial component of this system, aiming to suggest appropriate departments based on the patient's primary complaints. However, patients often lack adequate medical knowledge, resulting in vague symptom descriptions. Moreover, the complexity of hospital department structures further complicates the task of department recommendation.

  
**Benchmark** & **Journey Organized** & **Multiple Choice** & **QA** & **Summarization** & **Classification** \\  MedExam & ✗ & ✗ & ✗ & ✗ & ✗ \\ MedBench (paper) & ✗ & ✗ & ✗ & ✗ & ✗ \\ MedBench (website) & ✗ & ✓ & ✓ & ✓ & ✓ \\ PromptCBLUE & ✗ & ✗ & ✓ & ✓ & ✓ \\ CMB & ✗ & ✗ & ✓ & ✓ & ✗ \\ MultiMedQA & ✗ & ✗ & ✓ & ✓ & ✗ \\ MedJourney (ours) & ✗ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of Ours and Existing Medical Benchmarks.

Figure 1: Four stages in patient journey which covers patients’ experience workflow. For each stage, we introduce several datasets to evaluate the performance of LLMs.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

a skilled medical LLM should also understand the small differences and details that can separate similar diseases or conditions. They must be good at recognizing patterns, thinking about different possibilities, and judging the chance of each possible diagnosis based on the evidence they have. This dataset includes 1,761 question-answer pairs that cover a total of 1,490 diseases.

#### 3.3.3 Treatment Prediction

Doctors typically rely on established medical guidelines and evidence-based medicine when devising treatment plans. With all the relevant information at their disposal, they tailor the treatment plan to suit the patient's unique needs. The treatment prediction dataset serves as a tool to assess whether medical LLMs can generate accurate treatment plans that not only address the patient's diagnosis but also take into account their individual circumstances. This dataset comprises a total of 148 question-answer pairs, encompassing 133 distinct treatment plans.

#### 3.3.4 Medication Prediction

When recommending medications to patients, doctors consider the effectiveness, safety, potential side effects, and dosage requirements of the drugs. Prescribing an incorrect medication can lead to serious repercussions, such as not addressing the actual issue, potentially exacerbating the patient's condition, or postponing the necessary treatment. Therefore, it's essential for medical LLMs to possess a deep understanding of medicine and provide the most precise medication recommendations tailored to a patient's unique situation. The medicine prediction dataset comprises a total of 1029 question-answer pairs, covering 722 distinct medications.

### Ongoing Care

At this stage, patients have transitioned from receiving in-hospital care to managing their health independently outside the hospital. There are three key tasks where LLM can be applied: 1) Drug QA (DQA), this task involves addressing common questions patients may have about their medications; 2) Insurance QA (IQA), this task includes answering frequently asked questions about medical insurance; 3) Mental Health QA (MQA), this task involves addressing common questions from patients who may be dealing with mental health issues outside the hospital.

#### 3.4.1 Drug QA

Drug QA is a critical component in the post-treatment phase of a patient's clinical journey, where patients often seek clarity on their prescribed medications. Understanding the indications, contraindications, side effects, dosage instructions, and potential interactions of medications is essential for safe and effective self-care. To address this need, we have curated a dataset comprising 137 question-answer pairs that reflect real-world patient inquiries about various medications. Table 5 provides two examples of medication question-answer pairs.

#### 3.4.2 Insurance QA

Medical insurance is another important aspect of the healthcare system, especially after the patients are discharged from the hospital. The patients need to pay for the medical expenses, and the medical

|p{142.3pt}|} 
**Question** & **Answer** \\  \(^{2}\)**49** & **17261** & **17261** \\  

Table 4: Example of Examination Prediction.

Figure 2: The distribution of disease subjects.

[MISSING_PAGE_FAIL:7]

like GPT-4 and QWen, incorporating a one-shot approach enhances performance across most tasks. This improvement is particularly noticeable for the summary tasks PDDS, as the LLMs learn the format of the output content. However, for delivery tasks, the performance of medical domain-specific models decreases. The introduced example may act as noise, negatively impacting performance.

#### 4.2.3 Entity Level Evaluation

For QA tasks, we incorporate an entity-based metric alongside general NLG metrics to evaluate LLMs from a semantic perspective. Specifically, we extract medical entities such as disease names, symptom terms, drug names, etc., from the golden answer of each instance. This extraction is initially performed by GPT-4 and subsequently double-checked manually. The frequency of these extracted entities is depicted in Figure 3(a). The 5 most frequent entities are listed in Figure 3(b).

Using these extracted entities, we compute the entity recall for all QA tasks. As demonstrated in Table 8, GPT-4 achieves near-optimal performance on almost all tasks, suggesting that GPT-4 is capable of incorporating key information in its responses.

    & **DR** & **PCDS** & **HQA** & **DRG** & **PDDS** & **FP** & **DP** & **TP** & **MP** & **DQA** & **IOA** & **MQA** \\ Acc & B-4 & B-4 & B-4 & B-4 & Acc & Acc & Acc & Acc & Acc & B-4 & B-4 & B-4 \\   \\  CharGLM3 & 0.130 & 9.828 & 2.462 & 2.138 & 6.818 & 0.398 & 0.313 & 0.331 & 0.318 & 7.108 & **5.323** & 3.740 \\ QWen-7B & 0.264 & 5.489 & 1.843 & 1.773 & 7.796 & 0.528 & 0.559 & 0.493 & 0.638 & 7.256 & 4.241 & 3.917 \\ QWen-14B & 0.280 & 8.271 & 2.151 & **3.740** & 6.560 & 0.630 & 0.579 & 0.588 & 0.646 & 8.296 & 4.993 & 4.071 \\ QWen-32B & 0.304 & 7.614 & 1.978 & 2.491 & 6.987 & 0.667 & 0.684 & 0.608 & 0.715 & **8.596** & 4.609 & **4.214** \\ QWen-72B & **0.308** & 9.032 & 2.266 & 2.649 & 9.318 & 0.674 & 0.692 & 0.635 & 0.742 & 8.371 & 4.671 & 4.129 \\   \\  CharGLM & 0.255 & **11.242** & **2.576** & 2.912 & 10.632 & 0.419 & 0.369 & 0.364 & 0.382 & 6.928 & 5.303 & 3.767 \\ GPT-4 & 0.306 & 5.913 & 1.485 & 1.649 & 7.453 & 0.613 & 0.632 & 0.503 & 0.561 & 6.623 & 3.694 & 3.598 \\   \\  HautaoGPT2-7B & 0.214 & 4.387 & 2.094 & 1.410 & 6.200 & 0.681 & 0.688 & 0.554 & 0.629 & 8.079 & 4.165 & 4.035 \\ HuautoGPT2-34B & 0.278 & 7.951 & 1.764 & 1.998 & **12.082** & **0.757** & **0.766** & **0.662** & **0.777** & 8.382 & 4.149 & 4.132 \\ DISC-MedLLM & 0.216 & 2.775 & 2.524 & 1.288 & 6.190 & 0.269 & 0.241 & 0.149 & 0.282 & 5.570 & 3.928 & 2.908 \\   

Table 6: Zero-Shot Performance on the Clinical Journey Dataset.

    & **DR** & **PCDS** & **HQA** & **DRG** & **PDDS** & **FP** & **DP** & **TP** & **MP** & **DQA** & **IOA** & **MQA** \\ Acc & B-4 & B-4 & B-4 & B-4 & Acc & Acc & Acc & Acc & B-4 & B-4 & B-4 \\   \\  CharGLM3 & 0.258 & 8.282 & **4.715** & 2.555 & **20.674** & 0.465 & 0.279 & 0.480 & 0.343 & 6.987 & **6.432** & 3.661 \\ QWen-7B & 0.292 & 10.414 & 2.386 & 2.410 & 12.868 & 0.412 & 0.436 & 0.365 & 0.485 & 7.207 & 5.129 & 4.074 \\ QWen-14B & 0.392 & 14.554 & 2.515 & 3.361 & 11.591 & 0.539 & 0.583 & 0.520 & 0.580 & 7.724 & 5.538 & 3.935 \\ QWen-32B & 0.354 & 12.567 & 2.349 & **4.305** & 13.788 & **0.676** & 0.739 & 0.709 & 0.699 & 7.971 & 5.030 & **4.570** \\ QWen-72B & 0.370 & 12.545 & 2.713 & 2.918 & 14.946 & 0.674 & **0.769** & **0.730** & **0.723** & **8.906** & 5.532 & 4.438 \\   \\  CharGLM & 0.328 & **21.316** & 4.437 & 3.199 & 18.714 & 0.364 & 0.345 & 0.357 & 0.355 & 5.915 & 6.091 & 3.911 \\ GPT-4 & **0.440** & 13.164 & 2.506 & 2.169 & 12.834 & 0.601 & 0.679 & 0.575 & 0.576 & 7.817 & 3.886 & 4.022 \\   \\  HautaoGPT2-7B & 0.208 & 2.182 & 1.674 & 1.526 & 5.224 & 0.410 & 0.399 & 0.250 & 0.266 & 8.178 & 4.545 & 4.061 \\ HautaoGPT2-34B & 0.274 & 12.159 & 2.567 & 2.958 & 16.244 & 0.641 & 0.588 & 0.466 & 0.659 &

#### 4.2.4 LLM Rating

We also leverage the LLMs to evaluate the performance of LLMs. We let GPT-4 to rate the performance of LLMs on benchmark, particularly for the QA questions.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The results are shown in Table 8.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The text box above displays the prompt that instructs GPT-4 to evaluate the performance of LLMs on the QA tasks in MedJourney. As shown in Table 9, if we average the performance on these 7 tasks. QWen-72B, GPT-4 and QWen 32b rank the top 3.

#### 4.2.5 Human Evaluation

In addition to the automatic metrics, we have also carried out a human evaluation of 10 Language Learning Models (LLMs) across 7 QA tasks. For this evaluation, we instructed the annotators to assign a score from 1 to 100, based on the following criteria. Table 10 displays the average human rating of performance of LLMs on each task.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

The sentences of Human Evaluation

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

Based on the 3 criteria below, rate the model performance on a scale of 1-100.

[left=0pt,right=0pt,topsep=0pt,leftmargin=*]

**Accuracy:** The response provided by the large language model is accurate and has no factual errors. Conclusions are not made arbitrarily.

**Helfulness:** The model's response provides the patient with clear, instructive and practical assistance, specifically addressing the medical task.

**Linguistic Quality:** The response logical. The model correctly understands the medical task, and the expressions smooth and natural.

In this analysis, we average the performance across various QA tasks. GPT-4 emerges as the top performer, followed by QWen-32B, which deviates from the LLM rating ranking presented in Section 4.2.4. This raises the question of which metric aligns best with human evaluation: the B-4 metric (Table 6), the entity-based metric (Table 8), or the LLM rating (Table 9). We rank 10 Language Learning Models (LLMs) based on the average metrics across the seven tasks, as shown in Table 11.

To calculate the alignment between each metric and human evaluation, we enumerate all pairs of the 10 models, resulting in a total of 45 pairs. If the order of a pair of models according to one metric is

  
**Model** & **PCDS** & **HQA** & **DRG** & **PDDS** & **DQA** & **IQA** & **MQA** \\   \\ ChatGIM3 & 0.461 & 0.227 & 0.132 & 0.381 & 0.151 & 0.287 & 0.220 \\ QWen-7B & 0.495 & 0.281 & 0.155 & 0.428 & 0.188 & 0.321 & **0.287** \\ QWen-14B & 0.513 & 0.262 & 0.096 & **0.428** & **0.192** & 0.326 & 0.286 \\ QWen-32B & 0.524 & 0.270 & 0.175 & 0.420 & 0.176 & 0.340 & 0.259 \\ QWen-72B & 0.495 & **0.288** & 0.182 & 0.417 & 0.188 & 0.324 & 0.255 \\   \\ ChatGPT & 0.462 & 0.250 & 0.151 & 0.363 & 0.148 & 0.302 & 0.247 \\ GPT-4 & **0.603** & 0.274 & **0.184** & 0.400 & 0.184 & **0.341** & 0.271 \\   \\ HautoGPT2-7B & 0.415 & 0.234 & 0.151 & 0.327 & 0.184 & 0.309 & 0.241 \\ HautoGPT2-34B & 0.441 & 0.239 & 0.142 & 0.357 & 0.194 & 0.314 & 0.265 \\ DISC-MedLLM & 0.312 & 0.212 & 0.157 & 0.274 & 0.126 & 0.211 & 0.180 \\   

Table 8: The Recall of Medical Entities for QA Tasks.

the same as that in human evaluation, we assign it a vote of 1; otherwise, it receives a vote of 0. We then use the alignment rate to measure the corelation of between a metric and human evaluation.

When comparing the rankings of each pair of LLMs to see if they align with human ratings, we find that the correlation between Human and GPT Judge is 84.4%; between Human and Entity, it's 73.3%; and between Human and B-4, it's 35.5%. However, when we combine the scores from GPT-Judge and Entity (calculated as GPT-Judge score/100 + Entity score), the alignment improves to 91.1%. This combined metric offers a robust measure for evaluating the performance of a Language Learning Model in the medical domain.

## 5 Conclusion

In this paper, we introduce a novel benchmark dubbed MedJourney, which is designed to evaluate the performance of LLMs from the perspective of a patient's clinical journey. We segment the entire journey into four stages, and for each stage, we propose multiple tasks that LLMs can undertake, providing corresponding test sets for evaluation. Of the 12 datasets proposed, seven are constructed from existing corpora, while the remaining five are newly proposed datasets. In addition to the prompt and target pairs, we also provide the key medical entities that needs to be addressed. The proposed MedJourney allows us to assess LLMs from a clinical perspective and identify which stages require further improvement.

  
**Model** & **PCDS** & **HOA** & **DRG** & **PDDS** & **DOA** & **IOA** & **MOA** & **AVERAGE** \\   \\ ChsuGLM3 & 87 & 46 & 61 & 83 & 75 & 37 & 72 & 66 \\ QWE-7B & 96 & 65 & 70 & 87 & 78 & 53 & 73 & 75 \\ QWE-14B & 97 & 64 & 61 & 88 & 77 & 56 & 73 & 74 \\ QWE-32B & 96 & 65 & 74 & 87 & **79** & 62 & 70 & 76 \\ QWE-72B & 97 & 62 & 73 & 86 & **79** & 51 & 72 & 74 \\   \\ ChsuGPT & 94 & 59 & 66 & 80 & 75 & 46 & 79 & 71 \\ GPT-4 & **98** & **68** & **76** & **91** & 78 & **64** & **82** & **80** \\   \\ Hutano-GPT2-78 & 90 & 56 & 68 & 76 & 78 & 49 & 70 & 70 \\ hunting-GPT2-34B & 96 & 61 & 74 & 86 & **79** & 58 & 73 & 75 \\ DISC-MeLLM & 68 & 42 & 67 & 80 & 71 & 39 & 64 & 62 \\   

Table 10: The Human Evaluation on QA Tasks in MedJourney.

  
**Rank** & **B-4** & **Early** & **GPT Judge** & **GPT Judge + Entity** & **Human** \\ 
1 & ChsuGPT & GPT-4 & QWE-72B & GPT-4 & QFT-4 & QFT-4 \\
2 & Hunting-GPT2-34B & QWE-32B & QWE-72B & QWE-32B \\
3 & QWE-72B & QWE-72B & QWE-72B & Hunting-GPT2-34B \\
4 & QWE-14B & QWE-72B & QWE-72B & QWE-72B & QWE-72B \\
5 & ChsuGLM3 & QWE-14B & QWE-14B & QWE-14B & QWE-14B & QWE-72B \\
6 & QWE-32B & Hunting-GPT2-34B & Hutano-GPT2-34B & Hutano-GPT2-34B & QWE-14B \\
7 & QWE-7B & ChatGPT & Hunting-GPT2-7B & ChatGPT & ChatGPT \\
8 & GPT-4 & Human-GPT2-72B & ChatGPT & Hunting-GPT2-7B & Hunting-GPT2-7B \\
9 & Hunting-GPT2-7B & ChatGLM3 & ChatGLM3 & ChatGLM3 & ChatGLM3 & ChatGLM3 \\
10 & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM & DISC-MeLLM \\   

Table 11: The Ranking of 10 LLMs w.r.t Each Evaluation Metric.

  
**Model** & **PCDS** & **HOA** & **DRG** & **PDDS** & **DOA** & **IOA** & **MOA** & **AVERAGE** \\   \\ ChsuGLM3 & 80.16 & 78.34 & 81.83 & 82.35 & 82.66 & 82.41 & 87.96 & 82.24 \\ QWE-7B & 87.37 & 83.32 & 85.83 & 86.67 & 87.32 & **87.53** & **90.30** & 86.91 \\ QWE-14B & 89.37 & 85.58 & 81.69 & 87.06 & 87.68 & 86.09 & 89.44 & 86.70 \\ QWE-32B & 88.24 & 85.80 & **86.22** & 87.32 & 88.13 & **87.53** & 90.08 & 87.62 \\ QWE-72B & 89.34 & **86.57** & 85.64 & **87.84** & **85.88** & 86.42 & 89.45 & **87.73** \\   \\ GhorOPT & 88.18 & 83.10 & 69.95 & 86.05 & 83.43 & 84.06 & 89.89 & 83.52 \\ GPT-4 & **89.44** & 85.86 & 85.86 & 87.51 & 87.80 & 87.33 & 90.19 & 87.71 \\   \\ HutanoGPT2-78B & 81.89 & 79.41 & 82.94 & 81.41 & 87.24 & 84.99 & 88.61 & 83.78 \\ HuangGPT2-34B & 89.11 & 82.44 & 84.58 & 85.75 & 86.34 & 86.28 & 89.38 & 86.27 \\ DISC-MeLLM & 72.45 & 78.89 & 81.83 & 77.69 & 79.37 & 77.01 & 86.31 & 79.08 \\   

Table 9: The GPT-4 Evaluation on QA Tasks in MedJourney.