# MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding

Xinyu Fang\({}^{1,2*}\), Kangrui Mao\({}^{3*}\), Haodong Duan\({}^{2*}\)\({}^{}\),

**Xiangyu Zhao\({}^{2,3}\), Yining Li\({}^{2}\), Dahua Lin\({}^{2,4,5}\), Kai Chen\({}^{2}\)**

\({}^{1}\)Zhejiang University \({}^{2}\)Shanghai AI Laboratory \({}^{3}\)Shanghai Jiao Tong University

\({}^{4}\)The Chinese University of Hong Kong \({}^{5}\)CPII under InnoHK

\({}^{*}\) Equal Contribution \({}^{}\) Corresponding Author

fangxinyu,duanhaodong@pjlab.org.cn

###### Abstract

The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evaluation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.

## 1 Introduction

As a ubiquitous format for multimedia, video holds a pivotal role in people's lives, serving purposes such as knowledge dissemination, sharing life experiences, and entertainment. The rapid proliferation of video content has reshaped communication, learning, and connection in the digital age. The vast amount of online video content underscores the importance of algorithmic video understanding. Current video understanding paradigms, which often focus on specific tasks , typically excel only on in-domain data. An ideal video understanding system should demonstrate robust zero-shot capabilities, accurately discern contextual, emotional, and linguistic details within a video, and engage in free-form dialogues with humans .

With the rapid development of Large Language Models (LLMs) , Large Vision Language Models (LVLMs)  have also seen significant advancements. Typical video-language models developed by researchers utilize frame-level  or clip-level  visual features extracted by vision encoders , align these features with language embeddings via a projector, and process these embeddings with a fine-tuned large language encoder . The models are fine-tuned with video instruction data and quantitatively assessed on free-form VideoQA benchmarks . The current evaluation of Video-LLMs is characterized by the following limitations:1. **Short Videos**: Existing VideoQA datasets primarily consist of short videos, typically lasting less than a minute. Meanwhile, most web video content spans several minutes or longer, creating a discrepancy between the evaluation benchmark and real-world application scenarios.
2. **Limited Capabilities**: Current VideoQA benchmarks are limited to several basic video tasks , including concept existence, object relationship recognition, and activity recognition. There are more fine-grained perception and reasoning capabilities  not encompassed by existing benchmarks.
3. **Biased Evaluation**: Existing evaluation paradigms employ GPT-3.5 to score open-ended answers generated by video-language models. Our preliminary study indicates that GPT-3.5-based evaluation is less accurate and exhibits significant discrepancy relative to human preferences, diminishing the credibility of the evaluation results.

To address these problems, we develop a new VideoQA benchmark, **MMBench-Video**, to evaluate the effectiveness of LVLMs in video understanding. It incorporates approximately 600 web videos with rich context from YouTube, spanning 16 major categories, including News, Sports, _etc._, covering most video topics people watch in their daily lives. Each video ranges in duration from 30 secs to 6 mins, to accommodate the evaluation of video understanding capabilities on longer videos. The benchmark includes roughly 2,000 original question-answer (QA) pairs, contributed by volunteers, covering a total of 26 fine-grained capabilities. During dataset collection, we implement quality control strategies to explicitly increase the proportion of temporal indispensable questions1. Quantitative statistics show that MMBench-Video significantly differs from existing benchmarks in terms of temporal duration, context richness, and temporal indispensability.

During evaluation, an LVLM produces free-form responses to visual questions. Given the variability in the lengths and styles of ground-truth answers, accurately assessing these responses presents a significant challenge. In light of the limitations observed in previous evaluations powered by GPT-3.5, we propose the use of the more powerful GPT-4  for automated scoring. This approach prioritizes semantic similarity while overlooking minor discrepancies in language organization. Employing a carefully crafted evaluation prompt, our GPT-4-based evaluation exhibits improved quality in terms of accuracy, consistency, and alignment with human judgment.

Based on MMBench-Video, we perform a thorough evaluation of mainstream LVLMs, including open-source video-language models (Video-LLMs), as well as both open-source and proprietary LVLMs for image understanding. We report their performance across diverse capabilities, as depicted in Fig. 1. The performance rankings enable direct comparisons between models, revealing critical insights into

Figure 1: **Comparing mainstream LVLMs on MMBench-Video. Two radar graphs illustrate the performance for each coarse (L-2) and each fine-grained (L-3) capability, respectively.**

their limitations. Surprisingly, existing Video-LLMs exhibit subpar performance on MMBench-Video, significantly underperforming proprietary LVLMs and even lagging behind open-source LVLMs, such as Idefics2  and InternVL-Chat-v1.5 . To further investigate these models' capabilities, we employ image VQA benchmarks to assess their image understanding skills, again observing a substantial gap between Video-LLMs and the state-of-the-art LVLMs. The comprehensive assessment underscores the significant performance disparities between Video-LLMs and leading LVLMs in both spatial and temporal understanding, highlighting areas requiring future improvement.

In summary, the contributions of this work are as follows:

\(\)**Innovative VideoQA Benchmark:** MMBench-Video features long-form, diverse videos sourced from the web, encompassing a broad spectrum of topics. It includes original, high-quality visual questions crafted by volunteers, spanning dozens of fine-grained capabilities.

\(\)**Enhanced Scoring Methodology:** We assess the limitations of using low-quality LLMs, such as GPT-3.5, for scoring model responses. To address this, we implement a GPT-4-based evaluation paradigm, which offers superior accuracy, consistency, and a closer alignment with human judgments.

\(\)**In-depth Evaluation:** Our comprehensive assessment of various LVLMs on MMBench-Video reveals detailed insights into their performance across multiple fine-grained capabilities. The results underscore the current limitations of Video-LLMs in spatial and temporal understanding, guiding future research and development.

## 2 Related Work

### Large Vision-Language Models

The success of Large Language Models (LLMs) such as GPTs [47; 7; 42] and LLaMA [52; 53] has spurred significant advancements in Large Vision-Language Models (LVLMs). Flamingo  has demonstrated impressive few-shot capabilities by integrating gated cross-attention blocks to connect pre-trained vision and language models. BLIP [30; 17] employs a Querying Transformer to bridge the modality gap between a frozen image encoder and a language encoder. LLaVA  leverages GPT-4 to create instruction-following data for vision-language tuning, with its learning paradigm and instruction tuning corpus being widely adopted by subsequent works [36; 11; 1; 16]. In the realm of video-language models, Video-ChatGPT  aligns frame-level vision features with language embeddings via a linear projector, whereas VideoChat  utilizes a learnable Q-former, inspired by BLIP-2. Subsequent works like Video-LLaMA  integrate audio features, and Video-LLaVA  learns from a mixed dataset of images and videos. Additionally, proprietary APIs such as GPT-[4v/4o] , Gemini , and Reka  have been made publicly available, supporting various input formats including single or multiple images. We present a comprehensive evaluation of existing LVLMs, encompassing Video-LLMs as well as open-source and proprietary LVLMs for images, using the proposed MMBench-Video to provide a detailed landscape of their capabilities.

### Video Question Answering

Video Question Answering (VideoQA) is a critical method for assessing the depth of understanding that models possess regarding video content. The research community has progressively developed a wide array of VideoQA benchmarks, spanning various visual domains such as movies , TV shows [29; 22], video games , synthetic scenarios , and egocentric videos . These benchmarks typically assess models trained on their respective training sets, demanding concise answers for evaluation. However, Large Vision and Language Models (LVLMs), which are often not trained on domain-specific data, face challenges in adapting to these benchmarks due to their diverse answer styles. To mitigate this, Video-ChatGPT  employs GPT-3.5 as a scoring mechanism for free-form responses from VLMs. The method was applied to evaluate several popular benchmarks [62; 25; 56], covering topics including concept existence, objection relationship, and activity recognition. Despite its broad adoption [34; 58; 48; 65], this approach is limited by suboptimal accuracy and stability, as well as poor alignment with human preferences. Additionally, those benchmarks primarily consist of short videos, which contrasts with the typical length of web videos. In response, we present MMBench-Video, a novel dataset tailored for longer videos, challenging models to generate detailed, free-form responses to complex questions. We adopt GPT-4-based evaluation, which improves correctness and robustness, offering a more stable evaluation strategy compared to previous methods.

## 3 MMBench-Video

In this section, we delve into the meticulous construction of MMBench-Video, outlining our strategic approach to video question selection, the conceptualization and design of a comprehensive capability taxonomy, and the innovative methods employed to enhance the temporal relevance and quality of questions. Additionally, we present detailed statistics of MMBench-Video and contrast it with existing VideoQA benchmarks, thereby illustrating its unique features and contributions to the field.

### Benchmark Construction

**Video Collection.** To create a VideoQA benchmark, a prevalent approach involves generating question-answer pairs for videos sourced from existing datasets. For example, MSRVTT-QA and MSVD-QA are derived from video retrieval datasets [57; 10], while ActivityNet-QA is constructed using an action recognition dataset . Most existing VideoQA datasets are limited to short videos with a constrained number of shots, exhibiting limited diversity in content. To develop a benchmark that more closely mirrors the web video content commonly consumed by viewers, we propose the creation of a long-form, multi-shot VideoQA benchmark. The benchmark draws its content directly from YouTube, offering several distinct advantages. Firstly, YouTube's extensive metadata, including video titles, click metrics, and subtitles, provides valuable context for video understanding. Secondly, as a leading global streaming platform, YouTube's vast user base ensures the dataset's diversity.

Drawing inspiration from the YouTube-8M  labels, our categorization scheme encompasses 16 major categories (Fig. 3), spanning from engaging topics like 'Entertainment and Sports' to enlightening subjects such as 'Science and Knowledge'. Volunteers are instructed to navigate through YouTube and collect videos that align with these designated categories. In line with our objective to amass long-form content, volunteers are directed to disregard videos with durations of less than 30 seconds. Although we impose no upper limit on the length of the web videos collected, all

Figure 2: **Overview of ability dimensions in MMBench-Video. Currently, MMBench-Video incorporates three levels of ability dimensions (L-1 to L-3), encompassing 26 distinct leaf abilities.**question-answer pairs composed for a video will be derived from a clip no longer than 6 minutes. This helps maintain a practical balance between video duration and the task complexity.

**Capability Taxonomy.** Inspired by MMBench , we have developped a 3-level (L-1 to L-3) hierarchical capability taxonomy (Fig. 2). The top level encompasses two broad capabilities: Perception and Reasoning. Besides the six L-2 capabilities inherited from MMBench, we further introduce three additional L-2 capabilities specific to MMBench-Video: Hallucination, Commonsense Reasoning, and Temporal Reasoning. **Hallucination** assesses whether a model is prone to generating content that includes misleading or inaccurate information. **Commonsense Reasoning** evaluates a model's ability to integrate necessary commonsense knowledge into its reasoning processes. **Temporal Reasoning** examines a model's proficiency in understanding the relationships between events unfolding at different video points. This taxonomy comprises a total of 26 leaf capabilities, which collectively address a comprehensive spectrum of cognitive processes involved in video comprehension.

**Composing Questions and Answers.** A well-known issue in existing VideoQA benchmarks is the prevalence of non-temporal questions, which are those that can be accurately answered based on nearly any frame within a video, rendering them effectively'static'. These questions fail to adequately assess a model's ability to temporal understanding. In the curation of MMBench-Video, we prioritize questions that necessitate temporal reasoning and strive to minimize the occurrence of static questions. Recognizing the necessity of evaluating certain coarse perception capabilities, such as Video Style and Video Topic, it is impractical to entirely eliminate static questions. Instead, we focus on significantly reducing their proportion within the benchmark.

In MMBench-Video, each video is accompanied by multiple independent questions designed to assess one or more specific leaf capabilities. For instance, a question that requires identifying and counting a particular type of object would evaluate both Object Recognition and Counting capabilities. To ensure the quality and relevance of the questions and their corresponding answers, volunteers involved in the collection process are provided with the following five guidelines to adhere to:

1. Each question should **evaluate one or multiple leaf capabilities** within the established taxonomy.
2. You are encouraged to formulate **temporal indispensable questions**, as long as it's feasible for the corresponding video content and capability category.
3. **Avoid including specific timestamps** in the questions, such as "at 03:20 in the video". Please use relative expressions like "at the end of the video" or "before/after a specific event" instead.
4. The questions should be **free-form** and exhibit **linguistic diversified**. Besides standard formats like _What/Who/How_, questions can also adopt a conversational style2.

5. Please provide **informative and detailed answers** for each question.

All generated question-answer pairs in MMBench-Video will be subjected to a meticulous cross-validation process to confirm their accuracy and adherence to the established guidelines. In addition to this, we implement an LVLM-based filtering mechanism to identify and eliminate a portion of static questions, as detailed in the supplementary material. The final MMBench-Video dataset comprises a diverse selection of web videos sourced from YouTube, accompanied by human-composed, original question-answer pairs designed to assess a comprehensive array of fine-grained capabilities.

**Evaluation Paradigm.** Given the varied length and style of ground-truth answers, automated robust evaluation that aligns with human judgments can be challenging. To address this, we propose a 3-grade marking scheme and utilize GPT-4  as our adjudicator. GPT-4 assigns a score from 0 to 3 based

   Benchmarks & QA pairs & Number of & Question Length & Answer Length & Video Duration & Shot Number \\  & Generation & Capabilities & mean(std) words & mean(std) words & mean(std) sec & mean(std) \\  MSVD-QA  & Automatic & 2 & 6.6(2.5) & 1.0(0.0) & 9.8(6.6) & 2.4(3.4) \\ MSRVTT-QA  & Automatic & 2 & 7.4(3.4) & 1.0(0.0) & 15.1(5.2) & 3.4(2.9) \\ TQIF-QA  & Automatic/Human & 4 & 9.7(2.3) & 1.5(0.9) & 3.7(2.0) & 1.2(1.4) \\ ActivityNet-QA  & Human & 3 & 8.9(2.4) & 1.3(0.7) & 111.5(66.1) & 12.9(20.9) \\  MMBench-Video & Human & **26** & **10.9**(4.1) & **8.4**(7.7) & **165.4**(80.7) & **32.6**(33.5) \\   

Table 1: **Comparing the statistics of MMBench-Video and other widely adopted VideoQA benchmarks.** When reporting the video statistics, we follow the format of “mean value (standard deviation)”.

on the content similarity between the model's output and the ground truth. Our experiments show that this evaluation framework exhibits strong consistency and alignment with human assessments.

### Dataset Statistics

MMBench-Video comprises **609** video clips across 16 major categories, as depicted in Fig. 3), with durations spanning from 30 seconds to 6 minutes. The dataset has an average video length of **165 seconds**, totaling **28 hours** in aggregated duration. The duration distribution of the clips within MMBench-Video is illustrated in Fig. 4. The dataset includes **1,998** question-answer (QA) pairs, with each QA assessing one or multiple capabilities of a vision-language model. The distribution of QAs corresponding to each capability is visualized in Fig. 2. To highlight the distinct value of MMBench-Video, we compare its statistics with those of existing VideoQA benchmarks:

**Duration & Shot Numbers3.** MMBench-Video is specifically designed as a **long-form, multi-shot** video dataset. As indicated in Tab. 1, our dataset boasts a substantially greater average duration than existing benchmarks. As shown in Fig. 5, videos in our benchmark display a long-tail distribution in shot numbers, with a maximum of 210 shots. This significantly surpasses all other benchmarks in average shot count.

**Linguistic Characteristics of QAs.** MMBench-Video features free-form video QA with rich linguistic diversity. In benchmarks such as MSVD-QA and MSRVTT-QA, questions are automatically generated and invariably begin with pronouns such as 'what', 'who', _etc._ Conversely, a significant proportion of questions in MMBench-Video are framed in a conversational manner, enhancing linguistic diversity (further details are available in the supplementary materials). Regarding answers, previous VideoQA benchmarks often provide responses that are limited to a single word or a brief phrase. In contrast, MMBench-Video strives to offer more comprehensive answers that extend beyond a single word. This is evident in the distribution of answer lengths, as shown in Tab. 1.

**Capability Coverage.** Existing benchmarks typically cover only a limited set of fine-grained capabilities  and often lack an explicit capability taxonomy. For instance, the majority of questions in MSVD-QA and MSRVTT-QA assess the ability to determine the existence of concepts(such as humans or objects) and to recognize relationships between objects. In contrast, ActivityNet-QA and TGIF-QA extend this by including assessments of activity recognition and repetition counting. In MMBench-Video, we have established a comprehensive taxonomy encompassing 26 fine-grained capabilities, with each capability being evaluated using dozens to hundreds of original QAs.

Temporal Indispensability.In contrast to existing VideoQA benchmarks, MMBench-Video is designed to be temporal indispensable. In a preliminary study, we find that a great proportion of QAs in existing datasets can be correctly answered by LVLMs without providing the temporal context. The underlying factors can be categorized into two primary ones: (1) The brevity of source videos, characterized by the limited number of shots, allows for its content to be adequately represented by a single frame. (2) Many of the QAs are too simplistic and can be answered through guesswork rather than comprehension. For instance, MSVD-QA and MSRVTT-QA are replete with 'who' questions, which are commonly answered with general terms like'someone','man', or 'woman'. In MMBench-Video, we have made significant efforts to mitigate these factors.

To quantitatively measure the temporal indispensability of each VideoQA benchmark, we randomly sample 1000 QAs from orginal VideoQA datasets and comprehensive video understanding benchmark, and conduct a study on the subsets as well as MMBench-Video. We evaluate GPT-40 (by far the most powerful LVLM) on these benchmarks under 1-frame and 8-frame settings, and present the results in Tab. 2. Notably, GPT-40 using a 1-frame input achieves a normalized score of approximately 50%, retaining over 75% of its performance compared to an 8-frame input across previous VideoQA datasets. Even the latest benchmarks for comprehensive video understanding struggle to fully assess a model's temporal capabilities. In contrast, when assessed on the MMBench-Video, GPT-40 using a 1-frame input preserves only 47.8% of its efficacy compared to its performance with an 8-frame input, yielding a normalized score of just 26.0%. This marked difference underscores the temporal importance of MMBench-Video.

## 4 Experiment

Utilizing MMBench-Video, we assess a diverse array of large vision-language models (LVLMs), encompassing Video-LLMs and image-based LVLMs, both open-source and proprietary. For Video-LLMs, we utilize the default hyperparameters specified in their respective open-source implementations for inference. For image-based LVLMs, we conduct evaluations based on VLMEvalKit , employ greedy decoding during inference and cap the maximum number of output tokens at 512.

### Main Results

Open-Source Video-LLMs.We first identify and evaluate representative open-source Video-LLMs using MMBench-Video. Adhering to their default settings, these Video-LLMs process a sequence of video frames, with the number of frames varying from eight to dozens. Interestingly, we observe that all Video-LLMs exhibit comparably subpar performance on MMBench-Video, despite notable performance disparities on other benchmarks. For instance, VideoChat2 surpasses Video-ChatGPT by 18% on the MSVD-QA score (3.9 vs. 3.3), yet the performance gap narrows to just 6% on the MMBench-Video score (0.99 vs. 0.93). All video LLMs attain an average score close to 1 (out of a total of 3), with the top-performing model LLaVA-NeXT-Video  reaching a mere 1.14. These findings suggest that the current state of video models' proficiency in understanding MMBench-Video is nascent, underscoring the challenges and emphasizing the necessity for advancements in video LLMs to enhance their capability and effectiveness in interpreting varied video content.

Open-Source LVLMs for Images.A significant number of LVLMs [18; 37; 6; 59; 28; 13; 44; 54] have been developed to comprehend image content and execute visual reasoning tasks. During our evaluation, we focused on LVLMs that support the multi-image inference interface. We assess several prominent open-source LVLMs: Idefics2-8B , Qwen-VL-Chat , mPLUG-Owl2 , InternVL-Chat-v1.5 , InternVL2  and VILA1.5  using MMBench-Video. To ascertain the models' ability to effectively leverage multiple input frames, we evaluated them under two distinct settings: 1-frame and 8-frame inputs. Results in Tab. 3 indicate that all models, except Qwen-VL-Chat, exhibit a substantial enhancement in performance when utilizing 8 frames compared to a single frame. Notably, VILA-1.5-40B emerges as the top performer, achieving an impressive average score of 1.61 with 14 frames as inputs, significantly surpassing all other evaluated video LLMs.

**Proprietary LVLMs for Images.** Unlike their open-source counterparts, most proprietary LVLMs accept arbitrary interleaved images and text as input. We evaluate several proprietary LVLMs, including Claude-3v, Gemini-Pro-v[1.0/1.5], GPT-4v, and GPT-4o on MMBench-Video with varying numbers of frames. Claude-3v struggles with 8-frame inputs and is only evaluated under the 4-frame setting. As anticipated, it exhibits the poorest performance among proprietary LVLMs when handling multiple frames. In contrast, other proprietary models demonstrate notably superior performance compared to the state-of-the-art open-source VILA-1.5. Particularly impressive is GPT-4o, which, when processing 16 frames, achieves an outstanding overall score of **1.86**. This result positioned GPT-4o 63% ahead of the best open-source video LLM and 16% ahead of the best open-source image LVLM. To assess the potential of advanced proprietary models, we experiment with the fps sampling method. By increasing the number of video frames, the model's perception improves as adjacent frames provide mutual support, resulting in more accurate interpretations. This approach also captures previously missed content, enhancing the model's ability to answer questions reliant on such information and boosting its reasoning skills. The only observed drawback is a slight increase in hallucination, possibly due to excessive video content leading to responses not grounded in reality.

### Performance of Video-LLMs on Image VQA Benchmarks

Intuitively, a Video-LLM is expected to not only possess all capabilities of an image-based LVLM but also exhibit video-specific competencies, such as future prediction or causal reasoning. In light of the underwhelming performance of Video-LLMs on MMBench-Video, we broaden our evaluation to include image VQA benchmarks to determine if these models have the necessary skills for comprehending static content. We evaluate five Video-LLMs on two extensive image VQA benchmarks: MMBench  and MMStar . To accommodate the input format of Video-LLMs, we create pseudo video clips by duplicating static frames, which then serves as the input for the evaluation. In Tab. 4, we list the performance of Video-LLMs alongside several representative image LVLMs for comparative analysis. On both benchmarks, existing Video-LLMs exhibit subpar performance. Notably, top-performing Video-LLMs such as PLLaVA and Video-LLaVA show performance that is either on par with or inferior to LLaVA-v1.5-7B, a rudimentary baseline for image multimodal understanding, and significantly trail behind the state-of-the-art image LVLM,

   &  &  \\   & **FP-S** & **FP-C** & **CP** & **LR** & **AR** & **RR** & **Overall** & **CP** & **FP** & **IR** & **LR** & **Math** & **ST** & **Overall** \\   \\  Video-Chao(OPT & 41.87 & 27.37 & 32.87 & 13.71 & 53.05 & 30.46 & 34.50 & 40.80 & 24.80 & 36.00 & 26.00 & 28.00 & 22.40 & 29.67 \\ Video-LAVA & 57.44 & 42.46 & 62.98 & 14.52 & 68.90 & 43.10 & 52.32 & 55.20 & 20.40 & 37.60 & 25.20 & 25.60 & 24.00 & 31.33 \\ Chat-UnViN & 47.75 & 38.75 & 57.18 & 9.68 & 62.19 & 33.91 & 45.04 & 50.00 & 30.80 & 42.80 & 30.40 & 30.00 & 24.40 & 34.73 \\ VideoChao(22) & 24.91 & 30.72 & 54.14 & 7.26 & 54.88 & 23.18 & 41.02 & 47.60 & 22.80 & 32.80 & 27.20 & 26.40 & 13.20 & 28.33 \\ PLLaVA-7B & 59.17 & 40.78 & 60.50 & 17.74 & 58.54 & 58.05 & 25.79 & 53.60 & 34.40 & 48.00 & 32.40 & 30.00 & 17.20 & 34.73 \\   \\  MiniCRV-V2 & 78.89 & 50.84 & 72.93 & 26.61 & 57.00 & 65.52 & 66.02 & 58.00 & 32.40 & 50.00 & 38.40 & 32.80 & 22.80 & 39.07 \\ LLAVA-v1.5-7B & 69.90 & 56.98 & 70.17 & 25.81 & 67.07 & 53.45 & 61.38 & 57.20 & 24.40 & 41.60 & 28.40 & 26.40 & 20.40 & 33.07 \\ Imroll-U-curv1.5 & 85.88 & 73.18 & 89.94 & 86.06 & 85.98 & 89.46 & 79.95 & 70.40 & 52.80 & 62.50 & 54.80 & 56.00 & 39.60 & 57.07 \\ Idofei2-8B & 81.31 & 65.36 & 73.20 & 41.94 & 80.49 & 76.44 & 72.29 & 66.00 & 42.60 & 42.60 & 49.60 & 40.00 & 37.20 & 49.47 \\ Phi-Vision & 78.89 & 61.45 & 76.80 & 47.58 & 79.27 & 74.14 & 72.29 & 60.00 & 38.80 & 59.20 & 45.20 & 42.40 & 40.80 & 47.73 \\  

Table 4: **Comparison of Image Models and Video Models on MMBench and MMStar. We follow the official practice to perform evaluation on these two benchmarks. For MMBench, we report the results on MMBench-DEV-EN-v1.1. We adopt the abbreviations for capabilities that are defined in the original papers.**

   &  &  &  &  \\   & & **Mean** & **CP** & **FP-S** & **FP-C** & **HL** & **Mean** & **LR** & **AR** & **RR** & **CSR** & **TR** & **Mean** \\   & ✗ & 0.29 & 0.03 & 0.10 & 0.08 & 1.84 & 0.16 & 0.38 & 0.51 & 0.46 & 0.18 & 0.79 & 0.54 \\  & ✗ & 1.22 & 1.17 & 1.18 & 0.87 & 1.90 & 1.17 & 0.69 & 1.40 & 1.33 & 0.71 & 1.62 & 1.31 \\   & ✗ & 1.66 & 1.90 & 1.63 & 1.52 & 1.88 & 1.68 & 1.61 & 1.87 & 1.50 & 1.52 & 1.65 & 1.63 \\  & ✗ & 1.94 & 1.98 & 1.92 & 1.71 & 1.72 & 1.90 & 2.05 & 2.07 & 1.97 & 2.00 & 1.99 & 2.01 \\   & ✗ & 1.90 & 2.13 & 1.86 & 1.56 & 2.20 & 1.90 & 2.26 & 2.04 & 1.47 & 2.00 & 1.85 & 1.91 \\  & ✗ & 2.05 & 2.14 & 2.07 & 1.79 & 1.64 & 2.04 & 2.31 & 2.22 & 1.78 & 2.33 & 2.05 & 2.12 \\  

Table 5: **GPT-4o’s performance can be further improved by incorporating YouTube generated subtitles. We report the performance on a subset of MMBench-Video, for which the auto generated subtitles are available.**InternVL-v1.5. This evaluation underscores the current limitations in the spatial understanding capabilities of Video-LLMs.

### Incorporating Speech Further Improves Proprietary LVLMs

Video inherently comprises both visual and audio signals. However, the majority of existing LVLMs for video understanding predominantly focus on visual features, often neglecting the valuable information embedded in audio signals. To explore the potential impact of integrating audio features on video understanding, we conducted experiments using video title tracks (VTT) sourced from YouTube, which are automatically generated through speech recognition techniques. We incorporate these subtitles into the prompt as supplementary context. Experimental results in Tab. 5 reveal that the inclusion of audio/speech information enhances the performance of the state-of-the-art proprietary model, GPT-4o. The subtitles offer a rich source of high-density information, facilitating the LLM's ability to accurately address the questions, thereby leading to a comprehensive performance improvement. Nonetheless, the increased information richness also heightens the risk of hallucinations, where the model may produce responses about non-existent content. The effectiveness hinges on the information density and the level of redundancy, necessitating a careful balance in applications.

### The Superior Performance of GPT-4 as a Judge

Due to the discrepancy between the predictions of Video-LLM and the ground truth answers, existing VideoQA benchmarks largely rely on a judge model to assess the model responses. The capability of judge models can significantly influence the final results. To quantitatively study the impact of judge models, we utilize different versions of GPT-3.5 and GPT-4 for evaluation and report the results in Tab. 6. We observe that GPT-3.5 tends to assign higher scores (typically 2 and 3), which can result in inflated final results and potential inaccuracies. To investigate the alignment with human preferences across different judge models, we conduct study based on a randomly selected subset of 100 questions. Two of the authors manually rate the responses from Video-LLaVA and GPT-4o, and we then report the mean absolute error between different judge models and the averaged human ratings. Tab. 7 shows that GPT-3.5 exhibits a significantly larger discrepancy with human preferences and greater inter-version variance. In contrast, Qwen2-72B-Instruct aligned more closely with human ratings, suggesting the potential of using advanced open-source LLMs as evaluators. Compared to the other two, GPT-4 demonstrated greater resistance to manipulation and fairly evaluated predictions without bias, supporting its role as an evaluator for the MMBench-Video benchmark.

## 5 Conclusion

This work introduces MMBench-Video, a novel long-form, multi-shot VideoQA benchmark specifically designed to evaluate the capabilities of LVLMs in understanding video content. MMBench-Video encompasses a diverse range of video topics and fine-grained capabilities. Extensive evaluations on MMBench-Video allow us to identify significant performance limitations among existing Video-LLMs in both spatial and temporal understanding.

  
**Judge Model** & **LVLM** & **Video-LLaVA** & **GPT-4o** \\ 
**GPT-3.5-Turbo** & **1106** & 2.09 & 2.45 \\
**6613** & 1.80 & 2.11 \\ 
**GPT-4-Turbo** & **1106** & 1.05 & 1.62 \\
**0125** & 0.90 & 1.61 \\ 
**Qwen2-72B-Instruct** & 1.15 & 1.80 \\   

Table 6: **Evaluation results obtained with different GPT judges on MMBench-Video. The overall mean scores are reported.**

  
**Judge Model** & **LVLM** & **Video-LLaVA** & **GPT-4o** \\ 
**GPT-3.5-Turbo** & **1106** & 0.98 & 0.815 \\
**0613** & 0.89 & 0.685 \\ 
**GPT-4-Turbo** & **1106** & 0.36 & 0.295 \\
**0125** & 0.36 & 0.255 \\ 
**Qwen2-72B-Instruct** & 0.41 & 0.320 \\   

Table 7: **The mean absolute error (MAE) of different GPT Judges with human preferences on a randomly selected subset.**