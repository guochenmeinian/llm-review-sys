# Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length

Xuezhe Ma\({}^{}\)  Xiaomeng Yang\({}^{}\)  Wenhan Xiong\({}^{}\)  Beidi Chen\({}^{}\)  Lili Yu\({}^{}\)

Hao Zhang\({}^{}\)  Jonathan May\({}^{}\)  Luke Zettlemoyer\({}^{}\)  Omer Levy\({}^{}\)  Chunting Zhou\({}^{}\)

\({}^{}\)AI at Meta

\({}^{}\)University of Southern California

\({}^{}\)Carnegie Mellon University

\({}^{}\)University of California San Diego

Equal Contribution. Correspondence to chuntinz@meta.com

###### Abstract

The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, an neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including _complex exponential moving average (CEMA)_, _timestep normalization_ layer, _normalized attention_ mechanism and _pre-norm with two-hop residual_ configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). The improvements of Megalodon over Transformers are robust throughout a range of benchmarks across different tasks and modalities.

**Code**: https://github.com/XuezheMax/megalodon

## 1 Introduction

In many real-world applications, such as multi-turn conversation, long-document comprehension, and video generation, large language models (LLMs) must efficiently process long sequential data, understand internal long-range dynamics, and generate coherent output. The Transformer architecture (Vaswani et al., 2017), despite its remarkable capabilities, faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling (Wang et al., 2024; Zhou et al., 2024). Even with recently proposed distributed attention solutions (Li et al., 2023; Liu et al., 2024), computing a single training step of a 7B parameter model over a 1M-token sequence is more than 100 times slower than performing the equivalent computation using 256 separate sequences of 4K tokens each.

Techniques like efficient attention mechanisms (Tay et al., 2020; Ma et al., 2021) and structured state space models (Gu et al., 2022; Poli et al., 2023; Gu and Dao, 2023) have been introduced to overcome these limitations, aiming to enhance scalability and performance. However, the practical application of these methods still falls short of Transformers (Tay et al., 2022; Gu and Dao, 2023). This work introduces an unlimited context model that outperforms the canonical Transformer architecture on real-world language modeling.

We introduce Megalodon, an improved Mega architecture (Ma et al., 2023), which harnesses the gated attention mechanism with the classical exponential moving average (EMA) (Hunter, 1986) approach (SS2). To further improve the capability and efficiency of Megalodon on large-scale long-context pretraining, we propose multiple novel technical components. First, Megalodon introduces the _complex exponential moving average (CEMA)_ component, which extends the multi-dimensional damped EMA in Mega to the complex domain (SS3.1). Then, Megalodon proposes the _timestep normalization_ layer, which generalizes the group normalization layer (Wu and He, 2018) to auto-regressive sequence modeling tasks to allow normalization along the sequential dimension (SS3.2). To improve large-scale pretraining stability, Megalodon further proposes _normalized attention_ (SS3.3), together with _pre-norm with two-hop residual_ configuration by modifying the widely-adopted pre- and post-normalization methods (SS3.4). By simply chunking input sequences into fixed blocks, as is done in Mega-chunk (Ma et al., 2023), Megalodon achieves linear computational and memory complexity in both model training and inference.

Empirically, we demonstrate the potential of Megalodon as a general architecture for modeling long sequences, by evaluating its performance across multiple scales of language modeling, as well as downstream domain-specific tasks. Through a direct comparison with Llama2, while controlling for data and compute, Megalodon-7B significantly outperforms the state-of-the-art variant of Transformer used to train Llama2-7B (Touvron et al., 2023) on both training perplexity (Figure 1) and across downstream benchmarks (Table 1). Evaluation on long-context modeling, including perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al., 2020) prove Megalodon's ability to model sequences of unlimited length. Additional experimental results on small/medium-scale benchmarks, including LRA (Tay et al., 2021), ImageNet (Deng et al., 2009), Speech Commands (Warden, 2018), WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019), demonstrate the robust improvements of Megalodon across scales and modalities.

  
**Model** & **Size** & **Tokens** & **Context** & **MMLU** & **BoolQ** & **HellaSw** & **PIQA** & **SIQA** & **WinoG** & **Arce-** & **Avc-** & **NQ** & **TQA** \\  Mamba & 3B & 0.67 & 2K & 26.2 & 71.0 & 71.0 & 78.1 & – & 65.9 & 68.2 & 41.7 & – & – \\ RwKV & 7B & 1.1T & 4K & – & – & 70.8 & 77.3 & – & 68.4 & 74.9 & 46.1 & – & – \\  MPT & 7B & 1T & 4K & 26.8 & 75.0 & 76.4 & 80.6 & 48.5 & 68.3 & 70.2 & 42.6 & 20.8 & 50.4 \\ Mistral & 7B & – & 16K & 60.1 & **83.2** & **81.3** & **82.2** & 47.0 & **74.2** & 80.0 & **54.9** & 23.2 & 62.5 \\ Gemma & 8B & 6T & 8K & **64.3** & **83.2** & 81.2 & 81.2 & **51.8** & 72.3 & **81.5** & 53.2 & 23.0 & 63.4 \\ Llama2 & 13B & 2T & 4K & 54.8 & 81.7 & 80.7 & 80.5 & 50.3 & 72.8 & 77.3 & 49.4 & **31.2** & **65.1** \\  Llama2 & 7B & 2T & 4K & 45.3 & 77.4 & 77.2 & 78.8 & 48.3 & 69.2 & 75.2 & 45.9 & 25.7 & 58.5 \\ Megalodon & 7B & 2T & 32K & 49.8 & 80.5 & 77.5 & 80.1 & 49.6 & 71.4 & 79.8 & 53.1 & 25.7 & 60.5 \\   

Table 1: **Performance on standard academic benchmarks, compared to open-source base models. We reported model size, context length and total data tokens during model pretraining. – indicates that the number was not reported in the original paper.**

Figure 1: **Negative log-likelihood for Megalodon-7B, Llama2-7B and Llama2-13B.**

Background: Moving Average Equipped Gated Attention (Mega)

In this section, we setup notations, briefly review the key components in the Mega architecture (Ma et al., 2023), and discuss the existing problems in Mega.

Following the notations in Mega, we use \(=\{_{1},_{2},,_{n}\}^{n  d}\) and \(=\{_{1},_{2},,_{n}\}^{n  d}\) to denote the input and output sequences with length \(n\), and assume the representations of the input and output sequences have the same dimension \(d\).

### Multi-dimensional Damped EMA

Mega embeds an EMA component into the calculation of the attention matrix to incorporate inductive biases across the timestep dimension. Concretely, the multi-dimensional damped EMA first expands each dimension of the input sequence \(\) individually into \(h\) dimensions via an expansion matrix \(^{d h}\), then applies damped EMA to the \(h\)-dimensional hidden space. Formally, for each dimension \(j\{1,2,,d\}\):

\[_{t}^{(j)} =_{j}_{t,j}\] (1) \[_{t}^{(j)} =_{j}_{t}^{(j)}+(1-_{j} _{j})_{t-1}^{(j)}\] \[_{t,j} =_{j}^{T}_{t}^{(j)}\]

where \(_{t}^{(j)}^{h}\) is the expanded \(h\)-dimensional vector for the \(j\)-th dimension at timestep \(t\). \((0,1)^{d h}\), \((0,1)^{d h}\) are the decaying and damping factors, respectively. \(_{t}^{(j)}^{h}\) is the EMA hidden state for the \(j\)-th dimension at timestep \(t\). \(^{d h}\) is the projection matrix to map the \(h\)-dimensional hidden state back to \(1\)-dimensional output \(_{t,j}\).

### Moving Average Equipped Gated Attention

In the gated attention mechanism in Mega, the output from EMA (1) is used to compute the shared representation (Hua et al., 2022), because it encodes contextual information through EMA. Subsequently, Mega introduces the reset gate, the update gate, and computes the candidate activation with the update gate and the residual connection. The technical details are provided in Appendix A.

### Existing Problems in Mega

To reduce the quadratic complexity in the full attention mechanism, Mega simply split the sequences of queries, keys and values in (14-16) into chunks of length \(c\). The attention in (17) is individually applied to each chunk, yielding linear complexity \(O(kc^{2})=O(nc)\). Technically, the EMA sub-layer in Mega helps capture local contextual information near each token, mitigating the problem of losing contextual information beyond chunk boundaries in the chunk-wise attention.

Despite the impressive successes of Mega, it still suffers its own problems: i) the performance of Mega with chunk-wise attention still fails behind the one with full attention, due to the limited expressiveness of the EMA sub-layer in Mega. ii) for different tasks and/or data types, there are architectural divergences in the final Mega architectures. For example, different normalization layers, normalization patterns (pre-norm vs. post-norm) and attention functions (\(f()\) in (17)) are applied to different data types (see Ma et al. (2023) for details). iii) There are no empirical evidences showing that Mega is scalable for large-scale pretraining.

## 3 Megalodon

To address the aforementioned problems of Mega, in this section we describe the novel technical advancements of Megalodon.

### CEMA: Extending Multi-dimensional Damped EMA to Complex Domain

As discussed in Ma et al. (2023), the EMA component can be regarded as a simplified state space model with diagonal state matrix. Directly inspired from Gu et al. (2022), as almost all matrices diagonalize over the complex plane, a straight-forward idea to improve EMA capability is to extend to work over the complex number system \(\). We propose the _complex exponential moving average (CEMA)_, which re-writes Eq. (1):

\[_{t}^{(j)} =_{j}(_{j}+i_{j})_{t }^{(j)}+(1-_{j}_{j})(_{j}+i_{j}) _{t-1}^{(j)}\] \[_{t,j} =(_{j}^{T}_{t}^{(j)})\] (2)

where \(\), \(^{d h}\) are the real number parameters same as in EMA. Different from EMA, \(^{d h}\) in CEMA are complex numbers. \(_{j}^{h},\ j\{1,2,,d\}\) are the \(h\) arguments. To uniformly space the \(h\) arguments over the period \(2\), we parameterize \(_{j}\) as:

\[_{j,k}=_{j}, k\{1,2,,h\}\] (3)

where the learnable parameter \(^{d}\) depicts the \(d\) base angles. By decaying the absolute value of each \(h_{t}\), CEMA preserves the decaying structure in kernel weights, which is a key principle to the success of convolutional models on long sequence modeling (Li et al., 2023c).

### Timestep Normalization

Despite the impressive performance of Layer Normalization combined with Transformer, it is obvious that layer normalization cannot directly reduce the internal covariate shift along the spatial dimension (a.k.a timestep or sequential dimension) (Ioffe and Szegedy, 2015). Group Normalization (Wu and He, 2018) normalizes hidden representations both along the timestep dimension and a subset of the feature dimension, which has obtained improvements over Layer Normalization on a range of computer vision tasks. However, it cannot be directly applied to Transformer on auto-regressive sequence modeling, due to the leakage of future information via the mean and variance across the timestep dimension.

In Megalodon, we extend Group Normalization to the auto-regressive case by computing the cumulative mean and variance. Formally, suppose an input sequence \(=\{_{1},_{2},,_{n}\}^{ n d}\), and \(k\) groups along the feature dimension with \(d_{g}=d/k\) elements per group. Then, the mean and variance of the first group at timestep \(t\{1,2,,n\}\) are:

\[_{t}=}_{i=1}^{t}_{j=1}^{d_{g}}x_{i,j},_ {t}^{2}=}_{i=1}^{t}_{j=1}^{d_{g}}(x_{i,j}-_{t})^{2}\] (4)

Figure 2 illustrates Layer Normalization and Timestep Normalization. To efficiently and precisely calculate the cumulative mean and variance in each timestep, we provide hardware-friendly implementation on modern hardware (GPU) (see Appendix B.1).

### Normalized Attention in Megalodon

Previous studies have investigated the saturation and instability issues in the original scaled dot-product attention (17). A number of novel techniques have emerged to modify the scaled dot-product attention, among which normalized attention mechanisms, such as (scaled-) cosine attention (Luo et al., 2018; Liu et al., 2022) and QK-normalization (Henry et al., 2020), have stood out for the simplicity and effectiveness.

Figure 2: **Normalization methods**. The elements in blue or pink are the regions to compute means and variances. We omit the batch dimension for simplicity.

Directly inspired from these normalized attention mechanisms, we propose the normalized attention mechanism specifically defined for Mega to improve its stability. Formally,

\[^{} =() ^{n d}\] (5) \[ =^{}W_{z}+b_{z},^{}=}{ \|\|} ^{n z}\] (6) \[ =_{q}^{}+_{q} ^{n z}\] (7) \[ =_{k}^{}+_{k} ^{n z}\] (8)

where \(\) and \(\) are computed by using the normalized shared representation \(^{}\) instead of \(\). Note that we remove the SiLU (Ramachandran et al., 2017) activation function \(_{}\) in (13), because the normalization on \(\) has incorporated non-linearity into \(^{}\). Then the attention operation in (17) has been changed to:

\[=f_{}(^{T}) ^{n v}\] (9)

As we use learnable \(_{q}\), \(_{k}\) in (7) and (8), we can remove the scaled term \(()\). In addition, we found that with the normalized attention, the softmax function \(f_{}\) obtains the best or at least comparable performance on different tasks and data modalities (see Appendix C). Hence, throughout this paper we use softmax as the default attention function.

### Pre-Norm with Two-hop Residual

Normalization configurations are crucial in stably training deep architectures, and pre-normalization (Xiong et al., 2020) has become the default normalization configuration because of its better convergence properties than post-normalization in the original Transformer architecture (Vaswani et al., 2017). However, extensive studies have investigated the instability issue of pre-normalization when scaling up model size (Davis et al., 2021; Liu et al., 2022). Formally, a Transformer-based block in pre-normalization can be formulated as (shown in Figure 3 (b)):

\[} =(())+\] \[ =((}))+}\] \[=((}))+( ())+\] (10)

where the output \(\) is the sum of the input \(\) and the output of each component in one block. Hence, the range and/or variance of \(\) keeps increasing for deeper blocks, causing the instability issue. In

Figure 3: Illustration of the Megalodon architecture. Figure (a) shows a sketch of one Megalodon layer. Figure (b) and (c) display the configurations of pre-norm and pre-norm with two-hop residual, respectively.

the original Mega architecture, the update gate \(\)(19) is used for a gated residual connection (21) to mitigate this problem (Parisotto et al., 2020; Xu et al., 2020). However, the update gate \(\) introduces more model parameters and the instability issue still exists when scaling up model size to 7 billion.

Megalodon introduces a new configuration named _pre-norm with two-hop residual_, which simply re-arranges the residual connections in each block (shown in Figure 3 (c):

\[} =(())+\] \[ =((}))+\] (11)

where the input \(\) is reused as the residual connection of the FFN layer. Since \(}\) is directly followed by a normalization layer, we remove the update gate \(\) and use standard residual connection. The graphical architecture of a Megalodon sub-layer is visualized in Figure 3 (a). Note that the Timestep Normalization is only applied before the attention layer. Before the FFN layer, we still use Layer Normalization. The reasons are two-fold: i) Layer Normalization is faster than Timestep Normalization; ii) the output vector of each token from the attention layer is a mixture of vectors from contextual tokens via attention weights. Hence, normalizing the attention output along the feature dimension is similar to indirectly normalize along the timestep dimension.

### 4-Dimensional Parallelism in Distributed LLM Pretraining

Efficient distributed training algorithm is essential to train a large-scale language model, and several parallelization mechanisms have been introduced. The three most commonly used parallelism strategies are data, tensor (Shoeybi et al., 2019) and pipeline parallelism (Huang et al., 2019). However, the 3-dimensional parallelism is still insufficient to scale up the context length of LLMs (Li et al., 2023; Liu et al., 2024).

Benefiting from the chunk-wise attention in Megalodon, we can efficiently parallelize it along the new timestep/sequence dimension, which is orthogonal to all the aforementioned three parallelism dimensions. In Megalodon, the only communications between devices in one chunk-parallel group are the last hidden state of CEMA and the cumulative mean and variance of Timestep Normalization in each block. Using asynchronous communication, we can minimize the overhead of chunk parallelization by hiding the communication costs in the computation of other components inside the same block and/or other blocks.

## 4 Experiments

To evaluate the scalability and efficiency of Megalodon on long-context sequence modeling, we scale up Megalodon to 7-billion model size and apply it to large-scale language model pretraining on 2 trillion tokens. We also conduct experiments on small/medium-scale sequence modeling benchmarks, including Long Range Arena (LRA) (Tay et al., 2021), raw speech classification on Speech Commands (Warden, 2018), image classification on ImageNet-1K (Deng et al., 2009), and language-modeling on WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019). 2 Empirically, Megalodon significantly outperforms all the state-of-the-art baseline models on these tasks across various data modalities.

### LLM Pretraining

Architectural DetailsIn our Megalodon-7B model, we adopt most of architectural hyperparameters from Llama2-7B to ensure fair comparison: Megalodon-7B consists of 32 blocks, with feature dimension \(d=4096\). Following Llama2, we use the SwiGLU activation function (Shazeer, 2020) in the feed-forward layer, and rotary positional embedding (RoPE, Su et al. (2021)). We set the attention chunk size \(c=4096\), which is the same as the pretraining context length in Llama2. Benefiting from the attention gate (\(\) in (18)), we use a much smaller number of attention heads \(h=4\) in Megalodon-7B, comparing to \(h=32\) in Llama2-7B. In addition, we apply pre-norm with two-hop residual (SS3.4), using Timestep Normalization (SS3.2) and Layer Normalization (Ba et al., 2016), while Llama2 models apply pre-normalization with RMSNorm (Zhang and Sennrich, 2019).

Data and Pretraining DetailsWe use the same mix of publicly available data from Llama2, ensuring that the model are trained on exactly the same 2-trillion tokens. We also use the same tokenizer as Llama2, whose vocabulary size is \(32\)K.

We trained Megalodon-7B using the AdamW optimizer (Loshchilov and Hutter, 2019), with \(_{1}=0.9\), \(_{2}=0.95\), \(=1e-8\). The learning rate is \(3.5e-4\) and cosine learning rate schedule is applied with warmup of \(2500\) steps. We use a weight decay of \(0.1\) and gradient clipping of \(1.0\), and no dropout is applied during training. The context length in pretraining is \(32\)K (4 attention chunks). The global batch size is 4M tokens, and is distributed on 256 NVIDIA A100 GPUs (16K tokens per A100). We set data parallel size to 128, chunk parallel size to 2 and tensor parallel size to 1.

Data and Computation EfficiencyWe evaluate the efficiency of Megalodon w.r.t both the data and computation perspectives. For data efficiency, we display the negative log-likelihood (NLL) for Megalodon-7B, Llama2-7B and Llama2-13B w.r.t processed tokens during training in Figure 1. Megalodon-7B obtains significantly better (lower) NLL than Llama2-7B under the same amount of training tokens, demonstrating better data efficiency. Moreover, Megalodon suffers less training spikes than the Transformer-based architecture in Llama2. Note that at the first 1/4 of the pretraining process (\(<500\)B tokens), the NLL of Megalodon-7B is slightly worse than Llama2-7B. We found that the main reason is that we increased the base \(\) of RoPE from \(10,000\) in Llama2 to \(100,000\) in Megalodon, which slows down model convergence at the beginning of the pretraining process. At the end, Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and Llama2-13B (1.67).

For computation efficiency, we conduct experiments of running Llama2-7B and Megalodon-7B using the same amount of computational resources and comparing their training speed under various context lengths. Specifically, we execute each experiment to train a model with global batch size 4M tokens distributed on 256 NVIDIA A100 GPUs (16K tokens per A100) and calculate the word/token per second (WPS) to measure the training speed. Figure 4 illustrates the average WPS per device of Llama2-7B and Megalodon-7B using 4K and 32K context lengths, respectively. For Llama2 models, we accelerate the computation of full attention with Flash-Attention V2 (Dao, 2024). Under 4K context length, Megalodon-7B is slightly slower (about \(6\%\)) than Llama2-7B, due to the introduction of CEMA and Timestep Normalization. When we scale up context length to 32K, Megalodon-7B is significantly faster (about \(32\%\)) than Llama2-7B, demonstrating the computation efficiency of Megalodon for long-context pretraining. In addition, Megalodon-7B-\(32\)K, which utilizes chunk parallelism (SS3.5), achieves about \(94\%\) utilization of Megalodon-7B-4K.

### Short-Context Evaluation on Academic Benchmarks

We compare Megalodon-7B to Llama2 models on standard academic benchmarks with short contexts (\(<4\)K tokens), closely following the settings in Llama2 (Touvron et al., 2023). The benchmarks are grouped into the categories listed below:

* **Commonsense Reasoning** (0-shot): HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e and -c (Clark et al., 2018).
* **World Knowledge** (5-shot): NaturalQuestions (NQ, Kwiatkowski et al. (2019)) and TriviaQA (TQA, Joshi et al. (2017)).
* **Reading Comprehension** (0-shot): BoolQ (Clark et al., 2019).
* **Popular aggregated results** (5-shot): MMLU (Hendrycks et al., 2020).

Table 1 summarizes the results of Megalodon and Llama2 on these academic benchmarks, together with other open-source base models, including MPT (MosaicML, 2023), RWKV (Peng et al., 2023), Mampa (Gu and Dao, 2023), Mistral (Jiang et al., 2023) and Gemma (Mesnard et al., 2024). Pretrained on the same 2T tokens, Megalodon-7B surpasses Llama2-7B across all the

Figure 4: Average WPS per device.

benchmarks. On some tasks, Megalodon-7B achieves comparable or even better performance with Llama2-13B. Note that Mistral-7B and Gemma-8B were pretrained on much larger datasets than Megalodon-7B, hence the results are not directly comparable.

### Long-Context Evaluation

Perplexity over Long SequencesTo demonstrate the capability of Megalodon to make use of very long contexts to improve next-token prediction, we start by conducting the evaluation of valid perplexity on different context lengths. Concretely, we construct a validation dataset which consists of 1,920 selected books. Each of these books contains sequences with at least 2M tokens. The validation dataset is constructed by first randomly shuffling all the files and then concatenating them. Figure 5 shows the perplexity (PPL) of the validation dataset in various context lengths ranging from 4K to 2M. We observe that the PPL decreases monotonically with context length, validating the effectiveness and robustness of Megalodon on modeling extremely long sequences.

Long-Context QA tasks in ScrollsNext, we evaluate Megalodon on long-context open-book question answering (QA) tasks in the Scrolls dataset (Shaham et al., 2022), including NarrativeQA (Kocisky et al., 2018), Qasper (Daisigi et al., 2021) and QMSum (Zhong et al., 2021). Following Xiong et al. (2023), we use a simple prompt {CONTEXT} Q: {QUESTION} A: for all the tasks, and evaluate 0-shot F1-score on NarrativeQA, 2-shot F1-score on Qasper and 1-shot geometric-ROUGE3 on QMSum. Table 2 lists the results of Megalodon-7B, together with other open-source long-context models in the scale of 7B, namely Xgen-7B-8K (Nijkamp et al., 2023), MPT-7B-8K (MosaicML, 2023), YaRN-7B-128k (Peng et al., 2024), Llama2-7B-4K (Touvron et al., 2023) and Llama2-7B-32K (Llama2-L, Xiong et al. (2023)). Megalodon-7B obtains the best F1 on NarrativeQA, and competitive results with Llama2-7B Long. It should be noticed that Llama2-7B Long extends the context length of Llama2-7B from 4K to 32K by continually pretraining it on additional 500B tokens from long-context data.

### Instruction Finetuning

To evaluation the generalization capability of Megalodon on instruction following and alignment, We finetune the base model of Megalodon-7B on a proprietary instruction-alignment data under a controlled setting. We did not apply any RLHF techniques to further finetune it. Table 3 summarizes the performance of chat models in 7B scale on MT-Bench4. Megalodon exhibits superior performance on MT-Bench compared to Vicuna (Chiang et al., 2023), and comparable performance to Llama2-Chat, which utilizes RLHF for further alignment instruction finetuned Megalodon in Appendix D.

Figure 5: PPL in various context lengths.

  
**Model** & **NaQA** & **Qasper** & **QMSum** \\  Xgen & 17.4 & 20.5 & 6.8 \\ MPT & 18.8 & 24.7 & 8.8 \\ Yarn & 20.9 & 26.2 & 11.4 \\  Llama2 & 18.8 & 19.8 & 10.1 \\ Llama2-L\({}^{*}\) & 23.5 & **28.3** & **14.5** \\  Megalodon & **23.9** & 28.0 & 13.1 \\   

Table 2: **Results on Scrolls. \({}^{*}\) Llama2-L (Xiong et al., 2023) continually trains Llama2 on 500B tokens for length extension.**

### Evaluation on Medium-Scale Benchmarks

ImageNet ClassificationTo evaluate Megalodon on image classification task, we conduct experiments on the Imagenet-1K (Deng et al., 2009) dataset, which consists of 1.28M training images and 50K validation images from 1000 classes. We mostly follow DeiT's approach of applying several data augmentation and regularization methods that facilitate the training process, and adopt most the hyperparameters from Ma et al. (2023). For classification task, we replace the timestep normalization with the standard group normalization method. Top-1 accuracy on the validation set is reported in Table 4 to assess various models. Megalodon obtains about \(1.3\)% accuracy improvement over DeiT-B (Touvron et al., 2021), and \(0.8\)%. improvement over Mega(Ma et al., 2023).

Auto-regressive Language Modeling on PG-19We also evaluate Megalodon on auto-regressive language modeling on the medium-scale PG19 (Rae et al., 2019) datasets. We use the same vocabulary from Block-Recurrent Transformer (Hutchins et al., 2022) and adopt most of its hyper-parameters to train a Megalodon model with 1.3B parameters. Table 5 illustrate the word-level perplexity (PPL) of Megalodon on PG-19, together with previous state-of-the-art models, including Compressive Transformer (Rae et al., 2020), Perceiver AR (Hawthorne et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022) and MegaByte (Yu et al., 2024). Megalodon significantly outperforms all the baselines.

## 5 Conclusion

We have introduced Megalodon, an improved Mega architecture with multiple novel technical components, including complex exponential moving average (CEMA), the timestep normalization layer, normalized attention and pre-norm with two-hop residual configuration, to improve its capability, efficiency and scalability. Through a direct comparison with Llama2, Megalodon achieves impressive improvements on both training perplexity and across downstream benchmarks. Importantly, experimental results on long-context modeling demonstrate Megalodon's ability to model sequences of unlimited length. Additional experiments on small/medium-scale benchmarks across different data modalities illustrate the robust improvements of Megalodon, which lead to a potential direction of future work to apply Megalodon for large-scale multi-modality pretraining.