# The Waymo Open Sim Agents Challenge

Nico Montali John Lambert Paul Mougin Alex Kuefler Nicholas Rhinehart Michelle Li Cole Gulino Tristan Emrich Zoey Yang Shimon Whiteson Brandyn White Dragomir Anguelov Waymo LLC

###### Abstract

Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. In this work, we introduce the Waymo Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology, present results for a number of different baseline simulation agent methods, and analyze several submissions to the 2023 competition which ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains open for submissions and we discuss open problems for the task.

## 1 Introduction

Simulation environments allow cheap and fast evaluation of autonomous driving behavior systems, while also reducing the need to deploy potentially risky software releases to physical systems. While generation of synthetic sensor data was an early goal [19; 43] of simulation, use cases have evolved as perception systems have matured. Today, one of the most promising use cases for simulation is system safety validation via statistical model checking [1; 16] with Monte Carlo trials involving realistically modeled traffic participants, i.e., _simulation agents_.

Simulation agents are controlled objects that perform realistic behaviors in a virtual world. In this challenge, in order to reduce the computational burden and complexity of simulation, we focus on simulating agent behavior as captured by the outputs of a perception system, e.g., mid-level object representations [2; 79] such as object trajectories, rather than simulating the underlying sensor data [13; 37; 62; 76] (see Figure 1).

A requirement for modeling realistic behavior in simulation is the ability for sim agents to respond to arbitrary behavior of the autonomous vehicle (AV). "Pose divergence" or "simulation drift"  is defined as the deviation between the AV's behavior in driving logs and its behavior during simulation, which may be represented through differing position, heading, speed, acceleration, and more. Directly replaying logged behavior of all other objects in the scene [32; 34; 35]

Figure 1: WOSAC models the simulation problem as simulation of mid-level object representations, rather than as sensor simulation.

under arbitrary AV planning may have limited realism because of this pose divergence. Such log-playback agents tend to heavily overestimate the aggressiveness of real actors, as they are unwilling to deviate from their planned route under any circumstances. On the other hand, rule-based agents that follow heuristics such as the Intelligent Driver Model (IDM)  are overly accommodating and reactive. We seek to evaluate and encourage the development of sim agents that lie in the middle ground, adhering to a definition of _realism_ that implies matching the full distribution of human behavior.

To the best of our knowledge, to date there is no existing benchmark for evaluation of simulation agents. Benchmarks have spurred notable innovation in other areas related to autonomous driving research, especially for perception [6; 10; 24; 58], motion forecasting [6; 10; 22; 72; 78], and motion planning . We believe a standardized benchmark can likewise spur dramatic improvements for simulation agent development. Among these benchmarks, those focused on motion forecasting are perhaps most similar to simulation, but all involve open-loop evaluation, which is clearly deficient compared to our closed-loop evaluation. Furthermore, we introduce realism metrics which are suitable to evaluating long-term futures. Relevant datasets such as the Waymo Open Motion Dataset (WOMD)  exist today that contain real-world agent behavior examples, and we build on top of WOMD to build WOSC. In this challenge, we focus on a subset of the possible perception outputs, e.g., traffic light states or vehicle attributes are not modeled, but we leave this for future work.

The challenges our benchmark raises are unique, and if we can make real progress on it, we can show that we've solved one of the hard problems in self-driving. We have a number of open questions: Are there benefits to scene-centric, rather than agent-centric, simulation methods? What is the most useful generative modeling framework for the task? What degree of motion planning is needed for agent policies, and how far can marginal motion prediction take us? How can simulation methods be made more efficient? How can we design a benchmark and enforce various simulator properties? During our first iteration of the WOSC challenge, user submissions have helped us answer a subset of these questions; for example, we observed that most methods found it most expedient to build upon state-of-the-art marginal motion prediction methods, i.e. operating in an agent-centric manner.

In this work, we describe in detail the Waymo Open Sim Agents Challenge (WOSAC) with the goal of stimulating interest in traffic simulation and world modeling. Our contributions are as follows:

* An evaluation framework for autoregressive traffic agents based on the approximate negative log likelihood they assign to logged data.
* An evaluation platform, an online leaderboard, available for submission at [https://waymo.com/open/challenges/2023/sim-agents/](https://waymo.com/open/challenges/2023/sim-agents/).
* An empirical evaluation and analysis of various baseline methods, as well as several external submissions.

## 2 Related Work

**Multi-Agent Traffic Simulation** Simulators have been used to train and evaluate autonomous driving planners for several decades, dating back to ALVINN . While simulators such as CARLA , SUMO , and Flow  provide only a heuristic driving policy for sim agents, they have still enabled progress in the AV motion planning domain [11; 12; 15]. Other recent simulators such as Nocturme  use a simplified world representation that consists of a fixed roadgraph and moving agent boxes.

   & **Multiple** & **Vehicle** & **System** & **System** & **System** \\  & **Object** & **Output** & **Exeminate** & **Exemination** & **Opactives** \\  & **CATEGORIES** & **Contributions** & **Contributions** & **Contributions** & **Exeminate** \\   Multi-Agent Trajectory Forecasting & ✓ & \(((x_{i},_{i},_{i}^{*},_{i}^{*}))_{i=1}^{}\) & ✗ & Orts-Loop & Kinematic accuracy and model covering \\ AV Motion Planning & ✗ & \(((x_{i},_{i},_{i}))_{i=1}^{}\) or controls & ✓ & Closed-Loop & Safety, control, progress \\ Agent and Environment Simulation & ✓ & \((_{i})_{i=1}^{}\),\(_{i}\) & ✗ & Closed-Loop & Distributed-Localism \\  

Table 1: A comparison of three autonomous-vehicle behavior related tasks which involve generation of a desired future sequence of physical states: trajectory forecasting, planning, and simulation. Note that observations \(o_{t}\) include simulated agent and environment properties.

Simulation agent modeling is closely related to the problem of trajectory forecasting, as a sim agent could execute a set of trajectory predictions as its plan . However, as trajectory prediction methods are traditionally trained in open-loop, they have limited capability to recover from out of domain predictions encountered during closed-loop simulation . In addition, few forecasting methods produce consistent joint future samples at the scene level . Sim agent modeling is also related to planning, as each sim agent could execute a replica of a planner independently . However, each of these three tasks differ dramatically in objectives, outputs, and constraints (see Table 1).

**Learned Sim Agents** Learned sim agents in the literature differ widely in assumptions around policy coordination, dynamics model constraints, observability, as well as input modalities. While coordinated scene-centric agent behavior is studied in the open-loop motion forecasting domain , to the best of our knowledge, TrafficSim  is the only closed-loop, learned sim agent work to use a joint, scene-centric actor policy; all others operate in a decentralized manner without coordination , i.e., each agent in the scene is independently controlled by replicas of the same model using agent-centric inference. BITS and TrafficBots  use a unicycle dynamics model and Nocture uses a bicycle dynamics model  whereas most others do not specify any such constraint; others enforce partial observability constraints, such as Nocture . Other methods differ in the type of input, whether rasterized  or provided in a vector format . Some works focus specifically on generating challenging scenarios , and others aim for user-based controllability . Some are trained via pure imitation learning , while others include closed-loop adversarial losses , or multi-agent RL  in order to learn to recover from its mistakes . Some works such as InterSim  use a goal-conditioned problem formulation, while others do not .

**Evaluation of Generative Models** Distribution matching has become a common way to evaluate generative models , through the Frechet Inception Distance (FID) . Previous evaluation methods such as the Inception Score (IS)  reason over the entropy of conditional and unconditional distributions, but are not applicable in our case due to the multi-modality of the simulation problem. The FID improves the Inception Score by using statistics of real world samples, measuring the difference between the generated distributions and a data distribution (in the simulation domain, the logged distribution). However, FID has limited sensitivity per example due to aggregation of statistics over entire test datasets into a single mean and covariance.

**Evaluating Multi-Agent Simulation** There is limited consensus in the literature regarding how multi-agent simulation should be evaluated (see Table 2), and no mainstream existing benchmark exists. Given the importance of safety, almost all existing sim agent works measure some form of collision rate , and some multi-object joint trajectory forecasting methods also measure it via trajectory overlap . However, collision rate can be artificially driven to zero by static policies, and thus cannot measure realism. Quantitative evaluation of realism requires comparison with logged data. Such evaluation methods vary widely, from distribution matching of vehicle dynamics , to comparison of offroad rates , spatial coverage and diversity , and progress to goal . However, as goals are not observable, they are thus difficult to extract reliably. Requiring direct reconstruction of logged data through metrics such as Average Displacement Error (ADE) has also been proposed , but has limited effectiveness because

  
**Evaluation Protocol** &  **ADE or** \\ **mis2RD** \\  &  **Offroad** \\ **Rate** \\  &  **Collision** \\ **Rate** \\  &  **Insistence-Level** \\ **Distribution Matching** \\  &  **Dataset-Level** \\ **Distributive Matching** \\  &  **Spatial Coverage or** \\ **Diversity** \\  & 
 **Goal progress or** \\ **Completion** \\  \\  Com:SocialProd  & ✓ & & & ✓ & & & \\ Trijectron  & ✓ & & & ✓ & & & \\ PRECOO  & ✓ & & ✓ & & & & ✓ \\ BAKE  & & ✓ & ✓ & & & & ✓ \\ SMAArTS  & & & ✓ & & & & ✓ \\ TrafficSim  & ✓ & ✓ & ✓ & & & ✓ & \\ SimNet  & ✓ & & ✓ & & & & \\ Symptro  & ✓ & ✓ & ✓ & & ✓ & & ✓ \\ Nordure  & ✓ & & ✓ & & ✓ & ✓ & ✓ \\ BITS  & & ✓ & ✓ & & ✓ & ✓ & ✓ \\ Inception  & ✓ & ✓ & & & & & ✓ \\ Meuthive  & & ✓ & ✓ & & & & ✓ \\ TrafficBots  & ✓ & & ✓ & ✓ & & & \\ 
**WOSAC (Ours)** & ✓ & ✓ & ✓ & & & & \\   

Table 2: Existing evaluation methods for simulation agents. Entries are ordered chronologically by Arxiv timestamp. There is limited consensus in the literature regarding how multi-agent simulation should be evaluated.

there are generally multiple realistic actions the AV or sim agents could take at any given moment. To overcome this limitation, one option is to allow the user to provide multiple possible trajectories per sim agent, such as TrafficSim, which uses a minimum average displacement error (minADE) over 15 simulations. .

Recently, generative-model based evaluation has become more popular in the simulation domain, primarily through distribution matching metrics. Symphony  uses Jensen-Shannon distances over trajectory curvature. NeuralNDE  compares distributions of vehicle speed and inter-vehicle distance, whereas BITS  utilizes Wasserstein distances on agent scene occupancy using multiple rollouts per scene, along with Wasserstein distances between simulated and logged speed and jerk - two kinematic features which can encapsulate passenger comfort. The latter are computed as a distribution-to-distribution comparison on a dataset level, however, this type of metric has shown limited sensitivity in our experiments.

**Likelihood metrics** An alternative distribution matching framework is to measure point-to-distribution distances.  and  introduce a metric defined as the average negative log likelihood (NLL) of the ground truth trajectory, as determined by a kernel density estimate (KDE) [42; 50] over output samples at the same prediction timestep. This metric has found some adoption [54; 64], and we primarily build off of this metric in our work. We note that likelihood-based generative models of simulation such as PRECOG  and MFP  directly produce likelihoods, meaning that the use of a KDE on sampled trajectories to estimate likelihoods is not needed for such model classes. Concurrent work  also measures the NLL of the GT scene under 6 rollouts.

## 3 Traffic Simulation as Conditional Generative Modeling

Our goal is to encourage the design of traffic simulators by defining a data-driven evaluation framework and instantiating it with publicly accessible data. We focus on simulating agent behavior in a setting in which an offboard perception system is treated as fixed and given.

**Problem formulation.** We formulate driving as a Hidden Markov Model \(=,,p(o_{t}|s_{t}),p(s_{t}|s_{t-1}) \), where \(\) denotes the set of unobservable true world states, \(\) denotes the set of observations, \(p(o_{t}|s_{t})\) denotes the sampleable emission distribution, and \(p(s_{t}|s_{t-1})\) denotes the hidden Markovian state dynamics: the probability of the hidden state transitioning from \(s_{t-1}\) at timestep \(t-1\) to \(s_{t}\) at time \(t\). Each \(o_{t}\) can be partitioned into AV- and environment-centric components that vary in time: \(o_{t}=[o^{}_{t},o^{}_{t}]\). \(^{}_{t}\) can in general contain a rich set of features, but for the purpose of our challenge, it contains solely the poses of the non-AV agents. We denote the true observation dynamics as \(p^{}(o_{t}|s_{t-1})_{p(s_{t}|s_{t-1})}p(o_{t}|s_{t})\).

**The task.** The task to build a "world model" \(q^{}(o_{t}|o^{}_{<t})\) of \(p^{}(o_{t}|s_{t-1})\), \(o^{}_{<t}[o^{},o^{},o_{-H-1},,o_{ t-1}]\), i.e., it denotes a context of a static map observation, traffic signal observations, and the observation history, with history length \(H\).

**Task constraints:**

1. \(q^{}\) must be autoregressive for \(T\) steps, i.e., sim agent models must adhere to a 10Hz resampling procedure, re-observing the updated scene and consuming their previous outputs.
2. \(q^{}\) must factorize according to Eq. 1: \[q^{}(o_{t}|o^{}_{<t})=(o^{}_{t}|o^{}_{< t})q(o^{}_{t}|o^{}_{<t}),\] (1) where \(q(o^{}_{t}|o^{}_{<t})\) is a traffic simulator, and \((o^{}_{t}|o^{}_{<t})\) is an AV policy1. Any submission that fails to satisfy both of these properties will not be considered on WOSAC leaderboards, as determined by challenge submission reports. Requiring them to be generative enables sampling from an arbitrary traffic simulator-AV policy pair. These two properties imply the probabilistic graphical model shown in Fig. 2, modified from Fig. 9 of . Algorithms 1 and 2 illustrate valid and invalid submissions.

Much of the challenge of modeling \(p^{}\) lies in the fact that in many situations \(s_{t-1}\), \(p^{}\) assigns density to multiple outcomes due to uncertainty from agents in the scene, which means that both \((o_{t}^{}|o_{<t}^{c})\) and \(q(o_{t}^{}|o_{<t}^{c})\) often must contain multiple modes in order to perform well. We evaluate distribution-matching of \(p^{}\) relative to a dataset of logged outcomes. The required factorization into a AV observation-space policy and environment observation dynamics, \(q^{}(o_{t}|o_{<t}^{c})=(o_{t}^{}|o_{<t}^{c})q(o_{t}^{ }|o_{<t}^{c})\), is fairly flexible. We are agnostic to their particular structures. One noteworthy choice for the environment observation dynamics is a "multi-agent" factorization, in which \(q(o_{t}^{}|o_{<t}^{c})=_{a=1}^{A}_{a}(o_{t}^{,a}|o_ {<t}^{c})\), i.e., the environment observation dynamics factorizes into a sequence of \(A\) observation-space policies, and the environment observation itself is partitioned into \(A\) different components, one for each agent: \(o_{t}^{}=[o_{t}^{,1},,o_{t}^{,A}]\).

```
0: Map \(o^{}\) and traffic signals \(o^{}\). Initial actor states \(o_{-H-1:0}=\{o_{-H-1},,o_{0}\}\) where each \(o_{t}^{}=\{o_{t}^{,1},,o_{t}^{,A}\}\) for the \(A\) actors in the scene.
0: Simulated observations \(o_{1:T}=\{o_{1},o_{2},,o_{T}\}\) for \(T\) simulation timesteps.
1:for\(t=1,...,T\)do\(\) Simulate for requested number of timesteps
2:\(o_{t}^{}_{A^{}}(o_{<t};o^{},o^{})\)
3:for\(a=1,...,A\)do\(\) Produce next state for each actor at each timestep
4:\(o_{t}^{,a}_{a}(o_{<t};o^{},o^{})\)
5:\(o_{t}=\{o_{t}^{,a}: a 1 A\}\{o_{t}^{ }\}\)
6:return\(o_{1:T}=\{o_{1},o_{2},,o_{T}\}\)
```

**Algorithm 1** Valid: Factorized, Closed-Loop, Agent-Centric Simulation

```
0: Map \(o^{}\) and traffic signals \(o^{}\). Initial actor states \(o_{-H-1:0}=\{o_{-H-1},,o_{0}\}\) where each \(o_{t}^{}=\{o_{t}^{,1},,o_{t}^{,A}\}\) for the \(A\) actors in the scene.
0: Simulated observations \(o_{1:T}=\{o_{1},o_{2},,o_{T}\}\) for \(T\) simulation timesteps.
1:\(o_{1:T}^{}=_{A^{}}(o_{<1};o^{},o^{})\)
2:for\(a=1,...,A\)do\(\) Produce states at all future timesteps for each actor
3:\(o_{1:T}^{,a}_{a}(o_{<1};o^{},o^{})\)
4:return\(o_{1:T}=\{o_{1:T}^{,a}: a 1 A\}\{o_{1:T}^{ }\}\)
```

**Algorithm 2** Invalid: Factorized, Open-Loop, Agent-Centric Simulation

## 4 Benchmark Overview

### Dataset

For WOSC, we use the test data from the v1.2.0 release of the Waymo Open Motion Dataset (WOMD) . We treat WOMD as a set \(\) of scenarios where each scenario is a history-future

Figure 2: Graphical model of required factorization as a Bayes net: the two distributions from Eq. 1 are autoregressively interleaved: one represents the AV’s “policy” \((o_{t}^{}|o_{<t}^{c})\), and another represents the environmental dynamics \(q(o_{t}^{}|o_{<t}^{c})\); the graphical model represent \(T\)-steps of applying these two distributions. Thick outgoing arrows denote passing inputs from all parent nodes to all children.

pair \((o_{-H-1:0},o_{ 1})\). This dataset offers a large quantity of high-fidelity object behaviors and shapes produced by a state-of-the-art offboard perception system. We use WOMD's 9 second 10 Hz sequences (comprising \(H=11\) observations from 1.1 seconds of history and 80 observations from 8 seconds of future data), which contain object tracks at 10 Hz and map data for the area covered by the sequence. Across the dataset splits, there exists 486,995 scenarios in train, 44,097 in validation, and 44,920 in test. These 9.1 second windows have been sampled with varying overlap from 103,354 mined segments of 20 second duration. Up to 128 agents (one of which must represent the AV) must be simulated in each scenario for the 8 second future (comprising 80 steps of simulation).

**Agent Definition** We require simulation of all agents that have valid measurements at time \(t=0\), i.e. the last step of logged initial conditions before simulation begins. Because the test split data is sequestered, users will not have access to objects that appear after the time of handover, and so therefore could not be expected to simulate them. We require simulation of all three WOMD object types (vehicles, cyclists, and pedestrians). Objects' dimensions stay fixed as per the last step of history (while they do change in the original data).

**Submission** We do not enforce any motion model (also because we have multiple agent types), which means users need to directly report \(x\)/\(y\)/\(z\) centroid coordinates and heading of the objects' boxes (which could be generated directly or through an appropriate motion model). See the Appendix for additional information on the submission format.

By allowing users to produce the simulations themselves, we reduce the burden on the user by avoiding the need to submit containerized software for an evaluation server.

### Evaluation

Agents should generate realistic driving scenarios stochastically. We define "realistic agents" as those that match the actual distribution of scenarios observed during real-world driving. Unfortunately, we do not know the analytic form of the distribution, but we do have samples from it: the examples that make up WOMD. We therefore evaluate submissions using the approximate negative log likelihood (NLL) of real world samples under the distribution induced by the agents.

The NLL we wish to minimize is given by:

\[^{*}=-|}_{i=1}^{||} q^{ }(o_{ 1,i}|o_{<1,i}) \]

However, there are two problems with trying to minimize Equation 2 exactly in our problem setting. First, \(o_{ 1}\) is high-dimensional. Instead of trying to parameterize the entire ground truth scenario and compute its NLL under a simulated distribution, we therefore parameterize scenarios with a smaller number of component metrics (see Section 4.2.1) and aggregate them together into a composite NLL metric (see Section 4.2.2). Second, agents may support sampling but not pointwise likelihood estimation . In fact, we only require challenge entrants to submit samples from their agents, and therefore have no way of knowing the exact likelihood of logged scenarios under different agent submissions. To avoid this problem, we standardize the NLL computation by fitting histograms to the 32 submitted samples of agent futures, and compute NLLs under the categorical distribution induced by normalizing the histograms.

#### 4.2.1 Component Metrics

Breaking NLL\({}^{*}\) into component metrics has a few benefits. It mitigates the curse of dimensionality described in Section 4.2. It also adds more interpretability to the evaluation, allowing researchers to trade off between different types of errors.

**Time Series NLL**: Given the time series nature of simulation data, two choices emerge for how to treat samples over multiple timesteps for a given object for a given run segment: to treat them as time-independent or time-dependent samples. In the latter case, users would be expected to not only reconstruct the general behaviors present in the logged data in one rollout, but also recreate those behaviors over the exact same time intervals. To allow more flexibility in agent behavior, we use the former formulation when computing NLLs, defining each component metric \(m\) as an average (in log-space) over the time-axis, masked by validity \(v_{t}\): \(m=-1\{v_{t}\}}_{t}1\{v_{t}\}NLL _{t}\).

However, we note that as a result, a logged oracle will not achieve likelihoods of 1.0, whereas in the latter formulation a logged oracle would.

**Definitions** We compute NLLs over 9 measurements: kinematic metrics (linear speed, linear acceleration, angular speed, angular acceleration magnitude), object interaction metrics (distance to nearest object, collisions, time-to-collision), and map-based metrics (distance to road edge, and road departures). Please refer to Section A.6 of the Appendix for a complete description and additional implementation details.

#### 4.2.2 Composite Metric

After obtaining component metrics for each measurement, we aggregate them into a single composite metric \(^{K}\) for evaluating submissions:

\[^{K}=_{i=1}^{N}_{j=1}^{M}w_{j}m_{i,j}^{K},  28.452756pt_{j=1}^{M}w_{j}=1 \]

where \(N\) is the number of scenarios and \(M=9\) is the number of component metrics. The component metrics \(m\) and composite metric \(\) are also parameterized by a number of samples \(K=32\). The value \(m_{i,j}\) represents the likelihood for the \(j^{th}\) metric on the \(i^{th}\) example. The metric \(\) is simply a convex combination (i.e. weighted average) over the component metrics, where the weight \(w_{j}\) for the \(j^{th}\) metric is set manually. In the interest of promoting safety, the weighting for collision and road departure NLLs are set to be \(2\) larger than the weight for the other component metrics.

## 5 Experimental Results

In Figure 4 and Table 3, we present quantitative results for a handful of methods. We describe each method in more detail in the sections below.

Figure 3: Visualizations of simulation results on two separate WOMD scenes (top, bottom). Results for various baseline methods are shown on WOMD’s validation split, in 2d. ‘Simulation input’ represents the context history \(o_{-H-1:0}\), whereas all other columns visualize both \((o_{-H-1:0},o_{ 1})\). Two scenes are represented: one where the AV completes the execution of a left turn _(top row)_ and another where the AV remains stopped at a red traffic signal _(bottom row)_. Each rendering in the second and third columns depicts the entire duration of the scene. Trajectories of environment sim agents are drawn in a green-blue gradient, and trajectories of the AV agent are drawn in a red-yellow gradient (each as a sequence of circles in a temporal color gradient).

### Baselines

**Random Agent**: An agent that produces random trajectories \(\{(x_{t},y_{t},_{t})\}_{t=1}^{T}\), for \(T=80\), with \(x,y,(,^{2})\), with \(=1.0\) and \(=0.1\), in the AV's coordinate frame.

**Constant Velocity Agent**: An agent that extrapolates the trajectory using the last heading and speed recorded in the provided context/history. If no two-step difference can be computed based on the valid measurements (e.g. the object appeared only at the final step of context), we set a zero speed for such agents.

**Wayformer (Identical Samples) Agent**: An agent that produces a hybrid of open-loop and closed-loop data using a Wayformer  motion prediction model, by executing model inference autoregressively at 2Hz. The agents execute the policy forward for 5 simulation steps, and then replan. Results with a 10 Hz replan rate instead are also shown in Table 3, and an ablation on the replan rate is provided in the Appendix. The maximum-likelihood trajectory for each agent is identically repeated 32 times to produce 32 samples. Each agent is executed by the same policy in an agent-centric frame, batched together for inference, thus complying with the required factorization.

**Wayformer (Diverse Samples) Agent**: An agent that also utilizes Wayformer -generated trajectories, but samples diverse agent plans, from \(K\) possible trajectories according to their likelihood, instead of selecting the maximum-likelihood choice.

**Logged Oracle**: Agent that directly copies trajectories from the WOMD test split, with 32 repetitions.

### External Submissions

**MultiVerse Transformer for Agent simulation (MVTA)**: A method inspired by MTR [55; 56] that is trained and executed in closed-loop. MVTA uses a'receding horizon' policy with a GMM head, and consumes vector inputs.

**MVTE**: An enhanced version of MVTA  that samples a MVTA model from a pool of model variants to increase simulation diversity across rollouts.

**MTR+++**: A hybrid open-loop/closed-loop method with a 0.5Hz replanning rate that is inspired by MTR [55; 56] and searches for the densest subgraph in a graph of non-colliding future trajectories. For a description of other evaluated external submissions, please refer to Section A.4 of the Appendix.

### Learnings from the 2023 Challenge

During the course of our 2023 WOSAC Challenge (March 16, 2023 to May 23, 2023), associated with the CVPR 2023 Workshop on Autonomous Driving, we received 24 test set submissions, and 16 validation set submissions, from 10 teams. We continue to receive submission queries to our evaluation server for our standing leaderboard as new teams submit new methods.

**Trends** We observed several trends among submissions. First, the challenge champion, MVTA/MVTE , was the only method to utilize and benefit from closed-loop training. Other methods that were trained in open-loop, such as MTR+++  or our Wayformer-derived  baseline, found operating

Figure 4: Challenge composite metric results of various baselines on the _test_ split of WOMD.

at slower realm rates necessary to obtain high composite metric results (See Table 3). Second, almost all submissions used Transformer-based methods , except for JointMultiPath++, which used LSTM and MCG blocks . Third, all methods built primarily on top of existing motion prediction works, rather than upon existing motion planning works or sim agent methods from the literature. Only one method, MVTA/MVTE , incorporated aspects of an existing sim agent work, TrafficSim , as well as motion planning techniques, implementing a receding horizon planning policy. Thus, fourth, we observed the benefit of incorporating planning-based methods into a motion prediction framework. Fifth, most methods (excluding JointMultipath++ ) built upon the 2022 CVPR Waymo Open Motion Prediction challenge champion, MTR [55; 56], likely due to the open-source availability of its codebase and SOTA performance. Finally, all submissions operated in an agent-centric coordinate frame, rather than jointly sampling from a scene representation simultaneously.

**Likelihood Metrics Reward Diversity** We found that our likelihood-based metrics reward models that produce diverse futures. For example, generating 32 diverse rollouts per scene with a Wayformer model performs 11% better on our evaluation metrics than a Wayformer model that produces 32 identical rollouts per scene (see Figure 4).

**Collision Minimization as an Algorithmic Objective** Several methods designed algorithmic components to determine futures with a minimal number of collisions, e.g., MTR++  which used clique-finding in an undirected graph of collision-free future trajectories, and CAD , which used rejection sampling on open-loop futures that created collisions. This objective aligns with human preference, but as close calls and collisions do occur in real driving data distributions, optimizing for this objective could be seen as trimming the tail of the distribution; distracted drivers generally do exist in everyday real world driving, and in certain scenarios, one would expect a low-quality planner to perform poorly and produce collisions with sim agents, and so such should be taken into consideration for generating realistic simulations. This suggests a limitation of the WOMD , which has few examples from the tail distribution of real driving, and efforts to upsample collision data could prove useful. In addition, open-loop methods such as CAD  that prune collisions after the fact could prune collisions caused by the AV rather than by the sim agents, yielding a misleadingly optimistic view of the AV's performance.

**Composite Metric vs. (min-)ADE and ADE:** We see that among submissions to the test set, rankings by ADE and minADE and ranking by our NLL composite metric disagree. However, methods with lower minADE do tend to achieve higher composite scores; ADE does not exhibit such a trend.

**Composite Metric Results** The ordinal ranking shown in Figure 4 and Table 3 indicates that learned, stochastic sim agents outperform not only heuristic baselines but also learned, deterministic sim agents. We consider a composite metric score of 0.722 as a practical upper bound on submissions, because it involves access to test data via an oracle.

**Component Metric Results** In Table 3, we provide a breakdown of the composite metric into component metric results. As expected, the 'logged oracle' baseline achieves the highest likelihood in each of the 9 component metrics. The top performing method, MVTE  scored highest on all but one component metric (_linear acceleration likelihood_), where CAD  outperformed MVTE by 12% (likelihood of 0.253 vs. 0.222). Surprisingly, MVTE  has angular acceleration likelihoods within a percentage point of the 'logged oracle' (0.481 vs. 0.489). The gap between the top performing learned method (MVTE) and 'logged oracle' in both collision likelihood (0.893 vs. 1.000) and

  

distance-to-nearest object likelihood (0.383 vs. 0.485) indicates significant room for improvement in future work on interactive metrics.

### Qualitative Results

In Figure 3, we provide a qualitative comparison of various baselines on two WOMD scenarios. The results indicate that the complexity of behaviors within intersections far exceeds the capability of simple heuristics to predict. Collisions are evident from the constant velocity baselines in both examples. Additional qualitative examples from other sim agent methods are shown in Section A.5 of the Appendix.

## 6 Discussion

Limitations.For our 2023 Challenge, we manually verified the validity of each submission according to factorization and closed-loop requirements discussed in each team's report, and we observed that the technical rules were subtle. Several of the submissions that used open-loop or hybrid open-loop/closed-loop methods may have limited applicability for some simulation applications.

Even if we had instituted a benchmark based on Docker-containerized software submissions instead of uploading output trajectory submissions, enforcing our requirements algorithmically and automatically would still be challenging. Although many properties of function calls to Dockerized software can be measured, e.g. latency, as long as any arbitrary state is maintained by the user, the system could not enforce all details of the closed-loop nature of the function call. As a result, user-submitted simulation agent software would have to adhere to strict stateless input and output data APIs. The ability to do so would assist in removing ambiguity regarding whether methods that prune collisions post-hoc qualify as closed-loop.

If a user provides containerized simulator submissions, one approach to encourage adherence to our requirements and to further incentivize closed-loop behavior would be to provide and interact with an AV policy that the user does not control. In our benchmark, the user was allowed to control the AV, albeit through an independent policy; the ability to evaluate simulator submissions on separate, held-out AV motion planning policies and on new scenarios would allow further valuable analysis.

Future WorkObject insertion and deletion are important aspects of the simulation problem, yet we intentionally introduced an assumption of no object insertion or deletion in order to reduce the complexity of the first iteration of the WOSAC challenge for users. Motion planners trained or evaluated in a simulator must have the capability to exercise caution regarding areas of occlusion from which new objects may emerge at any timestep. In a future iteration of the challenge, we plan to introduce realism metrics that reward properly-modeled object insertion and deletion, e.g. distributional metrics on the number of vehicles appearing or disappearing at each frame, or the distance of simulated objects from the autonomous vehicle. The data distribution in the WOMD dataset already includes such object insertion and deletion.

Furthermore, we intentionally introduced an assumption of time-invariant object dimensions in our first iteration of WOSAC to simplify the modeling challenge for users. Time-variant object dimensions can be considered as a type of vehicle attribute, and object dimensions do actually change in the underlying data distribution provided in the WOMD dataset. We hope to include time-variant object dimension prediction as an aspect of the benchmark in future iterations.

As discussed in Section 5.3, given the prevalence of collision minimization algorithmic components among submissions, one may presume that collisions are not heavily represented in WOMD  data, or our metrics are limited in some way. Another approach would be to "fatten the tails" of the evaluation data distribution by generating synthetic, challenging initial conditions , or mining more close calls and collisions from real driving data.

Conclusion.In this work, we have introduced a new challenge for evaluation of simulation agents, explaining the rationale for the different criteria we require. We invite the research community to continue to participate.