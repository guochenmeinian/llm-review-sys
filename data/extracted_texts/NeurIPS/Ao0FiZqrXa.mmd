# Simple and Fast Distillation of Diffusion Models

Zhenyu Zhou\({}^{1,2}\) Defang Chen\({}^{3}\)1 Can Wang\({}^{1,2}\) Chun Chen\({}^{1,2}\) Siwei Lyu\({}^{3}\)

\({}^{1}\)Zhejiang University, State Key Laboratory of Blockchain and Data Security

\({}^{2}\)Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security

\({}^{3}\)University at Buffalo, State University of New York

{zhyzhou, defchern}@zju.edu.cn

Corresponding author. Work partially done during Defang's time at Zhejiang University.

###### Abstract

Diffusion-based generative models have demonstrated their powerful performance across various tasks, but this comes at a cost of the slow sampling speed. To achieve both efficient and high-quality synthesis, various distillation-based accelerated sampling methods have been developed recently. However, they generally require time-consuming fine tuning with elaborate designs to achieve satisfactory performance in a specific number of function evaluation (NFE), making them difficult to employ in practice. To address this issue, we propose **S**imple and **F**ast **D**istillation (SFD) of diffusion models, which simplifies the paradigm used in existing methods and largely shortens their fine-tuning time up to \(1000\). We begin with a vanilla distillation-based sampling method and boost its performance to state of the art by identifying and addressing several small yet vital factors affecting the synthesis efficiency and quality. Our method can also achieve sampling with variable NFEs using a single distilled model. Extensive experiments demonstrate that SFD strikes a good balance between the sample quality and fine-tuning costs in few-step image generation task. For example, SFD achieves 4.53 FID (NFE=2) on CIFAR-10 with only **0.64 hours** of fine-tuning on a single NVIDIA A100 GPU. Our code is available at https://github.com/zju-pi/diff-sampler.

## 1 Introduction

Diffusion models have attracted increasing interest in recent years due to their remarkable generative abilities across various domains, including image , video , audio , and molecular structures . These models progressively transform a noisy input into a realistic output through iterative denoising steps. Diffusion models are preferred over other generative models  for their high-quality synthesis, stable training and a strong theoretical foundation rooted in stochastic differential equations . However, achieving high-quality synthesis with diffusion models typically requires hundreds thousands of sampling steps, resulting in slow sampling speeds and a significant challenge for practical applications.

Figure 1: _Comparison of acceleration methods on diffusion models. For better visualization, the time axis is shifted by adding one hour to the actual time required. Our method achieves good performance with a small fine-tuning cost. Note that it takes about **200 hours** to train a diffusion model from scratch in this setting._Recent years have witnessed significant progress in accelerating the sampling of diffusion models [48; 30; 58; 60; 8; 38; 63; 4; 31; 45; 34; 50; 17; 52; 7]. These methods typically fall into two categories: _solver-based methods_ and _distillation-based methods_. Solver-based methods [48; 30; 58; 16; 60; 8; 38; 63; 4] consider sampling from diffusion models as solving differential equations, and employ fast numerical solvers to accelerate high-quality synthesis. However, these methods are limited by inherent truncation errors, and the sample quality becomes degraded when the number of function evaluations (NFEs) is relatively small (e.g., \( 5\)). Distillation-based methods, on the other hand, retain the structure of the original (teacher) diffusion model but aim to create a simplified (student) model that streamlines the iterative refinement process of diffusion models [31; 45; 34; 50; 17; 52; 7]. Extreme distillation-based methods even establish a direct one-to-one mapping between the implicit data distribution and a pre-specified noise distribution [31; 27; 50; 10; 56]. Although distillation-based methods have demonstrated impressive results, often outperforming solver-based methods in sampling quality given the total NFE budge is less than 5, they require expensive computational resources to fine tuning pre-trained diffusion models. As illustrated in Figure 1, the necessary time generally exceeds one hundred GPU hours, _even reaching the same order of magnitude required for training a diffusion model from scratch_. We attribute the time-consuming fine-tuning process to the following two factors:

* **The mismatch between fine-tuning and sampling steps**. There often exists significant fine-tuning costs in existing distillation-based methods that does not effectively contribute to the final performance due to the step mismatch. For example, progressive distillation [45; 34; 1] fine-tunes the diffusion model at thousands of timestamps but only a few steps (e.g., 8 or fewer) are used in sampling. Besides, consistency-based distillation  spends most fine-tuning efforts to ensure the consistency property , yet only 1 or 2 steps are used in sampling. Such inconsistencies waste excessive efforts in the fine-tuning process.
* **The complex optimization objectives**. The optimization objectives of distillation-based methods are getting increasingly complex, including the use of LPIPS [59; 50; 17; 56], adversarial training [46; 17] as well as various regularization terms [17; 56]. Despite the improved results, these additional components complicate the fine-tuning process.

In this paper, we introduce **S**imple and **F**ast **D**istillation (SFD) of diffusion models, which aims to achieve fast and high-quality synthesis with diffusion models in a few sampling steps, at minimal fine-tuning cost. Starting from the general framework behind distillation-based methods, we address the issue of step mismatch by fine-tuning only a small number of timestamps that will be used in sampling, which significantly improves the fine-tuning efficiency. The effectiveness of this strategy is underpinned by the key observation that, fine-tuning at a specific timestamp can positively impact the gradient direction at other timestamps (Section 3.1). Our SFD is then introduced as a simplified paradigm for the distillation of diffusion models, where the student learns to mimic the teacher's sampling trajectory while minimizing accumulated errors. We release the potential of this simple framework by identifying and addressing several small yet vital factors affecting the performance (Section 3.2). Moreover, we propose a variable-NFE version of our method named SFD-v, which enables a single distilled model to achieve sampling with various steps by introducing a _step-condition_ into the model (Section 3.3).

With 2 NFE, our SFD achieves a FID of \(4.53\) on CIFAR-10  with a training cost of just **0.64 hours** on a single NVIDIA A100 GPU, which is 1000\(\) faster than consistency distillation requiring about 1156 hours (see Figure 1). Quantitative and qualitative results on additional datasets, including

Figure 2: Comparison of synthesized images by Stable Diffusion v1.5  with guidance scale 7.5.

ImageNet 64\(\)64 , Bedroom 256\(\)256 , and image generation with Stable Diffusion , demonstrate the effectiveness and efficiency of our methods.

## 2 Preliminary

### Diffusion Models

Diffusion models bridge the implicit data distribution \(p_{d}\) and a Gaussian distribution \(p_{n}\) by progressively adding white Gaussian noise to the data and then iteratively reconstructing the original data from pure noise. Diffusion models are grounded in a theoretical framework based on stochastic differential equations (SDEs) , with the forward process injecting noise to data:

\[=(,t)t+g(t) _{t},\] (1)

where \((,t):^{d}^{d},g():\) are drift and diffusion coefficients, and \(_{t}^{d}\) denotes the Wiener process . The backward process reconstructs the original data from the noisy input, which can be achieved with a _reverse-time_ SDE that shares the same marginals determined by the forward SDE, i.e., \(=[(,t)-g^{2}(t)_{}  p_{t}()]t+g(t)}_{t}\), where \(_{} p_{t}()\) is known as _score function_[15; 33]. The reverse-time SDE can be further simplified to the _probability flow ordinary differential equation_ (PF-ODE) [51; 16; 3], \(=[(,t)-g^{2}(t)_{ } p_{t}()]t\). In particular, we consider \((,t)=\) and \(g(t)=\) in this paper, i.e.,

\[=-t_{} p_{t}()t,\] (2)

The score function is estimated as \(_{} p_{t}()-_{}( ,t)/t\) with a noise-prediction model \(_{}(,t)\) which is obtained by minimizing a regression loss with the weighing function \((t)\) for each \(t\)[12; 48; 63]:

\[_{t}()=(t)_{ p_{d},(,)}\|_{}( +t,t)-\|_{2}^{2}.\] (3)

With the noise-prediction model in place of the score function, the PF-ODE can be written as follows

\[=_{}(,t)t.\] (4)

Compared to the general reverse-time SDEs, the PF-ODE is preferred in practice for its conceptual simplicity and efficient sampling [51; 3]. To sample from a diffusion model with \(N\) steps, one first draws \(_{N} p_{n}=(,t_{}^{2})\) and then numerically solves Eq. 4 by a solver-based method [48; 29; 30; 58; 63; 4], following a hand-crafted time schedule \((N)=\{t_{0}=t_{},t_{1},,t_{N}=t_{}\}\). The obtained sample sequence \(\{_{n}\}_{n=0}^{N}\) is called the _sampling trajectory_.

### Distillation-based Diffusion Sampling

Through the lens of Eq. 4, we can interpret the noise-prediction model as a gradient field evolves over time, guiding samples towards the data distribution's manifold. Solver-based sampling methods do not change the gradient field and are convenient to implement [48; 29; 30; 58; 63; 4]. However, discretization errors prevent these methods from generating high-quality images within a few sampling steps. Distillation-based methods address this issue by fine-tuning the gradient field with the reference signals provided by a teacher (mostly a solver-based method) to build "shortcuts" on the sampling trajectory [45; 50; 34; 17]. This basic framework behind distillation-based methods, which we call _Trajectory Distillation_, is illustrated in Algorithm 1. Specifically, starting from latent encodings \(_{n+1}\) and \(}_{n+1}\) (\(0 n N-1\)) with a sampled \(n\), the sampling process is written as:

\[: }_{n}=(}_{n+1 },t_{n+1},t_{n},K;),\] (5) \[: _{n}^{}=(_{n+1},t_{n+1}, t_{n},1;)=_{n+1}+(t_{n+1}-t_{n})_{}(_{n+1},t_{n+1}),\] (6)

where \(K\) is the number of teacher sampling steps taken from \(t_{n+1}\) to \(t_{n}\); and \(\), \(\) are the parameters of the student and teacher model, respectively. In each training iteration, the student model \(\) is updated with the calculated loss function \(()=d(_{n}^{},}_{n})\) using a distance metric \(d(,)\). \((,,,;)\) can be any solver-based method with the fixed \(\) to provide reference signals. For example, in progressive distillation [45; 34], it is defined as the Euler sampler  with \(K=2\), while in consistency distillation [50; 17], the Heun sampler , \(K=1\), and a consistency loss are used.

Distillation-based methods have demonstrated impressive results but generally incur a significant computational overhead. In the following sections, we revisit the trajectory distillation framework and unlock its potential through a comprehensive assessment of the key factors affecting the performance.

## 3 Method

### Smooth Modification of the Gradient Field

As mentioned in Section 1, existing distillation-based methods incur significant fine-tuning costs that may not effectively contribute to the final sample quality [45; 34; 50], which is a key factor overburdening computational resources. Instead, we propose to fine-tune only a few timestamps that will be used in sampling. To validate our strategy, we initialize four different student models (denoted as MODEL(\(_{n}\)), \(0 n N-1\)) from a pre-trained teacher model \(\) using the second-order DPM-Solver(2S)  with \(N=4\) and \(K=3\). We then fine-tune each MODEL(\(_{n}\)) only on a certain timestamp \(t_{n+1}\) and make it align with the teacher predictions at the next timestamp \(t_{n}\). After fine-tuning, we evaluate the performance of each MODEL(\(_{n}\)) at all timestamps by comparing with the teacher's sampling trajectory \(\{x_{n}\}_{n=0}^{N}\) under the same setting. Specifically, we calculate the \(L_{2}\) distance in the following two formulas for all \(0 n,k N-1\),

\[: \|x_{n}-(_{n+1},t_{n+1},t_{n},1;) \|_{2},\] (7) \[(_{k}): \|x_{n}-(_{n+1},t_{n+1},t_{n},1;_{ k})\|_{2},\] (8)

and average the results over 1,000 trajectories. As shown in Figure 3, the distance calculated using the fine-tuned models is almost consistently smaller than that of the baseline. This is remarkable since each MODEL(\(_{n}\)) is only trained to match the teacher's sampling trajectory at a specific \(t_{n}\). Yet, its performance on other timestamps is mostly improved, even though different timestamps are far apart. This indicates that trajectory distillation does not disrupt the gradient field but enhances it smoothly. Since fine-tuning at different timestamps mutually reinforces the model, fine-tuning on a fine-grained time schedule is unnecessary. This insight forms the basis of our strategy. Beyond efficiency, we will demonstrate that our approach achieves high performance in the sequel.

### Simple and Fast Distillation of Diffusion Models

As for solver-based methods, the cost of a single sampling step varies depending on the design, which is commonly measured by the number of function evaluations (NFE). For the DDIM sampler  and other higher-order methods such as DPM-Solver++(3M)  and UniPC , one sampling step corresponds to one NFE, while two NFEs are required for the Heun sampler  and DPM-Solver(2S) . In the following, we distill a diffusion model to achieve sampling with two NFEs (\(N=2\) by default). We configure a reasonable baseline on the CIFAR10 dataset  and gradually improve the performance through extensive experiments. The improved configuration is proven to be effective across different NFEs and datasets.

**Default configuration**. The Heun sampler is used to generate the teacher sampling trajectory instead of DDIM for efficiency, which has been demonstrated in training consistency models [50; 17]. We set

Figure 3: _MODEL(\(_{n}\)) is trained to match the teacherâ€™s sampling trajectory at \(t_{n}\), but can enhance the matching at multiple timestamps. The time schedule follows the polynomial schedule with \(=7,t_{0}=0.002,t_{4}=80\)._\(K=3\) for Heun sampler, which gives sampling trajectories with 12 NFEs. For the time schedule, if not otherwise specified, we use the polynomial schedule where \(=7\), \(t_{}=0.002\) and \(t_{}=80\), following the EDM implementation . The squared L2 loss is used by default. For experiments in this section, we use a batch size of 128, learning rate of 5e-5 and fine-tune with 100,000 teacher sampling trajectories generated (around 780 training iterations).

**From local to global**. We start by analyzing the potential defects of trajectory distillation. As shown in Algorithm 1, trajectory distillation performs local fine-tuning. The term "local" indicates that the teacher model only generates part of the sampling trajectory (i.e., from \(t_{n+1}\) to \(t_{n}\)), and the optimization is independent across different \(n\). This raises two defects that limit both efficiency and performance: (i) Higher-order multi-step solvers that require history evaluation records are unsupported. (ii) The student model is trained to imitate only part of the teacher's sampling trajectory. During sampling, the errors accumulate since the student is never trained to perfectly fix them. To address these issues, we view trajectory distillation from a global perspective and introduce our _Simple and Fast_**D**_istillation_ of diffusion models (SFD) in Algorithm 2. In each training iteration of SFD, we first generate the whole teacher sampling trajectory and let the student imitate it step by step. During this process, the student model generates its own trajectories, enabling it to learn to fix the accumulated errors. In Figure 3(a), we compare both strategies (marked as "Vanilla" and "SFD") using the default configuration. It is shown that SFD exhibits better performance. In the following sections, we focus on SFD and seek to release its potential for efficient distillation of diffusion models.

**Efficient solver**. One of the key components that affects the efficiency of distillation-based methods is the choice of the teacher solver. To compare the performance of different solvers, we conduct experiments on both trajectory distillation and SFD with 3 representative solver-based methods: second-order Heun sampler, second-order DPM-Sovler(2S) and third-order DPM-Solver++(3M). Since history evaluations are unavailable, we exclude DPM-Solver++(3M) for trajectory distillation. The NFE of teacher sampling trajectories is kept 12 consistently (\(K=6\) is hence used for DPM-Solver++(3M)) and the results are shown in Figure 3(a). DPM-Solver++(3M) stands out, and the Heun sampler is shown to be suboptimal. Therefore, for distillation-based methods with trajectory distillation involved, it is recommended to explore replacing the Heun sampler (or DDIM sampler) with DPM-Solver(2S). We leave this to future works.

**Minimum and maximum timestamps**. Choosing DPM-Solver++(3M) as the teacher solver, we improve SFD by adjusting the start and end timestamps during training and sampling. For the minimum timestamp \(t_{}\), we empirically find that a slight increase improves the student and teacher sampling performance across various datasets. An ablation study of \(t_{}\) on CIFAR10 dataset is shown in Figure 5. We increase \(t_{}\) from 0.002 to 0.006. This change provides consistent improvements across different pre-trained diffusion models. For the maximum timestamp, we introduce analytical first step (AFS)  in the generation of student sampling trajectories, which takes one estimated step \(_{}(_{N},t_{N})_{N}/ ^{2}}\) at the beginning of sampling to save one NFE. Therefore, to obtain a 2-NFE SFD with AFS applied, we use \(N=3\) and \(K=4\). As shown in Figure 3(b), using AFS largely boosts the performance of SFD, indicating that one more inaccurate step can outperform one less step. This improvement also benefits from the nature of SFD, where the error incurred in the first step can be fixed by later steps (see the visualization in Figure 9). The detailed algorithm of SFD with AFS is included in Appendix C.

Figure 4: Ablation studies of 2-NFE distillation on CIFAR10. The FID is evaluated by 50,000 generated samples with the same latent encodings and is reported every 10 iterations. We achieve the best performance with SFD, DPM-Solver++(3M) teacher, AFS, \(t_{}=0.006\) and L1 loss.

**Loss metric**. In Figure 3(c), we test various distance metrics for the loss function including squared L2 distance, L1 distance, LPIPS distance  and Pseudo-Huber distance . Among these metrics, L1 distance outperforms. Note that the LPIPS distance is trained to evaluate the perceptual distance between two images but not corrupted ones, which may explain its suboptimal performance.

With these improvements, the SFD achieves a fast convergence with only around 300 training iterations, which only takes around **8 minutes** on a single NVIDIA A100 GPU. The performance of the obtained 2-NFE SFD is even comparable with the 2-NFE model trained by progressive distillation , which takes more than 100 hours under our estimation. The quantitative results are included in Table 1.

To verify our findings in Section 3.1, we test the extrapolation ability of SFD with untrained NFEs on CIFAR10. The results are shown in Figure 6 where the markers indicate the NFEs our methods are trained to sample with. Take "SFD, NFE4" as an example, where the SFD is only trained to sample with NFE of 4 and its performance is marked by a star. When using this SFD to sample with untrained NFEs (i.e., 2,3,5,6), even though the timestamps have never been trained in these cases, the performance is still decent and largely outperforms the DDIM sampler (DDIM with NFE of 6 gives a FID of 35.62, far exceeds the range of the figure). This empirically verifies our hypothesis that the gradient field is not disrupted but is smoothly enhanced. The change of the gradient field of a certain timestamp can also change that of nearby timestamps in a similar way.

Moreover, in Figure 7, we leverage the three-dimensional projection technique proposed in  to visualize the sampling trajectories generated by SFD with 5 NFEs and that of the teacher solver SFD is trained to imitate. Due to the use of AFS, the first sampling step of SFD is inaccurate. But the accumulated errors are largely reduced in the following steps thanks to the global distillation used in our SFD as discussed in Section 3.2. We include more visualised trajectories in Appendix D.2.

### Towards Variable-NFE Distillation

One attractive property of diffusion models is that the sample quality can be consistently improved by increasing the sampling steps, which is currently unsupported by most distillation-based methods. Progressive distillation  partially addresses this issue using the multi-stage training. However, the model saved in each training round only supports sampling with a certain trained step (i.e., 1, 2, 4, 8, \(\)). The sample quality of consistency models  designed for the one-step sampling also deteriorates under larger sampling steps as revealed by . Moreover, the unique encoding property of ODE-based diffusion sampling is corrupted in multi-step consistency models due to the noise injected in every step.

   Method & Teacher & \(t_{}\) & AFS & Loss & FID \\  Vanilla & Heun & 0.002 & N/A & I.2 & 46.84 \\ Vanilla & DPM(2S) & 0.002 & N/A & I.2 & 16.69 \\ SFD & Heun & 0.002 & False & I.2 & 20.88 \\ SFD & DPM(2S) & 0.002 & False & I.2 & 12.50 \\ SFD & DPM++(3M) & 0.002 & False & I.2 & 11.65 \\ SFD & DPM++(3M) & 0.006 & False & I.2 & 10.93 \\ SFD & DPM++(3M) & 0.002 & True & I.2 & 7.17 \\ SFD & DPM++(3M) & 0.006 & True & I.2 & 5.67 \\ SFD & DPM++(3M) & 0.006 & True & LPIPS & 5.10 \\ SFD & DPM++(3M) & 0.006 & True & PH & 4.90 \\ SFD & DPM++(3M) & 0.006 & True & L1 & 4.57 \\   

Table 1: Quantitative results of the ablations.

To address this issue, consistency trajectory models (CTM)  introduce a new condition into the student model, which specifies the next time to arrive, referred to as the \(t_{}\)-condition. Unlike CTM, introducing a _step-condition_ to our SFD is more efficient. By informing the student model of the number of sampling steps, our SFD can perform sampling with different NFEs. We refer to this variable-NFE version of our method as SFD-v.

In every training iteration of SFD-v, the total number of sampling steps \(N\) is first sampled uniformly from a pre-specified list of steps. Then, the time schedule \((N)\) is generated, and the subsequent training process is the same as training SFD. As shown in the ablation study in Figure 8, the step-condition consistently outperforms the \(t_{next}\)-condition. For the injection of step-condition, we treat it the same way as the time embedding in diffusion models (see Appendix D.1 for more details). We include the algorithm of training SFD-v in Appendix C.

### Distillation under Classifier-free Guidance

Stable Diffusion , a latent diffusion model combined with classifier-free guidance , has shown to be highly effective in high-resolution image generation. The classifier-free guidance extends the flexibility of the generation of diffusion models by introducing the guidance scale \(\). Given a conditioning information \(c\), the noise-prediction model is rewritten as

\[}_{}(,t,c)=_{}( ,t,c)+(1-)_{}(,t,c=).\] (9)

However, Stable Diffusion requires a large number of network parameters and sampling steps to produce a satisfying generation. Moreover, the cost of every sampling step doubles since both conditional and unconditional evaluations are involved in Eq. 9. Distilling the Stable Diffusion model into a few steps is challenging because of source-intensive requirements and the flexibility given by the guidance scale. To address this issue, existing methods either introduce an \(\)-condition into their model [34; 22; 32], or simply discard the guidance scale [55; 46].

Here, we propose a new strategy with the observation on the sampling trajectories generated by Stable Diffusion under different guidance scales. Following the three-dimensional projection technique proposed in , we visualize the sampling trajectories generated by Stable Diffusion in its latent space using DPM-Solver++(2M) starting from 20 fixed latent encodings. As shown in Figure 9, sampling trajectories projected to the three-dimensional subspace exhibit a regular boomerang shape, which is consistent with the findings in the previous work . Furthermore, we observe that the sampling trajectories become more complex as the guidance scale increases, making trajectory distillation on the high guidance scale even more challenging. This observation naturally leads to our strategy: _perform distillation with a guidance scale of 1 and sampling with any guidance scale_. Our strategy enables accelerated training since the unconditional evaluation in Eq. 9 is eliminated.

## 4 Experiments

### Experiment Setting

**Pre-trained models and datasets**. Both the network parameters of student and teacher models are initialized from pre-trained diffusion models provided by EDM  and LDM . We report quantitative as well as qualitative results on datasets with various resolutions including CIFAR10

Figure 9: We visualize 20 sampling trajectories generated by DPM-Solver++(2M)  with 20 steps using the three-dimensional projection technique proposed in .

32\(\)32 , ImageNet 64\(\)64  and latent-space LSUN-Bedroom 256\(\)256 . For Stable Diffusion , we use the v1.5 checkpoint and generate images with a resolution of 512\(\)512.

**Training**. The configuration obtained in Section 3.2 can be applied to different NFEs and datasets. Generally, in the training of SFD and SFD-v, we use DPM-Solver++(3M)  as the teacher solver with \(K=4\) (see Appendix D.2 for an ablation study on \(K\)). The use of adjusted \(t_{}=0.006\), AFS and L1 loss introduced in Section 3.2 all lead to improved results. Minor changes are needed for text-to-image generation with Stable Diffusion, where we use DPM-Solver++(2M), which is the default setting used in Stable Diffusion and \(K=3\). In this case, \(t_{}\) is increased from 0.03 to 0.1 and the AFS is disabled due to the complex trajectory shown in Figure 8(c). These experiment settings are collected in Table 6 in Appendix.

**Optimization**. We use Adam optimizer  with \(_{1}=0.9\) and \(_{2}=0.999\) and a batch size of 128 across all datasets. A learning rate of 1e-5 is used for ImageNet and LSUN-Bedroom while 5e-5 is used in other cases. We divide the learning rate by 10 halfway through training. Our SFD is trained with a total of 200K teacher trajectories generated (around 1.5K training iterations). We train SFD-v to enable sampling with NFE from 2 to 5, the total training iterations is multiplied by 4 accordingly. All experiments are conducted with a maximum of 4 NVIDIA A100 GPUs. To enable a batch size of 128 using Stable Diffusion, we accumulate the gradient through several rounds.

**Evaluation**. We measure the sample quality via Frechet Inception Distance (FID)  with 50K images in general. For text-to-image generation, we use a guidance scale of 7.5 to generate 5K images with prompts from the MS-COCO  validation set. The FID is evaluated following the protocol in  where the validation set serves as reference images. The CLIP score is computed using the ViT-g-14 CLIP model  trained on LAION-2B .

### Main Results

We mainly compare our proposed SFD and SFD-v with progressive distillation  and consistency distillation . In Table 2 and 3, we report unconditional and conditional results on the pixel-space image generation. To compare the training cost, we estimate the training time measured by hours consumed on a single NVIDIA A100 GPU, following the training settings in the original papers. The detail of our estimation is included in Appendix B. Our SFD achieves comparable results as progressive distillation but only requires a very small fine-tuning cost (100\(\) to 200\(\) speedup). At the same time, it is hard for solver-based methods to give high-quality generation within a few steps

   Method & NFE & FID & 
 Training time \\ (A100 hours) \\  \\    \\ DDIM  & 10 & 15.69 & 0 \\  & 50 & 2.91 & 0 \\ DPM++(3M)  & 5 & 24.97 & 0 \\  & 10 & 3.00 & 0 \\ AMED-Plugin  & 5 & 6.61 & \(\) 0.08 \\  & 10 & 2.48 & \(\) 0.11 \\ GITS  & 5 & 8.38 & \(<\) 0.01 \\  & 10 & 2.49 & \(\) 0.01 \\   \\ PD  & 1 & 9.12 & \(\) 195 \\  & 2 & 4.51 & \(\) 171 \\ Guided PD  & 1 & 8.34 & \(\) 146 \\  & 2 & 4.48 & \(\) 128 \\  & 4 & 3.18 & \(\) 119 \\ CD  & 1 & 3.55 & \(\) 1156 \\  & 2 & 2.93 & \(\) 1156 \\ CTM  & 1 & 1.98 & \(\) 83 \\ CTM  w/o GAN loss & 1 & \(>\) 5 & \(\) 60 \\   \\
**SFD (ours)** & 2 & 4.53 & 0.64 \\  & 3 & 3.58 & 0.92 \\  & 4 & 3.24 & 1.17 \\  & 5 & 3.06 & 1.42 \\   \\  & 2 & 4.28 \\  & 3 & 3.50 \\  & 4 & 3.18 \\  & 5 & 2.95 \\   

Table 2: Results on CIFAR10 \(32 32\).

   Method & NFE & FID & 
 Training time \\ (A100 hours) \\  \\    \\ DDIM  & 10 & 16.72 & 0 \\  & 50 & 4.09 & 0 \\ DPM++(3M)  & 5 & 25.49 & 0 \\  & 10 & 5.67 & 0 \\ AMED-Plugin  & 5 & 13.83 & \(\) 0.18 \\  & 10 & 5.01 & \(\) 0.32 \\ GITS  & 5 & 10.75 & \(<\) 0.02 \\  & 10 & 4.48 & \(\) 0.02 \\   \\ PD  & 1 & 15.39 & \(<\) 5533 \\  & 2 & 8.95 & \(<\) 4611 \\ Guided PD  & 1 & 22.74 & \(<\) 5553 \\  & 2 & 9.75 & \(<\) 4611 \\  & 4 & 4.14 & \(<\) 4150 \\ CD  & 1 & 6.20 & \(<\) 7867 \\  & 2 & 4.70 & \(<\) 7867 \\ CTM  & 1 & 2.06 & \(<\) 902 \\  & 2 & 1.90 & \(<\) 902 \\   \\
**SFD (ours)** & 1 & 12.89 & 6.86 \\
**SFD (ours)** & 2 & 10.25 & 3.34 \\  & 3 & 6.35 & 4.63 \\  & 4 & 4.99 & 5.98 \\  & 5 & 4.33 & 7.11 \\   \\  & 2 & 9.47 \\  & 3 & 5.78 \\  & 4 & 4.72 \\  & 5 & 4.21 \\   

Table 3: Results on ImageNet \(64 64\).

due to increased errors. In accordance with our finding in Section 3.1, the SFD-v shows consistently better results than SFD, although the training cost of SFD-v and SFD is roughly the same for each specified sampling step. These observations also apply to the performance of SFD and SFD-v on the latent-space image generation on LSUN-Bedroom shown in Table 4. In Table 5, we show the performance of our methods in terms of FID and CLIP scores with a guidance scale of 7.5. The results demonstrate the effectiveness of our strategy proposed in Section 3.4 where the training is performed with the guidance scale set to 1. The qualitative results are shown in Figure 2. We include more results in Appendix D.2.

Although generating images with one NFE is possible with SFD and SFD-v, we find it suboptimal. To address this, we propose a second-stage one-NFE distillation, initializing network parameters from a fine-tuned SFD model. In this second stage, the teacher solver is set to DDIM (as used in SFD), and we use AFS (\(N=2\)) and \(K=2\). The training procedure remains the same as that of SFD. The results, marked as "second-stage", are reported in Tables 2 to 4. We provide an ablation study on the effectiveness of the second stage and the LPIPS metric  in Figure 10. The second-stage training significantly boosts performance and is more efficient. Additionally, combining L1 loss with LPIPS loss yields better results. Since the teacher requires a smaller NFE, the training of each iteration of the second-stage distillation is fast. Therefore, for CIFAR10/ImageNet, we perform second-stage distillation with 2000/800K sampling trajectories (around 15/6K training iterations), and the learning rate is set to 10 times larger. For LSUN-Bedroom, we use 800K trajectories and disable the LPIPS loss.

## 5 Conclusion

In this paper, we introduce **S**imple and **F**ast **D**istillation (SFD) of diffusion models to achieve fast and high-quality generation with diffusion models in a few sampling steps at minimal fine-tuning cost. Through a comprehensive investigation of several important factors, we unlock SFD's potential, achieving sample quality comparable to progressive distillation while reducing fine-tuning costs by over 100 times. To enable sampling with variable NFEs using a single distilled model, we propose SFD-v, which incorporates step-condition as an additional input. Our methods strike a good balance between sample quality and fine-tuning costs for few-step image generation, offering a new paradigm for distillation-based accelerated sampling of diffusion models.

**Limitation and future work**. Despite demonstrating efficient training, the FID results of our methods currently do not match those of the state-of-the-art. In future work, we plan to further explore the core mechanisms affecting the performance of trajectory distillation. Additionally, given the recent discoveries of the remarkable regular geometric structure of diffusion models , we aim to tailor an appropriate time schedule for our methods. We also intend to validate the effectiveness of the factors we have discussed in other distillation-based methods.

**Broader impacts**. Similar to existing works on content creation, our methods have the potential to be misused for malicious generation, which could have harmful social impacts. However, this risk

   Method & NFE & FID \\  DPM++(3M)  & 8 & 4.61 \\ AMED-Plugin  & 8 & 4.19 \\ PD  & 1 & 16.92 \\  & 2 & 8.47 \\ CD  & 1 & 7.80 \\  & 2 & 5.22 \\
**SFD (ours)** (second-stage) & 1 & 13.88 \\
**SFD (ours)** & 2 & 10.39 \\  & 3 & 6.42 \\  & 4 & 5.26 \\  & 5 & 4.73 \\
**SFD-v (ours)** & 2 & 9.25 \\  & 3 & 5.36 \\  & 4 & 4.63 \\  & 5 & 4.33 \\   

Table 4: Results on LSUN-Bedroom \(256 256\).

Figure 10: _Ablation study on one-NFE distillation._

   Method & Steps & FID-SK & CLIP Score \\  DPM++(2M)  & 2 & 91.5 (98.8*) & 0.20 (0.19*) \\  & 4 & 31.1 (34.1*) & 0.29 (0.29*) \\  & 8 & 25.1 (25.6) & 0.32 (0.30*) \\ Guided PD  & 2 & 37.3 & 0.27 \\  & 4 & 26.0 & 0.30 \\  & 8 & 26.9 & 0.30 \\ SnapFusion  & 8 & 24.2 & 0.30 \\
**SFD-v (ours)** & 2 & 42.9 & 0.24 \\  & 3 & 27.6 & 0.27 \\  & 4 & 24.2 & 0.28 \\  & 5 & 23.5 & 0.29 \\   

Table 5: Text-to-image generation with Stable Diffusion v1.5 . *: Reported in the original paper . We use a guidance scale of 7.5, which is the default setting in the original repository.

can be mitigated through advanced deepfake detection techniques. By continuously improving these detection methods, we can help ensure the responsible and ethical use of our technology.

## 6 Acknowledgement

Zhenyu Zhou and Can Wang are supported by the Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study, China (Grant No: SN-ZJU-SIAS-001), National Natural Science Foundation of China (Grant No: U1866602) and the advanced computing resources provided by the Supercomputing Center of Hangzhou City University.