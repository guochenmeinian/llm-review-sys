# Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs

Emmanuel Abbe

EPFL

emmanuel.abbe@epfl.ch

&Elisabetta Cornacchia

EPFL, MIT

ecornacc@mit.edu

&Aryo Lotfi

EPFL

aryo.lotfi@epfl.ch

###### Abstract

Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer \(\) neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the specific regime of the theoretical results.

## 1 Introduction

Starting with easy concepts is an effective way to facilitate learning for humans. When presented with simpler ideas or tasks, individuals can develop a foundation of knowledge and skills upon which more complex concepts can be built. This approach allows learners to gradually gain competence and accelerate learning .

For machine learning algorithms, this is typically referred to as _Curriculum Learning (CL)_. Several empirical studies have shown that presenting samples in a meaningful order can improve both the speed of training and the performance achieved at convergence, compared to the standard approach of presenting samples in random order .

The theoretical study of CL is mostly in its infancy. While some works have analytically shown benefits of CL for certain targets  (see Section 1.1 for a discussion on these), it remained open to provide a separation result between curriculum training and standard training on a common sampling distribution. Specifically, one seeks to demonstrate how a standard neural network, trained using a gradient descent algorithm on a given dataset and horizon, can successfully learn the target when certain samples are presented first during training, whereas learning does not occur if samples are presented in a random order.

This paper provides such an instance. We focus on the case of parities, as a classical case of challenging target for differentiable learning. In the experimental part of the paper, we report results suggesting that a similar picture can hold for other large leap functions as defined in , although more investigations are required to understand when and how curriculum learning should be developed for more general functions.

We assume that the network is presented with samples \((x,_{S}(x))\), with \(x\{ 1\}^{d}\) and \(_{S}(x)_{i S}x_{i}\), for some set \(S\{1,,d\}\) of fixed size. We consider datasets where a small fraction \(\) ofthe inputs are _sparse_, meaning they have, on average, few negative coordinates and mostly positive coordinates, and the remaining inputs are _dense_, with an average of half negative and half positive coordinates. We refer to Section 2 for the definition of the input distribution. Parities seem to be promising candidates for theoretical investigations into CL, as they are hard to learn in the statistical query learning model , or on dense data by any neural network trained with gradient-based methods having limited gradient precision , but they are efficiently learnable on sparse data by regular architectures  (see Section 1.1 for further discussion).

We say that a neural network is trained _with curriculum_ if it goes through an initial phase where it is exposed to batches consisting only of sparse samples (see discussion below on the interpretation of these as'simpler' examples). Following this phase, the network is trained using batches sampled from the entire dataset. We compare this curriculum training approach with standard training, where the network receives batches of samples selected randomly from the entire dataset at each step. Our intuition is as follows. If the fraction of sparse samples is small, without curriculum, the network is exposed to batches of mostly uniform inputs, and the contribution of sparse inputs in estimating the gradients will be limited. As a result, learning parities on the mixed distribution without curriculum is roughly as hard as learning parities on dense inputs (Theorem 2). On the contrary, if the network is initially exposed to batches consisting of sparse inputs, it can efficiently identify the support of the parity, and subsequent fitting using batches sampled from the entire dataset enables learning the target with fewer training steps compared to standard training (Theorem 1).

In particular, we show that if the fraction of sparse samples \(\) is small enough, (e.g., \(<d^{-4.5}\), where \(d\) is the input dimension), a \(2\)-layer fully connected network of size \((d)\) trained by layerwise SGD (or noisy-SGD, see Def. 1) with curriculum can learn any target \(k\)-parity, with \(k\) bounded, in \(T=(d)\) steps, while the same network (or any network of similar size) trained by noisy-SGD without curriculum cannot learn the target in less than \((d^{2})\) steps. We refer to Section 2 for a more detailed explanation of our results. While our theoretical results use simplifying assumptions (e.g., layerwise training), our experiments show separations between curriculum and standard training in a broader range of settings.

### Related Literature

Curriculum Learning (CL) was first introduced in the machine learning context by the seminal work of . This paper along with many other subsequent works presented empirical evidence that training neural networks with a curriculum can be beneficial in both the training speed and the performance achieved at convergence in certain instances in different domains such as computer vision , natural language processing (NLP) , and reinforcement learning . We refer to relevant surveys  for a more comprehensive list of applications of CL in different domains. In general, defining a complexity measure on samples and consequently an ordering of training inputs is not trivial. Some works determine the easiness of samples based on predefined rules (e.g., shape as used in ), while others also use the model's state and performance to evaluate the samples dynamically as in self-paced learning (SPL) . In this work, we follow the former group as we define the hardness of samples based on the number of \(-1\) bits, i.e., the Hamming weight. Note that in parity targets, \(+1\) is the identity element. Thus, one can view the number of \(-1\) bits as the _length_ of the sample. In this sense, our work resembles  which use the length of the text inputs as a part of their curriculum.

Despite the considerable empirical work on CL, there appears to be a scarcity of theoretical analysis. Few theoretical works focused on parity targets, as in this paper. While parities are efficiently learnable by specialized algorithms that have access to at least \(d\) samples (e.g. Gaussian elimination over the field of two elements), they are hard to learn under the uniform distribution (i.e. on dense data) by any neural network trained with gradient-based methods with limited gradient precision . Parities on sparse data can be learned more efficiently on an active query model, which can be emulated by a neural network . However, such emulation networks are far from the practical and homogeneous networks, and distribution class-dependent. A natural question that has been considered is whether more standard architectures can also benefit from a simpler data distribution. In , the authors make a first step by showing that parities on sparse inputs are efficiently learnable on a 1-layer network with a parity module appended to it, with a one-step argument. In , it is shown that for a two-layer fully connected network, sparse inputs help but with a variant of the distribution that allows for leaky labels.  obtains experimental results for a curriculum algorithm presenting sparse samples first with heuristics for regular networks and hyperparameters, but without proofs. In , the authors prove that a two-layer fully connected net can benefit from sparse inputs, with a one-step gradient argument requiring a large learning rate. This paper improves on  not only in terms of having more natural training settings and hyperparameters (i.e., we use SGD with bounded batch size and bounded learning rate), it is also establishing a separation between learning with and without curriculum on a common sampling distribution, while  does not (in fact in their work curriculum involves both sparse and dense inputs while standard training involves dense inputs only). To the best of our knowledge, our paper is the first work that establishes a rigorous separation between training on adequately ordered samples and randomly ordered samples drawn from the same distribution, and our theorem gives a condition on the mixture under which this separation holds (interestingly the separation does not take place for all mixtures parameters). Beyond parities,  studies a teacher-student model, where the target depends on a sparse set of features and where the variance on the irrelevant features. Furthermore,  show that on some convex models, CL provides an improvement in the speed of convergence of SGD. In contrast, our work covers an intrinsically non-convex problem.

## 2 Setting and Informal Contributions

We consider the problem of learning a target function \(f\{ 1\}^{d}\{ 1\}\) that belongs to the class of \(k\)-parities on \(d\) bits, i.e.,

\[f\{_{S}(x)=_{j S}x_{j}:S[d],|S|=k\},\] (1)

where we denoted by \([d]\{1,,d\}\). We assume that the network has access to a dataset \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\) where the inputs \(x^{s}\) are sampled i.i.d. from a _mixed_ distribution defined as follows:

\[_{}=_{}+(1-)_{u},\] (2)

where \(_{}=()^{ d1}\), for some \([-1,1]\), \(_{u}=\{ 1\}^{d2}\) and \(\) is some fixed parameter, and \(y^{s}=f(x^{s})\). We will assume \((0,1)\). In other words, we assume that the dataset contains approximately \( m\) samples with in average \(d\) negative bits (sparse samples), and \((1-)m\) samples with in average half negative bits (dense samples). We consider training the neural network with stochastic gradient descent, with or without gradient noise (SGD or noisy-SGD, see Def. 1).

Informal Description of the Curriculum and Standard Training Algorithms.We compare two training strategies: standard training and curriculum training. In the standard training, at each step, mini-batches are drawn from the whole dataset. For curriculum training, we first need to isolate the set of sparse inputs in our dataset. Let us denote this set by \(X_{1}:=\{x^{s} X:H(x^{s})<(-)d\}\), where \(H(x):=_{j[d]}(x_{j}=-1)\) denotes the _Hamming weight_ of \(x\), which is the count of negative bits in \(x\), and by \(Y_{1}=\{y^{s} Y:x^{s} X_{1}\}\) the corresponding set of labels. Our curriculum strategy consists of training on only sparse samples for the first \(T_{1}\) steps, where \(T_{1} T\) is some well-chosen time horizon and \(T\) is the total number of training steps. After this initial phase, the network is trained on samples belonging to the whole dataset. To sum up, we compare the following two training strategies:

* (standard training): At all \(t[T]\), mini-batches are sampled from \((X,Y)\);
* (curriculum training): At \(t T_{1}\), mini-batches are sampled from \((X_{1},Y_{1})\) and at \(T_{1}<t T\) mini-batches are sampled from \((X,Y)\).

Informal Description of the Positive and Negative Results.In this paper, we prove that if the fraction of sparse samples \(\) is small enough, there is a separation in the number of training steps needed to learn the target parity up to a given accuracy. We say that an algorithm on a neural network learns a target function \(f\{ 1\}^{d}\{ 1\}\) up to accuracy \(1-\), for \( 0\), in \(T\) steps, if it outputs a network \((x;^{T})\) such that:

\[_{x_{}}(((x; ^{T}))=f(x)) 1-,\] (3)

where \((.)\) denotes the sign function. In our main positive result, we consider training with the _covariance loss_ (Def. 2) and we refer to Remark 2 for considerations on other losses. Let us state our main positive result informally.

**Theorem 1** (Theorem 3, Informal).: _Consider the input distribution \(_{}\) with mixture parameters \(=(d^{-1/2})\)3, \(=(1)\). Let \(>0\). Then, a \(2\)-layer \(\) neural network of size \(O(d)\) initialized with an isotropic distribution, trained with the covariance loss, standard (i.e., non-diverging) learning rate schedule and either (i) a layer-wise curriculum-noisy-SGD algorithm with gradient range \(A=()\), noise-level \(=(d^{-1})\) and sparse samples first, or (ii) a layer-wise curriculum-SGD algorithm with sparse samples first, can learn any \(k\)-parities, \(k=(1)\), up to accuracy at least \(1-\) in \(T=(d)/^{2}\) steps._

We further show an additional positive result valid for a layer-wise curriculum SGD with the hinge loss, which holds for large batch size and diverging learning rate (Theorem 4). On the flip side, we have the following.

**Theorem 2** (Theorem 5, Informal).: _Consider the input distribution \(_{}\) with mixture parameters \(=(d^{-3.5-})\), \(>0\), and \(=(1)\). Then, any neural network of size \((d)\) (any depth), with any initialization and activation, trained by a noisy-SGD algorithm (layer-wise or joint) with gradient range \(A=()\), noise-level \(=(d^{-1})\), batch size \(B=(^{-2})\), any almost surely differentiable loss function and any learning rate schedule, will need at least \(T=( d^{1+})\) steps to achieve accuracy \(+\) for any \(k\)-parities (\(k 6\))._

Conclusion of Results.When using noisy-SGD, the curriculum learning algorithm that takes sparse samples first, for the first layer training, and then full samples, can learn in a regime of hyperparameters and training steps (Theorem 1) that is contained in the hyperparameter regime of the negative result (Theorem 2), for which it is shown that the absence of curriculum cannot learn without additional training steps. Further, it is shown in part (ii) of the first theorem, that using SGD rather than noisy-SGD still allows to learn with the curriculum training; this result does however not come with a counter-part for the lower-bound, i.e., we cannot show that SGD without curriculum could not learn such parities, as there is currently no proof technique to obtain rigorous lower-bounds for SGD algorithms (as for the case of noiseless honest-SQ algorithms); see for instance discussions about this in . However, it is not expected that SGD can do better than noisy-SGD on'regular' neural networks .

## 3 Positive Results for Curriculum Training

The Algorithm.Let us define the noisy-SGD algorithm, that is used in both our positive and negative results.

**Definition 1** (Noisy-SGD).: _Consider a neural network \((.;)\), with initialization of the weights \(^{0}\), and a dataset \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\). Given an almost surely differentiable loss function \(L\), the updates of the noisy-SGD algorithm with learning rate \(_{t}\) and gradient range \(A\) are defined by_

\[^{t+1}=^{t}-_{t}(_{s=1}^{B}[_ {^{t}}L((x^{s,t};^{t}),y^{s,t},x^{s,t})]_{A}+Z^ {t}),\] (4)

_where for all \(t\{0,,T-1\}\), \(Z^{t}\) are i.i.d. \([-,]\), for some noise level \(\), and they are independent from other variables, \(B\) is the batch size, and \([z]_{A}:=_{|y| A}\,|y-z|\), i.e., whenever the gradient exceeds \(A\) (resp. \(-A\)) it is rounded to \(A\) (resp. \(-A\))._

For brevity, we will write \(L(^{t},y,x) L((x;^{t}),y,x)\). We assume that for all \(s[B]\), \((x^{s,t},y^{s,t})\) are chosen from \((X_{t},Y_{t})\), where \((X_{t},Y_{t})\) can be either the whole dataset \((X,Y)\), or a subset of it. We include the noise \(Z^{t}\) and the gradient range \(A\) to cover the noisy-SGD algorithms,used in SQ lower bounds e.g. in . Note that if we set \(=0\) and \(A\) large enough, we recover the standard SGD algorithm without noise.

We assume that our dataset is drawn from the mixed distribution defined in (2), with parameters \(,\). For the purposes of this section, we assume \(=(d^{-1/2})\) and \((0,1)\). As a preliminary step, we isolate the sparse inputs from our dataset, by mean of Hamming weight. In particular, we define \(X_{1}:=\{x^{s} X:H(x^{s})<(-)d\}\), where \(H(x):=_{j[d]}(x_{j}=-1)\) denotes the _Hamming weight_ of \(x\), and \(Y_{1}=\{y^{s} Y:x^{s} X_{1}\}\). Note that, due to the concentration of Hamming weight, for \(d\) large enough, there will be approximately \(m\) samples in \(X_{1}\) (see Lemma 1). We assume that the training set is large enough, such that each sample is observed at most once during training.

In this section, we consider a \(2\)-layer neural network \((x;)\), with \(=(a,w,b)\), defined by: \((x;)_{i=1}^{N}a_{i}(w_{i}x+b_{i})\), where \((x)=(x)\{x,0\}\). We assume that the number of hidden units is \(N k+1\), where \(k\) is the degree of the target parity. We initialize \(w_{ij}^{0}=0\), \(a_{i}^{0}=\) and \(b_{i}^{0}= d+1\), for some \(,>0\), for all \(i,j\). We refer to \(\) and \(\) as the second layer initialization scale and the bias initialization scale, respectively. We adopt a layer-wise curriculum training approach. During the initial phase of training, we exclusively train the first layer of the network using samples taken from \((X_{1},Y_{1})\). Furthermore, we project the weights of the first layer, to guarantee that they stay bounded. In the second phase, we train only the second layer using the entire merged dataset. We refer to Algorithm 1 in Appendix A for the pseudo-code of the algorithm used in our proof. In the context of standard learning without curriculum, similar layer-wise training strategies are considered in e.g., .  uses a layer-wise curriculum training, but with a one-step argument that requires a diverging learning rate. In our analysis, we keep the bias weights fixed to specific values during the whole training. We believe that one could extend the result to include scenarios with more general initializations and where the bias weights are trained, but this would require further technical work.

The Covariance Loss.We train \((x;)\) using noisy-SGD with a specific loss function, namely the _covariance loss_, that we define here. The covariance loss appears in  with a slightly different definition: in particular, the following definition does not depend on the average estimator output, as opposed to their definition.

**Definition 2** (Covariance Loss).: _Let \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\) be a dataset, where for all \(s\), \(x^{s}\) and \(y^{s}\{ 1\}\), and let \(=_{s[m]}y^{s}\). Assume \(||<1\). Let \(\{ 1\}\) be an estimator. We define the covariance loss as_

\[L_{}(,y^{s},x^{s})(1-y^{s}-(x^ {s})(y^{s}-))_{+},\] (5)

_where \((a)_{+}\{a,0\}\)._

The covariance loss can be applied to classification datasets with non-negligible fractions of \(+1\) and \(-1\) labels. In Proposition 1, we show that a small covariance loss implies a small classification error.

**Proposition 1**.: _Let \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\) be a dataset, where for all \(s\), \(x^{s}\) and \(y^{s}\{ 1\}\). Let \(=_{s[m]}y^{s}\) and assume \(||<1-\), for some \(>0\). Let \(\{ 1\}\) be an estimator. If \(_{s[m]}L_{}(,x^{s},y^{s})<\), then \(_{s[m]}(((x^{s})) y^{s})< \)._

The proof of Proposition 1 can be found in Appendix D.

_Remark 1_.: We remark that for balanced datasets (\(=0\)), the covariance loss coincides with the hinge loss. On the other hand, note that the right hand side of (5) can be rewritten as \(((1-y^{s})(1-y^{s}(x^{s})))_{+}\). Thus, in some sense, the samples in the under-represented class (where \(y^{s} 0\)) are assigned a higher loss compared to those in the over-represented class (where \(y^{s} 0\)).

Main Theorem.We are now ready to state our main theorem.

**Theorem 3** (Main Positive Result).: _Let \((x;)\) be a \(2\)-layer \(\) neural network with \(N k+1\) hidden units, 2nd layer init. scale \(>0\) and bias init. scale \(>0\). Let \(_{}\) be a mixed distribution, as defined in (2), with parameters \(0<}{}\), and \((0,1)\). Assume we have access to a dataset \((X,Y)=\{x^{s},y^{s}\}_{s[m]}\), where \(x^{s}}{{}}_{}\), \(y^{s}=_{S}(x^{s})\), with \(|S|=k\). Assume \(\) is trained according to the layer-wise curriculum-SGD algorithm (Algorithm 1), with the covariance loss, batch size \(B\), noise level \(\) and gradient range \(A\). For all \(>0\), there exist \(C,C_{1},C^{*}>0\) such that if \( 1/N(2 d+2)\), \(\), \(A=()\), \(m 4C^{*}(3d)d^{-1}B\), and if_

\[T_{1} =d(d)^{2}}{L_{}^{2}}, _{1}=}{C d(d)^{2}},\] (6) \[T_{2} =+2 k+)^{2}N^{2}d}{^{2}^{ 2}(d)}, _{2}=+2 k+)^{2}dN},\] (7)

_where \(L_{}=^{k-1}-^{k+1}\), then, with probability at least \(1-6Nd^{-C^{*}}\), the algorithm outputs a network such that \(_{s[m]}L_{}(^{T_{1}+T_{2}},x^{s},y^{s})\)._

The following corollary is a direct consequence of Theorem 3 and Proposition 1.

**Corollary 1**.: _Let the assumptions of Theorem 3 be satisfied and assume that \(=O()\) and \(,k\) constants. There exists a \(2\)-layer \(\) network of size at most \((d+2)(k+1)\) and initialization invariant to permutations of the input neurons, such that for all \(>0\) the layer-wise curriculum noisy-SGD algorithm with gradient range \(A=()\), after at most \(T=(d)\) steps of training with bounded learning rates, outputs a network such that \(_{x_{}}(((x; ^{T}))=f(x)) 1-,\) with high probability._

_Remark 2_.: The choice of the covariance loss is motivated by a simplification of the proof of Theorem 3. Indeed, with this loss, we can show that in the first part of training the weights incident to the relevant coordinates move non-negligibly, while the other weights stay close to zero (see proof outline). If we used another loss, say the hinge loss, all weights would move non-negligibly. However, we believe that one could still adapt the argument to cover the hinge loss case, by adding another projection to the iterates. In the following, we also give a positive result for the hinge loss, with a different proof technique, that holds for large batch size and large learning rate (Theorem 4).

_Remark 3_.: While we show that with appropriate hyperparameters, the layer-wise curriculum-SGD can learn \(k\)-parities in \((d)\) steps, we need to notice that to implement the curriculum, one has to first retrieve \((X_{1},Y_{1})\). This involves computing the Hamming weight of \(d/\) samples, which adds computational cost if the curriculum learning includes this part. Nevertheless, in the experiments that we performed, the recovery of \((X_{1},Y_{1})\) was consistently efficient, with an average of less than \(1\) second running time for a dataset of \(10\) million samples of dimension \(100\). On the other hand, the reduction of training steps due to the curriculum yields significant time savings.

_Remark 4_.: Theorem 3 provides an upper bound on the sample complexity required to learn using a curriculum, of \((dB/)\) for SGD with batch size \(B\). Consequently, while the number of iterations needed for learning remains unaffected by \(\), the sample complexity increases as \(\) decreases. Additionally, we do not currently have a lower bound on the sample complexity for learning parities with standard training with offline SGD. However, our experiments in Section 5 demonstrate a noticeable difference in the sample complexity required for learning with and without a curriculum. These findings may encourage future work in establishing a theoretical advantage in the number of samples needed to learn with curriculum.

_Remark 5_.: We further remark that the proof method of Theorem 3 yields a tighter upper bound on curriculum learning's sample complexity than the 1-step argument used in . Specifically, using the technique of , we achieve an upper bound of \((d^{2}/)\), as opposed to the \((d/)\) given by Theorem 3 for SGD with bounded batch size.

_Remark 6_.: \(T_{1}\) depends on the degree of the parity \(k\) through the parameter \(L_{}\). We believe that such dependence cannot be removed, however, we remark that choosing \(\) sufficiently close to 1 (e.g. \(=1-\)) allows bounding \(L_{}\) independently from \(k\). Indeed, our experiments (Figure 4) show mild dependence on the parity degree for curriculum learning with large \(\).

Proof Outline.The proof of Theorem 3 follows a similar strategy as in . We decompose the dynamics into a drift and a martingale contribution, and we show that the drift allows us to recover the support of the parity in \((d)\) steps, while in the same time horizon, the martingale contribution remains small, namely of order \(O(1/)\). In particular, we show that after the first phase of training, with high probability, all weights corresponding to coordinates in the parity's support are close to \(\) and the others are close to zero. The fit of the second layer on the whole dataset corresponds to the analysis of a linear model and follows from standard results on the convergence of SGD on convex losses (e.g., in ). The formal proof can be found in Appendix A.

Hinge Loss.While the use of the covariance loss simplifies considerably the proof of Theorem 3, we believe that the advantages of curriculum extend beyond this specific loss. Here, we present a positive result that applies to a more commonly used loss function, namely the hinge loss.

**Theorem 4** (Hinge Loss).: _Let \((x;)=_{i=1}^{N}a_{i}(w_{i}x+b_{i})\) be a 2-layer fully connected network with activation \((y)(y)\) and \(N=(d^{2}(1/))\). Assume we have access to a dataset \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\), with \(x^{s}}{{}}_{}\), with parameters \(>0,(0,1)\), and \(y^{s}=_{S}(x)\), for some target parity \(_{S}\), with \(|S|=k\). Then, there exists an initialization and a learning rate schedule, such that for any target parity \(_{S}\) and for any \(>0\), the layer-wise curriculum SGD on the hinge loss with batch size \(B=(d^{10}/^{2}(1/))\), noise level \(=(}{d^{}})\), \(T_{1}=1\) and \(T_{2}=(d^{6}/^{2})\) with probability \(1-3\), outputs a network such that \(_{x_{}}(((x; ^{T}))=f(x)) 1-\)._

As opposed to Theorem 3, Theorem 4 only holds for SGD with large batch size and unbounded learning rate schedules. The proof of Theorem 4 follows a 1-step argument, similarly to , and it is deferred to Appendix B.

## 4 Negative Result for Standard Training

In this section, we present a lower bound for learning \(k\)-parities on the mixed distribution of eq. (2), without curriculum.

**Theorem 5** (Main Negative Result).: _Assume we have access to a dataset \((X,Y)=\{(x^{s},y^{s})\}_{s[m]}\), with \(x^{s}}{{}}_{}\), with parameters \(=o_{d}(1),(0,1)\), and \(y^{s}=_{S}(x)\), with \(|S|=k\). Let \((x;)\) be a fully-connected neural network of size \(P\), with initialization that is invariant to permutations of the input neurons. Then, the noisy-SGD algorithm with noise level \(\), gradient range \(A\), batch size \(B\), any learning rate schedule, and any loss function, after \(T\) steps of training without curriculum, outputs a network such that_

\[_{x_{}}(((x; ^{T}))=f(x))+(^{-1 }+C_{k}^{2}^{4k}+)^{1/2},\] (8)

_where the probability is over \(x_{}\) and any randomness in the algorithm, and where \(C_{k}\) is a constant that depends on \(k\)._

_Remark 7_.: For the purposes of Theorem 5, the network can have any fully connected architecture and any activation such that the gradients are well-defined almost everywhere. Furthermore, the loss can be any function that is differentiable almost everywhere.

Theorem 5 follows from an SQ-like lower bound argument and builds on previous results in [Theorem 3]. We defer the formal proof to Appendix C. Theorem 5 implies the following corollary.

**Corollary 2**.: _Under the assumptions of Theorem 5, if \(=(d^{-3.5-})\) and \(k 6\), any fully connected network of size at most \((d)\) with any permutation-invariant initialization and activation, trained by the noisy-SGD algorithm with batch size \(B=(^{-2})\), noise-level \(=(1/d)\) and gradient range \(A=()\), after \(T=(d)\) steps of training, will output a network such that \(_{x_{}}(((x; ^{T}))=f(x))+Cd^{-}(d)^{c}\), for some \(c,C>0\)._

In particular, in Corollary 2, the network can have any width and depth (even beyond \(2\) layers), as long as the total number of parameters in the network scales as \((d)\) in the input dimension. Combining Corollary 1 and Corollary 2, we get that if \(=(d^{-3.5-})\), for some \(>0\), and \(k 6\), and for \(=1/10\), say, a \(2\)-layer fully connected network can learn any \(k\) parities up to accuracy \(1-\) in \(T=(d)\) steps with curriculum, while the same network in the same number of steps without curriculum, can achieve accuracy at most \(1-2\), for \(d\) large enough.

_Remark 8_.: While our results show a first separation in the number of training steps between curriculum and standard training, we do not claim that the given range of \(\) is maximal. For instance, one could get a tighter negative bound by considering Gaussian, instead of uniform, noise in the noisy-SGD iterates. However, getting a positive result with the curriculum for Gaussian noise would require a more involved argument. Our experiments show separation in the number of training steps and in the sample complexity for different values of \(\).

## 5 Experiments

In this section, we present empirical results demonstrating the benefits of the proposed curriculum method.4 We use a multi-layer perceptron (MLP) with 4 hidden layers of sizes \(512\), \(1024\), \(512\), and \(64\) as our model. We optimize the model under the \(_{2}\) loss using mini-batch SGD with batch size 64. We also train all layers jointly. Each of our experiments is repeated with 10 different random seeds, and the results are reported with \(95\%\) confidence intervals. Note that generalization error is always computed based on \(_{}\). Here, we mostly focus on the common settings not covered by our theoretical results. We present additional results on other architectures, namely, two-layer mean-field networks  and Transformers  and covariance and hinge loss functions in Appendix E.2.

Number of Samples.First, we focus on the difference in sample complexity, between curriculum and standard training. We consider learning \(f(x_{1},,x_{100})=x_{1}x_{2}x_{3}x_{4}x_{5}\), with inputs sampled from a mixed distribution with \(=0.98\) and different values of \(=10^{-4},10^{-3},10^{-2},0.1,0.5\). For each \(\), we train the network on training sets of different sizes (between \(10^{4}\) and \(10^{6}\)) with and without curriculum. When using the curriculum, we first select the sparse samples as those with Hamming weight smaller than \((-)d\), and we train on those until convergence (specifically, training loss smaller than \(10^{-2}\)). We then train the model on the entire training set until convergence. Similarly, when the curriculum is not used, the model is trained using all the available samples until convergence.

Figure 1 shows the validation accuracy achieved for different values of \(,m\). It can be seen that for some values of \(\), there is a reduction in the number of samples needed to achieve validation accuracy close to \(1\) when the curriculum is used. For the purpose of visualization, in Figure 1 (bottom-right), we sketch the number of samples needed to achieve accuracy close to \(1\) for different values of \(\), based on the previous plots. We notice that the sample complexity decreases as \(\) is increased, for both curriculum and standard training. Among the \(\) values that we tried, the gap between the two training strategies is maximal for \(=0.01\), and it decreases as \(\) gets larger or smaller. Also, note

Figure 1: Comparison of the performance of the curriculum strategy and standard training for different training set sizes and values of \(\). In the bottom-left plot, we report the number of samples needed to achieve accuracy close to \(1\) for different values of \(\) based on the other plots.

that for small values of \(\) the sample complexity for standard training is constant and equals \(10^{6}\). We note that with \(10^{6}\) samples, we can learn \(f(x)\) even with uniform inputs (\(=0\)).

Number of Training Steps.Now we focus on the number of iterations needed for learning the same \(f\) as above. In order to study the number of iterations independently of the number of samples, we draw fresh mini-batches at each step. When using the curriculum, we sample from \(_{}\) in the initial phase and from \(_{}\) in the second phase, while when using standard training we sample from the mixed distribution at each step. In all cases, we train until convergence. The number of iterations needed for learning \(f\) for different values of \(\) is depicted in Figure 2 (top-left). It can be seen that the number of training steps needed for learning with the curriculum is constant w.r.t. \(\). In contrast, for standard training, the number of iterations increases with the decrease of \(\). Both these observations are consistent with our theoretical results. In particular, for small values of \(\), the curriculum significantly reduces the number of training steps, while for large \(\) it provides little or no benefit, and for \(=0.5\), it is harmful.

Dependency on \(\) and \(k\).Next, we investigate the dependency of our results on \(\) and on the degree of parity \(k\). For evaluating dependency on \(\), we consider the function \(f\) as above and different values of \(\). First, we consider the number of training steps needed for learning \(f\) based on \(\). The results are shown in Figure 2 (top-right). It can be seen that while the curriculum is not beneficial for \(=0.02\) (where \(_{}\) is close to \(_{u}\) and all samples are dense), it can significantly reduce the number of training steps for moderate and large values of \(\). Further, we fix \(=0.01\) and compare the performance of the curriculum and standard training for different training set sizes. The results are demonstrated in Figure 2 (bottom-right). It is shown that for large values of \(\) such as \(0.98\) and \(0.90\), there is a gap in the number of samples needed for achieving generalization accuracy close to \(1\) between curriculum and standard training. On the other hand, for smaller values of \(\), this gain is diminished. Nonetheless, note that the relationship between the performance of the curriculum method and \(\) is not monotonous (e.g., see the number of steps plot), and this is indeed reflected in our analysis. More precisely, in Theorem 1, \(T_{1}\) scales with \(1/L_{}^{2}\) where \(L_{}=^{k-1}-^{k+1}\).

Figure 2: (Top-left) Number of training steps needed for learning \(5\)-parity for different values of \(\). The gap between curriculum and standard training decreases as \(\) increases. (Top-right) The number of training steps for different values of \(\) based on \(\). (Bottom-right) Performance for different values of \(\) and different sample sizes. Curriculum boosts the learning for large values of \(\). (Bottom-left) Performance for different parity functions. (The curves of standard training for \(k=7\) and \(k=10\) overlap.) The benefit of curriculum potentially increases with the degree of the parity.

To analyze the effect of the degree of parity, \(k\), we fix \(=0.98,=0.01\) and the input dimension \(d=100\), and we compare the performance of the model trained with different amounts of training data. The results are portrayed in Figure 2 (bottom-left). One can see that, while without curriculum the parity problem becomes harder to learn as \(k\) increases, the complexity with curriculum has mild dependence on \(k\) and it allows one to learn all the parities considered with \(10^{5}\) samples. The plot exhibiting the dependence of the number of optimization steps on \(k\) is reported in Appendix E.2.

Beyond Parities.Finally, we study target functions composed of multiple monomials. We consider three examples in 100-dimensional space: (i) \(f_{}(x)=x_{1}x_{2}x_{3}x_{4}x_{5}+x_{1}x_{2}x_{3}x_{4} x_{5}x_{6}\), (ii) \(f_{}(x)=x_{1}x_{2}x_{3}x_{4}x_{5}+x_{6} x_{11}\), and (iii) \(f_{}(x)=(x_{1}x_{2}+x_{1} x_{6})\). We also fix \(=0.98\). In Figure 3, we investigate the gain of curriculum method in sample complexity for \(=0.01\) (top row) and the number of training steps (bottom row) needed to learn. The plots in the bottom row are obtained by training the network with fresh batches. The left, middle, and right column correspond to \(f_{}\), \(f_{}\), and \(f_{}\), respectively. Note that curriculum is beneficial for \(f_{}\), while it hardly helps with \(f_{}\). Interestingly, for \(f_{}\), curriculum is advantageous for weak learning of the function. More specifically, it helps learning \(x_{1}x_{2}x_{3}x_{4}x_{5}\) monomial of \(f_{}\). Consequently, for the corresponding iterations plot (Figure 3 bottom-middle), we plot the number of iterations needed for getting a loss below \(0.26\). This suggests that the curriculum method put forward in this paper mostly helps with learning of the monomial with the lowest degree. We leave improvements of our curriculum method to future work.

## 6 Conclusion

In this paper, we introduced a curriculum strategy for learning parities on mixed distributions. Our approach involves training on sparse samples during the initial phase, which leads to a reduction in the number of required training steps and sample complexity compared to standard training. In the experiments section, we presented some results for certain functions with multiple monomials. Although a complete picture requires further investigation, these results suggest that starting from sparse samples provides advantages in the early stages of training. It would be interesting to investigate what variation of our curriculum strategy fits best the case of multiple monomials, beyond the initial phase. For example, one could explore multiple curriculum phases where training progresses from samples with increasing Hamming weight, or employ a self-paced curriculum that dynamically selects samples during training. Such analyses could offer valuable insights into the broader applicability of our approach. Here we focused mainly on establishing a formal separation in the number of training steps for the canonical case of parities.

Figure 3: Considering gains in sample complexity (top row) and number of steps (bottom row) beyond parity targets. Plots in the left (corresponding to \(f_{}(x)=x_{1}x_{2}x_{3}x_{4}x_{5}+x_{1}x_{2}x_{3}x_{4} x_{5}x_{6}\)), middle (\(f_{}(x)=x_{1}x_{2}x_{3}x_{4}x_{5}+x_{6} x_{11}\)), and right (\(f_{}(x)=x_{1}x_{2}+x_{1} x_{6}\)) columns present cases in which curriculum is beneficial, is only beneficial for weak learning, and is not beneficial, respectively.