# Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness

Chung-En Tsai

Department of Computer Science and Information Engineering

National Taiwan University

chungentsai@ntu.edu.tw

Ying-Ting Lin

Department of Computer Science and Information Engineering

National Taiwan University

R08922060@ntu.edu.tw

Yen-Huan Li

Department of Computer Science and Information Engineering

Department of Mathematics

Center for Quantum Science and Engineering

National Taiwan University

yenhuan.li@csie.ntu.edu.tw

###### Abstract

This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-round time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.

## 1 Introduction

Designing an optimal algorithm for online portfolio selection (OPS), with respect to both regret and computational efficiency, has remained a significant open problem in online convex optimization for over three decades1. OPS models long-term investment as a multi-round game between two strategic players--the market and the Investor--thereby avoiding the need for hard-to-verify probabilistic models for the market. In addition to its implications for robust long-term investment, OPS is also a generalization of probability forecasting and universal data compression .

The primary challenge in OPS stems from the absence of Lipschitzness and smoothness in the loss functions. Consequently, standard online convex optimization algorithms do not directly apply. For example, standard analyses of online mirror descent (OMD) and following the regularized leader (FTRL) bound the regret by the sum of the norms of the gradients (see, e.g., the lecture notes byOrabona  and Hazan ). In OPS, the loss functions are not Lipschitz, so these analyses do not yield a sub-linear regret rate. Without Lipschitzness, the self-bounding property of smooth functions enables the derivation of a "small-loss" regret bound for smooth loss functions . However, the loss functions in OPS are not smooth either.

The optimal regret rate of OPS is known to be \(O(d T)\), where \(d\) and \(T\) denote the number of investment alternatives and number of rounds, respectively. This optimal rate is achieved by Universal Portfolio . Nevertheless, the current best implementation of Universal Portfolio requires \(O(d^{4}T^{14})\) per-round time , too long for the algorithm to be practical. Subsequent OPS algorithms can be classified into two categories.

* Algorithms in the first category exhibit near-optimal \((d)\) per-round time and moderate \(()\) regret rates. This category includes the barrier subgradient method , SoftBayes , and LB-OMD .
* Algorithms in the second category exhibit much faster \(((d)(T))\) regret rates with much longer \(((d)(T))\) per-round time, which is, however, significantly shorter than the per-round time of Universal Portfolio. This category includes ADA-BARONS , PAE+DONS , BISONS , and VB-FTRL .

The aforementioned results are worst-case and do not reflect how "easy" the data is. For instance, if the price relatives of the investment alternatives remain constant over rounds, then a small regret is expected. _Data-dependent regret bounds_ refer to regret bounds that maintain acceptable rates in the worst case and much better rates when the data is easy. In this work, we consider three types of data-dependent bounds.

* A _small-loss bound_ bounds the regret by the cumulative loss of the best action in hindsight.
* A _gradual-variation bound_ bounds the regret by the gradual variation (6) of certain characteristics of the loss functions, such as the gradients and price relatives.
* A _second-order bound_ bounds the regret by the variance or other second-order statistics of certain characteristics of the loss functions, such as the gradients and price relatives.

Few studies have explored data-dependent bounds for OPS. These studies rely on the so-called no-junk-bonds assumption , requiring that the price relatives of all investment alternatives are bounded from below by a positive constant across all rounds. Given this assumption, it is easily verified that the losses in OPS become Lipschitz and smooth. Consequently, the result of Orabona et al.  implies a small-loss regret bound; Chiang et al.  established a gradual-variation bound in the price relatives; and Hazan and Kale  proved a second-order bound also in the price relatives. These bounds are logarithmic in the number of rounds \(T\) in the worst cases and can be constant when the data is easy.

The no-junk-bonds assumption may not always hold. Hazan and Kale  raised the question of whether it is possible to eliminate this assumption. In this work, we take the initial step towards addressing the question. Specifically, we prove Theorem 1.1.

**Theorem 1.1**.: _In the absence of the no-junk-bonds assumption, two algorithms exist that possess a gradual-variation bound and a small-loss bound, respectively. Both algorithms attain \(O(d(T))\) regret rates in the best cases and \(()\) regret in the worst cases, with \((d)\) per-round time._

Theorem 1.1 represents the first data-dependent bounds for OPS that do not require the no-junk-bonds assumption. To the best of our knowledge, this also marks the first data-dependent bounds for online convex optimization with non-Lipschitz non-smooth losses. In the worst cases, Theorem 1.1 ensures that both algorithms can compete with the OPS algorithms of the first category mentioned above. In the best cases, both algorithms achieve a near-optimal regret with a near-optimal per-round time, surpassing the OPS algorithms of the second category in terms of the computational efficiency. Table 1 in Appendix A presents a detailed summary of existing OPS algorithms in terms of the worst-case regrets, best-case regrets, and per-round time.

We also derived a second-order regret bound by aggregating a variant of optimistic FTRL with different learning rates. The interpretation of the result is not immediately clear. Therefore, we detail the result in Appendix I.

Technical Contributions.The proof of Theorem 1.1 relies on several key technical breakthroughs:

* Theorem 3.2 provides a general regret bound for optimistic FTRL with self-concordant regularizers, which may not be barriers, and time-varying learning rates. The bound generalizes those of Rakhlin and Sridharan  and Zimmert et al. [38, Appendix H] and is of independent interest.
* Existing results on small-loss and gradual-variation bounds assume the loss functions are smooth, an assumption that does not hold in OPS. In Section 4, we present Lemma 4.3 and Lemma 4.7, which serve as local-norm counterparts to the Lipschitz gradient condition and the self-bounding property of convex smooth functions, respectively.
* To apply Theorem 3.2, the gradient estimates and iterates in optimistic FTRL must be computed concurrently. Consequently, we introduce Algorithm 2, a variant of optimistic FTRL with the log-barrier and validate its definition and time complexity.
* The gradual-variation and small-loss bounds in Theorem 1.1 are achieved by two novel algorithms, Algorithm 3 and Algorithm 4, respectively. Both are instances of Algorithm 2.

Notations.For any natural number \(N\), we denote the set \(\{1,,N\}\) by \([N]\). The sets of non-negative and strictly positive numbers are denoted by \(_{+}\) and \(_{++}\), respectively. The \(i\)-th entry of a vector \(v^{d}\) is denoted by \(v(i)\). The probability simplex in \(^{d}\), the set of entry-wise non-negative vectors of unit \(_{1}\)-norm, is denoted by \(_{d}\). We often omit the subscript for convenience. The closure and relative interior of a set \(\) is denoted by \(\) and \(\), respectively. The \(_{p}\)-norm is denoted by \(\|\|_{p}\). The all-ones vector is denoted by \(e\). For any two vectors \(u\) and \(v\) in \(^{d}\), their entry-wise product and division are denoted by \(u v\) and \(u v\), respectively. For time-indexed vectors \(a_{1},,a_{t}^{d}\), we denote the sum \(a_{1}++a_{t}\) by \(a_{1:t}\).

## 2 Related Works

### Log-Barrier for Online Portfolio Selection

All algorithms we propose are instances of optimistic FTRL with the log-barrier regularizer. The first use of the log-barrier in OPS can be traced back to the barrier subgradient method proposed by Nesterov . Later, Luo et al.  employed a hybrid regularizer, which incorporated the log-barrier, in the development of ADA-BARRONS. This marked the first OPS algorithm with a regret rate polylogarithmic in \(T\) and an acceptable per-round time complexity of \(O(d^{2.5}T)\). Van Erven et al.  conjectured that FTRL with the log-barrier (LB-FTRL) achieves the optimal regret. The conjecture was recently refuted by Zimmert et al. , who established a regret lower bound for LB-FTRL. Jezequel et al.  combined the log-barrier and volumetric barrier to develop VB-FTRL, the first algorithm with near-optimal regret and an acceptable per-round time complexity of \(O(d^{2}T)\). These regret bounds are worst-case and do not directly imply our results.

### FTRL with Self-Concordant Regularizer

Abernethy et al.  showed that when the regularizer is chosen as a self-concordant barrier of the constraint set, the regret of FTRL is bounded by the sum of dual local norms of the gradients. Rakhlin and Sridharan  generalized this result for optimistic FTRL.

The requirement for the regularizer to be a barrier is restrictive. For instance, while the log-barrier is self-concordant, it is not a barrier of the probability simplex. To address this issue, van Erven et al. , Mhammedi and Rakhlin , and Jezequel et al.  introduced an affine transformation such that, after the transformation, the log-barrier becomes a self-concordant barrier of the constraint set. Nonetheless, this reparametrization complicates the proofs.

Theorem 3.2 in this paper shows that optimistic FTRL with a self-concordant regularizer, _without the barrier requirement_, still satisfies a regret bound similar to that by Rakhlin and Sridharan . The proof of Theorem 3.2 aligns with the analyses by Mohri and Yang , McMahan , and Joulani et al.  of FTRL with optimism and adaptivity, as well as the local-norm based analysis by Zimmert et al. [38, Appendix H].

In comparison, Theorem 3.2 generalizes the analysis of Zimmert et al. [38, Appendix H] for optimistic algorithms and time-varying learning rates; Theorem 3.2 differs from the analyses of Mohri and Yang, McMahan , and Joulani et al.  in that they require the regularizer to be strongly convex, whereas the log-barrier is not.

### Data-Dependent Bounds

The following is a summary of relevant literature on the three types of data-dependent bounds. For a more comprehensive review, readers may refer to, e.g., the lecture notes of Orabona .

* Small-loss bounds, also known as \(L^{}\) bounds, were first derived by Cesa-Bianchi et al.  for online gradient descent for quadratic losses. Exploiting the self-bounding property, Srebro et al.  proved a small-loss bound for convex smooth losses. Orabona et al.  proved a logarithmic small-loss bound when the loss functions are not only smooth but also Lipschitz and exp-concave.
* Chiang et al.  derived the first gradual-variation bound, bounding the regret by the variation of the gradients over the rounds. Rakhlin and Sridharan  interpreted the algorithm proposed by Chiang et al.  as optimistic online mirror descent and also proposed optimistic FTRL with self-concordant barrier regularizers. Joulani et al.  established a gradual-variation bound for optimistic FTRL.
* Cesa-Bianchi et al.  initiated the study of second-order regret bounds. Hazan and Kale  derived a regret bound characterized by the empirical variance of loss vectors for online linear optimization. In the presence of the no-junk-bonds assumption, Hazan and Kale  proved a regret bound for OPS characterized by the empirical variance of price relatives.

Except for those for specific loss functions, these data-dependent bounds assume either smoothness or Lipschitzness of the loss functions. Nevertheless, both assumptions are violated in OPS.

Recently, Hu et al.  established small-loss and gradual-variation bounds in the context of Riemannian online convex optimization. We are unaware of any Riemannian structure on the probability simplex that would render the loss functions in OPS geodesically convex and geodesically smooth. For instance, Appendix B shows that the loss functions in OPS are not geodesically convex on the Hessian manifold induced by the log-barrier.

## 3 Analysis of Optimistic FTRL with Self-Concordant Regularizers

This section presents Theorem 3.2, a general regret bound for optimistic FTRL with regularizers that are self-concordant _but not necessarily barriers_. This regret bound forms the basis for the analyses in the remainder of the paper and, as detailed in Section 2.2, generalizes the results of Rakhlin and Sridharan  and Zimmert et al. [38, Appendix H].

Consider the following online linear optimization problem involving two players, Learner and Reality. Let \(^{d}\) be a closed convex set. At the \(t\)-th round,

* first, Learner announces \(x_{t}\);
* then, Reality announces a vector \(v_{t}^{d}\);
* finally, Learner suffers a loss given by \( v_{t},x_{t}\).

For any given time horizon \(T\), the regret \(R_{T}(x)\) is defined as the difference between the cumulative loss of Learner and that yielded by the action \(x\); that is,

\[R_{T}(x)_{t=1}^{T} v_{t},x_{t}-_{t =1}^{T} v_{t},x, x.\]

The objective of Learner is to achieve a small regret against all \(x\). Algorithm 1 provides a strategy for Learner, called optimistic FTRL.

We focus on the case where the regularizer \(\) is a self-concordant function.

**Definition 3.1** (Self-concordant functions).: _A closed convex function \(:^{d}(-,]\) with an open domain \(\) is said to be \(M\)-self-concordant if it is three-times continuously differentiable on \(\) and_

\[|D^{3}(x)[u,u,u]| 2M u,^{2}(x) u^{3/2}, x,u ^{d}.\]Suppose that \(^{2}\) is positive definite at a point \(x\). The associated local and dual local norms are given by \(\|v\|_{x}(x)v}\) and \(\|v\|_{x,*}(x)v}\), respectively. Define \((t) t-(1+t)\).

**Theorem 3.2**.: _Let \(\) be an \(M\)-self-concordant function such that \(\) is contained in the closure of \(\) and \(_{x}(x)=0\). Suppose that \(^{2}(x)\) is positive definite for all \(x\) and the sequence of learning rates \(\{_{t}\}\) is non-increasing. Then, Algorithm 1 satisfies_

\[R_{T}(x)}+_{t=1}^{T}( v_{t}- {v}_{t},x_{t}-x_{t+1}-M^{2}}(M\|x_{t}-x_{t+1} \|_{x_{t}})).\]

_If in addition, \(_{t-1}\|v_{t}-_{t}\|_{x_{t},*} 1/(2M)\) for all \(t\), then Algorithm 1 satisfies_

\[R_{T}(x)}+_{t=1}^{T}_{t-1}\|v_{t}-_{t}\|_{x_{t},*}^{2}.\]

The proof of Theorem 3.2 is deferred to Appendix D. It is worth noting that the crux of the proof lies in Lemma D.1; the remaining steps follow standard procedure.

## 4 "Smoothness" in Online Portfolio Selection

### Online Portfolio Selection

Online Portfolio Selection (OPS) is a multi-round game between two players, say Investor and Market. Suppose there are \(d\) investment alternatives. A portfolio of Investor is represented by a vector in the probability simplex in \(^{d}\), which indicates the distribution of Investor's wealth among the \(d\) investment alternatives. The price relatives of the investment alternatives at the \(t\)-th round are listed in a vector \(a_{t}^{d}_{+}\).

The game has \(T\) rounds. At the \(t\)-th round,

* first, Investor announces a portfolio \(x_{t}^{d}\);
* then, Market announces the price relatives \(a_{t}^{d}_{+}\);
* finally, Investor suffers a loss given by \(f_{t}(x_{t})\), where the loss function \(f_{t}\) is defined as \[f_{t}(x)- a_{t},x.\]

The objective of Investor is to achieve a small regret against all portfolios \(x\), defined as2

\[R_{T}(x)_{t=1}^{T}f_{t}(x_{t})-_{t=1}^{T}f_{t}(x),  x_{t=1}^{T}f_{t}.\]

In the context of OPS, the regret corresponds to the logarithm of the ratio between the wealth growth rate of Investor and that yielded by the constant rebalanced portfolio represented by \(x\).

**Assumption 1**.: _The vector of price relatives \(a_{t}\) is non-zero and satisfies \(\|a_{t}\|_{}=1\) for all \(t\)._The assumption on \(\|a_{t}\|_{}\) does not restrict the problem's applicability. If the assumption does not hold, then we can consider another OPS game with \(a_{t}\) replaced by \(_{t} a_{t}/\|a_{t}\|_{}\) and develop algorithms and define the regret with respect to \(_{t}\). It is obvious that the regret values defined with \(a_{t}\) and \(_{t}\) are the same.

The following observation, readily verified by direct calculation, will be useful in the proofs.

**Lemma 4.1**.: _The vector \(x(- f_{t}(x))\) lies in \(\) for all \(x\) and \(t\)._

### Log-Barrier

Standard online convex optimization algorithms, such as those in the lecture notes by Orabona  and Hazan , assume that the loss functions are either Lipschitz or smooth.

**Definition 4.2** (Lipschitzness and smoothness).: _A function \(\) is said to be Lipschitz with respect to a norm \(\|\|\) if_

\[|(y)-(x)| L\|y-x\|, x,y {dom}\]

_for some \(L>0\). It is said to be smooth with respect to the norm \(\|\|\) if_

\[\|(y)-(x)\|_{*} L^{}\|y-x\|,  x,y\] (1)

_for some \(L^{}>0\), where \(\|\|_{*}\) denotes the dual norm._

Given that \( a_{t},x\) can be arbitrarily close to zero on \(f_{t}\) in OPS, it is well known that there does not exist a Lipschitz parameter \(L\) nor a smoothness parameter \(L^{}\) for all loss functions \(f_{t}\). Therefore, standard online convex optimization algorithms do not directly apply.

We define the log-barrier as3

\[h(x)-d d-_{i=1}^{d} x(i),(x(1),,x(d)) _{++}^{d}.\] (2)

It is easily checked that the local and dual local norms associated with the log-barrier are given by

\[\|u\|_{x}\|u x\|_{2},\|u \|_{x,*}=\|u x\|_{2}.\] (3)

In the remainder of the paper, we will only consider this pair of local and dual local norms.

Note that Lipschitzness implies boundedness of the gradient. The following observation motivates the use of the log-barrier in OPS, showing that the gradients in OPS are bounded with respect to the dual local norms defined by the log-barrier.

**Lemma 4.3**.: _It holds that \(\| f_{t}(x)\|_{x,*} 1\) for all \(x\) and \(t\)._

A similar result was proved by van Erven et al. [35, (2)]. We provide a proof of Lemma 4.3 in Section E for completeness.

The following fact will be useful.

**Lemma 4.4** (Nesterov [27, Example 5.3.1 and Theorem 5.3.2]).: _The log-barrier \(h\) and loss functions \(f_{t}\) in OPS are both \(1\)-self-concordant._

### "Smoothness" in OPS

Existing results on small-loss and gradual-variation bounds require the loss functions to be smooth. For example, Chiang et al.  exploited the definition of smoothness (1) to derive gradual-variation bounds; Srebro et al.  and Orabona et al.  used the "self-bounding property," a consequence of smoothness, to derive small-loss bounds.

**Lemma 4.5** (Self-bounding property [33, Lemma 2.1]).: _Let \(f:^{d}\) be an \(L\)-smooth convex function with \(f=^{d}\). Then, \(\| f(x)\|_{*}^{2} 2L(f(x)-_{y^{d}}f(y))\) for all \(x^{d}\)._

While the loss functions in OPS are not smooth, we provide two smoothness characterizations of the loss functions in OPS. The first is analogous to the definition of smoothness (1).

**Lemma 4.6**.: _Let \(f(x)=- a,x\) for some \(a_{+}^{d}\). Under Assumption 1 on the vector \(a\),_

\[\|(x f(x))-(y f(y))\|_{2} 4 \{\|x-y\|_{x},\|x-y\|_{y}\}, x,y ,\]

_where \(\) denotes the entrywise product and \(\|\|_{x}\) and \(\|\|_{y}\) are the local-norms defined by the log-barrier (3)._

The second smoothness characterization is analogous to the self-bounding property (Lemma 4.5). For any \(x\) and \(v^{d}\), define

\[_{x}(v)^{d}x^{2}(i)v(i)}{_{i=1}^{d}x^{2}( i)}, x,\] (4)

where \(x(i)\) and \(v(i)\) denote the \(i\)-th entries of \(x\) and \(v\), respectively.

**Lemma 4.7**.: _Let \(f(x)=- a,x\) for some \(a_{+}^{d}\). Then, under Assumption 1 on the vector \(a\), it holds that_

\[\| f(x)+_{x}( f(x))e\|_{x,*}^{2} 4f(x),  x,\]

_where the notation \(e\) denotes the all-ones vector and \(\|\|_{x,*}\) denotes the dual local norm defined by the log-barrier (3)._

**Remark 4.8**.: _The value \(_{x}(v)\) is indeed chosen to minimize \(\|v+ e\|_{x,*}^{2}\) over all \(\)._

The proofs of Lemma 4.6 and Lemma 4.7 are deferred to Appendix E.

## 5 LB-FTRL with Multiplicative-Gradient Optimism

Define \(g_{t} f_{t}(x_{t})\). By the convexity of the loss functions, OPS can be reduced to an online linear optimization problem described in Section 3 with \(v_{t}=g_{t}\) and \(\) being the probability simplex \(\). Set the regularizer \(\) as the log-barrier (2) in Optimistic FTRL (Algorithm 1). By Lemma 4.4, for the regret guarantee in Theorem 3.2 to be valid, it remains to ensure that \(_{t}\), the estimate of \(g_{t}\), is selected to satisfy \(_{t-1}-_{t}}_{x_{t},*} 1/2\) for all \(t\). However, as \(x_{t}\), which defines the dual local norm, depends on \(_{t}\) in Algorithm 1, selecting such \(_{t}\) is non-trivial.

To address this issue, we introduce Algorithm 2. This algorithm simultaneously computes the next iterate \(x_{t+1}\) and the gradient estimate \(_{t+1}\) by solving a system of nonlinear equations (5). As we estimate \(x_{t+1} g_{t+1}\) instead of \(g_{t+1}\), we call the algorithm LB-FTRL with Multiplicative-Gradient Optimism. Here, LB indicates that the algorithm adopts the log-barrier as the regularizer.

```
1:Input: A sequence of learning rates \(\{_{t}\}_{++}\).
2:\(h(x)-d d-_{i=1}^{d} x(i)\).
3:\(_{1} 0\).
4:\(x_{1}_{x}_{0}^{-1}h(x)\).
5:for all\(t\)do
6: Announce \(x_{t}\) and receive \(a_{t}\).
7:\(g_{t} f_{t}(x_{t})\).
8: Choose an estimate \(p_{t+1}^{d}\) for \(x_{t+1} g_{t+1}\).
9: Compute \(x_{t+1}\) and \(_{t+1}\) such that \[x_{t+1}_{t+1}=p_{t+1},\\ x_{t+1}_{x} g_{1:t},x+ _{t+1},x+_{t}^{-1}h(x).\] (5)
10:endfor ```

**Algorithm 2** LB-FTRL with Multiplicative-Gradient Optimism for OPS

By the definitions of the dual local norm (3) and \(p_{t}\) (5), we write

\[-_{t}}_{x_{t},*}= g_{t}-x_{t}_{t }}_{2}= g_{t}-p_{t}}_{2}.\]It suffices to choose \(p_{t}\) such that \(_{t-1}{\|{x_{t} g_{t}-p_{t}}\|_{2}} 1/2\). Indeed, Algorithm 3 and Algorithm 4 correspond to choosing \(p_{t}=x_{t-1} g_{t-1}\) and \(p_{t}=0\), respectively. Algorithm 5 in Appendix I corresponds to choosing \(p_{t}=(1/_{0:t-2})_{t=1}^{t-1}_{-1}x_{} g_{}\).

Theorem 5.1 guarantees that \(x_{t}\) and \(_{t}\) are well-defined and can be efficiently computed. Its proof and the computational details can be found in Appendix F.1.

**Theorem 5.1**.: _If \(_{t}p_{t+1}[-1,0]^{d}\), then the system of nonlinear equations (5) has a solution. The solution can be computed in \((d)\) time._

Algorithm 2 corresponds to Algorithm 1 with \(v_{t}=g_{t}\), \(_{t}=_{t}=p_{t} x_{t}\), and \((x)=h(x)\). Corollary 5.2 then follows from Theorem 3.2. Its proof can be found in Appendix F.2.

**Corollary 5.2**.: _Assume that the sequence \(\{_{t}\}\) is non-increasing and \(p_{t}(-,0]^{d}\) for all \(t\). Under Assumption 1, Algorithm 2 satisfies_

\[R_{T}(x)}+_{t=1}^{T}(-p_ {t} x_{t},x_{t}-x_{t+1}}-}({\| x_{t}-x_{t+1}\|_{x_{t}}}))+2,\]

_In addition, for any sequence of vectors \(\{u_{t}\}\) such that \(_{t-1}{\|{(g_{t}+u_{t}) x_{t}-p_{t}}\|_{2}} 1/2\) and \(,x_{t}-x_{t+1}}=0\) for all \(t\), Algorithm 2 satisfies_

\[R_{T}(x)}+_{t=1}^{T}_{t-1}{\|{(g_{t}+u _{t}) x_{t}-p_{t}}\|_{2}^{2}}+2.\]

**Remark 5.3**.: _The vectors \(u_{t}\) are deliberately introduced to derive a small-loss bound for OPS._

## 6 Data-Dependent Bounds for OPS

### Gradual-Variation Bound

We define the _gradual variation_ as

\[V_{T}_{t=2}^{T}{\|{ f_{t}(x_{t-1})- f_{t-1}(x_ {t-1})}\|_{x_{t-1},*}^{2}}_{t=2}^{T}_{x}{\|{  f_{t}(x)- f_{t-1}(x)}\|_{x,*}^{2}},\] (6)

where \({\|{}\|_{*}}\) denotes the dual local norm associated with the log-barrier. The definition is a local-norm analog to the existing one , defined as \(_{t=2}^{T}_{x}{\|{ f_{t}(x)- f_{t-1}(x)} \|^{2}}\) for a _fixed_ norm \({\|{}\|}\). Regarding Lemma 4.3, our definition appears to be a natural extension.

In this sub-section, we introduce Algorithm 3, LB-FTRL with Last-Multiplicative-Gradient Optimism, and Theorem 6.1, which provides the first gradual-variation bound for OPS. Algorithm 3 is an instance of Algorithm 2 with \(p_{1}=0\) and \(p_{t}=x_{t-1} g_{t-1}\) for \(t 2\). Note that the learning rates specified in Theorem 6.1 do not require the knowledge of \(V_{T}\) in advance and can be computed on the fly.

The proof of Theorem 6.1 can be found in Appendix G.

**Theorem 6.1**.: _Let \(_{0}=_{1}=1/(16)\) and \(_{t}=)}\) for \(t 2\). Then, Algorithm 3 satisfies_

\[R_{T}(x)( T+8)+512d^{2}}+ T+2-128,  T.\]

By the definition of the dual local norm (3) and Lemma 4.1,

\[V_{T}=_{t=2}^{T}{\|{x_{t-1} f_{t}(x_{t-1})-x_{t-1}  f_{t-1}(x_{t-1})}\|_{2}^{2}} 2(T-1).\]

As a result, the worst-case regret of Algorithm 3 is \(O( T)\), comparable to the regret bounds of the barrier subgradient method , Soft-Bayes , and LB-OMD  up to logarithmic factors. On the other hand, if the price relatives remain constant over rounds, then \(V_{T}=0\) and \(R_{T}=O(d T)\)Time Complexity.The vectors \(g_{t}\) and \(p_{t+1}\), as well as the the quantity \(V_{t}\), can be computed using \(O(d)\) arithmetic operations. By Theorem 5.1, the iterate \(x_{t+1}\) can be computed in \((d)\) arithmetic operations. Therefore, the per-round time of Algorithm 3 is \((d)\).

### Small-Loss Bound

In this sub-section, we introduce Algorithm 4, Adaptive LB-FTRL, and Theorem 6.2, the first small-loss bound for OPS. The algorithm is an instance of Algorithm 2 with \(p_{t}=0\). Then, \(_{t+1}=0\) and \(x_{t+1}\) is directly given by Line 8 of Algorithm 4. Note that Theorem 5.1 still applies.

```
1:\(h(x):=-d d-_{i=1}^{d} x(i)\).
2:\(x_{1}_{x}_{0}^{-1}h(x)\).
3:for all\(t\)do
4: Announce \(x_{t}\) and receive \(a_{t}\).
5:\(g_{t} f_{t}(x_{t})=-}{ a_{t},x_{t}}\).
6:\(_{t}_{x_{t}}(g_{t})\) (see the definition (4)).
7:\(_{t}}{^{d}\|g_{t}+_{x_{ t}}\|_{x_{t},}^{2}}}\).
8:\(x_{t+1}_{x} g_{1:t},x+ _{t}^{-1}h(x)\).
9:endfor ```

**Algorithm 4** Adaptive LB-FTRL for OPS

The proof of Theorem 6.2 is provided in Appendix H.

**Theorem 6.2**.: _Let \(L_{T}^{}=_{x}_{t=1}^{T}f_{t}(x)\). Then, under Assumption 1, Algorithm 4 satisfies_

\[R_{T}(x) 2( T+2)^{}+4d^{2}+d}+d( T+2)^{2}.\]

Under Assumption 1,

\[L_{T}^{}=_{x}_{t=1}^{T}- a_{t},x _{t=1}^{T}-}{d}=_{t=1}^{T} d=T  d.\]

Assuming \(T>d\), the worst-case regret bound is \(O( T)\), also comparable to the regret bounds of the barrier subgradient method , Soft-Bayes , and LB-OMD  up to logarithmic factors. On the other hand, suppose that there exists an \(i^{}[d]\) such that \(a_{t}(i^{})=1\) for all \(t[T]\). That is, the \(i^{}\)-th investment alternative always outperforms all the other investment alternatives. Then, \(L_{T}^{}=0\) and \(R_{T}=O(d^{2}T)\).

Assumption 1 does not restrict the applicability of Theorem 6.2, as mentioned earlier. If the assumption does not hold, Theorem 6.2 is applied with respect to the normalized price relatives \(_{t}=a_{t}/\|a_{t}\|_{}\). In this case, \(L_{T}^{}\) is defined with respect to \(\{_{t}\}\).

Time Complexity.Since \(\|g_{t}+_{t}e\|_{x_{t,*}}^{2}=\|x_{t} g_{t}+_{t} x_{t}\|_{2}^{2}\), it is obvious that computing \(_{t}\), \(_{t}\), and \(g_{1:t}\) can be done in \(O(d)\) arithmetic operations. By Theorem 5.1, the iterate \(_{t+1}\) can be computed in \((d)\) arithmetic operations. Hence, the per-round time of Algorithm 4 is \((d)\).

## 7 Concluding Remarks

We have presented Theorem 6.1 and Theorem 6.2, the first gradual-variation and small-loss bounds for OPS that do not require the no-junk-bonds assumption, respectively. The algorithms exhibit sublinear regrets in the worst cases and achieve logarithmic regrets in the best cases, with per-round time almost linear in the dimension. They mark the first data-dependent bounds for non-Lipschitz non-smooth losses.

A potential direction for future research is to extend our analyses for a broader class of online convex optimization problems. In particular, it remains unclear how to extend the two smoothness characterizations (Lemma 4.6 and Lemma 4.7) for other loss functions.

Orabona et al.  showed that achieving a regret rate of \(O(d^{2}+ L_{T}^{})\) is possible under the no-junk-bonds assumption, where \(L_{T}^{}\) denotes the cumulative loss of the best constant rebalanced portfolio. This naturally raises the question: can a similar regret rate be attained without relying on the no-junk-bonds assumption? If so, then the regret rate will be constant in \(T\) in the best cases and logarithmic in \(T\) in the worst cases. However, considering existing results in probability forecasting with the logarithmic loss [5, Chapter 9]--a special case of OPS without the no-junk-bonds assumption--such a data-dependent regret rate seems improbable. Notably, classical rate-optimal algorithms for probability forecasting with the logarithmic loss, such as Shtarkov's minimax-optimal algorithm, the Laplace mixture, and the Krichevsky-Trofimov mixture, all achieve logarithmic regret rates for all possible data sequences [5, Chapter 9].

Zhao et al.  showed that for Lipschitz and smooth losses, an algorithm with a gradual-variation bound automatically achieves a small-loss bound. Generalizing their argument for non-Lipschitz non-smooth losses is a natural direction to consider.