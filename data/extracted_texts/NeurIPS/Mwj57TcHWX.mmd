# DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning

Weikang Wan\({}^{1}\)\({}^{*}\),  Ziyu Wang\({}^{2*}\),  Yufei Wang\({}^{3}\)\({}^{*}\),

**Zackory Erickson\({}^{3}\),  David Held\({}^{3}\)**

\({}^{1}\) Computer Science and Engineering Department, University of California San Diego

\({}^{2}\) Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{3}\) Robotics Institute, Carnegie Mellon University

w2wan@ucsd.edu, ziyu-wan21@mails.tsinghua.edu.cn

yufeiw2@andrew.cmu.edu, zackory@cmu.edu, dheld@andrew.cmu.edu

Equal contribution. This work was performed when Weikang Wan and Ziyu Wang were interning at CMU.

###### Abstract

This paper introduces DiffTORI, which utilizes **Diff**erentiable **T**rajectory **O**ptimization as the policy representation to generate actions for deep **R**einforcement and **I**mitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTORI addresses the "objective mismatch" issue of prior model-based RL algorithms, as the dynamics model in DiffTORI is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTORI for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods in both domains.

## 1 Introduction

Recent works have shown that the representation of a policy can have a substantial impact on the learning performance [1; 2; 3; 4]. Prior works have explored the use of feed-forward neural networks , energy-based models , or diffusion [1; 5] as the policy representation. In this paper, we propose to use differentiable trajectory optimization [3; 6; 7; 8; 9] as the policy representation to generate actions for deep reinforcement learning (RL) and imitation learning (IL) with high-dimensional sensory observations (images/point clouds).

Trajectory optimization is an effective and widely used algorithm in control, defined with a cost function and a dynamics function. It can be viewed as a policy [3; 6], where the parameters of the policy specify the cost function and the dynamics function. Given the learned cost and dynamics functions as well as the input state (e.g., images, point clouds, robot joint states), the policy then computes the actions by solving the trajectory optimization problem. Trajectory optimization can also be made to be differentiable, which allows back-propagating through the trajectory optimization process [3; 8; 10; 6; 9; 11; 12; 13]. In prior work, differentiable trajectory optimization has beenapplied to system identification [3; 6; 9], inverse optimal control , imitation learning [3; 6; 8; 14; 7] and control/planning for robotics problems with low-dimensional states [3; 6; 8; 15].

In this paper, we propose to combine differentiable trajectory optimization with deep model-based RL algorithms. Because we use differentiable trajectory optimization to generate actions , we are able to compute the policy gradient loss on the generated actions to learn the dynamics and cost functions to optimize the reward. This approach addresses the "objective mismatch" issue [16; 17] of current model-based RL algorithms, i.e. models that achieve better training performance (e.g., lower MSE) in learning a dynamics model are not necessarily better for control. Our method addresses this issue, as the latent dynamics and reward models are both optimized to maximize the task performance by back-propagating the policy gradient loss through the trajectory optimization process. We show that our method outperforms prior state-of-the-art model-based RL algorithms on 15 tasks from the DeepMind Control Suite  with high-dimensional image inputs.

We further benchmark our method for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM)  and Diffusion , and term our method DiffTORI (**D**ifferentiable **T**rajectory **O**ptimization for **R**einforcement and **I**mitation Learning). We observe that our training procedure using differentiable trajectory optimization leads to better performance compared to the EBM approach used in prior work, which can suffer from training instability due to the requirement of sampling high-quality negative examples . We also outperform diffusion-based approaches  due to our procedure of learning a cost function that we optimize at test time. We show DiffTORI achieves state-of-the-art performance across 35 different tasks: 5 tasks from Robomimic  with image inputs, 9 tasks from Maniskill1  and Maniskill2  with point cloud inputs, and 22 tasks from MetaWorld  with point cloud inputs.

Our work is closely related to prior work [3; 8; 6] in employing differentiable trajectory optimization as a policy representation. Compared to these prior work, we are the first to show how differentiable trajectory optimization can be combined with deep model based RL algorithms, training dynamics, reward, Q function, and the policy end-to-end using task loss. In contrast, prior work either focuses on imitation learning [3; 8], assumes known dynamics and reward structures and learns only a few parameters , or first learns the dynamics model with the dynamics prediction loss (instead of the task loss), and then uses the fixed learned dynamics for control . We are also the first to show that the policy class represented by differentiable trajectory optimization can scale up to high-dimensional sensory observations like images and point clouds, achieving state-of-the-art performances in standard RL and imitation learning benchmarks. In contrast, prior works [3; 8; 6] only test their methods in customized tasks with ground-truth low-level states, and do not report performance on standard benchmarks with more complex tasks and high-dimensional observations.

In summary, the contributions of our paper are as following:

* We introduce DiffTORI, which uses differentiable trajectory optimization as the policy representation for deep reinforcement learning and imitation learning.
* We conduct extensive experiments to compare DiffTORI against prior state-of-the-art methods on 15 tasks for model-based RL and 35 tasks for imitation learning in standard benchmarks with high-dimensional sensory observations, and show that DiffTORI achieves superior performances.
* We perform analysis and ablations of DiffTORI to provide insights into its performance gains.

## 2 Related Works

**Differentiable optimization and implicit policy representation:** Our work follows the line of research on differentiable optimization, which embeds optimization problems as a layer in neural networks for end-to-end learning. Early works focus on differentiating through convex optimization problems [23; 24]. Recent works extend the range of optimization problems that can be made differentiable [11; 12; 6; 8; 9; 10]. The mostly related prior work to ours are Amos et al.  and Jin et al. , which first proposed to treat trajectory optimization as an implicit policy and demonstrated its effectiveness in the setting of behavior cloning, system identification, and control for robotics problems with low-dimensional states. Another closely related recent work is Romero et al. , where they embed a differentiable quadratic program with learnable cost matrices and known dynamics into the last layer of the actor in PPO, with applications for quadcopter flying. Ours differ from this work as we learn non-linear costs parameterized by a full neural network, and we also learn the dynamics instead of assuming it is known. We also show our method work with high-dimensional sensory inputs such as images and point clouds. Cheng et al. [25; 26] proposes to learn the parameters of a PID controller by unrolling the controller and system dynamics into a computation graph and optimizing the controller parameters via gradient descent with respect to the task loss, assuming known dynamics. DiffTORI does not assume any prior knowledge on the dynamics or policy class; Instead of representing the policy as a predefined controller, our policy is represented as performing trajectory optimization with the learned dynamics, reward and Q functions represented as neural networks. Sacks et al.  proposes to learn the update rule in MPPI, represented as a neural network, using reinforcement learning, with known dynamics and cost functions. Instead of learning the update rule, we learn the dynamics, reward, Q function used in trajectory optimization to generate the actions. We perform differentiable trajectory optimization instead of RL to optimize the parameters of these functions. Differentiable optimization has also been applied in other robotics domains such as autonomous driving [14; 28; 29], navigation [7; 30], motion planning [31; 12], and state estimation . We are the first to show how differentiable trajectory optimization can be combined with deep model-based RL.

**Model-based reinforcement learning:** Compared to model-free RL, model-based RL usually has higher sample efficiency as it is solving a simpler supervised learning problem when learning the dynamics model. Recently, researchers have identified a fundamental problem for model-based RL, known as "objective mismatch" . Recent works have proposed a single objective which is a lower bound on the true return of the policy, for joint model and policy learning in model-based RL [17; 33]. Our approach also addresses the objective mismatch problem. In contrast to these two prior work which only optimizes a lower bound on the true return, our approach directly optimizes the task reward. Further, these approaches are only demonstrated using low-dimensional state-based observations whereas our approach is able to handle high-dimensional image or point cloud observations. In contrast to these works, we use Theseus  to analytically compute the gradient of the true objective for updating the model. Another related work, Nikishin et al.  proposes to learn a dynamics and reward model in model-based RL, and derive an implicit policy as the softmax policy associated with the optimal Q function under the learned dynamics and reward, learned by back-propagating the RL loss via implicit function theorem. In contrast, we derive the implicit policy as the optimal solution from performing trajectory optimization with the learned dynamics, reward and Q function.

**Policy architecture for deep imitation learning:** Imitation learning can be formulated as the supervised regression task of learning to map observations to actions from demonstrations. Some recent work explores different policy architectures (e.g., explicit policy, implicit policy , diffusion policy ) and different action representations (e.g., mixtures of Gaussian [35; 19], spatial action maps , action flow , or parameterized action spaces ) to achieve more accurate learning from demonstrations, to model the multimodal distributions of demonstrations, and to capture sequential correlation. Our method outperforms explicit or diffusion policy approaches due to our procedure of learning a cost function that we optimize at test time. In comparison with the implicit policy, which also employs test-time optimization with a learned objective, we use a different and more stable training procedure via differentiable trajectory optimization.

## 3 Background

### Differentiable Trajectory Optimization

In robotics and control, trajectory optimization solves the following type of problems:

\[_{a_{0},,a_{T}}&_{t=0}^{ T-1}c(s_{t},a_{t})+C(s_{T})\\ s.t.& s_{t+1}=d(s_{t},a_{t})\] (1)

where \(c(s_{t},a_{t})\) and \(C(s_{T})\) are the cost functions, and \(s_{t+1}=d(s_{t},a_{t})\) is the dynamics function. In this paper, we consider the case where the cost function and the dynamics functions are neural networks parameterized by \(\): \(c_{}(s_{t},a_{t})\), \(C_{}(s_{T})\), and \(d_{}(s_{t},a_{t})\).

Let \(a_{0}(),...,a_{T}()\) be the optimal solution to the trajectory optimization problem, which is a function of the model parameters \(\). Differentiable trajectory optimization is a class of method that enablescomputation of the gradient of the actions with respect to the model parameters \(()}{}\). Specifically, in this paper we use Theseus , which is an efficient application-agnostic open source library for differentiable nonlinear least squares optimization. Theseus works well with high-dimensional states, e.g., images or point clouds, along with using neural networks as the cost and dynamics functions.

### Model-Based RL preliminaries

We use the standard MDP formulation: \(,,,,\) where \(\) is the state space, \(\) is the action space, \((s,a)\) is the reward function, \((|s,a)\) is the transition dynamics function, and \([0,1)\) is the is the discount factor. The goal is to learn a policy \(\) to maximize the expected return: \(_{s_{t},a_{t}}[_{t=1}^{}^{t}R(s_{t},a_{t})]\). In this paper we work on problems where the state space \(S\) are high-dimensional sensory observations, e.g., images or point clouds. Model-based RL algorithms first learn a dynamics model, and then use it for learning a policy. When applied to model-based RL, our method builds upon TD-MPC , a recently proposed model-based RL algorithm which we review briefly here. We choose TD-MPC for its simplicity and state-of-the-art performance. However, our method is compatible with any model-based RL algorithm that learns a dynamics model and a reward function. TD-MPC consists of the following components: first, an encoder \(h_{}\), which encodes the high-dimensional sensory observations, e.g., images, into a low-dimensional state \(z_{t}=h_{}(s_{t})\). In the latent space, a latent dynamics model \(d_{}\) is also learned: \(z_{t+1}=d_{}(z_{t},a_{t})\). A latent reward predictor \(R_{}\) is learned which predicts the task reward \(r\): \(=R_{}(z_{t},a_{t})\). Finally, a value predictor \(Q_{}\) learns to predict the Q value: \(=Q_{}(z_{t},a_{t})\). Note that we use \(\) to denote all learnable parameters including the encoder, the latent dynamics model, the reward predictor, and the Q value predictor. These models are trained jointly using the following objective:

\[_{}(;)=_{i=1}^{t+H}^{i-t} _{}(;_{i}),\] (2)

where \(\) is a trajectory \((s_{t},a_{t},r_{t},s_{t+1})_{t:t+H}\) sampled from a replay buffer \(\), \(_{+}\) is a constant that weights near-term predictions higher, and the single-step loss is:

\[_{}(;_{i})= c_{_{}(_{i}, _{t})-r_{i}_{2}^{2}}{c_{}}+c_{_{ } Q_{}(_{i},_{t})-(r_{i}+ Q _{^{-}}(_{i+1},_{}(_{i+1})))_{2}^{2} }_{}}\] (3) \[+c_{_{}}\]

where \(^{-}\) are parameters of target networks that are periodically updated using the parameters of the learning networks. As shown in (3), the parameters \(\) is optimized with a set of surrogate losses (reward prediction, value prediction, and latent consistency), rather than directly optimizing the task performance, known as the objective mismatch issue . At test time, model predictive path integral (MPPI)  is used for planning actions that maximize the predicted rewards and Q functions in the latent space. A policy \(_{}\) is further learned in the latent space using the latent Q-value function, which is used to generate action samples in the MPPI process.

Figure 1: **Overview of DiffTORI for model-based RL. In contrast to prior work in model-based RL  that uses non-differentiable MPPI (left), we utilize differentiable trajectory optimization to generate actions (right). DiffTORI computes the policy gradient loss on the generated actions and back-propagates it through the optimization process, to optimize the encoder as well as other latent space models (latent reward predictor and latent dynamics function) to maximize task performance.**

Method

### Overview

The core idea of DiffTORI is to use trajectory optimization as the policy \(_{}\), where \(\) parameterizes the dynamics and cost functions. Given a state \(s\), DiffTORI generates the actions \(a()\) by solving the trajectory optimization problem in (1) with \(s_{0}=s\). To optimize the policy parameters \(\), we use differentiable trajectory optimization to compute the gradients of the loss \((a())\) with respect to the policy parameters: \((a())}{}\), where the exact form of the loss depends on the problem setting.

An overview of applying DiffTORI to model-based RL is shown in Figure 1. Existing model-based RL algorithms such as TD-MPC suffer from the objective mismatch issue: the latent dynamics and reward (cost) functions are learned to optimize a set of surrogate losses (as in (3)), instead of optimizing the task performance directly. DiffTORI addresses this issue: by computing the policy gradient loss on the optimized actions from trajectory optimization and differentiating through the trajectory optimization process, the dynamics and cost functions are optimized directly to maximize the task performance. We describe DiffTORI for model-based RL in Section 4.2.

We also apply DiffTORI to imitation learning; an overview is shown in Figure 2. In contrast to explicit policies that generate actions at test-time by forward passes of the policy network, DiffTORI generates the actions via test-time trajectory optimization with a learned cost function. This is in the same spirit of implicit behaviour cloning  which learns an energy function and optimizes with respect to it to generate actions at test-time. However, we observe that our training procedure using differentiable trajectory optimization leads to better performance compared to the EBM approach used in prior work, which can suffer from training instability due to the requirement of sampling high-quality negative examples . We describe DiffTORI for imitation learning in detail in Section 4.3.

### Differentiable trajectory optimization applied to model-based RL

We build DiffTORI on top of TD-MPC for model-based RL. Similar to TD-MPC, DiffTORI consists of an encoder \(h_{}\), a latent dynamics model \(d_{}\), a reward predictor \(R_{}\), and a Q-value predictor \(Q_{}\) (see Sec. 3.2). We use \(\) to denote all learnable parameters to be optimized in DiffTORI. As shown in Figure 1, the key to DiffTORI is to change the non-differentiable MPPI planning algorithm in TD-MPC to a differentiable trajectory optimization, and include the policy gradient loss on the generated actions to optimize the model parameters \(\) directly for task performance.

Formally, given a state \(s_{t}\), we use the encoder \(h_{}\) to encode it to the latent state \(z_{t}\), and then construct the following trajectory optimization problem in the latent space:

\[ a()=*{arg\,max}_{a_{t},,a_{ t+H}}&_{l=t}^{H-1}^{l-t}R_{}(z_{t},a_{t})+^{H}Q_{ }(z_{H},a_{H})\\ s.t.& z_{t+1}=d_{}(z_{t},a_{t})\] (4)

where \(H\) is the planning horizon. In this paper we leverage Theseus  to solve (4) in a differentiable way. Since Theseus only supports solving non-linear least-square optimization problems without constraints, we remove the dynamics constraints in the above optimization problem by manually rolling out the dynamics into the objective function. For example, with a planning horizon of \(H=2\), we turn the above optimization problem into the following one:

\[a()=*{arg\,max}_{a_{t},a_{t+1},a_{t+2}}R_{}(z_{t},a_ {t})+R_{}(d_{}(z_{t},a_{t}),a_{t+1})+Q_{}(d_{}(d_{ }(z_{t},a_{t}),a_{t+1}),a_{t+2})\] (5)

We set the values of \(H\) following the schedule as in TD-MPC, and we use the Levenberg-Marquardt algorithm in Theseus to solve the optimization problem. Following TD-MPC, we also learn a policy \(_{}\) in the latent space using the learned Q-value predictor \(Q_{}\), and the output from the policy is used as the action initialization for solving (4).

Let \(a()\) be the solution of the above trajectory optimization problem, obtained using Theseus as described above. DiffTORI is learned with the following objective, which jointly optimizes the encoder, latent dynamics model, latent reward model, and the Q-value predictor:

\[^{RL}_{DiffTORI}(;)& =_{i=t}^{t+H}^{i-t}(_{TD-MPC}(; _{i})+c_{0}_{PG}(;_{i}))\\ _{PG}(;_{i})&=-_{}(s _{i},a())\] (6)where \(_{}\) is the Q function learned via Bellman updates  which is used to compute the deterministic policy gradient , and \(c_{0}\) is the weight for this loss term. \(_{}\) is learned in the original state space \(\) instead of the latent space to provide accurate policy gradients. The key idea here is that we can backpropagate through the policy gradient loss \(_{PG}\), which backpropagates through \(a()\) and then through the differentiable trajectory optimization procedure of Equation 4 to update \(\).

### Differentiable Trajectory Optimization applied to imitation learning

We also use DiffTORI for model-based imitation learning. A comparison of DiffTORI to other types of policy classes used in prior work is shown in Figure 2. In this approach, DiffTORI consists of an encoder \(h_{}\) and a latent dynamics function \(d_{}\), as before. However, in the setting of imitation learning, we do not assume access to a reward function \((s,a)\). Instead, we generate actions by solving the following trajectory optimization problem:

\[ a()=*{arg\,max}_{a_{t},,a _{t+H}}&_{l=t}^{H}^{l-t}f_{}(z_{t},a_{t})\\ s.t.& z_{t+1}=d_{}(z_{t},a_{t}),\] (7)

in which \(f_{}(z_{t},a_{t})\) is a function over the latent state \(z_{t}\) and actions \(a_{t}\) that we will optimize using the imitation learning loss, as described below. Similarly, We use \(\) to denote all learnable parameters to be optimized in DiffTORI, including the parameters of the encoder \(h_{}\), the latent dynamics model \(d_{}\), and the function \(f_{}\) in the imitation learning setting.

In imitation learning, we assume access to an expert dataset \(D=\{(s_{i},a_{i}^{*})\}_{i=1}^{N}\) of state-action pairs \((s_{i},a_{i}^{*})\). In the most basic form, the loss \(\) for DiffTORI can be the mean square error between the the expert actions \(a_{i}^{*}\) and the actions \(a()\) returned from solving (7):

\[_{BC}()=_{i=1}^{N}\|a()-a_{i}^{*}\|\] (8)

The key idea here is that we can backpropagate through the imitation loss \(_{BC}\), which backpropagates through \(a()\) and then through the differentiable trajectory optimization procedure of Equation 7 to update \(\). This enables us to learn the function \(f_{}(z_{t},a_{t})\) used in the optimization Equation 7 directly by optimizing the imitation loss \(_{BC}()\). Because this loss is optimized through the trajectory optimization procedure (Equation 7), we will learn a function \(f_{}(z_{t},a_{t})\) such that optimizing Equation 7 returns actions that match the expert actions.

**Multimodal DiffTORI:** The loss in Equation 8 will not be able to capture multi-modal action distributions in the expert demonstrations. To address this, we use a Conditional Variational Auto-Encoder (CVAE)  as the policy architecture, which has the ability to capture a multi-modal action distribution . The CVAE encoder encodes the state \(s_{i}\) and the expert action \(a_{i}^{*}\) into a latent state vector \(z_{i}\). The key idea in our approach is that the decoder in CVAE takes the form of a trajectory optimization algorithm, given by Equation 7. It takes as input the sampled latent \(\) from the Gaussian Prior, and the state \(s_{i}\) and uses differentiable trajectory optimization to decode the

Figure 2: **Overview of our method on Imitation Learning.** DiffTORI (right) learns a cost function via differentiable trajectory optimization and performs test-time optimization with it, which is different from prior work (left) that uses an explicit policy or diffusion without test-time optimization. Although implicit policy shares the same spirit as DiffTORI, we observe that the training procedure of DiffTORI using differentiable trajectory optimization leads to better performance compared to the EBM approach used in prior work , which can suffer from training instability.

action \(a()\). Because this trajectory optimization is differentiable, we can backpropagate through it to learn the parameters \(\) for the encoder, dynamics \(d_{}\), and the function \(f_{}\) used in Equation 7. See Appendix D for further details.

**Action refinement:** We note that DiffTORI provides a natural way to perform action refinement on top of a base policy. Given an action from any base policy, we can use this action as the initialization of the action variables for solving the trajectory optimization problem; the trajectory optimizer will iteratively refine this action initialization with respect to the optimization objective of Equation 7. In our experiments, we find DiffTORI always outperforms the base policies when using their actions as the initialization and other ways of performing action refinement, such as residual learning.

## 5 Experiments

### Model-based Reinforcement Learning

We conduct experiments on 15 DeepMind Control suite tasks, which involve simulated locomotion and manipulation tasks, such as making a cheetah run or swinging a ball into a cup. All tasks use image observations and the control policy does not have direct access to the underlying states.

We compare to the following baselines: **SAC**, a commonly used off-policy model-free RL algorithm. **DrQ-v2**, a state-of-the-art model-free RL algorithm for image observations that adds data augmentation on top of SAC. **TD-MPC**, a state-of-the-art model-based RL algorithm, which DiffTORI builds on. All training details such as hyper-parameters, and psudo-code can be found in Appendix B. All experiments use NVIDIA 2080 Ti GPUs.

Figure 3: Performance of DiffTORI, in comparison to 4 prior state-of-the-art model-based and model-free RL algorithms, on 15 tasks from DeepMind control suite. DiffTORI achieves the best performance when averaged across all tasks. Results are averaged with 4 seeds, and the shaded regions represent the \(95\%\) confidence interval.

Figure 3 shows the learning curves for all methods on all tasks. The top-left subplot shows the normalized performance averaged across all 15 tasks, which is computed as the achieved return divided by the max return from any algorithm. As shown, DiffTORI (red curve) outperforms all compared baselines by a noticeable margin. On 14 out of the 15 tasks (except Quadruped-walk), DiffTORI achieves the highest performance among compared algorithms. We especially note that the performance of DiffTORI is much higher than TD-MPC, which DiffTORI builds on, showing the benefit of adding the policy gradient loss and directly differentiating through it to optimize the learned latent spaces. Although DiffTORI achieves higher sample efficiency, one limitation of DiffTORI is that it requires more wall-clock time for training, due to the need for solving and differentiating through the trajectory optimization process. We show detailed results on computational efficiency (return vs wall-clock time) of DiffTORI in Appendix A.1.2. We also perform ablation studies to examine how each loss term in (6) contributes to the final performance of DiffTORI in Figure 6 in Appendix A.1.3.

### Imitation Learning

Below we show results of DiffTORI on 3 commonly used imitaiton learning benchmarks: MetaWorld , RoboMimic , ManiSkill , and the comparison to state-of-the-art methods on these three benchmarks. We also compare to one closely related prior work  on one of their customized tasks in Appendix A.3.

#### 5.2.1 MetaWorld

MetaWorld  is a large-scale benchmark that includes 100 robotic manipulation tasks, and has been recently used for evaluating different imitation learning algorithms . The policy observation is point clouds of the scene, and the action is the 3d translation of the robot end-effector. We test on 22 tasks with different levels of difficulties: Medium, Hard, and Very Hard (See Table 1 for all the tasks). 10 demonstrations are used for all tasks . We compare DiffTORI with the following baselines: **DP3**, a 3D version of diffusion policy that achieves state-of-the-art performances on this benchmark, outperforming other algorithms such as the original diffusion policy  with 2d image inputs. **Residual + DP3**: Since DiffTORI refines the actions from a base pre-trained DP3 policy, we additionally compare to this baseline that also leverages the actions from a base pre-trained DP3 policy. Specifically, we learn a residual policy on top of the base pre-trained policy, which takes

    &  \\  & Soccer & Push Wall & Peg Insert Side & Bin Picking & Basketball & Box Close & Coffee Pull \\  DP3 & 32.3\(\)3.5 & 42.0\(\)2.6 & 57.3\(\)0.3 & 13.0\(\)1.0 & **88.0\(\)**5.3 & 62.0\(\)2.0 & 65.0\(\)1.6 \\ Residual + DP3 & 33.0\(\)2.6 & 43.3\(\)3.1 & 59.3\(\)4.2 & 13.3\(\)3.1 & **87.3\(\)**3.1 & 67.3\(\)5.0 & 59.3\(\)1.5 \\ DiffTORI (Ours) + DP3 & **40.7\(\)**1.2 & **50.0\(\)**2.0 & **64.7\(\)**4.2 & **22.0\(\)**5.3 & **88.0\(\)**4.0 & **73.3\(\)**4.6 & **70.7\(\)**7.0 \\    &  \\  & Coffee Push & Hammer & Sweep & Sweep Into & Assemble & Hand Insert & Pick out of Hole & Pick Place \\  DP3 & 53.0\(\)3.6 & 32.3\(\)3.5 & 74.7\(\)3.1 & 30.3\(\)11.2 & 68.7\(\)1.5 & 18.3\(\)2.1 & 55.0\(\)1.6 & 56.7\(\)2.5 \\ Residual + DP3 & 49.3\(\)4.2 & 31.3\(\)1.2 & 71.3\(\)3.0 & **45.7\(\)**3.8 & 64.7\(\)3.1 & 19.7\(\)2.1 & 61.3\(\)2.3 & 52.0\(\)2.0 \\ DiffTORI (Ours) + DP3 & **60.7\(\)**5.0 & **37.3\(\)**3.1 & **90.7\(\)**3.1 & **45.3\(\)**3.6 & **74.0\(\)**4.0 & **24.7\(\)**1.2 & **63.3\(\)**1.6 & **61.3\(\)**6.3 \\    &  \\  & Push Wall & Back & ShifT Place & Disassemble & Stack Pull & Pick Pick Pose Wall & **Average** \\  DP3 & 21.3\(\)7.5 & 55.3\(\)4.9 & 27.7\(\)2.9 & 34.0\(\)6.4 & 53.0\(\)6.9 & **94.3\(\)**2.1 & 38.3\(\)5.7 & 48.8 \\ Residual + DP3 & 20.0\(\)3.5 & 55.3\(\)4.2 & 35.3\(\)2.2 & 35.3\(\)1.2 & 56.0\(\)2.0 & 91.3\(\)2.3 & **44.7\(\)**4.2 & 49.8 \\ DiffTORI (Ours) + DP3 & **30.0\(\)**3.5 & **64.7\(\)**3.1 & **42.0\(\)**5.0 & **40.7\(\)**3.1 & **59.3\(\)**3.6 & **94.0\(\)**3.5 & **44.7\(\)**2.3 & **56.5** \\   

Table 1: Success rates (\(\)) of DiffTORI, DP3 and Residual + DP3 on 22 MetaWorld tasks. DiffTORI consistently achieves higher or on-par success rates on all 22 tasks.

    & IBC & BC-RNN &  Residual \\ +BC-RNN \\  &  DiffTORI (Ours) \\ +BC-RNN \\  &  Diffusion \\ + Diffusion \\  &  IBC \\ + Diffusion \\  &  Residual \\ + Diffusion \\  & 
 DiffTORI (Ours) \\ + Diffusion \\  \\  Square & \(0.96\)0.00 & 0.18\(\)0.00 & 0.16\(\)0.01 & 0.10\(\)0.02 & 0.12\(\)0.03 & 0.32\(\)0.05 & 0.12\(\)0.02 & **0.08\(\)**0.01 \\ Transport & \(1.00\)0.00 & 0.28\(\)0.03 & 0.26\(\)0.03 & 0.17\(\)0.02 & 0.07\(\)0.04 & 0.92\(\)0.03 & 0.08\(\)0.01 & **0.04\(\)**0.01 \\ ToolHang & \(1.00\)0.00 & 0.33\(\)0.04 & 0.28\(\)0.03 & 0.18\(\)0.00 & 0.10\(\)0.00 & 0.94\(\)0.01 & 0.10\(\)0.00 & **0.08\(\)**0.01 \\ Push-T & 0.89\(\)0.01 & 0.30\(\)0.02 & 0.28\(\)0.02 & 0.25\(\)0.02 & **0.09\(\)**0.00 & 0.92\(\)0.01 & **0.09\(\)**0.00 & **0.09\(\)**0.01 \\ 
**Average** & 0.96 & 0.27 & 0.25 & 0.18 & 0.10 & 0.78 & 0.10 & **0.07** \\   

Table 2: Failure rates (\(\)) of all methods on the Robomimic tasks. DiffTORI achieves the lowest failure rates on all tasks with diffusion policy as the base policy.

as input the action from the base policy, and outputs a delta action that is added to the base action. This is the most standard and simple way of doing residual learning. All training details such as hyper-parameters and pseudo-code can be found in Appendix B.

Table 1 presents the task success rates, averaged over 50 evaluation episodes, of all compared algorithms. As shown, DiffTORI consistently achieves higher (or on par) success rates than the other 2 compared baselines. The improvement in success rates is larger on tasks where the original DP3 policy struggles, e.g., a 15% improvement on the task of Shelf Place and Sweep Into; and as expected, when the base DP3 policy is already doing well on the task, there is not much room of improvement left for DiffTORI, e.g., on Basketball and Stick Push. The simple way of learning a residual policy on top of the DP3 policy does not always improve the performance of the base policy, and even leads to lower success rates. This demonstrates that DiffTORI is a more effective way to leverage a pre-trained policy. On average, the success rates of DiffTORI is \(7.7\%\) higher than that of DP3, a substantial improvement with only 10 demonstrations.

#### 5.2.2 Robomimic

Robomimic  is another commonly used benchmark designed to study imitation learning for robot manipulation. The benchmark encompasses a total of 5 tasks with two types of demonstrations: collected from proficient humans (PH) or a mixture of proficient and non-proficient humans. We use the PH demonstrations, and evaluate on three of the most challenging tasks: Square, Transport, and ToolHang. We use image-based observations and the default velocity controller for all the tasks. In addition to Robomimic, we compare to another task, Push-T from the diffusion policy  task set, to demonstrate that we can learn multimodal cost functions by using the CVAE training loss.

We compare to the following baselines: **IBC**: An implicit policy that learns an energy function conditioned on both action and observation using the InfoNCE loss . **BC-RNN**: A variant of BC that uses a Recurrent Neural Network (RNN) as the policy network to encode a history of observations. This is the best-performing baseline in the original Robomimic  paper. **Residual + BC-RNN**: We use a pretrained BC-RNN as the base policy, and learn a residual policy on top of it. The residual policy takes as input the action from the base policy, and outputs a delta action which is added to the base action. **Diffusion Policy**: A policy that uses the diffusion model as the policy class. It refines noise into actions via a learned gradient field. **IBC + Diffusion**: A version of IBC that uses the action from a pre-trained Diffusion Policy as the action initialization in the test-time optimization process. **Residual + Diffusion**: Similar to Residual + BC-RNN, but using a pre-trained Diffusion Policy as the base policy. For DiffTORI, we compare two variants of it: DiffTORI + BC-RNN and DiffTORI + Diffusion Policy, which uses a pre-trained BC-RNN or a pre-trained diffusion policy as the base policy to generate the initialization action for solving the trajectory optimization problem. In Appendix A.2, we also present results of DiffTORI with zero initialization or random initialization, instead of initializing the action from a base policy.

The results are shown in Table 2. We find that DiffTORI+Diffusion Policy achieves the lowest failure rates consistently across all tasks. Even though Diffusion Policy has almost saturated on these tasks with very low failure rates, DiffTORI can still further reduces it. Furthermore, irrespective of the base policy used -- whether BC-RNN or Diffusion Policy -- DiffTORI always brings noticeable improvement in the performance over the base policy. While learning a residual policy does lead to improvements upon the base policy, DiffTORI shows a significantly greater performance boost. In addition, by comparing DiffTORI+Diffusion Policy with IBC+Diffusion Policy, we find that using the same action initialization for IBC is considerably less effective than using the same action initialization in DiffTORI. In many tasks, even when the base Diffusion Policy already exhibits low failure rates, IBC+Diffusion Policy still results in poor performances, indicating the training objective used in IBC actually deteriorates the base actions.

We also show the benefit of using a CVAE architecture for DiffTORI, which enables DiffTORI to capture multimodal action distributions. With different latent samples from CVAE, we get

    & PrickCube & Fill & Hang & Execute & Pour & OpenCabinet & OpenCabinet & PauChair & MoveBucket & **Average** \\  & & & & & & & & & \\  BC & 0.19\(\)0.3 & 0.72\(\)0.04 & 0.76\(\)0.02 & 0.25\(\)0.02 & 0.13\(\)0.04 & 0.74\(\)0.03 & 0.35\(\)0.04 & 0.12\(\)0.04 & 0.10\(\)0.04 & 0.34 \\ BC + residual & 0.21\(\)0.04 & 0.75\(\)0.02 & 0.75\(\)0.02 & 0.27\(\)0.03 & 0.12\(\)0.01 & 0.49\(\)0.02 & 0.36\(\)0.03 & 0.15\(\)0.02 & 0.10\(\)0.01 & 0.36 \\ DiffTORI (Ours) + BC & **0.32\(\)**0.02 & **0.82\(\)**0.01 & **0.85\(\)**0.01 & **0.29\(\)**0.01 & **0.17\(\)**0.02 & **0.53\(\)**0.02 & **0.45\(\)**0.02 & **0.20\(\)**0.02 & **0.15\(\)**0.02 & **0.42** \\   

Table 3: On Maniskill tasks, DiffTORI consistently achieves higher success rates (\(\)) on all tasks.

different objective functions \(f_{}(z,a)\) and dynamics functions \(d_{}(z,a)\), allowing DiffTORI to generate different actions from the same state. Figure 4 illustrates the multimodal objective function learned by DiffTORI (right), and the resulting multimodal actions (left). The left subplot shows that when starting from the same action initialization \(a_{init}\), with two different latent samples, DiffTORI optimizes \(a_{init}\) into two different actions, \(_{1}\) and \(_{2}\) that move in distinct directions. The trajectory optimization procedure that iteratively updates the action is represented by dashed lines transitioning from faint to solid. From these two actions, two distinct trajectories are subsequently generated to push the T-shape object towards its goal. The middle and right subplots show the objective function landscapes for the 2 different samples, as well as the initial action \(a_{init}\), and the final optimized action \(}\) and \(}\). We note the two landscapes are distinct from each other with different optimal solutions, showing that DiffTORI can generate multimodal objective functions and thus capture multimodal action distributions. We note that the learned objective function \(f\) is not necessarily a "reward" function as those learned via inverse RL . It is just a learned "objective function", such that optimizing it with trajectory optimization would yield actions that minimize the imitation learning loss with respect to the expert actions in the demonstration. We leave exploring the connections with inverse RL for future work.

#### 5.2.3 ManiSkill

ManiSkill [20; 21] is a benchmark for learning generalizable robotic manipulation skills with 2D & 3D visual input. It includes a series of rigid body tasks and soft body tasks. We choose 9 tasks (4 soft body tasks and 5 rigid body tasks) from ManiSkill  and ManiSkill  and use 3D point cloud input for all the tasks. We use the end-effector frame as the observation frame  and use the PD controller with the end-effector delta pose as the action.

We build our method on top of the strongest imitation learning baseline in ManiSkill2 released by the authors, which is a Behavior Cloning (BC) policy with PointNet  as the encoder. Again, we also compare to BC+residual, which learns a residual policy that takes as input the action from the BC policy and outputs a delta correction. The results are shown in Table 3. As shown, DiffTORI + BC consistently achieves higher success rates than both baselines on all tasks, demonstrating the strong effectiveness of using differentiable trajectory optimization as the policy class.

## 6 Conclusion and Discussion

We introduce DiffTORI that uses differentiable trajectory optimization to generate the policy actions for deep reinforcement learning and imitation learning. The key is to utilize the recent progress in differentiable trajectory optimization to compute the gradients of the loss with respect to the parameters of the cost and dynamics function of trajectory optimization, and learn them end-to-end. When applied to model-based reinforcement learning, DiffTORI addresses the "objective mismatch" issue of prior methods. We also test DiffTORI for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare it to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 35 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTORI outperforms prior state-of-the-art methods.

Figure 4: By using a CVAE, DiffTORI can learn multimodal objectives functions via sampling different latent vectors from CVAE (right). By performing trajectory optimization with these two different objective functions, DiffTORI can generate multimodal actions (left).