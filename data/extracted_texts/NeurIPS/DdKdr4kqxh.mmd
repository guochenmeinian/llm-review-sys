# Identifying Spatio-Temporal Drivers of Extreme Events

Mohamad Hakam Shams Eddin   Juergen Gall

Institute of Computer Science, University of Bonn

Lamarr Institute for Machine Learning and Artificial Intelligence

{shams, gall}@iai.uni-bonn.de

###### Abstract

The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine learning approaches to identify such spatio-temporal relations from data. The task, however, is very challenging since there are time delays between extremes and their drivers, and the spatial response of such drivers is inhomogeneous. In this work, we propose a first approach and benchmarks to tackle this challenge. Our approach is trained end-to-end to predict spatio-temporally extremes and spatio-temporally drivers in the physical input variables jointly. By enforcing the network to predict extremes from spatio-temporal binary masks of identified drivers, the network successfully identifies drivers that are correlated with extremes. We evaluate our approach on three newly created synthetic benchmarks, where two of them are based on remote sensing or reanalysis climate data, and on two real-world reanalysis datasets. The source code and datasets are publicly available at the project page [https://hakamshams.github.io/IDE](https://hakamshams.github.io/IDE).

## 1 Introduction

A frontier research challenge is to understand the affects of global change on the magnitude and probability of extreme weather events . Overall, the evolution of extreme events such as agricultural droughts results from stochastic processes , conditions at ecosystem scales , and the interaction between the Earth land and atmospheric variables as a part of a complex system of feedbacks . However, the relative impacts of these factors differ depending on the event . The time delays between extremes and their drivers vary seasonally , and the spatial response of these drivers is inhomogeneous . A major challenge is therefore to model the spatio-temporal relations between extremes and their drivers during the development of these events . The overarching goal of this modelling is to improve our understanding of the patterns and impacts of such events. This would improve our ability to project duration and intensity of extreme events and hence assisting in adaptation planning .

In this work, we propose an approach that identifies spatio-temporal drivers in multivariate climate data that are correlated with the impact of extreme events. For the extreme events, we focus on agricultural droughts as an example, which can be measured by extremely low values of the vegetation health index (VHI). As drivers for such measurable extreme events, we consider anomalies in atmospheric and hydrological state variables like temperature or soil moisture, as well as land-atmosphere fluxes like evaporation. The task of identifying end-to-end spatio-temporal drivers for measurable impacts of extremes has not been addressed before, and it is very challenging since the drivers can occur earlier in time and at a different location than the measured extreme event as illustrated in Fig. 1.

To address this challenging task, we propose a network that is trained to predict spatio-temporally extremes. Instead of simply predicting the extremes, the network quantizes the spatio-temporal input variables into binary states and predicts the extremes only from the spatio-temporal binary maps for each time series of input variables. In this way, the network is enforced to identify only drivers in the input variables that are spatio-temporally correlated with extreme events. While the network is trained using annotations of impacts of extreme events, which can be derived from remote sensing or reported data, we do not have any annotations of drivers or anomalies in the input variables.

Since drivers of extreme events are not fully understood, we cannot quantitatively measure the accuracy of the identified drivers on real-world data. We therefore propose a framework for generating synthetic data that can be used to assess the performance of our model as well as other baselines quantitatively. We evaluate our approach on three synthetic datasets where two of them are based on remote sensing or reanalysis climate data. Our evaluation shows that our approach outperforms approaches for interpretable forecasting, spatio-temporal anomaly detection, out-of-distribution detection, and multiple instance learning. Furthermore, we conduct empirical studies on two real-world reanalysis climate data. Our contributions can be summarized as follow:

* We introduce the new task of identifying spatio-temporal drivers of extreme events and three benchmarks for evaluating this highly important task.
* We propose a novel approach for identifying spatio-temporal drivers in climate data that are spatio-temporally correlated with the impacts of extreme events.
* We further verify our approach on two long-term real-world reanalysis datasets including various physical variables from five biogeographical diverse regions.

## 2 Related works

Anomalies and extremes detection in climate data.The identification of climatic changes and extreme weather has been a subject of many studies [12; 13]. Typical algorithms for extreme events detection are built upon domain knowledge in setting usually empirical thresholds for the physical variables through sensitivity experiments [6; 14]. Many works applied multivariate and statistical methods to detect extreme events such as droughts [6; 9; 15; 16; 17]. However, individual events are difficult to generalized across multiple events  and predefined indicators become less effective with changing climate . Thus, machine and deep learning methods have been proposed as an alternative to classical methods, i.e., for supervised anomaly detection [19; 20] and for the detection of extremes in climate data [14; 21; 22; 23; 24].

While methods for forecasting vegetation indices [25; 26; 27; 28] do not focus on extremes, future impacts of extremes like agricultural droughts can be derived from forecast vegetation indices like the vegetation health index (VHI). For instance, the work  uses a climate simulation as input and forecasts the vegetation health index. Since predicting VHI directly is difficult, the approach predicts the normalized difference vegetation index and the brightness temperature instead. Both indices are then normalized and used to estimate VHI. Although we obtain the impacts of extreme events in our study from vegetation indices, our approach is not limited to such extremes. Since we use a binary representation of extremes, our approach can also be applied to other extremes that cannot be derived from satellite products, but that are stored in a binary format in databases.

While we aim to learn the relations between the impacts of extreme events and their relevant drivers from a data-driven perspective, spatio-temporal relations within the Earth system can also be inferred by causal inference and causal representation learning [29; 30; 31; 32; 33; 34; 35; 36]. In contrast to statistical methods , data-driven methods do not require a prior hypothesis about drivers for extremes. Instead, they generate hypotheses that can be verified by statistical methods in a second step. We believe that this is an important direction since climate reanalysis provides huge amount of data and it is infeasible to test all combinations. This is also known as a curse of dimensionality in causal discovery problems  and data-driven approaches are therefore needed to generate potential candidates.

Anomaly detection algorithms.Since we focus on anomalies in land and meteorological data as drivers, we give an overview of approaches for anomaly detection and discuss their applicability to our task, which has not been previously addressed. _One-class:_ The main stream in one-class anomaly detection is to model the distribution of the normal data during training and consider the deviation from the learned features as anomalies. This includes distance-based [39; 40; 41], patch-based[41; 42; 43], student-teacher [44; 45; 46; 47], and embedding-based approaches [48; 49; 50; 43; 44; 43]. In general, the alignment between the anomaly type and the assumptions of the methods is the critical factor for their performance . One of the limiting factors to apply these methods to climate data is that they assume priori knowledge about what is considered as normal. Furthermore, not all detected anomalies are drivers of an extreme event. _Reconstruction-based:_ These methods assume that a trained model to reconstruct normal data will be unsuccessful in reconstructing anomalies, while it will reconstruct normal data well. Despite being widely applied for anomaly detection problems [53; 54; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64], these methods face the same problem as the one-class methods. In addition, many studies showed that anomalies can still be reconstructed by the trained model . _Self-supervised learning:_ These methods rely on the hypothesis that a model trained for a pretext task on normal data will be successful only on similar normal data during inference [66; 67; 68; 69]. In addition to the above discussed limitations, finding a suitable pretext task for anomaly detection is challenging. For instance, common tasks such as solving a jigsaw puzzle  will fail in homogeneous regions. _Pseudo-anomaly:_ The intuition in pseudo-based anomaly detection is to convert the problem of unsupervised learning into a supervised one by synthesizing abnormal data during training [70; 71; 72; 73; 74; 75; 76; 77]. Since these methods depend partially on the degree to which the proxy anomalies correspond to the unknown true anomalies , applying these approaches to our task would require some knowledge about the coupling between the variables and extremes. _Multivariate anomaly detection:_ Multivariate approaches detect anomalies simultaneously in multiple data streams [78; 79; 80; 81; 82]. The main difference to our task is that we aim to detect drivers across multiple data streams that do not necessary occur simultaneously. _Multiple instance learning:_ Multiple instance learning (MIL) has been proposed for weakly supervised anomaly detection [83; 84; 85; 86; 87; 88; 89; 90; 91]. In MIL-based algorithms, the model is provided with labeled positive and negative bags where each bag includes a set of instances. The model is then trained to classify the instances inside the bags giving only the high level supervision, i.e., label of the bag . A weakly supervised approach has been also applied for hyperspectral anomaly detection . Most algorithms such as [94; 95; 96; 97] choose the top-k potentially anomalous snippets within each video. This makes it challenging to apply them since the abnormality ratio varies in real-world applications . Furthermore, the MIL detector can be biased toward a specific class depending on the context .

## 3 Method

Our aim is to design a model that is capable of identifying spatio-temporal drivers of extremes in multivariate climate data, i.e., Earth observations or climate reanalysis. In particular, we want to identify anomalies that are spatio-temporally related to extreme events like agricultural droughts. This is different to standard anomaly detection since we are not interested in all anomalies, but in spatio-temporal configurations of variables that potentially cause an extreme event with some time

Figure 1: Overview of the objective of this work. We are interested in identifying spatio-temporal relations between the measurable impacts of extremes like the vegetation health index \(\) and their drivers \(\). As drivers, we focus on anomalies in state variables of the land-atmosphere and hydrological cycle. The task is very challenging since the drivers can occur at a different region than the extreme event and earlier in time.

delay and at a potentially different location as illustrated in Fig. 1. To achieve this, we propose a network that is trained end-to-end on observed extremes where we focus on agricultural droughts. The network classifies the input variables before an extreme event occurs into spatio-temporal drivers without additional annotations besides the annotated extremes. The network then aims to predict future extremes based on the identified drivers. It needs to be noted that we are interested in input variables that do not define an extreme event, but we aim to find anomalies in input variables that are correlated with the occurrence of an extreme event. We will thus denote potential drivers as anomalies.

An overview of our approach is shown in Fig. 2. The input are weekly climate variables at sequential time steps \(( t_{-7},, t_{0})\) and the model is composed of three main parts. First, a feature extractor extracts relevant features from each input variable independently. The second component is a quantization layer that takes the extracted features as input and classifies the input variables into drivers. The role of the quantization layer is to transform the input variable into a binary representation (1 = drivers and 0 otherwise). This ensures that no additional information is encoded as an input to the subsequent classifier except that if the input variable at a specific location and time is a driver or not. The third component is a classifier that takes as input the variables, location, and time where drivers have been identified and predicts where extreme droughts occur at the time step \( t_{0}\). All model components are trained jointly.

We denote the input data as \(^{V C T Lat Lon}\), where \(V\) is the input variables, \(C\) is the channel dimension for each variable (i.e., mean and standard deviation of the week), \(T\) is the temporal resolution (\( t_{-T+1},, t_{0}\)), and \(Lat\) and \(Lon\) are the spatial extensions. The model has two outputs, \(_{2}^{V T Lat Lon}\) representing the binary classification of the input variables into potential drivers of the extreme events, and probabilities \(^{Lat Lon}\) to predict extreme events at the time step \( t_{0}\). In the following, we describe the model components:

**Feature extraction.** First, embedded features \(f_{}:^{V K T Lat  Lon}\) are extracted independently for each input variable \(v V\) with \(K\) embedding dimensions and parameters \(\). The rationale behind this independence is to prevent that drivers leak into other variables. We use the Video Swin Transformer model  as backbone to capture long-range interrelations across time and space. The input \(\) is projected into a higher feature dimension \(K\) and followed by two Video Swin Transformer layers. The first layer has two consecutive 3D shifted window blocks for a spatio-temporal feature extraction. The second layer consists of one block for a temporal feature

Figure 2: An overview of the proposed model to identify the spatio-temporal relations between extreme agricultural droughts and their drivers. The input variables are first encoded into features. In a subsequent step, a lockup free quantization layer (LFQ) takes the extracted features and classifies the variables into a binary representation of drivers, where we consider the drivers as anomalous events in the input variables. Finally, a classifier is used to predict impacts of extreme events from the identified drivers.

extraction. The later is useful to focus only on the temporal evolution of the variables. An ablation study regarding the backbone is provided in Sec. C.4.

**Quantization layer.** The role of this layer is to map \(\) from an embedded space into a compact binary representation \(\) suitable for detecting drivers. Using vector quantization (VQ) , each embedded feature vector \(z\) is assigned into a learnable codebook feature vector \(z_{q}_{q}\) based on the Euclidean distance:

\[VQ:z z_{q},q=*{arg\,min}_{q\{1,,Q\}}\|z-z_{q} \|_{2}\,, \]

where \(Q\) is the size of the codebook. Recently, lookup-free quantization (LFQ)  substitutes the learnable codebook with a set of integers \(\) with \(||=Q\) and represents the embedding space as a Cartesian product of binary numbers. This omits the need for a distance metric to do the nearest vector assignment and simplifies the quantization. Based on experimental results, we built the vector quantizer on LFQ with two integers \(Q=2\) (\(q=1\) for drivers and \(q=0\) otherwise). Given a feature vector \(z\), LFQ first maps \(z\) into a scalar value \(z_{l}_{l}^{V 1 T Lat Lon}\). For multi-modality, we use two sequential 3D CNNs on each input variable followed by a shared linear layer that maps \(z\) to \(z_{l}\) and reduces the dimensions from \(K\) to \(1\). The quantization is then given by the sign of \(z_{l}\):

\[z_{q}=(z_{l})=(-_ {\{z_{l} 0\}}+_{\{z_{l}>0\}}),\ \ \ \ q=_{\{z_{l}>0\}}\,, \]

where \(q\) represents the class (\(q=1\) or \(q=0\)), and Linear is a linear layer that converts \((z_{l})\) back to the dimension \(K\) of the input after the quantization. Note that \(_{q}\) has only two unique vectors \(z_{q=1}\) for a driver and \(z_{q=0}\) otherwise.

**Prediction of extreme events.** We use a classifier that predicts the probably of extreme events \(\) at the time step \( t_{0}\) from the identified drivers \(_{q}\). We use a 3D CNN classifier instead of a transformer to reduce the computations. For training, we only know the ground truth of extremes at time step \( t_{0}\) denoted by \(}_{2}^{Lat Lon}\). While we could compute the cross-entropy between \(\) and \(}\), we found that a single 3D CNN is insufficient to detect all drivers that are correlated with an extreme event. Instead, we use \(V+1\) 3D CNNs where each predicts \(_{v}\). While the first \(V\) 3D CNNs take the identified drivers for a single variable \(v\) as input, the last one takes the identified drivers of all variables as input. The multiple CNNs are only used for training. During inference, \(\) is only predicted by the multivariate CNN where all variables are jointly used. The loss is thus given by

\[_{(extreme)}=\,-_{v=1}^{V+1}}( _{v})+(1-})(1-_{v})\,, \]

where \(_{2}^{T Lat Lon}\) is a mask for the valid regions. We actually utilize a weighted version of \(_{(extreme)}\) to mitigate the class imbalance issue (Sec. C.3). While the loss \(_{(extreme)}\) ensures that extreme events can be predicted from the identified drivers, we need to add standard loss terms for the quantization to ensure that the learned codes and thus drivers are compact:

\[_{(quantize)}=_{(commit)}\|_{l}-((_{l}))\|_{2}^{2}+_{(ent)}[H( _{l})]-_{(div)}H[( _{l})]\,. \]

The commitment loss \(\|_{l}-((_{l}))\|_{2}^{2}\) prevents the outputs of the encoder from growing and encourages \(_{l}\) to commit to the codes , where sg stands for the stopgradient operator with zero partial derivative. The term \([H(_{l})]\) encourages that the entropy per quantized code is low , meaning that it provides more confident assignments. Whereas the term \(H((_{l}))\) increases the entropy inside the batch to encourage the utilization of all codes . The last important ingredient is a loss that ensures that only spatio-temporal regions are identified that correlate with an extreme event. To this end, we look at regions and intervals where no extreme event occurred and use these examples without drivers. Formally, we use \(}_{t}_{2}^{Lat Lon}\) as the union of extreme ground truth at all time steps (\( t_{-T+1},, t_{0}\)) and compute the loss by

\[_{(driver)}=_{(driver)}|_{q}-(z_{q=0})|(1- }_{t})\,, \]

where \(z_{q=0}\) is the quantization code for normal data without drivers. The model is trained end-to-end with the joint optimization of the loss function:

\[_{,,}_{(extreme)}, },}_{}+_{(quantize)}_{l}}_{}+_{(driver)}_{q},}_{t},,_{q=0}}_{}\,, \]

where \(,,\) are the learnable parameters, and \(_{(commit)}\), \(_{(ent)}\), \(_{(div)}\), and \(_{(driver)}\) are weighting parameters. Ablation studies are provided in Sec. 5.1 and in Appendix Sec. C.

Dataset

### Defining extreme agricultural droughts from remote sensing

We are interested in a specific impact of extreme events namely extreme agricultural drought. To define such extreme event, we rely on the observational satellite-based vegetation health index (VHI) obtained from NOAA . This remote sensing product cannot be directly derived from the input reanalysis, which makes the task very challenging. VHI approximates the vegetation state based on a combination of the brightness temperature and normalized difference vegetation index (VHI \(=0\) for unfavorable condition and VHI \(=100\) for favorable condition). Extreme agricultural droughts are usually defined as VHI! 26 . The dataset has a temporal coverage of 1981-onward and is provided globally on a weekly basis. We mapped this dataset into the same domains of the reanalysis data as described in Sec. 4.2 and used this dataset as ground truth for extreme events. Note that VHI is a general vegetation index and should be interpreted carefully. Details about this index, the dataset and pre-processing are provided in the Appendix Sec. I.

### Climate reanalysis

Reanalysis data aim to provide a coherent and complete reconstruction of the historical Earth system state as close to reality as possible. During reanalysis, short-term forecasts from numerical climate models are refined with observations within the so called data assimilation framework . We conducted the experiments on two real-world reanalysis datasets; CERRA reanalysis  and ERA5-Land . ERA5-Land is widely used for global climate research and it is provided hourly at \(0.1^{} 0.1^{}\) on the regular latitude longitude grid. CERRA is a regional reanalysis for Europe and is provided originally at \(5.5 5.5\) on its Lambert conformal conical grid with a \(3\)-hourly temporal resolution. We aggregated these two datasets on a weekly basis and selected the years within the period overlapping with the remote sensing data. In addition, we mapped ERA5-Land into \(6\) CORDEX domains  over the globe and conduct experiments on each region separately. We do the experiments with \(6\) common variables from ERA5-Land and CERRA based on their connections to agricultural droughts. For each variable and week, we computed the mean and standard deviation separately. More details regarding the variables and the domains along with the training/validation/test splits are provided in the Appendix Sec. H and Tables 20 and 21.

### Synthetic dataset

Although ground truth for extreme droughts can be obtained from remote sensing, an important methodological question remains as how to reliably have a meaningful quantitative evaluation of the identified drivers and their relations to the extreme events. To solve this critical issue, we introduce a new synthetic dataset that mimics the properties of Earth observations including drivers and anomalies. We are aware that the dynamic of the synthetic data are simplified compared to real Earth observations. However, we rely on this generated dataset to perform the quantitative evaluation of the proposed approach. In a first step, we generate the normal data. For instance to generate synthetic data of 2m temperature from CERRA reanalysis, the normal signal at a specific time and location is generated based on the typical value of 2m temperature at that time and location (i.e., the mean or median value from a long-term climatology). The second step is to generate anomalies conditioned on the occurrence of extremes. To achieve this, we assign binary spatio-temporally connected flags as extreme events randomly within the datacube and track their precise spatio-temporal locations. Then based on a predefined coupling matrix between the variables and the extreme event, we generate anomalous events only for the variables that are defined to be correlated with the extremes. We consider these anomalies as the drivers for the extreme events. Finally, we add additional random anomalous events for all variables. We synthesize overall \(46\) years of data; \(34\) years for training, \(6\) subsequent years for validation and the last \(6\) years for testing. The challenge is to identify the drivers, i.e., the anomalous events that are correlated with extreme events. Examples of the synthetic data are shown in Fig. 3 and in Appendix in Figs. 7-12. Technical details are explained in Appendix Sec. A.

Experimental results

First, we conducted experiments and ablation studies on the synthetic datasets (Sec. 4.3). We also empirically verified the effectiveness of the proposed design compared to baselines on this synthetic dataset. Then, we validate the model on two real-world datasets over five continents in Sec. 5.2.

**Setup and implementation details.** We set the hidden dimension \(K\) to \(16\) by default. The temporal resolution is \(T=6\) for the synthetic data and \(T=8\) for real-world data. Since, seasonal cycles are typical in climate data, we deseasonalize locally by subtracting the median seasonal cycle and normalizing by the seasonal variance for each pixel. Details regarding the model and implementation setup are given in Appendix Sec. G. For evaluation, we use the F1-score, intersection over union (IoU), and overall accuracy on both classes (OA).

### Experiments on the synthetic datasets

We show the results on the Synthetic CERRA described in Sec. 4.3 and in Appendix Table 3. The generated dataset mimics a set of variables (\(V=6\)) using statistics from the real-world CERRA reanalysis . We artificially correlated four variables with extremes (2m temperature, total cloud cover, total precipitation, and volumetric soil moisture) and kept two variables uncorrelated (albedo and relative humidity).

**Comparison to the baselines.** We compare the new approach to interpretable forecasting approaches using integrated gradients  and to \(8\) baselines from \(3\) different categories of anomaly detection approaches; one-class unsupervised [39; 110; 51], reconstruction-based [59; 65], and multiple instance learning [94; 95; 96]. We also compare to a naive baseline which labels all variables as drivers for pixels where extreme events occur. The implementation details of these baselines are given in Appendix Sec. E.

The quantitative results are shown in Table 1. The naive baseline is impacted by two main issues; first by the time delay between drivers and extreme events, and second not all variables are correlated with the extremes. The second issue affects the one-class and reconstruction-based baselines where

    &  &  \\   & Algorithm & F1-score (\(\)) & IoU (\(\)) & OA (\(\)) & F1-score (\(\)) & IoU (\(\)) & OA (\(\)) \\    & Naive 47.93 & 31.52 & 98.61 & 51.24 & 34.45 & 98.37 \\   & Integrated Gradients I  & 24.15\(\)9.94 & 14.12\(\)6.71 & 92.18\(\)2.94 & 23.14\(\)7.05 & 13.27\(\)4.65 & 91.58\(\)2.25 \\   & Integrated Gradients II  & 31.23\(\)4.40 & 18.58\(\)3.05 & 95.26\(\)1.03 & 30.34\(\)4.27 & 17.95\(\)2.94 & 94.19\(\)1.22 \\   & OCSVM  & 28.21\(\)2.49 & 16.44\(\)1.67 & 95.64\(\)0.16 & 29.98\(\)2.26 & 17.66\(\)1.54 & 94.91\(\)0.19 \\  & IF  & 34.99\(\)0.56 & 21.28\(\)0.42 & 97.16\(\)0.02 & 37.16\(\)0.67 & 22.84\(\)0.51 & 96.61\(\)0.03 \\  & SimpleNet  & 75.31\(\)0.07 & 60.39\(\)0.10 & 99.20\(\)0.01 & 73.50\(\)0.24 & 58.11\(\)0.30 & 98.91\(\)0.02 \\   & STEALNet  & 55.98\(\)0.90 & 38.87\(\)0.86 & 98.47\(\)0.03 & 57.74\(\)0.95 & 40.60\(\)0.93 & 98.22\(\)0.03 \\  & UniAD  & 47.53\(\)0.17 & 31.18\(\)0.14 & 97.44\(\)0.02 & 49.23\(\)0.41 & 32.65\(\)0.36 & 97.18\(\)0.05 \\   & DeepMIL  & 70.68\(\)1.61 & 54.68\(\)1.91 & 99.22\(\)0.03 & 71.54\(\)1.60 & 55.72\(\)1.92 & 99.09\(\)0.04 \\  & ARNet  & 72.92\(\)0.85 & 57.39\(\)1.06 & 99.26\(\)0.01 & **73.68\(\)**0.86 & 58.34\(\)1.08 & 99.13\(\)0.02 \\  & RTFM  & 60.09\(\)0.31 & 42.95\(\)0.31 & 98.34\(\)0.03 & 61.88\(\)0.28 & 44.81\(\)0.30 & 98.12\(\)0.02 \\   & Ours\({}^{*}\) & 82.78\(\)0.53 & 70.63\(\)0.78 & 99.51\(\)0.02 & 80.44\(\)0.70 & 67.28\(\)0.97 & 99.29\(\)0.04 \\  & Ours\({}^{}\) & **83.45\(\)**0.37 & **71.60\(\)**0.54 & **99.52\(\)**0.01 & **80.65\(\)**0.

they suffer mostly from false positives. In fact, both integrated gradients models achieve high F1-scores for detecting extremes (93.32 for Integrated Gradients I and 93.80 for Integrated Gradients II), but they have worse performance on identifying the drivers. SimpleNet is trained with our model as a feature extractor which explains its good performance. However, SimpleNet showed a drop of performance when it is tested on other datasets (see Appendix Sec. B for results on two more synthetic datasets). Among the reconstruction-based approach, STEALNet outperforms UniAD. This is probability because STEALNet exploits more weakly supervision information during training by maximizing the reconstruction loss for locations with extreme flags. MIL-based baselines are more suitable for the task. Finally, our model consistently outperforms the baselines on all metrics. Qualitative samples in comparison with baselines are presented in Fig. 3. The qualitative examples indicate that our model and the MIL-based baselines except RTFM are capable of learning which variables are correlated with the extremes. The main weakness of RTFM is the reliance on feature magnitudes and the cross attention module (see Appendix Sec. E and Table 2), which make it more prone to produce false positives. Other baselines predict incorrect relations between the variables and extremes. Regarding the explainable AI methods, when we add more interactions between the variables (Integrated Gradients II), the gradients tend to omit some variables (soil moisture). Both integrated gradients models have also difficulties with the synthetic t2m, which includes red noise by design. These results demonstrate that networks that predict the extremes directly from the input variables utilize much more information even when it is not correlated with an extreme. It is thus beneficial to introduce a bottleneck into the network that enforces the network to explicitly identify drivers of extremes.

**Performance on easy-to-hard correlation settings.** We conduct an additional experiment to assess the model performance in relation to the correlation setup between the variables and extremes. We generate a synthetic CERRA dataset starting with only one correlated variable with the target extreme. We then generate different versions of the dataset by increasing the number of correlated variables with the extremes up to \(100\%\). This analysis allows us to point out the strengths and weaknesses of

Figure 4: F1-score with different correlation settings between the input variables and extremes.

Figure 3: Qualitative results on the synthetic CERRA reanalysis from the test set at time step 2160. \(\) is the prediction, \(\) is the ground truth, and \(\) is the false positive. Albedo and relative humidity are not correlated with extremes, meaning that they do not contain drivers, but only random anomalies.

the comparative models for different scenarios and where our model becomes more effective, as well as where it could struggle most. The results are shown in Fig. 4. One-class, reconstruction-based and RTFM baselines benefit with increasing the number of correlated variables. In case of 6/6, the task reduces to an anomaly detection task. The performance of our model and MIL-based baselines generally decreases when the number of correlated variables increases as the task of finding all correlated anomalies becomes harder. Nevertheless, our approach performs best in all settings.

**Ablation study.** We conducted a set of ablation studies. This includes three main experiments:

**Loss functions.** As shown in Table 2 (a), \(_{(quantize)}\) and \(_{(driver)}\) are essential for training. As other quantization models, ours can not be trained without \(_{(quantize)}\), which ensures that the outputs of \(f_{}\) do not grow and commit to the binary embedding. \(_{(driver)}\) unifies the representation of drivers for all variable as \(q=1\), which boosts the extreme detection. Moreover, the results demonstrate the impact of using \(V{+}1\) 3D CNNs (multi-head) instead of one for \(_{(extreme)}\). If a single 3D CNN is used, drivers are only identified in a small subset of variables. We discuss this more in detail in Sec. C.7.

**Feature extractor.** In Table 2 (b), we show the benefit of having independent feature extractors for driver detection. In a first experiment, we share the feature extractor \(f_{}\) among the variables. The performance is worst. Second, we replaced the temporal attention by a cross attention between the variables similar to  and  where each variable performs a cross attention with the other variables. We can see a drop of performance for the second experiment. We noticed that anomalies propagate between variables when adding connections in the feature extraction stage. This also explains the poor performance of RTFM compared to other MIL baselines. The best performance is shown for the proposed setup (last row), which also shows the benefit for the temporal attention.

**Temporal resolution \(T\).** Table 2 (c) evaluates the impact of the temporal resolution on driver and extreme detection. \(T{=}6\) provides a good balance between driver and extreme detection. More ablation studies on other aspects of the model design can be found in Appendix Sec. C.

### Experiments on real-world datasets

We evaluate our model on two reanalysis data with diverse geographical and climate regions (Sec. 4.2). The input for the experiment is the normalized mean and standard deviation of each week. We exclude pixels over water surfaces, desert, and snow. In addition to the quantitative evaluation on the synthetic data, we aim to verify our method considering the following aspects:

**Quantitative results.** We expect that the developed model can identify drivers in real-world scenarios. We demonstrate this by measuring how well the model can predict extreme agricultural droughts from the identified drivers. The results verify that the model can predict the droughts across different regions and datasets (see Appendix Sec. D and Table 14). Note that compared to the synthetic dataset, the real-world drought prediction is much more difficult.

**Extreme detection without anomaly detection.** We trained the model without the quantization step, meaning without driver detection. This can be considered as an upper bound on the extreme detection accuracy since there is no information reduction by the quantization. We found that when we trained on the synthetic and real-world EUR-11 data, the F1-score for detecting extreme events increased only by \( 0.96\%\) and \( 1.93\%\), respectively, compared to the model with quantization (see Appendix Table 12). This verifies that the detected drivers are highly correlated with the extremes.

**Qualitative results and spatial distribution.** In Fig. 5 (a), we show the spatial distribution of the identified drivers at a specific time over EUR-11. Shown are the identified drivers up to 7 weeks

Table 2: Ablation studies from the validation set. The metric is F1 on the driver/extreme detection.

(\( t_{-7}\)) before the extreme agricultural droughts at time \( t_{0}\). We can see that the prediction of drivers and extremes are spatially correlated with the ground truth.

**Physical consistency.** In Fig. 5 (b), we show the relation between the input reanalysis data, extreme droughts, and identified drivers. For this experiment, we selected pixels with extreme events during summer (weeks \(25\)-\(38\)) and visualize the average distribution of drivers with time. The red line at \( t_{0}\) indicates the beginning of the extreme droughts. \(Z_{score}\) in the underneath curve represents the deviation from the mean computed from the climatology. It is expected that evaporation reduces soil moisture, which dries out the soil and vegetation . Our model indicates that over Europe, the evaporation and soil moisture are the most informative variables to detect drivers related to extreme droughts. All pixels experienced a pronounced decline in soil moisture and an increase in evaporation as the events evolved. Please see Sec. D.3 for more discussion on the scientific validity.

## 6 Discussions and conclusions

We introduced a model that can identify the spatio-temporal relations between impacts of extreme events and their drivers. For this, we assumed that there exist precursor drivers, primarily as anomalies in assimilated land surface and atmospheric data, for every observable impact of extremes. We demonstrated the effectiveness of our approach by measuring to which degree the identified drivers can be used to predict extreme agricultural droughts. Apart from experiments on two real-world datasets, we also presented a new framework to generate synthetic datasets that can be used for spatio-temporal anomaly detection and climate research. The results on the synthetic datasets show that the approach is not limited to droughts and can be applied to other extremes. While we have shown that our approach outperforms other approaches, the study has some limitations. First, evaluating ability to handle a very large number of climate variables in a unified model needs further examination. Similarly, performing an additional pre-processing of specific variables like accumulating precipitation over many weeks might also improve the results. Second, modelling the temporal relations is limited by the time window \(T\). Moreover, teleconnections of climatic anomalies can occur in distant regions on Earth, e.g., affects of El Nino and La Nina variability on drought and flood . Modelling and disentangle such large spatio-temporal relations across the globe is an open research problem. Third, it would be appealing to provide scores for drivers instead of a binary classification. This could be achieved by measuring the distance to the nearest code in the LFQ. Forth, the prediction of the model depends on the capacity of reanalysis data to accurately represent the local environmental factors and land-atmospheric feedbacks. Most importantly, drawing conclusions on drivers from weak predictive models may lead to unreliable interpretations. Finally, our model does not identify causal relationships.

Despite these limitations, our approach demonstrates a clear capability in identifying drivers and anomalies in climate data which would allow a more timely event attributions during and right after extreme events. The identified spatio-temporal relations between extreme events and their drivers could support the understanding and forecasting of extremes.

Figure 5: (a) Qualitative results on ERA5-Land over the EUR-11 domain. Shown are the identified drivers localized spatio-temporally \(7\) weeks before the extreme agricultural drought events. (b) Temporal evolution of drivers during the extremes.

Acknowledgments and Disclosure of Funding

We thank Petra Friederichs, Till Fohrmann, Sebastian Buschow, and Svenja Szemkus for the insightful discussions on detection and attribution of weather and climate extremes. We would also like to thank Olga Zatsarynna and Emad Bahrami for the technical discussion related to feature extraction. Finally, we thank the four anonymous reviewers for their comments and suggestions which improved the quality of this paper.

This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - SFB 1502/1-2022 - project no. 450058266 within the Collaborative Research Center (CRC) for the project Regional Climate Change: Disentangling the Role of Land Use and Water Management (DETECT) and by the Federal Ministry of Education and Research (BMBF) under grant no. 01IS24075C RAINA.

We acknowledge EuroHPC Joint Undertaking for awarding us access to Leonardo at CINECA, Italy, through EuroHPC Regular Access Call - proposal No. EHPC-REG-2024R01-076. The authors also gratefully acknowledge the granted access to the Marvin cluster hosted by the University of Bonn.