# Rad-NeRF: Ray-decoupled Training of Neural

Radiance Field

 Lidong Guo\({}^{1}\)

&Xuefei Ning\({}^{1}\)\({}^{}\)

&Yonggan Fu\({}^{2}\)&Tianchen Zhao\({}^{1}\)

&Zhuoliang Kang\({}^{3}\)&Jincheng Yu\({}^{1}\)&Yingyan (Celine) Lin\({}^{2}\)&Yu Wang\({}^{1}\)\({}^{}\)

\({}^{1}\)Tsinghua University \({}^{2}\)Georgia Institute of Technology \({}^{3}\)Meituan

Both authors contribute equally to this work.Corresponding authors: Xuefei Ning (foxdoraame@gmail.com), Yu Wang (yu-wang@tsinghua.edu.cn).

###### Abstract

Although the neural radiance field (NeRF) exhibits high-fidelity visualization on the rendering task, it still suffers from rendering defects, especially in complex scenes. In this paper, we delve into the reason for the unsatisfactory performance and conjecture that it comes from interference in the training process. Due to occlusions in complex scenes, a 3D point may be invisible to some rays. On such a point, training with those rays that do not contain valid information about the point might interfere with the NeRF training. Based on the above intuition, we decouple the training process of NeRF in the ray dimension softly and propose a **R**ay-**d**ecoupled Training Framework for neural rendering **(Rad-NeRF)**. Specifically, we construct an ensemble of sub-NeRFs and train a soft gate module to assign the gating scores to these sub-NeRFs based on specific rays. The gate module is jointly optimized with the sub-NeRF ensemble to learn the preference of sub-NeRFs for different rays automatically. Furthermore, we introduce depth-based mutual learning to enhance the rendering consistency among multiple sub-NeRFs and mitigate the depth ambiguity. Experiments on five datasets demonstrate that Rad-NeRF can enhance the rendering performance across a wide range of scene types compared with existing single-NeRF and multi-NeRF methods. With only 0.2% extra parameters, Rad-NeRF improves rendering performance by up to 1.5dB. Code is available at https://github.com/thu-nics/Rad-NeRF.

## 1 Introduction

Novel view synthesis is an important task within the domains of computer vision and computer graphics, playing an essential role in a variety of applications, such as autonomous driving, augmented reality, and so on. Recently, Neural Radiance Field (NeRF)  has emerged as a promising solution, achieving high-fidelity visualizations on the novel view synthesis task. It implicitly encodes 3D scenes through neural networks and trains the networks using volume rendering.

Despite NeRF's excellent scene representation ability, it still suffers from rendering defects when dealing with complex scenes, such as 360-degree unbounded scenes  and large scenes with free shooting trajectories . One of the main reasons is the limited model capacity. However, directly increasing the network's size yields marginal performance improvement .

**Our fundamental intuition is that the training interference from invisible rays affects NeRF's performance.** Let us consider a simple case of a 360-degree unbounded scene with a central object(truck) and a distant object (car). As illustrated in Figure 1(a), a 3D point located on the distant object can be observed from ray-1 and ray-2, but is invisible to ray-3 due to the occlusion presented by the central object. Although NeRF models transmittance in its volume rendering formula, it exhibits low geometric modeling accuracy and inaccurate sampling distribution in complex scenes, especially at the start of training, as Figure 1(c) shows. So, 3D points on the distant object might be sampled by the ray-3, and the model is trained on these points by the ray-3 color. However, ray-3 does not contain any meaningful information about the distant object, potentially interfering with the NeRF's training. In contrast, considering the different visibility of the object to different rays, our intuition is that rather than using one NeRF, assigning the rays terminating at the distant object to NeRF-1 and the rays terminating at the central object to NeRF-2 could be better, as shown in Figure 1(b).

To verify the above intuition, we manually select two sets of images in the TAT dataset. One set contains 80 images of the train's front side, while the other set includes the former set and 80 backside images. We train two NeRFs using these two sets respectively. As shown in Figure 2, the model trained on the mixed set performs worse on the front side, which matches our intuition.

To mitigate the training interference caused by invisible rays, the intuition solution is to decouple the training of the rays terminating at different regions. To this end, we propose a **ray-decoupled training framework for neural rendering (Rad-NeRF)**. Within the Rad-NeRF framework, an ensemble of sub-NeRFs has different preferences for different rays through a gate module. With the help of the gate module, sub-NeRFs' outputs are fused by post-volume-rendering fusion to yield final rendering results. Notably, the gate module is jointly optimized with NeRF, allowing it to automatically learn the preference of each sub-NeRF for various rays in an end-to-end manner. This **learnable gating** design makes Rad-NeRF generally applicable to diverse scenes, which stands in contrast to prior multi-NeRF methods  that rely on manually defined allocation rules.

Additionally, we design a depth-based mutual learning method for the multi-NeRF framework to ensure the rendering consistency among multiple sub-NeRFs. In addition to learning colors, sub-NeRFs teach each other with their rendered depths. Traditional NeRF methods may struggle with generalization to novel views despite accurately rendering training views, as they often fail to capture precise geometry . In contrast, our depth-based mutual learning approach serves as a form of geometric regularization, alleviating the depth ambiguity and avoiding overfitting.

Figure 1: A case in 360-degree unbounded scenes (bird-eye view). (a) For the distant object, invisible ray-3 interferes with ray-1/2 training. (b) The ray-based multi-NeRF framework considers variable visibility of objects to different rays and decouples training in the ray dimension. (c) Compared to the theoretical weight distribution, the sampling along ray-3 is inaccurate incurring training interference.

Figure 2: Oracle experiment: Training interference from invisible rays affects NeRF’s performance.

To verify the effectiveness of Rad-NeRF, we conduct extensive experiments on various types of datasets. The results show that Rad-NeRF can exhibit _anti-aliasing effects_ and obtain _superior geometry modeling_, thus consistently improving the rendering quality of novel views. In addition, Rad-NeRF is _parameter-efficient_ and _super simple to implement_. With only 0.2% extra parameters, Rad-NeRF can increase rendering performance by up to 1.5dB compared to Instant-NGP. By scaling the number of sub-NeRFs through ray-wise decoupling, Rad-NeRF achieves better performance-to-parameter scalability than scaling other dimensions, such as the MLP width or the feature grid.

## 2 Related Work

### Neural Radiance Field

Neural Radiance Field (NeRF)  has received much attention since it was proposed. It uses MLPs to implicitly represent 3D objects or scenes, achieving realistic rendering results. There have been intensive studies on NeRF's extension, including increasing NeRF's training/inference efficiency [36; 8; 21; 25; 18; 5], applying NeRF to specific scenes (large/unbounded/poor-textured) [15; 37; 2; 31; 26; 27; 38], applying NeRF to other tasks (surface reconstruction/scene editing) [35; 20; 29; 14; 33; 32], increasing NeRF rendering quality in few-shot setting [10; 12; 19; 7]. In this work, we aim to increase NeRF's rendering quality in complex scenes, and propose a multi-NeRF training framework, which can leverage the techniques proposed by these single-NeRF researches.

### Multi-NeRF Representation

Due to the limited model capacity, the multi-NeRF method is widely adopted to improve the rendering quality, which can be categorized into point- and ray-based multi-NeRF methods.

**Point-based multi-NeRF method.** These methods divide the 3D space in the point-dimension [30; 37; 38]. 3D points in different regions are computed by different sub-NeRFs. For example, NeRF++  proposes the sphere inversion transformation to map an infinite space to a bounded sphere first, and it uses two NeRFs to model the foreground and background regions respectively. Switch-NeRF  also partitions the scenes in the point-dimension. These methods do not consider the different visibility of a target region to different views and cause training interference on complex scenes with many occlusions. For example, the front side of an object is not visible when it is observed from the back view or blocked by an occlusion. Training the sub-NeRF with rays that do not contain any valid information about the target region might interfere with the training.

**Ray-based multi-NeRF methods.** These methods allocate training rays to different sub-NeRFs and train sub-NeRFs independently. Block-NeRF  and Mega-NeRF  perform the ray allocation in the image-granularity and pixel-granularity, respectively. Both of them need a manually defined allocation rule, which requires prior scene knowledge and cannot be easily adapted to other types of scenes. The former work trains sub-NeRFs in large-scale road scenes with prior knowledge of the image shooting position distribution, and the latter one trains sub-NeRFs in open drone scenes and allocates the rays based on the ray intersecting positions with a horizontal plane. However, defining a ray allocation rule for complex scenes lacking prior scene-specific knowledge remains challenging. Another related work is NID , which proposes a mixture-of-experts NeRF for generalizable scene modeling. In this work, different experts serve as the basis to construct the implicit field of different scenes and the gating module takes in the new scene's image as the input (i.e., image-granularity).

In this work, we propose a gate-guided multi-NeRF mutual learning framework, performing the allocation and decoupling the training in the ray dimension softly. Compared to other multi-NeRF methods, Rad-NeRF boosts the rendering quality without the need for prior scene knowledge.

## 3 Preliminary

NeRF  uses neural networks to represent 3D scenes implicitly. Two MLPs model the density and color of spatial points respectively. The input of density MLP \(F_{}\) is the 3D point coordinate \(\). The input of color MLP \(F_{c}\) includes view direction \(\) and feature \(f\) output by density MLP. NeRF proposes the volume rendering method to render each pixel of an image. It samples \(N\) points along the ray and renders the pixel's color \(()\) by discretely summing density \(_{i}\) and color \(_{i}\) of each point \(i\), which approximates the integral \(C()\) as follows:

\[C()=_{0}^{+}w(t)(t)dt( )=_{i=1}^{N}w_{i}_{i},\] (1) \[T_{i}=(-_{j=1}^{i-1}_{j}_{j}) w _{i}=T_{i}(1-e^{-_{i}_{i}}),\] (2)

where \(t_{i}\) is the distance between \(i\)-th sample's position and the starting point of the ray, \(_{i}=t_{i+1}-t_{i}\) is the distance between adjacent samples and \(T_{i}\) represents the probability that the ray travels from the start to point \(i\) without hitting. The NeRF optimization is based on color supervision.

## 4 Rad-NeRF

NeRF faces the challenge of limited model capacity when rendering complex scenes [37; 30; 38]. However, directly increasing the number of model parameters yields marginal improvement in the rendering quality , posing an important research question: "how to effectively scale up the capacity of NeRF". While the multi-NeRF methods have been proposed as an effective technique in response to this question, they still face limitations in handling complex scenes (with many occlusions and arbitrary shooting trajectories) due to training interference among invisible rays. In this work, we propose a ray-decoupled training framework (Rad-NeRF), effectively scaling up model's capacity by decoupling training in the ray dimension in a learnable way. Figure 3 gives an overview of Rad-NeRF.

### Gate-guided Multi-NeRF Fusion

Motivated by the intuition and oracle experiment discussed in Section 1, rather than using a single NeRF model, designing a multi-NeRF structure that considers different visibility of the region to different rays and decouple NeRF's training in the ray-dimension could be better. We design a ray-based multi-NeRF model structure and introduce a soft gate module to learn the preference of each sub-NeRF for various rays in a learnable way.

**Multi-NeRF Structure.** As shown in Figure 3, we employ a shared feature grid among sub-NeRFs and keep MLP decoders independent for the multi-NeRF structure. As different rays may pass through the same region of 3D space, weight sharing for the feature grid helps training, owing to the feature grid's responsibility for encoding features of 3D spatial points. As validated by the Oracle experiment, training with regions A and B facilitates the training of the shared feature grid

Figure 3: The overview of Rad-NeRF. We construct a multi-NeRF framework based on the hybrid representation, where the feature grid is shared for all sub-NeRFs and the MLP decoders are independent. **(Left)** Given a ray, the soft gate module encodes the ray’s data and outputs a soft score. Then, guided by the gating score, sub-NeRFs’ outputs are fused after the volume rendering process. **(Right)** The fused rendered depth of the ray is used to regularize each sub-NeRF’s geometric encoding.

and improves the rendering quality(PSNR 18.685 vs 18.488). Meanwhile, as the MLP decoder is designed to encode view information, constructing an ensemble of independent MLP decoders helps to decouple the training in the ray dimension, and thus maintains the preference of sub-NeRFs for various rays. Additionally, such structure is a multi-model extension of Instant-NGP , helping to avoid a significant increase in the number of parameters and training complexity. The hybrid representation also maintains high training efficiency.

**Soft Gate Module.** We incorporate a soft gate module to assign gating scores to the sub-NeRFs for each ray. The soft gate module is jointly optimized with NeRF, enabling it to learn the preference of each sub-NeRF for different rays in an end-to-end manner. In contrast to manually assigning training rays to sub-NeRFs, this learnable gating design makes Rad-NeRF **generally applicable to diverse scenes lacking prior scene-specific knowledge**. In Section 5.2, we will also show that the gate module can learn to assign reasonable gating scores that reflect the object visibility of rays, aligning with our intuition that decoupling training in the ray-dimension is important.

Specifically, we employ a four-layer MLP followed by a Softmax function as the gate module. The gate module takes the starting point and direction (\(o\),\(d\)) of a ray \(\) as the input, and outputs the gating scores \(()\) of multiple sub-NeRFs associated with this ray. Instead of applying any sparsification strategies on the gating score \(()\) as in previous work , such as top-k gating function , we use soft gating scores to enhance the smoothness and consistency of rendered results.

As discussed in Section 3, each ray corresponds to a pixel on the image. Following the volume rendering process, we can obtain \(K\) rendered colors for each ray, where \(K\) is the number of sub-NeRFs. Subsequently, multi-NeRFs' outputs are fused in a post-volume-rendering ordering to obtain the final rendering results. The fused color \(()\) of the ray \(\) can be written as below:

\[()=_{k=1}^{K}G_{k}()_{k}(),\] (3)

where \(G_{k}()\) is the \(k\)-th element of gating score \(()\) and \(_{k}()\) is the rendered color of \(k\)-th sub-NeRF for the ray \(\).

### Depth-based Mutual Learning

By the learnable soft gating design, different sub-NeRFs learn different encodings of the scene. We introduce a mutual learning method to enhance the rendering consistency and robustness of sub-NeRFs, wherein each sub-NeRF not only learns from ground truth but also learns from each other. Due to the lack of the ground truth for per-ray depth, NeRF may fail to learn accurate geometry despite accurately rendering training views, which adversely affects its generalization to novel views. To address this, we perform mutual learning with the rendered depths of sub-NeRFs, which serves as a form of geometric regularization and helps the model find more robust geometric solutions. The per-ray depth estimation \(()\) can be written as Equation 4, where \(t_{i}\) is the i-th sample's distance from the starting point on the ray.

\[()=_{i=1}^{N}w_{i}t_{i},\] (4)

In practice, we first fuse the rendered depths of sub-NeRFs guided by the gating score \(()\). Then we use L2 distance to quantify the match of each sub-NeRF's rendered depth \(_{k}()\) and the fused depth \(()\). Our depth-based mutual learning loss is defined as below, where \(\) is the set of sampled rays:

\[L_{dml}=_{}_{k=1}^{n}\|_{k}( )-()\|^{2},\] (5)

Compared to directly averaging multiple sub-NeRFs' depth predictions, the gate-guided fused depth \(()\) is more accurate, as the gating score \(()\) can reflect the prediction confidence of each sub-NeRF for the ray \(\).

### The Overall Training Loss

The overall loss function of Rad-NeRF is given by:

\[L=L_{c}+_{1}L_{dml}+_{2}L_{cv},\] (6)

where \(L_{c}=_{}\|C()-()\|^{2}\) (\(C()\) is the ground truth color value of ray \(\)) is the rendering loss. \(_{1}\) and \(_{2}\) are the weights for regularization terms, which are the only hyper-parameters to be set. The value of \(_{1}\) is chosen from \(1 10^{-4}\) and \(5 10^{-3}\). \(_{2}\) is set to \(1 10^{-2}\) on all the datasets. \(L_{cv}\) is the balancing regularization on the Coefficient of Variation of the soft gating scores, which prevents the gate module from collapsing onto a specific sub-NeRF. The details of \(L_{cv}\) are described and discussed in the Appendix B.

## 5 Experiments

### Datasets and Baselines

**Datasets.** We use five datasets from different types of scenes to evaluate our Rad-NeRF. (1) Object dataset: we take _Masked Tanks-And-Temples dataset (MaskTAT)_ for evaluation, which are photographed objects with masked background; (2) 360-degree inward/outward-facing datasets: we take _Tanks-And-Temples (TAT) dataset_ with unmasked background  and _NeRF-360-v2 dataset_ to evaluate on scenes with large dynamic depth range; (3) free shooting-trajectory datasets: we conduct experiments on _Free-Dataset _ and _ScanNet dataset_, which are large outdoor and indoor scenes respectively. Both larger view ranges and more irregular shooting trajectories pose greater challenges for NeRF rendering.

**Baselines.** We compare our Rad-NeRF with two types of methods: one type uses the grid-based NeRF framework as we do, including PlenOctrees , DVGO , Instant-NGP  and F2-NeRF . The other one is the MLP-based NeRF method, including NeRF , NeRF++ , MipNeRF  and MipNeRF360 , which is inefficient in training and needs almost one day for training in complex scenes. Note that we also implement the NGP-version of Block-NeRF , Switch-NeRF  and Mega-NeRF  to validate the superiority of Rad-NeRF to other multi-NeRF methods. The implementation details of Mega-NGP are shown in the Appendix F.

### Comparative Studies

**Rad-NeRF achieves higher rendering quality than existing single- and multi-NeRF methods.** We report the main quantitative results on the complex scenes and the object dataset in Table 1 and Appendix D respectively. Within no more than one hour of training, Rad-NeRF achieves higher rendering quality compared to other fast training methods and multi-NeRF methods, including Switch-NGP and Block-NGP. We can also see that while Rad-NeRF is designed for complex scene

    &  &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF++ & 20.419 & 0.663 & 0.451 & 27.211 & 0.728 & 0.344 & 24.592 & 0.648 & 0.467 \\ MipNeRF360 & **22.061** & **0.731** & **0.357** & **28.727** & **0.799** & **0.255** & **27.008** & **0.766** & 0.295 \\  MipNeRF360\({}_{short}\)* & 20.078 & 0.617 & 0.508 & 25.484 & 0.631 & 0.452 & 24.711 & 0.648 & 0.466 \\ DVGO & 19.750 & 0.634 & 0.498 & 25.543 & 0.679 & 0.380 & 23.485 & 0.633 & 0.479 \\ Instant-NGP & 20.722 & 0.657 & 0.417 & 27.309 & 0.756 & 0.316 & 25.951 & 0.711 & 0.312 \\ F2-NeRF & – & – & – & 26.393 & 0.746 & 0.361 & 26.320 & **0.779** & **0.276** \\  Switch-NGP\({}^{}\) & 20.512 & 0.654 & 0.432 & 26.524 & 0.740 & 0.331 & 25.755 & 0.694 & 0.341 \\ Block-NGP\({}^{}\) & 20.783 & 0.659 & 0.415 & 27.436 & 0.761 & 0.298 & 26.015 & 0.702 & 0.325 \\   Rad-NeRF & **21.708** & **0.672** & **0.398** & **27.871** & **0.769** & **0.298** & **26.449** & 0.719 & **0.285** \\   

* \({}^{}\) MipNeRF360 requires nearly one day for training. For a fair comparison, we also report its results with one-hour of training.
* \({}^{}\) We adapt Switch-NeRF and Block-NeRF to the Instant-NGP fast training framework.

Table 1: Quantitative results in complex scenes.

rendering, it can also improve the rendering performance of objects. We also integrate Rad-NeRF with the recent SOTA single-NeRF framework ZipNeRF , named Rad-ZipNeRF, in the Appendix I. Rad-ZipNeRF obtains better rendering performance, validating Rad-NeRF's potential for integration with different frameworks.

**Rad-NeRF achieves better recovery of distant details and accurate rendering for less textured regions.** The qualitative results are shown in Figure 4. Compared to other methods, Rad-NeRF achieves better rendering quality in both outdoor and indoor scenes. In outdoor scenes, Rad-NeRF produces detailed and realistic rendering results for the sky and other distant objects. In indoor scenes, Rad-NeRF generates more accurate details for less textured regions such as the wall. Rad-NeRF takes advantage of the gate-guided training decoupling in the ray dimension to boost the model's performance effectively. Results on the ScanNet dataset are shown in the Appendix C.

**The gate module learns to reasonably assign gating scores.** We visualize how the gate module performs training decoupling in Figure 5. As the two sub-NeRFs exhibit complementary gating scores, we omit sub-NeRF2's visualization for brevity. (1) In the Truck scene, the gate module assigns different preferences to sub-NeRF1 in foreground/background regions, thereby mitigating the interference from foreground rays on sub-NeRF1's training with the background region. (2) In the Train scene, sub-NeRF1 exhibits higher preferences for the back side, thereby mitigating the

Figure 4: Qualitative comparisons on three complex scenes. Rad-NeRF achieves better recovery of details for distant objects and less textured regions such as the wall. (Zoom in for the details, e.g., sky, banister, roadblock, wall.)

Figure 5: Visualization of the gating scores of sub-NeRF1 on two different views (visualization of sub-NeRF2 is omitted for brevity).

training interference from invisible frontside rays. (3) In the Caterpillar scene, the gating module assigns different preferences to foreground/background regions or the different sides of the caterpillar, which are clearly distinguished. The visualization demonstrates that Rad-NeRF learns reasonable ray allocations, matching our intuition. Besides, we observe that in some specific scenes, such as the Truck scene, the gating score visualization indeed shows a significant difference between the edge and the central region, correlating with the aliasing issue. Such observation illustrates that tackling the aliasing issue in some scenarios is another insightful explanation of the Rad-NeRF's effectiveness, which is supplementary to our original motivation targeting scenarios with heavy occlusions.

**Scaling up NeRF with the Rad-NeRF framework is more effective than scaling the MLP width, increasing the feature grid size, or adding more feature grids.** By default, we set the number of sub-NeRFs to 2 in all experiments. As shown in Figure 6, when the number of sub-NeRFs increases, Rad-NeRF consistently obtains average performance gains on the ScanNet dataset while only marginally increasing the number of model parameters. Compared with directly increasing the hidden dimension of MLP decoders or the size of the feature grid, Rad-NeRF has better performance-model size scalability. Furthermore, we observe that the model with four sub-NeRFs converges faster than the one with two sub-NeRFs while achieving better rendering quality with the same training iterations, as Figure 7 shows. The ease of training convergence can be attributed to two aspects. On the one hand, the number of learnable parameters and training complexity increases marginally. On the other hand, our gate module (a 4-layer MLP without sinusoidal position encoding) decouples the training in the ray dimension and reduces training interference.

### Comparison with Gaussian Splatting

We additionally compare Rad-NeRF against 3D Gaussian splatting (3DGS)  as a non-neural approach that represents the current state of the art with regard to quality and rendering speed. The comparison is conducted on MaskTAT  and ScanNet  datasets. MaskTAT is an object dataset without point clouds, and ScanNet contains indoor scenes with many less textured regions.

**Rad-NeRF performs better than 3DGS in some cases.** We report results on Table 9, and show qualitative highlights in Figure 8. For the MaskTAT dataset, we initialize 3D GS with random points. Our method performs best over 3D GS and Instant-NGP. For the ScanNet dataset, we initialize 3D GS with the point cloud provided by the dataset. However, there are many less textured regions in

Figure 8: Qualitative comparisons with 3D GS.

indoor scenes that affect the accuracy and density of point clouds. Optical distortion exists in the rendered pictures of 3D GS. In contrast, Rad-NeRF renders more smoothly than all baselines.

**Potential combination of Rad-NeRF with 3DGS.** NeRF is characterized by its neural network-based ray-related predictions, which provide flexibility for cross-scene generalization and enable the application of Rad-NeRF's ray-wise training decoupling approach. In contrast, the plain 3D GS framework parametrizes the scene using a global, non-ray-related representation, making Rad-NeRF inapplicable. However, Rad-NeRF could potentially be applied to generalizable 3D GS frameworks that integrate neural network-based ray-related predictions .

### Ablation Studies

In this section, we conduct ablation studies on Rad-NeRF using the TAT dataset . The key takeaways from our results are summarized below. Some additional ablation studies and analyses are presented in the Appendix E.

**Importance of the gate-guided multi-NeRF fusion and depth-based mutual learning.** The ablation results of the two key components are shown in Table 2. Uniform fusion simply averages multi-NeRFs' outputs to get final results without a gate module. In this way, sub-NeRFs focus on all the training rays instead of having their own preferences, which can not effectively improve rendering quality. For the depth-based mutual learning method, we observe that it enables a smoother and more reasonable depth prediction, as shown in Figure 10. In addition to improving rendering consistency, it also acts as a geometric regularization to reduce the depth ambiguity and avoid overfitting.

We further provide visualizations of different sub-NeRFs' rendering results in Figure 11, which validates that the proposed depth-based mutual learning scheme will not encourage all sub-NeRFs to converge to the same output. On the one hand, the soft gating module allocates different rays to different sub-NeRFs, making them learn from different views. On the other hand, the depth-based mutual learning scheme only lets sub-NeRFs learn the depth from each other rather than the overall rendered density or RGB distribution.

   Method & Metric & M60 & Playground & Train & Truck & Avg \\   & PSNR\(\) & 19.229 & 22.863 & 17.531 & 23.569 & 20.798 \\  & SSIM\(\) & 0.633 & 0.694 & 0.596 & 0.746 & 0.667 \\  & LPIPS\(\) & 0.431 & 0.414 & 0.451 & 0.345 & 0.411 \\   & PSNR\(\) & 18.912 & 23.399 & 17.371 & 24.665 & 21.087 \\  & SSIM\(\) & 0.621 & 0.694 & 0.589 & 0.758 & 0.666 \\  & LPIPS\(\) & 0.436 & 0.402 & 0.449 & 0.329 & 0.404 \\   & PSNR\(\) & 19.051 & 23.901 & 19.369 & 24.509 & **21.708** \\  & SSIM\(\) & 0.631 & 0.689 & 0.612 & 0.757 & **0.672** \\   & LPIPS\(\) & 0.429 & 0.402 & 0.431 & 0.333 & **0.399** \\   

Table 2: Ablation results of gate-guided multi-NeRF fusion and depth-based mutual learning.

Figure 10: Depth visualization comparison between w/o \(L_{dml}\) and w/ \(L_{dml}\) on TAT dataset. Zoom in to see the details of sky and ground.

**Importance of the ray-level allocation.** We evaluate the results of different fusion dimensions in Table 3. Compared to fusing multi-NeRFs' outputs in the point dimension, our ray-based method performs better, validating the superiority of the visibility-aware multi-NeRF method.

**Importance of pixel-granularity fusion.** We compare different fusion granularity in Table 4. In image-granularity fusion, all pixels of an image have the same preference for model parameters, which may not be reasonable. An illustrative example is an image capturing both the central object and the background region, such as the _Truck_ scene shown in Figure 5. In such a case, the rays hitting these two regions should be assigned different model parameters. In contrast, pixel-granularity fusion provides a more fine-grained understanding of the image and scene.

## 6 Limitations

As the gating module (a 4-layer MLP without sinusoidal position encoding) incorporates smoothness prior implicitly, it exhibits smooth and close scores to the nearest seen view for unseen views. Consequently, the generalization of the gating module relies on sufficient training data, and thus Rad-NeRF does not perform well in the few-shot setting (see Appendix K for more results). On the contrary, the proposed method is suitable for the rendering of complex scenes, which themselves often require sufficient training data.

## 7 Conclusion

This work proposes a ray-decoupled training framework (Rad-NeRF) for neural rendering. To alleviate the issue of the training interference problem in complex scenes, we construct a multi-NeRF framework and decouple the training of NeRFs in the ray dimension. Additionally, we propose a depth-based mutual learning method that improves the multi-NeRF rendering consistency and reduces the depth ambiguity, thereby improving generalization to novel views. Extensive experiments across various datasets validate Rad-NeRF's effectiveness and better performance-parameter scalability.

We prospect for further exploration to fully exploit the potential of Rad-NeRF. Here, we outline several possible directions:(1) As researchers may choose different frameworks based on specific situational requirements, adapting Rad-NeRF to different single-NeRF frameworks including 3D GS (non-neural approach) is a valuable next step. (2) The number of sub-NeRFs can be determined automatically based on scene complexity and training resources. (3) We hope the newly proposed scaling dimension, which increases the number of sub-NeRFs through ray-wise decoupling, will enable modeling of complex scenes in a parameter-efficient manner.

   Fusion Granularity & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  Image-level & 21.503 & 0.669 & 0.408 \\ Pixel-level (Ours) & **21.708** & **0.672** & **0.399** \\   

Table 4: Ablation results of fusion granularity.

Figure 11: Independent and fused rendering results of sub-NeRFs on TAT dataset.

   Fusion Dimension & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  Point-level & 20.796 & 0.661 & 0.413 \\ Ray-level (Ours) & **21.708** & **0.672** & **0.399** \\   

Table 3: Ablation results of fusion dimensions.