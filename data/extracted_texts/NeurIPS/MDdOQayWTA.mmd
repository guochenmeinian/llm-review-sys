# A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits

Jungyhun Lee, Se-Young Yun

Kim Jaechul Graduate School of AI

KAIST

Seoul, Republic of Korea

{jh_lee00, yunseyoung}@kaist.ac.kr

&Kwang-Sung Jun

Department of Computer Science

University of Arizona

Tucson, AZ, USA

kjun@cs.arizona.edu

###### Abstract

We present a unified likelihood ratio-based confidence sequence (CS) for _any_ (self-concordant) generalized linear model (GLM) that is guaranteed to be convex and numerically tight. We show that this is on par or improves upon known CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In particular, for the first time, our CS for Bernoulli has a \((S)\)-free radius where \(S\) is the norm of the unknown parameter. Our first technical novelty is its derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform prior/posterior, despite the latter being a rather unpopular choice for deriving CSs. As a direct application of our new CS, we propose a simple and natural optimistic algorithm called OFUGLB, applicable to _any_ generalized linear bandits (**GLB**; Filippi et al. (2010)). Our analysis shows that the celebrated optimistic approach simultaneously attains state-of-the-art regrets for various self-concordant (not necessarily bounded) **GLBs**, and even \((S)\)-free for bounded **GLBs**, including logistic bandits. The regret analysis, our second technical novelty, follows from combining our new CS with a new proof technique that completely avoids the previously widely used self-concordant control lemma (Faury et al., 2020, Lemma 9). Numerically, OFUGLB outperforms or is at par with prior algorithms for logistic bandits.

## 1 Introduction

One paramount task in statistics and machine learning is to estimate the uncertainty of the underlying model from (possibly noisy) observations. For example, in interactive machine learning scenarios such as bandits (Lattimore and Szepesvari, 2020; Robbins, 1952; Thompson, 1933) and recently reinforcement learning with human feedback (RLHF; Christiano et al. (2017); Ouyang et al. (2022)), at each time step \(t\), the learner chooses an action \(_{t}\) from an available set of actions \(_{t}\) and observes reward or outcome \(r_{t}\) that is modeled as a distribution whose mean is an unknown function \(f^{*}\) of \(_{t}\); i.e., \(r_{t} p(|_{t};f^{*})\). One popular choice of such a model is the **generalized linear model** (GLM; McCullagh and Nelder (1989)) that extends exponential family distributions to have a linear structure in its natural parameter as \(,_{*}\), where \(_{*}\) is an unknown parameter. In other words, the mean function is \(f^{*}()=(,_{*})\) for some inverse link function \(\). This encompasses a wide range of distributions, which in turn makes it ubiquitous in various real-world applications, such as news recommendations (Bernoulli; Li et al. (2010, 2012)), social network influence maximization (Poisson; Gisselbrecht et al. (2015); Lage et al. (2013)), and more. In such tasks, the learner must estimate the uncertainty about \(_{*}\)_at each time step_\(t 1\), given observations \(\{(_{s},r_{s})\}_{s=1}^{t-1}\), to make wise decisions. One popular and useful way to capture the uncertainty is via a _time-uniform confidence sequence (CS)_\(\{_{t}()\}_{t=1}^{}\), which takes the form of \([2t 1:_{*}_{t}()]\). Recently, CS has been described as one of the key components for _safe anytime-valid inference (SAVI)_ that can ensure the validity/safeness of sequentially adaptive statistical inference (Ramdas et al., 2023).

Existing CSs for GLMs, however, are far from ideal. Much of the prior works focus on obtaining CS for specific instantiations of GLMs, such as Gaussian (Abbasi-Yadkori et al., 2011; Flynn et al., 2023) and Bernoulli (Abeille et al., 2021; Faury et al., 2020, 2022; Lee et al., 2024). Especially for Bernoulli, all the existing CSs suffer from \((S)\) factor in the radius, where \(S\) is the norm of the unknown parameter \(_{}\). Emmenegger et al. (2023); Jun et al. (2017); Li et al. (2017) proposed generic CSs that work for any convex GLMs, but their radii all suffer from a globally worst-case curvature of \(\), which is detrimental in many cases (e.g., for Bernoulli, it scales as \(e^{S}\)).

Contributions.First, we propose a _unified_ construction of likelihood ratio-based CS for any convex GLMs (Theorem 3.1) and then instantiate it as an ellipsoidal CS for self-concordant GLMs, including Bernoulli, Gaussian, and Poisson distributions (Theorem 3.2). _Notably, we keep track of all the constants so that any practitioner can directly implement it without trouble._ The proof uses ingredients from time-uniform PAC-Bayesian bounds (Chugg et al., 2023) - martingale + Donsker-Varadhan representation of KL + Ville's inequality. The main technical novelty lies in using _uniform_ prior/posterior for the analysis, inspired by various literature on portfolios (Blum and Kalai, 1999) and fast rates in statistical/online learning (Foster et al., 2018; Grunwald and Mehta, 2020; Hazan et al., 2007; van Erven et al., 2015).

Secondly, we apply our novel CSs to contextual generalized linear bandits (**GLB**; Filippi et al. (2010)) with changing (and adversarial) arm-sets, and propose a new algorithm called **Optimism in the Face of Uncertainty for Generalized Linear Bandits** (OFUGLB). OFUGLB employs the simple and standard optimistic approach, choosing an arm that maximizes the upper confidence bound (UCB) computed by our CS (Abbasi-Yadkori et al., 2011; Auer, 2002). We show that OFUGLB achieves the state-of-the-art regret bounds for self-concordant (possibly _unbounded_) **GLB** (Theorem 4.1). This is the first time a computationally tractable, _purely_ optimistic strategy attains such \((S)\)-free regret for logistic bandits in that OFUGLB does not involve an explicit warmup phase and only involves convex optimization subroutines. Our other significant main technical contribution is the analysis of OFUGLB, as naively applying existing analysis techniques for optimistic algorithms (Abeille et al., 2021; Lee et al., 2024) yields a regret bound whose leading term scales with \((S)\). We identify the key reason for such additional dependency as the use of self-concordance control lemma (Faury et al., 2020, Lemma 9), and provide an alternate analysis that completely bypasses it, which may be of independent interest in the bandits community and beyond.

## 2 Problem Setting

We consider the realizable (online) regression with the **generalized linear model** (GLM; McCullagh and Nelder (1989)) whose conditional probability measure of \(r\) is given as

\[dp(r|;_{})=(,_{ }-m(,_{})}{g()}+h(r,) )d,\] (1)

where \(\) is the dispersion parameter, and \(\) is some known base measure (e.g., Lebesgue, counting). We assume the following:

**Assumption 1**.: \(_{}^{d}(S):=\{ ^{d}:\|\|_{2} S\}\) _for some known \(S>0\). Also, \(\) is nonempty, compact, and convex with intrinsic dimension1\(d\)._

**Assumption 2**.: _The domain \(X\) for arm (context) \(\) satisfies \(X^{d}(1)\)._

**Assumption 3**.: \(m\) _is three times differentiable and convex, i.e., \(m^{}\) exists and \(:=m^{} 0\)._

In the **generalized linear bandit (GLB)** problem, at each time \(t[T]\), the learner observes a time-varying, arbitrary (adversarial) arm-set \(_{t} X\), chooses a \(_{t}_{t}\), and receives a reward \(r_{t} p(|_{t},_{})\). Let \(_{[T]}:=_{t=1}^{T}_{t}\) and \(_{t+1}:=(_{t},r_{t},_{t+1})\) with \(_{0}=(_{1})\) be the filtration in the canonical bandit model (Lattimore and Szepesvari, 2020, Chapter 4.6). From well-known properties of GLMs (McCullagh and Nelder, 1989), we have that \([r_{t}|_{t}]=m^{}(_{t},_{} )(_{t},_{})\) and \([r_{t}|_{t}]=g()(_{t},_{ })\), where \(\) is the _inverse link function_. We also define the following quantity describing the maximum slope of \(\): \(R_{}:=_{_{[T]},} (,)\).

Note that many common distributions, such as Gaussian (\((z)=z\), \(R_{}=1\)), Poisson (\((z)=e^{z}\), \(R_{}=e^{S}\)), and Bernoulli (\((z)=(1+e^{-z})^{-1}\), \(R_{}=1/4\)), fall under the umbrella of GLM.

Unified Likelihood Ratio-based Confidence Sequence for GLMs

The learner's goal is to output a time-uniform confidence sequence (CS) for \(_{}\), \([ t 1:_{}_{t}()]\), where \(\) is w.r.t. the randomness of the confidence sets \(_{t}()\). In this work, we are particularly interested in the log-likelihood-based confidence set "centered" at the _norm-constrained_, batch maximum likelihood estimator (MLE):

\[_{t}():=\{:_{t}()-_{t}(}_{t})_{t}()^{2} \},\] (2)

where \(_{t}()^{2}\) is the "radius" of the CS that we will define later, \(_{t}()\) is the negative log-likelihood of \(\) w.r.t. data collected up to \(t-1\), and \(}_{t}\) is the corresponding MLE:

\[_{t}():=_{s=1}^{t-1}\{_{s}() _{s},+m(_{ s},)}{g()}\},}_{t}:= *{arg\,min}_{}_{t}().\] (3)

Note that \(h(r_{s},)\) is omitted as it plays no role in the confidence set nor the MLE.

The form of the confidence set is the same as Lee et al. (2024) and convex relaxation of Abeille et al. (2021), all of which utilizes a single, cumulative & _constrained_ MLE \(}_{t}\) to compute the loss at time \(t\). Other approaches include using a single regularized MLE \(}_{t}\) that may lie outside of \(\)(Abbasi-Yadkori et al., 2011), using a sequence of MLEs \(\{}_{s}\}_{s=1}^{t}\) to compute the loss at time \(t\)(Abbasi-Yadkori et al., 2012; Emmenegger et al., 2023; Faury et al., 2022; Jun et al., 2017; Wasserman et al., 2020), and computing the _expected_ loss over some distribution (e.g., Gaussian) without committing to point estimators (Flynn et al., 2023). As one can see later, our derivation of the CS resembles the last approach: we also start from an expectation of loss over a prior distribution of \(\) without committing to an estimator. Yet, we introduce a single estimator \(}_{t}\) to avoid the computational difficulty of evaluating the expectation.

Our first main contribution is the following unified confidence sequence for _any_ GLMs, regardless of whether it is bounded or not, as long as the corresponding log-likelihood loss is Lipschitz:

**Theorem 3.1** (Unified CS for GLMs).: _Let \(L_{t}\) be the Lipschitz constant11 of \(_{t}()\) that may depend on \(\{(_{s},r_{s})\}_{s=1}^{t-1}\). Then, we have \([ t 1:_{}_{t}( )]\), where_

\[_{t}()^{2}=+_{c_{t}(0,1]}\{d }+2SL_{t}c_{t}\}+d(e }{d}),\]

_where the last inequality follows from the choice \(c_{t}=1}\)._

**Remark 1** (Generality of our Unified CS).: _The above holds for any distribution over any Polish space, although \(_{t}()\) is convex if and only if \(_{t}\) is convex. For GLMs, convexity is guaranteed._

Practically, the computation of \(L_{t}\) involves a potentially non-concave maximization over a convex set, which is NP-hard in general (Murty and Kabadi, 1987). In Table 1, we provide _closed-form_ (up to absolute constants), high-probability upper bounds for \(L_{t}\)'s for various GLMs. Note that for the learner to implement the CS, she also needs to know \(S\), or its upper bound.

Comparisons to Prior Works.Lai (1976) derived the first generic CS for the exponential family based on a generalized likelihood ratio, but it is only applicable for \(\) and is hard to instantiate. Recently, several works have provided CSs for either generic GLMs (Emmenegger et al., 2023; Jun et al., 2017; Li et al., 2017) or specific GLMs (linear: Abbasi-Yadkori et al. (2011); Flynn et al. (2023), logistic: Abeille et al. (2021); Faury et al. (2020); Lee et al. (2024)). The generic CSs are generally not tight as the "radius" often scales with \(:=_{ X,(, )^{-1}}\), which scales exponentially in \(S\) for Bernoulli (Faury et al., 2020). For instance, Theorem 1 of Jun et al. (2017) and Theorem 1 of Li et al. (2017) propose ellipsoidal CSs that provably satisfy \(\|}_{t}-_{}\|_{_{t}}^{2} _{1}(t,)\), with \(_{1}\) always scaling with \(\). Emmenegger et al. (2023) proposed a weighted sequential likelihood testing-based CS \(_{t}\) and showed its efficacy empirically. Theoretically, they showed that \(_{t}\) satisfies \(D(,_{*})_{2}(t,)\) for some Bregman divergence \(D(,)\) and a \(_{2}\) always scaling with \(\) as well. We believe their relaxation is not tight enough to warrant a fair comparison and leave to future work on theoretically comparing our CS to theirs. Chowdhury et al. (2023) proposed Bregman divergence-based CSs for generic exponential families, which are quite closely related to our CS; see Appendix A for further discussions. On the other hand, the CSs for specific GLMs are inapplicable to GLM models beyond what they are designed for and may not even be sufficiently tight. The prior state-of-the-art (likelihood ratio-based) CS radius for Bernoulli is \((S(1/)+d(St/d))\) of Lee et al. (2024), while our theorem gives us \(((1/)+d(St/d))\). Note that we _completely_ remove the \((S)\)-dependency from the radius, resolving an open problem posited by Lee et al. (2024). Later in Section 4, we show this is significant, both theoretically _and_ numerically.

### Ellipsoidal Confidence Sequence for Self-Concordant GLMs

We now provide an _ellipsoidal_ relaxation of Theorem 3.1 for the following class of GLMs:

**Assumption 4** (Russac et al. (2021)).: _GLM is (**generalized) self-concordant**, _i.e., the following quantity is well-defined (finite): \(R_{s}:=\{R 0:|(,)|  R(,),\; X,\}\)._

For instance, Bernoulli satisfies this with \(R_{s}=1\), and more generally, GLM bounded by \(R\) a.s. satisfy this assumption with \(R_{s}=R\)(Sawarni et al., 2024, Lemma 2.1). Many unbounded GLMs also satisfy this assumption, such as Gaussian (\(R_{s}=0\)), Poisson (\(R_{s}=1\)), and Exponential (\(R_{s}=0\)).

For such _self-concordant GLMs_, we have the following slightly relaxed ellipsoidal CS, whose proof is deferred to Appendix D:

**Theorem 3.2** (Ellipsoidal CS for Self-Concordant GLMs).: _With the same notations as Theorem 3.1, we have that for any \( 0\), \([ t 1:_{*}_{t}(, )]\), where_

\[_{t}(,):=\{^{d}:\| -}_{t}\|_{^{2}_{t}( }_{t})+_{d}}^{2}_{t}()  2(1+SR_{s})(4S^{2}+_{t}()^{2})\}.\]

Let us denote \(A B\) if \(A cB\) for some absolute constant \(c>0\). Note that the relaxation is order-wise strict only when \(R_{s}>0\). For instance, for Gaussian where \(R_{s}=0\), the ellipsoidal relaxation does not introduce additional \(S\)-dependency when we choose \(=(})\). We then have that \(^{2}_{t}(}_{t})=} _{s=1}^{t-1}_{s}_{s}^{*}=:}_{t}\), and \(L_{t} St\) with high probability (Proposition C.1). Combining everything, we have \(\|-}_{t}\|_{_{t}}^{2}^{2} ((t/)+d(St/d))\), which _completely_ matches the prior state-of-the-art radius as in Lemma D.10 of Flynn et al. (2023) with \(c=^{2}S^{2}\).

In bandits, the ellipsoidal CS allows one to equivalently rewrite the optimistic optimization in the UCB algorithm (Auer et al., 2002) as a _closed form bonus-based optimization_ over the arm-set \(_{t}\):

\[*{arg\,max}_{_{t},_ {t}(,)},=*{arg\,max} _{_{t}},}_{t}+ ()}\|\|_{(^{2}_{t}(}_{t})+_{d})^{-1}},\] (4)

i.e., there is no need to solve a convex optimization for each arm. In the high-dimensional scenario where \(t=o(d)\), one can compute \((^{2}_{t}(}_{t})+_{d})^{-1}\) with a time complexity of \((td^{2})\) per round via the Sherman-Morrison formula (Sherman and Morrison, 1950), which is more efficient than the naive matrix inversion that takes \((d^{3})\) time complexity.

### Proof of Theorem 3.1 - PAC-Bayes Approach with Uniform Prior

We consider \(M_{t}():=(_{t}(_{})-_{t} ()),\) the likelihood ratio between the (estimated) distribution corresponding to \(\) and the true distribution corresponding to \(_{}\). This has been the subject of study for over 50 years (Darling and Robbins, 1967a,b; Lai, 1976; Robbins and Siegmund, 1972) and recently revisited by statistics and machine learning communities (Emmenegger et al., 2023; Flynn et al., 2023; Ramdas et al., 2023; Wasserman et al., 2020).

We follow the usual recipes for deriving time-uniform PAC-Bayesian bound (Alquier, 2024; Chugg et al., 2023). We start with the following time-uniform property:

**Lemma 3.1**.: _Let \((0,1)\). For any data-independent probability measure \(\) on \(\), we have:_

\[( t 1:_{}[M_{t}( )]),\] (5)

_where \(\) is over the randomness of the data (and thus randomness of \(_{t}\)'s)._

Proof.: First, it is easy to see that \(M_{t}()=_{s=1}^{t}|_{t};)}{dp( r_{s}|_{s};_{})}\) is a nonnegative martingale w.r.t. \(_{t}\):

\[[M_{t}()|_{t-1}]=M_{t-1}() [|_{t};)}{dp(r_{t}|_{t};_{ })}_{t-1}]=M_{t-1}()_{t};)}{dp(r|_{t};_{})}dp(r|_{t}; _{})}_{=1}.\]

Now consider the random variable \(_{}[M_{t}()]\), which is adapted to \(_{t}\). This is a martingale, as

\[[_{}[M_{t}()]|_ {t-1}]}{{=}}_{} [[M_{t}()|_{t-1}]]=_{ }[M_{t-1}()]\]

where \((*)\) follows from Tonelli's theorem. We conclude by Ville's inequality (Ville, 1939). 

We recall the variational representation of the KL divergence:

**Lemma 3.2** (Theorem 2.1 of Donsker and Varadhan (1983)).: _For two probability measures \(,\) over \(\), we have the following: \(D_{}(||)=_{g:} _{}[g()]-_{}[e^{g()}]\)._

We then have the following:

**Lemma 3.3**.: _For any data-independent prior \(\) and any sequence of adapted posterior distributions (possibly learned from the data) \(\{_{t}\}\), the following holds: for any \((0,1)\),_

\[( t 1:_{t}(_{})-_{ _{t}}[_{t}()]+D_{}(_{t}||)).\] (6)

Proof.: Note that

\[_{}[M_{t}()]-_{t }(_{})=_{}[(- _{t}())]}{{}} _{_{t}}[-_{t}()]-D_{ }(_{t}||),\]

where \((*)\) follows from Lemma 3.2 with \(g()=-_{t}()\). By Lemma 3.1, we have that \(( t 1:_{}[M_{t}()])\). Rearranging gives the desired statement. 

**Remark 2** (Choice of KL).: _One can replace KL with other divergences with similar variational formulations (Ohnishi and Honorio, 2021). As we will show later, KL suffices for our purpose._

Up to now, it is well-known in the PAC-Bayes literature. Our main technical novelty lies in how to choose \(\) and \(_{t}\), which is as follows: for \(c_{t}(0,1]\) to be determined later, we set

\[=(),_{t}=(_{t}(1-c_{t})}_{t}+c_{t}),\] (7)

where \(()\) is the uniform distribution and \(+=\{+:\}\) for a vector \(^{d}\).

Then, denoting \(()\) as the (Lebesgue) volume in \(^{d}\), we have

\[D_{}(_{t}||)=()}{ ()}=()}{((1-c_{t})}_{t}+c_{t})}=()}{(c_{t})}=( )}{c_{t}^{d}()}=d}.\]We also have that

\[_{_{t}}[_{t}()]=_{t}(}_{t})+_{_{t}}[ _{t}()-_{t}(}_{t})] _{t}(}_{t})+2SL_{t}c_{t},\]

where follows from the Lipschitzness of \(_{t}()\) and the fact that for \(=(1-c_{t})}_{t}+c_{t}} _{t}\), \(\|-}_{t}\|_{2}=c_{t}\|}-}_{t}\|_{2} 2Sc_{t}\). We conclude by minimizing over \(c_{t}(0,1]\). 

### Intuitions Behind the Proof of Theorem 3.1

Constrained MLE and Uniform Prior/Posterior.As we consider **constrained MLE**, we know that \(}_{t}\), i.e., our "belief" on our MLE is precisely the prior \(=()\). Then, as we want the true parameter \(_{*}\) to be close to \(}_{t}\), we want to show that a sufficiently large "posterior volume" is near \(}_{t}\), formalized as \(_{t}=((1-c_{t})}_{t}+c_{t})\) for some time-dependent shrinkage factor \(c_{t}(0,1]\). We later appropriately choose \(c_{t}\) to optimize the PAC-Bayesian bound.

We remark that the uniform prior/posterior has been previously considered in universal portfolios (Blum and Kalai, 1999, Theorem 1) and fast rates in online learning (Foster et al., 2018; Hazan et al., 2007); see Appendix A for discussions on relations to fast rates literature. To our knowledge, we are the first to use such uniform prior/posterior in the (time-uniform) PAC-Bayes context.

**Remark 3** (Use of Regularized MLE?).: _When one uses regularized MLE instead of constrained, as it is not guaranteed to be in \(\), one cannot directly use the same uniform prior/posterior. One approach may be to appropriately project the regularized MLE onto \(\) (e.g., Eqn. (9) of Faury et al. (2020)). However, the previously considered projections that guarantee the tightness of the resulting CS involve a nonconvex optimization and are, thus, computationally intractable. One could also consider using high regularization, which may result in additional dependencies on \(S\) in the final CS radius. We conjecture that similarly tight guarantees can be recovered with regularized MLE if one uses other appropriate prior/posterior whose supports are the entire \(^{d}\) (e.g., Gaussian)._

Relations to Theorem 3 of Foster et al. (2018).Let us first briefly recall its proof. The authors first consider a distribution \(P_{t}()\) over the parameter \(W\) (see their Algorithm 1) and use \(\)-mixability of the logistic loss to obtain an inequality involving the negative-log-integral term \(_{}(-_{t}(Wx_{t},y_{t}))dW\). They define \(S= W^{*}+(1-)\), where \(W^{*}\) is the ground-truth optimal parameter and \([0,1)\) is to be determined later. The proof concludes by chaining \(_{}_{S}\) with the \(_{}\)-Lipschitzness of the logistic loss and expanding the integral.

Our proof is inspired by the above, with some key differences. While the negative-log-integral also arises in our scenario, we adopt a more compact, streamlined PAC-Bayes approach. In our case, a similar quantity \(_{}[(-_{t}())]\) arises from our Donsker-Varadhan representation (Lemma 3.2). We then apply Ville's inequality to obtain the time-uniform PAC-Bayes bound (Lemma 3.1), and our choices of prior/posterior resemble their choice of \(S\). Our Lipschitzness argument at the end also resembles their \(_{}\)-Lipschitzness argument.

## 4 **O**fuglb: A Generic UCB Algorithm for Self-Concordant GLBs

As a direct application of our CS, we consider self-concordant **GLB**(Filippi et al., 2010; Janz et al., 2024), where at each time \(t\), the learner chooses a \(_{t}_{t}\) dependent on the history \(\{(_{s},r_{s})\}_{s=1}^{t-1}\) and receives \(r_{t} p(|_{t},_{*})\). The learner's goal is to minimize the (pseudo-)regret, \((T):=_{t=1}^{T}\{(_{t,},_{ })-(_{t},_{})\},\) where \(_{t,}:=_{_{t}}(,_{})\).

Inspired by the optimism principle (Abbasi-Yadkori et al., 2011; Auer, 2002), based on our new, improved confidence sequence (Theorem 3.1), we propose OFUGLB (Algorithm 1), a generic UCB-type algorithm that applies to _any_ instantiations of **GLB**. Through a new proof technique that allows us to circumvent \(\)- and \((S)\)-dependencies in the leading term, our unified algorithm attains or improves the known state-of-the-art regret bound for the class of _self-concordant_ **GLB**, which encompasses a zoo of well-studied stochastic bandits such as linear (Abbasi-Yadkori et al., 2011; Auer, 2002), Poisson (Gisselbrecht et al., 2015), logistic (Abeille et al., 2021; Faury et al., 2020), etc.

We define the following problem difficulty quantities: recalling that \(_{[T]}=_{t[T]}_{t}\),

\[_{}(T):=_{t[T]}( _{t,},_{})},(T):=_{ _{[T]},}( ,)}.\] (8)

These may scale exponentially in \(S\), e.g., for logistic bandits (Faury et al., 2020; Filippi et al., 2010), but we will later show that through our new analysis, the leading term of the regret scales _inversely_ with \(_{}(T)\), and the transient term scales linearly with \((T)\).

We now present the _unified & state-of-the-art_ regret guarantee for self-concordant **GLB**s:

**Theorem 4.1** (OFUGLB for Self-Concordant **GLB**).: _OFUGLB attains the following regret bound for self-concordant_ **GLB** _with probability at least \(1-\):_

\[(T)(T)} }{d}}ST}{d}}}_{}+ R_{s}R_{}(T)(1+)}_{},\]

_where \(L_{T}\) is as defined in Theorem 3.1 and we assume that \(=(d}{d}).\)_

### Proof Sketch of Theorem 4.1 - Regret Analysis of OFUGLB

We first emphasize that even though we have a tight CS (Theorem 3.1), naively combining it with existing regret analyses of logistic bandits (Abeille et al., 2021; Lee et al., 2024)_still_ results in an extra factor of \(S\) in the leading term. The prior proof applies the Cauchy-Schwartz inequality w.r.t. the (regularized) Hessian \(_{t}(_{})=+_{s=1}^{t-1}_{s}( _{})_{s}_{s}^{}\) with \(_{s}():=(_{s},)\), which forces the use of self-concordant lemma (Abeille et al., 2021, Lemma 8). This results in a CS of the form \(\|_{}-}_{t}\|_{_{t}(_{ })}=(S_{t}())\). Then, using the same regret decomposition of Abeille et al. (2021) and the optimism principle, the leading term of the regret is bounded as

\[_{t}_{t}(_{})_{t,}-_{t}, _{} S_{t}()_{t}(_{})}}_{(T)}} \|_{t}(_{})} {x}_{t}\|^{2}_{_{t}(_{})^{-1}}}}_{}}.\]

Our proof begins by applying Cauchy-Schwartz w.r.t. \(}_{t}(}_{t})\), derived from the integral remainder in first-order Taylor expansion of \(_{t}()\) at \(}_{t}\). With this, we have that \(\|_{}-}_{t}\|_{}_{t}( }_{t})}=(_{t}())\) (Lemma E.6), avoiding the extra \(S\). However, as \(}_{t}(}_{t})=_{s=1}^{t-1}( _{s},}_{t})_{s}_{s}^{}\) for some well-defined scalar function \(_{s}\), the elliptical potential lemma (as done above) is _not_ applicable due to the explicit dependency on \(}_{t}\)! This difficulty is analogous to the analysis of Logistic-UCB-2 in Faury et al. (2020), where a similar difficulty arose because their improved bonus \(_{t,2}\) depends on the current estimate of the parameter as well (see their Lemma 4). They circumvent this issue by explicitly modifying the UCB algorithm to incorporate additional constraints on the "admissible log-odds," which leads to a computationally inefficient algorithm.

Notably, we show via a new proof technique that the vanilla UCB can _implicitly_ handle those constraints by designating a "worst-case" parameter over all future iterations (Eqn. (21)). We develop many other intriguing results, such as a novel self-concordance lemma that bounds the difference of \(\)'s with that of \(\)'s times \(R_{s}\) (Lemma E.3). We provide the full proof in Appendix E.

### Instantiations and Discussions of Theorem 4.1

In Table 2, we instantiate Theorem 4.1 for various self-concordant **GLBs**. It can be seen that our OFUGLB attains state-of-the-art regret guarantees in all considered scenarios, either by achieving (linear) or improving upon (bounded, logistic) the known regret bounds! Note that the instantiation for (sub-)Gaussian linear bandits is meant to be a sanity check because tighter confidence sets are available in Flynn et al. (2023) and Chowdhury et al. (2023, Appendix F).

To our knowledge, only a few works deal with generic, (possibly unbounded) self-concordant **GLBs**. Jun et al. (2017) proposed UCB-style **GLOC** and its variants, which, however, incur regret bounds scaling with \(_{}(T)\) in the leading term. Concurrent with our work, Liu et al. (2024) prove that all **GLBs** with light-tailed base distribution are self-concordant, and propose OFU-GLB with regret of \(}(d(T)}+d_{}(T))\). Another line of works (Abeille and Lazaric, 2017; Dong et al., 2019; Janz et al., 2024; Kim et al., 2023; Kveton et al., 2020) considers randomized exploration-based algorithms, including Thompson sampling, which we discuss further in Appendix A.

Below, we discuss our results for bounded **GLB**, logistic, and Poisson bandits in-depth.

Bounded GLB.The only prior work applicable to general bounded **GLB** is Sawarni et al. (2024), where the authors propose RS-GLinCB with regret as in Table 2. Compared to our regret, they are slightly better as their transient term scales as \(_{}(T)\) while ours scales as \((T)\), but we have a much better dependency on \(R\) (\(R\) vs. \(R^{5}\)). Despite this seeming gap, as RS-GLinCB relies on an explicit warm-up scheme, our OFUGLB is expected to have superior numerical performance as it avoids excessive exploration in the early phase. We will elaborate more on this issue in Section 5. Also, it should be noted that Sawarni et al. (2024) requires a _nonconvex_ optimization as a subroutine to obtain \((S)\)-free regret (see their Appendix E). Still, RS-GLinCB has its advantages in that it only requires \((^{2}T)\) switches while we require \((T)\) switches; it is an interesting open problem whether a lazy variant of OFUGLB with same (or better) regret guarantee is possible.

Logistic Bandits.Although the logistic bandit is a special case of the bounded **GLB**, the number of prior works and its practical applicability to recommender systems (Li et al., 2010, 2012) and recently RLHF (Das et al., 2024) makes it deserving of separate discussions. We first review the prior works on logistic bandits. Faury et al. (2020) was the first to obtain a regret bound of \(}(d+d^{2}(T))\) (up to some dependencies on \(S\)) that is \(\)-free in the leading term. Subsequently, a local minimax regret lower bound of \(((d/S)(T)})\) was proven (Abeille et al., 2021, Theorem 2)2, suggesting that more nonlinearity helps, and several works have focused on proposing and analyzing algorithms with matching upper bounds. One line of works (Abeille et al., 2021; Lee et al., 2024), including this work, focuses on getting a tight _convex_ CS for logistic losses, which then directly gives an OFUL-type algorithm. Abeille et al. (2021) first proposed a likelihood ratio-based CS, albeit somewhat loose

 
**GLB** & **Our regret bound** & **Prior state-of-the-art** \\  \(R\)-Bounded & \(d(T)}}+d^{2}RR_{}(T)\) & \(d(T)}}+d^{2}R^{5}S^{2}_{ }(T)\\ }]}\\ }]}\\ }]}\\ \) \\  Logistic & \(d(T)}}+d^{2}(T)\) & \(d(T)}}+d^{2}S^{2}_{ }(T)\\ }]}\\ }]}\\ \) \\  Linear\({}^{a}\) & \( d\) & \( d\) \\  Poisson & \(dS(T)}}+d^{2}e^{2S}(T)\) & \(d^{3/2}(T)}}\\ }]}^{b}\\ }]}\\ \) \\   \({}^{a}\) We choose \(c=^{2}S^{2}\) in Lemma D.10 of Flynn et al. (2023).

\({}^{b}\) Here, we omit the dependencies on \(S\) and the transient term from explicit warmup.

Table 2: Regret bounds of OFUGLB for various self-concordant **GLBs**. Logarithmic factors are omitted to avoid a cognitive overload. Let \(_{}(T):=_{_{i=1}^{T}_{t}} {((,_{}))}\) and \(g()=(1)\). Here, “\(R\)-Bounded” means \(|r_{t}| R\;a.s.\).

in \(S\). Lee et al. (2024) proposed a new framework for converting an achievable online learning algorithm to a tighter CS and proposed a UCB algorithm that attains the prior (to this work) state-of-the-art regret bound of \(}(dS(T)}+R_{}(T))\) with \(R_{}(T)\) being arm-set geometry-dependent transient term3 From a computational perspective, Faury et al. (2022) proposed an online Newton step-based algorithms that attain the regret bound of \(}(dS(T)}+d^{2}S^{6}(T))\) using only \(( t)\) computational cost _and_\((1)\) storage per time step; the computational cost was later improved to \((1)\) in Zhang and Sugiyama (2023). Another line of works (Mason et al., 2022; Sawarni et al., 2024) proposed algorithms that perform an _explicit_ warm-up in the early stages. Thanks to the explicit warmup, both attain regret with \((S)\)-free leading term, e.g., \(}(d(T)}+d^{2}S^{2}_{ }(T))\) by Sawarni et al. (2024). However, the explicit warmup typically lasts for \(((T))\) or \((_{}(T))\) time steps, resulting in potentially very large initial regret. This is later verified in our logistic bandits experiments. Our OFUGLB is the first purely optimism-based UCB algorithm (no explicit warmup) that attains a \((S)\)-free leading term in the regret.

Poisson Bandits.Despite its potential to model various real-world problems involving count feedback, Poisson bandits have not been studied often in the literature. Gisselbrecht et al. (2015) was the first to consider contextual Poisson bandits and proposed UCB and optimistic Bayesian-based algorithms (May et al., 2012), but without any regret guarantees. To our knowledge, our Theorem 4.1 provides the first regret bound for the (finite-dimensional) contextual Poisson bandits without reward boundedness assumption. On a related note, Mutny and Krause (2021) consider Poisson bandits with the intensity function in an RKHS. Their linear RKHS formulation is, however, incompatible with our log-linear formulation; see their Appendix A.1 for further discussions.

## 5 Experiments

We perform experiments on logistic bandits to complement the theoretical improvement in our regret bounds and CS. The codes are available in our GitHub repository4, forked from the previous repository5 of Faury et al. (2022). Our GitHub provides the unified implementations of all considered algorithms, which we hope will be helpful in future research and benchmarking of logistic bandits. In Appendix G, we provide the missing implementation details and additional experimental results.

Baselines and Setting.We compare our OFUGLB (likelihood ratio-based CS; Theorem 3.1) and OFUGLB-e (ellipsoidal CS; Theorem 3.2) to the following five baselines: EMK (Emmenegger et al., 2023), EVILL (Janz et al., 2024), RS-GLinCB (Sawarni et al., 2024), OFULog+ (Lee et al., 2024), and ada-OFU-ECOLog(Faury et al., 2022). Note that the last two are specific to logistic bandits, and RS-GLinCB is specific to bounded **GL**Bs. We emphasize that when implementing OFUGLB and OFUGLB-e, we use the precise theoretical hyperparameters as given in our theorem statements without further tuning. To highlight the practical effectiveness of our theoretical algorithms in comparison to randomized exploration, which is known to perform well in practice (Chapelle and Li, 2011; Russo et al., 2018), we use a single, untuned hyperparameter for EVILL6. For the experimental setup in this section, we set \(T=10000\), \(d=2\), and \(=0.05\). We consider time-varying arm-set: at each \(t[T]\), an arm-set \(_{t}^{d}(1)\) of size \(|_{t}|=20\) is uniformly sampled. We set \(_{}=}\) for \(S\{4,6,8,10\}\). Lastly, we consider \(10\) independent repeats per setting for statistical significance.

Results and Discussions.The results are shown in Figure 1. Note that in all considered settings, OFUGLB, EMK, and EVILL outperform every other baseline, both in terms of regret and numerical tightness of the CS. Moreover, for \(S\{8,10\}\), our OFUGLB seems to achieve the best performance, although more comprehensive numerical studies would shed more light on this matter. As for our OFUGLB-e, despite having worse performance than OFUGLB, EMK, and EVILL, it always attains better numerical performance than the remaining algorithms. Notably, OFUGLB and EMK achieve at par or better numerical regret compared to EVILL _with heuristic hyperparameter_. This highlights the effectiveness of our theoretical results even compared to heuristically tuned randomized exploration.

One interesting observation is that even though OFULog+ achieves a much tighter CS at the end, its regret is much worse than OFULB-e. We posit that this is related to the interplay between the CS and arm set geometries, and we leave further study of such discrepancy to future work. Another is that RS-GLinCB with the exact theoretical hyperparameters (Sawarni et al., 2024) performs the worst, even worse than ada-OFU-ECOLog. We believe this is because their theoretical hyperparameters are not numerically tight, forcing the algorithm to explore throughout the entire duration, probably as their Switching Criterion I (line 4 of their Algorithm 2) is always true. To use RS-GLinCB in practice, one must tune7 the hyperparameters to _explicitly_ control the degree of exploration, which is not the case for our OFULB, making ours a viable, practical algorithm with a _provable guarantee_ as well. Lastly, note how the likelihood-based CS, \(\{:_{t}() c_{t}\}\) for \(c_{t}=( t)\), resembles an ellipsoid. This is because the "normalized" sublevel value \(c_{t}/t\) (as there are \(t\) summands in the LHS) gets smaller, making the second-order Taylor expansion more accurate.

## 6 Conclusion

This paper introduces a novel and _unified_ likelihood ratio-based CS for generic (convex) GLMs, encompassing widely-used models such as Gaussian, Bernoulli, and Poisson. Our CS is equipped with exact constants for various scenarios, making it suitable for any practitioner. The proof involves leveraging key techniques from PAC-Bayes bounds with a uniform prior/posterior. We then propose OFULB, a generic UCB algorithm applicable to _any_**GLBs**, achieving state-of-the-art regret bounds across all self-concordant **GLBs**. The proof involves novel regret decomposition and maximally avoiding the self-concordance control lemma (Faury et al., 2020, Lemma 9), which may be of independent interest. Notably, for logistic bandits, OFUGLB is the first pure-optimism-based algorithm that achieves \((S)\)-free leading term in the theoretical regret, which is numerically verified to perform best. This work opens up various future directions, which we discuss in detail in Appendix B.

Figure 1: Time-varying arm-sets. (First row) Regret plots of all considered algorithms. (Second row) Magnified regret plots. (Third row) Confidence set plots at the final time \(t=10000\) when applicable. Each column represents a different logistic bandit instance for \(S\{4,6,8,10\}\).