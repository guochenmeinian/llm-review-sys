# Permutree Process

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper presents a Bayesian nonparametric (BNP) method based on an innovative mathematical concept of the _permutree_, which has recently been introduced in the field of combinatorics. Conventionally, combinatorial structures such as permutations, trees, partitions and binary sequences have frequently appeared as building blocks of BNP models, and these models have been independently researched and developed. However, in practical situations, there are many complicated problems that require master craftsmanship to combine these individual models into a single giant model. Therefore, a framework for modelling such complex issues in a unified manner has continued to be demanded. With this motivation, this paper focuses for the first time in the context of machine learning on a tool called the permutree. It encompasses permutations, trees, partitions, and binary sequences as its special cases, while also allowing for interpolations between them. We exploit the fact that permutrees have a one-to-one correspondence with special permutations to propose a stochastic process on permutrees, and further propose a data modeling strategy. As a significant application, we demonstrate the potential for phylogenetic analysis, which involve coalescence, recombination, multiple ancestors, and mutation.

## 1 Introduction

**Various combinatorial structures** - _Permutations_, _trees_, _partitions_, and _binary sequences_ have been frequently utilized in Bayesian modeling, and conventionally, various models have been studied separately for each subject. _Permutations_ have been used in a wide range of applications such as Bayesian ranking , matrix reordering , and the traveling salesman problem . Various random permutation models, such as the Mallows model , the permuton models  and the modified Chinese restaurant process , have been employed in Bayesian modeling. _Trees_ are typically used for hierarchical clustering  and multiple resolution regression . In the Bayesian literature, the Dirichlet diffusion tree , the Mondrian process  and the Polya tree  are particularly popular models. _Partitions and binary sequences_ are fundamental tools in machine learning, with numerous examples of their usage in clustering, factor analysis, feature selection, and more. For the modeling of partitions and binary sequences, the Dirichlet process mixture model , the Pitman-Yor process mixture model , the Chinese restaurant process , and the stick-breaking process  for random partitions, and the beta-Bernoulli process and the Indian buffet process  for random binary sequences have frequently been employed.

**Combination of different combinatorial structures** - In real-world applications of machine learning, it is often a useful strategy to combine several different combinatorial structures to model data, rather than using only one combinatorial structure. For example, the combination of partitioning and factor models is particularly popular, including the infinite factorial hidden Markov models , the subset infinite relational model , the infinite latent factor model with the infinite mixture model  and the kernel beta process . As another example, the combination of tree structuresand partitioning have also been actively studied, including the hierarchical Dirichlet process , the nested Dirichlet process [84; 64], their hybrid models [2; 71; 50], the infinite context-free grammar  and the tree-structured stick-breaking process [1; 65]. Furthermore, permutations are occasionally employed in conjunction with clustering to analyze relational data . As we have discussed so far, this kind of strategy of combining multiple models into a single model is one promising direction for research and development. However, advancing research in this direction necessitates the evaluation of an enormous number of models in a combinatorial fashion, which becomes infeasible due to the exponentially increasing number of potential combinations. Consequently, we are striving to initiate a paradigm shift towards exploring an entirely new approach capable of unifying these models.

**Key insight** - In our pursuit of creating a unified model capable of encompassing permutations, trees, partitions, and binary sequences, we are incorporating the concept of _permutrees_, which has recently emerged in the field of combinatorics, into the realm of Bayesian nonparametric (BNP) machine learning. Permutrees not only serve as a framework that includes permutations, trees, partitions, and binary sequences as distinct cases but also exhibit intriguing properties of interpolation between them. Figure 1 (a)-(d) provides a concise visual representation of the key characteristics.

**Our contributions** - The main contribution of this paper is to produce, by using the concept of permutrees, a stochastic process that can represent combinatorial structures such as permutations, binary trees, partitions and binary sequences in a unified manner for the first time. Section 3 exploits the one-to-one correspondence between permutations and certain permutations using a two-dimensional marked point process to construct this process, which we call a _permutree process_. Section 4 derives a data modelling strategy using this stochastic process by analogy with the stick-breaking process that is frequently used in BNP machine learning. Section 5 demonstrates the application of phylogenetic analysis of DNA sequence data dealing with multiple biological events such as coalescence, recombination, mutation and multiple ancestry in a unified manner.

## 2 Preliminaries: Permutree and related objects

**Permutree** - A _permutree_ is a new mathematical tool invented recently in the field of combinatorics, which not only represent permutations, trees, partitions, and binary sequences as special cases, but can also interpolate between them . Let us begin with the definition of a permutree. We consider a directed tree \(\) with a vertex set \(\) of \(n\) (\(n\)) vertices of degree at least \(2\), and a set of terminal nodes of degree \(1\) (See also Figure 1 (a)). For technical reasons (discussed immediately below), we dare to pay particular and explicit attention here to the set \(\) of the "interior vertices" (i.e., vertices of degree at least \(2\)) other than the terminal nodes. Each vertex \(\) is assigned a natural number \(p()\) as a label, using the bijective vertex labeling (one-to-one correspondence) \(p:[n]:=\{1,2,,n\}\) based on the following _permutree requirements_ (Definition 1 in ):

1. Each vertex \(\) has one or two parents, and one or two children.
2. Each vertex \(\) has one or two parents, and one or two children.
3. Each vertex \(\) has one or two parents, and one or two children.

Figure 1: Overview of new combinatorial structures invented in . **Left**: A permutree that is a combinatorial object that includes the concepts of permutations, binary trees, clusters, factors, etc., but can also interpolate between them. The permutree is, as defined, a “directed” tree, but for visibility, the direction of the edges from Parent to Child is omitted in the diagrams. **Middle**: Variant concepts required to represent stochastic processes on permutrees indirectly through _decorated permutations_ in Section 3. **Right**: Special cases of permutrees. Remark 2.1 provides details on each interpretation.

* If a vertex \(\) has a left parent (or child), then all labels in the subtree of the left ancestor (or descendant) of \(\) are smaller than \(p()\). If \(\) has a right parent (or child), then all labels in the subtree of the right ancestor (or descendant) of \(\) are greater than \(p()\).

A directed tree \(\) that satisfies the above requirements can be expressed more intuitively and clearly by introducing the notion of _decorations_ to the vertices \(\). See also Figure 1 (e). We introduce the \(n\)-tuple decorations \(():=(()_{1},,()_{n}) \{,,,\}^{n}\), defined as follows: (i) \(()_{p()}=\) if \(\) has one parent and one child, (ii) \(()_{p()}=\) if \(\) has two parents and two children, (iii) \(()_{p()}=\) if \(\) has one parent (lower in Figure 1 (e)) and two children (upper), and (iv) \(()_{p()}=\) if \(\) has two parents (lower) and one child (upper). The symbolic feature of permutrees can represent various combination objects in a unified manner as follows:

**Remark 2.1**.: _(See Example 4 in .) **Permutation** - Permutrees with decoration \(^{n}\) have a one-to-one correspondence with permutations of \([n]\). For example, by reading the horizontal labels in the order of the natural number of vertical labels, Figure 1 (i) represents a permutation \(\). **Binary tree** - Permutrees with decoration \(^{n}\) have a one-to-one correspondence with rooted planar binary trees on \(n\) vertices. See Figure 1 (j) for an example. **Cambrian tree** - Permutrees with decoration \(\{,\}^{n}\) are exactly the Cambrian trees proposed in . See Figure 1 (k) for an example. **Binary sequence** - Permutrees with decoration \(^{n}\) have a one-to-one correspondence with binary sequences with length \(n-1\). The \(i\)th element of the binary sequence is determined according to the following procedure: for any \(i[n-1]\), there exists \(p()=i\) and \(p()=i+1\), and if \(\) is the parent of \(\), output \(1\), otherwise output \(0\). See Figure 1 (l) for \(\) as an example._

Now that we have summarized the important property of permutrees, we will describe the findings necessary to construct a stochastic process on a permutree, which is the main focus of this paper. As a motivation for describing the following findings, imagine actually drawing an instance of permutree on a hand-drawn blackboard. At this point, we notice that the _horizontal_ positional relationship of vertices \(\) is explicitly given by the natural number label \(p()\), however, the _vertical_ positional relationship is still ambiguous (In Figure 1, (f) is identical to (g) in terms of the permutree, but distinct in terms of the increasing tree). Hence, in order to construct a stochastic process on permutrees in a concise and clear manner, a mechanism to control the vertical positioning of the vertices of the permutrees is required. With this motivation in mind, we introduce two useful notions, an _increasing tree_ (Figure 1 (g)) and a _leveled permuttree_ (Figure 1 (h)).

**Leveled permuttree** - To define the leveled permuttree, we start by introducing an additional notion of an _increasing tree_. We consider a directed tree \(\) with vertex set \(\). Each vertex \(\) is assigned a natural number label \(q()\), using the bijective vertex labeling (one-to-one correspondence) \(q:[n]\) such that, if \(\) is the parent of \(\), then \(q()<q()\) is satisfied. Intuitively, the function \(q\) serves to label the vertices \(\) from \(1\) to \(n\) vertically from bottom to top (Figure 1 (g)). Then, a _leveled permutree_ is a directed tree \(\) with a vertex set \(\) endowed with two bijective vertex labelings \(p,q:[n]\) which respectively define a permutree and an increasing tree. By using two types of labels \(p\) and \(q\), the horizontal and vertical arrangement of the vertices \(\) can be explicitly specified, as shown in Figure 1 (h). The leveled permutree is a useful tool when considering the generative model of the permutree in Section 3, because its specification is clear.

The notion of a leveled permutree so far has improved the prospects for dealing with permutrees. However, leveled permutrees are still combinatorial and geometric, and are not yet easy to handle computationally (in terms of Bayesian modeling, which is the main objective of this paper). Finally, we would like to wrap up this section by revealing one of the most important aspects of leveled permutrees: their relationship to _decorated permutations_.

**Decorated permutation** - For the description of decorated permutations, the notion of a _permutation table_ should be prepared first. A permutation table is a geometrical representation of a permutation \(\) with \(n\) length by the \((n n)\)-table, with rows labeled by positions from bottom to top and columns labeled by values from left to right, and with a dot at column \(i\) and row \((i)\) for all \(i[n]\). Figure 2 (left) shows an example for a permutation \(\). Now that we are ready, we move on to the description of a decorated permutation. A decorated permutation is a permutation table where each dot is decorated by \(\), \(\), \(\), or \(\). Figure 2 (left bottom) shows an illustration of a decorated permutation. One of the important properties of decorated permutations is shown below.

**Proposition 2.2**.: _(See Proposition 8 in .) There exists one-to-one correspondence between decorated permutations with decorations \(\{,,,\}^{n}\) and leveled permutrees with \(()=\)._Now that we have reviewed the permutree findings, the next and subsequent sections will address three challenges: (i) How can we construct a stochastic process that can represent any permutree (in Section 3)? (ii) How can we construct a BNP prior model of the data using the stochastic process on the permutree (in Section 4)? (iii) What likelihood models can we combine the BNP prior with in actual machine learning applications (in Section 5)?

## 3 Permutree processes

The goal of this section is to construct a stochastic process that can represent any permutree; ideally, as is the basic philosophy of BNP, that stochastic process should also be able to simultaneously represent randomness with respect to complexity (in the context of permutrees, the number of vertices). In fact, our construction below can represent every permutree with an unlimited number of finite or infinite number of vertices in a unified manner, depending on certain hyperparameters. One thing to note in advance is that the stochastic process described in this section does not refer to any modeling of data. We will discuss data modeling in more detail in the next Section 4.

**Key insight** - Our strategy is to use point processes. Recall that, as discussed in Section 2, permutrees can be represented through leveled permutrees (surjection), and furthermore, leveled permutrees have a one-to-one correspondence (bijection) with decorated permutations (Proposition 2.2). Thanks to these facts, instead of dealing directly with permutrees (seemingly difficult to handle), we can obtain a model of permutrees indirectly by considering a model of decorated permutations. So how can we model decorated permutations? We represent the random decorated permutations as a _marked point process_ by considering random permutations as a _point process_ and random decorations as _marks_.

**Marked point process for decorated permutations** - We consider a marked point process consisting of a point process and associated marks, which can be expressed as \(\{(_{i},m_{i}):i=1,2,\}\), where \(_{1},_{2},\) are locations and \(m_{1},m_{2},\) are associated marks. Specifically, we employ the following Poisson process on a \(2\)-dimensional plane \(\) with discrete marks (Figure 2 right):

* We draw the random locations \(_{1},_{2},\) from a Poisson point process on the plane \(\) with the intensity function \(:^{+}\), where \(^{+}=\{r:r>0,r\}\). Although not essential, for the sake of simplicity, we use a _homogeneous_ Poisson

Figure 2: **Left: One-to-one correspondence between decorated permutations and leveled permutrees. Right: Permutree process as marked point process - We introduce an intensity function \(\) on the plane \(\) (top left). Next, we generate random locations \(_{1},,_{n}\) from the Poisson point process with intensity \(\) (middle left). Then, for each random location, we independently assign one of the decorations \(\{,,,,\}\) from the categorical distribution as a random mark \(m_{i}\) (\(i=1,,n\)) (bottom left). By reading the positional relationship of the points as a permutation table, the resulting marked point \(\{(_{i},m_{i}):i=1,2, n\}\) can be converted to a _decorated permutation_. Furthermore, by the transformation used in Proposition 2.2 , the decorated permutation can be converted to a leveled permutree, as follows. First, we draw auxiliary lines (dashed lines colored red) below decorations \(\), \(\) and above decorations \(\), \(\). From this point on, we will stretch the permutree edges, and it is important to emphasize that the permutree edges do not cross these auxiliary lines. Next, focusing on the auxiliary lines extending to the bottom, we can view these as dividing the lower region into smaller subregions (indicated by the red ovals). The edges are then extended one by one from each subregion. As we extend the edges from the bottom to the top, when they reach the height of each vertex, we connect the adjacent edges to that vertex (indicated by the gray box). By doing this until all vertices are covered, we obtain a _leveled permutree_.

point process, that is, \((A)=(A)\) for all measurable subset \(A\) of \(\), where \(()\) indicates the Lebesgue measure, and \(0<<\) is a tunable variable. For convenience, let \(_{i}=(l_{i,1},l_{i,2})\), where \(l_{i,1}\) and \(l_{i,2}\) are the horizontal and vertical positions, respectively.
* We draw the random marks \(m_{1},m_{2},,m_{n}\) independently from a categorical distribution on \(\{,,,\}\): \((c_{},c_{},c_{})\), where \(c_{*} 0\) (\(*\{,,,\}\)) denotes the probability that decoration \(*\) is adopted.

**Transformation to leveled permutree** - The above marked point process can immediately lead to a random leveled permutree with the following procedure. Recall that, as discribed in Section 2, the leveled permutree is defined by (i) the decorations on the vertices \(\) and (ii) the two bijective vertex labelings \(p,q:[n]\). For the decoration of vertices, we consider the point set of the marked point process as the vertex set \(\), and the mark \(m_{i}\) assigned to the \(i\)-th point as the decoration of the \(i\)-th vertex \(_{i}\). Thus, the remainder to be considered is the setting of two functions \(p\) and \(q\). By construction, we can obtain the indices \(a_{1},,a_{n}\) so that the random positions \(_{1},,_{n}\) are in ascending order in the horizontal direction, that is, \(l_{a_{1},1}<l_{a_{2},1}<<l_{a_{n},1}\) (Recall that \(_{i}=(l_{i,1},l_{i,2})\), and \(l_{i,1}\) represents the horizontal position). Similarly, in the vertical direction, we can obtain the indices \(b_{1},,b_{n}\) so that \(l_{b_{1},2}<l_{b_{2},2}<<l_{b_{n},2}\). Now, if we choose to set \(p(_{a_{i}})=i\) and \(q(_{b_{i}})=i\) for \(i=1,2,,n\), then \(p\) and \(q\) satisfy the requirement of bijective functions. By the above, we have seen that indeed the marked point process provides us with what we need to define a leveled permutree, that is, the vertex decorations and two bijective functions \(p\) and \(q\). Finally, Figure 2 (right) show the procedure for explicitly converting a marked point process to a random leveled permutree. Inheriting Proposition 2.2 and the result (with the proof procedure) of [75, Proposition 8], we can confirm that this transformation is well defined (See Appendix E for details).

## 4 Data modeling with permutree process

The purpose of this section is to show how the permutree process described earlier can be used for modeling actual data. More specifically, this consists of the following two issues:

* **How to represent data using permutrees**: As permutrees themselves are simply mathematical objects, we must be clear about how we relate them to data modeling and analysis. In fact, there are many possible ways to describe data by permutrees. We consider the situation where a _data path_ (a lineage to describe the data in conjunction with some likelihood model, such as the evolutionary model in Section 5) from one of the lower terminal nodes to one of the upper terminal nodes on the permutree is assigned to each data (Figure 3 (a), top). For example, if we restrict the permutree to one of its special cases, the binary tree, this data path is attributed to the path from the root to the terminal node, which is a situation commonly used in hierarchical clustering (Figure 3 (a), bottom).1 We show a strategy to represent this random data path using a special variant of the nested Chinese restaurant process . * **How to "implement" a permutree process**: In the previous section, we have shown that a marked point process with an intensity function \(:^{+}\) can be used for the stochastic process on permutrees. On the other hand, another important topic is to clarify how to implement models (or more practically, what intensity function \(\) to use) suitable for data analysis. Our strategy is to use the analogy of the stick-breaking process  to represent the infinite number of marked points generated from the marked point process, which is the entity of the permutree process. This can be viewed as a special case of using beta intensity in the horizontal direction and uniform intensity in the vertical direction as the intensity function \(\) of the permutree process.

Experts in the BNP field might remind themselves that there are many other strategy options for the above topics in the light of the various findings that have emerged in the history of the development of the BNP method over the last \(20\) years. We will, for the sake of space, summarize in Appendix D the various ideas and their respective advantages and disadvantages with respect to those historical findings, including whether it is possible to extend the conventional tool of the "ordered" Chinese restaurant process  for random binary trees to random permutrees. The main body of this paper focuses on the most straighforward strategy.

**Permutree of infinite size** - We first generate random positions \(_{1},_{2},,_{k}=(l_{k,1},l_{k,2}),\) in the point process of the permutree process, as shown in Figure 3 (b), using the stick-breaking process:

\[_{k}(1,),\ \ \ \ \ l_{k,1}=_{i=1}^{k} _{i}_{i^{}=1}^{i-1}(1-_{i^{}})},\ \ \ \ \ l_{k,2}(),\] (1)

where \(>0\) is the concentration parameter. As in the original permutree process, each point mark \(m_{k}\)\((k=1,2,)\) is generated from a categorical distribution: \(m_{k}(c_{},c_{},c_{})\). As mentioned earlier, by the procedure in Figure 2 (right), we can transform this sample (i.e., a set of infinite number of marked points) drawn from the permutree process into a uniquely single permutree.

**Data assignments to bottom terminal nodes** - Next, we can represent data modeling by the _paintbox_ scheme for the random permutree generated from the marked stick-breaking process described earlier. We associate one uniform random variable \(U_{j}\) for each data indexed by \(j=1,2,,N\)\((N)\): \(U_{j}()\). Similar to Kingman's representation to the exchangeable partitions, called _paintbox_ schemes , we choose which terminal node on the lower edge of the permutree to assign the \(j\)th data to, depending on which stick in the stick-breaking process this random variable \(U_{j}\) is located on \(\), as shown in Figure 3 (b).

**Data path modeling** - Finally, we model the path assignment for each data by choosing a path that starts at this assigned lower terminal node and reaches one of the upper terminal nodes through the following _two-table Chinese restaurant process_ (i.e., variant of two-class Dirichlet allocation):

* We break up the set of data flowing in, following the left-right table-assignment operation below2: the first data is chosen uniformly at random from either the left or the right table. For the \(n\)th data, the left table is chosen with probability \((_{}+/2)/(n+)\) and the right table with probability \((_{}+/2)/(n+)\), where \(>0\) is a hyperparameter, and \(_{}\) and \(_{}\) are the number of data allocated so far to the left and right tables respectively. * We merge the sets of data flowing from the two lower branches and feed them into the upper.
* It would be straightforward to perform operations whose marks are \(\) and \(\) together. Another promising option is the representation of data flowing from the left parent to the left child and from the right parent to the right child. This can be interpreted as giving the mark \(\) the ability to _partitioning_. This interpretation also plays an important role in the validity of _finite truncation_, which will be discussed below.

Figure 3: **(a) Data path** - We consider each data as having a data path (a lineage to describe the data in conjunction with some likelihood model) from one of the lower terminal nodes to one of the upper terminal nodes on the permutree (top). This will convince us of its generality and applicability, as it is attributed to the hierarchical clustering from one of the terminal nodes to the root when restricting the permutree to the special case of a binary tree (bottom). **(b) Marked stick breaking process** - Inspired by the stick-breaking representation  for the construction of Dirichlet processes , in order to represent a random permutree of infinite size, we can represent the random vertex positions of the permutree by the stick-breaking process in the horizontal direction and uniform random measures in the vertical direction. **(c) Two-table Chinese restaurant process** (Variant of two-class Dirichlet allocation) - The data allocated to the lower terminal nodes are successively merged and distributed depending on the mark of each vertex, according to the law of the ‘the rich get richer’, to select paths.

- We pass on the whole data set that flows in, all the way to the top.

For notational simplicity, we will denote the random variable for the \(j\)th data path by \(Z_{j}\). For a sample \(z\) of data paths between the upper and lower terminal nodes of the permutree (specified by a sequence of edges), the above generative probabilistic model allows us to evaluate the probability \([Z_{j}=z]\) of the \(j\)th data choosing a data path sample \(z\).

**Property #1: Exchangeability** - Random data paths based on the generative probability model described above have _exchangeability_, an important property common to most BNP models . Simply put, the model probability is invariant to the indexing of the data. As a result, it follows the philosophy of BNP models that even if the actual data to be observed is finite, the model itself, with infinite complexity, can reflect the uncertainty due to unobserved data. More specifically, this can be summarised as the following statement:

**Proposition 4.1** (Exchangeability).: _For any permutation \(\) of length \(N\)\((N)\), we have \([Z_{1}=z_{1},Z_{2}=z_{2},,Z_{N}=z_{N}]=[Z_{(1) }=z_{1},Z_{(2)}=z_{2},,Z_{(N)}=z_{N}]\), where \(z_{j}\)\((j[N])\) is a sample of paths of random permutrees. (See Appendix A.1 for proof.)_

**Property #2: Validity of finite truncation** - The above generative probability model requires in principle an infinite number of random variables for its description, but finite truncation works reasonably well for a finite number of actual observed data. This poses an inherently non-trivial challenge that is not present in the validity of approximating the stick-breaking process  for the Dirichlet process  with a finite number of stick-breaking procedures, which is a typical topic in the past . The reason for this non-triviality is that the substructure of a permutree with infinite size is, in principle, affected by an infinite nummber of all marked vertices. Therefore, restricting the structure of the permutree to only some marked vertices may have a significant impact on the structure of the permutree. However, as the following statement shows, the substructure of the permutree has the good property that it depends only on a subset of marked vertices.

**Proposition 4.2** (Finite truncation).: _In the above generative probability model of data indexed by \(j=1,2,,N\)\((N)\), we consider an event that all random variables \(U_{j}\)\((j=1,,N)\), representing the horizontal position of the \(j\)th data, falls in the range \([0,1-)\) as a situation with a sufficiently high probability \([_{j=1}^{N}0 U_{j}<1-]=_{j=1}^{N} [0 U_{j}<1-]>1-(N)\), where \(>0\) is a tiny real value. In this situation, there exists some natural number \(K<\), and all data paths are assigned with probability \(1\) only to paths on the finite-size random permutree generated from the random marked points \(_{1},_{2},,_{K}\). (See Appendix A.2 for proof.)_

## 5 Application to phylogenetic permutree analysis

This section presents an application example of using the prior model representation of data using permutrees, which has been described in Section 4, in conjunction with a likelihood model in a specific application. One of the most promising applications of permutrees would be phylogenetic tree analysis for DNA molecular sequence data (e.g., CAGTC). DNA sequences from one or more populations are related by a branching structure known as genealogy. The complex correlative structure of a collection of DNA sequences can be represented as a phylogenetic tree, a record of _coalescence_, _recombination_, and _mutation_ events in the history of the target organism: _coalescence_ refers to the event in which two sequences are attributed to a common ancestor, _recombination_ refers to the event in which a lineage splits into two sub-lineages when looking back in time from the present to the past, and _mutation_ refers to the change of each letter of a DNA sequence over time.

**Challenges of conventional methods** - The most standard structure that has been used in phylogenetic analysis is the binary tree . In fact, binary trees are very well suited to represent _coalescence_ events in genealogy. However, one drawback of binary tree models is that they are not suitable for representing _recombination_ events in a way that is compatible with coalescence events. To circumvent this drawback, the ancestral recombination graphs (ARGs) have sometimes been used as models that can represent both coalescence and recombination at the same time . However, it is not easy to model or infer ARGs directly, and often indirect ways of representing models by other perspectives (e.g., the fragmentation-coagulation process ) have been explored, or approximate models (e.g., the coalescent hidden Markov model  and the sequentially Markov coalescent model ) have been considered. Moreover, conventional phylogenetic tree analysis, including not only ARGs but also binary tree models,generally imposes a strong assumption that observed DNA sequences or observed taxa have a single ancestor. In other words, this implies that the inferred phylogenetic tree should be a strongly connected graph. Needless to say, such an assumption is reasonable for taxa that have been carefully selected by biologists. On the other hand, when we want to use a large number of taxa that are too large to be selected by experts as observation data (i.e., the situation that BNP methods are really aiming for), a mechanism that allows multiple ancestors to be inferred in a data-driven manner will be very useful. In light of the above, phylogenetic tree analysis requires a model that can represent coalescence, recombination, multiple ancestors, and mutation in a unified manner.

**Phylogenetic permutree** - As input observation data, we used DNA (molecular) sequences observed at letter length \(S\) over \(N\) species. For example, the sequence \(\) (i.e., \(N=1\) species) has length \(S=6\). We regard these DNA sequences as following a _phylogenetic permutree_. Specifically, we represent coalescence, recombination, multiple ancestry, and mutation events in genealogy by combining the four types of the decorations \(,,,\) with the following interpretations. We note that, to be consistent with the traditional notation of phylogenetic tree analysis, the past (upper) to present (lower) direction as biological events is the opposite of the parent (lower) to child (upper) direction of the permutree as a purely mathematical object that we have used in the diagrams so far.

* A coalescence event represents two lineages (bottom side of Figure 4 (b)) having a common ancestral lineage (top side).
* A recombination event represents the joining of two exclusive subsequences of two lineages (top side of Figure 4 (c)) by one lineage (bottom).
* We give the decoration \(\) the role of division so that a single permutree can represent a phylogenetic tree with multiple ancestors. Specifically, as shown in Figure 4 (d), we connect the two left edges and connect the two right edges resulting in two tree structures unconnected to each other on either side of decoration \(\).
* We assume that no mutation occurs while going back in time from a vertex to a vertex with \(\) (Figure 4 (e)). This allows us to set the mutation rate in the evolutionary model as a single parameter common to all branches, and the mutation rate can be adjusted according to the permutree itself.

**Evolutionary models on permutrees** - Statistical models of gene mutation have a history of more than half a century, and a vast number of models have been proposed. An excellent recent review article can be found, for example, in . For simplicity, we adopt two of the most popular models, the Jukes-Cantor model (JC)  and the generalized time reversible model (GTR) , for DNA sequences (i.e., words with \(\), \(\), \(\), and \(\) as letters of the alphabet \(\{\),\(\),\(\),\(\}\), such as \(\)). JC is defined as a Markov process in which (1) all letters are independently generated from a uniform categorical distribution on \(\{\),\(\),\(\),\(\}\) as initialization and (2) one letter (e.g., \(\)) changes to another letter (e.g., \(\)) after \(t\) seconds with probability \((1-(-4 t))/4\) or does not mutate with probability \((1+3(-4 t))/4\), where \(\) (\(>0\)) is a hyperparameter representing the mutation rate. Simply put, JC means that the transition probabilities of letters in mutation are fixed. GTR, on the other hand, can be regarded as a more flexible version of the JC model, in which the letter transition probabilities themselves are also estimated from the data as hidden parameters.

Figure 4: **(a) Phylogenetic permutree** can simultaneously and unifiedly represent **(b) coalescence**, **(c) recombination**, multiple ancestry through **(d) partition**, and **(e) mutation**. We note that the past (upper) to present (lower) direction (indicated by \(\)) as biological events is the opposite of the parent (lower) to child (upper) direction (indicated by \(\)) of the permutree as a purely mathematical object.

**Demonstration** - We use the following three benchmark datasets  for DNA sequences: RMPS (\(N=64\) species, \(S=1008\) length) , HWB (\(N=41\), \(S=1137\)) , and ZB (\(N=50\), \(S=1133\)) . In addition, to establish a situation where the permutree notion would be useful (i.e., multiple ancestry derived from exclusive disconnected graphs), we extract the sequences of these datasets by \(S=1000\) length from the beginning and mix them to create a dataset we call COMB (\(N=155\)). We use the marked stick-breaking process (referred to as MSBP; Section 4) as our proposed model. Since MSBP can easily adjust the representational capabilities of of its own model, as ablation studies, we use MSBP-bTree as the one restricted to binary trees (with the prior \((c_{},c_{},c_{},c_{})( /2,0,0,/2)\)), MSBP-cTree as the one restricted to Cambrian trees (with Dirichlet\((/3,0,/3,/3)\), and MSBP-pTree as the main proposal permutrees (with Dirichlet\((/4,/4,/4)\), where we set \(=0.01\). For the evolutionary model, we employ the mutation rate \((^{},^{})\), where \(^{}=0.1\). We only present the case of \(K=100\) as the truncation level here, while we report the other cases in Appendix C. We compare these models to the hierarchical Dirichlet process hidden Markov model (HDPHMM) [8; 94; 5], the fragmentation-coagulation process (FCP) , and the binary tree model with the MrBayes [38; 46], the probabilistic path Hamilton Monte Carlo (ppHMC) , and the nested combinatorial sequential Monte Carlo (ncSMC) . It is noted that HDPHMM and FCP do not use evolutionary models because they represent sequence data directly without tree structure. We held out \(20\%\) letters of the input sequences for testing, and each model was trained using the remaining \(80\%\) of the letters. Each inference method uses MCMC to estimate the posterior distribution by the following \(100\) samples: each method extracts \(5\) MCMC runs with different random numbers, and each MCMC run is sampled every \(50\) iterations after \(2000\) burn-in until \(3000\) iterations. We evaluate the models using perplexity as a criterion: \(()=(-(\,p())/E)\), where \(E\) is the number of missing letters in the input sequences. Figure 5 shows the comparison of the prediction performance of each method for the four sets of data. As an overall trend, it can be seen that the Cambrian tree and permutree models show better prediction performance than the binary tree model, which has limited expressive power.

## 6 Discussion and limitation

This paper (i) imports the notion of permutrees, recently invented in combinatorics, to Bayesian analysis, (ii) proposes the stochastic process that can represent various models such as permutations, trees, partitions, and factors in a unified manner, (iii) and applies it to phylogenetic permutree analysis.

**Limitations** - While our proposed permutree process can represent various combinatorial structures in a unified "prior model," the likelihood model that describes the data (as we have shown in the context of phylogenetic tree analysis in Section 5, for example) must be prepared separately by the user or engineer. Thus, while the permutree process is a tool that allows data-driven inference of the model structure as a broad framework, the design of the likelihood model needs to be carefully done manually. In the near future, the exploration of representing this likelihood model in some kind of black box function model would be an important research direction.

**Remaining challenge** - In the technical context of the BNP field, an important topic is whether a marginalized representation of the marked stick-breaking process, an infinite-dimensional intermediate random variable in the representation of data paths with exchangeability described in Section 4, can be obtained. This topic is a question closely related to the Aldous-Hoover-Kallenberg representation theorem for exchangeability in general [3; 36; 41]. As a more familiar analogy, it corresponds to the fact that if we marginalise the stick-breaking process representation in a Dirichlet process infinite mixture model, then we obtain the Chinese restaurant process representation. Our strategy and budding attempts on this question are summarized in Appendix D.

Figure 5: Experimental results of test perplexity (mean\(\)std) comparison for real-world data.