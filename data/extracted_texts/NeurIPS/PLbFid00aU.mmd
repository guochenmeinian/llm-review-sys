# The Impact of Geometric Complexity on Neural Collapse in Transfer Learning

Michael Munn

Google Research

munn@google.com

&Benoit Dherin

Google Research

dherin@google.com

&Javier Gonzalvo

Google Research

xavigonzalvo@google.com

Equal contribution.

###### Abstract

Many of the recent remarkable advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models. However, a theoretical framework which explains this empirical success is incomplete and remains an active area of research. Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training. In this paper, we explore the geometric complexity of a model's learned representations as a fundamental mechanism that relates these two concepts. We show through experiments and theory that mechanisms which affect the geometric complexity of the pre-trained network also influence the neural collapse. Furthermore, we show how this effect of the geometric complexity generalizes to the neural collapse of new classes as well, thus encouraging better performance on downstream tasks, particularly in the few-shot setting.

## 1 Introduction

Many of the recent remarkable advances in modern machine learning owe their success in part to transfer learning . While there are different approaches to this technique, the standard one involves two stages. In a first stage, called pre-training, ones trains a deep neural network on a general, large-scale dataset in the form of a supervised or unsupervised source task; e.g., ImageNet or CIFAR-100  for image models or the Common Crawl, C4 or LM1B datasets  for language models. In the second stage, one then leverages portions of the pre-trained network to use as features map or embeddings that can then be adapted, or fine-tuned, on a more specific target task. Often these target tasks are unknown at the time of pre-training and labeled data may be very scarce, such as in the context of few-shot learning . However, despite these limitations, this approach often results in a fine-tuned model that achieves quite impressive performance substantially better than training on the target task alone and requiring less computational resources . Despite this empirical success, a comprehensive theoretical understanding of the mechanisms underlying this effectiveness of transfer learning is not fully understood and remains an active area of research .

One interesting line of research suggests that the effectiveness of transfer learning is due to the implicit biases encoded in pre-trained models . These implicit biases effectively constrain the hypothesis space, guiding the model towards solutions that have a preference for smoother functions , simpler geometry in the loss surface  or reduced complexity of the internal learned representations . In the same way that these implicit biases have been used to help explain the success of deep learning, recent work has shown that the notion of neural collapse  and flatness of the loss surface  can also inform the mechanisms behindtransfer learning. This suggests that these preferences during pre-training also act as a form of prior knowledge which is highly transferable to downstream tasks, even with limited task-specific data.

In this paper, we present a novel perspective that further sheds light on these mechanisms and implicit biases hidden within transfer learning. Our approach analyzes the geometric complexity [16; 17] of the internal representations learned by the deep neural networks during pre-training and provides a complementary theoretical framework which unifies previous work examining the role of neural collapse and loss surface flatness in transfer learning [22; 36]. In particular, we show through experiments and theory that the geometric complexity of the pre-trained network directly controls the neural collapse of the pre-trained model and thus its efficacy in transfer learning, particularly in the few-shot setting. We argue that geometric complexity (similarly to flatness of the loss surface and neural collapse) can be used as hidden progress metrics, cf. , for transfer learning, serving as an informative proxy toward the transfer power of a pre-trained network.

Our primary contributions are the following:

* We uncover relationships between learning-path flatness, neural network geometric-complexity, and embedding neural-collapse providing a framework to understand how these implicit biases interact (Section 4).
* We show through theory that the geometric complexity (GC) of a neural network controls neural collapse and verify this empirically by showing how mechanisms which regularize the GC in turn put pressure on the neural collapse (Section 4.1 and Fig. 1).
* We demonstrate both theoretically and empirically that pre-trained networks with lower GC promote lower neural collapse on new unseen target classes, and thus enable improved transfer accuracy during fine-tuning (Section 5 and Fig. 4).
* We prove a new generalization bound in terms of geometric complexity (Section 4.3).
* We show that the empirical GC can be accurately and efficiently estimated on a small number of samples, input coordinates, and output features making it computationally tractable compared to other progress metrics in machine learning (Section 4.2 and Fig. 2).

Notation.Throughout, we denote by \(\|\|\) the L2 Euclidean norm and by \(\|\|_{F}\) the Frobenius norm.

## 2 Background and Related Work

The implicit biases introduced by an optimization algorithm play a crucial role in deep learning and in the generalization ability of the learned models [43; 44; 55; 64]. They help ensure that the model not only finds a solution with low error but also one with low complexity which generalizes well [26; 67]. Uncovering the mechanisms of these implicit regularizers is crucial for understanding how the model learns, both as a means to improve generalization and as valuable leverage for designing more efficient algorithms and decreasing costly experiment iteration cycles.

Here we focus on three seemingly different themes behind implicit regularization in deep learning and explain how they are related: the loss surface slope measuring the flatness of the learning path [3; 29; 45], the geometric complexity of the learned model function measuring its variability with respect to a dataset [16; 42], and the neural collapse [32; 46] measuring how a neural network efficiently clusters its learned representations of the input class examples in embedding space.

Flatness in parameter space.In parameter space, the learning dynamics is fully characterized by the discrete optimization path \(_{t}\) where \(t\). At each step, one can compute the flatness of the learning path as the slope of the tangent space at \(_{t}\) to the loss surface \(=\{(,L()):\;^{n}\}\). As shown in [3; Appendix A.2], this slope coincides with the loss-gradient square norm: \((_{t})=\| L(_{t})\|^{2}\). This quantity has been used to bound a generalization gap in  and as a beneficial explicit regularizer in [3; 24; 58] indicating that learning curves with lower slope values tend to also have better test performance.

Moreover, many standard optimizers and common training heuristics have been shown to put an implicit pressure on the learning-path flatness, making it an implicit bias of the training procedure [3; 5; 10; 13; 17; 25; 39; 58]. Related to the learning curve slope is the optima sharpness

\[(_{*})=H(_ {*})\]where \(_{*}\) is a global minima toward which the learning dynamics converges to \(_{t}_{*}\) and \(H\) is the Hessian of the loss. Since the sharpness of an optima corresponds to the mean curvature of the loss surface at that point, flat minima (i.e., minima with low curvature in all directions) can be reached only through learning paths with shallower slopes.

Neural collapse in embedding space.Neural collapse  refers to a phenomenon observed in deep learning where the embedding network (i.e., the subnetwork \(f\) before the logit layer \(g\) in a neural network \(h(x)=(g(f(x)))\) collapses the input points around their respective class means. Furthermore, these class means form a simplex creating an equiangular tight frame (ETF) centered around the global mean with roughly equal distance between its vertices. Intuitively, such a phenomenon is beneficial to generalization since the embeddings of different classes are optimally separated, making the job of the classifier head \((g(z))\) easier. To measure neural collapse, the authors in  introduce the _class-distance normalized variance_ (\(\)) as

\[V_{f}(Q_{i},Q_{j}):=_{f}(Q_{i})+_{f} (Q_{j})}{2\|_{f}(Q_{i})-_{f}(Q_{j})\|^{2}},\] (1)

where \(Q_{r}=q_{r}(x)dx\) is the input distribution for classes \(r\{i,j\}\) and where

\[_{f}(Q_{r})=_{x Q_{r}}[f(x)] {Var}_{f}(Q_{r})=\|_{f}(Q_{r})-f(x)\|^{2}q_{r}(x)dx.\] (2)

Neural collapse between two classes happens when their class variance decreases while the distance between their class means increases, causing lower values of their CDNV. Following , given a well-balanced input distribution \(Q=(Q_{1}++Q_{k})\) with \(k\) classes (\(Q_{i}\) being the input distribution for class \(i\)), _neural collapse_ (NC) during training is characterized by the following limit as the number of training steps \(t\) goes to infinity:

\[_{t}(f_{t},Q)=0 (f,Q):=_{i j}(V_{f}(Q_{i},Q_{j}) ),\] (3)

and where \(f_{t}\) is the learned embedding network at step \(t\). Thus, more neural collapse (i.e., more clustering around better separated class-means) happens with lower \(\) values. Proposition 5 from  proves that \((f,Q)\) bounds a generalization gap and lower values of \((Q,f)\) are correlated with better test performance of the neural network \(f\), and  shows that explicitly regularizing for neural collapse is beneficial.

Geometric complexity in function space:The _geometric complexity_ (GC) of a function \(f:^{d}^{k}\) w.r.t. a data distribution \(Q=q(x)dx\) on \(^{d}\) is defined as the expectation of the gradient Frobenius square-norm w.r.t. to the data distribution  given by

\[(f,Q):=_{x Q}[\|_{x}f(x)\|_{F}^{2} ].\]

Intuitively, the GC measures the function complexity or variability and is closely related to the Dirichlet energy in geometric analysis . Provided mild Lipschitz smoothness assumptions [42, Proposition 3.4], the GC can be estimated accurately on batches of training data \(D\) by its empirical counterpart

\[}(f,D)=_{x D}\|_{x}f(x)\| _{F}^{2},.\] (4)

Previous work has explored the relationship between the GC and model generalization. When measured on the full neural network, lower GC values correlate experimentally with higher test accuracy  and it has also been used as a beneficial explicit regularizer . In , the authors ignore the softmax activation and study the GC measured with respect to the logit network, exploring its connection with various implicit and explicit regularizers and training heuristics. The logit GC has also be used to prove a margin based multi-class generalization bound , assuming that the input distribution \(Q\) satisfies a mild assumption known as the Poincare inequality .

In fact, both this work and the proof of the generalization bound rely on the Poincare inequality, which we argue is indeed a natural and mild assumption for the types of data distributions typically encountered in machine learning; see Appendix A.6 for further discussion. For completeness, we recall it here, cf. . A distribution \(Q\) satisfies a Poincare inequality if, for all differentiable functions \(v\) defined on \((Q)\), there exists a constant \(c\) such that

\[_{v}(Q) c_{x Q}[\|_{x}v\|_{F} ^{2}]=c(v,Q).\] (5)Note that the same assumption is used in  to prove the law of robustness for overparameterized neural networks via isoperimetric inequalities. In this paper, we peel back yet another layer of the network and consider the embedding geometric complexity measured on a feature map (Section 3).

Relationship between flatness, neural collapse, and geometric complexity.The learning-path flatness as measured by its slope influences the geometric complexity of the learned solutions [16, Theorem 5.1]. Namely, for dense layers a regularizing pressure on the slope of the learning path in parameter space transfers to a regularizing pressure on the geometric complexity of the learned solution in function space. A similar result also been shown for special attention layers as well .

In this paper, we will see that imposing a regularization pressure on the embedding geometric complexity encourages more neural collapse of the model. This results in the following chain of influences from regularization pressure:

 learning path flatness & \(\) & function geometric complexity & \(\) & embedding neural collapse \\  & _Regularizing Pressure_ & _Lower CDNV_ \\ 

## 3 Problem Formulation

We are interested in understanding the relationship between the geometric complexity (GC) and neural collapse (NC) in the transfer learning setting and explore this relationship in two stages. In the first stage, we examine the general impact of the GC on NC during model training/pre-training. Secondly, we examine how this relationship provides insight into the mechanisms behind transfer learning and the advantageous implicit biases of the pre-training stage. In short, lower GC during pre-training on source classes leads to lower NC for target classes and improved transfer accuracy.

For the first stage of our inquiry (Section 4), we are concerned with a \(k\)-classification task. Let \(D:=\{(x_{i},y_{i})\}_{i=1}^{m}\) be a dataset drawn from a distribution \(Q\) with \(x_{i}^{d}\) and \(y_{i}\{e_{j} j=1,,k\}\) where \(e_{j}^{k}\) are the canonical basis vectors representing a one-hot encoding of the labels. We aim to learn this task using a neural network denoted by a function \(h_{}:^{d}^{k}\) parameterized by \(\) (though to simplify notation we subsequently drop this dependence on \(\)).

We can write \(h_{}(x)=(g(f(x))\) where \(g:^{p}^{k}\) is a classifier head mapping from the feature space \(^{p}\) to the prediction layer \(^{k}\) and \(f:^{d}^{p}\) is a feature mapping from the input space to the penultimate embedding layer of the network before the classification head. The standard way of training \(h_{}\) is to find, via some stochastic optimization technique, an optimal parameter configuration \(_{*}\) such that \(_{*}=_{}_{i=1}^{m}(h_{}(x_{i} ),y_{i})\) for a given loss function \(:^{k}^{k}[0,)\).

To analyze the role of neural collapse and geometric complexity in this setting, we focus our attention on the feature map \(f:^{d}^{p}\). The NC as described in Section 2 is defined using this sub-network map \(f\) of \(h\) and thus we also measure the geometric complexity with respect to this sub-network feature mapping as well. Throughout this paper, we use the term _embedding GC_ to refer to the GC of such a feature map \(f\) and unless otherwise specified, when we refer to the model GC we mean the embedding GC, not the GC of the logit network as in .

Our second stage of inquiry (Section 5) is focused on the role of geometric complexity in transfer learning. For this setting, we assume there is some \(l\)-class classification target task we would like to solve and corresponding target distribution \(\). We aim to learn a classifier \(h^{}:^{d}^{l}\) given some training data \(D^{}:=\{(x^{}_{i},y^{}_{i})\}_{i=1}^{m^{}}\) which is potentially very limited in size. In order to find a good solution, we can leverage a pre-trained feature mapping that has been trained on some other source task and source distribution \(\) where more data is available. For example, by leveraging a feature map \(f\) pre-trained as in the above setup, the target task classifier \(h^{}\) can instead be trained on the outputs of our feature map; i.e., \(\{f(x^{}_{i}),y^{}_{i})\}_{i=1}^{m^{}}\). In this way, we can write \(h^{}=(g^{} f)\) where the parameters of \(f\) are fixed and we only need to learn the parameters of \(g^{}:^{p}^{l}\). Ideally, provided \(f\) is a rich enough feature map trained on a general enough source distribution dataset, then the task for learning \(g^{}\) is much simpler.

Geometric complexity and neural collapse

In this section, we explore the general relationship between the embedding GC and neural collapse showing that the geometric complexity can be used to bound neural collapse. Next, we see how the geometric complexity can be efficiently and accurately estimated, both as an approximation of the theoretical \(\) as well as through a number of sampling techniques for the empirical \(\) via batch sampling, feature sampling and label sampling. Lastly, following , we derive a new generalization bound for neural networks expressed in terms of the embedding geometric complexity; cf. .

### Geometric complexity controls neural collapse

Previously, it has been shown  that the \(\) can be controlled implicitly through choice of learning rate and batch size, as well as through standard explicit regularization. Practically, this means that these same beneficial tuning strategies which ensure that models have low \(\) should also work to keep the \(\) low as well. The following proposition (which we prove in Appendix A.1) bounds the neural collapse by the geometric complexity provided that the input distribution satisfies a Poincare inequality also assumed in  and .

**Proposition 4.1**.: _Suppose that we have a balanced multi-class input-distribution \(Q\) with \(k\) classes satisfying the Poincare inequality in (5) for some constant \(c\), then the geometric complexity of a network embedding \(f\) bounds its neural collapse as measured by (3); namely, we have the following bound_

\[(f,Q)(f,Q)}{k-1}(_{i j} ^{2}}),\] (6)

_where \(k\) is the number of classes, and \(d_{ij}\) is the distance between the mean of class \(i\) and class \(j\)._

We call the RHS of the bound in Eq. (6), excepting the Poincare constant \(c\), the **geometric collapse**:

\[(f,Q)}{k-1}(_{i j}^{2}}),\] (7)

where \(k\) is the number of classes, and \(d_{ij}=||_{f}(Q_{i})-_{f}(Q_{j})||\) denotes the Euclidean distance between the mean \(_{f}(Q_{i})\) and \(_{f}(Q_{j})\) of class \(i\) and class \(j\) (resp.) in embedding space.

This quantity can be seen as another proxy metric measuring neural collapse. It is made of two main parts decoupling the variances and mean differences present in the neural collapse framework. The variances are consolidated into a single factor through the GC, while the mean differences are averaged across classes in a separate factor. This separation allows the overall intra-class variability to be influenced through the GC term, ensuring sufficient between-class separation through the mechanisms identified in neural collapse.

This bound provides a powerful method to control the overall within-class variability via the L2-norm of the model gradient. As a result, the geometric collapse provides a simplified and more refined approach to managing class separability and variance in deep learning models.

We verify the relationship posed in Eq. (6) through experiments on VGG-13 trained on CIFAR-10 (see Figure 1). Through both implicit and explicit regularizers the pressure on the embedding \(\) translates to a direct pressure on the neural collapse of the model via the geometric collapse. As already observed in , lower levels of GC coincide with higher test accuracy as show in Figure 5 in Appendix A.5 containing the learning curves of that experiment.

In Appendix A.5 we see the same relationship holds across various datasets; e.g., MNIST, Fashion-MNIST, CIFAR-100, and architectures; e.g., ResNet-18, ResNet-50. To avoid possible confounding factors that could arise through batch size and learning rate manipulations, we also directly regularize with the geometric complexity in Appendix A.5.6; the same relations hold in this case too.

### The geometric complexity is a reliable and robust measure

One of the key advantages of the GC as a complexity measure is its computational efficiency.

Under mild regularity constraints on the model function the theoretical geometric complexity of a map can be efficiently estimated by its empirical counterpart, as stated in the proposition below, already proven in  for logit networks (see Appendix A.2 for a proof). Additionally and crucially, we verify that the empirical \(\) is a robust and reliable measure with respect to that data sample.

**Proposition 4.2**.: _Let \(f:^{d}^{p}\) be a map with Lipschitz constant \(L\) and let \(D_{X}\) be a sample of \(m\) elements drawn independently from an input distribution \(Q\). Then, for any \(>0\), we have with probability \(1-/2\) the following bound_

\[(f,Q)}(f,D_{X})+L}{2m}}.\] (8)

For a function \(f\) and data sample \(D_{X}\) as in Proposition 4.2, to measure \(}(f,D_{X})\) requires computing the Frobenius norm of a potentially very large Jacobian matrix, particularly when \(p\) represents the embedding dimension of a feature map. Note that,

\[}(f,D_{X})=_{i=1}^{m}_{j=1}^{p}_{s =1}^{d}((x^{(i)})}{ x_{s}})^{2}.\] (9)

Although the computational complexity of the GC is impervious to increasing complexity of the network architecture, e.g., in terms of parameter count or number of layers, one may run into computation bottlenecks as the sample size \(m\) increases or when increasing the dimensionality \(d\) (resp. \(p\)) of the inputs (resp. outputs). In these scenarios, it is necessary to find efficient means to accurately approximate or sample the Jacobian; cf. .

Figure 1: Controlling the neural collapse via the model geometric complexity for VGG-13 trained on CIFAR-10. Lower embedding \(\) produces lower geometric collapse (Eq. 7) and more neural collapse (i.e., lower \(\)) for **Top row:** increased learning rates, **Middle row:** decreased batch sizes, and **Bottom row:** increased L2 regularization.

We explore the robustness of the GC when measured via samplings along these three axes; i.e., decreasing the number of examples \(m\) in the batch, randomly sampling the full Jacobian matrix which has order \(d p\), and randomly sampling the number of model outputs \(p\). As shown Figure 2, we find that the value of the sampled \(}\) remains stable and consistent to its true value through these fairly simple and naive sampling tricks.

### A new generalization bound with GC through NC

In , the authors derive a margin-based multi-class generalization bound for neural networks which depends on the geometric complexity \((h,Q)\) of the full logit network. In this section, we extend this result. With inspiration from [22, Proposition 5], we show that the geometric complexity measured on the sub-network feature map \((f,Q)\) bounds the classification error of the nearest-mean classifier defined by the feature map \(f\).

As in , given a balanced \(k\)-class classification problem, let \(S=_{c[k]}S_{c}\) denote all class samples and define the nearest-mean classifier by \(h_{f,S}(x):=*{argmin}_{c[k]}\|f(x)-_{f}(S_{c})\|\) given by the feature map \(f\) where \(_{f}(S_{c})\) denotes the sample mean of the set \(S_{c}\) under the map \(f\). Define the generalization error

\[:=_{(x,y) P}[[h_{f,S}(x) y]]\]

where \([h_{f,S}(x) y]\) is the indicator function of the error set \([h_{f,S}(x) y]\) and \(P\) is the full data distribution from which samples \(S\) are drawn.

**Proposition 4.3**.: _Suppose that we have a balanced sample \(S\) from a \(k\)-class input-distribution \((x,y) P\) with \(m_{c}\) samples per class. Assume further that the induced input distribution \(x Q\) satisfies a Poincare inequality as in (5) for some constant \(c\). Then, for any \(>0\), with probability \(1-/2\) we have the following bound for the generalization error_

\[[] 16c(+})(}(f,S)+L}{2m_{c}k}})(_{i j}^{2}}),\] (10)

_where \(p\) is the embedding dimension of the feature map \(f\) and \(d_{ij}=\|_{f}(S_{i})-_{f}(S_{j})\|\). Note, the outer expectation on the left hand side is taken over the samples \(S\) used to produce the classifier \(h_{f,S}\)._

Proof.: This follows immediately from [22, Proposition 5] which gives the bound in terms of the neural collapse instead of the geometric complexity. By using Proposition A.1 we can replace the neural collapse with the geometric complexity and using Proposition 4.2 we then replace the theoretical GC with the empirical GC, yielding the additional term with the Poincare constant. 

In Figure 3, we plot the LHS and RHS of the generalization bound in (10) from Proposition 4.3 for a VGG-13 model trained on CIFAR-10. In this plot, we see that the bound is not vacuous, demonstrating a relatively tight fit. Note that on the RHS, we omitted the term involving the Lipschitz constant \(L\), assuming it is small due to its denominator. Additionally, we estimated a lower bound for the Poincare constant \(c\) by comparing the magnitudes of the RHS and LHS in the inequality (6), based on results in Figure 1. This approximation yielded \(c 1000\) in our setup.

Figure 2: The \(\) computation is robust and consistent to sampling via **Left:** number of examples in the batch, **Middle:** number of elements in the Jacobian, or **Right:** by sampling the embedding dimension of the model. Here the \(\) and subnet \(\) have been computed over 20 trials, plotting the mean and standard deviations for a ResNet-18 model that has been trained to convergence on CIFAR-100. The true value of the \(\) for each setting is indicated by dotted line.

## 5 The impact of geometric complexity on transfer learning

Many implicit regularization mechanisms in gradient descent exert a pressure on geometric complexity which in turn constrain the neural collapse. In this section, we discuss the affect of this implicit bias in transfer learning. Specifically, we show with theory how this bias during pre-training can help explain the mechanisms behind transfer learning. Namely, that lower GC in a pre-trained embedding network promotes neural collapse on new target classes, simplifying the fine-tuning process.

However, our bound makes it clear that certain compatibility conditions between the pre-training (source) distribution and the fine-tuning (target) distribution are needed, which we argue are satisfied in image foundational models and large language models. At last, we verify empirically that known methods to reduce geometric complexity for the pre-trained embedding result in better performance during fine-tuning on unseen tasks, in agreement with our neural collapse bound for transfer learning.

### Lower pre-trained GC leads to improved fine-tuning

To analyze the impact of GC in the context of transfer learning, we consider the same formal setup as in (22, Proposition 2). Assume a class \(c\) in a set \(\) of classes is represented by a class-conditional distribution \(Q_{c}(x)=P(x|y=c)\). Both the pre-training/source classes \(_{1},,_{k}\) and the fine-tuning/target classes \(Q_{1},,Q_{l}\) are assumed to be drawn from a distribution over all classes in \(\). The induced input distribution on all the classes (i.e., the combined source and target input distribution) is denoted \(Q\), while the source-only input distribution is denoted by \(\). Let \(^{*}\) denote the set of pre-trained feature maps \(f:^{d}^{p}\) selected by the training procedure (e.g., the set of trained embeddings with different initialization seeds but the same training protocol) and consider

\[(^{*})=_{f^{*}}_{ c^{}} \|_{f}(Q_{c})-_{f}(Q_{c^{}})\|.\] (11)

With this, we can state the transfer learning bound

**Proposition 5.1**.: _Suppose that the source and target input distributions satisfy a Poincare inequality with constant \(c_{}\) and \(c_{Q}\) (resp.). Then, with probability \(1-\), over the selection \(Q_{c}\) and \(Q_{c^{}}\) of target class distributions, we have that the CDNV expectation for two target classes is bounded by the geometric complexity of the embedding network \(f\) in the following way_

\[_{Q_{c} Q_{c^{}}}[V_{f}(Q_{c},Q_{c})]  }(f,)}{k-1}( _{i j}^{2}})\] \[+(8+_{c C}( ^{*},Q_{c})}{(^{*})})( (^{*},)}{(k-1)(^{*})})\] \[+(1+x (Q)\\ f^{*}}\|f(x)\|}{(^{*})^{2}}) (}c_{Q}_{c} (^{*},Q_{c})}{(^{*})^{2}})\]

Figure 3: VGG-13 trained on CIFAR-10 with 5 random seeds.

_where \(k\) is the number of source classes, \((^{*},Q_{c})=_{f^{*}}(f,Q_{c})\), \(d_{ij}=\|_{f}(Q_{c_{i}})-_{f}(Q_{c_{j}})\|\), and \((^{*},)\) is a complexity measure for \(^{*}\) over \(Q\) (see Appendix A.3)._

Proof.: This follows immediately from [22, Proposition 2] which proves this bound for the NC and distribution variances. Using our Proposition 4.1, we can replace the NC by the GC and swap the variances by the corresponding geometric complexities via the respective Poincare inequality. 

Interpretation.In the bound above, the first term depends on the geometric complexity measured over the source distribution \(\) and decreases as the number of source classes \(k\) increases. Thus, we can predict that lower \((f,)\) encourages lower NC values (i.e. more neural collapse) on the new target classes provided that the second and third term of the bound are also small.

However these other two terms involve both the source and target classes, making the full bound no longer dependent only on the geometric complexity over \(\).

Moreover these two terms can apriori explode since the infimum over the class-mean distances \((^{*})\) no longer only involves source classes (where this distance is bounded away from zero due to neural collapse into an equidistant simplex) but also target classes (for which we no longer have theoretical guarantees).

However, if the source labels are granular enough in the sense the target labels can be represented as combinations of the source labels (as for instance a target label of "dog" can be subsumed as all the breeds of dogs in source classes on ImageNet), then the issues above are mitigated. Formally, we can summarize this granularity compatibility condition between the source and target classes as follows: For each label \(c\) of the target class there exists labels \(c_{1},,c_{k}\) in the source classes such that \(Q_{c}=1/k(Q_{c_{1}}++Q_{c_{k}})\). Because \((f,)\) is linear, we have

\[_{c}(^{*},Q_{c})_{c[k]} (^{*},_{c})\]

and thus recover dependence only on source classes.

To address the \((^{*})\) term, observe that the compatibility condition above implies in terms of class-means that \(_{f}(Q_{c})=(_{f}(Q_{c_{1}})++_{f}(Q_{c_{k}}))\). Because of neural collapse during pre-training, these source class-means form the vertices of a face in the neural-collapse class-mean simplex making the target-class mean \(_{f}(Q_{c})\) the barycenter of this face. Since the distance between any two points taken in the vertices of a simplex plus its barycenters is bounded away from zero, so is \((^{*})\). Note, albeit intuitive and natural, this granularity condition is hard to verify in practice.

### Improve fine-tuning by controlling pre-trained GC

When the compatibility conditions above are satisfied, Proposition 5.1 indicates that increasing the amount of neural collapse for the target classes can be achieved by lowering the embedding GC during pre-training. We verify this indeed occurs experimentally using the same regularization techniques exploited in Section 4. Furthermore, we verify that these implicit methods of controlling pre-training GC produce feature maps that perform better on fine-tuning tasks on CIFAR-FS with a ResNet-18 in Figure 4 and on mini-ImageNet with VGG-16 in Appendix A.5.5.

## 6 Limitations and Conclusion

There is a notable limitation when extending our findings to language modeling and, for example, large language models (LLMs). However, this limitation is not specific to our work per-se, but instead it is a limitation of the application of neural collapse to language models in general, as described in the recent work . Namely, language modeling, as conducted via training by token prediction, creates a classification task where the conditions for neural collapse are implausible. The main problem, in addition to an imbalanced vocabulary, is that the embedding dimension for language models is typically far less than the number of classes (i.e., the total vocabulary size), making the neural collapse simplex impossible to exist. Extending the notion of neural collapse to the language models is an open question and an active area of research and beyond the scope of this work.

Uncovering and understanding the implicit biases that enable successful transfer learning is a critical area of research in modern machine learning. Here we provide a framework connecting three different themes behind implicit regularization: flatness of the loss surface, geometric complexity, and neural collapse. We show that the embedding geometric complexity directly controls the neural collapse during training and, moreover, plays a role in the success of transfer learning.

Extensive experiments on different image classification and fine tuning tasks across different hyperparameters verify our hypothesis. We believe this opens up an intriguing direction of research further exploring the role of geometric collapse and geometric complexity in deep learning and provides valuable insight for designing more efficient and effective techniques for model training and fine-tuning.