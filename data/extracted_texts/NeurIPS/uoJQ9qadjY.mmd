# Learning to Reason Iteratively and Parallelly for

Complex Visual Reasoning Scenarios

 Shantanu Jaiswal\({}^{1,2}\)  Debaditya Roy\({}^{2}\)  Basura Fernando\({}^{2,3}\)  Cheston Tan\({}^{2,3}\)

\({}^{1}\) Carnegie Mellon University \({}^{2}\) IHPC, A*STAR Singapore

\({}^{3}\) Centre for Frontier AI Research, A*STAR Singapore

Correspondence to: sjaiswa3@cs.cmu.edu

###### Abstract

Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural _Iterative_ and _Parallel Reasoning Mechanism_ (IPRM) that combines two distinct forms of computation - iterative and parallel - to better address complex VQA scenarios. Specifically, IPRM's _"iterative"_ computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query _"determine the color of pen to the left of the child in red t-shirt sitting at the white table"_). Meanwhile, its _"parallel"_ computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of operations that are mutually independent (e.g. when counting individual colors for the query: _"determine the maximum occurring color amongst all t-shirts"_). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM's internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors. Source code at: https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism

## 1 Introduction

Visual reasoning and question answering (VQA) at its core requires a model to identify relevant visual operations, execute those operations, and then combine their results to make an inference. Complex visual reasoning scenarios (depicted in fig. 1) are particularly challenging in this regard. They require models to reason compositionally over a large number of reasoning steps and to engage in a variety of higher-level reasoning operations such as causal linking, logical reasoning, and spatiotemporal processing that extend beyond core perception capabilities. In this context, two powerful computational priors exist - iterative and parallel. While each has its own limitations, when combined, they can complement each other and effectively address the challenges of complex VQA tasks. Specifically, **iterative computation**, wherein individual operations are identified and composed in a step-by-step manner, is a beneficial prior for multi-step reasoning scenarios explored by past VQA works . However, pure iterative computation can exhibit limitations inscenarios wherein the entailed visual operations are independent of one-another, or where distinct stimuli need to be processed and tracked simultaneously.

For example, consider the first scenario shown in fig. 1. When executing the language phrase _"maximum occurring shape"_ (i.e. _"what shape appears the most"_), a purely iterative method would: (i) compute the count of each shape (each of which itself could take multiple iterations), (ii) then update and maintain the counts in memory (without forgetting count of all previous shapes), and (iii) finally, recall each shape's count to compute the _"maximum"_. Besides taking more reasoning steps than required, such computation also increases the demand for information retention and recall in memory, which in this scenario could scale by the number of shapes to be counted. In complex video reasoning scenarios, a purely iterative method would similarly struggle in tracking and reasoning over multiple co-occurring events. In such scenarios, from both efficiency and efficacy perspectives, it is advantageous to process operations or stimuli in parallel, rather than solely iteratively.

More generally, **parallel computation** facilitates the simultaneous maintenance and exploration of distinct reasoning paths, and thereby enables reasoning to be more comprehensive, efficient and robust. For example, to compute _"maximum occuring shape"_, parallel compute can enable distinct shape queries to be simultaneously computed prior to computing the _"maximum"_ operation. Similarly, it is effective for other scenarios illustrated in fig. 1, such as in processing independent logical clauses (_"[are X] and [Y made of plastic]"_) or when tracking and processing co-occurring events in videos.

Such computation can be implicitly realized in conventional transformer-based parallel attention mechanisms . However, transformer-based attention does not explicitly incorporate iterative compositional computation , which as described is beneficial for multi-step reasoning scenarios wherein operations need to be composed sequentially. Accordingly, while parallel computation may effectively compute the result of _"maximum occurring shape"_ in fig. 1, it would potentially struggle to integrate the result with further operations such as _"green object with..."_, _"small object in front of green..."_, and _"color of..."_ that need to be computed step-by-step to answer the question.

Based on the above insights, we design the **Iterative and Parallel Reasoning Mechanism (IPRM),** a novel neural reasoning architecture that combines step-by-step iterative computation with the ability to process multiple independent operations and stimuli simultaneously. Inspired by how humans utilize working memory  to facilitate complex reasoning, IPRM internally maintains a latent memory of parallel "operation states", keyed to which are "results states". Given vision and language inputs, IPRM performs the following _iterative_ computation. First, it forms a new set of _parallel_ operations by retrieving relevant language information conditioned on its prior operation states. Then, it "executes" these operations _parallelly_ by retrieving relevant visual information conditioned on its new operations as well as prior result states. Finally, it integrates its new operations (and their results) into memory by dynamically composing these operations with one-another as well as prior operation states, and subsequently, repeats the entire process in its next _iterative_ step.

This strategy effectively enables us to take advantage of both parallel and iterative computations and notably helps improve state-of-arts across various complex image and video reasoning tasks using a single reasoning mechanism. Equally importantly, IPRM's internal computations can be visualized

Figure 1: Complex VQA scenarios (CLEVR-Humans , GQA , CLEVRER-Humans ), AGQA and STAR) wherein combination of iterative (step-by-step) computation (blue phrases) and parallel computation (orange phrases) can be beneficial for reasoning.

across reasoning steps, which helps better interpret what operations it was doing and accordingly where it was looking visually when processing a complex reasoning scenario.

## 2 Iterative and Parallel Reasoning Mechanism

Our proposed iterative-and parallel-reasoning mechanism (IPRM) is a fully-differentiable neural architecture. Given visual features \(_{}^{N_{V} D_{V}}\) and language or task-description features \(_{}^{N_{L} D_{L}}\), IPRM outputs a _"reasoning result"_\(_{}^{D_{m}}\) and, optionally, a set of _"reasoning result tokens"_\(_{}^{N_{m} D_{m}}\). As previously mentioned, IPRM operates iteratively for \(T\) reasoning steps and internally, maintains an explicit memory \(:\{_{},_{}\}\). The memory is modelled as a set of _"operation states"_\(_{}^{N_{op} D_{m}}\), keyed to which are _"result states"_\(_{}^{N_{op} D_{m}}\) as shown in fig. 1. Here, \(N_{op}\) denotes the number of parallel operations to be computed while \(D_{m}\) denotes the mechanism's internal feature dimension. On a high level, at each reasoning step (denoted by \(t\{1,,T\}\)), IPRM performs the following computations:

1. First, conditioned on the existing operation states \(_{}\), we retrieve relevant information from language or task-description features \(_{}\) to form a new set of latent operations \(_{}^{N_{op} D_{m}}\). We term this computation as _"Operation Formation"_. \[_{}=(_{};_{})\] (1)
2. Then, conditioned on the latent operations \(_{}\) and the existing result state \(_{}\), we attend and retrieve relevant information from visual features \(_{}\) which represents a new set of latent results \(_{}^{N_{op} D_{m}}\) corresponding to \(_{}\). We term this computation as _"Operation Execution"_. \[_{}=(_{};[_{},_{}])\] (2)
3. Finally, to facilitate interaction amongst parallel operations, we perform inter-operation attention. Here, each operation \(_{};\{1,,N_{op}\}\), is composed with other operations in \(_{}\) as well as prior operation states \(_{}\) within a lookback-window \(W\). The corresponding result \(_{}\) is similarly composed with other results \(_{}\) and prior result states denoted as \(_{}\). We term this computation as _"Operation Composition"_ \[_{}=(\{_{}, _{}\},_{[(t-W):t]})\] (3) As shown in eq. 1, this output is the new memory state \(_{}:\{_{},_{}\}\).

The overall computation flow is illustrated in fig. 1 and we provide specific details and intuitions behind these computations in the following sub-sections.

Figure 2: IPRMâ€™s computation flow diagram. First, a new set of N-parallel latent operations \(_{}\) are retrieved from language features \(_{}\) conditioned on prior operation states \(_{}\). Then, visual features \(_{}\) are queried conditioned on both \(_{}\) and prior result states results \(_{}\), to form the new results \(_{}\). Finally, both \(_{}\) and \(_{}\) are passed to the Operation Composition Unit (see 2.3), the output of which becomes the new memory state \(\).

### Operation Formation

The _"operation formation"_ stage conceptually models a reasoner that based on its prior set of operations, decides what language features to retrieve in order to form the next set of relevant operations. This can be effectively implemented through conventional attention mechanisms. Specifically, the cumulative set of prior operations (maintained in \(}\)) can be projected to form the 'query' \(}^{N_{op} D_{m}}\) representing "what features to look for". The language features \(}\) can be projected to form the 'key' \(}^{N_{L} D_{m}}\) and 'value' \(}^{N_{L} D_{m}}\). Finally, the new set of latent operations \(}\) can be retrieved by computing \((},},})\). These steps are formally represented below:

\[}=}((}( }))),}=}(}),}=}(})\] (4) \[}=(},}, })\] (5)

Here, \(}^{D_{m} D_{m}}\), \(}^{D_{m} D_{m}}\), \(}^{D_{m} D_{l}}\) and \(}^{D_{m} D_{l}}\). Note \(}\) and \(}\) are not computation-step dependent and only computed once. We use a simple linear-modulated formulation (with appropriate broadcasting and projection weight \(}^{D_{k} 1}\)) to implement \((.)\) (further details in appendix sec. 13).

### Operation Execution

In the _"operation execution"_ stage, the reasoner determines what visual features need to be retrieved depending on both the newly formed operations and existing result states. To model the constituent visual attention mechanism, we draw insights from existing recurrent visual reasoning methods  that incorporate feature modulation for memory-guided attention. Specifically, we retrieve a set of feature modulation weights \(}^{N_{op} D_{m}/r}\) through a joint projection of the new operations \(}\) and prior results \(}\) as shown in eq. 6.

\[}=}([}(}),}(})])\] (6)

Here, \(r\) is a feature reduction ratio . \(}\) is then applied dimension wise to a projection of \(}\) to retrieve an intermediate attention key \(_{V,t}}^{N_{op} N_{k} D_{m}/r}\). The final attention key \(}\) is then obtained through a joint multi-layer-projection of \(_{V,t}}\) and the previously projected representation of \(}\) as shown in eq. 7.

\[_{V,t}}=}}( }),\ }=}((}([}( }),_{V,t}}])))\] (7)

Finally, the attention query and value are formed through separate projections of \(}\) and \(}\) respectively. These are then fed together with \(}\) to the attention function to retrieve the new operation results \(}\) as shown in eq. 8. Intuitively, the overall process allows for both prior results and the new set of operations to jointly guide visual attention.

\[},}=}(}),}(}),}=(}, },})\] (8)

Here, \(}^{D_{m}/r D_{m}}\), \(}^{D_{m}/r D_{m}}\), \(}^{D_{m}/r 2D_{m}/r}\), \(}^{D_{m}/r D_{v}}\), \(}^{D_{m}/r 2D_{m}/r}\), \(}^{D_{m}/r D_{m}/r}\), \(}^{D_{m}/r D_{m}}\) and \(}^{D_{m} D_{m}}\).

### Operation Composition

Finally, in the _"operation composition"_ stage, the reasoner first integrates the executed operations \(}\) and their results \(}\) into the existing memory state \(}\) through a simple recurrent update as shown in eqs. 7 and 10. Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states \(_{op,t+1}}\) with other operation states in \(_{op,t+1}}\) and also prior operation states in \(}\). Here, \(W\) is an attention look-back window.

This composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, \(_{op,t+1}}\) is projected to obtain a set of queries \(}\), while the token-wise concatenation of \(_{op,t+1}}\) and \(}\) are projected to obtain the operation attention keys \(}\) and values \(}\). A second set of values \(}\) are also formed through projection of respective result statesas shown in eq. (14). Further, an identity attention mask \(}}\) is used to ensure that operations in \(}\), can only attend to other operations and not themselves. This is done to enable a higher degree of operation composition. As shown in eq. (15), \(}\), \(}\), \(}\) and \(}}\) are passed to the attention operation, which outputs an intermediate representation \(_{op,t+1}}\) and the softmaxed-attention weights \(}\). \(_{op,t+1}}\) is then added to a projection of \(_{op,t+1}}\) to effectively combine attended operation states with the original operation states, and thereby form the next mem. operation state \(}\).

Finally, the next result states are obtained by applying \(}\) on \(}\) and then adding a projection of \(_{res,t+1}}\) as shown in eq. (17). Note \(}\) is specifically utilized to ensure that results are composed based on attentions between operation states. Here, all the mentioned weights \(_{}^{D_{m} D_{m}}\) and \([;]\) represents token-wise concatenation. \(}}\) in eq. (15) is an identity matrix which is concatenated with zeros (i.e. unmasked) for window tokens if window len. \(>0\).

\[_{op,t+1}}=}(})+ }(})\] (9) \[_{res,t+1}}=}(} )+}(})\] (10) \[}=}(_{op,t+1}})\] (11) \[}=}([_{op,t+1}}; }])\] (12) \[}=}([_{op,t+1}}; }])\] (13) \[}=}([_{res,t+1}} ;}])\] (14) \[_{op,t+1}},}=(},},},}})\] (15) \[}=_{op,t+1}}+}(_{op,t+1}})\] (16) \[}=}(})+}(_{res,t+1}})\] (17)

**Obtaining Reasoning Summary** As mentioned before, our proposed mechanism outputs a set of _"reasoning result tokens"_\(}\) and a _"reasoning result"_\(}\). \(}\) is simply equivalent to the last memory result states \(}\). To obtain \(}\), we perform attention on the last operation states \(}\) by utilizing a summary representation \(}^{D_{l}}\) of \(}\) as the attention-query.

We set \(}\) to be the first token in case of transformer-based language backbones and as last hidden state in case of LSTM-based language backbones. As shown in eq. (18), \(}\) is projected to obtain a single-token attention query \(}\) while \(}\) is projected to obtain the attention keys \(}\). The attention value is simply the result states \(}\), and the output of the attention function is the _"reasoning result"_. Intuitively, this computation corresponds to the reasoner deciding which final operation states in \(}\) are most relevant to the summary of the input language or task-description \(}\), based on which corresponding result states \(}\) are weighted and retrieved.

\[},} =}(}),}(})\] (18) \[} =(},},})\] (19)

Here, \(}^{D_{m} D_{l}}\) and \(}^{D_{m} D_{m}}\).

**Reasoning mechanism general applicability.** Our proposed iterative and parallel reasoning mechanism is an end-to-end trainable neural module. It can be conveniently applied on top of different vision and language backbones, and be trained directly as a new computational block with no specific adjustments. Further, IPRM is weight-tied which means that its number of parameters is constant regardless of number of computation steps and parallel operations. We provide parameter and computational details along with module implementation in appendix sec.

Figure 3: Operation Composition Unit

## 3 Experiments

We evaluate IPRM on STAR, AGQAv2 and CLEVRER-Humans for video reasoning tasks and CLEVR-Humans, GQA and CLEVR-CoGenT for image reasoning tasks. For all tasks, we set IPRM's parallel operations (\(N_{op}\)) to 6, reasoning steps (\(T\)) to 9, reduction ratio (\(r\)) to 2 and window length (\(W\)) to 2 (informed by ablative analysis detailed in sec. 3.3). We follow task-specific practices (detailed in appendix C.1) for respective vision and language backbones in our primary experiments, besides also demonstrating integration of IPRM with large-scale VL backbones such as CLIP. Further, besides task-specific methods, we also consider two prominent transformer-based VL modules as baselines - concat-att (where lang. and vis. tokens are concatenated as in , ) and cross-att (where lang. tokens are "query" to vis. tokens as "key" and "value"; as in ). Further implementation and training details are provided in appendix sec.4.

### Video Reasoning and Question Answering (STAR, AGQAv2 and CLEVRER-Humans)

We first evaluate IPRM on recent video reasoning benchmarks. STAR and AGQAv2 comprise real-world videos and test multiple reasoning skills in context of situational reasoning and compositional spatiotemporal reasoning respectively. STAR contains 60K questions testing four broad types of video reasoning abilities: _feasibility_, _interaction_, _prediction_ and _sequence_. Meanwhile, AGQAv2 contains 2.27M balanced questions distributed across 16 different question types. As shown in table, IPRM obtains 69.9% average acc. on STAR and 60.4% overall acc. on AGQAv2, outperforming prior videoQA-specific methods by 5% on both benchmarks.

Interestingly, on STAR IPRM obtains an 8% and 7% improvement over SeViLA-BLIP2 on the predictive and sequencing scenarios respectively. This is possibly due to IPRM's capability to reason over multiple events simultaneously across frames, which may enhance its capacity to retrieve and cumulatively reason an relevant information needed to predict future events and determine appropriate sequences. However, IPRM performs less effectively than SeViLA-BLIP2 in feasibility scenarios, possibly because these scenarios require not only visual reasoning but also commonsense knowledge, which can benefit from integration with larger-scale vision-language backbones and auxiliary training tasks such as introduced in LRR.

Similarly, On AGQAv2, IPRM improves performances across various question types, notably achieving a 8% improvement in questions that require determining sequence of events. Further, IPRM also outperforms both 4-layer concat- and cross-attention modules on STAR (scaling further attention-layers was not found to benefit performance as detailed in appendix table).

Next, we evaluate IPRM on the CLEVRER-Humans benchmark, which comprises synthetic videos of simultaneous object motions and multiple collisions, and tests a model's ability to determine causal links between events. We perform zero-shot, finetuned and from-scratch evaluation. As shown in table, IPRM outperforms task-specific neurosymbolic models NS-DR and VR-DP as well as state-of-the-art ALOE across the three settings. Specifically, IPRM improves zero-shot per-question acc. by 7%, finetuned per-question acc. by 18.8% and scratch per-question acc. by 6.2%. These results further suggest that IPRM can better track and process co-occuring events, and in this case, more accurately determine causal links.

  Model & Int. & Seq. & Pred. & Feas. & Avg. \\  LRR & 73.7 & 71.0 & 71.3 & 65.1 & 70.3 \\ LRR/w/o surrogate & 54.5 & 48.7 & 44.3 & 45.5 & 48.2 \\ All-in-One & 47.5 & 50.8 & 47.7 & 44.0 & 47.5 \\ Temp[ATP] & 50.6 & 52.8 & 49.3 & 40.6 & 48.3 \\ MIST & 55.5 & 54.2 & 54.2 & 44.4 & 51.1 \\ InternetVideo & 62.7 & 65.6 & 54.9 & 51.9 & 58.7 \\ SeViLA-BLIP2 & 63.7 & 70.4 & 63.1 & **62.4** & 64.9 \\ Concat-Att-AL & 68.1 & 71.4 & 66.6 & 55.2 & 65.3 \\ Cross-Att-AL & 67.5 & 72.1 & 64.4 & 58.5 & 65.6 \\  IPRM & **71.8** & **77.7** & **71.0** & 59.1 & **69.9** \\  

Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (0). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.

### Compositional Image Reasoning (CLEVR-Humans, CLEVR-CoGen and GQA)

Here, we evaluate IPRM on challenging compositional image reasoning benchmarks. CLEVR-Humans tests generalization of multi-hop reasoning to free-form human crowdsourced questions which entail reasoning skills/scenarios beyond a model's training on the original CLEVR dataset and provides a limited finetuning set (2.5% of CLEVR). Similarly, CLEVR-CoGenT tests generalization on novel attribute compositions not observed in training (e.g.,"_gray cubes_" and "_red cylinders_" are in training but eval. is on "_gray cylinders_" and "_red cubes_"; see suppl. for exact specification). The CLOSURE  benchmark further tests systematic generalization for different question type compositions.

As shown in table 3, IPRM achieves 3.9% and 3.8% improvements in zero-shot and fully-finetuned performance over prior state-of-art vision-language model MDETR. Further, IPRM neither requires bounding-box pre-training supervision (as done in MDETR) nor functional programs, and can be trained directly with only vision-language inputs and task supervision. fig. 5 illustrates IPRM's performance across different training ratios compared to MDETR and cross- and concat-att transformer modules. Notably, IPRM exceeds MDETR's fully-finetuned performance by 1.1% with only half of training data. Further, IPRM exhibits these improvements while being relatively lightweight (4.4M params) in comparison to MDETR's transformer blocks (17.4M; \(\)4x more parameters) and cross- and concat-VL attention methods (16.8M and 12.6M respectively). These results suggest that IPRM exhibits strong generalization capabilities and more sample-efficient learning of novel reasoning skills and scenarios in context of multi-step imageQA.

For CLEVR-CoGenT, IPRM achieves state-of-art results in out-of-domain generalization on novel attribute compositions and outperforms MDETR (having parallel transformer compute) by 3.6% and MAC (iterative method) by 2%. This suggests the combination of iterative and parallel computation as done in IPRM can implicitly enable more disentangled feature processing and thereby improve compositional learning of primitive attributes in context of multi-step imageQA. Similarly, on CLOSURE, IPRM achieves an average zero-shot accuracy of 75.6% which is highest amongst fully neural reasoning methods (that require no extra supervision) and is close to the neurosymbolic method NS-VQA which utilizes ground truth programs and object bounding boxes supervision. Further, detailed breakdown of models on CLOSURE question types is provided in appendix table

   &  &  &  \\  & Opt. & Qs. & Opt. & Qs. & Opt. & Qs. \\  NS-DR  & 51.0 & 32.0 & - & - & - & - \\ VRDP  & 50.9 & 31.6 & - & - & - & - \\  CNN-LSTM  & 56.3 & 30.0 & 51.7 & 34.2 & 51.5 & 30.8 \\ CNN-BERT  & 52.9 & 32.0 & 52.4 & 30.2 & 50.1 & 30.4 \\ ALOC  & 54.0 & 26.9 & 51.8 & 31.7 & 52.7 & 32.1 \\  IPRM & **61.7** & **38.9** & **74.1** & **53.0** & **62.0** & **38.3** \\  

Table 2: Comparison of methods for CLEVR-Humans  (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings.

We also evaluate IPRM on the GQA benchmark which tests compositional VQA on real-world images. We perform comparisons with prior VQA methods as well as large-scale VL models such as LXMERT and 12-in-1 that do not utilize the ground truth scene graphs in GQA. As shown in table 1, IPRM achieves 60.3%, outperforming prior reasoning methods such as MCAN and LCGN as well as large-scale VL models such as LXMERT and 12-in-1. However, IPRM's performance is behind the larger VL model OSCAR which performs pretraining on 4.3M data samples collated from COCO, VG, SBU and Flickr. Note, IPRM is a standalone reasoning module only trained on GQA balanced with no pre-training. Further, IPRM when trained with perfect perception (i.e. ground truth object bounding boxes and attributes), achieves 87.2% on GQA validation set suggesting strong reasoning performances can be achieved through advancements in visual detectors and backbones. Finally, in appendix B.3, we demonstrate that IPRM more effectively enhances the performance of frozen CLIP variants on complex reasoning benchmarks compared to scaling traditional cross- and concat-attention modules.

### Model ablations and reasoning visualization

We perform ablations to study the contributions of IPRM's salient components. First, we analyze the impact of varying number of parallel operations (\(N_{op}\)) against number of iterative computation steps (\(T\)). We compare models with \(N_{op}\{1,3,6,9\}\) and \(T\{1,3,6,9\}\), resulting in 16 different models. Given the large amount of ablative models, we perform analysis on a reduced-resolution setting of CLEVR-Humans with pretraining on CLEVR for 15 epochs. As shown in fig. 1 (plot (i)), we find that \(T\) and \(N_{op}\) appear to be co-dependent and that neither by itself can lead to high performance. E.g. setting \(T\) = 1 generally results in performances around 75% regardless of \(N_{op}\), while setting \(N_{op}\)= 1 or 3 results in a sharp performance drop of 3% when changing \(T\) to 9 from 6. In contrast, for \(N_{op}>3\), we observe that performance increases steadily with higher \(T\), suggesting that a higher number of parallel operations may prevent overfitting in a model with high computation steps. Overall, we find that (\(N_{op}=6\), \(T=9\)) and (\(N_{op}=9\), \(T=9\)) are the two best performing models achieving above 82% accuracy (with the former preferred as \(N_{op}=6\) performs better for diff. \(T\) compared to \(N_{op}=9\)).

Next, we study the impact of the operation composition block (OPC) by evaluating \(N_{op}=6\) (and diff. computation steps \(T\)) with and without OPC. As shown in fig. 1 (plot (ii)), while \(T=1\) has a relatively low drop of \(\)2%, the performance drops are more significant for higher \(T\). The (\(N_{op}=6\), \(T=9\)) model which reached \(\)82% acc. with OPC, drops to \(\)74% acc. without OPC.

Finally, we study the impacts of the dimension reduction ratio (\(r\)) and memory-lookback window length (\(W\)). As shown in fig. 1 (plot (iii)), \(r=2\) leads to negligible drop (0.3%) in performance compared to no dimension reduction (i.e. \(r=1\)) while being computationally faster and requiring less memory. However, setting \(r\) to \(8\) significantly deteriorates performance. Similarly, as shown in fig. 1 (plot (iv)), a setting of window-length \(W\) is 2 works sufficiently well, and decreasing it below that significantly impacts performance.

  & LCGN  & MCAN  & LXMERT*  & 12-in-1*  & OSCAR*  & CFR**  & IPRM \\  GQA & 55.8 & 57.4 & 60.0 & 60.0 & **61.6** & 72.1 & 60.3 \\  

Table 4: Performance comparison on GQA with imageQA methods and large-scale models that do not utilize ground-truth scene graphs. * indicates large-scale pretrained VL model. **Utilizes ground truth scene graphs, programs and bounding boxes for auxiliary training.

Figure 6: IPRM Model ablations in order: **(i)** Impact of number of parallel operations (\(N_{op}\)) vs computation steps (\(T\)). **(ii)** Impact of Operation Composition Block (OPC). **(iii)**: Impact of reduction ratio (\(r\)) and **(iv)** memory window length (\(W\)).

[MISSING_PAGE_FAIL:9]

contributes an alternative and possibly more effective reasoning mechanism that can be integrated with such models in future to enhance complex VQA capabilities.

**Memory and recurrence-augmented transformers.** Multiple works have identified limitations of purely feedforward computation as realized in transformers and worked on encoding recurrence  and memory-augmented computation . Notably, Recurrent Memory Transformer  and MemFormer introduce recurrent and dynamic memory to improve language modelling capabilities. More recently, EMAT  introduces efficient memory to augment knowledge retrieval, MemViT  introduces a cache memory to retain prior context for long-video tasks, and  introduces memory for more effective action anticipation. While these methods study recurrent and memory-augmented computation on specific natural language processing and computer vision tasks, our work focuses on the integration of iterative-parallel computation and working memory in a single neural reasoning mechanism beneficial for complex VQA scenarios.

## 5 Conclusion

We introduced a novel fully-differentiable and end-to-end trainable iterative and parallel reasoning mechanism (IPRM) to address complex VQA scenarios. We comprehensively evaluated IPRM on various complex image and video VQA benchmarks testing distinct reasoning capabilities, and found it improves state-of-arts on multiple such benchmarks. We also performed quantiative ablations to study individual impacts of parallel and iterative computation besides qualitative analysis of IPRM's reasoning computation visualization.

## 6 Limitations and Future work

Here, we note possible limitations of IPRM. Similar to existing VQA and deep-learning methods, IPRM may reflect biases that are present in the training distribution of VQA benchmarks. This may lead it to overfit to certain image inputs or question forms and possibly provide skewed answers in such scenarios. Further, the utilized vision-language backbones in our experiments may also entail visual, language and cultural biases in their original training distribution which may permeate to IPRM upon integration for VQA scenarios. In this regard, we hope the capability to visualize intermediate reasoning of IPRM and diagnose its error cases (as shown in section 3.3) can serve a useful tool to benefit interpretability in VQA and identify possible reasoning biases that may emerge in the model.

For future work, scaling the IPRM architecture to a foundational video-language model by integrating it with large-scale transformer-based vision-language models and relevant instruction-tuning approaches presents an exciting research opportunity. Moreover, while we designed and evaluated IPRM in the context of complex VQA, we believe it has the potential to operate as a general reasoning mechanism applicable to tasks beyond visual reasoning and question answering, such as language processing and embodied reasoning.