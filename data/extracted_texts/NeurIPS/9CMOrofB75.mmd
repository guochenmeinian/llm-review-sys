# Evaluating the design space of diffusion-based generative models

Yuqing Wang

Simons Institute

University of California, Berkeley

yq.wang@berkeley.edu

&Ye He

School of Mathematics

Georgia Institute of Technology

yhe367@gatech.edu

&Molei Tao

School of Mathematics

Georgia Institute of Technology

mtao@gatech.edu

###### Abstract

Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in Karras et al. . It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in Song et al.  is more preferable, but when it is less trained, the design in Karras et al.  becomes more preferable.

## 1 Introduction

Diffusion models became a very popular generative modeling approach in various domains, including computer vision [20; 7; 27; 28; 38; 51], natural language processing [6; 34; 37], various modeling tasks [15; 41; 55], and medical, biological, chemical and physical applications [3; 17; 43; 49; 23; 56] (see more surveys in [53; 11; 14]). Karras et al.  provided a unified empirical understanding of the derivations of model parameters, leading to new state-of-the-art performance. Karras et al.  further upgraded the model design by revamping the network architectures and replacing the weights of the network with an exponential moving average. As diffusion models gain wider usage, efforts to understand and enhance their generation capability become increasingly meaningful.

In fact, a rapidly increasing number of theoretical works already analyzed various aspects of diffusion models [32; 19; 52; 16; 12; 8; 18; 9; 13; 39; 44; 25]. Among them, a majority [32; 19; 52; 16; 12; 8; 18] focus on sampling/inference; more precisely, they assume the score error is within a certain accuracy threshold (i.e. the score function is well trained in some sense), and analyze the discrepancy between the distribution of the generated samples and the true one. Meanwhile, there are a handful of results [44; 25] that aim at understanding different facets of the training process. See more detailed discussions of existing theoretical works in Section 1.1.

However, as indicated in Karras et al. , the performance of diffusion models also relies on the interaction between design components in both training and sampling, such as the noise distribution, weighting, time and variance schedules, etc. While focusing individually on either the training or generation process provides valuable insights, a holistic quantification of the actual generation capability can only be obtained when both processes are considered altogether. Therefore, motivated by obtaining _deeper theoretical understanding of how to maximize the performance of diffusion models_, this paper aims at establishing a full generation error analysis, combining both the optimization and sampling processes, to partially investigate the design space of diffusion models.

More precisely, we focus on the variance exploding setting , which is also the foundation of continuous forward dynamics in Karras et al. . Our main contributions are summarized as follows:

* For denoising score matching objective, we establish the exponential convergence of its gradient descent training dynamics (Theorem 1). We develop a new method for proving a key lower bound of gradient under the semi-smoothness framework [1; 35; 57; 58].
* We extend the sampling error analysis in  to the variance exploding case (Theorem 2), under only the finite second moment assumption (Assumption 3) of the data distribution. Our result applies to various variance and time schedules, and implies a sharp almost linear complexity in terms of data dimension under optimal time schedule.
* We conduct a full error analysis of diffusion models, combining training and sampling (Theorem 3).
* We qualitatively derive the theory for choosing the noise distribution and weighting in the training objective, which coincides with Karras et al.  (Section 4.1). More precisely, our theory implies that the optimal rate is obtained when the total weighting exhibits a similar "bell-shaped" pattern used in Karras et al. .
* We develop a theory of choosing time and variance schedules based on both training and sampling (Section 4.2). Indeed, when the score error dominates, i.e., the neural network is less trained and not very close to the true score, polynomial schedule  ensures smaller error; when sampling error dominates, i.e., the score function is well approximated, exponential schedule  is preferred.

Conclusions and limitations are in Appendix A.

### Related works

**Sampling.** There has been significant progress in quantifying the sampling error of the generation process of diffusion models, assuming the score function is already approximated within certain accuracy. Most existing works [e.g., 16; 12; 8] focused on the variance preserving (VP) SDEs, whose discretizations correspond to DDPM. For example, Benton et al.  is one of the latest results for the VPSDE-based diffusion models, and it only needs a very mild assumption: the data distribution has finite second moment. The iteration complexity is shown to be almost linear in the data dimension and polynomial in the inverse accuracy, under exponential time schedule. However, a limited amount of works [32; 24; 54] analyzed the variance exploding (VE) SDEs, whose discretizations correspond to Score Matching with Langevin dynamics (SMLD) [45; 46]. To our best knowledge, Yang et al.  obtained the best result so far for VE assuming the data distribution has bounded support: the iteration complexity is polynomial in the data dimension and the inverse accuracy, under the uniform time schedule. In contrast, our work only assumed that the data distribution has finite second moment, and by extending the stochastic localization approach in  to VESDE, we obtain an iteration complexity that is polynomial in the data dimension and the inverse accuracy, under more general time schedules as well. Note the improved complexity in terms of the inverse accuracy and the data dimension dependencies; in fact, under the exponential time schedule, our complexity is almost linear in the data dimension, which recovers the state-of-the-art result for VPSDE-based diffusion models.

**Training.** To our best knowledge, the only works that quantify the training process of the diffusion models are Shah et al.  and Han et al. . Shah et al.  employed the DDPM formulation and considered data distributions as mixtures of two spherical Gaussians with various scales of separation, together with \(K\) spherical Gaussians with a warm start. Then the score function can be analytically solved, and they modeled it in a teacher-student framework solved by gradient descent. They also provided the sample complexity bound under these specific settings. In contrast, our results work for general data distributions for which the true score is unknown, and training analysis is combined with

Figure 1: Structure of this paper.

sampling analysis. Han et al.  considered the GD training of a two-layer ReLU neural network with the last layer fixed, and used the neural tangent kernel (NTK) approach to establish a first result on generalization error. They uniformly sampled the time points in the training objective, assumed that the Gram matrix of the kernel is away from 0 (implying a lower bound on the gradient), and lacked a detailed non-asymptotic characterization of the training process. In contrast, we use the deep ReLU network with \(L\) layers trained by GD and prove instead of assuming that the gradient is lower bounded by the objective function. Moreover, we obtain a non-asymptotic bound for the optimization error, and our bound is valid for general time and variance schedules, which allows us to obtain a full error analysis.

**Convergence of neural networks training.** The convergence analysis of neural networks under gradient descent has been a longstanding challenge and has been developed into an extensive field. Here we will only focus on results mostly related to the techniques used in this paper. One line of them is approaches directly based on neural tangent kernel (NTK) [22; 21; 5; 47; 36]. However, existing works in this direction focus more on either scalar output, or vector output but with only one layer trained under two-layer networks, which is insufficient for diffusion models. Another line of research also considers overparameterized models in a regime analogous to NTK, though not necessarily explicitly resorting to kernels. Instead, it directly quantifies the lower bound of the gradient [1; 35; 2; 57; 58] and uses a semi-smoothness property to prove exponential convergence. Our results align with the latter line, but we develop a new method for proving the lower bound of the gradient and adopt assumptions that are closer to the setting of diffusion models. See more discussions in Section 3.1.

### Notations

We denote \(\|\|\) to be the \(^{2}\) norm for both vectors and matrices, and \(\|\|_{F}\) to be the Frobenius norm. For the discrete time points, we use \(t_{i}\) to denote the time point for forward dynamics and \(t_{i}^{}\) for backward dynamics. For the order of terms, we follow the theoretical computer science convention to use \((),(),()\). We also denote \(f g\) if \(f Cg\) for some universal constant \(C\).

## 2 Basics of diffusion-based generative models

In this section, we will introduce the basic forward and backward dynamics of diffusion models, as well as the denoising score matching setting under which a model is trained.

### Forward and backward processes

Consider a forward diffusion process that pushes an initial distribution \(P_{0}\) to Gaussian

\[dX_{t}=-f_{t}\,X_{t}\,dt+^{2}}\,dW_{t},\] (1)

where \(dW_{t}\) is the Brownian motion, \(X_{t}\) is a \(d\)-dim. random variable, and \(X_{t} P_{t}\). Under mild assumptions, the process can be reversed and the backward process is defined as follows

\[dY_{t}=(f_{T-t}\,Y_{t}+2_{T-t}^{2} p_{T-t}(Y_{t})) dt+^{2}}\,d_{t},\] (2)

where \(Y_{0} P_{T}\), and \(p_{t}\) is the density of \(P_{t}\). Then \(Y_{T-t}\) and \(X_{t}\) have the same distribution with density \(p_{t}\), which means the dynamics (2) will push (near) Gaussian distribution back to (nearly) the initial distribution \(P_{0}\). To apply the backward dynamics for generative modeling, the main challenge lies in approximating the term \( p_{T-t}(Y_{t})\) which is called _score function_. It is common to use a neural network to approximate this score function and learn it via the forward dynamics (1); then, samples can be generated by simulating the backward dynamics (2).

### The training of score function via denoising score matching

In order to learn the score function, a natural starting point is to consider the following score matching objective [e.g., 29]

\[_{}()=_{t_{0}}^{T}w(t) _{X_{t} P_{t}}\|S(;t,X_{t})-_{x} p_{t}(X_{t})\|^{2}\,dt\] (3)

where \(S(;t,X_{t})\) is a \(\)-parametrized neural network, \(w(t)\) is some _weighting function_, and the subscript means this is the continuous setup. Ideally one would like to minimize this objective function to obtain \(\); however, \(p_{t}\) in general is unknown, and so is the true score function \(_{x} p_{t}\). One of the solutions is denoising score matching proposed by Vincent , where one, instead of directly matching the true score, leverages conditional score for which initial condition is fixed so that \(p_{t|0}\) is analytically known.

More precisely, given the linearity of forward dynamics (1), its exact solution is explicitly known: Let \(_{t}=_{0}^{t}f_{s}\,ds\), and \(_{t}^{2}=2_{0}^{t}e^{2_{s}-2_{t}}_{s}^{2}\,ds\). Then the solution is \(X_{t}=e^{-_{t}}X_{0}+_{t},\) where \((0,I)\). We also have \(X_{t}|X_{0}(e^{-_{t}}X_{0},_{t}^{2}I)\) and \(g_{t}(x|y)=(2_{t}^{2})^{-d/2}(-\|x-e^{-_{t}}y\|^{2}/(2 _{t}^{2}))\), which is the density of \(X_{t}|X_{0}\). Then the objective can be rewritten as

\[_{}() =_{t_{0}}^{T}w(t)_{X_{0}}_{X_{ t}|X_{0}}\|S(;t,X_{t})- g_{t}(X_{t}|X_{0})\|^{2}dt+ _{t_{0}}^{T}w(t)C_{t}dt\] \[=_{t_{0}}^{T}w(t)_{t}} _{X_{0}}_{}\|_{t}S(;t,X_{t})+\|^{ 2}dt+_{t_{0}}^{T}w(t)C_{t}dt\] (4)

where \(C_{t}=_{X_{t}}\| p_{t}\|^{2}-_{X_{0}}_{X_{t}|X_{0}}\| g_{t}(X_{t}|X_{0})\|^{2}.\) For completeness, we will provide a detailed derivation of these results in Appendix C and emphasize that it is just a review of existing results in our notation.

Throughout this paper, we adopt the variance exploding (VESDE) setting , where \(f_{t}=0\) and hence \(_{t}=0\), which also aligns with the setup in the classic of EDM .

## 3 Error analysis for diffusion-based generative models

In this section, we will quantify both the training and sampling processes, and then integrate them into a more comprehensive generation error analysis.

### Training

In this section, we consider a practical implementation of denoising score matching objective, represent the score by a deep ReLU network, and establish the exponential convergence of GD training dynamics.

**Training objective function.** Consider a quadrature discretization of the time integral in (4) based on deterministic1 collocation points \(0<t_{0}<t_{1}<t_{2}<<t_{N}=T\). Then

\[_{}()}()+,\] (5)

where \(=_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})C_{t_{j}}\), and

\[}()=_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1}) _{t_{j}}}_{X_{0}}_{}\|_{t_{j}}S(;t_{j},X_{t_{j}})+\|^{2}.\] (6)

Define \(_{j}=w(t_{j})(t_{j}-t_{j-1})_{t_{j}}}\) to be the _total weighting_. Consider the empirical version of \(}\) (6). Denote the initial data to be \(\{x_{i}\}_{i=1}^{n}\) with \(x_{i} P_{0}\), and the noise to be \(\{_{ij}\}_{N=1}^{N}\) with \(_{ij}(0,I_{d})\). Then the input data of the neural network is \(\{t_{j},X_{ij}\}_{i=1,j=1}^{n,N}=\{t_{j},x_{i}+_{t_{j}}_{ij}\}_ {i=1,j=1}^{n,N}\) and the output data is \(\{_{ij}/_{t_{j}}\}_{i=1,j=1}^{n,N}\) if \(_{t_{j}} 0\). Consequently, \(}()\) (6) can be approximated by the following

\[}_{em}()=_{i=1}^{n}_{j=1}^{N}_{ j}\|_{t_{j}}S(;t_{j},x_{i}+_{t_{j}}_{ij})+_{ij} \|^{2}.\] (7)

We will use (7) as the training objective function in our analysis. For simplicity, we also denote \(f(;i,j)=_{j}|_{t_{j}}S(;t_{j},x_{i}+_ {t_{j}}_{ij})+_{ij}|^{2}\) and then \(}_{em}()=_{i=1}^{n}_{j=1}^{N}f( ;i,j)\). Note the time dependence can be absorbed into the \(X\) dependence. More precisely, because \(_{t}\) is a monotonically increasing function of \(t\), we can replace \(t_{j}\) in the inputs by \(_{t_{j}}\) to indicate the time dependence. This is then equivalent to augmenting \(X_{ij}\) to be \(d+1\) dimensional with \((x_{i})_{d+1}:=0\) and \((_{ij})_{d+1}:=1\). For simplified presentation, we will slightly abuse notation and still use \(d\) as the input dimension rather than \(d+1\).

**Architecture.** The analysis of diffusion model training is in general very challenging. One obvious factor is the complex score parameterizations used in practice such as U-Net  and transformers [40; 34]. In this paper, we simplify the architecture and consider deep feedforward networks. Although it is still far from practical usage, note this simple structure can already provide insights about the design space, as shown in later sections, and is more complicated than existing works [25; 44] related to the training of diffusion models (see Section 1.1). More precisely, we consider the standard deep ReLU network with bias absorbed:

\[S(;t_{j},X_{ij})=W_{L+1}(W_{L} W_{1}(W_{0}X_{ij})),\] (8)where \(=(W_{0},,W_{L+1})\), \(W_{0}^{m d},W_{L+1}^{d m}\), \(W_{}^{m m}\) for \(=1,,L\), and \(()\) is the ReLU activation.

**Algorithm.** Let \(^{(k)}=(W_{0},W_{1}^{(k)},,W_{L}^{(k)},W_{L+1})\). We consider the gradient descent (GD) algorithm as follows

\[^{(k+1)}=^{(k)}-h}_{em}(^{(k)}),\] (9)

where \(h>0\) is the learning rate. We fix \(W_{0}\) and \(W_{L+1}\) throughout the training process and only update \(W_{1},,W_{L}\), which is a commonly used setting in the convergence analysis of neural networks .

**Initialization.** We employ the same initialization as in Allen-Zhu et al. , which is to set \(W_{}^{(0)}_{ij}(0,)\) for \(=0,,L\), \(i,j=1,,m\), and \((W_{L+1}^{(0)})_{ij}(0,)\) for \(i=1,d\), \(j=1,m\).

For this setup, the main challenge in our convergence analysis for denoising score matching lies in the nature of the data. 1) The output data that neural network tries to match is an unbounded Gaussian random vector, and cannot be rescaled as assumed in many theoretical works (for example, Allen-Zhu et al.  assumed the output data to be of order \(o(1)\)). 2) The input data \(X_{ij}\) is the sum of two parts: \(x_{i}\) which follows the initial distribution \(P_{0}\), and a Gaussian noise \(_{t_{j}}_{ij}\). Therefore, any assumption on the input data needs to agree with this noisy and unbounded nature, and commonly used assumptions like data separability  can no longer be used.

To deal with the above issues, we instead make the following assumptions.

**Assumption 1** (On network hyperparameters and initial data of the forward dynamics).: _We assume the following holds:_

_1. Data scaling:_ \(\|x_{i}\|=(d^{1/2})\) _for all_ \(i\)_._

_2. Input dimension:_ \(d=(((nN)))\)_._

We remark that the first assumption focuses only on the initial data \(x_{i}\) instead of the whole solution of the forward dynamics \(X_{ij}\) which incorporates the Gaussian noise. Also, this assumption is indeed not far away from reality; for example, it holds with high (at least \(1-((-(d)))\)) probability for standard Gaussian random vectors. The requirement for input dimension \(d\) is to ensure that \(d\) is not too small, or equivalently the number of data points is not exponential in \(d\).

We also make the following assumptions on the hyperparameters of the denoising score matching.

**Assumption 2** (On the design of diffusion models).: _We assume the following holds:_

_1. Weighting:_ \(_{j=1}^{N}w(t_{j})(t_{j}-t_{j-1})_{t_{j}}=(N)\)_._

_2. Variance:_ \(_{t_{0}}>0\) _and_ \(_{t_{N}}=(1)\)_._

The first assumption is to guarantee that the weighting function \(w(t)\) is properly scaled. This expression \(w(t_{j})(t_{j}-t_{j-1})_{t_{j}}\) is obtained from proving the upper and lower bounds of the gradient of (7), and is different from the total weighting \(_{i}\) defined above. In the second assumption, \(_{t_{0}}>0\) ensures the output \(_{ij}/_{t_{j}}\) is well-defined. The \(_{t_{N}}=(1)\) guarantees that the scales of the noise \(_{t_{j}}_{ij}\) and the initial data \(x_{i}\) are of the same order at the end of the forward process, namely, the initial data \(x_{i}\) is eventually push-forwarded to near Gaussian with the proper variance. Therefore, Assumption 2 aligns with what has been used in practice (see Section 4 and Karras et al. , Song et al.  for examples).

The following theorem summarizes our convergence result for the training of the score function.

**Theorem 1** (Convergence of GD).: _Define a set of indices to be \(^{(s)}=\{(i,j)|f(^{(s)};i,j) f(^{(s)};i^{},j^ {})i^{},j^{})\). Then given Assumption 1 and 2, for any \(_{ train}>0\), there exists some \(M(_{ train})=(n,N,d,L,T/t_{0},( }))\), s.t., when \(m M(_{ train})\), \(h=(w(t_{j})(t_{j}-t_{j-1})_{t_{j}}})\), and \(k=(d^{}{2}}n^{2}N(}))\), with probability at least \(1-(nN)(-(d^{2_{0}-1}))\), we have_

\[}_{em}(^{(k)})_{s=0}^{k-1}(1-C_{5}h\;w(t_{j ^{*}(s)})(t_{j^{*}(s)}-t_{j^{*}(s)-1})_{t_{j^{*}(s)}}(-1}{2}}}{n^{3}N^{2}}))}_{em}( ^{(0)})\]

_where the universal constant \(C_{5}>0\), \(a_{0},1\), and \((i^{*}(s),j^{*}(s))=_{(i,j)^{(s)}}w(t_{j})(t_{j}-t_{j-1}) _{t_{j}}\). Moreover, when \(K=(d^{}{2}}n^{2}N(}))\),_

\[}_{em}(^{(K)})_{ train}.\]The above theorem implies that for denoising score matching objective \(}_{em}()\), GD has exponential convergence. For example, if we simply take \(j^{*}=_{j}w(t_{j})(t_{j}-t_{j-1})_{t_{j}}\), then \(}_{em}(^{(k+1)})\) is further upper bounded by \((1-C_{6}h\ w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})_{t_{j^{*}}} (-1}{2}}}{n^{3}N^{2}}))^{k+1}}_{em}(^{(0)})\). The rate of convergence can be interpreted in the following way: 1) at the \(k\)th iteration, we collect all the indices of the time points into \(^{(k)}\) where \(f(;i,j)\) has the maximum value; 2) we then choose the maximum of \(w(t_{j})(t_{j}-t_{j-1})_{t_{j}}\) among all such indices and denote the index to be \(j^{*}(k)\), and obtain the decay ratio bound for the next iteration as \(1-C_{6}h\ w(t_{j^{*}(k)})(t_{j^{*}(k)}-t_{j^{*}(k)-1})_{t_{j^{*}(k) }}(-1}{2}}}{n^{3}N^{2}})\).

**Remark 1** (Can \(_{}\) be arbitrarily small? Some ramifications of the denoising setting).: _Let us first see some facts about \(}_{em}\) and \(}\). Under minimal assumption of the existence of score function and in the zero-time-discretization-error limit, the score matching objective can be made zero and therefore the denoising score matching objective is bounded below by \(-\), which is nonnegative and zero only when the data distribution is extremely special (we thus write \(->0\) from hereon unless confusion arises). That is, \(_{}}()_{}}=->0\) according to (4). Since \(}_{em}}\) as the sample size of the training data set \(n\), we have \(}_{em}--c_{n}>0\) for some constant \(c_{n}>0\) and \(c_{n} 0\) as \(n\)._

_However, Theorem 1 seems to imply \(}_{em}(^{(k)}) 0\) as \(k\) since \(}_{em}(^{(K)})_{}\) and \(_{}\) is arbitrary, and it seems to contradict the \(->0\) lower bound. However, there is no contradiction due to the combination of two facts. First, the theorem states that for arbitrary \(_{}>0\), there exists a critical size, such that for overparameterized network beyond this size, GD can render the loss \(}_{em}()\) eventually no greater than \(_{}\). If we fix the network size, i.e., with \(m,L,d\) given, then \(K\) is given, and Theorem 1 says nothing about GD's behavior after \(K\) iterations. That is, we do not know whether \(_{k}}_{em}(^{(k)})=0\). Second, our optimization setting requires the sample size \(n\) to be smaller than the network width \(m\) (Assumption 1). Thus, when \(m\) is fixed, the sample size \(n\) is upper bounded._

_The above discussion implies, within the validity of our theory, for any fixed network width \(m\), if \(_{}\) is small, the sample size \(n\) cannot be too large, meaning \(}_{em}()-}()\) may not be small. Therefore, we can simultaneously have \(}_{em}()\) close to 0 and \(}()\) close to \(->0\)._

**Main technical steps for proving Theorem 1.** The proof of Theorem 1 is in Appendix D, where the analysis framework is adapted from Allen-Zhu et al. . Roughly speaking, the key proof in this framework is to establish the lower bound of the gradient. Then by integrating it into the semi-smoothness property of the neural network, we can obtain the exponential rate of convergence of gradient descent. For the lower bound of gradient, we develop a new method to deal with the difficulties in the denoising score matching setting (see the discussions earlier in this section).

Our new proof technique adopts a different decoupling of the gradient and leverages a high probability bound based on a high-dimensional geometric idea. See Appendix D.1 for a proof sketch and more details.

### Sampling

In this section, we establish a nonasymptotic error bound of the backward process in the variance exploding setting, which is an extension to Benton et al. . For simplified notations, denote the backward time schedule as \(\{t_{j}^{}\}_{0 j N}\) such that \(0=t_{0}^{}<t_{1}^{-}<<t_{N}^{}=T-\).

**Generation algorithm**. We consider the exponential integrator scheme for simulating the backward SDE (2) with \(f_{t} 0\)2. The generation algorithm can be piecewisely expressed as a continuous-time SDE: for any \(t[t_{j}^{-},t_{j+1}^{})\),

\[d_{t}=2_{T-t}^{2}S(;T-t_{j}^{},_{t_{j}^{ }})dt+^{2}}d_{t}.\] (10)

**Initialization**. Denote \(q_{t}:=(_{t})\) for all \(t[0,T-]\). We choose the Gaussian initialization, \(q_{0}=(0,_{T}^{2})\).

Our convergence result relies on the following assumption.

**Assumption 3**.: _The distribution \(P_{0}\) has a finite second moment: \(_{x P_{0}}[\|x\|^{2}]=_{2}^{2}<\)._

Next we state the main convergence result, whose proof is provided in Appendix E.

**Theorem 2**.: _Under Assumption 3, for any \((0,1)\) and \(T>1\), we have_

\[(p_{}|q_{T-})_{2}^{2}}{_{T}^{2}}}_{_{I}}+^{N-1} _{j}_{T-t_{j}^{-}}^{2}_{Y_{t_{j}^{-}} p_{T-t_{j}^{-}} }[|S(;T-t_{j}^{-},Y_{t_{j}^{+}})- p_{T-t_{j}^{-}}(Y_{t_{j}^{-} })|^{2}]}_{E_{S}}\] (11) \[+d^{N-1}_{j}_{t_{j}^{-}}^{t_{j+1} ^{-}}^{4}}{_{T-t}^{4}}dt+_{2}^{2} {_{0}^{t_{1}^{-}}_{T-t}^{2}dt}{_{T}^{4}}+(_{2 }^{2}+d)_{j=1}^{N-1}(1-e^{-_{T-t_{j}^{-}}^{2}})_{T-t_{j}^{-}}^{4}-_{T-t_{j+1}^{-}}^{2}}{_{T-t_ {j-1}^{-}}^{2}_{T-t_{j}^{-}}^{4}}}_{^{-}}^{2}_{T-t_{j}^{-}}^{4}}.\]

_where \(_{j}:=t_{j+1}^{*}-t_{j}^{-}\) for all \(j=0,1,,N-1\) is the stepsize of the generation algorithm in (10)._

Theorem 2 is a VESDE-based diffusion model's analogy of what's proved in Benton et al.  for VPSDE-based diffusion model, only requiring the data distributions to have finite second moments, and it achieves the sharp almost linear data dimension dependence under the exponential time schedule. The major differences from  are (1) the initialization error in the VESDE case is handled differently (see Lemma 10); (2) Theorem 2 applies to varies choices of time schedules, which enables to investigate the design space of the diffusion model, as we will discuss in Section 4. Worth mentioning is, Yang et al.  also obtained polynomial complexity results for VESDE-based diffusion models with uniform stepsize, but under stronger data assumption (assuming compact support). Compared to their result, complexity implied by Theorem 2 has better accuracy and data dimension dependencies. A detailed discussion on complexities is given in Appendix I.1.

Terms \(E_{I},E_{D},E_{S}\) in (11) represent the three types of errors: initialization error, discretization error, and score estimation error, respectively. Term \(E_{I}\) quantifies the error between the initial density of the sampling algorithm \(q_{0}\) and the ideal initialization \(p_{T}\), which is the density when the forward process stops at time \(T\). Term \(E_{D}\) is the error stemming from the discretization of the backward dynamics. Term \(E_{S}\) characterizes the error of the estimated score function and the true score, and is related to the optimization error of \(}_{em}\). Important to note is, in Theorem 2, population loss is needed instead of the empirical version \(}_{em}\) (7). Besides this, the weighting \(_{j}_{T-t_{j}^{-}}^{2}\) is not necessarily the same as the total weighting in \(}_{em}\) (7) \(_{j}\), depending on choices of \(w(t_{j})\) and time and variance schedules (see more discussion in Section 4). We will later on integrate the optimization error (Theorem 1) into this score error \(E_{S}\) to obtain a full error analysis in Section 3.3.

**Remark 2** (sharpness of dependence in \(d\) and \(_{2}^{2}\)).: _In one of the simplest cases, when the data distribution is Gaussian, the score function is explicitly known. Hence \((p_{}|q_{T-})\) can be explicitly computed as well, which verifies that the dependence of parameters \(d\) and \(_{2}^{2}\) is sharp in \(E_{I}\) and \(E_{D}\)._

### Full error analysis

In this section, we combine the analyses from the previous two sections to obtain an end-to-end generation error bound.

Before providing the main result of this section, let us first clarify some terminologies.

**Time schedule, variance schedule, and total weighting.** The terms _time schedule_ and _variance schedule_ respectively refer to the choice of \(t_{j}^{-}\) and \(_{t_{j}}\) in sampling. Meanwhile, note both the training and sampling processes require the proper choices of time and variance, and these choices are not necessarily the same for both processes. For training, the effect of these two is integrated into the _total weighting_\(_{j}\), which is also influenced by an additional weighting parameter \(w(t_{j})\). In this theoretical paper, when studying the generation error, we aim to apply the optimization result to better understand the effect of optimization on sampling. Therefore, to simplify the analysis and discussions in Section 4, we choose the same time and variance schedules for both training and sampling.

The main result is stated in the following.

**Theorem 3**.: _Under the same conditions as Theorem 1,2, and that \(K\) is such that GD reaches \(_{}\) in at most \(K\)th iterations, we have_

\[(p_{}|q_{T-}) E_{I}+E_{D}+_{1 j N} }^{2}}{w(t_{N-j})}(_{}+_{ n}+_{}+_{})\]

_where \(E_{I},E_{D}\) are defined in Theorem 2, \(_{}\) is defined in Theorem 1, \(_{n}=|}(^{(K)})-}_{em}(^{(K )})+}_{em}(^{*})-}(^{*})|\), \(_{}=|}(^{*})-}( _{})|\), \(_{}=|}(_{})+|\). In these terms, \(\) is defined in (5),\(^{}=_{,s.t.,}_{em}()=0}}()\) and \(_{F}=_{\{:S()\}}|}( )+|\) with \(=\{), with }d=(((nN))),m= n,N,d,L,T/t_{0}\}\)._

In this theorem, the discretization error \(E_{D}\) and initialization error \(E_{I}\) are the same as Theorem 2. For the score error \(E_{S}\), our optimization result is valid for general time schedules and therefore can directly fit into the sampling error analysis, which is in contrast to existing works [25; 44] (see more discussions in Section 1.1). The coefficient \(_{j}^{2}_{t_{N-j}}w(t_{N-j})\) results from different weightings in \(E_{S}\) and \(}_{em}\), i.e., \(_{j}^{2}_{T-t_{j}^{}}\) and \(_{j}\). We will discuss the effect of \(_{j}^{2}_{t_{N-j}}/w(t_{N-j})\) under different time and variance schedules in Section 4.

The way we bound \(_{Y_{t_{j}^{}}-p_{T-t_{j}^{}}}[\|S(;T-t_{j}^{}, Y_{t_{j}^{}})- p_{T-t_{j}^{}}(Y_{t_{j}^{}})\|^{2}]\) in \(E_{S}\) (see Theorem 2) is to decompose it into the optimization error \(_{ train}\), statistical error \(_{n}\), estimation error \(_{ est}\), and approximation error \(_{ approx}\). This gives clear intuition to results, but we also note it may not give a tight bound. In fact, we have

\[_{n}+_{ train} =|}(^{(K)})-}_{em}( ^{(K)})+}_{em}(^{})-}(^{ })|+|}_{em}(^{(K)})-}_{em}(^ {})|\] \[}(^{(K)})+}(^{ }) 2_{}}()-2>0.\]

\(_{n}\) can still be small if we take \(n\), but that means \(_{ train}\) has to be large, and our generation error bound cannot be made 0. It is unclear yet whether this is due to limitation of our analysis or intrinsic, and will be left for future investigation.

Another related note is, in this paper, we focus on \(_{ train}\) and the effect of optimization, but the analyses of \(_{n},\ _{ est},\ \ _{ approx}\) are also important and possible [13; 39; 25; 50]. On the other hand, again, whether it is optimal to decompose the full error into these four is unclear.

To better see the parameter dependence of the error bound in Theorem 3, the following is an example with simplified results, where we employ the schedules in EDM .

**Corollary 1** (Full error analysis under EDM  designs).: _Under the same conditions as Theorem 3, we have_

\[KL(p_{}|q_{T-})_{2}^{2}}{T^{2}}+T^{}}{^{}N}+(_{2}^{2}+d)(T^{}}{^{}N}+T^{}}{ ^{}N^{2}})+(C_{9}+(1-C_{8}h(}}{n^{3}N^{2}}))^{K}),\]

_where \(C_{8},C_{9}>0\) and \(a=7\) in ._

4 Theory-based understanding of the design space and its relation to existing empirical counterparts

This section theoretically explores preferable choices of parameters in both training and sampling, and shows that they agree with the ones used in EDM  and Song et al.  in different circumstances.

### Choice of total weighting for training

This section develops the optimal total weighting \(_{j}\) for training objective (7). We qualitatively show in two steps that "bell-shaped" weighting, which is the one used in EDM , will lead to the optimal rate of convergence: Step 1) \(|_{t_{j}}S(;t_{j},X_{ij})+_{ij}|\) as a function of \(j\) is inversely "bell-shaped"; Step 2) \(f(;i,j)=_{j}\|_{t_{j}}S(;t_{j},X_{ij})+_{ij}\|\) should be close to each other for any \(i,j\).

1.1 Inversely "bell-shaped" loss \(\|_{t_{j}}S(;t_{j},X_{ij})+_{ij}\|\) as a function of time index \(j\)

**Proposition 1**.: _Under the same assumptions as Theorem 1, for any \(\) and \(i=1,,n\), we have_

1. \(_{1}>0\)_,_ \(\ _{1}>0\)_, s.t., when_ \(0_{t_{j}}<_{1}\)_,_ \(\|_{t_{j}}S(;t_{j},x_{i}+_{t_{j}}_{ij})+_{ ij}\|>\|_{ij}\|-_{1}\)_._
2. \(_{2}>0\)_,_ \(\ M>0\)_, s.t., when_ \(_{t_{j}}>M\)_,_ \(\|_{t_{j}}S(;t_{j},x_{i}+_{t_{j}}_{ij})+_{ ij}\| M^{2}(\|S(;t_{j},_{ij})\|-_{2})\)_._

The above proposition can be interpreted in the following way. Given any network \(S\), when \(_{t_{j}}\) is very small, 1 implies that \(\|_{t_{j}}S(;t_{j},x_{i}+_{t_{j}})+_{ij}\|\) is away from 0 by approximately \(|_{ij}|\) which is of order \(\) with high probability, i.e., it cannot be small. When \(_{t_{j}}\) is large, 2 shows that as it becomes larger and larger, i.e., as \(M\) increases, \(\|_{t_{j}}S(;t_{j},x_{i}+_{t_{j}})+_{ij}\|\) will also increase. Therefore, the function \(\|_{t_{j}}S(;t_{j},X_{ij})+_{ij}\|\) has most likely an inversely "bell-shaped" curve in terms of \(j\) dependence.

#### 4.1.2 Ensuring comparable values of \(f(;i,j)\) for optimal rate of convergence

**Corollary 2**.: _Under the same conditions as Theorem 1, for some large \(K^{}>0,\) if \(|f(^{(k+K^{})};i,j)-f(^{(k+K^{})};l,s)|\) holds for all \(k>0\) and all \((i,j),(l,s)\), with some small universal constant \(>0\), then we have, for some constant \(C_{7}>0\),_

\[}_{em}(^{(k+K^{})})(1-C_{7}h_{j=1, ,N}w(t_{j})(t_{j}-t_{j-1})_{t_{j}}(}{2}}}{n^{3}N^{2}}))^{k}}_{em}(^{(K^{ })}).\]

The above corollary shows that if \(f(^{(k)};i,j)\)'s are almost the same for any \(i,j\), then the decay ratio of the next iteration is minimized. More precisely, the index set \(^{(k)}\) defined in Theorem 1 is roughly the whole set \(\{1,,N\}\), and therefore \(w(t_{j^{*}})(t_{j^{*}}-t_{j^{*}-1})_{t_{j^{*}}}\) can be taken as the maximum value over all \(j\), which consequently leads to the optimal rate.

#### 4.1.3 "Bell-shaped" weighting: our theory and EDM

Combining the above two aspects, the optimal rate of convergence leads to the choice of total weighting \(_{j}\) such that \(f(;i,j)=_{j}_{t_{j}}S(;t_{j},X_{t_{j}})+ _{ij}\) is close to each other; as a result, the total weighting should be chosen as a "bell-shaped" curve as a function of \(j\) according to the shape of the curve for \(\|_{t_{j}}S(;t_{j},X_{t_{j}})+_{ij}\).

Before comparing the preferable weighting predicted by our theory and the intuition-and-empirics-based one in EDM , let us first recall that the EDM training objective3 can be written as \(_{ p_{ train}}_{y,n}()\|D_{}(y+n;)-y\|^{2}\)

\[=}\, e^{--P_{ mean})^{2}}{2P_{  std}^{2}}}^{2}+_{ data}^{2}}{ _{ data}^{2}}_{X_{0},}\|s(;t,X_{t})+\|^{2 }\,d,\] (12)

where \(Z_{1}\) is a normalization constant, and we denote \(_{ EDM}()=e^{--P_{ mean})^{2}}{2P_ { std}^{2}}}^{2}+_{ data}^{2}}{ _{ data}^{2}}\) to be the _total weighting_ of EDM. Note the dependence on \(\) and time \(j\) can be freely switched due to their 1-to-1 correspondence.

Figure 2 plots the total weighting of EDM \(_{ EDM}\) as a function of \(\). As is shown in the picture, this is a "bell-shaped" curve4, which coincides with our choice of total weighting in the above theory. When \(\) is very small or very large, according to Proposition 1, the lower bound of \(\|_{t_{j}}S(;t_{j},X_{t_{j}})+_{ij}\) cannot vanish and therefore needs the smallest weighting over all \(\). When \(\) takes the middle value, the scale of the output data \(_{ij}/_{j}\) is roughly the same as the input data \(X_{ij}\) and therefore makes it easier for the neural network to fit the data, which admits larger weighting.

### Choice of time and variance schedules

This section will discuss the choice of time and variance schedules based on the three errors \(E_{S},E_{D},E_{I}\) in the error analysis of Section 3.3. Two situations will be considered based on how well the score function is approximated in training: when the network is less trained, \(E_{S}\) dominates and polynomial schedule  is preferable; when the score function is well approximated, \(E_{D}+E_{I}\) dominates and exponential schedule  is better.

#### 4.2.1 When score error \(E_{s}\) dominates

As is shown in Theorem 3, the main impact of different time and variance schedules on score error \(E_{S}\) appears in the term \(_{j}_{t_{N-j}}^{2}w(t_{N-j})\), when the score function is approximated to a certain accuracy. It remains to compute \(w(t)\) under various choices of schedules.

**General rule of constructing \(w(t)\).** To ensure fair comparisons between different time and variance schedules, we maintain a fixed total weighting in the training objective. Additionally, to facilitate comparisons with practical usage, we adopt the total weighting in EDM, i.e., \(_{j}=C_{3}_{ EDM}(_{t_{j}}),\) for some universal constant \(C_{3}>0\). The reason for using the EDM total weighting is that according to Section 4.1, our total weighting \(_{j}\) should be "bell-shaped" as a function of \(j\), which agrees qualitatively with the one used in EDM.

**Polynomial schedule  vs exponential schedule .** We fix \(_{n},_{ train}\) and apply the two schedules (Table 1) separately to the above total weighting \(\) (hence \(w\)). Then, compute \(_{j}_{t_{N-j}}^{2}/w(t_{N-j})\) which is a factor in score error \(E_{S}\) (Thm.3) in Table 2. The Exp.'s result \((_{}-_{}(_{}^{2}}{_{}^{2}})^{1/N})\) is larger5 than the Poly.'s result \((_{}-(_{}^{1/}-_{}^{1/}-_{}^{1/}}{N})^{})\) for large \(N\), meaning the poly. time schedule in EDM is better than the exp. schedule in . Note these two terms are both of order \(1/N\) as \(N\) and therefore the difference lies in their prefactors.

#### 4.2.2 When discretization error \(E_{d}\) and initialization error \(E_{l}\) dominate

In this section, we compare the two different schedules in Table 1 by studying the iteration complexity of the sampling algorithm, i.e., number of time points \(N\), when \(E_{D}\) + \(E_{I}\) dominates.

**General rules of comparison.** We consider the case when the discretization and initialization errors are bounded by the same quantity \(\), i.e., \(E_{I}\) \(\). Then according to Theorem 2 and Theorem 3, we compute the iteration complexity for achieving this error using the two schedules in Table 1. To make the comparison more straightforward, we adopt \(T=t_{N}=((^{-1}))\) and therefore \(_{}=(^{-1/2})\). More details are provided in Appendix I.1.

**Polynomial schedule  vs exponential schedule .** As is shown in the last column of Table 2, the iteration complexity under exponential schedule  has the poly-logarithmic dependence on the ratio between maximal and minimal variance (\(_{}/_{}\))6, which is better than the complexity under polynomial schedule , which is polynomially dependent on \(_{}/_{}\). Both complexities are derived from Theorem 2 by choosing different parameters.

**Remark 3** (The existence of optimal \(\) in the polynomial schedule ).: _For fixed \(_{}\) and \(_{}\), the optimal \(\) that minimizes the iteration complexity is \(=(_{}}{_{}})\). In , it was empirically observed that with fixed iteration complexity, there is an optimal value of \(\) that minimizes the FID. Our result indicates that, for fixed \(_{}\) and \(_{}\), hence the desired accuracy in KL divergence being fixed, there is an optimal value of \(\) that minimizes the iteration complexity to reach the fixed accuracy. Even though we consider a different metric/divergence instead of FID, our result still provides a quantitative support to the existence of optimal \(\) observed in ._