# Counterfactual Evaluation of

Peer-Review Assignment Policies

 Martin Saveski

University of Washington

msaveski@uw.edu

&Steven Jecmen

Carnegie Mellon University

sjecmen@cs.cmu.edu

&Nihar B. Shah

Carnegie Mellon University

nihars@cs.cmu.edu

&Johan Ugander

Stanford University

jugander@stanford.edu

###### Abstract

Peer review assignment algorithms aim to match research papers to suitable expert reviewers, working to maximize the quality of the resulting reviews. A key challenge in designing effective assignment policies is evaluating how changes to the assignment algorithm map to changes in review quality. In this work, we leverage recently proposed policies that introduce randomness in peer-review assignment--in order to mitigate fraud--as a valuable opportunity to evaluate counterfactual assignment policies. Specifically, we exploit how such randomized assignments provide a positive probability of observing the reviews of many assignment policies of interest. To address challenges in applying standard off-policy evaluation methods, such as violations of positivity, we introduce novel methods for partial identification based on monotonicity and Lipschitz smoothness assumptions for the mapping between reviewer-paper covariates and outcomes. We apply our methods to peer-review data from two computer science venues: the TPDP'21 workshop (95 papers and 35 reviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We consider estimates of (_i_) the effect on review quality when changing weights in the assignment algorithm, e.g., weighting reviewers' bids vs. textual similarity (between the review's past papers and the submission), and (_ii_) the "cost of randomization", capturing the difference in expected quality between the perturbed and unperturbed optimal match. We find that placing higher weight on text similarity results in higher review quality and that introducing randomization in the reviewer-paper assignment only marginally reduces the review quality. Our methods for partial identification may be of independent interest, while our off-policy approach can likely find use in evaluating a broad class of algorithmic matching systems.

## 1 Introduction

The assignment of papers to reviewers is one of the most important parts of the peer-reviewed publication process [1; 2; 3]. In computer science, conferences are the primary terminal venue for publications, with recent iterations of large conferences such as NeurIPS and AAAI receiving several thousand submissions . As a result, these and other large conferences must rely on automated systems to decide what members of the impaneled reviewer pool will review each paper. Review matching systems typically use three sources of information: (_i_) bids, i.e., reviewers' self-reported preferences to review the papers; (_ii_) text similarity between the paper and the reviewer's publications; and (_iii_) reviewer- and author-selected subject areas. Given a prescribed way to combine thesesignals into a single score, an optimization procedure then proposes a reviewer-paper assignment that maximizes the sum of the scores of the assigned pairs .

The design of peer-review systems has received considerable research attention [4; 6; 7; 8]. Popular peer-review platforms such as OpenReview and Microsoft CMT offer many features that conference organizers can use to assign reviewers. However, it has been persistently challenging to evaluate how changes to peer-review assignment algorithms affect review quality. An implicit assumption underlying such approaches is that review quality is an increasing function of bid enthusiasm, text similarity, and subject area match, but how to combine these signals into a score is approached via heuristics. Researchers typically observe only the reviews actually assigned by the algorithm and have no way of measuring the quality of reviews under an assignment generated by an alternative algorithm.

One approach to comparing different assignment policies is running A/B tests. Several conferences (NeurIPS'14 [9; 10], WSDM'17 , ICML'20 , and NeurIPS'21 ) have run A/B tests to evaluate various aspects of their review process, such as differences between single- vs. double-blind review. However, such experiments are extremely costly in the peer review context, with the NeurIPS experiments requiring a significant number of additional reviews, overloading already strained peer review systems. Moreover, A/B tests typically compare only a handful of design decisions, while assignment algorithms typically require making many such decisions (Section 2).

In this work, we propose off-policy evaluation as a less costly alternative that exploits existing randomness to enable the comparison of many alternative policies. Our proposed technique "harvests"  the randomness introduced in peer-review assignments generated by recently-adopted techniques that counteract fraud in peer review. In recent years, in-depth investigations have uncovered evidence of rings of colluding reviewers in a few computer science conferences . These reviewers conspire to manipulate the paper assignment in order to give positive reviews to the papers of co-conspirators. To mitigate this kind of collusion, conference organizers have adopted various techniques, including a recently introduced randomized assignment algorithm . This algorithm limits the maximum probability (a parameter set by the organizers) of any reviewer getting assigned any particular paper. This randomization thus limits the expected rewards of reviewer collusion at the cost of some reduction in the expected sum-of-similarities objective, and has been implemented in OpenReview since 2021 and used by several conferences, including AAAI'22 and AAAI'23.

The key insight of the present work is that under this randomized assignment policy, a range of reviewer-paper pairs other than the exactly optimal assignment become probable to observe. We can then adapt the tools of off-policy evaluation and importance sampling to evaluate the quality of many alternative policies. A major challenge, however, is that off-policy evaluation assumes overlap between the on-policy and the off-policy, i.e., that each reviewer-paper assignment that has a positive probability under the off-policy also had a positive probability under the on-policy. In practice, positivity violations are inevitable even when the maximum probability of assigning any reviewer-paper pair is low enough to induce significant randomization, especially as we are interested in evaluating a wide range of design choices of the assignment policy. To address this challenge, we build on existing literature for partial identification and propose methods that bound the off-policy estimates while making weak assumptions on how positivity violations arise.

More specifically, we propose two approaches for analysis that rely on different assumptions on the mapping between the covariates (e.g., bid, text similarity, subject area match) and the outcome (e.g., review quality) of the reviewer-paper pairs. First, we assume _monotonicity_ in the covariates-outcome mapping. Understood intuitively, this assumption states that if reviewer-paper pair \(i\) has higher or equal bid, text similarity, and subject area match than a reviewer-paper pair \(j\), then we assume that the quality of the review for pair \(i\) is higher or equal to the review for pair \(j\). Alternatively, we assume _Lipschitz smoothness_ in the covariate-outcome mapping. Intuitively, this assumption captures the idea that two reviewer-paper pairs that have similar bids, text similarity, and subject area match, should result in a similar review quality. We find that this Lipschitz assumption naturally generalizes so-called _Manski bounds_, the partial identification strategy that assumes only bounded outcomes.

We apply our methods to data collected by two computer science venues that used the recently-introduced randomized assignment policy: the 2021 Workshop on Theory and Practice of Differential Privacy (_TPDP_) with 95 papers and 35 reviewers, and the 2022 AAAI Conference on Advancement in Artificial Intelligence (_AAAI_) with 8,450 papers and 3,145 reviewers. We evaluate two design choices: (_i_) how varying the weights of the bids vs. text similarity vs. subject area match (latter available only in AAAI) affects the overall quality of the reviews, and (_ii_) the "cost of randomization",i.e., how much the review quality decreased as a result of introducing randomness in the assignment. As our measure of assignment quality, we consider the expertise and confidence reported by the reviewers for their assigned papers. We find that our proposed methods for partial identification assuming monotonicity and Lipschitz smoothness significantly reduce the bounds of the estimated review quality off-policy, leading to more informative results. Substantively, we find that placing a larger weight on text similarity results in higher review quality, and that introducing randomization in the assignment leads to a very small reduction in review quality.

Beyond our contributions to the design and study of peer review systems, the methods proposed in this work should also apply to other matching systems such as recommendation systems [18; 19; 20], advertising , and ride-sharing assignment systems . Further, our contributions to off-policy evaluation under partial identification should be of independent interest.

Our code is available at: https://github.com/msaveski/counterfactual-peer-review.

## 2 Preliminaries

We start by reviewing the fundamentals of peer-review assignment algorithms.

**Reviewer-Paper Similarity.** Consider a peer review scenario with a set of reviewers \(\) and a set of papers \(\). Standard assignment algorithms for large-scale peer review rely on "similarity scores" for every reviewer-paper pair \(i=(r,p)\), representing the assumed quality of review by that reviewer for that paper. These scores \(S_{i}\), typically non-negative real values, are commonly computed from a combination of up to three sources of information: (1) text-similarity scores \(T_{i}\) between each paper and reviewer's past work, using various techniques [23; 24; 25; 26; 27; 28]; (2) overlap \(K_{i}\) between the self-nominated subject areas selected by each reviewer and each paper's authors; and (3) reviewer-provided "bids" \(B_{i}\) on each paper. Without any principled methodology for evaluating the choice of similarity score, conference organizers manually select a parametric functional form and choose parameter values by spot-checking a few reviewer-paper assignments. For example, a simple similarity function is a convex combination of the component scores: \(S_{i}=w_{}T_{i}+(1-w_{})B_{i}\). Conferences have also used more complex non-linear functions: NeurIPS'16  used the functional form \(S_{i}=(0.5T_{i}+0.5K_{i})2^{B_{i}}\), while AAAI'21  used \(S_{i}=(0.5T_{i}+0.5K_{i})^{1/B_{i}}\). The range of possible functional forms results in a wide design space, which we explore in this work.

**Deterministic Assignment.** Let \(Z\{0,1\}^{||||}\) be an assignment matrix where \(Z_{i}\) denotes whether the reviewer-paper pair \(i\) was assigned or not. Given a matrix of reviewer-paper similarity scores \(S_{ 0}^{||||}\), a standard objective is to find an assignment of reviewers to papers that maximizes the sum of similarities of the assigned pairs, subject to constraints that each paper is assigned to an appropriate number of reviewers, each reviewer is assigned no more than a maximum number of papers, and conflicts of interest are respected [5; 27; 30; 31; 32; 33; 34]. This optimization problem can be formulated as a linear program. We provide a detailed formulation in Appendix A. While other objective functions have been proposed [35; 36; 37], here we focus on the sum-of-similarities objective.

**Randomized Assignment.** As one approach to strategyproofness, Jecmen et al.  introduce the idea of using randomization to prevent colluding reviewers and authors from being able to guarantee their assignments. Specifically, the program chairs first choose a parameter \(q\). Then, the algorithm computes a randomized paper assignment, where the marginal probability \(P(Z_{i}=1)\) of assigning any reviewer-paper pair \(i\) is at most \(q\). These marginal probabilities are determined by a linear program, which maximizes the expected similarity of the assignment subject to the probability limit \(q\) (detailed formulation in Appendix A). A reviewer-paper assignment is then sampled using a randomized procedure that iteratively redistributes the probability mass placed on each reviewer-paper pair until all probabilities are either zero or one.

**Review Quality.** The above assignments are chosen based on maximizing the (expected) similarities of assigned reviewer-paper pairs, but those similarities may not be accurate proxies for the quality of review that the reviewer can provide for that paper. In practice, automated similarity-based assignments result in numerous complaints of low-expertise paper assignments from both authors and reviewers . Meanwhile, self-reported assessments of reviewer-paper assignment quality can be collected from the reviewers themselves after the review. Conferences often ask reviewers to score their _expertise_ in the paper's topic and/or _confidence_ in their review [6; 38; 29]. Other indicators of review quality can also be considered; e.g., some conferences ask "meta-reviewers" or other reviewersto evaluate the quality of written reviews directly [38; 39]. In this work, we consider self-reported expertise and confidence as our measures of review quality.

## 3 Off-Policy Evaluation

One attractive property of the randomized assignment described above is that while only one reviewer-paper assignment is sampled and deployed, many other assignments could have been sampled, and those assignments could equally well have been generated by some alternative assignment policy. The positive probability of other assignments allows us to investigate whether alternative assignment policies might have resulted in higher-quality reviews.

Let \(A\) be a randomized assignment policy with a probability density \(P_{A}\), where \(_{Z\{0,1\}^{||||}}P_{A}(Z)=1\); \(P_{A}(Z) 0\), \( Z\); and \(P_{A}(Z)>0\) only for feasible assignments \(Z\). Let \(B\) be another policy with density \(P_{B}\), defined similarly. We denote by \(P_{A}(Z_{i})\) and \(P_{B}(Z_{i})\) the marginal probabilities of assigning reviewer-paper pair \(i\) under \(A\) and \(B\) respectively. Finally, let \(Y_{i}\), where \(i=(r,p)\), be the measure of the quality of reviewer \(r\)'s review of paper \(p\).

We follow the potential outcomes framework for causal inference . Throughout this work, we will let \(A\) be the on-policy or the logging policy, i.e., the policy that the review data was collected under, while \(B\) will denote one of several alternative policies of interest. In Section 5, we will describe the specific alternative policies we consider in this work. Define \(N=_{i}Z_{i}\) as the total number of reviews, fixed across policies and set ahead of time. We are interested in the following estimands:

\[_{A}=_{Z P_{A}}[_{i }Y_{i}Z_{i}],_{B}=_{Z P_{B}}[ {1}{N}_{i}Y_{i}Z_{i}],\]

where \(_{A}\) and \(_{B}\) are the expected review quality under policy \(A\) and \(B\), respectively.

In practice, we do not have access to all \(Y_{i}\), but only those that were assigned. Let \(Z^{A}\{0,1\}^{||||}\) be the assignment sampled under the on-policy A, drawn from \(P_{A}\). We define the following Horvitz-Thompson estimators of the means:

\[_{A}=_{i}Y_{i}Z_{i} ^{A},_{B}=_{i} Y_{i}Z_{i}^{A}W_{i},W_{i}=(Z_{i})}{P_{A}(Z_{i})}\; i .\] (1)

For now, suppose that \(B\) has positive probability only where \(A\) is positive (also known as satisfying "positivity"): \(P_{A}(Z_{i})>0\) for all \(i\) where \(P_{B}(Z_{i})>0\). Then, all weights \(W_{i}\) where \(P_{B}(Z_{i})>0\) are bounded. As we will see, many policies of interest \(B\) go beyond the support of \(A\).

Under the positivity assumption, \(_{A}\) and \(_{B}\) are unbiased estimators of \(_{A}\) and \(_{B}\) respectively . Moreover, the Horvitz-Thompson estimator is admissible in the class of all unbiased estimators . Note that \(_{A}\) is the empirical mean of the observed assignment sampled on-policy, and \(_{B}\) is a weighted mean of the observed assignment based on inverse probability weighting. These estimators also rely on a standard causal inference assumption of "no interference"; i.e., that the outcomes \(Y_{i}\) do not depend on the assignments \(Z_{j}^{A}\) for any other reviewer-paper pair \(j i\). In Appendix B, we discuss the implications of this assumption in the peer review context.

**Challenges.** In off-policy evaluation, we are interested in evaluating a policy \(B\) based on data collected under policy \(A\). However, our ability to do so is typically limited to policies where the assignments that would be made under \(B\) are possible under \(A\). In practice, many interesting policies step outside of the support of \(A\). Outcomes for reviewer-paper pairs outside the support of \(A\) but with positive probability under \(B\) ("positivity violations") cannot be estimated and must either be imputed by some model or have their contribution to the average outcome (\(_{B}\)) bounded.

In addition to positivity violations, we identify three other mechanisms through which missing data with potential confounding may arise in the peer review context: absent reviewers, selective attrition, and manual reassignments. For absent reviewers, i.e., reviewers who have not submitted _any_ reviews, we do not have a reason to believe that the reviews are missing due to the quality of the reviewer-paper assignment. Hence, we assume that their reviews are missing at random, and impute them with the weighted mean outcome of the observed reviews. For selective attrition, i.e., when some but not all reviews are completed, we instead employ conservative bounding techniques as for policy-based positivity violations. Finally, reviews might be missing due to manual reassignments by the program chairs, after the assignment has been sampled. As a result, the originally assigned reviews will be missing and new reviews will be added. In such cases, we treat removed assignments as attrition (i.e., bounding their contribution) and ignore the newly introduced assignments as they did not arise from any determinable process. Concretely, we partition the reviewer-paper pairs into the following (mutually exclusive and exhaustive) sets:

\(^{-}\): positivity violations, \(\{i=(r,p):P_{A}(Z_{i})=0 P_{B}(Z_{i})>0\}\),

\(^{Abs}\): missing reviews where the reviewer was absent (submitted no reviews),

\(^{Att}\): remaining missing reviews, and

\(^{+}\): remaining pairs without positivity violations or missing reviews, \(()(^{Att}^{ Abs}^{-})\).

In the next section, we present methods for imputing or bounding the contribution of \(^{-}\) to the estimate of \(_{B}\), and \(^{Abs}\) and \(^{Att}\) to the estimates of \(_{A}\) and \(_{B}\).

## 4 Imputation and Partial Identification

In the previous section, we defined three sets of reviewer-paper pairs \(i\) for which outcomes \(Y_{i}\) must be imputed rather than estimated: \(^{-}\), \(^{Abs}\), \(^{Att}\). In this section, we describe varied methods for imputing these outcomes that rely on different strengths of assumptions, including methods that output point estimates and methods that output lower and upper bounds of \(_{B}\). In Section 5, we apply these methods to peer-review data from two computer science venues.

For missing reviews where the reviewer is absent (\(^{Abs}\)), we assume that the reviewer did not participate in the review process for reasons unrelated to the assignment quality (e.g., too busy). Specifically, we assume that the reviewers are missing at random and thus impute the mean outcome among \(^{+}\), the pairs with no positivity violations or missing reviews: \(=(_{i^{+}}Y_{i}Z_{i}^{A}W_{i})/(_{i ^{+}}Z_{i}^{A}W_{i})\).

In contrast, for positivity violations (\(^{-}\)) and the remaining missing reviews (\(^{Att}\)), we allow for the possibility that these reviewer-paper pairs being unobserved is correlated with their unobserved outcome. Thus, we consider imputing arbitrary values for \(i\) in these subsets, which we denote by \(Y_{i}^{}\) and place into a matrix \(Y^{}^{||||}\), leaving entries for \(i^{-}^{Att}\) undefined. This strategy corresponds to setting \(Y_{i}=Y_{i}^{}\) for \(i^{-}^{Att}\) in estimator (1). To obtain bounds, we impute both the assumed minimal and maximal values of \(Y_{i}^{}\).

These modifications result in a Horvitz-Thompson off-policy estimator with imputation. To denote this, we redefine \(_{B}\) to be a function \(_{B}:^{||||}\), where \(_{B}(Y^{})\) denotes the estimator resulting from imputing entries from a particular choice of \(Y^{}\):

\[_{B}(Y^{})= (_{i^{+}}Y_{i}Z_{i}^{A}W_{i}+ _{i^{Att}}Y_{i}^{}Z_{i}^{A}W_{i}+_{i ^{Abs}}Z_{i}^{A}W_{i}+_{i^{-}}Y_{i} ^{}P_{B}(Z_{i})).\]

The estimator computes the weighted mean of the observed (\(Y_{i}\)) and imputed outcomes (\(Y_{i}^{}\) and \(\)). We impute \(Y_{i}^{}\) for the attrition (\(^{Att}\)) and positivity violation (\(^{-}\)) pairs, and \(\) for the absent reviewers (\(^{Abs}\)). Note that we weight the imputed positivity violations (\(^{-}\)) by \(P_{B}(Z_{i})\) rather than \(Z_{i}W_{i}\), since the latter is undefined. Under the assumption that the imputed outcomes are accurate, \(_{B}(Y^{})\) is an unbiased estimator of \(_{B}\). We can further construct confidence intervals using the methods described in Appendix C.

Next, we detail several methods by which we choose \(Y^{}\). These methods rely on various assumptions of different strength about the unobserved outcomes.

Mean Imputation.As a first approach, we assume that the mean outcome within \(^{+}\) is representative of the mean outcome among the other pairs. This is a strong assumption, since the presence of a pair in \(^{-}\) or \(^{Att}\) may not be independent of their outcome. For example, if reviewers choose not to submit reviews when the assignment quality is poor, \(\) is not representative of the outcomes in \(^{Att}\). Nonetheless, under this strong assumption, we can simply impute the mean outcome \(\) for all pairs necessitating imputation. Setting \(Y_{i}^{}=\) for all \(i^{-}^{Att}\), we consider the following point estimate of \(_{B}\): \(_{B}()\). While following from an overly strong assumption, we find it useful to compare our findings under this assumption to findings under subsequent weaker assumptions.

Model Imputation.Instead of simply imputing the mean outcome, we can assume that the unobserved outcomes \(Y_{i}\) are some simple function of known covariates \(X_{i}\) for each reviewer-paperpair \(i\). If so, we can directly estimate this function using a variety of statistical models, resulting in a point estimate of \(_{B}\). In doing so, we implicitly take on the assumptions made by each model, which determine how to generalize the covariate-outcome mapping from the observed pairs to the unobserved pairs. These assumptions are typically quite strong, since this mapping may be very different between the observed pairs (typically good matches) and unobserved pairs (typically less good matches).

Specifically, given the set of all observed reviewer-paper pairs \(=\{i^{+}:Z_{i}^{A}=1\}\), we train a model \(m\) using the observed data \(\{(X_{i},Y_{i}):i\}\). Let \(^{(m)}^{||||}\) denote the outcomes predicted by that model for each pair. We then consider \(_{B}(^{(m)})\) as a point estimate of \(_{B}\). In our experiments, we employ standard methods for classification, ordinal regression, and collaborative filtering: logistic regression (_clf-logistic_); ridge classification (_clf-ridge_); ordered logit (_ord-logit_); ordered probit (_ord-probit_); SVD++ collaborative filtering (_cf-svd++_); and K-nearest-neighbors collaborative filtering (_cf-knn_). Note that, unlike the other methods, the methods based on collaborative filtering model the missing data by using only the observed reviewer-paper outcomes (\(Y\)). We discuss our choice of methods, hyperparameters, and implementation details in Appendix E.

**Manski Bounds.** As a more conservative approach, we can exploit the fact that the outcomes \(Y_{i}\) are bounded, letting us bound the mean of the counterfactual policy without making any assumptions on how the positivity violations arise. Such bounds are often called _Manski bounds_ in the econometrics literature on partial identification. To employ Manski bounds, we assume that all outcomes \(Y\) can take only values between \(y_{}\) and \(y_{}\), e.g., self-reported expertise and confidence scores are limited to a pre-specified range on the review questionnaire. Then, setting \(Y_{i}^{}=y_{}\) or \(Y_{i}^{}=y_{}\) for all \(i^{-}^{Att}\), we can estimate the upper and lower bound of \(_{B}\) as \(_{B}(y_{})\) and \(_{B}(y_{})\). In Appendix C, we discuss how we construct confidence intervals for these estimates that asymptotically contain the true value of \(_{B}\) with probability at least \(95\%\) by adapting the inference procedure proposed in .

**Monotonicity and Lipschitz Smoothness.** We now propose two styles of weak assumptions on the covariate-outcome mapping that can be leveraged to achieve tighter bounds on \(_{B}\) than the Manski bounds. In contrast to the strong modeling assumptions used in the mean and model imputation, these assumptions can be more intuitively understood and justified as conservative assumptions given particular choices of covariates.

_Monotonicity._ The first weak assumption we consider is a _monotonicity_ condition. Intuitively, monotonicity captures the idea that we expect higher expertise for a reviewer-paper pair when some covariates are higher, all else equal. For example, in our experiments, we use the similarity component scores (bids, text similarity, subject area match) as covariates. Specifically, for covariate vectors \(X_{i}\) and \(X_{j}\), define the _dominance_ relationship \(X_{i} X_{j}\) to mean that \(X_{i}\) is greater than or equal to \(X_{j}\) in all components and \(X_{i}\) is strictly greater than \(X_{j}\) in at least one component. Then, the monotonicity assumption states that: if \(X_{i} X_{j}\), then \(Y_{i} Y_{j}\), \((i,j)()^{2}\).

Using this assumption to restrict the range of possible values for the unobserved outcomes, we seek upper and lower bounds on \(_{B}\). Recall that \(\) is the set of all observed reviewer-paper pairs. One challenge is that the observed outcomes themselves (\(Y_{i}\) for \(i\)) may violate the monotonicity condition. Thus, to find an upper or lower bound, we compute _surrogate values_\(_{i}\) that satisfy the monotonicity constraint for all \(i^{Att}^{Abs}^{-}\) while ensuring that the surrogate values \(_{i}\) for \(i\) are as close as possible to the outcomes \(Y_{i}\). The surrogate values \(_{i}\) for \(i^{Att}^{-}\) can then be imputed as outcomes.

Inspired by isotonic regression , we implement a two-level optimization problem. The primary objective minimizes the \(_{1}\) distance between \(_{i}\) and \(Y_{i}\) for pairs with observed outcomes \(i\). The second objective either minimizes (for a lower bound) or maximizes (for an upper bound) the sum of \(Y_{i}\) for the unobserved pairs \(i^{Att}^{Abs}^{-}\), weighted as in \(_{B}\). Define the universe of relevant pairs \(=^{Att}^{Abs}^{-}\) and define \(\) as a very large constant. This results in the following pair of optimization problems:

\[(_{min}^{M},_{max}^{M})= *{argmin}_{_{i}:i} _{i}|_{i}-Y_{i}|(_{i ^{Att}^{Abs}}_{i}W_{i}+_{i ^{-}}_{i}P_{B}(Z_{i})),\] (2) s.t. \[_{i}_{j} (i,j)\{^{2}:X_{i} X_{j}\}, y_{ }_{i} y_{} i.\]

The sign of the second objective term depends on whether a lower (negative) or upper (positive) bound is being computed. The last set of constraints corresponds to the same constraints used to construct the Manski bounds described earlier, which is combined here with monotonicity to jointly constrain the possible outcomes. The above problem can be reformulated and solved as a linear program using standard techniques. We can then estimate the upper and lower bound of \(_{B}\) as \(_{B}(_{min}^{Mon})\) and \(_{B}(_{max}^{Mon})\), and construct 95% confidence intervals as discussed in Appendix C.

Lipschitz Smoothness.The second weak assumption we consider is a _Lipschitz smoothness_ assumption on the correspondence between covariates and outcomes. Intuitively, this captures the idea that we expect two reviewer-paper pairs who are very similar in covariate space to have similar expertise. For covariate vectors \(X_{i}\) and \(X_{j}\), define \(d(X_{i},X_{j})\) as some notion of distance between the covariates. Then, the Lipschitz assumption states that there exists a constant \(L\) such that \(|Y_{i}-Y_{j}| Ld(X_{i},X_{j})\) for all \((i,j)()^{2}\). In practice, we can choose an appropriate value of \(L\) by studying the many pairs of observed outcomes in the data (Section 5 and Appendix G), though this approach assumes that the Lipschitz smoothness of the covariate-outcome function is the same for observed and unobserved pairs.

As in the previous section, we introduce surrogate values \(_{i}\) and implement a two-level optimization problem to address Lipschitz violations within the observed outcomes (i.e., if two observed pairs are very close in covariate space but have different outcomes). The resulting pair of optimization problems is defined exactly as in problem (2), except that the first set of constraints is replaced with constraints \(|_{i}-_{j}| Ld(X_{i},X_{j}),(i,j) \). We include the complete LP formulation in Appendix D. We denote the solutions to these problems by \(_{min}^{L}\) and \(_{max}^{L}\). As before, the sign of the second objective term depends on whether a lower (negative) or upper (positive) bound is being computed. The last set of constraints are again the same constraints used to construct the Manski bounds described earlier, which here are combined with the Lipschitz assumption to jointly constrain the possible outcomes. In the limit, as \(L\), the Lipschitz constraints become vacuous and we recover the Manski bounds. This problem can again be reformulated and solved as a linear program using standard techniques. We can then estimate the upper and lower bound of \(_{B}\) as \(_{B}(_{min}^{L})\) and \(_{B}(_{max}^{L})\), and construct 95% confidence intervals as discussed in Appendix C.

## 5 Experiments

We apply our framework to data from two venues that used randomized paper assignments as described in Section 2, the 2021 Workshop on Theory and Practice of Differential Privacy (_TPDP_) and the 2022 AAAI Conference on Advancement in Artificial Intelligence (_AAAI_). In both settings, we aim to understand the effect that changing parameters of the assignment policies would have on review quality. The analyses were approved by our Institutional Review Boards.

Datasets.TPDP.The TPDP workshop received 95 submissions and had a pool of 35 reviewers. Each paper received exactly 3 reviews, and reviewers were assigned 8 or 9 reviews, for a total of 285 assigned reviewer-paper pairs. The reviewers were asked to bid on the papers and could place one of the following bids (the corresponding value of \(B_{i}\) is shown in the parenthesis): _"very low"_ (\(-1\)), _"low"_ (\(-0.5\)), _"neutral"_ (\(0\)), _"high"_ (\(0.5\)), or _"very high"_ (\(1\)), with _"neutral"_ as the default. The similarity for each reviewer-paper pair was defined as a weighted sum of the bid score, \(B_{i}\), and text-similarity scores, \(T_{i}\): \(S_{i}=w_{}T_{i}+(1-w_{})B_{i}\), with \(w_{}=0.5\). The randomized assignment was run with an upper bound of \(q=0.5\). In their review, the reviewers were asked to assess the alignment between the paper and their _expertise_ (between \(1\): irrelevant and \(4\): very relevant), and to report their review _confidence_ (between \(1\): educated guess and \(5\): absolutely certain). We consider these two responses as our measures of quality.

AAAI.In the AAAI conference, submissions were assigned to reviewers in multiple sequential stages across two rounds of submissions. We examine the stage of the first round where the randomized assignment algorithm was used to assign all submissions to a pool of "senior reviewers." The assignment involved 8,450 papers and 3,145 reviewers; each paper was assigned to one reviewer, and each reviewer was assigned at most 3 or 4 papers based on their primary subject area. The similarity \(S_{i}\) for every reviewer-paper pair \(i\) was based on three scores: text-similarity \(T_{i}\), subject-area score \(K_{i}\), and bid \(B_{i}\). Bids were chosen from the following list (with the corresponding value of \(B_{i}\) shown in the parenthesis, where \(_{}=1\) is a parameter scaling the impact of positive bids as compared to neutral/negative bids): _"not willing"_ (\(0.05\)), _"not entered"_ (\(1\)), _"in a pinch"_ (\(1+0.5_{}\)), _"willing"_ (\(1+1.5_{}\)), _"eager"_ (\(1+3_{}\)). The default option was _"not entered"_. Similarities were computed as: \(S_{i}=(w_{}T_{i}+(1-w_{})K_{i})^{1/B_{i}}\), with \(w_{}=0.75\). The actual similarities differed from this base similarity formula in a few special cases (e.g., missing data); we provide the full description of the similarity computation in Appendix F. The randomized assignment was run with \(q=0.52\). Reviewers reported an _expertise_ score (between 0: not knowledgeable and 5: expert) and a _confidence_ score (between 0: not confident and 4: very confident), which we consider as our quality measures.

**Assumption Suitability.** For both the monotonicity and Lipschitz assumptions (as well as the model imputations), we work with the covariates \(X_{i}\), a vector of the two (TPDP) or three (AAAI) component scores used in the computation of similarities. In Appendix G, we consider whether these assumptions are reasonable with respect to our choice of covariates by evaluating them on the observed outcomes. For the monotonicity assumption, we find that it holds on large majorities of observed outcome pairs. For the Lipschitz assumption, we use a normalized \(_{1}\) distance and choose values of \(L\) corresponding to less than \(10\%\), \(5\%\), and \(1\%\) violations on the observed outcomes.

**Results.** We now present the analyses of the two datasets using the methods introduced in Section 4. For brevity, we report our analysis using self-reported expertise as the quality measure \(Y\), but include the results using self-reported confidence in Appendix L. When solving the LPs that output alternative randomized assignments (Appendix A), we often encounter multiple unique optimal solutions and employ a persistent arbitrary tie-breaking procedure to choose amongst them (Appendix H).

_TPDP._ We perform two analyses on the TPDP data, shown in Figure 1 (left). First, we analyze the choice of how to interpolate between the bids and the text similarity when computing the composite similarity score for each reviewer-paper pair. We examine a range of assignments, from an assignment based only on the bids (\(w_{}=0\)) to an assignment based only on the text similarity (\(w_{}=1\)), focusing our off-policy evaluation on deterministic assignments (i.e., policies with \(q=1\)). Setting \(w_{}[0,0.75]\) results in very similar assignments, each of which has Manski bounds overlapping with the on-policy (Figure 1A, left). Within this region, the models (Figure 1B, left), monotonicity bounds (Figure 1C, left), and Lipschitz bounds (Figure 1D, left) all agree that the expertise is similar to the on-policy. However, setting \(w_{}(0.75,0.9)\) results in a significant improvement in average expertise, even without any additional assumptions (Figure 1A, left). Finally, setting \(w_{}(0.9,1]\) leads to assignments that are significantly different from the assignments supported by the on-policy, which results in many positivity violations and wider confidence intervals, even under the monotonicity and Lipschitz smoothness assumptions (Figures 1A-D, left). Note that within this region, the models significantly disagree on the expertise (Figure 1B, left), indicating that the strong assumptions made by such models may not be accurate. Altogether, these results suggest that putting more weight on the text similarity (versus bids) leads to higher-expertise reviews.

Second, we investigate the "cost of randomization" to prevent fraud, measuring the effect of increasing \(q\) and thereby reducing randomness in the optimized random assignment. We consider values between \(q=0.4\) and \(q=1\) (optimal deterministic assignment). Recall the on-policy has \(q=0.5\). When varying \(q\), we find that except for a small increase in the region around \(q=0.75\), the average expertise for policies with \(q>0.5\) is very similar to that of the on-policy (Figures 1A-D, right). These results suggest that using a randomized instead of a deterministic policy does not lead to a significant reduction in self-reported expertise, an observation that should be contrasted with the previously documented reduction in the expected sum-similarity objective under randomized assignment ; see further analysis in Appendix I.

_AAAI._ We perform three analyses on the AAAI data, shown in Figure 1 (right). First, we examine the effect of interpolating between the text-similarity scores and the subject area scores by varying \(w_{}\), again considering only deterministic policies (i.e., \(q=1\)). The on-policy sets\(w_{ text}=0.75\). Due to large numbers of positivity violations, the Manski bounds are uninformative (Figure 1E, left), so we turn to the other estimators. The model imputation analysis indicates that policies with \(w_{ text} 0.75\) may have slightly higher expertise than the on-policy and indicates lower expertise in the region where \(w_{ text} 0.5\) (Figure 1F, left). However, the models differ somewhat in their predictions for low \(w_{ text}\), indicating that the assumptions made by these models may not be reliable. The monotonicity bounds more clearly indicate low expertise compared to the on-policy when \(w_{ text} 0.25\), but are also slightly more pessimistic about the \(w_{ text} 0.75\) region than the models (Figure 1G, left). The Lipschitz bounds indicate slightly higher than on-policy expertise for \(w_{ text} 0.75\) and potentially suggest slightly lower than on-policy expertise for \(w_{ text} 0.25\) (Figure 1H, left). Overall, all methods of analysis indicate that low values of \(w_{ text}\) result in worse assignments, but the effect of considerably increasing \(w_{ text}\) is unclear.

Figure 1: Expertise of off-policies varying \(w_{ text}\) and \(q\) for TPDP and \(w_{ text}\), \(_{ bid}\), and \(q\) for AAAI, computed using the different estimation methods described in Section 4. The dashed blue lines indicate Manski bounds around the on-policy expertise, and the grey lines indicate Manski bounds around the off-policy expertise. The error bands (denoted \(_{B}^{CI}\) for the Manski bounds, \(_{B|M}^{CI}\) for the monotonicity bounds, and \(_{B|L}^{CI}\) for the Lipschitz bounds) represent confidence intervals that asymptotically contain the true value of \(_{B}\) with probability at least \(95\%\), as described in Appendix C. We observe similar patterns using confidence as an outcome, as shown in Figure 7 in the Appendix. Note that to focus on the most relevant regions of the plots, the vertical axes do not start at zero.

Second, we examine the effect of increasing the weight on positive bids by varying the values of \(_{}\). Recall that \(_{}=1\) corresponds to the on-policy and a higher (respectively lower) value of \(_{}\) indicates greater (respectively lesser) priority given to positive bids relative to neutral/negative bids. We investigate policies that vary \(_{}\) within the range \(\), and again consider only deterministic policies (i.e., \(q=1\)). The Manski bounds are again too wide to be informative (Figure 1E, middle). The models all indicate similar values of expertise for all values of \(_{}\) and are all slightly more optimistic about expertise than the Manski bounds around the on-policy (Figure 1F, middle). The monotonicity and Lipschitz bounds both agree that the \(_{} 1\) region has slightly higher expertise as compared to the on-policy (Figure 1G-H, middle). Overall, our analyses provide some indication that increasing \(_{}\) may result in slightly higher levels of expertise.

Finally, we also examine the effect of varying \(q\) within the range \([0.4,1]\). Recall that the on-policy sets \(q=0.52\). We see that the models (Figure 1F, right), the monotonicity bounds (Figure 1G, right), and the Lipschitz bounds (Figure 1H, right) all strongly agree that the region \(q 0.6\) has slightly higher expertise than the region \(q[0.4,0.6]\). However, the magnitude of this change is small, indicating that the "cost of randomization" is not very significant.

Power Investigation: Purposefully Bad Policies.As many of the off-policy assignments we consider have relatively similar estimated quality, we also ran additional analyses to show that our methods can discern differences between good policies (optimized toward high reviewer-paper similarity assignments) and policies intentionally chosen to have poor quality ("optimized" toward low reviewer-paper similarity assignments). We refer the reader to Appendix J for further discussion.

## 6 Discussion and Conclusion

In this work, we evaluate the quality of off-policy reviewer-paper assignments in peer review using data from two venues that deployed randomized reviewer assignments. We propose new techniques for partial identification that allow us to draw useful conclusions about the off-policy review quality, even in the presence of large numbers of positivity violations and missing reviews. For a more extensive treatment of the methods proposed in this work, we refer the reader to Khan et al. .

**Limitations.** One limitation of off-policy evaluation is that our ability to make inferences inherently depends on the amount of randomness introduced on-policy. For instance, if there is a small amount of randomness, we will be able to estimate only policies that are relatively close to the on-policy unless we are willing to make some assumptions. The approaches presented in this work allow us to examine the strength of the evidence under a wide range of types and strengths of assumptions (model imputation, boundedness of the outcome, monotonicity, and Lipschitz smoothness) and to test whether these assumptions lead to converging conclusions. We emphasize that these assumptions (except for the boundedness of the outcome) are fundamentally unverifiable in our setting, given that the outcomes for unassigned pairs are unobserved. Thus, our resulting conclusions about the quality of alternative assignments cannot be directly verified without actually deploying the off-policies in question. Additionally, the measurement of review quality is a difficult question for any evaluation method: our analyses use the reviewers' self-reported expertise and confidence scores as proxies for review quality, but the self-reported nature of these measures makes them subject to various biases. Our conclusions should be interpreted in light of the possibility of misalignment between self-reported expertise/confidence and the true review quality. Finally, our substantive conclusions are based on analyses of data from two venues, and thus further work is needed to test their generalizability.

**Future Work.** In the context of peer review, the present work considers only a few parameterized slices of the vast space of reviewer-paper assignment policies, while there are many other substantive questions that our methodology can be used to answer. For instance, one could evaluate assignment quality under a different method of computing similarity scores (e.g., different NLP algorithms ), additional constraints on the assignment (e.g., based on seniority or geographic diversity ), or objective functions other than the sum-of-similarities (e.g., various fairness-based objectives [35; 47; 48; 36]). Additional thought should also be given to the trade-offs between maximizing review quality and broader considerations of reviewer welfare: while assignments based on high text similarity may yield slightly higher-quality reviews, reviewers may be more willing to review again if the assignment policy follows their bids more closely. Beyond peer review, our work is applicable to off-policy evaluation in other matching problems, including education [49; 50], advertising , and ride-sharing .