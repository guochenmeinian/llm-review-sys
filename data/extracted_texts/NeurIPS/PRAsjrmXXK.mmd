# Group Robust Preference Optimization

in Reward-free RLHF

 Shyam Sundhar Ramesh\({}^{1}\)

University College London (UCL)

&Yifan Hu

ETH Zurich, EPFL

&Iason Chaimalas

University College London (UCL)

&Viraj Mehta

TensorZero

&Pier Giuseppe Sessa

ETH Zurich

&Haitham Bou Ammar

University College London (UCL)

Huawei Noah's Ark Lab

&Ilija Bogunovic

University College London (UCL)

###### Abstract

Adapting large language models (LLMs) for specific tasks usually involves fine-tuning through reinforcement learning with human feedback (RLHF) on preference data. While these data often come from diverse labelers' groups (e.g., different demographics, ethnicities, company teams, etc.), traditional RLHF approaches adopt a "one-size-fits-all" approach, i.e., they indiscriminately assume and optimize a single preference model, thus not being robust to unique characteristics and needs of the various groups. To address this limitation, we propose a novel Group Robust Preference Optimization (GRPO) method to align LLMs to individual groups' preferences robustly. Our approach builds upon reward-free direct preference optimization methods, but unlike previous approaches, it seeks a robust policy which maximizes the worst-case group performance. To achieve this, GRPO adaptively and sequentially weights the importance of different groups, prioritizing groups with worse cumulative loss. We theoretically study the feasibility of GRPO and analyze its convergence for the log-linear policy class. By fine-tuning LLMs with GRPO using diverse group-based global opinion data, we significantly improved performance for the worst-performing groups, reduced loss imbalances across groups, and improved probability accuracies compared to non-robust baselines.

## 1 Introduction

As the usage of large language models (LLMs) has grown in recent years, the question of their _alignment_ has come to the forefront. Their remarkable capability to address a wide range of tasks (Radford et al. ) stems from pre-training on a self-supervised objective over internet-scale text. This vast internet-scale content, however, carries a higher risk of biases, inaccuracies, and controversial content than smaller, curated datasets. Thus, ensuring that the model's responses and behaviors correspond to human intentions and values is crucial.

Typical approaches to alignment  involve gathering preference feedback from human labelers to train models that reflect their desires. Such approaches often treat individual preferences as samples from a broader preference distribution. However, this perspective often oversimplifies the complex reality that human societies consist of numerous _distinct groups_ (e.g., different demographics, ethnicities, company teams, etc.), each with their own set of preferences that can significantlydiverge. Consequently, prevalent alignment strategies tend to adopt a "one-size-fits-all" model and disproportionately favor the preferences of the majority group, often at the expense of minority groups and their preferences, as illustrated in Figure 1.

To improve alignment performance for even the most disadvantaged groups, we propose to robustly solve the problem of diverse group preferences by (i) including group information in the context of the LLM and (ii) optimizing against the _worst-case_ alignment performance across all groups. We develop policies that guarantee equitable performance across all groups, ensuring that no group is disproportionately disadvantaged due to inherent biases or imbalances in the training data.

**Related work.** The established process for alignment of LLMs using Reinforcement Learning from Human Feedback (RLHF) is set out in [45; 60] and . The RLHF fine-tuning process consists of learning a reward model from human comparisons between responses to a given prompt, using the Bradley-Terry model . Then, one performs policy optimization using Proximal Policy Optimization  to learn a policy that maximizes the learned reward function. For a comprehensive overview and perspective of the RLHF topic, we refer the reader to [24; 7; 23].

Due to the challenges of tuning PPO and the vulnerability of reward models ([50; 37; 17; 49]), alternative approaches to PPO-based RLHF have been proposed, including rejection sampling fine-tuning [14; 18; 49; 30] and conditional supervised fine-tuning [20; 54; 9]. In particular, Rafailov et al.  introduce Direct Preference Optimization (DPO), which optimizes policies directly based on human preferences, avoiding the need for a separate reward model. This approach simplifies training and reduces reward overfitting. Other studies, such as [1; 58; 47; 43; 16], propose novel _reward-free_ RLHF methods, with some bypassing preference datasets altogether ([16; 6]). We utilize a reward-free framework similar to [37; 1], however, unlike previous works that assume a single preference distribution, we consider multiple preference distributions from diverse groups. Further, we aim to robustly fine-tune the LLM to ensure minimal disparity in performance across all groups. Other studies addressing robustness in preference optimization include  and . However, these works primarily focus on different aspects of robustness, such as robustness to noise and resilience against out-of-preference data.

Robust language modeling techniques have been studied by [33; 52] to optimize performance of language models over a wide-range of topics. They consider robust pre-training of language models based on the group Distributionally Robust Optimization (DRO) approach. A concrete theoretical study of the group DRO approach was performed in  and applied to vision problems. These are designed by extending previous minimax algorithm for solving DRO from . In the RLHF setup,  consider weighting of loss from different topics (harmless vs helpful) for robust reward learning. Also, Chakraborty et al.  consider robust policy optimization by learning multiple reward functions corresponding to sub-populations and learning a robust policy w.r.t. the learned

Figure 1: Current reward-free preference optimization methods typically optimize based on average human feedback. This often aligns predominantly with the preferences of the majority group (G1, R1 > R2) at the expense of minority groups (G2, R2 > R1). In contrast, our GRPO algorithm introduces adaptive weighting for different user groups and prioritizes optimizing for the worst-case group performance, leading to better alignment for the most disadvantaged groups.

rewards. Differing from these works, we embed group robustness directly into the reward-free tuning paradigm. We provide an concrete algorithm that adaptively weighs the loss for different groups and optimizes for a policy that minimizes the weighted loss. Further, our algorithm employs a novel gradient estimator tailored to the group robust DPO problem.

In the non-robust setup,  also explore group-based preference learning with LLMs by including group information in prompts with a transformer module that is trained optimally to choose an example sequence of prompts, LLM responses, and group preferences for in-context learning.  consider alignment with user preferences assuming that each user has varied importance over the distinct metrics in their multi-objective reward model. In contrast, we are not modeling multi-reward objectives but consider a reward-free setting. And, our methodology directly models each group's preferences through a group-dependent latent reward model where the group dependency is injected through the prompt. Further, unlike the non-robust problem setups of [57; 49], we consider the robust alignment problem optimizing for the worst group performance. We detail other related works extensively in Appendix A.

**Main Contributions.** The following are the main contributions of this work: (i) We present GRPO, the group robust formulation of Direct Preference Optimization (DPO) , wherein we augment the context of the LLM with the group information, and pose the problem as a robust optimization problem to minimize the worst-case loss amongst the diverse groups. We also show that a naive application of group robustness to the LLM policy maximization objective does not offer robustness benefits. To the best of our knowledge, this is the first study that focuses on group robustness in RLHF preference optimization; (ii) We analyze the theoretical aspects of GRPO by examining the convergence and the feasibility of finding optimal solutions within the log-linear policy class. (iii) We present a tailored algorithm to tackle this robust optimization challenge, providing convergence guarantees for certain loss functions; (iv) We show the versatility of our approach by demonstrating how our algorithm can be utilized with other reward-free preference optimization methods such as Identity Preference Optimization (IPO) . In particular, for the GR-IPO objective optimized over the log-linear policy class, we derive a closed-form weighted regression update for the policy parameters rather than a gradient update. To the best of our knowledge, this is a novel contribution towards efficient fine-tuning through preferential data; (v) Our empirical evaluations across synthetic datasets, real-world data, and publicly available LLMs show that the proposed GRPO significantly improves performance for the worst-performing groups, reduces loss imbalances across groups, and increases probability accuracies compared to non-robust baselines.

## 2 Background

We address the challenge of fine tuning a large language model (LLM) to align with user preferences. This process usually follows the Reinforcement Learning from Human Feedback (RLHF) protocol, using either an explicit reward model ([3; 34; 45; 60]) or an implicit reward model (). RLHF typically comprises three key phases: (i) supervised fine-tuning of an initial (pre-trained) large-language model, (ii) reward learning, and (iii) RL fine-tuning.

In the _supervised fine-tuning phase_ (SFT), the goal is to fine-tune a pre-trained LLM on a specific high-quality dataset suited for the downstream task of interest. It results in a probabilistic model expressing the probability of the response \(y\) given a prompt \(x\) as \(_{}(y|x)\). Subsequently, in the _reward learning phase_, the goal is to learn a reward model from a dataset of prompts \(x\) and responses \(y_{w}\), \(y_{l}\), with \(y_{l} y_{w} x\) meaning that human labellers preferred \(y_{w}\) over \(y_{l}\). It is typically assumed that preferences follow some choice models with an _unknown_ reward (utility) \(r^{*}(x,y)\) function. A popular model is the Bradley-Terry model  that assumes the preference distribution \(p\) admits the following form :

\[p(y_{1} y_{2} x)=(x,y_{2}))}{(r^{*}(x,y_{1}))+ (r^{*}(x,y_{2}))}.\] (1)

Based on the above model, a maximum likelihood estimate of the reward function is obtained as:

\[_{r}\{_{R}(r;)-_{(x,y_{w},y_{l}) }[((r(x,y_{w})-r(x,y_{l}))) ]\},\] (2)

where \(()\) is the sigmoid function and \(\) represents the dataset consisting of \(\{(x,y_{w},y_{l})\}\).

Then, in the RL fine-tuning phase the objective is to train a policy \(\) that maximizes the learned reward function. Simultaneously, the policy should stay closely aligned with the reference, \(_{}\), as quantified by the KL divergence, leading to the following KL-regularized optimization problem:

\[_{}\ _{x_{x}}_{y}[r(x,y )]-[(y|x)\|_{}(y|x)].\] (3)

**Direct Preference Optimization (DPO).** The recent approach proposed by  exploits the closed-form solution of the problem in Equation (3) and sidesteps the explicit modelling of rewards to directly optimize the policy. Specifically, under the Bradley-Terry preference model, the reward function can be expressed directly in terms of the optimal policy \(^{*}\) as follows:

\[r(x,y)=(y x)}{_{}(y x)}+ Z (x),\] (4)

for a partition function \(Z(x)=_{y}_{}(y|x)(r(x,y))\). Via a change of variable, finding the optimal reward function in Equation (2) is equivalent to finding the optimal policy \(^{*}\) utilizing the given set of preference data \(\). With a slight abuse of notation, we use \(\) to denote \(^{*}\). Denote \(h_{}(x,y_{w},y_{l}):=(|x)}{_{}(y_{w}|x)})- (|x)}{_{}(y_{l}|x)})\). Then, Equation (2) translates into the DPO loss:

\[_{}(,)=-*{}_{(x,y_ {w},y_{l})}( h_{}(x,y_{w},y_{l})).\] (5)

With a parameterized policy model \(_{}\), minimizing the DPO loss involves calculating the gradient over \(\) using backpropagation and the log-probabilities of each completion, \(y_{w}\) and \(y_{l}\), given the prompt \(x\) for both the policy \(_{}\) and the reference policy \(_{}\).

## 3 Group Robust Preference Optimization

In this section, we discuss Group Robust Preference Optimization (GRPO), i.e., instead of learning a reward function that maximizes the likelihood, we aim to derive (implicitly) a robust reward function and subsequently learn a robust DPO policy.

**Group Preferences.** Suppose that preferences come from an underlying latent reward \(r^{*}(x,y,g)\), with \(g=\{1,2,,K\}\) indexing the groups. When group information is available (e.g., as a text), we can represent the reward as \(r^{*}(x_{g},y)\), where \(x_{g}=x g\) denotes merging1 of the prompt with group information (e.g., string concatenation). We continue to apply a Bradley-Terry model as described in Equation (1), substituting \(x\) with \(x_{g}\). Moreover, we assume access to a collective dataset \(=_{g=1}^{K}_{g}\) where \(_{g}=\{(x_{g}^{(i)},y_{w}^{(i)},y_{l}^{(i)})\}_{i=1}^{N_{g}}\) with the available group information. Additionally, our dataset accommodates the exposure of different groups to identical prompts, meaning that the same \(x\) can appear across various groups \(g\) in our dataset, and these groups may favor different responses \(y\).

Given such \(\), although one may obtain a common reward model using Equation (2), it could result in poor generalization for particular groups, especially with significant group-wise disparities in the data (see Figure 1). Such disparities might stem from imbalanced data across groups or difficulties associated with learning different groups.

**GRPO Objective.** Consequently, we propose to measure the alignment of the reward model on the _worst-case group_ loss:

\[_{g G}\ _{R}(r;_{g}).\] (6)

Incorporating the reward expression from (Equation (4)) into (Equation (6)), we establish the _group robust preference optimization_ (GRPO) objective for a specified policy \(\):

\[_{}():=_{g}_{}( ,_{g})=_{g}-_{(x_{g},y_{w},y_ {l})_{g}} h_{}(x_{g},y_{ w},y_{l}).\] (7)

Leveraging the equivalent formulation of maximizing over discrete set, the GRPO problem becomes

\[_{}_{}()=_{}_{_{K-1}} _{g=1}^{K}_{g}-_{(x_{g},y_{w},y_{l})_{ g}}( h_{}(x_{g},y_{w},y_{l}) ,\] (8)where \(_{K-1}\) represents the \((K-1)\)-dimensional simplex of probabilities.2 The inner maximization becomes a linear programming over simplex such that \(\) represents the _weights_ of groups. In addition, it forms a two-player zero-sum game (see Section 3.1), where the policy \(\) and \(\) act as opponents with inversely related payoffs. The DPO loss (logistic log loss) in Equation (7) can be replaced with alternatives like hinge or squared loss (see ). We label this objective GR-DPO when using DPO loss, and explore GRPO with squared loss in Section 4.1.

**Applications.** In this study, we do not assume any specific distribution for groups \(_{g}\). The collection of prompts per group, \(_{x_{g}}\), may have varying degrees of overlap. The GRPO framework accommodates both distinct and overlapping prompt scenarios across different groups.

Apart from human groups, GRPO can be useful in scenarios where groups \(_{g}\) represent distinct tasks or topics within preference datasets, like helpful/harmful, truthful/unanswerable instances, or domain-specific categories (e.g., math, physics, chemistry). Typically, these prompt distributions \(\{_{x_{g}}\}_{q=1}^{N}\) are disjoint, and GRPO seeks to optimize performance even across the most challenging categories.

GRPO is also applicable in scenarios where groups reflect diverse user preferences for a _shared_ set of prompts, with the goal of achieving equitable performance across user groups. This contrasts with non-robust DPO, which aims to optimize preferences on average and might overlook minority groups.

Lastly, we acknowledge that the max-min objective of Equation (8) might be overly conservative, potentially degrading average performance. We explore a more balanced approach between worst-case and standard preference optimization objective in Appendix B.4.

### Further Discussion and Insights

This section provides two insights regarding the GR-DPO loss in Equation (8).

**Log-linear policy class.** The zero-sum game perspective allows us to explore the presence of a Nash equilibrium, serving as a benchmark for convergence during the policy optimization process. Given that the domain of \(\) is a simplex \(_{K}\) (in Equation (8)), we further define a parameterized policy class \(_{}\) for the policy \(_{}\). We assume that the parameterized policy \(_{}\) is of the form \(_{}(y x)=(x,y)}{_{y Y} f_{}( x,y)}\), where \(f_{}\) is a linear function or a neural network, and \(\) belongs to a convex set \(\).

In LLM fine-tuning, sometimes practitioners concentrate on modifying solely the final layer. It corresponds to a linear function, \(f_{}(x,y)=^{T}(x,y)\), with \((x,y)\) denoting the embedding derived from the language model removing its last layer, and \(\) as the parameters of the last layer. When applying this linear parameterization, in conjunction with a uniform reference policy \(_{}\), the robust objective outlined in Equation (8) is as follows (details in Appendix B.1):

\[_{}_{_{K-1}}_{g=1}^{K}_{g} -_{(x_{g},y_{w},y_{l})_{g}} (x,y_{w})-(x,y_{l}), .\] (9)

The objective defined in Equation (9) is concave with respect to \(\) and convex with respect to \(\). This structure allows the invocation of the minimax theorem for convex-concave functions () to assert the existence of a Nash equilibrium.

**Proposition 3.1**.: _Under log-linear parameterization of the policy class, there exists a Nash equilibrium for the group robust direct preference optimization problem in Equation (9)._

**Robust policy optimization.** The earlier derivation for the GR-DPO objective \(_{}()\) relies on incorporating robustness in the reward modeling step (in Equation (7)) while using the solution to the non-robust KL-regularized reward maximization objective in Equation (4).

Interestingly, we can obtain the identical expression for \(_{}()\) if incorporating robustness in the KL-regularized reward maximization objective and using the reward function learnt in a non-robust way. Consider the robust KL-regularized reward maximization

\[_{}_{g}_{x_{g}_{x_{g}},y (|x_{g})}r(x_{g},y)-(y x_{g})|| _{}(y x_{g}).\] (10)

The following proposition characterizes such an invariant property.

**Proposition 3.2**.: _Substituting the closed-form solution of the robust KL-regularized policy maximization problem (Equation (10)) into the robust reward maximization objective in Equation (6) leads to the same group robust DPO loss \(_{}\) in Equation (8)._

The analysis leverages the fact that the optimal policy of Equation (10) is identical to the solution of the non-robust KL-regularized reward maximization in Equation (4) and is derived in Appendix B.2.

## 4 Algorithm

In this section, we discuss the policy optimization algorithm for solving the group robust DPO problem in Equation (8). In particular, we aim to design an algorithm that performs updates in the parameterized space \(^{d}\), i.e., updating \(\) of the parameterized policy \(_{}\). Leveraging the perspective of the 2-player zero-sum game, we propose an alternating updating algorithm wherein one updates \(\) and \(\) alternatively. We summarize the overall approach in Algorithm 1, which we discuss and analyze next.

We employ the DPO loss \(l(_{};)=(( h_{_{}}()))\) (Equation (5)) in Algorithm 1, however, our algorithm can support other preference optimization losses (see Section 4.1). The algorithm performs a gradient descent type update on \(\) and a deterministic mirror ascent on \(\) using a Bregman divergence with the distance generating function as the KL divergence. Since the \(\) lies in a simplex and the objective is linear, the update of \(\) becomes multiplicative weights update with renormalization to a simplex via softmax (see Nemirovski et al.  for details). Further, the weights \(\) are determined by the cumulative losses \(l(_{};)\) accured by each group, ensuring that groups with higher cumulative losses get higher weights. The size of the group \(N_{g}\) appears as the empirical distribution \(_{g}\) involves \(N_{g}\). We call it alternating update as the updated \(^{t}\) is used in the update from \(^{t-1}\) to \(^{t}\). In particular, the gradient descent type update on \(\) is weighted by \(\) in order to orient the update towards groups with higher losses. The projection operator \(_{}\) ensures the updated \(^{t}\) lies within \(\).

```
1:Initialize: Step size \(_{}\) for group weights \(\), step size \(_{}\) for policy \(\) with weights \(\), initial weights \(^{(0)}\) of the policy and weights over each group \(^{(0)}\), Projection operator \(_{}\)
2:Input: Dataset \(\) with size \(N=||\), group size \(N_{g}\) for \(g=\{1,2,,K\}\), loss \(l(_{};)\)
3:for\(t=1,,T\)do
4:\(^{}^{(t-1)}\)
5:\(g(N_{1}/N,,N_{K}/N)\), \((x_{g},y_{w},y_{l})_{g}\)
6:\(^{}_{g}^{}_{g}_{} ;(x_{g},y_{w},y_{l}))}{N_{g}}\) // Update weights for group \(g\)
7:\(^{(t)}^{}/_{g^{}}^{}_{g^{ }}\) // Renormalize \(\)
8:\(^{(t)}_{}^{(t-1)}-_{} _{g}_{}l(_{(t-1)};(x_{g},y_{w},y_{l}))}{N_{g}}\) // Use \(\) to update \(\)
9:endfor
10:Return: Output the robust policy \((^{(T)})\) ```

**Algorithm 1** Mirror Descent for Group Robust Preference Optimization (GRPO)

**What does the weighted DPO update do?** In Line 9 in Algorithm 1, the algorithm performs parameter updates based on the weighted gradients. By using the DPO loss, i.e., \(l(_{};)=( h_{_{}}()\) (see Equation (5)), we obtain the following gradient update expression ignoring the \(N/N_{g}\) constant

\[^{(t)}_{g}_{}l(_{^{(t-1)}};(x_{g},y_{ w},y_{l}))=^{(t)}_{g}_{}( h_{_{ ^{(t-1)}}}(x_{g},y_{w},y_{l}))\] (11) \[=^{(t)}_{g}r_{^{(t-1)}}(x_{g},y_{l})-r_ {^{(t-1)}}(x_{g},y_{w})[_{}_{^{(t- 1)}}(y_{w}|x_{g})-_{}_{^{(t-1)}}(y_{l}|x_{g})].\]

The final term plays the critical role of enhancing the likelihood of the preferred response while simultaneously diminishing the likelihood of the rejected response. This adjustment is proportional to the disparity in rewards between the two responses. Moreover, the inclusion of \(_{g}\) is pivotal for ensuring group robustness. This coefficient scales the gradient w.r.t. \(\) based on the cumulative loss previously received by all samples within a specific group. Such a mechanism ensures that the model's focus is increasingly directed towards groups that have historically suffered higher losses. Additionally, the scaling factor \(N_{g}\) guarantees that groups with a smaller volume of data do not face a disadvantage.We defer further details in obtaining the gradient update expression to Appendix B.3.

We demonstrate the global convergence with the following proposition.

**Proposition 4.1**.: _Suppose that the loss \(l(;(x_{g},y,y^{}))\) is non-negative, convex, \(B_{}-\)Lipschitz continuous, and bounded by \(B_{l}\) for all \((x_{g},y,y^{}) \) and \(\|\|_{2} B_{}\) for all \(\) with convex \(^{d}\). The error of the average iterate of Algorithm 1, i.e., \(_{^{(1:T)}}=_{t=1}^{T}^{t}\), satisfies_

\[[_{}(_{^{(1:T)}})]-_{ }_{}(_{})=T^{-1/2} .\]

We defer the proof of this proposition to Appendix E. The analysis follows from an adaptation of the analysis in Nemirovski et al.  for the proposed sampling strategy in Algorithm 13. We note that when fine-tuning only the final layer of a LLM, the output policy exists within the log-linear policy class (see Section 3.1), and the corresponding loss function satisfies the assumptions in Proposition 4.1 (see Lemma E.1).

### Group Robust Identity Preference Optimization

The standard regularized reward maximization objective (Equation (3)) in DPO , tends to overlook the KL-regularization and learn deterministic policies. This learned policy assigns preference probability one to winning responses in the data which is often not realistic (see [Section 4.2] and Appendix C). Recently, Azar et al.  show that the standard regularized reward maximization objective (Equation (3)) in DPO  tends to overlook the KL-regularization and learn deterministic policies (see [1, Section 4.2] and Appendix C). They thus propose an alternative approach called _Identity Preference Optimization_ (IPO) that is more likely to learn a randomized policy which assigns appropriate probability to the preferred response and prevents overfitting. Following a similar derivation as we did for group robust DPO with details given in Appendix C, we develop the corresponding group robust IPO (GR-IPO):

\[_{}\ _{}():=_{g}_{ }(,_{g})=_{_{K-1}}_{g=1}^{K} _{g}*{}_{(x_{g},y_{w},y_{l})_{g}}h_{}(x_{g},y_{w},y_{l})-^{2} .\]

For the log-linear policy class (introduced in Section 3.1), the objective function simplifies to

\[_{}_{_{K-1}}_{g=1}^{K}_{g} _{(x_{g},y_{w},y_{l})_{g}} (x_{g},y_{w})-(x_{g},y_{l}),-^{2} .\]

To solve the GR-IPO above, it suffices to use Algorithm 1 with slight modifications, see Algorithm 2 in Appendix C. In particular, the update of \(\) is replaced by a weighted regression update:

\[*{arg\,min}_{}_{(x_{g}, y_{w},y_{l})}}{N_{g}} (x_{g},y_{w})-(x_{g},y_{l}),-^{2} .\]

For fixed \(\), we show (in Appendix C) that such an update admits a closed-form solution:

\[=(S^{T}WS)^{-1}S^{T}W W:=[}}{N_{g^{(1)}}},,}}{N_{g^{(N)}}}],\]

where \(g^{(i)}\) is the group of each sample \(i\), \(N_{g^{(i)}}\) is the number of samples in group \(q^{(i)}\) and \(\) is a column vector of ones of dimension \(N\). Here \(S\) is a matrix \(S:=[((x_{g}^{(1)},y_{w}^{(1)})-(x_{g}^{(1)},y_{l}^{(1)}))^{T}, ,((x_{g}^{(N)},y_{w}^{(N)})-(x_{g}^{(N)},y_{l}^{(N)}))^{T}].\) Each row of \(S\) represents the difference in feature mappings \(\) of the preferred and less preferred response for each prompt. The group robust IPO (GR-IPO) algorithm is presented in Appendix C, and its empirical results are shown in Section 5.

## 5 Experiments

In this section, we study the empirical performance of our proposed Algorithm 1 on synthetic and real-world datasets4. First, we simulate multi-group data disparities by varying the size and preference distributions of two synthetic groups. In the real-world setup, we study the alignment of an LLM to the real preferences of people from various countries. We examine whether GRPO aligns the LLM in a more equitable manner to reduce discrepancies in alignment among various groups. Finally, we demonstrate that performance is improved by explicitly addressing the grouped nature of the data during alignment.

### Synthetic Experiments

We evaluate the performance of Algorithm 1 using synthetically generated group preference data for the loss function \(l(_{,.})\) - either DPO loss or IPO loss and denote them as GR-DPO and GR-IPO, respectively. We compare them against vanilla DPO and IPO (), and the importance-sampling (IS) variants of DPO and IPO (where the loss of each datapoint is inversely weighted by its group data size).

**Experimental Setup.** Our experiments are designed to analyze settings where there exist multiple groups with distinct characteristics. We adapt the standard (non-group based) experimental setup proposed by  for the group preferences setting by incorporating group information into the reward function \(r:\). Here, \(\) represents a two-dimensional state space \(\), \(\) denotes a discrete action space \(\{0,1,2,3,,n\}\), and \(\) signifies a discrete group space \(\{0,1,2,,K\}\). The reward function, defined by the group-dependent feature vector \((x,y,g)\) and parameter vector \(_{g}\), is given as \(r(x,y,g):=(x,y,g),_{g}\), while the feature vectors \((x,y,g)\) have a coordinate-flipped relationship and are defined in Appendix D.1.

We consider the following scenarios: **(i)** Groups are imbalanced in terms of size but have the same distribution over responses, **(ii)** Groups are balanced in terms of size but have different response distributions, and **(iii)** Groups are imbalanced in terms of size and also have different response distributions. Note that having different response distributions leads to a difference in the difficulty of learning, since groups with responses distant from each other (in terms of rewards or preference probabilities) are typically more distinguishable and easier to learn. We discuss in Appendix D.2 how we generate these three scenarios.

**Implementation.** Leveraging the linearity in the reward model, we utilize a log-linear policy class parameterized by \(\): \(_{}(y|x)=}{_{y^{}},g),)}}\). We run Algorithm 1 for both DPO and IPO loss relative to the policy class detailed above with a dataset of 300 action pairs with preferences.

**Evaluation Metrics.** We use the following criteria to assess the performance of the algorithms:

_Max Validation Loss._ For each group \(g\), with preference data denoted as \((x^{i},y^{i}_{w},y^{i}_{l},g)_{i=1}^{N_{g}}\), where \(N_{g}\) is the number of data points in the group, we compute the DPO/IPO validation loss separately for each group and identify the maximum loss among them in each run.

_Max Reward Error._ This metric compares the true reward of the optimal action determined by \(^{*}_{g}\) with that of the action deemed optimal by estimate \(\) for each group, and identifies the maximum error across all groups in each run. In particular, for data in the form \((x^{i},g)_{i=1}^{N_{g}}\), which includes only states and groups, we calculate reward errors for every group \(g\) as follows: \(_{(x,g)(x^{i},g)_{i=1}^{N_{g}}}[_{y}(x,y,g), ^{*}_{g}-(x,_{y}(x,y,g),,g),^{*}_{g}]\).

**Results.** We present the average performance of Algorithm 1 (error bars over 20 seeds) alongside baseline methods in Figure 2 for scenario **(iii)**, while scenarios **(i)** and **(ii)** are Figures 4 and 5

Figure 2: Synthetic experiments: Algorithm 1 (GR-DPO and GR-IPO) leads to a significantly lower worst-case validation loss and reward error compared to importance sampling (IS-DPO/IPO) and vanilla methods (DPO, IPO). Results refer to the scenario in which groups have different sizes and responsesâ€™ distribution.

in Appendix D.2 due to space constraints. Our findings indicate that the robust methods consistently surpass both vanilla and importance-sampling approaches. Notably, the robust methods demonstrate significant superiority in uneven group scenarios, where the importance-sampling technique falls short as it exclusively deals with data imbalance.

### Global Opinion Experiments

For the real-data experiments, we consider the survey dataset _GlobalOpinionQA_ () and the publicly available Gemma-2B model . 5 The data contains multiple choice questions answered by participants from various countries, amounting to 2,554 questions covering various topics, including politics, media, technology, religion, race, and ethnicity. For each question, the dataset provides a probability vector over the choices, signifying the percentage of people from a particular country choosing each option. Note that this probability vector would be different for different countries. Hence, the goal is to align the LLM to the probability vector corresponding to each country in a robust manner.

We consider the following five countries in the dataset: Nigeria, Egypt, India, China and Japan, with data sizes 572, 570, 376, 309, and 712, respectively. We construct our training set as follows: For the SFT training, we choose the best option (the choice with the highest probability) as the target. For both IPO and GR-IPO training, we consider the best option as the winning response and another randomly chosen option as the losing response. We outline the exact prompt we use in Appendix D.

We run the SFT training for one epoch over the training data on the pre-trained Gemma-2B model. For both IPO/GR-IPO training we use the AdamW  optimizer with adaptive learning rates. For SFT/IPO/GR-IPO training, we apply the LoRA strategy to fine-tune all layers of the model. We then evaluate both the methods based on the worst group loss and accuracy. Here, the loss refers to the IPO loss for each group and the accuracy refers to the percentage of winning response and losing response pairs correctly ordered by the learned preference function (Equation (35)). We defer further training and hyperparameter details to Appendix D.

**Results.** We present the average performance of GR-IPO over five seeds alongside IPO in Figure 3 (top plots). Our findings indicate that GR-IPO outperforms IPO in terms of maximum group loss and minimum group reward accuracies. Moreover, GR-IPO effectively reduces the imbalance in loss values among different groups. Additionally, we observe an improvement in log-probability accuracies (which measure if the probability assigned by the fine-tuned model is higher for the winning response compared to the losing response) for both IPO and GR-IPO, with GR-IPO demonstrating better alignment for the worst-performing group compared to IPO.

**Insights.** We further note that the worst-performing groups are Groups-2,5, as shown in Figure 3. GR-IPO improves the loss for these groups by assigning more weight to them, as illustrated in Figure 3 (bottom middle plot). Additionally, we plot the initial log-probability accuracies for different groups in Figure 3 (bottom right plot), assessing how accurately the SFT model classifies the winning versus losing response for different groups. It is evident that Groups-2,5 are already underperforming. Given that SFT training converges within one epoch without any discrepancies between groups, this indicates that the base LLM inherently struggles with classifying responses for Groups-2,5. However, by employing GR-IPO, we have mitigated the imbalance in the performance of the fine-tuned model.

## 6 Conclusions

We formalize the problem of robustly aligning an LLM to preference distributions from diverse groups. To tackle the same, we introduced GRPO, a group robust formulation of reward-free RLHF, aiming to minimize worst-case loss among groups. We explored the theoretical aspects of GRPO and demonstrated its improved robust alignment performance through various experiments. We believe our approach will be highly valuable for future tailored LLM fine-tuning, specifically aimed at aligning with the needs of diverse teams and user groups. In a broader context, it holds promise for mitigating biases and discrepancies across various societal groups encountered in the task-specific adaptation of LLMs.

**Limitations.** When the dataset is balanced among groups and difficulty levels are comparable, our GRPO approach does not offer a significant advantage over standard reward-free RLHF algorithms. In addition, minimax methods often improve the worst group's performance at the cost of reducing the average or best group's performance. Our proposed GRPO formulation (see Equation (8)),stemming from a minimax framework, will comply with the same property. Hence, in scenarios where optimizing worst-case performance is less critical, we define a _trade-off parameter_ to balance between the worst-case performance and the average performance. This modified objective and the necessary algorithmic changes are elaborated in Appendix B.4. The appropriate tuning of the trade-off parameter for the specific application remains a subject for future investigation. Further, we focus on settings with known groups, which are common in pluralistic alignment datasets and tasks (see ). When groups are unknown, one can still undertake several approaches such as clustering, representation learning, feature analysis, expert consultations, etc., to help uncover group structures in the data.

## 7 Acknowledgments

PGS was gratefully supported by ELSA (European Lighthouse on Secure and Safe AI) funded by the European Union under grant agreement No. 101070617. YH was supported as a part of NCCR Automation, a National Centre of Competence (or Excellence) in Research, funded by the Swiss National Science Foundation (grant number 51NF40_225155). IB was supported by the EPSRC New Investigator Award EP/X03917X/1; the Engineering and Physical Sciences Research Council EP/S021566/1; and Google Research Scholar award. SSR was supported by Department of Electronic and Electrical Engineering and the Institute of Communications and Connected Systems at UCL. The authors would like to thank William Bankes, Seongho Son, Matthieu Zimmer, Afroditi Papadaki, Eduardo Pignatelli, and Nagham Osman for the useful discussion.