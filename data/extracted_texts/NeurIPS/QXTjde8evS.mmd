# DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology

Marco Aversa

University of Glasgow and Dotphoton

Glasgow, United Kingdom

marco.aversa@glasgow.ac.uk

&Gabriel Nobis

Fraunhofer HHI

Berlin, Germany

gabriel.nobis@hhi.fraunhofer.de

&Miriam Hagele

Aignostics

Berlin, Germany

miriam.haegele@aignostics.com

&Kai Standvoss

Aignostics

Berlin, Germany

kai.standvoss@aignostics.com

&Mihaela Chirica

Institute of Pathology, LMU Munich

Munich, Germany

mihaela.chirica@med.uni-muenchen.de

&Roderick Murray-Smith

University of Glasgow

Glasgow, United Kingdom

roderick.murray-smith@glasgow.ac.uk

&Ahmed Alaa

UC Berkeley

Berkeley, California

amalaa@berkeley.edu

&Lukas Ruff

Aignostics

bErlin, Germany

lukas.ruff@aignostics.com

&Daniela Ivanova

University of Glasgow

Glasgow, United Kingdom

daniela.ivanova@glasgow.ac.uk

&Wojciech Samek

Fraunhofer HHI and TU Berlin

Berlin, Germany

wojciech.samek@hhi.fraunhofer.de

&Frederick Klauschen

Institute of Pathology, LMU Munich

Munich, Germany

f.klauschen@lmu.de

&Bruno Sanguinetti

Dotphoton

Zug, Switzerland

bruno.sanguinetti@dotphoton.com

&Luis Oala

Dotphoton

Zug, Switzerland

luis.oala@dotphoton.com

###### Abstract

We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiringsmall patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffIInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data.

## 1 Introduction

Deep learning (DL) models are promising auxiliary tools for medical diagnosis [1; 2; 3]. Applications like segmentation and classification have been refined and pushed to the limit on natural images . However, these models trained on rich datasets still have limited applications in medical data. While segmentation models rely on sharp object contours when applied to natural data, in medical imaging, the model struggles to detect a specific feature because it has a "limited ability to handle objects with missed boundaries" and often "miss tiny and low-contrast objects" [5; 6]. Therefore, task-specific medical applications require their own specialised and fine-grained annotation. Data labelling is arguably one of the most critical bottlenecks in healthcare machine learning (ML) applications. In histopathology, pathologists examine the histological slide at multiple levels, usually starting with a lower magnification to analyse the tissue architecture and cellular arrangement and gradually proceeding to a higher magnification to examine cell morphology and subcellular features, such as the appearance and number of nucleoli, chromatin density and cytoplasm appearance. Annotating features within gigapixel whole slide images (WSIs) with this level of detail demands effort and time, often leading to sparse, limited annotated data. In addition, due to privacy regulations and ethics [7; 8], having access to medical data can be challenging since it has been shown that it is possible to extract patients' sensitive information  from this data.

In histopathology, state-of-the-art ML models require the context of the entire WSIs, with features at different scales, in order to distinguish between different tumor sub-types, grades and stages . Despite the demonstrated effectiveness of diffusion models (DMs) in generating natural images compared to other approaches, they still have rarely been applied in medical imaging. Existing

Figure 1: a) Examples of synthetic and real \(2048 2048\) images. b) Pairs of \(512 512\) synthetic tiles (top) with the closest real images found with Inception-v3 near-neighbour (bottom).

generative models in histopathology can generate images of relatively small resolution compared to WSIs. To give a few examples, the application of Generative Adversarial Networks (GANs) in cervical dysplasia detection , glioma classification , and generating images of breast and colorectal cancer , generate images with \(256 128\) px, \(384 384\) px and \(224 224\) px, respectively. In spite of their current limitations in generating images at scales necessary to fully address all medical concerns, the use of synthetic data in medical imaging can provide a valuable solution to the persistent issue of data scarcity [14; 15; 16; 17]. Models generally improve after data augmentation and synthetic images are equally informative as real images when added to the training set [18; 19]. Data augmentation could also help with the underrepresentation in data sets of rare cancer subtypes. By adding synthetic images to the training set, Chen et al.  demonstrated that their model had better accuracy in detecting chromophobe renal cell carcinoma, which is a rare subtype of renal cell carcinoma. Furthermore, Doleful et al.  showed how synthetic histological images could be used for educational purposes for pathology residents. Regarding the challenges highlighted before, we present a novel sampling method to generate large histological images with long-range pixel correlation (see Fig. 1), aiming to extend up to the resolution of the WSI.

Our contributions are as follows: 1) We introduce DiffInfinite, a hierarchical generative framework that generates arbitrarily large images, paired with their segmentation masks. 2) We introduce a fast outpainting method that can be efficiently parallelized. 3) The quality of DiffInfinite data is evaluated by ten experienced pathologists as well as downstream machine learnings tasks (classification and segmentation) and anti-duplication metrics to assess the leakage of patient data from the training set.

## 2 Related Work

Large-content image generation can be reduced to inpainting/outpainting tasks. Image inpainting is the problem of reconstructing unknown or unwanted areas within an image. A closely related task is image outpainting, which aims to predict visual content beyond the boundaries of an image. In both cases, the newly in- or outpainted image regions have to be visually indistinguishable with respect to the rest of the image. Such image completion approaches can help utilise models trained on smaller patches for the purpose of generating large images, by initially generating the first patch, followed by its extension outward in the desired direction.

Traditional approachesTraditional methods for image region completion rely on repurposing known image features, necessitating costly nearest neighbour searches for suitable pixels or patches [22; 23; 24; 25; 26]. Such methods often falter with complex or large regions . In contrast, DL enables novel, realistic image synthesis for inpainting and outpainting. Some methods like Deep Image Prior  condition new image areas on the existing image, while others aim to learn natural image priors for realistic generation [28; 29].

Generative modelling for conditional image synthesisGANs have dominated image-to-image translation tasks like inpainting and outpainting for years [28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41; 42]. Recently, DMs have surpassed GANs in various image generation tasks . Palette  was the first to apply DMs to tasks like inpainting and outpainting. RePaint  and ControlNet  demonstrate resampling and masking techniques for conditioning using a pre-trained diffusion model. SinDiffusion  and DiffCollage  offer state-of-the-art outpainting solutions using DMs trained with overlapping patches. In parallel to our work, Bond-Taylor and Willcocks  developed a related approach called \(\)-Diff which trains on random coordinates, allowing the generation of infinite-resolution images during sampling. However, in contrast to our approach the method does not involve image compression in a latent space.

Synthetic data assessmentThe authenticity of synthetic data produced by DMs, trained on vast paired labelled datasets , remains contentious. Ethical implications necessitate distinguishing if generated images are replicas of training data [51; 52]. The task is complicated due to subjective visual similarities and diverse dataset ambiguities. Various metrics have been proposed for quantifying data replication, including information theory distances from real data , consistency measurements using downstream models [54; 55], comparison with inpainted areas , and detection of "forgotten" examples .

## 3 Preliminaries

Diffusion ModelsDMs [57; 58; 59] represent a class of parameterized Markov chains that effectively optimize the lower variational bound associated with the likelihood function of the unknown data distribution. By iteratively adding small amounts of noise until the image signal is destroyed and then learning to reverse this process, DMs can approximate complex distributions much more faithfully than GANs . The increased diversity of samples while preserving sample fidelity comes at the cost of training and sampling speed, with DMs being much slower than GANs . The universally adopted solution to this problem is to encode the images from pixel space into a lower dimensional latent space via a Vector Quantised-Variational AutoEncoder (VQ-VAE), and perform the diffusion process over the latents, before decoding back to pixel space . Pairing this with the Denoising Diffusion Implicit Models (DDIMs) sampling method  leads to faster sampling while preserving the DM objective

\[z_{t-1}=}(-}_{ }(z_{t},t)}{}})+-_{t}^{ 2}}_{}(z_{t},t)+_{t}_{t},\] (1)

where \(z_{t}\) is the latent variable at time step \(t\) in the VQ-VAE latent space, \(_{t}\) is the noise scheduler, \(_{}\) is the noise learned by the model and \(_{t}\) is random noise. Conditioning can be achieved either by specifically feeding the condition with the noised data [44; 63], by guiding an unconditional model using an external classifier [64; 65] or by classifier-free guidance  used in this work, where the

Figure 2: DiffImfinite generation method. a) Large-scale context mask generation. A diffusion model conditioned on a large-scale conditional prompt (e.g. Adenocarcinoma subtype) generates a low-resolution mask. The mask is upsampled via linear interpolation to the desired image size. b) Diffusion steps on large images. Given a random position, we select a sub-tile with its segmentation mask. A diffusion model generates in parallel the next step conditioned on each conditional label, or prompt, found in the mask. The outputs are masked individually with the corresponding label. The next step is the union of all the sub-patches. c) Tracking time steps pixel-wise. We keep track of the time step of each pixel in the large image. The model evolves only the pixels with the higher time step on each iteration.

convex combination

\[_{}(z_{t},c)=(1+)_{}(z_{t},c)- _{}(z_{t},),\] (2)

of a conditional diffusion model \(_{}(z_{t},c)\) and an unconditional model \(_{}(z_{t},)\) is used for noise estimation. The parameter \(\) controls the tradeoff between conditioning and diversity, since \(>0\) introduces more diversity in the generated data by considering the unconditional model while \(=0\) uses only the conditional model.

## 4 Infinite Diffusion

The DiffInfinite approach we present here1, is a generative algorithm to generate arbitrarily large images without imposing conditional independence, allowing for long-range correlation structural information. The method overcomes this limitation of DMs for large-content generation by deploying multiple realizations of a DM on smaller patches. In this section, we first define a mathematical description of this hierarchical generation model and then describe the sampling method paired with a masked conditioned generation process.

### The Method

Let \(X\) be a large-content generating random variable taking values in \(^{KD}\). Using the approach of latent diffusion models , the high-dimensional content is first mapped to the latent space \(^{D}\) by \((X)=Y_{}\). For simplicity, we assume throughout this work the existence of an ideal encoder-decoder pair \((,)\) such that \(((X))=X\) is the identity on \(^{KD}\). Assume further, to have a reverse time model \((SM_{},_{})\) at hand consisting of a sampling method \(SM_{}\) and a learned model \(_{}\) trained on small patches \(Z_{}\) taking values in \(^{d}\). The reverse time model transforms \(z_{T}(0,I_{d})\) over the time steps \(t\{T,T-1,...,1\}\) recursively by

\[z_{t-1}=SM_{}(z_{t})\] (3)

to an approximate instance of \(_{}\). We aim to sample instances from \(_{}\) by deploying multiple realizations of the reverse time model \((SM_{},_{})\). Towards that goal, define the set of projections

\[:=\{proj_{I}:^{D}^{d}\ |\ I \ d\ ^{D}\},\] (4)

where \(proj\) models a crop \(proj(Y)^{d}\) of \(d\) connected pixels from the latent image \(Y\). Since the model \(_{}\) is trained on images taking values in \(^{d}\) the standing assumption is

**Assumption 1**: _Any projection \(proj\) maps \(Y\) to the same distribution \(proj(Y)_{}\) in \(^{d}\)._

Since the goal is to approximate an instance of \(_{}\), we initialize the sampling method by \(y_{T}(0,I_{D})\) and proceed in the following way: Given \(y_{t}\), randomly choose \(proj_{I_{1}},...,proj_{I_{m}}\) independent of the state \(y_{t}\) such that \(proj_{I_{1}},...,proj_{I_{m}}\) are non equal crops that cover all latent pixels in \(^{D}\). To be more precise, for every \(i\{1,...,D\}\) we find at least one \(j\{1,...,m\}\) with \(i I_{j}\). For every projection \(proj_{I_{1}},...,proj_{I_{m}}\) we calculate the crop \(z_{t}^{j}=proj_{I_{j}}(y_{t})\) of the current state \(y_{t}\) and perform one step of the reverse time model following the sampling scheme

\[z_{t-1}^{j}=SM_{}(z_{t}^{j}), j\{1,...,m\}.\] (5)

This results in overlapping estimates \(z_{t-1}^{1},...,z_{t-1}^{m}\) of the subsequent state \(t-1\) and we simply assign to every pixel in the latent space the first value computed for this pixel such that

\[[y_{t-1}]_{i}=[z_{t-1}^{j}]_{l},j=j^{}\ |\ i I_{j^{}}}\] (6)

and \(l\) refers to the entry in \(z_{t-1}^{j}\) corresponding to \(i\) with \([proj_{I_{j}}(y_{t-1})]_{l}=[y_{t-1}]_{i}\). Hence, starting from \(y_{T}(0,I_{D})\) we sample in the first step from a distribution

\[y_{T-1} p_{T-1,}(y\,|\,y_{T},proj_{I_{1}},...,proj_{I_{m}}).\] (7)

Using Bayes' theorem, this distribution simplifies to

\[p_{T-1,}(y\,|\,y_{T},proj_{I_{1}},...,proj_{I_{m}})=p_{T-1,}(y\,| \,y_{T}),\] (8)since we sample the projections independently from \(y_{T}\). Repeating the argument, we sample in every step from a distribution \(y_{t-1} p_{t-1,}(y|y_{t},...,y_{T})\) over \(^{D}\) instead of sampling from \(z_{t-1} q_{t-1,}(z|z_{t},...,z_{T})\) over \(^{d}\). Hence, we approximate the true latent distribution \(_{}\) by the approximate distribution with density \(p_{0,}(y|y_{1},...,y_{T})\). In contrast to , our method does not use the assumption of conditional independence and the method can be applied to a wide range of DMs, without an adjustment of the training method. As the authors of  point out in their section on limitations, the assumption of conditional independence is not well-suited in cases of a data distribution with long-range dependence. For image generation in the medical context, we aim to circumvent this assumption as we do not want to claim that the density of a given region depends only on one neighboring region. The drawback of dropping the assumption is that we only approximate the reverse time model of the latent image distribution \(_{}\) indirectly, by multiple realizations of a reverse time model that approximates \(_{}\).

### Semi-supervised Guidance

In order to generate diverse high-fidelity data, DMs require lots of training data. Perhaps, training on a few samples still extracts significant features but it lacks variability, resulting in simple replicas. Here, we show how to enhance synthetic data diversity using classifier-free guidance as a semi-supervised learning method. In the classifier-free guidance , a single model is trained conditionally and unconditionally on the same dataset. We adapt the training scheme using two separate datasets. The model is guided by a small and sparsely annotated dataset \(q_{1}\), used for the conditional training step, while extracts features by the large unlabelled dataset \(q_{0}\), used on the unconditional training step (see Alg.1)

\[(z_{0},c)=(z_{0},) q_{0}(z_{0})&u p_{ unc}\\ (z_{0},c) q_{1}(z_{0},c)&,\] (9)

where \(u\) is sampled from a uniform distribution in , \(p_{unc}\) is the probability of switching from the conditional to the unconditional setting and \(\) is a null label. During the sampling, a tradeoff between conditioning and diversity is controlled via the parameter \(\) in eq.2.

### Sampling

High-level content generationThe outputs of DMs have pixel consistency within the training image size. Outpainting an area with a generative model might lead to unrealistic and odd artifacts due to poor long-range spatial correlations. Here, we show how to predict pixels beyond the image's boundaries by generating a hierarchical mapping of the data. The starting point is the generation of the highest-level representation of the data. In our case, it is the sketch of the cellular arrangement in the WSI (see Figure 1(a)). Since higher-frequency details are unnecessary at this stage, we can downsample the masks until the clustering pattern is still recognizable. The diffusion model, conditioned on the context prompt (e.g. Adenocarcinoma subtype), learns the segmentation masks which contain the cellular macro-structures information.

Random patch diffusionGiven a segmentation mask \(M\), we can proceed with the large image sampling according to Section 4.1 in the latent space \(^{D}\) of \(Y=(X)\) (see Alg.2). Since we trained a conditional diffusion model with conditions \(c_{1},...,c_{N}\), the learned model takes the form \(_{}(z_{t},t)=(_{}(z_{t},t|c_{1}),...,_{ }(z_{t},t|c_{N}))\). Given \(y_{t}\), we first sample projections \(proj_{I_{1}},...,proj_{I_{m}}\), corresponding to different crops of \(d\) connected pixels up to the \(m\)-th projection with \(_{j=1}^{m}I_{j}=\{1,...,D\}\) and \(_{j=1}^{m}I_{j}_{j=1}^{m-1}I_{j}+\) (see the left hand-side of Figure 1(b)). Note that \(m\) is not fixed, but varies over the sampling steps and is upper bounded by the number of possible crops of \(d\) connected pixels. The random selection of

Figure 3: Comparison of sampling speed for DiffCollage and DiffInfinite, measuring diffusion steps required for image sampling. Demonstrating increased efficiency of DiffInfinite for larger images.

the projection is implemented such that regions with latent pixels of low projection coverage are more likely. Secondly, we calculate for every projection \(j\{1,...,m\}\) the crop \(proj_{I_{j}}(y_{t})\) and perform one step of the DDIM sampling procedure using the classifier-free guidance model \((1+)_{}(z_{i}^{j},t,c)-_{}(z_{i}^{j},t,)\), where \(_{}\) is the learned model and \(z_{i}^{j}=proj_{I_{j}}(y_{t})\). This results for every pixels \(i I_{j}\) in \(N\) values \(DDIM_{,c_{1}}(proj_{I_{j}}(y_{t})),...,DDIM_{,c_{N}}(proj_{I_{j}}(y _{t}))\), one for every condition \(c_{i}\) (see the right hand-side of Figure 2b). If \(i I_{j^{}}\) for all \(j^{}\), the pixel \(i\) has not been considered yet and we assign \(i\) the value \([y_{t-1}]_{i}=[DDIM_{,M_{i}}(proj_{I_{j}}(y_{t}))]_{l}\), where \(l\) corresponds to the pixel \(i\) under the projection \(I_{j}\) and \(M_{i}\) is the value of \(i\) in the mask \(M\). Since we are updating random projections of the overall image, in the \(t\)-th step pixels either have the time index \(t\) or \(t+1\), resulting in a reversed diffusion process of differing time states. We initialize a tensor \(L_{t}\), with the same size \(D\) as the latent variable, to keep track of the time index for each pixel. Each element is set to \(L_{T} T\). In the \(j\)-th iteration of the \(t\)-th step we only update the pixels that have not been considered in one of the previous iterations of the \(t\)-th diffusion step, hence all the pixels in \(i I_{j}\) with \(proj_{I_{j}}(L_{t})_{i}=t+1\), similarly to the inpainting mask in the Repaint sampling method .

```
1: Randomly train on labelled or unlabelled data with probability \(p_{unc}\), \(u Uniform\)
2:\((z_{0},c)=\{(z_{0},c) q_{0}(z_{0}).\) if\(u p_{unc}\)then
3: Sample random time step
4:\(t Uniform\{1,...,T\}\)
5: Sample noise. \((0,)\)
6: Compute data, \(z_{1}= x_{0}+_{t}\)
7: Take gradient descent: step
8:\(_{}\|c-_{}(z_{1},t,c)\|^{2}\)
9:until converged ```

**Algorithm 1** DiffInfinite Training

```
1: High-level segmentation mask \(M^{D}\) and learned model \(_{}\)
2: Synthetic image \(X\) with the mask size initialization:
3:\(y_{T}(0,)\), index set \(I_{0}-\) and time state tensor \(L_{T} T\)
4:Repeat
5:for\(t(T-1,...0)\)do
6:while\(_{j=0}^{m}I_{j}+\{1,...,D\}\)do
7:\(m m+1\)
8: Select randomly \(proj_{I_{m}}(\{proj_{I_{1}},,proj_{I_{m-1}}\}\)
9: Crep \(z_{1}^{m}=proj_{I_{m}}(y_{t})\)
10:for all conditions \(n\{1,...,N\}\)do
11: DDM sampling with classifier-free guidance
12:\(z_{1}^{m}|_{1} p_{,c}(z_{1}^{m},z_{})\)
13:endfor
14:for all indices \(i I_{m}\)do
15:if\(i I_{j}\)do
16:\([y_{t-1}]_{i}[z_{t-1}^{m}|M_{i}]_{l}\)
17:\(proj_{I_{m}}(L_{t})_{i} t\)
18: such that \([proj_{I_{m}}(y_{t-1})]_{l}=[z_{t-1}^{m}|M_{i}]_{l}\)
19: end if
20: endfor
21: endwhile
22:endfor
23:\(X(y_{0})\) ```

**Algorithm 2** DiffInfinite Sampling

To restore the pixels that already received an update, i.e. every pixel \(i I_{j}\) with \(proj_{I_{j}}(L_{t})_{i}=t\), we store a replica of the previous diffusion step for every pixel. Finally, we update all the time states in \(L_{t}\) that received an update in the \(j\)-th iteration to \(t\) resulting in \(proj_{I_{j}}(L_{t})_{i}=t\) for all \(i I_{j}\). See the top row of Fig.2c for an illustration of the evolution of \(L_{t}\). The random patch diffusion can also be applied to mask generation, where the only condition is the context prompt. This method can generate segmentation masks of arbitrary sizes with the correlation length bounded by two times the training mask image size.

ParallelizationThe sampling method proposed has several advantages. In Zhang et al.  each sequential patch is outpainted from the previous one with 50\(\%\) of the pixels shared. Here, the randomization eventually leads to every possible overlap with the neighboring patches. This introduces a longer pixel correlation across the whole generated image, avoiding artifacts due to tiling. In Figure 3, we show that the number of steps in the whole large image generation process is drastically reduced with the random patching method with respect to the sliding window one. Moreover, in the sliding sampling method, the model can be paralleled only \(2\) or \(4\) times, depending if we are outpainting the image horizontally or on both axis. In our approach, we can parallelize the sampling up to the computational resource limit.

## 5 Data Assessment

To assess synthetic images for medical image analysis, we need to take various dimensions of data assessment into account. We extend traditional metrics from the natural image community with qualitative and quantitative assessments specific to the medical context. For the qualitative analysis, a team of pathologists evaluated the images for histological plausibility. The quantitative assessment entailed a proof-of-concept that a model can learn sensible features from the synthetically generated image patches for a relevant downstream task. As data protection is highly relevant regarding patient data, we performed evaluations to rule out memorization effects of the generative model.

### Traditional Fidelity

We evaluate the fidelity of synthetic \(512 512\) images by calculating Improved Precision (IP) and Improved Recall (IR) metrics between 10240 real and synthetic images .2 The IP evaluates synthetic data quality, while the IR measures data coverage. Despite their unsuitability for histological data , Frechet-Inception Distance (FID) and Inception Score (IS)  are reported for comparison with  and Shrivastava and Fletcher .3 The metrics' explanations and formulas can be found in Appendix C.

In Table 1 (left), we report an IP of \(0.94\) and an IR of \(0.70\), indicating good quality and coverage of the generated samples. However, we note that these metrics are only somewhat comparable due to the different types of images generated by MorphDiffusion  and NASDM . For the large images of size \(2048 2048\), we rely solely on the IP and IR for quantitative evaluation due to the limited number of \(200\) generated large images. As shown in Figure 3(a) of , FID is unsuitable for evaluating such a small sample size, while IP and IR are more reliable. In Table 1 (right), we find that generating images first results in slightly higher IR, while generating the mask first achieves an IP of 0.98. For the sake of completeness we also report the scores then combining the two datasets. To compare our method to DiffCollage we generate \(200\) images using . DiffInfinite performs better than DiffCollage wrt. to IP and IR. The drop of IR to \(0.22\) might be a result of the tiling artifacts observable in the LHS of Figure 11.

### Domain Experts Assessment

To assess the histological plausibility of our generated images, we conducted a survey with a cohort of ten experienced pathologists, averaging 8.7 years of professional tenure. The pathologists were tasked with differentiating between our synthetized images and real image patches extracted from whole slide images. We included both small patches (512 \(\) 512 px) as commonly used for downstream tasks as well as large patches (2048 \(\) 2048 px). Including large patches enabled us to additionally evaluate the modelled long-range correlations in terms of transitions between tissue types as well as growth patterns which are usually not observable on the smaller patch sizes but essential in histopathology. In total the survey contained 60 images, in equal parts synthetic and real images as well as small and large patches. The overall ability of pathologists to discern between real and synthetic images was modest, with an accuracy of 63%, and an average reported confidence level of 2.22 on a 1-7 Likert scale. While we observed high inter-rater variance, there was no clear correlation between experience and accuracy (r(8) =.069, p=.850), nor between confidence level and accuracy (r(8) =.446, p=.197). Furthermore there was no significant correlation between the participants' completion time of the survey and the number of correct responses (r(8) = -.08, p=.826).

Surprisingly, we found a similar performance for both, real and synthetic images. This indicates that, while clinical practice is mostly based on visual assessment, it is not a common task for pathologists to be restricted to parts of the whole slide image only. More detailed visualizations of the individual scores can be found in Appendix B. Besides this satisfactory result, we additionally wanted to

    & **IP**\(\) & **IR**\(\) & **IS**\(\) & **FID**\(\) \\  Morph-Diffusion  & 0.26 & **0.85** & 2.1 & 20.1 \\ NASDM  & - & - & **2.7** & **15.7** \\ DiffInfinite (a) & **0.94** & 0.70 & **2.7** & 26.7 \\        & **IP**\(\) & **IR**\(\) \\  DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\        & **IP**\(\) & **IR**\(\) \\  DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\        & **IP**\(\) & **IR**\(\) \\  DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\    
    & **IP**\(\) & **IR**\(\) \\  DiffCollage & 0.94 & 0.22 \\ DiffInfinite (b) & 0.95 & **0.48** \\ DiffInfinite (c) & **0.98** & 0.44 \\ DiffInfinite (b) \& (c) & **0.98** & 0.33 \\   

Table 1: Metrics to quantitatively evaluate the quality of the generated images. Left: scores for images of size \(512 512\). DiffInfinite (a) first generates a mask and secondly an image following Section 4.1. Right: scores for real and generated images of size \(2048 2048\) resized to \(512 512\). All methods use the same model trained on small patches of size \(512 512\). DiffCollage corresponds to the method proposed in . DiffInfinite (b) uses the real masks, while DiffInfinite (c) first generates a mask and secondly the large image. DiffInfinite (b) & (c) refers to the mixture of the generated dataset from DiffInfinite (b) and DiffInfinite (c).

explore the limitations of our method by assessing the nuanced differences pathologists observed between synthetic and real images. While overall the structure and features seemed similar and hard to discern, they sometimes reported regions of inconsistent patterns, overly homogeneous chromatin in some of the synthetic nuclei, peculiarities in cellular and intercellular structures, and aesthetic elements. These seemed to be especially pronounced in tumours regions where sometimes the tissue architecture appeared exaggerated, the transition to stroma or surrounding tissue was too abrupt and some cells lacked distinguishable nucleoli or cytoplasm. We attribute the nuanced effect of larger image size on the accuracy on this observation (cf. Fig. 5C). Overall the finding of the conducted survey demonstrates how complex the task of distinguishing between real and synthetically generated data is even for experienced pathologists while still highlighting potential areas to improve the generative model.

### Synthetic Data for Downstream Tasks

A major interest in the availability of high quality labeled synthetic images is their use in downstream digital pathology applications. In this area, two primary challenges are the binary classification of images into cancerous or healthy tissues and the segmentation of distinct tissue areas in the tumor microenvironment. The unique ability of our technique to generate images of different cancer subtypes through the context prompt as well as the ability to create new segmentation masks and their corresponding H&E images specifically addresses these two challenges. Notably, expert annotations are costly and time consuming to acquire thus emphasizing the benefits of being able to train on purely synthetic datasets or augmenting annotated data in the low data regime. To showcase these two usecases we performed a series of experiments in both classification and segmentation settings. For all experiments, we trained a baseline classier on a relatively small number of expert annotations IH1 (#patches = 3726)) -- the same that were used to train DiffInfinite -- and additionally trained one model purely on synthetic data (IH1-S, #patches = 9974, \(=0\)), and one model on the real data augmented with the synthetic images. To generate target labels for the classification experiments, we simplified the segmentation challenge by categorizing patches with at least 0.05% of pixels labeled as 'Carcinoma' in the segmentation masks as 'Carcinoma'. All other patches were labeled 'Non-Carcinoma'. We evaluated all three classification models on several out-of-distribution datasets. We utilized two proprietary datasets (from the same cancer type with similar attributes but from distinct patient groups: IH1 (# patients=13, # patches=704) and IH3 (# patients=2, # patches=2817). Moreover, we assessed the models using two public datasets (NCK-CRC  and PatchCamelyon ), both representing tissue from different organs with distinct morphologies. Our findings, summarized in Table 1(a), suggest that a classifier's out-of-distribution performance, trained with limited sample size and morphological diversity, can vary significantly (ranging from 0.628 to 0.857 balanced accuracy). This variability cannot be attributed solely to morphology but may also be influenced by

Table 2: Zero-shot evaluation results of the downstream tasks, encompassing both classification and segmentation scenarios. We employed three distinct models for each scenario: The first, ”Trained Real,” was trained using real data (in-house IH1), which also served as the training set for DiffInfinite. The second, ”Trained Synthetic,” was trained using samples generated from DiffInfinite, and the third, ”Trained Augmented,” utilized a combination of real and synthetic data. Our evaluation extends across separate lung cohorts (internal datasets IH2 and IH3) and additional indications (external datasets NCT, CRC, PCam), with varying degrees of data drift introduced.

factors such as resolution and variations in scanning and staining techniques. Training exclusively with a larger set of synthetic images can enhance performance on some datasets (specifically IH2 and IH3), underscoring the advantages of leveraging the full training data in a semi-supervised manner within the generative model. Incorporating synthetic data as an augmentation to real data not only prevents the classifier's performance decline, as seen on NCT-CRC and Patchcamelyon, on similar datasets but also bolsters its efficiency on more distinct ones. For the more challenging segmentation task we again trained three segmentation models to differentiate between carcinoma, stroma, necrosis, and a miscellaneous class that included all other tissue types, such as artifacts. The baseline performance of the real data model on a distinct group of lung patients (dataset IH2) of a \(F_{1}\) score of \(0.614 0.009\) (across three random seeds) highlights the difficulty of generalizing out of distribution in this tasks. While the purely synthetic model was not able to fully recover the baseline performance (\(0.471 0.039\)), augmenting the small annotated dataset with synthetic data enhanced predictive performance to an \(F_{1}\) score of \(0.710 0.021\). This boost of 10 percentage points in performance demonstrates that the synthetic data provide new, relevant information to the downstream task. In summary, our findings demonstrate the feasibility of meeting or surpassing baseline performance levels for both tasks using either entirely synthetic data or within an augmented context. Nevertheless, the advantages of employing synthetic data in downstream tasks continue to pose a challenge, not only within the medical image domain but also across various other domains , thus requiring more comprehensive assessment and thorough examination.

### Considerations on Memorization

In medicine the adherence to privacy regulations is a sensitive requirement. While it is generally not possible for domain experts to infer patient identities from the image content of a histological tile or slide alone , developers and users of generative models are well advised to understand the risk of correspondence between the training data and the synthesized data. To this end, we evaluate the training and synthesized data against two memorization measures. The authenticity score \(A\) by  aims to measure the rate by which a model generates new samples (higher score means more innovative samples). Similarly,  aims to estimate the degree of data copying \(C_{T}\) from the training data by the generative model. A \(C_{T} 0\) implies data copying, while a \(C_{T} 0\) implies an underfitting of the model. The closer to \(0\) the better. See Appendix C for a precise closed form of the measures and Table 5 for the full quantitative results, indicating that the DiffInfinite model is not prone to data copying across all resolutions and variations considered here 4. The \(A\) range between \(0.86\) and \(0.98\), signifying a high rate of authenticity. While other papers unfortunately do not report such detailed memorization statistics for their models, the results by  suggest that a score \( 0.8\) is not trivial to achieve. None of the models under consideration in  (VAE, DCGAN, WGAN-GP, ADS-GAN) achieve more than \(0.82\) in \(A\) on simpler data (MNIST). This interpretation is strengthened by the results of a \(C_{T} 0\) which indicates that the model might even be underfitting and is not in a data copying regime. Qualitative results on the nearest neighbour search between training and synthetic data in Figure 1 further corroborate these quantitative results.

## 6 Conclusions

DiffInfinite offers a novel sampling method to generate large images in digital pathology. Due to the high-level mask generation followed by the low-level image generation, synthetic images contain long-range correlations while maintaining high-quality details. Since the model trains and samples on small patches, it can be efficiently parallelized. We demonstrated that the classifier-free guidance can be extended to a semi-supervised learning method, expanding the labelled data feature space with unlabelled data. The biological plausibility of the synthetic images was assessed in a survey by \(10\) domain experts. Despite their training, most participants found it challenging to differentiate between real and synthetic data, reporting an average low confidence in their decisions. We found that samples from DiffInfinite can help in certain downstream machine learning tasks, on both in- as well as out-of-distribution datasets. Finally, authenticity metrics validate DiffInfinite's capacity to generate novel data points with little similarity to the training data which is beneficial for the privacy preserving use of generative models in medicine.

Acknowledgements

We would like to acknowledge our team of pathologists who provided valuable feedback in and outside of the conducted survey - special thank you to Frank Dubois, Niklas Preinssl, Cleopatra Schreiber, Vitaly Garg, Alexander Arnold, Sonia Villegas, Rosemarie Krupar and Simon Schallenberg. Furthermore, we would like to thank Marvin Sextro for his support in the analyses. This work was supported by the Federal Ministry of Education and Research (BMBF) as grants [SyReal (01IS21069B)]. RM-S is grateful for EPSRC support through grants EP/T00097X/1, EP/R018634/1 and EP/T021020/1, and DI for EP/R513222/1. MA is funded by Dotphoton, QuantIC and a UofG Ph.D. scholarship.