# Where Did I Come From? Origin Attribution of AI-Generated Images

Zhenting Wang

Rutgers University

zhenting.wang@rutgers.edu

&Chen Chen

Sony AI

ChenA.Chen@sony.com

&Yi Zeng

Virginia Tech

yizeng@vt.edu

&Lingjuan Lyu

Sony AI

Lingjuan.Lv@sony.com

&Shiqing Ma

University of Massachusetts Amherst

shiqingma@umass.edu

Work partially done during Zhenting Wang's internship at Sony AI.Corresponding Author

###### Abstract

Image generation techniques have been gaining increasing attention recently, but concerns have been raised about the potential misuse and intellectual property (IP) infringement associated with image generation models. It is, therefore, necessary to analyze the origin of images by inferring if a specific image was generated by a particular model, i.e., origin attribution. Existing methods only focus on specific types of generative models and require additional procedures during the training phase or generation phase. This makes them unsuitable for pre-trained models that lack these specific operations and may impair generation quality. To address this problem, we first develop an alteration-free and model-agnostic origin attribution method via reverse-engineering on image generation models, i.e., inverting the input of a particular model for a specific image. Given a particular model, we first analyze the differences in the hardness of reverse-engineering tasks for generated samples of the given model and other images. Based on our analysis, we then propose a method that utilizes the reconstruction loss of reverse-engineering to infer the origin. Our proposed method effectively distinguishes between generated images of a specific generative model and other images, i.e., images generated by other models and real images.

## 1 Introduction

In recent years, there has been a rapid evolution in image generation techniques. With the advances in visual generative models, images can now be easily created with high quality and diversity . There are three important milestones in the field of image generation and manipulation, i.e., Generative Adversarial Networks (GAN) , Variational AutoEncoders (VAE) , and diffusion models . Various image generation models are built based on these three models  to make the AI-generated images more realistic.

With its wide adoption, the security and privacy of image generation models becomes critical . One severe and important issue is the potential misuse and intellectual property (IP) infringement of image generation models . Users may generate malicious images containing inappropriate or biased content using these models and distribute them online. Furthermore, trained models may be used without authorization, violating the model owner's intellectual property. For example, malicious users may steal the model's parameters file and use it for commercial purposes. Others may create AI-generated images and falsely claim them as their own artwork (e.g., photos and paintings) togain recognition, which also violates the model's IP. Therefore, it is essential to track the origin of AI-generated images. The origin attribution problem is to identify whether a specific image is generated by a particular model. As shown in Fig. 1, assuming we have a model \(_{1}\) and its generated images, the origin attribution algorithm's objective is to flag an image as belonging to model \(_{1}\) if it was generated by that model. On the other hand, the algorithm should consider the image as non-belonging if it was created by other models (e.g., \(_{2}\) in Fig. 1) or if it is a real image.

One existing way to infer the source of specific images is image watermarking. It works by embedding ownership information in carrier images to verify the owner's identity and authenticity . The image watermarking-based method requires an additional modification to the generation results in a post-hoc manner, which may impair the generation quality. Also, it might not necessarily reflect the use of a particular model in the process of generating an image, which can reduce its credibility as evidence in lawsuits. Furthermore, the watermark can also be stolen, and malicious users can engage in criminal activities and disguise their identities using the stolen watermark. Another approach to identifying the source models of the generated samples  is injecting fingerprinting into the models (e.g., modifying the model architecture) and training a supervised classifier to detect the fingerprints presented in the image. While their goal is similar to ours, these methods have several limitations. Firstly, they require extra operations during the model training phase, and they cannot be applied on pre-trained models without additional operations, such as modifying the model architecture . Secondly, since these methods modify the training or inference process of generative models, the model's generation performance may be affected. Thirdly, these previous studies mainly focus on a particular kind of generative model, i.e., GAN . In contrast, our goal is to develop an origin attribution approach for different types of generative models, including diffusion models  (model-agnostic), without requiring any extra operations in the training phase and image generation phase (alteration-free). We summarize the differences between our method and existing methods in Table 1.

In this paper, we propose a method for origin attribution that is based on the input reverse-engineering task on generative models (i.e., inverting the input of a particular generative model for a specific image). The intuition behind our method is that the reverse-engineering task is easier for belonging images than non-belonging images. Therefore, we design our method based on the differences in the reconstruction loss for the reverse-engineering between generated images of a given model and other images. The origin attribution method we propose starts by using the model to generate a set of images and calculate the reconstruction loss on these generated images. To eliminate the influence of the inherent complexities of the examined images, we also calibrate the reconstruction loss by considering the hardness of the reverse-engineering on any other model that has strong generation ability, but different architectures and training data. Afterwards, the algorithm computes the calibrated reconstruction loss for the examined image and uses statistical hypothesis testing to determine if the reconstruction loss falls within the distribution of reconstruction losses observed in the generated images. This allows us to distinguish images generated by the given model from other images in a model-agnostic and alteration-free manner. Based on our design, we implemented a prototype called RONAN (**R**everse-engineering-based **O**rigi**N **A**ttribution**N) in PyTorch and evaluated it on three different types of generative models (i.e., unconditional model, class-to-image model, and text-to-image model) including various GANs , VAEs , and diffusion models such as latest Consistency Model  and Stable Diffusion . Results demonstrate our

    &  &  \\  &  &  &  \\  Image Watermark  & \(}}\) & ✗ & ✗ \\ Classifier-based  & ✗ & ✗ & ✗ \\ Ours & ✗ & ✗ & ✗ \\   

Table 1: Summary of the differences between our method and existing methods.

Figure 1: Illustration for origin attribution problem. The origin attribution algorithm aims to judging whether the given images belong to a particular model, i.e., Model \(_{1}\).

method is effective for the "alteration-free and model-agnostic origin attribution" task. On average, RONAN achieves 96.07% of true positive rate with a false positive rate around 5.00%.

Our contributions are summed as follows: 1 We introduce a new task called "alteration-free and model-agnostic origin attribution", which entails determining whether a specific image is generated by a particular model without requiring any additional operations during the training and generation phases. 2 To accomplish this task, we analyze the differences in the reconstruction loss for reverse-engineering between the generated images of a given model and other images. Based on our analysis, we design a novel method that involves conducting input reverse-engineering and checking whether the reconstruction loss of the examined sample falls within the distribution of reconstruction losses observed in the generated images. 3 We evaluate our method on various different image generation models. The results show that our method effectively distinguishes images generated by the given model from other images in a model-agnostic and alteration-free manner. Our code can be found in https://github.com/ZhentingWang/RONAN.

## 2 Related Work

**Detection of AI-Generated Contents.** Detecting AI-generated content has become extremely important with the growing concerns about the misuse of AIGC technology . The detection of AI-generated content is a binary classification problem that involves distinguishing generated samples from real ones. In the CV field, existing research has found that visually imperceptible but machine-distinguishable patterns in generated images, such as noise patterns [16; 32], frequency signals [33; 34; 35; 36] and texture representation  can be used as the clues of AI-generated images. Researchers also proposed methods to detect sentences generated by generative NLP models such as ChatGPT [38; 39; 40; 41]. Although these methods achieve promising performance for distinguishing AI-generated content and real content, they cannot infer if a specific content is generated by a given generative model, which is a novel but more challenging task and is the main focus of this paper.

**Tracking Origin of Generated Images.** There are several ways to track the source of the generated images. Image watermarking that pastes ownership information in carrier images [20; 21; 22; 23] can be adapted to discern whether a specific sample is from a specific source. Watermarks can take the form of specific signals within the images, such as frequency domain signals  or display-camera transformations . However, it requires an additional modification to the generation results in a post-hoc manner, and it does not necessarily reflect whether the image was generated by a particular model when the judges use it as the evidence (different models can use the same watermark). Another way is to inject fingerprints into the model during training and train a supervised classifier on it to discern whether an image is from a fingerprinted GAN model [24; 25; 26; 27]. For example, Yu et al.  modify the architecture of the convolutional filter to embed the fingerprint, and they train the generator alongside a fingerprinting classifier capable of identifying the fingerprint and its corresponding source GAN models. It requires a modified model architecture, altered training process, and an additional procedure to train a source classifier. There are several existing methods [43; 44; 45] track the origin of the generated images by exploiting the input reconstruction. However, the practicality of these approaches is limited due to their reliance on strong assumptions (see SS 5.5 for more details).

## 3 Problem Formulation

We focus on serving as an inspector to infer _if a specific sample is generated by a particular model in an alteration-free and model-agnostic manner_. To the best of our knowledge, this paper is the first work focusing on this problem. To facilitate our discussion, we first define the belonging and non-belonging of the generative models.

**Definition 3.1** (Belonging of Generative Models).: Given a generative model \(:_{}\) where \(\) is the input space and \(_{}\) is the output space. A sample \(\) is a **belonging** of model \(\) if and only if \(_{}\). We call a sample \(\) is a **non-belonging** if \(_{}\).

**Inspector's Goal.** Given a sample \(\) and a generative model \(:_{}\) where \(\) is the input space and \(_{}\) is the output space of \(\), the inspector's goal is to infer if a given image \(\) is a belonging of \(\). Formally, the goal can be written as constructing an inference algorithm \(:(,)\{0,1\}\) that receives a sample \(\) and a model \(\) as the input, and returns the inference result (i.e., 0 denotes belongings, and 1 denotes non-belongings). The algorithm \(\) can distinguish not only the belongings of the given model \(\) and that of the other models (e.g., trained on different training data, or have different model architectures), but also the belongings of \(\) and the natural samples that are not generated by AI. The inspector also aims to achieve the following goals:_Alteration-free:_ The algorithm \(\) does not require any extra operations/modifications in the training phase and image generation phase.

_Model-agnostic:_ The algorithm \(\) can be applied to different types of image generative models with different architectures.

**Inspector's Capability.** The inspector has white-box access to the provided model \(\), thus the inspector can get the intermediate output and calculate the gradient of the models. In addition, the inspector cannot control the development and training process of the provided models. Note that the white-box access to the inspected model is practical especially in the scenarios where the inspector is the owner of the inspected models.

**Real-world Application.** The inspection algorithm can be widely used in various scenarios where it is necessary to verify the authenticity of generated images. We provide three examples as follows:

_Copyright protection of image generation models:_ A copyright is a kind of intellectual property (IP) that provides its owner the exclusive right to perform a creative work . In this scenario, a party suspects that a specific image may have been generated by their generative model without authorization, such as if a malicious user has stolen the model and used it to generate images. The party can then request an inspector to use our proposed method to infer if the doubtful image was indeed generated by their particular model, and the resulting inference can be used as a reference in a lawsuit. It is important to clarify that infering an image as a belonging of a model does not imply the IP of this image is totally belonging to this model. In fact, determining the ownership of the IP related to the generated images remains an unresolved challenge in the field of law. This complexity arises due to the involvement of multiple entities (such as contributors of training data, model trainers, input/prompt providers, and the models themselves) throughout the image generation process. The infering results of our origin attribution method can serve as a valuable reference for addressing IP protection concerns, instead of a definitive conclusion.

_Tracing the source of maliciously generated images:_ Assume a user creates malicious images containing inappropriate or biased content and distributes them on the internet. The cyber police can utilize our proposed method to infer if the image was generated by a model belonging to a specific user. The resulting inference can be used as a reference for criminal evidence in lawsuits.

_Detecting AI-powered plagiarism:_ Our method can also be used for detecting AI-powered plagiarism. For example, imagine a scenario where an individual generates AI-created images (e.g., using Midjourney) and dishonestly presents them as their own original artwork (e.g., photographs and paintings) to gain recognition and reputation. In such cases, the model owner (e.g., Midjourney's owner) may suspect that the image is generated using their model (e.g., Midjourney). Our proposed method can then be employed to uncover such instances of AI-powered plagiarism.

## 4 Method

Our method is built on the input reverse-engineering for image generation models. In this section, we start by formulating the input reverse-engineering task, followed by an analysis of the disparities in reconstruction loss between images generated by a particular model and those from other sources. We then proceed to present a detailed algorithm for our method.

### Reverse-engineering

Recent researches [47; 17] demonstrate that the input of the modern image generative models is reconstructable by using the generated images. Here, we view the reverse-engineering as an optimization problem. Formally, it can be defined as follows:

**Definition 4.1** (Input Reverse-engineering).: Given a generative model \(:_{}\), and an image \(\), an input reverse-engineering task is optimizing the input \(\) to make the corresponding output image from the model \(()\) as close as possible to the given image \(\).

The input reverse-engineering task is performed by a **reverse-engineering algorithm**\(:(,) R\), which can be written as:

\[^{}=*{arg\,min}_{}( (),),(,)=( (^{}),)\] (1)

where \(\) is a metric to measure the distance between different images, and \(^{}\) is the reverse-engineered input. The given value for the reverse-engineering algorithm \(\) is a specific image \(\) and a particular model \(\). The returned value of the algorithm \(\) is the distance between the given image and its reverse-engineered version, i.e., \(((^{}),)\), which is called as the **reconstruction loss**. We use the reconstruction loss to measure the hardness of the input reverse-engineering task.

Based on the definitions and formulations, we have the following theorem:

**Theorem 4.2**.: _Given a generative model \(:_{}\), and a reverse-engineering algorithm \(\), if the model is deterministic (i.e., it produces the same output given the same input) and the reverse-engineering algorithm is perfect (i.e., it can find the global minimum of the reconstruction loss for the reverse-engineering), then for any \(_{}\) (belonging) and \(^{}_{}\) (non-belonging) we have \((,^{})>(,)\)._

The proof for Theorem 4.2 can be found in the Appendix A. The theorem demonstrates that the reconstruction loss of the images generated by a specific model will be lower than that of images that are not generated by the model. The theorem also establishes that the distribution of reconstruction loss values for belonging and non-belonging images is perfectly separable. Thus, we can use a threshold value to separate the belonging images and non-belonging images. In the real world, many image generation models incorporate random noises into their image generation procedures to enhance the variety of images they produce. However, these models can also be deemed deterministic since we can regard all the random noises utilized in the generation procedure as parts of the inputs. On the other hand, in reality, the reverse-engineering algorithm may get stuck at a local minimum, and it is hard to guarantee the achievement of the global minimum. This is where the formula \(((,^{})>(, ))\) becomes relevant as it serves as a relaxation for Theorem 4.2, explaining the practical scenario. In this formula, \(\) (e.g., \(90\%\)) acts as a separability level for distinguishing between the two distributions: belonging images and non-belonging images.

To investigate the practical scenario, we conduct experiments on the CIFAR-10  dataset using DCGAN , VAE , and StyleGAN2-ADA  models. The results are depicted in Fig. 2, where the x-axis represents the reconstruction loss measured by the MSE (Mean Squared Error)  metric, and the y-axis indicates the percentage of images whose reconstruction loss value corresponds to the corresponding value on the x-axis. We use blue color to denote 100 generated images of the given model and orange to represent 100 real images randomly sampled from the training data of the model. The results indicate that the reconstruction losses of the generated images (belongings) and those not generated by this model (non-belongings) can be distinguished.

### Calibration

Different images have different inherent complexities . Some images may be harder to reverse-engineer due to their higher complexity (e.g., containing more objects, colors, and details). In that case, the reconstruction loss will also be influenced by the inherent complexities of the examined images. To increase the separability level of belonging images and others, we disentangle the influence of the inherent complexities and the belonging status by measuring the complexity of the image and use it to calibrate the reconstruction loss. By default, we use the reconstruction loss on a reference image generation model \(_{r}\) that is trained on a different dataset as the measurement of the complexity, i.e., \(()=(_{r},)\) (we use the Consistency model  pre-trained on ImageNet dataset  as the reference model by default). We also discuss other implements to measure the complexity of the inspected images in Appendix F. The calibrated reconstruction loss \(^{}(,)\) is defined as follows:

\[^{}(,)=(, )}{()}\] (2)

Fig. 2: Reconstruction loss distributions for belonging images and real images.

### Belonging Inference via Hypothesis Testing

We use Grubbs' Hypothesis Testing  to infer if a specific sample \(\) is a belonging of the particular given model \(\). We have a null hypothesis \(H_{0}:\)_is a non-belonging of \(\)_, and the alternative hypothesis \(H_{1}:\)_is a belonging of \(\)_. The null hypothesis \(H_{0}\) is rejected (i.e., the alternative hypothesis \(H_{1}\) is accepted) if the following inequality (Eq. 3) holds:

\[^{}(,)-}{}<}t_{/N,N-2}^{2}}{N-2+t_{/ N,N-2}^{2}}}\] (3)

Here, \(\) and \(\) are the mean value and standard deviation for the calibrated reconstruction loss on belonging samples of model \(\). Since model \(\) is given to the inspector, the inspector can calculate \(\) and \(\) by using \(\) to generate multiple images with randomly sampled inputs. \(N\) is the number of generated belonging images. \(^{}(,)\) is the calibrated reconstruction loss of the examined image \(\). \(t_{/N,N-2}\) is the critical value of the \(t\) distribution with \(N-2\) degrees of freedom and a significance level of \(/N\), where \(\) is the significance level of the hypothesis testing (i.e., 0.05 by default in this paper). The critical value of the \(t\) distribution (i.e., \(t_{/N,N-2}\)) can be computed using the cumulative distribution function (See Appendix B for more details).

### Algorithm

We propose Algorithm 1 to determine if a specific sample belongs to a given model. The input of Algorithm 1 is the examined data \(\) and the given model \(\). The output of this algorithm is the inference results, i.e., belonging or non-belonging. In line 3, we use the given model to generate \(N\) (i.e., 100 by default in this paper) images with randomly sampled inputs and calculate the mean value (\(\)) and standard deviation (\(\)) for the calibrated reconstruction loss on the generated belonging samples. This step can be done offline, i.e., it only needs to be performed once for each model. In line 5, we calculate the calibrated reconstruction loss of the examined image (Eq. 2), the reconstruction loss is computed via gradient descent optimizer (Adam  by default in this paper). In line 7, we determine if the examined image \(\) belongs to the model \(\) or not by conducting the Grubbs' Hypothesis Testing  (SS 4.3). The given image is flagged as a belonging of the given model if the corresponding hypothesis is accepted.

```
0: Model: \(\), Examined Data: \(\)
0: Inference Results: Belonging or Non-belonging
1:functionInference(\(,\))
2:\(\) Obtaining Belonging Distribution (Offline)
3:\(,,N=()\)
4:\(\) Reverse-engineering
5:\(^{}(,)\) Calibrated Reconstruction Loss [Eq. 2]
6:\(\) Determining Belonging
7:\(=(^{}( ,),,,N)\)[Eq. 3]
8:return\(\) ```

**Algorithm 1** Origin Attribution

## 5 Experiments and Results

In this section, we first introduce the setup of the experiments (SS 5.1). We evaluate the effectiveness of RONAN (SS 5.2) and provide a case study on Stable Diffusion v2 model  (SS 5.3). We then conduct ablation studies in SS 5.4, and discuss the comparison to existing reconstruction based attribution methods in SS 5.5. The discussion about the efficiency and robustness against image editing can be found in the Appendix.

### Setup

Our method is implemented with Python 3.8 and PyTorch 1.11. We conducted all experiments on a Ubuntu 20.04 server equipped with six Quadro RTX 6000 GPUs.

**Models.** Eleven different models are included in the experiments: DCGAN , VAE , StyleGAN2-ADA , DDIM , DDPM , TransGAN , StyleGAN XL , Consistency DiffusionModel , Control-GAN , StackGAN-v2 , and Stable Diffusion v2 . These models are representative image generation models.

**Performance Metrics.** The effectiveness of the method is measured by collecting the detection accuracy (Acc). For a particular model, given a set of belonging images and non-belonging images, the Acc is the ratio between the correctly classified images and all images. We also show a detailed number of True Positives (TP, i.e., correctly detected belongings), False Positives (FP, i.e., non-belongings classified as belongings), False Negatives (FN, i.e., belongings classified as non-belongings) and True Negatives (TN, i.e., correctly classified non-belongings).

### Effectiveness

In this section, we evaluate the effectiveness of RONAN from two perspectives: (1) its effectiveness in distinguishing between belongings of a particular model and real images; (2) its effectiveness in differentiating between belongings of a particular model and those generated by other models.

**Distinguishing Belonging Images and Real Images.** To investigate RONAN's effectiveness in distinguishing between belonging images of a particular model and real images, given an image generation model, we start by differentiating between the generated images of the given model and the training data of the model. The investigated models include DCGAN , VAE , StyleGAN2-ADA  trained on the CIFAR-10  dataset, Consistency Model  trained on the ImageNet  dataset, and ControlGAN  trained on the CUB-200-2011  dataset. Among them, DCGAN and VAE are unconditional image generation models. StyleGAN-ADA and the latest diffusion model Consistency Model, are class-conditional models. In addition to distinguishing belongings and the training data, we also conduct experiments to distinguish belongings from unseen data that has a similar distribution to the training data (i.e., the test data of the dataset). For each case, we evaluate the results on 100 randomly sampled belonging images and 100 randomly sampled non-belonging images. The results are demonstrated in Table 2. As can be observed, the detection accuracies (Acc) are above 85% in all cases. On average, the Acc is 94.2% for distinguishing belongings and training data, and is 95.9% for distinguishing belongings and unseen data. The results demonstrate that

    &  &  &  &  \\   & & & TP & FP & FN & TN & Acc & TP & FP & FN & TN & Acc \\   & DCGAN & CIFAR-10 & 96 & 0 & 4 & 100 & 98.0\% & 95 & 0 & 5 & 100 & 97.5\% \\  & VAE & CIFAR-10 & 95 & 0 & 5 & 100 & 97.5\% & 96 & 0 & 4 & 100 & 98.0\% \\   & StyleGAN2-ADA & CIFAR-10 & 96 & 2 & 4 & 98 & 97.0\% & 95 & 0 & 5 & 100 & 97.5\% \\  & Consistency Model & ImageNet & 96 & 24 & 4 & 76 & 86.0\% & 96 & 11 & 4 & 89 & 92.5\% \\   & ControlGAN & CUB-200-2011 & 95 & 10 & 5 & 90 & 92.5\% & 96 & 8 & 4 & 92 & 94.0\% \\   

Table 2: Detailed results on distinguishing belonging images and real images.

   Training Dataset & Model \(_{1}\) & Model \(_{2}\) & TP & FP & FN & TN & Acc \\   &  & VAE & 96 & 0 & 4 & 100 & 98.0\% \\  & & StyleGAN2ADA & 96 & 0 & 4 & 100 & 98.0\% \\  & & DDMM & 97 & 0 & 3 & 100 & 98.5\% \\  & & DDPM & 96 & 0 & 4 & 100 & 98.0\% \\  & & DGPAN & 97 & 0 & 3 & 100 & 98.5\% \\   &  & DCGAN & 97 & 0 & 3 & 100 & 98.5\% \\  & & StyleGAN2ADA & 95 & 0 & 5 & 100 & 97.5\% \\  & & DDIM & 96 & 0 & 4 & 100 & 98.0\% \\  & & DDPM & 96 & 0 & 4 & 100 & 98.0\% \\  & & TransGAN & 97 & 0 & 3 & 100 & 98.5\% \\   &  & DCGAN & 96 & 2 & 4 & 98 & 97.0\% \\  & & VAE & 95 & 1 & 5 & 99 & 97.0\% \\  & & DDIM & 96 & 1 & 4 & 99 & 97.5\% \\  & & DDPM & 96 & 0 & 4 & 100 & 98.0\% \\  & & TransGAN & 95 & 6 & 5 & 94 & 94.5\% \\   & Consistency Model & StyleGAN XL & 95 & 8 & 5 & 92 & 93.5\% \\  & StyleGAN XL & Consistency Model & 96 & 10 & 4 & 90 & 93.0\% \\   & ControlGAN & StackGAN-v2 & 96 & 17 & 4 & 83 & 89.5\% \\  & StackGAN-v2 & ControlGAN & 96 & 14 & 4 & 86 & 91.0\% \\   

Table 3: Results for distinguishing belonging images and images generated by other models with different architectures. Here, Model \(_{1}\) is the examined model, Model \(_{2}\) is the other model that has same training data but different architectures.

RONAN achieves good performance in distinguishing between belonging images of a particular model and real images.

**Distinguishing Belonging Images and Images Generated by Other Models.** In this section, we study RONAN's effectiveness to distinguish between belonging images of a particular model and the images generated by other models. For a given model \(_{1}\), we consider two different types of other models \(_{2}\), i.e., the model trained on the same dataset but with different architectures and the model that has the same architecture but is trained on a different dataset.

_Models with Different Architectures._ We first evaluate RONAN's effectiveness in distinguishing between belonging images of a particular model and the images generated by other models with the same training data but different architectures. For the models trained on the CIFAR-10  dataset, the used model architectures are DCGAN , VAE , StyleGAN2-ADA , DDIM , DDPM , TransGAN . For the Imagenet  dataset, the involved models are the latest diffusion model Consistency Model  and StyleGAN XL . For the CUB-200-2011  dataset, we use text-to-image models ControlGAN  and StackGAN-v2 . To measure the effectiveness of RONAN, we collect its results on 100 randomly sampled belonging images and 100 randomly sampled non-belonging images. The results are shown in Table 3, where Model \(_{1}\) denotes the examined model, and Model \(_{2}\) represents the other model that has the same training data but a different architecture. The results show that the average detection accuracy (Acc) of RONAN is 96.45%, confirming its good performance for distinguishing between belongings of a given model and the images generated by other models with different architectures.

_Models Trained on Different Datasets._ We also evaluate RONAN's effectiveness in distinguishing between belonging images of a particular model and the images generated by other models with the same model architecture but trained on different datasets. The model used here is the diffusion model Consistency Model . We use a model trained on the ImageNet  dataset and a model trained on the LSUN  dataset. The results are demonstrated in Table 4, where Model \(_{1}\) denotes the examined model, and Model \(_{2}\) means the other model that has the same model architecture but is trained on a different dataset. The results show that RONAN can effectively distinguish between belonging images of a particular model and the images generated by other models with the same model architecture but different training data. On average, the detection accuracy of our method is 93.0%. In the Appendix, we also discuss the results when the training data of the model \(_{2}\) and that of the model \(_{1}\) have overlaps. Empirical results demonstrate that RONAN is still effective even the training data of the examined model and that of the other model are similar (i.e., they have large overlaps).

### Case Study on Stable Diffusion v2

In this section, we conduct a case study on the recent Stable Diffusion v2  model. We first randomly collect 20 images of Shiba dogs from the internet and use these images as the non-belonging images. We then use the prompt "A cute Shiba on the grass." and feed it into the Stable Diffusion v2 model to generate 20 belonging images. We apply RONAN on the model, and evaluate its performance in distinguishing the belonging images and non-belonging images. The results show that the detection accuracy of RONAN is 87.5%, with 18 TP, 3 FP, 2 FN, 17 TN. In Fig. 3, we show the visualization of a belonging image and a non-belonging image, as well as their corresponding reverse-engineered images. Note that the non-belonging image and the belonging image have similar inherent complexities (i.e., their reconstructed loss with the MSE metric on the reference model are 0.029 and 0.022, respectively). For the non-belonging image, the reverse-engineered image is more noisy and blurred, while the reverse-engineered image of the belonging image seems nearly identical to the original image. These results show the potential to apply our method on state-of-the-art models such as Stable Diffusion v2. More visualizations and examples can be found in Appendix J.

### Ablation Study

In this section, we evaluate the impact of different metrics used in calculating reconstruction loss, and the impact of reconstruction loss calibration.

   Training dataset &  Training dataset \\ of Model \(_{1}\) \\  & 
 TP \\ of Model \(_{2}\) \\  & FP & FN & TN & Acc \\  ImageNet & LSUN & 95 & 13 & 5 & 87 & 91.0\% \\ LSUN & ImageNet & 96 & 6 & 4 & 94 & 95.0\% \\   

Table 4: Results for distinguishing belonging images and images generated by other models with different training data. Here, Model \(_{1}\) is the examined model, Model \(_{2}\) is the other model that has same architecture but different training data.

**Different Metrics.** In the reverse-engineering task (Eq. 1), we use a metric \(\) to measure the distance between different images. By default, we select MSE  as the metric. In addition to MSE, we also evaluate the results on other image distance metrics, i.e., MAE (Mean Absolute Error) , SSIM (Structural Similarity Index Measure) , and LPIPS (Learned Perceptual Image Patch Similarity) . The task is to distinguish between belonging images and real images (i.e., training images of the model here). The model used in this section is the StyleGAN2-ADA  trained on CIFAR-10  dataset. The results are shown in Table 5. As we can observe, the detection accuracy (Acc) under MAE, MSE, SSIM, and LPIPS are 94.5%, 97.0%, 85.5%, and 96.0%, respectively. Overall, the MSE metric achieves the highest performance in distinguishing belonging images and real images. Thus, we select MSE as our default metric.

**Impacts of Reconstruction Loss Calibration.** To eliminate the influence of images' inherent complexities, we calibrate the reconstruction loss by considering the hardness of the reverse-engineering on a reference model (SS 4.2). To measure the effects of the calibration step, we compare the detailed TP, FP, FN, TN, and Acc for the method with and without the calibration step. We use the Stable Diffusion v2  model and follow the experiment settings described in SS 5.3. The results in Table 6 demonstrate the detection accuracy for the method with and without the calibration step are 75.0% and 87.5%, respectively. These results show that the calibration step can effectively improve the performance of RONAN.

### Comparison to Existing Reconstruction based Attribution Methods

Albright et al.  and Zhang et al.  are two existing attribution methods that also apply the reconstruction on the input. In this section, we compare our method to them. Given an inspected image, Albright et al. and Zhang et al. works by enumerting and conducting inversion on all models within a set of suspicious candidate models (referred to as the "candidate set" in this section), and attribute the model with the lowest reconstruction loss as the source of the image. Their methods rely on the assumptions that the inspector can have the white-box access to all models in the candidate set, and the examined image must be generated by one of the models in the candidate set. These assumptions diminish the practicability of their methods, whereas our method does not have such requirements. Thus, our threat model and experiment settings are fundamentally different to theirs. Our paper focus on the problem of determining if a given image is generated by a single given model or not. Consider a scenario where a model owner wants to verify if an image is generated by a model owned by him/her. While our method only needs to conduct the inversion on this specific model. Albright et al. and Zhang et al. need to compare the reconstruction loss of this particular model with a large number of suspicious models. There are several cases where Albright et al. and Zhang et al. are ineffective in addressing this problem: 1 In cases where the inspector lacks white-box access

   Method & TP & FP & FN & TN & Acc \\  w/o Calibration & 17 & 7 & 3 & 13 & 75.0\% \\ w/ Calibration & 18 & 3 & 2 & 17 & 87.5\% \\   

Table 6: Effects of reconstruction loss calibration.

Figure 3: Visualization of the belonging image and non-belonging image for Stable Diffusion v2 and their corresponding reverse-engineered images. The non-belonging image and the belonging image have similar inherent complexities.

to some of the suspicious models, deriving reconstruction on them and getting inference results becomes infeasible. Notably, more and more state-of-the-art image generation models (e.g., such as Midjourney  and DALL-E2 ) are close-sourced and they only provide the black-box API to the users. 2 Albright et al. and Zhang et al. are prone to making wrong predictions if the real source model is not included in the candidate set. This is attributed to their underlying assumption that the examined image must originate from one of the models within the candidate set. Ensuring the real source model is included within the candidate set is a very hard problem in practice. 3 Equally noteworthy, Albright et al. and Zhang et al. do not work when the inspected images are real images due to their strong assumption (i.e., the examined image must be generated by one of the model in the candidate set). Our method does not have the above problems.

Despite the threat models are different, we also conduct the comparison experiments. We consider the setting for distinguishing the belonging images of the inspected model \(_{1}\) and the generated images of other models \(_{2}\), and the inspected model here is Stable Diffusion v2. For our method, we assume the inspector can only access \(_{1}\). For Albright et al. and Zhang et al., we assume the inspector can access both \(_{1}\) and \(_{2}\). The results can be found in Table 7. While the average detection accuracy of Albright et al. and Zhang et al. are only 50%, our method achieves over 90% detection accuracy, meaning that our method outperforms Albright et al. and Zhang et al. generated images of other models \(_{2}\), and the inspected model here is Stable Diffusion v2. For our method, we assume the inspector can only access \(_{1}\). For Albright et al. and Zhang et al., we assume the inspector can access both \(_{1}\) and \(_{2}\). The results can be found in Table 7. While the average detection accuracy of Albright et al. and Zhang et al. are only 50%, our method achieves over 90% detection accuracy, meaning that our method outperforms Albright et al. and Zhang et al. generated images of other models. Although the method proposed in Laszkiewicz et al. achieves promising performance, it is important to note that it is not model-agnostic. More specifically, it assumes the last layer of the inspected model is invertible. Therefore, it is not applicable to the models that use non-invertible activation functions (such as the commonly-used ReLU function) in the last layer. It also has assumptions on the architectures of the models, making it can not be used for many modern diffusion models [7; 63].

## 6 Discussion

**Limitations.** While our method can achieve origin attribution in a alteration-free and model-agnostic manner, the computation cost might be higher than that of watermark-based methods [20; 21; 22; 23] and classifier-based methods [24; 25; 26; 27]. More discussion about the efficiency of our method can be found in the Appendix. Speeding up the reverse-engineering will be our future work. This paper focuses on the image generation models. Expanding our method for origin attribution on the model in other fields, (e.g., void generation models such as Imagen Video , language generation models like ChatGPT , and graph generation models [66; 67]) will also be our future work.

**Ethics.** Research on security and privacy of machine learning potentially has ethical concerns [68; 69; 70; 71; 72; 73; 74; 75; 76; 77; 78]. This paper proposes a method to safeguard the intellectual property of image generation models and monitor their potential misuse. We believe that our approach will enhance the security of image generation models and be beneficial to generative AI.

## 7 Conclusion

In this paper, we take the first effort to introduce the "alteration-free and model-agnostic origin attribution" task for AI-generated images. Our contributions for accomplishing this task involves defining the reverse-engineering task for generative models and analyzing the disparities in reconstruction loss between generated samples of a given model and other images. Based on our analysis, we devise a novel method for this task by conducting input reverse-engineering and calculating the corresponding reconstruction loss. Experiments conducted on different generative models under various settings demonstrate that our method is effective in tracing the origin of AI-generated images.

## Appendix A Proof for Theorem 4.2

We start our analysis from ideal generative model and reconstruction algorithm, which we define as deterministic generative model and perfect reconstruction algorithm:

**Definition A.1** (Deterministic Generative Model).: Given a generative model \(:_{}\), it is deterministic if it always produce the same output \(_{}\) given the same input \(\).

**Definition A.2** (Perfect Reverse-engineering Algorithm).: Given a reverse-engineering algorithm \(\), if it is guaranteed that the returned reconstruction loss \(l\) is the global minima, then we say \(\) is a perfect reverse-engineering algorithm.

Proof.: Assume the given output sample \(\) is generated by input \(\). Since the given model \(\) is deterministic, we have:

\[=()\] (4)

In this case, the distance between the \(\) and \(()\) is 0, i.e., \(((),)=0\). Based on Theorem 4.1, as \(\) is a perfect reverse-engineering algorithm, it can find the input that can produce the minimal reconstruction loss. Therefore, we have:

\[_{},(,)=0\] (5)

Similarly, for sample \(^{}_{}\). There does not exist an input \(i^{}\) that can produce \(^{}\), meaning that there does not exist an input \(^{}\) that have \(((^{}),^{})=0\). Thus, we have:

\[^{}_{},(, ^{})>0\] (6)

Finally, we have for any \(_{}\) and \(^{}_{}\) we have \((,^{})>(,)\).

## Appendix B Computing Critical Value of the \(t\) Distribution.

In SS 4.3, we use the critical value of the \(t\) distribution to obtain the results of the hypothesis testing. In this section, we discuss the detailed process for calculating the critical value. For the t-distribution, we have the probability density function:

\[f(t)=)}{( {}{2})}(1+}{})^{-(+1)/2}\] (7)

In Eq. 7, \(\) is the number of degrees of freedom and \(\) is the gamma function. Based on Eq. 7, we have the cumulative distribution function:

\[(t<t^{})=_{-}^{t^{}}f(u)du=1- (}^{2}+},,)\] (8)

where \(\) denotes the incomplete beta function. Therefore, given a confidence level \(\) and the number of degrees of freedom \(\), we have can use Eq. 9 to obtain the value of the critical value \(t_{,}\).

\[(t<t_{,})=1-\] (9)

## Appendix C More Results for Distinguishing Belonging Images and Images Generated by Other Models.

In this section, we provide more results for distinguishing belonging images of the model and the images generated by other models. We first consider distinguishing the belonging images and the images generated by the other models having the same architecture and overlapped training data. Given a model \(_{1}\), we focus on the other model \(_{2}\) which shares the same architecture as \(_{1}\) and has training data that overlaps with \(_{2}\)'s training data. The model architecture used here is DCGAN . We trained \(_{1}\) on the full CIFAR-10  dataset, while \(_{2}\) is trained on the randomly sampled subsets of the CIFAR-10 dataset. The results are presented in Table 8, where the first column indicates the proportion of overlap between the training data of \(_{1}\) and \(_{2}\). The second column of Table 8 displays the accuracy of RONAN in detecting the differences. Notably, even when 90% of \(_{2}\)'s training data overlaps with that of \(_{1}\), the detection accuracy remains above 95%. These results demonstrate that our method is still effective when the training data of the inspected model and that of the other model are similar. We also consider distinguishing the belonging images and the images generated by the other models having the same training data and similar architectures. We use DCGAN  here and modifies the number of layers and the number of channels in the first Conv layer in the models to make \(_{1}\) and \(_{2}\) have similar architectures. Both \(_{1}\) and \(_{2}\) here are trained on CIFAR-10  dataset. The results can be found in Table 9 and Table 10. Based on these results, our method is effective even the inspected model and other models have the same training dataset and similar architectures. We also provide a summary of the combinatorial setups used for distinguishing belonging images and images generated by other models in Table 11.

## Appendix D A Thought Experiment on VAE.

If a image generation model perfectly fits the training samples, then the images generated by it will be indistinguishable to its training samples (which are real images). In this section, we investigate a question: "Can image generation models perfectly fit the training samples?". To study this question, we conduct a thought experiment on VAE . In detail, we randomly sample 10 images from MNIST  dataset, and use VAE  with different numbers of neurons in the hidden layer to fit these 10 images. We set the epoch number to 5000 to ensure the training losses that measures the distance between the generated samples and the training samples of the model are converged. The detailed final training losses with different model sizes are shown in Table 12. There are two parts in the total training loss \(_{}\): the reconstruction part \(_{}\) and the KL-divergence part \(_{}\), i.e., \(_{}=_{}+_{ }\). In Table 12, Detailed values for each part are demonstrated.

   \(_{1}\)’s Number of Channels in the first Conv Layer & \(_{2}\)’s Number of Channels in the first Conv Layer & Acc \\ 
64 & 48 & 97.5\% \\
48 & 64 & 96.0\% \\   

Table 10: Results when the inspected model and the other model has the similar architectures but their number of channels in the first Conv layer are different.

   Overlap Fraction & Acc \\ 
50\% & 98.5\% \\
60\% & 98.0\% \\
70\% & 98.5\% \\
80\% & 96.0\% \\
90\% & 96.5\% \\   

Table 8: Results when the inspected model and the other model has the same architecture and their training data has overlaps.

With the increases in the model sizes, the final total training losses reduce at first. However, when the model is large enough, the final total training loss becomes stable at the region from 53.00-54.00. The results show that even the VAEs with enough large sizes can not fit all pixels perfectly. We also conduct the reverse-engineering of the training samples on the trained VAE with 1000 neurons in the hidden layer. The results show that we can not reconstruct the exact training samples.

As pointed out by existing works , morden generative models (e.g., VAEs, GANs, and Diffusion models) including the state-of-the-art models such as Stable Diffusion  and DALL-E  will leave forensics traces in the frequency spaces of the generated images. This is caused by indispensable operations used in modern generative models (e.g., Upsampling operations and Convolutional Layers). Also, perfectly fitting the training samples means finding the global optimum in the optimization process. However, the essential gradient descent optimizers used in modern generative models (e.g., SGD and Adam) typically can not get the global optimum. Based on the above analysis and empirical results, at least we can conclude that the probabilities that the real images belong to the output space of generative models are very low.

## Appendix E Discussion on Different Reverse-engineering Methods

In this section, we discuss the details of the reverse-engineering process on different models and the effects of different reverse-engineering methods. The details of the reverse-engineering for different

    & _{1}\)} & _{2}\)} \\   & Model & Training Dataset & Model & Training Dataset \\   & DCGAN & CIFAR-10 & VAE & CIFAR-10 \\  & DCGAN & CIFAR-10 & StyleGAN2ADA & CIFAR-10 \\  & DCGAN & CIFAR-10 & DDIM & CIFAR-10 \\  & DCGAN & CIFAR-10 & DDPM & CIFAR-10 \\  & DCGAN & CIFAR-10 & TransGAN & CIFAR-10 \\  & VAE & CIFAR-10 & DCGAN & CIFAR-10 \\  & VAE & CIFAR-10 & StyleGAN2ADA & CIFAR-10 \\  & VAE & CIFAR-10 & DDPM & CIFAR-10 \\  & VAE & CIFAR-10 & DDPM & CIFAR-10 \\  & VAE & CIFAR-10 & StyleGAN2ADA & CIFAR-10 \\  & VAE & CIFAR-10 & TransGAN & CIFAR-10 \\  & StyleGAN2ADA & CIFAR-10 & DDPM & CIFAR-10 \\  & StyleGAN2ADA & CIFAR-10 & DDPM & CIFAR-10 \\  & StyleGAN2ADA & CIFAR-10 & TransGAN & CIFAR-10 \\  & Consistency Model & ImageNet & StyleGAN XL & ImageNet \\  & StyleGAN XL & ImageNet & Consistency Model & ImageNet \\  & ControlGAN & CUB-200-2011 & StackGAN-v2 & CUB-200-2011 \\  & StackGAN=v2 & CUB-200-2011 & ControlGAN & CUB-200-2011 \\   & DCGAN & CIFAR-10 & DCGAN with different numbers of layers & CIFAR-10 \\  & DCGAN & CIFAR-10 & DCGAN with different numbers of channels & CIFAR-10 \\   & Consistency Model & ImageNet & Consistency Model & LSUN \\  & Consistency Model & LSUN & Consistency Model & ImageNet \\   & DCGAN & CIFAR-10 & DCGAN & 50\% subset of CIFAR-10 \\  & DCGAN & CIFAR-10 & DCGAN & 60\% subset of CIFAR-10 \\   & DCGAN & CIFAR-10 & DCGAN & 70\% subset of CIFAR-10 \\   & DCGAN & CIFAR-10 & DCGAN & 80\% subset of CIFAR-10 \\   

Table 11: Summary of combinatorial setups for distinguishing belonging images and images generated by other models.

   Num of Neurons in Hidden Layer & \(_{}\) & \(_{}\) & \(_{}\) \\ 
1 & 184.24 & 183.93 & 0.31 \\
5 & 105.27 & 99.68 & 5.59 \\
10 & 77.20 & 69.07 & 8.13 \\
50 & 53.52 & 47.55 & 5.97 \\
100 & 53.23 & 47.99 & 5.24 \\
500 & 53.04 & 48.21 & 4.83 \\
1000 & 53.81 & 48.93 & 4.88 \\   

Table 12: Detailed final training losses with different model sizes in the thought experiments on VAE.

models are summarized in Table 13. We also investigate the effects of different reverse-engineering methods. For StyleGAN2-ADA  and StyleGAN-XL , we use the random noise space (i.e., Z space) to reconstruct the images by default. We also conducted experiments on using intermediate latent space (i.e., \(\)+ space) to reconstruct the images for distinguishing belonging images and images generated by other models (see setting for Table 3). The results on StyleGAN2-ADA with the CIFAR-10  dataset and StyleGAN XL with ImageNet dataset are shown in Table 14. As can be observed, using the \(\)+ space has similar accuracy to using Z space, meaning that our method is not sensitive to the selection of the input spaces used for optimization.

We also conduct experiments for different reverse-engineering methods designed for diffusion models (i.e., DDIM inversion , Parmar et al.  and our method) on the Stable Diffusion v2  model with the setting described in SS 5.3. The results are shown in Table 15. DDIM inversion  is an inversion approach performing the reverse of the DDIM sampling. It is based on the assumption that the ODE process can be reversed in limited of small steps. It only has unsatisfying inversion performance on the conditional diffusion models (e.g., Stable Diffusion) because it will magnify the accumulated error in the inversion process (since it ignores the classifier-free guidance in the diffusion process). As can be seen in the above table, the accuracy of using DDIM inversion is only 55.5%. Parmar et al.  improve the DDIM inversion by using an approximated prompt as the conditional guidance in the inversion process. The approximated prompts are generated by a caption model (i.e., BLIP). This method's inversion quality is dependent on the captions used. Given a generated image of the inspected model, since the caption model can not get the accurate prompt used for generating this image, the inversion using this method is also not accurate, leading that the reconstruction losses of the belonging images and non-belonging images are not highly separable (the accuracy is 62.5%). Compared to DDIM inversion  and Parmar et al. , our method achieves higher accuracy.

## Appendix F Discussion on Different Methods for Measuring the Complexity of the Images

In this section, we discuss the different methods for measuring the complexity of the images.

We first discuss the reference model based method. Here, we conducted experiments that using different models as reference models. The inspected model here is the StyleGAN2-ADA  model trained on CIFAR-10  dataset and the setting here is identical to that used in the Table 2 (belongings vs training data). The results can be found in Table 16. As can be observed, using different reference models yields similar results, meaning that our method is not sensitive to the selection of the reference models. The measurement of the image complexity can also be implemented

   Reference Model & Acc \\  Consistency Model & 97.0\% \\ StyleGAN XL & 95.0\% \\ Stable Diffusion & 96.0\% \\   

Table 16: Results on using different models as reference models.

   Model & Space & Acc \\  StyleGAN2-ADA & Z space & 97.0\% \\ \(\)+ space & 96.0\% \\  StyleGAN XL & Z space & 93.0\% \\ \(\)+ space & 93.5\% \\   

Table 14: Results under different inversion methods for StyleGANs.

   Model & Details of Reverse-engineering \\  DCGAN & Using gradient decent to optimize the input in the noise space. \\ VAE & Using gradient decent to optimize the input in the noise space. \\ StyleGAN2-ADA & Using gradient decent to optimize the input in the noise space. \\ Consistency Model & Using gradient Decent to optimize the input in the noise space. \\ StyleGAN XL & Using gradient decent to optimize the input in the noise space. \\ ControlGAN & Using gradient decent to optimize the feature in the intermediate feature space before the \(G_{3}\) layer. \\ StackGAN-v2 & Using gradient decent to optimize the feature in the intermediate feature space before the \(G_{2}\) layer. \\ Stable Diffusion 2 & Using gradient decent to optimize the feature in the intermediate feature space before the decoder. \\   

Table 13: Details of reverse-engineering on different models.

by calculating the 2D entropy of the image . Using this entropy-based method as the image complexity measurement also yields 97.0% final detection accuracy in our setting.

## Appendix G Adaptive Attack

In this section, we evaluate the robustness of RONAN against the adaptive attack where the malicious user is aware of it and try to bypass the inspection of RONAN. For example, when the malicious user use a specific model to generate an image, he/she can make a slight modification on the image to bypass the inspection of the origin attribution algorithm. We consider the image editing as the adaptive attack because it can preserve most of the information in the original image while changing the image. To investigate if RONAN is robust to the image-editing-based adaptive attack, we use the _1977 instagram filter3 and the box blur filter to conduct results. The model used here is the DCGAN  model trained on the CIFAR-10  dataset. The results are shown in Table 17 and Table 18. For _1977 instagram filter, we can see that the detection accuracy of RONAN is still above 90% even under the adaptive attack. For box blur filter, we show the results under different box sizes of the filter. Besides the detection accuracy (Acc) of our method, we also demonstrate the Structural Similarity Index (i.e., SSIM ) between the original images and the edited images, which can measure the similarity between them. A higher SSIM value means the edited images are more similar to the original images. As can be observed, our method remains effective under relatively small box sizes. As the box sizes expand, however, the detection accuracy of our method diminishes. This outcome is understandable and acceptable, as it corresponds to a rapid reduction in the Structural Similarity Index (SSIM) between the edited images and their unaltered counterparts. When employing larger box sizes, it is conceivable that an adaptive attacker might find ways to elude our method's scrutiny, yet this comes at the cost of substantially compromising the quality of the edited images. Consequently, our method maintains its effectiveness even in the face of adaptive attacks that seek to maintain the quality of the edited images.

## Appendix H Efficiency

In this section, we discuss the efficiency of RONAN. To study the efficiency, we measure the runtime of our method on different models (i.e., DCGAN , VAE , StyleGAN2-ADA , Consistency Model , and Stable Diffusion v2 ). For each model, we run 5 times on one Quadro RTX 6000 GPU and collect the average runtime and its standard deviation. The results can be found in Table 19. Our method can be accelerated by using mixed precision training . Further approach for speeding up the input reverse-engineering process will be our future work.

## Appendix I More Results on Stable Diffusion

In this section, we provide more results on the Stable Diffusion. Here, we use the Stable Diffusion v2 model as the inspected model. The scenario considered here is distinguishing 1000 images generated by Stable Diffusion v2 model (the inspected model) and 1000 images generated by DeepFloyd-IF-II-L 4 model. Note that the images here are generated using the prompts randomly sampled from the

   TP & FP & FN & TN & Acc \\ 
96 & 15 & 4 & 85 & 90.5\% \\   

Table 17: Results under adaptive attack with instagram filter editing.

   Box Size & Acc & SSIM \\ 
1 & 92.5\% & 0.8920 \\
2 & 83.0\% & 0.7446 \\
3 & 58.0\% & 0.5174 \\
4 & 53.5\% & 0.3530 \\   

Table 18: Results under adaptive attack with box blur filter editing.

   Model & Runtime (s) \\  DCGAN & 16.53 \(\) 0.35 \\ VAE & 51.63 \(\) 0.84 \\ StyleGAN2-ADA & 54.44 \(\) 0.79 \\ Consistency Model & 153.65 \(\) 2.02 \\ Stable Diffusion & 605.45 \(\) 3.26 \\   

Table 19: Runtime on different models.

PromptHero5 website. The results demonstrate that our method achieves 91.6% detection accuracy, meaning it is effective under this scenario.

## Appendix J More Visualizations on Stable Diffusion

In this section, we provide more visualizations for our case study on Stable Diffusion v2 model  (SS 5.3). We show more visualizations for the belonging images and their corresponding reverse-engineered images in Fig. 4. In Fig. 5, we demonstrate more visualizations for the non-belonging images of the Stable Diffusion v2 model, and also show their reverse-engineered images. The detailed reconstruction loss and the calibrated reconstruction loss are reported in Fig. 4 and Fig. 5. The distance metric used here is MSE. As we can observed, the belonging images have much lower calibrated reconstruction loss than the non-belonging images, further demonstrating the effectiveness of our approach.

Figure 4: More visualization of the belonging images for Stable Diffusion v2 , and their corresponding reverse-engineered images. The distance metric used here is MSE.