# UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction

Yansong Ning\({}^{1}\), Hao Liu\({}^{1,2}\)

\({}^{1}\) AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)

\({}^{2}\) CSE, The Hong Kong University of Science and Technology

yning092@connect.hkust-gz.edu.cn liuh@ust.hk

Corresponding author.

###### Abstract

Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents **UrbanKGent**, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama 2 and Llama 3 family, we obtain UrbanKGC agent family2, consisting of UrbanKGent-7/8/13B version. We perform a comprehensive evaluation on two real-world datasets using both human and GPT-4 self-evaluation. The experimental results demonstrate that UrbanKGent family can not only significantly outperform 31 baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4, by more than 10% with approximately 20 times lower cost. Compared with the existing benchmark, the UrbanKGent family could help construct an UrbanKG with hundreds of times richer relationships using only one-fifth of the data. Our data and code are available at https://github.com/usail-hkust/UrbanKGent.

## 1 Introduction

Urban Knowledge Graph (UrbanKG) aims to model intricate relationships and semantics within a city by extracting and organizing urban entities (e.g., POIs, road networks, etc.) into a multi-relational heterogeneous graph . As an emerging building block, multi-sourced urban data are widely used to construct an UrbanKG to provide critical knowledge for various knowledge-enhanced urban downstream tasks, such as traffic management, pollution monitoring, and emergency response . UrbanKG has gradually become an essential tool of the modern smart city.

In prior literature, many efforts have been devoted to urban knowledge graph construction (UrbanKGC) using massive urban data sources. In particular, one commonly used approach  is to extract entities from structured urban data (e.g., geographic data, city sensor data, and traffic data) and define the relationships between obtained urban entities based on manually designed rules or patterns. However, these approaches suffer heavy reliance on a deep understanding of the application domain and are labor-intensive. Recently, inspired by the success of the Large Language Models (LLMs)in knowledge graph construction [9; 10; 11], the LLMs have been adopted to improve UrbanKGC. For instance, GeoLM  is pretrained on crowdsourced geographical corpus for geospatial entity recognition and relation extraction. K2  retains Llama-2-7B model on manually processed and filtered geoscience text corpus for geospatial relation extraction. Nevertheless, these works rely on high quality **corpus annotation** and computational extensive **model retraining**, which may discourage researchers from adopting UrbanKG for their own work.

LLM agent [14; 15] has recently emerged and shown remarkable zero-shot capability for autonomous domain-specific task completion. For example, Voyager  is a LLM-powered agent for zero-shot game exploration without re-training, and LLMLight  is a traffic signal control agent with zero-shot LLM reasoning ability. These studies motivate us to construct tailored LLM agents to address the aforementioned limitations in UrbanKG construction.

In fact, constructing an LLM agent compatible with various UrbanKGC tasks is a non-trivial problem due to the following two challenges: _(1) Challenges 1: How to adapt LLMs for UrbanKGC?_ LLMs may not align well with the specific task due to the gap  between the natural language processing corpus for training LLMs and the domain-specific corpus in urban domain . For example, the urban text data is usually heterogeneous and contains multifaceted urban knowledge (e.g., spatial, temporal, and functional aspects) . As shown in Figure 1(a), the text description of _"Columbia University"_ reflects its geographic spatial locations (i.e., spatial relationship), construction timelines (i.e., temporal relationship), and how it provides educational service for the city (i.e., functional relationship). LLMs may require a **tailored alignment to understand heterogeneous urban relationships** to extract these urban spatial, temporal, and functional relations accurately. _(2) Challenges 2: How to improve the capacity of LLMs for UrbanKGC?_ The effectiveness of LLMs for urban knowledge graph construction is restricted by their feeble numerical computation capacity [20; 21], leading to their disability in complex geospatial relationship extraction [22; 23]. However, the urban geospatial relationship plays a vital role in urban semantic modeling  and has been widely incorporated in previous UrbanKGs [8; 24]. As can be seen in Figure 1, extracting _"disconnected"_ relation between the geo-entity _"Columbia University"_ and _"Empire State Building"_ is useful for urban geo-semantic modeling. Accurately extracting such geospatial relationship demands necessary geospatial computing (e.g., utilizing latitude and longitude for distance calculation) and reasoning (i.e., deriving calculation results for geospatial relation reasoning) capabilities. It is appealing to improve the **geospatial computing and reasoning ability** of LLMs to satisfy the UrbanKGC task requirement.

To address the aforementioned challenges, in this study, we propose **UrbanKGent**, a unified LLM agent framework for automatic UrbanKG construction. Figure 2 illustrates the overview of UrbanKGent. For a given city, we first generate a knowledgeable instruction set for UrbanKGC tasks (relational triplet extraction and knowledge graph completion) from urban geographic and text data sources. By heterogeneity-aware and geospatial-infused instruction generation, as shown in Figure 1(a), various urban spatiotemporal relationship knowledge can be encoded into the instruction, which facilitates alignment between LLMs with the target UrbanKGC tasks. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectory derived by distilling GPT-4 with the above constructed instructions. Based on geospatial tool augmentation and self-refinement, the deficiency of LLMs in geospatial computing and reasoning could be alleviated, and unfaithful trajectories could be filtered out. Finally, we perform hybrid instruction fine-tuning based on the enhanced and refined trajectories on Llama 2-7/13B and Llama 3-8B variants  by using LoRA . The obtained UrbanKGent agent including 7/8/13B version, is feasible for completing multiple UrbanKGC tasks cost-effectively without extra GPT-API cost.

Figure 1: Illustrative example of urban relational triplet extraction and knowledge graph completion. (a) The heterogeneous relationship understanding limitation of LLMs can be addressed by injecting prior urban knowledge into instruction. (b) The geospatial computing limitation of LLMs can be alleviated by invoking external geospatial tools.

We conduct comprehensive experiments on two UrbanKGC tasks in two metropolises (New York City and Chicago) using both human evaluation and GPT-4-based self-evaluation. The empirical results validate the effectiveness of the proposed LLM agent for completing various UrbanKGC tasks. Moreover, the obtained UrbanKGent family (7/8/13B version) could help extract the same scale of triplets and entities of existing UrbanKG benchmark  using only one-fifth of data, and even expand the types of relations by hundreds of times.

Our contributions are summarized as follows: (1) We propose the first UrbanKGC agent framework **UrbanKGent** and UrbanKGent family to provide real-world UrbanKGC service, offering new opportunities to advance UrbanKG studies. (2) We propose a knowledgeable instruction generation module and a tool-augmented iterative trajectory refinement method, which align LLMs to UrbanKGC tasks and compensate for their geospatial computing and reasoning inability. (3) Extensive experiments on two real-world datasets validate the effectiveness of proposed framework and uncover its exceptional performance across UrbanKGC tasks.

## 2 UrbanKGC Data Description

### Data Collection

We first acquire urban knowledge for two large cities New York City and Chicago from two data sources. Table 1 summarizes the statistics of the raw datasets.

#### 2.1.1 Geographic Data

The geographic data provides critical urban spatial structure information and functional semantics, which has been widely used in previous UrbanKG studies [8; 24; 27; 28].

**Area-Of-Interst (AOI) Data**. AOI data describes the urban spatial area structure, including urban commercial areas (e.g., shopping centers), residential areas (e.g., communities), and so on. In this work, we first follow UUKG  to acquire the AOI name and geometry value from NYC Gov 3 and CHI Gov 4. Next, we use the AOI name to search their text description from Wikipedia and C4 5 dataset. Each AOI record contains an AOI name, a polygon geometry value, and a text description. For example, _["Jamaica Bay", polygon (-73.86 40.58,...), "Jamaica Bay is an estuary..."]_ is the record of the AOI _"Jamaica Bay"_ with geometry value and text description.

**Road Network Data**. Road data describes the urban spatial network, including urban motorways, overpasses, and so on. We first follow  to obtain the road name and geometry value from Open Street Map (OSM 6. Then, following the same text acquisition operation in AOI data, we crawl the textual description of each road record from Wikipedia. Each road record contains a road name, a linestring geometry value, a road type and a text description. For example, _["Central Park Avenue", linestring (-73.87 40.90,...), primary, "Central Park Avenue is a boulevard in..."]_ describes the primary road named _"Central Park Avenue"_ with a linestring geometry value and its textual description.

**Point-Of-Interest (POI) Data**. POI data represents different urban functions (e.g., residential and commercial), which have been widely adopted in many recent UrbanKG works [24; 6; 8]. We first follow  to obtain the POI name, and geometry value from OSM. Then the textual description of each POI record could be crawled from Wikipedia following the similar process. Each POI record contains a POI name, a coordinate geometry, a POI type, and a text description. For example,

Figure 2: The framework of UrbanKGent.

["Trump World Tower", coordinate (-73.96 40.75), residential, "Trump World Tower is a residential condominium..."] is the record of the POI "Trump World Tower"._

#### 2.1.2 Text Data

The text data provides rich contextual knowledge of the city space from different perspectives (e.g., the spatial context) , and it plays an important role in geospatial understanding. In this work, we collect two types of text corpus.

**Review Data**. The review of urban places provides commercial information that citizens use to make business decisions , playing a critical role in urban knowledge distillation. We collect review data from Google Map 7. Specifically, we first manually split the city into multiple rectangular regions, then we utilize the Google Map API to query the places contained within each region and their reviews. Each review record contains a place name, a coordinate geometry value, a rating, and a text review. For example, [_"Lifestyles Academy Inc", coordinate (-87.87 41.65), 4.9, "Very nice organization and..."]_ is the review record of place "_Lifestyles Academy Inc"_.

**Web Page Data**. The web page data works as the general text corpus for the city, and it contains rich geoscience knowledge that has been utilized in recent urban entity and relation extraction studies . We collect web page data from the Google search engine. Specifically, we first input the name of the crawled AOI, Road, and POI record into Google. Then we concatenate the textual sentences of the top 10 retrieved web pages. Each web page record contains a long urban text description.

### Data Preprocessing

Before constructing the UrbanKGC dataset, we first preprocess the raw datasets. We filter out AOIs, roads, POIs, reviews, and web pages whose crawled textual descriptions are null value, too short (e.g., less than ten word description) or meaningless (e.g., just repeating the POI name). In addition, we remove irrelevant information from the text description, such as non-English characters, non-ASCII gibberish, website addresses, and so on. More details can be found in Appendix A.

## 3 Preliminary

This section presents the UrbanKGC task definition and provides task analysis.

### Task Definition and Problem Formulation

Before diving into the technical details, we first introduce the definition of UrbanKG:

**Definition 1**: _UrbanKG. The UrbanKG is defined as a multi-relational graph \(\) = (\(\), \(\), \(\)), where \(\), \(\) and \(\) is the set of urban entities, relations and facts, respectively. In particular, facts are defined as \(=\{ h,r,t h,t,r\}\), where each triplet \( h,r,t\) describes head entity \(h\) is connected with tail entity \(t\) via relation \(r\)._

The UrbanKG encodes diverse urban semantic knowledge by connecting urban entities into a multi-relational graph. This work aims to construct an UrbanKG from collected unstructured text data. We decompose the UrbanKG construction (UrbanKGC) process into two sequential knowledge graph construction tasks, namely relational triple extraction  and knowledge graph completion . We first provide the basic definition for these two subtasks, and then introduce the problem formulation of this work.

#### 3.1.1 Task Definition

**Relational Triplet Extraction (RTE).** Given the unstructured texts, this task achieves joint extraction of entities and their relations  which are in the form of a triplet \( h,r,t\). For instance, given the

  Dataset & Description & New York City & Chicago \\   & \# of AOI & 192 & 136 \\  & \# of road & 6,765 & 2,241 \\  & \# of POI & 5,872 & 5,877 \\   & \# of review & 16,360 & 13,627 \\  & \# of web page & 11,596 & 7,283 \\  

Table 1: The statistics of raw datasets.

urban text sentence _"Columbia University is a private Ivy league research university in New York City."_, this task aims to identify two entities _"Columbia University"_ and _"New York City"_ and their relation _"locate-in"_, described as triplet _<Columbia University, locate-in, New York City>_.

**Knowledge Graph Completion (KGC).** Given a head entity \(h\) and a tail entity \(t\), this task is to predict the missing relation between them . For instance, given the head entity _"Columbia University"_ and the tail entity _"Empire State Building"_, this task is to predict that their missing relation, e.g., _"disconnected"_, described as triplet _<Columbia University, disconnected, Empire State Building>_.

#### 3.1.2 Problem Formulation

Given the urban unstructured text data, the desired output is an UrbanKG \(\). In this paper, this problem is decomposed into two sequential subtasks: (1) **Relational Triplet Extraction**: the first task extracts relational triplet \( h,r,t\) from the urban text data. The output of RTE task is \(_{1}\) =(\(\), \(_{1}\), \(_{1}\)), where \(\) and \(_{1}\)is the set of extracted entities and relations, while \(_{1}\) is the set of extracted triplets. (2) **Knowledge Graph Completion**: for the given head entity \(h\) and tail entity \(t\) in \(_{1}\), the second task is to predict the geospatial relationship8 between them. The output of this task is \(_{2}\) =(\(\), \(_{2}\), \(_{2}\)), where \(_{2}\) and \(_{2}\) is the set of completed relations and triplets. By sequentially completing the above two tasks, we can obtain the constructed UrbanKG \(\) = (\(\), \(_{1}_{2}\), \(_{1}_{2}\)).

### Quantitative Task Analysis

As shown in Figure 1, we qualitatively find that LLMs lack urban heterogeneous relationship understanding ability and experience in geospatial computing and reasoning difficulty when adopting it for UrbanKGC tasks. This subsection presents a quantitative analysis of these two challenges.

**Heterogenous Relationship Understanding.** The ability to understand heterogeneous relationships is ubiquitous in distilling knowledge from the massive urban corpus. For example, the text description in Figure 1 illustrates a place from spatial location, temporal time, and functional aspects. Capturing these heterogeneous semantics is important for urban knowledge distillation. We perform quantitative analysis by randomly sampling 50 urban text data and then prompt GPT-4 to complete relational triplet extraction by providing only the basic task description. As shown in Figure 3(a), we find the LLMs experience serious misjudgment (i.e., extract wrong triplets or miss the triplet) on urban spatial, temporal, and functional triplet extraction. This indicates the limited capacity of LLMs to understand heterogeneous relationships.

**Geospatial Computing and Reasoning.** Geospatial computing and reasoning techniques are widely used in many previous UrbanKG studies [8; 24] for urban geospatial relation extraction. In recent works [23; 31], the geospatial skills of LLMs have also been demonstrated to lack geospatial awareness and reasoning ability . To identify potential limitations, we quantitatively investigate how LLMs can perform on geospatial relation completion tasks. Specifically, we construct 100 head and tail entity pairs, covering five geospatial relations in the KGC task, and then prompt GPT-4 to predict with basic task description and geospatial relation candidates. As shown in Figure 3(b), we find that GPT-4 performs poorly on five geospatial relation completion. This further validates the disability of LLMs in geospatial computing and reasoning.

## 4 UrbanKGC Agent Construction

This section presents the proposed UrbanKGC agent construction framework.

Figure 3: Quantitative performance analysis of prompting GPT-4 for UrbanKGC tasks. The result is obtained by comparing 50 GPT-4’s outputs with the human’s annotation.

### Overview

The overall pipeline of the UrbanKGent framework is illustrated in Figure 4. _(1) Knowledgeable Instruction Generation_ consists of the heterogeneity-aware and geospatial-infused instruction generation modules for aligning LLMs to UrbanKGC tasks. _(2) Tool-augmented Iterative Trajectory Refinement_ proposes geospatial tool interface invocation and iterative self-refinement mechanisms to enhance and refine generated trajectory. _(3) Hybrid Instruction Fine-tuning_ fine-tune LLMs based on the refined trajectories for cost-effectively completing diverse UrbanKGC tasks.

### Knowledgeable Instruction Generation

We first construct the knowledgeable instruction to adopt LLMs for two UrbanKGC tasks, including relational triplet extraction (RTE) and knowledge graph completion (KGC). Figure 4(a) illustrates the overview of the instruction construction process of these two tasks.

**Heterogeneity-aware Instruction Generation for Relational Triplet Extraction.** As discussed in Section 3, the urban text contains diverse heterogeneous relationships, thus we consider multiple views with both urban entity and relation definition for relational triplet extraction. In particular, we construct a multi-view instruction template for the urban relational triplet extraction, including spatial view, temporal view, and functional view. Each view is a multi-turn question-answer dialog  consisting of entity recognition, relation extraction, and triplet extraction module.

For the spatial view, we devise a two-turn dialog to align LLMs for spatial triplet extraction. In the first turn, we inject spatial entity and relation definition into the instruction template to guide LLMs to understand spatial characteristics and then extract potential spatial entities (e.g., _University_) and relations types (e.g., _locate-in_). In the second turn, the extracted types are explicitly fed into the instruction template for spatial triplet extraction. Intuitively, the spatial view allocates dedicated urban knowledge for LLMs to extract urban spatial relationships. Similarly, we construct the temporal view and functional view for corresponding temporal and functional triplet extraction, independently.

**Geospatial-infused Instruction Generation for Knowledge Graph Completion.** Despite heterogeneity-aware instruction enabling LLMs to extract urban triplets from various perspectives, the geospatial relationship between geospatial entities cannot be directly extracted. Therefore, we introduce a geospatial-infused instruction generation module to guide LLMs to complete missing geospatial relationships.

First, we incorporate the geometry information (i.e., the latitude and longitude) of geo-entities into instruction, so that the LLMs can utilize these geospatial values for relation inference. Second, we add the geospatial relationship definition to the instruction to guide LLMs in understanding the geospatial relationship definition. Intuitively, LLMs can refer to geospatial knowledge and make practical solutions for the KGC task. We provide the detailed instruction template in the Appendix B.

### Tool-augmented Iterative Trajectory Refinement

#### 4.3.1 Trajectory Generation

With the initial UrbanKGC instructions constructed, the following step is to generate reasoning trajectories , which will be used to fine-tune LLMs tailored to UrbanKGC task. Specifically, we follow FireAct  and use Chain-of-Thought (CoT) , a gradient-free technique, to prompt

Figure 4: An overview of UrbanKGent Construction.

GPT-4 (i.e., add prompt trigger _"Let's think step by step"_ at the end of RTE and KGC instructions template) to generate the reasoning trajectories for UrbanKGC tasks.

The generated CoT trajectories could provide a step-by-step reasoning solution for UrbanKGC tasks. Nevertheless, the complex geospatial relationships cannot be easily extracted as discussed in Section 3 and recent geospatial reasoning works . Therefore, we introduce a tool invocation module to guide LLMs to invoke tailored external geospatial tools  to enhance their geospatial computing and reasoning capacity for UrbanKGC tasks.

#### 4.3.2 Tool Invocation for Trajectory Augmentation

We conduct two sequential procedures: tool invocation for geospatial computing support and trajectory deliberation for reasoning enhancement.

**Tool Invocation.** First, we construct a geospatial reasoning toolkit (e.g., distance calculation, eight interfaces in total shown in Table 6) by prompting GPT-4 for self-programming. Then, we construct tailored prompts to guide LLMs to invoke these interfaces. Specifically, the prompt is concatenated with an illustrative description of the function of each geospatial tool and a task instruction (i.e., _"Which types of tool interface you need"_). Intuitively, the external tool allocates calculation results for LLMs to infer missing geospatial relation. The toolkit description can be found in Appendix C.

**Trajectory Deliberation.** After manipulation with external tools, we prompt LLMs to refine uncertain reasoning steps based on these obtained manipulation results. Specifically, we construct the prompt by concatenating with manipulation results (e.g., the distance and geohash value of geo-entity) and a task instruction (i.e., _"Please refine your reasoning process"_). After feeding the prompt into GPT-4, the enhanced trajectory is obtained. Detailed prompt information can be found in Appendix C.

#### 4.3.3 Iterative Trajectory Self-refinement

Despite tool-augmented deliberation improving the geospatial computing and reasoning ability of LLMs, enhanced trajectories may not all be faithful . To alleviate potential error and ensure the trajectory quality , we refine these trajectories via an iterative self-refinement mechanism . Specifically, we iterate two sequential blocks: (i) Trajectory verifier: given the trajectory, the verifier aims to provide feedback for refining the reasoning process; (ii) Trajectory updater: given the trajectory and feedback, the updater will further refine the current trajectory based on the feedback.

**Trajectory Verifier.** We construct a tailored prompt to ask LLMs to generate feedback. Specifically, we use a simple but effective trigger (_"Judge whether all extracted triplets are correct and provide improvement suggestion"_) to prompt LLMs to provide feedback. If the trajectory no longer requires modification, we let LLMs respond with _"This is a faithful trajectory"_. Such a verification step lets LLMs make reflections and improve the correctness of the trajectory.

**Trajectory Update.** The updater utilizes provided feedback to refine the current trajectory via prompt trigger _"Follow suggestion to refine the reasoning process"_. Intuitively, the feedback may address multiple aspects (e.g., missed triplet in the RTE task or unfaithful reasoning process in the KGC task) of the unfaithful trajectories.

We iterate the trajectory verifier and updater until the predefined stopping condition is satisfied. The stopping condition is determined by either meeting the maximum number of iterations (we set it at three to avoid excessive cost) or when the verifier confirms all trajectories are faithful. Upon meeting the stopping condition, we use the last refined trajectory for further fine-tuning. Detailed prompt information can be found in Appendix C.

### Hybrid Instruction Fine-Tuning

To construct a cost-effective UrbanKGC agent, we further utilize trajectories (generated by GPT-4) to fine-tune a smaller open-source LLM for faster inference speed and lower cost (i.e., prompting GPT-4 for UrbanKGC is expensive). Specifically, we finetune the LLM via the mixed-task instruction-tuning strategy . The goal is to enhance the LLMs' capabilities in diverse UrbanKGC tasks.

**Mixture Training.** Set the base language model as \(\), and \(_{}(y x)\) represents the probability distribution of response \(y\) given instruction \(x\). We consider the trajectory set on two UrbanKGC tasks, i.e., \(_{RTE}\) and \(_{KGC}\). Since both the instruction and the target output are formatted in natural language, we can unify the training into an end-to-end sequence-to-sequence way. Formally, the optimization process aims to minimize the loss of language model \(\) as follows:

\[=_{(x,y)_{}}[_ {}(y x)]+_{(x,y)_{}} [_{}(y x)],\] (1)

where \(x\) and \(y\) represent the instruction input and instruction output in the trajectory, respectively.

**Training Setup.** We choose the chat version of open-sourced Llama 2-7/13B and Llama-3-8B as our backbone models, and fine-tune Llama using LoRA strategy .

### Inference on UrbanKGC Task

Via hybrid instruction fine-tuning, the obtained LLM UrbanKGen can be trained to follow the instructions to finish the UrbanKGC task. We prompt UrbanKGen to complete UrbanKGC tasks by following the pipeline shown in Figure 4. For the RTE task, we sequentially execute entity recognition, relation extraction, and relational triplet instruction generation, iterative self-refinement and output the extracted triplets. For the KGC task, we sequentially execute KGC instruction generation, external tool augmentation, iterative self-refinement block, and finally output the completed triplets.

## 5 Experiments

### Experimental Settings

**Dataset.** In this work, two sequential tasks (i.e., RTE and KGC) of UrbanKGC are within an open-world setting (i.e., no predefined ontology) [40; 41]. We construct the RTE and KGC datasets of NYC and CHI by sampling uniformly from five raw data in Table 1, respectively. As shown in Table 2, we first construct two small datasets (i.e., NYC-Instruct and CHI-Instruct) for instruction fine-tuning and two middle datasets (i.e., NYC and CHI) to validate the performance of the constructed UrbanKGC agent. The remaining data works as the large-scale UrbanKGC dataset (i.e., NYC-Large and CHI-Large) in real-world scenarios shown in Table 5. The three types of datasets are non-overlapping to prevent data leakage. More dataset construction details are in Appendix A.

**Baseline Methods.** We provide a comprehensive comparison of our method with existing paradigms: **(1) End-to-end Models:** For the zero-shot RTE task, we utilize the end-to-end generation model RelationPrompt  and PRGC . For the KGC task, we fine-tune KG-BERT  and KG-T5  with the QA pairs constructed from the self-instruct dataset. **(2) LLMs-based Zero-shot Reasoning :** We directly prompt the LLMs with basic task definitions to get the answer without training. **(3) LLMs-based In-context Learning :** We sample 3-shot QA pairs as demonstrations from the self-instruct dataset as examples and get the answers from the LLMs without training. **(4) Vanilla Fine-tuning :** We directly fine-tune the LLMs using the QA pairs constructed from the self-instruct dataset, and then prompt the LLMs with basic task definition without demonstrations. **(5) UrbanKGen Inference:** We directly prompt the LLMs using the UrbanKGgent inference pipeline in Section 4.5. The prompt templates of the above baseline methods are in Appendix B.

**Implementation and Detail Settings.** In our experiment, we select Vicuna , Alpaca , Mistrial , Llama-2 , Llama-3 , GPT-3.5  and GPT-4  as the backbone LLM \(\). All experiments are conducted on eight NVIDIA A800 GPUs. For the GPT-3.5 and GPT-4, we adopt the gpt-3.5-turbo-16k-0613 API and gpt-4-0613 API.

**Evaluation Protocol.** Since UrbanKGC tasks in this work follow an open-world setting where labels are not visible, the classical metric (e.g., F1 and Hits@10) is not applicable. In this work, we regard evaluation as the binary classification, i.e., if the extracted triplet in RTE task is correct and if the completed relation in KGC task is correct. We follow recent LLMs-based KGC works  to employ accuracy as an evaluation metric. To make a comprehensive evaluation of the experimental results, we employ both of the human evaluation and GPT evaluation, which has been widely used in many LLM studies [52; 45]. For **Human Evaluation**, we employ human annotators to evaluate the results on

   &  &  &  &  &  \\   & RTE & 232 & 2,089 & 40,480 & 122 & 1,102 & 28,868 \\  & KGC & 232 & 2,080 & 33,534 & 122 & 1,101 & 28,607 \\  

Table 2: The statistics of constructed UrbanKGC dataset.

200 random samples. As for the **GPT Evaluation**, we use GPT-4 to evaluate the model performance on the full data to escape intensive labor. In this work, the GPT-4's evaluation has been demonstrated to be consistent with the human evaluation. Detail is in Appendix D.

### Main Result

The performance results are reported in Table 3. As can be seen, the constructed agent outperforms all thirty-one baseline models on two UrbanKGC datasets. Specifically, the UrbanKgent-13B achieves (15.56%, 14.29%, 14.89%, and 11.90%) improvements compared with the state-of-the-art GPT-4 with the same inference pipeline on NYC. The improvements on CHI are (15.22%, 17.07%, 13.46%, and 13.95%), respectively. Moreover, the UrbanKgent-7/8B also achieve comparable performance compared with the GPT-4.

Meanwhile, we observe that the zero-shot LLMs perform poorly in the UrbanKGC tasks, even using GPT-4. In addition, although the demonstrations provided by In-context-learning can incorporate the UrbanKGC task information, the performance gain is limited. Besides, we find that fine-tuning LLMs can make obvious improvements in the overall performance. Through vanilla fine-tuning, the Llama-2-7/13B and Llama-3-8B could achieve comparable performance with GPT-3.5 under the ZSL settings.

Moreover, although the various LLM backbones using the UrbanKGent inference pipeline perform slightly worse than the vanilla fine-tuning method, they could obtain better performance compared

  Dataset & \# Entity & \# Relation & \# Triplet & Data Volume \\  NYC-Large & 228,928 & 2,138 & 905,442 & 40,480 \\ CHI-Large & 95,813 & 1,336 & 563,290 & 28,607 \\  NYC-IJUKG & 236,287 & 13 & 930,240 & 236,277 \\ CHI-UUKG & 140,602 & 13 & 564,400 & 140,577 \\  

Table 4: Statistics comparison of constructed UrbanKGs in New York and Chicago between UrbanKGent and existing benchmark.

   &  &  &  \\  & & **GPT (acc/confidence)** &  &  &  \\  & & **RTE** & **KGC** & **RTE** & **KGC** & **RTE** & **KGC** & **RTE** & **KGC** \\   End-to-end \\ Models & KG-BERT & - & 0.24/3.15 & - & 0.23 & - & 0.19/4.12 & - & 0.24 \\  & KG-T5 & - & 0.21/4.02 & - & 0.21 & - & 0.15/3.98 & - & 0.24 \\  & RelationPrompt & 0.12/3.38 & - & 0.12 & - & 0.21/3.53 & - & 0.18 & - \\  & PRC & 0.08/4.01 & - & 0.06 & - & 0.13/4.15 & - & 0.15 & - \\   Zero-shot \\ Reasoning \\  } & Vicuna-7B & 0.12/2.84 & 0.19/4.06 & 0.14 & 0.16 & 0.22/4.12 & 0.14/4.03 & 0.21 & 0.18 \\  & Alpaca-7B & 0.11/3.75 & 0.17/3.87 & 0.15 & 0.17 & 0.23/3.96 & 0.16/4.15 & 0.20 & 0.16 \\  & Miami-7B & 0.14/1.42 & 0.21/4.11 & 0.17 & 0.18 & 0.21/3.75 & 0.15/3.76 & 0.19 & 0.19 \\  & Llama-2-7B & 0.14/1.98 & 0.18/3.75 & 0.16 & 0.18 & 0.26/1.96 & 0.15/2.83 & 0.21 & 0.22 \\  & Llama-3-8B & 0.15/4.02 & 0.15/4.02 & 0.20 & 0.21 & 0.24/3.75 & 0.15/4.08 & 0.22 & 0.22 \\  & Llama-2-13B & 0.21/20.7 & 0.28/3.91 & 0.19 & 0.22 & 0.22/2.19 & 0.16/2.47 & 0.22 & 0.24 \\  & Llama-2-7B & 0.25/3.07 & 0.28/3.75 & 0.22 & 0.24 & 0.27/3.55 & 0.16/2.47 & 0.24 & 0.23 \\  & Llama-3-7B & 0.26/4.18 & 0.29/4.31 & 0.23 & 0.24 & 0.26/3.98 & 0.17/4.26 & 0.25 & 0.23 \\  & GPT-3.5 & 0.29/4.11 & 0.36/4.47 & 0.31 & 0.23 & 0.13/3.79 & 0.31/3.16 & 0.31 & 0.29 \\  & GPT-4 & 0.38/4.03 & 0.39/3.82 & 0.41 & 0.29 & 0.39/4.08 & 0.32/4.03 & 0.43 & 0.35 \\   In-context \\ Learning \\  } & Llama-2-7B & 0.18/2.15 & 0.21/3.96 & 0.19 & 0.18 & 0.25/2.44 & 0.18/3.27 & 0.23 & 0.20 \\  & Llama-3-8B & 0.17/4.06 & 0.18/3.53 & 0.21 & 0.22 & 0.28/4.31 & 0.17/4.14 & 0.24 & 0.21 \\  & Llama-2-13B & 0.26/3.52 & 0.31/3.28 & 0.23 & 0.24 & 0.28/2.65 & 0.21/2.53 & 0.25 & 0.26 \\  & GPT-3.5 & 0.41/4.65 & 0.42/4.08 & 0.42 & 0.31 & 0.36/4.24 & 0.36/4.23 & 0.39 & 0.36 \\   Unilla \\ Fine-tuning \\  } & Llama-2-7B & 0.32/4.37 & 0.38/3.65 & 0.32 & 0.27 & 0.29/3.80 & 0.30/3.65 & 0.33 & 0.31 \\  & Llama-3-8B & 0.31/4.18 & 0.35/4.18 & 0.35 & 0.26 & 0.31/4.18 & 0.29/4.15 & 0.32 & 0.34 \\  & Llama-2-13B & 0.35/4.26 & 0.41/3.92 & 0.39 & 0.29 & 0.31/4.14 & 0.29/3.87 & 0.37 & 0.35 \\   Urban-7B \\ UrbanKGent \\  } & Vicuna-7B & 0.24/3.07 & 0.24/3.95 & 0.29 & 0.23 & 0.27/4.12 & 0.22/3.95 & 0.23 & 0.25 \\  & Alpaca-7B & 0.26/3.85 & 0.27/3.83 & 0.26 & 0.22 & 0.27/3.83 & 0.21/4.12 & 0.27 & 0.29 \\  & Mistral-7B & 0.26/4.15 & 0.25/4.08 & 0.28 & 0.23 & 0.25/5.61 & 0.21/4.08 & 0.25 & 0.26 \\  & Llama-7B & 0.27/3.05 & 0.26/4.12 & 0.28 & 0.24 & 0.27/2.87 & 0.24/5.44 & 0.26 & 0.29 \\  & Llama-3-8B & 0.29/4.15 & 0.31/4.08 & 0.33 & 0.26 & 0.26/3.28 & 0.24/3.97 & 0.30 & 0.31 \\  & Llama-2-13B & 0.31/3.87 & 0.32/3.56 & 0.35 & 0.27 & 0.28/3.24 & 0.26/3.28 & 0.31 & 0.32 \\  & Llama-2-7B & 0.34/2.85 & 0.34/2.37 & 0.33 & 0.29 & 0.29/3.80 & 0.28/4.01 & 0.32 & 0.34 \\  & Llama-3-7B & 0.35/4.26 & 0.36/4.81 & 0.34 & 0.28 & 0.29/4.12 & 0.29/4.81 & 0.31 & 0.35 \\  & GPT-3.5 & 0.43/4.12 & 0.46/3.88 & 0.43 & 0.34 &with zero-shot reasoning and In-context learning paradigms. Such results demonstrate the benefit of knowledgeable instruction design and external tool innovation, but also indicate its performance bottleneck. As a deeper exploration, our work fills this gap through hybrid instruction fine-tuning, and the fine-tuned UrbanKGC agents, whether 7B, 8B or 13B, can achieve state-of-the-art performance in UrbanKGC tasks. We provide an in-depth analysis of the proposed UrbanKGent framework in Appendix E.2.

### Agent Application

We first derive UrbanKGent-13B for initial UrbanKGs acquisition in New York City and Chicago. After proper filtering and merging of the triplets, we obtain two large-scale UrbanKGs shown in Table 4. Compared with existing UrbanKG benchmark , we only use roughly one-fifth of the data for constructing the UrbanKGs with the same scale of triplets and entities, and even expanding the variety of relationships to a hundred times the original types. Moreover, we also provide efficiency analysis in Figure 5. As can be seen, UrbanKGent-13B achieves lower inference speed in latency and reduce the cost by roughly 20 times in both of RTE and KGC tasks. More details is in Appendix E.3.

## 6 Related work

**Domain-Oriented Agent Construction.** The concept of language agent  has become very popular recently, and a variety of LLM agents targeting different domains have been proposed. For example, Voyager  is constructed for automated game exploration, WebGPT  is an HTML agent for diverse document understanding tasks, LLMLight  constructs a language agent for transportation domain, K2 , GeoGalactica  and GeoLLM  propose to re-train language agent for geospatial semantic understanding. In addition, many recent works like Auto-GPT  and CAMEL  aim at proposing an autonomous agent framework for agent construction. Nevertheless, there is still no UrbanKGC agent construction framework for the urban computing domain.

**LLMs for Knowledge Graph Construction.** Recently, the advent of LLMs  invigorated the field of NLP. Many studies have begun to explore the potential of LLMs in the domain of KG construction. For example, [32; 57] finds that transforming the NER and RE task into a multi-turn question-answering dialog could improve the model performance.  explicitly derive syntactic knowledge to guide LLMs to think, which could develop the performance of NER. Despite these LLM-driven KG construction methods [58; 40] in general domains being widely investigated, KG construction in urban domain still remains an open challenge .

**Urban Knowledge Graph.** Urban knowledge graph has been proven useful in various urban tasks, such as traffic flow prediction [60; 61; 27; 62], mobility prediction , site selection , city profiling , crime prediction and so on [8; 64; 65]. Their common approach involves manually extracting urban entities and defining urban relations to construct an urban knowledge graph. For example,  construct a dedicated spatiotemporal knowledge graph regarding trajectory and timestamp as entities to improve trajectory prediction and  construct user check-in relations to help mobility prediction. Nevertheless, existing UrbanKGs heavily rely on manual design, leading to high labor costs.

## 7 Conclusion

In this work, we proposed UrbanKGent, the first automatic UrbanKG construction agent framework with LLMs. We first constructed a knowledgeable instruction set to adopt LLMs for different UrbanKGC tasks. Then, we proposed a tool-augmented iterative trajectory refinement module to facilitate the instruction tuning of various large language models. Extensive experimental results demonstrate the advancement of UrbanKGent in improving UrbanKGC tasks. The obtained UrbanKGent agent family, consisting of 7/8/13B version, with lower latency and cost compared with deriving GPT-4 for UrbanKG construction. We hope the open-source UrbanKGent can foster future urban knowledge graph research and broader smart city applications.

Figure 5: The model latency and cost of constructed UrbanKGent-13B and GPT-4 in UrbanKGC. We report the total inference time and cost of 1,000 RTE and KGC tasks.