# TaskMet: Task-Driven Metric Learning for Model Learning

Dishank Bansal

Work done as part of the Meta AI residency program.

Ricky T. Q. Chen Mustafa Mukadam Brandon Amos

Meta

###### Abstract

Deep learning models are often used with some downstream task. Models solely trained to achieve accurate predictions may struggle to perform well on the desired downstream tasks. We propose using the task loss to learn a metric which parameterizes a loss to train the model. This approach does not alter the optimal prediction model itself, but rather changes the model learning to emphasize the information important for the downstream task. This enables us to achieve the best of both worlds: a prediction model trained in the original prediction space while also being valuable for the desired downstream task. We validate our approach through experiments conducted in two main settings: 1) decision-focused model learning scenarios involving portfolio optimization and budget allocation, and 2) reinforcement learning in noisy environments with distracting states. The source code to reproduce our experiments is available here.

## 1 Introduction

Machine learning models for prediction are typically trained to maximize the likelihood on a training dataset. While the models are capable of universally approximating the underlying data generating process to predict the output, they are prone to approximation errors due to limited training data and model capacity. These errors lead to suboptimal performance in downstream tasks where the models are used. Furthermore, even though a model may appear to have reasonable predictive performance on the metric and training data it was trained on, such as the mean squared error, employing the model for a downstream task may require the model to focus on different parts of the data that were not emphasized in the training for predictive performance. Overcoming the discrepancy between the model's prediction task and performance on a downstream task is the focus of our paper.

Examples of settings where the model's prediction loss \(_{}\) is mis-matched from the downstream task \(_{}\) include the following, which table1 also summarizes:

1. the _portfolio optimization_ setting from Wilder et al. (2019), which predicts the expected returns from stocks for a financial portfolio. Here, the \(_{}\) is the MSE and \(_{}\) is from the regret of running a portfolio optimization problem on the output;
2. the _allocation_ setting from Wilder et al. (2019), which predicts the value of items that are being allocated, e.g. click-through-rates for recommender systems. Here, \(_{}\) is the MSE and \(_{}\) measures the result of allocating the highest-value items.

Figure 1: The _MSE_ results in a model close to the true model in the prediction space, but may give poor task performance. _Decision-focused learning_ (DFL) methods optimize the task loss, but may deviate from the prediction space. _TaskMet_ optimizes the task loss while retaining the prediction task.

3. the _model-based reinforcement learning_ setting of learning the system dynamics from Nikishin et al. (2022). Here, \(_{}\) is the MSE of dynamics model and the \(_{}\) measures how well the agent performs for downstream value predictions.

Motivated by examples such as in table 1, the research topics of _end-to-end task-based model learning_(Bengio, 1997; Donti et al., 2017), _decision-focused learning_(Wilder et al., 2019), and _Smart "Predict, then Optimize"_(Elmachtoub and Grigas, 2022) study how to use information from the downstream task to improve the model's performance on that particular task. Task-based learning has applications in financial price predictions (Bengio, 1997; Elmachtoub and Grigas, 2022), inventory stock, demand, and price forecasting (Donti et al., 2017; Elmachtoub and Grigas, 2022; El Balghiti et al., 2019; Mandi et al., 2020; Liu et al., 2023), dynamics modeling for model-based reinforcement learning (Farahmand et al., 2017; Amos et al., 2018; Farahmand, 2018; Bhardwaj et al., 2020; Voelcker et al., 2022; Nikishin et al., 2022), renewable nowcasting (Vohra et al., 2023), vehicular routing (Shi and Tokekar, 2023), restless multi-armed bandits for maternal and child care (Wang et al., 2022), medical resource allocation (Chung et al., 2022), and budget allocation, matching, and recommendation problems (Kang et al., 2019; Wilder et al., 2019; Shah et al., 2022).

**Limitations of task-based learning.** Task-based model learning comes with the goal of being able to discover task-relevant features and data-samples on its own without the need of explicit inductive biases. The current trend for end-to-end model learning uses task loss along with the prediction loss to train the prediction models. Though easy to use, these methods may be limited by 1) the prediction overfitting to the particular task, rendering it unable to generalize; 2) the need to tuning the weight combining the task and prediction losses as in eq.1.

**Our contributions.** We propose one way of overcoming these limitations: use the task-based learning signal not to directly optimize the weights of the model, but to _shape_ a prediction loss that is constructed in a way so that the model will always stay in the original prediction space. We do this in section3 via metric learning in the prediction space and use the task signal to learn a parameterized Mahalanobis loss. This enables more interpretable learning of the model using the metric compared to learning with a combination of task loss and prediction loss. The learned metric can uncover underlying properties of the task that are useful for training the model, e.g. as in figs.4 and 7. Section4 shows the empirical success of metric learning on decision focused model learning and model-based reinforcement learning. Figure1 illustrates the differences to prior methods.

## 2 Background and related work

**Task-based model learning**. We will mostly focus on solving regression problems where the dataset \(:=\{(x_{i},y_{i})\}_{i=1}^{N}\) consists of \(N\) input-output pairs, which we will assume to be in Euclidean space. The model makes a prediction \(:=f_{}(x)\) and is parameterized by \(\). The model has an associated prediction loss, \(_{}\), and is used in conjunction with some downstream task that provides a task loss, \(_{}\), which characterizes how well the model performs on the task. The most relevant related work to ours includes the approaches of Bengio (1997); Donti et al. (2017); Farahmand et al. (2017); Kang et al. (2019); Wilder et al. (2019); Nikishin et al. (2022); Shah et al. (2022); Voelcker et al. (2022); Nikishin et al. (2022); Anonymous (2023); Shah et al. (2023), which learn the optimal prediction model parameter \(\) to minimize the task loss \(_{}\):

\[^{}:=*{arg\,min}_{}_{}( )+_{}(), \]

where \(\) is a regularization parameter to weigh the prediction loss which is MSE error (eq.2) in general. Alternatives to eq.1 include 1) _Smart, "Predict, then Optimize"_(SPO) methods (Elmachtoub and Grigas, 2022; El Balghiti et al., 2019; Mandi et al., 2020; Liu et al., 2023), which

    & \((x)\) & \((y)\) & \((_{})\) & \((_{})\) \\  Setting & Features & Predictions & Prediction Loss & Task Loss \\  Portfolio optimization & Stock information & Expected return of a stock & MSE & Portfolio’s performance \\ Budget allocation & Item information & Value of item & MSE & Allocation’s performance \\ Model-based RL & Current state and action & Next state & MSE & Value estimation given the model \\   

Table 1: Settings we focus on where there is a discrepancy between the prediction task of a model and the downstream task where the model is deployed, i.e., \(_{}_{}\).

consider surrogates for when the derivative is undefined or uninformative, or 2) changing the prediction space from the original domain into a latent domain with task information, e.g. task-specific latent dynamics for RL (Hafner et al., 2019, 2019; Hansen et al., 2022). Extensions such as Gupta and Zhang (2023), Zharmagambetov et al. (2023), Ferber et al. (2023) learn surrogates to overcome computationally expensive losses in eq.1. Sadana et al. (2023) provide a further survey of this research area.

Separate from above line of work, the computer vision and NLP communities have also considered task-based losses for models: (Pinto et al., 2023) tune vision models with task rewards, e.g. for detection, segmentation, colorization, and captioning; Wu et al. (2021) consider representation learning for multiple tasks, Fernando and Tsokos (2021), Phan and Yamamoto (2020) consider weighted loss for class imbalance problems in classification, object detection.

Works such as Farahmand et al. (2017), Voelcker et al. (2022) use task loss in a different way compared to the above methods. They use task loss as a weighting term in the MSE loss itself. So the models are trained to focus more on samples with higher task loss. In their work, the task is the estimation of the value function in model-based RL. This can be seen as the instantiation of our work where the task loss is directly used as a metric instead of learning a metric.

Other related work on metric learning such as Hastie and Tibshirani (1995), Yang and Jin (2006), Weinberger and Tesauro (2007), Kulis et al. (2013), Hauberg et al. (2012), Kaya and Bilge (2019) often learns a non-Euclidean metric or distance that captures the geometry of the data and then solves a prediction task such as regression, clustering, or classification in that geometry. Other methods such as Voelcker et al. (2022) can handcraft metrics based on domain knowledge. In contrast to these, in the task-based model learning, we propose that the downstream task (instead of the data alone) gives the relevant metric for the prediction, and that it is possible to use end-to-end learning as in eq.4 to obtain the task-based metric.

**How our contribution fits in.** The mentioned methods mainly deal with using task-based losses to condition the model learning either by differentiation through task loss to update the model or using it directly as weighing for MSE prediction loss. Whereas our work focuses on using task loss to _learn_ a parameterized prediction loss which is then used to train the model. The task loss is not _directly_ used for model training.

## 3 Task-driven metric learning for model learning

We first present why it's useful to see the prediction space as a non-Euclidean metric space with an unknown metric, then show how task-based learning methods can be used to learn that metric.

### Metrics in the prediction space -- Mahalanobis losses

When defining a loss on the model, we are forced to make a choice about the geometry to quantify how good a prediction is. This geometric information is often implicitly set in standard learning settings and there are often no other reasonable choices without more information. For example, a supervised model \(f_{}\) parameterized by \(\) is often trained with the mean squared error (MSE)

\[^{*}_{}:=*{arg\,min}_{}_{(x,y) }[(f_{}(x)-y)^{2}]. \]

The MSE makes the assumption that the geometry of the prediction space is Euclidean. While it is a natural choice, it may not be ideal when the model needs to focus on important parts of the data that are under-emphasized under the Euclidean metric. This could come up by needing to emphasize some samples over others, or some dimensions of the prediction space over others.

While there are many possible geometries and metric spaces that could be defined over prediction spaces, they are difficult to specify without more information. We focus on the metric space defined by the Mahalanobis norm \(\|z\|_{M}:=(z^{}Mz)^{1/2}\), where \(M\) is a positive semi-definite matrix. The Mahalanobis norm results in the prediction loss

\[_{}(,):=_{(x,y)} [\|f_{}(x)-y\|^{2}_{_{}(x)}], \]

where \(_{}\) is a metric parameterized by \(\) and this is also conditional on the feature \(x\) so it can learn the importance of the regression space from each part of the feature space.

Many methods can be seen as hand-crafted ways of setting a Mahalanobis metric, including: 1) normalizing the input data by making the metric appropriately scale the dimensions of the prediction, 2) re-weighting the samples as in Donti et al. (2017); Lambert et al. (2020) by making the metric scale each sample based on some importance factor, or 3) using other performance measures, such as the value gradient in Voelcker et al. (2022).

More generally beyond these, the Mahalanobis metrics help emphasize the:

1. _relative importance of dimensions_. the metric allows for down- or up-weighting different dimensions of the prediction space by changing the diagonal entries of the metric. Figure 2 illustrates this.
2. _correlations in the prediction space_. the quadratic nature of the loss with the metric allows the model to be aware of correlations between dimensions in the prediction space.
3. _relative importance of samples_. heteroscedastic metrics \((x)\) enable different samples to be weighted differently for the final expected cost over the dataset.

Without more information, parameterizing and specifying the best metric for learning the model is challenging as it involves the subproblem of understanding the relative importance between predictions. We suggest that when it is available, the downstream task information characterizing the overall model's performance can be used to learn a metric in the prediction space. Hence, learning model parameters with a metricized loss can be seen as conditioning the learning problem. The ability to learn the metric end-to-end enables the task to condition the learning of the model in any or all of the three ways described above. This approach offers an interpretable method for the task to guide the model learning, in contrast to relying solely on task gradients for learning model parameters, which may or may not align effectively with the given prediction task.

### End-to-end metric learning for model learning

The key idea of the method is to learn a metric end-to-end with a given task, which is then used to train the prediction model as shown in eq. (3). The learning problem of the metric and model parameters are formulated as the bilevel optimization problem

\[^{}:=*{arg\,min}_{} _{}(^{}()), \] \[*{subject\,\,to} ^{}()=*{arg\,min}_{}_{}(,) \]

where \(\) and \(\) are (respectively) the metric and model parameters, \(_{}\) is the metricized prediction loss (eq. (3)) to train the prediction model, and \(_{}\) is the task loss defined by the task at hand (which could be another optimization problem, e.g. eq. (8), or another learning task, e.g. eq. (10).

**Gradient-based learning.** We learn the optimal metric \(_{^{}}\) with the gradient of the task loss, i.e. \(_{}_{}(^{}())\). Using the chain rule and assuming we have the optimal \(^{}()\) for some metric

Figure 2: Examples of the Mahalanobis loss from eq. (3) in a 2-dimensional prediction task. The model’s loss is zero only when \(=y^{}\). Here, the metric \(_{}(x)\) increases the weighting on the \(y_{0}\) component of the loss and thus emphasizes the predictions along this dimension.

parameterization \(\), this derivative is

\[_{}_{}(^{}())=_{} _{}()_{=^{}()} ()}{} \]

To calculate the term \(_{}_{}(^{}())\), we need to compute two gradient terms: \(_{}_{}()_{=^{ }()}\) and \(^{}()/\). The former can be estimated in standard way since \(_{}()\) is an explicit function of \(\). However, the latter cannot be directly calculated because \(^{}\) is a function of optimization problem which is multiple iterations of gradient descent, as shown in 11. (5). Backpropping through multiple iterations of gradient descent can be computationally expensive, so we use the implicit function theorem (appendix A) on the first-order optimality condition of 11, i.e. \(_{}(,)}{}=0\). Combining these, \(_{}_{}(^{}())\) can be computed with

\[_{}_{}(^{}())=_{} _{}() _{}(,)}{^{2}})^{-1}_{}(,)}{ }_{=^{}()}}_{^{}/} \]

The implicit derivatives in 11 may be challenging to compute or store in memory because the Hessian term \(^{2}_{}(,)/^{2}\) is the Hessian of the prediction loss with respect to the model's parameters. Approaches such as Lorraine et al. (2020) are able to scale related implicit differentiation problems to models with millions of hyper-parameters. The main insight is that the Hessian does not need to be explicitly formed or inverted and the entire implicit derivative term needed for backpropagation can be obtained with an implicit solver. We follow Blondel et al. (2022) and compute the implicit derivative by using conjugate gradient on the normal equations.

## 4 Experiments

We evaluate our method in two distinct settings: 1) when the downstream task involves an optimization problem parameterized by the prediction model output, and 2) when the downstream task is another learning task. For the first setting, we establish our baselines by replicating experiments from previous works such as Shah et al. (2022) and Wilder et al. (2019). These baselines encompass tasks like portfolio optimization and budget allocation. In the second setting, we focus on model-based

Figure 3: TaskMet learns a metric for predictions with the gradient from a downstream task loss.

reinforcement learning. Specifically, we concentrate on learning a dynamics model (prediction model) and aim to optimize the Q-value network using the learned dynamics model for the Cartpole task (Nikishin et al., 2022). Appendix B provides further experimental details and hyper-parameter information.

### Metric parameterization

We parameterize the metric using a neural network with parameters \(\), denoted as \(_{}:=L_{}^{}L_{}\), where \(L_{}\) is an \(n n\) matrix, where \(n\) is the dimension of the prediction space. This particular factorization constraint ensures that the matrix is positive semi-definite, which is crucial for it to be considered a valid metric. The neural network parameters are initialized to make \(_{}\) closer to the identity matrix \(\), representing the Euclidean metric. The learned metric can be conditional on the input, denoted as \(_{}(x)\), or unconditional, represented as \(_{}\), depending on the problem's structure.

### Decision-Focused Learning

#### 4.2.1 Background and experimental setup

We use three standard resource allocation tasks for comparing task-based learning methods (Shah et al., 2022; Wilder et al., 2019; Donti et al., 2017; Futoma et al., 2020). In this setting, resource utility prediction based on some input features constitute a prediction model, resource allocation constitutes the downstream task which is characterized by \(_{}\) The prediction model's output parameterized the downstream resource optimization. The settings are implemented exactly as in Shah et al. (2022) and have task losses defined by

\[_{}():=_{(x,y)}[g(z^{*}( ),y)] \]

where \(z^{*}():=_{z}g(z,)\) and \(g(z,y^{})\) is some combinatorial optimization objective over variable \(z\) parameterized by \(y^{}\). The task loss \(_{}\) is the expected value of objective function with decision variable \(z^{*}()\) induced by the prediction model \(=f_{}(x)\) under the ground truth parameters \(y\). We use corresponding surrogate losses to replicate the \(z^{*}()\) optimization problem as in Shah et al. (2022), Wilder et al. (2019), Xie et al. (2020) and differentiate through the surrogate using cvxpylayers(Agrawal et al., 2019).

These settings evaluate the ability of TaskMet to capture the correlation between model predictions and differentiate between different data-points in accordance to their importance for the optimization problem. Hence, we consider a heteroscedastic metric, i.e., \(_{}(x)\).

**Baselines.** We compare with standard baseline losses for learning models:

1. The standard MSE loss \(^{}=_{}_{(x,y)}[(f_{}( x)-y)^{2}]\). This method doesn't use any task information.
2. DFL (Wilder et al., 2019), which trains the prediction model with a weighted combination of \(_{}\) and \(_{}\) as in eq.1.
3. LODL Shah et al. (2022), which learns a parametric loss for each point in the training data to approximate the \(_{}\) around that point. That is, \(LODL_{_{n}}(_{n})_{}(_{n})\) for all \(n\). They create a dataset of \(\{(_{n},_{}(_{n}))\}\) for \(_{n}\) sampled around the \(y_{n}\). After this they learn the LODL loss for each point as \(_{n}^{}=_{_{n}}_{k=1}^{K}(LODL_{_{n}} (y_{n}^{k})-_{}(y_{n}^{k}))^{2}\). We chose the "Quadratic" variant of their method which is the closest to ours, where \(LODL_{_{n}}()=(-y)^{}_{n}(-y)\) where \(_{n}\) is a learned symmetric Positive semidefinite (PSD) matrix. LODL also uses eq.1 to learn the model parameters, but using \(LODL_{_{n}}(_{n})_{}(_{n})\)

**Experimental settings.** We use the following experimental settings from (Shah et al., 2022):

1. **Cubic**: This setting evaluates methods under model mismatch scenario where the model being learned suffers with severe approximation error. In this task, it is important for methods to allocate model capacity to the points more critical for the downstream tasks. _Prediction Model_: A linear prediction model \(f_{}(x):= x\) is learned for the problem where the ground truth data is generated by cubic function, i.e., \(y_{i}=10x_{i}^{3}-6.5x_{i},x_{i} U[-1,1]\). _Downstream task_: Choose top \(B=1\) out of \(M=50\) resources \(}=[_{1},,_{M}]\), \(z^{*}(}):=_{i}}\)2. **Budget Allocation**: Choose top \(B=2\) websites to advertise based on Click-through-rates (CTRs) predictions of \(K\) users on \(M\) websites. _Prediction Model_: \(}_{m}=f_{}(x_{m})\) where \(x_{m}\) is given features of \(m^{}\) website and \(}_{m}=[_{m,1},,_{m,K}]\) is the predicted CTRs for \(m^{}\) website for all \(K\) users. _Downstream task_: Determine \(B=2\) websites such that the expected number of users that click on the ad at least once is maximized, i.e., \(z^{}(}_{m})=_{z}_{j=0}^{K}(1-_{i=0}^{M}z_ {i}_{ij})\) where \(z_{i}\{0,1\}\).
3. **Portfolio Optimization**: The task is to choose a distribution over \(M\) stocks in Markowitz portfolio optimization (Markowitz and Todd, 2000; Michaud, 1989) that maximizes the expected return under the risk penalty. _Prediction Model_: Given the historical data \(x_{m}\) about a stock \(m\), predict the future stock price \(_{m}\). Combining prediction over \(M\) stocks to get \(}=[_{1},,_{M}]\). _Downstream Task_: Given the correlation matrix \(Q\) of the stocks, choose a distribution over stocks \(^{}(})=_{}^{} }-^{}Q\,_{i=0}^ {M}z_{i} 1 0 z_{i} 1, i\)

We run our own experiments for LODL (Shah et al., 2022) using their public code.

#### 4.2.2 Experimental results

Table 2 presents a summary of the performance of different methods on all the tasks. Each problem poses unique challenges for the methods. The _cubic_ setting suffers from severe approximation errors, hence the learning method needs to allocate limited model capacity more towards higher utility points compared to lower utility points. The MSE method performs the worst as it lacks task information and only care about prediction error. DFL with \(=0\) performs better than MSE, but it can get trapped in local optima, leading to higher variance in the problem (Shah et al., 2022). LODL (\(=0\)) performs among the highest in this problem since it uses learned loss for each point. TaskMet performs as good as LODL as it can capture the relative importance of higher utility points versus lower utility points using the learned metric, resulting in more accurate predictions for those points (see fig. 4). In _budget allocation_, DFL (with \(=0\)) performs the best, since it is solely optimizer over \(_{}\), but on the other hand it has \(10\) orders of larger prediction error as shown in table 3 indicating that the model is overfit to the task, LODL (\(=0\)) suffers from the same problem. TaskMet has the \(2^{}\) best Decision Quality without overfitting on the task, i.e., low prediction error. In _Portfolio Optimization_, the decision quality correlates highly with the model accuracy/prediction error as in this setting the optimization problem mostly depends upon the accurate prediction of the stocks. This is the reason that MSE, DFL (\(=10\)) performs the best, but DFL (\(=0\)) performs the worst, since it has solely

    &  &  \\   & & Cubic & Budget & Portfolio \\  MSE & \(-0.96 0.02\) & \(0.54 0.17\) & \(0.33 0.03\) \\ DFL & \(0\) & \(0.61 0.74\) & \(0.91 0.06\) & \(0.25 0.02\) \\ DFL & \(10\) & \(0.62 0.74\) & \(0.81 0.11\) & \(0.34 0.03\) \\ LODL & \(0\) & \(0.96 0.005\) & \(0.84 0.105\) & \(0.17 0.05\) \\ LODL & \(10\) & \(-0.95 0.005\) & \(0.58 0.14\) & \(0.30 0.03\) \\  TaskMet & \(0.96 0.005\) & \(0.83 0.12\) & \(0.33 0.03\) \\    \\ 

Table 2: Normalized test decision quality (DQ) on the decision oriented learning problems.

Figure 4: (Cubic problem) TaskMet learns a metric that prioritizes points that are the most important the downstream task. The euclidean metric (MSE) puts equal weight on all points and leads to a bad model with respect to the downstream task.

    &  &  \\   & & Cubic & Budget (\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(\)\(being trained on \(_{}\) without any \(_{}\). As shown in table 2 and table 3, TaskMet is the only method that consistently performs well considering both task loss and prediction loss, across all the problem settings, this is due to the ability of the metric to infer problem-specific features without manual tuning, unlike other methods.

### Model Based Reinforcement Learning

#### 4.3.1 Background and experimental setup

Model-based RL suffers from objective-mismatch (Bansal et al., 2017; Lambert et al., 2020). This is because dynamics models trained for data likelihood maximization do not translate to optimal policy. To reduce objective-mismatch, different losses (Farahmand et al., 2017; Voelcker et al., 2022) have been proposed to learn the model which is better suited to learning optimal policies. TaskMet provides an alternative approach towards reducing objective-mismatch, as the prediction loss is directly learnt using task loss. We set up the MBRL problem as follows. Given the current state \(s_{t}\) and control \(a_{t}\) at a timestep \(t\) of a discrete-time MDP, the _dynamics model_ predicts the next state transition, i.e. \(_{t+1}:=f_{}(s_{t},a_{t})\). The prediction loss is commonly the squared error loss \(_{s_{t},a_{t},s_{t+1}}\|s_{t+1}-f_{}(s_{t},a_{t})\|_{2}^{2}\), and the downstream task is to find the optimal Q-value/policy. Nikishin et al. (2022) introduces idea of _optimal model design_ (OMD) to learn the dynamics model end-to-end with the policy objective via implicit differentiation. Let \(Q_{}(s,a)\) be the action-conditional value function parameterized by \(\). The Q network is trained to minimize the Bellman error induced by the model \(f_{}\):

\[_{Q}(,):=_{s,a}[Q_{w}(s,a)-^{ }Q_{}(s,a)]^{2}, \]

where \(\) is moving average of \(\) and \(^{}\) is the model-induced Bellman operator \(^{}Q_{}(s,a):=r_{}(s,a)+\,_{p_{ }(s,a,s^{})}[_{a^{}} Q(s^{},a^{})]\). Q-network optimality defines \(\) as an implicit function of the model parameters \(\) as \(^{}()=*{arg\,min}_{}_{Q}( ,)_{Q}(,)}{ }=0\). Now we have task loss which is optimized to find optimal Q-values:

\[_{}(^{}()):=_{s,a}[Q_{ ^{}()}(s,a)-Q_{}(s,a)]^{2} \]

where the Bellman operator induced by ground-truth trajectory and reward is \(Q_{}(s,a):=r(s,a)+\,_{s,a,s^{}} _{a^{}} Q_{}(s^{},a^{})\).

Figure 5: OMD (Nikishin et al., 2022) uses the planning task loss to learn the model parameters using implicit gradients. TaskMet add one more optimization step over OMD and instead of learning the model parameters using task loss, we learn the metric which then is used to learn model parameters.

Figure 6: Results on the cartpole with distracting states (Nikishin et al., 2022). Figure 7: Our learned metric successfully distinguishes the real states from the distracting states, i.e. the real states take a higher metric value.

**OMD setup.** OMD end-to-end optimizes the model for the task loss, i.e. \(^{}=_{}_{}(^{}())\).

**TaskMet setup.** For metric learning, we extend OMD to learn a metric using task gradients, to train the model parameters, see fig.5. Metric learning just adds one more level of optimization to OMD and results in the _tri-level_ problem

\[^{}=*{arg\,min}_{}& _{}(^{})\\ &^{}(^{ })=*{arg\,min}_{}_{Q}(,^{})  \]

where \(_{}(^{})\) and \(_{Q}(,^{})\) are defined in eq.109and eq.9, respectively, and \(_{}(,)=_{s_{t},a_{t},s_{t+1}}\| s_{t+1}-f_{}(s_{t},a_{t})\|_{_{}(s_{t})}^{2}\) is the metricized prediction loss in eq.3.

To learn \(^{}\) using gradient descent, we estimate \(_{}_{}\) as

\[_{}_{}= _{}_{}(^{}) }{^{}}}{}\] \[= _{}_{}(^{}) ((,^{})}{^{2} })^{-1}(,^{})}{ }_{^{}(^{})} (_{}(,)}{ ^{2}})^{-1}_{}(,)}{}_{^{}()} \]

#### 4.3.2 Experimental results

We replicated experiments from Nikishin et al. (2022) on the Cartpole environment. The first experiment involved state distractions, where the state of the agent was augmented with noisy and uninformative values. In this setting, we considered an unconditional diagonal metric of dimension \(n\), which is the dimension of the state space, i.e. \(_{}:=()\), where \(^{n}\). As shown in fig.6, the MLE method performed the worst across different numbers of distracting states, as it allocated its capacity to learn distracting states as well. TaskMet outperformed the other methods in all scenarios. The superior performance of TaskMet with distracting states can be attributed to the metric's ability to explicitly distinguish informative states from noise states using the task loss and then train the model using the given metric, as shown in fig.7. The learned metric in fig.7 assigned the highest weight to the third dimension of the state space, which corresponds to the pole angle -- the most indicative dimension for the reward. This shows that the metric can differentiate state dimensions based on their importance to the task.

We also consider a setting with reduced model capacity, where the network is under-parametrized, forcing the model to prioritize how it allocates its capacity. In this scenario, we employ a full conditional metric, denoted as \(_{}=_{}(x)\), which enables the metric to weigh dimensions and state-action pairs differently. We conducted the experiment using a model size of 3 hidden units in the layer. As depicted in fig.8, TaskMet achieves a better return on evaluation compared to MLE and OMD. Additionally, it is evident that TaskMet achieves a lower MSE on the model predictions compared to OMD, indicating that learning with the metric contributes to a better dynamics model.

Figure 8: Results on cartpole with a reduced model capacity from Nikishin et al. (2022).

Conclusion and discussion

In conclusion, this paper addresses the challenge of combining task and prediction losses in task-based model learning. While task-based learning methods offer the advantage of discovering task-relevant features and data samples without explicit inductive biases, the current trend of using task loss alongside prediction loss has potential limitations. These limitations include overfitting of the prediction model to a specific task, rendering it ineffective for other tasks, and the lack of interpretability in the task-relevant features learned by the prediction model.

To overcome these limitations, the paper introduces the concept of task-driven metric learning, which integrates the task loss into a parameterized prediction loss. This approach enables end-to-end learning of metrics to train prediction models, allowing the models to focus on task-relevant features and dimensions in the prediction space. Moreover, the resulting prediction models become more interpretable, as metric learning serves as a preconditioning step for gradient-based model training. The effectiveness of the method is shown using different scales of experimental setting - decision oriented tasks as well as downstream learning tasks.

One of the limitations of the method is stability of learning the metric. Bad gradients can lead collapsed metric which can lead to unrecoverable bad predictions. Hence, hyper-parameter tuning of learning rate for metric learning and parameterization choices of the metric are crucial. Possible extensions to this work includes end-to-end metric learning with multiple task losses, learning metric for training dynamics models to be used for long-horizon planning tasks, etc.

#### Acknowledgments

We would like to thank Brian Karrer, Claas Voelcker, Karen Ullrich, Leon Bottou, Maximilian Nickel, and Mike Rabbat insightful comments and discussions.