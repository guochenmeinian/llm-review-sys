# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Introduction

As artificial intelligence (AI) becomes more ubiquitous and sophisticated, humans are increasingly working alongside AI systems to accomplish various tasks, ranging from medical diagnosis [9; 28], content moderation  to writing  and programming . One of the promises of AI is to enhance human performance and efficiency by providing fast and accurate solutions. However, the literature on human-AI collaboration has revealed that humans often underperform expectations when working with AI systems [36; 49; 77; 40]. Moreover, studies have shown that providing explanations for the AI's predictions often does not yield additional performance gains and, in fact, can make things worse [49; 15; 32]. These negative results of human-AI performance may be attributed to two possible reasons. First, humans have miscalibrated expectations about AI's ability, which can lead to over or under-reliance . Second, the cost of verifying the AI's answer with explanations is often higher than that of solving the task without the AI as some explanations don't help in verifying the AI answer .

The central question remains: how can we collaborate better with AI models? In this work, we propose an intuitive framework for thinking about human-AI collaboration where the human first decides on each example whether they should rely on the AI, ignore the AI, or collaborate with the AI to solve the task. We refer to these three actions as the _AI-integration decisions_. The human-AI team will perform optimally if the human knows which action is best on a task-by-task basis. We propose IntegrAI (Figure 1), an algorithm that leverages data from baseline human interactions with the AI to learn near-optimal integration decisions, in the form of _natural language rules_ that are easily understandable. These rules are then taught to the human through an onboarding stage, analogous to an onboarding class that humans might take before operating machines and equipment. Onboarding additionally calibrates the human's expectations about AI performance. We further investigate surfacing the AI-integration decisions found by IntegrAI as recommendations to the human within an AI dashboard used after onboarding. The hope is that onboarding and the dashboard help the human know which action they should take, thereby leading to effective AI adoption for enhanced decision-making.

Learning AI-integration rules requires a dataset of paired examples and human predictions (Figure 1 Step 1). Each rule is defined as a bounded local region centered around a learned point in a potentially multi-modal embedding space spanning the task space and natural language (Figure 1 Step 2). For example, CLIP embeddings  connect image and text spaces for tasks involving images, and typical text embeddings  are used for natural language tasks such as question answering. The regions are obtained with a novel region discovery algorithm. Then, a text description of the region is generated, resulting in a rule that indicates whether the human should ignore, rely on, or collaborate with the AI. We obtain descriptions using a novel procedure that connects the summarization ability of a large language model (LLM)  with the retrieval ability of embedding search to find similar and dissimilar examples. The procedure first queries the LLM to describe points inside the region (Figure 1 Step 3). The embedding space is then leveraged to find counterexamples inside and outside the region to refine the description.

We first evaluate the ability of our region finding and region description algorithms to find regions that will aid the human-AI team in several real-world datasets with image and text modalities. We then investigate the efficacy of both algorithms in synthetic scenarios where we know the ground truth regions. Finally, we conduct user studies on tasks with real-world AI models to evaluate our onboarding and AI-integration recommendation methodology. Our main task is detecting traffic lights in noisy images  from the perspective of a road car, motivated by applications to self-driving cars. The user study reveals that our methods significantly improve the accuracy of the human-AI team by 5.2% compared to without onboarding. We investigate a second task of multiple choice question answering using the MMLU dataset  and find that onboarding has no effect on performance and that only displaying AI-integration recommendations has a negative effect. To summarize, the key contributions of this paper are as follows:

* We propose a novel region-finding and region-description algorithm, IntegrAI, that outperforms prior work to derive rules that can guide humans in collaborating with AI models for shared decision-making.
* We demonstrate the effect of onboarding and displaying AI-integration recommendations on two real-world tasks, and find that onboarding has a significant positive effect on one task and that integration recommendations are not useful.

Related Work

A growing literature of empirical studies on AI-assisted decision making has revealed that human-AI teams do not perform better than the maximum performance of the human or AI alone even with AI explanations [7; 71; 49]. Recent work has explored modifying AI explanations to depend on human behavior [77; 51] and showed positive results with synthetic AI models. A growing literature exists on on oboarding humans to work on AI models [67; 17; 45; 57; 16; 41], however, our work enables the automated creation of onboarding without any human in the loop. We compare to a representative from work on discovering regions of errors in AI models [24; 10; 66; 82; 87; 80; 23; 63; 37], our work instead focuses on regions of disparate performance between human and AI. Learning to defer methods learn models that decide using a secondary AI model who between the human and the AI classifier should predict [52; 56], this paper focuses on the reverse setting where the human makes all decisions but utilizes some of the thinking from that literature. Our AI-integration recommendations are also related to personalized policies . Our MMLU experiments share similarities with recent work [13; 86; 74; 38]. Further comparison to prior work can be found in Appendix A.

## 3 AI Assisted Decision Making

**Setting.** We consider a setting where a human is making decisions with the help of an AI agent who provides advice to complete a **task**. Formally, the **human** has to make a decision \(Y\) given access to information about the context as \(Z\) and the AI's advice \(A\). We denote the human as a potentially randomized function \(H(Z,A;_{h})\) with parameters \(_{h}\) which are unobservable. On the other hand, the **AI** agent provides advice based on its viewpoint of the context as \(X\), which might be different from the humans viewpoint \(Z\) as the human may have side information. The AI model provides advice according to \(M(X;_{m}):=A\), the advice always includes a candidate decision \(\) and possibly an explanation of the decision. We assume that the observed tasks are drawn from an underlying distribution, \(_{X,Z,Y}\), over the contexts of AI and human, and the ground truth. For simplicity of exposition, we will assume that \(X=Z\) when describing our methods.

**Task Metrics.** The human wants to make a decision that optimizes various metrics of interest. Given a ground truth decision \(Y\) and a chosen decision \(\), the loss is given by \(l(Y,):^{+}\). We denote the loss \(L(H,M)\) of the Human-AI team over the entire domain as:

\[L(H,M):=_{x,z,y}[l((y,H(z,M(x;_{ m});_{h}))]\] (1)

In our example, this could be the 0-1 loss \(_{Y=}\). We are further interested in metrics that convey efforts undertaken by the human during the process. Particularly, we focus on _time to make a decision_, which can be measured when the human makes decisions.

**Human-AI team.** The decision of the Human-AI team is represented by the function \(H(Z,A;_{h})\). The human without the AI is denoted by \(H(Z,;_{h}):=H(Z;_{h})\), which is obtained by setting the AI advice to the empty set, i.e. no advice. In theory, we expect the human with advice to perform at least as well as without advice, simply because the human can always ignore the advice. However, the literature on Human-AI teams has clearly demonstrated that this is often not the case . An **effective** Human-AI team is one where the human with the AI's advice achieves a more favorable trade-off in terms of the metrics than without the advice.

**Framework for Cooperation.** Our insight into forming an effective team is to explicitly recommend when the human should consider the advice and how to incorporate it into their decision. We propose a two-stage framework for the human to cooperate with the AI: the human first decides whether to ignore the AI advice, use the AI's decision or integrate the AI advice to make a decision with explicit cooperation. Each of these three cases provides a clear path to the second stage of making the final output decision.

**Definition 1**.: _The AI-integration function \(R(Z,A;_{r})\), also referred to as integrator, formalizes a framework for the human to cooperate with the AI:_

\[R(Z,A;_{r})=0& H(Z;_{h})\\ 1&\\ 2& H(Z,A;_{h})\] (2)

_In this work, we only consider the actions of ignoring or using the AI decision: \(R\{0,1\}\) and leave the action of \(R=2\) for future work._The integration function can be thought of as a specific formalization of the human mental model of the AI [5; 6]. Given an integration function \(R\) and a human \(H\), we can define a hypothetical human decision maker \(H_{R}\) who first computes \(R\) and then follows its recommendation to make a final decision. Similarly, for each human \(H(Z,A;_{h})\), we can associate an integration function \(R_{H}\), such that \(H_{R_{H}}=H\). By fixing \(H(Z;_{h})\), one can try to minimize the loss \(L(H_{R},M)\) over all possible choices of \(R\), an optimal point of such an integration function is denoted \(R^{*}\). Following the recommendations of the optimal AI-integration function leads to an **effective** Human-AI team.

**Learning Rules and Onboarding.** There are two problems that need to be solved to achieve this vision: how do we learn such an \(R^{*}\) and how can we ensure that the human follows the recommendations of this \(R^{*}\)? In the next section, we outline how we approximate the optimal integration function; this is fundamentally a machine-learning problem with its own challenges that we tackle in Section 4. The second obstacle is that the human should know to follow the recommendations of \(R^{*}\). To ensure this, we propose an **onboarding** stage where the human learns about the AI and the optimal integration function and additionally display the recommendations as part of an AI dashboard. In this onboarding stage, we will help the human shape their internal parameters \((_{h},_{r})\) to improve performance. This is a human-computer interaction (HCI) problem that we tackle in Section 5.

## 4 Learning Rules for Human-AI Cooperation: IntegrAI

In this section, we discuss how to learn an _integrator_ function \(:\{0,1\}\) to approximate an optimal integrator while being understandable to the human. We first describe the ingredients for this learning (integrator as a set of regions, objective function, dataset) before detailing how we learn regions and describe them in Sections 4.1 and 4.2 respectively.

**Integrator as a Set of Regions.** Since the integrator \(\) will be used to both onboard the human and provide test-time recommendations as part of an AI dashboard, it should be easily understandable to humans. If the goal was to build the most accurate integrator, we could use work on learning to defer [56; 19; 58]. To address this requirement, we propose to parameterize the integrator in terms of a set of local data regions, each with its own integration decision label as well as a natural language description. More specifically, we aim to learn a variable number of regions \(N_{1},N_{2},\) as functions of \((X,A)\), the observable context and the AI's advice. Each region \(N_{k}\) consists of the following: 1) an indicator function \(I_{N_{k}}:\{0,1\}\) that indicates membership in the region, 2) a textual description of the region \(T_{k}\), and 3) a takeaway for the region consisting of an integration decision in \(r(N_{k})\{0,1\}\). We additionally want these regions to satisfy a set of constraints so that they are informative for the human and suitable for onboarding.

**Maximizing Human's Performance Gain.** Since we are working with human decision-makers, we have to account for the fact that the human implicitly has a **prior** integration function \(R_{0}\), which represents how the human would act without onboarding. Thus, in learning integrator regions, our goal is to maximize the human performance gain relative to their prior. The performance gain is defined as follows for points in a region \(N\):

\[G(N,,R_{0})=_{i N}l(y_{i},H_{R_{0}}(x_{i},a_{i}))-l(y_{i},H_{ }(x_{i},a_{i})),\] (3)

where \(l\) is the loss defined in Section 3. Note that the notion of a human's prior mental model was also discussed by  but we expand on the notion and are able to learn priors as we discuss below.

**Dataset with Human Decisions.** We assume we have access to a dataset \(D=\{x_{i},y_{i},h_{i},r_{0i}\}_{i=1}^{n}\) sampled from \(\), where \(x_{i}\) is the AI-observable context, \(y_{i}\) is the optimal decision on example \(i\), \(h_{i}\) is a human-only decision, defined in Section 3 as \(H(x_{i};_{h})\), and \(r_{0i}\{0,1\}\) is an indicator of whether the human relied on AI on example \(i\). We thus regard the samples \(\{r_{0i}\}_{i=1}^{n}\) as a proxy for prior human integration function \(R_{0}\). The prior integration decisions of the human \(r_{0i}\) are collected through a data collection study where the human predicts with the AI without onboarding. When the human presses on the "Use AI" button (see Figure 3) we record \(r_{0i}=1\), and 0 otherwise. The human predictions \(h_{i}\) are collected through a secondary data collection study where the human makes predictions without the AI. We also assume that we are given an AI model \(M\), from which we obtain AI decisions \(_{i} a_{i}\) from the AI advice \(a_{i}=M(x_{i};_{m})\)1. Given the dataset \(D\), AI decisions \(_{i}\), and loss \(l(.,.)\), we can define optimal per-example integration decisions \(r_{i}^{*}\) by comparing human and AI losses on the example: \(r_{i}^{*}=(l(y_{i},h_{i})>l(y_{i},_{i}))\).

### Region Discovery Algorithm

**Representation Space.** In this subsection, we describe a sequential algorithm that starts with the prior integration function \(R_{0}\) and adds regions \(N_{k}\) one at a time. The domain \(\) for the task may consist of images, natural language, or other modalities that possess an interpretable representation space. We follow a similar procedure for all domains. The first step is to map the domain onto a potentially cross-modal embedding space using a mapper \(E(.)\), where one of the modes is natural language. The motivation is that such an embedding space will have local regions that share similar natural language descriptions, enabling us to learn understandable rules. For example, for natural images, we use embeddings from the CLIP model . The result of this step is to transform the dataset \(\{x_{i}\}_{i=1}^{n}\) into a dataset of embeddings \(\{e_{i}\}_{i=1}^{n}\) where \(e_{i}_{}^{d}\).

**Region Parameterization.** We define region \(N_{k}\) in terms of a centroid point \(c_{k}\), a scaled Euclidean neighborhood around it, and an integration label \(r_{k}\{0,1\}\). The neighborhood is in turn defined by a radius \(_{k}\) and element-wise scaling vector \(w_{k}\). Both \(c_{k}\) and \(w_{k}\) are in \(_{}\), the concatenation of the embedding space and the AI advice space. The indicator of belonging to the region is then \(I_{N_{k}}(e_{i},a_{i})=_{||w_{k}((e_{i},a_{i})-c_{k})||_{2}< _{k}}\), where \(\) is the Hadamard (element-wise) product.

**Region Constraints.** We add the following constraints on each region \(N_{k}\) to make them useful to the human during onboarding. First, the region size in terms of fraction of points contained must be bounded from below by \(_{l}\) and above by \(_{u}\). Second, the examples in each region must have high agreement in terms of their optimal per-example integration decisions \(r_{i}^{*}\). Specifically, at least a fraction \(\) of the points in a region must have the same value of \(r_{i}^{*}\).

IntegrAI-Discover.In round \(k\), we add a \(k\)th region to the integrator \(R_{k-1}\) from the previous round (with \(k-1\) regions) to yield \(R_{k}\). After \(m\) rounds, the updated integrator \(R_{m}\) is defined as follows: Given a point \((x,a)\), if it does not belong to any of the regions \(N_{1},,N_{T}\), then we fall back on the prior. Otherwise, we take a majority vote over all regions to which \((x,a)\) belongs:

\[R_{T}(x,a)=R_{0}(x,a)&I_{N_{k}}(x,a)=0,\ k=1,,T\\ (\{r(N_{k}):kI_{N_{k}}(x,a)=1\} )&\] (4)

In round \(k\), we compute the potential performance gain for each point if we were to take decision \(r\) on the point as \(g_{i,r}=l(y_{i},H_{R_{k-1}}(z_{i},a_{i}))-l(y_{i},r)\). The optimization problem to find the optimal regions is non-differentiable due to the discontinuous nature of the region indicators. To make this optimization problem differentiable, we relax the constraints as penalties with a multiplier \(\) and replace the indicators with sigmoids scaled by a large constant \(C_{1}\):

\[&_{c,,w,r}\ _{i=1}^{n}(C_{1}(-||w (e_{i},a_{i})-c)||+)) g_{i,r}-(_{i=1}^{n} (C_{1}(-||w(e_{i},a_{i})-c)||+))\\ &(_{r_{i}^{*}=r}- n),0)- (_{i=1}^{n}(C_{1}(-||w((e_{i},a_{i})-c)||+))- _{u}n,0)\]

The lower bound constraint with \(_{l}\) can be added in a similar fashion. We find the value of \(r\) with an exhaustive search and use a gradient based procedure for the other optimization variables. The full optimization details are in Appendix B.

### Region Description Algorithm

We now describe our region description algorithm aimed at making the rules for integration human understandable. Natural language descriptions are a good match for this objective. Specifically, we would like to find a contrastive textual description \(T_{k}\) of each region \(N_{k}\) that describes it in a way to distinguish it from the rest of the data space.

**Textual Descriptions for Regions:** The first step is to have a textual description \(t_{i}\) for each example in our dataset \(D\) based on \(x_{i}\). If textual descriptions are not available, we can obtain them by utilizingmodels that map from the domain \(\) to natural language such as captioning models for images, summarization models for text, or exploiting metadata to construct a natural language description. One idea to obtain a region description is to ask an LLM (such as GPT-3.5) to summarize all textual descriptions of points inside the region. However, there are two issues with this approach: first, the region may contain thousands of examples so we need an effective way to select which points to include, and second, the obtained region description may not contrast with points outside the region. To resolve these issues, we propose an algorithm that iteratively refines region descriptions with repeated calls to an LLM (\(\)) in 1 (IntegrAI-Describe). The algorithm starts with an initial description and then finds counterexamples to that description at each round: examples outside the region with high cosine similarity with respect to embeddings (sim) of the region description and examples inside the region with low similarity to the region description. Then we add those counterexamples to our example sets and derive a new region description, we use an especially created prompt with an exemplar to the LLM to get the region description at each round, the prompt can be found in Appendix C.

**Illustrative Example.** Suppose we want to describe a region consisting of images of highways during the night, with no cars present (see Figure 1 for images of BDD ). Our method's initial description is "The highway during the night with clear, rainy or snowy weather", not mentioning that the highway has no cars, particularly because the captions of examples \(t_{i}\) only mention the presence of cars and not their absence. In the second round, the algorithm finds the counterexample \(s^{-}\) as 'city street during the night with clear weather with a lot cars' and counterexample \(s^{+}\) "highway during the night with clear weather". The new description \(T_{k}^{1}\) becomes "clear highway during the night with various weather conditions, while outside the region are busy city street at night with clear weather". After one more round, the description \(T_{k}^{2}\) becomes "driving on a clear and uncongested highway during the night in various weather conditions". We now proceed in the next section to describe how we onboard the human decision maker using the regions.

## 5 Onboarding and Recommendations to Promote Rules

Once rules for integration have been learned as described in the previous section, the task is to teach these rules to the human with an **onboarding** stage and encourage their use at test time. We accomplish this through an **onboarding** process followed by test-time **recommendations**, as described next.

**Human-AI Card.** The onboarding process accordingly consists of an _introductory_ phase and a _teaching_ phase. In the _introductory phase_, the user is first asked to complete a few practice examples of the task on their own to gain familiarity. The user is then presented with general information about the AI model in the form of a **human-AI card**, akin to a model card , that includes AI model inputs and outputs, training data, training objectives, overall human and AI performance along with AI performance on subgroups that have performance that deviate from average performance (details in Appendix D).

The **teaching phase** is structured as a sequence of "lessons", each corresponding to a region \(N_{k}\) resulting from the algorithm of Section 4.1. The specific steps in each lesson are as follows:

**Step 1: Human predicts on example.** A representative from the region is selected at random that has an optimal integration decision identical to that of the region. The human is asked to perform the task for the chosen representative and is shown the AI's output along with the option to use the AI's response.

**Step 2: Human receives feedback.** After submitting a response, the user is told whether the response is correct and whether the AI is correct.

**Step 3: From Example to Region Learning.** The user is informed that the representative belongs to a larger region \(N_{k}\) and is provided with the associated recommendation \(r_{k}\), textual description \(t_{k}\), and AI and human performance in the region as well as the raw examples from the region in a gallery viewer.

After completing all lessons as above, a second pass is done where the user is re-shown all lessons for which their response was incorrect. This serves to reinforce these lessons and is similar to online learning applications such as Duolingo for language learning . Our teaching approach is motivated by literature showing that humans learn through examples and employ a nearest neighbor type of mechanism to make decisions on future examples [12; 68; 29]. We follow this literature by showing concrete data examples in the belief that it effectively teaches humans how to interact with AI. We improve on the approach in  by incorporating the introductory phase, showing pre-defined region descriptions more clearly established, and iterating on misclassified examples.

**Recommendations in AI Dashboard.** At test time, we can check whether an example \(x\) and its corresponding AI output \(a\) fall into one of the learned regions \(N_{k}\). If they do, then our dashboard shows the associated recommendation \(r_{k}\) and description \(t_{k}\) alongside the AI output \(a\).

## 6 Method Evaluation

**Objective.** In this experimental section2, we evaluate the ability of our algorithms to achieve three aims: (Aim 1) is learning an integration function that leads to a human-AI team with low error; (Aim 2) is discovering regions of the data space that correspond to the underlying regions where human vs AI performance is different; and, (Aim 3) is evaluating the ability of our region description algorithm to come up with accurate descriptions of the underlying regions. Full experimental details are in Appendix E.

**Datasets and AI Models.** The experiments are performed on two image object detection datasets and two text datasets. The image datasets include Berkeley Deep Drive (BDD)  where the task is to detect the presence of traffic lights from blurred images of the validation dataset (10k), and the validation set of MS-COCO (5k) where the task whether a person is present in the image 3. The text-based validation datasets comprise of Massive Multi-task Language Understanding (MMLU) , and Dynamic Sentiment Analysis Dataset (DynaSent) . The pre-trained Faster R-CNN models  are considered for the BDD and MS-COCO. For MMLU, a pre-trained fan-t5 model  is utilized, whereas a pre-trained sentiment analysis roBERTa-base model is used for DynaSent . Each dataset is split into 70-30 ratio for training and testing five different times so as to obtain error bars of predictions. We obtain embeddings for the text datasets using a sentence transformer  and CLIP for the image datasets 

**Baselines.** We benchmark our algorithm with different baseline methods that can find regions of the space. The baselines include: (a) DOMINO  which is a slice-discovery method for AI errors, (b) K-Means following the approach of , and (c) the double-greedy algorithm from

Figure 2: Test Error (\(\)) of the human-AI system when following the decisions of the different integrators baselines as we vary the number of regions maximally allowed for each integrator on the BDD dataset.

 that finds regions for Human-AI onboarding. For the regions obtained from these baselines, we compute the optimal integration decision that results in minimal loss. For our method, we set \(_{u}=0.5,_{l}=0.01,=0.0\) for Aim 1 and \(_{u}=0.1,_{l}=0.01,=0.5\) for Aim 2 and random prior decisions (50-50 for 0 and 1). In the context of the region-description algorithms, we compare to the SEAL approach , a simple baseline that picks the best representative description from the existing dataset (best-caption) and ablations of our method.

**Learning Accurate Integrators ( Aim 1).** The goal is to measure the ability of our method in learning integration functions that lead to low Human-AI team error (the loss \(L(H,M)\)). This can be well represented by measuring the errors on the training set (discovering regions of error) and the test set (generalization ability). In Table 1, we show the results of our method and the baselines at learning integrators and find that our method can find regions that are more informative with respect to Human vs AI performance on the test data and significantly better on the training data. Figure 2 shows that on BDD our method can find an integrator that leads to lower loss at test times than the baselines with a minimal number of regions.

**Recovering Ground truth Regions (Aim 2).** We just established that the regions discovered by our algorithm result in a Human-AI team with lower error than human or AI alone. However, it still needs to be verified if the regions denote any meaningful and consistent regions of space. We utilize a synthetic setup by simulating the AI model and the human responses such that there exist (randomized) regions in the data space where either the human or the AI are accurate/inaccurate. These regions are defined in terms of metadata. As an example on the BDD dataset, we can define the AI to be good at daytime images and bad at images of highways, whereas the human to be good at nighttime images and bad at images of city streets. We employ our algorithm and the baselines to discover the regions and compare them with the ground truth region corresponding to the partition of the data, which is essentially a clustering task with ground truth clusters. Results are shown in Table 2 indicating that the ground truth regions can be recovered to an extent.

**Describing Regions (Aim 3).** We conduct an ablation study where we evaluate the power of contrasting and self-correcting ability of Algorithm 1 against baselines. On the MS-COCO dataset, we take regions defined in terms of the presence of a singular object (e.g., 'apple') and try to uncover a single-word description of the region from the image captions. We use standard captioning metrics that compare descriptions from the algorithms to the object name, we include a metric called "sent-sim" that simply measures cosine similarity with respect to a sentence transformer . We compare to ablations of Algorithm 1 with \(m\{0,5,10\}\) (rounds of iteration) and without having examples outside the region (IntegerAI, \(S^{-}=\)). Results are in Table 3 and show that including examples outside the region improves all metrics while increasing iterations (\(m\)) only slightly improves results.

    & BDD & MMLU & DynaSent & MS-COCO \\  IntegrAI (ours) & **17.8 \(\) 0.2** & **45.3 \(\) 0.3** & 20.2 \(\) 0.3 & **22.6 \(\) 0.4** \\ DGMIDN  & 18.9 \(\) 0.4 & 48.1 \(\) 0.2 & 20.0 \(\) 0.2 & 22.7 \(\) 0.4 \\ K-MEANS  & 19.0 \(\) 0.5 & **45.3 \(\) 0.3** & 20.0 \(\) 0.2 & 23.2 \(\) 0.1 \\ DoubleGreedy  & 18.9 \(\) 0.1 & 46.1 \(\) 0.6 & 20.0 \(\) 0.2 & 23.8 \(\) 0.4 \\   

Table 1: Error (\(\)) on the test set (in %) (region discovery) of the human-AI system when following the integrator resulting from the different methods at region size 10 on the different non-synthetic datasets.

    & best-caption & SEAL & IntegrAI (m=5) & IntegrAI (m=10) & IntegrAI (\(S^{-}=\)) & IntegrAI (m=0) \\  METEOR & \(12.9 1.9\) & \(9.16 1.89\) & \(\) & \(25.4 3.3\) & \(24.3 3.3\) & \(25.4 3.2\) \\ sent-sim & \(39.8 1.9\) & \(44.1 2.5\) & \(66.0 3.2\) & \(\) & \(65.1 3.2\) & \(67.0 3.1\) \\ ROUGE & \(5.81 1.2\) & \(0.0 0.0\) & \(27.9 5.1\) & \(\) & \(25.6 4.9\) & \(32.6 5.4\) \\ SPICE & \(12.7 1.9\) & \(7.53 2.3\) & \(\) & \(\) & \(41.1 5.8\) & \(43.8 5.8\) \\   

Table 3: Evaluation of our region description algorithm (Algorithm 1) on a synthetic evaluation of MS-COCO where the different algorithms try to describe a set of points that contain the presence of an object. For example, a region is defined if it contains the object “apple”. Then the different algorithms try to describe the region and we compare it to the description “apple”.

For the apple example, the ablated method finds the description to be "fruit" whereas our method finds it as "apple", thereby eliminating any possibility of confounding effects. More details are provided in the appendix.

## 7 User Studies to Evaluate Onboarding Effect

**Tasks.** We perform user studies on two tasks: 1) predicting the presence of a traffic light in road images from the BDD dataset  and 2) answering multiple-choice questions from the MMLU  dataset. For BDD, we blur the images with Gaussian blur to make them difficult for humans and use the faster r-cnn model for the AI. Participants can see the AI's prediction, bounding box on the image as an explanation and the model's confidence score. For MMLU, participants are shown a question, four possible answers and have to pick the best answer and the prediction of GPT-3.5-turbo . We also obtain an explanation by using the prompt "Please explain your answer in one sentence", both the AI answer and the explanation are shown. GPT-3.5 obtains an accuracy of 69% during our evaluation and we restrict our attention to specific subjects within the MMLU dataset. Specifically, we sample 5 subjects (out of the 57 in MMLU) where ChatGPT has significantly better performance than average, 5 where it's significantly worse, and 4 subjects where performance is similar to average performance. We sample 150 questions from each subject and additionally sample 150 questions from OpenBookQA dataset  to use as attention checks. We show the prediction interfaces in Figure 3. Details are in Appendix F.

**Participants.** We submitted an IRB application and the IRB declared it exempt as is, all participants agreed to a consent form for sharing study data. We recruited participants from the crowdsourcing website Profile  from an international pool filtering for those fluent in English, have above 98% approval rating, have more than 60 previous submissions, and have not completed any of our studies before. For BDD, participants are compensated 53 per 20 examples in the study and then some receive a bonus of $2 for good performance. For MMLU, we pay participants $3 for every 15 questions. We collected information about participants' age, gender (52% identify as Female), knowledge of AI, and other task-specific questions. Participants have to correctly answer three initial images without blur (questions from OpenBookQA for MMLU) and encounter attention checks throughout their studies to further filter them, we exclude participants who fail all attention checks.

**Experimental Conditions.** For BDD, we initially collect responses from 25 participants that predict without the AI and then predict with the help of AI (but no onboarding), we use this data to collect the dataset \(D\) of prior human integration decisions and human predictions to find 10 regions using IntegrAI. We then run four different experimental conditions with 50 unique participants in each where they predict on 20 examples: (1) human predicts alone (H) and human predicts with the help of AI (H-AI) but no onboarding and random order between with and without AI, (2) human

Figure 3: (a) Interface for humans to predict a traffic light on images from BDD dataset in the presence of AI’s prediction, confidence score, and bounding box and (b) interface for humans to answer multiple choice questions from MMLU dataset with AI’s prediction and explanation.

receives onboarding using our method and then in a random order also receives recommendations (Onboard(ours)+Rec) or no recommendations (Onboard(ours)), (3) human goes through only a modified onboarding procedure that only uses step 1 and step 2 from section 5 and uses regions from DOMINO  (Onboard(baseline)) and finally (4) human does not receive onboarding but receives the AI-integration recommendations (Rec). For MMLU, participants are tested on 15 examples per condition and onboarding goes through 7 regions found by our algorithm. We run IntegrAI on both the dataset embeddings, metadata (subject name), and an embedding of ChatGPT explanations separately. We find 10 regions based on the metadata and 2 regions based on the ChatGPT explanations. Due to budget constraints, we only run conditions 1-2-4. Note that all participants receive the introduction phase of the onboarding (AI model information) regardless if they go through onboarding or not.

**Results.** In Table 4 and Table 5 we display various results from the user studies for BDD and MMLU respectively across all experimental conditions. We note the average accuracy (\(\) standard error) across all participants for their final prediction, how often they pressed the "Use AI Answer" button that we call AI-reliance, the time it took for them to make a prediction on average per example (we filter any time period of more than 2 minutes). Finally, we compute using a two-sample independent t-test (* a paired t-test only between Human and Human-AI conditions) the p-value and t-test when comparing each condition (the columns) to the Human-AI condition where the human receives no onboarding. Since we perform multiple tests, we need to correct for multiple hypothesis testing so we rely on the Benjamini/Hochberg method.

**Analysis.** For BDD, we first observe that human and AI performance is very comparable at around 79%, which reduces slightly to 77.2% when the human collaborates with the AI without onboarding. Participants who go through onboarding have a significantly higher task accuracy compared to those who didn't go through onboarding (corrected p-value of 0.042) with a 5.4% increase. The onboarding baseline fails to significantly increase task accuracy, showcasing that the increase is not due to just task familiarity but possibly due to insights gained from regions showcased. Displaying recommendations in addition to onboarding (Onboard(ours)+Rec) does not improve performance but adds time to the decision-making process (7.6s compared to 5.9s without). For MMLU, we note that there is a 20% gap between human and AI performance, but human+AI with and without onboarding can obtain an accuracy of around 75% which is slightly higher than AI alone; onboarding had no additional effect. Interestingly, we find a weakly significant negative effect of only showing AI-integration recommendations which decrease accuracy by 5% and adds 6 seconds of time per example.

**Discussion.** We believe that for MMLU, due to the wide gap between human and AI accuracy and convincing ChatGPT explanations, onboarding did not improve performance. Moreover, it is clear that in their current form, displaying the AI-integration recommendation is not an effective strategy and that onboarding on its own is sufficient. Finally, note that even the Human-AI baseline benefits from the human-AI card which might explain that the team is at least as good as it's components.

**Limitations.** Onboarding and recommendations can significantly affect human decision making. If the recommendations are inaccurate, they could lead to drops in performance and thus require safeguarding. Onboarding and recommendations can be tailored to the specific human by leveraging their characteristics to few-shot learn their prior integrator and prediction abilities.

  Metric & AI only & Human & Human+AI & Onboard(ours)+Rec & Onboard(ours) & Onboard(baseline) & Rec \\  Accuracy (\%) & \(79.0 0.7\) & \(78.5 1.7\) & \(77.2 1.4\) & \(79.9 1.4\) & \(82.6 1.3\) & \(80.4 1.4\) & \(84.1 1.8\) \\ Test w H-AI & \(0.027,1.211\) & \(04.05,0.752\) & N/A & \(0.268,1.352\) & \(0.012,7.47\) & \(0.261,1.525\) & \(0.206,1.839\) \\ AI reliance (\%) & N/A & N/A & \(16.5 3.1\) & \(66.5 2.4\) & \(25.5 3.4\) & \(24.4 4.4\) & \(21.4 3.2\) \\ Time/example (s) & N/A & \(5.408 0.289\) & \(7.78 0.517\) & \(7.622 0.371\) & \(5.936 0.288\) & \(6.841 0.543\) & \(8.717 0.516\) \\  

Table 4: Results from our user studies. For accuracy, time per example, and AI-reliance we report mean and standard error across participants. The “Test vs H-AI” row reports the adjusted p-value and t-test for a two-sample t-test between the human+AI condition and the other conditions (columns).

  Metric & AI only & Human & Human+AI & Onboard(ours)+Rec & Onboard(ours) & Rec \\  Accuracy (\%) & \(72.9 0.6\) & \(52.8 2.2\) & \(75.0 1.7\) & \(73.7 1.8\) & \(74.4 1.7\) & \(69.8 1.8\) \\ Test vs H-AI & \(0.230,-1.488\) & \(0.0,-7.899\) & N/A & \(0.747,-0.53\) & \(0.792,-0.265\) & \(0.101,-2.08\) \\ AI reliance (\%) & N/A & N/A & \(40.0 3.6\) & \(34.5 3.7\) & \(40.6 3.8\) & \(34.2 2.9\) \\ Time/example (s) & N/A & \(30.608 2.109\) & \(23.623 1.66\) & \(22.917 1.362\) & \(20.977 1.509\) & \(29.535 1.883\) \\  

Table 5: Results from our user studies for MMLU.