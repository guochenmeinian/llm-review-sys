# Instruction Tuning With Loss Over Instructions

Zhengyan Shi\({}^{1}\) Adam X. Yang\({}^{2}\) Bin Wu\({}^{1}\)

Laurence Aitchison\({}^{2}\) Emine Yilmaz\({}^{1}\) Aldo Lipani\({}^{1}\)

\({}^{1}\)University College London \({}^{2}\)University of Bristol

{zhengxiang.shi.19,bin.wu.23,emine.yilmaz,aldo.lipani}@ucl.ac.uk

{adam.yang,laurence.aitchison}@bristol.ac.uk

###### Abstract

Instruction tuning plays a crucial role in shaping the outputs of language models (LMs) to desired styles. In this work, we propose a simple yet effective method, Instruction Modelling (IM), which trains LMs by applying a loss function to the instruction and prompt part rather than solely to the output part. Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (_e.g.,_ MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (_e.g.,_ MTBench and AlpacaEval). Remarkably, in the most advantageous case, IM boosts model performance on AlpacaEval 1.0 by over 100%. We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples. We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning. Further analysis substantiates our hypothesis that our improvement can be attributed to reduced overfitting to instruction tuning datasets. It is worth noting that we are not proposing IM as a replacement for the current instruction tuning process. Instead, our work aims to provide practical guidance for instruction tuning LMs, especially in low-resource scenarios. Our code is available at https://github.com/ZhengxiangShi/InstructionModelling.

## 1 Introduction

Figure 1: Performance differences between Instruction Tuning (IT) and our proposed method instruction modelling (IM) trained on 7 instruction tuning datasets. These datasets contain prompts and responses but do not contain preference pairs. Specifically, we use the Less datasets  and Alpagasus datasets , which are subsets of Plan V2 , Dolly , and Stanford Alpaca to ensure good performance. We also report the results on the LIM dataset. (**Left**) The mean performance across 18 traditional NLP tasks (see ยง4.1 for details). (**Right**) The win rate on the AlpacaEval 1.0 benchmark . Please refer to ยง4.2 for details.

Language models (LMs) are trained to predict the next token on massive corpora, enabling them to learn general-purpose representations transferable to various language understanding or generation tasks . However, it does not align LMs to act in accordance with the user's intentions . To enable this transfer, various methods for aligning language models  have thus been proposed, one of which is instruction tuning (IT) . Recent study  proposes Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learnt almost entirely during pretraining, only minimal instruction tuning data is required to enable high-quality outputs in the desired output style. Existing works  mainly perform instruction tuning by focusing the loss computation solely on the output segments.

In this work, we demonstrate that in many scenarios, incorporating the loss computation for instructions or prompts, which we refer to as Instruction Modelling (IM) (see SS3), could substantially improve the performance of instruction tuning on both various NLP tasks (_e.g._, MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (_e.g._, MT-Bench and AlpacaEval), as shown in Figure 1. Remarkably, in the most favourable case, our proposed method IM boosts performance on AlpacaEval 1.0 by over 100%. As illustrated in Figure 2, our study further identifies two key factors influencing the effectiveness of IM: (1) **The ratio between instruction length and output length** (see Figure 2 Left). Our analysis shows that our approach IM is especially beneficial for datasets characterised by lengthy instructions or prompts paired with comparably brief outputs, such as Code Alpaca and Less MMLU Chat; (2) **The number of training examples** (see Figure 2 Right). We demonstrate that our approach IM performs better under the SAH, where a small amount of training examples are available (see SS4.2).

Recent works  suggest that LMs can quickly memorise training examples even after seeing them just once. We hypothesise that the improvement stems from reducing instruction tuning's tendency to overfit, particularly under limited training resource conditions: Instruction tuning on brief outputs or a small amount of data can potentially lead to rapid overfitting. To substantiate our hypothesis, our analysis shows that (1) IM exhibits higher training losses but lower test losses on new instruction tuning data; (2) The outputs generated by IM have a lower similarity to the training examples compared to those from IT, as indicated by BLEU scores; and (3) IM leads to less performance degrade on NLP tasks across training epochs (see SS4.3). Additionally, our study reveals that this overfitting cannot be effectively addressed by applying Kullback-Leibler (KL) divergence for regularisation , as it compromises the model's ability to follow instructions. Our further analysis reveals that the advantages of IM persist across different LMs and model sizes, and that IM could be effectively combined with the previous approach (_i.e.,_ Neftune). Meanwhile, we investigate the relationship between output length and win rate for our approach (see SS4.4).

Figure 2: (**Left**) Performance improvement, achieved by our approach instruction modelling (IM) compared to Instruction Tuning (IT) on the AlpacaEval 1.0, against the ratio between average instruction length and average output length in instruction tuning datasets (training size noted in parentheses). We highlight several representative instruction tuning datasets in yellow. Our analysis suggests that IM is especially beneficial for datasets characterized by lengthy instructions or prompts paired with comparably brief outputs, such as Code Alpaca and Less MMLU Chat. (**Right**) Performance improvement achieved by our approach IM over IT on the AlpacaEval 1.0 against the number of training examples in instruction tuning datasets. Here we maintain a fixed ratio between instruction and output length of 10. This analysis suggests that IM is particularly effective under the low-resource setting or Superficial Alignment Hypothesis. Please refer to ยง4.2 for details.

In summary, the main contributions of this paper are:

* We propose Instruction Modelling (IM), aiming to enhance both the instruction-following and general performance on NLP tasks of LMs. Through extensive experiments across 21 benchmarks, we demonstrate that, in many scenarios, IM substantially improves the performance of LMs trained on various instruction tuning datasets, particularly notable in the AlpacaEval 1.0 benchmark where it boosts scores by over 100%.
* Our study identifies key factors influencing the effectiveness of IM, including the ratio between instruction length and output length and the number of training examples. We are not proposing IM as a replacement for current instruction tuning processes. Rather, we provide empirical guidance for fine-tuning LMs, especially under low-resource scenarios.
* We provide underlying mechanisms that make IM effective, specifically how it mitigates overfitting, thereby enhancing the LMs' performance across various tasks.

## 2 Related Work

Instruction Tuning.LMs can better align with user intents through fine-tuning on datasets consisting of instructions and human-written completions [3; 44]. Early studies mainly focus on NLP tasks, showing that fine-tuning with various NLP datasets trained with instruction output pairs improves cross-task generalisation [1; 33; 43; 44; 51; 54; 65; 66]. Recent works explore the creation of instruction tuning datasets by LMs themselves [63; 25; 69; 36] or through crowdsourcing approaches [13; 78]. Such instruction-tuning phrase [30; 53; 59; 26; 73] enables LLMs to generalise beyond instructions in the training set, largely enhancing their practical utility.

Data Selection for Instruction Tuning.Research on instruction tuning for LMs presents diverging perspectives on the optimal data scale for supervised fine-tuning. A prevailing view recommends fine-tuning on expansive datasets to enhance LM performance across various NLP tasks, thereby improving zero-shot and few-shot learning capabilities [1; 33; 44; 65; 43; 42; 51; 57; 64; 67]. For example, Flan V2 comprises over a million question-answer pairs from diverse NLP sources , and Natural Instructions features 61 distinct tasks and 193k task instances . Conversely, another research trajectory prioritises data quality over quantity [20; 70; 40; 32]. The Superficial Alignment Hypothesis (SAH)  advocates for using smaller, high-quality datasets, arguing that LMs primarily acquire their capabilities during the pretraining phase and thus require only minimal data for effective instruction tuning. For instance, LIMA  employs a carefully curated set of 1k diverse prompts to generate stylistically consistent responses, aimed at creating a helpful AI assistant. AlpaGasus  and LESS  employ methods to select high-quality data based on LLM-generated judgements and gradient signals. However, both views agree on the importance of (1) the quality of pre-trained base LMs and (2) the diversity and quality of the IT data.

Regularisation Through Language Modelling Objectives.Pretraining data and language modelling objectives have been used as a regularisation technique in fine-tuning LMs. In particular, [15; 39] fine-tunes LMs on labelled data, with unsupervised learning on unlabelled data for auxiliary tasks as regularisation.  mixes the alignment objective with the next token prediction objective using pretraining data to mitigate alignment tax in reinforcement learning from human feedback (RLHF).  adopts the masked language objective on the pretraining or downstream task corpus to preserve pre-trained features, and shows improvements in calibration and accuracy.  investigates the effect of incorporating instruction loss weighting on instruction tuning, suggesting that the instruction loss ratio is an important hyperparameter when fine-tuning short-completion data but is irrelevant when using long-completion data. In this work, we propose a broader guideline that does not introduce new hyperparameters but focuses on when and how to include loss over instruction effectively. We refer to our approach as Instruction Modelling because it combines elements of both language modelling and instruction tuning.

## 3 Our Approach

In this section, we first revisit the background of Instruction Tuning (IT) and then introduce our proposed method, instruction modelling (IM).

Instruction Tuning.In instruction tuning, each input is a concatenation of an instruction \(I\) and a completion \(C\). Let \(I\) be the instruction sequence \(\{I_{1},I_{2},,I_{m}\}\) and \(C\) be the completion (output) sequence \(\{C_{1},C_{2},,C_{n}\}\), where \(I\) may include special prompt template tokens (such as "<|user|>" and "<|assistant|>"). The total input sequence \(x\) is \(\{I_{1},I_{2},,I_{m},C_{1},C_{2},,C_{n}\}\). The model predicts each token in \(C\) given all the previous tokens in \(I\) and \(C\) up to that point:

\[P(C_{1},C_{2},,C_{n}|I_{1},I_{2},,I_{m})=_{j=1}^{n}P(C_{j}|I_{1},I_{2},,I_{m},C_{1},C_{2},,C_{j-1})\] (1)

The loss function, \(\), for instruction tuning is the negative log-likelihood of the completions given the instructions, expressed as follows:

\[=- P(C_{1},C_{2},,C_{n}|I_{1},I_{2},,I_{m})=-_{j =1}^{n} P(C_{j}|I_{1},I_{2},,I_{m},C_{1},C_{2},,C_{j-1})\] (2)

This approach aims to optimise the predictions for the completion sequence \(C\), using the instruction sequence \(I\) as contextual information.

Our Approach: Instruction Modelling.Our approach, instruction modelling, is an expansion of instruction tuning by incorporating loss calculation for both the instruction and the completion tokens, except it omits any special prompt template tokens. The model is trained to predict both the instruction and completion parts of \(x\) but excludes tokens that are part of prompt templates (denoted as \(T\)). For simplicity, we consider that these template tokens are not part of \(I\) or \(C\). The model predicts the next token given all previous tokens (both instructions and completions up to that point):

\[P(x)=P(I_{1},I_{2},...,I_{m},C_{1},C_{2},...,C_{n})=_{t=1}^{m+n}P(x_{t}|x _{1},x_{2},...,x_{t-1})\] (3)

The loss function, \(\), for instruction modelling calculates the negative log-likelihood for both instruction and completion tokens, excluding any prompt template tokens. It is computed as follows:

\[=-_{t=1}^{m+n} P(x_{t}|x_{1},x_{2},...,x_{t-1}) (x_{t} T),\] (4)

where \((x_{t} T)\) is an indicator function that is 1 if \(x_{t}\) is not a template token and 0 otherwise. This ensures that the loss is computed only over the meaningful tokens, not over the static template tokens. Our approach allows the model to improve its understanding of both the instructions and the completions while being sensitive to the context provided by both segments of the input sequence.

## 4 Experiments and Results

In this section, we evaluate the effectiveness of our proposed method instruction modelling (IM) by comparing it with Instruction Tuning (IT) and other baselines on various datasets.

### Experimental Setup

Instruction Tuning Datasets.We assess our method, IM, across various instruction tuning datasets, detailed as follows: (1) Stanford Alpaca (\(52\,002\) examples); (2) Dolly (\(15\,011\) examples); (3) Sharegpt (\(50\,000\) examples); (4) Code Alpaca (\(20\,022\) examples); (5) Science Literature (\(7\,544\) examples); (6) WizardLM (\(30\,000\) examples); (7) Tulu V2 (\(326\,181\) examples). Additionally, we incorporate instruction tuning datasets under the low-resource setting or SAH: (8) Llima (\(1\,030\) examples); (9) Less1, where high-quality instruction tuning data are selected from Flan V2 and Dolly. Here, we use the Less MMLU Chat (\(13\,533\) examples), Less BBH ICL (\(13\,533\) examples), and Less Tydiqa (\(13\,533\) examples); (10) Alpagasus2, which offers three subsets: Alpagasus Dolly 3k (\(2\,996\) examples), Alpagasus Dolly 9k (\(9\,229\) examples) selected from Dolly, and Alpagasus Alpaca 5k (\(5\,305\) examples) selected from Stanford Alpaca. See dataset details and statistical analysis in Appendix SSA.

[MISSING_PAGE_FAIL:5]

a detailed breakdown of experimental results for traditional NLP tasks across six categories, as well as performance on additional benchmarks for open-ended generation (_i.e.,_ MT-Bench and AlpacaEval). The experimental results show that our approach (IM) can improve the performance of instruction tuning on various NLP tasks and open-ended generation benchmarks. Specifically, on the Alpagasus Dolly 3k dataset, IM improves the overall mean score of NLP tasks to 48.95, an increase of 2.37 points from the baseline. Similarly, on the Alpagasus Dolly 9k dataset, we observe an improvement of 2.46 points in the mean NLP score. These improvements are mirrored in the LLM-based evaluations. Specifically, IM raises scores on the AlpacaEval 1.0 benchmark, achieving approximately a ten-point increase on the Alpagasus Dolly 9k dataset and doubling the performance on datasets such as Less MMLU Chat and Less Tydiqa. However, the extent of improvement varies across different datasets. For example, the LIMA dataset shows more modest gains, prompting our further analysis to understand the factors influencing the effectiveness of IM.

#2: Our approach IM is especially beneficial for datasets characterised by lengthy instructions or prompts paired with comparably brief outputs.To better understand the impact factors on the effectiveness of IM, we extend our experiments to more instruction-tuning datasets, such as Science Literature, Code Alpaca and Tulu V2. Interestingly, as shown in Figure 2 Left, we find that IM is particularly effective in scenarios where datasets characterised by lengthy instructions and shorter outputs, such as Less MMLU Chat and Less BBH ICL. For example, in datasets like Less MMLU Chat and Less Tydiqa, IM shows remarkable efficacy. In contrast, the Tulu V2 dataset, with an instruction to output length ratio of about 0.5, benefits less compared to the Science Literature dataset, which has a much higher ratio of 24.7. We hypothesise that this trend can be attributed to the tendency of language models trained on datasets with shorter outputs to overfit. In cases where the instructions are longer, IM acts as an effective form of regularisation, mitigating this issue. For further details on the experimental setup, refer to the Appendix in SSC.

#3: Our approach IM performs better with fewer training examples.We find that another important factor in the effectiveness of IM is the quantity of training examples. Specifically, we design additional experiments by sampling different numbers of examples from the Tulu V2 datasets, which contain about 320k training examples and achieve a modest improvement compared to other datasets in Figure 2 Left. We ensure that our samples maintain an instruction-to-output length ratio of around 10. As all these samples are from Tulu V2, we can assume they are from the same distribution. As shown in Figure 2 Right, IM demonstrates substantial performance improvements on the AlpacaEval 1.0 as the number of training examples decreases. This suggests that IM could be particularly valuable for developing robust models in resource-constrained scenarios or under the SAH. For further details on the experimental setup, please refer to the Appendix in SSC.

### Instruction Modelling Mitigates Overfitting of Instruction Tuning

This section explores the underlying interpretation behind the effectiveness of our approach. Our experimental results demonstrate that IM can alleviate the overfitting problem of Instruction Tuning. Below we will discuss our findings in detail.

#1. Train and test loss analysis.Figure 3 clearly illustrates the effectiveness of our approach IM in mitigating overfitting issues compared to IT. For both IM and IT, here we only compute the loss over the output part. In the training loss distribution for the LIMA dataset, IM exhibits a slightly higher mean loss of \(1.45\) compared to \(1.37\) for IT, suggesting that IM does not overfit to the training data as much as IT does. This is further corroborated in the test loss distribution on the Tulu V2 dataset (using a 10% randomly sampled data set), where IM demonstrates a lower mean test loss of \(1.17\) compared to \(1.32\) for IT. This indicates that IM maintains better generalisation to new data, emphasising the model's capability to learn effectively without fitting excessively to training examples. For more examples, see Appendix SSD.

#2. BLEU score analysis.Here we generate outputs using the instructions from the training examples via greedy decoding, and then compare the generated outputs with the ground truth outputs in training examples and report the results. We use the BLEU score (up to n-gram order 4)  to measure the similarity between outputs, where a higher score on outputs indicates a higher overlap with training examples. As shown in Table 2, outputs generated by IM consistently have lower BLEU

scores than those generated by IT. This suggests that IM produces outputs have less overlap with the ground truth outputs in training examples, indicating less overfitting.

### Instruction Tuning Tax on the NLP tasks.

Previous works show that training LMs with RLHF causes an _Alignment Tax_ on the NLP tasks . In this study, we observe that instruction tuning can sometimes lead to diminished model capabilities in some areas, such as multilinguality and commonsense reasoning. To this end, we further explore the impact of instruction tuning on the performance of NLP tasks. Figure 4 illustrates that our approach IM generally has a lower instruction tuning tax compared to IT, suggesting better robustness under low-resource settings. We provide additional experiments for win rates across epochs in Appendix SSAE.

### Can we simply use KL divergence loss as regularisation for instruction tuning?

In this analysis, we demonstrate that the application of KL divergence loss in instruction tuning, which is widely used as regularisation for aligning LMs , cannot easily address the overfitting issue of instruction tuning. Table 3 offers a detailed comparison across various NLP benchmarks and open-ended language generation tasks, particularly using AlpacaEval 2.0, with models trained with and without KL divergence loss. Our findings are as follows: (1) Incorporating KL Loss reduces overfitting and reduces the performance degradation on traditional NLP tasks. For example, on the

    &  & Less & Less & Less & Alpagasus & Alpagasus \\  & & Tydiqa & & & & & \\  & & & & & & \\ 
**IT** & 18.15 & 69.21 & 72.43 & 60.96 & 72.26 & 61.76 & 60.99 \\
**IM** (ours) & 17.30\({}_{10.85}\) & 65.63\({}_{13.58}\) & 69.20\({}_{12.23}\) & 53.94\({}_{17.02}\) & 70.50\({}_{11.76}\) & 60.61\({}_{11.15}\) & 59.04\({}_{11.95}\) \\   

Table 2: Average BLEU Score comparison of IM and IT, where a lower score indicates less overfitting. Green and red arrows indicate performance changes against the baseline (IT).

Figure 4: Mean performance on 18 NLP tasks over epochs using Llama-2-7B-Base. This analysis suggests that IM experiences a lower instruction tuning tax compared to IT.

Figure 3: **(Left) Training loss distribution for each example between our approach instruction modelling (IM) and Instruction Tuning (IT) on the LIMa dataset. (Right) Test loss distribution for each example between IM and IT on the Tulu V2 dataset, using a 10% randomly sampled data for efficacy. Mean losses are marked by dashed lines. For both IM and IT, here we only compute the loss over the output part. IM has a higher train loss with lower test loss, suggesting that IM effectively mitigates the overfitting issues compared to IT. See Appendix SSAD for more examples.**

Dolly dataset, incorporating KL Divergence Loss leads to less instruction tuning tax in NLP tasks, with scores rising from \(45.54\) to \(49.31\). (2) KL Loss detrimentally affects the model's instructions following abilities. For example, on the LIMA dataset, we observe a substantial decrease in AlpacaEval 2.0 from \(2.58\) to \(0.06\). For additional experiments and implementation details, see Appendix SSF.

### Further Analysis

#1. The advantage of our proposed method persists with different language models and sizes.As shown in Figure 5, our analysis demonstrates that our proposed method 1M consistently outperforms the IT across different models and sizes, including OPT-6.7B and Llama-2-13B-Base, on 18 traditional NLP tasks and the AlpacaEval 1.0 benchmark. These findings underline the effectiveness of our approach irrespective of the underlying language model or its scale.

#2. Relationship between the model output length and the win rate.In this analysis, we explore the potential connection between win rates on the AlpacaEval and the increased output length . As shown in Figure 6, our result reveals that our approach 1M does not necessarily generate longer outputs than IT across different data utilisation levels from the Tulu V2 dataset. Specifically, the output lengths for both approaches are similar despite varying levels of data utilisation. Furthermore, IM consistently outperforms the IT, suggesting that improvements in performance as measured by win rates on the AlpacaEval 1.0 are not dependent on the output length. We provide additional analysis on other instruction tuning datasets under the SAH in Appendix SSG.

#3. Our proposed method 1M could further improve the model performance with Neptune.Table 4 demonstrates the combined effects of our proposed method IM and Neptune on performance across various NLP tasks and the AlpacaEval 1.0 benchmark. The integration of Neptune with IM

Figure 5: Comparison of Instruction Tuning (IT) and instruction modelling (IM) methods using OPT-6.7B (**Top Row**) and Llama-2-13B-Base (**Bottom Row**) trained on 7 instruction tuning datasets. (**Left**) The mean performance across 18 NLP tasks. (**Right**) The win rate on the AlpacaEval 1.0 benchmark.

    & &  &  \\   & **Llama-2-7B-Base** & **IT w/o KL Loss** & **IT w/ KL Loss** & **IT w/o KL Loss** & **IT w/ KL Loss** \\ 
**NLP Tasks** & 49.32 & 48.79\(_{0.53}\) & 49.26\(_{0.06}\) & 45.54\(_{3.78}\) & 49.31\(_{0.01}\) \\
**AlpacaEval 2.0** & 0.01 & 2.58\(_{2.57}\) & 0.06\(_{0.05}\) & 2.28\(_{2.27}\) & 0.04\(_{0.03}\) \\   

Table 3: Performance on 18 NLP benchmarks and AlpacaEval 2.0. Green and red arrows indicate performance changes against the baseline (Llama-2-7B-Base). This analysis suggests that while applying KL Loss in the instruction tuning helps mitigate performance degradation in NLP tasks, it substantially harms the model performance in open-ended generation tasks.

generally further improves the win rates in AlpacaEval 1.0, showing notable improvements in several datasets such as a \(13.31\)% increase on Less Tydiqa and a \(12.55\)% boost on Alpagasus Alpaca 5k (in absolute). However, this combination leads to a performance drop in certain contexts, such as a lower performance on NLP tasks on Less MMLU Chat and Less BBH ICL. This indicates that while Neftune may enhance model robustness under certain conditions, its benefits are context-dependent, highlighting the need for the careful application of Neftune when used in conjunction with IM to optimise effectiveness across diverse evaluation settings.

## 5 Epilogue

Conclusion.Our study proposes Instruction Modelling, which trains LMs with loss over instructions rather than outputs only. Our experimental evaluations demonstrate that our approach largely improves the performance of LMs on both NLP tasks and open-ended generation benchmarks in some scenarios, especially under the Superficial Alignment Hypothesis and low-resource setting where minimal training data is used for instruction tuning. Our analysis has shed light on two key factors that influence the effectiveness of our approach, (1) the ratio between instruction and output lengths, and (2) the quantity of training data, providing practical insights for optimising instruction-based training methods. Additionally, our analysis reveals the mechanisms behind the effectiveness of IM, particularly its ability to reduce overfitting, showing that applying instruction losses in some scenarios can lead to more robust and adaptable LMs.

Limitations and Broader Impact.Here we discuss some potential limitations and the broader impact of our work. Several limitations are outlined as follows: (1) The success of our approach relies on the quality and diversity of the instructions and prompts in the training datasets. Poorly defined or ambiguous instructions may undermine the effectiveness of IM, leading to sub-optimal performance; and (2) It is crucial to ensure that the instructions are ethically sound and free from

    &  & Less & Less & Less & Alpagasus & Alpagasus & Alpagasus \\  & & Tydiqa & MMLU Chat & BBH ICL & Alpaca 5k & Dolly 9k & Dolly 3k \\   \\ 
**IM** & 32.94 & 10.10 & 9.78 & 44.15 & 19.52 & 30.77 & 15.11 \\
**IM +Neftune** & 30.77\(\)2.17 & 23.41\(\)13.31 & 12.45\(\)2.67 & 48.25\(\)4.10 & 32.07\(\)12.55 & 38.28\(\)7.51 & 23.35\(\)3.24 \\   \\ 
**IM** & 49.60 & 48.70 & 47.84 & 49.15 & 47.47 & 48.00 & 48.95 \\
**IM +Neftune** & 49.47\(\)0.13 & 49.44\(\)0.34 & 47.73\(\)0.11 & 48.62\(\)0.55 & 48.70\(\)12.3 & 48.63\(\)0.63 & 49.54\(\)0.59 \\   

Table 4: Performance comparison of IM and IM +Neftune on AlpacaEval 1.0 and various NLP benchmarks. Green and red arrows indicate performance changes against the baseline (IM). This analysis shows that adding Neftune to IM could further improve model performance.

Figure 6: (**Left**) Output length comparison between our approach instruction modelling (IM) and Instruction Tuning (IT) across various data utilisation levels from the Tulu V2 dataset, as evaluated on the AlpacaEval dataset. (**Right**) Performance comparison (measured by win rate) between IM and IT on the AlpacaEval 1.0 across various data utilisation levels from the Tulu V2 dataset. This analysis suggests that the improvement provided by IM is not necessarily associated with the increased output lengths. See more length analysis in Appendix ยงG.

harmful or biased content. Training on inappropriate or toxic instructions may result in undesirable outputs. Previous works [5; 7; 4] have extensively discussed the risks and potential harms associated with LMs, including the amplification of undesirable biases learned from training data [5; 2; 9]. Our work has the potential to positively impact the community by helping to mitigate overfitting, resulting in models that are more robust and generalise better to new data, especially in low-resource scenarios. This can enhance the reliability and trustworthiness of AI systems in real-world applications.