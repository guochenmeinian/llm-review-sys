# Neural Cover Selection for Image Steganography

Karl Chahine & Hyeji Kim

Department of Electrical and Computer Engineering

University of Texas at Austin

Austin, TX 78712

{karlchahine, hyeji.kim}@utexas.edu

###### Abstract

In steganography, selecting an optimal cover image--referred to as cover selection--is pivotal for effective message concealment. Traditional methods have typically employed exhaustive searches to identify images that conform to specific perceptual or complexity metrics. However, the relationship between these metrics and the actual message hiding efficacy of an image is unclear, often yielding less-than-ideal steganographic outcomes. Inspired by recent advancements in generative models, we introduce a novel cover selection framework, which involves optimizing within the latent space of pretrained generative models to identify the most suitable cover images, distinguishing itself from traditional exhaustive search methods. Our method shows significant advantages in message recovery and image quality. We also conduct an information-theoretic analysis of the generated cover images, revealing that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's principles in parallel Gaussian channels. Our code can be found at https://github.com/karlchahine/Neural-Cover-Selection-for-Image-Steganography.

## 1 Introduction

Image steganography embeds secret bit strings within typical cover images, making them imperceptible to the naked eye yet retrievable through specific decoding techniques. This method is widely applied in various domains, including digital watermarking (Cox et al. ), copyright certification (Bilal et al. ), e-commerce (Cheddad et al. ), cloud computing (Zhou et al. ), and secure information storage (Srinivasan et al. ).

Traditionally, hiding techniques such as modifying the least significant bits have been effective for embedding small data volumes up to 0.5 bits per pixel (bpp) (Fridrich et al. ). Leveraging advancements in deep learning, recent approaches employ deep encoder-decoder networks to embed and extract up to 6 bpp, demonstrating significant enhancements in capacity (Chen et al. , Baluja , Zhang et al. ). The encoder takes as input a cover image **x** and a secret message **m**, outputting a steganographic image **s** that appears visually similar to the original **x**. The decoder then estimates the message \(}\) from **s**. The setup is illustrated in Fig. 1 (left).

The effectiveness of steganography is significantly influenced by the choice of the cover image **x**, a process known as cover selection. Different images have varying capacities to conceal data without detectable alterations, making cover selection a critical factor in maintaining the reliability of the steganographic process (Baluja , Yaghmaee and Jamzad ).

From a theoretical standpoint, numerous studies have employed information-theoretic analyses to investigate cover selection and determine the capacity limits of information-hiding systems, thereby identifying the maximum number of bits that can be embedded (Moulin et al. , Cox et al. , Moulin and O'Sullivan ). For instance, in Moulin and O'Sullivan , the steganographicsetup is conceptualized as a communication channel where the cover image **x** acts as side information. However, such models are based on impractical assumptions: firstly, the steganographic process is additive--where the message **m** is simply added to the cover **x**; and secondly, it presupposes that the cover elements adhere to a Gaussian distribution.

From a practical standpoint, existing techniques for cover selection predominantly rely on exhaustive searches to identify the most suitable cover image. These methods evaluate a variety of image metrics to determine the best candidate from a database. Some strategies include counting modifiable discrete cosine transform (DCT) coefficients to select images with a higher coefficient count for covers (Kharrazi et al. (2006)), assessing visual quality to determine embedding suitability (Evsutin et al. (2018)), and estimating the embedding capacity based on image complexity metrics (Yaghmaee and Jamzad (2010); Wang and Zhang (2019)).

Traditional methods for selecting cover images have three key limitations: (i) They rely on heuristic image metrics that lack a clear connection to steganographic effectiveness, often leading to suboptimal message hiding. (ii) These methods ignore the influence of the encoder-decoder pair on the cover image choice, focusing solely on image quality metrics. (iii) They are restricted to selecting from a fixed set of images, rather than generating one tailored to the steganographic task, limiting their ability to find the most suitable cover.

Recent progress in generative models, such as Generative Adversarial Networks (GANs) (Goodfellow et al. (2020)) and diffusion models (Song et al. (2020); Ho et al. (2020)), have ignited significant interest in the area of guided image generation (Shen et al. (2020); Avrahami et al. (2022); Brooks et al. (2023); Gafni et al. (2022); Kim et al. (2022)). Inspired by these innovations, we propose a novel approach that addresses the aforementioned limitations by treating cover selection as an optimization problem.

In our proposed framework, a cover image **x** is first inverted into a latent vector, which is then passed through a pretrained generative model to reconstruct the cover image. This image is processed by a neural steganographic encoder to embed a secret message, followed by a decoder to recover the message. We optimize the latent vector to generate an enhanced cover image \(^{*}\), minimizing message recovery errors while preserving the visual and semantic integrity of the image. Fig. 1 (right) presents message recovery errors for randomly selected images before and after optimization. Our approach of optimizing the cover image uncovers a novel way to analyze the transformation from **x** to \(^{*}\), revealing that the encoder embeds messages in low-variance pixels, analogous to the water-filling algorithm in parallel Gaussian channels. To the best of our knowledge, this is the first work that examines neural steganographic encoders by framing cover selection as a guided image reconstruction problem.

Figure 1: **Left: Image steganography framework: the encoder takes as input the cover image **x** and a secret binary message **m** and outputs the steganographic image **s**. The decoder then estimates \(}\) from **s**. **Right:** Randomly sampled cover images from the ImageNet dataset before and after optimization using our framework (described in Section 3). These optimized images demonstrate a significantly reduced error \(||-}||\) while maintaining high image quality.

Our contributions are outlined as follows:

1. _Framework._ We describe the limitations of current cover selection methods and introduce a novel, optimization-driven framework that combines pretrained generative models with steganographic encoder-decoder pairs. Our method guides the image generation process by incorporating a message recovery loss, thereby producing cover images that are optimally tailored for specific secret messages (Section 3).
2. _Experiments._ We validate our methodology through comprehensive experimentation on public datasets such as CelebA-HQ, ImageNet, and AFHQ. Our results demonstrate that the error rates of the optimized images are an **order of magnitude lower** than those of the original images under specific conditions. Impressively, this optimization not only reduces error rates but also enhances the overall image quality, as evidenced by established visual quality metrics. We explore this intriguing phenomenon by examining the correlation between image quality metrics and error rates (Section 3.3).
3. _Interpretation._ We investigate the workings of the neural encoder and find it hides messages within low variance pixels, akin to the water-filling algorithm in parallel Gaussian channels. Interestingly, we observe that our cover selection framework increases these low variance spots, thus improving message concealment (Section 4).
4. _Practical considerations._ We extend our guided image generation process to practical applications, demonstrating its robustness against steganalysis and resilience to JPEG compression, as detailed in Section 5.

**Related work.** Recent research has explored the use of generative models in steganography. Zhang et al. (2019) introduced a training framework where steganographic encoders and decoders are trained adversarially, similar to GANs. Yu et al. (2024) harness the image translation capabilities of diffusion models to transform a secret image directly into a steganographic image, bypassing the embedding process, a framework known as coverless steganography (Qin et al. (2019)). Shi et al. (2018) is notably relevant, as they created a GAN framework designed to produce images robust against steganalysis. However, there are three key distinctions: (i) they overlooked message error rates, focusing solely on evading detection, compromising the effectiveness of cover images for message recovery; (ii) they trained their GAN from scratch, failing to leverage the advantages of existing pretrained models; and (iii) the images generated were randomly sampled and not user-selectable, limiting application flexibility.

## 2 Preliminaries

**Image steganography** aims to hide a secret bit string \(\{0,1\}^{H W B}\) into a cover image \(^{H W 3}\) where the payload \(B\) denotes the number of encoded bits per pixel (bpp) and \(H,W\) denote the image dimensions. As depicted in Fig. 1 (left), the hiding process is done using a steganographic encoder \(Enc\), which takes as input \(\) and \(\) and outputs the steganographic image \(\) which looks visually identical to \(\). A decoder \(Dec\) recovers the message, \(}=Dec()\) with minimal error rate \(-}||_{0}}{H W B}\).

**Cover selection** involves generating the ideal cover image \(\), to achieve three primary objectives: (i) minimize the error rate as defined above, (ii) ensure that the steganographic image \(\) visually resembles \(\) as closely as possible, and (iii) maintain the integrity of the cover image \(\) using established perceptual quality metrics.

**Denoising Diffusion Implicit Models (DDIMs)**(Song et al. (2020)) are a class of generative models that learn the data distribution by adopting a two-phase mechanism. The forward phase incorporates noise into a clean image, while the backward phase incrementally removes the noise. The formulation for the forward diffusion in DDIM is presented as:

\[_{t}=}_{t-1}+},(,),\] (1)

where \(_{t}\) is the noisy image at the \(t\)-th step, \(_{t}\) is a predefined variance schedule, and \(t\) spans the discrete time steps from \(1\) to \(T\). The DDIM's backward sampling equation is:\[_{t-1}=_{t-1}}_{}(_{t},t)+ _{t-1}-_{t}^{2}}_{}(_{t},t)+_{t}^{2},_{}(_{t},t) =_{t}-_{t}}_{}( _{t},t)}{_{t}}},\] (2)

where \((,)\), \(_{t}=_{i=1}^{t}_{i}\), and \(_{}\) is a denoising function reliant on the pretrained noise estimator \(_{}\).

This sampling allows the use of different samplers by changing the variance of the noise \(_{t}\). Especially, by setting this noise to 0, the DDIM backward process becomes deterministic, defined uniquely by the initial variable \(_{T}\). This initial value can be seen as a latent code, commonly utilized in DDIM inversion, a process that utilizes DDIM to convert an image to latent noise and subsequently reconstruct it to its original form (Kim et al. (2022)).

**Generative Adversarial Networks (GANs)**Goodfellow et al. (2020)) are another type of generative model designed to learn the data distribution \(p()\) of a target dataset through a min-max game between two networks: a generator (\(G\)) and a discriminator (\(D\)). The generator creates synthetic samples \(G()\) from a random noise vector \(\), drawn from a simple distribution \(p()\) such as a standard normal. The discriminator evaluates samples it receives--either real data \(\) from \(p()\) or fake data from \(G\)--and tries to accurately classify them as real or fake. The objective of \(G\) is to generate data that \(D\) mistakes as real, while \(D\) aims to distinguish between actual and generated data effectively.

## 3 Methodology

We propose two cover selection methodologies using pretrained Denoising Diffusion Implicit Models (DDIM) and pretrained Generative Adversarial Networks (GAN) (Sections 3.1, 3.2), and compare the performances of the two approaches (Section 3.3). Detailed descriptions of the training procedures are in Appendix B. Broadly speaking, starting with a cover image \(\) randomly selected from the dataset, we gradually optimize this image to minimize the loss \(||-}||\). Intriguingly, while our primary focus is on reducing the error rate, we observe that all three objectives of cover selection outlined in Section 2 are concurrently achieved. We investigate this phenomenon in Section 3.3.

### DDIM-based cover selection

As depicted in Fig. 2, our DDIM approach consists of two steps. We get inspired from DDIM inversion, which refers to the process of using DDIM to achieve the conversion from an image to a latent noise and back to the original image (Kim et al. (2022)).

**Step 1: latent computation.** The initial cover image \(_{0}\) (where the subscript denotes the diffusion step) goes through the forward diffusion process described in Eq. 3 to get the latent \(_{T}\).

Figure 2: DDIM-based cover selection framework overview. The input cover image \(_{0}\) is first converted to the latent space \(_{T}\) via forward diffusion. Then, guided the message recovery loss, the latent space is fine-tuned, and the updated cover image is generated via the reverse diffusion process. The DDIM model as well as the steganographic encoder-decoder pair are pretrained.

\[_{t+1}=_{t+1}}}(_{t},t)+_{t+1}}}(_{t},t)\] (3)

**Step 2: guided image reconstruction**. We optimize \(_{T}\) to minimize the loss \(||-}||\). Specifically, \(_{T}\) goes through the backward diffusion process described in Eq. 2 generating cover images that minimize the loss. We evaluate the gradients of the loss with respect to \(_{T}\) using backpropagation and use standard gradient based optimizers to get the optimal \(_{T}^{*}\) after some optimization steps.

We use a pretrained DDIM (parametrized by \(\)), and a pretrained LISO, the state-of-the-art steganographic encoder and decoder from Chen et al. (2022), also described in Appendix A. The weights of the DDIM and the steganographic encoder-decoder are fixed throughout \(_{T}\)'s optimization process.

The idea is based on the approximation of forward and backward differentials in solving ordinary differential equations (Song et al. (2020)). In the case of deterministic DDIM (\(_{t}=0\)), Eq. 2 can be used to perform the forward and backward process (Kim et al. (2022)) and achieve accurate image reconstruction. Instead of adopting a fully deterministic DDIM, we find that having a deterministic forward process (Eq. 3) with a stochastic backward process (Eq. 2) yields better results for our setup.

### GAN-based cover selection

In the GAN-based approach, we start with a latent vector \(\) randomly initialized from a Gaussian distribution, which serves as input to the generator \(G\). The objective is to identify an optimized \(^{*}\) such that the cover image \(G(^{*})\) minimizes the loss \(||-}||\), i.e.:

\[^{*}=*{argmin}_{}||Dec(Enc(G(), )-||\] (4)

Where \(Enc\), \(Dec\) and \(\) are the steganographic encoder, decoder and secret message respectively as described in Section 2. We evaluate the gradients of the loss with respect to \(\) using backpropagation and use standard gradient based optimizers to get the optimal \(^{*}\) that minimizes the loss. All other modules (\(Enc,Dec,G\)) are differentiable, pretrained and fixed during the optimization. We utilize BigGAN's pretrained conditional generator (Brock et al. (2018)), and a pretrained LISO steganographic encoder-decoder pair (Chen et al. (2022)).

**Note:** To achieve consistency with the DDIM approach described in Section 3.1, instead of starting with a randomly generated latent vector \(\), we can begin a cover image \(\) and apply established GAN inversion techniques to map it to its corresponding latent space (Xia et al. (2022)).

### Performance comparison: DDIM & GAN

We compare the performance of the approaches from Sections 3.1 and 3.2 in Table 1. We show the results of 10 randomly selected classes from the ImageNet dataset (Russakovsky et al. (2015)). Following Chen et al. (2022), we assess error rate defined as \(-}||_{0}}{H W B}\), the structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) (Wang et al. (2004)) to measure changes between cover and steganographic images. We further evaluate the generated cover image quality using the no-reference BRISQUE metric (Mittal et al. (2012)). Our methods outperform traditional exhaustive search techniques detailed in Section 1, which are omitted from the table for brevity.

**Methods:** For the DDIM-based cover selection, we generate a batch of 500 cover images, denoted as \(\{_{0}^{(i)}\}_{i=1}^{500}\), and apply the cover selection framework to each image independently (Section 3.1). Similarly, for the GAN-based cover selection, we produce a batch of 500 randomly initialized latent vectors, represented as \(\{^{(i)}\}_{i=1}^{500}\), and independently run the cover selection framework for each vector (Section 3.2). We train a steganographic encoder-decoder pair using 1000 training images from each class, adhering to the method used in Chen et al. (2022). We then use this trained model, in addition to a diffusion model and BigGAN's conditional generator, both pretrained on ImageNet. We consider a payload of \(B=4\) bpp (we explore different payloads in Section 5.1). The secret messages are random binary bit strings, sampled from an independent \(Bernoulli(0.5)\) distribution. We use the binary cross-entropy loss to optimize message recovery. For a comprehensive explanation on our hyperparameter selection, please refer to Appendix B.

**Observation 1:** As shown in Table 1 the optimized images produced by both DDIM and GAN exhibit significantly lower error rates compared to the original images by over **50\(\%\)** for some classes.

Surprisingly, although our training objective for cover selection focused solely on minimizing the error rate, we observed improved image quality as evidenced by BRISQUE, SSIM, and PSNR scores. This intriguing relationship between higher image quality and lower error rates is further explored in Appendix H. In summary, our analysis reveals that certain image complexity metrics, including edge density and entropy, negatively correlate with both error rates and BRISQUE scores. This suggests that our cover selection framework modifies features such as edges and entropy during optimization, resulting in enhancements to both image quality and error reduction.

**Observation 2:** DDIM-based optimization consistently outperforms GAN-based methods across all metrics, aligning with previous findings on DDIM's superior image generation capabilities (Dhariwal and Nichol (2021)). We further explore and compare the outputs of both methods, presenting sample steganographic images before and after optimization in Appendix G. Notably, DDIM maintains the semantic integrity of images, preserving key elements like object positions and orientations--such as a bird's unchanged gaze. In contrast, GANs may significantly modify an image's composition, even altering a bird's gaze from left to right, which impacts its semantic content.

_For the remainder of the paper, we will utilize the DDIM-based approach, due to its enhanced performance in both error reduction and image quality._

## 4 Analysis

In this section, we explore the reasons behind the enhanced performance achieved by our framework. Initially, we analyze the behavior of the pretrained steganographic encoder (Section 4.1). Our observations indicate that the encoder preferentially embeds messages within pixels of low variance. To validate these findings, we compare the encoder's behavior with the waterfilling technique applied to parallel Gaussian channels (Section 4.2). Lastly, we demonstrate that the cover selection optimization effectively increases the presence of low variance pixels. This adjustment equips the encoder with greater flexibility to hide messages, thereby improving overall performance (Section 4.3). We present the results for the ImageNet Robin class with a payload of \(B=4\) bpp. Additional results for various classes and datasets are presented in Appendix D.

### Encoding in low-variance pixels

We begin by investigating the underlying mechanism of the pretrained steganographic encoder (Chen et al. (2022)). We hypothesize that the encoder preferentially hides messages in regions of low pixel variance. To test this hypothesis, we structure our analysis into two steps.

**Step 1: variance analysis.** In Fig. 3 (top), we illustrate the variance of each pixel position for the three color channels, calculated across a batch of images and normalized to a range between 0 and 1, as detailed in Appendix D. The plot reveals significant disparities in variance, with certain regions displaying notably lower variance compared to others.

**Step 2: residual computation.** Using the same batch of images, we pass them through the steganographic encoder to obtain the corresponding steganographic images. We then compute the residuals by calculating the absolute difference between the cover and steganographic images and averaging

   &  & _{}\)} &  &  \\  Classes & Original & GAN & DDIM & Original & GAN & DDIM & Original & GAN & DDIM & Original & GAN & DDIM \\  Robin & 2.48 & 1.32 & **1.01** & 27.8 & **18.95** & 19.81 & **0.72** & 0.68 & 0.64 & 22.34 & 23.38 & **23.85** \\ Snow Leopard & 0.84 & **0.36** & 0.55 & 23.71 & 18.28 & **17.26** & **0.75** & 0.74 & 0.72 & 23.71 & 24.54 & **24.96** \\ Daiy & 1.75 & **0.97** & 1.43 & 9.95 & 9.79 & **0.71** & 0.61 & 0.59 & **0.61** & 26.01 & **26.7** & 26.63 \\ Drifting Platform & 2.29 & 1.88 & **1.85** & **25.08** & 25.85 & 27.42 & **0.41** & 0.39 & 0.37 & 21.33 & **21.56** & 21.41 \\ Hartubeken & 0.21 & 0.15 & **0.12** & 16.97 & 16.17 & **13.63** & 0.55 & 0.55 & **0.56** & 24.83 & 25.27 & **26.34** \\ American Egret & 0.95 & **0.77** & 0.78 & 24.4 & 22.9 & **12.03** & 0.63 & 0.63 & **0.64** & 22.72 & 22.87 & **24.49** \\ Ourl & 0.21 & **0.02** & 0.09 & 26.01 & 27.77 & **21.3** & 0.73 & **0.76** & 0.71 & 24.02 & 24.62 & **26.01** \\ Chlushua & 0.79 & 0.59 & **0.55** & 18.45 & 17.92 & **14.33** & 0.58 & 0.56 & **0.59** & 23.13 & 23.55 & **24.44** \\ Cheetah & 2.15 & 2.02 & **1.53** & 41.2 & 40.53 & **35.01** & **0.56** & 0.53 & 0.43 & 21.46 & 21.61 & **21.75** \\ Ladyâ€™s Slipper & 0.17 & 0.08 & **0.07** & 22.53 & 11.13 & **10.24** & 0.71 & 0.68 & **0.76** & 24.65 & 26.13 & **26.15** \\  

Table 1: Comparative performance of GAN-based and DDIM-based cover selection techniques on the ImageNet dataset, with a payload \(B=4\) bpp. DDIM-optimized images achieve a significant gain over the original images and GAN-optimized images in both error rate reduction and image quality.

these differences across the batch. This process yields three maps, one for each color channel, which are subsequently normalized to a range between 0 and 1. Those maps are plotted in Fig. 3 (bottom).

As shown in Fig. 3, we observe correlations between the variance and the magnitude of the residual values; where pixels with lower-variance tends to have higher residual magnitudes. To quantify this observation, we introduced a threshold value of 0.5. In the residual maps (from Step 2), locations exceeding this threshold are classified as "high-message regions" and assigned a value of 1. Conversely, locations in the variance maps (from Step 1) falling below this threshold are defined as "low-variance regions", also set to 1. We discovered that **81.6%** of the high-message regions coincide with low-variance pixels. This substantial overlap confirms our hypothesis and underscores the encoder's tactic of utilizing low-variance areas to embed messages, which is a highly desired and natural behavior. We highlight that we are the first to make this observation, despite there being several relevant works on learning-driven steganography; none of these prior studies conducted an interpretation analysis of the encoder to uncover this behavior.

Interestingly, we find that the learned message embedding behavior closely aligns with the waterfilling strategy, the theoretically optimal embedding strategy for parallel Additive Gaussian Noise channels, a fundamental concept in communication theory (Cover ). This strategy involves embedding more messages in lower-variance pixel positions, which increases message recovery accuracy. Surprisingly, steganography methods tend to adopt this strategy implicitly, without explicit training to do so. In the subsequent section, we delve deeper into this analogy and further demonstrate the relationship between these two processes.

### Analogy to waterfilling

To validate the findings presented in Section 4.1, we draw parallels between our analysis and the waterfilling problem for Gaussian channels. We conceptualize the process of hiding secret messages as transmitting information through \(N\) parallel communication channels, where \(N\) corresponds to the number of pixels in an image. In this analogy, each pixel operates as an individual communication link, with the secret message functioning as the signal to be hidden and later recovered. The cover image, which embeds the hidden message, serves as noise unknown to the decoder.

We consider a simple additive steganography scheme: \(s_{i}=x_{i}+_{i}m_{i}\), for \(i=1,2,...,N\), where \(N=H W 3\) is the image dimension, \(m_{i}=\{-1,1\}\) indicates the \(i\)-th message to be embedded, \(_{i}\) its corresponding power, \(x_{i}\) and \(s_{i}\) represent the \(i\)-th element of the cover and steganographic images respectively. We assume a power constraint \(P\) that restricts the deviation between the cover and steganographic images: \(E[_{i=1}^{N}(s_{i}-x_{i})^{2}] P\).

This formulation is similar to the waterfilling solution for \(N\) parallel Gaussian channels (Cover ), where the objective is to distribute the total power \(P\) among the \(N\) channels so as to maximize the capacity \(C\), which is maximum rate at which information can be reliably transmitted

Figure 3: Normalized pixel variances (top) and residuals (bottom) calculated across a batch of 500 Robin images for each color channel, before optimization.

over a channel, defined as: \(C=_{i=1}^{N}_{2}(1+^{2}}{_{i}^{2}})\), where \(_{i}^{2}\) is the variance of \(x_{i}\). The problem can be formulated as a constrained optimization problem, where the optimal power allocation is given by \(_{i}^{2}=(-_{i}^{2})^{+}\), where \((x)^{+}=(x,0)\) and \(\) is chosen to satisfy the power constraint.

We calculate \(\{_{i}^{2}\}_{i=1}^{3 H W}\) using a batch of images, and find the optimized \(\{_{i}^{2}\}_{i=1}^{3 H W}\) using the approach described above. We plot the \(_{i}\)'s for each color channel in Fig. 4.

We observe a degree of similarity when comparing with Fig. 3 (bottom). To quantitatively assess this resemblance across color channels, we quantize the three matrices by setting values greater than 0.5 to 1 and values less than 0.5 to 0. For each channel, the similarity is calculated using the equation \((_{ij}^{(k)}=_{ij}^{(k)})}{256  256}\), where \(_{ij}^{(k)}\) and \(_{ij}^{(k)}\) are the \((i,j)\)-th pixels of the quantized waterfilling and residual matrices, respectively, for the channel \(k\). The computed similarity scores are **81.8%** for red, **65.5%** for green, and **74.9%** for blue, revealing varying degrees of resemblance with the waterfilling strategy across the color channels. The variation underscores that the waterfilling strategy is implemented more effectively in some channels than in others.

### Impact of cover selection

A natural question becomes: what is the cover selection optimization doing? We plot the variance maps of the optimized cover images in Fig. 5.

We notice that the number of low variance spots significantly increased as compared to Fig. 3 (top), meaning that the encoder has more freedom in encoding the secret message. Quantitatively, we find that **92.4%** of the identified high-message positions are encoded in low-variance pixels, as compared to **81.6%** before optimization. Given that the encoder preferentially embeds data in these low variance areas, this increase provides greater flexibility for data embedding, thereby explaining the performance gains observed in our framework.

## 5 Practical settings

In this section, we adapt our framework for practical considerations. We evaluate its performance across different payloads (Section 5.1), adapt it for JPEG compression (Section 5.2), and confirm security against steganalysis (Section 5.3). Computational times are detailed in Appendix I. We use two datasets, CelebA-HQ (Karras et al. (2017)) and AFHQ-Dog (Choi et al. (2020)), using the same settings described in Section 3.3.

Figure 4: Power coefficients \(_{i}\) for each color channel, calculated using a batch of 500 Robin images.

Figure 5: Normalized pixel variances across a batch of 500 Robin images for each color channel, after optimization.

### Payload impact on performance

We explore different payload capacities \(B\), highlighted in Table 2. We show the results for \(B=1,2,3,4\) bits per pixel (bpp). DDIM-optimized images show error rates significantly lower than originals, with image quality metrics like BRISQUE, SSIM, and PSNR largely preserved, though some quality decline was noted at lower bpp levels in CelebA-HQ and AFHQ-Dog. We include sample generated cover images generated using the DDIM framework in Appendix E. Despite experimenting with various regularization techniques aimed at maintaining image quality, no noticeable improvement was observed (Appendix C). Considering this, extending our framework to explore novel regularization techniques for such payload capacities is an interesting future direction. We also provide example cover and steganographic images generated by the LISO framework under different payload values in Appendix F.

### JPEG compression

Robustness against lossy image compression is crucial for steganography. We extend our framework to accommodate JPEG compression (Wallace (1991)). Following Athalye et al. (2018), we implement an approximate JPEG layer where the forward pass executes standard JPEG compression, while the backward pass operates as an identity function. Once the encoder-decoder pair is trained, we generate a JPEG-compliant cover image following the framework described in Section 3.1, augmented by adding a JPEG layer post-encoding. In Table 3, we demonstrate that our framework achieves improved error rates for \(B=1\) bpp, thereby validating our approach's capability to optimize cover images under JPEG compression constraints. In addition, we show robustness results to Gaussian noise in Appendix K.

### Steganalysis

Steganalysis systems are designed to detect whether there is hidden information within images. As these tools evolve, neural steganography techniques now integrate these systems into their end-to-end pipelines to create images that can bypass detection (Chen et al. (2022), Shang et al. (2020)). We show our results in Table 4 on the AFHQ-Dog dataset. Following the approach in Chen et al. (2022), we evaluate the security of our optimized images by measuring the detection rate using the steganalysis tool XuNet (Xu et al. (2016)) and also record message recovery error rates. The image quality metrics, such as BRISQUE, SSIM, and PSNR, are comparable to those listed in Table 2 and have therefore been omitted for brevity. We explore two different scenarios:

   &  &  \\  Dataset & Original & DDIM & Original & DDIM & Original & DDIM \\  CelebA-HQ & 0.12 & **0.06** & 21.09 & **21.53** \\ AFHQ-Dog & 0.15 & **0.11** & 19.34 & **19.63** \\  

Table 3: JPEG results for \(B=1\) bpp.

   &  &  &  &  &  \\   & & Original & DDIM & Original & DDIM & Original & DDIM \\   & 1 bpp & 2.6E-04 & **1.5E-05** & **2.75** & 4.07 & **0.95** & 0.94 & 36.25 & **36.37** \\  & 2 bpp & 2.3E-03 & **9E-04** & **5.9** & 9.7 & 0.91 & **0.92** & 31.82 & **32.46** \\  & 3 bpp & 0.011 & **0.002** & 9.95 & **9.83** & 0.86 & **0.87** & 32.16 & **33.88** \\  & 4 bpp & 0.051 & **0.019** & 11.91 & **11.04** & 0.81 & **0.83** & 30.91 & **32.46** \\   & 1 bpp & SE-05 & **0.00** & 12.14 & **12.11** & **0.94** & 0.93 & 36.84 & **36.87** \\  & 2 bpp & SE-04 & **6.8E-05** & **4.12** & 7.19 & 0.93 & **0.94** & **35.1** & 34.4 \\   & 3 bpp & 0.007 & **0.002** & 10.34 & **6.87** & **0.86** & 0.85 & 32.5 & **32.6** \\   & 4 bpp & 0.11 & **0.009** & 13.49 & **13.42** & 0.75 & **0.76** & 28.97 & **28.99** \\  

Table 2: Performance comparison across AFHQ-Dog and CelebA-HQ across various payloads. We observe that the error rates of DDIM-optimized images are significantly lower than original images.

**Scenario 1:** In this scenario, the experimental setup remains the same as described in Section 3.1 and illustrated in Fig. 2. The steganographic encoder-decoder pair is trained without regularizers to evade steganalysis detection. The DDIM-optimized images exhibit comparable detection rates at payloads of \(B=1\) and \(B=4\), superior performance at \(B=2\), and inferior performance at \(B=3\), all while achieving significantly lower error rates. While it is puzzling that detection rates do not consistently decrease with lower payload size, this phenomenon is also observed in LISO Chen et al. (2022), on which our framework is built. We provide a more detailed discussion in Appendix J.

**Scenario 2:** We leverage the differentiability of XuNet as described in Chen et al. (2022). During the optimization of the steganographic encoder-decoder pair, we introduce an additional loss term to account for steganalysis. This adjustment leads to a notable reduction in detection rates across all payload sizes, while maintaining consistently low error rates for both original and DDIM-optimized images. Notably, DDIM-optimized images exhibit even lower detection and error rates compared to the original images, demonstrating superior performance.

Further implementation details, along with results using an alternative steganalysis method, SRNet (Boroumand et al. (2018)), are provided in Appendix J.

## 6 Conclusion

We propose a novel cover selection framework for steganography leveraging pretrained generative models. We demonstrate that by carefully optimizing the latent space of these models, we generate steganographic images that exhibit high visual quality and embedding capacity. Additionally, our information-theoretic analysis shows that message hiding predominantly occurs in low-variance pixels, reflecting the waterfilling algorithm's approach to parallel Gaussian channels. Our framework is versatile, allowing for the incorporation of further constraints to produce JPEG-resistant steganographic images or to evade detection by particular steganalysis systems. For future work, we aim to expand our analysis (Section 4.2) to draw similarities with correlated Gaussian channels, moving beyond the independent channels considered in this work.