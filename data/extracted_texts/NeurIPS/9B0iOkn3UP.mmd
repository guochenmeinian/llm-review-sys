# Computational Aspects of Bayesian Persuasion under Approximate Best Response

Kunhe Yang

University of California, Berkeley

kunheyang@berkeley.edu

&Hanrui Zhang

Chinese University of Hong Kong

hanrui@cse.cuhk.edu.hk

###### Abstract

We study Bayesian persuasion under approximate best response, where the receiver may choose any action that is not too much suboptimal, given their posterior belief upon receiving the signal. We focus on the computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies for the sender. Despite the absence of the revelation principle -- which has been one of the most powerful tools in Bayesian persuasion -- we design polynomial-time exact algorithms for the problem when either the state space or the action space is small, as well as a quasi-polynomial-time approximation scheme (QPTAS) for the general problem. On the negative side, we show there is no polynomial-time exact algorithm for the general problem unless \(=\). Our results build on several new algorithmic ideas, which might be useful in other principal-agent problems where robustness is desired.

## 1 Introduction

Bayesian persuasion [Kamenica and Gentzkow, 2011] concerns the problem where a principal (the "sender") incentivizes a self-interested agent (the "receiver") to act in certain ways by selectively revealing information about the state of the world. A commonly cited simplistic example -- which nonetheless illustrates the essence of the problem -- is that of selling apples.

**Example 1.1**.: _Suppose a buyer (the receiver) is debating whether they should buy an apple from a seller (the sender). A priori, the buyer believes the apple (which is, say, a random apple from a large batch of apples) to be a "good" one with probability \(1/3\), and a "bad" one with probability \(2/3\). Moreover, suppose the buyer derives utility \(1\) from buying a good apple, and \(-1\) from buying a bad one. Then, aiming to maximize their expected utility, without any further information, the buyer should simply not buy the apple, because buying it would lead to expected utility \(1+(-1)=-<0\). The seller, who has perfect knowledge of the quality of the apple, and wants to "persuade" the buyer to buy the apple, can of course simply reveal the quality of the apple. Assuming the seller can do so in a credible way (which is a fundamental assumption in Bayesian persuasion), the buyer will buy the apple whenever it is good, which happens with probability \(1/3\)._

_In fact, the seller can do even better by employing the following strategy: the seller promises to send a "signal" to the buyer regarding the quality of the apple, which can be either "probably good" or "definitely bad". More specifically, if the apple is good, the sender signals that it is "probably good"; if the apple is bad, the seller signals randomly, that it is "probably good" notwithstanding with probability \(0.499\), and that it is "definitely bad" with probability \(0.501\). Now consider the buyer's perspective, assuming the seller does act up to their promises. If the signal says "definitely bad", then the buyer should certainly not buy the apple. If, however, the signal says "probably good", then the buyer faces a more interesting decision. All the buyer knows is that the probability of (1) receiving "probably good" and (2) the apple is actually good, happening simultaneously, is \(1/3\), and that the probability of (1) receiving "probably good" and (2) the apple is actually bad, happeningsimultaneously, is \(2/3 0.499<1/3\). So, given the signal, the conditional probability that the apple is good is larger than \(1/2\), which means the conditional expected utility of buying the apple is now positive. In other words, the seller has persuaded the rational buyer to buy the apple whenever the signal is "probably good", which happens with probability \(1/3+2/3 0.499 2/3\). This turns out to be (almost) the best that the seller can do._

Since its introduction by Kamenica and Gentzkow (2011), Bayesian persuasion has attracted enormous attention from both economists and computer scientists. Indeed, a satisfactory solution to the problem is often twofold, involving an economic characterization that confines the sender's strategy space without loss of generality, and an efficient algorithm that finds the optimal strategy within this confined strategy space. For example, in the standard (and somewhat idealized) model of Bayesian persuasion, a prominent principle in economics, namely the revelation principle, states that without loss of generality, the sender's optimal strategy is to simply recommend an action (depending on the state of the world, effectively revealing partial information thereof) for the receiver to take, and make sure it is in the receiver's best interest to always follow such a recommendation, given their posterior belief of the state of the world. The computational problem of searching for the optimal strategy within this structured space of strategies turns out to be much easier than that of searching over the unconstrained strategy space. Such characterize-and-solve approaches have proved extremely successful in Bayesian persuasion -- at least in settings where they apply.

The real world, unfortunately, is less precise than the idealized model. Even in the simplistic example of selling apples, there appear to be numerous subtleties that could potentially derail the supposedly (almost) optimal strategy of the seller. To name a few: the sender may not know exactly the receiver's prior belief, which might be different from the true distribution of the state of the world; the sender may not know exactly the receiver's utility function, which might be hard to evaluate even for the receiver themself; the device that generates randomness for the sender may be imperfect, affecting the receiver's posterior belief formed upon receiving a signal. In all these cases, the small inaccuracy can completely change the receiver's behavior: in the example of selling apples, the buyer would never buy the apple if they believe the apple is good a priori with probability \(0.32\) instead of \(1/3\), or if buying a good apple gives them utility \(0.98\) instead of \(1\), or if the device that generates randomness for the seller works in a way such that when the apple is bad, the seller actually signals "probably good" with probability \(0.501\) instead of \(0.499\). From the perspective of the sender, it would appear as if the receiver is acting in a somewhat suboptimal and unexpected way. As we will show later, the powerful revelation principle no longer applies in such cases, and the problem of finding the sender's optimal strategy becomes much trickier. On top of that, things become even more complicated when there are more than \(2\) possible states of the world, and/or more than \(2\) actions for the receiver to choose between. All this brings us to our main question: despite the lack of a structural characterization, is there a principled and efficient way of finding the optimal strategy for the sender that is robust against such inaccuracy?

### Our Results and Techniques

In this paper, we study Bayesian persuasion in a natural model that accounts for the kind of subtleties discussed above: roughly speaking, the model allows the receiver to choose any action that is "not too much suboptimal", given their posterior belief upon receiving the signal. We focus our attention on computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies.

Direct-revalation signaling schemes are suboptimal.Our first finding is negative: there exists extremely simple problem instances (with only \(2\) possible actions and \(3\) possible states), where any direct-revelation signaling scheme (i.e., a strategy of the sender where each possible signal is a recommendation of a single action, as discussed above) is suboptimal at least by a factor of 2, or an additive gap of \(1/2\). This implies that the revelation principle ceases to work in our model, and the characterize-and-solve approach can no longer be employed.

LP formulation, and efficient algorithm with small action spaces.The above result highlights the need for new algorithmic ideas, and we present several of them in this paper. The first one is a linear program (LP) formulation for optimal strategies. The LP formulation has a similar high-level structure to the standard one for Bayesian persuasion: the variables correspond to probabilities that each signal is sent in each possible state of the world, and the constraints enforce the "semantics" of the signals, in terms of how the receiver is expected to respond. However, one complication (among others) in our model is that multiple actions might be taken as the response to any fixed signal, so the semantics of a signal must be rich enough to encode the subset of actions that are possible in response to that signal (plus any additional information required to describe a signal). In particular, this means in general, the number of possible signals is exponential in the number of actions. As such, the LP formulation alone implies an efficient algorithm for our problem only when the number of actions is constant or logarithmic. Nonetheless, the LP formulation serves as a building block of our further algorithmic results.

Efficient algorithm with small state spaces.Next we design an efficient algorithm when the number of states is constant. To see how this is possible, observe that a subset of actions can each be chosen as the response to a signal, only if the posterior utilities corresponding to these actions are all close to the best possible. In other words, if there does not exist a posterior belief given which a subset of actions are all almost optimal, then this subset itself is "infeasible" as a set of possible responses. Such a subset of actions can never describe a signal in any strategy. Now the hope is to argue that the number of feasible subsets of actions is not too large. It turns out this can be obtained as a consequence of a result in combinatorial geometry, which bounds the number of "cells" in a low-dimensional space cut by a number of hyperplanes. We show that all feasible subsets induce a partition of these cells, so the same bound applies to the number of subsets too. In fact, one can show that the number of feasible subsets of actions is \(n^{O(m)}\), where \(n\) and \(m\) are the number of actions and that of states, respectively.

Now we know the number of relevant subsets of actions cannot be too large, but it remains a problem to find these subsets. To this end, we make yet another geometric observation: the cells that correspond to feasible subsets of actions form a single "connected component", which means we can enumerate all feasible subsets by traversing this component. More specifically, we start from any feasible subset, and try all its "neighbors" by swapping in or out a single possible action. For each neighbor, we check its feasibility by solving another LP. By repeating this procedure we can reach all feasible subsets, in time polynomial in the number of feasible subsets. Once we have computed all feasible subsets, we simply solve the LP for optimal strategies restricted to these subsets, which is of polynomial size when the number of states \(m\) is constant. This gives us an efficient algorithm with constant-size state spaces.

Hardness of exact computation of the general problem.Knowing that the problem can be solved with small action spaces or small state spaces, it is then natural to seek an efficient algorithm that works unconditionally, without restrictions on any parameters of the problem. We show, unfortunately, that such an algorithm does not exist unless \(=\). We do so by reducing from an "equally hard" variant of the subset sum problem: given a set of \(2n\) integers that sum to \(0\), decide whether there are \(n\) integers out of the \(2n\) that also sum to \(0\). The idea of the reduction is that such a set of \(n\) integers corresponds to a signal that gives the sender the highest posterior utility possible. To ensure this, the utility functions need to exhibit delicate structures, such that a signal that corresponds to either too many or too few integers must be suboptimal. The latter appears to be a quite ambivalent condition -- in fact, this is only possible with the kind of inaccuracy that we consider. We believe the ideas of our reduction are potentially useful in other principal-agent problems where robustness is required.

Approximation in quasi-polynomial time.In light of the hardness result, we turn our attention to approximation algorithms. We present a quasi-polynomial-time approximation scheme (QPTAS): for any target additive error \(>0\), we give an \(\)-approximate algorithm that runs in time quasi-polynomial in \(m\) and \(n\), where the time complexity may depend on \(\). The idea is to cover the space of all possible posterior utility functions using small cells, and consider a representative point in each cell with a small error. It is known that a covering of size \(O( n/^{2})\) exists, which has already proven useful in other game-theoretic computational problems (Althofer, 1994; Lipton et al., 2003; Gan et al., 2023). However, one difficulty in our model is that there are certain types of errors that are unacceptable, no matter how small. For example, fixing a strategy, the sender's utility can be discontinuous in the receiver's utility function, and any error near points of discontinuity can lead to a huge gap in the sender's utility. Similarly, because of the robustness component in the model, a small error in the sender's utility may lead to a totally different response from the receiver. To avoid such gaps, we allow approximation only in the sender's utility, and refrain from estimating the action that the receiver will respond with. Instead, we consider the approximate worst-case utility of the sender, which turns out to be tractable using linear constraints involving approximate utility functions. Combining this with the LP formulation discussed above, we have an LP of quasi-polynomial size, which can be solved in quasi-polynomial time.

### Related Works

We defer a detailed discussion of related work to Appendix A. Our work connects to two lines of work, the computational aspect of Bayesian persuasion (Dughmi and Xu, 2016; Dughmi, 2017, 2014; Bhaskar et al., 2016; Rubinstein, 2017; Babichenko and Barman, 2016, 2017; Xu, 2020; Zhou et al., 2022) and various notions of robust Bayesian persuasion (de Clippel and Zhang, 2022; Chen and Lin, 2023; Camara et al., 2020; Zu et al., 2021; Collina et al., 2023; Kosterina, 2022; Dworczak and Pavan, 2022; Hu and Weng, 2021; Castiglioni et al., 2020; Wu et al., 2022; Babichenko et al., 2022). We also discuss the comparison between our paper and that of Gan et al. (2023) on robust Stackelberg games.

## 2 Preliminaries

Our model builds on the classic Bayesian persuasion model with a single sender and a single receiver. At a high level, the model formalizes a scenario where the sender, possessing private information about the true state, aims to influence the receiver's decision-making by strategically sending partial information about the true state according to a pre-committed signaling scheme. We start by introducing the basic setting and notations, and then introduce the robust Bayesian persuasion model that considers a receiver who acts not exactly, but approximately in accordance with their best interest in the decision-making process.

### Bayesian persuasion: the classical model

Basic setting and notations.Let \(\) be the states of the world with \(||=m\), and \(()\) be the set of all probability distributions on \(\). For all distributions \(()\), let \(()\) be the support of \(\), i.e., \(()\{()>0\}\). We use \(_{0}()\) to denote the prior distribution over states and assume that \(_{0}\) is common knowledge between the sender and the receiver.

Signaling scheme.Let \(\) (\(||<\)) be a finite set of signals. A _signaling scheme_\(:\) is a randomized mapping from the states of the world to probability distributions over signals. In the Bayesian persuasion protocol, the sender first commits to a signaling scheme \(\), then observes the true state of the world \(_{0}\), and sends signal \(()\) to the receiver. We use \((,)\) to denote the probability of sending signal \(\) conditioning on observing \(\) as the true state, and \(()=_{^{}}_{0}(^{}) (^{},)\) to denote the marginal probability of a signal \(\) being realized. Upon receiving the signal \(\), the receiver performs a Bayes update on \(_{0}\) using knowledge about the scheme \(\) to obtain a posterior belief \(_{}()\), i.e.,

\[_{}()=()(,)}{( )}.\]

In addition, algebraic manipulations based on the Bayes' rule suggest that the signaling scheme \(\) can be viewed as the process of creating a distribution over posterior distributions that satisfy the Bayes plausibility condition (Kamenica and Gentzkow, 2011; Gentzkow and Kamenica, 2016):

\[,_{0}()=_{}( )_{}().\] (Bayes plausibility)

Throughout this paper, we will frequently adopt this perspective, especially in the characterization of (robust) utilities.

Utilities and best response setsAlthough only the receiver can take actions, their action influences both the sender and the receiver's utility, both of which also depend on the state of the world. Let \(\) be the action space of the receiver with \(||=n\). We use \(s:\) to denote the sender's utility and \(r:\) to denote the receiver's utility, where both utilities are normalized to be between \(0\) and \(1\). Additionally, for distributions \(()\), we abuse the notations and use \(s(,a)=_{}\,s(,a)\) and \(r(,a)=_{}\,r(,a)\) to denote the sender and receiver's expected utilities under the state distribution \(\).

Receiver's strategiesA receiver's strategy, denoted with \(:\), is a (possibly randomized) mapping from the observed signals to actions which specifies the receiver's strategy of choosing responses. Given a signaling scheme \(\) and a receiver strategy \(\), we use \(S(,)\) to denote the expected sender utility, which is computed as

\[S(,)_{}() s(_{ },())=_{}_{}_{0}( )(,)s(,()).\]

The classical model of Bayesian persuasion assumes that the receiver's strategy \(()\) is always the exact best response of the posterior distribution \(_{}\) that maximizes their expected posterior utility (ties are broken in favor of the sender). Formally, the best response action action is defined as

\[a^{}(_{})*{argmax}_{a}r( _{},a),@note{footnote}{When there are multiple actions $a^{}(_{})$ that maximizes the receiver's utility, we define $a^{}(_{})$ to be the maximizer that achieves the highest sender's utility.}\]

and the receiver's strategy is assumed to satisfy \(()=a^{}(_{})\). In this paper, we will relax this exact best-response assumption to allow for approximate best responses.

### Robust Bayesian persuasion with approximate best responses

In this paper, we allow for some degree of suboptimality in the receiver's response. Specifically, we assume that the expected utility under receiver's response \(()\) is not too suboptimal compared to the best action \(a^{}(_{})\). Formally, if we use \(_{}()\) to denote the set of \(\)-optimal responses on the input belief:

\[_{}()\{a\ |\ r(,a)>r(,a^{}())-. \},\] (1)

then a receiver's strategy \(:\) is \(\)-best response (or \(\)-BR) if it satisfies \(()_{}(_{})\) for all \(\). We use \(_{}()\) to denote the set of all \(\)-BR strategies under the signaling scheme \(\).

Robust sender utilityIn this paper, we adopt the max-min adversarial robustness perspective and aim to offer robustness guarantees against worst-case \(\)-BR strategies. We use the _robust utility_\(_{}\) of a signaling scheme to characterize the expected utility under the worst-case \(\)-BR strategy:

\[_{}()=_{_{}_{}( )}S(,_{}).\]

The sender aims to maximize the robust utility through optimizing the signaling scheme. Equivalently, the sender faces a bi-level optimization problem of the following max-min form:

\[_{}^{}_{^{}}_{ }(^{})=_{^{}}_{_{}_{}(^{})}S(^{},_{}).\] (sender's objective)

Solving the optimization problem in (sender's objective) includes (1) finding an appropriate signal space \(\) that is sufficient to achieve the optimal utility and (2) optimizing for the optimal signaling scheme given the signal space.

**Remark 2.1** (worst-case \(\)-BR strategy).: _It is not hard to see that the worst-case \(\)-BR strategy \(_{}\) that achieves (sender's objective) will choose the worst action that minimizes the sender's utility for each posterior distribution separately, i.e.,_

\[_{}()*{argmin}_{a_{}(_ {})}s(_{},a).\]

_Therefore, the robust utility under any signaling scheme \(\) can be equivalently written as_

\[_{}()=_{}() s( _{},_{}())=_{}() _{a_{}(_{})}s(_{},a).\]

**Remark 2.2** (Strict inequality in definition of \(_{}\) set).: _In eq. (1), we use strict inequality so that the sender's strategy space is closed and compact, and the sender's objective is well-defined. This is a non-essential choice: one could instead define \(\)-BR responses using weak inequality and investigate the sender's supremum robust utility, which would not change the nature of the problem._LP formulation and algorithm with small action spaces

As we show in Appendix B, the powerful revelation principle fails in the robustness model that we study. In fact, we establish the following claim.

**Proposition 3.1** (Proposition B.1 in Appendix B).: _There exists a sequence of Bayesian persuasion instances with a robustness level \(=(1)\), such that the following holds in the limit: any direct-revelation scheme \(\) is suboptimal, at least by a factor of \(2\) or an additive gap of \(\). That is, for any direct-revelation scheme \(\),_

\[_{}()_{}^{*}, _{}()_{}^{*}- {1}{2}.\]

The above proposition highlights the need for new algorithmic ideas to efficiently compute (approximately) optimal robust schemes.

In this section, we present a linear program (LP) that computes the optimal robust utility \(_{}^{*}\) and the optimal robust signaling scheme \(^{}\). Although the high-level idea is similar to the LP formulation for computing the optimal non-robust scheme in standard Bayesian persuasion (Dughmi and Xu, 2016; Dughmi, 2017), our robust LP formulation follows significantly different semantics.

We characterize the maximum number of signals needed to achieve the optimal robust utility in Lemma 3.2. This lemma can be viewed as a generalized (and unfortunately, much less powerful due to lack of best responses) version of the revelation-principle style argument in (Kamenica and Gentzkow, 2011; Kamenica and Gentzkow, 2011) that accounts for worst-case approximate best responses. See Appendix C for the proof of the lemma.

**Lemma 3.2**.: _There exists an optimal robust signaling scheme that is supported on at most \(n 2^{n-1}\) signals, in which each signal \(\) corresponds to a unique pair of \((A,)\) such that \(=a^{}(_{})\) and \(A=_{}(_{})\)._

According to Lemma 3.2, it suffices to consider the following signal space:

\[=\{(A,) A,\; A\},\]

where each signal \(=(A,)\) satisfies both \(a^{}(_{})=\) and \(_{}(_{})=A\). Recall from Remark 2.1 that the robust utility can be written as

\[_{}^{}=_{}_{(A,)} ((A,))_{a_{}(_{(A,)} )}s(_{(A,},a).\]

Translating this max-min optimization to a constrained maximization problem, and enforcing the abovementioned semantics for each signal, we arrive at the following (non-linear) program in Fig. 1.

Although the program in Fig. 1 is not yet a linear program, both the first and second constraints can be equivalently written as linear constraints on the conditional probabilities \((,(A,))\) that define the signaling scheme \(\), using the following observation based on the Bayes' rule:

\[,(_{}) s(_{ },a)=_{}_{0}()(, ) s(,a).\] (2)

Therefore, it remains to characterize the \(\)-BR response sets, and would be ideal if they could also be equivalently expressed as linear constraints. Unfortunately, the definition of \(\)-BR sets involves strict-inequality constraints due to the issues discussed in Remark 2.2. In particular, for each \((A,)\), the third constraint is equivalent to

(1) \[ a A,\;s(_{},a)>s(_{},)-; \; a A,\;s(_{},a) s(_ {},)-,\]

Figure 1: A program for the optimal robust signaling scheme supported on \(=\{(A,)\}\).

where (1) involves a strict inequality and could lead to open polytopes. Due to such subtleties, we first consider the LP in Fig. 2 which is a relaxation of the program in Fig. 1 by dropping the strict-inequality constraint (1). The last two constraints in Fig. 1 guarantee that variables \((,A,)\) give rise to a valid signal distribution for every state \(\).

Since Fig. 2 is a relaxation of the original program, its optimal objective provides an upper bound for \(_{}^{*}\). However, as we will show later, not only does the optimal objective value exactly equal \(_{}^{*}\), but the optimal variables \(^{*}\) also exactly characterize the optimal robust signaling scheme that achieves this \(_{}^{*}\). This gives us an algorithm that efficiently computes the optimal robust signaling scheme when the number of action spaces is constant or polynomial. We formalize this in Proposition 3.3, and provide a proof in Appendix C.

**Proposition 3.3** (Efficient algorithm for small action space).: _The optimal robust signaling scheme can be computed by the linear program in Fig. 2 with size \(O(2^{n}mn)\)._

## 4 Efficient algorithm with small state spaces

In this section, we focus on the robust persuasion instances where the action space is large, but the state space is small. For such instances, our key observation is that among the \((n-1)2^{n}\) tuples in \(\), only a small fraction of them can be realized by posterior distributions and therefore serve as feasible signal candidates. In Section 4.1, we characterize the structural properties of feasible tuples \((A,)\) and draw connection to the polytopes in the simplex supported on the state space. In Section 4.2, we leverage these structural insights and design an efficient algorithm that accelerates the computation of the optimal robust signaling scheme.

### Structural properties

We formally define the feasibility of a candidate tuple \((A,)\) according to the existence of a posterior distribution that has \(A\) as its \(\)-BR set and \(\) as its best response action.

**Definition 4.1** (Feasible subset-action tuple).: _A tuple \((A,)\) is feasible if there exists some posterior distribution \(()\) such that \(a^{*}()=\) and \(_{}()=A\). We use \(^{}\) to denote the set of all feasible tuples in \(\)._

For each feasible tuple \((A,)^{}\), let \(_{(A,)}\) be the set of posterior distributions that satisfy both constraints in Definition 4.1:

\[_{(A,)}\{()\ |\ a^{*}()= ,\ _{}()=A\}\]

It is not hard to see that the subsets \(_{(A,)}\) partitions the simplex, because each distribution in the simplex is associated with a unique best response action and \(\)-BR set. That is,

\[()=_{(A,)^{}}_{(A,)}.\]

Figure 2: Relaxed LP for the optimal robust signaling scheme

In addition, each \(_{(A,)}\) is a polytope in the simplex that is defined by the following constraints that are linear in the distribution \(\):

\[_{(A,)}r(,a) r(,),&  a A;\\ r(,a)>r(,)-,& a A;\\ r(,a) r(,)-,& a a.\] (3)

Our key observation is that the linear constraints that characterize different \(_{(A,)}\) all take one of the following two forms based on an ordered pair of actions \((a,a^{})\):

1. Either \(r(,a) r(,a^{})-\), or its complement \(r(,a)>r(,a^{})-\);
2. \(r(,a) r(,a^{})\),

which give rise to no more than \(2n^{2}\) hyperplanes. Although the number of hyperplanes scales with the potentially large number of actions, when the state space is small, these hyperplanes, and the polytopes they define, all live within a low-dimensional space. According to a fundamental theorem in computational geometry (see, e.g., Theorem 28.1.1 in ), these hyperplanes cut the \(m\)-dimensional into at most \(O((2n^{2})^{m-1})\) cells. This observation helps us to bound the number of feasible tuples in the following lemma. The proof of Lemma 4.2 is deferred to Appendix D.2.

**Lemma 4.2**.: _The size of the feasible tuples \(^{}\) satisfy \(|^{}|\{n2^{n-1},n^{O(m)}\}\)._

Next, we establish some structural properties on the set of feasible tuples \(^{}\) that could facilitate the efficient search over candidate signals. In the remainder of this section, we first define a bounded-degree _symmetric difference graph_ on all tuples in \(\), then show that all feasible tuples in \(^{}\) form a connected component in the symmetric difference graph.

**Definition 4.3** (Symmetric difference graph).: _In the symmetric difference graph \(G(,E)\), each vertex is a tuple \((A,)\). Each edge \(\{(A_{1},_{1}),(A_{2},_{2})\} E\) represents that the symmetric difference between tuples \((A_{1},_{1})\) and \((A_{2},_{2})\) is of size \(1\), which is satisfied for the following two cases:_

* \(A_{1}=A_{2}\)_, and_ \(_{1}_{2}\)_._
* \(_{1}=_{2} A_{1} A_{2}\)_, and_ \(|(A_{1} A_{2})(A_{2} A_{1})|=1\)_._

The symmetric difference graph \(G\) characterizes the structural relationship between tuples in \(\), in which only any two tuples are connected if and only if their symmetric difference is small enough. Together with the following lemma, \(G\) provides a useful framework for us to search within the exponentially large vertex set.

**Lemma 4.4** (Connectivity).: _The subgraph of \(G\) induced by \(^{}\) is connected._

We defer the proof of Lemma 4.4 to Appendix D.1. At a high level, the proof translates the connectivity of the polytopes \(_{(A,)}\) in the simplex space to the connectivity of feasible tuples in the symmetric distance graph \(G\) by analyzing the geometric properties of the common face of shared by every pair of adjacent polytopes.

### Algorithm

In this section, we present Algorithm 1 that leverages the connectivity of the feasible tuples to efficiently search for \(^{}\). This algorithm essentially performs depth-first-search (DFS) on the connectivity graph by recursively searching all the neighbors with symmetric difference of size \(1\) to the given tuple that it's currently searching. The initial tuple to start the Explore procedure can be set to any tuple that is already known to be feasible, e.g., the \(\)-BR set and the optimal response associated with the prior distribution \(_{0}\).

Note that each tuple \((A,)\) is feasible if and only if there exists \(\) that satisfies all three constraints in Equation (3) simultaneously, which can be written as checking the feasibility of the corresponding LP (with strict-inequality constraints) on the distributions \(\). To replace strict-inequality constraints with non-strict constraints, we propose to check the LP via a margin-maximization trick in Fig. 4 of Appendix D.3. This LP is defined on a closed polytope, and asserts feasibility if and only if the optimal margin is strictly positive, i.e., \(^{*}>0\).

**Theorem 4.5**.: _Running Algorithm 2 with initial tuple \((_{}(_{0}),a^{}(_{0}))\) finds all feasible tuples \(^{}\) by solving at most \(n^{O(m)}\) LPs, each of size \(O(m+n)\)._

Proof.: The correctness of this algorithm is ensured by the connectivity property in Lemma 4.4. It remains to upper bound the number of feasibility checks performed. Since Algorithm 1 only checks new a vertex when it has not been checked before, the total number of feasibility checks is upper bounded by the size of the closed neighborhood of \(^{}\). Since each tuple \((A,)\) has at most \(O(n)\) neighbors, the degree of any vertex in the symmetric difference graph \(G\) is upper bounded by \(O(n)\). Therefore, the size of the closed neighborhood is at most \(O(n)|^{}| n^{O(m)}\). 

The final step for optimizing signaling schemes is to solve the LP in Fig. 2 with the original signal space \(\) replaced by feasible tuples \(^{}\). We summarize the entire procedure and its complexity in Algorithm 2 and Corollary 4.6.

**Corollary 4.6**.: _When the number of states \(m\) is small, the optimal signaling scheme can be efficiently computed by Algorithm 2, which involves solving \(n^{O(m)}\) LPs of size \(O(m+n)\) and then solve a single LP of size \(m n^{O(m)}\)._

``` Input: tuple \((A,)\)
1 Check the feasibility of \((A,)\) by solving the LP in Fig. 4, \(^{}\) optimal objective value;
2if\(^{}>0\)then
3 Mark \((A,)\) checked and feasible;
4foractions \(a A\{\}\)do
5 If \((A\{a\},)\) is unchecked, call Explore(\((A\{a\},)\));
6 If \((A,a)\) is unchecked, call Explore(\((A,a)\));
7
8 end for
9foractions \(a A\)do
10 If \((A\{a\},)\) is unchecked, call Explore(\((A\{a\},)\));
11 end for
12
13 end for
14
15 end for
16
17 end for ```

**Algorithm 2**Algorithm for small state spaces

## 5 Approximation algorithm for the general problem

In this section, we first show that the general problem without any restrictions is computationally hard, then present a quasi-polynomial time approximation scheme (QPTAS) for the problem. The following claim (proof defered to Appendix E) establishes the computational hardness of finding the exact optimal robust signaling scheme. This result implies that a polynomial-time exact algorithm for the robust persuasion problem does not exist unless \(=\).

**Theorem 5.1**.: _The problem of computing the exact optimal robust utility is \(\)-hard._

In the following, we present a QPTAS that computes an \(\)-approximate optimal \(\)-robust signaling scheme \(\) for any given \(>0\), such that

\[_{}()_{}^{}-.\]Our \(\)-optimal signaling scheme is supported on a significantly different signal space compared to the optimal schemes for cases of small state space and small action space. The high-level idea is, instead of enumerating each \((_{}(),a^{}())\) set, we partition the simplex \(()\) into small cells, each centered around a \(k\)-uniform distribution \(\) (to be defined later), and incorporate additional semantics to ensure that the utility profiles \(s(,)\) are representative enough for all distributions within this cell. It is useful to have the utilities at \(\) being representative because we can then determine the \(\)-BR sets according to the relative magnitude of utilities at \(\) and translate the requirements into linear constraints.

Formally, for an integer \(k\), a distribution \(()\) is \(k\)-uniform if \(,\ ()=}{k}\) for some integer \(k_{} k\). Let \(_{k}()\) denote the set of all \(k\)-uniform distributions on the state space. We have \(|_{k}|=O(m^{k})\). Moreover, by , for \(k_{}=}\), we have:

\[(),\ _{k_{ }},\  a,\ |s(,a)-s(,a)|.\]

Accordingly, we define the \(\)-cell around \(_{k}\) to be

\[_{}^{}=\{():\  a ,\ |s(,a)-s(,a)|\}.\]

Note that given \(\) and \(\), \(_{}^{}\) is equivalent to a set of \(2n\) linear constraints on \(\).

We consider the following signal space:

\[=\{(,,a) _{k_{^{}}},\ , A\},k_{ ^{}}=)^{2}} ,\ ^{}=\] (4)

Define LP variables \((,,,)\) for each \(\) and each signal \((,,)\) to represent the conditional probabilities of sending that signal. Each signal \(=(,,)\) is specified by the \(k\)-uniform distribution \(\) that centers the cell, the best response action \(\), and the action \(\) in \(\)-BR set that minimizes the sender's strategy. The LP is defined in Fig. 3.

Intuitively, the first two constraints in Fig. 3 are designed to ensure \(_{(,,)}_{}^{ }\). The third and fourth constraints describe the semantics of the receiver's response strategy. Finally, the last two constraints guarantee that \(\) represents a valid signal distribution on each state. We summarize the guarantee of the QPTAS in Theorem 5.2 and formally prove it in appendix F.

**Theorem 5.2**.: _For any \(>0\) and \(>0\), the optimal solution to the LP in Fig. 3 is an \(\)-approximate \(\)-robust signaling scheme that is supported on \(O(n^{2}m^{ 12.5(2n)/(^{2})})\) signals._

Figure 3: QPTAS for computing an \(\)-approximate robust signaling scheme, where \(^{}=\).

#### Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No. DMS-1928930 and by the Alfred P. Sloan Foundation under grant G-2021-16778, while Hanrui Zhang was in residence at the Simons Laufer Mathematical Sciences Institute (formerly MSRI) in Berkeley, California, during the Fall 2023 semester.