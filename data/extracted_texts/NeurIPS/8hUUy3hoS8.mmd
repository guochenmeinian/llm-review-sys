# StreamBench: Towards Benchmarking Continuous Improvement of Language Agents

Cheng-Kuang Wu\({}^{1,2}\)

Zhi Rui Tam\({}^{1}\)

Chieh-Yen Lin\({}^{1}\)

Yun-Nung Chen\({}^{1,2}\)

Hung-yi Lee\({}^{2}\)

\({}^{1}\)Appier AI Research

\({}^{2}\)National Taiwan University

{brian.wu, ray.tam}@appier.com

Equal contribution

###### Abstract

Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios. Source code: https://github.com/stream-bench/stream-bench. Benchmark website: https://stream-bench.github.io.

## 1 Introduction

Recently, large-scale pretraining  and instruction fine-tuning  have driven paradigm shifts in how we interact with language models. These advancements allow us to use them out-of-the-box to solve problems. Consequently, many benchmarks have emerged to evaluate the general capabilities of these models. Some notable examples include MMLU , GSM8K , and BIG-Bench-Hard . All these benchmarks aim to assess LLMs' _innate capabilities_, which we define as the general knowledge or reasoning abilities demonstrated when used out-of-the-box.

In addition to LLMs' strong _innate capabilities_, recent works have shown that LLM agents, which are LLMs augmented with extra components such as memory, retrievers, or tools, are able to improve themselves from experience. MemPrompt  shows that memory-enhanced GPT-3 can improve through time by storing past user feedback and retrieve them in the future. Reflexion  demonstrates that LLM agents can perform better in future trials by running repeated trials on the same dataset via self-reflection. ExpeL  further shows that LLM agents can learn from cross-task experience and improve performance without executing repeated trials on the target task.

Given LLM agents' self-improvement abilities, there remains a missing piece in the current evaluation landscape. Beyond measuring LLMs' _innate capabilities_ with aforementioned _offline_ benchmarks , it is important to assess their capacity to improve over time since we would like our systems to gradually improve after deployment. This gap motivated us to develop a new evaluation scenario-an _online_ setting to measure LLM agents' ability to continuously enhance their performance over time.

This _online_ setting focuses on scenarios where LLM agents attempt to solve a specific downstream task and improve themselves from an input-feedback sequence, with the goal to maximize the accuracy for the whole sequence of the agent's predictions.

Given these rationales, we introduce StreamBench, a benchmark designed to evaluate LLM agents' ability to improve themselves over an input-feedback sequence. StreamBench simulates an environment where LLM agents are exposed to a sequence of users' natural language requirements and feedback. To the best of our knowledge, StreamBench is the first benchmark to evaluate LLM agents in streaming scenarios with a diverse range of tasks. StreamBench aims to inspire further efforts to develop more adaptive LLM agents, thereby enhancing their practical effectiveness. Our contributions can be summarized as follows:

* We introduce StreamBench, the first benchmark designed to evaluate LLM agents' ability to improve over an input-feedback sequence in an _online_ setting across a wide range of tasks.
* We propose several simple yet effective baselines for enhancing LLM agents' performance in streaming scenarios, including a cost-effective multi-agent method that outperforms other baselines while maintaining the average cost of a single agent.
* We conduct analysis on the advantages and potential pitfalls of the proposed methods, providing insights into effective streaming strategies of LLMs.

## 2 Formulation

Consider a streaming scenario involving an agent, an external environment, and a sequence of inputs:

**Agent.** We define an _agent_ as an LLM parameterized by \(\) and augmented with additional components to enhance the agent's capabilities, such as the external memory \(\) and a retriever \(r()\) to store and retrieve useful information. Given an instance \(x\) in natural language, a prompting template \(p()\), and a retrieval function \(r()\), the agent's output is denoted as \(=f(p(x,r())|)\).

**Environment.** The external environment, denoted as \(g()\), provides feedback to the agent. The nature of \(g()\) varies depending on the specific downstream task and the type of feedback being collected. Potential roles for \(g()\) include human users, code execution environments, and API responses.

**Input-feedback sequence.** Consider a sequence of input stream where each input is denoted by \(x_{t}\), with \(t\) representing the \(t\)-th time step. After the agent provides the output \(_{t}\), the environment provides feedback signal \(fb_{t}=g(x_{t},_{t})\). Figure 1 shows an overview of the streaming scenario.

Algorithm 1 presents a simple framework for language agents to continuously learn from feedback. Benchmark users can adapt Algorithm 1 or develop their own algorithms to update components of their language agents, with the goal to maximize the accuracy of the entire sequence.

Figure 1: (Left) A schematic diagram showing the online evaluation setting of StreamBench, where agents update their components (\(p,r,\), or \(\)) from an input-feedback sequence to achieve the highest final accuracy (refer to Section 3.1 for details). (Right) Performance curve on the DDXPlus dataset on StreamBench. Agents are able to gradually improve with our proposed streaming baselines.

Traditionally, updating the agent at each time step \(t\) involves updating the model parameters \(\). However, as foundation models grow increasingly larger, frequently updating the agent's network parameters has become computationally expensive. Recent advancements offer promising alternatives for iterative improvement by updating other components of the agent. For example, one can adapt existing iterative prompt refinement strategies to refine \(p()\)[9; 10; 11], update the weights of the retriever \(r()\)[12; 13; 14], expand the agent's memory \(\)[6; 15], or use parameter-efficient fine-tuning techniques for augmenting \(\). These different strategies open new possibilities for continuous adaptation of agents without relying solely on network parameter updates. In this work, we develop several baselines for improving agents over time, with a particular focus on updating \(p()\) and \(\). The baselines demonstrate both simplicity and effectiveness. We leave methods for updating \(r()\) and \(\), which require computationally expensive network parameter updates, for future research.

## 3 StreamBench

### General setup

Streaming sequenceMost public datasets are inherently static, meaning each instance does not have a time-related dependency. To adapt them for our streaming setup, we serialize each selected dataset in Section 3.2 by assigning a time step to each instance. To avoid arbitrary sequence assignment in the original datasets, we randomly shuffle each dataset using a fixed random seed. We release each dataset's assigned sequence obtained by this random seed in the supplementary materials to ensure reproducibility on StreamBench. Additionally, to ensure the robustness of our evaluation, we conduct experiments on different shuffled sequences with five random seeds, as discussed in Section 5.2. We also discuss the effects of distributional shifts in Appendix C.2.

Feedback signalsChoosing appropriate type of feedback signal is a crucial consideration in StreamBench. Firstly, cost and practicality play a significant role; in practice, obtaining ground truth \(y_{t}\) at each time step can be prohibitively expensive. For example, providing the exact code in programming tasks or the complete schema of each API call in tool use tasks is often impractical. In contrast, partial feedback, such as the helpfulness or correctness of the agent's output, is relatively easy to obtain-such as the "thumbs up" or "thumbs down" buttons commonly found in user interfaces of LLM applications. Given these rationales, we formalize the type of \(fb_{t}\) as follows:

\[fb_{t}=g(x_{t},_{t}),fb_{t}\{0,1\}\]

where \(fb_{t}\) is a scalar serving as a proxy for the correctness of \(_{t}\) with respect to \(x_{t}\), determined by the environment \(g()\) of the given downstream tasks. The feedback \(fb_{t}\{0,1\}\) is binary, indicating whether the agent's output \(_{t}\) is correct. This simplified feedback setting aims to offer a unified evaluation framework for ensuring consistency and practicality across diverse tasks. We leave other designs of \(fb_{t}\), such as ground truth or natural language feedback, for future works.

EvaluationIn practice, an agent's goal is to satisfy as many user requirements as possible over a time sequence. We thus evaluate an agent by its aggregate metric at the final time step (\(T\)). For example, the final metric on a given dataset can be calculated as \(^{T}h(_{t},y_{t})}{T}\), where \(h\) is the function for calculating the corresponding metric on a given dataset. Table 1 shows metrics for each dataset.

### Datasets

To measure LLM agents' capacity for continuous improvement post-deployment, we select a diverse set of downstream tasks with potential real-world applications. Following the setting in Section 3.1, these tasks share the property that their ground truth output \(y_{t}\) is costly to obtain at each time step.

Text-to-SQLFor text-to-SQL tasks, the agent has to convert users' natural language queries into SQL code to meet their data requirements. StreamBench integrates three prominent datasets: Spider , CoSQL , and BIRD . These datasets represent a progressive difficulty curve, allowing for evaluation of how well agents improve when faced with data of varying difficulties.

Python programmingTo evaluate coding ability improvement, we use the DS-1000  dataset, which consists of real-world Python programming questions from StackOverflow. To successfully solve a given question, the agent must provide a solution and pass the associated test cases.

Tool useThe ability to use external tools is a significant milestone in the development of LLMs, as it compensates for certain limitations, such as performing precise arithmetic operations or conducting web searches. For this purpose, we utilize the large-scale tool usage dataset ToolBench , and select the subset that includes stable and low-latency tool APIs collected in a previous work .

Medical diagnosisTo assess LLMs' continuous improvement in applying expert knowledge, we use the DDXPlus  dataset, where agents must make a medical diagnosis out of 49 diagnoses based on patient profiles detailing their symptoms. This setup mimics how medical doctors improve their diagnostic skills through accumulated patient encounters. Evaluating LLMs on this dataset helps us understand their potential for continuous improvement in a highly specialized domain.

Question answeringQuestion answering (QA) tasks evaluate an agent's ability to reason over supporting facts to answer users' questions. We adopt the distractor setting in HotpotQA , which requires reasoning over multiple supporting or distracting documents to answer questions. This helps measure the agent's improved proficiency in reasoning over grounded knowledge to provide accurate answers. Given the extensive volume of questions, we sampled 1,500 out of the total 7,410 questions.

Detailed information of the aforementioned datasets are provided in Table 1 and Appendix F.

## 4 Experiments

### Baselines

A key objective of StreamBench is to compare the performance gains of LLM agents using _non-streaming_ versus _streaming_ methods. In _non-streaming_ settings, methods focus on optimizing performance at a per-instance level, with improvements made independently for each testing instance. For these _non-streaming_ methods, the overall performance boost on the testing set stems from improvements on individual testing instances. In contrast, _streaming_ methods utilize information from past instances to improve future performance. For _streaming_ methods, we adapt two previously proposed methods and introduce two new approaches to explore effective streaming strategies.

  
**Task** &  & **Python** & **Tool Use** & **Medical** & **QA** \\ 
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\  Input (\(x_{t}\)) &  & Question & User query & Symptoms & Question \\ Output (\(y_{t}\)) &  & Code & API calls & Diagnosis & Answer \\ Metric &  & Pass@1 & Accuracy & Accuracy & Exact Match \\  Test size (\(T\)) & 2,147 & 1,007 & 1,534 & 1,000 & 750 & 1,764 & 1,500 \\   

Table 1: Input, output, evaluation metrics, and number of testing instances of selected datasets.

#### 4.1.1 Non-streaming methods

Zero-shotIt reflects the basic instruction-following abilities of LLMs for solving a given task.

Few-shotIt involves providing several ground truth \((x,y)\) pairs in the prompting template \(p()\). For datasets with training sets, we construct few-shot examples from the training data. For datasets without training sets, we compose few-shot examples and inspect their quality to ensure reliability. We include few-shot examples for each dataset in the supplementary materials for reproducibility.

Chain-of-thought (CoT)Following previous work , we employ a trigger phrase (e.g., "Let's think step by step.") to instruct the LLM to generate the reasoning process before providing its final answer. We then extract the answer in the correct format from the generated reasoning text.

Self-RefineSelf-Refine is a technique where the LLM iteratively improves its output based on self-feedback. The model generates an initial response and refines it through multiple iterations of refinement. It leverages LLMs' ability to self-evaluate and adjust its responses at a per-instance level.

#### 4.1.2 Streaming methods

GrowPromptWe adapt the previously proposed method GrowPrompt , where \((x_{t},_{t},fb_{t})\) of the latest time steps are stored in a sliding window \(\). The contents of \(\) are incorporated into the prompt at inference time to output \(y_{t}=f(p(x_{t},)|)\). This provides the agent with information from the past \(k\) instances, where \(k\) is a hyperparameter. Since LLM agents take text input, we verbalize \(fb_{t}\{0,1\}\) to inform the agent of whether its output \(y_{t}\) correctly satisfies the input \(x_{t}\).

MemPromptAs an advanced version of GrowPrompt, MemPrompt  incorporates an external memory \(\) to store \((x_{t},_{t},fb_{t})\) of all past time points. During inference, a retriever \(r()\) is used to select \(k\) elements from \(\), and \(fb_{t}\) is also verbalized to inform the agent of \(_{t}\)'s correctness.

Self-StreamICLPrevious works [27; 28] have found that incorrect examples can negatively impact in-context learning (ICL) performance, though the extent varies for different LLMs. Based on these insights, we hypothesize that while GrowPrompt and MemPrompt use verbalized \(fb_{t}\) to inform the agent about the correctness of its output, incorrect \(_{t}\) still introduces distracting signals that can hinder improvement. Therefore, we propose to save \((x_{t},_{t})\) pairs to memory \(\)_only when \(fb_{t}=1\)_, eliminating the need to save verbalized \(fb_{t}\). This method, called Self-StreamICL, operates similarly to regular ICL, except that the labels are now self-generated and gradually accumulate over the data stream, without the need to preconstruct few-shot examples. For more details, refer to Algorithm 2.

Multi-Agentic-Memory StreamICIn Self-StreamICL, the agent learns exclusively from its own past experiences. However, we hypothesize that different LLM agents possess distinct strengths and weaknesses, so they can potentially benefit from the experiences of other agents. To explore this idea, we introduce Multi-Agentic-Memory StreamICL (MAM-StreamICL), which employs a multi-agent framework where multiple LLM agents share a common memory. This shared memory incorporates the past outputs of all agents, allowing each agent to learn from the diverse experiences of the others.

We implement a simple round-robin-like scheduling to switch between different LLM agents outlined in Algorithm 2. This ensures that each agent contributes to the shared memory in a balanced manner. Our experiments show that this straightforward strategy can boost performance beyond the average performance of the individual agents. In fact, Self-StreamICL can be seen as a special case of MAM-StreamICL with only one LLM agent.

Note that the high cost associated with scaling is the most critical drawback of multi-agent methods proposed by previous works, and the key advantage of MAM-StreamICL is its cost-effectiveness. Unlike methods such as Multiagent Debate  and RECONCILE , the cost of MAM-StreamICL does not scale proportionally with the number of agents. Instead, _the cost is equivalent to the averaged cost of a single agent, since only one agent is assigned to answer at each time step_.

### Implementation details

We conduct experiments using three LLM families: GPT [31; 32], Gemini [33; 34], and Claude . For our main experiments, we use the endpoints gpt-3.5-turbo-0125, gemini-1.0-pro-001, and Claude-3-haiku-20240307. These models represent cost-effective LLMs, balancing performance and affordability. The models initialize the \(K=3\) agents in MAM-StreamICL. For methods with \(\) (MemPrompt, Self-StreamICL, and MAM-StreamICL), we implement \(\) as a vector database. We use BAAI/bge-base-en-v1.5 to encode \(x_{t}\) as key embeddings and save \(x_{t}\), \(_{t}\) (and \(fb_{t}\) for MemPrompt) as values. For hyperparameters and prompts, refer to Appendix A and F.

### Main results

The main results are shown in Table 2, which lists the averaged performance of the three LLM agents. The only exception is MAM-StreamICL, which only runs once on the streaming sequence where each agent takes turns at each time step. For results of each respective model, refer to Appendix B.

Overall, streaming methods outperform non-streaming methods, though the extent of improvement varies across different datasets. These results demonstrate the value of leveraging input-feedback streams to enhance agent performance on downstream tasks. In addition, as demonstrated by the robust performance of Self-StreamICL compared to GrowPrompt and MemPrompt, leveraging feedback as simple as correctness can enable agents to improve even more through self-generated outputs \(_{t}\). This provides an important insight: rather than solely focusing on prompting pipelines to boost per-instance performance, adopting simple yet effective streaming approaches, such as collecting correctness feedback, could potentially lead to notable improvements on LLM agents. Lastly, MAM-StreamICL shows the most notable and consistent performance boost across all datasets.

  
**Task** &  &  &  &  &  \\ 
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\   & & & & & & \\  Zero-Shot & 67.89 & 50.55 & 29.60 & 37.70 & 61.38 & 52.85 & 48.49 \\ Few-Shot & 68.55 & 50.61 & 30.40 & 33.33 & 68.58 & 60.98 & 53.11 \\ CoT & 61.53 & 46.01 & 27.23 & 25.93 & 58.98 & 58.20 & 52.47 \\ Self-Refine & 67.75 & 49.49 & 29.62 & 36.30 & 60.67 & 52.89 & 43.53 \\   & & & & & & \\  GrowPrompt & 69.90 & 51.97 & 30.35 & 33.77 & 65.07 & 55.10 & 51.38 \\ MemPrompt & 70.78 & 53.29 & 31.99 & 35.47 & 64.31 & 54.02 & 52.62 \\ Self-StreamICL & 74.63 & 55.05 & 35.31 & 41.30 & 71.33 & 70.56 & 54.80 \\ MAM-StreamICL & **75.69** & **55.17** & **36.38** & **43.10** & **75.87** & **83.50** & **55.20** \\   

Table 2: Averaged performance of three LLM agents across different baselines and datasets.

## 5 Discussion

### What makes effective streaming strategies?

This subsection provides insights into the key aspects that contribute to successful streaming strategies. We identify two effective factors for streaming improvements as follows:

#### 5.1.1 Collecting correct self-output

To investigate whether incorrect self-output hinders agents' improvement, we conducted ablation studies on GrowPrompt and MemPrompt. In the default setting in Table 2, both methods use all \(k\) retrieved \((x_{t},_{t},fb_{t})\) triples during inference (use all). In contrast, the ablations either use only the triples where \(fb_{t}=0\) (only incorrect), or use only the triples where \(fb_{t}=1\) (only correct).

The ablation results in Figure 2 reveal several findings. First, using incorrect self-output degrades performance, sometimes even worse than the zero-shot baseline. In contrast, using only correct self-output consistently boosts performance over the zero-shot baseline, with particularly consistent improvements observed in the MemPrompt (only correct) method. An important observation is that, even if \(fb_{t}\) is verbalized to inform the agent whether its \(_{t}\) correctly satisfies \(x_{t}\) in GrowPrompt and MemPrompt, simply informing the agent that its self-output is incorrect does not help it learn from past mistakes. Conversely, telling the LLM agent what it does correctly is very effective in facilitating improvement. These findings underscore the importance of collecting and utilizing correct self-output in streaming. This also explains the intuition and effectiveness behind Self-StreamICL, where input-output pairs are saved to \(\) only when the self-output is correct.

#### 5.1.2 Sharing memory across multiple agents

Another key insight is the benefit of sharing memory across multiple agents, as demonstrated in MAM-StreamICL. To analyze why memory sharing works, we use DDXPlus as an example and visualize the confusion matrices for a subset of diagnoses related to upper respiratory tract diseases.

Figure 3 presents the confusion matrices for three different LLM agents: gpt-3.5-turbo-0125, gemini-1.0-pro, and claude-3-haiku-20240307, along with the matrix of MAM-StreamICL. Each matrix illustrates the proficiency of an agent across various medical diagnosis categories. It is evident that each model excels in certain areas while struggling in others. For instance, gpt-3.5-turbo-0125 shows high accuracy in predicting "acute rhinosinusitis" and "allergic sinusitis" but struggles with "chronic rhinosinusitis" and "URTI". In contrast, gemini-1.0-pro performs well in "URTI", and claude-3-haiku could solve "chronic rhinosinusitis".

The diversity in performance across models suggests that their collective past experiences can provide complementary strengths, thereby enhancing overall performance when these experiences are shared. Since each agent takes turn to solve an incoming \(x_{t}\) at each time point \(t\), the shared memory system allows the agents to benefit from others while maintaining a cost similar to that of a single agent. We also conduct further ablation studies in Appendix D to discuss the importance of sharing memory.

Figure 2: Correctness ablations. The y-axis denotes performance difference from zero-shot. The results are the average of three LLM endpoints. Please refer to Appendix D for results of each LLM.

### Robustness to different streaming sequences

Given the time-variant nature of streaming, evaluating each method's robustness across different data streams is essential. Therefore, we rerun the streaming baselines with 5 random seeds on five tasks. Figure 4 presents the averaged performance and standard errors of claude-3-haiku across 5 shuffled sequences, with results for gpt-3.5-turbo and gemini-1.0-pro provided in Appendix C. The performance ranking of streaming baselines remains mostly consistent across datasets, with Self-StreamICL and MAM-StreamICL being the top performers. Due to the high cost of running all 5 sequences on StreamBench, we select a fixed sequence for fair comparison among future benchmark users. However, we also release all 5 sequences for those who wish to conduct a thorough evaluation.

### Would stronger LLMs still benefit from streaming?

To evaluate whether stronger LLMs still benefit from streaming, we tested two newer models: gpt-4o-2024-08-06 and gemini-1.5-flash-001. Due to the high cost, we only run the methods shown in Table 3. We found that with Self-StreamICL, these stronger models still showed significant performance improvements. This demonstrates that even the most advanced models can leverage the information from streaming data to further enhance their performance across diverse tasks.

### Cost analysis

For benchmark users to estimate the cost, the token usage of all baselines is listed in Appendix E.

Figure 4: Averaged performance and standard errors of each method on five shuffled sequences.

Figure 3: Confusion matrices of the diagnoses subset of upper respiratory tract diseases in DDXPlus.

  
**Task** &  &  &  &  &  \\ 
**Dataset** & **Spider** & **CoSQL** & **BIRD** & **DS-1000** & **ToolBench** & **DDXPlus** & **HotpotQA** \\  _gemini-1.5-flash_ & & & & & & & \\  Zero-shot & 69.63 & 48.26 & 33.83 & 50.20 & 69.47 & 58.90 & 60.60 \\ Few-shot & 71.40 & 49.35 & 37.03 & 50.60 & 72.13 & 73.58 & 64.87 \\ CoT & 72.52 & 52.73 & 35.14 & 44.80 & 68.00 & 64.06 & 63.13 \\ Self-StreamICL & **77.83** & **56.21** & **41.20** & **52.20** & **75.07** & **86.34** & **65.20** \\  _gpt-4o-2024-08-06_ & & & & & & & \\  Zero-shot & 73.54 & 53.33 & 34.42 & 54.90 & 72.40 & 70.64 & 65.53 \\ Few-shot & 76.85 & 57.60 & 36.25 & 52.30 & 71.47 & 83.45 & 66.87 \\ CoT & 72.52 & 54.82 & 31.16 & 41.90 & 66.80 & 73.02 & 62.80 \\ Self-StreamICL & **80.58** & **59.19** & **42.63** & **59.40** & **76.27** & **92.01** & **67.00** \\   

Table 3: Performance of gpt-4o-2024-08-06 and gemini-1.5-flash-001 on StreamBench.

Related work

### Online learning

Online learning  explores the incremental updating of models as new data arrives, making it valuable for dynamic improvement in downstream applications. Traditionally, it focuses updating network weights, such as in methods for training recurrent neural networks , online representation learning for image classification , and adapting language models to learn new world knowledge . Recent advancements have introduced strategies for improving LLM agents by updating prompts [9; 10; 11], memory [6; 15], or retrievers [12; 13; 14]. These new strategies are promising for designing new algorithms to adapt LLMs in the online setting. However, there are no standard testbeds for this setup. Addressing this gap, we propose StreamBench, the first benchmark to pave the way for developing more dynamic adaptation methods for LLM agents.

### Improvement from feedback with LLMs

Recent works have shown that LLMs can improve from feedback when augmented with prompting pipelines or memory mechanisms, forming two main research branches. One is instance-level improvement methods, such as ReAct , Self-ICL , and Self-Refine . These methods focus on boosting performance on each input instance without leveraging information from past instances. The other is time-sequence-level improvement methods. For example, MemPrompt  enhances GPT-3 by storing past user feedback and retrieve them in the future. Reflexion  shows that LLM agents can perform better in future trials by running repeated trials on the same dataset, but this is not always practical in real-world scenarios. ExpeL  shows that LLM agents can benefit from cross-task experience without needing repeated trials on the target task. However, these works use different datasets and lack a standardized evaluation setting. StreamBench bridges this gap by providing a consistent empirical testbed across diverse tasks to evaluate LLM agents' improvement.

## 7 Conclusion

In this work, we introduce a new evaluation setting to measure LLM agents' performance improvement on downstream tasks, and propose StreamBench as an instance of this setting. There are two major findings in our experiments. Firstly, collecting correct self-generated outputs improve performance consistently, while informing agents of their incorrect outputs sometimes degrade performance. Secondly, sharing memory across multiple agents is a promising cost-effective technique, as MAM-StreamICL achieves robust performance while maintaining the average cost of a single agent.

StreamBench serves as a stepping stone towards more adaptive AI systems. Future directions include exploring _online active learning_ where agents could inquire feedback only when necessary, or viewing multi-agent collaboration as multi-arm bandits (MABs) to develop more sophisticated methods for selecting agents and sharing memory at each time point. It is also practical to investigate the utilization of different feedback signals beyond correctness, such as users' natural language feedback. We hope that this work inspires development of adaptive methodology for improving LLM agents.

## 8 Limitations

Tasks and modality coverageThe current version of StreamBench includes tasks such as programming, text-to-SQL conversion, medical diagnosis, question-answering, and tool use. While diverse, they do not encompass all possible types of tasks or domains where LLMs can be applied. StreamBench is also limited to text and does not cover other modalities such as image and audio.

Sim2Real gapAlthough we have attempted to simulate feedback signals as practical as possible, there may still be a gap between the simulated correctness feedback in StreamBench and the feedback encountered in real-world applications. Real-world feedback can be more diverse, noisy, and context-dependent, which may not be fully captured by the current benchmark.