# No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning

No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning

Zixing Song

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

zxsong@cse.cuhk.edu.hk

&Yifei Zhang

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

yfzhang@cse.cuhk.edu.hk

&Irwin King

The Chinese University of Hong Kong

New Territories, Hong Kong SAR

king@cse.cuhk.edu.hk

###### Abstract

Graph Neural Networks (GNNs) are crucial for machine learning applications with graph-structured data, but their success depends on sufficient labeled data. We present a novel active learning (AL) method for GNNs, extending the Expected Model Change Maximization (EMCM) principle with more prediction performance on unlabeled data. By presenting a Bayesian interpretation for the node embeddings generated by GNNs under the semi-supervised setting, we efficiently compute the closed-form EMCM acquisition function as the selection criterion for AL without re-training. Our method establishes a direct connection with expected prediction error minimization, offering theoretical guarantees for AL performance. Experiments demonstrate our method's effectiveness compared to existing approaches, in terms of both accuracy and efficiency.

## 1 Introduction

Graph Neural Networks (GNNs) have gained significant recognition in machine learning applications, particularly for graph-structured data [71; 32; 68]. Nevertheless, their efficacy is predominantly contingent on the availability of ample labeled data. The labeling process, often demanding human intervention and domain knowledge, poses high costs in real-world applications. Active Learning (AL) presents a compelling solution to mitigate the issue of limited labeled data . AL boosts the performance of passive learning by iteratively training a model based on the current set of labeled nodes and selecting the query nodes to expand this set based on the different query heuristics or the designed acquisition function. The main idea is to identify the most informative nodes for the oracle to annotate, thereby enhancing test node predictions upon their addition to the training set.

Recently, active learning methods for graph-structured data have seen an upsurge, adapting general active learning techniques to accommodate the non-IID nature of graphs [88; 89; 90]. Predominantly, these methods employ heuristic approaches, selecting nodes based on rudimentary measures such as uncertainty and influence score. While efficient, these strategies lack a direct correlation with expected prediction performance on remaining unlabeled nodes--our primary concern. Hence, We advocate for performance-based active learning methods, selecting nodes that directly optimize anticipated model performance .To balance efficiency, we employ the Expected Model Change Maximization(EMCM) algorithm [6; 7] from general active learning. EMCM chooses nodes maximizing expected model change in parameters or predictions, serving as a surrogate for expected prediction error.

Implementing the EMCM principle in GNNs is both challenging and meaningful. First, the acquisition function for EMCM calculates the expected model change following the addition of a single labeled node, without the need for re-training. Hence, a Bayesian probabilistic model is preferred in AL . This approach allows for efficient updating of posterior beliefs about the model's parameters or predictions upon introducing a new labeled node, aligning perfectly with performance-based methods like EMCM. However, conventional GNNs lack Bayesian interpretation, while Bayesian GNNs impose significant computational demands, rendering precise approximation of the expected model change without re-training practically impossible. Therefore, a lightweight Bayesian probabilistic learning framework could enable the application of EMCM on graphs. Second, the direct adaptation of EMCM for GNNs, despite its computational advantages over other performance-based methods, remains impractical due to the large number of nodes, which is common in real-world cases. Third, a deep connection can be established between the EMCM method on graphs and the expected prediction error, offering a theoretical guarantee regarding the potentially optimal AL performance.

In light of these insights, we are the first to extend the EMCM principle to GNNs, contributing in the following ways. First, we revisit the training process of the Simplified Graph Convolution (SGC) model  for the semi-supervised node classification as an example to obtain an equivalent view of a bi-level optimization problem. This motivates us to propose a regularized single-level optimization learning framework that possesses a clear Bayesian interpretation. Second, we derive the posterior mean and variance of the node embeddings projected onto the truncated graph spectral subspace via the Laplace approximation. This enables the efficient computation of the acquisition function of EMCM for graphs through a closed-form solution. Third, we provide theoretical insights into the proposed method, essentially equating to the selection of the node that directly minimizes the expected prediction errors of the remaining unlabeled nodes, assuming its addition to the training set. This theoretical interpretation aligns with the ultimate goal of AL. Fourth, We perform comprehensive experiments across several datasets, demonstrating the efficacy and efficiency of our proposed method.

## 2 Preliminary

### Problem Formulation

We consider a graph \(=(,)\) with \(||=n\) nodes. Node feature vectors are represented as \(_{i}^{d}\), aggregated into a node feature matrix \(=[_{1}^{},,_{n}^{}]^{n d}\). The adjacency matrix of \(\) is given as \(\{0,1\}^{n n}\) (\(^{n n}\) if the graph is weighted), aligned with edge set \(\). Assume there exists a labeling oracle that can map each node \(i\) (\(i=\{1,,n\}\)) to its ground-truth one-hot label vector \(_{i}\{0,1\}^{c}\). \(c\) is the number of classes. The node set \(\) is partitioned into the labeled node set \(_{l}\) (\(|_{l}|=n_{l}\)) and the unlabeled node set \(_{u}\) (\(|_{u}|=n_{u}\)). At first, only the ground-truth labels of labeled nodes \(\{_{i}\}_{i_{l}}\) are revealed to the learning model.

We investigate the pool-based active learning setting on the aforementioned graph (Appendix (D.1)). The learning model initially undergoes standard training on the labeled set \(_{l}\), \(\{_{i}\}_{i_{l}}\). Subsequently, it identifies a query node set \(_{q}\) from \(_{u}\) (\(_{q}_{u}\)) and solicits the labeling oracle for associated labels. Following this, the labeled and unlabeled sets are updated (\(_{l}_{l}_{q}\), \(_{u}_{u}_{q}\)). This training cycle and query node selection process repeats until the labeling budget \(B\) is depleted (\(|_{l}|=n_{l}+B\)). We distinguish between sequential active learning, where one node is selected to query the oracle (\(|_{q}|=1\)), and batch active learning, where a batch of \(b\) nodes is chosen (\(|_{q}|=b^{+}\)). The primary objective is to deduce the soft labels of nodes within the unlabeled set \(\{}_{i}\}_{i_{u}}\) (\(}_{i}^{c}\)), utilizing the labeling oracle under a total labeling budget \(B\).

More specifically, in each query node selection step under the transductive setting, we aim to solve Eq. (1) to choose the most informative node(s) and minimize the expected prediction error on the remaining unlabeled nodes. Here, \(^{c}^{c}^{+}\) signifies the selected loss function.

\[*{arg\,min}_{_{q}:|_{q}|=b}_{i _{u}_{q}}[(}_{i},_{i})]\] (1)

### Graph Neural Networks for Semi-supervised Node Classification

Graph Neural Networks (GNNs) excel in learning representations for graph-structured data [71; 62; 10; 57; 40; 95; 43; 92; 11; 61; 94; 59; 82; 60; 58; 39; 93; 9]. Among them, the Graph Convolutional Networks (GCN)  model is the most representative method. Considering the GCN or other GNN models for the node classification task under the semi-supervised setting, there are two main steps during one iteration: (1) Forward pass that fuses both node features \(\) and structure information \(\) into the low-dimensional representation or node embeddings \(()^{n c}\) (before the softmax function) with the training parameters \(\); and (2) Backward pass that updates \(\) through gradient decent according to the training loss function over all labeled nodes as \(_{}_{i_{l}}((_{i }()),_{i})\). Here, \(_{i}()\) denotes the \(i\)-th row of \(()\). Typically, \((,)\) is set as the cross-entropy loss after applying the Softmax function \(()\) on node embedding \(_{i}()\). Different GNNs mainly have different designs in the forward pass, but the backward pass remains the same in general. For a \(K\)-layer GCN, we optimize the training loss such that \(()=}((}( (}^{(0)}))^{( K-2)}))^{(K-1)}\). Here \(}=+\) represents the adjacency matrix with the added self-loop and \(}=+\) with the diagonal degree matrix \(=(d_{1},,d_{n})\) where \(d_{j}=_{j}_{i,j}\). The normalized adjacency matrix is \(}}=}^{-1/2}}}^{-1/2}\). We denote \(=\{^{(i)}\}_{i=0}^{K-1}\). Note that GCN conducts linear transformation and non-linearity activation \(()\) repeatedly. SGD  reduces this excess complexity by removing non-linearities and collapsing training parameters \(\) between consecutive layers. Similarly, a \(K\)-layer SGC also optimizes the training loss such that the node embedding matrix \(()\) is fixed as \(()=}}^{K}\).

### General Active Learning and Active Learning on Graphs

Active Learning (AL)  selects the most "informative" samples for querying, falling into three categories: uncertainty-based, representativeness-based, and performance-based methods. Uncertainty-based methods target the most uncertain instances [83; 101; 50], using criteria like entropy and margin. Techniques like Query by Disagreement (QBD) and Query by Committee (QBC) [56; 16; 52] also reside here, employing uncertainty sampling to reduce version space. Representativeness-based methods select samples best representing the input distribution, using methods such as density-weighting [14; 46; 27; 28] and clustering [49; 13; 72]. Performance-based methods directly optimize informativeness via Eq. (1) or surrogates, considering the impact of revealing an instance's label on future outcomes. Techniques like Expected Error Reduction (EER) , Expected Variance Reduction (EVR) , and Expected Model Change Maximization (EMCM)  are examples.

There is a recent surging trend to explore AL strategies on graph-structured data. It is unsuitable to directly apply general AL techniques to GNNs since the nodes are not i.i.d. but linked with edges such that connected nodes tend to have the same label. AGE  and ANRMAB  incorporate node embedding density and PageRank centrality into their node selection. GEEM  adapts the Expected Error Reduction (EER) method for graphs, while RIM  accounts for noisy oracles in node labeling. Furthermore, ALG  optimizes the effectiveness of all nodes influenced by GNNs, and IGP  maximizes information gain propagation. Detailed reviews are found in Appendix B.

### Motivations and Challenges

#### 2.4.1 Bayesian Probabilistic Interpretations

Active learning strategies for graphs often directly utilize GCN or other GNN models as their backbone [4; 88; 89; 90; 37], capitalizing on their exemplary representation learning abilities. However, under semi-supervised settings, these models typically lack a Bayesian probabilistic interpretation, a fundamental aspect of AL that quantifies uncertainty for uncertainty-based methods and lays the groundwork for performance-based methods, which incorporate prior knowledge and update the posterior beliefs about the model parameters or predictions, assuming if new labeled data becomes available. Relying on GCN or GNN models without Bayesian interpretations may lead to inefficient sample selection and hinder learning progress. Therefore, integrating a Bayesian probabilistic interpretation within the GNN framework is a desirable enhancement to active learning methodologies. Existing Bayesian GNNs provide well-defined Bayesian interpretations [96; 24], yet they often come with significant computational costs. This renders them impractical for active learning environments,where model re-training is imperative after each query set selection. To address these challenges, we aim to introduce a lightweight, general Bayesian learning framework for GNN models. This will facilitate subsequent query node set selection by enabling rigorous, informed decision-making .

#### 2.4.2 Efficient Computation

While numerous existing graph active learning methods extend uncertainty-based or representativeness-based methods for their simplicity and efficiency (Table 6 in Appendix B.2), these approaches can be sensitive to outliers and largely reliant on prerequisite density estimation functions or clustering algorithms. We instead advocate for performance-based methods, which directly examine the potential impact on the model should an unlabeled node's label be revealed. Well-established methods such as EER  and EVR  and their graph-based adaptations  are notorious for their extremely high computational overhead. As an alternative, we concentrate on the EMCM concept for AL on graphs. This approach selects nodes that yield the maximum expected model change, striking a balance between theoretical interpretations and computational costs. Our proposed general Bayesian learning framework for GNN models enables efficient computation of expected model changes using closed-form solutions. For graphs with an exceptionally large number of nodes, we further employ a spectral approximation technique to curtail computational costs.

#### 2.4.3 Solid Theoretical Guarantees

A significant limitation of most current AL techniques for graphs is the absence of theoretical connections with the expected prediction errors in Eq. (1), the ultimate goal in AL. CSAL  offers an in-depth theoretical analysis of label complexity within the context of AL on graphs, but it only verifies the existence of such an algorithm without presenting a concrete implementation. This gap between theory and practice hinders the direct applicability of CSAL. To deploy AL algorithms in real-world situations, it is crucial to devise methods rooted in rigorous theoretical foundations that address this limitation, thereby enabling more efficient learning with fewer labeled examples.

## 3 Methodology

Our starting point is to propose a Bayesian probabilistic learning framework for semi-supervised node classification task based on GNNs. Instead of following the existing literature [88; 90; 37] that use GNNs directly as the backbones for graph AL, our Bayesian probabilistic learning framework naturally supports the dynamic process of AL and allows for seamless updates of the model as new labeled data become available, thus eliminating the need for full model re-training during node selection. This key advantage of our Bayesian probabilistic learning framework paves the way for efficient and interpretable design of our subsequent acquisition function. Following the principle of Expected Model Change Maximization (EMCM)  in general AL, we can adjust the likelihood and efficiently update posterior beliefs about the expected change in model parameters, assuming the label of a candidate node from the pool becomes available. More discussions are found in Appendix C.

### A Bayesian Probabilistic Learning Framework for GNNs under Semi-supervised Setting

#### 3.1.1 Optimization View of SGC

We choose the SGC model  as an example for derivation and present a Bayesian probabilistic learning framework for the semi-supervised node classification. Recall that in Sec. 2.2, SGC aims to solve the following optimization problem Eq. (2) for semi-supervised node classification.

\[_{}_{i_{i}}((_{i}( )),_{i}),()=}}^{K}.\] (2)

The objective function in Eq. (2) characterizes the backpropagation process, while the constraint in Eq. (2) describes forward propagation. Different GNNs typically exhibit unique forms of node embeddings \(()\) in Eq. (2). Notably, several recent studies [42; 102] have focused on unifying the forward pass of GNNs within an optimization framework, but these approaches neglect the backward pass for GNN training. Some research works integrate label information into the unifying framework for GNNs [84; 74], yet none of them provide clear Bayesian interpretations. However, we can still transform the constraint in Eq. (2) into another optimization subproblem  based on Theorem 3.1.

**Theorem 3.1**.: _The forward pass of a \(K\)-layer SGC, \(()=}^{K}\), optimizes the following problem by performing \(K\) steps of gradient descent:_

\[}^{*}=*{arg\,min}_{} {Tr}(^{}}), =}.\]

_Here, \(}\) is initialized as \(}^{(0)}=\) and \(}=-}}\) is the normalized Laplacian matrix. Then,_

\[()=}}^{K}=}^{*}\]

Therefore, Eq. (2) now becomes a bi-level optimization problem as Eq. (3).

\[_{}_{i_{l}}((_{i }()),_{i}),( )=}^{*},}^{ *}=*{arg\,min}_{,=,}^{(0)}=}( ^{}}).\] (3)

#### 3.1.2 From Optimization View to Bayesian Probabilistic View

Although Eq. (3) is an equivalent optimization view of SGC for semi-supervised node classification, it does not possess a clear Bayesian interpretation. The upper-level objective in Eq. (3) minimizes the supervision loss, and the lower-level constraint \((^{}})\) minimizes the graph regularization loss, promoting the homophily assumption that connected nodes share the similar embeddings/labels. Therefore, motivated by , we absorb the lower-level constraint into the objective function itself as a relaxed single-level optimization problem in Eq. (4) by minimizing both two terms simultaneously.

\[_{()=}=_ {i_{l}}((_{i}()), _{i})+(()^{ }}_{}()),\] (4)

where \(}_{}=}+\) with \(\) as the balancing factor between two terms. The proposed optimization problem Eq. (4) immediately leads to a Bayesian probabilistic interpretation . For notational simplicity, we focus on the detailed derivation of the binary case in the main body. In the special case of binary node classification, \(()^{n c}\) can be reduced to \(()^{n}\), and \(()\) now becomes the Sigmoid function \(()\) in Eq. (5).

\[_{\\ ()=}=_{i_{l}}((u_{i}()),y_{i})+ (()^{}}_{}()),\] (5)

Note that now \(u_{i}()\) and \(y_{i}\{0,1\}\). Then Theorem 3.2 explicitly reveals the Bayesian probabilistic interpretation of the optimization framework in Eq. (5) .

**Theorem 3.2**.: _Solving Eq. (5) is equivalent to finding the maximum a posteriori (MAP) estimate of the posterior probability distribution with the density \((())\) as_

\[(())(( ))(-_{}(())).\] (6)

_Here, the prior \((())\) follows a Gaussian prior \((,}_{}^{-1})\) and the likelihood \((-_{}(()))\) is defined by the likelihood potential \(_{}(())_{i_{l}} ((u_{i}()),y_{i})\). \(=[y_{i}]_{i_{l}}\{0,1\}^{n_{l}}\)._

The Gaussian prior denotes a prior belief over the distribution of the node embedding \(()\) governed by the graph structure or captured by the Laplacian matrix in the covariance term. It relates to the second trace term in Eq. (5) or the forward pass of SGC. The likelihood \(((()))\) represents the underlying assumptions about how the observed labels \(y_{i}\) are generated, determined by \((,)\). It relates to the first supervision loss term in Eq. (5) or the backward pass for training SGC.

Theorem 3.2 provides a clear Bayesian interpretation of SGC for semi-supervised node classification. The forward pass defines a prior over node embeddings parametrized by \(\), while the backward pass specifies the likelihood based on the supervision loss function. Together, they yield the posterior, guiding the update of node embeddings after observing limited labels. This insight enables the utilization of the EMCM method , which selects nodes leading maximum model change. For efficiency in query node selection, we focus on SGC in this work, despite more complex GNNs like GCN yielding a non-Gaussian prior in Eq. (6). The analysis of more complex GNNs, such as GCN, yields a non-Gaussian prior while keeping all other aspects unchanged in Theorem 3.2. However, for computational efficiency in subsequent query node selection, we adhere to the SGC model for now.

### Expected Model Change Maximization for Graph Active Learning

#### 3.2.1 Challenges for Applying EMCM on Graphs

We choose the EMCM method  for three main reasons. First, it is a performance-based approach that directly assesses the expected impact on the model if a sample is selected for a query, aligning perfectly with the goal of AL. Second, it offers lower computational costs compared to other performance-based methods like EER. Third, it is ideally supported by our proposed Bayesian probabilistic learning framework which provides an efficient way to estimate the expected model change without re-training. In the sequential active learning setting, our objective is to select the most informative node \(k^{*}=_{k_{u}}(k)\) with a designed acquisition function \(()\). Similar to other performance-based methods, EMCM utilizes a _look-ahead_ model  with the modified objective as \(^{k,y_{k}^{+}}=+((u_{k}()),y_{k}^{ +})\) with \(\) from Eq. (5), where we add the unlabeled node \(k_{u}\) with the _hypothetical label_\(y_{k}^{+}\{0,1\}\) due to the unavailability of the true label \(y_{k}\). We select the node that could maximally change the node embeddings if it is added to the query set, via

\[k^{*}=_{k_{u}}(k)=_{k_{ u}}_{y_{k}^{+}\{0,1\}}(y_{k}^{+} k)\|(}^{*})-(^{*})\|_{2}.\] (7)

Here, \(}^{*}=^{k,y_{k}^{+}}\), \(^{*}=\) and \((y_{k}^{+} k)\) is the predicted probability of label \(y_{k}^{+}\) for node \(k\) estimated by the current model. The high-level motivation of Eq. (7) is that the generalization error changes only when the current model is updated. Nodes that do not update the node embeddings are useless for AL. Nodes causing significant model changes in terms of node embeddings are expected to lead to faster convergence to the optimal model. More detailed illustrations of EMCM can be found in Appendices B.3 and C.2.1.

Re-training the look-ahead model for the fixed node \(k\) in Eq. (7) to obtain the exact solution is computationally infeasible due to the large number of unlabeled nodes. However, the proposed equivalent Bayesian learning framework Eq. (6) offers an efficient alternative. By utilizing rank-one updates of the current model's posterior mean and covariance (Eq. (5)), the posterior mean and covariance of the look-ahead model can be computed efficiently without the need for re-training. It significantly reduces the computational cost involved in applying the EMCM method on graphs.

#### 3.2.2 Truncated Spectral Projection and Laplace Approximation for Efficiency

The computation of \((^{*})\) or \((}^{*})\) through the Bayesian learning method in Eq. (6) still requires storing a large covariance matrix \(}_{}^{-1}^{n n}\). Motivated by spectral clustering , we utilize the first \(m\) (\(m n\)) smallest eigenvalues of \(}\) and their corresponding eigenvectors that contain important geometric information of the graph. Namely, we introduce \(_{}=(_{1}+,, _{m}+)\) and \(=[^{1},,^{m}]^{n m}\), where \(^{i}\) is the eigenvector corresponding to the \(i\)-th eigenvalue \(_{i}\). We can now project the node embedding \(()\) onto the space spanned by these \(m\) eigenvectors by \(=^{}()\). Based on the orthogonality of \(\) (\(^{}=\)), we can easily convert Eq. (5) as,

\[_{}}=_{i_{i}}(( _{i}^{}),y_{i})+( ^{}_{}).\] (8)

Eq. (8) now restricts the model latent space from \(^{n}\) to \(^{m}\), speeding up the model training and reducing the spatial complexity. A similar Bayesian interpretation of Eq. (8) regarding \(^{m}\) can be trivially extended from Theorem 3.2. We term this technique as truncated spectral projection.

To further reduce the computational overhead, we apply Laplace approximation on the posterior distribution of \(()\) (instead of \((())\)) corresponding to Eq. (6). The key idea is to approximate the non-Gaussian posterior via suitable Gaussian distributions here so as to obtain the analytical closed-form of the look-ahead model's posterior mean and covariance later .

**Theorem 3.3**.: _The Laplace approximation of the posterior distribution regarding \(=^{}()^{m}\) according to Eq. (8) and Eq. (6) is given as follows._

\[(},}_{ }}),}=*{arg\,min}_{} },}_{}}=(_{ }+^{}(_{i_{i}}F^{}((_{i}^{}}),y_{i})_{i}_{i}^{}))^{-1}.\] (9)

_Here, we define \(F^{}(x,y):=}{ x^{2}}(x,y)\). \(_{i}\) represents the \(i\)-th standard basis vector._Theorem 3.3 gives a closed-form approximated Gaussian posterior distribution of \(\). Note that \(}=^{}(^{*})\) denotes the _current_ model's \(}\) MAP estimator in Eq. (8) based on Theorem 3.2.

#### 3.2.3 Expected Model Change Acquisition Function

Following the definition of \(^{k,y_{k}^{+}}\) over \(\), the look-ahead model now optimizes \(}^{k,y_{k}^{+}}=}+((_{ i}^{}),y_{k}^{+})\) with \(}\) in Eq. (8). We use \(}^{k,y_{k}^{+}}\) to denote the look-ahead model's \(}^{k,y_{k}^{+}}\) MAP estimator. The exact value of \(}^{k,y_{k}^{+}}\) can be obtained by re-training the look-ahead model. We instead compute an approximation of \(}^{k,y_{k}^{+}}\) by one single step of Newton's method on the objective \(}^{k,y_{k}^{+}}\) without re-training, given by Theorem 3.4. Since the look-ahead model is built upon the current model, using a single step of Newton's Method can quickly approximate the solution for \(}^{k,y_{k}^{+}}\) based on the local information (gradient and curvature) around the current model's MAP estimator .

**Theorem 3.4**.: _The look-ahead MAP estimator \(}^{k,y_{k}^{+}}\) can be obtained by performing one step of Newton's method on \(}^{k,y_{k}^{+}}\) from the current MAP estimator \(}\) as_

\[}^{k,y_{k}^{+}}=}-_{ k}^{}}),y_{k}^{+})}{1+F^{}(( _{k}^{}}),y_{k}^{+})(_ {k}^{})}_{}}(_ {k}^{})^{}}\] (10)

_We define \(F(x,y)(x,y)\), \(F^{}(x,y)}{ x^{2}}(x,y)\). \(_{i}\) represents the \(i\)-th standard basis vector._

Now, we finally come back to the original acquisition function for applying EMCM on graphs in Eq. (7). The predicted probability of label \(y_{k}^{+}\) can be set as the soft label prediction for node \(k\) by the current model \((y_{k}^{+}=1 k)=(_{k}^{}( {}))\) (\((y_{k}^{+}=0 k)=1-(_{k}^{}( ))\)). Thanks to the truncated spectral projection (\(}=^{}(^{*}),}^{k,y_{k}^{+}}=^{}(}^{* })\)) and Laplace approximation, we can now measure the change of \(\|(}^{*})-(^{*})\|_{2}\) via \(\|}^{k,y_{k}^{+}}-}\|_{2}\) due to the orthogonal invariance of \(l_{2}\) norm. Based on Theorem 3.4 and Eq. (7), the final designed acquisition function is

\[(k)=_{y_{k}^{+}\{0,1\}}(y_{k}^{+}=1|k)^{y_{k}^{+}} (y_{k}^{+}=0|k)^{1-y_{k}^{+}}|_{k}^{ }}),y_{k}^{+})}{1+F^{}((_{k}^ {}}),y_{k}^{+})_{k}^{}}_{}}_{k}}|\|}_{}} _{k}\|_{2}.\] (11)

Here, \((y_{k}^{+}=1|k)=(_{k}^{}())\) and define \(_{k}(_{k}^{})^{}\) as the \(k\)-th row vector of \(\). The proposed active learning algorithm expecet**D mOdel** Change maximiza**T**ion **O**g**Raphs (DOCTOR) in the sequential setting is presented in Algorithm 2 in Appendix D.2. For the batch active learning setting and the multi-class classification setting, we refer to Appendices D.3 and D.4.

### Theoretical Insights of the Proposed Method DOCTOR

We analyze the profound connection between the proposed EMCM method and the Expected Error Minimization (EEM) or EER method for graph active learning. We will show that the designed acquisition function Eq. (11) will be reduced to the one for EEM under some assumptions and thus it has a direct connection to the ultimate goal for graph AL in Eq. (1). For clarity, we focus on the fundamental regression task within the sequential AL setting. We choose the regression for theoretical analysis to get rid of the non-linear activation functions in the Bayesian learning framework (\(()\) in Eq. (5)) from the beginning. We also fix the loss function \((,)\) as the common squared error loss.

**Theorem 3.5**.: _We assume \((x,y)=(x-y)^{2}\) is used for the regression task in the sequential active learning setting (b=1). In the \(t\)-th iteration step, the proposed DOCTOR algorithm aims to solve_

\[_{k_{u}^{(t)}}(k)=_{k_{u}^{(t)}} _{k}^{}}_{}}}\| }_{}}_{k}\|_{2}^{2}.\] (12)

_And the Expected Error Minimization on graphs aims to solve \(_{k_{u}^{(t)}}_{i_{u}^{(t)}\{k \}}[(_{i},y_{i})]\). Then,_

\[*{arg\,min}_{k_{u}^{(t)}}_{i_ {u}^{(t)}\{k\}}[(_{i},y_{i})]=*{arg\,max}_{k _{u}^{(t)}}_{k}^{}}_{ }}}\|}_{}}_{k}\|_{2}^{2}.\] (13)Theorem 3.5 demonstrates the equivalence between the proposed DOCTOR algorithm and the EEM method on graphs, given certain basic assumptions. Consequently, from a theoretical standpoint, DOCTOR can implicitly select the node that minimizes the expected prediction error if added into the training set during the next iteration, as directly correlated with the initial objective in Eq. (1). Moreover, DOCTOR offers significantly reduced computational overhead through truncated spectral projection and Laplace approximation, as compared to EEM-based approaches like GEEM .

## 4 Experiments

We now verify the effectiveness of DOCTOR on five real-world graphs. We aim to evaluate DOCTOR in terms of prediction accuracy, efficiency, generalization ability, and interpretability (Appendix G).

### Experimental Setup

DatasetsWe evaluate DOCTOR for the pool-based active learning setting on five datasets, including three citation networks (i.e., **Citeseer**, **Cora** and **PubMed**) , one social network (**Reddit**) , and one large-scale OGB dataset (**ogbn-arxiv**) . Details can be found in Appendix F.1.

BaselinesWe compare DOCTOR with six state-of-the-art baseline methods for active learning on graphs. **Random** selects the nodes to query randomly. **AGE** Combines different query strategies linearly with time-sensitive parameters. **ALG** select nodes that maximize the effective reception field in GCN. **RIM** further considers the influence quality during node selection. **IGP** choose nodes that maximize information gain propagation. **GraphPart** selects representative nodes within each graph partition to query. Implementation details are referred to in Appendix F.2.

### Experimental Results

Accuracy ComparisonWe compare DOCTOR with baselines for multi-class node classification in the batch AL setting (Appendix D.4). We choose a small set of two randomly sampled labeled nodes for each method as the initial pool. The query node size is fixed as \(b=5\) in each iteration and the labeling budget is fixed as 20 labels per class (\(B=20c\)). The backbone is set as the SGC model. Table 1 shows the classification accuracy on five datasets with the same labeling budget. We repeat each method 10 times and report the mean and variance regarding the accuracy. Remarkably, the proposed DOCTOR achieves the best in all the cases and improves the second-best baseline by nearly 1% on some datasets. The improvement on the Reddit dataset is rather marginal since it is designed for the inductive setting and DOCTOR is originally proposed for the transductive setting.

To show the influence of the labeling budget, we display the accuracy of different AL methods under different labeling budgets on three citation datasets in Figure 2. We range the labeling budgets \(B\) from \(2c\) to \(20c\) with \(c\) as the number of classes. The results in Figure 2 demonstrate that with the increase in labeling budget, the accuracy of DOCTOR grows as well, outperforming many of the baselines with a greater margin.

Efficiency ComparisonTo evaluate the efficiency of DOCTOR, we conducted an analysis of its running time per AL iteration alongside other baselines using the Cora dataset. We introduce another performance-based graph AL method called GEEM , which extends the EEM principle

  
**Method** & **Cora** & **Citeseer** & **PubMed** & **Reddit** & **ogbn-arxiv** \\  Random & 78.9\(\)0.9 & 70.7\(\)0.7 & 78.4\(\)0.5 & 91.3\(\)0.5 & 68.3\(\)0.4 \\ AGE & 82.5\(\)0.5 & 71.4\(\)0.5 & 79.4\(\)0.7 & 91.5\(\)0.4 & 68.7\(\)0.3 \\ ALG & 82.6\(\)0.6 & 73.6\(\)0.6 & 80.8\(\)0.3 & 92.4\(\)0.6 & 70.0\(\)0.2 \\ RIM & 84.1\(\)0.8 & 73.2\(\)0.7 & 80.2\(\)0.4 & 92.0\(\)0.7 & 70.5\(\)0.8 \\ IGP & 86.3\(\)0.7 & 75.8\(\)0.4 & 83.5\(\)0.5 & 93.3\(\)0.4 & 70.9\(\)0.5 \\ GraphPart & 86.5\(\)1.2 & 74.0\(\)2.0 & 81.5\(\)1.6 & 92.7\(\)1.0 & 72.3\(\)2.1 \\  DOCTOR & **86.9\(\)0.7** & **76.5\(\)0.8** & **84.3\(\)0.9** & **93.5\(\)0.5** & **73.0\(\)1.2** \\   

Table 1: Test accuracy (%) on five datasets with the same labeling budget (20 labels per class).

Figure 1: Running Time Comparison (at log scale).

for graphs. However, GEEM's scalability is extremely limited on large-scale datasets. We consider the training time of GEEM as the baseline and measure the relative running time of each method. Figure 1 illustrates these results, where the numbers atop each bar represent the multiplier improvement in running time compared to GEEM. Our method, DOCTOR, exhibits significant efficiency improvements, surpassing GEEM dozens of times while achieving comparable results to ALG. The more detailed time complexity of the proposed DOCTOR method can be found in Appendix G.1.

Generalization to Other GNNsNote that the acquisition function in the proposed DOCTOR algorithm is specifically designed for the SGC method . Nonetheless, we can still substitute the backbone training model, SGC, with other GNNs in each iteration of the AL loop (line 5 in Algorithm 2) without any difficulties. The resulting acquisition function will be computed based on the new node embeddings generated by other GNNs without any other changes. We select some common GNNs, including GCN , APPNP  and GraphSAGE , to verify the generalization ability of the proposed acquisition function. We change the backbone model of other baselines accordingly as well. We fix the labeling budget as 20 labels per class. All experiments are conducted in the transductive AL setting on the Citeseer dataset and the results are summarized in Table 2. Like other baselines, the proposed acquisition function in the proposed DOCTOR method generalizes well to other GNN backbone models without any significant changes regarding prediction accuracy, even though it is originally designed for SGC. The underlying reason is that apart from SGC, most other GNNs can also be incorporated into the proposed Bayesian learning framework with more complex forms of prior and the currently derived form of the acquisition function can approximate the exact solution when other GNNs are used as backbones.

InterpretabilityWe perform a case study on a single iteration step of DOCTOR in the sequential AL setting using the Cora dataset. We choose one query node from a pool of 10 randomly sampled unlabeled nodes for visualization purposes. We focus on one iteration step in the exact middle of the entire AL process. We compare the approximate EMC, computed based on Eq. (11) in the DOCTOR method, with the exact EMC, computed based on Eq. (7) by adding the node's label for re-training. We use the maximum value of the exact EMC among these 10 nodes as the baseline, and we report all other EMC values in a relative manner in Figure 3. The results show that our efficient computation of the designed acquisition function via truncated spectral projection and Laplace approximation approximates the exact solution quite well. Additionally, we sort the candidate nodes in descending

  Method & SGC & GCN & APPNP & GraphSAGE \\  Random & 70.7\(\)0.7 & 70.8\(\)0.8 & 71.0\(\)0.6 & 70.6\(\)0.8 \\ AGE & 71.4\(\)0.5 & 71.6\(\)0.6 & 71.3\(\)0.5 & 71.4\(\)0.7 \\ ALG & 73.6\(\)0.6 & 73.8\(\)0.5 & 74.1\(\)0.7 & 74.0\(\)0.9 \\ RIM & 73.2\(\)0.7 & 73.0\(\)0.8 & 73.3\(\)0.8 & 73.2\(\)0.9 \\ IGP & 75.8\(\)0.4 & 75.4\(\)0.9 & 75.5\(\)0.6 & 75.6\(\)1.0 \\ GraphPart & 74.0\(\)2.0 & 74.3\(\)1.6 & 74.5\(\)1.3 & 74.9\(\)1.5 \\  DOCTOR & **76.5\(\)0.8** & **76.2\(\)0.7** & **76.6\(\)0.9** & **76.4\(\)0.9** \\  

Table 2: Test accuracy (%) of different methods with different backbone GNNs on the Citeseer dataset.

Figure 3: Interpretation Analysis.

Figure 2: Test accuracy with different labeling budgets on three datasets.

order based on the exact EMC value and obtain the respective test error by re-training the model with the node's label. Figure 3 illustrates that by choosing the node that has the maximum approximate EMC value without re-training via the proposed acquisition function Eq. (11), we can minimize the test error on the remaining unlabeled nodes by adding this node to the labeled node set for the next round of training. This key observation empirically validates the insights of Theorem 3.5.

## 5 Conclusion

In this work, we extend the EMCM principle for GNNs based on a provided Bayesian learning framework. This allows us to efficiently compute a closed-form acquisition function that can be used to select the most informative nodes to label. The proposed DOCTOR method establishes theoretical connections with the ultimate goal of minimizing expected prediction error in AL. This makes it a promising tool for training GNNs in real-world applications with limited labeled data.

## 6 Limitations

The acquisition function in DOCTOR is derived from the SGC model but has good generalizability to other GNNs empirically. We leave the investigation of the closed-form MAP estimator of other GNNs as future work. The acquisition function in DOCTOR is originally designed for the sequential active learning setting, and we only use a simple sampling method to extend it for the batch active learning setting. We also leave the investigation of a more advanced batch AL extension of DOCTOR as future work. The theoretical connection between our method with entropy minimization in terms of strictly proper scoring rules can be further investigated in detail . Other potential applications of our method can be explored in the hyperbolic space [76; 79; 77; 80; 78; 75; 81] or in the natural language processing domain [34; 36; 35; 20; 19; 63; 44; 99; 100; 98].