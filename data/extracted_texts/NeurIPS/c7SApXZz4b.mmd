# Stronger Than You Think:

Benchmarking Weak Supervision on Realistic Tasks

 Tianyi Zhang\({}^{1}\) Linrong Cai\({}^{2}\) Jeffrey Li\({}^{1}\) Nicholas Roberts\({}^{2}\) Neel Guha\({}^{3}\) Frederic Sala\({}^{2}\)

\({}^{1}\)University of Washington \({}^{2}\)University of Wisconsin-Madison \({}^{3}\)Stanford University

tzhang26@uw.edu, lcai54@wisc.edu

Equal contribution.

###### Abstract

Weak supervision (WS) is a popular approach for label-efficient learning, leveraging diverse sources of noisy but inexpensive _weak labels_ to automatically annotate training data. Despite its wide usage, WS and its practical value are challenging to benchmark due to the many knobs in its setup, including: data sources, labeling functions (LFs), aggregation techniques (called label models), and end model pipelines. Existing evaluation suites tend to be limited, focusing on particular components or specialized use cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF sets that are suboptimally written, producing insights that may not generalize to real-world settings. We address these limitations by introducing a new benchmark, BoxWRENCH,2 designed to more accurately reflect _real-world usages of WS_. This benchmark features tasks with (1) higher class cardinality and imbalance, (2) notable domain expertise requirements, and (3) opportunities to re-use LFs across parallel multilingual corpora. For all tasks, LFs are written using a careful procedure aimed at mimicking real-world settings. In contrast to existing WS benchmarks, we show that supervised learning requires substantial amounts (1000+) of labeled examples to match WS in many settings.

## 1 Introduction

Weak supervision (WS) aims to address the labeled data bottleneck for supervised machine learning. It uses multiple weak but inexpensive sources of signal and combines them into high-quality _pseudolabels_ that can be used for training downstream models . These weak sources can be diverse, including but not limited to: heuristic rules encoded into small programs, queries to knowledge bases, and pretrained models. Frameworks implementing WS are hugely popular and are widely applied in industry  and academic settings .

WS frameworks typically have a simple three-stage approach. First, they formalize weak sources into _labeling functions (LFs)_. In contrast to manual labeling, these can be automatically applied to an entire unlabeled dataset. Next, since LFs are inherently noisy and may conflict with one another, a _label model (LM)_ is used to estimate the quality of each source (typically _without_ accessing ground truth labels) and then to aggregate their outputs into high-quality pseudolabels. Finally, these pseudolabels can be used to train a downstream model. A vast literature studies variations on this basic recipe, with diverse approaches to crafting LFs, creating LMs, and noise-aware training of end-models .

For practitioners, a key question is _when is WS useful?_ While it is natural to produce benchmarks that answer this, surprisingly, there has been relatively little work doing so. One reason for this may be the overall complexity of WS pipelines. The performance of a WS system varies with (1) theunderlying task and data, (2) the LFs, (3) the choice of LM, and (4) the choice of end model and training procedure. Several benchmarks predominantly focus on _only one of these_. For example, WRENCH  focuses primarily on evaluating (3), the LM, while AutoWS-Bench-101  focuses on (2), the LFs, and specifically, techniques for automatically generating model-based LFs.

Recently, Zhu et al.  tackle the goal of quantifying the value of WS. They argue that the benefits of WS are often overestimated by showing that fine-tuning on only 50 ground-truth labels can achieve comparable--or better--results than certain WS approaches for many _benchmark datasets_. They suggest that WS may not be broadly useful, as obtaining 50 "clean" labels is rarely prohibitive, and data at this scale (or larger) may still be needed for tuning or evaluation even when using WS.

In this work, we show that these findings result _from the simplicity of existing datasets_ rather than the inherent _weakness_ of WS. In particular, we identify key issues with the current WS benchmarks that led to this result and show that WS may be _stronger than is thought_ in more realistic settings:

1. **Benchmark datasets usually have too few classes, are balanced, or aren't specialized enough** to be representative of real-world datasets.
2. WS depends on the quality of LFs, and **LFs from current benchmarks can be improved**.
3. **Previous benchmarks do not capture the adaptability of LFs across task specification,** a key practical advantage of WS deployments compared to manual labeling.

We introduce a new benchmark, BoxWRENCH, that addresses these three challenges. It enables us to quantify the practical advantages of WS in a wide range of settings. Our findings indicate that even simple WS approaches often provide substantial value. We address the issues we identified by:

* Proposing new WS benchmarks based upon tasks that involve **high-cardinality label spaces, imbalanced classes, and/or require specific domain knowledge**.
* Showing that by adhering to careful LFs design practices, we can write effective LFs for these tasks that can even improve upon existing benchmark LFs.
* Using the MASSIVE dataset , we study simple but effective strategies for adapting existing LFs written for English data to parallel versions of the task in other languages.

Our benchmark consists of _five_ text-classification WS tasks that showcase the power of WS in a variety of challenging real-world scenarios. For two of our tasks, we produce new LFs, while for one, we improve the existing LFs from WRENCH . The design of these LFs follows a rigorous procedure that we release as part of our benchmark, acting as guidance for LF design and for WS benchmarking overall. We publicly release the code and other assets for our study at https://github.com/jeffreywpli/stronger-than-you-think.

## 2 Background and Related Work

We provide a brief background on WS techniques and benchmarking efforts. We note that the term WS can be overloaded, as it is also applied to other families of techniques that generally aim to learn from indirect or noisy forms of supervision , particularly in computer vision [28; 20]. Here, we use WS to refer to approaches that fall under _programmatic WS_[34; 47].

**Weak Supervision.** WS aggregates multiple imperfect label sources, each formalized as a _labeling function_ (LF), to synthesize labels for unlabeled data. Perhaps the most popular types of LFs are heuristics or rules obtained from subject matter experts  encoded into programs. Other potential sources include knowledge-base queries, pretrained models, and more [16; 21; 8; 29; 17]. Many works in WS focus on improving either how LFs are crafted or how they are aggregated. For LF construction, variations such as learning small models on tiny amounts of data , or using code-generating large language models to craft LFs have been proposed [19; 18; 15]. For aggregation, the simplest technique is to perform a _majority vote_ while other methods seek to infer (without using ground truth labels) the accuracy of the LFs with a _label model_ (LM) and thus perform a higher-quality aggregation [33; 13; 40]. Orthogonal to both directions, other works study more effective strategies for training end models on WS-generated pseudo-labels [45; 24].

WS Benchmarks.Existing WS benchmarks typically focus on a particular component of a WS system or a particular use case. WRENCH  primarily benchmarks different label models and is therefore aimed at aggregation. AutoWS-Bench-101 , in contrast, studies the effectiveness of automated LF construction techniques. Finally, WALNUT  studies WS techniques in the context of natural language understanding. All of these benchmarks are highly useful, but do not attempt to measure the value of WS techniques more broadly. A recent effort by Zhu et al.  tackles this question and finds that in particular settings, WS may not be of great value. Specifically, it suggests that only a small amount of labeled data is sufficient to train a supervised model to a level of quality equivalent to that provided by WS. We are inspired by this work, studying whether we can obtain similar findings across a broader range of realistic scenarios.

## 3 Methodology and Datasets

We establish the goals, problem setting, datasets, and experimental setup used in BoxWRENCH.

### Goals

The ultimate goal of BoxWRENCH is to bridge WS research and practice by introducing more realistic benchmarks for WS. The first step towards such a goal is to gain a better understanding of the question: _when is WS useful?_ To do so, we first gather a suite of datasets that addresses two key areas in which current WS benchmarks fall short.

1. Benchmark datasets tend to be simplistic, exhibiting properties not representative of many real-world problems. This includes having a small label space (often binary), balanced label distributions, and relying on general rather than domain-specific knowledge.
2. Current WS benchmarks are used with de-facto LF sets that vary in quality. A poorly written LF set may also result in a less realistic benchmark (e.g. if a task involves domain expertise but experts were not involved in writing the LFs).

To address the first issue, we introduce WS tasks that directly target the aforementioned gaps: focusing on those with greater class counts, class imbalance, and domain-specificity. To address the second issue, we place care into writing higher-quality LFs for all datasets, including improving existing LFs. Using these datasets, we aim to measure _how many labeled examples are needed_ before supervised learning catches up to WS techniques. We formalize this notion by plotting the performance curves of WS techniques and fully supervised learning (as functions of the number of labels) and analyzing the **crossover points** where these curves intersect. To show that WS is effective, the crossover point in which supervised learning surpasses WS should be sufficiently high, i.e., WS cannot be matched by simply labeling a trivial amount of examples. Using crossover points to measure the effectiveness of WS, we aim to establish a regime in which WS is practically useful on our suite of more challenging and realistic datasets.

### Problem Formulation

Let \(\) be our data distribution. We first sample an unlabeled training set with \(n_{}\) examples \(X_{}=[x_{i}]_{i=1}^{n_{}}\), and a small labeled validation set of \(n_{}\) examples: \(D_{}=[X_{},Y_{}]\) with \(X_{}=[x_{i}]_{i=n_{}+1}^{n_{}+n_{}}\) and \(Y_{}=[y_{i}]_{i=n_{}+1}^{n_{}+n_{}}\) with \(x_{i}\) and \(y_{i}\). We are interested in learning a function \(f_{}:\) that minimizes the expected risk \(R(f_{})=_{(x,y)}[L(f_{}(x),y)]\) where \(L\) is a loss function and \(\) are the model parameters. To estimate \(R(f_{})\), we assume the existence of an i.i.d. test set \(D_{}\), for which labels are also available. In the case of WS, we assume that labels for \(X_{}\) are provided by a set of labeling functions \(_{}=\{_{j}\}_{j=1}^{m}\) where each LF, \(_{j}:X Y\{-1\}\) encodes some heuristic that labels or abstains (denoted by -1) on each input \(x_{i}\). Using a label model LM that aggregates the LF votes, \(_{i,j}=_{j}(x_{i})\), we obtain \(_{i}=([_{i,j}]_{j=1}^{m})\), namely, the weak label for \(x_{i}\). We construct the weak labels for the training set as \(_{}=[_{i}]_{i=1}^{n_{}}\). We then use \(D_{}=[X_{},_{}]\) as the weakly labeled training set, to train a model \(f_{_{}}\).

Following Zhu et al. , we compare the performance \(f_{_{}}\) with two other models that train directly on the validation labels: \(f_{_{}}\) is the model trained only on \(D_{}\), which serves as a critical baseline for assessing the usefulness of WS. \(f_{_{}}\) is a model that results from using \(f_{_{}}\) as an initialization and further performs continuous fine-tuning on \(D_{}\). Weak supervision is considered useful given \(D_{}\) if the performance of either \(f_{_{}}\) or \(f_{_{}}\) remains significantly higher than that of \(f_{_{}}\).

### Datasets

Existing WS benchmarks often exhibit low class cardinality (under 5), highly balanced label distributions (i.e. near uniform), and simplistic underlying tasks that do not require domain-specific knowledge. However, real-world tasks often do not conform to these assumptions. Thus, to evaluate WS in realistic regimes, we propose a collection of datasets3 with varying levels of class cardinality and imbalance, as well as requirements for domain expertise. We describe each of the datasets used in BoxWRENCH, with their metadata shown in Table 1, and describe their LFs.

* **Banking77** comprises online banking queries annotated with one of 77 user intents. It has 209 keyword-based LFs.
* **ChemProt** is a chemical relation classification dataset comprising 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts. The dataset was studied in . Previous works on WS created LFs for the dataset and showed the efficacy of WS in the dataset . We push the boundaries of WS in the dataset by modifying the LFs to incorporate the distance between the chemical and protein entities in the text, among other minor modifications (see Appendix C and Appendix K).
* **Claude94** is based on UNFAIR-ToS , which includes 50 Terms of Service (ToS) from online platforms and sentence-level annotations with 8 types of unfair contractual terms (potentially violating user rights according to EU consumer law).5 LFs for this dataset were created by one of the authors of this work who is a law graduate student. * **MASSIVE** is a corpus of human-to-voice assistant interactions, where the task is to predict one of 60 fine-grained or 18 coarse-grained annotations of user intents. The corpus includes parallel versions across 52 different languages, where each example is present in each language. In our work, we explore reusing the existing LFs written for the English variant of the 18-class prediction task to generalize to additional languages and more fine-grained classes. For non-English variants of MASSIVE18, we apply English LFs by first translating each example back to English using DeepL (see Figure 2). For MASSIVE60, we leverage the original LFs from MASSIVE18 to predict the superclass for each instance. To generalize from coarse to fine-grained predictions in MASSIVE60, we randomly select a subclass corresponding to the predicted superclass from the original LFs. * **Amazon31** is built from the Amazon product reviews  dataset consisting of reviews and their categories.6 Due to each class's high overlap and conflict rate, we merged several labels and reduced the class cardinality to 31. 
LF Design Pipeline.We randomly selected samples with clean labels from the training set to create a development set for LFs. We use 250 examples as a development set for Amazon31, which is consistent with the development of LFs for Banking77 and MASSIVE18 in . For Claude9, the development set had 24 examples. We manually inspected labeled examples in the development set, and identified patterns for each class. Then we create multiple keyword-based, dictionary-based, and

   Dataset & Class & Train & Valid & Test \\  Banking77 & 77 & 9,003 & 1,000 & 3,080 \\ ChemProt & 10 & 12,600 & 1,607 & 1,607 \\ Claude9 & 9 & 5,469 & 200 & 2057 \\ MASSIVE\{18, 60\} & 18, 60 & 11,564 & 3,305 & 1,651 \\ Amazon31 & 31 & 131,781 & 5,805 & 17,402 \\   

Table 1: The datasets used in BoxWRENCH and their metadata. For MASSIVE, the dataset sizes are the amounts _per available language_.

regular expression-based LFs for each class7. We calculated LF statistics (which do not require label information) on the original training and validation sets, including coverage and LF conflict ratios. To evaluate the final LFs, we calculate their accuracy scores on the original validation set.

### Experimental setup

We describe various aspects of our experimental setup: our pipeline, learning scenarios, and a description of how we evaluate when WS is useful.

**Pipeline.** For backward compatibility, our experimental pipeline is based on WRENCH . This allows users to experiment with different LMs, EMs, and LFs in a standardized format, and with WS-specific tooling for dataset manipulation. We use majority vote (MV) as the default LM for most of our WS experiments, following WRENCH's takeaway that MV is one of the most consistent methods across various tasks. Appendix I includes results for COSINE  and ARS2 , but we did not find that they consistently improved upon MV. That being said, more effective WS methods in the future, would further strengthen our conclusions about WS's effectiveness.

For the end-model, we fine-tune a pretrained RoBERTa model in most cases  and also try BERT variants for domain-specific (i.e., LegalBERT for Claude9 and Sci-BERT for ChemProt) and non-English tasks (BERT-base-chinese, NB-BERT-base, and BERT-base-japanese [14; 30; 31]

Figure 1: We compare end models trained with three different pipelines: (1) _Supervised_, which uses clean labels from the validation set for fine-tuning; (2) _Weakly Supervised_, which trains on labels obtained by applying all LFs and aggregating them with the label model, using the validation set for hyperparameter search and early stopping. _Continuous-Fine-Tuning_: which takes a weakly supervised model and then continuously fine-tuning it on the same clean validation labels.

Figure 2: Using an off-the-shelf translator (DeepL) to reuse English LFs for the multilingual variants of MASIVE18. Our approach is to apply the existing LFs written for MASSIVE18-En by first translating non-English versions of the dataset to English.

for MASSIVE18). Following the procedure used by Zhu et al. , we randomly select a set of hyperparameters from their provided hyperparameter search spaces. Whenever training on weak labels, we do early stopping based on the validation data, following the practices of WRENCH, while we fine-tune for a fixed 6,000 total steps whenever training on clean labels, following Zhu et al. . We run all the experiments on NVIDIA A100, A40, A6000, A4000, and RTX-4090 GPUs.

**Learning Scenarios.** We include the three types of learning scenarios that we compare: supervised learning, WS learning, and CFT, all involving clean labels coming from only the validation set (as shown in Figure 1). Notably, we can successfully replicate the results of  with our setup on existing WS benchmarks, as shown in Appendix E.

* **Supervised:** We use clean labels from the validation set directly for fine-tuning an end model.
* **Weakly Supervised:** We aggregate all weak labels from the training data with an LM, then fine-tune an end model using the training data with the aggregated labels. Generally in WS, such as in WRENCH, the clean validation labels are used to guide hyperparameter search (for both the LM and end model). In our setup, we use it for early stopping.
* **Continuous-Fine-Tuning (CFT):** We further fine-tune the models trained using only weak supervision on the same set of clean validation labels.

**Crossover Points.** To measure the usefulness of WS, we plot the performances of the three aforementioned methods for various amounts of clean validation data and inspect where their performance curves intersect. Specifically, the crossover point that we care about is between the supervised method, which uses only clean labels, and the better of WS and SFT, which make use of the weak labels.

## 4 Results and Analysis

In this section, we present our main results and analysis. In Section 4.1, we first investigate the crossover points for our new datasets and compare them with existing ones. In Section 4.2, we show that crossover points can be significantly increased when LFs are written more carefully. Section 4.3 showcases another dimension of the usefulness of WS, as we demonstrate how LFs can be adapted in a multilingual setting. Finally, we study whether different LMs perform better on our more challenging new tasks in 4.4.

### Comparing crossover points between existing and new benchmarks

We first conducted a crossover point analysis for the existing datasets from WRENCH  in Figure 3, extending the results from . We confirm that for most of these tasks, the crossover points between Supervised (green) and CFT (blue) are quite small, less than 200 for four of six tasks. Notably, these four datasets all have considerably smaller label cardinalities compared to most datasets in BoxWRENCH (see Appendix D). We also performed the analysis on the named entity recognition datasets from WRENCH, with similar results (see Appendix H). Overall, these experiments verify that existing benchmarks are often inadequate for capturing realistic scenarios where WS holds significant utility over hand-labeling.

We then conducted the same experiments on our new datasets, and analyzed their crossover points in Figure 4. For both Amazon31 and Banking77, the crossover points are beyond 1,000 clean labels. For Claude9, the validation set is smaller and even training on all available examples does not result in a crossover. Instead, the gap between the CFT and supervised-only methods remains 5% higher.

### Improving LFs leads to higher crossovers

Previously, the LFs of ChemProt from Yu et al. , which are keywords-based, have a coverage of 0.86 on the test set and a precision of 0.55 (i.e., the accuracy of the set of examples where LM does not abstain). To improve the quality of the LF set, we made the following changes. First, we changed the keywords used in existing LFs. Next, we modified seven LFs by using the absolute difference in entity positions in the text as features. After making these changes, the resulting LF set had a coverage of 0.81 and a precision of 0.63. When constructing these LFs, we consulted a Ph.D student in Neuroscience to leverage his domain expertise in Chemistry and Biology for insights on writing LFs. More details about the new LFs are included in Appendix C.

These modifications resulted in improved performance for CFT given the same amount of labeled data. When measuring F1, the crossover points grew to around 1600 as shown in Figure 5 (compared to 800 for the original LFs). When measuring accuracy, the improvement was smaller but consistent across the range of validation set sizes (see Figure 7). This experiment demonstrates that more careful writing of LFs can yield higher crossover points. Current research benchmarks may suffer from suboptimal LF sets, and this example highlights how simple adjustments can enhance results.

### Cross-lingual adaptability of LFs

In this subsection, we present a set of experiments to highlight an additional advantage of WS over manual labeling: its adaptability. Specifically, we examine the ability to reuse existing LFs in a multilingual setting on the MASSIVE18 dataset. We consider the setting in which one has already trained a weakly supervised model based on LFs written for the dataset's English version and wishes also to obtain a strong model for other languages. In doing so, we consider the following approaches:

* **Oracle-LFs-L** (for L \(\) {Chinese, Japanese, Norwegian}). Here, we directly use the label matrix induced by the LFs for the English version of MASSIVE18 on the non-English versions of the dataset. Note this is only possible because MASSIVE maintains one-to-one correspondences between the examples from different languages.

Figure 4: Crossover points on three of our datasets: Amazon31, Banking77, and Claude9. For both Amazon31 and Banking77, the crossover points are beyond 1,000 clean labels. For Claude9, the validation set is smaller and even training on all available examples does not result in a crossover.

Figure 3: Crossover points on six existing WS benchmarks: AGNews, Semeval, Yelp, ChemProt, Trec, IMDB. The crossover points for these tasks are low, less than 200 on four out of the six tasks, which is substantially lower than the crossover points in BoxWRENCH datasets.

* **L2En-LFs**, this method intends to provide a more realistic scenario for reusing the English LFs. In practical scenarios, we might have a set of English LFs LF\({}_{}\) and need to train a model in another language, L, without the resources to create additional LFs in a foreign language. In this case, we use DeepL  to first translate our non-English unlabeled data to English and then apply LF\({}_{}\) to obtain weak labels. Figure 2 illustrates this pipeline.
* **L2En-Inference-Supervised.** This is an important baseline where we evaluate whether the English model alone can be directly used in other languages by simply first translating test examples into English. In Figure 6, the validation size for this method represents the amount of clean data used to train the English model.

Here we present the accuracy of \(f_{_{}}\), \(f_{_{}}\), and \(f_{_{}}\) with our methods in Figure 6, where different suffixes are used with each method for clarity. We use L2En-Inference-Supervised as an additional baseline, representing the use of non-language-specific models, which we observe underperforms training language-specific models (while also incurring additional costs at inference time).

Meanwhile, we see that L2En-LFs demonstrates promise of adapting the English LFs, leading to crossover points of over 1000 in Chinese and Japanese, despite coming for "free" (i.e., without writing additional LFs in the target languages). It also achieves accuracy comparable to Oracle-LFs.

Finally, we also introduce a realistic setup in which even the amount of _unlabeled_ data is limited for a rarer language, Norwegian. Specifically, we assume only a 20% subset of the Norwegian version of the MASSIVE is available, which we label using L2En-LFs. We then _augment_ this data by translating the corresponding weakly labeled English examples into Norwegian for the remaining 80%. As shown in Figure 9, this leads to a crossover points of above **1500** for fine-tuning Norwegian BERT

Figure 5: Crossover points on ChemProt with our new LFs. The new LFs for ChemProt demonstrate a higher crossover point when measuring F1 performance.

Figure 6: Crossover points on the multilingual MASSIVE18 dataset. The comparison of the green and yellow solid lines showed the importance of having a language-specific model. Surprisingly, using our method from Figure 2 (blue solid line) is able to achieve the same or even better performance compared to the light blue dotted line, which used the weak labels directly from the English version of MASSIVE18.

(NB-BERT-base) . This showcases that for rarer languages, for which foundation models may be less powerful and labels harder to obtain, adapting LFs can be of even larger value.

### Label model ablations

In real-world applications, WS is typically integrated with different LMs to optimize performance. Several LMs are frequently employed in WS frameworks, including Majority Vote, Dawid-Skene , Snorkel , and FlyingSquid .

In our study, we conducted a comprehensive evaluation of WS using these LMs on our datasets. The performance of each LM was systematically assessed to determine its effectiveness in various scenarios. Detailed results of this evaluation are presented in Table 2, showcasing the comparative performance and highlighting the strengths of each model (see Appendix F for the full table with Amazon31 and Banking77). For most of the new datasets, the CFT method outperforms the supervised-only method with a clear margin, especially in low-resource settings. Majority Vote and Dawid-Skene performed the best among the label models tested.

    & Claude9 & ChemProt8  & MASSIVE18 \\ 
**6.25\% Validation Size** & 12 & 100 & 127 \\ +Majority vote & 0.153\(\)0.026 & **0.625\(\)0.018** & **0.811\(\)0.004** \\ +DawidSkene & **0.153\(\)0.025** & 0.610\(\)0.020 & 0.797\(\)0.010 \\ +Snorkel & 0.132\(\)0.022 & 0.617\(\)0.012 & 0.811\(\)0.007 \\ +FlyingSquid & 0.109\(\)0.005 & 0.606\(\)0.021 & 0.751\(\)0.011 \\ +Supervised Only & 0.109\(\)0.005 & 0.504\(\)0.030 & 0.753\(\)0.014 \\ 
**12.5\% Validation Size** & 25 & 200 & 254 \\ +Majority vote & 0.175\(\)0.020 & **0.685\(\)0.010** & 0.845\(\)0.006 \\ +DawidSkene & **0.184\(\)0.022** & 0.654\(\)0.024 & 0.836\(\)0.004 \\ +Snorkel & 0.157\(\)0.016 & 0.649\(\)0.015 & **0.845\(\)0.005** \\ +FlyingSquid & 0.124\(\)0.014 & 0.640\(\)0.015 & 0.818\(\)0.009 \\ +Supervised Only & 0.123\(\)0.015 & 0.585\(\)0.022 & 0.820\(\)0.013 \\ 
**25\% Validation Size** & 50 & 401 & 508 \\ +Majority vote & 0.345\(\)0.114 & **0.726\(\)0.021** & 0.863\(\)0.004 \\ +DawidSkene & **0.362\(\)0.107** & 0.712\(\)0.014 & **0.863\(\)0.007** \\ +Snorkel & 0.303\(\)0.090 & 0.711\(\)0.016 & 0.859\(\)0.002 \\ +FlyingSquid & 0.170\(\)0.025 & 0.696\(\)0.010 & 0.856\(\)0.009 \\ +Supervised Only & 0.165\(\)0.052 & 0.684\(\)0.023 & 0.855\(\)0.010 \\ 
**50\% Validation Size** & 100 & 803 & 1016 \\ +Majority vote & **0.483\(\)0.071** & **0.778\(\)0.005** & 0.882\(\)0.005 \\ +DawidSkene & 0.477\(\)0.074 & 0.775\(\)0.003 & 0.884\(\)0.005 \\ +Snorkel & 0.462\(\)0.065 & 0.767\(\)0.009 & **0.885\(\)0.005** \\ +FlyingSquid & 0.232\(\)0.057 & 0.762\(\)0.008 & 0.880\(\)0.005 \\ +Supervised Only & 0.253\(\)0.043 & 0.773\(\)0.010 & 0.883\(\)0.004 \\ 
**100\% Validation Size** & 200 & 1607 & 2033 \\ +Majority vote & **0.582\(\)0.036** & **0.820\(\)0.004** & **0.899\(\)0.002** \\ +DawidSkene & 0.572\(\)0.038 & 0.820\(\)0.005 & 0.893\(\)0.002 \\ +Snorkel & 0.557\(\)0.020 & 0.814\(\)0.004 & 0.899\(\)0.002 \\ +FlyingSquid & 0.352\(\)0.037 & 0.813\(\)0.006 & 0.894\(\)0.005 \\ +Supervised Only & 0.347\(\)0.020 & 0.816\(\)0.007 & 0.898\(\)0.003 \\   

Table 2: Test accuracy (ChemPort, MASSIVE18)/F1 scores (Claude9) of supervised-only methods and CFT across different proportions of clean data used, LMs, and datasets. **First** and _second_ best results are **Bolded** and _underlined_.

## 5 Limitations

There are several limitations to this work. (1) We primarily focus on text classification tasks, which are more common in practice; while other benchmarks such as WRENCH Zhang et al.  include both text classification and sequence-tagging. Similar investigations and LF improvements would be valuable to study using our codebase in future work. (2) In our experiments, we used BERT-based end models and relatively simple label models such as Majority Vote, Dawid-Skene, Snorkel, COSINE, and ARS2. More recent end models and label models could also be worth testing with our pipeline. (3) We did not thoroughly tune hyperparameters while training weakly supervised models, following the setup from . With more careful hyperparameter tuning, WS has the potential to achieve better results. (4) The MASSIVE dataset has one-to-one correspondences across languages, while in real-life scenarios, usage patterns and distribution shifts may exist across different languages, even on identical tasks.

## 6 Conclusions

In this paper, we introduce BoxWRENCH, a benchmark that expands the evaluation of WS by addressing the limitations of existing benchmarks. By incorporating high-class cardinality, imbalance, and the need for domain expertise, BoxWRENCH better reflects real-world data and tasks. Our results show that on these more realistic tasks, weak supervision demonstrates significant utility, particularly in scenarios where traditional labeling is cost-prohibitive. We also show that careful LF design and adapting existing LFs in multilingual settings can significantly enhance the applicability of WS across diverse contexts. BoxWRENCH sets a new standard for evaluating WS, with publicly released datasets, benchmarks, and tools to advance WS research and its practical deployment.