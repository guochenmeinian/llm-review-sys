# OT4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation

Yaming Guo\({}^{1,4}\), Chen Zhu\({}^{2}\), Hengshu Zhu\({}^{3}\), Tieru Wu\({}^{1}\)

\({}^{1}\)School of Artificial Intelligence, Jilin University

\({}^{2}\)School of Management, University of Science and Technology of China

\({}^{3}\)Computer Network Information Center, Chinese Academy of Sciences

\({}^{4}\)The Hong Kong University of Science and Technology (Guangzhou)

{yamingguo98,zc3930155,zhuhengshu}@gmail.com,wutr@jlu.edu.cn

Corresponding authors

###### Abstract

Optimization over permutations is typically an NP-hard problem that arises extensively in ranking, matching, tracking, etc. Birkhoff polytope-based relaxation methods have made significant advancements, particularly in penalty-free optimization and probabilistic inference. Relaxation onto the orthogonal group offers unique potential advantages such as a lower representation dimension and preservation of inner products; however, equally effective approaches remain unexplored. To bridge the gap, we present a temperature-controlled differentiable transformation that maps unconstrained vector space to the orthogonal group, where the temperature, in the limit, concentrates orthogonal matrices near permutation matrices. This transformation naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations. Additionally, by deriving a re-parameterized gradient estimator, this transformation also provides efficient stochastic optimization over the latent permutations. Extensive experiments involving the optimization over permutation matrices validate the effectiveness of the proposed method.

## 1 Introduction

Permutation refers to the reordering of elements within a finite set, commonly encountered in problems involving bijections between two equally sized sets . A permutation of \(n\) elements can be denoted by an \(n n\) permutation matrix, which is a square binary matrix that has exactly one entry of \(1\) in each row and each column, with all other entries being \(0\). We denote the set of all \(n\)-order permutation matrices as \(_{n}:=\{P\{0,1\}^{n n}_{i}P_{i,j}=1,_{j}P_{i,j }=1\ ( i,j)\}\). This work considers optimization over permutation matrices:

\[_{P_{n}}f(P).\] (1)

Due to the combinatorial nature of permutation matrices, the cardinality of the set \(_{n}\) grows factorially with the dimension \(n\), typically rendering the problem NP-hard . From a theoretical perspective, one of the most renowned special cases of Equation (1) is the quadratic assignment problem, which has attracted extensive research . In practical terms, Equation (1) also arises extensively in various machine learning tasks, including ranking , matching , tracking , etc.

Previous studies have proposed relaxing permutation matrices into continuous spaces, including the convex hull of permutation matrices--the Birkhoff polytope --and their embeddings in a differentiable manifold--the orthogonal group . Recently, relaxation methods involving the Birkhoff polytope have made significant advancements, particularly in penalty-free optimizationand probabilistic inference [59; 43; 49]. As a notable example, Mena et al.  utilize the Sinkhorn operator  to transform matrices into the Birkhoff polytope, bringing them closer to permutation matrices under temperature control. This approach avoids introducing penalty terms and supports variational inference.

However, providing equally good relaxation methods within the orthogonal group remains an unexplored area. Indeed, relaxation onto the orthogonal group offers several unique potential advantages, such as: i) a lower representation dimension (\(\)) compared to the Birkhoff polytope (\((n-1)^{2}\)), leading to a smaller search space; ii) the orthogonal matrix preserve the inner product of vectors, which is useful for tasks requiring the maintenance of geometric structures. In light of the above advantages, this work aims to develop an effective method for relaxing the permutation matrices onto the orthogonal group, with a particular focus on:

* Flexibility: can control the degree of approximation to permutation matrices.
* Simplicity: does not rely on additional penalty terms.
* Scalability: enables learning the latent variable model with permutations.

In this paper, we present _Orthogonal Group-based **T**ransformation **for** Permutation Relaxation_ (OT4P), a temperature-controlled differentiable transformation. OT4P maps unconstrained vector space to the orthogonal group, where the temperature, in the limit, concentrates orthogonal matrices near permutation matrices. As illustrated in Figure 1, OT4P involves two steps: I) map a vector (\(\)) to an orthogonal matrix (\(\)) utilizing the Lie exponential; II) move the orthogonal matrix (\(\)) along the geodesic, controlled by temperature, to another orthogonal matrix (\(\)), making it nearer to the closest permutation matrix (\(_{}\) or \(_{}\)). OT4P naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations. In addition, OT4P, combined with the re-parameterization trick, provides stochastic optimization over the latent permutations.

In summary, our main contributions are as follows:

1. We present OT4P, a differentiable transformation for relaxing permutation matrices onto the orthogonal group, characterized by its flexibility, simplicity, and scalability (Section 3.1).
2. We use OT4P to implement a parameterization for the relaxation of permutation matrices, which has the advantages of not altering the original problem, not complicating the original problem, and an efficient optimization process (Section 3.2).
3. We derive a gradient estimator using OT4P and the re-parameterization trick, providing an efficient tool for stochastic optimization over latent permutations (Section 3.3).
4. We validate the effectiveness of the proposed method through extensive experiments involving the optimization of permutation matrices, including finding mode connectivity, inferring neuron identities, and solving permutation synchronization (Section 4).

## 2 Preliminaries

In this section, we give a brief overview of the Riemannian geometry  and the Lie group theory  involved, with a more comprehensive version available in Appendix B.

Figure 1: Illustration of OT4P with colored dots to help visualize the transformation. In the limit of temperature, the orthogonal matrices obtained from OT4P converge near the permutation matrices.

An \(n\)-dimensional _manifold_\(\) is a space that can be locally approximated by a Euclidean space \(^{n}\), where each point \(x\) possesses a _tangent space_\(T_{x}\) as a first-order local approximation of \(\) around \(x\). The _Riemannian metric_ is a collection \(m:=\{m_{x} x\}\) of inner products \(m_{x}(,):T_{x} T_{x}\), which may define distances on the manifold. A _geodesic_ is a smooth curve that the tangent vector is parallel transported along the curve w.r.t. the Levi-Civita connection.

A _Lie group_\(G\) is a differentiable manifold equipped with differentiable group operations, whose tangent space at the identity \(e\) is the _Lie algebra_\(\). For each \(g G\), there exist diffeomorphisms (Definition 2) given by the _left translation_\(L_{g}(x):=gx\) (\( x G\)), which lead to a vector space isomorphism (Definition 1) that relates the tangent space \(T_{g}G\) to the Lie algebra, i.e., \((L_{g})_{e}: T_{g}G\). Analogously, one can introduce the _right translation_: \(R_{g}(x):=xg\) (\( g,x G\)). A Riemannian metric \(m\) on Lie group \(G\) is called _left-invariant_ (_right-invariant_) if it renders each left (right) translation an isometry (Definition 3), allowing us to associate neighborhoods of the identity \(e\) with any point \(g G\) using left (right) translation. If a metric on the Lie group is both left and right invariant, it is termed the _bi-invariant_ metric.

The Lie group we are interested in is the _orthogonal group_\((n)\), which consists of all \(n n\) orthogonal matrices \(O\) satisfying \(O^{}O=OO^{}=I\). We equip \((n)\) with the canonical metric, a bi-invariant metric defined as \( A,B_{}:=(A^{}B)\), where \(,_{}\) is the Frobenius inner product and \(()\) is the trace of a matrix. The subset of \((n)\) with determinant \(+1\) forms a subgroup known as the _special orthogonal group_, denoted by \((n):=\{O^{n n} O^{}O=I, O=+1\}\). The Lie algebra \((n)\) of the Lie group \((n)\) comprises \(n n\) skew-symmetric matrices, expressed as \((n):=\{A^{n n} A^{}=-A\}\). The _Lie exponential_\(()\), coinciding with the _matrix exponential_ in the context of the matrix Lie group, maps elements in \((n)\) to \((n)\), defined by

\[(A):=I+_{k=1}^{}}{k!}.\] (2)

The series in Equation (2) converges for all matrices \(A\). The local inverse function of the matrix exponential is supposed to be the _matrix logarithm_, which is defined as follows:

\[(A):=_{k=1}^{}(-1)^{k+1}}{k}.\] (3)

The series in Equation (3) converges whenever \(\|A-I\|_{}<1\), where \(\|A\|_{}:=}}\) represents the Frobenius norm induced by the Frobenius inner product.

## 3 Relaxing permutation on orthogonal group

In Section 3.1, we introduce the two steps of the proposed OT4P and analyze its key properties. Then, in Section 3.2, we demonstrate how to use OT4P to implement a parameterization for the relaxation of permutation matrices, emphasizing the advantages of such a parameterization. Finally, in Section 3.3, we provide efficient stochastic optimization over the latent permutations using OT4P and the re-parameterization trick.

### The proposed OT4P transformation

The proposed OT4P comprises two steps: I) map a point in the vector space to an orthogonal matrix; II) move the orthogonal matrix along the geodesic under temperature control, bringing it nearer to the closest permutation matrix. We summarize the pseudo-code of OT4P in Algorithm 1.

#### Step I

Consider an unconstrained vector space \(^{}\). For a vector \(a^{}\), we can fill it into an upper triangular \(n n\) matrix with zero in the diagonal. For example, in the case of \(n=3\):

\[[a_{1},a_{2},a_{3}]=a A=0&a_{1}&a_{2}\\ 0&0&a_{3}\\ 0&0&0.\]In the following, we employ matrices \(A^{n n}\) rather than vectors \(a\) to represent the elements in \(^{}\). A skew-symmetric matrix is uniquely determined by \(\) scalars, i.e., the entries above the main diagonal. Therefore, there exists an isomorphism between the vector space \(^{}\) and the Lie algebra \((n)\) formed by skew-symmetric matrices, given by

\[:^{} (n)\] (4) \[A  A-A^{}.\]

As mentioned in Section 2, we can use the matrix exponential (Lie exponential) \(()\) to map the Lie algebra \((n)\) to the Lie group, i.e., special orthogonal group \((n)\):

\[:(n) (n)\] (5) \[A (A).\]

Combining Equation (4) and Equation (5), we can map the unconstrained vector space \(^{}\) to the special orthogonal group \((n)\), denoted as

\[:^{} (n)\] (6) \[A (A-A^{}).\]

This mapping belongs to the classical category in Lie group theory and serves as an efficient solution for addressing orthogonal constraints in the field of machine learning . Similarly to Lezcano Casado , we present the important properties of the mapping \(()\) below.

**Theorem 1**.: _The mapping \(()\) is differentiable, surjective, and it is injective on the domain \(:=\{A^{}\,_{k}(A -A^{})(-,),\, k\}\) with \(_{k}()\) the eigenvalues. Additionally, the set \((n)()\) has a zero Lebesgue measure in \((n)\)._

The theorem indicates that each orthogonal matrix in \((n)\) can be represented by a vector in \(^{}\), with each representation being uniquely defined within set \(\), provided it exists there. However, permutation matrices may include \(-1\) as one of their eigenvalues (see Figure 4), with their corresponding representations precisely lying on the boundary of \(\). If the optimal solution to Equation (1) is a permutation matrix with an eigenvalue of \(-1\), it may lead the optimization path to deviate from \(\). To counter this, we propose shifting the boundary of \(\) to other eigenvalues by left-multiplying the result of Equation (6) with an orthogonal matrix \(B(n)\). Theoretically, the left translation \(L_{B}(O):=BO\) (\( O(n)\)) creates a diffeomorphism (Definition 2) on \((n)\), where the representation of the permutation matrix \(P\) in \(\) is changed from \((P)\) to \((B^{}P)\)2. We have empirically observed that the left translation \(L_{B}\) effectively relocates the majority of permutation matrices' representations into the interior of \(\). More discussion can be found in Appendix C.

#### Step II

Given an orthogonal matrix \(O(n)\), we would like to move it toward the closest permutation matrix along the geodesic. To achieve this, we first need to find the permutation matrix \(P_{n}\) closest to \(O\), which can be expressed as

\[(O):=*{arg\,max}_{P_{n}} P,O_{ },\] (7)

where \( A,B_{}=*{trace}(A^{}B)\) denotes the Frobenius inner product. Equation (7) is a linear assignment problem that can be solved in cubic time using the Hungarian algorithm , with further details available in the Appendix E.

Once \(P\) is found, we can move \(O\) towards \(P\) along the geodesic \(OP\). In the Lie group, utilizing the mapping \(()\), movement along the geodesic can be transformed into a more manageable movement on the Lie algebra. However, since \(O\) or \(P\) may be far from the identity matrix \(I\), the convergence speed of the series expansion of \(()\) may be slow or even fail to converge.

We propose to carry out the above process in the tangent space \(T_{P}(n)\) rather than in the Lie algebra \((n)\). Due to the bi-invariant metric \(,_{}\) equipped on \((n)\), the left translation \(L_{P}(O)=PO\ ( O(n))\) establishes an isometry (Definition 3) between the neighborhoods of \(I\) and \(P\), and its derivative \((L_{P})_{e}: T_{P}(n)\) provides an isomorphism (Definition 1) between the Lie algebra \((n)\) and the tangent space \(T_{P}(n)\). Hence, we first push \(O(n)\) into the neighborhood of \(I\), then map it to the Lie algebra \((n)\) using \(()\), and finally pull the result into the tangent space \(T_{P}(n)\). In this way, we define the logarithm map at \(P\) as

\[_{}:(n)&  T_{P}(n)\\ O& P(P^{}O).\] (8)

Equation (8) maps \(O(n)\) near \(P\) to the tangent space \(T_{P}(n)\), and its convergence domain also concentrates near \(P\). Similarly, we can define its local inverse mapping:

\[_{}:T_{P}(n )&(n)\\ A& P(P^{}A).\] (9)

Equation (9) maps \(A T_{P}(n)\) onto the special orthogonal group \((n)\) located near \(P\). With the aforementioned tools, we can easily move orthogonal matrix \(O\) toward its closest permutation matrix \(P\) by interpolation. Specifically, we map \(P\) and \(O\) to the tangent space \(T_{P}(n)\) for linear interpolation, and then map the interpolation result back to \((n)\), given as

\[&=P(P^{ }[ P(P^{}O)+(1-)P(P^ {}P)])\\ &=P(P^{}[ P(P^ {}O)])\\ &=P((P^{}O))\\ &=P(P^{}O)^{}.\] (10)

The second equation stems from the fact that \((I)=\), and the last equation follows from \(A^{}=((A))\) when \(\|A-I\|_{}<1\). The temperature parameter \((0,1]\) is used to control the degree to which the resulting orthogonal matrix \(\) approaches \(P\). It is clear that \(_{ 0^{+}}-P_{}=0\).

Figure 2: Illustration of the mappings \(_{P}\) and \(_{P}\). The left translation \(L_{P}\) establishes an isometry between the neighborhoods of \(I\) and \(P\), and its derivative \((L_{P})_{e}\) provides an isomorphism between \((n)\) and \(T_{P}(n)\).

Remark on odd permutations.The attentive reader may notice that Equation (10) presupposes \(P(n)\), which works only for \(P\) corresponds to even permutations. However, we can readily extend it to cases where \(P\) corresponds to odd permutations. Firstly, we solve \(_{(n)}-P_{F}^{2}\) to identify an agent of \(P\) within \((n)\), which admits an analytical solution \(=PD\) with \(D=(\{1,,1,-1\})\). Then, by substituting \(P\) with \(\) in Equation (10), \(O\) is moved toward \(\) to obtain \(\). Finally, we right-multiply \(\) by \(D^{}\) to map it to the neighborhood of \(P\), resulting in \(=D^{}\). Essentially, the neighborhoods of \(P\) and its agent \(\) are linked through an isometry, specifically the right translation \(R_{D}(O):=OD\ ( O(n))\). Consequently, when \(\) is moved close enough to \(\), the resulting orthogonal matrix \(\) will also be sufficiently close to \(P\). Please refer Appendix D for more theoretical details.

Let \(_{P}\) denote the set of orthogonal matrices in \((n)\) whose closest permutation matrix is \(P\), and let \(_{P}^{}\) represent its image obtained through Equation (10). Equation (10) actually transforms the submanifold \(_{P}\) into a new submanifold \(_{P}\) that is closer to the permutation matrix \(P\). We define the manifold consisting of all images as \(_{}:=\{_{P}^{}(n)\ |\ P _{n}\}\). Consequently, the special orthogonal group \((n)\) is mapped to a manifold \(_{}\) that tightly wraps around the permutation matrices, which can be more formally expressed as:

\[_{}:(n) _{}\] (11) \[O (O)D([(O)D]^{}O)^{}D ^{}.\]

The aforementioned mapping covers all cases, where \(D=(\{1,,1,-1\})\) for odd permutations and \(D=I\) for even permutations. In Figure 5, we provide a visualization of the results of \(_{}()\) as the temperature parameter \(\) varies. The mapping \(_{}()\) is meaningless at points where \(()\) w.r.t. Equation (7) is discontinuous. It is important to note that \(()\) is a piecewise constant function, changing only at points where multiple permutation matrices are equidistant. The following theorem presents key properties of the mapping \(_{}()\).

**Theorem 2**.: _The mapping \(_{}()\) is differentiable, surjective, and injective on each submanifold \(_{P}\). Additionally, the set of meaningless points for \(_{}()\) has a zero Lebesgue measure in \((n)\)._

The theorem shows that any point in the relaxation manifold \(_{}\) of permutation matrices can be uniquely identified by an orthogonal matrix in the special orthogonal group \((n)\), where the set of meaningless elements (i.e., not mapped any point in \(_{}\)) can be disregarded. Using the composite mapping \(_{}\), we create a one-to-one correspondence between \(:=\{A^{}\ |\ \,_{k}(A-A^{ })(-,),\, k\}\) and \(_{}\), except for points associated with zero measure sets in \((n)\) that are either not representable by \(()\) or are meaningless for \(_{}()\). In other words, points in the manifold \(_{}\), which tightly wraps around the permutation matrices, can almost be represented one-to-one by points in \(\) that lie in the unconstrained vector space.

### Parameterization for gradient-based optimization

This section demonstrates how to use OT4P to implement a parameterization for the relaxation of permutation matrices, thereby allowing gradient-based optimization for Equation (1). More importantly, we present three advantages of this parameterization, making it a reasonable solution.

Recalling the manifold \(_{}\) obtained from Equation (11), which converges around the permutation matrices controlled by the temperature parameter \(\). We first relax Equation (1) into an optimization problem on the manifold \(_{}\):

\[_{O_{}}f(O).\] (12)

Using the composite mapping \(_{}\), we transform the constrained optimization problem on the manifold \(_{}\) into an unconstrained optimization problem in the vector space \(^{}\):

\[_{A^{}}f(_{}(A)).\] (13)

For the aforementioned optimization problem, we can employ standard optimization techniques, such as SGD and Adam algorithms , to approximate the solution. Below, we thoroughly discuss the three advantages brought about by the parameterization of OT4P.

The surjectivity does not alter the original problem.The surjectivity of the mapping \(_{}\) implies that every point in the manifold \(_{}\) has at least one corresponding pre-image in the vector space \(^{}\). This guarantees that, during the optimization process, any point within the manifold \(_{}\) can be reached, thereby preventing the overlooking of any potential solutions to Equation (12). In particular, if we find a solution \(A\) while dealing with Equation (13), we can solve Equation (12) by mapping \(O=_{}(A)\).

The injectivity does not complicate the original problem.If the optimization stays within \(:=\{A^{} _{k}(A-A^{})(-,),\, k\}\), the mapping \(_{}\) map different elements in \(\) to different elements on \(_{}\). This means that each update in \(\) results in a unique outcome in \(_{}\), thereby reducing unnecessary redundant searches. Furthermore, the mapping \(_{}\) does not introduce spurious local minima, as each local minima in \(_{}\) creates a single local minima in \(\).

The efficient optimization process.At first glance, the mapping \(_{}\) involves matrix exponential and matrix power, which might demand substantial computational resources during the optimization process. Lezcano-Casado and Martinez-Rubio  has proposed a cheap method for computing matrix exponential \(()\) and its gradient, thanks to the efficient utilization of the scaling-squaring technique and Pade approximation . Therefore, we will focus on how to handle the matrix power function efficiently.

* **Forward process.** The orthogonal matrix \(O\) can be factorized, utilizing eigendecomposition, as \(O=QXQ^{-1}\), where \(Q^{n n}\) with each column representing an eigenvector of \(O\), and \(X=(\{_{1},,_{n}\})\) is a diagonal matrix whose elements are the eigenvalues of \(O\). In this case, the matrix power \(O^{}\) can be computed by applying the power function to the eigenvalues while keeping the eigenvectors unchanged , yielding \(O^{}=Q(\{_{1}^{},,_{n}^{}\} )Q^{-1}\). It is evident that calculating \(O^{}\) is not significantly more complex than computing \(n\) scalar powers.
* **Backward process.** Given an orthogonal matrix \(O\), we assume that \(=_{}(O)\) has been obtained through Equation (11). Then, there exists a unique orthogonal matrix \(W_{}\) such that \(=W_{}O\) due to the closure property of the Lie group. Therefore, in the forward pass, one can initially acquire \(\) using Equation (11), followed by computing the equivalent transformation of the mapping \(_{}\) as \(W_{}=O^{}\). In this way, the forward pass is streamlined into \(=W_{}O\), thereby rendering the backward pass highly efficient, as it only involves one linear transformation.

### Re-parameterization provides stochastic optimization

In the previous section, we considered deterministic optimization over permutation matrices. However, in many scenarios, we commonly build a probabilistic model to express the uncertainty inherent in the problem . For such a task, it is crucial to have the ability to learn latent variable models associated with the latent nodes corresponding to permutations . This section demonstrates how to perform stochastic optimization over the latent permutations using \(\) and the re-parameterization trick.

We restrict our attention to the scenario where the latent variable is a permutation matrix, \(z=P\), without loss of generality. Therefore, consider the probabilistic form of Equation (1) as follows

\[_{P q(P;)}f(P).\] (14)

The above equation deals with a distribution over permutation matrices rather than a single permutation matrix as in Equation (1). Evaluating and differentiating Equation (14) is challenging due to the expectation involving a sum of \(n!\) terms. To remedy this, we employ the re-parameterization trick [32; 56; 14]. In particular, we simulate \(q(P;)\) using the mappings \(()\) w.r.t. Equation (7) and \(()\) w.r.t. Equation (11), expressed as

\[P q(P;) P=((A+B))\,\,\,\,:=\{A,B^{}\},\] (15)

where \( q()\) is a random noise distribution. Equation (15) cleverly decouples the stochastic nature of the distribution \(q(P;)\), thereby obviating the dependence of the expectation on the parameters \(\). This enables us to draw multiple samples for the evaluation of Equation (14) with lower variance.

However, there exists a not differentiable mapping \(()\), which hinders gradient-based optimization for \(\). Recalling \(\), we can approximate Equation (15) by relaxing the mapping \(()\) to \(_{}()\). It is evident that samples drawn from distribution \(_{}((A+B))\) converge almost surely to those from distribution \(((A+B))\) owing to \(_{ 0}_{}=\). To summarize, we can bring the gradient inside the expectation, as depicted below:

\[_{ q()}f(_{}((A+B )))=_{ q()} f(_{ }((A+B))),\] (16)

which can now be computed using Monte Carlo .

## 4 Experiments

This section conducts experiments to evaluate the performance of \(\) in optimization problems and probabilistic tasks. All experimental details not stated here, along with additional results, can be found in Appendix F. The core code for \(\) is available at https://github.com/YamingGuo98/OT4P.

### Finding mode connectivity

In the first experiment, we consider an optimization problem inspired by the concept of linear mode connectivity. Recent studies have shown that neural networks trained with SGD belong to a set whose weights can be permuted so that we linearly connect those weights with no detriment to the loss [13; 57]. For demonstration purposes, we examine a multi-layer perceptron (MLP) with \(L\) layers and denote its weights as \(=\{W_{l}|l[L]\}\). To find the optimal permutation between models \(_{A}\) and \(_{B}\), Ainsworth et al.  propose the following data-free optimization problem:

\[_{=\{P_{i}_{n_{i}}\}}\|W_{1}^{(A)}-P_{1}W_{1}^{(B)}\|_{ }^{2}+\|W_{2}^{(A)}-P_{2}W_{2}^{(B)}P_{1}^{}\|_{}^{2} ++\|W_{L}^{(A)}-P_{L}W_{L}^{(B)}\|_{}^{2}.\] (17)

The above problem is challenging because it does not admit a polynomial-time constant-factor approximation scheme . We can use \(\) to relax permutation matrices, as demonstrated in Section 3.2, enabling a gradient-based solution to Equation (17).

We explore a variety of network architectures, including MLP5 (5-layer MLP) , VGG11 , and ResNet18 . The weights for these networks are derived from official pre-trained models in PyTorch , with the exception of the MLP5, which is initialized randomly. For model \(_{B}\), we randomly sample permutation matrices from a uniform distribution and apply them to permute the weights, yielding model \(_{A}=(_{B})\). The AdamW  with an initial learning rate of \(0.1\) is employed to minimize the loss w.r.t. Equation (17), with a maximum of \(500\) iterations. To evaluate the results, we use the \(_{1}\)-Distance, \(\|_{A}-(_{B})\|_{1}\), to measure the difference from the target weights. Additionally, we flatten the permutation matrices and evaluate their alignment with the ground truth using Precision, Recall, and Hamming Distance.

We compare: 1) Weight Matching, which goes through each layer and greedily selects its best permutation matrix \(P_{i}\); 2) Sinkhorn, relaxing the permutation matrices to the vicinity of the Birkhoff polytope utilizing the Sinkhorn operator [61; 49]; and 3) \(\), our proposed method, which is evaluated with various temperature parameters. The results are reported in Tables 1 and 4, with each experiment conducted five times. The findings indicate that Weight Matching occasionally fails to reach ground truth due to its sensitivity to random initialization. Additionally, Sinkhorn yields poor results in the VGG11 network architecture, which we attribute to the relaxation on the Birkhoff polytope producing unreliable local minima. In contrast, \(\) finds the optimal permutation matrix in most cases. Indeed, a neural network can be conceptualized as a geometric object whose vertices correspond to the rows of the weight matrices . A reasonable relaxation of the matching task in Equation (17) involves rigid transformations represented by orthogonal matrices. The proposed \(\) relaxes the permutation matrices into the orthogonal group, which is likely the primary reason for its powerful performance.

### Inferring neuron identities

In the second experiment, we tackle a probabilistic task motivated by the study of the neural dynamics in C. elegans . This worm serves as a model organism in neuroscience, with its complete neuronal connectivity known and represented by the adjacency matrix \(A\{0,1\}^{n n}\). However, matching traces from the observed neural dynamics \(Y^{n 1}\) to the neurons in the reference connectome \(A\) poses a challenging task. Linderman et al.  propose simulating neural activity using a linear dynamical system \(Y_{t}=P(A W)P^{}Y_{t-1}+\), where \(\) is Gaussian noise, \(W^{n n},P_{n}\) are latent variables, and \(\) is element-wise product. Our goal is to infer the latent permutation \(P\) to align the observed \(Y\) with the shared dynamics matrix \(W\). We address this task by maximizing the marginal log-likelihood, i.e., \(_{P q(P;)} p(Y|P)\), using the techniques outlined in Section 3.3.

Taking the methodology in Linderman et al. , we generate parameters \(A\), \(W\), and \(P\) with \(n=250\) and randomly generate \(1000\) samples, where the noise follows a Gaussian distribution \((0,0.01)\). We formulate tasks of varying difficulty depending on the different proportions of known neurons . Conceiving a constraint matrix \(C^{n n}\) where all elements are initialized to \(1\), if we ascertain that the reference neuron \(i\) corresponds to the observed neuron \(j\), then set all elements to \(0\) in the \(i\)-th row and \(j\)-th column except for \(C_{i,j}\) (see Equation (22) for an example). This constraint is enforced by zeroing corresponding entries before solving Equation (7). We conduct \(500\) iterations using the Adam optimizer  with an initial learning rate of \(0.01\). We report the marginal log-likelihood of the best model throughout the training, ranked first by Hamming Distance and then by the marginal log-likelihood (estimated with \(5\) repeats). As done in Section 4.1, Precision, Recall, and Hamming Distance are utilized to evaluate the permutation matrices obtained from the best model.

For comparison, we include: 1) Naive, which does not enforce that \(P\) is a permutation matrix and instead normalizes each row using the softmax function; 2) Gumbel-Sinkhorn, introducing Gumbel noise for re-parameterization before the Sinkhorn operator; and 3) OT4P, our proposed method using the re-parameterization trick, with different temperature parameters. Each experiment is conducted five times, and the results are presented in Tables 2 and 5. We observe that Naive fails to produce any meaningful solutions, and Gumbel-Sinkhorn performs poorly in the more challenging scenario (Known \(5\%\)). In contrast, OT4P consistently identifies the optimal permutation, except for OT4P (\(=0.7\)), which achieves suboptimal results in the Known \(5\%\) setting. One possible reason why OT4P performs better is that the orthogonal group (\(\)) has a lower dimension than the Birkhoff polytope (\((n-1)^{2}\)). This lower dimensionality makes the randomness simulated by the noise more effective in exploring latent permutations.

### Solving permutation synchronization

In the third experiment, we aim to explore the effectiveness of our proposed OT4P on large-scale problems. We specifically focus on the permutation synchronization problem [51; 47; 5], which tries to improve matching across multiple objects. Consider \(k\) objects with \(n\) points each, and let the permutation matrix \(P(i,j)_{n}\) to represent the correspondence between points in objects \(i\) and

    &  &  &  \\   & \((1+_{1})()\) & Precision (\(\)) & \((1+_{1})()\) & Precision (\(\)) & \((1+_{1})()\) & Precision (\(\)) \\  Weight Matching & \(0.000 0.00\) & \(100.0 0.00\) & \(0.000 0.00\) & \(100.00 0.00\) & \(1.215 2.72\) & \(99.97 0.06\) \\ Sinkhorn & \(0.000 0.00\) & \(100.0 0.00\) & \(11.61 0.07\) & \(63.08 3.14\) & \(9.830 1.081\) & \(95.56 0.88\) \\  OT4P (\(=0.3\)) & \(0.000 0.00\) & \(100.0 0.00\) & \(0.000 0.00\) & \(100.00 0.00\) & \(0.000 0.00\) & \(100.0 0.00\) \\ OT4P (\(=0.5\)) & \(0.000 0.00\) & \(100.0 0.00\) & \(0.818 1.83\) & \(99.99 0.03\) & \(0.000 0.00\) & \(100.0 0.00\) \\ OT4P (\(=0.7\)) & \(0.000 0.00\) & \(100.0 0.00\) & \(0.000 0.00\) & \(100.00 0.00\) & \(0.000 0.00\) & \(100.0 0.00\) \\   

Table 1: \(_{1}\)-Distance (converted by \((1+x)\)) and Precision (\(\%\)) of algorithms for finding mode connectivity across different network architectures.

    &  &  &  \\   & \( p(Y|P)()\) & Precision (\(\)) & \( p(Y|P)()\) & Precision (\(\)) & \( p(Y|P)()\) & Precision (\(\)) \\  Naive & \(-3040 43.4\) & \(8.960 7.85\) & \(-2917 225\) & \(29.68 17.2\) & \(-1690 539\) & \(78.40 12.6\) \\ Gumbel-Sinkhorn & \(-2256 574\) & \(62.08 16.0\) & \(-239.8 119\) & \(98.16 19.5\) & \(-144.8 27.1\) & \(99.84 0.358\) \\  OT4P (\(=0.3\)) & \(-130.9 10.9\) & \(100.0 0.00\) & \(-127.5 11.1\) & \(100.00 0.00\) & \(-126.7 11.0\) & \(100.0 0.00\) \\ OT4P (\(=0.5\)) & \(-164.0 36.8\) & \(100.0 0.00\) & \(-149.7 25.0\) & \(100.0 0.00\) & \(-148.2 27.6\) & \(100.0 0.00\) \\ OT4P (\(=0.7\)) & \(-829.3 831\) & \(74.16 35.9\) & \(-183.1 46.2\) & \(100.0 0.00\) & \(-171.8 40.3\) & \(100.0 0.00\) \\   

Table 2: Marginal log-likelihood and Precision (%) of algorithms for inferring neuron identities across different proportions of known neurons.

\(j\). Permutation synchronization seeks to identify the underlying permutations \(P_{i}(i[k])\) such that \(P(i,j)=P_{i}P_{j}^{}\) for all \(i,j[k]\), which can be expressed as the following optimization problem:

\[_{\{P_{i}_{n}\}}_{i,j}^{k} P(i,j)-P_{i}P_{j}^{} _{}^{2}.\] (18)

We select baselines: 1) Reg optimizes in Euclidean space with a regularization term \(_{j}(_{j}P_{i,j}-1)^{2}\) that encourages each column to sum to \(1\). 2) OrthReg  optimizes over the special orthogonal group, using a regularization term \((P^{T}(P-P P))\) (\(\) is element-wise product) to force orthogonal matrices to converge to permutation matrices. 3) RiemanBirk  optimizes on Birkhoff polytope utilizing Riemannian gradient descent. 4) Sinhhorn  optimizes in the vicinity of the Birkhoff polytope, using the Sinkhorn operator to adjust positive matrices into approximate doubly stochastic matrices. All algorithms employ the Adam optimizer for \(100\) iterations, with RiemanBirk utilizing Riemannian Adam . The initial learning rates are tuned within the set \(\{0.1,0.01,0.001,0.0001\}\).

We use the WILLOW-ObjectClass dataset  to generate problem instances (see Appendix F.4 for more details) and utilize the F-score to evaluate the alignment between the flattened permutation matrices and the ground truth. Each experiment is conducted five times, and the results are shown in Figure 3. RiemanBirk and Sinkhorn demonstrate poorer performance. A primary reason is that both methods are based on Birkhoff polytope to relax permutations, leading to unreliable local minima and preventing optimal solutions. Benefiting from the potential advantages offered by the orthogonal group, OrthReg generally produces competitive results. However, due to the instability of its regularization term, OrthReg sometimes underperforms, which may necessitate careful adjustment of the regularization coefficient for each class. In contrast, our proposed OT4P consistently outperforms other methods and demonstrates robustness to variations in the hyperparameter \(\).

## 5 Conclusion

In this paper, we present a novel differentiable transformation, OT4P, designed for relaxing permutation matrices over the orthogonal group. This method is characterized by its flexibility, simplicity, and scalability. OT4P is utilized to parametrize the relaxation of permutation matrices, with advantages of not altering the original problem, not complicating the original problem, and an efficient optimization process. By deriving a gradient estimator, OT4P further provides an efficient tool for stochastic optimization over latent permutations. Extensive experiments show that OT4P achieves competitive results in optimization problems and probabilistic tasks compared to relaxation methods on the Birkhoff polytope. We believe our elementary work is a significant step toward relaxing permutations onto the orthogonal group. Please see Appendix A for further discussion, including related work, limitations, and broader impacts.

Figure 3: F-scores (\(\%\)) for different algorithms on the WILLOW-ObjectClass dataset, where the size of permutation synchronization problem instances varies along the horizontal axis.