# SLM: A Smoothed First-Order Lagrangian Method for Structured Constrained Nonconvex Optimization

Songtao Lu

IBM Research, Thomas J. Watson Research Center

Yorktown Heights, NY 10598

songtao@ibm.com

###### Abstract

Functional constrained optimization (FCO) has emerged as a powerful tool for solving various machine learning problems. However, with the rapid increase in applications of neural networks in recent years, it has become apparent that both the objective and constraints often involve nonconvex functions, which poses significant challenges in obtaining high-quality solutions. In this work, we focus on a class of nonconvex FCO problems with nonconvex constraints, where the two optimization variables are nonlinearly coupled in the inequality constraint. Leveraging the primal-dual optimization framework, we propose a smoothed first-order Lagrangian method (SLM) for solving this class of problems. We establish the theoretical convergence guarantees of SLM to the Karush-Kuhn-Tucker (KKT) solutions through quantifying dual error bounds. By establishing connections between this structured FCO and equilibrium-constrained nonconvex problems (also known as bilevel optimization), we apply the proposed SLM to tackle bilevel optimization oriented problems where the lower-level problem is nonconvex. Numerical results obtained from both toy examples and hyper-data cleaning problems demonstrate the superiority of SLM compared to benchmark methods.

## 1 Introduction

In this paper, we consider a structured constrained optimization framework that has broad applications in diverse machine learning problems. The goal of this problem is to minimize the objective function using two blocks of optimization variables which are coupled in the functional constraint. More formally, we express this class of nonlinear constrained programming problems as follows:

\[_{x,y}f(x,y), g(x,y)-g^{*}(x)\] (1)

where feasible set \(\) is convex and compact, the objective function \(f(x,y)\) is smooth and nonconvex, constraint \(g(x,y)\) is smooth and nonconvex and it also satisfies the Polyak-Lojasiewicz (PL) condition with respect to (w.r.t.) \(y\), \(g^{*}(x)_{y}g(x,y)\) (which is also called value function ) is obtained by minimizing \(g(x,y)\) over \(y\), and constant \(>0\).

When \(=0\), problem (1) reduces to the classic bilevel optimization (BO) problem (aka mathematical programs with equilibrium constraints ). More specifically, it takes the form of

\[_{x,y}f(x,y), y(x) _{y^{}}g(x,y^{}),\] (2)

where \(f(x,y)\) and \(g(x,y^{})\) denote the upper-level (UL) and lower-level (LL) objective functions, \((x)\) represents the set that contains the global optimal solutions of the LL problem w.r.t. the block-\(y^{}\). This BO formulation has been shown to be a useful framework for modeling various multi-taskmachine learning problems, e.g., hyper-parameter optimization [3; 4], actor-critic schemes in reinforcement learning , meta-learning [6; 7], AUC maximization , distributed bilevel optimization [9; 10; 11], etc. In these scenarios, the decision/optimization variables are coupled in both levels of the minimization problems and the evaluation of the UL gradient depends on the optimal solution at the LL. Consequently, it is necessary to optimize the UL and LL loss functions jointly w.r.t. the block variables \(x\) and \(y\).

### Related Work

**Bilevel Optimization**. Motivated by the numerous applications of BO, developing the efficient solvers for handling these problems has attracted great interest in the optimization community. There are two major directions: 1) focus on solving the BO problem in a general manner; 2) improve the computation or iteration complexity of the BO algorithms. It has been shown in  that a first-order method can find the global optimal solution when the UL objective function is strongly convex and the LL problem is convex. However, the strong convexity assumption restricts the applicability of the algorithm to many real-world machine learning problems. In contrast, some recent works have considered the case where the UL objective function is nonconvex and have shown that, given an oracle that computes the Jacobian and the inverse of Hessian matrices of the LL objective function, the complexity of finding an \(\)-stationary point is \((^{-1})\)[18; 13], which is the same as that of gradient descent. But this setting is still restrictive as they still assume that the LL problem is strongly convex.

**Non-Singleton at the LL**. When the LL problem is not strongly convex, the computation of the UL gradient \( f(x,(x))\) can become ill-posed due to the presence of multiple optimal solutions. To address this issue, a new metric called \((,)\)-Goldstein stationary point was introduced in , which aims to capture the discontinuity of \( f(x,(x))\). Under the assumption that \(f(x,)\) and \(g(x,)\) are convex, it was shown that an inexact gradient-free method can find the Goldstein stationary point at a rate of \((^{-1}^{-4})\). In addition to the new metric used for theoretical convergence analysis, the concept of Holderian error bound was employed in , which imposes further conditions on the shape of the LL objective function. For a class of simple BO problems where only one set of block variables needs to be optimized, the conditional gradient-based bilevel optimization (CG-BiO) algorithm proposed in  was shown to converge to stationary points at a rate of \((^{-1})\) or \((^{-2})\), depending on whether the Holderian error bound holds.

In , the author considered the case of a nonconvex LL objective function but with an assumption that eliminates the presence of zero eigenvalues of the LL Hessian matrix. Another property used to characterize the landscape of the LL function is the PL condition. Based on the formulation in (1), researchers [16; 15] developed penalty methods for solving BO problems and discussed the relationship between local/global optimal solutions of the penalized problem formulation and the constrained problem (2) (i.e., when \(=\) in (1)). It was shown that the value-gap-based penalty-based bilevel gradient descent (V-PBGD) algorithm  can converge to stationary points of the reformulated single-level problem. Additionally, under the assumption of the constant rank con

   Algorithms & Problem & Opt. Framework & Oracle & Non-Singleton & LL/C & Rate \\  ConEx  & & primal-dual & first-order & ✓ & ncvx & \((})\) \\ ITD/AID  & (2) & ITD/AID & second-order & & & scvx & \(()\) \\ F\({}^{2}\)SA  & (2) & penalty & first-order & & & scvx & \((})\) \\ BOME  & (2) & penalty & first-order & & & PL & \((})\) \\ V-PBGD  & (2) & penalty & first-order & ✓ & PL & \(}()\) \\ 
**SLM** & (1) & primal-dual & first-order & ✓ & PL & \(}()\) \\   

Table 1: Comparison of existing related representative works on nonconvex functional constrained and/or bilevel optimization, where non-singleton: the existence of multiple global optimal solutions at the LL problem, oracle: requirements of accessing first-order and/or second-order derivative of either UL or LL loss functions, LL/C: the property that the LL objective funciton/constraint satisfies in problem (2) or (1), \(}\): denotes that the iteration complexity hides the dependence on the hyperparameters, e.g., \(\), cvx: convex, ncvx: nonconvex, scvx: strongly convex.

straint quantification condition, the bilevel optimization made easy (BOME) algorithm developed in  can achieve Karush-Kuhn-Tucker (KKT) points of the BO problem in (2).

**Computational Issue**. Another line of research focuses on reducing the computational complexity of implementing BO optimization algorithms. As mentioned earlier, computing the UL gradient can be computationally expensive. Existing works have employed strategies such as the Neumann series for calculating the inverse of Hessian matrices , or utilized reverse-mode iterative differentiation (ITD) and approximate implicit differentiation (AID) techniques [22; 13] for hypergradient computation. However, these strategies introduce a double-loop structure in the algorithm design, requiring users to specify hyperparameters for the inner loop subroutine. Subsequently, single-loop BO algorithms have been developed [23; 24], which alleviate the burden of hyperparameter tuning in the inner loop. Nevertheless, they still require the computation of the Jacobian matrix and Hessian-vector product.

To circumvent these computations, some existing works reformulate the BO problem as a single-level functional constrained optimization (FCO) problem, as shown in (1), and apply well-established constrained optimization techniques, such as primal-dual methods, for solving the BO problem using only gradients. In , it was shown that when the UL problem is strongly convex and the LL problem is convex, the primal-dual algorithm can find the global optimal solution at a rate of \((^{-1/2})\). For the case where the UL problem is nonconvex, the fully first-order stochastic approximation (F\({}^{2}\)SA) method proposed in  can find an \(\)-stationary solution at a rate of \((^{-3/2})\) when the LL problem is strongly convex. Similarly, penalty methods [26; 16] also just require the computation of UL and LL gradients while achieving the desired optimization results.

**Functional Constrained Optimization**. The formulation based on function values establishes a connection between FCO and BO [1; 27; 28]. By leveraging existing well-developed constrained optimization methods [12; 29; 30], efficient solutions to the BO problem can still be obtained. When the block variables \(x\) and \(y\) are linearly coupled, the alternating direction method of multipliers (ADMM)  has emerged as a successful approach by decoupling the optimization process into subproblems. Recent advancements in smoothed augmented Lagrangian methods [30; 32; 33] have improved the convergence rates for solving nonconvex linearly constrained problems. For nonlinear functional constraints with one block of variables, the constraint extrapolation (ConEx) method  has shown promising results in finding KKT points with a convergence rate of \((^{-3/2})\) for nonconvex smooth functions. Besides, the properties of PL functions studied in min-max optimization problems  are expected to be useful in demonstrating the continuity of \( f(x,(x))\) for BO problems within the framework of primal-dual methods. A comparison between our work and closely related previous works on FCO and BO is shown in Table 1.

### Main Contributions of This Work

In this work, we propose a novel approach called the smoothed first-order Lagrangian method (SLM) to tackle the FCO problem presented in (1). The key advantage of SLM is that it only requires the computation of gradients, making it computationally efficient. Remarkably, under mild assumptions and a local regularity condition, we establish a strong error bound for SLM to solve this class of problems, and prove that SLM can converge to the KKT points even when the nonconvex objective in the constraint has multiple optimal solutions. To summarize, the main contributions of this work are highlighted as follows:

* This class of structured FCO problems is generic and can be used to formulate multiple machine learning problems. The proposed SLM is computationally efficient as it relies only on the gradient of the objective functions. Moreover, the obtained convergence rate of the proposed SLM, which is \((^{-1})\), is optimal for finding the \(\)-KKT points of this class of problems.
* SLM can be applied to solve nonconvex bilevel related optimization problems in the presence of nonconvexity and non-singleton at the lower level, when the LL objective function satisfies the PL condition.
* The numerical results showcase the competitiveness of the proposed SLM compared to state-of-the-art BO algorithms in both a toy example and a data hyper-cleaning problem w.r.t. convergence speed and achievable test accuracy.

Due to space constraints, all technical proofs are provided in the supplementary material.

Notations: The distance between \(y\) and the set \((x)\) is defined as \(d_{(x)}(y)_{y^{}(x)}\|y-y^{}\|\). A high-dimensional ball with a radius of \(R\) is denoted as \((R)\).

## 2 Smoothed First-Order Lagrangian Method

In this section, we will introduce the three-layer structure of our first-order method designed to solve problem (1). Towards this end, we can write the Lagrangian of this constrained problem as follows:

\[(x,y;) f(x,y)+(g(x,y)-g^{*}(x)-)\] (3)

where the nonnegative \(\) denotes the Lagrange multiplier or dual variable.

Next, it is natural to have

\[_{x}(x,y;) =_{x}f(x,y)+(_{x}g(x,y)-_{x}g^{*}(x)),\] (4a) \[_{y}(x,y;) =_{y}f(x,y)+_{y}g(x,y),\] (4b) \[_{}(x,y;) =g(x,y)-g^{*}(x)-.\] (4c)

In contrast to the classical primal-dual algorithm design, here, \(g^{*}(x)\) is typically unknown. Based on the definition of \(g^{*}(x)\), it is not hard to know that obtaining a closed-form expression for \( g^{*}(x)\) requires the computation of the Jacobian matrix and the inverse of Hessian matrice even for the case when \(g(x,y)\) is strongly convex w.r.t. \(y\). Although many existing works have provided the iterative methods of computing these quantities for each iteration [13; 23; 24], the calculational cost and/or memory budget are still high for large-scale problems. Hence, it is motivated to have an efficient way of approximating the value and gradient of \(g^{*}(x)\) in the algorithm design.

### Bottom-Layer

One of the most straightforward approaches is to generate a sequence \(u_{t}\) indexed by \(t\) as an inner loop to obtain \(u^{*}(x)_{u}g(x,u)\). When the objective function \(g(x,y)\) satisfies certain properties, such as the PL condition or the strongly convex condition, applying gradient descent can lead to finding the global optimal solution of this subproblem with a linear convergence rate .

Let \(r\) denote the index of the outer iteration steps. Then, the approximate of \(u^{*}(x)\) at each iteration, i.e., \(u^{r}\), can be estimated through the following process:

\[u^{r}_{t+1}=u^{r}_{t}-_{u}g(x^{r},u^{r}_{t}),\] (5)

where \(u^{r}_{t}\) represents the \(t\)th iterate of the inner loop, \(\) denotes the step-size, and \(u^{r}_{0}\) is initialized within a bounded ball (which will ensure the boundedness of the iterates). After running this algorithm for \(T^{r}\) iterations at each step, we set \(u^{r+1}=u^{r}_{T^{r}}\) for updating the subsequent optimization variables.

### Medium-Layer

Given that the quality of \(g^{*}(x)\) can be guaranteed, the most interesting part lies in solving the constrained problem w.r.t. block \(y\). Even if we assume that \(g(x,y)\) follows the PL condition, the nonconvexity of \(f(x,y)\) still poses challenges for the first-order methods in finding the optimal solution. To address this, we propose adding the quadratic proximal terms that renders the Lagrangian w.r.t. \(x\) and \(y\) strongly convex, as follows:

\[K(x,y,w,z;)(x,y;)+\|x-w\|^{2}+ \|y-z\|^{2}.\] (6)

Here, \(K(x,y,w,z;)\) can be regarded as a modified Lagrangian for this problem. Let \(L_{f}\) and \(L_{g}\) denote as the smoothness constants of \(f\) and \(g\), respectively. Choosing a sufficiently large value for \(p\) (specifically, \(p=(L_{f}+ L_{g})\)) ensures that \(K(x,y,w,z;)\) becomes strongly convex.

Subsequently, we can develop the following gradient-based primal-dual algorithm for solving this constrained subproblem:

\[^{r+1} =_{+}[^{r}+_{} (x^{r},y^{r},w^{r},z^{r},u^{r+1};^{r})],\] (7a) \[y^{r+1} =y^{r}-_{y}K(x^{r},y^{r},w^{r},z^{r};^{r+1}),\] (7b) \[z^{r+1} =z^{r}+(y^{r+1}-z^{r}),\] (7c)

where \((x,y,w,z,u;)}(x,y,u;) +\|x-w\|^{2}+\|y-z\|^{2}\) and \(}(x,y,u;) f(x,y)+(g(x,y)-g(x,u)-)\), \(,,\) are the step-sizes, and \(_{+}\) denotes the projection of the dual variable onto a box constraint. In particular, \(_{+}[]\) is given by \(_{+}[]=0\) if \( 0\); \(_{+}[]=\) if \(0<\); \(_{+}[]=\) if \(>\).

_Remark 1_.: The auxiliary sequence \(z^{r}\) is defined as an exponentially weighted average sequence of \(y^{r+1}\), which has been previously used in smoothed gradient descent ascent for nonconvex min-max optimization [36; 37] problems and in the smoothed proximal augmented Lagrangian method for linearly constrained nonconvex objective problems . However, to the best of our knowledge, this is the first time that the smoothed primal-dual method is applied to solve nonconvex objective problems with nonlinear functional constraints.

### Top-Layer

After updating the variables \(y\), \(z\), \(u\), and \(\), the variables \(x\) and \(w\) are updated as follows:

\[x^{r+1} =_{}[x^{r}-_{x}(x ^{r},y^{r+1},w^{r},z^{r+1},u^{r+1};^{r+1})],\] (8a) \[w^{r+1} =w^{r}+(x^{r+1}-w^{r}),\] (8b)

where \(_{}\) denotes the projection operator, and \(\) is the step-size.

The detailed algorithm description is provided in Algorithm 1.

```
1:step-sizes: \(,,,,\), variables: \(x^{1},w^{1},y^{1},z^{1},^{1}\)
2:for\(r=1,2,,T\)do
3:for\(t=0,1,2,,T^{r}-1\)do
4: Initialize \(u_{0}^{r}(R)\)
5:\(u_{t+1}^{r}=u_{t}^{r}-_{u}g(x^{r},u_{t}^{r})\)\(\) bottom layer
6:endfor
7:\(u^{r+1}=u_{T^{r}}^{r}\)
8:\(^{r+1}=P_{+}[^{r}+(g(x^{r},y^{r})-g(x^{r},u^{r+1 })-)]\)\(\) dual variable update
9:\(y^{r+1}=y^{r}-(_{y}f(x^{r},y^{r})+^{r+1}_{y}g(x ^{r},y^{r})+p(y^{r}-z^{r}))\)\(\) medium layer
10:\(z^{r+1}=z^{r}+(y^{r+1}-z^{r})\)
11:\(x^{r+1}=_{}\![x^{r}\!-\!(_{x}f(x^{r},y^{r+1})\!+\!^{r+1}(_{x}g(x^{r},y^{r+1})\!-\!_{x}g(x^{r}, u^{r+1}))\!+\!p(x^{r}-w^{r})\!)]\)
12:\(w^{r+1}=w^{r}+(x^{r+1}-w^{r})\)\(\) top layer
13:endfor ```

**Algorithm 1** Smoothed first-order **L**argrangian **M**ethod (SLM)

## 3 Theoretical Convergence Results

Before presenting the theoretical convergence guarantees of SLM, it is necessary to introduce the following main classes of assumptions. These assumptions are essential in establishing the descent properties of some quantifiable (potential) function, enabling SLM to converge to KKT points of this class of nonconvex FCO problems. More detailed definitions and properties regarding these assumptions are deferred to the supplement.

### Assumptions

The assumptions are mainly related to the continuity and boundedness of the objective function, as well as the mathematical property of the constraint.

1. (Smoothness) Assume that functions \(f(x,y),g(x,y)\) are differentiable and jointly smooth with constants \(L_{f},L_{g}\) w.r.t. both \(x,y\).
2. (Compactness) The feasible set \(\) (projection friendly) is convex and compact.
3. (Boundedness) Assume that the objective function \(f(x,y)\) is lower bounded and denoted as \(\).
4. (Coercivity) The set \(\{y|f(x,y) R,g(x,y)-g^{*}(x)\}\) is bounded for any \(R>0\).
5. (PL condition) Function \(g(x,y)\) satisfies the PL condition, i.e., there exists a constant \(_{g}\) such that \(\|_{y}g(x,y)\|^{2} 2_{g}(g(x,y)-g^{*}(x)), x,y\).

_Remark 2_.: Assumptions A1-A3 are standard in the optimization literature, while A4, also referred to as coerciveness, is a commonly used assumption in theoretical convergence analyses to guarantee boundedness of the iterates, as observed in methods like smoothed-GDA , ADMM , BO algorithms [39, Assumption 3], asynchronous algorithms [40, Assumption 5], etc. In practice, regularization terms are often included in the objective function, which can ensure that the level set is bounded . For instance, as suggested in , introducing a small \(_{2}\)-penalty to a non-negative loss (e.g. cross-entropy or mean-squared loss) can establish boundedness for both the level set and the iterates. An example involving neural networks can be found in [42, Theorem 2]. A5 is a reasonable and practical assumption for neural network applications. Previous studies, such as , have demonstrated theoretically that when neural networks are overparametrized, the loss function satisfies the PL condition. Other examples include the nonconvex discounted return objective in reinforcement learning , and the loss function of a specific class of linear quadratic regulator models [45, Lemma 3].

_Local Regularity Condition._ Let \(^{*}(w,z),^{*}(w,z)\) be the KKT solution of problem \(_{x,y}f(x,y)+\|x-w\|^{2}+\|y-z\|^{2},\;g(x,y) g^{*}(x)+\). Then, there exist positive constants \(\) and \(\) such that when \(\|w-^{*}(w,z)\|^{2}+\|z-^{*}(w,z)\|^{2}\), the following inequality holds \(g(^{*}(w,z),^{*}(w,z)) g^{*}(^{*}(w,z))+(w,z)\), where \((w,z)\).

_Remark 3_.: Given the feasibility of achieving the global optimal solution of \(g^{*}(^{*}(w,z))\) and the requirement for variable \(y\) to minimize both objective functions, it is reasonable to expect that \(^{*}(w,z)\) can be close to \( g(^{*}(w,z),y)\) up to some very small constant. It is worth noting that this assumption is fairly mild, as we only require this condition to hold in the neighborhoods of the KKT points.

Given these assumptions, we are now in a position to provide the following theoretical convergence guarantees for SLM.

### Convergence Rates of SLM

Let \((x^{r},y^{r})\) be defined as \((x^{r},y^{r})[^{-1}(x^{r}-_{}(x ^{r}-_{x}(x^{r},y^{r};^{r})));_{y}(x^{r},y^{r};^{r})]\). We denote \(\|(x^{r},y^{r})\|\) as the stationary gap.

**Theorem 1**.: _(Convergence Rate of SLM to the KKT Points of problem (1)) Suppose that A1-A5 are satisfied, and the local regularity condition holds. Assume that the iterates \(\{x^{r},y^{r},w^{r},z^{r},u^{r},^{r}\}\) are generated by SLM. If the step-sizes are chosen as \(,(1/^{r})\), \(=(1/L_{g})\), \(p=(^{r})\), \(=(1)\), \(=(^{1.5})\) when \(0<<1\), and \(=(1)\) when \(>1\), and \(T^{r}=((r^{r}))\), \((1/)\), then, \(=()\) and the following results hold_

1. _Every limit point of_ \(\{x^{r},y^{r},w^{r},z^{r},u^{r},^{r}\}\) _generated by SLM is a KKT point of problem (_1_)._
2. \[_{r=1}^{T}\|(x^{r},y^{r})\|^{2}= (),\] (9a) \[_{r=1}^{T}|g(x^{r},y^{r})-g^{*}(x^{r})-|_{+}^{2}= ()\] (9b) \[_{r=1}^{T}|(g(x^{r},y^{r})-g^{*}(x^{r})-) ^{r}|^{2}=()\] (9c) _where_ \[||_{+}\] _takes the positive part, and_ \[T\] _denotes the total number of iterations._

_Remark 4_.: From Theorem 1, we can conclude that the iteration complexity of SLM to reach an \(\)-KKT stationary point is \((1/)\). It is also worth noting that this convergence rate is optimal in terms of \(\), as it matches the lower bound established by the first-order method for finding an \(\)-stationary point of nonconvex smooth optimization problems .

**Corollary 1**.: _(Convergence Rate of SLM to \(\)-KKT Solutions of the BO problem (2)) Suppose that A1-A5 hold and the local regularity condition holds. Assume that the iterates \(\{x^{r},y^{r},w^{r},z^{r},u^{r},^{r}\}\) are generated by SLM. Given the condition that \(=\) and assuming \(=\), if the step-sizes \(,=(1/^{r})\), \(=(^{1.5})\), \(=(1/L_{g})\), \(=(1)\), \(T^{r}=((1/))\) and \(p,=(1/)\), then \(=()\) and the iteration complexity of SLM for finding an \(\)-KKT stationary solution is \((1/^{3.5})\)._

_Remark 5_.: Here, an \(\)-KKT stationary solution of this FCO problem refers to a point \(\{x^{*},y^{*}\}\) that satisfies \(\|(x^{*},y^{*})\|^{2}\) and \(|g(x^{*},y^{*})-g^{*}(x^{*})-|_{+}^{2}\), and \(|(g(x^{*},y^{*})-g^{*}(x^{*})-)^{*}|^{2}\).

## 4 Proof Sketch

In this section, we will present the main theorem proving techniques employed to establish the results in Theorem 1. Let

\[D(w,z;) _{x,y}K(x,y,w,z;),\] (10a) \[P(w,z) _{x,y}_{0}K(x,y, w,z;),\] (10b) \[x^{*}(w,z;),y^{*}(w,z;) _{x,y}K(x,y,w,z;),\] (10c) \[^{*}(w,z),^{*}(w,z) _{x,y}_{0 }K(x,y,w,z;).\] (10d)

These four quantities are associated with the dual error bounds. The first two quantities, given by (10a) and (10b), define the optimal values of \(x,y\) given \(w,z\), and \(\), while the last two quantities, defined by (10c) and (10d), represent the optimal solutions for these intermediate subproblems.

It is obvious that when \(p\) is sufficiently large, \(K(x,y,w,z;)\) is strongly convex jointly w.r.t. \(x\) and \(y\). First, under A1 and A5, we can easily show that

\[d^{2}_{(x^{r})}(u^{r}_{T^{r}})_{g}1-}^{T^{r}}(g(x^{r},u^{r}_{0})-g^{*}(x^{r})),\] (11)

so under A2 and the assumption that \(u^{r}\) is initialized within a bounded set, it follows that \(d^{2}_{(x^{r})}(u^{r}_{T^{r}})\) decreases to \(0\) at a linear rate. This inequality (11) is instrumental in quantifying the bias term \(|g^{*}(x^{r})-g(x^{r},u^{r+1})|^{2}\) that arises due to the inaccurate estimation of \(_{y}g(x,y)\). Next, we will present the following three lemmas which play a crucial role in establishing the proof of Theorem 1.

### Descent Lemmas

After one round update of SLM (i.e., from \((x^{r},y^{r},w^{r},z^{r},^{r})\) to \((x^{r+1},y^{r+1},w^{r+1},z^{r+1},^{r+1})\), we can obtain the following result.

**Lemma 1**.: _Under A1-A5, suppose that the sequence is generated by SLM. When \(,(1/^{r+1})\), \(0<<1\), \(T^{r}=((r^{r+1}))\), and \(p>L\), there exists a constant \(\) such that_

\[(x^{r+1},y^{r+1},w^{r+1},z^{r+1};^{r+1})- (x^{r},y^{r},w^{r},z^{r};^{r})\] \[-(-^{2}+_{g}^{  2})_{g}^{2}}{^{2}})\|x^{r+1}-x^{r}\|^{2}-(-^{2}_{g}^{2}}{^{2}})\|y^{r+1}-y^{ r}\|^{2}\] \[-p(-(++36_{1}^{2})\!\!)\|z^{r+1}-z^{r}\|^{2}-p(-(+))\|w^{r+1}-w^{r}\|^{2}\] \[+36p(\|y^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z ^{r}))-^{*}(w^{r},z^{r})\|^{2}+\|x^{*}(w^{r},z^{r};_{+}^{r+1}(w^ {r},z^{r}))-^{*}(w^{r},z^{r})\|^{2})\] \[-(-48p_{2}^{2})|g( x^{*}(w^{r},z^{r};^{r+1}),y^{*}(w^{r},z^{r};^{r+1}))-g^{*}(x^{*}(w^{r},z ^{r};^{r+1}))-|^{2}\] \[+(_{g}L_{g}^{2}+2_{g}^{2})}}{r^{2}}-\|^{r+1}-^{r}\|^{2}\] (12)

_where \(_{1}(p-L)/p\), \(_{2}=(p+L)/(p-L)\), \(_{3}=1/(p-L)\), \(L=L_{f}+(2L_{g}+L_{g}^{2}/(2_{g}))\), \(_{+}(w,z)=_{+}[+_{}K(x^{*}(w,z; ),y^{*}(w,z;),w,z;)]\), \(_{g},_{g}^{}\) denote Lipschitz constant of \(g(x,y)\) and \(g^{*}(x)\) respectively, the potential function is_

\[(x^{r},y^{r},w^{r},z^{r};^{r}) K(x^{r},y^{r},w^{r},z^{r};^{r})-2D(w^{r},z^{r};^{r})+2P(w^{r},z^{r}),\] (13)

_and \(D_{} 2_{g}^{-1}( g(x,u_{0})- g^{*}(x))  x,u_{0}(R)\)._

It can be observed that the coefficients in front of the terms \(\|x^{r+1}-x^{r}\|^{2}\), \(\|y^{r+1}-y^{r}\|^{2}\), \(\|z^{r+1}-z^{r}\|^{2}\), \(\|w^{r+1}-w^{r}\|^{2}\) and \(|g(x^{*}(w^{r},z^{r};^{r+1}),y^{*}(w^{r},z^{r};^{r+1}))-g^{*}(x^{* }(w^{r},z^{r};^{r+1}))-|^{2}\) can be negative when the step-sizes are properly chosen and the bias term can be summed up to a constant. Hence, to ensure a sufficient decrease of \((x^{r},y^{r},w^{r},z^{r};^{r})\), it is necessary to bound the positive terms w.r.t. \(\|y^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}(w^{r},z^{r})\|^ {2}\) and \(\|x^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}(w^{r},z^{r})\|^ {2}\). To accomplish this, two novel dual error bounds are provided as follows.

### Dual Error Bounds

**Lemma 2**.: (Weak Dual Error Bound) _Under A1-A3 and A5, suppose that the sequence \(\{x^{r},y^{r},w^{r},z^{r},u^{r},^{r}\}\) is generated by SLM. Then, it holds that_

\[\|y^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}(w^{ r},z^{r})\|^{2}+\|x^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}(w^ {r},z^{r})\|^{2}\] \[_{}\|^{r+1}-_{+}^{r+1}(w^{r},z ^{r})\|\|(w^{r},z^{r})-_{+}^{r+1}(w^{r},z^{r})\|\] (14)

_where_

\[_{}+_{g}^{ })_{2}}{2(p-L)},(w,z)_{0 }K(^{*}(w,z),^{*}(w,z),w,z;).\] (15)

Although (14) has quantified the variations between \(y^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))\) and \(^{*}(w^{r},z^{r})\) and between \(x^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))\) and \(^{*}(w^{r},z^{r})\) when the dual variable is perturbed1, this bound is in a non-homogeneous form. Therefore, it is not sufficient to demonstrate the \((1/)\) convergence rate of the sequence generated by SLM. Fortunately, under the local regularity condition, we can obtain the following stronger result, which provides further insight.

**Lemma 3**.: (Strong Dual Error Bound) _Under A1-A3, A5 and the local regularity condition, suppose that the sequence \(\{x^{r},y^{r},w^{r},z^{r},u^{r},^{r}\}\) is generated by SLM. Then, there exists a constant \(_{}\) such that_

\[\|y^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}(w ^{r},z^{r})\|+\|x^{*}(w^{r},z^{r};_{+}^{r+1}(w^{r},z^{r}))-^{*}( w^{r},z^{r})\|\] \[ 2_{}|g(x^{*}(w^{r},z^{r};^{r+1}), y^{*}(w^{r},z^{r};^{r+1}))-g^{*}(x^{*}(w^{r},z^{r};^{r+1}))-|\] (16)

_where_

\[_{}_{}+p+ L_{ g}}{}}.\] (17)

Given this inequality, we can quantify the sufficient descent of the potential function, which further leads to the convergence rate of SLM.

## 5 Numerical Results

In this section, we evaluate our proposed algorithm for solving the bilevel optimization related problems and compare the performance of our proposed SLM with the state-of-the-art methods.

### Toy Example

First, we consider the following toy example

\[_{x}f(x,y) x^{2}+y^{2}+3x^{2}(y),  g(x,y)-g^{*}(x)\] (18)

where the LL objective function is \(g(x,y) xy^{2}+3x^{2}(y)\) and \(=1 10^{-3}\). It can be easily checked that function \(g(x,y)\) satisfies the PL condition w.r.t. variable \(y\) and function \(f(x,y)\) is nonconvex w.r.t. \(x\) and \(y\) jointly.

In the numerical results, we initialize variables as \(x^{1}=y^{1}=u^{1}=1\) and choose the step-sizes for updating these variables (\(u,y,x\)) as \(1 10^{-3}\) for all the compared methods. Additionally, we set \(p=1\) and \(=0.5\) for SLM. It can be seen from Figure 1 that SLM provides the advantage

Figure 1: Convergence performance of SLM, BOME , and V-PBGD  on the toy example.

of enforcing the constraint to be satisfied even the LL objective is nonconvex. Moreover, all the algorithms converge to the global optimal solution of the LL problem. It is worth noting that SLM achieves the lowest UL objective value compared to the other methods. This can be attributed to the relaxation introduced by \(\) in achieving the optimal solution of the LL problem, which provides more flexibility in searching for lower UL objective values.

### Data Hyper-Cleaning

Next, we further evaluate SLM for the data hyper-cleaning problem, which has gained widespread adoption in the machine learning community [3; 16]. The goal of this learning task is to identify and select t the clean data samples from the polluted ones, enabling the pre-trained model to generalize effectively on the clean test dataset. The optimization problem is commonly formulated as follows:

\[_{x,y}^{}(y), y_{y^{}} ^{}(x,y^{})\] (19)

where \(^{}\) and \(^{}\) represent the validation (at the UL) and training (at the LL) losses, respectively. The LL loss function is defined as \(^{}_{i=1}^{m}(x_{i})(y)\), where \(m\) denotes the total number of training data samples, \((x)\) is the sigmoid function, and \(y\) denotes the weights of the neural network. Adhering to the experimental settings used to test the V-PBGD as shown in , we are given \(5,000\) training data samples from the MNIST dataset. Among these samples, \(50\%\) are randomly polluted with incorrect labels. Additionally, we have a validation dataset consisting of \(5,000\) samples and a separate test set comprising \(10,000\) samples. The model parameter \(y\) includes the neural network weights of a hidden layer with a size of \(10 784\) and bias terms with a size of \(10\).

It has been shown in  that V-PBGD outperforms significantly the state-of-the-art methods, including BOME , IAPTT-GM , RHG , T-RHG , for this particular example. We adapt the code used to implement V-PBGD to evaluate the performance of SLM and set \(=p=20\), \(=0.5\), \(=1.2\) for SLM. From Figure 2, it can be observed that SLM initially converges at a similar rate to V-PBGD in terms of the test loss values. However, after a certain number of iterations, SLM achieves smaller loss values. The reason behind this observation is that once the constraint is satisfied, the optimization variables primarily focus on minimizing the UL objective values, leading to lower test losses in this example. Although SLM achieves a test accuracy similar to V-PBGD, with the highest test accuracy of \(90.44\%\), when the number of iterations becomes large (e.g., exceeding \(3,000\) in the figure), the model begins to overfit the training data. However, SLM maintains a test accuracy of \(88.8\%\), while V-PBGD can only attain \(88.1\%\).

As for the constraint violation, we define \(c_{}^{}(x,y)-v(x)\), where \(v(x)\) is computed through the updating process of variable \(u\). This measure represents the distance between the LL objective value and the optimal value. V-PBGD results in a \(c_{}\) value of approximately \(0.68\), whereas SLM achieves a \((c_{}-)\) value of \(0.001\) when the total number of iterations exceeds \(2,000\) in the figure. This further reinforces the fact that due to the presence of the optimization variables appearing in both levels or the coupling between the LL and UL variables, it is highly challenging for \(y\) in the BO formulation to find the global optimal solution of the LL problem. In contrast, SLM can find KKT solutions for this class of structured FCO problems as long as the hyper-parameter \(\) is appropriately defined.

### Neural Network on MNIST Data Set

We also evaluate the performance of these algorithms on a \(2\)-layer neural network. The size of the first layer is \(784 300\), and the size of the second layer is \(300 10\). The activation function used is the sigmoid function. For all the compared algorithms, we set the step-sizes of the block-\(x\) and block-\(y\) updates to \(1\). The step-size \(\) for the auxiliary block-\(u\) update is set to \(0.1\). Additionally, we choose \(=0.01\), \(=0.5\), and \(p=20\) for SLM, while for BOME, we use \(=0.1\). The remaining settings remain the same as mentioned in the main text for the linear case.

The results, averaged over \(5\) independent trials, are presented in Figure 3. It can be observed from Figure 3(a) that SLM with \(=1\) achieves the lowest test loss and converges faster compared to

Figure 2: Comparison between SLM and V-PBGD on the data hyper-cleaning task over the MNIST dataset.

BOME. When \(=0.5\), the convergence rate of SLM becomes slower. Both of these observations align with the theoretical convergence analysis. The test loss obtained by V-PBGD initially decreases and then increases, which is reasonable as this method optimizes both the UL (validation) and LL (training) loss values, with the penalty parameter increasing as the number of iterations rises. In contrast, SLM primarily minimizes the objective loss values, specifically the validation loss in this case, leading to better generalization performance as long as the constraint is satisfied. The constraint tolerance parameter \(\) and the dual variable implicitly play the tradeoff between the training and validation losses.

Regarding the test accuracy shown in Figure 3(b), SLM with \(=1\) achieves a minimum of \(94.74 0.04\%\), which is the lowest among all the algorithms compared. V-PBGD can reach \(94.65 0.09\%\) but converges slower than SLM with \(=1\). SLM with \(=0.5\) and BOME converge relatively quickly initially and reach peak accuracies of \(94.53 0.06\%\) and \(94.52 0.11\%\), respectively. However, it is evident that the test accuracy achieved by BOME decreases rapidly as the algorithm proceeds, even though the test loss continues to decrease, forming a clear U-shape. This implies that BOME overfits the validation data when the number of iterations is large. In contrast, the test accuracy achieved by SLM remains relatively stable, or at least does not decline as rapidly as that attained by BOME. This further confirms that the use of structured constraints in this class aids in improving the generalization performance of bilevel models.

## 6 Concluding Remarks

In this work, we focused on solving a class of structured nonconvex FCO problems, where the functional constraint satisfies the PL condition w.r.t. one block of variables. In practical applications, this class of FCO problems can effectively model a wide range of nested or hierarchical learning problems, including data hyper-cleaning, meta-learning, corset selection, and more. To address these challenges, we developed a smoothed primal-dual algorithm based on the Lagrangian method. The proposed algorithm achieves a convergence rate of \(}(^{-1})\) to the \(\)-KKT solutions of this problem by utilizing only the first-order oracles for both the objective function and the constraint.

The major difficulty from a theoretical perspective lies in showing the dominance of the descent achieved by the algorithm over the ascent induced by enforcing the nonconvex functional constraint. To the best of our knowledge, this is the first result showcasing that the dual error bound also holds for solving this class of nonlinear constraints. We conducted extensive numerical experiments to evaluate the performance of the proposed SLM against benchmark nonconvex BO methods. The results demonstrate that SLM not only ensures the satisfaction of the functional constraint but also achieves small objective values.

It is important to note that this work focuses on the scenario with a single constraint. Future research endeavors will explore the extension of the algorithm design and theoretical results to more general settings. Addressing this limitation will contribute to a broader applicability of the proposed method.

Figure 3: Convergence performance of SLM, BOME , and V-PBGD  on the neural network.