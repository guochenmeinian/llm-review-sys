# Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers

Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers

Gavia Gray

Cerebras Systems

Toronto, Canada

gavia.gray@cerebras.net

&Aman Tiwari

Subjective

London, UK

&Shane Bergsma

Cerebras Systems

Toronto, Canada

&Joel Hestness

Cerebras Systems

Sunnyvale, CA

###### Abstract

Per-example gradient norms are a vital ingredient for estimating gradient noise scale (GNS) with minimal variance. Observing the tensor contractions required to compute them, we propose a method with minimal FLOPs in 3D or greater tensor regimes by simultaneously computing the norms while computing the parameter gradients. Using this method we are able to observe the GNS of different layers at higher accuracy than previously possible. We find that the total GNS of contemporary transformer models is predicted well by the GNS of only the normalization layers. As a result, focusing only on the normalization layer, we develop a custom kernel to compute the per-example gradient norms while performing the Layer-Norm backward pass with zero throughput overhead. Tracking GNS on only those layers, we are able to guide a practical batch size schedule that reduces training time by 18% on a Chinchilla-optimal language model.

## 1 Introduction

The gradients gathered during the backward pass while training a neural network are typically inspected via their Frobenius norm, the magnitude of the vector. This gradient vector may be viewed as the sum of gradients computed over each individual example in the minibatch. Each of these has its own norm. In this work, we develop a method to access these norms that works at any scale, for three common layer types in deep learning models: linear, normalization and embedding layers.

One primary application of a per-example gradient norm is in estimating the Gradient Noise Scale (GNS) , a metric that has been shown to be useful in training large scale models . The uncertainty of the GNS estimator depends directly on the size of the batch used to compute the small batch gradient norm as shown in Section 2.1. So, the most precise estimate of the GNS is obtained by computing the gradient norms for _each_ example in the minibatch: the per-example gradient norm.

To demonstrate GNS measurement in practice we perform experiments on contemporary language model architectures, providing a detailed visualisation of the movement of the GNS components throughout training, presented in Section 4. By inspecting these components it was found that the GNS of the model is highly correlated between layer types, which we give an intuition for in Figure 1.

However, the practical utility of measuring GNS with per-example gradient norms is only present if it can be gathered without affecting training time. Focusing on LayerNorm  layers, we note the main speed bottleneck is the memory I/O when not implemented as a fused kernel. To demonstrate this, we develop a custom kernel to compute both the backward pass and the per-example gradient norms at the same time. Using this kernel the throughput overhead of gathering the per-example gradient is zero, even outperforming PyTorch's LayerNorm at larger dimensions. We apply this to a practical batch size schedule case study in Section 5.

To reiterate, the contributions of this work are:

* A minimal FLOP algorithm and implementation for computing gradients and per-example gradient norms of linear layers simultaneously.1 * Observations that the measured GNS for LayerNorm layers is highly correlated with the GNS of the remaining layers.
* Development of an example kernel to implement tracking the GNS of LayerNorm layers that does not affect network throughput (tokens/sec).
* Demonstration of a real application of GNS tracking in a batch size schedule experiment that obtains an 18% wall-time speedup in training a Chinchilla-optimal  LLM.

## 2 Background

### Gradient Noise Scale

GNS is a metric derived from observing a second order Taylor expansion of the change in a loss function under the following assumption on the noise in the gradient estimate ,

\[G_{()}(G(),( )),\] (1)

where \(G_{}\) is the observed gradient, \(B\) is the batch size, and \(\) the parameters of the model. Here, \(G\) is the unobserved "true" gradient and \(\) is the covariance of the gradient estimate. The Taylor expansion mentioned is,

\[[L(- G_{est})]=L()-|G|^{2}+ ^{2}(G^{T}HG+).\] (2)

Where \(\) is the learning rate and \(H\) is the Hessian of the loss. On the right hand side is a factor that depends on \(B\). It may be shown  that the optimal step size and optimal change in the loss is achieved when \(B=_{}:=tr(H)/G^{T}HG\). Averaging this optimal step over an entire run, and measuring this value by a grid search, yields \(_{}\) which describes a batch size that meets an optimal tradeoff between cost and training speed. It is shown by analysis and experiment that \(_{}_{}\).

As this depends on the Hessian, which is typically unavailable, McCandlish et al.  suggest making the assumption that the Hessian is diagonal, which yields

\[_{}=G}.\] (3)

Figure 1: Gradient noise scale (GNS) is typically computed by comparing per-minibatch (aggregated-across-layers) gradients to gradients “Aggregated” across minibatches. We estimate GNS with lower variance by making each minibatch a single example, and maintain per-layer GNS estimates. We find the magnitude of gradients (visualized by the length of red arrows) to be consistent across layers, enabling overall GNS to be computed very cheaply using only gradient stats from LayerNorm layers.

To compute \(_{}\) McCandlish et al.  define the unbiased estimators \(\) and \(_{2}^{2}\) as:

\[_{2}^{2} :=}-B_{}}(B_{}  G_{B_{}}_{2}^{2}-B_{} G _{B_{}}_{2}^{2}) G^{T}G\] (4) \[ :=}-1/B_{}}( G _{B_{}}_{2}^{2}- G_{B_{}} _{2}^{2}) tr(),\] (5)

where \(B_{}\) and \(B_{}\) are the batch sizes used to compute the gradients \(G_{B_{}}\) and \(G_{B_{}}\), respectively (potentially corresponding to _Aggregated_ and _Minibatch_ gradients as depicted in Figure 1).

\( G_{B_{}}_{2}\) is trivially computed using the gradients accumulated for the optimizer but \( G_{B_{}}_{2}\) is not. One option is to use the gradients communicated between Distributed Data Parallel (DDP) nodes, but this has two downsides: (1) the variance of the estimate is tied to the DDP configuration and (2) the estimate is not available in all training configurations. For example, experiments on a single GPU cannot use this method. One can also access the gradients during gradient accumulation, but this similarly depends on the training configuration. A full taxonomy of the options for computing \( G_{B_{}}_{2}\) is provided in Appendix A.

For each observation of \( G_{B_{}}_{2}\) we may observe multiple \( G_{B_{}}_{2}\), typically \(B_{}/B_{}\) of them. On each step the estimate of \( G_{B_{}}_{2}^{2}\) is therefore a mean over \(B_{}/B_{}\) samples, whose variance is reduced according to the law of large numbers. However, the GNS is a ratio of the unbiased estimators in Equations 4 and 5, so it may not be clear how this affects uncertainty in the GNS estimate. Figure 2 explores this relationship by simulation of a setting where the GNS is set to 1 while varying \(B_{}\) and \(B_{}\). We find it is always better (less uncertainty) to use the smallest possible \(B_{}\) to estimate the GNS, while the choice of \(B_{}\) is irrelevant.

### Efficient Per-example Gradient Norms

Goodfellow  proposes a trick to compute gradient norms for individual examples in a minibatch, which would provide the minimum variance estimate of the GNS as described in Section 2.1. Neglecting the original derivation, by writing the desired squared norm as a tensor contraction the trick may be reproduced automatically via einsum path optimization [49; 15]. The tensor contraction for per-example gradient norms, \(n_{b}^{2}\), of a linear layer in the 2D setting is,

\[n_{b}^{2}=_{i,k}(w^{})_{bik}^{2}=_{i,k}x_{bi}x_{bi}y_{bik}^{ }y_{bk}^{},\]

where \(x\) are the activations prior to a linear layer, \(y^{}\) are the gradients of the loss with respect to the outputs of the linear layer and \(w^{}\) are the gradients of the loss with respect to the weights of the linear layer.

Figure 2: The variance of the GNS estimator for different \(B_{}\) (left) and \(B_{}\) (right) sizes. \(B_{}=l\) and \(B_{}=s\) in legends. Stderr is estimated using a jackknife resampling method for ratio estimators . For the same number of samples processed, a smaller \(B_{}\) always has a lower standard error, while the size of the large batch, \(B_{}\) does not affect the standard error.

Li et al.  extend this trick to the three dimensional case. For inputs \(^{B T I}\) and outputs \(^{B T K}\), the per-example gradient norm \(n_{b}\) is,

\[n_{b}^{2}=(w^{})_{bik}^{2}=(_{t}x_{bti}y^{}_{btk})^{2}=x_{bti}y^{ }_{btk}x_{bti}y^{}_{buk}=^{T},^{}^{ T}_{F}^{2},\]

which has \(O(T^{2})\) memory complexity in the sequence length \(T\).2. Index sets are \(b[1,B],\ i[1,I],\ k[1,K],\ t,u[1,T]\). At some point, the I/O cost of computing the per-example gradient norms by computing the full \(w^{}_{b}\) explicitly will be cheaper. Noting this fact motivated the work in Section 3 and the practical relationship between these resource costs is explored in Section 3.1.

### Related Work

Gradient normsOne common motivation for computing per-example gradient norms is for differential privacy. By bounding the gradient for any single example, we can ensure each example has a limited impact on the final parameters [45; 36]. Per-example gradient clipping has been performed with convolutional networks  and sequential models, e.g., LLMs . These methods allow control over per-example gradient norms even when training with large batch sizes. Approaches like these are implemented in the differential-privacy library Opacus , and have support natively in PyTorch, but are less efficient than the methods proposed in this paper. An alternative mechanism to manifest per-example gradient norms is to simply use a batch size of one. While not efficient enough for training large-scale networks, such sequential training may arise in situations such as reinforcement learning, where per-example gradient clipping has also been performed (to improve stability ).

Gradient noise scaleThe Gradient Noise Scale  has been widely used for training large-scale neural networks. For example, Brown et al.  note the GNS was measured during training and used to guide batch sizing when training GPT-3. Dey et al.  mention that operating near the critical batch size, as dictated by the GNS, is important for hyperparameter transfer under the maximal update parameterization . Even when not explicitly mentioned in publications, open source code often implements the GNS (e.g., see codebases [21; 13] for GPT-NeoX  and Hourglass Diffusion Transformer ).

Measurements similar to the GNS have also been used in a range of prior work to guide batch sizing for minibatch SGD [10; 17; 5; 55]. Chen et al.  show experimentally that wider networks can be trained using larger batches; they also establish a theoretical connection between wider networks and gradient variance, albeit for simple two-layer networks. In contrast, Shallue et al.  found empirically that _narrower_ Transformers scale better to larger batch sizes. Smith and Le  propose a noise scale based not on gradient variance, but on the learning rate, dataset size, and batch size (similar to the notion of temperature in Section 4.1). Zhang et al.  find the critical batch size depends on the choice of optimizer. Faghri et al.  introduce a gradient clustering and stratified sampling approach to minimize minibatch gradient variance, and use this approach as a tool to help understand optimization.

Gradient varianceBeyond computing the GNS, our method can support other applications where measuring the distribution of per-example gradients is useful or informative. Gradient variance has been used to classify the _difficulty_ of examples , which can be used, for example, to surface problematic examples for human auditing. The question of whether gradient distributions tend toward Gaussian in the (central) limit is of theoretical significance , with implications toward the ability of SGD to escape sharp minima and land in wide basins [63; 41; 48]. Bounded gradient variance is also assumed in some convergence analysis [8; 62], as noted in .

Perhaps the most familiar use of gradient variance is of course in adaptive optimizers like Adagrad, Adam, and others that reduce step sizes in high-gradient-noise directions [20; 57; 46; 33; 44]. Hilton et al. [28, App. C] directly relate Adam second moment statistics to a _component-wise_ version of the GNS. Optimizers typically estimate gradients jointly across training steps and minibatches, however vSGD  leverages separate components for gradient momentum and for gradient variation across samples. Zhang et al.  find the variance of gradient norms across examples predictive of whether vanilla SGD outperforms adaptive optimizers, however recent work has shown Adam to outperform SGD even in the (noise-free) full gradient descent setting [34; 35].

## 3 Simultaneous Per-example Gradient Norms

As described in Section 2, computing GNS requires small batch gradient norms. Typically, these may be gathered during gradient accumulation or DDP communication.3 However, these methods are not universally applicable and may not be available in all training configurations. In this section we describe a method for baking the computation of the per-example gradient norms into the computation graph, making it universally applicable. The typical tensor contraction used to compute the backward gradient in a linear layer using the input activations, \(\), and gradients, \(\), is,

\[w^{}_{k,l}= x_{ k}g_{ l},\]

in other words, a sum over vector outer products for every vector in the trailing dimension. In principle, it is possible to access the intermediate tensor containing the batch dimension \(w^{}_{bkl}= x_{b k}g_{b l}\). This allows us to compute the per-example gradient norms with FLOPs scaling at the same rate as the normal, non-per-example backward pass (Figure 3), albeit at increased I/O cost due to having to materialize the intermediate tensor.

A generic algorithm to compute the per-example gradient norms simultaneously with the weight gradient in a standard linear layer is provided in Algorithm 1 using einsum for readability and portability.4 The reason for the correction in step 4 can be seen by considering the gradient of loss function \(L\) with respect to the weights on a single example \(b\), \(w_{b}\),

\[_{w_{b}}_{b}L(x_{b})=_{w_{b}}L(x_{b}),\]

computing the squared norm of this will therefore contain a factor of \(1/B^{2}\), which must be corrected for.

```
0: gradient tensor \(\) of shape \((B,...,L)\), input activation tensor \(\) of shape \((B,...,K)\)
0: weight gradient tensor \(^{}\) of shape \((K,L)\), mean of per-example squared norms \(\|^{}_{b}\|_{2}^{2}\)
1:\(^{}_{b}(`b...k,b...l bkl, ,)\)
2:\(_{w}(`bkl b`,^{ 2}_{b})\)
3:\(^{}(`bkl kl,^{ }_{b})\)
4:\(\|^{}_{b}\|_{2}^{2} 1/B( _{w},`b) B^{2}\)# reduce by mean then apply correction
5:return\(^{}\), \(\|^{}_{b}\|_{2}^{2}\) ```

**Algorithm 1** Linear Layer Simultaneous Per-Example Gradient Norm Computation

### FLOPs and I/O Costs

The computational cost of computing per-example gradient norms can be broken down into FLOPs, in Figure 3, and I/O, in Figure 4, with matrix multiplication on current devices being potentially bottlenecked by both. We estimate ideal FLOP and DRAM I/O costs, assuming optimal reuse of data loaded from DRAM into SRAM with no recomputation. In practice, duplicate computation may be used to improve wall-clock time and to fit within hardware limitations of the amount of shared memory available. We compare here against the efficient per-example gradient norm method described by Li et al. , which the authors note is only efficient (in terms of I/O cost) when \(2T^{2}<PD\), where \(T\) is the sequence length, \(P\) is input and \(D\) is output dimension of the linear layer. This bound is discussed further in Appendix E.

In terms of FLOPS, Figure 3 shows the simultaneous per-example gradient norms are almost always preferable, only being more expensive for very short sequence lengths in small models. The reason for this is shown on the right hand side; the number of FLOPs required to compute the simultaneous per-example gradient norms is independent of the sequence length.

The I/O cost shown in 4 illustrates a tradeoff in computing the per-example gradient norm. The simultaneous method is more expensive at large model sizes with short sequence length because it must act on a large intermediate tensor.

To estimate model flops, we use PyTorch's FLOPCounterMode, which only measures the FLOPs in matrix multiplications and attention computation, however these make up the vast majority of the FLOPs in a Transformer model.

## 4 Gradient Noise Scale in Transformer Language Models

Using the methods described in previous sections to measure per-example gradient norms and estimate the GNS, we perform experiments on a 111M parameter Chinchilla-optimal language model [19; 29] using the OpenWebText dataset .5 As the prior work was performed on Pile , Appendix C.1 describes an experiment to check the optimality of the Chinchilla model on this dataset. We also found Flash attention led to numerical instability, which we were able to mitigate with an architectural modification described in Appendix C.2.

Figure 4: Total I/O cost of computing per-example gradient norms, assuming gradients and parameters are stored with 4 bytes of precision. The relative IO cost of Simultaneous per-example gradient norms is less than Li et al.  for very long contexts for all model scales, approximately equivalent for models of 10B parameters and 4096 context length, and higher for shorter contexts with larger models. The IO cost of LN (LayerNorm) per-example gradient norms alone is much lower than either method.

Figure 3: FLOP cost of computing per-example gradient norms. (Left) Total FLOP cost. (Right) Proportional cost versus one model forward and backward pass. The FLOP cost of Simultaneous per-example gradient norms is strictly dominant to alternative methods (left) and the ratio of this additional cost to the FLOP cost of processing the entire model does not depend on context length (right).

All experiments computed per-example gradient norms for all layers in the model with the exception of the performance results of Sections 5.1 and 5.2, which only computed per-example gradient norms for the normalization layers. Each experiment was run on Nvidia A10 GPUs, in either 12 or 24 hours depending on the precision used, Bfloat16 or Float32 respectively. We used the nanoGPT6 codebase with the layers described in Section 3 added.

Having an accurate estimate of the GNS statistics \(\|\|_{2}^{2}\) and \(\) allows us to visualize the movement of both in a phase space during training as shown in Figure 5. LayerNorm layers are separate from the rest of the network because their statistics are much smaller and to illustrate how the resulting GNS estimates on the right track each other. To observe these trends in another training regime, see Figure 14 in Appendix D.1.

### The Temperature of Training

McCandlish et al. [39, App. C] observed that the GNS measurement depends on the batch size and learning rate used in training. In fact, from the derivation outlined in Section 2.1, the gradient noise scale is only well-defined at the optimal learning rate. Using a toy model of a quadratic loss function, they observed that the GNS should be inversely proportional to the temperature, \(T\), a ratio of batch size \(B\) to learning rate \(\):

\[_{}_{} {T}=.\]

This enables a testable prediction that the GNS will increase with increasing batch size or with descending learning rate. This prediction was found to accurately describe experiments on a small

Figure 5: GNS phase plot: Linear/Embedding layers are separated from LayerNorm layers by row. Component estimators of Equations 4 and 5 are shown (left) with the GNS over the course of training on the (right).

convolutional model on the SVHN dataset. We repeat it here in the setting described above in Figure 6. To match the results of McCandlish et al. , all interventions tested should yield the same result. We find the GNS does indeed react predictably to changes in the learning rate, but the reactions to changes in the batch size are not predicted by the theory.

### GNS Correlates Between Layer Types

Inspection of Figure 5 suggests the LayerNorm layers produce a similar GNS, when combined, as the total GNS of the model. Before describing how to quantify this relationship we must first note that the unbiased estimators \(\|\|_{2}^{2}\) and \(\) are noisy. All GNS figures presented in this paper and other work smooth both of these estimators, typically with an Exponential Moving Average (EMA) filter, before computing the GNS ratio.7

So, when quantifying the relationship between the GNS of different layers, it must be compared for different smoothing factors. Here, we show the regression coefficients with respect to the alpha of the EMA filter in Figure 7. The results show that the GNS of the LayerNorm and Attention layers are highly predictive of the total GNS of the model. In both cases, the slope is approximately 1.4, meaning the total GNS is approximately 1.4 times the GNS of the LayerNorm or Attention layers.

Comparing the quality of this fit versus the quality of prior work's overall fit of the GNS to the critical batch size (measured empirically) , the quality seems acceptable and we do not need to apply this 1.4x correction factor, rather we just note that the true \(_{}\) may be greater than the measured \(_{}\).

## 5 Batch Size Scheduling

We focus on two concerns that affect the practicality of batch size scheduling. First, measuring the appropriate batch size without incurring any additional training time. We find this is possible with the method described in Section 5.1. Second, whether batch size scheduling is effective in practice. We find it can offer significant savings in the required number of tokens processed in Section 5.2.

### Universal GNS with Zero Overhead

Capturing a GNS estimate for a linear layer is powerful, but efficiently doing so presents a challenge. Such an estimate requires accumulating per-example gradients of hidden_size\({}^{2}\) across the sequence dimension, compared to just hidden_size with LayerNorm. This increased size requires using more complex reductions in the kernel, rather than a simple warp reduction followed by shared-memory atomic reduction with a final atomic global reduction (as we can implement for LayerNorm per-example gradients within shared memory). In addition, linear layer kernels are already highly optimized and require using advanced techniques to keep GPU tensor cores fed with data, so

Figure 6: During the middle of training a 111M parameter language model on OpenWebText, the learning rate, \(\) or batch size, \(B\) were varied, restarting the run from the same point. This Figure replicates an experiment from McCandlish et al.  showing how varying the ratio causes changes in the measured GNS, but here only due to changes in the learning rate. Changes in the batch size do not have the predicted effect.

combining such a kernel with per-example gradient computation - with its own memory overheads and corresponding available bandwidth reduction - would be a difficult undertaking.

We thus implemented a LayerNorm-specific CUDA kernel that also captures GNS. In experiments with language models at different scales, illustrated in Figure 8, we find this kernel has practically zero overhead compared to PyTorch's LayerNorm implementation. The complete source code for this kernel is provided with the accompanying code for this paper8.

### Case Study: Batch Size Schedule

As a case study we continue with the 111M parameter language model on OpenWebText described above. Over three seeds, we run both a fixed batch size and a batch size schedule that increases linearly with the number of tokens processed to the original batch size. We vary the batch size during training by varying the number of gradient accumulation steps.

Figure 8: Comparison of average time taken for a LayerNorm forward and backward pass with gradient accumulation when using PyTorch’s native implementation versus our custom kernel computing per-example gradient norms in tandem. Measured on an Nvidia H100 GPU.

Figure 7: Regression of total GNS using the GNS of each layer type. (Left) GNS of each layer type and the total GNS are plotted against the number of tokens processed for varying EMA alpha settings. (Center & Right) The slope and Pearson’s correlation coefficient of the regression of the total GNS against the GNS of each layer type, respectively, as a function of the same EMA alpha values. The total GNS (black) on the left is predicted well by individual layer types as indicated by the correlation coefficients (right), however the type with slope closest to 1 is LayerNorm (center), only overestimating the GNS by less than 40% across EMA alpha values.

The results of this experiment are shown in Figure 9. The left plot shows the progression of the loss for both models, with the range of values captured over different seeds. The mean loss for the linear batch size schedule leads the fixed batch size throughout training. On the right, this lead is quantified by interpolating the number of tokens saved to achieve the same loss. The precise schedule used is shown in Figure 15 in Appendix D.2.

## 6 Limitations

In this paper, we only studied Transformers, which include Normalization sub-layers natively. While Transformers are ubiquitous in machine learning, there are many models, including variations of RNNs, CNNs, and state-space models, that do not use such layers conventionally. However, we note LayerNorm could be added to these networks with very little overhead (in fact, the desire to normalize activations in RNNs was one of the original motivations for developing LayerNorm; application of batch normalization  to RNNs was "not obvious" ). Nevertheless, investigating LayerNorm-based GNS in these other models requires further work.

Our work is also part of efforts to improve efficiency and address the increasing costs of training and tuning large neural networks . We provide both a more-efficient technique for computing the GNS, and also, by enabling use of GNS statistics, we support compute-efficient training recipes, such as use of dynamic batch sizes. While some have argued that hyperscalers may re-invest any efficiency savings into ever-larger models , for academic researchers, such savings could allow pushing the state-of-the-art, while still getting results in a reasonable timeframe. Recent efforts to enable frontier-model-performance within academic budgets are encouraging, both to reduce memory [38; 18] and save compute [37; 2]. Of course, even for such economical approaches, "extensive hyperparameter search" may still be required . There is a growing awareness that hyperparameter tuning has a negative impact on equity in AI research, as tuning success depends directly on researcher finances . A correlated trend is to use better training measurements (such as gradient noise in batch and step size optimizers (Section 2.3)) to reduce dependence on hyperparameters, and in this way we hope our work can also ultimately improve research equity.

## 7 Conclusion

This work set out to provide a practical method for computing the per-example gradient norms necessary to compute the GNS independent of the training configuration. In the process we discovered that not all the layers are necessary for a practical estimate of the GNS and that the per-example gradient norms can be computed for the normalization layers with zero overhead. This enabled practical experiments, such as a batch size schedule and replicating prior GNS observations. We are hopeful that democratising access to GNS statistics, on any device, will enable subsequent discoveries.

Figure 9: (Left) Linear batch size schedule tracking the GNS over 2.2 billion tokens processed. Loss is plotted over a smoothed range from 3 runs using different Seeds. (Right) The number of tokens saved over the fixed batch size run to achieve the same loss.