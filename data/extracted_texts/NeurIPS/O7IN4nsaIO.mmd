# Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes

Yan Huang

College of Control Science and Engineering

Zhejiang University, China

huangyan5616@zju.edu.cn &Xiang Li

Department of Computer Science

ETH Zurich, Switzerland

xiang.li@inf.ethz.ch &Yipeng Shen

College of Control Science and Engineering

Zhejiang University, China

22332074@zju.edu.cn &Niao He

Department of Computer Science

ETH Zurich, Switzerland

niao.he@inf.ethz.ch &Jinming Xu

College of Control Science and Engineering

Zhejiang University, China

jimmyxu@zju.edu.cn

###### Abstract

In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of \(}(^{-(4+)})\) for any small \(>0\), matching that of the centralized counterpart. To our best knowledge, D-AdaST is the _first_ distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results.

## 1 Introduction

Distributed optimization has seen significant research progress over the last decade, resulting in numerous algorithms (Nedic and Ozdaglar, 2009; Yuan et al., 2016; Lian et al., 2017; Pu and Nedic, 2021). However, the traditional focus of distributed optimization has primarily been on minimization tasks. With the rapid growth of machine learning research, various applications have emerged that go beyond simple minimization, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Gulrajani et al., 2017), robust optimization (Mohri et al., 2019; Sinha et al., 2017), adversary training of neural networks (Wang et al., 2021), fair machine learning (Madras et al., 2018), and justto name a few. These tasks typically involve a minimax structure as follows:

\[_{x}\,_{y}f(x,y),\]

where \(^{p}\), \(^{d}\), and \(x,y\) are the primal and dual variables to be learned, respectively. One of the simplest yet effective methods for solving the above minimax problem is Gradient Descent Ascent (GDA) (Dem'yanov and Pevnyi, 1972; Nemirovski et al., 2009) which alternately performs stochastic gradient descent for the primal variable and stochastic gradient ascent for the dual variable. This approach has demonstrated its effectiveness in solving minimax problems, especially for convex-concave objectives (Hsieh et al., 2021; Daskalakis et al., 2021; Antonakopoulos et al., 2021), i.e., the function \(f(,y)\) is convex for any \(y\), and \(f(x,)\) is concave for any \(x\).

Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014), and AMSGrad (Reddi et al., 2018), are often integrated with GDA to effectively solve minimax problems with theoretical guarantees in convex-concave settings (Diakonikolas, 2020; Antonakopoulos et al., 2021; Ene and Le Nguyen, 2022). These adaptive methods are capable of adjusting stepsizes based on historical gradient information, making it robust to hyper-parameters tuning and can converge without requiring to know problem-dependent parameters (a characteristic often referred to as being "parameter-agnostic"). However, in the nonconvex regime, it has been shown by Lin et al. (2020); Yang et al. (2022) that it is necessary to have a time-scale separation in stepsizes between the minimization and maximization processes to ensure the convergence of GDA and GDA-based adaptive algorithms. In particular, the stepsize ratio between primal and dual variables needs to be smaller than a threshold depending on the properties of the problem such as the smoothness and strong-concavity parameters (Li et al., 2022; Guo et al., 2021; Huang et al., 2021), which are often unknown or difficult to estimate in real-world tasks, such as training deep neural networks.

Applying GDA-based adaptive methods into decentralized settings poses additional challenges due to the presence of inconsistency in locally computed adaptive stepsizes. In particular, it has been shown that the inconsistency of stepsizes can result in non-convergence in federated learning with heterogeneous computation speeds (Wang et al., 2020; Sharma et al., 2023). This is mainly due to the lack of a central node coordinating the stepsizes of nodes in distributed settings, making it difficult to converge, as observed in minimization problems (Liggett, 2022; Chen et al., 2023b). As a result, the following question arises naturally:

_"Can we design an adaptive minimax method that ensures the time-scale separation and consistency of stepsizes with provable convergence in fully distributed settings?"_

**Contributions.** In this paper, we aim to propose a distributed adaptive method for efficiently solving nonconvex-strongly-concave (NC-SC) minimax problems. The contributions are threefold:

* We construct counterexamples showing that directly applying adaptive methods designed for centralized problems will lead to inconsistencies in locally computed adaptive stepsizes, resulting in non-convergence in distributed settings. To tackle this issue, we propose the _first_ distributed adaptive minimax method, named D-AdaST, that incorporates an efficient stepsize tracking mechanism to maintain consistency across local stepsizes, which involves transmission of merely two extra (scalar) variables. The proposed algorithm exhibits time-scale separation in stepsizes and parameter-agnostic capability in fully distributed settings.
* Theoretically, we prove that D-AdaST is able to achieve a near-optimal convergence rate of \(}(^{-(4+)})\) with arbitrarily small \(>0\) to find an \(\)-stationary point for distributed NC-SC minimax problems. In contrast, we also prove the existence of a constant steady-state error in both the lower and upper bounds for GDA-based distributed minimax algorithms when being directly integrated with the adaptive stepsize rule without the stepsize tracking mechanism. Moreover, we explicitly characterize the transient times that ensure time-scale separation and quasi-independence of network, respectively.
* We conduct extensive experiments on real-world datasets to verify our theoretical findings and the effectiveness of D-AdaST on a variety of tasks, including robust training of neural networks and optimizing Wasserstein GANs. In all tasks, we demonstrate the superiority of D-AdaST over several vanilla distributed adaptive methods across various graphs, initial stepsizes and data distributions (see also additional experiments in Appendix A).

### Related Works

**Distributed nonconvex minimax methods.** In the realm of federated learning, Deng and Mahdavi (2021) introduce Local SGDA algorithm combining FedAvg/Local SGD with stochastic GDA and show an \(}(^{-6})\) sample complexity for NC-SC objective functions. Sharma et al. (2022) provide improved complexity result of \(}(^{-4})\) matching that of the lower bound of first-order algorithms for both NC-SC and nonconvex-Polyak-Lojasiewicz (NC-PL) settings (Li et al., 2021; Zhang et al., 2021). Yang et al. (2022) integrate Local SGDA with stochastic gradient estimators to eliminate the data heterogeneity. More recently, Zhang et al. (2023) adopt compressed momentum methods with Local SGD to increase the communication efficiency of the algorithm. For decentralized nonconvex minimax problems, Liu et al. (2020) study the training of GANs using decentralized optimistic stochastic gradient and provide non-asymptotic convergence with fixed stepsizes. Tsaknakis et al. (2020) propose a double-loop decentralized SGDA algorithm with gradient tracking techniques (Pu and Nedic, 2021) and achieve \(}(^{-4})\) sample complexity. With a stronger assumption of average smoothness, some studies employ variance reduction techniques to accelerate convergence (Zhang et al., 2021; Chen et al., 2022; Xian et al., 2021; Tarzanagh et al., 2022; Wu et al., 2023; Chen et al., 2024; Zhang et al., 2024), which require more memory and computational resources due to the need for larger batch-sizes or full gradient evaluations. However, all the above-mentioned methods use a fixed or uniformly decaying stepsize, requiring the prior knowledge of smoothness and concavity.

**(Distributed) adaptive minimax methods.** For centralized nonconvex minimax problems, Yang et al. (2022) show that, even in deterministic settings, GDA-based methods necessitate the time-scale separation of the stepsizes for primal and dual updates. Many attempts have been made for ensuring the time-scale separation requirement (Lin et al., 2020; Yang et al., 2022; Bot and Bohm, 2023; Huang et al., 2023). However, these methods typically come with the prerequisite of having knowledge about problem-dependent parameters, which can be a significant drawback in practical scenarios. To this end, Yang et al. (2022) introduce a nested adaptive algorithm named NeAda that achieves parameter-agnosticism by incorporating an inner loop to effectively maximize the dual variable, which can obtain an optimal sample complexity of \(}(^{-4})\) when the strong-concavity parameter is known. More recently, Li et al. (2023) introduce TiAda, a single-loop parameter-agnostic adaptive algorithm for nonconvex minimax optimization which employs separated exponential factors on the adaptive primal and dual stepsizes, improving upon NeAda on the noise-adaptivity. There has been few works dedicated to adaptive minimax optimization in federated learning settings. For instance, Huang et al. (2024) introduces a federated adaptive algorithm that integrates the stepsize rule of Adam with full-client participation, resembling the centralized counterpart. Ju et al. (2023) study a federated Adam algorithm for fair federated learning where the objective function is properly weighted to account for heterogeneous updates among nodes. To the best of our knowledge, it is still unknown how one can design an adaptive minimax method capable of fulfilling the time-scale separation requirement and being parameter-agnostic in _fully distributed settings_.

**Notations.** Throughout this paper, we denote by \([]\) the expectation of a random variable, \(\|\|\) the Frobenius norm, \(,\) the inner product of two vectors, \(\) the Hadamard product (entry wise), \(\) the Kronecker product. We denote by \(\) the all-ones vector, \(\) the identity matrix and \(=^{T}/n\) the averaging matrix with \(n\) dimension. For a vector or matrix \(A\) and constant \(\), we denote \(A^{}\) the entry-wise exponential operations. We denote \((x):=f(x,y^{*}(x))\) as the primal function where \(y^{*}(x)=}{}f(x, y)\), and \(_{}()\) as the projection operation onto set \(\).

## 2 Distributed Adaptive Minimax Methods

We consider the distributed minimax problem collaboratively solved by a set of agents over a network. The overall objective of the agents is to solve the following finite-sum problem:

\[_{x^{p}}\ _{y}f(x,y)= _{i=1}^{n}_{_{i}_{i}}[F_{i} (x,y;_{i})]}_{:=f_{i}(x,y)},\] (1)

where \(f_{i}:^{p+d}\) is the local private loss function accessible only by the associated node \(i=\{1,2,,n\}\), \(^{d}\) is closed and convex, and \(_{i}_{i}\) denotes the data sample locally stored at node \(i\) with distribution \(_{i}\). We consider a graph \(=(,)\), here, \(=\{1,2,...,n\}\) represents the set of agents, and \(\) denotes the set of edges consisting of ordered pairs \((i,j)\)representing the communication link from node \(j\) to node \(i\). For node \(i\), we define \(_{i}=\{j(i,j)\}\) as the set of its neighboring nodes. Before proceeding to the discussion of distributed algorithms, we first introduce the following notations for brevity:

\[_{k}:=[x_{1,k},x_{2,k},,x_{n,k}]^{T}^{n  p},\ _{k}:=[y_{1,k},y_{2,k},,y_{n,k}]^{T}^ {n d},\]

where \(x_{i,k}^{p},y_{i,k}\) denote the primal and dual variable of node \(i\) at each iteration \(k\), and

\[_{x}F(_{k},_{k};_{k}^{x}) :=[,_{x}F_{i}(x_{i,k},y_{i,k};_{i,k}^{x} ),]^{T},\] \[_{y}F(_{k},_{k};_{k}^{y}) :=[,_{y}F_{i}(x_{i,k},y_{i,k};_{i,k}^{y} ),]^{T}\]

are the corresponding partial stochastic gradients with i.i.d. samples \(_{k}^{x},_{k}^{y}\) in a compact form.

Next, we will first explain the pitfalls of directly applying centralized adaptive stepsize rules to decentralized settings, and then introduce our newly proposed solution to address the challenge.

### Non-Convergence of Direct Extensions

For the distributed minimax optimization problem as depicted in (1) involving NC-SC objective functions, we will show shortly that the Distributed Stochastic Gradient Descent Ascent (D-SGDA) method may not converge due to the inability of time-scale separation with constant stepsizes (c.f., Figure 1), which is also observed in centralized settings (Lin et al., 2020; Yang et al., 2022b). To address this issue, one can adopt the adaptive stepsize rule used in centralized TiAda (Li et al., 2023) for each individual node, which is renowned for its ability to adaptively fulfill the time-scale separation requirements. As a result, we arrive at the following Distributed TiAda (D-TiAda) algorithm.

\[_{k+1} =W(_{k}-_{x}V_{k+1}^{-}_{x}F (_{k},_{k};_{k}^{x})),\] (2a) \[_{k+1} =_{}(W(_{k}+_{y} U_{k+1}^{-}_{y}F(_{k},_{k};_{k}^{y}) )),\] (2b)

where \(_{x}\) and \(_{y}\) are the stepsizes, \(W\) is a doubly-stochastic weight matrix induced by graph \(\)(Xiao et al., 2006) (c.f., Assumption 4), and

\[V_{k+1}^{-}=\{v_{i,k+1}^{-}\}_{i=1}^{n},  U_{k+1}^{-}=\{u_{i,k+1}^{-}\}_{i=1}^{ n},\] (3)

with \(v_{i,k+1}=\{m_{i,k+1}^{x},m_{i,k+1}^{y}\},u_{i,k+1}=m_{i,k+1}^{y}\), and

\[m_{i,k+1}^{x}=m_{i,k}^{x}+\|_{x}F_{i}(x_{i,k},y_{i,k};_{i,k }^{x})\|^{2},\ m_{i,k+1}^{y}=m_{i,k}^{y}+\|_{y}F_{i} (x_{i,k},y_{i,k};_{i,k}^{y})\|^{2}\] (4)

are the local accumulated gradient norm. Note that we impose a maximum operator in the preconditioner \(v_{i,k}\), and employ different stepsize decaying rates, i.e., \(0<<<1\), for the primal and

Figure 1: Comparison among D-SGDA, D-TiAda and D-AdaST for NC-SC quadratic objective function (6) with \(n=2\) nodes and \(_{x}=_{y}\). In (a), it shows the trajectories of primal and dual variables of the algorithms, the points on the black dash line are stationary points of \(f\). In (b), it shows the convergence of \(\|_{x}f(x_{k},y_{k})\|^{2}\) over the iterations. In (c), it shows the convergence of the inconsistency of stepsizes, \(_{v}^{2}\) defined in (8), over the iterations. Notably, \(_{v}^{2}\) fails to converge for D-TiAda and \(_{v}^{2}=0\) for non-adaptive D-SGDA.

dual variables, respectively. Such design allows to balance the updates of \(x\) and \(y\), and achieves the desired time-scale separation without requiring any knowledge of parameters (Li et al., 2023).

However, in the distributed setting, such direct extension may fail to converge to a stationary point because \(v_{i,k}\) and \(u_{i,k}\) can be inconsistent due to the difference of local objective functions \(f_{i}\), In particular, we can rewrite the above vanilla distributed optimization algorithm (2) in the sense of average system of primal variables as below,

\[_{k+1} =_{k}-_{x}_{k}^{-}^{T}}{n}_{x}F(_{k},_{k};_{k}^{x} )}_{}-}_{ k+1}^{-})^{T}}{n}_{x}F(_{k},_{k};_{k}^{x} )}_{},\] (5)

where \((}_{k}^{-})^{T}:=[,v_{i,k}^{ -}-_{k}^{-},]\), \(_{k}:=^{T}_{k}/n\) and \(_{k}:=1/n_{i=1}^{n}v_{i,k}\).

It is evident that, in comparison to centralized adaptive methods, an unexpected term (i.e., \(}_{k}\)) on the right-hand side (RHS) arises due to inconsistencies. This term introduces inaccuracies in the directions of gradient descent, degrading the optimization performance. The theorem presented below reveals a gap near the stationary points in a properly designed counterexample, indicating the non-convergence of D-TiAda. The proof is available in Appendix B.3.

**Theorem 1**.: _There exists a distributed minimax problem in the form of Problem (1) and certain initialization such that after running D-TiAda with any \(0<<0.5<<1\) and \(_{x},_{y}>0\), it holds that for any \(t=0,1,2,\), we have,_

\[_{x}f(x_{t},y_{t})=_{x}f(x_{0},y_{0}) ,_{y}f(x_{t},y_{t})=_{y} f(x_{0},y_{0}),\]

_where \(_{x}f(x_{0},y_{0})\) and \(_{y}f(x_{0},y_{0})\) can be arbitrarily large depending on the initialization._

**Remark 1**.: _The counterexample we constructed consists of three nodes, forming a complete graph. Without the stepsize tracking, D-TiAda will remain stationary, and the iterates will not progress if initiated along a specific line. In this counterexample, the only stationary point is at \((0,0)\), but initial points along the line (c.f., Eq. (72)) can be positioned arbitrarily far away from this stationary point, implying the non-convergence of D-TiAda with certain initialization._

Apart from the counterexample discussed in Theorem 1, we also experimentally observe the divergence of of D-SGDA and D-TiAda even in a simple scenario involving only two connected agents. This phenomenon is illustrated in Figure 1 and the functions are depicted as follows:

\[f_{1}(x,y) =-y^{2}+y-x+xy-x^{2},\] (6) \[f_{2}(x,y) =-y^{2}+y-x+2xy-2x^{2}.\]

It is not difficult to verify that the points on the line \(3y=5x+2\) are stationary points of \(f(x,y)=1/2(f_{1}(x,y)+f_{2}(x,y))\). It follows from Figure 1(a) and 1(b) that D-SGDA does not converge to a stationary point because of the lack of time-scale separation, and D-TiAda also fails to converge due to stepsize inconsistency, as shown in Figure 1(c). In contrast, the utilization of the stepsize tracking protocol in D-AdaST ensures convergence to a stationary point, with the inconsistency in stepsizes gradually diminishing (c.f., Lemma 9). These two motivating examples effectively highlight the challenges associated with applying adaptive minimax algorithms to distributed settings.

### The Proposed D-AdaST Algorithm

To address the issue of stepsize inconsistency across different nodes, we propose the following Distributed Adaptive minimax optimization algorithm with Stepsize Tracking protocol, termed D-AdaST, which allows us to asymptotically eliminate the stepsize inconsistency in a decentralized manner over networks. The pseudo-code for the algorithm is summarized in Algorithm 1, and can be rewritten in a compact form as follows:

\[_{k+1}^{x} =W(_{k}^{x}+_{k}^{x}),\] (7a) \[_{k+1}^{y} =W(_{k}^{y}+_{k}^{y}),\] (7b) \[_{k+1} =W(_{k}-_{x}V_{k+1}^{-}_{x}F (_{k},_{k};_{k}^{x})),\] (7c) \[_{k+1} =_{}(W(_{k}+_{y}U_ {k+1}^{-}_{y}F(_{k},_{k};_{k}^{y} ))),\] (7d)where \(_{k}^{x}=[,m_{i,k}^{x},]^{T}\), \(_{k}^{y}=[,m_{i,0}^{y},]^{T}\) denote the tracking variables for the accumulated global gradient norm, i.e., for \(z\{x,y\}\),

\[^{T}}{n}_{k+1}^{z}=_{i=1}^{ n}(_{t=0}^{k}\|g_{i,t}^{z}\|^{2}+m_{i,0}^{z})\]

while \(_{k}^{z}=[, g_{i,k}^{z}^{2},]^{T}\), and \(V_{k},U_{k}\) are diagonal matrices with \(v_{i,k}=\{m_{i,k}^{x},m_{i,k}^{y}\}\) and \(u_{i,k}=m_{i,k}^{x}\). Note that we also provide a variant of D-AdaST with coordinate-wise adaptive stepsizes in Algorithm 2, along with its convergence analysis in Appendix B.5.

```
0:\(x_{i,0}^{p}\), \(y_{i,0}\), buffers \(m_{i,0}^{x}=m_{i,0}^{y}=c>0\), stepsizes \(_{x},_{y}>0\), exponential factors \(0<<<1\) and weight matrix \(W\).
1:for iteration \(k=0,1,\), each node \(i[n]\), do
2: Sample i.i.d. \(g_{i,k}^{x}=_{x}F_{i}(x_{i,k},y_{i,k};_{i,k}^{x})\) and \(g_{i,k}^{y}=_{y}F_{i}(x_{i,k},y_{i,k};_{i,k}^{y})\).
3: Accumulate the gradient norm: \[m_{i,k+1}^{x}=m_{i,k}^{x}+\|g_{i,k}^{x}\|^{2},\ m_{i,k+1}^{y}=m_{i,k}^{y}+ \|g_{i,k}^{y}\|^{2}.\]
4: Compute the ratio: \[_{i,k+1}=(m_{i,k+1}^{x})^{}/\{(m_{i,k+1}^{x})^{},(m_{ i,k+1}^{y})^{}\} 1.\]
5: Update primal and dual variables locally: \[x_{i,k+1}=x_{i,k}-_{x}_{i,k+1}(m_{i,k+1}^{x})^{-}g_ {i,k}^{x},\ y_{i,k+1}=y_{i,k}+_{y}(m_{i,k+1}^{y})^{-}g_{i,k}^{y}.\]
6: Communicate adaptive stepsizes and decision variables with neighbors: \[\{m_{i,k+1}^{x},m_{i,k+1}^{y},x_{i,k+1},y_{i,k+1}\} _{j_{i}}W_{i,j}\{m_{j,k+1}^{x},m_{j,k+1}^{y},x_{j,k+ 1},y_{j,k+1}\}\]
7: Projection of dual variable on the set \(\): \(y_{i,k+1}_{}(y_{i,k+1})\).
8:endfor ```

**Algorithm 1** Distributed Adaptive Minimax Method with Stepsize Tracking (D-AdaST)

## 3 Convergence Analysis

In this section, we present the main convergence results for the proposed D-AdaST algorithm and compare it with D-TiAda to show the effectiveness of the proposed stepsize tracking protocol.

To this end, letting \(_{k}:=1/n_{i=1}^{n}u_{i,k}\), we define the following metrics to evaluate the level of inconsistency of stepsizes among nodes, which are ensured to be bounded by Assumption 3.

\[_{v}^{2}:=_{i[n],k>0}\{(v_{i,k}^{-}-_{k}^{-})^{2}/(_{k}^{-})^{2}\},\ _{u}^{2}:=_{i[n],k>0}\{(u_{i,k}^{-}-_{k }^{-})^{2}/(_{k}^{-})^{2}\}.\] (8)

### Assumptions

We consider the NC-SC setting of Problem (1) with the following assumptions that are commonly used in the existing works (c.f., Remark 2 and Remark 3). Notably, for the function and algorithm class determined by the assumptions of this work, Li et al. (2021) derived a lower complexity bound of \((^{-4})\) and proved that such a dependency on \(\) is optimal (c.f., Remark 2).

**Assumption 1** (\(\)-strong concavity in \(y\)).: _Each objective function \(f_{i}(x,y)\) is \(\)-strongly concave in \(y\), i.e., \( x^{p}\), \( y,y^{}\) and \(>0\),_

\[f_{i}(x,y)-f_{i}(x,y^{})_{ y}f_{i}(x,y),y-y^{}+\|\ y-y^{}\|^{2}.\] (9)

**Assumption 2** (Joint smoothness).: _Each objective function \(f_{i}(x,y)\) is \(L\)-smooth in \(x\) and \(y\), i.e., \( x,x^{}^{p}\) and \( y,y^{}\), there exists a constant \(L\) such that for \(z\{x,y\}\),_

\[\|_{z}f_{i}(x,y)-_{z}f_{i}(x^{},y^{ })\|^{2} L^{2}(\|x-x^{}\|^{2} +\|y-y^{}\|^{2}).\] (10)

_Furthermore, \(f_{i}\) is second-order Lipschitz continuous for \(y\), i.e., for \(z\{x,y\}\),_

\[\|_{zy}^{2}f_{i}(x,y)-_{zy}^{2}f_{i}(x^{ },y^{})\|^{2} L^{2}(\|x-x^{} \|^{2}+\|y-y^{}\|^{2}).\] (11)

**Remark 2**.: _Assumption 1 does not require the convexity in \(x\) and the objective function thus can be nonconvex. Assumption 1 and 2 ensure that \(y^{*}()\) is smooth (c.f., Lemma 2), which is essential for achieving (near) optimal convergence rate (Chen et al., 2021; Li et al., 2023). Besides, it can be verified that the constructed 'hard' examples for obtaining the lower complexity bound in Li et al. (2021) satisfy the above second-order Lipschitz continuity (11) on \(y\), implying that the achievable optimal complexity for the function and algorithm class considered in this work is \((^{-4})\)._

**Assumption 3** (Stochastic gradient).: _For i.i.d. sample \(_{i}\), the stochastic gradient of each \(i\) is unbiased, i.e., \( x^{p},y\), \(_{_{i}}[_{z}F_{i}(x,y;_{i})]= _{z}f_{i}(x,y)\), for \(z\{x,y\}\), and there is a constant \(C>0\) such that \(\|_{z}F_{i}(x,y;_{i})\| C\)._

**Remark 3**.: _Assumption 3 on unbiased stochastic gradient is widely used for establishing convergence rates of both minimization and minimax optimization methods with AdaGrad (Kavis et al., 2022; Li et al., 2023) or Adam (Zou et al., 2019; Chen et al., 2023a; Huang et al., 2024) adaptive stepsize. We note that under Assumption 2, this assumption can be easily satisfied in many real-world tasks by imposing constraints on the compact domain of \(f\), e.g., neural networks with rectified activation (Dinh et al., 2017) and GANs with projections on the critic (Gulrajani et al., 2017)._

Next, we make the following assumption on the underlying graph to ensure its connectivity.

**Assumption 4** (Graph connectivity).: _The weight matrix \(W\) induced by graph \(\) is doubly stochastic, i.e., \(W=,^{T}W=^{T}\) and \(_{W}:=\|W-\|_{2}^{2}<1\)._

Note that one can always find a proper weight matrix \(W\) compliant to the graph that satisfies Assumption 4 once the underlying graph is undirected and connected. For instance, the weight matrix can be easily determined based on the Metropolis-Hastings protocol (Xiao et al., 2006). Moreover, this assumption is more general than that in Lian et al. (2017); Borodich et al. (2021) in the sense that \(W\) is not required to be symmetric, implying that certain directed graphs can be included in this assumption, e.g., directed ring and exponential graphs (Ying et al., 2021).

### Main Results

We are now ready to present the key convergence results in terms of the primal function \((x):=f(x,y^{*}(x))\) with \(y^{*}(x)=}{}f( x,y)\), whose proofs can be found in Appendix B.4.

**Theorem 2**.: _Suppose Assumption 1-4 hold. Let \(0<<<1\) and the total iteration \(K\) satisfy_

\[(\{(^{2}^{4}}{_{y}^{2}} )^{},\;()^{2}} )^{\{,\}}\})\] (12)

_with \(:=L/\) to ensure time-scale separation and quasi-independence of the network. For D-AdaST, we have 1_

\[_{k=0}^{K-1}[\|(_{k} )\|^{2}]=}(}+ )^{}K^{}})+}(}+)K^{}} ).\] (13)

**Remark 4** (Near-optimal convergence).: _Theorem 2 implies that if the total number of iterations satisfies the conditions (12), the proposed D-AdaST algorithm converges to a stationary point exactly for Problem (1) with an \(}(^{-(4+)})\) sample complexity for arbitrarily small \(>0\), e.g., letting\(=0.5+/(8+2)\) and \(=0.5-/(8+2)\). It is worth noting that this rate is near-optimal compared to the existing lower bound of \((^{-4})\)(Li et al., 2021) for a class of smooth NC-SC functions. Moreover, this result recovers the centralized TiAda algorithm (Li et al., 2023) as a special case, i.e., setting \(_{W}=0\), without assuming the existence of interior optimal point (c.f., Assumption 3.3 Li et al. (2023)). To the best of our knowledge, there is no existing fully parameter-agnostic method that achieves a convergence rate of \(}(^{-4})\), even in a centralized setting._

**Remark 5** (Parameter-agnostic property and transient times).: _The above results show that D-AdaST converges without requiring to know any problem-dependent parameters, i.e., \(L\), \(\) and \(_{W}\), or tuning the initial stepsize \(_{x}\) and \(_{y}\), and is thus parameter-agnostic. Moreover, we explicitly characterize the transient times (c.f., Eq. (12)) that ensure time-scale separation and quasi-independence of the network, respectively. Indeed, we can see that if \(\) and \(\) are close to each other, the time required for time-scale separation to occur increases significantly, which has been observed in (Li et al., 2023). On the other hand, if \(\) and \(\) are relatively large, then \(}(1/K^{1-}+1/K^{1-})\) dominates the other terms, indicating independence on the network. These observations highlight the trade-offs between the convergence rate and the required duration of the transition phase._

For proper comparison, we also derive an upper bound for D-TiAda as follows. Together with the lower bound in Theorem 1, we demonstrate that without the stepsize tracking mechanism, the inconsistency among local stepsizes prevents D-TiAda from converging in the distributed setting.

**Corollary 1**.: _Under the same conditions of Theorem 2. For the proposed D-TiAda, we have_

\[_{k=0}^{K-1}[\|( _{k})\|^{2}] =}(}+)^{}K^{}})\] (14) \[+}(}+)K^{}})+}((_{v}^{ 2}+^{2}_{u}^{2})C^{2}).\]

## 4 Experiments

In this section, we conduct experiments to validate the theoretical findings and demonstrate the effectiveness of the proposed algorithm on real-world machine learning tasks. We compare the proposed D-AdaST with the distributed variants of AdaGrad (Duchi et al., 2011), TiAda (Li et al., 2023) and NeAda (Yang et al., 2022b), namely D-AdaGrad, D-TiAda and D-NeAda, respectively. These experiments run across multiple nodes with different networks, and we consider heterogeneous distributions of local objective functions/datasets. For example, each node can only access samples with a subset of labels on MNIST and CIFAR-10 datasets, which is a common scenario in decentralized and federated learning tasks (Sharma et al., 2023; Huang et al., 2022). The experiments cover three main tasks: synthetic function, robust training of the neural network, and training of Wasserstein GANs (Heusel et al., 2017). For the exponential factors of stepsize, we set \(=0.6\) and \(=0.4\) for both D-TiAda and D-AdaST. More detailed settings and additional experiments with different initial stepsizes, data distributions and choices of \(\) and \(\) can be found in Appendix A.

**Synthetic example.** We consider a distributed minimax problem with the following NC-SC local objective functions over exponential networks with \(n=50\) (\(_{W}=0.71\)) and \(n=100\) (\(_{W}=0.75\)).

\[f_{i}(x,y)=-y^{2}+L_{i}xy-^{2}}{2}x^{2}-2L_{i }x+L_{i}y,\] (15)

where \(L_{i}(1.5,2.5)\). The local gradient of each node is computed with an additive \((0,0.1)\) Gaussian noise. It follows from Figure 2 (a) and 2 (b) that the proposed D-AdaST algorithm outperforms other distributed adaptive methods for both initial stepsize settings, especially in cases with a favorable initial stepsize ratio, as illustrated in plots (b) and (d) where \(_{x}/_{y}=0.2\). Similar observation can be found in Figure 2 (c) and 2 (d), demonstrating the effectiveness of D-AdaST.

**Robust training of neural networks.** Next, we consider the task of robust training of neural networks, in the presence of adversarial perturbations on data samples (Sharma et al., 2022; Deng and Mahdavi, 2021). The problem can be formulated as \(_{x}_{y}1/n_{i=1}^{n}f_{i}(x;_{i}+y)- \|y\|^{2}\), where \(x\) denotes the parameters of the model, \(y\) denotes the perturbation and \(_{i}\) denotes the data sample of node \(i\). Note that if \(\) is large enough, the problem is NC-SC. We conduct experiments on MNIST dataset over different networks, e.g., ring graph, exponential (exp.) graph (Ying et al., 2021) and dense graph with \(n/2\) edges for each node. We consider a heterogeneous scenario in which each node possesses only two distinct classes of labeled samples, resulting in heterogeneity among the local datasets across nodes, while the data is i.i.d within each node.

In Figure 3, we compare D-AdaST with D-AdaGrad, D-TiAda and D-NeAda, using adaptive stepsizes in AdaGrad (first row) and Adam (second row, name suffixed with Adam) respectively, it can be observed from the first three columns that the proposed D-AdaST outperforms the others on three different graphs and it is not very sensitive to the graph connectivity (i.e., \(_{W}\)), demonstrating the quasi-independence of network as indicated in Theorem 2. It should be noted that Adam-like algorithms exhibit more fluctuations in the later stages of optimization as the gradient norm vanishes, leading to an inevitable increase in the Adam stepsize as the optimization process converges (Kingma and Ba, 2014). In plots (d) and (h), we further demonstrate that D-AdaST can scale efficiently with respect to the number of nodes, while keeping a constant batch-size of 64 for each node. This showcases the algorithm's ability to handle large-scale distributed scenarios effectively.

**Generative Adversarial Networks.** We further illustrate the effectiveness of D-AdaST on another popular task of training GANs, which has a generator and a discriminator used to generate and distinguish samples respectively (Goodfellow et al., 2014). In this experiment, we train Wasserstein GANs (Gulrajani et al., 2017) on CIFAR-10 dataset in a decentralized setting where each discriminator is 1-Lipschitz and has access to only two classes of samples. We compare the inception score of D-AdaST with D-Adam and D-TiAda adopting Adam-like stepsizes in Figure 4. It can be observed from the figure that D-AdaST achieves higher inception scores in three cases with different initial stepsizes, and has a small score loss as the initial step size changes. We believe that this example shows the great potential of D-AdaST in solving real-world problems.

Figure 3: Comparison of the algorithms on training robust CNN on MNIST dataset. The first row shows the results of AdaGrad-like stepsize, and the second row is for Adam-like stepsize. For the first three columns, we compare the algorithms on _different graphs_ with \(n=20\). For the last column, we show the scalability of D-AdaST in terms of number of nodes. Initial stepsizes are set as \(_{x}=0.01,_{y}=0.1\) for AdaGrad-like stepsize, and \(_{x}=0.1,_{y}=0.1\) for Adam-like stepsize.

Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts \(n=\{50,100\}\) and _different initial stepsizes_ (\(_{y}=0.1\)).

## 5 Conclusion

We introduced a new distributed adaptive minimax method, D-AdaST, designed to tackle the issue of non-convergence in nonconvex-strongly-concave minimax problems caused by the inconsistencies among locally computed adaptive stepsizes. Vanilla distributed adaptive methods could suffer from such inconsistencies, as highlighted by the carefully designed counterexamples for demonstrating their potential non-convergence. In contrast, our proposed method employs an efficient adaptive stepsize tracking protocol that not only ensures the time-scale separation, but also guarantees stepsize consistency among nodes and thus effectively eliminates steady-state errors. Theoretically, we showed that D-AdaST can achieve a near-optimal convergence rate of \(}(^{-(4+)})\) with any arbitrarily small \(>0\). Extensive experiments on both real-world and synthetic datasets have been conducted to validate our theoretical findings across various scenarios.