# ManiBCI: Manipulating EEG BCI with Invisible and Robust Backdoor Attack via Frequency Transform

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The electroencephalogram (EEG) based brain-computer interface (BCI) has taken the advantages of the tremendous success of deep learning (DL) models, gaining a wide range of applications. However, DL models have been shown to be vulnerable to backdoor attacks. Although there are extensive successful attacks for image, designing a stealthy and effective attack for EEG is a non-trivial task. While existing EEG attacks mainly focus on single target class attack, and they either require engaging the training stage of the target DL models, or fail to maintain high stealthiness. Addressing these limitations, we exploit a novel backdoor attack called **ManiBCI**, where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage. Specifically, ManiBCI is a three-stages clean label poisoning attacks: **1)** selecting one trigger for each class; **2)** learning optimal injecting EEG electrodes and frequencies masks with reinforcement learning for each trigger; **3)** injecting the corresponding trigger's frequencies into poisoned data for each class by linearly interpolating the spectral amplitude of both data according to the learned masks. Experiments on three EEG datasets demonstrate the effectiveness and robustness of ManiBCI. The proposed ManiBCI also easily bypass existing backdoor defenses. Code will be published after the anonymous period.

## 1 Introduction

Deep learning (DL) has greatly boosted the performances of the electroencephalogram (EEG) based brain-computer interfaces (BCI), which have been widely used in medical diagnosis , healthcare , and device control [3; 4]. While DL-based systems are shown to be vulnerable to backdoor attacks (BA) [5; 6; 7], where an adversary embeds a hidden backdoor into a DL models to maliciously control it's outputs for inference samples containing particular triggers (a.k.a, poisoned samples), the security of the DL-based EEG BCI has been long neglected.

However, compared to image, designing an effect and stealthy BA for EEG is not trivial for three difficulties, which lead to three questions. **D1**: EEG data has a significantly low signal-to-noise ratio (SNR) , even the accuracies of original EEG tasks are very low . **Q1**: How to develop an EEG BA with high attack success rate (ASR) while preserving the clean accuracies of original task? **D2**: Previous studies demonstrated for different EEG tasks, there are some different critical EEG electrodes and frequencies that strongly related to the performance of EEG BCI [10; 11; 12; 13; 14], indicating that the trigger-injection strategy (_i.e._, which electrodes and frequencies to inject triggers) inevitably affect the performance of BA. **Q2**: How to find the optimal strategy for different EEG tasks? **D3**: Certain classes of EEG have specific morphology that can easily be identified by human expert, _e.g._, in epilepsy detection, the amplitudes of the ictal phase EEG are larger than those of the normal state phase EEG . **Q3**: How to maintain the consistency of the label and the morphology?The first BA for EEG modality is demonstrated in Fig 1 (d), where the narrow period pulse (NPP) signals are added as the trigger for single target class attack [16; 17]. To generate invisible trigger, the adversarial loss is applied to learn a spatial filter as the trigger function . Recently, some BA for time series (EEG signal is a kind of time series) adopt generative adversarial net (GAN) to produce poisoned data [19; 20]. However, there are rich information in the frequency domain of EEG [21; 22; 23; 24]. No matter these BA are stealthy or not, they all inject unnatural perturbation in the temporal domain, which will inevitably bring unnatural frequency into the real EEG frequency domain.

In this paper, we propose a novel backdoor attack for **man**ipulating EEG **BCI** called **ManiBCI** to address **Q1**, which injects triggers in the frequency domain. Specifically, ManiBCI is a three-stage clean label poisoning attack demonstrated in Fig 1 (a-c): **1)**: selecting \(c\) triggers from \(c\) classes, as these triggers are all real EEG, the frequency of these triggers are all natural. Thus, the poisoned data are similar to the real EEG as shown in Fig 2(b). **2)**: learning optimal injecting strategies for each trigger with reinforcement learning to enhance the performance of EEG BA, addressing **Q2. 3)**: injecting each trigger's frequencies into clean EEG of the same class as the triggers for each class, which maintains the consistency of the label and morphology, addressing **Q3**.

The main contributions of this paper are summarized below:

* We propose a novel backdoor attack for EEG BCI called **ManiBCI**, which can attack arbitrary class while preserving stealthiness without engaging the training stage.
* To the best of our knowledge, it is the first work that considers the efficacy of different EEG electrodes and frequencies in EEG backdoor attacks with reinforcement learning.
* Extensive experiments on three EEG BCI datasets demonstrate the effectiveness of ManiBCI and the robustness against several common EEG preprocessings and backdoor defenses.

## 2 Related Work

### Backdoor Attacks

Backdoor attacks has been deeply investigated in image processing filed [25; 26; 27]. BadNets  is the first BA, where the adversary maliciously control the DL to misclassify the input images contain suspicious patches to a target class. Other non-stealthy attacks like blended  and sinusoidal strips based  were studied then. To achieve higher stealthiness, some data poisoning BA were developed,

Figure 1: (a)-(c) The framework of ManiBCI: (a) The trigger selection and EEG data distribution from the view of manifold learning. (b) Learning optimal electrodes and frequencies injection strategies. (c) The generation process of ManiBCI. (d) The payloads of the existing backdoor attacks. (e) The payloads of ManiBCI, which can arbitrarily manipulate the output of EEG BCI models.

including shifting color spaces , warping , regularization  and frequency-based [33; 34; 35; 36; 37; 38]. Other stealthy attacks [39; 40; 41] generate invisible trigger patterns by adversarial loss, which requires the control of the model's training process.

Recently, the EEG-based BCIs have shown to be vulnerable to BA [16; 17; 18]. The NPP signals are added to clean EEG to generate non-stealthy poisoned samples in [16; 17], which significantly modifies the spectral distribution (as shown in Fig 2 (a)) and results in low stealthiness. From the view of data manifold in Fig 1 (a), NPP-added EEG are fake data. To generate more stealthy poisoned data which stay in the real data boundary. The adversarial loss has been applied backdoor EEG BCI  and time series [19; 20], but these methods require controlling the training process of the backdoor models and can only attack a single target class. Meng _et.al._ tried to achieve multi-target attacks with adding different types of signals to clean EEG, _i.e._, NPP, sawtooth, sine, and chirp . However, these signals are not stealthy in both the temporal and frequency domain. To attack multi-target class with high stealthiness, Marksman backdoor  generates invisible sample-specific patterns for each possible class, but it needs controlling the training stage. Moreover, generating trigger patterns with a neural network for each sample is time-consuming.

Different from the EEG BA in the temporal domain, we firstly propose to attack in the frequency domain. Our attack is more stealthy than NPP-based attack, faster than other trigger generation attack, and more practical as requiring no control of the target models. It is worth noting that the frequency-based BA for image [33; 34; 35; 36; 37; 38] cannot be applied for time series, as they do not consider the characteristics of time series and fail to maintain the stealthiness for poisoned time series data.

### Backdoor Defenses

To cope with the security problems of backdoor attacks, several categories of defensive methods have been developed. Neural Cleanse  is a trigger reconstruction based methods. If the reconstructed trigger pattern is significantly small, the model is identified as a backdoor model. Assuming the trigger is still effective when a triggered sample is combining with a clean sample, STRIP  detects the backdoor model by feeding the combined samples into the model to see if the predictions are still with low entropy. Spectral Signature  detects the backdoor model based on the latent representations. Fine-Pruning  erases the backdoor by pruning the model.

Besides the above defenses designed for backdoor attacks, there are some common EEG preprocessing methods, such as bandstop filtering and down-sampling, should be considered when designing a practical robust backdoor attack for EEG BCI in the real-world scene.

## 3 Methodology

### EEG BCI Backdoor Attacks and Threat Model

Under the supervised learning setting, a classifier \(f\) is learned using a labeled training set \(=\{(x_{1},y_{1}),...,(x_{N},y_{N})\}\) to map \(f:\), where \(x_{i}\) and \(y_{i}\). The attacker in single target class backdoor attacks aims to learn a classifier \(f\) behaves as follows:

\[f(x_{i})=y_{i},\ \ f(T(x_{i}))=c_{tar},\ \ c_{tar},\ (x_{i},y_{i}) ,\] (1)

where \(T:\) is the trigger function and \(c_{tar}\) is the target label. For multi-target class backdoor attacks, the trigger function has an extra parameter \(c_{i}\), which manipulates the behavior of \(f\) flexibly:

\[f(x_{i})=y_{i},\ \ f(T(c_{i},x_{i}))=c_{i},\ \  c_{i}, (x_{i},y_{i}).\] (2)

We consider a malicious data provider, who generates a small number of poisoned samples (labeled with the target class) and injects them into the original dataset. A victim developer collects this poisoned dataset and trains his model, which will be infected a backdoor.

We use a cross-validation setting to evaluate all BAs, each EEG dataset \(\) is divided into three parts: training set \(_{train}\), poisoning set \(_{p}\), and test set \(_{test}\). Specifically, for a dataset contains \(n\) subjects, we select one subject's data as \(_{p}\) one by one, and the remaining \(n-1\) subjects to perform

Figure 2: t-SNE visualization.

leave-one-subject-out (LOSO) cross-validation, _i.e._, one of the subjects as \(_{test}\), and the remaining \(n-1\) subjects as \(_{train}\) (one of the subjects in \(_{train}\) is chosen to be validation set). In summary, for a dataset contains \(n\) subjects, there are \(n(n-1)\) runs to validate each EEG BCI backdoor attack method. A poisoned subset \(_{p}\) of \(M\) (\(M<N\)) examples is generated based on \(_{p}\). Then \(_{p}\) is combined with \(_{train}\) to acquire \(=\{_{p},_{train}\}\). The poisoning ratio is defined as : \(=M/N\).

### Reinforcement Learning for Optimal Trigger-Injection Strategies

The learning of the injecting electrodes set \(_{e}^{c_{i}}\) and frequencies set \(_{f}^{c_{i}}\) for each selected trigger in class \(c_{i}\) can be formulated as a non-convex optimization problem. Under this optimization framework, the strategy generator function will learn the optimal \(_{e}^{c_{i}}\) and \(_{f}^{c_{j}}\) for each EEG trigger to implement ManiBCI BA on target DL model \(f\), which is supposed to have a high clean accuracy (CA) on the clean data and attack success rate (ASR) on the poisoned data:

\[_{_{e}^{c_{i}},_{f}^{c_{i}}}_{(x_{i},y_{i })}[(f(x_{i}),y_{i})+(f((x_{i},x_{c_{i}}^{t},,_{e}^{c_{i}},_{f}^{c_{i}})),c_{ i})].\] (3)

However, finding the optimal adaptive injecting strategies for each trigger is not trivial as the searching space is too large (_e.g._, if injecting half of the 62 electrodes, there are \( 4.65 10^{17}\) cases for deciding \(_{e}^{c_{i}}\)). Reinforcement learning (RL) is an appropriate method for tackling this questions. The objective of RL is to find a sampler \(\) to maximize the expect of the reward function:

\[^{*} =_{}_{()}[R()]=_{ }_{}[R() p_{}()]\] (4) \[=_{}_{}[R()_{0}(s_{1}) _{t=1}^{T-1}(a_{t}|s_{t})(s_{t+1}|s_{t},a_{t})],\]

where \(R()\) is reward function of a trajectory \(=(s_{1},a_{1},r_{1},....s_{T})\), the \(s_{i},a_{i},r_{i}\) means the state, action, and reward at time \(i\). The \(_{0}\) indicates the sampler of initial state. In our settings, the action (strategies) do not affect the state (triggers). Hence, we can simplify Eq 4 by removing the states \(s_{i}\):

\[^{*}=_{}_{}[R()_{t=1}^{ T-1}(a_{t})].\] (5)

However, we do not care about the reward of the whole trajectory, we only acquire a single strategy for each trigger. Thus, we replace the \(R()\) with \(R(a_{t})\) and select the \(a_{t}\) whose \(R(a_{t})\) is the biggest as the optimal strategy. Here, an RL algorithm called policy gradient  is adopted to learn an agent (_i.e._, policy network \(_{}^{c_{i}}\) with parameters \(\)) to find the optimal strategy for each trigger. After removing the state \(s_{t}\) and replacing \(R()\), the gradient estimator is:

\[=_{}_{_{}()}[R()]= _{}[R(a_{t}) p_{_{}}(a_{t})]=_{t} [R_{t}(a_{t})_{}_{}],\] (6)

where \(a_{t}\) and \(R_{t}\) is the action and estimator of the reward function at timestep \(t\). The expectation \(_{t}\) indicates the empirical average. Here, \(a_{t}=\{_{e}^{c_{i}},_{f}^{c_{i}}\}\). The parameters of \(_{}^{c_{i}}\) are updated by \(_{t+1}=_{t}+\), \(\) is the learning rate. We run the RL for T steps and take the best \(a_{t}\) as the strategy.

The CA and ASR are obtained by implementing ManiBCI only on \(\). Specifically, we use a concise network as the agent which takes the extracted spatial-temporal features from triggers into account to generate better policy. This agent has two output vectors \(v_{1}^{E},v_{2}^{F}\), where \(E\) and \(F\) is the number of EEG electrodes and frequencies. The electrodes and frequencies are in \(_{e}^{c_{i}}\) and \(_{f}^{c_{i}}\) only if the corresponding positions in \(v_{1}\) and \(v_{2}\) have Top-_k_ values, \(k\) is \( E\) for electrodes and \( F\) for frequencies, where \(,(0,1]\) are hyperparameters.

Besides the performance of CA and ASR, there are two important concerns: **C1:** Robustness against common EEG preprocessing-based defenses; **C2:** Stealthiness against human perceptions. The reason why we consider **C1** is that the bandstop filtering is widely used for preprocessing EEG signals. For instance, if we inject the triggers into a concentrated frequency band 50-60Hz, it is easy to filter the trigger out using a 50Hz low pass filter, resulting in attack failure. Thus, scattering the injection positions in various frequency can effectively evade from specific frequency filter defenses. To address **C2**, injecting the trigger into higher frequencies is more invisible than lower frequencies . Taking all into consideration, we define the estimator of the reward function \(R_{t}\) as follows:

\[R_{t}(a_{t})=R_{t}(_{e}^{c_{i}},_{f}^{c_{i}})=+ \;+\;(_{f}^{c_{i}})+( _{f}^{c_{i}}),\] (7)where the \(^{c_{j}}_{f}\) indicates the set of all injecting frequency positions, and \(()\) calculates the minimal distance between each pair of positions. Thus, \((^{c_{i}}_{f})\) is the discrete (DIS) loss, and \((^{c_{j}}_{f})\) is the high frequency (HF) loss, which can scatter the injection positions in various frequency bands and inject as high frequencies as possible. The \(,,\) are hyperparameters.

### Poisoned Data Generation via Frequency Transform

After selecting the \(C\) triggers from each class and learning the strategy for each trigger, the poisoned data are generated by injecting these triggers into clean data with the corresponding strategies. As shown in Fig 1(c), given a clean data \(x_{i}_{p}\) with label \(c_{i}\), and a trigger data \(x^{t}_{c_{i}}\), let \(^{A}\) and \(^{P}\) be the amplitude and phase components of the fast Fourier transform (FFT) result of a EEG signals, we denote the amplitude and phase spectrum of \(x_{i}\) and \(x^{t}_{c_{i}}\) as:

\[_{x_{i}}=^{A}(x_{i}),_{x^{t}_{c_{i}}}= ^{A}(x^{t}_{c_{i}}),\ \ \ _{x_{i}}=^{P}(x_{i}),_{x^{t}_{c_{i}}}= ^{P}(x^{t}_{c_{i}}).\] (8)

The new poisoned amplitude spectrum \(^{P}_{x_{i}}\) is produced by linearly interpolating \(_{x_{i}}\) and \(_{x^{t}_{c_{i}}}\). In order to achieve this, we produce a binary mask \(^{c_{i}}^{E F}=1_{(j,k)},j^{c_{i}}_ {c},k^{c_{i}}_{f}\), whose value is 1 for positions of all corresponding to elements in both electrode and frequency sets and 0 elsewhere. Denoting \((0,1]\) as the linear interpolating ratio, the new poisoned amplitude spectrum can be computed as follows, where \(\) indicates Hadamard product:

\[^{P}_{x_{i}}=[(1-)_{x_{i}}+_{x^{t }_{c_{i}}}]^{c_{i}}+_{x_{i}}(1-^{c _{i}}).\] (9)

Finally, we adopt the injected poisoned amplitude spectrum \(^{P}_{x_{i}}\) and the clean phase spectrum \(_{x_{i}}\) to get the poisoned data by inverse FFT \(^{-1}\):

\[x^{p}_{i}=^{-1}(^{P}_{x_{i}},_{x_{i}}).\] (10)

By generating \(x^{p}_{i}\) through this frequency injection approach, we obtain a subset \(_{p}=\{x^{p}_{1},...,x^{p}_{M}\}\), which will combine with \(_{train}\) to form the whole traing dataset \(\). The EEG DL model \(f\) is then trained with \(\) to obtain the ability of behvasing as equation 2.

## 4 Experiments

### Datasets, Baselines, and Experimental Setup

**Emotion Recognition (ER) Dataset** SEED  is a discrete EEG emotion dataset studying three types of emotions: happy, neutral, and sad. SEED collected EEG from 15 subjects.

**Motor Imagery (MI) Dataset** BCIC-IV-2a  dataset recorded EEG from 9 subjects while they were instructed to imagine four types of movements: left hand, right hand, feet, and tongue.

**Epilepsy Detection (ED) Dataset** CHB-MIT  is an epilepsy dataset required from 23 patients. We cropped and resampled the CHB-MIT dataset to build an ED dataset with four types of EEG: icital, preictal, postictal, and interictal phase EEG.

**Non-stealthy Baselines** As mentioned in previous sections, to the best of our knowledge, ManiBCI is the first work that studies multi-trigger and multi-target class (MT) backdoor in EEG BCI. For comparison, we design several baseline approaches which can be divided into two main groups: non-stealthy and stealthy. Non-stealthy attacks contains **PatchMT** and **PulseMT**. For a benign EEG segment \(x^{E T}\). PatchMT is a multi-trigger and MT extension of BadNets  where we fill the first \( T\) timepoints of a EEG segments with a constant number, _e.g._, {0.1, 0.3, 0.5} for three-class task. PulseMT is a multi-trigger and MT extension of NPP-based backdoor attacks  where we use NPP signals with different amplitudes, _e.g._, {-0.8, -0.3, 0.3, 0.8} for different target classes.

**Stealthy Baselines** Previous works generate stealthy poisioned samples by controlling the training stage and can only attack single target class [18; 19; 20]. As they control the training of target model, it is unfair to directly compare their methods with ManiBCI. There is no stealthy MT BA for EEG. Thus, we design two MT stealthy attacks baselines: **CompMT** and **AdverMT**. CompMT generates poisoned samples for different target classes by compressing the amplitude of EEG with different 

[MISSING_PAGE_FAIL:6]

policy gradient outperforms GA while only spending \(16\%\) training time of GA. We plot the learning curve of RL in Appendix F.3, which demonstrates that RL learns well strategies within 50 epochs, i.e., only trains 50 backdoor models and saves lots of time. The random algorithm can achieve a not bad results, proving that our methods can be applied without RL if some performance drop is acceptable.

#### 4.2.3 Performance of Learned Mask Strategies on Other Target Models

We demonstrate that the injecting strategies learned on a EEG classifier \(f\) can be used to attack other EEG classifiers \(\). In other words, Marksman can still be effective when the adversary has no knowledge of the target models \(\). To perform the experiments, we use the strategy learned with a classifier \(f\), then generate poisoned samples to attack another classifier \(\) whose network is different from \(f\). Table 3 shows the performance difference, it can be observed that the difference is relatively small in most of the cases, demonstrating the transferability of the injecting strategy learned with reinforcement learning.

#### 4.2.4 Attack Performance with Different Hyperparameters

We investigate the influences of three different hyperparameters: poisoning rate \(\), frequency injection rate \(\), and electrode injection rate \(\). The performance of attacking EEGNet on the ED dataset are displayed in Fig 3. It can be seen that the ASRs are positively correlated with poisoning rate. Note that it is non-trivial for multi-target class attack, thus the ASR is not high compared to the single class attack. ManiBCI outperforms other attacks in all cases and is robust to the change of \(\) and \(\).

    &  &  & Epilepsy \\   & Clean & Attack & Time \(\) & Clean & Attack & Time \(\) & Clean & Attack & Time \(\) \\  Random & 0.520 & 0.771 & - & 0.291 & 0.857 & - & 0.501 & 0.721 & - \\ Genetic Algorithm & 0.516 & 0.826 & 15.2h & 0.302 & 1.000 & 10.0h & 0.492 & 0.862 & 30.5h \\ Policy Gradient & 0.535 & 0.857 & 2.5h & 0.323 & 1.000 & 1.8h & 0.477 & 0.944 & 5.2h \\   

Table 2: Clean and attack performance with with different trigger search optimization algorithms, the poisoning rate is set to 10%. The target model is EEGNet.

Figure 3: Clean (/C) and attack (/B) performance with different poisoning or injection rates.

    &  &  &  \\   & \(\) : DeepCNN & \(\) : LSTM & \(\) : EEGNet & \(\) : LSTM & \(\) : EEGNet & \(\) : DeepCNN \\  Datasets & Clean & Attack & Clean & Attack & Clean & Attack & Clean & Attack & Clean & Attack & Clean & Attack \\   & 0.458 & 0.781 & 0.485 & 0.938 & 0.516 & 0.813 & 0.490 & 0.936 & 0.516 & 0.863 & 0.497 & 0.779 \\  & 0.026 & 0.051 & 0.034 & 0.016 & 0.019 & 0.044 & 0.029 & 0.018 & 0.019 & 0.006 & 0.037 & 0.053 \\   & 0.316 & 1.000 & 0.265 & 0.946 & 0.309 & 1.000 & 0.264 & 0.972 & 0.306 & 1.000 & 0.306 & 1.000 \\  & 0.001 & 0.000 & 0.001 & 0.020 & 0.014 & 0.000 & 0.000 & 0.006 & 0.017 & 0.000 & 0.009 & 0.000 \\   & 0.442 & 0.759 & 0.469 & 0.806 & 0.448 & 0.943 & 0.445 & 0.813 & 0.448 & 0.926 & 0.427 & 0.850 \\  & 0.027 & 0.069 & 0.025 & 0.059 & 0.029 & 0.001 & 0.001 & 0.052 & 0.029 & 0.018 & 0.042 & 0.022 \\   

Table 3: Clean and attack performance on other models. Red values represent the decreasing performance in attacks with \(f\) is the same as \(\). Blue values mean increments or unchanged.

### Robustness of ManiBCI

In this section, we evaluate the robustness of our ManiBCI against different EEG preprocessing method and various representative backdoor defenses.

#### 4.3.1 Robustness against EEG Preprocessing Methods

To develop an EEG BCI, it is very common to preprocess the raw EEG signals, _e.g._, 1) band-stop filtering and 2) down-sampling. An EEG backdoor attack is impractical in real scenarios if it is no longer effective when the target model is trained with the preprocessed poisoned EEG. Hence, we must take the robustness against preprocessing methods into account, which is widely ignored in the image backdoor attack field. The performance of each method facing different preprocessing methods are presented in Table 4. It can be observed that our ManiBCI is robust in all cases. However, when removing the DIS loss, the performance of ManiBCI decreases a lot after EEG preprocessing, especially facing the 30 Hz high-stop filtering preprocessing due to the HF loss that encourages the policy network learns to injecting high frequency.

#### 4.3.2 Robustness against Neural Cleanse: Trigger Inversion

Neural Cleanse (NC)  calculate a metric called Anomaly Index by reconstructing trigger pattern for each possible label. The Anomaly Index is positively correlated with the size of the reconstruction trigger. A model with Anomaly Index > 2 is considered to be backdoor-injected. We display the Anomaly Indexes of the clean models and the backdoor-injected model by ManiBCI in Fig 4. It can be seen that ManiBCI can easily bypass NC. The reconstructed trigger patterns on three datasets are presented in Appendix F.1.

#### 4.3.3 Robustness against STRIP: Input Perturbation

We evaluate the robustness of ManiBCI against STRIP , which perturbs the input EEG and calculates the entropy of the predictions of these perturbed EEG data. Based on the assumption that the trigger is still effective after perturbation, the entropy of backdoor input tends to be lower than that of the clean one. The results are plotted in Fig 5, it can be seen that the entropy distributions of the backdoor and clean samples are similar.

    & Preprocessing &  &  &  &  & Average \\   & Method & Clean & Attack & Clean & Attack & Clean & Attack & Clean & Attack & ASR \\  }}{}}\)} & ManiBCI & 0.535 & 0.857 & 0.512 & 0.829 & 0.463 & 0.892 & 0.518 & 0.908 & 0.876 \\  & w/o DIS & 0.506 & 0.859 & 0.492 & 0.816 & 0.466 & 0.333 & 0.498 & 0.807 & 0.652 \\  }}{}}\)} & ManiBCI & 0.323 & 1.000 & 0.285 & 1.000 & 0.329 & 1.000 & 0.321 & 1.000 & 1.000 \\  & w/o DIS & 0.298 & 1.000 & 0.264 & 1.000 & 0.322 & 0.250 & 0.284 & 0.990 & 0.746 \\  }}{}}\)} & ManiBCI & 0.497 & 0.944 & 0.492 & 0.914 & 0.494 & 0.856 & 0.516 & 0.818 & 0.920 \\  & w/o DIS & 0.515 & 0.250 & 0.477 & 0.864 & 0.508 & 0.250 & 0.510 & 0.249 & 0.454 \\   

Table 4: Clean and attack performance on three datasets after different EEG preprocessing methods. The target model is EEGNet. M w.o. DIS means removing the DIS loss in ManiBCI.

Figure 4: Anomaly Index of three models on three datasets.

Figure 5: Performance against STRIP on three datasets, the target model is EEGNet.

#### 4.3.4 Robustness against Spectral Signature: Latent Space Correlation

Spectral Signature  detects the backdoor samples by statistical analysis of clean data and backdoor data in the latent space. Following the same experimental settings in , we randomly select 5,000 clean samples and 500 ManiBCI backdoor samples and plot the histograms of the correlation scores in Fig 6. There is no clear separation between these two sets of samples, showing the stealthiness of ManiBCI backdoor samples in the latent space.

#### 4.3.5 Robustness against Fine-Pruning

We evaluate the robustness of Marksman against Fine-Pruning , a model analysis based defense which finds a classifier's low-activated neurons given a small clean dataset. Then it gradually prunes these low-activated neurons to mitigate the backdoor without affecting the CA. We can observe from Fig 7 that the ASR drops considerably small when pruning ratio is less than 0.7, suggesting that the Fine-Pruning is ineffective against ManiBCI.

### Visualization of Backdoor Attack Samples

To evade from human perception (**C2** in Section 3.2), we design to obatin injecting strategies with HF loss. It can be seen from the bottom row of Fig 8 that ManiBCI (with HF loss) generates stealthy poisoned EEG, which is almost the same as the clean EEG, demonstrating the **High Stealthiness**. The poisoned EEG will be conspicuous compared to the clean EEG if remove the HF loss.

## 5 Conclusion

In this paper, we proposed ManiBCI, a novel EEG backdoor for manipulating EEG BCI, where the adversary can arbitrarily control the output for any input samples. To the best of our knowledge, ManiBCI is the first method that considers which EEG electrodes and frequencies to be injected by adopting a reinforcement learning called policy gradient to learn the adaptive injecting strategies for different EEG triggers and tasks. We specially design the reward function in RL to enhance the robustness and stealthiness of ManiBCI. The perturbation of the trigger on clean EEG is almost invisible. Our experimental results over three common EEG datasets demonstrate the effectiveness of ManiBCI and the stealthiness against the existing representative defenses. This work calls for defensive studies to counter ManiBCI for EEG modality.

Figure 8: The Clean EEG (Blue), Trigger-injected EEG (Orange) and the Residual (Red) of the ED dataset. The \(x\)-axis is the timepoints, the \(y\)-axis is the normalized amplitude. Top row: w.o. HF loss; Bottom row: with HF loss. Each column indicates each possible class.

Figure 6: Performance against Spectral Signature on three datasets, the target model is EEGNet.

Figure 7: Performances of EEGNet against Fine-Pruning on three datasets.