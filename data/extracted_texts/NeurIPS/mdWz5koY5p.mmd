# RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space

Jingdi Chen

The George Washington University

jingdic@gwu.edu

&Hanhan Zhou

The George Washington University

hanhan@gwu.edu

&Yongsheng Mei

The George Washington University

ysmei@gwu.edu

&Carlee Joe-Wong

Carnegie Mellon University

cjoewong@andrew.cmu.edu

&Gina Adam

The George Washington University

ginaadam@gwu.edu

&Nathaniel D. Bastian

United States Military Academy

nathaniel.bastian@westpoint.edu

&Tian Lan

The George Washington University

tlan@gwu.edu

###### Abstract

Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return. In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss. Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).

Introduction

Deep Reinforcement Learning (DRL) has significantly advanced real-world applications in various domains [5; 12; 13; 16; 17; 29; 44; 69; 71]. However, the black-box nature of DRL's deep neural networks, with their multi-layered structures and millions of parameters, makes them largely uninterpretable. This lack of interpretability is particularly problematic in safety-critical sectors such as healthcare and aviation, where clarity in machine decision-making is crucial [14; 15; 27; 31; 43; 45; 51].

Decision trees (DTs) address this problem by supporting rules and decision lists that enhance human understanding [1; 11; 35; 36; 38; 56]. However, there are two main challenges to applying DTs in DRL: (1) DTs extracted from DRL often lack performance guarantees [2; 22]. There have been imitation learning-type approaches that train DTs with samples drawn from the trained DRL policy , but the return gap is characterized in terms of number of samples needed and does not apply to DTs with arbitrary size constraints (e.g., on the maximum number of nodes \(L\)). (2) Although learning DT policies for interpretability has been investigated in the single-agent RL setting [37; 42; 55; 58], it is under-explored in the multi-agent setting, except for centralized methods like MAVIPER , **which lacks a performance guarantee**. In multi-agent, decentralized settings where rewards are jointly determined by all agents' local DTs, any changes in one agent's DT will impact the optimality of other agents' DTs. The return gap for such decentralized DTs has not been considered.

In this paper, we propose a DT extraction framework, a Return-Gap-Minimization Decision Tree (RGMDT) algorithm, that is proven to achieve a closed-form guarantee on the expected return gap between a given RL policy and the resulting DT policy. **Our key idea** is that each decision path of the extracted DT maps a subset of observations to an action attached to the leaf node. Thus, constructing a DT can be considered an iterative clustering problem of the observations into different decision paths, with actions at leaf nodes as labels. Since the action-value function \(Q(o,a)\) represents the potential future return, it can be leveraged to obtain a return gap bound for the process. We show that it recasts the DT extraction problem as a non-Euclidean clustering with respect to a loss defined using \(Q(o,a)\). Due to its iterative structure, RGMDT supports an iterative algorithm to generate return-gap-minimizing DTs of arbitrary size constraints.

Further, we extend our algorithm to **multi-agent settings** and provide a performance bound on the return gap. A naive approach that simply converts each agent's (decentralized) policy into a local DT cannot ensure global optimality of the resulting return since the reward is jointly determined by the DTs constructed for all agents. We develop an **iteratively-grow-DT** process, which iteratively identifies the best step to grow the DT of each agent, conditioned on the current DTs of other agents (**by revising the resulting action-value function**) until the desired complexity is reached. Thus, the impact of decentralized DTs on the joint reward is captured during this process. The method ensures that we learn decentralized DTs yet also guarantees a return gap.

More precisely, we show that the problem is recast into a **non-Euclidean clustering** problem in the observation space with \(n\) agents. Each agent \(j\) computes an updated action-value function by conditioning it on other agents' current DTs (thus capturing the impact of their DT constructions on each other). Then, guided by this updated action-value-function, agent \(i\) grows its DT by adding another label (i.e., decision path and corresponding leaf node) by a clustering function \(=[_{i}(a_{i}|o_{i},_{-i}), i]\), which is conditioned on its local observations \(o_{i}\) and the DT policies of other agents \(_{-i}\). The process continues iteratively across all agents until their decentralized DTs reach the desired size constraints. To analyze the resulting return gap, we show that the difference between the given DRL policy and the DTs can be characterized using a Policy Change Lemma, relying on the average distance between joint action values in each cluster (i.e., decision path). Intuitively, if the observations following the same decision path and arriving at the same leaf node are likely maximized with the same action, the DT policy would have a small return gap with the DRL policy \(^{*}\). We show that the return gap is bounded by \(O((L+1)-1)n}Q_{})\), with respect to the maximum number of leaf nodes \(L\) of the DT, the number of agents \(n\), the highest action-value \(Q_{}\) and the average cosine-distance \(\) between joint action-value vectors corresponding to the same clusters. The result allows us to minimize this upper bound to find the optimal DTs.

The main **contributions** are: (1). We quantify the return gap between an oracle DRL policy and its extracted DTs via an upper bound with respect to the DT size constraints. (2). We propose RGMDT, an algorithm that constructs multi-agent decentralized DTs of arbitrary sizes by minimizing the upper bound of the return gap. Instead of drawing samples from the DRL policy for DT learning, RGMDT recasts the problem as an iterative non-Euclidean clustering problem of the observations into different decision paths, with actions at leaf nodes as labels. (3). Rather than generating a DT and subsequently applying pruning algorithms to achieve the desired complexity, the RGMDT framework constructs DTs of any given size constraint while minimizing the upper bound of the resulting expected return gap. (4). We show that RGMDT significantly outperforms baseline DT algorithms and can be applied to complex tasks like D4RL with significant improvement.

The remainder of this paper is organized as follows. Section 2 reviews related work on decision trees and their integration with reinforcement learning, with a focus on interpretability and multi-agent systems. Section 3 introduces our problem formulation and defines the return gap. In Section 4, we present the theoretical results, recasting return gap minimization as clustering in a non-Euclidean space, and deriving bounds for both single-agent and multi-agent settings. Section 5 details the construction of SVM-based decision trees and the iteratively growing framework for minimizing return gaps, with pseudocode in the appendix. Section 6 describes the experimental setup and presents empirical results. Finally, Section 7 concludes with a summary of contributions and future directions.

## 2 Related Work

**Effort on Interpretability for Understanding Decisions.** To enhance interpretability in decision-making models, one strategy involves crafting interpretable reward functions within inverse reinforcement learning (IRL), as suggested by [10; 67; 68]. This approach offers insights into the underlying objectives guiding the agents' decisions. Agent behavior has been conceptualized as showing preferences for certain counterfactual outcomes , or as valuing information differently when under time constraints . However, extracting policies through black-box reinforcement learning (RL) algorithms often conceals the influence of observations on the selection of actions. An alternative is to directly define the agent's policy function with an interpretable framework. Reinforcement learning policies have thus been articulated using a high-level programming language , or by framing explanations around desired outcomes , facilitating a more transparent understanding of decision-making processes.

**Interpretable RL via Decision Tree-based models.** Since their introduction in the 1960s, DTs have been crucial for interpretable supervised learning [47; 50; 53]. The CART algorithm , established in 1984, is foundational in DT methodologies and underpins Random Forests (RF)  and Gradient Boosting (GB) , which are benchmarks in predictive modeling. These techniques are central to platforms like ranger and scikit-learn and continue to evolve, as seen in the iterative random forest, which explores stable interactions in data [4; 52; 64; 70]. To interpret an RL agent, Frosst et al  explain the decisions made by DRL policies by using a trained neural network to create soft decision trees. Coppens et al.  propose distilling the RL policy into a differentiable DT by imitating a pre-trained policy. Similarly, Liu et al.  apply an imitation learning framework to the Q-value function of the RL agent. They also introduce Linear Model U-trees (LMUTs), which incorporate linear models in the leaf nodes. Silva et al.  suggest using differentiable DTs directly as function approximators for either the Q function or the policy in RL. Their approach includes a discretization process and a rule list tree structure to simplify the DTs and enhance interpretability. Additionally, Bastani et al.  propose the VIPER method, which distills policies as neural networks into a DT policy with theoretically verifiable capabilities that follow the Dataset Aggregation (DAGGER) method , specifically for imitation learning settings and nonparametric DTs. Ding et al.  try to solve the instability problems when using imitation learning with tree-based model generation and apply representation learning on the decision paths to improve the decision tree-based explainable RL results, which could achieve better performance than soft DTs. Milani et al. extend VIPER methods into multi-agent scenarios  in both centralized and decentralized ways, they also summarize a paper about the most recent works in the fields of explainable AI (artificial intelligence) , which confirms the statements that small DTs are considered naturally interpretable.

However, traditional DT methods are challenging to integrate with RL due to their focus on correlations within training data rather than accounting for the sequential and long-term implications in dynamic environments. Imitation learning approaches have been explored in single-agent RL settings [42; 55; 58]. For instance, VIPER  trains DTs with samples from a trained DRL policy, but its return gap depends on the number of samples needed and does not apply to DTs with arbitrary size constraints (e.g., maximum number of nodes \(L\)). DT algorithms remain under-explored in multi-agentsettings, except for MAVIPER  that extends VIPER to multi-agent scenarios while lacking performance guarantees and remaining a heuristic design.

## 3 Preliminaries and Problem Formulation

A **Dec-POMDP** models cooperative MARL, where agents lack complete information about the environment and only have local observations. We formulate a Dec-POMDP as a tuple \(D= S,A,P,,n,R,\), where \(S\) is the joint **state** space and \(A=A_{1} A_{2} A_{n}\) is the joint **action** space, where \(=(a_{1},a_{2},,a_{n}) A\) denotes the joint action of all agents. \(P(^{}|,):S A S\) is the **state transition function**. \(\) is the **observation** space. \(n\) is the total number of agents. \(R(,):S A\) is the **reward function** in terms of state \(\) and joint action \(\), and \(\) is the discount factor. Given a policy \(\), we consider the average expected return \(J()=_{T}(1/T)E_{}[_{t=0}^{T}R_{t}]\). The goal of this paper is to minimize the return gap between the pre-trained RL policy providing action-values \(^{*}=[_{i}^{*}(a_{i}|o_{1},,o_{n}), i]\) and the DT policy \(=[_{i}(a_{i}|o_{i},_{-i}), i]\) where decision tree policies \(_{-i}=\{_{j}=_{j}(o_{j},l_{j}), j i\}\), where \(l_{j}=g(o_{j})\), and the function \(g\) is the clustering function for observation \(o_{j}\). Define the **return gap** as:

\[ J(^{*})-J().\] (1)

While the problem is equivalent to maximizing \(J()\), the return gap can be analyzed more easily by contrasting \(\) and \(^{*}\). We derive an upper bound of the return gap and then design efficient clustering strategies to minimize it. We consider the discounted observation-based state value and the corresponding action-value functions for the Dec-POMDP:

\[V^{}()=_{}[_{i=0}^{}^{i} R_{t+ i}_{t}=,_{t}],Q^{}(, )=_{}[_{i=0}^{}^{i} R_{t+i} _{t}=,_{t}=],\] (2)

where \(t\) is the current time step. Re-writing the average expected return as an expectation of \(V^{}()\):

\[J()=_{ 1}E_{}[(1-)V^{}()],\] (3)

where \(\) is the initial observation distribution at time step \(t=0\), i.e., \((0)\). We will leverage this state-value function \(V^{}()\) and its corresponding action-value function \(Q^{}(,)\) to unroll the Dec-POMDP and derive a closed-form upper-bound to quantify the return gap.

## 4 Theoretical Results and Methodology

We first recast the problem as clustering in a **non-Euclidean Metric Space** and then prove a single-agent result. Then we expand the single-agent result to multi-agent settings. Since directly minimizing the return gap is intractable, we bound the performance of RGMDT with the return gap between the _oracle_ policy \(^{*}=[_{i}^{*}(a_{i}|o_{1},,o_{n}), i]\)**corresponding to obtaining the action-values \(Q^{^{*}}\)** and optimal decision tree policy \(^{L}=[^{L}_{1},,^{L}_{n}]\), where each \(_{i}\) **can only have \(L\) nodes**, and \(n\) is the total number of agents. For simplicity, we use \(V^{*}\) to represent \(V^{^{*}}\), and \(Q^{*}\) to represent \(Q^{^{*}}\). We assume that observation/action spaces defined in the Dec-POMDP tuple are discrete with finite observations and actions, i.e., \(||<\) and \(|A|<\). For Dec-POMDPs with continuous observation and action spaces, the results can be easily extended by considering cosine-distance between action-value functions and replacing summations with integrals, or sampling the action-value functions as an approximation.

**Lemma 4.1**.: _(Policy Change Lemma.) For any policies \(^{*}\) and DT policy \(^{L}\) with \(L\) leaf nodes, the optimal expected average return gap is bounded by:_

\[J(^{*})-J(^{L}) _{l}_{ l}[Q^{*}(,_{ t}^{^{*}})-Q^{^{L}}(,_{t}^{^{L}})]d_{ }^{^{L}}(),\] (4) \[d_{}^{^{L}}() =(1-)_{t=0}^{}^{t} P(_{t}= |^{L},),\]

_where \(d_{}^{^{L}}()\) is the \(\)-discounted visitation probability under decision tree \(^{L}\) and initial observation distribution \(\), and \(_{ l}\) is a sum over all observations corresponding to the decision path from the parent node to the leaf node \(l\), where \(l\) indicates its class._

**Proof Sketch.** Our key idea is to leverage the state value function \(V^{^{L}}()\) and its corresponding action-value function \(Q^{^{L}}(,)\) in Eq.(2) to unroll the Dec-POMDP from timestep \(t=0\) and onward. Detailed proof is provided in the Appendix.

Then we define the **action-value vector** corresponding to observation \(o_{j}\), i.e.,

\[^{*}(o_{j})=[^{*}(o_{j},_{-j}), o_{-j}],\] (5)

where \(_{-j}\) are the observations of all other agents and \(^{*}(o_{j},_{-j})\) is a vector of action-values weighted by marginalized visitation probabilities \(d_{}^{}(_{-j}|o_{j})\) and corresponding to different actions, i.e., \(^{*}(o_{j},_{-j})=[Q^{*}(o_{j},_{-j},^{* }(a_{j}|o_{j}),_{-j}) d_{}^{}(_{-j}|o_{j})]\). **At the initial iteration step of the iteratively-grow-DT process**, since we did not grow the DT for agent \(j\) yet, we use _oracle_ policy \(^{*}\) to give a deterministic action \(a_{j}^{*}=_{a_{j}}Q_{j}^{*}(o_{j},a_{j})\) based on \(o_{j}\) for obtaining action-value vectors, and \(_{-j}\) are all possible actions for agents except for agent \(j\), which makes \(^{*}(o_{j},_{-j})\) a vector where each entry corresponds to the full set of actions \(_{-j}\) across all agents except for agent \(j\).

Next, constructing decision trees can be considered an iterative clustering problem of the observations into different decision paths \(l_{j}=g(o_{j})\), with actions at leaf nodes as labels. Then, the \(o_{j}\) are divided into clusters, each labeled with the clustering label \(l_{j}\) to be used for constructing DT. We bound the policy gap between \(_{(j)}^{*}\) and \(_{(j)}^{L}\),using the average cosine-distance of action-value vectors \(^{*}(o_{j})\) corresponding to \(o_{j}\) in the same cluster and its cluster center \((l)=_{o_{j} l}_{l}(o_{j})^{*}(o_{j})\) under each label \(l\). Here \(_{l}(o_{j})=d_{}^{^{L}}(o_{j})/d_{}^{^{L}} (l)\) is the marginalized probability of \(o_{j}\) in cluster \(l\) and \(d_{}^{^{L}}(l)\) is the probability of label \(l\) under DT \(^{L}\), and the environments' initial observation distribution is represented by \((t=0)\). To this end, we let \((o_{j})=D_{cos}(^{*}(o_{j}),(l))\) be the cosine-distance between vectors \(^{*}(o_{j})\) and \((l)\) and consider the **average cosine-distance \(\)** across all clusters represented by different clustering labels \(l\) for one iteration of growing DT:

\[_{l}d_{}^{}(l)_{o_{j} l}_{l}(o_{j} )(o_{j}),\] (6)

The result is summarized in Thm. 4.2.

**Theorem 4.2**.: _(Impact of Decision Tree Conversion.) Consider two optimal policies \(_{(j)}^{*}\) and \(_{(j)}^{L}\) obtained from the policy providing action-values and the DT, the optimal expected average return gap is bounded by:_

\[J(_{(j)}^{*})-J(_{(j)}^{L}) O((L+1)- 1)}Q_{})\] (7)

_where \(Q_{}\) is the maximum absolute action-value of \(^{*}(o_{j})\) in each cluster as \(Q_{}=max_{o_{j}}\|^{*}(o_{j})\|_{2}\), and \(\) is the average cosine-distance defined in Eq.(6), \(L\) is maximum number of leaf nodes of the resulting DT._

**Proof Sketch.** We give an outline below and provide the proof in the Appendix.

_Step 1: Recasting DT construction into a Non-Euclidean Clustering._ Viewing the problem as a clustering of \(o_{j}\), restrict policy \(_{(j)}\) (conditioned on \(l_{j}\)) to take the same actions for all \(o_{j}\) in the same cluster under the same label \(l_{j}\). We perform clustering on the observation \(o_{j}\) within the observation space \(_{j}\) by grouping the corresponding action-value vectors \(^{*}(o_{j})\) in the action-value vector space \(_{j}\), using the cosine-distance function \(D_{}\) as the clustering metric. We demonstrate that \((_{j},D_{})\) constitutes a **Non-Euclidean Metric Space**. The findings are detailed in Lemma 4.3, with the full proof provided in the Appendix.

**Lemma 4.3**.: _(Cosine Distance Metric Space Lemma.) Let \(_{j}\) be a set of observations with associated vector representations in \(^{m}\) obtained through a mapping function \(^{*}\), and let \(D_{cos}:_{j}_{j}\) be a distance function defined as:_

\[D_{cos}(o_{j}^{a},o_{j}^{b})=1-f(^{*}(o_{j}^{a}),^{*}(o_{j}^{b})) =1-^{*}(o_{j}^{a})^{*}(o_{j}^{b})}{^{*}(o_{j} ^{a})\|\|^{*}(o_{j}^{b})\|}\] (8)

_where \(f\) denotes the cosine similarity. Then, the pair \((_{j},D_{cos})\) forms a metric space. The proof is in Appendix.__Step 2: Rewrite the return gap in vector form_. Re-writing the optimal expected average return gap derived in Policy Change Lemma 4.1 in vector terms using action-value vectors \(^{*}(o_{j})\) and an auxiliary maximization function \(_{}(^{*}(o_{j}))\) that returns the largest component of vector \(^{*}(o_{j})\):

\[J(^{*}_{(j)})-J(_{(j)})_{l}d_{}^{}(l)[_{o_{j}  l}_{l}(o_{j})_{}(^{*}(o_{j}))-_{}(_ {o_{j} l}_{l}(o_{j})^{*}(o_{j}))],\] (9)

_Step 3: Projecting action-value vectors toward cluster centers_. By projecting \(^{*}(o_{j})\) toward \((l)\), \(^{*}(o_{j})\) could be re-written as \(^{*}(o_{j})=Q^{}(o_{j})+_{o_{j}}_{l}\), then we could upper bound \(_{max}(^{*}(o_{j}))\) by:

\[_{}(^{*}(o_{j}))_{}(_{o_{j}} _{l})+_{}(Q^{}(o_{j})).\]

Taking a sum over all \(o_{j}\) in the cluster, we have \(_{o_{j} l}_{l}(o_{j})_{}(_{o_{j}} _{l})=_{}(_{l})\), since the projected components \(_{o_{j}}_{l}\) should add up to exactly \(_{l}\). To bound Eq.(9)'s return gap, it remains to bound the orthogonal components \(Q^{}(o_{j})\).

_Step 4: Deriving the upper bound w.r.t. cosine-distance._ We derive an upper bound on the return gap by bounding the orthogonal projection errors using the average cosine distance within each cluster.

\[_{max}(Q^{}(o_{j})) O()}Q_{}).\] (10)

Using the concavity of the square root with Eq.(6), we derive the desired upper bound \(J(^{*}_{(j)})-J(_{(j)}) O(Q_{})\)**for one iteration of growing the DT**, then **for \(I=_{2}(L+1)-1\) iterations** (\(I\) is the depth of the DT), the upper bound is \(O((L+1)-1)}Q_{})\). Here we assume that the DT is a perfectly balanced or full binary tree. If it is a complete binary tree, the number of iterations is \(I=_{2}(L)\). In the worst-case scenario of a highly unbalanced tree, the number of iterations is \(I=L-1\). Then we extend this upper bound to multi-agent settings in Thm. 4.4.

**Theorem 4.4**.: _In \(n\)-agent Dec-POMDP, the return gap between policy \(^{*}\) corresponding to the obtained action-values and decision tree policy \(^{L}\) conditioned on clustering labels is bounded by:_

\[J(^{*})-J(^{L}) O((L+1)-1)}nQ_{}).\] (11)

**Proof Sketch.** Beginning from \(^{*}=[^{*}_{i}(a_{i}|o_{1},o_{2},,o_{n}), i]\), we can construct a sequence of \(n\) policies, each replacing the conditioning on \(o_{j}\) by constructed decision tree \(^{L}_{j}\), for \(j=1\) to \(j=n\), one at a time. This will result in the decision tree policies \(^{L}=[^{L}_{j}(a_{j}|o_{j},^{L}{}_{-j})]\). Applying Thm. 4.2 for \(n\) times, we prove the upper bound between \(J(^{*})\) and \(J(^{L})\) for multi-agent scenarios.

_Remark 4.5_.: Thm. 4.4 holds for any arbitrary finite number of leaf nodes \(L\). Furthermore, increasing \(L\) reduces the average cosine distance (since more clusters are formed) and, consequently, a reduction in the return gap due to the upper bound derived in Thm. 4.4.

## 5 Constructing SVM-based Decision Tree to Minimize Return Gaps

The result in Thm. 4.4 inspires a **iteratively-grow-DT** framework - RGMDT, which constructs return-gap-minimizing multi-agent decentralized DTs of arbitrary sizes. This is because RGMDT grows a binary tree iteratively (in both single and multi-agent cases) until the desired complexity is reached. The method addressed **two challenges**: (1). RGMDT constructs the optimal DT that minimizes the return gap given the complexity of the DT (e.g., the number of the leaf nodes). (2). RGMDT addressed the scalability problems of multi-agent DT construction with provided theoretical guarantees. We summarize the pseudo-code in the Appendix.

**Non-Euclidean Clustering Labels Generation.** We approximate the non-Euclidean clustering labels \(l_{j}=g(o_{j})\) for each agent using DNNs parameterized by \(=\{_{1},,_{n}\}\). Prior to growing the decision tree (DT) for each agent \(j\), we sample a minibatch of \(K_{1}\) transitions \(_{j}\) from the replay buffer \(\), which includes observation-action-reward pairs from all agents. We then identify the top \(K_{2}\) most frequent observations \(^{k_{2}}_{-j}\) in \(_{j}\), and retrieve their associated actions using the pre-trained policy \(^{*}\), forming a set \(_{-j}\). We combine these samples with \((o_{j},^{*}(o_{j}))\) from \(_{j}\) to form the dataset \(\) for training. The _oracle_ critic networks, parameterized by \(\), compute the action-values for this dataset, approximating the vectors \(^{*}(o_{j})\). \(g_{_{j}}(o_{j})\) is updated by optimizing a Regularized Information Maximization (RIM) loss function :

\[(g_{_{i}})=_{p=1}^{K_{1}}_{q N_{K_{3}}(p)}[D_{cos }(^{*}(o_{j}^{p}),^{*}(o_{j}^{q})]\|_{J}^{p}-l_{j}^{q}\|^{2 }-[H(m_{j})-H(m_{j}|o_{j})],\] (12)

the first term is a locality-preserving clustering loss, which enhances the cohesion of clusters by encouraging action-value vectors close to each other to be grouped together. This is achieved using the cosine distance \(D_{}\) to identify the \(K_{3}\) nearest neighbors of each action-value vector. The second term, the mutual information loss, quantifies the mutual information between the observation \(o_{j}\) and the cluster label \(m_{j}\). It aims to balance cluster size and clarity by evaluating the difference between the marginal entropy \(H(m_{j})\) and the conditional entropy \(H(m_{j}|o_{j})\).

**Single-agent DT Construction via Iterative Clustering.** To grow a decision tree (DT) for agent \(j\), each decision path of RGMDT maps a subset of observations to an action attached to the leaf node (**decision paths for different leaf node counts are visualized in Appendix**). The process iteratively groups observations and assigns actions as labels at each path's end. The main challenge is determining the optimal division of observations at each node and defining the split conditions. **For each split**, the RGMDT framework uses a sample matrix of observations \(^{d n}\) and associated class labels \(^{1 n}\), where \(n\) and \(d\) are the number of examples and dimensions, respectively. Each label \(l_{i}=g(o_{i})\) corresponds to a category \(\{l_{1},l_{2},,l_{L}\}\), which is generated from **non-euclidean clustering function \(g\)** with **cosine-distance loss function**\((g_{_{i}})\), and \(L\) being the maximum number of leaf nodes. We use **Support Vector Machines (SVM)** to identify the optimal hyperplane \(()\) that maximizes the margin between the closest class points, known as support vectors. This results in the optimized hyperplane \(^{*}()\) defined by \(^{*}-p^{*}=0\). The DT, structured as a binary tree for multi-class classification, incorporates this SVM-derived hyperplane. Each node splits into two child nodes based on the hyperplane's criteria: left for \(^{*}-p^{*}<0\) and right for \(^{*}-p^{*} 0\). **Note that the splits are still based on linear thresholds.** The construction of the \(_{j}\) is complete until it reaches the maximum number of leaf nodes \(L\) by **iteratively repeating the splitting and clustering for \(I=_{2}(L)+1\) iterations**. The child node in the last iterations will become a leaf node assigned the class label \(l=_{i}\{}{r}\ |\ i=l_{1},l_{2},,l_{L}\}\), where \(r\) is the total number of sample points in the current leaf node, and \(n_{i}\) is the number of sample points in class \(i\). Classification can be conducted based on the \(_{j}\). For an observation with an unknown class, it constantly goes through the split nodes and finally reaches a leaf node where \(l\) indicates its class .

**Iteratively-Growing-DT for Multi-agent.** In each iteration, we obtain a revised \(Q(,)\) by conditioning it on the current DTs of all other agents, i.e., if two actions are merged into the same decision path, then the \(Q(,)\) are merged too. Then we grow the DT as guided by the revised \(Q(,)\), which iteratively identifies the best step to grow the DT of each agent conditioned on the current DTs of other agents. To simplify, consider agents \(i j\) as a conceptual super agent \(-j\). With a constraint of \(L\) leaf nodes, we iteratively grow DTs for agent \(j\) and super agent \(-j\), growing one more level for each agent per iteration with a total of \(_{2}(L)+1\) iterations. At the initial iteration (\(i=0\)), for agent \(j\), we calculate \(^{*}_{i=0}(o_{j},_{-j})=[Q^{*}(o_{j},_{-j}, ^{*}(a_{j}|o_{j}),_{-j}) d^{}_{}(_{-j}|o_{j})]\) and grow DT \(^{i=0}_{j}\) using the clustering function \(g(o_{j})\). For super agent \(-j\), the DT is based on \(^{*}_{i=0}(_{-j},o_{j})\) which integrates the DT from agent \(j\). Each subsequent iteration updates the DTs by recalculating \(^{*}\) using the prior iteration's DTs to ensure consistent action choices within clusters for both agents.

## 6 Evaluation and Results

**Experiment Setup and Baselines.** We test RGMDT on both **discrete** and **continuous** state space problems in the maze environments and the **D4RL**. To explore how well RGMDT scales in **multi-agent** environments, we also constructed similar target-chasing tasks in the maze following the same settings as the Predator-Prey tasks in the Multi-Agent Particle Environment (MPE)  (detailed in Appendix). We use Centralized Q Learning and Soft Actor-critic (SAC)  to obtain the action-value vectors. We compare RGMDT against strong **baselines**. The baselines include **different types of Imitation DTs**: Each DT policy is directly trained using a dataset collected by running the expert policies for multiple episodes. No resampling is performed. The observations of an agent are the features, and the actions of that agent are the labels. **(1). Single Tree Model**using **CART**, which is the most famous traditional DT algorithm whose splits are determined by choosing the best cut that maximizes the gain. **(2).** DT algorithms with Bootstrap Aggregating **error correction methods**: **Random Forest (RF)** that reduces variance by averaging multiple deep DTs, trained on different parts of the same training set; **Extra Trees (ET)** that uses the whole dataset to grow the trees which make the boundaries more randomized. **(3). Boosting For Error Correction, Gradient Boosting DTs** that builds shallow trees in succession where each new tree helps to correct errors made by the previously trained tree. **(4). Multi-agent Verifiable Reinforcement Learning via Policy Extraction (MAVIPER):** A centralized DT training algorithm that jointly grows the trees of each agent , which extends VIPER  in multi-agent settings. All the experiments are **repeated 3-5 times** with different seeds. More details about the evaluations are in the Appendix.

**Single Agent Task.** In the single-agent task, an agent is trained to navigate to a target without colliding with walls to complete the task. We increase the complexity of the environment and evaluate the RGMDT across three different levels of complexity (detailed in Appendix). For each experiment, we compare agents' performance using RGMDT against baselines using two metrics: the number of time steps required to complete the task (fewer is better) and the mean episode rewards (higher is better). We conducted each experiment five times for both metrics using different random seeds to ensure robustness.

Figures 0(a)-0(c) illustrate the **normalized task completion steps** for all methods relative to RGMDT's average performance. RGMDT consistently outperforms the baselines by completing tasks in fewer steps, increasing its advantage in more complex tasks. In the most challenging task, where only 10 steps are allowed in complete the hard task, RGMDT typically finishes in about 5 steps,

    &  &  \\   & 40 nodes & 64 nodes & 40 nodes & 64 nodes \\ 
**RGMDT** & \(\) & \(\) & \(\) & \(\) \\ CART & \(443.47 153.49\) & \(448.85 154.12\) & \(458.47 131.48\) & \(460.90 90.40\) \\ RF & \(345.81 178.43\) & \(452.89 134.76\) & \(456.41 124.23\) & \(489.92 71.69\) \\ ET & \(196.55 147.92\) & \(448.98 119.40\) & \(441.01 138.43\) & \(451.80 83.24\) \\   

Table 1: RGMDT achieved higher rewards with a smaller sample size and node counts on D4RL.

Figure 1: Evaluation on Maze tasks. (a)-(c): RGMDT (purple bar) completes the tasks in fewer steps than all the baselines. (d)-(f): RGMDT (blue line) achieves a higher mean episode reward than all the baselines in all scenarios with varying complexities, which illustrates its ability to minimize the return gap in hard environments.

whereas the baselines often fail, resulting in normalized steps that are twice as high as RGMDT. Figures (d)d to (f)f display the **mean episode reward** curves over 300/500 episodes for methods tested post-offline training with DTs of maximum 4 nodes in 3 types of tasks. RGMDT consistently outperforms all baselines, with the performance improvement becoming more obvious as task complexity increases. In the simplest task, both RGMDT and baseline DTs earn positive rewards. However, RGMDT shows a significant performance advantage in the medium complexity task. In the most challenging task, while most baselines struggle to complete the task, RGMDT completes it and achieves higher rewards. The results show RGMDT's effectiveness in minimizing the negative effects of fewer decision paths, thereby maintaining its performance in increasingly complex environments.

Figure 2 demonstrates the impact of leaf node counts on RGMDT's performance in the hard task, comparing mean episode rewards for RGMDT and baselines with \(|L|=4,8,16,32\) leaf nodes. RGMDT's performance improves with an increasing number of leaf nodes. Notably, with just \(|L|=4\) leaf nodes, RGMDT is the only method to complete the task and outperform all baselines. With more than \(|L|=4\) leaf nodes, RGMDT's performance approaches the near-optimal levels of the expert RL policy. This performance supports Thm. 4.4's prediction that return gaps are bounded by \(O((L)+1)}nQ_{})\), and that these gaps decrease as more leaf nodes reduce the average cosine-distance across clusters, as noted in Remark 4.5. **Decision paths for different leaf node counts are visualized in the Appendix.**

**D4RL.** Table. 1 presents the achieved rewards for four algorithms (RGMDT, CART, RF, and ET) on the Hooper problem instance from the D4RL datasets , evaluated across different training sample sizes (800,000 and 80,000) and DT node counts (40 and 64). Each cell shows the achieved reward and its standard deviation. The bold values indicate that RGMDT achieves higher rewards than all baselines, especially with smaller sample sizes and node counts. When trained with \(800,000\) samples, RGMDT performs better when the node counts are reduced from \(64\) to \(40\), while the baselines' achieved rewards decrease. When the sample size is reduced tenfold to \(80,000\), RGMDT achieves \(69.33\%\) and \(21.40\%\) improvements in the achieved rewards with \(64\) and \(40\) nodes, respectively, whereas other baselines suffer from the sample size reduction. It shows that RGMDT provides a succinct, discrete representation of the optimal action-value structure, leading to less noisy decision

Figure 3: Comparisons on the \(n\)-agent tasks: (a)-(c) 2 agents, (d)-(f) 3 agents. RGMDT with limited leaf nodes can learn these tasks much faster than the baselines and have better final performance, even when most of the baselines fail on the task with \(2\) and \(4\) leaf nodes.

Figure 2: The normalized mean episode reward increases as the total number of leaf nodes increases(Hard Maze).

paths and allowing agents to discover more efficient decision-making DT conditioned on the non-euclidean clustering labels with smaller sample sizes and fewer nodes.

**Multi-Agent Task.** In Figures 2(a)-2(c), RGMDT outperforms all baselines, particularly as the number of leaf nodes decreases. Notably, with just 2 leaf nodes, all baselines fail to complete the task, whereas RGMDT succeeds with significantly higher rewards. This performance trend continues in more challenging scenarios, as shown in Figures 2(d)-2(f), where no baseline can complete the 3-agent task with 4 leaf nodes. This demonstrates RGMDT's capability to adapt to fewer leaf nodes and more complex environments by efficiently utilizing information from action-value vectors. Figure 4 confirms that RGMDT's performance improves with more leaf nodes, achieving near-optimal levels with \(|L|=4\) or more, consistent with the findings of Thm. 4.4 and supporting its application in multi-agent settings as noted in Remark 4.5.

**Ablation Study:** Table. 2 compares various configurations of the RGMDT model in a multi-agent setting, highlighting task completion and mean rewards. Notably, RGMDT completed tasks with an average reward of \(41.95 3.08\). Configurations without using SVM to derive the linear splitting hyperplane at each split and employing algorithms like CART, ET, RF, and GBDT show varied success: ET and GBDT complete tasks, while CART and RF cannot. Removing the Non-Euclidean Clustering Module or the iteratively-grow-DT specifically designed for multi-agent contexts results in task failures, particularly the latter, which records the lowest mean reward. Results show all changes improved RGMDT, especially the non-Euclidean clustering and iteratively-grow-DT designs.

## 7 Conclusion and Future Works

This paper introduces an iteratively-grow-DT framework for MARL, which views clustering label generation as a non-euclidean clustering problem and quantifies the optimal return gap between the given RL policy and the resulting DT policy with a closed-form upper bound. We propose a novel class of DT algorithm, RGMDT, designed to minimize the return gap using limited leaf nodes. RGMDT significantly outperforms the baselines and achieves nearly optimal returns. Further research will delve into the regression tree that is more suitable for the continuous state/action spaces, and we will also include the re-sampling module to enhance our algorithm.

   Description & Task Completed & Mean Reward \\  Described RGMDT model & **Yes** & \(\) \\ Removed SVM hyperplane, using CART & No & \(26.16 3.74\) \\ Removed SVM hyperplane, using ET & Yes & \(36.79 2.15\) \\ Removed SVM hyperplane, using RF & No & \(21.59 3.81\) \\ Removed SVM hyperplane, using GBDT & Yes & \(37.76 2.42\) \\ Removed Non-Euclidean-Clustering Module & **No** & \(23.85 2.23\) \\ Removed iteratively-grow-DT process & **No** & \(16.29 4.72\) \\   

Table 2: All changes contributed to the improvement of RGMDT, especially the non-Euclidean clustering and iteratively-grow-DT designs.

Figure 4: The reward of RGMDT (starred purple bar) outperforms all baselines and increases with the number of leaf nodes, achieving performance comparable to the expert RL (no hatch style bar).