# VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation

Shiwei Wu\({}^{1,2}\), Joya Chen\({}^{2}\), Kevin Qinghong Lin\({}^{2}\), Qimeng Wang\({}^{3}\), Yan Gao\({}^{3}\)

**Qianli Xu\({}^{4}\), Tong Xu\({}^{1}\)\({}^{1}\)\({}^{2}\), Yao Hu\({}^{3}\), Enhong Chen\({}^{1}\)\({}^{2}\), Mike Zheng Shou\({}^{2}\)\({}^{2}\)\({}^{2}\)**

\({}^{1}\)University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence

\({}^{2}\)Show Lab, National University of Singapore \({}^{3}\)Xiaohongshu Inc.

\({}^{4}\)Institute for Infocomm Research, A*STAR

Corresponding Author.

###### Abstract

A well-known dilemma in large vision-language models (_e.g._, GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (_i.e._, key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens "skipping layers" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for certain transformer layer, we learn to skip the computation for a high proportion (_e.g._, 80%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately ~42% time and ~30% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets. The code and checkpoints will be made available at github.com/showlab/VideoLLM-online.

## 1 Introduction

Recent advancements in large language models , particularly with GPT-4o , have led many to believe that the development of a J.A.R.V.I.S.-like AI assistant is becoming increasingly feasible. Such an assistant would operate in a streaming manner, remain always-on, and be multimodal to facilitate interaction with users.

While existing video-based Large Multi-modal Models (LMMs)  have shown significant capabilities in general visual content understanding and reasoning, these models primarily operate in an offline setting, provide response for a few sampled frames within a video in the event-level, which falls short in online settings where there is a need for prompt, concise, and frame-aligned answers for the continuous video frames, as shown in Figure 1. Forinstance, in response to a query such as "remind me when I should add salt", the online assistant ought to evaluate each incoming frame and give temporal-aligned suggestions, taking into account historical visual and linguistic context, rather than merely summarizing the video at the event level. Consequently, as shown in Figure 1, online assistants face significant computational demands and challenges as they are required to engage in causal modeling for every frame of the long video, and current approaches  only rely exclusively on the CLS token for each frame, limiting the vision capability to spatial understanding, which is inadequate for scenarios that require fine-grained scene understanding.

It is intuitive to enhance spatial understanding by integrating additional pooled spatial tokens per frame. However, expanding vision resolution in the online scenario is challenging. Due to the dense attention mechanism and the deep layer design of existing LLMs, the training cost, including GPU memory and training time, increases quadratically as the number of vision tokens expands (_e.g._, from 0.6k\(\)6k vision tokens for a video consisting of 600 frames), which poses significant challenges to scaling up vision capabilities. Long videos, particularly online streaming videos, exhibit high redundancy in visual content, which suggests that a sparser approach could be used to process visual signals, potentially reducing the need for full attention in vanilla transformer-based LLMs without sacrificing performance.

In this paper, we propose VideoLLM-MoD, an efficient approach to scaling up vision resolution for online video large language models. Inspired by the Mixture-of-Experts (MoE)  and Mixture-of-Depth (MoD)  in LLMs, which utilizes conditional logic to route tokens to one of many expert feed-forward networks (FFNs) or certain intermediate layers, we propose that vision tokens in specific blocks can be either routed to subsequent self-attention and FFN operations or bypassed via residual connections. This approach, which we term _Mixture-of-Depth for vision tokens_, allows for the natural reduction of redundant vision information. Consequently, the model can learn which vision tokens are crucial, thereby optimizing computation accordingly. We surprisingly discovered that sparse operation at the token-level won't harm both vision capability and language modeling and is even better since it preserves original context as well as neglects the redundant vision signals, and it can dramatically reduce the training cost as shown in Figure 2, serving as "free lunch" in vision scaling. For each frame, instead of distributing FLOPs uniformly across all vision tokens in every decoder layer, we utilize a learnable module LayerExpert to allocate compute to critical vision tokens within the frame dynamically. Only a few vision tokens selected by the top-\(k\) route mechanism are processed by the following self-attention and FFN (Feed Forward Network) operations and the remains are skipped through residual connection. As shown in Figure 1, compared to directly dropping

Figure 1: Comparison of efficient processing challenges in offline and online video. Online video introduces distinct challenges due to the need for real-time processing as frames stream continuously.

Figure 2: Training Computation Cost. VideoLLM-MoD exhibits greater efficiency compared to the baseline.

vision tokens or merging [39; 20; 36] them to reduce computation, the _skip within context_ mechanism preserves the completeness of the context, allowing for equivalent vision capability with significantly less computational effort. The proposed approach can also be generalized to traditional offline video settings seamlessly, such as COIN  and EgoExo4D  benchmarks.

We summarize our technical contributions as follows:

* We propose VideoLLM-MoD, an efficient approach to scaling vision resolution for online VideoLLM with reduced computational cost and similar or even better performance.
* We propose LayerExpert to determine which vision tokens should be processed at certain layers, leveraging the model to adaptively allocate computation to critical regions within incoming frames.
* Our experiments on Ego4D, EgoExo4D, and COIN benchmarks demonstrate the effectiveness and generalizability of our VideoLLM-MoD.

## 2 Related Work

**Efficient Modeling in Transformer-based Models.** The notorious squared complexity in vanilla Transformers  is a well-known problem, as it is one of the key bottlenecks in scaling the sequence length. In recent large multimodal models (LMMs) [1; 39; 50; 93; 63], prefix visual tokens are used as a fixed budget for context, significantly contributing to their efficiency. This issue becomes more pronounced in online video scenarios with denser video frames. Previous studies on large language models (LLMs)[79; 24; 10; 18] have explored the use of sparse computation to maintain performance during inference while reducing computational costs. However, these methods still incur significant training expenses. Efforts to reduce training costs through token pruning and merging  techniques are not suitable for online scenarios, as they require offline computation to directly reduce token levels. Mixture-of-Depth  investigates the allocation of computation across model depth for language tokens, balancing performance with speed. Our model, VideoLLM-MoD, extends this approach to online video. We found that reducing vision computation in the context across model depth not only maintains but can even improve performance by removing high redundancy in video.

**Large Multimodal Models for Online Video Understanding.** Inspired by the success of numerous large language models (LLMs) [6; 63; 61; 85; 86], a series of large multimodal models (LMMs) [1; 39; 50; 93; 15] have subsequently been developed to further enhance our comprehension of the world. Current large multimodal models (LMMs) are capable of addressing a variety of standard benchmarks in video understanding, including temporal action localization , and video dialogue and question answering [40; 69; 54; 88; 44], while also demonstrating strong potential in broader multimodal applications [12; 13; 11; 46; 76; 77; 78]. However, while these models analyze entire video frames to make predictions in an "offline" setting, they are not optimized for real-time applications such as augmented reality (AR) glasses and autonomous driving systems. In light of this growing demand, benchmarks for online scenario such as action detection [90; 75] and anticipation [26; 89], which are designed to interpret events at the current timestamp without access to future data, are becoming increasingly critical. VideoLLM-online  serves as the first attempt to build an assistant using LLMs in an online video scenario. However, its spatial capabilities are limited, as it uses only a single CLS token to represent each frame, and expanding the vision scale is computationally expensive. VideoLLM-MoD proposes an efficient approach to scaling vision resolution by reducing vision computation in context, thereby enhancing spatial ability without incurring high computational costs.

**Scaling up Vision Resolution for Large Multi-modal Models.** Scaling up the visual resolution for LMMs is an effective approach to enhancing vision capabilities. By utilizing \(5\) more vision tokens compared to LLaVA-1.5 , LLaVA-NeXT  achieved improved vision understanding. However, scaling vision tokens in online video scenarios presents significant challenges, as the training cost increases quadratically with the expansion of vision tokens, requiring the processing of every incoming frame in long videos. To handle long-context vision tokens in LMMs, CogAgent  integrates high-resolution image features into a low-resolution pathway via cross-attention across decoder layers. LLaMA-VID  utilizes context-attention to represent each frame with two key tokens. Both approaches are only applicable for offline video, as the high latency induced by the additional cross-attention mechanism is unacceptable in online scenarios. In contrast, VideoLLM-MoD receives streaming video-language input continuously and can reduce computation efficiently during every forward pass without additional overhead. This enables temporal-aligned responses, making it suitable for real-time applications.

Method

In this section, we introduce our VideoLLM-MoD framework, an efficient approach to training an online video large language model with a larger vision resolution.

### Model architecture.

We depict the overall model architecture as shown in Figure 6, drawing parallels to LLaVA [50; 19; 49] in its design. The model is composed of three principal components: an image encoder, an MLP projector, and a language model. Each video frame embedding is represented as \((1+h_{p} w_{p}) c\), which denotes the CLS token and the average pooled spatial tokens. The frame embeddings extracted by the image encoder are subsequently processed through the MLP projector to frame tokens. These tokens are interwoven with language tokens, forming the input for an LLM. We incorporate LoRA  in every linear layer of the LLM for efficient tuning. To select the most critical vision tokens, certain layers are also equipped with LayerExpert module, as detailed in Section 3.2.

Following VideoLLM-online , in addition to the language modeling (LM) loss, we also utilize an additional streaming loss to ensure the model remains silent when it is unnecessary to output responses. Both training objectives employ cross-entropy loss as follows:

\[L=_{j=1}^{N}( P_{j}^{\{_{j+ 1}\}}}_{LMLoss}- P_{j}^{\{\}}}_{StreamingLoss}),\] (1)

where \(l_{j}\) and \(s_{j}\) are condition indicators: \(l_{j}\) is 1 if the \(j\)-th token is a language response token, and 0 otherwise; \(s_{j}\) is 1 if both (1) the \(j\)-th token is the _last_ token of a frame2, and (2) \(l_{j+1}=0\). The streaming EOS loss is applied to frames prior to responding. \(P_{j}^{\{_{j+1}\}}\) represents the probability associated with the \((j+1)\)-th text token, as output by the language model head for the \(j\)-th token, while \(P_{j}^{\{\}}\) indicates that probability for the EOS token. The two objectives are balanced using the streaming loss weight \(\).

### Scale up Vision Resolution for Online Video.

**Motivation.** The performance of online assistants improves with increased vision scale (_i.e.,_\((1+h_{p} w_{p})\)). However, enhancing vision resolution in online scenarios is challenging because the number of vision tokens grows with video duration, leading to quadratic computational complexity. As shown in Table 7, online videoLLMs must process every frame of long videos during both training and inference to maintain the integrity of the complete visual and linguistic historical contexts, which places significant demands on GPU memory and computational resources.

We hypothesize that videos exhibit high redundancy in temporal and spatial since consecutive frames often share a large portion of their content, especially if the frames are captured in quick succession. Just as humans continuously "see" their surroundings without always "focusing" on every visual detail, it is intuitive that we should skip some vision tokens in certain layers when processing them with a deep model.

**Selecting vision tokens via LayerExpert in certain block.** We leverage a per-block LayerExpert module to learn the selecting/routing behavior, _i.e.,_ learn which vision tokens require more or less processing than others. The LayerExpert identify the "importance score" (in scalar weights) of each vision token within a frame, and only the top-\(k\) vision tokens are processed by the following operations. Notably, since the vision tokens of different frames in the online streaming scenario are processed in a causal manner, the top-\(k\) selection is performed at the frame level, meaning the top-\(k\) vision tokens are selected within each frame. The language tokens are always processed since they are much less redundant and significantly fewer in number compared to vision tokens.

Specifically, suppose we have a sequence of length \(N\) interleaved with \(n_{t}\) language tokens and \(n_{v}\) vision tokens. For the given layer \(l\), the sequence \(X^{l}=\{(x^{l}_{t_{i}},x^{l}_{v_{i}}) 1 t_{i} n_{t},1  v_{i} n_{v}\}\). Within the \((1+h_{p} w_{p})\) vision tokens of each frame, the LayerExpert determines the importance score \(\) for a given vision token using a linear projection \(^{l}_{t_{i}}=w^{T}_{}x^{l}_{v_{i}}\). Then, vision tokens are selected based on a vision keep ratio \(r\) for following processing, and \(P^{l}_{r}\) is the \((1-r)\)-th percentile among the weights \(\) of frame vision tokens. The block's output for the given vision token is as follows:

\[x^{l+1}_{v_{i}}=_{v_{i}}f_{i}(}^{l})+x^{l}_{v_{i }},&_{v_{i}}>P^{l}_{r}\\ x^{l}_{v_{i}},&_{v_{i}} P^{l}_{r}\] (2)

where the \(^{l}\) represents the interleaved tokens consisting of all language tokens and top-\(k\) vision tokens in layer \(l\), and \(f_{i}\) denotes the subsequent self-attention and the FFN operations.

### Efficiency analysis of VideoLLM-MoD.

We further analyze the computation cost of our approach. Except for the decoder layers, the other modules, including LayerExpert, the MLP projector, and LoRA, are fixed given certain inputs and are significantly smaller than the decoder layers of the language model. Therefore, we ignore their FLOPs computation and only consider the computation of the multi-head attention (MHA) and feed-forward network (FFN) modules in the FLOPs estimation.

Suppose the language model has \(L\) total hidden layers, in which \(d\) and \(m\) denote the hidden size dimension, and the intermediate size of FFN, respectively. The input sequence is interleaved with \(n_{v}\) vision tokens and \(n_{t}\) language tokens. We insert LayerExpert in \(K\) layers with a vision keep ratio \(r\) inside of the entire decoder layers. For each attention head, the theoretical FLOPs of the layer with LayerExpert is:

\[_{}=4(n_{t}+rn_{v})d^{2}+2(n_{t}+rn_{v})^{2}d+2(n _{t}+rn_{v})dm,\] (3)

while \(r=1\) in vanilla transformer layers. Since in the online video setting, vision tokens are significantly more than language tokens, _i.e.,_\(n_{v} n_{t}\), the FLOPs of entire decoder is proportional to the vision keep ratio \(r\) and the number of layers equipped with LayerExpert as follows:

Figure 3: VideoLLM-MoD selects the top-\(k\) vision tokens within each frame in certain layers via LayerExpert. We observe that performance drops dramatically with _Early-exit_ as critical vision tokens miss subsequent processing. By retaining crucial vision tokens in certain layers and reducing redundant tokens that may mislead understanding, VideoLLM-MoD achieves better performance with significantly lower computation costs compared to _Full-computation_ baseline.

\[_{}=_{k=K+1}^{L}_{}(r)+ _{k=K+1}^{L}_{}\] (4)

We further calculate the total FLOPs3 of VideoLLM-MoD and the _Full-computation_ baseline during the training phase in a real-world scenario. As shown in Figure 3(a), the practical FLOPs of VideoLLM-MoD is only 0.6\(\) that of the baseline, and this value can be further reduced if the vision scale of each frame is larger, demonstrating the excellent scalability of our approach.

By skipping redundant vision tokens in certain layers, VideoLLM-MoD not only reduces training computation costs but also improves inference efficiency. As shown in Figure 3(b), reducing the intermediate KVcache in historical states allows us to support 1.7\(\) longer context and achieve a comparative inference speed compared to baseline, facilitating deployment in real-world applications.

## 4 Experiments

### Experimental Settings

**Datasets.** We validate the effectiveness of our proposed VideoLLM-MoD on both online and offline settings, including egocentric video dataset Ego4D  and EgoExo4D , as well as instructional video dataset COIN .

* **Ego4D Narration Stream Benchmark**: Following VideoLLM-online , we utilize the dense Ego4D timestamp-narrations to create a streaming set, aiming to generate timely narrations similar to those produced by Ego4D human annotators .
* **Ego4D long-term action anticipation (LTA) Benchmark**: This benchmark requires predicting the next \(Z=20\) actions (verbs and nouns) for a given video based on the previous 8 steps. Following previous studies , we use the standard Ego4D v2 splits,
* **EgoExo4D Fine-grained Keystep Recognition Benchmark**: This task involves recognizing fine-grained key steps from procedural egocentric videos during the test phase, using models that can leverage multiple time-synchronized views during training.
* **COIN Benchmarks**: Following previous works , we evaluate our model on six common benchmarks of the COIN dataset: step recognition, step forecasting, task summarization, procedure forecasting, and procedure forecasting with a goal.

Figure 4: Efficiency analysis of VideoLLM-MoD in both training and inference phase.

**Evaluation metrics and implementation details**: For online benchmark, following VideoLLM-online , we use the Language Modeling Perplexity (_LM-PPL_) and _LM-Correctness_ to evaluate the language modeling capability at the given timestamp. To evaluate the temporal alignment capability as an online assistant, we use the Time Difference (_TimeDiff_) and _Flucency_ to comprehensively evaluate both the language modeling and temporal effectiveness. We trained all models on 8\(\) NVIDIA A100 GPUs. For each module, we use SigLIP-L/16  as the visual encoder, a 2-layer MLP as the multimodal projector, and Meta-Llama-3-8B-Instruct  for the language model. For the vision embedding of each video frame, we use \((1+3 3)\) tokens (CLS token + averaged pooled spatial tokens), with a frame rate of 2 FPS. We add LoRA  to all linear layers of the language model with a rank of 128 and a scaling factor of 256. Additional details can be found in Appendix A.3. For the trade-off between computation cost and performance, we insert LayerExpert every other layer and set the keep ratio \(r\) to 0.2 as the default setting.

### Online Experiments

We compare our VideoLLM-MoD model with various baselines on the Ego4D narration benchmark, as shown in Table 1. We analyze the baselines in detail as follows.

**VideoLLM-online .** For a fair comparison, we re-implemented the VideoLLM-online  baseline using the same visual encoder and language model as VideoLLM-MoD in all experiments. By embedding each frame using only the CLS token, VideoLLM-online  achieves slightly worse performance on this benchmark due to the relatively simple narration, which does not heavily rely on fine-grained vision. Moreover, we found that larger vision resolution can indeed benefit performance, as shown in Figure 5, and in experiments that demand more detailed visual information as shown in Table 4, 5.

**Full-computation.** Using a vanilla transformer architecture, all vision tokens are processed densely across every layer, which significantly increases the training cost.

**EarlyExit.** Building on studies of language-only LLMs , we adapt this approach to the online video setting. All vision tokens are processed in the shallow layers, then skipped in the deeper layers (equivalent to VideoLLM-MoD with \(r=1\) in the first few layers and \(r=0\) in the remaining layers). Empirically, we found that early exit at Layer 2 offers the best tradeoff between performance and computational cost, also highlighted in previous studies . This approach shows the lowest computation but the worst performance, as it misses most of the vision information.

**LayerSkip.** Introduced in previous LLM studies , we adapted the approach to the online scenario, skipping all vision tokens in every other layer (treated as VideoLLM-MoD interleaving layers with \(r=1\) and \(r=0\)). Compared with VideoLLM-MoD, the performance drops significantly as critical vision tokens miss processing in certain layers.

**Our VideoLLM-MoD** exhibits the best tradeoff in online video scenarios, significantly reducing computational costs when processing excessive frames without sacrificing performance compared to the _Full-computation_ baseline. Moreover, we discovered that our approach performs better than the _Full-computation_ baseline in practical use, as shown in Figure 5. It seems counterintuitive that fine-tuning LLM with MoD performs better than using the vanilla model. The _dynamic layer skipping_ methodology of the former results in less vision computation during the forward process, which is likely to weaken the spatial understanding capability. However, we argue that this increases the learning difficulty, as it forces the MoD gate at each layer to focus on the important vision tokens in current causal contexts. This may reduce the risk of overfitting and learn a more robust model.

    & Frame &  &  &  \\  & Strategy & & \& Speedup & _LM-PPL\(\)_ & _TimeDiff\(\)_ & _Flucency\(\)_ & _LM-Correctness\(\)_ \\  VideoLLM-online  & 1 & 5.75T & 8hrs \& n/a & 2.43 & 2.04 & 45.1\% & 48.1\% \\  Full-computation & & 48.29T & 24hrs \& n/a & 2.40 & 2.05 & 45.3\% & 49.0\% \\ EarlyExit &  & 7.14T & 10hrs \& 2.4\(\) & 2.50 & 2.29 & 41.3\% & 46.2\% \\ LayerSkip & & 26.35T & 13hrs \& 1.8\(\) & 2.52 & 2.24 & 42.0\% & 46.5\% \\ VideoLLM-MoD & & 30.74T & 14hrs \& 1.7\(\) & 2.41 & 2.04 & 45.2\% & 48.9\% \\   

Table 1: Online experiments on the Ego4D narration benchmark. VideoLLM-MoD achieves comparable metrics to the _Full-computation_ baseline with less computation cost.

### Ablation Study

#### Insertion strategy of LayerExpert.

Table 2 presents the ablation study results for the We constructed different settings for inserting LayerExpert in the transformer layers. _All_ and _Interleaved_ refers to insert LayerExpert in every/every other layer. The _Interleaved_ strategy demonstrates a better trade-off between computation cost and performance.

The postfix _-Deep_ denotes that vision token skipping is performed only in the deep layers (_i.e.,_ layers after Layer 2). Previous studies [79; 10] indicate that attention allocation across all tokens in the shallow layers (the first two layers) is much more balanced compared to the deep layers, making these shallow layers more vulnerable to token skipping. Our results with and without _-Deep_ also indicate this phenomenon.

**Selecting the critical vision token.** As shown in Table 3, to validate the necessity and effectiveness of allocating computation to the crucial vision tokens, we created two variants that select vision tokens either randomly or uniformly. The poorer performance on _TimeDiff_ indicates that the online capability is significantly impacted by missing critical vision information. This suggests that determining which vision tokens deserve processing is essential for maintaining performance while reducing redundancy. We also conducted ablations on the number of vision tokens to retain based on the vision keep ratio \(r\). Even with relatively fewer tokens and FLOPs, VideoLLM-MoD achieves satisfactory results, further demonstrating the critical vision selection capability of LayerExpert and highlighting the high redundancy present in the video.

### Offline Experiments

We demonstrate the generalizability of our proposed VideoLLM-MoD on traditional offline video scenarios, including recognition, summarization, and forecasting tasks. As shown in Table 4(a), our method achieves the best performance compared to end-to-end baselines on the Ego4D LTA benchmark, with results only slightly lower than Palm  and AntGPT , which utilize EgoVLP  pretrained features followed by cascading performance enhancement methods.

By expanding the vision resolution, VideoLLM-MoD achieves state-of-the-art performance, significantly surpassing VideoLLM-online , which only uses the CLS token for each frame. This is particularly evident in tasks requiring complex spatial context understanding, such as the EgoExo4D Fine-grained Keystep recognition benchmark , as shown in Table 4(b). Furthermore, our method achieves the best performance on most of the COIN benchmarks , as illustrated in Table 4, even outperforming our full-computation baseline. By adaptively focusing on processing critical vision

    &  &  \\  & How/To100M & Step & Task & Next & Proc. & Proc.+ \\  CipBERT  & ✓ & 30.8 & 65.4 & - & - \\ TimeSformer  & ✗ & 46.5 & 85.3 & 40.1 & 17.0 & 40.1 \\ Papikas  & ✗ & 51.0 & 85.8 & 43.2 & - \\ DistantSup  & ✗ & 54.1 & 90.0 & 39.4 & - & 41.3 \\ VideoTF  & ✗ & 56.5 & 91.0 & 42.4 & 40.2 & 46.4 \\ ProcedureVRL  & ✗ & 56.9 & 90.8 & 46.8 & - \\ VideoTaskGraph  & ✗ & 57.2 & 90.5 & 40.2 & - & - \\ VideoLLM-online  & ✓ & 62.5 & 92.2 & 49.3 & 48.6 & 53.3 \\ Ours (Full-computation) & ✓ & 63.1 & 92.7 & 49.1 & 49.8 & **54.1** \\ Ours & ✓ & ✓ & **63.4** & **92.8** & **49.7** & **49.8** & 53.3 \\   

Table 4: Results on COIN benchmarks (left to right): step recognition, task recognition, next forecasting, procedure forecasting, procedure forecasting with a goal.

    Insertion strategy \\  } &  & FLOPs \\  All & 42.5\% & 47.5\% & 13.18T \\ All-Deep & 44.8\% & 48.8\% & 15.37T \\ Interleaved & 45.2\% & 48.9\% & 30.74T \\ Interleaved-Deep & 45.2\% & 49.0\% & 31.83T \\   

Table 2: Ablations on the insertion strategy of LayerExpert in transformer layers. The _Interleaved_ strategy strikes the best trade-off among the variants.

    Keep \\ Strategy \\  } &  Keep \\ Ratio \(r\) \\  } &  \\  & & _LM-PPL\(\)_ & _TimeDiff\(\)_ & _Fluency\(\)_ & _LM-Correctness\(\)_ & FLOPs \\  Random & \(r=0.2\) & 2.45 & 2.18 & 43.6\% & 48.1\% & 30.74T \\  Uniform & \(r=0.2\) & 2.42 & 2.17 & 43.9\% & 48.6\% & 30.74T \\   & \(r=0.1\) & 2.43 & 2.11 & 44.7\% & 48.1\% & 28.54T \\  & \(r=0.2\) & 2.41 & 2.04 & 45.2\% & 48.9\% & 30.74T \\   & \(r=0.3\) & 2.41 & 2.05 & 44.9\% & 48.7\% & 32.93T \\   

Table 3: Ablations on different vision selection strategies. Choosing which vision tokens to process is crucial for efficient vision computation allocation.

[MISSING_PAGE_FAIL:9]

causal contexts. This strategy may reduce the risk of overfitting, thereby resulting in a more robust model as shown in Figure 5 cases.

## 5 Conclusion, Limitations, and Broader Impacts

In this paper, we introduced VideoLLM-MoD, which scales vision resolution for video large language models in online video through efficient select critical video tokens via LayerExpert. Our model can significantly reduce computational costs and memory usage with similar or even better performance compared with _Full-computation_ baseline. Experiments on Ego4D, EgoExo4D, and COIN benchmarks confirm its efficacy and generalizability, making VideoLLM-MoD a robust solution for online video applications.

**Limitations.** As our primary focus was on developing an online assistant for ego-centric or instructional scenarios, we did not conduct extensive explorations on exo-centric video datasets.

**Broader Impacts.** Beyond the online scenario, we hope our work can provide insights and contribute to general video understanding tasks, particularly those involving long videos.

Figure 5: Cases of VideoLLM-MoD on Ego4D GoalStep  video data.