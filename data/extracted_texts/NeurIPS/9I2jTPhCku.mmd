# An Information-Theoretic Understanding of

Maximum Manifold Capacity Representations

 Berivan Isik

Electrical Engineering

Stanford University

berivan.isik@stanford.edu

&Victor Lecomte1

Computer Science

Stanford University

vlecomte@stanford.edu

&Rylan Schaeffer1

Computer Science

Stanford University

rschaef@stanford.edu

&Mikail Khona

Physics

MIT

mikail@mit.edu

&Yann LeCun

Data Science & FAIR

NYU & Meta AI

NYU & Meta AI

ravid.shwartz.ziv@nyu.edu

&Marky Gromov

Physics & FAIR

UMD & Meta AI

gromovand@meta.com

&Sanmi Koyejo

Computer Science

Stanford University

sanmi@cs.stanford.edu

###### Abstract

Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is interesting for at least two reasons. Firstly, MMCR is an oddity in the zoo of MVSSL methods: it is not (explicitly) contrastive, applies no masking, performs no clustering, leverages no distillation, and does not (explicitly) reduce redundancy. Secondly, while many self-supervised learning (SSL) methods originate in information theory, MMCR distinguishes itself by claiming a different origin: a statistical mechanical characterization of the geometry of linear separability of data manifolds. However, given the rich connections between statistical mechanics and information theory, and given recent work showing how many SSL methods can be understood from an information-theoretic perspective, we conjecture that MMCR can be similarly understood from an information-theoretic perspective. In this paper, we leverage tools from high dimensional probability and information theory to demonstrate that an optimal solution to MMCR's nuclear norm-based objective function is the same optimal solution that maximizes a well-known lower bound on mutual information between views.

Multi-view self-supervised learning (MVSSL; also known as Joint Embedding SSL) is a powerful approach to unsupervised learning. The core idea is to create multiple transformations, or "views", of unsupervised data, then use these transformed data in a supervised-like manner to learn generally useful representations. MVSSL methods are diverse but can be loosely grouped into a number of different families: (1) contrastive, such as CPC , MoCo 1 , SimCLR , MoCo 2 , CMC , RPC  and TiCo ; (2) clustering such as Noise-as-Targets , DeepCluster , Self-Labeling , Local Aggregation , SwAV ; (3) distillation/momentum such as BYOL , DINO , SimSiam , TiCo ; (4) redundancy reduction such as Barlow Twins , VICReg , TiCo . Many MVSSL methods either explicitly originate from information theory  or can be understood from an information-theoretic perspective .

Recently,  proposed a new MVSLL method named Maximum Manifold Capacity Representations (MMCR) that achieves similar (if not superior) performance to leading MVSSL methods. MMCR is interesting for at least two reasons. Firstly, MMCR does not fit neatly into any of the MVSSL families: it is not (explicitly) contrastive, it applies no masking, it performs no clustering, it leverages no distillation, and it does not (explicitly) reduce redundancy. Secondly, unlike many MVSSL methods that originate in information theory, MMCR distances itself, writing that estimating mutual information in high dimensions has proven difficult and that more closely approximating mutual information may not improve representations; MMCR instead claims an origin in the statistical mechanical characterization of the geometry of linear separability of data manifolds.

In this work, we seek to better understand what solutions the MMCR loss function incentivizes and how it relates to other well-known MVSSL methods. Our contributions are specifically as follows:

1. We derive a distribution of embeddings that provably minimizes the MMCR loss with high probability by leveraging tools from high dimensional probability.
2. We connect this distribution to information theory by showing it maximizes a well-known variational lower bound on the mutual information between multiple views' embeddings.

## 1 Background

Multi-View Self-Supervised Learning (MVSSL)Let \(f_{}:\) denote our neural network with parameters \(\). Suppose we have a dataset \(\{_{n}\}_{n=1}^{N}\) and a set of transformations (sometimes called augmentations, or "views") \(\) such as color jittering, Gaussian blur, solarization, etc. For each datum \(_{n}\) in a batch of inputs, we sample \(K\) transformations \(t^{(1)},t^{(2)},...,t^{(K)}\), then transform the datum: \(t^{(1)}(_{n}),...,t^{(K)}(_{n})\). We feed these transformed data into the network and obtain _embeddings_ or _representations_:

\[_{n}^{(k)}\ }{=}\ f_{}(t^{(k)}(_{n})) . \]

In practice, \(\) is commonly \(^{D}\) or the \(D\)-dimensional hypersphere \(^{D-1}}{=}\{^{D}:^{T} {z}=1\}\). Given that this work will later touch on information theory, we need notation to refer to the random variables; we use \(Z_{n}^{(k)}\) to denote the random variable for the embedding whose realization is \(_{n}^{(k)}\), and \(X_{n}^{(k)}\) to denote the random variable for the transformed datum whose realization is \(t^{(k)}(_{n})\).

Maximum Manifold Capacity Representations (MMCR)MMCR  originates from classical results regarding performance of linear binary classifiers [12; 16; 17]. Consider \(N\) points in dimension \(D\), with arbitrarily assigned binary class labels; _what is the probability that a linear binary classifier will be able to successfully classify the points?_ Statistical mechanical calculations reveal that in the _thermodynamic_ limit (\(N,D\); \(N/D(0,)\)), a phase transition occurs at capacity \(_{c}=2\). More precisely, if \(<_{c}\), the linear binary classifier will succeed with probability \(1\); but if \(>_{c}\), the classifier will succeed with probability \(0\). MMCR is based on an extension of this result from points to manifolds . MMCR proceeds in the following manner: MMCR takes the embeddings output by the network and normalizes them to lie on the hypersphere: \(_{n}^{(1)},...,_{n}^{(K)}^{D-1}\). Then, MMCR compute the average embedding per datum:

\[_{n}\ }{=}\ _{k}_{n}^{(k)}. \]

Next, MMCR forms a \(N D\) matrix \(M\) where the \(n\)-th row of \(M\) is \(_{n}\) and defines the loss:

\[_{MMCR}\ \ }{=}\ \ -\|M\|_{*}\ \ }{=}\ \ -_{r=1}^{rank(M)}\ _{r}(M), \]

where \(_{r}(M)\) is the \(r\)-th singular value of \(M\) and \(\|\|_{*}\) is the nuclear norm (trace norm, Schatten 1-norm). Minimizing the MMCR loss means maximizing the nuclear norm of the mean matrix \(M\). The authors of MMCR note that no closed form solution exists for singular values of an arbitrary matrix,but when \(N=2,D=2\), a closed form solution exists that offers intuition: \(\|M\|_{*}\) will be maximized when (i) the norm of each mean is maximized i.e., \(\|_{n}\|_{2}=1\) (recalling that \(0\|_{n}\|<1\) since the representations live on the hypersphere), and (ii) the means \(_{1},_{2}\) are orthogonal to one another. While we commend the authors for working to offer intuition, it is unclear to what extent the \(N=2,D=2\) setting sheds light on MMCR in general, as MMCR was theoretically derived and numerically implemented in the large data and high dimension regime.

## 2 An Information Theoretic Understanding of MMCR

In this section, we prove and intuitively explain two properties of MMCR that shed light on it as well as relate it to other MVSSL methods. We specifically consider MMCR's regime of large dataset size \(N\) and high embedding dimension \(D\). We contribute two results:

1. The MMCR loss \(_{MMCR}\) is minimized by (a) making each mean \(_{n}=_{k}_{n}^{(k)}\) lie on the surface of the hypersphere, and (b) making the distribution of means as close to uniform on the hypersphere as possible.
2. This configuration of means maximizes a well-known variational lower bound on the mutual information between embeddings  that was recently used to study and unify several multi-view SSL (MVSSL) families .

More formally, we begin by adapting two useful definitions from relevant prior works [25; 15]:

**Definition 2.1** (Perfect Reconstruction).: _We say a network \(f_{}\) achieves perfect reconstruction if \(,\,t^{(1)},t^{(2)}\), \(^{(1)}=f_{}(t^{(1)}())=f_{}(t^{(2)}())=^{ (2)}\)._

**Definition 2.2** (Perfect Uniformity).: _Let \(p(Z)\) be the distribution over the network representations induced by the data sampling and transformation sampling distributions. We say a network \(f_{}\) achieves perfect uniformity if the distribution \(p(Z)\) is the uniform distribution on the hypersphere._

We will show that a network that achieves both perfect reconstruction and perfect uniformity obtains the lowest possible MMCR loss by first showing that \(_{MMCR}\) has a lower bound and then showing that such a network achieves this bound.

**Proposition 2.3**.: _Suppose that \( n[N],_{n}^{T}_{n} 1\). Then, \(0||M||_{*}N&N D\\ &N D.\)_

Proof.: Let \(_{1},,_{(N,D)}\) denote the singular values of \(M\), so that \(\|M\|_{*}=_{i=1}^{(N,D)}_{i}\). The lower bound follows by the fact that singular values are nonnegative. For the upper bound, we have

\[_{i=1}^{(N,D)}_{i}^{2}=MM^{T}=_{n= 1}^{N}_{n}^{T}_{n} N.\]

Figure 1: **Numerical simulations confirm that a network achieving perfect reconstruction and perfect uniformity achieves the lowest possible MMCR loss.** Away from the \(N=D\) threshold, uniform random vectors achieve the theoretically derived upper bound on the nuclear norm (i.e. lower bound on \(_{MMCR}\)).

Then, by Cauchy-Schwarz on the sequences \((1,,1)\) and \(_{1},,_{(N,D)}\), we get

\[_{i=1}^{(N,D)}_{i}^{(N,D)}1) (_{i=1}^{(N,D)}_{i}^{2})}= N&N D\\ &N D.\]

**Proposition 2.4**.: _Let \(f_{}\) achieve perfect reconstruction. Then, \(\|_{n}\|_{2}=1\)\( n\)._

Proof.: Because \(f_{}\) achieves perfect reconstruction, \( n, t^{(1)},t^{(2)}\), \(_{n}^{(1)}=_{n}^{(2)}\). Thus \(_{n}=(1/K)_{k}_{n}^{(k)}=(1/K)_{k}_{n}^{(1)}=_{n}^{(1)}\), and since \(\|_{n}^{(1)}\|_{2}=1\), we have \(\|_{n}\|_{2}=1\). 

**Theorem 2.5**.: _Let \(f_{}:^{D}\) be a network that achieves perfect reconstruction and perfect uniformity. Then \(f_{}\) achieves the lower bound of \(_{MMCR}\) with high probability. Specifically:_

\[\|M\|_{*}=N(1-O(N/D))&N D\\ (1-O(D/N))&N D\]

_with high probability in \((N,D)\)._

We defer the proof to Appendix A but offer intuition here. To show the inequality in Proposition 2.3 is roughly tight, we need to show the singular values \(_{i}\) are all roughly equal to each other. When \(N D\), since \(M\) has few rows \(_{n}\), they are almost perfectly orthogonal to each other, so all \(N\) singular values will be \(_{n}\|=1\). When \(N D\), since \(M\) has many rows, for any \(x^{D}\) the sum \(\|Mx\|_{2}^{2}=_{n}(_{n}^{T}x)^{2}\) will be concentrated, so \(M\) scales all vectors roughly equally, and therefore its \(D\) singular values are all roughly equal to each other. We confirm this via numerical simulations (Fig. 1); for code, see Appendix B.

We now turn to addressing why perfect reconstruction and perfect uniformity matter from an information theoretic perspective. The results here for MVSSL are known, e.g., [25; 15], but we repeat them to make the connection with MMCR. For input datum \(X\), consider the mutual information between the learned embeddings of two different views \(Z^{(1)}=t^{(1)}(X)\) and \(Z^{(2)}=t^{(2)}(X)\); the mutual information must be at least as great as the sum of two terms: the ability of one embedding to "reconstruct" the other, plus the entropy of the embeddings:

\[I[Z^{(1)};Z^{(2)}]_{p(Z^{(1)},Z^{(2)})}[  q(Z^{(1)}|Z^{(2)})]}_{}+]}_{}, \]

where \(q(Z^{(1)}|Z^{(2)})\) is a variational distribution because the true distribution \(p(Z^{(1)}|Z^{(2)})\) is unknown.

**Theorem 2.6**.: _Let \(f_{}:^{D}\) be a network, the number of views per datum be constant, and \(\) be the variational family of distributions on the hypersphere. Then \(f_{}\) maximizes the mutual information lower bound Eqn. 4 if and only if \(f_{}\) achieves perfect reconstruction and perfect uniformity._

Proof.: Perfect reconstruction maximizes reconstruction term. Perfect uniformity maximizes entropy since the maximum entropy is achieved with the uniform distribution over the support . 

**Theorem 2.7**.: _Let \(f_{^{*}}\) be a network that achieves perfect reconstruction and perfect uniformity, let the number of views per datum \(K\) be a constant, and let \(\) be the variational family of distributions on the hypersphere. Then \(f_{^{*}}\) is both a minimizer of \(_{MMCR}\) and a maximizer of the variational lower bound of mutual information Eqn. 4._

Proof.: The proof follows by Theorem 2.5 and Theorem 2.6. 

## 3 Discussion

In this work, we leveraged tools from high dimensional probability to prove that in the large data and high dimensional regime, the MMCR loss is minimized with high probability by a network achieving perfect reconstruction and perfect uniformity. These two properties together are known to maximize a well-known variational lower bound on the mutual information between multi-view embeddings.