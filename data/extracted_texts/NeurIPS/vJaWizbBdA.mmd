# ERBench: An Entity-Relationship based

Automatically Verifiable Hallucination Benchmark for Large Language Models

Jio Oh\({}^{}\)1 Soyeon Kim\({}^{}\)1 Junseok Seo\({}^{1}\) Jindong Wang\({}^{2}\) Ruochen Xu\({}^{3}\)

Xing Xie\({}^{2}\) Steven Euijong Whang\({}^{1}\)1

\({}^{1}\)KAIST \({}^{2}\)Microsoft Research Asia \({}^{3}\)Microsoft Azure

Equal contribution {harryoh99, purplehibird}@kaist.ac.kr. \({}^{}\)Corresponding author \(\)swhang@kaist.ac.kr\(\)

###### Abstract

Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that _utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks_ as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have _integrity constraints_ that can be used to better construct complex in-depth questions and verify answers: (1) _functional dependencies_ can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) _foreign key constraints_ can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.

## 1 Introduction

Large Language Models (LLMs) [1; 2] have become prevalent and are increasingly popular in a wide range of applications, including natural language processing, chatbots, content generation, and information retrieval, to name a few. However, a fundamental issue of LLMs is hallucination [3; 4; 5; 6], which refers to the phenomenon that LLMs generate fake, unverified, or non-existent information especially for knowledge-related and safety-critical applications. Hallucination remains one of the most severe issues that should be addressed, and we focus on factual hallucination.

To address factual hallucination, it is necessary to develop benchmarks that are comprehensive, intricate, automatically verifiable, and can be scaled efficiently. One approach is to construct manual benchmarks by human annotators [7; 8; 9; 10], which are expensive and not scalable. Another approach is to automatically construct evaluation samples using knowledge graphs  by converting triples to simple factual questions or use existing QA datasets . Although these benchmarks may scale asthe answers can be verified automatically and updated with more knowledge, their questions are still simplistic or unmodifiable, thus lacking the ability to evaluate on intricate tasks.

We contend that utilizing existing relational databases is a promising approach to construct a benchmark that has both merits. Until now, many LLM benchmarks have been constructed based on knowledge graphs. Although these benchmarks can scale, the main limitation is that the questions tend to be simplistic as they are based on triples. In comparison, relational databases contain structured data where they have schema information and follow the entity-relationship (ER) model, which supports various integrity constraints that make sure the data is well formed. A schema can be designed using traditional ER diagrams or more recent notions like UMLs. By using a database's schema, records, and integrity constraints, it is possible to construct arbitrarily-long multi-hop questions based on multiple relations that also have clear automatically-verifiable answers based on the integrity constraints. Using databases thus opens up opportunities to extensively evaluate LLMs on a vast amount of knowledge in a principled fashion.

In this paper, we propose **ERBench**, an LLM benchmark based on the ER model. ERBench supports complex questions and are automatically verifiable (see Fig. 1). The questions can be automatically constructed using ER diagrams. For example, if a movie's length is determined by its _title_ and _year_, and the ER diagram shows the entity movie with three attributes _title_, _year_, and _length_, then one can ask an LLM _Does the movie titled Star Wars produced in 1977 run for more than 60 minutes?_. We use two popular integrity constraints - functional dependencies (FDs) and foreign key constraints (FKCs) to make the questions verifiable. FDs are used to infer an attribute's value based on other attribute values. In our example, if the FD _title_, _year_\(\)_director_, _length_ holds for movies, then the director and length of _Star Wars (1977)_ are determined (_George Lucas_ and _121 minutes_, respectively). We can construct both binary and multiple-choice questions asking for the inferred values. A foreign key is a set of attributes in one relation that refers to the primary key of another relation, which identifies records, and an FKC ensures that the foreign key values actually exist in the other relation. Using FKCs, ERBench can support questions with increasing complexity by generating multi-hop questions via joining multiple relations that have FKCs and inferring longer FDs that span them.

ERBench is also extensible in terms of data, modality, and prompting. First, ERBench can be easily updated as its underlying database changes and thus support continuous evaluation. Second, ERBench supports multimodal questions where one can replace attribute text values with other data types like images. Third, we can further diversify the questions using recent techniques like chain-of-thought , few-shot prompting , and knowledge augmentation . ERBench can thus evaluate any improved LLM.

We conduct extensive experiments using \(5\) public databases and evaluate several popular LLMs: GPT-3.5 , GPT-4 , Llama-270B-Chat , Gemini-Pro , Claude-3-Sonnet , and Mistral-7B-Instruct . We perform comprehensive analyses in terms of answer and rationale accuracies and hallucination rates using single-hop, multi-hop, and multimodal questions and also perform prompt engineering and fine-tuning. We show how ERBench can effectively evaluate any LLM by

Figure 1: ERBench constructs questions from a relational database using its schema, records, and integrity constraints and automatically verifies the LLM responses.

not only checking for answer correctness, but also effectively verifying their rationales by looking for the critical keywords that should be mentioned.

**Summary of Contributions.** (1) We propose ERBench, the first LLM benchmark to systematically utilize relational databases to construct complex questions where the model reasoning can be automatically verified. (2) We show how any database can be converted to a benchmark using its schema, records, and integrity constraints. (3) We extensively evaluate contemporary LLMs using ERBench and demonstrate how ERBench is effective and scalable, remaining relevant over time.

## 2 Preliminaries

LLM Factual HallucinationWe would like to evaluate LLMs in terms of their hallucination levels. We define hallucination as generated content that is nonsensical or unfaithful to the provided source content . Hallucination may occur because the data sources are flawed in various ways or the data is utilized imperfectly and cannot be recalled properly . In any case, we are interested in whether an LLM not only gives correct answers, but also has the correct thought process and thus consider hallucination on two levels: (1) The LLM gives a wrong answer. For example, if the question asks whether Firenze and Florence are the same city, the answer is _Yes_ (Firenze is the Italian name of Florence), and _No_ is considered as hallucination. (2) The LLM gives a correct answer, but with a wrong rationale. If the answer is _Yes_, but the rationale is that both cities are in the United States, then this is considered as hallucination as well. We note that recent hallucination benchmarks like Head-to-Tail  are good at evaluating (1), but are not designed to evaluate (2). Our key idea is to utilize relational databases to generate questions that can be used to evaluate both (1) and (2).

Relational DatabasesWe utilize relational databases, which are based on the ER model. A relation consists of a schema containing attributes and records containing the attribute values. When designing a schema, a typical approach is to start with an intuitive ER diagram or UML to determine which entities and relationships are needed and how they are connected with each other. For example, the movie ER diagram in Fig. 1 has three entity sets _Movie_, _Star_, and _Director_. In addition, there could be relationships, e.g., _Stars-in_ between _Movie_ and _Star_ and _Directed-by_ between _Movie_ and _Director_. The resulting schema of the database could then be _Movie_(_title, year, director, length_), _StarsIn_(_name_, _age_), and _Director_(_name_, _birth year_).

The relations usually have integrity constraints. A _functional dependency_ (FD) is a relationship between two sets of attributes \(X\) and \(Y\) where the \(X\) values determine the \(Y\) values. For example, Fig. 1 shows the FD _title, year \(\) director, length_ because a movie's title and year can be used to identify the movie, which in turn determines its director and length. Likewise, there is another FD _name \(\) birth year_ where the director name determines his or her birth year. We denote an FD as \(X Y\) and use it to evaluate the LLM's rationale as we explain later. A _foreign key constraint_ (FKC) is a set of attributes in one relation that refers to the primary key attributes in another relation, which is used to identify records. Fig. 1 shows that the _director_ attribute of _Movie_ is a foreign key to the _name_ attribute of _Director_. Using a foreign key, we can also join two relations and construct FDs that span them. In Fig. 1, the FD _title, year \(\) birth year_ is a result of joining the _Movie_ and _Director_ relations and combining the first two _Movie_ and _Director_ FDs. This multi-relation FD construction enables us to construct questions with arbitrary complexity as we explain later.

The integrity constraints thus determine the correctness of records, and our idea is to utilize them to verify the LLM responses as well. A natural question to ask is whether the integrity constraints themselves are always correct. Since the integrity constraints are determined by the database owner, it is the owner's responsibility to determine if they should hold in general.

## 3 ERBench

We explain how ERBench utilizes FDs to construct two types of questions - binary and multiple-choice - and automatically verifies LLM responses. We then explain how complex multi-hop questions are constructed by joining relations with FKCs and expanding the FDs. Finally, ERBench can be extended to diverse data types, modalities, and prompt engineering.

### Binary and Multiple-choice Question Construction using FDs

Given a relation \(R\) and an FD \(X Y\), we can specify the \(R.X\) values and ask a question involving the \(R.Y\) values. For example, using the database in Fig. 1, we can ask the question \(q_{1}\): _For the movie with the title \(\)Harry Potter and the Philosopher's Stone\(\) produced in \(\)2001\(\), is the length larger than \(\)100\(\) minutes?_. Optionally, the question can be generated by an LLM using the ER diagram.

We can construct binary questions where the answers are _Yes_ or _No_. We do allow an LLM to answer _Unsure_ if it is not confident about its answers in order to significantly reduce hallucinations . Hence, similar to Head-to-Tail , we use the prompt _Answer the following question in yes or no, and then explain why. Say unsure if you don't know and then explain why_. Note that we ask for further explanation to also verify the LLM's rationale. For \(q_{1}\) above, the correct answer is _Yes_ as the movie is 152 minutes long.

We can also construct multiple-choice questions where we provide several correct options and one incorrect option, which the LLM needs to figure out. The correct options can be generated using any FD. The incorrect option can be generated by choosing one FD \(X Y\) where we know the correct \(Y\) value and choosing a different \(Y\) value from the relation. Optionally, to check whether the LLM is not just guessing, we can also add the option _None of the above_ and make it the correct answer. If we extend \(q_{1}\) above, a correct option would be _The movie length is 152 minutes_, while an incorrect one can be constructed by replacing the 152 minutes with another length in the relation.

### Automatic Verification of LLM Responses

When verifying an LLM response, ERBench checks if both the answer and rationale are correct. The answer checking is straightforward where we check if the LLM selected the right binary or multiple-choice option. To check the rationale, we look for the inferred values of the FD applied on the current record. For example, let us assume the FD _released year, star, director \(\) title_. If we ask the binary question \(q_{2}\): _Is there a movie, released in (2001), starring \(\)Emma Watson\(\) where \(\)Chris Columbus\(\) is the director?_, we not only look for the answer _Yes_, but also the movie title _Harry Potter_ within the rationale. The checking for multiple-choice questions is similar.

We may run into an entity resolution problem where the LLM mentions an entity that is essentially the same as the inferred one, but is written differently. In the above example, we may be looking for _Harry Potter_, but the LLM mentions _Harry J. Potter_, which contains the middle initial of _Harry Potter_. We perform entity resolution based on heuristics including conventional string matching. Another possible solution is to use an LLM itself for the matching. While ChatGPT has indeed been used to replace human judgement , we also believe this may give an unfair advantage to GPT compared to other LLMs and choose not to use this method.

### Multi-hop Question Construction using FKCs

We can increase the complexity of a question by making it a multi-hop question [23; 24], which can be used to test whether an LLM can think in multiple steps. We use the straightforward extension of joining multiple relations to implement the multiple hops. The relations must have foreign key relationships so that the FDs can span the relations. Given two relations \(R(X,Y)\) and \(S(Y,Z)\) where \(R\)'s key is \(X\), suppose there is a foreign key constraint from \(R.Y\) to \(S.Y\). For any tuple \(e\) representing an entity in \(R\), we can ask a question only using its \(X\) and \(Z\) values to see if the LLM knows the hidden \(Y\) value. For example, by joining the _Movie_ and _Director_ relations in Fig. 1, we can construct the 2-hop question \(q_{3}\): _Was the director who directed the movie \(\)Harry Potter and the Philosopher's Stone\(\) that was released in \(\)2001\(\) born in the \(\)1950s\(\)?_. Here \(e\) is the original _Harry Potter and the Philosopher's Stone_ movie, and the hidden \(Y\) value is _Chris Columbus_. If the LLM knows _Chris Columbus_, then it should be able to confirm that his birth year is _1958_, while giving _Yes_ as an answer. Notice that the verification of multi-hop questions is exactly the same as for single-hop questions. Thus, we can construct arbitrarily-complex, but automatically-verifiable questions.

### Extensions

ERBench is extensible in terms of data, modality, and prompting. ERBench supports continuous evaluation in the sense that if the underlying databases change, ERBench can be updated automatically by simply reflecting the record updates to the questions as well. As LLMs need to be evaluatedwith newer data over time, it is important that the benchmark itself is also updatable. ERBench can also support multimodal data by making the underlying database multimodal. For example, we can replace a text attribute in a relation with an image and then pose the same question to the LLM. Finally, LLMs can use any prompt engineering techniques and still be evaluated with ERBench. We highlight the ones we use in our experiments: (1) Chain-of-thought  is a step-by-step approach of prompting and is more likely to give an LLM better context for answering the question; (2) Few-shot prompting  provides demonstrations to the LLM to give it more context before answering questions; and (3) Knowledge augmentation [15; 16; 17] utilizes a search engine to augment the generated answer with search results. Note that ERBench is orthogonal to whatever prompt engineering is used.

## 4 Experiments

We test the performances of LLMs on questions based on databases that are constructed from public data. We evaluate LLMs based on whether their answers and rationales are both correct.

**LLMs Compared.** We compare GPT-3.5 , GPT-4 , Llama2-70B-Chat , Gemini-Pro , Claude-3-Sonnet , and Mistral-7B-Instruct . For multimodal LLMs, we evaluate GPT-4V  and Gemini-Pro-Vision . We access the LLMs through Hugging Face, Microsoft Azure AI Studio APIs, the Google Gemini API, or Anthropic's API. To exclude randomness in the LLM responses, we set all the temperature parameters to zero.

**Datasets and Functional Dependencies.** We perform experiments on \(5\) datasets representing different domains and call them _Movie_, _Soccer_, _Airport_, _Music_, and _Book_ (see Sec. A.1 for details). We also use separate _Director_, _Club_ and _Olympic_ relations that are joined with the _Movie_ and _Soccer_ relations for multi-hop questioning. All the data we use are available on Kaggle or public Github repositories. Table 1 and Table 8 (in Sec. A.2) show the FDs we use to verify the LLM responses for binary and multiple-choice questions, respectively.

**Performance Measures.** We utilize existing LLM hallucination measures  and newly introduce two measures involving rationale evaluation. All four measures are useful for accurately analyzing LLM hallucinations (see Sec. B.1 for details).

* _Answer Accuracy_ (**A**) : Portion of LLM responses that are correct.
* _Rationale Accuracy_ (**R**): Portion of responses whose rationales contain the FD-inferred values.
* _Answer-Rationale Accuracy_ (**AR**): Portion of responses that are not only correct, but also contain FD-inferred values in their rationales.
* _Hallucination Rate_ (**H**) : Portion of responses that are incorrect, excluding those where LLMs admit uncertainty in their responses (e.g., _Unsure_). Specifically, \(\!=\!1\!-\!\!-\!\), where **M** denotes the percentage of LLM responses that admit they cannot answer the given question (i.e., _missing rate_). A lower **H** value is better.

**Measuring Performance based on Internal Knowledge.** When measuring LLM performances, we also take into account their internal knowledge of the entities within the questions. That is, if an LLM is hallucinating on a question because it simply does not know the entity mentioned (e.g., the entity may not exist in its training data) we may want to skip that question for evaluation. We can assess an LLM's knowledge by directly prompting if it knows an entity (e.g., _Do you know about the movie "Harry Potter"?_; see more details in Sec. B.2). For our datasets, the numbers of known entities per LLM are shown in Sec. B.3. For a fair evaluation, we perform three types of evaluations: (1) each LLM is evaluated only with questions with entities that it has knowledge of; (2) all LLMs are evaluated with questions that all the LLMs have knowledge of; and (3) all LLMs are evaluated

  
**Dataset** & **FD** & **Example Question** \\    & _released year, star, director_ & _Is there a movie, released in (2009), starring \(\)CCH Pounder\(\)_ \\  & \(\) _movie title_ & _where \(\)James Cameron\(\) is the director?_ \\   & _nationality, club, jersey number_ & _Is there a soccer player from \(\)Portugal\(\) who played for_ \\  & \(\) _player name (2019 year only)_ & \(\)_lwentus\(\) with uniform number \(\)7_ in \(\)_lwentus\(\) in 2019?_ \\   

Table 1: FDs and binary questions of two datasets. See Sec. A.2 for examples of other datasets.

with all questions, regardless of their knowledge. Due to space constraints, we only present results of (1), and the (2) and (3) results can be found in Sec. B.4.

### Results for Single-hop Questions

Table 2 shows the LLM performances using single-hop binary and multiple-choice questions on the 5 datasets. We first explain how we construct the questions and then analyze the LLM performances.

**Binary Questions.** We use the basic questions in Table 1 and expect a _Yes_ answer. In addition, we construct negated versions of these questions and expect a _No_ answer. For example, we can negate a basic question for the _Movie_ dataset in Table 1 as _Is it true that there are no movies released in \( 2009\), starring \(\)CCH Pounder\(\) where \(\)James Cameron\(\) is the director?_. The reason we always negate a basic question instead of say changing one of its attribute values into an incorrect one is to still perform the FD-based verification. If the question cannot utilize FDs anymore, we can no longer just look for inferred values, but need to analyze the entire LLM response to verify the rationale. Table 2 shows that GPT-4 tends to have superior performances, especially in terms of **A** and **R**. Gemini-Pro and Claude-3-Sonnet have lower **A** and **R**, but also low **H**. All three LLMs along with GPT-3.5 have worse performances for the negated questions. Llama2 and Mistral, on the other hand, tend to give trivial _No_ answers for most questions, which results in high performances on the negated questions, but low performances on the basic ones.

**Multiple-choice Questions.** For the multiple-choice questions, we generate 2-4 choices using the attributes on the right-hand side of the FDs, with one incorrect choice using the construction in Sec. 3.1. For each question, we generate three versions with rephrased choices (e.g., "born in US" can be rephrased to "birthplace is US" and "place of birth is US") and report the averaged LLM performance to account for prompt sensitivity (see Sec. A.2 for more details). Table 2 shows that most LLMs perform better on multiple-choice questions than on binary questions, with Claude-3-Sonnet showing a particularly notable improvement. GPT-3.5 and Gemini-Pro show similar performances, and GPT-4 often results in the best performances. Llama2 and Mistral perform well for the _Movie_

    & &  &  &  &  &  \\ 
**Model** & **Metric** & BN\({}^{}\) & BN\({}^{}\) & MC & BN\({}^{}\) & BN\({}^{}\) & MC & BN\({}^{}\) & BN\({}^{}\) & MC & BN\({}^{}\) & BN\({}^{}\) & MC & BN\({}^{}\) & BN\({}^{}\) & MC \\    & **A** & **.85** &.06 &.97 &.35 &.00 &.83 &.13 &.00 &.71 &.58 &.20 &.91 & **.77** &.01 & **.55** \\  & **R** &.81 &.11 &.96 &.28 &.02 &.60 &.01 &.00 &.44 &.36 &.18 &.68 &.13 &.01 & **.55** \\  & **A** & **.80** &.05 &.96 &.24 &.00 &.55 &.01 &.00 &.44 &.34 &.14 &.66 &.12 &.00 &.12 \\  & **H** (1) & **.15** &.94 &.03 &.64 &.10 &.17 &.67 &.97 &.29 &.27 &.76 &.09 &.05 &.94 &.45 \\   & **A** &.65 &.51 &.97 &.47 &.19 & **.91** &.68 &.11 & **.96** & **.83** &.63 & **.97** &.41 &.02 &.48 \\  & **R** &.81 &.76 &.97 & **.70** &.49 & **.69** & **.23** & **.16** & **.90** & **.74** & **.56** &.87 & **.21** &.02 &.34 \\  & **AR** &.64 &.50 &.97 & **.41** &.17 & **.69** & **.20** & **.03** & **.90** & **.68** & **.54** &.86 & **.19** &.01 & **.34** \\  & **H** (1) &.35 &.47 &.03 &.38 &.04 &.02 &.32 &.80 & **.04** &.15 &.01 & **.01** &.08 &.01 & **.01** \\   & **A** &.05 & **1.0** &.93 &.02 & **1.0** & n/a &.00 & **1.0** & n/a &.76 & **1.0** &.91 &.02 & **1.0** & n/a \\  & **R** &.53 & **.92** &.97 &.18 & **.62** & n/a &.00 &.02 & n/a &.31 &.29 &.71 &.02 &.05 & n/a \\  & **AR** &.05 & **.92** &.91 &.02 & **.62** & n/a &.00 &.02 & n/a &.29 &.29 &.67 &.00 & **.05** & n/a \\  & **H** (1) &.95 & **.00** &.07 &.98 & **.00** & n/a & 1.0 & **.00** & n/a &.24 & **.00** &.09 &.98 & **.00** & n/a \\   & **A** &.28 &.00 &.92 &.01 &.00 &.89 &.00 &.00 &.76 &.51 &.02 &.89 &.00 &.00 &.47 \\  & **R** &.66 &.27 &.97 &.06 &.05 &.55 &.00 &.00 &.33 &.14 &.05 &.55 &.00 &.00 &.13 \\  & **AR** &.26 &.00 &.92 &.00 &.00 &.51 &.00 &.00 &.33 &.11 &.01 &.54 &.00 &.00 &.11 \\  & **H** (1) &.40 & 1.0 &.08 & **.17** &.29 &.11 & **.00** &.42 &.24 & **.02** &.10 &.11 & **.01** &.01 &.53 \\   & **A** &.30 &.15 & **.99** &.21 &.01 & n/a &.52 &.01 &.91 &.46 &.38 & **.97** &.24 &.00 & n/a \\  & **R** & **.87** &.86 & **.99** &.36 &.02 & n/a &.08 &.07 &.80 &.30 &.23 & **.93** &.18 & **.06** & n/a \\ -Sonnet & **AR** &.29 &.15 & **.99** &.17 &.01 & n/a &.05 &.01 &.76 &.26 &.23 & **.90** &.14 &.00 & n/a \\  & **H** (1) &.70 &.83 & **.01** &.30 & **.00** & n/a & **.00** & **.00** &.09 &.33 &.01 &.03 &.10 &.00 & n/a \\   & **A** &.48 & **1.0** &.72 & **.54** &.99 &.44 & **.71** & **1.0** &.13 &.41 &.96 &.56 &.16 &.98 &.40 \\  & **R** &.54 &.66 &.75 &.10 &.18 &.21 &.00 &.00 &.09 &.03 &.04 &.37 &.01 &.01 &.04 \\   & **AR** &.31 &.66 &.64 &.06 &.18 &.19 &.00 &.00 &.05 &.03 &.04 &.14 &.00 &.01 &.

dataset among the 5 datasets. We also extend the multiple-choice questions with a _None of the above_ option and show that the LLM performances tend to decrease as the proportion of this option increases (see more details in Sec. B.6).

Instead of trying to rank LLMs by performance, we would like to make observations from a benchmark perspective: (1) rationale accuracy (**R**) tends to be worse than answer accuracy (**A**) and (2) the LLM performances vary significantly across question types, even when using the same entities. We thus conclude that there is much room for improvement for LLM rationale and that LLM benchmarking should always involve diverse questions for a comprehensive analysis.

### Rationale Verification Accuracy

We evaluate ERBench's effectiveness in evaluating an LLM's rationale. ERBench's main strategy is to utilize FDs to pinpoint critical keywords that must appear in an LLM's rationale. However, there are inevitable corner cases, where a rationale contains the right keyword, but is incorrect. To see if these cases are frequent, we manually inspect 500 randomly-chosen responses per model for single-hop questions across all datasets and see if ERBench correctly evaluates the rationales. Table 3 shows that ERBench's correctness is higher than 95.5% on average and higher than 92% for any model. We also perform an error analysis of these results and larger-scale experiments by comparing ERBench with GPT-Judge  in Sec. B.5, which show similar results.

### Results for Multi-hop Questions

For the multi-hop questions, we construct 2-hop and 3-hop questions for the _Movie_ and _Soccer_ datasets, respectively, using the construction in Sec. 3.3 (see more details in Sec. B.7). Since we are evaluating multiple steps of the LLM's reasoning, we also extend the **R** and **AR** measures as follows:

* **R**-ext: We compute the portion of rationales that occur in the \(i^{th}\) hop that are correct, for each \(i\) [1, \(\), total # hops]. We then take the average of these portions.
* **AR**-ext: We compute the **AR** value of answers and rationales that occur in the \(i^{th}\) hop, for each \(i\) [1, \(\), total # hops], in order to analyze any "snowball effect"  on how early hop reasoning errors affect the final accuracy.

Table 4 shows the LLM performances on the two datasets. Compared to the single-hop question results, most LLMs naturally have worse answer and rationale accuracies. Even if the answer accuracies are high, the rationale accuracies are low, underscoring the need to evaluate LLM rationales. Using Chain-of-Thought prompting  (denoted as "+ CoT") has mixed results where the demonstrations sometimes guide the LLMs to retrieve better knowledge about entities, but may also add unintended biases. Finally, the **AR**-ext results show that the **AR** performance does not decrease for more hops, which means that answering the early hops correctly is important. In Sec. B.8, we also show the importance of the correctness of initial hops to avoid any snowballing of incorrectness.

### Results for Multimodal Questions

We explore the extensibility of ERBench by incorporating multimodality, specifically images, into GPT-4V and Gemini-Pro-Vision. Using the multimodal question construction in Sec. 3.4, we introduce multimodality in single-hop questions of the two datasets: _Movie_ and _Soccer_. We replace each _title_ attribute value with a movie poster image in _Movie_ dataset and each _club_ attribute value with a soccer club logo in _Soccer_ dataset (see the actual prompts in Sec. C.1). The results are shown in Table 5. Overall, the integration of image modality tends to improve the performance compared to Table 2. The improvements are more pronounced when using Gemini-Pro-Vision. ERBench is thus also effective in evaluating multimodal models.

    &  &  \\ 
**Model** & **Metric** & \(_{(7)}\) & \(_{(80)}\) & MC & \(_{(7)}\) & \(_{(80)}\) & MC \\    & **A** & **.93** & **.95** & **.92** &.45 & **.45** & **.90** \\  & **R** & **.97** & **.97** & **.85** & **.40** & **.38** & **.67** \\  & **AR** & **.91** & **.93** & **.84** & **.38** & **.38** & **.64** \\  & **H** (i) & **.06** & **.05** & **.06** & **.07** & **.00** & **.03** \\   & **A** &.82 &.17 &.59 & **.67** &.01 &.68 \\  & **R** &.95 &.94 &.73 &.38 &.20 &.56 \\   -Pro-V & **AR** &.78 &.16 &.58 &.34 &.01 &.43 \\   & **H** (i) &.18 &.83 &.41 &.31 &.97 &.32 \\   

Table 5: LLM performances using multimodal questions on 2 datasets.

    & **Gemini** & **Claude-3 Mistral** \\   Acc (\%) & 97.4 & 94.4 & 92.8 & 94.8 & 96.8 & 97.0 \\  

Table 3: ERBench’s manual verification accuracy.

### Results on Prompt Engineering Methods

We further diversify our questions using prompt engineering techniques for a more extensive evaluation of LLMs. In Sec. 4.3, we use chain-of-thought  techniques in multi-hop questions to encourage step-by-step reasoning and observe improved LLM performances (Table 4). In addition, we use few-shot prompting  in single-hop questions where we provide 8 (2-4) demonstrations before asking each binary (multiple-choice) question (see detailed demonstration prompts in Sec. C.2). Table 23 in Sec. B.9 shows that the demonstrations indeed improve LLM performances for both types of questions for most domains. Finally, we implement a simple version of knowledge augmentation  using the LangChain API , which adds a background knowledge passage of entities from Wikipedia before each prompt. Surprisingly, we often observe a degradation in LLM performance as the passage may actually mislead the LLM instead of help it; see Table 24 and Sec. B.10 for a more detailed analysis.

### Results for Fine-tuning

We analyze how fine-tuning affects LLM performance using ERBench. We use GPT-3.5 and fine-tune it for 2 epochs on (1) 3,000 entities of the _Soccer_ dataset and (2) the combination of 4 datasets - _Movie_, _Soccer_, _Music_, and _Book_ - to check whether data from different distributions can improve LLM performance. We then observe how its performances on the 5 datasets change. We use similar questions as in Sec. C.2. As a result, Table 6 shows that fine-tuning is mostly helpful across all datasets, but there is still room for improvement. Interestingly, increasing the number of datasets for fine-tuning does not necessarily lead to an increase in performance compared to just fine-tuning on the _Soccer_ dataset, although there is still a boost compared to not fine-tuning. Similar to Sec. 4.3, some prompts for fine-tuning occasionally add unintended biases to the model.

## 5 Related Work

There are many benchmarks that evaluate LLM responses, and we categorize them by what they evaluate and whether they scale. There are also LLM hallucination detection methods that are more focused on improving the LLMs instead of evaluating them, and we summarize them in Sec. C.

    & &  &  \\  & &  &  &  &  \\ 
**Model** & **Metric** & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) & BN(\({}_{}\)) \\    & **A** &.74 &.00 &.36 &.54 &.81 &.82 &.59 &.76 \\  & **R\({}_{}\)** &.92 &.92 &.95 &.79 &.09 &.15 &.67 &.60 \\  & **AR\({}_{}\)** &.73.70 &.00/.00 &.35/.34 &.53/.50 &.01/.05/.03 &.00/.00 &.47/.54/.54 &.28/.31/.31 \\  & **H** (i) &.21 &.96 &.62 &.27 &.17 &.18 &.33 &.21 \\   & **A** &.59 &.40 & **.80** &.66 &.46 &.55 &.79 &.70 \\  & **R\({}_{}\)** & **.98** & **.98** & **.98** & **.98** &.59 &.60 & **.80** & **.80** \\  & **AR\({}_{}\)** &.59/.59 &.40/.39 & **.80**/.79 &.66/.65 &.38/.42/.41 &.18/.19/.19 &.52/.55/.55 & **.68/.73/.73** \\  & **H** (i) &.41 &.60 &.20 &.34 &.25 &.13 &.20 &.29 \\   & **A** &.02 & **1.0** &.79 &.06 & **.88** &.12 &.84 &.33 \\  & **R\({}_{}\)** &.95 &.95 &.97 &.95 &.25 &.30 &.47 &.56 \\  & **AR\({}_{}\)** &.02/.02 & **.98**/.**92 &.78/.77 &.06/.05 &.00/.00/.00 &.44/.55/.44 &.12/.14/.13 &.45/.62/.62 \\  & **H** (i) &.98 & **.00** &.21 &.94 &.12 &.88 &.15 &.65 \\   & **A** &.19 &.01 &.31 &.02 &.00 &.00 &.18 &.41 \\  & **R\({}_{}\)** &.42 &.60 &.40 &.28 &.01 &.07 &.60 &.46 \\  & **AR\({}_{}\)** &.19/.19 &.01/.01 &.31/.30 &.02/.01 &.00/.00/.00 &.00/.00/.00 & **.66/.73/.75** &.16/.15/.21 \\  & **H** (i) &.02 &.33 & **.00** &.20 & **.00** & **.00** &.60 &.15 \\   & **A** &.44 &.57 &.56 &.02 &.57 & **1.0** &.21 &.14 \\  & **R\({}_{}\)** &.95 &.95 &.96 &.95 &.58 &.26 &.39 &.27 \\ -Sonnet & **AR\({}_{}\)** &.44/.42 &.57/.55 &.56/.53 &.02/.02 &.21/.23/.18 &.33/.34/.33 &.58/.60/.59 &.39/.40/.40 \\   & **H** (i) &.50 &.37 &.44 &.97 &.20 &.18 &.22 &.14 \\   

Table 4: LLM performances using the binary basic (BN(\(\))) and binary negated (BN(\({}_{}\))) multi-hop questions w/wo CoT prompting on 2 datasets. We exclude Mistral as it knows too few entities (less than 20), resulting in mostly “n/a” values; see Sec. B.3 for the # of entities known by each LLM. For each question type, we mark the best performance in bold among all models w/wo CoT prompting.

Many benchmarks evaluate factual knowledge of LLMs, which can be (1) general [7; 32; 33; 34; 35; 36; 37; 38; 39; 40; 8; 9; 41] or (2) specialized, where the questions are about medicine and healthcare [42; 43], certain languages [44; 45], financial tasks , or car systems . In comparison, ERBench can be applied to any domain as long as its underlying database reflects the knowledge of that domain.

Another line of benchmarks evaluates specific functionalities or qualities of LLMs. Functionality evaluations assess LLM performance on tasks such as long text generation [48; 49], semantic role identification , knowledge location , report or knowledge generation [52; 53; 54], text summarization [55; 56], attributing answers , fact checking , and multitasking . Quality evaluations, on the other hand, focus on aspects like consistency [60; 61] and reliability [62; 12; 63; 64] of LLM responses. ERBench aligns most closely with fact-checking and reliability, but the key contribution is the automatic evaluation of both model answers and rationales.

Recent scalable LLM benchmarks utilize existing QA datasets [12; 10] or knowledge graphs  to automatically generate questions at scale. For example, one can easily convert subject-predicate-object triples in a knowledge graph into a question that asks for the object. However, these approaches fail to verify the thought process of LLMs as they only check whether the final answers are correct. In comparison, ERBench is the first to utilize relational databases for LLM evaluation and can perform automatic rationale verification and systematic multi-hop question generation by leveraging database integrity constraints. We provide a more detailed comparison with benchmarks based on knowledge graphs in Sec. C.

## 6 Conclusion

We proposed ERBench, which pioneers a new direction of LLM benchmark construction by systematically converting any relational database based on the ER model to an LLM benchmark. ERBench starts from a schema and generates questions using an ER diagram and ensures that the LLM responses can be automatically verified using functional dependencies in a principled fashion. In addition, ERBench uses foreign key constraints to join relations and construct multi-hop questions, which can be arbitrarily complex and used to evaluate the intermediate answers of LLMs. Finally, ERBench is extensible in terms of data, modality, and prompt engineering. We generated our own LLM benchmark in \(5\) domains and performed comprehensive analyses in terms of answer and rationale accuracies and hallucination rates using single, multi-hop, and multimodal questions and also performed prompt engineering and fine-tuning. Overall, ERBench is effective in evaluating any LLM's thought process by pinpointing critical keywords.

Societal Impact & LimitationERBench can perform comprehensive evaluations of LLMs, which can help LLM users make informed choices. ERBench's limitation is that it only checks for critical keywords to verify LLM rationales. Although we demonstrated that this strategy is very effective, an interesting future work is to verify rationales based on their entire contents.

    & &  &  &  &  &  \\ 
**Model** & **Metric** & BN\({}_{(V)}\) & BN\({}_{(S)}\) & BN\({}_{(V)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) & BN\({}_{(N)}\) \\    & A &.85 &.06 &.35 &.00 &.13 &.00 &.58 &.20 &.77 &.01 \\  & **R** &.81 &.11 &.28 &.02 &.01 &.00 &.36 &.18 &.13 &.01 \\  & **AR** &.80 &.05 &.24 &.00 &.01 &.00 &.34 &.14 &.12 &.00 \\  & **H** (\(\)) &.15 &.94 &.64 & 1.0 &.67 &.97 &.27 &.76 & **.05** &.94 \\   & A &.88 & **.89** &.92 &.92 & **.94** &.91 & **.93** & **.93** & **.94** &.68 \\  & R &.93 & **.95** &.75 &.71 & **.09** & **.08** & **.61** & **.55** &.09 &.10 \\  + FT w/ Soccer & AR &.84 & **.86** &.70 &.66 & **.09** & **.08** & **.60** & **.54** &.09 &.09 \\  & **H** (\(\)) &.12 & **.11** &.08 &.08 & **.06** &.09 & **.07** & **.07** &.06 &.32 \\   & A & **.95** &.34 & **.97** & **.96** &.51 & **1.0** &.20 &.91 &.50 & **.95** \\  & **R** & **.98** & **.95** & **.81** & **.78** &.07 &.07 &.53 &.53 & **.17** & **.17** \\ + FT w/ 4 Datasets & AR & **.93** &.34 & **.78** & **.75** &.05 &.07 &.16 &.49 & **.13** & **.17** \\  & **H** (\(\)) & **.05** &.65 & **.03** & **.04** &.49 & **.00** &.80 &.09 &.50 & **.05** \\   

Table 6: GPT-3.5 performances on (1) only the _Soocer_ dataset and (2) the combined dataset of _Movie_, _Soocer_, _Music_, and _Book_. See Sec. B.3 for the number of entities known by GPT-3.5 after fine-tuning. Lower **H** values are better, whereas higher **A**, **R**, and **AR** values are better.