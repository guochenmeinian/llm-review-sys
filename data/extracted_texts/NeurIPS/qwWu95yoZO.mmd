# What to Say and When to Say it:

Live Fitness Coaching as a Testbed for

Situated Interaction

 Sunny Panchal\({}^{1}\)1 **Apratim Bhattacharyya\({}^{1}\)1 **Guillaume Berger\({}^{1}\)**Antoine Mercier\({}^{1}\)

**Cornelius Bohm\({}^{2}\)2 **Florian Dietrichkeit\({}^{}\)**Reza Pourreza\({}^{1}\)**Xuanlin Li\({}^{3}\)3 **Pulkit Madan\({}^{1}\)**

**Mingu Lee\({}^{1}\)**Mark Todorovich\({}^{1}\)**Ingo Bax\({}^{1}\)**Roland Memisevic\({}^{1}\)

\({}^{1}\)Qualcomm AI Research\({}^{}\)2 **Aignostics GmbH**\({}^{3}\)UC San Diego

Authors contributed equally

###### Abstract

Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the Qevd benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching - a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.

## 1 Introduction

Datasets that combine visual information and language have greatly contributed to advancing the abilities of AI models over the past years, ranging from captioning , to visual questions answering , to visual dialogue , and beyond. Particularly impressive showcases of this progress are recent models such as GPT-4o  and Gemini , which can interact with users in real-time.

Despite the impressive recent progress, existing vision-language models still lag far behind human capabilities. While state-of-the-art models can be queried (e.g., through prompting) to comment on events shown in the camera stream, they lack the ability to interact asynchronously with the user as demanded by the situation, rather than only when prompted. Such interactive scenarios that are grounded in the spatial and temporal context of an unfolding situation are commonly referred to as "situated" . Addressing such situated interactive scenarios will be a key to developing real-world assistive vision-language models.

A notable type of situated interaction is the instructional or coaching scenario, where an instructor guides a user through a complex activity, such as live fitness coaching. The real-world domain oflive fitness coaching has several benefits that make it an ideal test-bed for studying situated assistive vision-language models: Firstly, fitness routines have a controlled structure in that users are expected to, but may not, follow a prescribed series of actions. Secondly, despite the structured nature of fitness routines, coaching in this domain remains a complex and unsolved problem for current vision-language systems. The nuances of human motion pose a significant challenge for these systems to effectively understand the dynamic situation and respond interactively. Finally, live fitness coaching represents a rapidly growing real-world application. The increasing popularity of home workouts , highlights the practical need and potential impact of developing effective solutions in this area. A successful vision-language model for live fitness coaching could thus offer users tangible benefits.

Currently available large-scale video datasets [24; 27; 46; 55; 72] provide a rich set of annotations with expert demonstrations in domains such as cooking or house-hold activities. Expert demonstrations alone are not sufficient for real-world instructional scenarios, such as live fitness coaching, where the user may make mistakes. This has been addressed recently for some ego-centric instructional tasks by  and . Successfully guiding a user through a fitness routine additionally necessitates the ability to understand fine-grained human actions, to provide appropriate instructions, and to provide corrective feedback--grounded in those fine-grained human actions--to correct any mistakes made by the user.

Overall, our main contributions are: (1) We propose the first large scale benchmark and dataset, Qualcomm Exercise Videos Dataset (Qevd), aimed at the development of video-language models for live coaching. Qevd+ contains over 474+ hours of videos for fitness activity recognition and coaching. It includes short-clip videos (Qevd-Fit-300K) (\(\)5 seconds in length) annotated with 1M+ question-answer pairs, and long-range videos (>3 minutes in length) annotated with live feedback (Qevd-Fit-Coach, cf., Fig. 1); (2) We perform a comprehensive evaluation of state-of-the-art VLMs on the Qevd-Fit-Coach benchmark, revealing that the task is largely unsolved and offers significant room for improvement; (3) As a step towards closing the gap towards situated interaction, we propose a novel video-language model, Stream-VLM, which, instead of being limited to turn-based interactions, can decide on-the-fly when and what to say to the user. The model is trained end-to-end to perform real-time visual interaction with a user based on live camera input.

Figure 1: Long-range interactive videos from our Qevd-Fit-Coach benchmark. Live feedbacks provided to the participants are shown below each frame. Corrective feedbacks in red.

## 2 Related Work

**Datasets for Activity Recognition.** There is a large body of work on visual activity recognition. This includes NTU RGB+D 120 , FineGym , UCF101 , Kinetics , Moments in Time , ActivityNet , AVA-Kinetics , Charades , Something-Something , Something-Elee , and others. Unlike our Qevd benchmark and dataset, these datasets do not contain detailed multi-modal annotations such as questions and feedbacks, or long videos with multiple actions or events. This restricts their utility in the development of interactive video-language models. Furthermore, while these datasets focus on a wide range of human activities, they contain only a few fitness activity related classes, if any (Tab. 1, col 2).

**Datasets for Procedural Activities.** The Epic-Kitchens  dataset provides ego-centric videos of non-scripted daily kitchen activities, with post-hoc recorded narrations. YouCook2  provides cooking videos annotated with instructions on how to prepare specific meals, largely featuring expert chefs. HowTo100M  provides narrated instructional videos, featuring a variety of activities. These datasets are not interactive (Tab. 1, col 4) as they feature first-person instructions or narrations. Furthermore, they largely feature experts and do not include mistakes likely to be made by novices (Tab. 1, col 5). In contrast, Qevd is interactive, features participants with diverse skill levels, and thereby, includes mistakes likely to be made by novices.

Ego-Exo4D  includes a highly diverse set of activities, performed by participants with a variety of skill levels. Assembly-101  features videos of people with diverse skill levels assembling and disassembling 101 "take-apart" toy vehicles. However, these datasets are not interactive and they do not include corrective feedbacks (Tab. 1, col 6). Qevd is interactive and includes corrective feedbacks from the perceptive of the fitness coach.

Similar to our work, WTAG  and HoloSnsist  include corrective feedbacks and are focused on the development of interactive AI assistants. However, they focus on domains such as cooking or object manipulation. As they are recorded from an ego-centric perspective, they do not contain complex human actions (Tab. 1, col 3). Furthermore, while they include mistakes and associated corrective feedbacks, they do not include diverse examples of possible mistakes per target task (Tab. 1, col 7). Qevd is recorded from the perspective of a virtual fitness coach and includes fine-grained

    &  &  &  &  \\  & & **Actions** & **Interactive** & **Mistakes** & **Feedbacks** & **Expertise** & **Length** \\   \\  NTU RGB+D  & Fitness & ✓ & \(\) & \(\) & \(\) & ✓ & – \\ FineGym  & Fitness & ✓ & \(\) & \(\) & \(\) & ✓ & 708 \\   \\  YouCook2  & Cooking & \(\) & \(\) & \(\) & \(\) & \(\) & 176 \\ Epic-Kitchens  & Cooking & \(\) & \(\) & \(\) & \(\) & \(\) & 100 \\ HowTo100M  & Daily-life & ✓ & \(\) & \(\) & \(\) & \(\) & 134k \\ Ego-4D  & Daily-life & \(\) & \(\) & \(\) & \(\) & \(\) & 3670 \\ Ego-Exo4D  & Daily-life & \(\) & \(\) & ✓ & \(\) & \(\) & 1422 \\ Assembly-101  & Toy assm. & \(\) & \(\) & ✓ & \(\) & \(\) & 513 \\   \\  WTAG  & Cooking & \(\) & ✓ & ✓ & ✓ & \(\) & 10 \\ HoloSnsist  & Obj. manip. & \(\) & ✓ & ✓ & ✓ & \(\) & 166 \\  Qevd (Ours) & Fitness & ✓ & ✓ & ✓ & ✓ & ✓ & 474 \\   

Table 1: Comparison of dataset statistics. _Domain_: target domain of the dataset; _Human Actions_: whether the dataset contains fine grained human actions; _Interactive_: whether the dataset captures interactions between two or more agents, e.g., a fitness coach and a participant; _Mistakes_: whether the dataset contains correct and incorrect actions towards a task; _Corrective Feedback_s: whether the dataset contains corrective feedback provided in response to incorrect actions; _Domain Expertise_: whether the dataset contains the required domain expertise to provide fine-grained feedbacks for mistakes; _Length_: total length in hours.

human actions - diverse exercises and their variations including a wide diversity of mistakes per exercise.

**Datasets for VQA and Reasoning.** ActivityNet Captions , Vaters, TRECVID , HD-VILA , TGIF , WebVid , Charades , STAR  and Agqa among others, focus largely on video captioning and question answering tasks. Finally, there exist a wide range of datasets on visual reasoning , including visual dialogue, for example , all of which, in contrast to our work are based on still images rather than videos. FixMyPose contains annotated instructions for pose correction but is limited to pairwise images and is activity-agnostic.

**Models for Situated Interactions.** There is also a growing body of work on enabling LMs to generally reason over visual input . However, the existing models can answer only high-level questions about depicted scenes and objects. Models for exercise feedback are discussed in  among others. However, such models are based on the recognition of whether or not an exercise was performed, or on counting repetitions, rather than providing interactive guidance and reasoning about the user's movements from a third-person viewpoint, which is the focus of this work.

## 3 Fitness Interactive Coaching Dataset and Benchmark

We now introduce Qevd, including Qevd-Fit-300K, and the Qevd-Fit-Coach dataset and benchmark, in detail. We begin with a detailed description of the Qevd-Fit-Coach benchmark, followed by additional details of the Qevd-Fit-300K and Qevd-Fit-Coach datasets.

### Qevd-Fit-Coach Benchmark

The Qevd-Fit-Coach benchmark contains videos of participants performing a structured work-out while receiving live feedback. These feedbacks may be corrective, affirmative, or informative, depending on user activity, to improve their form and pacing as they follow the workout using the temporal structure described below.

**Feedback Structure.** The feedbacks in the Qevd-Fit-Coach benchmark have the following structure: At the start of each exercise, acknowledging feedback is given once the user has started; otherwise, a reminder to do so is provided. A corrective feedback is provided as soon as a mistake is clearly visible. Similarly, when the user begins to correct their mistake, feedback is provided to acknowledge and guide the user to successfully correct the error. If the user is performing the exercise

    &  &  \\   & **Train** & **Test** & **Train** & **Test\({}^{}\)** \\  Number of Videos & 281,660 & 16,429 & 149 & 74 \\ Unique Participants & 1,800+ & 100 & 21 & 7 \\ Average Duration (s) & 5.6 \(\) 1.1 & 5.6 \(\) 1.2 & 213.4 \(\) 3.1 & 213.7 \(\) 3.3 \\ Exercises per Video & 1 & 1 & 5-6 & 5-6 \\ Total Number of Exercises & 148 & 148 & 23 & 23 \\ Total Classes & 1842 & 1558 & - & - \\   \\  Total High-level Questions & 535,299 & 31,326 & - & - \\ Total Fine-grained Questions & 377,678 & 28,849 & - & - \\   \\  Total Feedbacks & 573,637 & 36,333 & 5,403 & 2,484 \\ Average Feedbacks per Video\({}^{}\) & 2.0 \(\) 10.1 & 2.1 \(\) 10.2 & 5.0 \(\) 1.3 & 5.0 \(\) 1.2 \\ Average Silence Period (s)\({}^{}\) & n/a & n/a & 5.2 \(\) 1.4 & 5.3 \(\) 1.2 \\ Average Feedback Length (words) & 8.9 \(\) 5.1 & 9.2 \(\) 5.1 & 6.3 \(\) 3.8 & 6.6 \(\) 4.0 \\   

Table 2: Qevd summary statistics. \({}^{}\)The test split of the long-range videos forms our Qevd-Fit-Coach benchmark. \({}^{}\) Average is reported per exercise for the long-range videos. \({}^{}\) Only a single feedback is provided at the _end_ of the short clips.

correctly, feedback focuses on repetition counting. When repetition counts are not possible, such as with deltoid stretches, users receive positive, encouraging feedback, regularly with an average silence period of \(5\) seconds between successive feedback. Finally, at the end of each exercise, a feedback focused on the overall performance during that exercise is provided. This temporal structure ensures that feedbacks in the Qevd-Fit-Coach benchmark occur at predictable time-steps, aligned to visually salient moments. The annotated feedbacks of each video were verified by a second annotator.

An example from the Qevd-Fit-Coach benchmark, illustrating a trimmed workout session with two exercises--jumping jacks followed by high knees--is shown in Fig. 1. The segment begins with affirmative feedback to acknowledge the participant starting the exercise. Next, a series of feedbacks to correct user mistakes and affirm their compliance are provided. Initially, the participant exhibits a low range of motion and is asked to correct this. Once the range of motion improves, the participant receives encouraging feedback. However, they then stop moving their arms, incurring another corrective feedback. Note that this feedback considers their previous arm movements. The user then moves on to the high knees exercise after being requested to do so. Here, the user receives corrective feedback to raise their knees to the appropriate height and to improve their pace. The session ends with positive encouraging feedback acknowledging that the user has currently performed the exercise. These examples highlight the highly interactive coaching sessions in our Qevd-Fit-Coach benchmark and showcase the tight coupling between the participant actions and timely feedback.

**Statistics:** In total, the Qevd-Fit-Coach benchmark consists of \(\)\(4.5\) hours of recorded workout sessions. Each session is \(\)\(3.5\) minutes long and consists of \(5\) to \(6\) randomly selected exercises arranged in \(30\) second segments. The overall list of \(23\) exercises is provided in the appendix. It includes a total of \(7\) unique participants with a cumulative recording length of \(\)\(20\) minutes to \(\)\(1.5\) hours.

### Fitness Datasets for Training

Together, the Qevd-Fit-300K and Qevd-Fit-Coach datasets in Qevd are designed to instill domain understanding for fitness coaching and provide effective feedbacks during live coaching sessions. They consist of three annotation types (Fig. 2), which are described in detail below.

**Fine-grained Fitness Activity Labels.** This includes \(460+\) hours of labeled (short) videos (Qevd-Fit-300K) crowd-sourced from over \(1,900\) unique participants in the wild. They cover \(148\) different exercises and their variations including: varied pacing, performing common mistakes, and modified form. Exercise variations were determined top-down through consultation with expert fitness instructors. Participants were then provided detailed instructions and an accompanying reference video to perform the exercises and their pre-determined variations. Approximately \(10\) variations were collected per exercise. We show such variations for the push-ups exercise in Fig. 2 (top). Additionally, there are \(49\) types of general activities such as "grabbing a towel" or "drinking from a bottle". Video lengths are in the \(2\) to \(10\) second range. There are approximately \(3,500\) videos per exercise on average (\(\)\(300\)k clips overall) and a total of \(1,800+\) fine-grained classes capturing the exercise variations and general activities. Each video and corresponding labels were manually reviewed for correctness by at least one unique crowd-worker. These labels support training vision models for fine-grained understanding of human motion associated with fitness exercises.

**Fitness Questions.** In addition to the fine-grained labels, question-answer pairs querying video properties are provided for each fine-grained short video. This data can be used to provide further grounding of the LM's concepts in the observed visual input. The questions can be broadly divided into two types: high-level and fine-grained, as shown in Fig. 2. The high-level questions are directed at the overall exercise type and performance of the participant, e.g., in Fig. 2, high-level questions include "What exercise is the user doing?", "Is the user doing this right?". Fine-grained questions are designed to teach fine-grained details of exercises performed by the participant, e.g., "Was the user's squat shallow and used only one arm?", or "Is the user going as fast as possible?". These questions are generated using the Mixtral-8B-Instruct LLM  and the question generation process follows the scheme detailed in . Overall statistics for this subset, including a breakdown of the provided splits, can be found in Tab. 2.

**Fitness Feedbacks.** We provide a set of annotations from a second-person perspective to support feedback in a live coaching session. It contains both fine-grained short videos mentioned above (Qevd-Fit-300K) and additional long-range videos (Qevd-Fit-Coach). For the fine-grained short videos, an average of \(2\) feedbacks per video are provided (Tab. 2). These feedbacks occur at the end of each video, as shown in Fig. 2 (bottom). In the shown example, the feedbacks focus on improving the form of the participant, specifically, by encouraging them to squat deeper and punch with both arms. We collected an additional \(\)\(9\) hours of fitness coaching sessions following the same methodology as the Qevd-Fit-Coach benchmark. These sessions contain an average of \(5\) feedbacks per exercise, totaling approximately \(35\) feedbacks per workout, including instructions (See Tab. 2).

## 4 Baseline Stream-VLM

Current state-of-the-art vision-language models [15; 34; 36; 69; 66; 43] are largely _turn-based_--they take an image or video as input along with an instruction and produce a textual output. In contrast, our Qevd-Fit-Coach benchmark requires models to provide feedback proactively, i.e., without explicit prompting, based solely on the participants' actions in a _streamed_ setting. To this end, we propose a baseline streaming video language model, Stream-VLM, specialized to the fitness coaching domain. It consists of a 3D-CNN-based vision backbone [1; 45] for understanding fine-grained fitness actions and a LLaMA-2-based language backbone . Special action tokens are

Figure 2: Example annotations available on the short video clips from the Qevd-Fit-300K dataset. Annotations include question/answer pairs from our fitness questions dataset and feedback from a coaching perspective.

introduced to enable feedback delivery without explicit prompting. We begin by describing the vision backbone in detail, followed by the special action tokens, and finally, the training scheme.

**Vision backbone.** Our vision backbone is designed to robustly recognize motion cues crucial for fitness coaching. This is in contrast to current state-of-the-art video-language models , which typically use CLIP-/ViT-based vision encoders to capture scene content rather than motion. Specifically, our architecture is based on a publicly available 3D CNN  capable of recognizing a wide range of behavior patterns, including simple exercises. It consists of a mix of 2D and 3D convolutional layers, ensuring that the model can pick up both motion and content information of individual frames to make predictions--both of which are relevant to provide appropriate feedbacks. Additionally, the convolutional layers are causal, making the model well-suited for the streaming setting of our Qevd-Fit-Coach benchmark. Features from the vision backbone are fused into the LM backbone through cross attention at several layers following the methodology of .

**Action tokens.** The Stream-VLM baseline uses two special action tokens <_next>_ and <_feedback>_ to enable proactive feedbacks. The <_next>_ token allows the model to opt not to say anything and request the next video frame as input from the visual stream. Conversely, when the model does decide to say something, it generates a <_feedback>_ token. Through the introduction of these tokens, the model can be trained end-to-end to switch between stream observation and response generation without the need for external prompting heuristics. Specifically, in the coaching setting this allows the model to observe user activity and learn _when_ to provide feedback based on _what_ the user was observed doing.

In Fig. 3 the Stream-VLM guides a user through a squats exercise. It observes the user for a few repetitions by requesting frames from the visual stream using the <_next>_ token. Then, at the time-step it decides to provide feedback, it outputs the <_feedback>_ token. This is followed by the feedback: "Smooth on the way down...". After the model is finished providing feedback, it requests the next video frame using the <_next>_ token.

**Training scheme.** Our Stream-VLM streaming baseline is trained end-to-end in three stages: (1) The vision backbone (3D CNN) is pre-trained on ImageNet, followed by the Qevd-Fit-300K short-clips video collection described in Sec. 3.2; (2) Next, the model is trained end-to-end on the fitness questions and fitness feedbacks annotations (excluding the long-range videos) from the Qevd-Fit-Coach dataset. The purpose of this stage is to align the vision backbone (3D CNN) and LM with the pre-trained action recognition capability of the vision backbone. Hence, only the adapter (cross-attention layer) weights are updated; (3) Finally, the model is fine-tuned on long-range videos from the Qevd-Fit-Coach fitness feedbacks subset with feedback annotations interleaved

Figure 3: Architecture of the Stream-VLM model. The visual stream is processed by a 3D CNN and the language backbone is a LLaMA-2-7B model; special action tokens (<_next>_ and <_feedback>_ ) are highlighted in orange.

to reflect an interactive streaming setting. We limit the model training to \(30\)-second individual exercise segments and leave it to future work to train on workouts spanning multiple exercises. The LLaMA-2 language backbone is fine-tuned using LoRA (dim = 32) . The 3D CNN and adapter (cross-attention layer) weights are kept frozen. Additional details are provided in the appendix.

## 5 Experiments

In this section we evaluate current state-of-the-art (open source) video-language models and explore their limitations in the interactive streaming setting of our Qevd-Fit-Coach benchmark. We also evaluate our Stream-VLM model and highlight potential avenues to address the key challenges associated with the Qevd-Fit-Coach benchmark.

### Evaluation Metrics.

The following metrics are used to capture both the fluency ("what to say") and temporal accuracy ("when to say it") of generated feedback.

**Fluency.** We use the METEOR , ROUGE-L  and BERT  metrics to evaluate fluency. The METEOR, and ROUGE-L metrics assess lexical similarity between the ground truth and predicted feedbacks: e.g., in the case of a corrective feedback where the person is not moving their arms, these metrics would prefer predicted feedbacks referring to the "arm" and "not moving". The BERT score on the other hand matches feedbacks at a semantic level.

To compute these metrics, we first temporally match predicted and ground truth feedbacks. Each ground truth response is matched to the closest predicted response within a \(3\) second window, maintaining their temporal order. The respective METEOR, ROUGE-L and BERT scores are then computed on only the matched feedbacks.

**Automated evaluation (LLM-Accuracy).** In addition to the metrics above, we employ an LLM for holistic feedback evaluation [40; 43]. In contrast to the metrics above, LLMs offer the advantage that they better correlate with human preferences . We provide the LM with ground truth and predicted feedbacks. The LM then scores the predicted feedbacks holistically for accuracy. We use the state-of-the-art open-source LLaMA-3-70B-Instruct  LLM, with scores in the range \(1\) to \(5\). This LLM-Accuracy metric along with METEOR, ROUGE-L and BERT metrics ensures that the

   Method & METEOR\(\) & ROUGE-L\(\) & BERT\(\) & LLM-Acc.\(\) & T-F-Score\(\) \\  Socratic-LLaMA-2-7B & 0.094 & 0.071 & 0.860 & 2.17 & 0.50\({}^{}\) \\ Video-ChatGPT \({}^{*}\) & 0.108 & 0.093 & **0.863** & 2.33 & 0.50\({}^{}\) \\ LLaMA-VID \({}^{*}\) & 0.106 & 0.090 & 0.860 & 2.30 & 0.50\({}^{}\) \\  Stream-VLM & **0.127** & **0.112** & **0.863** & **2.45** & **0.56** \\  Stream-VLM (w/o 3D CNN) & 0.090 & 0.083 & 0.857 & 2.11 & 0.51 \\ Stream-VLM (w/o Pre-training) & 0.095 & 0.087 & 0.858 & 2.08 & 0.52 \\ Stream-VLM (w/o Action-Tokens) & 0.125 & 0.110 & 0.861 & 2.41 & 0.50\({}^{}\) \\   

Table 4: Evaluation of models fine-tuned on Qevd on the Qevd-Fit-Coach benchmark. (\({}^{}\)indicates results of non-interactive models evaluated at regular intervals; \({}^{*}\)indicates models fine-tuned by ourselves.)

   Method & METEOR\(\) & ROUGE-L\(\) & BERT\(\) & LLM-Acc.\(\) \\  InstructBLIP  & 0.047 & 0.040 & 0.839 & 1.56 \\ Video-LLaVA  & 0.057 & 0.025 & 0.847 & 2.16 \\ Video-ChatGPT  & 0.098 & 0.078 & 0.850 & 1.91 \\ Video-LLaMA  & 0.101 & 0.077 & 0.859 & 1.29 \\ LLaMA-VID  & 0.100 & **0.079** & **0.859** & 2.20 \\ LLaVA-NeXT  & **0.104** & 0.078 & 0.858 & **2.27** \\   

Table 3: Zero-shot evaluation on the Qevd-Fit-Coach benchmark.

predicted feedbacks match the ground-truth both at the semantic level while containing references to specific important terms.

**Temporal F-Score.** To measure temporal accuracy we assess whether predicted responses occur at the correct time-step and compute a temporal F-score. Predicted responses are classified as true or false positives based on whether they temporally match ground truth responses as described for the fluency metrics. Predicted responses without a ground truth match are false positives and ground truth responses without a matching predicted response are false negatives. This allows us to calculate temporal precision, recall, and hence, the temporal F-Score.

### Evaluation on the Qevd-Fit-Coach Benchmark

We begin by evaluating state-of-the-art (open source) video-language models, including Instruct-BLIP , Video-LLaVA , Video-ChatGPT , Video-LLaMA , LLaMA-VID , and LLaVA-NeXT, on the Qevd-Fit-Coach benchmark (see Tab. 3). Since these models are "turn-based"--they cannot respond interactively to an input video--we prompt them to provide feedbacks at regular intervals. Specifically, to remain faithful to the streaming setting in our Qevd-Fit-Coach benchmark, we always prompt these models (zero-shot) with the entire video, including a history of generated feedbacks, up to the latest time-step. We use an interval of \(5\) seconds, equivalent to the average silence period within an exercise segment in the Qevd-Fit-Coach dataset. While LLaMA-VID  and LLaVA-NeXT  perform best among zero-shot baselines, overall performance across all zero-shot models is weak as shown in Tab. 3. We present qualitative examples in Fig. 4, highlighting the repetitive and uninformative nature of the feedback provided by LLaMA-VID and LLaVA-NeXT. They are unable to provide corrective feedback at the right time largely due to their lack of fitness domain knowledge and their "turn-based" nature.

Next, we address the lack of fitness domain knowledge by fine-tuning Video-ChatGPT and LLaMA-VID on Qevd following the process discussed in Sec. 4. As shown in Tab. 4, fine-tuning significantly improves performance as expected. However, the performance gain is still limited by the CLIP-/ViT-based visual encoders, which are not well-suited for representing fine-grained human motion, not to mention the limitations incurred from their turn-based nature.

To deal with these issues, our Stream-VLM baseline uses a 3D CNN trained to recognize fine-grained fitness activities and special action tokens to enable interactive feedback. We also consider the following ablations of the model: (1) instead of the 3D CNN, we use a CLIP-based encoder sim

Figure 4: Predicted feedbacks on the Fit-Coach benchmark. The “turn-based” LLaMA-VID and LLaVA-NeXT models are unable to provide corrective feedback and instead generate overly generic and repetitive feedback. The Stream-VLM model has learned to provide relevant feedback at the appropriate time.

ilar to Video-ChatGPT  ("w/o 3D CNN"); (2) we skip pre-training the Stream-VLM with the Qevd-Fit-300K short-clips fitness questions and feedbacks dataset ("w/o Pre-training"); (3) we use a non-interactive turn-based version without the _<next>_ and _<feedback>_ action tokens ("w/o Action Tokens"). We also consider a text-only Socratic model, Socratic-LLaMA-2-7B . In this model, we prompt the language-only LLaMA-2-7B LLM to generate feedback for the previous \(5\) seconds of user activity, provided as a list of activity descriptions in \(1\)-second intervals. Similar to the zero-shot video-language model evaluations, the full history of described activity and generated feedbacks is included in the prompt. The textual description of user activity is based on the activations of the aforementioned fine-tuned 3D CNN. An example prompt is provided in the appendix.

The results in Tab. 4 demonstrate that the Stream-VLM model surpasses the performance of the other models. Crucially, we see a significant improvement in the temporal F-score (0.59 vs 0.50) in comparison to the turn-based models. This is also illustrated in Fig. 4, where the Stream-VLM model is shown to provide relevant corrective feedback at the appropriate time as opposed to the turn-based baselines. The improved quality of the feedbacks is also reflected in the METEOR, ROUGE-L and LLM-Accuracy metrics. The drop in both fluency and temporal accuracy resulting from the pre-training ablation (w/o Pre-training) supports the quality and utility of the fitness feedbacks and questions within our Qevd-Fit-300K dataset. Furthermore, the advantage of the 3D CNN is demonstrated by two observations: the weak performance of Stream-VLM when the 3D CNN is ablated (w/o 3D CNN), and the strong performance of the Socratic-LLaMA-2-7B baseline. In the latter, an off-the-shelf LLaMA-2-7B model outperforms state-of-the-art vision-language models using only the activations of the 3D CNN as a prompt.

Overall, while there is significant room for improvement, these results suggest that end-to-end training is a viable path towards good performance on our Qevd-Fit-Coach benchmark and more broadly, on the task of responding interactively to events within a visual stream.

## 6 Conclusion

We propose Qevd, a novel interactive visual coaching benchmark and dataset, as a test-bed for real-time, real-world situated interaction, and demonstrate that this task is challenging for existing LLM-based architectures. As a first step towards closing the gap to situated interaction, we present Stream-VLM, a streaming vision-language model baseline that learns not only what to say, but also when to say it, based on user activity in the incoming video stream. Overall, we consider our work a starting point for research into end-to-end training of domain-specific interactive vision models, and hope that our data and baselines will encourage further work in this area.

**Privacy and ethics.** The data was collected under a direct agreement with the crowd workers, permitting research and commercial use. Furthermore, a detector was used on all videos to detect any issues, such as individuals in the background, followed by manual inspection of the videos that scored above a threshold, to remove such videos from the collection. Personal identifiable information from the videos was removed to the extent possible, e.g., audio and meta-data. Participants received appropriate and fair compensation for the regions where they were located.

**Limitations.** Our work shows that contextual, situated interactions are possible to a degree for an AI model, when a significant amount of aligned training data is made available, and the interaction is confined to a highly restricted (albeit real-world) task domain. The ability to interact in broader domains, with less domain-specific training data, and with higher accuracy are open research problems. A related open problem is supplementing visual real-time input with speech input. A further limitation is that the predictions of models trained on the data cannot be guaranteed to be free from any bias with respect to, for example, a subject's age or gender.

**Broader Impact.** In addition to potential bias mentioned in the previous paragraph, language models can produce harmful and biased content, make incorrect claims and produce wrongful advice. This needs to be taken into account when interacting with, deploying or building on these models, particularly in sensitive domains like fitness coaching where incorrect advice may lead to physical harm. Although the grounding in visual input supports the generation of language that is contextual, it is not a remedy against these deficiencies of language models. It is also important to consider that any computer vision model processing visual information about human subjects could, in principle, extract information beyond what is required for the use-case, such as biometric information.