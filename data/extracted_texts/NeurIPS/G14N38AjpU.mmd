# Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing

Shangshang Yang\({}^{1,3}\) Xiaoshan Yu\({}^{1,3}\) Ye Tian\({}^{1,3}\) Xueming Yan\({}^{2}\) Haiping Ma\({}^{1,3,4}\)

Corresponding authors.

Xingyi Zhang\({}^{1,3}\)

\({}^{1}\)Anhui University \({}^{2}\)Guangdong University of Foreign Studies

\({}^{3}\)Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education

\({}^{4}\)Department of Information Materials and Intelligent Sensing Laboratory of Anhui Province

{yangshang0308, yxs1eo, field910921}@gmail.com xueming126@126.com hpma@ahu.edu.cn xyzhanghust@gmail.com

###### Abstract

Transformer has achieved excellent performance in the knowledge tracing (KT) task, but they are criticized for the manually selected input features for fusion and the defect of single global context modelling to directly capture students' forgetting behavior in KT, when the related records are distant from the current record in terms of time. To address the issues, this paper first considers adding convolution operations to the Transformer to enhance its local context modelling ability used for students' forgetting behavior, then proposes an evolutionary neural architecture search approach to automate the input feature selection and automatically determine where to apply which operation for achieving the balancing of the local/global context modelling. In the search space design, the original global path containing the attention module in Transformer is replaced with the sum of a global path and a local path that could contain different convolutions, and the selection of input features is also considered. To search the best architecture, we employ an effective evolutionary algorithm to explore the search space and also suggest a search space reduction strategy to accelerate the convergence of the algorithm. Experimental results on the two largest and most challenging education datasets demonstrate the effectiveness of the architecture found by the proposed approach.

## 1 Introduction

With the rapid development of online education systems, such as Coursera and ASSISTment , there have been a massive amount of available data about student-system interactions recorded in these systems. Based on the students' historical learning activities over time, knowledge tracing (KT), as a fundamental task in intelligent education, aims to trace students' knowledge states on knowledge concepts, which plays an important role in both personalized learning and adaptive learning  and thus has received growing attention from the scientific community.

Since the concept of KT was first proposed by Corbett and Anderson in Bayesian knowledge tracing (BKT) , considerable efforts have been devoted to developing various KT approaches, which can mainly be split into two categories. The first type of KT approaches are traditional approaches based on probabilistic models or logistic models, such as dynamic BKT , LFA , PFA , and KTM . The second type of approaches  have shown significantly better performance and generalization since they utilize various deep neural networks (DNNs) to solve the KT task, such asrecurrent neural networks (RNNs) as adopted in , long short-term memory networks (LSTMs) as adopted in , graph neural networks as adopted in , and Transformer as adopted in [7; 33; 36].

Among these DNN-based approaches, Transformer-based KT approaches have exhibited significantly better performance due to the better ability to tackle sequence tasks. However, as demonstrated in [16; 36], when there is no extra specific information introduced, the attention mechanism in the vanilla Transformer  cannot capture students' forgetting behavior , since its providing single global context modelling usually assigns high importance to the highly related records to predict the current record even with position information. However, those related records distant in time commonly pose little influence on the current prediction. Moreover, the input features in these approaches are manually selected based on domain knowledge and simply fused as the inputs to the Transformer encoder and decoder, but the manually selected features and the simple feature fusion method may miss some valuable features and complex feature interaction information.

To address the issues for further improving the performance, we first equip the Transformer with convolutions to boost its local context modelling ability used for students' forgetting behavior, then propose an evolutionary neural architecture search (NAS)  approach for KT (termed ENAS-KT), which aims to automatically select input features and determine where and which operation is applied to strike the balancing of local/global context modelling. Our main contributions are as:

* To the best of our knowledge, we are the first to apply NAS to search the Transformer architecture for the KT task. Here, the proposed ENAS-KT aims to find the best architecture that holds the optimal selected input features and the ideal balancing of the local/global context modelling, which can model students' forgetting behavior well and thus hold significantly good prediction performance on KT.
* In the proposed ENAS-KT, we design a novel search space for the Transformer and devise an effective evolutionary algorithm (EA) as the search approach to explore the search space. The search space not only considers the input feature selection of the Transformer but also equips it with local context modelling by replacing the original global path containing the attention module with the sum of a global path and a local path that could contain different convolutions. Besides, an effective hierarchical fusion method is suggested to fuse selected features. The proposed ENAS-KT not only employs a trained super-net for fast solution evaluation but also suggests a search space reduction strategy to accelerate the convergence.
* Experimental results validated on two largest and most challenging datasets show that the architecture found by the proposed approach holds significantly better performance than state-of-the-art (SOTA) KT approaches.

## 2 Related Work

### Transformer-based Knowledge Tracing

Existing Transformer-based KT approaches focus on taking different features as the inputs to the Transformer encoder and decoder based on human expertise without changing the model architecture. In , Choi _et al._ proposed to directly employ the vanilla Transformer for KT, namely SAINT, where the exercise embedding and knowledge concept embedding are taken as the input of the Transformer encoder part and the response embedding is taken as the input to the Transformer decoder part. As a result, the SAINT achieves significantly better performance than other KT approaches. To model the forgetting behavior in students' learning, Pu _et al._ further fed the timestamp embedding of each response record to the Transformer model. Besides, to model the structure of interactions, the exercise to be answered at the next time is also fed to the model. As the successor of the SAINT, due to the problem in SAINT that students' forgetting behavior cannot be modelled well by the global context provided by the attention mechanism, the SAINT+  aims to further improve the performance of KT by mitigating the problem. To this end, two temporal feature embedding, the elapsed time and the lag time, are combined with response embedding as input to the decoder part.

Different from above two approaches that only focus on how to manually select input features to alleviate the forgetting behavior problem, this paper focuses on not only realizing the input feature automatic selection of the Transformer but also the Transformer architecture automatic design by proposing an EA-based NAS approach, where convolution operations are considered in the Transformer to enhance its local context ability used for students' forgetting behavior.

### Neural Architecture Search for Transformer

NAS has been applied to many domains , such as convolution neural networks in computer vision , RNNs in natural language processing  or automatic speech recognition .

So _et al._ are the first to apply NAS to design the Transformer architecture, where an EA is employed to explore the devised NASNet-like search space consisting of two stackable cells for translation tasks. To save computation resources, a progressive dynamic hurdles method is employed to early stop the training of poor performance architectures. Inspired by this work, there have been a series of NAS work proposed to search the Transformer architecture in different research fields. For NLP, the representative approaches include NAS-BERT  and HAT . For ASR, the representative approaches include the LightSpeech , and DARTS-CONFORMER . For CV, the representative approaches include the AutoFormer , GLiT , and ViTAS . The Transformer architectures found by these approaches have demonstrated significantly better performance than the SOTA competitors in their own domains.

Despite the emergence of these NAS approaches for the Transformer, their designed search spaces for different domains cannot be directly used for solving the feature selection problem and the problem of balancing local context and global context in this paper. To the best of our knowledge, we are the first to apply the NAS to search the Transformer architecture for the KT task. Different from NAS-Cell  that directly applies existing NAS methods used for general LSTMs to KT, our work designs effective search space for the Transformer to strengthen its modeling capability in KT.

## 3 Preliminaries

### Knowledge Tracing Problem

Given a student's response records on a set of exercises over time, i.e., a sequence of interactions denoted by \(I=\{I_{1},I_{2}, I_{n}\}\), the KT task  aims to predict the probability that student answers correctly on the exercise in the next interaction \(I_{n+1}\). \(I_{i}=(e_{i},o_{i},dt_{i},r_{i})\) is the \(i\)-th interaction of the student, \(e_{i}\) denotes the the \(i\)-th exercise assigned to the student, and \(o_{i}\) contains other related information about the interaction \(I_{i}\), such as the timestamp, the type of exercise \(e_{i}\), knowledge concepts in \(e_{i}\), and so on. \(dt_{i}\) represents the elapsed time the student took to answer, \(r_{i}\{0,1\}\) is the student's response/answer on exercise \(e_{i}\), where \(r_{i}\)=1 means the answer is correct otherwise the answer is incorrect.

Based on the interactions before the \(t\)+1-th interaction \(I_{t+1}\), the KT focuses on predicting the probability of the student answering correctly on exercise \(e_{t+1}\), formally:

\[P(r_{t+1}|I_{1},I_{2},,I_{t},\{e_{t+1},o_{t+1}\}), \]

where the response \(r_{t+1}\) and the elapsed time \(dt_{t+1}\) cannot be accessed for the prediction of \(I_{t+1}\).

### Transformer-based KT

Transformer  consists of an encoder part and a decoder part, where each part is composed of a stack of \(N\) identical blocks, and each block has a multi-head self-attention (MHSA) module and a position-wise feed-forward network (FFN) module. Then, each encoder block can be expressed as

\[block_{En}:h=((h)+h),\ h=((X,X,X )+X), \]

where \(X\) is the input of the encoder block, and \(()\) is layer normalization . Each decoder block can be denoted by

\[block_{De}h=((X,X,X)+X)\\ h=((h)+h),\ h=((h,O_{En},O_{En}) +h), \]

where \(()\) is the \(()\) with the mask operation to prevent current position to attend the later positions, and \(O_{En}\) is the last encoder block's output.

For the Transformer-based KT approaches, the following sequential inputs \(X_{input}\) with length of \(L\):

\[ X_{input}=\{E,O,DT,R\}=\{E=\{e_{1},e_{2},,e_{ L}\},\\ O=\{o_{1},o_{2},,o_{L}\},DT=\{dt_{start},dt_{1},,dt_{L-1}\}, \ R=\{r_{start},r_{1},,r_{L-1}\}\} \]will be first mapped into a set of embedding \(X_{embed}=\{embed_{i} R^{L D}|1 i Num\}\), where \(D\) is the embedding dimension, \(Num\) is the number of available features in a dataset, \(dt_{start}\) and \(r_{start}\) are the start tokens. \(O\) could contain multiple features and thus produce multiple embedding. Then, two set of embedding \(X_{En}\) and \(X_{De}\) will be selected from \(X_{embed}\) and fed to the encoder part and decoder part, respectively. Finally, the decoder will output the predicted response \(=\{_{1},_{2},,_{L}\}\) as

\[=Decoder(Fusion(X_{De}),O_{En},O_{En}),\,O_{En}=Encoder(Fusion(X_{En})), \]

where \(Fusion()\) is a fusion method to get a \(L D\) tensor. \(Encoder()\) and \(Decoder()\) denote the forward passes of the encoder part and decoder part, whose each block's forward pass is as shown in (2) or (3). Due to the nature of KT task, a mask operation is necessary for each MHSA module to prevent current position from accessing the information in later positions. Thus the MHSA module mentioned in subsequent sections refers to \(()\).

Existing Transformer-based KT approaches empirically fuse the exercise-related embedding and response-related embedding as the inputs to encoder and decoder parts, respectively. However, the adopted feature fusion method is simple and the selected feature embedding sets may be not the best choice. Therefore, we further employ NAS to realize the input feature selections of both encoder and decoder parts and suggest an effective hierarchical feature fusion.

## 4 The Proposed ENAS-KT

To achieve the automatic input feature selection and automate the balancing of local/global modelling, we design a novel search space for the Transformer, and propose an evolutionary algorithm to search the optimal architecture. The following two subsections will present the details.

### Search Space Design

We first replace post-LN  with the pre-LN  to stabilize the training of the Transformer. Then, we design a novel search space for the Transformer, which has been presented in Fig. 1 and there exist three important designs: (1) A selective hierarchical input module; (2) The sum of a global path and a local path to replace the original global path; (3) The global operations containing MHSA module, FFN module, and zero operation.

#### 4.1.1 Selective Hierarchical Input Module

Given all available input features \(X_{input}=\{E,O,DT,R\}\) in a student's response records, we can obtain each candidate embedding in \(X_{embed}=\{embed_{i} R^{L D}|1 i Num\}\) by categorical embedding or continuous embedding. For example, the categorical embedding of \(E R^{1 L}\) can be computed by

\[embed_{cate}=onehot(E)*_{cate},_{cate} R^{num_{ E} D}, \]

Figure 1: The proposed search space. There are five candidate operations for the local operation and three operations for the global operation.

where \(num_{E}\) is the number of all exercises, \(one\)-\(hot()\) outputs a \(L num_{E}\) one-hot-vector matrix, and \(_{cate}\) denotes the trainable parameters of the embedding layer. And the continuous embedding of \(DT R^{1 L}\) can be computed by

\[embed_{cont}=DT^{T}*_{cont},_{cont} R^{1 D}. \]

Based on all candidate embedding \(X_{embed}\), a selective hierarchical input module is proposed to first select two sets of suitable embedding from \(X_{embed}\) and then fuse the selected embedding as the inputs to encoder and decoder, respectively. To this end, we employ two binary vectors to determine which candidate embedding is selected:

\[}=(b_{i}\{0,1\})^{1 Num},1 sum(}),\; }=(b_{i}\{0,1\})^{1 Num},1 sum(}), \]

where \(1 i Num\) and \(1 sum(})\) ensures at least one embedding is selected. \(b_{i}=1\) means embedding \(embed_{i}\) in \(X_{embed}\) is selected and \(b_{i}=0\) otherwise. By doing so, two sets of selected embedding \(X_{embed}^{En}\) and \(X_{embed}^{De}\) used for the encoder and decoder parts can be obtained by

\[X_{En}=\{embed_{i} X_{embed}|b_{i}=1,b_{i}}\},\;X_{De}= \{embed_{i} X_{embed}|b_{i}=1,b_{i}}\}, \]

Instead of directly applying the simple sum or concatenation operation to \(X_{En}\), we propose a hierarchical fusion method to fully aggregate the selected embedding \(X_{En}\) as

\[(X_{En}):\;input_{En}=concat(temp)*_{out},\;temp =_{x_{i},x_{j} X_{En}}^{x_{i} x_{j}}(concat([x_{i},x_{j}])* _{ij}), \]

where \(concat()\) is concatenation operation, \(()\) is the tanh  activation function, \(_{ij} R^{2D D}\) and \(_{out} R^{|temp|*D D}\) are trainable matrices, and \(|temp|\) denotes the number of temporary features in \(temp\). The first item in equation (10) is to combine the selected embedding in pairs to fully fuse a pair of embedding without the influence of other embedding, which is inspired by the feature fusion in multi-modal tasks . After obtaining \(temp\), the second item in equation (10) is to directly apply the concatenation operation to \(temp\) followed by a linear transformation to get the final input \(input_{En}\) to the encoder part. The input \(input_{De}\) to the decoder part can be obtained in the same way.

As shown in Fig. 1, the size of \(_{out}\) is fixed as \( D\) for easily training super-Transformer, but the temporary features related to non-selected embedding are set to zeros. For example, feature 2 is not selected, thus the 1st, 4th, and 5th temporary features are set to zeros.

#### 4.1.2 The Global Path and The Local Path

Since students' forgetting behaviors in the KT cannot be directly modelled well by the single MHSA module, we aim to incorporate the Transformer with the local context modelling. Thus, we split the original global path containing the MHSA module in each encoder block into the sum of two paths: one path is the original global path, and another path is the newly added local path containing convolution operations. Each decoder block is also set to be composed of these two paths. After applying the pre-LN to the input \(X\) of each block as \(x_{1}=(X)\), each encoder and decoder block's forward pass in (2) and (3) can be rewritten as

\[_{En}gp=(x_{1}+(x_{1 },x_{1},x_{1}))\\ lp=(x_{1}+LO(x_{1}))\\ h=h+(h),h=lp+gp\;\;_{De} gp=(x_{1}+(x_{1},x_{1},x_{1}))\\ gp=gp+(gp,O_{En},O_{En})\\ lp=(x_{1}+LO(x_{1}))\\ h=h+(h),h=lp+gp, \]

where \(h\) refers to the output, \(gp\) is the latent representation obtained by the global path, \(lp\) is the latent representation obtained by the local path, and \(LO()\) denotes a local operation used for the local path.

As shown in Fig. 1, there are five candidate operations for \(LO()\), including four one-dimension convolution operations with kernel sizes in \(\{3,5,7,11\}\) and a zero operation that returns a zero tensor. When the zero operation is used for \(LO()\), the \(block_{En}\) and \(block_{De}\) will degenerate to the encoder and decoder block in the vanilla Transformer.

#### 4.1.3 The Global Operation

As demonstrated in the Sandwich Transformer , reordering the MHSA model and FFN module may lead to better performance. For this aim, a global operation is used to replace the first MHSAmodule and the FFN module of each encoder and decoder block in (11), where \(()\) and \(()\) are replaced by \(GO_{1}()\) and \(GO_{2}()\). Here, \(GO_{1}()\) and \(GO_{2}()\) represent the adopted global operation. As shown in the light orange area on the right of Fig. 1, there are three candidate operations for \(GO_{1/2}()\), including the MHSA module, FFN module, and a zero operation.

With the above modifications, the proposed Transformer architecture predicts the response \(=\{_{1},_{2},,_{L}\}\) as

\[=Decoder(Hier(X_{De}),O_{En},O_{En}),\;O_{En}=Encoder(Hier(X_{En})), \]

where \(X_{En}\) and \(X_{De}\) are obtained based on \(}\), \(}\), and \(X_{embed}\) as shown in (8) and (9).

#### 4.1.4 Encoding Scheme

Under the proposed search space, there theoretically exist \(Num^{2}*2^{2(Num-1)}*(3*3*5)^{2N}\) candidate Transformer architectures in total, where each candidate Transformer architecture \(\) can be encoded by \(=[},},(lo\{0,,4\},go_{1},go_{ 2}\{0,1,2\})^{1 2N}]\).

Here \(}\) and \(}\) denote which embedding is selected for the inputs to encoder and decoder parts, and \(2N\) triplets \((lo,go_{1},go_{2})\) determine which operations are adopted for each block, where \(lo\), \(go_{1}\), and \(go_{2}\) determine which operation is used for \(LO()\), \(GO_{1}()\), and \(GO_{2}()\).

Next, we search the optimal Transformer architecture by solving the following optimization problem:

\[_{}()=f_{val\_auc}(,D_{val}), \]

where \(f_{val\_auc}(,D_{val})\) denotes the AUC (_area under the curve_) value of \(\) on dataset \(D_{val}\).

### Search Approach Design

**Overview.** An overview of the proposed approach is shown in Fig. 2, where a super-Transformer is first trained and then an EA with \(Pop\) individuals and \(Gen\) generations is used for searching.

#### 4.2.1 Super-Transformer Training

Inspired by the sandwich rule , we propose to make the training rule adapt to our super-Transformer by utilizing three sub-models in one batch of training of the super-Transformer, including a global sub-model \(sub_{g}\), a local sub-model \(sub_{l}\), and a randomly sampled sub-model \(sub_{r}\). \(sub_{g}\) consists of pure global operations but no local operations, whose \(}\) and \(}\) are randomly generated; similarly, \(sub_{l}\) consists of only convolution operations with the smallest kernel size.

#### 4.2.2 Search Space Reduction-based Evolutionary Search

After training the supernet, a tailored EA is employed for searching by solving the problem in (13). The proposed EA takes the binary tournament selection  and single-point crossover together with the bit-wise mutation  to mate and generate individuals, where a search space reduction strategy is suggested to reduce the search space for accelerating the convergence at each generation.

**Search Space Reduction Strategy.** This strategy aims to iteratively remove some worse operations from the search space based on all individuals' fitness statistics to speed up the convergence. The item on each bit of \(\) is an operation set, such as the item on first bit is \(\{0,1\}\), and the items on

Figure 2: The overview of the proposed ENAS-KT.

the bits related to \(lo\) are \(\{0,1,2,3,4\}\). To start with, combine individuals in the population and offspring population; secondly, record each operation fitness in list \(SF\), where each operation fitness is obtained by averaging the fitness of the individuals containing the corresponding operation; thirdly, compute fitness standard deviation of each each operation set as list \(SF_{std}\); next, based on \(SF\) and \(SF_{std}\), the worst two operations will be deleted, and the worst operation in the operation set with the largest standard deviation will be also removed, where there is at least one operation kept for each bit. By doing so, the ever-reducing search space makes the search process easier. **More details** (about the super-net training, the EA, and the reduction strategy) can be found in Appendix A.

## 5 Experiments

### Experimental Settings

**Datasets.** For convincing validation, two largest and most challenging real-world education datasets EdNet  and RAIEd2020  were used in experiments, whose statistics are summarized in Table 1.

In addition to the above five feature embedding in Table 1: exercise \(exer\), skill \(sk\), tag \(tag\), tag-set \(tagset\), and bundle/explanation \(bundle\), there are other seven feature embedding in candidate input embedding \(X_{embed}\) (\(Num=12\)), including answer embedding \(ans\), continuous embedding for elapsed time \(cont\_ela\), categorical embedding for elapsed time in terms of seconds \(cate\_ela\_s\), continuous embedding for lag time \(cont\_lag\), categorical embedding for lag time in terms of seconds, minutes, and days (termed \(cate\_lag\_s\), \(cate\_lag\_m\), and \(cate\_lag\_d\)).

**Compared Approaches**. The following eight SOTA KT approaches were used for comparison:

* LSTM-based and Hawkes-based methods: DKT , HawkesKT , CT-NCM , and NAS-Cell . HawkesKT and CT-NCM consider extra students' forgetting behaviors in modelling, and NAS-Cell is NAS-based method for searching LSTMs' cells.
* Attention mechanism-based neural network methods: SAKT  and AKT . Here AKT considers extra students' forgetting behavior in its attention weights.
* Transformer-based methods: SAINT  and SAINT+ . But SAINT+ considers extra features of elapsed time and lag time in the decoder input to mitigate the forgetting behavior.

  
**Datasets/Statistics** &  \# of interactions \\ (\# of tags) \\  &  \# of students \\ (\# of tag-sets) \\  &  \# of exercises \\ (\# of bundles) \\  & 
 \# of skills \\ (\# of explanation) \\  \\ 
**EdNet/RAIEd2020** &  95,293,926/99,271,300 \\ (302)/(189) \\  &  84,309/393,656 \\ (1,792)/(1,520) \\  &  13,169/13,523 \\ (9,534)/(-) \\  & 
 7/7 \\ (-)/(2) \\  \\   

Table 1: Statistics of the two largest education datasets for knowledge tracing, EdNet and RAIEd2020.

  
**Dataset** & **Metric** & **DKT** & **HawkesKT** & **CT-NCM** & **SAKT** & **AKT** & **SAINT** & **SAINT+ NAS-Cell** & **Ours** \\   & EdNet & 0.13495 & **0.019578** & 1.9974 & 2.0864 & 1.2330 & 2.7492 & 3.1862 & 1.8692 & 3.8232 \\  & RAIEd2020 & 0.13531 & **0.019932** & 2.0431 & 2.1317 & 1.2335 & 2.7945 & 3.2315 & 1.9145 & 4.1262 \\    & **RMSE**\(\) & 0.4653 & 0.4475 & 0.4364 & 0.4405 & 0.4399 & 0.4322 & 0.4285 & 0.4345 & **0.4209** \\  & **ACC**\(\) & 0.6537 & 0.6888 & 0.7063 & 0.6998 & 0.7016 & 0.7132 & 0.7188 & 0.7143 & **0.7295** \\  & **AUC**\(\) & 0.6952 & 0.7487 & 0.7743 & 0.7650 & 0.7686 & 0.7825 & 0.7916 & 0.7796 & **0.8062** \\   & **RMSE**\(\) & 0.4632 & 0.4453 & 0.4355 & 0.4381 & 0.4368 & 0.4310 & 0.4272 & 0.4309 & **0.4196** \\  & **ACC**\(\) & 0.6622 & 0.6928 & 0.7079 & 0.7035 & 0.7076 & 0.7143 & 0.7192 & 0.7167 & **0.7313** \\   & **AUC**\(\) & 0.7108 & 0.7525 & 0.7771 & 0.7693 & 0.7752 & 0.7862 & 0.7934 & 0.7839 & **0.8089** \\   

Table 2: The comparison between ENAS-KT and compared approaches in term of model performance and parameters. The results under **splitting setting of 80% and 20%** on EdNet and RAIEd2020 are reported in terms of RMSE, ACC, and AUC, averaged over 5-fold cross-validation. The Wilcoxon rank sum test with a significance level \(\)=0.05 is used for analyzing the significance, where ’+’,’—’, and ’\(\)’ denote **Ours** is significantly better than, worse than, and similar to the compared approach, respectively. “M” is short for “million” in “Param.(M)”, counting the number of model parameters.

**Parameter Settings.** According to [7; 36], all students were randomly split into 70%/10%/20% for training/validation/testing. The maximal length of input sequences was set to 100 (\(L\)=100), we truncated student learning interactions longer than 100 to several sub-sequences, and 5-fold cross-validation was used. The number of blocks \(N\)=4, embedding size \(D\)=128, the hidden dimension of FFN was set to 128. To train the supernet, epoch number, learning rate, dropout rate, and batch size was set to 60, 1e-3, 0.1, and 128, and the number of warm-up steps in Noam scheme  was set to 8,000. To train the best architecture, epoch number and the number of warm-up steps was set to 30 and 4,000, and others kept same as above. \(Pop\) and \(Gen\) was set to 20 and 30. For a fair comparison, parameter settings of compared approaches are same as those in their original papers. We adopted AUC, _accuracy_ (ACC), and _root mean squared error_ (RMSE) as evaluation metrics. All experiments were implemented with PyTorch and run under NVIDIA 3080 GPU. The source code of the proposed approach is publicly available at [https://github.com/DevilYangS/ENAS-KT](https://github.com/DevilYangS/ENAS-KT).

### Overall Results

To verify the effectiveness of the proposed approach, Table 2 summarizes the performance of the proposed ENAS-KT and all comparison KT approaches on predicting students' responses across EdNet and RAIEd2020 datasets, where the average RMSE, ACC, and AUC values over 5-fold cross-validation are reported and the best results are in bold. For more convincing, the Wilcoxon rank sum test  with a significance level \(\)=0.05 is used for analyzing the significance. In addition, the number of model parameters for each approach is reported for profound observations.

From Table 2, we can obtain the following five observations: (1) The performance of Transformer-based KT approaches is significantly better than LSTM-based KT approaches and attention mechanism-based KT approaches. (2) From the comparisons between DKT and CT-NCM, SAKT and AKT, SAINT and SAINT+, it can be found that considering the forgetting behavior in the KT can indeed improve the prediction performance to some extent under whichever type of neural architecture. (3) The RMSE, ACC, and AUC values achieved by the architecture obtained by the proposed approach on both two datasets are significantly better than that achieved by comparison approaches, which indicates the architecture found by the proposed approach is more effective in tracing students' knowledge states. (4) Compared to NAS-Cell, the architectures found by the proposed approach are more effective, which demonstrates the effectiveness of the proposed search space and search algorithm. (5) In terms of model parameters, HawkesKT is the best, DKT is the second-best, while the architectures found by the proposed ENAS-KT are worse than other KT models. Actually, high model parameters of **Ours** are attributed to its input part, which is used to get the input embeddings and aggregate these embeddings.

To analyze the influence of model parameters on the performance of the best-found architecture, four parameter constraints (including 2M, 2.5M, 3M, and 3.5M) are added to the proposed ENAS-KT, respectively, where the parameters of all generated architectures cannot exceed the used parameter constraint. As a result, four architectures are found on EdNet, denoted as ENAS-KT(2M), ENAS-KT(2.5M), ENAS-KT(3M), and ENAS-KT(3.5M), and Table 3 gives a brief comparison between these found architectures some KT models. As can be seen, the proposed ENAS-KT can still find good potential architecture under the parameter constraint, and the found architecture's performance increases with model parameters increasing, but the increasing in model parameters is caused by the change of the found model architecture, not the embedding or hidden size. Therefore, we can conclude that the effectiveness of the proposed approach is mainly not attributed to its high model parameters, to some extent, which can be mainly attributed to the suggested search space. Note that the comparison between SAINT/SAINT+ and SAINT(more)/SAINT+(more) can indirectly validate the above conclusion that only increasing the model parameters to improve model performance does not work and is unworthy, which further demonstrates the effectiveness of the proposed search space. For more convincing, **more results** under other splitting settings can be found in Appendix B.

### Search Result Analysis

**Architecture Visualization.** The best-found architectures are plotted in Fig. 3. We can find that these two best-found architectures have very similar structures, their encoder parts prefer the convolution operations and FFN modules, especially for their first encoder blocks, while their decoder parts prefer the global operation: the MSHA module, especially for the decoder blocks close to the output, where the convolution operations with large kernel sizes are also preferred. To investigate the observation,we further transfer the best architecture found on one dataset to another datasets. The best architecture searched on EdNet dataset achieves an AUC value of **0.8078** on RAIEd2020 dataset, which is very close to the best performance on RAIEd2020 dataset, and the best architecture searched on the EdNet dataset also holds a very promising AUC value on EdNet: **0.8053**. Thus, the best-found architecture holds good transferability and generalization.

**Selected Features.** In addition, their selected input features for encoder and decoder parts are also similar. Fig. 4(a) compares the selected features of SAINT, SAINT+, and the best architecture on EdNet. We can observe that the selected features in SAINT and SAINT+ are a subset of the selected features in the best-found architecture, where the elapsed time embedding and the lag time embedding are considered, and extra features \(tag\) and \(bund\) are also considered.

### Ablation Study

**Effectiveness of Each Component**. To validate the effectiveness of each devised component, four variant architectures (_A-D_) are first given and SAINT+ is taken as the baseline. The comparison results on Ednet dataset have been summarized in Table 4. The variant architecture \(A\) refers to the vanilla Transformer with all candidate features concatenated and then mapped by a linear transformation as inputs, variant \(B\) is the same as \(A\) but only takes the selected features in the best architecture as inputs, variant \(C\) is the same as \(A\) but applies the hierarchical fusion method to the selected features, and variant \(D\) is the Transformer architecture in (11) whose local operations are fixed with \(Conv\)\(1 3\) and adopts _C_'s input module. The comparison among SAINT+, \(A\), \(B\), and \(C\) verifies the effectiveness of the proposed hierarchical fusion method and selected features in the architecture. The comparison between \(C\) and \(D\) indicates incorporating convolution operations with Transformer is helpful to improve performance. Moreover, the results of \(D\) and the best architecture validate the effectiveness of the proposed approach, including the effectiveness of search space as well as the search approach.

Similarly, other four variant architectures (_E-H_) are established in Table 4 to show the influence of each designed component on the best-found architecture's performance. As can be seen, the searched model architecture, the selected features, and the devised fusion module play important roles while the searched model architecture holds the biggest influence on the performance of **Ours**.

Figure 3: The best architectures found on EdNet (a) and RAIEd2020 (b), here we simplify the connections in each encoder/decoder block for better observation.

  
**Model** & **Param.(M)** & **AUC** & **Model** & **Param.(M)** & **AUC** & **Model** & **Param.(M)** & **AUC** \\  ENAS-KT(2M) & 1.938 & 0.8021 CT-NCM & 1.997 & 0.7743 & SAINT & 2.749 & 0.7825 \\ ENAS-KT(2.5M) & 2.389 & 0.8047 & SAKT & 2.086 & 0.7650 & SAINT+ & 3.186 & 0.7916 \\ ENAS-KT(3M) & 2.918 & 0.8056 & AKT & 1.233 & 0.7686 & SAINT(more) & **4.910** & 0.7828 \\ ENAS-KT(3.5M) & 3.489 & 0.8059NAS-Cell & 1.869 & 0.7796SAINT+(more) & **5.627** & 0.7921 \\   

Table 3: The comparison on EdNet dataset in terms of AUC and the number of model parameters: some KT approaches and best architectures found by ENAS-KT with different parameter constraints. SAINT(more) and SAINT+(more) refer to two models with larger hidden and embedding sizes (192).

To investigate the effectiveness of ENAS-KT with less search cost, as shown in Table 4, a variant ENAS-KT(f) is created. Although its searched architecture's performance is slightly worse than **Ours**, the model performance is better than other approaches and its search cost only takes 9.1 hours, which is much fewer than **Ours** and competitive to NAS-Cell and others (can be found in Appendix B).

**Effectiveness of Search Space Reduction Strategy.** To validate the effectiveness of the devised search space reduction strategy, Fig. 4(b) depicts the convergence profiles of best validation AUC value obtained by the proposed approaches with and without search space reduction strategy on both two datasets, which are denoted by _Approach_ and _Approach(-Reduction)_, respectively. As can be seen, the suggested search space reduction strategy can indeed accelerate the convergence of the proposed evolutionary search approach and leads to better final convergence. **More experimental discussions** about the proposed ENAS-KT can be found in Appendix B.

## 6 Conclusion

In this paper, we first introduced the local context modelling held by convolutions to the Transformer, then automated the balancing of the local/global context modelling and the input feature selection by proposing an evolutionary NAS approach. The designed search space not only replaces the original global path in Transformer with the sum of a global path and a local path that could contain different convolutions but also considers the selection of input features. Besides, we utilized an EA to search the best architecture and suggested a search space reduction strategy to accelerate its convergence. Experiments on two largest datasets show the effectiveness of the proposed approach.

  
**Method** & **RMSE\(\) ACC\(\) AUC\(\)+/\(\)** \\  SAINT+ & 0.4285 0.71880.7916 3/0/0 \\ \(A\): All Features + Concat & 0.4276 0.72030.7937 3/0/0 \\ \(B\): Selected Features + Concat & 0.4262 0.72170.7958 3/0/0 \\ \(C\): Selected Features + Hierarchical & 0.4250 0.72360.7987 3/0/0 \\ \(D\): \(C\)’s Input + Convolution & 0.4235 0.72530.8012 3/0/0 \\  \(E\): **Ours** (without Hierarchical Fusion, with Concat) & 0.4223 0.72690.8041 3/0/0 \\ \(F\): **Ours** (without the Selected Features, with All Features) & 0.4221 0.7260.8030 3/0/0 \\ \(G\): **Ours** (without Selected Features \& Hierarchical, with SAINT+’s input) & 0.4238 0.72490.8008 3/0/0 \\ \(H\): **Ours** (without the Searched Architecture, with SAINT+’s model), i.e., \(C\) & 0.4250 0.72360.7987 3/0/0 \\    & 0.4224 0.72710.8036 3/0/0 \\    & **0.4209** **0.72950.8062** & - \\   

Table 4: The effectiveness validation of the proposed different components on the EdNet dataset, and the influence of the searched model architecture and the selected features on the best-found architecture’s performance is also presented. The Wilcoxon rank sum test with \(\)=0.05 is performed.

Figure 4: The comparison of selected features on EdNet (a), and the effectiveness of search space reduction strategy (b).