# AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation

AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation

 Daiki E. Matsunaga

KAIST

dematsunaga@ai.kaist.ac.kr

&Jongmin Lee

UC Berkeley

jongmin.lee@berkeley.edu

&Jaeseok Yoon

KAIST

jsyoon@ai.kaist.ac.kr

&Stefanos Leonardos

King's College London

stefanos.leonardos@kcl.ac.uk

&Pieter Abbeel

UC Berkeley

pabbeel@cs.berkeley.edu

&Kee-Eung Kim

KAIST

kekim@kaist.ac.kr

###### Abstract

One of the main challenges in offline Reinforcement Learning (RL) is the distribution shift that arises from the learned policy deviating from the data collection policy. This is often addressed by avoiding out-of-distribution (OOD) actions during policy improvement as their presence can lead to substantial performance degradation. This challenge is amplified in the offline Multi-Agent RL (MARL) setting since the joint action space grows exponentially with the number of agents. To avoid this curse of dimensionality, existing MARL methods adopt either value decomposition methods or fully decentralized training of individual agents. However, even when combined with standard conservatism principles, these methods can still result in the selection of OOD joint actions in offline MARL. To this end, we introduce AlberDICE, an offline MARL algorithm that alternatively performs centralized training of individual agents based on stationary distribution optimization. AlberDICE circumvents the exponential complexity of MARL by computing the best response of one agent at a time while effectively avoiding OOD joint action selection. Theoretically, we show that the alternating optimization procedure converges to Nash policies. In the experiments, we demonstrate that AlberDICE significantly outperforms baseline algorithms on a standard suite of MARL benchmarks.

## 1 Introduction

Offline Reinforcement Learning (RL) has emerged as a promising paradigm to train RL agents solely from pre-collected datasets . Offline RL aims to address real-world settings in which further interaction with the environment during training is dangerous or prohibitively expensive, e.g., autonomous-car driving, healthcare operations or robotic control tasks . One of the main challenges for successful offline RL is to address the distribution shift that arises from the difference between the policy being learned and the policy used for data collection. Conservatism is a commonly adopted principle to mitigate the distribution shift, which prevents the selection of OOD actions via conservative action-value estimates  or direct policy constraints .

However, avoiding the selection of OOD actions becomes very challenging in offline Multi-Agent RL (MARL)2, as the goal is now to stay close to the states and _joint_ actions in the dataset. This is not trivial since the joint action space scales exponentially with the number of agents, a problem known as the _curse of dimensionality_. Previous attempts to address these issues include decomposing the joint action-value function under strict assumptions such as the Individual-Global-Max (IGM) principle [30; 33; 36; 40], or decentralized training which ignores the non-stationarity caused by the changing policies of other agents [18; 25; 38]. While effective in avoiding the curse of dimensionality, these assumptions are insufficient in avoiding OOD joint action selection even when applying the conservatism principles.

To illustrate the problem of joint action OOD, consider the XOR game in Figure 1. In this game, two agents need to coordinate to achieve optimal joint actions, here, either \((A,B)\) or \((B,A)\). Despite its simple structure, the co-occurrence of two global optima causes many existing algorithms to degenerate in the XOR game . To see this, suppose we have an offline dataset \(D=\{(A,A,0),(A,B,1),(B,A,1)\}\). In this situation, IGM-based methods  represent the joint \(Q(a_{1},a_{2})\) as a combination of individual \(Q_{1}(a_{1})\) and \(Q_{2}(a_{2})\), where action \(B\) is incentivized over action \(A\) by both agents in the individual \(Q\) functions. As a consequence, IGM-based methods end up selecting \((B,B)\), which is the OOD joint action. Similarly, decentralized training methods  also choose the OOD joint action \((B,B)\), given that each agent assumes that another agent is fixed with a data policy of selecting \(A\) with probability \(\). Furthermore, we can see that even behavior-cloning on the expert-only dataset, i.e., \(D=\{(A,B),(B,A)\}\), may end up selecting OOD joint actions as well: each individual policy \(_{1}(a_{1})\) and \(_{2}(a_{2})\) will be uniform over the two individual actions, leading to uniform action selection over the entire joint action space; thus, both \((A,A)\) and \((B,B)\) can be selected. Consequently, OOD joint actions can be hard to avoid especially in these types of environments with multiple global optima and/or when the offline dataset consists of trajectories generated by a mixture of data collection policies.

Our approach and resultsTo address these challenges, we introduce _AlberDICE (**AL**termate **BE**st **R**esponse Stationary **DI**stribution **C**orrection **E**stimation), a novel offline MARL algorithm for avoiding OOD actions in the joint action space while circumventing the curse of dimensionality. We start by presenting a coordinate descent-like training procedure where each agent sequentially computes their best response policy while fixing the policies of others. In order to do this in an offline manner, we utilize the linear programming (LP) formulation of RL for optimizing stationary distribution, which has been adapted in offline RL  as a stable training procedure where value estimations of OOD actions are eliminated. Furthermore, we introduce a regularization term to the LP objective which matches the stationary distributions of the dataset in the _joint action space_. This regularization term allows AlberDICE to avoid OOD joint actions as well as the curse of dimensionality without any restrictive assumptions such as factorization of value functions via IGM or fully decentralized training. Overall, our training procedure only requires the mild assumption of Centralized Training and Decentralized Execution (CTDE), a popular paradigm in MARL [17; 28; 30] where we assume access to all global information such as state and joint actions during training while agents act independently during execution.

Theoretically, we show that our regularization term preserves the common reward structure of the underlying task and that the sequence of generated policies converges to a Nash policy (Theorem 4.2). We also conduct extensive experiments to evaluate our approach on a standard suite of MARL environments including the XOR Game, Bridge , Multi-Robot Warehouse , Google Research Football  and SMAC , and show that AlberDICE significantly outperforms baselines. To the best of our knowledge, AlberDICE is the first DICE-family algorithm successfully applied to the MARL setting while addressing the problem of OOD joint actions in a principled manner.3

## 2 Background

Multi-Agent MDP (MMDP)We consider the fully cooperative MARL setting, which can be formalized as a Multi-Agent Markov Decision Process (MMDP) 4. An \(N\)-Agent MMDP is defined by a tuple \(G=,,,r,P,p_{0},\) where \(=\{1,2,,N\}\) is the set of agent indices, \(s\) is the state, \(=_{1}_{N}\) is the joint action space, \(p_{0}()\) is the initial state

Figure 1: XOR Game

[MISSING_PAGE_FAIL:3]

penalty for deviating from the data distribution, which is a commonly adopted principle in offline RL . Satisfying the Bellman-flow constraints (3) guarantees that \(d(s,a_{i},_{-i}):=d_{i}(s,a_{i})_{-i}(_{-i}| s)\) is a valid stationary distribution in the MMDP.

As we show in our theoretical treatment of the regularized LP in Section 4, the selected regularization term defined in terms of joint action space critically ensures that every agent \(i\) optimizes the _same_ objective function in (2). This ensures that when agents optimize alternately, the objective function always monotonically improves which, in turn, guarantees convergence (see Theorem 4.2). This is in contrast to existing methods such as , where each agent optimizes the _different_ objective functions. Importantly, this is achieved while ensuring conservatism. As can be seen from (2), the KL-regularization term is defined in terms of the _joint_ stationary distribution of _all_ agents which ensures that the optimization of the regularized LP effectively avoids OOD joint action selection.

The optimal solution of the regularized LP (2-3), \(d_{i}^{*}\), corresponds to the stationary distribution for a best response policy \(_{i}^{*}\) against the fixed \(_{-i}\), and \(_{i}^{*}\) can be obtained by \(_{i}^{*}=^{*}(s,a_{i})}{_{a_{i}}d_{i}^{*}(s,a_{i})}\). The (regularized) LP (2-3) can also be understood as solving a (regularized) _reduced MDP_\(_{i}=,_{i},_{i},_{i},,p_{0}\) for a single agent \(i\), where \(_{i}\) and \(_{i}\) are defined as follows5:

\[_{i}(s^{}|s,a_{i}):=_{_{-i}}_{-i}( _{-i}|s)P(s^{}|s,a_{i},_{-i}),\;_{i}(s,a_{i}) :=_{_{-i}}_{-i}(_{-i}|s)r(s,a_{i}, _{-i}).\]

Then, \(d_{i}^{*}\) is an optimal stationary distribution on the reduced MDP, \(_{i}\), but the reduced MDP is non-stationary due to other agents' policy, \(_{-i}\), updates. Therefore, it is important to account for changes in \(_{-i}\) during training in order to avoid selection of OOD joint actions.

Lagrangian FormulationThe constrained optimization (2-3) is not directly solvable since we do not have a white-box model for the MMDP. In order to make (2-3) amenable to offline learning in a model-free manner, we consider a Lagrangian of the constrained optimization problem:

\[_{_{i}}_{d_{i} 0}_{(s,a _{i}) d_{i}\\ _{-i}_{-i}(s)}[r(s,a_{i},_{-i}) ]-_{s,a_{i},_{-i}}d_{i}(s,a_{i})_{-i}( _{-i}|s)(s,a_{i})_{-i}(_{-i}|s)}{d^{D}(s,a_{i}) _{-i}(_{-i}|s,a_{i})}\] \[+_{s^{}}_{i}(s^{})(1-)p_{0}(s^{ })-_{a_{i}^{}}d_{i}(s^{},a_{i}^{})+\!\!\! _{s,a_{i},_{-i}}\!\!\!\!P(s^{}|s,a_{i},_{-i})d_ {i}(s,a_{i})_{-i}(_{-i}|s)\] (4)

where \(_{i}(s)\) is the Lagrange multiplier for the Bellman flow constraints6. Still, (4) is not directly solvable due to its requirement of \(P(s^{}|s,a_{i},_{-i})\) for \((s,a_{i}) d_{i}\) that are not accessible in the offline setting. To make progress, we re-arrange the terms in (4) as follows

\[_{_{i}}_{d_{i} 0}\;(1-)_{s_{0}  p_{0}}[_{i}(s_{0})]+_{(s,a_{i}) d_{i}}- (s,a_{i})}{d^{D}(s,a_{i})}\] (5) \[+_{ _{-i}_{-i}(s)\\ s^{} P(s,a_{i},_{-i})}[r(s,a_{i}, _{-i})-_{-i}(_{-i}|s)}{ _{-i}^{D}(_{-i}|s,a_{i})}+_{i}(s^{}) -_{i}(s)]}_{=:e_{_{i}}(s,a_{i})}\] \[=_{_{i}}_{d_{i} 0}\;(1-)_{s_{0}  p_{0}}[_{i}(s_{0})]+_{(s,a_{i}) d^{D}}(s,a_{i})}{d^{D}(s,a_{i})}e_{_{i}}(s,a_{i})-(s,a_{i})}{ d^{D}(s,a_{i})}\] (6) \[=_{_{i}}_{w_{i} 0}\;(1-)_{s_{0}  p_{0}}[_{i}(s_{0})]+_{(s,a_{i}) d^{D}}w_{i}(s,a_{i} )e_{_{i}}(s,a_{i})- w_{i}(s,a_{i})\] (7)

where \(e_{_{i}}(s,a_{i})\) is the advantage by \(_{i}\), and \(w_{i}(s,a_{i})\) are the stationary distribution correction ratios between \(d_{i}\) and \(d^{D}\). Finally, to enable every term in (7) to be estimated from samples in the offline dataset \(D\), we adopt importance sampling, which accounts for the distribution shift in other agents' policies, \(_{-i}\):

\[_{_{i}}_{w_{i} 0}\;(1-)_{p_{0}}[_{i}(s_{0})]+ _{(s,a_{i},_{-i}s^{}) d^{D}}w_{i}(s,a_{i} )_{-i}(_{-i}|s)}{_{-i}^{D}( _{-i}|s,a_{i})}_{_{i}}(s,a_{i},_{-i},s^{ })- w_{i}(s,a_{i})\] (8)where \(_{_{i}}(s,a_{i},_{-i},s^{}):=r(s,a_{i},_{-i})- _{-i}(_{-i}|s)}{_{-i}^ {D}(_{-i}|s,a_{i})}+_{i}(s^{})-_{i}(s)\). Every term in (8) can be now evaluated using only the samples in the offline dataset. Consequently, AlberDICE aims to solve the unconstrained minimax optimization (8) for each agent \(i\). Once we compute the optimal solution \((_{i}^{*},w_{i}^{*})\) of (8), we obtain the information about the optimal policy \(_{i}^{*}\) (i.e. the best response policy against the fixed \(_{-i}\)) in the form of distribution correction ratios \(w_{i}^{*}=_{i}}(s,a_{i})}{d^{D}(s,a_{i})}\).

Pretraining autoregressive data policyTo optimize (8), we should be able to evaluate \(_{-i}^{D}(_{-i}|s,a_{i})\) for each \((s,a_{i},_{-i}) D\). To this end, we pretrain the data policy via behavior cloning, where we adopt an MLP-based autoregressive policy architecture, similar to the one in . The input dimension of \(_{-i}^{D}\) only grows linearly with the number of agents. Then, for each \(i\), we optimize the following:

\[_{_{-i}^{}}_{(s,a_{i},_{-i }) d^{D}}[_{j=1,j i}^{N}_{-i}^{D}(a_{j}|s,a_{i},a_{<j})]\] (9)

While, in principle, the joint action space grows exponentially with the number of agents, learning a joint data distribution in an autoregressive manner is known to work quite well in practice .

Practical Algorithm: Minimax to MinStill, solving the nested minimax optimization (7) can be numerically unstable in practice. In this section, we derive a practical algorithm that solves a single minimization only using offline samples. For brevity, we denote each sample \((s,a_{i},_{-i},s^{})\) in the dataset as \(x\). Also, let \(}_{x D}[f(x)]:=_{x D}f(x)\) be a Monte-Carlo estimate of \(_{x p}[f(x)]\), where \(D=\{x_{k}\}_{k=1}^{|D|} p\). First, we have an unbiased estimator of (7):

\[_{_{i}}_{w_{i} 0}(1-)}_{s_{0} D_{0}}[ _{i}(s_{0})]+}_{x D}w_{i}(s,a_{i})_{i}(x) _{_{i}}(x)- w_{i}(s,a_{i})\] (10)

where \(_{i}(x)\) is defined as:

\[_{i}(x):=_{-i}(_{-i}|s)}{ _{-i}^{D}(_{-i}|s,a_{i})}=_{j}(a_{j}|s)}{ _{-i}^{D}(_{-i}|s,a_{i})}.\] (11)

Optimizing (10) can suffer from large variance due to the large magnitude of \((x)\), which contains products of \(N-1\) policies. To remedy the large variance issue, we adopt Importance Resampling (IR)  to (10). Specifically, we sample a mini-batch of size \(K\) from \(D\) with probability proportional to \((x)\), which constitutes a resampled dataset \(D_{_{i}}=\{(s,a_{i},_{-i},s^{})_{k}\}_{k=1}^{K}\). Then, we solve the following optimization, which now does not involve the importance ratio:

\[_{_{i}}_{w_{i} 0}(1-)}_{s_{0} D_{0}}[ _{i}(s_{0})]+_{i}}_{x D_{_{i}}}w_{i} (s,a_{i})_{_{i}}(x)- w_{i}(s,a_{i})\] (12)

where \(_{i}:=}_{x D}[_{i}(x)]\). It can be proven that (12) is still an unbiased estimator of (7) thanks to the bias correction term of \(\). The resampling procedure can be understood as follows: for each data sample \(x=(s,a_{i},_{-i},s^{})\), if other agents' policy \(_{-i}\) selects the action \(_{-i} D\) with low probability, i.e., \(_{-i}(_{-i}|s) 0\), the sample \(x\) will be removed during the resampling procedure, which makes the samples in the resampled dataset \(D_{_{i}}\) consistent with the reduced MDP \(_{i}\)'s dynamics. Finally, to avoid the numerical instability associated with solving a min-max optimization problem, we exploit the properties of the inner-maximization problem in (12), specifically, its concavity in \(w_{i}\), and derive its closed-form solution.

**Proposition 3.1**.: _The closed-form solution for the inner-maximization in (12) for each \(x\) is given by_

\[_{_{i}}^{*}(x)=(_{_{i}}(x)-1)\] (13)

By plugging equation (13) into (12), we obtain the following minimization problem:

\[_{_{i}}_{i}}_{x D_{_{i}}} (_{_{i}}(x)-1)+(1-) _{s_{0} p_{0}}[_{i}(s_{0})]=:L(_{i}).\] (14)

As we show in Proposition B.1 in the Appendix, \((_{i})\) is an unconstrained convex optimization problem where the function to learn \(_{i}\) is _state-dependent_. Furthermore, the terms in (14) are estimated only using the \((s,a_{i},_{-i},s^{})\) samples in the dataset, making it free from the extrapolation error by bootstrapping OOD action values. Also, since \(_{i}(s)\) does not involve joint actions, it is not required to adopt IGM-principle in \(_{i}\) network modeling; thus, there is no need to limit the expressiveness power of the function approximator. In practice, we parameterize \(_{i}\) using simple MLPs, which take the state \(s\) as an input and output a scalar value.

Policy ExtractionThe final remaining step is to extract a policy from the estimated distribution correction ratio \(w_{i}^{*}(s,a_{i})=_{i}}(s,a_{i})}{d^{D}(s,a_{i})}\). Unlike actor-critic approaches which perform intertwined optimizations by alternating between policy evaluation and policy improvement, solving (14) directly results in the optimal \(_{i}^{*}\). However, this does not result in an executable policy. We therefore utilize the I-projection policy extraction method from  which we found to be most numerically stable

\[_{_{i}}}(d^{D}(s)_{i}(a_{i}|s) _{-i}(_{-i}|s)||d^{D}(s)_{i}^{*}(a_{i}|s) _{-i}(_{-i}|s))\] (15) \[= _{_{i}}}_{s D,a_{i}_{i}} - w_{i}^{*}(s,a_{i})+}(_{i}(a_{i}|s)||_{i}^{D}(a_{i}|s ))\] (16)

In summary, AlberDICE computes the best response policy of agent \(i\) by: (1) resampling data points based on the other agents' policy ratios \(\) (11) where the data policy \(_{-i}^{D}(_{-i}|s,a_{i})\) can be pretrained, (2) solving a minimization problem to find \(_{i}^{*}(s)\) (31) and finally, (3) extracting the policy using the obtained \(_{i}^{*}\) by I-projection (15). In practice, rather than training \(_{i}\) until convergence at each iteration, we perform a single gradient update for each agent \(_{i}\) and \(_{i}\) alternatively. We outline the details of policy extraction (Appendix E.2) and the full learning procedure in Algorithm 1 (Appendix E).

## 4 Preservation of Common Rewards and Convergence to Nash Policies

In the previous sections, AlberDICE was derived as a practical algorithm in which agents alternately compute the best response DICE while avoiding OOD joint actions. We now prove formally that this procedure converges to Nash policies. While it is known that alternating best response can converge to Nash policies in common reward settings , it is not immediately clear whether the same result holds for the regularized LP (2-3), and hence the regularized reward function of the environment, preserves the common reward structure of the original MMDP. As we show in Lemma 4.1, this is indeed the case, i.e., the modified reward in (2-3) is shared across all agents. This directly implies that optimization of the corresponding LP yields the same value for all agents \(i\) for any joint policy, \(\), with factorized individual policies, \(\{_{i}\}_{i}\).

**Lemma 4.1**.: _Consider a joint policy \(=(_{i})_{i}\), with factorized individual policies, i.e., \((|s)=_{i}_{i}(a_{i}|s)\) for all \((s,) S\) with \(=(a_{i})_{i N}\). Then, the regularized objective in the LP formulation of AlberDICE, cf. equation (2), can be evaluated to_

\[_{s,a_{i},_{-i}}d_{i}^{}(s,a_{i})_{-i}(_{-i}|s) (s,a_{i},_{-i}),\]

_with \((s,a_{i},_{-i}):=r(s,a_{i},_{-i})- (s)(|s)}{d^{D}(s,a_{i},-a_{-i})}\), for all \((s,) S\). In particular, for any joint policy, \(=()_{i}\), with factorized individual policies, the regularized objective in the LP formulation of AlberDICE attains the same value for all agents \(i\)._

We can now use Lemma 4.1 to show that AlberDICE enjoys desirable convergence guarantees in tabular domains in which the policies, \(_{i}(a_{i}|s)\), can be directly extracted from \(d_{i}(s,a_{i})\) through the expression \(_{i}(a_{i}|s)=(s,a_{i})}{_{a_{j}}d_{i}(s,a_{j})}\).

**Theorem 4.2**.: _Given an MMDP, \(G\), and a regularization parameter \( 0\), consider the modified MMDP \(\) with rewards \(\) as defined in Lemma 4.1 and assume that each agent alternately solves the regularized LP defined in equations (2-3). Then, the sequence of policy updates, \((^{t})_{t 0}\), converges to a Nash policy, \(^{*}=(_{i}^{*})_{i}\), of \(\)._

The proofs of Lemma 4.1 and Theorem 4.2 are given in Appendix D. Intuitively, Theorem 4.2 relies on the fact that the objectives in the alternating optimization problems (2-3) involve the same rewards for all agents for any value of the regularization parameter, \( 0\), cf. Lemma 4.1. Accordingly, every update by any agent improves this common value function, \((^{}(s))_{s S}\), and at some point the sequence of updates is bound to terminate at a (local) maximum of \(\). At this point, no agent can improve by deviating to another policy which implies that the corresponding joint policy is a Nash policy of the underlying (modified) MMDP. For practical purposes, it is also relevant to note that the process may terminate at an \(\)-Nash policy (cf. Definition 2.1), since the improvements in the common value function may become arbitrarily small when solving the LPs numerically.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

Our results for GRF and SMAC in Tables 4 and 5 show that AlberDICE performs consistently well across all scenarios, and outperforms all baselines especially in the Super Hard maps, Corridor and 6h8z. The strong performance by AlberDICE corroborates the importance of avoiding OOD joint actions in order to avoid performance degradation.

### Does AlberDICE reduce OOD joint actions?

In order to evaluate how effectively AlberDICE prevents selecting OOD joint actions, we conducted additional experiments on two SMAC domains (8m_vs_9m and 3s5z_vs_3s6z) as follows. First, we trained an uncertainty estimator \(U(s,)\) via fitting random prior \(f:S_{1}_{N} ^{m}\) using the dataset \(D=\{(s,)_{k}\}_{k=1}^{|D|}\). Then, \(U(s,)=\|f(s,)-h(s,)\|^{2}\) outputs low values for in-distribution \((s,)\) samples and outputs high values for out-of-distribution \((s,)\) samples. Figure 5(a) shows a histogram of uncertainty estimates \(U(s,_{1}(s),,_{N}(s))\) for each \(s D\) and the joint action selected by each method. We set the threshold \(\) for determining OOD samples to 99.9%-quantile of \(\{U(s,a):(s,a) D\}\). Figure 5(b) presents the percentage of selecting OOD joint actions by each method. AlberDICE selects OOD joint actions significantly **less** often than ICQ (IGM-based method) and OMAR (decentralized training method) while outperforming them in terms of success rate (see Table 5).

## 6 Related Work

DICE for Offline RLNumerous recent works utilize the LP formulation of RL to derive DICE algorithms for policy evaluation [20; 21; 22]. OptiDICE  was introduced as the first policy optimization algorithm for DICE and as a stable offline RL algorithm which does not require value estimation of OOD actions. While OptiDICE can be naively extended to offline MARL in principle, it can still fail to avoid OOD joint actions since its primary focus is to optimize over the joint action space of the MMDP and does not consider the factorizability of policies. We detail the shortcomings of a naive extension of OptiDICE to multi-agent settings in Appendix C.

Value-Based MARL

A popular method in cooperative MARL is (state-action) value decomposition. This approach can be viewed as a way to model \(Q(s,a)\)_implicitly_ by aggregating \(Q_{i}\) in a specific manner, e.g., sum , or weighted sum . Thus, it avoids modelling \(Q(s,a)\)_explicitly_ over the joint action space. QTRAN  and QPLEX  further achieve full representativeness of IGM

Figure 5: Experimental results to see how effectively AlberDICE avoids OOD joint actions.

    & 3s5z (Hard) & 5m_vs_6m (Hard) & Corridor (SH) & 6hvs8z (SH) & 8m_vs_9m (Hard) & 3s5z_vs_3s6z (SH) \\  & (\(N=8\)) & (\(N=5\)) & (\(N=6\)) & (\(N=8\)) & (\(N=8\)) \\  BC & \(0.30 0.05\) & **0.23 \(\) 0.02** & \(0.90 0.02\) & \(0.11 0.02\) & \(0.48 0.05\) & \(0.45 0.03\) \\ ICQ & \(0.18 0.08\) & **0.18 \(\) 0.10** & \(0.78 0.03\) & \(0.00 0.00\) & \(0.12 0.21\) & \(0.31 0.04\) \\ OMAR & **0.43 \(\) 0.04** & **0.18 \(\) 0.02** & \(0.92 0.02\) & \(0.15 0.03\) & \(0.45 0.05\) & **0.60 \(\) 0.05** \\ MADTKD & \(0.12 0.02\) & **0.19 \(\) 0.02** & \(0.67 0.01\) & \(0.09 0.02\) & \(0.14 0.04\) & \(0.18 0.02\) \\ OptiDICE & \(0.28 0.05\) & **0.21 \(\) 0.02** & \(0.91 0.02\) & \(0.13 0.00\) & \(0.47 0.05\) & \(0.42 0.04\) \\ AlberDICE & **0.47 \(\) 0.03** & **0.24 \(\) 0.03** & **0.98 \(\) 0.00** & **0.21 \(\) 0.03** & **0.67 \(\) 0.06** & **0.63 \(\) 0.03** \\   

Table 5: Mean success rate and standard error (over 5 random seeds) on SMAC\({}^{9}\). These approaches have been shown to perform well in high-dimensional complex environments including SMAC . However, the IGM assumption and the value decomposition structure have been shown to perform poorly even in simple coordination tasks such as the XOR game .

Policy-Based MARLRecently, policy gradient methods such as MAPPO  have shown strong performance on many complex benchmarks including SMAC and GRF. Fu et al.  showed that independent policy gradient with separate parameters can solve the XOR game and the Bridge environment by converging to a deterministic policy for one of the optimal joint actions. However, it requires an autoregressive policy structure (centralized execution) to learn a stochastic optimal policy which covers multiple optimal joint actions. These empirical findings are consistent with theoretical results [15; 44] showing that running independent policy gradient can converge to a Nash policy in cooperative MARL. On the downside, policy gradient methods are trained with on-policy samples and thus, cannot be extended to the offline RL settings due to the distribution shift problem .

Offline MARLICQ  was the first MARL algorithm applied to the offline setting. It proposed an actor-critic approach to overcome the extrapolation error caused by the evaluation of unseen state-action pairs, where the error is shown to grow exponentially with the number of agents. The centralized critic here uses QMIX  and thus, it inherits some of the weaknesses associated with value decomposition and IGM. OMAR  is a decentralized training algorithm where each agent runs single-agent offline RL over the individual Q-functions and treats other agents as part of the environment. As a consequence, it lacks theoretical motivation and convergence guarantees in the underlying MMDP or Dec-POMDP. MADTKD  extends Multi-Agent Decision Transformers  to incorporate credit assignment across agents by distilling the teacher policy learned over the joint action space to each agent (student). This approach can still lead to OOD joint actions since the teacher policy learns a joint policy over the joint action space and the actions are distilled individually to students.

## 7 Limitations

AlberDICE relies on Nash policy convergence which is a well-established solution concept in Game Theory, especially in the general non-cooperative case where each agent may have conflicting reward functions. One limitation of AlberDICE is that the Nash policy may not necessarily correspond to the global optima in cooperative settings. The outcome of the iterative best response depends on the starting point (region of attraction of each Nash policy) and is, thus, generally not guaranteed to find the optimal Nash policy . This is the notorious equilibrium selection problem which is an open problem in games with multiple equilibria, even if they have common reward structure (See Open Questions in ). Nonetheless, Nash policies have been used as a solution concept for iterative update of each agents as a way to ensure convergence to factorized policies in Cooperative MARL . Furthermore, good equilibria tend to have larger regions of attraction and practical performance is typically very good as demonstrated by our extensive experiments.

## 8 Conclusion

In this paper, we presented AlberDICE, a multi-agent RL algorithm which addresses the problem of distribution shift in offline MARL by avoiding both OOD joint actions and the exponential nature of the joint action space. AlberDICE leverages an alternating optimization procedure where each agent computes the best response DICE while fixing the policies of other agents. Furthermore, it introduces a regularization term over the stationary distribution of states and joint actions in the dataset. This regularization term preserves the common reward structure of the environment and together with the alternating optimization procedure, allows convergence to Nash policies. As a result, AlberDICE is able to perform robustly across many offline MARL settings, even in complex environments where agents can easily converge to sub-optimal policies and/or select OOD joint actions. As the first DICE algorithm applied to offline MARL with a principled approach to curbing distribution shift, this work provides a starting point for further applications of DICE in MARL and a promising perspective in addressing the main problems of offline MARL.