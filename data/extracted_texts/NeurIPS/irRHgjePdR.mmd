# Improving Compositional Generalization using

Iterated Learning and Simplicial Embeddings

 Yi Ren

University of British Columbia

renyi.joshua@gmail.com

&Samuel Lavoie

Universite de Montreal & Mila

samuel.lavoie.m@gmail.com

Mikhail Galkin

Intel AI Lab

mikhail.galkin@intel.com

&Danica J. Sutherland

University of British Columbia & Amii

dsuth@cs.ubc.ca

Aaron Courville

Universite de Montreal & Mila

aaron.courville@gmail.com

Work done in part during an internship at Mila.

###### Abstract

Compositional generalization, the ability of an agent to generalize to unseen combinations of latent factors, is easy for humans but hard for deep neural networks. A line of research in cognitive science has hypothesized a process, "iterated learning," to help explain how human language developed this ability; the theory rests on simultaneous pressures towards compressibility (when an ignorant agent learns from an informed one) and expressivity (when it uses the representation for downstream tasks). Inspired by this process, we propose to improve the compositional generalization of deep networks by using iterated learning on models with simplicial embeddings, which can approximately discretize representations. This approach is further motivated by an analysis of compositionality based on Kolmogorov complexity. We show that this combination of changes improves compositional generalization over other approaches, demonstrating these improvements both on vision tasks with well-understood latent factors and on real molecular graph prediction tasks where the latent structure is unknown.

## 1 Introduction

Deep neural networks have shown an amazing ability to generalize to new samples on domains where they have been extensively trained, approaching or surpassing human performance on tasks including image classification , Go , reading comprehension , and more. A growing body of literature, however, demonstrates that some tasks that can be easily solved by a human can be hard for deep models. One important such problem is compositional generalization (, _comp-gen_ for short). For example, Schott et al.  study manually-created vision datasets where the true generating factors are known, and demonstrate that a wide variety of current representation learning methods struggle to learn the underlying mechanism. To achieve true "artificially intelligent" methods that can succeed at a variety of difficult tasks, it seems necessary to demonstrate compositional generalization. One contribution of this paper is to lay out a framework towards understanding and improving compositional generalization, and argue that most currently-common training methods fall short.

In wondering how deep networks can learn to compositionally generalize, we might naturally ask: how did humans achieve such generalization? Or, as a particular case, how did human languages evolve components (typically, words) that can systematically combine to form new concepts? This has been a long-standing question in cognitive science and evolutionary linguistics. One promising hypothesis is known as _iterated learning_ (IL), a procedure simulating cultural language evolution . Aspects of this proposal are supported by lab experiments , a Bayesian model , the behavior of neural networks in a simple emergent communication task , and real tasks like machine translation  and visual question answering .

To link the study in cognitive science and deep learning, we first analyze the necessary properties of representations in order to generalize well compositionally. By linking the compositionality and the Kolmogorov complexity, we find iteratively resetting and relearning the representations can introduce compressibility pressure to the representations, which is also the key to the success of iterated learning. To apply iterated learning in a general representation learning problem, we propose to split the network into a backbone and a task head, and discretize the representation at the end of the backbone using _simplicial embeddings_ (SEM, ). This scheme is more practical than LSTM  encoders previously used for neural iterated learning . We observe in various controlled vision domains that SEM-IL can enhance compositional generalization by aligning learned representations to ground-truth generating factors. The proposed method also enhances downstream performance on molecular graph property prediction tasks, where the generating process is less clear-cut.

## 2 Compositional Generalization

Generalization is a long-standing topic in machine learning. The traditional notion of (in-distribution) generalization assumes that training and test samples come from the same distribution, but this is insufficient for many tasks: we expect a well-trained model to generalize to some novel scenarios that are unseen during training. One version of this is _compositional generalization_ (comp-gen) , which requires the model to perform well on novel combinations of semantic concepts.

### Data-generating assumption and problem definition

Any type of generalization requires some "shared rules" between training and test distributions. We hence assume a simple data-generating process that both training and test data samples obey. In Figure 1, the semantic generating factors, also known as latent variables, are divided into two groups: the task-relevant factors (or semantic generating factors) \(=[G_{1},...,G_{m}]\), and task-irrelevant (or noise) factors **O**. This division depends on our understanding of the task; for example, if we only want to predict the digit identity of an image in the color-MNIST dataset , then \(m=1\) and \(G_{1}\) represents the digit identity. All the other generating factors such as color, stroke, angle, and possible noise are merged into **O**. If we want to predict a function that depends on both identity and color, e.g. identifying blue even numbers, we could have \(=[G_{1},G_{2}]\) with \(G_{1}\) the identity and \(G_{2}\) the color.

Each input sample \(\) is determined by a deterministic function \((,)\). The task label(s) \(\) only depend on the factors **G** and possible independent noise \(\), according to the deterministic function \((,)\). Note \((,)\!\!\!(,)\), and that **O**, **G**, and \(\) are independent. The data-generating distribution \(P(,)\) is determined by the latent distributions \(P()\) and \(P()\), along with the \(\) and \(\). We assume \(\) and \(\) are fixed across environments (the "rules of production" are consistent), while \(P()\) and \(P()\) might change between training and test.2

For compositional generalization, we wish to model the problem of generalizing to new combinations of previously seen attributes: understanding "red circle" based on having seen "red square" and "blue circle." Thus, we may assume that the supports of \(P()\) are non-overlapping between train and test. (If this assumption is not true, it only makes the problem easier.) In summary, our goal is to find an algorithm \(\) such that, when trained on a dataset \(_{train} P_{train}^{n}\), \(\) achieves small test risk \(_{P_{test}}((_{train}))\). Here \(P_{train}\) and \(P_{test}\) should satisfy these conditions:

* \(P_{train}\) and \(P_{test}\) have **G**, **O**, \(\) jointly independent, and \(=(,)\), \(=(,)\).
* \(\) and \(\) are the same deterministic functions for \(P_{train}\) and \(P_{test}\).
* In challenging cases, we may have \([P_{train}()][P_{test}()]=\).

### Representation Learning and Ladder of Compositionality

For compositional generalization, we expect that the model must extract atomic semantic features from the training data, and systematically re-combine them in a procedure akin to how the data is generated . We thus consider a typical representation learning framework, which resembles the inverse of the data generation process (Figure 1(a), bottom). We use a backbone \(h:\) to convert the input signal \(\) into a representation \(\), and a task head \(g:\) to solve the given task based on that representation \(\). The prediction of the model is \(}=(g h)()\).

Intuitively, we would like our learned \(\) to uncover the hidden \(\), and \(g()\) to recover \((,)\). We thus analyze how the relationship between \(\) and \(\) influences the model's generalization capability, building off principles such as information bottleneck . Inspired by the "ladder of causation" , we propose a "ladder of compositionality" in Figure 1(b), which outlining a series of conditions on \(\) and \(\). We hypothesize that comp-gen roughly requires reaching the highest rung of that ladder:

**Hypothesis 1**.: _To generalize compositionally, the learned \(\) should capture exactly the information in \(\) and nothing more (\(\) to \(\) should be a bijection), and moreover it should preserve the "structure" of \(\) (i.e. the mapping from \(\) to \(\) should be an isomorphism)._

More on this hypothesis, the ladder, and relationship to models of disentanglement  are discussed in Appendix A. In short, we find that a model trained using common learning methods relying on mutual information between input \(\) and supervision \(\) cannot reliably reach the final stage of the ladder - it is necessary to seek other inductive biases in order to generalize compositionally.

## 3 Compressibility pressure and Compositional mapping

From the analysis above, we need to find other inductive biases to obtain compositional mappings. Inspired by how compositionality emerges in human language,3 we speculate that the _compressibility_ pressure is the key. Note that this pressure does not refer to compressing information from \(\) to \(\) (as in Stage III does), but whether a mapping can be expressed in a compact way by reusing common rules. In this section, we will first link compressibility pressure to Kolmogorov complexity by defining different mappings using group theory. As the Kolmogorov complexity is hard to compute, making explicit regularization difficult, we propose to implicitly regularize via _iterated learning_, a procedure in cognitive science proposed to increase compositionality in human-like language.

### Compositional mappings have lower Kolmogorov complexity

From Occam's razor, we know efficient and effective mappings are more likely to capture the ground truth generating mechanism of the data, and hence generalize better. The efficiency is determined by how compressed the mapping is, which can also be measured by Kolmogorov complexity [71; 47].

Figure 1: Left: the data-generating assumption and a typical representation learning method (in red). We use the model \((g h)()\) for downstream predictions. Right: the ladder of compositionality stating the requirements of \(\) using the entropy-related measurements; see Appendix A for more.

To build a link between compositionality and Kolmogorov complexity, we can first describe different bijections between **z** and **G** using group theory, and then use the description length to compare the complexity of a typical element. Specifically, assuming \(,\) and \(||=||\), the space of all bijections between **z** and **G** is an isomorphism of a symmetric group \(S_{||}\). If \(=[G_{1},...,G_{m}]\) and each \(G_{m}\) has \(v\) different possible values, \(||=v^{m}\). For clarity in the analysis, we assume **z** also has the same shape. Then, any bijection between **z** and **G** can be represented by an element in \(S_{v^{m}}\).

The space of compositional mapping, which is a subset of all bijections, has more constraints. Recall how a compositional mapping is generated (see Appendix A.4 for more details): we first select \(z_{i}\) for each \(G_{j}\) in a non-overlapping way. Such a process can be represented by an element in \(S_{m}\). After that, we will assign different "words" for each \(z_{i}\), which can be represented by an element in \(S_{v}\). As we have \(m\) different \(z_{i}\), this procedure will be repeated \(m\) times. In summary, any compositional mapping can be represented by an element in the group \(S_{v}^{m} S_{v} S_{v^{m}}\), where \(\) is the semidirect product in group theory. The cardinality of \(S_{v^{m}}\) is significantly larger than \(S_{v}^{m} S_{v}\), and so a randomly selected bijection is unlikely to be compositional. Thus

**Proposition 1** (Informal).: _For \(m,v 2\), among all bijections, any compositional mapping has much lower Kolmogorov complexity than a typical non-compositional mapping._

We prove this by constructing descriptive protocols for each bijection. As a compositional mapping has more _reused rules_, its description length can be smaller (see Appendix B.1 for more details).

### Compressibility pressure is amplified in iterated learning

Now, our target is finding bijections with higher compositionality and lower Kolmogorov complexity, which are both non-trivial. Because the ground truth **G** is usually inaccessible and the Kolmogorov complexity is hard to calculate. Fortunately, researchers find that human language also evolved to become more compositional without knowing **G**. Authors of  hypothesize that the _compressibility pressure_, which exists when an innocent agent (e.g., a child) learns from an informed agent (e.g., an adult), plays an important role. Such pressure is reinforced and amplified when the human community repeats this learning fashion for multiple generations.

However, the aforementioned hypothesis assumes that simplicity bias is inborn in the human cognition system. Will deep neural agents also have similar preferences during training? The answer is yes. By analyzing an overparameterized model on a simple supervised learning problem, we can strictly prove that repeatedly introducing new agents to learn from the old agent (then this informed agent becomes the old agent for the next generation) can exert a non-trivial regularizing effect on the number of "active bases" of the learned mapping. Restricting the number of active bases encourages the model to reuse the learned rules. In other words, this regularization effect favors mappings with lower Kolmogorov complexity, which is exactly what we expect for compositional generalization. Due to the space limits, we left the formulation and proof of this problem in Appendix B.2.

### Complete the proposed solution

We thus expect that iteratively resetting and relearning can amplify the compressibility pressure, which helps us to reach the final rung of the ladder from the third. Before that, we need another pressure to reach third rung (i.e., ensure a bijection between **z** and **G**). _Expressivity pressure_, constraining the learned mapping to be capable enough to accomplish the downstream tasks, is what we need.

The complete iterated learning hypothesis of Kirby et al.  claims that the compositional mapping emerges under the interaction between the compressibility pressure (i.e., efficiency) and the expressivity pressure (i.e., effectiveness). Inspired by this, we propose to train a model in generations consisting of two phases. At the \(t\)-th generation, we first train the backbone \(h\) in an _imitation phase_, where a student \(h_{t}^{S}\) learns to imitate **z** sampled from a teacher \(h_{t}^{T}\). As analyzed above, iteratively doing so will amplify the compressibility pressure. Then, in the following _interaction phase_, the model \(g_{t} h_{t}^{S}\) follows standard downstream training to predict **y**. The task head \(g_{t}\) is randomly initialized and fine-tuned together with the backbone in this phase. By accomplishing this phase, the expressivity pressure is introduced. The fine-tuned backbone \(h_{t}^{S}\) then becomes the teacher \(h_{t+1}^{T}\) for the next generation, and we repeat, as illustrated in Figure 2 and Algorithm 1.

Another problem with applying iterated learning to deep neural networks is how to create the discrete message, i.e., **z**. Discretization is not _necessary_: for example, the imitation phase could use \(L_{2}\) loss to match a student's continuous representations to the teacher's. We find greatly improved performance with our discretization scheme, however, due to much-increased compressibility pressure. It is also possible  to use e.g. an LSTM encoder at the end of \(h()\) to produce discrete \(\), and an LSTM decoder at the start of \(g()\). The interaction phase is then not directly differentiable; though many estimator options exist [6; 39; 78], training tends to be difficult due to high bias and/or variance.

Instead, we consider a simplicial embedding layer (SEM, ), which has proven effective on many self-supervised learning tasks. As illustrated in Figure 2(c), a dense representation \(\) (the output of the original backbone) is linearly transformed into \(m\) vectors \(z_{i}^{v}\). Then we apply a separate softmax with temperature \(\) to each \(z_{i}\), yielding \(_{i}\) which are, if the temperature is not too high, approximately sparse; the \(_{i}\) are then concatenated to a long vector \(\). The overall process is

\[_{i}=_{}(z_{i})=[/}}{_{ k=1}^{}e^{z_{ik}/}}]_{j}^{v}= _{1}^{}&&_{m}^{}^{}^{mv}.\] (1)

By using an encoder with a final SEM layer, we obtain an approximately-sparse \(\). In the imitation phase, we generate discrete pseudo-labels by sampling from the categorical distribution defined by each \(_{i}\), then use cross-entropy loss so that the student is effectively doing multi-label classification to reconstruct the teacher's representations. In the imitation phase, the task head \(g\) operates directly on the long vector \(\). The full model \(g h\) is differentiable, so we can use any standard task loss. Pseudocode for the proposed method, SEM-IL, is in the appendix (Algorithm 1).

## 4 Analysis on Controlled Vision Datasets

We will first verify the effectiveness of the proposed SEM-IL method on controlled vision datasets, where the ground truth \(\) is accessible. Thus, we can directly observe how \(\) gradually becomes more similar to \(\), and how the compressibility and expressivity pressures affect the training process. In this section, we consider a regression task on 3dShapes , where recovering and recombining the generating factors is necessary for systematic generalization. The detailed experimental settings and results on additional similar datasets, dSprites  and MPI3D-real , are given in Appendix C.

### The Effectiveness of SEM-IL

Better comp-gen performanceWe first show the effectiveness of the proposed method using results on 3dShapes, containing images of objects with various colors, sizes, and orientations against various backgrounds. Here \(\) numerically encodes floor hue, wall hue, object hue, and object scale into discrete values, and the goal is to recover a particular linear function of that \(\). (Results for a simple nonlinear function were comparable.)

We compare five algorithms:

* Baseline: directly train a ResNet18  on the downstream task.
* SEM-only: insert an SEM layer to the baseline model.
* IL-only: train a baseline model with Algorithm 1, using MSE loss during imitation.
* SEM-IL: train an SEM model with Algorithm 1.
* Given-G: train an SEM model to reproduce the true \(\) (which would not be known in practice), then fine-tune on the downstream task.

Figure 2: An illustration of iterated learning and SEM layer design.

In the first panel of Figure 3, we see that the baseline and SEM-only models perform similarly on the training set; IL-based methods periodically increase in error at the beginning of each generation, but are eventually only slightly worse than the baselines on training data. On the test set, however, evaluating compositional generalization by using values of **G** which did not appear in training, SEM-IL brings significant improvement compared with other methods. Using only SEM or only IL gives no improvement over the baseline, however; it is only their combination which helps, as we will discuss further shortly. The (unrealistic) oracle method Given-G is unsurprisingly the best, since having **z** similar to **G** is indeed helpful for this task.

How z evolves during learningTo see if better generalization ability is indeed achieved by finding **z** that resembles the structure of **G**, we check their topological similarity4

\[(,)(d_{z}(^{( i)},^{(j)}),d_{G}(^{(i)},^{(j)}))\] (2)

where \(d_{z}\) and \(d_{G}\) are distance metrics, \(^{(i)}\) is the predicted representation of \(^{(i)}\), and \(^{(i)}\) is the corresponding ground-truth generating factors. This measurement is widely applied to evaluate the compositionality of the mappings in cognitive science  and emergent communication . Following existing works, we use the Hamming distance for **G** and discretized **z** in SEM-based methods, and cosine distance for continuous **z** in non-SEM methods. We expect \(h()\) to map **x** with similar **G** to close **z**, and dissimilar **x** to distant **z**, so that \((,)\) will be high.

The third panel of Figure 3 shows that the SEM-only model quickly reaches a plateau after 200 epochs and then slowly decreases, while SEM-IL, after briefly stalling at the same point, continues to increase to a notably higher topological similarity. In the last panel, however, the IL-only method doesn't improve \(\) over the baseline: it seems both parts are needed.

### Discretized Representation is Beneficial for the Imitation Phase of IL

To explain why SEM and IL cooperate well, we need to look deeper into how the compressibility pressure influences the learning of representations. This pressure induced by iterated learning, which helps us to find mappings with lower Kolmogorov complexity, leads to representations that are more compositional and systematic . However, in prior works, these mappings were only considered in conjunction with some discretized representation [54; 60]. While IL could be used with continuous representation during the imitation phase, similar to born-again networks , we found that our algorithm benefits a lot from the discretized representations.

To get a clear picture of why discretized representations are so important, we divide \(h()\) into \(m\) sub-mappings \(h_{i}()\), which map \(\) to \(_{i}^{v}\). We can understand each \(_{i}\) as a categorical distribution over \(v\) different possible values. As such, during training, the model learns discrete features of the dataset and assigns confidence about each feature for every sample. The neural network will tend to more quickly learn simpler mappings [5; 24], and will assign higher confidence according to the mapping it has learned. In other words, if a mapping does not align well with **G**, it is more likely to give idiosyncratic learned \(_{i}\), and will lead to low confidence for most samples. On the contrary, \(_{i}\) belonging to compositional mappings will be more general, and on average tend towards higher confidence.

Figure 3: Left: compositional generalization performance on a regression task. Right: topological similarity for IL and non-IL methods. Note the values of \(\) in the two panels are not comparable, as the structure of **z** in the two settings (with or without SEM) is different.

The imitation phase reinforces this bias when the new student learns from the _sampled_ pseudo labels \(_{i}\) from the teacher's prediction \(_{i}\). As such, confident predictions, which are more likely to belong to the compositional mappings, will be learned faster (and harder to forget) by the student. On the contrary, for less confident features where \(P(_{i})\) is flat, \(_{i}\) could change across epochs. This makes it hard for the student to remember any related \((,_{i})\). For example, a student will be reluctant to build a stable mapping between "red" and \(z_{1}\) if the teacher communicates \((\)"red square", \(_{1}=0)\), \((\)"red square", \(_{1}=2)\) in three consecutive epochs.

Furthermore, using the sampled pseudo-labels can help the student to align the learned \((,_{i})\) better. Assume during training, the student already remembers some pairs like \((\)"blue circle", \(_{1}=0)\), \((\)"blue square", \(_{1}=0)\), \((\)"blue star", \(_{1}=0)\), but the teacher is not confident in \((\)"blue apple", \(_{1})\), perhaps because apples are rarely blue. Following the analysis above, as \(P(_{1})\) is flat, the teacher may generate \(_{1} 0\) a significant portion of the time. However, if the teacher happens to generate \(_{1}=0\) at some point, the student would learn \((\)"blue apple", \(_{1}=0)\) faster than those with \(_{1} 0\), because it aligns well with the other information stored in the student network. The parameter updates caused by the learning of other \((\)"blue [shape]", \(_{1}=0)\) will also promote the learning of \((\)"blue apple", \(_{1}=0)\), similar to how "noisy" labels are fixed as described by .

To support the explanations above, we can first observe the correlation between the teacher's confidence and the model's learning speed for \((,_{i})\). Specifically, for each **x**, the teacher makes \(m\) predictions with the corresponding categorical distribution \(_{i}\), \(i[m]\). For each \((,_{i})\), the confidence is measured by the negative logarithm of the teacher's predicted probability, \(-[_{i}]_{j}\) where \(*{argmax}_{j}[_{i}]_{j}\). The learning speed of \((,_{i})\) is measured by the integral of the student's prediction with training time \(t\), i.e., \(_{t=0}[_{i}(t)]_{j}\), where \(j\) is the value provided by the teacher and \(_{i}(t)\) is student's prediction at time \(t\). As illustrated in the first panel of Figure 4, the \((,_{i})\) with higher confidence are usually learned faster by the student.

We also provide the learning curves of \((,_{i})\) with high/intermediate/low confidence (each with 10 samples) in the other three panels of the figure. The curves for high-confidence samples all converge to \([_{i}(t)]_{j}=1\) while those for low-confidence predictions could converge to a value less than 0.3. This means the student might make predictions that are different from the teacher's supervision. By highlighting such low-confidence \((,_{i})\) pairs in the scatter plot, we find they are all low-confidence samples. Another interesting observation from the high-confidence curves is that some \((,_{i})\) pairs are not remembered by the student in the first generation: they emerge at some point and gradually dominate as the training goes on. This phenomenon matches our analysis of how the sampled pseudo-labels help the student align \((,_{i})\) to its knowledge well. To further support this explanation, Appendix C shows that performance is substantially harmed by taking pseudo-labels from the \(*{argmax}\), rather than sampling from \(_{i}\).

To recap, this subsection provided an explanation (along with some supporting evidence) for why the combination of SEM and IL is so important, based on the perspective of sample difficulty, which we believe to be a significant factor in the success of this algorithm.

## 5 Application: Molecular Property Prediction

Given the success in controlled vision examples, we now turn to a real problem where the true generative process is unknown. We focus on predicting the properties of molecular graphs, for several

Figure 4: First panel: correlation between teacher’s confidence and student’s learning speed for each \((,_{i})\); \(_{i}\) is the prediction of the \(l\)-th attribute in imitation phase. “Consistent” means the student makes the same prediction as the teacher. Other panels: learning curves of the student’s predictions.

reasons. First, molecular graphs and their labels might follow a (chemical) procedure akin to that in Figure 1: for instance, one \(G_{i}\) might be the existence of a specific functional group, or the number of specific atoms. Different molecular properties could then be determined by different subsets of \(G_{i}\), as we desired in the compositional generalization problem. Furthermore, the generating mechanisms (GenX and GenY) should be consistent and determined by nature. Second, benchmark datasets in this community contain various types of tasks (e.g., binary classification, multi-label classification, and regression) with similar input signals: performing well on different tasks will broaden the scope of our algorithm. Furthermore, the scaffold split used by most molecular datasets corresponds well to the compositional generalization setup we consider here. (We also try some more challenging splits, using structural information.) Last, learning meaningful representations that uncover the generating mechanisms of molecules is important, of practical significance, and difficult: it can potentially help predict the properties of unknown compounds, or accelerate the discovery of new compounds with specific properties, but scaling based on massive datasets as in recent work on vision or language seems more difficult. We hope our analysis can provide a new perspective on this problem.

### Improvement on the Downstream Performance

We conduct experiments on three common molecular graph property datasets: ogbg-molhiv (1 binary classification task), ogbg-molpcba (128 binary classification tasks), and PCQM4v2 (1 regression task); all three come from the Open Graph Benchmark . We choose two types of backbones, standard GCN  and GIN . For the baseline experiments, we use the default hyperparameters from . As the linear transform added in SEM-based method gives the model more parameters, we consider "baseline+" to make a fair comparison: this model has an additional embedding layer, but no softmax operation. Detailed information on these datasets, backbone models, and hyper-parameters is provided in Appendix D.

From Table 1, we see the SEM-IL method almost always gives the best performance. Unlike in the controlled vision experiments (Figure 3), however, SEM alone can bring significant improvements in this setting. We speculate that compressibility pressure might be more significant in the interaction phase (i.e. standard training) when the generating mechanism is complex. This suggests it may be possible to develop a more efficient algorithm to better impose compressibility and expressivity pressures at the same time.

### Probing Learned \(\) by Meaningful Structures

In the controlled vision examples, we know that SEM-IL not only enhances the downstream performance, but also provides \(\) more similar to the ground-truth generating factors, as seen by the improvement in topological similarity. However, as the generating mechanism is usually inaccessible in real problems, we indirectly measure the quality of \(\) using graph probing . Specifically, we first extract some meaningful substructures in a molecule using domain knowledge. For example, we can conclude whether a benzene ring exists in \(\) by directly observing its 2D structure. With the help of the RDKit tool , we can generate a sequence of labels for each \(\), which is usually known as the "fingerprint" of molecules (denoted \(()\{0,1\}^{k}\), indicating whether each specific structure exists in \(\)). Then, we add a linear head on top of the fixed \(\) and train it using a generated training set \((,()),_{train}\), and compare the generalization performance on the generated test set

   &  &  &  &  &  &  &  &  &  &  &  &  &  \\   & Baseline & 82.41\(\)1.14 & 76.25\(\)30.38 & 75.65\(\)0.91 & 72.32\(\)1.86 & 21.44\(\)0.25 & 22.13\(\)0.46 & 22.13\(\)0.36 & 20.87\(\)0.62 & 0.125\(\)0.002 \\ GCN & Baseline & 81.61\(\)0.63 & 75.58\(\)0.47 & 70.32\(\)0.43\(\)0.75 & 27.17\(\)1.02 & 22.31\(\)0.34 & 22.67\(\)0.30 & 22.61\(\)0.45 & 20.60\(\)0.37 & 0.118\(\)0.004 \\  & SEM-dil & 84.01\(\)0.10 & 74.00\(\)0.67 & 78.41\(\)1.57 & 21.82\(\)3.22 & 22.30\(\)0.46 & 25.90\(\)0.71 & 22.91\(\)**0.21** & **0.129\(\)0.002** \\  & **SEM-IL** & **84.59\(\)0.68** & **7.99\(\)0.76** & **7.84\(\)0.67** & **7.492\(\)0.78** & **2.81\(\)0.72** & **2.75\(\)0.74** & **2.75\(\)0.84** & 21.90\(\)0.81 & **0.102\(\)0.005** \\   & 81.76\(\)1.04 & 89.81\(\)2.42 & 76.59\(\)1.44 & 71.63\(\)2.21 & 22.30\(\)0.32 & 22.62\(\)0.49 & 22.52\(\)0.52 & 20.52\(\)0.49 & 20.18\(\)0.42 & 0.109\(\)0.003 \\ GIN & Baseline & 81.55\(\)0.72 & 77.01\(\)0.94 & 94.77\(\)1.62 & 69.73\(\)1.30 & 22.58\(\)0.32 & 22.91\(\)0.40 & 22.10\(\)0.21 & 20.25\(\)0.42 & 0.108\(\)0.003 \\  & SEM-dil & 83.05\(\)0.90 & 78.21\(\)0.78 & 76.29\(\)0.66 & 72.70\(\)0.49 & 26.01\(\)0.52 & 25.66\(\)0.47 & 22.26\(\)0.39 & 21.50\(\)0.48 & 0.106\(\)0.004 \\  & **SEM-IL** & **83.32\(\)1.51** & **78.61\(\)0.73** & **78.06\(\)1.24** & **72.89\(\)0.48** & **2.90\(\)0.48** & **2.80\(\)0.61** & **24.41\(\)0.47** & **23.89\(\)0.77** & **0.098\(\)0.005** \\  

Table 1: Downstream performance on different tasks. The numbers of AUROC and average precision are in percent form. For PCQM, we report the validation performance, as the test set is private and inaccessible. Means and standard deviations of 5 seeds are given. Valid/test-full means the standard train/val/test split provided by the dataset. Valid/test-half means we train the model on half of the training data which is _less similar_ to the validation and test sets. See Appendix D for more.

\((,()),_{test}\). For fair comparison, we set \(m=30\) and \(v=10\) to make \(\) and \(\) be the same width, excluding the influence of the linear head's capacity.

In the experiments, we use the validation split of molhiv as \(_{train}\) and the test split as \(_{test}\), each of which contain 4,113 distinct molecules unseen during the training of \(\). The generalization performance of ten different substructures is reported in Table 2. The first block (first two rows) of the table demonstrates the performance of two types of models before training. They behave similarly across all tasks and give a higher AUROC than a random guess. Then, comparing the three algorithms in each block, we see SEM-based methods consistently outperform the baseline, which supports our hypothesis well. SEM-IL outperforms SEM-only on average, but not for every task; this may be because some structures are more important to the downstream task than others.

Comparing the results across the four blocks, we find that the task in the interaction phase also influences the quality of \(\): the \(\) trained by molpcba is much better than those trained by molhiv. To figure out where this improvement comes from, we first use only 10% of the training samples in molpcba to make the training sizes similar, then make the supervisory signal more similar by using only one task from molpcba. As illustrated in the last two blocks in the table, we can conclude that the complexity of the task in the interaction phase, which introduces the expressivity pressure, plays a more important role in finding better \(\).

Based on this observation, we can improve SEM-IL by applying more complex interaction tasks. For example, existing works on iterated learning use a referential game or a reconstruction task in the interaction phase, which could introduce stronger expressivity pressure from a different perspective. Furthermore,  demonstrates that SEM works well with most contrastive learning tasks. We hope the fundamental analysis provided in this paper can shed light on why SEM and IL collaborate so well and also arouse more efficient and effective algorithms in the future.

## 6 Related Works

**Iterated Learning and its Applications.** Iterated learning (IL) is a procedure that simulates cultural language evolution to explain how the compositionality of human language emerges . In IL, the knowledge (i.e., the mapping between the input sample and its representation) is transferred between different generations, during which the compositional mappings gradually emerge and dominate under the interaction between compressibility and expressivity pressures. Inspired by this principle, there are some successful applications in symbolic games , visual question answering , machine translation , multi-label learning , reinforcement learning , etc.

There are also many algorithms training a neural network for multiple generations, which could possibly support the principles proposed in iterated learning. For example,  proposes to iteratively distill the downstream logits from the model in the previous generation, and finally bootstrap all the models to achieve better performance on image classification task; this can be considered as an IL algorithm merging the imitation and interaction phases together.  proposes to re-initialize the latter layers of a network and re-train the model for multiple generations, which is similar to an IL algorithm that only re-initializes the task head.  extends such a reset-and-relearn training to reinforcement learning and shows that resetting brings benefits that cannot be achieved by other regularization methods such as dropout or weight decay. In the era of large language models, self-refinement in-context learning  and self-training-based reinforcement learning  can also

   &  &  &  &  &  &  &  &  &  &  \\   &  & 0.870 & 0.958 & 0.811 & 0.629 & 0.959 & 0.615 & 0.627 & 0.706 & 0.692 & 0.812 & 0.732 \\  &  & 0.872 & 0.958 & 0.812 & 0.635 & 0.597 & 0.638 & 0.613 & 0.692 & 0.683 & 0.815 & 0.731 \\  Train & Baseline & 0.874 & 0.948 & 0.916 & 0.700 & 0.717 & 0.694 & 0.804 & 0.740 & 0.703 & 0.913 & 0.801 \\ on & SEM-ML & 0.893 & **0.899** & 0.938 & 0.722 & 0.517 & 0.729 & 0.823 & 0.763 & 0.763 & 0.918 & 0.836 \\ Molhiv & **SEM-ML** & **0.907** & 0.980 & **0.967** & **0.781** & **0.801** & **0.794** & **0.903** & **0.815** & **0.869** & **0.965** & **0.878** \\  Train & Baseline & 0.921 & 0.938 & 0.968 & 0.866 & 0.875 & 0.835 & 0.875 & 0.855 & 0.956 & 0.968 & 0.901 \\ on & SEM-ML & **0.942** & **0.991** & 0.981 & 0.888 & 0.916 & **0.854** & **0.921** & 0.888 & 0.897 & 0.980 & 0.926 \\ Molpcba & SEM-ML & 0.940 & 0.948 & 0.982 & **0.910** & **0.931** & 0.849 & 0.912 & **0.910** & **0.921** & **0.981** & **0.931** \\  Train & Baseline & 0.923 & 0.906 & 0.962 & 0.835 & 0.837 & 0.832 & 0.870 & 0.833 & 0.864 & 0.962 & 0.895 \\ on & SEM-ML & **0.943** & 0.993 & 0.989 & 0.872 & 0.906 & 0.835 & 0.913 & **0.876** & 0.900 & **0.989** & 0.922 \\
100+ \(_{}\) & SEM-ML & 0.938 & **0.994** & 0.975 & **0.891** & **0.918** & **0.847** & **0.927** & 0.874 & **0.907** & **0.985** & **0.927** \\  Train & Baseline & 0.927 & 0.947 & 0.948 & 0.723 & 0.750 & 0.689 & 0.845 & 0.758 & 0.782 & 0.947 & 0.831 \\ on & SEM-ML & 0.906 & **0.899** & 0.958 & **0.722** & 0.899 & 0.725 & 0.876 & **0.720** & 0.835 & 0.957 & 0.861 \\ pcb-1-task & **0.906** & 0.988 & **0.963** & 0.741 & **0.851** & **0.744** & **0.887** & 0.765 & **0.869** & **0.962** & **0.867** \\  

Table 2: AUROC for graph probing based on different \(\); random guessing would be \( 0.5\).

benefit from iteratively learning from the signals generated by agents in the previous generation. We left the discussion and analysis on these more complex real systems in our future work.

**Knowledge Distillation and Discrete Bottleneck.** Broadly speaking, the imitation phase in SEM-IL, which requires the student network to learn from the teacher, can be considered as a knowledge distillation method . Different from the usual setting, where the student learns from the teacher's prediction on a downstream task, we assume a data-generating mechanism and create a simplex space for the generating factors. By learning from the teacher in this space, we believe the compressibility pressure is stronger and is more beneficial for the compositional generalization ability.

For the discretization, there are also other possible approaches, e.g.,  uses an LSTM to create a discrete message space, and  proposes a method using a vector quantized bottleneck . We choose SEM  for its simplicity and universality: it is easy to insert it into a model for different tasks. Besides, SEM has proved to be effective on self-supervised learning tasks; we extend it to classification, regression, and multi-label tasks.

**Compressibility, learning dynamics, and Kolmogorov complexity** Recently, with the success of large language models, the relationship between compressibility and generalization ability gradually attracted more attention . Authors of  propose that how well a model is compressed corresponds to the integral of the training loss curve when negative logarithmic likelihood loss is used. Although this claim assumes the model sees each training sample only once, which might not be consistent with the multiple-epochs training discussed in this paper, the principles behind this claim and our analysis are quite consistent: the mappings generalize better and are usually learned faster by the model. Furthermore, authors of  link the generalization ability to Kolmogorov complexity. Our analysis in Appendix B also supports this claim well. Hence we believe the evolution of the human cognition system can provide valuable insights into deep learning systems.

**Graph Representation Learning.** Chemistry and molecular modeling are some of the main drivers of neural graph representation learning since its emergence  and graph neural networks, in particular. The first theoretical and practical advancements [27; 40; 80] in the GNN literature were mostly motivated by molecular use cases. Furthermore, many standard graph benchmarks [15; 16; 37] include molecular tasks on node, edge, and graph-levels, e.g., graph regression in ZINC and PCQM4Mv2 or molecular property prediction in ogbg-molhiv and ogbg-molpcba datasets. Graph Transformers [14; 43; 59] exhibit significant gains over GNNs in molecular prediction tasks. Self-supervised learning (SSL) on graphs is particularly prominent in the molecular domain highlighted by the works of GNN PreTrain , BGRL , and Noisy Nodes . We will extend the proposed method to different models and different pretraining strategies in our future work.

## 7 Conclusion

In this paper, we first define the compositional generalization problem by assuming the samples in the training and test sets share the same generating mechanism while the generating factors of these two sets can have different distributions. Then, by proposing the compositionality ladder, we analyze the desired properties of the representations. By linking the compositionality, compressibility, and Kolmogorov complexity together, we find iterated learning, which is well-studied in cognitive science, is beneficial for our problem. To appropriately apply iterated learning, we attach an SEM layer to the backbone model to discretize the representations. On the datasets where the true generating factors are accessible, we show that the representations learned by SEM-IL can better portray the generation factors and hence lead to better test performance. We then extend the proposed algorithm to molecular property prediction tasks and find it improves the generalization ability.

The main drawback of the current solution is the time-consuming training: we must run multiple generations and some common features might be re-learned multiple times, which is inefficient. Hence a more efficient way of imposing compressibility is desired.

Overall, though, our analysis and experiments show the potential of the SEM-IL framework on compositional generalization problems. We believe a better understanding of where the compressibility bias comes from in the context of deep learning can inspire more efficient and non-trivial IL framework designs. Clearly defining the compositional generalization problem and finding more related practical applications can also promote the development of IL-related algorithms.