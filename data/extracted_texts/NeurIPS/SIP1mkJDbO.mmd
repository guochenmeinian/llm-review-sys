# AlphaFold Meets Flow Matching

for Generating Protein Ensembles

 Bowen Jing, Bonnie Berger, Tommi Jaakkola

CSAIL, Massachusetts Institute of Technology

bjing@mit.edu, {bab, tommi}@csail.mit.edu

###### Abstract

Recent breakthroughs in protein structure prediction have pointed to structural ensembles as the next frontier in the computational understanding of protein structure. At the same time, iterative refinement techniques such as diffusion have driven significant advancements in generative modeling. We explore the synergy of these developments by combining AlphaFold and ESMFold with flow matching, a powerful modern generative modeling framework, in order to sample the conformational landscape of proteins. When trained on the PDB and evaluated on proteins with multiple recent structures, our method produces ensembles with similar precision and greater diversity compared to MSA subsampling. When further fine-tuned on coarse-grained molecular dynamics trajectories, our model generalizes to unseen proteins and accurately predicts conformational flexibility, captures the joint distribution of atomic positions, and models higher-order physiochemical properties such as intermittent contacts and solvent exposure. These results open exciting avenues in the computational prediction of conformational flexibility.

## 1 Introduction

The success of AlphaFold2  at the 14th Critical Assessment of Structure Prediction (CASP) marked a watershed moment in the computational understanding of protein structure. Since then, predicting multiple conformational states of proteins has emerged as the next frontier towards the ultimate aim of understanding protein _function_ from sequence . Significant conformational changes are critical in the function of transporters, channels, enzymes, motor proteins, receptors, and many other proteins, and even relatively static proteins experience thermal fluctuations whose properties are implicated in the strength and selectivity of molecular interactions. Hence, a generative model of protein structure which builds upon the level of accuracy of single-structure predictors, but reveals conformational heterogeneity, would be of great value to computational structure biology.

In this work, we combine highly accurate protein structure prediction models such as AlphaFold2  and ESMFold  with _flow matching_, a powerful modern generative modeling framework, in order to sample the conformational landscape of proteins. Structure prediction models have been trained as _regression_ models based which predict a single protein structure for a given (sequence or MSA) input. Our synthesis relies on the key insight that modern iterative generative modeling frameworks such as diffusion and flow-matching provide a general recipe to convert such regression models to become _generative models_ of protein structure with relatively little modification to the architecture and training objective. Thus, by fine-tuning structure prediction models on the generative modeling objectives, we obtain powerful and accurate distributional models of protein structure. In particular, while existing approaches to generating structural ensembles have focused on modifying the MSA input via subsampling , mutagenesis , or clustering , our method generates ensembles without resorting to such input ablation techniques and thus can be equally applied to language-model based structure predictors such as ESMFold.

We investigate the performance of our flow-matching variants of AlphaFold2 and ESMFold (named AlphaFlow and ESMFlow) in two settings. First, we aim to predict diverse yet accurate ensembles for recently deposited proteins in the PDB. Second, we exploit the ability to train on ensembles beyond the PDB to model conformational ensembles from coarse-grained molecular dynamics. In both settings, our model successfully generates a diverse set of structures and in the latter case accurately recapitulates physiochemical properties of the ground truth ensembles.

## 2 Method

### Flow Matching

_Flow matching_[15; 4; 3] is a recently proposed generative modeling paradigm that resembles and builds upon the significant success of diffusion models [10; 19] in generative modeling of images and molecules. The fundamental object in flow matching is a conditional probability path \(p_{t}(_{1}),t\): a family of densities conditioned on a data point \(_{1} p_{}\) which interpolates between a common prior distribution \(p_{0}(_{1})=q()\) and an approximate Dirac \(p_{1}(_{1})(-_{1})\). Given a conditional vector field \(u_{t}(_{1})\) that generates the time evolution of this conditional probability path, one then regresses against the _marginal vector field_ with a neural network:

\[(,t;) v(,t):=_{_{ 1} p_{t}(_{1})}[u_{t}(_{ 1})]\] (1)

At convergence, the learned vector field \((,t;)\) is a neural ODE that evolves the prior distribution \(q()\) to the data distribution \(p_{}()\). Score-matching in diffusion models can be seen as a special case of flow matching; however, as discussed in Appendix A.1, flow matching for protein structure circumvents many difficulties that would otherwise arise with diffusion.

Designing a flow-matching generative framework amounts to the choice of a conditional probability path and its corresponding vector field. Inspired by the interpolant-based perspective on flow matching , we define the conditional probability paths via the sampling process

\[_{1}=(1-t)_{0}+t_{1}, _{0} q(_{0})\] (2)

i.e., by sampling a noise from the prior \(q()\) and interpolating linearly with the data point. This conditional probability path is associated with the conditional flow field

\[u_{t}(_{1})=(_{1}-)/(1-t)\] (3)

which resembles the field proposed in  with \((t)=1-t\). Customarily, we then learn a neural network to approximate the marginal vector field according to Equation 1. However, a reparameterization \((,t;)=(}_{1}(,t;)- )/(1-t)\) reveals that we can equivalently learn the expectation of \(_{1}\):

\[}_{1}(,t;)_{_{1}  p_{t}(_{1})}[_{1}]\] (4)

This reparameterization is very similar to those employed for image diffusion models . Crucially, since \(_{1}\) refers to samples from the data distribution, this allows the use of models that predict the data itself, i.e., _protein structures_, to be easily adapted in a flow-matching framework.

Figure 1: **Conceptual overview of AlphaFlow / ESMFlow.** (A) Samples are drawn from a harmonic (polymer-structured) prior. (B) The structure is progressively refined under a neural ODE (C) At each step, the AlphaFold2 or ESMFlow prediction parameterizes the direction of the flow. (D) The final structure is a sample from the predicted distribution of structures.

### Flow Matching for Protein Ensembles

To apply flow matching to protein ensembles, we consider a protein structure to be described by the 3D coordinates of its \(\)-carbons (\(\)-carbon for glycine): \(^{N 3}\). We then choose the prior distribution \(q()\) over the positions of these \(\)-carbons to be a _harmonic prior_ defined by

\[q()[-_{i=1}^{N-1}_{i}-_{i+1}^{2}]\] (5)

This prior distribution ensures that the samples along the conditional probability path, and hence inputs to the neural network \(}_{1}\), always remain polymer-like, physically plausible 3D structures. To parameterize the neural network \(}_{1}(,t;)\), we can directly use the AlphaFold2 and ESMFold protein structure prediction models if we modify them to accept an _input structure_\(\) corresponding to the state at time \(t\), in addition to the protein sequence or MSA. Not coincidentally, this is reminiscent of the idea of _template structures_ employed by certain variants of AlphaFold2. Thus, we develop an input embedding module very similar to AlphaFold2's template embedding stack (detailed in Appendix A) and fine-tune both AlphaFold2 and ESMFold with this additional module and input.

The parameterization of learning the conditional expectation of \(_{1}\) (Equation 4) suggests that the neural network should be trained with an \(L_{2}\)-loss, i.e., MSE. However, there are several issues with this direct approach. (1) The structure prediction networks not only predict \(\)-carbon coordinates, but also all-atom coordinates and residue frames. (2) The input to the network is \(SE(3)\)-invariant by design, and thus the network can distinguish outputs only up to the action of \(SE(3)\), which would occur arbitrarily high MSE loss. (3) The networks obtain best performance (and were orginally trained) with the \(SE(3)\)-invariant Frame Aligned Point Error (FAPE) loss.

To reconcile these issues with the flow-matching framework, we redefine the space of protein structures to be the _quotient_ space \(^{3 N}/SE(3)\), with the prior distribution projected to this space. We redefine the interpolation between two points in this space to be linear interpolation in \(^{3}\) after RMSD-alignment. Further, because the quotient space is no longer a vector space, there is no longer a notion of "expectation" of a distribution; instead, we aim to learn the more general Frechet mean of the conditional distribution \(p(_{1})\):

\[}_{1}(,t;)_{}_{1}} ^{2}(_{1},}_{1})\,dp_{t}( _{1})\] (6)

where we have leveraged the convenient property that FAPE is a valid metric . This regression target gives rise to a training loss identical to the original FAPE, except now _squared_. The squaring is in fact essential to the ability to model multiple conformations: the original AlphaFold2 preferentially models a single conformation rather than a mixture of multiple correct conformations, which is a direct consequence of the triangle inequality respected by FAPE. With these modifications, the final result for the training and inference procedures are detailed in Appendix A. While the changes are technical deviations from the direct flow matching framework, they are appropriate adaptations to the problem domain and we find them to work well in practice.

## 3 Experiments

We fine-tune both AlphaFold2 and ESMFold on the PDB with our flow-matching framework. We use the OpenFold  implementation of AlphaFold2 and the original CASP14 weights, as well as OpenProteinSet  for training MSAs. We use the January 2023 snapshot of the PDB and a training cutoff of May 1, 2018 and May 1, 2020 for AlphaFold2 and ESMFold. The models are respectively tuned for 1.28M and 720k training examples on the PDB with 40% sequence clustering, crops of size 256, a batch size of 64, and no templates. Training and inference (including the untuned models for comparison) are done without recycling. We use 10 inference steps in all experiments.

### PDB Conformational States

We first validate the ability of AlphaFlow and ESMFlow to sample distinct conformational states of proteins deposited in the Protein Data Bank (PDB). We collect all proteins which (1) are represented by 2-30 chains deposited after the AlphaFlow training cutoff, (2) have lengths between 256-768residues, (3) have at least two structural clusters when the chains are clustered with a threshold of 0.85 symmetrized IDDT-C\(\) and complete linkage. From the resulting 563 proteins (represented by 2843 chains), we subsample 100 proteins (represented by 500 chains) to form the test set. For each protein, we sample 50 predictions with AlphaFlow/ESMFlow, unmodified AlphaFold/ESMFold, and varying degrees of MSA subsampling using the UniProt reference sequence. Each set of predictions is evaluated on three metrics for each protein: **precision**--the average similarity from a prediction to the closest crystal structure; **recall**--the average similarity from a crystal structure to the closest prediction; and **diversity**--the average dissimilarity between pairs of prediction. Precision and recall are measured with symmetrized IDDT-C\(\) and diversity is measured with \(1-\)IDDT-C\(\).

The median results across the 100 test targets are shown in Figure 2, _left_. AlphaFlow, similar to MSA subsampling, significantly increases the prediction diversity relative to the unmodified AlphaFold at the cost of reduced precision. Somewhat surprisingly, however, neither AlphaFlow nor MSA subsampling manages to meaningfully improve recall, showing that these methods generally do not succeed in increasing the coverage of experimentally determined structures relative to the baseline AlphaFold. Nonetheless, we highlight that compared to MSA subsampling, AlphaFlow obtains similar recall but significantly higher diversity at the same level of precision. Figure 2, _right_ offers an explanation for this phenomenon: because MSA subsampling is an input _ablation_ technique, the sampled conformations become more diverse but also drift away from the true structures as the input--and hence predictive signal--is increasingly ablated. In contrast, the AlphaFlow predictions are obtained from the full input and remain clustered around the ground truth conformations while reaching the same or greater levels of diversity.

### Molecular Dynamics Ensembles

Unlike inference-time approaches to conformational sampling, our generative modeling framework opens up the possibility of training on ensemble data from beyond the PDB to enable a more comprehensive modeling of protein flexibility. To assess this capability, we tune and evaluate AlphaFlow on protein ensembles from coarse-grained molecular dynamics simulations. Because of the lack of standardized large-scale protein MD data, we construct our own dataset of coarse-grained simulations of 1060 medium-sized proteins (128-256 residues) with no more than 50% sequence similarity. Each protein is simulated with the MARTINI force field  and Go contact restraints 

Figure 2: **Comparison of AlphaFlow vs MSA subsampling on PDB conformational states.**_Left_: precision-recall and precision-diversity curves for the benchmarked methods and varying degrees of MSA subsampling. Metrics are reported in terms of symmetrized IDDT-C\(\). _Right_: PCAs of two example targets, showing the experimental structures (\(\)) along with samples from AlphaFlow and MSA subsampling. Units are in Ã… RMSD.

for 2.5 \(\)s with snapshots saved every 100 ps, yielding 25k frames per protein. Training, validation, and test splits of 893/39/99 proteins are defined based on cutoff dates of May 1, 2018 and May 1, 2019 respectively. Further dataset and simulation details in Appendix B. Finally, after all-atom backmapping with GenZProt , we fine-tune AlphaFlow (i.e., continuing from the model evaluated on PDB ensembles) for an addition 39k training examples.

To evaluate the models, we sample 250 predictions from AlphaFlow, ESMFlow, and varying degrees of MSA subsampling with AlphaFold for each of the 99 test targets. We then compare these predicted ensembles with to the ground-truth MD ensemble on several metrics of increasing complexity (detailed definitions in Appendix B). We summarize the results (Table 1) as follows:

* Variation in the AlphaFlow ensembles are quantitatively predictive of both global and residue level flexibility (ensemble diversity, RMSF). In contrast, MSA subsampling reveals epistemic uncertainty but does not accurately reflect true physical flexibility.
* The AlphaFlow ensembles are distributionally more accurate, both when the atomic position distributions are considered independently and when considered jointly (projected to the top 2 principal axes). In fact, in 25% of the ensembles the principal component of variation of the AlphaFlow ensemble has \(>0.5\) cosine similarity with the MD principal component.
* MD ensembles are typically intended for downstream analysis of observables such as contacts and solvent exposure . To probe if we model these higher-order properties accurately, we identify the set of _weak contacts_ in our predicted ensembles and compare the (1) Jaccard similarity with the ground truth set and (2) the correlation with the ground truth contact probability. We repeat the analysis with the identification of _exposed residues_--those which are buried in the crystal structure but become exposed to solvent in the simulation--and their exposure probability. (Such residues are a key feature in the identification of cryptic pockets .) In both cases, AlphaFlow improves significantly over MSA subsampling.

## 4 Conclusion

We have presented AlphaFlow and ESMFlow, which combine AlphaFold2 and ESMFold with flow-matching towards the goal of sampling protein ensembles. We do so via a minimally invasive fine-tuning of existing model weights, retaining the powerful architectures and pretraining investment of these highly-accurate structure predictors. Compared to existing approaches for obtaining multiple structure predictions, our method is the first to go beyond _inference-time_ input modifications, is applicable to single-sequence structure predictors such as ESMFold, and takes the first steps towards a more principled _training-time_ approach to modeling structural diversity. Hence, our approach represents an important step towards more comprehensive modeling of protein structure and function.

    & & &  \\   & & AlphaFlow & 16 & 64 & 256 & AlphaFold \\   & Ensemble diversity \(\) & **0.72** & 0.29 & 0.41 & 0.35 & 0.29 \\  & Global RMSF \(\) & **0.81** & 0.35 & 0.52 & 0.55 & 0.52 \\  & Per-target RMSF \(\) & **0.86** & 0.50 & 0.62 & 0.66 & 0.66 \\   & Atomic \(_{2}\)-distance & **1.90** & 4.08 & 2.50 & 2.39 & 2.40 \\  & MD PCA \(_{2}\)-distance & **0.78** & 1.11 & 1.03 & 1.06 & 1.05 \\  & Joint PCA \(_{2}\)-distance & **1.80** & 4.06 & 2.32 & 2.27 & 2.28 \\  & \% PC-sim \(>0.5\) & **25** & 4 & 5 & 3 & 4 \\   & Weak contacts \(J\) & **0.54** & 0.25 & 0.17 & 0.11 & 0.07 \\  & Weak contacts \(\) & **0.70** & 0.56 & 0.63 & 0.64 & 0.63 \\   & Exposed residue \(J\) & **0.57** & 0.33 & 0.28 & 0.22 & 0.20 \\   & Exposed residue \(\) & **0.82** & 0.50 & 0.47 & 0.40 & 0.37 \\   

Table 1: **Evaluation on MD ensembles**. For each method, we compare the predicted ensemble with the ground truth MD ensemble according to various metrics (detailed in Appendix B). When applicable, the median across proteins is reported. \(\): Spearman correlation; \(J\): Jaccard similarity.