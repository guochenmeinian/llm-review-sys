# The Best of Both Worlds in Network Population Games: Reaching Consensus & Convergence to Equilibrium

The Best of Both Worlds in Network Population Games: Reaching Consensus & Convergence to Equilibrium

Shuyue Hu

Shanghai Artificial Intelligence Laboratory

hushuyue@pjlab.org.cn

&Harold Soh

National University of Singapore

harold@comp.nus.edu.sg

&Georgios Piliouras

Singapore University of Technology and Design

georgios@sutd.edu.sg

###### Abstract

Reaching consensus and convergence to equilibrium are two major challenges of multi-agent systems. Although each has attracted significant attention, relatively few studies address both challenges at the same time. This paper examines the connection between the notions of consensus and equilibrium in a multi-agent system where multiple interacting sub-populations coexist. We argue that consensus can be seen as an intricate component of intra-population stability, whereas equilibrium can be seen as encoding inter-population stability. We show that smooth fictitious play, a well-known learning model in game theory, can achieve both consensus and convergence to equilibrium in diverse multi-agent settings. Moreover, we show that the consensus formation process plays a crucial role in the seminal thorny problem of equilibrium selection in multi-agent learning.

## 1 Introduction

Two fundamental problems of multi-agent systems are _consensus formation_[26; 3; 12; 30; 5; 28; 86; 68] and _learning in games_[48; 33; 66; 58; 57; 70; 71; 13; 55]. The former (sometimes also referred to as cooperative control, flocking, synchronization, and social norms) aims to understand how a decentralized system of agents, which starts off in an initially disordered state, is able to develop shared coordinated beliefs, opinions, or behaviors . The latter (also known as multi-agent learning) examines whether, how and what type of equilibrium arises as the result of the long-run learning processes of individual agents . Each problem has attracted significant interest, yet, these two problems are largely studied in isolation.

Is there a unified way to think about consensus formation and learning in games? We argue that _consensus_ and _equilibrium_, the fundamental notions of these two fields, can be both understood as _stability_ concepts in a multi-agent system where there co-exist multiple interacting sub-populations (or populations hereafter). Consensus can be seen as an intricate component of _intra_-population stability that reflects whether each population is internally cohesive, i.e., a population is stable if all the agents belonging to this population achieve consensus and develop the same belief, opinion, or behavior. In contrast, game-theoretic (e.g., Nash) equilibrium can be seen at a more macro level as encoding _inter_-population stability, i.e., the stability between different populations is achieved by alignment to their population-wise incentives. Namely, a stable system of populations is characterized by the internal cohesion of each population as well as the game-theoretic equilibration between different populations. The merits of this way of thinking are twofold. It presents a non-trivial connection between consensus and equilibrium, bringing them together under the same umbrella of stability concepts for a system of populations. Moreover, it distinguishes between intra-population stability and inter-population stability so that the emerging phenomena that it can describe are significantly richer, and that it allows for important distinctions that would have otherwise been impossible.

In the rich literature on consensus formation, the idea of building up a connection to game-theoretic equilibration is not entirely new. Classic mathematical models, such as the Degroot model  and the Friedkin-Johnsen model , showed that consensus of opinions can be reached if individuals form their opinions taking a weighted average of their own opinions and the opinions of others. In seminal work, Bindel et al.  showed that such a repeated averaging process is equivalent to a best-response play in the game with a quadratic reward function measuring the distance between individuals' opinions, leading to interest in understanding consensus from the perspective of learning in games . In parallel, Marden et al.  demonstrated how the cooperative control problem of consensus can be formulated as better reply with inertia dynamics in potential games, while Young  proved the emergence of social norms using evolutionary game theory. These notable advances, however, typically assumed the reward structure to encode an incentive to align individual agents' opinions or behaviors with those of others. Little is formally known about consensus formation in the absence of such a coordinative reward structure, or whether such a coordinative reward structure is a prerequisite for achieving both consensus formation and game-theoretic equilibration.

In the literature on multi-agent learning, the concept of consensus has been less explored compared to the abundant study of convergent or non-convergent learning behaviors in games . We argue that consensus is of crucial importance to multi-agent learning--it ensures not only intra-population stability when there co-exist multiple interacting populations but also complete predictability and computational efficiency in more general settings. Imagine the case that for a system of agents, the average probability (\(_{A}\) and \(_{B}\)) of choosing between two strategies (\(A\) and \(B\)) is shown to equilibrate at \(_{A}=_{B}=0.5\). If consensus is reached, it is completely predictable that every agent plays \(A\) and \(B\) with equal probability \(0.5\). However, if consensus is not reached, there are many possibilities for agents' behaviors, to name a few--it is possible (i) that exactly half of the agents play \(A\) with probability \(1\) and the other half plays \(B\) with probability \(1\), and (ii) that some agents play \(A\) and \(B\) with equal probability \(0.5\) whereas the rest constantly switches between these two strategies. The unpredictability in agents' behaviors can be problematic for system design, especially in safety-critical scenarios, where a system designer ought to rule out the possibility of getting the system to evolve into some (e.g., unsafe) states . From the perspective of computational efficiency, consensus is usually a desirable property and is embedded in the mean field game literature to simplify the characterization of equilibrium--by exploiting the homogeneity of the population, it suffices to understand the behavior of a single representative agent facing the mean field .

Putting these together motivates our central questions:

* _Are there natural multi-agent learning models that can achieve the best of both worlds--reaching consensus as well as convergence to equilibrium--in diverse settings?_
* _How does the consensus formation process affect equilibrium selection in multi-agent learning?_

**Our Model.** We provide an affirmative answer to the first question, and our solution is--_smooth fictitious play_ (SFP) --a well-known belief-based learning model in game theory. In this paper, we consider a _network of populations_, where each vertex represents a population (or continuum) of agents and each edge denotes that agents from connected populations are paired up to play 2-player subgames. We call this type of game a network population game,1 as it has a close connection to the network (or graphical polymatrix) games  and the population games . By SFP, each agent forms beliefs about the strategies of agents of the connected populations based on the average plays of those populations, and responds with an aggregated smoothed best response. To allow for a meaningful study of consensus formation, we assume that agents within each population have different beliefs and different behaviors in the initial state. In other words, our model is a _heterogeneous belief_ model , unlike the homogeneous belief models extensively studied in the SFP literature .

Our Approach & Key Results.To see whether a consensus is reached, one needs a measure of it. A natural approach is to assume the system state to be a probability measure (or distribution) over the set of beliefs (referred to as the belief distribution). This notion of a system state allows us to quantify how much agents reach a consensus by examining the _variance_ of the belief distribution. Intuitively, a value of zero for the variance means that agents have the same belief and have achieved consensus. However, analyzing the evolution of the belief distribution requires new techniques beyond the classic approach of stochastic approximation . We address this challenge by establishing a system of nonlinear partial differential equations--_continuity equations2_--to describe the continuous-time change in the belief distribution (Proposition 1). This approach allows us to analyze the evolution of the variance without deriving an analytical form of the belief distribution. We show that for _any_ network population games, the variance of beliefs decreases to zero at a quadratic rate with time in every population (Theorem 1). This means that even if the 2-player subgames that agents (of different populations) play are not coordinative but zero-sum, consensus will still be reached. Put differently, a coordinative reward structure is not a prerequisite for achieving consensus.

Figure 1: System evolution in 2-population stag hunt games. Over time, the belief distributions become more concentrated (left panel: the predictions of our continuity equations, middle panel: the empirical distributions in agent-based simulations), and the variance of beliefs and the distance (L1 norm) to the equilibrium are both decreasing (right panel). Even though the consensus formation process starts with the same initial mean belief, a small variance of initial beliefs results in the risk dominant equilibrium \((H,H)\) (subplot (b)), whereas a large variance of initial beliefs results in the payoff dominant equilibrium \((S,S)\) (subplot (a)). A video for the evolution of these two belief distributions is available at _https://sites.google.com/view/shuyue-hu_.

The next step is to examine the convergence to equilibrium. Our finding that the variance of beliefs decreases to zero suggests that the asymptotic behavior of a population that starts with different beliefs can be understood by studying that of a single representative agent. Inspired by this, we temporarily override the assumption of the heterogeneous belief model and assume a single representative agent for each population, resulting in the classic form of network games with each vertex representing a single agent. Under this assumption, we use Lyapunov arguments to prove the convergence of SFP to quantal response equilibria (QRE)3 in two classes of network games that capture network competition and network coordination. Specifically, we show that for a weighted zero-sum network game, SFP converges to a unique QRE even if the underlying game has many distinct Nash equilibria (Theorem 2). We also show that for an exact potential network game that exhibits a star structure, SFP converges to the set of QRE (Theorem 3). We then use the techniques of asymptotically autonomous dynamical systems to show that the convergence to QRE holds for network population games as well (Theorem 4 and Theorem 5). Therefore, SFP converges to QRE in all weighted zero-sum network (population) games as well as all exact potential star-graph (population) games.

At this point, we have shown that SFP can achieve consensus as well as convergence to equilibrium in a wide range of network population games. Specifically, consensus is always reached, regardless of the specific setting. In the language of stability concepts, by SFP, the intra-population stability is always guaranteed, but the inter-population stability may require additional assumptions about the incentive nature as well as the network topology. This may lead to an error-prone intuition that the consensus formation process is orthogonal to the equilibrium selection process and thus has little effect on it. This is indeed _not_ true. We illustrate this through an example of a 2-population stag hunt game, where different variances of initial beliefs result in qualitatively different long-term system behaviors. As we show in Figure 1, even though the initial mean belief is the same, the consensus formation process starting with a low level of initial disagreement (a small value for initial variance) results in the risk dominant equilibrium \((H,H)\), while starting with a high level of initial disagreement (a large value for initial variance) leads to the payoff dominant equilibrium \((S,S)\). We provide additional evidence covering all possible initial mean beliefs under the same 2-population stag-hunt game setting in Figure 2 (Section 6). Figure 2 shows that increasing the variance of initial beliefs from \(0\) to \(0.02\), \(0.05\), and \(0.1\) expands the region of attraction of the payoff dominant equilibrium \((S,S)\), allowing a wider range of initial mean beliefs to approach it. Thus, in the case of network coordination, consensus formation plays a crucial role in the seminal thmry problem of equilibrium selection, and a larger initial variance can promote convergence to the equilibrium \((S,S)\). In the case of network competition, convergence to a unique QRE is formally guaranteed (Theorem 4); we empirically verify this in 5-population asymmetric matching pennies games with many distinct Nash equilibria (Section 1 of the Appendix).

Other Related Work.SFP is a well-studied model that converges in various classes of games, including most 2-player-2-action games , n-player potential game s, and zero-sum and identical-interest stochastic games . However, SFP in network (population) games has not been previously explored. Our research extends the findings of Fudenberg and Takahashi  (who showed that agents with different beliefs in a 2-population setting will converge to the same belief) to the network settings, but we demonstrate that the variance of initial beliefs plays a crucial role in equilibrium selection, which has not been found in . On the other hand, networked multi-agent learning is a current frontier in AI research . Previous studies have shown the non-convergent behavior of replicator dynamics in (weighted) zero-sum network games , the convergence of smooth Q-learning in weighted potential  and weighted zero-sum network games , and the convergence of fictitious play in zero-sum network games . This paper provides new convergence results for SFP in both network games and network population games, covering a wide range of generic 2-population-2-action settings, star-graph settings that admit an exact potential, and all weighted zero-sum settings. Finally, the evolution of probability distributions over initial conditions or strategies has also received increasing interest . Continuity equations have been used as a tool to show that under Cross learning, agents with different mixed strategies in a population game will not converge to the same mixed strategy , and to also describe the more complicated Q-learning dynamics in various settings . This paper formally shows that the probability distribution over initial conditions can eventually degenerate to a point mass, and leveraging on this, presents a novel technique for proving the convergence of learning dynamics.4

## 2 Preliminaries

A **network population game**\(=(N,(V,E),(S_{i},_{i})_{ i V},(_{ij})_{(i,j)  E})\) consists of a multi-agent system \(N\) distributed over a connected graph \((V,E)\), where \(V=\{1,...,n\}\) is the set of vertices each represents a population (continuum) of agents, and \(E\) is the set of pairs, \((i,j)\), of population \(i j V\). For each population \(i V\), agents of this population has a finite set \(S_{i}\) of pure strategies (or actions) with generic elements \(s_{i} S_{i}\). Agents may also use mixed strategies (or choice distributions). For an arbitrary agent \(k\) in population \(i\), its mixed strategy is a vector \(_{i}(k)_{i}\), where \(_{i}\) is the simplex in \(^{|S_{i}|}\) such that \(_{s_{i} S_{i}}x_{is_{i}}(k)=1\) and \(x_{is_{i}}(k) 0, s_{i} S_{i}\). Each edge \((i,j) E\) defines a series of two-player subgames between populations \(i\) and \(j\). We denote the payoff matrices for agents of population \(i\) and \(j\) in these two-player subgames by \(_{ij}^{|S_{i}||S_{j}|}\) and \(_{ji}^{|S_{j}||S_{i}|}\), respectively. For every time step, each agent chooses a (mixed or pure) strategy and plays that strategy in all two-player subgames with agents from the connected (or neighbor) populations. Let \(=(_{i},\{_{j}\}_{(i,j) E})\) be a mixed strategy profile, where \(_{i}\) (or \(_{j}\)) denotes a generic mixed strategy in population \(i\) (or \(j\)). Given the mixed strategy profile \(\), the expected payoff of using \(_{i}\) in the game \(\) is

\[r_{i}()=r_{i}(_{i},\{_{j}\}_{(i,j) E}) _{(i,j) E}_{i}^{}_{ij}_{j}.\] (1)

**Smooth (or stochastic) fictitious play** (SFP) is a belief-based model for learning in games. By SFP, agents act as if they are Bayesian such that they assume that the strategies of other agents are drawn from some fixed but unknown distribution. Given a game \(\), consider an arbitrary agent \(k\) in a population \(i V\). Let \(V_{i}=\{j V:(i,j) E\}\) be the set of neighbor populations. Agent \(k\) maintains a weight \(^{i}_{js_{j}}(k)\) for each opponent strategy \(s_{j} S_{j}\) of each neighbor population \(j V_{i}\). For every time step \(t\), agents refine these weights based on the average play of each neighbor population, i.e., for each opponent strategy \(s_{j} S_{j},j V_{i}\),

\[^{i}_{js_{j}}(k,t+1)=^{i}_{js_{j}}(k,t)+_{js_{j}}(t),\] (2)

where \(_{js_{j}}\) is the mean probability of playing strategy \(s_{j}\) in population \(j\).5 For simplicity, we assume the initial sum of weights \(_{s_{j} S_{j}}^{i}_{js_{j}}(k,0)\) to be the same for every agent in the system and denote this initial sum by \(\). Based on the weights, agent \(k\) forms a belief about the neighbor population \(j\) such that each opponent strategy \(s_{j}\) is played with probability

\[^{i}_{js_{j}}(k)=_{js_{j}}(k)}{_{s^{}_{j} S_ {j}}^{i}_{js^{}_{j}}(k)}.\] (3)

Let \(^{i}_{j}(k)\) be the vector of beliefs with the \(s_{j}\)-th element equals \(^{i}_{js_{j}}(k)\). Given a game \(\), agent \(k\)'s expected payoff of using a pure strategy \(s_{i} S_{i}\) in response to the aggregate beliefs \(^{i}_{j}(k), j V_{i}\) about neighbor populations is

\[u_{is_{i}}(k)=_{j V_{i}}_{s_{i}}^{}_{ij}^{i}_{j}(k),\] (4)

where \(_{s_{i}}\) is a unit vector with the \(s_{i}\)-th element equal \(1\). By convention [33; 34; 44], we suppose that agents' payoffs are perturbed and that agents play smooth best responses maximizing their perturbed payoffs. Agent \(k\)'s perturbed payoff of using a mixed strategy \(_{i}(k)_{i}\) is defined as

\[_{i}(_{i}(k),\{_{j}^{i}(k)\}_{j V_{i}})= _{j V_{i}}_{i}(k)^{}_{ij}_{j}^{i}(k)+  v(_{i}(k)),\] (5)

where \(>0\), and the function \(v\) is strictly concave and its gradient becomes arbitrarily large near the boundary of the simplex \(_{i}\).6 For the function \(v\), a typical choice is \(v(_{i}(k))=-_{s_{i} S_{i}}x_{is_{i} }(k)(x_{is_{i}}(k))\). Under this form, the maximization of the perturbed payoff \(_{i}\) yields a unique choice distribution \(_{i}(k)\) from the logit choice rule , that is,

\[x_{is_{i}}(k)=}(k))}{_{s^{}_{i} S_{i}} ( u_{is^{}_{i}}(k))},\] (6)

where \(\) is the temperature (or the degree of rationality). For clarity of our presentation, we choose to focus on the logit choice rule in this paper; however, all our results readily generalize to any function \(v\) satisfying the above two standard assumptions. In the following, we usually drop the time index \(t\) and agent index \(k\) in the bracket (depending on the context) for notational convenience.

## 3 Belief Dynamics in Network Population Games

Observe that for an arbitrary agent \(k\) of population \(i\), its belief \(_{j}^{i}(k)\) about population \(j\) is in the simplex \(_{j}=\{_{j}^{i}(k)^{|S_{j}|}|_{s_{j} S_{ j}}_{js_{j}}^{i}(k)=1,_{js_{j}}^{i}(k) 0, s_{j} S_{j}\}\). We assume that the system state is characterized by a Borel probability measure \(P\) defined on the state space \(=_{i V}_{i}\). Given \(_{i}_{i}\), we write the marginal probability density function as \(p(_{i},t)\). Note that \(p(_{i},t)\) represents the density of agents having the belief \(_{i}\)_about_ population \(i\)_throughout the system_; it does _not_ represent the density of agents of population \(i\) that have the belief \(_{i}\), _nor_ the density of agents having the belief \(_{i}\) about population \(i\) in a certain population. Define \(=\{_{i}\}_{i V}\). Since agents maintain separate beliefs about different neighbor populations, the joint probability density function \(p(,t)\) can be factorized, i.e., \(p(,t)=_{i V}p(_{i},t)\). We make the following assumption for the initial marginal density functions.

**Assumption 1**.: _At time \(t=0\), for each population \(i V\), the marginal density function \(p(_{i},t)\) is continuously differentiable and has zero mass at the boundary of the simplex \(_{i}\)._

This assumption excludes the setting that a population of agents starts off with the same belief as well as the setting that agents start off with extreme beliefs. Under this mild condition, we derive a partial differential equation to describe the evolution of the belief distribution.

**Proposition 1**.: _The continuous-time dynamic of the marginal density function \(p(_{i},t)\) for each population \(i V\) is described by a partial differential equation_

\[-_{i},t)}{ t}=(p(_{i},t)}_{i}-_{i}}{+t+1})\] (7)

_where \(\) is the divergence operator and \(}_{i}\) is the mean mixed strategy with each \(s_{i}\)-th element_

\[_{is_{i}}=_{_{j V_{i}}_{j}}})}{_{s^{}_{i} S_{i}}( u_{is^{}_{i} })}_{j V_{i}}p(_{j},t)(_{j V_{i}}d _{j})\] (8)

_where \(u_{is_{i}}=_{j V_{i}}_{s_{i}}^{}_{ij}_{j}\)._

The partial differential equation in Equation 7 is also known as a _continuity equation_ in the study of transport phenomena (e.g., of mass or energy) in a physical system. We remark that continuity equations, tracking the evolution of a probability distribution, do not allow for general solutions. However, continuity equations provide a tractable way to examine the evolution of the moments of the probability distribution, without obtaining an analytical form of that distribution. The analytical form for the dynamic of the variance of beliefs (the second moment of the belief distribution) is given as follows.

**Theorem 1**.: _The dynamic of the variance of beliefs \(_{i}\) about each population \(i V\) is governed by an ordinary differential equation such that for each strategy \(s_{i}\),_

\[(_{is_{i}})}{dt}=-(_{is_{i}})}{+t +1}.\] (9)

_At given time \(t\), \((_{is_{i}})=()^{2}^{ 2}(_{is_{i}})\), where \(^{2}(_{is_{i}})\) is the initial variance. Thus, the variance \((_{is_{i}})\) decays to zero at a quadratic rate with time._

Note that the above theorem makes no assumption about the 2-player subgames agents play, the network structure of populations, or even the initial belief distribution. Therefore, it shows that for any network of populations, the variance of beliefs about each population always asymptotically tends to zero. As an immediate consequence of this theorem, the asymptotic distribution of the beliefs is as follows:

**Corollary 1**.: _The probability density function \(p(_{i},t)\) for every population \(i V\) will eventually evolve into a Dirac delta function, implying that in the long run agents reach a consensus and play the same mixed strategy within each population._

The intuition of Theorem 1 and Corollary 1 is that under SFP, agents revise their beliefs about other populations based on the average play of those populations; consequently, the observations are the same within an agent population, and thus it is only a matter of time that the agent population will develop the same belief and play the same strategy. Once consensus is reached, agents within a population will not develop different beliefs afterwards, as ensured by Theorem 1. In this sense, the agent population is internally cohesive and achieves intra-population stability.

However, it does not mean that different agent populations will eventually play the same strategy. This is because agents' strategies are the smooth best responses to their aggregated beliefs about their own set of neighbor populations (which generally vary depending on the network topology). This naturally raises the question of how different agent populations align with their population-wise incentives (which can be of a competitive nature). This also sets our paper apart from previous work that studies game-theoretic equilibration in the context of consensus formation , in which consensus is achieved as the result of agents' incentives to align their opinions or behaviors to those of others (i.e., the incentives are of a coordinative nature).

Before diving into the convergence results, we take a moment to analyze the mean belief (first moment) dynamic. Intuitively, the mean belief represents the average system state. The following proposition provides an analytical form of the mean belief dynamic.

**Proposition 2**.: _The mean belief dynamic for each population \(i V\) is governed by an ordinary differential equation such that for each strategy \(s_{i}\),_

\[_{is_{i}}}{dt}}(\{_{j}\}_{j V _{i}})-_{is_{i}}}{+t+1}+}_{s_{j} S _{j}}f_{s_{i}}(\{_{j}\}_{j V_{i}})}{( _{js_{j}})^{2}}(_{js_{j}})}{2(+t+1)},\] (10)

_where \(f_{s_{i}}(\{_{j V_{i}})}\) is the logit choice rule (Equation 6) applied to strategy \(s_{i} S_{i}\), and \((_{js_{j}})\) is the variance of belief \(_{js_{j}}\) in the entire system._

We remark that the mean belief dynamic, to be exact, should be under the joint influence of infinitely many moments of the belief distribution. However, the higher the order of the moments, the less of the influence of the moments, and by convention, the effects of the third and higher moments can be assumed negligible .7 Thus, in this proposition, the mean belief dynamic depends on the current mean belief and also on the current variance of beliefs.

Recall that Theorem 1 shows that the variance of beliefs will eventually decrease to zero. As a result, the second term of the mean belief dynamic (Equation 10) will asymptotically tend to zero. This suggests that the asymptotic mean belief dynamic of a population that starts with different beliefs in the initial state might be understood by studying the belief dynamic of a single representative agent.

Intuitively, if each population can be represented by a single agent, then the concerned network population games will effectively degenerate into the classic form of network games with each vertex representing a single agent. Inspired by this, in the next section, we temporarily focus on SFP in the classic network games.

## 4 Convergence of Smooth Fictitious Play in Classic Network Games

In a classic network game (or network game for short), the only difference with respect to the network population games is that now each vertex represents a single agent. We derive the belief dynamic of each vertex (or agent) in the following proposition.

**Proposition 3**.: _In a network game, the belief dynamic of each agent is governed by an ordinary differential equation such that for each agent \(i\) and each strategy \(s_{i}\),_

\[}}{dt}=}(\{_{j}\}_{j V_{i}})-_{is _{i}}}{+t+1}=}-_{is_{i}}}{+t+1}.\] (11)

It follows from this proposition that the equilibrium state should satisfy the following property.

**Proposition 4**.: _In a network game, the equilibrium state should satisfy that \(_{i}^{*}=_{i}^{*}\) for every strategy \(s_{i} S_{i}\) and every population \(i V\). Thus, the equilibrium state corresponds to the solution to the system of equations_

\[x_{is_{i}}^{*}=}_{s_{i}}^{ }_{ij}_{j}^{*})}{_{s_{i}^{} S_{i}} (_{j V_{i}}_{s_{i}^{}}^{}_{ ij}_{j}^{*})}, s_{i} S_{i}, i V.\] (12)

_Such an equilibrium state coincides with the Quantal Response Equilibria  of the network game._

By QRE, in this paper, we refer to their canonical form also referred to as logit QRE in the literature . We remark that QRE  is a game theoretic solution concept under bounded rationality, and is an extension of the standard Nash equilibrium which allows for errors in choice.

Weighted zero-sum network gamescapture the case of network competition, and are the generalization of 2-player zero-sum games to the network settings . A network game is weighted zero-sum if there exist positive constants (or weights) \(_{1},,_{n}\) such that

\[_{i V}_{i}r_{i}()=_{(i,j) E}(_{i} _{i}^{}_{ij}_{j}+_{j}_{j}^{ }_{ji}_{i})=0,_{i  V}_{i},\] (13)

where \(r_{i}()\) is the expected payoff of using a mixed strategy \(_{i}\) given the mixed strategy profile \(=(_{i},\{_{j}\}_{(i,j) E})\). If subgames along every edge \((i,j) E\) are 2-player zero-sum, i.e., \(_{i}^{}_{ij}_{j}+_{j}^{} _{ji}_{i}=0\), then the network game is zero-sum; however, the converse is not generally true. As opposed to 2-player zero-sum games that admit a unique value at equilibrium , the Nash equilibrium payoffs in weighted zero-sum network games need not be unique, and it generally allows for infinitely many Nash equilibria. In the following theorem, we show that regardless of the number of Nash equilibria in a network game, the belief dynamic is guaranteed to converge to a unique QRE.

**Theorem 2**.: _For any weighted zero-sum network game, the belief dynamic converge to a unique QRE which is globally asymptotically stable._

Exact potential network games capture the case of network coordination. A network game admits an exact potential if for each edge \((i,j) E\), the payoff matrices of the two-player subgame satisfy \(_{ij}=_{ji}^{}\)[21; 20]. Pure or mixed Nash equilibria in network coordination are complex. As reported in recent works [20; 17; 4], finding a pure Nash equilibrium is PLS-complete. Hence, learning in the general case of network coordination is difficult and generally requires some conditions [67; 69]. In the following theorem, we show the convergence result in some cases of network coordination.

**Theorem 3**.: _For any exact potential network game that exhibits a star structure, each orbit of the belief dynamic converges to the set of QRE._

We remark that we employ the Lyapunov approach (i.e., establishing the existence of the Lyapunov function) to prove the above convergence results. As a result, the convergence to equilibrium is equivalent to the network of agents being stable in the sense of Lyapunov.

## 5 Convergence of Smooth Fictitious Play in Network Population Games

We shall now formalize that the convergence results in network games can be seamlessly carried over to network population games with different beliefs in the initial state, and establish the inter-population stability in network population games. Recall that the mean belief dynamic (Proposition 2) depends on the current mean belief as well as the current variance of beliefs, with the variance decreasing over time (Theorem 1). Leveraging on the definition of _asymptotically autonomous_ dynamical systems , we establish the following lemma.

**Lemma 1**.: _In a network population game, for every population \(i V\), the mean belief dynamic (Equation 10) is asymptotically autonomous with the limit equation being \(_{i}}{dt}=_{i}-_{i}\), which after time-reparameterization is equivalent to the belief dynamic in a network game (Equation 11)._

Thieme  provided the following seminal result that formally connects the asymptotic behavior of an asymptotically autonomous dynamical system to that of its associated limit equation.

**Lemma 2** (Thieme  Theorem 4.2).: _Given a metric space \((X,d)\), let the solution flows of an asymptotically autonomous system and its limit equation be \(\) and \(\), respectively. Assume that the equilibria of \(\) are isolated compact \(\)-invariant subsets of \(X\). The \(\)-\(\)-limit set of any pre-compact \(\)-orbit contains a \(\)-equilibrium. The point \((s,x),s t_{0},x X\), have a pre-compact \(\)-orbit. Then the following alternative holds: 1) \((t,s,x) e,t,\) for some \(\)-equilibrium e, and 2) the \(\)-\(\)-limit set of \((s,x)\) contains finitely many \(\)-equilibria which are chained to each other in a cyclic way._

Combining the above lemmas and the convergence results under the setting of classic network games, the convergence results of SFP in network population games are readily deducible:

**Theorem 4**.: _For any weighted zero-sum network population game, the belief (as well as the choice distribution) of every individual converges to a unique QRE._

**Theorem 5**.: _For any exact potential network population game that exhibits a star structure, the belief (as well as the choice distribution) of every individual converges to the set of QRE._

Note that under the 2-population settings, there is only one possible graph structure--one edge connecting the two populations. Hofbauer and Hopkins  have shown that SFP converges in all 2-player weighted potential games, and observed that almost all 2-player-2-action games are either 2-player weighted potential games or 2-player weighted zero-sum games. Therefore, combining their results and our aforementioned results, SFP converges to the set of QRE under almost all the 2-population-2-action settings (including all the 2-population-2-action weighted potential settings and all the 2-population-2-action weighted zero-sum settings).

## 6 Experiments: Equilibrium Selection in Two-Population Stag Hunt Games

In a two-population stag hunt game, agents from two populations are paired up to choose between two actions \(H\) (hare) and \(S\) (stag). If both players hunt a hare, they both receive a reward of \(1\). If

Figure 2: Initial disagreement helps select the payoff dominant equilibrium \((S,S)\) (yellow: the equilibrium \((S,S)\), blue: the equilibrium \((H,H)\)). As the variance of initial beliefs (denoted by \(^{2}(_{1H})\)) increases from the left to right panel, a wider range of initial mean beliefs will approximately reach the equilibrium \((S,S)\) in the limit. For simplicity, we set \(^{2}(_{1H})=^{2}(_{2H})\). The black line on the diagonal is there for ease of comparison across different panels.

both players hunt a stag, they both receive a reward of \(4\). If one hunts a hare and the other hunts a stag, the former receives a reward of \(2\) whereas the latter receives nothing. There are two pure strategy Nash equilibria in each subgame: the risk dominant equilibrium \((H,H)\) and the payoff dominant equilibrium \((S,S)\), which naturally raises the problem of equilibrium selection.

Figure 1 showed that given the same initial mean belief, changing the variances of initial beliefs can result in different limit behaviors. In Figure 2, we systematically studied the effect of the variance of initial beliefs by visualizing how it affects the regions of attraction to different equilibria. To be more specific, we numerically solved the mean belief dynamics for a wide range of initial mean beliefs, given different variances of initial beliefs, and colored the regions of attraction. We observed in Figure 2 that as the variance of initial beliefs increases (from the left to right panel), a wider range of initial mean beliefs results in the convergence to the QRE that approximates the payoff dominant equilibrium \((S,S)\). Hence, consensus is inevitable, but the initial disagreement still provides an approach to equilibrium selection and helps select the highly desirable equilibrium.

Note that in the case of network competition, the convergence to a unique QRE is formally guaranteed; we empirically verified this through a 5-population asymmetric matching pennies game presented in Appendix A.

## 7 Conclusions

This paper presents a formal treatment for SFP in network population games where each population of agents start off with different beliefs and strategies. Representing the system state with a distribution over beliefs, we show that consensus is always reached, and that agents eventually pointwise converge to the same belief, regardless of specific settings. In addition, we establish the convergence of SFP to Quantal Response Equilibria in general competitive network population games as well as exact potential network population games with star structure. Empirically, we show that although the initial belief heterogeneity vanishes in the limit, it plays a crucial role in equilibrium selection and helps select highly desirable equilibria.