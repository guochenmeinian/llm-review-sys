# CoT Solution:

StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving+
Footnote †: This research was partially supported by the Center for Perceptual and Interactive Intelligence (CPII) Ltd. under the Innovation and Technology Commission’s InnoHK scheme.

Chang Gao\({}^{}\), Haiyun Jiang\({}^{}\), Deng Cai\({}^{}\), Shuming Shi\({}^{}\), Wai Lam\({}^{}\)

\({}^{}\)The Chinese University of Hong Kong \({}^{}\)Tencent AI Lab

\({}^{}\)School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University

{gaochang,wlam}@se.cuhk.edu.hk haiyunjiangnlp@gmail.com

{jcykcai,shumingshi}@tencent.com

This research was partially supported by the Center for Perceptual and Interactive Intelligence (CPII) Ltd. under the Innovation and Technology Commission's InnoHK scheme.Work done during an internship at Tencent AI Lab.Haiyun Jiang is the corresponding author.

###### Abstract

Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples. To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts. It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.2% \(\) 38.8%), commonsense reasoning (70.3% \(\) 72.5%), algorithmic reasoning (73.7% \(\) 85.0%), and symbolic reasoning (30.0% \(\) 79.2%). Further analysis reveals that StrategyLLM is applicable to various LLMs and demonstrates advantages across numerous scenarios.

## 1 Introduction

Recent advances in large language models (LLMs) have facilitated the development of prompting techniques [26; 43; 20; 8]. In particular, chain-of-thought (CoT) prompting methods [43; 6; 12; 41], which condition LLMs on a few task examples with step-by-step solutions, guide LLMs to break down complex reasoning processes into simpler steps. These approaches have markedly improved performance compared to standard few-shot prompting across a variety of tasks.

Despite their potential, current CoT approaches employing few-shot prompts with instance-specific solutions may face challenges in terms of _generalizability_ and _consistency_. Concerning _generalizability_, the solution can be highly specific to the question in each instance, limiting its applicability to other instances. For example, as illustrated in the left part of Figure 1, a solution for a particular system of linear equations with two variables may not provide valuable insights for addressing another system with three variables. Furthermore, the solutions in different instances within the few-shot prompt may exhibit a lack of task-level _consistency_, which complicates the process for LLMs to develop effective solutions for tackling new instances. As demonstrated in the left part of Figure 1, the two specific solutions are based on different approaches: Solution 1 employs expression substitution, while Solution 2 utilizes equation subtraction, which may not provide consistent guidance for LLMs to solve new instances. To address these limitations, it is crucial to incorporate _effective problem-solving strategies_ and develop _consistent strategy-based solutions_ within few-shot prompts. The right part of Figure 1 presents an effective strategy, i.e., Gaussian Elimination Method, offering generalizable steps applicable to any system of linear equations. By providing this strategy and consistently applying it across various instances in the few-shot prompt, LLMs can be better equipped to generate effective solutions for new task instances.

This paper aims to _construct generalizable and consistent strategy-based few-shot prompts for various tasks automatically, while being highly cost-efficient_. Our proposed framework, StrategyLLM, draws inspiration from human cognitive processes to derive general problem-solving strategies. This approach enables LLMs to reason inductively, i.e., deriving general strategies from specific task instances, and deductively, i.e., applying general strategies to particular task examples, to formulate prompts. An example of strategy-based prompts can be seen in Figure 5. The inductive reasoning process enhances _generalizability_ by formulating general problem-solving strategies, while the deductive reasoning process improves _consistency_ by producing consistent solutions using a given strategy. Developing effective problem-solving strategies is crucial to the success of our framework. To achieve this, we design StrategyLLM as a multi-agent collaboration framework comprising four LLM-based agents--strategy generator, executor, optimizer, and evaluator, as shown in Figure 2. The strategy generator initially creates a pool of strategies that are executed on task examples to assess accuracy, with qualified strategies cached based on a threshold and further evaluated. Unqualified ones may be optimized and re-evaluated iteratively. Through the collaboration of these intelligent agents, our framework is capable of autonomously generating, evaluating, and selecting effective strategies for various tasks and eliminating the need for human involvement.

Crucially, the strategy-based few-shot prompt generation phase is applied once for a given task, after which the learned prompt can be employed for inference on the entire test set. This inference process does not require any additional input beyond the standard few-shot prompting settings. The prompt generation process is highly cost-effective as it necessitates only a few task examples. In particular, StrategyLLM expends less than $0.24 to develop a strategy-based prompt for a variety of tasks using the latest version of GPT-3.5-Turbo.

We conduct comprehensive evaluations of StrategyLLM on 13 datasets across 4 challenging tasks: math reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning. The experimental results reveal the following key findings: (1) StrategyLLM outperforms competitive baselines on all tasks without using any human-annotated reasoning processes; (2) StrategyLLM can be applied to various LLMs and is robust to different groups of task examples; (3) StrategyLLM can generate generalizable and consistent prompts in a cost-effective manner. These findings demonstrate the potential of StrategyLLM as an effective, efficient, and reliable problem-solving framework.

Figure 1: Comparison of specific solutions and strategy-based solutions.

## 2 StrategyLLM

Our StrategyLLM framework is designed to efficiently create strategy-based few-shot prompts for a wide range of tasks. Subsequently, these prompts can be utilized for inference. In this section, we will introduce our framework in detail. The inference procedure will be discussed in Section 3.

Overview of StrategyLLMAs presented in Figure 2, our framework consists of four key agents: strategy generator, executor, optimizer, and evaluator. The prompts for the strategy generator and executor are presented in Figure 3 and Figure 4, respectively. The prompts of the strategy optimizer are in Appendix C. Typically, only a few task examples are used in the collaboration process, making our framework highly efficient.

The collaboration process begins with the strategy generator formulating a pool of strategies based on its understanding of the target task. These strategies then undergo two rounds of validation and selection. In the first round, the strategy executor applies each strategy to a set of task examples to yield its execution result and compute its execution accuracy. Strategies that meet or exceed a pre-set threshold of execution accuracy are deemed qualified and are cached with their corresponding execution results and accuracy. If the number of qualified strategies is less than a pre-defined number \(k\), the optimizer refines the unqualified strategies using their execution results. These enhanced strategies are then sent back to the strategy executor for the next iteration. This cycle may repeat until a sufficient number of qualified strategies are achieved or the maximum iteration limit is reached. Following this, all cached strategies are ranked based on their execution accuracy, and the top \(k\) strategies are selected. In the second round, the strategy evaluator constructs strategy-based few-shot prompts for each candidate strategy using itself and its execution result and assesses all candidate strategies using their corresponding prompts for inference on a validation set.

NotationsWe use \(p\), \(q\), \(st\), \(so\), and \(a\) to denote the prompt, question, strategy, solution, and answer, respectively. During inference, given a question \(q\), the language model \(M:(p,q)(so,a)\) generates a solution \(so\) and an answer \(a\) for it conditioned on the prompt \(p\). We denote the target task as \(t\), its definition as \(d\), and the set of task examples as \(\). Each example in \(\) is a \((q,a)\) pair.

Strategy DefinitionIn this paper, a task-solving strategy \(st\) is defined as a systematic approach designed for universal application across task examples, comprising a series of subtasks that encode task knowledge to address the target task \(t\). It is characterized by the following properties: (1) _Task-Level Applicability_: The strategy is formulated in a manner that allows for its application across all task instances, ensuring universality and consistency in its implementation. (2) _Structured Organization_: The strategy comprises a sequence of subtasks that are organized in a logical order to collectively tackle the target task. These subtasks are interconnected and contribute to the overall achievement of the task objective. (3) _Task Knowledge Encoding_: The strategy encapsulates general

Figure 2: Overview of StrategyLLM. Initially, the strategy generator creates a pool of strategies, which are then applied by the strategy executor to task examples to calculate execution accuracy. Qualified strategies meeting a pre-defined threshold are cached, and if necessary, unqualified strategies are optimized and re-evaluated in iterative cycles. Once a sufficient number of qualified strategies are obtained or the maximum iteration number is reached, the top \(k\) strategies are ranked by execution accuracy and evaluated using a validation set.

task knowledge and principles, avoiding any specific details unique to individual task examples. These properties collectively contribute to the effectiveness and efficiency of a strategy by promoting consistency, clarity, and informed decision-making in addressing task-level challenges. By embodying these properties, a strategy can serve as a valuable tool to navigate complex tasks and achieve optimal outcomes. An example of the strategy is presented in Figure 5.

Strategy Generator \(G\)The strategy generator, represented as \(G:(,d,n)\{st_{j}\}_{j=1}^{n}\), aims to generate \(n\) diverse strategies for the target task \(t\) based on a set of task examples \(\) and the task definition \(d\) using temperature sampling.

Figure 4: Prompt of the strategy executor.

Figure 3: Prompt of the strategy generator.

Strategy Executor \(X\)The strategy executor, denoted as \(X:(,d,st)(_{st},eacc_{st})\), writes solutions to a set of task examples \(\) following the strategy \(st\) to obtain the execution result \(_{st}=\{(q,so,a)\}_{i=1}^{||}\) of \(st\). The execution accuracy \(eacc_{st}\) is calculated as the proportion of examples whose solutions yield correct answers, reflecting the degree of alignment between the strategy and task. Therefore, we select strategies with high execution accuracy as qualified strategies.

Strategy Optimizer \(O\)The strategy optimizer, represented as \(O:(,d,st,_{st}) st^{o}\), optimize the strategy \(st\) according to its execution result \(_{st}\) to obtain the updated strategy \(st^{o}\). Firstly, the strategy optimizer \(O\) analyzes why some solutions in \(_{st}\) are not correct and provides suggestions for improving \(st\). Secondly, it modifies \(st\) to obtain \(st^{o}\) based on the analysis and suggestions.

Strategy Evaluator \(E\)We select top \(k\) candidate strategies according to the execution accuracy. However, to ensure efficiency, we use a limited number of task examples for execution, making the execution accuracy not a very informative metric for choosing strategies. Therefore, we introduce a strategy evaluator to further evaluate the candidate strategies on a validation set \(\). This process only requires to perform inference once for each candidate strategy and is efficient. The strategy evaluator, denoted as \(E:(st,_{st},) vacc_{st}\), computes the validation accuracy \(vacc_{st}\) of the strategy \(st\) on \(\). To achieve this, it constructs the strategy-based few-shot prompt \(p_{st}=(st,_{st})\) and conducts inference on \(\) using \(p_{st}\). An example of strategy-based prompts is presented in Figure 5. The validation accuracy \(vacc_{st}\) is calculated as the percentage of validation examples whose answers are correct, reflecting the effectiveness of \(st\) in real-world scenarios. Strategies with high validation accuracy can be used for inference.

## 3 Inference

Through collaborative efforts among multiple agents, we have obtained multiple candidate strategies, each with its few-shot prompt and validation accuracy. Depending on the task at hand, we can select one or more strategies with high validation accuracy for inference. For simple or specific tasks, a single optimal strategy that solves all task examples effectively may exist, making it sufficient to use only one strategy. However, for complex or diverse tasks, it is unlikely to find a strategy with absolute superiority. In such cases, adopting multiple strategies for inference is more appropriate, as they may excel in different task examples. To harness the strengths of multiple strategies, we employ two methods. The first method involves taking a majority vote on all answers obtained by multiple strategies, akin to the self-consistency (SC) method . The second method requires LLMs to determine the final answer by considering the solutions derived from multiple strategies in a zero-shot (ZS) manner, making it more proper for complex and diverse tasks. We denote the first and second methods as StrategyLLM-SC and StrategyLLM-ZS, respectively. The prompt for the second approach is provided in Appendix D.

## 4 Experiments

### Experimental Setup

Evaluation Tasks and DatasetsWe evaluate StrategyLLM on a variety of tasks:

Figure 5: Comparison of the strategy-based, standard, and chain-of-thought (CoT)  prompt.

* **Math Reasoning**: We use the challenging MATH benchmark  which comprises problems from mathematics competitions that require more than standard K-12 mathematics tools. It consists of seven datasets of different subjects, namely, Algebra (AL), Prealgebra (PA), Intermediate Algebra (IA), Counting and Probability (CP), Number Theory (NT), Geometry (GE), and Precalculus (PC).
* **Commonsense Reasoning**: We employ StrategyQA  and the Date Understanding (DU) task from Big-Bench Hard [38; 9]. StrategyQA necessitates inferring a multi-hop strategy to answer questions, while the DU task involves deducing a date from a given context.
* **Algorithmic Reasoning**: We adopt the Word Sorting (WS) task and the Multi-step Arithmetic (MA) task from Big-Bench Hard [38; 9]. The WS task involves sorting a list of words lexicographically, and the MA task requires solving multi-step equations with basic arithmetic operations.
* **Symbolic Reasoning**: We utilize the Last Letter Concatenation (LLC) task from , which requires concatenating the last letters of words in a sequence. In the few-shot prompt, the model only sees examples with two words. To evaluate the generalization abilities of different methods, we construct three out-of-distribution test sets (LLC-4, LLC-8, and LLC-16) with 4, 8, and 16 words in a sequence, respectively.

BaselinesWe conduct experiments in the few-shot setting and compare StrategyLLM with the following baselines:

* **Standard Prompting (SP)**: SP is the most direct approach for problem-solving. In SP, the prompt \(p\) contains a set of question-answer pairs without intermediate reasoning steps.
* **Chain-of-Thought (CoT) Prompting**: CoT incorporates step-by-step solutions for questions in the prompt \(p\) to elicit the multi-step reasoning capabilities of LLMs. We use few-shot CoT prompts from  for StrategyQA, DU, and LLC, and prompts from  for WS and MA. For MATH datasets, we create few-shot CoT prompts by randomly sampling 4 examples from each dataset's training set since these datasets contain human-annotated solutions. The CoT prompts for these datasets are in Appendix H.
* **Self-Consistency with CoT (CoT-SC)**: CoT-SC generates a set of solutions using CoT via temperature sampling to obtain multiple answers. Subsequently, it takes a majority vote over these answers to determine the final answer. For experiments, we sample 3 reasoning paths using temperature sampling with a temperature of 0.7.

  
**Methods** & AL & PA & IA & CP & NT & GE & PC & Avg \\  SP & 32.0 & 50.0 & 17.5 & 27.0 & 20.5 & 21.0 & 20.5 & 26.9 \\ SolutionLLM & 58.5 & 56.5 & 13.5 & 33.0 & 32.0 & 28.0 & 19.5 & 34.4 \\ CoT & 57.0 & 57.5 & 15.0 & 33.5 & 28.0 & 23.0 & 20.0 & 33.4 \\ CoT-SC & 59.0 & 62.0 & 16.5 & 34.5 & 28.0 & 24.5 & 15.0 & 34.2 \\  StrategyLLM & 58.5 & 57.5 & 18.0 & 35.0 & 29.5 & 24.5 & 22.5 & 35.1 \\ StrategyLLM-SC & 60.0 & 61.5 & 18.0 & 38.5 & 30.5 & 28.0 & **24.0** & 37.2 (+8.8\%) \\ StrategyLLM-ZS & **64.5** & **65.5** & **19.0** & **39.0** & **32.5** & **28.5** & 22.5 & **38.8 (+13.4\%)** \\   

Table 1: Experimental results on the math reasoning task. The numbers in parentheses represent the relative improvement compared to CoT-SC.

    &  &  &  \\   & StrategyQA & DU & Avg & WS & MA & Avg & LLC-4 & LLC-8 & LLC-16 Avg \\  SP & 56.5 & 48.5 & 52.5 & 73.3 & 2.0 & 37.7 & 0 & 0 & 0 & 0 \\ SolutionLLM & 59.5 & 52.0 & 55.8 & 74.7 & 55.3 & 65.0 & 81.5 & 25.5 & 0 & 35.7 \\ CoT & 64.0 & 70.5 & 67.3 & 67.2 & 84.0 & 75.6 & 68.5 & 22.0 & 0 & 30.2 \\ CoT-SC & 70.0 & 70.5 & 70.3 & 61.3 & 86.0 & 73.7 & 68.0 & 22.0 & 0 & 30.0 \\  StrategyLLM & 67.5 & 68.5 & 68.0 & **80.0** & 86.7 & 83.4 & **98.0** & 86.5 & 51.5 & 78.7 \\ StrategyLLM-SC & **71.0** & **74.0** & **72.5** (**+3.1\%**) & 79.3 & **90.7** & **85.0** (**+15.4\%**) & **98.0** & **87.0** & **52.5** & **79.2 (+164.0\%)** \\ StrategyLLM-ZS & 70.0 & 72.5 & 71.3 (+1.4\%) & 78.7 & 89.3 & 84.0 (+14.1\%) & **98.0** & 86.0 & 44.0 & 76.0 (+153.3\%) \\   

Table 2: Experimental results on the commonsense, algorithmic, and symbolic reasoning tasks. The numbers in parentheses represent the relative improvement compared to CoT-SC.

* **SolutionLLM**: We construct this baseline to leverage LLMs to directly write the solution for each example in the few-shot prompts using greedy decoding, without using any strategies. The prompt of SolutionLLM is in Appendix E. Since both SolutionLLM and StrategyLLM generate prompts using LLMs, we can eliminate the potential effect of human expertise in the comparison, isolating the impact of incorporating effective strategies.

Implementation DetailsWe employ GPT-3.5 (gpt-3.5-turbo-16k-0613)  as the language model for our main experiments, serving as the backend for the strategy generator, executor, optimizer, and evaluator. For a fair comparison with baselines such as CoT, we use the same examples in their few-shot prompts for strategy generation, execution, and optimization. We select the top 1 or 3 strategies with the highest validation accuracy for inference. This allows us to demonstrate the performance of the optimal strategy and the benefits of using multiple strategies. We adopt greedy decoding for inference. Details of the strategies for each dataset can be found in Appendix G. The validation set size is 100 for all the datasets. For datasets with over 200 test examples, we randomly sample 200 examples for testing to reduce API costs. More details can be found in Appendix B.

### Main Results

Tables 1 and 2 present the experimental results of StrategyLLM and several baselines across four reasoning tasks. We have the following observations:

* _StrategyLLM is an effective and efficient framework for problem-solving_. StrategyLLM using multiple strategies, i.e., StrategyLLM-SC and StrategyLLM-ZS, outperforms all baselines across the four reasoning tasks. Furthermore, StrategyLLM employing the best discovered strategy consistently outperforms CoT and SolutionLLM. Notably, StrategyLLM automatically constructs generalizable and consistent few-shot prompts for tackling various tasks without human expertise, while CoT relies on human-annotated examples for each task.
* _Explicitly incorporating effective strategies significantly enhance the complex reasoning and out-of-distribution (OOD) generalization abilities of LLMs._ For example, our framework demonstrates more considerable improvements on the MATH benchmark compared to the simpler commonsense reasoning datasets. Furthermore, StrategyLLM substantially surpasses CoT and SolutionLLM on the three OOD test sets of the LLC task, showcasing the generalizability of effective strategies.
* _Adopting multiple strategies brings obvious benefits on complex or diverse tasks_. The performance of StrategyLLM is significantly improved by using multiple strategies on the math and commonsense reasoning tasks. The benefits of leveraging multiple strategies on simpler or more specific tasks, i.e., symbolic and algorithmic reasoning, is less significant. These observations indicate that our framework is capable of creating multiple complementary strategies for diverse or complex tasks. Furthermore, StrategyLLM-ZS outperforms StrategyLLM-SC on the math reasoning task, showing that allowing LLMs to determine the answer is more appropriate for intricate tasks.

## 5 Analysis

Evaluating the robustness of StrategyLLMWe conduct an investigation to assess the robustness of our StrategyLLM framework with respect to varying groups of examples. For this purpose, we

  
**Methods** & AL-dev & AL-random & CP-dev & CP-random \\  SP & 36.0 & 29.1\(\)3.9 & 25.5 & 26.8\(\)2.5 \\ SolutionLLM & 58.0 & 56.5\(\)2.2 & 31.0 & 32.2\(\)2.8 \\ CoT & 57.5 & 55.1\(\)1.5 & 34.0 & 33.4\(\)1.2 \\ CoT-SC & 59.5 & 58.3\(\)1.2 & 31.5 & 33.0\(\)1.2 \\  StrategyLLM & 57.0 & 54.7\(\)2.5 & 34.5 & 35.6\(\)2.3 \\ StrategyLLM-SC & **64.0** & 58.9\(\)1.1 & 36.5 & 38.4\(\)1.3 \\ StrategyLLM-ZS & 62.5 & **60.8\(\)2.6** & **38.5** & **38.8\(\)1.7** \\   

Table 3: Experimental results on two math reasoning datasets, namely AL and CP, with different groups of examples.

select two math reasoning datasets with diverse examples, namely AL and CP, and randomly sample 5 distinct groups of examples from their respective training sets. We then report the mean and standard deviation of the results. Additionally, we employ the validation set to identify a group of 4 examples from the training set. Specifically, we use the OpenAI embedding model API (the text-embedding-3-large model) to map training and validation questions to embeddings and subsequently select the 4 training examples with the highest cosine similarities to all validation examples. We designate these groups of examples as AL-dev and CP-dev, respectively. The results, as presented in Table 3, demonstrate that StrategyLLM consistently delivers satisfactory performance on both datasets, suggesting that StrategyLLM is a robust and reliable framework for problem-solving.

Exploring the universality of StrategyLLMTo investigate the universality of our StrategyLLM framework, we apply it to a variety of LLMs to evaluate its effectiveness. For closed-source models, we utilize GPT-4 (gpt-4-0613)  and Claude-3-Sonnet (claude-3-sonnet-20240229) . For open-source models, we employ Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct , Mixtral-8x7B-Instruct-v0.1, and Mixtral-8x22B-Instruct-v0.1 . We conduct experiments on the CP, StrategyQA, and MA datasets, which represent three distinct reasoning tasks. The results, summarized in Tables 4 and 5, reveal that integrating effective strategies for constructing generalizable and consistent few-shot prompts yields significant benefits across a range of model capabilities and task complexities, underscoring the framework's universality. StrategyLLM notably enhances performance in open-source models such as Meta-Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v0.1, particularly on the CP and MA datasets which demand complex reasoning, indicating the effectiveness of our framework in scenarios requiring sophisticated problem-solving. These findings further corroborate StrategyLLM's robustness and reliability as a problem-solving framework.

Comparing reasoning via task-level strategy and instance-specific planningOur framework facilitates generalizable and consistent reasoning by developing task-level strategies. To evaluate

    &  &  \\   & CP & StrategyQA & MA & Avg & CP & StrategyQA & MA & Avg \\  SolutionLLM & 52.0 & 75.5 & 96.7 & 74.7 & 21.0 & 73.5 & 69.3 & 54.6 \\ CoT & 49.5 & 80.5 & 92.7 & 74.2 & 26.0 & 69.0 & 72.7 & 55.9 \\ CoT-SC & 54.5 & **83.5** & 94.7 & 77.6 & 26.0 & 75.0 & 76.7 & 59.2 \\  StrategyLLM & 52.5 & 81.5 & **98.7** & 77.2 & **28.0** & 75.0 & 83.3 & 62.1 \\ StrategyLLM-SC & **56.0** & **83.5** & **98.7** & **79.4** (**+2.4\%**) & **28.0** & **77.0** & **88.0** & **64.3** (**+8.6\%**) \\   

Table 4: Experimental results of closed-source models on the CP, StrategyQA, and MA datasets. The numbers in parentheses represent the relative improvement compared to CoT-SC.

    &  &  \\   & CP & StrategyQA & MA & Avg & CP & StrategyQA & MA & Avg \\  SolutionLLM & 20.5 & 64.0 & 43.3 & 42.6 & 51.5 & 79.0 & 72.0 & 67.5 \\ CoT & 16.0 & 61.0 & 44.7 & 40.6 & 48.5 & 80.5 & 81.3 & 70.1 \\ CoT-SC & 19.5 & 71.0 & 45.3 & 45.3 & 47.0 & 81.5 & 82.0 & 70.2 \\  StrategyLLM & 24.5 & **74.0** & 64.7 & 54.4 & 51.5 & 82.0 & 88.0 & 73.8 \\ StrategyLLM-SC & **25.0** & **74.0** & **66.0** & **55.0** (**+21.5\%**) & **54.0** & **83.5** & **91.3** & **76.3** (**+8.7\%**) \\    &  &  \\   & CP & StrategyQA & MA & Avg & CP & StrategyQA & MA & Avg \\  SolutionLLM & 22.5 & 61.0 & 34.7 & 39.4 & 44.5 & 72.0 & 60.7 & 59.1 \\ CoT & 24.5 & 63.0 & 59.3 & 48.9 & 41.0 & 72.0 & 80.0 & 64.3 \\ CoT-SC & 26.5 & 73.5 & 62.7 & 54.2 & 40.5 & 75.0 & 80.7 & 65.4 \\  StrategyLLM & 28.5 & 73.5 & 76.0 & 59.3 & 44.5 & 76.5 & 84.0 & 68.3 \\ StrategyLLM-SC & **32.0** & **75.0** & **78.0** & **61.7** (**+13.7\%**) & **47.5** & **77.0** & **89.3** & **71.3** (**+9.0\%**) \\   

Table 5: Experimental results of open-source models on the CP, StrategyQA, and MA datasets. The numbers in parentheses represent the relative improvement compared to CoT-SC.

the necessity of effective task-level strategies, we compare our framework against two baselines: (1) Plan-and-Solve Prompting , which directs LLMs to formulate specific plans for each test instance at inference time and execute these plans to solve the instances; (2) CoT+Strategy, which combines the CoT prompt with instructions that guide LLMs to devise a task-solving strategy and apply it to a specific test example at inference time. The prompt for CoT+Strategy is detailed in Appendix F.

The performance of GPT-3.5 on the CP, StrategyQA, and MA datasets, representing three distinct reasoning tasks, is presented in Table 6. Our observations are as follows: (1) StrategyLLM significantly outperforms both Plan-and-Solve Prompting and CoT+Strategy across all three datasets. This highlights the superiority of generalizable task-level strategies over instance-specific plans in enhancing performance across various problem-solving contexts. This improvement can be attributed to two key factors: (a) our task-level strategies encapsulate essential task-level knowledge, thereby providing professional and high-level guidance; (b) generating high-quality, specific plans for each test example at inference time is inherently challenging, making it difficult to ensure the quality of these plans. (2) Even when explicitly encouraged to devise a general task-solving strategy in the CoT+Strategy method, the LLM tends to produce strategies that are highly specific to the test example and encode limited task-level knowledge. This underscores the necessity of creating generalizable strategy-based few-shot prompts.

Analyzing the cost of strategy-based prompt generationIn this analysis, we evaluate the cost of the strategy-based prompt generation process. The process includes the strategy generator, executor, optimizer, and evaluator, each contributing to the overall cost for each reasoning task. Table 7 details the average cost incurred by our StrategyLLM framework in generating a candidate strategy-based prompt, calculated by dividing the total cost of the process by the number of candidate strategies \(k\). The costs are presented in terms of input and output tokens and the money associated with using GPT-3.5-Turbo. The results indicate that our framework is economically efficient. The average cost for gpt-3.5-turbo-16k-0613 ranges from $0.33 to $1.12 across the four reasoning tasks. For the latest version of GPT-3.5-Turbo, specifically gpt-3.5-turbo-0125, the cost is considerably lower, ranging from $0.08 to $0.24. Generally, tasks of higher complexity consume more tokens due to their inherently longer solutions.

Examining results across various difficulty levelsThe problems in the MATH benchmark are classified by difficulty on a scale of 1 to 5. The easiest problems are assigned a difficulty level of 1, while the most challenging problems are given a difficulty level of 5. Figure 6 illustrates the performance of CoT-SC and StrategyLLM-SC on the seven datasets within the MATH benchmark, considering different difficulty levels. It is evident that the enhanced performance of StrategyLLM-SC over CoT-SC stems from its ability to tackle more complex problems, underscoring the significance of generalizable strategies in augmenting intricate reasoning.

More analysis can be found in Appendix A.

  
**Methods** & CP & StrategyQA & MA & Avg \\  Plan-and-Solve & 26.0 & 54.0 & 69.3 & 49.8 \\ Plan-and-Solve-SC & 27.5 & 64.5 & 70.0 & 54.0 \\ CoT+Strategy & 30.5 & 63.0 & 62.7 & 52.1 \\ CoT+Strategy-SC & 36.5 & 70.0 & 70.0 & 58.8 \\ StrategyLLM & 35.0 & 67.5 & 86.7 & 63.1 \\ StrategyLLM-SC & **38.5** & **71.0** & **90.7** & **66.7** \\   

Table 6: Comparison of Plan-and-Solve, CoT+Strategy, and StrategyLLM.

Figure 6: Comparison of CoT-SC and StrategyLLM-SC performance on the MATH benchmark across various difficulty levels.

## 6 Related Work

Prompting LLMs for Problem SolvingThe prominent chain-of-thought (CoT) prompting approach  has inspired a variety of prompting methods aimed at enhancing the problem-solving abilities of LLMs. These methods include using programming languages to describe the reasoning process [6; 13; 28], representing the reasoning process with complex structures such as trees or graphs [46; 3; 36; 47], applying task decomposition [49; 19; 34; 4], implementing self-correction with automatic feedback [22; 29; 30; 5; 7], and combining different prompting techniques [27; 50]. However, most of these approaches require manual annotation of reasoning processes, limiting their generalizability and flexibility. By comparison, our StrategyLLM framework can automatically construct strategy-based few-shot prompts for any task, ensuring generalizable and consistent solutions following effective strategies. This approach sets it apart from existing automatic prompt construction methods [48; 37; 45], which may generate inconsistent solutions within the prompt. The plan-and-solve prompting method  aims to address missing-step errors by requesting LLMs to generate a plan before solving a specific example in a zero-shot manner. The plan is instance-specific and significantly different from the task-solving strategy which can be applied to all task examples. The learning-to-plan approach  learns a text plan for each task to assist LLMs in problem-solving. The plan, which is not necessarily a strategy, can be any instruction helpful for solving the task. Moreover, it demands a large training and validation set during the learning process, resulting in high costs. In contrast, our framework is efficient and cost-effective.

LLM-based Autonomous AgentsThe adoption of autonomous agents driven by LLMs across various disciplines is revolutionizing our methodologies for tackling problems, making decisions, and fostering innovation [39; 44]. These agents have been utilized to enhance the reasoning capabilities of LLMs [42; 24; 11], contribute to social simulation [33; 23; 25; 21], and advance software development [35; 17; 10]. In this paper, we employ multiple LLM-based agents to collaborate in the generation, execution, optimization, and evaluation of problem-solving strategies.

## 7 Discussion

Limitation and ImpactThe key idea behind StrategyLLM is to harness the knowledge and reasoning capabilities of LLMs to develop and refine task-solving strategies tailored to specific tasks. By utilizing the extensive knowledge embedded in these LLMs, which are trained on diverse data sources spanning multiple domains, StrategyLLM is able to generate generalizable strategies that incorporate domain-specific expertise. However, if the model possesses limited knowledge in a particular domain, it is unlikely to create effective strategies for that domain. In such cases, merely optimizing the prompt may not significantly improve performance, and domain-specific continual training may be necessary. As LLMs continue to expand their knowledge bases and enhance their reasoning capabilities, their ability to generate generalizable strategies for diverse tasks is expected to improve, implying the potential of our StrategyLLM framework.

ConclusionThis paper proposes StrategyLLM, harnessing the power of LLMs to construct generalizable and consistent few-shot prompts for various tasks efficiently. Our framework's effectiveness and reliability are substantiated through extensive evaluations on four challenging tasks: mathematical reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning. Further analysis reveals that our framework exhibits robustness across different task example groups, application to various LLMs, cost-efficiency in prompt generation, and effectiveness in complex reasoning.

    & Math & Commonsense & Algorithmic & Symbolic \\  \# Input Tokens & 287.83K & 228.67K & 107.27K & 70.94K \\ \# Output Tokens & 63.14K & 33.15K & 32.95K & 28.48K \\ Cost of gpt-3.5-turbo-16k-0613 & \$1.12 & \$0.82 & \$0.45 & \$0.33 \\ Cost of gpt-3.5-turbo-0125 & \$0.24 & \$0.16 & \$0.10 & \$0.08 \\   

Table 7: Average cost of prompt generation across four reasoning tasks.