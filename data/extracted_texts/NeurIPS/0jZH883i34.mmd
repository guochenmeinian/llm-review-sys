# Model Sparsity Can Simplify Machine Unlearning

Jinghan Jia\({}^{1,}\) Jiancheng Liu\({}^{1,}\) Parikshit Ram\({}^{2}\) Yuguang Yao\({}^{1}\) Gaowen Liu\({}^{3}\)

Yang Liu\({}^{4,5}\)&Pranay Sharma\({}^{6}\)&Sijia Liu\({}^{1,2}\)

\({}^{1}\)Michigan State University, \({}^{2}\)IBM Research, \({}^{3}\)Cisco Research,

\({}^{4}\)University of California, Santa Cruz, \({}^{5}\)ByteDance Research, \({}^{6}\)Carnegie Mellon University

\({}^{}\)Equal contribution

###### Abstract

In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm, termed prune first, then unlearn, which infuses a sparse model prior into the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest unlearning methods) when using sparsity-aware unlearning. Furthermore, we demonstrate the practical impact of our proposed MU methods in addressing other machine learning challenges, such as defending against backdoor attacks and enhancing transfer learning. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse.

## 1 Introduction

Machine unlearning (**MU**) initiates a reverse learning process to scrub the influence of data points from a trained machine learning (**ML**) model. It was introduced to avoid information leakage about private data upon completion of training [1; 2; 3], particularly in compliance with legislation like 'the right to be forgotten'  in General Data Protection Regulation (GDPR) . The _direct but optimal_ unlearning approach is _exact unlearning_ to _retrain_ ML models from scratch using the remaining training set, after removing the data points to be scrubbed. Although retraining yields the _ground-truth_ unlearning strategy, it is the most computationally intensive one. Therefore, the development of _approximate but fast_ unlearning methods has become a major focus in research [6; 7; 8; 9; 10].

Despite the computational benefits of approximate unlearning, it often lacks a strong guarantee on the effectiveness of unlearning, resulting in a performance gap with exact unlearning . In particular, we encounter two main challenges. _First_, the performance of approximate unlearning can heavily rely on the configuration of algorithmic parameters. For example, the Fisher forgetting method  needsto carefully tune the Fisher information regularization parameter in each data-model setup. _Second_, the effectiveness of an approximate scheme can vary significantly across the different unlearning evaluation criteria, and their trade-offs are not well understood. For example, high 'efficacy' (ability to protect the privacy of the scrubbed data) _neither_ implies _nor_ precludes high 'fidelity' (accuracy on the remaining dataset) . This raises our **driving question (Q)** below:

**(Q)** _Is there a theoretically-grounded and broadly-applicable method to improve approximate unlearning across different unlearning criteria?_

To address **(Q)**, we advance MU through a fresh and novel viewpoint: **model sparsification**. _Our key finding_ is that model sparsity (achieved by weight pruning) can significantly reduce the gap between approximate unlearning and exact unlearning; see **Fig. 1** for the schematic overview of our proposal and highlighted empirical performance.

Model sparsification (or weight pruning) has been extensively studied in the literature [13; 14; 15; 16; 17; 18; 19], focusing on the interrelation between model compression and generalization. For example, the notable lottery ticket hypothesis (**LTH**)  demonstrated the existence of a sparse subnetwork (the so-called 'winning ticket') that matches or even exceeds the test accuracy of the original dense model. In addition to generalization, the impact of pruning has also been investigated on model robustness [20; 21; 22], fairness [23; 24], interpretability [25; 26], loss landscape [16; 26], and privacy [27; 28]. In particular, the privacy gains from pruning [27; 28] imply connections between data influence and model sparsification.

More recently, a few works [29; 30] attempted to draw insights from pruning for unlearning. In Wang et al. , removing channels of a deep neural network (**DNN**) showed an unlearning benefit in federated learning. And in Ye et al. , filter pruning was introduced in lifelong learning to detect "pruning identified exemplars"  that are easy to forget. **However**, different from the above literature that customized model pruning for a specific unlearning application, our work systematically and comprehensively explores and exploits the foundational connections between unlearning and pruning. We summarize our **contributions** below.

\(\) First, we provide a holistic understanding of MU across the full training/evaluation stack.

\(\) Second, we draw a tight connection between MU and model pruning and show in theory and practice that model sparsity helps close the gap between approximate unlearning and exact unlearning.

\(\) Third, we develop a new MU paradigm termed 'prune first, then unlearn', and investigate the influence of pruning methods in the performance of unlearning. Additionally, we develop a novel'sparsity-aware unlearning' framework that leverages a soft sparsity regularization scheme to enhance the approximate unlearning process.

\(\) Finally, we perform extensive experiments across diverse datasets, models, and unlearning scenarios. Our findings consistently highlight the crucial role of model sparsity in enhancing MU.

## 2 Revisiting Machine Unlearning and Evaluation

**Problem setup.** MU aims to remove (or scrub) the influence of some targeted training data on a trained ML model [1; 2]. Let \(=\{_{i}\}_{i=1}^{N}\) be a (training) dataset of \(N\) data points, with label

Figure 1: Schematic overview of our proposal on model sparsity-driven MU. Evaluation at-a-glance shows the performance of three unlearning methods (retraining-based exact unlearning, finetuning-based approximate unlearning , and proposed unlearning on 95%-sparse model) under five metrics: unlearning accuracy (UA), membership inference attack (MIA)-based unlearning efficacy, accuracy on remaining data (RA), testing accuracy (TA), and run-time efficiency (RTE); see summary in **Tab. 1**. The unlearning scenario is given by class-wise forgetting, where data points of a single class are scrubbed. Each metric is normalized to \(\) based on the best result across unlearning methods for ease of visualization. Results indicate that model sparsity reduces the gap between exact and approximate MU without loss in efficiency.

information encoded for supervised learning. \(_{}\) represents a subset whose influence we want to scrub, termed the **forgetting dataset**. Accordingly, the complement of \(_{}\) is the **remaining dataset**, _i.e._, \(_{}=_{}\). We denote by \(\) the model parameters, and \(_{}\) the **original model** trained on the entire training set \(\) using _e.g._, empirical risk minimization (ERM). Similarly, we denote by \(_{}\) an **unlearned model**, obtained by a scrubbing algorithm, after removing the influence of \(_{}\) from the trained model \(_{}\). The **problem of MU** is to find an accurate and efficient scrubbing mechanism to generate \(_{}\) from \(_{}\). In existing studies [2; 7; 12], the choice of the forgetting dataset \(_{}\) specifies different unlearning scenarios. There exist two main categories. First, _class-wise forgetting_[7; 12] refers to unlearning \(_{}\) consisting of training data points of an entire class. Second, _random data forgetting_ corresponds to unlearning \(_{}\) given by a subset of random data drawn from all classes.

**Exact and approximate MU methods.** The _exact unlearning_ method refers to _retraining_ the model parameters from _scratch_ over the remaining dataset \(_{}\). Although retraining from scratch (that we term **Retrain**) is optimal for MU, it entails a large computational overhead, particularly for DNN training. This problem is alleviated by _approximate unlearning_, an easy-to-compute proxy for Retrain, which has received growing attention. Yet, the boosted computation efficiency comes at the cost of MU's efficacy. We next review some commonly-used approximate unlearning methods that we improve in the sequel by leveraging sparsity; see a summary in **Tab.1**.

\(\)_Fine-tuning (**FT**)_[6; 12]: Different from Retrain, FT fine-tunes the pre-trained model \(_{}\) on \(_{}\) using a few training epochs to obtain \(_{}\). The rationale is that fine-tuning on \(_{}\) initiates the catastrophic forgetting in the model over \(_{}\) as is common in continual learning .

\(\)_Gradient ascent (**GA**)_[7; 8]: GA reverses the model training on \(_{}\) by adding the corresponding gradients back to \(_{}\), _i.e._, moving \(_{}\) in the direction of increasing loss for data points to be scrubbed.

\(\)_Fisher forgetting (**FF**)_[9; 12]: FF adopts an additive Gaussian noise to 'perturb' \(_{}\) towards exact unlearning. Here the Gaussian distribution has zero mean and covariance determined by the \(4\)th root of Fisher Information matrix with respect to (w.r.t.) \(_{}\) on \(_{}\). We note that the computation of the Fisher Information matrix exhibits lower parallel efficiency in contrast to other unlearning methods, resulting in higher computational time when executed on GPUs; see Golatkar et al.  for implementation details.

\(\)_Influence unlearning (**IU**)_[10; 32]: IU leverages the influence function approach  to characterize the change in \(_{}\) of a training point is removed from the training loss. IU estimates the change in model parameters from \(_{}\) to \(_{}\), _i.e._, \(_{}-_{}\). IU also relates to an important line of research in MU, known as \(\)-\(\) forgetting [29; 35; 36]. However, it typically requires additional model and training assumptions .

We next take a step further to revisit the IU method and re-derive its formula (**Prop. 1**), with the aim of enhancing the effectiveness of existing solutions proposed in the previous research.

**Proposition 1**: _Given the weighted ERM training \(()=_{}L(,)\) where \(L(,)=_{i=1}^{N}[w_{i}_{i}(,_{i})]\), \(w_{i}\) is the influence weight associated with the data point \(_{i}\) and \(^{T}=1\), the model update from \(_{}\) to \(()\) yields_

\[():=()-_{} ^{-1}_{}L(/N-,_{}),\] (1)

_where \(\) is the \(N\)-dimensional vector of all ones, \(=/N\) signifies the uniform weights used by ERM, \(^{-1}\) is the inverse of the Hessian \(_{,}^{2}L(/N,_{})\) evaluated at \(_{}\), and \(_{}L\) is the gradient of \(L\). When scrubbing \(_{}\), the unlearned model is given by \(_{}=_{}+(_{})\). Here \(_{}^{N}\) with entries \(w_{,i}=_{_{}}(i)/[_{ }]\) signifying the data influence weights for MU, \(_{_{}}(i)\) is the indicator function with value \(I\) if \(i_{}\) and 0 otherwise, and \(|_{}|\) is the cardinality of \(_{}\)._

**Proof**: We derive (1) using an implicit gradient approach; see Appendix A.

It is worth noting that we have taken into consideration the weight normalization effect \(^{T}=1\) in (1). This is different from existing work like Izzo et al. [10, Sec. 3] using Boolean or unbounded

  Unlearning &  & Representative work \\ Methods & UA & All-Efficacy & RA & T& RTE & Representative work \\  FT & ✓ & ✓ & ✓ & ✓ & 0.06\(\) & [6; 12] \\ GA & ✓ & ✓ & ✓ & ✓ & 0.02\(\) & [7; 8] \\ PF & & ✓ & ✓ & ✓ & 0.9\(\) & [9; 12] \\ IU & ✓ & ✓ & ✓ & 0.08\(\) & [10; 32] \\  Ours & ✓ & ✓ & ✓ & ✓ & 0.07\(\) & This work \\  

Table 1: Summary of approximate unlearning methods considered in this work. The marker ‘\(\)’ denotes the metric used in previous research. The number in RTE is the run-time cost reduction compared to the cost of Retrain, based on our empirical studies in Sec. 5 on (CIFAR-10, ResNet-18). Note that GA seems better than ours in terms of RTE, but it is less effective in unlearning.

weights. In practice, we found that IU with weight normalization can improve the unlearning performance. Furthermore, to update the model influence given by (1), one needs to acquire the second-order information in the form of inverse-Hessian gradient product. Yet, the exact computation is prohibitively expensive. To overcome this issue, we use the first-order WoodFisher approximation  to estimate the inverse-Hessian gradient product.

**Towards a 'full-stack' MU evaluation.** Existing work has assessed MU performance from different aspects . Yet, a single performance metric may provide a limited view of MU . By carefully reviewing the prior art, we focus on the following empirical metrics (summarized in Tab. 1).

\(\)_Unlearning accuracy (UA)_: We define \((_{})=1-_{_{}}(_{})\) to characterize the _efficacy_ of MU in the accuracy dimension, where \(_{_{}}(_{})\) is the accuracy of \(_{}\) on the forgetting dataset \(_{}\). It is important to note that a more favorable UA for an approximate unlearning method should **reduce its performance disparity with the gold-standard retrained model (Retrain)**; a higher value is not necessarily better. This principle also extends to other evaluation metrics.

\(\)_Membership inference attack (MIA) on \(_{}\) (MIA-Efficacy)_: This is another metric to assess the _efficacy_ of unlearning. It is achieved by applying the confidence-based MIA predictor  to the unlearned model (\(_{}\)) on the forgetting dataset (\(_{}\)). The MIA success rate can then indicate how many samples in \(_{}\) can be correctly predicted as forgetting (_i.e._, non-training) samples of \(_{}\). A _higher_ MIA-Efficacy implies less information about \(_{}\) in \(_{}\); see Appendix C.3 for more details.

\(\)_Remaining accuracy (RA)_: This refers to the accuracy of \(_{}\) on \(_{}\), which reflects the _fidelity_ of MU , _i.e._, training data information should be preserved from \(_{}\) to \(_{}\).

\(\)_Testing accuracy (TA)_: This measures the _generalization_ ability of \(_{}\) on a testing dataset rather than \(_{}\) and \(_{}\). TA is evaluated on the whole test dataset, except for class-wise forgetting, in which testing data points belonging to the forgetting class are not in the testing scope.

\(\)_Run-time efficiency (RTE)_: This measures the computation efficiency of an MU method. For example, if we regard the run-time cost of Retrain as the baseline, the computation acceleration gained by different approximate unlearning methods is summarized in Tab. 1.

## 3 Model Sparsity: A Missing Factor Influencing Machine Unlearning

**Model sparsification via weight pruning.** Model sparsification could not only facilitate a model's training, inference, and deployment but also benefit model's performance. For example, LTH (lottery ticket hypothesis)  stated that a trainable sparse sub-model could be identified from the original dense model, with test accuracy on par even better than the original model. **Fig. 2** shows an example of the pruned model's generalization vs. its sparsity ratio. Here one-shot magnitude pruning (**OMP**)  is adopted to obtain sparse models. OMP is computationally the lightest pruning method, which directly prunes the model weights to the target sparsity ratio based on their magnitudes. As we can see, there exists a graceful sparse regime with lossless testing accuracy.

**Gains of MU from sparsity.** We first analyze the impact of model sparsity on MU through a lens of _unrolling stochastic gradient descent_ (**SGD**) . The specified SGD method allows us to derive the _unlearning error_ (given by the weight difference between the approximately unlearned model and the gold-standard retrained model) when scrubbing a single data point. However, different from Thudi et al. , we will infuse the model sparsity into SGD unrolling.

Let us assume a binary mask \(\) associated with the model parameters \(\), where \(m_{i}=0\) signifies that the \(i\)th parameter \(_{i}\) is pruned to zero and \(m_{i}=1\) represents the unmasked \(_{i}\). This sparse pattern \(\) could be obtained by a weight pruning method, like OMP. Given \(\), the **sparse model** is \(\), where \(\) denotes the element-wise multiplication. Thudi et al.  showed that if GA is adopted to scrub a single data point for the original (dense) model \(\) (_i.e._, \(=\)), then the gap between GA and Retrain can be approximately bounded in the weight space. **Prop. 2** extends the existing unlearning error analysis to a sparse model.

Figure 2: Testing accuracy of OMP-based sparse ResNet-18 vs. the dense model on CIFAR-10.

**Proposition 2**: _Given the model sparse pattern \(\) and the SGD-based training, the unlearning error of GA, denoted by \(e()\), can be characterized by the weight distance between the GA-unlearned model and the gold-standard retrained model. This leads to the error bound_

\[e()=(^{2}t\|(_{t}-_{0})\|_{2}())\] (2)

_where \(\) is the big-O notation, \(\) is the learning rate, \(t\) is the number of training iterations, \((_{t}-_{0})\) denotes the weight difference at iteration \(t\) from its initialization \(_{0}\), and \(()\) is the largest singular value (\(\)) of the Hessian \(^{2}_{,}\)( for a training loss \(\)) among the unmasked parameter dimensions, i.e., \(():=_{j}\{_{j}(^{2}_{,} ),m_{j} 0\}\)._

_Proof_: _See Appendix B. \(\)_

We next draw some key insights from **Prop. 2**. _First_, it is clear from (2) that the unlearning error reduces as the model sparsity in \(\) increases. By contrast, the unlearning error derived in Thudi et al.  for a dense model (_i.e._, \(=\)) is proportional to the dense model distance \(\|_{t}-_{0}\|_{2}\). Thus, model sparsity is beneficial to reducing the gap between (GA-based) approximate and exact unlearning. _Second_, the error bound (2) enables us to relate MU to the spectrum of the Hessian of the loss landscape. The number of active singular values (corresponding to nonzero dimensions in \(\)) decreases when the sparsity grows. However, it is important to note that in a high-sparsity regime, the model's generalization could decrease. Consequently, it is crucial to select the model sparsity to strike a balance between generalization and unlearning performance.

Inspired by Prop. 2, we ask: _Does the above benefit of model sparsification in MU apply to other approximate unlearning methods besides GA?_ This drives us to investigate the performance of approximate unlearning across the entire spectrum as depicted in Tab. 1. Therefore, **Fig. 3** shows the unlearning efficacy (UA and MIA-Efficacy), fidelity (RA), and generalization (TA) of different approximate unlearning methods in the sparse model regime. Here class-wise forgetting is considered for MU and OMP is used for weight pruning. As we can see, the efficacy of approximate unlearning is significantly improved as the model sparsity increases, _e.g._, UA and MIA-Efficacy of using FT over 90% sparsity. By contrast, FT over the dense model (0% sparsity) is the least effective for MU. Also, the efficacy gap between exact unlearning (Retrain) and approximate unlearning reduces on sparse models. Further, through the fidelity and generalization lenses, FT and FF yield the RA and TA performance closest to Retrain, compared to other unlearning methods. In the regime of ultra-high sparsity (99%), the efficacy of unlearning exhibits a tradeoff with RA and TA to some extent.

## 4 Sparsity-Aided Machine Unlearning

Our study in Sec. 3 suggests the new MU paradigm 'prune first, then unlearn', which leverages the fact that (approximate) unlearning on a sparse model yields a smaller unlearning error (Prop. 2) and improves the efficacy of MU (Fig. 3). This promising finding, however, raises some new questions. First, it remains elusive how the choice of a weight pruning method impacts the unlearning performance. Second, it leaves room for developing sparsity-aware MU methods that can directly scrub data influence from a dense model.

**Prune first, then unlearn: Choice of pruning methods.** There exist many ways to find the desired sparse model in addition to OMP. Examples include pruning at random initialization before training [40; 41] and simultaneous pruning-training iterative magnitude pruning (**IMP**) . Thus, the problem of pruning method selection arises for MU. From the viewpoint of MU, the unlearner would

Figure 3: Performance of approximate unlearning (FT, GA, FF, IU) and exact unlearning (Retrain) in efficacy (UA and MIA-Efficacy), fidelity (RA), and generalization (TA) vs. model sparsity (achieved by OMP) in the data-model setup (CIFAR-10, ResNet-18). The unlearning scenario is class-wise forgetting, and the average unlearning performance over 10 classes is reported. We remark that being closer to Retrain performance is better for approximate MU schemes.

prioritize a pruning method that satisfies the following criteria: \(\)_least dependence_ on the forgetting dataset (\(_{}\)), \(\)_lossless generalization_ when pruning, and \(\)_pruning efficiency_. The rationale behind \(\) is that it is desirable _not_ to incorporate information of \(_{}\) when seeking a sparse model prior to unlearning. And the criteria \(\) and \(\) ensure that sparsity cannot hamper TA (testing accuracy) and RTE (run-time efficiency). Based on \(\)-\(\), we propose to use two pruning methods.

\(\)**SynFlow** (synaptic flow pruning) : SynFlow provides a (training-free) pruning method at initialization, even without accessing the dataset. Thus, it is uniquely suited for MU to meet the criterion \(\). And SynFlow is easy to compute and yields a generalization improvement over many other pruning-at-initialization methods; see justifications in .

\(\)**OMP** (one-shot magnitude pruning) : Different from SynFlow, OMP, which we focused on in Sec. 3, is performed over the original model (\(_{}\)). It may depend on the forgetting dataset (\(_{}\)), but has a much weaker dependence compared to IMP-based methods. Moreover, OMP is computationally lightest (_i.e._ best for \(\)) and can yield better generalization than SynFlow .

Furthermore, it is important to clarify that IMP (iterative magnitude pruning) is _not_ suitable for MU, despite being widely used to find the most accurate sparse models (_i.e._, best for criterion \(\)). Compared with the proposed pruning methods, IMP has the largest computation overhead and the strongest correlation with the training dataset (including \(_{}\)), thereby deviating from \(\) and \(\). In **Fig. 4**, we show the efficacy of FT-based unlearning on sparse models generated using different pruning methods (SynFlow, OMP, and IMP). As we can see, unlearning on SynFlow or OMP-generated sparse models yields improved UA and MIA-Efficacy over that on the original dense model and IMP-generated sparse models. This unlearning improvement over the dense model is consistent with Fig. 3. More interestingly, we find that IMP _cannot_ benefit the unlearning efficacy, although it leads to the best TA. This is because IMP heavily relies on the training set including forgetting data points, which is revealed by the empirical results - the unlearning metrics get worse for IMP with increasing sparsity. Furthermore, when examining the performance of SynFlow and OMP, we observe that the latter generally outperforms the former, exhibiting results that are closer to those of Retrain. Thus, _OMP is the pruning method we will use by default_.

**Sparsity-aware unlearning.** We next study if pruning and unlearning can be carried out simultaneously, without requiring prior knowledge of model sparsity. Let \(L_{}(;_{},_{})\) denote the unlearning objective function of model parameters \(\), given the pre-trained state \(_{}\), and the remaining training dataset \(_{}\). Inspired by sparsity-inducing optimization , we integrate an \(_{1}\) norm-based sparse penalty into \(L_{}\). This leads to the problem of '\(_{1}\)**-sparse MU**':

\[_{}=*{arg\,min}_{}L_{ }(;_{},_{})+\|\|_{1},\] (3)

where we specify \(L_{}\) by the fine-tuning objective, and \(>0\) is a regularization parameter that controls the penalty level of the \(_{1}\) norm, thereby reducing the magnitudes of 'unimportant' weights.

In practice, the unlearning performance could be sensitive to the choice of the sparse regularization parameter \(\). To address this limitation, we propose the design of a sparse regularization scheduler. Specifically, we explore three schemes: (1) constant \(\), (2) linearly growing \(\) and (3) linearly decaying \(\); see Sec. 5.1 for detailed implementations. Our empirical evaluation presented in **Tab. 2**

   MU & UA & MIA-Efficacy & RA & TA & RTE (min) \\  Retrain & 5.41 & 13.12 & 100.00 & 94.42 & 42.15 \\ \(_{1}\)-sparse MU + constant \(\) & 6.60 (1.19) & 14.64 (1.52) & 96.51 (3.49) & 87.30 (7.12) & 2.53 \\ \(_{1}\)-sparse MU + linear growing \(\) & 3.80 (1.61) & 8.75 (4.37) & 97.13 (2.87) & 90.63 (3.79) & 2.53 \\ \(_{1}\)-sparse MU + linear decaying \(\) & **5.35 (0.06)** & **12.71 (0.41)** & **97.39 (2.61)** & **91.26 (3.16)** & 2.53 \\   

Table 2: MU performance comparison of using \(_{1}\)-sparse MU with different sparsity schedulers of \(\) in (3) and using Retrain. The unlearning scenario is given by random data forgetting (10% data points across all classes) on (ResNet-18, CIFAR-10). A performance gap against Retrain is provided in (\(\)).

Figure 4: Influence of different pruning methods (SynFlow, OMP, and IMP) in unlearning efficacy (UA and MIA-Efficacy) and generalization (TA) on (CIFAR-10, ResNet-18). **Left**: UA vs. TA. **Right**: MIA-Efficacy vs. TA. Each point is a FT-based unlearned dense or sparse model (75% or 95% sparsity), or a retrained dense model.

[MISSING_PAGE_FAIL:7]

**Model sparsity improves approximate unlearning.** In **Tab. 3**, we study the impact of model sparsity on the performance of various MU methods in the 'prune first, then unlearn' paradigm. The performance of the exact unlearning method (Retrain) is also provided for comparison. Note that the better performance of approximate unlearning corresponds to the smaller performance gap with the gold-standard retrained model.

_First_, given an approximate unlearning method (FT, GA, FF, or IU), we consistently observe that model sparsity improves UA and MIA-Efficacy (_i.e._, the efficacy of approximate unlearning) without much performance loss in RA (_i.e._, fidelity). In particular, the performance gap between each approximate unlearning method and Retrain reduces as the model becomes sparser (see the '95% sparsity' column vs. the 'dense' column). Note that the performance gap against Retrain is highlighted in \(()\) for each approximate unlearning. We also observe that Retrain on the 95%-sparsity model encounters a 3% TA drop. Yet, from the perspective of approximate unlearning, this drop brings in a more significant improvement in UA and MIA-Efficacy when model sparsity is promoted. Let us take FT (the simplest unlearning method) for class-wise forgetting as an example. As the model sparsity reaches \(95\%\), we obtain \(51\%\) UA improvement and \(8\%\) MIA-Efficacy improvement. Furthermore, FT and IU on the 95%-sparsity model can better preserve TA compared to other methods. Table 3 further indicates that sparsity reduces average disparity compared to a dense model across various approximate MU methods and unlearning scenarios.

_Second_, existing approximate unlearning methods have different pros and cons. Let us focus on the regime of \(95\%\) sparsity. We observe that FT typically yields the best RA and TA, which has a tradeoff with its unlearning efficacy (UA and MIA-Efficacy). Moreover, GA yields the worst RA since it is most loosely connected with the remaining dataset \(_{}\). FF becomes ineffective when scrubbing random data points compared to its class-wise unlearning performance. Furthermore, IU causes a TA drop but yields the smallest gap with exact unlearning across diverse metrics under the \(95\%\) model sparsity. In Appendix C.4, we provide additional results on CIFAR-100 and SVHN datasets, as shown in Tab. A3, as well as on the ImageNet dataset, depicted in Tab. A5. Other results pertaining to the VGG-16 architecture are provided in Tab. A4.

**Effectiveness of sparsity-aware unlearning.** In **Fig. 5**, we showcase the effectiveness of the proposed sparsity-aware unlearning method, _i.e._, \(_{1}\)-sparse MU. For ease of presentation, we focus on the comparison with FT and the optimal Retrain strategy in both class-wise forgetting and random data forgetting scenarios under (CIFAR-10, ResNet-18). As we can see, \(_{1}\)-sparse MU outperforms FT in the unlearning efficacy (UA and MIA-Efficacy), and closes the performance gap with Retrain without losing the computation advantage of approximate unlearning. We refer readers to Appendix C.4 and Fig. A2 for further exploration of \(_{1}\)-sparse MU on additional datasets.

**Application: MU for Trojan model cleanse.** We next present an application of MU to remove the influence of poisoned backdoor data from a learned model, following the backdoor attack setup , where an adversary manipulates a small portion of training data (_a.k.a._ poisoning ratio) by injecting a backdoor trigger (_e.g._, a small image patch) and modifying data labels towards a targeted incorrect label. The trained model is called _Trojan model_, yielding the backdoor-designated incorrect prediction if the trigger is present at testing. Otherwise, it behaves normally.

Figure 5: Performance of sparsity-aware unlearning vs. FT and Retrain on class-wise forgetting and random data forgetting under (CIFAR-10, ResNet-18). Each metric is normalized to \(\) based on the best result across unlearning methods for ease of visualization, while the actual best value is provided (_e.g._, \(2.52\) is the least computation time for class-wise forgetting).

Figure 6: Performance of Trojan model cleanse via proposed unlearning vs. model sparsity, where ‘Original’ refers to the original Trojan model. **Left**: ASR vs. model sparsity. **Right**: SA vs. model sparsity.

We then regard MU as a defensive method to scrub the harmful influence of poisoned training data in the model's prediction, with a similar motivation as Liu et al. . We evaluate the performance of the unlearned model from two perspectives, backdoor attack success rate (**ASR**) and standard accuracy (**SA**). **Fig. 6** shows ASR and SA of the Trojan model (with poisoning ratio \(10\%\)) and its unlearned version using the simplest FT method against model sparsity. Fig. 6 also includes the \(_{1}\)-sparse MU to demonstrate its effectiveness on model cleanse. Since it is applied to a dense model (without using hard thresholding to force weight sparsity), it contributes just a single data point at the sparsity level 0%. As we can see, the original Trojan model maintains \(100\%\) ASR and a similar SA across different model sparsity levels. By contrast, FT-based unlearning can reduce ASR without inducing much SA loss. Such a defensive advantage becomes more significant when sparsity reaches \(90\%\). Besides, \(_{1}\)-sparse MU can also effectively remove the backdoor effect while largely preserving the model's generalization. Thus, our proposed unlearning shows promise in application of backdoor attack defense.

**Application: MU to improve transfer learning.** Further, we utilize the \(_{1}\)-sparse MU method to mitigate the impact of harmful data classes of ImageNet on transfer learning. This approach is inspired by Jain et al. , which shows that removing specific negatively-influenced ImageNet classes and retraining a source model can enhance its transfer learning accuracy on downstream datasets after finetuning. However, retraining the source model introduces additional computational overhead. MU naturally addresses this limitation and offers a solution.

**Tab. 4** illustrates the transfer learning accuracy of the unlearned or retrained source model (ResNet-18) on ImageNet, with \(n\) classes removed. The downstream target datasets used for evaluation are SUN397  and OxfordPets . The employed finetuning approach is linear probing, which finetunes the classification head of the source model on target datasets while keeping the feature extraction network of the source model intact. As we can see, removing data classes from the source ImageNet dataset can lead to improved transfer learning accuracy compared to the conventional method of using the pre-trained model on the full ImageNet (_i.e._, \(n=0\)). Moreover, our proposed \(_{1}\)-sparse MU method achieves comparable or even slightly better transfer learning accuracy than the retraining-based approach . Importantly, \(_{1}\)-sparse MU offers the advantage of computational efficiency 2\(\) speed up over previous method  across all cases, making it an appealing choice for transfer learning using large-scale models. Here we remark that in order to align with previous method , we employed a fast-forward computer vision training pipeline (FFCV)  to accelerate our ImageNet training on GPUs.

**Additional results.** We found that model sparsity also enhances the privacy of the unlearned model, as evidenced by a lower MIA-Privacy. Refer to Appendix C.4 and Fig. A1 for more results. In addition, we have expanded our experimental scope to encompass the 'prune first, then unlearn' approach across various datasets and architectures. The results can be found in Tab. A3, Tab. A4, and Tab. A5. Furthermore, we conducted experiments on the \(_{1}\)-sparse MU across different datasets, the Swin-Transformer architecture, and varying model sizes within the ResNet family. The corresponding findings are presented in Fig. A2 and Tab. A6, A7, A8 and A9.

## 6 Related Work

While Sec. 2 provides a summary of related works concerning exact and approximate unlearning methods and metrics, a more comprehensive review is provided below.

**Machine unlearning.** In addition to exact and approximate unlearning methods as we have reviewed in Sec. 2, there exists other literature aiming to develop the probabilistic notion of unlearning [55; 56; 57; 58], in particular through the lens of differential privacy (DP) . Although DP enables unlearning with provable error guarantees, they typically require strong model and algorithmic assumptions and could lack effectiveness when facing practical adversaries, _e.g._, membership inference attacks. Indeed, evaluating MU is far from trivial [8; 9; 11]. Furthermore, the attention on MU has also been raised

    &  &  &  &  \\  & Acc & Acc & Time & Acc & Time & Acc & Time \\   \\  Method  & 85.70 & 85.79 & 71.84 & 86.10 & 61.51 & 86.32 & 54.53 \\ \(_{1}\)-sparse MU & 85.83 & 85.34 & 86.12 & 50.19 & 86.26 & 26.49 \\   \\  Method  & 46.55 & 46.67 & 73.26 & 47.14 & 61.43 & 47.31 & 55.24 \\ \(_{1}\)-sparse MU & 46.75 & 47.30 & 66.77 & 47.25 & 50.96 & 47.37 & 27.12 \\   

Table 4: Transfer learning accuracy (Acc) and computation time (mins) of the unlearned ImageNet model with \(n\{100,200,300\}\) classes removed, where SUN397 and OxfordPets are downstream target datasets on linear probing transfer learning setting. When \(n=0\), transfer learning is performed using the pretrained model on the full ImageNet, serving as a baseline, together with the method in  for comparison.

in different learning paradigms, _e.g._, federated learning [29; 60], graph neural networks [61; 62; 63], and adversarial ML [64; 65]. In addition to preventing the leakage of data privacy from the trained models, the concept of MU has also inspired other emergent applications such as adversarial defense against backdoor attacks [6; 50] that we have studied and erasing image concepts of conditional generative models [66; 67].

**Understanding data influence.** The majority of MU studies are motivated by data privacy. Yet, they also closely relate to another line of research on understanding data influence in ML. For example, the influence function approach  has been used as an algorithmic backbone of many unlearning methods [6; 10]. From the viewpoint of data influence, MU has been used in the use case of adversarial defense against data poisoning backdoor attacks . Beyond unlearning, evaluation of data influence has also been studied in fair learning [68; 69], transfer learning , and dataset pruning [70; 71].

**Model pruning.** The deployment constraints on _e.g._, computation, energy, and memory necessitate the pruning of today's ML models, _i.e._, promoting their weight sparsity. The vast majority of existing works [13; 14; 15; 16; 17; 18; 19] focus on developing model pruning methods that can strike a graceful balance between model's generalization and sparsity. In particular, the existence of LTH (lottery ticket hypothesis)  demonstrates the feasibility of co-improving the model's generalization and efficiency (in terms of sparsity) [72; 73; 74; 40; 75]. In addition to generalization, model sparsity achieved by pruning can also be leveraged to improve other performance metrics, such as robustness [20; 21; 22], model explanation [25; 26], and privacy [76; 27; 28; 77].

## 7 Conclusion

In this work, we advance the method of machine unlearning through a novel viewpoint: model sparsification, achieved by weight pruning. We show in both theory and practice that model sparsity plays a foundational and crucial role in closing the gap between exact unlearning and existing approximate unlearning methods. Inspired by that, we propose two new unlearning paradigms, 'prune first, then unlearn' and'sparsity-aware unlearn', which can significantly improve the efficacy of approximate unlearning. We demonstrate the effectiveness of our findings and proposals in extensive experiments across different unlearning setups. Our study also indicates the presence of _model modularity_ traits, such as weight sparsity, that could simplify the process of machine unlearning. This may open up exciting prospects for future research to investigate unlearning patterns within weight or architecture space.

## 8 Acknowledgement

The work of J. Jia, J. Liu, Y. Yao, and S. Liu were supported by the Cisco Research Award and partially supported by the NSF Grant IIS-2207052, and the ARO Award W911NF2310343. Y. Liu was partially supported by NSF Grant IIS-2143895 and IIS-2040800.