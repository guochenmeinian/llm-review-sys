# DreamSteerer: Enhancing Source Image Conditioned Editability using Personalized Diffusion Models

Zhengyang Yu\({}^{1}\)

&Zhaoyuan Yang\({}^{2}\)&Jing Zhang\({}^{1}\)

Australian National University\({}^{1}\)

GE Research\({}^{2}\)

{zhengyang.yu,jing.zhang}@anu.edu.au   zhaoyuan.yang@ge.com

Work was partially done while Zhengyang Yu was an intern at GE Research.Corresponding author.

###### Abstract

Recent text-to-image personalization methods have shown great promise in teaching a diffusion model user-specified concepts given a few images for reusing the acquired concepts in a novel context. With massive efforts being dedicated to personalized generation, a promising extension is personalized editing, namely to edit an image using personalized concepts, which can provide a more precise guidance signal than traditional textual guidance. To address this, a straightforward solution is to incorporate a personalized diffusion model with a text-driven editing framework. However, such a solution often shows unsatisfactory editability on the source image. To address this, we propose DreamSteerer, a plug-in method for augmenting existing T2I personalization methods. Specifically, we enhance the source image conditioned editability of a personalized diffusion model via a novel Editability Driven Score Distillation (EDSD) objective. Moreover, we identify a mode trapping issue with EDSD, and propose a mode shifting regularization with spatial feature guided sampling to avoid such an issue. We further employ two key modifications to the Delta Denoising Score framework that enable high-fidelity local editing with personalized concepts. Extensive experiments validate that DreamSteerer can significantly improve the editability of several T2I personalization baselines while being computationally efficient. Project page: https://github.com/Dijkstra14/DreamSteerer.

## 1 Introduction

Text-to-Image Diffusion Probabilistic Models (T2I DPMs)  have revolutionized novel content creation due to their superior capacity in both sample fidelity and mode coverage, as well as their flexibility in achieving effortless concept composition  and user-friendly control . Despite the success of these models, the limited expressiveness of natural language may lead to ambiguity in real-world scenarios where users demand greater specificity and engagement in the creation process.

This challenge has sparked extensive research in _T2I Personalization_. Specifically, personalization is the process of teaching T2I DPMs a novel visual concept with a few (usually 3-8) reference images by fine-tuning the pre-trained model parameters. After personalization, the personal concept is linked to a rare token in the text encoder dictionary, e.g., "[my_dog]", enabling flexible reuse of the visual concept in new contexts, e.g., "a photo of [my_dog] wearing an astronaut suit". Despite the success of these models, the majority of previous works have been focusing on personalized generation, emphasizing the preservation of concept identity across varying textual conditions. In this work, we explore a natural extension on these methods to perform image editing with acquired concepts, namely _personalized editing_, which can be promising in enabling higher levelcontrol over the editing direction than traditional text-driven editing frameworks [21; 34; 55]. The primary goal is to synthesize a high-fidelity image that aligns the appearance and content of the target concept, as well as the structural layout and background of the source image without blindly resorting to copying learned reference images. A straightforward solution to this problem is to incorporate a personalized diffusion model into an existing text-driven editing model, by which personalized editing appears to be trivial via swapping a source subject token in the source prompt by the special token, e.g., "a photo of (dog \(\) [my_dog]) wearing an astronaut suit". However, such a naive incorporation often leads to severe distortion or failure in natural adaptation to the source image layout. We attribute the essential causes for such failure to the limited scope of reference images in personalization. Under a lack of data diversity, existing personalization methods are prone to collapsing into the patterns of reference images  and entangling subject-relevant and subject-irrelevant information . This causes a significant loss in the model's prior knowledge related to the source category, resulting in poor editability [48; 60] in a new context. Additionally, the stricter demands for maintaining structural layout [54; 72] and preserving subject-irrelevant information [12; 21] necessitate a higher level of editability than personalized generation. In more challenging editing scenarios, a certain level of extrapolation on the attributes of personalized concepts may be required, e.g. the editing process may require significant change in subject structure, pose or style to achieve a high-fidelity

Figure 1: DreamSteerer enables efficient editability enhancement for a source image with any existing T2I personalization models, leading to significantly improved editing fidelity in various challenging scenarios. When the structural difference between source and reference images are significant, it can naturally adapt to the source while maintaining the appearance learned from the personal concept.

editing result on the source image. Without information on the source image during personalization, existing personalization methods can hardly be guaranteed to be editable on arbitrary source images.

To overcome these challenges, in this paper, we propose the first approach that enhances the source image conditioned editability using existing personalized T2I DPMs (DreamSteerer). Inspired by the one-step Bayes optimal denoising using Tweedie's formula [15; 59] with DPMs [3; 7; 11; 47; 69], and the probabilistic score distillation sampling [22; 57; 74], we formulate editability enhancement on the source image as a novel score distillation objective, dubbed Editability Driven Score Distillation (EDSD). Then we identify the existence of a mode trapping issue when directly optimizing EDSD. Inspired by recent success in zero-shot semantic correspondence [1; 6; 80] based on the strong spatial awareness of UNet attention features, we propose a spatial guided sampling strategy that produces a regularization set which alleviates the mode trapping issue via shifting the mode of personalized DPMs distribution. We further make two major modifications on the Delta Denoising Score (DDS)  framework for a valid adaptation from text-driven editing to personalized editing.

We summarize our main contributions as: 1) we identify and analyze the lack of editability in existing T2I personalization methods for editing real-world images, 2) we propose the DreamSteerer framework, a plug-in method that is compatible with arbitrary personalization baselines and requires only a small number of fine-tuning steps (\(\) 10) to achieve significant improvement in editability on a source image, 3) we validate the effectiveness of DreamSteerer on 3 different baselines [16; 39; 62], demonstrating its efficacy, especially in challenging personalized editing scenarios such as significant structural gaps and data-hungry cases (Fig. 1). See Fig. 3 for an overall framework.

## 2 Related Work

T2I Personalization.Building upon the success of T2I DPMs [52; 61; 64], recent works have shown the promise of personalized T2I synthesis [16; 19; 39; 62]. Textual Inversion  encapsulates personalized concepts by word embedding optimization, which is further improved by more expressive representation spaces [2; 73]. DreamBooth  fine-tunes the full model parameters of a pre-trained DPM that conditions on a rare word token. Efficient fine-tuning methods [9; 18; 45; 71; 77] have been introduced for improving efficiency and generalizability. Custom Diffusion  optimizes both a word embedding as well as the UNet cross-attention key and value projections for improved compositionality. We explore enhancement in editability on these three different types of models. More recent studies in encoder-based personalization [26; 63; 65; 79] propose new training paradigms to condition a Diffusion Model on single or multiple input images. Although these works can enable faster inference, they necessitate extensive pre-training and typically limit application to particular domains. New forms of conditioning, objectives or adaptors are proposed for different purposes such as stylization [23; 29; 51], improved identity preservation [13; 43], composability [37; 56; 82; 85], or generation fidelity . Unlike these works, we focus on addressing an inherently different task of improving the editability of personalized Diffusion Models.

Image Editing.The conditioning mechanisms [14; 24] and inversion techniques [49; 68] have enabled DPMs to achieve image editing. Earlier works [32; 34; 40] relied on heavy optimization and often failed in local editing, while other works [4; 52] achieved this using user-provided mask guidance. Prompt-to-Prompt  achieves both local and global editing using cross-attention map injection without requiring extra guidance. Similarly, PnP  proposes employing UNet attention features to achieve image-to-image translation. Masactrl  extends Prompt-to-Prompt  with mutual self-attention to manipulate subject pose or view. Despite the success of these works, most of them are designed only for text-driven editing, leaving image editing with personalized concepts still under-explored. Unlike preliminary attempts [10; 17] that rely on specific baselines, we consider editing with visual concepts acquired by an arbitrary existing personalization method.

Score Distillation.Pooled et al.  proposes a novel Score Distillation Sampling (SDS) method that can synthesize 3D assets without requiring 3D training data. For improving sample fidelity and mode coverage of SDS, VSD  utilizes LoRA adapters to model a Wasserstein gradient flow, NFSD  employs negative prompts, CSD  introduces Classifier Guidance. SDS has been improved and extended for different downstream purposes, such as conditional generation of different modality assets [28; 36], text-driven visual editing [22; 67] and text-aligned generation .

Preliminaries

Diffusion Probabilistic Models (DPMs).With an input image latent state \(_{0}\), a corresponding text prompt \(\), and a diffusion process defined as \(q(_{t}_{0}):=(_{t} ;}_{0},(1-_{t}))\) where \(_{t}\) represents the forward process variance at time \(t\) and \(_{t}\) is the noised latent state of the input \(_{0}\), a diffusion probabilistic model parameterized by \(\) can be trained using the denoising objective:

\[_{}(_{0},;)=_{ _{t} q(_{t}|_{0}),t(1,T), (,)}[\|_{} (_{t},,t)-\|_{2}^{2}]\.\] (1)

T2I Personalization with DPMs.Given a diffusion model \(_{_{0}}\) pre-trained for Text-to-Image (T2I) generation using Eq. 1, and a small set of image latent states and prompt pairs \(^{}_{}=\{(}^{ },}^{})_{n}\}_{n=1}^{N}\), which encapsulate the personalized concepts the user wishes the diffusion model to capture.

A general approach of T2I personalization methods [16; 62] is to fine-tune a subset of the source model parameters on \(^{}_{}\) by optimizing the denoising objective as:

\[=*{arg\,min}_{}_{(}^{ },}^{})^{ }_{}}_{}(}^{ },}^{};)\] (2)

where \(\) is initialized with the pre-trained weights \(_{0}\). The text prompt \(}^{}\) takes the form of "a photo of a \([S]\)", where the placeholder \([S]\) corresponds to a new word embedding representing the specific subject. At inference time, the fine-tuned model \(_{}\) can be used to generate creative images of the specific subject in novel contexts, such as "a photo of a \([S]\) sitting next to a mirror".

Score Distillation.Score Distillation is a mechanism for sampling from a source diffusion model under predefined constraints, achieved by performing probability density distillation from the source diffusion model \(_{}\) to a parameterized differentiable function. In the context of image domains, we represent this differentiable function as \(x()\), which renders the image with parameter \(\). SDS  is the pioneering approach in score distillation, which optimizes \(x()\) through a mode-seeking process. Specifically, given a target prompt \(}\) that describes the desired edit, the gradient of the distillation objective w.r.t. parameter \(\) is computed as follows:

\[_{}_{}(,x(),})= _{t(1,T),(,)}(t)(_{}(x_{t}(),},t )-)\] (3)

where \(x_{t}()\) is a noised latent state of \(x()\) at time step \(t\), and \((t)\) is a constant determined by the forward process variance. SDS is known to be a mode-seeking process and suffers from issues such as low diversity, over-saturation, and over-smoothness , which lead to sub-optimal performance on downstream tasks. Thus, various SDS variants have been proposed to enhance performance.

DDS  extends SDS to handle text-driven image editing. Specifically, given a source image latent state \(_{0}^{}\), a source prompt \(^{}\) that is aligned with \(_{0}^{}\), and a target prompt \(}\) that describes the desired edit, the parameter \(\) is initialized using the source image such that \(x()=_{0}^{}\). For a certain differentiable function \(x()\), \(\) can then be updated using the following delta score direction:

\[_{}_{}(,x(),})= _{t(1,T),(,)}(t)(_{}(x_{t}(),},t )-_{}(_{t}^{},^{ },t)).\] (4)

To simplify the notations, we incorporate all constants associated with score computations into \((t)\) for the remainder of the paper.

From text-driven editing to personalized editingExisting text-driven editing works like DDS generate high-quality output for image editing with open-world vocabulary; however, their capability of personalized concepts editing remains underexplored. In this work, we focus on bridging such a gap. Given a diffusion model \(_{}\) personalized via Eq. 2 and the latent state of a source image \(_{0}^{}\) with an aligned source prompt \(^{}\) (e.g., "a photo of a cat sitting next to a mirror"), our objective is to edit \(_{0}^{}\) using the personalized concept \([s]\) captured by \(_{}\). For example, an edit might be "a photo of a (cat \([s]\)) sitting next to a mirror," where \([s]\) represents your childhood pet cat. We define the target prompt as \(^{}\). The desiderata for personalized editing are concluded as follows* _the overall structure of edited image should align with the source image,_
* _the edited part should capture the appearance and content of the personal subject,_
* _the instruction-irrelevant part should be preserved as much as possible._

### Preliminary Editing with Existing Personalization Baseline

We explore a preliminary solution to personalized editing by employing SDS in Eq. 3 and DDS in Eq. 4 based on the personalized DPM \(_{}\), where the differentiable function is initialized by the source image latent state, i.e., \(x()=_{0}^{}\), and can be updated with \(_{}_{}(,x(),^{})\). However, this straightforward solution yields unsatisfactory performance as shown in Fig. 5 (b)-(c). Specifically, the editing result with SDS suffers from the same low fidelity issues as mentioned by prior work [22; 74]. With DDS, although the overall layout preservation is improved, there still reveals a lack of editability. Additionally, we observe that DDS leads to a severe bias of certain attributes on the source class, leading to unsatisfactory editing results.

Source Score Bias Correction.We suspect that such bias is caused by the distribution shift of \(_{}\) during personalization. As shown in Fig. 2, a DreamBooth trained for the concept "plushie_tortoise" shows significant bias towards the yellow color in the corresponding source class "tortoise", which leads to the incorrect color of the edited image as shown in Fig. 5 (c). To address this, we use the pre-trained diffusion model \(_{_{0}}\) to conduct the source score prediction in Eq. 4, yielding the modified delta score named as DDS-S, i.e.,

\[_{}_{}=_{t,} (t)(_{}(x_{t}(),^{},t )-_{_{0}}(_{t},^{},t )).\] (5)

As shown in Fig. 5 (d), the modified delta score can effectively alleviate the source score bias issue.

## 4 The DreamSteerer Method

### Editability Driven Score Distillation (EDSD)

Direct incorporation between an existing personalized model and DDS-S in Eq. 5 relieves the bias caused by inaccurate source score. However, as shown in Fig.5 (a) and (d), a deficiency in editability still persists, evident from the structural misalignment between source and edited images. We hypothesize that this issue stems from the misalignment between score estimations of the pre-trained and personalized models, underscoring the necessity for adjustments to the personalized model. An intriguing direction is to adjust the personalized model such that the objective of the DDS-S is further optimized. For this purpose, we first recover the loss from the gradient form of the DDS-S in Eq. 5 as \(_{}=_{t,}\|_{}(x_{t} (),^{},t)-_{_{0}}( _{t}^{},^{},t)\|_{2}^{2}\). For the purpose of editability enhancement, instead of performing score distillation w.r.t. the rendering parameter \(\), we perform score distillation w.r.t. the personalized model parameters \(\). Specifically, we first introduce a perturbation on the source latent state using the personalized DPM \(_{}\) as follows:

\[_{t}():=_{t}^{}-} (_{t}^{},y^{},t )-}_{},\] (6)

which is equivalent to the noised latent state of Tweedie's estimation [11; 15] of the personalized model \(_{}\) w.r.t. \(_{t}^{}\) (Sec. B).

Figure 2: Source class bias of DreamBooth trained for “plushie_tortoise”.

Then, we define the editability-driven loss as \(_{}=_{t,}[\|_{}(_{t }(),^{},t)-_{_{0}}(_{t}^{},^{},t)\|_{2}^{2}]\), which is similar to the \(_{}\) but with different parameters to optimize. To reduce the computational cost, with a slight abuse of notation, we use \(_{}L_{}_{}}{ }}{}\)

\[_{}_{}=_{t,} (t)(_{}(_{t}(),^{ },t)-_{_{0}}(_{t}^{ },^{},t)) (_{t}(),^{},t)}{_{ t}()}()}{},\] (7)

where all constants are absorbed in \((t)\). Intuitively, the perturbation introduced by the single denoising step in Eq. 6 can be interpreted as a small editing step on \(_{t}^{}\) by \(_{}\). Therefore, optimizing \(_{}\) can be understood as distilling information from the source model \(_{_{0}}\) into the personalized diffusion model \(_{}\) through \(_{t}()\). Specifically, without bias to the reference dataset, the source model \(_{_{0}}\) has better editability on the source image \(_{t}^{}\) than the personalized model \(_{}\), thus its score estimation, i.e., the noising prediction, tends to be more accurate. As \(_{}\) is trained on \(_{}^{}\) via personalization, in order for the personalized model \(_{}\) to achieve a similar score estimation, the "edited" \(_{t}()\) would be prone to have characteristics similar to the personal subject without losing the overall fidelity, through which enhancement in editability can be achieved. In Eq. 7, with parameter sharing, \(_{}\) naturally serves as the score estimator for \(_{t}()\), without requiring additional training. The detailed gradient flow is shown in Fig. 3.

### Mode Shifting regularization with spatial feature guided sampling.

The mode trapping issue of EDSD.Directly optimizing the personalized model parameter \(\) via Eq. 7 results in a peculiar characteristic observed in the edited and generated images, specifically the generated and edited results show patterns that fall between the reference images \(_{}^{}\) and the source image \(_{0}^{}\). As shown in Fig. 4 (f), when using a silver cat image as source and a brown cat as reference subject, the generated images reveal a hybrid appearance compared to the source model generations in Fig. 4 (e), showcasing features of both silver and brown cats. This observation suggests that EDSD has induced \(_{}\) to collapse to a trivial trapping point between the modes of the source image and the reference images.

Spatial feature guided sampling.To avoid such a mode trapping issue and maintain the concept of the personal subject, we regularize EDSD by jointly training the model on a set of personal subject images. Instead of accessing the reference dataset, we use images generated by the personalized model \(_{}(|\ ^{})\). We find that guiding the generated samples to have a structural layout akin to the source image \(_{0}^{}\) not only serves as an effective regularization but also shifts the model distribution \(p_{}(\ |\ ^{})\) towards a more editable region than the original mode centered around reference images.

Recent works [6; 72] show that the Self-Attention (SA) features of the T2I DPMs are embedded with detailed spatial information, thus having a strong sense of spatial layout that allows building inter-image semantic correspondence using these features [1; 80]. Motivated by these findings, we propose a spatial feature guided sampling strategy using these features from the source image.

As shown in Fig. 3, we begin by performing the DDIM inversion  on the source image using the pre-trained DPM \(_{_{0}}\). At each time step \(t\), for the \(\)-th SA layer from \(_{_{0}}\), an intermediate feature

Figure 3: Overall framework of DreamSteerer (the gradient flows are illustrated with dashed lines).

vector \(_{}(_{t})\) is extracted, which is further projected linearly into queries \(Q_{t}^{}=f_{Q}^{}(_{}(_{t}))\), keys \(K_{t}^{}=f_{K}^{}(_{}(_{t}))\) and values \(V_{t}^{}=f_{V}^{}(_{}(_{t}))\). The final output SA feature is computed via \(}_{t}^{}=(Q_{t}^{}(K_{t}^{K} )^{T}/})V^{}\), where \(C^{}\) is the channel size of the keys and queries. The output spatial features \(}_{t}^{}^{S^{} C^{}}\) are encoded with localized semantic information, where \(S^{}\) is the number of spatial locations in the \(\)-th layer. These features are cached and serve as guidance for sampling with the personalized model \(_{}(^{})\). Specifically, after running DDIM inversion with the source model \(_{_{0}}(^{})\), we obtain the inverted initial latent state of the source image \(_{T}^{}\), starting from which the reverse diffusion process is conducted using the personalized model \((^{})\). At each time step \(t\), the same set of spatial features \(\{_{t}^{}\}\) are extracted, and we aim to condition the model prediction on the source image \(_{0}^{}\) by matching the pairs \(}_{t}^{}\) and \(}_{t}^{}\) at corresponding spatial locations. For this purpose, following previous work , we compute the patchwise contrastive loss between the spatial features as follows

\[_{}(_{t},}_{t})=_{l= 1}^{L}_{s=1}^{S_{}}_{}(_{t}^{ ,s},}_{t}^{,s},}_{t}^{,S_{}  s}),\] (8)

where we treat the spatial feature vectors from the same spatial location as positive pairs, those from different spatial locations are treated as negative pairs and \(_{}(,^{+},^{-} )=-[^{+}/)}{( ^{+}/)+_{s=1}^{N}(_{ n}^{-}/)}]\) with some coefficient \(>0\). With Eq. 8, for the generation with the personalized model at time step \(t\), we model the likelihood of a noisy latent state \(_{t}\) matching the structural layout of the source image at time step t as \(_{_{t}} p_{}(}_{t}_{t}) _{_{t}}_{}(_{t}, }_{t})\). Following the Classifier Guidance , we modify the noise prediction of Classifier-Free Guidance as follows

\[_{}^{}(_{t},^{ },}_{t}) =_{}^{}(_{t},^{ })-_{t}_{_{t}}_{ }(_{t},}_{t})\] (9) \[-_{t}_{_{t}}[ p_{}( _{t}^{})+ p_{}( }_{t}_{t})],\]

where \(\) is a weight that balances two scores. Using such a spatial feature guided sampling strategy, we can sample a set of images \(^{}\). As shown in Fig. 4 (h), these images are prone to have a similar structure to the source image without losing the appearance of the personal concept \([s]\).

Figure 4: The effect of different regularization strategies on the editing and generation results of a DreamBooth baseline. The source prompt is “a photo of a cat sitting next to a mirror”.

**Mode shifting regularization.** With the set of guided samples \(_{}\), a mode shifting regularization term is jointly optimized with EDSD as

\[-(_{}_{}+_{} _{}),\;_{}:=_{ ^{}^{}}_{}(^{},^{};),\] (10)

where \(\) is the learning rate. As shown in Fig. 4 (g), the mode trapping issue in Fig. 4 (f) can be avoided with the mode shifting regularization in Eq. 10, where the generated images maintain appearance fidelity comparable to source model generations in Fig. 4 (e); meanwhile, the generations exhibit patterns akin to the source image. e.g., features like the blue collar, the presence of "two cats" and subject pose closely resemble those in the source image in Fig. 4 (a), indicating that the mode of \(p_{}()\) has been effectively steered to enhance editability for the source image. Furthermore, this enhancement is evidenced by the noticeable improvement in appearance fidelity of edited images from Fig. 4 (c) to Fig. 4 (d). See Fig. 3 for an overall framework.

### Automatic Subject Masking

Given the final steered personalized model \(_{}\) after optimization through Eq. 10, although Eq. 5 leads to pleasant target concept alignment,we observe that the subject irrelevant part may not be properly maintained due to the structural layout difference between source and target subjects as shown in Fig. 5. Inspired by recent work [21; 70] showing that the Cross-Attention (CA) maps concentrate on the relevant regions of the corresponding prompt token, we automatically extract subject masks \(M(_{0}^{})\) (refer to Sec. J for details) and we define such masked delta score as DDS-SM:

\[_{}_{}=_{t,} (t)M(_{0}^{})(_{}( x_{t}(),^{},t)-_{_{0}}(_{t}, ^{},t)),\] (11)

which better focuses on editing the subject-relevant regions as shown in Fig. 5 (g).

## 5 Experiments

Evaluation metrics.In accordance with the desired editing properties discussed in Sec. 3, we evaluate the effectiveness of DreamSteerer from three perspectives: 1) semantic similarity with the reference images using CLIP image similarity with CLIP ViT-B/32 and CLIP ViT-L/14 , 2) perceptual similarity with the source image using LPIPS with AlexNet  and VGG , 3) structural similarity with the source image using SSIM  and MS-SSIM . Additionally, to validate the editing fidelity of our proposed method, we report the No-Reference Image Quality Assessment (IQA) metrics Topiq , Musiq  and LIQE .

    &  &  &  &  \\  & (\(\)) & (\(\)) & Abs & \(\) & SSIM (\(\)) & MS-SSIM (\(\)) & Topiq & Mono & LIOE \\  
**Tetal inversion** & 0.785 & 0.346 & 0.113 & 0.181 & 0.8235 & 0.880 & 577 & 68.4 & 4.23 \\
**DreamSteer** & **0.788** (+.49) & **0.753**(+.95) & **0.115**(+13.45) & **0.187**(+.825) & **0.833**(+13.9) & **0.898**(+.26) & **393**(+2.58) & **703**(+2.58) & **4.39**(+3.85) \\  DreamBoth  & 0.794 & 0.751 & 0.294 & 0.247 & 0.758 & 0.785 & 604 & 70.4 & 4.35 \\
**DreamSteer** & **0.796**(+.25) & **0.761**(+.13) & **0.418**(+.19) & **0.229**(+.35) & **0.748**(+.15) & **0.815**(+.37) & **0.608**(+.65) & **71.9**(+2.15) & **4.44**(+2.15) \\  Custom Diffusion  & 0.781 & 0.743 & 0.199 & 0.229 & 0.767 & 0.808 & 591 & 60.8 & 4.34 \\
**DreamSteer** & **0.784**(+.19) & **0.246**(+.75) & **0.182**(+.89) & **0.227**(+.51) & **0.779**(+.15) & **0.833**(+.31) & **0.612**(+.36) & **73.1**(+.24) & **4.41**(+.5) \\   

Table 1: Comparison with different baselines (DreamSteerer uses the same model as baseline).

Figure 5: Illustration on the effect of the proposed components on editing with a DreamBooth baseline (1st row shows the editing results; 2nd row shows the editing directions, where brown means zero).

Implementation details.We evaluate our plug-in method on three personalization baselines: Textual Inversion , DreamBooth  and Custom Diffusion , which include the 3 mainstream types of models outlined in Sec. 2. We use the pre-trained checkpoints provided by DreamCatcher , with 16 concepts for each baseline encompassing living, non-living, in-door, and outdoor subjects. For each baseline, 70 random real-world images are used, focusing on the challenging editing scenarios as shown in Fig 1. For a fair comparison, all experiments use DDS-SM (Eq. 11) as the editing method.

Comparison with baseline methods.Table. 1 compares DreamSteerer against baselines [16; 39; 62]. Our work shows clear improvement in all 3 types of metrics, with substantial gains in the perceptual and structural alignment with the source images. Even for challenging editing scenarios (see Fig. 1) such as mirror reflections and significant structural changes, where baseline methods often result in distortions and unfaithful structure maintenance, our method effectively calibrates these issues to achieve high-fidelity results. Refer to Supp. F for more editing results. We find that the automatic metrics do not fully reflect the superior performance of our method compared to the baselines, particularly in terms of the quality of edited images. Therefore, we conduct a user preference study using the same criteria in Supp. L; our work is preferred by the users by a significant margin against the baselines.

Ablation study.We ablate EDSD and Mode Shifting Regularization to demonstrate their effectiveness in our framework. As shown in Table. 2 and Fig. 6, without EDSD, the CLIP-I scores remain comparable to those of the full model. However, the performance in source image alignment deteriorates significantly, as indicated by the lower LPIPS and SSIM scores. Without Mode Shifting

    &  & Mode &  &  &  &  \\  & & Shifting & CLIP B/32 & CLIP L/14 & Alex & VGG & & \\   & ✓ & ✓ & 0.788 & 0.753 & 0.115 & 0.167 & 0.833 & 0.898 \\  & ✗ & ✓ & 0.788 & 0.749 & 0.132 & 0.182 & 0.821 & 0.880 \\  & ✓ & ✗ & 0.779 & 0.746 & 0.090 & 0.144 & 0.850 & 0.922 \\   & ✓ & ✓ & 0.796 & 0.761 & 0.182 & 0.229 & 0.768 & 0.815 \\  & ✗ & ✓ & 0.794 & 0.755 & 0.200 & 0.246 & 0.753 & 0.792 \\  & ✓ & ✗ & 0.782 & 0.745 & 0.134 & 0.186 & 0.805 & 0.869 \\   & ✓ & ✓ & 0.784 & 0.746 & 0.182 & 0.227 & 0.779 & 0.833 \\  & ✗ & ✓ & 0.779 & 0.737 & 0.217 & 0.262 & 0.748 & 0.795 \\   & ✓ & ✗ & 0.760 & 0.730 & 0.100 & 0.152 & 0.841 & 0.917 \\   

Table 2: Ablation study on EDSD and Mode Shifting, the best and second best results are highlighted.

Figure 6: Ablation study on EDSD and Mode Shifting Regularization.

regularization, exceptionally high structural and perceptual alignment scores are achieved. However, as depicted in Fig. 6, this often results in a severe loss of target subject appearance information, reflected in consistently poor CLIP-I scores. Overall, combining EDSD and Mode Shifting Regularization achieves the best trade-off between source image alignment and target concept alignment. Fig. 5 provides a component analysis, showing that EDSD effectively improves the editability of the baseline DreamBooth model, resulting in higher structural alignment with the source image in terms of the edited results and the editing directions (refer to Supp. I).

One-shot performance.We further evaluate the performance of DreamSteerer against baselines under an extreme data-hungry scenario of one-shot personalization. We observe that Textual Inversion, which does not update the Diffusion Model parameters, cannot provide valid editing results under this setting. Therefore, DreamBooth and Custom Diffusion are used. As shown in Table 3, DreamSteerer maintains superior performance under these conditions. Notably, DreamBooth, relying on full fine-tuning, exhibits a severe bias towards the pose and structure of the reference image. Despite such strong bias, DreamSteerer improves its performance by a significant margin, as shown in Fig. 7.

More comparisons with existing works.See Sec. D for a discussion on the setting-level differences between our work and existing subject swapping works [17; 42]. We use a modified Delta Denoising Score as the base editing model, as this method provides stable editing results with all the personalized models we use. However, DreamSteerer is not restricted to a specific type of editing pipeline. We also evaluate the effectiveness of our method as a plug-in for personalized editing with Prompt-to-Prompt . See Sec. D for a comparison and a discussion on how Prompt-to-Prompt may be incompatible with certain base personalization models due to the limitations of the existing latent state inversion method.

## 6 Conclusion

In this work, we identify that existing T2I personalization models fail to deliver satisfactory image editing results. Therefore, we present DreamSteerer, an efficient plug-in method designed to enhance the editability of images conditioned on the source image. DreamSteerer fine-tunes the personalization parameters by training a novel Editability Driven Score Distillation objective under the constraint of a Mode Shifting regularization term based on spatial feature-guided samples. Through experiments, we show that DreamSteerer can significantly improve the editing fidelity of various existing baselines, particularly in challenging editing cases and data-hungry personalization scenarios. We consider DreamSteerer as a pivotal bridge from text-driven image editing to personalized image editing.

    &  &  &  \\  & CLIP-I & CLIP-I & & & & \\  DreamBooth  & 0.790 & 0.717 & 0.222 & 0.264 & 0.742 & 0.777 \\
**DreamSteerer** & **0.801** (+1.4\%) & **0.748** (+4.3\%) & **0.160** (-27.9\%) & **0.212** (-16.7\%) & **0.781** (+5.2\%) & **0.847** (+9.0\%) \\  Custom Diffusion  & 0.796 & 0.740 & 0.155 & 0.210 & 0.782 & 0.856 \\
**DreamSteerer** & **0.799** (+4.\%) & **0.753** (+1.8\%) & **0.145** (-6.4\%) & **0.200** (-4.7\%) & **0.796** (+1.7\%) & **0.873** (+1.9\%) \\   

Table 3: Comparison with baselines under one-shot scenario.

Figure 7: Comparison of one-shot performance.

Acknowledgement

Special thanks to Peter Tu for providing us with his valuable inputs and support that greatly helped improving our work and for motivating and encourage us on further implementations of our work. This research was, in part, funded by the U.S. Government - DARPA ECOLE HR00112390061 and DARPA TIAMAT HR00112490421. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.