# Harmonizing Stochasticity and Determinism:

Scene-responsive Diverse Human Motion Prediction

 Zhenyu Lou\({}^{1}\)   Qiongie Cui\({}^{2}\)

Tuo Wang\({}^{4}\)   Zhenbo Song\({}^{2}\)   Luoming Zhang\({}^{1}\)   Cheng Cheng\({}^{5}\)   Haofan Wang\({}^{3}\)

Xu Tang\({}^{3}\)   Huaxia Li\({}^{3}\)   Hong Zhou\({}^{1}\)

\({}^{1}\)Zhejiang University, \({}^{2}\)Nanjing University of Science and Technology, \({}^{3}\)Xiaohongshu Inc,

\({}^{4}\)University of Texas at Austin, \({}^{5}\)Concordia University,

11915044@zju.edu.cn cuiqongjie@126.com

Corresponding author

###### Abstract

Diverse human motion prediction (HMP) is a fundamental application in computer vision that has recently attracted considerable interest. Prior methods primarily focus on the stochastic nature of human motion, while neglecting the specific impact of the external environment, leading to the pronounced artifacts in prediction when applied to real-world scenarios. To fill this gap, this work introduces a novel task: predicting diverse human motion within real-world 3D scenes. In contrast to prior works, it requires harmonizing the deterministic constraints imposed by the surrounding 3D scenes with the stochastic aspect of human motion. For this purpose, we propose DiMoP3D, a diverse motion prediction framework with 3D scene awareness, which leverages the 3D point cloud and observed sequence to generate diverse and high-fidelity predictions. DiMoP3D can comprehend the 3D scene and determine the probable target objects and their desired interactive pose based on the historical motion. Then, it plans the obstacle-free trajectories toward these interested objects and generates diverse and physically consistent future motions. On top of that, DiMoP3D identifies deterministic factors in the scene and integrates them into stochastic modeling, making the diverse HMP in realistic scenes become a controllable stochastic generation process. On two real-captured benchmarks, DiMoP3D has demonstrated significant improvements over state-of-the-art methods, showcasing its effectiveness in generating diverse and physically consistent motion predictions within real-world 3D environments. More details and the video demo are available at the webpage https://sites.google.com/view/dimop3d.

Figure 1: Comparison of our DiMoP3D with the SoTA baseline . Purple meshes represent observations, and yellow meshes denote predictions. DiMoP3D produces high-fidelity, diverse sequences tailored to real-world 3D scenes, while BeLFusionâ€™s inadequate scene context integration leads to issues such as object penetration, motion incoherence, and scene inconsistency.

Introduction

Human motion prediction (HMP), _i.e.,_ forecasting future human poses based on observation, is crucial for applications including autonomous vehicles and human-robot collaboration [15; 20; 39; 44; 54; 60; 66]. Many existing works [1; 22; 40; 76; 89] formulate HMP as a deterministic problem, aiming to generate a single future sequence. However, it fails to capture the inherent stochasticity of human motion, where multiple plausible outcomes can arise from a single observation. Recent research has shifted towards diverse or stochastic HMP, to achieve multiple plausible predictions [3; 6; 30; 59; 72], which holds the potential in real-world applications and is the focus of our work.

Recent advances in diverse HMP primarily focus on stochastic predictions, where a random factor from the latent space conditions the diversity of predictions alongside observed motions [3; 12; 30; 59; 71; 72; 82]. While these methods predict multiple plausible futures from a single past motion, they typically disregard the 3D environment, operating within an idealized, context-free framework. This limitation becomes apparent in real-world applications, where motion must conform to physical and semantic scene constraints [29; 67; 68; 78; 90], leading to issues like obstacle penetration and unrealistic interactions, as in Figure 1. This gap underscores the need for a new task that merges diverse HMP within real-world 3D scenes, enhancing both practicality and applicability.

Recognizing existing limitations, this work introduces a novel task, making diverse HMP within real-world 3D scenes. Its objective is to break the previous idealized context-free setup towarding a realistic and practical setting, which involves several key challenges: **(1) Harmonizing Stochasticity and Determinism:** This task necessitates a delicate harmonization between the stochastic nature of human motion and the deterministic constraints from 3D scenes, thereby broadening the scope of traditional HMP; **(2) Scene-Motion Intermodal Coordination:** It requires analyzing coordination between human motion and scene dynamics to align predictions with contextual elements, which involves identifying human intentions and potential interactive objects; **(3) Behaviorally Coherent Physical Consistency:** The predictions must adhere to deterministic constraints, including physical consistency (_e.g.,_ avoid collision) and behavior coherence (_e.g.,_ sitting on a chair, not lying).

To tackle these challenges, we introduce DiMoP3D **(Diverse Motion Prediction in 3D Scenes), a framework for generating diverse, physically consistent, and plausible human motion predictions in real-world 3D scenes, which comprises three main components: (1) Context-aware Intermodal Interpreter analyzes potential areas of human interest and goals, essentially intentions within a scene. Our method enhances traditional scene understanding by integrating 3D point clouds with observed motions for context-aware intermodal analysis. It first encodes the point cloud, segments object instances, and then pinpoints potential interaction targets, emphasizing intended factors while filtering out less probable ones. This strategy further transforms the task of diverse HMP into a controllable stochastic generation process; (2) Behaviorally-consistent Stochastic Planner then constructs behaviorally consistent action plans, representing stochastic conditional factors. Recognizing that human interaction with specific objects often follows deterministic patterns, we prioritize predicting the final human pose upon reaching each target, and generate obstacle-free trajectories toward it; (3) Self-prompted Motion Generator harmonizes the stochastic nature of human motion with deterministic constraints in a self-prompted manner to produce varied predictions based on the conditional factor. To ensure coherence with this factor, it employs a denoising diffusion model, guiding the motion denoising process toward a deterministic, obstacle-free final state.

Our contributions are threefold: (1) We introduce a novel and challenging task of predicting diverse human motions within real-world 3D scenes, advancing beyond the traditional scope of diverse HMP from an idealized context-free setting to a more realistic and practical one. (2) We propose DiMoP3D to tackle this task. It harmonizes the deterministic constraints of 3D scenes with the stochastic nature of human motion, enabling diverse and plausible motion predictions in real-world scenarios. (3) Evaluations on two real-captured benchmarks, GIMO and CIRCLE, show that DiMoP3D significantly outperforms existing SoTA methods, particularly in terms of physical consistency.

## 2 Related Work

### Stochastic Human Motion Prediction

Human motion prediction diverges into deterministic and stochastic methods. Deterministic models aim to predict a singular sequence that closely aligns with future movements [18; 19; 40; 49; 76],yet often encounter quality degradation over longer time spans (\(>\) 1-\(sec\)) due to the stochastic nature of human movement. In contrast, stochastic HMP [2; 3; 5; 44; 82] embraces this variability by generating a range of plausible future motions and modeling the distribution of human behaviors. Notably, it also encompasses the most probable future sequence targeted by deterministic models, as a likely scenario within its broader distribution. Such methodologies enhance applications across autonomous driving [7; 33; 43], patient care [10; 42], and human flow prediction [32; 36], by embracing a wider range of potential scenarios and have become a focal point of contemporary research.

Dominant approaches include VAEs, GANs, flow networks, and diffusion models [3; 6; 12; 30; 38; 59; 71; 72; 82]. Despite promising progress in modeling stochastic human motions, a critical challenge persists: human motion is not only stochastic but also heavily influenced by the external environment. In real-world settings, human motion is intricately intertwined with surrounding scenes, necessitating that future trajectories comply with the scene's physical constraints (_e.g.,_ avoid object penetration). Additionally, predicted motions should be semantically consistent with the expected human-object interactions (_e.g.,_ sit for a chair, lie for a bed). Addressing these requirements calls for a sophisticated approach to diverse HMP that balances the stochastic aspect of human motion with the deterministic factors imposed by the surrounding 3D scenes, which is the focus of our work.

### 3D Scene Encoding and Scene-aware Motion Prediction

3D scene understanding is essential in various applications, prompting extensive research on representations like RGB-D maps [9; 52; 61], scene graphs [77; 79; 84], 3D voxels [23; 48; 74], and notably, 3D point clouds [55; 56; 65; 87]. Recognizing the importance of 3D scene information in human motion prediction [14; 21; 26; 35; 80], our work integrates 3D point clouds as the scene representation due to their direct derivation from sensing technologies.

To enhance fidelity of HMP in real-world scenarios, the connection between human actions and scene context has made scene-aware motion generation a major research focus [8; 17; 28; 88]. Early methods [8; 17; 67] relied on object bounding boxes, 2D images, and depth maps, which are insufficient for capturing real 3D environments. Recent advances use 3D point clouds for scene representation [29; 69; 88]: GIMO  employs a bidirectional transformer to fuse human motion and scene features,  predicts future contact maps, and  extracts global and local salient points.

However, these methods predict a single sequence [4; 50; 88], whereas our approach models a distribution of potential outcomes. Some stochastic methods add diversity, but  relies on 2D inputs, limiting 3D interactions, and  uses predefined objects without inferring targets. In contrast, our model parses the 3D scene through cross-modal, object-specific human interest analysis and predicts scene-aware motions in real 3D scenarios, greatly enhancing adaptability and authenticity.

### Motion Synthesis in 3D Scenes

Recent advancements in paired scene-motion data [4; 27; 88] have sparked a new research direction in synthesizing motions in 3D scenes [29; 41; 47; 53; 69; 78]. Specifically, SAMP  employs a conditional variational autoencoder (cVAE) to generate one frame per forward pass within a three-stage stochastic pipeline, COUCH  introduces a human-chair dataset with an auto-regressive, contact-satisfying method, and DN-Synt  proposes a hierarchical framework for effective scene-aware human motion synthesis. With the rise of language models, methods utilizing natural language prompts have emerged [16; 69; 75; 85]. HUMANISE  proposes an attention-based language-prompted synthesis method. Additionally, diffusion models have shown significant promise [16; 29; 37; 63; 64; 70]. Notably, AffordMotion  employs scene affordance as an intermediate representation, achieving state-of-the-art performance in object interaction synthesis.

Our novel task of scene-aware diverse HMP is partly inspired by advances in motion synthesis but is tailored for distinctly different applications and challenges. This task is further distinguished by key innovations: (1) Temporal-Dependent Prompting. Contrary to motion synthesis, which often lacks temporal context and follows static user instructions, our approach conditions predictions on the unique prompt of historical human motion, enabling autonomous inference of human intentions. (2) Context-Aware Scene Parsing. Moving beyond traditional scene understanding, our method infers potential movement targets through the integration of context-aware intermodal insights. Utilizing scene determinism to enhance the fidelity of diverse HMP predictions.

## 3 Method

### Problem Setup

Given \(L\) historical human poses \(_{1:L}=\{_{1},_{2},,_{L}\}\) within 3D scenes represented by point clouds \(^{n_{p} 6}\), our goal is to predict \(K\) different scene-consistent future motions \(\{}_{L:L+ L}^{(i)}\}_{i=1}^{K}\). Here, \(n_{p}\) denotes the number of points, each described by 3D spatial coordinates and RGB color information. \(L\) and \( L\) denote the lengths of the observed and predicted sequences, respectively. Each pose is described as the SMPL-X representation \(_{l}=(_{l},_{l},_{l})\), where \(_{l}^{3}\) denotes the global translation, \(_{l} SO(3)\) denotes the orientation, and \(_{l}^{32}\) refers to the body pose embedding. We set \(L=3\)-\(sec\) and \( L=5\)-\(sec\) to achieve a long-term prediction [40; 49]. Then, the task can be formulated as:

\[P(}_{L:L+ L}|_{1:L},)=_{}P(}_{L:L+ L}|_{1:L},,)P(|_{1:L},)d.\] (1)

The stochastic process \(\) is sampled jointly from the past motion \(_{1:L}\) and the scene \(\), and then utilized to condition the prediction motion \(}_{L:L+ L}\). We propose DiMoP3D to solve this novel task, which involves the following novelties: **Context awareness**: unlike traditional diverse HMP methods [12; 59; 71] that focus solely on human motion, our task is more challenging as it requires harmonizing the stochastic nature of human motion with the deterministic constraints of the surrounding 3D scenes. **Autonomous intention estimation**: different from motion synthesis, our task requires independent intention estimation based on past motion, to prompt future motion prediction.

### Context-Aware Interpreter

Scene information plays a crucial role in predicting future motion [8; 17; 67]. Despite the stochastic manner of human motion, it is still feasible to deduce human interests and likely goals within a scene. For instance, in Figure 2, the door behind the person is unlikely to be the target based on their trajectory away from it, while the sofa, coffee table, and distant chair may emerge as potential points of interest. Diverging from traditional scene parsing methods, our approach integrates a scene-motion intermodal coordination to better suppose human intentions in real-world settings.

Figure 2: The architecture of DiMoP3D. DiMoP3D incorporates two modalities of input, the past motion and the 3D scene point cloud. Initially, the Context-aware Intermodal Interpreter encodes the point cloud to features \(_{s}\), identifies interactive objects \(\), and uses a cross-modal InterestNet to pinpoint potential interest areas, sampling a target instance \(_{g}\) according to interest map \(M\). Following this, the Behaviorally-consistent Stochastic Planner forecasts the interactive human end-pose \(}_{L+ L}\), and devises an obstacle-free trajectory \(}^{plan}\) towards this pose. The sampled end-pose and trajectory are incorporated as a stochastic conditional factor to prompt the Self-prompted Motion Generator to generate physically consistent future motions.

In machine vision, 3D point clouds sourced directly from sensing devices have become fundamental for scene representation [55; 65; 87] and serve as the input for our scene interpreter. Acknowledging the significant influence of past motions and scene context on future human movements, we emphasize the need for a context-aware intermodal analysis that integrates historical motion with scene point clouds to infer potential human intentions. Furthermore, since human movements typically involve interactions with target objects [11; 24; 81], our approach identifies objects within the scene and computes a human interest score for each, rather than analyzing isolated points. This helps determine specific interactive targets, making the motion prediction process more controllable and enabling diverse prediction by sampling different interactive targets.

To achieve this, our scene interpreter employs a UNet-like encoder-decoder architecture , comprising an instance segmenter for object recognition and an interest net for human interest inference. These two modules share the same point cloud encoder for efficiency but use different decoders, as illustrated in the yellow box in Figure 2. The shared encoder processes and downsamples the point cloud \(\) into a compact feature representation \(_{s}^{n_{p}^{} c}\), with \(n_{p}^{}<\!\!<n_{p}\) (\(8 n_{p}^{} 50\) in our cases), and \(c\) represents the feature dimension. Subsequently, two decoders are employed to segment objects \(\), and predict human interests \(\) in the scene, respectively:

\[_{s}=(),\ \ =( _{s}),\ \ =(_{s},_{1:L}).\] (2)

Here, \(=\{_{1},_{2},...,_{n_{o}}\}\) denotes the set of \(n_{o}\) segmented objects, each being a subset of the scene pointcloud (\(_{i}\)). \(^{n_{p} 1}\) denotes the per-point interest map, with higher values indicating a greater likelihood of targeting specific scene elements. Once \(\) is obtained, we compute the probability \(P_{i}\) of each object \(_{i}\) being selected as an interaction target:

\[M_{i}=[p]\,/\,len(_{i}),\ \ p_{i},\] (3) \[\{P_{1},P_{2},...,P_{n_{o}}\}=(\{M_{1},M_{2},...,M_{ n_{o}}\};),\] (4)

where \(len(_{i})\) denotes the number of points in \(_{i}\), \([p]\) is the interest value of point \(p\), and \(=0.5\) represents the temperature factor that controls randomness in sampling. During inference, we sample the target object \(_{g}(g\!\!\{1,2,...,n_{o}\})\) based on the probability distribution \(\{P_{1},P_{2},...,P_{n_{o}}\}\).

In cases where there is no human-object interaction or objects are beyond reach within \( L\), no explicit target object exists. To handle this, we include the ground as a potential target, voxelized into smaller patches to improve granularity. Each ground patch, treated as an individual object, has a side length of \(s=0.5\) meters to balance accuracy and efficiency. The interest scores and interaction probabilities for ground patches are then calculated similarly to other objects, as in Eq.3 and Eq.4.

Our scene interpreter aligns observed motions with scene context, filtering out less likely engagement areas and identifying potential targets. We note that this approach makes the diverse HMP controlled by those deterministic elements of the scene, thereby enhancing the physical consistency of predictions. Representing human intention through scene-motion intermodal analysis, the selected target object \(_{g}\) directs the subsequent planning process, as outlined below.

### Behaviorally-Consistent Stochastic Planner

Ensuring collision-free and scene-consistent human behavior is crucial but often overlooked, while learning these patterns directly requires impractically large datasets. To tackle these challenges, we employ an action planner to plan obstacle-free trajectories toward the target \(_{g}\) and deterministically predict the interactive human end-pose \(}_{end}\) associated with \(_{g}\). These intermediate predictions serve as stochastic conditional factors, guiding to craft future motions that respect physical constraints and typical human-environment interactions while incorporating motion diversity.

To enable effective navigation and collision avoidance, we utilize a scene height map \(_{H}^{n_{s} 1}\) to delineate accessible areas and detect obstacles, inspired by [68; 73]. We first compute obstacle-free trajectories toward the target object \(_{g}\) using a modified A* algorithm (details on generating diverse trajectories are in Appendix B), and then employ a single-layer transformer \(\) to predict per-frame human velocity, sampling discretize points from the planned trajectory:

\[^{plan}=^{}(_{H};_{1:L},}_{end }),\ \ (}^{plan},t_{end})=Sample(^{plan},(_{1:L})).\] (5)

Here, \(^{plan}\) denotes the continuous trajectory, \(}^{plan}=\{}^{plan}_{L+1},}^{plan}_{L+2},...,}^{plan}_{L+ L}\}\) represents the sampled discretize trajectory points, and \(t_{end}\) is the estimated timestamp for the end of the interactive motion. Since the length of this trajectory varies across sequences, the human may not reach the target object exactly at the prediction horizon \( L=5\)-\(sec\). In these cases, if the target object is too distant to reach within \( L\) (\(t_{end}> L\)), we truncate the trajectory \(^{plan}\) to fit within \( L\) and adjust the target to the nearest ground patch. Conversely, if the target is reached too early (\(t_{end}< L\)), we keep the subject relatively static after \(t_{end}\). This adjustment ensures that the planned trajectory aligns with the prediction horizon. We then discretize the continuous trajectory to obtain the per-frame global translation \(}^{plan}\) to guide subsequent motion generation:

\[}^{plan}=\{}_{L+1}^{plan},}_{L+2}^{plan},...,}_{L+ L}^{plan}\}.\] (6)

In addition to physical constraints, human interactions with specific objects often follow deterministic patterns despite potential action diversity. For instance, "set," and "wipe," are reasonable actions for a table, whereas "sit" and "lie" are not. Traditional diverse HMP methods typically overlook these Human-Object Interaction (HOI) patterns, leading to motion and scene inconsistencies. Differing from these methods, our approach predicts the target object \(_{g}\) in advance, enabling us to predict the interactive HOI end-pose \(}_{end}\) before the full-sequence prediction:

\[}_{end}=(_{g}).\] (7)

This end-pose represents the final state of the prediction, secures appropriate human interaction with the scene. To this end, our planner constructs an obstacle-free trajectory, and the predicted end-pose (along with \(t_{end}\)) as a stochastic conditional factor \(\) from a scene-motion intermodal perspective:

\[=(}^{plan},}_{end},t_{end}).\] (8)

This factor further prompts the motion generator to predict behaviorally consistent motions.

### Self-Prompted Motion Generator

By constructing a stochastic conditional factor \(\) in advance, DiMoP3D operates as a self-prompted motion generator, which harmonizes the stochastic factor with deterministic motion generation rooted in \(\). To generate diverse predictions that closely align with the predicted conditional factor \(\), we utilize a motion diffuser, taking advantage of the diffusion model's ability to effectively guide intermediate results [12; 59; 71]. Additionally, to further maintain semantic coherence and physical consistency, we propose a semantic alignment inspector to supervise the denoising process.

For simplicity, we denote the sequence at noising step \(t\) as \(^{t}\). Diffusion is modeled as a Markov noising process \(\{^{t}\}_{t=0}^{T}\), with \(^{0}\) drawn from the data distribution, and:

\[q(^{t}|^{t-1})=(}^{t-1},(1- _{t})).\] (9)

Here \(_{t}(0,1)\). When \(_{T}\) approaches 0, we approximate \(_{1:L+ L}^{T}(,)\), where \(\) and \(\) represent the zero matrix and the identity matrix, respectively. To effectively integrate scene and motion features in our predictions, our diffusion model \(_{D}\) employs a transformer decoder to model distribution akin to the reversed diffusion process, leveraging its capacity for cross-modal attention. Instead of predicting noise, we predict the clean sample directly, following [63; 64; 83]:

\[}^{0}=_{D}(}^{t},_{s},t),\] (10)

where \(}^{0}\) represents the intermediate denoising result at each denoising step.

To align the predicted sequence with the observed \(_{1:L}\), the planned trajectory \(}^{plan}\), and the forecasted end-pose \(}_{end}\) at time \(t_{end}\), we adjust the corresponding segments after each denoising step. To be specific, for each frame \(}_{i}^{0}=(}_{i}^{0},}_{i}^{0},} _{i}^{0})\):

\[}_{i}^{0}=_{i}^{0}&i L,\\ (}_{i}^{plan},}_{i}^{0},}_{i}^{0})&L<i<L+t_{end}, i.\\ }_{L+ L}^{t}&i+t_{end},\\ }_{i}^{0}&i>L+t_{end},\] (11)

This modified prediction is then noised back before the next denoising step:

\[}^{t-1}=(}^{t-1},_{t}),\] (12)where \(_{t}\) represents the posterior variance based on \(\) at step \(t\). Upon completing \(T\) denoising steps, the motion generator yields a cohesive sequence \(}^{0}\), which integrates smoothly with the observed sequence and aligns with the planned trajectory and goals.

To enhance the consistency between the predicted motion and the target object, we introduce a semantic alignment inspector leveraging MotionCLIP . It computes a HOI semantic loss via natural language descriptions as follows:

\[_{sem}=_{l=1}^{L+ L}1-_{Motion}(}_{l:L+ L}^{t}),_{Text}( _{g}).\] (13)

Here, \(_{Motion},_{Text}\) denote MotionCLIP's motion and text encoders, respectively, with \(_{g}\) signifying the interaction description template related to the class of the sampled target object \(_{g}\).

Acknowledging that HOI predominantly occurs later in motion sequences, our semantic loss formulation weights later motion frames more heavily to accurately capture these interactions.

## 4 Experiment

### Experimental Setup

**Dataset-1: GIMO**, which records motion sequences represented by full-body SMPL-X poses with \( 129K\) frames. It consists of 14 scenes with 3D point clouds, each scene is captured by a 3D LiDAR sensor, containing 10-20 objects with \( 500K\) vertices. For a fair comparison, we follow the official split to divide the dataset into training and testing sets according to the scenes.

**Dataset-2: CIRCLE** comprises 10 hours of high-fidelity full-body motion sequences from 5 subjects across nine apartment scenes. Utilizing a Vicon system with 12 cameras at 120 FPS and the AI Habitat VR environment for virtual world simulation, CIRCLE achieves precise motion and scene capture. It offers an integrated apartment mesh, designating each room as an individual scene. The dataset encompasses motion sequences for 128 tasks, totaling over 7,000 sequences.

We also notice other related datasets [27; 28; 50], yet find limitations precluding their use (_e.g.,_ jittering, sequence length, absence of human meshes).

**Baselines.** Our DiMoP3D is compared with four contemporary methods: DLow , SmoothDMP , BeLFusion , and BiFU . DLow  applies a flow network, SmoothDMP  is VAE-based, and BeLFusion  is diffusion-based, which achieves SoTA performance in diverse HMP. These three, however, do not focus on scene-aware diverse HMP, setting BiFU , a deterministic scene-aware method, apart as an essential control for our analysis.

**Metrics.** To align with existing literature that evaluates human skeleton metrics, we utilize the SMPL model  to convert body parameters \(_{1:L+ L}\) into skeletons \(_{1:L+ L}=\{_{1},_{2},...,_{L+ L}\}\), where each \(_{i}^{n_{j} 3}\) represents a skeleton with \(n_{j}=22\) joints, following .

DiMoP3D is evaluated for diversity, accuracy, and physical consistency in scene-aware predictions. We begin with the well-established pipeline in : Prediction diversity is quantified using the Average Pairwise Distance **(APD)** by computing the L2 distance across predicted sequences. The Average Displacement Error **(ADE)** measures the reconstruction accuracy among the whole predicted sequence, while the Final Displacement Error (**FDE**) measures accuracy of the furthest frame, alongside their multimodal counterparts, **MMADE** and **MMFDE**, for diverse HMP scenarios.

To measure the physical consistency of the predicted motion within 3D scenes, an additional metric, the Average Cumulated Penetration Depth **(ACPD)** is introduced, following [78; 83]:

\[()=_{l=L+1}^{L+ L}_{n=1}^{n_{ j}}max-(_{l}[n],),\ 0\ ,\] (14)

where \([n]\) denotes the position of the \(n\)-th joint in the skeleton, and \((,)\) refers to the signed distance function  of the scene point cloud \(\). For training details, please refer to Appendix A.

### Main Results

Table 1 demonstrates DiMoP3D's superiority over the baseline methods across nearly all evaluation metrics on both datasets. The non-scene-aware methods (Dlow, SmoothDMP, BeLFusion) exhibit limited motion accuracy (ADE, FDE, MMADE, MMFDE) and physical scene consistency (ACPD), which we hypothesize is due to (1) lack of scene awareness, resulting in notable inconsistency in real-world applications, and (2) the absence of explicit motion goals, which hinders precise long-term (5-_sec_) motion forecasting. Despite their higher scores in diversity (APD), this is attributed to their erratic and unpredictable predictions, disregarding the scene context (detailed in Sec 4.4).

DiMoP3D's enhanced performance stems from three key factors: (1) Diversity. The stochastic conditional factor introduces diversity through multiple mechanisms: the intermodal interpreter sets broad motion objectives, the stochastic planner generates a variety of end-poses and trajectories, and the motion generator achieves diverse motion poses. This multi-faceted approach ensures a breadth of plausible actions are considered, enabling DiMoP3D to achieve a considerable APD score. (2) Accuracy. DiMoP3D outperforms every baseline in ADE, FDE, MMADE, and MMFDE for a large margin, even the deterministic BiFU. By estimating future human action based on a scene-motion intermodal analysis, DiMoP3D implicitly infers the subject's intent. This boosts the probability of accurately identifying the subject's genuine intent as the basis for prediction, thereby improving the prediction precision. The combination of accurate intermodal scene interpreting and stochastic planning ensures precise motion prediction for each sequence. (3) Physical consistency. Our motion generator employs a diffusion model, prompted by the predicted stochastic factors. It also ensures motion coherence through priors overwrite at each denoising step. This dual focus on deterministic constraints enables DiMoP3D to achieve superior motion-scene consistency.

This superior performance demonstrates the efficacy of our DiMoP3D in predicting diverse human motion in 3D scenes, as further evidenced by subsequent ablation studies and visualizations.

### Ablation Studies

In Table 2, we dissect the impact of excluding four pivotal components from DiMoP3D. First, eliminating InterestNet markedly decreases performance across ADE, FDE, MMADE, and MMFDE (first row). This decline stems from the process of selecting the target object \(_{g}\) from \(\), which reverts to random sampling without scene-motion crossmodal analysis, impairing DiMoP3D's ability to deduce human intentions. Consequently, the accuracy of predicting real actions diminishes, highlighting the significance of integrating multimodal scene-motion analysis for scene-aware HMP.

  & Ablation & APD\(\) & ADE\(\) & FDE\(\) & MMADE\(\) & MMFDE\(\) & ACPD\(\) \\   & 52.63 & 6.17 & 7.48 & 7.20 & 8.09 & 1.00 \\  & 46.97 & 5.95 & 7.27 & 6.72 & 7.84 & 1.53 \\  & **57.29** & 6.39 & **6.81** & 7.28 & 7.45 & 3.29 \\  & 47.79 & 5.82 & 6.82 & 6.75 & 7.46 & 1.06 \\  & 48.30 & **5.66** & **6.81** & **6.57** & **7.44** & **0.98** \\  

Table 2: Ablation of four main components in DiMoP3D over the sequences of the GIMO .

Addressing the role of stochastic planner, its absence undermines the planning of actions, including the prediction of end-poses by the HOI-Estimator and trajectory planning via the A* TrajectoryPlanner. Without these two components, the motion generator struggles to predict end-poses or future trajectories with scene consistency. Notably, omitting the HOI-Estimator results in imprecise interactive end-poses, often causing the subject to intersect with the target object in later frames, as evidenced by increased ACPD and reduced FDE and MMADE (second row). Similarly, excluding the TrajectoryPlanner significantly elevates ACPD (third row), indicating frequent subject penetrations into the scene context while approaching the end-pose. These findings underscore the vital role of coordinated end-pose and trajectory prediction in predicting motion within 3D scenes effectively.

Finally, the SemanticInspector enhances the scene-motion alignment through natural language, with its omission resulting in higher ADE and MMADE. Please refer to Appendix C for further ablations.

### Visualization

To delve deeper into DiMoP3D, in Figure 3, we showcase DiMoP3D's predictions across two scenarios, contrasting them with the SoTA baseline BeLFusion .

In bedroom scenario, the subject stands still behind the door. BeLFusion's predictions show notable issues with object and wall penetrations. Furthermore, sample-1 and 3 are marked by abrupt, illogical movements, and sample-3 and 4 display glaring scene inconsistencies: sample-3 has the subject sitting on the bare floor, and sample-4 involves the subject reaching for non-existent items. Conversely, DiMoP3D ensures physical consistency, directing each prediction towards a specific movement goal: opening a window, lying on the bed, accessing a cabinet, and sitting on a chair.

In the seminar room scene, the subject is moving forward. BeLFusion struggles again, with sample-1, 3, and 4 depicting the subject unrealistically exiting the room, and sample-2 showing an inconsistent motion of picking up a curtain. DiMoP3D, however, delivers high-fidelity predictions, depicting the subject walking through a door, grasping items, sitting, and pulling down curtains, respectively.

To emphasize DiMoP3D's predictive diversity, we further visualize various end-poses generated by the HOI-Estimator in Figure 4. Overall, DiMoP3D consistently delivers diverse, realistic, and physically-consistent motion predictions with clear objectives, benefiting from our conditional factor prediction schema which models human-object interactions and navigates obstacle-free trajectories.

Figure 3: Visual comparisons between DiMoP3D and SoTA BeLFusion in bedroom and seminar room scenarios. BeLFusionâ€™s predictions, which rely solely on past human motion without considering 3D scene context, are shown on the left. In contrast, DiMoP3D, displayed on the right, incorporates interactive goals and designs obstacle-free trajectories for each sequence. Purple meshes depict observed motions, while yellow ones signify predicted future motions. For clarity, distortions in BeLFusionâ€™s predictions are marked: red boxes for object penetration, green boxes for motion incoherence, and yellow boxes for scene inconsistency.

### Compared with Motion Synthesis Methods

In this section, we compare our DiMoP3D with three scene-aware motion synthesis approaches on GIMO : SAMP , DN-Synt , and AffordMotion . SAMP and DN-Synt utilize VAE architectures, while AffordMotion employs a diffusion-based model.

To adapt these methods for our diverse HMP task, we initialize motion synthesis from the last observed frame \(_{L}\) and encode the complete observed sequence \(_{1:L}\) into a unified embedding for historical motion conditions using a 2-layer transformer encoder, similar to the embedding technique in . Additionally, we introduce the FID metric, commonly used in motion synthesis, to evaluate the discrepancy between the distributions of generated and original dataset motions.

The results in Table 3 reveal an intriguing pattern: synthesis methods exhibit higher ADE than FDE. This occurs because, although these methods include explicit end-pose estimators yielding accurate final pose predictions, they struggle to condition on past motion. Consequently, they produce motion incoherence and significant prediction errors along the trajectory from the observation to the final pose. Notably, AffordMotion achieves the best APD and FID scores, which we attribute to its design that prioritizes fidelity over accuracy and allows a higher degree of freedom. Meanwhile, DiMoP3D also demonstrates competitive performance in these metrics. These findings underscore DiMoP3D's capability to harmonize the stochastic nature of human motion and the deterministic constraints from the scene and the past motion, achieving superior performance in diverse scene-aware HMP, and maintaining competitive diversity and fidelity even when compared to SoTA synthesis method.

## 5 Conclusion and Limitation

This work introduces a novel task of predicting diverse human motion in 3D scenes, along with a novel framework, DiMoP3D, to address it. By incorporating multimodal motion-scene analysis, DiMoP3D identifies areas or objects the subject is likely to interact with, enabling diverse, accurate, and physically consistent human motion prediction. Evaluated on the GIMO and CIRCLE datasets, DiMoP3D reduces ADE and FDE by nearly half compared to the state-of-the-art baseline BeLFusion, while maintaining high physical consistency. These results underscore the importance of scene awareness in diverse human motion prediction for real-world applications.

Despite its strong performance, DiMoP3D predicts motion in a fixed sequence length. When the actual sequence length differs, it either keeps the subject relatively static or truncates the sequence. Future work could explore variable-length motion prediction or end-to-end prediction, where the motion generator predicts sequence length and generates motion simultaneously.

Figure 4: Visualizations of diverse predicted end-poses across five object point clouds. The HOI-Estimator can generate a variety of human-object interactive poses tailored to specific scenarios.

 Method & APD \(\) & ADE \(\) & FDE \(\) & MMADE \(\) & MMFDE \(\) & FID \(\) & ACPD \(\) \\  SAMP  & 31.73 & 9.83 & 9.28 & 11.16 & 10.13 & 1.493 & 1.69 \\ DN-Synt  & 44.60 & 9.71 & 7.16 & 11.29 & 7.83 & 1.026 & 1.21 \\ AffordMotion  & **52.54** & 8.96 & 8.14 & 10.38 & 8.95 & **0.687** & 1.26 \\
**DiMoP3D** & 48.30 & **5.66** & **6.81** & **6.57** & **7.44** & 0.769 & **0.98** \\  

Table 3: Comparison of DiMoP3D with scene-aware motion synthesis methods.