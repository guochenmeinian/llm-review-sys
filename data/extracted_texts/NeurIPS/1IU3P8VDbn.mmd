# Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?

Haoang Chi\({}^{1,2}\), He Li\({}^{2}\), Wenjing Yang\({}^{2}\), Feng Liu\({}^{3}\), Long Lan\({}^{2}\), Xiaoguang Ren\({}^{1}\),

**Tongliang Liu\({}^{4}\), Bo Han\({}^{5}\)**

\({}^{1}\) Intelligent Game and Decision Lab, \({}^{2}\) National University of Defense Technology,

\({}^{3}\) University of Melbourne, \({}^{4}\) University of Sydney, \({}^{5}\) Hong Kong Baptist University

{haoangchi618,fengliu.ml}@gmail.com, rxg_nudt@126.com

{lihe_117,wenjing.yang, long.lan}@nudt.edu.cn,

tongliang.liu@sydney.edu.au, bhanml@comp.hkbu.edu.hk

Equal contributionCorresponding author

###### Abstract

Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (_level-_1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (_level-_2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in _level-_1 causal reasoning. To bridge the gap towards _level-_2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G\({}^{2}\)-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G\({}^{2}\)-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond _level-_1 and making strides towards _level-_2.

## 1 Introduction

The emergent of large language models (LLMs), such as GPT 4 , Gemini 1.5 , and Claude 3 , have significantly changed the paradigm of how people work and do research in recent years, demonstrating competitive abilities (LLMs) in instruction following , in-context learning , reasoning , coding  and etc. LLMs have demonstrated remarkable abilities in processing and generating human-like text, leading to a belief that they may possess the intelligence akin to human cognition. Reasoning is an essential component of human intelligence and a prominent characteristic that distinguishes humans from other species . Many recent workshave dived into evaluating and improving LLMs' general reasoning capabilities, such as logical reasoning  and mathematical reasoning .

In this work, we focus on causal reasoning, an advanced reasoning form . In the context of LLMs, causal reasoning is to discern the cause-and-effect relationships that govern the physical world from a text . Current LLMs appear to have demonstrated some degree of causal reasoning capabilities . Sometimes, when asked about the cause or effect of a given text, LLMs can accurately provide responses satisfying the laws of causality that originate from the physical world. For example, in Figure 1(a), the LLM can work out reasonable causes (i.e., "increased access to educational resources") for academic performance improvement. Faced with such encouraging performances, we have to pose a question:

_Does this reflect LLMs' genuine causal reasoning capability or only a "mirage"?_

The answer leans more towards the latter. We find that LLMs are adept at qualitatively solving causal reasoning tasks related to common knowledge (e.g., Figure 1 (a)), but they struggle to tackle more advanced task types (e.g., Figure 1 (b)), such as discovering new causal knowledge and estimating specific causal quantities. For example, in Figure 1(b), when asked about the effect of an imagined, unusual case ("developing railway stations as social hubs..."), the LLM's answer ("enhance public transportation accessibility") is clearly irrelevant with such an action. Recent works  also came to a similar conclusion. Given the performance differences on tasks of varying difficulty, we propose a hypothesis: the apparent (_level_-1) causal reasoning capabilities of LLMs can be primarily attributed to associated knowledge from their training corpora, rather than engaging in genuine, human-like (_level_-2) reasoning. We will justify this hypothesis from two aspects.

From a methodological perspective, the widely-used transformer-based LLMs essentially perform next-token prediction , which is realized in an autoregressive manner. Autoregressive model  originates from time-series analysis with an important assumption: the current value is determined by past values, not related to future values. For texts, however, the fact that the current token depends on the past tokens does not necessarily mean that there is a causal relationship between them. In addition, philosopher David Hume raised a viewpoint that sequential causality is not equivalent to logical causality . Therefore, this mechanism makes LLMs good at reusing the causal knowledge in their training corpora but often makes them struggle to comprehend and generate texts that capture genuine causal knowledge. We also use structural causal models (SCMs) to formally account for this intuitive conclusion (see Section 4.1).

From an empirical perspective, to validate the hypothesis that LLMs only possess _level_-1 causal reasoning capabilities, we introduce a new causal question & answer (Q&A) benchmark (Figure

Figure 1: The motivation of this work. (a) LLMs work well on common causal reasoning tasks, whose topics usually are widely discussed. (b) LLMs struggle to tackle rare tasks, whose corpora are possibly brand new for them. (c) Here is an example of the CausalProbe 2024 benchmark that is introduced to examine the true level of causal reasoning in LLMs, including an easy one-choice version (CausalProbe-E), a hard one-choice version (CausalProbe-H), and an uncertain multiple-choice version (CausalProbe-M). They have analogous formats but different construction strategies. (d) Compared to previous causal Q&A benchmarks, the studied LLMs exhibit a significant performance drop on CausalProbe 2024. (*) represents our benchmarks.

1(c)), named CausalProbe 2024, whose corpora was made public later than the release of the studied LLMs.3 Given that the training data is rarely disclosed, we can, at a minimum, guarantee that the corpora of CausalProbe 2024 is not included verbatim in the training data of the studied LLMs. Compared to earlier causal Q&A benchmarks, such as COPA , e-CARE  and CausalNet , all the studied LLMs (e.g., LLMaMA 2 7B, LLMaMA 3 8B, GPT 3.5 turbo, Claude 3 Opus) exhibit a significant performance drop on CausalProbe 2024 (Figure 1(d)). As the earlier benchmarks are potentially part of the training data, these longitudinal comparisons largely support our hypothesis.

Existing studies about LLMs' causal reasoning mainly focused on only assessments [15; 26; 49] and designing prompt-based approaches [4; 25]. These studies have taken a significant step forward in advancing LLMs' causal reasoning. However, they overlooked two basic principles of human reasoning: general knowledge and intention. For example, when we reason about a mathematical problem, we always take the basic axioms as a reference, with the ultimate goal as guidance. Inspired by this fact, we propose a causal reasoning framework for LLMs called G\({}^{2}\)-Reasoner, which incorporates general knowledge and goal-oriented prompts during reasoning. Specifically, we use the retrieval-augmented generation (RAG) to incorporate external knowledge bases. Then we stimulate the LLMs to consistently discern correct causal relationships in contexts to reach the final responses. Experiments show that G\({}^{2}\)-Reasoner significantly enhances LLMs' causal reasoning capabilities towards _level-2_, especially on fresh even fictitious tasks (e.g., CausalProbe 2024), which is consistent for both open-source and closed-source LLMs.

## 2 Related Work

In this section, we review the related works, including LLMs' reasoning, LLMs' causal reasoning, and LLMs' causal reasoning benchmarks. In Appendix D, we discuss the related works in detail.

### Reasoning in Large Language Models

Reasoning ability is crucial to LLMs' performance on tasks such as theorem proving, problem-solving, and robotics [41; 58].  propose a comprehensive review of the foundation models' reasoning, they discuss reasoning tasks including commonsense reasoning, mathematical reasoning, logical reasoning, and causal reasoning. Commonsense reasoning refers to the reasoning process that utilizes commonsense and daily life experiences . Several commonsense question-answering datasets have been proposed to test LLMs' commonsense reasoning ability [19; 29; 74; 75]. Causal reasoning is the process of identifying and understanding the cause-and-effect relationships between variables or events, which involves identifying potential causes and effects within a system or context . One distinctive feature of causal reasoning is that it involves counterfactual reasoning, which means reasoning within a hypothetical scenario. Our work focuses on the LLMs' causal reasoning, particularly counterfactual reasoning.

### Causal Reasoning in Large Language Models

Causal Reasoning tasks include causal discovery, cause attribution, and causal effect estimation [30; 58; 65]. Causal discovery aims to recover the latent causal structure of variables. Cause attribution refers to uncovering potential causes behind a process, while causal effect estimation aims to investigate the effect of cause variables [26; 76]. While LLMs already show the ability to uncover causal relationships from context, their abilities have some limitations [37; 4; 39; 77]. Counterfactual reasoning is an essential task of causal reasoning. The difference between counterfactual reasoning and other causal tasks is counterfactual reasoning involves reasoning in hypothetical scenarios [28; 58]. Several studies conclude that LLMs have limitations when encountering hypothetical scenarios in counterfactual reasoning [33; 66].

### Causal Reasoning Benchmarks in Large Language Models

There have been extensive studies on causal reasoning benchmarks. Existing causal reasoning benchmarks are mainly causal question-answering datasets.  employs language rules to extract causal questions from ten large question-answering datasets to form the CausalQA. CRAB  is a dataset that aims to assess LLMs' abilities to understand causal relationships among real-world events. FCR  is a human-labeled dataset that includes 24K question-answering pairs. Cladder  is a dataset that involves symbolic questions and corresponding ground truth answers,  employs causal graphs and structural causal models to generate the dataset. CausalProbe 2024 is different from the above benchmarks, as its contents are based on the latest and authoritative information, which is unlikely to be encompassed by the pre-training corpora of LLMs.

## 3 Problem Formalization

In this section, we introduce and clarify the necessary definitions used in this work. First, we provide a formal definition for causal reasoning in the context of LLMs. Then, we introduce two levels of causal reasoning capability to reveal the limitations of LLMs in this aspect. Last, we use causal language to depict the causal reasoning of LLMs.

In this work, the scope of causal reasoning of LLMs we study is reasoning about causal knowledge in textual form, distinguish from the numerical form in statistical causal inference .

**Definition 1** (causal reasoning in LLMs).: _In the context of large language models, the causal reasoning consists of two aspects:_

* _comprehend the given contexts and discern the causal relationship within them;_
* _responds to the causality-related queries, obeying the contexts and objective laws of causality._

To reach the major conclusions of this work, we first categorize LLMs'causal reasoning capability into two levels, motivated by the results of cognition science  and causality science .

**Definition 2** (_level-1 causal reasoning).: _Level-1 causal reasoning involves retrieving causal knowledge embedded in model parameters and contextual information. This form of reasoning is typically fast and well-suited for handling simple cause-and-effect relationships._

**Definition 3** (_level-2 causal reasoning).: _Level-2 causal reasoning leverages sophisticated reasoning mechanisms and internal parametric knowledge and contexts to deduce causal knowledge, including new/unseen causal knowledge. This form of reasoning is typically slow and capable of deriving new causal knowledge._

The above two definitions are inspired by 'Thinking, Fast and Slow' . _Level-2_ causal reasoning is not necessarily always better than _level-1_. Ideally, the interplay and adaptive switching and of these two levels of causal reasoning are crucial for LLMs to work both rapidly and reliably. In this work, we aim to explore the causal reasoning capabilities of current LLMs in terms of these two levels and dig into the underlying reasons.

**Remark 1**.: _In this work, we only consider a simple type of causal reasoning tasks that contain a single cause-effect pair. The cases of multiple cause-effect pairs and mediators are excluded. In addition, we primarily consider qualitative causal reasoning tasks (e.g., causal discovery ), rather than quantitative ones (e.g., treatment effect estimation ). In summary, the causal reasoning tasks that we focus on can be categorized into two type: 1) what is the reason \(\); 2) what is the result \(\). Due to the complexity of natural language, there are many different sentence patterns to express it._

Figure 2: An diagram of illustrating how autoregression fails to capture the correct causal knowledge.

## 4 LLMs cannot Perform Genuine Causal Reasoning

In this section, we aim to explore the real causal reasoning capability of current LLMs, in terms of our pre-defined two capability levels: _level-_1 (Definition 2) and _level-_2 (Definition 3). We study this problem from both a methodological perspective and an empirical perspective.

### Autoregressive LLMs are not Necessarily Causal

Although autoregressive LLMs have achieved great successes, recently, they began to be doubted by this community [34; 42]. We aim to study why the autoregressive mechanism prevents LLMs from acquiring _level-_2 causal reasoning capabilities. Essentially, the decoder-only transformer-based LLMs that are widely used today to perform next-token prediction, are trained with an autoregressive loss . In statistics, autoregression model is based on a fundamental assumption: in a sequence, the current value is determined by past values, not related to future values. However, causal knowledge expressed through natural languages does not necessarily satisfy this assumption. This is because sequential causality is not equivalent to logical causality . For example, due to the variability of natural language, sentence patterns may lead to false sequential causal relationships, and Figure 2 shows a toy instance. This sentence consists of four concepts, i.e., _"rain"_, "_school closure_", "_cannot go to school_", and "_learn the new programming language at home_". However, we can easily find that the sequential causal relationship in this sentence is totally incorrect. Thus, autoregressive LLMs suffer from capturing logical causal knowledge in complex texts, restricting their generalization abilities on unseen tasks. Note that due to the vast training corpus of LLMs, they may have encountered different textual expressions with the same semantic meaning, thus still being able to infer correct causal relationships in some complex tasks.

Assume that \(=(c_{1},c_{2},,c_{k})\) is a tokenized context and \((w_{1},w_{2},,w_{t})\) are already generated tokens, LLMs can obtain a distribution of the next token \(w_{t+1}\): \(P(w_{t+1}|,w_{1},w_{2},,w_{t})\), which is usually obtained by a softmax function. Then, LLMs predict the next token from this distribution in a sampling method, such as greedy sampling and beam search. Autoregressive training makes LLMs memorize common causal knowledge expressions. Based on the above discussion, there are two major issues: 1) If the context \(\) is not sequentially causal and unfamiliar for LLMs, they tend to misunderstand the causal knowledge in \(\); 2) If \(P(w^{*}_{t+1}|,w_{1},w_{2},,w_{t})\) is large4 but the text represented by \((,w_{t-1},w_{t},w^{*}_{t+1})\) is inconsistent with the laws of causality or the context \(\), LLMs tend to respond an incorrect causal reasoning result. In contrast, if the context \(\) and generated \((,w_{t-1},w_{t},w^{*}_{t+1})\) express correct causal knowledge and are familiar for LLMs, LLMs will perfectly address this task. Thus, the autoregression mechanism makes LLMs' causal reasoning primarily rely on correct causal knowledge in a large number of training corpora, i.e., the _level-_1 causal reasoning. In the following, we will validate this hypothesis from an empirical perspective.

Figure 3: The diagram of G\({}^{2}\)-Reasoner. This framework consists of two modules. One module is a retrieval-augmented generation (RAG) system to retrieve general knowledge that is related to the causal question. Another is a goal-oriented prompt to steer LLMs to race toward the ultimate goal of causal reasoning.

### Empirical Study on Causal Reasoning Capabilities of LLMs

In this section, we study the capability boundary of LLMs' causal reasoning for causal reasoning. Recall the hypothesis that LLMs are only capable of performing _level_-1 reasoning, primarily attributed to the causal knowledge embedded in their parameters during training, but struggle to perform genuine causal reasoning in complex or uncommon cases. Therefore, in order to physically avoid LLMs seeing the same corpora as the training data, we introduce a new causal question & answer (Q&A) benchmark, named CausalProbe 2024 (Figure 1(c)). The corpora used to construct CausalProbe 2024 are published later than January 1, 2024, which is later than the training data cut-off times of all the studied LLMs (Table 1). Thus, the studied LLMs should not have seen corpora that is the same or very similar to CausalProbe 2024. The construction details of CausalProbe 2024 are presented in Section 6.1. CausalProbe 2024 consists of three datasets: CausalProbe 2024 Easy (_abbr._, CausalProbe-E), CausalProbe 2024 Hard (_abbr._, CausalProbe-H), and CausalProbe 2024 Multiple-choice (_abbr._, CausalProbe-M). These three datasets progressively probe whether LLMs can solve novel causal reasoning tasks, whether they can distinguish misleading false causal relationships, and whether they can reason about multiple causal relationships within a scenario.

To validate whether LLMs heavily rely on the causal knowledge embedded in their parameters, we employ three earlier causal Q&A benchmarks as a comparison, i.e., COPA , e-CARE , and CausalNet . The corpora of COPA is released earlier than 2011. An example of their comparison is shown in Figure 9. The corpora of e-CARE (i.e., GenericsKB ) is released earlier than 2020. CausalNet is also a fresh causal Q&A benchmark and its corpora are generated by ChatGPT, serving as an intermediate comparison. The corpora of COPA and e-CARE are likely to exactly be the training data of the studied models. For CausalNet, we have reason to speculate that its corpora may be similar to the training data of ChatGPT, although a large temperature hyperparameter can bring some creativity. Their detailed introduction in presented in Appendix F. In terms of the format, CausalProbe 2024 is consistent with them, including _ID_, _question_, _choices_, and _answer_. In addition, CausalProbe 2024 additionally provides contexts as the background knowledge of questions. In Section 6.1, we discuss the reasonability of providing contexts rather than only providing questions.5 To doubly guarantee the freshness of CausalProbe 2024, we employ a LLM's membership inference attack method, Min-\(K\)% Prob , to detect how much the corpora of a benchmark potentially comes from a LLM's training data. The results are shown in Table 3, showing that the corpora of CausalProbe 2024 are fresher for the studied LLMs than other benchmarks.

From Figure 1(d), we can observe significant performance drops on CausalProbe-E and CausalProbe-H for all four studied LLMs. Especially for CausalProbe-H, Claude 3 opus, a current SOTA LLM, only achieves the average exact match less than 70%. The popular and competitive open-source LLM, LLaMA 2 7B chat, just get it half right. As the corpora of CausalProbe 2024 comes from news, it is close to everyday life and hardly consists of professional concepts and unfamiliar words. Thus, the main cause of performance degradation is the freshness of corpora, indicating the fact that LLMs only are capable of doing _level_-1 causal reasoning, instead of genuine _level_-2 causal reasoning. The empirical result is in perfect agreement with the analysis derived from the autoregressive mechanism.

## 5 \(}\)-Reasoner: A General-Knowledge-Assisted and Goal-Driven Reasoner

Until now, we have discovered of the limitation of LLMs' causal reasoning capabilities from both methodological and empirical perspectives. To move towards _level_-2 causal reasoning, we seek inspiration from human beings. Human reasoning processes, including causal reasoning, are driven by specific reasoning tasks and supported by extensive foundational knowledge acquired throughout life, following established reasoning patterns such as deductive and inductive reasoning . When

  LLaMA 2 7B chat & LLaMA 3 8B instruct & GPT 3.5 turbo & Claude 3 opus & CausalProbe 2024 \\  Sep 2022 & Mar 2023 & Sep 2021 & Aug 2023 & Jan 2024 \\  

Table 1: The data cut-off time comparison of the studied LLMs and CausalProbe 2024 benchmark.

solving plane geometry proof problems, students apply three basic axioms as criteria while working toward the target proposition. Unlike humans, LLMs perform causal reasoning through next-token predictions based solely on patterns learned during training, without necessary knowledge to guide their reasoning. Drawing inspiration from causal inference, we formalize textual causal reasoning tasks6 using a causal graph, as shown below.

Causal model for causal reasoning in LLMs.Based on Definition 1, we use a causal graph (Figure 4) to formally depict the textual causal reasoning task of LLMs. Based on this formalization, we can identify the key factors for LLMs to perform causal reasoning well. Given a cause semantic variable \(X\) and an effect semantic variable \(Y\), naturally, we have \(X Y\). The fact that \(X\) causing \(Y\) is driven by the laws of the physical world or imagined/virtual worlds \(C\). Then, the semantic variables \(X\) and \(Y\) are transformed into a variable \(T\) that represents the natural language, through a mapping \(h\), i.e., \(h(X,Y,)=T\), where \(\) is a random exogenous variable. In this formalization, \(h\) can be viewed as a language system to transform two causal concepts into a paragraph of text. The variable \(\) represents various factors in generating readable text from causal concepts \(X\) and \(Y\), such as language type, context, and mode of expression (e.g., active or passive voice). While linguistic variability (\(\)) enriches the diversity and flexibility of natural language, it poses challenges for LLMs' causal reasoning. Consider the causal relationship between "smoking" (\(X\)) and "lung cancer" (\(Y\)). This relationship can be expressed in various ways, such as: 1) "A history of smoking is a common risk factor for lung cancer," and 2) "Knowing that smoking greatly increases the risk of lung cancer, why take the risk?" Although these sentences convey the same underlying causal relationship, they differ significantly in their linguistic structure and expression (i.e., different \(\)). The natural language expression \(T\) encapsulates both the fundamental causal relationship between \(X\) and \(Y\), as well as the contextual nuances and linguistic style represented by \(\).

We can easily find that the causal graph (Figure 4) is exactly a causal model with a confounding variable and a conditioned collider . \(T\) is conditioned because the observed natural language descriptions in textual causal reasoning tasks are inherently deterministic. Following a fundamental principle of causal inference, when the collider \(T\) is conditioned (\(T=T_{0}\)), it creates an association between \(X\) and \(Y\). Thus, natural language descriptions of causal reasoning tasks provide valuable information for determining causal relationships.6 For cause-to-effect tasks, given \(P_{Y}\) (the distribution of possible effects \(Y\) generated by LLMs), our objective is as follows:

\[_{Y P_{Y}}[Y|X=X_{0},T=T_{0},C]\] (1)

where \(X_{0}\) is the cause described in a causal reasoning question. However, to account for the confounding variable \(C\), we can apply the total probability formula, i.e.,

\[_{Y P_{Y}}_{C P_{C}}[Y|X=X_{0},T=T_{0},C] =_{Y P_{Y}}[Y|X=X_{0},T=T_{0}],\] (2)

where \(P_{C}\) is the general knowledge base. While strictly ensuring the validity of Eq. (2) would require a complete general knowledge base, which is impractical, Eq. (2) nonetheless provides an insightful approach to addressing causal reasoning problems in LLMs and is helpful in practice (Section 6.3).

Goal-driven prompt.As discussed in Section 4.1, the autoregressive nature of LLMs hinders their understanding of correct causal relationships. As generated sequences lengthen, LLMs tend to lose coherence and deviate from their initial targets . To maintain focused generation, we design a goal-driven prompt that guides LLMs in identifying correct causal relationships during the decoding process, as shown in Figure 3.

Motivated by human reasoning mechanism and causal graph theory, we propose a novel causal reasoning framework (Figure 3), named G\({}^{2}\)-Reasoner, which involves general knowledge as a basis and intended goals as a guide. Specifically, G\({}^{2}\)-Reasoner leverages a small (\(\)16 Mb) general knowledge Q&A dataset8 as the knowledge base, enabling the model to draw upon related knowledge as a reference when performing causal reasoning tasks. Specifically, when querying LLMs a causal reasoning question, we first retrieve related general knowledge from a vector database constructed with the general knowledge dataset, through the pipeline of retrieval-augmented generation (RAG). Using the retrieved general knowledge as a basis, we employ the proposed goal-oriented prompt to steer LLMs to perform causal reasoning in a targeted manner, rather than aimlessly generating answers. This is the first step taken to advance LLMs towards _level_-2 causal reasoning.

## 6 Experiments

In this section, we will discuss two points: 1) the construction of the CausalProbe 2024 benchmark and its superiority; 2) the main implementation details ; 3) the result analysis of G\({}^{2}\)-Reasoner and baseline methods. The CausalProbe 2024 benchmark and the source codes are presented in this URL: https://github.com/Haoang97/CausalProbe-2024.

### Construction of CausalProbe 2024 and its superiority

We will introduce the construction procedure of the CausalProbe 2024 benchmark and provide a brief analysis. In general, the procedure contains two stages: 1) collecting the latest web articles to form a corpora; 2) employing an LLM to generate the question-answer pairs from the corpora.

Corpora collection.To ensure the quality of the collected corpora, we choose two well-known media: British Broadcasting Corporation (BBC) and the Guardian.9 BBC [Link] is a British public service broadcaster founded in 1922, one of the oldest and largest broadcasting companies in the world. The Guardian [Link] is a national newspaper in the UK, established in 1821, with its online edition being particularly influential. We downloaded the latest articles from BBC and the Guardian. Specifically, the downloaded articles are from **January 1, 2024, to April 29, 2024**, later than the releases of the studied LLMs (Figure 1). The articles cover categories such as technology, environment, business, health, world news, culture, and climate, statistics of the downloaded articles are shown in Table 10.

    & LLMa 2 LLMa 3 \\   &  & 13.27 & 16.64 \\  & Min-20\% Prob & 10.57 & 12.21 \\  & Min-30\% Prob & 8.97 & 10.32 \\   &  & 13.08 & 14.48 \\  & Min-20\% Prob & 11.20 & 12.98 \\  & Min-30\% Prob & 9.92 & 10.89 \\   &  & 10.84 & 11.3 \\  & Min-20\% Prob & 8.84 & 9.45 \\  & Min-30\% Prob & 7.51 & 8.00 \\   &  & 9.34 & 9.03 \\  & Min-20\% Prob & 7.27 & 7.29 \\  & Min-30\% Prob & 5.90 & 5.69 \\   &  & 9.93 & 9.70 \\  & Min-20\% Prob & 7.86 & 7.77 \\   & Min-30\% Prob & 6.65 & 6.49 \\   

Table 3: Results of training data detection using Min-\(K\)% Prob . We conduct this evaluation on LLaMA 2 7B and LLaMA 3 8B. The metric is the average negative log-likelihood on a dataset. A smaller value indicates better freshness. “C-E”, “C-H” represent CausalProbe-E and CausalProbe-H.

    & LLaMA 2 LLaMA 3 \\   &  & 13.27 & 16.64 \\  & Min-20\% Prob & 10.57 & 12.21 \\  & Min-30\% Prob & 8.97 & 10.32 \\   &  & 13.08 & 14.48 \\  & Min-20\% Prob & 11.20 & 12.98 \\  & Min-30\% Prob & 9.92 & 10.89 \\   &  & 10.84 & 11.3 \\  & Min-20\% Prob & 8.84 & 9.45 \\  & Min-30\% Prob & 7.51 & 8.00 \\   &  & 9.34 & 9.03 \\  & Min-20\% Prob & 7.27 & 7.29 \\  & Min-30\% Prob & 5.90 & 5.69 \\   &  & 9.93 & 9.70 \\  & Min-20\% Prob & 7.86 & 7.77 \\   & Min-30\% Prob & 6.65 & 6.49 \\   

Table 3: Results of training data detection using Min-\(K\)% Prob . We conduct this evaluation on LLaMA 2 7B and LLaMA 3 8B. The metric is the average negative log-likelihood on a dataset. A smaller value indicates better freshness. “C-E”, “C-H” represent CausalProbe-E and CausalProbe-H.

Bechmark construction.We generated two sub-datasets from the same corpora using two different strategies. Since it's expensive to create them manually, we employ GPT 3.5 turbo as an assistant to automatically generate them. The overall pipeline diagram is shown in Figure 8.

* **CausalProbe-E**. We construct this dataset following the format of the CausalQA . In detail, we first query the GPT 3.5 turbo to analyze an example data in CausalQA, then we offer GPT 3.5 turbo articles and ask it to generate Q&A pairs similar to the example data. Finally, the highest-quality Q&A pairs are selected as the reference, which is adopted in the prompt template of generating Q&A data. The related prompt templates are shown in Figure 13(a), 13(d).
* **CausalProbe-H**. We construct CausalProbe-H with another strategy. Specifically, first, we provide GPT 3.5 turbo an example article and ask it to summarize this article; second, we ask it to extract several cause-effect pairs based on the summary; third, we ask it to create some incorrect cause-effect pairs regarding this summary; finally, multiple multi-choice questions are generated according to these correct and incorrect cause-effect pairs. The highest-quality multi-choice Q&A data is selected as the reference, which is adopted in the prompt template of generating Q&A data. The related prompt templates are shown in Figure 13. The made-up fake cause-effect pairs can be used to examine the LLMs' genuine causal reasoning capability when encountering counterfactual disturbance term.
* **CausalProbe-M**. We construct an uncertain multiple-choice version, with the number of correct answers for each question ranging from 1 to 4. LLMs are required to distinguish the correctness or incorrectness of each causal proposition, avoiding LLMs relying on random guess to get correct answers. We provided GPT-40 mini10 the prompt templates of CausalProbe-H and additional prompt to realize varying number of answers, shown in Figure 13. To ensure each question really has at least one correct answer, we manually checked the correctness of the provided answers. 
Due to the broad range of topics in the corpus, it may contain ethically questionable content such as conflicts. Therefore, we used LLMs to filter the unethical questions from the preliminary generated data. Moreover, to comply with the context length limit of GPT 3.5 turbo, we removed articles exceeding 15,000 characters. After filtering, our dataset contains 3,461 unique Q&A data. We also provide statistical analysis and more details of CausalProbe 2024 in Appendix I.

Superiority of CausalProbe 2024.1) Contextual information.As we have simply mentioned, earlier three causal Q&A benchmarks lack the necessary background knowledge for each question. Although these benchmarks greatly promote the development of LLMs' causal reasoning, their formats are a little bit unreasonable. Even for humans, when we perform reading comprehension tests, we also need full context as a reference. Although we find that many of these questions can be answered by us, this heavily relies on our knowledge reserve, which is similar to LLMs. The knowledge reserve contributes their _level-_1 causal reasoning capability. Thus, we involve the contexts into CausalProbe 2024 as briefly as possible, which is more realistic, and they significantly improves the performance (Figure 5 and 6). **2) Hierarchical capability assessment.** Three datasets of CausalProbe 2024 hierarchically assess causal reasoning ability levels. CausalProbe-E assesses the genuine causal reasoning ability on novel problems. Furthermore, CausalProbe-H assesses the ability to identify misleading or deceptive causal propositions in new tasks. Last, CausalProbe-M assesses the ability to identify multiple valid causal statements in novel tasks, greatly ensuring that LLMa cannot obtain right answers through random guessing. By evaluating LLMs on these three datasets in sequence, we can determine their actual level of causal reasoning ability.

Guarantee the freshness of CausalProbe 2024.To doubly guarantee the freshness of CausalProbe 2024 than earlier benchmarks, we use an LLM's training data detection method, Min-\(K\)% Prob , to evaluate them as mentioned before. The detailed introduction for this method is in Appendix E. As the APIs of GPT and Claude no longer provide the logarithmic likelihood input texts, we only make this evaluation on open-source LLMs. The results in Table 3 show that CausalProbe 2024 is fresher. Here \(K\) denotes the tokens in the bottom \(K\) percent of log-likelihood.

We employ several measures to control the data quality of CausalProbe 2024. The detailed information is shown in Appendix H.

### Implementation Details

All the experiments are conducted on the Ubuntu 20.04 system and NVIDIA RTX A6000 GPUs. For closed-source LLMs, i.e., GPT 3.5 turbo and Claude 3 opus, we call the API provided by their officials. We provide more implementation details in Appendix C.

### Result Analysis

To evaluate G\({}^{2}\)-Reasoner, we compare it with three common LLM reasoning methods, i.e., vanilla, chain-of-thought (CoT) , and retrieval-augmented generation (RAG) . The "vanilla" refers to directly perform zero-shot inference. We evaluate four LLMs on CausalProbe 2024 and other three causal Q&A benchmarks, whose detailed introduction is presented in Appendix F. The full results are shown in Table 2. First, the LLMs' performance decreases monotonically as the benchmark corpora become more fresh. A counterintuitive phenomenon is that CoT usually perform a little worse than vanilla on three earlier benchmarks. This may indicate that CoT cannot improve reasoning on simple or common tasks for LLMs. For G\({}^{2}\)-Reasoner, it mostly can outperform the vanilla method. However, limited by the scale of the general knowledge base and the power of vector databases, G\({}^{2}\)-Reasoner cannot significantly outperform the baselines, which is the direction of our future efforts. In fact, RAG is just the ablated G\({}^{2}\)-Reasoner, and it usually cannot reach the vanilla method, indicating the effectiveness of our goal-oriented prompt.

We evaluate the four LLMs on CausalProbe-M, where they showed a more significant performance decline compared to their results on CausalProbe-E and -H. Under exact match, which required all correct answers to be precisely identified, all models struggled. However, when using partial match, where missing some correct options was acceptable but selecting incorrect options was not, GPT and Claude performed relatively well, achieving accuracy rates of approximately 75% and 85% respectively. While these results demonstrate that current LLMs cannot fully comprehend each causal proposition, revealing limitations in their causal reasoning abilities, there is an encouraging finding: the models rarely make false positive errors when identifying causal relationships.

Note that G\({}^{2}\)-Reasoner's performance relies on general knowledge bases. The reported results were obtained using a very small knowledge base (around 16 MB), yet G\({}^{2}\)-Reasoner generally achieved non-marginal improvements. If we use a significantly larger one, such as Wikipedia API, performance can be boosted a lot. However, due to resource constraints, we couldn't repeat all experiments with it.

## 7 Conclusion and Future Outlook

This work investigates the causal reasoning capabilities of LLMs and argues that current LLMs are limited to _level_-1 causal reasoning. To verify this hypothesis, we introduce a new causal Q&A benchmark, CausalProbe 2024, revealing that LLMs struggle with causal reasoning in unseen contexts and primarily rely on the causal knowledge in training data. To fill this gap, we proposes G\({}^{2}\)-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into causal reasoning of LLMs. Experiments demonstrate that G\({}^{2}\)-Reasoner can enhance the causal reasoning capability, particularly in fresh and counterfactual contexts. This work provides valuable insights into the current state of LLMs' causal reasoning and offers a promising attempt to move towards _level_-2 causal reasoning, bringing LLMs closer to reaching genuine causal reasoning capabilities. While this work takes an important step forward, it still does not enable LLMs to achieve _level_-2 causal reasoning. Further research in this field could potentially lead to significant advancements towards stronger artificial intelligence.

Acknowledgements

This work was partially supported by the National Natural Science Foundation of China (Grant No. 62372459, No. 62376282, No. 91948303). We express our sincere gratitude to Dr. Jie Tan for her valuable discussions and support from the National Natural Science Foundation of China (Grant No. 62402499).