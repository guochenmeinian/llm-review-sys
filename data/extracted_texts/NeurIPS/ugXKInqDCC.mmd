# AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies

Xixi Hu, Bo Liu, Xingchao Liu, Qiang Liu

The University of Texas at Austin

{hxixi,bliu,xcliu,lqiang}@cs.utexas.edu

###### Abstract

Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution is uni-modal. Our comprehensive empirical evaluation shows that AdaFlow achieves high performance with fast inference speed.

## 1 Introduction

Imitation Learning (IL) is a widely adopted method in robot learning [1; 2]. In IL, an agent is given a demonstration dataset from a human expert finishing a certain task, and the goal is for it to complete the task by learning from this dataset. IL is notably effective for learning complex, non-declarative motions, yielding remarkable successes in training real robots [3; 4; 5; 6].

Figure 1: AdaFlow is a fast imitation learning policy. It adaptively adjust the number of simulation steps when generating actions. For low-variance states, it functions as a one-step action generator. For high-variance states, it employs more steps to ensure accurate action generation. This adaptive approach enables AdaFlow to achieve an average generation speed close to one step per task completion.

The primary approach for IL is Behavioral Cloning (BC) [7; 8; 9; 10], where the agent is trained with supervised learning to acquire a deterministic mapping from states to actions. Despite its simplicity, vanilla BC struggles to learn diverse behaviors in states with many possible actions [11; 10; 6; 12]. To improve it, various frameworks have been proposed. For instance, Implicit BC  learns an energy-based model for each state and searches the actions that minimize the energy with optimization algorithms. Diffuser [13; 14] and Diffusion Policy  adopts diffusion models [15; 16] to generate diverse actions, which has become the default method for training on large-scale robotics data [17; 18; 19; 20].

The computational cost of the learned policies at the execution stage is important for an IL framework in a real-world deployment. Unfortunately, none of the previous frameworks enjoys both efficient inference and diversity. Although energy-based models and diffusion models can generate multi-modal action distributions, they require _recursive processes_ to generate the actions. These recursive processes usually involve tens (or even hundreds) of queries before reaching their stopping criteria.

In this paper, we propose a new IL framework, named AdaFlow, that learns a dynamic generative policy that can autonomously adjust its computation on the fly, thus cheaply outputs multi-modal action distributions to complete the task. AdaFlow is inspired by recent advancements in flow-based generative modeling [21; 22; 23; 24]. We learn probability flows, which are essentially ordinary differential equations (ODEs), to represent the policies. These flows are powerful generative models that precisely capture the complicated distributions, but similar to energy-based models and diffusion models, they still require _multiple recursive iterations_ to simulate the ODEs in the inference stage.

AdaFlow differs from existing flow generative models like Rectified Flow  and Consistency Models , by utilizing the _initially learned ODE_ to maintain low training and inference costs, and function as a one-step generator for deterministic target distributions. In contrast, both of these methods require an additional distillation or reflow  process to achieve fast inference. To improve the efficiency, we propose an adaptive ODE solver based on the finding that the simulation error of the ODE is closely related to the variance of the training loss at different states. We let the action generation model to output an additional variance scalar alongside the action it produces. During the execution of the policy, we change the step size according to the variance predicted at the current state. Equipping the flow-based policy with the proposed adaptive ODE solver, AdaFlow wisely allocates computational resources, yielding high efficiency without sacrificing the diversity provided by the flow-based generative models. Specifically, in states with deterministic action distributions, AdaFlow generates the action in _one step_ - as efficient as naive BC.

Empirical results across multiple benchmarks demonstrate that AdaFlow achieve consistently good performance across success rate with high execution efficiency. Specifically, our contributions are:

Figure 2: Illustrating the computation adaptivity of AdaFlow (orange) on simple regression task. In the upper portion of the image, we use Diffusion Policy (DDIM) and AdaFlow to predict \(y\) given \(x\), with deterministic \(y=0\) when \(x 0\), and bimodal \(y= x\) when \(x>0\). Both DDIM and AdaFlow fit the demonstration data well. However, the simulated ODE trajectory learned by Diffusion-Policy with DDIM (red) is not straight no matter what \(x\) is. By contrast, the simulated ODE trajectory learned by AdaFlow with fixed step (blue) is a straight line when the prediction is deterministic (\(x 0\)), which means the generation can be exactly done by one-step Euler discretization. At the bottom, we show that AdaFlow can adaptively adjust the number of simulation steps based on the \(x\) value according to the estimated variance at \(x\).

* We proposed AdaFlow as a generative model-based policy for decision making tasks, capable of generating actions almost as quickly as a single model inference pass.
* We conducted comprehensive experiments across decision making tasks, including navigation and robot manipulation, utilizing benchmarks such as LIBERO  and RoboMimic . AdaFlow consistently outperforms existing state-of-the-art models, despite requiring 10x less inference times to generate actions.
* We offer a theoretical analysis of the overall error in action generation by AdaFlow, providing a bound that ensures precision and reliability.

## 2 Related Work

Diffusion/Flow-based Generative Models and Adaptive InferenceDiffusion models [28; 16; 15; 29] succeed in various applications, e.g., image/video generation [30; 31; 32; 33], audio generation , point cloud generation [35; 36; 37; 38], etc.. However, numerical simulation of the diffusion processes typically involve hundreds of steps, resulting in high inference cost. Post-hoc samplers have been proposed to solve this issue [39; 40; 41; 42; 43; 44] by transforming the diffusion process into marginal-preserving probability flow ODEs, yet they still use the same number of inference steps for different states. Although adaptive ODE solvers, such as adaptive step-size Runge-Kutta , exist, they cannot significantly reduce the number of inference steps. In comparison, the adaptive sampling strategy of AdaFlow is specifically designed based on intrinsic properties of the ODE learned rectified flow, and can achieve one-step simulation for most of the states, making it much faster for decision-making tasks in real-world applications. Recently, new generative models [21; 25; 22; 23; 46; 24; 47] have emerged. These models directly learn probability flow ODEs by constructing linear interpolations between two distributions, or learn to distill a pretrained diffusion model [26; 47] with an additional distillation training phase. Empirically, these methods exhibit more efficient inference due to their preference of straight trajectories. Among them, Rectified flow achieves one step generation with reflow, a process to straighten the ODE. However, it requires a costly synthetic data construction.

By contrast, AdaFlow only leverages the initially learned ODE, but still keeps cheap training and inference costs that are similar to behavior cloning. We achieve this by unveiling a previously overlooked feature of these flow-based generative models: they act as one-step generators for deterministic target distributions, and their variance indicates the straightness of the probability flows for a certain state. Leveraging this feature, we design AdaFlow to automatically change the level of action modalities given on the states.

Diffusion Models for Decision MakingFor decision making, diffusion models obtain success as in other applications areas [48; 49; 50; 51]. In a pioneering work, Janner et al.  proposed "Diffuser", a planning algorithm with diffusion models for offline reinforcement learning. This framework is extended to other tasks in the context of _offline reinforcement learning_, where the training dataset includes reward values. For example, Ajay et al.  propose to model policies as conditional diffusion models. The application of DDPM  and DDIM  on visuomotor policy learning for physical robots  outperforms counterparts like Behavioral Cloning. Freund et al.  exploits two coupled normalizing flows to learn the distribution of expert states, and use that as a reward to train an RL agent for imitation learning. AdaFlow admits a much simpler training and inference pipeline compared with it. Despite the success of adopting generative diffusion models as decision makers in previous works, they also bring redundant computation, limiting their application in real-time, low-latency decision-making scenarios for autonomous robots. AdaFlow propose to leverage rectified flow instead of diffusion models, facilitating adaptive decision making for different states while significantly reducing computational requirements. In this work, similar to Diffusion Policy , we focus on offline imitation learning. While AdaFlow could theoretically be adapted for offline reinforcement learning, we leave it for future works.

## 3 AdaFlow for Imitation Learning

To yield an agent that enjoys both multi-modal decision-making and fast execution, we propose AdaFlow, an imitation learning framework based on flow-based generative policy. The merits of AdaFlow lie in its adaptive ability: it identifies the behavioral complexity at a state before allocatingcomputation. If the state has a deterministic choice of action, it outputs the required action rapidly; otherwise, it spends more inference time to take full advantage of the flow-based generative policy. This handy adaptivity is made possible by leveraging a combination of elements: 1) a special property of the flow 2) a variance estimation neural network and 3) a variance-adaptive ODE solver. We formally introduce the whole framework in the sequel.

### Flow-Based Generative Policy

Given the expert dataset \(D=\{(s^{(i)},a^{(i)})\}_{i=1}^{n}\), our goal is to learn a policy \(_{}\) that can generate trajectories following the target distribution \(_{E}\). \(_{}\) can be induced from a state-conditioned flow-based model,

\[_{t}=v_{}(_{t},t s)t,\ \ _{0} (0,I).\] (1)

Here, \(s\) is the state and the velocity field is parameterized by a neural network \(\) that takes the state as an additional input. To capture the expert distribution with the flow-based model, the velocity field can be trained by minimizing a state-conditioned least-squares objective,

\[L()=(_{0},_{0}) D\\ _{0}(0,I)}{}\!\![_{0}^ {1}\|-_{0}-v_{}(_{t},t)\|_{2}^{2 }t],\] (2)

where \(_{t}\) is the linear interpolation between \(_{0}\) and \(_{1}=\):

\[_{t}=t+(1-t)_{0}.\] (3)

We should differentiate \(_{t}\) which is the ODE trajectory in (1) from the linear interpolation \(_{t}\). They do not overlap unless all trajectories of ODE (1) are straight. See Liu et al.  for more discussion.

With infinite data sampled from \(_{E}\), unlimited model capacity and perfect optimization, it is guaranteed that the policy \(_{}\) generated from the learned flow matches the expert policy \(_{E}\).

### The Variance-Adaptive Nature of Flow

Typically, to sample from the distribution \(_{}\) at state \(s\), we start with a random sample \(_{0}\) from the Gaussian distribution and simulate the ODE (Eq. (1)) with multi-step ODE solvers to get the action. For example, we can exploit \(N\)-step Euler discretization,

\[_{t_{i+1}}=_{t_{i}}+v_{}(_{t_{i}},t_{i }),\ \ t_{i}=,0 i<N.\] (4)

After running the solver, \(_{1}\) is the generated action. This solver requires inference with the network \(N\) times for decision making in every state. Moreover, a large \(N\) is needed to obtain a smaller numerical error.

However, different states may have different levels of difficulty in deciding the potential actions. For instance, when traveling from a city A to another city B, there could be multiple ways for transportation, corresponding to a multi-modal distribution of actions. After the way of transportation is chosen, the subsequent actions to take will be almost deterministic. This renders using a uniform Euler solver with the same number of inference steps \(N\) across all the states a sub-optimal solution. Rather, it is preferred that the agent can vary its decision-making process as the state of the agent changes. The challenge is how to quantitatively estimate the _complexity of a state_ and employ the estimation to _adjust the inference of the flow-based policy_.

Variance as a Complexity IndicatorWe notice an intriguing property of the policy learned by rectified flow, which connects the complexity of a state with the training loss of the flow-based policy: if the distribution of actions is deterministic at a state \(\) (i.e., a Dirac distribution), the trajectory of rectified flow ODE is a straight line, i.e., _a single Euler step_ yields an exact estimation of \(_{1}\).

**Proposition 3.1**.: _Let \(v^{*}\) be the optimum of Eq. (2). If \(_{_{E}}()=0\) where \(_{E}( s)\), then the learned ODE conditioned on \(\) is_

\[_{t}=v^{*}(_{t},t)t=(-_{ 0})t,\ \  t,\] (5)

_whose trajectories are straight lines pointing to \(_{1}\) and hence can be calculated exactly with one step of Euler step:_

\[_{1}=_{0}+v^{*}(_{0},0).\]Note that the straight trajectories of (5) satisfies \(_{t}=t+(1-t)_{0}\), which makes it coincides with the linear interpolation \(_{t}\). As show in , this happens only when all the linear trajectories do not intersect on time \(t[0,1)\).

More generally, we can expect that the straightness of the ODE trajectories depends on how deterministic the expert policy \(_{E}\) is. Moreover, the straightness can be quantified by a conditional variance metric defined as follows:

\[^{2}(x,t) =(-_{0}_{t}=x,)\] (6) \[=[\|-_{0}-v^{*}(_{t},t )\|^{2}_{t}=x,].\]

**Proposition 3.2**.: _Under the same condition as Proposition 3.1, we have \(^{2}(_{t},t)=0\) from (5)._

The proof of the above propositions is in Appendix A.1. To summarize, the variance of the state-conditioned loss function at \((_{t},t)\) can be an indicator of the multi-modality of actions. When the variance is zero, the flow-based policy can generate the expected action with only one query of the velocity field, saving a huge amount of computation. In Section 3.3, we will show the variance can be used to bound the discretization error, thereby enabling the design of an adaptive ODE solver.

Variance Estimation NetworkIn practice, the conditional variance \(^{2}(x,t)\) can be empirically approximated by a neural network \(^{2}_{}(x,t)\) with parameter \(\). Once the neural velocity \(v_{}\) is learned, we can estimate \(_{}\) by minimizing the following Gaussian negative log-likelihood loss:

\[_{}_{0}^{1}-_{0}-v_{ }(_{t},t|)\|^{2}}{2^{2}_{}(_{t},t| {s})}+^{2}_{}(_{t},t|)t.\] (7)

We adopt a two-stage training strategy by first training the velocity network \(v_{}\) then the variance estimation network \(_{}\). In practice, the second stage just involves fine-tuning a few linear layers on top of the trained velocity network. Alternatively, we can optimize both the variance estimation and action generation simultaneously, which can extend training time. Our experiments show that joint training and two-stage training yield comparable performance.

### Variance-Adaptive Flow-Based Policy

Because the variance indicates the straightness of the ODE trajectory, it allows us to develop an adaptive approach to set the step size to yield better estimation with lower error during inference.

To derive our method, let us consider to advance the ODE with step size \(_{t}\) at \(_{t}\):

\[_{t+_{t}}=_{t}+_{t}v^{*}(_{t},t).\] (8)

The problem is how to set the step size \(_{t}\) properly. If \(_{t}\) is too large, the discretized solution will significantly diverge from the continuous solution; if \(_{t}\) is too small, it will take excessively many steps to compute.

We propose an adaptive ODE solver based on the principle of matching the discretized marginal distribution \(p_{t}\) of \(_{t}\) from (8), and the ideal marginal distribution \(p_{t}^{*}\) when following the exact ODE(1). This is made possible with a key insight below showing that the discretization error can be bounded by the conditional variance \(^{2}(_{t},t s)\).

**Proposition 3.3**.: _Let \(p_{t}^{*}\) be the marginal distribution of the exact ODE \(_{t}=v^{*}(_{t},t s)t\). Assume \(_{t} p_{t}=p_{t}^{*}\) and \(p_{t+_{t}}\) the distribution of \(_{t+_{t}}\) following (8). Then we have_

\[W_{2}(p_{t+_{t}}^{*},p_{t+_{t}})^{2}_{t}^{2} _{_{t} p_{t}}[^{2}(_{t},t s)],\]

_where \(W_{2}\) denotes the 2-Wasserstein distance._

We provide the proof in Appendix A.2. Hence, given a threshold \(\), to ensure that an error of \(W_{2}(p_{t+_{t}}^{*},p_{t+_{t}})^{2}^{2}\), we can bound the step size by \(_{t}/(_{t},t)\). Because \(_{t}\) at time \(t\) should not be large than \(1-t\), we suggest the following rule for setting the step size \(_{t}\) at \(_{t}\) at time \(t\):

\[_{t}=(_{t},t)}, \ \ \ [_{},\ 1-t]),\] (9)

where we impose an additional lower bound \(_{}\) to avoid \(_{t}\) to be unnecessarily small. Besides, the proposed adaptive strategy guarantees to instantly arrive at the terminal point when \(^{2}(_{t},t s)=0\) as \(_{t}=1-t\). Moreover, it aligns with Section. 3.2 since for states with deterministic actions, it sets \(_{0}=1\) to generate the action in one step. We incorporate the above insights to the execution in Algorithm 1.

Global Error AnalysisProposition 3.3 provides the local error at each Euler step. In the following, we provide an analysis of the overall error for generating \(_{1}\) when we simulate ODE while following the adaptive rule (9). To simplify the notation, we drop the dependency on the state \(s\), and write \(v_{t}^{*}()=v^{*}(,t)\).

**Assumption 3.4**.: _Assume \( v_{t}^{*}_{Lip} L\) for \(t,\) and the solutions of \(_{t}=v_{t}(_{t})t\) has bounded second curvature \(\|}_{t}\| M\) for \(t\)._

This is a standard assumption in numerical analysis, under which Euler's method with a constant step size of \(_{}\) admits a global error of order \(O(_{})\).

**Proposition 3.5**.: _Under Assumption 3.4, assume we follow Euler step (8) with step size \(_{t}\) in (9) starting from \(_{0}=_{0} p_{0}^{*}\). Let \(p_{t}\) be the distribution of \(_{t}\) we obtained in this way, and \(p_{t}^{*}\) that of \(_{t}\) in (3). Note that \(p_{1}^{*}\) is the true data distribution. Set \(=M_{}_{}^{2}/2\) for some \(M_{}>0\), and \(_{}=1/N_{}\)._

_Let \(N_{}\) be the number of steps we arrive at \(_{1}\) following the adaptive schedule. We have_

\[W_{2}(p_{1}^{*},p_{1}) C}}{N_{}} _{},\]

_where \(C\) is a constant depending on \(M\), \(M_{}\) and \(L\)._

The idea is that the error is proportional to \(}}{N_{}}\), suggesting that the algorithm claims an improved error bound in the good case when it takes a smaller number of steps than the standard Euler method with constant step size \(_{min}\). We provide the proof in Appendix A.3.

**Discussion of AdaFlow and Rectified Flow.** Rectified Flow operates in two stages: the first is learning an ordinary differential equation (ODE), and the second involves a technique called "reflow" used to straighten the learned trajectory. Theoretically, reflow allows for one-step action generation. However, using reflow introduces two major drawbacks: 1) It significantly prolongs training time, particularly because generating the required pseudo noise-data pairs through ODE simulation is computationally expensive; 2) It leads to poorer generation quality due to straightened ODE. In contrast, our method utilizes only the original ODE, eliminating the need for an additional reflow or distillation process, and consistently achieves more accurate action generation.

## 4 Experiments

We conducted comprehensive experiments on four sets of tasks: **1)** a simple 1D toy example to demonstrate the computational adaptivity of AdaFlow; **2)** a 2D navigation problem; and two robot 

[MISSING_PAGE_FAIL:7]

**AdaFlow achieves high diversity and success with low NFE; Diffusion Policy and BC lag in comparison.** We compare AdaFlow against baseline methods in Table 2. We additionally visualize the rollout trajectories from each learned policy in Figure 3 as a qualitative comparison of the learned behavior across different methods. From the results, we see that AdaFlow with an average Number of Function Evaluation (NFE) of 1.56 NFE can achieve highly diverse behavior and high success rate in the meantime. By contrast, Diffusion Policy only demonstrates diverse behavior when NFE is larger than 5 and falls behind in success rate even with 20 NFE compared to AdaFlow. BC, on the other hand, has high success rate while performing relatively poorly in terms of behavior diversity.

### Robot Manipulation Tasks

**Experiment Setup** To further validate how AdaFlow performs on practical robotics tasks, we compare AdaFlow against baselines on a Push-T task , the RoboMimic  benchmark (Lift, Can, Square, Transport, ToolHang) and the LIBERO  benchmark. For the Push-T task and the tasks in RoboMimic, we follow the exact experimental setup described in Diffusion Policy . Following the Diffusion Policy, we add three additional baseline methods: 1) LSTM-GMM, BC with the LSTM model and a Gaussian mixture head, 2) IBC, the implicit behavioral cloning , an energy-based model for generative decision-making, and 3) BET . For the LIBERO tasks, we pick a subset of six Kitchen tasks and follow the setup described in the LIBERO paper (Check Figure 4 for the description of the six tasks).

**AdaFlow consistently outperforms competitors in varied robot manipulation tasks with high efficiency.** The results of the Push-T task and the RoboMimic benchmark are summarized in Table 3. From the table, we observe that AdaFlow consistently achieves comparable or higher success rates across different challenging manipulation tasks, compared against all baselines, with only an average

   Method & NFE\(\) & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Average \\  Rectified Flow (_Needs reflow_) & 1 & 0.90 & 0.82 & 0.98 & 0.82 & 0.82 & 0.96 & 0.88 \\  Diffusion Policy & 1 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ Diffusion Policy & 2 & 0.00 & 0.58 & 0.36 & 0.66 & 0.36 & 0.32 & 0.38 \\ Diffusion Policy & 20 & 0.94 & **0.84** & **0.98** & 0.78 & 0.82 & 0.92 & 0.88 \\ AdaFlow & **1.27** & **0.98** & 0.80 & **0.98** & **0.82** & **0.90** & **0.96** & **0.91** \\   

Table 4: Success Rate on LIBERO Benchmark. The highest success rate for each task are highlighted in **bold**.

Figure 4: **LIBERO tasks.** We visualize the demonstrated trajectories of the robot’s end effector.

    &  &  &  &  &  &  &  \\   & & ph & mh & ph & mh & ph & mh & ph & mh & ph \\  Rectified Flow (_Needs reflow_) & 1 & 1.00 & 1.00 & 0.94 & 1.00 & 0.94 & 0.92 & 0.90 & 0.76 & 0.88 & 0.92 \\  LSTM-GMM & 1 & **1.00** & **1.00** & **1.00** & 0.95 & 0.86 & 0.76 & 0.62 & 0.67 & 0.69 \\ IBC & 1 & 0.79 & 0.15 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.75 \\ BET & 1 & **1.00** & **1.00** & **1.00** & **1.00** & 0.76 & 0.68 & 0.38 & 0.21 & 0.58 & - \\ Diffusion Policy & 1 & 0.04 & 0.04 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.04 \\ Diffusion Policy & 2 & 0.64 & 0.98 & 0.52 & 0.66 & 0.56 & 0.12 & 0.84 & 0.68 & 0.68 & 0.34 \\ Diffusion Policy & 100 & **1.00** & **1.00** & **1.00** & **1.00** & **1.00** & **0.97** & 0.90 & 0.72 & **0.90** & 0.91 \\ AdaFlow & **1.17** & **1.00** & **1.00** & **1.00** & 0.96 & 0.98 & 0.96 & **0.92** & **0.80** & 0.88 & **0.96** \\   

Table 3: Success rate on RoboMimic Benchmark. The highest success rate for each task are highlighted in **bold**.

[MISSING_PAGE_FAIL:9]

Conclusion

We present AdaFlow, a novel imitation learning algorithm adept at efficiently generating diverse and adaptive policies, addressing the trade-off between computational efficiency and behavioral diversity inherent in current models. Through extensive experimentation across various settings, AdaFlow demonstrated superior performance across multiple dimensions including success rate, behavioral diversity, and training/execution efficiency. This work lays a robust foundation for future research on adaptive imitation learning methods in real-world scenarios.