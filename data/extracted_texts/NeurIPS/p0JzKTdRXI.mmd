# Efficient Experimentation for Estimation of Continuous and Discrete Conditional Treatment Effects

Muhammed T. Razzak *

OATML Group

University of Oxford

&Panos Tigas *

OATML Group

University of Oxford

&Andrew Jesson

Blei Lab

Columbia University

&Yarin Gal

OATML Group

University of Oxford

&Uri Shalit

Technion - Israel Institute of Technology

###### Abstract

Accurately estimating personalized treatment effects often demands substantial data, incurring high costs across diverse applications such as personalized advertisement delivery and clinical trials. Existing methodologies employ deep models to estimate treatment effects in high-dimensional data, often relying on randomly selected experiments. We explore the potential of active learning techniques to enhance the efficiency of experimentation. Our focus centers on a relatively underexplored yet common scenario where each unit is subject to experimentation only once. We build upon the Bayesian active learning framework, to select units, and a treatment to apply to the unit, that maximize the information gain from each experiment. Our approach is flexible, accommodating both discrete and continuous treatment settings. Furthermore, we address the inefficiencies in batch experimentation by employing a greedy and a policy gradient-based optimization strategy. We validate the effectiveness of our proposed method on synthetic and high-dimensional semi-synthetic datasets (based on IHDP and TCGA). Our results show significant improvements in experimentation efficiency over the baseline methods.

## 1 Introduction

Experimentation often incurs significant costs in terms of resources, time, and funding. This necessitates the development of more efficient and targeted experimental designs. Adaptive trial design is a promising paradigm for conducting experiments, allowing more efficient use of scarce resources (Foster et al., 2020). Given a pool of samples and a set of (possibly continuous) interventions or treatments we wish to study, our goal is to construct an experiment that yields the most informative results about the effects of the interventions on a predefined outcome, while minimizing costs and resources used.

The majority of experimental design literature has primarily focused on a setting in which one may conduct experiments on the same object multiple times, usually with differing actions (also known as treatments, or interventions) (Rainforth, 2017). A somewhat less explored scenario is where one can actively select a unit from a pool of available units, and then select only a _single_ action to apply to this unit. This scenario has been referred to as active learning for trials (Deng et al., 2011) or adaptive trials (Chen et al., 2021). This scenario is relevant in clinical trials, animal studies, and any otherscenario where the units are heterogeneous and can change after being subjected to the intervention, or can only be treated once for ethical or cost reasons.

Our paper makes several contributions to the field of active learning for experimentation:

1. We propose a new setting for active learning in which the objective is to efficiently select the most informative unit-treatment combinations for experimentation (Section 2).
2. We utilize deep learning methods enabling us to model units with high-dimensional covariates and explore a wide range of treatment options (binary, discrete, and continuous options). We select unit-treatment combinations with the highest Expected Information Gain by introducing both a greedy algorithm and novel gradient-based algorithm to optimize the joint mutual information between all experiments selected in each batch (Section 3).
3. We demonstrate the efficacy of our methods on synthetic and high-dimensional semi-synthetic datasets, resulting in significant improvements in efficiency (Section 4).

## 2 Problem Formulation

Within the Neyman-Rubin causal model (Rubin, 1974), each unit \(\) has a potential outcome \(^{}()\) associated with every intervention \(\), where \(\) is the (possibly infinite) set of interventions under consideration. We are interested in efficiently learning the conditional average potential outcome (CAPO) function, \([^{}=]\). In the regime of continuous interventions, this is often referred to as the (conditional) "dose response function."

The expectation of the potential outcome given a set of covariates \(=\) describing \(\) is identifiable from data where \(\) is directly intervened on, assuming that \(\) and \(\) are causal parents of \(\), that the observed outcome corresponds to the potential outcome of the assigned treatment, and that the treatment assigned to one unit, \(\), does not effect the outcome observed for any other unit, \(^{}\). Under these conditions, the CAPO takes the form of the expectation

\[(,)[= ,=].\] (1)

We are specifically interested in a \(k\)-round, hybrid, "pool based" setting, consisting of a non-replenishable pool of units, \(=\{_{i}\}_{i=1}^{k}\), and a set of possible interventions \(T\). This is a common setting, where experimenters will do sequential experiments either in batch or non-batch setting and have access to each unit once. A common example is preclinical trials, done in stages on animals to study safety, efficacy, and biological activity of a drug. Each unit, described by high-dimensional covariates, is unique and non-replenishable.

We consider both discrete and continuous intervention spaces, \(\). At each round \(i[k]\), the experimenter uses a policy, \(: \), to select a \(b\)-sized batch of unit-intervention pairs, \(s=\{(_{j})=_{j},_{j}\}_{i=j}^{b}\). The experiments are then run and the experimenter observes the outcomes, \(\{_{j}\}_{j=1}^{b}\), for each.

At the end of each round, the experimenter fits an estimator, \((,;)\), of the CAPO, \((,)\), to the data accumulated, \(_{}=_{i=1}^{k}\{\{_{j}, _{j},_{j}\}_{j=1}^{b}\}_{i}\).

## 3 Method

This section outlines our proposed selection policies which aim to maximize the Expected Information Gain over a set of experiment designs in a batch of size \(b\), denoted by \(_{i=1:b}=(_{i},_{i})\).

In contrast to traditional experimental design, we need to acquire units, \(\), **without replacement** and select the treatment, \(\), **with replacement**. Furthermore, while units are discrete and drawn from a pool set, treatments can be discrete or continuous.

The problem is defined as selecting a batch of experiments that maximize the EIG objective:

\[\{(,)_{1:b}\}^{*}=*{arg\,max}_{\{(,)_{1:b}\}}(\{(,)_{1:b}\})\] (2)

where the utility set function is defined as the EIG objective \(() I(Y;,_{ })\).

We propose several acquisition policies for both discrete and continuous treatment settings:

Top-K.This strategy selects the top-k design with the highest estimated EIG per design. However, this does not maximize the EIG of the batch, as it does not take into account how other experiments in the batch affect the mutual information of each individual score. This can result in redundant experiments being conducted and results in inefficient experimentation (Foster et al., 2021; Kirsch et al., 2019).

Softmax Top-K.Kirsch et al. (2022) note this inefficiency and propose sampling the top-k experiments from a Softmax distribution, with temperature \(\), to the EIG scores. This selection policy has shown superior performance in terms of efficiency. It is hypothesized that this is due to the perturbation of the distribution taking into account that the mutual information changes when other experiments are selected.

Greedy Optimization.When the set function we wish to optimize is known to be submodular and non-monotonic, then a greedy optimization-based algorithm can be used to maximize the set, which enjoys \(1-\) approximation guarantees (Nemhauser et al., 1978). As shown in (Tigas et al., 2022; Kirsch et al., 2019), Expected Information Gain over continuous outcomes (i.e. regression problems) is both submodular and non-monotonic thus it is suitable for applying a Greedy optimization-based algorithm.

Policy-Gradient Based Optimization.To alleviate the approximation shortcomings of the greedy optimization-based algorithm, we notice that the objectives are differentiable with respect to the trial parameters (units and dosages), thus we can employ gradient-based methods to design experiments that maximize our objectives. We parametrize the policy \(_{}(X,T)\) as joint distribution over units \(X\) and dosages \(T\).

We provide a more details on each method in Appendix B.

## 4 Experiments

We evaluate our acquisition policies on synthetic and semi-synthetic datasets, and show significant improvements over the baseline.

Datasets.We evaluate on 3 datasets: a synthetic dataset, and two semi-synthetic datasets based on the features/covariates in the IHDP (Hill, 2011; Shalit et al., 2017) and TCGA datasets (Cancer Genome Atlas Research Network et al., 2013). For the **synthetic** dataset, suitable for discrete and continuous treatments, we develop a dose-response function based on the generalized dose response function developed in Taleb & West (2023). The covariates of the units are 1-dimensional and normally distributed, with the treatment being either a continuous range or discrete values. For the two semi-synthetic datasets, we obtain covariates from two standard benchmark datasets in causal inference, IHDP and TCGA. The units in **IHDP** dataset contain 25 features, while the units in the **TCGA** dataset contain 4000 features. Further details are provided in Appendix C.

Model.Our objectives rely on methods that are capable of modelling uncertainty and handling high-dimensional data modalities. For this we rely upon Deep Bayesian Neural Networks (BNNs). We utilize Deep Ensembles (Lakshminarayanan et al., 2017) which can be seen as approximate Bayesian Inference (Wilson & Izmailov, 2020). We utilize a simple multi-layer perceptron S-learner (Rumelhart et al., 1986; Kunzel et al., 2019). We provide the hyperparameters in Appendix D.

Metrics.The Mean Integrated Square Error (MISE) measures how well the model estimates the conditional average potential outcome (CAPO) across the entire dosage space:

\[=_{} _{i=1}^{N}_{_{}}y^{i}(,t)-^{i}(,t)^{2}t\,.\] (3)

The metric is computed over a held-out test-set. This metric reflects our objective: to efficiently build an understanding of the entire unit-treatment-response function, through as few experiments as necessary.

Results.Table 1 shows the MISE for proposed acquisition functions on the continuous synthetic dataset, for various acquisition sizes and number of experiments conducted. We provide further discussion and results in Appendix A.

Our experiments demonstrate several key findings:

EIG optimization Enhances Covariate Coverage and Counterfactual Estimation.We investigate how designing experiments for high EIG changes the covariate and treatment training density, on the discrete synthetic dataset. As illustrated in Figure 1 (in A.1), the use of the EIG objective resulted in a more strategic distribution of the covariate space, with a higher density in regions where the outcome is more uncertain or variable. In the treatment space, we find a level of overlap that allows for more accurate estimation of the counterfactuals (Jesson et al., 2021). This improved coverage and estimation leads to a reduction in MISE, as evidenced by the more accurate covariate-response curves shown in the figure.

Synthetic Dataset Performance.The Greedy and Soft Top-K acquisition functions demonstrate strong performance on the discrete synthetic dataset. Notably, the Greedy method consistently achieves the lowest MISE values across various acquisition sizes and numbers of experiments conducted, making it the best-performing method for discrete treatments. When comparing the performance on the continuous synthetic dataset to the discrete synthetic dataset, we observe that the MISE values are generally lower for the discrete case. The policy gradient method begins to outperform the other acquisition functions as the acquisition size increases.

Policy-gradient offers robust and scalable performance on high-dimensional data.We evaluate our acquisition functions' performance on two semi-synthetic datasets: IHDP (Table 3 and TCGA (Table 4 in Appendix A). These datasets are designed to test the efficiency of our methods in high-dimensional settings, the primary focus of our work. The Greedy and Policy methods consistently achieve the lowest MISE values. For instance, on the IHDP dataset, Greedy achieves a MISE value of 0.1107 for the smallest acquisition size (4) when conducting 192 experiments, outperforming the random baseline by **27.1%**. The Policy method performs well, particularly for larger acquisition sizes and a higher number of experiments conducted.

The results on these datasets demonstrate that our methods and acquisition functions are scalable, versatile, and effective in real-world scenarios. The greedy and policy-gradient acquisition functions, in combination with deep learning for effective representation learning, provide an effective framework for efficient experimentation and estimation of conditional treatment effects.

## 5 Conclusion

In this paper, we have proposed a novel framework for efficiently estimating treatment effects. We provided theoretical justification for our information-theoretic based objective functions, along with our combinatorial optimization. We demonstrated our methods on both synthetic and semi-synthetic datasets, showing significant gains in efficiency over the standard baseline. Future work could include extending our methods to handle larger batch sizes, and incorporating additional objectives for additional utility.

    &  &  &  \\  Experiment & Combined & 34 & 24 & 26 & 42 & 32 & 42 & 32 & 34 \\  Random & 0.2796\(\)0.0208 & 0.1814\(\)0.015 & 0.1197\(\)0.010 & 0.2551\(\)0.0075 & 0.1794\(\)0.0156 & 0.1842\(\)0.0085 & 0.259\(\)0.0152 & 0.180\(\)0.0064 & 0.1416\(\)0.0108 \\ Soft Top-K & 0.2727\(\)0.0247 & 0.0033\(\)0.0185 & 0.0189\(\)0.0448 & 0.2724\(\)0.0110 & 0.1153\(\)0.0014 & 0.0684\(\)0.0059 & **0.0442\(\)0.0091** & 0.457\(\)0.1153 & 0.150\(\)0.0075 & 0.047\(\)0.0113 \\ Greedy & 0.1722\(\)0.0061 & 0.0077\(\)0.0166 & 0.0047\(\)0.0035 & **0.0433\(\)0.037** & 0.0044\(\)0.0072 & 0.0041\(\)0.0012 & 0.147\(\)0.0032 & 0.175\(\)0.0036 & 0.175\(\)0.0035 & 0.0159\(\)0.0170 \\ Policy & **0.1833\(\)0.0032** & **0.0077\(\)0.0015** & 0.0045\(\)0.0009 & 0.2208\(\)0.0166 & **0.0032\(\)0.0021** & 0.0473\(\)0.0036 & 0.2906\(\)0.1397 & **0.0071\(\)0.0045** & **0.0001\(\)0.0025** \\   

Table 1: MISE for proposed acquisition functions on the continuous synthetic dataset, for various acquisition sizes and number of experiments conducted.

   &  &  &  \\  Experiment & Combined & 34 & 24 & 26 & 42 & 32 & 42 & 34 & 26 \\  Random & 0.2796\(\)0.0208 & 0.1814\(\)0.015 & 0.1197\(\)0.010 & 0.2551\(\)0.0075 & 0.1794\(\)0.0156 & 0.1842\(\)0.0085 & 0.259\(\)0.0152 & 0.180\