# Revisiting Implicit Differentiation for Learning Problems in Optimal Control

Ming Xu

School of Computing

Australian National University

mingda.xu@anu.edu.au

&Timothy Molloy

School of Engineering

Australian National University

timothy.molloy@anu.edu.au

Stephen Gould

School of Computing

Australian National University

stephen.gould@anu.edu.au

###### Abstract

This paper proposes a new method for differentiating through optimal trajectories arising from non-convex, constrained discrete-time optimal control (COC) problems using the implicit function theorem (IFT). Previous works solve a differential Karush-Kuhn-Tucker (KKT) system for the trajectory derivative, and achieve this efficiently by solving an auxiliary Linear Quadratic Regulator (LQR) problem. In contrast, we directly evaluate the matrix equations which arise from applying variable elimination on the Lagrange multiplier terms in the (differential) KKT system. By appropriately accounting for the structure of the terms within the resulting equations, we show that the trajectory derivatives scale linearly with the number of timesteps. Furthermore, our approach allows for easy parallelization, significantly improved scalability with model size, direct computation of vector-Jacobian products and improved numerical stability compared to prior works. As an additional contribution, we unify prior works, addressing claims that computing trajectory derivatives using IFT scales quadratically with the number of timesteps. We evaluate our method on a both synthetic benchmark and four challenging, learning from demonstration benchmarks including a 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.

## 1 Introduction

This paper addresses end-to-end learning problems that arise in the context of constrained optimal control, including trajectory optimization, inverse optimal control, and system identification. We propose a novel, computationally efficient approach to computing analytical derivatives of state and control trajectories that solve constrained optimal control (COC) problems with respect to underlying parameters in cost functions, system dynamics, and constraints (e.g., state and control limits).

The efficient computation of these _trajectory derivatives_ is important in the (open-loop) solution of optimal control problems for both trajectory optimization and model predictive control (MPC). Such derivatives are also crucial in inverse optimal control (also called inverse reinforcement learning or learning from demonstration), where given expert demonstration trajectories, the objective is to compute parameters of the cost function that best explain these trajectories. Extensions of inverse optimal control that involve inferring parameters of system dynamics in addition to cost functions also subsume system identification, and provide further motivation for the efficient computation of trajectory derivatives. Our proposed method of computing trajectory derivatives enables directminimization of a loss function defined over optimal trajectories (e.g., imitation loss) using first-order descent techniques and is a direct alternative to existing works [2; 20; 22], including those based on bespoke derivations of Pontryagin's maximum principle (PMP) in discrete time [20; 22].

Analytical trajectory derivatives for COC problems are derived by differentiating through the optimality conditions of the underlying optimization problem and applying Dini's implicit function theorem. Derivatives are recovered as the solution to a system of linear equations commonly referred to as the (differential) KKT system. This framework underlies all existing works [2; 20; 22] as well as our proposed approach. Similarly to Amos et al. , we construct the differential KKT system, however we use the identities from Gould et al.  that apply variable elimination on the Lagrange multipliers relating to the dynamics and constraints. We show how to exploit the block-sparse structure of the resultant matrix equations to achieve linear complexity in computing the trajectory derivative with trajectory length. Furthermore, we can parallelize the computation, yielding superior scalability and numerical stability on multithreaded systems compared to methods derived from the PMP [20; 22].

Our specific contributions are as follows. First, we derive the analytical trajectory derivatives for a broad class of constrained (discrete-time) optimal control problems with additive cost functions and dynamics described by first-order difference equations. Furthermore, we show that the computation of these derivatives is linear in trajectory length by exploiting sparsity in the resulting matrix expressions. Second, we describe how to parallelize the computation of the trajectory derivatives, yielding lower computation time and superior numerical stability for long trajectories. Third, we show how to directly compute vector-Jacobian products (VJPs) with respect to some outer-level loss over optimal trajectories, yielding further improvements to computation time in the context of bi-level optimization. This setting commonly arises in the direct solution of inverse optimal control problems [17; 33; 34]. Finally, we provide discussion unifying existing methods for computing trajectory derivatives [2; 20; 22]. We dub our method IDOC (Implicit Differentiation for Optimal Control) and validate it across numerous experiments, showing a consistent speedup over existing approaches derived from the PMP. Furthermore, for constrained problems, IDOC provides significantly improved trajectory derivatives, resulting in superior performance for a general learning from demonstration (LfD) task.1.

## 2 Related Work

Differentiating Through Optimal Control Problems.An optimal control (OC) problem consists of (system) dynamics, a cost function, an optimal control policy, and a set of state and/or control constraints. Learning problems in optimal control involve learning some or all of these aspects, with the problem of learning dynamics referred to as system identification , the problem of learning the cost function referred to as inverse optimal control (or inverse reinforcement learning or learning from demonstration) [34; 17; 21; 33; 37; 51], and the problem of learning the control policy referred to as reinforcement learning . Recent work  has shown that solving these

Figure 1: **Left a): Our approach addresses learning problems in optimal control such as imitation learning. IDOC provides a method for computing the trajectory derivative or alternatively, vector-Jacobian products with respect to an outer-level loss, important for solving this problem using an end-to-end learning approach. Right b): IDOC yields superior trajectory derivatives for the imitation learning task when inequality constraints (rocket tilt and thrust limits) are present.**

learning problems can be approached in a unified end-to-end fashion by minimizing task-specific loss functions with respect to (unknown) parameters of the associated OC problem. This unified formulation places great importance on the efficient computation of derivatives of optimal trajectories with respect to said parameters. Methods reliant on computing trajectory derivatives have, however, been mostly avoided (and argued against) in the robotics and control literature due to concerns about computational tractability. For example, bi-level methods of inverse optimal control [34; 33] have mostly been argued against in favor of methods that avoid derivative calculations by instead seeking to satisfy optimality conditions derived from KKT [24; 12] or PMP conditions [17; 32; 23; 21].

Analytical Trajectory Derivatives.Our method is a direct alternative to methods such as DiffMPC , PDP  and its extension Safe-PDP , which differentiate through optimality conditions to derive trajectory derivatives. Common to these methods is identifying that trajectory derivatives can be computed by solving an auxiliary affine-quadratic OC problem, and furthermore, that this can be done efficiently using a matrix Riccati equation. While our approach still differentiuates the optimality conditions, we avoid solving matrix Riccati equations and show that this enables easy computation of VJPs, parallelization across timesteps and improved numerical stability. Like Safe-PDP, IDOC computes derivatives through COC problems with smooth inequality constraints, however we show in our experiments that our derivatives are significantly more stable during training. Section 4.3 provides a detailed description of the differences between IDOC and existing methods.

Differentiable Optimization.Our work falls under the area of differentiable optimization, which aims to embed optimization problems within end-to-end learning frameworks. Gould et al.  proposed the deep declarative networks framework, and provide identities for differentiating through continuous, inequality-constrained optimization problems. Combinatorial problems [31; 44], as well as specialized algorithms for convex problems  have also been addressed. Differentiable optimization has been applied to end-to-end learning of state estimation [49; 42] and motion planning problems [6; 2; 26] in robotics, see Pineda et al.  for a recent survey. In addition, numerous machine learning and computer vision tasks such as pose estimation [9; 40], meta-learning , sorting [11; 7] and aligning time series  have been investigated under this setting.

## 3 Constrained Optimal Control Formulation

In this section, we provide an overview of the COC problems we consider in this paper. These problems involve minimizing a cost function with an additive structure, subject to constraints imposed by system dynamics and additional arbitrary constraints such as state and control limits. As a result, they can be formulated as a non-linear program (NLP). We will discuss how to differentiate through these COC problems in Section 4.

### Preliminaries

First, we introduce notation around differentiating vector-valued functions with respect to vector arguments, consistent with Gould et al. . Let \(f:^{n}^{m}\) be a vector-valued function with vector arguments and let \(f^{n m}\) be the (matrix-valued) derivative where elements are given by

\[(f(x))_{ij}=}{ x_{j}}(x).\] (1)

For a scalar-valued function with (multiple) vector-valued inputs \(f:^{n}^{m}\) evaluated as \(f(x,y)\), let the second order derivatives \(^{2}_{XY}=_{X}(_{Y}f)^{}\). See Gould et al.  for more details.

### Optimization Formulation for Constrained Optimal Control

We can formulate the (discrete-time) COC problem as finding a cost-minimizing trajectory subject to constraints imposed by (possibly non-linear) system dynamics. In addition, trajectories may be subject to further constraints such as state and control limits.

To begin, let the dynamics governing the COC system at time \(t\) be given by \(x_{t+1}=f_{t}(x_{t},u_{t};)\), where \(x_{t}^{n}\), \(u_{t}^{m}\) denotes state and control variables, respectively2. For time horizon \(T>0\)let \(x(x_{0},x_{1},,x_{T})\) and \(u(u_{0},u_{1},,u_{T-1})\) denote the state and control trajectories, respectively. Furthermore, let \(^{d}\) be the parameter vector that parameterizes our COC problem. Our COC problem is then equivalent to the following constrained optimization problem

\[&J(x,u;)_{t=0}^{T-1}c_{ t}(x_{t},u_{t};)+c_{T}(x_{T};)\\ &x_{0}=x_{}\\ x_{t+1}-f_{t}(x_{t},u_{t};)=0& t\{0,,T-1\}\\ g_{t}(x_{t},u_{t};) 0& t\{0,,T-1\},\\ h_{t}(x_{t},u_{t};)=0& t\{0,,T-1\},\\ g_{T}(x_{T};) 0&\\ h_{T}(x_{T};)=0,&\] (2)

where \(f_{t}:^{n}^{m}^{d}^{n}\) describe the dynamics, and \(c_{t}:^{n}^{m}^{d}\), \(c_{T}:^{n}^{d}\) are the instantaneous and terminal costs, respectively. Furthermore, the COC system may be subject to (vector-valued) inequality constraints \(g_{t}:^{n}^{m}^{d}^{q_ {t}}\) and \(g_{T}:^{n}^{d}^{q_{T}}\) such as control limits and state constraints, for example. Additional equality constraints \(h_{t}:^{n}^{m}^{d}^{s_ {t}}\) and \(h_{T}:^{n}^{d}^{s_{T}}\) can also be included.

The decision variables of Equation 2 are the state and control trajectory \((x,u)\), whereas parameters \(\) are assumed to be fixed. Equation 2 can be interpreted as an inequality-constrained optimization problem with an additive cost structure and vector-valued constraints for the initial state \(x_{0}\), dynamics, etc. over all timesteps. Let \(()(x^{},u^{})\) be an optimal solution to Equation 2. We can treat the optimal trajectory \(()\) as an _implicit_ function of parameters \(\), since Equation 2 can be solved to yield an optimal \(()\) for any (valid) \(\).

We can solve Equation 2 for the optimal trajectory \(()\) using a number of techniques. We can use general purpose solvers [13; 45], as well as specialized solvers designed to exploit the structure of COC problems, e.g., ones based on differential dynamic programming [30; 19]. Regardless, for the purposes of computing analytical trajectory derivatives using our method (as well as methods that differentiate through optimality conditions [20; 22; 2]), we only need to ensure that our solver returns a vector \(()\) which is a (local) minimizer to Equation 2. We now describe how to differentiate through these optimal control problems, important in the end-to-end learning context.

## 4 Trajectory Derivatives using Implicit Differentiation

In this section, we present our identities for computing trajectory derivatives \(()\) based on those derived in Gould et al.  for general optimization problems by leveraging first-order optimality conditions and the implicit function theorem. We exploit the block structure of the matrices in these identities that arises in optimal control problems to enable efficient computation and furthermore, show that computing trajectory derivatives is linear in the trajectory length \(T\).

Before we present the main analytical result, recall the motivation for computing \(()\). Suppose in the LfD context we have a demonstration trajectory \(^{}\) (we can extend this to multiple trajectories, but choose not to for notational simplicity). Furthermore, define a loss \(((),^{})\) which measures the deviation from predicted trajectory \(()\) to demonstration trajectory \(^{}\). Ultimately, if we wish to minimize the loss \(\) with respect to parameters \(\) using a first-order decent method, we need to compute \(_{}((),^{})\) by applying the chain rule. Specifically, we compute \(_{}()=_{}()()\), which requires trajectory derivative \(()\).

### Preliminaries

First, we first reorder \(\) such that \(=(x_{0},u_{0},x_{1},u_{1},,x_{T})^{n_{} 1}\), where \(n_{}=(n+m)T+n\). This grouping of decision variable blocks w.r.t. timesteps is essential for showing linear time complexity of the computation of the derivative. Let \(_{t}^{n+m}\) represent the subset of variables in \(\) associated to time \(t\), with final state \(_{T}=x_{T}^{n}\). Next, we stack all constraints defined in Equation 2 into a single vector-valued constraint \(r\) comprised of \(T+2\) blocks. Specifically, let \(r(;)(r_{-1},r_{0},r_{1},,r_{T})^{n_{r}}\), where \(n_{r}=(T+1)n+s+q\). The first block is given by \(r_{-1}=x_{0}\), and subsequent blocks are given by

\[r_{t}=(_{t}(x_{t},u_{t};),h_{t}(x_{t},u_{t};),x_ {t+1}-f(x_{t},u_{t};))&0 t T-1\\ (_{T}(x_{T};),h_{T}(x_{T};))&t=T.\] (3)

Here, \(_{t}\) are the subset of _active_ inequality constraints for \(\) (detected numerically using a threshold \(\)) at timestep \(t\). Note, \(s=_{t=0}^{T}s_{t}\), \(q=_{t=0}^{T}|_{t}|\) are the total number of additional equality and active inequality constraints (on top of the \(T\) dynamics and \(n\) initial state constraints). Each block represents a group of constraints associated with a particular timestep (except the first block \(r_{-1}\)). As we will see shortly, grouping decision variables and constraints in this way will admit a favorable block-sparse matrix structure for cost/constraint Jacobians and Hessians required to compute \(()\).

### Analytical Results for Trajectory Derivatives

**Proposition 1** (Idoe).: _Consider the optimization problem defined in Equation 2. Suppose \(()\) exists which minimizes Equation 2. Furthermore, assume \(f_{t}\), \(c_{t}\), \(g_{t}\) and \(h_{t}\) are twice differentiable for all \(t\) in a neighborhood of \((,)\). If \((A)=n_{r}\) and furthermore, \(H\) is non-singular, then_

\[D()=H^{-1}A^{}(AH^{-1}A^{})^{-1}(AH^{-1}B-C)-H^{-1}B,\] (4)

_where_

\[A =D_{}r(;)^{n_{r} n_{}}\] \[B =D_{}^{2}J(;)-_{t=-1}^{T}_{i=1}^{|r_{t} |}_{t,i}D_{}^{2}r_{t}(;)_{i}^{n_{}  d}\] \[C =D_{}r(;)^{n_{r} d}\] \[H =D_{}^{2}J(;)-_{t=-1}^{T}_{i=1}^{|r_{t}|} _{t,i}D_{}^{2}r_{t}(;)_{i}^{n_{} n _{}},\]

_and Lagrange multipliers \(^{n_{r}}\) satisfies \(^{}A=D_{}J(;)\)._

Proof.: This is a direct application of Proposition 4.5 in Gould et al. . 

**Remark 1**.: _Matrix \(H\) is block diagonal with \(T+1\) blocks._

To see this, \(=_{}^{2}J(;)\) has a block diagonal structure with blocks given by \(_{_{},_{}}^{2}c_{t}(_{t};)\), which follows from the assumption of additive costs in the COC problem described in Equation 2. In addition, constraints \(g_{t}\), \(h_{t}\) depend only on \(_{t}\) and furthermore, the dynamics constraint is first order in \(_{t+1}\). Therefore, \(_{}^{2}r_{t}(;)_{i}\) is only non-zero in the block relating to \(_{t}\) for all \(t\) and \(i\). We plot the block-sparsity structure of \(H\) in Figure 1(a).

**Remark 2**.: _Matrix \(A\) is a block-banded matrix that is two blocks wide._

This is shown by noting that the only non-zero blocks in \(A\) correspond to \(_{_{t}}r_{t}(;)\) and \(_{_{t+1}}r_{t}(;)\). The former relates to \(g_{t}\), \(h_{t}\) and the \(-f_{t}(_{t};)\) component of the dynamics constraint. The latter only relates to the \(x_{t+1}\) component of the dynamics constraint. Finally, the initial condition block \(r_{-1}\) only depends on \(_{0}\), hence the only non-zero block is given by \(_{_{0}}r_{-1}(_{0};)\). We plot the block-sparsity structure of \(A\) in Figure 1(b).

**Proposition 2**.: _Evaluating Equation 4 has \(O(T)\) time complexity._

Proof.: From Remarks 1 and 2, \(H\) and \(A\) have a block diagonal and block-banded structure with \(T+1\) and \(2T+2\) blocks, respectively. Therefore, we can evaluate \(H^{-1}A^{}\) in \(O(T)\) time and furthermore, \(H^{-1}A^{}\) has the same structure as \(A\). It follows that we can also evaluate \(AH^{-1}B-C\) in \(O(T)\) time after partitioning \(B\) and \(C\) into blocks based on groupings of \(\) and \(r\).

Next, observe that \(AH^{-1}A^{}\) yields a block tridiagonal matrix with \(T+2\) blocks on the main diagonal (see Figure 1(c) for a visualization of the block-sparse structure). As a result, we can evaluate \((AH^{-1}A^{})^{-1}(AH^{-1}B)\) by solving the linear system \((AH^{-1}A^{})Y=AH^{-1}B\) for \(Y\) in \(O(T)\)time using any linear time (w.r.t. number of diagonal blocks) block tridiagonal linear solver. A simple example is the block tridiagonal matrix algorithm, easily derived from block Gaussian elimination.

Finally, \(H^{-1}B\) can easily be evaluated in \(O(T)\) time by solving \(T+1\) linear systems comprised of the blocks of \(H\) and \(B\). We conclude that evaluating Equation 4 requires \(O(T)\) operations. 

### Comparison to DiffMPC and PDP

All existing methods for computing analytical trajectory derivatives involve solving the linear system

\[H&A^{}\\ A&0}_{K}\\ -=-B\\ -C,\] (5)

where blocks \(H,A,B,C\) are defined in Section 4.2 and D\(\) is the desired trajectory derivative. We call Equation 5 the (differential) _KKT system_ and the matrix \(K\) the (differential) _KKT matrix_. Note, D\(\) is unique if \(K\) is non-singular3. To better contextualize IDOC, we now briefly compare and contrast how previous methods such as DiffMPC  and PDP  solve this KKT system.

DiffMPC.DiffMPC  proposes a method for differentiating through Linear Quadratic Regulator (LQR) problems, which are OC problems where the cost function is (convex) quadratic and the dynamics are affine. The authors show that solving the LQR problem using matrix Riccati equations can be viewed as an efficient method for solving a KKT system which encodes the optimality conditions. In addition, they show that computing trajectory derivatives involves solving a similar KKT system, motivating efficient computation by solving an auxiliary LQR problem in the backward pass. The matrix equation interpretation of the backward pass allows the efficient computation of VJPs for some downstream loss \(()\) in a bi-level optimization context.

DiffMPC extends to handling non-convex OC problems with box constraints on control inputs via an iterative LQR-based approach. Specificially, such problems are handled by first computing VJPs w.r.t. the parameters of an LQR approximation to the non-convex problem around the optimal trajectory \(,\). Next, the parameters of the approximation are differentiated w.r.t. the underlying parameters \(\) and combined using the chain rule. However, differentiating the quadratic cost approximation and dynamics requires evaluating costly higher-order derivatives, e.g., \(c()}{d^{2}}\), which are 3D tensors. These tensors are dense in general, although problem specific sparsity structures may exist.

PDP/Safe-PDP.PDP  and its extension, Safe-PDP , take a similar approach to DiffMPC in deriving trajectory derivatives. PDP derives trajectory derivatives by starting with PMP, which is well-known in the control community and applies to non-convex OC problems. Furthermore, Jin et al.  shows that the PMP and KKT conditions are equivalent in the discrete-time COC setting. Trajectory derivatives are obtained by differentiating the PMP conditions, yielding a new set of PMP conditions for an auxiliary LQR problem, which can be solved using matrix Riccati equations. This

Figure 2: Block-sparse structure for matrices \(H\), \(A\), and \(AH^{-1}A^{}\) assuming \(T=3\). For \(A\), we combined inequality and equality constraint groups \(g_{t}\) and \(h_{t}\) into a single group for simplicity. Shaded regions indicate (possible) non-zero blocks.

approach is fundamentally identical to DiffMPC, with only superficial differences for LQR problems (solving a differential KKT system versus differentiating PMP conditions)4.

For non-convex problems, PDP does not evaluate higher-order derivatives, unlike DiffMPC. However, it is not clear how to compute VJPs under approaches derived using the PDP framework. Safe-PDP extended PDP to handle arbitrary, smooth inequality constraints using the constrained PMP conditions, which extends the capability of DiffMPC to handle box constraints. Trajectory derivatives are computed by solving an auxiliary equality constrained LQR problem. Experiments in both Jin et al.  and Section 6 show that Safe-PDP yields unreliable derivatives for COC problems. While Jin et al.  conjected that instability was due to the set of active constraints changing between iterations, we find that IDOC still learns reliably in the presence of constraint switching. We instead observed that the poor gradient quality arises naturally from the equality constrained LQR solver used for the backward pass  not matching the solution obtained by directly solving the KKT system.

Idoc.In contrast, we solve Equation 5 for \(\) by first applying variable elimination on \(\), yielding Proposition 1. We will discuss in Section 5 how the equations resulting from this approach admit parallelization and favorable numerical stability compared to DiffMPC and PDP's auxiliary LQR approach. Furthermore, VJPs can be easily computed (unlike PDP) without evaluating higher-order derivatives (unlike DiffMPC). IDOC handles arbitrary smooth constraints like Safe-PDP, and we show in Section 6 that our derivatives for COC problems are reliable during training. However, we require the additional assumption that \(H\) is non-singular (which holds if and only if all blocks in \(H\) are non-singular), which is not required for \(K\) to be non-singular. An oftentimes effective solution when \(H\) is singular, initially proposed by Russell et al. , is to set \(H=H+I\) for small \(\), which is analogous to adding a proximal term to the cost function in Equation 2. We will now describe how to evaluate Equation 4 in linear time by exploiting the block-sparse structure of the matrix equation.

## 5 Algorithmic Implications of IDOC

### Parallelization and Numerical Stability

Parallelization.To evaluate Equation 4, we can leverage the block diagonal structure of \(H\) and block-banded structure of \(A\) to compute \(H^{-1}A^{}\) and \(H^{-1}B\) in parallel across all timesteps. Specifically, we evaluate the matrix product block-wise, e.g., \(H_{t}^{-1}B_{t}\) for all \(t\) in parallel (similarly with \(A\)). Following this, we can then compute \(A(H^{-1}A^{})\) in parallel also using the same argument.

In addition, we can solve the block tridiagonal linear systems involving \(AH^{-1}A^{}\) by using a specialized parallel solver [5; 36; 27; 41; 15; 18]. Unfortunately, none of these methods have open source code available, and so in our experiments, we implement the simple block tridiagonal matrix algorithm. Implementing a robust parallel solver is an important direction for future work, and will further improve the scalability and numerical stability of IDOC.

Numerical Stability.In addition to parallelization, another benefit of avoiding Riccati-style recursions for computing derivatives is improved numerical stability. For long trajectories and/or poorly conditioned COC problems (e.g., stiff dynamics), IDOC reduces the rounding errors that accumulate in recursive approaches. We show superior numerical stability in our experiments compared to PDP in Section 6, despite using the simple recursive block tridiagonal matrix algorithm for solving \(AH^{-1}A^{}\). Replacing this recursion with a more sophisticated block tridiagonal solver should further improve the stability of the backwards pass and is left as future work.

### Vector Jacobian Products

Another benefit of explicitly writing out the matrix equations for \(()\) given in Equation 4 is that we can now directly compute VJPs given some outer loss \(()\) over the optimal trajectory. Let \(v_{}()^{}^{n_{}  1}\) be the gradient of the loss w.r.t. trajectory \(()\). The desired gradient \(_{}(())\) is then given by \(_{}()=v^{}()\) using the chain rule. The resultant expression is

\[_{}(())=v^{}(H^{-1}A^{}(AH^{-1}A^ {})^{-1}(AH^{-1}B-C)-H^{-1}B).\] (6)The simple observation here is that we do not need to construct \(D_{}^{*}()\) explicitly, and can instead evaluate the VJP directly. We propose evaluating Equation 6 from left to right and block-wise, which will reduce computation time compared to explicitly constructing \(()\) and then multiplying with \(v\).

To see this, we follow the example in Gould et al.  and assume the blocks in \(H\) have been factored. Then for a single block (ignoring constraints for simplicity), evaluating \(v^{}(H_{t}^{-1}B_{t})\) is \(O((n+m)^{2}p)\) while evaluating \((v^{}H_{t}^{-1})B_{t}\) is \(O((n+m)^{2}+(n+m)p)\). Evaluating the VJP directly significantly reduces computation time compared to constructing the full trajectory derivative for problems with higher numbers of state/control variables and tunable parameters.

## 6 Experiments

While our contributions are largely analytical, we have implemented the identities in Equation 4 to verify our claims around numerical stability and computational efficiency on a number of simulated COC environments. We will show that IDOC is able to compute trajectory derivatives significantly faster compared to its direct alternative PDP  and Safe-PDP  with superior numerical stability.

### Experimental Setup

We evaluate IDOC against PDP  and Safe-PDP  in an LfD setting across four simulation environments proposed in Jin et al. , as well as a synthetic experiment. The simulation environments showcase the gradient quality for a realistic learning task, whereas the synthetic experiment is designed to measure numerical stability and computation times. For Safe-PDP, we evaluate against the method proposed for inequality constrained problems (Safe-PDP), as well as using an approximate log-barrier problem in place of the full problem (Safe-PDP (b)). The IPOPT solver  is used in the forward pass to solve the COC problem, and Lagrange multipliers \(\) are extracted from the solver output. More generally however, note that the method proposed in Gould et al.  can be used to recover \(\) if another solver is used where \(\) is not provided.

Imitation Learning/LfD.The LfD setting involves recovering the model parameters \(\) that minimizes the mean-squared imitation error to a set of \(N\) demonstration trajectories, given by

Figure 3: Learning curves for the imitation learning task over five trials. Bold lines represent mean loss, lighter lines represent individual trials. IDOC yields more stable gradients and lower final imitation loss across all environments and trials compared to both Safe-PDP (S-PDP) and Safe-PDP with log-barrier functions (S-PDP (b)).

\(((),^{})_{i}\|( )_{i}-_{i}^{}\|^{2}\). We include all parameters in the cost, dynamics and constraint functions in \(\) and furthermore, evaluate performance both with (S-PDP) and without (PDP) inequality constraints. We perform experiments in four standard simulated environments: cartpole, 6-DoF quadrotor5, 2-link robot arm and 6-DoF rocket landing. For a detailed description of the imitation learning problem as well as each COC task, see the appendix. In addition, we provide timings for all methods within the simulation environments in the appendix.

Synthetic Benchmark.The simulation experiments described above are not sufficiently large-scale to adequately measure the benefits of parallelization and numerical stability afforded by IDOC over PDP. To demonstrate these benefits, we constructed a large-scale, synthetic experiment where the blocks required to construct \(H,A,B\) and \(C\) (as discussed in Section 4) are generated randomly. Computation time against the horizon length \(T\), as well as the number of parameters \(d\) is reported. Numerical stability is measured by comparing mean-absolute error between trajectory derivatives evaluated under 32-bit and 64-bit precision for varying condition numbers over the blocks of \(H\). State and control dimensions are fixed at \(n=50\) and \(m=10\). Experiments are run on an AMD Ryzen 9 7900X 12-core processor, 128Gb RAM and Ubuntu 20.04. See the appendix for more details.

### Imitation Learning/LfD Results

In this section, we evaluate the effectiveness of gradients produced from IDOC against PDP and Safe-PDP [20; 22] for the imitation learning/LfD task. In the equality constrained setting, IDOC and PDP yield identical (up to machine precision) trajectory derivatives and imitation loss throughout learning; see the appendix for more detailed results and analysis. However, as shown in Figure 3, when inequality constraints are introduced, the derivatives produced by Safe-PDP and IDOC differ significantly. Safe-PDP fails to reduce the imitation loss due to unreliable gradients, whereas IDOC successively decreases the imitation loss. While Safe-PDP (b), which differentiates through a log-barrier problem, also provides stable learning, it ultimately yields higher imitation loss compared to IDOC. This is because the barrier problem is an approximation to the true COC problem. Further discussion around using log-barrier methods and COC problems is provided in Section 7.1.

### Synthetic Benchmark Results

Figure 4 illustrates the results of the synthetic benchmark. We observed that IDOC and PDP have similar computation time and scalability with problem size when computing full trajectory derivatives. However, computing VJPs with IDOC significantly reduces computation time and improves scalability

Figure 4: Synthetic experiments measuring a) scalability with horizon length \(T\), with fixed parameter size \(||=10\)k, b) scalability with number of parameters \(d\) with fixed horizon length \(T=1\)k, c) numerical stability with varying block condition number \((H_{t})\). Error bars measure standard error across 5 and 25 samples for computation time (a, b) and numerical stability (c), respectively.

Figure 5: Control Limit Learning.

by an order of magnitude with model size (i.e., \(d=||\)). Numerical stability experiments show that IDOC (full and VJP) yields lower round-off errors and improved stability compared to PDP. Using more sophisticated block tridiagonal solvers will further improve stability.

## 7 Discussion and Future Work

### Differentiating through Log-Barrier Methods

In the imitation learning setting, we assume that demonstration data \(^{}\) is an optimal solution to a COC problem with (unknown) parameters \(^{}\). Furthermore, we assume that a subset of timesteps \(\{1,,T\}\) yield active constraints. Under the log-barrier formulation, the constraint boundaries must be relaxed beyond their true values during learning to minimize the imitation loss. Concretely, under \(^{}\), \( g_{t}(x_{t}^{},u_{t}^{};^{})\) are undefined for \(t\). Therefore to recover \(()=^{}\), we must have \(g_{t}(x_{t}(),u_{t}())<0\) for \(t\), i.e., we cannot recover the true constraint function through minimizing the imitation loss. We verify this using the robot arm environment, and present the results in Figure 5, plotting the error between the estimated and true constraint value. We observe that IDOC recovers the true constraint value more closely compared to Safe-PDP (b).

Generality of IFT.The generality of the differentiable optimization framework, and the matrix equation formulation for trajectory derivatives given in Equation 4 yield additional conceptual benefits which may help us tackle even broader classes of COC problems. For example, we can relax the additive cost assumption and add a final cost defined over the full trajectory such as

\[h_{T}(;)=^{}Q,\] (7)

where \(Q=CC^{}\) and \(C^{n_{} k}\) for \(k n_{}\) is full rank. For IDOC, we can simply use the matrix inversion lemma to invert \(H\) in \(O(T)\). However, this is more complicated for methods derived from the PMP which rely on the specific additive cost structure of the underlying COC problem.

Dynamic Games.A generalization of our work is to apply the differentiable optimization framework to handle learning problems that arise in dynamic games  and have recently begun to employ PDP-based approaches . Being able to differentiate through solution and equilibrium concepts that arise in dynamic games enables the solution of problems ranging from minimax robust control  to inverse dynamic games [10; 33], and is a promising direction for future work.

Fast forward passes.While we have proposed a more efficient way of computing trajectory derivatives (i.e., the backward pass), we observe that the forward pass to solve the COC problem bottlenecks the learning process. Significant effort must be placed on developing fast solvers for COC problems for hardware accelerators to allow learning to scale to larger problem sizes.

Combining MPC and Deep Learning.A recent application for computing trajectory derivatives is combining ideas from deep learning (including reinforcement learning) with MPC [38; 50; 46; 48]. Common to these approaches is using a learned model to select the parameters for the MPC controller. By allowing for robust differentiation through a broader class COC problems compared to previous approaches, we hope that IDOC will allow for future development in this avenue of research.

## 8 Conclusion

In this paper, we present IDOC, a novel approach to differentiating through COC problems. Trajectory derivatives are evaluated by differentiating KKT conditions and using the implicit function theorem. Contrary to prior works, we do not solve an auxiliary LQR problem to efficiently solve the (differential) KKT system for the trajectory derivative. Instead, we apply variable elimination on the KKT system and solve the resultant matrix equations directly. We show that linear time evaluation is possible by appropriately considering equation structure. In fact, we show that IDOC is faster and more numerically stable in practice compared to methods derived from the PMP. We hope that our discussion connecting the fields of inverse optimal control and differentiable optimization will lead to future work which enables differentiability of a broader class of COC problems.