# SHAP-IQ: Unified Approximation of any-order Shapley Interactions

Fabian Fumagalli

Bielefeld University, CITEC

D-33619, Bielefeld, Germany

ffumagalli@techfak.uni-bielefeld.de &Maximilian Muschalik

LMU Munich, MCML Munich

D-80539, Munich, Germany

maximilian.muschalik@ifi.lmu.de &Patrick Kolpaczki

Paderborn University

D-33098, Paderborn, Germany

patrick.kolpaczki@upb.de &Eyke Hullermeier

LMU Munich, MCML Munich

D-80539, Munich, Germany

eyke@ifi.lmu.de &Barbara Hammer

Bielefeld University, CITEC

D-33619, Bielefeld, Germany

bhammer@techfak.uni-bielefeld.de

denotes equal contribution

###### Abstract

Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model. Shapley interaction indices extend the SV to define any-order feature interactions. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation. We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models.

## 1 Introduction

Feature attributions are a prevalent approach to interpret black box machine learning (ML) models [2; 7; 26]. However, in many real-world applications, such as understanding drug-drug interactions, mutational events or complex language models, quantifying _interactions_ between features is essential, too [48; 23; 43]. Feature interactions provide a more comprehensive explanation, which can be seen as an enrichment of feature attributions [3; 39; 40]. While feature attributions quantify the contribution of _single_ features to the model's prediction or performance, feature interactions quantify the contribution of a _group_ of features to the model's prediction or performance.

In this work, we are interested in feature interactions that make use of the Shapley value (SV) and its extension to Shapley interactions. The SV is a concept from cooperative game theory that has been used, apart from feature attributions , as a basis for many Shapley-based explanations [21; 15; 49]. It distinguishes itself through uniqueness given a set of intuitive axioms. A number of approaches extend Shapley-based explanations to feature interactions [17; 3; 39; 40]. Yet, in contrast to the SV, a "natural" extension of the intuitive set of axioms for a unique Shapley interaction index is less clear. Moreover, its efficient computation is challenging and, so far, approximation approaches are specifically tailored to the particular definition.

In this paper, we consider a more general class of interaction indices, known as cardinal interaction indices (CII) , which covers all currently proposed definitions and all other that satisfy the (generalized) linearity, symmetry and dummy axiom. We present SHAPley Interaction Quantification (SHAP-IQ), a sampling-based unified approximation method. It is substantiated by mathematical guarantees and can be applied to _any_ CII to approximate any-order interaction scores efficiently.

Contribution.Our main contributions include:

* We consider a general form of interaction indices, known as CII (Definition 3.3) and establish a novel representation (Theorem 4.1), which we utilize to construct SHAP-IQ (Definition 4.2), an efficient sampling-based estimator.2 * We show that SHAP-IQ is unbiased, consistent and provide a general approximation bound (Theorem 4.3). We further prove that SHAP-IQ maintains the efficiency condition for n-Shapley Values  and the Shapley Taylor Interaction Index  (Theorem 4.7).
* For the SV, we find a novel representation (Theorem 4.4). We further prove that SHAP-IQ is linked to Unbiased KernelSHAP  (Theorem 4.5) and greatly simplifies its representation.
* We use SHAP-IQ to compute any-order n-Shapley Values on different ML models and demonstrate that it outperforms existing baseline methods. We further contrast different existing CIIs and compare SHAP-IQ to the corresponding baseline approximation method.

## 2 Related Work

The Shapley Interaction Index (SII) , its efficiency preserving aggregation as n-Shapley Values (n-SII) , the Shapley Taylor Interaction (STI)  and the Faithful Shapley Interaction Index (FSI)  offer different ways of extending the SV to interactions, which extend on the linearity, symmetry and dummy axiom to provide a uniquely defined interaction index. SII and STI extend on axiomatic properties of the weighted sum for SV , whereas FSI extends on the axiomatic properties of the Shapley interaction as the solution to a weighted least square solution [32; 33].

Figure 1: Interaction scores for a movie review excerpt presented to a sentiment analysis model.

In the field of cooperative game theory, interactions have also been studied from a theoretical perspective as the solution of a weighted least square problem and the sum of marginal contributions with constant weights, which both yield a generalized Banzhaf value [18; 16].

In the ML community, interactions of features have been studied from a practical perspective for text  and image  data, for specific models, such as neural networks [42; 10; 36; 20] or tree based models . Other concepts of interactions have been discussed in  using marginal contributions, from a statistical perspective with functional decomposition  and improved white box models with interaction terms .

To approximate SII and STI, a permutation-based method [39; 40], as an extension of ApproShapley , was suggested, whereas FSI relies on a kernel-based approximation, similar to KernelSHAP , which utilizes a representation of the SV as the solution of a weighted least square problem . Unlike these specific approaches, we consider general CIIs, which subsume all of the above mentioned measures, and we propose a generic approximation technique, which can be accompanied by mathematical guarantees. In case of the SV, our approximation of the CII is related to Unbiased KernelSHAP , which is a variant of KernelSHAP . It is further related to stratified sampling approximations for the SV and our sampling approach can be seen as flexible framework to find the optimum allocation for each stratum .

## 3 The Cardinal Interaction Index (CII) and Shapley-based Explanations

In this section, we review Shapley-based explanations and introduce the CII, which we aim to approximate in Section 4. We further introduce existing baseline methods for specific CIIs and Unbiased KernelSHAP for the SV, which is linked to our proposed method.

Notations.We refer to the model behavior on a set of features \(D=\{1,,d\}\) as a function \(:(D)\), where \((D)\) refers to the power set of \(D\). We denote \(_{0}(T):=(T)-()\), which is the default setting in game theory and also known as _set function_[17; 14]. The subsets \(S D\) refer to the set of features (or players in game theory) of which the _interaction_ is computed, where we use lower case letters for the cardinality, i.e. \(s:=|S|\). The maximum order interaction of interest is denoted with \(s_{0}\) and we use the set \(_{k}:=\{T D:k t d-k\}\) and the set of interactions \(_{s_{0}}:=\{S D s s_{0},S\}\). For a subset \(T D\), we refer to the binary representation as \(Z_{T}=(z_{1},,z_{d})\{0,1\}^{d}\) with \(z_{i}=(i T)\) for \(i=1,,d\), where \(\) refers to the indicator function. We further denote the Shapley kernel [6; 26] as \((t):=^{-1}\).

Removal-based explanations  consider a model that is trained on \(d\) features, where the goal is to examine a _model behavior_ that is defined on a subset of features. The model behavior \(\) for a subset of features could, for instance, be a particular model prediction for one input (local explanation) or an overall measure of model performance (global explanation), if only this subset of features is known . To quantify the contribution for individual features, the change in model behavior is evaluated, if the feature is removed from the model. To restrict a ML model on a subset of features, different _feature removal_ techniques have been proposed, such as marginalization of features or retraining the model . To quantify the impact of a single feature \(i D\) on the model behavior \(\) it is then intuitive to compute the difference \(^{}_{(i)}(T)=(T\{i\})-(T)\) for subsets \(T D\{i\}\). For a distinct pair of features \((i,j)\) with \(i,j D\), a natural extension is \(^{}_{\{i,j\}}(T)=(T\{i,j\})-(T)-^{}_{\{i\}}(T)- ^{}_{\{j\}}(T)\) for \(T D\{i,j\}\), i.e. subtracting the contribution of single features from the joint impact of both features. The following definition generalizes this recursion and is known as _discrete derivative_ or _S-derivative_.

**Definition 3.1** (Discrete Derivative ).: _For \(S D\) the S-derivative of \(\) at \(T D S\) is_

\[^{}_{S}(T):=_{L S}(-1)^{s-l}(T L).\]

To obtain an attribution score, the marginal contributions on different subsets \(T D S\) are aggregated using a specific _summary technique_. In this work, we are interested in the approximation and extension of one particular summary technique for single features \(i D\), called the Shapley value , independent of model behavior and feature removal.

**Definition 3.2** (Shapley Value (SV) ).: _The SV is \(I^{}(i)=_{T D\{i\}}^ {}_{\{i\}}(T)\), \(i D\)._The SV is the unique attribution method that fulfills the axioms: symmetry (attributions are independent of feature ordering), linearity (in terms of the model behavior \(\)), dummy (if a feature does not change \(\) then its attribution is zero) and efficiency (the sum of attributions are equal to \(_{0}(D)\)) . However, the SV does not give any information about the interactions between two or more features. A suitable extension of the SV for interactions of features \(S D\) remains an open question, as different axiomatic extensions have been proposed [17; 39; 40]. In this work, we thus consider a broad class of interaction indices, known as CIIs [17; 40], which subsumes popular choices.

**Definition 3.3** (Cardinal Interaction Index (CII) ).: _A CII is an interaction index of the form_

\[I^{m}(S):=_{T D S}m_{s}(t)^{}_{S}(T)m_{s}(t)s=1,,s_{0}t=0,,d-s.\]

**Remark 3.4**.: _It was shown that every interaction index satisfying the generalized linearity, symmetry and dummy axioms can be represented as a CII . If \(_{t=0}^{d-s}m_{s}(t)=1\), then the CII is also referred to as a cardinal-probabilistic interaction index (CPII) ._

In this paper, we present a unified approximation technique for arbitrary CIIs.

### Shapley Interaction Index (SII) and other CIIs

In the following, we introduce prominent examples of CIIs. For further details on the axioms and exact definitions, we refer to the appendix. The SII  is a direct extension of the SV, that relies on an additional _recursive_ axiom to obtain a unique CII.

**Definition 3.5** (Shapley Interaction Index (SII)).: _The SII is a CII defined as_

\[I^{}(S):=_{T D S}m_{s}^{}(t) ^{}_{S}(T)m_{s}^{}(t):=.\]

It has been shown that the SII is a CPII . In contrast to the SV, the SII does not fulfill the efficiency axiom, which is a desirable property in the context of ML. Therefore an extension of SII, as well as other interaction indices have been proposed.

n-Shapley Values (n-SII) and other interaction indices.The efficiency axiom for interaction indices of _maximum interaction order_\(1 s_{0} d\) requires that the sum of \(I^{m}(S)\) up to order \(s_{0}\) equals \(_{0}(D)\).

**Definition 3.6** (Efficiency [39; 40]).: _A CII is efficient of order \(s_{0}\), if \(_{S S_{_{0}}}I^{m}(S)=_{0}(D)\), where \(_{s_{0}}\) is the set of interactions up to order \(s_{0}\)._

In , an aggregation of SII was proposed to obtain n-SII \(I^{}_{s_{0}}(S)\) of order \(s_{0}\) that satisfies efficiency. Other axiomatic approaches directly require efficiency together with the linearity, symmetry and dummy axioms, and omit the recursive axiom of SII. However, in contrast to the SV, this axiom alone does not yield a unique interaction index [39; 40]. The STI  requires the efficiency axiom and an additional interaction distribution axiom. On the other hand, the FSI  requires the efficiency axiom and the faithfulness property, that relates the interaction index to a solution of a constrained weighted least square problem. The choice of axioms of SII (n-SII), STI and FSI yield a unique interaction index that reduces to the SVs for \(s_{0}=1\). For FSI, it was shown that the top-order interactions define a CPII [40, Proposition 21], which is also easily verified for STI. All orders of interactions of FSI and STI can in general be represented as a CII, as they fulfill the linearity, symmetry and dummy axioms [17, Proposition 5]. However, it was noted that for FSI a simple closed-form solution for lower-order interactions in terms of discrete derivatives remains unclear [40, Lemma 70].

### Baseline Approximations of SII, STI and FSI.

By definition, the number of evaluations of \(\) in \(I\), which constitutes the limiting factor in ML, grows exponentially with \(d\) and thus, in practice, approximation methods are required. Currently, there does not exist an approximation for the general CII definition, as each index (SII, STI, FSI) requires a specifically tailored technique. Approximations of CII can be distinguished into permutation-based approximation (SII and STI) and kernel-based approximation (FSI). Both extend on existing methodsfor the SV, namely permutation sampling  for SII and STI, and KernelSHAP  for FSI. For a comprehensive overview of the original SV methods, we refer to the appendix. We now briefly discuss existing approaches, which will be used as baselines in our experiments.

Permutation-based (PB) Approximation for STI and SII [39; 40].The permutation-based (PB) approximation computes estimates of SII and STI based on a representation of uniformly sampled random permutations \((_{D})\), where \(_{D}\) is the set of all permutations, i.e. the set of all ordered sequences of the elements in \(D\). Then,

\[I^{}(S)=_{(_{D})}[ (S)_{S}^{}(u_{S}^{-}())]I^{}(S)=_{(_{D})}[ _{S}^{}(u_{S}^{-}())].\]

Here, \(u_{S}^{-}()\) refers to the set of indices in \(\) preceding the first occurrence of any element of \(S\) in \(\) and \(S\) is fulfilled, if all elements of \(S\) appear as a consecutive sequence in \(\). The estimators for SII and STI then compute an approximation by Monte Carlo integration by sampling \((_{D})\).

Kernel-based (KB) Approximation for FSI [40; 8].Kernel-based (KB) approximation estimates FSI based on the representation of \(I\) as a solution to a constrained weighted least square problem

\[I^{}=*{arg\,min}_{^{d_{s_{0}}}} _{T p(T)}[((T)-_{S_{s_ {0}}\\ S T}(S))^{2}]_{S_{s_{0}}}(S)=(D)( )=(),\] (1)

where \(p(T)(t)\), is a probability distribution over \(_{1}\) and \(d_{s_{0}}:=|_{s_{0}}|\). KB approximation for FSI estimates the expectation using Monte Carlo integration by sampling from \(p(T)\) and solves the approximated least-squares problem explicitly, similar to KernelSHAP [26; 8; 40]. For more details and pseudo code, we refer to the appendix.

### Unbiased KernelSHAP (U-KSH) for the SV

U-KSH constitutes a variant of KernelSHAP (KSH) , which relies on KB approximation for the SV. In contrast to KSH, U-KSH is theoretically well understood and it was shown that the estimator is unbiased and consistent . U-KSH finds an exact solution to (1) with \(s_{0}=1\) as

\[I^{}=A^{-1}(b-^{T}A^{-1}b-_{0}( )}{^{T}A^{-1}})A:=[ZZ^{T}],b=[Z_{0}(Z)]p(Z) (t).\]

U-KSH then approximates this solution using Monte Carlo integration.

**Definition 3.7** (Unbiased KernelSHAP (U-KSH) ).: _Given \(T_{1},,T_{K} p(T)(t)\) with binary representation \(Z_{1},,Z_{K}\{0,1\}^{d}\), U-KSH is defined as_

\[_{U}^{}:=A^{-1}(-^{T}A^ {-1}-_{0}()}{^{T}A^{-1}}), { where }:=_{k=1}^{K}Z_{k}_{0}(Z_{k}).\]

The main idea of U-KSH is that \(A\) can be computed explicitly independent of \(\) and only \(b\) has to be estimated . By linking U-KSH to our method (Theorem 4.5), we will show that \(_{U}^{}\) can be greatly simplified to a weighted sum.

## 4 SHAP-IQ: Unified Approximation of any-order CII

So far, there exists no unified approximation technique for the general CII. In particular, it is unknown if existing approximation techniques, such PB and KB, generalize to other indices [39; 40; 13]. Furthermore, PB approximation for SII and STI is very inefficient as each update of all estimates requires a significant number of model evaluations. KB approximation for FSI efficiently computes estimates, where one model evaluation can be used to update all interaction scores. It is, however, impossible to compute only a selection of interaction estimates and theoretical results for the estimator are difficult to establish. In the following, we introduce SHAP-IQ (Section 4.1), a unified sampling-based approximation method that can be applied to _any CII_. SHAP-IQ is based on a Monte Carlo estimate of a novel representation of the CII and well-known statistical results are applicable. In the special case of SV, we find a novel representation of the SV and show that SHAP-IQ is linked to U-KSH (Section 4.2). SHAP-IQ therefore greatly reduces the computational complexity of U-KSH. We further show (Section 4.3), that the sum of interaction estimates of SHAP-IQ remains constant and therefore maintains the efficiency property for STI and SII. Interestingly, for FSI this property does not hold, which should be investigated in future research. All proofs can be found in the appendix.

### SHAPley Interaction Quantification (SHAP-IQ)

A key challenge in approximating the CII efficiently is that the sum changes for every interaction subset \(S\). We thus first establish a novel representation of the CII. Based on this representation, we construct SHAP-IQ, an efficient estimator of the CII. We show that SHAP-IQ is unbiased, consistent and provide a general approximation bound.

Our novel representation of the CII is defined as a sum over all subsets \(T D\). In previous works, it was shown that such a representation does exist for games with \(()=0\), if the linearity axiom is fulfilled [17, Proposition 1]. We now explicitly specify this representation and show that the weights, for a CII, only depend on the sizes of \(T\) and the intersection \(T S\).3

**Theorem 4.1**.: _It holds \(I^{m}(S)=_{T D}_{0}(T)_{s}^{m}(t,|T S|)\) with \(_{s}^{m}(t,k):=(-1)^{s-k}m_{s}(t-k)\)._

Theorem 4.1 yields a novel representation of the CII, where the model evaluations \(_{0}(T)\) are independent of \(S\). This allows to utilize every model evaluation to compute all CII scores simultaneously by properly weighting with \(_{s}^{m}\). Notably, our representation relies on \(_{0}\) instead of \(\), which constitutes an important choice for approximation, on which we elaborate in the appendix.

To approximate \(I\), we introduce a _sampling order_\(k_{0} s_{0}\), for which we split the sum in Theorem 4.1 to subsets with \(T_{k_{0}}\) and \(T_{k_{0}}\) and rewrite

\[I^{m}(S)=c_{k_{0}}(S)+_{T p_{k_{0}}(T)}[_{0}(T)^{m}(t,|T S|)}{p_{k_{0}}(T)}]c_{k_{0}}(S):=_{T _{k_{0}}}_{0}(T)_{s}^{m}(t,|T S|),\]

where \(p_{k_{0}}\) is over \(_{k_{0}}\). SHAP-IQ then estimates the CII by Monte Carlo integration.

**Definition 4.2** (Shap-IQ).: _The Shapley Interaction Quantification (SHAP-IQ) of order \(k_{0}\) with \(K\) samples is_

\[_{k_{0}}^{m}(S):=c_{k_{0}}(S)+_{k=1}^{K}_{0}(T_{ k})^{m}(t_{k},|T_{k} S|)}{p_{k_{0}}(T_{k})}T_{1},,T_{K} p_{k_{0}}(T).\]

SHAP-IQ is outlined in the appendix and we establish the following important theoretical guarantees.

**Theorem 4.3**.: _SHAP-IQ is unbiased, \([_{k_{0}}^{m}(S)]=I^{m}(S)\), and consistent, \(_{k_{0}}^{m}(S)I^{m}(S)\). With \(^{2}(S):=[_{0}(T)^{m}(|T|,|T S|)} {p_{k_{0}}(T)}]\) and \(>0\), it holds \((|_{k_{0}}^{m}(S)-I^{m}(S)|>)(S)}{^{2}}\)._

SHAP-IQ provides efficient estimates of all CII scores with important theoretical guarantees. The sample variance \(^{2}\) can further be used for statistical analysis of the estimates.

Finding the sampling order \(k_{0}\) and distribution \(p_{k_{0}}\).In line with KSH and U-KSH , we find \(k_{0}\) in an iterative procedure, outlined in the appendix. We consider _sampling weights_\(q(t) 0\) for \(0 t d\) that grow symmetrically towards the center and consider a distribution \(p_{k_{0}}(T) q(t)\). Given a budget \(M\) and initial \(k_{0}=0\), we consider \(I^{m}(S)=_{T p_{k_{0}}(T)}[_{0}(T)^{m}( t,|T S|)}{p_{k_{0}}(T)}]\) and iteratively increase \(k_{0}\), if for a subset \(T\) of size \(k_{0}\) and \(d-k_{0}\), the condition \(M p_{k_{0}}(T) 1\) is fulfilled. The budget is then decreased by the number of subsets of that size, i.e. \(2}\). This essentially verifies iteratively, if the expected number of subsets exceeds the total number of subsets. For more details and possible choices of _sampling weights_\(q\), we refer to the appendix.

Computational Complexity.In contrast to PB approximations, SHAP-IQ allows to iteratively update _all_ interaction estimates with _one single_ model evaluation for any-order interactions. The weights \(_{s}^{m}(t,k)\) used for the updates can be efficiently precomputed. The updating process can be implemented efficiently using Welford's algorithm , where estimates have to be maintained for all interactions sets, i.e. \(d_{s_{0}}\) in total. In contrast to KB approximation, which requires to solve a weighted least square optimization problem with \(d_{s_{0}}\) variables, the computational effort per interaction increases linearly for SHAP-IQ. Furthermore, SHAP-IQ even allows to update selected interaction estimates, whereas, for instance, KB approximation for FSI requires to estimate all interactions. For more details on the implementation and computational complexity of the baseline methods, we refer to the appendix.

### SHAP-IQ for the Shapley Value

In this section, we show that SHAP-IQ, in the special case of single feature subsets \(s_{0}=1\), yields novel insights into the SV. Furthermore, SHAP-IQ corresponds to U-KSH and greatly simplifies its calculation. Utilizing Theorem 4.1, we find a novel representation of the SV for every feature \(i D\).

**Theorem 4.4**.: _With \(c_{1}(i)=(D)}{d}\) the SV is \(I^{}(i)=c_{1}(i)+_{T_{1}}_{0}(T)(t)[ (i T)-]\)._

SHAP-IQ admits a similar form (see appendix) and corresponds to U-KSH \(^{}_{U}\).

**Theorem 4.5** (SHAP-IQ simplifies U-KSH).: _For \(p(T)(t)\) it holds that \(^{}_{U}=^{m}_{1}\)._

Theorem 4.5 implies that the U-KSH estimator can be computed using the SHAP-IQ estimator, which greatly simplifies the calculation to a weighted sum. The main idea of the proof relies on the observation that not only \(A\) can be explicitly computed, but also \(A^{-1}\), cf. the appendix.

### The Sum of Interaction Scores and SHAP-IQ Efficiency

In this section, we are interested in the sum of CII scores, which we link to a property of SHAP-IQ estimates to maintain the efficiency axiom. By Theorem 4.1, we have \(_{S_{s_{0}}}I^{m}(S)=_{T D}_{0}(T)_{S _{s_{0}}}_{s}^{m}(t,|T S|)\). For the SV, by Theorem 4.4, this sum is zero for every \(T_{1}\). For higher order CIIs, we introduce the following definition.

**Definition 4.6**.: _A CII is s-efficient, if \(_{S D,|S|=s_{0}}_{s}^{m}(t,|T S|)=0\) for every \(T_{s_{0}}\)._

**Theorem 4.7**.: _SII and STI are s-efficient. In particular, SHAP-IQ estimates maintain efficiency for n-SII and STI._

Further, if a CII is s-efficient, then the sum of SHAP-IQ estimates remains constant. Although we did not provide a rigorous statement, it is easy to validate numerically that FSI is not s-efficient. This finding suggests that there are conceptional differences between these indices, that should be further investigated in future work. Using s-efficiency it is also possible to find an explicit formula for the sum of interaction scores for SII, which we give in the appendix.

## 5 Experiments

We conduct multiple experiments to illustrate the approximation quality of SHAP-IQ compared to current baseline approaches.4 We showcase SHAP-IQ estimates on any-order SII (n-SII), on top-order STI and FSI. For each interaction index, we use its specific approximation method as a baseline. For SII and STI, we use the PB approximation and for FSI the KB approximation, further described in the appendix. We then compute n-SII based on the estimated SII values. We compare the baseline methods with SHAP-IQ using \(p(T)(t)\). For each iteration we evaluate the approximation quality with different budgets up to a maximum budget of \(2^{14}\) model evaluations. To account for variation, we randomly evaluate the approximation method on 50 randomly chosen instances, further described below. To quantify the approximation quality, we compute multiple evaluation metrics for each interaction order: mean-squared error (MSE), MSE for the top-K interactions (MSE@K) and the ratio (precision) of estimated top-K interactions (Prec@K). The top-K interactions are determined in regards to their absolute value.

Models.For a language model (LM), we use a fine-tuned version of the DistilBERT transformer architecture  on movie review sentences from the original _IMDB_ dataset [27; 22] for sentiment analysis, i.e. \(\) has values in \([-1,1]\). In the LM, for a given sentence, different feature coalitions are computed by masking absent features in a tokenized sentence. The implementation is based on the _transformers_ API . We randomly sample \(50\) reviews of length \(d=14\) and explain each model prediction. For an image classification model (ICM), we use ResNet18  pre-trained on ImageNet  as provided by _torch_. We randomly sample \(50\) images and explain the prediction of the corresponding true class. To obtain the prediction of different coalitions, we pre-compute super-pixels with SLIC [1; 44] to obtain a function on \(d=14\) features and apply mean imputation on absent features. For a high-dimensional synthetic model with \(d=30\), we use a _sum of unanimity model_ (SOUM) \((T):=_{n=1}^{N}a_{n}(Q_{n} T)\), where \(N=50\) interaction subsets \(Q_{1},,Q_{N} D\) are chosen uniformly from all subset sizes and \(a_{1},,a_{N}\) are generated uniformly \(a_{n}()\). Note that the SOUM could also be viewed as an extension of the induced subgraph game  for a hypergraph with edges of different order.We randomly generate \(50\) instances of such SOUMs.

Ground-Truth (GT) Values.For the LM and the ICM we compute the ground-truth (GT) values explicitly using the representation from Theorem 4.1. For the high-dimensional SOUM it is impossible to compute the GT values naively. However, due to the linearity of the CII and the simple structure of a SOUM, we can compute the exact GT values of for any CII efficiently, cf. the appendix.

### Approximation of any-order SII and n-SII scores using SHAP-IQ

In this experiment, we apply SHAP-IQ on SII and compute estimates for the LM and the ICM up to order \(s=4\). We then compare the estimates with the baseline using the GT values for each order. The results are shown in Figure 2. We display the MSE for the LM (left) and the Prec@10 for the LM (middle) and ICM (right). We further compute the n-SII estimates by aggregating the SII estimates with \(s_{0}=4\) and visualize positive and negative interactions on single individuals as proposed in . Thereby, interactions are distributed equally among each participating feature, which was justified in [3, Theorem 6]. This representation amplifies the variance of our sampling-based estimator. We thus also present SHAP-IQ without sampling, i.e. \(c_{k_{0}}\). The results are shown in Figure 3 (left) and from left to right: GT values, SHAP-IQ, SHAP-IQ without sampling and baseline. Lastly, we illustrate the n-SII scores estimates for \(s_{0}=3\) of a movie review excerpt classified by the LM (right), where the interactions ("is","not"), ("not","bad"), and ("Ill","love","this") yield a highly positive score.

The results show that SHAP-IQ outperforms the baseline methods across different models and metrics. For the n-SII visualization, we conclude that the SHAP-IQ estimator without sampling is preferable, which yields more accurate results than SHAP-IQ and the baseline methods. In general, SHAP-IQ without sampling performs surprisingly strong, and we encourage further work in this direction.

### Approximation of different CIIs using SHAP-IQ

In this experiment, we apply SHAP-IQ on different CIIs, namely SII, STI and FSI. We compute top-order interactions for \(s_{0}=3\) and compare the results with the baselines. Our results are shown in Figure 4 (left) and further experiments and results can be found in the appendix. For the LM,

Figure 2: Approximation quality of SHAP-IQ and the baseline for orders \(s=1,2,3\) of SII measured by MSE for the LM (left) and Prec@10 for orders \(s=2,3\) for the LM (middle) and ICM (right).

SHAP-IQ clearly outperforms the baseline for SII and STI. For FSI, SHAP-IQ is outperformed by the KB approximation of the baseline. As SII and STI rely on PB approximation, our results indicate that KB approximation is more effective than PB approximation for this setting, which is in line with the strong performance of KernelSHAP  for the SV. However, SHAP-IQ, in contrast to KB approximation, provides a solid mathematical foundation with theoretical guarantees and we now consider a high-dimensional synthetic game, where SHAP-IQ outperforms all baselines.

SHAP-IQ on high-dimensional synthetic models.For the SOUM we compute the average and standard deviation of each evaluation metric for SHAP-IQ and the baselines for pairwise interactions (\(s_{0}=2\)) of each index. Our results are shown in Figure 4 (middle and right) and further experiments and results can be found in the appendix. SHAP-IQ outperforms all baseline methods in this setting, in particular, the KB approximation of FSI that performed strongly in ML context. The experiment highlights that there exists no approximation method that performs universally best.

Runtime Analysis.The runtime of SHAP-IQ is affected by different parameters. The computation of the sampling order \(k_{0}\) is a constant time operation given a number of features (cf. Algorithm 2 in the appendix). While the pre-computation of the weights (\(m_{s}\)) scales linearly with the number of features, the additional computational burden is negligible as it does not depend on \(_{0}\). The main computational cost stems from the model evaluations (access to the value function \(_{0}\)), which is bounded by a model's inference time. To illustrate the runtime performance, we compare SHAP-IQ with the baseline methods on the LM using different number of model evaluations \(K\). Figure 5 displays the runtime of SHAP-IQ and the corresponding baseline approaches, including all pre-computations. With increasing \(K\) the runtime complexity scales linearly, but the overhead of SHAP-IQ remains low. Note that the difference in STI can be attributed to less than \(K\) model evaluations, which is required to maintain efficiency, cf. lines 15-16 in Algorithm 6 of the appendix.

Figure 4: Approximation quality for top-order interactions of SII, STI, and FSI of the LM with \(s_{0}=3\) (left) and the SOUM with \(s_{0}=2\) (middle and right).

Figure 3: Visualization of n-SII and \(s_{0}=4\) (left) with (from left to right): GT, SHAP-IQ, SHAP-IQ without sampling, and baseline. Estimated n-SII scores (\(s_{0}=3\)) for a movie review (right).

## 6 Limitations

We presented SHAP-IQ, a unified approximation algorithm for any-order CIIs with important theoretical guarantees. SHAP-IQ relies on the specific structure in terms of discrete derivatives from Definition 3.3. This representation exists for every interaction indices that fulfills the linearity, symmetry and dummy axiom (17, Proposition 5). However, for FSI, which has been defined as the solution to the weighted least square problem, a closed-form representation for lower-order interactions in terms of discrete derivatives remains difficult to establish (40, Lemma 70). This limits the applicability of SHAP-IQ to top-order interactions of FSI, for which this representation is given in (40, Theorem 19). The FSI baseline performs strongly in ML context, in line with empirical findings for KernelSHAP (26) for the SV. However, the estimator is theoretically not well understood (8) and we have shown that it is not universally best. Moreover, for other CIIs, such as SII and STI, it is unlikely (13) that such an explicit form in terms of a weighted least square problem can be found, which limits the applicability of KB approximation to FSI. SHAP-IQ outperforms the baselines of SII and STI by a large margin, is generally applicable and supported by a solid mathematical foundation.

## 7 Conclusion

How to extend the SV to interactions is an open research question. In this work, we considered CIIs, a broad class of interaction indices, which covers all currently proposed indices, as well as all indices that fulfill the linearity, symmetry and dummy axiom. We established a novel representation of the CII, which we used to introduce SHAP-IQ, an efficient sampling-based approximation algorithm that is unbiased and consistent. For the special case of SV, SHAP-IQ can be seen as a generalization of U-KSH (8) and greatly simplifies its calculation as well as providing a novel representation of the SV. Furthermore, for n-SII and STI, SHAP-IQ maintains the efficiency condition, which is a direct consequence of a specific property, which we coin s-efficiency for CIIs. We applied SHAP-IQ in multiple experimental settings to compute any-order interactions of SII and n-SII, where SHAP-IQ consistently outperforms the baseline method and showcased the applicability of feature interaction scores to understand black-box language and image classification models. SHAP-IQ further benefits from a solid statistical foundation, which can be leveraged to improve the approximation quality.

Future work.Applying SHAP-IQ to real-world applications, such as NLP tasks (43) and genomics (48, 23), could yield valuable insights. However, the exponentially increasing number of interactions requires human-centered post-processing to enhance interpretability for practitioners and ML engineers, e.g. through automated dialogue systems (37). Further, it would be beneficial to discover the statistical capabilities of SHAP-IQ to provide confidence bounds or approximate interaction scores sequentially. Beyond model-agnostic approximation, model-specific variants could substantially reduce computational complexity. For instance, it is likely that ideas of TreeSHAP (25) for tree-based models can be extended to Shapley-based interactions.