# COSMIC: Compress Satellite Images Efficiently via Diffusion Compensation

Ziyuan Zhang\({}^{1}\) Han Qiu\({}^{1}\)* Maosen Zhang\({}^{1}\) Jun Liu\({}^{1}\)*

Bin Chen\({}^{2}\) Tianwei Zhang\({}^{3}\) Hewu Li\({}^{1}\)

\({}^{1}\) Tsinghua University, China

\({}^{2}\) Harbin Institute of Technology, Shenzhen, China

\({}^{3}\) Nanyang Technological University, Singapore

{ziyuan-z23,zhangs24}@mails.tsinghua.edu.cn, {qiuhan,juneliu}@tsinghua.edu.cn

chenbin2021@hit.edu.cn, tianwei.zhang@ntu.edu.sg, lihewu@cernet.edu.cn

###### Abstract

With the rapidly increasing number of satellites in space and their enhanced capabilities, the amount of earth observation images collected by satellites is exceeding the transmission limits of satellite-to-ground links. Although existing learned image compression solutions achieve remarkable performance by using a sophisticated encoder to extract fruitful features as compression and using a decoder to reconstruct, it is still hard to directly deploy those complex encoders on current satellites' embedded GPUs with limited computing capability and power supply to compress images in orbit. In this paper, we propose COSMIC, a simple yet effective learned compression solution to transmit satellite images. We first design a lightweight encoder (i.e. reducing FLOPs by \(2.6 5\)) on satellite to achieve a high image compression ratio to save satellite-to-ground links. Then, for reconstructions on the ground, to deal with the feature extraction ability degradation due to simplifying encoders, we propose a diffusion-based model to compensate image details when decoding. Our insight is that _satellite's earth observation photos are not just images but indeed multi-modal data with a nature of Text-to-Image pairing_ since they are collected with rich sensor data (e.g. coordinates, timestamp, etc.) that can be used as the condition for diffusion generation. Extensive experiments show that COSMIC outperforms state-of-the-art baselines on both perceptual and distortion metrics. The code is publicly available at https://github.com/Joanna-0421/COSMIC.

+
Footnote †: *Corresponding authors.

## 1 Introduction

The revival of the aerospace industry , coupled with reduced costs of launching rockets , has fueled an exponential increase in the number of nanosatellites, resulting in massive growth in images collected in-orbit. For example, the Sentinel-3 missions can collect a maximum of 20 TB raw data on satellites (mainly earth observation images) every day . However, the data transmission capability between satellites and ground stations has clear upper bounds . This situation of the rapid growth of images collected by satellites versus the limited transmission capability to the ground requires effective image compression on satellites before transmission back to Earth.

Current industrial compression solutions for satellite images rely on JPEG , JPEG2000 , or CCSDS123  (e.g. satellite BilSAT-1 ). These solutions are outperformed by various learned compression methods  in various cases. Existing learned image compression methods  use sophisticated encoders to extract fruitful features and then use a decoder to decompress .

Although we notice a novel promising trend of _deploying embedded GPUs on satellites_ in both academia [16; 17; 6; 45] and industry (e.g. satellite Phi-Sat-1 , Chaohu-1 , and Forest-1 ) which brings the potential opportunity of using learned compressors on satellites. It is still hard to directly adopt existing learned compression solutions for satellites since their sophisticated encoders are still too complex for GPUs on satellites (e.g. NVIDIA Jetson Xavier NX on Forest-1 ) which have limited computing capacity and power supply . We have two insights to fill the above gaps. (1) We first design a lightweight encoder on satellites with a higher priority of compression ratio than feature extraction ability. (2) At the receiver's end on the ground, we deal with this simple encoder's feature extraction ability degradation by compensating image contents when decoding. We choose diffusion as compensation due to its powerful generation capability and, more importantly, _satellites' earth observation photos are not only images but enjoy a multi-modal nature in which rich real-time sensor information is the description of the corresponding photo_. For instance, in Figure 1, the coordinates (e.g. latitude and longitude) denote the location of the image which describes its main category (e.g. sea, city, etc.) and the timestamp describes the image's lightning-like day or night.

In this paper, we propose to **CO**mpress **S**atellite **iM**age via **iI**ffusion **C**ompensation (COSMIC), a novel learned image compression method for satellites. COSMIC has two key components, i.e., (1) a lightweight encoder for compression on satellite and (2) a sophisticated decompression process with a decoder and a diffusion model on the ground with sufficient GPUs. First, we design a lightweight convolution architecture to extract local features and apply convolution to obtain an attention map of global features, to realize a lightweight image compression encoder in terms of FLOPs. Please note that lightweight encoders usually extract fewer key features which increases the difficulty for the decoder to decompress. Thus, our second component, decompression, has two parts including decoding with a corresponding decoder and, more importantly, a compensation model. Inspired by the multi-modal nature of the satellite's images, we aim to build text-to-image pairs (image and its sensor information like Figure 1) and use diffusion as the compensation model.

We compared COSMIC with 6 state-of-the-art (SOTA) baselines, 3 of which are based on generative models, considering both distortion and perceptual metrics. In addition, we constructed two image compression test sets based on satellite images by considering ordinary scenes and unique tile scenes in satellite imagery. Extensive experiments have proven that COSMIC significantly reduces the encoder's complexity to \(2.6 5\) fewer FLOPs while achieving better performance on almost all metrics than baselines. Our contributions can be summarized as follows.

* We propose a novel idea that uses a lightweight image compression encoder on satellites and leverages satellite images' text-to-image pairing nature for compensation when decompressing.
* We propose a novel compensation model based on stable diffusion to compensate image details when decompressing with the unique sensor data of satellite images as descriptions.
* We analyzed the characteristics of satellite images in detail and incorporated them into the training and inference stages. In addition, we constructed two datasets under satellite image transmission scenarios, taking into account the typical satellite image transmission tasks like tile scenes.

## 2 Background

### Earth observation missions on satellites

Earth observation missions (e.g. NASA's Landsat Program ) involve the use of satellite photos to monitor and collect data for tasks like forestry , agriculture [14; 41], land degradation , land use and land cover , biodiversity , and water resource [35; 47]. Traditional earth observation missions rely on a pipeline in which satellites take photos and then send them back to ground stations for analysis. Recently, along with the reduced cost of launching rockets and manufacturing

Figure 1: An example of the satellite’s earth observation image and this image’s corresponding sensor data as a description.

nanosatellites [21; 6], the photos on satellites are rapidly increasing which brings novel challenges for transmitting photos back to the ground. A recent promising approach is to deploy embedded GPUs on satellites to support DNN models to either filter useless data before transmission or make partial processing tasks on satellites. For instance, ESA's satellite Phi-Sat-1  first deploy Intel VPU on satellite to support DNN models for filtering useless photos (e.g. covered by clouds) that can save 30%+ transmission volume. OroraTech has launched AI nanosatellites with the NVIDIA Jetson Xavier NX for wildfire detection , and Orbital Sidekick uses NVIDIA Jetson AGX Xavier as the AI engine at the edge of the satellite to detect gas pipeline leaks . However, due to the inelastic computational capabilities of onboard satellites and limited power supply only from the sunshine (i.e., up to 15 Watt for GPUs on satellites ), a certain amount of images are still needed to be transmitted back to the ground. This brings an urgent need for satellite-specific image compression methods.

### Neural image compression methods

**Learned image compression methods** have achieved remarkable rate-distortion performance compared with classical information theory-based image compression methods, to each of which consists of an encoder \(\), a quantization \(\), and a decoder \(\). The encoder, as the most critical part, extracts key features from the image as the latent representation. Higher quality representation extracted by the encoder means less content loss at compressing which is more likely to reconstruct a higher quality image when decompression. Thus, SOTA approaches explore introducing more complex modules into the encoder.  integrates the transformer into the CNN encoder and uses the transformer-CNN mixture block to extract rich global features. The other approaches aims to reduce the complexity of the decoders that are deployed on edge devices like smartphones. For instance,  adopts shallow or even linear decoding transforms to reduce the decoding complexity, compensated by more powerful encoder networks and iterative encoding.

**Generative models for decompression.** Although VAE-based methods have achieved good performances, optimizing solely for mean square error (MSE) can lead to excessive image smoothing, resulting in visual artifacts. More recent works [36; 53; 30; 24] have combined VAEs with generative models (e.g. diffusion) to achieve better visual results.  uses a conditional diffusion model as the image compression decoder which improves visual results.  and  decouple the compression task and augmentation task, sending the output of the VAE codec to diffusion to predict the residual.

**Satellite image compression method.** There are some compression methods specifically for remote sensing images [57; 23; 50].  uses discrete wavelet transform to divide image features into high-frequency features and low-frequency features, and design a frequency domain encoding-decoding module to preserve high-frequency information, thereby improving the compression performance.  explore local and non-local redundancy through a mixed hyperprior network to improve entropy model estimation accuracy. Few of these works focus on onboard deployment.  use the CAE model to extract image features and reduce the image dimension to achieve compression, and deploy the model on VPU. However, this method only considers the reduction of image dimension and does not consider the arithmetic coding process in the actual transmission process, resulting in the image compression rate can only be adjusted by changing the model architecture.

**Limitations to use for satellites.** Most of the above approaches don't consider lightweight compression encoders which makes them impractical to deploy on satellite's embedded GPUs constrained by computing capacity and power supply. The approach used on VPU is impractical as the image compression rate is highly related to model architecture. Besides, none of them pay attention to the multi-modal nature of satellite earth observation images to introduce conditions to further improve decompression quality.

## 3 Preliminaries

### Problem formulation

We formulate the basic process of learned image compression. The encoder \(\) uses a non-linearly transformation to convert the input image \(\) into the latent representation \(\), which is subsequently discretized and entropy-coded by quantization \(\) under a learned hyper prior \(\). Under the stochastic Gaussian model, each discrete code \(_{}\) can be expressed as a Gaussian distribution with mean \(_{}\) and variance \(_{}\) given a hyper prior \(_{}\): \((_{} _{}=(_{},_{ }^{2}))\). The decoder \(\) reconstructs the discrete representation \(||\) to the image \(}\). The model can be optimized by the loss function (Eq. 1).

\[_{}=+=[-_{2} (||||)-_{ 2}()]+[( ,})],\] (1)

where \(\) is the bit rate of latent discrete coding, \(\) is the distortion between the original and the reconstructed image (measured by MSE), and \(\) controls the trade-off between rate and distortion.

### Diffusion model

Diffusion model is a type of generative model that can generate images from Gaussian noise through multi-step iterative denoising. These models include two Markov processes. First, the diffusion process gradually applies noise to the image until the image is destroyed and becomes complete Gaussian noise. Then, in the reverse stage, it learns the process of restoring the Gaussian noise to the original image. During the inference stage, given a random noise sample \(_{}(0,1)\), the diffusion model can denoise through \(\) steps and gradually generate a photorealistic image \(_{0}\). At each step \(\{0,1,...,\}\), intermediate variable \(_{}\) can be expressed as \(_{}=}}_{-1} +_{}_{}\), where \(_{}(0,1)\) is the variance hyperparameter of Gaussian distribution, and satisfies \(_{1}<_{2}<...<_{}\); \(_{}(0,1)\) is the Gaussian noise at step \(\). In the diffusion model, a noise prediction network (\(_{}\)) is used to predict the noise at step \(\), and \(_{-1}\) can be obtained from \(_{}\).

The diffusion model can also understand the content of the given conditions, such as text and images, and generate images consistent with the conditions. In this case, the noise prediction network takes three parameters: intermediate sample \(_{}\), timestep \(\), and given condition \(\) as input. To guarantee the noise predicted by the noise prediction network at the \(\)-th step of the reverse process has the same distribution as the noise introduced into the image during the diffusion process, diffusion model usually use \(_{2}\) to optimize the network following \(_{}\|_{}-_{}(_{},,)\|\).

Stable diffusion  is proposed to reduce the training cost, which implements the diffusion process in a low-dimensional latent space while retaining the high-dimensional information in the original pixel space for decoding. In this article, we aim to leverage the powerful generation ability of stable diffusion and its ability to maintain consistency with a given condition to provide compensation for the information lost by the image compression encoder.

## 4 Method

### Framework of Cosmic

As illustrated in Figure 2, COSMIC consists of two components: a compression module and a compensation module. To adapt to the satellite scenarios, the compression module includes a lightweight image compression encoder \(\) and an entropy model deployed on the satellite (Sec. 4.2), as well as an image compression decoder \(\) deployed at the ground station. To fix the content detail loss caused by the lightweight encoder, a compensation module is proposed, which is entirely deployed at the ground station. It has an encoder \(}\), aiming to extract compensation information \(_{0}\) from original images, which is received by decoder \(\) as compensation for latent representation \(^{}\) extracted by \(\) during training (Sec. 4.3). During the inference phase, the noise prediction network generates compensation information \(_{0}t\) from noise to simulate \(_{0}\) as a compensation (Sec. 4.4).

### Lightweight image compression encoder

To make the image compression encoder \(\) practical on satellites, we first make it lightweight. The main idea is to reduce the amount of calculation required for image compression in terms of FLOPs. We followed the classic architecture of image compression encoder , which is structured with downsampling convolutions with a stride of 2 and generalized divisive normalization (GDN)  arranged alternately (Figure 2 (a)). Since GDN provides the best performance for image compression when the number of channels is 192 , the increase in convolution filters will exponentially increase the calculation amount of convolution. To reduce the amount of computation required for image downsampling, we propose a lightweight convolution block (LCB), as in Figure 2 (d), which uses depthwise convolution to replace the ordinary convolution with a convolution kernel size of \(5 5\). To interact between different channels of the feature map, depthwise convolution is followed by a \(1 1\) convolution with a full number of channels. Inspired by , which proves that there is a lot of redundancy in the features extracted by convolution, in LCB, only half of the output feature maps are obtained through \(1 1\) convolution, and the remaining half of the output feature maps are obtained through linear transformations with cheap cost using redundant features.

Transformer-based methods [58; 37] can outperform CNN-based methods as the attention can capture non-local information of the images. However, the computational complexity of self-attention has a quadratic relationship with the size of the input feature map, which is not computationally friendly. Inspired by , we use two one-dimensional convolutions in series. The first convolution is used to extract horizontal information. On this basis, the second convolution is used to vertically synthesize the previously extracted horizontal information to obtain the global attention map. Meanwhile, the other branch uses LCB with one stride to capture local information, as shown in Figure 2 (c).

### Compensation-guided image compression

After giving a lightweight design of the image compression encoder \(\), we note that the representation ability of this encoder is inevitably degraded. Specifically, the features contained in the latent representation \(\) received by the ground station are not enough to let the decoder reconstruct a high-quality image. Thus, we explore compensation for the degradation in encoding.

It is well-known that stable diffusion has powerful generation capabilities for specified content from noise under the guidance of text information . The ground station can obtain not only the latent representation compressed by the encoder but also rich sensor data like the geographical location, time, camera parameters, etc. along with each image. We use these sensor data as conditions to guide diffusion generation to fix the missing image details. The training is divided into two stages. In the first stage, we train the compression model. As shown in (Figure 2 (b)), since the Image decoder \(\) needs two parts of information (i.e. \(^{}\) and \(}\) in Figure 2) for decoding, we introduce another image encoder \(}\) to extract compensation information \(}\) from the original image. In the first stage, \(\), \(}\) and \(\) are trained together. The reconstructed image \(}\) can be expressed as in Eq. 2.

\[}=(( (),}(} )))\] (2)

In the second stage of training, we freeze the parameters of \(\), \(}\) and \(\), and train the noise prediction network, with the goal of making the information generated by the diffusion model as close to \(}\) as possible, denoted as \(}^{}\), so as to generate the compensation information required by the decoder.

Figure 2: **COSMIC framework.** (a) Compression module for satellite images: a lightweight encoder and a compensation-based decoder (Sec. 4.3). (b) In the noise prediction network, each Cross-Attention (CA) block receives embedding of the Metadata Encoder (ME) (Sec. 4.4), and the Vanilla Convolution (VC) blocks use latent image discrete encoding to guide the prediction of noise for each diffusion step. (c) & (d) Convolution attention module and lightweight convolution block (Sec. 4.2).

During the inference phase, the trained diffusion model can generate compensation information \(}\). Therefore, we no longer need \(}\). The \(}\)\(\) generated by the diffusion model replaces the \(}\) extracted by \(}\) to help the image decoder decompress the image.

### Conditional diffusion model for loss compensation

As pointed out in Sec. 1, earth observation images collected by satellites are indeed multi-modal data. Here we consider that a satellite image \(_{0}\), when transmitted to the ground station, contains a discrete image coding \(\) paired with its sensor information denoted as numerical metadata \(\). For \(^{}\), just like diffusion handles timestep \(\), we use sinusoidal embedding (\(_{}\)) to encode them to \(_{}^{1}(j=1,2,... )\), where \(\) is the dimension of the clip embedding, as we concatenate the metadata embedding together as a description of an image. This process is expressed as in Eq. 3.

\[^{}=(([ _{}(_{1}),...,_{}(_{M})]))\] (3)

This final metadata condition will be incorporated into the latent representation using CrossAttention (CA) blocks to guide the generation process.

Stable diffusion was originally used for generative tasks, which have randomness. Here, we expect that stable diffusion generates image details that are not extracted by the satellite image encoder \(\), and still retain the overall structure of the image. To address this problem, we inject the discrete image coding \(\) into the Vanilla Convolution (VC) blocks of the noise prediction network to provide the structure information and improve content consistency, which can be described in Eq. 4.

\[}^{}=}+}( ),\] (4)

where \(}\) is the i-th feature map of the U-Net backbone, and \(}\) is the upsampling convolution used to align the dimensions between \(\) and \(}\). Guided by image coding and the metadata as a description, we use MSE loss to minimize the distance between target distribution and learned distribution in latent space as in Eq. 5.

\[_{}=_{,_{0}, (0,1)}[_{}- _{}(},, ,^{})^{2}]\] (5)

## 5 Experiments

### Setup

**Dataset.** We use the function Map of the World (fMoW) , which has 62 categories, and in which each image is paired with different types of metadata features, as our training data and test data. For training data, we randomly crop the image to a resolution of \(256 256\) pixels. For information collected by satellite sensors as metadata, we choose Coordinates, Timestamp, GSD, Cloud cover, Off-nadir Angle, Target Azimuth, Sun Azimuth, and Sun Elevation provided by fMoW.

For testing data, we constructed two test sets with different resolutions. One is from the fMoW test set, where to ensure a comprehensive representation of categories, we randomly selected one image from each category, cropped it to a resolution of \(256 256\), and used it as a standard test set. For another test set, we considered the actual scenario of the satellite to construct a tile test set. Since the image captured by the satellite is a large geographic region, the computing resources on the satellite are limited, and large-size images cannot be processed directly, so images should first be cut into smaller sub-images, this process is known as tiling [31; 51; 17; 11]. In this paper, for images of size \(2306 2306\), we first divide each image into 81 smaller patches of size \(256 256\) each, and then compress and decompress each patch individually, as illustrated in Figure 5 (a). After obtaining all the decompressed patches, we reassemble them as one image for further evaluation.

**Metrics.** We use 4 metrics for quantitative measures following previous works [39; 36; 53]. For distortion comparison, we use the Peak Signal-to-Noise Ratio (PSNR) and Multi-Scale Structural Similarity Index Measure (MS-SSIM) to validate the pixel fidelity and measure brightness, contrast, and structural information at different scales. For perceptual comparison, we choose Learned Perceptual Image Patch Similarity (LPIPS) and Frechet Inception Distance (FID).

**Model training details.** The training has two stages. First, we train the image compression encoder \(\), image encoder \(}\) and image decoder \(\) together using \(_{}\) for 100 epochs with a batchsize of 32.

Second, we freeze the parameters of the model trained in the first stage, use the pretrained stable diffusion model for the noise prediction network, and finetune it using \(_{}\) for 10 epochs with a batchsize of 4. All the training experiments are performed on \(10\) NVIDIA GeForce RTX 3090 using Adam optimizer with \(=1 10^{-4}\) and \(\{0.00067,0.0013,0.0026,0.005\}\). During inference, we utilize the DDIM sampling  with 25 steps.

**Baselines.** We consider 6 baselines including traditional methods, VAE-based methods, and generative model based methods. **Elic** proposes a multi-dimension entropy estimation model, which can effectively reduce the bit rate and improve the coding performance. **Hific** pays more attention to the perception of the model reconstruction effect, obtaining visually pleasing reconstructed images. Based on Hific, **COLIC** considers the semantic information of the image when designing the loss function, and treats structure and texture respectively. **CDC** is the first work to use the diffusion model as an image compression decoder, performing the reverse process of diffusion in pixel space to reconstruct the image. **HL_RS** is an image compression method, especially for remote sensing images, which processes the high-frequency part and the low-frequency part of the images separately to better preserve the important high-frequency features of remote sensing images. For these 5 baselines, we retrain their models with the fMoW dataset for a fair comparison. Besides, we choose JPEG2000, the industrial solution for satellite image compression , for comparison.

### Comparison with baselines

**RD performance.** Figure 3 shows the comparison results with baselines on two test sets. The dotted line in the figure represents the baselines, and the solid line represents COSMIC. We demonstrate results from two perspectives, i.e., distortion and perception. Across all transmission rates, COSMIC surpasses the baselines in terms of LPIPS, and FID. At low bpp, the MS-SSIM of COSMIC is lower compared to the baseline. This is mainly because as the bpp decreases, the encoder extracts less information, and during the decompression process, there is a greater reliance on diffusion-based generation (more details in Sec. 5.4). Additionally, due to the degraded feature extraction of the lightweight encoder, the feature obtained at low bpp is insufficient to guide the diffusion process in generating high-fidelity images. As the bpp increases, the latent coding contains more features, resulting in a significant improvement in the MS-SSIM of COSMIC, demonstrating SOTA performance. Note that the sensor data is transmitted to the ground by default in earth observation missions. Besides, the volume of these sensor data is negligible compared with images so we do not consider them when counting bpp. We show more results of more metrics and baselines including other VAE-based methods in Appendix B which COSMIC achieves the SOTA performance on all 6 perceptual metrics.

**Visual results.** Figure 4 shows the example of reconstructed images at low bitrates and high bitrates. For fair comparison, we only show the results of optimizing for image perception. Figure 5 (b) shows an example of high-resolution image reconstruction by COSMIC and baselines. Due to the tiling and

Figure 3: Trade-off between bitrates and different metrics on COSMIC and baselines. The \(\) (\(\)) means higher (lower) is better. The first row is for the fMoW test set (image size \(256 256\)). The second is for the tile test set by comparing between the stitched images and their original ones.

stitching process in image compression, we pay particular attention to the seams where different small patches form a larger image. JPEG2000 exhibits noticeable misalignment at the image seams. Hific and COLIC can not accurately restore the details of the seam. For example, in the picture outlined in orange, the car headlight at the seam has been reconstructed into a red dot. The diffusion-based CDC reconstruction also exhibits some color differences between different sub-images and shows noticeable misalignment, such as the eaves at the seam in the picture outlined in red. Compared to baselines, COSMIC maintains a higher similarity in structure and color between different sub-images, resulting in significant visual improvements.

### Encoder efficiency analysis

We evaluate the lightweight encoder of COSMIC in terms of FLOPs in Table 1. Compared with baselines, the _on-satellite FLOPs_ (including the encoder and entropy model) of COSMIC has been significantly reduced by roughly \(2.6 5\) while the overall performance of COSMIC can still outperform baselines under a similar bitrate.

### Ablation study

**w/o DC**. To demonstrate the compensatory role of diffusion in the image reconstruction process, we remove the diffusion compensation module, and the results are shown in Figure 6. We remove the compensation module and retrain the model to show the quantitative metrics. The result shows that the diffusion model plays an important role in decompressing images to get better perceptual metrics. To show the compensatory role of the diffusion model more clearly, we directly remove the diffusion model and only use the output information of the lightweight encoder \(\) to reconstruct the image. We find that at low bitrate, diffusion compensation is more important. Due to the insufficient feature extraction capability of the lightweight encoder, many image content details are lost to save the transmission rate, and diffusion needs to reconstruct much of the image content guided by the limited output of the encoder, along with the metadata. As the bitrate increases, more features extracted by the encoder can be retained, and at this point, diffusion only needs to compensate for some image details that the encoder failed to capture. The visual results are shown in Figure 7. The results indicate that using the diffusion model as compensation is very useful, especially with a small bitrate.

  Method & FLOPs (G) & PSNR\(\) & MS-SSIM\(\) & LPIPS\(\) & FID\(\) & bpp\(\) \\  CDC  & 13.1 & 31.98 & 0.982 & 0.0462 & 45.49 & 0.66 \\ COLIC  & 26.4 & 29.04 & 0.975 & 0.0530 & 32.56 & 0.64 \\ Hific  & 26.4 & 29.11 & 0.977 & 0.0384 & 27.01 & 0.60 \\ Eile  & 21.78 & **33.31** & **0.983** & 0.0683 & 60.80 & 0.54 \\ HL\_RS  & 11.87 & 31.30 & 0.979 & 0.0782 & 27.38 & 0.56 \\ COSMIC & **4.9** & 32.11 & 0.980 & **0.0359** & **13.50** & 0.59 \\  

Table 1: Comparison of the on-satellite FLOPs with baselines on the tile test set. Best performances are highlighted in **bold**.

Figure 4: Decompressed fMoW images (full images in supplementary material). \(1^{st}\) row: comparison under low bitrates, COSMIC shows better visual effects. Compared with CDC, COSMIC still gets slightly better visual reconstruction with less bitrates. \(2^{nd}\) row: comparison under high bitrates.

w/o CAM. To show the CAM module can capture non-local information, which can help the encoder \(\) to get higher quality representation. We remove the CAM module and show the results in Figure 6. The result shows that CAM module is effective and can achieve better RD performance.

**w/o ME**. To show that sensor data can guide the generation of diffusion, we remove the metadata encoder and only use image encoding for diffusion generation. Please note that there are mainly two kinds of sensor data here including the camera parameters such as the off-nadir angle and target azimuth used during image capture and the data on cloud cover, illumination, and ground sample distance, etc. Figure 6 indicates that sensor data helps guide the diffusion in reconstructing the image.

**Influence of decoding steps**. We further investigate the impact of different denoising step counts in the reverse diffusion process on the reconstructed image quality, as shown in Figure 8. We find that as the number of denoising steps increases, the perceptual metrics of the generated images gradually improve, which is consistent with the exploration of the relationship between denoising steps and FID scores in DDIM . While the perceptual results improve, distortion metrics such as PSNR experience a slight decline, showcasing the trade-off between perceptual quality and distortion. In practical deployment and downstream tasks, the number of denoising steps can be selected based on different emphases on distortion and perceptual performance to suit actual applications.

### Compression influence on downstream tasks

To confirm that COSMIC will not affect the downstream remote sensing tasks, such as image classification, we choose the image classification task in  to show the effect caused by compression, as shown in Table 2. The same test images are compressed at a low bitrate level and decompressed, then processed through the classification model to obtain the accuracy changes on different numbers of

  Classes & Original & JPEG2K  & Elic  & COLIC  & HIFIC  & CDC  & HL\_RS  & COSMIC \\ 
10 & 98.95\% & -4.21\% & -5.27\% & -1.06\% & -1.06\% & -2.11\% & -2.11\% & -1.06\% \\
15 & 97.92\% & -4.17\% & -3.84\% & -0.70\% & -0.70\% & -1.40\% & -1.39\% & -0.70\% \\
20 & 98.42\% & -3.16\% & -3.68\% & -0.53\% & -0.53\% & -1.06\% & -2.1\% & -1.06\% \\  

Table 2: Effect on image classification model.

Figure 5: (a) Illustration of the tile test set. A high-resolution image is divided into many small sub-images (or patches), each of which is compressed individually. The reconstructed sub-images are then placed back in their original positions and stitched together to form a high-resolution reconstructed image. (b) On the tile test set, we provide two detailed views of a stitching area (outlined in orange and red). The visual comparison between COSMIC and the baseline shows that COSMIC achieves the best visual effects in terms of texture alignment and consistency in color brightness.

classes. The result shows that JPEG2000 and Elic reduce the accuracy by the most and COSMIC is outstanding in learning methods.

## 6 Conclusion

We present COSMIC, a novel approach to compress images for satellite earth observation missions. We first design a lightweight encoder to adapt to the limited resources on satellites. Then, we introduce a conditional latent diffusion model by using the sensor data of satellite as instructions to compensate the missing details due to the lightweight encoder's degradation of feature extraction. Extensive results indicate that COSMIC can not only achieve SOTA compression performance for 2 typical satellite tasks but also guarantee the accuracy of satellite images' downstream tasks.

**Limitations & Future work.** Even though COSMIC compensates for encoder limitations using diffusion, at extremely low bpp (e.g., less than 0.1bpp), the information provided by latent image coding may not be enough to support diffusion in generating high-fidelity images. We think the main reason is that we only finetune the pretrained Stable Diffusion model, which lacks sufficient prior knowledge of satellite images. As a prospective solution, we will train a diffusion model specifically for satellite images or use historical satellite images as a reference for further improvements.

**Acknowledgement.** This work was supported by the National Key R&D Program of China (2022YFB3105202), National Natural Science Foundation of China (62106127, 62301189, 62132009), and key fund of National Natural Science Foundation of China (62272266).

Figure 8: Compression performance with different numbers of decoding step.

Figure 6: Ablation study with different variants of COSMIC. “w/o DC” indicates that diffusion compensation is not used during decoding. “w/o CAM”denotes the CAM module is removed in the encoder. “w/o ME” denotes that we remove the metadata encoder.

Figure 7: Contrast visual results. “w/ DC” means using of diffusion for compensation during decoding, while “w/o DC” indicates that diffusion compensation was not used during decoding.