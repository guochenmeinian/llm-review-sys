# xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology

Julius Hense\({}^{1,2,*,}\) &Mina Jamshidi Idaji\({}^{1,2,*,}\) &Oliver Eberle\({}^{1,2}\) &Thomas Schnake\({}^{1,2}\) &Jonas Dippel\({}^{1,2,3}\) &Laure Ciernik\({}^{1,2}\) &Oliver Buchstab\({}^{4}\) &Andreas Mock\({}^{4,5}\) &Frederick Klauschen\({}^{1,4,5,6}\) &Klaus-Robert Muller\({}^{1,2,7,8,}\)

\({}^{1}\)Berlin Institute for the Foundations of Learning and Data, Berlin, Germany

\({}^{2}\)Machine Learning Group, Technische Universitat Berlin, Berlin, Germany

\({}^{3}\)Agnostics GmbH, Berlin, Germany

\({}^{4}\)Institute of Pathology, Ludwig Maximilian University, Munich, Germany

\({}^{5}\)German Cancer Research Center, Heidelberg, and German Cancer Consortium, Munich, Germany

\({}^{6}\)Institute of Pathology, Charite Universitatsmedizin, Berlin, Germany

\({}^{7}\)Department of Artificial Intelligence, Korea University, Seoul, Korea

\({}^{8}\)Max-Planck Institute for Informatics, Saarbrucken, Germany

\({}^{*}\)Equal contribution

\({}^{}\){ j.hense, mina.jamshidi.idaji, klaus-robert.mueller }@tu-berlin.de

###### Abstract

Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology. Codes are available at: https://github.com/bifold-pathomics/xMIL.

## 1 Introduction

Multiple instance learning (MIL)  is a learning paradigm in which a single label is predicted from a bag of instances. Various MIL methods have been proposed, differing in how they aggregate instances into bag information . MIL has become particularly popular in histopathology, where gigapixel microscopy slides are cut into patches representing small tissue regions. From these patches, MIL models can learn to detect tumor  or classify disease subtypes , aiming to support pathologists in their routine diagnostic workflows. They have further demonstrated remarkable success at tasks that even pathologists cannot perform reliably due to a lack of known histopathological patterns associated with the target, e.g., predicting clinically relevant biomarkers  or outcomes like survival  directly from whole slide images.

Explaining which visual features a MIL model uses for its prediction is highly relevant in this context. It allows experts to sanity-check the model strategy , e.g., whether a model focuses on the disease area for making a diagnosis. This is particularly important in histopathology, where models operating in high-stake environments are prone to learning confounding factors like artifacts or staining differences instead of actual signal [20; 21; 22]. On top of that, MIL explanations can enable pathologists to discover novel connections between visual features and prediction targets. For example, the explanations could reveal a previously unknown association of a histopathological pattern with poor survival, leading to the identification of a targetable disease mechanism. Previous works have shown the potential of scientific knowledge discovery from explainable AI (XAI) [22; 23; 24; 25; 26].

Most studies have used attention scores as MIL explanations [3; 4; 6; 27; 28; 29]. However, it has been shown that attention heatmaps are limited in faithfully reflecting model predictions [30; 31; 32; 33]. Further MIL explanation methods have been proposed, including perturbation schemes passing modified bags through the model  and architectural changes towards fully additive MIL models . Nevertheless, these methods do not account for the complexities inherent to many histopathological prediction tasks, as they are limited to small bag sizes or disregard instance interactions.

We revisit MIL through the lens of XAI and introduce xMIL, a more general and realistic multiple instance learning framework including requirements for good explanations. We then present xMIL-LRP, an adaptation of layer-wise relevance propagation (LRP) [35; 36] to MIL. xMIL-LRP distinguishes between positive and negative evidence, disentangles instance interactions, and scales to large bag sizes. It applies to various MIL models without requiring architecture modifications, including Attention MIL  and TransMIL . We assess the performance of multiple explanation techniques via three toy experiments, which can serve as a novel benchmarking tool for MIL explanations in complex tasks with instance interactions and context-sensitive targets. We further perform faithfulness experiments on four real-world histopathology datasets covering tumor detection, disease subtyping, and biomarker prediction. xMIL-LRP consistently outperforms previous attempts across all tasks and model architectures, with the biggest advantages observed for Transformer-based biomarker prediction.

Figure 1 showcases the importance of understanding positive and negative evidence for a prediction. Only xMIL-LRP uncovers that the model found evidence for the presence of the biomarker, but stronger evidence against it. This explains why it predicted the biomarker to be absent and enables

Figure 1: In digital pathology, heatmaps guide the identification of tissue slide areas most important for a model prediction. The figure displays heatmaps from different MIL explanation methods (columns) for a head and neck tumor slide (top row) with a selected zoomed-in region (bottom row). The MIL model has been trained to predict HPV status. The xMIL-LRP heatmap shows that the model identified evidence in favor of an HPV infection at the tumor border (red area) and evidence against an HPV infection inside the tumor (blue area, lower half of the tissue). The dominant blue region explains why the model mispredicted the slide as HPV-negative. Investigation of the tumor border by a pathologist revealed a higher lymphocyte density, which is one of the known recurrent but not always defining visual features of HPV infection in head and neck tumors. xMIL-LRP allows pathologists to extract fine-grained insights about the model strategy. In contrast, the “attention” and “single” methods neither explain the negative prediction nor distinguish the relevant areas.

pathologists to extract insights about the visual features that support or reject the presence of the biomarker according to the model. The example illustrates the strength of our approach, suggesting that xMIL-LRP represents a significant advance for model debugging and knowledge discovery in histopathology.

The paper is structured as follows: In Section 2, we review MIL assumptions, models, and explanation methods related to this work. In Section 3, we introduce xMIL as a general form of MIL, and xMIL-LRP as a solution for it. In Section 4, we experimentally show the improved explanation quality of our approach. We demonstrate how to extract insights from example heatmaps in Section 5. Our contributions are summarized as follows:

* **Methodical**: Despite attempts to apply XAI to MIL models in histopathology (e.g. [6; 27; 28; 29; 33; 34; 37; 38; 39; 40]), there exists no formalism guiding the interpretation of the heatmaps and defining their desired properties. xMIL is a novel framework addressing this gap. Within xMIL, heatmaps estimate the instances' impact on the bag label, which makes their interpretation straightforward and insightful.
* **Empirical**: Our extensive empirical evaluation of XAI methods for MIL on synthetic and real-world histopathology datasets is the first of its kind. It reveals that the widely used MIL explanation methods regularly yield misleading results. In contrast, xMIL-LRP sets a new state-of-the-art for explainability in AttnMIL and TransMIL models in histopathology.
* **Insight generation**: Previous studies [33; 34] conducted qualitative assessments of heatmaps on easy-to-learn datasets like CAMELYON or TCGA NSCLC. The insights gained in these settings are limited to model debugging, i.e., "Does the model focus on the disease area?" To our knowledge, we are the first to present a method generating heatmaps that enable pathologists to extract fine-grained insights about the model in a difficult biomarker prediction task.

## 2 Background

### Multiple instance learning (MIL)

**MIL formulations**. In MIL, a sample is represented by a bag of instances \(X=\{_{1},,_{K}\}\) with a bag label \(y\), where \(_{k}^{D}\) is the \(k\)-th instance. The number of instances per bag \(K\) may vary across samples. In its standard formulation [1; 2; 3], the instances of a bag exhibit neither dependency nor ordering among each other. It is further assumed that binary instance labels \(y_{k}\{0,1\}\) exist but are not necessarily known. The binary bag label is \(1\) if and only if at least one instance label is \(1\), i.e., \(y=_{k}\{y_{k}\}\). Various extensions have been proposed [41; 42], each making different assumptions about the relationships between instances and bag labels.

**MIL models**. MIL architectures typically consist of three components as illustrated in Figure 2: a backbone extracting instance representations, an aggregation function fusing the instance representations into a bag representation, and a prediction head inferring the final bag prediction. As recent foundation models for histopathology have become powerful feature extractors suitable for a wide range of tasks [29; 43; 44; 45; 46], the weights of the backbone are often frozen, allowing for a more efficient training. For aggregation, earlier works used parameter-free mean or max pooling approaches [47; 48; 49]. Recently, attention mechanisms could improve performance, flexibly extracting relevant instance-level information using non-linear weighting [3; 6; 50] and self-attention [4; 51]. Attention MIL (AttnMIL)  computes a weighted average of the instances' feature vectors via a single attention head. TransMIL  uses a custom two-layer Transformer architecture, viewing instance representations as tokens. The bag representation is extracted from the class token at the final layer. TransMIL allows for computing arbitrary pairwise interactions between all instances relevant to the prediction task. While various extensions of AttnMIL and TransMIL have been proposed (e.g., [5; 6; 7; 8; 9; 52; 53; 10; 11; 12]), these two methods are arguably prototypical and among the most commonly used in the digital histopathology community.

**MIL explanation methods**. From the few studies investigating MIL interpretability, most of them use attention heatmaps [3; 4; 6; 27; 28; 29]. Moreover, basic gradient- and propagation-based methods have been explored for specific architectures and applications [54; 55]. Sadafi et al.  applied LRP to generate pixel-level attributions for single-cell images in a blood cancer diagnosis task, but did not consider its potential for instance-level explanations. Perturbation-based methods, building on model-agnostic approaches like SHAP , perturb bag instances and compute importance scoresfrom the resulting change in the model prediction; Early et al.  proposed passing bags of single instances through the model ("single"), dropping single instances from bags ("one-removed"), and sampling coalitions of instances to be removed ("MILLI"). Javed et al.  introduced "additive MIL", providing directly interpretable instance scores while constraining the model's ability to capture instance interactions.

### Limitations of MIL in histopathology

Histopathological datasets and prediction tasks are diverse and come with various inherent challenges. We highlight the following three features.

* **Instance ambiguity**. Instances are small high-resolution patches from large images. Their individual information content may be limited, as they can be subject to noise or only be interpretable as part of a larger structure. For example, it is not always possible to distinguish a benign high-grade adenoma from a malignant adenocarcinoma on a patch level due to their similar morphology.
* **Positive, negative, and class-wise evidence**. A single bag may contain evidence for multiple classes that a MIL model needs to weigh for correct decision-making. In survival prediction, for example, a strong immune response may support longer survival, while an aggressive tumor pattern speaks for shorter survival.
* **Instance interactions**. In many prediction tasks, it may be necessary to consider interactions between instances. A gene mutation may generate morphological alterations in the tumor area, the tumor microenvironment, and the healthy tissue, all of which may need to be considered together to reliably predict the biomarker.

Existing MIL formulations make explicit assumptions about the relationship between instances and bag labels , limiting their ability to capture the full complexity of a histopathological prediction task. The standard MIL formulation, in particular, does not consider any of the aforementioned aspects, rendering it an unsuitable framework for most histopathological settings.

Similarly, previous MIL explanation methods suffer from various shortcomings that limit their applicability in real-world histopathology datasets. The direct interpretability of attention scores is insufficient to faithfully reflect the model predictions [30; 31; 32]. Moreover, they cannot distinguish between positive, negative, or class-wise evidence . Purely gradient-based explanations may suffer from shattered gradients, resulting in unreliable explanations . Perturbation-based approaches come with high computational complexity. While the linear "single" and "one removed" methods require \(K\) forward passes per bag, MILLI scales quadratically with the number of instances . In histopathology, where bags typically contain more than 1,000 and frequently more than 10,000 instances, quadratic runtime is practically infeasible. Additive MIL and linear perturbation-based methods do not consider higher-order instance interactions. In prediction tasks depending on interactions, linear perturbation-based explanations may fail to provide faithful explanations, while additive models may not achieve competitive performances.

## 3 Methods

**Notation**. We denote vectors with boldface lowercase letters (e.g., \(\)), scalars with lowercase letters (e.g., \(x\)), and sets with uppercase letters (e.g., \(X\)).

### xMIL: An XAI-based framework for multiple instance learning

We address the limitations discussed in Section 2.2 and introduce a more general formulation of MIL: explainable multiple instance learning (**xMIL**). At its core, we propose moving away from the notion of instance labels towards context-aware _evidence scores_, which better reflect the intricacies of histopathology while laying the foundation for developing and evaluating MIL explanation methods.

**Definition 3.1** (Explainable multiple instance learning).: Let \(X=\{_{1},,_{K}\}\) be a bag of instances with a bag label \(y\).

(1) There exists an _aggregation function_\(\) that maps the bag to its label, i.e., \((X)=y\). We make no assumptions about the relationship among the instances or between the instances and the label \(y\).

(2) There exists an _evidence function_\(\) assigning an _evidence score_\((X,y,_{k})=_{k}\) to any instance \(_{k}\) in the bag, quantifying the impact the instance has on the bag label \(y\).

The aim of xMIL is to estimate (i) the aggregation function \(\) and (ii) the evidence function \(\).

**Definition 3.2** (Properties of the evidence function).: Let \(_{k},_{k^{}}\) be instances from a bag \(X\). We assume that \(\) has the following properties.

(1) _Context sensitivity_. The evidence score \(_{k}\) of instance \(_{k}\) may depend on other instances from \(X\).

(2) _Positive and negative evidence_. If \(_{k}>0\), the instance \(_{k}\) has a positive impact on the bag label \(y\). If \(_{k}<0\), then \(_{k}\) has a negative impact on \(y\). If \(_{k}=0\), then \(_{k}\) is irrelevant to \(y\).

(3) _Ordering_. If \(_{k}>_{k^{}} 0\), then instance \(_{k}\) has a higher positive impact on \(y\) than \(_{k^{}}\). If \(0_{k^{}}>_{k}\), then instance \(_{k}\) has a higher negative impact on \(y\) than \(_{k^{}}\).

Similar to our definition, previous works described context sensitivity and accounting for positive and negative evidence as desirable properties of MIL explanation methods . However, xMIL integrates these principles directly into the formalization of the MIL problem.

In contrast to previous MIL formulations, xMIL addresses the potential complexities within histopathological prediction tasks by refraining from posing strict assumptions on \(\). Via the evidence function \(\), we suggest that instances may vary in their ability to support or refute a class and that their influence may depend on the context within the bag. In practice, the evidence function is often unknown, as the notion of an "impact" on the bag label is hard to quantify. For the standard MIL setting, however, the binary instance labels fulfill the criteria of the evidence function. Therefore, xMIL is a more general and realistic formulation of multiple instance learning for histopathology.

We can learn the aggregation function \(\) via training a MIL model. To gain deeper insights into the prediction task by estimating the evidence function \(\), we design an explanation method for the learned aggregation function with characteristics suitable to the properties of the evidence function.

Figure 2: The two steps of xMIL: estimating the aggregation function (A) and the evidence function (B). Panel A shows a block diagram of a MIL model applied to a histopathology slide. The feature extraction module is typically a combination of a frozen foundation model followed by a shallow MLP. In most of the recent MIL models, the aggregation module uses attention mechanisms for combining the instance feature vectors into a single feature representation per bag. The prediction head is a linear layer or an MLP. Panel B schematically shows xMIL-LRP for explaining AttnMIL. In xMIL-LRP, the model output is backpropagated to the input instances. The colored lines represent the relevance flow. Red and blue colors encode the positive and negative values. The attention module is handled via the AH-rule as described in Section 3.2. As discussed in Section 3.3, the instance explanation scores can be computed at the output of the foundation model or at the input level.

### XML-LRP: Estimating the evidence function

We introduce xMIL-LRP as an efficient solution to xMIL, bringing layer-wise relevance propagation (LRP) to MIL. LRP is a well-established XAI method [35; 58] with a large body of literature supporting its performance in explaining various types of architectures in different tasks [32; 36; 59; 60; 61; 62]. Starting from the prediction score of a selected class, the LRP attribution of neuron \(i\) in layer \(l\) receives incoming messages from neurons \(j\) from subsequent layer \(l+1\), resulting in relevance scores \(r_{i}^{(l)}=_{j}}{_{i^{}}q_{ij^{}}} r_{j} ^{(l+1)}\), with \(q_{ij}\) being the contribution of neuron \(i\) of layer \(l\) to relevance \(r_{j}^{(l+1)}\)1. A variety of so-called "propagation rules" have been proposed  to specify the contribution \(q_{ij}\) in specific model layers. For the attention mechanism, as a core component of many MIL architectures, we employ the AH-rule introduced by Ali et al. . In a general attention mechanism, let \(_{k}=[z_{kd}]_{d}\) be the embedding vector of the \(k\)-th token and \(p_{kj}\) the attention score between tokens \(k\) and \(j\). The output vector of the attention module is \(_{j}=_{k}p_{kj}_{k}\). The AH-rule of LRP treats attention scores as a constant weighting matrix during the backpropagation pass of LRP. If \(R(y_{jd})\) is the relevance of the \(d\)-th dimension of \(_{j}=[y_{jd}]_{d}\), the AH-rule computes the relevance of the \(d\)-th feature of \(_{k}\) as:

\[R(z_{kd})=_{j}p_{kj}}{_{i}z_{id}p_{ij}}R(y_{jd}).\] (1)

This formulation can be directly applied to AttnMIL, and also adapted to a QKV attention block in a transformer, where \(_{k}\) is the embedding associated with the value representation.

We illustrate the effect of this rule in AttnMIL in Figure 2-B. The relevance flow separates the instances weighted by the attention mechanism into positive, negative, and neutral instances, resulting in more descriptive heatmaps that better show the relevant tissue regions compared to attention scores.

We further implement the LRP-\(\) rule for linear layers followed by ReLU activation function , as well as the LN-rule to address the break of conservation in layer norm , with details presented in Appendix A.2.

At the instance-level, xMIL-LRP assigns each instance \(_{k}=[x_{kd}]_{d}^{D}\) a relevance vector \(_{k}=[r_{kd}]_{d}\) with \(r_{kd}=R(x_{kd})=r_{kd}^{(0)}\) being the relevance score of the \(d\)-th feature of \(_{k}\). We define the instance-wise relevance score as an estimate for the evidence score of the instance as \(_{k}=_{d}r_{kd}\).

### Properties of xMIL-LRP and other explanation methods

The properties of xMIL-LRP are particularly suitable for estimating the evidence function:

**Context sensitivity**: xMIL-LRP disentangles instance interactions and contextual information as it jointly considers the relevance flow across the whole bag. LRP and Gradient \(\) Input (G\(\)I) are rooted in a deep Taylor decomposition of the model prediction  and consequently capture dependencies between features by tracing relevance flow through the components of the MIL model. While attention is context-aware, it is limited to considering dependencies of features at a specific layer. The "single" method is unaware of context. "One-removed" and additive MIL can only capture the impact of individual instances on the prediction.

**Positive and negative evidence**: xMIL-LRP relevance scores are real-valued and can identify whether an instance supports or refutes the model prediction. Features irrelevant to the prediction will receive an explanation score close to zero. Therefore, the range of explanation scores matches the range of the assumed evidence function. The same holds for additive MIL, MILLI, and "one-removed". Attention and "single" do not distinguish between positive and negative evidence.

**Conservation**: Following the conservation principle of LRP, xMIL-LRP provides an instance-wise decomposition of the model output, i.e., \(_{k}_{k}=_{k,d}r_{kd}=y\). This instance-level conservation also holds for additive MIL, but not for the other discussed methods. The local conservation principle of LRP  further allows us to analyze attribution scores at the instance feature vector level without requiring propagation through the foundation model--the instance-wise attribution scores are the same at any layer of the model.

Experiments and results

**Baseline methods**. We compared several explanation methods to our xMIL-LRP (see Appendix A.1 for details). For AttnMIL and TransMIL, we selected Gradient \(\) Input (**G\(\)I**) [64; 65] and Integrated Gradients (**IG**)  as gradient-based baselines. We further included the "single" perturbation method (**single**) , which involves using predictions for individual instances as explanation scores. Single is the only computationally feasible perturbation-based approach for the bag sizes considered here (up to 24,000). We evaluated raw attention scores for AttnMIL and attention rollout  for TransMIL (**attn**). In the random baseline (**rand**), instance scores were randomly sampled from a standard normal distribution. For additive attention MIL (AddMIL) , we assessed raw attention scores (**attn**) and the model-intrinsic instance-wise predictions (**logits**).

### Toy experiments

We designed novel toy experiments to assess and compare the characteristics of xMIL-LRP and the baseline methods for AttnMIL, TransMIL, and AddMIL in controlled settings. We focused on evaluating to what extent the explanations account for _context sensitivity_ and _positive and negative evidence_, i.e., the first two characteristics of the evidence function according to Definition 3.2, which we consider crucial aspects for explaining real-world histopathology prediction tasks.

Inspired by previous works [3; 34], we sampled bags of MNIST images , with each instance representing a number between **0** and **9**. We defined three MIL tasks for these bags:

* **4-Bags**: The bag label is class 1 if **8** is in the bag, class 2 if **9** is in the bag, class 3 if **8** and **9** are in the bag, and class 0 otherwise. The dataset was proposed by Early et al. . In this setting, the model needs to learn basic instance interactions.
* **Pos-Neg**: We define **4**, **6**, **8** as positive and **5**, **7**, **9** as negative numbers. The bag label is class 1 if the amount of unique positive numbers is strictly greater than that of unique negative numbers, and class 0 otherwise. The model needs to adequately weigh positive and negative evidence to make correct predictions.
* **Adjacent Pairs**: The bag label is class 1 if it contains any pair of consecutive numbers between **0** and **4**, i.e., (**0**,**1**), (**1**,**2**), (**2**,**3**) or (**3**,**4**), and class 0 otherwise. In this case, the impact of an instance is contextual, as it depends on the presence or absence of adjacent numbers.

To assess the explanation quality, we first defined valid _evidence scores_ as ground truths according to Definition 3.2. For each dataset, we require one evidence function per predicted class \(c\), denoted by \(^{(c)}(X,y,_{k})=_{k}^{(c)}\). We assigned \(_{k}^{(c)}=1\) if \(_{k}\) supports class \(c\), \(_{k}^{(c)}=-1\) if the instance refutes class \(c\), and \(_{k}^{(c)}=0\) if it is irrelevant. We aimed to measure whether an explanation method correctly distinguishes instances with positive, neutral, and negative evidence scores. Therefore, we computed a two-class averaged area under the precision-recall curve (AUPRC-2), measuring if the positive instances received the highest and the negative instances the lowest explanation scores. We assessed AttnMIL and TransMIL models and repeated each experiment 30 times. The details of the ground truth, the evaluation metric, and the experimental setup are provided in Appendix A.3.

Table 1 displays the test AUROC scores of the three models across datasets, demonstrating that the models solve the tasks to varying degrees, alongside the performances of the explanation methods. We find that xMIL-LRP outperformed the other explanation approaches across MIL models and datasets in all but one setting. It reached particularly high AUPRC-2 scores in the 4-Bags and Pos-Neg datasets while being most robust in the more difficult Adjacent Pairs setting. Attention severely suffered from the presence of positive and negative evidence, which it cannot distinguish by design. While IG performed comparably to xMIL-LRP for AttnMIL models, it was inferior for TransMIL. Notably, the test AUROC of AddMIL was worse in all settings, resulting in explanations that are not competitive with the post-hoc explanation methods on AttnMIL and TransMIL. This supports our point that AddMIL may not perform competitively in difficult prediction tasks. The single perturbation method provided good explanations in the Pos-Neg setting, where numbers have a fixed evidence score irrespective of the other instances in the bag. However, in 4-Bags and Adjacent Pairs, the method's performance decreased, as it always assigns the same score to the same instance regardless of the bag context. In contrast, xMIL-LRP is both context-sensitive and identifies positive and negative instances. Since we expect that these aspects are common features of many

real-world histopathological datasets, we conclude that our method is the only suitable approach for such complex settings.

### Histopathology experiments

**Datasets and model training.** To evaluate the performance of explanations on real-world histopathology prediction tasks, we considered four diverse datasets of increasing task difficulty covering tumor detection, disease subtyping, and biomarker prediction. These datasets had previously been used for benchmarking in multiple studies [12; 33; 46; 69].

* CAMELYON16  consists of 400 sentinel lymph node slides, of which 160 carry to-be-recognized metastatic lesions of different sizes. It is a well-established tumor detection dataset.
* The TCGA NSCLC dataset (abbreviated as NSCLC) contains 529 slides with lung adenocarcinoma (LUAD) and 512 with lung squamous cell carcinoma (LUSC). The prediction task is to distinguish these two non-small cell lung cancer (NSCLC) subtypes.
* The TCGA HNSC HPV dataset  (abbreviated as HNSC HPV) has 433 slides of head and neck squamous cell carcinoma (HNSC). 43 of them were affected by a human papillomavirus (HPV) infection diagnosed via additional testing . HPV infection is an essential biomarker guiding prognosis and treatment . The task is to identify the HPV status directly from the slides. Label imbalances and the complexity of the predictive signature are key challenges in this task.
* The TCGA LUAD TP53 (abbreviated as LUAD TP53) dataset contains 529 lung adenocarcinoma (LUAD) slides, 263 of which exhibit a mutation of the TP53 gene, which is one of the most common mutations across cancers. In lung cancer, it is associated with poorer prognosis and resistance to chemotherapy and radiation . Previous works showed that TP53 mutation can be predicted from LUAD slides [69; 73].

We generated patches at 20x magnification and obtained 10,454 \(\) 6,236 patches per slide across all datasets (mean \(\) std.). Features were extracted using the pre-trained CTransPath  foundation model and aggregated using AttnMIL or TransMIL.2 Additional details regarding the datasets and training procedure are described in Appendix A.4.

We report the mean and standard deviation of the test set AUROC over 5 repetitions in Table 2. In all but one case, TransMIL outperformed AttnMIL, with the largest margin observed in the difficult TP53 dataset. Our results generally align with performances reported in previous works [12; 46; 69].

**Faithfulness evaluation.** As the evidence functions \(\) of our histopathology datasets are unknown, we resorted to assessing _faithfulness_, i.e., how accurately explanation scores reflect the model prediction [74; 75]. The primary goal of the faithfulness experiments is to evaluate the ordering of relevance scores (Property 3 of the evidence function in Definition 3.2). Faithfulness can be quantified by progressively excluding instances from the most relevant first (MORF) to the least relevant last and measuring the change in prediction score. The area under the resulting perturbation curve (AUPC)

    &  &  &  \\   & AttnMIL & TransMIL & AddMIL & AttnMIL & TransMIL & AddMIL & AttnMIL & TransMIL & AddMIL \\ Test AUROC & \(1.00 0.00\) & \(1.00 0.00\) & \(0.98 0.02\) & \(0.97 0.00\) & \(0.98 0.00\) & \(0.89 0.03\) & \(0.88 0.08\) & \(0.92 0.06\) & \(0.77 0.07\) \\  Rand & \(0.31 0.00\) & \(0.31 0.00\) & – & \(0.42 0.00\) & \(0.42 0.00\) & – & \(0.54 0.00\) & \(0.54 0.00\) & – \\ Attn & \(0.53 0.00\) & \(0.52 0.00\) & \(0.54 0.01\) & \(0.45 0.00\) & \(0.46 0.03\) & \(0.48 0.02\) & \(0.61 0.01\) & \(0.60 0.01\) & \(0.63 0.04\) \\ Single & \(0.87 0.02\) & \(0.85 0.07\) & – & \(0.89 0.00\) & \(0.91 0.02\) & – & \(0.73 0.06\) & \(0.77 0.06\) & \\ Logits & – & – & \(0.79 0.11\) & – & – & \(0.68 0.17\) & – & – & \(0.71 0.09\) \\ G\(\)1 & \(0.72 0.08\) & \(0.40 0.07\) & – & \(0.72 0.14\) & \(0.44 0.06\) & – & \(0.63 0.05\) & \(0.57 0.05\) & – \\ IG & \(0.88 0.01\) & \(0.80 0.09\) & – & \(0.93 0.00\) & \(0.82 0.08\) & – & \(0.75 0.03\) & \(0.72 0.07\) & – \\ xMIL-LRP & \(\) & \(\) & – & \(0.91 0.01\) & \(\) & – & \(\) & \(\) & – \\   

Table 1: Results of the toy experiments. We report AUROC-2 scores of MIL explanation methods on three toy datasets measuring how well a method identified instances with positive and negative evidence scores (mean \(\) std. over 30 repetitions). The highest mean scores are bold and the second highest are underlined. We also display the model performances (“Test AUROC”, mean \(\) std.).

indicates how faithfully the identified ordering of the instances affects the model prediction. The lower the AUPC score, the more faithful the method. We calculated AUPC for correctly classified slides. Further methodological details are provided in Appendix A.5.

In Figure 3, we show the perturbation curves and AUPC boxplots for the patch-dropping experiment for TransMIL in our four datasets (Figure 4 shows the results for AttnMIL). Additionally, we summarize our results in Table 2. To test the difference in the AUPC values among the baseline explanation methods, we performed paired t-tests between the random baseline vs. all methods and xMIL-LRP vs. all other baselines. The p-values were corrected using the Bonferroni method for multiple comparison correction. All tests resulted in significant differences except for random baseline vs. G\(\)1 for CAMELYON16 and attention for HNSC HPV.

xMIL-LRP significantly achieved the lowest average AUPC compared to the baselines, providing the most faithful explanations across all tasks and model architectures. Especially evident with the TransMIL model, xMIL-LRP accurately decomposed the mixing of patch information via self-attention. Notably, the largest margin of xMIL-LRP to other methods could be observed in the more challenging biomarker prediction tasks of the HNSC HPV and LUAD TP53 datasets.

The results also reflect whether the explanation scores contain meaningful positive/negative evidence for the target class (Property 2 of the evidence function in Definition 3.2): if so, we expect the model's prediction to flip when all patches supporting the target class are excluded. In Figure 3, the model decision always flips when patches are excluded based on xMIL-LRP scores, whereas other methods show inconsistent results.

Attention scores, as the most widely used explanation approach for MIL in histopathology, did not provide faithful explanations outside the simple tumor detection setting in the CAMELYON16 dataset. This remarkably highlights their limited usefulness as model explanations and confirms previously reported results in other domains [30; 31; 32]. Passing single instances through the model ("single") achieved good faithfulness scores for simpler tasks and AttnMIL, but performed worse for Transformer-based biomarker prediction.

## 5 Extracting insights from xMIL-LRP heatmaps

The identification of predictive features for HPV infection in head and neck carcinoma from histopathological slides is a challenging task for pathologists. In this task, there are partially known morphological patterns associated with the class label. We provide a brief overview of the known histological features differentiating HPV-negative and HPV-positive HNSC in Appendix A.7 and Figure 5. In the following, we demonstrate how faithful xMIL-LRP explanations can support pathologists in gaining insights about the model strategy and inferring task-relevant features.

We extracted explanation scores for the best-performing TransMIL models. To increase the readability of resulting heatmaps, we clipped the scores per slide at the whiskers of their boxplots, which extended 1.5 times the interquartile range from the first and third quartiles. We then translated them into a zero-centered red-blue color map, with red indicating positive and blue negative scores. Notice that the explanation methods operate on different scales. For xMIL-LRP, a positive relevance score indicates support for the explained label, while a negative score contradicts it.

    &  &  \\   & CAMELYON16 & NSCLC & HNSC HPV & LUAD TP53 & CAMELYON16 & NSCLC & HNSC HPV & LUAD TP53 \\ Test AUROC & \(0.93 0.00\) & \(0.95 0.00\) & \(0.88 0.06\) & \(0.71 0.01\) & \(0.95 0.01\) & \(0.96 0.00\) & \(0.88 0.05\) & \(0.75 0.01\) \\  Rand & \(0.94 0.13\) & \(0.98 0.04\) & \(0.97 0.07\) & \(0.84 0.14\) & \(0.95 0.11\) & \(0.98 0.08\) & \(1.00 0.01\) & \(0.94 0.17\) \\ Attn & \(0.65 0.46\) & \(0.70 0.27\) & \(0.94 0.18\) & \(0.65 0.14\) & \(0.63 0.45\) & \(0.91 0.22\) & \(0.95 0.15\) & \(0.64 0.38\) \\ Single & \(0.61 0.43\) & \(0.42 0.26\) & \(0.73 0.23\) & \(0.34 0.16\) & \(0.42 0.35\) & \(0.53 0.26\) & \(0.92 0.13\) & \(0.73 0.33\) \\ G\(\)1 & \(0.92 0.19\) & \(0.81 0.35\) & \(0.81 0.25\) & \(0.44 0.23\) & \(0.82 0.36\) & \(0.79 0.30\) & \(0.87 0.20\) & \(0.66 0.40\) \\ IG & \(0.62 0.44\) & \(0.75 0.38\) & \(0.78 0.25\) & \(0.38 0.20\) & \(0.88 0.23\) & \(0.99 0.01\) & \(1.00 0.00\) & \(0.99 0.01\) \\ xMIL-LRP & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Results of the faithfulness experiments. AUPC values per dataset, MIL model, and explanation method (mean \(\) std. over all slides). _Lower scores indicate higher faithfulness_. The best performance per setting (significant minimum based on the paired t-tests) is highlighted in bold. We also display the model performances (“Test AUROC”, mean \(\) std. over 5 repetitions).

We revisit the example of the HNSC tumor with a false-positive prediction of an HPV infection in Figure 1. As previously noted, only xMIL-LRP indicates that the model recognizes evidence of HPV infection in the tumor border, but not the remaining tumor. Despite a prediction score close to 0, all relevance scores from the single method were between 0.95-0.97, suggesting that context-free single-instance bags may not be informative in this task. We observed this phenomenon across various slides.

Heatmaps of additional examples are provided in Appendix A.7. In Figure 6, xMIL-LRP accurately delineates and distinguishes HPV-positive tumor islands from the surrounding stroma. In this simple case, attention also provides a reasonable explanation. Figure 7 presents another correctly classified HPV-positive sample. Here, xMIL-LRP outlines spatially consistent slide regions with clear positive evidence, distinct from regions of negative or mixed evidence (top row). Most notably, the subepithelial mucous glands (bottom row), which are not associated with HPV, are correctly highlighted in blue, unlike in the attention map. In Figure 8, we display a false positive slide. In this case, xMIL-LRP allowed us to identify that the evidence of HPV-positivity can be attributed to an unusual morphology of an HPV-negative tumor that shares some morphological features usually associated with HPV infection (e.g., smaller tumor cells with hyperchromatic nuclei, dense lymphocyte infiltrates).

## 6 Conclusion

We introduced xMIL, a more general and realistic MIL framework for histopathology, formalizing requirements for MIL explanations via the evidence function. We adapted LRP to MIL as xMIL-LRP, experimentally demonstrated its advantages over previous explanation approaches, and showed how access to faithful explanations can enable pathologists to extract insights from a biomarker prediction model. Thus, xMIL is a step toward increasing the reliability of clinical ML systems and driving medical knowledge discovery, particularly in histopathology. Despite being motivated by the challenges in histopathology, our approach presented here can be directly transferred to other problem settings that require explaining complex MIL models, e.g., in video, audio, or text domains. Furthermore, a detailed analysis of potentially complex dependencies between instances, especially in the context of multi-modal inputs, represents a promising direction for future research.

Figure 3: Patch dropping results for TransMIL. The first row depicts the perturbation curves, where the solid lines are the average perturbation curve and the shaded area is the standard error of the mean at each perturbation step. Each boxplot on the second row shows the distribution of AUPC values for all test set slides per explanation methods. In each boxplot, the red line marks the median and the red dot marks the mean. _Lower perturbation curves and AUPCs represent higher faithfulness._