# A Layer-Wise Natural Gradient Optimizer for

Training Deep Neural Networks

 Xiaolei Liu

Ant Group

Hangzhou, China

liuxiaolei.lxl@mybank.cn

&Shaoshuai Li

Ant Group

Hangzhou, China

lishaoshuai.lss@mybank.cn

&Kaixin Gao

Ocean University of China

Qingdao, China

gaokaixin06@163.com

Joint first author, these authors contributed equally to this work.

Binfeng Wang

Ant Group

Hangzhou, China

wangbinfeng.wbf@mybank.cn

###### Abstract

Second-order optimization algorithms, such as the Newton method and the natural gradient descent (NGD) method exhibit excellent convergence properties for training deep neural networks, but the high computational cost limits its practical application. In this paper, we focus on the NGD method and propose a novel layer-wise natural gradient descent (LNGD) method to further reduce computational costs and accelerate the training process. Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete backpropagation. Then, each block matrix is approximated as a Kronecker product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces equal before and after approximation. By these two steps, we provide a new approximation for the Fisher information matrix, which can effectively reduce the computational cost while preserving the main information of each block matrix. Moreover, we propose a new adaptive layer-wise learning rate to further accelerate training. Based on these new approaches, we propose the LNGD optimizer. The global convergence analysis of LNGD is established under some assumptions. Experiments on image classification and machine translation tasks show that our method is quite competitive compared to the state-of-the-art methods.

## 1 Introduction

With the rapid increase in the size of deep neural networks (DNNs) models in both areas of computer vision (CV) and natural language processing (NLP), there have been remarkable attentions given to optimizing algorithms. An effective optimizer can significantly improve the training speed of models while ensuring high prediction performance. First-order gradient descent methods are workhorses of training DNNs, which can be broadly divided into two categories: methods use a same learning rate, such as stochastic gradient descent (SGD)  and its accelerations [2; 3], and methods use adaptive learning rate, such as AdaDelta , RMSProp , ADAM  and Adabelief . Although first-order gradient descent methods enjoy low computational cost and ease of implementation, they might suffer from sensitivity to hyperparameters and slow convergence. It is challenging to reduce the number of iterations and computational time of these methods.

Some work has considered introducing curvature information when updating parameters of DNNs to improve the convergence speed and overcome the above shortcomings of the first-order methods. However, second-order optimization methods need to store and compute the inverse of curvature matrix, which brings expensive storage and computation costs and limits the application of second-order methods in training large-scale DNNs. Therefore, many approximate second-order methods have been proposed for training large-scale models. For example, Keskar and Berahas  proposed a stochastic quasi-Newton algorithm for training recurrent neural networks. Yao et al.  approximated the Hessian matrix as a diagonal operator, which is achieved by applying Hutchinson's method, and proposed the AdaHessian method. Goldfarb, Ren and Bahamou  developed Kronecker-factored block-diagonal BFGS and its limited-memory variants L-BFGS methods for training DNNs. Generalized Gauss-Newton methods, such as the Hessian-free method  and the Krylov subspace method , also have been proposed to approximate the Hessian matrix.

The natural gradient descent (NGD) method , which preconditions the gradient by the Fisher information matrix instead of the Hessian matrix, also has shown effectiveness in training DNNs . NGD explores the steepest direction of the objective function when the parameter space has a Riemannian metric structure and has a faster convergence speed. In particular, NGD can also be seen as an approximation of the Netwon method when the objective function and the manifold metric are compatible . However, it is still impossible to directly compute the inverse of the Fisher information matrix for DNNs with millions or even billions parameters. Quite a few approximate approaches have been proposed. Under some independency assumptions, Martens and Grosse  proposed the Kronecker-factored approximate curvature (KFAC) method, in which the Fisher information matrix is approximated as a block diagonal matrix and each block matrix is further approximated as the Kronecker product of two smaller matrices. Then, KFAC was extended to convolutional neural networks , recurrent neural networks  and variational Bayesian neural networks  and showed significant speedup during training. In addition, George et al.  proposed the eigenvalue-corrected Kronecker factorization (EKFAC) method. Gao et al.  proposed the trace-restricted Kronecker-factored approximate (TKFAC) method. These approaches all focus on the Kronecker-factored approximations of the Fisher information matrix. What's more, some works have also considered large-scale distributed computing using NGD for training DNNs and shows excellent experimental performance .

In this paper, our main focus is on the NGD method. Motivated by the effectiveness of diagonal approximations and the significance of diagonal elements in the curvature matrix, we prioritize the diagonal information and integrate it into our approximation and introduce a novel method, namely Layer-wise Natural Gradient Descent (LNGD). Our contributions can be given as follows:

* Based on the block diagonal approximation of the Fisher information matrix, we propose a layer-wise sample method to more efficiently compute each block matrix corresponding to each layer. By assuming that the predictive distribution of the output after the activation function for each layer follows a Gaussian distribution, each block matrix can be directly computed using the inputs and the outputs separately, without having to perform a complete back-propagation.
* For each block matrix corresponding to each layer, we further approximate it as a Kronecker product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces equal before and after approximation. With this operation, we further reduce the cost of computing inverse matrices while still preserving the main information of each block matrix.
* In order to further accelerate the training, we propose an adaptive layer-wise learning rate by optimizing a quadratic model, in which parameters in the same layer share the same adaptive learning rate. Moreover, a faster approach of computing the adaptive layer-wise learning rate is also provided, making it speed up training while maintaining computationally efficient.
* Based on the novel approximation mentioned above of the Fisher information matrix and the adaptive layer-wise learning rate, we propose the LNGD optimizer for training DNNs. The global convergence analysis are also established under some assumptions.
* We perform experiments on image classification and machine translation tasks. Numerical results show that LNGD converges faster than SGD, ADAM and KFAC, and LNGD provides an significant improvement in computational time savings when achieves convergence.

The rest of this paper is organized as follows. Section 2 gives the notations and introduces the NGD method. In Section 3, we propose a novel approximation of the Fisher information matrix and the adaptive layer-wise learning rate. Furthermore, we give the framework of LNGD and establish the convergence analysis. Section 4 presents the results of experiments on image classification and machine translation tasks. The conclusion is drawn in Section 5.

## 2 Notations and Preliminaries

In this paper, for a matrix \(\), we use \(_{ij}\) to denote its \((i,j)\)th entry, \(()\) to denote its trace and \(\|\|_{}\) to denote its Frobenius norm. We use \(\) and \(\) to denote the Hadamard and Kronecker product of two matrices. In the following, we briefly introduce the NGD method for training DNNs. During the training process of neural networks, the purpose is to find the vector of parameters \(\) which minimizes the loss function \(h()\). If the loss function \(h()\) is chosen as the the cross-entropy loss function, \(h()\) can be given as \(h()=[- p(|,)]\), where \(p(|,)\) is the density function of a predictive distribution \(P_{|}()\), and \(,\) are the training inputs and labels, respectively. Next, we give the definition of natural gradient, which gives the steepest direction of the objective function when the parameter space has a Riemannian metric structure. The natural gradient is defined as \(^{-1}_{}()\), where \(\) is the Fisher information matrix given by

\[=}_{ q(), p (|,)}[_{} p(| ,)_{} p(|,)^{}].\] (1)

In Eq. (1), the input \(\) is independently sampled from a distribution \(Q_{}\) with density function being \(q()\) and the label \(\) is sampled from the predictive distribution \(P_{|}()\). In the following pages, we abbreviate \(_{ q(), p(|,)}\) as \(\) unless otherwise specified. Consider a neural network with \(L\) layers, for each layer \(l[L]\) with \([L]=\{1,2,,L\}\), we denote \(_{l-1}\) and \(_{l}\) as the input (the activation from the previous layer) and the matrix of weights of this layer, respectively. What's more, \(_{l}=(_{l})\) and \(=(_{1},,_{L})^{}=(( _{1})^{},,(_{L})^{})^{}\), where \(()\) indicates vectorization of a matrix. For convenience, we denote the derivative of the loss function with respect to \(\) as \(=-_{} p(|, {})\). Then the Fisher information matrix can be expressed as \(=[^{}]\).

Due to the high computational and storage costs caused by the inverse operation of high-dimensional matrices, it is impractical to directly compute \(^{-1}\) in the training of DNNs. The family of Kronecker-factored approximations provides an effective approach for computing \(^{-1}\) of parameters in high-dimensional space, which is usually achieved by two steps. In the first step, by assuming that the parameters between different layers are independent, these methods approximate the entire Fisher information matrix as a block diagonal matrix, i.e.,

\[(_{1},_{2},,_{L}),\] (2)

where \(_{l}=[_{l}_{l }^{}]\) for any \(l[L]\). By this way, the Fisher information matrix can be approximated by \(L\) block matrices. This step transforms the inverse of the entire Fisher information matrix into the inverse of a series of small block matrices. In the second step, these methods further approximate each block matrix as the Kronecker product of some smaller factors. This approximation can transform the inverse of each block matrix into the inverse of some smaller factors combining the properties of the Kronecker product.

## 3 LNGD: A Layer-Wise Second-Order Optimizer

In this section, we first introduce the layer-wise sample approximation strategy. Then, we present the details of adaptive layer-wise learning rate mechanism and give the specific framework of LNGD. Finally, elaborate theoretical analysis of LNGD's convergence is also provided.

### Layer-Wise Sample Approximation

For NGD methods to train DNNs, the Fisher information matrix can be approximated by a block diagonal one according to different layers as given by Eq. (2), this approximation can be found in [14; 15; 19; 22] and references therein. We call such a block diagonal approximate Fisher information matrix the layer Fisher information matrix, which is computed based on a distribution \(Q_{}\) and a predictive distribution \(P_{|}()\) as given in Eq. (1). To obtain the layer Fisher information matrix, we need perform a complete back-propagation to sequentially compute \(_{L},_{L-1},,_{1}\), which still consumes much computing time.

In this subsection, we propose a layer-wise sample approximation of the Fisher information matrix, in which each block matrix \(_{l}\) is computed based on the \(l\)th layer's prediction distribution \(P_{_{l}|_{l-1}}(_{l})\) with the input \(_{l-1}^{d_{l}}\) of this layer and the input \(_{l}^{d_{l+1}}\) of the \((l+1)\)th layer instead of using the same predictive distribution \(P_{|}()\) for all layers. Specifically, for \(_{l}\), we assume that the predictive distribution \(P_{_{l}|_{l-1}}(_{l})\) follows Gaussian distribution, which is usual used as prior by variational auto-encoder , so \(_{l}\) can be computed by sampling from a normal distribution with expectation being \(_{l}\) and variance being \(\). Similar assumption can also be found in , in which the normality is also supported by a central limit theorem under the independence assumption. By this layer-wise sample approximation, we can compute the layer Fisher information matrix without having to perform a complete back-propagation and thus improve the computational efficiency.

Next, we can give the formula of each block \(_{l}\) in the layer Fisher information matrix as

\[_{l} =[}_{l} }_{l}^{}]=[(}_ {l})(}_{l})^{}]=[(_{l}_{l-1}^{})(_{l} _{l-1}^{})^{}]\] \[=[(_{l-1}_{l-1}^{})( _{l}_{l}^{})]^{m_{d}l_{l} m_{d}l_{l}},\]

where \(}_{l}=-_{} p(_{l}| _{l-1},_{l})\), \(_{l}=-_{_{l}} p(_{l}|_{l-1},_{l})^{m_{l}}\) with \(_{l}=_{l}_{l-1}\), and \(p(_{l}|_{l-1},_{l})\) is the density function of the distribution \(P_{_{l}|_{l-1}}(_{l})\).

In practice, the dimension of each block matrix \(_{l}\) is often still too large to directly compute its inverse matrix. Therefore, additional approximation methods are required to handle this computational difficulty. Suppose that the predictive distribution of \(_{l}\) follows Gaussian distribution with expectation being \(_{l}\) and variance being \(\), and each element of activation output \(_{l}\) is independent and identically distributed random number, then each element of partial derivative \(_{l}\) is also independent and identically distributed. It is easy to show that \(_{l}\) can be seen as a matrix with \(d_{l} d_{l}\) block matrices, in which each block is an \(m_{l} m_{l}\) matrix and the off-diagonal elements are zero. Therefore, \(_{l}\) can be approximated as

\[_{l}[(_{l-1}_{l-1}^{}) (_{l}_{l}^{})].\] (3)

Figure 1: Comparison of the exact Fisher information matrix \(\) and our approximation \(_{}\). We use LNGD to train MNIST on a fully-connected neural network, whose architecture is 196-20-20-20-20-10. We show the results of the Fisher information matrix of the first layer with 20 units in top, which is a \(400 400\) matrix. The bottom portion displays partially enlarged parts of the top marked with red square, which is a \(40 40\) matrix. Within both the top and bottom sections, on the left is the exact Fisher information matrix \(\), in the middle is our approximation \(_{}\), and on the right is the absolute error between them. The brightness levels correspond to the sizes of the absolute values.

Combining the property that \(()^{-1}=^{-1}^{-1}\) for any two invertible matrices \(\) and \(\), we can significantly reduces the computational complexity. Thus, some approaches have considered approximating the Fisher information matrix as the Kronecker product of two factors [14; 15; 19; 20; 22]. Inspired by these works, we also approximate \(_{l}\) as the Kronecker product of two factor matrices \(_{l}^{d_{l} d_{l}}\) and \(_{l}^{d_{l} d_{l}}\). To get factor matrices \(_{l}\) and \(_{l}\), we first replace \((_{l}_{l}^{})\) in Eq. (3) by its trace and obtain \(_{l}\). Then we compute \(_{l}\) while keeping that \((_{l})=(_{l}_{l})\). Specifically, \(_{l}\) is given by

\[_{l}=[(_{l-1}_{l-1}^{}) ((_{l}_{l}^{}))]=[( _{l-1}_{l-1}^{})_{l}^{} _{l}],\] (4)

On the other hand, \(_{l}\) can be computed by

\[_{l}=[(_{l-1}^{}_{l-1}) (_{l}_{l}^{})]}{[( _{l-1}^{}_{l-1})(_{l}^{}_{l})]}.\] (5)

Based on Eq. (4) and Eq. (5), we can show that \((_{l})=(_{l}_{l})\).

Fig. 1 presents the visualization results of the exact Fisher information matrix \(\), our approximation \(_{}\), and the absolute error between them. Brighter pixels indicate higher values. From the left column in the top row, we observe the elements in the principal diagonal exhibit quite higher values, indicating their significance with rich information. Similarly, \(_{}\) can also emphasize the importance of the diagonal elements. The error figure reveals that the errors of the diagonal elements are small, which indicates that \(_{}\) provides a good approximation effect for the diagonal elements. Furthermore, to achieve a clearer visualization, we show the results of the partially enlarged area marked with red square in the bottom row. Here, we can observe more clearly that \(_{}\) achieves a favorable approximation effect on the diagonal elements. What's more, \(_{}\) can also provide an effective approximation of the elements in the auxiliary diagonals. These visualizations demonstrate the effectiveness of our proposed approximation in capturing the main elements of the Fisher information matrix. Therefore, our proposed approximation \(_{}\) is efficient and \(_{}\) can retain most of information.

### Adaptive Layer-Wise Learning Rate

In this subsection, we propose an adaptive layer-wise learning rate to accelerate training DNNs. We first consider the cases that use the same learning rate for all elements and the adaptive element-wise learning rate. Then we present the adaptive layer-wise learning rate scheme.

Suppose that \(^{k}\) is the update direction of the function \(h:^{n}\) at the iteration point \(^{k}\). We first recall the gradient descent methods for getting the minimization of \(h\), in which the update rule can be given as \(^{k+1}=^{k}-^{k}^{k}\), where \(^{k}\) is the learning rate, which can be chosen according to the value of the quadratic model

\[h(^{k}-^{k}^{k}) h(^{k})-^{k}^{k},_{}h( ^{k})+)^{2}}{2}(^{k})^{} _{}^{2}h(^{k})^{k}.\]

Once the update direction is chosen, the minimizer of \(^{k}\) can be given by

\[^{k}=^{k},_{}h(^{k}) }{(^{k})^{}_{}^{2}h(^{k}) ^{k}}\] (6)

if \((^{k})^{}_{}^{2}h(^{k})^{k}\) is nonzero. If \(_{}^{2}h(^{k})\) is positive definite and \(^{k}=(_{}^{2}h(^{k}))^{-1}_{}h(^{k})\), then \(^{k}=1\), which leads to the classical Newton method. In gradient decent methods, the learning rate is often regarded as the most important hyperparameter that highly influences model training. A fixed learning rate may lead to slow convergence or suboptimal performance in some cases. Therefore, many works have considered using adaptive learning rate in gradient decent methods [5; 6; 28]. In the following, we consider giving an adaptive element-wise learning rate automatically scaled by the direction \(^{k}\). In this case, the update rule of parameters is given by \(^{k+1}=^{k}-^{k}^{k}= ^{k}-^{k}^{k}\), where \(^{k}^{n}\) is the learning rate, \(^{k}^{n n}\) is a diagonal matrix with \((^{k})_{ii}=(^{k})_{i}\) and \((^{k})_{ij}=0\) when \(i j\) for \(i,j[n]\) and "\(\)" denotes the element-wise product. The second Taylor expansion of \(h(-)\) at iteration \(k\) is

\[h(^{k}-^{k}^{k}) h(^{k})-^{k}^{k},_{}h(^{k})+(^{k}^{k})^{ }_{}^{2}h(^{k})^{k}^{k}.\]Taking the derivative of \(h\) with respect to \(^{k}\) and letting it equal to \(\), we get

\[2^{k}_{}^{2}h(^{k})^{k}^{k}-^{k}_{}h(^{k})=,\]

which yields that

\[^{k}=(_{}^{2}h(^{k})^{k})^{-1 }_{}h(^{k})\] (7)

if \(^{k}\) and \(_{}^{2}h(^{k})\) are positive definite.

Note that in Eq. (7), it is impractical to compute the inverse of \(_{}^{2}h(^{k})^{k}\) directly for large-scale models due to high computational and storage costs. For second-order optimization methods in deep learning, some methods have considered approximating the curvature matrix by a block diagonal one according to different layers . What's more, some works have observed that parameters in the same layer have gradients of similar magnitudes. Therefore, a common learning rate can be efficiently shared by these parameters . Inspired by these works, we propose a novel adaptive layer-wise learning method as follows. Suppose that \(^{k}=((_{1}^{k})^{},(_{2}^{k})^{},,(_{L}^{k})^{})^{}\) is the update direction of a \(L\) layers neural network at the iteration point \(^{k}=((_{1}^{k})^{},(_{2}^{k})^{}, ,(_{L}^{k})^{})^{}\), the update rule of \(^{k}\) is given as \(^{k+1}=^{k}-}^{k}}^ {k}\), where

\[}^{k}=(_{1}^{k},_{2}^{k}, ,_{L}^{k})\] (8)

is a block diagonal matrix and \(}^{k}^{L}\) is the learning rate. The approximate second Taylor expansion of \(h(-}})\) at iteration \(k\) is

\[h(^{k}-}^{k}}^{k}) h(^{k})-}^{k}}^{k}, _{}h(^{k})+(} ^{k}}^{k})^{}^{k}}^{k} {}^{k},\] (9)

where \(^{k}=(_{1}^{k},_{2}^{k},, _{L}^{k})\) and \(_{l}^{k}=_{}_{l}}^{2}h(^{k})\) for \(l[L]\) and the Hessian matrix is approximated by the block diagonal matrix \(^{k}\). Taking the derivative of \(h\) with respect to \(}^{k}\) and letting it equal to \(\), we get \((}^{k})^{}_{}h(^{k})=( }^{k})^{}^{}}^{k}}^{k}\), which yields that

\[^{k}=(_{1},_{2},,_{L})^{-1}(( _{1}^{k})^{}_{_{l}}h(^{k}),(_{2}^{k})^{}_{_{2}}h(^{k}),(_{L}^{k})^{ }_{_{L}}h(^{k}))^{}\] (10)

if \(_{l}\) is nonzero, where \(_{l}=(_{1}^{k})^{}_{1}^{k}(_{1}^{k})\) for \(l[L]\).

If a same learning rate is used for all layers, as the same way of computing the adaptive layer-wise learning rate, we can get

\[=^{k})^{}_{}h(^{k})}{( ^{k})^{}^{k}^{k}}.\] (11)

**Theorem 1**.: _Let \(g()\) and \(g_{L}()\) be the approximate second Taylor expansions of \(h(-)\) and \(h(-}})\) as given in (9), where \(}^{n L}\), \(}^{L}\) and \(\) are given by (8), (10) and (11) respectively, then we have \(g_{L}() g()\)._

Proof.: The proof is given in the appendix. 

By Theorem 1, we know that the adaptive layer-wise learning rate may lead to a faster decline in terms of function values. In our proposed algorithm, we choose \(^{k}=(^{k})^{-1}_{}h(^{k})\), where \(^{k}\) is the Fisher information matrix and can be seen as a approximation of the Hessian matrix. Then, the Fisher information matrix is approximated by a block diagonal matrix each block matrix is approximated by the Kronecker product of two factor matrices. In each layer, the update direction \(_{l}^{k}\) is scaled by a layer-wise damping learning rate \(_{l}^{k}\) according to (10), which is given by

\[_{l}^{k}=_{l}^{k})^{}_{}h(_{l}^{k})}{(_{l}^{k})^{}_{l}^{k}_{l}^{k }+},\] (12)

where \(>0\) is a parameter. Using this adaptive layer-wise learning rate can accelerate layers with smaller gradients. Moreover, this approach can also avoid computing the inverse matrix in element-wise learning rate (13) and remain computationally efficient.

### Algorithm Schema

To effectively apply LNGD in training DNNs, several certain techniques need to be employed. In this section, we primarily focus on introducing the damping technique, which is a commonly used in second-order methods. Meanwhile, a simple method can be used to compute the adaptive layer-wise learning rate according to Eq. (12) since the cost of computing \((_{l}^{k})^{}_{l}^{k}_{l}^{k}\) is relatively expensive. Finally, we discuss the utilization of exponential moving averages to enhance the training process.

**A new damping technique**: Damping plays a crucial role in second-order optimization methods. Large damping can weaken the effect of curvature matrix, while small damping may cause computational difficulty and inaccuracy since most eigenvalues of the Fisher information matrix are close to zero and only a small number of eigenvalues take on large values. To make training stable, we propose the following damping for the \(l\)th layer: \(_{l}=(((_{l})/d_{l},_{1}),_{2})\), where \(_{1}\) and \(_{2}\) are two constants to constrain the minimum and maximum of damping, and \(d_{l}\) is the number of weight parameters. In our method, \(_{l}\) is approximated as the Kronecker product of two factors \(_{l}\) and \(_{l}\), so we add the damping to each factors by \(}_{l}=_{l}+_{l}^{}\) and \(}_{l}=_{l}+_{l}^{}\), where \(_{l}^{}=(((_{l})/n,_{1}),_{2})\) and \(_{l}^{}=(((_{l})/n,_{1}),_{2})\).

**Compute the learning rate the faster**: In order to compute the adaptive layer-wise learning rate given in Eq. (12) more quickly, we turn matrix computation into vector computation. Specifically,

\[(_{l}^{k})^{}_{l}^{k} _{l}^{k}&=(_{l}^{k})^{}}_{(x,y p(x,y))}[_{l}_{l}^{}]_{l}^{k}=}_{(x,y p(x,y))}[ (_{l}^{k})^{}_{l}_{l}^{}_{l}^{k}]\\ &=}_{(x,y p(x,y))}[((_{l}^{k})^{ }_{l})^{2}][((_{l}^{ k})^{}_{l})^{2}],\] (13)

where \(_{l}^{k}\) is the empirical Fisher information matrix and \(N\) is the number of samples. The empirical version of Fisher information matrix with no need for sampling from the model's prediction distribution, making it more computationally efficient.

**Exponential moving averages**: In line with previous studies, we incorporate exponential moving averages into our approach. This involves updating the estimate by combining the previous estimate, weighted by \(\), with the estimate calculated from the new mini-batch, weighted by \(1-\). That is

\[}_{l}^{k+1}}_{l}^{k+1}+( 1-)}^{k}}_{l}^{k+1}}_{l}^{k+1 }+(1-)}_{l}^{k}.\] (14)

In summary, our proposed algorithm is shown in Algorithm1.

### Convergence Analysis

In this subsection, we give the convergence analysis of LNGD. Following the model used in previous works about analysing the gradient descent [31; 32; 33] and NGD [34; 35], we consider a two-layer neural network activated by the ReLU function with \(m\) neurons in the hidden layer as follows:

\[f(,a,)=}_{r=1}^{m}a_{r}( _{r}^{}),\]

where \(_{1},_{2},,_{m}^{d}\) are the weight vectors of the first layer, \(^{d}\) is the input, \(a_{r}\) is the weight of unit \(r\) in the second layer and \(()\) is the ReLU activation function, i.e., \((x)=\{0,x\}\). Let \(=[f(,a,_{i}),f(,a,_{2}),,f(,a,_{n})]^{}\). In the following, we only give the result of convergence of Algorithm 1, the specific proof, which uses some conclusions in [36; 37; 38; 39], is given in the appendix.

**Theorem 2**.: _(Convergence rate of LNGD) Under the Assumption 1 and the assumption that \(()=d\). If we set the number of hidden units \(m=(}_{,}^{2} _{}}{^{2}^{3}_{}^{ 4}})\), we i.i.d initialize \(_{r}(0,)\), \(a_{r}[\{-1,+1\}]\) for any \(r[m]\), and we set the step size \(}\). Then with probability at least \(1-\) over the random initialization, we have for \(k=0,1,2,\)_

\[\|-^{k}\|_{2}^{2}(1-)^{k}\| -^{0}\|_{2}^{2}.\]

Proof.: The proof is given in the appendix.

```
0: learning rate \(\), learning rate parameter \(\), damping parameter \(\), damping constraints \(_{1}\),\(_{2}\), momentum parameter \(\), exponential moving average parameter \(\), Fisher information matrix and its inverse update intervals \(_{}\) and \(_{}\).
1:\(k 0\), \(\). Initialize \(}_{l}\) and \(}_{l}\) for any \(l[L]\).
2:while convergence is not reached do
3: Select a new mini-batch
4:for all\(l[L]\)do
5:if\(k 0\) (mod \(_{}\)) then
6: Update the factors \(}_{l}\) and \(}_{l}\) using Eq. (14)
7:endif
8:if\(k 0\) (mod \(_{}\)) then
9: Compute the inverses of \(}_{l}\) and \(}_{l}\)
10:endif
11: Compute \(_{_{l}}h()\) using backpropagation
12: Compute the approximated natural gradient \((}_{l}^{-1}}_{l}^{-1})_{_ {l}}h()\)
13: Compute the adaptive learning rate \(_{l}\) using Eq. (12)
14:\(-_{l}(}_{l}^{-1}}_{l}^{-1})_{_{l}}h()\)
15:\(+\) (Update momentum)
16:\(_{l}_{l}+\) (Update parameters)
17:endfor
18:\(k k+1\)
19:endwhile
20:return\(\) ```

**Algorithm 1** LNGD

## 4 Experiments

In order to verify the effectiveness of the proposed optimizer, we apply the optimizer to both image classification and machine translation tasks. We first present the optimization performance of our optimizer by comparing with several baselines. Then, we pay attention to the contribution of different modules of our optimizer by conducting elaborate ablation analysis, which is given in the appendix. Unless otherwise stated, the batch size for all experiments in the following is set to 256. The initial learning rate hyperparameters for all optimizers are tuned using a grid search with values \(\{1e-4,3e-4,,1,3\}\). The damping parameter \(\) in KFAC are tuned using a grid search with values \(\{1e-6,1e-4,3e-4,1e-3,,1e-1,3e-1\}\). The minimum and maximum of damping parameters \(_{1}\) and \(_{2}\) in LNGD are set to \(1e-5\) and \(1e-2\). The moving average parameter and the momentum correlating with KFAC and LNGD are set to 0.95 and 0.9, respectively. Furthermore, a weight decay of 0.004 is applied in all optimizers. All experiments run on a single A100 GPU using TensorFlow. We average the results of 5 runs and the hyper-parameter settings for these optimizers are the best values randomly searched for many times.

### CIFAR-10 Training

We first report the optimizing performance on CIFAR-10 , which is a standard task used to benchmark optimization methods [6; 41; 42; 43; 44]. Following these previous works, the changes of testing accuracy and training loss versus time as well as epoch are reported in Fig. 2, and detailed statistics are shown in Table6. From Fig. 2, it can be observed that LNGD exhibits the most rapid decline in training loss during the initial epochs and seconds. This suggests that LNGD is effective in quickly reducing the training loss and reaching convergence. All optimization methods convergent at around 200 epochs. However, it is observed that second-order optimization methods, such as KFAC and LNGD, achieve a lower training loss compared to first-order optimization methods like SGD and Adam. In terms of testing accuracy, as depicted in Fig. 2 (b) and (d), LNGD achieves a top-1 accuracy of 91\(\%\) at the fastest rate. It only requires 36 epochs and 189.69 seconds to achieve this accuracy level. In comparison, as presented in Table6, SGD and ADAM require at least 100\(\%\) and 30\(\%\) more epochs and time, respectively, to achieve similar accuracy. Relative to KFAC, LNGD reduces the number of epochs and time by around 20\(\%\) and 21\(\%\), respectively. Furthermore, as shown in Table6, LNGD gets the highest final testing accuracy after convergence.

### ImageNet Training

We extend our examination of optimizer efficacy to a larger image classification dataset, ImageNet-1K . The changes of testing accuracy and training loss versus time and epoch are reported in Fig.3 and Table2. The results show that the LNGD optimizer is highly efficient in training large image datasets in terms of both speed and accuracy. LNGD, which requires only 36 epochs and 6.46 hours, is much faster in achieving the top-1 testing accuracy of 75.9\(\%\) than other baselines. This is a significant improvement over SGD, which takes 100\(\%\) more epochs and 75\(\%\) more time to reach the same accuracy level. As for Adam, it exhibits a rapid decrease in loss during training and reaches convergence at a fast rate. However, the best achieved testing accuracy is only 74.05\(\%\), indicating that when training large-scale image tasks, a trade-off between efficiency and effectiveness needs to be considered. Compared to KFAC, although LNGD is better for only 3 epochs, it leads to 19\(\%\) reduction in terms of the computing time. The training loss results further support the efficiency of LNGD, as it maintains the fastest rate of decline during the initial stages of training and ultimately yields the lowest training loss upon convergence. Overall, the results suggest that LNGD is a highly efficient optimizer for large-scale image classification tasks, providing faster convergence and better accuracy than other commonly used optimizers.

### Transformer Training

In this experiment, we apply LNGD to the Transformer-Big model  with 213.7M parameters. The training datasets is WMT English-German machine translation corpus . We use Bleu  as the evaluation metrics, which is frequently used in machine translation tasks. The setting of learning rate updating strategy for SGD, Adam, KFAC and LNGD are the same as in ImageNet training.

In Fig.4 and Table3, we present the comparative evaluation of the performance of LNGD against SGD, Adam, and KFAC in terms of testing accuracy and training loss. ADAM demonstrates superior performance over SGD, as evidenced by a more rapid decrease in training loss and a lower converged loss value. This observation aligns with previous empirical findings that ADAM is highly effective for transformer models. KFAC exhibits further enhancements in performance compared to Adam,

   & Epoch & Total Time & Time Per Epoch & Acceleration & Best Test Acc \\  SGD & 79 & 268.67s & 3.4s & 29\% & 91.88\% \\ ADAM & 72 & 248.83s & 3.77s & 23\% & 92.62\% \\ KFAC & 45 & 241.86s & 5.87s & 21\% & 93.34\% \\ LNGD & 36 & 189.69s & 5.08s & & 93.61\% \\  

Table 1: Detailed statistics on CIFAR-10 when top-1 testing accuracy achieves 91%.

Figure 3: Numerical performance on ResNet-50 with ImageNet.

Figure 2: Numerical performance on ResNet-18 with CIFAR-10.

yet it does not surpass the efficacy of LNGD. LNGD outperforms its counterparts with the swiftest reduction in training loss and the highest convergence rates. In terms of testing accuracy, measured by the Bleu score, LNGD achieves a top-1 Bleu score of 32\(\%\) with remarkable efficiency, which is able to reduce the required steps by approximately 24\(\%\) and computing time by 16\(\%\) compared to Adam. When compared to KFAC, LNGD still shows significant improvements, reducing the steps by around 14\(\%\) and computing time by 24\(\%\). As for SGD, it cannot reach the top-1 Bleu score of 32\(\%\) and the best testing accuracy is only 31.8\(\%\), which indicates that SGD is not a good choice for large language processing tasks. In summary, the results provide strong evidences for the effectiveness of LNGD as an optimization algorithm for transformer models and shed light for large practical NLP tasks where time and computational resources are quite limited.

## 5 Conclusion

In summary, we propose a novel NGD optimizer named as LNGD for training DNNs, specifically targeting the computational inefficiencies that impede the practical application of conventional natural gradient techniques in large-scale neural networks. Our approach strategically computes Fisher information matrices for each individual layers using sample approximation and dynamically adjusts learning rates leveraging curvature information. This method facilitates a more refined representation of the optimization landscape at the layer level. Besides, we provide convergence analysis of LNGD. Experimental evaluations indicate its competitive performance relative to existing state-of-the-art optimizers. This work hold significant potential for enhancing the efficiency and scalability of training processes in deep learning frameworks.