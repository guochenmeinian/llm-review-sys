# Exponential Quantum Communication Advantage in Distributed Inference and Learning

Dar Gilboa

Google Quantum AI

Venice, CA, United States &Hagay Michaeli

Technion

Haifa, Israel &Daniel Soudry

Technion

Haifa, Israel &Jarrod R. McClean

Google Quantum AI

Venice, CA, United States

darg@google.com.

Equal contribution.

###### Abstract

Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints. We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest overhead relative to standard gradient-based methods. We show that certain graph neural networks are particularly amenable to implementation within this framework, and moreover present empirical evidence that they perform well on standard benchmarks. To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems that hold regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponentially with model depth. We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification. Communication of quantum states that potentially limit the amount of information that can be extracted from them about the data and model parameters may also lead to improved privacy guarantees for distributed computation. Taken as a whole, these findings form a promising foundation for distributed machine learning over quantum networks.

## 1 Introduction

As the scale of the datasets and parameterized models used to perform computation over data continues to grow , distributing workloads across multiple devices becomes essential for enabling progress. The choice of architecture for large-scale training and inference must not only make the best use of computational and memory resources, but also contend with the fact that communication may become a bottleneck . This is particularly pertinent as models grow so large that they cannot rely on high-bandwidth interconnects within datacenters , but are instead trained across multiple datacenters . When using modern optical interconnects, classical computers exchange bits represented by light. This however does not fully utilize the potential of the physical substrate; given suitable computational capabilities and algorithms, the _quantum_ nature of light can be harnessed as a powerful communication resource. Here we show that for a broad class of parameterized models, if quantum bits (_qubits_) are communicated instead of classical bits, an exponential reduction in the communication required to perform inference and gradient-based training can be achieved. This protocol additionally guarantees improved privacy of both the user data andmodel parameters through natural features of quantum mechanics, without the need for additional cryptographic or privacy protocols. To our knowledge, this is the first example of generic, exponential quantum advantage on problems that occur naturally in the training and deployment of large machine learning models. These types of communication advantages help scope the future roles and interplay between quantum and classical communication for distributed machine learning.

Quantum computers promise dramatic speedups across a number of computational tasks, with perhaps the most prominent example being the ability revolutionize our understanding of nature by enabling the simulation of quantum systems, owing to the inherently quantum nature of many many physical phenomena . However, much of the data that one would like to compute with in practice seems to come from an emergent classical world rather than directly exhibiting quantum properties. While there are some well-known examples of exponential quantum speedups for classical problems, most famously factoring  and related hidden subgroup problems , these tend to be isolated and at times difficult to relate to practical applications that involve learning from data. In addition, even though significant speedups are known for certain ubiquitous problems in machine learning such as matrix inversion  and principal component analysis , the advantage is often lost when including the cost of loading classical data into the quantum computer or of reading out the result into classical memory. This is because the complexity of loading dense classical data into the amplitudes of a quantum state (which is typically the encoding needed to obtain an exponential runtime advantage) and of reading out the amplitudes from a quantum state into classical memory, are both polynomial in the number of amplitudes . In applications where an efficient data access model avoids the above pitfalls, the complexity of quantum algorithms tends to depend on condition numbers of matrices which scale with system size in a way that reduces or even eliminates any quantum advantage . It is worth noting that much of the discussion about the impact of quantum technology on machine learning has focused on computational advantage. However quantum resources are not only useful in reducing computational complexity -- they can also provide an advantage in communication complexity, enabling exponential reductions in communication for some problems . Inspired by these results, we study a setting where quantum advantage in communication is possible across a wide class of machine learning models. This advantage holds without requiring any sparsity assumptions or elaborate data access models such as QRAM .

We focus on compositional distributed learning, known as _pipelining_. While there are a number of strategies for distributing machine learning workloads that are influenced by the requirements of different applications and hardware constraints , splitting up a computational graph in a compositional fashion (Figure 1) is a common approach. We describe distributed, parameterized quantum circuits that can be used to perform inference over data when distributed in this way, and can be trained using gradient methods. The ideas we present can also be used to optimize models that use certain forms of data parallelism (Appendix C). In principle, such circuits could be implemented on quantum computers that are able to communicate quantum states.

We show the following:

* Even for simple distributed quantum circuits, there is an exponential quantum advantage in communication for the problem of estimating the loss and the gradients of the loss with respect to the parameters (Section 3). This additionally implies a privacy advantage from Holevo's bound (Appendix H). We also show that this is advantage is not a trivial consequence of the data encoding used, since it does not hold for certain problems like linear classification (Appendix E).
* We study a class of models that can efficiently approximate certain graph neural networks (Section 4), and show that they both maintain the exponential communication advantage and achieve performance comparable to standard classical models on common node and graph classification benchmarks (Section 5).
* For certain distributed circuits, there is an exponential advantage in communication for the entire training process, and not just for a single round of gradient estimation. This includes circuits for fine-tuning using pre-trained features. The proof is based on convergence rates for stochastic gradient descent under convexity assumptions (Appendix D).
* The ability to interleave multiple unitaries encoding nonlinear features of data enables expressivity to grow exponentially with depth, and universal function approximation in some settings. This implies that these models are highly expressive in contrast to popular belief about linear restrictions in quantum neural networks (Appendix F).

## 2 Preliminaries

### Large-scale learning problems and distributed computation

Pipelining is a commonly used method of distributing a machine learning workload, in which different layers of a deep model are allocated distinct hardware resources [56; 87]. Training and inference then require communication of features between nodes. Pipelining enables flexible changes to the model architecture in a task-dependent manner, since subsets of a large model can be combined in an adaptive fashion to solve many downstream tasks. Additionally, pipelining allows sparse activation of a subset of a model required to solve a task, and facilitates better use of heterogeneous compute resources since it does not require storing identical copies of a large model. The potential for large models to be easily fine-tuned to solve multiple tasks is well-known [25; 20], and pipelined architectures which facilitate this are the norm in the latest generation of large language models [99; 16]. Data parallelism, in contrast, involves storing multiple copies of the model on different nodes, training each on a subsets of the data and exchanging information to synchronize parameter updates. In practice, different parallelization strategies are combined in order to exploit trade-offs between latency and throughput in a task-dependent fashion [115; 61; 97]. Distributed quantum models were considered recently in , but the potential for quantum advantage in communication in these settings was not discussed.

### Communication complexity

Communication complexity [117; 65; 98] is the study of distributed computational problems using a cost model that focuses on the communication required between players rather than the time or computational complexity. The key object of study in this area is the tree induced by a communication protocol whose nodes enumerate all possible communication histories and whose leaves correspond to the outputs of the protocol. The product structure induced on the leaves of this tree as a function of the inputs allows one to bound the depth of the tree from below, which gives an unconditional lower bound on the communication complexity. The power of replacing classical bits of communication with qubits has been the subject of extensive study [30; 23; 27]. For certain problems such as Hidden Matching  and a variant of classification with deep linear models  an exponential quantum communication advantage holds, while for other canonical problems such as Disjointness only a polynomial advantage is possible . Exponential advantage was also recently shown for the problem of sampling from a distribution defined by the solution to a linear regression problem

Figure 1: _Left:_ Distributed, compositional computation. Dashed lines separate devices with computational and storage resources. The circular nodes represent parameterized functions that are allocated distinct hardware resources and are spatially separated, while the square nodes represent data (yellow) and outputs corresponding to different tasks (green). The vertical axis represents time. This framework of hardware allocation enables flexible modification of the model structure in a task-dependent fashion. _Right:_ Computation of gradient estimators \(g_{}\) at different layers of a model distributed across multiple devices by pipelining. Computing forward features \(_{}\) and backwards features \(_{}\) (also known as computing a forward or backward pass) requires a large amount of classical communication (grey) but an exponentially smaller amount of quantum communication (yellow). \(\) is the classical loss function, and \(_{0}\) an operator whose expectation value with respect to a quantum model gives the analogous loss function in the quantum case.

. While there are many models of both quantum and classical communication, our results apply to _randomized_ classical communication complexity, wherein the players are allowed to exchange random bits independent of their problem inputs, and are allowed to output an incorrect answer with some probability (bounded away from \(1/2\) for a problem with binary output). It is also worth noting that communication advantages of the type we demonstrate can be naturally related to space advantages in streaming algorithms that may be of interest even in settings that do not involve distributed training .

At a glance, the development of networked quantum computers may seem much more challenging than the already herculean task of building a fault tolerant quantum computer. However, for some quantum network architectures, the existence of a long-lasting fault tolerant quantum memory as a quantum repeater, may be the enabling component that lifts low rate shared entanglement to a fully functional quantum network , and hence the timelines for small fault tolerant quantum computers and quantum networks may be more coincident than it might seem at first. As such, it is well motivated to consider potential communication advantages alongside computational advantages when talking about the applications of fault tolerant quantum computers. In Appendix G we briefly survey approaches to implementing quantum communication in practice, and the associated challenges.

In addition, while we largely restrict ourselves here to discussions of communication advantages, and most other studies focus on purely computational advantages, there may be interesting advantages at their intersection. For example, it is known that no quantum state built from a simple (or polynomial complexity) circuit can confer an exponential communication advantage, however states made from simple circuits can be made computationally difficult to distinguish . Hence the use of quantum pre-computation  and communication may confer advantages even when traditional computational and communication cost models do not admit such advantages due to their restriction in scope.

## 3 Distributed learning with quantum resources

In this work we focus on parameterized models that are representative of the most common models used and studied today in quantum machine learning, sometimes referred to as quantum neural networks [79; 38; 28; 104]. We will use the standard Dirac notation of quantum mechanics throughout. A summary of relevant notation and the fundamentals of quantum mechanics is provided in Appendix A. We define a class models with parameters \(\), taking an input \(x\) which is a tensor of size \(N\). The models take the following general form:

**Definition 3.1**.: \(\{A_{}(^{A}_{},x)\},\{B_{}(^{B}_{},x)\}\) _for \(\{1,,L\}\) are each a set of unitary matrices of size \(N^{} N^{}\) for some \(N^{}\) such that \( N^{}=O( N)\)3. The \(^{A}_{},^{B}_{}\) are vectors of \(P\) parameters each. For every \(,i\), we assume that \(}{^{A}_{}}\) is anti-hermitian and has two eigenvalues, and similarly for \(B_{}\)4._

_The model we consider is defined by_

\[(_{=L}^{1}A_{}(^{A}_{ },x)B_{}(^{B}_{},x)),\] (3.1)

_where \((x)\) is a fixed state of \( N^{}\) qubits._

_The loss function is given by_

\[(,x)_{0},\] (3.2)

_where \(_{0}\) is a Pauli matrix that acts on the first qubit._

In standard linear algebra notation, the output of the model is a unit norm \(N^{}\)-dimensional complex vector \(_{L}\), defined recursively by

\[_{0}=(x),_{}=A_{}(^{A}_{},x)B_{} (^{B}_{},x)_{-1},\] (3.3)where the entries of \(_{L}\) are represented by the amplitudes of a quantum state. The loss takes the form \((,x)=(_{L}^{})^{T}_{0}_ {L}\) where \({}^{}\) indicates the entrywise complex conjugate, and this definition includes the standard \(L^{2}\) loss as a special case.

Subsequently we omit the dependence on \(x\) and \(\) (or subsets of it) to lighten notation, and consider special cases where only subsets of the unitaries depend on \(x\), or where the unitaries take a particular form and may not be parameterized. Denote by \(_{A(B)}\) the entries of the gradient vector that correspond to the parameters of \(\{A_{}\}(\{B_{}\})\).

In the special case where \(x\) in a unit norm \(N\)-dimensional vector, a simple choice of \(|(x)\) is the amplitude encoding of \(x\), given by

\[|(x)=|x=_{i=0}^{N-1}x_{i}|i .\] (3.4)

However, despite its exponential compactness in representing the data, a naive implementation of the simplest choice is restricted to representing quadratic features of the data that can offer no substantial quantum advantage in a learning task , so the choice of data encoding is critical to the power of a model. The interesting parameter regime for classical data and models is one where \(N,P\) are large, while \(L\) is relatively modest. For general unitaries \(P=O(N^{2})\), which matches the scaling of the number of parameters in fully-connected networks. When the input tensor \(x\) is a batch of datapoints, \(N\) is equivalent to the product of batch size and input dimension.

The model in Definition 3.1 can be used to define distributed inference and learning problems by dividing the input \(x\) and the parameterized unitaries between two players, Alice and Bob. We define their respective inputs as follows:

\[:&|(x) ,\{A_{}\},\\ :&\{B_{}\}.\] (3.5)

The problems of interest require that Alice and Bob compute certain joint functions of their inputs. As a trivial base case, it is clear that in a communication cost model, all problems can be solved with communication cost at most the size of the inputs times the number of parties, by a protocol in which each party sends its inputs to all others. We will be interested in cases where one can do much better by taking advantage of quantum communication.

Given the inputs eq. (3.5), we will be interested chiefly in the two problems specified below.

**Problem 1** (Distributed Inference).: _Alice and Bob each compute an estimate of \(|_{0}|\) up to additive error \(\)._

The straightforward algorithm for this problem, illustrated in fig. 2, requires \(L\) rounds of communication. The other problem we consider is the following:

**Problem 2** (Distributed Gradient Estimation).: _Alice computes an estimate of \(_{A}|_{0}|\), while Bob computes an estimate of \(_{B}|Z_{0}|\), up to additive error \(\) in \(L^{}\)._

### Communication complexity of inference and gradient estimation

We show that inference and gradient estimation are achievable with a logarithmic amount of quantum communication, which will represent an exponential improvement over the classical cost for some cases:

Figure 2: Distributed quantum circuit implementing \(\) for \(L=2\). Both \(\) and its gradients with respect to the parameters of the unitaries can be estimated with total communication that is polylogarithmic in the size of the input data \(N\) and the number of trainable parameters per unitary \(P\).

**Lemma 1**.: _Problem 1 can be solved by communicating \(O( N)\) qubits over \(O(L/^{2})\) rounds._

Proof: Appendix B.

**Lemma 2**.: _Problem 2 can be solved with probability greater than \(1-\) by communicating \(( N( P)^{2}(L/)/^{4})\) qubits over \(O(L^{2})\) rounds. The time and space complexity of the algorithm is \(\ L\ (N, P,^{-1},(1/))\)._

Proof: Appendix B.

This upper bound is obtained by simply noting that the problem of gradient estimation at every layer can be reduced to a shadow tomography problem :

**Theorem 1** (Shadow Tomography  solved with Threshold Search ).: _For an unknown state \(\) of \( N\) qubits, given \(K\) known two-outcome measurements \(E_{i}\), there is an explicit algorithm that takes \(^{ k}\) as input, where \(k=(^{2}K N(1/)/^{4})\), and produces estimates of \(E_{i}\) for all \(i\) up to additive error \(\) with probability greater than \(1-\). \(\) hides subdominant polylog factors._

Using reductions from known problems in communication complexity, we can show that the amount of classical communication required to solve this problem is polynomial in the size of the input, and additionally give a lower bound on the number of rounds of communication required by any quantum or classical algorithm:

**Lemma 3**.:
1. _The classical randomized communication complexity of Problem_ 1 _and Problem_ 2 _with_ \(<1/2\) _is_ \(((,L))\)_._ 5__ 2. _Any algorithm (quantum or classical) for Problem_ 1 _or Problem_ 2 _requires either_ \((L)\) _rounds of communication or_ \((N/L^{4})\) _qubits (or bits) of communication._

Proof: Appendix B

The implication of the second result in Lemma 3 is that \((L)\) rounds of communication are necessary in order to obtain an exponential communication advantage for small \(L\), since otherwise the number of qubits of communication required can scale linearly with \(N\).

The combination of Lemma 1, Lemma 2 and Lemma 3 immediately implies exponential savings in communication for gradient estimation and inference:

**Theorem 2**.: _If \(L=O((N)),P=O((N))\) and sufficiently large \(N\), solving Problem 1 or Problem 2 with nontrivial success probability requires \(()\) bits of classical communication, while \(O((N,1/)(1/))\) qubits of communication suffice to solve these problems with probability at least \(1-\)._

The regime where \(L=O((N))\) is relevant for many classes of machine learning models. The required overhead in terms of time and space is only polynomial when compared to the straightforward classical algorithms for these problems.

The distribution of the model as in eq. (3.5) is an example of pipelining. Data parallelism is another common approach to distributed machine learning in which subsets of the data are distributed to identical copies of the model. In Appendix C we show that it can also be implemented using quantum circuits, which can then trained using gradient descent requiring quantum communication that is logarithmic in the number of parameters and input size.

Quantum advantage is possible in these problems because there is a bound on the complexity of the final output, whether it be correlated elements of the gradient up to some finite error or the low-dimensional output of a model. This might lead one to believe that whenever the output takes such a form, encoding the data in the amplitudes of a quantum state will trivially give an exponential advantage in communication complexity. We show however that the situation is slightly more nuanced, by considering the problem of inference with a linear model:

**Lemma 4**.: _For the problem of distributed linear classification, there can be no exponential advantage in using quantum communication in place of classical communication._

The precise statement and proof of this result are presented in Appendix E. This result also highlights that the worst case lower bounds such as Lemma 3 may not hold for circuits with certain low-dimensional or other simplifying structure.

## 4 Graph neural networks

The communication advantages in the previous section apply to relatively unstructured data and quantum circuits (essentially the only structure in the problem is related to the promise of the vector-in-subspace problem ), and it is a priori unclear how relevant they are to circuits that approximate useful neural networks, or act on structured data. Here we consider a class of shallow graph neural networks that achieve good performance on node classification tasks on large graphs . We prove that an exponential quantum communication advantage still holds for this class of models.

Consider a graph with \(N\) nodes. Define a local message passing operator on the graph \(A\) which may be the normalized Laplacian or some other operator. Given some \(N D_{0}\) matrix of graph features \(X\), we consider models of the following form:

\[(X)=((AXW_{1}))W_{2}\] (4.1)

where \(W_{1}^{D_{0} D_{1}},W_{2}^{D_{1} D_{2}}\) are parameter matrices, \(\) is a non-linearity and \(\) is a sum pooling operator acting on the first index of its input, which can be represented as multiplication by an \(N/t N\) matrix \(\). Since we would like a scalar output and a nonlinearity that can be implemented on a quantum computer, we instead compute

\[(X)=[((AXW_{1}))W_{2}],\] (4.2)

with

\[(x)=ax^{2}+bx+c\] (4.3)

and \(W_{2}^{D_{1} N/t}\) where \(t\) is the size of the pooling window.

This allows us to define the following problem:

**Problem 3** (Quadratic graph network inference (\(_{N,t}\))).: _Alice is given \(X\), Bob is given \(A,W_{1},W_{2}\). Only Alice is allowed to send messages. Their goal is to estimate \((X/||X||_{F})\) to additive error \(\)._

This models a scenario where only Bob has access to the connectivity of the graph, while Alice has access to the graph features. The normalization ensures that the choice of \(X\) does not introduce a dependence of the final output on \(N\).

In the following, we denote by \(R_{}^{}\) and \(Q_{}^{}\) the classical (public key randomized) communication complexity and quantum communication complexity respectively. We show:

**Lemma 5**.: \(R_{}^{}(_{N,t})=()\) _for any \()(t+)}\)._

Proof: Appendix B

**Lemma 6**.: \(Q_{}^{}(_{N,t})=O((|a|\,^{2}+|b|\, )\|W_{2}\|_{}(ND_{0})/)\) _where \(=\|W_{1}\|\,\|A\|\)._

Proof: Appendix B

If this upper bound was a polynomial function of \(N\), it would imply that an exponential communication advantage is impossible. For the parameter choices that realize classical communication lower bound, this is not the case, implying the following:

**Lemma 7**.: _An exponential quantum advantage in communication holds for solving the inference problem \(_{N,t}\) up to error \()(t+)}\), for any \(t\) such that \(t=(N)\)._Proof: Appendix B. Note that this exponential advantage does not hold only for a single setting of the model weights, but rather for the entire family of models that can be used to solve \(f-_{N,t}\) for functions \(f\) that satisfy eq. (B.24).

Note that generically, one would not expect the numerator in the upper bound of Lemma 6 to scale polynomially with \(N\). If \(A\) is for example a normalized graph Laplacian then \(||A|| 2\). If we use a standard initialization scheme for the weights (say Gaussians with variance \(1/(n_{in}+n_{out})\)), the upper bound scales like \(O((|a|+|b|)(ND_{0})(t,D_{0},D_{1})/)\) in expectation. Note that if the model output decays polynomially with \(N\), the upper bound will not be useful since one would need to choose \(\) to be inverse polynomial in \(N\). This could happen for example in a classification task considered in Section 5.2.1 when the classes are exactly balanced, or when the network is untrained and not sensitive to the structure in the data. While it is difficult to argue analytically about the scaling out the network output or the norms of the weight matrices after training due to the nonlinearity of the dynamics, we empirically compute these and find that they remain controlled for the datasets we study (see Appendix J.3).

In Section 5, we show that models of the form studied here achieve good performance on standard benchmarks, commensurate with state of the art models. Of particular relevance are the graph classification problems considered in Section 5.2.1, where the output takes the form eq. (4.2).

## 5 Experimental results

### Model

We evaluate our model6 (as defined in Equation (4.1)) on several graph tasks using common benchmarks and the DGL library . We use the SIGN model proposed by  as a baseline. The SIGN model can be seen as an instance of our model where the message passing operator \(A\) represents a column stack of \(R\) hops, the original features of \(X\) are duplicated \(R\) times and \(W_{1}\) is a block diagonal matrix. In Section 5.2 we simply replace the PReLU activation function with a second-degree polynomial with trainable coefficients  and compare the models on three node classification tasks. In Section 5.3, we implement a more general form of SIGN by relaxing \(W_{1}\) to be a dense matrix and evaluate our model over several graph-classification datasets.

### Node classification

We evaluate our model on three public node classification datasets: OGBN-Products , Reddit , and Cora . For both the baseline and polynomial model, we use SIGN with 5 hops of the neighbor averaging operator. We train on each dataset for 1000 epochs using Adam optimizer and report the test accuracy averaged on 10 runs (full details in Appendix J). Our results in Table 3 show that replacing the PReLU activation function with a second-degree polynomial causes a reduction of less than 1% on all of the tested datasets.

#### 5.2.1 Decision problems

We reduce the node classification task into a binary graph classification task by proposing the following decision problem: for a pair of classes \((c_{1},c_{2})\), return 1 if \(c_{1}\) has more nodes; otherwise, return 0. We solve this task for each pair of classes by summing the node classification model output across all nodes and choosing the class with the higher score. We use the _node_ classification training,

    &  &  \\  Model & OGBN-Products & Reddit & Cora & OGBN-Products & Reddit & Cora \\  SIGN (PReLU) & 79.48 \(\) 0.07 & 96.55 \(\) 0.02 & 78.84 \(\) 0.37 & 84.39 \(\) 1.73 & 90.33 \(\) 0.33 & 88.10 \(\) 5.61 \\ SIGN (Poly) & 78.51 \(\) 0.05 & 96.31 \(\) 0.03 & 78.69 \(\) 0.26 & 83.70 \(\) 1.48 & 89.37 \(\) 0.60 & 87.14 \(\) 3.92 \\   

Table 1: Test Accuracy for Node Classification and Decision Problem. Replacing PReLU with a polynomial of degree 2 causes a slight reduction in accuracy (less than 1%) for both node classification and decision problem across all datasets.

choose the model with the highest validation accuracy on the _graph_ classification task, and report its accuracy on the test sets. The model output in this form is given by eq. (4.1).

### Graph classification

We evaluate our model on several graph classification benchmarks: bioinformatics datasets (MUTAG, PTC, NCI1, PROTEINS) [105; 50; 35; 21] and social networks (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI) . For the bioinformatics datasets, we use the standard categorical node features. As proposed in , we use one-hot encoding of the node degree as node features for the COLLAB and IMDB datasets, and for REDDIT datasets all nodes are set with an identical scalar feature of 1. We convert the polynomial SIGN model in Section 5.2 into a graph classification model by inserting a SumPool operator as described in Equation (4.1). We use the sign diffusion operator  and stack \(R_{i}\) instances of each of its four message passing operators, where \(\{R_{i}\}_{i=1}^{4}\) are selected during a hyperparameter tuning, as well as the hidden dimension size and optimization setting (see Appendix J for more details). We follow the validation regime proposed by ; we perform 10-fold cross-validation, train each fold for 350 epochs using Adam optimizer, and report in Table 2 the maximal value and standard-deviation of the averaged validation accuracy curve. For all datasets, except for REDDIT, our model achieves comparable to or better than other commonly used models, despite most of them using multiple layers. While the results show that on most datasets our shallow architecture suffices given sufficient width in the message passing and hidden layer, we hypothesize that datasets without any node features (such as REDDIT) require at least two layers of message passing.

## 6 Discussion

This work constitutes a preliminary investigation into a generic class of quantum circuits that has the potential for enabling an exponential communication advantage in problems of classical data processing including training and inference with large parameterized models over large datasets, with inherent privacy advantages. Communication constraints may become even more relevant if such models are trained on data that is obtained by inherently distributed interaction with the physical world . The ability to compute using data with privacy guarantees can be potentially applied to proprietary data. This could become highly desirable even in the near future as the rate of publicly-available data production appears to be outstripped by the growth rate of training sets of large language models .

A limitation of the current results is that it's unclear to what extent powerful neural networks can be approximated using quantum circuits, even though we provide positive evidence in the form of the results on graph networks in Section 4. Additionally, the advantages we study require deep (\((N)\)), fault-tolerant quantum circuits. While this is a common feature of problems for which quantum communication advantages hold, the overhead of quantum error-correction in such circuits may be considerable. Detailed resource estimates would be necessary to understand better the practicality of this approach for achieving useful quantum advantage. Our results naturally raise further questions regarding the expressive power and trainability of these types of circuits, which may be of independent interest. We collect some of these in Appendix I.

    &  \\  Model & MUTAG & PTC & NCI1 & PROTEINS & COLLAB & IMDB-M & REDDIT-M \\  GIN  & 89.40\(\)5.60 & 64.60\(\)7.0 & 82.17\(\)1.7 & 76.2 \(\)2.8 & 80.2 \(\)1.90 & 52.3 \(\)2.8 & 57.5\(\)1.5 \\ DropGIN & 90.4 \(\)7.0 & 66.3 \(\)8.6 & - & 76.3 \(\)6.1 & - & 51.4 \(\)2.8 & - \\ DGCNN & 85.8 \(\)1.7 & 58.6 \(\)2.5 & - & 75.5 \(\)0.9 & - & 47.8 \(\)0.9 & - \\ U2GNN  & 89.97\(\)3.65 & 69.63\(\)3.60 & - & 78.53\(\)4.07 & 77.84\(\)1.48 & 53.60\(\)3.53 & - \\ HGP-SL & - & - & 78.45\(\)0.77 & 84.91\(\)1.62 & - & - & - \\ WKPI & 88.30\(\)2.6 & 68.10\(\)2.4 & 87.5 \(\)0.5 & 78.5\(\)0.4 & - & 49.5 \(\) 0.4 & 59.5 \(\) 0.6 \\  SIGN (ours) & 92.02\(\)6.45 & 68.0 \(\)8.17 & 77.25\(\)1.42 & 76.55\(\)5.10 & 81.82\(\)1.42 & 53.13\(\)3.01 & 54.09\(\)1.76 \\   

Table 2: Graph Classification Test Accuracy. Our model achieves comparable results to GIN and other known models on most datasets (see full table in Table 5).