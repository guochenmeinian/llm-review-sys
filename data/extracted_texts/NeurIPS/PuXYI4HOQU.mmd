# Fundamental Convergence Analysis of Sharpness-Aware Minimization

Pham Duy Khanh

Ho Chi Minh City University of Education

khanhpd@hcmue.edu.vn

&Hoang-Chau Luong

VNU-HCM University of Science

lhchau20@apcs.fitus.edu.vn

Boris S. Mordukhovich

Wayne State University

boris@math.wayne.edu

&Dat Ba Tran

Wayne State University

tranbadat@wayne.edu

###### Abstract

The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method  that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks  allows its extensions to efficient normalized versions of SAM such as F-SAM , VaSSO , RSAM , and to the unnormalized versions of SAM such as USAM . Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.

## 1 Introduction

This paper concentrates on optimization methods for solving the standard optimization problem

\[*{minimize} f(x)x^{n},\] (1)

where \(f:^{n}\) is a continuously differentiable (\(^{1}\)-smooth) function. We study the convergence behavior of the gradient-based optimization algorithm _Sharpness-Aware Minimization_ together with its efficient practical variants . Given an initial point \(x^{1}^{n}\), the original iterative procedure of SAM is designed as follows

\[x^{k+1}=x^{k}-t f(x^{k}+)}{\| f(x^{k })\|})\] (2)

for all \(k\), where \(t>0\) and \(>0\) are respectively the _stepsize_ (in other words, the learning rate) and _perturbation radius_. The main motivation for the construction algorithm is that by making the backward step \(x^{k}+)}{\| f(x^{k})\|}\), it avoids minimizers with large sharpness, which is usually poor for the generalization of deep neural networks as shown in Keskar et al. .

### Lack of convergence properties for SAM due to constant stepsize

The consistently high efficiency of SAM has driven a recent surge of interest in the analysis of the method. The convergence analysis of SAM is now a primary focus on its theoretical understandingwith several works being developed recently (e.g., Ahn et al. , Andriushchenko and Flammarion , Dai et al. , Si and Yun ). However, none of the aforementioned studies have addressed the fundamental convergence properties of SAM, which are outlined below where the stationary accumulation point in (2) means that every accumulation point \(\) of the iterative sequence \(\{x^{k}\}\) satisfies the condition \( f()=0\).

The relationship between the properties above is summarized as follows:

\[\|\}\}}{}  .\]

The aforementioned convergence properties are standard and are analyzed by various smooth optimization methods including gradient descent-type methods, Newton-type methods, and their accelerated versions together with nonsmooth optimization methods under the usage of subgradients. The readers are referred to Bertsekas , Nesterov , Polyak  and the references therein for those results. The following recent publications have considered various types of convergence rates for the sequences generated by SAM as outlined below:

(i) Dai et al. [2023, Theorem 1]

\[f(x^{k})-f^{*}(1-t(2-Lt))^{k}(f(x^{0})-f^{*})+^{2}}{2 (2-Lt)}\]

where \(L\) is the Lipschitz constant of \( f\), and where \(\) is the constant of strong convexity constant for \(f\).

(ii) Si and Yun [2023, Theorems 3.3, 3.4]

\[_{i=1}^{k}\| f(x^{i})\|^{2}=( +})_{i=1}^{k}\| f(x^{i}) \|^{2}=()+L^{2}^{2},\]

where \(L\) is the Lipschitz constant of \( f\). We emphasize that none of the results mentioned above achieve any fundamental convergence properties listed in Table 1. The estimation in (i) only gives us the convergence of the function value sequence to a value close to the optimal one, not the convergence to exactly the optimal value. Additionally, it is evident that the results in (ii) do not imply the convergence of \(\{ f(x^{k})\}\) to \(0\). To the best of our knowledge, the only work concerning the fundamental convergence properties listed in Table 1 is Andriushchenko and Flammarion . However, the method analyzed in that paper is unnormalized SAM (USAM), a variant of SAM with the norm being removed in the iterative procedure (2c). Recently, Dai et al.  suggested that USAM has different effects in comparison with SAM in both practical and theoretical situations, and thus, they should be addressed separately. This observation once again highlights the necessity for a fundamental convergence analysis of SAM and its normalized variants.

Note that, using exactly the iterative procedure (2), SAM does not achieve the convergence for either \(\{x^{k}\}\), or \(\{f(x^{k})\}\), or \(\{ f(x^{k})\}\) to the optimal solution, the optimal value, and the origin, respectively. It is illustrated by Example 3.1 below dealing with quadratic functions. This calls for the necessity of employing an alternative stepsize rule for SAM. Scrutinizing the numerical experiments conducted for SAM and its variants (e.g., Foret et al. [2021, Subsection C1], Li and Giannakis [2023, Subsection 4.1]), we can observe that in fact the constant stepsize rule is not a preferred strategy. Instead, the cosine stepsize scheduler from Loshchilov and Hutter , designed to decay to zero and then restart after each fixed cycle, emerges as a more favorable approach. This observation

  (1) & \(\| f(x^{k})\|=0\) \\ (2) & stationary accumulation point \\ (3) & \(\| f(x^{k})\|=0\) \\ (4) & \(f(x^{k})=f()\) with \( f()=0\) \\ (5) & \(\{x^{k}\}\) converges to some \(\) with \( f()=0\) \\  

Table 1: Fundamental convergence properties of smooth optimization methodsmotivates us to analyze the method under diminishing stepsize, which is standard and employed in many optimization methods including the classical gradient descent methods together with its incremental and stochastic counterparts (Bertsekas and Tsitsiklis, 2000). Diminishing step sizes also converge to zero as the number of iterations increases, a condition satisfied by the practical cosine step size scheduler in each cycle.

### Our Contributions

#### Convergence of SAM and normalized variants

We establish fundamental convergence properties of SAM in various settings. In the convex case, we consider the perturbation radii to be variable and bounded. This analysis encompasses the practical implementation of SAM with a constant perturbation radius. The results in this case are summarized in Table 2.

In the nonconvex case, we present a unified convergence analysis framework that can be applied to most variants of SAM, particularly recent efficient developments such as VaSSO (Li and Giannakis, 2023), F-SAM (Li et al., 2024), and RSAM (Liu et al., 2022). We observe that all these methods can be viewed as inexact gradient descent (IGD) methods with absolute error. This version of IGD has not been previously considered, and its convergence analysis is significantly more complex than the one in Khanh et al. (2023b, 2024a, 2023a, 2024b), as the function value sequence generated by the new algorithm may not be decreasing. This disrupts the convergence framework for monotone function value sequences used in the aforementioned works. To address this challenge, we adapt the analysis for algorithms with nonmonotone function value sequences from Li et al. (2023), which was originally developed for random reshuffling algorithms, a context entirely different from ours.

We establish the convergence of IGD with absolute error when the perturbation radii decrease at an _arbitrarily slow rate_. Although the analysis of this general framework does not theoretically cover the case of a constant perturbation radius, it poses no issues for the practical implementation of these methods, as discussed in Remark 3.6. A summary of our results in the nonconvex case is provided in the first part of Table 3.

#### Convergence of USAM and unnormalized variants

Our last theoretical contribution in this paper involves a refined convergence analysis of USAM in Andriushchenko and Flammarion (2022). In the general setting, we address functions satisfying the \(L\)-descent condition (4), which is even weaker than the Lipschitz continuity of \( f\) as considered in Andriushchenko and Flammarion (2022). The summary of the convergence analysis for USAM is given in the second part of Table 3.

As will be discussed in Remark G.4, our convergence properties for USAM use weaker assumptions and cover a broader range of applications in comparison with those analyzed in (Andriushchenko and Flammarion, 2022). Furthermore, the universality of the conducted analysis allows us to verify all the convergence properties for the extragradient method (Korpelevich, 1976) that has been recently applied in Lin et al. (2020) to large-batch training in deep learning.

 
**Classes of function** & **Results** \\  General setting & \( f(x^{k})=0\) \\  Bounded minimizer set & stationary accumulation point \\  Unique minimizer & \(\{x^{k}\}\) is convergent \\  

Table 2: Convergence properties of SAM for convex functions in Theorem 3.2

 
**SAM and normalized variants** &  \\ 
**Classes of functions** & **Results** & **Classes of functions** & **Results** \\  General setting & \( f(x^{k})=0\) & General setting & stationary accumulation point \\  General setting & \( f(x^{k})=f^{*}\) & General setting & \( f(x^{k})=f^{*}\) \\  KL property & \(\{x^{k}\}\) is convergent & \( f\) is Lipschitz & \( f(x^{k})=0\) \\   & & KL property & \(\{x^{k}\}\) is convergent \\  

Table 3: Convergence properties of SAM together with normalized variants (Corollary 3.5, Appendix D), and USAM together with unnormalized variants (Theorem 4.2)

### Importance of Our Work

Our work develops, for the first time in the literature, a fairly comprehensive analysis of the fundamental convergence properties of SAM and its variants. The developed approach addresses general frameworks while being based on the analysis of the newly proposed inexact gradient descent methods. Such an approach can be applied in various other circumstances and provides useful insights into the convergence understanding of not only SAM and related methods but also many other numerical methods in smooth, nonsmooth, and derivative-free optimization.

### Related Works

**Variants of SAM**. There have been several publications considering some variants to improve the performance of SAM. Namely, Kwon et al. (2021) developed the Adaptive Sharpness-Aware Minimization (ASAM) method by employing the concept of normalization operator. Du et al. (2022) proposed the Efficient Sharpness-Aware Minimization (ESAM) algorithm by combining stochastic weight perturbation and sharpness-sensitive data selection techniques. Liu et al. (2022) proposed a novel Random Smoothing-Based SAM method called RSAM that improves the approximation quality in the backward step. Quite recently, Li and Giannakis (2023) proposed another approach called Variance Suppressed Sharpness-aware Optimization (VaSSO), which perturbed the backward step by incorporating information from the previous iterations. As Li et al. (2024) identified noise in stochastic gradient as a crucial factor in enhancing SAM's performance, they proposed Friendly Sharpness-Aware Minimization (F-SAM) which perturbed the backward step by extracting noise from the difference between the stochastic gradient and the expected gradient at the current step. Two efficient algorithms, AE-SAM and AE-LookSAM, are also proposed in Jiang et al. (2023), by employing adaptive policy based on the loss landscape geometry.

**Theoretical Understanding of SAM**. Despite the success of SAM in practice, a theoretical understanding of SAM was absent until several recent works. Barlett et al. (2023) analyzed the convergence behavior of SAM for convex quadratic objectives, showing that for most random initialization, it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature. Ahn et al. (2024) introduces the notion of \(\)-approximate flat minima and investigates the iteration complexity of optimization methods to find such approximate flat minima. As discussed in Subsection 1.1, Dai et al. (2023) considers the convergence of SAM with constant stepsize and constant perturbation radius for convex and strongly convex functions, showing that the sequence of iterates stays in a neighborhood of the global minimizer while Si and Yun (2023) considered the properties of the gradient sequence generated by SAM in different settings.

**Theoretical Understanding of USAM**. This method was first proposed by Andriushchenko and Flammarion (2022) with fundamental convergence properties being analyzed under different settings of convex and nonconvex and optimization. Analysis of USAM was further conducted in Behdin and Mazumder (2023) for a linear regression model, and in Agarwala and Dauphin (2023) for a quadratic regression model. Detailed comparison between SAM and USAM, which indicates that they exhibit different behaviors, was presented in the two recent studies by Compagnoni et al. (2023) and Dai et al. (2023). During the final preparation of the paper, we observed that the convergence of USAM can also be deduced from Mangasarian and Solodov (1994), though under some additional assumptions, including the boundedness of the gradient sequence.

## 2 Preliminaries

First we recall some notions and notations frequently used in the paper. All our considerations are given in the space \(^{n}\) with the Euclidean norm \(\|\|\). As always, \(:=\{1,2,\}\) signifies the collections of natural numbers. The symbol \(x^{k}}{{}}\) means that \(x^{k}\) as \(k\) with \(k J\). Recall that \(\) is a _stationary point_ of a \(^{1}\)-smooth function \(f^{n}\) if \( f()=0\). A function \(f:^{n}\) is said to posses a Lipschitz continuous gradient with the uniform constant \(L>0\), or equivalently it belongs to the class \(^{1,L}\), if we have the estimate

\[\| f(x)- f(y)\| L\,\|x-y\|\ \ \ \ x,y^{n}.\] (3)This class of function enjoys the following property called the _\(L\)-descent condition_ (see, e.g., Izmailov and Solodov (2014, Lemma A.11) and Bertsekas (2016, Lemma A.24)):

\[f(y) f(x)+ f(x),y-x+\|y-x \|^{2}\] (4)

for all \(x,y^{n}\). Conditions (3) and (4) are equivalent to each other when \(f\) is convex, while the equivalence fails otherwise. A major class of functions satisfying the \(L\)-descent condition but not having the Lipschitz continuous gradient is given by Khanh et al. (2023b, Section 2) as \(f(x):= Ax,x+ b,x +c-h(x),\) where \(A\) is an \(n n\) matrix, \(b^{n}\), \(c\) and \(h:^{n}\) is a smooth convex function whose gradient is not Lipschitz continuous. There are also circumstances where a function has a Lipschitz continuous gradient and satisfies the descent condition at the same time, but the Lipschitz constant is larger than the one in the descent condition.

Our convergence analysis of the methods presented in the subsequent sections benefits from the _Kurdyka-Lojasiewicz \((KL)\) property_ taken from Attouch et al. (2010).

**Definition 2.1** (Kurdyka-Lojasiewicz property).: We say that a smooth function \(f:^{n}\) enjoys the _KL property_ at \( f\) if there exist \((0,]\), a neighborhood \(U\) of \(\), and a desingularizing concave continuous function \(:[0,)[0,)\) such that \((0)=0\), \(\) is \(^{1}\)-smooth on \((0,)\), \(^{}>0\) on \((0,)\), and for all \(x U\) with \(0<f(x)-f()<\), we have

\[^{}(f(x)-f())\| f(x)\| 1.\] (5)

_Remark 2.2_.: It has been realized that the KL property is satisfied in broad settings. In particular, it holds at every _nonstationary point_ of \(f\); see Attouch et al. (2010, Lemma 2.1 and Remark 3.2(b)). Furthermore, it is proved at the seminal paper (Lojasiewicz, 1965) that any analytic function \(f:^{n}\) satisfies the KL property on \(^{n}\) with \((t)~{}=~{}Mt^{1-q}\) for some \(q[0,1)\). Typical functions that satisfy the KL property are _semi-algebraic_ functions and in general, functions _definable in o-minimal structures_; see Attouch et al. (2010, 2013); Kurdyka (1998).

We utilize the following assumption on the desingularizing function in Definition 2.1, which is employed in Li et al. (2023). The satisfaction of this assumption for a general class of desingularizing functions is discussed in Remark G.1.

**Assumption 2.3**.: There is some \(C>0\) such that whenever \(x,y(0,)\) with \(x+y<\), it holds that

\[C[^{}(x+y)]^{-1}(^{}(x))^{-1}+(^{}( y))^{-1}.\]

## 3 SAM and normalized variants

### Convex case

We begin this subsection with an example that illustrates SAM's inability to achieve the convergence of the sequence of iterates to an optimal solution of strongly convex quadratic functions by using a constant stepsize. This emphasizes the necessity of avoiding this type of stepsize in our subsequent analysis.

_Example 3.1_ (SAM with constant stepsize and constant perturbation radius does not converge).: Let the sequence \(\{x^{k}\}\) be generated by SAM in (2) applied to the strongly convex quadratic function \(f(x)= Ax,x- b,x\), where \(A\) is an \(n n\) symmetric, positive-definite matrix and \(b^{n}\). Then for any fixed small constant perturbation radius and for some small constant stepsize together with an initial point close to the solution, the sequence \(\{x^{k}\}\) generated by this algorithm does not converge to the optimal solution.

The details of the above example are presented in Appendix A.1. Figure 1 gives an empirical illustration for Example 3.1. The graph shows that, while the sequence generated by GD converges to \(0\), the one generated by SAM gets stuck at a different point.

As the constant stepsize does not guarantee the convergence of SAM, we consider another well-known stepsize called diminishing (see (7)). The following result provides the convergence properties of SAM in the convex case for that type of stepsize.

**Theorem 3.2**.: _Let \(f:^{n}\) be a smooth convex function whose gradient is Lipschitz continuous with constant \(L>0\). Given any initial point \(x^{1}^{n}\), let \(\{x^{k}\}\) be generated by the SAM method with the iterative procedure_

\[x^{k+1}=x^{k}-t_{k} f(x^{k}+_{k})}{\| f (x^{k})\|})\] (6)

_for all \(k\) with nonnegative stepsizes and perturbation radii satisfying the conditions_

\[_{k=1}^{}t_{k}^{2}<,\ _{k=1}^{}t_{k}=,\ _{k}_{k}<.\] (7)

_Assume that \( f(x^{k}) 0\) for all \(k\) and that \(_{k}f(x^{k})>-\). Then the following assertions hold:_

**(i)**__\(_{k} f(x^{k})=0\)_._

**(ii)** _If \(f\) has a nonempty bounded level set, then \(\{x^{k}\}\) is bounded, every accumulation point of \(\{x^{k}\}\) is a global minimizer of \(f\), and \(\{f(x^{k})\}\) converges to the optimal value of \(f\). If in addition \(f\) has a unique minimizer, then the sequence \(\{x^{k}\}\) converges to that minimizer._

Due to the space limit, the proof of the theorem is presented in Appendix C.1.

### Nonconvex case

In this subsection, we study the convergence of several versions of SAM from the perspective of the inexact gradient descent methods.

This algorithm is motivated by while being different from the Inexact Gradient Descent methods proposed in . The latter constructions consider relative errors in gradient calculation, while Algorithm 1 uses the absolute ones. This approach is particularly suitable for the constructions of SAM and its normalized variants. The convergence properties of Algorithm 1 are presented in the next theorem.

**Theorem 3.3**.: _Let \(f:^{n}\) be a smooth function whose gradient is Lipschitz continuous with some constant \(L>0\), and let \(\{x^{k}\}\) be generated by the IGD method in Algorithm 1 with stepsizes and errors satisfying the conditions_

\[_{k=1}^{}t_{k}=,\ t_{k} 0,_{k=1}^{}t_{k} _{k}<,\ _{k}<2.\] (8)

Figure 1: SAM with constant stepsize does not converge to optimal solution_Assume that \(_{k}f(x^{k})>-\). Then the following convergence properties hold:_

**(i)**__\( f(x^{k}) 0,\) _and thus every accumulation point of_ \(\{x^{k}\}\) _is stationary for_ \(f\)_._

**(ii)** _If_ \(\) _is an accumulation point of the sequence_ \(\{x^{k}\}\)_, then_ \(f(x^{k}) f().\)__

**(iii)** _Suppose that_ \(f\) _satisfies the KL property at some accumulation point_ \(\) _of_ \(\{x^{k}\}\) _with the desingularizing function_ \(\) _satisfying Assumption_ 2.3_. Assume in addition that_

\[_{k=1}^{}t_{k}(^{}(_{i=k}^{ }t_{k}_{k}))^{-1}<,\] (9)

_and that_ \(f(x^{k})>f()\) _for sufficiently large_ \(k\)_. Then_ \(x^{k}\) _as_ \(k\)_. In particular, if_ \(\) _is a global minimizer of_ \(f\)_, then either_ \(f(x^{k})=f()\) _for some_ \(k\)_, or_ \(x^{k}\)_._

The proof of the theorem is presented in Appendix C.2. The demonstration that condition (9) is satisfied when \((t)=Mt^{1-q}\) with some \(M>0\) and \(q(0,1)\), and when \(t_{k}=\) and \(_{k}=}\) with \(p 0\) for all \(k\), is provided in Remark G.3.

The next example discusses the necessity of the last two conditions in (8) in the convergence analysis of IGD while demonstrating that employing a constant error leads to the convergence to a nonstationary point of the method.

_Example_ 3.4 (IGD with constant error converges to a nonstationary point).: Let \(f:\) be defined by \(f(x)=x^{2}\) for \(x\). Given a perturbation radius \(>0\) and an initial point \(x^{1}>\), consider the iterative sequence

\[x^{k+1}=x^{k}-2t_{k}(x^{k}-}{|x^{k}|})\ \ \ k,\] (10)

where \(\{t_{k}\}[0,1/2],t_{k} 0,\) and \(_{k=1}^{}t_{k}=\). This algorithm is in fact the IGD applied to \(f\) with \(g^{k}= f(x^{k}-(x^{k})}{|f^{}(x^{k})|})\). Then \(\{x^{k}\}\) converges to \(\), which is not a stationary point of \(f\).

The details of the example are presented in Appendix A.2. We now propose a general framework that encompasses SAM and all of its normalized variants including RSAM (Liu et al., 2022), VaSSO (Li and Giannakis, 2023) and F-SAM (Li et al., 2024). Due to the page limit, we refer readers to Appendix D for the detailed constructions of those methods. Remark D.1 in Appendix D also shows that all of these methods are special cases of Algorithm 1a, and thus all the convergence properties presented in Theorem 3.3 follow.

_Step \(0\)._ Choose \(x^{1}^{n},\{_{k}\},\{t_{k}\}(0,)\), and \(\{d^{k}\}^{n}\{0\}.\) For \(k 1\) do the following:

_Step \(1\)._ Set \(x^{k+1}=x^{k}-t_{k} f(x^{k}+_{k}}{\|d^{k}\|})\).

**Algorithm 1a** General framework for normalized variants of SAM

**Corollary 3.5**.: _Let \(f:^{n}\) be a \(^{1,L}\) function, and let \(\{x^{k}\}\) be generated by Algorithm 1a with the parameters_

\[_{k=1}^{}t_{k}=,\ t_{k} 0,_{k=1}^{}t_{k} _{k}<,\ _{k}<.\] (11)

_Assume that \(_{k}f(x^{k})>-\). Then all convergence properties presented in Theorem 3.3 hold._

The proof of this result is presented in Appendix C.5.

_Remark 3.6_.: Note that the conditions in (11) do not pose any obstacles to the implementation of a constant perturbation radius for SAM in practical circumstances. This is due to the fact that a possible selection of \(t_{k}\) and \(_{k}\) satisfying (11) is \(t_{k}=\) and \(_{k}=}\) for all \(k\) (almost constant), where \(C>0\). Then the initial perturbation radius is \(C\), while after \(C\) million iterations, it remains greater than \(0.99C\). This phenomenon is also confirmed by numerical experiments in Appendix E on nonconvex functions. The numerical results show that SAM with almost constant radii \(_{k}=}\) has a similar convergence behavior to SAM with a constant radius \(=C\). As SAM with a constant perturbation radius has sufficient empirical evidence for its efficiency in Foret et al. (2021), this also supports the practicality of our almost constant perturbation radii.

## 4 USAM and unnormalized variants

In this section, we study the convergence of various versions of USAM from the perspective of the following Inexact Gradient Descent method with relative errors.

_Step 0_. Choose some \(x^{0}^{n}, 0\), and \(\{t_{k}\}[0,)\). For \(k=1,2,,\) do the following:

_Step 1_. Set \(x^{k+1}=x^{k}-t_{k}g^{k}\), where \(g^{k}- f(x^{k})\| f(x^{k})\|\).

**Algorithm 2** IGDr

This algorithm was initially introduced in Khanh et al. (2023b) in a different form, considering a different selection of error. The form of IGDr closest to Algorithm 2 was established in Khanh et al. (2024a) and then further studied in Khanh et al. (2024a, 2023a, 2024b). In this paper, we extend the analysis of the method to a general stepsize rule covering both constant and diminishing cases, which was not considered in Khanh et al. (2024a).

**Theorem 4.1**.: _Let \(f:^{n}\) be a smooth function satisfying the descent condition for some constant \(L>0,\) and let \(\{x^{k}\}\) be the sequence generated by Algorithm 2 with the relative error \([0,1)\), and the stepsizes satisfying_

\[_{k=1}^{}t_{k}=\ \ \ \ t_{k}[0,}]\] (12)

_for sufficiently large \(k\) and for some \(>0\). Then either \(f(x^{k})-\), or we have the assertions:_

**(i)** _Every accumulation point of_ \(\{x^{k}\}\) _is a stationary point of the cost function_ \(f\)_._

**(ii)** _If the sequence_ \(\{x^{k}\}\) _has any accumulation point_ \(\)_, then_ \(f(x^{k}) f()\)_._

**(iii)** _If_ \(f^{1,L}\)_, then_ \( f(x^{k}) 0\)_._

**(iv)** _If_ \(f\) _satisfies the KL property at some accumulation point_ \(\) _of_ \(f\)_, then_ \(\{x^{k}\}\)_._

**(v)** _Assume in addition to (iv) that the stepsizes are bounded away from_ \(0\)_, and the KL property in_ (iv) _holds with the desingularizing function_ \((t)=Mt^{1-q}\) _with_ \(M>0\) _and_ \(q(0,1)\)_. Then either_ \(\{x^{k}\}\) _stops finitely at a stationary point, or the following convergence rates are achieved:_

\(\) _If_ \(q=1/2\)_, then_ \(\{x^{k}\},\ \{ f(x^{k})\}\)_,_ \(\{f(x^{k})\}\) _converge linearly as_ \(k\) _to_ \(\)_,_ \(0\)_, and_ \(f()\)_._

\(\) _If_ \(q(1/2,1)\)_, then_

\[\|x^{k}-\|=(k^{-}),\ \| f(x^{k})\|=(k^{-}), \ f(x^{k})-f()=(k^{-}).\]

Although the ideas for proving this result is similar to the one given in Khanh et al. (2024a), we do provide the full proof in the Appendix C.3 for the convenience of the readers. We now show that using this approach, we derive more complete convergence results for USAM in Andriushchenko and Flammarion (2022) and also the extragradient method by Korpelevich (1976); Lin et al. (2020).

_Step 0_. Choose \(x^{0}^{n}\), \(\{_{k}\}[0,)\), and \(\{t_{k}\}[0,)\). For \(k=1,2,,\) do the following:

_Step 1_. Set \(x^{k+1}=x^{k}-t_{k} f(x^{k}+_{k} f(x^{k}))\).

**Algorithm 2b**(Korpelevich, 1976) Extragradient Method

_Step 0_. Choose \(x^{0}^{n}\), \(\{_{k}\}[0,)\), and \(\{t_{k}\}[0,)\). For \(k=1,2,,\) do the following:

_Step 1_. Set \(x^{k+1}=x^{k}-t_{k} f(x^{k}-_{k} f(x^{k}))\).

[MISSING_PAGE_FAIL:9]

The results on CIFAR-10 and CIFAR-100 indicate that SAM with **Diminish 3** stepsize usually achieves the best performance in both accuracy and training loss among all tested stepsizes. In all the architectures used in the experiment, the results consistently show that diminishing stepsizes outperform constant stepsizes in terms of both accuracy and training loss measures. Additional numerical results on a larger data set and without momentum can be found in Appendix F.

## 6 Discussion

### Conclusion

In this paper, we provide a fundamental convergence analysis of SAM and its normalized variants together with a refined convergence analysis of USAM and its unnormalized variants. Our analysis is conducted in deterministic settings under standard assumptions that cover a broad range of applications of the methods in both convex and nonconvex optimization. The conducted analysis is universal and thus can be applied in different contexts other than SAM and its variants. The performed numerical experiments show that our analysis matches the efficient implementations of SAM and its variants that are used in practice.

### Limitations

Our analysis is only conducted in deterministic settings, which leaves the stochastic and random reshuffling developments to our future research. The analysis of SAM coupling with momentum methods is also not considered in this paper. Another limitation pertains to numerical experiments, where only SAM was tested on three different architectures of deep learning.