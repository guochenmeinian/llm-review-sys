# SPACE: Single-round Participant Amalgamation for Contribution Evaluation in Federated Learning

Yi-Chung Chen

National Taiwan University

r10942081@ntu.edu.tw

&Hsi-Wen Chen

National Taiwan University

hwchen@arbor.ee.ntu.edu.tw

&Shun-Gui Wang

National Taiwan University

r11921099@ntu.edu.tw

&Ming-Syan Chen

National Taiwan University

mschen@ntu.edu.tw

###### Abstract

The evaluation of participant contribution in federated learning (FL) has recently gained significant attention due to its applicability in various domains, such as incentive mechanisms, robustness enhancement, and client selection. Previous approaches have predominantly relied on the widely adopted Shapley value for participant evaluation. However, the computation of the Shapley value is expensive, despite using techniques like gradient-based model reconstruction and truncating unnecessary evaluations. Therefore, we present an efficient approach called Single-round Participants Amalgamation for Contribution Evaluation (SPACE). SPACE incorporates two novel components, namely _Federated Knowledge Amalgamation_ and _Prototype-based Model Evaluation_ to reduce the evaluation effort by eliminating the dependence on the size of the validation set and enabling participant evaluation within a single communication round. Experimental results demonstrate that SPACE outperforms state-of-the-art methods in terms of both running time and Pearson's Correlation Coefficient (PCC). Furthermore, extensive experiments conducted on applications, client reweighting, and client selection highlight the effectiveness of SPACE. The code is available at https://github.com/culiver/SPACE.

## 1 Introduction

With the rapid growth of mobile services, the combination of rich user interactions and robust sensors means they are able to access an unprecedented amount of data. While conventional machine learning approaches require centralizing the training data in a single machine or a data center, which may limit the scalability of machine learning applications, Federated Learning (FL)  has been proposed to leave the training data distributed on the client side and then aggregate locally computed updates, e.g., the gradient of the neural network, to a central coordinating server, without accessing the raw data. A line of subsequent efforts has significantly improved the efficiency , and robustness  of FL. Moreover, such distributed training paradigm has been adapted to client personalization , and heterogeneous model architecture , making it even more practical.

Recently, there has been increasing interest in evaluating the participant's contribution to FL for a fair incentive mechanism  based on the participant contribution in attracting data owners. Additionally, deciding the participant weights  during the training process based on their contributions can accelerate the convergence of the training process but also enhance the model's robustness by alleviating the negative effect from the confused or even incorrectdata. Moreover, if the communication budget is limited, participant contributions can be utilized as a reference to select important participants [7; 11; 53; 56; 73], reducing communication costs while maintaining the model's performance.

One straightforward method to evaluate participant contribution is to assess their individual performance independently. For instance, the accuracy of the local model has been employed as a measure , as well as the consistency between the local and global models, indicating their similarity to the consensus of the coalition [45; 72]. However, these approaches do not fully consider the impact of client cooperation. To consider cooperation, some studies have adopted the influence function  from centralized data evaluation to assess participant contribution [57; 64]. However, calculating the influence function involves clients sending their locally computed Hessian matrices to the server, leading to unacceptable client computation and communication costs. On the other hand, researchers have explored using the Shapley value  in cooperative game theory to measure participant contributions. While the exact computation of the Shapley value requires enumerating all possible client combinations, Song et al.  approximate the target model with gradient updates to avoid model retraining. To further speed up the computation, Wei et al.  and Liu et al.  propose truncation techniques, eliminating unnecessary model reconstruction and evaluation.

Despite the aforementioned efforts, the approximation cost of the Shapley value can still be prohibitively high for the following two reasons. **i) Multi-Round Training:** FL algorithms, such as FedAvg, involve multiple communication rounds wherein models are transmitted between the server and clients, and local training takes place on the client side before achieving model convergence. Previous studies [57; 58; 59; 32; 48] compute participant contributions for each communication round and aggregate them to obtain the overall participant contribution. Consequently, the communication and computation efforts escalate with an increase in the number of communication rounds. **ii) Dataset-Dependent Evaluation:** As demonstrated in , the evaluation of models constitutes the primary computational time component required for calculating the Shapley value. Even with the adoption of permutation sampling approximation, the complexity of evaluating per-round contributions depends on the size of the validation dataset on the server. As the validation set continually grows, the evaluation process becomes computationally unacceptable.

In this paper, we propose _Single-round Participants Amalgamation for Contribution Evaluation (SPACE)_ to efficiently calculate the participant contribution in FL. Specifically, we analyze the data distribution between the server and the client dataset. If the server and client distributions are similar, the client dataset proves helpful in correctly classifying the data in the validation set of the server. While previous works [51; 52; 37; 68] have shown that prototypes, i.e., the embedding vectors representing each class, can be employed as a privacy-preserving approach to compare data distribution, we propose _Prototype-based Model Evaluation_, which evaluates the model performance by measuring the similarities between the local and server prototypes. Therefore, the distribution of the validation set can be constructed and thus eliminates the dependency of evaluation complexity on the size of the validation set.

To derive meaningful prototype embeddings, we propose a novel approach called _Federated Knowledge Amalgamation_, wherein client models can be viewed as multiple distributed teacher models, and their knowledge can be distilled to educate the server model simultaneously. Thus, federated knowledge amalgamation enables us to distill knowledge from all local models in just one communication round to reduce the time for prototype construction. The educated model is distributed to each participant, who then constructs prototypes based on their local data. We then evaluate the model performance by measuring the similarities between server and client prototypes. Besides, unlike previous works that adopt the model performance as the utility function, we modify the utility function by incorporating a logistic function. This adjustment aims to reflect users' satisfaction better, thereby enhancing the rationality of the utility function in real-world scenarios. Extensive results demonstrate the advantage of SPACE in multiple applications, including contribution evaluation, client re-weighting, and client selection.

The contributions of this paper are summarized as follows:

* We propose SPACE, an efficient and effective approach for client contribution evaluation in an FL setting, which significantly reduces the required communication and computational cost and exhibits outstanding performance in terms of Pearson's correlation coefficient (PCC).
* We introduce _Prototype-based Model Evaluation_, which effectively removes the computation dependency on the size of the validation set. To efficiently establish a robust embedding space for building prototypes, we introduce _Federated Knowledge Amalgamation_ that enables the aggregation of embedding space information from local models in an FL setting, all within a single round of communication.
* Extensive experiments have been conducted to show that SPACE outperforms previous state-of-the-art methods in terms of both running time and PCC. SPACE consistently exhibits exceptional performance in applications including client reweighting and selection.

## 2 Related Works

**Contribution evaluation in FL.** Client contribution evaluation in federated learning (FL) has gained significant attention for its potential applications, including attack detection [14; 31], debugging [25; 26], incentive mechanisms [33; 45; 46; 48; 55; 65; 72], and convergence acceleration [54; 57]. This concept draws inspiration from centralized data evaluation tasks [16; 19]. Recent works have proposed methods tailored for FL. Pandey et al.  consider the relative accuracy of the local model as individual contribution. Zhang et al.  calculate the cosine similarity between local gradients and a unit vector pointing towards the optimal model as contributions. Shi et al.  measure contribution based on the cosine similarity between local and global gradients. However, these methods do not fully capture the impact of cooperation. To address this limitation, the Shapley value is widely adopted for evaluating client contribution in FL due to its desirable properties, including efficiency, symmetry, linearity, and null player property. However, directly computing the Shapley value incurs exponential computational costs involving model retraining and inference. To alleviate this burden, Song et al.  reconstruct the target model using gradient updates to avoid model retraining. Wang et al.  enhance the evaluation using permutation sampling and a group testing approach. Additionally, Wei et al.  and Liu et al.  incorporate truncation techniques to reduce computation costs. Despite these efforts, approximating the Shapley value still requires significant computation.

**Knowledge Amalgamation.** Knowledge amalgamation (KA), a variant of knowledge distillation , involves a setup with one student model and multiple teacher models. KA has been widely adopted in prior works [5; 20; 34; 43; 44; 61; 66; 67] due to its desirable properties. Shen et al.  introduced KA, where compact feature representations are learned from teachers and used to update the student model. Shen et al.  proposed a strategy that extracts task-specific knowledge from diverse teachers to create component networks, which are then combined to construct the student network. Luo et al.  introduced a common feature learning scheme in heterogeneous network settings, transforming the features of all teachers into a shared space. Xie et al.  proposed a related approach applying KA to transfer prior knowledge from multiple pre-trained models. However, their focus is on knowledge transfer from given pre-trained models, while our approach is the first to employ KA within a conventional FL framework. In our setting, client models can be viewed as distributed teacher models, and their knowledge can be distilled to educate the server model.

## 3 Preliminary

Since federated learning aims to construct a unified, global machine-learning model to collect data information located on numerous remote devices (client), the objective is to develop this model while ensuring that the data generated by each device is processed and stored locally, with only intermediate updates being sent to a central server at regular intervals. The primary objective is usually to minimize the objective function as follows. Let \(\) denote a set consisting of \(n\) clients,

\[^{*}=_{}_{i}^{n}p_{i}L_{i}(),\] (1)

where \(p_{i} 0\) and \(_{i}p_{i}=1\). \(L_{i}\) is the objective function for the \(i\)-th client. Generally, each client may utilize the same objective function, such as the empirical risk, which can be defined as follows.

\[L_{i}=_{(x_{i,j},y_{i,j})_{i}}l(,x_{i,j},y_{i,j}),\] (2)

where \(_{i}\) is the dataset in \(i\)-th client. \(p_{i}\) specifies the relative impact of each device, such as the uniform weighting \(\) or depending on the number of samples in each dataset \(_{i}|}{_{i}^{n}|_{i}|}\).

Despite early efforts to enhance the efficacy of federated learning, many data owners hesitate when it comes to partaking in data federation owing to apprehensions surrounding the possibility of inequitable remuneration. Furthermore, in federated learning, privacy concerns preclude the server from directly accessing unprocessed data stored on client devices. Consequently, there is a pressing need for a privacy-preserving federated contribution evaluation method that ensures equitable distribution of rewards.

Specifically, the evaluation of participant contributions can be abstracted into cooperative game scenarios. Given a cooperative game (\(,V\)), where \(\) denotes the set of \(n\) participants and \(V()\) is a utility function defined as \(V:2^{n}\), which assigns a value to a coalition \(\), the objective of payoff allocation is to determine a payoff allocation vector, denoted by \(=(c_{1},...,c_{n})\), where \(c_{i}\) represents the payoff for the \(i\)-th participant in \(\).

The idea of cooperative games has been extensively researched [2; 3], with the Shapley value being the most commonly used metric due to its existence and uniqueness in a cooperative game. Additionally, the Shapley value is characterized by a set of desirable axiomatic properties, including (i) efficiency, (ii) symmetry, (iii) null player, and (iv) linearity, which make it a desirable approach for evaluating contributions. The Shapley value is defined as follows,

\[_{i}(,V)=_{S\{i\}}(V(S\{i\})-V(S)).\] (3)

\(\) denotes the subset of participants from \(\). The Shapley value can be seen as the average marginal gain that a participant joins the coalitions. To accelerate the approximation, the Monte Carlo estimation method for approximating Shapley values from  is adopted,

\[_{i}=}_{}[V(S_{}^{i}\{i\})-V(S_{}^{i} )],\] (4)

where \(S_{}^{i}\) is the set of data from FL participants joining before \(i\) in the \(\)-th permutation of the sequence of FL participants. The calculated Shapley values are clipped to be non-negative to follow the non-negative assumption as in .

## 4 Method

Shapley value is a widely used approach for evaluating participant contributions in federated learning due to its desirable properties. However, despite the proposal of various acceleration techniques

Figure 1: Overall architecture of SPACE. Initially, the clients train their respective local models using their local datasets. Subsequently, the locally optimized models are frozen and serve as teacher models to educate the student model on the server by Federated Knowledge Amalgamation. Then, the educated model \(^{*}\) is distributed to all clients, who then utilize it to construct their corresponding prototypes \(_{i}\). These prototypes are then uploaded to the server. The server employs the Prototype-based Model Evaluation approach to assess the modelâ€™s performance and determine the contribution \(\) of each participant.

such as Monte Carlo sampling, gradient-based model reconstruction, and truncation, the computation of the Shapley value remains prohibitively slow due to the following two key challenges **Multi-Round Training**: multiple communication rounds are required for model convergence, and **Dataset-dependent Evaluation**: the evaluation of model performance depends on the size of the validation set on the server. Thus, we propose the Single-round Participant Amalgamation for Contribution Evaluation (SPACE) approach, which analyzes participant contributions using only one communication round. The key idea behind SPACE is to compare the distribution between the local dataset and the validation dataset directly via prototype embeddings of the local dataset and the validation dataset . To establish robust prototype embeddings, we introduce Federated Knowledge Amalgamation to distill feature information from all locally optimized models to the server model simultaneously, which enables the server model to learn information in a single communication round. Once the prototypes are obtained, we evaluate the performance of coalition \(\) by comparing the similarities between the prototypes formed by clients in coalition \(_{}\) and prototypes formed by validation set \(_{}\). Using prototypes eliminates the need to iterate over all samples in the validation set, resulting in faster performance evaluation. Figure 1 shows the architecture of SPACE.

### Federated Knowledge Amalgamation

As previously mentioned, the SPACE framework utilizes prototypes to assess participant contributions. Therefore, employing a semantically meaningful deep model is essential to ensure the extraction of expressive features from data samples. One straightforward approach is to utilize the converged model achieved through federated training, which encompasses training with all the local data. However, the convergence of federated learning algorithms like FedAvg necessitates numerous rounds of local-global communication and local training. Consequently, the evaluation of contributions can only be conducted after extensive training, resulting in excessively high costs and impeding the utilization of participant contributions in applications like client selection.

In contrast to conventional knowledge distillation tasks, which typically involve a single student model and a single teacher model, knowledge amalgamation techniques are employed to entail one student model and multiple teacher models. Furthermore, in federated learning, the client models can be considered teacher models that acquire knowledge from their local datasets. Consequently, the application of knowledge amalgamation facilitates the construction of an expressive embedding space for federated learning.

SPACE first distributes the global model \(\) to each client and each client implements empirical risk minimization to obtain a locally optimized model \(_{i}\). The knowledge amalgamation is then applied to transfer the knowledge to the initial global model \(\) with the locally optimized models serving as teacher models and the server validation dataset for distillation. To simultaneously distill knowledge from all the teacher models, the features of both the student and teacher models must be projected to common feature space for the feature alignment.

Therefore, we use feature projector modules (FPM) to project the intermediate feature of models to the common feature space. Since the data distributions of clients are different, the projections from the original feature space to the common feature space are also different. Therefore, each model, including the student model and teacher models, would be assigned an individual learnable FPM for projection. After projection, the student model then starts to learn information from teacher models by minimizing the feature distillation loss \(L_{feat}\), which is the L1 distance between the projected student features \(}_{s}\) and the projected teacher features \(}_{t_{i}}\). However, due to the differences in local data distribution, the teacher models are not equally important, and the loss should be weighted differently depending on the sample used for knowledge amalgamation. We define the weighting of each teacher model \(w_{i}\) by its confidence in predicting the class of data sample. Specifically, given a data sample \(d\) with its class as \(j\) for distillation, the weighting of each teacher model is the softmax of the \(j\)-th logit of its prediction \((_{i},d)\), which can be written as follows:

\[L_{feat}=_{i}w_{i}|}_{s}-}_{t_{i}}|,\ w_{i}=(_{i},d)_{j}}}{_{i}e^{ (_{i},d)_{j}}}\] (5)

To prevent the FPM from projecting all the features to a constant vector, which is a trivial optimal solution for the feature distillation loss, reconstruction loss is proposed to ensure the projected feature contains the knowledge in the original feature space. For each teacher model, we apply an inverse feature projector module (iFPM) to reconstruct features in the original feature space, which projects \(}_{t_{i}}\) to the reconstructed feature \(_{t_{i}}\). The reconstruction loss is defined as follows:

\[L_{rec}=_{i}|_{t_{i}}-}_{t_{i}}|\] (6)

where \(_{t_{i}}\) is the original feature of \(i\)-th client before projection. Aside from feature distillation, we also use prediction distillation to enhance the overall amalgamation performance. For prediction distillation, the student model only learns from the teacher that predicts the highest confidence in the correct prediction. The prediction distillation loss \(L_{KL}\) is defined as follow:

\[L_{KL}=KL((_{S},d),(_{best},d)),_{best }=_{_{i}}((_{i},d)_{j})\] (7)

where \(KL()\) denotes the KL divergence loss. The overall amalgamation loss \(L_{amlg}\) becomes, \(L_{amlg}=_{feat}L_{feat}+_{rec}L_{rec}+_{KL}L_{KL}\), where \(\) are used for balancing the loss terms. After knowledge amalgamation, the server model \(^{*}\) is then distributed to each client for extracting features and building prototypes of each category.

While the theoretical complexity of knowledge amalgamation is \(O(n)\), practical considerations arise as the number of clients \(n\) increases, leading to an increase in the number of FPMs that require training. This poses a challenge due to the limited memory capacity of GPUs, as training all FPMs simultaneously may not be feasible. To address this issue, we suggest a hierarchical amalgamation framework that alleviates the high GPU memory requirement. By allocating a GPU memory budget, we can amalgamate groups of \(G\) teacher models instead of all \(n\) teacher models at once. Although this modification increases the complexity of knowledge amalgamation to \(O(n n)\), it enables the implementation of knowledge amalgamation on any GPU device capable of simultaneously amalgamating at least two teacher models.

### Prototype-based Model Evaluation

The concept of utilizing prototypes in classification models has its roots in the Prototypical Networks introduced by Snell et al. , which were initially proposed for addressing few-shot classification tasks. In this framework, the classification of an embedded query point is achieved by identifying the nearest class prototype. The underlying assumption is that an embedding space exists where data points tend to cluster around a single prototype representation for each class. By leveraging prototypes, it becomes unnecessary to compute the distances between all samples in the query set and the support set, as required in Sung et al. . The prototypes capture the distribution of samples in the support set, simplifying the classification process.

In our study, we adopt a similar perspective to Snell et al.  by considering the clients' local datasets as the support set. From the embedding space derived through the Federated Knowledge Amalgamation described earlier, we construct prototypes for each client, denoted as \(_{i}^{(j)}\), representing the prototype for class \(j\) of the \(i\)-th client. To address the challenge of evaluation complexity dependence on the size of the validation set, we introduce a modification to the evaluation step. Rather than iterating through all validation samples, we devise a strategy where prototypes are built for the server's validation set. These prototypes, denoted as \(_{V}^{(j)}\), represent the prototypes for class \(j\) of the validation set. Model evaluation is then performed by comparing the similarities of the prototypes between the clients and the server. This modification effectively eliminates the computational burden associated with iterating through all validation samples. The prototypes are built as follows:

\[_{i}^{(j)}=_{i,j}|}_{(x,y)_ {i,j}}f(^{*},x)\] (8)

where \(_{i,j}\) implies the data that is class \(j\) in the client \(i\)-th dataset and function \(f()\) is used to extract feature from data \(x\) with model weight \(^{*}\). We set the prototype of clients as a zero vector if the client does not possess any data of the corresponding class, such that \(_{i}^{(j)}=\) if \(|_{i,j}|=0\). When clients cooperate in coalitions, the prototypes for class \(j\) of coalition \(\) is denoted as \(_{}^{(j)}\), which can simply be achieved by the weighted sum of prototypes in the coalition.

\[_{}^{(j)}=_{i}_{i,j} |}{_{i}|_{i,j}|}_{i}^{(j)}\] (9)

We then define the performance of the coalition by the confidence of correctly classifying the given prototypes of the validation set. The confidence is calculated by the relative value between the similarity of the same class and the sum of the similarities of all classes, while a softmax function is applied to ensure the value lies within the range \(\). To be more specific, the value is computed as follows:

\[V^{}()=|}_{j}^{(j)}_{},^{(j)}_{})}}{_{k}e^{ Sim(^{(j)}_{},^{(k)}_{})}},\] (10)

where \(||\) represents the number of classes in the classification task. We adopt cosine similarity as our similarity function.

### Contribution Evaluation

Shapley value is applied to evaluate the participant contribution. However, adopting the model performance \(V^{}\) as a utility function may usually lead to a violation of rationality. This violation occurs when the marginal performance gain from additional clients decreases as the coalition size increases, as observed in . Consequently, the expected value of the marginal gain becomes smaller than the individual value, thereby violating individual rationality , which states that \(c_{i} u(\{i\})\), where \(u()\) denotes a utility function. To illustrate this issue, we provide an example below:

**Example 4.1**.: Given two clients, each client's local dataset allows them to achieve model accuracy of \(0.8\) and \(0.6\), respectively. However, when collaborating in a federated learning setting, they can attain a model performance of \(0.9\). The Shapley value of each participant can be determined by using the model performance as the utility function, resulting in Shapley values of them becoming \(0.55\) and \(0.35\). Thus, their payoffs are lower with cooperation.

Despite the seemingly irrational nature of the decision, data owners in real-world scenarios demonstrate a willingness to participate in the federation in the above example due to the higher level of user satisfaction achieved by a model with an accuracy of 0.9. Relying solely on model performance as the utility function fails to adequately capture the underlying user satisfaction. Based on this observation, we rectify the conventional utility function with a logistic function , representing the percentage of users satisfied with the model performance.

\[V()=(S)-T)}}\] (11)

where \(k\) denotes the logistic growth rate, which determines the steepness of the curve and \(T\) represents the threshold, where half of the population is satisfied. The choice of \(T\) should be greater than \(_{i}V^{}(\{i\})\) to encourage the cooperation of participants because the convexity of the left-sided logistic function would up-weight the utility value of stronger coalitions. When \(k\) is set to infinity, the individual rationality would be satisfied since the \(V(\{i\})\) is compressed to zero and participant contribution is lower-bounded by zero. 1

Finally, the contribution of \(i\)-th participant can be calculated by Shapley value defined in Equation 4. The proposed rectification of the utility function with the logistic function makes the utility value more sparse because the utility of the coalition whose performance does not surpass the threshold \(T\) would be highly compressed. Inspired by , we propose a pruning technique that leverages the sparsity to further accelerate the computation of the Shapley value. We set a pruning threshold \(\), such that given a coalition \(\) with \(V()\), we prune the computation of subsets of \(\). 2

### Complexity Analysis

Table 1 presents a comparative analysis of SPACE's computational complexity in contrast to established methods. The sampling-based approximation of the Shapley value, applied in TMC-Shapley, GTG-Shapley, and SPACE, necessitates sampling complexity of \(n n\), as studied in . It is worth noting that SPACE achieves a notably reduced computational complexity in calculating the Shapley value at merely \(O(n n||)\), which represents a substantial reduction compared to the \(O(n n R_{g}|_{}|)\) required by other Shapley value-based approaches. This efficiency gain is attributed to the use of the single-round amalgamation and the prototype-based model evaluation. The prototype-based model evaluation eliminates the necessity for repeated iteration over the validation set. Consequently, the computation of the Shapley value is facilitated by direct comparisons of

distances between the weighted sum of prototypes. Moreover, by employing the knowledge amalgamation technique, SPACE is capable of evaluating participant contributions with a single communication round, thereby significantly alleviating the communication and computational load on clients with limited resources by a factor of \(R_{g}\). This acceleration does introduce an additional computation load for knowledge amalgamation on the server, with a complexity of \(O(n n R_{a}|_{}|)\). However, this trade-off is generally more suitable for generic federated learning frameworks, given that the server typically possesses superior computational resources compared to the clients.

## 5 Experiment

In this section, we demonstrate the effectiveness of the proposed SPACE method for evaluating participant contributions. We compare it with previous approaches and show that SPACE achieves superior computational efficiency without compromising performance. We also evaluate the SPACE on two federated-learning tasks, client reweighting, and client selection.

### Experimental Setup

**Datasets and Non-IID Setting.** Following , we conduct experiments on the widely adopted image dataset _MNIST_ and _CIFAR10_. For a fair comparison, we adopted the experimental settings used in the HFL framework  with two different scenarios: mislabeled data and non-IID data. In the mislabeled data scenario, we deliberately introduced label corruption in a variable proportion of the total clients. Specifically, we corrupted different percentages of clients, ranging from 0% to 80%. Within each client, we intentionally corrupted \(50\%\) of the local training labels to simulate the presence of mislabeled data. For the non-IID scenario, we selected a subset of clients, ranging from \(0\%\) to \(80\%\), and assigned them local datasets containing incomplete categories. We set the total number of clients to \(10\) for the MNIST dataset and \(5\) for the CIFAR10 dataset.

**Baselines and Implementation.** We compare SPACE with four state-of-the are methods, including _i) GT-Shapley_,_ii)__TMC-Shapley_, _iii)__GTG-Shapley_ and _iv)__DIG-FL_. 3 We also compare a variant of SPACE, _v)__SPACE(Avg)_, which applies FedAvg instead of knowledge amalgamation for aggregation. For adjusting the utility function, we empirically set \(k\) as 100 while \(T\) as 0.95 and 0.5 for evaluation on MNIST and CIFAR10.

**Evaluation Metric.** The performance of all approaches is assessed from two perspectives: accuracy and efficiency. To evaluate the accuracy, we employ Pearson's Correlation Coefficient to measure the correlation between the estimated and actual Shapley values. The actual Shapley values are obtained by performing retraining for a total of \(2^{n}\) times. In terms of efficiency, we consider two vital metrics. Execution time (in seconds) is measured on a single V100 GPU without parallel training to assess the time efficiency of the approaches. Communication cost (in megabytes) is calculated according to the formula \(Comm=2 n R_{g}||\), where \(R_{g}\) denotes the number of communication rounds, \(||\) represents the number of model parameters(MB) and constant of 2 stands for the upload and download of the models.

  Method & Communication Cost & Client Computation Cost & Server Computation Cost \\  Real Shapley & \(2^{n} R_{g}||\) & \(2^{n} R_{g} R_{l}|_{i}|\) & \(2^{n}|_{}|\) \\ GT-Shapley & \(n( n)^{2} R_{g}||\) & \(n( n)^{2} R_{g} R_{l}|_{i}|\) & \(n( n)^{2}|_{}|\) \\ TMC-Shapley & \(n n R_{g}||\) & \(n n R_{g} R_{l}|_{i}|\) & \(n n|_{}|\) \\ GTG-Shapley & \(R_{g}||\) & \(R_{g} R_{l}|_{i}|\) & \(n n R_{g}|_{}|\) \\ DIG-FL & \(R_{g}||\) & \(R_{g} R_{l}|_{i}|\) & \(}|_{}|\) \\ SPACE(Ours) & \(||+|}||}|\) & \(}|_{i}}|\) & \(n n R_{a}|_{}|\) \\  

Table 1: Comparison of complexity between previous works. \(R_{l}\), \(R_{g}\), and \(R_{a}\) denote the number of iterations for local training, global communication rounds, and knowledge amalgamation. \(||\) represents the number of model parameters, \(||\) denotes the dimension of prototypes, and \(||\) indicates the number of classes. \(|_{i}|\) and \(|_{}|\) represent the sizes of the local dataset and the validation set.

### Quantitative Results

**Participant contribution.** Table 2 presents the outcomes of participant contribution evaluation. The proposed SPACE approach demonstrates outstanding efficiency and achieves the highest Pearson correlation coefficient (PCC) in scenarios involving mislabeled data. In terms of PCC, SPACE performs similarly to SPACE(Avg), but with superior efficiency resulting from the single-round knowledge amalgamation. The communication and computational costs associated with GT-Shapley and TMC-Shapley are excessively high due to the requirement for model retraining. However, TMC-Shapley shows improved efficiency by attaining high PCC results with fewer sampled permutations. GTG-Shapley adopts a reconstruction technique to expedite the computation process, which effectively reduces the required communication cost, yet the testing time in GTG-Shapley remains considerably long. The resource-saving version of DIG-FL achieves comparable computational efficiency to SPACE. Nevertheless, it has a tendency to overestimate the contribution of clients with incomplete categories and exhibits inferior PCC results in non-IID scenarios.

**Client Reweighting.** In non-IID scenarios or when some clients have erroneous data, aggregating local models based on data ratio may lead to suboptimal performance. Leveraging participant contribution as a weighting factor can enhance the robustness of federated learning . We propose two reweighting mechanisms, namely static and dynamic, with the former employing participant contribution calculated using the knowledge amalgamated model and the latter recalculating participant contribution for each communication round using the current model. Figure 2 shows the reweighting results. Despite the dynamic approach introducing additional computational overhead, it yields improved performance. The static and dynamic approaches achieve accuracy improvements of 2.53% and 2.69%, respectively, under the non-IID scenario, and 3.21% and 3.52%, respectively, under the mislabel scenario, for the CIFAR10 dataset. The DIG-FL approach measures client contribution in federated learning by the similarities between the gradients of the local datasets and the gradients of the validation set. However, this approach disregards clients in communication rounds where the two gradients diverge, leading to significant variances in the federated training process.

**Client Selection.** Client selection is a critical issue when dealing with a large number of clients. Li et al.  proposed a client sampling approach based on the multinomial distribution (MD) that ensures zero bias but has significant variances. To tackle this issue, Fraboni et al.  suggested clustered sampling (CS), which has been theoretically shown to reduce variance. They proposed two CS algorithms: Algorithm 1 clusters clients based on data ratio, and Algorithm 2 incorporates the similarities between local and global models. Algorithm 2 outperforms both MD and Algorithm 1. However, Algorithm 2's time complexity is \(O(R_{g}(n^{2}||+X))\), where \(||\) is the number of model parameters and \(O(X)\) is the complexity of the clustering algorithm. This computational cost

  Dataset & Scenario & GT & TMC & GTG & DIG-FL & SPACE(Avg) & SPACE \\   & Non-IID & 0.6877 & **0.9824** & 0.9287 & 0.8715 & 0.9713 & 0.9448 \\  & Mislabel & 0.4230 & 0.9507 & 0.9608 & 0.9580 & 0.9529 & **0.9612** \\  & Time(s) & 97137 & 84796 & 62473 & 315 & 294 & **160** \\  & Comm(MB) & 11813.12 & 10292.48 & 35.2 & 35.2 & 35.2 & **1.76** \\   & Non-IID & 0.6089 & 0.8877 & 0.8208 & 0.7546 & **0.9540** & 0.9290 \\  & Mislabel & 0.5192 & 0.9595 & 0.4148 & 0.9598 & 0.9565 & **0.9641** \\  & Time(s) & 7468 & 4950 & 835 & 536 & 315 & **295** \\  & Comm(MB) & 307800 & 202920 & 11400 & 11400 & 11400 & **570** \\  

Table 2: Results of participant contribution evaluation.

Figure 2: The effect of client reweighting on model performance.

results from updating a similarity matrix and re-clustering at each communication round. We propose a method that combines SPACE with CS. We utilize the prototypes obtained from SPACE to construct the similarity matrix, eliminating the need for repeated updates. Additionally, we enhance the weighting by incorporating the estimated contribution, resulting in a new weighting scheme for client \(p_{i}\) given by \((1-)p_{i}+ c_{i}\), where \(\) is a hyper-parameter that balances the two terms. As in , the CIFAR10 dataset is distributed among 100 clients using a Dirichlet distribution with \(\{10,0.001\}\), where smaller \(\) implies larger heterogeneity. We set \(=0\) and \(=0.5\) when \(=10\) and \(=0.001\), respectively. Figure 3 presents the results of client selection. Our approach, which combines prototypes and participant contribution, outperforms prior methods, indicating the superiority of our proposed method. This improvement can be attributed to the fact that prototypes enhance the accuracy of the clustering process. Furthermore, as discussed in Section 5.2, the incorporation of participant contribution has proven to be useful in enhancing the performance of federated learning, especially in non-IID settings.

## 6 Conclusion

We propose Single-round Participant Amalgamation for Contribution Evaluation (SPACE), an efficient approach for evaluating participant contribution in federated learning. The novelty of SPACE lies in two key components: Federated Knowledge Amalgamation and Prototype-based Model Evaluation. By leveraging a single communication round, Federated Knowledge Amalgamation constructs a robust embedding space, while Prototype-based Model Evaluation reduces the complexity associated with the validation set size. Our experimental results demonstrate that SPACE outperforms existing methods in contribution evaluation and shows versatility in client reweighting and selection. However, efficient client selection for amalgamation and the impact of the validation set quality are areas that require further exploration in future research.

## Broader Impact

In our study, we highlight the importance of participant contribution through three applications: incentive mechanisms, client reweighting, and client selection. Furthermore, the potential applications extend to client debugging and attack detection. Since the computational complexity associated with participant contribution hampers its widespread implementation, we propose a novel FL framework, SPACE, to alleviate this issue and pave the way for broader adoption of participant contribution. An efficient algorithm for evaluating participants provides decision-makers with valuable insights before initiating extensive federated learning processes, which can be both computationally and temporally intensive. Furthermore, rapid evaluation offers enhanced flexibility when dealing with dynamic cooperative entities, facilitating quick adaptation to new collaborations--a capability that previous methods lack. Our proposed approach, SPACE, significantly contributes to these applications. However, our framework relies on the assumption of having a proper and reliable validation set with accurately labeled data. This prerequisite can be particularly challenging to fulfill in real-world settings and thus limits the application to specific scenarios characterized by significant commercial incentives [33; 65]. Exploring methods to decrease dependence on the validation set while maintaining functionality, even when erroneous data impact many clients, presents an intriguing direction for future exploration. We defer this critical aspect to be addressed in future research efforts.

Figure 3: The effect of client selection on model performance.