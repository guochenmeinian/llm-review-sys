# LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference

Hongwu Peng\({}^{1}\)

\({}^{*}\)Ran Ran\({}^{2}\)

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

Ran Ran\({}^{2}\)

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

Yukui Luo\({}^{3}\)

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

Jiah Zhao\({}^{1}\)

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

Shaoyi Huang\({}^{1}\)

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

Kiran Thorat\({}^{1}\)

\({}^{*}\)These authors contributed equally.

\({}^{1}\)University of Connecticut, \({}^{2}\)North Carolina State University,

\({}^{3}\) University of Massachusetts Dartmouth, \({}^{4}\)University of Rochester,

\({}^{5}\)Indiana University Bloomington, \({}^{6}\)Northeastern University

{hongwu.peng, jiahui.zhao, shaoyi.huang, kiran.gautam.thorat}@uconn.edu,

{rran, wwen2}@ncsu.edu, yluo2@umassd.edu, tgeng@ur.rochester.edu

cw166@iu.edu, x.xu@northeastern.edu, caiwen.ding@uconn.edu

###### Abstract

The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth. (2) A compact node-wise polynomial replacement policy with a second-order trainable activation function, steered towards superior convergence by a two-level distillation approach from an all-ReLU based teacher model. (3) an enhanced HE solution that enables finer-grained operator fusion for node-wise activation functions, further reducing multiplication level consumption in HE-based inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that LinGCN excels in latency, accuracy, and scalability for homomorphically encrypted inference, outperforming solutions such as CryptoGCN. Remarkably, LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving an inference accuracy of ~75% and notably reducing multiplication depth. Additionally, LinGCN proves scalable for larger models, delivering a substantial 85.78% accuracy with 6371s latency, a 10.47% accuracy improvement over CryptoGCN. The codes are shared on Github1.

## 1 Introduction

Graph learning, a deep learning subset, aids real-time decision-making and impacts diverse applications like computer vision , traffic forecasting , action recognition , recommendation systems , and drug discovery . However, the growth of Graph Convolution Network (GCN)model sizes  introduces challenges in integrating encryption into graph-based machine learning services. Platforms such as Pinterest's PinSAGE  and Alibaba's AliGraph  operate on extensive user/item data graphs, increasing processing time. Furthermore, deploying GCN-based services in the cloud raises privacy concerns  due to potential adversarial attacks on client data, such as gradient inversion attacks . Given the sensitive information in graph embeddings, it's crucial to implement lightweight, privacy-focused strategies for cloud-based GCN services.

Privacy-Preserving Machine Learning (PPML) with Homomorphic Encryption (HE) helps protect sensitive graph embeddings, allowing client data encryption before transmission and enabling direct data processing by the cloud server. However, this security comes with substantial computational overhead from HE operations such as rotations, multiplications, and additions. For instance, using the Cheon-Kim-Kim-Song (KKKS) , a Leveled HE (LHE)  scheme, the computational overhead escalates significantly (e.g. \( 3\)) with the total number of successive multiplications, which is directly related to the layers and non-linear implementations in deep GCNs.

Three research trends have emerged to lessen computational overhead and inference latency for HE inference. First, _leveraging the graph sparsity_, as in CryptoGCN , reduces multiplication level consumption. Second, _model quantization_, seen in TAPAS  and DiNN , binarizes activation and weight to 1 bit, leading to a 3%-6.2% accuracy loss on smaller datasets and limited scalability. Third, _nonlinear operation level reduction_, as in CryptoNet  and Lola , substitutes low-degree polynomial operators like square for ReLU in CNN-based models. However, these methods struggle with performance due to absent encryption context of graph features, and the latter two are not suitable for GCN-based models.

To bridge the research gap, in this paper, we propose LinGCN, an end-to-end policy-guided framework for non-linear reduction and polynomial replacement. This framework is designed to optimize deep Spatial-Temporal Graph Convolution Network (STGCN)-based models for HE-based inference. We conduct extensive experiments on the NTU-XVIEW  skeleton joint  dataset and compare our approach with CryptoGCN . The pareto frontier comparison of accuracy-latency between LinGCN and CryptoGCN  demonstrates that LinGCN surpasses CryptoGCN  by over 5% accuracy under the same latency budget, and achieves a 14.2-fold latency reduction under the same accuracy constraint (~75% accuracy).

Our proposed framework, LinGCN, is principally influenced by two observations: the need to conserve levels in the CKKS scheme and the crucial role of synchronized linearization. In the CKKS scheme, deeper networks demand a larger polynomial degree, thereby increasing the latency of homomorphically encrypted operators. Non-linear layer pruning can mitigate this by reducing level consumption. However, unstructured non-linear pruning is insufficient; structural linearization becomes essential for effective level reduction, underscoring synchronized linearization's significance in optimizing CKKS performance. We summarize our contributions as follows:

1. We present a **differentiable structural linearization** algorithm, paired with a parameterized discrete indicator function, which guides the structural polarization process. This methodology is co-trained with model weights until the optimization objective is attained. Our approach facilitates fine-grained node-level selection of non-linear locations, ultimately yielding a model with reduced multiplication levels.
2. We introduce a node-wise polynomial replacement policy utilizing a second-order trainable polynomial activation function. This process is further steered by an all-ReLU based teacher model employing a two-level distillation approach, thus promoting superior convergence.
3. We have engineered a corresponding HE solution that facilitates finer-grained operator fusion for node-wise activation functions. This development further mitigates multiplication level consumption during our HE-based inference process.

Figure 1: Frontier of LinGCN vs. CryptoGCN 

Background and Related Work

**CKKS Homomorphic Encryption Scheme.** The CKKS scheme , based on the ring learning with errors (RLWE) problem, allows arithmetic operations on encrypted fixed-point numbers. It provides configurable precision via encryption noise as natural error \(e\). We denote the cyclotomic polynomial degree as \(N\) and the polynomial coefficient modulus as \(Q\), both cryptographic parameters. A ciphertext \(ct\) encrypts a message \(m\) with noise \(e\) as \(ct=m+e\) (mod \(Q\)). The CKKS scheme's security level , measured in bits, suggests \(2^{128}\) operations to break a \(=128\) encryption. CKKS supports operations including ciphertext addition _Add(\(ct_{1},ct_{2}\))_, ciphertext multiplication _CMult(\(ct_{1},ct_{2}\))_, scalar multiplication _PMult(\(ct\), pt)_, rotation _Rot(\(ct\), k)_ and rescaling _Rescale(\(ct\))_. Scalar multiplication multiplies a ciphertext with plaintext, and rotation cyclically shifts the slot vector. For example, \(Rot(ct,k)\) transforms \((v_{0},...,v_{N/2-1})\) into \((v_{k},...,v_{N/2-1},v_{0},...,v_{k-1})\). The level of a ciphertext (\(L\)), the number of successive multiplications it can undergo without bootstrapping, is reduced by the \(Rescale\) operation after each multiplication. With the level-reduced target network, we do not consider using bootstrapping  in this work.

**STGCN for Graph Dataset with Timing Sequence.** The focus of this study revolves around the STGCN, a highly esteemed category of GCNs proposed by . The STGCN model primarily employs two core operators, Spatial graph convolution (GCNConv) and Temporal convolution, designed to extract spatial and temporal information from input graph data, respectively. The Spatial convolution operation, as defined in Equation 1, precisely illustrates the process of extracting spatial information. In Equation 1, the variables \(\), \(\), \(_{i}\), \(_{i}\), and \(_{i,}\) correspond to the adjacent matrix, degree matrix, input feature, weight parameter, and output feature, respectively .

\[_{i,out}=^{-}(+)^{-}_{i}_{i}\] (1)

**Related Work.** CryptoNets  pioneered Privacy-Preserving Machine Learning (PPML) via Homomorphic Encryption (HE) but suffered from extended inference latency for large models. Subsequent studies, like SHE  which realizes realize the TFHE-based PPML, and those employing Multi-Party Computation (MPC) solutions[29; 30; 31], reduced latency but faced high communication overhead and prediction latency. Research like TAPAS  and XONN  compressed models into binary neural network formats, while studies like CryptoNAS , Sphynx , and SafeNet  used Neural Architecture Search (NAS) to find optimal architectures. Delphi , SNL , and DeepReduce  replaced ReLU functions with low order polynomial or linear functions to accelerate MPC-based Private Inference (PI). Nevertheless, these strategies proved suboptimal for homomorphic encrypted inference. Solutions like LoLa , CHET , and HEAR  sought to use ciphertext packing techniques but were not optimized for GCN-based models.

**Threat Model.** In line with other studies [12; 37; 39], we presume a semi-honest cloud-based machine learning service provider. The well-trained STGCN model's weight, including adjacency matrices, is encoded as plaintexts in the HE-inference process. Clients encrypt data via HE and upload it to the cloud for privacy-preserving inference service. The server completes encrypted inference without data decryption or accessing the client's private key. Finally, clients decrypt the returned encrypted inference results from the cloud using their private keys.

## 3 LinGCN Framework

### Motivation

We present two pivotal observations that serve as the foundation for our LinGCN.

**CKKS Based Encrypted Multiplication.** As described in section 2, the message \(m\) is scaled by a factor \(=2^{p}\) before encoding, resulting in \( m\). Any multiplicative operations square the ciphertext's scale, including inherent noise \(e\). The \(Rescale\) operation reduces this scale back to \(\) by modswitch , reducing \(Q\) by \(p\) bits and lowering the level, as depicted in Figure 2. When \(Q q_{o}\) after \(L\) multiplications and rescaling, the ciphertext's multiplicative level reaches 0. At this stage, bootstrapping  is necessary for further multiplication, unless a higher \(Q\) is used to increase the multiplicative level \(L\).

**Observation 1: Saving Level is Important in CKKS Scheme.** Within a STGCN model, the graph embedding is encrypted with an initial polynomial degree, denoted as \(N\), and a coefficient modulus,represented by \(Q\). These factors are determined by the network's multiplicative depth. As the multiplication depth increases, the coefficient modulus must also increase, subsequently leading to a higher polynomial degree \(N\). To maintain a security level above a certain threshold (for example, 128-bit) in the face of deeper multiplicative depths, a larger \(N\) is necessitated [37; 38; 12]. A larger \(N\) inherently results in increased complexity in each homomorphic encryption operation such as Rot and CMult. These operations are integral to the computation of convolution (Conv) and graph convolution (GCNConv). We present a comparative analysis of operator latency with differing polynomial degrees \(N\) in Figure 2. Thus, it can be inferred that pruning certain operators to conserve levels not only diminishes the latency of the pruned operators but also reduces other operators' latency.

**Observation 2: Synchronized Linearization Matters.** Our study mainly targets reducing multiplication depth in standard STGCN layers, comprising an initial spatial GCNConv operation, two non-linear operators, and a temporal convolution. Under the CKKS base HE scheme, each operator's input needs synchronized multiplication depths, with any misalignment adjusted to the minimal depth. This is particularly vital for the GCNConv operator, while later node-wise separable operators enable structured non-linear reduction to boost computational efficiency.

Unstructured non-linear reduction strategies, common in MPC setups like AutoReP , PASNet  SNL , RRNet , DELPHI , and SAFENet , prove suboptimal for homomorphic encrypted inference. Our example in Figure 3(b) illustrates node-wise unstructured non-linear reduction in an STGCN layer, leading to unsynchronized remaining multiplication depth budgets for nodes after the layer. This approach is ineffective in reducing levels consumption for homomorphic encrypted inference, emphasizing the necessity for a more structured non-linear reduction approach. To address the limitations of unstructured non-linear reduction, we propose a structured approach, illustrated in Figure 3(c), where we apply an equal level of non-linear reduction to each target node, reducing the overall multiplication depth. In contrast to CryptoGCN's layer-wise pruning method , our scheme provides nodes with the flexibility to perform non-linear operations at preferred positions. This fine-grained, structural non-linear reduction may improve the non-linear reduced structure, potentially augmenting the efficiency and effectiveness of HE inference.

### Learning for Structural Linearization

**Problem Formulation.** Our methodology is aimed at the node-wise structural dropping of non-linear operators within a given \(L\)-layered STGCN model \(f_{W}\), parameterized by \(W=\{W_{i}\}_{i=0}^{2L}\). This model maps the input \(X_{0} R^{V C T}\) to the target \(Y R^{d}\). Here, \(V\), \(C\), and \(T\) represent the number of nodes, channels, and frames, respectively. The primary objective of our approach is to eliminate the non-linear operator (designated as \(_{n}\)) in a structured, node-wise manner. The ultimate aim is to achieve a reduced multiplication level while minimizing any resultant drop in model accuracy. This goal serves to optimize the efficiency of homomorphic encrypted inference for STGCN model.

We introduce an indicator parameter, denoted as \(h\), to mark the node-wise non-linear operator dropping. The following conditions apply: (1) If \(h_{i,k}=0\), the non-linear operator \(_{n}\) is substituted by the linear function \(f(x)=x\); (2) If \(h_{i,k}=1\), the operator \(_{n}\) is utilized. Here, \(h_{i,k}\) signifies

Figure 3: Unstructured vs. structural linearization. Unstructured one doesn’t lead to effective level reduction.

Figure 2: Top: Rescale decreases the ciphertext level. Bottom: Higher polynomial degree leads to longer HE operator’s latency.

the \(k_{th}\) node of the \(i_{th}\) non-linear layer. The proposed node-wise non-linear indicator parameter \(h_{i,k}\) facilitates the expression of the \(i_{th}\) non-linear layer with partial linearization, as given by \(X_{i,k}=h_{i,k}_{n}(Z_{(i-1),k})+(1-h_{i,k}) Z_{(i-1),k}\). Here, \(Z_{i-1}\) denotes the input of the \(i_{th}\) non-linear layer. Consequently, the issue of structural linearization can be formulated as follows:

\[}=}\ \ _{acc}(f_{W}(X_{0}),Y)+_{i=1}^{2L}||h_{i}||_{0}\] (2) \[ j,k[1,V],\ (h_{2i,j}+h_{2i+1,j})=(h_{2i,k}+h_{2i+1,k})\]

In our formulation, \(||h_{r}||0\) represents the \(L_{0}\) norm or the count of remaining non-linear operators. \(acc\) signifies cross-entropy loss, while \(\) is the \(L_{0}\) norm penalty term in the linearization process. The second term of Eq. 2 poses a challenge due to its non-differentiability, stemming from zero norm regularization and additional constraints on the discrete indicator parameter \(h_{i}\). Hence, this issue becomes difficult to handle using traditional gradient-based methods.

**Differentiable Structural Linearization.** To handle the non-differentiable discrete indicator parameter, we introduce a trainable auxiliary parameter, \(h_{w}\), to denote its relative importance. However, the transformation from \(h_{w}\) to the final indicator \(h\) must comply with the structural constraint in Eq.2, ruling out simple threshold checks used in SNL . Structural pruning methods , useful for weight or neuron pruning, are ill-suited for our problem due to its exponential complexity arising from 2 non-linear selections of 25 nodes, and its distinct nature from weight or neuron pruning.

To tackle this challenge, we propose a structural polarization forward process, detailed in Algorithm 1. The algorithm first ranks the relative importance of two auxiliary parameters (\(h_{w(2i,j)}\) and \(h_{w(2i+1,j)}\)) for every \(j_{th}\) node within the \(i_{th}\) STGCN layer, obtaining the higher and lower values and their respective indices. We then sum each \(i_{th}\) layer's higher auxiliary parameter value into \(s_{h}\), and lower auxiliary parameter value into \(s_{l}\). Next, we conduct a threshold check of \(s_{h}\) and assign the final polarization output of the indicator value into the corresponding locations of higher value indices, applying the same polarization for lower values. The proposed structural polarization enforces the exact constraint as given in Eq. 2, where each STGCN layer maintains a synchronized non-linear count across all nodes. The structural polarization is also capable of capturing the relative importance of the auxiliary parameter within each node and applying the proper polarization to every position, all with a complexity of only \(O(V)\). Each STGCN layer retains the flexibility to choose all positions to conduct the non-linear operation, or just select one position for each node to conduct the non-linear operation, or opt not to perform the non-linear operation at all. This selection is based on the importance of their auxiliary parameters.

```
0:\(h_{w}\): auxiliary parameter
0:\(h\): final indicator
1:for\(i=0\) to \(L\)do
2:\(s_{h},s_{l}=0\) and \(ind_{h},ind_{l}\)
3:for\(j=1\) to \(V\)do
4:if\(h_{w(2i,j)}>h_{w(2i+1,j)}\)then
5:\(s_{h}\)\(h_{w(2i,j)}\), \(s_{l}+=\)\(h_{w(2i+1,j)}\)
6:\(ind_{h}(2i,j)\), \(ind_{l}(2i+1,j)\)
7:else
8:\(s_{h}\)\(+=\)\(h_{w(2i+1,j)},s_{l}+=\)\(h_{w(2i,j)}\)
9:\(ind_{h}(2i+1,j)\), \(ind_{l}(2i,j)\)
10:endif
11:endfor
12:\(h_{ind_{h}}=s_{h}>0\) and \(h_{ind_{l}}=s_{l}>0\)
13:endfor ```

**Algorithm 1** Structural Polarization.

Despite its benefits, the structural binarization in Algorithm 1 is discontinuous and non-differentiable. To approximate its gradient, we use coarse gradients  via straight through estimation (STE), a good fit for updating \(h_{w}\) and reflecting the optimization objective in Eq.2. Among various methods such as Linear STE, ReLU and clip ReLU STE , we adopt Softplus STE  due to its smoothness and recoverability discovered in , to update \(h_{w}\) as follows:

\[}{ h_{w(i,k)}}=_{acc }}{ X_{i,k}}(_{n}(Z_{i-1})-Z_{i-1})}{  h_{w(i,k)}}+}{ h_{w,(i,k)}},\ \ }{ h_{w(i,k)}}=Softplus(h_{w(i,k)})\] (3)

The gradient of the auxiliary parameter, as displayed in Eq. 3, consists of two components: the gradient from the accuracy constraint and the gradient from the linearization ratio constraint. The gradient stemming from the accuracy constraint will be negative if it attempts to counteract the linearization process, thereby achieving a balance with the \(L_{0}\) penalty term \(_{i=1}^{2L}||h_{i}||_{0}\). The entirety of the linearization process is recoverable, which allows for the exploration of a near-optimal balance between accuracy and linearization ratio via the back-propagation process.

### Learnable Polynomial Replacement with Teacher Guidance

CryptoNet  and DELPHI  suggest using quadratic functions \(y=x^{2}\) as ReLU replacements for private inference. A second-order polynomial replacement \(y=x^{2}+x\) for ReLU was proposed but resulted in decreased performance. CryptoGCN  uses a layer-wise trainable polynomial function \((ax^{2}+bx+c)\) for the STGCN model, but it suffers from significant accuracy loss and scalability problems. To tackle these issues, we offer a two-tiered strategy: node-wise trainable polynomial replacement and a distillation technique to facilitate the convergence of replacement.

**Node-wise Trainable Polynomial Replacement.** In our HE setup (see Figure 4), each node can independently perform non-linear, convolution, and further non-linear operations without increasing the multiplication depth. We suggest using a node-wise trainable polynomial function as the non-linear function (see Eq. 4). To prevent gradient explosion from quadratic terms, we include a small constant \(c\) to adjust the gradient scale of the \(w_{2}\) parameter .

\[_{n}(x)=c w_{2}x^{2}+w_{1}x+b\] (4)

**Improved Convergence Through Distillation.** Training a polynomial neural network from scratch often overfits, leading to accuracy loss [50; 12]. To counteract this, we recommend initially training a baseline ReLU model as the teacher, then transferring its knowledge to the polynomial student model to less accuracy degradation. The student model's parameters are initialized from the teacher's and polynomial function parameters start at \((w_{2}=0,w_{1}=1,b=0)\). To aid knowledge transfer, we use KL-divergence loss  and peer-wise normalized feature map difference penalty. The loss function for polynomial replacement is as follows:

\[_{p}=(1-)_{CE}(f_{W,s}(X_{0}),Y)+_{ KL}(f_{W,s}(X_{0}),f_{W,t}(X_{0}))+_{i=1}^{L}MSE( }{||X_{i,s}||_{2}},}{||X_{i,t}||_{2}})\] (5)

In the above equation, \(\) is employed to balance the significance between the CE loss and the KL divergence loss components, while \(\) acts as the penalty weight for the peer-wise feature map normalized \(L_{2}\) distance term, analogous to the approach in .

### Put it Together

The comprehensive workflow of our proposed LinGCN framework is outlined in Algorithm 2. Initially, we construct the student model, \(M_{S}\), utilizing the parameters of the teacher model, \(M_{T}\), and subsequently initialize the indicator auxiliary parameter \(h_{w}\) for \(M_{S}\). Following this, \(M_{S}\) undergoes training iterations involving updates for structural linearization. Upon

Figure 4: Overview of single layer STGCN with non-linear reduction and computation.

achieving convergence, the indicator function \(h\) is fixed, and the ReLU function in \(M_{S}\) is replaced with a polynomial function. Subsequently, we initialize \(w_{poly}\) and train the final model using a combination of CE loss and the two-level distillation loss as defined in Eq. 5. The resulting model output will feature structurally pruned node-wise polynomial functions, rendering it primed for the final stage of homomorphic encryption-based inference.

**Further Operator Fusion to Save Level.** Figure 4 shows an example of the single-layer STGCN structure generated by our LinGCN framework. The STGCN utilizes both input nodes information and aggregates them to generate output node features. We adopt the same AMA format  for GCNConv computation, as such, we are able to fuse the plaintext weights \(c w_{2}\) of node-wise polynomial function into GCNConv layer and save one multiplication depth budget. Layer operators involve another temporal convolution layer and non-linear/linear layer, we can still fuse the plaintext weights \(c w_{2}\) from the second polynomial function into the convolution operator and save the multiplication depth budget. As such, the ReLU-reduced example shown in Figure 4 only consumes 3 multiplication level of ciphertext as opposed to the original 4 multiplication depth budget. For the convolution operator, we adopt the same computation flow from [12; 38].

## 4 Experiment

### Experiment Setting

**HE Parameter Setting.** We performed experiments with two sets of encryption parameters: one without and one with non-linear reduction. We used a scaling factor \(=2^{33}\) in both cases to ensure comparable accuracy. This scaling consumes 33 bits from the ciphertext modulus \(Q\) per level. Without non-linear reduction, a 3-layer STGCN network needs 14 levels, and a 6-layer network needs 27. For a security level of 128-bit, we set \(Q\) to 509 (932) and the polynomial degree \(N\) to \(2^{15}(2^{16})\) for 3 (6) layers STGCN network. In the non-linear reduction case, the total level count ranges from 14 to 9 for 3-layer, and 27 to 16 for 6-layer networks. With the reduced models, we can use smaller Q and \(N\) while maintaining 128-bit security. Specifically, we set \(Q\) to 410 to 344 and \(N\) to \(2^{14}\) for the 3-layer network (from 3 non-linear reduced), and \(Q\) to 767 to 569 and \(N\) to \(2^{15}\) for the 6-layer network (starting from 5 non-linear reduced).

**Experiment Dataset.** The **NTU-RGB+D** dataset , the largest available with 3D joint annotations for human action recognition, is utilized in our study. With 56,880 action clips in 60 classes, each is annotated with \((X,Y,Z)\) coordinates of 25 joints (nodes) per subject. We chose the NTU-cross-View (NTU-XView) benchmark given its representativeness as a human skeleton joint dataset, containing 37,920 training and 18,960 evaluation clips. For a thorough evaluation, 256 frames were used from each clip, resulting in a \(2 3 256 25\) input tensor for two individuals each with 3 channels.

**Baseline Model.** Our experiment runs on 8*A100 GPU server, and uses the state-of-the-art STGCN architecture for human action recognition, which combines GCN and CNN. We tested three configurations: STGCN-3-128, STGCN-3-256, and STGCN-6-256, where the first number represents the layer count, and the second the last STGCN layer's channel count. These networks contain a stack of three or six STGCN layers, followed by a global average pooling layer and a fully-connected layer. They were trained for 80 epochs using SGD, with a mini-batch size of 64, a momentum of 0.9, weight decay of \(10^{-4}\), and dropout 0.5. The initial learning rate (LR) was 0.1, with a decay factor of 0.1 at the 10th and 50th epochs. The baseline accuracy of models can also be found in Table 1.

**LinGCN Algorithm Setting.** In our LinGCN algorithm, we used the baseline model from Table 1 as the teacher model in Algorithm 2. For structural linearization, we used SGD optimizer with learning rate (LR) 0.01 for both weight and auxiliary parameters. The \(L_{0}\) norm penalty \(\) was varied from 0.1to 10 for desired linearized layers, over 25 epochs. Then, ReLU was replaced with a second-order polynomial in the polynomial replacement iterations. We set scaling factor \(c\) at 0.01, parameters \(\) and \(\) at 0.2 and 200 respectively. This process took 90 epochs, with SGD optimizer and initial LR of 0.01, decaying by a factor of 0.1 at the \(40th\) and \(80th\) epochs.

**Private Inference Setup.** Our experiments used an AMD Ryzen Threadripper PRO 3975WX machine, single threaded. The RNS-variant of the CKKS scheme  was employed using Microsoft SEAL version 3.7.2 . We adopted the Adjacency Matrix-Aware (AMA) data formatting from CryptoGCN  to pack input graph data and performed GCNConv layer inference via multiplying with sparse matrices \(A_{i}\) split from the adjacency matrix \(A\). The temporal convolution operation was optimized using the effective convolution algorithm from [38; 12; 55]. The computation for the global average pooling layer and fully-connected layer also follows the SOTA [38; 12].

### Experiment Result and Comparison

**LinGCN Outperforms CryptoGCN .** The proposed **LinGCN** algorithm demonstrates superior performance compared to CryptoGCN . The STGCN-3-128 and STGCN-3-256 models, which are the basis of this comparison, share the same backbone as those evaluated in CryptoGCN . Hence, we have undertaken a comprehensive cross-work comparison between the LinGCN framework and CryptoGCN . The latter employs a heuristic algorithm for layer-wise activation layer pruning, which necessitates a sensitivity ranking across all layers. This approach, however, proves to be ineffective, leading to significant accuracy drops as the number of pruned activation layers increases. Comparative results between LinGCN and CryptoGCN  are provided in Table 2 and Table 3. In the context of LinGCN, the number of non-linear layers presented in the table represents the effective non-linear layers post-structural linearization. For the STGCN-3-128 and STGCN-3-256 models, LinGCN yielded an accuracy of 77.55% and 80.28% respectively for the baseline 6 non-linear layers model, exceeding CryptoGCN  by 3.3% and 4.98%. This outcome signifies the superiority of our proposed teacher-guided node-wise polynomial replacement policy over the naive layer-wise polynomial replacement provided in CryptoGCN .

Importantly, the LinGCN model for the 6 non-linear layers employs a more fine-grained node-wise polynomial function, which is fused into the preceding Conv or GCNConv layers. Thus, the encryption level required is lower than that of CryptoGCN , resulting in reduced latency. When considering non-linear layer reduction performance, LinGCN experiences only a 1.2 to 1.7 % accuracy decline with a 2 effective non-linear layer reduction, whereas CryptoGCN  displays approximately 4% accuracy degradation when pruned for 2 non-linear layers. Remarkably, LinGCN achieves an accuracy improvement of more than 6% over CryptoGCN  for models with 4 non-linear layers. Even in models with only 2 non-linear layers, LinGCN exhibits exceptional accuracy

  Model & Layer-wise number of channel configuration & Accuracy (\%) \\  STGCN-3-128 & 3-64-128-128 & 80.64 \\  STGCN-3-256 & 3-128-256-256 & 82.80 \\  STGCN-6-256 & 3-64-64-128-128-256-256 & 84.52 \\  

Table 1: All ReLU based model architecture and accuracy

  Model &  \\   & Non-linear & Test acc & Latency \\  & layers & (\%) & (s) \\  LinGCN & 6 & 80.29 & 4632.05 \\  LinGCN & 5 & 79.07 & 4166.12 \\ 
**LinGCN** & **4** & **78.59** & **3699.49** \\  LinGCN & 3 & 76.41 & 2428.88 \\  LinGCN & 2 & 74.74 & 2143.46 \\  LinGCN & 1 & 71.98 & 1873.40 \\  CryptoGCN & 6 & 75.31 & 10580.41 \\  CryptoGCN & 5 & 73.78 & 4850.93 \\  CryptoGCN & 4 & 71.36 & 4831.93 \\  

Table 2: STGCN-3-128 comparison

  Model &  \\   & Non-linear & Test acc & Latency \\  & layers & (\%) & (s) \\  LinGCN & 6 & 80.29 & 4632.05 \\  LinGCN & 5 & 79.07 & 4166.12 \\ 
**LinGCN** & **4** & **78.59** & **3699.49** \\  LinGCN & 3 & 76.41 & 2428.88 \\  LinGCN & 2 & 74.74 & 2143.46 \\  LinGCN & 1 & 71.98 & 1873.40 \\  CryptoGCN & 6 & 75.31 & 10580.41 \\  CryptoGCN & 5 & 73.78 & 4850.93 \\  CryptoGCN & 4 & 71.36 & 4831.93 \\  

Table 3: STGCN-3-256 comparison (75.16% and 74.74%). While this accuracy aligns with the performance of 6 non-linear layer models in CryptoGCN , LinGCN significantly outperforms it in terms of private inference latency, providing more than a five-fold reduction compared to CryptoGCN .

**Non-linear Layer Sensitivity.** We conduct a sensitivity analysis on the STGCN-3-256 model to investigate the effect of non-linear layers. The results are displayed in Figure 5, wherein the \(i\) layers in the figure represent the remaining effective non-linear layers in the model, and every \(2i-1\) and \(2i\) exhibit a total number of non-linear nodes summing up to an integer multiple of 25. During the automatic structural linearization process, the gradient propagation is orchestrated to strike a balance between two goals: (i) preserving the feature structural information at the deeper layers, and (ii) mitigating the over-smoothing effect  resulting from the deep linearized GCN layers. Consequently, the middle 4th layer's nonlinearity is most important within the STGCN model.

**LinGCN is Scalable.** Beyond the evaluation of the 3-layer model, we also apply our LinGCN to a 6-layer STGCN-6-256 model, which no prior work evaluated, maintaining the same settings to assess the trade-off between accuracy and latency. An intriguing finding is that the teacher-guided full polynomial replacement not only maintains model performance but also surpasses the accuracy of the teacher baseline by 0.95%, achieving an accuracy of 85.47%. This suggests that the STGCN model equipped with a smooth polynomial activation function may enhance model expressivity. Experiments reveal a significant redundancy in the non-linear layers of the STGCN-6-256 model, as it exhibits no accuracy degradation up to the reduction of 8 non-linear layers. When the pruning extends to 10 non-linear layers, the accuracy experiences a slight dip of 3.2% compared to the non-reduction one.

**Pareto Frontier of LinGCN.** Figure 1 compares the Pareto frontier between LinGCN and CryptoGCN , showing LinGCN consistently surpassing CryptoGCN by at least 4% in accuracy across all latency ranges. As latency constraints ease (around 6000s), LinGCN's accuracy notably increases, improving by roughly 10% over CryptoGCN. Impressively, LinGCN maintains an accuracy of 75.16% while achieving a private inference latency of 742s, making it 14.2 times faster than CryptoGCN.

### Abalation Studies

**Non-linear Reduction & Replacement Sequence matters.** Our LinGCN algorithm employs a sequence of non-linear reduction and polynomial replacement, proving more efficacious in terms of search space convergence compared to the inverse sequence of polynomial replacement followed by non-linear reduction. The reasoning behind this lies in the possibility that the polynomial replacement process may produce a model optimized for a given architecture, hence subsequent changes to the architecture, such as non-linear reduction, could lead to significant accuracy degradation. Keeping all other parameters constant and merely altering the replacement sequence, we present the accuracy evaluation results of STGCN-3-256 model in Figure 5(a). As depicted in the figure, the sequence of polynomial replacement followed by non-linear reduction incurs an accuracy degradation exceeding 2% compared to the baseline sequence across all effective non-linear layer ranges.

**LinGCN Outperforms Layer-wise Non-linear Reduction.** A significant innovation in our LinGCN framework is the structural linearization process. This process allows nodes to determine their preferred positions for non-linearities within a single STGCN layer, as opposed to enforcing a rigorous constraint to excise the non-linearity across all nodes. This less restrictive approach yields a more detailed replacement outcome compared to the layer-wise non-linear reduction. To demonstrate the impact of this method, we kept all other parameters constant for of STGCN-3-256 model, altering

  Model &  \\   & Non-linear & Test acc & Latency \\  & layers & (\%) & (s) \\  LinGCN & 12 & 85.47 & 21171.80 \\  LinGCN & 11 & 86.24 & 19553.96 \\  LinGCN & 7 & 85.08 & 8186.35 \\  LinGCN & 5 & 83.64 & 7063.51 \\ 
**LinGCN** & **4** & **85.78** & **6371.39** \\  LinGCN & 3 & 84.28 & 5944.81 \\  LinGCN & 2 & 82.27 & 5456.12 \\  LinGCN & 1 & 75.93 & 4927.26 \\  

Table 4: LinGCN for STGCN-6-256 model.

Figure 5: STGCN-3-256 structural linearization

only the structural linearization to layer-wise linearization, and subsequently evaluated the model's accuracy, as shown in Figure 5(b). The results indicate that layer-wise linearization results in a 1.5% decrease in accuracy compared to structural linearization within the range of 2 to 5 effective non-linear layers. This demonstrates the effectiveness of the proposed structural linearization methodology.

**Distillation Hyperparameter Study.** In prior experiments, we kept \(\) and \(\) at 0.2 and 200 respectively. To assess their influence during distillation, we tested \(\) and \(\) across a range of values while distilling the STGCN-3-256 model with 6 non-linear layers. We test \([0.1,0.2,0.3,0.4,0.5]\) and \(\). Notably, we maintained \(=200\) and \(=0.2\) for the respective \(\) and \(\) ablations. The study, shown in Figure 5(c) and Figure 5(d), confirms that \(=0.2\) and \(=200\) yield optimal accuracy, and larger penalties might lower accuracy due to mismatch with the teacher model. This study thus justifies our selection of the \(\) and \(\).

**LinGCN Generalize to Other Dataset.** Without loss of generality, we extended our evaluation on Flickr  dataset, which is a representative node classification dataset widely used in GNN tasks.It consists of 89,250 nodes, 989,006 edges, and 500 feature dimensions. This dataset's task involves categorizing images based on descriptions and common online properties. For the security setting, we assume that node features are user-owned and the graph adjacency list is public. The Flickr dataset has a larger adjacent list but smaller feature dimension compared to the NTU-XVIEW dataset. We utilize three GCN layers with 256 hidden dimensions. Each GCN layer has similar structure as STGCN backbone architecture and has two linear and nonlinear layers. We conduct full-batch GCN training to obtain ReLU-based baseline model accuracies of 0.5492/0.5521 for validation/test dataset. We obtain the accuracy/latency tradeoff detailed in the Table 5. LinGCN framework substantially diminishes the number of effective nonlinear layers, which leads to 1.7 times speedup without much accuracy loss.

## 5 Discussion and Conclusion

Our LinGCN optimizes HE-based GCN inference by reducing multiplication levels through a differentiable structural linearization algorithm and a compact node-wise polynomial replacement policy, both guided by a two-level distillation from an all-ReLU teacher model. Additionally, we improve HE solutions for GCN private inference, enabling finer operator fusion for node-wise activation functions. LinGCN outperforms CryptoGCN by 5% in accuracy and reducing latency by 14.2 times at -75% accuracy, LinGCN sets a new state-of-the-art performance for private STGCN model inference. In the future, we will expand the proposed algorithm on other neural networks.