# A Theoretical Analysis of the Test Error of

Finite-Rank Kernel Ridge Regression

 Tin Sum Cheng Aurelien Lucchi Ivan Dokmanic

Department of Mathematics and Computer Science

University of Basel

{tisnum.cheng, aurelien.lucchi, ivan.dokmanic}@unibas.ch

Anastasis Kratsios

Department of Mathematics and Statistics,

McMaster University and Vector Institute,

kratsioa@mcmaster.ca

&David Belius

Faculty of Mathematics and Computer Science

UniDistance Suisse

david.belius@cantab.ch

###### Abstract

Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g. when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.

## 1 Introduction

Generalization is a central theme in statistical learning theory. The recent renewed interest in kernel methods, especially in Kernel Ridge Regression (KRR), is largely due to the fact that deep neural network (DNN) training can be approximated using kernels under appropriate conditions , in which the test error is more tractable analytically and thus enjoys stronger theoretical guarantees. However, many prior results have been derived under conditions incompatible with practical settings. For instance  give asymptotic bounds on the KRR test error, which requires the input dimension \(d\) to tend to infinity. In reality, the input dimension of the data set and the target function is typically finite. A technical difficulty to provide a sharp non-asymptotic bound on the KRR test error comes from the infinite dimensionality of the kernel 1. While this curse of dimensionality may be unavoidable for infinite-rank kernels (at least without additional restrictions), one may likely derive tighter bounds in a finite-rank setting. Therefore, other works  focused on a setting where the kernel is of finite-rank, where non-asymptotic bounds and exact formula of test error can be derived.

Since different generalization behaviours are observed depending on whether the rank of the kernel rank \(M\) is smaller than the sample size \(N\), one typically differentiates between the under-parameterized (\(M<N\)) and over-parameterized regime (\(M>N\)). We focus on the former due to its relevance to several applications in the field of machine learning, including random featuremodels [32; 35; 25; 14; 17] such as reservoir computers [14; 15; 16; 11] where all the hidden layers of a deep learning model are randomly generated, and only the final layer is trainable, or when fine-tuning the final layers of pre-trained deep neural networks for transfer learning  or in few-shot learning . The practice of only re-training pre-trained deep neural networks final layer [40; 23] is justified by the fact that earlier layers encode general features which are common to similar tasks thus fine-tuning can support's a network's ability to generalize to new tasks . In this case, the network's frozen hidden layers define a feature map into a finite-dimensional RKHS, which induces a finite-rank kernel; however, similar finite-rank kernels are often considered  in which features are extracted directly from several hidden layers in a deep pre-trained neural network which is then fed into a trainable linear regressor.

ContributionOur main objective is to provide sharp non-asymptotic upper and lower bounds for the finite-rank KRR test error in the under-parameterized regime. We make the following contributions:

1. **Non-vacuous bound in ridgeless case:** In contrast to prior work, our bounds exhibit better accuracy when the ridge parameter \( 0\), matching the intuition that a smaller ridge yields a smaller test error;
2. **Sharp non-asymptotic lower bound:** We provide a sharp lower bound of test error, which matches our derived upper bound as the sample size \(N\) increases. In this sense, our bounds are tight;
3. **Empirical validation:** We experimentally validate our results and show our improvement in bounds over .

As detailed in Section D, Table 1 compares our results to the available risk-bounds for finite-rank KRR.

Organization of PaperOur paper is organized as follows: Section 2 motivates the importance of the under-parameterized finite-rank KRR. Section 3 introduces the notation and necessary background material required in our results' formulation. Section 4 summarizes our main findings and illustrates their implications via a numerical study. All notation is tabulated in Appendix A for the reader's convenience. All proofs can be found in Appendices B and C, and numerical validation of our theoretical findings in Appendix D.

## 2 Applications

In this section, we motivate the importance of the under-parameterized finite-rank KRR in practical settings. For readers more interested in our main results, please start from Section 3.

Application: Fine-tuning Pre-Trained Deep Neural Networks For Transfer LearningConsider the transfer-learning problem of fine-tuning the final layer of a pre-trained deep neural network model

   Assumptions / results & Mohri & Amini & Bach  & This paper \\  & et al.  & et al. & & \\  Include inconsistent case & ✓ & ✗ & (✓)* & ✓ \\ Bias-variance decomposition & ✗ & ✓ & ✓ & ✓ \\ Test error high probability upper bound & ✓ & ✗ & ✓ & ✓ \\ Test error high probability lower bound & ✗ & ✗ & ✗ & ✓ \\ Bounds improve with smaller ridge? & ✗ & - & ✗ & ✓ \\ Upper bound decay rate** & \(}\) & - & \(+\) & \((+)}\) \\   

Table 1: Comparison of risk-bounds for finite-rank kernel ridge regressors. (*): The bound for the inconsistent case is implicit. See Section 5 for details. (**): By decay, we mean the difference between the upper bound and its limit as \(N\).

\(f:^{d}^{D}\) so that it can be adapted to a task that is similar to what it was initially trained for. This procedure defines a finite-rank kernel regressor because, for any \(x^{d}\), \(f\) can be factored as

\[f(x) =A(x),\] \[(x) = W^{(L)} W^{(1)}(x),\]

where \(A\) is a \(D d_{L}\)-matrix, \(\) denotes element-wise application of a univariate non-linear function, for \(l=1,,L\), \(L\) is a positive integer, and \(W^{(l)}:^{d_{l-1}}^{d_{l}}\), \(d=d_{0}\). In pre-training, all parameters defining the affine maps \(W^{(l)}\) in the hidden layers \( W^{(L)},, W^{(1)}\) are simultaneously optimized, while in fine-tuning, only the parameters in the final \(A\) are trained, and the others are frozen. This reduces \(f\) to a finite-rank kernel regressor with finite-rank kernel given for \(x,^{d}\) by

\[K(x,)}}{{=}}(x)^{} ().\]

Stably optimizing \(f\) to the new task requires strict convexity of the KRR problem

\[_{A}\,_{n=1}^{N}(A(x_{n})-y_{n})^{2}+\|A\|_{F}^ {2},\] (1)

where the hyperparameter \(>0\) ensures strong convexity of the KRR's loss function and where \(\|A\|_{F}\) denotes the Frobenius norm of A. The unique solution \(\) to (1), determines the optimally trained KRR model \((x)}}{{=}}(x)\) corresponding to the finite-rank kernel \(K\).

Application: Random Feature ModelsPopularized by  to enable more efficient kernel computations, random feature model has recently seen a substantial spike in popularity and has been the topic of many theoretical works [20; 27]. Note that random feature models are finite-rank kernel regressors once their features are randomly sampled [32; 35; 25; 14; 11].

Application: General UseWe emphasize that, though fine-tuning and random feature models provide simple typical examples of when finite-rank KRR arise in machine learning, there are several other instances where our results apply, e.g. when deriving generalization bounds for infinite-rank kernels by truncation, thereby replacing them by finite-rank kernels; e.g. .

## 3 Preliminary

We now formally define all concepts and notation used throughout this paper. A complete glossary is found in Appendix A.

### The Training and Testing Data

We fix a (non-empty) input space \(^{d}\) and a _target function_\(:\) which is to be learned by the KRR from a finite number of i.i.d. (possibly noisy) samples \(}}{{=}}(, )=(x_{i})_{i=1}^{N},(y_{i})_{i=1}^{N}^{d  N}^{N}\). The inputs \(x_{i}\) are drawn from a _sampling distribution_\(\) on \(\) and outputs are modelled as \(y_{i}=(x_{i})+_{i}^{N 1}\) for some i.i.d. independent random variable \(_{i}\) which is also independent of the \(\), satisfying \([]=0\) and \([^{2}]}}{{=}} ^{2} 0\). Our analysis is set in the space of all square-integrable "function" with respect to the sampling distribution \(L_{2}()}}{{=}}\{f: :_{x}[f(x)^{2}]<\}\).

We abbreviate \(=()+^{N 1}\), where \(()=[(x_{i})]_{i=1}^{N}\) and \(=[_{i}]_{i=1}^{N}\).

### The Assumption: The Finite-Rank Kernel Ridge Regressor

As in [2; 6], we fix a rank \(M_{+}\) for the kernel \(K\).

**Definition 3.1** (Finite Rank Kernel).: _Let \(M\) be a positive integer. A (finite) rank-\(M\) kernel \(K\) is a map \(K:\) defined for any \(x,x^{}\) by_

\[K(x,x^{})=_{k=1}^{M}_{k}_{k}(x)_{k}(x^{}),\] (2)_where the positive numbers \(\{_{k}\}_{k=1}^{M}\) are called the eigenvalues, and orthonormal functions \(_{k} L_{}^{2}\) are called the eigenfunctions. 2_

**Remark 3.2** (Eigenvalues are Ordered).: _Without loss of generality, we will assume that the kernel \(K\)'s eigenvalues are ordered \(_{1}_{2}_{M}>0\)._

We denote by \(}}{{=}}\{_{1} ^{-1/2}_{1},,_{M}^{-1/2}_{M}\}\) the reproducing kernel Hilbert space (RKHS) associated to \(K\). See Appendix B for additional details on kernels and RKHSs.

Together, the kernel \(K\) and a ridge \(>0\) define an optimal regressor for the training data \(\).

**Definition 3.3** (Kernel Ridge Regressor (KRR)).: _Fix a ridge \(>0\), the regressor \(f_{,}\) of the finite-rank kernel \(K\) is the (unique) minimizer of_

\[f_{,}}}{{=}}_{f }_{i=1}^{N}(f(x_{i})-y_{i})^{2}+\| f\|_{}^{2}.\] (3)

### The Target Function

The only regularity assumed of the target function \(\) is that it is square-integrable with respect to the sampling distribution \(\); i.e. \( L_{}^{2}\). We typically assume that \(\) contains strictly more features than can be expressed by the finite-rank kernel \(K\). Thus, \(\) can be arbitrarily complicated even if the kernel \(K\) is not. Since \(\{_{k}\}_{k=1}^{M}\) is an orthonormal set of \(L_{}^{2}\), we decompose the target function \(\) as

\[=^{M}_{k}_{k}}_{ M }+_{>M}_{>M},\] (4)

for some real numbers \(_{k}\)'s and \(_{>M}\) and for some normal function \(_{>M}\) orthogonal to \(\{_{k}\}_{k=1}^{M}\). We call \(_{>M}\) the orthonormal complement and \(_{>M}\) the complementary coefficient. The component \(_{ M}}}{{=}}_{k=1}^{M} _{k}_{k}\) of \(\) is in \(\). For the case \(_{>M}=0\), we call it a _consistent_ case, as the target function \(\) lies in the hypothesis set \(\), else we call it an _inconsistent_ case.

Alternatively, the orthonormal complement \(_{>M}\) can be understood as some input-dependent noise. Assume we have chosen a suitable finite rank kernel \(K\) with corresponding RKHS \(\) such that the target function lies in \(\). For this purpose, we can write the target function as \(_{ M}=_{k=1}^{M}_{k}_{k}\) for some real numbers \(_{k}\)'s. Suppose that we sample in a noisy environment; then for each sample input \(x_{i}\), the sample output \(y_{i}\) can be written as

\[y_{i}=_{ M}(x_{i})}_{}+_{>M}_{>M}(x_{i})}_{}+}_{}.\] (5)

### Test Error

Next, we introduce the subject of interest of this paper in more detail. Our statistical analysis quantifies the deviation of the learned function from the ground truth of the _test error_.

**Definition 3.4** (KRR Test Error).: _Fix a sample \(=(,()+)\). The finite-rank KRR (3)'s test error is_

\[_{,}}}{{=}} _{x,}[(f_{,}(x)-(x))^{2} ]=_{}[_{}(f_{, }(x)-(x))^{2}d(x)].\] (6)

_The analysis of this paper also follows the classical bias-variance decomposition, thus we write_

\[_{,}=+,\]

_where bias measures the error when there is no noise in the label, that is,_

\[}}{{=}}_{}( f_{(,()),}(x)-(x))^{2}d(x),\] (7)

_and variance is defined to be the difference between test error and bias: variance \(}}{{=}}_{,}-\)._Main Result

We now present the informal version of our main result, which gives high-probability upper and lower bounds on the test error. This result is obtained by bounding both the bias and variance, and the probability that the bounds hold is quantified with respect to the sampling distribution \(\). Here we emphasize the condition that \(N>M\) and hence our statement is valid only in under-parametrized case.

We can assume the data-generating distribution \(\) and eigenfunctions \(_{k}\) are well-behaved in the sense that:

**Assumption 4.1** (Sub-Gaussian-ness).: _We assume that the probability distribution of the random variable \(_{k}(x)\), where \(x\), has sub-Gaussian norm bounded by a positive constant \(G>0\), for all \(k\{1,...,M\}\{>M\}\)3._

In particular, if the random variable \(_{k}(x)\) is bounded, the assumption 4.1 is fulfilled.

**Theorem 4.2** (High Probability Bounds on Bias and Variance).: _Suppose Assumption 4.1 holds, for \(N>M\) sufficient large, there exists some constants \(C_{1},C_{2}\) independent to \(N\) and \(\) such that, with a probability of at least \(1-2/N\) w.r.t. random sampling, we have the following results simultaneously:_

1. _[label=()]_
2. _Upper-Bound on Bias: The bias is upper bounded by:_ \[_{>M}^{2}+\|_{ M}\|_{ }^{2}+(\|\|_{L_{}^{2}}^{2}+2\| _{ M}\|_{}^{2})}+C_{1} ,\] (8) _where we denote_ \(_{ M}}}{{=}}_{k=1}^{M} _{k}_{k}=-_{>M}_{>M}\)_;_
3. _Lower-Bound on Bias: The bias is lower bounded by an analogous result:_ \[_{>M}^{2}+_{M}}{( _{M}+)^{2}}\|_{ M}\|_{}^{2}-( {1}{4}\|\|_{L_{}^{2}}^{2}+}{_{1}+ }\|_{ M}\|_{}^{2})}-C_{1} .\] (9)
4. _Upper-Bound on Variance: The variance is upper bounded by:_ \[^{2}(1+}+C_{2} );\] (10)
5. _Lower-Bound on Variance: The variance is lower bounded by an analogous result:_ \[^{2}}{(_{M}+)^{2}}^{2 }(1-})-C_{2}^{2}.\] (11)

_For \( 0\), we have a simpler bound on the bias: with a probability of at least \(1-2/N\), we have_

\[&_{ 0}_{>M}^{2} (1+)+6_{>M}^{2}()^{};\\ &_{ 0}_{>M}^{2} (1-)-6_{>M}^{2}()^{}.\] (12)

_If \(_{>M}^{2}=0\), then we are in the consistent case, meaning that \(\) belongs to \(\). In this case, we have a simpler bound on the bias: with a probability of at least \(1-2/N\), we have_

\[\|\|_{}^{2}(1+2})+C_{1}.\] (13)Proof Sketch:_ The main technical tools are 1) more careful algebraic manipulations when dealing with terms involving the regularizer \(\) and 2) the use of a concentration result for a sub-Gaussian random covariance matrix in  followed by the Neumann series expansion of a matrix inverse. Hence, unlike previous work, our result holds for any \(\), which can be chosen independence to \(N\). The complete proof of this result, together with the explicit form of the lower bound, is presented in Theorems C.19 and C.20 in the Appendix C. 

**Remark 4.3**.: _Note that one can also easily derive bounds on the expected value of the test error to allow comparisons with prior works, such as ._

**Remark 4.4**.: _The main technical difference from prior works is that we consider the basis \(\{_{k}\}_{k=1}^{M}\) in the function space \(L_{}^{2}\) instead of \(\{_{k}^{-1/2}_{k}\}_{k=1}^{M}\) in the RKHS \(\). This way, we exploit decouple the effect of spectrum from the sampling randomness to obtain a sharper bound. For further details, see Remark C.12 in Appendix C._

Combining Theorem 4.2 and the bias-variance decomposition in Definition 3.4, we have both the upper and lower bounds of the test error on KRR.

**Corollary 4.4.1**.: _Under mild conditions on the kernel \(K\), for \(N\) sufficiently large, there exist some constants \(C_{1},C_{2}\) independent to \(N\) and \(\) such that, with a probability of at least \(1-2/N\) w.r.t. random sampling, we have the bounds on the test error \(_{,}\):_

\[_{,} _{>M}^{2}+\|_{ M}\|_{ }^{2}+(\|\|_{L_{}^{2}}^{2}+2 \|_{ M}\|_{}^{2})}+C_{1} \] \[+^{2}(1+}+C_{2} );\] \[_{,} _{>M}^{2}+_{M}}{( _{k}+)^{2}}\|_{ M}\|_{}^{2}-( \|\|_{L_{}^{2}}^{2}+}{_{1}+ }\|_{ M}\|_{}^{2})} -C_{1}\] \[+^{2}(1-}-C_{2} ).\]

_In particular, we have:_

\[_{ 0}_{,}(1+)_{>M}^{2}+6_{>M}^{2}()^{}+^{2}(1+} +C_{2}).\]

_The corresponding lower bounds are given analogously:_

\[_{ 0}_{,}(1-)_{>M}^{2}-6_{>M}^{2}()^{}+^{2}(1-} -C_{2}).\]

See Section 5 for more details and validations of Theorem 4.2.

## 5 Discussion

In this section, we first elaborate on the result from Theorem 4.2, which we then compare in detail to prior works, showcasing the improvements this paper makes. Finally, we discuss several future research directions.

### Elaboration on Main Result

BiasFrom Eq. (8), we can draw the following observations: 1) The term \(_{>M}^{2}\) in the upper bound is the finite rank error due to the inconsistency between RKHS and the orthogonal complement of the target function, which cannot be improved no matter what sample size we have. We can also view this term as the sample-dependent noise variance (see Eq. (5)). Hence, unlike the sample-independent noise variance \(\) in Eq. (10), the sample-dependent noise variance does not vanish when \(N\). 2) Note that the third term \(\|\|_{L_{}^{2}}^{2}}\) is a residue term proportional to \(\|\|_{L_{}^{2}}^{2}\) and vanisheswhen \(N\), which means we have better control of the sample-dependent noise around its expected value for large \(N\). Also, note that the factor \(\|\|_{L^{2}_{}}^{2}=_{k=1}^{M}_{k}^{2}+_{>M}^{2}\) depends solely on the target function \(\) but not on the kernel \(K\) or ridge parameter \(\). 3) The second plus the fourth terms \((1+2})\|_{ M}\|_{}^{2}\) depends strongly on the kernel training: the sum is proportional to the ridge \(\), and the RKHS norm square \(\|_{ M}\|_{}^{2}=_{k=1}^{M}_{k }^{2}}{_{k}}\) measures how well-aligned the target functions with the chosen kernel \(K\) is. Again, a larger \(N\) favors the control as the residue \(} 0\) as \(N\). 4) The fifth term \(C_{1}\) is a small residue term with fast decay rate, which the other terms will overshadow as \(N\).

Ridge parameterThe bounds in Eq. (12) demonstrate that in the ridgeless case, the bias can be controlled solely by the finite rank error \(_{k}^{2}\) with a confidence level depending on \(N\); also the upper and lower bounds coincide as \(N\).

VarianceFor the variance bounds in Eq. (10), we have similar results: the variance can be controlled solely by the (sample-independent) noise variance \(^{2}\) with a confidence level depending on \(N\), also the upper and lower bounds coincides as \(N\).

### Comparison with Prior Works

We discuss how our results add to, and improve on, what is known about finite-rank KRRs following the presentation in Table 1.

Classical tools to study generalizationThe test error measures the average difference between the trained model and the target function. It is one of the most common measures to study the generalization performance of a machine learning model. Nevertheless, generally applicable statistical learning-theoretic tools from VC-theory , (local) Rademacher complexities , PAC-Bayes methods , or smooth optimal transport theory  all yield pessimistic bounds on the KRR test error. For example, Mohri et al.  bound the generalization error using Rademacher Complexity- there exists some constant \(C>0\) independent to \(N\) and \(\) such that, with a probability of at least \(1-\):

\[-}(1+ {2}}{2}}).\] (14)

If we set \(\) to \(2/N\), the decay in the generalization gap is \((})\), which is too slow compared to other kernel-specific analyses.

Truncated KRRAmini et al.  suggests an interesting type of finite-rank kernels: for any (infinite-rank) kernel \(K^{()}\), fix a sample \(=(,)\) of size \(N\) and define a rank-\(N\) kernel \(K\) by the eigendecomposition of the kernel matrix \(^{()}\). Note that different random sample results in a completely different rank-\(N\) kernel \(K\). Assume that the target function \(\) lies in the \(N\)-dimensional RKHS corresponding to \(K\), then one can obtain an exact formula for the expected value of the test error of the kernel \(K\) (but not only the original \(K^{()}\)). However, the formula obtained in  only takes into account the expectation over the noise \(\), but not over the samples \(x\). Hence, our paper yields a more general result for the test error.

Upper bound ComparisonTo the best of our knowledge, the following result is the closest and most related to ours:

**Theorem 5.1**.: _(Proposition 7.4 in ) With notation as before, assume, in addition, that: 1) \(_{>M}=0\), that is, \(\); 2) for all \(k=1,...,M\), we have \(^{M}_{k} R}\) ; 3) and \(N(+}{8}}{})\).__Then we have, with a probability of at least \(1-\),_

\[  4\|\|_{}^{2};\] \[ R^{2}}{ N}(1+2 ).\]

In comparison to , Theorem 4.2 makes the following significant improvements:

1. In Theorem 5.1, as the ridge parameter \( 0\), the minimum requirement of \(N\) and the upper bound of the variance explode; while in our case, both the minimum requirement of \(N\) and the bounds on the variance are independent of \(\).
2. While  also mentioned the case where \(_{>M} 0\), however their bound for the inconsistent case is implicit; 4 while our work clearly states the impact of \(_{>M}\) on the test error. 3. Our upper bounds are sharper than : 1. For the bias, assume \(_{>M}=0\) for comparison purposes. Then by line (13) our upper bound would become: \[\|\|_{}^{2}(1+2})+C_{1},\] which improves 's upper bound bias \( 4\|\|_{}^{2}\) significantly. First, we observe a minor improvement in the constant (by a factor of \(\)) when \(N\). Second and more importantly, for finite \(N\), we observe a non-asymptotic decay on the upper bound, while 's upper bound is insensitive to \(N\). 2. For the variance, if we replace \(\) by \(2/N\) in 's upper bound: with probability of at least \(1-2/N\), \[R^{2}}{ N}(1+2 N),\] (15) which has a decay rate of \(\), our work shows a much faster decay of \(\). Moreover, we show that the "unscaled" variance, that is \(N()\), is actually bounded by \((1+})\), while 's upper bound on it would explode.
4. We also provide a lower bound for the test error that matches with the upper bound when \(N\), while  does not derive any lower bound.

While our work offers overall better bounds, there are some technical differences between the bounds of  and ours:

1. We require some explicit mild condition on input distribution \(\) and on kernel \(K\), while  rely on the assumption \(^{M}_{k}} R\) which gives an alternative implicit requirement;
2. Our work eventually assumes under-parametrization \(M<N\), while  does not requires this. Essentially, we exploit the assumption on the under-parameterized regime to obtain a sharper bound than . In short, we select the \(L_{}^{2}\) basis \(_{k}\) for analysis while  selects the basis in RKHS, which eventually affects the assumptions and conclusions. For more details, see Appendix C.

Nystrom Subsampling and Random Feature ModelNystrom Subsampling and Random feature models are two setting closely related to our work.  bound the test error from above where the regularizer \(=(N)\) decays as \(N\). In comparison, one main contribution of our work is that we provide both tighter upper bound and tighter lower bound than these prior works (they derive the same convergence rate on the upper bound up to constants but they do not derive a lower bound). Another major difference is that our bounds work for any regularization independent to \(N\).

Effective DimensionIn [34; 35] and other related works on kernel, the quantity \(()=_{k=1}^{M}}{_{k}+}= [}]\) is called the effective dimension and it appeared as a factor in the upper bound. In our work, we can define a related quantity \(^{2}()=_{k=1}^{M}^{2}}{(_{k}+ )^{2}}=[}^{2}]\), which appears in our bound (See Proposition C.14 for details.). Note that \(^{2}()() M\). Indeed, we can sharpen the factor \(\) in line (10) to \(^{2}()}{N}\).

### Experiments

We examine the test error and showcase the validity of the bounds we derived for two different finite-rank kernels: the truncated neural tangent kernel (tNTK) and the Legendre kernel (LK) (see details of their definitions in Appendix D). Figure 1 shows the true test error as a function of the sample size \(N\). Figure 2 plots our upper bound compared to , clearly demonstrating significant improvements. We for instance note that the bound is tighter for any value of \(\). Finally, Figure 9 shows both the upper and lower bound closely enclose the true test error. For more details on the experiment setup, including evaluating the lower bound, please see Appendix D.

### Future Research Direction

An interesting extension would be to combine our results with [36; 10] in the over-parameterized regime to give an accurate transition and observe double descent. However, since we are interested in the sharpest possible bounds for the under-parameterized regime, we treat these cases separately. We will do the same for the over-parameterized regime in future work. A special case is where the parameterization is at the threshold \(M=N\) and \(=0\). The blow-up of variance as \(M N\) is well-known, where [5; 9] gives empirical and theoretical reports on the blow-up for kernels. We expect that we can formally prove this result for any finite-rank kernels using our exact formula in Proposition C.7 and some anti-concentration inequalities on random matrices.

Figure 1: KRR on a finite-rank kernel. Left: KRR training; right: test error for varying \(N\), over 10 iterations. The upper and lower quartiles are used as error bars.