# VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction

**Hanlin Chen\({}^{1}\)** **Fangyin Wei\({}^{2}\)** **Chen Li\({}^{1}\)** **Tianxin Huang\({}^{1}\)** **Yunsong Wang\({}^{1}\)** **Gim Hee Lee\({}^{1}\)**

\({}^{1}\) School of Computing, National University of Singapore

\({}^{2}\) Princeton University

hanlin.chen@u.nus.edu gimhee.lee@nus.edu.sg

https://hlinchen.github.io/projects/VCR-GauS/

Although 3D Gaussian Splitting has been widely studied because of its realistic and efficient novel-view synthesis, it is still challenging to extract a high-quality surface from the point-based representation. Previous works improve the surface by incorporating geometric priors from the off-the-shelf normal estimator. However, there are two main limitations: 1) Supervising normals rendered from 3D Gaussians effectively updates the rotation parameter but is less effective for other geometric parameters; 2) The inconsistency of predicted normal maps across multiple views may lead to severe reconstruction artifacts. In this paper, we propose a Depth-Normal regularizer that directly couples normal with other geometric parameters, leading to full updates of the geometric parameters from normal regularization. We further propose a confidence term to mitigate inconsistencies of normal predictions across multiple views. Moreover, we also introduce a densification and splitting strategy to regularize the size and distribution of 3D Gaussians for more accurate surface modeling. Compared with Gaussian-based baselines, experiments show that our approach obtains better reconstruction quality and maintains competitive appearance quality at faster training speed and 100+ FPS rendering.

Figure 1: **View-Consistent D-Normal Regularizer. Pseudo normals predicted from pretrained monocular normal estimators tend to be inconsistent across different views (left). Our method calculates a confidence map indicating the confidence of the pseudo normals (middle). The confidence is used to weigh the loss imposed on our proposed D-Normals. Our method achieves new state-of-the-art surface reconstruction results and rendering quality comparable with prior work.**

## 1 Introduction

Multi-view stereo (MVS) is a long-standing problem that aims to create 3D surfaces of an object or scene captured from multiple viewpoints [9; 5; 25; 41]. This technique has applications in robotics, graphics, virtual reality, _etc_. Recently, rendering methods [49; 59; 26; 16] have enhanced the quality of reconstructions. These approaches which are often based on implicit neural representations require extensive training time. For instance, Neuralangelo  uses hash encoding  for creating high-fidelity surfaces but requires 128 GPU hours for a single scene. On the other hand, the novel 3D Gaussian Spatting method  employs 3D Gaussians to render complex scenes photorealistically in real-time, offering a more efficient alternative. Consequently, many recent works attempted the utilization of Gaussian Splatting for surface reconstruction [8; 15; 45; 19]. Although they achieve success in object-level reconstruction, it is still challenging to extract a high-quality surface for large scenes. Previous works  improve the surface for scene-level reconstruction by incorporating geometric priors from the off-the-shelf normal estimator. However, there are two main limitations for Gaussian-based reconstruction: 1) Supervising normals rendered from 3D Gaussians effectively updates the rotation parameter but is less effective for other geometric parameters; 2) The predicted normal maps are inconsistent across multiple views, which may lead to severe reconstruction artifacts.

In this paper, we introduce a novel view-consistent Depth-Normal (D-Normal) regularizer to alleviate the above-mentioned limitations. As illustrated in Fig. 2, we notice that the supervision of the Gaussian normals can effectively update its rotations but is less effective for affecting its positions. Consequently, the supervision of Gaussian normals is not as effective as NeuS-based methods [49; 59; 26] whose normal is the gradient of the signed distance function (SDF) that is directly related to the position in 3D space. To solve this issue, we are inspired by the depth and normal estimation [1; 56] to introduce a D-Normal formulation, where the normal is derived from the gradient of rendered depth instead of directly blended from 3D Gaussians. Unlike existing works that obtain depth from the center position of 3D Gaussians, we compute the depth as the intersection of the ray and the compressed Gaussians. Specifically, we first make the Gaussians suitable for 3D reconstruction by applying a scale regularization similar to NeuSG  to compress the 3D Gaussian ellipsoids into a plane. Subsequently, the computation of the depth can be simplified to the intersection between a ray and a plane. As a result, our novel parametrization of the depth allows effective full supervision of the Gaussian geometric parameters by any data-driven monocular normal estimator.

To mitigate the inconsistent normal predictions across views, we further propose an uncertainty-aware normal regularizer as shown in Fig. 1. Particularly, we introduce a confidence term for each normal prediction. A high confidence means low uncertainty leading to enhancement of

Figure 2: **Illustration of rendered normal supervision and the D-Normal regularizer.** (a) As a result of the back-propagation through alpha-blending via Eq. 1, rendered normal supervision \(_{}\) moves Gaussians closer to (**P\({}_{1}\)**) or away from (**P\({}_{2}\)**) the intersecting ray. When the normal of a Gaussian is closer to the GT surface normal, the supervision pushes this Gaussian (**P\({}_{1}\)**) towards the ray to increase its weight in the rendering equation, and vice-versa (**P\({}_{2}\)**). (b) Such movement of Gaussians stops when the rendered normal loss \(_{}\) is equal to zero. In either case ((a) or (b)), the rendered normal loss cannot move Gaussian towards the surface. In contrast, (c) the D-Normal regularizer \(_{}\) can move Gaussians towards or away from GT surface. **P\({}_{1}\)** and **P\({}_{2}\)** are the 3D positions corresponding to the mean depth of two neighboring pixels (rays) via Eq. 10. The D-Normal \(}_{d}\) is derived from **P\({}_{1}\)** and **P\({}_{2}\)** in Eq. 11. \(_{}\) encourages \(}_{d}\) to align with the ground truth normal \(\), resulting in Gaussians moving towards or away from the surface.

the normal regularization, and vice-versa. Typically, the predicted normal maps from different views are combined to assess the uncertainty of a specific view. However, it is challenging to find correspondence across different views. We circumvent this issue by using the rendered normal learned from multi-view normal priors since we notice that it represents an average of normal priors across views. Furthermore, the confidence term is computed as the cosine distance between the rendered and predicted normals. Although the normal supervision has made the normals more accurate, there is still a minor error leading to depth error arising from the remnant large Gaussians. We thus devise a new densification that splits large Gaussians into smaller ones to represent the surface better. Finally, we incorporate a new splitting strategy to alleviate the surface bumps caused by densification. Experiments show that our approach outperforms Gaussian-based baselines in terms of both reconstruction quality and rendering speed.

Our **main contributions** are summarized below:

* We formulate a novel multi-view D-Normal regularizer that enables full optimization of the Gaussian geometric parameters to achieve better surface reconstruction.
* We further design a confidence term to weigh our D-Normal regularizer to mitigate inconsistencies of normal predictions across multiple views.
* We introduce a new densification and splitting strategy to alleviate depth error towards more accurate surface modeling.
* Our method outperforms prior work in terms of reconstruction accuracy and running efficiency on the benchmarking Tank and Temples, Replica, MipNeRF360, and DTU datasets.

## 2 Related Work

**Novel View Synthesis.** The pursuit of novel view synthesis began with Soft3D , which integrated deep learning and volumetric ray-marching to form a continuous, differentiable density field for geometry representation [18; 42]. While effective, this approach was computationally expensive. Neural Radiance Fields (NeRF)  improved render quality with importance sampling and positional encoding, but the deep neural networks slowed down processing. Subsequent methods aimed to optimize both quality and speed. Techniques like position encoding and band-limited coordinate networks are combined with neural radiance fields for pre-filtered scene representation [2; 3; 28]. Innovations to speed up rendering included leveraging spatial data structures and adjusting MLP size [6; 11; 14; 17; 35; 44]. Notable examples are InstantNGP , which uses a hash grid and a reduced MLP for faster computation, and Plenoxels , which employs a sparse voxel grid to eliminate neural networks entirely. Both use Spherical Harmonics to enhance rendering. Despite these advancements, challenges remain in representing empty space and maintaining image quality with structured grids and extensive sampling. Recently, 3D Gaussian Splitting (3DGS)  has addressed these issues with unstructured and GPU-optimized splatting, achieving faster and higher-quality rendering without neural components. In this work, we utilize the advantage of Gaussian Splatting to perform surface reconstruction and incorporate normal priors to guide the reconstruction, especially for large indoor and outdoor scenes.

**Multi-View Surface Reconstruction.** Surface reconstruction is key in 3D vision. Traditional MVS methods [4; 9; 5; 25; 38; 41; 40] use feature matching for depth [4; 38] or voxel-based shapes [9; 5; 25; 41; 46]. Depth-based methods combine depth maps into point clouds, while volumetric methods estimate occupancy and color in voxel grids [9; 5; 29]. However, the finite resolution of voxel grids limits precision. Learning-based MVS modifies traditional steps such as feature matching [31; 48; 60], depth integration , or depth inference from images [20; 51; 61; 52; 58]. Further advancements [49; 53] integrated implicit surfaces with volume rendering, achieving detailed surface reconstructions from RGB images. These methods have been extended to large-scale reconstructions via additional regularization [59; 26]. Despite these impressive developments, efficient large-scale scene reconstruction remains a challenge. For example, Neuralangelo  requires 128 GPU hours for reconstructing a single scene from the Tanks and Temples Dataset . To accelerate the reconstruction process, some works [15; 19] introduce the 3D Gaussian splitting technique. However, these works still fail in large-scale reconstructions. In this work, we focus on introducing normal regularization for large-scale reconstructions.

**3D Gaussian Splitting.** Since 3DGS  was introduced, it has been rapidly extended to surface reconstruction. We highlight the distinctions between our method and concurrent works SuGaR , 2DGS , NeuSG , and DN-Splatter . In contrast to SuGaR and 2DGS with unsatisfactory performance on large-scale scenes, our method focuses on introducing normal regularization to improve large-scale reconstructions. 2DGS obtaining 2D Gaussian primitives by setting the last entry of scaling factors to zero which is hard to optimize by original Gaussian Splatting technique as noted in [65; 19], while our method utilizes scale regularization to flatten 3D Gaussians which are easier to optimize. NeuSG utilizes both 3D Gaussian splitting and neural implicit rendering jointly and extracts the surface from an SDF network, while our approach is faster and conceptually simpler by leveraging only Gaussian splatting for surface approximation. Although normal prior is also used for indoor scenes, DN-Splatter may show severe reconstruction artifacts due to their normal supervision can only update the rotation parameters and normal maps inconsistencies across multiple views. Moreover, we do not use the ground truth depth for supervision utilized by DN-Splatter. In comparison, our work is designed to solve both limitations.

## 3 Our Method

Our proposed view-consistent D-Normal regularizer efficiently reconstructs complete and detailed surfaces of scenes from multi-view images. Sec. 3.1 provides an overview of 3D Gaussian Splitting . Our normal and depth formulation of 3D Gaussians is detailed in Sec. 3.2. Sec. 3.3 introduces our proposed regularizations. The densification and splitting of the Gaussian is described in Sec. 3.4. Fig. 3 depicts our whole framework.

### Preliminaries: 3D Gaussian Splatting

3D Gaussian Splatting  is an explicit 3D scene representation with 3D Gaussians. Each Gaussian is defined by a covariance matrix \(\) and a center point \(^{3}\) which is the mean of the Gaussian. The 3D Gaussian distribution can be represented as:

\[G()=(-)^{}^{- 1}(-)\}}.\] (1)

To maintain positive semi-definiteness during optimization, the covariance matrix \(\) is expressed as the product of a scaling matrix \(\) and a rotation matrix \(\):

\[=^{}^{},\] (2)

Figure 3: **Overview of our VCR-GauS.** During densification and splitting, our method only keeps the Gaussians at the first intersections and splits large Gaussians into smaller ones along the major principle axis. The rendered normals are supervised with pseudo normals predicted from a pretrained monocular normal estimator in \(_{}\). We further calculate an uncertainty map based on the discrepancies between the rendered and pseudo normals (_cf._ Eq. 13) to weigh the loss \(_{}\) between pseudo normals and D-Normals derived from the rendered depth maps. We compare different approaches for normal calculation (Top Right) and show our intersection depth (Bottom Right).

where \(\) is a diagonal matrix, stored by a scaling factor \(^{3}\), and the rotation matrix \(\) is represented by a quaternion \(^{4}\).

For novel view rendering, the splatting technique  is applied to the Gaussians on the camera planes. Using the viewing transform matrix \(\) and the Jacobian of the affine approximation of the projective transformation \(\), the transformed covariance matrix \(^{}\) can be determined as:

\[^{}=^{} ^{}.\] (3)

A 3D Gaussian is defined by its position \(\), quaternion \(\), scaling factor \(\), opacity \(o\), and color represented with spherical harmonics coefficients \(^{k}\). For a given pixel, the combined color and opacity from multiple Gaussians are weighted by Eq. 1. The color blending for overlapping points is:

\[}=_{i M}_{i}_{i}_{j=1}^{i-1}(1- _{j}),\] (4)

where \(_{i}\) and \(_{i}=o_{i}G(_{i})\) denote the color and density of a point, respectively.

### Geometric Properties

To reconstruct the 3D surface, we introduce two geometric properties: normal and depth of a Gaussian, which are used to render the corresponding normal map and depth map for regularization.

**Normal Vector.** Following NeuSG , the normal of the Gaussian can be represented as the direction of the minimized scaling factor. The normal in the world coordinate system is defined as:

\[=[k,:]^{3},k=([s_{1},s_{2 },s_{3}]),\] (5)

The normal \(\) and position \(\) are transformed into the camera coordination system with the camera extrinsic matrix, which we subsequently take as the default unless otherwise stated.

**Intersection Depth.** The existing work  obtains the depth from the center position \(=(p_{x},p_{y},p_{z})\) of each Gaussian in the camera coordinate system. However, this formulation is inaccurate and results in the depth from each Gaussian center being unrelated to its normal \(\) during optimization. A more reasonable depth calculation is to compute the depth of the intersection between the Gaussian and the ray emitted from the camera center. To simplify the computation of intersection and represent the surface, we incorporate a scale regularization loss \(_{}\) from NeuSG  to squeeze the 3D Gaussian ellipsoids into highly flat shapes. This loss constrains the minimum component of the scaling factor \(=(s_{1},s_{2},s_{3})^{}^{3}\) for each Gaussian towards zero:

\[_{}=\|(s_{1},s_{2},s_{3})\|_{1}.\] (6)

This process effectively flattens the 3D Gaussian towards a planar shape which we represent by \((,)\). As a result, any point \(_{p}\) on the plane follows the incidence equation given by: \((_{p}-)=0\). We further denote any point \(_{l}\) on a ray that passes through the origin in 3D space as \(_{l}=t\), where \(t\) is the distance from the point to the origin along the ray. We set \(_{l}=_{p}\) at the intersection of the ray with the plane, which we can then solve for the depth of the intersection along the \(z\)-axis as:

\[d(,)=_{z}*()/(),\] (7)

where \(_{z}\) is the z-value of the ray direction \(\). From the equation, we can see the intersection depth is related to both the position \(\) and the normal \(\) of the Gaussian. This not only offers more accurate depth calculation but also enables the D-Normal regularization to backpropagate its loss to all different Gaussian parameters.

### View-Consistent D-Normal Regularization

We first introduce our D-Normal formulation to allow the full optimization of the Gaussian geometric parameters. We further propose a confidence term to relieve the constraint from uncertain predictions and strengthen it from certain ones to avoid the wrong guidance from the inconsistent normal priors from a pretrained monocular model across multiple views.

**D-Normal Regularizer.** To improve the reconstruction quality, we utilize a normal prior \(\) predicted from a pretrained monocular deep neural network  to supervise the rendered normal map \(}\) with L1 and cosine losses:

\[_{} =\|}-\|_{1}+(1-} ),\] (8) \[} =_{i M}_{i}_{i}_{j=1}^{i-1}(1- _{j})/_{i M}_{i}_{j=1}^{i-1}(1-_{j}).\] (9)

However, normal regularization alone is insufficient for surface reconstruction as compared with NeuS-based methods. There are two main reasons for this: 1) Updating the position of a Gaussian using \(G()\) only moves it closer to or farther from the intersecting ray, as shown in Fig. 2 (a) (the mathematical proof is provided in A.2 of the supplemental material); 2) NeuS-based methods calculate normals as gradients of the SDF function from the input position \(\) and therefore normal regularization effectively influences position updates. However, since the normal is only related to the rotation of the Gaussian in 3DGS, supervising the rendered normals does not efficiently update positions as shown in Fig. 3. To solve this problem and inspired by normal and depth estimation [1; 56], we propose a new depth-normal formulation. First, we render the depth map by weighted summing the depths:

\[=_{i M}d_{i}_{i}_{j=1}^{i-1}(1-_{j})/_{i M }_{i}_{j=1}^{i-1}(1-_{j}),\] (10)

where \(d_{i}\) is the intersection depth from Eq. 7. Subsequently, we convert the rendered depth \(\) to a D-Normal \(}_{d}\) and use the predicted normal \(\) by a pretrained model to supervise the depth via the D-Normal \(}_{d}\). In particular, the D-Normal is computed by back-projecting the depth map into point clouds \(\{_{k}(,)\}\) with the camera intrinsic matrix. The D-Normal \(}_{d}\) is then computed by the cross-product with the horizontal and vertical finite differences from the neighboring points:

\[}_{d}(,)=( ,)_{h}(,)}{| _{v}(,)_{h}( ,)|}.\] (11)

From this equation, we can see the D-Normal is a function of both the normal \(\) and the position \(\) of Gaussians. This allows the regularization on the D-Normal to optimize both normal \(\) and position \(\). The D-Normal regularization is formulated as:

\[_{}=\|}_{d}-\|_{1}+(1-}_{d}).\] (12)

**Confidence.** Although the D-Normal regularizer resolves the issue with the Gaussian position in the supervision of its normal, the normal maps predicted by a pretrained model are not always accurate. This is especially problematic when inconsistencies arise across multiple views. We thus introduce a confidence term \(w\) to emphasize the regularization for high certainty areas while reducing on low certainty areas. Typically, the normals from different views are combined to assess the certainty of a specific view. However, it is challenging to find correspondence between different views. We circumvent the challenge by using the rendered normal learned from multi-view pseudo normals, which represents an average of the pseudo normals across the views. As a result, we can use the rendered normal to gauge the uncertainty of the predicted normal in the current view. Specifically, the confidence term \(w\) is computed as the cosine distance between the rendered and predicted normals, _i.e._:

\[w=\{(}_{d}-1)/\},\] (13)

where \(\) is a hyper-parameter. Consequently, the view-consistent D-Normal regularizer is defined as:

\[_{}=w*(\|}_{d}-\|_{1}+(1-}_{d})).\] (14)

The overall loss function combining these elements is:

\[_{}=_{}+_{1}_{ }+_{2}_{}+_{3}_{},\] (15)

with \(_{1}\), \(_{2}\) and \(_{3}\) balancing the individual components. \(_{}\) includes L1 and D-SSIM losses.

### Densification and Splitting

We observe that the original densification and splitting in Gaussian Splatting causes depth error as well as surface bumps and protrusions to appear. To address this issue, we further propose a new densification and splitting strategy as depicted in Fig. 3 (Bottom Left).

**Densification.** Although normal supervision has made the normals more accurate, there is still a minor error \(\) leading to depth error arising from the remnant large Gaussians since Gaussian size is not the consideration in the original "large position gradient" selection criteria for Gaussians to be densified. As illustrated in Fig. 3(a), a very small normal error at the edges can result in a significant depth error \( r\) away from the center for larger Gaussians (top of figure). Comparatively, the depth error is small for smaller Gaussians since \(r^{}\) becomes relatively smaller (bottom of figure). Consequently, we subdivide the larger Gaussians into smaller Gaussians to keep the depth error small. To achieve this, we first randomly sample camera views from a cuboid that encompasses the entire scene for object-centric outdoor scenes and from the training views for indoor scenes. Since we aim to densify only the surface Gaussians, we only keep the first intersected Gaussian and discard the rest for each ray emitted from the camera. Subsequently, we densify only those with a scale above a threshold \(\) among the collected Gaussians.

**Splitting.** We notice that the Gaussians tend to protrude the ground truth surface after densification due to the clustering of many Gaussians. As also observed in Mip-Splatting , Gaussians splitted from the same parents tend to remain clustered with relatively stable positions due to the sampling from the same Gaussian distribution. To avoid clustering, we split the old Gaussian into two new Gaussian along the axis with the largest scale instead of using the Gaussian sampling with the position of the Gaussian as mean and the 3D scale of the Gaussian as variance. The positions of the new Gaussians evenly divide the maximum scale of the old Gaussian. Other parameters of new Gaussians are obtained following the original 3DGS. This process is shown in Fig. 3(b).

## 4 Experiments

We first evaluate our method on 3D surface reconstruction in Sec. 4.1. We also report the rendering results in Sec. 4.1. Additionally, we validate the effectiveness of the proposed techniques in Sec. 4.2.

    &  &  \\  & NeuS & MonoSDF & Geo-Neus & SuGaR & 3DGS & 2DGS & Ours \\  Barn & 0.29 & 0.49 & 0.33 & 0.14 & 0.13 & 0.36 & 0.62 \\ Caterpillar & 0.29 & 0.31 & 0.26 & 0.16 & 0.08 & 0.23 & 0.26 \\ Courthouse & 0.17 & 0.12 & 0.12 & 0.08 & 0.09 & 0.13 & 0.19 \\ Ignatius & 0.83 & 0.78 & 0.72 & 0.33 & 0.04 & 0.44 & 0.61 \\ Meetingroom & 0.24 & 0.23 & 0.20 & 0.15 & 0.01 & 0.16 & 0.19 \\ Truck & 0.45 & 0.42 & 0.45 & 0.26 & 0.19 & 0.26 & 0.52 \\  Mean & 0.38 & 0.39 & 0.35 & 0.19 & 0.09 & 0.30 & 0.40 \\ Time & 24h & 24h & 24h & 1h & 14.3m & 34.2m & 53 m \\  FPS & & 10 & & - & 159 & 68 & 145 \\   

Table 1: **Quantitative results on the Tanks and Temples Dataset .** Reconstructions are evaluated with the official evaluation scripts and we report F1-score, average optimization time and FPS. Ours outperforms all 3DGS-based surface reconstruction methods by a large margin and performs better than neural implicit methods by a minor margin while optimizing significantly faster.

Figure 4: **Illustration of the rationals behind the densification and splitting strategies.** (a) Comparison between large and small Gaussians of depth errors caused by a small normal error (in side view). (b) Comparison of the original and the proposed splitting strategies (in bird-eye view).

**Dataset.** We evaluate the performance of our method on various datasets. For surface reconstruction, we evaluate on Tanks and Temples (TNT) . To further validate the effectiveness of our method, we compare with other methods on Replica . Although we focus on the large-scale reconstruction, we also report our results on DTU , which can be seen in the supplementary. Furthermore, we evaluate the rendering results on Mip-NeRF360 . For all the datasets, we use COLMAP  to generate a sparse point cloud for each scene as initialization.

    &  &  &  \\  & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LIPPS \(\) \\  NeRF & 21.46 & 0.458 & 0.515 & 26.84 & 0.790 & 0.370 & \\ Deep Blending & 21.54 & 0.524 & 0.364 & 26.40 & 0.844 & 0.261 & \\ Instant NGP & 22.90 & 0.566 & 0.371 & 29.15 & 0.880 & 0.216 & 10 \\ MERF & 23.19 & 0.616 & 0.343 & 27.80 & 0.855 & 0.271 & \\ MipNeRF360 & 24.47 & 0.691 & 0.283 & 31.72 & 0.917 & 0.180 & \\  Mobile-NeRF & 21.95 & 0.470 & 0.470 & - & - & - & - \\ BakedSDF & 22.47 & 0.585 & 0.349 & 27.06 & 0.836 & 0.258 & 100 \\  
3DGS & 24.64 & 0.731 & 0.234 & 30.41 & 0.920 & 0.189 & 134 \\ SuGaR & 22.93 & 0.629 & 0.356 & 29.43 & 0.906 & 0.225 & - \\
2DGS & 24.21 & 0.709 & 0.276 & 30.10 & 0.913 & 0.211 & 27 \\ Ours & 24.31 & 0.707 & 0.280 & 30.53 & 0.921 & 0.184 & 128 \\   

Table 2: **Quantitative results on Mip-NeRF 360 .** Our method achieves NVS rendering quality and speed comparable with other Gaussian-based methods.

Figure 5: **Qualitative comparison on TNT dataset.** From top to bottom, we show the reconstructed meshes from our method, SuGar, 2DGS, and NeuS, as well as the ground truth colored point cloud. Our method reconstructs more complete surfaces featuring smoother planar regions and finer details.

[MISSING_PAGE_FAIL:9]

### Ablation Studies

We verify the effectiveness of different design choices on reconstruction quality, including regularization terms, intersection depth, and densification on the TNT dataset  and report the F1-score. We first examine the effect of our view-consistent D-Normal regularization. Our full model (Tab. 4 E) provides the best performance (0.40 F1-score). The performance drops 0.10 F1-score from 0.4 to 0.3 without the D-Normal regularizer (Tab. 4 A) while keeping rendered normal regularization. It proves that it is insufficent to supervise only the normal maps rendered from Gaussian Splatting. The visualization in Fig. 6 demonstrates that our d-normal regularization can effectively push the 3D Gaussians towards the surface. Furthermore, the result drops by 0.04 F1-score without the confidence (Tab. 4 B) and with the D-Normal regularizer. It demonstrates that confidence can mitigate the problem of inconsistency of the predicted normal maps. From Fig. 7, we can observe that disabling the confidence leads to an unsmooth surface. Both of these validate the effectiveness of the view-consistent D-Normal regularization. Additionally, the absence of intersection depth (Tab. 4 C) results in poor performance. Lastly, the performance increases from 0.33 F1-score to 0.40 with our densification and split (Tab. 4 D), proving small Gaussians represent surfaces better than large Gaussians.

## 5 Conclusion

In this work, we have introduced a view-consistent D-Normal regularizer for efficient, high-quality, and compact surface reconstruction. We formulate the D-Normal regularizer that directly couples normal with the other geometric parameters. This allows for the full update of all geometric parameters during normal regularization. We also propose a confidence term that weighs our D-Normal regularizer to mitigate inconsistencies of normal predictions across multiple views. Finally, we introduce a densification and splitting strategy to regularize the scales and distribution of 3D Gaussians for more precise surface modeling. Our evaluations on diverse datasets demonstrate that our method outperforms existing works in surface reconstruction.

**Acknowledgement.** This research / project is supported by the National Research Foundation (NRF) Singapore, under its NRF-Investigatorship Programme (Award ID. NRF-NRFI09-0008).

   Ablation Item & Precision \(\) & Recall \(\) & F-score \(\) \\  A. w/o D-Normal & 0.27 & 0.34 & 0.30 \\ B. w/o confidence & 0.36 & 0.37 & 0.36 \\ C. w/o intersection depth & 0.35 & 0.37 & 0.35 \\ D. w/o densify and split & 0.32 & 0.35 & 0.33 \\  E. Full & **0.39** & **0.42** & **0.40** \\   

Table 4: **Ablation on TNT . Bold** indicates best result.

Figure 7: **Qualitative ablation for the confidence. Without the confidence weight, the reconstructed surface shows protrusions caused by the inconsistent pseudo normal maps across different views.**