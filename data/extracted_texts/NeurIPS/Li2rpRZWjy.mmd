# Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts

Anna Meszaros

University of Cambridge, Cambridge, United Kingdom

Szilvia Ujvary

Wieland Brendel

Patrik Reizinger

Joint senior authors. Correspondence to am3049@cam.ac.uk. Code available at: github.com/meszarosanna/rule_extrapolation

Ferenc Huszar

Fichen Huszar

Joint senior authors. Correspondence to am3049@cam.ac.uk. Code available at: github.com/meszarosanna/rule_extrapolation

###### Abstract

LLMs show remarkable emergent abilities, such as inferring concepts from presumably out-of-distribution prompts, known as in-context learning. Though this success is often attributed to the Transformer architecture, our systematic understanding is limited. In complex real-world data sets, even defining what is out-of-distribution is not obvious. To better understand the OOD behaviour of autoregressive LLMs, we focus on formal languages, which are defined by the intersection of rules. We define a new scenario of OOD compositional generalization, termed _rule extrapolation_. Rule extrapolation describes OOD scenarios, where the prompt violates at least one rule. We evaluate rule extrapolation in formal languages with varying complexity in linear and recurrent architectures, the Transformer, and state space models to understand the architectures' influence on rule extrapolation. We also lay the first stones of a normative theory of rule extrapolation, inspired by the Solomonoff prior in algorithmic information theory.

## 1 Introduction

Autoregressive language models (AR LMs) can reach both low training and test loss, but even minimal test loss is not predictive for out-of-distribution (OOD) model performance , i.e. when the test data has vanishing probability under the training distribution. Despite the success of deploying modern language models in OOD situations, OOD generalization is not well understood theoretically. Recently, studies started to focus on a specific form of OOD generalization: compositional generalization in language models . To systematically examine compositional generalization of AR LMs, we study a particular notion of OOD generalization, which we call rule extrapolation.

_Rule extrapolation is a form of compositional generalization: it studies OOD behavior of language models trained on formal languages defined by a logical conjunction of rules._

For example, the \(a^{n}b^{n}\) language is the intersection of two rules: (R1) the number of \(a\)'s is equal to the number of \(b\)'s and (R2) \(a\)'s precede \(b\)'s. The prompt **bbaab** cannot be completed to obey the R2, but it is still possible to satisfy (R1) (e.g., **bbaab**\(a\)). When a language model trained on an intersection of rules remains consistent with one of the rules when another is broken, we say it successfully extrapolated the rule beyond its training data.

A limited experiment by Reizinger et al. (2024) indicated that Transformers exhibit much-better-than-chance rule extrapolation performance on the formal grammar \(a^{n}b^{n}\), despite lacking any explicit inductive biases encouraging this behaviour. However, it remains unclear whether the behaviour observed was specific to the Transformer or whether it holds more generally on a wider range of formal languages. Inspired by this work, we conduct a thorough empirical investigation of the role of architecture in rule extrapolation on a range of formal languages. As a non-rigorous baseline, we also conducted a small pilot human study to understand how people would generalize the rules. We chose to study rule extrapolation because it appears to be a rational, or at least desirable, behaviour. However, we lack a normative reason why this behaviour should be considered rational. It is unclear whether any OOD behaviours could be considered rational. This question led us to investigate how a general rational algorithm for OOD prompt completion might be formalized. That is, instead of asking what models do, we ask what they _should_ do if they were to be consistent with some principles of rational inference. We turn to Algorithmic Information Theory (AIT) to formalize a normative model. We propose a non-parametric prior for next-token prediction inspired by the Solomonoff prior Solomonoff (2001); Li and Vitanyi (1997). This prior helps resolve how a rational model should behave in situations that are mathematically underspecified by their training: to extrapolate the simplest theories consistent with training data. Although, like Solomonoff's induction, our rational algorithm is uncomputable, it helps explain some of our empirical observations about rule extrapolation in practical language models. Our **contributions** are:

* We use formal languages to define scenarios for evaluating sequence models' OOD compositional generalization, which we call _rule extrapolation_ (SS 2.2);
* We empirically evaluate different models' rule extrapolation in formal languages with varying complexity, we study linear, recurrent, Transformer and State Space models. We show that there is no single architecture that emerges as a clear winner of rule extrapolation. Though Transformers fare very well in most scenarios we investigated, they struggle on regular languages (SS 4);
* Inspired by algorithmic information theory, we propose a normative theory for OOD prompt completion, which posits that rule learning and extrapolation should be governed by the relative simplicities of rules (SS 5);
* To demonstrate the presence of a similar simplicity bias in Transformers, We visualise the training dynamics enabling rule extrapolation on the \(a^{n}b^{n}\) language. We find that the model first learns a set obeying the easier rule, and then identifies the language as its subset (SS 5.3).

  
**Language** & **Category** & **Rule 1** & **Rule 2** \\  \(L_{1}=\{bo\}\) & regular & \(\#a\) even & starts with \(b\) \\ \(L_{2}=\{b^{n}a^{2m}\}\) & regular & \(\#a\) even & \(b\)’s before \(a\)’s \\ \(L_{3}=\{a^{n}b^{n}\}\) & context-free & \(\#a=\#b\) & \(a\)’s before \(b\)’s \\ \(L_{4}=\) Dyck & context-free & paired and nested \([\,]\) & paired and nested \((\,)\) \\ \(L_{5}=\{a^{n}b^{n}c^{n}\}\) & context-sensitive & \(\#a=\#b=\#c\) & \(a\)’s before \(b\)’s before \(c\)’s \\ \(L_{6}=\) CS Dyck & context-sensitive & paired \([\,]\) & paired \((\,)\) \\   

Table 1: **Formal languages used in our paper:** The languages are categorized according to the Chomsky hierarchy, and they can be considered as the intersection of two rules: (R1) and (R2)

Figure 1: **Rule extrapolation summary for all models and languages (Tab. 1):** The Transformer is the best on context-free and context-sensitive languages, whereas the LSTM and Mamba excel on regular languages. We also plot chance-level performance as gray rectangles. Mean accuracies and standard deviations (averaged over 5 seeds)

Background and related work

### Formal languages

Formal languages are linguistic constructions that simplify the study of natural languages. Their advantage is their well-defined set of symbols and rules. Although they fall short of capturing the nuances and irregularities of human languages, they are very powerful with immense practical relevance--e.g., programming languages are formal languages.

Formal languages consist of words with symbols coming from a possibly infinite alphabet. Chomsky (1956) has categorized formal languages into four types with increasing complexity: regular, context-free, context-sensitive, and recursively enumerable languages. _Regular_ languages have rules that can be expressed via regular expressions, e.g., \(L_{1}=\{b:a\}\) and \(L_{2}=\{b^{n}a^{2m}:n>0\}\). _Context-free grammars_ have rules that do not depend on the context--programming languages such as C or Python belong to this category, e.g., an if-else block in the programming language C always has the same structure. For demonstration purposes, we will use two simpler languages: \(L_{3}=\{a^{n}b^{n}:n>0\}\), \(L_{4}=\{\}\). _Context-sensitive grammars_ have rules that depend on the position in the sequence--we will use the standard example of \(L_{5}=\{a^{n}b^{n}c^{n} n>0\}\) and \(L_{6}=\{\}\). We omitted the recursively enumerable grammars similar to Deletang et al. (2022), as they require an infinite tape to simulate, which is impossible.

### Out-of-distribution (OOD) generalization

In modern deep learning theory, the test loss distinguishes the performance of models with low training loss by evaluating the model on unseen data sampled from the same distribution (i. e., i.i.d.) as the data it was trained on. When the test loss is (near-)minimal, the model has statistical generalization ability. Therefore several studies focused on establishing bounds on the generalization gap (Vapnik and Chervonenkis, 1971; Dziugaite and Roy, 2017; Perez-Ortiz et al., 2021). Along with the question of whether the test loss is sufficiently low, another one arose: does the test loss have a unique minimum? Identifiability is a property of a family of statistical models, concerning the uniqueness of the data generator model recovered from the observed data. In machine learning, identifiability implies the uniqueness of the test loss' minimum, and the model it corresponds to, which is desirable since it enables us to interpret the model and reason about its properties.

Theoretical tools such as statistical generalization or identifiability are mostly concerned about the i.i.d. scenario, i.e., when the training and test data come from the same distribution. However, this is an unrealistic assumption for language models (LMs), especially when pretrained models are used for various downstream tasks. Despite the clear OOD nature of these tasks, OOD generalization of these models is not understood theoretically. Recently, several works addressed a special type of OOD generalization called compositional generalization in vision models (Schott et al., 2021; Wiedemer et al., 2023; Brady et al., 2023; Yang et al., 2023; Lachapelle et al., 2023); however, such studies are only started emerging for natural language (Ahuja and Mansouri, 2024; Han and Pado, 2024; Ramesh et al., 2024; Lake and Baroni, 2023; Nogueira et al., 2021; Dziri et al., 2023; Saparov et al., 2023). Deletang et al. (2022) and Ruoss et al. (2023) conducted a similar experimental investigation to ours, the tasks they evaluate on are also derived from formal language recognition and thus grouped according to the Chomsky hierarchy, but they focus on length generalization.

Reizinger et al. (2024) show that despite any explicit inductive bias or regularization, Transformers can exhibit much-better-than-chance extrapolation performance on some synthetic grammars. However, it is unclear whether this behavior is specific to the Transformer and/or the formal language. Furthermore, there are some tasks such as addition and parity that are known to be very hard (or even impossible) to solve by Transformers, at least without tricks (Zhou et al., 2023). Inspired by these works, our paper investigates the role of architecture in different formal languages.

Rule extrapolation.To understand the OOD behavior in AR LMs, we study a particular notion of OOD generalization, which we term _rule extrapolation_. Rule extrapolation is a subclass of compositional generalization, for formal languages are defined by composing multiple rules. When assessing rule extrapolation, the model is pre-trained on formal language data, i.e., the support is the intersection of all language rules. Then, OOD data is presented, where a subset of rules is violated, thus having zero probability over the training distribution. If the completed OOD prompts satisfy the not violated rules, we say the model extrapolates the rules. For example, the \(a^{n}b^{n}\) language is the intersection of two rules: (R1) the number of \(a\)'s equals the number of \(b\)'s and (R2) \(a\)'s precede \(b\)'s. The prompt **bbaab** cannot be completed to obey the second rule. In this case, rule extrapolation means that the completed prompt satisfies the first rule (e.g., **bbaab\(a\)**).

### Inductive biases in sequence models

Several deep learning architectures, such as CNNs or GNNs, were designed to capture specific structural data properties. Such inductive biases in sequence models remain to be understood (Reizinger et al., 2024). McCoy et al. (2020) and Murty et al. (2023) studied whether different architectures on language processing tasks have an inductive bias towards hierarchical structure. (Murty et al., 2023) showed that with sufficient training, the transformer architecture can represent hierarchical sentence structure and use this structure to generalize correctly. Several works establish forms of simplicity bias (Valle-Perez et al., 2019; Dingle et al., 2018; Mingard et al., 2020). Goldblum et al. (2023) demonstrate that (even randomly initialized) language models are biased towards low algorithmic complexity. Weiss et al. (2021) developed a formal programming language called RASP to model the inner workings of the Transformer, whereas Zhou et al. (2023) defined a subset, called RASP-L, and proved length generalization in Transformer, emphasizing a simplicity bias in terms of RASP-L code length. Chen et al. (2024) attribute the development of grammatical capabilities to Syntactic Attention Structure (SAS), wherein specific Transformer heads tend to focus on specific syntactic relations. These approaches leverage the tools of theoretical computer science to reason about the success of Transformers, hinting at the role of a structural inductive bias. For example, in-context learning (ICL) performance depends on the ordering of layers in the Transformer (Press et al., 2020), and also the structure of the training data (Chan et al., 2022). LM inductive biases have also been studied from a mechanistic interpretability perspective. Most notably, Olsson et al. (2022) propose that ICL is due to induction heads (a type of specialised attention heads). Mechanistic interpretability approaches can also identify and disable the computational circuits responsible for bad behaviors (Li et al., 2024) and locate ones that capture factual knowledge (Meng et al., 2023). These works constitute important progress; though we take a step back to ask: is the good performance attributable to the Transformer? Are (at least some of) these emergent capabilities present in simpler models such as linear models or RNNs?

## 3 Experimental setup

### Architectures

To study when rule extrapolation emerges, we compare five architectures: linear models, LSTMs (Hochreiter and Schmidhuber, 1997), Transformers (Vaswani et al., 2023), and State Space Models (SSMs) (focusing on Momba (Gu and Dao, 2023)), and the recently introduced xLSTM (Beck et al., 2024). The Transformer (Vaswani et al., 2023) caused a breakthrough in Natural Language Processing (NLP) by introducing the (self-)attention mechanism, allowing it to capture global dependencies efficiently in both directions, unlike the standard LSTM. Adapted from dynamical systems, SSMs have recently entered language modeling, and became increasingly popular, such as this work's focus, Momba (Gu and Dao, 2023). In this architecture, the attention mechanism (where every token must "attend" to every other token) is replaced by a single SSM block, allowing the model to selectively focus on relevant information. The on-par performance of the Transformer and the SSM along with the removal of the attention block raises the question of whether the SSM also show rule extrapolation abilities. Recently, Beck et al. (2024) proposed an extension of the LSTM, which includes matrix-valued memory cells, new gating and memory mixing mechanisms, and several computational improvements. Training details, data set sizes, and model parameters are in Appx. B.

### Datasets

Our data sets follow the hierarchy of (Chomsky, 1956). The advantage of the classification is that the categories exhibit fundamental differences. However, this hierarchy is based on computational linguistics concepts. Therefore, there might be no connection between the language's complexity in the Chomsky hierarchy and what a neural network finds difficult to learn. Each language we study obeys two rules, and the OOD prompts violate the corresponding R2, but the prompt can still be completed to satisfy the other. Following (R1) and/or (R2) provide different information: following (R1) means the LM still adheres to a rule even when the other is violated (in the whole sequence), whereas adhering to (R2) on the completion shows that the LM still tries to satisfy that.

The used formal languages and their categorization and rules are included in Tab. 1. We define two rules for each language to keep the results comparable; however, we acknowledge that these can lead to rules of different complexity (cf. the chance levels for \(L_{3}\) and \(L_{5}\) in Tabs. 4 and 6), and also that the rules can potentially be defined in multiple equivalent ways.

**Regular grammars.** Regarding the hierarchy, the two simplest data sets are _regular_ languages \(L_{1}=\{b:\}\) and \(L_{2}=\{b^{n}a^{2m}:n,m>0\}\). The rules of the language \(L_{1}\) are: (R1) there are even number of \(a\)s in the sequence; and (R2) the sequence starts with a \(b\). For \(L_{1}\), the OOD prompts consist of prompts that violate (R2), i.e. start with an \(a\), but all theseprompts can be completed to satisfy (R1). For language \(L_{2}\), the rules are: (R1) there are even number of \(a\)s in the sequence; and (R2) \(b\)s precede \(a\)s. The OOD prompts for \(L_{2}\) violate (R2). They start with a single \(a\), then a block of \(b\)s and possibly a block of \(a\)s.

**Context-free grammars.** We implemented two _context-free_ grammars \(L_{3}\) and \(L_{4}\): \(L_{3}=\{a^{n}b^{n}:n>0\}\), i.e., (R1) the number of \(a\)s and \(b\)s match; and (R2) \(a\)s precede \(b\)s. For \(L_{3}\), OOD prompts violate (R2), i.e., the prompts include \(b\) tokens followed by \(a\) tokens.

Our fourth formal language is a bracketing (Dyck-) language, i.e., \(L_{4}=\{\)sequences of nested and paired parentheses and brackets\(\}\), e.g. "(\([\ \ ](\ )\))" The rules of the language are: (R1) brackets are nested and paired; and (R2) parentheses are nested and paired. Paired means that every opening bracket/parenthesis has a closing pair; nested means that between an opening and closing bracket/parenthesis, all other tokens must be paired--contrast this with \(L_{6}\). For \(L_{4}\), ID prompts begin with "(\([\)" and OOD prompts start with "\(][\)"; both are followed by a sequence where the parentheses and the square brackets are matched.

**Context-sensitive grammars.** We implemented two _context-sensitive_ grammars \(L_{5}\) and \(L_{6}\). \(L_{5}=\{a^{n}b^{n}c^{n}:n>0\}\). Though it seems very similar to \(L_{3}\), its grammar rules make it context-sensitive, i.e., the tokens generated depend on multiple tokens. The grammar rules can be summarized as: (R1) the number of \(a\)s, \(b\)s, and \(c\)s are the same; (R2) \(a\)s precede \(b\)s and \(b\)s precede \(c\)s; and The OOD prompts are sequences which violate (R2). All these prompts can still be completed to obey (R1). \(L_{6}\) is a context-sensistve Dyck-language, i.e., \(L_{6}=\{\)sequences of paired, but not necessarily nested parentheses and brackets\(\}\), e.g. "(\([\ \ ]\))" The rules of the language are: (R1) brackets are paired; and (R2) parentheses are paired. Akin to \(L_{4}\), for \(L_{6}\) ID prompts begin with "(\([\)" and OOD prompts start with "\(][\)"; both are followed by a sequence where the parentheses and the square brackets are matched.

### Metrics.

We monitor training and test loss. We evaluate the accuracy of both rules (R1/R2) separately and simultaneously both for in-distribution samples, and also for OOD prompts. As OOD prompts are designed that (R2) cannot be satisfied, we evaluate its accuracy in the most lenient way. That is, we either calculate it on the completion or, for the Dyck languages, on the part after the closing parenthesis ")". An example for the \(L_{3}\) OOD prompt **abbb** is as follows: the completion **abbb**\(aa\) is considered correct for (R2), but **abbb**\(abaa\) is not, as it has an \(a\) after a \(b\) in the _completion_. Our evaluation is restricted to prompt completions with an EOS token. We also monitor the accuracy of the next token prediction via greedy decoding (i.e., using the token with the largest probability). Our results report the minimum of the test loss to measure whether the models are in the saturation regime . We select the _largest_ values for the rule accuracies. We choose this evaluation as small variations in the test loss could lead to large deviations (as predicted by Liu et al. ). We also report chance level accuracies as a baseline, quantifying how complex a given rule is. Chance level accuracy in each case refers to the performance of a model that always predicts each token (excluding the start-of-sequence (SOS) token) as the next token with equal probability2. We report means and standard deviations across 5 seeds. Similar to , we provide a non-representative human baseline based on a small pilot study, where participants have seen three examples for \(L_{1},L_{3}\) then were asked to complete five OOD sequences for each (Appx. B.6). We corrected for invalid answers and emphasize that we only aim to provide a sense of how humans measure against neural networks, without reaching any statistical conclusions.

## 4 Results

**Regular grammars.** Perhaps surprisingly, modern architectures perform the worst on regular languages \(L_{1}\) (Tab. 2) and \(L_{2}\) (Tab. 3) : both Mamba and the Transformer are worse in- and out-of-distribution than the LSTM--the xLSTM only matches the LSTM in OOD performance on (R1). Furthermore, the Transformer's accuracies are below chance level even for in-distribution, despite having approximately the same test loss as the LSTM and Mamba. The Linear model seemingly manages to obey perfectly (R2) in-distribution on \(L_{2}\), which happens because this model only predicts EOS on test prompts, and the ID test prompt already satisfies (R2). In the other categories, Linear is at or below chance-level. In our small pilot study, humans performed akin to Mamba on \(L_{1}\) (Tab. 14). Zhou et al.  observed that Transformers struggle with addition or parity calculation, which might explain the Transformer's low performance on regular languages, as both \(L_{1},L_{2}\) require calculating the parity of \(a\) tokens.

Context-free grammars.On the context-free grammars \(L_{3},L_{4}\), the conclusion is different. On \(L_{3}\) (Tab. 4), although all four models achieve perfect accuracy on (R2) both in- and out-of-distribution, and all models except the Linear, (near) perfectly obey (R1) in-distribution, the Transformer extrapolates (R1) to the largest extent (\(66\%\)), followed by the LSTM (\(38\%\)) and Mamba (\(30\%\)). The seemingly perfect (R2) ID and OOD extrapolation for the Linear model is, again, due to EOS token generation. On the Dyck language \(L_{4}\) (Tab. 5), the Transformer has the best extrapolation performance, and Mamba is better than the LSTM. On \(L_{3}\), the human participants in our small study had performed better on following (R2) on the completion than extrapolating (R1); however, the Transformer was better than humans in extrapolating both (R1) and (R2).

Context-sensitive grammars.The grammar \(L_{5}\) (Tab. 6) is similar to \(L_{3}\), i.e., the Transformer performs best. Intuitively, the sequences in the form of \(\{a^{n}b^{n}\}\) and \(\{a^{n}b^{n}c^{n}\}\) are rather similar, despite the latter being context-sensitive in Chomsky's hierarchy. Rule extrapolation accuracies for (R1) in \(L_{5}\) are lower than for \(L_{3}\), which can be attributed to the higher complexity of (R1) in the context-sensitive grammar (cf. chance levels in Tabs. 4 and 6). For the context-sensitive Dyck language \(L_{6}\) (Tab. 7), the Transformer and LSTM perform similarly on both OOD (R1) and (R2).

Results summary.We conclude that on different grammars, different architectures perform best (Fig. 1). Although the Transformer has a consistently good performance on the investigated context-free and -sensitive grammars, LSTM and Mamba are better choices for the studied regular grammars. We hypothesize that it happens because these languages require calculating parity, in which the Transformer struggles [Zhou et al., 2023]. The xLSTM generally lies somewhere between the LSTM and the Transformer. The Linear model has very limited capabilities for modeling formal grammars as it cannot even minimize the test loss. In our small pilot study on \(L_{1},L_{3}\), humans found the tasks difficult: they performed better than chance, though the LSTM performed better on \(L_{1}\), and the Transformer on \(L_{3}\) (Tab. 14)--we emphasize that our human-machine comparison only provides intuition, rather than a rigorous evaluation of human performance, which is left for future work.

  
**Model** & **Test loss** & **ID R1** & **ID R2** & **OOD R1** & **OOD R2** completion \\  Chance & N/A & 0.105 & 0.356 & 0.154 & 0.445 \\ Linear & \(2.553_{ 0.159}\) & \(0.200_{ 0.000}\) & \(1.000_{ 0.000}\) & \(0.275_{ 0.000}\) & \(1.000_{ 0.000}\) \\ LSTM & \(0.019_{ 0.000}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & \(0.376_{ 0.209}\) & **1.000\({}_{ 0.000}\)** \\ Mamba & \(0.019_{ 0.000}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & \(0.296_{ 0.043}\) & **1.000\({}_{ 0.000}\)** \\ Transformer & \(0.022_{ 0.002}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & **0.657\({}_{ 0.862}\)** & **1.000\({}_{ 0.000}\)** \\ xLSTM & \(0.019_{ 0.000}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & \(0.438_{ 0.252}\) & **1.000\({}_{ 0.000}\)** \\   

Table 4: **Test loss and rule-following accuracies for the context-free language \(L_{3}=\{a^{n}b^{n}\}\)**: the Transformer can extrapolate (R1) the best.

  
**Model** & **Test loss** & **ID R1** & **ID R2** & **OOD R1** & **OOD R2** completion \\  Chance & N/A & \(0.473\) & \(0.250\) & \(0.500\) & \(0.750\) \\ Linear & \(1.927_{ 2.537}\) & \(0.422_{ 0.034}\) & \(1.000_{ 0.000}\) & \(0.513_{ 0.045}\) & \(0.000_{ 0.000}\) \\ LSTM & \(0.037_{ 0.000}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & \(0.000_{ 0.000}\) \\ Mamba & \(0.038_{ 0.000}\) & \(0.901_{ 0.088}\) & **1.000\({}_{ 0.000}\)** & **0.959\({}_{ 0.847}\)** & **0.073\({}_{ 0.129}\)** \\ Transformer & \(0.039_{ 0.000}\) & \(0.158_{ 0.357}\) & \(0.182_{ 0.405}\) & \(0.067_{ 0.214}\) & \(0.000_{ 0.000}\) \\ xLSTM & \(0.037_{ 0.000}\) & \(0.833_{ 0.408}\) & \(0.833_{ 0.408}\) & **1.000\({}_{ 0.000}\)** & \(0.000_{ 0.000}\) \\   

Table 3: **Test loss and rule-following accuracies for the regular language \(L_{2}=\{b^{n}a^{2m}\}\)**: the LSTM and the xLSTM can extrapolate (R1) the best, closely followed by Mamba

## 5 Normative theory of OOD prompt completion

The previous sections empirically assessed an example of rational OOD prompt completion: rule extrapolation. In this section, instead of asking what happens, we take a step back to ask what _should_ happen: how an ideal model should learn and extrapolate rules. We propose a non-parametric prior and prediction scheme for OOD prompt completion, that can be seen as a generalization of Solomonoff induction (Solomonoff, 2001; Li and Vitanyi, 1997) to settings relevant for AR LMs. Although our algorithm, just like Solomonoff induction, is uncomputable, we argue that it formalises a rational approach capable of OOD extrapolation in AR sequence models. Rather than a practical algorithm itself, it should be interpreted as a guide towards building and assessing future practical models. Our conceptual approach is not without precedent: ideas from AIT have recently been popularized as "North Stars" for guiding practical implementations (Theis, 2024; Goldblum et al., 2023), and have been applied in practical algorithms (Grau-Moya et al., 2024).

We first introduce our approach on the high-level, via the following story.

**A story of OOD prompt completion.** Suppose that Bob has a Language Model \(p_{}\), that autoregressively generates \(M\) i.i.d. sequences of length \(m\), \(\{(x_{1,j},x_{2,j}, x_{m,j})\}_{j=1}^{M}:=(x_{1j}^{m})_{i=1}^{M}\). Since the sequences are generated autoregressively, we may call \((x_{1j}^{m-1})_{j=1}^{M}\) the _ID prompts_, and each \(m^{th}\) element \((x_{m,j})_{j=1}^{M}\) its _ID completions_. Suppose that Charlie, Bob's enemy, generates a \(n-\)length sequence from the same LM, and intervenes (in the causal sense) on it, so that the resulting sequence \((x_{1},x_{2}, x_{n-1}):=x_{1}^{n-1}\) has zero probability under the LM. We call this the _OOD prompt_. Despite \(p_{}(x_{1}^{n-1})=0\), the LM still defines the conditional probability of completing the OOD prompt \(x_{1}^{n-1}\). Charlie then asks an observer, Alice, to predict how Bob's LM will complete the OOD prompt \(x_{1}^{n-1}\), i.e., what \(x_{n}\) will be. Fig. 2 shows the probabilistic assumptions of Alice: the completions are generated independently, according to the same procedure (i.e., using the same LM). We use the conditional independence assumption \(x_{n}(x_{1}^{m})_{j=1}^{M} x_{1}^{n-1}\) in eq. (4) below.

  
**Model** & **Test loss** & **ID R1** & **ID R2** & **OOD R1** & **OOD R2** completion \\  Chance & N/A & \(0.127\) & \(0.127\) & \(0.127\) & \(0.382\) \\ Linear & \(4.013_{ 0.254}\) & \(0.000_{ 0.000}\) & \(0.000_{ 0.000}\) & \(0.000_{ 0.000}\) & \(1.000_{ 0.000}\) \\ LSTM & \(0.645_{ 0.019}\) & **0.981\({}_{ 0.042}\)** & **0.956\({}_{ 0.041}\)** & **1.000\({}_{ 0.000}\)** & **0.894\({}_{ 0.145}\)** \\ Mamba & \(0.675_{ 0.018}\) & \(0.745_{ 0.076}\) & \(0.807_{ 0.185}\) & \(0.684_{ 0.159}\) & \(0.810_{ 0.212}\) \\ Transformer & \(0.640_{ 0.016}\) & **1.000\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** & **0.980\({}_{ 0.048}\)** & **0.973\({}_{ 0.044}\)** \\ xLSTM & \(0.671_{ 0.021}\) & \(0.791_{ 0.179}\) & \(0.765_{ 0.155}\) & \(0.767_{ 0.158}\) & \(0.715_{ 0.121}\) \\   

Table 6: **Test loss and rule-following accuracies for the context-sensitive language \(L_{5}=\{a^{n}b^{n}c^{n}\}\)**: the Transformer can extrapolate (R1) the best

  
**Model** & **Test loss** & **ID R1** & **ID R2** & **OOD R1** & **OOD R2** completion \\  Chance & N/A & \(0.127\) & \(0.127\) & \(0.127\) & \(0.382\) \\ Linear & \(6.145_{ 0.647}\) & \(0.000_{ 0.000}\) & \(0.000_{ 0.000}\) & \(0.000_{ 0.000}\) & \(1.000_{ 0.000}\) \\ LSTM & \(0.266_{ 0.014}\) & \(0.961_{ 0.075}\) & \(0.969_{ 0.050}\) & \(0.543_{ 0.282}\) & **1.000\({}_{ 0.000}\)** \\ Mamba & \(0.277_{ 0.014}\) & \(0.697_{ 0.152}\) & \(0.607_{ 0.140}\) & \(0.644_{ 0.164}\) & \(0.886_{ 0.129}\) \\ Transformer & \(0.273_{ 0.018}\) & **0.974\({}_{ 0.480}\)** & **0.973\({}_{ 0.199}\)** & **0.980\({}_{ 0.000}\)** & **1.000\({}_{ 0.000}\)** \\ xLSTM & \(0.273_{ 0.013}\) & \(0.706_{ 0.116}\) & \(0.665_{ 0.155}\) & \(0.689_{ 0.164}\) & **0.991\({}_{ 0.018}\)** \\   

Table 5: **Test loss and rule-following accuracies for the context-free Dyck language \(L_{4}\)**: the Transformer can extrapolate (R1) the best.

In the rest of this section, we construct an algorithmic prior that formalizes these assumptions, and argue why it is a promising approach to study OOD compositional generalization theoretically.

### The Solomonoff prior

The Solomonoff prior assigns a prior probability to individual data points based on some algorithmic notion of how difficult it is to generate that data point. It embodies Occam's razor and Epicure's principle, as simple data points have a larger probability, and every possible explanation is included in the prior (see also Appx. C.2). For simplicity, we define the Solomonoff prior for discrete sample spaces, though similar arguments hold for the continuous case. To encourage readability, we define technical terms in Appx. C.2, and highlight them in blue here. Let us fix a monotone universal Turing machine (UTM). Solomonoff's universal prior  is defined over arbitrary-length sequences \(x_{1}^{N}:=(x_{1},x_{2},,x_{N})\) as

\[p_{S}(x_{1}^{N})=(p_{i})p_{i}(x_{1}^{N}),\] (1)

where we sum over all discrete lower semicomptable semimeasures \(p_{i}(x_{1}^{N})\) implementable on the UTM . We will refer to the \(p_{i}(x_{1}^{N})\) as mixture components or _explanations_ of the data. The prior on weights \((p_{i})\) is an arbitrary semimeasure, i.e., \( i:\ (p_{i})>0\) and \(_{i}(p_{i}) 1\). Frequently, \((p_{i})\) is chosen as \(2^{-K(p_{i})}\), the prefix Kolmogorov complexity of \(p_{i}\) in the UTM (see Defn. C.5 in Appx. C.2).

**Predictive form.** The above formulation of the Solomonoff prior has the predictive form [13, Chapter 3.2.3], where \((p_{i} x_{1}^{N-1})\) is updated via Bayesian inference:

\[p_{S}(x_{N} x_{1}^{N-1})=_{i}(p_{i} x_{1}^{N-1})p_{i}(x_{N}  x_{1}^{N-1}),(p_{i} x_{1}^{N-1})=)p_{i}(x_{1}^{N-1})}{p_{S}(x_ {1}^{N-1})}\] (2)

**Convergence of predictions.** Suppose that the true distribution of \((x_{1},x_{2},,x_{N})\) is \(\). The Solomonoff prior (with any valid sequence of weights) satisfies .

\[p_{S}(x_{N} x_{1}^{N-1})(x_{N} x_{ 1}^{N-1})-1.\] (3)

### A predictive model for OOD prompt completion

Our goal is to define a similar prior, and predictive scheme that fits our scenario of AR next-token prediction, and where we can express the notion of completing an out-of-distribution prompt \(x_{1}^{n-1}\), even when our prior assigns zero probability to the prompt.

The Solomonoff prior assigns nonzero prior mass to every possible prompt, i.e. there exist no OOD problems for the Solomonoff prior, as each possible test distribution is included in the prior as a mixture component \(p_{i}\). However, by definition, the Solomonoff prior can only take in a single sequence \(x_{1}^{n}\). This means that it can only model pre-training and (OOD) testing together, since the pre-training and testing data need to be concatenated into the same sequence . Intuitively, it is more natural to separate those processes. To achieve this, we propose an adapted version of the Solomonoff prior, modifying it two ways, and justifying our approach below:

Figure 2: Graphical model representing our approach for OOD prompt completion. Although Bob’s LM \(p_{}\) assigns zero probability to the OOD prompt, it defines a conditional probability distribution for its completions. Our probabilistic model assumes that Bob’s LM completes the ID and OOD prompt independently, according to the same procedure (e.g. the same LM architecture and parameters are used for generating the completions). This is the same as assuming that the Markov factors marked in blue are the same, i.e. \(p(|,\,p_{})=p(| ,\,p_{})\), and the conditional independence OOD completion \(\) ID prompt \(|\) OOD prompt.

1. We condition the prediction on a pre-training dataset \(\) of \(M\) independent and identically distributed (i.i.d.) sequences of finite length \(m\), i.e. \(=\{x_{1j}^{m}\}_{j=1}^{M}\). \(\) is sampled from the distribution \(p_{}^{M}()=_{j=1}^{M}_{k=2}^{m}p_{}(x _{k,j} x_{1,j}^{k-1})\). For simplicity, we assume that each pre-training datapoint has equal length \(m\).
2. Instead of modelling semimeasures as joints over sequences \(\{(x_{1}^{N})\}_{N}\), we model semimeasures as lists of conditionals, just as how AR LMs model probability distributions over \(\{(x_{k} x_{1}^{k-1})\}_{k=2}^{N}\), enumerating them with index \(i=1,2,\), denoting each semimeasure as \(p_{i|}\) to emphasize the lists of conditionals representation. That is, \(p_{i|}(x_{k} x_{1}^{k-1})\) and \(p_{i|}()\) mean \(p_{i}(x_{k} x_{1}^{k-1})\) and \(p_{i}()=_{j=1}^{M}_{k=2}^{m}p_{i}(x_{k,j} x_{1,j}^{k- 1})\), respectively. Note that the pre-training distribution \(p_{}\) also belongs to the set of \(p_{i|}\). We define a mixture over all lists of discrete lower semicomputable semimeasures \(p_{i|}\) implementable on the UTM See Appx. C.1 for details.

The motivation for modelling \(x_{1}^{N}\) as a list of conditionals is because the mapping from lists of conditional factorizations to joint semimeasures consistent with them is a many-to-one mapping, because zero-probability sequences have multiple factorizations (see Appx. C.1 for justification and more details on this notation). If the prompt \(x_{1}^{n-1}\) comes from a distribution different from \( p_{}^{M}\), that assigns zero probability mass to \(x_{1}^{n-1}\), the probability \(p_{}(x_{n} x_{1}^{n-1})\) is left undefined if only the joint probability \(p_{}(x_{1}^{n})\) is specified. This is not a problem in the Solomonoff prior, as it assigns nonzero probability mass to every (computable) sequence. But once we introduce the conditioning on \(\), this step becomes necessary. The above two modifications generalize the predictive form of the Solomonoff prior as follows (we color-code the equation denoting modification (i) in red and modification (ii) in \(\)):

\[p_{R}(x_{n} x_{1}^{n-1},):=_{i}(p_{i|})p_{i|}(x_{n} x_{1}^{n-1}),(p_{i|})=)p_{i|}()}{p_{}()}.\] (4)

**Interpreting \(p_{R}\).**  Starting from a prior weight \(\) over all possible explanations \(p_{i|}=\{p_{i}(x_{k} x_{1}^{k-1})\}_{k=2}^{n}\), the posterior probability of \(p_{i|}\) given \(\) is computed (eq. (4), right). The \(n^{th}\) step prediction by \(p_{i}\), conditioned on a possibly OOD test prompt, is then weighted by this posterior. It is important that the prediction \(p_{i|}(x_{n} x_{1}^{n-1})\) is _not_ conditioned on the pre-training data \(\), and the posterior \((p_{i|})\) is _not_ conditioned on the test prompt \(x_{1}^{n-1}\). This, as stated above, separates pre-training from testing, enabling us to define the completion of OOD test prompts. When \(\) equals \(x_{1}^{n-1}\), \(p_{R}\) reduces to \(p_{S}\), and thus the posterior prediction converges according to eq. (3).

**Choice of the weight prior \((p_{i|})\).**  For OOD test prompts, there are multiple explanations \(p_{i|}\) consistent with \(\). Therefore, the behaviour of \(p_{R}\), even when \(||\) tends to infinity, depends on the prior weight \((p_{i|})\). This differs from the Solomonoff prior, which converges to the true posterior regardless of the weights (eq. (3)) [Hutter, 2005]. Thus, \(\) must be chosen to allow the extrapolation of simple explanations consistent with the data. We define \((p_{i|}):=2^{-K(p_{i|})}\), penalising exponentially the length of the shortest program (implemented on the fixed UTM) \(K(p_{i|})\) that can approximate \(p_{i|}\) (each conditional probability) for every prompt \(x_{1}^{n}\). This encodes Occam's razor into the prior, and is consistent with the optimal weights of the Solomonoff prior [Hutter, 2005].

### Towards explaining training dynamics and rule extrapolation

Here, we argue informally that our normative algorithm provides a notion of a rational pre-training process, and thus helps explain the training dynamics of practical LMs, and is also capable of rule extrapolation. We support our arguments by showing the role of simplicity bias (towards low Kolmogorov complexity) in the dynamics of learning the \(a^{n}b^{n}\) language with Transformers.

**Explaining training dynamics.**  We analize the dynamics of learning rule extrapolation. We report results on the Transformer (training dynamics of Mamba and the LSTM are in Appx. A), trained on the \(a^{n}b^{n}\) language--where high rule extrapolation ability is achieved. Fig. 3 shows that first, the model learns the sequences obeying (R2), then it learns the language (R1) \(\) (R2) as its subset.

We argue that the order in which rules are learnt is governed by the relative simplicity of the rules, quantified by Kolmogorov complexity. Given a formal language with rules (R1) and (R2), let \(p_{1}\), \(p_{2}\) and \(p_{1,2}\) be distributions defined by LMs that generate sequences that satisfy (R1), (R2) and (R1) \(\) (R2), respectively. If, e.g., \(K(p_{2}) K(p_{1,2})\), our normative algorithm will first learn (R2), and then learn the (R1) \(\) (R2) as its subset. In the \(a^{n}b^{n}\) language, (R2) (\(a\)'s before \(b\)'s), is, on average, simpler to generate than (R1) (#_a_#_b_) and (R1) \(\) (R2). Therefore, we expect our normative algorithm to first learn (R2), and then learn (R1) \(\) (R2) as its subset. Remarkably, our Transformer employs the same strategy ( Fig. 3), verifying the presence of simplicity bias. This result is matches past observations that Transformers are biased towards low Kolmogorov complexity (Goldblum et al., 2023).

Towards explaining rule extrapolation.Our normative algorithm has been designed to complete OOD prompt based on the simplest explanations consistent with the pre-training data. On the high level, this approach is consistent with rule extrapolation. We conjecture that approximating our normative algorithm similarly to the approach of Grau-Moya et al. (2024), will result in models with superior rule extrapolation properties. We leave this promising direction to future work.

## 6 Discussion

Conclusion.We argue that focusing on rule extrapolation and formal languages gives us sound (theoretical) tools to analyze and better understand out-of-distribution behaviour in language models, such as the role of different architectures. Our empirical findings emphasize that no single universal architecture exists for autoregressive sequence modeling. Though Transformers fare very well in most scenarios we investigated, they struggled on regular languages. Therefore, we argue that the architecture's inductive bias should be considered when selecting models since the architecture that performs the best depends on the nature of the task. Furthermore, we analyse the training process enabling rule extrapolation, we find that the model first identifies the whole set obeying one of the rules, then it learns the language (intersection of all rules) as its subset. Beyond advancing our empirical understanding, we also proposed a normative theory of OOD prompt completion. Our normative algorithm predicts the next token based on simple explanations consistent with the data, and allows us to explain and contextualise some of our empirical observations.

Impact.Rule extrapolation is a special case of compositional generalization in language models. While other OOD generalisation types were examined previously, this is the first work studying rule extrapolation. This novel concept has the potential to impact LLM research both on conceptual and practical levels. General compositional generalization notions examine whether from learning multiple concepts/rules separately, the model can understand the composition of the concepts/intersection of the rules. However, in rule extrapolation, we measure the reverse direction: from the composition/intersection, can the model identify the concepts/rules separately? Importantly, this direction is less straightforward. Rule extrapolation allows for easy study of compositional generalisation ability on a variety of datasets, such as formal or programming languages. Therefore rule extrapolation has the potential to become an established benchmark task for evaluating current and future LM architectures.

Limitations.We defined and empirically evaluated rule extrapolation in simple formal languages, where analysis is tractable and demonstrates that models can "go beyond" their training data. We acknowledge that our data sets are far from natural language where rule extrapolation may be difficult to demonstrate. Studying formal languages may still have practical relevance, e. g. for programming languages or formal mathematics. Even though we considered different hyperparameter setups presented in the appendix, we have not performed exhaustive ablations over the hyperparameters or analysis of architectures. Furthermore, model variants, like different attention or positional encoding, may impact our findings.

Figure 3: **Training dynamics of rule learning for a Transformer trained on the \(a^{n}b^{n}\) language:** we color-code the log probability of all sequences of length \(8\) consisting of \(a\)’s and \(b\)’s and ending with EOS at initialization (**left**_left_), during (**left**_middle_) and after training (**left**_right_). The sequences are separated according to which rule they obey. While at initialization, the probabilities are distributed roughly evenly, during training the model starts to assign higher probabilities to sequences satisfying (R2). After training the most likely sequences are the ones in (R1) \(\) (R2), the others are negligible. The same trend can be seen on the **right**, where the normalized sum of the probabilities of the four categories (satisfying (R1) and (R2), only (R1), only (R2) and neither) is plotted during training.