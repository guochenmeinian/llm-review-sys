# EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries

Sunjun Kweon\({}^{1}\), Jiyoun Kim\({}^{1}\), Heeyoung Kwak\({}^{2,3}\), Dongchul Cha\({}^{2,4}\),

**Hangyul Yoon\({}^{1}\)**, Kwanghyun Kim\({}^{5}\), **Jeewon Yang\({}^{1}\)**, **Seunghyun Won\({}^{6}\)**, **Edward Choi\({}^{1,}\)**

KAIST\({}^{1}\)  NAVER Digital Healthcare LAB\({}^{2}\)  Naver Cloud\({}^{3}\)

NAVER Healthcare LAB\({}^{4}\)  Ewha Womans University College of Medicine\({}^{5}\)

Seoul National University Bundang Hospital\({}^{6}\)

{sean0042,jiyoun.kim,edwardchoi}@kaist.ac.kr\({}^{1}\)

These authors contributed equallyCorresponding author

###### Abstract

Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce **EHRNoteQA**, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers ten diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (_e.g., the length and number of discharge summaries_). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman(\(\)): 0.78, Kendall(\(\)): 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings. EHRNoteQA is publicly available under PhysioNet credential access at https://doi.org/10.13026/acga-ht95, and the code is available at https://github.com/ji-youn-kim/EHRNoteQA.

## 1 Introduction

Discharge summaries are clinical notes in Electronic Health Records (EHRs) written by healthcare professionals upon patient discharge, encapsulating a patient's overall medical course from admission to discharge. These documents provide crucial patient information such as medical events, diagnoses, and treatments, which are vital for healthcare professionals in making informed clinical decisions, particularly in scenarios such as patient readmission and transitions of care (i.e., patient handoffs) [5; 7]. Healthcare professionals often need to refer to and compare information across multiple discharge summaries to gain a comprehensive understanding of the patient's medical history, especially when a patient is admitted multiple times. Nevertheless, the length and complexity of discharge summaries often make it challenging to extract necessary information efficiently , a problem exacerbated when dealing with accumulated discharge summaries across multiple patient admissions.

Recent advancements in Large Language Models (LLMs) present a promising solution to this challenge. With their capability to efficiently analyze extensive and complex information within EHRs [3; 16; 18; 47; 63; 67], LLMs have the potential to serve as question-answering (QA) agents within healthcare institutions, assisting healthcare professionals in obtaining answers to queries regarding patient discharge summaries. However, prior to deploying LLMs in such practical scenarios, it is necessary to develop a benchmark to assess the performance of LLMs in this specific context.

While several clinical benchmarks [23; 43; 24; 19] have been recently employed to evaluate LLMs in the clinical domain [50; 51; 40; 33; 55; 41; 8], they primarily focus on general medical questions (_e.g., "What is the treatment for adenomyosis?"_), without addressing practical use cases involving specific patient records. Furthermore, although QA datasets based on EHR discharge summaries exist [44; 65; 15; 38], they fall short in reflecting the diverse real-world scenarios in which physicians inquire on a patient's discharge summary. These are the following reasons: 1) the datasets only consist of questions that require information within a single note (_e.g., "What is the dosage of Nitroglycerin?"_), lacking questions that necessitate information across multiple notes (_e.g., "How did the dosage of Nitroglycerin change between visit x and y?"_) 2) most of the datasets are constructed based on predefined clinical annotations (e.g., i2b2 ) or with a predetermined topic focus (e.g., relations, drugs), resulting in a constrained scope of question topics. These limitations underscore the need for a new benchmark to effectively evaluate LLMs in answering queries posed by healthcare professionals in actual clinical practice.

To this end, we present **EHRNoteQA**, a novel benchmark to evaluate LLMs in real-world clinical scenarios for answering clinicians' questions regarding patient discharge summaries. EHRNoteQA is built upon the MIMIC-IV EHR , and consists of 962 different QA pairs each linked to distinct patients' discharge summaries. Each QA pair is initially generated using GPT-4 and then carefully reviewed and refined individually by three clinicians to ensure clinical relevance. EHRNoteQA reflects real-world clinician queries for the following reasons: 1) it includes questions that require information across multiple discharge summaries (_e.g., "How did the post-discharge medication regimen change between visits x and y?"_), 2) the questions span a diverse set of 10 topics (_i.e., treatment, assessment, problem, etiology, sign/symptom, vitals, test results, history, instruction, plan_). For each question, EHRNoteQA includes a single complete answer along with four distractor answer choices, enabling evaluation of LLMs in both open-ended and multi-choice formats. For both formats, we provide a reliable evaluation method for measuring LLM performance.

We evaluate 27 LLMs using EHRNoteQA and analyze various factors that influence model performance, such as the length and number of discharge summaries. Furthermore, to ensure practical relevance in real clinical scenarios, we conduct additional experiments to validate EHRNoteQA as a reliable proxy for actual expert evaluation; we first have the clinicians manually evaluate the LLM performance on discharge summary-based questions asked by unknown clinicians (to avoid any bias). Then we measure the correlation between this performance, and automatically-evaluated LLM performance on diverse benchmarks including EHRNoteQA. The results show that LLM scores on EHRNoteQA have a higher correlation with clinician-evaluated LLM scores (Average: Spearman(\(\)): 0.78, Kendall(\(\)): 0.62) compared to other benchmarks, highlighting its practical relevance in evaluating LLMs in real clinical settings.

## 2 Related Works

General Clinical Benchmark.Recent studies [50; 51; 40; 33; 55; 41; 8] have employed various benchmarks to assess the performance of LLMs in the clinical domain. Benchmarks such as MedQA  and MedMCQA  consist of questions sourced from medical licensing exams. PubMedQA  comprises QA pairs formulated from PubMed articles, with questions derived from their titles. Furthermore, MMLU  is a multi-task test set of various topics, including clinical subjects such as medical genetics and professional medicine, sourced from textbooks and standardized tests. While these benchmarks may be useful for assessing the clinical domain knowledge of LLMs, they are not sufficient for evaluating the practical capabilities of models in assisting healthcare professionals in real-world clinical settings . Specifically, tasks such as extracting and interpreting information from individual patient records are not included in these benchmarks. Therefore, achieving high performance on these benchmarks does not ensure that a model will perform well in these realistic scenarios unless explicitly evaluated in such settings.

Discharge Summary QA Datasets.Previous works have explored patient-specific QA using EHR discharge summaries (see Table 1 for comparison). Pampari et al.  introduced emrQA, the first public clinical note QA dataset generated from expert-annotated question templates and i2b2 annotations [58; 57; 59; 60]. Yue et al.  proposed a dataset built on MIMIC-III discharge summaries  by extracting answer evidence, followed by generating corresponding questions using a neural question generator model. Additionally, Fan  and Moon et al.  proposed discharge summary-based QA datasets on why-questions and drug-reason relations, respectively.

However, these existing datasets fall short in reflecting the complexity and diversity of real-world questions posed by physicians. Firstly, they only address questions based on a single note, neglecting the frequent clinical need to reference multiple discharge summaries for patients with multiple admissions. Secondly, the question topics are constrained. For example, emrQA is confined to topics within i2b2 annotations, such as smoking, medication, obesity, and heart disease. Although Yue et al.  aimed to increase diversity, Lehman et al.  noted that 96% of the questions still follow emrQA's templates, indicating continued limitation. Fan  focused solely on why-questions, limiting the scope of topics, while Moon et al.  centered on drug-reasoning based on n2c2 annotations .

  
**Dataset** & **Questions** & **Documents** & **Patients** & **Answer Format** & **Grounded Documents** & **Topics** \\  Pampari et al.  & 73,111 & 303 & 303 & Text Span & Single & 5 \\ Fan  & 245 & 138 & 138 & Text Span & Single & 1 \\ Yue et al.  & 1,287 & 36 & 36 & Text Span & Single & 5 \\ Moon et al.  & 96,939 & 505 & 505 & Text Span & Single & 1 \\  EHRNoteQA (Ours) & 962 & 1,659 & 962 & Multi-Choice \& Open-Ended & Multiple & 10 \\   

Table 1: Comparison of EHRNoteQA with existing discharge summary QA datasets.

Figure 1: Overview of the EHRNoteQA dataset construction process. It involves (1) patient sampling from the MIMIC-IV EHR database, (2) initial data generation using GPT-4, (3) clinician removal of improper data, and (4) clinician modification of the data.

## 3 EHRNoteQA

### Patient Filtering & Sampling

We construct EHRNoteQA (see Figure 1) using discharge summaries from the MIMIC-IV EHR database , containing anonymized patient records from Beth Israel Deaconess Medical Center (BIDMC) between 2008 and 2019. The MIMIC-IV database includes a total of 331,794 discharge summaries for 145,915 unique patients, averaging 2.3 notes (admissions) per patient. However, these discharge summaries are often lengthy, with the average length of a single patient's accumulated discharge summaries around 8,000 tokens. This poses a challenge for current LLMs, as few can process contexts longer than 8,000 tokens.

To address this, we first preprocess the notes by removing excessive whitespace, reducing the average length by 10% (see Appendix C.1). We then categorize patients by the length of their accumulated discharge summaries to match the context-length of existing LLMs 3. Specifically, we divide the patients into two groups: Level 1, consisting of patients whose accumulated discharge summary length does not exceed 3,000 tokens, suitable for models that can handle up to 4,000 tokens, and Level 2, consisting of patients whose accumulated discharge summary length ranges from 3,000 to 7,000 tokens, suitable for models that can handle up to 8,000 tokens. Each group includes a 1,000-token buffer to accommodate additional prompts and outputs beyond the discharge summary text. Together, these two groups cover about 70% of the patients in the MIMIC-IV database.

As shown in Table 2, the first group (Level 1) includes patients who had been admitted either one or two times, as indicated by the number of discharge summaries. The second group (Level 2) includes patients who had been admitted one to three times. We randomly sample a total of 1,000 patients--550 from Level 1 and 450 from Level 2--and use their discharge summaries for the next step.

### Initial Data Generation

Based on the discharge summaries sampled from Section 3.1, we utilize GPT-4  to generate the initial draft of EHRNoteQA. For each patient's accumulated discharge summaries, GPT-4 is tasked to generate the initial QA data, including 1) a clinically meaningful question that clinicians might ask regarding the patient's discharge summaries, 2) the complete answer to the question, and 3) four incorrect answer options (see Figure 1 bottom right for an example). This data construction enables EHRNoteQA to evaluate LLMs in two ways: through an _open-ended method_ in which the model's free-form answer to the question is evaluated, and through a _multi-choice method_ in which the selected answer choice is evaluated. Note that we use GPT-4 on Azure's HIPAA-compliant platform (see Appendix B.2), adhering to the MIMIC-IV data usage regulations. Detailed prompts and costs can be found in the Appendix C.2.

   &  &  &  \\ 
**Level** & \# D.S. & \# Patients & Avg. Length & \# Patients & Avg. Length & Patients & Avg. Length \\ 
1 & 1 & 38,926 & 1,819 & 275 & 1,787 & 264 & 1,812 \\  & 2 & 437 & 2,147 & 275 & 2,146 & 265 & 2,085 \\   & 1 & 44,645 & 3,514 & 150 & 3,501 & 145 & 3,497 \\
2 & 2 & 14,176 & 4,470 & 150 & 4,581 & 144 & 4,520 \\  & 3 & 1,161 & 4,956 & 150 & 5,030 & 144 & 5,102 \\ 
**Total** & 99,345 & - & 1,000 & - & 962 & - \\  

Table 2: Number of patients and average discharge summary length for MIMIC-IV, sampled MIMIC-IV, and EHRNoteQA across different levels and number of admissions (# _D.S._). _D.S._ denotes Discharge Summaries.

### Clinician Review & Modification

Each question-answer pair generated by GPT-4 is reviewed and modified by three clinicians to ensure clinical relevance. For each QA pair, the clinicians are provided with the discharge summaries, the question, the complete answer, and the four incorrect answers. They follow the instructions below for the review and modification process.

Firstly, the clinicians assess whether the GPT-4-generated questions are appropriate. Questions that are not clinically important based on the patient's discharge summaries (_e.g., "What were the patient's symptoms prior to being diagnosed with a wound abscess?"_) or are not in a form that a physician would typically ask (_e.g., "Can we conclude that the patient presented any positive leukocyte or nitrate urine test result?"_) are identified and removed. Of the initial 1,000 questions, 38 were removed, leaving 962.

Next, the data that passed the initial review undergoes three further revisions. Firstly, questions that are ambiguous, overly detailed, or asks for unnecessary additional detail are modified. Of the 962 questions, 206 underwent this revision process. Subsequently, based on the revised questions, the answers are also reviewed and modified to align with the modified question. Even if the questions are not modified, answers that are inaccurate or incomplete are also revised. In total, 338 answers were modified out of the 962. Finally, the incorrect answer choices are revised to represent plausible distractors instead of being obviously incorrect (e.g., clinically contradictory or not mentioned in the discharge summary, making them too easy to identify as incorrect). Among the 3,842 incorrect answers (962 questions with 4 incorrect answers each), 966 were revised. Figure 1 shows an example of these revisions, with more examples provided in the Appendix C.3.

### Data Analysis

Our final EHRNoteQA dataset, after revisions by clinicians, comprises 962 questions paired with the discharge summaries of 962 distinct patients. As shown in Table 2, Level 1 data includes a total of 529 patients: 264 patients admitted once (one discharge summary) and 265 patients admitted twice. Level 2 data includes a total of 433 instances: 145 patients admitted once, 144 patients admitted twice, and 144 patients admitted three times.

We analyze the types of information (topics) addressed by the questions in EHRNoteQA. We establish 10 categories (_i.e., treatment, assessment, problem, etiology, sign/symptom, vitals, test results, history, instruction, plan_) and define the criteria for each category (_e.g., Etiology: Questions asking about the causes of a patient's symptoms and problems_). Each of the 962 questions were categorized manually by the authors, with examples and proportions presented in Table 3. The categorization criteria along with additional examples can be found in the Appendix D.

  
**Category** & **Example** & **Proportion** \\  Treatment & What was the treatment provided for the patient’s left breast cellulitis? & 64\% \\ Assessment & Was the Mitral valve repair carried out successfully? & 19\% \\ Problem & What was the main problem of the patient? & 19\% \\ Etiology & Why did the patient’s creatinine level rise significantly upon admission? & 20\% \\ Sign/Symptom & What was the presenting symptom of the patient’s myocardial infarction? & 12\% \\ Vitals & What was the range of the patient’s blood pressure during second stay? & 3\% \\ Test Results & What were the abnormalities observed in the patient’s CT scans? & 14\% \\ History & Has the patient experienced any surgical interventions prior to the acute appendicitiss? & 12\% \\ Instruction & How was the patient instructed on weight-bearing after his knee replacement? & 3\% \\ Plan & What is the future course of action planned for patient’s left subclavian stenosis? & 5\% \\   

Table 3: Examples and proportions of question categories in EHRNoteQA. Note that the proportions do not sum to 100% since a single question can be categorized into multiple categories when containing more than one topic (_e.g., Etiology and Treatment: “What were the primary causes of the patient’s shortness of breath and how were these managed during the hospital stay?”_)Experiments

### Setup

**Models.** We evaluate the performance of 27 LLMs on EHRNoteQA including 3 GPT series (_i.e._, GPT-4, GPT-4-turbo, GPT-3.5-turbo) and 24 open-source LLMs (refer to Table 4). These LLMs are instruction-tuned models capable of processing context lengths of at least 4k tokens. For our Level 1 data, we evaluate LLMs with maximum context length of at least 4k tokens, while for Level 2 data, we evaluate LLMs capable of handling at least 8k tokens. To ensure privacy preservation during the evaluation process, we leverage Azure's HIPAA-compliant platform for the GPT series models, while performing local inference for the open-source LLMs (see Appendix B.2 for detailed settings).

**Evaluation.** As stated in Section 3.2, EHRNoteQA supports both open-ended and multi-choice QA. For open-ended QA, we use GPT-4 to evaluate the model outputs instead of using traditional methods such BLEU  or ROUGE-L , which often exhibit low correlation with human evaluations [36; 16]. Specifically, we prompt GPT-4 to evaluate the model output as either correct or incorrect, given the patient discharge summaries, the question, the correct answer, and the model output. To validate the accuracy of this evaluation method, three clinicians manually scored 10 different question-output pairs for each of the 27 models (totaling 270 pairs) and the results were compared to the evaluations by GPT-4 on the same set. The resulting Cohen's Kappa agreement (\(\)) between evaluations by GPT-4 and the three clinicians are 0.757, 0.855, and 0.880, respectively, while the intra-agreement among clinicians range from 0.808 to 0.903. Although the agreement of 0.757 falls slightly below this range (0.808 to 0.903), the overall high agreements (0.855 and 0.880) between GPT-4 and the clinicians, as well as the intra-clinician agreement, demonstrate the reliability of this open-ended evaluation method using GPT-4.

For multi-choice evaluation, we also use GPT-4 instead of the widely used probability-based evaluation methods [17; 32], which will be further discussed in Section 4.3. Specifically, we input the model output, the five answer choices, and the correct answer, and prompt GPT-4 to evaluate the model output as either correct or incorrect. The authors manually assessed this evaluation method4, which revealed a 98% accuracy. In summary, we use GPT-4 evaluation for both open-ended and multi-choice QA on EHRNoteQA as our primary evaluation method. Detailed information about evaluation such as GPT-4 prompt, pricing, and reliability check is provided in Appendix E.

### Results

The results of evaluating 27 LLMs on EHRNoteQA using both multi-choice and open-ended answering formats are presented in Table 4. The following analyses can be derived from the results:

**Impact of Model Size.** Smaller models generally tend to score lower compared to larger models. However, exceptions exist based on factors such as the foundation model and the fine-tuned instruction dataset. For instance, the 7b size model (_Mistral-7B-OpenOrca_) outperforms a model ten times its size (_Llama2-70b-chat_). Furthermore, while GPT-4 achieves the highest scores in both multi-choice and free-text evaluations, _Llama3-70b-Instruct_ shows performance close to GPT-4.

**Impact of Foundation Model.** The choice of the foundation model significantly affects performance. For instance, when comparing model scores of the same size, those based on _Mistral-7b_ (average: 73.29) generally outperform those based on _Llama2-7b_ (average: 65.69).

**Impact of Instruction Set.** Performance varies significantly depending on the instruction set used to train the model, even with the same foundation model. For example, the instruction-tuned models based on _Llama-13b_ exhibit a wide range of scores: 71.46 to 86.01 for multi-choice format and 66.16 to 79.21 for open-ended format.

**Effectiveness of Clinical Instruction Sets.** Even when models are trained on clinical domain instructions, their performance varies depending on the specific task. For example, _qCammel-13_ and _qCammel-70_ trained on synthetic patient-doctor dialogues, do not show significant advantages compared to those trained on general instructions, whereas _Asclepius-7_ and _Asclepius-13_, trained on synthetic clinical note instructions, generally score higher on EHRNoteQA.

**Impact of Note Length on Model Performance.** When comparing the scores of models capable of handling both Level 1 and Level 2 data (models supporting at least 8k context length), we observe that scores generally decrease from Level 1 to Level 2 within the same model. This decrease highlights the increased complexity in comprehending and interpreting extensive clinical contexts presented in longer patient notes. Notably, while _Mistral-8X7B_ shows a minor score reduction, _Mistral-7B_ exhibits a more significant drop. This reveals an important insight: while both models perform similarly at Level 1, they diverge significantly at Level 2, indicating that a model's proficiency at Level 1 does not necessarily translate to similar performance at Level 2, where tasks involve longer patient notes.

**Impact of Number of Notes on Model Performance.** In addition to comparing model performance across different note lengths, we also evaluate the impact of the number of notes on model performance. Specifically, within the same level data, we compare the scores of data with only one note to those of data with multiple notes (e.g., 2, 3 respectively). The results are presented in Appendix E.2 Consistent with the observed performance regarding note length, scores generally decrease as the number of notes increase for the same model, underscoring the challenge in interpreting cumulative discharge summaries across multiple admissions. Notably, while the scores for _Camel-Platypus2-13b_ and _Llama2-13b-chat_ are similar for the one-note questions in Level 1, _Camel-Platypus2-13b_ shows a minor 5.21% reduction from one to two notes, whereas _Llama2-13b-chat_ exhibits a more significant 13.5% drop. This reveals that proficiency with a single note does not necessarily translate to similar performance with two notes, where tasks involve interpreting information across more patient notes.

### The Reliability of EHRNoteQA as a Proxy for Clinician Evaluations

Prior to deploying an LLM as a QA agent in clinical settings such as responding to questions on discharge summaries, it is desirable for clinicians to directly assess the models' performance . A

    &  &  &  &  &  \\    & & **Level 1** & **Level 2** & **Level 1** & **Level 2** & \\   & GPT4 & 97.16 & 95.15 & 91.30 & 89.61 & &  \\  & GPT4-Turbo & 95.27 & 94.23 & 91.30 & 86.61 & &  \\  & GPT3.5-Turbo & 88.28 & 84.99 & 82.23 & 75.52 & &  \\   & Llama3-70b-Instruct & 94.33 & 91.92 & 89.04 & 86.84 & Llama3-70b &  \\  & Llama2-70b-chat & 84.88 & – & 78.83 & – & Llama2-70b &  \\  & qCammel-70 & 85.63 & – & 78.26 & – & Llama2-70b &  \\  & Camel-Platypus2-70b & 89.79 & – & 78.83 & – & Llama2-70b &  \\  & Platypus2-70b-Instruct & 90.36 & – & 80.53 & – & Llama2-70b &  \\ 
8x7B & Mistral-8x7b-Instruct & 87.52 & 86.61 & 88.28 & 81.52 & Mistral-7b &  \\ 
30B & MPT-30b-Instruct & 79.96 & 75.52 & 67.11 & 62.59 & MPT-30b-8k &  \\   & Llama2-13b-chat & 73.65 & – & 70.32 & – & Llama2-13b &  \\  & Vicuna-13b & 82.04 & – & 70.51 & – & Llama2-13b &  \\  & WizardLM-13b & 80.91 & – & 74.67 & – & Llama2-13b &  \\  & qCammel-13 & 71.46 & – & 66.16 & – & Llama2-13b &  \\  & OpenOrca-Platypus2-13b & 86.01 & – & 79.21 & – & Llama2-13b &  \\  & Camel-Platypus2-13b & 78.07 & – & 67.86 & – & Llama2-13b &  \\  & Synthia-13b & 79.21 & – & 74.48 & – & Llama2-13b &  \\  & Asclepius-13b\({}^{1}\) & – & – & 75.24 & – & Llama2-13b &  \\   & Gemma-7b-it & 77.50 & 67.21 & 63.71 & 54.27 & Gemma-7b &  \\  & MPT-7b-8k-instruct & 59.55 & 51.27 & 56.71 & 53.81 & MPT-7b-8k &  \\  & Mistral-7b-Instruct & 82.04 & 64.90 & 72.97 & 53.81 & Mistral-7b &  \\  & Dolphin-2.0-inistral-7b & 76.18 & – & 69.75 & – & Mistral-7b &  \\  & Mistral-7b-OpenOrca & 87.15 & – & 79.58 & – & Mistral-7b &  \\  & SynthIA-7b & 78.45 & – & 74.67 & – & Mistral-7b &  \\  & Llama2-7b-chat & 65.78 & – & 58.98 & – & Llama2-7b &  \\  & Vicuna-7b & 78.26 & – & 59.74 & – & Llama2-7b &  \\  & Asclepius-7b\({}^{1}\) & – & – & 66.92 & – & Llama2-7b &  \\   

Table 4: Results of 27 LLMs using both multi-choice and open-ended question answering methods for EHRNoteQA. Empty cells in Level 2 indicate that the model does not support context lengths up to 8k. \({}^{1}\)Asclepius is trained to provide only open-ended responses.

key question arises: Would a model that scores highly on EHRNoteQA also receive high scores from clinicians' evaluation? In other words, can EHRNoteQA serve as a reliable proxy for actual expert evaluation? Furthermore, how would other benchmarks, such as existing discharge summary QA and clinical knowledge benchmarks, compare to EHRNoteQA in representing real-world assessment? To address these questions, we conduct an additional experiment with 19 LLMs5, measuring the correlation between LLM performance manually evaluated by clinicians, and LLM performance automatically evaluated on diverse benchmarks including EHRNoteQA.

For manually evaluating LLM performance, three clinicians assessed the LLM responses to discharge summary-based questions, resulting in individual scores for each of the 19 models. For the questions, we used DiSCQ , a collection of questions asked by medical experts based on the MIMIC-III discharge summaries . The medical experts involved in DiSCQ are different from those who participated in the creation of EHRNoteQA to avoid potential bias. From a total of 1,089 DiSCQ questions, 300 were randomly selected, with each clinician assigned to 100 distinct questions. Each clinician evaluated the responses of the 19 LLMs regarding the 100 DiSCQ questions (a total of 1,900 responses), marking responses as either correct or incorrect based on the corresponding discharge summaries. Through this process, we obtained each score of the 19 LLMs from each of the three clinicians, referred to as _clinician-evaluated LLM scores_ in the paper. The DiSCQ questions assigned to each clinician and their evaluated scores for the models can be found in the Appendix F.

We then measure the correlation between these _clinician-evaluated LLM scores_ against the automatically evaluated LLM scores on EHRNoteQA Level 1 data and several other benchmarks. These

    &  &  &  \\   &  &  &  &  &  &  \\   \\    } & **Clinician A** & - & - & 0.854 & 0.712 & 0.947 & 0.834 \\  & **Clinician B** & 0.854 & 0.712 & - & - & 0.867 & 0.724 \\  & **Clinician C** & 0.947 & 0.834 & 0.867 & 0.724 & - & - \\   \\    } & **Open-Ended** & **0.770** & 0.609 & **0.805** & **0.617** & 0.801 & 0.657 \\  & **Multi-Choice** & 0.766 & **0.661** & 0.732 & 0.574 & **0.812** & **0.661** \\    } & **emrQA** & 0.696 & 0.522 & 0.653 & 0.518 & 0.661 & 0.475 \\  & **Are** & 0.509 & 0.344 & 0.502 & 0.315 & 0.542 & 0.344 \\    } & **MedQA** & 0.590 & 0.453 & 0.497 & 0.354 & 0.683 & 0.535 \\  & **MedMCQA** & 0.672 & 0.512 & 0.505 & 0.378 & 0.737 & 0.594 \\  & **PubMedQA** & 0.122 & 0.100 & 0.071 & 0.059 & 0.167 & 0.088 \\  & **MMLU* & 0.684 & 0.543 & 0.646 & 0.503 & 0.804 & 0.637 \\    } & **ARC** & 0.534 & 0.425 & 0.522 & 0.373 & 0.583 & 0.460 \\  & **Helliswag** & 0.284 & 0.206 & 0.247 & 0.177 & 0.373 & 0.265 \\  & **MMLU** & 0.579 & 0.437 & 0.567 & 0.408 & 0.651 & 0.507 \\  & **TrurufludQA** & 0.652 & 0.484 & 0.650 & 0.538 & 0.741 & 0.590 \\  & **Winogrande** & 0.439 & 0.307 & 0.383 & 0.278 & 0.480 & 0.336 \\  & **GSMK** & 0.202 & 0.159 & 0.256 & 0.165 & 0.222 & 0.147 \\  & **AVG** & 0.575 & 0.429 & 0.596 & 0.425 & 0.619 & 0.476 \\   \\    } & **GPT-4 Eval** & **0.770** & **0.609** & **0.805** & **0.617** & **0.801** & **0.657** \\  & **BLEU** & 0.155 & 0.112 & 0.037 & 0.059 & 0.014 & -0.006 \\  & **ROUGE-L** & 0.500 & 0.324 & 0.398 & 0.283 & 0.356 & 0.241 \\
**Open-Ended** & **Exact Match** & 0.422 & 0.288 & 0.336 & 0.236 & 0.266 & 0.194 \\  & **SentenceBERT** & 0.710 & 0.524 & 0.726 & 0.555 & 0.652 & 0.453 \\  & **ClinicalBERT** & 0.536 & 0.382 & 0.552 & 0.389 & 0.394 & 0.288 \\    } & **GPT-4 Eval** & **0.766** & **0.661** & **0.732** & **0.574** & **0.812** & **0.661** \\  & **Probability(index)** & 0.622 & 0.472 & 0.596 & 0.444 & 0.676 & 0.519 \\   & **Probability(value) & 0.514 & 0.437 & 0.523 & 0.456 & 0.549 & 0.437 \\   

Table 5: The correlation between _clinician-evaluated LLM scores_ by three individual clinicians and 1) the evaluations by other clinicians, 2) EHRNoteQA, 3) various other benchmarks, and 4) the methods of evaluating the open-ended and multiple-choice formats of EHRNoteQA. Within the benchmark comparison, bold indicates the highest correlation and underlined the second highest.

benchmarks include data from previous studies on discharge summary QA (emrQA  and Yue et al. ), as well as clinical domain knowledge benchmarks commonly used in LLM research (MedQA , PubMedQA , MMLU* 6, MedMCQA ), and general domain benchmarks from the HuggingFace open-LLM Leaderboard  (ARC , HellaSwag , MMLU , TruthfulQA ,Winogrande , GSM8K ) for comprehensive comparison. Following our EHRNoteQA evaluation method, we use the same automated evaluation method using GPT-4 for the discharge summary QA datasets (emrQA and Yue et al. ) and clinical benchmarks (MedQA, PubMedQA, MMLU*, MedMCQA), while general benchmarks (ARC, HellaSwag) scores are obtained directly from the Huggingface open-LLM leaderboard . Specific scores are detailed in Appendix F.3.

The correlations are presented in Table 5. Each column shows the Spearman(\(\)) and Kendall(\(\)) correlation coefficients between the _clinician-evaluated LLM scores_ and the benchmark scores. First, when measuring the correlation between the clinician-evaluated LLM scores of clinician A, B, and C, we find high intra-clinician correlations. Such results provide preliminary credibility of clinician evaluations, as each clinician evaluated a different subset of data. Notably, EHRNoteQA demonstrates the highest correlation in both multi-choice and free-text formats for all clinicians, consistently outperforming all other benchmarks. These results highlight the efficacy of EHRNoteQA in evaluating LLM performance within the specified clinical context, compared to other benchmarks.

Lastly, we reconfirm the validity of our GPT-4-based evaluation method for EHRNoteQA by comparing the correlations between _clinician-evaluated LLM scores_ and EHRNoteQA scores obtained through other evaluation methods. For open-ended questions, when evaluating with traditional methods such as BLEU , ROUGE-L , Exact Match, and cosine similarity (using Sentence-BERT  and ClinicalBERT ), the results show lower correlation compared to our GPT-4 based evaluation scores. For multi-choice questions, many studies employ probability-based scoring. Following this approach, we test two types of probability-based scoring methods on EHRNoteQA using _LM-Evaluation Harness_: one measuring the probability of the answer index (e.g., A) and another measuring the probability of the correct answer choice text. However, both results show lower correlation compared to our GPT-4 based evaluation scores.

## 5 Discussion

**Limitations.** The model performance on EHRNoteQA may not directly align with the model performance when evaluating on EHR discharge summaries outside of the MIMIC dataset. EHRNoteQA is built upon the MIMIC-IV dataset, and when computing its correlation with real-world clinical practice, we used questions derived from the MIMIC-III dataset. This experiment setting was inevitable because MIMIC is currently the only publicly available EHR dataset accessible for research purposes. However, it is important to note that if clinical institutions aim to apply LLMs to their own EHR systems, EHRNoteQA can serve as an initial benchmark for their model selection.

**Future Direction.** The current version of EHRNoteQA is tailored to align with the context length of currently available LLMs, categorizing the data into Level 1 (4k tokens) and Level 2 (8k tokens). As models capable of handling longer context lengths are developed and released, we plan to extend EHRNoteQA to include datasets with more admissions and longer discharge summaries. Additionally, while EHRNoteQA focuses on discharge summaries due to their frequent use in clinical practice, we recognize the importance of expanding our dataset to include other types of notes essential in healthcare settings, such as radiology notes and physician notes.