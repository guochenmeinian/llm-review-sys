# InsActor: Instruction-driven Physics-based Characters

Jiawei Ren1\({}^{1}\) Mingyuan Zhang1\({}^{1}\) Cunjun Yu1\({}^{2}\) Xiao Ma\({}^{3}\) Liang Pan\({}^{1}\) Ziwei Liu\({}^{1}\)

\({}^{1}\) S-Lab, Nanyang Technological University

\({}^{2}\) National University of Singapore

\({}^{3}\) Dyson Robot Learning Lab

Equal contribution

###### Abstract

Generating animation of physics-based characters with intuitive control has long been a desirable task with numerous applications. However, generating physically simulated animations that reflect high-level human instructions remains a difficult problem due to the complexity of physical environments and the richness of human language. In this paper, we present **InsActor**, a principled generative framework that leverages recent advancements in diffusion-based human motion models to produce instruction-driven animations of physics-based characters. Our framework empowers InsActor to capture complex relationships between high-level human instructions and character motions by employing diffusion policies for flexibly conditioned motion planning. To overcome invalid states and infeasible state transitions in planned motions, InsActor discovers low-level skills and maps plans to latent skill sequences in a compact latent space. Extensive experiments demonstrate that InsActor achieves state-of-the-art results on various tasks, including instruction-driven motion generation and instruction-driven waypoint heading. Notably, the ability of InsActor to generate physically simulated animations using high-level human instructions makes it a valuable tool, particularly in executing long-horizon tasks with a rich set of instructions. Our project page is available at jiawei-ren.github.io/projects/insactor/index.html

## 1 Introduction

Generating life-like natural motions in a simulated environment has been the focus of physics-based character animation . To enable user interaction with the generated motion, various conditions such as waypoints have been introduced to control the generation process . In particular, human instructions, which have been widely adopted in text generation and image generation, have recently drawn attention in physics-simulated character animation . The accessibility and versatility of human instructions open up new possibilities for downstream physics-based character applications.

Therefore, we investigate a novel task in this work: generating physically-simulated character animation from human instruction. The task is challenging for existing approaches. While motion tracking  is a common approach for character animation, it presents challenges when tracking novel motions generated from free-form human language. Recent advancements in language-conditioned controllers  have demonstrated the feasibility of managing characters using instructions, but they struggle with complex human commands. On the other hand, approaches utilizing conditional generative models to directly generate character actions  fall short of ensuring the accuracy necessary for continuous control.

To tackle this challenging task, we present **InsActor**, a framework that employs a hierarchical design for creating instruction-driven, physics-based characters. At the high level, InsActor generates motionplans conditioned on human instructions. This approach enables the seamless integration of human commands, resulting in more coherent and intuitive animations. To accomplish this, InsActor utilizes a diffusion policy [23; 5] to generate actions in the joint space conditioned on human inputs. It allows flexible test-time conditioning, which can be leveraged to complete novel tasks like waypoint heading without task-specific training. However, the high-level diffusion policy alone does not guarantee valid states or feasible state transitions, making it insufficient for direct execution of the plans using inverse dynamics . Therefore, at the low level, InsActor incorporates unsupervised skill discovery to handle state transitions between pairs of states, employing an encoder-decoder architecture. Given the state sequence in joint space from the high-level diffusion policy, the low-level policy first encodes it into a compact latent space to address any infeasible joint actions from the high-level diffusion policy. Each state transition pair is mapped to a skill embedding within this latent space. Subsequently, the decoder translates the embedding into the corresponding action. This hierarchical architecture effectively breaks down the complex task into two manageable tasks at different levels, offering enhanced flexibility, scalability, and adaptability compared to existing solutions.

Given that InsActor generates animations that inherently ensure physical plausibility, the primary evaluation criteria focus on two aspects: fidelity to human instructions and visual plausibility of the animations. Through comprehensive experiments assessing the quality of the generated animations, InsActor demonstrates its ability to produce visually captivating animations that faithfully adhere to instructions, while maintaining physical plausibility. Furthermore, thanks to the flexibility of the diffusion model, animations can be further customized by incorporating additional conditions, such as waypoints, as illustrated in Figure 1, showcasing the broad applicability of InsActor. In addition, InsActor also serves as an important baseline for language conditioned physics-based animation generation.

## 2 Related Works

### Human Motion Generation

Human motion generation aims to produce versatile and realistic human movements [12; 21; 22; 39]. Recent advancements have enabled more flexible control over motion generation [2; 10; 38; 9]. Among various methods, the diffusion model has emerged as a highly effective approach for generating language-conditioned human motion [36; 44]. However, ensuring physical plausibility, such as avoiding foot sliding, remains challenging due to the absence of physical priors and interaction with the environment [42; 29; 32]. Some recent efforts have attempted to address this issue by incorporating physical priors into the generation models , such as foot contact loss . Despite this progress, these approaches still struggle to adapt to environmental changes and enable interaction with the environment. To tackle these limitations, we propose a general framework for generating long-horizon human animations that allow characters to interact with their environment and remain

Figure 1: **InsActor** enables controlling physics-based characters with human instructions and intended target position. The figure illustrates this by depicting several “flags” on a 2D plane, each representing a relative target position such as (0,2) starting from the origin.

robust to environmental changes. Our approach strives to bridge the gap between understanding complex high-level human instructions and generating physically-simulated character motions.

### Language-Conditioned Control

Language-Conditioned Control aims to guide an agent's behavior using natural language, which has been extensively applied in physics-based animation and robot manipulation [35; 20; 19] to ensure compliance with physical constraints. However, traditional approaches often necessitate dedicated language modules to extract structured expressions from free-form human languages or rely on handcrafted rules for control [33; 43; 34]. Although recent attempts have trained data-driven controllers to generate actions directly from human instructions [18; 15], executing long-horizon tasks remains challenging due to the need to simultaneously understand environmental dynamics, comprehend high-level instructions, and generate highly accurate control. The diffusion model, considered one of the most expressive models, has been introduced to generate agent actions .

Nonetheless, current methods are unable to accurately control a humanoid character, as evidenced by our experiments. Recent work utilizing diffusion model to generate high-level pedestrian trajectories and ground the trajectories with a low level controller . Compared with existing works, InsActor employs conditional motion generation to capture intricate relationships between high-level human instructions and character motions beyond pedestrian trajectories, and subsequently deploys a low-level skill discovery incorporating physical priors. This approach results in animations that are both visually striking and physically realistic.

## 3 The Task of Instruction-driven Physics-based Character Animation

We formulate the task as conditional imitation learning, in which we learn a goal-conditioned policy that outputs an action \( A\) based on the current state \( S\) and an additional condition \( C\) describing the desired character behavior. The environment dynamics are represented by the function \(:S A S\).

The task state comprises the character's pose and velocity, including position \(\), rotation \(\), linear velocity \(}\), and angular velocity \(}\) for all links in local coordinates. Consequently, the task state, \(\), contains this information: \(:=\{,,},}\}\). Following common practice, we employ PD controllers to drive the character. Given the current joint angle \(\), angular velocity \(}\), and target angle \(}\), the torque on the joint actuator is computed as \(k_{p}(}-)+k_{d}(}-})\), where \(}=0\), \(k_{p}\) and \(k_{d}\) are manually specified PD controller gains. We maintain \(k_{p}\) and \(k_{d}\) identical to the PD controller used in DeepMimic . The action involves generating target angles for all joints to control the character.

Our work addresses a realistic setting in which demonstration data consists solely of states without actions, a common scenario in motion datasets collected from real humans where obtaining actions is challenging [26; 3; 13]. To train our model, we use a dataset of trajectory-condition pairs \(=\{(^{i},^{i})\}_{i=1}^{N}\), where \(=\{_{1}^{*},...,_{L}^{*}\}\) denotes a state-only demonstration generated by the expert of

Figure 2: The overall framework of InsActor. At the high level, the diffusion model generates state sequences from human instructions and waypoint conditions. At the low level, each state transition is encoded into a skill embedding in the latent space and decoded to an action.

length \(L\). We use \(^{*}\) to denote states from the expert demonstration. For example, a human instruction can be "walk like a zombie" and the trajectory would be a state sequence describing the character's motion. Our objective is to learn a policy in conjunction with environment dynamics \(\), which can replicate the expert's trajectory for given instruction \( C\).

## 4 The Framework of InsActor

The proposed method, InsActor, employs a unified hierarchical approach for policy learning, as depicted in Figure 2. Initially, a diffusion policy interprets high-level human instructions, generating a sequence of actions in the joint space. In our particular case, the action in the joint space can be regarded as the state of the animated character. Subsequently, each pair of actions in the joint space are mapped into the corresponding skill embedding in the latent space, ensuring their plausibility while producing desired actions for character control in accordance with motion priors. Consequently, InsActor effectively learns intricate policies for animation generation that satisfy user specifications in a physically-simulated environment. The inference process of InsActor is detailed in Algorithm 1.

### High-Level Diffusion Policy

For the high-level state diffusion policy, we treat the joint state of the character as its action. We follow the state-of-the-art approach of utilizing diffusion models to carry out conditional motion generation . We denote the human instruction as \(\) and the state-only trajectory as \(\).

Trajectory Curation.In order to use large-scale datasets for motion generation in a physical simulator, it is necessary to retarget the motion database to a simulated character to obtain a collection of reference trajectories. Large-scale text-motion databases HumanML3D  and KIT-ML  use SMPL  sequences to represent motions. SMPL describes both the body shape and the body poses, where the body poses include pelvis location and rotation, the relative joint rotation of the 21 body joints. We build a simulated character to have the same skeleton as SMPL. We scale the simulated character to have a similar body size to a mean SMPL neutral shape. For retargeting, we directly copy the joint rotation angle, pelvis rotation, and translation to the simulated character. A vertical offset is applied to compensate for different floor heights.

Diffusion Models.Diffusion models  are probabilistic techniques used to remove Gaussian noise from data and generate a clean output. These models consist of two processes: the diffusion process and the reverse process. The diffusion process gradually adds Gaussian noise to the original data for a specified number of steps, denoted by \(T\), until the distribution of the noise closely approximates a standard Gaussian distribution, denoted by \((,)\). This generates a sequence of noisy trajectories denoted by \(_{1:T}=\{_{1},...,_{T}\}\). The original data is sampled from a conditional distribution, \(_{0} p(_{0})\), where \(\) is the instruction. Assuming that the variance schedules are determined by \(_{t}\), the diffusion process is defined as:

\[q(_{1:T}|_{0})\,:=\,_{t=1}^{T}q(_{t}| _{t-1}),\ \ \ q(_{t}|_{t-1})\,:=\,(_{t};}_{t-1},_{t}),\] (1)

where \(q(_{t}|_{t-1})\) is the conditional distribution of each step in the Markov chain. The parameter \(_{t}\) controls the amount of noise added at each step \(t\), with larger values resulting in more noise added. The reverse process in diffusion models is another Markov chain that predicts and removes the added noise using a learned denoising function. In particular, we encode language \(\) into an encoded latent vector, \(}\), using the classical transformer  as the language encoder, \(}=()\). Thus, the reverse process starts with a distribution \(p(_{T}):=(_{T};,)\) and is defined as:

\[p(_{0:T}|})\,:=\,p(_{T})_{t=1}^{T}p(_{t-1}|_{t},}),\ \ \ p(_{t-1}|_{t},})\,:=\, (_{t-1};(_{t},t,}),(_ {t},t,})).\] (2)

Here, \(p(_{t-1}|_{t},})\) is the conditional distribution at each step in the reverse process. The mean and covariance of the Gaussian are represented by \(\) and \(\), respectively. During training, steps \(t\) are uniformly sampled for each ground truth motion \(_{0}\), and a sample is generated from \(q(_{t}|_{0})\). Instead of predicting the noise term \(\), the model predicts the original data \(_{0}\) directly which hasthe equivalent formulation [28; 36]. This is done by using a neural network \(f_{}\) parameterized by \(\) to predict \(_{0}\) from the noisy trajectory \(_{t}\) and the condition \(}\) at each step \(t\) in the denoise process. The model parameters are optimized by minimizing the mean squared error between the predicted and ground truth data using the loss function:

\[_{}=_{t[1,T],_{0} p(_{ 0}|)}[_{0}-f_{}(_{t},t,}) ],\] (3)

where \(p(_{0}|)\) is the conditional distribution of the ground truth data, and \(\) denotes the mean squared error. By directly predicting \(_{0}\), this formulation avoids repeatedly adding noise to \(_{0}\) and is more computationally efficient.

Guided Diffusion.Diffusion models allow flexible test-time conditioning through guided sampling, for example, classifier-guided sampling . Given an objective function as a condition, gradients can be computed to optimize the objective function and perturb the diffusion process. In particular, a simple yet effective _inpainting_ strategy can be applied to introduce state conditions , which is useful to generate a plan that adheres to past histories or future goals. Concretely, the inpainting strategy formulates the state conditioning as a Dirac delta objective function. Optimizing the objective function is equivalent to directly presetting noisy conditioning states and inpainting the rest. We leverage the inpainting strategy to achieve waypoint heading and autoregressive generation.

Limitation.Despite the ability to model complex language-to-motion relations, motion diffusion models can generate inaccurate low-level details, which lead to physically implausible motions and artifacts like foot floating and foot penetration . In the context of state diffusion, the diffuser-generated states can be invalid and the state transitions can be infeasible. Thus, direct tracking of the diffusion plan can be challenging.

### Low-Level Skill Discovery

To tackle the aforementioned challenge, we employ low-level skill discovery to safeguard against unexpected states in poorly planned trajectories. Specifically, we train a Conditional Variational Autoencoder to map state transitions to a compact latent space in an unsupervised manner . This approach benefits from a repertoire of learned skill embedding within a compact latent space, enabling superior interpolation and extrapolation. Consequently, the motions derived from the diffusion model can be executed by natural motion primitives.

Skill Discovery.Assuming the current state of the character is \(}_{l}\), the first step in constructing a compact latent space for skill discovery is encoding the state transition in a given reference motion sequence, \(_{0}=\{_{1}^{*},...,_{L}^{*}\}\), into a latent variable, \(\), and we call this skill embedding. This variable represents a unique skill required to transition from \(}_{l}\) to \(_{l+1}^{*}\). The neural network used to encode the skill embedding is referred to as the encoder, \(q_{}\), parameterized by \(\), which produces a Gaussian distribution:

\[q_{}(_{l}|}_{l},_{l+1}^{*}):=( _{l};_{}(}_{l},_{l+1}^{*}),_{} (}_{l},_{l+1}^{*})),\] (4)where \(_{}\) is the mean and \(_{}\) is the isotropic covariance matrix. Once we obtain the latent variable, a decoder, \(p_{}(_{l}|}_{l},_{l})\), parameterized by \(\), generates the corresponding actions, \(\), by conditioning on the latent variable \(_{l}\) and the current state \(}_{l}\):

\[_{l} p_{}(_{l}|}_{l},_{l})\] (5)

Subsequently, using the generated action, the character transitions into the new state, \(}_{l+1}\), via the transition function \((}_{l+1}|}_{l},_{l})\). By repeating this process, we can gather a generated trajectory, denoted as \(}=\{}_{1},...,}_{L}\}\). The goal is to mimic the given trajectory \(_{0}\) by performing the actions. Thus, to train the encoder and decoder, the main supervision signal is derived from the difference between the resulting trajectory \(}\) and the reference motion, \(_{0}\).

Training.Our approach leverages differentiable physics to train the neural network end-to-end without the need for a separate world model . This is achieved by implementing the physical laws of motion as differentiable functions, allowing the gradient to flow through them during backpropagation. Concretely, by executing action \(_{l} p_{}(_{l}|}_{l},_{l})\) at state \(}_{l}\), the induced state \(}_{l+1}\) is differentiable with respect to the policy parameter \(\) and \(\). Thus, directly minimizing the difference between the predicted state \(}\) and the \(^{*}\) gives an efficient and effective way of training an imitation learning policy . The Brax  simulator is used due to its efficiency and easy parallelization, allowing for efficient skill discovery. It also ensures that the learned skill is trained on the actual physical environment, rather than a simplified model of it, leading to a more accurate and robust representation.

Thus, the encoder-decoder is trained with an objective that minimizes the discrepancy between resulting trajectories and Kullback-Leibler divergence between the encoded latent variable and the prior distribution, which is a standard Gaussian,

\[_{}= _{0}-}+ D_{ {KL}}(q_{}(|,^{})( ,)),\] (6)

where \(\) denotes the mean squared error and \((,^{})\) is a pair of states before and after transition. The latter term encourages the latent variables to be similar to the prior distribution, ensuring the compactness of the latent space. \(\) is the weight factor that controls the compactness. During inference, we map the generated state sequence from the diffusion model to the skill space to control the character.

## 5 Experiments

The goal of our experiment is to evaluate the effectiveness and robustness of InsActor in generating physically-simulated and visually-natural character animations based on high-level human instructions. Specifically, we aim to investigate _1_) whether InsActor can generate animations that adhere to human instructions while being robust to physical perturbations, _2_) whether InsActor can accomplish waypoint heading while being faithful to the language descriptions, and _3_) the impact of several design choices, including the hierarchical design and the weight factor for skill space compactness.

Figure 3: **Qualitative results of InsActor with corresponding instructions. Top:** only human instruction. **Bottom:** human instruction and waypoint target.

### Implementation Details

To implement the experiment, we use Brax  to build the environment and design a simulated character based on DeepMimic . The character has 13 links and 34 degrees of freedom, weighs 45kg, and is 1.62m tall. Contact is applied to all links with the floor. For details of neural network architecture and training, we refer readers to the supplementary materials.

### Evaluation Protocols

Datasets.We use two large scale text-motion datasets, KIT-ML  and HumanML3D , for training and evaluation. KIT-ML has 3,911 motion sequences and 6,353 sequence-level language descriptions, HumanML3D provides 44,970 annotations on 14,616 motion sequences. We adopt the original train/test splits in the two datasets.

Metrics.We employ the following evaluation metrics:

1. _R Precision_: For every pair of generated sequence and instruction, we randomly pick 31 additional instructions from the test set. Using a trained contrastive model, we then compute the average top-k accuracy.
2. _Frechet Inception Distance (FID)_: We use a pre-trained motion encoder to extract features from both the generated animations and ground truth motion sequences. The FID is then calculated between these two distributions to assess their similarity.
3. _Multimodal Distance:_ With the help of a pre-trained contrastive model, we compute the disparity between the text feature derived from the given instruction and the motion feature from the produced animation. We refer to this as the multimodal distance.
4. _Diversity:_ To gauge diversity, we randomly divide the generated animations for all test texts into pairs. The average joint differences within each pair are then computed as the metric for diversity.
5. _Success Rate:_ For waypoint heading tasks, we compute the Euclidean distance between the final horizontal position of the character pelvis and the target horizontal position. If the distance is less than 0.5 m, we deem it a success. We perform each evaluation three times and report the statistical interval with 95% confidence.

    &  &  &  \\    & Top 1 & & & & & Dist\(\) \\   & \(0.243.000\) & \(0.420.021\) & \(0.522.039\) & \(2.310.097\) & \(1.055.162\) & \(4.259.014\) \\ PADL  & \(0.091.003\) & \(0.172.008\) & \(0.242.015\) & \(3.482.038\) & \(3.889.104\) & \(2.940.031\) \\ InsActor (Ours) & **0.352\(.013\)** & **0.550\(.010\)** & **0.648\(.015\)** & **1.808\(.027\)** & **0.786\(.055\)** & **4.392\(.071\)** \\  ^{}\)} & \(0.253.013\) & \(0.384.006\) & \(0.447.006\) & \(2.764.003\) & \(1.973.100\) & \(4.252.040\) \\ PADL\({}^{}\) & \(0.100.012\) & \(0.158.011\) & \(0.217.015\) & \(3.783.069\) & \(4.706.298\) & \(3.168.065\) \\ InsActor (Ours)\({}^{}\) & **0.332\(.013\)** & **0.496\(.017\)** & **0.599\(.008\)** & **2.147\(.061\)** & **1.043\(.091\)** & **4.359\(.073\)** \\   

Table 1: **Quantitative results on the KIT-ML test set. \(\): with perturbation.**

    &  &  &  &  \\    & Top 1 & & & & Dist\(\) \\  DReCon  & \(0.265.007\) & \(0.391.004\) & \(0.470.001\) & \(2.570.002\) & \(1.244.040\) & \(4.070.062\) \\ PADL  & \(0.144.003\) & \(0.227.012\) & \(0.297.018\) & \(3.349.030\) & \(2.162.022\) & \(3.736.091\) \\ InsActor (Ours) & **0.331\(.000\)** & **0.497\(.015\)** & **0.598\(.001\)** & **1.971\(.004\)** & **0.566\(.023\)** & **4.165\(.076\)** \\  ^{}\)} & \(0.233.000\) & \(0.352.001\) & \(0.424.004\) & \(2.850.002\) & \(1.829.002\) & \(4.008.147\) \\ PADL\({}^{}\) & \(0.117.005\) & \(0.192.003\) & \(0.254.000\) & \(3.660.040\) & \(2.964.115\) & \(3.849.159\) \\ InsActor (Ours)\({}^{}\) & **0.312\(.001\)** & **0.455\(.006\)** & **0.546\(.003\)** & **2.203\(.006\)** & **0.694\(.005\)** & **4.212\(.154\)** \\   

Table 2: **Quantitative results on the HumanML3D test set. \(\): with perturbation.**

### Comparative Studies for Instruction-driven Character Animation

Comparison Methods.We compare InsActor with two baseline approaches: _1)_ DReCon . We adapted the responsive controller framework from DReCon . We use the diffuser as a kinematic controller and train a target-state tracking policy. The baseline can also be viewed as a Decision Diffuser  with a long planning horizon, where a diffuser plans the future states and a tracking policy solves the inverse dynamics. _2)_ PADL : We adapt the language-conditioned control policy in PADL , where language instructions are encoded by a pretrained cross-modal text encoder  and input to a control policy that directly predict actions. It is also a commonly used learning paradigm in conditional imitation learning . Since the two baselines have no publicly available implementations, we reproduce them and train the policies with DiffMimic .

Settings.We utilize two different settings to assess InsActor's robustness. In the first setting, we evaluate the models in a clean, structured environment devoid of any perturbation. In the second setting, we introduce perturbations by spawning a 2kg box to hit the character every 1 second, thereby evaluating whether the humanoid character can still adhere to human instructions even when the environment changes.

Results.We present the results in Table 1 and Table 2 and qualitative results in Figure 3. Compared to the dataset used in PADL that consists of 131 motion sequences and 256 language captions, our benchmark dataset is two orders larger, _where the language-conditioned single-step policy used in PADL has difficulty to scaling up_. In particular, the inferior performance in language-motion matching metrics suggests that a single-step policy fails to understand unseen instructions and model the many-to-many instruction-motion relation. Compared to PADL, DReCon shows a better result in language-motion matching thanks to the high-level motion planning. However, unlike Motion Matching used in DReCon that produces high-quality kinematic motions, _the diffuser generates invalid states and infeasible state transitions, which fails DReCon's tracking policy and results in a low FID_. In comparison, InsActor significantly outperforms the two baselines on all metrics. Moreover, the experiment reveals that environmental perturbations do not significantly impair the performance of InsActor, showcasing InsActor's robustness.

    &  &  &  &  &  \\    & & Top 1 & & & & & \\  \(\) & ✓ & \(0.264.011\) & \(0.398.016\) & \(0.460.018\) & \(2.692.034\) & \(1.501.095\) & \(4.370.066\) \\ \(\) & \(\) & \(0.068.011\) & \(0.145.030\) & \(0.188.024\) & \(3.707.096\) & \(1.106.093\) & \(4.148.098\) \\ \(\) & ✓ & \(.013\) & \(.010\) & \(.015\) & \(.027\) & \(.055\) & \(.071\) \\   

Table 4: **Ablation on hierarchical design.** Evaluated on the KIT-ML test set.

    &  &  &  &  &  &  &  \\   & & & Top 3 & & & & & Rate\(\) \\  DReCon  & ✓ & ✓ & \(0.178.000\) & \(4.192.019\) & \(8.607.114\) & \(2.583.157\) & \(0.380.002\) \\  InsActor (Ours) & \(\) & ✓ & \(0.089.001\) & \(4.106.001\) & \(3.041.101\) & \(3.137.029\) & \(.002\) \\ InsActor (Ours) & ✓ & \(\) & \(.001\) & \(.004\) & \(.023\) & \(.076\) & \(0.081.004\) \\ InsActor (Ours) & ✓ & ✓ & \(0.388.003\) & \(2.753.009\) & \(2.527.015\) & \(3.285.034\) & \(0.907.002\) \\   

Table 3: **Quantitative results for the waypoint heading task.** Evaluated on HumanML3D. We set the start point at (0,0) and the waypoint uniformly sampled from a 6x6 square centered at (0,0). It is considered a successful waypoint heading if the final position is less than 0.5m away from the waypoint. **L**: Language. **W**: Waypoint.

### Instruction-driven Waypoint Heading

Waypoint Heading.Thanks to the flexibility of the diffusion model, InsActor can readily accomplish the waypoint heading task, a common task in physics-based character animation . This task necessitates the simulated character to move toward a target location while complying with human instructions. For instance, a human instruction might be, "walk like a zombie." In this case, the character should navigate toward the target position while mimicking the movements of a zombie.

Guided Diffusion.We accomplish this using guided diffusion. Concretely, we adopt the inpainting strategy in Diffuser . Prior to denoising, we replace the Gaussian noise in the first and last 25% frames with the noisy states of the character standing at the starting position and target position respectively.

Results.We conduct this experiment with the model trained on HumanML3D. We contrast InsActor with DReCon and two InsActor variants: _1)_ InsActor without language condition, and _2)_ InsActor without targeting. Our experimental results demonstrate that InsActor can effectively accomplish the waypoint heading task by leveraging guided diffusion. The model trained on HumanML3D is capable of moving toward the target position while following the given human instructions, as evidenced by a low FID score and high precision. Although adding the targeting position condition to the diffusion process slightly compromises the quality of the generated animation, the outcome is still satisfactory. Moreover, the success rate of reaching the target position is high, underscoring the effectiveness of guided diffusion. Comparing InsActor with its two variants highlights the importance of both the language condition and the targeting in accomplishing the task. Comparing InsActor with DReCon shows the importance of skill mapping, particularly when more infeasible state transitions are introduced by the waypoint guiding. Without skill mapping, DReCon only has a 38.0% success rate, which drops drastically from the 90.7% success rate of InsActor.

Multiple Waypoints.Multiple waypoints allow users to interactively instruct the character. We achieve this by autoregressively conditioning the diffusion process to the history motion, where a qualitative result is shown in Figure 4. Concretely, we inpaint the first 25% with the latest history state sequences. We show qualitative results for multiple-waypoint following in Figure 1 and more in the supplementary materials.

### Ablation Studies

Hierarchical Design.To understand the importance of the hierarchical design in this task, we performed an ablation study on its structure. We compared our approach to two baselines: _1)_ A policy with only a high-level policy, wherein the diffusion model directly outputs the skills, analogous to the _Diffuser_ approach ; _2)_ A low-level policy that directly predicts single-step skills. We show the results in Table 4. By leveraging skills, the low-level policy improves from PADL but still grapples with comprehending the instructions due to the absence of language understanding. Conversely, without the low-level policy, the skills generated directly by the diffusion model are of poor precision. Although the use of skills safeguards the motions to be natural and score high in FID, the error accumulation deviates the plan from the language description and results in a low R-precision. The experimental results underscore the efficacy of the hierarchical design of InsActor.

Figure 4: **Qualitative results of InsActor with history conditioning. Generation is conditioned on the second human instruction and history motion.**Weight Factor.InsActor learns a compact latent space for skill discovery to overcome infeasible plans generated by the diffusion model. We conduct an ablation study on the weight factor, \(\), which controls the compactness of the skill space. Our findings suggest that a higher weight factor results in a more compact latent space, however, it also curtails the instruction-motion alignment. Conversely, a lower weight factor permits a greater diversity in motion generation, but it might also lead to less plausible and inconsistent motions. Hence, it is vital to find a balance between these two factors to optimize performance for the specific task at hand.

## 6 Conclusion

In conclusion, we have introduced InsActor, a principled framework for physics-based character animation generation from human instructions. By utilizing a diffusion model to interpret language instructions into motion plans and mapping them to latent skill vectors, InsActor can generate flexible physics-based animations with various and mixed conditions including waypoints. We hope InsActor would serve as an important baseline for future development of instruction-driven physics-based animation. While InsActor is capable of generating such animations, there are crucial but exciting challenges ahead. One limitation is the computational complexity of the diffusion model, which may pose challenges for scaling up the approach to more complex environments and larger datasets. Additionally, the current version of InsActor assumes access to expert demonstrations for training, which may limit its applicability in real-world scenarios where such data may not be readily available. Furthermore, while InsActor is capable of generating physically-reliable and visually-plausible animations, there is still room for improvement in terms of the quality and diversity of generated animations. There are mainly two aspects for future development, improving the quality of the differentiable physics for more realistic simulations and enhancing the expressiveness and diversity of the diffusion model to generate more complex and creative animations. Besides them, extending InsActor to accommodate different human body shapes and morphologies is also an interesting direction.

From a societal perspective, the application of InsActor may lead to ethical concerns related to how it might be used. For instance, InsActor could be exploited to create deceptive or harmful content. This underscores the importance of using InsActor responsibly.