# AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning

Minghao Chen\({}^{1}\), Yihang Li\({}^{2}\), Yanting Yang\({}^{3}\), Shiyu Yu\({}^{5}\), Binbin Lin\({}^{3,4*}\), Xiaofei He\({}^{2}\)

\({}^{1}\)School of Computer Science, Hangzhou Dianzi University

\({}^{2}\)State Key Lab of CAD&CG, Zhejiang University

\({}^{3}\)School of Software Technology, Zhejiang University \({}^{4}\)Fullong Inc. \({}^{5}\)NingBo Port Group

minghaochen01@gmail.com

###### Abstract

Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce a _case-conditioned prompting_ strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4% with GPT-4-turbo and 86.2% with GPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at https://github.com/minghchen/automanual.

## 1 Introduction

Recently, autonomous agents based on Large Language Models (LLM), e.g., ReAct , Reflexion , SayCan , WebGPT , and Voyager , have demonstrated their potential to complete long-horizon tasks in grounded environments. These LLM agents operate by generating thoughts and actions that are executable in the environment. For customized environments, such as robotics  and games , prior methods provide detailed instructions and in-context examples to familiarize LLM with action functions (API) and the target environment. However, unlike these agents, humans can autonomously build and update their understanding of an unfamiliar environment through dynamic interaction.

Several existing methods enable LLM agents to reflect on feedback  or save successful experiences as skills  to enhance the performance and reduce the reliance on human-provided examples. However, these reflections and skills have not been well exploited to foster a deeper understanding of the environment. As a result, directly using saved skills as in-context examples can lead to the **Path Dependence** problem, i.e., the agent blindly replicates the paths of previous successes, failing to adapt appropriately to new scenarios. Such problems are more severe in real-world situations characterized by high variability.

A previous work, ExpeL , gathers the trajectories of LLM agents and extracts cross-task rules from them. However, these rules are extracted offline, making ExpeL suffer from the same distributional shift problem as Offline RL . Meanwhile, due to the simplicity of rule management, its rules are always armchair general and unhelpful for the Path Dependency problem. In this paper, we propose a novel framework called **AutoManual** to build a well-organized understanding of the environment that can guide multi-task planning effectively. AutoManual leverages a dynamic rule system that not only extracts valuable experience, including skills and reflections, into different types of rules but also allows for continuously updating these rules in response to new situations. Additionally, error-prone details are explicitly described in the rules to improve the robustness of planning.

AutoManual follows two alternating iterative processes to optimize the rules. First, given the observation and task of an episode, the Planner agent utilizes currently discovered rules to write free-form code as an actionable plan. The interaction between the environment and the Planner will loop until the episode ends. Second, based on this trajectory, the Builder agent will update relevant rules through the rule system. This online updating mechanism can timely verify whether the rules have deviations and are applicable to the Planner. After rules optimization, the Formulator agent categorizes these rules according to their application scenarios and compiles a comprehensive manual in Markdown format.

The challenge lies in enabling the Builder to accurately extract applicable rules from a long trajectory, as LLM are prone to generating hallucinations. To address this, we employ a _case-conditioned prompting_ strategy, which directs the Builder to focus on specific rules according to the case of the trajectory. For example, if errors occurred in a trajectory, the Builder is first asked to determine which caused the error: an unrecorded situation occurred, or the Planner failed to follow existing rules. Based on this answer, the Builder will be given corresponding prompts to update relevant rules.

To summarize, our contributions are the following:

* We adopt actionable code as the way for the Planner agent to interact with the environment. We introduce a structured rule system that allows the Builder agent to manage multiple types of knowledge from these code-based interactions.
* We propose an alternating process between the Planner and Builder agents to optimize rules in an online manner and resolve the Path Dependency problem. To improve readability, the Formulator agent is introduced to reorganize and formalize the rules into a Markdown manual.
* To facilitate rule management, we employ a _case-conditioned prompting_ strategy, which guides the Builder to manage specific types of rules for different trajectory cases.
* Starting from a single demonstration, AutoManual can generate detailed instruction manuals for complex environments like ALFWorld and MiniWoB++. These manuals allow LLM agents to achieve remarkable success rates of 97.4% with GPT-4-turbo and 86.2% with GPT-3.5-turbo on ALFWorld, 98.3% with GPT-4-turbo and 92.7% with GPT-3.5-turbo on MiniWoB++.

## 2 Related Works

### LLM for Agents Planning

Large Language Models (LLM) exhibit powerful reasoning and planning capabilities [11; 12; 28; 33; 39] while requiring much fewer demonstrations than traditional learning methods. With this planning capability as the core, LLM agents are being developed for use in robotics [1; 7; 18; 20; 23], game-playing [14; 24; 27; 39], software development [3; 15], and other fields . Prior studies [16; 21; 33] allow agents to adjust actions or plans based on environmental feedback to improve planning performance. Given the powerful programming capability of LLM, several works, e.g., CodeAsPolicy , ProgPrompt  and AdaPlanner , propose to use Python code as the plan of LLM agents. This form of output can automatically respond to in-plan feedback and achieve better performance than the action and JSON format [21; 25].

### Self-improvement of LLM Agents

Embodied agent research has long sought to enable agents to self-improve through interactive experiences. Unlike traditional learning-based agents that require extensive iterations for optimization,Reflexion  allows LLM agents to reflect on previous failures and quickly improve their plans. Some works [32; 34; 36] combine tree search with reflection to deliberately seek a better solution. Apart from failure experiences, prior studies [21; 24; 39] utilize successful experiences as skills to assist future planning. Voyager  stores generated and verified programs into the skill library as a new skill for more complex tasks. AdaPlanner  also discovers and archives successful programs into skill memory for future similar tasks. However, these methods stop updating skills after storing them, which inevitably leads to the Path Dependency problem.

Another series of works [26; 31; 38] employs LLM as a prompt optimizer to enhance its own performance. In contrast to our approach, which addresses challenges in unfamiliar environments, these studies focus on enhancing LLM reasoning performance. As a result, their optimized prompts are typically brief and lack environmental knowledge.

### Memory Management of LLM Agents

For LLM agents, learning from past experiences can also be viewed as managing the episodic memory . CLIN  proposes to keep updating a memory centered on causal abstractions for new trials. Retrieval-Augmented Planning (RAP)  retrieves past experiences corresponding to the current situation. MemGPT  allows LLM to select content to retain in working memory and to search for information in long-term memory. Generative Agents  retrieve memories based on recency, importance, and relevance to the current situation. Generative Agents also generate tree-structured reflections, but they focus on a continuous scenario rather than task-oriented rules.

### LLM for Rule Discovery

Several recent works also investigate the rule discovery capabilities of LLM. Zhu et al.  propose Hypotheses-to-Theories (HtT), enabling LLM to induce and deduce rules for basic reasoning tasks. For LLM agents, ExpeL  gathers the trajectories of Reflexion agents and extracts cross-task rules from them. Furthermore, AutoGuide  generates state-aware rules and retrieves rules relevant to the test-time state. Unlike ExpeL and AutoGuide, which extract rules from offline experiences, we update rules in an online manner, verifying their reliability and applicability. For more discussion of differences, refer to Appendix C.

## 3 Methods

### AutoManual Overview

Our AutoManual framework, shown in Fig 1, consists of three main stages. **Building stage:** The Planner agent and Builder agent collaborate to build rules from the interactive environment. The Consolidator agent merges or deletes redundant rules when the rules exceed the maximum rule number. **Formulating stage:** The Formulator agent categorizes the rules, summarizes the key points, and formulates them into a manual in Markdown form. **Testing stage:** Based on the generated manual, a test-time Planner agent will be evaluated through test tasks and scenarios.

Formally, an Interactive Environment can be modeled as a Partially Observable Markov Decision Process (POMDP): \((,,,,)\). At the start of each episode, a scenario \(s_{0}\) will be initialized, a text-grounded task \(g\) and the initial observation \(o_{0}\) (processed into textual form) will be given. The environment can be interacted with through permissible actions (API) set \(\). After executing an action \(a\), the environment will return the result of the action and the new observation \(o^{}\) based on the dynamics \(T(s^{}|s,a)\) and \(O(o^{}|s^{})\). Finally, when the episode is done, a binary reward \(r\{-1,1\}\) indicating the failure or success of the task will be returned.

We approach the learning of environmental rules as an optimization problem:

\[_{}E_{s_{0},g}E_{(|)}r(_{})\] (1)

where \(\) denotes all rules in our rule system, \((|)\) denotes the policy of the Planner given the current rules \(\) and \(_{}\) denotes a trajectory of \((|)\) starting from \([o_{0},g]\). Classic policy gradient methods  solve such problems through stochastic gradient ascent, i.e., executing the current policy to obtain the episodic reward and back-propagating gradients to update the parameters.

Inspired by this online reinforcement learning paradigm, we follow two alternative processes to optimize the rules \(\): 1. The Planner practices the current rules through interaction within an episode. 2. The Builder updates the rules \(\) based on this trajectory. Compared to traditional parameter optimization, sample-inefficient gradient ascent is replaced by text-based rule management. We design a well-structured rule system described in Section 3.3 to ensure the rule updating contributes to rewards. Additionally, to limit the role of human expertise, we only provide a simple example demonstrating the output format to agents. Then, manually derive several initial rules from this example as the starting point of the optimization.

### Planner Agent for Interactive Planning

As demonstrated by the success of Voyager  and AdaPlanner , code-based planning can leverage the powerful programming capability of LLM and automatically react to in-plan feedback. Voyager and AdaPlanner output and refine a complete solution function for the task, which is potentially reusable. However, this function-form output is difficult to adjust in response to environmental feedback, as it requires maintaining the integrity of the plan throughout.

Our Planner Agent outputs free-form code as its plan, which aligns more with the natural programming capabilities of LLM [7; 22]. This form simplifies planning by only generating code necessary for the current environmental situation and feedback without the overhead of integrating previously executed code. As shown in Fig 2, at the start of a new episode, the Planner receives system prompts, current rules \(\), relevant samples from the skill and reflection libraries, the target task \(g\), and initial observation \(o_{0}\). System prompts contain the role, permissible actions \(\), response guidelines, and a simple example (detailed in Appendix H). The output of the Planner is structured into four segments during each cycle:

1. **Analysis:** The understanding of the current situation and reflection on previous errors if exist.
2. **Related Rules:** Rules (along with their IDs) that need to be considered in this situation.
3. **Overall Plan:** The general plan to complete the task.
4. **Code:** A block of Python code divided into steps. The Planner is encouraged to define helpful functions in the code, which might be reusable in similar scenarios.

We denote this response of the Planner as \([thought_{t},code_{t}]\), where \(thought_{t}\) denotes the first three segments. \(code_{t}\) executed in the environment is followed by feedback \(c_{t}\), which informs the subsequent output cycle. This process iterates until the episode ends or a response limit is reached.

Figure 1: **AutoManual Overview: AutoManual operates in three stages: (1) Building Stage: The Planner agent interacts with the environment by coding actionable plans. After receiving the current trajectory of the Planner, the Builder agent manages rules through the online rule system. (2) Formulating Stage: The Formulator agent formulates the resulting rules into a Markdown manual. (3) Testing Stage: A test-time Planner agent utilizes the manual to complete testing tasks.**

As shown in Fig 2, according to the episodic reward, we categorize the result into **Direct Success**, **Indirect Success** (errors occurred but were solved later), and **Failure**. In the case of Direct or Indirect Success, the Planner will be prompted to organize its previous code into a code block. For Indirect Success, it additionally summarizes the mistakes and misunderstandings that cause errors. For the Failure case, the Planner will be prompted to reflect on the reason for the failure carefully, suggest reasonable corrections, and specify the code segment that caused the error. We denote this response of the Planner as _conclusion_. Finally, we obtain a trajectory of the Planner:

\[_{}=(o_{0},g,[_{1},_{1}],c_{1},...,[ _{T},_{T}],c_{T},)\] (2)

**Skill Library and Reflection Library:** Apart from rules, we also manage and transmit conclusions from previous episodes, which provide essential details for generating planning code. In the case of Direct or Indirect Success, we save the code block in _conclusion_ as a skill for that task type 1 into the skill library . In the Failure case, we save its _conclusion_ as a reflection for that task type into the reflection library. When a new task comes, the code block of the most similar task is retrieved from the skill library. If there is no existing skill for the new task type, the reflection for that task type will be returned. As mentioned in the Introduction, compared with rules, these skills and reflections contain more programming details but are less generalizable to new scenarios, i.e., the Path Dependence problem.

**Cooperation between Agents:** In our framework, rule management is not solely the responsibility of the Builder; the Planner also plays a critical role by explicitly identifying the rules it engages in its response. This cooperation is facilitated by including the Planner's thoughts within the trajectory \(\), which is provided to the Builder. This synergy enhances the identification and adjustment of problematic rules. In addition, _conclusion_ from the Planner contains the detailed success process or reflections on errors, which further assist the Builder in managing corresponding types of rules.

### Builder and Consolidator Agents for Rule Management

Upon receiving the trajectory \(_{}\), the Builder has to manage the rules through the rule system.

Figure 2: **The Planner Trajectory:** Given the current task and rules, the Planner will interact with the environment through free-form code. Based on the trajectory result, the Planner will generate a corresponding conclusion, which will be saved in the skill or reflection library.

**Rule System:** We intuitively identify rules as the kinds of knowledge that help task completion, including the analyses of the observed phenomenon \(T(o^{}|o,a)\), the mechanism \(T(s^{}|s,a)\), and the correlation between the reward \(r\) and \(_{}\), i.e., the success process or the occurred error. Therefore, unlike ExpeL  and AutoGuide , which derive general insight from the trajectory, our system categorizes six specific rule types to extract environmental knowledge that targets different aspects of the trajectory. Furthermore, each rule in our system is enhanced with **Example** attribute to illustrate its application and important details, making it grounded and well-understood. Specifically, each rule in the rule system has these four attributes:

1. **Rule Type:** The type of the rule, chosen from ["Special Phenomenon", "Special Mechanism", "Useful Helper Method", "Success Process", "Corrected Error", "Unsolved Error"];
2. **Rule Content**: A description of the rule, beginning with the scope of its applicable scenarios;
3. **Example**: An example or code from the trajectory demonstrates this rule, where additional remarks, e.g. error-prone details, can also be added to it;
4. **Validation Logs**: Logs that track the rule's application and updates, including episode and rule IDs that trace the rule's evolution, serving as a reference for the Builder and Consolidator.

The Builder manages the rules through the following functions of the rule system:

* _write_rule(**rule_attributes_**): Write down a new rule with its four attributes.
* _update_rule(rule_id, **rule_attributes_**): Rewrite the attributes of a existing rule.
* _stop_generating()_: When the trajectory is not needed or insufficient to derive any more new rules, the function should be called.

Similar to hierarchical reflections in Generative Agents , we allow the Builder to utilize existing rules to induce more general or deeper rules and record their dependence in Rule Content or Validation Logs, more discussed in Appendix D.

**Case-Conditioned Prompting:** To mitigate the risk of erroneous rule creation, such as deriving rules of success from a failed trajectory, we employ case-conditioned prompts. As illustrated in Fig 3, the Builder first analyzes and determines if the major errors stem from "Imperfect Rules" or "Imperfect Agent". Based on this analysis and the trajectory results, targeted prompts guide the Builder in rule management 2. For example, in a case of indirect success due to imperfect rules (Case \(2\)), the prompts will guide the Builder to extract or update the success process, helper methods, and error reflections

Figure 3: **Case-Conditioned Prompts:** Given the current trajectory, the Builder classifies the cause of the major error as ”Imperfect Rules” or ”Imperfect Agents”. Then, the Builder will get the base prompt and corresponding prompt to guide its rule management.

in corresponding rule types. Finally, the Builder responds with the potential rules detailing their relation with existing rules and uses the functions of the rule system to manage rules.

**Rule Consolidation:** When the number of rules in the rule system exceeds \(N_{max}\), the Consolidator agent steps in to consolidate related rules and delete redundant rules. It uses three functions of the rule system: _get_trajectory(episode_id)_, _update_rule(rule_id, **rule_attributes)_ and _delete_rule(rule_id)_. Given the current rules, the Consolidator identifies potentially relevant or overlapped rules, uses _get_trajectory_ function to investigate the trajectories they depend on, and finally calls the remaining functions to manage the rules. During the management, the Consolidator ensures that consolidation retains details of rules and examples.

### Manual Formulation

Once the building stage is complete, we can obtain a set of rules targeted to different situations, whose applicability has been validated through online optimization. Our next goal is to enhance their readability and global understanding. To achieve this, we introduce the Formulator agent, designed to transform these rules into a user-friendly manual, analogous to a teacher imparting a wealth of knowledge through easily digestible lessons. As depicted in Fig 1, the Formulator begins by categorizing all rules based on their target scenarios. This categorization aids in structuring the manual and ensures that related rules are discussed together, which enhances the logical flow and accessibility of the information. For each category, the Formulator drafts an introduction, summarizing the rules it contains and highlighting the key points and overall principles that govern the specific scenarios. Finally, the Formulator compiles the rules and their introductions into a comprehensive manual formatted in Markdown.

## 4 Experiments

In line with prior works [2; 21], we conduct the experiments on three interactive environments: (1) **ALFWorld** is a text-based virtual household environment containing six distinct task types. We run the building stage on 36 tasks (6 tasks for each task type) sampled from the training set of ALFWorld, and each task is run only once. Following previous works [16; 21; 33], we run the testing stage on the validation unseen set containing 134 tasks across these six types. (2) **MiniWoB++** is a simulated web environment where agents complete diverse tasks on the Internet by performing keyboard and mouse actions. Prior works [5; 21] selects 9 task types with environmental feedback and 44 task types without feedback from MiniWoB++ tasks. We perform experiments on 9 task types with feedback or on all 53 task types. At each stage, we randomly sample 6 tasks for each task type. (3) **WebArena** introduces realistic web environments by emulating the functionality and data of popular websites. This benchmark poses significant challenges for LLM agents due to its large observation and action space, along with tasks that require longer planning horizons. Following AutoGuide , our experiments focus on the Reddit domain within WebArena.

During the building and formulating stages, we use GPT-4-turbo (gpt-4-1106-preview) for all agents. At the testing stage, we equip the Planner agent with GPT-4-turbo or GPT-3.5-turbo (gpt-3.5-turbo-1106), to evaluate the effect of generated manuals on relatively smaller LLM.

**Compared Methods:** In the experiments, we compare AutoManual with the following methods of LLM Agent: (1) **ReAct** prompts LLM to generate the reasoning trace using CoT  and next-step action; (2) **Reflexion** agents generate reflection on task feedback signals, which is saved in the memory for subsequent trials; (3) **ExpeL** extract insights and skills from the offline trajectories of Reflexion agents; (4) **RCI** agent recursively criticizes and improves its output for solving computer tasks; (5) **AdaPlanner** allows the LLM agent to generate and adaptively refine a code-style plan; (6) **Planner+Lib.** represents our Planner agent equipped with skill and reflection libraries (SS3.2) during building and testing stages without any rules. We re-implement prior methods with GPT-3.5 and GPT-4 versions the same as ours for fair comparisons.

ReAct, Reflexion, and ExpeL provide LLM agents with \(12\) human examples (\(2\) examples per task type) of ALFWorld. For AdaPlanner, they provide 6 human examples (\(1\) example per task type) of ALFWorld as the start of skill discovery. For our methods, agents are provided only one human example of the simplest task (Put) on ALFWorld. On MiniWob++, our agents are provided one human example (search-engine) for tasks with feedback and 4 examples for all tasks. On WebArena, our agents are also provided with one human demonstration. To reduce randomness, we performed each experiment three times and reported the average. More details of the implementation and prompts for AutoManual can be found in the Appendix.

### Main Results

**Main Results on ALFWorld:** As shown in Tab. 1, AutoManual significantly outperforms the existing methods, evidenced by overall success rates of 86.2% when using GPT-3.5-turbo for the testing stage and 97.4% when using GPT-4-turbo. Noticeably, AutoManual requires little expert prior knowledge about the environment and is only provided with one human example to achieve excellent results. In comparison, the rules induced by ExpeL hardly improve performance, as its offline trajectories are composed of individual actions rather than code. We find the performance of AdaPlanner is lower than reported. One reason is that AdaPlanner requires LLM to output specific formats to complete its function-form code, which is difficult for creative LLM, e.g., GPT-4-turbo. In addition, AdaPlanner and Planner+Lib. are inferior to AutoManual because they only store successful paths as skills and inevitably face the Path Dependence problem. Especially, tasks in _Put Two_ have various scenarios, such as "two objects can occur at the same receptacle or different receptacles", that require different processes to solve (Appendix G shows an example). Furthermore, Planner+Lib. often does not mark error-prone points in its skills, such as "target objects may appear in unconventional locations".

  Methods & Examples & Put & Clean & Heat & Cool & Examine & Put two & ALL \\   \\ ReAct  & 12 & 75.0 & 24.7 & 37.7 & 36.4 & 44.4 & 11.8 & 41.9 \\ Reflexion  & 12 & 87.5 & 44.1 & 73.9 & 50.0 & 61.1 & 35.3 & 59.8 \\ ExpeL  & 12 & 62.5 & 61.3 & 30.4 & 61.9 & 55.5 & 35.3 & 52.2 \\ AdaPlanner  & 6 & 83.3 & 46.2 & 65.2 & 74.2 & 68.5 & 52.9 & 63.3 \\  Planner+Lib. & 1 & 77.8 & **88.2** & 82.6 & 72.7 & 37.0 & 27.5 & 66.5 \\ AutoManual & 1 & **95.8** & 79.6 & **87.0** & **78.8** & **100.0** & **66.7** & **86.2** \\   \\ ReAct  & 12 & 95.8 & 76.3 & 69.6 & 86.4 & 72.2 & 52.9 & 76.8 \\ Reflexion  & 12 & **100.0** & 95.7 & 78.3 & 86.4 & 77.8 & 70.6 & 85.9 \\ ExpeL  & 12 & 94.4 & 82.8 & 72.4 & 81.8 & 72.2 & 58.8 & 79.2 \\ AdaPlanner  & 6 & 88.9 & 90.3 & 85.5 & 75.8 & 64.8 & 41.2 & 76.4 \\  Planner+Lib. & 1 & **100.0** & 93.5 & **100.0** & 93.9 & 88.9 & 39.2 & 88.1 \\ AutoManual & 1 & **100.0** & **98.9** & **100.0** & **95.4** & **100.0** & **90.2** & **97.4** \\  

Table 1: Success rate (%) of LLM agent methods on ALFWorld test tasks. For each method, the number of all human examples used is listed. “Planner+Lib.” represents only using skill&reflection library during the building and testing stages. We run all experiments 3 times and show the average.

  Methods & Examples & With feedback (\(\)) & Examples & ALL (\(\) types) \\   \\ RCI  & 22 & 45.6 & 104 & 77.3 \\ AdaPlanner  & 13 & 71.6 & 38 & 89.4 \\  Planner+Lib. & 1 & 63.6 & 4 & 87.0 \\ AutoManual & 1 & **82.2** & 4 & **92.7** \\   \\ RCI  & 22 & 60.4 & 104 & 88.6 \\ AdaPlanner  & 13 & 74.1 & 38 & 90.3 \\  Planner+Lib. & 1 & 80.2 & 4 & 94.4 \\ AutoManual & 1 & **94.5** & 4 & **98.3** \\  

Table 2: Success rate (%) of LLM agent methods on 9 task types with feedback and all 53 task types of MiniWoB++. For each method, the number of human examples used is listed.

  Methods & Examples & Suc(\%) \\  ReAct  & 2 & 6.0 \\ AutoGuide  & 19 & 43.7 \\ SteP  & 14 & 55.0 \\  Planner & 1 & 51.1 \\ AutoManual & 1 & **65.1** \\  

Table 3: Test on WebArena (Reddit).

**Main Results on MiniWoB++:** As shown in Tab. 2, the performance of AutoManual exceeds the previous methods and Planner+Lib. by a large margin. Especially in 9 task types with feedback, these tasks have higher diversity and require LLM agents to cope with various situations. For example, the tasks in _login-user-popup_ type will interrupt the agent's plan at any time, requiring the agent to cope with unexpected situations. Therefore, solely imitating previous successful experiences without extracting targeted rules will lead to task failure. Additionally, due to the flexibility of free-form codes, our method shows better adaptability while requiring fewer expert examples than prior methods.

**Learning Curves.** We show the success rate curves (testing with GPT-4-turbo or GPT-3.5-turbo) when gradually increasing the tasks of the building stage in Fig 4. In the left image, we share rules across all task types (Cross-task Type), as in AutoManual, or each task type builds a separate set of rules (Single-task Type) during the building stage. Fig 4 (a) demonstrates that sharing rules across task types can facilitate rule optimization. The rules for each task type deepen understanding of the environment, thereby boosting the planning of other tasks. In Fig 4 (b), we compare AutoManual and Planner+Lib. on 9 tasks with feedback in MiniWob++. We find that Planner+Lib. tends to get stuck with a lower success rate. In the face of highly diverse scenarios, Skill Library cannot express the rules behind the environment, thus falling into the Path Dependency problem.

### Ablation Study

In this ablation study, we quantify the impact of each core component of the AutoManual framework on performance, specifically focusing on success rates and error reduction during task execution. Since we allowed the Planner to replan up to 3 times, each task could have up to 4 error steps.

**Online v.s. Offline Rule Management:** We perform offline AutoManual by collecting all trajectories and then managing rules from them. As Tab 4 shows, without online rule management, the generated manual can only slightly improve planning (from 88.1% to 90.7%). This is because more mundane mistakes and fewer direct successes will occur in the trajectories (the distributional shift problem), and the rules cannot be verified by feedback from the environment.

**Skill&Reflection Libraries:** Retrieving historical conclusions is essential for correct planning, as they record massive interacting details that can complement the rules. Without them, there will be

   Online & Skill\&Reflect Lib. & Case Prompt & Formulation & Avg. Error Steps (\(\)) & Success Rate (\%) \\   & & & & 2.3 & 77.6 \\  & ✓ & & & 1.5 & 88.1 \\  & ✓ & ✓ & ✓ & 1.3 & 90.7 \\  ✓ & & ✓ & ✓ & 1.6 & 89.5 \\ ✓ & ✓ & & ✓ & 1.0 & 93.8 \\ ✓ & ✓ & ✓ & & 0.5 & 96.5 \\ ✓ & ✓ & ✓ & ✓ & **0.3** & **97.4** \\   

Table 4: Ablation study of AutoManual on ALFWorld when testing with GPT-4-turbo.

Figure 4: (a) The success rate curve with standard deviation when testing GPT-4-turbo or GPT-3.5-turbo on ALFWorld. Building is performed cross-task or single-task type. (b) The success rate curve with standard deviation using AutoManual or Planner+Lib. when testing with GPT-4-turbo or GPT-3.5-turbo on 9 task types with feedback in MiniWob++.

more errors in the details, and the success rate drops from 97.4% to 89.5%. However, as discussed previously, using plain experiences without inducing rules will lead to Path Dependency.

**Case-Conditional Prompts:** This strategy further improves the rule management process by reducing the hallucination, as evidenced by an increase in success rate from 93.8% to 97.4%. These prompts ensure that the Builder updates rules reasonably and grounded.

**Effect of Manual Formulation:** The final formulation of rules into a comprehensive manual contributed to the success rate of 97.4% and decreased average error steps, demonstrating the effectiveness of presenting rule-based knowledge in an organized and accessible format. It not only aids the Planner in mastering multiple rules but is also friendly for human reading.

## 5 Conclusion

In this paper, we introduce AutoManual, a framework significantly advancing LLM agents by enabling adaptability and continual learning through online rule optimization. Utilizing the structured rule system, AutoManual autonomously generates comprehensive manuals, achieving high success rates in benchmarks like ALFWorld and MiniWoB++. This approach reduces reliance on human-provided examples and expert interventions, illustrating a robust method for enhancing agent generalization and addressing the Path Dependency problem in diverse environments.

## 6 Acknowledgements

Thanks to Dr. Zhe Zeng for her invaluable assistance with the OpenAI API and GPT Plus services. This work was supported in part by The National Nature Science Foundation of China (Grant No: 62273303), in part by Yongjiang Talent Introduction Programme (Grant No: 2022A-240-G).