# Tropical Expressivity of Neural Networks

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose an algebraic geometric framework to study the expressivity of linear activation neural networks. A particular quantity that has been actively studied in the field of deep learning is the number of linear regions, which gives an estimate of the information capacity of the architecture. To study and evaluate information capacity and expressivity, we work in the setting of tropical geometry--a combinatorial and polyhedral variant of algebraic geometry--where there are known connections between tropical rational maps and feedforward neural networks. Our work builds on and expands this connection to capitalize on the rich theory of tropical geometry to characterize and study various architectural aspects of neural networks. Our contributions are threefold: we provide a novel tropical geometric approach to selecting sampling domains among linear regions; an algebraic result allowing for a guided restriction of the sampling domain for network architectures with symmetries; and an open source library to analyze neural networks as tropical Puiseux rational maps. We provide a comprehensive set of proof-of-concept numerical experiments demonstrating the breadth of neural network architectures to which tropical geometric theory can be applied to reveal insights on expressivity characteristics of a network. Our work provides the foundations for the adaptation of both theory and existing software from computational tropical geometry and symbolic computation to deep learning.

## 1 Introduction

Deep learning has become the undisputed state-of-the-art for data analysis and has wide-reaching prominence in many fields of computer science, despite still being based on a limited theoretical foundation. Developing theoretical foundations to better understand the unparalleled success of deep neural networks is one of the most active areas of research in modern statistical learning theory. _Expressivity_ is one of the most important approaches to quantifiably measuring the performance of a deep neural network--such as how they are able to represent highly complex information implicitly in their weights and to generalize from data--and therefore key to understanding the success of deep learning.

_Tropical geometry_ is a reinterpretation of algebraic geometry that features piecewise linear and polyhedral constructions, where combinatorics naturally comes into play [e.g., 1, 2, 3]. These characteristics of tropical geometry make it a natural framework for studying the linear regions in a neural network--an important quantity in deep learning representing the network information capacity . The intersection of deep learning theory and tropical geometry is a relatively new area of research with great potential towards the ultimate goal of understanding how and why deep neural networks perform so well. In this paper, we propose a new perspective for measuring and estimating the expressivity and information capacity of a neural networks by developing and expanding known connections between neural networks and tropical rational functions in both theory and practice.

Related Work.Tropical geometry has been used to characterize deep neural networks with piecewise linear activation functions, including two of the most popular and widely-used activation functions, namely, rectified linear units (ReLUs) and maxout units. The first explicit connection between tropical geometry and neural networks establishes that the decision boundary of a deep neural network with ReLU activation functions is a tropical rational function . Concurrently, it was established that the maxout activation function fits input data by a tropical polynomial . These works considered neural networks whose input domain is Euclidean, which was recently developed to incorporate tropically-motivated input domains, in particular, the tropical projective torus . Most recently, tropical geometry has been used to construct convolutional neural networks that are robust to adversarial attacks via tropical decision boundaries .

Contributions.In this paper, we establish novel algebraic and geometric tools to quantify the expressivity of a neural network. Networks with a piecewise linear activation compute piecewise linear functions where the input space is divided into areas; the network computing a single linear function on each area. These areas are referred to as the _linear regions_ of the network; the number of distinct linear regions is a quantifiable measure of expressivity of the network [e.g., 5]. In our work, we not only study the number of linear regions, we aim to understand their _geometry_. The main contributions of our work are the following.

* We provide a geometric characterization of the linear regions in a neural network via the input space: estimating the linear regions is typically carried out by random sampling from the input space, where randomness may cause some linear regions of a neural network to be missed and result in an inaccurate information capacity measure. We propose an _effective sampling domain_ as a ball of radius \(R\), which is a subset of the entire sampling space that hits all of the linear regions of a given neural network. We compute bounds for the radius \(R\) based on a combinatorial invariant known as the _Hoffman constant_, which effectively gives a geometric characterization and guarantee for the linear regions of a neural network.
* We exploit geometric insight into the linear regions of a neural network to gain dramatic computational efficiency: when networks exhibit invariance under symmetry, we can restrict the sampling domain to a _fundamental domain_ of the group action and thus reduce the number of samples required. We experimentally demonstrate that sampling from the fundamental domain provides an accurate estimate of the number of linear regions with a fraction of the compute requirements.
* We provide an open source library integrated into the Open Source Computer Algebra Research (OSCAR) system  which converts both trained and untrained arbitrary neural networks into algebraic symbolic objects. This contribution then opens the door for the extensive theory and existing software on symbolic computation and computational tropical geometry to be used to study neural networks.

The remainder of this paper is organized as follows. We provide an overview of the technical background on tropical geometry and its connection to neural networks in Section 2. We then devote a section to each of the contributions listed above--Sections 3, 4, and 5, respectively--in which we present our theoretical contributions and numerical experiments. We close the paper with a discussion on limitations of our work and directions for future research in Section 6.

## 2 Technical Background

In this section, we give basic definitions from tropical geometry required to write tropical expressions for neural networks.

### Tropical Polynomials

Algebraic geometry studies geometric properties of solution sets of polynomial systems that can be expressed algebraically, such as their degree, dimension, and irreducible components. _Tropical geometry_ is a variant of algebraic geometry where the polynomials are defined in the _tropical semiring_, \(}=(\{\},,)\) where the addition and multiplication operators are given by \(a b=(a,b)\) and \(a b=a+b\), respectively. Define \(a b:=a-b\).

Using these operations, we can write polynomials as \(_{m}a_{m}T^{m}\), where \(a_{i}\) are coefficients, \(T\), and where the sum is indexed by a finite subset of \(^{n}\). In our work, we consider the following generalizations of tropical polynomials.

**Definition 2.1**.: A _tropical Puiseux polynomial_ in the indeterminates \(T_{1},,T_{n}\) is a formal expression of the form \(_{m}a_{m}T^{m}\) where the index \(n\) runs through a finite subset of \(_{ 0}^{m}\) and \(T^{m}=T_{1}^{m_{1}} T_{n}^{m_{n}}\), and taking powers in the tropical sense.

**Definition 2.2**.: A _tropical Puiseux rational map_ in \(T_{1},,T_{n}\) is a tropical quotient of the form \(p q\) where \(p,q\) are tropical Puiseux polynomials.

Tropical (Puiseux) polynomials and rational maps induce functions from \(^{n}\), which take a point \(x^{n}\) to the number obtained by substituting \(T=x\) in the algebraic expression and performing the (tropical) operations. It is important to note that tropically, the formal algebraic expression contains strictly more information than the corresponding function, since different tropical expressions can induce the same function.

### Tropical Expressions for Neural Networks

We now overview and recast the framework of , which establishes the first explicit connection between tropical geometry and neural networks, in a slightly different language for our results.

As in , the neural networks we will focus on are fully connected multilayer perceptrons with ReLU activation, i.e., functions \(^{n}^{m}\) of the form \( L_{d} L_{i-1} L_{1}\) where \(L_{i}:^{n_{i-1}}^{n_{i}}\) is an affine map and \((t)=\{t,0\}\). For the remainder of this paper, we use the term "neural network" to refer solely to these. We will always assume that the weights and biases of our neural networks are rational numbers. From a computational perspective, this is not a serious restriction since this is sufficient to describe any neural network with weights and biases given by floating point numbers. We refer to the tuple \([n,n_{1},,n_{d-1},m]\) as the _architecture_ of the neural network.

One of the key observations intersecting tropical geometry and deep learning is that, up to rescaling of rational weights to obtain integers, neural networks can be written as tropical rational functions [11, Theorem 5.2]. From a more computational perspective, it is usually preferable to avoid such rescaling and simply work with the original weights. The proof of Theorem 5.2 in  can directly be adapted to show that any neural network can be written as the function associated to a tropical Puiseux rational map. In their language, this corresponds to saying that any neural network is a _tropical rational signomial_ with nonnegative rational exponents.

## 3 Sampling Domain Selection Using a Hoffman Constant

Estimating the number of linear regions of a neural network typically proceeds by sampling points from the input domain and counting the memberships of these points. To guarantee that membership is exhaustive, we seek a sampling domain as a sufficiently large ball so that all linear regions are intersected. At the same time, we would like for the ball to be as small as possible to guarantee efficient sampling. We are thus searching for the smallest ball from which we can sample in such a way that all linear regions are intersected. Given the polyhedral geometry of tropical Puiseux rational maps, it turns out that the radius of this smallest ball that we seek is closely related to the _Hoffman constant_, which is a combinatorial invariant.

Our contribution in this section is a definition of a Hoffman constant of a neural network; we demonstrate its relationship to the smallest sampling ball and propose algorithms to compute its true value and lower and upper bounds.

### Defining a Neural Network Hoffman Constant

In simpler terms, the Hoffman constant can be expressed for a matrix as follows. Let \(A\) be an \(m n\) matrix. For any \(b^{m}\), let \(P(A,b)=\{x^{n}:Ax b\}\) denote the polyhedron determined by \(A\) and \(b\). For a nonempty polyhedron \(P(A,b)\), let \(d(u,P(A,b))=\{\|u-x\|:x P(A,b)\}\) denote the distance from a point \(u^{n}\) to the polyhedron, measured under an arbitrary norm \(\|\|\)on \(^{n}\). Then there exists a constant \(H(A)\) only depending on \(A\) such that

\[d(u,P_{A,b}) H(A)\|(Au-b)_{+}\|\] (1)

where \(x_{+}=\{x,0\}\) is applied coordinate-wise . The constant \(H(A)\) is called the _Hoffman constant_ of \(A\).

The Hoffman Constant for Tropical Polynomials and Rational Functions.Let \(f:^{n}\) be a tropical Puiseux polynomial and let \(=\{U_{1},,U_{m}\}\) be the set of linear regions of \(f\). Let \(f(x)=a_{i1}x_{1}++a_{in}x_{n}+b_{i}\) occur on the region \(U_{i}\). Further, let \(A=[a_{ij}]_{m n}\) be the matrix of coefficients in the expression of \(f\) over \(\). The linear region \(U_{i}\) is defined by the following inequalities

\[a_{i1}x_{1}++a_{in}x_{n}+b_{i} a_{j1}x_{1}++a_{jn}x_{n}+b_{j}, \;j=1,2,,m.\] (2)

In matrix form, (2) is equivalent to

\[(A-a_{i})x b_{i}-b\] (3)

where \(\) is a column vector of all 1's; \(a_{i}\) is the \(i\)th row vector of \(A\); and \(b\) is a column vector of all \(b_{i}\). Denote \(_{U_{i}}:=A-a_{i}\) and \(_{U_{i}}:=b_{i}-b\). Then the linear region \(U_{i}\) is captured by the linear system of inequalities \(_{U_{i}}x_{U_{i}}\).

**Definition 3.1**.: Let \(f:^{n}\) be a tropical Puiseux polynomial. The _Hoffman constant of \(f\)_ is defined as

\[H(f)=_{U_{i}}H(_{U_{i}}).\]

Care needs to be taken in defining a Hoffman constant for a tropical Puiseux rational map: We want to avoid having all linear regions defined by systems of linear inequalities, since there exist linear regions which are not convex. To do so, we consider convex refinements of linear regions induced by intersections of linear regions of tropical polynomials.

**Definition 3.2**.: Let \(p q\) be a difference of two tropical Puiseux polynomials. Let \(A_{p}\) (respectively \(A_{q}\)) be the \(m_{p} n\) (respectively \(m_{q} n\)) matrix of coefficients for \(p\) (respectively \(q\)). The _Hoffman constant of \(p q\)_ is

\[H(p q):=HA_{p}\\ A_{q}-a_{i_{p}}\\ a_{i_{q}}:i_{p}=1,,m_{p};\;i_{q}=1,,m_{q} }.\] (4)

Let \(f\) be a tropical Puiseux rational map. Then the _Hoffman constant of \(f\)_ is defined as the minimal Hoffman constant of \(H(p q)\) over all possible expressions of \(f=p q\).

Given the correspondence between neural networks and tropical Puiseux rational maps, the Hoffman constant is well-defined for any neural network and may be computed from the geometry and combinatorics of its linear regions.

### The Minimal Effective Radius

For a neural network whose tropical Puiseux rational map is \(f:^{n}\), let \(=\{U_{1},,U_{m}\}\) be the collection of all linear regions. For any \(x^{n}\), define the _minimal effective radius_ of \(f\) at \(x\) as

\[R_{f}(x):=\{r:B(x,r) U_{i},U_{i}\}\]

where \(B(x,r)\) is the ball of radius \(r\) centered at \(x\). That is, \(R_{f}(x)\) is the minimal radius such that the ball \(B(x,r)\) intersects all linear regions. It is the smallest required radius of sampling around \(x\) in order to express the full classifying capacity of the neural network \(f\).

We start with the following lemma which relates the minimal effective radius to the Hoffman constant when \(f\) is a tropical Puiseux polynomial.

**Lemma 3.3**.: _Let \(f\) be a tropical Puiseux polynomial and \(x^{n}\) be any point, then_

\[R_{f}(x) H(f)_{U_{i}}\|(_{U_{i}}x- _{U_{i}})_{+}\|.\] (5)In particular, we are interested in studying when \(^{m}\) and \(^{n}\) are equipped with the \(\)-norm. In this case, the minimal effective radius can be related to the Hoffman constant and function value of \(f=p q\). For a tropical Puiseux polynomial \(p(x)=_{1 i m_{p}}\{a_{i}x+b_{i}\}\), let \((x)=_{1 j m_{q}}\{a_{j}x+b_{j}\}\) be its min-conjugate.

**Proposition 3.4**.: _Let \(f=p q\) be a tropical Puiseux rational map. For any \(x^{n}\), we have_

\[R_{f}(x) H(p q)\{p(x)-(x),\,q(x)-(x)\}.\] (6)

### Computing and Estimating Hoffman Constants

The PVZ Algorithm.In , the authors proposed a combinatorial algorithm to compute the precise value of the Hoffman constant for a matrix \(A^{m n}\), which we refer to as the _Peia-Vera-Zuluaga (PVZ) algorithm_ and sketch its main steps here.

**Definition 3.5**.: A set-valued map \(:^{n}^{m}\) assigns a set \((x)^{m}\). The map is surjective if \((^{n})=_{x}(x)=^{m}\). Let \(A^{m n}\). For any \(J\{1,2,,m\}\), let \(A_{J}\) be the submatrix of \(A\) consisting of rows with indices in \(J\). The set \(J\) is called \(A\)_-surjective_ if the set-valued map \((x)=A_{J}x+\{y^{J}:y 0\}\) is surjective.

Notice that \(A\)-surjectivity is a generalization of linear independence of row vectors. We illustrate this observation in the following two examples.

**Example 3.6**.: If \(J\) is such that \(A_{J}\) is full-rank, then \(J\) is \(A\)-surjective, since for any \(y^{J}\), there exists \(x^{n}\) such that \(y=A_{J}x\).

**Example 3.7**.: Let \(A=_{m n}\) be the \(m n\) matrix whose entries are 1's. For any subset \(J\) of \(\{1,,m\}\) and for any \(y^{J}\), let \(x^{n}\) such that \(_{i}x_{i}\{y_{j},j J\}\). Then \(y-A_{J}x 0\). Thus any \(J\) is \(A\)-surjective.

The PVZ algorithm is based on the following characterization of Hoffman constant.

**Proposition 3.8**.: _[_17_, Proposition 2]_ _Let \(A^{m n}\). Equip \(^{m}\) and \(^{n}\) with norm \(\|\|\) and denote its dual norm by \(\|\|^{*}\). Let \((A)\) be the set of all \(A\)-surjective sets. Then_

\[H(A)=_{J(A)}H_{J}(A)\] (7)

_where_

\[H_{J}(A)=_{y^{m}\|y\| 1}_{x ^{n}\\ A_{J}x yJ}\|x\|=^{n}_{+},\|v\| ^{*}=1}\|A_{J}^{}v\|^{*}}.\] (8)

This characterization is particularly useful when \(^{m}\) and \(^{n}\) are equipped with the \(\)-norm, since the computation of (8) reduces to a linear programming (LP) problem. The key problem is how to maximize over all \(A\)-surjective sets. To do this, the PVZ algorithm maintains three collections of sets \(\), \(\), and \(\) where during every iteration: (i) \(\) contains \(J\) such that \(J\) is \(A\)-surjective; (ii) \(\) contains \(J\) such that \(J\) is not \(A\)-surjective; and (iii) \(\) contains candidates \(J\) whose \(A\)-surjectivity will be tested.

To detect whether a candidate \(J\) is surjective, the PVZ algorithm requires solving

\[\|A_{J}^{T}v\|_{1},\,\,s.t.\,\,v^{J}_{+},\|v\|_{1}=1.\] (9)

If the optimal value is positive, then \(J\) is \(A\)-surjective, and \(J\) is assigned to \(\) and all subsets of \(J\) are removed from \(\). Otherwise, the optimal value is 0 and there is \(v^{J}_{+}\) such that \(A_{J}^{}v=0\). Let \(I(v)=\{i J:v_{i}>0\}\) and assign \(I(v)\) to \(\). Let \(\) be any set containing \(I(v)\). Replace all such \(\) by sets \(\{i\},i I(v)\) which are not contained in any sets in \(\). The implementation used in our paper directly uses the MATLAB code provided by .

Lower and Upper Bounds.A limitation of the PVZ algorithm is that during each loop, every set in \(\) needs to be tested, and each test requires solving a LP problem. Although solving one LP problem in practice is fast, a complete while loop calls the LP solver many times.

Here, we propose an algorithm to estimate lower and upper bounds for Hoffman constants. An intuitive way to estimate the lower bound is to sample a number of random subsets from \(\{1,,m\}\) and test for \(A\)-surjectivity. This method bypasses optimizing combinatorially over \((A)\) of \(A\)-surjective sets and gives a lower bound of Hoffman constant by Proposition 3.8.

To get an upper of Hoffman constant, we use the result from .

**Theorem 3.9**.: _[_18_, Theorem 4.2]_ _Let \(A^{m n}\). Let \((A)\) be a set of subsets of \(J\{1,,m\}\) such that \(A_{J}\) is full rank. Let \(^{*}(A)\) be the set of maximal elements in \((A)\). Then the Hoffman constant measured under 2-norm is bounded by_

\[H(A)_{J^{*}(A)}(A_{J})}\] (10)

_where \((A)\) is the smallest singular value of \(A\)._

Using the fact that \(\|\|_{1}\|\|_{2}\), and the characterization from (8), we see that the upper bound also holds when \(^{m}\) and \(^{n}\) are equipped with the \(\)-norm. However, enumerating all maximal elements in \((A)\) is not an improvement over enumerating \(A\)-surjective sets from a computational perspective. Instead, we will retain the strategy as in lower bound estimation to sample a number of sets from \(\{1,2,,m\}\) and approximate the upper bound by (10). We verify this approach via synthetic data. The experiments are relegated to the Appendix.

## 4 Symmetry and the Fundamental Domain

In this section, we study a geometric characterization of the sampling domain for networks exhibiting symmetry. This corresponds to _invariant neural networks_.

### Linear Regions of Invariant Neural Networks

The notion of invariance for a neural network describes when a manipulation of the input domain does not affect the output of the network. The manipulations we consider here are group actions.

**Definition 4.1**.: Let \(:^{n}\) be a piecewise linear function, and let \(G\) be a group acting on the domain \(^{n}\). \(\) is _invariant_ under the group action of \(G\) if for any element \(g G\), \( g=\).

Given an invariant neural network, we can then define a sampling domain that takes into account the effect of the group action.

**Definition 4.2**.: Let \(G\) be a group acting on \(^{n}\). A subset \(^{n}\) is a _fundamental domain_ if it satisfies two following conditions: (i) \(^{n}=_{g G}g\); and (ii) \(g() h()=\) for all \(g,h G,g h\).

The fundamental domain of a group \(G\) therefore provides a periodic tiling of \(^{n}\) by acting on \(\). This is very useful in the context of numerical sampling for neural networks which are invariant under some symmetry, since it means we can sample from a smaller subset of the input domain with a guarantee to find all the linear regions in the limit. This allows us, in principle, to be able to use far fewer samples while maintaining the same density of points.

**Theorem 4.3**.: _Let \(f:^{N}\) be a tropical rational map invariant under group \(G\). Let \(^{N}\) be a fundamental domain of \(G\). Suppose \(\) is the set of linear regions. Define the following two sets_

\[_{c} :=\{A:A\}\] \[_{n} :=\{A:A\}.\]

_Then_

\[|G||_{c}||||G||_{c}|+_{A _{n}_{c}}|}.\]

_where \(|G_{A}|\) is the size of the stabilizer of \(A\)._

This gives us a method for estimating the total number of linear regions from sampling in the fundamental domain using _multiplicity_, which we discuss next.

### Sampling from the Fundamental Domain

To demonstrate the potential performance improvements in numerical sampling exploiting symmetry in the network architecture, we consider permutation invariant neural networks inspired by deep sets . Our numerical sampling approach is inspired by very recent work in this area .

**Lemma 4.4** ().: _An \(m m\) matrix \(W\) acting as a linear operator of the form \(W= I_{m m}+(^{T})\), where \(,\) is permutation equivariant, meaning \(WPx=PWx\) for any \(x^{m}\), so it commutes with any permutation matrix._

Using a weight matrix of this form, we can construct permutation invariant neural networks by setting the bias to 0, applying a ReLU activation after multiplication by \(W\), and then summing. In this case, the network is invariant under the group action \(S_{n}\), so the fundamental domain is the set of points with increasing coordinates, i.e., \(=\{(x_{1},,x_{n}):x_{1} x_{2} x_{n}\}\). This splits \(^{n}\) into \(n!\) tiles, so we have a clear and significant advantage in restricting sampling to the fundamental domain.

Note, however, that it is important to address the multiplicities of symmetric linear regions correctly: If a given Jacobian of shape \(n 1\) has no repeated elements, this means it is contained in the interior of some group action applied to the fundamental domain. This means there are \(n!\) total linear regions with this Jacobian. If, on the other hand, there are repeated coefficients in a given Jacobian \(J\), we consider the set \(C(J)\) of counts of repeated elements. For example, for \(J=,C(J)=(2,1)\). Then the multiplicity of a given Jacobian is given by

\[(J)=c!}.\]

Using this multiplicity calculation we can efficiently estimate the number of linear regions while reducing the number of point samples by a factor of \(n!\). This provides a dramatic gain in sampling efficiency.

In Figure 1, we present the results when Algorithm 2 is run with \(R=10,N=10,M=50\). These results show that the fundamental domain estimate performs well for low dimensional inputs but appears to overcount linear regions as \(n\) increases. Despite divergence, there is still utility in this metric because we are often more concerned with obtaining an upper bound on the expressivity of a neural network than an exact figure and the fundamental domain estimate does not undercount the number of linear regions.

## 5 Symbolic Neural Networks

Here, we present the details on our practical contribution of a symbolic representation of neural networks as a new library integrated into OSCAR .

### Computing Linear Regions of Tropical Puiseux Rational Maps

We present an algorithm that can compute the linear regions of _any_ tropical Puiseux rational function. Intuitively, we do this by computing the linear regions of the numerator and denominator, and then considering intersections of such regions and how they fit together. Thus, a first step is to understand how the computation of linear regions works for tropical Puiseux polynomials. The key to our approach will be to exploit the polyhedral connection of tropical geometry and recast the problem in the language of polyhedral geometry. This, among other things, will allow us to make use the extensive polyhedral geometry library in OSCAR  for implementation.

One important upshot from this study is that there is a strong connection between the number of linear regions of a tropical Puiseux rational function and the number of monomials that appear in its algebraic expression. Note, however, that the two are independent, in the sense that two Puiseux rational functions could have the same number of linear regions but different numbers of (nonzero) monomials, and conversely, the same number of monomials and a different number of linear regions. For instance, computing the number of linear regions requires some combinatorial data about the intersections of the polyhedra defined by monomials.

First, we need to know how to compute the linear regions of tropical polynomials. Let \(P=_{n}a_{n} x^{n}\) where by \(x^{n}\) we mean \(x_{1}^{n_{1}} x_{k}^{n_{k}}\) and powers are taken in the tropical sense. Thenas function \(^{k}\), \(P\) is given by \(_{n}\{a_{n}+n_{1}x_{1}+n_{k}x_{k}\}.\) It follows that the linear regions of \(P\) are precisely the sets of the form

\[S_{n}=\{x^{n} a_{m}+m_{1}x_{1}+m_{k}x_{k} a_{n} +n_{1}x_{1}+n_{k}x_{k}m n\}.\]

For any set \(U\) on which \(P\) is linear, we write \(L(P,U)\) for the corresponding linear map. This gives us

\[L(P,S_{n})(x)=a_{n}+n_{1}x_{1}+n_{k}x_{k}.\] (11)

We now rewrite (11) using polyhedral geometry. Recall that a polyhedron in \(^{k}\) is a set of the form \(P(A,b)=\{x^{k} Ax b\}\). We claim that each linear region is a polyhedron: For a fixed index \(n\), define the matrix \(A_{n}\) to be the \((N-1) k\) matrix whose rows are the vectors \(m-n\), where \(m\) ranges over the support of the coefficients of \(P\) (ordered lexicographically) and \(b_{n}\) to be the vector with entries \(a_{n}-a_{m}\). Then \(S_{n}=P(A_{n},b_{n})\). This gives us a way to encode the computation of the linear regions of tropical Puiseux polynomials using polyhedral geometry. As a direct consequence, intersections of linear regions of tropical Puiseux polynomials are also polyhedra. In particular, there are algorithms from polyhedral geometry for determining whether such polyhedra are realizable. One of the key observations given by our algorithm is that the linear regions of tropical Puiseux rational maps are _almost_ given by \(k\)-dimensional intersections of the linear regions of the numerator and the denominator. Indeed, note that if \(U\) is a linear region of \(p\) and \(V\) a linear region of \(q\), then we have \(L(U V,p q)=L(U,p)-L(V,q)\). The only issue that arises is that there might be some repetition in the \(L(U V,p q)\) as \(U\) ranges over the linear regions of \(p\) and \(V\) over the linear regions of \(q\). In particular, linear regions of \(p q\) might end up corresponding to unions of such \(U V\).

### Computing Linear Regions

Determining the linear regions of a neural network may be approached _numerically_ or _symbolically_. The numerical approach exploits the fact that linear regions of a neural network correspond to regions where the gradient is constant. Thus, to estimate the number of linear regions, we can evaluate the gradient on a sample of points (e.g., a mesh) in some large box \([-R,R]^{n}\). For sufficiently large \(R\) and a sufficiently dense sample of points, we get an accurate estimate. The symbolic approach, on the other hand, exploits the connection between neural networks and tropical Puiseux rational maps. Indeed, we can symbolically compute a Puiseux rational map that represents the neural network and then compute the number of linear regions using the approach outlined in section 5.1.

To compare each method, we ran the computations on smaller networks with varying sizes to compare run times and precision. For the symbolic approach, we generate 20 neural networks with random weights for each architecture and then compute the tropical Puiseux rational function associated to each neural network and compute the linear regions using Algorithm 3.

For the numerical approach, we also work with synthetic data and generate \(1000\) neural networks with random weights for each architecture. We then estimate the number of linear regions in a box of size \([-10,10]^{n}\) and sample \(1000\) points from this domain.

In both cases, we use He initialization for the weights, i.e., we generate weights with distribution \(N(0,})\) where \(d\) is the input dimension. The data we obtain in this manner is summarized in Tables 10 and 11. For the symbolic approach, we also track the number of nonzero monomials to compare this quantity with the number of linear regions. For networks with \(3\) layers, we find the numerical estimate to be quite close, but for \(4\) it seems to diverge. This could be because in the numerical approach, we are only counting the number of unique Jacobians that can be found in the domain. A situation could arise where the same linear function is disconnected and hence counted twice by the symbolic approach but only once for the numerical approach.

The main observations from our experimental study are as follows. The numerical approach is faster, but offers no guarantee of precision: When running the computation for a given \(R\) and mesh grid, there seems to be no easy way of determining whether we have indeed hit all the linear regions or whether we have obtained an accurate estimate of the arrangements of these regions. It is possible to either overestimate or underestimate the number of linear regions. In particular, there is a priori no obvious way to select the parameters. We found the symbolic approach to be more precise, but slower. In general, the number of monomials seems to be far larger than the number of linear regions, which contradicts the intuition of Figure 2.

Both algorithms suffer from the curse of dimensionality: in the case of the numerical approach, the number of samples in a meshgrid grows exponentially with respect to the dimension. In the case of the symbolic approach, calculations with polytopes seem to scale poorly with dimension and with the complexity of the neural network.

## 6 Discussion: Limitations & Directions for Future Research

In this paper, we set up a framework to interpret and analyzed the expressivity of neural networks using techniques from polyhedral and tropical geometry. We demonstrated several ways in which a symbolic interpretation can often enable computational optimizations for otherwise intractable tasks and provided new insights into the inner workings of these networks. To the best of our knowledge, ours is the first work to provide practical tropical geometric theory and algorithms to numerically compute and analyze the expressivity of a neural network both in terms of inherent neural network quantities as well as tropical geometric quantities.

Despite the theoretical and practical advancement of tropical deep learning that our work offers, it is nevertheless subject to limitations, which we now discuss and which inspire directions for future research.

Experimental Limitations.The curse of dimensionality is a common theme in deep learning, and our work is unfortunately no exception. The methods introduced in this paper are quite fast for small enough networks, but scale poorly with dimension and more complex architectures.

We note that the main computational bottlenecks of the Puiseux rational function associated with a neural network are the implementation of fast multivariate Puiseux series operations. Our current computations rely on a custom implementation of this type of operation, and one potential avenue for improvement would be using such methods once they have been implemented in OSCAR .

For the computation of linear regions, both the numerical and symbolic approaches suffer from the curse of dimensionality. For instance, the numerical approach requires sampling on a mesh grid in a box of the form \([-R,R]^{n}\) where \(n\) is the input dimension. In particular, the number of points needed is proportional to the volume, which scales exponentially in \(n\). Similarly, the symbolic approach relies on the computation of the Puiseux rational function associated with a neural network and polytope computations, both of which are challenging computational problems in higher dimensions.

Most of our computations rely on carrying out some elementary computations many times. Thus, another avenue of improvement for this would be to parallelize.

Structural Limitations.Much of what we are studying are basically framed as a combinatorial optimization problem, which are known to be difficult. In particular, computing the Hoffman constant is equivalent to the Stewart-Todd condition measure of a matrix and both quantities are NP-hard to compute in general cases [17; 21].

Further studying and understanding where and how symbolic computation algorithms can be made more efficient, e.g., by parallelization, would make our proposed approaches more applicable to larger neural networks. Our work effectively proposes a new intersection of symbolic computation and deep learning, so there remains infrastructure to set up to make methods from these two fields compatible.