# UPS: Unified Projection Sharing for Lightweight

Single-Image Super-resolution and Beyond

 Kun Zhou\({}^{1,2}\),  Xinyu Lin\({}^{1,2}\),  Zhonghang Liu\({}^{3}\),  Xiaoguang Han\({}^{1}\),  Jiangbo Lu\({}^{2}\)

\({}^{1}\)SSE, CUHK-Shenzhen, \({}^{2}\)SmartMore Corporation \({}^{3}\)SMU, Singapore

hanxiaoguang@cuhk.edu.cn, jiangbo.lu@gmail.com

Project leaderCo-first authorCorresponding author

###### Abstract

To date, Transformer-based frameworks have demonstrated impressive results in single-image super-resolution (SISR). However, under practical _lightweight_ scenarios, the complex interaction of deep image feature extraction and similarity modeling limits the performance of these methods, since they require simultaneous _layer-specific_ optimization of both two tasks. In this work, we introduce a novel Unified Projection Sharing (UPS) algorithm to decouple the feature extraction and similarity modeling. To achieve this, we establish a unified projection space defined by a learnable projection matrix, for similarity calculation across _all_ self-attention layers. As a result, deep image feature extraction remains a per-layer optimization manner, while similarity modeling is carried out by projecting these image features onto the shared projection space. Extensive experiments demonstrate that our proposed UPS achieves state-of-the-art performance relative to leading lightweight SISR methods, as verified by various popular benchmarks. Moreover, our unified optimized projection space exhibits encouraging robustness performance for unseen data (degraded and depth images). Finally, UPS also demonstrates promising results across various image restoration tasks, including real-world and classic SISR, image denoising, and image deblocking.

## 1 Introduction

Single-image super-resolution is a fundamental task in computer vision, aiming to enhance the resolution and quality of a low-resolution image. Recently, Transformer-based methods , especially, SwinIR , combines the benefits of window-based self-attention and convolutional feature extraction, thus achieving effective similarity modeling and feature extraction. It yields promising outcomes, reducing computational demand compared to global/non-local attention mechanisms.

However, the coupled optimization in existing Transformer-based methods may face two challenges. First, in a lightweight configuration characterized by a very limited number of learnable parameters, performing layer-specific optimization for both image feature extraction and similarity modeling remains challenging. Second, such a tightly coupled optimization scheme (image feature extraction and projection similarity are synchronously updating in each layer during the training phase) may suffer from co-adaptation issue , potentially leading to inferior results.

Interestingly, we observe that projection spaces (layer) in trained SwinIR-light exhibit _substantial_ layer-to-layer (CKA ) similarities4. Fig. a.(1-3) below shows over \(0.95\) (\(0.99\), \(0.95\), \(0.96\)) for\(\{2,3,4\}\)) (projection layer) pairs get over 0.9 scores (ranging from 0 to 1)5. This experiment suggests that all the projection layers are _highly similar_.

To mitigate the two problems, we are motivated by the observation and explore a novel Unified Projection Sharing (UPS) technique for lightweight SISR. In particular, UPS decouples the deep image feature representation and similarity learning: it performs the layer-specific image feature extraction while calculating the self-similarity in a unified projection space. In other words, the similarity modeling is optimized in a _layer-invariant_ manner, effectively separating the learning of both two tasks. More specifically, UPS accomplishes self-similarity modeling with the following three steps: (i) UPS defines a unified projection space by a learnable matrix; (ii) for each self-attention layer, it projects deep image features onto the unified projection space; (iii) it calculates the attention map using the Cosine similarity metric in the projection space and performs attention-based aggregation.

Our proposed UPS consistently demonstrates superior performance compared to existing approaches across all testing benchmarks. Notably, our method outperforms the second-best model by more than \(0.33\)dB on the Manga109 dataset for the \( 2\) settings. Furthermore, our model exhibits significant improvement over our baseline model, SwinIR-light , achieving enhancements of up to \(0.50\)dB, \(0.55\)dB, \(0.47\)dB on the Manga109 dataset for the \( 2, 3, 4\) settings, while utilizing fewer parameters. Our contributions are summarized as follows:

\(-\) We propose UPS, an effective decoupled SISR optimization framework, to address the challenge of simultaneous layer-specific feature extraction and similarity modeling for lightweight SISR.

\(-\) UPS simplifies the similarity optimization process by learning a layer-invariant projection space, leading to effective aggregation (activating more local/non-local pixels as shown in Fig. 3) and improved performance, even with reduced model capacity (see Fig. 1) and less training samples (see the data efficiency analysis in Sec. A.2).

\(-\) Extensive robustness analysis in Sec. 5.4, 5.5, A.3, A.4, have confirmed the good generalization ability of our proposed UPS for unseen data, such as noisy image and depth map SR.

## 2 Related Works

**CNN-based SISR.** Due to their low complexity and helpful feature extraction abilities, CNNs have been widely used for SISR task. SRCNN  pioneered the use of deep convolutional neural network (CNN) architectures specifically designed for single image super-resolution (SISR). SRCNN consists of only three layers: patch extraction, non-linear mapping, and reconstruction. It has demonstrated competitive performance compared to traditional non-deep methods, inspiring the development of numerous lightweight CNN approaches in the SISR field. ESPCN  introduced a compact network

Figure 1: (a) We observe that the SwinIR-light (termed as Base) models exhibit significant similarities (CKA ) in projection layers. (b) Comparison between our proposed UPS and SOTA lightweight SISR models on BSD100  for \( 2\) setting. A bigger circle size means a larger number of parameters. While being the most computationally and parameter-efficient, UPS-S (a more lightweight version of our method) demonstrates highly competitive results compared to SOTA methods.

architecture that employs sub-pixel convolutional layers to upscale low-resolution image features. In contrast, LapSRN  utilizes image structure priors across different pyramid representations, resulting in improved performance while minimizing computational overhead. Taking inspiration from dictionary-learning models [15; 16; 17], LAPAR  learns linear coefficients associated with pre-defined basic up-sampling kernels to produce an optimal pixel-specific kernel, achieving superior super-resolution results. LatticeNet  designs a parameter-efficient convolutional lattice block to extract hierarchical contextual features. Despite their computational efficiency, CNN-based models are limited in terms of long-term aggregation due to content-invariant similarity optimization.

**Transformer-based SISR.** Recently, Transformer-based techniques [20; 21; 22; 23; 24; 25] have achieved remarkable outcomes in SISR but still suffer from high complexity. The computational cost of non-local self-similarity modeling increases quadratically with the size of the image. Inspired by the success of Swin Transformer , numerous window-based Transformer frameworks emerged to address the efficiency of SISR. For example, SwinIR  introduces a residual window-based transformer block (RSTB) for image feature extraction and similarity-based aggregation, outperforming previous CNN-based and Transformer-based approaches. DLGSA-l  proposes a global sparse attention technique to enhance the aggregation of relevant tokens. NGswin  incorporates the N-Gram context to attain a larger receptive field, activating more neighboring pixels for effective aggregation. However, when it comes to lightweight setups, the optimization of coupled feature extraction and similarity calculation is limited, resulting in inferior performance.

**Efficient Transformers.** On the other hand, some advanced transformers have been proposed to reduce the computational complexity, enhancing inference or training efficiency. ShareFormer  presents a local similarity map-sharing scheme between neighboring attention layers for lower latency. Thus, ShareFormer shares a static similarity map for neighboring attention layers while UPS calculates dynamic similarity maps with layer-refined features in a shared projection space.

Skip-Attention  cuts off some intermediate attention layers to improve efficiency and performance for high-level tasks. LaViT  proposes a residual-based attention downsampling that fuses the initial calculated attention scores to guide the aggregation of the following layers, resulting in faster efficiency and improved classification accuracy.

Therefore, Skip-Attention and LaViT follow the existing coupled optimization scheme (reduce some attention calculations), and UPS proposes a decoupled learning strategy to enhance performance. We will cite the insightful studies and add this discussion to our revised paper.

## 3 Understanding Swin Transformer

**Preliminaries.** Swin Transformer  proposes an effective self-attention mechanism, achieving long-range information capture at a lower computation complexity. Inspired by Swin Transformer, several subsequent methods [7; 32; 28] dedicated to solving lightweight SISR have emerged, consistently enhancing the quality of super-resolved images. Fig. 2(a) illustrates the general framework architecture of the Swin Transformer-based SISR method. It consists of three primary components: a shallow head module, a deep image feature extraction and aggregation (FEA) module, and a tail reconstruction module. The head module is tasked with converting the input low-resolution RGB image into a high-dimensional feature space. The FEA module, the key role in the whole architecture, is composed of multiple (\(N\)) Swin Transformer layers (STLs). Each STL has two main objectives: (i) extracting image features and (ii) modeling similarities using a learnable projection space. The former focuses on capturing essential image features, while the latter employs window-based self-attention to facilitate spatially adaptive aggregation. Notably, similarity modeling optimizes a projection space to obtain pixel-wise correlations, which is achieved by projecting image features into the learnable projection space and calculating similarity scores. Finally, the tail module generates the final high-resolution output image, completing the SISR process. In the subsequent section, we will delve into the details of the STL, with a particular emphasis on deep feature extraction and similarity modeling aspects.

### Decomposing Swin Transformer Layer

Efficient deep feature extraction and similarity modeling are accomplished by the Swin Transformer Layer (STL), the fundamental unit within the FEA module of the Swin Transformer. Illustrated in Fig. 2(b), in the \(i\)-th STL, the process begins by employing a convolutional layer to extract deep image feature \(_{i}\) from an input feature \(F_{i-1}\), which is the output of the preceding \((i-1)\)-th STL:

\[_{i}=(F_{i-1}).\] (1)

Subsequently, the STL executes a conventional window-based self-attention mechanism, comprising four basic steps: (i) window-partitioning, (ii) deep feature projection for similarity calculation, (iii) aggregation based on similarity to merge neighboring pixels, and (iv) patch merging.

**(i) Window-partitioning.** Initially, the updated image feature \(_{i}\) is reshaped into \(}\) non-overlapping patches, each with a shape of \(M^{2} C\), where \(M^{2}\) represents the spatial size of each patch and \(C\) is the channel dimension.

**(ii) Layer-specific projection.** Following window-partitioning, each divided image patch \(X_{i}\) from \(_{i}\) is projected to generate the corresponding query, key, and value matrices \(Q_{i},K_{i},V_{i}\):

\[Q_{i}=X_{i}P_{i}^{Q}, K_{i}=X_{i}P_{i}^{K}, V_{i}=X_{i}P_{i}^{V},\] (2)

where \(P_{i}^{Q},P_{i}^{K},P_{i}^{V} R^{\{d C\}}\) denote the learnable projection parameters specific to the \(i\)-th STL, while \(Q_{i},K_{i},V_{i} R^{\{M^{2} d\}}\) represent the projected features of patch \(X_{i}\) and \(d\) is the projection dimension. The similarity matrix is then computed:

\[S_{i}=(K_{i}^{T}}{}+B_{i}),\] (3)

where \(B_{i}\) represents a relative position encoding, and \(S_{i}\) is the predicted similarity map for the \(X_{i}\).

**(iii) Similarity-based Aggregation.** Later on, neighboring information within the patch \(X_{i}\) is aggregated based on the computed similarity map \(S_{i}\):

\[Y_{i}=S_{i}V_{i}.\] (4)

**(iv) Patch Merging.** Finally, all the aggregated image patches are reshaped into a 2D image feature which is fed into the next STL for further processing.

**Discussion.** With sufficient model capability, i.e., millions of parameters, SwinIR , a SOTA Swin Transformer SISR model, exhibits strong abilities for the SISR task. However, in resource-constrained, lightweight settings as previously mentioned, it potentially poses challenges to simultaneously optimize deep image feature extraction and projection space. We will compare the per-layer projection space optimization with our proposed UPS scheme later.

Figure 2: Overview of Transformer-based architecture for lightweight SISR. There are three main components: (i) a head shallow feature extraction module, (ii) a deep feature extraction and aggregation (FEA) module consisting of \(N\) Swin Transformer layers (\(_{1}\),\(\), \(_{N}\)), and (iii) a tail reconstruction module. Previous transformers (i.e., SwinIR , NGSwin ) synchronously perform multiple layer-specific deep image feature extraction (FE) and projection space (PS) optimization within a Swin Transformer Layer (STL). In contrast, we develop a decoupled Swin Transformer Layer (D-STL) in UPS to optimize per-layer feature extraction and a unified projection space (“PS\({}_{u}\)” defined by a learnable projection matrix \(U^{Q}\)).

## 4 Unified Projection Sharing for Lightweight SISR

**Overview.** To address the entanglement optimization of image feature extraction and similarity modeling, we introduce a Unified Projection Sharing (UPS) technique for lightweight SISR. Fig. 2(b), (c) summarizes the optimization schemes of existing Transformer-based SISR frameworks and our proposed UPS. As can be seen, previous methods typically focus on jointly optimizing deep image features and similarity modeling within each layer. In contrast, UPS adopts a shared projection space for similarity modeling, allowing layer-specific feature extraction while separating the optimization of similarity calculation.

### Unified Projection Sharing

We follow the general framework structure of Swin Transformer but use decoupled projection space optimization. As shown in Fig. 2(c), UPS consists of three basic modules, namely the convolutional head module, FEA module, and reconstruction tail module. In the FEA, we develop a decoupled Swin Transformer layer (D-STL) for deep image feature extraction, while optimizing a unified projection space for similarity modeling. Next, we will provide a detailed description of our D-STL.

### Decoupled STL (D-STL)

We take the \(i\)-th D-STL for illustration. Given an input image feature \(F_{i-1}\) produced by the last \((i-1)\)-th D-STL, we aim to perform feature updating as well as self-similarity-based aggregation. Similarly, we adopt the Eq. 1 to conduct deep image feature extraction and obtain the transformed image feature \(_{i}\). Then we employ the window-partitioning process to reshape the \(\) into \(}\) non-lapped image patches.

**Unified Projection.** Unlike the layer-specific projection scheme in Swin Transformers, we introduce a layer-invariant (unified) projection space defined by a learnable matrix \(U^{Q} R^{\{D C\}}\) (\(D\) refers to the unified projection dimension) and project the deep feature \(X_{i}\) on this unified projection space:

\[Q_{i}=X_{i}U^{Q}, V_{i}=X_{i}.\] (5)

After that, we consider the calculation of the self-similarity in the unified projection space. Motivated by ReLUFormer  that addresses the over-centralized distribution in Softmax by incorporating ReLU activation for self-similarity calculation, we get the similarity scores as:

\[S_{i}=((Q_{i},Q_{i}^{T})+B_{i}).\] (6)

Note that we conduct normalization operation for the projected image features \(Q_{i}\)6. Subsequently, we utilize the Cosine similarity metric, followed by a ReLU activation function, to obtain the final similarity map \(S_{i}\). We also assess our design in Sec. 5.3. Finally, leveraging the calculated similarity map \(S_{i}\), we perform image feature aggregation using Eq. 4.

**Discussion.** In Algorithm. 1, 2, we provide side-by-side illustrations of standard STL and our D-STL and highlight the differences between the two methods. The STL in previous Swin Transformers learns the coupled projection spaces and deep image feature extraction. In contrast, by a unified projection optimization scheme, each of our D-STLs only focuses on the deep image feature extraction. It largely

Figure 3: Comparison between SOTA SISR models and ours. We show the SR results overlaid with the local attribution map (LAM ) of each model. The LAM visually illustrates the activation of local and non-local pixels involved in super-resolving the highlighted patch within the red box. The numbers beneath are the DI (\(\))  and PSNR (\(\)) values. Zoom in for better visual comparison.

[MISSING_PAGE_FAIL:6]

**Scalable Model Size.** Generally, we train our UPS and UPS-S with different configurations. Our UPS model follows the setting of SwinIR-light , consisting of 4 D-RSTB blocks with 6 decoupled Swin Transformer layers (channel size: 60). Additionally, our UPS-S model is more lightweight with 4 compact D-RSTB blocks with varying numbers of decoupled Swin Transformer layers (6, 4, 4, 5) and a channel size of 48. Training various UPS models requires approximately 2-3 days.

**Benchmark Datasets.** Following previous studies [7; 28; 18], we utilize the DIV2K  image dataset for training. Subsequently, we conduct comprehensive evaluations on several widely-used SISR benchmarks, including Set5 , Set14 , BSD100 , Urban100 , and Manga109 . Our quantitative comparison is based on PSNR and SSIM. Consistent with established research, we report the results specifically for the Y channel derived from the YCbCr color space.

### Comparison with SOTA Methods

We perform extensive comparisons with a wide range of lightweight SISR models: MAFFSRN (ECCV20) , LAPAR-A (NeurIPS20) , LatticeNet (ECCV20) , RLFN (CVPRW22) , SwinIR-light , NGswin , SwinIR-NG , and DLGSA-I (ICCV23) . More comprehensive comparisons with early SOTA lightweight models can be accessed in our supplementary material.

**Quantitative Comparison.** Tab.1 illustrates the quantitative evaluation. Our proposed UPS consistently outperforms existing methods across all benchmarks. Notably, UPS exceeds the second-best model by over 0.33dB on the Manga109 dataset for the \( 2\) setting. Additionally, our model shows significant improvements over SwinIR-light , achieving improvements of up to 0.5dB, 0.55dB, and 0.47dB on the Manga109 dataset  for the \( 2\), \( 3\), and \( 4\) settings, respectively, while using fewer parameters. These results confirm the effectiveness of our decoupled optimization strategy. Significantly, when constrained by model complexities, our UPS demonstrates superior performance over SwinIR-light . As shown in Tab.1, our approach achieves a 0.55dB increase in PSNR for \( 2\) super-resolution on the Urban100 dataset, highlighting the advantages of our decoupled optimization in feature extraction and similarity modeling under compact parameter conditions.

Additionally, we evaluate the inference efficiency of various state-of-the-art (SOTA) lightweight single image super-resolution (SISR) models. As shown in Table 2, UPS reduces the overall inference cost by 33\(\%\) in terms of FLOPs compared to our baseline model, SwinIR-light

Last, we have extensively explored the benefits of UPS for real-world SR and other frameworks, including HAT and DRCT, under both lightweight and parameter-intensive scenarios. Our experiments show that the proposed UPS consistently enhances efficiency and performance across all these settings (real-world SR, lightweight, and SISR tasks).

For real-world super-resolution (Real-world SR), as shown in Tab. 3 of the PDF file (also the table below), our proposed UPS-GAN outperforms other state-of-the-art GAN-based [44; 45] and even Diffusion-based methods (Reshift  and StableSR ) in terms of NIQE, NRQM, and PI metrics, achieving the best quantitative results (5.09/6.84/4.19). This confirms the effectiveness of UPS for real-world SR tasks.

**Qualitative Comparison.** Fig. 4 presents some visual examples. It is evident that UPS is capable of producing correct image textures with fewer super-resolved artifacts. Conversely, previous CNN-based and Transformer-based frameworks either fail to reconstruct clear image patterns or suffer from unpleasing artifacts. We provide more visual comparison in the supplementary material.

   Metrics & RFDN-L & LatticeNet & DLGSA-light & Omni-SR & SwinIR-light & UPS \\  Time (ms) \(\) & **13** & **18** & 225 & 112 & 175 & 119 \\ FLOPs (G) \(\) & **146** & 170 & 170 & 195 & 244 & **163** \\ Memory (GB) \(\) & **1577** & **1639** & 1800 & 1842 & 2051 & 1785 \\   

Table 2: Results of inference time (ms), FLOPs (G) and GPU memory usage (MB). The speed is tested on an NVIDIA GeForce RTX 2080Ti GPU with an input size of 256 \(\) 256 under \(\)2 lightweight SISR. FLOPs is calculated at an output resolution of 1280 x 720.

### Ablation Studies

To verify the effectiveness of our design, we conduct a series of comprehensive experiments. Note that we primarily adopt the UPS-S architecture for fast analysis. We generally report the quantitative results on the Set14 benchmark under the \( 2\) setting.

**Different Groups of Projection Space.** Unlike conventional attention-based frameworks, our approach introduces a unified projection space for similarity calculation. In this evaluation, we investigate the impact of incorporating additional groups of projection space. The quantitative results in Fig.5 (Left) shows that a higher number of block-wise projection groups reduce performance compared to our unified projection method. Additionally, layer-specific projection optimization exhibits inferior performance. Fig.5 (Right) confirms that our unified projection-sharing scheme outperforms models with multiple projection spaces. This experiment highlights the challenges of coupled optimization in image feature extraction and similarity modeling.

**Similarity Calculation.** Differing from the conventional similarity calculation paradigm (Matrix dot product + SoftMax), we incorporate the Cosine distance followed by a ReLU activation for similarity computation. Here, to assess the effectiveness of our choice, we train different models that utilize various combinations of distance metrics (Matrix dot and Cosine) and activation functions (SoftMax and ReLU). The results, as displayed in the left Fig. 6, indicate that our design (Cosine + ReLU) achieves the best performance among all the competing strategies. Additionally, we provide a corresponding visual comparison in the right Fig. 6. It shows that our design activates more non-local pixels, resulting in a larger valid receptive field and more precise reconstructed image details.

**Projection Matrix Dimension.** We investigate the impact of various projection dimensions for similarity calculation, ranging from 8 to 256 (\(D=8,32,64,128,256\)). Tab. 4 indicates that performance initially improves as the projection dimension increases, but slightly drops after reaching extremely high dimensions (e.g., 256). We also observe that higher projection dimensions lead to increased computational costs on both learnable parameters and FLOPS.

   Dimension & 8 & 32 & 64 & 128 & 256 \\  PSNR (dB) & 33.83 & 33.86 & 33.90 & **34.00** & 33.96 \\ SSIM & 0.9206 & 0.9207 & 0.9207 & 0.9220 & **0.9221** \\ \#Params & 452K & 452K & 452K & 453K & 454K \\ FLOPS & 107G & 113G & 119G & 124G & 159G \\   

Table 4: The effect of varying projection dimensions on similarity calculation.

Figure 4: Qualitative comparison between LAPAR-A , SwinIR-light , NGSwin  and UPS (ours) on four popular benchmarks (BSD100 , Urban100 , Manga109  and DIV2K .) under \( 4\) setting. Our predictions present more detailed textures and fewer artifacts.

   Metrics & BSRGAN & RealSR & ResShift\(\) & StableSR\(\) & SwinIR-GAN & UPS-GAN \\  NIQE \(\) & 5.66 & 5.83 & 8.37 & **5.24** & 5.49 & **5.09** \\ NRQM \(\) & 6.27 & 6.32 & 4.56 & 6.12 & **6.48** & **6.84** \\ PI \(\) & 4.75 & **4.40** & 7.03 & 4.66 & 4.72 & **4.19** \\   

Table 3: Non-reference results of real-world SISR on RealSRSet .

**Similarity Map Visualization.** To better understand the effect of our projection-sharing scheme, we visualize the deep image features of our baseline model (SwinIR-light) and UPS. For an LR image, Fig. 7 illustrates the updating of deep image features at layers \(i=12,23\) in both the base and UPS models. The right arrows indicate the input-output data flow for each layer. Following that, we present a detailed visualization for Alg.1,2 in our paper7. Notably, the similarity maps \(S_{12},S_{23}\) produced by UPS (highlighted in the **red box**) are more effective in aggregating neighboring pixels, resulting in sharper final SR image.

### Extension 1: Image Denoising and JPEG Image Deblocking

In this section, we explore the benefits of applying UPS for other image restoration tasks. While SwinIR (baseline model) requires millions of parameters for image denoising and deblocking, our lightweight UPS frameworks handle these common low-level restoration tasks more efficiently. As shown in Fig. 8, the results, indicate that UPS achieves performance comparable to the large baseline model (SwinIR) while requiring only \(\) of the model complexity on Denoising. Additionally, the compact baseline models exhibit inferior performance relative to our UPS. We describe the framework of these models in Sec. A.6 of our appendix.

Figure 5: Analysis of several UPS-S models with different projection groups, including the layer-specific projection model (consists of 19 attention layers). (Left): PSNR/SSIMs are examined for quantitative comparison. (Right): Visual results on the Urban100  benchmark under x2 setting.

Figure 6: Impact of different similarity calculation methods. The left Table shows the quantitative results of employing different similarity calculation methods on the Urban100  (\( 2\)). The right figure gives a visual example to illustrate the SR results overlaid the LAM  maps of each model. The numbers beneath are the DI (\(\))  and PSNR (\(\)) values.

Figure 7: Visual comparison of layer-specific projection optimization and our proposed UPS scheme. UPS achieves better similarity calculation and yields better image structural restoration.

### Extension 2: Depth Map Super-resolution

We also compare it with our baseline model (SwinIR-light) on the depth SR task. To do this, **without** training on any depth images, we directly test the two models on the NYU V2  depth benchmark8 under \( 4\) and \( 16\) settings. We use the PSNR, SSIM, and RMSE (the root-mean-square error) metrics for quantitative evaluation. The quantitative results are shown in the left Fig. 9. We can see UPS consistently outperforms its baseline model on all the objective metrics. For instance, UPS exhibits superior performance compared to SwinIR-light with a PSNR improvement of **0.54dB** (**0.73dB**) for \( 4\) (\( 16\)) configurations. The visual examples in the right Fig. 9 illustrate that UPS generates clearer structures, leading to higher accuracy when compared to our baseline model.

In addition to these two extensions, we also explore the improvement over SwinIR, DRCT, HAT for both lightweight and classic SISR to comprehensive analyze the potential capabilities of the proposed UPS. Please refer to the appendix sections.

## 6 Conclusion

In this paper, we propose a unified projection sharing (UPS) technique for lightweight SISR. A layer-invariant projection space is optimized for similarity modeling. Comprehensive experiments have demonstrated the effectiveness of the proposed decoupled learning algorithm. Notably, UPS achieves state-of-the-art performance on multiple SISR benchmarks. Moreover, UPS-S exhibits competitive results compared with leading approaches, while requiring fewer learnable parameters. Additionally, experiments indicate that our proposed UPS demonstrates superior data efficiency. Code will be made publicly available at https://github.com/redrock303/UPS-NeurIPS2024.

**Acknowledgments.** This work is partially supported by Shenzhen Science and Technology Program KQTD20210811090149095 and also the Pearl River Talent Recruitment Program 2019QN01X226. The work was supported in part by the Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen-HK S&T Cooperation Zone, Guangdong Provincial Outstanding Youth Fund (No. 2023B1515020055), the National Key R&D Program of China with grant No. 2018YFB1800800, by Shenzhen Outstanding Talents Training Fund 202002, by Guangdong Research Projects No. 2017ZT07X152 and No. 2019CX01X104, by Key Area R&D Program of Guangdong Province (Grant No. 2018B030338001) by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), and by Shenzhen Key Laboratory of Big Data and Artificial

Figure 8: Extension on other image restoration problems. (Left) UPS attains comparable results compared with its larger baseline SwinIR and outperforms SwinIR-C with similar model sizes. (Right) A visual example of image deblocking.

Figure 9: Generalization comparison between our baseline model and UPS. The quantitative results on NYU V2  (\( 4\), \( 16\)) are displayed in the left table, while the right figure illustrates two visual examples. Additionally, normalized error maps are included in the left corner to facilitate comparison.

Intelligence (Grant No. ZDSYS201707251409055). It is also partly supported by NSFC-61931024, NSFC-62172348, and Shenzhen Science and Technology Program No. JCYJ20220530143604010.