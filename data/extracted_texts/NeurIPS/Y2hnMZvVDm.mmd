# Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time

Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time

Arvind Mahankali

Stanford University

amahanka@stanford.edu

&Jeff Z. HaoChen

Stanford University

jhaochen@stanford.edu

&Kefan Dong

Stanford University

kefandong@stanford.edu

&Margalit Glasgow

Stanford University

mglasgow@stanford.edu

&Tengyu Ma

Stanford University

tengyuma@stanford.edu

Equal ContributionEqual Contribution

###### Abstract

Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size \(n=O(d^{3.1})\) where \(d\) is the dimension of the inputs, the network trained with projected gradient flow converges in \((d)\) time to a non-trivial error that is not achievable by kernel methods using \(n d^{4}\) samples, hence demonstrating a clear separation between unmodified gradient descent and NTK. As a corollary, we show that projected gradient descent with a positive learning rate and a polynomial number of iterations converges to low error with the same sample complexity.

## 1 Introduction

Training neural networks requires optimizing non-convex losses, which is often practically feasible but still not theoretically understood. The lack of understanding of non-convex optimization limits the design of new principled optimizers for training neural networks that use theoretical insights.

Early analysis on optimizing neural networks with linear or quadratic activations [29; 36; 51; 35; 65; 38; 48; 39; 60] relies on linear algebraic tools that do not extend to nonlinear and non-quadratic activations. The neural tangent kernel (NTK) approach analyzes nonconvex optimization under certain hyperparameter settings, e.g., when the initialization scale is large and the learning rate is small (see, e.g., Du et al. , Jacot et al. , Li and Liang , Arora et al. , Daniely et al. ). However, subsequent research shows that neural networks trained with practical hyperparameter settings typically outperform their corresponding NTK kernels . Furthermore, the initialization and learning rate under the NTK regime does not yield optimal generalization guarantees [76; 22; 77; 34].

Many recent works study modified versions of stochastic gradient descent (SGD) and prove sample complexity and runtime guarantees beyond NTK [23; 2; 58; 52; 1; 7; 5; 79; 78; 25; 6; 67; 70; 20; 59]. These modified algorithms often contain multiple stages that optimize different blocks of parametersand/or use different learning rates or regularization strengths. For example, the work of Li et al.  uses a non-standard parameterization for two-layer neural networks and runs a two-stage algorithm with sharply changing gradient clipping strength; Damian et al.  use one step of gradient descent with a large learning rate and then optimize only the last layer of the neural net. Nichani et al.  construct non-standard regularizers based on the NTK feature covariance matrix, in order to prevent the movement of weights in certain directions which hinder generalization. Additionally, Abbe et al.  only train the hidden layer for \(O(1)\) time in the first stage of their algorithm, and then train the second layer to convergence in the second stage. They do not study how the population loss decreases during the first stage. However, oftentimes vanilla (stochastic) gradient descent with a constant learning rate empirically converges to a minimum with good generalization error. Thus, the modifications are arguably artifacts tailored to the analysis, and to some extent, over-using the modification may obscure the true power of gradient descent.

Another technique to analyze optimization dynamics is the mean-field approach , which views the collection of weight vectors as a (discrete) distribution over \(^{d}\) (where \(d\) is the input dimension) and approximates its evolution by an infinite-width neural network, where the weight vector distribution is continuous and evolves according to a partial differential equation. However, these works do not provide an end-to-end polynomial runtime bound on the convergence to a global minimum in the concrete setting of two-layer neural networks (without modifying gradient descent). For example, the works of Chizat and Bach  and Mei et al.  do not provide a concrete bound on the number of iterations needed for convergence. Mei et al.  provide a coupling between the trajectories of finite and infinite-width networks with exponential growth of coupling error but do not apply it to a concrete setting to obtain a global convergence result with sample complexity guarantees. (See more discussion below and in Section 3.) Wei et al.  achieve a polynomial number of iterations but require exponential width and an artificially added noise. In other words, these works, without modifying gradient descent, cannot prove convergence to a global minimizer with polynomial width and iterations. We also discuss additional related works in Appendix A.

In this paper, we provide a mean-field analysis of projected gradient flow on two-layer neural networks with _polynomial width_ and quartic activations. Under a simple data distribution, we demonstrate that the network converges to a non-trivial error in _polynomial time_ with _polynomial samples_. Notably, our results show a sample complexity that is superior to the NTK approach. As a corollary, we show that projected gradient descent with polynomially small step size and polynomially many iterations can converge to a low error, with sample complexity that is superior to NTK. Our proof is similar to the standard bound on the error of Euler's method.

Concretely, the neural network is assumed to have unit-norm weight vectors and no bias, and the second-layer weights are all \(\) where \(m\) is the width of the neural network. The data distribution is uniform over the sphere. The target function is of the form \(y(x)=h(q_{}^{}x)\) where \(h\) is an _unknown_ quartic link function and \(q_{}\) is an unknown unit vector. Our main result (Theorem 3.4) states that with \(n=O(d^{3.1})\) samples, a polynomial-width neural network with random initialization converges in polynomial time to a non-trivial error, which is statistically not achievable by any kernel method with an inner product kernel using \(n d^{4}\) samples. To the best of our knowledge, our result is the first to demonstrate the advantage of _ummodified_ gradient descent on neural networks over kernel methods.

The rank-one structure in the target function, also known as the single-index model, has been well-studied in the context of neural networks as a simplified case to demonstrate that neural networks can learn a latent feature direction \(q_{}\) better than kernel methods . Many works on single-index models study (stochastic) gradient descent in the setting where only a single vector (or "neuron") in \(^{d}\) is trained. This includes earlier works where the link function is monotonic, or the convergence is analyzed with quasi-convexity , along with more recent work  for more general link functions. Since these works only train a single neuron, they have limited expressivity in comparison to a neural network, and can only achieve zero loss when the link function equals the activation. We stress that in our setting, the link function \(h\) is unknown, not necessarily monotonic, and does not need to be equal to the activation function. We show that in this setting, the first layer weights will converge to a _distribution_ of neurons that are correlated with but not exactly equal to \(q_{}\), so that even without bias terms, their mixture can represent the link function \(h\). Our analysis demonstrates that gradient flow, and gradient descent with a consistent inverse-polynomial learning rate, can _simultaneously_ learn the feature \(q_{}\), and the link function \(h\), which is a key challenge that is side-stepped in previous works on neural-networks which use two-stage algorithms .

The main novelty of our population dynamics analysis is designing a potential function that shows that the iterate stays away from the saddle points.

Our sample complexity results leverage a coupling between the dynamics on the empirical loss for a finite-width neural network and the dynamics on the population loss for an infinite-width neural network. The main challenge stems from the fact that some exponential coupling error growth is inevitable over a certain period of time when the dynamics resemble a power method update. Heavily inspired by Li et al. , we address this challenge by using a direct and sharp comparison between the growth of the coupling error and the growth of the signal. In contrast, a simple exponential growth bound on the coupling error similar to the bound of Mei and Montanari  would result in a \(d^{O(1)}\) sample complexity, which is not sufficient to outperform NTK.

## 2 Preliminaries and Notations

We use \(O(),,\) to hide only absolute constants. Formally, every occurrence of \(O(x)\) in this paper can be simultaneously replaced by a function \(f(x)\) where \(|f(x)| C|x|, x\) for some universal constant \(C>0\) (each occurrence can have a different universal constant \(C\) and \(f\)). We use \(a b\) as a shorthand for \(a O(b)\). Similarly, \((x)\) is a placeholder for some \(g(x)\) where \(|g(x)||x|/C, x\) for some universal constant \(C>0\). We use \(a b\) as a shorthand for \(a(b)\) and \(a b\) as a shorthand to indicate that \(a b\) and \(a b\) simultaneously hold.

**Legendre Polynomials.** We summarize the necessary facts about Legendre polynomials below, and present related background more comprehensively in Appendix C. Let \(P_{k,d}:[-1,1]\) be the degree-\(k\)_un-normalized_ Legendre polynomial , and \(_{k,d}(t)=}P_{k,d}(t)\) be the _normalized_ Legendre polynomial, where \(N_{k,d}-\) is the normalizing factor. The polynomials \(_{k,d}(t)\) form an orthonormal basis for the set of square-integrable functions over \([-1,1]\) with respect to the measure \(_{d}(t)(1-t^{2})^{}}\), i.e., the density of \(u_{1}\) when \(u=(u_{1},,u_{d})\) is uniformly drawn from sphere \(^{d-1}\). Hence, for every function \(h:[-1,1]\) such that \(_{t_{d}}[h(t)^{2}]<\), we can define \(_{k,d}_{t_{d}}[h(t)_{k,d}(t)]\) and consequently, we have \(h(t)=_{k=0}^{}_{k,d}_{k,d}(t)\).

## 3 Main Results

We will formally define the data distribution, neural networks, projected gradient flow, and assumptions on the problem-dependent quantities and then state our main theorems.

_Target function._ The ground-truth function \(y(x):^{d}\) that we aim to learn has the form \(y(x)=h(q_{}^{}x)\), where \(h:\) is an _unknown_ one-dimensional _even_ quartic polynomial (which is called a link function), and \(q_{}\) is an _unknown_ unit vector in \(^{d}\). Note that \(h(s)\) has the Legendre expansion \(h(s)=_{0,d}+_{2,d}_{2,d}(s)+_{4,d}_{4,d}(s)\).

_Two-layer neural networks._ We consider a two-layer neural network where the first-layer weights are all unit vectors and the second-layer weights are fixed and all the same. Let \(()\) be the activation function, which can be different from \(h()\). Using the mean-field formulation, we describe the neural network using the distribution of first-layer weight vectors, denoted by \(\):

\[f_{}(x)_{u}[(u^{}x)]\,.\] (3.1)

For example, when \(=(\{u_{1},,u_{m}\})\), is a discrete, uniform distribution supported on \(m\) vectors \(\{u_{1},,u_{m}\}\), then \(f_{}(x)=_{i=1}^{m}(u_{i}^{}x)\), i.e. \(f_{}\) corresponds to a finite-width neural network whose first-layer weights are \(u_{1},,u_{m}\). For a continuous distribution \(\), the function \(f_{}()\) can be viewed as an infinite-width neural network where the weight vectors are distributed according to \(\) (and can be viewed as taking the limit as \(m\) of the finite-width neural network). We assume that the weight vectors have unit norms, i.e. the support of \(\) is contained in \(^{d-1}\). The activation \(:\) is assumed to be a fourth-degree polynomial with Legendre expansion \((s)=_{k=0}^{4}_{k,d}_{k,d}(s)\).

The simplified neural network defined in Eq. (3.1), even with infinite width (corresponding to a continuous distribution \(\)), has a limited expressivity due to the lack of biases and trainable second-layer weights. We characterize the expressivity by the following lemma:

**Lemma 3.1** (Expressivity).: _Let \(_{2}=_{2,d}/_{2,d}\) and \(_{4}=_{4,d}/_{4,d}\). Suppose for some \(d\) and \(q_{}\), there exists a network \(\) such that \(f_{}(x)=h(q_{}^{}x)\) on \(^{d-1}\). Then we have \(_{0,d}=_{0,d}\), and \(0_{2}^{2}_{4}_{2} 1\). Moreover, if this condition holds with strict inequalities, then for sufficiently large \(d\), there exists a network \(\) such that \(f_{}(x)=h(q_{}^{}x)\) on \(^{d-1}\). (A more explicit version is stated in Appendix G.1.)_

Informally, an almost sufficient and necessary condition to have \(f_{}(x)=h(q_{}^{}x)\) for some \(\) is that there exists a random variable \(w\) supported on \(\) such that \([w^{2}]_{2}\) and \([w^{4}]_{4}\), which is equivalent to \(0_{2}^{2}_{4}_{2} 1\). In particular, assuming the existence of such a random variable \(w\), the perfectly-fit network \(\) that fits the target function has the form

\[q_{}^{}u}{{=}}w,u-q_{} ^{}u q_{}^{}uq_{}\.\] (3.2)

Motivated by this lemma, we will assume that \(_{2},_{4}\) are universal constants that satisfy \(0_{2}^{2}_{4}_{2} 1\), and \(d\) is chosen to be sufficiently large (depending on the choice of \(_{2}\) and \(_{4}\)). In addition, we also assume that \(_{4} O(_{2}^{2})\), that is, the equality \(_{2}^{2}_{4}\) is somewhat tight -- this ensures that the distribution of \(w=q_{}^{}u\) under the perfectly-fitted neural network is not too spread-out. We also assume that \(_{2}\) is smaller than a sufficiently small universal constant. This ensures that the distribution of \(q_{}^{}u\) under the perfectly-fitted network does not concentrate on 1, i.e. the distribution of \(u\) is not merely a point mass around \(q_{}\). In other words, this assumption restricts our setting to the most interesting case where the landscape has bad saddle points (and thus is fundamentally more challenging to analyze). We also assume for simplicity that \(_{0,d}=_{0,d}\) (because otherwise, the activation introduces a constant bias that prohibits perfect fitting), even though adding a trainable scalar to the neural network formulation can remove the assumption. If \(_{0,d}=_{0,d}\), then we can assume without loss of generality that \(_{0,d}=_{0,d}=0\), since \(_{0,d}\) and \(_{0,d}\) will cancel with each other in the population and empirical mean-squared losses (defined below). In summary, we make the following formal assumptions on the Legendre coefficients of the link function and the activation function.

**Assumption 3.2**.: _Let \(_{2}=_{2,d}/_{2,d}\) and \(_{4}=_{4,d}/_{4,d}\). We first assume \(_{4} 1.1_{2}^{2}\). For any universal constant \(c_{1}>1\), we assume that \(_{2,d}^{2}/c_{1}_{4,d}^{2} c_{1}_{2,d}^{2}\), and \(_{4} c_{1}_{2}^{2}\). For a sufficiently small universal constant \(c_{2}>0\) (which is chosen after \(c_{1}\) is determined), we assume \(0_{2} c_{2}\). We also assume that \(d\) is larger than a sufficiently large constant \(c_{3}\) (which is chosen after \(c_{1}\) and \(c_{2}\).) We also assume \(_{0,d}=_{0,d}=0\), and \(_{1,d}=_{3,d}=0\)._

Our intention is to replicate the ReLU activation as well as possible with quartic polynomials; our assumption that \(_{2,d}^{2}_{2,d}^{2}\) is indeed satisfied by the quartic expansion of ReLU because \(_{2,d}} d^{-1/2}\) and \(_{2,d}} d^{-1/2}\) (see Proposition C.3). Following the convention defined in Section 2, we will simply write \(_{4,d}^{2}_{2,d}^{2}\), \(_{4} 1.1_{2}^{2}\), and \(_{4}_{2}^{2}\).

Our assumptions rule out the case that \(_{4}=_{2}^{2}\), for the sake of simplicity. We believe that our analysis could be extended to this case with some modifications. Our analysis also rules out the case where \(_{2}=0\) and \(_{4} 0\), due to the restriction that \(_{4} c_{1}_{2}^{2}\). The case \(_{2}=0\) and \(_{4} 0\) would have a significantly different analysis, and potentially a different sample complexity, since our analysis in Section 4.2 and Section 5 makes use of the fact that the initial phase of the population and empirical dynamics behaves similarly to a power method, which follows from \(_{2}\) being nonzero.

_Data distribution, losses, and projected gradient flow._ The population data distribution is assumed to be \(^{d-1}\). We draw \(n\) training examples \(x_{1},,x_{n}}}{{}}^{d-1}\). Thus, the population and empirical mean-squared losses are:

\[L()=*{}_{x^{d-1}}( f_{}(x)-y(x))^{2},()=_{i=1}^{n}(f_{}(x_{i})-y(x_{i}) )^{2}\,.\] (3.3)

To ensure that the weight vectors remain on \(^{d-1}\), we perform projected gradient flow on the empirical loss. We start by defining the gradient of the population loss \(L\) with respect to a particle \(u\) at \(\) and the corresponding Riemannian gradient (which is simply the projection of the gradient to the tangent space of \(^{d-1}\)):

\[_{u}L()=*{}_{x^{d-1}}[(f_{ }(x)-y(x))^{}(u^{}x)x]\,,_{u}L()=(I-uu^{})_{u}L()\,.\] (3.4)

Here we interpret \(L()\) as a function of a collection of particles (denoted by \(\)) and \(_{u}L()\) as the partial derivative with respect to a single particle \(u\) evaluated at \(\). Similarly, the (Riemannian) gradient of the empirical loss \(\) with respect to the particle \(u\) is defined as

\[_{u}()=_{i=1}^{n}(f_{}(x_{i})-y(x_{i} ))^{}(u^{}x_{i})x_{i}\,,_{u} ()=(I-uu^{})_{u}()\,.\] (3.5)

**Population, Infinite-Width Dynamics.** Let the initial distribution \(_{0}\) of the infinite-width neural network be the uniform distribution over \(^{d-1}\). We use \(\) to denote a particle sampled uniformly at random from the initial distribution \(_{0}\). A particle initialized at \(\) follows a deterministic trajectory afterwards -- we use \(u_{t}()\) to denote the location, at time \(t\), of the particle that was initialized at \(\). Because \(u_{t}()\) is a deterministic function, we can use \(\) to index the particles at any time based on their initialization. The projected gradient flow on an infinite-width neural network and using population loss \(L\) can be described as

\[^{d-1},u_{0}() =\,,\] (3.6) \[()}{dt} =-_{u_{t}}L(_{t})\,,\] (3.7) \[\ _{t} =\ u_{t}()\ \ _{0})\,.\] (3.8)

**Empirical, Finite-Width Dynamics.** The training dynamics of a neural network with width \(m\) can be described in this language by setting the initial distribution to be a discrete distribution uniformly supported on \(m\) initial weight vectors. The update rule will maintain that at any time, the distribution of neurons is uniformly supported over \(m\) items and thus still corresponds to a width-\(m\) neural network. Let \(_{1},,_{m}}{}^{d-1}\) be the initial weight vectors of the width-\(m\) neural network, and let \(_{0}=(\{_{1},,_{m}\})\) be the uniform distribution over these initial neurons. We use \(\{_{1},,_{m}\}\) to index neurons and denote a single initial neuron as \(_{0}()=\). Then, we can describe the projected gradient flow on the empirical loss \(\) with initialization \(\{_{1},,_{m}\}\) by:

\[_{t}()}{dt} =-_{_{t}()}(_{t})\,,\] (3.9) \[\ _{t} =\ _{t}()\ \ _{0})\,.\] (3.10)

We first state our result on the population, infinite-width dynamics.

**Theorem 3.3** (Population, infinite-width dynamics).: _Suppose Assumption 3.2 holds, and let \((0,1)\) be the target error. Let \(_{t}\) be the result of projected gradient flow on the population loss, initialized with the uniform distribution on \(^{d-1}\), as defined in Eq. (3.7). Let \(T_{*,}=\{t>0\ |\ L(_{t})(_{2,d}^{2}+ _{4,d}^{2})^{2}\}\) be the earliest time \(t\) such that a loss of at most \((_{2,d}^{2}+_{4,d}^{2})^{2}\) is reached. Then, we have_

\[T_{*,}_{2,d}^{2}_{2}} d+}{_{2,d}^{2}_{2}^{8}}( }{}).\] (3.11)

The proof is given in Appendix D. The first term on the right-hand side of Eq. (3.11) corresponds to the burn-in time for the network to reach a region around where the Polyak-Lojasiewicz condition holds. We divide our analysis of this burn-in phase into two phases, Phase 1 and Phase 2, and we obtain tight control on the factor by which the signal component, \(q_{*}^{}\,u\), grows during Phase 1, while Phase 2 takes place for a comparatively short period of time. (This tight control is critical for our sample complexity bounds where we must show that the coupling error does not blow up too much -- see more discussion below Lemma 5.4.) Phases 1 and 2 are mostly governed by the quadratic components in the activation and target functions, and the dynamics behave similarly to a power method update. After the burn-in phase, the dynamics operate for a short period of time (Phase 3) in a regime where the Polyak-Lojasiewicz condition holds. We explicitly prove the dynamics stay away from saddle points during this phase, as further discussed in Section 4.2.

We note that Theorem 3.3 provides a concrete polynomial runtime bound for projected gradient flow which is not achievable by prior mean-field analyses  using Wasserstein gradient flow techniques. For instance, while the population dynamics of Abbe et al.  only trains the hidden layer for \(O(1)\) time, our projected gradient flow updates the hidden layer for \(O( d)\) time. The main challenge in the proof is to deal with the saddle points that are not strict-saddle  in the loss landscape which cannot be escaped simply by adding noise in the parameter space .3 Our analysis develops a fine-grained analysis of the dynamics that shows the iterates stay away from saddle points, which allows us to obtain the running time bound in Theorem 3.3 which can be translated to a polynomial-width guarantee in Theorem 3.4. In contrast, Wei et al.  escape the saddles by randomly replacing an exponentially small fraction of neurons, which makes the network require exponential width.

Next, we state the main theorem on projected gradient flow on empirical, finite-width dynamics.

**Theorem 3.4** (Empirical, finite-width dynamics).: _Suppose Assumption 3.2 holds. Suppose \(=\) is the target error and \(T_{*,}\) is the running time defined in Theorem 3.3. Suppose \(n d^{}( d)^{(1)}\) for any constant \(>3\), and the network width \(m\) satisfies \(m d^{2.5+/2}( d)^{(1)}\), and \(m d^{C}\) for some sufficiently large universal constant \(C\). Let \(_{t}\) be the projected gradient flow on the empirical loss, initialized with \(m\) uniformly sampled weights, defined in Eq. (3.10). Then, with probability at least \(1-}\) over the randomness of the data and the initialization of the finite-width network, we have that \(L(_{T_{*,}})(_{2,d}^{2}+_{4, d}^{2})^{2}\)._

Plugging in \(=3.1\), Theorem 3.4 suggests that, when the network width is at least \(d^{4.05}\) (up to logarithmic factors), the empirical dynamics of gradient descent could achieve \(_{}^{2}(1/ d)^{2}\) population loss with \(d^{3.1}\) samples (up to logarithmic factors).

In our analysis, we will establish a coupling between neurons in the empirical dynamics and neurons in the population dynamics. The main challenge is to bound the coupling error during Phase 1 where the population dynamics are similar to the power method. During this phase, we show that the coupling error (i.e., the distance between coupled neurons) remains small by showing that it can be upper bounded in terms of the growth of the signal \(q_{}^{T}u\) in the population dynamics. Such a delicate relationship between the growth of the error and that of the signal is the main challenge to proving a sample complexity bound better than NTK. Even an additional constant factor in the growth rate of the coupling error would lead to an additional \((d)\) factor in the sample complexity. Prior work (e.g. Mei et al. , Abbe et al. ) also establishes a coupling between the population and empirical dynamics. However, these works obtain bounds on the coupling error which are exponential in the running time -- specifically, their bounds have a factor of \(e^{KT}\) where \(K\) is a universal constant and \(T\) is the time. In many settings, including ours, \(( d)\) time is necessary to achieve a non-trivial error, (as we further discuss in Section 4.2) and thus this would lead to a \(d^{O(1)}\) bound on the sample complexity, where \(O(1)\) is a unspecified and likely loose constant, which cannot outperform NTK. Our work addresses this challenge by comparing the growth of the coupling error with the growth of the signal, which enables us to control the constant factor in the growth rate of the coupling error.

**Projected Gradient Descent with \(1/(d)\) Step Size.** As a corollary of Theorem 3.4, we show that projected gradient descent with a small learning rate can also achieve low population loss in \((d)\) iterations. We first define the dynamics of projected gradient descent as follows. As before, we let \(_{1},,_{m}}{}^{d-1}\) be the initial weight vectors, and we let \(=(\{_{1},,_{m}\})\) denote the uniform distribution over these vectors. Thus, we can write a single neuron at initialization as \(_{0}()=\) for \(\{_{1},,_{m}\}\). The dynamics of projected gradient descent, on a finite-width neural network and using the empirical loss \(\), can be then described as

\[_{t+1}() =_{t}()-_{_{t}( )}(_{t})}{\|_{t}()- _{_{t}()}(_{t})\|_{2}}\,,\] (3.12) \[_{t} =_{t}()_{0})\,.\] (3.13)

where \(>0\) is the learning rate.4 We show that projected gradient descent with a \(1/(d)\) learning rate can achieve a low population loss in \((d)\) iterations:

**Theorem 3.5** (Projected Gradient Descent).: _Suppose we are in the setting of Theorem 3.4. Let \(_{t}\) be the discrete-time projected gradient descent on the empirical loss, initialized with \(m\) weight vectors sampled uniformly from \(^{d-1}\), defined in Eq. (3.13). Finally, assume that \(_{2,d}^{2}+_{4,d}^{2})d^{2}}\) for a sufficiently large universal constant \(B\). Then, \(L(_{T_{*,}/})(_{2,d}^{2}+_{4,d}^{2})^{2}\)._

Theorem 3.5 follows from Theorem 3.4 together with a standard inductive argument to bound the discretization error. The full proof is in Appendix F.

The following theorem states that in the setting of Theorem 3.4, kernel methods with any inner product kernel require \((d^{4}( d)^{-6})\) samples to achieve a non-trivial population loss. (Note that the zero function has loss \(_{x^{d-1}}[y(x)^{2}](_{4,d})^{2}\).)

**Theorem 3.6** (Sample complexity lower bound for kernel methods).: _Let \(K\) be an inner product kernel. Suppose \(d\) is larger than a universal constant and \(n d^{4}( d)^{-6}\). Then, with probability at least \(1/2\) over the randomness of \(n\) i.i.d data points \(\{x_{i}\}_{i=1}^{n}\) drawn from \(^{d-1}\), any estimator of \(y\) of the form \(f(x)=_{i=1}^{n}_{i}K(x_{i},x)\) of \(\{x_{i}\}_{i=1}^{n}\) must have a large error: \(_{x^{d-1}}(y(x)-f(x))^{2}(_{4,d} )^{2}\)._

Theorem 3.4 and Theorem 3.6 together prove a clear sample complexity separation between gradient flow and NTK. When \(_{4,d}_{}\) (i.e., \(_{2},_{4} 1\)), gradient flow with finite-width neural networks can achieve \((_{4,d})^{2}( d)^{-2}\) population error with \(d^{3.1}\) samples, while kernel methods with any inner product kernel (including NTK) must have an error at least \((_{4,d})^{2}/2\) with \(d^{3.9}\) samples.

On a high level, Abbe et al. [2; 3] prove similar lower bounds in a different setting where the target function is drawn randomly and the data points can be arbitrary (also see Kamath et al. , Hsu et al. , Hsu ). In comparison, our lower bound works for a fixed target function by exploiting the randomness of the data points. In fact, we can strengthen Theorem 3.6 by proving a \((_{k,d}^{2})\) loss lower bound for any universal constant \(k 0\) (Theorem H.2). We defer the proof of Theorem 3.6 to Appendix H.

## 4 Analysis of Population Dynamics

### Symmetry of the Population Dynamics

A key observation is that due to the symmetry in the data, the population dynamics \(_{t}\) has a symmetric distribution in the subspace orthogonal to the vector \(q_{}\). As a result, the dynamics \(_{t}\) can be precisely characterized by the dynamics in the direction of \(q_{}\).

Recall that \(w=q_{}^{}u\) denotes the projection of a weight vector \(u\) in the direction of \(q_{}\). Let \(z=(I-q_{}}^{})u\) be the remaining component. For notational convenience, without loss of generality, we can assume that \(q_{}=}\) and write \(u=(w,z)\), where \(w[-1,1]\) and \(z}^{d-2}\). _We will use this convention throughout the rest of the paper_. We will use \(w_{t}()\) to refer to the first coordinate of the particle \(u_{t}()\), and \(z_{t}()\) to refer to the last \((d-1)\) coordinates.

**Definition 4.1** (Rotational invariance and symmetry).: _We say a neural network \(\) is rotationally invariant if for \(u=(w,z)\), the distribution of \(z w\) is uniform over \(}^{d-2}\) almost surely. We also say \(\) is symmetric w.r.t a variable \(w\) if the density of \(w\) is an even function._

We note that any polynomial-width neural network (e.g., \(\)) is very far from rotationally invariant, and therefore the definition is specifically used for population dynamics. If \(\) is rotationally invariant and symmetric, then \(L()\) has a simpler form that only depends on the marginal distribution of \(w\).

**Lemma 4.2**.: _Let \(\) be a rotationally invariant neural network. Then, for any target function \(h\) and any activation \(\),_

\[L()=_{k=0}^{}(_{k,d}\,_{u }[P_{k,d}(w)]-_{k,d})^{2}.\] (4.1)

_In addition, suppose \(h\) and \(\) satisfy Assumption 3.2 and \(\) is symmetric. Then_

\[L()=_{2,d}^{2}}{2}\,_{u}[P_{2,d}(w)]-_{2}^{2}+_{2,d}^{2}}{2}\, _{u}[P_{4,d}(w)]-_{4}^{2}.\] (4.2)The proof of Lemma 4.2 is deferred to Appendix D.1. Eq. (4.1) says that if \(\) is rotationally invariant, then \(L()\) only depends on the marginal distribution of \(w\). Eq. (4.2) says that if the distribution of \(w\) is additionally symmetric and \(\) and \(h\) are quartic, then the terms corresponding to odd \(k\) and the higher order terms for \(k>4\) in Eq. (4.1) vanish. Note that \(P_{2,d}(s) s^{2}\) and \(P_{4,d}(s) s^{4}\) by Eq. (C.4). Thus, \(L()\) essentially corresponds to matching the second and fourth moments of \(w\) to some desired values \(_{2}\) and \(_{4}\). Inspired by this lemma, we define the following key quantities: for any time \(t 0\), we define \(D_{2,t}=_{u_{t}}[P_{2,d}(w)]-_{2}\) and \(D_{4,t}=_{u_{t}}[P_{4,d}(w)]-_{4}\), where \(_{t}\) is defined according to the population, infinite-width dynamics (Eq. (3.6), Eq. (3.7) and Eq. (3.8)).

We next show that the rotational invariance and symmetry properties of \(_{t}\) are indeed maintained:

**Lemma 4.3**.: _Suppose we are in the setting of Theorem 3.3. At any time \(t[0,)\), \(_{t}\) is symmetric and rotationally invariant._

Rotational invariance follows from the rotational invariance of the data. To show symmetry, we use Eq. (4.1), and the facts that \(P_{k,d}\) is an odd polynomial for odd \(k\) and \(_{t}\) is symmetric at initialization. Using Lemma 4.2 and Lemma 4.3, we obtain a simple formula for the dynamics of \(w_{t}\):

**Lemma 4.4** (1-dimensional dynamics).: _Suppose we are in the setting of Theorem 3.3. Then, for any \(^{d-1}\), writing \(w_{t}:=w_{t}()\), we have_

\[}{dt}=^{2})(P_{t}(w_{t})+Q_{t}(w_{t}))}_ { v(w_{t})},\] (4.3)

_where for any \(w[-1,1]\), we have \(P_{t}(w)=2_{2,d}^{2}D_{2,t}w+4_{4,d}^{2}D_{4,t}w^{3}\), and \(Q_{t}(w)={_{d}}^{(1)}w+{_{d}}^{(3)}w^{3}\), where \(|{_{d}}^{(1)}|,|{_{d}}^{(3)}|^{2}|D_{2,t}|+_{4,d}^{2}|D_{4,t}|}{}\). More specifically, \({_{d}}^{(1)}=2_{2,d}^{2}D_{2,t}-2_{4,d}^ {2}D_{4,t}-1}\) and \({_{d}}^{(3)}=4_{4,d}^{2}D_{4,t}-1}\)._

Eq. (4.3) is a properly defined dynamics for \(w\) because the update rule for \(w_{t}\) only depends on \(w_{t}\) and the quantities \(D_{2,t}\) and \(D_{4,t}\) -- additionally, \(D_{2,t}\) and \(D_{4,t}\) only depend on the distribution of \(w\). We henceforth refer to the \(w_{t}()\) as particles -- this is well-defined by Lemma 4.4.

### Analysis of One-Dimensional Population Dynamics

We also use \([-1,1]\) to refer to the first coordinate of \(\), the initialization of a particle under the population dynamics. We note that \(=,e_{1}\) and therefore, the distribution of \(\) is \(_{d}\). For any time \(t 0\), we use \(w_{t}()\) to refer to \(w_{t}()\) for any \(^{d-1}\). This notation is also well-defined by Lemma 4.4. We divide our proof of Theorem 3.3 into three phases, which are defined as follows. For ease of presentation, we will only consider particles \(w_{t}()\) for \(>0\) in the following discussion. Our argument also applies for \(<0\) by the symmetry of \(_{t}\) (Lemma 4.3).

**Definition 4.5** (Phase 1).: _Let \(w_{}=\) and \(_{}=}\). Let \(T_{1}>0\) be the minimum time such that \(w_{T_{1}}(_{})=w_{}:=\). We refer to the time interval \([0,T_{1}]\) as Phase 1._

Note that essentially all of the particles are less than \(_{}=}\) at initialization by tail bounds for \(_{d}\). During Phase 1, the term corresponding to \(D_{2,t}\) in the velocity dominates, and all particles grow by a \(}{( d)^{2}}\) factor. During Phase 1, the loss does not decrease much, but a large portion of the particles have grown by a large factor. During Phase 2, the particles and their velocity will become large.

**Definition 4.6** (Phase 2).: _Let \(T_{2}>0\) be the minimum time such that either \(D_{2,T_{2}}=0\) or \(D_{4,T_{2}}=0\). We refer to the time interval \([T_{1},T_{2}]\) as Phase 2. Note that \(T_{2}>T_{1}\) by Lemma D.8._

**Definition 4.7** (Phase 3).: _Phase 3 is defined as the time interval \([T_{2},)\)._

We divide this phase into two cases, which are both more challenging to analyze than Phases 1 and 2: (i) \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\), and (ii) \(D_{2,T_{2}}<0\) and \(D_{4,T_{2}}=0\). Here we discuss Case 1 -- the analyses of Cases 1 and 2 are in Appendix D.5 and Appendix D.6 respectively. Suppose Case 1 holds, i.e. \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\). Then, for all \(t T_{2}\), we will have \(D_{2,t} 0\) and \(D_{4,t} 0\) (Lemma D.19 and Lemma D.20), meaning that for \(t T_{2}\) a \((_{2})\) fraction of particles will be far from \(0\) and \(1\) (Lemma D.17). However, this does not guarantee a large average velocity -- unlike in Phases 1 and 2, the velocity \(v(w)=P_{t}(w)+Q_{t}(w)\) defined in Lemma 4.4 may have a positive root \(r(0,1)\), and the root \(r\) can give rise to bad stationary points because for certain values of \(r\), if \(\) degenerates to the singleton distribution on the root \(r\), the velocity at \(r\) would be exactly \(0\). Indeed, we can construct such an \(r\), intuitively because \(D_{2,t}\) and \(D_{4,t}\) have different signs in \(P_{t}(w)\) and \(Q_{t}(w) O(1/d)\) is a lower order term (Lemma D.35).

Thus, we must leverage some information about the trajectory to show that the average velocity is large when \(D_{2,t},D_{4,t}\) have different signs. To this end, we prove that \(_{u_{t}}[(w-r)^{2}]_{w,w^{}_ {t}}(w-w^{})^{2}\) is large by designing a novel potential function \((w):=(}})\). We show that \(|(w)-(w^{})|\) is always increasing for any two particles \(w,w^{}\) (Lemma D.13). Because \(\) is Lipschitz on an interval bounded away from \(0\) and \(1\), a lower bound on \(|(w)-(w^{})|\) also leads to a lower bound on \((w-w^{})^{2}\), and thus the particles away from \(0\) and \(1\) will have a large variance. Recall that when \(D_{2,t}>0\) and \(D_{4,t}<0\), a large portion of particles are away from \(0\) and \(1\), and hence, the average velocity is large. In Appendix B, we include simulations to illustrate the effects of Phases 1, 2 and 3.

## 5 Analysis of Empirical, Finite-Width Dynamics

**Coupling Between Empirical and Population Dynamics.** To analyze the difference between the empirical and population dynamics, we define an intermediate process \(_{t}\), where the initial particles are from \(_{0}\), but the dynamics of the particles then follow the population trajectories:

\[_{0} =_{0}=(\{_{1},,_{m}\})\,,\] \[_{t} =u_{t}()_{0})\,.\] (5.1)

For \((\{_{1},,_{m}\})\), let \(_{t}()=u_{t}()\). Let \(_{t}\) be the joint distribution of \((_{t}(),_{t}())\). Then, \(_{t}\) forms a natural coupling between \(_{t}\) and \(_{t}\). We will use \(_{t}\) and \(_{t}\) as shorthands for the random variables \(_{t}(),_{t}()\) respectively in the rest of this section. We define the average distance \(_{t}^{2}:=_{(_{t},_{t})_{t }}[\|_{t}-_{t}\|_{2}^{2}]\). Intuitively, \(f_{_{t}}(x)\) and \(f_{_{t}}(x)\) are close when \(_{t}\) and \(_{t}\) are close, which is formalized by the following lemma:

**Lemma 5.1**.: _In the setting of Theorem 3.4, let \(T}{_{2,d}^{2}+_{4,d}^{2}}\). Then, with probability at least \(1-(-d^{2})\) over the initialization, we have for all \(t[0,T]\) that \(_{x^{d-1}}[(f_{_{t}}(x)-f_{_{t}}(x))^{2}] _{2,d}^{2}+_{4,d}^{2})d^{2}( d)^{O (1)}}{m}\) and \(_{x^{d-1}}[(f_{_{t}}(x)-f_{_{t}}(x ))^{2}](_{2,d}^{2}+_{4,d}^{2})_{t}^{2}\)._

The proof of Lemma 5.1 is deferred to Appendix E. As a simple corollary of Lemma 5.1, we can upper bound \(_{x^{d-1}}[(f_{_{t}}(x)-f_{_{t}}(x))^{2}]\) by the triangle inequality. Thus, so long as \(_{T_{*,}}\) is small, we can show that the empirical and population dynamics achieve similar test error at time \(T_{*,}\).

**Upper Bound for \(_{T_{*,}}\).** The following lemma gives our upper bound on \(_{T_{*,}}\):

**Lemma 5.2** (Final Bound on \(_{T_{*,}}\)).: _In the setting of Theorem 3.4, we have \(_{T_{*,}} d^{-}\)._

We prove Lemma 5.2 by induction over the time \(t T_{*,}\). Let us define the maximal distance \(_{,\,t}:=_{(_{t},_{t})( _{t})}\|_{t}-_{t}\|_{2}\) where the max is over the _finite_ support of the coupling \(_{t}\). We will control \(_{t}\) and \(_{,\,t}\) simultaneously using induction. The inductive hypothesis is:

**Assumption 5.3** (Inductive Hypothesis).: _For some universal constant \(1>>1/2\) and \((0,-)\), we say that the inductive hypothesis holds at time \(T\) if, for all \(0 t T\), we have \(_{t} d^{-}\) and \(_{,\,t} d^{-}\)._

Showing that the inductive hypothesis holds for all \(t\) requires studying the growth of the error \(\|_{t}-_{t}\|\). We analyze it using the decomposition5\(\|_{t}-_{t}\|_{2}^{2}=A_{t}+B_{t}+C_{t}\,,\) where \(A_{t}:=-2(_{}L(_{t})-_{}L(_{t}), _{t}-_{t})\), \(B_{t}:=-2_{}L(_{t})-_{}L( _{t}),_{t}-_{t}\), and \(C_{t}:=-2_{}(_{t})-_{ }L(_{t}),_{t}-_{t}\). Intuitively, \(A_{t}\) captures the growth of the coupling error due to the population dynamics, \(B_{t}\) the growth due to the discrepancy between the gradients from the finite-width and infinite-width networks, and \(C_{t}\) the growth due to the discrepancy between finite samples and infinite samples. The following lemma gives upper bounds on these terms:

**Lemma 5.4** (Bounds on \(A_{t},B_{t},C_{t}\)).: _In the setting of Theorem 3.4, suppose the inductive hypothesis (Assumption 5.3) holds up to time \(t\), for some \(t T_{*,}\). Let \(_{t}:=_{t}-_{t}\). Let \(T_{1}\) be the runtime of Phase 1 (where Phase 1 is defined in Definition 4.5). Then, we have_

\[A_{t}4_{2,d}^{2}|D_{2,t}|\|_{t}\| ^{2}+O_{2,d}^{2}}{ d}|D_{2,t}|\|_{t} \|^{2}&0 t T_{1}\\ O(1)(_{2,d}^{2}+_{4,d}^{2})_{2}\| _{t}\|^{2}&T_{1} t T_{*,}.\] (5.2)

_Additionally, with probability at least \(1-1\) over the initialization and dataset, we have \(B_{t}(_{2,d}^{2}+_{4,d}^{2}) {d( d)^{O(1)}}{}+_{t}\|_{t}\|_{2}\), \([B_{t}]_{2,d}^{2}+_{4,d}^{2} )d( d)^{O(1)}}{}_{t}+(_{2,d}^{2}+ _{4,d}^{2})_{t}^{3}\) and \(C_{t}(_{2,d}^{2}+_{4,d}^{2})( d)^{O(1)} }\|_{t}\|_{2}+}{n} \|_{t}\|_{2}^{2}+}{n}^{2}\|_{t}\|_{2}+}{n}^{2}\|_ {t}\|_{2}^{2}\)._

The proof is in Appendix E. We now give an explanation for each of these bounds. For \(A_{t}\), we establish two separate bounds, one which holds during Phase 1, and one which holds during Phases 2 and 3. Intuitively, the signal part in a neuron (i.e. \(w^{2}\)) grows at a rate \(4_{2,d}^{2}|D_{2,t}|\) during Phase 1 (which follows from Lemma 4.4) and in Eq. (5.2) we show that the growth rate of the coupling error due to the growth in the signal is also at most \(4_{2,d}^{2}|D_{2,t}|\). Intuitively, the growth rates match because the signal parts of the neurons follow similar dynamics to a power method during Phase 1. For example, in the worst case when \(_{t}\) is mostly in the direction of \(e_{1}\), the factor by which \(_{t}\) grows is the same as that of \(w\). More mathematically, this is due to the fact that the first-order term in Lemma 4.4 is the dominant term in \(v(w)-v()\) for any two particles \(w,\) (where \(v(w)\) is the one-dimensional velocity defined in the previous section). To get a sample complexity better than NTK, the precise constant factor \(4\) in the growth rate \(4_{2,d}^{2}|D_{2,t}|w\) is important -- a larger constant would lead to \(_{T_{1}}\) being larger by a poly\((d)\) factor, thus increasing the sample complexity by poly\((d)\).

The same bound for \(A_{t}\) no longer applies during Phases 2 and 3. This is because the dynamics of \(w\) are no longer similar to a power method, and the higher-order terms may make a larger contribution to \(v(w)-v()\) than to the growth of \(w\), or vice versa. Thus we use a looser bound \(O(1)(_{2,d}^{2}+_{4,d}^{2})_{2}\| _{t}\|^{2}\) in Eq. (5.2). However, since the remaining running time after Phase 1, \(T_{*,}-T_{1}\), is very short (only poly\(( d)\) -- this corresponds to the second term of the running time in Theorem 3.3), the total growth of the coupling error is at most \((( d))\), which is sub-polynomial and does not contribute to the sample complexity. Note that if the running time during Phases 2 and 3 is longer (e.g. \(O( d)\)) then the coupling error would grow by an additional \(d^{O(1)}\) factor during Phases 2 and 3, which would make our sample complexity worse than NTK.

Our upper bounds on \(B_{t}\) and \(C_{t}\) capture the growth of coupling error due to the finite width and samples. We establish a stronger bound for \([B_{t}]\) than for \(B_{t}\). In our proof, we use the bounds for \(B_{t}\) and \([B_{t}]\) to prove the inductive hypothesis for \(_{t}\) and \(_{,t}\) respectively. We prove the bound for \(C_{t}\) by expanding the error due to finite samples into second-order and fourth-order polynomials and applying concentration inequalities for higher moments (Lemma I.7).

All of these bounds together control the growth of \(\|_{t}-_{t}\|\) at time \(t\). Using Lemma 5.4 we can show that for some properly chosen \(\) and \(\), the inductive hypothesis in Assumption 5.3 holds until \(T_{*,}\) (see Lemma E.3 for the statement), which naturally leads to the bound in Lemma 5.2. The full proof can be found in Appendix E.

## 6 Conclusion

In this paper, we prove a clear sample complexity separation between vanilla gradient flow and kernel methods with any inner product kernel, including NTK. Our work leads to several directions for future research. The first question is to generalize our results to the ReLU activation. Another question is whether gradient descent can achieve less than \(d^{3}\) sample complexity in our setting. The work of Arous et al.  shows that if the population loss has information exponent \(2\), then a sample complexity of \(d\) (up to logarithmic factors) can be achieved with a single-neuron student network -- it is an open question if a similar sample complexity could be obtained in our setting. A final open question is whether two-layer neural networks can be shown to attain arbitrarily small generalization error \(\) -- one limitation of our analysis is that we require \((1/ d)\).