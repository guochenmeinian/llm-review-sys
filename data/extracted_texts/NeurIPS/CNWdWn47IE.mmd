# DataComp-LM: In search of the next generation of training sets for language models

Jeffrey Li\({}^{}\)\({}^{1,2}\) Alex Fang\({}^{}\)\({}^{1,2}\) Georgios Smyrnis\({}^{}\)\({}^{4}\) Maor Iyqi\({}^{}\)\({}^{5}\)

Matt Jordan\({}^{4}\) Samir Gadre\({}^{3,6}\) Hritik Bansal\({}^{8}\) Etash Guha\({}^{1,15}\) Sedrick Keh\({}^{3}\) Kushal Arora\({}^{3}\)

Saurabh Garg\({}^{13}\) Rui Xin\({}^{1}\) Niklas Muennighoff\({}^{22}\) Reinhard Heckel\({}^{12}\) Jean Mercat\({}^{3}\) Mayee Chen\({}^{7}\) Suchin Gururangan\({}^{1}\) Mitchell Wortsman\({}^{1}\) Alon Albalak\({}^{19,20}\) Yonatan Bitton\({}^{14}\)

Marianna Nezhurina\({}^{9,10}\) Amro Abbas\({}^{23}\) Cheng-Yu Hsieh\({}^{1}\) Dhruba Ghosh\({}^{1}\) Josh Gardner\({}^{1}\)

Maciej Kilian\({}^{17}\) Hanlin Zhang\({}^{18}\) Rulin Shao\({}^{1}\) Sarah Pratt\({}^{1}\) Sunny Sanyal\({}^{4}\) Gabriel Ilharco\({}^{1}\)

Giannis Daras\({}^{4}\) Kalyani Marathe\({}^{1}\) Aaron Gokaslan\({}^{16}\) Jieyu Zhang\({}^{1}\) Khyathi Chandu\({}^{11}\)

Thao Nguyen\({}^{1}\) Igor Vasiljevic\({}^{3}\) Sham Kakade\({}^{18}\) Shuran Song\({}^{6,7}\) Sujay Sanghavi\({}^{4}\) Fartash Faghri\({}^{2}\) Sewoong Oh\({}^{1}\) Luke Zettlemoyer\({}^{1}\) Kyle Lo\({}^{11}\) Alaaeldin El-Nouby\({}^{2}\) Hadi Pouransari\({}^{2}\)

Alexander Toshev\({}^{2}\) Stephanie Wang\({}^{1}\) Dirk Groeneveld\({}^{11}\) Luca Soldaini\({}^{11}\) Pang Wei Koh\({}^{1}\)

Jenia Jitesv\({}^{9,10}\) Thomas Kollar\({}^{3}\) Alexandros G. Dimakis\({}^{4,21}\)

Yair Carmon\({}^{5}\) Achal Dave\({}^{13}\) Ludwig Schmidt\({}^{1,1}\) Vaishaal Shankar\({}^{1}\)

###### Abstract

We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-baseline, enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with \(6.6\) less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the DCLM benchmark, framework, models, and datasets at [https://datacomp.ai/dclm](https://datacomp.ai/dclm).

## 1 Introduction

Large training datasets are an important driver of progress in the recent language modeling (LM) revolution . As the cost of training state-of-the-art language models continues to grow, researchers increasingly focus not only on _scaling_ but also on _improving_ training datasets that enable efficient generalization on a wide range of downstream tasks. Indeed, there is a growing number of proposals for filtering data, removing (near-) duplicates, finding new data sources, weighting data points, generating synthetic data, and so on .

A key challenge in this emerging research area is a lack of controlled comparisons. While the aforementioned proposals generally use the same evaluation datasets, researchers often compare models that are trained with different architectures, compute, or hyperparameters. Hence, it is often unclear what data curation strategies work best: Are the results of training set A better than training set B because training set A is truly better, or because the model trained on A was combined with a better architecture, learning rate schedule, or more compute? Disentangling the many factors influencing the quality of a language model is crucial to understanding which data curation strategies work best and ultimately building better language models.

Beyond the lack of standardized benchmarks, another challenge for research on training data is that details about training sets are becoming increasingly rare, even for open weight models such as the Llama, Mistral, or Gemma models . For all of these models, the training sets are not publicly available, and the corresponding model documentation only provides a coarse description of the respective training data, if any at all. As a result, it is currently unclear what ingredients constitute a state-of-the-art training set for langauge models.

To address these challenges, we introduce **DataComp for Language Models (DCLM)**, the first large-scale benchmark for language model training data curation. In DCLM, researchers propose new training sets and data curation algorithms and then evaluate their datasets by training LMs with a _fixed_ training recipe on their data. By measuring the performance of the resulting model on downstream tasks, researchers can quantify the strengths and weaknesses of the corresponding training set.

To enable DCLM, we contribute a comprehensive experimental testbed. A key component is **DCLM-Pool**, a corpus of 240 trillion tokens derived from Common Crawl . DCLM-Pool is the largest public corpus for language model training and forms the cornerstone of the DCLM filtering track,

Figure 1: **Improving training sets leads to better models that are cheaper to train. Using DataComp-LM, we develop a high-quality dataset, DCLM-baseline, which we use to train models with state-of-the-art trade-offs between compute and performance. We compare on both _(left)_ a Core set of tasks and on _(right)_ MMLU 5-shot. Specifically DCLM-baseline (orange) shows favorable performance relative to both close-source models (crosses) and other open-source datasets and models (circles). Models in this figure are from . Table 33 provides a table version of this figure.**where participants aim to curate the best possible training set out of DCLM-Pool. In addition, we provide open-source software for processing large datasets with several filtering approaches.

The high cost of training language models makes it necessary to understand the performance of training recipes across different compute and data scales. Hence, our third contribution is an investigation of **scaling trends for dataset design**. We find that models as small as 400M parameters can still provide signal on which training sets perform better at larger scales. Based on our experiments, we organize DCLM into five compute scales spanning a range of about \(600\) in compute from 400M parameter models to over-trained 7B models. This multi-scale design makes DCLM accessible to researchers with varying compute budgets.

As a starting point for DCLM, we conduct **416 baseline experiments** with different training sets and compute scales. Our experiments identify model-based filtering as a key component for effective data curation. We also show that details of the filtering model can have a large impact on performance, ranging from 35% to 44% accuracy on MMLU 5-shot  at the 7B parameter scale (280B training tokens). Interestingly, a simple bigram classifier, combined with a carefully selected set of positive and negative examples, performs best among the classifiers we experimented with. In addition, we find that human quality judgments have only limited value in identifying high-quality training data.

Finally, we combine our results into **DCLM-baseline, a new state-of-the-art public training set** for language models. When training a 7B parameter language model on 2.6 trillion tokens using DCLM-baseline, the resulting model achieves \(64\)% on MMLU, which is state-of-the-art among open-data models and close to models such as Mistral-7B-v0.3 (63%) or Llama 3 8B (66%) that are trained with up to \(6.6\) more compute (Llama 3 8B). Compared to Llama 2 7B, training a 7B parameter model on 280B tokens from DCLM-baseline achieves 5 pp higher MMLU while being trained with \(7\) less compute. As our 7B model uses a standard decoder-only Transformer , our results also highlight that a systematic approach to data curation is key to training performant language models.

We publicly release our DCLM framework, models, and training sets at [https://datacomp.ai/dclm](https://datacomp.ai/dclm) to enable other researchers to participate in DCLM and to strengthen the empirical foundations for data-centric research on language models.

## 2 Related work

We summarize closely related work in this section and provide additional related work in Appendix B.

Data curation for language models.To collect large datasets for training LMs , researchers typically resort to web crawls, which can contain undesirable content that can be improved via curation. Most data curation efforts focus on methods for improving model performance , including filtering by language , heuristic-based filtering , quality filtering , data deduplication  and mixing . While prior work examines a limited set of filters, we conduct the largest public investigation of data curation, resulting in a strong DCLM-baseline dataset.

Figure 2: **The DCLM workflow.**_(A)_ A participant first chooses a scale, where larger scales reflect more training tokens or model parameters. _(B)_ A participant then filters a pool of data (filtering track) or mixes data of their own (mixing track) to create a dataset. _(C)_ Using the curated dataset, a participant trains a language model, with standardized training code and scale-specific hyperparameters, which is then _(D)_ evaluated on 53 downstream tasks to judge dataset quality.

Open-source datasets.As the scale of LMs has increased over the past years , the community has curated larger datasets to match. Early works include the C4 dataset with 160 billion (B) tokens and The Pile  with 300B tokens. More recently, RefinedWeb  contains 600B tokens, Dolma  3 trillion (T) tokens, FineWeb 15T tokens , and RedPajama-v2 30T tokens . There are also large domain-specific datasets, such as the code-focused StackV2 with 900B tokens , as well as high-quality filtered subsets such as FineWeb-Edu  with 1.3T tokens. We include performance comparisons with various datasets in Figure 1 and examine FineWeb's LightEval evaluation framework more closely in Appendix G. We release the largest pool of raw text data to date with 240T web-crawled tokens. We also release DCLM-baseline, a high-quality dataset from our pool that yields better models than prior datasets.

Data-centric benchmarks.Past work on benchmarking data improvements includes dataset distillation , curriculum learning , and transfer learning . In DataComp  and DataPerf , participants iterate on a dataset with a fixed model and training recipe for vision, vision-language, and speech tasks. For LMs, the Data-Juicer  effort includes benchmarks for cleaning and mixing _fine-tuning_ data while the BabyLM challenge _Loose_ track  focuses on efficient development of 125M to 220M parameter LMs pretrained on 10M to 100M tokens. With a 200T token pool and 7B models, DCLM is the largest data-centric benchmark for language models.

## 3 The DataComp for language models (DCLM) benchmark

This section describes the main components of DCLM. We start with DCLM-Pool, the raw text corpus underlying our benchmark (Section 3.1). We then develop the DCLM workflow, visualized in Figure 2: selecting a competition scale (Section 3.2), curating a dataset by filtering DCLM-Pool and potentially mixing in other sources (Section 3.3), training a model with fixed hyperparameters (Section 3.4), and evaluating the model to score the dataset (Section 3.5).

### DCLM-Pool

DCLM-Pool is an unfiltered web-text corpus comprised of all Common Crawl  data prior to 2023. Based on Section 4.2, we re-extract text from HTML using resiliparse instead of using Common Crawl's pre-extracted text. DCLM-Pool contains 200B documents (370TB after zip compression), resulting in 240T GPT-NeoX  tokens. See Appendix E for additional details.

Decontamination.Test set samples often contaminate language model training sets ; however, the effect of such samples on downstream performance remains largely unclear . To allow researchers to better understand contamination, we release decontamination tooling instead of decontaminating DCLM-Pool directly. First, as used in Section 4.6, we implement our own decontamination process for two popular tasks, MMLU and Hellaswag . Second, we allow participants to examine their datasets for overlap with _all_ of our test sets based on Lee et al. . We ask all submissions to disclose a decontamination report and avoid using highly-contaminated data. For the highest scoring submissions, we plan to specifically evaluate them for contamination.

    &  &  &  &  &  \\  & parameters & & & & H100 hours & \\ 
400M-1x & 412M & 8.2B & 2.0e19 & 26 & 469B \\
1B-1x & 1.4B & 28.8B & 2.4e20 & 240 & 1.64T \\
3B-1x & 2.8B & 55.9B & 9.4e20 & 740 & 3.18T \\
7B-1x & 6.9B & 138B & 5.7e21 & 3,700 & 7.85T \\
7B-2x & 6.9B & 276B & 1.1e22 & 7,300 & 15.7T \\   

Table 1: **DCLM competition scales.** DCLM contains five competition scales, enabling research in varying compute regimes. Each scale specifies the model size (‘Model parameters’, \(N\)), the number of tokens seen during training (‘Train tokens’, \(D\)), and the size of the original pool that can be used for filtering (‘Pool size’). We provide an estimate of the compute required for training (‘Train FLOPs’\(=6ND\)) and GPU hours (‘Train H100 hours’) using the OpenLM training framework .

### Competition scales: Supporting participants with different compute constraints

To ensure DCLM is accessible to researchers with different compute constraints and to facilitate the study of scaling trends, we create different competition scales spanning three orders of compute magnitude (Table 1).Each scale (i.e., 400M-1x, 1B-1x, 3B-1x, 7B-1x, and 7B-2x) specifies the number of model parameters (e.g., 7B) and a _Chinchilla multiplier_ (e.g., 1x). The number of training tokens for each scale is \(20\) number of parameters \(\) Chinchilla multiplier so that a multiplier of 1x corresponds to a compute allocation that Hoffmann et al.  found near-optimal.

A potential pitfall in our multi-scale design is that the ranking of data curation methods may change when increasing the compute scale. To better understand this concern, in Figure 3, we plot the performance of 10 methods at the 7B-1x scale as a function of their performance at smaller scales. We find high rank correlation between the results for smaller scales (400M-1x, 1B-1x, 3B-1x) and those for the larger 7B-1x scale (Pearson's \(r=0.838\), \(r=0.956\), \(r=0.982\) respectively), suggesting better curation strategies at smaller scales transfer to larger scales. For more competition scale ablations, including experiments suggesting dataset improvements are largely orthogonal to training hyperparameters, see Appendix H.

### Benchmark tracks: Filtering and mixing

After choosing a scale, participants choose one of two tracks. (i) In the _filtering track_, participants propose algorithms to select training data from a candidate pool. We start with five pools, one for each scale in Table 1, which are random document subsets of DCLM-Pool. We restrict initial pool sizes by scale to encourage scalable filtering strategies and reflect realistic data download and storage constraints. (ii) In the _mixing track_, a submission may combine documents from potentially many sources. For instance, participants can synthesize documents from DCLM-Pool, a custom crawl, Stack Overflow, and Wikipedia. Appendix C provides detailed rules for each track, and Appendix D describes our extensible open-source tooling for executing filtering and mixing operations.

### Training

To isolate the effect of dataset interventions, we fix a training recipe at each scale. Based on prior ablations on model architectures and training [4; 31; 39; 62; 79; 93; 133; 167; 168; 180], we adopt a decoder-only Transformer (e.g., GPT-2, Llama) [133; 167; 171], implemented in OpenLM . We also provide unified data processing utilities. Appendix F contains additional training details.

### Evaluation

Our full evaluation suite, based on LLM-Foundry , contains 53 downstream tasks suitable for base model evaluation (i.e., without finetuning): from question answering to open-ended generation formats, considering varied domains like coding, text-book knowledge, and common-sense reasoning. To evaluate data curation algorithms, we focus on three main performance metrics. First, we consider _MMLU 5-shot accuracy_, which is widely used to compare state-of-the-art models like GPT-4  and Llama 3 70B . Second, we propose Core _centered accuracy_, computed over a subset of 22 tasks (e.g., HellaSwag  and ARC-E ) that provide a low-variance signal even at small scales, linearly rescaling the accuracy per task so that 0 corresponds to random guessing and 1 corresponds to perfect accuracy. Finally, we report Extended _centered accuracy_, which averages the centered performance for all of our 53 tasks. For more metric details, see Appendix G.

Figure 3: **Datasets rank consistently across competition scales in DCLM. This makes it possible to iterate on data curation at small scales.**

## 4 Building high-quality training datasets with DCLM

We now show how the DCLM workflow can lead to high-quality datasets and quantify the effect of data curation methods. This section describes the process of converting Common Crawl into our dataset, DCLM-baseline, as shown in Figure 4. We provide ablation experiments for each step along the way. We first evaluate open-source datasets as a starting point (Section 4.1). Next, we experiment with alternatives for several key phases of dataset construction: text extraction (Section 4.2), deduplication (Section 4.3), and model-based filtering (Section 4.4). We then experiment with mixing in high-quality sources (Section 4.5) and provide a contamination analysis (Section 4.6). In Section 5, we scale up this approach to train a 7B model for 2T tokens.

### Evaluating existing training datasets

We begin by evaluating several well-known open-source datasets (C4, RefinedWeb, RedPajama, and Dolma-V1) in Table 2. While all four datasets use various heuristic filters and data cleaning steps, we find that RefinedWeb performs the best on our Core and Extended metrics at the 7B-1x scale. RefinedWeb applies the following filtering pipeline: Common Crawl text extraction, heuristic selection rules (e.g., to remove spam), and deduplication of repeated content. Interestingly, RefinedWeb is solely filtered from Common Crawl, unlike RedPajama and Dolma-V1, which additionally mix in curated, "high-quality" sources like Wikipedia. The comparison suggests the relative strength of filtering, which we explore later in our experiments.

**Takeaway:** For DCLM-baseline and other experiments, we adopt RefinedWeb's heuristic filters.

### Text extraction

Text extraction is a crucial early processing step that pulls content from raw HTML. To understand the effect of this step, we compare three extraction approaches: resiliparse, trafilatura (used

   Dataset & Core & Extended \\  C4 & 34.2 & 18.0 \\ Dolma-V1 & 35.0 & 18.4 \\ RedPajama & 35.3 & 18.2 \\ RefinedWeb & **36.9** & **19.8** \\   

Table 2: **Comparison to existing datasets (7B-1x scale). Despite not mixing high-quality sources (unlike Dolma-V1 and RedPajama), RefinedWeb performs best.**

Figure 4: **Construction of DCLM-baseline from DCLM-Pool. Before this pipeline, we extracted DCLM-Pool from Common Crawl with resiliparse. Percentages are based on the total number of original documents.**by RefinedWeb), and the Common Crawl-provided WET files that contain pre-extracted text. We then apply RefinedWeb's heuristic quality filters to each of the text extractions. In Table 3, we find both resiliparse and trafilatura improve Core by at least 2.5 points over the WET extraction. This is significant because most open source datasets, including C4, RedPajama, and Dolma-V1, use WET files, which could partially explain their worse performance in Table 2. While resiliparse and trafilatura have similar downstream performance, resiliparse is \(8\) faster to run and hence more practical for large-scale processing. For more analysis, see Appendix K.

### Deduplication

Web-crawled datasets often contain many duplicate or near-duplicate data strings. Removing these duplicates serves the dual purpose of improving performance by reducing memorization [35; 94] and increasing data diversity. For deduplication, we explore MinHash , as part of a suffix array pipeline [94; 127], and near-duplicate Bloom filtering, which modifies an exact document and paragraph deduplication scheme . We find that both approaches provide comparable downstream performance: within 0.2 Core percentage points at the 7B-2x scale. However, our modified Bloom filter scales more easily to datasets surpassing 10TB. We provide additional analysis in Appendix L.

**Takeaway:** We use a Bloom filter for DCLM-baseline and MinHash for other experiments.

### Model-based quality filtering

Recent literature [59; 156; 28] indicates that using learnable models as quality filters leads to downstream improvements. In this section, we investigate model-based filtering.

Comparing model-based filtering approaches.We compare many strategies: 1) _PageRank score filtering_ to retain documents based on how likely they are to be linked to other documents, 2) _Semantic Deduplication (SemDedup)_ to remove documents with similar informational content , 3) _linear classifiers_ fit on pre-trained BGE text embeddings , 4) _AskLLM_ that prompts an LM to see if a

   Filter & Core & Extended \\  RefinedWeb reproduction & 27.5 & 14.6 \\  Top 20\% by Pagerank & 26.1 & 12.9 \\ SemDedup  & 27.1 & 13.8 \\ Classifier on BGE features  & 27.2 & 14.0 \\ AskLLM  & 28.6 & 14.3 \\ Perplexity filtering & 29.0 & 15.0 \\ Top-k average logits & 29.2 & 14.7 \\ fastText OH-2.5 +ELI5 & **30.2** & **15.4** \\   

Table 4: **Quality filtering comparison (1B-1x scale). We evaluate various choices for model-based quality filters. Training a fastText classifier for filtering performs best.**

   Text Extraction & Core & Extended \\  resiliparse & 24.1 & **13.4** \\ trafilatura & **24.5** & 12.5 \\ WET files & 20.7 & 12.2 \\   

Table 3: **Comparison of text extractors (1B-1x scale). We apply three approaches for text extraction from HTML, process their output using the RefinedWeb heuristic filters, and evaluate the models trained on the resulting datasets. We find stricter extractors such as resiliparse and trafilatura are superior to WET files provided by Common Crawl.**document is helpful , 5) _Perplexity filtering_ where we retain low perplexity sequences following CCNet , 6) _Top-k average logits_ where we average the top-\(k\) model logits over all words in a document to score how confident a model is that the correct words are within \(k\) reasonable choices, and 7) fastText binary classifiers to distinguish data quality. For training classifiers, we train on \( 400k\) documents split equally between positive and negative classes. We experiment with different options for positive data and fix negative data as a random sample from a version of our RefinedWeb reproduction. For the perplexity filtering and the top-k average logits strategies, we utilize a 154M parameter causal Transformer trained on a mix of English Wikipedia, the books subset of RedPajama-v1, and peS2o . We compare the aforementioned approaches in Table 4 and find that fastText-based filtering outperforms all other approaches. We next aim to understand how fastText training recipes affect its effectiveness as a data filtering network .

Text classifier ablations.To better understand the limits of fastText, we train several variants, exploring different choices for the reference data (i.e., the examples given positive labels), feature space, and filtering threshold, as shown in Table 5. For reference positive data, we considered commonly used sources like Wikipedia , OpenWebText, and RedPajama-books, following the reference data used for GPT-3 . We also try a novel approach, using instruction-formatted data, drawing examples from OpenHermes 2.5  (OH-2.5) and high-scoring posts from the r/ExplainLikeImFive (EL15) subreddit. Overall, we find, when controlling for other hyperparameters, the fastText OH-2.5 +EL15 approach gives a 3.5 percentage point lift on Core compared to the other more conventional choices. It is natural to ask whether using OH-2.5 data for filtering could preclude additional gains from instruction-tuning. In Appendix Q, we show this is not the case, further suggesting the strength and compatibility of this approach with modern finetuning paradigms. Finally, we observe that using a fairly strict threshold, which keeps the top-10% of examples, helps over more permissive top-15% and top-20% thresholds. We further study the unintuitive behavior of dataset filtering and its connection to human judgment in Appendix N.

### Dataset mixing

Often, Common Crawl (CC) is combined with other data sources that are considered high-quality  (e.g., Wikipedia, StackExchange, and peS2o ). Since DCLM participants can include additional data sources in our mixing track, we examined the potential benefits of adding high-quality sources to training sets derived from Common Crawl only. We compare a model trained on 100% filtered CC data to models trained with the mixing proportion from Llama 1 and RedPajama: 67% CC, and 33% from Wikipedia, Books, Stack exchange, arXiv, and Github. For the CC component, we consider different variants: a subset of our DCLM-baseline, RedPajama's CC portion, RefinedWeb, and C4. The results in Table 6 show that mixing improves performance for the lower-performing CC subsets (C4, RedPajama-CC, and RefinedWeb). In the case of DCLM-baseline however, mixing actually hurts performance on average, which suggests it can be counterproductive given performant filtering. For additional mixing results, see Appendix M.

   Dataset & Threshold & Core & MMLU & Extended \\  OH-2.5 + ELI5 & 10\% & **41.0** & **29.2** & 21.4 \\ Wikipedia & 10\% & 35.7 & 27.0 & 19.1 \\ OpenWebText2 & 10\% & 34.7 & 25.0 & 18.7 \\ GPT-3 Approx & 10\% & 37.5 & 24.4 & 20.0 \\  OH-2.5 + ELI5 & 15\% & 39.8 & 27.2 & **21.5** \\ OH-2.5 + ELI5 & 20\% & 38.7 & 24.2 & 20.3 \\   

Table 5: fastText **ablations** (7B-1x scale). We ablate choices for the positive data (top) and threshold (bottom). ‘Dataset’ is the positive set, while the negatives are randomly sampled our RefinedWeb reproduction. ‘Threshold’ is the percentile used for filtering based on fastText scores. ‘‘GPT-3 Approx” refers to a mix of Wikipedia, OpenWebText2, and RPJ Books, as in .

### Decontamination

Here, we examine whether contamination of our pretraining data with evaluation data influences our results for DCLM-baseline. We focus on MMLU and Hellaswag as our evaluation sets of choice, given their popularity as metrics for language model performance at the 7B scale.

As an experiment, we attempt to remove examples from these two sets that exist in DCLM-baseline. For both, our strategy is to flag training documents that contain the question text along with one of the corresponding answer options. For these flagged examples, we then remove all matched question and option strings. In order to improve recall for MMLU, which contains some long passage-based questions, we opt to detect only the last sentence from each question, reducing the chance of missing questions due to formatting differences. Based on inspection, this still incurs many false positives. We then train a 7B-2x model with our DCLM-baseline without the detected overlaps. As seen in Table 7, this does not lead to decreases in model performance, so our performance gains on these two tasks are not likely to be caused by increased presence of their test examples in our dataset.

We also apply the above removal strategy for MMLU on Dolma-V1.7  and FineWeb-Edu . The results can be seen in Table 25 in Appendix O, from which we observe that DCLM-baseline has roughly similar contamination stats as these other high performing datasets. We also provide further analysis that extends to our entire evaluation suite in Appendix O.

## 5 Scaling up DCLM-baseline to the trillion token scale

Here, we test if datasets that perform well on the DCLM benchmark also maintain their strength with an order of magnitude more compute. To ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T DCLM-baseline with the StarCoder  and ProofPile2  datasets to arrive at a 4.1T token dataset. We train a 7B model for 2.5T tokens on this dataset with the same hyperparameters as our largest competition scale except for two separate cool-downs phase for the 200B and 270B tokens on a modified distribution that was 70% DCLM-baseline with a tighter fastText threshold, and 30% math datasets (see Appendix Q). We then take a "model soup" of these two separate cool-downs. Finally, we adopt the continual pretraining methodology from Pouransari et al.  for 100B tokens on the same distribution to increase the context length from 2048 to 8192 (see Appendix Q.2).

In Table 8, we show that our model outperforms all 7B models trained on public training sets and approaches closed-data models trained for more tokens such as Llama-8B, Mistral-7B, and Gemma

    & & Core &  \\ Dataset & Base & w/ RPJ extras & Base & w/ RPJ extras \\  C4 & 23.7 & 25.9 (\(+2.2\)) & 12.5 & 13.3 (\(+0.8\)) \\ RPJ CC only & 24.0 & 25.7 (\(+1.7\)) & 12.1 & 13.5 (\(+1.4\)) \\ RefinedWeb & 25.1 & 26.5 (\(+1.4\)) & 12.9 & 13.1 (\(+0.2\)) \\ DCLM-baseline & 31.1 & 29.9 (\(-1.2\)) & 16.0 & 15.0 (\(-1.0\)) \\   

Table 6: **Mixing high-quality sources with subsets of CommonCrawl (1B-1x scale). We evaluate the impact of mixing high-quality sources (‘RPJ extras’) with various datasets derived from CommonCrawl, using the mixing ratios from Llama/RPJ. Numbers in parentheses indicate the gain or loss in performance due to mixing compared to using only the base dataset.**

   Dataset & \(E\) = MMLU & \(E\) = Hellaswag \\  DCLM-baseline & 51.8 & 77.9 \\ DCLM-baseline (\(E\) removed) & 52.7 & 78.4 \\   

Table 7: **MMLU and Hellaswag overlap removal (7B-2x scale). We remove overlaps detected with MMLU and Hellaswag, in cases where a question and one of its options are detected. We compare models trained before and after this decontamination step, and see that performance does not fall.**7B. Additionally, in Appendix P, we show that our model achieves strong instruction-tuning (IT) performance. After instruction tuning on publicly available IT datasets, our model maintains most of its benchmark performance and achieves an AlpacaEval2.0 LC Win-rate of 16.6, which outperforms Gemma-Instruct (10.4), while approaching the strong performance of Mistral-v0.2-7B (17.1) and Llama3-Instruct (22.9). Finally, in Appendix Q.3, we show results from training a 1B model on 4.3T tokens from DCLM-baseline, StarCoder and ProofPile2 combined, resulting in a strong, small model that outperforms prior small models including Gemma-2B and Qwen2-1.5B.

## 6 Conclusion and limitations

We introduced the DCLM testbed and demonstrated how it leads to new state-of-the-art training sets. Our exploration of the dataset design space is only the beginning and has clear limitations. Due to compute constraints, we could only ablate design dimensions individually and could not test all approaches at larger scales nor train models beyond 7B parameters. We also could not sufficiently explore run-to-run variation. Moreover, there are many variations of DCLM-baseline that we did not explore, such as alternatives to shared deduplication and using differently trained filtering models. We also conducted most of our experiments with only one tokenizer (GPT-NeoX), and other tokenizers may perform better on multilingual tasks or math. Still, we hope that this paper is a starting point for further research on data curation that pushes the state-of-the-art beyond DCLM-baseline.

While models trained on DCLM-baseline are competitive on common language understanding tasks, they currently do not perform as well on code and math. We view this as a consequence of our focus on language understanding in the first version of DCLM, and not an inherent limitation of our benchmark or dataset. Prior work has shown that adding specific training data and post training methods for code and math can substantially improve models on those domains [14; 96; 175; 194; 199]; combining DCLM-baseline with these domain-specific training sets and extending DCLM to cover code and math are interesting future directions. Other important dimensions to expand DCLM along are fairness, multilinguality, and safety. We include some analysis in Appendix S and hope that our open-source testbed can strengthen data-centric research in these directions as well.

   Model & Params & Tokens & 
 Open \\ dataset? \\  & Core & MMLU & Extended \\   \\  Llama2 & 7B & 2T & ✗ & 49.2 & 45.8 & 34.1 \\ DeepSeek & 7B & 2T & ✗ & 50.7 & 48.5 & 35.3 \\ Mistral-0.3 & 7B &? & ✗ & 57.0 & 62.7 & 45.1 \\ QWEN-2 & 7B &? & ✗ & 57.5 & **71.9** & 50.5 \\ Llama3 & 8B & 15T & ✗ & 57.6 & 66.2 & 46.3 \\ Gemma & 8B & 6T & ✗ & 57.8 & 64.3 & 44.6 \\ Phi-3 & 7B &? & ✗ & **61.0** & 69.9 & **57.9** \\   \\  Falcon & 7B & 1T & ✓ & 44.1 & 27.4 & 25.1 \\ OLMo-1.7 & 7B & 2.1T & ✓ & 47.0 & 54.0 & 34.2 \\ MAP-Neo & 7B & 4.5T & ✓ & **50.2** & **57.1** & **40.4** \\   \\  FineWeb edu & 7B & 0.14T & ✓ & 38.7 & 26.3 & 22.1 \\ FineWeb edu & 7B & 0.28T & ✓ & 41.9 & 37.3 & 24.5 \\ DCLM-baseline & 7B & 0.14T & ✓ & 44.1 & 38.3 & 25.0 \\ DCLM-baseline & 7B & 0.28T & ✓ & 48.9 & 50.8 & 31.8 \\ DCLM-baseline + & 7B & 2.6T & ✓ & **57.1** & **63.7** & **45.4** \\ StarCoder + ProofPile2 & & & & & & \\   

Table 8: **State-of-the-art comparison** (beyond 7B-2x scale). We compare our final model with other 7–8B parameter models. DCLM-baseline yields a model that outperforms models trained on open datasets and is competitive with models trained on private datasets.

Acknowledgements.We would like to thank Lilith Bat-Leah, Loubna Ben Allal, Samy Bengio, Mia Chiquier, Adrien Gaidon, Lizzy Grant, Tom Gunter, Awni Hannun, Jonathan Hayase, Mike Lewis, Percy Liang, Ian Magnusson, Yifan Mai, Sewon Min, David Mizrahi, Praveen Paritosh, Guilherme Penedo, Kyle Richardson, Weijia Shi, Karanjeet Singh, Joshua Susskind, Oyvind Tajford, Carl Vondrick, and Elle Wohlmuth, for helpful feedback at various stages of the project. We would like to thank Mike Garrison and Romil Shah for help with compute and infrastructure.

This research was supported by Allen Institute for AI, Open Philanthropy, Institute for Foundations of Machine Learning (IFML), AFOSR MURI grant FA9550-22-1-0380, Israeli Science Foundation (ISF) grant no. 2486/21, Singapore AI Visiting Professorship Programme AIVP-2024-001, Alon Fellowship, Adelis foundation, Israeli Council for Higher Education, Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023, NSF Grants AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844,, research gifts by Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering, NSF Graduate Research Fellowship. MN acknowledges funding by the Federal Ministry of Education and Research of Germany under grant no. 01IS22094B WestAI - AI Service Center West.

We gratefully acknowledge compute budget granted by Gauss Centre for Supercomputing e.V. and by the John von Neumann Institute for Computing (NIC) on the supercomputers JUWELS Booster and JURECA at Julich Supercomputing Centre (JSC)