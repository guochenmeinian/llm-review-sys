# Identifiable Contrastive Learning with

Automatic Feature Importance Discovery

 Qi Zhang\({}^{1}\) Yifei Wang\({}^{2}\) Yisen Wang\({}^{1,3}\)

\({}^{1}\) National Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{2}\) School of Mathematical Sciences, Peking University

\({}^{3}\) Institute for Artificial Intelligence, Peking University

Equal Contribution.Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn).

###### Abstract

Existing contrastive learning methods rely on pairwise sample contrast \(z_{x}^{}z_{x^{}}\) to learn data representations, but the learned features often lack clear interpretability from a human perspective. Theoretically, it lacks feature identifiability and different initialization may lead to totally different features. In this paper, we study a new method named tri-factor contrastive learning (triCL) that involves a 3-factor contrast in the form of \(z_{x}^{}Sz_{x^{}}\), where \(S=(s_{1},,s_{k})\) is a learnable diagonal matrix that automatically captures the importance of each feature. We show that by this simple extension, triCL can not only obtain identifiable features that eliminate randomness but also obtain more interpretable features that are ordered according to the importance matrix \(S\). We show that features with high importance have nice interpretability by capturing common classwise features, and obtain superior performance when evaluated for image retrieval using a few features. The proposed triCL objective is general and can be applied to different contrastive learning methods like SimCLR and CLIP. We believe that it is a better alternative to existing 2-factor contrastive learning by improving its identifiability and interpretability with minimal overhead. Code is available at https://github.com/PKU-ML/Tri-factor-Contrastive-Learning.

## 1 Introduction

As a representative self-supervised paradigm, contrastive learning obtains meaningful representations and achieves state-of-the-art performance in various tasks by maximizing the feature similarity \(z_{x}^{}z_{x}^{+}\) between samples augmented from the same images while minimizing the similarity \(z_{x}^{}z_{x}^{-}\) between independent samples . Besides the empirical success, recent works also discuss the theoretical properties and the generalization performance of contrastive learning .

However, there still exist many properties of contrastive learning that are not guaranteed. In this paper, we focus on a significant one: the feature identifiability. Feature identifiability in the representation learning refers to the property there exists a single, global optimal solution to the learning objective. Consequently, the learned representations can be reproducible regardless of the initialization and the optimizing procedure is useful. As a well-studied topic, identifiability is a desirable property for various tasks, including but not limited to, transfer learning , fair classification  and causal inference . The previous works propose that contrastive learning obtains the linear feature identifiability while lacking exact feature identifiability, i.e., the optimal solutions obtain a freedom of linear transformations . As a result, the different features are coupled, which hurts the interpretability and performance of learned representations.

In this paper, we propose a new contrastive learning model: tri-factor contrastive learning (triCL), which introduces a 3-factor contrastive loss, i.e., we replace \(z_{x}^{}z_{x^{}}\) with \(z_{x}^{}Sz_{x^{}}\) when calculating the similarity between two features, where \(S\) is a learnable diagonal matrix called importance matrix. We theoretically prove that triCL absorbs the freedom of linear transformations and enables **exact feature identifiability**. Besides, we observe that triCL shows other satisfying properties. For example, the generalization performance of triCL is theoretically guaranteed. What is more, we find that the diagonal values of the importance matrix \(S\) in triCL indicate the degrees of feature importance. In Figure 1, we visualize the samples that have the largest values in the most and least important dimensions ordered by the importance matrix. We find the samples activated in the more important dimensions are more semantically similar, which verifies the order of feature importance in triCL is quite close to the ground truth. Theoretically, we prove that the dimensions related to the larger values in the importance matrix make more contributions to decreasing the triCL loss. With the automatic discovery of feature importance in triCL, the downstream task conducted on the representations can be accelerated by selecting the important features and throwing the meaningless features.

As triCL is a quite simple and general extension, we apply it to different contrastive learning frameworks, such as SimCLR  and CLIP . Empirically, we first verify the identifiability of triCL and further evaluate the performance of triCL on real-world datasets including CIFAR-10, CIFAR-100, and ImageNet-100. In particular, with the automatic discovery of important features, triCL demonstrates significantly better performance on the downstream tasks using a few feature dimensions. We summarize our contributions as follows:

* We propose tri-factor contrastive learning (triCL), the first contrastive learning algorithm which enables exact feature identifiability. Additionally, we extend triCL to different contrastive learning methods, such as spectral contrastive learning, SimCLR and CLIP.
* Besides the feature identifiability, we analyze several theoretical properties of triCL. Specifically, we construct the generalization guarantee for triCL and provide theoretical evidence that triCL can automatically discover the feature importance.
* Empirically, we verify that triCL enables the exact feature identifiability and triCL can discover the feature importance on the synthetic and real-world datasets. Moreover, we investigate whether triCL obtains superior performance in downstream tasks with selected representations.

Figure 1: The visualization of samples on ImageNet-100 that have the largest values in 10 selected dimensions of representations learned by tri-factor contrastive learning (each row represents a dimension).

Related Work

**Self-supervised Learning.** Recently, to get rid of the expensive cost of the labeled data, self-supervised learning has risen to be a promising paradigm for learning meaningful representations by designing various pretraining tasks, including context-based tasks , contrastive learning  and masked image modeling . Among them, contrastive learning is a popular algorithm that achieves impressive success and largely closes the gap between self-supervised learning and supervised learning [27; 4; 20; 33]. The idea of contrastive learning is quite simple, i.e., pulling the semantically similar samples (positive samples) together while pushing the dissimilar samples (negative samples) away in the feature space. To better achieve this objective, recent works propose different variants of contrastive learning, such as introducing different training objectives [36; 18; 35], different structures [4; 6; 16; 39], different sampling processes [11; 25; 38; 7] and different empirical tricks [20; 3].

**Theoretical Understandings of Contrastive Learning.** Despite the empirical success of contrastive learning, the theoretical understanding of it is still limited.  establish the first theoretical guarantee on the downstream classification performance of contrastive learning by connecting the InfoNCE loss and Cross Entropy loss. As there exists some unpractical assumptions in the theoretical analysis in , recent works improve the theoretical framework and propose new bounds [1; 2; 34]. Moreover,  analyzes the downstream performance of contrastive learning from a graph perspective and constructs the connection between contrastive learning and spectral decomposition. Recently, researchers have taken the inductive bias in contrastive learning into the theoretical framework and shown the influence of different network architectures [17; 31]. Except for the downstream classification performance of contrastive learning, some works focus on other properties of representations learned by contrastive learning.  discuss the feature diversity by analyzing the dimensional collapse in contrastive learning.  prove that the contrastive models are identifiable up to linear transformations under certain assumptions.

## 3 Preliminary

**Contrastive Pretraining Process.** We begin by introducing the basic notations of contrastive learning. The set of all natural data is denoted as \(_{u}=\{_{i}\}_{i=1}^{N_{u}}\) with distribution \(_{u}\), and each natural data \(_{u}\) has a ground-truth label \(y()\). An augmented sample \(x\) is generated by transforming a natural sample \(\) with the augmentations distributed with \((|)\). The set of all the augmented samples is denoted as \(=\{x_{i}\}_{i=1}^{N}\). We assume both sets of natural samples and augmented samples to be finite but exponentially large to avoid non-essential nuances in the theoretical analysis and it can be easily extended to the case where they are infinite . During the pretraining process, we first draw a natural sample \(_{u}\), and independently generate two augmented samples \(x(|)\), \(x^{+}(|)\) to construct a positive pair \((x,x^{+})\). For the negative samples, we independently draw another natural sample \(^{-}_{u}\) and generate \(x^{-}(|^{-})\). With positive and negative pairs, we learn the encoder \(f:^{d}^{k}\) with the contrastive loss. For the ease of our analysis, we take the spectral contrastive loss  as an example:

\[_{}(f)=-2_{x,x^{+}}f(x)^{}f(x^{+})+ _{x}_{x^{-}}(f(x)^{}f(x^{-}))^{2}.\] (1)

We denote \(z_{x}=f(x)\) as the features encoded by the encoder. By optimizing the spectral loss, the features of positive pairs \((z_{x}^{}z_{x^{+}})\) are pulled together while the negative pairs (\(z_{x}^{}z_{x^{-}}\)) are pushed apart.

**Augmentation Graph.** A useful theoretical framework to describe the properties of contrastive learning is to model the learning process from the augmentation graph perspective . The augmentation graph is defined over the set of augmented samples \(\), with its adjacent matrix denoted by \(A\). In the augmentation graph, each node corresponds to an augmented sample, and the weight of the edge connecting two nodes \(x\) and \(x^{+}\) is equal to the probability that they are selected as a positive pair, i.e.,\(A_{xx^{+}}=_{_{u}}[(x|) (x^{+}|)].\) And we denote \(\) as the normalized adjacent matrix of the augmentation graph, i.e., \(=D^{-1/2}AD^{-1/2}\), where \(D\) is a diagonal matrix and \(D_{xx}=_{x^{}}A_{xx^{}}\). To analyze the properties of \(\), we denote \(=U V^{}\) is the singular value decomposition (SVD) of the normalized adjacent matrix \(\), where \(U^{N N},V^{N N}\) are unitary matrices, and \(=(_{1},,_{N})\) contains descending singular values \(_{1}_{N} 0\).

Exact Feature Identifiability with Tri-factor Contrastive Learning

In this section, we propose a new representation learning paradigm called tri-factor contrastive learning (triCL) that enables exact feature identifiability in contrastive learning. In Section 4.1, we prove that contrastive learning obtains linear identifiability, i.e., the freedom in the optimal solutions are linear transformations. In Section 4.2, we introduce the learning process of triCL and theoretically verify it enables the exact feature identifiability.

### Feature Identifiability of Contrastive Learning

When using a pretrained encoder for downstream tasks, it is useful if the learned features are reproducible, in the sense that when the neural network learns the representation function on the same data distribution multiple times, the resulting features should be approximately the same. For example, reproducibility can enhance the interpretability and the robustness of learned representations . One rigorous way to ensure reproducibility is to select a model whose representation function is _identifiable in function space_. To explore the _feature identifiability_ of contrastive learning, we first characterize its general solution.

**Lemma 4.1** ().: _Let \(=U V^{}\) be the SVD decomposition of the normalized adjacent matrix \(\). Assume the neural networks are expressive enough for any features. The spectral contrastive loss (Eq. 1) attains its optimum when \(\;x\),_

\[f^{*}(x)=}}(U_{x}^{k}(_{1 },,_{k})R)^{},\] (2)

_where \(U_{x}\) takes the \(x\)-th row of \(U\), \(U^{k}\) denotes the submatrices containing the first \(k\) columns of \(U\), and \(R^{k k}\) is an arbitrary unitary matrix._

From Lemma 4.1, we know that the optimal representations are not unique, due to the freedom of affine transformations. This is also regarded as a relaxed notion of feature identifiability, named _linear feature identifiability_\(\) defined below .

**Definition 4.2** (Linear feature identifiability).: Let \(\) be a pairwise relation in the encoder function space \(=\{f:^{k}\}\) defined as:

\[f^{}f^{*} f^{}(x)=Af^{*}(x), \;x,\] (3)

where \(A\) is an invertible \(k k\) matrix.

It is apparent that the optimal encoder \(f\) (Eq. 2) obtained from the spectral contrastive loss (Eq. 1) is linearly identifiable. Nevertheless, there are still some ambiguities _w.r.t._ linear transformations in the model. Although the freedom of linear transformations can be absorbed on the linear probing task , these representations may show varied results in many downstream tasks e.g., in the k-NN evaluation process. So we wonder whether we could achieve the exact feature identifiability. We first further define two kinds of more accurate feature identifabilities below.

**Definition 4.3** (Sign feature identifiability).: Let \(\) be a pairwise relation in the encoder function space \(=\{f:^{k}\}\) defined as:

\[f^{}f^{*} f^{}_{j}(x)= f ^{*}_{j}(x),\;x,j[k].\] (4)

where \(f_{j}(x)\) is the \(j\)-th dimension of \(f(x)\).

**Definition 4.4** (exact feature identifiability).: Let \(\) be a pairwise relation in the encoder function space \(=\{f:^{k}\}\) defined as:

\[f^{} f^{*} f^{}(x)=f^{*}(x),\;x .\] (5)

### Tri-factor Contrastive Learning with Exact Feature Identifiability

Motivated by the trifactorization technique in matrix decomposition problems  and the equivalence between the spectral contrastive loss and the matrix decomposition objective , we consider adding a learnable diagonal matrix when calculating the feature similarity in the contrastive loss to absorb the freedom of linear transformations. To be specific, we introduce a contrastive learning model that enables exact feature identifiability, named _tri-factor contrastive learning (triCL)_, which adopts a tri-term contrastive loss:

\[_{}(f,S)=-2_{x,x^{+}}f(x)^{}Sf(x^{+})+ _{x}_{x^{-}}(f(x)^{}Sf(x^{-}))^{2},\] (6)

where we name \(S=(s_{1},,s_{k})\) as the importance matrix and it is a diagonal matrix with \(k\) non-negative _learnable_ parameters satisfying \(s_{1} s_{k} 0\)3. Additionally, the encoder \(f\) is constrained to be decorrelated, _i.e.,_

\[_{x}f_{i}(x)^{}f_{j}(x)=1,&i=j\\ 0,&i j,\;i,j[k],\] (7)

for an encoder \(f:^{d}^{k}\). One way to ensure feature decorrelation is the following penalty loss,

\[_{}(f)=\|_{x}f(x)f(x)^{}-I\|^{ 2},\] (8)

leading to a combined triCL objective,

\[_{}(f,S)=_{}(f,S)+_{ }(f).\] (9)

Similar feature decorrelation objectives have been proposed in non-contrastive visual learning methods with slightly different forms, _e.g.,_ Barlow Twins .

Since triCL automatically learns feature importance \(S\) during training, it admits a straightforward feature selection approach. Specifically, if we need to select \(m\) out of \(k\) feature dimensions for downstream tasks (_e.g.,_ in-time image retrieval), we can sort the feature dimensions according to their importance \(s_{i}\)'s (after training), and simply use the top \(m\) features as the most important ones. Without loss of generality, we assume \(s_{1} s_{k}\), and the top \(m\) features are denoted as \(f^{(m)}\).

**Identifiability of TriCL.** In the following theorem, we show that by incorporating the diagonal importance matrix \(S\) that regularizes features along each dimension, triCL can resolve the linear ambiguity of contrastive learning and become sign-identifiable.

**Theorem 4.5**.: _Assume the normalized adjacent matrix \(\) has distinct largest \(k\) singular values (\(\;i,j[k]\), \(_{i}_{j}\) when \(i j\)) and the neural networks are expressive enough, the tri-factor contrastive learning (triCL, Eq. 9) attains its optimum when \(\;x\), \(j[k]\)_

\[f_{j}^{}(x)=}}(U_{x}^{k})_{j},S^{*}= (_{1},,_{k}),\] (10)

_which states that the tri-factor contrastive learning enables the **sign feature identifiability**._

As shown in Theorem 4.5, the only difference remaining in the solutions (Eq. 10) is the sign. To remove this ambiguity, for dimension \(j\), we randomly select a natural sample \(_{u}\), encode it with the optimal solution \(f^{}\) of triCL, and observe the sign of its \(j\)-th dimension \(f_{j}^{}()\). If \(f_{j}^{}()=0\), we draw another sample and repeat the process until we obtain a non-zero feature \(f_{j}^{}()\). We then store the sample as an original point \(x_{0j}\) and adjust the sign of different learned representations as follows:

\[_{j}^{}(x)=(-1)^{1(f_{j}^{}(x_{0j})>0)} f_{j}^{}(x ), x,j[k]\] (11)

By removing the freedom of sign, the solution becomes unique and triCL enables the exact feature identifiability:

**Corollary 4.6**.: _Set \(^{}\) as the final learned encoder of tri-factor contrastive learning, and then triCL obtains the exact feature identifiability._

## 5 Theoretical Properties of Tri-factor Contrastive Learning

Besides the feature identifiability, we analyze other theoretical properties of triCL in this section. Specifically, in Section 5.1, we provide the generalization guarantee of triCL and we present another advantage of triCL: triCL can automatically discover the importance of different features. In Section 5.2, we extend triCL to other contrastive learning frameworks.

### Downstream Generalization of Tri-factor Contrastive Learning

In the last section, we propose a new contrastive model (triCL) that enables exact feature identifiability. In the next step, we aim to theoretically discuss the downstream performance of the identifiable representations learned by triCL. For the ease of our theoretical analysis, we first focus on a common downstream task: Linear Probing. Specifically, we denote the linear probing error as the classification error of the optimal linear classifier on the pretrained representations, i.e., \((f)=_{g}_{_{u}}[g(f( )) y()]\). By analyzing the optimal solutions of triCL when using the full or partial features, we obtain the following theorem characterizing their downstream performance.

**Theorem 5.1**.: _We denote \(\) as the probability that the natural samples and augmented views have different labels, i.e., \(=_{_{u}}_{ (|)}[y() y(x)]\). Let \(f^{}_{SCL}\) and \((f^{}_{triCL},^{})\) be the optimal solutions of SCL and triCL, respectively. Then, SCL and triCL have the same downstream classification error when using all features_

\[(f^{}_{SCL})=((f^{}_{triCL})) c_{1}_ {i=k+1}^{N}_{i}^{2}+c_{2},\] (12)

_where \(_{i}\) is the \(i\)-th largest eigenvalue of \(\), and \(c_{1},c_{2}\) are constants. When using only \(m k\) features of the optimal features, triCL with the top \(m\) features admits the following error bound_

\[(f^{(m)}_{triCL}) c_{1}_{i=m+1}^{N}_{i}^{2}+c_{ 2}:=U(f^{(m)}_{triCL}).\] (13)

_Instead, without importance information, we can only randomly select \(m\) SCL features (denoted as \(f^{(m)}_{SCL}\)), which has the following error bound (taking expectation over all random choices)_

\[(f^{(m)}_{SCL}) c_{1}((1-)_{i=1}^{k} _{i}^{2}+_{i=k+1}^{N}_{i}^{2})+c_{2}:=U(f^{ (m)}_{SCL}).\] (14)

_Comparing the two upper bounds, we can easily conclude that_

\[U(f^{(m)}_{SCL})-U(f^{(m)}_{triCL})( _{i=1}^{m}_{i}^{2}-_{i=m+1}^{k}_{i}^{2}) 0.\] (15)

_Thus, triCL admits a smaller error when using a subset of features for downstream classification._

Theorem 5.1 shows that triCL is particularly helpful for downstream tasks when we select a subset of features according to the learned feature importance. When using all features, the two methods converge to the same downstream error bound, which also aligns with our observations in practice.

### Extensions to Other Contrastive Learning Frameworks

In the above sections, we propose tri-factor contrastive learning (triCL) which enables the exact feature identifiability and obtains an ordered representation while achieving the guaranteed downstream performance. In the next step, we extend the triCL to a unified contrastive learning paradigm that can be applied in different contrastive learning frameworks.

**Extension to Other SSL Methods.** As replacing the 2-factor contrast \(f(x)^{}f(x^{})\) with the 3-factor contrast \(f(x)^{} Sf(x^{})\) is a simple operation and not constrained to the special form of spectral contrastive loss, we extend triCL to other contrastive frameworks. We first take another representative contrastive learning method SimCLR  as an example. Comparing the InfoNCE loss in SimCLR and the spectral contrastive loss, we find they are quite similar and the only difference is that they push away the negative pairs with different loss functions (\(l_{2}\) loss v.s. \(logsumexp\) loss). So we propose the tri-InfoNCE loss by changing the terms of negative samples in triCL (Eq. 9) to a tri-logsumexp term, i.e.,

\[_{}(f)= -_{x,x^{+}} Sf(x^{+}))}{_{x^{-}}(f(x)^{} Sf(x^{-}))}+\|_{x}f(x)f(x)^{}-I\|^{2}.\] (16)Besides the InfoNCE loss used in SimCLR, we present more types of tri-factor contrastive loss in Appendix C. In summary, triCL can be applied in most of the contrastive frameworks by replacing the \(f(x)^{}f(x^{})\) term with a tri-term \(f(x)^{}Sf(x^{})\) when calculating the similarity, where the importance matrix \(S\) is a diagonal matrix capturing the feature importance.

**Extension to the Multi-modal Domain.** Different from the symmetric contrastive learning frameworks like SCL  and SimCLR , the multi-modal contrastive frameworks like CLIP  have asymmetric networks to encode asymmetric image-text pairs. So we extend the tri-factor contrastive loss to a unified asymmetric form:

\[_{}(f_{A},f_{B},S)=& -2_{x_{a},x_{b}}f_{A}(x_{a})^{}Sf_{B}(x_{b})+_{x_{a}^{-},x_{b}^{-}}(f_{A}(x_{a}^{-})^{}Sf_{B}(x_{b}^{-}))^{ 2}\\ &+\|_{x_{a}}f_{A}(x_{a})f_{A}(x_{a})^{}-I \|^{2}+\|_{x_{b}}f_{B}(x_{b})f_{B}(x_{b})^{}-I\| ^{2}.\] (17)

where \(f_{A},f_{B}\) are two different encoders with the same output dimension \(k\). Uniformly, we denote that the positive pairs \((x_{a},x_{b})_{O}\) and the negative samples \(x_{a}^{-},x_{b}^{-}\) are independently drawn from \(_{A},_{B}\). With different concrete definitions of \(_{O},_{A},_{B}\), different representation learning paradigms can be analyzed together. For single-modal contrastive learning, the symmetric process in Section 3 is a special case of the asymmetric objective. For multi-modal learning, we denote \(_{A},_{B}\) as the distributions of different domain data and \(_{O}\) as the joint distribution of semantically related pairs (e.g., semantically similar image-text pairs in CLIP). We theoretically prove that the asymmetric tri-factor contrastive learning still obtains the exact feature identifiability and guaranteed generalization performance in Appendix A.

## 6 Experiments

In this section, we provide empirical evidence to support the effectiveness of tri-factor contrastive learning. In section 6.1, we empirically verify that the tri-factor contrastive learning can enable the exact feature identifiability on the synthetic dataset. In section 6.2, we empirically analyze the properties of the importance matrix and demonstrate the advantages of automatic feature importance discovery on various tasks. Furthermore, in section 6.3, we empirically compare the generalization performance of tri-factor contrastive learning with different baselines including SimCLR  and spectral contrastive learning (SCL)  on CIFAR-10, CIFAR-100 and ImageNet-100.

### The Verification of the Identifiability on the Synthetic Dataset

Due to the randomness of optimization algorithms like SGD, we usually can not obtain the optimal encoder of the contrastive loss. So we consider verifying the feature identifiability on the synthetic dataset. To be specific, we first construct a random matrix \(\) with size \(5000 3000\) to simulate the augmentation graph. Then we compare two different objectives: \(\|-FG^{}\|_{F}^{2}\), \(\|-FSG^{}\|_{F}^{2}\), where \(F\) is a matrix with size \(5000 256\), \(G\) is a matrix with size \(3000 256\) and \(S\) is a diagonal with size \(256 256\). With the analysis on , these two objectives are respectively equivalent to the spectral contrastive loss and tri-factor contrastive loss. According to the Eckart-Young Theorem , we obtain the optimal solutions of them with the SVD algorithms. For two objectives, we respectively obtain 10 optimal solutions and we calculate the average Euclidean distance between 10 solutions. We investigate that the optimal solutions of the trilaterization objective are equal (average distance is 0) while there exist significant differences between optimal solutions of bifactorization (average distance is 101.7891 and the variance of distance is 17.2798), which empirically verifies the optimal solutions of contrastive learning obtain the freedom while triCL can remove it. More details can be found in Appendix B.

### The Automatic Discovery of Feature Importance

Except for the visualization example in Figure 1, we further quantitatively explore the properties of ordered representations learned by triCL.

**The Distribution of the Importance Matrix.** We first observe the distribution of the diagonal values in the learned importance matrix \(S\). Specifically, we pretrain the ResNet-18 on CIFAR-10, CIFAR-100 and ImageNet-100  by triCL. Then we normalize the importance matrices and ensure the sum of the diagonal values is 1. In Figure 2(a), we present the diagonal values of the importance matrices trained on different datasets and we find they are highly related to the properties of the datasets. For instance, CIFAR-100 and ImageNet-100, which possess a greater diversity of features, exhibit more active diagonal values in the top 50 dimensions compared to CIFAR-10. While for the smallest diagonal values, the different importance matrices are quite similar as most of them represent meaningless noise.

**The K-NN Accuracy on Selected Dimensions.** With the ResNet-18 and the projector pretrained on CIFAR-10, CIFAR-100 and ImageNet-100, we first sort the dimensions according to the importance matrix \(S\). Then we conduct the k-NN evaluation for every 10 dimensions. As shown in Figure 2, the k-NN accuracy decreases slowly at first as these dimensions are related to the ground-truth labels and they share a similar significance. Then the k-NN accuracy drops sharply and the least important dimensions almost make no contributions to clustering the semantically similar samples. Note that the k-NN accuracy drops more quickly on CIFAR-10, which is consistent with the fact it has fewer types of objects.

**Linear Evaluation on Selected Dimensions.** We first train the ResNet-18 with triCL and spectral contrastive learning (SCL) on CIFAR-10. Then we select 20 dimensions from the learned representations (containing both the backbone and the projector). For triCL, we sort the dimensions according to the descending order of the importance matrix and select the largest 20 dimensions (1-20 dimensions), middle 20 dimensions (119-138 dimensions), and smallest 20 dimensions (237-256 dimensions). And for SCL, we randomly choose 20 dimensions. Then we train a linear classifier following the frozen representations with the default settings of linear probing. As shown in Figure 3, the linear accuracy decreases in the descending order of dimensions in triCL and the linear accuracy of the largest 20 dimensions of triCL is significantly higher than random 20 dimensions of SCL, which verifies that triCL can discover the most important semantic features related to the ground-truth labels.

**Image Retrieval.** We conduct the image retrieval on ImageNet-100. For each sample, we first encode it with the pretrained networks and then select dimensions from the features. For the methods learned by triCL, we sort the dimensions according to the values in the importance matrix \(S\) and select the largest dimensions. For SCL, we randomly choose the dimensions. Then we find \(100\) images that have the largest cosine similarity with the query image and calculate the mean average precision (mAP) that returned images belong to the same class as the query ones. As shown in Figure 3, we observe the 50 dimensions of triCL show the comparable performance as the complete representation while the performance of SCL continues increasing with more dimensions, which further verifies that triCL can find the most important features that are useful in downstream tasks.

**Out-of-Distribution Generalization.** Besides the in-domain downstream tasks, we also examine whether the importance matrix can sort the feature importance accurately with out-of-domain shifts. To be specific, we pretrain the ResNet-18 with SCL and triCL on ImageNet-100 and then conduct

Figure 2: The distributions of the discovered feature importance and the verifications on whether the order of feature importance indicated by the importance matrix is accurate.

linear probing with 40 dimensions of learned representations on the out-of-domain dataset stylized ImageNet-100 . During the downstream evaluation process, we sort the dimensions according to the descending order of the importance matrix and select the largest 40 dimensions (1-40 dimensions), middle 40 dimensions (237-276 dimensions), and smallest 40 dimensions (473-512 dimensions) of the features learned by triCL. And for SCL, we randomly select 40 dimensions. As shown in Figure 3(c), we observe that the transfer accuracy of the most important features learned by triCL shows significantly superior performance, which shows the robustness and effectiveness of the importance matrix.

### Transfer Learning on Benchmark Datasets

**Setups.** During the pretraining process, we utilize ResNet-18  as the backbone and train the models on CIFAR-10, CIFAR-100 and ImageNet-100 . We pretrain the model for 200 epochs On CIFAR-10, CIFAR-100, and for 400 epochs on ImageNet-100. We select two self-supervised methods as our baselines and apply tri-factor contrastive learning on them, including spectral contrastive learning (SCL) , SimCLR . When implementing the tri-factor contrastive learning, we follow the default settings of the baseline methods. During the evaluation process, we consider two transfer learning tasks: linear evaluation and finetuning. During the linear evaluation, we train a classifier following the frozen backbone pretrained by different methods for 50 epochs. During the finetuning process, we train the whole network (including the backbone and the classifier) for 30 epochs.

**Results.** As shown in Table 1, we find that triCL shows comparable performance as the original contrastive learning methods in transfer learning on different real-world datasets, which is consistent with our theoretical analysis. Meanwhile, we observe that triCL shows significant improvements on some tasks, e.g., triCL improves the finetune accuracy of SCL by 1.7% on CIFAR-100. Compared to the original contrastive learning methods, the representations learned by triCL keep comparable transferability with stronger identifiability and interpretability.

    &  &  \\  & CIFAR10 & CIFAR100 & ImageNet100 & CIFAR10 & CIFAR100 & ImageNet100 \\  SCL & **88.4** & 60.3 & **72.3** & 92.3 & 66.1 & **76.4** \\ tri-SCL & 88.3 & **60.8** & 71.2 & **92.7** & **67.8** & 76.3 \\  SimCLR & **87.9** & **60.2** & 73.5 & 92.2 & 72.5 & **75.9** \\ tri-SimCLR & **87.9** & 59.8 & **74.1** & **92.3** & **73.3** & 75.6 \\   

Table 1: The linear probing acuuracy and finetune accuracy of ResNet-18 pretrained by tri-factor contrastive learning and self-supervised baselines on CIFAR-10, CIFAR-100, ImageNet-100. TriCL averagely obtains a comparable performance as the original contrastive learning. Moreover, triCL achieves significant improvements on some tasks, such as finetuning on CIFAR-100.

Figure 3: With the automatic discovery of feature importance, Tri-factor contrastive learning (triCL) obtains superior performance than spectral contrastive learning (SCL) on downstream tasks that use selected representations.

Conclusion

In this paper, we propose a new self-supervised paradigm: tri-factor contrastive learning (triCL) which replaces the traditional 2-factor contrast with a 3-factor form \(z_{x}^{}Sz_{x}^{}\) when calculating the similarity in contrastive learning, where \(S\) is a learnable diagonal matrix named importance matrix. With the importance matrix \(S\), triCL enables the exact feature identifiability. Meanwhile, the diagonal values in the importance matrix reflect the importance of different features learned by triCL, which means triCL obtains the ordered representations. Moreover, we theoretically prove that the generalization performance of triCL is guaranteed. Empirically, we verify that triCL achieves the feature identifiability, automatically discovers the feature importance and achieves the comparable transferability as current contrastive learning methods.