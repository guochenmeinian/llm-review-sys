# Learning Group Actions on Latent Representations

Yinzhu Jin

Department of Computer Science

University of Virginia

yj3cz@virginia.edu

&Aman Shrivastava

Department of Computer Science

University of Virginia

as3ek@virginia.edu

&P. Thomas Fletcher

Department of Electrical and Computer Engineering

Department of Computer Science

University of Virginia

ptf8v@virginia.edu

###### Abstract

In this work, we introduce a new approach to model group actions in autoencoders. Diverging from prior research in this domain, we propose to learn the group actions on the latent space rather than strictly on the data space. This adaptation enhances the versatility of our model, enabling it to learn a broader range of scenarios prevalent in the real world, where groups can act on latent factors. Our method allows a wide flexibility in the encoder and decoder architectures and does not require group-specific layers. In addition, we show that our model theoretically serves as a superset of methods that learn group actions on the data space. We test our approach on five image datasets with diverse groups acting on them and demonstrate superior performance to recently proposed methods for modeling group actions.

## 1 Introduction

Group actions are a natural mathematical representation of symmetries and geometric transformations of data. Recent work has demonstrated that explicitly modeling and learning such group actions in neural networks can be beneficial for many tasks, such as learning latent representations , generative models , and classifiers . While many existing works model group actions on the data space, to the best of our knowledge, nearly all prior works have overlooked group actions on latent factors.

However, there are certain scenarios where we desire to model group actions on factors that are not directly observed. Figure 1 provides an illustrative example. The top row shows a rotating image of a '7', i.e., an orbit under the group action of \((2)\) on the image space. (Technically, because of image interpolation, this is only approximately a group action.) This can be effectively modeled using existing approaches designed to learn group actions on the data space. However, if we introduce a slight modification, adding a fixed block, then the digit will be partially occluded for some rotations. The occluded part of the digit is absent from these images but reappears when the digit continues to rotate. This new scenario can no longer be modeled as a group action on the data space. Instead, the group is now acting on the underlying factors. By encoding the representation of these latent factors, specifically the digit in this example, and learning the group action on it, we will be able to correctly model this scenario. While this is a synthesized example, it does exemplify a common phenomenon in real-world images, namely occlusion. Other scenarios in real-world data involve group actions on latent factors. For example, consider taking photographs of a rotating 3D object. The rotation group will be acting on the 3D object geometry but not on the resulting 2D images.

We propose to learn group actions in the latent space, without the necessity to conform to group actions on the data space. Employing an autoencoder framework, we learn the latent data representations on which the group acts. Our method is not confined to any specific group or a certain set of groups. It also does not depend on any group-specific layers for encoding and decoding the data, i.e., it allows wide flexibility in the architecture of the autoencoder, as long as it is expressive enough. We present several examples of groups to explain how our model can be applied. Furthermore, we theoretically demonstrate that our model is capable of learning group actions on the data space as well, positioning it as an extension of general data space group action modeling methods.

In summary, there are four key elements to our contributions:

* We propose a method to learn group actions in the latent space, which allows our model to perform group actions on the latent factors.
* Our approach is flexible and can be applied to different groups without group-specific architectures to obtain latent representations. This enables the model to benefit from any advanced deep learning frameworks.
* We focus on rendering new data given the applied group action rather than merely learning the representation.
* The proposed strategy can be seen as an extension of prior works on modeling group actions in the data space. It can still be used to model group actions on the data space, which we show is a special case of the more general setup.

## 2 Related works

There has been considerable research on incorporating group actions into data representation learning and to benefit downstream tasks. For instance, Dey et al.  introduces a GAN-type model with a discriminator that is equivariant to the \(p4\) or \(p4m\) group, resulting in enhanced generative performance even with limited training samples. Moreover, instead of learning the group actions in the data space, Park et al.  learn representations equivariant to the latent group actions with a contrastive loss. Wang et al.  propose to exploit extrinsic equivariance to model latent representation symmetries and demonstrate benefits in a downstream reinforcement learning task.

Furthermore, part of these works are interested in rendering new data by applying group actions. A significant portion of the studies concentrates on a specific group that acts on the data space. To give an example, Hoogeboom et al.  put forth a diffusion model that maintains equivariance to 3D Euclidean transforms at each denoising time step, specifically tailored for 3D molecule generation. In contrast, Yim et al.  propose a generative model of protein backbones, modeled as a product of rigid body motions, using a manifold diffusion model . These approaches rely on architectures tailored to be equivariant to a specific group or a set of groups. Cohen and Welling  first introduced a group-equivariant convolutional neural network (CNN) featuring group-equivariant convolutional layers. This architecture is designed for groups that represent discrete transformations, including translations, rotations, and reflections. This was followed by multiple works extending group equivariance to broader classes of groups and architectures [5; 23; 31]. In another vein, Satorras et al.  present an \((n)\) equivariant graph neural network, where each graph convolutional layer is inherently designed to be \((n)\)-equivariant.

In alignment with our proposed method, several other works strive to construct models with more flexibility on the types of groups involved. For example, Quessard et al.  parameterize a group representation as a product of rotation matrices. The model assumes a finite number of group elements and learns a representation for each. Training involves sequential data collected from a series of

Figure 1: Top row: a group action on the image; bottom row: a group action on the digit but not on the image

group elements acting on the underlying generative factor in a known order. It is noteworthy that this model uses the product of groups itself as the latent representation. However, a key distinction is that it cannot effectively model group actions on sets containing more than one orbit, due to its direct mapping from the group to the data space. On the other hand, Winter et al.  propose learning latent representations consisting of group-invariant components and group elements for acting on the data space. While their framework is not specific to any particular group, the model requires group-specific architectures capable of generating group-invariant representations, e.g., steerable CNNs . In another approach, Hwang et al.  suggest predicting the group element that transforms one data point to another. Utilizing a VAE  type architecture, they use a Euclidean vector space as the group and define the group action on the data space to correspond to addition in the latent space. This results in a group action on the data space by enforcing the encoder and decoder to both be diffeomorphic mappings. However, this design choice significantly limits the flexibility as it can only model groups that are isomorphic to a real vector space.

Another line of work that is related to our work is novel view synthesis, where the goal is to take an input image of a scene and generate a new image of that scene from a novel camera pose. Although our method is not specifically a novel view synthesis model, i.e., it is a method for more general latent group actions, we do demonstrate its effectiveness in novel view synthesis tasks as an example of 3D rotation group actions. Therefore, we compare to existing geometry-free novel view synthesis methods [9; 28]. Unlike geometry-aware methods [18; 27; 35], geometry-free approaches do not necessitate test-time optimization, which is also the case for our method. For example, Sajjadi et al.  utilizes a transformer architecture which also support multiple view inputs. On the other hand, Dupont et al.  introduces equivariance into the latent representations. However, unlike our method, they directly apply group actions to the 3D latent volumes as if they are 3D images. This results in higher time complexity despite their relatively small model sizes. Also note that novel view synthesis is by definition intended to learn the view transformation of the whole scene. While our approach is more flexible and for example can be applied to the scenario where only a foreground object is rotating, while the background stays fixed.

## 3 Group actions on latent representations

We will consider an autoencoder that takes a data point \(x\) and encodes it into a latent representation \(z\) through the encoder mapping \(E:\). The decoder mapping \(D:\) maps a latent representation back into the data space. Typically, the data space, \(\), and latent space, \(\), are real vector spaces, but the mathematical development below does not require this. We will denote the image of \(D\) as \(^{}=(D)\), which is the space of all possible reconstructions of the decoder. Furthermore, we consider a group \(G\) that acts on the latent space \(\) via

\[:G.\]

When the context is clear, we will denote the group action as \(g\,.\,z=(g,z)\). We will also use the notation \(_{g}:\) to mean \(_{g}(z)=(g,z)\). As a reminder, a group action is required to follow two rules for all \(g_{1},g_{2} G\) and all \(z\):

\[e\,.\,z =z,\] \[g_{2}\,.(g_{1}\,.\,z) =(g_{2}g_{1})\,.\,z,\]

where \(e G\) denotes the identity element.

### Varying and invariant latent representations

In general scenarios, we need to model both factors that are varying with the group action and those that are invariant to the group action. Coming back to the example of taking photos of rotating 3D objects, the background is invariant to the rotations. Therefore, we propose to split the latent space \(\) into a direct product of varying and invariant parts: \(=_{v}_{i}\). We will write a point \(z\) as

\[z=[z_{v};z_{i}],\]

where \(z_{v}_{v}\) and \(z_{i}_{i}\) represent factors that are varying with and invariant to the group action, respectively. Given a group action \(g\,.\) on \(_{v}\), we define the group action on \(\) to be:

\[g\,.\,z=[g\,.\,z_{v};z_{i}].\]It is clear that the identity element \(e G\) will leave \(z\) fixed. Next, we check the associativity:

\[g_{2}\!\!(g_{1}\!\!z) =[g_{2}\!\!g_{1}\!\!z_{v};z_{i}]\] \[=[(g_{2}\!\!g_{1})\!\!z_{v};z_{i}]\] \[=(g_{2}\!\!g_{1})\!\!z,\]

and therefore, this is a valid group action on \(\).

### Latent space group action model

The goal of training is to jointly learn to autoencoder a latent representation of our data along with a group action on those latent factors. For the target group action \(\), we directly compute \((g,z)\). The output of this group action is then sent as input to a decoder, \(D\). During training, we look at a pair of data, \(x_{1},x_{2}\), at a time, whose corresponding latent representations, \(z_{1},z_{2}\), lie in the same orbit, and we are given \(g G\) such that \(z_{2}=g\!\!z_{1}\).

The loss is simply the reconstruction loss with group actions, where the objective is to minimize:

\[_{}(x_{2},D(g\!\!z_{1})).\]

\(_{}\) can be any standard reconstruction loss appropriate for data in \(\), e.g., binary cross entropy (BCE) loss or mean square error (MSE), etc. Similarly, we symmetrically add the reconstruction loss of \(x_{1}\), which gives the final form:

\[(x_{1},x_{2})=_{}(x_{2},D(g\!\!z_{1}))+ _{}(x_{1},D(g^{-1}\!\!z_{2})).\]

We also find adding LPIPS  loss is also helpful for image rendering. It is especially beneficial when the given image only contains very limited information of the latent factor on which the group acts. For example, if we rotate a 3D object, having single random angle of view only gives us scarce information. In our experiments, we found that the perceptual loss guides the model to render more visually convincing images in this circumstance.

### Skip connections and attention

When applying group actions to the latent representation \(z\), we would like to maintain generalizability by acting on \(z\) itself instead of transforming the latent coordinates as done by Dupont et al. . This requires us to sufficiently "mix" spatial dimensions when the given group action is affecting the image globally. However, it is a well-known fact that this will result in some loss of image details . To address this, we include skip connections in our architecture inspired by the U-Net .

We skip connect the higher resolution features from the earlier stages of the downsampling path to the corresponding step of the upsampling path. Since the spatial dimensions of the higher resolution features are not mixed adequately, an attention module  is applied before concatenating to the upsampling path, where the upsampled feature is acting as the query. This gives us the overall framework of our method, as illustrated in Figure 2. The downsampling module can employ either convolutional layers paired with pooling layers or strided convolutions. Conversely, for upsampling, transposed convolutional layers in conjunction with plain convolutional layers can be utilized.

Figure 2: Illustration of our latent space group action model.

Induced group actions on the data space

In this section, we discuss the conditions under which a group action on the latent space, \(\), will induce a group action on the output space of the decoder, \(^{}=(D)\). It is worth mentioning that we don't define this as a group action on the full data space, \(\), because it doesn't make sense to consider points in \(\) that can't be reconstructed by the decoder. We start with a definition:

**Definition 4.1**.: A decoder \(D\) is called **consistent** with a group action of \(G\) on \(\) if for any \(z_{1},z_{2}\) such that \(D(z_{1})=D(z_{2})\), it is the case that \(D(g\,.\,z_{1})=D(g\,.\,z_{2})\) for any \(g G\).

**Proposition 4.2**.: _Let \(D\) be a decoder consistent with a group action \(:G\). Then \(D\) induces a group action \(:G^{}^{}\) on \(^{}=(D)\), defined as follows. For any \(x^{}\), there exists \(z\) such that \(D(z)=x\). Then \((g,x)=D(g\,.\,z)\). In other words, the following diagram commutes:_

Proof.: We first note that the induced group action \(\) is a well-defined mapping. That is, \((g,x)\) does not depend on the latent representation of \(x\), precisely because of the consistency condition on \(D\). Next, we show that \(\) satisfies the properties of a group action. For any \(g_{1},g_{2} G\) and \(x^{}\), we can pick a \(z\) such that \(x=D(z)\), and we have

\[(e,x) =D((e,z))=D(z)=x,\] \[(g_{2},(g_{1},x)) =(g_{2},D((g_{1},z)))\] \[=D((g_{2},(g_{2},z)))\] \[=D((g_{2}g_{1},z))\] \[=(g_{2}g_{1},x),\]

which are the two properties for \(\) to be a group action. 

The consistency condition of Definition 4.1 is difficult to check directly. The next result shows a more intuitive condition that a decoder that can be inverted by an encoder satisfies the consistency condition for any group action on its latent space.

**Proposition 4.3**.: _Let \(E:\) and \(D:\) be an autoencoder such that \(E(D(z))=z\) for all \(z\). Let \(:G\) be a group action. Then \(D\) is consistent with \(\) and thus induces a group action \(\) on \(^{}\) (as defined in Proposition 4.2)._

Proof.: The condition \(E(D(z))=z\) implies that \(D\) is injective. To see this, consider two points \(z_{1},z_{2}\) such that \(z_{1} z_{2}\). Then it must be the case that \(D(z_{1}) D(z_{2})\) in order for \(E(D(z_{1}))=z_{1} z_{2}=E(D(z_{2}))\). Next, from Definition 4.1 it is clear that injectivity of \(D\) implies it is consistent with any group action. 

We note that the condition that \(E(D(z))=z\) for all \(z\), i.e., that \(E\) be a left-inverse of \(D\), does not imply \(D(E(x))=x\). In other words, the encoder and decoder are not necessarily (two-sided) inverses of each other, as is the case in . In that work, the two-sided inverse requirement means that \(\) and \(\) must be vector spaces of the same dimension. For the one-sided inverse condition in Proposition 4.3, \(\) and \(\) do not have to be isomorphic sets. For example, if \(\) and \(\) are vector spaces, \(\) can have smaller dimension than \(\).

## 5 Examples

### 2D and 3D rotations

We look at 2D and 3D rotation groups, \((2)\) and \((3)\). The rotation matrix \(g(k)\) acts on \(\) in the manner described in Section 3.1. As for the group action of \((k)\) on \(_{v}\), we apply a given rotation matrix to every subset of \(k\) dimensions separately. More specifically, we reshape \(z_{v}\) into an \(k n\) matrix, with \(n=(z_{v})/k\). Denoting this matrix as \(z_{v}^{}\), we define the group action on \(z\) as:

\[g\,.\,z=[(g z_{v}^{});z_{i}],\]

where \(()\) indicates reshaping the given matrix into a vector.

### Image contrast transformations

An image contrast transformation is a diffeomorphic function from \(\) to \(\) that is applied to each pixel value. We can define a two-parameter family of image contrast transformation as the affine group on the real line, \(G=(^{1})=_{>0}\), as follows. Note an element \((a,b) G\) represents a scaling by \(a\) and shift by \(b\) of the real line. The group operation of \(G\) is \((a_{1},b_{1})(a_{2},b_{2})=(a_{1}a_{2},b_{1}+a_{1}b_{2})\). Now, we show that \(G\) can also act on the unit interval. Let \((a,b) G\) and \(x\) be a pixel intensity, and define

\[(a,b)\,.\,x=(a(x)+b).\]

The sigmoid function \(\) and \(\) are inverse functions of each other. We can prove this is a valid group action on \(\), where the group being \((1)\). The output of \(()\) is always in \(\), and therefore, \(\) is closed under the group action. We can model this in the latent space with a simpler \((^{1})\) group action:

\[(a,b)\,.\,z=[az_{v}+b;z_{i}].\]

### Cyclic group transformations

In additional to the continuous groups, our method can also be applied to discrete group actions. The cyclic group \(C_{k}\) is equivalent to the set \(\{0,1,2,,k-1\}\) equipped with the binary operation defined as addition modulo \(k\). In implementation, this can be represented as the subset of \((2)\) consisting of discrete rotations by angles \(\{0,,,,\}\). Then this representation of \(C_{k}\) can be incorporated into our model as described in Section 5.1.

## 6 Experiments

We conduct experiments on five different image datasets. Our models and all the baseline models train on pairs of images. We show some sample pairs from each dataset in Figure 3.

### Datasets

**Rotated MNIST** dataset is obtained by simply rotating images from MNIST dataset  about the center by random angles \(\), where \([0,2)\). Bilinear interpolation is used for sampling. We use the original train-test split and image size of \(28 28\). The group action for this dataset is 2D rotation with the group being \((2)\). The ground truth actions between each pair is easily computable from the rotation angles. To ensure that the group action is also on the data space, as stated in Proposition 4.3, we introduce an additional loss term, \(\|z-E(D(z))\|_{2}^{2}\), where \(z\) denotes latents encoded from real data. Please see the appendix for more details.

**Rotated and blocked MNIST** dataset is further processed from the rotated MNIST dataset by adding one randomly placed \(7 7\) white square to each image with probability \(0.8\). The group action is still

Figure 3: Data samples used in the experiments. Each column represents a pair related by a group action, i.e., \(x_{1}=g\,.\,x_{2}\).

2D rotation. Moreover, images of a pair either have no squares or have squares at the same locations. In other words, the square is invariant to the group action. This gives us a dataset where the rotation is acted on the latent factor--representation of the digits--and not on the whole image.

**Brain MRI** dataset is derived from the Human Connectome Project . It consists of 1113 3D brain magnetic resonance images with brain segmentations from FreeSurfer . We took mid-coronal 2D slices for each subject, among which 880 images are used for training and 233 for testing. Then the image contrast transformation defined in Section 5.2 is performed only to the pixels in the brain, leaving the remaining parts (skull, neck, etc.) unchanged. Note that this is not a simple contrast transformation on the whole image. The model needs to identify the brain pixels, and thus, this is again a group action on a latent factor and not the data itself. We randomly sample \(a=e^{t},t(0,0.25)\) and \(b(0,0.25)\) 100 times for each original image.

**Neural 3D mesh renderer (NMR)** dataset has been used in multiple previous works in the field of novel view synthesis. This dataset is derived from ShapeNet  by rendering each object at 24 fixed views around the object in a cycle. This forms a cyclic group, \(C_{24}\), acting on the latent factor - the camera angle. We stick to the original split for training and evaluations.

**Plane in the sky** dataset is our own rendering of ShapeNet Core  airplane objects. Each airplane is put in a real sky background cropped from the SWIMSEG sky image dataset . We uniformly sample 100 random 3D rotation matrices from \((3)\) and rotate the plane. Different from the novel view synthesis problem, this results in sets of images with the same sky background and varying plane orientations. We randomly split out \(20\%\) as the testing set.

### Comparison to baselines

Implementation details and baseline models.All our experiments are implemented with the PyTorch  package. As illustrated in Figure 2, our encoder consists of several convolutional downsampling modules, while our decoder comprises convolutional upsampling modules. We used skip connections and attention modules for the two rendered 3D objects datasets. For more details of model architectures for each dataset, readers can refer to the appendix. The training is performed by randomly sampling pairs of data points from the same orbit. We used MSE for the reconstruction loss, and further added LPIPS loss for rendered 3D objects datasets with a weight of \(0.0005\).

For the two MNIST derived datasets, we compare to Hwang et al.  and Winter et al. . While for the brain MRI set, we only compare to Hwang et al. , since Winter et al.  requires tailored equivariant layer for each group and there is no readily available one for the image contrast transformation group. For the two datasets that are rendered images of 3D objects, we compare to two novel view synthesis models - Sajjadi et al.  and Dupont et al. .

For the encoders of Hwang et al. , which encode the group action given each pair, we use almost identical architectures as our encoders. The only difference is that their encoders are variational and require reparameterization. We carefully follow their work to implement the decoders, which are built upon Glow  type of normalizing flow architectures. All the other baselines have official or officially recognized implementations.

To evaluate the model's ability in learning group actions accurately, we compare the predicted \(D(g\,.\,z)\) with the ground truth image. Since Hwang et al.  encodes group actions based on pairs only, we present another pair of samples, \(x_{3}\) and \(x_{4}\), s.t. \(x_{4}=g\,.\,x_{3}\), for their models to encode \(g\). In our experiments, we find their model performs best when given \(x_{3}\) and \(x_{4}\) having the same absolute angles, or the same \((a,b)\) w.r.t. the original images, as \(x_{1}\) and \(x_{2}\) respectively.

Qualitative results.The sample reconstructions for the first three datasets are shown in Figure 4. As we can see, Hwang et al.  demonstrates problems learning correct rotation angles for both MNIST derived datasets.. Visually, Winter et al.  and our models exhibits comparable performance on the rotated MNIST, indicating the capability of our models to model group actions on \(\) as well. As for the rotated and blocked MNIST, their model attempts to learn rotation invariant representations, leading to challenges in correctly modeling the blocks. At times, the model tends to align blocks with the digit structures, as the rotation is acted on the digit, as illustrated in the first column of the sample outputs. While for the brain MRI dataset, both models show impressive performance, and it is difficult to determine a noticeable advantage visually.

The sample reconstructions for the two rendered 3D objects datasets are shown in Figure 5. As NMR is a novel view synthesis and two baseline models are designed for this task, we can see they both render reasonable reconstructions with Sajjadi et al.  giving sharper images. Our model also correctly learns view angle changes and even captures image details better in some cases. While for the plane in the sky dataset, which is not a novel view synthesis dataset, we can see both baselines struggle to give convincing results. Dupont et al.  has relatively better reconstructions while Sajjadi et al.  sometimes miss the plane orientation significantly. Our model renders much higher quality images with correct orientations and better details.

    &  &  &  \\   & \(\)PSNR & \(\)SSIM & \(\)PSNR & \(\)SSIM & \(\)PSNR & \(\)SSIM \\  Winter et al.  & 21.97 & 0.874 & 14.05 & 0.586 & NA & NA \\ Hwang et al.  & 15.29 & 0.992 & 10.19 & 0.990 & 27.43 & **1.000** \\
**Ours** & **26.07** & **1.000** & **23.55** & **1.000** & **35.99** & **1.000** \\   

Table 1: Quantitative results on MNIST derived datasets and brain MRI dataset

Figure 4: Sample reconstructions on MNIST derived datasets and brain MRI dataset

Figure 5: Sample reconstructions on NMR dataset and plane in the sky dataset

**Quantitative results.** We chose peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) as quantitative metrics for the first three datasets and present resulting values in Table 1. Our models consistently achieved the best results across all three datasets and both metrics. For the NMR dataset and plane in the sky dataset, we further compute LPIPS (VGG) in addition to PSNR and SSIM. The results are reported in Table 2. Our models also achieves best results consistently, which aligns with the qualitative results. We can also observe that between the two baselines, Sajjadi et al.  is performing better on NMR dataset, while Dupont et al.  performing better on the plane in the sky dataset. Finally, it's also worth noting that we checked how well Proposition 4.3 is satisfied with the additional training loss to encourage it. The average \(z\) reconstruction \(L2\) distance is \(0.304\). Comparing to the standard deviation of \(z\) over the dataset being \(4.043\), we conclude that this property is approximately met.

**Discussion.** It is worth noting that the model by Winter et al.  is specifically tailored for group actions on the data space \(\), which accounts for the performance discrepancy on the rotated and blocked MNIST, where the block does not rotate with the rest of the image. Additionally, even the rotated MNIST is not precisely a group action on \(\), given the interpolations occurring during rotation. Their original experiment utilized blurred rotated MNIST, different from our dataset, as a measure to mitigate this problem. As for Hwang et al. , it models the group as a Euclidean space \(^{d}\) under addition in the latent space. We can see that their model especially struggles on the rotation groups, \((2)\), which are not Euclidean spaces. Furthermore, the normalizing flow architecture used for encoder/decoder enforces these mappings to be diffeomorphisms, resulting in a group action on \(\), which accounts for its poor performance on the blocked MNIST.

Dupont et al.  incorporates group equivariance in the latent space by transforming the latent coordinates. Although this exactly models novel view synthesis task, it is too rigid for more general scenarios, as in the plane in the sky dataset. Furthermore, we can see that it is not very good at capturing details from the results on NMR dataset. However, the introduction of equivariance might have helped modeling more difficult 3D rotations of the planes, giving it better performance compared to Sajjadi et al. . On the other hand, Sajjadi et al.  is a transformer type of model that does not incorporate group equivariance. It is better at capturing details and rendering sharp images as seen in results on NMR dataset. However, it fails on the plane dataset, which is a more challenging task than most novel view synthesis datasets, given full 3D random rotations and single input view.

### Ablation study

We also performed an ablation study on the NMR dataset to explore how skip connections paired with attention modules and the addition of LPIPS loss impact our model performance. We define the following three ablation models: Ablation 1 - our model without the skip connection and trained without the LPIPS loss; Ablation 2 - our model without the skip connection; Ablation 3 - our model trained without the LPIPS loss. The results are listed in Table 3 and some samples are shown in Figure 6. Combining skip connections with LPIPS loss yields the best performance across all metrics,

    &  &  \\   & \(\)PSNR & \(\)SSIM & \(\)LPIPS & \(\)PSNR & \(\)SSIM & \(\)LPIPS \\  Dupont et al.  & 26.91 & 0.899 & 0.091 & 24.25 & 0.773 & 0.239 \\ Sajjadi et al.  & 27.87 & 0.912 & 0.066 & 23.53 & 0.489 & 0.280 \\
**Ours** & **28.91** & **0.947** & **0.050** & **25.24** & **0.821** & **0.112** \\   

Table 2: Quantitative results on 3D objects rendered datasets

    & \(\)PSNR & \(\)SSIM & \(\)LPIPS \\  Ablation 1 & 27.31 & 0.933 & 0.080 \\ Ablation 2 & 26.96 & 0.930 & 0.063 \\ Ablation 3 & 28.09 & 0.941 & 0.072 \\
**Ours** (full) & **28.91** & **0.947** & **0.050** \\   

Table 3: Quantitative results of ablation study on NMR datasetsignificantly enhancing image details. Skip connections alone provide a performance boost, but adding LPIPS loss without skip connections offers minimal improvement. Notably, even without skip connections or LPIPS, our model accurately captures object orientation and performs comparably to baseline models, highlighting the robustness of our latent group action modeling strategy.

We further investigate how varying and invariant information is captured by swapping the \(z_{v}\) and \(z_{i}\) components of the latent representations between two inputs and decoding the results. Some examples are shown in Figure 7. Interestingly, the model generalizes to new combinations of varying factors (e.g., digit and plane shapes) and invariant factors (e.g., the white block and the sky), despite not being explicitly trained in such fashion.

## 7 Conclusion

In this paper, we propose a novel approach of learning group actions on latent representations. Our experimental results demonstrate that our method can effectively model a broader range of scenarios than existing models of group actions on the data space. In addition to being able to model group actions in the latent space, we show both theoretically and empirically that our strategy is also capable of modeling group actions on the data space. Furthermore, we achieve state of the art performance on the geometry-free novel view synthesis task, and we outperform previous approaches to learning group actions in more general cases. We note that our model requires ground truth group actions during training, which might not be available in some cases. We leave it as future work to apply our method in semi-supervised or unsupervised manners.

Figure 6: Samples from ablation models and our full model.

Figure 7: Samples generated by swapping invariant and varying parts of latent representations.