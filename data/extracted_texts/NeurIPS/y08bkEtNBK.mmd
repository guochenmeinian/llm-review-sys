# WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting

WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting

 Yuxin Jia   Youfang Lin   Xinyan Hao   Yan Lin   Shengnan Guo   Huaiyu Wan

School of Computer and Information Technology, Beijing Jiaotong University, China

Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing, China

{yuxinjia, yflin, xinyanhao, ylincs, guoshn, hywan}@bjtu.edu.cn

Corresponding author

###### Abstract

Capturing semantic information is crucial for accurate long-range time series forecasting, which involves modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. Previous works have partially addressed these issues separately, but have not been able to address all of them simultaneously. Meanwhile, their time and memory complexities are still not sufficiently low for long-range forecasting. To address the challenge of capturing different types of semantic information, we propose a novel Water-wave Information Transmission (WIT) framework. This framework captures both long- and short-term repetitive patterns through bi-granular information transmission. It also models global and local correlations by recursively fusing and selecting information using Horizontal Vertical Gated Selective Unit (HVGSU). In addition, to improve the computing efficiency, we propose a generic Recurrent Acceleration Network (RAN) which reduces the time complexity to \(()\) while maintaining the memory complexity at \((L)\). Our proposed method, called Water-wave Information Transmission and Recurrent Acceleration Network (WITRAN), outperforms the state-of-the-art methods by 5.80% and 14.28% on long-range and ultra-long-range time series forecasting tasks respectively, as demonstrated by experiments on four benchmark datasets. The code is available at: https://github.com/Water2sea/WITRAN.

## 1 Introduction

Time series forecasting is a valuable tool across diverse fields, such as energy, traffic, weather and so on. Compared with short-range time series forecasting, long-range forecasting offers the advantage of providing individuals with ample time to prepare and make informed decisions. For instance, accurate long-range traffic and weather forecasts enable individuals to plan their travel arrangements and attire accordingly. To improve the accuracy of such forecasting, it is essential to utilize longer historical sequences as input for the forecasting models (Liu et al., 2021; Zeng et al., 2023).

Previous studies (Wu et al., 2023; Nie et al., 2023) have highlighted the importance of capturing semantic information in long-range time series to achieve accurate predictions. However, the semantic information in long-range time series is diverse, so how to analyze and capture it becomes a major challenge. Specifically, semantic information includes two main aspects: (1) Global and local correlations. The local semantic information usually contains short-range changes within the data, while the global semantic information reflects the long-range trends present in the time series (Wang et al., 2023), which can be referred to as global-local semantic information. (2) Long- and short-term repetitive patterns. Time series often exhibit repetitive patterns at different timescales (Laiet al., 2018; Liu et al., 2021), such as hourly or daily cycles, which can be identified as periodic semantic information. Furthermore, it is crucial to model both aspects of semantic information simultaneously and ensure that the modeling approach remains computationally efficient, avoiding excessive complexity.

Unfortunately, although the existing state-of-the-art methods have shown great performance, they encounter difficulties in addressing the aforementioned challenges simultaneously. Specifically, **CNN-based methods**(Bai et al., 2018; Franceschi et al., 2019; Sen et al., 2019; Wu et al., 2023; Wang et al., 2023) have linear complexity with respect to the sequence length \(L\), but they are either limited by the size of the convolution receptive field (Wang et al., 2023) or constrained by the 1D input sequence (Wu et al., 2023), making it difficult to capture both important semantic information simultaneously. **Transformer-based methods** can be broadly classified into two categories based on whether the point-wise attention is used or not. The used ones (Vaswani et al., 2017; Zhou et al., 2021; Zhou et al., 2022a) capture correlations between points in the sequence, yet face challenges in capturing hidden semantic information directly from point-wise input tokens (Nie et al., 2023; Wu et al., 2023). The others (Li et al., 2019; Wu et al., 2021; Liu et al., 2021) struggles either with achieving sufficiently low computational complexity or with effectively capturing periodic semantic information. **Other methods**(Zhou et al., 2022b; Zeng et al., 2023) also exhibit limitations in capturing semantic information mentioned above. Further details can be found in Section 2.

**RNN-based methods**(Hochreiter and Schmidhuber, 1997; Chung et al., 2014; Rangapuram et al., 2018; Salinas et al., 2020) have significant advantages in capturing global and local semantic information through their recurrent structure, as shown in Figure 1(a), while maintaining linear complexity. However, they suffer from gradient vanishing/exploding (Pascanu et al., 2013) and information forgetting issues (refer to Appendix B for more details), making them less suitable for direct application in long-range forecasting tasks. Fortunately, it has been proven that by splitting the information transmissions into patches between a few adjacent time steps and processing them individually, it is possible to maintain both global and local semantic information (Nie et al., 2023; Wu et al., 2023). This insight is highly inspiring, as it suggests that by dividing the inputs of RNNs into numerous subseries and processing them separately, we can effectively address the significant limitation mentioned earlier, without compromising the efficiency of long-range forecasting.

Figure 1: Information transmission process diagram of different forecasting models.

Based on the aforementioned insights, we propose a novel framework called **W**ater-wave **I**nformation **T**ransmission and **R**ecurrent **A**cceleration **N**etwork (**WITRAN**) which comprises two key components: The **W**ater-wave **I**nformation **T**ransmission (**WIT**) framework and the **R**ecurrent **A**cceleration **N**etwork (**RAN**). The overall information transmission process of WITRAN is illustrated in Figure 1(i). Firstly, to capture the periodic semantic information of long- and short-term, we rearrange the input sequences according to their natural period, as shown in Figure 2(a). This rearrangement allows for information transmission in two directions, resembling the propagation of water-wave energy, as shown in Figure 2(b). The horizontal red arrows indicate the intra-periodic information transmission along the time steps, while the vertical blue arrows indicate the inter-periodic information transmission. Secondly, to preserve the characteristics of long- and short-term periodic semantic information, we propose a novel **I**nforizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which incorporates **G**ated **S**elective **C**ells (**GSC**s) separately in both directions. To capture the correlation between periodic semantic information of both directions at each time step, we design fusion and selection operations in GSC. With a recurrent structure, HVGSU progressively captures more local semantic information until it encompasses the global semantic information. Thirdly, to improve efficiency, we propose the Recurrent Acceleration Network (RAN), which enables parallel processing of information transmission in both directions. Notably, RAN maintains a memory complexity of \((L)\) while reducing the time complexity to \(()\). In addition, RAN could serve as a universal framework for integrating other models to facilitate information fusion and transmission. In summary, our main contributions are as follows:

* We propose a **W**ater-wave **I**nformation **T**ransmission and **R**ecurrent **A**cceleration **N**etwork (**WITRAN**), which represents a novel paradigm in information transmission by enabling bi-granular flows. We provide a comprehensive comparison of WITRAN with previous methods in Figure 1 to highlight its uniqueness. Furthermore, in order to compare the differences between WITRAN and the model (a)-(h) in Figure 1 more clearly, we have prepared Table 1 to highlight the advantages of WITRAN.
* We propose a novel **I**nforizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which captures long- and short-term periodic semantic information by using **G**ated **S**elective **C**ell (**GSC**) independently in both directions, preserving the characteristics of periodic semantic information. The fusion and selection in GSC can model the correlations of long- and short-term periodic semantic information. Furthermore, utilizing a recurrent structure with HVGSU facilitates the gradual capture of semantic information from local to global within a sequence.
* We present a **R**ecurrent **A**cceleration **N**etwork (**RAN**) which is a generic acceleration framework that significantly reduces the time complexity to \(()\) while maintaining the memory complexity of \((L)\). We summarize the complexities of different methods in Table 2, demonstrating the superior efficiency of our method.
* We conduct extensive experiments on four benchmark datasets across various fields (energy, traffic and weather). The empirical studies demonstrate the remarkable performance of WITRAN, which achieves relative improvements of \(5.80\%\) and \(14.28\%\) in long-range and ultra-long-range forecasting respectively. In addition, the introduction of the generic RAN framework greatly improves the computational efficiency.

   Advantages & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) & (\(\)D Rank) \\  Non-price semantic information capture & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\  Special design \(\) capture long-term prediction patterns & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ \\  Enhanced (\(\) + \(2\) largest model global consensus) & ✓(1) & ✗ & ✓(1) & ✗ & ✗ & ✓(2) & ✓(2) & ✗ & ✓(1) \\  WiI study the gradient using/residuals prediction patterns \(\)D Rank & ✗ & - & - & - & - & - & - & - & ✓ \\   

Table 1: Advantages of **WITRAN** compared to other methods.

   Methods & RNN & CNN & Transformer & LogTrans & Transformer & Autoformer & Transformer & FEDformer & FEDformer & FEDformer & FEDformer & FEDM & **F**nIo**TST & **MCN** & **WITRAN (ours)** \\  Time & \((L)\) & \((L)\) & \((L^{2})\) & \((L L)\) & \((L L)\) & \((L L)\) & \((L)\) & \((L)\) & \((L)\) & \((L/S)^{3}\) & \((L)\) & \(()\) \\  Memory & \((L)\) & \((L)\) & \((L)\) & \((L^{2})\) & \((L

## 2 Related Work

Time series forecasting methods can be broadly categorized into statistical methods and neural network methods. Neural network methods, in turn, can be further classified into several subcategories based on the specific techniques employed, including RNN-based, CNN-based, Transformer-based, and other Deep Neural Networks (DNNs) by the methods used.

**Statistical Methods** with major representatives include ARIMA (Box and Jenkins, 1968), Prophet (Taylor and Letham, 2018), and Holt Winter (Athanasopoulos and Hyndman, 2020). However, the inherent variability and complexity of real-world time series often render these methods inadequate, as they are built on hypotheses that may not align well with the characteristics of such data. As a result, the performance of these statistical methods tends to be limited.

**RNN-based Methods**(Hochreiter and Schmidhuber, 1997; Chung et al., 2014; Rangapuram et al., 2018; Salinas et al., 2020) possess the inherent advantage of capturing semantic information in time series data through their recurrent structure, which mimics the natural information transmission process of time series. This allows them to progressively capture semantic information from local to global contexts. These methods exhibit linear time and memory complexity, enabling efficient processing of time series data with a sequence length of \(L\). However, when dealing with longer sequences, the problems of gradient vanishing/exploding (Pascanu et al., 2013) and information forgetting (see Appendix B for more details) further limit it.

**CNN-based methods**(Bai et al., 2018; Franceschi et al., 2019; Sen et al., 2019; Wu et al., 2023; Wang et al., 2023) are adept at capturing local semantic information through the application of convolutional kernels, while maintaining linear time and memory complexity. However, on the one hand, most of these methods face challenges in capturing comprehensive global information due to the limited receptive field of individual convolutional layers, which make the training process more difficult and overhead (Wang et al., 2023). On the other hand, it is difficult to directly capture long- and short-term repetitive patterns on 1D inputs. MICN (Wang et al., 2023) adopted downsampling 1D convolutions and isometric convolutions combined with a multi-branch framework, which can effectively solve the former problem, but still suffers from the latter one. TimesNet (Wu et al., 2023) transforms 1D inputs into 2D space, leveraging a parameter-efficient inception block to capture intra- and inter-period variations. However, it solves the latter problem while still facing the former one.

**Transformer-based methods** have shown advancements in time series forecasting, with two main categories. The first category uses point-wise attention mechanisms, such as Vanilla Transformer (Vaswani et al., 2017), Informer (Zhou et al., 2021), and FEDformer (Zhou et al., 2022), while facing challenges in extracting sufficient semantic information from individual time points. The second category employs non point-wise dot product techniques, including LogTrans (Li et al., 2019), Autoformer (Wu et al., 2021), Pyraformer (Liu et al., 2021) and PatchTST (Nie et al., 2023). They reduce computational complexities to a certain degree, yet face difficulties in incorporating the long-term correlations in time-series. For a detailed description of these methods, please refer to Appendix A. Furthermore, it is worth noting that the majority of the aforementioned methods fail to achieve lower complexity than RNN-based methods. For a comprehensive comparison of the theoretical time and memory complexity, as well as experimental results, please refer to Appendix C.

**Other DNN methods** have also demonstrated promising performance. For example, FiLM (Zhou et al., 2022) utilizes Legendre Polynomials and Fourier projection methods to capture historical in

Figure 2: Input rearrangement and water-wave information transmission.

formation, eliminate noise, and expedite computations using low-rank approximation. DLinear (Zeng et al., 2023) has showcased impressive outcomes by employing simple linear operations. Nevertheless, both approaches encounter difficulties in capturing various repeating patterns present in the sequence.

## 3 The WITRAN Model

The time series forecasting task involves predicting future values \(Y^{P c_{ out}}\) for \(P\) time steps based on the historical input sequences \(X=\{x_{1},x_{2},,x_{H}\}^{H c_{ in}}\) of \(H\) time steps, where \(c_{ in}\) and \(c_{ out}\) represent the number of input and output features respectively. In order to integrate enough historical information for analysis, it is necessary for \(H\) to have a sufficient size (Liu et al., 2021; Zeng et al., 2023). Furthermore, capturing semantic information from the historical input is crucial for accurate forecasting, which includes modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. However, how to address them simultaneously is a major challenge. With these in mind, we propose WITRAN, a novel information transmission framework akin to the propagation of water waves. WITRAN captures both long- and short-term periodic semantic information, as well as global-local semantic information simultaneously during information transmission. Moreover, WITRAN reduces time complexity while maintaining linear memory complexity. The overall structure of WITRAN is depicted in Figure 3.

### Input Module

To facilitate the analysis of long- and short-term repetitive patterns, inspired by TimesNet (Wu et al., 2023), we first rearrange the sequence from 1D to 2D based on its natural period, as illustrated by Figure 2(a). Importantly, our approach involves analyzing the natural period of time series and setting appropriate hyperparameters to determine the input rearrangement, rather than using Fast Fourier Transform (FFT) to learn multiple adaptive periods of inputs in TimesNet. Consequently, our method is much simpler. Additionally, in order to minimize the distribution shift in datasets, we draw inspiration from NLinear (Zeng et al., 2023) and employ an adaptive learning approach to determine whether to perform simple normalization. The input module can be described as follows:

\[ X_{ 1D}&=X&,norm=0\\ X-x_{H}&,norm=1\\ X_{ 2D}&=([X_{ 1D},TF_{ en}]),\] (1)

here, \(X_{ 1D}^{H c_{ in}}\) represents the original input sequences, \(x_{H}^{c_{ in}}\) represents the input at the last time step of the original sequence, \(TF_{ en}^{H c_{ time}}\) represents the temporal contextual features of original input sequence (e.g., HourOfDay, DayOfWeek, DayOfMonth and DayOfYear), where \(c_{ time}\) is the dimension of time features. \(X_{ 2D}^{R C(c_{ in}+c_{ time})}\) represents the inputs after rearrangement, where \(R\) denotes the total number of horizontal rows and \(C\) denotes the vertical columns. \(norm\) is an adaptive parameter for different tasks. \([]\) represents the concat operation and \(\) represents the rearrange operation, with reference to Figure 2(a).

Figure 3: Overall structure of WITRAN.

### Horizontal Vertical Gated Selective Unit

To capture long- and short-term periodic semantic information and reserve their characteristics, we propose a novel **H**orizontal **V**ertical **G**ated **S**elective **U**nit (**HVGSU**) which consists of **G**ated **S**elective **C**ells (**GSCs**) in two directions. To capture the correlation at each time step between periodic semantic information of both directions, we design the specific operations in GSC. Furthermore, HVGSU is capable of capturing both global and local information via a recurrent structure. In this subsection, we will provide a detailed introduction to them.

HvguAs depicted in Figure 3, the process of HVGSU via a recurrent structure is:

\[H_{ hor},H_{ ver},Out=(X_{ 2D}),\] (2)

where \(H_{ hor}^{L R d_{ model}}\) and \(H_{ ver}^{L C d_{ model}}\) represent the horizontal and the vertical output hidden state of HVGSU separately. \(L\) is the depth of the model, and \(Out^{R C d_{ model}}\) denotes the output information of the last layer.

In greater detail, the cellular structure of HVGSU is shown in Figure 4(b), which consists of GSCs in two directions to capture the periodic semantic information of long- and short-term. The cell operations for row \(r\)\((1 r R)\) and column \(c\)\((1 c C)\) in layer \(l\)\((1 l L)\) can be formalized as:

\[h_{r,\;c,\;l}^{ hor}=_{ hor}(input_{r,\;c, \;l},\;h_{r,\;c,\;l,\;l}^{ hor},\;h_{r-1,\;c,\;l}^{ ver},\;h_{r,\;c,\;l }^{ ver})\\ h_{r,\;c,\;l}^{ ver}=_{ ver}(input_{r,\;c,\;l,\;h_{r-1,\;c,\;l }^{ ver},\;h_{r,\;c-1,\;l}^{ hor})}\\ o_{r,\;c,\;l}=[h_{r,\;c,\;l}^{ hor},\;h_{r,\;c,\;l}^{ ver}],\] (3)

here, \(input_{r,\;c,\;l}^{d_{ in}}\). When \(l=1,\;input_{r,\;c,\;l}=x_{r,\;c}^{c_{ in}+c_{ time}}\) represents the input for the first layer, and when \(l>1,\;input_{r,\;c,\;l}=o_{r,\;c,\;l-1}^{2 d_{ model}}\) represents the input for subsequent layers. \(h_{r,\;c-1,\;l}^{ hor}\) and \(h_{r-1,\;c,\;l}^{ ver}^{d_{ model}}\) represent the horizontal and vertical hidden state inputs of the current cell. Note that when \(r=1,\;h_{r-1,\;c,\;l}^{ ver}\) is replaced by an all-zero tensor of the same size, and the same is true for \(h_{r,\;c-1,\;l}^{ hor}\) when \(c=1\). \([]\) represents the concatenation operation and \(o_{r,\;c,\;l}^{2 d_{ model}}\) represents the output of the current cell.

GscInspired by the two popular RNN-based models, LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Chung et al., 2014] (for more details, see Appendix A), we propose a Gated Selective Cell (GSC) to fuse and select information. Its structure is shown in Figure 4(a), which comprises two gates: the selection gate, and the output gate. The fused information consists of input and principal-subordinate hidden states, and the selection gate determines the retention of the original principal information and the addition of the fused information. Finally, the output gate determines the final output information of the cell. The different colored arrows in Figure 4(a) represent different semantic information transfer processes. The black arrow represents the input, the red arrows represent the process of transmitting principal hidden state information, the blue arrow represents the subordinate hidden state, and the purple arrows represent the process by which fused information of

Figure 4: The structure of our method.

principal-subordinate hidden states is transmitted. The formulations are given as follows:

\[ S_{t}&=(W_{s}[h_{t-1}^{ },\;h_{t-1}^{},\;x]+b_{s})\\ O_{t}&=(W_{o}[h_{t-1}^{},\;h_{t-1}^ {},\;x]+b_{o})\\ h_{f}&=(W_{f}[h_{t-1}^{},\;h_{t- 1}^{},\;x]+b_{f})\\ ^{}}&=(1-S_{t})  h_{t-1}^{}+S_{t} h_{f}\\ h_{t}^{}&=(^{}}) O_{t},\] (4)

where \(h_{t-1}^{}\) and \(h_{t-1}^{}^{d_{}}\) represent the input principal and subordinate hidden state, \(x^{d_{in}}\) represents the input. \(W_{*}^{d_{}(2d_{+d_{in}})}\) are weight matrices and \(b_{*}^{d_{}}\) are bias vectors. \(S_{t}\) and \(O_{t}\) denote the selection gate and the output gate, \(\) denotes an element-wise product, \(()\) and \(()\) denote the sigmoid and tanh activation function. \(h_{f}\) and \(^{}}^{d_{}}\) represent the intermediate variables of the calculation. \(h_{t}^{}\) represents the output hidden state.

### Recurrent Acceleration Network

In traditional recurrent structure, for two adjacent time steps in series, the latter one always waits for the former one until the information computation of the former one is completed. When the sequence becomes longer, this becomes slower. Fortunately, in the WIT framework we designed, some of the time step information can be computed in parallel. As shown in Figure 2(b), after a point is calculated, the right point in its horizontal direction and the point below it in its vertical direction can start calculation without waiting for each other. Therefore, we propose the Recurrent Acceleration Network (RAN) as our accelerated framework, which enables parallel computation of data points without waiting for each other, greatly improving the efficiency of information transmission in HVGSU. We place parallelizable points in a slice, and the updated information transfer process is shown in Figure 4(c). Each green box in Figure 4(c) represents a slice, and the number of green boxes is the number of times we need to recurrently compute. The meanings of the remaining markers are the same as those in Figure 2. Under the RAN framework, the recurrent length has changed from the sequence length \(L=R C\) to \(R+C-1\), while the complexity of \(R\) and \(C\) is \(()\). Thus, we have reduced the time complexity to \(()\) via the RAN framework. It should be noted that although we parallelly compute some data points, which may increase some memory, the complexity of parallel computation, \(()\), is far less than the complexity of saving the output variables, \((L)\), because we need to save the output information of each point in the sequence. Implementation source code for RAN is given in Appendix D.

### Forecasting Module

In the forecasting module, we address the issue of error accumulation in the auto-regressive structure by drawing inspiration from Informer (Zhou et al., 2021) and Pyraformer (Liu et al., 2021), combining both horizontal and vertical hidden states, and then making predictions through a fully connected layer, as illustrated in Figure 3.

We utilize the last row of the horizontal hidden states as it contains sufficient global and latest short-term semantic information from the historical sequence. In contrast, all columns of the vertical hidden states, which capture different long-term semantic information, are all preserved. The combined operation not only maximizes the retention of the various semantic information needed for predicting the points, but also avoids excessive redundancy in order to obtain accurate predictions. This module can be formalized as follows:

\[ H_{}^{}&=(h_{})\\ H_{}&=([H_{}^{ },H_{}])\\ &=(H_{})\\ Y&=(()+ TFE_{ }),\] (5)

where \( TFE_{}^{P d_{}}\) represents time features encoding of the forecasting points separately. \(h_{}^{L 1 d_{}}\) represents the last row hidden state in \(H_{}\). Repeat\(()\) is for repeat operation and \(H_{rep}^{ hor}^{L C d_{ model}}\) represents the hidden state after the repeat operation. \(H_{ h-v}^{C(L*2d_{ model})}\) is the vector of horizontal and vertical combination after reshape operation. FC1 and FC2 represent two fully connected layers respectively. \(^{C(R_{ de}*d_{ model})}\) represents the intermediate variables of the calculation and \(R_{ de} C=P\). \(Y\) represents the output of this module, and it is worth noting that we need to utilize the adaptive parameter \(norm\) for denormalization, when \(norm=1,Y=Y+x_{H}\).

## 4 Experiment

DatasetsTo evaluate the proposed WITRAN, we conducted extensive experiments on four widely recognized real-world benchmark datasets for long-range and ultra-long-range forecasting. These datasets cover energy, traffic, and weather domains. We split all datasets into training, validation and test set in chronological order by the ratio of 6:2:2. More details about the datasets and implementation are described in Appendix E and Appendix F.

BaselinesIn light of the underperformance produced by classical models such as ARIMA and simple RNN/CNN models, as evidenced by (Zhou et al., 2021) and (Wu et al., 2021), and the subpar performance exhibited by certain Transformer-based models like LogTrans (Li et al., 2019) and Reformer (Kitaev et al., 2020), as shown in (Wu et al., 2023) and (Wang et al., 2023), our study primarily incorporates six transformer-based models: PatchTST (Nie et al., 2023), FEDformer (Zhou et al., 2022), Pyraformer (Liu et al., 2021), Autoformer(Zhou et al., 2022), Informer (Zhou et al., 2021), Vanilla Transformer (Vaswani et al., 2017), and four non-transformer-based methods: MICN (Wang et al., 2023), TimesNet (Wu et al., 2023), DLinear (Zhou et al., 2022), FiLM (Zhou et al., 2022). Please refer to Appendix A and Appendix H for more details of the baselines and implementation.

### Experimental Results

For a better comparison, we adopted the experimental setup of Pyraformer (Liu et al., 2021) for long-range and ultra-long-range series forecasting. In addition, channel-independence is crucial for multivariate time series prediction (Nie et al., 2023), so it is necessary to verify the performance of models on a single channel to ensure their effectiveness across all channels in multivariate time series prediction. In this paper, experiments were conducted on a single channel. Note that in order to fairly compare the performance of each model, we set up the search space so that each model can perform optimally on each task. For further details, please refer to Appendix F.

Long-range Forecasting ResultsWe conducted five tasks on each dataset for long-range prediction, and the results are shown in Table 3. Taking the task setting 168-336 on the left side of the Table 3 as an example, it indicates that the input length is 168 and the prediction length is 336. Notably, our proposed WITRAN achieves state-of-the-art performance, surpassing the previous best method with an average MSE reduction of 5.803%. Specifically, WITRAN exhibits an average MSE reduction of 10.246% for ECL, 3.879% for traffic, 2.519% for ETTh1, 4.431% for ETTh2, and 7.939% for weather. Additionally, we note that the competition among the baselines is intense due to their best performance on each task, but none of them consistently performs well across all tasks. In contrast, WITRAN demonstrates its robust competitiveness across various tasks and datasets. In addition, we can find that in most cases, a longer input length yields greater improvement with the same prediction length. All findings above collectively highlight WITRAN's efficacy in addressing diverse time-series forecasting tasks in real-world applications. Further results and showcases are presented in Appendix H and Appendix J.

Ultra-long-range Forecasting ResultsWe also show the three tasks for ultra-long-range prediction results on each dataset in Table 4. The tasks on the left side of the table hold the same interpretation as above. Notably, WITRAN achieves an 14.275% averaged MSE reduction. More specifically, WITRAN demonstrates an average MSE reduction of 39.916% for ECL, 3.122% for traffic, 14.837% for ETTh1, 2.441% for ETTh2, and 11.062% for weather. In particular, our method showcases substantial improvements of over 10% in ECL, ETTh1, and weather, reinforcing our ability to predict ultra-long-range time series. And by comparing the results of ultra-long-range forecasting and long-range Forecasting, we can find that our method is more competitive for ultra-long-range prediction,

[MISSING_PAGE_FAIL:9]

can be found in Appendix G. Here, we provide a concise summary of the key findings: 1) The fusion and selection design of GSC enables both long- and short-term semantic information to be captured for each historical input data points. 2) Setting up independent cells in both directions enables to extract semantic information in the long- and short-term respectively. 3) The specially designed combined operation can make fuller use of the captured local and global semantic information while ensuring that the information is not redundant. 4) RAN offers advantages in terms of speed and space complexity. The utilization of RAN eliminates the need to store excessive intermediate variables, as shown in the previous section.

### Robustness Analysis

We have followed MICN (Wang et al., 2023) and introduced a simple white noise injection to demonstrate the robustness of our model. Specifically, we randomly select a proportion \(\) of data from the original input sequence and apply random perturbations within the range \([-2X_{i},2X_{i}]\) to the selected data, where \(X_{i}\) denotes the original data. After the noise injection, the data is then used for training, and the MSE and MAE metrics are recorded in Table 5.

It can be found that as the perturbation proportion increases, there is a slight increase in the MSE and MAE metrics in terms of forecasting. It indicates that WITRAN demonstrates good robustness when dealing with less noisy data (up to 10\(\%\)), and it possesses a significant advantage in effectively handling various abnormal data fluctuations.

## 5 Conclusions

In this paper, we propose WITRAN, a novel Water-wave Information Transmission framework and a universal acceleration framework. WITRAN effectively captures both long- and short-term repetitive patterns and global and local correlations with \(()\) time complexity and \((L)\) memory complexity. The experimental results demonstrate the remarkable performance and efficiency of WITRAN. However, this recurrent structure is still not optimally efficient for Python-based implementations because of the information waiting between slices. Therefore, we plan to explore the integration of WITRAN into an interface using C++, similar to the implementation of nn.GRU/nn.LSTM in PyTorch in the future, to further improve its efficiency.