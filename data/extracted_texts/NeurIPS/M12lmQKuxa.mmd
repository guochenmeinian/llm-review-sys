# Multiple Physics Pretraining

for Physical Surrogate Models

 Michael McCabe

Contact: mmccabe@flatironinstitute.org

Bruno Regaldo-Saint Blancard

Iciam Parker

Ruben Ohana

Miles Cranmer

Alberto Bietti

Michael Eickenberg

Siavash Golkar

Geraud Krawezik

Francois Lanusse

Mariel Pettee

Tiberiu Tesileanu

Kyunghyun Cho

Shirley Ho

###### Abstract

We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from scratch or finetuning pretrained video foundation models.

+
Footnote †: Code: https://github.com/PolymathicAI/multiple_physics_pretraining

+
Footnote †: Code: https://github.com/PolymathicAI/multiple_physics_pretraining

## 1 Introduction

In recent years, the fields of natural language processing and computer vision have been revolutionized by the success of large models pretrained with task-agnostic objectives on massive, diverse datasets . This has, in part, been driven by the use of self-supervised pretraining methods which allow models to utilize far more training data than would be accessible with supervised training . These so-called "foundation models" have enabled transfer learning on entirely new scales. Despite their task-agnostic pretraining, the features they extract have been leveraged as a basis for task-specific finetuning, outperforming supervised training alone across numerous problems especially for transfer to settings that are insufficiently data-rich to train large models from scratch .

Deep learning for computational science has begun to see first steps in this direction. Large domain-specific pretrained models have emerged in diverse fields such as chemistry [6; 7], medicine [8; 9], astrophysics [10; 11], and climate  and the trend only seems to be growing as more and more models are developed for new fields both as refined versions of existing large language models and as new models trained entirely on field-specific data.

In this work, we demonstrate that similar approaches can be extended to the surrogate modeling of spatiotemporal physical systems. Spatiotemporal prediction tasks, like those found in fluids, solids, or general continuum mechanics, have attracted significant attention from the deep learning community. From direct prediction methods [13; 14; 15; 16; 17] to neural PDE solvers [18; 19], researchers have sought to develop fast, accurate models for physics either as faster surrogates for the partial differential equation (PDE) solvers that dominate the field or to simulate systems that cannot be exactly described or resolved by current mechanistic models and available hardware. While directly outperforming PDE solvers is difficult , deep learning has already begun to impact fields like atmospheric science [21; 22; 23] and cosmology [24; 25; 26], where the systems are too large or too imprecisely described to be simulated exactly.

Unfortunately, outside of a few observation-rich outliers, settings where numerical simulation is expensive or unreliable also tend to be settings where the difficulty of acquiring training data makes it impractical to train surrogates conventionally. Most deep learning-based surrogates thus far have focused on specific problems or individual families of parameterized PDEs. However, for these low-data settings, it would be valuable to have large, task-agnostic models with a broad understanding of common physical behavior to act as a foundation for finetuning.

Contributions. We introduce Multiple Physics Pretraining (MPP), a new approach for task-agnostic pretraining of physical surrogate models. Our method enables large-scale pretraining for transfer across diverse physics which we study using fluid-oriented benchmarks. Our specific contributions are:

* We develop MPP, a pretraining approach in which we embed multiple hetereogeneous physical systems into a shared embedding space and learn to autoregressively predict the dynamics of all systems simultaneously.
* We show that single transformer models pretrained with MPP are able to match or surpass modern baselines trained only on specific pretraining sub-tasks without applying task-specific finetuning to the MPP models.
* We demonstrate the transfer capabilities of models trained with MPP on systems with limited training examples (referred to as low-data systems thereafter).
* We open-source our code and provide our pretrained models at a variety of sizes for the community to experiment with on their own tasks.

## 2 Background

Notation.Let \(S\) be an arbitrary physics-driven spatiotemporal dynamical systems, either described by a parameterized family of PDEs with fixed parameters, or where snapshots are gathered from observation of a unique physical phenomenon. To simplify notation, we discuss systems with a single state variable in one spatial dimension. A continuous state variable for system \(S\) is represented as \(u^{S}(x,t):[0,L_{S}][0,)\). We discretize the system uniformly in space and time at resolutions \(N_{S}\), \(T_{S}\) respectively. A snapshot \(_{t}^{S}^{N_{S}}\) represents the value of state variable \(u^{S}\) at all \(N_{S}\) spatial discretization points at time \(t\). Our pretraining task is then to learn a single model \(\) that can take a uniformly spaced sequence of \(T_{S}\) snapshots \(_{t}^{S}=[_{t-T_{s} t_{S}}^{S},,_{t}^{S}]\) from system \(S\) sampled from some distribution over systems and predict \((_{t}^{S})\) such that \((_{t}^{S})_{t+ t_{S}}^{S}\).

Autoregressive Pretraining.In vision and language, the dominant pretraining strategies include autoregressive prediction , masked reconstruction [2; 3], and contrastive learning . In language, autoregressive generation emerged as a convenient self-supervised task. In surrogate modeling of dynamical systems, next-step prediction is often a primary goal. Thismakes autoregressive pretraining a natural choice of objective for training time-dependent surrogate models.

We note that it is common to use the simulation parameters to condition the predictions of models operating on PDE-generated data . In MPP, the model must instead implicitly infer the impact of these parameters on the dynamics from the history provided in \(_{t}^{S}\).

Surrogate Modeling for Spatiotemporal Physical Systems.We are primarily concerned with modeling dynamical systems varying in both time and space, where the time evolution of the system is intrinsically tied to spatial relationships amongst the state variables according to physical laws. Partial differential equations (PDEs) are one of the primary modeling tools for this setting. They are often derived from fundamental conservation laws of properties such as mass, momentum, and energy . Many PDEs describe variations of the same physical laws, which is why concepts like diffusion, advection, reactivity, and connections between time and spatial gradients appear in many different PDEs. These shared underlying principles suggest we can extract features relevant to multiple physical systems.

## 3 Related Work

Foundation models.Massive pretrained models dubbed "foundation models" , particularly large transformer-based architectures , have recently attracted significant attention. The most prevalent foundation models are pretrained language models like GPT  and BERT . Emergent abilities  demonstrated by large language models highlight the importance of scale in manifesting higher-order capabilities absent at smaller scales. Vision has seen similar developments with the growth of masked  and contrastive  pretraining. The data in this work is insufficiently diverse to call the resulting models "foundational". However, we provide the first large-scale implementation of successful multiple nonlinear physics pretraining for spatiotemporal systems.

Scientific transfer learning.The high cost of training scientific models from scratch has led to significant exploration of transfer learning. Prior work has explored transfer learning in operator networks in such scenarios as conditional shift  or new domains, boundary conditions, or distributions over parameters . However, these too need to be retrained from scratch for new differential operators in the PDE. More recently, efforts have been made to explore transfer across operators and benefits from training on multiple physical systems simultaneously.  in particular explores how transfer scales in this setting. However, their study is limited to steady-state linear systems with periodic boundary conditions. Other works have explored similarly restricted classes or low dimensional, low resolution systems .

## 4 Scalable Multiple Physics Pretraining

### Compositionality and Pretraining

Many specialized PDEs demonstrate a form of compositionality, as a range of physical phenomena can be described by core components like nonlinear advection or diffusion, but then are augmented or restricted by specialized terms representing concepts like buoyancy or system constraints. To motivate a useful pretraining procedure from this compositionality, we want to show two things:

1. Learning partially overlapping physics is beneficial for transfer learning
2. Single models can simultaneously learn many types of physics

If both of these are true, then we could train a single model which could transfer effectively to many types of physics. We start by examining the first assertion in a very simple spatiotemporal setting: constant-coefficient advection-diffusion. Let \((x,t)\) be a scalar defined on a periodic spatial domain, \(v\) a constant one-dimensional velocity coefficient and \(\)a constant diffusion coefficient, then:

Advection: \[+(v)=0\] (1a) Diffusion: \[+(-)=0\] (1b) Advection-Diffusion: \[+(v-)=0.\] (1c)

If our first assertion is true, we would expect pretraining on the advection and diffusion terms individually could be beneficial for transfer to advection-diffusion equations.

We find that this is indeed the case. We pretrain a spatiotemporal transformer model on a large amount of trajectories (100,000 each) with uniformly sampled coefficients (\(v[-3,3],\ [10^{-3},1.]\)) generated from the advection and diffusion equations while fine-tuning on restricted samples from advection-diffusion simulations. The pretrained model is able to achieve much lower error with far fewer samples (Figure 1) despite the fact that it never saw advection and diffusion occurring in the same trajectory during pretraining.

To address question two, we must handle much larger spatial resolutions, varying scales, and heterogeneous relationships between fields. Over the rest of this section, we develop an approach for handling these challenges.

### Architecture

Axial Attention. Given the success of large transformer models in other domains, we employ a scalable axial attention  transformer backbone. For a (2+1)-dimensional system with \(T H W\) tokens, conventional dense attention attends over all tokens simultaneously and has cost \(O((HWT)^{2})\). Axial attention instead performs a series of attention operations over each axis in turn, limiting the cost to \(O(H^{2}+W^{2}+T^{2})\). In Figure 2, it can be seen that while we perform attention on each axis independently, spatial attention utilizes one set of linear projections for both the height (y) and width (x) axes.

Axial attention has been used in a number of video transformers  due to the improved scalability in higher dimensions. While the tools used in our transformer backbone were introduced in prior work, our choice of using fully axial attention differs from ViViT which opted to only separate space and time attention. We favor scalability over maximizing accuracy and so chose the fully axial formulation. In subsequent sections we refer to this architecture as an Axial ViT (AViT).

Field Embedding and Normalization. Embedding multiple physical systems into a single shared representation is complicated by the fact that fields from different systems may operate on entirely different scales in terms of both magnitude and resolution. This is one of the primary challenges that must be addressed for multiple-physics pretraining.

To unify the magnitudes, we utilize reversible instance normalization [49, RevIN]. We compute the mean and standard deviation of each channel over the space-time dimensions and use them to normalize the input fields. These statistics are saved and used to denormalize the model outputs. While this approach was initially developed for time-series forecasting, the effect is similar to that reported in Subramanian et al. , where it was found to be beneficial to rescale the inputs to a fixed norm during training.

After rescaling, the data is projected into a shared embedding space. This is the only component with weights that are unique to each source system. Given a system \(S\) with

Figure 1: Finetuning a model pretrained on large amounts of advection and diffusion data outperforms models trained from scratch on advection-diffusion data across a wide range of data availability (16-100K examples).

state variables \(u(x,t),\ v(x,t),\ p(x,t)\), we project each sample point or "pixel" into a space of dimension \(D^{}\):

\[(x,t)=u(x,t)_{u}+v(x,t)_{v}+p(x,t)_{p}\] (2)

where \(\) are embedding vectors in \(^{D^{}}\). This can be seen as a convolution with \(1 1\) filters where the input channels of the filter are sub-selected to correspond to the fields present within a given dataset. On the right side of Figure 2, the filter is assembled by sub-selected columns of the larger filter corresponding to the provided fields. It is important to note that this initial projection setup is amenable to fine-tuning to unseen field types. This can be achieved by adding new channels to the initial embeddings, and training them from random initialization. In our models, the shared full resolution space is converted into patched tokens by a sequence of strided convolutions separated by pointwise nonlinearities as in Touvron et al. .

The predictions are reconstructed from the processed tokens by reversing this process. The tokens are decoded by a sequence of transposed convolution blocks and projected onto the output fields by taking coordinate-wise inner products with reconstruction vectors \(\):

\[u(x,t+ t)=(x,t+ t),_{u}.\] (3)

This can similarly be implemented as a \(1 1\) convolution with the output channels of the convolution filter sub-selected. The mean and standard deviation computed from the inputs are then applied to these normalized outputs to produce the final de-normalized predictions as in Kim et al. .

### Balancing Objectives During Training

Task Sampling.Our pretraining procedure operates on multiple levels of sampling. The task distribution varies in system \(S\), spatial resolution \(N_{S}\), and time resolution \(T_{S}\) and we want diverse batches that accurately capture the signal this provides. However, sampling a full batch from multiple systems at different resolutions simultaneously would be inefficient on modern hardware as it would require batch processing of differently shaped tensors. Multi-GPU training adds an additional complication as the variance in execution time due

Figure 2: (Left) MPP works by individually normalizing each example using Reversible Instance Normalization (RevIN) then embedding each field individually into a shared, normalized space. A single transformer backbone can then predict the next step for multiple sets of physics. We use an AViT backbone which attends over space and time axis sequentially. Spatial attention is further split by axis, though these share linear projection weights. (Right) The embedding and reconstruction matrices are formed by subsampling a larger \(1 1\) convolutional filter using unique field indices passed with the input data.

to unbalanced workloads can lead to inefficient hardware usage. We mitigate both of these concerns with a simple randomization scheme involving gradient accumulation. Gradient accumulation utilizes multiple backward passes per synchronization step. We therefore sample a single system \(S\) uniformly from \(\) for each micro-batch. With \(m\) micro-batches per synchronization step, we reduce the work-per-GPU variance \(_{}^{2}\) to \(_{}^{2}\), significantly reducing the average lost cycles due to work discrepancies. This could likely be further reduced by an approximate packing problem solution , but we found the random approach was sufficient for our needs. As we employ gradient accumulation in order to increase our batch sizes, this sampling procedure incurs no additional cost.

Scaled Training Objective. The simplest approach to obtaining updates from the different tasks is to add their gradients. However, as the magnitudes of the state variables can vary significantly between systems, unweighted losses will result in the gradients from the problems with the largest scales drowning out losses on smaller scales . To partially control this behavior, we train using the normalized MSE (NMSE) defined as:

\[_{}=|}_{S}(_{i}^{S})-_{t+1}^{S}\|_{2}^{2}}{\|_{t+1}^{S}\| _{2}^{2}+}\] (4)

where \(\) denotes the micro-batch and \(\) is a small number added for numerical stability. This does not account for the full variation in difficulty. Even if sub-task losses have similar magnitudes at the start of training, it is possible for some systems to converge quickly while other losses remain high. Nonetheless, we found that this allows our training process to produce strong results on multiple systems simultaneously.

## 5 Experiments

We design our experiments to probe two vital questions about the utility of MPP:

1. Can large transformer models learn the dynamics of multiple physical systems simultaneously?
2. Does MPP provide a finetuning advantage over existing spatiotemporal foundation models for new autoregressive prediction tasks?

Data. We use the full collection of two-dimensional time-dependent simulations from PDEBench  as our primary source for diverse pretraining data. This includes systems governed by four unique nonlinear PDEs at a variety of state variables available, resolutions, initial conditions, boundary conditions, and simulation parameters. The specific PDEs are the compressible and incompressible Navier-Stokes equations, the shallow-water equations, and a 2D Diffusion-Reaction equation. Full details on the data used can be found in Appendix B.1.

Training settings. \(T^{S}\) is fixed at 16 for all experiments as our VideoMAE comparison in Section 5.2 was unable to scale to larger sizes without gradient checkpointing. Autoregressive training is performed only one step ahead--no longer rollouts, noise corruption, or post-processing are included for stability. Training from scratch and MPP pretraining are always performed on the AViT architecture described in section 4.2. Full training details including data splits, optimization details, and hardware are documented in Appendix C.

Figure 3: Processing different physics (indicated by color) with different native resolutions incur varying wall-clock times (arrow lengths). To reduce the loss of GPU-cycles, we use gradient accumulation as a stochastic load-balancing mechanism, reducing the variance in work between all-reduce synchronizations.

### Pretraining Performance

First, we compare MPP-pretrained models to dedicated baselines from prior work across all available systems. The models are pretrained at a variety of sizes so we can begin to explore to benefits of scaling our approach. Precise model sizes can be found in Appendix C.1. Unlike the baselines which are trained on only one system and so must only learn one parameter regime, our models (denoted by MPP-AViT-*) must handle all systems and regimes without finetuning. The effect of physical parameters, forcing, and simulation parameters must be inferred from context \(_{t}^{S}\). The PINN , UNet , and FNO  results are sourced from Takamoto et al.  while the results from Shen et al.  with a finetuned SWIN  are used for ORCA. Results are reported in terms of Normalized RMSE (NRMSE, the square root of Equation 4) averaged over fields and examples, as in Takamoto et al. .

Our pretrained models are able achieve high-end performance on all datasets (Table 1) despite the difficulty of multi-task training . In fact, there is only one case where our pretrained models do not outperform all baselines. In some cases, the improvement over the baselines is nearly an order of magnitude in NRMSE and the performance improves with scale. However, we clarify that we are not claiming these results are optimal--we can, for instance, improve upon them by finetuning our own models on specific tasks. Rather, this experiment answers affirmatively that large transformers can learn multiple sets of dynamics simultaneously. Trajectories from pretrained models are displayed in Appendix D.4.

### Transfer to Low-data Domains

We remove all compressible fluid data from the training corpus and pretrain on the three remaining spatiotemporal systems. We evaluate transfer to two specific compressible Navier-Stokes datasets:

* "Near": \(M=0.1\), viscosity\(=10^{-2}\), Random Periodic Initial Conditions
* "Far": \(M=1.0\), viscosity\(=10^{-8}\), Turbulent Initial Conditions

Snapshots of the kinetic energy for the finetuning systems and incompressible training data are visualized in Figure 4. While quantitatively evaluating the physics gap is an unsolved problem, the names reflect both prior physical knowledge and qualitative evaluation. "Near" features a low Mach number, the dimensionless quantity that correlates with compressible

   Model & \#Param & SWE & DiffRe2D & CNS M1.0 & CNS M0.1 \\  MPP-AViT-Ti & 7.6M & 0.0066 & 0.0168 & 0.0442 & 0.0312 \\ UNet & 7.7M & 0.083- & 0.84– & 0.4725 & 1.6650 \\ FNO & 466K & 0.0044 & 0.12– & 0.1685 & 0.2425 \\ PINN & 8.5K\({}^{}\) & 0.017- & 1.6— & — & — \\  ORCA-SWIN-B & 88M & 0.0060 & 0.82– & — & — \\ MPP-AViT-B & 116M & 0.0024 & 0.0106 & 0.0281 & 0.0172 \\  MPP-AViT-S & 29M & 0.0039 & 0.0112 & 0.0319 & 0.0213 \\ MPP-AViT-L & 409M & 0.0022 & 0.0098 & 0.0208 & 0.0147 \\   

Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines. MPP-pretrained models learn multiple physical systems at least as well as standard baselines. Top performing within size range and overall are bolded. Dashes indicate precision not available. \({}^{}\) While the PINN is much smaller, these models are fit per-example.

Figure 4: Kinetic energy for representative incompressible training and compressible finetuning data. The “near” compressible snapshot resembles the training snapshot while “far” displays turbulent small scales not seen in the incompressible simulation.

behavior, and viscosity similar to that of the incompressible simulation. "Far" has wildly different turbulent behavior that induces small scale structure never seen during training. However, despite the similarity in physical behavior, the simulations are still quite different: the compressible and incompressible simulations in PDEBench differ in spatial and temporal resolution, initial condition distribution, boundary conditions, viscosity, and velocity range in addition to the difference in compressibility. We use these sets to compare the finenting performance of MPP, training from scratch, and an existing pretrained spatiotemporal transformer, VideoMAE  pretrained on both K400  and SSV2  datasets.

Figure 5 shows that the MPP models outperform VideoMAE and training from scratch by a large margin in the low-data regime. Numerical results are listed in Appendix C. VideoMAE displays surprisingly strong finenting performance given that the pretraining data is conventional video, but it is unable to match the much lower memory (Table 2) MPP-AViT-B in either setting. Predictably, both pretraining approaches are less accurate in the long-run on the turbulent "far" dataset. However, in the short-term the physical pretraining seems to provide an even larger advantage in this regime compared to the far smoother "near" data. Rollout visualizations are included in Appendix D.5.

## 6 Conclusion

Limitations and Future Work. Creating a true foundation model for fluids, continuum mechanics, or general physics requires significantly more data diversity capturing far more behavior at resolutions that are practically useful to researchers in these fields than what is included in this paper. Additionally, the architecture used in our current work assumes uniformly gridded data. Training a foundation model that can be extended to engineering-grade problems requires the ability to handle highly non-uniform grids and arbitrary geometries. Nonetheless, this work addressed important roadblocks in the development of foundation models for these fields.

The worlds of science and engineering are filled with complex phenomena that could tremendously benefit from fast surrogates, but that are lacking sufficient data for training those surrogates. Our approach, Multiple Physics Pretraining, offers new opportunities for training highly transferable models for use in these settings. We demonstrated that transformers are able to be finetuned effectively when trained on partially overlapping physics. This suggests value in large pretrained models trained on diverse physics. Our experiments showed transformers pretrained with MPP learn multiple sets of physics competitively with many dedicated approaches and that this knowledge transfers even across significant physical gaps. As physical datasets for machine learning mature, this capability paves the way for the development of true foundation models for spatiotemporal physics.

   Model & Max Memory \\  VideoMAE & 79.3 GB \\ AViT-B & 24.7 GB \\  AViT-Ti & 6.7 GB \\ AViT-S & 11.5 GB \\ AViT-L & 59.7 GB \\   

Table 2: Memory usage during finetuning on 16\(\)3\(\)512\(\)512 inputs for batch size 1 using mixed precision.

Figure 5: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of “far” seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.