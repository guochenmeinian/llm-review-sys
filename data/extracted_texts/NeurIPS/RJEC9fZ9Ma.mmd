# Neural Collapse To Multiple Centers For Imbalanced Data

Hongren Yan, Yuhua Qian, Furong Peng, Jiachen Luo, Zheqing Zhu, Feijiang Li

Schanxi University

Corresponding Author. Email: jinchengqyh@126.com

###### Abstract

Neural Collapse (NC) was a recently discovered phenomenon that the output features and the classifier weights of the neural network converge to optimal geometric structures at the Terminal Phase of Training (TPT) under various losses. However, the relationship between these optimal structures at TPT and the classification performance remains elusive, especially in imbalanced learning. Even though it is noticed that fixing the classifier to an optimal structure can mitigate the minority collapse problem, the performance is still not comparable to the classical imbalanced learning methods with a learnable classifier. In this work, we find that the optimal structure can be designed to represent a better classification rule, and thus achieve better performance. In particular, we justify that, to achieve better classification, the features from the minor classes should align with more directions. This justification then yields a decision rule called the Generalized Classification Rule (GCR) and we also term these directions as the centers of the classes. Then we study the NC under an MSE-type loss via the Unconstrained Features Model (UFM) framework where (1) the features from a class tend to collapse to the mean of the corresponding centers of that class (named Neural Collapse to Multiple Centers (NCMC)) at the global optimum, and (2) the original classifier approximates a surrogate to GCR when NCMC occurs. Based on the analysis, we develop a strategy for determining the number of centers and propose a Cosine Loss function for the fixed classifier that induces NCMC. Our experiments have shown that the Cosine Loss can induce NCMC and has performance on long-tail classification comparable to the classical imbalanced learning methods.

## 1 Introduction

Deep neural networks are popular choices classification tasks. Researchers try to demystify the deep representations learned from data . A recent paper  observed "Neural Collapse" (NC) phenomenon: all the backbone network output features from each class converge into their corresponding vertices of an equiangular tight frame (ETF) and the within-class variability collapses.

The layer-peeled model (LPM)  and unconstrained feature model (UFM)  are the simplified model to study NC, in which the backbone output feature and the classifier weights are assumed free variables to optimize. NC phenomena occur with different loss functions. The optimality of UFM satisfies NC under the CE loss with constraints , regularization , or even no explicit constraint . MSE objectives also induce NC at global optimality . There is another line of works that extend UFM or LPM on deeper linear layers .

Data imbalance has been recently considered in NC literature. In particular, Fang et al.  originally observe the "minority collapse" phenomenon that the classifier weights of the minorityclasses will approach each other when the imbalance level goes high. Thrampoulidis et al.  theoretically study the Unconstrained-features SVM problem, whose global minima take the form of Simplex-Encoded-Labels Interpolation (SELI), a more general structure compared to the ETF. Dang et al.  inspects the ReLU-activated output features of deep linear network collapse to a general orthogonal frame for imbalanced data, where the orthonormal vectors of the frame are rescaled.

There is a line of works that connect the NC to the DNN performance [10; 24; 25]. Some researchers treat NC as a tool to alleviate minority collapse problem in imbalanced learning [26; 27; 28]. We reproduce these methods with backbone network ResNet50  and datasets cifar10/cifar100, **Table** 3 shows NC-inspired methods ETF and ARB only outperform the plain model (ResNet50 with CE loss) slightly (or even worse in a few settings), which indicates that minority collapse is one but not the only problem that harms the generalization of learning model. One possibly important issue is that NC on training sample does not necessarily imply the NC on the distribution, as pointed out by Hui et al. . This inconsistency can lead to severe performance degeneration when the sample is too scarce to represent certain classes.

Neural Collapse implies "maximal separateness" between classes, which inspires some works to consider fixed classifiers in the training[31; 32; 33; 34; 35]. However, these methods do not display advantages over the classical imbalanced learning methods equipped with learnable classifiers.

In this paper, we study the connection between the optimal structure induced by neural collapse and its corresponding classification rule, and propose a MSE-type loss function that improves the imbalanced learning with fixed classifier. Specifically,

1. Through the analysis of hard-to-predict feature distribution (the features that are distributed randomly around the mean of the classifiers), we find that the classification accuracy is improved if the features from the minor classes align with more directions and those from major classes with less directions, which corresponds to a decision rule called the **Generalized Classification Rule** (GCR) discriminated from the Regular 1-Nearest Neighbor Classification Rule (RCR) induced by general NC in literature, and we also term these directions as the **centers** of the classes.

2. We design an MSE-type objective that describes the average distance between the centers and a given feature. We show in the theoretical framework of Unconstrained Feature Model (UFM) that, for balanced or imbalanced data and fixed or learnable classifiers, the output features collapse but skew from the classifiers at terminal phase of training (TPT), which is different from the original Neural Collapse phenomenon and is termed "**N**eural **C**ollapse to **M**ultiple **C**enters" (NCMC) (Theorem 3.3 and 3.4); moreover, we find that RCR (with respect to classifiers) becomes a surrogate of GCR at NCMC (Remark 3.9 and Proposition C.1).

3. We design a practical loss function for fixed classifiers and a class-aware strategy for determining the number of centers for each class. The loss induces the NCMC which is justified in theory and experiments and achieves comparable performance on several datasets with varying imbalance ratios to the classical imbalanced learning methods such as LDAM , KCL , ARBLoss , which indicates that NCMC can improve generalization in imbalanced learning.

## 2 Preliminaries

### Neural Collapse

Consider a classification task with \(K\) classes and \(n_{k}\) training samples per class, i.e., overall \(N:=_{k=1}^{K}n_{k}\) samples. DNN-based classifiers generally have the form

\[_{}()=_{}_{}()+\] (1)

where \(_{}():^{D}^{d}\) is the feature mapping \((d K)\), \(_{0}=[_{1},,_{K}]^{}^{K  d}\) with \(_{k}^{d}\) the weight vector of class \(k\), and \(^{K}\) the bias of classifier, respectively. \(=\{_{0},,\}\) is the set of the trainable network parameters, which includes the parameters \(\) of a nonlinear compositional feature mapping (e.g., \(_{}()=(_{L}( (_{2}(_{1}) )))\) where \(()\) is an element-wise nonlinear function). Let \([A]\) be the set \(\{1,2,,A\}\) for positive integer \(A\).

The network parameters are optimized by minimizing an empirical risk

\[_{}_{k=1}^{K}_{i_{k}=1}^{n_{k}}( }}(_{k,i_{k}})+, _{k})+(),\] (2)

where \((,)\) is a loss function (e.g., cross-entropy or MSE) and \(()\) is a regularization term (e.g., squared \(L_{2}\)-norm). Let us denote the feature vector of the \(i_{k}\)-th training sample of the \(k\)-th class by \(_{k,i_{k}}\) (i.e., \(_{k,i_{k}}=}(_{k,i_{k}})\) ), with \(i_{k}[n_{k}]\). Denote the centralized mean of feature from class \(k\) by \(}_{k}:=_{k}-_{G}\) where \(_{k}:=}_{i_{k}=1}^{n_{k}}_{k,i_{k}}\) and \(_{G}:=_{k=1}^{K}_{k}\); let \(}:=[}_{1},}_{2}, ,}_{K}]\).

Recently noticed NC phenomenon  shows the weight vectors align with the class-mean features

\[}}}^{} }}^{}}(_{K}-_{K}_{K}^{})\] (3)

with

\[_{k,1}=_{k,2}==_{k,n_{k}}\] (4)

for all \(k[K]\) at the terminal phase of training. where we use \(_{K}\) to denote the \(K K\) identity matrix, \(_{K}\) to denote the all-ones vector of size \(K 1\). The alignment may have alternative shapes for other problem settings such as \(\) is ReLU-activated feature or the data class is imbalanced.

### NC for Unconstrained Features Model with Regularized MSE Loss

To understand the emergence of symmetric structures, recent papers study the "unconstrained features model" (UFM), where the features \(\{\}\)\(}}}}}}}}}}}}}}\) and \(_{0}\) are treated as free variables. The rationality behind this simplification is the powerful expressivity of a trained neural network. Some use UFM to study the NC phenomenon under MSE loss.

Let \(=[_{1,1},,_{1,n_{1}},_{2,1}, ,_{K,n_{K}}]^{d N}\). In balanced case where \(n_{1}=n_{2}==n_{K}\), \(\) is associated with the one-hot vectors matrix \(=_{K}_{n}^{}^{K Kn}\), where \(\) denotes the Kronecker product. The optimization of the following problem

\[_{},,}\|} +_{N}^{}-\|_{F}^{2}+}}}{2}\|}\|_{F}^{2}+}{2} \|\|_{F}^{2}+}{2}\|\|_{2}^{2},\] (5)

gives the NC to ETF, where \(_{}},_{H}\), and \(_{b}\) are positive regularization hyper-parameters and \(\|\|_{F}\) denotes the Frobenius norm. A closely related model is the Bias-Free models

\[_{},}\|}- \|_{F}^{2}+}}}{2}\|}\|_{F}^ {2}+}{2}\|\|_{F}^{2},\] (6)

which proves to converge to an Orthogonal Frame . In the next section, we study a bias-free variant of (6).

## 3 Main Results

In this section we show why the regular classification rule is not optimal, and propose the generalized classification rule and its surrogate losses, then offer a UFM analysis on the NC phenomenon under these losses. Since our paper focuses on bias-free model, we will simply call \(_{0}\) the classifier.

### Nearest-Neighbor Classification Rule Revisit: A Toy Example

Let \(^{d_{0}}\) (\(d_{0}\) is the input dimension) be the underlying data population, and \(Z:=()^{d}\) represent the feature population of the trained backbone network \(\); Assume the trained classifier \(_{1},_{2},,_{K}\) align with training samples drawn from \(\) perfectly and thus achieve zero training error.

Additionally, assume the classifiers are orthonormal and equally normed. Meanwhile, let the classes of training data \(_{1},_{2},_{K}\) be arranged such that the class sample sizes follow a descending order, i.e. \(n_{1}>n_{2}>>n_{K}\).

According to NC, when the neural network arrives at the terminal phase of training, the regular classification rule (**RCR**) with respect to \(_{0}\) is

\[c=*{argmax}_{k[K]}\{_{k}^{}z\}_{k=1}^{K},\] (7)

i.e., the \(1\)-nearest neighbor decision rule.

Assume \(Z_{1}P_{1}+_{2}P_{2}\), the mixture of the subpopulation \(P_{1}\) which is correctly classified by the decision rule and the subpopulation \(P_{2}\) that is classified at random, where \(_{1}\) and \(_{2}\) are the positive weights with \(_{1}+_{2}=1\).

In the analysis, let \(P_{2}\) have the form \(z=+p\), with \(p(0,sI_{d})\) and \(:=\) where \(s\) is a small positive number and \(=_{k=1}^{K}_{k}\), which is termed a **Hard-To-Predict** feature distribution near the global mean of the classifier. For convenience we denote events \(E_{k}:=\{z_{k}|z P_{2}\}\) for all \(k[K]\), and note that \(_{k=1}^{K}P\{E_{k}\}=1\)

By virtue of the expressivity of the deep neural network, the population of the major classes will be more concentrated to the classifier than that of the minor classes, so that populations trained by major classes have less probability falling in \(P_{2}\), i.e,

\[P\{E_{1}\} P\{E_{2}\} P\{E_{K}\}.\] (8)

Then by the gaussianity of \(p\) and the orthogonality of the classifiers \(_{1},_{2},,...,_{K}\), the probabilities of correctly classifying data from \(P_{2}\) are the same, that is,

\[P\{_{k}^{}z>_{k^{} k}_{k^{ }}^{}z|z E_{k}\}=P\{_{k}^{}p>_{k^{} k }_{k^{}}^{}p|p(0,sI_{d})\}=,\] (9)

since \(_{k}^{}z>_{k^{}}^{}z_{k}^{ }(+p)>_{k^{}}^{}(+p)_{k}^{}p> _{k^{}}^{}p\) given \(_{k}^{}=_{k^{}}^{}\). Therefore, by the total probability

\[P_{RCR}:=P\{z|z P_{2}\}= _{k=1}^{K}P\{E_{k}\}=.\] (10)

This calculation inspires us to come up with a different classification rule that outperforms RCR on \(P_{2}\). Indeed, we can assign the label \(y=k\) to the set \(\{}_{j}^{(k)}\}_{j=1}^{f_{k}}\) of vectors, for all \(k[K]\); denote \(F=_{k=1}^{K}f_{k}\) and \(_{ext}=_{k=1}^{K}_{j=1}^{f_{k}} }_{j}^{(k)}\); also similar to the RCR, assume all these \(F\) vectors are equally normed and mutually orthogonal. These vectors correspond to a **Generalized Classification Rule** (GCR)

\[c=*{argmax}_{k[K]}\{_{j[f_{k}]} }_{j}^{(k)},z\},\] (11)

or equivalently, \(f_{k}\)-nearest neighbor classification rule, where number of nearest neighbors, \(f_{k}\), of an example depends on which class it belongs to. Again by the properties of normal distribution and total probability formula, we obtain

\[P_{GCR}:=P\{z|z P_{2}\}= _{k=1}^{K}f_{k}P\{E_{k}\}.\] (12)

Recall the ascending sequence of \(P\{E_{1}\} P\{E_{K}\}\), so that any choice of \(f_{1} f_{2} f_{K}\) gives higher correctly-classified probability for \(z P_{2}\). If we in addition consider pairwise comparison instead of the one-vs-all fashion in (9), we have for any pair \(k k^{}\)

\[P\{}_{k},z>}_{ k^{}},z z E_{k} E_{k^{}}\}=,\] (13)

for **RCR**, and

\[P\{_{j[f_{k}]}}_{j}^{(k)},z> _{j^{}[f_{k^{}}]}}_{j^{ }}^{(k^{})},z z E_{k} E_{k^{}}\}= }{f_{k}+f_{k^{}}}\] (14)

for **GCR**. In this case, we require \(f_{k}<f_{k^{}}\) for \(k<k^{}\) for all pairs of \(k,k^{}[K]\), or equivalantly, \(f_{1}<f_{2}<<f_{K}\). The analysis as a whole has yielded the following proposition.

**Proposition 3.1**.: _assume (1) all the classifiers, \(\{_{k}\}\) in RCR or \(\{}_{j}^{(k)}\}\) in GCR are orthonormal frames; (2) \(Z_{1}P_{1}+_{2}P_{2}\), the mixture of the subpopulation \(P_{1}\) which is correctly classified by the decision rule and the subpopulation \(P_{2}\) where \(_{1}\) and \(_{2}\) are the positive weights with \(_{1}+_{2}=1\); (3) \(P_{2}\) has the form \(z=+p\) or \(z=_{ext}+p\) depending on which classification rules used, with \(p(0,sI_{d})\) where \(s\) is a small positive number; (4) \(P\{E_{1}\} P\{E_{2}\} P\{E_{K}\}\). Then \(P_{GCR} P_{RCR}\) for \(f_{1} f_{2} f_{K}\)._

The **Hard-To-Predict features** in the proposition 3.1 are considered to be drawn randomly around the mean of the classifiers. Thus, we are motivated to use more orthogonal directions to classify the minor class. However, the GCR in the analysis does not apply to the practical training of the neural network easily. Indeed, the loss vanishes quickly when training directly with this rule. For this reason, we consider finding a surrogate loss that a) induces neural collapse to an orthogonal frame, and b) the classification rule at the neural collapse approximate GCR.

### Center and Multi-Center Frame

We first define the "centers" that resemble \(}_{j}^{(k)}\)'s in **GCR** (11) (in definition 3.2). Let \(f,f_{1},f_{2},,f_{K}_{+}\) be preset factors such that \(fK=_{k=1}^{K}f_{k}\), \(N=_{k=1}^{K}n_{k}\), and \(S:=_{k=1}^{K}f_{k}n_{k}\) and \([0,]\) be angle constant. Let the linear classifier \(_{0}\) satisfies

\[_{k}^{}_{k}>0, and_{k}^{} _{k^{}}=0k k^{}.\] (15)

and the data features \(=[_{1,1},,_{1,n_{1}},_{2,1}, ,_{K,n_{K}}]^{d N}\).

**Definition 3.2** (Center of Class \(k\)).: Let \(d>(f+1)K\), \(:=[_{1}^{(1)},,_{f_{1}}^{(1)},_{1}^{(2)},,_{f_{K}}^{(K)}]^{}\) is a matrix consisting of \(fk\) rows of \(d\)-dimensional vectors \(_{j}^{(k)^{}}\)'s and satisfies equality

\[[^{}|}^{}][^{}| }^{}]^{}=\] (16) \[(\|_{1}\|^{2}\,_{f_{1}}, \|_{2}\|^{2}\,_{f_{2}},,\|_{K}\|^{2}\, _{f_{K}},\|_{1}\|^{2}\,,\|_{2}\|^{2}\,,,\| _{K}\|^{2})\] (17)

where\([|]\) is the column augmentation of the matrix, and \((,,)\) is the diagonalization of the block matrices. Then a **center** of class \(k\) is defined as

\[_{j}^{(k)}:=_{j}^{(k)}+_{k},\;j[f_{k}].\] (18)

A **multi-center frame** is the matrix consists of \(fK\) rows of \(_{j}^{(k)^{}}\), i.e.

\[=[_{1}^{(1)},,_{f_{1}}^{(1)},_{1}^{(2)},,_{f_{K}}^{(K)}]^{},\] (19)

Let \(\) denote the constraint of the tuple \((,_{0})\) such that \([^{}|}^{}]\) satisfies (16) and \(\|_{k}\|^{2}>0\) are positive for all \(k[K]\).

By the definition 3.2, \(_{1}^{(k)}=_{2}^{(k)}==_{f_{k}}^{(k)}\) for all \(k[K]\), verbally, the centers of each class are equally-normed, and \(_{j}^{(k)^{}}\,_{k}^{}=0\) for all tuple \((k,k^{},j)[K][K][f_{k}]\) with \(k^{} k\). Note \(d(f+1)K\) is a necessary condition for the existence of \((f+1)K\) mutually orthogonal d-dim vectors in equation (16). Figure 3 illustrates the centers of Class 1 and Class 2.

Let \(}^{(k)}:=}_{j=1}^{f_{k}}_{j}^{( k)}\) be the mean of the centers of class \(k\). Since all \(_{j}^{(k)}\), \(j[f_{k}]\) are equally normed and equi-angular for each \(k\), we can denote \(_{k}^{*}=(}^{(k)},_{j}^{(k)})\) and \(_{k}^{*}=(}^{(k)},_{k})\) with no ambiguity. It is easy to check that \(_{k}^{*}:=+f_{k}^{2}}{f_{k}}}\) and \(_{k}^{*}:=}{^{2}+f_{k}^{2} ^{2}}}\).

Then we define the bias-free regression loss for the UFM w.r.t the feature \(_{k,i_{k}}\) and the multi-center frame by

\[\|_{k,i_{k}}-_{k,i_{k}}\|_{2}^{2}:= _{j=1}^{f}(_{j}^{(k)^{}}_{k,i_{k}}-1)^{2}}_{}+_{j^{ }=1}^{f}_{k^{} k}^{K}(_{j^{}}^{(k^{})^{ }}_{k,i_{k}})^{2}}_{}\] (20)to measure the average extent to which a feature \(_{k,i_{k}}\) collapses to its class centers \(_{j}^{(k)}\)'s while stays away from centers of other classes, where \(_{k,i_{k}}\) is the "\(f_{k}\)-hot coding":

\[_{k,1}=_{k,2}=_{k,f_{k}}=[0,,0, _{_{m=1}^{k-1}f_{m}\ 0\ },_{f_{k}\ 1\ },\ 0,,0]^{}.\]

Let \(:=[_{1,1},_{1,2},...,_{1,n_{1}}, _{2,1},,_{K,n_{K}}]^{F N}\), the minimization of the regression loss over all features subject to \(\) turns into our prototype optimization problem in this paper

\[:_{,,_{0}}\|-\|_{F}^{2}+_{0}}{2}\|_{0} \|_{F}^{2}+}{2}\|\|_{F}^{2}\ (,_{0}) .\] (21)

### Neural Collapse to Multiple Centers (NCMC)

The following theorem (proved in appendix D) characterizes the global solutions of the optimization when the data is balanced.

**Theorem 3.3**.: _Given \(n_{1}=n_{2}==n_{K}\) and \(d(f+1)K\). If \(K_{0}}^{*}\), then any global minimizer \((,_{0}^{*},^{*})\) of P satisfies_

\[_{k,1}^{*}==_{k,n_{k}}^{*}=_ {k}^{*}}^{(k)}\,\  k[K].\] (22) \[_{k^{}}^{*},_{k}^{*}=0,\ _{j}^{(k)*^{ }}_{k}^{*}=_{j^{}}^{(k)*^{}}_{k^{ }}^{*},\ \  j,j^{}[f_{k}]\,,\ \ and\ k,k^{}[K]\] (23) \[_{1}^{*^{}}_{1}^{*}==_{K }^{}^{*}{}_{K}\,,\ and\ \ \|_{1}^{*}\|^{2}==\|_{K}^{*}\|^{2}=_{0}+_{0}}{n_{H}}}^{* }}{_{0}}{n_{H}}^{2}^{*}}\,,\] (24) \[_{0}\,\|_{k}\|_{2}^{2}=n_{H}\, \|_{k}\|_{2}^{2}\,,\ and\ \|_{1}^{*}\|_{2}==\|_{K}^{*}\|_{2}\,,\] (25)

_or otherwise, the objective **P** is minimized by \((,_{0}^{*},^{*})=(,,)\)._

For imbalanced data and non-identical expansion factors \(f_{k}\), the following theorem shows the relationship between the optimal conditions and the parameters \(f_{k}\), \(\), and \(n_{k}\) (proved in appendix D).

**Theorem 3.4**.: _If \(_{k}^{*}>}_{0}}_ {H}}{n_{k}}}\), for all \(k[K]\), then the global optimizer \((,_{0}^{*},^{*})\) of **P** satisfies_

\[_{k,1}^{*}==_{k,n_{k}}^{*}=_ {k}^{*}}^{(k)}\,\  k[K].\] (26) \[_{j}^{(k)*^{}}_{k}^{*}=_{j^{ }}^{(k)*^{}}_{k}^{*},\ \  j,j^{}[f_{k}]\,,\ and\ k[K]\] (27) \[_{k^{}}^{*},_{k}^{*}=0,  k k^{}\] (28) \[\|_{k}^{*}\|^{2}=}}{f_{k}n_ {k}}+}}{n_{k}_{H}}}_{k}^{*}}{ }}{n_{k}_{H}}^{2}_{k}^{*}}\ and\ \|_{k}^{*}\|^{2}=}}{n_{k}_{H}}\,\|_{k}^{*}\|^{2}\,,\] (29)

_or otherwise the objective **P** is minimized by \((,_{0}^{*},^{*})=(,,)\)._

Compared to the Theorem 3.3, Theorem 3.4 indicates that \(\|_{k}\|\), \(\|_{k}\|\), and the ratio between them depends on all the expansion factors \(f_{j}\)'s and the size of the class \(n_{j}\). Both theorems show the features of class \(k\) converge in the direction of \(}^{(k)}\), the mean of the centers of class \(k\). We term this type of collapse "Neural Collapse to Multiple Centers (NCMC)".

_Remark 3.5_.: The optimality of \(\) is controlled by the centers.

_Remark 3.6_.: NCMC differs from UFM analyses in existing literature since \(_{k}^{*}\) and \(_{k}^{*}\) are not aligned at optimum, and the norm of the optimal classifier depends on the expansion factors of the classes \(f_{k}\)

**Corollary 3.7** (Corollary of Theorem 3.4).: _The optimality condition of **P** as **V** and \(_{0}\) are both fixed satisfies_

\[_{k,1}^{*}==_{k,n_{k}}^{*}=_{k} ^{*}}^{(k)}\,,\; k[K].\] (30) \[_{j^{}}^{(k)},_{k}^{*}= _{k^{}}^{*},_{k}^{*}=0\; k^{ } k[K],j^{}[f_{k^{}}]\] (31) \[_{1}^{(k)^{}}_{k}==_{f_{ k}}^{(k)^{}}_{k}, k[K]\] (32) \[\|_{k}^{*}\|_{2}=^{*}}{f_{k}^ {2}^{*}+_{H}S}\] (33)

_Remark 3.8_.: According to the proof of the corollary, the fixed classifier case does not require the condition w.r.t the lower bound of \(^{*}\). Moreover, it is also clear that although \(_{k}^{*}\) aligns with \(}^{(k)}\), the length of it relies on the value of \(f_{k}\) nonlinearly. Indeed, from the Corollary,

\[\|_{k}^{*}\|_{2}=^{*}}{f_{k}^ {2}^{*}+_{H}S}=^{*}+S}{f_ {k}_{k}^{*}}}\]

has the global maximum \(_{k}^{*}=S}{f_{k}}}\) if \(_{H}S<f_{k}\).

_Remark 3.9_.: NCMC induces an approximate rule to **GCR** in the following two aspects:

**(1)** the centers are "almost orthogonal" to each other: two centers from different classes are orthogonal to each other. The angle between two enters from the same class is \((_{j}^{(k)},_{j^{}}^{(k)})}{\| _{k}\|^{2}})=^{2}\) for \(j j^{}\). as \(\) is small, the angle is close to \(\).

**(2)** Under NCMC of our problem setting, the **RCR** w.r.t. the \(_{0}\) (see (7)) can be considered a surrogate of **GCR** to some extent: if a hard-to-predict feature can be classified by **RCR** with a margin correctly, then it can be classified correctly by **GCR** with probability larger than \(\). We will discuss this more formally in the proposition C.1.

### NCMC for Fixed Classifier

In the Theorem 3.3 and Theorem 3.4, we present the NC conditions for **P**. However, solving **P** requires optimization of a scaled orthonormal frame on a non-euclidean manifold, which is computationally expensive for overparameterized models. We hope the classifier can be fixed while not losing its performance severely. We first analyze the fixed classifier via UFM (proof in the appendix D), and later in the next section we propose a practical loss function for the fixed classifier.

### Class-Aware Strategy for Determining the Number of Centers

The proposition3.1 indicates the extra dimensions help improve the classification of "hard-to-predict" samples in the distribution, and basically the expansion factors should satisfy \(f_{1}<f_{2}<<f_{K}\) when the class size decreases, i.e., \(n_{1} n_{2} n_{K}\). The principle of generating these \(f_{k}\)'s is twofold: _(1):_\(f_{1} 1\); _(2):_ the ratio of ascending \(\{f_{k}\}\)) shall approximates the ratio of descending of \(\{n_{k}\}\), i.e., \(}{f_{k+1}}}{n_{K-k-1}}\) for all \(k[K]\).

Concretely, we use the **Class-Aware Strategy** to generate the expansion factors \(f_{k}\) when \(f 2\):

Step 1: Given the descending list \([n_{1},n_{2},,n_{K}]\) and scale \([n_{1},n_{2},,n_{K}]\) to \([}{N},}{N},,}{N}]\);

Step 2: Calculate \([a_{1},a_{2},,a_{K}]\) where \(a_{k}=}{N}+1\), to ensure each element is positive;

Step 3: Reverse the order of the list to \([a_{K},a_{K-1},,a_{1}]\) then add \(1\) from the left until the sum of the elements in the list equals \(fK\).

For example, when \((n_{1},n_{2},n_{3})=(1,3,3)\) and \(f=3\) then Step 1 outputs \((,,)\); Step 2 outputs \((1,3,3)\); Step 3 outputs \((f_{1},f_{2},f_{3})=(4,4,1)\).

Experiments

In this section, we **(1)** propose a cosine loss function for fixed classifier; **(2)** verify NCMC induced by the cosine loss through experiments; **(3)** show how \(f\) and \(\) influence the learning performance; **(4)** Compare long-tail classification performance to SETF method with fixed classifier and other classical methods with learnable classifier.

### Datasets and Training Schedule

We set long-tailed classification tasks on five datasets, CIFAR-10 , CIFAR-100 , SVHN , STL-10 , and large dataset ImageNet with two architectures ResNet-50 and densnet-150 (details in appendix F). Let \(:=}{n_{min}}\) represent the imbalance ratio of the long-tailed sampling from the dataset, where \(n_{max}\) and \(n_{min}\) are the size of the largest class and the smallest class, resp. The accuracy results are the average of three repeated experiments with different seeds. The best and second-best results are boldfaced and underlined.

### The Cosine Regression Loss

**The Cosine Regression Loss.** Motivated by the toy example 3.1 and the theoretical justification of NCMC, we propose the regularized loss for the fixed unit-norm multi-center frame that satisfies definition 3.2:

\[(,_{k,i_{k}})=_{j=1}^{f_{h}}(_{j}^{(k)},_{k,i_{k}})+(\|_{k,i_{ k}}\|-1)^{2}.\] (34)

where \((,)=\|,}{\| \|}-1\|_{2}^{2}\) is termed **Cosine Loss**, \(\) and \(\) are regularization coefficients (we relegate the selection of the coefficients to appendix F ). This is called a Cosine loss discard all \(_{j}^{(k^{})}\) terms of loss (20) for features from class \(k\), where \(k^{} k\), since in practice \(h_{k}\) aligns with \(}^{(k)}\) only if \(_{j}^{(k^{})},_{}=0\). The derivative of the loss with respect to some feature \(\) then is given by

\[\,(,)}{}=- \|^{2}}(1-,} {\|\|})(-, }{\|\|}),\] (35)

(the derivation is postponed to the Appendix E). It shows the gradient changes both the magnitude and the direction of the features with a scale \(\|\|^{-2}\) of the feature, compared to the dot-regression loss in . The regularizer guarantees the gradient does not vanish for the features with large norm and explode for the features with small norm since \((,)\) is scaled by \(\|\|^{-2}\).

Note that, the loss uses the Multi-Center Frame \(W\) in the training while, due to Remark 3.9, we still use \(_{0}\) as the classifier. In the context, we term the objective (34) as \(f_{1}=f_{2}==f_{K}\) the "Average Loss" (AL), and that with \(f_{k}\) selected by class-aware strategy the "Class-Aware Loss" (CAL).

### Neural Collapse

We design experiments to verify loss \(\), AL and CAL induce NCMC. The collapse is measured by \(:=\|_{k,i_{k}}/\|_{k,i_{k}}\|- }^{(k)}/\|}^{(k)}\|\|^{2}\). For simplicity, we calculate the mean and standard deviations of the vector \(_{k,i_{k}}/\|_{k,i_{k}}\|-}^{(k )}/\|}^{(k)}\|\) when the mean and standard deviation tends to zero, we can show that NC occurs. We fix \(f=20\) and \(=0.2\); we pick the tuple \([f_{1},f_{2},,f_{K}]\) by the Class-Aware Strategy. Figure 1 shows that both AL and CAL induce NC. The change of mean is rapid and the standard deviation first increases and then converges to zero slower. Variability is defined as the average of the sum of \(=1-_{k,i_{k}}}{\|_{k,i_{k}}\|} _{k}}{\|_{k}\|}\) over all \(k\) and \(i_{k}\), and measures the alignment of \(\) and \(\), and we observe the in all three objectives the variabilities stay away from zero. Figure 4 plots the NCMC of loss \(\) with or without regularization on the norm of features. The regularization results in heavier NC.

We also provide plots of NCMC on VGG and LeNet for CAL that demonstrate the universality of the phenomenon (refer to **Table 5**). We also draw heatmaps of the neural collapse for better visualization. Fig.6 is the heatmap of \(}^{}}\) where \(}=[}_{1},}_{2},}_{K}]\) and \(}_{k}:=_{k}}{\|}_{k}\|}\) is the normalized class-mean features. Fig.7 is the heatmap for the normalized features in class 9 of CIFAR-10; it is noted that all features in this class stay close to each other from the initialization to the end of training.

### Long-Tailed Classification

We conduct an ablation study with ResNet50 on CIFAR-100. **Table** 1 presents the performance of ResNet50 on both balanced CIFAR-100 and its imbalanced sample with or without Mixup training under CAL at imbalance ratio \(=\{0.005,0.01,0.02,1(balanced\ case)\}\). We do not use objective \(\) since it is inferior to the Cosine Regression Loss (see **Table** F.1). \(\) is incompatible with mixup training and it may be degenerated by the myriad of distraction terms in the objective.

As shown in **Table** 1, the performances get improved for almost all imbalanced ratios when the CE is replaced with our designs, especially CAL. And our method is compatible with Mixup training. However, the SETF and our method have lower accuracy than the CE in the balanced case. The analysis in the Proposition 3.1 tells us that in the balanced case where \(f_{k}\)'s are identical, our theory will not have a significant positive effect on the classification. Our method has a similar form to SETF

who fixes the ETF classifier and emphasizes the gradient norm balance among the classes. On the contrary, CAL concentrates on the fitting to the multiple centers. We compare our method CAL to CE and SETF on four small datasets CIFAR-10, CIFAR-100, SVHN, STL-10 (**Table** 2, and **Table** 5). It shows the stability of our Class-Aware Strategy and displays a significant improvement to the original networks.

The hyper-parameters have influences on the performance: if \(\) is small, the centers of a class are very close to each other, else if \(\) gets larger, the margin of the classifier will be smaller; parameter \(f\) encodes partial imbalance information of the classes, when \(f\) is small all \(f_{k}\) are close to each other and cannot offer useful supervision for the "hart-to-predict" samples. Refer to appendix F for more information about hyperparameter selection.

We summarize in **Table** 6 the performance of CAL as \(f\) and \(\) vary. The result empirically demonstrates: **(1)** fixing \(f\), the accuracy roughly peaks at some \(\) bounded away from \(0\) and \(/2\). **(2)** at \(=/2\), The frame collapses to the classifier, resulting in an accuracy rate similar to SETF  where the norm of the gradient is weighted by the class imbalance ratio; **(3)** Frame that is orthogonal

    &  &  \\  & \(0.005\) & \(0.01\) & \(0.02\) & balanced & \(0.005\) & \(0.01\) & \(0.02\) & balanced \\  
**ResNet50** & & & & & & & \\
**CE** & 35.6\(\)0.3 & 37.5\(\)0.4 & 43.3\(\)0.2 & **79.5\(\)**0.3 & 42.4\(\)0.5 & 46.7\(\)0.3 & 52.7\(\)0.3 & **81.8\(\)**0.1 \\
**SETF** & 38.1\(\)0.4 & 42.6\(\)0.2 & 48.4\(\)0.3 & 78.7\(\)0.2 & 43.0\(\)0.1 & 48.3\(\)0.5 & 52.5\(\)0.3 & 79.7\(\)0.2 \\
**CAL** & **40.6\(\)**0.3 & **44.7\(\)**0.2 & **50.2\(\)**0.5 & 78.5\(\)0.4 & **46.5\(\)**0.5 & **50.1\(\)**0.3 & **54.3\(\)**0.4 & 79.4\(\)0.4 \\   

Table 1: An ablation study of Mixup method for training ResNet50 on CIFAR-100 using different classifiers and loss functions. The numbers in the second row are imbalance ratios. The parameters are fixed with \(f=20\), and \(=0.2\)

Figure 1: An illustration of the Neural Collapse curves of AL (a and b, on CIFAR100 with \(=0.01\) and CIFAR 10 with \(=0.01\) respectively), CAL (c and d, on CIFAR100 with \(=0.01\) and CIFAR 10 with \(=0.01\) respectively). The AL is equipped with f=20 and CAL uses Class-Aware Strategy.

to the classifier does not learn anything in the training since the representation is irrelevant to the classifier, thus providing no useful discrimination information.

We compare our method to several classical methods including NC-inspired methods ARB loss and SETF, margin-based LDAM-DRW, contrastive learning method KCL and original CE. The **Table**3 shows the methods in comparison. We observe that our method is comparable to or even better than the others. The comparison indicates: **(1)** Our method has some advantages for heavily imbalanced cases. One of the underlying mechanisms is when the minor classes are underestimated due to the lack of sample, they are likely to display the gaussianity such that the proposition 3.1 and class-aware strategy work fine; as the imbalance ratio \(\) rise, the minor classes lose their randomness during training, where our method fails. **(2)** Our method does not compete with the KCL and ARBLoss on ImageNet, reflecting the limitations of our method in the flexibility of the direction and magnitude of the classifier weights when learning large datasets. We also compare our method to a recent work RBL  in the Appendix H that demonstrates the potential limitation of the fixed classifier and the advantage of CAL. **(3)** two classical methods LDAM-DRW and KCL do not compete with other NC-inspired methods trained with Mixup training, demonstrating the effectiveness Mixup training strategy. **(4)** Our method with the parameter chosen in the experiment outperforms CE negligibly or performs worse than CE when the imbalance ratio approaches \(1\). However, we find picking \(f=10\), \(=0.5\) gives an accuracy rate \( 0.2\) on Cifar-10 with \(=0.1\), which shows the significance of hyperparameter selection for our method.

## 5 Conclusion

In this paper, we rethink the regular 1-Nearest Neighbor Classification Rule (RCR) in imbalanced learning; an analysis of the Hard-To-Predict feature indicates under certain circumstances the generalized classification rule (GCR) is superior to RCR, which implies that minor classes should compare to more "neighbors" in the classification. Then we introduce neural collapse to multiple centers (NCMC) under an MSE-type loss, where the centers play a role similar to the neighbors in GCR. According to the framework of the Unconstrained Features Model, the features of each class collapse to the class mean of the centers in balanced or imbalanced settings for learnable or fixed classifiers. We notice that at NCMC, RCR resembles GCR in terms of the hard-to-predict feature distribution. We then propose the cosine loss, a surrogate regression objective of the MSE-type loss called Cosine Loss, that applies to the fixed classifier; and develop the class-aware strategy for determining the number of centers of each class, inspired by the analysis of the Hard-To-Predict Feature. The cosine loss practically induces the NCMC at the terminal phase of training; the combo of the class-aware strategy and the loss with the fixed classifier demonstrates its effectiveness in long-tailed classification. Our work shows the possibility of obtaining a task-specific classification rule by designing the optimal structure at neural collapse under customized losses; it provides a connection among the optimal structure of the feature-classifier alignment, the classification rule, and the generalization in the learning problem.

    &  &  \\  & \(0.005\) & \(0.01\) & \(0.02\) & \(0.005\) & \(0.01\) & \(0.02\) \\  _ResNet_ & & & & & & \\ CE & 72.3\(\)0.1 & 78.6\(\)0.2 & 84.0\(\)0.1 & 42.4\(\)0.5 & 46.7\(\)0.3 & 52.7\(\)0.3 \\ SETF & 74.2\(\)0.5 & 79.7\(\)0.4 & 83.8\(\)0.3 & 43.0\(\)0.1 & 48.3\(\)0.5 & 52.5\(\)0.3 \\ CAL & **80.0\(\)**0.5 & **84.1\(\)**0.4 & **85.9\(\)**0.2 & **46.5\(\)**0.5 & **50.1\(\)**0.3 & **54.3\(\)**0.4 \\  _DenseNet_ & & & & & & \\ CE & 71.1\(\)0.5 & 77.7\(\)0.3 & 84.1\(\)0.1 & 42.9\(\)0.2 & 47.4\(\)0.2 & 53.3\(\)0.2 \\
**SETF** & 72.9\(\)0.4 & 78.5\(\)0.3 & 83.4\(\)0.3 & 42.3\(\)0.2 & 46.3\(\)0.3 & 52.6\(\)0.2 \\
**CAL** & **78.1\(\)**0.2 & **81.1\(\)**0.2 & **84.5\(\)**0.2 & **46.3\(\)**0.3 & **50.1\(\)**0.2 & **54.0\(\)**0.2 \\   

Table 2: Long-tailed classification accuracy (%) with ResNet and DenseNet on CIFAR-10 and CIFAR-100.

    &  &  &  \\  & \(0.005\) & \(0.01\) & \(0.02\) & \(0.1\) & \(0.005\) & \(0.01\) & \(0.02\) & \(0.1\) & \\ 
**CE** (Mixup) & 72.3\(\)0.1 & 78.6\(\)0.2 & 84.0\(\)0.1 & 91.9\(\)0.1 & 42.4\(\)0.5 & 46.7\(\)0.3 & 52.7\(\)0.3 & **67.9\(\)**0.1 & 44.2\(\)0.3 \\
**LDAM-DRW** & 47.4\(\)0.3 & 80.1\(\)0.3 & 84.1\(\)0.2 & 90.0\(\)0.2 & 39.5\(\)0.3 & 44.2\(\)0.5 & 50.0\(\)0.2 & 62.5\(\)0.2 & 47.7 \\
**KCL** & 75.0\(\)0.3 & 80.9\(\)0.2 & 84.5\(\)0.3 & 90.7\(\)0.4 & 40.3\(\)0.4 & 44.8\(\)0.3 & 52.0\(\)0.2 & 63.0\(\)0.2 & 51.5 \\
**SETF** & 74.2\(\)0.5 & 79.7\(\)0.4 & 83.8\(\)0.3 & 91.3\(\)0.4 & 43.0\(\)0.1 & 48.3\(\)0.5 & 52.5\(\)0.3 & 66.1\(\)0.3 & 44.7 \\
**ARBLoss** & 79.5\(\)0.7 & 83.8\(\)0.4 & **86.1\(\)**0.3 & 91.5\(\)0.3 & 42.7\(\)0.8 & 47.1\(\)0.5 & 49.7\(\)0.2 & 64.4\(\)0.4 & **52.8** \\
**CAL** & **80.0\(\)**0.5 & **84.1\(\)**0.4 & 85.9\(\)0.2 & **92.0\(\)**0.3 & **46.5\(\)**0.5 & **50.1\(\)**0.3 & **54.3\(\)**0.4 & 65.9\(\)0.3 & 49.7\(\)0.2 \\   

Table 3: A comparison of several recent methods of long-tail classification trained on ResNet50. \(f=20\) and \(=0.2\) are fixed. The values without \(\) are that we did not reproduce.