# Towards the Transferability of Rewards Recovered

via Regularized Inverse Reinforcement Learning

 Andreas Schlaginhaufen

SYCAMORE, EPFL

andreas.schlaginhaufen@epfl.ch

&Maryam Kamgarpour

SYCAMORE, EPFL

maryam.kamgarpour@epfl.ch

###### Abstract

Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task . However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition . In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.

## 1 Introduction

Reinforcement learning (RL) has achieved remarkable success in various domains such as robotics , autonomous driving , or fine-tuning of large language models . Despite these advances, a key challenge lies in designing appropriate reward functions that reflect the desired outcomes and align with human values. Misaligned rewards can lead to suboptimal behaviors , undermining the potential benefits of RL in practical scenarios. Inverse reinforcement learning (IRL), also known as inverse optimal control  or structural estimation , addresses this problem by inferring a reward from demonstrations of an expert acting optimally in a Markov decision process (MDP).

Compared to behavioral cloning , which directly fits a policy to the expert's demonstrations, IRL is believed to provide a more transferable description of the expert's task , as recovering the expert's underlying reward would enable us to train a policy in a new environment with different dynamics. However, it is also known that the reward corresponding to some optimal policy is not unique , making it difficult to recover the expert's true underlying reward. This raises the question: _Is a reward recovered via IRL transferable to a newenvironment in the sense that its optimal policy aligns with the expert's true reward?_ For example, in autonomous driving, could we effectively reuse a reward learned from demonstrations of one car in a given city to train or fine-tune a policy for another car in another city?

Ensuring transferability is challenging, as neither the optimal policy corresponding to a reward nor the reward corresponding to an optimal policy is unique. This leads to trivial solutions to the IRL problem, such as constant rewards that make all policies optimal. Common approaches to address this challenge include characterizing the entire set of rewards for which the expert is optimal (Metelli et al., 2021), or assuming the expert is optimal with respect to an entropy regularized RL problem (Ziebart, 2010), leading to many popular IRL and imitation learning algorithms (Ho and Ermon, 2016; Fu et al., 2017; Garg et al., 2021). Entropy regularization results in a unique and more uniform optimal policy, serving as a model for the expert's bounded rationality (Ortega et al., 2015).

In the entropy-regularized setting, several recent works study the set of rewards for which a given expert policy is optimal. In particular, Cao et al. (2021); Skalse et al. (2023) show that under entropy regularization, the expert's reward can be identified up to so-called potential shaping transformations (Ng et al., 1999). The authors of (Schlagnhaufen and Kamgarpour, 2023) extend this result to more general steep regularization. Furthermore, they show that to guarantee transferability to any transition law, the expert's reward needs to be identified up to a constant. The latter can be achieved either by restricting the reward class, e.g., to state-only rewards (Amin et al., 2017), or by learning from multiple experts with the same reward but different transition laws, given that a specific rank condition is satisfied (Cao et al., 2021; Rolland et al., 2022). However, the above results cannot be applied directly in practice, as they rely on having full access to the experts' policies, whereas in practice, we typically only have a finite set of demonstrations available.

ContributionsWe consider the framework of regularized IRL (Jeon et al., 2021) and address the transferability of rewards recovered from a finite set of expert demonstrations.

* We define a novel notion of transferability (Definition 3.1), to address the practical limitation of not having perfect access to the experts' policies. Furthermore, we show that when learning from finite data, the conditions developed under full access to the experts' policies are not sufficient to guarantee transferability (Example 3.3).
* Instead of a binary rank condition, we propose to use principal angles to characterize the similarity and dissimilarity between transition laws (Definition 3.8). Based on these principal angles, we then establish two key transferability results: 1) a guarantee for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws (Theorem 3.10), and 2) a guarantee for transferability to local changes in the transition law when learning from a single expert (Theorem 3.11).
* Assuming oracle access to a probably approximately correct (PAC) algorithm for the forward RL problem, we provide a PAC algorithm for the IRL problem, which in \((K^{2}/^{2})\) steps recovers a reward for which, with high probability, all \(K\) experts are \(\)-optimal (Theorem 4.1). Together with our results on transferability, this establishes end-to-end guarantees for learning transferable rewards from a finite set of expert demonstrations.
* We experimentally validate our results in a gridworld environment (Section 5).1

## 2 Background

NotationGiven \(x\) and \(y\) in some Euclidean vector space \(\), we denote the \(p\)-norm by \(\|x\|_{p}\), the orthogonal projection onto a closed convex set \(\) by \(_{}(x)=_{y}\|x-y\|_{2}\), and the standard dot product by \( x,y\). For a linear operator \(A\), we denote its image and rank by \(A\) and \(A\), respectively. Given two sets \(\) and \(\), we denote \(+\) for their Minkowski sum and \(^{}\) for the set of all functions mapping from \(\) to \(\). Additionally, we denote \(_{}\) for the probability simplex over \(\) and \(\) for the indicator function. The interior \(\), the relative interior \(\), the relative boundary \(\), and the convex hull \(\) of some set \(\) are defined in Appendix A, along with an overview of all other notations.

Regularized MDPsWe consider a regularized MDP (Geist et al., 2019) defined by a tuple \((,,P,_{0},r,,h)\). Here, \(\) and \(\) represent finite state and action spaces with \(||,||>1\), \(_{0}_{}\) the initial state distribution, \(P_{}^{}\) the transition law, \(r^{}\) the reward, and \((0,1)\) the discount factor. Furthermore, \(h:\) is a strictly convex regularizer that is defined on a closed convex set \(^{}\) with \(_{}\). Starting from some initial state \(s_{0}_{0}\) the agent can at each step in time \(t\), choose an action \(a_{t}\), will arrive in state \(s_{t+1} P(|s_{t},a_{t})\), and receives reward \(r(s_{t},a_{t})\). The goal is to find a Markov policy \(_{}^{}\) maximizing the regularized objective \(_{}[_{t=0}^{}^{t}[r(s_{t},a_{t})-h( (|s_{t}))]]\). Following the classical linear programming approach to MDPs (Puterman, 2014), this can be cast equivalently as the convex optimization problem

\[_{}J(r,), J(r,):= r, -(),\] (O-RL)

where \(\) denotes the set of occupancy measures, \(^{}(s,a):=(1-)_{}[_{t=0}^{}^{t }(s_{t}=s,a_{t}=a)]\), and we have \(():=_{(s,a)}[h(^{}(|s))]\), with \(^{}\) being the policy corresponding to \(\) (see Appendix A). The set of occupancy measures is characterized by the Bellman flow constraints

\[=\{_{+}^{}:(E-  P)^{}=(1-)_{0}\}_{ },\]

where \(E:^{}^{}\) and \(P:^{}^{}\) are the linear operators mapping \(v^{}\) to \((Ev)(s,a)=v(s)\) and \((Pv)(s,a)=_{s^{}}P(s^{}|s,a)v(s^{})\), respectively.

Due to the strict convexity of \(h\), the regularized MDP problem has a unique optimal policy (Geist et al., 2019), hence guaranteeing the uniqueness of the optimal occupancy measure in (O-RL). In addition, we assume that the gradients of \(h\) become unbounded towards the relative boundary of the simplex as detailed in Assumption 2.1 below.

**Assumption 2.1** (Steep regularization).: Suppose that \(h:\) is differentiable in \(\) and that \(_{l}\| h(p_{l})\|_{2}=\) if \((p_{l})_{l}\) is a sequence in \(\) converging to a point \(p_{}\).

Assumption 2.1 ensures that the optimal policy is non-vanishing, and together with Assumption 2.2 below, we also have that the optimal occupancy measure is non-vanishing.

**Assumption 2.2** (Exploration).: Let \((s):=_{a}(s,a)_{}>0\) for any \(s\) and \(\).

One way to guarantee Assumption 2.2 is to impose a lower bound on the initial state distribution \(_{0}\). In the following, it will be convenient to denote the optimal solution to (O-RL) for the reward \(r\) as

\[(r):=*{argmax}_{}J(r,),\]

and the suboptimality of some occupancy measure \(\) for the reward \(r\) as

\[(r,):=_{^{}}J(r,^{})-J(r,).\] (1)

That is, \(=(r)\) if and only if \((r,)=0\).

_Remark 2.3_.: As we aim to analyze the transferability of rewards to new transition laws \(P_{}^{}\), it will often be useful to explicitly specify the dependency on \(P\). We do so by adding a subscript - e.g. we write \(_{P}\), \(_{P}\), and \(_{P}\). However, for better readability, we drop these subscripts whenever there is no potential for confusion.

Inverse reinforcement learningGiven a dataset of trajectories sampled from an expert \(^{}\) that is optimal for some reward \(r^{}\), the goal in IRL is to recover a reward \(\), within a predefined reward class \(^{}\), such that the expert is optimal for \(\). That is, ideally, we aim to find a reward in the feasible reward set

\[(^{}):=\{r:^{} (r)\}.\] (2)

However, since we don't have direct access to the expert's policy but only to a finite set of demonstrations, the best we can hope for is an algorithm that with high probability outputs a reward \(\) such that \((,^{})\) is small - i.e. an algorithm that is PAC (Syed and Schapire, 2007).

Reward equivalenceThe reward corresponding to an optimal occupancy is not unique. For example, all rewards in the affine subspace \(r+\), where \(:=(E- P)\) is the subspace of so-called potential shaping transformations, correspond to the same optimal occupancy measure (Ng et al., 1999). From a geometric perspective, the subspace \(=(E- P)\) lies perpendicular to the set of occupancy measures \(\). Therefore, adding an element of \(\) to the reward leaves the performance difference between any two occupancy measures invariant. Hence, it is often convenient to consider these rewards as equivalent (Kim et al., 2021) and to measure distances between rewards in the resulting quotient space. Given a linear subspace \(^{}\), the quotient space \(^{}/\) is the set of all equivalence classes \([r]_{}:=r^{}^{}:r^{}-r}\), which is itself a vector space with addition and multiplication operation defined by \([r]_{}+[r^{}]_{}=[r+r^{}]_{}\) and \(c[r]_{}=[cr]_{}\) for \(c\). Intuitively, \(^{}/\) is the vector space obtained by collapsing \(\) to zero, or in other words, it is isomorphic to the orthogonal complement of \(\). We endow \(^{}/\) with the quotient norm \(\|[r]_{}\|_{2}:=_{v}\|r+v\| _{2}=\|_{v^{}}\|_{2}\) and we say that \(r\) and \(r^{}\) are close in \(^{}/\) if \(\|[r]_{}-[r^{}]_{}\|_{2}\) is small. Moreover, the expert's reward is said to be identifiable up to some equivalence class \([]_{}\) if \((^{})[r^{}]_{}\). In this paper, we will consider the equivalence relations induced by constant shifts, i.e., \(=:=r^{}: r(s,a)=c}\), and by potential shaping transformations, i.e., \(=\). Note that since \(\) is a subspace of \(\) and \(\) is \(||\)-dimensional, \([r]_{}\) is a strict subset of \([r]_{}\) whenever \(||>1\).

## 3 Transferability

In this section, we present our main results on transferability in IRL. To this end, we first introduce the problem of learning \(\)-transferable rewards from multiple experts acting in different environments.

### Problem formulation

Let \(^{}\) be a compact reward class, and suppose we are given access to \(K\) expert data sets,

\[_{k}^{}=\{(s_{0}^{k,i},a_{0}^{k,i},,s_{H^ {}-1}^{k,i},a_{H^{}-1}^{k,i})\}_{i=0}^{N^{ }-1}, k=0,,K-1,\]

consisting of trajectories sampled independently from the experts \(^{}_{P^{0}},,^{}_{P^{K-1}}\). Each expert is optimal for the same unrevealed reward \(r^{}\), but under different transition laws, \(P^{0},,P^{K-1}\). Our goal is to recover a reward \(\) that is transferable across a set of transition laws \(_{}^{}\). Specifically, the optimal occupancy measure corresponding to \(\) should remain approximately optimal for \(r^{}\) under every transition law in \(\). This yields the following definition of \(\)-transferability.

**Definition 3.1** (\(\)-transferability).: Fix some \(>0\). We say that \(\) is \(\)-transferable to some set of transition laws \(_{}^{}\) if \(_{P}(r^{},_{P}())\) for all \(P\). We say that \(\) is exactly transferable to \(\) if it is \(\)-transferable to \(\) with \(=0\).

The error margin of \(\) is crucial, as exact transferability is unrealistic when learning from finite expert data. Moreover, note that Definition 3.1 is a definition of uniform transferability, as it requires \(\) to be \(\)-transferable to any \(P\) with the same fixed \(\). In the following, we will analyze the transferability of a reward \(\) for which all experts are \(\)-optimal for some \(>0\). That is,

\[_{P^{k}}(,^{}_{P^{k}}), k=0,,K-1.\] (3)

In particular, we aim to establish appropriate conditions for choosing \(\) so as to guarantee \(\)-transferability to some set of transition laws \(\). In Section 4, we will then provide an IRL algorithm that, with high probability, outputs a reward \(\) such that (3) holds.

_Remark 3.2_.: As discussed in Appendix J, the assumption of perfect expert optimality with respect to \(r^{}\) can be relaxed to allow for a misspecification error. All our results remain applicable in this setting but include an additional error term due to the experts' suboptimality.

### Related work

Most previous work has focused on reward identifiability. For a single expert, Cao et al. (2021); Skalse et al. (2023); Schlaginhaufen and Kamgarpour (2023) show that under Assumption 2.1 (steepness)the feasible reward set (2) can be expressed as

\[(^{})=(^{})+ =[r^{}]_{}.\] (4)

In other words, steepness ensures that the expert's reward is identifiable up to potential shaping. To identify the reward up to a constant, we can either restrict the reward class, e.g. to state-only rewards as explored by Amin et al. (2017), or learn from multiple experts (Cao et al., 2021; Rolland et al., 2022). In particular, when we are given access to two experts, \(^{}_{P^{0}}\) and \(^{}_{P^{1}}\), we can identify the experts' reward up to the intersection

\[_{P^{0}}(^{}_{P^{0}})_{P^{1}}(^{ }_{P^{1}})=[r^{}]_{_{P^{0}}}[r^{} ]_{_{P^{1}}}=r^{}+_{P^{ 0}}_{P^{1}}.\]

That is, for the unrestricted reward class, \(=^{S}\), the reward is identifiable up to a constant if and only if \(_{P^{0}}_{P^{1}}=\). Or equivalently, if and only if the rank condition

\[\,E- P^{0}, E- P^{1} =2||-1,\] (5)

is satisfied (Rolland et al., 2022). Moreover, Schlaghinhutern and Kamgarpour (2023) show that identifying the expert's reward up to a constant is a necessary and sufficient condition for exact transferability to any full-dimensional set \(^{}_{}\) (a set \(\) whose interior, with respect to the subspace topology on \(^{}_{}\)(Bourbaki, 1966), is non-empty).

LimitationsThe above results assume perfect access to the expert's policy, which isn't realistic. In practice, we can only learn a reward for which the experts are approximately optimal. In Example 3.3 below, we show that under approximate optimality of the experts, the learned reward can perform very poorly in a new environment, even if the rank condition in Equation (5) is satisfied.

**Example 3.3**.: We consider a two-state, two-action MDP with \(==\{0,1\}\), uniform initial state distribution, discount rate \(=0.9\), and Shannon entropy regularization \(h=-\) (see Appendix C). Suppose the expert reward is \(r^{}(s,a)=1\{s=1\}\) and consider the transition laws, \(P^{0}\) and \(P^{1}\), defined by \(P^{0}(0|s,a)=0.75\) and \(P^{1}(0|s,a)=0.25+ 1\,\{s=0,a=0\}\) for some \([0,0.75]\). Also, consider the two experts \(^{}_{P^{0}}=_{P^{0}}(r^{})\) and \(^{}_{P^{1}}=_{P^{1}}(r^{})\), and suppose we recovered the reward \((s,a)=-r^{}\). Then, as detailed in Appendix E, the following holds: 1) We have \(_{P^{0}}(,^{}_{P^{0}})=0\) and \(_{P^{1}}(,^{}_{P^{1}})=()\). That is, for small \(\), the reward \(\) is a good solution to the IRL problem, as both experts are approximately optimal under \(\). 2) The rank condition (5) between \(P^{0}\) and \(P^{1}\) is satisfied for any \( 0\). 3) For a new transition law \(P\) defined by \(P(0|s,a)=1\,\{s=1,a=0\}\), we have \(_{P}(r^{},_{P}()) 4.81\), i.e. \(_{P}()\) performs poorly under the experts' reward.

### Theoretical insights

To establish a sufficient condition for \(\)-transferability, our goal is to bound the suboptimality of an optimal occupancy measure, \((r)\), for some reward \(r^{}\), in terms of reward distances measured in the quotient space \(^{}/\). To this end, we first establish the relationship between the suboptimality in Equation (1) and the Bregman divergence corresponding to the occupancy measure regularization.

Bregman divergencesThe Bregman divergence (Teboulle, 1992) associated to \(\) is defined as

\[D_{}(,^{})=()-(^{})- (^{}),-^{}.\]

**Proposition 3.4**.: _Under Assumptions 2.1 and 2.2, we have \((r^{},)=D_{}(,(r^{}))\) for any \(\)._

Proposition 3.4 above demonstrates that the suboptimality of an occupancy measure \(\) for the reward \(r^{}\) coincides with the Bregman divergence between \(\) and the optimal occupancy measure under \(r^{}\). This generalizes (Mei et al., 2020, Lemma 26) from entropy regularization to any steeply regularized MDP. The proof is presented in Appendix D.6.

Reward approximationNext, we show that under strong convexity and local Lipschitz gradients, the Bregman divergence between two optimal occupancy measures is bounded in terms of reward distances in \(^{}/\).

**Assumption 3.5** (Regularity).: Suppose the following holds:

1. The regularizer \(\) is \(\)-strongly convex over the set of occupancy measures \(\). That is, we have \[(^{})()+(),^{ }-+\|^{}-\|_{2}^{2},,^{}.\]
2. The gradient \(\) is locally Lipschitz continuous over \(\). That is, for any closed convex subset \(\) there exists \(L_{}>0\) such that \[\|()-(^{})\|_{2} L_{ }\|-^{}\|_{2},,^{} .\]

We will show later that Assumption 3.5 is met for Shannon and Tsallis entropy regularization (see Proposition D.9). Under the above assumption, the following lemma establishes the desired upper and lower bound on the Bregman divergence between two optimal occupancy measures with respect to reward distances measured in \(^{S}/\).

**Lemma 3.6**.: _Suppose Assumptions 2.1,2, and 3.5 hold, and let \(r,r^{}\). Then, we have_

\[}}{2}\|[r]_{}-[r^{}]_{ }\|_{2}^{2}(r^{},(r))=D_ {}((r),(r^{}))\|[r]_{}-[r^{}]_{}\|_{2}^{2},\] (6)

_for some problem-dependent constant \(_{}>0\)._

_Remark 3.7_.: The proof of Lemma 3.6 hinges on the duality between equivalence classes of rewards and optimal occupancy measures (see Appendix B). The main idea is to leverage duality of Bregman divergences, and a dual smoothness and strong convexity result in Proposition D.7. A key challenge arises because, by Assumption 2.1, the regularizer cannot be globally smooth. This results in a problem-dependent dual strong convexity constant \(_{}\)(Goebel and Rockafellar, 2008). In Proposition D.9, we will provide a lower bound on \(_{}\) for the specific choices of Shannon and Tsallis entropy regularization. For more details, we refer to the full proof in Appendix D.6.

The above lemma has two key implications: First, the lower bound in (6) implies that if we recover a reward \(\) for which all experts are approximately optimal, then the distance between \(\) and \(r^{}\) can be bounded in the quotient spaces \(^{S}/_{P^{}}\). Second, the upper bound shows that to control the performance of \(_{P}()\) in a new environment \(P\), we need to tightly bound the distance between \(\) and \(r^{}\) in \(^{S}/_{P}\). As distances in \(^{S}/_{P}\) are bounded by distances in \(^{S}/\), this can be achieved by bounding the distance between \(\) and \(r^{}\) in \(^{S}/\). However, revisiting Example 3.3 in light of Lemma 3.6 shows that even though \(\) and \(r^{}\) are close in \(^{S}/_{P^{}}\), this does not guarantee their proximity in \(^{S}/\) and \(^{S}/_{P}\).

**Example 3.3** (continued).: Recall the definition \(_{P^{}}=(E- P^{k})\). Given that in Example 3.3 we have \(_{P^{}}(,^{}_{P^{}})=0\) and \(_{P^{}}(,^{}_{P^{}})= ()\), Lemma 3.6 ensures that \(\) and \(r^{}\) coincide in \(^{S}/_{P^{}}\), and for small \(\), they are close in \(^{S}/_{P^{}}\). However, as illustrated in Figure1(a) this doesn't ensure that \(\) and \(r^{}\) are close in \(^{S}/\) and \(^{S}/_{P}\). In particular, it can be computed that \(\|[]_{_{P}}-[r^{}]_{_{P}}\|_ {2} 1.51\), which by Lemma 3.6 explains the poor transferability to \(P\).

### Sufficient conditions for transferability

With Lemma 3.6 in place, we are set to present our results on \(\)-transferability. Example 3.3 indicates that a sufficient condition for learning transferable rewards from experts (\(K=2\)) should not rely solely on the binary rank condition (5), which only checks if \(_{P^{}}_{P^{}}=\). Instead, we should consider the relative orientation between \(_{P^{}}\) and \(_{P^{}}\). To formalize this, we need to introduce the concept of principal angles between linear subspaces, as outlined in Definition 3.8 below.

**Definition 3.8** (Principal angles (Galantai, 2013)).: Let \(,^{n}\) be two subspaces of dimension \(m n\). The principal angles \(0_{1}(,)_{m}(, )=:_{}(,)/2\) between \(\) and \(\) are defined recursively via

\[(_{i}(,))=_{v,w} v,w\|v\|_{2}=\|w\|_{2}=1,\, v,v_{j} = w,w_{j}=0,\,j=1,,i-1,\]

where \(v_{j},w_{j}\) are the maximizers corresponding to the angle \(_{j}\). For two transition laws \(P,P^{}\), we define \(_{i}(P,P^{}):=_{i}(_{P},_{P^{}})\) and refer to \(_{i}(P,P^{})\) as the \(i\)-th principal angles between \(P\) and \(P^{}\).

Principal angles are the natural generalization of angles between two lines or planes to higher dimensional subspaces. For principal angles between transition laws, we have the following proposition.

**Proposition 3.9**.: _Let \(P,P^{}_{}^{S}\) and \(H_{}=1/(1-)\). Then, we have \(_{1}(P,P^{})=0\) and \((_{}(P,P^{})) H_{}|/||}\|P-P^{}\|\), where \(\|\|\) denotes the spectral norm._

The proof can be found in Appendix D.7. The above result shows that while the first principal angle between two transition laws is always zero, all principal angles are small if the transition laws are close to one another. In Example 3.3, we have \((_{2}(P^{0},P^{1}))=()\), indicating that the second and in this case maximal principal angle is small when \(\) is small (see Appendix E). The following result shows that when learning from two experts, the transferability error is directly controlled by the second principal angle between the experts' transition laws.

**Theorem 3.10**.: _Let \(K=2\), \(_{2}(P^{0},P^{1})>0\), and suppose that Assumptions 2.1,2, and 3.5 hold. If \(_{P^{}}(,_{P^{}}^{}) \) for \(k=0,1\), then \(\) is \(\)-transferable to \(=_{}^{}\) with_

\[=/[_{}(_ {2}(P^{0},P^{1})/2)^{2}].\]

_Sketch of proof._ The main idea of the proof is illustrated in Figure 1(b). First, it follows from Lemma 3.6 that \(\) and \(r^{}\) are \(=/_{}}\)-close in \(^{}/_{P^{}}\) for \(k=0,1\), respectively. From Figure 1(b) we see - using basic trigonometry - that this implies that \(\) and \(r^{}\) are at least \(=/(/2)\)-close in \(^{}/\). As shown in the full proof in Appendix F, the relevant angle, \(\), is the second principal angle \(_{2}(P^{0},P^{1})\). The result then follows from the upper bound in Lemma 3.6. 

Some observations are in order. First, the above theorem shows that the larger the second principal angle between the two experts' transition laws, the better the recovered reward transfers to a new environment. Second, observe that \(_{2}(P^{0},P^{1})>0\) is equivalent to the rank condition (5), as the second principal angle between two subspaces is non-zero if and only if their intersection is at most one-dimensional. Therefore, for exact transferability, Theorem 3.10 requires the rank condition (5) to be satisfied and \(=0\), recovering the results by Cao et al. (2021); Rolland et al. (2022); Schlaginhaufen and Kamgarpour (2023). But in contrast to past results, Theorem 3.10 applies to more realistic scenarios, where \(\) is merely small, not zero. Finally, we note that Theorem 3.10 can be trivially generalized to \(K 2\) experts by replacing \(_{2}(P^{0},P^{1})\) with the maximum of \(_{2}(P^{k},P^{l})\) over \(0 k l K-1\). However, such bounds may be loose for \(K>2\), potentially leaving considerable room for improvement in this setting.

Local transferabilityWhen learning a reward \(\) from a single expert (\(K=1\)), Schlaginhaufen and Kamgarpour (2023) show that, without reducing the dimension of the reward class, \(\) cannot be exactly transferable to any neighborhood of the expert's transition law \(P_{0}\). However, Theorem 3.11 below shows that by allowing for an \(\) of error, we can guarantee transferability to a neighborhood of \(P_{0}\).

Figure 1: (\(a\)) illustrates the equivalence classes \([]_{}\) and \([r^{}]_{}\), corresponding to the transition laws \(P^{0},P^{1},P\) from Example 3.3, for a small \(\), in \(^{}/\). The blue lines correspond to \(P^{0}\), the red lines to \(P^{1}\), and the gray lines to \(P\). Furthermore, the shaded areas illustrate the approximation error around \([r^{}]_{_{P^{}}}\), as guaranteed by Lemma 3.6. (\(b\)) illustrates the uncertainty set for the recovered reward when learning from two experts, as discussed in the proof sketch of Theorem 3.10.

**Theorem 3.11**.: _Let \(K=1\), \(D:=_{r,r^{}} r-r^{}_{2}\), and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \(_{P^{0}}(,^{})\), then \(\) is \(_{P}\)-transferable to \(P_{}^{S A}\) with_

\[_{P}=2\{2/_{},D^{2} (_{}(P^{0},P))^{2}\}/.\]

The above theorem (which is proven in Appendix G) shows that the reward learned from a single expert transfers to transition laws that are sufficiently close to the expert's, where the closeness is measured in terms of the maximal principal angle. In other words, while a large second principal angle between two experts' transition laws, as per Theorem 3.10, ensures that the reward recovered from these two experts is transferable to arbitrary transition laws, a small largest principal angle between two transition laws ensures that a reward recovered in one environment can be successfully transferred to the other environment.

_Remark 3.12_.: As discussed in Appendix H, we can compute the principal angles using a singular value decomposition. Moreover, given estimates \(^{0},^{1}\) of the transition laws \(P^{0},P^{1}\), the error in the estimate of \(_{i}(P^{0},P^{1})\) scales with \((\{||P^{0}-^{0}||,||P^{1}-^{1}||\})\).

RegularizersTo provide more insights about Theorems 3.10 and 3.11, we provide explicit values for the primal and dual strong convexity constants, \(\) and \(_{}\), respectively. To this end, we focus on the Shannon entropy regularization \(h(p)=-(p)\) and the Tsallis-1/2 entropy regularization \(h(p)=-_{1/2}(p)\) as defined in Appendix C. While the Shannon entropy regularization is commonly used in IR (Ziebart, 2010; Ho and Ermon, 2016), the Tsallis-1/2 entropy is more often adopted in the multi-armed bandit literature Zimmert and Seldin (2021). Both regularizations satisfy Assumption 2.1 as well as Assumption 3.5 with the constants detailed in Proposition D.9 in the appendix. In general, the Tsallis entropy leads to a slightly smaller strong convexity constant \(\), but avoids an exponential dependence on the effective horizon \(H_{}=1/(1-)\) in \(_{}\). Below, we summarize the implications of Proposition D.9 for \(\)-transferability of a reward \(\) recovered from two experts.

**Corollary 3.13**.: _Suppose the conditions in Theorem 3.10 hold. Furthermore, let \(H_{}:=1/(1-)\), \(R:=_{r} r_{}\), \(D=_{r,r^{}} r-r^{}_{2}\), and \(<D\). Then, for the Shannon entropy \(\) is \(\)-transferable to \(=_{}^{S A}\) with_

\[=^{2}D||||^{2+H_{}} (}{})}{_{}^{2} (_{2}(P^{0},P^{1})/2)^{2}},\]

_and for the Tsallis entropy with_

\[=H_{}^{5}D||||^{2}( 2R/+3|})^{3}}{_{}^{2}( _{2}(P^{0},P^{1})/2)^{2}}.\]

We observe that transferability generally becomes more challenging with decreasing regularization parameter \(\), i.e. if the expert's policy becomes more deterministic. Furthermore, we see that it is easier to recover a transferable reward in a Tsallis entropy-regularized MDP. Corollary 3.13 also shows that the constant between \(\) and \(\) tends to be large, meaning that we need to recover a reward for which the experts are \(\)-optimal with a very small \(\) to guarantee \(\)-transferability for a reasonable \(\). However, it's important to note that our results provide sufficient conditions for the worst case, and it remains for future work to determine under what conditions these constants can be improved.

_Remark 3.14_.: Our results in this section, especially Proposition 3.4 and Lemma 3.6, are critically relying on the steepness of the regularization (Assumption 2.1), which is essential to ensure that the expert's reward can be identified up to the equivalence class of potential shaping transformations. Although we can still upper bound the suboptimality \((r^{},())\) in terms of the distance between \(\) to \(r^{}\) in \(^{S A}/\) without this assumption (see Proposition D.10), we no longer have a lower bound as in Lemma 3.6, which is essential for establishing closeness of \(\) and \(r^{}\) in \(^{S A}/\). Hence, we expect it to be difficult to obtain guarantees similar to those in Theorem 3.10 and 3.11 for the unregularized setting, without either reducing the dimension of the reward class (Amin et al., 2017) or making specific assumptions about the feasible reward sets (Metelli et al., 2021, Assumption 4.1).

## 4 Algorithm

To provide end-to-end guarantees for recovering transferable rewards from a finite set of expert demonstrations, we analyze the convergence and sample complexity (in terms of expert demonstrations) of an algorithm for recovering a reward for which all \(K\) experts are approximately optimal. To this end, we focus on the reward class \(=\{r^{}:\|r\| _{1} 1\}\). Furthermore, we assume oracle access to a \((,)\)-PAC algorithm for the forward problem (O-RL). That is, a polynomial-time algorithm, \(^{,}\), that outputs a policy \(=^{,}(r)\) such that with probability at least \(1-\) it holds that \((r,^{})\) (see e.g. (Lan, 2023) for a specific example). The key idea of our meta-algorithm is to learn a reward minimizing the sum of the suboptimalities of the \(K\) experts \(_{P^{0}}^{},,_{P^{K-1}}^{}\). This leads us to the following multi-expert IRL problem

\[_{r}_{k=0}^{K-1}_{P^{k}}(r,_{ ^{}_{k}}),\] (O-IRL)

where \(_{^{}_{k}}(s,a):=(1-)/N^{}_ {i=0}^{N^{}-1}_{t=0}^{H^{}-1}^{t}1\{s_{t}^{k,i}= s,a_{t}^{k,i}=a\}\) is an empirical expert occupancy measure. To solve Problem (O-IRL), we propose the projected gradient descent scheme as detailed in Algorithm 1 below, where \(_{P^{k}}(,N,H)\) samples \(N\) independent trajectories of length \(H\) from policy \(\). Using a stochastic online gradient descent analysis, Theorem 4.1 shows that any PAC algorithm for the forward problem yields a PAC algorithm for the inverse problem.

``` Input:\(,T,\{^{}_{k}\}_{k=0}^{K-1},N,H,_{},_{}\). Initialize:\(^{}_{}\) and \(r\) arbitrarily. for\(i=0,,T-2\)do for\(k=0,,K-1\)do \(_{k,t}=^{_{},_{}}_{P^{k}}(r_{t})\)// Forward RL. \(_{k,t}=_{P^{k}}(_{k,t},N,H)\)  end for \(g_{t}=_{k=0}^{K-1}(_{_{k,t}}-_{^{}_{k}})\) \(r_{t+1}=_{}(r_{t}- g_{t})\)// Reward step.  end for Return:\(:=_{t=0}^{T-1}r_{t}\). ```

**Algorithm 1**Multi-expert IRL

**Theorem 4.1**.: _Suppose that \(N^{}=K(||||/)/ ^{2}\) and \(H^{}=(K/)/(1/)\). Running Algorithm 1 for \(T=K^{2}/^{2}\) iterations with step-size \(=1/(K)\), where \(_{}=^{2}/K^{3} \), \(_{}=(/K)\), \(N=K(K||||/() )/^{2}\), and \(H=H^{}\), it holds with probability at least \(1-\) that \(_{P^{k}}(,_{P^{k}}^{}),\;\;\;k=0,,K-1\)._

The above result generalizes (Syed and Schapire, 2007, Theorem 2) by considering multiple experts and by proving convergence in terms of the expert suboptimality. We refer to Appendix I for the proof and the precise constants. Theorem 4.1 shows that with \((K/^{2})\) demonstrations of each expert, we recover in \((K^{2}/^{2})\) steps of Algorithm 1 a reward \(\) for which all experts are \(\)-optimal. Together with Theorem 3.10 and 3.11, this provides a bound on the sample and time complexity of recovering in \(\)-transferable rewards in regularized IRL.

## 5 Experiments

To validate our results experimentally, we adopt a stochastic variant of the WindyGridworld environment (Sutton and Barto, 2018). In this environment, the agent moves to the intended grid cell with a probability of \((1-)\) and is pushed one step further in the direction of the wind with a probability of \(\). Using Algorithm 1, we recover a reward \(\) from demonstrations of two experts, both exposed to the same wind strength \(\) but different wind directions - North and East. The experiments are repeated for a varying number of expert demonstrations \(N^{}\{10^{3},10^{4},10^{5},10^{6}\}\) and wind strengths \(\{0.01,0.1,0.5,1.0\}\). We then test the transferability to two different environments: one with South wind, \(P^{}\), and a zero-wind environment with cyclically shifted actions, \(P^{}\). Figure 2(a) shows that the second principal angle between the two experts' transition laws \(P_{0}\) and \(P_{1}\) increases with increasing wind strength. Moreover, Figure 2(b)-(d) show that both the closeness between \(\) and \(r^{}\) in \(^{S}/\) and the transferability to \(P^{}\) and \(P^{}\) improve with a larger second principal angle, as expected from Theorem 3.10. For a more detailed discussion of the experiments we refer to Appendix K.

## 6 Conclusion

SummaryIn this paper, we investigated the transferability of rewards in regularized IRL. We showed that the conditions established under full access to the experts' policies do not guarantee transferability when learning a reward from a finite set of expert demonstrations. To address this issue, we proposed using principal angles as a more refined measure of the similarity and dissimilarity of transition laws. Assuming a strongly convex and locally smooth regularization, we then showed that if we recover a reward for which at least two experts are nearly optimal, and their environments are sufficiently different in terms of the second principal angle between their transition laws, then the recovered reward is universally transferable. Furthermore, we showed that if two environments are sufficiently similar in terms of the maximal principal angle between their transition laws, rewards learned in one environment can be effectively transferred to the other environment. Additionally, we provided explicit constants for the Shannon and Tsallis entropy, as well as a PAC algorithm for recovering a reward for which all experts are approximately optimal. As a result, we established end-to-end guarantees for learning transferable rewards in regularized IRL. Additionally, we experimentally validated our results through gridworld experiments.

Limitations and future workOur results provide only sufficient conditions for transferability. It would be valuable to investigate necessary conditions to check whether our bounds are tight. Furthermore, extending our analysis to lower-dimensional reward classes could reduce the complexity of learning transferable rewards. Although our paper focuses on discrete state and action spaces, an exciting avenue for future research would be to extend our results to continuous state and action spaces, which are more commonly encountered in practice. We expect that our proof methods can be generalized to this setting, but the analysis will be more intricate due to the infinite-dimensional reward and occupancy measure spaces. Finally, as our work is mainly theoretical, experimental validation on real-world applications could provide valuable insight into the practical aspects and challenges of transferability.

AcknowledgmentsAndreas Schlaginhaufen is funded by a PhD fellowship from the Swiss Data Science Center.

Figure 2: (_a_) shows the second principal angle between the experts, for varying wind strength \(\). (_b_) shows the distance between \(\) and \(r^{}\) in \(^{S}/\) for a varying number of expert demonstrations \(N^{}\) and wind strength \(\). (_c_) and (_d_) show the transferability to \(P^{}\) and \(P^{}\) in terms of \(_{P^{}}(r^{},_{P^{ }}())\) and \(_{P^{}}(r^{},_{P^{}}())\), respectively. The circles indicate the median and the shaded areas the 0.2 and 0.8 quantiles over 10 independent realizations of the expert data.