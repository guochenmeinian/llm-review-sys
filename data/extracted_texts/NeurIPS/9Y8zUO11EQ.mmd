# SWT-Bench: Testing and Validating Real-World

Bug-Fixes with Code Agents

 Niels Mundler\({}^{1}\), Mark Niklas Muller\({}^{1,2}\), Jingxuan He\({}^{1}\), Martin Vechev\({}^{1}\)

\({}^{1}\) Department of Computer Science, ETH Zurich \({}^{2}\) LogicStar.ai

{niels.muendler, mark.mueller, jingxuan.he, martin.vechev}@inf.ethz.ch

mark@logicstar.ai \({}^{2}\)

###### Abstract

Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at github.com/logic-star-ai/SWT-Bench.

## 1 Introduction

As the complexity of software systems increases, rigorous testing is becoming more important than ever to ensure their reliability and correctness. However, while a large portion of these tests aims to reproduce previously reported _issues_(Kang et al., 2023), such issue reproduction is often disliked by professional developers (Straubinger and Fraser, 2023). Therefore, automatic generation of tests reproducing such issues from informal natural language descriptions is a promising path toward improving both code quality and developer productivity. Finally, generated tests can be leveraged as formal specifications to boost the effectiveness of automatic code repair tools (Chen et al., 2023).

However, while automatic code generation, in particular using Code Agents, is an extremely active research area (i.e. Yang et al. (2024); Tao et al. (2024); Zhang et al. (2024); Bouzenia et al. (2024b); OpenDevin (2024); Bouzenia et al. (2024a); Schafer et al. (2024); Alshahwan et al. (2024a)), there is comparatively little work investigating automatic test generation directly. Indeed, while prior work has proposed methods based on symbolic execution (Lukasczyk and Fraser, 2022), specialized transformers (Tufano et al., 2020), and general-purpose LLMs (Li et al., 2023; Alshahwan et al., 2024b; Kang et al., 2023, 2024; Chen et al., 2023b), Code Agents have not been considered in this context, and even less work is applicable to the issue reproduction setting. Finally, large-scale, diverse test-generation datasets are lacking for Python, which is one of the most popular programming languages at the time of writing (TIOBE, 2024; PYPL, 2024) and a focus of Code Agent research.

A Benchmark for Test GenerationIn this work, we propose SWT-Bench, a novel and comprehensive dataset for test generation with the goal of issue reproduction in Python. SWT-Bench contains over \(1\,900\) samples, each consisting of a GitHub issue, a golden code patch resolving the issue by adjusting the code, and a set of golden reference tests, obtained by transforming the popular SWE-Bench (Jimenez et al., 2023) from code repair to test generation. We leverage the fact that any code repair task can be transformed into a test generation task, even in the absence of golden tests, by utilizing the golden code patch for evaluation. Concretely, for every generated test, we determine whether it reproduces the described issue, by checking whether it fails on the original repository but passes after the golden patch is applied. The golden reference tests, used in SWE-Bench for the evaluation of code repair performance, are solutions in this test generation setting. We illustrate this evaluation process of SWT-Bench in Fig. 1. Further, we report the coverage of the code modified by the golden patch as a more fine-grained evaluation metric for generated tests.

Benchmarking Test Generation MethodsWe evaluate various existing test generation approaches on SWT-Bench, including directly prompting state-of-the-art LLMs to generate tests for the given issue, a state-of-the-art issue reproduction method libro(Kang et al., 2023), and different Code Agents adapted to the task of test generation (Yang et al., 2024; Zhang et al., 2024; Aider, 2024). Interestingly, we find that despite being designed for code repair, the Code Agent SWE-Agent outperforms non-agent methods at test generation, both reproducing more issues and achieving higher coverage, and generally find all agents to perform strongly in both areas. However, we still observe significant complementarity between the different approaches, with an ideal ensemble of the best four methods solving \(71\%\) more samples than the best single method. Further, while the performance on code repair and test generation is generally correlated, this does not hold on a per-sample basis. This indicates that reproducing an issue with a test and fixing this issue are distinct tasks of different difficulty. Finally, we find that generated tests can serve as a strong signal for the correctness of proposed code fixes, with SWE-Agent achieving over twice the precision on fixes that pass self-generated tests that failed before the fix was applied.

Key ContributionsOur key contributions are:

* We introduce SWT-Bench, a new benchmark for test-based issue reproduction based on an extensive dataset of real-world software repositories, user issues, code patches, and test cases (SS3).
* We propose to adapt Code Agents to the task of test generation for issue reproduction (SS4).
* We provide an extensive evaluation of SWT-Bench, and demonstrate that, while issue reproduction is generally hard, Code Agents perform well, outperforming prior methods (SS5).

## 2 Related Work

Code DatasetsOver recent years, a variety of code datasets such as HumanEval (Chen et al., 2021), APPS (Hendrycks et al., 2021), and MBPP (Austin et al., 2021) have been proposed to assess the capabilities of code synthesis and repair systems (Lin et al., 2017; Li et al., 2022). However, these largely focus on interview-style coding challenges or function-level code synthesis and do not capture the complexity of real-world codebases. Further, they have been shown to often include insufficient test cases to properly assess the correctness of the generated code (Liu et al., 2023a).

Recently, a range of repository-level code-generation benchmarks (Liu et al., 2023b; Jain et al., 2024) including the popular SWE-Bench (Jimenez et al., 2023) have emerged, as modern LLMs

Figure 1: Evaluation of an SWT-Bench instance. Given an issue description in natural language and the corresponding codebase, the task is to generate tests that reproduce the issue. We considered a test to reproduce the issue if it fails on the codebase before the pull request (PR) is accepted, i.e., before the golden patch is applied, but passes after. We call this a fail-to-pass test (\(F\!\!P\)).

began to saturate the simpler function-level benchmarks. However, none of these benchmarks were designed to assess test generation.

The only dataset for reproducing bugs based on real-world issues, Defects4J (Just et al., 2014), focuses on Java, is outdated, limited in size, and contains only short bug descriptions rather than detailed issue reports. In contrast, SWT-Bench is based on Python, which is better supported by modern Code Agents, contains more recent issue reports, and is significantly larger.

Automated Unit Test GenerationMany approaches have been suggested to automate (unit) test generation leveraging symbolic execution (Lukasczyk & Fraser, 2022), specialized transformers (Tufano et al., 2020), and general purpose LLMs (Li et al., 2023; Alshahwan et al., 2024; Kang et al., 2023; Tufano et al., 2020; Kang et al., 2024; Schafer et al., 2024; Alshahwan et al., 2024; Chen et al., 2023b). Depending on their focus, they can be used to increase test coverage (Alshahwan et al., 2024; Schafer et al., 2024), find edge cases (Lukasczyk & Fraser, 2022), or reproduce reported issues (Kang et al., 2023). Issue-reproducing tests are especially interesting, as they can be used to validate automatically generated code repair candidates and thus improve the precision of code repair systems (Chen et al., 2023a). However, most test-generation approaches are not applicable to issue reproduction. We therefore evaluate the most recent applicable method, libro(Kang et al., 2023), and a range of other LLM-based baselines on SWT-Bench.

Code AgentsOver the last year, LLMs have been equipped with tools to observe and interact with their environment over multiple turns and preserve a state across these turns (Wang et al., 2024). These so-called agents have proven successful on a range of complex tasks, including code repair and synthesis (Bouzenia et al., 2024; OpenDevin, 2024; Zhang et al., 2024; Yang et al., 2024; Tao et al., 2024; Bouzenia et al., 2024; Aider, 2024). Such Code Agents can typically search, read, and edit code using an agent computer interface (ACI) (Yang et al., 2024). In this work, we leverage such Code Agents for generating issue-reproducing tests by changing their instructions.

## 3 Benchmarking Test Generation

In this section, we outline the structure of the proposed benchmark, SWT-Bench, and how we leverage it to measure the capabilities of LLMs and Code Agents for test generation.

### Notation and Definitions

We first introduce the notation to describe codebases, their test suites, and changes to these codebases in the form of patches. We denote a codebase \(R\) after applying patch \(X\) as \(R X\). Several patches can be applied sequentially, i.e. \(R X Y\) is the codebase \(R\) after applying a first patch \(X\) and then a second one \(Y\). When a code patch \(X\) is applied to \(R\), a set of tests \(T\) can be used to check the correctness of the applied patch.

A single test \(s\) can either pass (P) or fail (F) after we execute it within the context of codebase \(R\). We consider a test to fail if an error is thrown during its execution, e.g., an AssertionError or ValueError. Such test errors frequently occur if \(R\) lacks or misimplements the functionality targeted by the test. They can also occur due to other reasons, such as incorrect syntax or formatting of the test \(s\). Conversely, a test passes when running the test triggers no error. We define this process as an execution function: \((s,R)\{P,F\}\).

We consider a test \(s\) to reproduce a described issue \(I\) of \(R\), which is resolved by patch \(X\) if it fails on the original codebase (i.e. \((s,R)=F\)) but passes on the patched codebase (i.e. \((s,R X)=P\)). We denote these fail-to-pass tests with \(F P\) and define \(F F\), \(P P\), and \(P F\) tests similarly. If a test transitions from failing on \(R\) to any state on \(R X\), we denote it as \(F\) and vice versa for \( F\). Further, we consider a set of tests \(T\) to be successful at reproducing the issue \(I\), if it contains at least one \(F P\) test and no \( F\) test, or equivalently \( s T,(s,R)=F s T,(s,R X)=P\).

### Benchmark Overview

To construct SWT-Bench, we leverage the same underlying data as SWE-Bench (Jimenez et al., 2023) and summarize its three-stage construction process here for completeness.

1. Scrape a total of \(\!90\,000\) pull requests (PRs) from 12 popular open-source Python repositories from GitHub.
2. Filter PRs to only include those that were merged, resolved a GitHub issue, and made changes to at least one test file.
3. Filter PRs to feature at least one \(F P\) test, removing PRs that result in installation or runtime errors.

This results in \(2\,294\) task instances, each consisting of a GitHub issue, a golden patch \(X^{*}\) fixing the issue, and a set of golden reference tests \(T^{*}\).

However, we find that for \(311\) instances, the golden patch can not be evaluated without errors or does not fix the described issue reliably, i.e., some tests of \(T^{*}\) fail on \(R X^{*}\). The main reasons are flaky test suites, e.g., django cases where HTTP requests sometimes return 500 Internal Server Error although the related code was not changed, erroneous test suite setup, e.g., the test suite tool tox not allowing external tools invoked in the sphinx setup, and time outs, e.g., when slow tests in the sympy library are run. We exclude these, leaving a total of \(1\,983\) instances in SWT-Bench. To enable cheaper evaluation we create SWT-BenchLite, a subset of \(276\) issues, corresponding to SWE-Bench-Lite.

We summarize key statistics of SWT-Bench in Table 1 and show its repository composition in Fig. 2. While issue descriptions are on average only \(318\) words long, the longest one reaches \(8\,756\) words. Generally, repository complexity is high with on average over \(1\,500\) files and over \(300\,000\) lines of code. Many repositories feature large test suites of \(>120\) and up to \(4\,800\) tests, already covering \(70\%\) of the lines in the to-be-patched code. Most of these existing tests are unaffected by the golden patch with basically no \(F P\) and only \(1.5\ P F\) tests on average. The golden tests remove on average \(0.3\) tests and add another \(2.9\) new test cases, of which roughly two-thirds are \(F P\). The test patches edit on average \(31.8\) lines in 1-2 files. Due to the filtering for unresolved issues during dataset curation, no golden tests are \(F F\) or \(P F\).

### Metrics

We propose two main metrics to evaluate the test generation performance of any method; Success rate (\(\)) and change coverage (\(\)), described below. We further introduce the necessary but insufficient property of patch well-formedness (\(\)).

Success RateThe success rate \(\) measures the portion of instances where the generated tests \(T\) reproduced the issue according to the definition in SS3.1, i.e. at least one test in \(T\) transitions from failing to passing and none fail after applying the patch. This is the most important performance measure, as the presence of \(F P\) and the absence of \( F\) tests are key for test-driven development and automatic code generation. We further report the portion of instances for which at least one Fail-to-Pass (\(F P\)), Fail-to-Any (\(F\)), and Pass-to-Pass (\(P P\)) was generated. While \(F\) tests, i.e., all tests that fail on the original codebase, are not necessarily desirable, only \(F\) tests can result in the reproducing \(F P\) test, whereas \(P\) tests can never reproduce an issue. As \(F\) can further be identified without knowledge of the golden code patch, generation methods can aim to always produce an \(F\) test. Finally, \(P P\) tests indicate that the model generated well-formed and valid, but unrelated tests.

    & & Mean & Max \\  Issue Text & \# Words & 318.0 & 8756 \\ Codebase & \# Files & 1563.0 & 2757 \\  & \# Lines & 337K & 772K \\   & \# \(F P\) & 0.0 & 4 \\  & \# \(F F\) & 5.0 & 183 \\ Existing Tests & \# \(P P\) & 116.7 & 4837 \\  & \# \(P F\) & 1.5 & 607 \\  & \# Total & 123.2 & 4842 \\  & Coverage & 70.1\% & 100\% \\   & \# \(F P\) & 2.0 & 958 \\  & \# \(P P\) & 0.9 & 339 \\ Golden Tests & \# Added & 2.9 & 958 \\  & \# Removed & 0.3 & 104 \\  & \# Files Ed. & 1.5 & 15 \\  & \# Lines Ed. & 31.8 & 581 \\   

Table 1: Characterization of different attributes of SWT-Bench instance.

Figure 2: Distribution of SWT-Bench instances over GitHub repositories.

Change CoverageCoverage is an important metric to determine what portion of a codebase is tested. While path coverage measures this optimally, the exponential number of paths makes it infeasible in practice. We thus follow common practice, and instead measure line coverage. As we aim to specifically test the code described in the issue text, we consider only the coverage of the changes made by the golden code patch. Further, we observe that patches may include portions of non-executable lines, e.g. documentation or configuration files, and exclude them. Specifically, we consider all lines that are executed by the original test suite \(T^{R}\) or the golden test suite \(T^{*}\) on both \(R\) and \(R X^{*}\) to be executable, and track coverage of such executable lines.

Finally, we consider both the coverage of removed (including modified) lines of code in the original codebase and added (including modified) lines of code in the patched codebase, illustrated in Fig. 3.

Formally, given the number of times \(^{R}_{T^{R}}(l)^{ 0}\) a specific line of code \(l\) was executed when running the test suite \(T^{R}\) on codebase \(R\), we define the executable lines of the patch \(X\) as

\[^{*}_{r} =\{l X_{r}^{R}_{T^{R}}(l)+^{R}_{T^{ *}}(l)>0\}\] \[^{*}_{a} =\{l X_{a}^{R X}_{T^{R}}(l)+^{R  X}_{T^{*}}(l)>0\}\]

where \(X_{r}\) and \(X_{a}\) are the lines added and removed by patch \(X\), respectively, and \(T^{*}\) are the golden tests. Finally, we obtain the change coverage of the generated tests \(T\) as

\[^{X}_{T}=^{*}_{r}^{R} _{T^{R} T}(l)>^{R}_{T^{R}}(l)\}|+|\{l^{*}_{a} ^{R X}_{T^{R} T}(l)>^{R X}_{T^{R}}(l) \}|}{|^{*}_{r}|+|^{*}_{a}|}.\]

Where \(X\) and \(T\) are clear from context, we drop them for notational clarity. If none of the lines modified by the golden patch \(X\) are executed by any test, i.e., \(|^{*}_{r}|+|^{*}_{a}|=0\), we exclude this instance from our coverage analysis (\(1\%\) of cases).

Patch Well-FormednessMany LLMs struggle to generate well-formed code patch files (Jimenez et al., 2023) and the methods we investigate employ different approaches to mitigate this issue. To assess them, we additionally measure the patch applicability \(\) as the portion of instances for which a well-formed patch was generated. We define \(\) as the portion of instances for which the generated patch \(X\) can be applied to the original codebase \(R\) without errors. Since well-formedness is necessary for any test to be executed, it always exceeds \(\), \(F\!\!P\), and related rates.

## 4 Automatic Test Generation

We first discuss how the test generation task differs from code repair, before introducing a novel code diff format based on these insights that is optimized for fault tolerance. Finally, we propose a range of test generation methods based on directly querying LLMs and leveraging Code Agents.

### Test Generation vs Code Repair

Automatic test generation is closely related to code repair: Instead of predicting a patch \(X\) that fixes the described issue and is then evaluated using a golden test \(T^{*}\), we aim to predict reproducing tests \(T\) which are then evaluated on both the original state of the codebase \(R\) and the state after applying the golden code patch \(X^{*}\). However, there are some key differences between the two tasks: First, adapting an existing test suite to reproduce an issue typically only requires adding new tests. Concretely, \(71\%\) of golden tests in SWT-Bench only add new test functions, with another \(28\%\) modifying existing functions, and only \(1\%\) removing functions. Second, testing permits and requires a more granular analysis. While fixed code is either correct and passes all test cases or incorrect when failing any of them, generated tests can be correct but irrelevant to the issue (\(P\!\!P\)), call relevant code but fail to expose the precise bug (increase in coverage), reproduce the issue with varying comprehensiveness on edge cases (\(F\!\!P\), with varying coverage), or fail in different ways.

Figure 3: Illustration of change coverage \(\) of the generated tests \(T\), given the test suite \(T^{R}\) of the original code base \(R\), the golden patch \(X^{*}\), and the golden tests \(T^{*}\).

### A Code Diff Format for Automatic Test Generation

Code changes are typically represented in the unified diff format, i.e., in the git patch and diff format. While using this format to represent code changes is both precise and human-readable, it is very sensitive to misspecifications, requiring, e.g., the exact line numbers of code changes to be specified and specific code snippets (including all to-be-changed lines) to be repeated verbatim. As a result, many LLMs struggle to produce well-formed patch files Jimenez et al. (2023). Even when loosening the strict diff requirements and fuzzy-matching the generated diff to a best-fit part of the code, GPT-4 only succeeded in \(48\%\) of cases, resulting in only \(10\) correctly reproduced issues.

To alleviate this issue, we propose an adjusted patch format optimized for LLM generation that is easier to adhere to and more robust. Specifically, our custom diff format allows entire functions or classes to be inserted, replaced, or deleted, given the full function or class definition and (fault-tolerant) location in the code. We show an example in Fig. 4, comparing it to the unified diff format. Based on whether the model wants to rewrite an existing function or insert a new function, the provided code is then substituted or inserted at the code location. This format is particularly well suited for test generation which usually only requires adding test functions. We provide a more formal description of this format in App. A and demonstrate its effectiveness in SS5.

### Direct LLM Generation of Tests

We consider four baselines for test generation: Direct zero-shot prompting with the unified patch format (ZeroShot), zero-shot prompting with our novel patch format (ZeroShotPlus), selecting the best out of 5 patches using an oracle Pass@5), and the state-of-the-art test generation method, libroKang et al. (2023), which uses a range of heuristics to pick the most promising among multiple generated tests. In all methods, the LLM is instructed to add tests to reproduce and cover the described issue in the codebase. We describe these methods below, deferring further details to App. E.

ZeroShot prompts the model with the issue description, a subset of the codebase retrieved using BM-25 Robertson and Zaragoza (2009), and instructions to generate a patch file in unified diff format. This method corresponds to the LLM-only baseline in SWE-Bench Jimenez et al. (2023).

ZeroShotPlus is similar to ZeroShot but leverages our custom diff format, discussed in SS4.2, which is optimized for LLMs and robustness to minor specification errors.

Pass@5 uses our ZeroShotPlus prompting scheme to generate 5 proposal tests and then uses an oracle to pick the best one. While this is of course not practical in a real-world setting, it allows us to assess the potential of the LLM to generate good test cases given an effective selection mechanism. libroKang et al. (2023), is the current state-of-the-art for LLM-based test generation. Similar to Pass@5 it generates multiple proposal tests using ZeroShotPlus prompting. However, instead of using an oracle, it combines multiple heuristics to select the best test cases. In particular, it runs all generated tests and then selects the one inducing an error that is most similar to the problem description. This permits not only checking whether a generated diff is well-formed and the proposed test fails on the original codebase but also selecting the most relevant test case. As libro was originally proposed for Java, we adapt it to our Python setting, as detailed in App. B.

### Code Agents for Test Generation

LLM-based agents are systems that take actions based on LLM-generated text, providing tools to observe and interact with their environment over multiple turns and preserve some state across these

Figure 4: Comparison of the default unified diff format (left) and our fault-tolerant version (right).

turns. In the case of Code Agents, they can typically search, read, and edit code using an agent computer interface (ACI) (Yang et al., 2024). Recent work has shown that such Code Agents are particularly effective for complex repository-level code synthesis and repair tasks, outperforming unaided LLMs by a significant margin (Bouzenia et al., 2024; OpenDevin, 2024; Zhang et al., 2024; Yang et al., 2024; Tao et al., 2024). In this work, we leverage Code Agents for automatic test generation by adjusting their instructions. Specifically, we adapt SWE-Agent(Yang et al., 2024), aider(Aider, 2024), and AutoCodeRover(Zhang et al., 2024).

SWE-Agent(Yang et al., 2024) provides the LLM direct access to (augmented) command line tools and processes the output of these tools to be more easily parseable by an LLM. In particular, they provide special tools for searching, viewing, and editing files. Beyond initial instructions, they provide little guardrails or structure for the LLM and let it interact with a limited shell environment.

aider(Aider, 2024) performs a repository indexing step to guide file selection and then includes all selected files in the next prompts. Further, model-generated summaries and reflections on previous actions are leveraged to augment the context. Before an edit is applied, it undergoes validation via static analysis and repository test cases using project-specific evaluation harnesses.

AutoCodeRover(Zhang et al., 2024) separates the code repair task into two distinct stages. In the first stage, the LLM is tasked with collecting all required context for the task at hand. To this end, it is equipped with a range of advanced code search and navigation tools, allowing it, e.g., to retrieve class signatures, function definitions, or surrounding code snippets. Once the LLM believes it has gathered sufficient context, it proceeds to the second stage. There, the LLM is tasked with generating the actual code patch in a single step, retrying only if the patch can not be applied.

Adapting Code Agents for Test GenerationAs SWE-Agent, aider, and AutoCodeRover were designed for program repair, we adapt their system and instruction prompts to focus on creating high-quality test cases. We find that the underlying LLMs are capable of following these changed instructions and successfully generate test cases for up to \(87\%\) of issues. Typically, the instruction changes were as simple as replacing phrases like "solve the issue" with "create unit tests that cover the issue". We provide a more detailed description of the used prompts in App. E.

We experiment with instructing SWE-Agent explicitly to execute the generated test cases before submitting them. We call this variant SWE-Agent+ and find that this increases the success rate \(\) from \(15.9\%\) to \(18.5\%\) (see Table 2). Note we do not provide any information on _how_ to run the tests. This contrasts the libro setting, in which the test execution commands are assumed to be known.

## 5 Experimental Evaluation

We leverage SWT-Bench to compare the performance of different test generation methods and underlying LLMs (SS5.2), their relation with the code repair setting (SS5.3), and the impact of instance characteristics (SS5.4). We further explore hyperparameter ablations in App. C.

### Experimental Setup

We consider GPT-4 (gpt-4-1106-preview OpenAI 2023), GPT-4o mini (gpt-4o-mini-2024-07-18 OpenAI 2024), Claude 3.0 Haiku(Anthropic, 2023), Claude 3.5 Sonnet (Anthropic, 2024), Mistral Large 2 (Team, 2024b) (served via the Mistral AI API), and Mistral 7x22b (Team 2024a served by Together AI TogetherAI 2023), as underlying LLMs, using GPT-4 unless indicated otherwise. We sample at temperature \(t=0\) for all zero-shot methods and agents and at \(t=0.7\) for libro and Pass@5. For SWE-Agent, aider, and AutoCodeRover, we use their default settings, restricting the number of API calls to 20, reflection steps to 3, and interaction rounds to 10, respectively. For libro we sample 5 tests. Due to budget constraints, we focus our evaluation on SWT-Bench-Lite. In App. C we explore and justify this choice of hyperparameters in detail.

### Automatic Test Generation

Comparing Test Generation MethodsWe compare test generation performance in Table 2 where all methods have access only to the issue description and the original codebase. We observe that using the original git code-diff format, ZeroShot only generates well-formed patchesfor \(48.6\%\) of issues. Using our novel test-specific code-diff format (ZeroShotPlus) boosts this rate to \(89.5\%\) yielding an almost 3x increase in success rate (\(\)) to \(9.4\%\). While picking the best among five generated tests (Pass@5) even yields an \(\) of \(20.3\%\), the heuristics employed by libro can only convert about half of this gap into an \(\) of \(14.1\%\), still beating AutoCodeRover and aider SWE-Agent, however, outperforms libro at an \(\) of \(15.9\%\), increased to \(18.5\%\), when instructed to check its generated tests (SWE-Agent+). This stronger performance is significant at \(p<0.1\%\). Interestingly, SWE-Agent produces fewer \(F\) tests than aider and libro despite having much higher applicability and yielding a higher \(\).

We conclude that general-purpose Code Agents already perform as well as domain-specific test generation methods, with simple test-specific adjustments providing significant improvements.

Coverage of Generated TestsWe analyze the change coverage \(\) of the generated tests, i.e., the portion of executable golden patch code that is covered by the generated tests, in Table 3. Across all methods, we observe a significantly higher coverage on successful instances (\(^{}\)), indicating that coverage is indeed correlated with test quality but more granular than \(\). Interestingly, SWE-Agent+ achieves notably higher coverage on successful instances than SWE-Agent highlighting the impact of providing agents with more test-generation-specific tools to identify promising tests. Further, libro achieves lower coverage than most Code Agents, most likely as a consequence of preferring shorter tests.

Model EffectWe compare the effect of different underlying LLMs for SWE-Agent in Table 4. We observe that not only \(\) but even applicability (\(\)) is highly sensitive to the underlying LLM's performance, with Haiku, GPT-4o mini, and Mixtral achieving significantly lower performance than GPT-4. More capable models like Claude 3.5 Sonnet and Mistral Large 2 perform on par, with the latter even outperforming GPT-4.

### Code Repair and Test Generation

Test Generation for a Given Code PatchTo assess the effectiveness of automatic test generation at testing specific, provided fixes, we investigate the effect of providing a (possibly incorrect) code patch, the files it changed, and the test file to be modified instead of the files retrieved with BM25, reporting results in

   Model & \(\) & \(\) & \(F\!\!\) & \(^{}\) \\  Mistral L. 2 & \(76.1\) & **16.3** & \(51.4\) & \(23.0\) \\ GPT-4 & **87.3** & \(15.9\) & \(48.2\) & \(26.5\) \\ Cl. 3.5 Sonnet & \(67.8\) & \(12.3\) & **59.1** & **30.3** \\ GPT-4o mini & \(71.0\) & \(9.8\) & \(36.2\) & \(20.9\) \\ Cl. 3.0 Haiku & \(20.3\) & \(2.5\) & \(6.9\) & \(3.0\) \\ Mixtral 8x22B & \(3.3\) & \(0.7\) & \(1.8\) & \(0.9\) \\   

Table 4: Comparison of different underlying LLMs for SWE-Agent, all in %.

   Method & \(\) & \(\) & \(F\!\!\) & \(F\!\!P\) & \(P\!\!P\) \\  Golden & \(100.0\) & \(100.0\) & \(100.0\) & \(100.0\) & \(11.2\) \\ Pass@5 & \(93.1\) & \(20.3\) & \(62.7\) & \(22.1\) & \(7.2\) \\  ZeroShot & \(48.6\) & \(3.6\) & \(38.8\) & \(5.8\) & \(3.6\) \\ ZeroShotPlus & \(89.5\) & \(9.4\) & \(55.4\) & \(10.1\) & **7.2** \\ libro & **92.0** & **14.1** & **60.1** & **15.2** & **7.2** \\  AutoCodeRover & \(47.1\) & \(9.1\) & \(43.8\) & \(9.1\) & \(7.6\) \\ aider & \(66.7\) & \(12.7\) & **57.6** & \(17.0\) & \(8.7\) \\ SWE-Agent & **87.3** & \(15.9\) & \(48.2\) & \(16.7\) & \(9.8\) \\ SWE-Agent+ & \(85.5\) & **18.5** & \(46.4\) & **19.2** & **10.1** \\   

Table 2: Rate of well-formed patches (\(\)), successful tests (\(\)), potentially reproducing initially failing tests (\(F\)), reproducing fail-to-pass tests (\(F P\)), and correct but unhelpful pass-to-pass tests (\(P\!\!P\)), in %.

   Method & \(^{}\) & \(^{}\) & \(^{}\) \\  Golden & \(72.0\) & \(72.0\) & - \\ Pass@5 & \(31.3\) & \(65.6\) & \(22.5\) \\  ZeroShot & \(7.6\) & \(34.9\) & \(6.6\) \\ ZeroShotPlus & \(21.5\) & **76.7** & \(15.7\) \\ libro & **23.8** & \(64.2\) & **17.0** \\  AutoCodeRover & \(17.9\) & \(61.3\) & \(13.6\) \\ aider & **27.8** & \(59.5\) & **23.1** \\ SWE-Agent & \(26.5\) & \(64.7\) & \(19.1\) \\ SWE-Agent+ & \(27.6\) & **69.4** & \(18.0\) \\   

Table 3: Change Coverage \(\) [%] as defined in ยง3.3 aggregated over all instances, \(\) instances and non \(\) instances (\(\)).

Table 5 in %. We use ZeroShotPlus to generate incorrect patches, resampling \(\)\(5\) times and excluding instances where we could not generate an incorrect but applicable patch, reducing the sample size to \(n=172\). Providing the test files to change almost doubles \(\) from \(8.1\%\) to \(15.1\%\), pulling even with SWE-Agent. We observe that meanwhile providing a code patch and the files it changed has a much smaller impact, increasing \(\) only to \(10.5\%\) for both the golden patch and an incorrect patch. This highlights the importance of retrieving the correct context for generating relevant tests. Meanwhile, GPT-4 is able to leverage the correct patch, and to improve the coverage increase of the relevant lines by almost \(50\%\), from \(12.5\%\) to \(18.4\%\).

Filtering Code Fixes with Generated TestsState-of-the-art code generation methods only resolve around \(20\%\) of cases on SWE-Bench-Lite (Jimenez et al., 2023). Without suitable tests to distinguish correct from incorrect fixes, the overhead from manual testing (Yang et al., 2008) would thus outweigh any benefits from automatic code generation. To address this issue, we use SWE-Agent to generate both bug fixes and tests, in a similar manner to Chen et al. (2023a). We then filter the generated bug fixes, retaining only those where all generated tests are \(F\!\!P\) or \(P\!\!P\). While only achieving \(20\%\) recall, this more than doubles the precision of SWE-Agent to \(47.8\%\), making it significantly more practically useful, highlighting the importance of test generation, and opening an avenue to transferring the results from Chen et al. (2023a) towards more complex and realistic scenarios with more expensive inference and evaluation steps.

Correlation of Test Generation and Code RepairWe analyze the overlap between solved instances of SWE-Bench and SWT-Bench, showing results in Table 6. We observe that the overlap is small for both methods, with no statistical evidence of correlation (p-values of \(80.4\%\) and \(72.8\%\) for ZeroShotPlus and SWE-Agent, respectively, under the null hypothesis of independence and uniform hardness), indicating that generating tests and fixes are distinct tasks of different difficulties. We explore this relationship in more detail in App. D.

### Test Generation Success and Instance Characteristics

Effect of Issue Description LengthWe investigate the relationship between issue description length and test generation performance in Fig. 5. We observe a general trend that issues with longer descriptions are easier to generate tests for, with all methods achieving a higher \(\) for longer descriptions, however tending to slightly decrease for very long descriptions. This is likely due to the increased amount of information available in longer descriptions, while too-long descriptions may contain many distractors and make it difficult to extract relevant information for the LLM. SWE-Agent+, which actively summarizes context, limiting file content and reducing history, is least sensitive to issue description length, achieving approximately the same \(\) for all but the shortest lengths.

Effect of Data ContaminationAs SWT-Bench is based on historic GitHub issues, they may be contained in the pre-training data of the LLMs we use. To investigate this issue, we conducted an experiment comparing the performance of ZeroShotPlus on all issues created after the Knowledge Cutoff (KC) of GPT-4 (April 2023) to a random subset of the same size of instances created before, and report the results in Table 7. While we observe a small performance difference, we can not confirm its statistical significance (\(p 37\%\)) due to the low number of samples created after the KC. Further,

   & SWT & SWE & Overlap & p-Value [\%] \\ 
2eroshotPlus & 26 & 16 & 1 & 80.4\% \\
5WE-Agent & 44 & 50 & 7 & 72.8\% \\  

Table 6: Overlap in solved instances of SWE-Bench and SWT-Bench.

Figure 5: Distribution of success rate (\(\)) across issue description lengths in # tokens

   PR created & \(n\) & \(\) & \(\) & \(F\!\!\) & \(F\!\!P\) & \(P\!\!P\) & \(\)\({}^{}\) \\  before \(KC\) & 83 & 56.6 & 6.0 & 42.2 & 8.4 & 4.8 & 35.9 \\ after \(KC\) & 83 & 47.0 & 4.8 & 39.8 & 4.8 & 3.6 & 35.9 \\  

Table 7: Performance of ZeroShotPlus on PRs before/after GPT-4 knowledge cutoff (\(KC=\) 30th April 2023) in %.

all methods in Table 2 use the same underlying LLM and should thus benefit from any potential contamination to a similar degree, allowing for a fair comparison between different methods.

Method ComplimentarityWe consider four diverse methods from SS5.2 and analyze the overlap in the instances for which they are able to generate successful tests. We show the results in Fig. 6. While the best-performing approach, SWE-Agent+, alone is able to solve \(51\) instances, the combination of all four approaches is able to solve \(87\) instances, highlighting the benefit of exploring a variety of approaches for test generation.

## 6 Limitations and Future Work

While our novel SWT-Bench covers a wide range of real-world issues, it has several limitations: It is limited to Python, which may limit the generalizability of our findings to other programming languages. Second, the dataset is based on popular GitHub repositories, which may not be representative of common software development practices and does preclude the generation of a private holdout test set. Finally, the dataset is limited to bug reproduction and issues that can be easily covered by adding test cases and does not measure edge case detection or global coverage increase.

Further, as discussed in SS5.4, most issues in SWT-Bench have been created before the knowledge cutoff of state-of-the-art models, posing a risk for data contamination. One approach to address this issue is to create a rolling version of SWT-Bench, based only on the most recently created GitHub issues. However, this comes at the cost of direct comparability of results and increased cost for reproducing results for all baselines on a changing evaluation set.

Addressing these limitations would be an interesting direction for future work. As concrete starting points, we found several common errors even in the best performing method SWE-Agent+ that could be addressed through specialized monitoring: Adding passing tests that do not reproduce the given issue, getting stuck in loops after generating inapplicable edit commands, failing to execute the test environment correctly and adding tests with syntax errors or using invalid variables.

## 7 Conclusion

We proposed SWT-Bench, a novel benchmark for generating reproducing tests from GitHub issue descriptions and the corresponding code bases. SWT-Bench leverages the dataset underlying the popular SWE-Bench which additionally contains a golden patch fixing the described issue. We judge whether a generated test reproduces the described issue by checking whether the test fails before applying this golden patch and succeeds afterward. We measure both the rate of such fail-to-pass tests and the coverage of the golden patch, providing a corresponding evaluation harness. We evaluated a variety of LLM-based test generation methods including Code Agents on SWT-Bench and found that Code Agents already outperform other approaches with only minor adaptations for the test-generation task. Finally, we demonstrated the great potential of generated tests to serve as a signal for the correctness of code fixes, i.e., we double the precision of Code Agents by filtering the generated patches to only those that cause a previously failing self-generated test to pass.

Figure 6: Overlap in instances solved by the four best performing methods.