# Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift

Jiawei Ge

Operations Research & Financial Engineering

Princeton University

jg5300@princeton.edu

&Debarghya Mukherjee

Department of Mathematics and Statistics

Boston University

mdeb@bu.edu

&Jianqing Fan

Operations Research & Financial Engineering

Princeton University

jqfan@princeton.edu

equal contribution

###### Abstract

As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through real-world datasets, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.

## 1 Introduction

In the modern era of big data and complex machine learning models, extensive data collected from diverse sources are often used to build a predictive model. However, the assumption of independent and identically distributed (i.i.d.) data is frequently violated in practical scenarios. Take algorithmic fairness as an example: historical data often exhibit sampling biases towards certain groups, like females being underrepresented in credit card data. Over time, the differences in group proportions have diminished, leading to distribution shifts. Consequently, models trained on historical data may face shifted distributions during testing, and proper adjustments are needed. Distribution shift has garnered significant attention from statistical and machine learning communities under various names, i.e., transfer learning , domain adaptation , domain generalization , continual learning , multitask learning etc. While numerous methods are available in the literature for training predictive models under distribution shift, uncertainty quantification under distribution shift has received relatively scant attention despite its crucial importance. One notable exception is conformal prediction under distribution shift;  proposed a variant of standard conformal inference methods to accommodate test data from a distinct distribution from the training data under the covariate shift. Recently,  introduced an adaptive conformal inference approach suitable for continuously changing distributions over time. Additionally, quantile regression under distribution shift offers another avenue for addressing uncertainty quantification under distribution shift .

Although few methods exist for constructing prediction intervals under distribution shift, most focus primarily on ensuring coverage guarantee rather than minimizing interval width. This prompts the immediate question:

_Can we generate prediction intervals in the target domain that provide both i) coverage guarantee and ii) minimal width?_

This paper seeks to address this question by leveraging model aggregation techniques . Suppose we have \(K\) different methods for constructing prediction intervals in the _source_ domain. Our proposed approach efficiently combines these methods to produce prediction intervals in the _target_ domain with adequate coverage and minimal width. When individual methods are the elementary basis functions, such as the kernel basis, the resulting aggregation is indeed a construction of the prediction interval based on the basis functions. Our methodology draws inspiration primarily from recent work  on prediction interval aggregation under the i.i.d. setting. However, a key distinction lies in our focus on _unsupervised domain adaptation_, where we can access labeled samples from the source and unlabeled samples from the target domain. Certain assumptions regarding the similarities between these domains are necessary to facilitate knowledge transfer from the source to the target domain. We explore two types of similarities in this paper: i) _covariate shift_, where we assume that the distribution of the response variable \(Y\) given \(X\) is consistent across both domains, albeit the distribution of \(X\) may differ, and ii) _domain shift_, where we assume that the conditional distribution of \(Y\) given \(X\) remains unchanged up to a measure-preserving transformation. Covariate shift is a well-explored concept in transfer learning and has also garnered attention in uncertainty quantification. It allows different distributions of \(X\) while maintaining identical conditional distributions \(Y|X\) across domains. For constructing conformal prediction intervals within this framework, see  and references therein. On the other hand, _distribution shift_ is more general, allowing both the distribution of \(X\) and the conditional distribution of \(Y|X\) to differ across domains. Our methods in this context draw upon domain matching principles via transport map, as proposed in  and further elaborated in subsequent works like , among others. The key assumption is the existence of a measure-preserving/domain-aligning map \(T\) from the target to the source domain, such that the conditional distribution of \(Y|X\) on the target domain matches \(Y|T(X)\) on the source domain, i.e., conditional distributions matches upon domain alignment. The case where the domain-aligning map is the optimal transport map has received considerable attention in the literature, e.g., see . Empirical evidence supports the efficacy of domain alignment through optimal transport maps across various datasets. For instance, in , a variant of this method is applied for domain adaptation in image recognition tasks, such as recognizing similarities between USPS , MNIST , and SVHN digit images , as well as between different types of images in the Office-home dataset , including artistic and product images. Additionally, in , the authors explore the impact of domain alignment via optimal transport maps on the face recognition problem, where different poses give rise to distinct domains. However, most of these works concentrate on training predictors that perform well on the target domain without any guarantee regarding uncertainty quantification. To our knowledge, this is the first work to propose a method with rigorous theoretical guarantees for constructing prediction intervals on the target domain under the domain-aligning assumption within an unsupervised domain adaptation framework. We now summarize our contributions.

**Our Contributions:** This paper introduces a novel methodology for aggregating various prediction methods available on the source domain to construct a unified prediction interval on the target domain under both covariate shift and domain shift assumptions. Our approach is simple and easy to implement and requires solving a convex optimization problem, which can even be simplified to a linear program problem in certain scenarios. We also establish rigorous theoretical guarantees, presenting finite sample concentration bounds to demonstrate that our method achieves adequate coverage with a small width. Furthermore, our methodology extends beyond model aggregation; it can be used to construct efficient prediction intervals from any convex collection of candidate functions. In the paper, we adopt this broader perspective, discussing how the aggregation of prediction intervals emerges as a particular case. Lastly, we validate the effectiveness of our approach by analyzing real-world datasets.

We also want to highlight the differences between our method and a related method proposed in . We deal with unsupervised domain adaptation, i.e., we do not observe any label from the target domain, in contrast to , which only deals with i.i.d. data. Hence, significant changes in methodology are required to address the domain shift. Furthermore, as pointed out in Section 3, the shift may cause the optimization problem non-convex, for which we need to introduce a convex surrogate (e.g., the hinge function), leading to additional theoretical challenges.

## 2 Notations and preliminaries

NotationThe covariates of the source and the target domains are denoted by \(_{S}\) and \(_{T}\), respectively, and \(:=_{S}_{T}\). The space of the label is denoted by \(\). We use the notation \(_{}\) (resp. \(_{T}\)) to denote the expectation with respect to the source (resp. target) distribution. The expectation with respect to sample distribution is denoted by \(_{n,S}\) and \(_{n,T}\). We use \(p_{S}\) (resp. \(p_{T}\)) to denote the probability density function of \(X\) on the source and the target domain, respectively. Throughout the paper, we use \(c\) to denote universal constants, which may vary from line to line.

### Problem formulation

Our setup aligns with the unsupervised domain adaption; we assume to have \(n_{S}\) i.i.d. labeled samples \(\{X_{S,i},Y_{S,i}\}_{i=1}^{n_{S}}_{S}(X,Y)\) from the source domain, and \(n_{T}\) i.i.d. unlabeled samples \(\{X_{T,i}\}_{i=1}^{n_{T}}_{T}(X)\) from the target domain. Given any \(>0\), ideally, we want to construct a valid prediction interval with minimal width on the target domain:

\[_{u,l}_{T}[u(X)-l(X)],\ \ \ \ _{T}(l(X)  Y u(X)) 1-\,.\] (2.1)

In many practical contexts, the preferred prediction interval takes the form of \(m(X) g(X)\), where \(m(X)\) is a predictor for \(Y\) given \(X\) (an estimator of \(_{T}[Y X]\)), and \(g(X)\) gauges the uncertainty of the predictor \(m(X)\). The optimizer of (2.1) takes this simplified form when the distribution of \(Y-_{T}[Y X]\) is symmetric around \(0\). Moreover, it offers a straightforward interpretation as the pair \((m,g)\) is a predictor and a function quantifying its uncertainty. Within the framework of this simplified prediction interval, we need to estimate \(m\) and \(g\). Estimating the conditional mean function \(m\) is relatively easy and has been extensively studied; one may use any suitable parametric/non-parametric method. Upon estimating \(m\), we need to estimate \(g\) so that the prediction interval \([m(X) g(X)]\) has both adequate coverage and minimal width. This translates into solving the following optimization problem:

\[_{f}_{T}[f(X)],\ \ \ \ _{T}((Y-m(X))^{2}>f(X))\,.\] (2.2)

Let \(f_{0}\) be the solution of the above optimization problem. Then the optimal prediction interval is \([m_{0}(x)(x)}]\). However, the key challenge here is that we do not observe the response variable \(Y\) from the target, and consequently, solving (2.2) becomes infeasible. Hence, we must rely on transferring our knowledge acquired from labeled observations in the source domain, which necessitates making certain assumptions regarding the similarity between the two domains. Depending on the nature of these assumptions regarding domain similarity, our findings are presented in two sections: Section 3 addresses covariate shift under the bounded density ratio assumption, while Section 4 considers a more general distribution assumption under measure-preserving transformations. Furthermore, as will be shown later, this problem, though well-defined, is not easily implementable. Therefore, we propose a surrogate convex optimization problem in this paper and provide its theoretical guarantees.

### Complexity measure

The complexity of the function class \(\) is usually quantified through the Rademacher complexity, defined as follows.

**Definition 2.1** (Rademacher complexity).: _Let \(\) be a function class and \(\{X_{i}\}_{i=1}^{n}\) be a set of samples drawn i.i.d. from a distribution \(\). The Rademacher complexity of \(\) is defined as_

\[_{n}()=_{,}[_{f }_{i=1}^{n}_{i}f(X_{i})],\] (2.3)

_where \(\{_{i}\}_{i=1}^{n}\) are i.i.d. Rademacher random variables that equals to \( 1\) with probability \(1/2\) each._

## 3 Covariate shift with bounded density ratio

Setup and methodologyIn this section, we focus on the covariate shift problems, where the marginal densities \(p_{S}(X)\) and \(p_{T}(X)\) of the covariates may vary between the source and target domains, albeit the conditional distribution \(Y|X\) remains the same. Denote by \(m_{0}(x)=_{T}[Y|X=x]=_{S}[Y|X=x]\), the conditional mean function. For the ease of the presentation, we assume \(m_{0}\) is known. If unknown, one may use the labeled source data to estimate it using a suitable parametric/non-parametric estimate (e.g., splines, local polynomial, or deep neural networks), subsequently substituting \(m_{0}\) with \(\) in our approach. The density ratio of the source and the target distribution of \(X\) is denoted by \(w_{0}(x):=p_{T}(x)/p_{S}(x)\). We henceforth assume that the density ratio is uniformly bounded:

**Assumption 3.1**.: _There exists \(W\) such that \(_{x_{S}}w_{0}(x) W\)._

If \(w_{0}\) is known, (2.2) has the following sample level counterpart:

\[_{f}\ \ _{n,T}[f(X)],\ \ \ \ _{n,S}[w_{0}(X)_{(Y-m_{0}(X))^{2}>f(X)}] \,,\] (3.1)

which is NP-hard owing to the presence of the indicator function. However, in many practical scenarios, it is observed that the shape of the prediction band does not change much if we change the level of coverage (i.e., \(\)); only the bands shrink/expand. Indeed, the true shape determines the average width; if the shape is wrong, then the width of the prediction band is quite likely to be unnecessarily large. Therefore, to obtain a prediction interval with adequate coverage and minimal width, one should first identify the shape of the prediction band and then shrink/expand it appropriately to get the desired coverage. This motivates the following two steps procedure:

**Step 1:** (Shape estimation) Obtain an initial estimate \(_{ init}\) by solving (3.1) for \(=0\) (to capture the shape):

\[_{f}\ \ _{n,T}[f(X)]\,,\ \ \ \ f(X_{i}) (Y_{i}-m_{0}(X_{i}))^{2}\ \ 1 i n_{S}:w_{0}(X_{i})>0\,.\] (3.2)

**Step 2:** (Shrinkage) Refine \(_{ init}\) by scaling it down using \(()\), defined as:

\[()=\{ 0:_{n,S}[w_{0}(X) _{(Y-m_{0}(X))^{2}>_{ init}(X)}] \}\,.\] (3.3)

The final prediction interval is:

\[_{1-}(x)=[m_{0}(x)-() _{ init}(x)},m_{0}(x)+()_{ init}(x)}]\,.\] (3.4)

In Step 1, we relax (3.1) by effectively setting \(=0\). This relaxation aids in determining the optimal shape while also converting (3.1) into a convex optimization problem (equation (3.2)) as long as \(\) is a convex collection of functions. Furthermore, in (3.2), we only consider those source observations for which \(w_{0}(x)>0\), as otherwise, the samples are not informative for the target domain. In practice, \(w_{0}\) is typically unknown; one may use the source and target domain covariates to estimate \(w_{0}\). Various techniques are available for estimating the density ratio (e.g.,  and references therein). However, any such estimator \((x)\) can be non-zero for \(x\) where \(w_{0}(x)=0\) due to estimation error. Consequently, \(\) may not be efficient in selecting informative source samples. To mitigate this issue, we propose below a modification of (3.2), utilizing a hinge function \(h_{}(t):=\{0,(t/)+1\}\):

\[_{f}\ \ _{n,T}[f(X)]\] (3.5)

with \(\) and \(\) should be chosen based on sample size \(n_{S}\) and the estimation accuracy of \(\). When \(=w_{0}\) (i.e., the density ratio is known), then by choosing \(=0\) and \( 0\), (3.5) recovers (3.2). As \(h_{}\) is convex, the optimization problem (3.5) is still a convex optimization problem. We summarize our algorithm in Algorithm 1.

Theoretical resultsWe next present theoretical guarantees of the prediction interval obtained via Algorithm 1. For technical convenience, we resort to data-splitting; we divide the source data into two equal parts (\(_{S,1}\) and \(_{S,2}\)), use \(_{S,1}\) and \(_{T}\) to solve (3.5), and \(_{S,2}\) to obtain the shrink level \(()\). Without loss of generality, we assume \(m_{0} 0\) (otherwise, we set \(Y Y-m_{0}(X)\)). A careful inspection of Step 1 reveals that \(_{ init}\) aims to approximate a function \(f^{*}\) defined as follows:

\[f^{*}=_{f}_{T}[f(X)]\,\,\,\,\,Y^{2}<f(X)\,\,\,.\] (3.6)

In other words, \(_{ init}\) estimates \(f^{*}\) that has minimal width among all functions covering the response variable. This is motivated by the philosophy that the _right shape leads to a smaller width_. The following theorem provides a finite sample concentration bound on the approximation error of \(_{ init}\):

**Theorem 3.2**.: _Suppose \(Y^{2}-f^{*}(X) B\) on the source domain and has a density bounded by \(L\). Also assume \(\|f\|_{} B_{}\) for all \(f\). Then for_

\[ L+W}}+ (_{S}[\|(X)-w_{0}(X)]]+(W+W^{}) }})\,,\] (3.7)

_we have with probability at least \(1-3e^{-t}\):_

\[_{T}[_{ init}(X)]_{T}[f^{*}(X)]+2 _{n_{T}}(-f^{*})+2B_{}}}\]

_where \(W^{}=\|\|_{}\)._

The bound in the above theorem depends on the Rademacher complexity of \(\) (the smaller, the better), the estimation error of \(w_{0}\), and an interplay between the choice of \((,)\). The lower bound on \(\) in (3.7) depends on both \(\) and \(1/\). Although it is not immediate from the above theorem why we need to choose \(\) to be as small as possible, it will be apparent in our subsequent analysis; indeed if \(\) is large in (3.5), then \(_{ init} 0\) will be a solution of (3.5). Consequently, the shape will not be captured. Therefore, one should first choose \(\) (say \(^{*}\)), that minimizes the lower bound (3.7), and then set \(=^{*}\) equal to the value of the right-hand side of (3.7) with \(=^{*}\), which ensures that \(^{*}\) is optimally defined to capture the shape accurately. Once the shape is identified, we shrink it properly in Step 2 to attain the desired coverage and reduce the width. Although ideally \(() 1\), it is not immediately guaranteed as we use separate data (\(_{S,2}\)) for shrinking. The following lemma shows that \(() 1\) for any fixed \(>0\) as long as the sample size is large enough. Recall that the data were split into exactly half with size \(n_{S}=|_{S}|\).

**Lemma 3.3**.: _Under the aforementioned choice of \((^{*},^{*})\), we have with high probability:_

\[/2}_{i_{S,2}}(X_{i})_{\{(Y_{ i}-m_{0}(X_{i}))^{2}>_{ init}(X_{i})\}}\,,\]

_for all large \(n_{S}\), provided that \(\) is a consistent estimator of \(w_{0}\). Hence, \(() 1\)._

Our final theorem for this section provides a coverage guarantee for the prediction interval given by Algorithm 1.

**Theorem 3.4**.: _For the prediction interval obtained in (3.4), with probability greater than \(1-2e^{-t}\):_

\[|_{T}(Y^{2}>()_{ init}(X) _{S}_{T})-|_{S}[ |(X)-w(X)|]+(2W+W^{})}}+}}\]

_for some constant \(C>0\) and \(W^{}=\|\|_{}\)._Theorem 3.4 validates the coverage of the prediction interval derived through Algorithm 1, achieving the desired coverage level as the estimate of \(w_{0}\) improves and sample size expands. Theorems 3.2 and 3.4 collectively demonstrate the efficacy of our method in maintaining validity and accurately capturing the optimal shape of the prediction band, which in turn leads to small interval widths.

**Remark 3.5**.: _In our optimization problem, we've substituted the indicator loss with the hinge loss function to ensure convexity. However, it's worth noting that if we know the subset of \(_{S}\) where \(w_{0}(x)>0\) beforehand, we could directly optimize (3.2). This approach would be easy to implement and wouldn't involve tuning parameters \((,)\). A special case is when \(w_{0}(x)>0\) for all \(x_{S}\) (as is true in our experiment), which simplifies the condition in (3.2) to \(fX_{i}(Y_{i}-m_{0}(X_{i}))^{2}\) for all \(1 i n_{S}\). However, if this information is unavailable, one can still employ (3.2) by enforcing the constraint on all source observations. While this approach might result in wider prediction intervals, it is easy to implement and doesn't require tuning parameters._

## 4 Domain shift and transport map

Setup and methodologyIn the previous section, we assume a uniform bound on the density ratio. However, this may not be the case in reality; it is possible that there exists \(x(_{T})(_{S}^{c})\), which immediately implies that \(w_{0}(x)=\). In image recognition problems, if the source data are images taken during the day at some place, and the target data are images taken at night, then this directly results in an unbounded density ratio (due to the change in the background color). Yet a transport map could effectively model this shift by adapting features from the source to correspond with those of the target, maintaining the underlying patterns or object recognition capabilities across both domains. To perform transfer learning in this setup, we model the domain shift via a measure transport map \(T_{0}\) that preserves the conditional distribution, as elaborated in the following assumption:

**Assumption 4.1**.: _There exists a measure transport map \(T_{0}:_{T}_{S}\), i.e., \(T_{0}(X_{T})}{{=}}X_{S}\), such that: \(_{T}(Y X=x)}{{=}}_{S}(Y  X=T_{0}(x)),\  x_{T}\)._

This assumption allows the extrapolation of source domain information to the target domain via \(T_{0}\), enabling the construction of prediction intervals at \(x_{T}\) by leveraging the analogous intervals at \(T_{0}(x)_{S}\). Inspired by this observation, we present our methodology in Algorithm 2 that essentially consists of two key steps: i) constructing a prediction interval in the source domain and ii) transporting this interval to the target domain using the estimated transport map \(T_{0}\). If \(T_{0}\) (or its estimate) is not given, it must be estimated from the source and the target covariates. Various methods are available in the literature (e.g., ), and practitioners can pick a method at their convenience. Notably, the processes described in equations (4.1) and (4.2) follow the methodology (i.e., (3.2) and (3.3)) from Section 3 for scenarios without shift (i.e., \(w_{0} 1\)), adding a slight \(\) to ensure coverage even when \(\) is complex.

```
1:Input: conditional mean function \(m_{0}\) on the source domain, transport map estimator \(_{0}\), function class \(\), sample \(_{S}=\{(X_{S,i},Y_{S,i})\}_{i=1}^{n_{S}}\) and \(_{T}=\{X_{T,i}\}_{i=1}^{n_{T}}\), parameter \(\), coverage level \(1-\).
2: Obtain \(_{}\) by solving: \[_{f}\ \ }_{i=1}^{n_{S}}f(X_{S,i})\,,\ \ \ f(X_{S,i})(Y_{S,i}-m_{0}(X_{S,i}))^{2}\ \ i[n_{S}]\,.\] (4.1)
3: Obtain the shrink level \[():=\{>0:}_{i=1}^{n_{S}} _{(Y_{S,i}-m_{0}(X_{S,i}))^{2}(_{}(X_ {S,i})+)}\}\,.\] (4.2)
4:Output: \(}_{1-}(x)=[m_{0}_{0}(x)()(_{}_{0}(x)+ )}].\) ```

**Algorithm 2** Transport map

In Algorithm 2, we assume the conditional mean function \(m_{0}\) on the source domain is known. In cases where the conditional mean function \(m_{0}\) on the source domain is unknown, it can be estimated using standard regression methods from labeled source data, after which \(m_{0}\) is replaced by this estimate, \(\).

**Remark 4.2** (Model aggregation).: _Suppose we have \(K\) different methods \(\{f_{1},,f_{K}\}\) for constructing prediction intervals in the source domain. In the context of model aggregation, (4.1) then reduces to:_

\[_{_{1},,_{K}} }_{i=1}^{n_{S}}_{j=1}^{K}_{j} f_{j}(X_{S,i})}\] \[ _{j=1}^{K}_{j}f_{j}(X_{S,i})(Y_{S,i}-m_{0}(X_{S,i} ))^{2}\;\;i[n_{S}]\,,\] \[_{j} 0,\;1 j K\,.\]

_In other words, the function class \(\) is a linear combination of the candidate methods. The problem is then simplified to a linear program problem, which can be implemented efficiently using standard solvers._

Theoretical resultsWe now present theoretical guarantees of our methodology to ensure that our method delivers what it promises: a prediction interval with adequate coverage and small width. For technical simplicity, we split data here: divide the labeled source observation with two equal parts (with \(n_{S}/2\) observations in each), namely \(_{S,1}\) and \(_{S,2}\). We use \(_{S,1}\) to solve (4.1) and obtain the initial estimator \(_{}\), and \(_{S,2}\) to solve (4.2), i.e. obtaining the shrinkage factor \(()\). Henceforth, without loss of generality, we assume \(m_{0}=0\) and present the theoretical guarantees of our estimator. We start with an analog of Theorem 3.2, which ensures that with high probability \(_{}_{0}\) approximates the function that has minimal width among all the functions in \(\) composed with \(T_{0}\) that covers the labels on the target almost surely:

**Theorem 4.3**.: _Assume the function class \(\) is \(B_{}\)-bounded and \(L_{}\)-Lipschitz. Define_

\[=\{_{T}[f T_{0}(X)]:f,Y^{2} f  T_{0}(X)\}\,.\]

_Then we have with probability \( 1-e^{-t}\):_

\[_{T}[_{}_{0}(X)]+4 _{n_{S}}()+L_{}_{T}[|_{0}(X) -T_{0}(X)|]+4B_{}}}\,.\]

The upper bound on the population width of \(_{}_{0}(x)\) consists of four terms: the first term is the _minimal possible width_ that can be achieved using the functions from \(\), the second term involves the Rademacher complexity of \(\), the third term encodes the estimation error of \(T_{0}\), and the last term is the deviation term that influences the probability. Hence, the margin between the width of the predicted interval and the minimum achievable width is small, with the convergence rate relying on the precision of estimating \(T_{0}\) and the complexity of \(\), as expected.

We next establish the coverage guarantee of our estimator of Algorithm 2, obtained upon suitable truncation of \(_{}\). As mentioned, the shrinkage operation is performed on a separate dataset \(_{S,2}\). Therefore, it is not immediate whether the shrinkage factor \(()\) is smaller than \(1\), i.e., whether we are indeed shrinking the confidence interval (\(()>1\) is undesirable, as it will widen \(_{}\), increasing the width of the prediction band). The following lemma shows that with high probability, \(() 1\).

**Lemma 4.4**.: _With probability greater than or equal to \(1-e^{-t}\), we have:_

\[(()>1_{S,1},_{T}) e ^{-})^{2}n_{S}}{6p_{n_{S}}}},\]

_where_

\[p_{n_{S}}=_{S}(Y^{2}_{}(X)+\, \,_{S,1},_{T})( _{S}[Y^{4}]}{n_{S}}}+_{n_{S}}() )+}}\,.\]Here \(p_{n_{S}}\) is the conditional probability of a test observation \(Y\) falling outside \([-_{}(X)+},_{}(X)+}]\), which is small as evident from the above lemma. In particular, for model aggregation, if \(\) is the linear combination of \(K\) functions, then \(p_{n_{S}}\) is of the order \(}\). Hence, the final prediction interval is guaranteed to be a compressed form of \(_{}\) with an overwhelmingly high probability. We present our last theorem of this section, confirming that the prediction interval derived from Algorithm 2 achieves the intended coverage level with a high probability:

**Theorem 4.5**.: _Under the same setup of Theorem 4.3, along with the assumption that \(f_{S}(y x)\) is uniformly bounded by \(G\), we have with probability greater than \(1-cn_{S}^{-10}\) that_

\[|_{T}(Y^{2}()( _{}_{0}(X)+)_{S} _{T})-|\] \[ C}{n_{S}}}+GL_{} _{T}[|_{0}(X)-T_{0}(X)|].\]

As for Theorem 4.3, the bound obtained in Theorem 4.5 also depends on two crucial terms: Rademacher complexity of \(\) and estimation error of \(T_{0}\). Therefore, the key takeaway of our theoretical analysis is that the prediction interval obtained from Algorithm 2 asymptotically achieves nominal coverage guarantee and minimal width. Furthermore, the approximation error intrinsically depends on the Rademacher complexity of the underlying function class and the precision in estimating \(T_{0}\).

**Remark 4.6** (Measure preserving transformation).: _In our approach, \(T_{0}\) is employed to maintain measure transformation, although it may not necessarily be an optimal transport map. Yet, estimating \(T_{0}\) can be challenging in many practical scenarios. In such cases, simpler transformations like linear or quadratic adjustments are often utilized to align the first few moments of the distributions. Various methods provide such simple solutions, including, but not limited to, CORAL  and ADDA ._

## 5 Application

In this section, we illustrate the effectiveness of our method by applying it to five different datasets: i) airfoil dataset , ii) real estate data , iii) energy efficiency data , iv) appliance energy prediction data , and v) ET Dataset (ETT-small) . The first four datasets are freely available in the UCI repository, and the last dataset can be found in this GitHub link. Here, we illustrate the procedure using the airfoil dataset, and the details of our experiments using the other datasets can be found in Appendix C. The airfoil dataset includes \(1503\) observations, featuring a response variable \(Y\) (scaled sound pressure level) and a five-dimensional covariate \(X\) (log of frequency, angle of attack, chord length, free-stream velocity, log of suction side displacement thickness). We assess and compare the performance of our prediction intervals in terms of coverage and width with those generated by the weighted split conformal prediction method described in . We use the same data-generating process described in  to facilitate a direct comparison. We run experiments 200 times; each time, we randomly partition the data into two parts \(_{}\) and \(_{}\), where \(_{}\) contains \(75\%\) of the data, and \(_{}\) contains \(25\%\) of the data. Following , we _shift_ the distribution of the covariates of \(_{}\) by weighted sampling with replacement, where the weights are proportional to

\[w(x)=(x^{T}),=(-1,0,0,0,1).\]

These reweighted observations in \(_{}\), which we call \(_{}\), act as observations from the target domain. Clearly, by our data generation mechanism \(w_{0}(x)=f_{T}(x)/f_{S}(x)=c\) exp\((x^{})\), where \(c\) is the normalizing constant. The source and target domains share the same support under this configuration. As our methodology is developed for unsupervised domain adaptation, we do not use the label information of \(_{}\) to develop the target domain's prediction interval.

Density ratio estimationWe use the probabilistic classification technique to estimate the density based on the source and the target covariates. Let \(X_{1},,X_{n_{1}}\) be the covariates in dataset \(_{}\) and \(X_{n_{1}+1},,X_{n_{1}+n_{2}}\) be the covariates in dataset \(D_{}\). The density ratio estimation proceeds in two steps: (1) logistic regression is applied to the feature-class pairs \(\{(X_{i},C_{i})\}_{i=1}^{n}\), where \(C_{i}=0\) for \(i=1,,n_{1}\) and \(C_{i}=1\) for \(i=n_{1}+1,,n_{1}+n_{2}\), yielding an estimate of \((C=1 X=x)\), denoted as \((x)\); (2) the density ratio estimator is then defined as \((x)=}{n_{2}}(x)}{1-(x)}\). Further explanations are provided in Appendix B.

Implementation of our method and resultsAs the mean function \(m_{0}(x)=[Y X=x]\) (which is the same on the source and the target domain) is unknown, we first estimate it via linear regression, which henceforth will be denoted by \((x)\). To construct a prediction interval, we consider the model aggregation approach, i.e., the function class \(\) is defined as the linear combination of the following six estimates:

1. **Estimator 1**(\(f_{1}\)): A neural network based estimator with depth=1, width=10 that estimates the 0.85 quantile function of \((Y-(X))^{2} X=x\).
2. **Estimator 2**(\(f_{2}\)): A fully connected feed forward neural network with depth=2 and width=50 that estimates the 0.95 quantile function of \((Y-(X))^{2} X=x\).
3. **Estimator 3**(\(f_{3}\)): A quantile regression forest estimating the 0.9 quantile function of \((Y-(X))^{2} X=x\).
4. **Estimator 4**(\(f_{4}\)): A gradient boosting model estimating the 0.9 quantile function of \((Y-(X))^{2} X=x\).
5. **Estimator 5**(\(f_{5}\)): An estimate of \([(Y-(X))^{2} X=x]\) using random forest.
6. **Estimator 6**(\(f_{6}\)): The constant function 1.

Here, the quantile estimators are obtained by minimizing the corresponding check loss. The implementation of our method is summarized as follows: (1) We divide the training data \(_{}\) into two halves \(_{1}_{2}\). We utilize dataset \(_{1}\) to derive a mean estimator and six aforementioned estimates. We also employ the covariates from \(_{1}\) and \(_{}\) to compute a density ratio estimator. (2) We further split \(_{2}\) into two equal parts \(_{2,1}\) and \(_{2,2}\). \(_{2,1}\), along with covariates from \(_{}\), is used to find the optimal aggregation of the six estimates to capture the shape, i.e., for obtaining \(_{}\). The second part \(_{2,2}\) is used to shrink the interval to achieve \(1-=0.95\) coverage, i.e. to estimate \(()\). (3) We evaluate the effectiveness of our approach in terms of the coverage and average bandwidth on the \(D_{}\) dataset.

In Figure 1, we present the histograms of the coverage and the average bandwidth of our method, and a more general version of weighted conformal prediction in  over \(200\) experiments (see Appendix B for details), which show that our method consistently yields a shorter prediction interval than the weighted conformal prediction while maintaining coverage. Over 200 experiments, the average coverage achieved by our method was 0.964029 (SD = 0.04), while the weighted conformal prediction method achieved an average coverage of 0.9535 (SD = 0.036). Additionally, the average width of the prediction intervals for our method was 13.654 (SD = 2.22), compared to 20.53 (SD = 4.13) for the weighted conformal prediction. Regarding the performance of intervals over \(95\%\) coverage, our method achieved this in \(72.5\%\) of cases with an average width of 14.35 (SD = 2.22). In contrast, the weighted conformal prediction method did so in \(57\%\) of cases with an average width of 21.4 (SD = 4.39). Boxplots are presented in Appendix B for further comparison.

### Robustness of our method

One of the strengths of our approach lies in its resilience to the misspecification of certain components. The core idea behind our method is to combine multiple predictors to create a prediction interval that ensures sufficient coverage while maintaining a narrow average width. These individual prediction intervals may be based on estimators of conditional quantiles, means, variances, and other metrics. If some of the component prediction intervals exhibit poor performance, whether due to inadequate coverage or excessive width, our method typically assigns them lower weights, making it robust to their deficiencies. In contrast, other conformal methods that heavily depend on a single component tend to underperform, particularly with respect to average width. To illustrate this phenomenon, we evaluate our method under model misspecification using a simple one-dimensional simulation setup.

\[X ([-1,1]),\ \ ([-1,1]),\ \ X\!\!\] \[Y =}\,.\]As mentioned in the previous subsection, our method aggregates six predictor intervals, including an estimator for the conditional variance function (Estimator 5). The weighted variance-adjusted conformal prediction interval (WVAC) relies on accurately estimating this conditional variance. We estimate the conditional variance using a random forest with varying depths ({3, 5, 7, 15}). For simulation purpose, we generate \(n=2500\) samples, keeping \(75\%\) as source data and resampling the remaining 25% with weighted samples proportional to \(w(x)(1+(-2x))^{-1}\). As depth increases, overfitting leads to poor out-of-sample variance predictions. Table 1 summarizes our findings over 100 Monte Carlo iterations, showing that WVAC's average width increases with depth, while our method's average width remains stable.

## 6 Conclusion

This paper focuses on unsupervised domain shift problems, where we have labeled samples from the source domain and unlabeled samples from the target domain. We introduce methodologies for constructing prediction intervals on the target domain that are designed to ensure adequate coverage while minimizing width. Our analysis includes scenarios in which the source and target domains are related either through a bounded density ratio or a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. We further establish rigorous finite sample theoretical guarantees regarding the coverage and width of our prediction intervals. Finally, we demonstrate the practical effectiveness of our methodology through its application to the airfoil dataset.

  Max depth & Avg. width–Our Method & Avg. width–WVAC \\  
3 & 2.07(0.975) & 3.08(0.9712) \\ 
5 & 2.07(0.95) & 3.28(0.9664) \\ 
7 & 2.068(0.94) & 3.33(0.97) \\ 
15 & 2.08(0.97) & 5.00(0.97) \\  

Table 1: Robustness of our method and WVAC. The number inside the parenthesis is the median of coverage over these Monte Carlo iterations.

Figure 1: Experiments on Airfoil data using Algorithm 1