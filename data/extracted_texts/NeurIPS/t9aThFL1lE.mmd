# UnlearnCanvas: A Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models

UnlearnCanvas: A Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models

Yihua Zhang\({}^{1}\), Chongyu Fan\({}^{1}\), Yimeng Zhang\({}^{1}\), Yuguang Yao\({}^{1}\), Jinghan Jia\({}^{1}\), Jiancheng Liu\({}^{1}\), Gaoyuan Zhang\({}^{2}\), Gaowen Liu\({}^{3}\), Ramana Kompella\({}^{3}\), Xiaoming Liu\({}^{1}\), Sijia Liu\({}^{1,2}\)

\({}^{1}\)Michigan State University, \({}^{2}\)IBM Research, \({}^{3}\)Cisco Research

###### Abstract

The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnLearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at https://unlearn-canvas.netlify.app/.

Machine Learning, ICML

## 1 Introduction

The recent technological breakthroughs in text-to-image generation, driven by diffusion models (**DMs**), have shown an unprecedented capability to produce high-resolution, high-quality images across a diverse range of subjects [1; 2; 3; 4; 5; 6; 7; 8; 9; 10]. These models have become widely accessible to the public and are applied across various sectors, including advertising, the creative arts , and forensic sketching . One reason for DMs being able to generate a broad spectrum of content is their reliance on diverse internet-sourced data . However, this inclusiveness comes with risks, such as harmful generation , copyright issues , and biases or stereotypes [16; 17].

To alleviate the negative social impacts associated with DMs, machine unlearning (**MU**) techniques are catching increasing attention and have been studied in the field of text-to-image generation via DMs [18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33]. MU for DMs, also referred to as _DM unlearning_, aims to prevent the model from generating images when conditioned on an undesired concept (typically specified by a text prompt to be forgotten). Therefore, the problem of DM unlearning is also known as _concept erasing_[18; 19; 20; 21; 22; 23; 24; 25; 26]. Based on the types of unlearning requests, the targeted concept to be erased can be diverse, including not-safe-for-work (NSFW) prompts [14; 23], concrete object entities [18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32], and copyrighted information like artistic styles [23; 24; 25; 26; 28; 29; 30; 32]. In Fig. 1 (a), we provide an illustration of DMunlearning by demonstrating the removal of an undesired artistic style from DM-generated images. Additionally, we refer readers to Sec. 2 for more related work on DM unlearning.

Compared to MU for DMs, the unlearning studies in computer vision have primarily focused on image classification models . Given an image classification dataset, benchmarking unlearning performance for discriminative models is simpler, because MU typically involves either class-wise forgetting  or data-wise forgetting . The former aims to eliminate the influence of a specific image class, while the latter removes the influence of specific data points from the entire training set. In contrast, DM unlearning is often applied to scenarios involving the removal of higher-level and more abstract concepts, rather than specific training data points, which can also be difficult to localize within the DM training set. The primary method for assessing the performance of DM unlearning is using the I2P (inappropriate image prompts) dataset . This dataset mainly focuses on the safety assessment of _unlearned DMs_ (_i.e._, DMs post-unlearning), designed to erase specific harmful concepts like 'nudity' and 'violence'. Although I2P specifies which concepts to unlearn, evaluating the unlearning effectiveness and generation retainability of unlearned DMs under I2P remains challenging. This difficulty arises primarily from the lack of ground-truth data or objective criteria to precisely define safety versus non-safety. Furthermore, I2P cannot evaluate the performance of DM unlearning in other scenarios, such as object unlearning  and style unlearning .

To enhance the assessment of machine unlearning in DMs and establish a standardized evaluation framework, we propose the development of a new benchmark dataset, referred to as **UnLearnCavas**. Unlike I2P, UnLearnCavas is designed to evaluate the unlearning of artistic painting styles along with associated image objects (see an illustration in Fig. 2). It also provides ground-truth data annotations to precisely define the criteria for the effectiveness of unlearning and the retained model utility post-unlearning (see a result overview in Fig. 1 (b)). We summarize **our contributions** below.

\(\) We conduct a systematic review of existing MU methods for DMs and identified three unresolved challenges in their evaluations. We further provide an in-depth analysis of why a standardized benchmark for DM unlearning evaluation is crucial.

Figure 1: (a) An illustration of MU for DMs. (b) Overview of experiment settings and benchmark results. This benchmark focuses on three categories of quantitative metrics: the unlearning effectiveness (UA, Rob., FU, SU); the retainability of innocent knowledge (IRA, CRA, FR, SR); and the image generation quality (FID). Results are normalized to \(0\% 100\%\) per metric. No single method excels across all metrics. See a summary of these metrics in Tab. A1 and more results in Sec. 4.

Figure 2: An illustration of machine unlearning using UnLearnCavas. Concepts in the knowledge bank are categorized into different domains (style and object) and serve as potential unlearning targets. When one concept is unlearned, the rest concepts in both the same and different domains are required to be retained.

\(\) We propose UnlearnCanvas, a large-scale, high-resolution stylized image dataset with diverse styles and objects, to address current challenges in DM unlearning evaluation. Its dual supervision of styles and objects ensures stylistic consistency, enabling a comprehensive, standardized evaluation approach to precisely characterize unlearning efficacy, generation utility preservation, and efficiency.

\(\) We benchmark 9 state-of-the-art DM unlearning methods using **UnlearnCanvas**, covering standard assessments and examining more challenging scenarios, such as adversarial robustness, grouped object-style unlearning, and sequential unlearning. This comprehensive evaluation provides previously unknown insights into their strengths and weaknesses.

## 2 Related Work, Problem Statement, and Open Challenges

Related work.MU (machine unlearning) was originally developed to mitigate the (potentially detrimental) influence of specific data points in a pretrained ML model, without necessitating a complete retraining of the model after removing these unlearning data . The significance of MU has emerged with the purpose of data privacy protection, in response to regulations such as 'the right to be forgotten' . However, it has rapidly gained recognition for its role in promoting security, safety, and trustworthiness of ML models. Examples include defense against backdoor attacks , fairness enhancement , and controllable federated learning . More recently, DM unlearning has proven crucial in text-to-image generation to prevent the production of harmful, private, or illegal content . For example, DMs have demonstrated a susceptibility to generating NSFW (not-safe-for-work) images when conditioned on inappropriate text prompts (_e.g._, 'nudity' and 'violence') . Similarly, MU has also found applications in preventing the generation of copyrighted information, such as the misuse of artistic painting styles . In brief, DM unlearning can be regarded as a model edit operation, aimed at preventing DMs from producing undesired or inappropriate images.

As unlearning approaches continue to evolve, the need for _benchmarking_ their performance becomes increasingly critical. Standard datasets like CIFAR-10  and ImageNet  have long been used to evaluate unlearning performance when forgetting a specific subset of data points  or a targeted image class . Additionally, a recent benchmark specialized for unlearning in face recognition has been developed . However, crafting effective benchmarks for generative models introduces additional challenges. In the language domain, recent initiatives have specifically aimed to evaluate the efficacy of MU for large language models (LLMs). Notable examples include TOFU , which explores the unlearning of fictitious elements, and WMDP , aimed at removing hazardous knowledge from LLMs. In contrast, to the best of our knowledge, there is currently no effective benchmark dataset designed for DM unlearning in text-to-image generation. The predominant dataset, I2P , targets the removal of harmful concepts for safe image generation. However, I2P only covers a specialized unlearning scenario and lacks precise evaluation criteria to support a comprehensive, quantitative, and precise assessment of unlearning performance. This gap inspires us to develop a new benchmark dataset for DM unlearning.

**Problem statement.** Let us denote a pre-trained DM by \(_{}\), capable of synthesizing high-quality images conditioned on a provided text prompt \(c\) (_e.g._, 'A painting of a cat in Van Gogh style'). When there is a request to adapt the model \(_{}\) to prevent generating images in a specific concept, such as the 'Van Gogh' painting style, the MU problem arises. Here, the 'Van Gogh' style becomes the specified _unlearning target_, also known as the _erasing concept_. See Fig. 1-(a) for a visual representation of this unlearning problem. In this work, we specify the unlearning target as artistic styles and/or object classes to encompass the settings of style unlearning and object unlearning as described in the literature .

    &  &  \\  & Target \# & Quant. &  & Quant. &  \\  & & UE & RT & Qual. &  &  \\  ESD  & 11 & ✓ & ✓ & ✓ & 5 & ✗ & ✗ & ✓ \\ CA  & 4 & ✓ & ✗ & ✓ & 4 & ✗ & ✗ & ✓ \\ FAN  & 7 & ✓ & ✗ & ✓ & 1 & ✗ & ✗ & ✓ \\ UCE  & 11 & ✓ & ✗ & ✓ & 7 & ✗ & ✓ \\ SA  & 31 & ✓ & ✗ & ✓ & 1 & ✗ & ✗ & ✓ \\ SaIUs  & 11 & ✓ & ✓ & ✓ & 0 & ✗ & ✗ & ✗ \\ SPM  & 24 & ✓ & ✓ & ✓ & 5 & ✗ & ✗ & ✓ \\ SEOT  & 22 & ✓ & ✓ & ✓ & 2 & ✗ & ✗ & ✓ \\ EDHT  & 2 & ✓ & ✓ & ✓ & 0 & ✗ & ✗ & ✗ \\ SHS  & 1 & ✓ & ✗ & ✓ & 0 & ✗ & ✗ & ✗ \\   

Table 1: Overview of existing MU evaluations for DMs, which covers two unlearning scenarios: object unlearning and style unlearning. Each case is demonstrated by the number of targeted unlearning concepts (target #), qualitative evaluation (Qual.) via generated image visualization, and quantitative evaluation (Quant.) of unlearning effectiveness (UE) and retainability (RT).

**Motivation: Unresolved challenges in MU evaluation.** When building a rigorous _quantitative_ evaluation framework for DM unlearning, we identified three urgent challenges _(C1)-(C3)_ after carefully examining existing studies as shown in **Tab.**1.

_(C1) The absence of a consensus on a diverse unlearning target test repository._ As shown in Tab. 1, assessments of DM unlearning in various studies, in terms of both effectiveness (_i.e._, concept erasure performance) and retainability (_i.e._, preserved generation quality under non-forgotten concepts), typically use manually-selected unlearning targets from a _limited pool_ and focus on _object-centric_ evaluation. Precise evaluation for style unlearning and grouped object-style unlearning is lacking. Although the existing dataset WikiArt features a collection of real-world artworks by various artists, it does _not_ apply to object unlearning.

_(C2) The lack of a systematic study on'retainability' of DMs post-unlearning._ As highlighted in the'retainability' column of Tab. 1, there is a notable deficiency in the quantitative assessment of retainability for DMs post-unlearning. Retainability is crucial for measuring the potential side effects of a MU method. For instance, when unlearning an artistic style, it is essential to assess the DM's ability to generate images in other styles and all objects. This makes the WikiArt dataset less suitable for benchmarking DM unlearning, as it lacks object labels needed to characterize the side effects of style unlearning on object recognition.

_(C3) The precision challenge in evaluating DM-generated images._ Artistic styles are inherently complex to define and differentiate precisely, complicating the quantitative evaluation of forgetting and retaining performance in style unlearning. For example, the WikiArt dataset was recently used in  to train a classifier for detecting copyright infringement, merely achieving an accuracy of 72.80%. The difficulty in precisely recognizing artistic styles in WikiArt may further hamper the evaluation of DMs' post-unlearning generation. **Tab.**A3 illustrates the challenge of accurately recognizing the styles of images generated by DMs, even when finetuned on WikiArt. This is evidenced by the test-time classification accuracy using the style classifier, ViT-L/16 , finetuned on WikiArt. As we can see, only about half of the images are correctly classified into their corresponding style classes, _e.g._, 56.7% accuracy for classification on generated images using finetuned stable diffusion (SD) v2.0 , which is significantly lower than that on test images from the original WikiArt (85.4%).

The above challenges _(C1)-(C3)_ drive us to develop the UnLearnCanvas dataset as a solution for the systematic and comprehensive evaluation of DM unlearning in Sec. 3.

## 3 Our Proposal: UnlearnCanvas Dataset

**Construction of UnlearnCanvas.** As motivated in Sec. 2, UnLearnCanvas is created for ease of MU evaluation in DMs. Its construction involves two main steps: seed image collection and subsequent image stylization; see **Fig.**3 for a schematic overview. More information about the dataset is provided in Appx. A.

_Seed image collection._ UnLearnCanvas includes images in \(60\) unique artistic styles across \(20\) distinct objects. The style categories are built upon a set of high-resolution _seed images_, given by real-world photos from Pexels . There are \(20\) seed images collected for each of the \(20\) object classes. See step 1 in Fig. 3.

_Image stylization._ After collecting the seed images, the next step is the _stylization_ process. During this phase, each seed image is transformed into \(60\) predetermined artistic styles, ensuring high stylistic consistency while preserving the original content details. This stylization process is carried out using services provided by Fotor . Once all seed images have been stylized in all \(60\) styles, the dataset is constructed with super-high-resolution images and organized in a hierarchical structure that balances both style and object categories. See step 2 in Fig. 3.

Figure 3: Illustration of curating UnLearnCanvas.

[MISSING_PAGE_FAIL:5]

To assess in-domain retainability (_i.e._, the unlearned model's ability to retain generation quality within the same domain), images are generated with prompts in the same domain as the unlearning target (_e.g._, style-related prompts for style unlearning). These prompts cover all other styles provided in UnLearnCanvas except the one to be unlearned. _(c)_ To measure cross-domain retainability, images are generated with prompts in other domains. For instance, in the case of style unlearning as shown in Fig. 5, the object domain is considered separate from the style domain. Prompts related to objects are used to evaluate the DM's cross-domain generation capability after style unlearning.

_Phase IV: MU performance assessment (Fig. 5 (d))_. After Phase III, the answer set undergoes style/object classification for unlearning performance assessment. This classification results in three quantitative metrics 1-2. 1 Unlearning accuracy (**UA**): This represents the proportion of images generated by the unlearned DM using the unlearning target-related prompt that is _not_ correctly classified into the corresponding class. A _higher UA_ indicates better unlearning performance in preventing image generation from the unlearning target-related prompts. 2 In-domain retain accuracy (**IRA**): This is the classification accuracy of images generated by the unlearned DM using innocent prompts (not relevant to the unlearning target) within the same domain. 3 Cross-domain retain accuracy (**CRA**): Similar to IRA, this is the classification accuracy of images generated by the unlearned DM using innocent prompts in different domains. In addition to the accuracy metrics, we also use the **FID** score 4 to evaluate the distribution-wise generation quality of the unlearned DM. Furthermore, we monitor the efficiency of the unlearning process, considering factors such as **run-time** 5, **storage** space requirements 6, and **memory** costs 7.

**DM unlearning methods to be benchmarked.** In this work, we assess **9** most recently proposed MU methods for DMs, including **ESD**, **FMN**, **UCE**, **CA**, **SalUn**, **SEOT**, **SPM**, **EDif**, and **SHS**. We remark that SA , a recently proposed method, is excluded from our evaluation due to its excessive computational resource requirements and time consumption. Unless specified otherwise, SD v1.5 is the model used for unlearning. Detailed training settings for each method can be found in Appx. B.

## 4 Experiment Results

In this section, our benchmarking results are divided into two main parts. In Sec. 4.1, we comprehensively evaluate the existing DM unlearning methods on the established tasks of style and object unlearning. Extensive studies following the proposed evaluation pipeline (Fig. 5) reveal that focusing solely on unlearning effectiveness can lead to a biased perspective for DM unlearning if retainability is not concurrently assessed (Tab. 2 & Fig. 10). We also show that preserving CRA (cross-domain retainability) is more challenging than IRA (in-domain retainability), highlighting a gap in the current literature (Fig. 6). Furthermore, we find that different DM unlearning methods exhibit distinct unlearning mechanisms by examining their unlearning directions (Fig. 7). In Sec. 4.2, we introduce more challenging unlearning scenarios. First, we assess the robustness of current DM unlearning methods against adversarial prompts crafted based on (Fig. 8). Additionally, we examine the performance when facing unlearning targets with finer granularity, formed by style-object concept combinations (Tab. 3). Further, we leverage UnLearnCanvas to provide insights into DM unlearning in a sequential unlearning fashion (Tab. 11).

Figure 5: An illustration of the evaluation pipeline proposed in this work using UnLearnCanvas when unlearning a specific target concept ‘Van Gogh Style’. Unlearning performances (including the unlearning effectiveness and retainability) are quantitatively assessed (marked in blue) to accurately reflect the unlearning performance portrait. The unlearning target of the pipeline could traverse all the styles and objects to achieve a comprehensive evaluation.

### Benchmarking Current DM Unlearning Methods for Style and Object Unlearning

Overall performance of DM unlearning.In **Tab.**2, we provide an overview of the performance of existing unlearning methods, using the evaluation metrics associated with UnLearnCanvas.

First, as seen in Tab. 2, retainability is essential for a comprehensive assessment of DM unlearning. For example, in the scenario of style unlearning, ESD and UCE achieve similar UA (both around \(98\%\)), but their retainability (IRA and CRA) differs significantly, with one over \(80\%\) and the other around \(60\%\). Therefore, relying solely on UA can provide a skewed view of the performance of DM unlearning. _Second_, we observe that CRA is harder to retain than IRA. For example, UCE exhibits a significant gap between its IRA and CRA, both in style (\(60.22\%\) vs. \(47.71\%\)) and object (\(39.35\%\) vs. \(34.67\%\)) unlearning scenarios. Similar patterns are observed with other methods like FMN, CA, SEOT, SPM, and SHS. This pattern suggests that while MU methods are somewhat effective at preserving concepts within the same domain, they face greater challenges in maintaining performance on unlearning target-unrelated concepts across domains. This issue has been overlooked in previous studies due to the absence of a multi-label dataset like UnLearnCanvas for systematic unlearning analyses. _Third_, we find that a single unlearning method can perform differently across various domains, and no single method excels in all aspects. For example, FMN achieves a UA of 88.48% in style unlearning but a much lower UA of 45.64% in object unlearning. A similar phenomenon can be observed with the unlearning methods SEOT, SPM, and SHS, which exhibit significantly large performance gaps between style and object unlearning. Furthermore, each method exhibits unique strengths and weaknesses. For example, while ESD typically shows improvement in UA, it may lag in terms of IRA and CRA, particularly in the case of object unlearning. Some methods, such as SEOT and SPM, achieve extremely high storage efficiency, requiring no additional storage, but suffer from inferior UA (below 80%). These observations underscore the challenges of DM unlearning and highlight the need for further advancements in the field.

A closer look into retainability: A case study on ESD.We next perform an in-depth analysis of DMs' retainability after unlearning, focusing on the most popular ESD method . In **Fig.**6 **(left)**, we present a heatmap illustrating the per-style/object unlearning accuracy (diagonal) and retain accuracy (off-diagonal) for ESD. The heatmap segments are labeled to indicate different regions of interest (ROIs) in style and object unlearning related to our evaluation metrics: **A1/A2** for UA, **B1/B2** for IRA, and **C1/C2** for CRA. Key observations are highlighted below.

First, we observe distinct behaviors of ESD in unlearning styles compared to objects. For example, ROI C2 appears darker than C1, suggesting a reduced ability to retain styles during object unlearning. Similar patterns are seen between B2 and B1. For a clearer explanation, **Fig.**6 **(right)** provides image generation examples of the DM before and after unlearning within different ROIs. _Second_. We observe that style/object unlearning is relatively easier compared to retaining the generation performance of unlearned DMs conditioned on unlearning-unrelated prompts. This is evident in the contrast between ROIs A1/A2 and ROIs B1/B2 or C1/C2. One exception is the case of unlearning the object 'Sea', which exhibits relatively low UA in A2. This may be attributed to post-unlearning image generation still resembling sea waves, as shown by the image examples in Fig. 6 (right) for

    &  &  &  \\  & & **Style Unlearning** & &  &  &  \\  & **UA** (\(\)) & **IRA** (\(\)) & **CRA** (\(\)) & **UA** (\(\)) & **IRA** (\(\)) & **CRA** (\(\)) & **FID** (\(\)) & **Time Memory** & **Storage** (\(\)) \\  ESD  & \(98.58\%\) & \(80.97\%\) & \(93.96\%\) & \(92.15\%\) & \(55.78\%\) & \(42.23\%\) & \(65.55\%\) & \(6163\) & \(17.8\) & \(4.3\) \\ FMN  & \(88.48\%\) & \(56.77\%\) & \(46.60\%\) & \(45.64\%\) & \(90.63\%\) & \(73.46\%\) & \(131.37\) & \(350\) & \(17.9\) & \(4.2\) \\ UCE  & \(98.40\%\) & \(60.22\%\) & \(47.71\%\) & \(94.31\%\) & \(99.35\%\) & \(34.67\%\) & \(182.01\) & \(434\) & \(51\) & \(1.7\) \\ CA  & \(60.82\%\) & \(96.01\%\) & \(92.70\%\) & \(46.67\%\) & \(90.11\%\) & \(81.97\%\) & \(51.02\) & \(734\) & \(10.1\) & \(4.2\) \\ SalUn  & \(86.26\%\) & \(90.39\%\) & \(95.08\%\) & \(86.91\%\) & \(90.58\%\) & \(90.59\%\) & \(61.05\) & \(667\) & \(308\) & \(4.0\) \\ SEOT  & \(56.90\%\) & \(94.68\%\) & \(84.31\%\) & \(23.25\%\) & \(95.57\%\) & \(82.71\%\) & \(62.38\) & \(95\) & \(7.34\) & \(00\) \\ SPM  & \(90.94\%\) & \(92.39\%\) & \(8.43\%\) & \(71.25\%\) & \(90.79\%\) & \(81.65\%\) & \(59.79\) & \(29700\) & \(6.9\) & \(0.0\) \\ EDIft  & \(92.42\%\) & \(73.91\%\) & \(98.93\%\) & \(86.67\%\) & \(94.03\%\) & \(48.18\%\) & \(81.42\) & \(1567\) & \(27.8\) & \(4.0\) \\ SHS  & \(95.84\%\) & \(80.42\%\) & \(32.27\%\) & \(80.73\%\) & \(81.15\%\) & \(67.99\%\) & \(129.34\) & \(1223\) & \(31.2\) & \(4.0\) \\   

Table 2: Performance overview of different DM unlearning methods evaluated on UnLearnCanvas. The performance metrics include UA (unlearning accuracy), IRA, CRA, and FID. The symbols \(\) or \(\) indicate whether a higher or lower value represents better performance. Results are averaged over all the style and object unlearning cases. The best performance for each metric is highlighted in green, while significantly underperforming results are marked in red, indicating areas needing improvement for existing DM unlearning methods.

A2. Third, we observe an inherent trade-off between unlearning effectiveness and retainability. For example, while SalUn exhibits more consistent performance across different scenarios than ESD, it does not achieve the same level of UA, as indicated by the lighter diagonal values in Fig. A3. For the heatmap performance of other unlearning methods, please refer to Appx. D.

Understanding unlearning method's behavior via unlearning directions.As noted earlier, different unlearning methods display distinct unlearning behaviors. To gain insights into the underlying reasons for these differences, **Fig. 7** (a) and (b) visualize the 'unlearning directions' for ESD and

Figure 6: _Left_: Heatmap visualization of the unlearning accuracy and retainability of ESD on UnLearnCanvas. The \(x\)-axis shows the tested concepts for image generation using the unlearned model, while the \(y\)-axis indicates the unlearning target. Concept types are distinguished by color: styles in blue and objects in orange. The figure is divided into regions representing corresponding evaluation metrics and unlearning scopes (\(A\) for UA, \(B\) for IRA, \(C\) for CRA; ‘1’ for style unlearning, ‘2’ for object unlearning). Higher values in lighter colors denote better performance. The first row serves as a reference for comparison before unlearning. Zooming into the figure is recommended for detailed observation. _Right_: Representative cases illustrating each region with images generated before and after unlearning a specific concept.

Figure 7: Visualization of the unlearning directions of (a) ESD and (b) SalUn. This figure illustrates the conceptual shift of the generated images of an unlearned model conditioned on the unlearning target. Images generated by the post-unlearning models are classified and used to understand this shift. Edges leading from the object in the left column to the right signify that images generated conditioned on unlearning targets are instead classified as the shifted concepts after unlearning. This reveals the primary unlearning direction for each unlearning method. The most dominant unlearning direction for an object is visualized. Figure (c) provides visualizations of generated images using the prompt template ‘A painting of {_object_} in Sketch style.’ with _object_ being each unlearning target.

SalUn, respectively. These unlearning directions are determined by connecting the unlearning target with the predicted label of the generated image from the unlearned DM conditioned on the unlearning target. As shown in Fig. 7 (a), ESD demonstrates a focused shift in image generation after object unlearning, with a predominant transition towards generating images labeled by 'Sea' and 'Trees'. This behavior arises from ESD's optimization process, designed to steer the generation of the DM away from a predefined concept. Consequently, images generated by the ESD-induced unlearned model consistently lack clearly identifiable objects, resembling waves and trees, which leads to their classification into the 'Sea' and 'Trees' classes; see Fig. 7 (c) for examples of generated images. In contrast, SalUn exhibits a more diverse range of unlearning directions, shifting images to 11 different objects. This diversity results from SalUn's requirement to replace the unlearning target with a random concept. As shown in Fig. 7 (c), images generated by SalUn post-object unlearning still maintain some object contours (different from the original unlearning target) and better retain style information compared to ESD.

### Benchmarking Current DM Unlearning Methods in More Challenging Scenarios

Unlearning robustness against adversarial prompts.Recent studies [59; 60] have highlighted the vulnerabilities of DM unlearning to adversarial prompts, such as jailbreak attacks. In **Fig. 8**, we use the state-of-the-art attacking method, UnlearnDiffAtk , to craft adversarial prompts based on UnlearnCanvas and evaluate the robustness of unlearned DMs shown in Tab. 2. See Appx. B.4 for detailed setup. As we can see, all the DM unlearning methods experience a significant drop in UA, falling below \(60\%\) when confronted with adversarial prompts. Notably, methods like UCE, SEOT, and SHS exhibit particularly steep declines. Moreover, a high UA under normal conditions does not necessarily imply robustness to adversarial attacks. For instance, although UCE achieves a UA over 90% in normal settings, its performance plummets to below 50% against adversarial prompts. This stark contrast highlights the importance of the worst-case evaluation for MU methods.

Unlearning at a finer scale: Style-object combinations as unlearning targets.Next, we consider the fine granularity of the unlearning target, defined by a style-object combination, such as "An image of dogs in Van Gogh style". This unlearning challenge requires the unlearned model to avoid affecting image generation for dogs in non-Van Gogh styles and Van Gogh-style images with non-dog objects. See Appx. B.5 for detailed setup. Here, besides UA, the retainability performance is assessed in three contexts. (a) Style consistency (**SC**): retainability under prompts with different objects but in the same style, _e.g._, "An image of _cats_ in Van Gogh style". (b) Object consistency (**OC**): retainability under prompts featuring the same object in different styles, _e.g._, "An image of dogs in _Picasso_ style". (c) Unrelated prompting (**UP**): retainability for all other prompts. **Tab. 2** presents the performance of unlearning style-object combinations. Style-object combinations are more challenging to unlearn than individual objects or styles, as evidenced by a significant drop in UA--over 20% lower compared to values in Tab. 2. Retainability drops to below 20% for top-performing methods like ESD and UCE, originally highlighted for their efficacy. This is presumably

  
**Method** & **UA** & **SC** & **OC** & **UP** \\  ESD  & \(}\) & \(4.88\%\) & \(14.72\%\) & \(84.38\%\) \\ FMN  & \(45.37\%\) & \(}\) & \(62.74\%\) & \(83.25\%\) \\ UCE  & \(75.97\%\) & \(4.53\%\) & \(5.72\%\) & \(35.42\%\) \\ CA  & \(47.92\%\) & \(10.08\%\) & \(56.33\%\) & \(81.54\%\) \\ SalUn  & \(42.21\%\) & \(62.45\%\) & \(}\) & \(}\) \\ SEOT  & \(29.32\%\) & \(45.31\%\) & \(53.64\%\) & \(85.45\%\) \\ SPM  & \(45.72\%\) & \(41.34\%\) & \(36.32\%\) & \(67.82\%\) \\ EDiff  & \(71.33\%\) & \(35.23\%\) & \(26.32\%\) & \(51.52\%\) \\ SHS  & \(55.32\%\) & \(14.34\%\) & \(24.32\%\) & \(83.95\%\) \\   

Table 3: Performance of unlearning style-object combinations. The assessment includes UA and retainability in three contexts: SC (style consistency), OC (object consistency), and UP (unrelated prompting). The best result in each metric is highlighted in **green**.

Figure 8: UA of DM unlearning against adversarial prompts . Unlearned models in Tab. 2 are used as victim models to generate adversarial prompts.

due to ESD's underlying unlearning mechanism, which requires only a single prompt, resulting in a poor ability to precisely define the unlearning scope.

Evaluation in sequential unlearning.Furthermore, we evaluate the unlearning performance in sequential unlearning (SU) [70; 71], where unlearning requests arrive sequentially. This parallels the continual learning (CL) task, which requires models to not only unlearn new targets effectively but also maintain the unlearning of previous targets while retaining all other knowledge. Here, we consider unlearning 6 styles sequentially and the results are presented in **Tab.**A5. We remark that the method SEOT does not support sequential unlearning in its original implementation and thus is not included in Tab. A5. Our findings reveal significant insights. (1) _Degraded retainability_: Sequential unlearning requests generally degrade retainability across all methods, with RA values frequently dropping below the average levels previously seen in Tab. 2. Here RA is given by the average of IRA and CRA. (2) _Unlearning rebound effect_: Knowledge previously unlearned can be inadvertently reactivated by new unlearning requests. This is evidenced by decreasing UA values for earlier objectives as more unlearning tasks are introduced. This suggests that residual knowledge remains within the model and can be reactivated, aligning with findings from Fig. 8. This indicates the unlearned models by some MU methods do not essentially lose the generation ability of the unlearning target. (3) _Catastrophic retaining failure_: RA significantly drops at a certain request, exemplified by a sudden decrease in RA of UCE from \(81.42\%\) to \(29.38\%\) after the second request, \(_{2}\). This indicates that the seemingly acceptable side effects generated by some unlearning methods will drastically modify the knowledge representations when accumulated. This experiment illuminates the complex dynamics of knowledge removal and retention within DMs and highlights the potential pitfalls of existing unlearning methods when faced with sequential unlearning tasks. The observation of the 'unlearning rebound effect' and 'catastrophic retaining failure' particularly emphasizes the need for a more nuanced understanding of how knowledge is managed within DMs.

Visualizations.We provide plenty of visualizations, illustrations, and qualitative results. These visualizations are intended to deepen the understanding of the effects of different MU methods and clearly illustrate the challenges identified in previous sections. Specifically, in Fig. A11, we provide abundant generation examples of all the 9 methods benchmarked in this work in a case study of unlearning the 'Cartoon' style. Both the successful and failure cases are demonstrated in the context of unlearning effectiveness, in-domain retainability, and cross-domain retainability. Besides, in Fig. A12, we provide visualizations for the effect of adversarial prompts enabled by UnlearnDiffAtk .

## 5 Conclusion, Discussion, and Limitation

In this paper, we systematically reviewed existing MU (machine unlearning) methods on DMs (diffusion models) and identified key challenges in their evaluation systems that could lead to incomplete, inaccurate, and biased assessments. In response, we propose UnlearnCanvas, a high-resolution stylized image dataset designed to facilitate comprehensive evaluation of MU methods. We also introduce novel systematic evaluation metrics. By benchmarking nine state-of-the-art MU methods, we reveal novel insights into their strengths and weaknesses and also deepen the understanding of their underlying mechanisms. Our analysis of three challenging unlearning tasks highlights significant shortcomings of current methods, including a lack of robustness, difficulties in fine-scale unlearning, and issues arising from sequential unlearning tasks. These findings also point to meaningful future research directions aimed at developing satisfactory and practical MU methods for real-world applications. Furthermore, we recognize limitations in our benchmark, such as its focus on specific Stable Diffusion models and the text-to-image task in DMs. We hope this work serves as a foundation for future research to broaden MU evaluations across a wider range of model architectures and tasks in DMs.