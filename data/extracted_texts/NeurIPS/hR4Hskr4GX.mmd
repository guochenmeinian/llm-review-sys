# Constraint-Based Synthetic Data Generation for LLM Mathematical Reasoning

Timofey Fedoseev\({}^{1,2}\),, Dimitar I. Dimitrov\({}^{2,3}\), Timon Gehr\({}^{3}\), Martin Vechev\({}^{2,3}\)

\({}^{1}\) Ecole Polytechnique

\({}^{2}\) INSAIT, Sofia University "St. Kliment Ohridski"

\({}^{3}\) ETH Zurich

{timofefej.fedoseev}@polytechnique.edu \({}^{1}\)

{dimitar.liev.dimitrov}@insait.ai \({}^{2}\)

{timon.gehr, martin.vechev}@inf.ethz.ch \({}^{3}\)

 Dimitar I. Dimitrov\({}^{2,3}\), Timon Gehr\({}^{3}\), Martin Vechev\({}^{2,3}\)

\({}^{1}\) Ecole Polytechnique

\({}^{2}\) INSAIT, Sofia University "St. Kliment Ohridski"

\({}^{3}\) ETH Zurich

{timofej.fedoseev}@polytechnique.edu \({}^{1}\)

{dimitar.liev.dimitrov}@insait.ai \({}^{2}\)

{timon.gehr, martin.vechev}@inf.ethz.ch \({}^{3}\)

###### Abstract

Mathematical reasoning with large language models (LLMs) is an emerging research area. A recent breakthrough is the use of off-the-shelf tools LLMs are trained to utilize to offload complex tasks they cannot perform independently. Unfortunately, this approach is limited to popular tools, as many specialized tools lack the data to train these models on. Motivated by our observation that the current tools used with LLMs are insufficient for solving counting problems, in this work, we explore the problem of using Satisfiability Modulo Theories (SMT) solvers with LLMs. Namely, we leverage the SMT grammar to generate synthetic data consisting of problem statements and their solutions represented as Python code interacting with the Z3 API. Our experiments show that fine-tuning LLMs on this dataset substantially enhances their ability to generate accurate Z3 constraint encodings and improves their overall mathematical problem-solving capabilities.

## 1 Introduction

Mathematical reasoning is a key emerging area of competence for LLMs. Instead of asking for the solution of a mathematical problem directly, a common technique is to instead ask for program code that in turn solves the problem, offloading already easily automatable parts of the mathematical reasoning to off-the-shelf libraries . This works particularly well for popular libraries (such as SymPy ), because large amounts of high-quality training data can be found online. Common approaches are to train LLMs on solutions obtained from stronger LLMs , and/or to bootstrap from a large number of samples, discarding any incorrect solutions . However, not for every reasoning tool, training data is initially similarly abundant. Therefore, more elaborate data generation approaches may be helpful in order to more readily tap into the potential of less popular tools.

Motivated by the observation that existing methods based on SymPy often struggle to solve counting problems, we investigate an approach based on Z3Py , which excels at counting problems with a small enough answer. As expected due to the relatively lower popularity of Z3Py, even state-of-the-art LLMs such as GPT-4o are not yet proficient users of Z3Py.

In this work, we make the following contributions:

* A method for generating large-scale, high-quality synthetic data for Z3Py that can be used to enhance the mathematical problem-solving capabilities of current LLMs.
* An additional Z3Py synthetic dataset obtained through filtering of problems from existing datasets with traditional rejection sampling of solutions from LLMs.
* A quantitative evaluation on GPT-4o and GPT-4o-mini against their fine-tuned variants using the two approaches for data generation, showing that our new training data generationmethod improves the model performance on the Z3Py formalization task more than rejection sampling solutions of existing problems alone.
* A qualitative evaluation, showing the models fine-tuned on our Z3Py data can solve problems that they were unable to solve without fine-tuning, even if allowed to use arbitrary libraries.

## 2 Background: Enumerating Solutions with Z3Py

Satisfiability modulo theories (SMT) generalizes the Boolean satisfiability problem. Constraints can involve various different types of mathematical objects. An SMT solver takes in a set of constraints and determines whether the constraints can be satisfied, producing a solution ("model") if possible.

The Z3 theorem prover is a state-of-the-art open-source SMT solver. Z3Py is the Z3 API for Python. We solve counting problems with Z3Py using the technique of _blocking_: We iteratively solve constraints, adding constraints that the solution should not be one of the ones found previously. This way, the solver steps through all solutions. Once all solutions have been found, the solver returns that the constraints are unsatisfiable. This way we in particular obtain the number of possible solutions.

In some cases, state-of-the-art LLMs are able to generate Z3Py code that solves a problem specified in natural language. E.g., we prompted GPT-4o with "Please give me code using Z3Py that counts the number of ways to cover a 4x4 chessboard with 2x1 and 1x2 dominos.", and shortened the result:

```
1fromz3import*
2solver=Solver()
3
4n=4
5board=[(x,y)forkxinrange(n)foryinrange(n)]
6dom=[(Bool(f^h_(x)_(y)"),[(x,y),(x+1,y)])forkxinrange(n-1)foryinrange(n)]\
7+[(Bool(f^v_(x)_(y)"),[(x,y),(x,y+1)])forkxinrange(n)foryinrange(n-1)]
8solver.add(And([Sum([vforv,posindomif(x,y)inpos])==1forx,yinboard]))
9
10count=0
11whilesolver.check()==sat:
12model=solver.model()
13count+=1
14solver.add(Or([var!=model[var]forvar,indom]))
15print(count)#prints36(thecorrectanswer) ```

The code first generates pairs of Z3Py Boolean variables and pairs of squares covered, for each way to place a domino. Then, it adds constraints that say that every square of the board should be covered by exactly one domino. Finally, the solutions are enumerated using the blocking technique.

Unfortunately, despite succeeding in simple cases like this one, LLMs often fail to give a correct encoding, particularly for more involved problem statements. In this work, we show how to fine-tune LLMs on the task of producing correct code to solve combinatorics problems using Z3Py.

## 3 Datasets Used for Fine-tuning

We now present our methodology for generating datasets of problems with correct Z3Py solutions.

### Synthetic Problem Generation

To address the lack of training data for Z3py, we generate fully synthetic pairs of counting problems with corresponding Z3py solutions. We focus on four classes of ground sets: Sequences, permutations of the set \(\{1,2,,n\}\), numbers in base \(k\), and subsets of the set \(\{1,2,,n\}\). For each object, we define a set of supported constraints, where each constraint includes a natural language description and corresponding Z3py code. Constraints can be applied to the object itself or to an integer parameter derived from the object, such as the sum of the sequence or the number of inversions in a permutation.

When generating a problem, we sample a ground set and a set of constraints, where each constraint has a probability of \(1/2\) of being negated. These constraints are then combined to form a problem and a Z3Py solution. We run the solution and retain only the problems that finish within the time limit, returning a non-zero answer. Below is an example of a generated synthetic problem:A subset of the set \(\{1,2,,6\}\) (no two elements in the subset are consecutive integers) and (no three elements in the subset form an arithmetic progression) and (the subset sum is not divisible by \(10\)). Count the number of valid objects.

Here is the relevant part of the corresponding Z3py solution generated by our dataset generator:

```
1fromz3import*
2subset=[Bool(f'subset_{i}')foriinrange(6)]
3subset_sum=Sum([If(subset[i],i+1,0)foriinrange(6)])
4constraint_1=And([Not(And(subset[i], subset[i+1]))foriinrange(6-1])]
5constraint_2=And([Not(And([subset[i], subset[j],subset[k]]))\) foriinrange(6)forjinrange(i+1,6)\
6forkinrange(6)forjinrange(i+1,6)\
7forkinrange(j+1,6)ifj-i==k-j])
8constraint_3=subset_sum%10==0
9constraint_4=Not(constraint_3)
10constraint_5=And(constraint_2,constraint_4)
11constraint_6=And(constraint_1,constraint_5)
12solver=Solver()
13solver.add(constraint_6) ```

### Rephrasing Problems

We found that fine-tuning on raw synthetic problems leads to severe overfitting to the specific format of those problems (almost perfect scores on our validation synthetic problems), while degrading the performance on real problems from the NuminaMath dataset . To address this issue and make the problem statements sound more natural, we rephrase the problems using a language model.

To minimize the number of incorrect rephrasings, we use a technique similar to rejection sampling. For each problem, we prompt the language model to generate a naturally sounding problem statement. We then prompt it again to generate a new Z3py solution from the rephrased statement. We verify that the original and generated solutions produce the same integer answer. If they do, we count both the rephrasing and the new solution as correct.

In our experiments, we rephrased each problem twice and sampled three solutions for each rephrasing at temperature \(0.5\). For example, for the problem above, we generated the following rephrasing.

```
1fromz3import*
2subset=[Bool(f'subset_{i}')foriinrange(6)]
3subset_sum=Sum([If(subset[i],i+1,0)foriinrange(6)])
4constraint_1=And([Not(And(subset[i], subset[i+1]))foriinrange(6-1]))
5constraint_2=And([Not(And([subset[i], subset[j],subset[k]]))\)
6foriinrange(6)forjinrange(i+1,6)\
7forkinrange(j+1,6)ifj-i==k-j])
8constraint_3=subset_sum%10==0
9constraint_4=Not(constraint_3)
10constraint_5=And(constraint_2,constraint_4)
11constraint_6=And(constraint_1,constraint_5)
12solver=Solver()
13solver.add(constraint_6) ```

### NuminaMath Dataset

We applied our approach to problems from the NuminaMath dataset . We excluded the synthetic-math portion, as it contains rephrasings (which may be incorrect) of other problems. We retained only problems with integer answers and used the GPT-4o-mini model to filter for counting problems (which are potentially solvable using our Z3py approach). For validation, we selected the amc-aime portion of the dataset, resulting in \(145\) problems after filtering. The remaining \(8935\) filtered problems were used to construct a baseline training set.

For each problem in the training set, we generated 3 Z3py solutions for rejection sampling, using GPT-4o-mini and GPT-4o, respectively. This process left us with \(2624\) training problems for GPT-4o-mini and \(2883\) training problems for GPT-4o.

## 4 Evaluation

We fine-tuned both GPT-4o and GPT-4o-mini for one epoch using a standard learning rate on three datasets: (i) Problems filtered from the NuminaMath dataset, (ii) synthetic problems, and (iii) a mix of NuminaMath and synthetic problems.

Z3 solutionsWe evaluated a total of ten models: four before fine-tuning and six after fine-tuning. To assess their performance, we generated \(32\) samples with a temperature of \(0.7\) for each of the \(145\) problems in the validation set. We ran the default and fine-tuned versions of GPT4o-mini and GPT4o with a few-shot prompt specifically asking for solutions written in Z3py. We include two example problems with solutions in all prompts. We additionally ran the default models with a Z3py reference prepended to the prompt. A solution was considered correct if it executed properly using Z3py and returned the correct answer. We first compare the models in the pass@k metric . For each \(k\), ranging from \(1\) to \(32\), we calculated the expected number of problems solved at least once given that we subsample the results on each problem using \(k\) samples. For each problem solved \(x\) times out of \(32\) trials, the probability of at least one success in \(k\) attempts is given by:

\[P()=1-}{}\]

Summing the probabilities across problems provides the expected number of solved problems per \(k\).

We note that for GPT-4o, fine-tuning on synthetic data outperforms fine-tuning on the NuminaMath dataset, and fine-tuning on a mix of the two outperforms both. For GPT-4o-mini, the results are less clear, as fine-tuning on synthetic data seems to produce the best performance. We also observe that GPT-4o-mini fine-tuned on synthetic data performs better than GPT-4o.

  
**Model** & **Score (maj@32)** \\  gpt4o-mini & 29 \\ gpt4o-mini Z3py reference & 23 \\ gpt4o-mini NuminaMath & 45 \\ gpt4o-mini Synthetic & 49 \\ gpt4o-mini NuminaMath + Synthetic & 44 \\ gpt4o & 45 \\ gpt4o Z3py reference & 47 \\ gpt4o NuminaMath & 59 \\ gpt4o SyntheticData & 59 \\ gpt4o NuminaMath + Synthetic & 62 \\   

To enable a fair comparison in a setting where the answers are unavailable, we also calculate the number of correctly solved problems for each model using the majority vote across the \(32\) samples.

The relative performance of the models aligns with the pass@k metric, supporting the intuition that a model which solves a problem correctly more frequently is more likely to have a majority of correct samples.

General problem solvingAdditionally, we prompted the models to generate any Python code, potentially using SymPy, Z3Py, or any other library. On our validation data set, all fine-tuned models managed to solve additional tasks that were not solved without our fine-tuning: The strongest difference is on GPT-4o, where fine-tuning with NuminaMath yielded 7 additional solutions on our validation set. Of those solutions, 5 used Z3Py. For GPT-4o-mini, NuminaMath and NuminaMath+Synthetic both yielded 4 extra problems solved, where all of the solutions used Z3Py.

## 5 Conclusion and Future Work

We presented an algorithm for synthesizing training data for fine-tuning LLMs to solve mathematical problem statements using SMT solvers. Our experimental results demonstrate that fully synthetic random problem statements can be helpful for tasks with less readily available training data. Furthermore, we showed that fine-tuning on SMT-based solutions improves the general problem solving capabilities of LLMs. Future work includes more advanced synthetic problem statements, using fine-tuned models to bootstrap harder datasets from existing problem statements, as well as applying the approach to other less well-known libraries for mathematical reasoning in addition to Z3Py.

#### Acknowledgments

This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure).