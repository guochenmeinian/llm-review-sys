# Dual Cone Gradient Descent for Training

Physics-Informed Neural Networks

Youngsik Hwang

Artificial Intelligence Graduate School

UNIST

hys3835@unist.ac.kr &Dong-Young Lim

Department of Industrial Engineering

Artificial Intelligence Graduate School

UNIST

dlim@unist.ac.kr

Corresponding author.

###### Abstract

Physics-informed neural networks (PINNs) have emerged as a prominent approach for solving partial differential equations (PDEs) by minimizing a combined loss function that incorporates both boundary loss and PDE residual loss. Despite their remarkable empirical performance in various scientific computing tasks, PINNs often fail to generate reasonable solutions, and such pathological behaviors remain difficult to explain and resolve. In this paper, we identify that PINNs can be adversely trained when gradients of each loss function exhibit a significant imbalance in their magnitudes and present a negative inner product value. To address these issues, we propose a novel framework for multi-objective optimization, _Dual Cone Gradient Descent_ (DCGD), which adjusts the direction of the updated gradient to ensure it falls within a dual cone region. This region is defined as a set of vectors where the inner products with both the gradients of the PDE residual loss and the boundary loss are non-negative. Theoretically, we analyze the convergence properties of DCGD algorithms in a non-convex setting. On a variety of benchmark equations, we demonstrate that DCGD outperforms other optimization algorithms in terms of various evaluation metrics. In particular, DCGD achieves superior predictive accuracy and enhances the stability of training for failure modes of PINNs and complex PDEs, compared to existing optimally tuned models. Moreover, DCGD can be further improved by combining it with popular strategies for PINNs, including learning rate annealing and the Neural Tangent Kernel (NTK). Codes are available at https://github.com/youngsikhwang/Dual-Cone-Gradient-Descent.

## 1 Introduction

Physics-informed Neural Networks (PINNs) proposed in Raissi et al.  have created a new paradigm in deep learning for solving forward and inverse problems involving partial differential equations (PDEs). The key idea of PINNs is to integrate physical constraints, governed by PDEs, into the loss function of neural networks. This is in turn equivalent to finding optimal parameters for the neural network by minimizing a loss function that combines boundary loss and PDE residual loss. Thanks to their strong approximation ability and mesh-free advantage, PINNs have achieved great success in a wide range of applications .

Building upon this success, the applications of PINNs have been extended to solve other functional equations, including integro-differential equations , fractional PDEs , and stochastic PDEs . Moreover, numerous variants of PINNs have been developed to enhance their computational efficiency and accuracy via domain decomposition methods , advanced neural network architectures , modified loss functions , different sampling strategies , andprobabilistic PINNs [25; 26]. Recent studies have also explored optimizing PINNs by leveraging function space geometry, providing an alternative perspective to enhance accuracy and computational efficiency [27; 28].

Despite these achievements, several studies have reported that PINNs often fail to learn correct solutions for given problems ranging from highly complex to relatively simple PDEs [29; 30; 31]. Due to the unclear nature of pathologies in the training of PINNs, it has become a critical research topic to explain and mitigate these phenomena. For example, [32; 33] observed that PINNs tends to get stuck at trivial solutions while violating given PDE constraints over collocation points. The imbalance between PDE residual loss and boundary loss was explored in Wang et al. , and a spectral bias of PINNs was studied in Wang et al. . Yao et al.  discussed the gap between the loss function and the actual performance. Even with the insights from the aforementioned studies, a comprehensive understanding of PINN's failure modes remains largely unexplored in various scenarios.

In this paper, we explore these challenges from a novel perspective of multi-objective optimization. We first provide a geometric analysis showing that PINNs can be adversely trained when the gradients of each loss function exhibit a significant imbalance in their magnitudes, coupled with a negative inner product value. Based on this finding, we characterize a dual cone region where both PDE residual loss and boundary loss can be decreased simultaneously without the adverse training phenomenon. We then propose a novel optimization framework, _Dual Cone Gradient Descent_ (DCGD), for training PINNs which updates the gradient direction to be contained in the dual cone region at each iteration. Furthermore, we study the convergence properties of DCGD in a non-convex setting. In particular, we find that DCGD can converge to a Pareto-stationary point. We validate the superior empirical performance and universal applicability of DCGD through extensive experiments.

## 2 Preliminaries

Notation.The Euclidean scalar product is denoted by \(,\ \), with \(\|\|\) standing for the Euclidean norm (where the dimension of the space may vary depending on the context). For a subspace \(W\) of a vector space \(V\), its orthogonal complement \(W^{}\) is defined as

\[W^{}:=\{v V| u,\,v=0, u W\}.\]

For a vector \(v V\), the projection of \(v\) on a nontrivial subspace \(W\) is denoted by \(v_{\|W}\). Unless otherwise specified, \(V\) represents \(^{d}\) throughout the paper.

Related Works.Among various research directions in PINNs, we focus on reviewing optimization strategies for PINNs. These can be broadly categorized into three main approaches: adaptive loss balancing, gradient manipulation, and Multi-Task Learning (MTL). As an example of adaptive loss balancing algorithms, Wang et al.  proposed a learning rate annealing (LRA) algorithm that balances the loss terms by utilizing gradient statistics. Wang et al.  utilized the eigenvalues of the Neural Tangent Kernel (NTK) to address the disparity in convergence rates among different losses of PINNs. For gradient manipulation algorithms, the Dynamic Pulling Method (DPM) was proposed in  to prioritize the reduction of the PDE residual loss. In , the authors used the PCGrad algorithm, proposed in , for training PINNs to address multi-task learning challenges. Li et al.  developed an adaptive gradient descent algorithm (AGDA) that resolves the conflict by projecting boundary condition loss gradient to the normal plane of the PDE residual loss gradient. Yao et al.  recently developed MultiAdam, a scale-invariant optimizer, to mitigate the domain scaling effect in PINNs. Another important line of gradient manipulation involves Multi-Task Learning (MTL) algorithms, which optimize a single model to perform multiple tasks simultaneously [39; 40; 37; 41; 42; 43; 44]. We will discuss that several MTL algorithms can be unified within the proposed DCGD framework.

Physics-Informed Neural Networks.Let \(^{D}\) be a domain and \(\) be the boundary of \(\). We consider the following nonlinear PDEs:

\[[u]()&=f(), \\ [u]()&=g(), \] (2.1)

where \(\) and \(\) denote a nonlinear differential operator and a boundary condition operator, respectively. We approximate \(u()\) by a deep neural network \(u(;)\) parameterized by \(\). To train the neural network, the framework of PINNs minimizes the total loss function \(()\), which is a weighted sum of PDE residual loss \(_{r}()\) and boundary condition loss \(_{b}()\), defined by:

\[():=_{r}_{r}()+_{b} _{b}()\] (2.2) \[_{r}():=}_{i=1}^{N_{r}}| [u(;)](_{r}^{i})-f(_{r}^{i})|^{2}, _{b}():=}_{i=1}^{N_{b}}|[u( ;)](_{b}^{i})-g(_{b}^{i})|^{2},\]

where \(_{r},_{b} 0\) are weights of each loss term, \(\{_{r}^{i}\}_{i=1}^{N_{r}}\) denotes a set of collocation points that are randomly sampled in \(\), and \(\{_{b}^{i}\}_{i=1}^{N_{b}}\) the boundary sample points. Here, we set \(_{r}=_{b}=1\) throughout the paper. We note that the training of PINNs falls into the category of multi-objective learning due to its structure of the loss function \(()\) in Eq. (2.2).

## 3 Empirical Observations and Issues in Training PINNs

This section investigates issues that are frequently observed during the training of PINNs in the context of multi-objective learning. The parameter for the PINN solution \(u(;)\) is typically estimated by minimizing the total loss function \(()\) with a (stochastic) gradient descent method2:

\[_{t+1}=_{t}-(_{t}), t \]

where \(()\) is the gradient of the total loss function \(()\) with respect to \(\). However, a careless adoption of standard gradient descent methods may lead to an incorrect solution, as reducing the total loss does not necessarily imply a decrease in both the PDE residual loss and boundary loss. This phenomenon is clearly illustrated in Figure 1, which displays the curves of the total loss, PDE residual loss, and boundary loss over epochs for solving the viscous Burger's equation. Notably, while the total loss consistently decreases throughout the training, the PDE loss adversely increases.

Conflicting and dominating gradients in PINNs.This issue is highly related with discrepancies in the direction and magnitude between two gradients of the PDE residual and boundary loss. Specifically, we define two gradients to be _conflicting_ at the \(t\)-th iteration if they have a negative inner product value, i.e., \(<_{t}\) where \(_{t}\) is the angle between \(_{r}(_{t})\) and \(_{b}(_{t})\). When there are conflicting gradients, parameter updates to minimize one loss function might increase the other, leading to an inefficient learning process such as oscillating between optimizing for the two loss functions and resulting in degraded solution quality . Another problem arises when one gradient is much larger than the other, i.e., \(\|_{r}(_{t})\|\|_{b}(_{t})\|\) or \(\|_{r}(_{t})\|\|_{b}(_{t})\|\). The significant differences3 in the magnitudes of gradients in PINNs might create a situation where the optimization algorithm primarily minimizes one loss function while neglecting the other. This often results in slow convergence and overshooting, as the smaller gradient, though neglected, may be more crucial in finding a better solution. To mitigate the imbalance in the gradients, loss balancing approaches to rescale the weights of each loss term have been proposed [30; 31].

To examine these challenges in training PINNs, we record cosine value of the angle between \(_{r}\) and \(_{b}\), and the ratios of their magnitudes while training a PINN for the Helmholtz equation. Figure 2(a) shows that conflicting gradients are observed in about half of the total iterations. Moreover, we observe that the magnitude of the gradient of the PDE residual is several tens to hundreds of times larger than that of the boundary loss (See Figure 2(b)). That is, conflicting and dominating gradients are prevalent issues in the training of PINNs.

## 4 Methodology

In this section, we provide a geometric analysis to identify a dual cone region where both the PDE residual loss and the boundary loss can decrease simultaneously. Subsequently, we introduce a general framework for DCGD algorithms, ensuring that the updated gradient falls within this region. We then propose three specifications of DCGD algorithms: projection, average, and center. All proofs for main results in this section can be found in Appendix A.

### Dual Cone Region

The concept of a dual cone plays a pivotal role in our DCGD algorithm. Formally, a dual cone is defined as a set of vectors that have nonnegative inner product values with a given cone.

**Definition 4.1**.: (Dual cone) Let \(\) be a cone of \(^{d}\). Then, the set

\[^{*}=\{y| x,\,y 0x,y ^{d}\}\]

is called the _dual cone_ of \(\).

For each iteration \(t\), consider a cone denoted by \(_{t}\), which is generated by rays of two gradients, \(_{r}(_{t})\) and \(_{b}(_{t})\):

\[_{t}:=\{cx|c 0,x\{_{r}(_{t}), _{b}(_{t})\}\}\,.\]

In the context of PINNs, the dual cone of \(_{t}\), denoted by \(_{t}^{*}\), represents the set of gradient vectors where each vector is neither conflicting with the gradient of the PDE loss nor with the gradient of the boundary loss, i.e., for \(u_{t}^{*}\), \( u,\,_{r}(_{t}) 0\) and \( u,\,_{b}(_{t}) 0\).

In other words, when the total gradient \((_{t})\) is in \(_{t}^{*}\) (as depicted by the region of the red line in Figure 1), the standard gradient descent taking the direction \((_{t})\) will decrease both the PDE and boundary losses for a suitable step size. On the other hand, if \((_{t})_{t}^{*}\) (the region indicated by the blue line in Figure 1), one of the two losses will adversely increase even with sufficiently small step sizes.

This indicates that the training process of PINNs can significantly vary depending on whether the total gradient belongs to the dual cone region. The following theorem establishes the necessary and sufficient conditions under which the total gradient falls within the dual cone region in terms of the angle and relative magnitude between the gradients of the PDE residual and boundary loss.

**Theorem 4.2**.: _Suppose that \(_{r}(_{t})\) and \(_{b}(_{t})\) are given at each iteration \(t\). Let \(_{t}\) be the angle between \(_{r}(_{t})\) and \(_{b}(_{t})\), and \(R=_{r}(_{t})\|}{\|_{b}( _{t})\|}\) be their relative magnitude. Then, \((_{t})_{t}^{*}\) if and only if_

\[(i)\ _{b}(_{t}),\,_{r }(_{t}) 0\,,\] \[(ii)\ _{b}(_{t}),\,_ {r}(_{t})<0-_{t} R-}.\]

Theorem 4.2 provides a clear criterion for when conflicting and dominating gradients lead to adverse training in PINNs. For instance, the condition (ii) in Theorem 4.2 implies that the larger \(_{t}\) (the more conflicting they are), even a slight difference in their magnitudes can result in adverse training. In particular, Theorem 4.2 quantifies the extent of problematic relative magnitude between the twogradients, thereby clarifying the concept of dominating gradients, which has not been previously defined in the literature.

Thus, our strategy aims to devise an algorithm that chooses the updated gradient within the dual cone region at each gradient descent step. For notational simplicity, we write \(_{t}_{\|_{+}^{}}\) and \(_{t}_{\|_{b}^{}}\) to represent \((_{t})_{\|(_{r}(_{t}))^{}}\) and \((_{t})_{\|(_{b}(_{t}))^{}}\), respectively. In particular, we are interested in a simple and explicit subspace \(_{t}\), defined as the set of conic combinations of \(_{t}_{\|_{r}^{}}\) and \(_{t}_{\|_{b}^{}}\):

\[_{t}:=\{c_{1}_{t}_{\| _{r}^{}}+c_{2}_{t}_{\|_{b}^ {}}c_{1},c_{2} 0\},\] (4.1)

for two reasons. Firstly, all vectors in \(_{t}\) are easily computable due to the explicit expression of \(_{t}\), whereas the dual cone \(^{*}\) is implicitly defined. Secondly, \(_{t}\) contains two important components of \(_{t}^{*}\), which are the projections of \((_{t})\) onto \(_{r}(_{t})^{}\) and \(_{b}(_{t})^{}\) by its construction. The next proposition shows that \(_{t}\) always belongs to the dual cone region as illustrated in Figure 3.

**Proposition 4.3**.: _Suppose that \(_{r}(_{t})\) and \(_{b}(_{t})\) are given at each iteration \(t\). Consider \(_{t}\), the set of conic combinations of \(_{t}_{\|_{r}^{}}\) and \(_{t}_{\|_{b}^{}}\), defined in Eq. (4.1). Then, \(_{t}_{t}^{*}\)._

Consequently, the DCGD algorithm defines the updated gradient denoted by \(g_{t}^{}\) within \(_{t}\) at each iteration \(t\). A general framework for DCGD is presented in Algo 1.

``` Require: learning rate \(\), max epoch \(T\), initial point \(_{0}\) for\(t=1\)to\(T\)do  Choose \(g_{t}^{}_{t}^{*}\) \(_{t}=_{t-1}- g_{t}^{}\) endfor ```

**Algorithm 1** Dual Cone Gradient Descent (base)

### Convergence Analysis

To discuss the convergence properties of DCGD, we introduce the concept of Pareto optimality (adapted to the PINN setting), which is a key in multi-objective optimization .

**Definition 4.4**.: (Pareto optimal and stationary) A point \(^{d}\) is said to be _Pareto-optimal_ if there does not exist \(^{}^{d}\) such that

\[_{r}(^{})_{r}() _{b}(^{})_{b}().\]

In addition, a point \(^{d}\) is said to be _Pareto-stationary_ if there exists \(_{1},_{2}\) such that

\[_{1}_{r}()+_{2}_{b}( )=0,_{1},_{2} 0,_{1}+_{2}=1.\]

Intuitively, a Pareto-stationary point implies there is no feasible descent direction that would decrease all loss functions simultaneously. For example, consider a point \(_{t}\) at which the cosine of the angle \(_{t}\) between \(_{r}(_{t})\) and \(_{b}(_{t})\) is \(-1\), i.e., \((_{t})=-1\). Such a point is Pareto-stationary.

The following theorem guarantees the convergence of the DCGD algorithm proposed in Algo 1 under some regularities in a non-convex setting. Assume \((^{*}):=_{^{d}}()>-\).

**Theorem 4.5**.: _Assume that both loss functions, \(_{b}()\) and \(_{r}()\), are differentiable and the total gradient \(()\) is \(L\)-Lipschitz continuous with \(L>0\). If \(g_{t}^{}\) satisfies the following two conditions:_

1. \(2(_{t}),\,g_{t}^{}-\|g_{t}^{ }\|^{2} 0\)_,_
2. _There exists_ \(M>0\) _such that_ \(\|g_{t}^{}\| M\|(_{t})\|\)_,_

_then, for \(\), DCGD in Algo. 1 converges to a Pareto-stationary point, or converges as_

\[_{t=0}^{T}\|(_{t})\|^{2}(_{0})-(^{*}))}{ M(T+1)}.\] (4.2)

Theorem 4.5 states that DCGD converges to either a Pareto-stationary point, characterized by \(_{t}\) such that \((_{t})=-1\), or a stationary point at a rate of \((1/)\) in the nonconvex setting. Unlike single-objective (nonconvex) optimization where the goal is to pursue a stationary point, in multi-objective optimization, it is ideal to find a Pareto-stationary point that balances all loss functions. Thus, DCGD offers significant theoretical and empirical advantages over popular optimization algorithms like SGD and ADAM, which are only guaranteed to converge to a stationary point. The convergence of DCGD to a Pareto-stationary point is empirically verified in Section 4.4.

### Dual Cone Gradient Descent: Projection, Average, and Center

Different variants of DCGD can be designed by properly choosing the updated gradient \(g_{t}^{}\) in \(_{t}\) satisfying the conditions (i), (ii) of Theorem 4.5. We present three specific algorithms: projection, average, and center.

The first algorithm, named DCGD (Projection), uses the projection of the total gradient \((_{t})\) onto \(_{t}\) when \((_{t})_{t}^{*}\), which is the closest vector within \(_{t}\) to \((_{t})\). Specifically, the DCGD (Projection) algorithm specifies \(g_{t}^{}\) as follows: **(i)**\((_{t})\) if \((_{t})_{t}^{*}\), **(ii)**\(_{t}_{\|_{}}\) (\(c_{1}=1,c_{2}=0\)) if \((_{t})_{t}^{*}\) and \((_{t}),\,_{r}(_{t})<0\), **(iii)**\(_{t}_{\|_{}}\)(\(c_{1}=0,c_{2}=1\)) if \((_{t})_{t}^{*}\) and \((_{t}),\,_{b}(_{t})<0\). See also Eq. (E.1) and Algo. 2.

DCGD (Average) algorithm takes the average of \(_{t}_{\|_{}}\) and \(_{t}_{\|_{b}^{}}\) when the total gradient is outside \(_{t}^{*}\), i.e., \(c_{1}=c_{2}=\) if \((_{t})_{t}^{*}\). See Eq. (E.2) and Algo. 3.

We note that both DCGD (Projection) and DCGD (Average) use \((_{t})\) as \(g_{t}^{}\) without any manipulation when \((_{t})_{t}^{*}\). Moreover, they require determining if the total gradient is contained in the dual cone at each iteration, which may incur additional computational costs. On the other hand, \(g_{t}^{}\) of DCGD (Center) is given by

\[g_{t}^{}:=^{c},\,(_{t}) }{\|g_{t}^{c}\|^{2}}g_{t}^{c}g_{t}^{c}=_{b}(_{t})}{\|_{b}( _{t})\|}+_{r}(_{t})}{\|_{r} (_{t})\|},\] (4.3)

which is geometrically interpreted as the projection of \((_{t})\) onto the angle bisector \(g_{t}^{c}\) of \(_{r}(_{t})\) and \(_{b}(_{t})\). The following proposition shows that \(g_{t}^{}\) of DCGD (Center) resides within \(_{t}\)

**Proposition 4.6**.: _Consider the updated gradient \(g_{t}^{}\) of DCGD (Center) defined in Eq. (4.3). Then, \(g_{t}^{}_{t}\)._

Figure 4: The updated gradient \(g_{t}^{}\) of three DCGD algorithms.

The visualization of these three algorithms can be found in Figure 4 and their pseudocodes are provided in Appendix E. Moreover, the proposed DCGD algorithms satisfy the conditions (i) and (ii) of Theorem 4.5. Consequently, the following Corollary summarizes the convergence of the proposed DCGD algorithms.

**Corollary 4.7**.: _We impose the same assumptions as in Theorem 4.5. Then, DCGD (Projection), DCGD (Average), and DCGD (Center) converge to either a Pareto-stationary point or a stationary point._

In addition to the theoretical result in Corollary 4.7, Appendix D.1 provides an ablation study on the empirical performance of three specific algorithms for solving benchmark PDEs.

### Benefits of the DCGD framework

This subsection discusses benefits of DCGD through illustrative examples. We first investigate how the proposed DCGD algorithms resolve the conflicting gradient issue discussed in Section 3. Given each algorithm, at each iteration \(t\), we define \(_{t}^{r}\) as the angle between the updated vector and \(_{r}(_{t})\), and \(_{t}^{b}\) as the angle between the updated vector and \(_{b}(_{t})\). Also, let \(_{t}^{}=\{_{t}^{r},_{t}^{b}\}\). We highlight that both \(_{t}^{r}\) and \(_{t}^{b}\) are less than \(/2\) under DCGD algorithms, as they ensure that the updated vectors always belong to the dual cone. Figure 5 plots the distributions of \((_{t}^{})\) for four different optimization algorithms: ADAM, DCGD (Projection), DCGD (Average), and DCGD (Center) during the training of PINNs for solving the Helmholtz equation. It shows that three DCGD algorithms completely eliminate conflicting gradients in contrast to ADAM. Moreover, we observe that the distributions of \((_{t}^{})\) for DCGD (Projection) and DCGD (Average) are highly skewed toward zero, which implies that one of the two losses is unlikely to significantly improve. On the contrary, DCGD (Center) has a bell-shaped distribution with a mean of about 0.719, indicating that the two gradients are more aligned. This leads to a continuous reduction in both losses in a harmonious manner. Consistent with this observation, DCGD (Center) consistently outperforms DCGD (Projection) and DCGD (Average) in our experiments. Please refer to the ablation study D.1 for further comparisons.

We empirically demonstrate that DCGD can converge to a Pareto-stationary point. Consider a (slightly modified) toy example shown in [37; 41], which has two objective functions; see Appendix C.2 for more details. We solve the problem with 1,600 uniformly sampled initial points using ADAM, DCGD (Projection), DCGD (Average), and DCGD (Center). Then, we mark with a red dot the point at which the algorithm fails to reach a Pareto-stationary point. Figure 6 shows that while ADAM does not reach a Pareto-stationary point across many areas, all DCGD algorithms achieve convergence to Pareto-stationary points throughout the entire space.

Several MTL algorithms, such as PCGrad , MGDA , CAGrad , Aligned-MTL , and Nash-MTL  have been developed based on different and independent approaches. In contrast, the proposed DCGD framework provides a principled solution to the problem of conflicting gradients by directly characterizing the dual cone. As a result, our framework unifies many of these MTL

Figure 6: Toy example: the region where the algorithm fails to reach a Pareto-stationary point in multi-objective optimization

algorithms as special cases, offering significant contributions not only to PINNs but also to the MTL domain. Proofs for the unification of MTL algorithms within the DCGD framework can be found in Appendix B.

## 5 Numerical Experiment

This section demonstrates the superiority of DCGD through three distinct perspectives. In Section 5.1, we compare the performance of DCGD on five benchmark equations with that of a range of methods, including ADAM , Learning Rate Annealing (LRA) , Neural Tangent Kernel (NTK) , PCGrad , MGDA , CAGrad , Aligned-MTL , MultiAdam , and DPM . Section 5.2 shows that DCGD can provide more accurate solutions for failure modes of PINNs and complex PDEs where vanilla PINNs fail. In Section 5.3, we explore the compatibility of DCGD with existing loss balancing schemes such as LRA and NTK.

To compare the effectiveness of DCGD with other optimization algorithms, we measure the accuracy of the PINN solution trained by each optimizer using the relative \(L^{2}\)-error. Then, we run each experiment across \(10\) independent trials and report the mean, standard deviation, max, and min of the best accuracy.

### Comparison on benchmark equations

We solve three popular benchmark equations (the Helmholtz equation, the viscous Burgers' equation, and the Klein-Gordon equation) and two high-dimensional PDEs (5D-Heat equation and 3D-Helmholtz equation) using vanilla PINNs with different optimization techniques. For DCGD, we employ an adaptive gradient version of the DCGD (Center) algorithm, the DCGD (Center) combined with ADAM (see Algo 5) by default for all experiments, provided in Appendix D.1. For other methods, we perform careful hyperparameter tuning based on the recommendations in their papers. The PDE equations and detailed experimental setting are provided in Appendix C.4. However, we do not report the performance of DPM because it is not only highly sensitive to hyperparameters but also exhibit poor performance, consistently observed in .

Table 1 displays the mean and standard deviation of the relative \(L^{2}\) errors for each optimization algorithm applied to the three PDE equations. The error plots of approximated PINN solutions and other statistics of relative \(L^{2}\) errors are summarized in Appendix C.4. In the result tables, we highlight **the best** and _the second-best_ methods. While the second best methods vary across experiments, the proposed method consistently outperforms other algorithms achieving the lowest \(L^{2}\) errors. This result underscores the robustness and adaptability of our method for solving various PDEs.

### Failure mode of PINNs and Complex P(I)DEs

    & &  \\  Optimizer & Helmholtz & Burgers’ & Klein-Gordon & Heat (5D) & Helmholtz (3D) \\  ADAM & 0.0609 (0.0231) & 0.0683 (0.0285) & 0.0792 (0.0386) & 0.0097 (0.0072) & 0.6109 (0.2096) \\ LRA & 0.0066 (0.0025) & 0.0180 (0.0094) & 0.0069 (0.0037) & 0.0052 (0.0056) & 0.0831 (0.0123) \\ NTK & 0.0358 (0.0107) & 0.0224 (0.0061) & 0.0223 (0.0151) & 0.0027 (0.0012) & 0.4037 (0.2620) \\ PCGrad & 0.0109 (0.0031) & 0.0159 (0.0061) & 0.0286 (0.0064) & 0.0083 (0.0049) & 0.2532 (0.0476) \\ MODA & 0.7590 (0.1180) & 0.9780 (0.0462) & 0.6690 (0.2790) & - & 0.9883 (0.0217) \\ CAGrad & 0.0735 (0.0390) & 0.0321 (0.0063) & 0.1850 (0.0301) & 0.0043 (0.0016) & 0.5854 (0.3032) \\ Aligned-MTL & 0.6570 (0.0805) & 0.0294 (0.0129) & 0.5571 (0.1824) & 0.0013 (0.0004) & 0.9138 (0.0645) \\ MultiAdam & 0.0211 (0.0032) & 0.0875 (0.0303) & 0.0228 (0.0038) & 0.0099 (0.0007) & 0.7809 (0.0031) \\ DCGD & **0.0029 (0.0005)** & **0.0124 (0.0046)** & **0.0069 (0.0027)** & **0.0008 (0.0003)** & **0.0774 (0.0250)** \\   DCGD+LRA & 0.0023 (0.0007) & 0.0104 (0.0021) & 0.0050 (0.0013) & 0.0012 (0.0005) & 0.1045 (0.0485) \\ DCGD+NTK & 0.0057 (0.0035) & 0.0113 (0.0040) & 0.0055 (0.0014) & 0.0009 (0.0004) & 0.3525 (0.2659) \\   

Table 1: Average of relative \(L^{2}\) errors in 10 independent trials for each algorithm on three benchmark PDEs (3 independent trials for two high-dimensional PDEs). The value within the parenthesis indicates the standard deviation. ‘-’ denotes that the optimizer failed to converge.

We explore more challenging problems, including failure modes of PINNs and complex PDEs, where vanilla PINNs fail to approximate solutions, and highlight the universal applicability of DCGD. We refer to Appendix C.5 for detailed experimental settings.

First, we revisit the problem of a double pendulum in Steger et al. , which is highly sensitive to initial conditions. The goal is to solve the trajectory of \(\{(_{1}(t),_{2}(t))\}_{t t_{0}}\), governed by the nonlinear differential equation as discussed in Eq. (C.1). The reference solution and its first-order derivative are represented by the blue solid and dotted lines, respectively, in Figure 7. We train PINNs with SGD and ADAM to solve the double pendulum problem, where their solutions are depicted by the red solid and dotted lines in Figure 6(a) and Figure 6(b), respectively. The PINN solutions trained with SGD and ADAM fail to accurately approximate the reference solution. In contrast, the reference solution is successfully recovered by our DCGD algorithm (see Figure 6(c)). Second, we present the performance of DCGD for two challenging PDEs: the chaotic Kuramoto-Sivashinsky (KS) equation and the convection equation. For the chaotic KS equation, we combine DCGD with the causal training scheme of , the current state-of-the art result. For the convection equation, DCGD is applied to PINNsFormer of . As shown in Table 2, DCGD achieves the lowest relative \(L^{2}\) errors for the complex PDEs compared to the existing optimally tuned strategies, demonstrating its effectiveness in overcoming failure modes of PINNs. Third, the universal applicability of DCGD is not limited to specific architectures, sampling techniques, and training schemes. For example, A-PINN, designed for solving integral equations and integro-differential equations, achieves state-of-the art results in nonlinear Volterra IDEs . DCGD significantly improves the performance of A-PINN for solving Volterra IDEs, as shown in Table 2. Moreover, Table 4 shows that the performance of SPINN can be highly improved by applying DCGD for solving multi-dimensional PDEs.

### Compatibility of DCGD with existing methods

The proposed DCGD framework can be easily combined with existing PINN training strategies, including loss balancing methods. To illustrate this advantage, we have designed DCGD algorithms that integrate with LRA and NTK, named DCGD (Center) + LRA and DCGD (Center) + NTK, respectively. Please refer to Algo. 6 for the detailed implementation.

We apply DCGD (Center) + LRA and DCGD (Center) + NTK to the same experiments described in Section 5.1. Tables 1 and 3 demonstrate that the performance of DCGD algorithms can be further enhanced across all the experiments in terms of the mean, maximum, and minimum of relative \(L^{2}\) errors by integrating existing ideas from the literature.

## 6 Conclusion and Discussion

In this work, we provided a clear criterion for when PINNs might be adversely trained, in terms of the angle and relative magnitude ratio of the gradients of the PDE residual and boundary loss, through a geometric analysis. Based on this theoretical insight, we characterized a dual cone region where both losses can decrease simultaneously without gradient pathologies. We then proposed a general

   Equation & Baseline & DCGD \\  Chaotic KS & 0.0687 & **0.0376** \\ Convection & 0.4880 & **0.0246** \\ Volterra IDEs & 0.0068 & **0.0011** \\   

Table 2: Relative \(L^{2}\) errors for DCGD (Center) on Chaotic KS equation, Convection equation and Volterra IDEs.

Figure 7: Double pendulum problem: prediction of each method. SGD and ADAM find shifted solutions, but DCGD successfully approximates the reference solution.

framework for DCGD, which ensures that the updated gradient falls within the dual cone region, and provided a convergence analysis. Within this general framework, we introduced three specific DCGD algorithms and conduct extensive empirical experiments. Our experimental results demonstrate that the proposed DCGD algorithms outperform other optimization algorithms. In particular, DCGD is efficient in solving challenging problems such as failure modes of PINNs and complex PDEs compared to the current state-of-the art approaches. Furthermore, DCGD can be easily combined with other strategies and applied to variants of PINNs.

Although we have presented a novel optimization algorithm, DCGD, to address challenging issues in PINNs, there still remain some interesting and important questions. For instance, one could design a more powerful DCGD specification within the dual cone region that goes beyond the projection, average, and center techniques. Also, while we mainly consider multi-objective optimization for PINNs, future work can focus on more general and complex types of multi-task learning problems.