# If You Want to Be Robust, Be Wary of Initialization

Sofiane Ennadir

KTH

Stockholm, Sweden

&Johannes F. Lutzeyer

LIX, Ecole Polytechnique

IP Paris, France

&Michalis Vazirgiannis

KTH & Ecole Polytechnique

Stockholm, Sweden

&El Houcine Bergou

UM6P

Benguerir, Morocco

Corresponding Author: ennadir@kth.se

###### Abstract

Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model's robustness. We introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model's vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks. Extensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50% compared to alternative initialization approaches.

## 1 Introduction

Neural networks have demonstrated remarkable progress across various domains, ranging from computer vision  to natural language processing , proving their ability to model and extract complex insights from real-world datasets. Recently, Graph Neural Networks (GNNs)  have emerged as a powerful extension of neural networks specifically tailored to tackle graph-structured data. These models have led to rapid progress in solving tasks such as node and graph classification where their application have spanned from drug design , protein resistance analysis , session-based recommendations  to tabular data . Concurrently with their success, deep learning architectures have been shown to be unstable when subject to adversarial perturbations , resulting in unreliable predictions, consequently questioning these models' applicability in critical domains. While most adversarial robustness studies focus on the domain of computer vision, recent work  studying the robustness of GNNs has emerged. Given their rich nature, graphs allow different attack schemes, where the attacker can either choose to edit the graph structure (by adding/deleting edges) or edit the node/edge features. In parallel, recent studies have been devoted to studying approaches to defend against these attacks and enhance GNN robustness, such as input pre-processing techniques , low-rank approximation , edge-pruning  or adapting the message-passing schemes .

The majority of available defense studies focus on understanding the inner dynamics of GNNs to pinpoint and mitigate adversarial vulnerabilities. While analyzing the message-passing mechanism and implementing input pre-processing techniques remains a viable direction, comprehensive understanding necessitates exploration beyond traditional avenues. In this sense, investigating factors such as weight initialization strategies and the impact of other hyperparameters, notably those associated with optimization mechanisms, can offer new insights and perspectives on achieving GNN global robustness. Hyperparameter choices and tuning play a critical role in striking a balance between learning the underlying signals in the data and preventing overfitting to ensure the model's generalization. Hence, existing studies on initialization mainly evolve around understanding its effect on the model's convergence, stability and performance [34; 23]. In contrast, our work primarily focuses on examining the effect of initialization on a model's underlying adversarial robustness, representing to the best of our knowledge the first exploration of its kind. Our main objective is to provide a theoretical understanding of the link between weight initialization and other dynamics such as the number of training steps and the resulting model's robustness. With this perspective in mind, we start by formalizing robustness in the context of GNNs when subjected to structural and node feature-based adversarial attacks. Subsequently, we derive an upper bound that connects the model's robustness to the weight initialization strategies. Specifically, we illustrate that this bound depends on the initial weight norms and the number of training epochs. Finally, we validate our theoretical findings by demonstrating the effects of employing various initialization strategies on the model's robustness using benchmark adversarial attacks on real-world datasets. Note that while our analysis primarily focuses on the widely used Graph Convolutional Networks (GCNs)  and Graph Isomorphism Networks (GINs) , we highlight the versatility of our approach by providing a general upper bound applicable to any Deep Neural Networks in Section 5. This underlines the potential for extending our analysis to a wide range of architectures, showcasing its broad applicability in understanding and enhancing adversarial robustness in neural networks. We summarize our contributions as follows:

* We provide a theoretical analysis that links weight initialization strategies with adversarial robustness in GNNs. We specifically derive an upper bound connecting a model's robustness to weight initialization and the number of training epochs, demonstrating that the initialization strategy can significantly influence the network's adversarial robustness.
* We validate our theoretical findings by conducting extensive experiments across various models using different benchmark adversarial attacks on real-world datasets. These experiments demonstrate that certain weight initialization strategies can enhance the model's defense against adversarial attacks, without degrading its performance on clean datasets.
* While our primary focus is on GNNs, we extend our analysis to Deep Neural Networks, illustrating the broader applicability of our theoretical analysis and its corresponding insights.

## 2 Related Work

**Graph Adversarial Attacks.** Multiple studies focus on designing adversarial attacks capable of fooling a graph-based classifier [16; 35; 10]. The majority of these methods [42; 37] approach the adversarial aim as an optimization problem and employ different methods to solve it such as meta-learning . Furthermore, Nettack  constrained the problem by preserving degree distribution and imposing constraints on feature co-occurrence to generate unnoticeable perturbations. Finally, reinforcement learning was proposed recently as a means to generate graph adversarial attacks .

**Graph Adversarial Defenses.** Recent efforts have emerged to defend against the aforementioned adversarial attacks. In particular, methods such as low-rank matrix approximation coupled with graph anomaly detection  have been used. For example, GNN-Jaccard  proposed to pre-process the graph's adjacency matrix to detect potential manipulation of edges. Other methods such as edge pruning  and transfer learning  have been leveraged to limit the effect of poisoning attacks. Additionally, adaptations of the message-passing scheme, such as employing orthogonal weights  or introducing noise during training , have been shown to perform well in terms of defense. Furthermore, there is a growing interest in exploring robustness certificates [42; 4] as a means of ensuring model robustness. For instance,  used randomized smoothing to provide a highly scalable model-agnostic certificate for graphs. Additionally, other robustness certificates for GCN-based graph classification under topological perturbations have been proposed .

**Weight Initialization.** The impact of weight initialization has been extensively studied both theoretically and empirically where the main line of study consists of understanding the interplay between initialization techniques and the implicit regularization they induce, thereby elucidating their influence on a model's generalization capabilities [34; 23]. For instance, it has been showcased that sampling initial weights from the orthogonal group can speed up convergence . Similarly, alternative initialization approaches such as the Glorot Initialization  and Kaiming Initialization  have been proposed in efforts to improve the model's performance.

Our work stands apart from existing research on adversarial robustness as it represents, to the best of our knowledge, the first attempt to theoretically investigate the impact of initialization on a model's robustness. Moreover, our approach diverges fundamentally from existing literature on weight initialization as our focus lies in theoretically understanding the effect of initialization on a model's robustness rather than its implications for generalization or convergence.

## 3 Graph Adversarial Robustness

In this section, we start by introducing the notation and some fundamental concepts related to GNNs. We afterwards establish the problem setup together with the set of considered assumptions. We finally lay out a GNN's robustness formalization on which we will build our theoretical analysis.

### Preliminaries

Let \(G=(V,E)\) be a graph where \(V\) (\(|V|=n\)) is its set of vertices and \(E\) its set of edges. We denote \(A\{0,1\}^{n n}\) its adjacency matrix. The graph nodes are annotated with feature vectors \(X^{n d}\) (the \(i\)-th row of \(X\) corresponds to the feature of node \(i\)). We denote by \((i)\) the neighbors of node \(i V\) and \(\|\|_{2}\) the Euclidean (resp., spectral) norm for vectors (resp., matrices).

In this work, we consider the task of node classification. In this task, every node is assigned exactly one class from \(=\{1,2,,C\}\) and we consider \(d_{Y}\) as a distance within the output space \(\). The learning objective is to find a function \(f_{W}\), parameterized by \(W\), that assigns each node \(i V\) a class \(c\) while minimizing some classification loss (e. g., cross-entropy loss), denoted as \(\).

**GNNs.** A GNN model consists of a series of neighborhood aggregation layers that use the graph structure and the node features from the previous layers to generate new node representations. Specifically, GNNs update node feature vectors by aggregating local neighborhood information. In the particular case of GCNs, this process is described by the following iterative propagation:

\[h^{()}=^{()}(h^{(-1)}W^{()}),\] (1)

with \(W^{()}^{p q}\) being the weight matrix in the \(\)-th layer, \(p\) and \(q\) are embedding dimensions and \(^{()}\) is a non-linear activation function. Moreover, \(^{n n}\) denotes the normalized adjacency matrix \(=D^{-1/2}AD^{-1/2}\), where \(D=(|(1)|,|(2)|,,|(n)|)\) is the degree matrix.

**Problem Setup.** For our theoretical analysis, we assume that the model is based on 1-Lipschitz activation functions (which is a characteristic of commonly used activation functions such as tanh). Additionally, we consider the training loss function \(\) to be \(L\)-smooth and that it is minimized using gradient descent. We denote by \(W_{*}\) the local optimum towards which gradient descent iteratively converges. Specifically, for a learning rate \(\), the update at time step \(t\) for a layer \(i\) is:

\[W_{t+1}^{(i)}=W_{t}^{(i)}-(W_{t}^{(i)}).\]

It is worth emphasizing that although we focus on the node classification task, which is prevalent and well-studied in the literature of adversarial robustness, our analysis is equally applicable to other tasks such as graph classification. Moreover, while our theoretical analysis predominantly centers around using gradient descent as the optimizer, this choice does not limit the generality of our findings. One can employ a different optimizer and still yield the same insights and results by following a similar approach as the one outlined in this paper. Consequently, this specific setup should not be perceived as a limitation but rather as an analytical choice.

### Adversarial Robustness for Graph Neural Networks

Let \(f:(,)\) be a GNN-classifier following the framework outlined in Section 3.1. An adversarial attacks consists of generating an alternative graph \((,)\) that perturbs the original prediction \(f(A,)\) while not being far (semantically) from the original graph. Typically, this generated graph must adhere to a number of constraints related to its similarity to the original graph, defined by a perturbation budget \(\) controlling the number of edited edges or features. The set of these graphs is written as \(B([A,X];)=\{(,):_{P}(\|A-P P^{T}\|_{2}+\|X-P\|_{2})\}\), where \(\) represents the set of permutations of the adjacency matrix. While the previous formulation relies on the \(_{2}\) norm, other norms may be used depending on the domain of application and the specific use case. Building on previous work , the adversarial risk of a GNN can be defined as the expected error of adjacent graphs within the considered graph's neighborhood defined by \(\) written as:

\[_{}[f]=*{}_{(A,X)} [_{(,) B([A,X];)}d_{}(f(,),f(A,X))].\] (2)

In the current analysis, we focus on the \(_{2}\) norm as our output distance \(d_{}\) (which can be substituted by any norm - given the equivalence of norms). We theoretically approach the introduced adversarial risk by deriving an upper-bound, which reflects the model's expected error under input perturbation. Intuitively, a smaller upper bound reflects a smaller adversarial risk which in turn suggests a robust behavior locally. In this perspective, Definition 1 draws the link between the considered risk quantity and a model's robustness.

**Definition 1**.: (Adversarial Robustness). The graph-based function \(f:(,)\) is said to be \((,)-\) robust if its adversarial risk is upper-bounded by \(\), i. e., \(_{}[f]\).

The current definition addresses adversarial risk from a worst-case scenario perspective, which is the most prevalent approach in the literature. This means we aim to identify the neighbor graph that maximizes the harm (i. e., causes the greatest deviation from the original prediction). By upper-bounding the risk associated with this "worst-case" graph, we inherently account for all other potential adversaries within the same neighborhood, as their risk will be less than or equal to that of the worst-case scenario. We note that the nuances between the "average" and "worst-case" approaches have been thoroughly examined and justified in previous research .

## 4 On the Effect of Initialization

We start by considering the Graph Convolutional Networks (GCNs) within the broader context of Message Passing Neural Networks for node classification. This study investigates how initialization and other hyperparameters impact the final model's robustness. In this context, we aim to establish a connection between the introduced adversarial risk (Equation (2)) and the initial weight distribution and its evolution during training. Specifically, we seek to demonstrate that different choices in the initialization distribution and other relevant parameters lead to varying levels of model robustness, offering new insights into the potential trade-offs between initialization strategies and robustness. In this sense, we derive an upper-bound (denoted as \(\) in Definition 1) on the stability of a GCN-based classifier when the input graph's node features are subject to adversarial attacks.

**Theorem 2**.: _Let \(f:(,)\) denote a graph-based function composed of \(T\) GCN layers, where the initial weight matrix of the \(i\)-th layer is denoted by \(W_{0}^{(i)}\). For adversarial attacks only targeting node features of the input graph, with a budget \(\), we have (in respect to Definition 1):_

\[=_{i=1}^{T}(2^{t}\|W_{0}^{(i)}\|+2^{t+1} \|W_{*}^{(i)}\|)(_{u}_{u})\]

_with \(t\) being the number of training epochs and \(_{u}\) denoting the sum of normalized walks of length \((T-1)\) starting from node \(u\)._

The proof of Theorem 2 is provided in Appendix A. Theorem 2 provides a formal connection between the robustness of a GCN-based classifier and its initial weights, offering valuable insights into theireffects. From a first perspective, the derived upper-bound depends on the initial weight's norm. Specifically, a lower norm corresponds to a smaller upper-bound, indicative of a more robust model. However, while setting all initial weights to zero theoretically yields the smallest upper-bound and consequently the optimum robustness, this direction can detrimtally affect the model's performance on the learning task. Empirical evidence suggests that initializing weights to zero (or a constant) often leads to poor learning outcomes, as it constrains weight behavior during propagation, limiting subsequent back-propagation operations and resulting in convergence to unsatisfactory local minima (e. g., see Page 301 in ). From a second perspective, it appears that a higher number of training epochs leads to the looseness of the upper-bound, resulting in increased adversarial vulnerability. This latter observation provides proof and highlights the existence of the usually discussed trade-off between clean and attacked accuracy. Achieving a balance between increasing the number of epochs to achieve satisfactory clean accuracy and limiting them to attain a robust model is hence essential. While theoretically challenging to identify this equilibrium point, our experimental results demonstrate its existence. We note that the dependence of \(\) on \(t\) can be sharpened by having \((1+ L)^{t}\) instead of \(2^{t}\). With small \(\) (which is usually the case in practice), \((1+ L)^{t} 1+t L\) resulting in a bound which depends linearly on \(t\). The same remark applies to the remaining bounds derived in the paper. These insights, in the case of node-feature-based adversarial attacks, also extend to structural perturbations where Theorem 3 provides the exact bound for this case.

**Theorem 3**.: _Let \(f:(,)\) denote a graph-based function composed of \(T\) GCN layers, where the initial weight matrix of the \(i\)-th layer is denoted by \(W_{0}^{(i)}\). Let \(f\) be the number of used training epochs. When \(f\) is subject to structural attacks, with a budget \(\), we have (in respect to Definition 1):_

\[=_{i=1}^{T}(2^{t}\|W_{0}^{(i)}\|+2^{t+1} \|W_{*}^{(i)}\|)\|X\|(1+T_{i=1}^{T}(2^{t} \|W_{0}^{(i)}\|+2^{t+1}\|W_{*}^{(i)}\|)).\]

The computed upper-bound suggests that the effect of initialization is greater in the case of structural perturbations. This emphasis is resulting from the distinct dynamics within the message passing mechanism, where the influence of the adjacency matrix and node features varies during each propagation step. Precisely, for structural perturbations, the effect of the attack is considered at each propagation step through the perturbed adjacency matrix (in the aggregation step). Moreover, the impact is also amplified by the affected residual layers from previous iterations, resulting in a more significant attack result. This is different in the case of node-feature based adversarial attacks, since the node features are only directly taken into account in the first propagation. Overall, the main takeaway of the provided analysis in Theorems 2 and 3 is that "approximately-free" robustness enhancements can be derived from choosing the right initial weight's distribution and the right number of training epochs. We illustrate this specific point by analyzing the effect of the initial distributions choices on the model's robustness. Specifically, we consider the case of the Gaussian distribution, where Lemma 4 studies how the parameters of this distribution - namely, the mean and variance - exert an influence on the expected (in respect to the initial distribution) value of the adversarial risk.

**Lemma 4**.: _Let \(f:(,)\) denote a graph-based function composed of \(T\) GCN layers for which the initial weight are drawn from the Gaussian distribution \((,)\). When subject to node features based adversarial attacks, we have the following:_

\[_{W_{0}(,)}[_{}[f]] _{i=1}^{T}(2^{t}+()}+2^{t+1} \|W_{*}^{(i)}\|)(_{u}_{u} ).\]

The proof of Lemma 4 is provided in Appendix C. Given that a tighter upper bound inherently results in a higher level of robustness, the results derived in Lemma 4 illustrate the clear effect of initialization in the case of the Gaussian distribution. The derived bound shows that increasing the distribution parameters, both the mean and variance values, leads to a decrease in the victim model's underlying robustness. While one might intuitively aim to set these parameters as low as possible to achieve optimal robustness, doing so could potentially compromise the model's performance on clean datasets. Therefore, as previously mentioned, striking the right balance between clean accuracy and adversarial robustness is crucial.

**Extending the Results to the GIN.** The same previously applied analysis for the GCN-based models can be extended to take into account GIN-based classifiers. We consider the same set of assumptions and the same problem setup considered during the previously studied GCN case. We additionally assume that the input node feature space to be bounded, i. e., \(\|X\| B\). We note that this boundedness is a realistic assumption and that the value \(B\) can be easily computed for any real-world dataset.

**Theorem 5**.: _Let \(f:(,)\) denote a graph-based function composed of \(T\) GIN layers, where the initial weight matrix of the \(i\)-th layer is denoted by \(W_{0}^{(i)}\). For adversarial attacks only targeting node features of the input graph, with a budget \(\), we have:_

\[=_{l=1}^{T}(2^{t}\|W_{0}^{(i)}\|+2^{t+1}\|W_{ *}^{(i)}\|)[BT_{u}deg(u)+]\]

_with \(t\) being the number of training epochs and \(deg(u)\) is the degree of node \(u\)._

The proof of Theorem 5 is provided in Appendix D. Theorem 5 establishes an upper bound on the robustness of a GIN-based classifier against adversarial attacks targeting node features. We observe analogous insights, to the ones derived for a GCN-based classifier, regarding the influence of the initialization distribution and number of training epoch on the model's underlying robustness.

## 5 Generalization to Other Models

While our primary research focus lies within the domain of graph representation learning, a sub-field of the broader landscape of Deep Learning models, the fundamental principles of our theoretical analysis are applicable across various model architectures. Notably, and to our knowledge, the absence of a comparable study in current adversarial literature motivates our endeavor to bridge this gap. In this section, we aim to fill this gap by presenting a comprehensive analytical framework that provides the connection between weight initialization and the robustness of neural networks.

Let \(x^{n_{0}}\) denote an input vector where \(n_{0}\) is the input dimension. Let \(W^{(l)}^{n_{l-1},n_{l}}\) be the weight matrix and \(b_{l}^{n_{l}}\) the bias of the \(l^{}\) layer with \(n_{l}\) being its dimensionality. We focus on the general family of neural networks for which the computation during layer \(l\), using an activation function \(^{(l)}\), can be written as :

\[h^{(l)}=^{(l)}(W^{(l)}h^{(l-1)}+b^{(l)}).\]

We consider the same set of assumptions (stated in Section 3.1) as the one from previous section. We consider the \(_{2}\) norm as our input and output distances within the metric space \(^{n_{0}}\) and we consider an input attack budget \(\). The introduced adversarial risk in Equation 2 can be easily extended and tailored to the family of considered neural networks discussed in this section. Further clarification on this extension is provided in the Appendix (Section G.1). From this standpoint, by adapting the Definition 1, analogous effects of the weight initialization, provided in Theorem 6, can be observed.

**Theorem 6**.: _Let \(f:^{in} ^{out}\) be a \(T\)-layers neural network with \(W_{0}^{(i)}\) denoting the initial weight matrix of the \(i\)-th layer. When subject to adversarial attacks, \(f\) is \((,)-\) robust with:_

\[=_{i=1}^{T}(2^{t}\|W_{0}^{(i)}\|+2^{t+1} \|W_{*}^{(i)}\|)\]

The proof of Theorem 6 can be found in Section E of the Appendix. Similar to previous findings, the upper bound relies on key elements of the initialization process, specifically the initial weight norm and the number of training epochs. These results validate and extend the established link between initialization and a model's robustness in neural networks, highlighting the importance of selecting appropriate parameters. From the derived upper bound, which is also applicable to GCN and GIN cases, we observe that the number of training epochs exerts an effect on the bound. Specifically, while increasing the number of epochs can improve the model's performance on a clean dataset, it simultaneously leads to a deterioration in the model's adversarial robustness. Ideally, adversarial defense strategies aim to avoid this trade-off between clean and attacked accuracy, striving for robust models that do not compromise the initial performance. In this context, considering the strong-convexity of the loss function \(\), in addition to the previously made assumptions, we observe that the effect of the number of training epochs becomes less pronounced. Lemma 7 specifically provides the computed bound under these assumptions.

**Lemma 7**.: _Let \(f:^{in}^{out}\) be a \(T\)-layers neural network trained with a \(\)-strongly convex and \(L\)-smooth loss function. Let \(W_{0}^{(i)}\) denote the initial weight matrix of the \(i\)-th layer. When subject to adversarial attacks, with a budget \(\), we have that \(f\) is \((,)-\)robust with:_

\[=_{i=1}^{T}((1-/L)^{t}\|W_{0}^{(i)}\|+2 \|W_{*}^{(i)}\|)\]

The proof of Lemma 7 is provided in Section F of the Appendix. Since \( L\), increasing the number of training epochs results in the diminishing influence of the initialization weights. In this scenario, the bound depends solely on the final weights, a phenomenon previously explored in works such as Parseval networks  for neural networks and GCORN  for GNNs. This observation highlights the necessity of convexity in the loss function when training a neural network, as it plays a crucial role in enhancing the model's robustness, beyond the traditional considerations of classical training optimization perspectives.

## 6 Experimental Results

This section aims to empirically validate our theoretical findings using real-world benchmark datasets. We start by laying out our experimental setting, then we study the impact of various initialization strategies on a GCN's robustness. Next, we analyze the influence of training epochs on adversarial robustness. Finally, we extend our experimentation to considered family of DNNs in Section 5.

### Experimental Setting

**Experimental Setup.** Consistent with our theoretical analysis, this section focuses on the node classification task. We leverage the citation networks Cora and CiteSeer , with additional results on other datasets provided in the Appendix G. To mitigate the impact of randomness during training, each experiment was repeated 10 times, using the train/validation/test splits provided with the datasets. A 2-layers GCN classifier with identical hyperparameters and activation functions was employed across all the experiments. The models were trained using the cross-entropy loss function, and consistent values for the number of epochs and learning rate were maintained across all analysis. Further implementation details can be found in Appendix H. The necessary code to reproduce all our experiments is available on github https://github.com/Sennadir/Initialization_effect.

**Adversarial Attacks.** We consider two main gradient-based structural adversarial attacks: **(i)** 'Mettack' (with the 'Meta-Self' training strategy)  that formulates the problem as a bi-level problem solved using meta-gradients **(ii)** and the Proximal Gradient Descent (PGD)  which consists of iteratively adding small crafted perturbations using the gradient of the classifier's loss. We additionally provide results for the 'Dice' attack  in Appendix G. For our experiments, we considered perturbation rates ranging from \(10\%\) (i. e., \(0.1|E|\)) to \(40\%\) (i. e., \(0.4|E|\)).

**Evaluation Metrics.** We report the experimental findings in terms of the 'Attacked Accuracy', which is the model's test accuracy when subject to the attacks. Additionally, given that initialization have an impact on the model's generalization and performance, solely reporting the attacked accuracy fails in some specific cases to provide a comprehensive perspective. Thus, we adopt for some experiments the "Success Rate" metric, also commonly employed in adversarial literature, which encompasses the number of successfully attacked nodes while taking into account the model's initial clean accuracy.

### Effect Of Training Epochs

The theoretical analysis presented in Section 4 established a connection between the number of training epochs and the model's resulting robustness. The derived bound suggests that increasing the number of epochs results in the model becoming more vulnerable to adversarial attacks. The objective of this experimental section is to empirically validate this assertion using real-world datasets. To this end, at each training epoch, we assess the model's performance on the test set, considering both its clean accuracy and its accuracy under adversarial attacks.

Figure 1 illustrates the results of this analysis. The initial two subplots (a,b) display the findings on the Cora dataset, while the subsequent (c,d) subplots present results from the CiteSeer dataset. Foreach dataset, the first plot showcases the clean and attacked accuracy, while the second plot shows the Success Rate (the discrepancy between the clean and attacked accuracy for each budget). The experimental results demonstrate the existence of the previously discussed trade-off between clean and robust accuracies. Specifically, as anticipated, the clean accuracy exhibits a continual increase until reaching a plateau, corresponding to the convergence of the loss function to a minimum. Conversely, the attacked accuracy demonstrates a rising trend until reaching an inflection point, beyond which it begins to decline. These findings confirms the observations from the derived upper-bound, indicating that a higher number of epochs leads to increased vulnerability in the model. Ideally, users would aim to stop training at the inflection point, where the attacked accuracy is maximized while the clean accuracy remains proximal to its convergence point.

### Effect Of Initial Weight Distribution

We aim to validate the impact of the initial weight norms on the model's adversarial robustness. As previously discussed in Section 4, a larger weight norm leads to the relaxation of the upper-bound, potentially resulting in the model being more susceptible to adversarial attacks.

In this perspective, we start by investigating the effect of sampling from a Gaussian distribution, as studied in Lemma 4. We hence consider this latter by setting the mean value \(\) to a constant, and analyzing the impact of the variance parameter \(\). Intuitively, based on the upper-bound analysis, a higher variance value is anticipated to result in reduced model robustness. Figure 2 illustrates the resulting Success Rate across various variance values for both the "PGD" and "Mettack" methods, applied to the Cora and Citeseer datasets. The findings unequivocally validate the theoretical insights, demonstrating a direct correlation between increasing the variance (\(\)) and a higher Success Rates, indicating heightened vulnerability and reduced robustness of the model. Moreover, the impact of initialization becomes more pronounced when considering larger attack budgets, as outlined in the computed upper-bound. Notably, for certain budgets (e.g., \(30\%\) and \(40\%\)), the observed gap ranges between \(5\%\) and \(15\%\), underscoring the initial weights significant implications on the robustness.

Within the same context, we explore alternative initialization strategies, focusing on two primary cases. First, we investigate sampling initial weights from a uniform distribution \((-,)\), where \(\) can be seen as a scaling parameter for weight norms. Second, we consider employing a scaled orthogonal weight initialization strategy. While this our aim can be approached by sampling weights

Figure 1: Effect of training epochs on the model’s robustness on Cora (a,b) and CiteSeer (c,d).

Figure 2: Effect of the variance parameter on the model’s robustness in the case of Gaussian Initialization on PGD [on Cora (a) and Citeseer (b)] and Mettack [on Cora (a) and Citeseer (b)].

from a scaled random Gaussian distribution, we adopt the orthogonal initialization strategy proposed in prior work , which we further rescale by a factor \(\) to examine the impact on weight norms. In both cases, higher scaling parameter values of \(\) are anticipated to theoretically yield higher upper-bounds and consequently render the model more vulnerable, as indicated by our computed bounds. We conduct numerical computations on both the Cora and Citeseer datasets to assess the resulting adversarial robustness of a GCN across various \(\) values, as provided in Figure 3. The experimental results are exactly aligned with our theoretical findings showcasing the effect of the weight norm in the adversarial robustness. To summarize, while traditionally overlooked in prior studies on adversarial robustness, our experimentation underscores the critical importance of selecting appropriate initialization distributions and strategies for enhancing model robustness.

### Experimental Generalization

We extend our experimentation to empirically validate the theoretical generalizations provided in both Section 4 for the GINs and Section 5 for a DNNs. To this end, we consider these two models with various initialization schemes, including the previously used Orthogonal  and Uniform initialization in addition to the Kaiming  and Xavier Initialization . Our analysis primarily focuses on the PGD adversarial attack, using identical attack budgets as in the previous sections. Figure 4 presents the results on the GIN (a) using the Cora dataset and (b) on the DNN using the MNIST dataset. Notably, we observe that the different initialization methods yield similar clean accuracy (\(=0\)), yet as the attack budget increases, the discrepancy in attacked accuracy between them also grows. For instance, in the case of DNNs, the accuracy gap between the best and worst initialization methods for \(=0.1\) ranges around \(60\%\), proving our main assumption related to the impact of initialization on the model's robustness.

Figure 4: Effect of initialization on the GIN (a) and DNN (b) for different attack budgets.

Figure 3: Effect of the scaling parameter \(\) on the model’s robustness in the case of Uniform (a-d) and Orthogonal (e-h) Initialization when subject to PGD and Mettack using Cora and CiteSeer.

Conclusion & Limitations

The current study shows that the dynamics of learning in GNNs and DNNs have an important effect on the model's final robustness. Specifically, we theoretically showed that the model's robustness is connected to the weight initialization and the number of training epochs. We empirically validate our findings, where we can see that choosing the right initialization can yield huge "almost-free" robustness improvement. We additionally showed the existence of a trade-off between choosing the right number of epochs to have the best clean accuracy and the most robust model. While the current work did not propose an alternative or a solution, it has introduced a new perspective, which to our knowledge, was absent from the adversarial literature, opening the door to new research direction either by proposing new initialization schemes to improve robustness while guaranteeing good generalization or new gradient-based weight updates to enforce the robustness of the model or yet again by tracking robustness metrics alongside the loss function throughout training.