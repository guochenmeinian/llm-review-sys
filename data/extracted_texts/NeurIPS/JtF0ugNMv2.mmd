# T2T: From Distribution Learning in Training

to Gradient Search in Testing for

Combinatorial Optimization

 Yang Li, Jinpei Guo, Runzhong Wang, Junchi Yan

Dept. of Computer Science and Engineering & MoE Key Lab of AI

Shanghai Jiao Tong University

{yanglily, jinpei, runzhong.wang, yanjunchi}@sjtu.edu.cn

Correspondence author. This work was partly supported by National Key Research and Development Program of China (2020AAA0107600), NSFC (62222607) and STCSM (22511105100).

###### Abstract

Extensive experiments have gradually revealed the potential performance bottleneck of modeling Combinatorial Optimization (CO) solving as neural solution prediction tasks. The neural networks, in their pursuit of minimizing the average objective score across the distribution of historical problem instances, diverge from the core target of CO of seeking optimal solutions for every test instance. This calls for an effective search on each problem instance, while the model should serve to provide supporting knowledge that benefits the search. To this end, we propose _T2T (Training to Testing)_ framework that first leverages the generative modeling to estimate the high-quality solution distribution for each instance during training, and then conducts a gradient-based search within the solution space during testing. The proposed neural search paradigm consistently leverages generative modeling, specifically diffusion, for graduated solution improvement. It disrupts the local structure of the given solution by introducing noise and reconstructs a lower-cost solution guided by the optimization objective. Experimental results on Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS) show the significant superiority of T2T, demonstrating an average performance gain of 49.15% for TSP solving and 17.27% for MIS solving compared to the previous state-of-the-art.

## 1 Introduction

Machine Learning (ML) for Combinatorial Optimization (CO)2, abbreviated as ML4CO [1; 2; 3; 4], is a rapidly growing area, lying at the intersection between the two well-established communities. CO problems are essential in operational research as well as computer science, in both theory and practice. However, due to the inherent computational difficulty, e.g. NP-hardness, (approximately) solving these problems with effectiveness and efficiency poses significant challenges. Compared with traditional heuristic-based solvers, ML has recently shown its promising potential to automatically learn heuristics in a data-driven scheme [1; 2; 3; 4], or meanwhile with the aid of human knowledge which would normally improve the sample-efficiency [5; 6; 7; 8; 9]. Moreover, the introduction of neural networks also allows for the speedup by parallel matrix computation in forward inference.

A popular framework in ML4CO is developing neural networks to directly predict a solution or generate a solution with the aid of neural predictions, such that the corresponding objective score is minimized [10; 11; 12; 13]. However, the neural network can only fit limited mappings from problemparameters to (near) optimal solutions in the training set, resulting in subpar performance on unseen test instances [14; 15]. Therefore, it calls for neural solvers to efficiently explore solutions tailored to each specific problem instance. In the traditional framework, the network prediction typically converges to a local optimum with little diversity, struggling to further explore superior solutions. In this paper, we resort to a more effective framework that first learns to model the distribution of high-quality solutions conditioned on problem instances, then conducts a gradient-based search to obtain a superior solution specific to the given instance, as shown in Fig. 2. Existing alternative methods that conduct the tailored search in the testing phase, e.g. active search [16; 17] and meta-learning , require updating the model weights during testing, hindering their applications in computational intensive scenarios. In contrast, we develop a more efficient gradient search framework to obtain a superior solution specific to the given instance, without the need to update model weights.

We exploit the powerful generative neural CO solver  that learns to model the distribution of high-quality solutions conditioned on specific problem instances. The generative models are welcomed in their ability to output high-quality images , texts  and graphs  with great diversity, whereby these advantages are also welcomed in the ML4CO community. Some recent successes in generative learning of CO are demonstrated in classic CO problems e.g. Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS) (via Diffusion  and Variational Auto-Encoder ), Satisfiability Problem (SAT) and Mixed-Integer Linear Programming (MILP) (via graph generators [24; 25; 26]), also in more realistic applications e.g. routing for chip design (via Diffusion  and Generative Adversarial Network ). Notably, the very recent adaptation of the flourishing diffusion models in probabilistic generation  has resulted in state-of-the-art performance for solving CO problems e.g. TSP. The diffusion modeling for CO solving is illustrated in Fig. 2, where the model is designed to estimate the solution distribution given a certain instance \(G\), i.e. \(p_{}(_{0},G)\). Despite its high-quality solutions, a major drawback is the unawareness of optimizing the CO objective as the generative model simply learns to fit a distribution of solutions. To further improve the efficiency of the current costly from-scratch sampling, it stands crucial to introduce a more efficient search-based paradigm to further explore high-quality solutions in terms of the final objective of the problem.

To this end, we propose a gradient-based search paradigm to further leverage the learned solution distribution through training, bridging the solution distribution learning during generative modeling training **to** gradient search during testing for **CO**, dubbed T2T. The proposed search paradigm well aligns with the behavior of a progressive generative model especially for the SOTA embodiment i.e. diffusion. The diffusion model provides a probabilistic framework that enables a smooth and flexible transition between random noises and feasible solutions, allowing for varying degrees of structure disruption, with a reference denoising process to restore a solution. Leveraging this framework, it is natural to model the local search heuristic as two stages, i.e., disrupting the local structure and reconstructing it with a better objective score. Specifically, T2T first adds certain noise to the given solution and then denoises it to reconstruct a lower-cost solution in iterations, which is achieved by incorporating the objective optimization guidance into the denoising steps through gradient feedback in the loop, steering the recovered sampling towards the optimization direction, as illustrated in Fig. 3. T2T enhances the solving paradigm by incorporating a search procedure with explicit objective guidance, which enables more efficient sampling and allows for deeper exploitation of the estimated solution distribution.

For the discrete nature of CO, T2T builds itself upon discrete diffusion models [29; 30; 31], which in training learns the transition between categorical noises and feasible discrete solutions that are represented as probability maps of nodes/edges on graphs indicating whether each node/edge is included in the solution. The transition is tailored for each instance as it is conditioned on the corresponding instance's statistics. The training of T2T follows the standard training paradigm of diffusion models, which primarily equips the model with feasibility awareness and estimates the distribution of high-quality solutions. While during testing, we introduce the objective optimization guidance which serves as the complementary optimization and gradient search utility.

We showcase the efficacy of T2T on two typical NP-hard combinatorial optimization problems for edge-selecting and node-selecting types respectively, i.e., TSP and MIS, which have been attracting most attention in the community of ML4CO. Empirically, T2T exhibits strong performance superiority over existing state-of-the-art neural solvers on benchmark datasets across various scales. Notably, a significant performance gain is observed when compared to the purely constructive diffusion solver , thereby highlighting the effectiveness of the designed search-based paradigm.

The highlights of this paper include: 1) We introduce the framework of T2T which first estimates the high-quality solution distribution for each instance during training, and then leverages the diffusion process to perform an effective local search in the solution space during testing; 2) We incorporate the optimization objective guidance into the denoising model with theoretical analysis, which equips the model with gradient feedback directly from the instance-wise objective score, enabling it to perform optimization while ensuring general feasibility; 3) Extensive experiments show the promising performance of the proposed T2T, even when trained without high-quality solution labels.

## 2 Related Work

**Machine Learning for CO Solving.** ML solvers can be categorized into construction solvers and improvement solvers. Constructive solvers include autoregressive solvers [32; 12; 6; 23; 8] which step-by-step extend a partial solution to produce a complete feasible solution, and non-autoregressive solvers [33; 34; 14; 35; 19] which predict a solution (typically soft constrained) in one shot. Improvement solvers [36; 37; 5] often define local operators such as 2OPT [36; 37], node swapping [5; 37], sub-problem resolving [7; 38] to perform local search towards minimizing the optimization objective. The algorithm pipelines generally involve selecting specific regions and locally optimizing sub-solutions. While T2T can be a rather different improvement solver, which allows for parallel optimization across multiple sub-regions and equips the generative modeling with the search procedure.

Generative modeling for CO leverages the powerful expressive and modeling capabilities of generative models for direct CO solving (e.g. VAE [23; 39], GAN [28; 40; 41], Diffusion [19; 27; 42], as well as GFlowNets [43; 44], which, while not a typical generative model itself, is closely related to generative modeling), and for supporting CO solving [24; 25]. DIFUSCO  has become state-of-the-art for the TSP problem by leveraging powerful diffusion models. However, no instance-specific search paradigms are introduced in existing methods to fully utilize the estimated solution distribution. Instead, they rely on multiple from-scratch samplings to leverage the distribution, which is computationally expensive. Though  attempts to utilize the evolution algorithm to conduct multiple iterations of sampling, it yet incurs even more significant costs without explicit objective guidance in the loop. In contrast, T2T serves as an improvement solver that locally modifies the given solution with objective guidance in the loop. It offers the flexibility to introduce any degree of disruption and rewrite without the necessity to construct the solution from scratch.

**Diffusion Probabilistic Models.** Diffusion models (score-based models) typically comprise a noising process and a learnable denoising process, where neural networks predict the noise introduced in each step. Diffusion's inference can be seen as a score (gradient) guided search, which is a general principle across different settings [45; 46; 47]. Diffusion in continuous space (e.g., for image generation) [29; 48; 42; 49; 50; 51; 52] have established and consolidated the theoretical foundations of the general diffusion framework. Several landmark works have advanced the diffusion model through various aspects. DDIM  generalizes DDPM via a class of non-Markovian processes which gives rise to implicit models that produce high-quality samples much faster. For conditional generation, classifier guidance [29; 52] and classifier-free guidance  are introduced with the score estimate of the diffusion models to produce conditional results. Similar paradigms are adopted in discrete diffusion models for generating discrete data using binomial noises  or multinomial/categorical noises [30; 31]. Diffusion also demonstrates its capability to learn offline policies in reinforcement learning tasks [54; 55].

Approach

This section presents our proposed T2T framework, including the preliminaries, offline training pipeline based on generative modeling, and online testing pipeline with objective guided search.

### Preliminaries: Combinatorial Optimization on Graphs

Following the notations adopted in [13; 56; 57] we define \(\) as the universe of CO problem instances represented by graphs \(G(V,E)\), where \(V\) and \(E\) denote the node set and edge set respectively. CO problems can be broadly classified into two types based on the solution composition: edge-selecting problems which involve selecting a subset of edges, and node-selecting problems which select a subset of nodes. Let \(\{0,1\}^{N}\) denote the optimization variable. For edge-selecting problems, \(N=n^{2}\) and \(_{i:n+j}\) indicates whether \(E_{ij}\) is included in \(\). For node-selecting problems, \(N=n\) and \(_{i}\) indicates whether \(V_{i}\) is included in \(\). The feasible set \(\) consists of \(\) satisfying specific constraints as feasible solutions. A CO problem on \(G\) aims to find a feasible \(\) that minimize the given objective function \(f(;G):\{0,1\}^{N}_{ 0}\):

\[_{\{0,1\}^{N}}f(;G)\] (1)

In this paper, we study two primary and representative CO problems: TSP and MIS. TSP defines on an undirected complete graph \(G=(V,E)\), where \(V\) represents \(n\) cities and each edge \(E_{ij}\) has a non-negative weight \(w_{ij}\) representing the distance between cities \(i\) and \(j\). The problem can then be formulated as finding a Hamiltonian cycle of minimum weight in \(G\). For MIS, given an undirected graph \(G=(V,E)\), an independent set is a subset of vertices \(S V\) such that no two vertices in \(S\) are adjacent in \(G\). MIS is the problem of finding an independent set of maximum cardinality in \(G\).

### Offline Solution Distribution Learning on Training Set based on Diffusion

The basic diffusion modeling for solution generation in training is primarily based on 's design while this paper explicitly formulate the solving task as a conditional generation task. As mentioned in Sec. 3.1, the solutions of CO problems can be represented as \(\{0,1\}^{N}\) with \(\). For each entry, the model estimates a Bernoulli distribution indicating whether this entry should be selected. In implementation, each entry of solution \(\) is represented by a one-hot vector3 such that \(\{0,1\}^{N 2}\). Generative modeling aims to model the distribution of high-quality solutions conditioned on the given instance, while feasibility constraints can be broadly captured through learning and eventually hard-guaranteed by post-processing. For the purpose of problem-solving, the generative model is desired to estimate the solution distribution given a certain instance \(G\), i.e., \(p_{}(|G)\). In this section, we model this distribution through the discrete diffusion models [42; 30].

Following the notations of [42; 30], the general framework of diffusion includes a forward noising and a reverse denoising Markov process. The noising process, also denoted as the diffusion process, takes the initial solution \(_{0}\) sampled from the distribution \(q(_{0}|G)\) and progressively introduces noise to generate a sequence of latent variables \(_{1:T}=_{1},_{2},,_{T}\). Specifically, the noising process is formulated as \(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}| _{t-1})\). The denoising process is learned by the model, which starts from the final latent variable \(_{T}\) and denoises \(_{t}\) at each time step to generate the preceding variables \(_{t-1}\) based on the instance \(G\), eventually recovering the target data distribution. The formulation of the denoising process is expressed as \(p_{}(_{0:T}|G)=p(_{T})_{t=1}^{T}p_{}( _{t-1}|_{t},G)\), where \(\) denotes the parameters of the model. The training optimization aims to align \(p_{}(_{0}|G)\) with the data distribution \(q(_{0}|G)\) and the objective adopts the variational upper bound of the negative log-likelihood:

\[=_{q}[- p_{}(_{0}|_{1 },G)+_{t>1}D_{KL}[q(_{t-1}|_{t},_{0})  p_{}(_{t-1}|_{t},G)]]+C\] (2)

Specifically, the forward noising process is achieved by multiplying \(_{t}^{N 2}\) at step \(t\) with a forward transition probability matrix \(_{t}^{2 2}\) where \([_{t}|]_{ij}\) indicates the probability of transforming \(_{i}\) in each entry of \(_{t}\) to \(_{j}\). We set \(_{t}=[_{t}&1-_{t}\\ 1-_{t}&_{t}]\), where \(_{t}\)such that the transition matrix is doubly stochastic with strictly positive entries, ensuring that the stationary distribution is uniform which is an unbiased prior for sampling. The noising process for each step and the \(t\)-step marginal are formulated as:

\[q(_{t}|_{t-1})=(_{t};=_{t-1}_{t}) q(_{t}|_{0})= (_{t};=_{0}}_{t})\] (3)

where \((;)\) is a categorical distribution over \(N\) one-hot variables with probabilities given by vector \(\) and \(}_{t}=_{1}_{2}_{t}\). Through Bayes' theorem, the posterior can be achieved as:

\[q(_{t-1}|_{t},_{0})=_{t}| _{t-1},_{0})q(_{t-1}|_{0})}{q( _{t}|_{0})}=(_{t-1};= _{t}_{t}^{}_{0}}_{t-1}}{_{0}}_{t}_{t}^{ }})\] (4)

The neural network is trained to predict the logits of the distribution \(_{}(}_{0}|_{t},G)\), such that the denoising process can be parameterized through \(q(_{t-1}|_{t},}_{0})\):

\[p_{}(_{t-1}|_{t})_{}_{0 }}q(_{t-1}|_{t},}_{0})_{} (}_{0}|_{t},G)\] (5)

Specifically for implementation, the condition \(G\) is enforced as an input to the network \(\), which is embodied as an anisotropic graph neural network with edge gating mechanisms  following . For TSP, the instance condition consists of 2D coordinates of the vertices. The network input includes node embeddings from the 2D coordinates, along with edge embeddings from \(_{t}\). For MIS, the edges \(E\) serve as the instance condition, complemented by the inclusion of node embeddings from \(_{t}\) to collectively form the input. The output of the network is \(_{}(}_{0}|_{t},G)^{N 2}\) parameterizing \(N\) Bernoulli distributions for \(N\) entries in \(}_{0}\).

### Test-stage Gradient-based Search with Instance-wise Objective Feedback

In this section, we first present the technical solution for the objective gradient incorporation in denoising steps, and then illustrate the neural search paradigm that leverages the guided denoising steps.

#### 3.3.1 Objective-aware Denoising Process with Gradient Feedback

In the context of combinatorial optimization, the incorporation of objective optimization is necessary and important, which enables the direct involvement of objective and effective search over the solution space towards minimizing the score. As discussed in Sec. 3.2, at step \(t\) the model denoise \(_{t}\) by \(p_{}(_{t-1}|_{t},G)\). For the purpose of objective optimization, we aim to estimate \(p_{}(_{t-1}|_{t},G,y^{*})\) where \(y^{*}\) is the optimal objective score given the instance \(G\):

\[y^{*}=_{}l(;G)\] (6)

to guide the denoising process to \(^{*}=*{arg\,min}_{}l(;G)\). We will show later that this ability does not necessarily require the access of \(y^{*}\) or \(^{*}\), and can readily be achieved by the model learned in Sec. 3.2 without relying on any additionally trained networks. The adopted objective is defined below.

**Optimization Objective.** Since the generated samples are not guaranteed to satisfy the hard constraints, we incorporate constraint satisfaction into the objective and utilize a relaxed objective with constraint penalty, as adopted in [13; 56; 57]. Consider a relaxed cost function \(f_{r}(;G):\{0,1\}^{N}_{ 0}\) which defined on the constraint relaxed space \(\{0,1\}^{N}\) satisfying \(f_{r}(;G)=f(;G),\ \ \), and a constraint penalty \(g_{r}(;G):\{0,1\}^{N}_{ 0}\) where \(g_{r}(;G)=0\) if \(\) and \(g_{r}(;G) 1\) if \(\), the objective can be formulated as:

\[l(;G) f_{r}(;G)+ g_{r}(;G)\] (7)

where \(>_{}f(;G)\). For MIS, the constraint can be straightforwardly modeled as \(_{i}_{j}=0\) if \((i,j) E\), thus the objective can be formulated as \(l_{}(;G)-_{1 i N}_{i}+_{( i,j) E}_{i}_{j}\). While for TSP, since the constraints are difficult to explicitly express, we directly select the objective as \(l_{}= D\) where \(D_{+}^{n n}\) denotes the distance matrix.

**Objective Guidance in Denoising Steps.** The estimation of \(p_{}(_{t-1}|_{t},G,y^{*})\) can be modeled through the following proposition, adapted from the classifier guidance technique :

**Proposition 1**.: _The optimization-enforced denoising probability estimation \(p_{}(_{t-1}|_{t},G,y^{*})\) equals to \(Zp_{}(_{t-1}|_{t},G)p(y^{*}|_{t-1},G)\), where \(Z\) is a normalizing constant._The proof is given in the Appendix. As \(p_{}(_{t-1}|_{t},G)\) can be readily obtained from the trained network, the main challenge of estimating \(p_{}(_{t-1}|_{t},G,y^{*})\) lies in estimating \(p(y^{*}|_{t-1},G)\). Since \(_{t-1}\) is not accessible at step \(t\), we utilize Taylor expansion to approximate \( p(y^{*}|_{t-1},G)\) around \(_{t-1}=_{t}\), given that \(_{t-1}_{t}\):

\[ p(y^{*}|_{t-1},G)  p(y^{*}|_{t},G)+[_{_{t }} p(y^{*}|_{t},G)]^{}(_{t-1}-_{t})\] \[=[_{_{t}} p(y^{*}|_{t},G) ]^{}\!_{t-1}+|_{t},G)- [_{_{t}} p(y^{*}|_{t},G)]^{}\! _{t}}_{C(_{t})}\] (8)

where \(C(_{t})\) is irrelevant to \(_{t-1}\). Applying exponentiation, we obtain:

\[p(y^{*}|_{t-1},G)([_{_{t}} p (y^{*}|_{t},G)]^{}\!_{t-1})\] (9)

To determine \(p(y^{*}|_{t},G)\), we utilize energy-based modeling  to model the distribution \(p(y|_{t},G)\). In this context, we define the energy function as:

\[E(y,_{t},G)=|y-l(_{0}(_{t});G)|\] (10)

where \(_{0}(_{t})\) denotes the clean target sample \(_{0}\) corresponding to \(_{t}\). The energy function quantifies the compatibility between \(y\) and \((_{t},G)\), and it reaches zero when \(y\) is exactly the final objective score originating from \(_{t}\) with respect to \(G\). Such a design enables the best \(y\) matching the inputs to maintain the highest probability density, and the probability density is positively correlated with the matching degree. Then we employ the _Gibbs distribution_ to characterize the probability distribution over a collection of arbitrary energies:

\[p(y|_{t},G)=_{t},G))}{_{y^{}} (-E(y^{},_{t},G))}=_{0}( _{t});G)|)}{_{y^{}}(-|y^{}-l( _{0}(_{t});G)|)}\] (11)

Let \(Z=_{y^{}}(-|y^{}-l(_{0}(_{t});G) |)\) and substitute in \(y^{*}\), we have:

\[ p(y^{*}|_{t},G)=y^{*}-l(_{0}(_{t});G)- Z _{_{t}} p(y^{*}|_{t},G)=- _{_{t}}l(_{0}(_{t});G)\] (12)

Note the gradient operator has removed to effect of \(y^{*}\) as a constant, thereby eliminating the need to access \(y^{*}\) for achieving the target. And \(Z\) as the integral over the entire distribution is not affected by \(_{t}\) and thus can also be removed by the gradient operator. Recall that the network \(\) is trained to predict the logits of \(_{}(}_{0}|_{t})^{N 2}\) which includes the logits of \(N\) Bernoulli distributions, satisfying \([_{}(}_{0}|_{t})]_{i}= (1-[}_{0}]_{i},[}_{0} ]_{i})\). Thus \(l(_{0}(_{t});G)\) can be estimated using \(}_{0}\) as:

\[l(_{0}(_{t});G) l(_{}_{0 }_{}(}_{0}|_{t})}}_{0};G)\] (13)

By Eq. 9, 12 and 13, we obtain \(p(y^{*}|_{t-1},G)-_{_{t}}l( _{}_{0}_{}(}_{0} |_{t})}}_{0};G)^{}_{t-1}\), and the guided denoising probability estimation can now be achieved by \(p_{}(_{t-1}|_{t},G,y^{*})=Zp_{}(_{t- 1}|_{t},G)p(y^{*}|_{t-1},G)\) where \(Z\) is the normalizing constant, to realize the guided denoising.

#### 3.3.2 The T2T Framework for Efficient Differentiable Neural Search

With the objective gradient incorporated denoising process in Sec. 3.3.1, we now delve into T2T.

**Overview.** As shown in Fig. 3, the algorithm starts with an initial solution \(_{0}\) and conducts several iterations to enhance the given solution. Each iteration involves adding a certain degree of noise to disrupt the structure, denoising with objective gradient guidance to obtain a lower-cost soft-constrained solution, and subsequently post-processing to decode a feasible solution \(\). The algorithm eventually reports the solution with the lowest objective score ever achieved.

**Structure Disruption.** Recall that \(_{1},_{2},,_{T}\) are situated in spaces characterized by progressively escalating levels of disruption. To introduce a controlled degree of disruption to the given solution, we employ \(_{0}}_{ T}\) to derive the distribution of the disrupted solution \(q(_{ T}|_{0})\) as \(N\) Bernoulli distributions, where \(\) serves as a hyperparameter to control the degree of noise. Subsequently, the disrupted solution \(_{ T}\) can be sampled from \(q(_{ T}|_{0})\).

**Objective Guided Solution Reconstruction.** From \(_{ T}\), we employ \(p_{}(_{t}|_{t},G,y^{*})\) proposed in Sec. 3.3.1 to perform denoising, which aimed at recovering a potentially lower-cost \(_{0}^{}\). Additionally,we utilize the fast sampling algorithm introduced in DDIM  to accelerate sampling. The algorithm determines a \(M\)-element subset from latent variables \(_{1: T}\) to obtain \(\{_{_{1}},_{_{2}},,_{_{M}}\}\) (\(_{1}=1\) and \(_{M}= T\)), and directly models \(p_{}(_{_{i-1}}|_{_{i}},G,y^{*})\), which effectively reduces the number of denoising steps. In our implementation, we uniformly perform a 10-step reconstruction, i.e., \(M=10\).

**Post-inference Decoding.** Note the mechanism guarantees that the recovered solution \(\{0,1\}^{N 2}\) (consisting of \(N\) one-hot vectors), but it does not necessarily satisfy \(\). Thus, in the last step's inference, we directly output the logits of its distribution \(p_{}(_{0}^{})\) to produce \(_{0}^{}\) as the heatmap \(H^{N}\) where each element denotes each edge/node's confidence to be selected, and then we adopt greedy decoding to obtain a feasible solution. We follow previous works [59; 35; 19] to perform greedy decoding by sequentially inserting edges (for TSP) or nodes (for MIS) with the highest confidence if there are no conflicts. For TSP, the 2OPT heuristic  is optionally applied.

**Tradeoff Between Guidance Accuracy and Local Rewriting Degree.** Note that the objective-guided denoising process relies heavily on the accuracy of the estimation \(_{}(}_{0}|_{t})\). Intuitively, it is easier to restore the original solution from the data that has lower noise content. And with excessive noise interference, the estimation will deviate significantly from the ground truth, resulting in faulty guidance. This is also one of the reasons why we use a local rewriting mechanism. However, while the accuracy of the estimation increases as it moves closer to the original solution space, it also limits the degree of reconstruction in terms of the solution structure. Therefore, it is crucial to strike a balance between the estimation accuracy and the extent of local rewriting. To accomplish this, we employ the aforementioned hyperparameter \(\) to control an appropriate range of perturbation.

## 4 Experiments

All the experiments are performed on GPUs of NVIDIA Telsla A100. All the test evaluations are performed on a single GPU, while the trainings are conducted in parallel with four A100 GPUs. Source code is publicly available at https://github.com/Thinklab-SJTU/T2TCO.

### Experimental Setup

We test on two CO problems including TSP and MIS. The comparison includes SOTA learning-based solvers, heuristics, and exact solvers for each problem. For comparison fairness of learning solvers, the initial solutions of our approach are obtained by denoising random noises by Eq. 5 (same solution as the constructive diffusion solver DIFUSCO ). To balance the inference time, for experiments with runtime measurement, T2T utilizes a reduced number of sampling steps for constructing the initial solutions, specifically 20 steps. While DIFUSCO, as an important baseline, adopts 120 inference steps for TSP and 50 steps for MIS. The models for both TSP and MIS are trained with 1000 denoising steps, i.e., \(T=1000\). For gradient search, T2T generally involves 3 iterations with 10 guided denoising steps for each. See more details in Appendix.

Figure 3: T2T iteratively adds noise to the solution and performs denoising guided by the problem objective (e.g. TSP), to improve the solution as shown in the orange part. The bottom blue part shows the capability of diffusion to establish a smooth transition from random noises to feasible solutions. \(^{*}\): optimal solution; \(y^{*}\): optimal objective score; \(\): model parameters; \(G\): given instance.

### Experiments for TSP

**Datasets.** A TSP instance includes \(N\)\(2\)-D coordinates and a reference solution obtained by heuristics. Training and testing instances are generated via uniformly sampling \(N\) nodes from the unit square \(^{2}\), which is a standard procedure as adopted in [12; 23; 33; 61; 35; 19]. We experiment on various problem scales including TSP-50, TSP-100, TSP-500, and TSP-1000. The reference solutions for TSP-50/100 are labeled by the Concorde exact solver  and the solutions for TSP-500/1000 are labeled by the LKH-3 heuristic solver . The test set for TSP-50/100 is taken from [12; 33] with 1280 instances and the test set for TSP-500/1000 is from  with 128 instances for the fair comparison. This section also incorporates results on real-world TSPLIB4 dataset.

**Metrics.** Following [12; 33; 35; 19], we adopt three evaluation metrics: 1) Length: the average total distance or cost of the solved tours w.r.t. the corresponding instances, as directly corresponds to the objective. 2) Drop: the relative performance drop w.r.t. length compared to the global optimality or the reference solution; 3) Time: the average computational time to solve the problems.

**Results for TSP-50/100.** Table 1 presents the results. Since DIFUSCO  has been shown able to attain global optimality for instances with these scales, at the cost of a sampling decoding scheme, we degrade to the greedy decoding setting for a more meaningful evaluation. The baselines include state-of-the-art learning-based methods with greedy decoding, as well as traditional exact and heuristic solvers. Hyperparameter \(\) is set as \(0.25\). Both DIFUSCO and T2T adopt 50 inference steps. As evident from the results, in the setting of pure greedy decoding without 2OPT refinement, the proposed search-based paradigm achieves a substantial reduction in performance drops from 0.48% to 0.04% for TSP-50 and from 1.01% to 0.18% for TSP-100.

**Results for TSP-500/1000.** Table 2 presents the results. The baselines include recent learning methods with greedy decoding and sampling decoding (\( 4\)), i.e. producing multiple solutions in parallel and selecting the best one, as well as traditional exact and heuristic solvers. DIFUSCO and T2T are compared in the same conditions. DIFUSCO and T2T adopt 120 and 20 inference steps, respectively. The results of other baselines are quoted from  and , with the runtimes provided for reference. In the sampling decoding setting, compared to previous neural solvers, T2T improves the performance drop from 0.83% to 0.37% for TSP-500 and 1.30% to 0.78% for TSP-1000.

**Effect of Disruption Ratio \(\).** As discussed in Sec. 3.3.2, the hyperparameter \(\) is utilized to regulate the extent of perturbation, striking a balance between guidance accuracy and local rewriting degree. Fig. 4 (a) illustrates the impact of \(\) on the solving performance. The experiments are performed on TSP-500 with both greedy and sampling decoding settings. The results demonstrate the tradeoff existence and indicate that \(0.3 0.4\) can attain relatively better performance.

    &  &  &  \\   & & Length\(\) & Drop\(\) & Length\(\) & Drop\(\) \\  Concorde  & Exact & 5.69\({}^{*}\) & 0.00\% & 7.76\({}^{*}\) & 0.00\% \\
2OPT  & Heuristics & 5.86 & 2.95\% & 8.03 & 3.54\% \\ Farthest Insertion & Heuristics & 6.12 & 7.50\% & 8.72 & 12.36\% \\  AM  & RL+Grdy & 5.80 & 1.76\% & 8.12 & 4.53\% \\ GCN  & SL+Grdy & 5.87 & 3.10\% & 8.41 & 8.38\% \\ Transformer  & RL+Grdy & 5.71 & 0.31\% & 7.88 & 1.42\% \\ POMO  & RL+Grdy & 5.73 & 0.64\% & 7.84 & 1.07\% \\ Sym-NCO  & RL+Grdy & – & – & 7.84 & 0.94\% \\ Image Diffusion  & SL+Grdy & 5.76 & 1.23\% & 7.92 & 2.11\% \\ DIFUSCO  & SL+Grdy & 5.72 & 0.48\% & 7.84 & 1.01\% \\ T2T **(Ours)** & SL+Grdy & **5.69** & **0.04\%** & **7.77** & **0.18\%** \\  AM  & RL+Grdy+2OPT & 5.77 & 1.41\% & 8.02 & 3.32\% \\ GCN  & SL+Grdy+2OPT & 5.70 & 0.12\% & 7.81 & 0.62\% \\ Transformer  & RL+Grdy+2OPT & 5.70 & 0.16\% & 7.85 & 1.19\% \\ POMO  & RL+Grdy+2OPT & 5.73 & 0.63\% & 7.82 & 0.82\% \\ Sym-NCO  & RL+Grdy+2OPT & – & – & 7.82 & 0.76\% \\ DIFUSCO  & SL+Grdy+2OPT & 5.69 & 0.09\% & 7.78 & 0.22\% \\ 
**T2T (Ours)** & SL+Grdy+2OPT & **5.69** & **0.02\%** & **7.76** & **0.06\%** \\   

Table 1: _Greedy Decoding_ on TSP-50 and TSP-100. RL: Reinforcement Learning, SL: Supervised Learning, Gridy: Greedy Decoding. \({}^{*}\) denotes the baseline for the performance drop.

**Effect of Initial Solution Quality.** Fig. 4 (b) illustrates the performance variation when altering the initial solution quality. The quality is controlled by the number of denoising steps for producing the initial solution. The experiments are performed on TSP-500 with both greedy and sampling decoding settings. As shown, T2T exhibits a relatively stable performance across varying initial solution qualities, based on which we generally utilize 20 steps for constructing the initial solutions.

**Generalization Results on Synthetic Data and TSPLIB.** Based on the problem set {TSP-50, TSP-100, TSP-500, TSP-1000}, we train the model on a specific problem scale and then evaluate it on all problem scales. Table 4 presents the generalization results of T2T and DIFUSCO with greedy decoding. The results show the satisfying cross-domain generalization ability of T2T, e.g., the model trained on TSP-1000 achieves less than a 2% optimality gap on all other problem scales. Moreover, we evaluate our model trained with random 100-node problems on real-world TSPLIB instances with 50-200 nodes. The compared baselines include DIFUSCO and baselines listed in 's Table 3. Table 3 shows T2T's superiority achieving merely 0.133% optimality gap, 58.3% better than the best previous method. Detailed results of Table 3 and the results on large-scale TSPLIB instances with 200-1000 nodes in Appendix A. For each instance, we normalize the coordinates to  and solve with the solving algorithms.

**Validation of Gradient Search with Worse Distribution Learning in Training.** We investigate the performance over \(\) amount of training data and the model trained with lower-quality feasible solutions produced by _farthest insertion_ heuristic (with 7.50% gap to optimality). Results in Fig. 5 validate the effectiveness of gradient search even if the distribution is poorly learned, suggesting that T2T

   } &  &  &  \\   & & Length & Drop & Time & Length & Drop & Time \\  Concorde  & Exact & 16.55\({}^{*}\) & – & 37.66n & 23.12\({}^{*}\) & – & 6.65n \\ Gurobi  & Exact & 16.55 & 0.00\% & 45.63n & – & – & – \\ LKH-3 (default) & Heuristics & 16.55 & 0.00\% & 46.28n & 23.12 & 0.00\% & 2.57n \\ Farthest Insertion & Heuristics & 18.30 & 10.57\% & 0s & 25.72 & 11.25\% & 0s \\  AM  & RL+Gryk & 20.02 & 20.99\% & 1.51m & 31.15 & 34.75\% & 3.18m \\ GCN  & SL+Gryk & 29.72 & 79.61\% & 6.67n & 48.62 & 10.120\% & 28.52m \\ POMO+EA+ES-LB  & RL+AS+Gryk & 19.24 & 16.25\% & 12.80m & – & – & – \\ POMO+EA+S-TD  & RL+AS+Gryk & 24.54 & 48.22\% & 11.61n & 49.56 & 114.36\% & 63.45n \\ DIMES  & RL+Gryk & 18.93 & 14.38\% & 0.97n & 26.58 & 14.97\% & 2.08m \\ DIMES  & RL+AS+Gryk & 17.81 & 7.61\% & 2.10m & **24.91** & **7.745** & 4.49n \\ DIFUSCO  & SL+Gryk & 18.11 & 9.41\% & 5.70n & 25.72 & 11.24\% & 17.33m \\  T2T (O) & SL+Gryk & **17.39** & **5.09\%** & **5.09\%** & **5.01\%** & **8.07\%** & **11.866nm** \\  DIMES  & RL+Gryk+2OPT & 17.65 & 6.62\% & 1.01m & 24.83 & 7.38\% & 2.29m \\ DIMES  & RL+AS+Gryk+2OPT & 17.34 & 4.57\% & 2.10n & 24.33 & 5.22\% & 4.49m \\ DIFUSCO  & RL+Gryk+2OPT & 16.81 & 1.55\% & 5.75m & 23.55 & 1.86\% & 17.52m \\ T2T (Ours) & SL+Gryk+2OPT & 16.68 & 0.78\% & 4.98m & **23.41** & **1.25\%** & **15.90m** \\  EAN  & RL+AS+2OPT & 23.75 & 43.57\% & 57.7on & 47.73 & 106.46\% & 5.39n \\ AM  & RL+BS & 19.53 & 18.03\% & 11.99m & 29.90 & 29.23\% & 1.64n \\ GCN  & SL+BS & 30.37 & 33.55\% & 38.02nm & 51.26 & 121.73\% & 51.67m \\ DIMES  & RL+S & 18.84 & 13.84\% & 1.06n & 26.36 & 14.01\% & 2.83n \\ DIMES  & RL+AS+S & 17.80 & 7.55\% & 2.11h & 24.89 & 7.70\% & 4.53n \\ DIFUSCO  & SL+S & 17.48 & 5.65\% & 19.0m & 25.11 & 8.61\% & 59.18m \\ T2T (Ours) & SL+S & **17.02** & **2.84\%** & **15.08m** & **24.72** & **6.92\%** & **53.92nm** \\  DIMES  & RL+S+2OPT & 17.64 & 6.56\% & 1.10m & 24.81 & 7.29\% & 2.86m \\ DIMES  & RL+AS+S+2OPT & 17.29 & 4.48\% & 2.11h & 24.32 & 5.17\% & 4.53n \\ DIFUSCO  & SL+S+2OPT & 16.69 & 0.83\% & 19.05m & 23.42 & 1.30\% & 59.53m \\ T2T (Ours) & SL+S+2OPT & **16.61** & **0.37\%** & **16.03n** & **23.30** & **0.78\%** & **54.67m** \\   

Table 2: Results on TSP-500 and TSP-1000. AS: Active Search, S: Sampling Decoding, BS: Beam Search. \({}^{*}\) denotes the baseline for computing the performance drop. DIFUSCO and T2T are compared in the same running settings, while other numbers are quoted from [34; 35].

    & Drop \\  AM & 16.767\% \\ GCN & 40.035\% \\ Learn2OPT & 1.725\% \\ GNNGLS & 1.529\% \\ DIFUSCO & 0.319\% \\ T2T & **0.133\%** \\   

Table 3: Evaluation on TSPLIB dataset.

Figure 4: Effect of \(\) and initialization steps to the performance drop. For each subgraph, the left represents the greedy decoding, while the right represents the sampling decoding.

stands less sensitive to the quality of labels in training than other peer methods. It further demonstrates the generality of T2T especially when high-quality solution labels are not easily available.

### Experiments for MIS

**Datasets.** Two datasets are tested for the MIS problem following [70; 72; 71; 35; 19], include SATLIB  and Erdos-Renyi (ER) graphs . SATLIB contains SAT instances encoded in DIMACS CNF format, which are then reduced to MIS instances. ER graphs are randomly generated with each edge maintaining a fixed probability of being present or absent, independently of the other edges. We adopt ER graphs of \(700\) to \(800\) nodes with the pairwise connection probability set as \(0.15\).

**Metrics.** Following previous works [12; 33; 35; 19], we adopt three evaluation metrics to measure model performance: 1) Size: the average size of the solutions w.r.t. the corresponding instances, i.e. the objective. 2) Drop: the relative performance drop w.r.t. length compared to the optimal solution or the reference solution; 3) Time: the average computational time required to solve the problems.

**Main Results.** Table 5 presents results. The baselines include state-of-the-art neural methods with greedy decoding and sampling decoding (\( 4\)), as well as exact solver Gurobi  and heuristic solver KaMIS . The solving time of Gurobi is set as comparable to neural solvers, thus it does not reach optimality. DIFUSCO and T2T are compared in the same conditions. The results of other baselines are quoted from . DIFUSCO and T2T adopt 50 and 20 inference steps, respectively. In the greedy decoding setting, compared to previous neural solvers, T2T improves the performance drop from 0.33% to 0.22% for SATLIB dataset and from 18.53% to 11.83% for ER-[700-800] dataset.

## 5 Conclusion and Future work

We have presented a principled learning paradigm for solving hard-constrained optimization problems specifically combinatorial optimization, which bridges the offline training with reference solution as supervision to the instance-wise solving in testing which calls for more direct gradient feedback from the objective score. Experiments on TSP and MIS show the strong performance of our approach. With the potential generality, further validation across a wider range of CO problems is the immediate further work, including those with general representing capabilities such as Mixed Integer Programming (MIP) and ML-dominant special problems like Quadratic Assignment Problem (QAP). Another promising future direction is to explore the potential of recent (scalable) graph Transformers [75; 76; 77] as expressive encoder backbones for solving complex CO problems.

    & Training & TSP-50 & TSP-100 & TSP-500 & TSP-1000 \\  TSP-50 & \(50\) & \(\) & \(53.025\) & \(53.598\) & \(53.598\) & \(54.217\) \\   & T2T & \(\) & \(\) & \(\) & \(\) & \(\) \\  TSP-100 & DPUSCO & \(7.8214\) & \(\) & \(\) & \(80.3448\) & \(80.3194\) \\   & T2T & \(\) & \(\) & \(\) & \(\) & \(\) \\  TSP-500 & DPUSCO & \(7.1314\) & \(\) & \(\) & \(\) & \(\) \\   & T2T & \(\) & \(\) & \(\) & \(\) & \(\) \\  TSP-1000 & DPUSCO & \(24.174\) & \(420.9639\) & \(\) & \(\) & \(\) \\   & T2T & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: Generalization results. _Tour length_ and _drop_ with _Greedy Decoding_ are reported.

Figure 5: Results on TSP-50 for fewer-shot training with lower-quality supervision.

    &  &  &  \\   & & SZE!T & Drop\(\) & TME & SZE!T & Drop\(\) & TME \\  KaMIS  & Heuristics & 425.96\({}^{*}\) & – & 37.58m & 44.87\({}^{*}\) & – & 52.13m \\ Gurobi  & Exact & 425.95 & 0.00\% & 26.00m & 41.28 & 7.78\% & 50.00m \\  Intel  & SL+Grady & 420.66 & 1.48\% & 23.05m & 34.86 & 22.31\% & 6.06m \\ DIMACS  & RL+Grady & 421.24 & 1.11\% & 24.17m & 38.24 & 14.78\% & 6.12m \\ DIFUSCO  & SL+Grady & 424.56 & 0.33\% & 8.25m & 36.55 & 18.53\% & 8.82m \\  T2T (Ours) & SL+Grady & **425.02** & **0.22\%** & **81.21m** & **39.56** & **11.83\%** & **8.33m** \\   Intel  & SL+TS & – & – & – & 38.80 & 13.43\% & 20.00m \\ DGL  & SL+TS & – & – & – & 37.26 & 16.96\% & 22.71m \\ LWD  & RL+S & 422.22 & 0.88\% & 18.83m & 41.17 & 8.25\% & 6.33m \\ GFlowNets  & UL+S & 423.54 & 0.57\% & 23.22m & 41.14 & 8.53\% & 2.92m \\ DIFUSCO  & SL+S & 425.13 & 0.19\% & 26.32m & 40.35 & 10.07\% & 32.98m \\ 
**T2T (Ours)** & SL+S & **425.22** & **0.17\%** & 23.80m & **41.37** & **7.81\%** & 29.75m \\   

Table 5: Results on MIS. TS: Tree Search, UL: Unsupervised Learning. DIFUSCO and T2T are compared in the same running settings, while other numbers are quoted from [35; 43].