# Curvature Clues: Decoding Deep Learning Privacy

with Input Loss Curvature

 Deepak Ravikumar Efstathia Soufferi Kaushik Roy

Department of Electrical and Computer Engineering

Purdue University

West Lafayette, IN 47907

{dravikum, esoufler, kaushik}@purdue.edu

###### Abstract

In this paper, we explore the properties of loss curvature with respect to input data in deep neural networks. Curvature of loss with respect to input (termed input loss curvature) is the trace of the Hessian of the loss with respect to the input. We investigate how input loss curvature varies between train and test sets, and its implications for train-test distinguishability. We develop a theoretical framework that derives an upper bound on the train-test distinguishability based on privacy and the size of the training set. This novel insight fuels the development of a new black box membership inference attack utilizing input loss curvature. We validate our theoretical findings through experiments in computer vision classification tasks, demonstrating that input loss curvature surpasses existing methods in membership inference effectiveness. Our analysis highlights how the performance of membership inference attack (MIA) methods varies with the size of the training set, showing that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets, as demonstrated by our results on CIFAR10, CIFAR100, and ImageNet. These findings not only advance our understanding of deep neural network behavior but also improve the ability to test privacy-preserving techniques in machine learning.12

## 1 Introduction

Deep neural networks are being increasingly trained on sensitive datasets; thus ensuring the privacy of these models is paramount. Membership inference attacks (MIA) have become the standard approach to test a model's privacy (Murakonda and Shokri, 2020). These attacks take a trained model and aim to identify if a given example was used in its training. Recent work has linked curvature of loss with respect to input with memorization (Garg et al., 2024) and differential privacy (Dwork et al., 2006; Ravikumar et al., 2024). Inspired by this line of research, we investigate the properties of input loss curvature and leverage our insights to develop a new membership inference attack.

Curvature of loss with respect to input (termed input loss curvature) is defined as the trace of the Hessian of loss with respect to the input (Moosavi-Dezfooli et al., 2019; Garg and Roy, 2023). Prior works that study loss curvature have focused on two lines of research. The first line of research has focused on studying the loss curvature with respect to the weights of the deep neural net (Keskar et al., 2017; Wu et al., 2020; Jiang et al., 2020; Foret et al., 2021; Kwon et al., 2021; Andriushchenko and Flammarion, 2022) to better understand generalization. The second line of research studied the loss curvature with respect to the input (i.e. data) for gaining insight into adversarial robustness[Moosavi-Dezfooli et al., 2019, Fawzi et al., 2018], coresets [Garg and Roy, 2023] and memorization [Garg et al., 2024, Ravikumar et al., 2024], mainly focusing on its properties on the train set. Thus, there is a gap in our understanding of input loss curvature on unseen test examples.

Input loss curvature on the train set captures the prototypicality of an image. This is visualized in Figure 1 (left) which shows high curvature examples from ImagetNet [Russakovsky et al., 2015] train set. Figure 1 (right) builds intuition as to why this is the case - a sample with lots of support in the dataset is more likely to lie in low curvature regions while atypical/non-prototypical examples have less support in the training set and thus lie in higher curvature regions [Garg et al., 2024, Ravikumar et al., 2024]. With this established, we make a novel observation that on average the input loss curvature scores for test set examples are higher than train set examples. This is because test samples were not optimized for, hence they lie slightly off the flat minima, in regions of higher input curvature (also visualized in Figure 1). We leverage this insight and develop a theoretical framework that focuses on the distinguishability of train-test input loss curvature scores. Studying this distinguishability leads us to understand performance bounds of membership inference attacks [Shokri et al., 2017, Sablayrolles et al., 2019, Song and Mittal, 2021, Carlini et al., 2022].

Our theoretical analysis obtains an upper bound on the KL divergence between train-test input loss curvature scores. This provides a ceiling for membership inference attack performance. The analysis also reveals that the upper bound when using input loss curvature scores is dependent on the number of training set examples and the differential privacy parameter \(\). This insight helps us understand the conditions under which input loss curvature can be more effective at detecting train vs. test examples. Conditions such as the number of training examples and the privacy guarantees of the model.

To test our theoretical results we perform black-box (setting where adversaries have access only to model outputs) membership inference attack (MIA) using input loss curvature scores. However, input loss curvature score calculation needs access to model parameters (which are not available in a black box setting). To resolve this issue we propose using a zero-order input loss curvature estimation. Zero-order estimation can calculate input loss curvature without needing access to model parameters. Our input loss curvature-based black-box membership inference attack results show that real datasets contain enough training examples for curvature scores to outperform current state-of-the-art membership inference methods. Our results show that curvature based MIA outperforms prior state-of-the-art techniques on 10% or larger subsets of CIFAR100 dataset (about 5000 sample training set).

In summary, our contributions are as follows:

* Theoretical Foundation: We provide theoretical analysis to understand the train-test distinguishability with input loss curvature scores, demonstrating that input loss curvature is more

Figure 1: Visualizing low and high input curvature samples from a ResNet50 trained on ImageNet. Low input curvature training set images are prototypical and have lots of support in the trainset, while high input curvature train set examples have less support and are atypical. Test set examples lie around the training set images in higher curvature regions.

sensitive and hence better at detecting train vs. test set examples than current state-of-the-art membership inference attacks.
* Adapting Theory To Practice: We propose using zero order input loss curvature estimation to enable black box membership inference attack using input loss curvature scores.
* Better Attack: We conduct experiments to validate our theoretical results. Specifically, we show that input loss curvature enables more effective black box membership inference attacks when compared to existing state-of-the-art techniques.

## 2 Related Work

**Membership Inference Attacks** are used as a tool to test privacy (Murakonda and Shokri, 2020). These attacks aim to identify if a particular data point was included in a model's training dataset (Shokri et al., 2017). Existing techniques often leverage model outputs like loss values (Shokri et al., 2017; Yeom et al., 2018; Sablayrolles et al., 2019), confidence scores (Carlini et al., 2022) or modified entropy (Song and Mittal, 2021). Shokri et al. (2017) proposed the used of shadow models which are auxiliary models trained on subsets of the target model's data to aid in inference. Several modifications to this approach have been proposed. One important addition is the focus on example hardness, where the authors Sablayrolles et al. (2019) proposed an attack that scaled the loss using per example hardness threshold which is estimated by training shadow models. Watson et al. (2022) proposed a similar approach but in the offline case where they calibrated the example hardness using the average loss of shadow models not trained on the target example. Ye et al. (2022) models various attacks into four categories and performs differential analysis to explain the gaps between them. Both Ye et al. (2022) and Long et al. (2020) consider the entire loss distribution of samples that are not in the training set. However, they face challenges in extrapolating to low false positive rates (FPRs). To address this issue Carlini et al. (2022) propose using a parametric model along with shadow models to improve performance. Orthogonal to these approaches Choquette-Choo et al. (2021) suggests the use of input augmentations during evaluation to improve the performance of the attacks. Similarly Jayaraman et al. (2021) propose the MERLIN attack, which queries the target model multiple times on a sample perturbed with Gaussian noise.

**Input Loss Curvature** is defined as the trace of the Hessian of loss with respect to the input (Moosavi-Dezfooli et al., 2019; Garg and Roy, 2023). The aim is to measure the sensitivity of the deep neural network to a specific input. In general, loss curvature with respect to the parameters of deep neural nets has received lots of attention (Keskar et al., 2017; Wu et al., 2020; Jiang et al., 2020; Foret et al., 2021; Kwon et al., 2021; Andriushchenko and Flammarion, 2022), specifically due to its role in characterizing the sharpness of the learning objective which is closely connected to generalization. However, loss curvature with respect to the input data has received much less attention. It has been studied in the context of adversarial robustness (Fawzi et al., 2018; Moosavi-Dezfooli et al., 2019), coresets (Garg and Roy, 2023). It has recently been linked with memorization (Garg et al., 2024; Ravikumar et al., 2024) and privacy (Ravikumar et al., 2024). The authors in Moosavi-Dezfooli et al. (2019) showed that adversarial training decreases the input loss curvature and provided a theoretical link between robustness and curvature. In an orthogonal direction, Garg and Roy (2023) proposed the use of low input loss curvature examples as training dataset sets called coresets, which they showed to be more data-efficient. However, all of these works have focused on input loss curvature on the trainset. In this paper we focus on input loss curvature and its behavior on test or unseen examples to understand and improve the performance of membership inference attacks. Before we discuss our contributions, we present a few preliminaries, notation, and background needed for this paper

## 3 Notation and Background

**Notation.** Let us consider a supervised learning problem, where the goal is to learn a mapping from an input space \(^{d}\) to an output space \(\). The learning is performed using a randomized algorithm \(\) on a training set \(S\). Note that a randomized algorithm employs a degree of randomness as a part of its logic. The training set \(S\) contains \(m\) elements denoted as \(z_{1},,z_{m}\). Each element \(z_{i}=(x_{i},y_{i})\) is drawn from an unknown distribution \(\), where \(z_{i},x_{i}\), \(y_{i}\) and \(=\). Thus, we define the training set \(S^{m}\) as \(S=\{z_{1},,z_{m}\}\). Another related concept is that of adjacent datasets, which are obtained when the set's \(i^{th}\) element is removed and defined as

\[S^{ i}=\{z_{1},,z_{i-1},z_{i+1},,z_{m}\}.\]

Additionally, the concept of adjacent (or neighboring) datasets is linked to the distance between datasets. The distance between two datasets \(S\) and \(S^{}\), denoted by \(\|S-S^{}\|_{1}\), measures the number of samples that differ between them. The notation \(\|S\|_{1}\) represents the size of a dataset \(S\). When a randomized learning algorithm \(\) is applied to a dataset \(S\), it produces a hypothesis denoted by \(h_{S}^{}=(,S)\), where \(\) is the random variable associated with the algorithm's randomness. A cost function \(c:^{+}\) is used to measure the hypothesis's performance. The cost of the hypothesis \(h\) at a sample \(z_{i}\) is also referred to as the loss \(\) at \(z_{i}\), defined as \((h,z_{i})=c(h(x_{i}),y_{i})\). The performance of a hypothesis \(h\) is measured using risk \(R(h)=_{z_{i}}[(h,z_{i})]\) and approximated using empirical risk \(R_{emp}(h)=(1/m)_{i=1}^{m}(h,z_{i})\).

**Differential Privacy.** Introduced by Dwork et al. (2006), differential privacy is defined as follows: A randomized algorithm \(\) with domain \(^{m}\) is considered \(\)-differentially private if for any subset \(()\) and for any datasets \(S,S^{}^{m}\) differing in at most one element (i.e. \(||S-S^{}||_{1} 1\)):

\[_{}[h_{S}^{}] e^{}_{}[h_{S^{ }}^{}],\] (1)

where the probability is taken over the randomness of the algorithm \(\), with \(\).

**Error Stability** of a possibly randomized algorithm \(\) for some \(>0\) is defined as Kearns and Ron (1997):

\[ i\{1,,m\},\ |_{,z}[(h_{S}^{},z)]- _{,z}[(h_{S^{}}^{},z)]|\ ,\] (2)

where \(z\) and \(\).

**Generalization.** A randomized algorithm \(\) is considered to generalize with confidence \(\) and at a rate of \(^{}(m)\) if:

\[[|R_{emp}(h,S)-R(h)|^{}(m)].\] (3)

**Uniform Model Bias.** The hypothesis \(h\) produced by algorithm \(\) to learn the true conditional \(h^{*}=[y|x]\) from a dataset \(S^{m}\) has uniform bound on model bias denoted by \(\) if:

\[ S^{m},|_{}[R(h_{S}^{})-R(h^ {*})]|.\] (4)

\(\)**-Lipschitz Hessian.** The Hessian of \(\) is Lipschitz continuous on \(\), if \( z_{1},z_{2}\), and \( h()\), there exists some \(>0\) such that:

\[\|_{z_{1}}^{2}(h,z_{1})-_{z_{2}}^{2}(h,z_{2})\| \|z_{1}-z_{2}\|.\] (5)

**Input Loss Curvature.** As defined by Moosavi-Dezfooli et al. (2019); Garg et al. (2024), input loss curvature is the sum of the eigenvalues of the Hessian \(H\) of the loss with respect to input \(z_{i}\). This is conveniently expressed using the trace as:

\[_{}(z_{i},S)=(H)=(_{z_{i}}^{2} (h_{S}^{},z_{i}))\] (6)

\(\)**-adjacency.** A dataset \(S\) is said to contain \(\)-adjacent (read as upsilon-adjacent) elements if it contains two elements \(z_{i},z_{j}\) such that \(z_{j}=z_{i}+\) for some \( B_{p}()\) (read as \(\)-Ball). This condition can be ensured through construction. Consider a dataset \(S^{}\) which has no \(z_{j}\) s.t \(z_{j}=z_{i}+;z_{j},z_{i} S^{}\). We can construct \(S\) such that \(S=\{z\,|\,z S^{}\}\{z_{i}+\}\) for some \(z_{i} S^{}, B_{p}()\), thereby ensuring \(\)-adjacency.

**Membership Inference Threat Model.** In a membership inference security game Yeom et al. (2018); Jayaraman et al. (2021); Carlini et al. (2022), a challenger and an adversary interact to test the privacy of a machine learning model. The process begins with the challenger sampling a training dataset from a distribution and training a model on this data. The challenger then flips a coin to decide whether to select a fresh data point from the distribution, which is not part of the training set, or to choose a data point from the training set. This selected data point is given to the adversary, who has access to the same data distribution and the trained model. The adversary's task is to determine whether the given data point was part of the training set or not. If the adversary's guess is correct, the game indicates a successful membership inference attack. Such a game is said to be in a **black-box** setting when the adversary has access to only the challenger's output.

## 4 Theoretical Analysis

In this section, we analyze the distinguishability of train-test samples using KL divergence. We do so for two cases, the first case when using network's output probability, and second when using input loss curvature. The importance of analyzing network output probabilities stems from its utilization in the current state-of-the-art attack, LiRA (Carlini et al., 2022). Thus, studying both cases will let us theoretically analyze and compare their performance with input loss curvature.

Before we begin the analysis, we briefly discuss the conditional and marginal distributions of input loss curvature for train and test examples. Discussing this is important to build intuition for the analysis. Figure 2 visualizes the histogram (proxy for distribution) of input loss curvature for train and test examples from ImageNet (Russakovsky et al., 2015). Specifically, it plots the log of the input loss curvature \((_{})\) obtained on pre-trained ResNet50 (He et al., 2016) models from Feldman and Zhang (2020). It plots a histogram of these scores, showing the frequency of specific log curvature values.

A naive membership inference attack would apply a threshold to separate these distributions. However, it is common practice to consider sample specific scores (i.e., conditioned on the target sample). This is visualized in Figure 3 which plots the distribution of curvature scores for a single ImageNet sample, indicating differences when the sample is part of the training set versus when it is a test or unseen sample. The data was generated using models from Feldman and Zhang (2020). The figure also includes a kernel density estimate (KDE, shown as a dashed line) to better to visualize the underlying distribution. Figure 3 suggests that, similar to Carlini et al. (2022) sample conditional curvature scores can be modeled using a Gaussian parametric model. If we represent the curvature score \(_{}\) as a random variable, then \(_{}(,)\). The probability density function of \(_{}\) is a function of the randomness of the algorithm \(\), the dataset \(S\) and the \(i^{th}\) sample \(z_{i}\) and be denoted by \(p_{c}(,S,z_{i})\). Similarly, let \(p(,S,z_{i})\) denote the probability density function of the neural network's output probability, which is also parameterized by the randomness of the algorithm \(\), the dataset \(S\) and the \(i^{th}\) sample \(z_{i}\). With this setup we present the following theoretical results on the upper bound on the KL divergence between train and test distribution for the two cases. Theorem 4.1 presents the upper bound on the KL divergence when using the neural network's output probability scores, and Theorem 4.2 presents the upper bound on the KL divergence when using input loss curvature.

**Theorem 4.1** (Privacy bounds Train-Test KL Divergence).: _Assume \(\)-differential private algorithm, then the KL divergence between train-test distributions of the neural network's output probability is upper bound by the differential privacy parameter \(\) given by:_

\[_{}(p(,S,z_{i})\;||\;p( ,S^{ i},z_{i}))\] (7)

**Sketch of Proof.** Given that the algorithm \(\) is \(\)-differentially private, we know that the probability of the output \(h_{S}^{}\) on dataset \(S\) is bounded by \(e^{}\) times the probability of the same eventon the neighboring dataset \(S^{ i}\), i.e. \(_{}[h_{S}^{}] e^{}_{}[h_{S^{ i }}^{}]\). From this inequality it follows that the KL divergence between the output distributions of \(\) on \(S\) and \(S^{ i}\) is bounded by \((p(,S,z_{i}) p(,S^{ i},z_{i}))\). The full proof for Theorem 4.1 is provided in Appendix A.3.

**Theorem 4.2** (Dataset Size and Privacy bound Curvature KL Divergence).: _Let the assumptions of error stability 2, generalization 3, and uniform model bias 4 hold. Further, assume \(0 L\). Let the conditional distribution be parameterized with variance \(\). Then, any \(\)-differential private algorithm using a dataset of size \(m\) with a probability at least \(1-\) satisfies:_

\[_{}(p_{c}(,S,z_{i}) p_{c}( ,S^{ i},z_{i})))+c ]^{2}}{2^{2}}\] (8)

\[c=(4m-1)+2(m-1)+[\|\|^{3}]+L\] (9)

**Sketch of Proof.** We begin by assuming a Gaussian model for the curvature distribution and expand the formula for the KL divergence, which leads to expressions involving the mean and standard deviation of the curvature scores. To establish an upper bound for this expression, we prove a lemma concerning the upper bound of the mean curvature, utilizing results from Ravikumar et al. (2024). This enables us to express the upper bound on the mean curvature in terms of the privacy parameter \(\). Subsequently, this result is employed to derive an upper bound on the KL divergence of the curvature scores. The proof for Theorem 4.2 is provided in Appendix A.5.

**Discussion.** Theorem 4.1 and Theorem 4.2 provide the upper bound on KL Divergence between train and test distributions of the probability and input loss curvature scores, respectively. They imply the upper limit of MIA performance, and interestingly, the result of Theorem 4.2 suggests the role of the dataset size on MIA performance. However, the relation is not as straightforward as suggested by Equation 8. This is because, in real applications, parameter \(\) depends on \(\), and so does the loss bound \(L\). In fact, the loss bound is also dependent on the number of samples \(m\)(Bousquet and Elisseeff, 2002).

**Theorem 4.3** (Dataset Size and Curvature MIA Performance).: _Let the assumptions of error stability 2, generalization 3, and uniform model bias 4 hold. Further, assume \(0 L\), and the bounds of Theorem 4.1 and 4.2 are tight. Then, the performance of MIA using curvature scores exceeds that of confidence scores with a probability at least \(1-\) when:_

\[m>})-c}{L(1-e^{-})}\] (10)

Theorem 4.3 suggests that the performance of curvature based MIA will exceed that of probability score based methods when the size of the dataset used to train the target model exceed a certain threshold. Indeed, this is what we observe in practice (see section 6.4).

**Sketch of Proof.** The proof compares the upper bounds from Theorem 4.1 and Theorem 4.2, followed by a series of algebraic manipulations. This identifies the conditions under which the performance of curvature scores exceeds that of confidence scores. The full proof can be found in Appendix A.6.

**On the validity of the assumptions.** Before presenting our experiments to validate our theory, we briefly discuss the validity of our assumptions in practical settings. Research by Hardt et al. (2016) shows that stochastic gradient methods, such as stochastic gradient descent, achieve small generalization error and exhibit uniform stability. Thus, the assumptions of stability (Equation 2) and generalization (Equation 3) are justified. Model bias is a property of the model, and a uniform bound across different datasets seems reasonable. Note, uniform loss bound (and \(L\) independent of \(m\) as used by Theorem 4.3) holds true for certain losses and for statistical models and is often assumed in learning theory (Wang et al., 2016). Lastly, the \(\)-adjacency can be ensured through construction. For large datasets this may not be needed, because the size of the ball \(B_{p}()\) is unconstrained. Hence two samples from the same class that are similar may suffice. Given the size of modern datasets, this assumption is also reasonable.

## 5 Zero Order Input Loss Curvature MIA

To test if input loss curvature based membership inference performs better than existing methods we need an efficient technique to estimate curvature. We are interested in black-box membership inference attacks, where one does not have access to the target network's parameters. However, current techniques use Hutchinson's trace estimator (Hutchinson, 1989) to measure input loss curvature such as from Garg and Roy (2023); Garg et al. (2024) or Ravikumar et al. (2024). This approach needs to evaluate the gradient and hence requires access to model parameters. To solve this issue, we propose using a zero-order estimation technique. Zero-order curvature estimation starts with a finite-difference estimation. Consider a function \(f:^{n}\), then the Hessian at a given point \(z_{i}\) can be estimated as follows:

\[^{2}f(z_{i})=n^{2}+hv+hu)-f(z_{i}-hv+hu)-f(z_{i }+hv-hu)+f(z_{i}-hv-hu)}{4h^{2}}uv^{}\] (11)

where \(h\) is a small increment (a hyper parameter in out case), and \(u,v\) are vectors in \(^{n}\). In our case, to get the input loss curvature, we have \(f(g(x_{i}),y_{i})\), where \(g\) is the neural network, \(\) is the loss function and \(z_{i}=(x_{i},y_{i})\) are the image, label pair. The pseudo-code for obtaining input loss curvature score using zero order estimation shown in Algorithm 1 in Appendix A.1. To execute membership inference attack using input loss curvature scores, we propose the following methodology. First, we begin by training shadow models, similar to Shokri et al. (2017); Carlini et al. (2022). These shadow models are used to obtain empirical estimates of parametric model for the curvature score described in Section 4. During the inference phase, we employ a likelihood ratio between the sample being in the train set vs test set parametric models to identify the membership status of a given sample (see Appendix A.2 for pseudo-code). In addition, we perform a negative log likelihood ratio test. We denote the results of likelihood test as 'LR' and the results of the negative log likelihood ratio test as 'NLL'.

## 6 Experiments

### Experimental Setup

**Datasets.** To evaluate our theory, we consider the classification task using standard vision datasets. Specifically, we use the CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) datasets.

**Architectures.** For experiments on ImageNet we use the ResNet50 architecture (He et al., 2016). For CIFAR10 and CIFAR100, we used the ResNet18 architecture. Details regrading hyperparameters are provided in Appendix A.13. To improve reproducibility, we have open sourced the code at https://github.com/DeepakTatachar/Curvature-Clues.

**Training.** For experiments using private models, we trained ResNet18 models with the Opacus library (Yousefpour et al., 2021) using DP-SGD with a maximum gradient norm of 1.0 and a privacy parameter of \(=1 10^{-5}\). Shadow models for CIFAR10 and CIFAR100 were trained on a 50% subset of the data for 300 epochs. For ImageNet, we used pre-trained models from Feldman and Zhang (2020), trained on a 70% subset of ImageNet. More details about training and compute resources are provided in Appendix A.13.

**Metrics.** To evaluate curvature scores, we use AUROC and balanced accuracy. The Receiver Operating Characteristic (ROC) is the plot of the True Positive Rate (TPR) against the False Positive Rate (FPR). The area under the ROC is called Area Under the Receiver Operating Characteristic (AUROC). AUROC of 1 denotes an ideal detection scheme, since the ideal detection algorithm results in 0 false positive and false negative samples.

### Membership Inference

In this subsection, we compare the performance of the proposed input loss curvature based membership inference against prior MIA techniques.

**Setup:** We use CIFAR10, CIFAR100 and ImageNet datasets to test the MIA performance. We consider a **black-box** MIA setup similar to Carlini et al. (2022). We use ResNet18 for CIFAR10 and CIFAR100, for ImageNet we use the ResNet50 architecture. For all the membership inference attacks, we compute a full ROC curve and report the results. When using shadow models we 64 for CIFAR10 and CIFAR100 and 52 for ImageNet. The AUROC plot for CIFAR100 for various methods are shown in Figure 4. Table 1 reports the average balanced accuracy and AUROC values over three seeds on various MIA methods on the three datasets. The plot in Figure 4 is a log-log plot to emphasize performance of the proposed method at very low false positive rates (see the orange line y-intercept). We also studied the effect of augmentations and the results can be found in Appendix A.7, the takeaway was that adding more augmentations improved performance. Note that the results presented in Table 1 used 2 augmentations (image + mirror) for our technique and Carlini et al. (2022) for fair comparison.

**Results:** From Table 1, we see that the proposed method performs better than all existing MIA techniques on both ImageNet and CIFAR datasets. Apart from AUROC and balanced accuracy results, the log-log plot emphasizes the performance at really low FPR (i.e. the y intercept, high TPR at low FPR in Figure 4). Additional results at low FPR are available in Appendix A.8.

**Takeaways:** As predicted by Theorem 4.2, the higher KL divergence between train and test curvature score distributions results in superior MIA performance. Further, we observe that while using a negative log likelihood ratio test does better in AUROC and balanced accuracy, the parametric likelihood ratio test does better at low false positive rates as shown in Figure 4. Thus, the proposed use of curvature should be tailored based on use case. Applications that demand high AUROC can use NLL approach, while those that demand high TPR at very low FPR should use the LR technique.

### Effect of Privacy

In this section, we study the effect of differential privacy on MIA performance and test the \(\) relation predicted by Theorem 4.2.

**Setup**: To study the effect of privacy on MIA attack performance, we use DP-SGD trained models. We use privacy \(\) values of 1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50 with \(=1 10^{-5}\). We

    &  &  &  \\   & Bal. Acc. & AUROC & Bal. Acc. & AUROC & Bal. Acc. & AUROC \\ 
**Curv ZO NLL (Ours)** & **69.16 \(\) 0.08** & **77.45 \(\) 0.09** & **84.47 \(\) 0.21** & **93.49 \(\) 0.18** & **61.92 \(\) 0.87** & **68.82 \(\) 1.30** \\ Curv ZO LR (Ours) & 68.76 \(\) 0.04 & 72.28 \(\) 0.04 & 80.48 \(\) 0.10 & 90.15 \(\) 0.04 & 55.00 \(\) 0.17 & 58.89 \(\) 0.38 \\ Carlini et al. (2022) & 66.14 \(\) 0.01 & 73.46 \(\) 0.02 & 81.55 \(\) 0.13 & 88.89 \(\) 0.16 & 58.23 \(\) 0.29 & 61.73 \(\) 0.32 \\ Yeom et al. (2018) & 58.50 \(\) 0.02 & 63.23 \(\) 0.03 & 76.29 \(\) 0.39 & 82.11 \(\) 0.31 & 55.57 \(\) 0.52 & 60.44 \(\) 0.75 \\ Sablayrolles et al. (2019) & 66.93 \(\) 0.05 & 76.50 \(\) 0.04 & 70.22 \(\) 0.41 & 81.11 \(\) 0.39 & 56.65 \(\) 0.36 & 61.50 \(\) 0.79 \\ Watson et al. (2022) & 61.40 \(\) 0.06 & 69.44 \(\) 0.05 & 62.71 \(\) 0.31 & 71.66 \(\) 0.50 & 54.86 \(\) 0.59 & 58.58 \(\) 0.36 \\ Ye et al. (2022) & 66.16 \(\) 0.02 & 75.79 \(\) 0.05 & 80.73 \(\) 0.24 & 90.88 \(\) 0.19 & 59.62 \(\) 0.84 & 67.30 \(\) 1.25 \\ Song and Mittal (2021) & 57.88 \(\) 0.03 & 63.29 \(\) 0.03 & 75.58 \(\) 0.29 & 82.28 \(\) 0.27 & 55.63 \(\) 0.61 & 60.42 \(\) 0.85 \\   

Table 1: Comparison of the proposed curvature score based MIA with prior methods tested on ImageNet, CIFAR100, and CIFAR10 datasets. Results reported are the mean \(\) std obtained over 3 seeds. For CIFAR10 and CIFAR100 64 shadow models were used and 52 for ImageNet.

Figure 4: Comparing our method against existing techniques at _low FPR_. The proposed parametric Curv LR technique has the highest TPR at very low FPR.

Figure 5: Validating the upper bound from Theorem 4.2 by fitting the MIA performance (AUROC of Curv LR) on DP-SGD trained models for various privacy parameters \(\) values.

plot the result of the proposed curvature based MIA in Figure 5. In Figure 5 we also plots the best fit using \(s_{f}(L_{f}(1-e^{-})+c_{f})^{2}\), where \(s_{f},L_{f}\) and \(c_{f}\) are fit to the data.

**Results and Takeaways:** We see that the theoretical prediction from Theorem 4.2 about MIA performance is well matched. Since Theorem 4.2 provides an upper bound, the results validate the theory.

### Effect of Dataset Size

In this section, we study the effect of dataset size on MIA attack performance. This lets us test the relationship of MIA performance to \(m\) as predicted by Theorem 4.2 and Theorem 4.3.

**Setup:** For this experiment, we train models on increasing dataset size on CIFAR100. Specifically, we train multiple seeds on various subsets randomly chosen from the CIFAR100 training set. We also repeated the same by choosing the lowest curvature samples from CIFAR100 and train with the same subset sizes. Next, for each of the models we perform MIA attack and we present the results in Figure 6 (randomly chosen) and Figure 7 (lowest curvature samples).

**Results:** From Figure 6, we see that when samples are randomly chosen, the MIA performance decreases as we add more samples. This result is consistent with prior literature . Further as predicted by Theorem 4.3 beyond \(30-40\%\) subset Curv ZO LR out perform prior works and Curv ZO NLL outperforms prior works beyond \(10\%\) subset size.

To extend this analysis, we train models on curvature based coresets . These coresets of low input loss curvature samples have been shown to memorize less . Thus we expect MIA accuracy to increase as we increase coreset size. This is exactly what happens and is shown in Figure 7 which plots the AUROC and accuracy of the NLL curvature attack as we increase curvature coreset size. However, the MIA performance is higher for comparable size in Figure 6 and 7. **This suggests that curvature based coresets result in more susceptible models**, which is also supported by results in Song et al. .

**Takeaways:** We validate Theorem 4.3. We note that beyond a certain dataset size (of about \(30-40\%\) subset of the training set Curv ZO LR out perform prior works and Curv ZO NLL outperforms prior works beyond \(10\%\) subset size) curvature scores outperform probability score based MIA method. While Theorem 4.3 predicts that curvature-based MIA outperforms other methods on sufficiently large datasets. This condition is often met by real datasets as evident from the results presented.

## 7 Conclusion

In this paper, we explored the properties of input loss curvature on the test set. Specifically, we focus on using input loss curvature to distinguish between train and test examples. We established a theoretical framework deriving an upper bound on train-test KL Divergence based on privacy and training set size. To extend the applicability of input loss curvature computation to a black-box setting, we propose a novel zero-order curvature estimation method. This enables the development of a new cutting-edge black-box membership inference attack (MIA) methodology that leverages input loss curvature. Through extensive experiments on the ImageNet and CIFAR datasets, we demonstrate that our input loss curvature-based MIA method outperforms existing state-of-the-art techniques. Our results corroborate our theoretical predictions regarding the relationship between MIA performance and dataset size. Specifically, we show that beyond a certain dataset size, the effectiveness of curvature scores surpasses other methods. Importantly, we observe that this dataset size condition is frequently met in practical scenarios, as evidenced by our results on the CIFAR100 dataset. These findings advance our understanding of input loss curvature in the context of privacy and help build more secure deep learning models.