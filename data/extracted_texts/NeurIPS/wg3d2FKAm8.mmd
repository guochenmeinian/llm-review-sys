# Outlier-Robust Wasserstein DRO

Sloan Nietert

Cornell University

nietert@cs.cornell.edu &Ziv Goldfeld

Cornell University

goldfeld@cornell.edu &Soroosh Shafiee

Cornell University

shafiee@cornell.edu

###### Abstract

Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an \(\)-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.

## 1 Introduction

The safety and effectiveness of various operations rely on making informed, data-driven decisions in uncertain environments. Distributionally robust optimization (DRO) has emerged as a powerful framework for decision-making in the presence of uncertainties. In particular, Wasserstein DRO (WDRO) captures uncertainties of geometric nature, e.g., due to sampling or localized (adversarial) perturbations of the data points. The WDRO problem is a two-player zero-sum game between a learner (decision-maker), who chooses a decision \(\), and Nature (adversary), who chooses a distribution \(\) from an ambiguity set defined as the \(p\)-Wasserstein ball of a prescribed radius around the observed data distribution \(\). Namely, WDRO is given by1

\[_{}_{:\,_{p}(,)}_{Z}[(,Z)],\] (1)

whose solution \(\) is chosen to minimize risk over the Wasserstein ball with respect to (w.r.t.) the loss function \(\). WDRO has received considerable attention in many fields, including machine learning [6; 22; 45; 48; 59], estimation and filtering [36; 37; 46], and chance constraint programming [12; 55]. In many practical scenarios, the observed data may be contaminated by non-geometric perturbations, such as adversarial outliers. Unfortunately, the WDRO problem from (1) is not suited for handling thisissue, as even a small fraction of outliers can greatly distort the \(_{p}\) measurement and impede decision-making. In this work, we address this gap by proposing a novel outlier-robust WDRO framework that can learn well-performing decisions even in the presence of outliers. We couple it with a comprehensive theory of excess risk bounds, statistical guarantees, and computationally-tractable reformulations, as well as supporting numerical results.

### Contributions

We consider a scenario where the observed data distribution \(\) is subject to both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination, which allows an \(\)-fraction of data to be arbitrarily corrupted. Namely, if \(\) is the true (unknown) data distribution, then the Wasserstein perturbation maps it to some \(^{}\) with \(_{p}(^{},)\), and the TV contamination step further produces \(\) with \(\|-^{}\|_{}\) (e.g., in the special case of the Huber model , \(=(1-)^{}+\) where \(\) is an arbitrary noise distribution). To enable robust decision-making under this model, we replace the Wasserstein ambiguity set in (1) with a ball w.r.t. the recently proposed outlier-robust Wasserstein distance \(_{p}^{}\). The \(_{p}^{}\) distance (see (2) ahead) filters out the \(\)-fraction of mass from the contaminated distribution that contributed most to the transportation cost, and then measures the \(_{p}\) distance post-filtering. To obtain well-performing solutions for our WDRO problem, the \(_{p}^{}\) ball is intersected with a set that encodes standard moment assumptions on the uncorrupted data distribution, which are necessary for meaningful outlier-robust estimation guarantees.

We establish minimax optimal excess risk bounds for the decision \(\) that solves the proposed outlier-robust WDRO problem. The bounds control the gap \([(,Z)]-[(_{},Z)]\), where \(Z\) follows the true data distribution and \(_{}=*{argmin}_{}[(,Z)]\) is the optimal decision, subject to regularity properties of \(_{}=(_{},)\). In turn, our bounds imply that the learner can make effective decisions using outlier-robust WDRO based on the contaminated observation \(\), so long as \(_{}\) has low variational complexity. The bounds capture this complexity using the Lipschitz or Sobolev seminorms of \(_{}\) and clarify the distinct effect of each perturbation (Wasserstein versus TV) on the quality of the learned \(\) solution. We further establish their minimax optimality when \(p=1\), by providing a matching lower bound in the setting when an adversary picks a class of Lipschitz functions over which the learner must perform uniformly well. The excess risk bounds become looser as the data dimension \(d\) grows. We show that this degradation is alleviated when the loss function depends on the data only through \(k\)-dimensional affine features, by providing risk bounds that adapt to \(k\) instead of \(d\).

We then move to study the computational side of the problem, which may initially appear intractable due to non-convexity of the constraint set. We resolve this via a cheap preprocessing step that computes a coarse robust estimate of the mean  and replaces the original constraint set (that involves the true mean) with a version centered around the estimate. We adapt our excess risk bounds to this formulation and then prove a strong duality theorem. The dual form is reminiscent of the one for classical WDRO with adaptations reflecting the constraint to the clean distribution family and the partial transportation under \(_{p}^{}\). Under additional convexity conditions on the loss, we further derive an efficiently-computable, finite-dimensional, convex reformulation. The optimization results are also adapted to the setting with low-dimensional features. Using the developed machinery, we present experiments that validate our theory on simple regression/classification tasks and demonstrate the superiority of the proposed approach over classical WRDO, when the observed data is contaminated.

### Related Work

Distributionally robust optimization.The Wasserstein distance has emerged as a powerful tool for modeling uncertainty in the data generating distribution. It was first used to construct an ambiguity set around the empirical distribution in . Recent advancements in convex reformulations and approximations of the WDRO problem, as discussed in [8; 20; 35], have brought notable computational advantages. Additionally, WDRO is linked to various forms of variation [2; 9; 19; 43] and Lipschitz [7; 11; 44] regularization, which contribute to its success in practice. Robust generalization guarantees can also be provided by WDRO via measure concentration argument or transportation inequalities [18; 30; 31; 51; 53; 54]. Several works have raised concerns regarding the sensitivity of standard DRO to outliers [24; 27; 58]. An attempt to address this was proposed in  using a refined risk function based on a family of \(f\)-divergences. This formulation aims to prevent DRO from overfitting to potential outliers but is not robust to geometric perturbations. Further, their risk bounds require a moment condition to hold uniformly over \(\), in contrast to our bounds that depend only on \(_{}\). We are able to address these limitations by setting a WDRO framework based on partial transportation. While partial OT has been previously used in the context of DRO problems, it was introduced to address stochastic programs with side information in  rather than to account for outlier robustness. Another closely related line of work is presented in [4; 5], where the ambiguity set is constructed using an \(f\)-divergence to mitigate statistical errors and the Prokhorov distance to handle outlier data. The proposed model is both computationally efficient and statistically reliable. However, they have not investigated its minimax optimality or robustness against the Huber contamination model, which we aim to do in this paper. Additionally, a best-case favorable analysis approach has been proposed in  to address outlier data. This approach is an alternative to the worst-case distributionally robust method. However, it requires solving a non-convex optimization problem, significantly impacting its scalability, and is not accompanied by any proof of minimax optimality.

Robust statistics.The problem of learning from data under TV \(\)-corruptions dates back to . Over the years, various robust and sample-efficient estimators, particularly for mean and scale parameters, have been developed in the robust statistics community; see  for a comprehensive survey. The theoretical computer science community, on the other hand, has focused on developing computationally efficient estimators that achieve optimal estimation rates in high dimensions [13; 16]. Relatedly, the probably approximate correct (PAC) learning framework has been well-studied in similar models [1; 10]. Recently,  developed a unified robust estimation framework based on minimum distance estimation that gives sharp population-limit and promising finite-sample guarantees for mean and covariance estimation, as well linear regression. Their analysis centers on a generalized resilience quantity, which is essential to our work. We are unaware of any results in the settings above which extend to combined TV and \(_{p}\) corruptions. Finally, our analysis relies on the outlier-robust Wasserstein distance from [38; 39], which was shown to yield an optimal minimum distance estimate for robust distribution estimation under \(_{p}\) loss.

## 2 Preliminaries

Notation.Consider a closed, non-empty set \(^{d}\) equipped with the Euclidean norm \(\|\|\). A continuously differentiable function \(f:\) is called \(\)-smooth if \(\| f(z)- f(z^{})\|\|z-z^{}\|\), for all \(z,z^{}\). The perspective function of a lower semi-continuous (l.s.c.) and convex function \(f\) is \(P_{f}(x,) f(x/)\) for \(>0\), with \(P_{f}(x,)=_{ 0} f(x/)\) when \(=0\). The convex conjugate of \(f\) is \(f^{*}(y)_{x^{d}}y^{}x-f(x)\). We denote by \(_{}\) the indicator function of \(\), that is, \(_{}(z)=0\) if \(z\) and \(_{}(z)=\) otherwise. The convex conjugate of \(_{}\), denoted by \(_{}^{*}\), is termed as the support function of \(\).

We use \(()\) for the set of signed Radon measures on \(\) equipped with the TV norm \(\|\|_{}||()\), and write \(\) for set-wise inequality. The class of Borel probability measures on \(\) is denoted by \(()\). We write \(_{}[f(Z)]\) for expectation of \(f(Z)\) with \(Z\); when clear from the context, the random variable is dropped and we write \(_{}[f]\). Let \(_{}\) denote the covariance matrix of \(_{2}()\). Define \(_{p}()\{():_{z }_{}[\|Z-z_{0}\|^{p}]<\}\). The push-forward of \(f\) through \(()\) is \(f_{\#}()(f^{-1}())\), and, for \(()\), write \(f_{\#}\{f_{\#}:\}\). The \(p\)th order homogeneous Sobolev (semi)norm of continuously differentiable \(f:\) w.r.t. \(\) is \(\|f\|_{H^{1,p}()}_{}[\| f\|^{p}]^{1/p}\). The set of integers up to \(n\) is denote by \([n]\); we also use the shorthand \([x]_{+}=\{x,0\}\). We write \(,,\) for inequalities/equality up to absolute constants.

Classical and outlier-robust Wasserstein distances.For \(p[1,)\), the \(p\)-_Wasserstein distance_ between \(,_{p}()\) is \(_{p}(,)_{(,)}_{ }\|X-Y\|^{p}^{1/p}\), where \((,)\{(^{2}):\,( )=,\,()=\}\) is the set of all their couplings. Some basic properties of \(_{p}\) are (see, e.g., [42; 52]): (i) \(_{p}\) is a metric on \(_{p}()\); (ii) the distance is monotone in the order, i.e., \(_{p}_{q}\) for \(p q\); and (iii) \(_{p}\) metrizes weak convergence plus convergence of \(p\)th moments: \(_{p}(_{n},) 0\) if and only if \(_{n}}{{}}\) and \(\|x\|^{p}d_{n}(x)\|x\|^{p}d(x)\).

To handle corrupted data, we employ the _\(\)-outlier-robust \(p\)-Wasserstein distance2_, defined by

\[_{p}^{}(,)_{^{ }(^{d})\\ \|^{}-\|_{}}_{p }(^{},)=_{^{}( ^{d})\\ \|^{}-\|_{}}_{p }(,^{}).\] (2)

The second equality is a useful consequence of Lemma 4 in  (see Appendix A for details, along with an interpretation of \(_{p}^{}\) as a partial OT distance).

Robust statistics.Resilience is a standard sufficient condition for population-limit robust statistics bounds [49; 58]. The _\(p\)-Wasserstein resilience_ of a measure \(()\) is defined by

\[_{p}(,)_{^{}}_{^{}}_{p}(^{ },),\]

and that of a family \(()\) by \(_{p}(,)_{}_{p}(,)\). The relation between \(_{p}\) resilience and robust estimation is formalized in the following proposition.

**Proposition 1** (Robust estimation under \(_{p}\) resilience ).: _Fix \(0 0.49\). For any clean distribution \(()\) and corrupted measure \(()\) such that \(_{p}^{}(,)\), the minimum distance estimate \(=*{argmin}_{}_{p}^{ }(,)\) satisfies \(_{p}(,)+_{p}(,2)\).3_

Throughout, we focus on the bounded covariance class \(_{}():\, _{} I_{d}}\).

**Proposition 2** (\(_{p}\) resilience bound for \(_{}\)).: _Fixing \(0 0.99\) and \(1 p 2\), we have \(_{p}(_{},)\,^ {1/p-1/2}\)._

## 3 Outlier-robust WDRO

We perform stochastic optimization with respect to an unknown data distribution \(\), given access only to a corrupted version \(\). We allow both localized Wasserstein perturbations, that map \(\) to some \(^{}\) with \(_{p}(,^{})\), and \(\)\(\)-contamination that takes \(^{}\) to \(\) with \(\|-^{}\|_{}\). Equivalently, both perturbations are captured by \(_{p}^{}(,)\).4 To simplify notation, we henceforth suppress the dependence of the loss function \(\) on the model parameters \(\), writing \(\) for \((,)\) for a specific function and \(=\{(,)\}_{}\) for the whole class. Our full model is as follows.

**Setting A:** Fix a \(p\)-Wasserstein radius \( 0\) and \(\) contamination level \([0,0.49]\). Let \(^{}\) be a family of real-valued loss functions on \(\), such that each \(\) is l.s.c. with \(_{z}}<\), and fix a class \(_{p}()\) encoding distributional assumptions. We consider the following model:

1. Nature selects a distribution \(\), unknown to the learner;
2. The learner observes a corrupted measure \(()\) such that \(_{p}^{}(,)\);
3. The learner selects a decision \(\) and suffers excess risk \(_{}[]-_{}_{}[]\).

We seek a decision-making procedure for the learner which provides strong excess risk guarantees when \(_{}=*{argmin}_{}_{}[]\) is appropriately "simple." To achieve this, we introduce the _\(\)-outlier-robust \(p\)-Wasserstein DRO problem_:

\[_{}_{:\,_{p}^{}( ,)}_{}[].\] (OR-WDRO)

### Excess Risk Bounds

We quantify the excess risk of decisions made using OR-WDRO for the two most popular choices of order, \(p=1,2\). Proofs are provided in Supplement B.

**Theorem 1** (OR-WDRO risk bound).: _Under Setting \(A\), let \(\) minimize (OR-WDRO). Then, writing \(c=2(1-)^{-1/p}\), the excess risk is bounded by_

\[_{}[]-_{}[_{}] \|_{}\|_{}c+2_{1}(,2),&p=1,_{}\\ \|_{}\|_{^{1,2}()}c+2_{2}(,2 )+c+2_{2}(,2 )^{2},&p=2,_{}.\]

Note that \(c=O(1)\) since \( 0.49\). These bounds imply that the learner can make effective decisions when \(_{}\) has low variational complexity5. In contrast, there are simple regression settings with TV corruption that drive the excess risk of standard WDRO to infinity. Our proof derives both results as a special case of a general bound in terms of the \(_{p}\) regularizer, defined by \(_{,p}(;)_{^{}( ):_{p}(^{},^{})}_{ ^{}}[]-_{}[]\). Introduced in , this quantity appears implicitly throughout the WDRO literature. In particular, for each \(\), we derive the following bound:

\[_{}[]-_{}[]_{,p} _{_{p}}+(,2)}_{};,\] (3)

whose radius reveals the effect of each perturbation (viz. Wasserstein versus TV) on the quality of the decision. The first bound of the theorem follows by plugging in \(p=1\) and controlling \(_{,1}\) via Kantorovich duality. The second bound uses \(p=2\) and controls \(_{,2}\) by replacing \(\) with its Taylor expansion about \(Z\). We now instantiate Theorem 1 for the bounded covariance class \(_{}\).

**Corollary 1** (Risk bounds for \(_{}\)).: _Under the setting of Theorem 1 with \(_{}\), we have_

\[_{}[]-_{}[_{}] \|_{}\|_{}+ \,,&p=1,_{}\\ \|_{}\|_{^{1,2}()}(+\,)+(^{2}+d),&p=2,_{}.\]

Since \(_{}\) encodes second moment constraints, \(_{2}(_{},) d\) is independent of \(\). Therefore, the first bound is preferable as \( 0\) if \(\|_{}\|_{^{1,2}()}\|_{}\|_{}\), while the second is better when \(=(1)\) and \(\|_{}\|_{^{1,2}()}\|_{}\|_{}\).6 Distinct trade-offs are observed under stronger tail bounds like sub-Gaussianity, i.e., for \(_{}\{():\, _{}[e^{(^{}(Z-[Z])^{2})}] 2,\, ^{d-1}\}\).

**Corollary 2** (Risk bounds for \(_{}\)).: _Under the setting of Theorem 1 with \(_{}\), the excess risk \(_{}[]-_{}[_{}]\) is bounded up to constants by_

\[\|_{}\|_{}+}\,\,,&p=1,_{}\\ \|_{}\|_{^{1,2}()}+)}\,+^{2}+d+\,\,,&p=2,_{}.\]

**Remark 1** (Comparison to MDE under \(_{p}^{c}\)).: We note that the excess risk \(_{,p}c+2_{p}(,2);_{ }\) from (3) can alternatively be obtained by performing standard \(p\)-WDRO with an expanded radius \(c+2_{1}(,2)\) around the minimum distance estimate \(=*{argmin}_{}_{1}^{c}(,)\). However, obtaining \(\) is an expensive preprocessing step, and we are unaware of any efficient algorithms for such MDE in the finite-sample setting. In Supplement D, we explore recentering WDRO around a tractable estimate obtained from iterative filtering , but find the resulting risk to be highly suboptimal. Furthermore, the improvements to our risk bounds under low-dimensional structure, which are derived in Section 4, do not extend to decisions obtained from these alternative procedures.

We now show that Theorem 1 cannot be improved in general. In particular, the first bound is minimax optimal over Lipschitz loss families when \(_{}\).

**Proposition 3** (Lower bound).: _Fix \(=^{d}\) and \([0,0.49]\). For any \(L 0\), there exists a family \(_{L}(^{d})\), independent of \(\), such that for any decision rule \(:()\) there exists a pair \((,)_{}()\) with \(_{1}^{e}(,)\) satisfying \(_{}[()]-_{}_{ }[] L+\,\)._

Each family \(\) encodes a multivariate regression problem. Our proof combines a one-dimensional lower bound of  for linear regression with lower bounds of  for robust estimation under \(_{1}\).

### Statistical Guarantees

We next formalize a finite-sample model and adapt our excess risk bounds to it.

**Setting B:** Fix \(,,,\) as in Setting A, and let \(Z_{1},,Z_{n}\) be identically and independently distributed (i.i.d.) according to \(\), with empirical measure \(_{n}=_{i=1}^{n}_{Z_{i}}\). Upon observing these clean samples, Nature applies a \(_{p}\) perturbation of size \(_{0}\), producing \(\{Z^{}_{i}\}_{i=1}^{n}\) with empirical measure \(^{}_{n}\) such that \(_{p}(_{n},^{}_{n})_{0}\). Finally, Nature corrupts up to \( n\) samples to obtain \(\{_{i}\}_{i=1}^{n}\) with empirical measure \(_{n}\) such that \(\|_{n}-^{}\|_{}=_{i=1}^{n} {1}\{_{i} Z_{i}\}\). Equivalently, the final dataset satisfies \(_{p}^{}(_{n},_{n})_{0}\).7 The learner is now tasked with selecting \(\) given \(_{n}\).

The results from Section 3 apply whenever \(_{0}+_{p}(,_{n})\). In particular, we obtain the following corollary as an immediate consequence of Theorem 1 and Theorem 3.1 of .

**Corollary 3** (Finite-sample risk bounds).: _Under Setting B, fix \(\) minimizing (OR-WDRO) centered at \(=_{n}\) with \(_{0}+100\,[_{p}(,_{n})]\). Then the excess risk bounds of Theorem 1 hold with probability at least 0.99. If \(\{_{},_{}\}\), \(p=1\), and \(d 3\), or if \(=_{}\), \(p=2\), and \(d 5\), then \([_{p}(,_{n})]n^{-1/d}\)._

**Remark 2** (Smaller radius).: In the classic WDRO setting with \(_{0}==0\), the radius \(\) can be taken significantly smaller than \(n^{-1/d}\) if \(\) and \(\) are sufficiently well-behaved. For example,  proves that \(=(n^{-1/2})\) gives meaningful risk bounds when \(\) satisfies a \(T_{2}\) transportation inequality.8 While this high-level condition may be hard to verify in practice, Supplement E shows that this improvement can be lifted to an instance of our outlier-robust WDRO problem.

### Tractable Reformulations and Computation

For computation, we restrict to \(_{}\). Initially, (OR-WDRO) may appear intractable, since \(_{}\) is non-convex when viewed as a subset of the cone \(_{+}()\). Moreover, enforcing membership to this class is non-trivial. To remedy these issues, we use a cheap preprocessing step to obtain a robust estimate \(z_{0}\) of the mean \(_{}[Z]\), and we optimize over the modified class \(_{2}(,z_{0})(): _{}[\|Z-z_{0}\|^{2}]^{2}}\), with \(\|z_{0}-_{}[Z]\|+\) taken so that \(_{2}(,z_{0})\). Finally, for technical reasons, we switch to the one-sided robust distance \(_{p}^{}(\|)_{^{} (^{d}):^{}}_{p}( ^{},)\). Altogether, we arrive at the modified DRO problem

\[_{}_{_{2}(,z_{0}): _{p}^{}(_{n}\|)}_{}[],\] (4)

which, as stated next, admits risk bounds matching Corollary 1 up to empirical approximation error.

**Proposition 4** (Risk bound for modified problem).: _Consider Setting B with \(_{}\). Fix \(z_{0}\) such that \(\|z_{0}-_{}[Z]\| E=O(_{0}+)\), and suppose that \(_{p}(_{n},)\). Take \(\) minimizing (4) with \(=(_{0}+)(1-)^{-1/p}+_{p}(_{},)\) and \(=+E\). We then have_

\[_{}[]-_{}[_{}] \|_{}\|_{}_{0}++,&p=1,_{}\\ \|_{}\|_{^{1,2}()}_{0}+++ _{0}++^{2},&p=2,_{}.\]

Parameters \(,\) are taken so that \(_{2}(,z_{0})\) and \(_{p}^{}(_{n}\|)\). Noting this, the proof mirrors that of Theorem 1, using a \(_{p}\) resilience bound for \(_{2}(,z_{0})\). To ensure \(_{p}(_{n},)\) with decent probability, one should take \(\) to be an upper bound on \(_{}[_{p}(_{n},)]\). When \(p=2\), this quantity is only finite if \(\) is bounded or if \(\) encodes stronger tail bounds than \(_{}\) (see, e.g., ).

For efficient computation, we must specify a robust mean estimation algorithm to obtain \(z_{0}\) and a procedure for solving (4). The former is achieved by taking a coordinate-wise trimmed mean.

**Proposition 5** (Coarse robust mean estimation).: _Consider Setting \(B\) with \(_{}\) and \( 1/3\). For \(n=((d))\), there is a trimmed mean procedure, which applied coordinate-wise to \(\{_{i}\}_{i=1}^{n}\), returns \(z_{0}^{d}\) with \(\|z_{0}-_{}[Z]\|+_{0}\) with probability at least \(0.99\), in time \((d)\)._More sophisticated methods, e.g., iterative filtering , achieve dimension-free estimation guarantees at the cost of additional sample and computational complexity. We will return to these techniques in Section 4, but overlook them for now since they do not impact worst-case excess risk bounds.

We next show that that the inner maximization problem of (4) can be simplified to a minimization problem involving only two scalars provided the following assumption holds.

**Assumption 1** (Slater condition I).: Given the distribution \(_{n}\) and the fixed point \(z_{0}\), there exists \(_{0}()\) such that \(_{p}^{}(_{n}\|_{0})<\) and \(_{_{0}}[\|Z-z_{0}\|^{2}]<^{2}\). Additionally, we require \(>0\).

Notice that Assumption 1 indeed holds for \(_{0}=\) as applied in Proposition 4.

**Proposition 6** (Strong duality).: _Under Assumption 1, for any \(\) and \(z_{0}^{d}\), we have_

\[_{_{2}(,z_{0}):\\ _{p}^{}(_{n}\|)} _{}[]=_{_{1},_{2} _{+}\\ }_{1}^{2}+_{2}^{p}+ +\,_{_{n}}[\,(\,;_{1},_{2},)],\] (5)

_where \((z;_{1},_{2},)_{} \,\,()-_{1}\|-z_{0}\|^{2}-_{2}\|-z\|^{p}- _{+}\)._

The minimization problem over \((_{1},_{2},)\) is an instance of stochastic convex optimization, where the expectation of the implicit function \(\) is taken w.r.t. the contaminated empirical measure \(_{n}\). In contrast, the dual reformulation for classical WDRO only involves \(_{2}\) and takes the expectation of the implicit function \((z;_{2})_{}()- _{2}\|-z\|^{p}\) w.r.t. \(_{n}\). The additional \(_{1}\) variable above is introduced to account for the clean family \(_{2}(,z_{0})\), and the use of partial transportation under \(_{p}^{}\) results in the introduction of the operator \([]_{+}\) and the decision variable \(\).

**Remark 3** (Connection to conditional value at risk (CVaR)).: The CVaR of a Borel measurable loss function \(\) acting on a random vector \(Z()\) with risk level \((0,1)\) is defined as

\[_{1-,}[(Z)]=_{}+ \,_{Z}\,[(Z)-]_{+} .\]

CVaR is also known as expected shortfall and is equivalent to the conditional expectation of \((Z)\), given that it is above an \(\) threshold. This concept is often used in finance to evaluate the market risk of a portfolio. With this definition, the result of Proposition 6 can be written as

\[_{_{2}(,z_{0}):\\ _{p}^{}(_{n}\|)} _{}[]=_{_{1},_{2}_{+}} _{1}^{2}+_{2}^{p}+_{1-,_{n }}_{}\,()-_{1}\|-z_{0}\|^{2}- _{2}\|-Z\|^{p}.\]

When \( 0\) and \(\), whence CVaR reduces to expected value and the constrained class \(_{2}(,z_{0})\) expands to \(()\), the dual formulation above reduces to that of classical WDRO .

Evaluating \(\) requires solving a maximization problem, which could be in itself challenging. To overcome this, we impose additional convexity assumptions, which are standard for WDRO .

**Assumption 2** (Convexity condition).: The loss \(\) is a pointwise maximum of finitely many concave functions, i.e., \(()=_{j[J]}_{j}()\), for some \(J\), where \(_{j}\) is real-valued, l.s.c., and concave9. The set \(\) is closed and convex. The atoms of \(_{n}\) are in the relative interior of \(\).

**Theorem 2** (Convex reformulation).: _Under Assumption 1, for any \(\) satisfying Assumption 2 and \(z_{0}^{d}\), we have_

\[_{_{q}(,z_{0}):\\ _{p}^{}(_{n}\|)} _{}[]=\{&_{1}^{2}+ _{2}^{p}++_{i[n]}s_{i}\\ &\!\!,_{1},_{2}\!\!_ {+},s,_{ij}\!\!_{n}^{},_{ij}^{},_{ij }^{},_{ij}^{},_{ij}^{}\!\!^{d },\, i[n], j[J]\\ &s_{i}(-_{j})^{*}(_{ij}^{})+z_{0}^{}_{ij}^{ }+_{ij}\\ &+_{i}^{}_{ij}^{}+P_{h}(_{ij}^{ },_{2})+_{}^{}(_{ij}^{})- ,& i[n], j[J]\\ &_{ij}^{}+_{ij}^{}+_{ij}^{}+_{ij}^{ }=0,\;\|_{ij}^{}\|^{2}_{1}_{ij},&  i[n], j[J],.\]

_where \(P_{h}\) is the perspective function (i.e., \(P_{h}(,)= h(/)\)) of_\[h()_{\{z^{d}:\|z\| 1\}}(),&p=1 \\ }{p^{p}}\|\|^{},&p>1.\] (6)

The minimization problem in Theorem 2 is a finite-dimensional convex program. In Section 5, we use this result in conjunction with Proposition 5 to efficiently perform outlier-robust WDRO.

We conclude this section by characterizing the worst-case distribution, i.e., the optimal adversarial strategy, for our outlier-robust WDRO problem. To that end, we need the primal formulation below.

**Theorem 3** (Worst-case distribution).: _Under Assumption 1, for any \(\) satisfying Assumption 2 and \(z_{0}^{d}\), we have_

\[_{_{d}(_{z_{0}}):\\ W_{p}^{}(_{n}\|)}_{}[]= \{&-_{(i,j)[n][J]}P_{-_{j}}(_{ ij},q_{ij})\\ &q_{ij}_{+},\,_{ij} q_{ij}&  i[n], j[J]\\ &_{j[J]}q_{ij}& i[n]\\ &_{(i,j)[n][J]}q_{ij}=1\\ &_{(i,j)[n][J]}P_{\|\|^{p}}(_{ij}-q_{ij}_{i},q_{ ij})\\ &_{(i,j)[n][J]}P_{\|\|^{2}}(_{ij}-q_{ij}z_{0},q_{ij}) ^{2}.\]

_The discrete distribution \(^{}=_{(i,j)}q_{ij}^{}_{_{ij}^{} /q_{ij}}\) achieves the worst-case expectation on the left-hand side, where \((q_{ij}^{},_{ij}^{})_{(i,j)[n][J]}\) are optimizers of the maximization problem on the right and \(:=\{(i,j)[n][J]:q_{ij}^{}>0\}\)._

The maximization problem from Theorem 3 is the conjugate dual of the minimization in Theorem 2. Subsequently, we propose a systematic approach for constructing a discrete distribution based on a solution derived from the maximization problem that achieves the worst-case expected loss.

**Remark 4** (Comparison to WDRO worst-case distribution).: Recall that our robust WDRO approach reduces to the classic WDRO approach as \(=0\) and \(\). Consequently, this implies that the constraints \(_{j[J]}q_{ij} 1/(n(1-))\) and \(_{(i,j)[n][J]}P_{\|\|^{2}}(_{ij}-q_{ij}z_{0},q_{ij}) ^{2}\) can be dropped under this specific choice of \(\) and \(\). As a result, our construction simplifies to the approach presented in [35, Theorem 4.4] for WDRO problems.

**Remark 5** (Parameter tuning).: In practice, \(\), \(_{0}\), and the relevant tail bound may be unknown. Thus, in Appendix F, we consider learning under Setting B with \(=_{}()\) for potentially unknown \(\), \(\), and \(_{0}\). First, we observe that knowledge of upper bounds on these parameters is sufficient to attain risk bounds scaling in terms of said upper bounds. This approach avoids meticulous parameter tuning but may result in suboptimal risk. To efficiently match our risk bounds with known parameters, we show that it is necessary and sufficient to know \(_{0}\) and at least one of \(\) or \(\) (up to constant factors).

## 4 Low-Dimensional Features

While Proposition 3 shows that the excess risk bounds from Theorem 1 cannot be improved in general, finer guarantees can be derived when the optimal loss function depends only on \(k\)-dimensional affine features of the data. Defining \(^{(k)}\) as the union of the projections \(\{U_{\#}:\}\) over \(U^{k d}\) with \(UU^{}=I_{k}\)10, we improve the excess risk bound of Theorem 1 for this setting.

**Theorem 4** (Excess risk bound).: _Under Setting A, let \(\) minimize (OR-WDRO), and assume that \(_{}= A\) for an affine map \(A:^{d}^{k}\) and some \(:^{k}\). Writing \(c\!=\!2(1\!-\!)^{-1/p}\), we have_

\[_{}[\,]\!-\!_{}[_{}]\!\{ \|_{}\|_{}c+2_{1}( ^{(k)},2),&p\!=\!1,_{}\\ \|_{}\|_{^{1.2}()}c\!+\!2_{2}(^{(k )},2)\!+\!c\!+\!2_{2}(^{(k)},2)^{2},&p\!=\!2,_{}..\]

This dependence on \(^{(k)}\) rather than \(=^{(d)}\) is a substantial improvement when \(k d\).

**Corollary 4** (Risk bounds for \(_{}\)).: _Under the setting of Theorem 4 with \(_{}\), we have_

\[_{}[]-_{}[_{}] \|_{}\|_{}+ \,,&p=1,_{}\\ \|_{}\|_{^{1,2}()}(+\,)+(^ {2}+k),&p=2,_{}.\]

We again have a matching lower bound for the Lipschitz setting, this time using \(k\)-variate regression.

**Proposition 7** (Lower bound).: _Fix \(=^{d}\) and \([0,0.49]\). For any \(L 0\), there exists a family \(_{L}(^{d})\), independent of \(\), such that each \(\) decomposes as \(= A\) for \(A^{k d}\) and \(:^{k}\), and such that for any decision rule \(:()\) there exists a pair \((,)_{}()\) with \(_{1}^{}(,)\) satisfying \(_{}[()]-_{}_{}[] L+\,\)._

For computation, we turn to a slightly modified \(n\)-sample contamination model. Our analysis for the low-dimensional case only supports additive TV corruptions (sometimes called Huber contamination).

**Setting \(^{}\):** Fix \(,,,\) as in Setting A, and fix \(m=(1-)n\) for some \(n\). Let \(Z_{1},,Z_{m}\) be drawn i.i.d. from \(\), with empirical measure \(_{m}=_{i=1}^{m}_{Z_{i}}\). Upon observing these clean samples, Nature applies a \(_{p}\) perturbation of size \(_{0}\), producing \(\{Z_{i}^{}\}_{i=1}^{m}\) with empirical measure \(_{m}^{}\) such that \(_{p}(_{m},_{m}^{})_{0}\). Finally, Nature adds \( n\) samples to obtain \(\{_{i}\}_{i=1}^{n}\) with empirical measure \(_{n}\) such that \(_{m}^{}_{n}\). Equivalently, the final dataset satisfies \(_{p}^{}(_{n}\|_{m})_{0}\).

As before, we modify (OR-WDRO) using a centered alternative to \(_{}\). Defining \(_{}(,z_{0})( ):_{}[(Z-z_{0})(Z-z_{0})^{}]^{2}I_{d} }\), we consider the outlier-robust WDRO problem

\[_{}_{_{}(,z_{0}) :_{p}^{}(_{m}\|)}_{}[ ].\] (7)

To start, we provide a corresponding risk bound which matches Corollary 4 when \(k=O(1)\).

**Proposition 8** (Risk bound for modified problem).: _Consider Setting \(^{}\) with \(_{}\), and assume \(_{}= A\) for affine \(A:^{d}^{k}\) and \(:^{k}\). Fix \(z_{0}\) such that \(\|z_{0}-_{}[Z]\| E=O(_{0}+1)\), and assume \(_{p}(_{m},)\). If \(\) minimizes (7) with \(=_{0}+\) and \(=1+E\), then_

\[_{}[]-_{}[_{}] \|_{}\|_{}_{0}++\,,&p=1,_{}\\ \|_{}\|_{^{1,2}()}_{0}+ ++_{0}++ ^{2},&p=2,_{}.\]

Here, the stronger requirement for the robust mean estimate, the restriction to additive contamination, and the need to optimize over the centered \(_{}\) class rather than \(_{2}\) all stem from the fact that the resilience term \(_{p}((_{})_{k},)\) scales with \(\) rather than \(\). Fortunately, efficient computation is still possible. First, we employ iterative filtering  for dimension-free robust mean estimation.

**Proposition 9** (Refined robust mean estimation).: _Consider Setting \(\) or \(^{}\) with \(=_{}\) and \( 1/12\). For \(n=(d)\), there exists an iterative filtering algorithm which takes \(_{n}\) as input, runs in time \((nd^{2})\), and outputs \(z_{0}^{d}\) such that \(\|z_{0}-_{}[Z]\|_{0}+1\) with probability at least 0.99._

The analysis requires care when \(p=1\), since \(_{1}\) perturbations can arbitrarily increase the initial covariance bound. Fortunately, this increase can be controlled by trimming out a few samples.

Next, we show that computing the inner worst-case expectation in (7) can be simplified into a minimization problem involving only a scalar and a positive semidefinite matrix provided the following assumption holds (which is indeed the case in the setting of Proposition 8).

**Assumption 3** (Slater condition II).: Given the distribution \(_{n}\) and fixed point \(z_{0}\), there exists \(_{0}()\) such that \(_{p}^{}(_{n}\|_{0})<\) and \(_{_{0}}[(Z-z_{0})(Z-z_{0})^{}]\!^{2}I_{d}\). Further, we require \(\!>\!0\).

**Proposition 10** (Strong duality).: _Under Assumption 3, for any \(\) and \(z_{0}^{d}\), we have_

\[_{_{}(,z_{0}):\\ _{p}^{}(_{n}\|)}_{}[]=_{_{1}_{+}^{d}\\ _{2}_{+},}-z_{0}^{} _{1}z_{0}\!+\!^{2}[_{1}]\!+\!_{2} ^{p}\!+\!\!+\!\,_{_{n}} (\,\,;_{1},_{2},),\]

_where \((z;_{1},_{2},)_{} [()-^{}_{1}+2^{}_{1}z_{0}-_ {2}\|-z\|^{p}-]_{+}\)._

The minimization problem over the variables \((_{1},_{2},)\) belongs to the class of stochastic convex optimization problems. As before, we show that under the convexity condition from Assumption 2 we obtain a tractable reformulation that does not involve an extra optimization problem for evaluating \(\).

[MISSING_PAGE_EMPTY:10]