# Improving Deep Learning Optimization through Constrained Parameter Regularization

Jorg K.H. Franke

University of Freiburg, Germany

&Michael Hefenbrock

RevoAI, Karlsruhe, Germany

&Gregor Koehler

German Cancer Research Center (DKFZ)

Heidelberg, Germany

&Frank Hutter

ELLIS Institute Tubingen, Germany

University of Freiburg, Germany

###### Abstract

Regularization is a critical component in deep learning. The most commonly used approach, weight decay, applies a constant penalty coefficient uniformly across all parameters. This may be overly restrictive for some parameters, while insufficient for others. To address this, we present Constrained Parameter Regularization (CPR) as an alternative to traditional weight decay. Unlike the uniform application of a single penalty, CPR enforces an upper bound on a statistical measure, such as the L\({}_{2}\)-norm, of individual parameter matrices. Consequently, learning becomes a constraint optimization problem, which we tackle using an adaptation of the augmented Lagrangian method. CPR introduces only a minor runtime overhead and only requires setting an upper bound. We propose simple yet efficient mechanisms for initializing this bound, making CPR rely on no hyperparameter or one, akin to weight decay. Our empirical studies on computer vision and language modeling tasks demonstrate CPR's effectiveness. The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning.

## 1 Introduction

Deep neural networks are the bedrock of many state-of-the-art machine learning applications . While these models have exhibited unparalleled expressivity, they also possess millions, sometimes trillions, of parameters . This massive capacity makes them susceptible to overfitting, where models memorize nuances of the training data but underperform on unseen examples. To mitigate this, many different regularization techniques have been adopted, with weight decay and L\({}_{2}\) regularization [3; 4; 5] being the most popular. L\({}_{2}\) regularization penalizes the squared magnitude of model parameters and (decoupled) weight decay (which is equivalent to L\({}_{2}\) regularization for non-adaptive gradient algorithms ) multiplies all weights with a constant at every step. This seemingly simple act offers numerous benefits by curbing the growth of individual weights, reducing the risk of relying on any particular feature excessively, and thus promoting model generalization.

Figure 1: GPT2s training using Adam with weight decay or CPR (Kappa-IP). AdamCPR outperforms AdamW with the same budget and only requires 2/3 of the budget to reach the same score.

However, not all parameters in a neural network have the same role or importance and different weights could benefit from different regularization. Similarly, it is unclear if a single weight decay value is optimal for the entire duration of optimization, especially for large-scale training. Indeed, Ishii and Sato  showed that a small deep learning model could benefit from layer-wise weight decay values, and various works showed that scheduling weight decay could improve final performance [8; 9; 10; 11]. This indicates that a dynamic penalty for each individual parameter matrix (e.g., a weight matrix in a linear layer) could be beneficial for neural network training. Since both scheduling and parameter-wise weight decay require additional hyperparameters that are often sensitive to the task, we propose a different approach to obtain customized, dynamic parameter regularization. Instead of uniformly penalizing weights, we propose to keep them in a certain range, thus ensuring stability without imposing regularization where it is unnecessary. Constraining parameters, especially based on statistical measures like the L\({}_{2}\) norm, provide a flexible and adaptive form of regularization that accounts for the heterogeneity of parameters.

In this paper, we propose _Constrained Parameter Regularization (CPR)_, which enforces an upper bound on a statistical measure of individual parameter matrices. Consequently, regularization is expressed as a constrained optimization problem, which we address by an adaptation of the augmented Lagrangian method. The regularization of each parameter matrix is handled by a separate constraint and Lagrange multiplier, resulting in an individual regularization strength that adapts over time. The method requires the selection of desired constraint values as well as an update rate for the Lagrange multipliers. We found that the update rate can be fixed to 1.0. For choosing constraint values, we introduce four strategies, three of which require a single hyperparameter, while the last one is hyperparameter-free. We show in our experiments performance improvements over weight decay when pre-training or finetuning models for image classification (CIFAR100 and ImageNet), language modeling (OpenWebText), and medical image segmentation. For example, when training a GPT2s model, we achieved the same performance as AdamW but only require \(2/3\) of the budget, see Figure 1. Applying our method for fine-tuning, we find performance improvements and less catastrophic forgetting. In the following, and after discussing related work (Section 2) and background on weight decay and the augmented Lagrangian method (Section 3), we make the following contributions:

* Introducing CPR for individualized and dynamic weight regularization1. Specifically, formulating regularization as a constraint optimization problem and proposing CPR as a solution (Section 4.1). * Identifying four different strategies for initializing this constraint (Section 4.3). One of them, Kappa-WS, has a strong default that outperforms tuned AdamW; and another one, Kappa-IP, is entirely hyperparameter-free and yields even better performance in pre-training.
* Showing improved performance over weight decay in image classification, medical image segmentation, and pretraining and fine-tuning language models (Section 5).

## 2 Related Work

Weight decay is an effective regularization technique to improve the generalization and model performance , and the idea of adapting parameter regularization during training is not new. Lewkowycz and Gur-Ari  investigated the effect of L\({}_{2}\) regularization on overparameterized networks and found the time it takes the network to reach peak performance is proportional to the L\({}_{2}\) regularization parameter. They proposed an initialization scheme for L\({}_{2}\) regularization and an annealing schedule for the L\({}_{2}\) parameter. Yun et al.  use a combination of weight decay scheduling and knowledge distillation to improve performance on computer vision tasks. More recent works on self-supervised vision transformers also use a weight decay schedule [10; 11]. In contrast to our work, none of these proposes a dynamic and individual adaptation of each regularized parameter matrix. Also, a schedule comes with varying hyperparameter choices while CPR adapts arbitrarily many parameter matrices with only two hyperparameters (out of which one is fixed in all our experiments). Instead of using a schedule, Nakamura and Hong  proposes _AdaDecay_, where the \(L_{2}\) penalty is scaled by standardized gradient norms and a sigmoid function. Ghiasi et al.  propose another gradient-based approach, _Adaptive Weight Decay (AWD)_, which dynamically adjusts the weight decay based on the ratio of weight norms to gradient norms to balance the contributions from the cross-entropy and regularization losses aiming to improve the robustness. AMOS leverages model-specific information for initialization and gradients to adapt L2 regularization during the training. Another way to regularize parameters is to fix the norm of individual parameter matrices , to schedule the weight norm , or to limit the total norm of all parameters  to a fixed value. This fixed value is a more sensitive hyperparameter than the hyperparameter in our work.

Our proposed method is not the first to use Lagrangian methods in machine learning . Its application in deep learning so far focuses on variational methods and generative models: Rezende and Viola  introduced the _Generalized ELBO with Constrained Optimization_ algorithm to optimize VAEs using Lagrange multipliers optimized by the min-max scheme, and Kohl et al.  and Franke et al.  adapted the Lagrangian method from Rezende and Viola  to train probabilistic U-nets and probabilistic Transformer models. While these works adopt Lagrangian methods to handle several losses in joint optimization problems, our work leverages them to enable individual regularization strengths.

## 3 Background

### L\({}_{2}\) Regularization and Weight Decay

Regularization methods, such as L\({}_{2}\)-regularization or weight decay, are commonly used to restrict parameter updates and enhance generalization by reducing unnecessary complexity [3; 4; 5]. Both can be motivated by introducing a "cost" to weight magnitudes. Specifically, in L\({}_{2}\)-regularization, instead of minimizing only the loss function \(L(,,)\) with parameters \(\) and data \(=\{(_{n},_{n})\}_{n=0}^{N}\), a weighted penalty (regularization) term \(R()\) is added to the loss, resulting in the training objective

\[_{} L(,,)+ R(),\]

where \(R()=\|\|_{2}^{2}\) denotes the regularization function and \(^{+}\) the strength of the penalty. On the other hand, weight decay directly modifies the update rule of the parameters to

\[_{t+1}_{t}+(L,)- _{t},\]

where \((L,)\) denotes an optimizer providing the gradient-based update at iteration \(t\) and \(L=L(_{t},_{t},_{t})\) the loss. For example, \((L,)=-L(_{t},_{t},_{t})\) with learning rate \(^{+}\) in case of gradient descent. Thus, the main difference between weight decay and L\({}_{2}\)-regularization is that the gradients of the regularization accumulate in momentum terms in the case of L\({}_{2}\)-regularisation, while they are treated separately in (decoupled) weight decay. This has also been extensively discussed by Loshchilov and Hutter  with the introduction of the AdamW optimizer.

### The augmented Lagrangian method

We briefly review the augmented Lagrangian method for constrained optimization, see e.g. Bertsekas , which our method is based on. For the derivation, we follow the motivation of Nocedal and Wright [24, pp. 523-524]. Consider the following inequality-constrained optimization problem

\[*{minimize}_{}f() c()  0,\]

with \(f():^{n}\) and a constraint \(c():^{n}\). One way to address the constraint is to find an equivalent, unconstrained problem with the same optimal solution. For example,

\[*{minimize}_{}F() F()= _{ 0}\;f()+ c().\] (1)

Unfortunately, even if \(f()\) and \(c()\) are differentiable, \(F()\) is not differentiable. This is due to the maximization over \(\) in \(F()\), where in case of \(c()>0\), \(F()\). Consequently, we cannot run gradient-based optimization on this objective.

To alleviate this problem, we consider a smooth approximation of \(F()\), namely

\[(,_{t},)=_{ 0}\;f()+ c ()-(-_{t})^{2},\] (2)

where \(_{t}\) may be seen as a point we wish to remain proximal to and \(^{+}\) as a factor determining the strength with which this proximity is enforced. For \(\), \((,_{t},) F()\).

The maximization in \((,_{t},)\) has a closed form solution with \(^{}=(_{t}+ c())^{+}\), where \(()^{+}=\{0,\}\), see Appendix A for the derivation. Consequently, \((,_{t},)\) can be written as

\[(,_{t},)=f()+h(,_{t},)\]

with

\[h(,_{t},)=c()(_{t}+c( )),&_{t}+ c() 0\\ -_{t}^{2}&.\]

The constraint thus only interferes with the minimization (gradient) of \(f()\) if \(_{t}+ c() 0\). We can now try to solve the unconstrained problem \(}{}\ (,_{t},)\) with familiar methods, such as gradient descent, and obtain an approximate solution to the original problem. Specifically, the gradient of \((,_{t},)\) with respect to \(\) is given by

\[_{}(,_{t},)=_{}f()+ ^{}_{}c().\]

The quality of the approximation, and thus the solution, clearly depends on \(\) and \(_{t}\). To improve this approximation we can refine the choice of \(_{t}\) via an iterative procedure and repeat the optimization with \(_{t+1}^{}=(_{t}+ c())^{+}\). Intuitively, if the previous minimization of \((,_{t},)\) resulted in an infeasible solution with \(c()>0\), \(_{t+1}>_{t}\). Hence, the minimization of \((,_{t+1},)\) likely results in a solution with less constraint violation. On the other hand, if \(c() 0\), \(_{t+1}_{t}\). Subsequently, the influence of the constraint is decreased. This loop of alternating minimization of \((,_{t},)\) and updating \(_{t}\) can be repeated until a sufficiently good solution is found or the procedure converges if \(_{t}\) does not receive updates anymore. For multiple constraints \(c_{j}(),\ j=1,,J\), the above can be readily extended with a multiplier \(_{t}^{j}\) for each constraint. Since the maximization in the smooth approximation is separable in the \(_{t}^{j}\), the same update rule may be applied for each \(_{t}^{j}\) separately using the respective constraint \(c_{j}()\).

## 4 Constrained Parameter Regularization

In this section, we introduce Constrained Parameter Regularization (CPR), where we adapt the augmented Lagrangian method to enforce upper bounds on regularization terms. Compared to classical regularization, with a fixed regularization coefficient \(\), the proposed approach will allow for variable regularization coefficients \(_{t}^{j}\) (Lagrange multipliers) for \(j=1,,J\) parameter matrices \(^{j}\) that should be regularized. These regularization coefficients are updated alongside the network parameters \(\).

### Regularization through constraints

Classical weight decay, as introduced earlier, is used as a means to restrict the freedom of parameter adaptation. This restriction is applied with a scaling factor \(\) (hyperparameter) and applies uniformly to all parameters. However, we conjecture that applying an individual adaptation pressure instead may be beneficial. Unfortunately, this would require a separate coefficient for each parameter matrix where a separate weight decay should be applied. To avoid the need for separate scaling coefficients, we formulate regularization as a constrained problem. Here, the loss function \(L(,,)\), with network parameters \(\), takes the place of the objective. Consequently, the learning problem becomes

\[}{}\ L(,,)  c_{j}^{j}=R^{j} -^{j} 0, j=1,,J,\]

where \(R(^{j})\) is a regularization function (e.g., the squared L\({}_{2}\)-norm in case of weight decay) for a parameter matrix \(^{j},j=1,,J\), and \(^{j}\) denotes a chosen bound.

To solve equation 3, we follow the augmented Lagrangian method with slight modifications. First, instead of performing a full optimization of the loss before updating \(_{t}\), we perform updates in every step. This is motivated by the fact that full optimization is generally infeasible in a deep learning setting. Moreover, similar to the difference between weight decay and L\({}_{2}\)-regularization, we treat the update between the loss-dependent and the constraint-dependent part separately. Hence, instead of introducing \((,_{t},)\) analogously to equation 2, and performing optimization on this objective, we independently apply updates for both steps. Consequently, the constraint violations do not accumulate in momentum terms. We also remove the influence of the learning rate on the regularization. From a practical perspective, our modification does not interfere with gradient-based optimization algorithms and can be readily combined with any such optimizer. The full algorithm is given by Algorithm 1.

```
0: Loss Function \(L(,,)\) with parameters \(\), and data \(=\{(_{n},_{n})\}_{n=0}^{N}\)
0: Hyperparameters: Learning rate \(^{+}\), Lagrange multiplier update rate \(^{+}(=1.0)\)
0: Optimizer \(()\) for minimization, Regularization function \(R()\) (e.g. L2-norm)
1:\(_{t}^{j} 0\) for \(j=1,,J\)
2:\(^{j}(_{0}^{j})\) for \(j=1,,J\)\(\) Initializing the upper bound \(\), see Section 4.3
3:for\(_{t},_{t}\)do
4:\(_{t+1}_{t}+(L(_{t}, _{t},_{t}),)\)\(\) Classic parameter update using, e.g., Adam.
5:for each regularized parameter group \(_{t}^{j}\) in \(_{t}\)do
6:\(_{t+1}^{j}(_{t}^{j}+(R(_{t}^{ j})-^{j}))^{+}\)
7:\(_{t+1}^{j}_{t+1}^{j}-_{^{j}}R( _{t}^{j})_{t+1}^{j}\)
8:endfor
9:endfor ```

**Algorithm 1** Optimization with constrained parameter regularization (CPR).

Conceptually, the method can be understood as the \(_{t}^{j}\) accumulating constraint function values (weighted with \(\)) over the iterations \(t\). These then increase (or decrease) the influence of the constraint (via its gradient) on the search direction. When points in the feasible domain are found for which \(c_{j}() 0\), \(_{t}^{j}\) decreases until it eventually reaches \(0\). If, on the other hand, the optimal solution lies on the boundary, where \(c_{j}()=0\), \(_{t}^{j}\) should converge to a value where the update direction of the optimizer and the gradient of the constraints cancel each other. However, this situation is unlikely to occur in a deep learning setting due to the stochasticity of minibatches.

### How is CPR different from weight decay?

The optimality conditions of the CPR problem and an L\({}_{2}\)-regularized training objective reveal a connection between the two approaches. To see this, consider the training objective of L\({}_{2}\) regularization with a given \(\), assuming it has a minimum at \(^{}\). Consequently, at this point, we have \(0= L(^{})+ R(^{})\), and the value of the regularization function is \(R(^{})\).

If we set \(^{}=R(^{})\), the Karush-Kuhn-Tucker (KKT) (optimality) conditions for CPR are \(0= L(^{})+ R(^{})\) and \(R(^{})-^{} 0\) (which holds with equality), with the Lagrange multiplier \( 0\). We can see that for \(^{}=\), the solution pair \((^{},^{})\) satisfies the KKT conditions. Hence, there is a choice of \(\) (namely \(^{}\)) for which the CPR problem has the same optimal solution candidates as the L\({}_{2}\)-regularized training objective for a given \(\). CPR could therefore be seen as a different approach to searching for the same solution candidates but is parameterized with different hyperparameters (\(\) instead of \(\)). Unlike L\({}_{2}\)-regularization (or weight decay), CPR can mimic the behavior of different \(\) values for different parameter matrices. This behavior changes over time as the \(^{j}\) values are updated and thus leads to different training dynamics compared to weight decay. Additionally, focusing on a bound on the regularization function \(\) instead of a penalty coefficient \(\) may allow us to identify better indicators for the selection of (default) values for these hyperparameters.

### Initialization of Upper Bounds \(^{j}\)

The upper bound \(\) is the most crucial hyperparameter for CPR, and we identify four ways to initialize it. (1) Kappa-K: Set \(^{j}\) to the same value \(\) for all parameter matrices. (2) Kappa-kI\({}_{0}\): Set \(^{j}\) based on the initial parameter matrices' regularization function value: \(^{j} k R(_{t=0}^{j})\), with \(k^{+}\) as the factor of the initial measure. (3) Kappa-WS: Train the model parameters \(\) for a specific number of warm start (WS) steps \(s^{+}\) and then set \(^{j} R(_{t=s}^{j})\). (see algorithm for CPR with Kappa-WS in B). While the previous strategies all require a hyperparameter, our last strategy is essentially hyperparameter-free. (4) Kappa-IP: Use the first inflection point (IP) of the regularization function at step \(i\) (change of curvature over the training steps) to warm start each parameter matrix individually. Specifically, \(^{j} R(_{t=i}^{j})\) where \(i\) is the first iteration where \(_{t}_{t}R(^{j})<0\). The intuition behind this choice comes from the fact that the rate of change decreases at the inflection point. This hints at a saturation of the improvement expected through raising the value of the regularization function further. The position of the inflection point thus indicates a good choice for \(\), as it demonstrated healthy training dynamics while still restricting the model from over-adapting (see Section 5). Consequently, this method leverages the natural progression of the model's training rather than relying on an external hyperparameter, aiming to adaptively find a suitable upper bound.

## 5 Experiments

We now describe a set of experiments to understand CPR and its parametrization. Preliminary experiments showed that \(\) is not a sensitive hyperparameter and we chose \(=1.0\) for all our experiments. We provide a detailed analysis of \(\) in C. Similar to weight decay, we choose the squared L\({}_{2}\) norm as a default regularization function for CPR. We also tested an adaptive bound, where we adjusted kappa during training but found it not to be beneficial; details are reported in Appendix D. In the following experiments, we regularize all parameters in a network except for bias terms and normalization weights. Since CPR does not require additional gradient calculations or parameter updates, we find only a small runtime overhead with our CPR implementation (in PyTorch, no CUDA optimization, 0.4%-5.8% for GPT2) which is mentioned in each experiment individually and analyzed in Appendix I.

### Train an Image Classification Model (CIFAR100)

To evaluate CPR's effectiveness and design choices, we tested AdamW and Adam with CPR (Adam-CPR) in image classification using a ResNet18 on the CIFAR100 dataset [25; 26]. We compared AdamW to AdamCPR with the four initializations described in Section 4.3. The initialization Kappa-WS after \(s\) warm steps performed best, see Figure 2. We base our choice of the warm start on the \(500\) steps learning rate warmup out of \(20k\) total training steps and found a large range of hyperparameters that consistently outperform weight decay. Also, the hyperparameter-free method Kappa-IP outperforms weight decay. To detect the infection point, we found it sufficient to sweep the statistical measure in an interval of \(10\%\) of the learning rate warmup. We also apply this in all further experiments. The superior performance of Kappa-WS and Kappa-IP may be due to its general flexibility, as warm-started bounds may be considered "learned," reflecting the actual magnitudes and distributions of the parameter matrices during training. Appendix E contains training details and a plot with all initializations and standard deviation across three random seeds in Figure E.1. ResNet18 training took 15-20 minutes on a consumer GPU, with no significant runtime difference between

Figure 2: Percentage of correct labels (\(\)) of a ResNet18 trained on CIFAR100 with AdamW and AdamCPR with Kappa-IP or Kappa-WS. We use a learning rate warm-up of \(500\) steps and the best Kappa-WS value is \(2\) the warm-up steps. We report the mean of three runs with random seeds. We see that both CPR versions outperform weight decay

AdamW and AdamCPR. We also tested the standard deviation as a choice for the regularization function, which performed well but not better than the squared L\({}_{2}\) norm (see Figure E.2).

To investigate the relationship between the learning rate warm-up and the number of warm start steps \(s\) of Kappa-WS or Kappa-IP, we experimented with varying warm-up steps. We found that setting the CPR warm start steps \(s\) to twice the warm-up steps is a good initial choice. For very low warm-up steps, the best \(s\) was four times the warm-up count. Conversely, with a long warm-up phase, a shorter CPR warm start (\( 1\)) is preferable. Notably, the optimal choice of \(s\) is almost independent of the learning rate, as shown in Figure E.3. The optimal warm start steps are consistent across a wide range of learning rates. A simple baseline representing a similar regularization approach is a weight decay schedule. We evaluated a cosine schedule for decreasing and increasing weight decay values, similar to [10; 11]. The results, shown in Figure E.4, indicate that the decreasing schedule outperforms a fixed weight decay but not CPR. We tested if CPR is particularly good for noisy data and perfomed experiments on the noisy CIFAR100-C dataset . The results, in Figure E.5, show that AdamCPR outperforms AdamW slightly. However none of the optimizer and hyperparameter configurations lead to an outstanding performance on this task, we wouldn't claim that CPR is particularly good for noisy data. We also used CPR with SGD. We found, as shown in Figure E.6, that SGD with CPR outperforms SGD with weight decay when using the Kappa-WS initialization. However, Kappa-IP seems not to work with SGD, probably due to the changed convergence behavior in contrast to Adam.

Additionally, we compared our method to related work. We implemented AdaDecay  and evaluated the method for different alpha values, as seen in Figure E.7. We also compared AdamW and AdamCPR to adaptive Weight Decay (AWD)  and AMOS . Furthermore, we used Adam with parameter rescaling from Liu et al. . We found AdaDecay superior to AdamW, while AMOS and Rescaling performed less well. However, CPR outperforms all related approaches. We report all results across multiple learning rates and weight decay values in Figure E.8.

### Train an Image Classification Model (ImageNet)

We compare AdamW and AdamCPR in vision transformer  training on ImageNet . We choose to train the DeiT  model with 22M (small) and with 86M (base) parameters. We make use of the PyTorch Image Models library  and train with the configuration given in  for 300 epochs. To explore the impact of weight decay, we also train with a 10\(\) and 0.1\(\) the weight decay value. For CPR, we initialize with Kappa-WS (\(\) lr-warmup) and Kappa-IP. We observed a minor runtime increase when using CPR. For example, training the small model on 4 A100 GPUs took 14.85h for AdamW and 14.89h for AdamCPR. All relevant hyperparameters can be found in Appendix F. As seen in Table 1, AdamCPR outperforms AdamW for small and base DeiT training with both kappa initialization methods. Most notably, the hyperparameter-free regularization with Kappa-IP outperforms AdamW in both cases. However, in the base model training, Kappa-WS surpasses Kappa-IP.

### Fine-tuning a CLIP model

We conducted fine-tuning experiments using a CLIP model  on the ImageNet dataset. We used the ViT-B/32 model pre-trained by Radford et al. . The model was fine-tuned for 10 epochs following the hyperparameter choices of Wortsman et al.  (learning rate of \(3 10^{-5}\), cosine-annealing learning rate schedule with 500 warm-up steps) but without the special classification head initialization and the training was performed on a single GPU with a batch size of 512. We compare

 
**ImageNet** &  &  \\ 
**Pretraining** &  &  & Kappa IP \\  & & & & &  & \\  & 0.005 & 0.051 & 0.5 & 1x & 2x & 4x & \\  DeiT-Small (22M) & Top-1 Acc. (\%) & 76.97 & 79.03 & 79.16 & **79.81** & 79.33 & 78.04 & **79.84** \\ DeiT-Base (86M) & Top-1 Acc. (\%) & 76.19 & 78.59 & 80.56 & **81.19** & 79.61 & TBA & **80.95** \\  

Table 1: Comparison of AdamW and AdamCPR in a DeiT  pertaining on ImageNet. We train a small (22M parameters) and a base model (86M) with different regularization parameters.

AdamW with different weight decay values to AdamCPR in different configurations, where we report the top-1 accuracy after finetuning. The results in Table 2 show that the Kappa-WS initialization also leads to better results in this finetuning setting, comparing favorably to traditional weight decay. CPR with Kappa-IS performs similarly to the best weight decay values, but again, without the need for finding a regularization hyperparameter.

### Pretraining a Large Language Model (OpenWebText)

We performed experiments training a GPT2 language model  on Openwebtext . We compared AdamW on different weight decay values to AdamCPR using Kappa-WS with different warm start steps and Kappa-IP. We use a learning rate warmup for 5k steps (\(2.5\%\) of total training steps) followed by cosine annealing. Again, we select the warm start steps of \(\) based on the warmup steps of the learning rate and evaluate \(s(5k,10,20k)\) steps. We train the model sizes GPT2s and GPT2m with 124M and 354M parameters for 200k steps. The results are shown in Figure 3. CPR outperforms weight decay at all learning rates, in both model sizes and with both kappa initialization strategies. We also see that the Kappa-IP initialized CPR runs are less sensitive to the learning rate than weight decay \(\). Remarkably, CPR with the hyperparameter-free initialization Kappa-IP performs best, achieving \(0.2\) to \(0.3\) better perplexity than weight decay. To illustrate the performance difference, we trained a model with weight decay for a longer schedule to get the same performance as with CPR, the result is shown in Figure 1. CPR saves up to \(33\%\) training budget on that scale. Figure 5 shows the difference in training dynamics with CPR. We find that Kappa-IP is close to the optimal warm start step for Kappa-WS but find individual starting points for different layers, see Figure G.1. We provide details of the training and hyperparameters in Appendix H. We found no runtime overhead of CPR in contrast to AdamW training GPT2s but about \(2.5\%\) for GPT2m (see runtime analysis in Appendix I). We also evaluated AdaDecay , Adaptive Weight Decay (AWD)  and AMOS  in the GPT2s training setting but neither of the related methods outperforms AdamW nor AdamCPR, see results in Table H.1.

    &  &  \\   &  &  & Kappa IP \\   & 0.0001 & 0.001 & 0.01 & 0.1 & 1.0 & 1x & 2x & 4x & \\  Top-1 Acc. (\%) & 75.24 & 75.39 & 75.32 & 75.17 & 74.4 & 75.27 & **75.52** & 75.41 & 75.40 \\   

Table 2: Comparison of AdamW and AdamCPR for CLIP finetuning on ImageNet. We report the top-1 accuracy and follow the hyperparameters and schedule from WiSE-FT .

Figure 3: Perplexity (\(\)) \(\) std across three random seeds of GPT2s and GPT2m trained on OpenWebText with AdamW (left) and AdamCPR with Kappa-IP (middle) and AdamCPR with Kappa-WS (right). We use a learning rate warm-up of \(5k\) steps. The CPR with the hyperparameter-free strategy Kappa-IP outperforms weight decay but also CPR with warm start.

### Fine-tuning a Large Language Model

Probably a more common task than pre-training a large language model (LLM) is to fine-tune one. Hence, we evaluate CPR in the fine-tuning of the Mistral7B large language model , incorporating low-rank adaptation (LoRA) . Specifically, we fine-tune artificially generated biomedical question-answering (QA) pairs from the PubMedQA dataset . We fine-tune all attention and feed-forward weights using either AdamW or AdamCPR with a learning rate warm-up of 50 steps, followed by cosine annealing. We experiment with different values of weight decay and warm start steps for Kappa-WS, set at \(1\), \(2\), and \(4\) the learning rate warm-up steps. The fine-tuning was performed on four GPUs for about 1h. Each configuration is trained across three random seeds. We evaluate the LLM before and after the fine-tuning on the expert-annotated _PubMedQA_ QA instances and report the change in answer accuracy (means and standard deviations across three random seeds) in Figure 4. The fine-tuning enhances the performance on the PubMedQA benchmark and CPR outperforms AdamW for each learning rate. As in both the ImageNet and GPT2 experiments, the best Kappa-WS value was \(2\) the warm-up steps (here, \(50 2\)). We also tested Kappa-IP but it performed worse due to the lack of an inflection point for some parameters, short learning rate warmup, and different training dynamics with LoRA. We also found that CPR helps to mitigate catastrophic forgetting, therefore we evaluate before and after finetuning on a set of benchmarks and found that CPR with some learning rates helps to reduce a performance drop e.g. on the _TruthfulQA_ benchmark, which evaluates models' abilities to mimic human falsehoods , on up to \(3\%\) (see results in Figure 1). Detailed hyperparameters and plots including standard deviations are available in Appendix K.

### Medical Image Segmentation

Aside from image classification, we also applied CPR to (medical) image segmentation using the nnU-Net framework  and training with the _SGD optimizer_ in combination with CPR with Kappa-WS. For this, we considered the tasks of Multi-Atlas Labeling Beyond the Cranial Vault (BTCV)  where we improve the Dice score from \(83.99\) to \(84.23\), the Heart Segmentation task of the Medical Segmentation Decathlon  where we improve the Dice score from \(92.92\) to \(93.18\) and the 2020 version of the Brain Tumor Segmentation challenge (BraTS) task  where we improve the Dice score from \(76.22\) to \(76.65\). These results show that CPR also works in combination with SGD where we replace weight decay. Training details for the task and all results are in Appendix J.

## 6 Discussion

Our extensive evaluation of Constrained Parameter Regularization (CPR) across multiple tasks underscores its effectiveness as a robust alternative to traditional weight decay. A critical aspect of CPR's success is its initialization strategy. To this end, we propose four strategies to initialize the upper bound \(\). With our findings, we identify two strategies, Kappa-WS and Kappa-IP as prime candidates showing a strong performance, consistent across multiple tasks. The good performance of the warm-started bound Kappa-WS can be attributed to the fact that even a carefully chosen initialization of parameters does not consider the training task and data. Therefore, the actual parameter weights during training are better reflected in a warm-started bound, which also takes into

Figure 4: Percentage of performance change before and after fineuning Mistral 7B with pubmedQA artificial data (\(\)) with the use of AdamW (left) and AdamCPR with Kappa-WS (right). We use a learning rate warm-up of \(50\) steps. We see that CPR outperforms weight decay for each learning rate.

account the network's depth and the varying gradient updates in deeper layers. We found that setting the CPR warm start steps \(s\) to twice the learning rate warm-up steps serves as an effective initial configuration for any training setup. However in a pre-training setting, setting the upper bound based on the first inflection point of the regularization function (Kappa-IP) yields an additional advantage: It removes even the one hyperparameter present in the warm start strategy, bringing the regularization capabilities of CPR without any additional hyperparameters. Simultaneously, this strategy shows best-in-class performance in GPT2 training, seemingly even extending the range of usable learning rates on a given task. This reduces the effort in hyperparameter optimization not only for the optimal regularization but also for the optimal learning rate. CPR also changes the training dynamics, as shown in Figure 5 and Figure G.1. While both weight decay and CPR can achieve a similar final L2 regularization, the path to this norm is different. Weight decay allows for intermediate overadaptation with high L2 norms, whereas CPR controls the L2 norm throughout the entire training process. This results in a slower initial loss drop but a more consistent decay, leading to a better final performance.

A noted limitation of CPR is an increase in runtime by up to 6% for larger models (1.1B parameters), as detailed in Appendix I. However, for smaller models or larger batch sizes, this overhead is negligible. The benefit of CPR diminishes in scenarios where weight regularization has minimal impact, such as when training small models on large datasets with a high ratio of training samples to parameters. Future research could explore the application of CPR to even larger models and a broader range of tasks.

## 7 Conclusion

Constrained Parameter Regularization (CPR) offers a significant advancement in regularization techniques, providing a robust and efficient alternative to traditional methods. By enforcing an upper bound on the regularization function, CPR integrates seamlessly with gradient-based optimizers and incurs minimal runtime overhead. Its dynamic tailoring of regularization to individual parameter matrices and reduces hyperparameter optimization by eliminating the need for a weight regularization hyperparameter in pre-training. Our four experiments demonstrate that neural networks trained using CPR outperform those with traditional weight decay. These findings highlight CPR's potential as a versatile and powerful tool for improving model performance and open promising future research.

Figure 5: The training dynamics of AdamW (blue) and AdamCPR with Kappa-IP (green) in a GPT2s training run. The upper plot shows the squared L2 norm of the first fully connected weight in the fifth layer. Below we see the gradient of the squared L2 norm regarding the training steps. After the inflection point (7400), Kappa-IP initializes kappa \(^{j} R(^{j}_{t=i})\) and starts the regularization. The third plot shows CPRâ€™s lambda enforcing the constraint. At the bottom, we see the validation loss. AdamW converges faster in the beginning of the training but CPR leads to a more linear improvement and a better final performance.