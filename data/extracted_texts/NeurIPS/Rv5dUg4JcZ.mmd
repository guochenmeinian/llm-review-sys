# Learning a Single Neuron Robustly

to Distributional Shifts and Adversarial Label Noise

 Shuyao Li

University of Wisconsin-Madison

shuyao.li@wisc.edu

&Sushrut Karmalkar

University of Wisconsin-Madison

skarmalkar@wisc.edu

&Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

&Jelena Diakonikolas

University of Wisconsin-Madison

jelena@cs.wisc.edu

Equal contribution.

###### Abstract

We study the problem of learning a single neuron with respect to the \(L_{2}^{2}\)-loss in the presence of adversarial distribution shifts, where the labels can be arbitrary, and the goal is to find a "best-fit" function. More precisely, given training samples from a reference distribution \(p_{0}\), the goal is to approximate the vector \(^{*}\) which minimizes the squared loss with respect to the worst-case distribution that is close in \(^{2}\)-divergence to \(p_{0}\). We design a computationally efficient algorithm that recovers a vector \(}\) satisfying \(_{p^{*}}((})-y)^{2} C\,_{ p^{*}}((^{*})-y)^{2}+\), where \(C>1\) is a dimension-independent constant and \((^{*},p^{*})\) is the witness attaining the min-max risk \(_{\,:\,\|\| W}_{}_{(,y) p}( ()-y)^{2}-^{2}(,p_{0})\). Our algorithm follows a primal-dual framework and is designed by directly bounding the risk with respect to the original, nonconvex \(L_{2}^{2}\) loss. From an optimization standpoint, our work opens new avenues for the design of primal-dual algorithms under structured nonconvexity.

## 1 Introduction

The problem of learning a single neuron from randomly drawn labeled examples is a fundamental problem extensively studied in the machine learning literature. Given labeled examples \(\{(_{i},y_{i}):(_{i},y_{i})^{d}_{i=1} ^{N}\}\) drawn from a reference distribution \(p_{0}\), the goal in this context is to recover a parameter vector \(_{0}^{*}\) that minimizes the squared loss \(_{,p_{0}}()\) over a ball of radius \(W>0\):

\[_{0}^{*}:=*{arg\,min}_{^{d}:\|\| _{2} W}_{,p_{0}}();_{,p_{0}}() :=_{(,y) p_{0}}(()-y)^{2},\] (1)

where \(:\) is a known (typically non-linear) non-decreasing activation function (e.g., the ReLU activation \((t)=(0,t)\)) and we denote by \(_{0}=_{:\|\|_{2} W}_{,p_{0 }}()\) the minimum squared loss. In the realizable setting -- where \(y=(_{0}^{*})\) and thus \(_{0}=0\) -- this problem is well-understood and by now part of the folklore (see, e.g., ). The results for the realizable setting also naturally extend to zero-mean bounded-variance label noise.

The more realistic agnostic model  (a.k.a. adversarial label noise) aims to identify the best-fitting neuron for a reference distribution of the examples, without any assumptions on label structure. However, it is known that in this setting finding a parameter vector with square loss \(_{0}+\) requires \(d^{(1/)}\) time, even if the \(\)-marginal distribution is Gaussian . Even if we relax our goal to achieve error \(O(_{0})+\), efficient

[MISSING_PAGE_FAIL:2]

\(p_{0}\) and \((W):=\{:\|\|_{2} W\}\). We define the following:

\[L_{}(,;p_{0}) :=_{(,y) p}(()-y)^{2}- ^{2}(,_{0})=_{,p}()-^{2}(, _{0}),\] \[R(;p_{0}) :=_{p(p_{0})}L_{}(,;p_{0}),\ _{} :=*{arg\,max}_{p(p_{0})}L_{}(,;p _{0}),\] \[^{*} :=*{arg\,min}_{(W)}R(;p_{0 }),^{*}:=_{^{*}},\] \[ :=_{(,y) p^{*}}((^{*} )-y)^{2}=_{,p^{*}}(^{*}).\]

We say that \(L_{}(,;p_{0})\) is the regularized square loss function of a vector \(\) and a distribution \(\); and \(R(;p_{0})\) is the DRO risk of \(\) with respect to \(p_{0}\). We call \(^{*}\) the target distribution.

The minimization of the DRO risk as defined above corresponds to the regularized/penalized DRO formulation studied in prior work; see, e.g., [18; 19]. An alternate formulation would have been to instead optimize over a restricted domain. The two are equivalent because of Lagrangian duality. We show in Claim E.1 a concrete relation between our regularization parameter \(\) and the chi-squared distance between the population distribution \(_{0}\) and the target distribution \(^{*}\). We further require that \(\) is sufficiently large to ensure that the resulting \(^{2}(^{*},_{0})\) is smaller than an absolute constant, which is in line with the DRO being used for not too large ambiguity sets .

Empirical VersionIf the reference distribution is the uniform distribution on \(N\) labeled examples \((_{i},y_{i})^{d}\) drawn from \(p_{0}\), we call it \(_{0}=_{0}(N)\), and similarly define \((_{0})\). Note that \(R(^{*};_{0})=_{(_{0 })}_{(,y)}((^{*})-y)^{2 }-^{2}(,_{0})\); if we let \(}^{*}\) denote the distribution that achieves the maximum, \(^{*}\) has the same support as \(_{0}\) and can be interpreted as the reweighting of the samples that maximizes the regularized loss.

Formally, our goal is to solve the following learning problem.

**Problem 1.3** (Robustly Learning a Single Neuron Under Distributional Shifts).: Given error parameters \(,(0,1)\), regularization parameter \(>0\), set radius \(W>0\), and sample access to labeled examples \((,y)\) drawn i.i.d. from an unknown reference distribution \(_{0}\), output a parameter vector \(}(W)\) that is competitive with the DRO risk minimizer \(^{*}=_{(W)}R(;p_{0})\) in the sense that with probability at least \(1-\), \(\|}-^{*}\|_{2}^{2}+\) for an absolute constant \(C\).

While the stated goal is expressed in terms of \(\|}-^{*}\|_{2}\), under mild distributional assumptions that we make on the reference and target distributions, this guarantee implies being competitive with the best-fit function on \(^{*}\) in terms of both the square loss and the risk, namely \(_{,p^{*}}(})=O()+\) and \(R(,_{0})-_{(W)}R(},_{0})  O()+\). Further, our algorithm is primal-dual and it outputs a distribution \(\) that is close to \(^{*}\) in the chi-squared divergence.

Since the solution to Problem 1.3 has an error of \(O()+\), when we use the term "convergence" in our paper, we refer to the following weaker notion: the iterates of our algorithm _converge_ to the (set of) solutions such that asymptotically all iterates lie within the set of \(O()+\) solutions, which are the target solutions, as stated in Problem 1.3.

### Main Result

Our main contribution is the first polynomial sample and time algorithm for learning a neuron in a distributionally robust setting for a broad class of activations (Definition 1.1) and under mild distributional assumption on the target distribution (Assumptions 2.1 and 2.2 in Section 2.1).

**Theorem 1.4** (Main Theorem -- Informal).: _Suppose that the learner has access to \(N=(d/^{2})\) samples drawn from the reference distribution \(_{0}\). If all samples are bounded and the distribution \(^{*}\) satisfies the "margin-like" condition and concentration (Assumptions 2.1 and 2.2 in Section 2.1), then after \((d(1/))\) iterations, each running in sample near-linear time, with high probability Algorithm 1 recovers \(}\) such that \(\|}-^{*}\|_{2}^{2} C\ +\), for an absolute constant \(C\)._

We emphasize that Theorem 1.4 simultaneously addresses two types of robustness: firstly, robustness concerning labels (\(y\)); and secondly, robustness due to shifts in the distribution (\(_{0}\) being perturbed). This result is new even when specialized to any nontrivial activation like ReLU, realizable case (where \(=0\)), and the simplest Gaussian \(\)-marginal distribution. Without distributional robustness, existing approaches, as previously discussed, yield an error of \(O()+\) under certain \(\)-marginal conditions. We demonstrate that this error rate can be also achieved with respect to \(p^{*}\) in a distributionally robust context, as long as \(^{*}\) meets the same conditions specified in  -- among the mildest in the literature addressing non-distributionally robust agnostic setting.

### Technical Overview

Our technical approach relies on three main components, described below:

Local Error BoundsOur work is inspired by optimization-theory local error bounds ("sharpness") obtained for learning a single neuron with monotone unbounded activations under structured distributions without considering distributional shift or ambiguity . These bounds are crucial as they quantify growth of a loss function outside the set of target solutions, essentially acting as a "signal" to guide algorithms toward target solutions in our learning problems. Concretely, under distributional assumptions on \(^{*}\) from , the following sharpness property can be established: there is an absolute constant \(c_{1}>0\) such that \((2\|^{*}\|_{2})\),

\[\|-^{*}\|_{2}^{2}=()\ \ _{,^{*}}()-_{,^{*}}(^{*})  c_{1}\|-^{*}\|_{2}^{2}.\] (2)

The local error bounds in  assume identical reference and target distributions. Introducing distributional ambiguity -- as in our work -- invalidates this assumption, and as a result necessary distributional assumptions for sharpness may not apply to all distributions in the ambiguity set. In this work, distributional assumptions are exclusively applied to the target distribution to exploit the sharpness property proved in . We also assume that the sample covariates from the reference distribution are polynomially bounded; this assumption, which is without loss of generality, impacts only the sample and computational complexities and is satisfied by standard distributions.

Primal-Dual AlgorithmOur algorithm is a principled, primal-dual algorithm leveraging the sharpness property on the target distribution, the structure of the square loss, and properties of chi-squared divergence. We control a "gap-like" function of the iterates, \((},};}_{0}):=L_{ }(},}^{*};}_{0})-L_{ }(^{*},};}_{0})\). The idea of approximating a gap and showing it reduces at a rate \(1/A_{k}\), where \(A_{k}\) is a monotonically increasing function of \(k\), comes from  and has been extended to primal-dual methods, including DRO settings, in .

Unlike past work , our primal problem is nonconvex, even for ReLU activations without distributional ambiguity. Unfortunately, the previously mentioned results relying on convexity do not apply in our setting. Additionally, sharpness -- which appears crucial to approximating the target loss -- is a _local_ property, applying only to \(\) such that \(\|\|_{2} 2\|^{*}\|_{2}\), where \(\|^{*}\|_{2}\) is unknown. This condition is trivially met at initialization, but proving it holds for all iterates requires convergence. We address this issue via an inductive argument, effectively coupling convergence analysis with localization of the iterates.

Additionally, standard primal-dual methods  rely on bilinear coupling between primal and dual variables in \(L_{}(,};}_{0})\). In our case, \(L_{}(,};}_{0})\) is _nonlinear_ and _nonconvex_ in the first argument. Recent work  handled nonlinearity by linearizing the function using convexity of the loss, which makes the function bounded below by its linear approximation at any point. However, this approach cannot be applied to our problem as the loss is nonconvex. Instead, we control the chi-squared divergence between the target distribution and the algorithm dual iterates to bound \(L_{}(,}^{*};}_{0})\) from below, using a key structural result that we establish in Lemma 3.4. The challenges involved in proving this structural result require us to rely on chi-squared regularization and convex activation \(\). Generalizing our result to all monotone unbounded activations and other strongly convex divergences like KL would need a similar structural lemma under these broader assumptions.

An interesting aspect of our analysis is that we do not rely on a convex surrogate for our problem. Instead, we constructively bound a quantity related to the DRO risk of the original square loss, justifying our algorithmic choices directly from the analysis. Although we do not consider convex surrogates, the vector field \((;,y)\), scaled by \(2\), corresponds to the gradient of the convex surrogate loss \(_{0}^{}((t)-y)\,t\), which has been used in prior literature on learning a single neuron under similar settings without distributional ambiguity . In our analysis, the vector field \((;,y)\) is naturally motivated by the argument in the proof of Lemma 3.4.

"Concentration" of the Target DistributionTo prove that our primal-dual algorithm converges, we need to prove both an upper bound and a lower bound for \((},};}_{0})\). The lower bound relieson sharpness; however, we need it to hold for the _empirical target distribution_\((^{*})\). This requires us to translate distributional assumptions and/or their implications from \(p^{*}\) to \(^{*}\). Unfortunately, \(^{*}\) is not the uniform distribution over samples drawn from \(p^{*}\). Rather, it is the maximizing distribution in the empirical DRO risk, defined w.r.t. \(_{0}\). This means that prior uniform convergence results do not apply. Additionally, minimax risk rates from prior statistical results, such as those in , relate \(R(;_{0})\) and \(R(;p_{0})\). However, they do not help in our algorithmic analysis since they do not guarantee that the sharpness holds for \(^{*}\).

To address these challenges, we prove (in Corollary C.2) that as long as \(\) is sufficiently large, there is a simple closed-form expression for \(^{*}\) as a function of \(_{0}\) and an analogous relationship holds between \(p^{*}\) and \(p_{0}\). This allows us to leverage the fact that expectations of bounded functions with respect to \(_{0}\) closely approximate those with respect to \(p_{0}\) to show that expectations with respect to \(^{*}\) and \(p^{*}\) are similarly close. This result then implies that the sharpness also holds for \(^{*}\) (Lemma C.6). Full details are provided in Appendix C.

## 2 Preliminaries

In this section, we introduce the necessary notation and state basic facts used in our analysis.

NotationGiven a positive integer \(N\), \([N]\) denotes the set \(\{1,2,,N\}\). Given a set \(\), \(^{c}\) denotes the complement of \(\) when the universe is clear from the context. We use \(_{}\) to denote the characteristic function of a set \(\): \(_{}(x)=1\) if \(x\) and \(_{}(x)=0\) otherwise. For vectors \(\) and \(}\) from the \(d\)-dimensional Euclidean space \(^{d}\), we use \(,}\) and \(}\) to denote the standard inner product, while \(\|\|_{2}=\) denotes the \(_{2}\) norm. We use \((^{(1)},^{(2)},,^{(d)})\) to denote the entries of \(^{d}\). We write \(}\) to indicate \(^{(j)}}^{(j)}\) for all coordinates \(j\). For \(r>0\), \((r):=\{:\|\|_{2} r\}\) denotes the centered ball of radius \(r\). We use \(_{N}\) to denote the probability simplex: \(_{n}:=\{^{N}:_{j=1}^{N}^{(j)}=1, j [N]:^{(j)} 0\}\). We denote by \(_{d}\) the identity matrix of size \(d d\). We write \(A B\) to indicate that \(^{}(A-B) 0\) for all \(^{d}\). For two functions \(f\) and \(g\), we say \(f=(g)\) if \(f=O(g^{k}(g))\) for some constant \(k\), and similarly define \(\). We use notation \(_{c}()\) and \(_{c}()\) to hide polynomial factors in (typically absolute constant) parameters \(c\). For two distributions \(p\) and \(p^{}\), we use \(p p^{}\) to denote that \(p\) is absolutely continuous with respect to \(p^{}\), i.e., for all measurable sets \(A\), \(p^{}(A)=0\) implies \(p(A)=0\). Typically, \(\) and \(\) are empirical distributions, and \(\) is equivalent to the condition that the support of \(\) is a subset of the support of \(\). For \(p p^{}\), we use \(p}{p^{}}\) to denote their Radon-Nikodym derivative, which is the quotient of probability mass functions for discrete distributions. We use \(^{2}(p,p^{})\) to denote the chi-squared divergence of \(p\) w.r.t. \(p^{}\), i.e., \(^{2}(p,p^{})=(p}{p^{}}-1)^{2} p^{}\).

### Distributional Assumptions

Similar to , we make two assumptions about the target distribution of the covariates (\(p_{}^{*}\)). First, we assume that the optimal solution \(^{*}\) satisfies the following "margin-like" condition:

**Assumption 2.1** (Margin).: _There exist absolute constants \(,(0,1]\) such that \(_{ p_{}^{*}}[^{T}_{^{*} \|^{*}\|_{2}}]\), where \(p_{}^{*}\) is the \(\)-marginal distribution of \(p^{*}\)._

We also assume that \(p_{}^{*}\) is subexponential with parameter \(B\), which is an absolute constant.

**Assumption 2.2** (Subexponential Concentration).: _There exists a parameter \(B>0\) such that for any \((1)\) and any \(r 1\), it holds that \(_{ p_{}^{*}}[|| r](-Br)\)._

Appendix E of  shows that Assumptions 2.1 and 2.2 are satisfied by several important families of distributions including Gaussians, discrete Gaussians, all isotropic log-concave distributions, the uniform distribution over \(\{-1,0,1\}^{d}\), etc.

For simplicity, we assume the labeled samples \((^{(i)},y^{(i)})\) drawn from the reference distribution are bounded. This assumption, which does not affect the approximation constant for Problem 1.3, only impacts iteration and sample complexities. We state the bound on the covariates below, while a bound on the labels follows from prior work (Fact 2.6 stated in the next subsection).

**Assumption 2.3** (Boundedness).: _There exists a parameter \(S\) such that for any fixed \((1)\) it holds that \( S\) for all sample covariates \(\) in the support of \(_{0}\)._We also assume without loss of generality that \(\|^{*}\|_{2}^{2} C+\) for some absolute constant \(C\), since otherwise \(\) would be a valid \(O()+\) solution. Algorithmically, we can first compute the empirical risk (per Corollary C.3) of the output from our algorithm and of \(}=\) and then output the solution with the lower risk to get an \(O()+\) solution; see Claim E.2 for a detailed discussion.

### Auxiliary Facts

To achieve the claimed guarantees, we leverage structural properties of the loss function on the target distribution, implied by our distributional assumptions (Assumptions 2.1 and 2.2). Specifically, we make use of Lemma 2.2 and Fact C.4 from , summarized in the fact below.

**Fact 2.4** (Sharpness ()).: _Suppose \(^{*}\) and \(^{*}\) satisfy Assumptions 2.1 and 2.2. Let \(c_{0}=)}\). For all \((2\|^{*}\|)\) and \((1)\),_

\[_{ p_{}^{*}}[(( )-(^{*}))(-^{*})]  c_{0}\|-^{*}\|_{2}^{2},\] \[_{ p_{}^{*}}[()^{}]  5B.\]

Fact 2.4 applies to the population version of the problem. Such a result also holds for the target distribution of the empirical problem, which we state below. Note that this result cannot be obtained by appealing to uniform convergence results for learning a neuron (without distributional robustness).

**Lemma 2.5** (Empirical Sharpness; Informal. See Lemma C.6).: _Under Assumptions 2.1 to 2.3, for a sufficiently large sample size \(N\) as a function of \(B,W,S,,,,,d\) and with high probability, for all \((2\|^{*}\|)\) with \(\|-^{*}\|\) and \((1)\)._

\[_{_{}^{*}}[(( )-(^{*}))(-^{*} )] (c_{0}/2)\|-^{*}\|_{2}^{2}\] (3) \[_{_{}^{*}}[()^{}]  6B.\] (4)

As a consequence, for \(c_{1}=c_{0}^{2}/(24B)\) and any \((W)\) (where \(c_{0}\) is defined in Fact 2.4), we have

\[c_{1}\|-^{*}\|_{2}^{2}_{_{}^{*}}[(()-(^{*}))^{2}] 6 B^{2}\|-^{*}\|_{2}^{2},\] (5)

where the left inequality uses Cauchy-Schwarz and the right inequality uses \(\)-Lipschitzness of \(()\).

 also showed that the labels \(y\) can be assumed to be bounded without loss of generality.

**Fact 2.6**.: _Suppose \(^{*}\) and \(^{*}\) satisfy Assumption 2.1 and Assumption 2.2. Let \(y^{}=(y)\{|y|,M\}\) where for some sufficiently large absolute constant \(C_{M}\) we define_

\[M=C_{M}WB( BW/)\] (6)

_Then \(_{^{*}}((^{*})-y^{})^{2} _{^{*}}((^{*})-y)^{2}+= +\)._

We also make use of the following facts from convex analysis. First, let \(:^{N}\) be a differentiable function and the Bregman divergence of \(\) for any \(,^{N}\) be defined by

\[D_{}(,)=()-()-(), -.\]

**Fact 2.7**.: _Let \(()=()+,+b\) for some \(^{N}\) and \(b\). Then \(D_{}(,)=D_{}(,)\) for all \(,^{N}\), i.e., the Bregman divergence is blind to the addition of affine terms to function \(\)._

Second, we state the first-order necessary conditions that a local maximizer must satisfy.

**Fact 2.8** (First-Order Optimality Condition).: _Let \(\) be a closed, convex, and nonempty set and let \(f:\) be continuously differentiable. If \(^{*}\) is a local maximizer of \(f\) on \(\), then it holds that_

\[ f(^{*})(-^{*}) 0$}.\] (7)

_If \(f\) is also concave, then Equation (7) implies that \(^{*}\) is a global maximizer of \(f\)._

## 3 Algorithm and Convergence Analysis

In this section, we introduce our algorithm and state our main results, summarized in Theorem 3.1. We highlight the main components of our technical approach, while most of the technical details are deferred to the appendix, due to space constraints.

To facilitate the presentation of results, we introduce the following auxiliary notation: \((;,y):=(()-y)^{2}\), \((;,y):=2(()-y)\) and \(}=_{(,y)}_{*}}( ^{*};,y)\). We also note that Assumption 2.3 implies that for all samples \(\{_{i},y_{i}\}\), the function \((;_{i},y_{i})\) is bounded above by \(G\) and \(\)-Lipschitz for all \(i[N]\) and \((W)\), where \(G=2 S( WS+M)\) and \(=2^{2}S^{2}d\) (see Lemma B.4 in Appendix B). Starting from this section, we write \(L(,})\) to denote \(L_{}(,};}_{0})\), hiding the dependence on \(}_{0}\) and \(\). We also write \((,})\) for \((,};}_{0})\)

Our main algorithm (Algorithm 1) is an iterative primal-dual method with extrapolation on the primal side via \(_{i}\). The vector \(_{}_{i}}[(_{i};,y)]\) equals the (scaled) gradient of a surrogate loss used in prior works . In contrast to prior work, we directly bound the original square loss, with \(_{}_{i}}[(_{i};,y)]\) naturally arising from our analysis. Both updates \(_{i}\) and \(}_{i}\) are efficiently computable: \(_{i}\) involves a simple projection onto a Euclidean ball, and \(}_{i}\) involves a projection onto a probability simplex, computable in near-linear time .

``` Input:\(>0,,G,c_{1},_{0}=768^{4}B/c_{1}\), sample set \(\{(_{i},y_{i})\}_{i=1}^{N}\) Initialization:\(A_{-1}=a_{-1}=A_{0}=a_{0}=0,_{-1}=_{0}=,}_{-1}= }_{0}\); for\(i=1,,k\)do \(a_{i}=1+/8\}}{2\{,G\}}^{i-1} \{_{0},1/4\}/(2\{,G\}),A_{i}=a_{i}+A_{i-1}\); \((;,y)=2(()-(y) \{|y|,M\})\), where \(M\) is defined in Equation (6) ; \(_{i-1}=_{_{i-1}}[(_{i-1};,y)]+ }{a_{i}}(_{_{i-1}}[(_{i-1};,y)]-_{_{i-2}}[(_{i-2};,y)])\); \(_{i}=_{(W)}a_{i}_{i-1},+}{2}\|-_{i-1}\|_{2}^{2}}\); \(}_{i}=_{}a_{i}L(_{i},})-(_{0}+ A_{i-1})D_{^{2}(,_{0})}( },}_{i-1})}\); ```

**Algorithm 1**Main algorithm

**Theorem 3.1** (Main Theorem).: _Under Assumptions 2.1 to 2.3, suppose the sample size is such that \(N=_{B,S,,,,}}{ ^{2}}1+}{^{2}}(d+W^{4}(1/))\) and \( 8^{2}_{(2)}+}/c_{1}\), where \(_{(2)}=_{^{*}}[(^{*};,y)^{2}]\) and \(c_{1}\) is defined in Lemma 2.5. With probability at least \(1-\), for all iterates \(_{k},}_{k}\), it holds that_

\[}{4}\|^{*}-_{k}\|_{2}^{2}+ D_{}(}^ {*},}_{k})}{A_{k}}+B\,}{c_{1}}+,\]

_where \(D_{0}=\|^{*}-_{0}\|_{2}^{2}+_{0}^{2}(}^{*},}_{0})\) and \(^{2}(}^{*},}_{0}) c_{1}/(1536^{4}B)\) (and therefore \(D_{0}\) does not depend on the sample size N)._

_In particular, after at most \(k=(}{\{,c_{1}\}}(}{ }))\) iterations, it holds that_

\[\|_{k}-^{*}\|_{2}  C_{3}}+,\] (8) \[_{(,y)^{*}}[(_{k};,y)] (2+20B^{2}C_{3}^{2})\,\,\!+\!10^{2}B,\] (9) \[R(_{k};_{0})-_{(W)}R( ;_{0}) =R(_{k};_{0})-R(^{*};_{0}) C_{4}( \!+\!),\] (10)

_where \(C_{3}=16/c_{1}\) and \(C_{4}=1+2(10B^{2}+c_{1})C_{3}+c_{1}^{2}C_{3}^{2}\)._

We focus on the convergence of iterates \(_{i}\) as claimed in Equation (8); the loss bound (Equation (9)) follows directly from the iterate convergence, while the risk bound (Equation (10)) requires a more involved analysis. Complete details for Equations (9) and (10) are provided in Appendix F.

Our strategy for the convergence analysis is as follows. Consider \(\{a_{i}\}\), a sequence of positive step sizes, and define \(A_{i}\) as their cumulative sum \(_{j=1}^{i}a_{j}\). Our algorithm produces a sequence of primal-dual pairs \(_{i},}_{i}\), tracking a quantity related to the primal-dual gap, defined by:

\[(_{i},}_{i}):=L(_{i},}^{*})-L (^{*},}_{i})=(L(_{i},}^{*})-L( ^{*},}^{*}))+(L(^{*},}^{*})-L( {w}^{*},}_{i})).\]

We view \((L(_{i},}^{*})-L(^{*},}^{*}))\) as the "prOur strategy consists of deriving "sandwiching" inequalities for the (weighted) cumulative gap \(_{i=1}^{k}a_{i}(_{i},}_{i})\) and deducing convergence guarantees for the algorithm iterates from them. A combination of these two inequalities leads to the statement of Theorem 3.1, from which we can deduce that unless we already have an \(O()+\) solution, the iterates must be converging to the target solutions at rate \(1/A_{k}\), which we argue can be made geometrically fast.

OrganizationThe rest of this section is organized as follows -- under the standard assumptions we state in this paper, in Lemma 3.2, we prove a lower bound on \((,})\) for any choice of \(\) and \(}\). This can be used to get a corresponding lower bound on the weighted sum \(_{i=1}^{k}a_{i}(_{i},}_{i})\).

In Lemma 3.3 we then state an upper bound on \(_{i=1}^{k}a_{i}(_{i},}_{i})\); the proof of this technical argument is deferred to Appendix D. These two bounds together give us the first inequality in Theorem 3.1. Claim B.6 then bounds below the convergence rate for our choice of \(a_{i}\) in Algorithm 1; and indicates that it is geometric. Finally, we put everything together to prove Theorem 3.1.

To simplify the notation, we use \((}):=^{2}(},}_{0})\) throughout this section. Note that \(D_{}(},})=D_{}(}, {})=_{i=1}^{N}^{(i)}-}^{(i)})^{ 2}}{}_{0}^{(i)}}\) for any \(}\) and \(}\) in the domain.

### Lower Bound on the Gap Function

We begin the convergence analysis by demonstrating a lower bound on \((_{i},}_{i})\).

**Lemma 3.2** (Gap Lower Bound).: _Under the setting in which Lemma 2.5 holds, for all \((2\|^{*}\|_{2})\), \((,})-B}{c_{1}}}+}{2}\|-^{*}\|_{2}^{2}+ D_{}( }^{*},})\)._

Proof.: Writing \((()-y)^{2}=(()-( ^{*}))+((^{*})-y)^{2}\) and expanding the square, we have

\[L(,}^{*})-L(^{*},}^{*} )=_{(,y)}^{*}}[(()-y )^{2}-((^{*})-y)^{2}]\] \[= -2_{}^{*}}[((^{*} )-y)(()-(^{*}))]+_{ }^{*}}[((()-(^{*}))^{2}].\]

By the Cauchy-Schwarz inequality, we further have that

\[_{}^{*}}[((^{*}) -y)(()-(^{*}))]\] \[_{}^{*}}[((^{*} )-y)^{2}]_{}^{*}}[(( {x})-(^{*}))^{2}]}\] \[}}\|-^{*} \|_{2},\] (11)

where in the second inequality we used the definition of \(}\) and \(_{}_{}^{*}}[(( )-(^{*}))^{2}] 6B^{2}\|-^{*}\|_{2}^{2}\) from the right inequality in Equation (5).

On the other hand, by the left inequality in Equation (5), we also have

\[_{}^{*}}[(()-(^{*} ))^{2}] c_{1}\|-^{*}\|_{2}^{2}.\] (12)

Thus, combining Equation (3.2) and Equation (12), we get

\[L(,}^{*})-L(^{*},}^{*}) -2\|-^{*}\|_{2}}}+c_{1}\|-^{*}\|_{2}^{2}\] \[-B}{c_{1}}}+ }{2}\|-^{*}\|_{2}^{2},\] (13)

where the last inequality is by \(2\|-^{*}\|_{2}}}6B}{2c_{1}}}+}{2}\|-^{*}\|_{ 2}^{2}\), which comes from an application of Young's inequality (Fact B.1).

Finally, we use the optimality of \(}^{*}\), which achieves the maximum over all \(}\) for \(L(^{*},})\). By the definition of a Bregman divergence, Fact 2.7, and first-order necessary condition in Fact 2.8:

\[-L(^{*},})-(-L(^{*},}^{*}))=- _{}}L(^{*},}^{*}),}- }^{*}+D_{-L(^{*},)}(}, {}^{*}) D_{}(}^{*},}).\] (14)

Summing up Equation (13) and Equation (14) completes the proof.

### Upper Bound on the Gap Function

Having obtained a lower bound on the gap function, we now show an upper bound, leveraging our algorithmic choices. The proof is rather technical and involves individually bounding \(L(_{i},}^{*})\) above and bounding \(L(^{*},}_{i})\) below to obtain an upper bound on the gap function, which equals \(L(_{i},}^{*})-L(^{*},}_{i}).\) We state this result in the next lemma, while the proof is in Appendix D.

**Lemma 3.3** (Gap Upper Bound).: _Let \(_{i},}_{i},a_{i},A_{i}\) evolve according to Algorithm 1, where we take, by convention, \(a_{-1}=A_{-1}=a_{0}=A_{0}=0\) and \(_{-1}=_{0},\)\(}_{-1}=}_{0}.\) Assuming Lemma 2.5 applies, then, for all \(k 1,_{i=1}^{k}a_{i}(_{i},}_{i})\) is bounded above by_

\[\|^{*}-_{0}\|_{2}^{2}+_{0}D_{}( }^{*},}_{0})-A_{k}}{2}\|^{ *}-_{k}\|_{2}^{2}-(_{0}+ A_{k})D_{}(}^{*}, }_{k})\] \[+_{i=1}^{k}a_{i}}{4}\|^{*}-_{i}\|_{2}^ {2}+}_{(2)}}}{c_{1}}_ {i=1}^{k}a_{i}^{2}(}_{i},}^{*})+B}A_{k}}{c_{1}}.\]

A critical technical component in the proof of Lemma 3.3 is how we handle issues related to nonconvexity. A key technical result that we prove and use is the following.

**Lemma 3.4**.: _Let \(S_{i}:=_{_{i}}[((^{*})-( {w}_{i}))^{2}]+_{_{i}}[2((_{i} )-y)((^{*})-(_{i}))]\), \(_{i}\) evolve according to Line 6 in Algorithm 1 and suppose we are in the setting where Lemma 2.5 holds. Then, \(S_{i}_{_{i}}[((;,y),^{*}- {w}_{i})]-E_{i}\) where_

\[E_{i}=}{4}\|^{*}-_{i}\|_{2}^{2}+8^{2} {6B}}_{(2)}}/c_{1}^{2}( }_{i},}^{*})+(48^{2}B/c_{1})}.\] (15)

This bound is precisely what forces us to choose chi-squared as the measure of divergence between distributions and introduce a dependence on \(}_{(2)}\). One pathway to generalize our results to other divergences would be to find a corresponding generalization to Lemma 3.4.

### Proof of Main Theorem

Combining Lemma 3.2 and Lemma 3.3, we are now ready to prove our main result.

Proof of Theorem 3.1.: Combining the lower bound on the gap function from Lemma 3.2 with the upper bound from Lemma 3.3 and rearranging, whenever \(\|_{i}\|_{2} 2\|^{*}\|_{2}\) for all \(i k\) so that Lemma 2.5 applies, we get that

\[-B}{c_{1}}}A_{k}+_{i=1} ^{k}a_{i}}{2}\|_{i}-^{*}\|_{2}^{2}+_{i=1}^{k} a _{i}D_{}(}^{*},}_{i})_{i=1}^{k}a_{ i}(_{i},}_{i})\] \[\|^{*}-_{0}\|_{2}^{2}+_{0}D_{}( }^{*},}_{0})-A_{k}}{2}\|^{ *}-_{k}\|_{2}^{2}-(_{0}+ A_{k})D_{}(}^{*}, }_{k})\] \[+_{i=1}^{k}a_{i}}{4}\|^{*}-_{i}\| _{2}^{2}+}_{(2)}}}{c_{1}} _{i=1}^{k}a_{i}^{2}(}_{i},}^{*})+ {48^{2}B}A_{k}}{c_{1}}.\]

To reach the first claim of the theorem, we first argue that \(_{i=1}^{k}a_{i}(4^{2}}_{(2 )}}/c_{1})^{2}(}_{i},}^{*})- D_{}( }^{*},}_{i}) 0.\) This follows from (1) Corollary C.2, by which we have \(}^{*(j)}}_{0}^{(j)}/2\) for all \(j[N]\), hence

\[^{2}(}_{i},}^{*})=_{j[N]}(}^{*}{}^{(j)}-}_{i}^{(j)})^{2}/}^{*}{}^{(j )} 2_{j[N]}(}^{*}{}^{(j)}-}_{i}^{(j)}) ^{2}/}_{0}^{(j)}=2D_{}(}^{*}, }_{i})\]

and (2) our choice of \(,\) which ensures, with high probability, that \( 8^{2}}_{(2)}+}/c_{1} 8 ^{2}}_{(2)}}/c_{1}\), where the last inequality is because for the specified sample size, we have that \(}_{(2)}+_{(2)}\) by Corollary C.9.

Second, we similarly have that with probability \(1-\), \(}+\). Hence, since Bregman divergence of a convex function is non-negative, whenever \(\|_{i}\|_{2} 2\|^{*}\|_{2}\) for all \(i k\), we have

\[\|^{*}-_{k}\|_{2}^{2} ^{*}-_{0}\|_{2}^{2}+2_{0}D_{}( }^{*},}_{0})}{1+0.5c_{1}A_{k}}+B}{c_{1}}(+)\] (16) \[D_{}(}^{*},}_{k}) ^{*}-_{0}\|_{2}^{2}/2+_{0}D_{}( }^{*},}_{0})}{_{0}+ A_{k}}+B}{}(+)\] (17)

The bound \(^{2}(}^{*},}_{0}) c_{1}/(1536^{4}B)\) is proved in Claim E.1. Finally, in Appendix E, we inductively prove the following claim so that assumptions in Lemma 2.5 are satisfied.

**Claim 3.5**.: _For all iterations \(k 0\), \(\|_{k}\|_{2} 2\|^{*}\|_{2}\)._

The bound on the growth of \(A_{k}\) follows by standard arguments and is provided as Claim B.6. Since \(A_{k}\) grows exponentially with \((1+)^{k}\) where \(=/8\}}{2\{,G\}}\) and since \(D_{0}(1+)^{-k}\) can be enforced by setting \(k=(1+1/)(D_{0}/)(D_{0}/)/(1+)\), we have that after \((}{\{,c_{1}\}}(D_{0}/))\) iterations either \(\|_{k}-^{*}\|_{2}\) or \(\|_{i}-^{*}\|_{2} C_{3}}\). 

## 4 Conclusion

In this paper, we study the problem of learning a single neuron in the distributionally robust setting, with the square loss regularized by the chi-squared distance between the reference and target distributions. Our results serve as a preliminary exploration in this area, paving the way for several potential extensions. Future work includes generalizing our approach to single index models with unknown activations, expanding to neural networks comprising multiple neurons, and considering alternative ambiguity sets such as those based on the Wasserstein distance or Kullback-Leibler divergence.