# Stein \(\Pi\)-Importance Sampling

# Stein \(\)-Importance Sampling

 Congye Wang\({}^{1}\), Wilson Ye Chen\({}^{2}\), Heishiro Kanagawa\({}^{1}\), Chris. J. Oates\({}^{1}\)

\({}^{1}\) Newcastle University, UK

\({}^{2}\) University of Sydney, Australia

###### Abstract

Stein discrepancies have emerged as a powerful tool for retrospective improvement of Markov chain Monte Carlo output. However, the question of how to _design_ Markov chains that are well-suited to such post-processing has yet to be addressed. This paper studies Stein importance sampling, in which weights are assigned to the states visited by a \(\)-invariant Markov chain to obtain a consistent approximation of \(P\), the intended target. Surprisingly, the optimal choice of \(\) is not identical to the target \(P\); we therefore propose an explicit construction for \(\) based on a novel variational argument. Explicit conditions for convergence of _Stein_\(\)_-Importance Sampling_ are established. For \( 70\%\) of tasks in the PosteriorDB benchmark, a significant improvement over the analogous post-processing of \(P\)-invariant Markov chains is reported.

## 1 Introduction

Stein discrepancies are a class of statistical divergences that can be computed without access to a normalisation constant. Originally conceived as a tool to measure the performance of sampling methods (Gorham and Mackey, 2015), these discrepancies have since found wide-ranging statistical applications (see the review of Anastasiou et al., 2023). Our focus here is the use of Stein discrepancies for retrospective improvement of Markov chain Monte Carlo (MCMC), and here two main techniques have been proposed: (i) _Stein importance sampling_(Liu and Lee, 2017; Hodgkinson et al., 2020), and (ii) _Stein thinning_(Riabiz et al., 2022). In Stein importance sampling (also called _black box importance sampling_), the samples are assigned weights such that a Stein discrepancy between the weighted empirical measure and the target \(P\) is minimised. Stein thinning constructs a sparse approximation to this optimally weighted measure at a lower computational and storage cost. Together, these techniques provide a powerful set of post-processing tools for MCMC, with subsequent authors proposing a range of generalisations and extensions (Teymur et al., 2021; Chopin and Ducrocq, 2021; Hawkins et al., 2022; Fisher and Oates, 2023; Benard et al., 2023).

The consistency of these algorithms has been established in the setting of approximate, \(\)-invariant MCMC, motivated by challenging inference problems where only approximate sampling can be performed. In these settings, \(\) is implicitly an approximation to \(P\) that is as accurate as possible subject to computational budget. However, the critical question of how to _design_ Markov chains that are well-suited to such post-processing has yet to be addressed. This paper provides a solution, in the form of a specific construction for \(\) derived from a novel variational argument. Surprisingly, we are able to demonstrate a substantial improvement using the proposed \(\), compared to the case where \(\) and \(P\) are equal. The paper proceeds as follows: Section 2 presents an abstract formulation of the task and existing results for optimally-weighted empirical measures are reviewed. Section 3 derives our proposed choice of \(\) and establishes that Stein post-processing of samples from a \(\)-invariant Metropolis-adjusted Langevin algorithm (MALA) provides a consistent approximation of \(P\). The approach is stress-tested using the recently released PosteriorDB suite of benchmark tasks in Section 4, before concluding with a discussion in Section 5.

## 2 Background

To properly contextualise our discussion we start with an abstract mathematical description of the task. Let \(P\) be a probability measure on a measurable space \(\). Let \(()\) be the set of all probability measures on \(\). Let \(D_{P}:()[0,]\) be a _statistical divergence_ for measuring the quality of an approximation \(Q\) to \(P\), meaning that \(D_{P}(Q)=0\) if and only if \(Q=P\). In this work we consider approximations whose support is contained in a finite set \(\{x_{1},,x_{n}\}\), and in particular we consider _optimal_ approximations of the form

\[P_{n}^{}=_{i=1}^{n}w_{i}^{}(x_{i}), w^{} *{arg\,min}_{w 0,\ 1^{}w=1}D_{P}(_{i=1}^{n}w_{i}(x_{i})).\]

In what follows we restrict attention to statistical divergences for which such approximations can be shown to exist and be well-defined. The question that we then ask is _which states \(\{x_{1},,x_{n}\}\) minimise the approximation error \(D_{P}(P_{n}^{})\)_? Before specialising to Stein discrepancies, it is helpful to review existing results for some standard statistical divergences \(D_{P}\).

### Wasserstein Divergence

_Optimal quantisation_ focuses on the \(r\)-Wasserstein (\(r 1\)) family of statistical divergences \(D_{P}(Q)=_{(P,Q)}\|x-y\|^{r}(x,y)\), where \((P,Q)\) denotes the set of all _couplings_1 of \(P,Q(^{d})\), and the divergence is finite whenever \(P\) and \(Q\) have finite \(r\)-th moment. Assuming the states \(\{x_{1},,x_{n}\}\) are distinct, the corresponding optimal weights are \(w_{i}^{}=P(A_{i})\) where \(A_{i}\) is the _Voronoi neighbourhood_2 of \(x_{i}\) in \(^{d}\). Optimal states achieve the _minimal quantisation error_ for \(P\);

\[e_{n,r}(P)=_{x_{1},,x_{n}^{d}}D_{P}(_{i=1}^{n} w_{i}^{}(x_{i})),\]

the smallest value of the divergence among optimally-weighted distributions supported on at most \(n\) states. Though the dependence of optimal states on \(n\) and \(P\) can be complicated, we can broaden our perspective to consider _asymptotically_ optimal states, whose asymptotic properties can be precisely characterised. To this end, for \(A^{d}\), let \((A)\) denote the uniform distribution on \(A\), and define the universal constant \(C_{r}(^{d})=_{n 1}n^{r/d}\,e_{n,r}((^{d}))\). Suppose that \(P\) admits a density \(p\) on \(^{d}\). Then the \(r\)th _quantisation coefficient_ of \(P\) on \(^{d}\), defined as

\[C_{r}(P)=C_{r}(^{d})( p(x)^{d/(d+r)}\ x)^{(d+r)/d},\]

plays a central role in the classical theory of quantisation, being the rate constant in the asymptotic convergence of the minimal quantisation error; \(_{n}n^{r/d}e_{n,r}(P)=C_{r}(P)\); see Theorem 6.2 of Graf and Luschgy (2007). This suggests a natural definition; a collection \(\{x_{1},,x_{n}\}\) is called _asymptotically optimal_ if

\[_{n}n^{r/d}D_{P}(_{i=1}^{n}w_{i}^{}(x_{i}) )=C_{r}(P),\]

which amounts to \(P_{n}^{}\) asymptotically attaining the minimal quantisation error \(e_{n,r}(P)\). The main result here is that, if \(\{x_{1},,x_{n}\}\) are asymptotically optimal, then \(_{i=1}^{n}(x_{i})_{r}\), where convergence is in distribution and \(_{r}\) is the distribution whose density is \(_{r}(x) p(x)^{d/(d+r)}\); see Theorem 7.5 of Graf and Luschgy (2007). This provides us with a key insight; optimal states are _over-dispersed_ with respect to the intended distributional target. The extent of the over-dispersion here depends both on \(r\), a parameter of the statistical divergence, and the dimension \(d\) of the space on which distributions are defined.

The \(r\)-Wasserstein divergence is, unfortunately, not well-suited for use in the motivating Bayesian context. In particular, computing the optimal weights \(w_{i}=P(A_{i})\) requires knowledge of \(P\), which is typically not available when \(P\) is implicitly defined via an intractable normalisation constant. Onthe other hand, the optimal sampling distribution \(\) is explicit and can be sampled (for example using MCMC); for discussion of random quantiers in this context see Graf and Luschgy (2007, Chapter 9), Cohort (2004, p126) and Sonnleitner (2022, Section 4.5). The simple form of \(\) is a feature of the classical approach to quantisation that we will attempt to mimic in the sequel.

### Kernel Discrepancies

The theory of quantisation using kernels is less well-developed. A _kernel_ is a measurable, symmetric, positive-definite function \(k:\). From the Moore-Aronszajn theorem, there is a unique Hilbert space \((k)\) for which \(k\) is a reproducing kernel, meaning that \(k(,x)(k)\) for all \(x\) and \( f,k(,x)_{(k)}=f(x)\) for all \(f(k)\) and all \(x\). Assuming that \((k) L^{1}(P)\), we can define the weak (or _Pettis_) integral

\[_{P}()= k(,x)\;P(x),\] (1)

called the _kernel mean embedding_ of \(P\) in \((k)\). The _kernel discrepancy_ is then defined as the norm of the difference between kernel mean embeddings

\[D_{P}(Q)=\|_{Q}-_{P}\|_{(k)}= (Q-P)(x)(Q-P)(y)}\] (2)

where, to be consistent with our earlier notation, we adopt the convention that \(D_{P}(Q)\) is infinite whenever \((k) L^{1}(Q)\). The second equality in (2) follows immediately from the stated properties of a reproducing kernel. To satisfy the requirement of a statistical divergence, we assume that the kernel \(k\) is _characteristic_, meaning that \(_{P}=_{Q}\) if and only if \(P=Q\). In this setting, the properties of optimal states are necessarily dependent on the choice of kernel \(k\), and are in general not well-understood. Indeed, given distinct states \(\{x_{1},,x_{n}\}\), the corresponding optimal weights \(w^{}=(w_{1}^{},,w_{n}^{})^{}\) are the solution to the linearly-constrained quadratic program

\[*{arg\,min}_{w^{d}}\;w^{}Kw-2z^{}w  w 0,\;1^{}w=1\] (3)

where \(K_{i,j}=k(x_{i},x_{j})\) and \(z_{i}=_{P}(x_{i})\). This program does not admit a closed-form solution, but can be numerically solved. To the best of our knowledge, the only theoretical analysis of approximations based on (3) is due to Hayakawa et al. (2022), who established rates for the convergence of \(P_{n}^{}\) to \(P\) in the case where states are independently sampled from \(P\). The question of an optimal sampling distribution was not considered in that work.

Although few results are available concerning (3), relaxations of this program have been well-studied. The simplest relaxation of (3) is to remove both the positivity (\(w 0\)) and normalisation (\(1^{}w=1\)) constraints, in which case the optimal weights have the explicit representation \(w^{}=K^{-1}z\). The analysis of optimal states in this context has developed under the dual strands of _kernel cubature_ and _Bayesian cubature_, where it has been theoretically or empirically demonstrated that (i) if states are randomly sampled, the optimal sampling distribution will be \(n\)-dependent (Bach, 2017) and over-dispersed with respect to the distributional target (Briol et al., 2017), and (ii) _space-filling_ designs are asymptotically optimal for typical stationary kernels on bounded domains \(^{d}\)(Briol et al., 2019). Analysis of optimal states on unbounded domains appears to be more difficult; see e.g. Karvonen et al. (2021). Relaxation of either the positivity or normalisation constraints results in approximations that behave similarly to kernel cubature (see, respectively, Ehler et al., 2019; Karvonen et al., 2018). However, relaxation of either constraint can result in the failure of \(P_{n}^{}\) to be an element of \(()\), limiting the relevance of these results to the posterior approximation task.

Despite relatively little being known about the character of optimal states in this context, kernel discrepancy is widely used. The application of kernel discrepancies to an implicitly defined distributional target, such as a posterior distribution in a Bayesian analysis, is made possible by the use of a _Stein kernel_; a \(P\)-dependent kernel \(k=k_{P}\) for which \(_{P}(x)=0\) for all \(x\)(Oates et al., 2017). The associated kernel discrepancy

\[D_{P}(Q)=\|_{Q}\|_{(k_{P})}=(x,y)\;Q( x)Q(y)}\] (4)is called a _kernel Stein discrepancy (KSD)_(Chwialkowski et al., 2016; Liu et al., 2016; Gorham and Mackey, 2017), and this will be a key tool in our methodological development. The corresponding optimally weighted approximation \(P_{n}^{}\) is the Stein importance sampling method of Liu and Lee (2017). To retain clarity of presentation in the main text, we defer all details on the construction of Stein kernels to Appendix A.

### Sparse Approximation

If the number \(n\) of states is large, computation of optimal weights can become impractical. This has motivated a range of sparse approximation techniques, which aim to iteratively construct an approximation of the form \(P_{n,m}=_{i=1}^{m}(y_{i})\), where each \(y_{i}\) is an element from \(\{x_{1},,x_{n}\}\). The canonical example is the greedy algorithm which, at iteration \(j\), selects a state

\[y_{j}*{arg\,min}_{y\{x_{1},,x_{n}\}}D_{P}( {1}{j}(y)+_{i=1}^{j-1}(y_{i}))\] (5)

for which the statistical divergence is minimised. In the context of kernel discrepancy, the greedy algorithm (5) has computational cost \(O(m^{2}n)\), which compares favourably3 with the cost of solving (3) when \(m n\). Furthermore, under appropriate assumptions, the sparse approximation converges to the optimally weighted approximation; \(D_{P}(P_{n,m}) D_{P}(P_{n}^{})\) as \(m\) with \(n\) fixed. See Teymur et al. (2021) for full details, where non-myopic and mini-batch extensions of the greedy algorithm are also considered. The greedy algorithm can be viewed as a regularised version of the _Frank-Wolfe_ algorithm (also called _herding_, or the _conditional gradient_ method), for which a similar asymptotic result can be shown to hold (Chen et al., 2010; Bach et al., 2012; Chen et al., 2018). Related work includes Dwivedi and Mackey (2021, 2022); Shetty et al. (2022); Hayakawa et al. (2022). Since in what follows we aim to retrospectively improve MCMC output, where it is not unusual to encounter \(n 10^{4}\)-\(10^{6}\), sparse approximation will be important.

This completes our overview of background material. In what follows we seek to mimic classical quantisation by deriving a choice for \(\) that is straight-forward to sample using MCMC and is appropriately over-dispersed relative to \(P\). This should be achieved while remaining in the framework of kernel discrepancies, so that optimal weights can be explicitly computed, and coupled with a sparse approximation that has low computational and storage cost.

## 3 Methodology

The methods that we consider first sample states \(\{x_{1},,x_{n}\}\) using \(\)-invariant MCMC, then post-process these states using kernel discrepancies (Section 2.2) and sparse approximation (Section 2.3), to obtain an approximation to the target \(P\). A variational argument, which we present in Section 3.1, provides a suitable \(n\)-independent choice for \(\) (which agrees with our intuition from Section 2.1 that \(\) should be in some appropriate sense over-dispersed with respect to \(P\)). Sufficient conditions for strong consistency of the approximation are established in Section 3.3.

### Selecting \(\)

Here we present a heuristic argument for a particular choice of \(\); rigorous theoretical support for _Stein \(\)-Importance Sampling_ is then provided in Section 3.3. Our setting is that of Section 2.2, and the following will additionally be assumed:

**Assumption 1**.: _It is assumed that_

1. \(C_{1}^{2}:=_{x}k(x,x)>0\)__
2. \(C_{2}:=\ P(x)<\).

Note that (A2) implies that \((k) L^{1}(P)\), and thus (1) is in fact a strong (or _Bochner_) integral.

A direct analysis of the optimal states associated to the optimal weights \(w^{}\) appears to be challenging due to the fact that the components of \(w^{}\) are strongly inter-dependent. Our solution here is to instead consider optimal states associated with weights that, while not optimal, can be expected to perform much better than alternatives, with the advantage that their components are only weakly dependent. Specifically, we will be assuming that \(P\) is absolutely continuous with respect to \(\) (denoted \(P\)), and study convergence of self-normalised importance sampling (SNIS), i.e. the approximation \(P_{n}=_{i=1}^{n}w_{i}(x_{i})\), \(w_{i}(P/)(x_{i})\), where \(x_{1},,x_{n}\) are independent. Since \(w 0\) and \(1^{}w=1\), from the optimality of \(w^{}\) under these constraints we have that \(D_{P}(P_{n}^{}) D_{P}(P_{n})\). It is emphasised that the SNIS weights are a theoretical device only, and will not be used for computation; indeed, we can demonstrate that the SNIS weights \(w\) perform substantially worse than \(w^{}\) in general.

The analysis of SNIS weights \(w\) is tractable when viewed as approximation of the kernel mean embedding \(_{P}\) in the Hilbert space \((k)\). Indeed, recall that \(D_{P}(P_{n})=\|_{n}/\|_{(k)}\) where \(_{n}=(_{P_{n}}-_{P})\). Then, following Section 2.3.1 of Agapiou et al. (2017), we observe that

\[_{n}=(_{i=1}^{n}w_{i}k(,x_{i})-_{P})=}_{i=1}^{n}P}{}(x_{i})[ k(,x_{i})-_{P}]}{_{i=1}^{n}P}{ }(x_{i})}.\] (6)

The idea is to seek \(\) for which the asymptotic variance of \(_{n}\) is small. Supposing that

\[P}{}(x)^{2}\;(x)<,\] (S1)

from the weak law of large numbers the denominator in (6) converges in probability to 1. Further supposing that

\[\|P}{}(x)[k(,x)-_{P}]\|_{ (k)}^{2}\;(x)<,\] (S2)

from the Hilbert space central limit theorem the numerator in (6) converges in distribution to a Gaussian \(}_{i=1}^{n}(P/)(x_{i})[k( ,x_{i})-_{P}]}}{{}} (0,)\) where \(:(k)(k)\) is the covariance operator defined via

\[ f,g_{(k)}= f,P}{}(x)[k(,x)-_{P}]_{( k)} g,P}{}(x)[k(,x)-_{P}] _{(k)}\;(x),\]

see Section 10.1 of Ledoux and Talagrand (1991). Thus, from Slutsky's lemma applied to (6), we conclude that \(_{n}}}{{}}(0,)\). Recalling that \(nD_{P}(P_{n})^{2}=\|_{n}\|_{(k)}^{2}\), and noting that the mean square of the limiting Gaussian random variable is \(()\), a natural idea is to select the sampling distribution \(\) such that \(()\) is minimised.

Fortunately, the trace of \(\) can be explicitly computed. It simplifies presentation to restrict attention to a Stein kernel \(k=k_{P}\), for which \(_{P}=0\), giving \(()=(P/)(x)^{2}k_{P}(x)\; (x)\), where for convenience we have let \(k_{P}(x):=k_{P}(x,x)\). Assuming that \(P\) and \(\) admit densities \(p\) and \(\) on \(=^{d}\), the variational problem we wish to solve is

\[*{arg\,min}_{}}{(x)}k_{P}(x )\;x(x)\;x=1,\] (7)

where \(\) be the set of positive measures on \(^{d}\) for which (S1-2) are satisfied. To solve this problem, we first relax the constraints (S1-2) and solve the relaxed problem using the Euler-Lagrange equations, which yield

\[(x) p(x)(x)}.\] (8)

Note that the normalisation constant of \(\) is \(C_{2}\) from (1), whose existence we assumed. Then we verify that (S1-2) in fact hold for this choice of \(\). Indeed,

\[ =P}{}(x)^{2}\;(x)=C _{2}(x)}\;(x)}{C_{1}^{2}}<\] \[ =P}{}(x)^{2}k_{P}(x)\; (x)=C_{2}(x)}\;P(x)=C_{2}^{2}<,\]which shows that we have indeed solved (7). The sampling distribution \(\) we have obtained is characterised up to a normalisation constant in (8), so just like \(P\) we can sample from \(\) using techniques such as MCMC. It is interesting to note that \(\) is also optimal for standard importance sampling (i.e. without self-normalisation); see Lemma 1 of Adachi et al. (2022). The Stein kernel \(k_{P}\) determines the extent to which \(\) differs from \(P\), as we illustrate next.

### Illustration

For illustration, consider the univariate target \(P\) (black curve) in Figure 1.1, a 3-component Gaussian mixture model. Our recommended choice of \(\) in (8) is shown for both the _Langevin_-Stein kernel (purple curve) and the _KGMs_-Stein kernels with \(s\{1,3\}\) (green and blue curves). The Stein discrepancy corresponding to the Langevin-Stein kernel provides control over weak convergence (i.e. convergence of integrals of functions that are continuous and bounded), while the KGMs-Stein kernel provides additional control over the convergence of polynomial moments up to order \(s\); full details about the construction of Stein kernels are contained in Appendix A. The Langevin and KGM1-Stein kernels have \(k_{P}(x) x^{2}\), while the KGM3-Stein kernel has \(k_{P}(x) x^{6}\), in each case as \(|x|\), and thus greater over-dispersion results from use of the KGM3-Stein kernel. This over-dispersion is less pronounced4 in higher dimensions; see Appendix D.1.

To illustrate the performance of Stein \(\)-Importance Sampling, we generated a sequence \((x_{n})_{n}\) of independent samples from \(\). For each \(n\{1,,100\}\), the samples \(\{x_{1},,x_{n}\}\) were assigned optimal weights \(w^{}\) by solving (3), and the associated KSD was computed. As a baseline, we performed the same calculation using independent samples from \(P\). Figure 1.2 indicates that, for both Stein kernels, substantial improvement results from the use of samples from \(\) compared to the use of samples from \(P\). Interestingly, the KGM3-Stein kernel demonstrated a larger improvement compared to the Langevin-Stein kernel, suggesting that the choice of \(\) may be more critical in settings where KSD enjoys a stronger form of convergence control.

To illustrate a posterior approximation task, consider a simple regression model \(y_{i}=f_{i}(x)+_{i}\) with \(f_{i}(x)=x_{1}(1+t_{i}x_{2})\), \(t_{i}=i-5\), \(i=1,,10\), with \(_{i}\) independent \((0,1)\). The parameter \(x=(x_{1},x_{2})\) was assigned a prior \((0,I)\). Data were simulated using \(x=(0,0)\). The posterior distribution \(P\) is depicted in the leftmost panel of Figure 2, while our choice of \(\) corresponding to the Langevin (centre left), KGM3 (centre right) and _Riemann_-Stein kernels (right) are also displayed. For the Langevin and KGM3 kernels, the associated \(\) target their mass toward regions where \(P\) varies the most. The reason for this behaviour is clearly seen for the Langevin-Stein kernel since

Figure 1: Illustrating our choice of \(\) in 1D. (a) The univariate target \(P\) (black), and our choice of \(\) based on the Langevin–Stein kernel (purple), the KGM1–Stein kernel (green), and the KGM3–Stein kernel (blue). (b) The mean kernel Stein discrepancy (KSD) for Stein \(\)-Importance Sampling using the Stein kernels from (a); in each case, KSD was computed using the same Stein kernel used to construct \(\). Solid lines indicate the baseline case of sampling from \(P\), while dashed lines indicate sampling from \(\). (The experiment was repeated 100 times and standard error bars are plotted.)

\(k_{P}(x)=c_{1}+c_{2}\| p(x)\|^{2}\) for some \(c_{1},c_{2}>0\); see Appendix C for detail. The Riemann-Stein kernel can be viewed as a preconditioned form of the Langevin-Stein kernel which takes into account the geometric structure of \(P\); see Appendix A for full detail5. Results in Figure S2 demonstrate that Stein \(\)-Importance Sampling improves upon the default Stein importance sampling method (i.e. with \(\) and \(P\) equal) for all choices of kernel.

An additional illustration involving a GARCH model with \(d=4\) parameters is presented in Appendix D.4, where the effect of varying the order \(s\) of the KGM-Stein kernel is explored.

### Theoretical Guarantees

The aim of this section is to establish when post-processing of \(\)-invariant MCMC produces a strongly consistent approximation of \(P\), for our recommended choice of \(\) in (8). Our analysis focuses on MALA (Roberts and Stramer, 2002), leveraging the recent work of Durmus and Moulines (2022) to present explicit and verifiable conditions on \(P\) for our results to hold. In fact, we consider the more general _preconditioned_ form of MALA, where the symmetric positive definite preconditioner matrix \(M\) is to be specified. Our results also allow for (optional) sparse approximation, to circumvent direct solution of (3) (c.f. Section 2.3). The resulting algorithms, which we call _Stein \(\)-Importance Sampling_ (SIIIS-MALA) and _Stein \(\)-Thinning_ (SIIT-MALA), are quite straight-forward and contained, respectively, in Algorithms 2 and 3. The linearly-constrained quadratic programme in Algorithm 2 was solved using the Python v3.10.4 packages qpsolvers v3.4.0 and proxsuite v0.3.7. While it is difficult to analyse the computational complexity associated with these methods, we believe they are at worst \(O(n^{3})\).

Figure 2: Illustrating our choice of \(\) in 2D. The bivariate target \(P\) (left), together with our choice of \(\) based on the Langevin–Stein kernel (centre left), the KGM3–Stein kernel (centre right), and the Riemann–Stein kernel (right).

Let \(A B\) indicate that \(A-B\) is a positive semi-definite matrix for \(A,B^{n n}\). For a symmetric positive definite matrix \(A\) let \(\|z\|_{A}:=A^{-1}z}\) for \(z^{d}\). Let \(C^{s}(^{d})\) denote the set of \(s\)-times continuously differentiable real-valued functions on \(^{d}\).

**Theorem 1** (Strong consistency of SIIIS- and SIIT-MALA).: _Let Assumption 1 hold where \(k=k_{P}\) is a Stein kernel, and let \(D_{P}:()[0,]\) denote the associated KSD. Assume also that_

1. \( p C^{2}(^{d})\) _with_ \(_{x^{d}}\|^{2} p(x)\|<\)__
2. \(\,b_{1}>0,B_{1} 0\) _such that_ \(-^{2} p(x) b_{1}I\) _for all_ \(\|x\| B_{1}\)__
3. \(k_{P} C^{2}(^{d})\)_._
4. \(\,0<b_{2}<2b_{1}C_{1}^{2}\)_,_ \(B_{2} 0\) _such that_ \(^{2}k_{P}(x) b_{2}I\) _for all_ \(\|x\| B_{2}\)__

_Let \(P_{n}^{*}=_{i=1}^{n}w_{i}^{*}(x_{i})\) be the result of running Algorithm 2 and let \(P_{n,m}=_{i=1}^{m}(y_{i})\) be the result of running Algorithm 3. Let \(m n\) and \(m=(( n)^{})\) for some \(>2\). Then there exists \(_{0}>0\) such that, for all step sizes \((0,_{0})\) and all initial states \(x_{0}^{d}\), \(D_{P}(P_{n}^{*}) 0,D_{P}(P_{n,m}) 0\) almost surely as \(m,n\)._

The proof is in Appendix B. Compared to earlier authors6, such as Chen et al. (2019); Riabiz et al. (2022), a major novelty here is that our assumptions are _explicit_ and can often be verified (see also Hodgkinson et al., 2020). (A2) is strong log-concavity of \(P\) when \(B_{1}=0\), while for \(B_{1}>0\) this condition is slightly stronger than the related _distant dissipativity_ condition assumed in earlier work (Gorham and Mackey, 2017; Riabiz et al., 2022). (A4) holds for the Langevin-Stein kernel (i.e. weak convergence control) and for the KGM1-Stein kernel (i.e. weak convergence control + control over first moments), but not for the higher-order KGM-Stein kernels. Extending our proof strategy to the higher-order KGM-Stein kernels would require further research into the convergence properties of MALA, and this is expected to be difficult.

## 4 Benchmarking on PosteriorDB

The area of Bayesian computation has historically lacked a common set of benchmark problems, with classical examples being insufficiently difficult and case-studies being hand-picked (Chopin and Ridgway, 2017). To introduce objectivity into our assessment, we exploited the recently released PosteriorDB benchmark (Magnusson et al., 2022). This project is an attempt toward standardised benchmarking, consisting of a collection of posteriors to be numerically approximated. Here, we systematically compared the performance of SIIIS-MALA against the default Stein importance sampling algorithm (i.e. \(=P\); denoted SIS-MALA), and also against unprocessed \(P\)-invariant MALA (i.e. uniform weights), reporting results across the breadth of PosteriorDB. The test problems in PosteriorDB are defined in the Stan probabilistic programming language, and so BridgeStan (Roualdes et al., 2023) was used to directly access posterior densities and their gradients as required. For all instances of MALA, an adaptive algorithm was used to learn a suitable preconditioner matrix \(M\) during the warm-up period; see Appendix D.3. All experiments that we report can be reproduced using code available at https://github.com/congyewang/Stein-Pi-Importance-Sampling.

Results are reported in Table 1 for \(n=3 10^{3}\) samples from MALA. These focus on the Langevin-Stein kernel, for which our theory holds, and the KGM3-Stein kernel, for which it does not. There was a significant improvement of SIIIS-MALA over SIS-MALA in 73% of test problems for the Langevin-Stein kernel and in 65% of test problems for the KGM3-Stein kernel. Compared to unprocessed MALA, a significant improvement occurred in 100% and 97% of cases, respectively for each kernel. However, the extent of improvement decreased when the dimension \(d\) of the target increased, supporting the intuition that we set out earlier and in Appendix D.1. An in-depth breakdown of results, including varying the number \(n\) of samples that were used, and the performance SIIT-MALA, can be found in Appendices D.5 and D.6.

If \(P\) and its gradients are cheap to evaluate, the computational cost of MALA is lower than that of SIS-MALA, and one could run more iterations of MALA for an equivalent computational cost. But for more complex \(P\), the computational cost of all algorithms will be gated by the number of times \(P\) and its gradients need to be evaluated, making the direct comparison in Table 1 meaningful. Further, if we aim for a compressed representation of \(P\), then some form of post-processing of MALA would be required, which would then entail an additional computational cost.

Our focus is on the development of algorithms for minimisation of KSDs; the properties of KSDs themselves are out of scope for this work7. Nonetheless, there is much interest in better understanding the properties of KSDs, and we therefore also report performance of SIIS-MALA in terms of 1-Wasserstein divergence in Appendix D.7. The main contrast between these results and the results in Table 1 is that, being score-based, KSDs suffer from the _blindness to mixing proportions_ phenomena which has previously been documented in Wenliang and Kanagawa (2021); Koehler et al. (2022); Liu et al. (2023). Caution should therefore be taken when using algorithms based on Stein discrepancies

   & &  &  \\  Task & \(d\) & MALA & SIS - & SIIS & MALA & MALA & MALA \\  earnings-cam,height & 3 & 1.41 & 0.0674 & **0.0332** & 5.33 & 0.656 & **0.181** \\ gp,pois\_regr-gp\_regr & 3 & 0.298 & 0.0436 & **0.0373** & 1.22 & 0.385 & **0.223** \\ kidiq-kidscore\_momms & 3 & 1.04 & 0.109 & **0.0941** & 4.66 & 0.848 & **0.476** \\ kidiq-kidscore\_momms & 3 & 5.03 & 0.516 & **0.358** & 25.3 & 4.86 & **1.55** \\ messetting\_logensuitz,logvolume & 3 & 1.10 & 0.179 & **0.156** & 4.97 & 1.70 & **0.844** \\ arm-arma11 & 4 & 4.47 & 1.09 & **1.01** & 26.0 & 8.91 & **6.03** \\ earnings-logensuitz,logheight\_male & 4 & 9.46 & 1.96 & **1.59** & 53.9 & 1.54 & **8.65** \\ garch\_garch11 & 4 & 0.543 & 0.159 & **0.130** & 4.70 & 1.16 & **1.01** \\ kidiq-kidscore\_mombs & 4 & 5.21 & 0.982 & **0.897** & 29.3 & 7.25 & **5.05** \\ earnings-logensuitz,function\_z & 5 & 3.09 & 1.36 & **1.33** & 19.3 & 10.4 & **8.94** \\ kidiq\_kidscore\_interaction & 5 & 7.74 & **1.65** & 1.79 & 47.8 & 13.2 & **10.1** \\ kidiq\_with\_mom\_work\_kidscore\_interaction\_c & 5 & 1.35 & **0.659** & 0.711 & 7.92 & **4.05** & 4.17 \\ kidiq\_with\_mom\_work\_kidscore\_interaction\_c & 5 & 1.38 & **0.689** & 0.699 & 8.09 & **4.24** & 4.25 \\ kidiq\_with\_mom\_work\_kidscore\_interaction\_c & 5 & 1.11 & 0.500 & **0.499** & 6.62 & **2.63** & 3.25 \\ kidiq\_with\_mom\_work\_kidscore\_non\_work & 5 & 1.07 & **0.507** & 0.545 & 6.70 & **2.63** & 3.04 \\ low\_dm\_gauss\_mix\_low\_dim\_gauss\_mix & 5 & 5.51 & 1.87 & **1.76** & 37.5 & 14.7 & **11.3** \\ mesequite\_logensuitz,logya & 5 & 1.83 & 0.821 & **0.818** & 12.6 & 5.73 & **5.59** \\ hmm\_example\_from\_example & 6 & 1.99 & 0.578 & **0.523** & 11.6 & 4.13 & **3.40** \\ shire-blr & 6 & 479 & 154 & **134** & 3300 & 1100 & **854** \\ shir-blr & 6 & 201 & 66.7 & **60.3** & 1540 & **544** & 595 \\ arK-arK & 7 & 6.87 & 3.39 & **3.16** & 60.4 & 26.4 & **23.0** \\ mesequite\_logensuitz,logyash & 7 & 1.89 & **1.18** & 1.23 & 15.5 & **8.88** & 10.1 \\ bball\_drive\_ret\_ret\_0\_\_mann\_drive\_0 & 8 & 1.15 & **0.679** & 0.698 & 5.45 & 4.72 & **3.99** \\ bball\_drive\_event\_1\_1\_htm\_drive\_1 & 8 & 42.9 & **11.9** & 12.4 & 285 & 85.6 & **67.8** \\ housen\_ly\_max\_hor\_kid\_voleira & 8 & 4.62 & 2.29 & **2.15** & 47.4 & **18.8** & 18.9 \\ mesequite-logensuitz & 8 & 1.46 & **1.00** & 1.06 & 13.3 & **8.28** & 9.14 \\ mesequite-logensuitz,logya & 8 & 2.02 & **1.31** & 1.35 & 19.2 & **10.8** & 12.2 \\ mesequite-incesuitz & 8 & 0.429 & 0.268 & **0.235** & 3.71 & **21.27** & 2.42 \\ eight\_schools\_eight\_schools\_centered & 10 & 0.526 & **0.100** & 0.182 & 7.53 & **2.15** & 215 \\ eight\_schools\_ugit\_schools\_noncentered & 10 & 0.210 & 0.137 & **0.137** & 43.6 & 28.7 & **27.5** \\ neg172\_nes & 10 & 6.16 & 3.89 & **3.45** & 72.9 & 36.24 & **3.4** \\ neg176\_nes & 10 & 6.67 & 3.86 & **3.53** & 77.5 & 35.5 & **34.4** \\ neg1980\_nes & 10 & 4.34 & 2.68 & **2.57** & 49.8 & **25.4** & 25.7 \\ neg1984-nes & 10 & 6.18 & 3.75 & **3.43** & 71.3 & 34.9 & **33.6** \\ neg1985-nes & 10 & 7.40 & 3.70 & **3.27** & 81.4 & 34.6 & **32.4** \\ neg1992-nes & 10 & 7.52 & 4.32 & **3.84** & 89.1 & 39.7 & **37.3** \\ neg1998-nes & 10 & 6.44 & 3.87 & **3.53** & 74.1 & 36.4 & **34.3** \\ neg2000-nes & 10 & 3.35 & 2.22 & **2.20** & 38.6 & **21.3** & 22.8 \\ diamonds-diamonds diamonds & 26 & 196 & 157 & **143** & 5120 & 2990 & **2620** \\ mcycle\_gp\_accel\_rg & 66 & 11.3 & **8.25** & 9.79 & 960 & **623** & 815 \ the context of posterior distributions with multiple high probability regions that are spatially separated. This is also a failure mode for MCMC algorithms such as MALA, and yet there are still many problems for which MALA has been successfully used.

The alternative choice \(_{1}\), with \(_{1}(x) p(x)^{d/(d+1)}\), which provides a generic form of over-dispersion and is optimal for approximation in 1-Wasserstein divergence (c.f. Section 2.1), was also considered. Results in Appendix D.8 indicate that, while \(_{1}\) yields an improvement compared to the baseline of using \(P\) itself, \(_{1}\) may be less effective than our proposed \(\) when \(P\) is skewed.

## 5 Discussion

This paper presented Stein \(\)-Importance Sampling; an algorithm that is simple to implement, admits an end-to-end theoretical treatment, and achieves a significant improvement over existing post-processing methods based on KSD. On the negative side, second order derivatives of the statistical model are required, and we are ultimately bound to the performance of the KSD on which Stein \(\)-Importance Sampling is based. Our analysis focused on MALA, but there is in principle no barrier to deriving sufficient conditions for consistent approximation that are applicable to other sampling algorithms, such as the unadjusted Langevin algorithm. Of course, it remains to be seen whether SIS-MALA or any of its variants will stand the test of time compared to continued development in MCMC methodology, but we believe this line of research merits further investigation. For models for which access to second order derivatives is impractical, our methodology and theoretical analysis are directly applicable to gradient-free KSD (Fisher and Oates, 2023), and this would be an interesting direction for future work. Similarly, alternatives to KSD that are better-suited to high-dimensional \(P\) could be considered, such as the _sliced_ KSD of Gong et al. (2021a,b).

AcknowledgementsCW was supported by the China Scholarship Council. HK and CJO were supported by EP/W019590/1. The authors are grateful to Francois-Xavier Briol for feedback on an earlier draft of the manuscript, and to the anonymous Reviewers for their input.