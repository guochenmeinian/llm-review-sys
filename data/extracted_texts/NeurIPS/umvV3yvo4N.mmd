# Energy-Based Sliced Wasserstein Distance

Khai Nguyen

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

khainb@utexas.edu

&Nhat Ho

Department of Statistics and Data Sciences

The University of Texas at Austin

Austin, TX 78712

minhnhat@utexas.edu

###### Abstract

The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional Wasserstein distance. We then derive a novel sliced Wasserstein variant, _energy-based sliced Wasserstein_ (EBSW) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and Markov Chain methods. Finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the EBSW1.

## 1 Introduction

The sliced Wasserstein  (SW) distance is a sliced probability metric that is derived from the Wasserstein distance  as the base metric. Utilizing the closed-form solution of optimal transport on one-dimension , the SW distance can be computed very efficiently at the time complexity of \((n n)\) and the space complexity of \((n)\) when dealing with two probability measures that have at most \(n\) supports. Moreover, the sample complexity of the SW is only \((n^{-1/2})\) which indicates that it does not suffer from the curse of dimensionality in statistical inference. Therefore, the SW distance has been applied to various domains of applications including point-cloud applications e.g., reconstruction, registration, generation, and upsampling , generative models , domain adaptation , clustering , gradient flows , approximate Bayesian computation , variational inference , and many other tasks.

The SW distance can be defined as the expectation of the one-dimensional Wasserstein distance between two projected probability measures. The randomness comes from a random projecting direction which is used to project two original probability measures to one dimension. The probability distribution of the random projecting direction is referred to as the slicing distribution. Therefore, a central task that decides the effectiveness of the SW in downstream applications is designingslicing distribution. The conventional sliced Wasserstein distance  simply takes the uniform distribution over the unit-hypersphere as the slicing distribution. Despite being easy to sample from, the uniform distribution is not able to differentiate between informative and non-informative projecting distributions in terms of discriminating two interested probability measures through projection . To avoid a flat prior distribution like the uniform distribution, a different approach tries to find the best slicing distribution that can maximize the expectation. This distribution is found inside a parametric family of distribution over the unit-hypersphere . However, searching for the best slicing distribution often requires an iterative procedure which is often computationally expensive and unstable. Moreover, choosing the family for the slicing distribution is challenging since the number of distributions over the unit-hypersphere is limited. Widely used and explicit spherical distributions such as von Mises-Fisher distribution  might be misspecified while implicit distributions  are expensive, hard to adapt to downstream applications and uninterpretable.

In this paper, we aim to develop new choices of slicing distributions that are both discriminative in comparing two given probability measures and do not require optimization. Motivated by energy-based models , we model the slicing distribution by an unnormalized density function which gives a higher density for a more discriminative projecting direction. To induce that property, the density function at a projecting direction is designed to be proportional to the value of the one-dimensional Wasserstein distance between the two corresponding projected probability measures.

**Contribution.** In summary, our contributions are three-fold:

1. We propose a new class of slicing distribution, named _energy-based slicing_ distribution, which has the density function proportional to the value of the projected one-dimensional Wasserstein distance. We further control the flexibility of the slicing distribution by applying an energy function e.g., the polynomial function, and the exponential function, to the projected Wasserstein value. By using the energy-based slicing distribution, we derive a novel metric on the space of probability measures, named _energy-based sliced Wasserstein_ (EBSW) distance.

2. We derive theoretical properties of the proposed EBSW including topological properties, statistical properties, and computational properties. For topological properties, we first prove that the EBSW is a valid metric on the space of probability measures. After that, we show that the weak convergence of probability measures is equivalent to the convergence of probability measures under the EBSW distance. Moreover, we develop the connection of the EBSW to existing sliced Wasserstein variants and the Wasserstein distance. We show that the EBSW is the first non-optimization variant that is an upper bound of the sliced Wasserstein distance. For the statistical properties, we first derive the sample complexity of the EBSW which indicates that it does not suffer from the curse of dimensionality. For computational properties, we propose importance sampling, sampling importance resampling, and Markov Chain Monte Carlo methods to derive empirical estimations of the EBSW. Moreover, we discuss the time complexities and memory complexities of the corresponding estimations. Finally, we discuss the statistical properties of estimations.

3. We apply the EBSW to various tasks including gradient flows, color transfer, and point-cloud applications. According to the experimental result, the EBSW performs better than existing _projection-selection_ sliced Wasserstein variants including the conventional sliced Wasserstein  (SW), max sliced Wasserstein  (Max-SW), and distributional sliced Wasserstein (DSW) . More importantly, the importance sampling estimation of the EBSW is as efficient and easy to implement as the conventional SW, i.e., its implementation can be obtained by adding one to two lines of code.

**Organization.** The remainder of the paper is organized as follows. We first review the background on the sliced Wasserstein distance and its projection-selection variants in Section 2. We then define the energy-based sliced Wasserstein distance, derive their theoretical properties, and discuss its computational methods in Section 3. Section 4 contains experiments on gradient flows, color transfer, and point-cloud applications. We conclude the paper in Section 5. Finally, we defer the proofs of key results, and additional materials in the Appendices.

**Notations.** For any \(d 2\), we denote \(^{d-1}:=\{^{d}||||_{2}^{2}=1\}\) and \((^{d-1})\) as the unit hyper-sphere and its corresponding uniform distribution. We denote \(()\) as the set of all probability measures on the set \(\). For \(p 1\), \(_{p}()\) is the set of all probability measures on the set \(\) that have finite \(p\)-moments. For any two sequences \(a_{n}\) and \(b_{n}\), the notation \(a_{n}=(b_{n})\) means that \(a_{n} Cb_{n}\) for all \(n 1\), where \(C\) is some universal constant. We denote \(\) is the push-forward measures of \(\)through the function \(f:^{d}\) that is \(f(x)=^{}x\). For a vector \(X^{dm}\), \(X:=(x_{1},,x_{m})\), \(P_{X}\) denotes the empirical measures \(_{i=1}^{m}_{x_{i}}\).

## 2 Background

In this section, we first review the sliced Wasserstein distance and its projection-selection variants including max sliced Wasserstein distance and distributional sliced Wasserstein distance.

**Sliced Wasserstein.** The definition of sliced Wasserstein (SW) distance  between two probability measures \(_{p}(^{d})\) and \(_{p}(^{d})\) is:

\[_{p}(,)=(_{(^{d- 1})}[_{p}^{p}(,)])^{},\] (1)

where the Wasserstein distance has a closed form which is \(_{p}^{p}(,)=_{0}^{1}|F_{ }^{-1}(z)-F_{}^{-1}(z)|^{p}dz\) where \(F_{}\) and \(F_{}\) are the cumulative distribution function (CDF) of \(\) and \(\) respectively. However, the expectation in the definition of the SW distance is intractable to compute. Therefore, the Monte Carlo scheme is employed to approximate the value:

\[}_{p}(,;L)=(_{l=1}^{L}_{p }^{p}(_{l},_{l}))^{},\] (2)

where \(_{1},,_{L}}{{}}(^{d-1})\) and are referred to as projecting directions. The pushfoward measures \(_{1},,_{L}\) are called projections of \(\) (similar to \(\)). The number of Monte Carlo samples \(L\) is often referred to as the number of projections. When \(\) and \(\) are discrete measures that have at most \(n\) supports, the time complexity and memory complexity of the SW are \((Ln n)\) and \((L(d+n))\) respectively. It is worth noting that \(}_{p}^{p}(,;L)\) is an unbiased estimation of \(_{p}^{p}(,)\), however, \(}_{p}(,;L)\) is only asymptotically unbiased estimation of \(_{p}(,)\). Namely, we have \(}_{p}(,;L)_{p}(,)\) when \(L\) (law of large numbers).

**Distributional sliced Wasserstein.** As discussed, using the uniform distribution over projecting directions is not suitable for two general probability measures. A natural extension is to replace the uniform distribution with a "better" distribution on the unit-hypersphere. Distributional sliced Wasserstein  suggests searching this distribution in a parametric family of distributions by maximizing the expected distance. The definition of distributional sliced Wasserstein (DSW) distance  between two probability measures \(_{p}(^{d})\) and \(_{p}(^{d})\) is:

\[_{p}(,)=_{}(_{ _{}()}[_{p}^{p}(,)])^{ },\] (3)

where \(_{}()(^{d-1})\), e.g., von Mises-Fisher  distribution with unknown location parameter \(_{}():=(|,)\), \(=\). After using \(T 1\) (projected) stochastic (sub)-gradient ascent iterations to obtain an estimation of the parameter \(_{T}\), Monte Carlo samples \(_{1},,_{L}}{{}}_{ _{T}}()\) are used to approximate the value of the DSW. Interestingly, the metricity DSW holds for non-optimal \(_{T}\) as long as \(_{_{T}}()\) are continuous on \(^{d-1}\) e.g., vMF with \(<\). In addition, the unbiasedness property of the DSW is the same as the SW, namely, when \(L\), the empirical estimation of the DSW converges to the true value. The time complexity and space complexity of the DSW are \((LTn n)\) and \((L(d+n))\) in turn without counting the complexities of sampling from \(_{_{T}}()\). We refer to Appendix B.1 for more details, e.g., equations, algorithms, and discussion. PAC-Bayesian generalization bounds for DSW are investigated in .

**Max sliced Wasserstein.** By letting the concentration parameter \(\), the vMF distribution degenerates to the Dirac distribution \((|,)_{}\), we obtain the max sliced Wasserstein distance . The definition of max sliced Wasserstein (Max-SW) distance between two probability measures \(_{p}(^{d})\) and \(_{p}(^{d})\) is:

\[_{p}(,)=_{^{d-1}}_{p}( ,).\] (4)

Similar to the DSW, the Max-SW is often computed by using \(T 1\) iterations of (projected) (sub)-gradient ascent to obtain an estimation of the "max" projecting direction \(_{T}\). After that, the estimated value of the Max-SW is \(_{p}(_{T},_{T})\). The time complexity and space complexity of the Max-SW are \((Tn n)\) and \((d+n)\). It is worth noting that the Max-SW is only a metric at the global optimum \(^{}\), hence, we cannot guarantee the metricity of the Max-SW due to the non-convex optimization  problem even when \(T\). Therefore, the performance of Max-SW is often unstable in practice . We refer the reader to Appendix B.1 for more details e.g., equations, algorithms, and discussions about the Max-SW.

## 3 Energy-Based Sliced Wasserstein Distance

From the background, we observe that using a fixed slicing distribution e.g., in the SW, is computationally efficient but might be not effective. In contrast, using optimization-based slicing distributions is computationally expensive e.g., in the DSW, and is unstable e.g., in the Max-SW. Therefore, we address previous issues by introducing a novel sliced Wasserstein variant that uses optimization-free slicing distribution which can highlight the difference between two comparing probability measures.

### Energy-Based Slicing Distribution

We first start with the key contribution which is the energy-based slicing distribution.

**Definition 1**.: _For any \(p 1\), dimension \(d 1\), an energy function \(f:[0,)(0,)\) and two probability measures \(_{p}(^{d})\) and \(_{p}(^{d})\), the energy-based slicing distribution \(_{,}()\) supported on \(^{d-1}\) is defined as follow:_

\[_{,}(;f,p) f(W^{p}_{p}(^{} ,^{})):=_{p}(^{}_{}, ^{}))}{_{^{d-1}}f(W^{p}_{p}(^{}, ^{}))d},\] (5)

_where the image of \(f\) is in the open interval \((0,)\) is for making \(_{,}()\) continuous on \(^{d-1}\)_

In contrast to the approach of the DSW which creates the dependence between the slicing distribution and two input probability measures via optimization, the energy-based slicing distribution obtains the dependence by exploiting the value of the projected Wasserstein distance between two input probability measures at each support.

**Monotonically increasing energy functions.** Similar to previous works, we again assume that "_A higher value of projected Wasserstein distance, a better projecting direction"_. Therefore, it is natural to use a monotonically increasing function for the energy function \(f\). We consider the following two functions: the exponential function: \(f_{e}(x)=e^{x}\), and the shifted polynomial function: \(f_{q}(x)=x^{q}+\) with \(q,>0\). The shifted constant \(\) helps to avoid the slicing distribution undefined when two input measures are equal. In a greater detail, when \(=\), we have \(^{p}_{p}(^{},^{})=0\) for all \(^{d-1}\) due to the identity property of the Wasserstein distance. Hence, \(_{,}(;f,p) 0\) for \(^{d-1}\) and \(f(x)=x^{q}\) (\(q>0\)). Therefore, the slicing distribution \(_{,}(;f,p)\) is undefined due to an invalid density function. In practice, it is able to set \(=0\) since we rarely deal with two coinciding measures.

**Other energy functions.** We can choose any positive function for energy function \(f\) and it will result in a valid slicing distribution. However, it is necessary to come up with an assumption for the choice of the function. Since there is no existing other assumption for the importance of projecting direction, we will leave the investigation of non-increasing energy function \(f\) to future works.

**Example 1**.: _Let \(=(,v_{1}^{2})\) and \(=(,v_{2}^{2}))\) are two non-scale Gaussian distributions with the same means, we have their projections are \(=(^{},v_{1}^{2})\) and \(=(^{},v_{2}^{2})\). Based on the closed form of the Wasserstein distance between two Gaussians , we have \(^{2}_{2}(,)=(v_{1}-v_{2})^{2}\) for all \(^{d-1}\) which leads to \(_{,}(;f,p)=(^{d-1})\) for definitions of energy function \(f\) in Definition 1._

Example 1 gives a special case where we can have the closed form of the slicing function.

**Applications to other sliced probability metrics and mutual information.** In this paper, we focus on comparing choices of slicing distribution in the basic form of the SW distance. The proposed energy-based slicing distribution can be adapted to other variants of the SW that are not about designing slicing distribution e.g., non-linear projecting , orthogonal projecting directions , and so on. Moreover, the energy-based slicing approach can be applied to other sliced probability metrics e.g., sliced score matching , and sliced mutual information .

### Definitions, Topological, and Statistical Properties of Energy Based Sliced Wasserstein

With the definition of energy-based slicing distribution in Definition 1, we now are able to define the energy-based sliced Wasserstein (EBSW) distance.

**Definition 2**.: _For any \(p 1\), dimension \(d 1\), two probability measures \(_{p}(^{d})\) and \(_{p}(^{d})\), the energy function \(f:[0,)(0,)\), and the energy-based slicing distribution \(_{,}(;f,p)\), the energy-based sliced Wasserstein (EBSW) distance is defined as follows:_

\[_{p}(,;f)=(_{_{,}( ;f,p)}[W_{p}^{p}(,)])^{}.\] (6)

We now derive some theoretical properties of the EBSW distance.

**Topological Properties.** We first investigate the metricity of the EBSW distance.

**Theorem 1**.: _For any \(p 1\), energy-function \(f\), the energy-based sliced Wasserstein \(_{p}(,;f)\) is a semi-metric in the probability space on \(^{d}\), namely EBSW satisfies non-negativity, symmetry, and identity of indiscernibles._

The proof of Theorem 1 in given in Appendix A.1. Next, we establish the connections among the EBSW, the SW, the Max-SW, and the Wasserstein.

**Proposition 1**.: _(a) For any \(p 1\) and increasing energy function \(f\), we find that_

\[_{p}(,)_{p}(,;f).\]

_The equality holds when \(f(x)=c\) for some positive constant \(c\) for all \(x[0,)\)._

_(b) For any \(p 1\) and energy function \(f\), we have_

\[_{p}(,;f)_{p}(,) W_{p}(,).\]

Proof of Proposition 1 is in Appendix A.2. The results of Proposition 1 indicate that for increasing energy function \(f\), the EBSW is lower bounded by the SW while it is upper bounded by the Max-SW. It is worth noting that the EBSW is the first variant that changes the slicing distribution while still being an upper bound of the SW.

**Theorem 2**.: _For any \(p 1\) and energy function \(\), the convergence of probability measures under the energy-based sliced Wasserstein distance \(_{p}(,;)\) implies weak convergence of probability measures and vice versa._

Theorem 2 implies that for any sequence of probability measures \((_{k})_{k}\) and \(\) in \(_{p}(^{d})\), \(_{k+}_{p}(_{k},;)=0\) if and only if for any continuous and bounded function \(f:^{d}\), \(_{k+}_{k}= f\ \). The proof of Theorem 2 is in Appendix A.3.

**Statistical Properties.** From Proposition 1, we derive the sample complexity of the EBSW.

**Proposition 2**.: _Let \(X_{1},X_{2},,X_{n}\) be i.i.d. samples from the probability measure \(\) being supported on compact set of \(^{d}\). We denote the empirical measure \(_{n}=_{i=1}^{n}_{X_{i}}\). Then, for any \(p 1\) and energy function \(f\), there exists a universal constant \(C>0\) such that_

\[[_{p}(_{n},;f)] C,\]

_where the outer expectation is taken with respect to the data \(X_{1},X_{2},,X_{n}\)._

The proof of Proposition 2 is given in Appendix A.4. From this proposition, we can say that the EBSW does not suffer from the curse of dimensionality. We will discuss other statistical properties of approximating the EBSW in the next section.

### Computational Methods and Computational Properties

Calculating the expectation with respect to the slicing distribution \(_{,}(;f,p)\) is intractable. Therefore, we propose some Monte Carlo estimation methods to approximate the value of EBSW.

#### 3.3.1 Importance Sampling

The most simple and computationally efficient method that can be used is importance sampling (IS) . The idea is to utilize an efficient-sampling proposal distribution \(_{0}()(^{d-1})\) to provide Monte Carlo samples. After that, we use the density ratio between the original slicing distribution and the proposal distribution to weight samples. We can rewrite the EBSW distance as:

\[_{p}(,;f)=(_{_{0}( )}[_{p}^{p}(,)w_{,,_{0},f,p}()]}{_{_{0}()}[w_{, ,_{0},f,p}()]})^{},\] (7)

where \(_{0}()(^{d-1})\) is the proposal distribution, and:

\[w_{,,_{0},f,p}()=_{p}^{p}(, ))}{_{0}()}\]

is the importance weighted function. The detailed derivation is given in Appendix B.2. Let \(_{1},,_{L}\) be i.i.d samples from \(_{0}()\), the importance sampling estimator of the EBSW (IS-EBSW) is:

\[}_{p}(,;f,L)=(_{l=1}^{L}[ _{p}^{p}(,)_{,,_{0},f,p}( _{l})])^{},\] (8)

where \(_{,,_{0},f,p}(_{l})=,f,p}( _{l})}{_{l^{}=1}^{L}w_{,,_{0},f,p}(_{l^{ }})}\) is the normalized importance weights. When \(_{0}()=(^{d-1})=}\) (a constant of \(\), we can replace \(w_{,,_{0}}(_{l})\) with \(f(_{p}^{p}(,))\). When we choose the energy function \(f(x)=e^{x}\), computing the normalized importance weights

\[_{,,_{0},f,p}(_{l})=,f,p} (_{l})}{_{l^{}=1}^{L}w_{,,_{0},f}(_{l^{ }})}\]

is equivalent to computing the Softmax function.

**Computational algorithms and complexities.** The computational algorithm of IS-EBSW can be derived from the algorithm of the SW distance by adding only one to two lines of code for computing the importance weights. For a better comparison, we give algorithms for computing the SW distance and the EBSW distance in Algorithm 1 and Algorithm 4 in Appendix B.1 and Appendix B.2 respectively. When \(\) and \(\) are two discrete measures that have at most \(n\) supports, the time complexity and the space complexity of the IS-EBSW distance are \((Ln n+Lnd)\) and \((L(n+d))\) which are the same as the SW.

**Unbiasedness.** The IS approximation is asymptotically unbiased for \(_{p}^{p}(,;f)\). However, having a biased estimation is not severe since the unbiasedness cannot be preserved after taking the \(p\)-tooth (\(p>1\)) like in the case of the SW distance. Therefore, an unbiased estimation of \(_{p}^{p}(,;f)\) is not very vital. Moreover, we can show that \(_{p}^{p}(,;f,L)\) is an unbiased estimation of the power \(p\) of a valid distance. We refer the reader to Appendix B.2 for the detailed definition and properties of the distance. From this insight, it is safe to use the IS-EBSW in practice.

**Gradient Estimation.** In statistical inference, we might want to estimate the gradient \(_{}_{p}(_{},;f)\) for doing minimum distance estimator . Therefore, we derive the gradient estimator of the EBSW with importance sampling in Appendix B.2.

#### 3.3.2 Sampling Importance Resampling and Markov Chain Monte Carlo

The second approach is to somehow sample from the slicing distribution \(_{,}(;f,p)\). For example, when we have \(_{1},,_{L}\) are approximately distributed following \(_{,}(;f,p)\), we can take \((_{l=1}^{L}_{p}^{p}(_{l},_{l} ))^{}\) as the approximated value of the EBSW. Here, we consider two famous approaches in statistics: Sampling Importance Resampling  (SIR) and Markov Chain Monte Carlo (MCMC). For MCMC, we utilize two variants of the Metropolis-Hasting algorithm: independent Metropolis-Hasting (IMH) and random walk Metropolis-Hasting (RMH).

**Sampling Importance Resampling.** Similar to importance sampling in Section 3.3.1, the SIR uses a proposal distribution \(_{0}()\) to obtain \(L\) samples \(_{1}^{},,_{L}^{}\) and the corresponding normalizedimportance weights:

\[_{,,_{0},f,p}(^{}_{l})=,f,p}(^{}_{l})}{_{i=1}^{L}w_{,,_{0},f,p}(^{ }_{i})}.\]

After that, the SIR creates the resampling distribution which is a Categorical distribution \(()=_{l=1}^{L}_{,,_{0},f,p}(_{l}) _{^{}_{l}}\). Finally, the SIR draws \(L\) samples \(_{1},,_{L}}{{}}()\). We denote the SIR estimation of the EBSW distance as SIR-EBSW.

**Markov Chain Monte Carlo.** MCMC creates a Markov chain that has the stationary distribution as the target distribution. The most famous way to construct such a Markov chain is through the Metropolis-Hastings algorithm. Let the starting sample follow a prior distribution \(_{1}_{0}()(^{d-1})\), a transition distribution \(_{t}(_{t}|_{t-1})(^{d-1})\) for any timestep \(t>1\) is used to sample a candidate \(^{}_{t}\). After that, the new sample \(_{t}\) is set to \(^{}_{t}\) with the probability \(\) and is set to \(_{t-1}\) with the probability \(1-\) with

\[=(1,(^{}_{t};f)}{_{,}(_{t-1};f)}(_{t-1}|^{}_{t})}{ _{t}(^{}_{t}|_{t-1})})=(1,^{p}_{p}(^{}_{t},^{}_{t} )))}{f(^{p}_{p}(_{t-1},_{t-1}))} (_{t-1}|^{}_{t})}{_{t}(^{ }_{t}|_{t-1})}).\]

In theory, \(T\) should be large enough to help the Markov chain to mix to the stationary distribution and the first \(M<T\) samples are often dropped as burn-in samples. However, to keep the computational complexity the same as the previous computational methods, we set \(T=L\) and \(M=0\). The first choice of transition distribution is \(_{t}(_{t}|_{t-1})=(^{d-1})\) which leads to independent Metropolis-Hasting (IMH). The second choice is \(_{t}(_{t}|_{t-1})=(_{t}|_{t-1},)\) (the von Mises-Fisher distribution  with location \(_{t-1}\)) which leads to random walk Metropolis-Hasting (RMH). Since both above transition distributions are symmetric \(_{t}(_{t}|_{t-1})=_{t}(_{t-1}|_{t})\), the acceptance probability turns into:

\[=(1,^{p}_{p}(^{}_{t}, ^{}_{t})))}{f(^{p}_{p}(_{t-1}, _{t-1})))})\]

which means that the acceptance probability equals \(1\) and \(^{}_{t}\) is always accepted as \(_{t}\) if it can increase the energy function. We refer to the IMH estimation and the RMH estimation of the EBSW distance as IMH-EBSW and RMH-EBSW in turn.

**Computational algorithms and complexities.** We refer the reader to Algorithm 5, Algorithm 6, and Algorithm 7 in Appendix B.3 for the detailed algorithms of the SIR-EBSW, the IMH-EBSW, and the RMH-EBSW. Without counting the complexity of the sampling algorithm, the time complexity and the space complexity of both the SIR and the MCMC estimation of EBSW are \((Ln n+Lnd)\) and \((L(n+d))\) which are the same as the IS-EBSW and the SW distance. However, the practical computational time and memory of the SIR and the MCMC estimation depend on the efficiency of implementing the sampling algorithm e.g., resampling and acceptance-rejection.

**Unbiasedness.** The SIR and the MCMC sampling do not give an unbiased estimation for EBSW\({}^{p}_{p}(,;f)\). However, they are also unbiased estimations of the power \(p\) of a valid distance. We refer to Appendix B.3 for detailed definitions and properties of the distance. Therefore, it is safe to use the approximation from the SIR and the MCMC.

**Gradient Estimation:** In the IS estimation, the expectation is with respect to the proposal distribution \(_{0}()\) that does not depend on two input measures \(_{}\). In the SIR estimation and the MCMC estimation, the expectation is with respect to the slicing distribution \(_{_{},}(,f)\) that depends on \(_{}\). Therefore, the log-derivative trick (Reinforce) should be used to derive the gradient estimation. We give the detailed derivation in Appendix B.3. However, the log-derivative trick is often unstable in practice. A simpler solution is to create the slicing distribution from an independent copy of \(_{}\). In particular, we denote \(_{^{}}\) is the independent copy of \(_{}\) with \(^{}\) equals \(\) in terms of value. Therefore, we can obtain the slicing distribution \(_{_{^{}},}(;f)\) that does not depend on \(_{}\). This approach still gives the same value of distance, we refer to Appendix B.3 for a more careful discussion.

## 4 Experiments

In this section, we first visualize the shape of the energy-based slicing distribution in a simple case. After that, we focus on showing the favorable performance of the EBSW compared to the other sliced Wasserstein variants in point-cloud gradient flows, color transfer, and deep point-cloud reconstruction.

In experiments, we denote EBSW-e for the exponential energy function i.e., \(f(x)=e^{x}\), and EBSW-1 for the identity energy function i.e., \(f(x)=x\). We use \(p=2\) for all sliced Wasserstein variants.

### Visualization of energy-based slicing distribution

We visualize the shape of the energy-based slicing distribution in two dimensions in Figure 1. In particular, we consider comparing two empirical distributions in the left-most figure (taken from ). We utilize the SIR, the IMH, and the RMH to obtain \(10^{4}\) Monte Carlo samples from the energy-based slicing distribution. For the IMH and the RHM, we burn in the \(10^{4}\) samples. After that, we use the von Mises kernel density estimation to obtain the density function. We also present the ground-truth density of the energy-based slicing distribution by uniform discretizing the unit-sphere. Moreover, we also show the optimal vMF distribution from the DSW (tuning \(\{1,5,10,50,100\}\)) and the "max" projecting direction from the Max-SW (\(T=100\), step size \(0.1\)). The middle figure is according to the energy function \(f_{1}(x)=x\) and the right-most figure is according to the energy function \(f_{e}(x)=e^{x}\). From the figures, we observe that all sampling methods can approximate well the true slicing distribution. In contrast, the vMF distribution from v-DSW is misspecified to approximate the energy distribution, and the "max" projecting direction from the Max-SW can capture only one mode. We also observe that the exponential energy function makes the density more concentrated around the modes than the identity energy function (polynomial of degree 1).

### Point-Cloud Gradient Flows

We model a distribution \((t)\) flowing with time \(t\) along the gradient flow of a loss functional \((t)((t),)\) that drives it towards a target distribution \(\) where \(\) is our sliced Wasserstein variants. We consider discrete flows, namely. we set \(=_{i=1}^{n}_{Y_{i}}\) as a fixed empirical target distribution and the model distribution \((t)=_{i=1}^{n}_{X_{i}(t)}\). Here, the model distribution is parameterized by a time-varying point cloud \(X(t)=(X_{i}(t))_{i=1}^{n}(^{d})^{n}\). Starting from an initial condition at time \(t=0\), we integrate the ordinary differential equation \((t)=-n_{X(t)}[(_{i=1}^{n} _{X_{i}(t)},)]\) for each iteration. We choose \((0)\) and \(\) are two point-cloud shapes in ShapeNet Core-55 dataset . After that, we solve the flows by using the Euler scheme with \(500\) iterations and step size \(0.0001\).

**Quantitative Results.** We show the Wasserstein-2 distance between \((t)\) and \(\) (\(t\{0,100,200,300,400,500\}\)) and the computational time of the SW variants in Table 1. Here, we set \(L=100\) for SW, and EBSW variants. For the Max-SW we set \(T=100\), and report the best result for the step size for finding the max projecting direction in \(\{0.001,0.01,0.1\}\). For the v-DSW, we report the best result for \((L,T)\{(10,10),(50,2),(2,50)\}\), \(\{1,10,50\}\), and the learning rate for finding the location in \(\{0.001,0.01,0.1\}\). We observe that the IS-EBSW-e helps to drive the flow to converge faster than baselines. More importantly, the computational time of the IS-EBSW-e is approximately the same as the SW and is faster than both the Max-SW and the v-DSW. We report more detailed experiments with other EBSW variants and different settings of hyperparameters \((L,T)\) in Table 3 in Appendix C.1. From the additional experiments, we see that the EBSW-e variants give lower Wasserstein-2 distances than the baseline with the same scaling of complexity (same \(L\)/\(T\)). Despite having comparable performance, the SIR-EBSW-e, the IMH-EBSW-e, and the RMH-EBSW-e

Figure 1: Visualization of the true and the sampled energy-based slicing distributions, the optimal vMF distribution from the v-DSW, and the max projecting direction from the Max-SW.

(\(=10\)) are slower than the IS-EBSW-e variant. Also, we see that the EBSW-e variants are better than the EBSW-1 variants. We refer to Table 4 for comparing gradient estimators of the EBSW.

**Qualitative Results.** We show the point-cloud flows of the SW, the Max-SW, the v-DSW, and the IS-EBSW-e, in Figure 2. The flows from the SIR-EBSW-e, the IMG-EBSW-e, and the RMH-EBSW-e are added in Figure 4 in the Appendix C.1. From the figure, the transitions of the flows from the EBSW-e variants are smoother to the target than other baselines.

### Color Transfer

We build a gradient flow that starts from the empirical distribution over the normalized color palette (RGB) of the source image to the empirical distribution over the normalized color palette (RGB) of the target image. Since the value of the color palette is in the set \(\{0,,255\}^{3}\), we must do an additional rounding step at the final step of the Euler scheme with 2000 steps and step size \(0.0001\).

**Results.** We use the same setting for the SW variants as in the previous section. We show both transferred images, corresponding computational times, and Wasserstein-2 distances in Figure 3. We observe the same phenomenon as in the previous section, namely, the IS-EBSW-e variants perform the best in terms of changing the color of the source image to the target in the Wasserstein-2 metric while the computational time is only slightly higher. Moreover, the transferred image from the IS-EBSW-e is visually more similar to the target image in color than other baselines, namely, it has a less orange color. We refer to Figure 5 in Appendix C.2 for additional experiments including the results for the SIR-EBSW-e, the IMH-EBSW-e, the RMH-EBSW-e, the results for the identity energy function, the results for changing hyperparamters \((L,T)\), and the results for comparing gradient estimators. Overall, we observe similar phenomenons as in the previous gradient flow section.

### Deep Point-Cloud Reconstruction

We follow  to train point-cloud autoencoders with sliced Wasserstein distances on the ShapeNet Core-55 dataset . In short, we aim to estimate an autoencoder that contains an encoder \(f_{}\) that maps a point cloud \(X^{nd}\) to a latent code \(z^{h}\), and a decoder \(g_{}\) that maps the latent

   Distances & Step 0 (W\({}_{2}\) \(\)) & Step 100 (W\({}_{2}\) \(\)) & Step 200 (W\({}_{2}\) \(\)) & Step 300 (W\({}_{2}\) \(\)) & Step 400(W\({}_{2}\) \(\)) & Step 500 (W\({}_{2}\) \(\)) & Time (s \(\)) \\  SW & \(2048.29 0.0\) & \(986.93 9.55\) & \(350.66 5.32\) & \(99.69 1.85\) & \(27.03 0.65\) & \(9.41 0.27\) & \(\) \\ Max-SW & \(2048.29 0.0\) & \(506.56 9.28\) & \(93.54 3.39\) & \(22.2 0.79\) & \(9.62 0.22\) & \(6.83 0.22\) & \(28.38 0.05\) \\ v-DSW & \(2048.29 0.0\) & \(649.33 8.77\) & \(127.4 5.06\) & \(29.44 1.25\) & \(10.95 1.0\) & \(5.68 0.56\) & \(21.2 0.02\) \\ IS-EBSW-e & \(2048.29 0.0\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(17.63 0.02\) \\   

Table 1: Summary of Wasserstein-2 scores  (multiplied by \(10^{4}\)) from three different runs, computational time in second (s) to reach step 500 of different sliced Wasserstein variants in gradient flows.

Figure 2: Gradient flows from the SW, the Max-SW, the v-DSW, and the IS-EBSW-e in turn.

code \(z\) to a reconstructed point cloud \(^{nd}\). We want to have the pair \(f_{}\) and \(g_{}\) such that \(=g_{}(f_{}(X)) X\) for all \(X p(X)\) which is our data distribution. To do that, we solve the following optimization problem: \(_{_{i},}_{X(X)}[(P_{X},P_{g_{ }(f_{}(X))}]]\), where \(\) is a sliced Wasserstein variant, and \(P_{X}\) denotes the empirical distribution over the point cloud \(X\). The backbone for the autoencoder is a variant of Point-Net  with an embedding size of 256. We train the autoencoder for 200 epochs using an SGD optimizer with a learning rate of 1e-3, a batch size of 128, a momentum of 0.9, and a weight decay of 5e-4. We give more detail in Appendix C.3

**Quantitative Results.** We evaluate the trained autoencoders on a different dataset: ModelNet40 dataset  using two distances: sliced Wasserstein distance (\(L=1000\)), and the Wasserstein distance. We follow the same hyper-parameters settings as the previous sections and show the reconstruction errors at epochs 20, 100, and 200 from the SW, the Max-SW, the DSW, and the IS-EBSW-e in Table 2. The reconstruction errors are the average of corresponding distances on all point-clouds. From the table, we observe that the IS-EBSW-e can help to train the autoencoder faster in terms of the Wasserstein distance and the sliced Wasserstein distance. We refer to Table 5 in Appendix C.3 for similar ablation studies as in the previous sections including the results for the SIR-EBSW-e, the IMH-EBSW-e, the RMH-EBSW-e, the results for the identity energy function, the results for changing hyperparameters \((L,T)\), and the results for comparing gradient estimators.

**Qualitative Results.** We show some ground-truth point-clouds ModelNet40 and their corresponding reconstructed point-clouds from different models (\(L=100\)) at epochs 200 and 20 in Figure 6- 7 respectively. Overall, the qualitative results are visually consistent with the quantitative results.

## 5 Limitations and Conclusion

**Limitations.** The first limitation of EBSW is that its MCMC variants are not directly parallelizable, resulting in slow computation. Additionally, a universal effective choice of the energy-based function is an open question. In the paper, we use simple and computationally effective energy-based functions, such as the exponential function and the polynomial function. Finally, proving the triangle inequality of EBSW is challenging due to the expressiveness of the energy-based slicing distribution.

**Conclusion.** We have presented a new variant of sliced Wasserstein distance, named energy-based sliced Wasserstein (EBSW) distance. The key ingredient of the EBSW is the energy-based slicing distribution which has a density at a projecting direction proportional to an increasing function of the one-dimensional Wasserstein value of that direction. We provide theoretical properties of the EBSW including the topological properties, and statistical properties. Moreover, we propose to compute the EBSW with three different techniques including importance sampling, sampling importance resampling, and Markov Chain Monte Carlo. Also, we discuss the computational properties of different techniques. Finally, we demonstrate the favorable performance of the EBSW compared to existing projecting directions selection sliced Wasserstein variants by conducting experiments on point-cloud gradient flows, color transfer, and deep point-cloud reconstruction.

    &  &  &  \\   & SW\({}_{2}\)(\(\)) & W\({}_{2}\)(\(\)) & SW\({}_{2}\) (\(\)) & W\({}_{2}\)(\(\)) & SW\({}_{2}\) (\(\)) & W\({}_{2}\)(\(\)) \\  SW & \(2.97 0.14\) & \(12.67 0.18\) & \(2.29 0.04\) & \(10.63 0.05\) & \(2.15 0.04\) & \(9.97 0.08\) \\ Max-SW & \(2.91 0.06\) & \(12.33 0.05\) & \(2.24 0.05\) & \(10.40 0.06\) & \(2.14 0.10\) & \(9.84 0.12\) \\ v-DSW & \(2.84 0.02\) & \(12.64 0.02\) & \(2.21 0.01\) & \(10.52 0.04\) & \(2.07 0.09\) & \(9.81 0.05\) \\ IS-EBSW-e & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Reconstruction errors from three different runs of autoencoders trained by different distances. The sliced Wasserstein distance and the Wasserstein distance are multiplied by 100.

Figure 3: The figures show the source image, the target image, the transferred images from sliced Wasserstein variants, the corresponding Wasserstein-2 distances to the target color palette, and the computational time.