# On the Convergence of Loss and Uncertainty-based Active Learning Algorithms

Daniel Haimovich

Meta, Central Applied Science

danielha@meta.com

&Dima Karamshuk

Meta, Central Applied Science

karamshuk@meta.com

&Fridolin Linder

Meta, Central Applied Science

flinder@meta.com

&Niek Tax

Meta, Central Applied Science

niek@meta.com

&Milan Vojnovic

London School of Economics

m.vojnovic@lse.ac.uk

###### Abstract

We investigate the convergence rates and data sample sizes required for training a machine learning model using a stochastic gradient descent (SGD) algorithm, where data points are sampled based on either their loss value or uncertainty value. These training methods are particularly relevant for active learning and data subset selection problems. For SGD with a constant step size update, we present convergence results for linear classifiers and linearly separable datasets using squared hinge loss and similar training loss functions. Additionally, we extend our analysis to more general classifiers and datasets, considering a wide range of loss-based sampling strategies and smooth convex training loss functions. We propose a novel algorithm called _Adaptive-Weight Sampling (AWS)_ that utilizes SGD with an adaptive step size that achieves stochastic Polyak's step size in expectation. We establish convergence rate results for AWS for smooth convex training loss functions. Our numerical experiments demonstrate the efficiency of AWS on various datasets by using either exact or estimated loss values.

## 1 Introduction

In practice, when training machine learning models for prediction tasks (classification or regression), one often has access to an abundance of unlabeled data, while obtaining the corresponding labels may entail high costs. This may especially be the case in fields like computer vision, natural language processing, and speech recognition. Active learning algorithms are designed to efficiently learn a prediction model by employing a label acquisition, with the goal of minimizing the number of labels used to train an accurate prediction model.

Various label acquisition strategies have been proposed, each aiming to select informative points for the underlying model training task; including query-by-committee (Seung et al., 1992), expected model change (Settles et al., 2007), expected error reduction (Roy and McCallum, 2001), expected variance reduction (Wang et al., 2016), and mutual information maximization (Kirsch et al., 2019; Kirsch and Gal, 2022).

A common label acquisition strategy involves estimating uncertainty, which can be viewed as self-disagreement about predictions made by a given model. Algorithms using an uncertaintyacquisition strategy are referred to as _uncertainty-based_ active learning algorithms. Different variants of uncertainty strategies include margin of confidence, least confidence, and entropy-based sampling (Nguyen et al., 2022). Recently, a _loss-based_ active learning approach gained attention in research Yoo and Kweon (2019); Lahlou et al. (2022); Nguyen et al. (2021); Luo et al. (2021), and is now applied at scale in industry, such as for training integrity violation classifiers at Meta. This method involves selecting points for which there is a disagreement between the predicted label and the true label, as measured by a loss function. Since the true loss of a data point is unknown prior to the acquisition of the label, in practice, it is estimated using supervised learning. Loss-based sampling aligns with the spirit of the perceptron algorithm (Rosenblatt, 1958), which updates the model only for falsely-classified points.

Convergence guarantees for some uncertainty-based active learning algorithms have recently been established, such as for margin of confidence sampling (Raj and Bach, 2022). By contrast, there are only limited results on the convergence properties of loss-based active learning algorithms, as these only recently been started to be studied, e.g., Liu and Li (2023).

The primary focus of this paper is to establish convergence guarantees for stochastic gradient descent (SGD) algorithms where points are sampled based on their loss. Our work provides new results on conditions that ensure certain convergence rates and bounds on the expected sample size, accommodating various data sampling strategies. Our theoretical results are under assumption that the active learner has access to an oracle that provides unbiased estimate of the conditional expected loss for a point, given the feature vector of the point and the current prediction model. In practice, the loss cannot be evaluated at acquisition time since labels are yet unknown. Instead, a separate prediction model is used for loss estimation. In our experiments, we assess the impact of the bias and noise in such a loss estimator. Our convergence rate analysis accommodates also uncertainty-based data selection, for which we provide new results.

Uncertainty and loss-based acquisition strategies are also of interest for the data subset selection problem, often referred to as core-set selection or data pruning. This problem involves finding a small subset of training data such that the predictive performance of a classifier trained on it is close to that of a classifier trained on the full training data. Recent studies have explored this problem in the context of training neural networks, as seen in works like Toneva et al. (2019); Coleman et al. (2020); Paul et al. (2021); Sorscher et al. (2022); Mindermann et al. (2022). In such scenarios, the oracle can evaluate an underlying loss function exactly, avoiding the need for using a loss estimator.

There is a large body of work on convergence of SGD algorithms, e.g. see Bubeck (2015) and Nesterov (2018). These results are established for SGD algorithms under either constant, diminishing or adaptive step sizes. Recently, Loizou et al. (2021), studied SGD with the stochastic Polyak's step size, depending on the ratio of the loss and the squared gradient of the loss of a point. Our work proposes an adaptive-window sampling algorithm and provides its convergence analysis, with the algorithm defined as SGD with a sampling of points and an adaptive step size update that conform to the stochastic Polyak's step size in expectation. This is unlike to the adaptive step size SGD algorithm by Loizou et al. (2021) which does not use sampling.

### Summary of our Contributions

Our contributions can be summarizes as given in the following points:

\(\) For SGD with a constant step size, we present conditions under which a non-asymptotic convergence rate of order \(O(1/n)\) holds, where \(n\) represents the number of iterations of the algorithm, i.e., the number of unlabeled points presented to the algorithm. These conditions enable us to establish convergence rate results for loss-based sampling in the case of linear classifiers and linearly separable datasets, with the loss function taking on various forms such as the squared hinge loss function, generalized hinge loss function, or satisfying other specified conditions. Our results provide bounds for both expected loss and the number of sampled points, encompassing different loss-based strategies. These results are established by using a convergence rate lemma that may be of independent interest.

\(\) For SGD with a constant step size, we provide new convergence rate results for more general classifiers and datasets, with sampling of points according to an increasing function \(\) of the conditional expected loss of a point. In this case, we present conditions for smooth convex training loss functions under which a non-asymptotic convergence rate of order \(O(^{-1}(1/))\) holds, where \(\) is the primitive function of \(\). These results are established by leveraging the fact that the algorithm behavesakin to a SGD algorithm with an underlying objective function, as referred to as an _equivalent loss_ in Liu and Li (2023), allowing us to apply known convergence rate results for SGD algorithms.

\(\) We propose _Adaptive-Weight Sampling (AWS)_, a novel learning algorithm that combines a sampling-based acquisition strategy with an adaptive step-size SGD update, achieving the stochastic Polyak's step size update in expectation, which can be used with any differentiable loss function. We establish a condition under which a non-asymptotic convergence rate of order \(O(1/n)\) holds for AWS with smooth convex loss functions. We present uncertainty and loss-based strategies that satisfy this condition for binary classification, as well as an uncertainty strategy for multi-class classification.

\(\) We present numerical results that demonstrate the efficiency of AWS on various datasets.

### Related Work

The early proposal of the query-by-committee (QBC) algorithm by (Seung et al., 1992) demonstrated the benefits of active learning, an analysis of which was conducted under the selective sampling model by Freund et al. (1997) and Gilad-bachrach et al. (2005). Dasgupta et al. (2009) showed that the performance of QBC can be efficiently achieved by a modified perceptron algorithm with adaptive filtering. The efficient and label-optimal learning of halfspaces was studied by Yan and Zhang (2017) and, subsequently, by Shen (2021). Online active learning algorithms, studied under the name of selective sampling, include works by (Cesa-Bianchi et al., 2006, 2009, Dekel et al., 2012, Orabona and Cesa-Bianchi, 2011, Cavallanti et al., 2011, Agarwal, 2013). For a survey, refer to Settles (2012).

Uncertainty sampling has been utilized for classification tasks since as early as (Lewis and Gale, 1994), and subsequently in many other works, such as (Schohn and Cohn, 2000, Zhu et al., 2010, Yang et al., 2015, Yang and Loog, 2016, Lughofer and Pratama, 2018). Mussmann and Liang (2018) demonstrated that threshold-based uncertainty sampling on a convex loss can be interpreted as performing a pre-conditioned stochastic gradient step on the population zero-one loss. However, none of these works have provided theoretical convergence guarantees.

The convergence of margin of confidence sampling was recently studied by Raj and Bach (2022), who demonstrated linear convergence for linear classifiers and linearly separable datasets, specifically for the hinge loss function, for a family of selection probability functions and an algorithm that performs a SGD update with respect to the squared hinge loss function. However, our results for linear classifiers and linearly separable datasets differ, as our focus lies on loss-based sampling strategies and providing bounds on the convergence rate of a loss function and the expected number of sampled points. These results are established using a convergence rate lemma, which may be of independent interest. It is noteworthy that the convergence rate for uncertainty-based sampling, as in Theorem 3.1 of Raj and Bach (2022), can be derived by checking the conditions of the convergence rate lemma.

A loss-based active learning algorithm was proposed by Yoo and Kweon (2019), comprising a loss prediction module and a target prediction model. The algorithm uses the loss prediction module to compute a loss estimate and prioritizes sampling points with a high estimated loss under the current prediction model. Lahlou et al. (2022) generalize this idea within a framework for uncertainty prediction. However, neither Yoo and Kweon (2019) nor Lahlou et al. (2022) provided theoretical guarantees for convergence rates. Recent analysis of convergence for loss and uncertainty-based active learning strategies has been presented by Liu and Li (2023). Specifically, they introduced the concept of an equivalent loss, demonstrating that a gradient descent algorithm employing point sampling can be viewed as a SGD algorithm optimizing an equivalent loss function. While they focused on specific cases like sampling proportional to conditional expected loss, our results allow for sampling based on any continuous increasing function of expected conditional loss, and provide explicit convergence rate bounds in terms of the underlying sampling probability function.

In addition, Loizou et al. (2021) introduced a SGD algorithm featuring an adaptive stochastic Polyak's step size, which has theoretical convergence guarantees under various assumptions. This algorithm showcased robust performance in comparison to state-of-the-art optimization methods, especially when training over-parametrized models. Our work proposes a novel sampling method that employs stochastic Polyak's step size in expectation, offering a convergence rate guarantee for smooth convex loss functions, contingent on a condition related to the sampling probability function. Notably, we demonstrate the fulfillment of this condition for logistic regression and binary cross-entropy loss functions, encompassing both a loss-based strategy involving proportional sampling to absolute error loss and an uncertainty sampling strategy. Furthermore, we extend this condition to hold for an uncertainty sampling strategy designed for multi-class classification.

## 2 Problem Statement

We consider the setting of streaming algorithms where a machine learning model parameter \(_{t}\) is updated sequentially, upon encountering each data point, with \((x_{1},y_{1}),,(x_{n},y_{n})\) denoting the sequence of data points with the corresponding labels, assumed to be independent and identically distributed with distribution \(\). Specifically, we consider the class of projected SGD algorithms defined as: given an initial value \(_{1}\),

\[_{t+1}=_{_{0}}(_{t}-z_{t}_{} (x_{t},y_{t},_{t})),t 1\] (1)

where \(:\) is a training loss function, \(z_{t}\) is a stochastic step size with mean \((x_{t},y_{t},_{t})\) for some function \(:_{+}\), \(_{0}\), and \(_{_{0}}\) is the projection function, i.e., \(_{_{0}}(u)=_{v_{0}}||u-v||\). Unless specified otherwise, we consider the case \(_{0}=\), which requires no projection. For binary classification tasks, we assume \(=\{-1,1\}\). For every \(t>0\), we define \(_{t}=(1/t)_{s=1}^{t}_{s}\).

By defining the distribution of the stochastic step size \(z_{t}\) in Equation (1) appropriately, we can accommodate different active learning and data subset selection algorithms. In the context of active learning algorithms, at each step \(t\), the algorithm observes the value of \(x_{t}\) and decides whether or not to observe the value of the label \(y_{t}\). The value of \(z_{t}\) determine whether or not we observe the label \(y_{t}\). Deciding not to observe the value of the label \(y_{t}\) implies the step size \(z_{t}\) of value zero (not updating the machine learning model).

For the choice of the stochastic step size, we consider two cases: (a) _Constant-Weight Sampling_: a Bernoulli sampling with a constant step size, and (b) _Adaptive-Weight Sampling_: a sampling that achieves stochastic Polyak's step size in expectation. For case (a), \(z_{t}\) is the product of a constant step size \(\) and a Bernoulli random variable with mean \((x_{t},y_{t},_{t})\). For case (b), \((x,y,)\) is the "stochastic" Polyak's step size, and \(z_{t}\) is equal to \((x_{t},y_{t},_{t})/(x_{t},y_{t},_{t})\) with probability \((x_{t},y_{t},_{t})\) and is equal to \(0\) otherwise. Note that using the notation \((x,y,)\) allows for the case when the sampling probability does not depend on the value of the label \(y\).

For a _loss-based sampling_, \(\) is an increasing function of some loss function \(^{*}\), which does not necessarily correspond to the training loss function \(\). Specifically, for a binary classifier with \(p(x,y,)\) denoting the expected prediction label, _sampling proportional to the absolute error loss_ is defined as \((^{*})=^{*}\) where \(^{*}(x,y,)=|y-p(x,y,)|\) and \((0,1/2]\). For an _uncertainty-based sampling_, \(\) is a function of some quantity reflecting the uncertainty of the prediction model.

Our focus is on finding convergence conditions for algorithm (1) and convergence rates under these conditions, as well as bounds on the expected number of points sampled by the algorithm.

Additional Assumptions and NotationFor binary classification, we say that data is _separable_ if, for every point \((x,y)\), either \(y=1\) with probability \(1\) or \(y=-1\) with probability \(1\). The data is _linearly separable_ if there exists \(^{*}\) such that \(y=(x^{}^{*})\) for every \(x\). Linearly separable data has a _\(^{*}\)-margin_ if \(|x^{}^{*}|^{*}\) for every \(x\), for some \(^{*}\).

Some of our results are for _linear classifiers_, where the predicted label of a point \(x\) is a function of \(x^{}\). For example, a model with a predicted label \((x^{})\) is a linear classifier. For logistic regression, the predicted label is \(1\) with probability \((x^{})\) and \(-1\) otherwise, where \(\) is the logistic function defined as \((z)=1/(1+e^{-z})\). For binary classification, we model the prediction probability of the positive label as \((x^{})\), where \(:_{+}\) is an increasing function, and \((-u)+(u)=1\) for all \(u\). The absolute error loss takes value \(1-(x^{})\) if \(y=1\) or value \((x^{})\) if \(y=-1\), which corresponds to \(1-(yx^{})\). The binary cross-entropy loss for a point \((x,y)\) under model parameter \(\) can be written as \((x,y,)=-((yx^{}))\). Hence, absolute error loss-based sampling corresponds to the sampling probability function \(()=1-e^{-}\).

For any given \((x,y)\), the loss function \((x,y,)\) is considered _smooth_ on \(^{}\) if it has a Lipschitz continuous gradient on \(^{}\), i.e., there exists \(L_{x,y}\) such that \(||_{}(x,y,_{1})-_{}(x,y,_{2})||  L_{x,y}||_{1}-_{2}||\) for all \(_{1},_{2}^{}\). For any distribution \(q\) over \(\), \(_{(x,y) q}[(x,y,)]\) is \(_{(x,y) q}[L_{x,y}]\)-smooth.

Convergence Rate Guarantees

In this section, we present conditions on the stochastic step size of algorithm (1) under which we can bound the total expected loss and the expected number of samples. For the Constant-Weight Sampling, we provide conditions that allow us to derive bounds for linear classifiers and linearly separable datasets and more general cases. For Adaptive-Weight Sampling, we offer a condition that allows us to establish convergence bounds for both loss and uncertainty-based sampling.

### Constant-Weight Sampling

Linear Classifiers and Linearly Separable DatasetsWe focus on binary classification and briefly discuss extension to multi-class classification. We consider the linear classifier with the predicted label \((x^{})\). With a slight abuse of notation, let \((x,y,)(u)\) and \((x,y,)(u)\) where \(u=yx^{}\). We assume that the domain \(\) is bounded, i.e., there exists \(R\) such that \(||x|| R\) for all \(x\), \(||_{1}-^{*}|| S\) for some \(S 0\), and that the data is \(^{*}\)-margin linearly separable.

We present convergence rate results for the training loss function corresponding to the squared hinge loss function, i.e. \((u)=(1/2)\{1-u,0\}^{2}\). Our additional results also cover other cases, including a class of smooth convex loss functions and a generalized smooth hinge loss function, which are presented in the Appendix.

**Theorem 3.1**.: _Assume that \(^{*}>1\), the loss function is the squared hinge loss function, and the sampling probability function \(\) is such that for all \(u 1\), \((u)/2\) and_

\[(u)^{*}((u)):=(1-})\] (2)

_for some constants \(0< 2\) and \(/(^{*}-1)\). Then, for any initial value \(_{1}\) such that \(||_{1}-^{*}|| S\) and \(\{_{t}\}_{t>1}\) according to algorithm (1) with \(=1/R^{2}\),_

\[[(yx^{}_{n})][ _{t=1}^{n}(y_{t}x_{t}^{}_{t})]S^{2}}{},\]

_where \((x,y)\) is an independent sample of a labeled data point from \(\). Moreover, if the sampling is according to \(^{*}\), then the expected number of sampled points satisfies_

\[[_{t=1}^{n}^{*}((y_{t}x_{t}^{}_{t})) ]\{RS, n \}.\]

Condition (2) requires that the sampling probability function \(\) is lower bounded by an increasing, concave function \(^{*}\) of the loss value. This fact, along with the expected loss bound, implies the asserted bound for the expected number of samples. The expected number of samples is \(O()\) concerning the number of iterations and is \(O(1/(^{*}-1))\) concerning the margin \(^{*}-1\).

Theorem 3.1, and our other results for linear classifiers and linearly separable datasets, are established using a convergence rate lemma, which is presented in Appendix A.2, along with its proof. This lemma generalizes the conditions used to establish the convergence rate for an uncertainty-based sampling algorithm by Raj and Bach (2022), with the sampling probability function \((u)=1/(1+|u|)\), for some constant \(>0\). It can be readily shown that Theorem 3.1 in Raj and Bach (2022) follows from our convergence rate lemma with the training loss function corresponding to the squared hinge loss function and the evaluation loss function (used for convergence rate guarantee) corresponding to the hinge loss function. Further details on the convergence rate lemma are discussed in Appendix A.2.1.

The convergence rate conditions for multi-class classification with the set of classes \(\) are the same as for binary classification, with \(u(x,y,):=x^{}_{y}-_{y^{}\{y \}}x^{}_{y^{}}\), except for an additional factor of \(2\) in one of the conditions (see Lemma A.10 in the Appendix). Hence, all the observations remain valid for the multi-class classification case.

More General Classifiers and DatasetsWe consider algorithm (1) where \(z_{t}\) is product of a fixed step size \(\) and a Bernoulli random variable \(_{t}\) with mean \((x,y,)\). Let \(g_{t}=_{t}_{}(x_{t},y_{t},_{t})\), which is random vector because \(_{t}\) is a random variable and \((x_{t},y_{t})\) is a sampled point. Following Liu and Li (2023), we note that the algorithm (1) is an SGD algorithm with respect to an objective function \(\) with gradient

\[_{}()=[(x,y,)_{ }(x,y,)]\] (3)

where the expectation is with respect to \(x\) and \(y\). This observation allows us to derive convergence rate results by deploying convergence rate results that are known to hold for SGD under various assumptions on function \(\), variance of stochastic gradient vector and step size. A function \(\) satisfying condition (3) is referred to as an _equivalent loss_ in Liu and Li (2023).

Assume that the sampling probability \(\) is an increasing function of the conditional expected loss \((x,)=_{y}[(x,y,) x]\). With a slight abuse of notation, we denote this probability as \(((x,))\) where \(:_{+}\) is an increasing and continuous function. Let \(\) be the primitive of \(\), i.e. \(^{}=\). We then have

\[()=[((x,))].\] (4)

If \((x,y,)\) is a convex function, for every \((x,y)\), then \(\) is a convex function.

This framework for establishing convergence rates allows us to accommodate different sampling strategies and loss functions. The next lemma allows us to derive convergence rate results for expected loss with respect to loss function \(\) by applying convergence rate results for expected loss with respect to loss function \(\) (which, recall, is the equivalent loss function).

**Lemma 3.2**.: _Assume that for algorithm (1) with loss-based sampling according to \(\), for some functions \(f_{1},,f_{m}\), we have_

\[[_{t=1}^{n}(_{t})] _{}()+_{i=1}^{m}f_{i}(n).\] (5)

_Then, it holds:_

\[[_{t=1}^{n}(_{t})]  _{}^{-1}(())+_{i=1}^{m}^{- 1}(f_{i}(n)).\]

We apply Lemma 3.2 to obtain the following result.

**Theorem 3.3**.: _Assume that \(\) is a convex function, \(\) is \(L\)-smooth, \(_{0}\) is a convex set, \(S=_{_{0}}||-_{1}||\), and \([((x,))||_{}(x,y,)||^{2}]-|| _{}()||^{2}_{}^{2}\). Then, for algorithm (1) with \(=1/(L+(/R))\),_

\[[(_{n})][_{t=1}^{ n}(_{t})]_{}^{-1}(())+^{-1} (S_{}}{})+^{-1}( }{n}).\]

Note that the bound on the expected loss in Theorem 3.3 depends on \(\) through \(^{-1}\) and \(_{}^{2}\). Specifically, we have a bound depending on \(\) only through \(^{-1}\) by upper bounding \(_{}^{2}\) with \(_{_{0}}[||_{}(x,y,)||^{2}]\).

 \((x)\) & \((x)\) & \(^{-1}(x)\) \\  \(1-e^{-x}\) & \(x+e^{-x}-1\) & \(\) for small \(x\) \\  \(\{x,1\}\) & \(\{x^{2}&x 1\\ x-&x 1.\) & \(\{&x 1/2\\ x+&x 1/2.\) \\  \(\{(x/b)^{a},1\},a>0,b>0\) & \(\{}x^{1+a}&x a\\ x-&x a.\) & \(\{b^{}(1+a)^{}x^{ {1+a}}&x\\ x+b&x.\) \\  \(1-\) & \(x-(1+ x)\) & \(\) for small \(x\) \\  \(1-}\) & \(x-+}(1+)\) & \((((3/2)/)x)^{2/3}\) for small \(x\) \\ 

Table 1: Examples of sampling probability functions.

For convergence rates for large values of the number of iterations \(n\), the bound in Theorem 3.2 crucially depends on how \(^{-1}(x)\) behaves for small values of \(x\). In Table 1, we show \(\) and \(^{-1}\) for several examples of sampling probability function \(\). For all examples in the table, \(^{-1}(x)\) is sub-linear in \(x\) for small \(x\). For instance, for absolute error loss sampling under binary cross-entropy loss function, \((x)=1-e^{-x}\), \(^{-1}(x)\) is approximately \(\) for small \(x\). For this case, we have the following corollary.

**Corollary 3.4**.: _Under assumptions of Theorem 3.3, sampling probability \((x)=1-e^{-x}\), and \(n\{()^{2}2S_{}^{2}, LS^{2}\}\) it holds_

\[[(_{n})][_{t=1}^{ n}(_{t})]_{}^{-1}(())+2^{5/4} }}+2S}.\]

By using a bound on the expected total loss, we can bound the expected total number of sampled points under certain conditions as follows.

**Lemma 3.5**.: _The following bounds hold:_

1. _Assume that_ \(\) _is a concave function, then_ \([_{t=1}^{n}((x_{t},_{t}))]( [_{t=1}^{n}(_{t})])n\)_._
2. _Assume that_ \(\) _is_ \(K\)_-Lipschitz or that for some_ \(K>0\)_,_ \(()\{K,1\}\) _for all_ \( 0\)_, then_ \([_{t=1}^{n}((x_{t},_{t}))]\{ K[_{t=1}^{n}(_{t})],n\}.\)__

We remark that \(\) is a concave function for all examples in Table 1 without any additional conditions, except for \(()=\{(/b)^{a},1\}\) which is concave under assumption \(0<a 1\). We remark also that for every example in Table 1 except the last one, \(()\{K,1\}\) for some \(K>0\). Hence, for all examples in Table 1, we have a bound for the expected number of sampled points provided we have a bound for the expected loss.

### Adaptive-Weight Sampling

In this section we propose the _Adaptive-Weight Sampling (AWS)_ algorithm that combines Bernoulli sampling and an adaptive SGD update, and provide a convergence rate guarantee. The algorithm is defined by (1) with the stochastic step size \(z_{t}\) being a binary random variable that takes value \(_{t}:=(x_{t},y_{t},_{t})/(x_{t},y_{t},_{t})\) with probability \((x_{t},y_{t},_{t})\) and takes value \(0\) otherwise, where \(\) is some sampling probability function. Here, \((x,y,)\) is the expected SGD (1) step size, defined as

\[(x,y,)=\{,\}\]

whenever \(||_{}(x,y,)||>0\) and \((x,y,)=0\) otherwise, for constants \(,>0\), where

\[(x,y,):=(x,y,)||^{2}}{(x,y, )-_{^{}}(x,y,^{})}.\]

The expected step size \((x,y,)\) corresponds to the stochastic Polyak's step size used by a gradient descent algorithm proposed by (Loizou et al., 2021), which is accommodated as a special case when \((x,y,)=1\) for all \(x,y,\). AWS introduces a sampling component and re-weighting of the update to ensure that the step size remains according to the stochastic Polyak's step size in expectation. For many loss functions, \(_{^{}}(x,y,)=0\), for every \(x,y\). In these cases, \((x,y,)=||_{}(x,y,)||^{2}/(x,y,)\). For instance, for binary cross-entropy loss function, \(_{^{}}(x,y,)=_{^{}}(-((yx^{ }^{})))=0\), for all \(x,y\).

We next show a convergence rate guarantee for AWS.

**Theorem 3.6**.: _Assume that \(\) is a convex, \(L\)-smooth function, there exists \(^{*}\) such that \([(x,y,^{*})]-[_{}(x,y,)] ^{*}\), and the sampling probability function \(\) is such that, for some constant \(c(0,1)\), for all \(x,y,\) such that \(||_{}(x,y,)||>0\),_

\[(x,y,)\{(x,y,),1 \}.\] (6)_Then, we have_

\[[_{t=1}^{n}((x_{t},y_{t},_{t})-(x_{t},y_{t},_{t}^{*}))]^{*}+||_{1}-^{*}||^{2}\]

_where \(=\{1/(2L),\}\) and \(_{t}^{*}\) is a minimizer of \((x_{t},y_{t},^{})\) over \(^{}\)._

The bound on the expected average loss in Theorem 3.6 boils down to \(^{*}/c+(L/(c))||_{1}-^{*}||^{2}/n\) by taking \(=1/(2L)\). Notably, under the condition on the sampling probability in Theorem 3.6, the convergence rate is of order \(O(1/n)\). A similar bound is known to hold for SGD with adaptive stochastic Polyak step size for the finite-sum problem, as seen in Theorem 3.4 of Loizou et al. (2021). A difference is that Theorem 3.6 allows for sampling of the points.

Loss and Uncertainty-based Sampling for Linear Binary ClassifiersWe consider linear binary classifiers, focusing particularly on logistic regression and the binary cross-entropy training loss function. The following corollaries of Theorem 3.6 hold for sampling proportional to absolute error loss and an uncertainty-based sampling probability function, respectively.

**Corollary 3.7**.: _For sampling proportional to absolute error loss, \((u)=(1-(u))\), with \(/(4(1-c)L^{}) 1\) and \(=1/(2L)\), the bound on the expected loss in Theorem 3.6 holds._

**Corollary 3.8**.: _For the uncertainty-based sampling according to_

\[(u)=\{ R^{2},1\}\]

_where \(a(0,1/2]\) and \(H(a)=a(1/a)+(1-a)(1/(1-a))\), the bound on the expected loss in Theorem 3.6 holds._

Other CasesFor a constant sampling probability function with a value of at least \(/(2(1-c))\), condition (6) holds when \( 2(1-c)\). When \((x,y,)=(x,y,)^{}\), where \( 0\) and \((0,1]\), condition (6) holds under \(^{1-} 2(1-c)(1/(2L))^{}\), as shown in Appendix A.14. Condition (6) also holds for an uncertainty-based sampling in multi-class classification, as shown in Appendix A.15.

## 4 Numerical Results

In this section we evaluate our AWS algorithm, defined in Section 3.2. In particular, we focus on an instance of AWS with stochastic Polyak's expected step size for logistic regression and the loss-based sampling proportional to absolute error loss, which we refer to as _Adaptive-Weight Sampling - Polyak Absloss (AWS-PA)_. By, Corollary 3.7, AWS-PA converges according to Theorem 3.6. Here we demonstrate convergence on real-world datasets and compare with other algorithms.

The implementation of AWS-PA algorithm along with all the other code run the experimental setup that is described in this section is available at https://www.github.com/facebookresearch/AdaptiveWeightSampling.

We use a modified version of the _mushroom_ binary classification dataset (Chang and Lin, 2011) that was used by Loizou et al. (2021) for evaluation of their algorithm. This modification uses RBF kernel features, resulting in a linearly separable dataset for a linear classifier like logistic regression. Furthermore, we include five datasets that we selected at random from the 44 real-world datasets that were used in Yang and Loog (2018), a benchmark study of active learning for logistic regression: _MNIST 3 vs 5_LeCun et al. (1998), _parkinsons_Little et al. (2007), _splice_Noordewier et al. (1990), _ticatactoe_Aha (1991), and _credit_Quinlan (1987). While these datasets are not necessarily linearly separable, Yang and Loog (2018) has shown that logistic regression achieves a good quality-of-fit.

In our evaluation, we deliberately confine the training to a single epoch. Throughout this epoch, we sequentially process each data instance, compute the loss for each individual instance, and subsequently update the model's weights. This approach, known as _progressive validation_(Blum et al., 1999), enables us to monitor the evolution of the average loss. The constraint to a single epoch ensures that we calculate losses only for instances that haven't influenced model weights. For each sampling scheme, we conduct a hyper-parameter sweep to minimize the average progressive loss and apply a procedure to ensure that all algorithms sample comparable numbers of instances.

In Appendix B.1 we include further details on this procedure, the hyper-parameter tuning, and other aspects of the experimental setup.

Figure 1 demonstrates that AWS-PA leads to faster convergence than the traditional loss-based sampling with a constant step size (akin to Yoo and Kweon (2019)). It also shows that the traditional loss-based sampling approach converges more rapidly than random sampling on five of the six datasets. These results are obtained under a hyper-parameter tuning such that different algorithms have comparable data sampling rates. We provide additional experimental results that demonstrate the efficiency of AWS-PA in Appendix B.2.

In active learning applications, the true loss of a point cannot be computed before the corresponding label is obtained. Hence, in practice we do not know the true loss at the moment of making the sampling decision. Therefore, we assess the effect of using a _loss estimator_, instead of using the true loss values. We use a Random Forest regressor to estimate absolute error loss based on the same set of features as the target model and the target's model prediction as an extra feature. We retrain this estimator on every sampling step using the labeled points observed so far.

Figure 2 demonstrates that AWS-PA with the estimated absolute error losses performs similarly on all datasets to AWS-PA with the true absolute error losses. Moreover, for a majority of the datasets, the two variants of AWS-PA achieve similar data sampling rates; this is shown Appendix B.3 along with further discussion.

## 5 Conclusion

We have provided convergence rate guarantees for loss and uncertainty-based active learning algorithms under various assumptions. Furthermore, we introduced the novel _Adaptive-Weight Sampling (AWS)_ algorithm that combines sampling with an adaptive size, conforming to stochastic Polyak's step size in expectation, and demonstrated its convergence rate guarantee, contingent on a condition related to the sampling probability function.

Figure 1: Convergence in terms of average cross-entropy progressive loss of random sampling, loss-based sampling based on the absolute error loss, and our proposed algorithm (loss-based sampling with stochastic Polyak’s step size). Our proposed algorithm outperforms the baselines in most cases.

For future research, it would be interesting to establish tight convergence rates for the training loss function and the sampling cost, especially comparing policies using sampling with a constant probability with those using adaptive loss-based sampling probabilities. It would be interesting to explore adaptive-weight sampling algorithms with adaptive sizes different than those studied in this paper. Additionally, exploring a theoretical study on the impact of bias and noise in the loss estimator, used for evaluating the sampling probability function, on the convergence properties of algorithms could open up a valuable avenue for investigation.

Figure 2: Active learning sampling based on an estimator of the absolute error loss performs on par with the sampling based on the ground truth value of absolute error loss.