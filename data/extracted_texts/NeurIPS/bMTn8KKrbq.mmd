# Towards training digitally-tied analog blocks

_via_ hybrid gradient computation

 Timothy Nest1

timothy.nest@mila.quebec

&Maxence Ernoult2

maxence@rain.ai

###### Abstract

Power efficiency is plateauing in the standard digital electronics realm such that new hardware, models, and algorithms are needed to reduce the costs of AI training. The combination of energy-based analog circuits and the Equilibrium Propagation (EP) algorithm constitutes a compelling alternative compute paradigm for gradient-based optimization of neural nets. Existing analog hardware accelerators, however, typically incorporate digital circuitry to sustain auxiliary non-weight-stationary operations, mitigate analog device imperfections, and leverage existing digital platforms. Such heterogeneous hardware lacks a supporting theoretical framework. In this work, we introduce _Feedforward-tied Energy-based Models_ (ff-EBMs), a hybrid model comprised of feedforward and energy-based blocks housed on digital and analog circuits. We derive a novel algorithm to compute gradients end-to-end in ff-EBMs by backpropagating and "eq-propagating" through feedforward and energy-based parts respectively, enabling EP to be applied flexibly on realistic architectures. We experimentally demonstrate the effectiveness of this approach on ff-EBMs using Deep Hopfield Networks (DHNs) as energy-based blocks, and show that a standard DHN can be arbitrarily split into any uniform size while maintaining or improving performance with increases in simulation speed of up to four times. We then train ff-EBMs on ImageNet32 where we establish a new state-of-the-art performance for the EP literature (46 top-1 %) 4. Our approach offers a principled, scalable, and incremental roadmap for the gradual integration of self-trainable analog computational primitives into existing digital accelerators.

## 1 Introduction

Gradient-based optimization, the cornerstone and most energy greedy component of deep learning, fundamentally relies upon three factors: i) highly parallel digital hardware such as GPUs, ii) feedforward models and iii) backprop (BP). With skyrocketing demands of AI compute, reducing the energy consumption of AI systems has become a matter of great economic, societal and environmental urgency (Strubell et al., 2020), calling for the exploration of novel compute paradigms (Thompson et al., 2020; Scellier, 2021; Stern and Murugan, 2023).

One promising path towards this goal is analog in-memory computing (Sebastian et al., 2020): by mapping weights onto a crossbar of resistive devices, Kirchoff current and voltage laws inherently perform matrix-vector multiplications with constant time complexity (Cosemans et al., 2019). By stacking multiple such crossbars, an entire neural network can be mapped onto a physical system. An important formalism for such a system is that of _energy-based_ (EB) analog circuits[Kendall et al., 2020, Stern et al., 2023, Dillavou et al., 2023, Scellier, 2024], which are "self-learning" systems that can compute loss gradients through two relaxations to equilibrium (i.e. two "forward passes"). Such a procedure falls under the umbrella of energy-based learning (EBL) algorithms [Scellier et al., 2024]. One such algorithm, Equilibrium Propagation (EP) [Scellier and Bengio, 2017], particularly stands out for its strong theoretical guarantees, relative scalability in the realm of backprop alternatives [Laborieux and Zenke, 2022, 2023] and proven application on small analog systems with \(10,000\) greater energy-efficiency and substantial speedups compared to its GPU-based counterpart [Yi et al., 2023]. This suggests a new alternative compute paradigm for gradient-based optimization consisting of: i) analog hardware, ii) EBMs, and iii) EP.

In this paper, we propose a theoretical framework for extending end-to-end gradient computation to a realistic setting where the system in question may or may not be _fully_ analog. Such a setting is plausible in the near term, for two major reasons. First, analog circuits exhibit non-ideal physical behaviors which affect both the inference pathway [Wang et al., 2023, Ambrogio et al., 2023] and parameter optimization [Nandakumar et al., 2020, Spoon et al., 2021, Lammie et al., 2024], compromising performance. Second, owing to the latency and energy-consumption of resistive devices' write operations, analog circuits should be fully weight stationary - weights must be written before the inference procedure begins - which excludes many operations used conventionally in machine learning including activation functions, normalization, and attention [Spoon et al., 2021, Jain et al., 2022, Liu et al., 2023, Li et al., 2023]. Therefore, analog systems are likely to be used in combination with auxiliary digital circuitry, resulting in hybrid mixed precision systems [Haensch et al., 2018]. While the design of purely inferential engines made up of analog and digital parts is nearing commercial maturity of such systems has barely been explored. An important challenge remains in proving EBL algorithms can scale in a manner comparable to backprop, given the requirement of _simulating_ EB systems on GPUs. Because of the necessity of convergence, this amounts in practice in performing lengthy root finding algorithms to simulate physical equilibrium, limiting proof-of-concepts thereof to relatively shallow (5-6 layer) models [Scellier et al., 2024, Scellier, 2024].

Our work contends that the best of both worlds can be achieved with the following triad: i) hybrid digital _and_ analog hardware, ii) feedforward _and_ EB models, iii) BP _and_ EP. Namely, by modeling digital and analog parts as feedforward and EB modules respectively, we show how backprop and EP error signals can be chained end-to-end via feedforward and EB blocks respectively in a principled fashion. Rather than opposing digital and analog, or backprop and "alternative" learning algorithms, as is often done in the literature, we propose a novel hardware-aware building block which can, in principle, leverage advances from _both_ digital and analog hardware in the near-term. More specifically:

\(\) We propose _Feedforward-tied Energy-based Models_ (ff-EBMs, Section 3.1) as high-level models of mixed precision systems whose inference pathway read as the composition of feedforward and EB modules (Eq. (5), Alg. 1).

\(\) We show that gradients in ff-EBMs can be computed in an end-to-end fashion (Section 3.3), backpropagating through feedforward blocks and "eq-propagating" through EB blocks (Theorem 3.1, Alg. 2) and that this procedure is rooted in a deeply-nested optimization problem (Section 3.2).

\(\) Finally, we experimentally demonstrate the effectiveness of our algorithm on ff-EBMs where EBM blocks are Deep Hopfield Networks (DHNs) (Section 4).In particular we show that i) final and _transient_ gradient estimates computed by our algorithm (Alg. 2) near perfectly match gradients computed by end-to-end automatic differentiation (Section 4.2), which we also prove mathematically (Theorem 4.1), ii) a standard DHN model can be arbitrarily split into a ff-DHN with the equivalent

Figure 1: Illustrating BP-EP backward gradient chaining through feedforward (red) and energy-based (blue) blocks, accounting for digital and analog circuits respectively.

layers and architectural layers while maintaining or improving performance, remaining on par with automatic differentiation and being _up to four times faster to simulate_ depending on the convergence criterion at use to compute equilibrium (Section 4.3), iii) the proposed approach yields 46 % top-1 (70% top-5) validation accuracy on ImageNet32 when training a ff-EBM of 15 layers, beating current state-of-the-art for EP by a large margin, without relying on holomorphic transformations inside EBM blocks (Laborieux and Zenke, 2022, 2023)

## 2 Background

Notations.Given \(A:^{n}^{m}\) a differentiable mapping, we denote its _total_ derivative with respect to \(s_{j}\) as \(d_{s_{j}}A(s):=dA(s)/ds_{j}^{m}\), its _partial_ derivative with respect to \(s_{j}\) as \(_{j}A(s):= A(s)/ s_{j}^{m}\). When \(A\) takes scalar values (\(m=1\)), its _gradient_ with respect to \(s_{j}\) is denoted as \(_{j}A(s):=_{j}A(s)^{}\).

### Energy-based models (EBMs)

For a given static input and set of weights, Energy-based models (EBMs) implicitly yield a prediction through the minimization of an energy function. As such they are a particular kind of implicit model. Namely, an EBM is defined by a (scalar) energy function \(E:s,,x E(s,,x)\) where \(x\), \(s\), and \(\) respectively denote a static input, hidden and output neurons and model parameters, and each such tuple defines a configuration with an associated scalar energy value. Among all configurations for a given input \(x\) and some model parameters \(\), the model prediction \(s_{}\) is implicitly given as an equilibrium state which minimizes the energy function:

\[s_{}:=_{s}E(s,,x).\] (1)

### Standard bilevel optimization

Assuming that \(_{s}^{2}E(x,s_{},)\) is invertible, note that the equilibrium state \(s_{}\) implicitly depends on \(x\) and \(\) by virtue of the implicit function theorem (Dontchev et al., 2009). Therefore our goal when training an EBM-in a supervised setting, for instance - is to adjust the model parameters \(\) such that \(s_{}(x,)\) minimizes some cost function \(:s,y(s,y)\) where \(y\) is some ground-truth label associated to \(x\). More formally, this learning objective can be stated with the following _bilevel optimization problem_(Zucchet and Sacramento, 2022):

\[_{}(x,,y):=(s_{},y) s _{}=_{s}E(s,,x)\] (2)

Solving Eq. (2) in practice amounts to computing the gradient of its outer objective \((x,)\) with respect to \(\) (\(d_{}(x,)\)) and then performing gradient descent over \(\).

### Equilibrium Propagation (EP)

An algorithm used to train an EBM model in the sense of Eq. (2) may be called an EBL algorithm (Scellier et al., 2024). Equilibrium Propagation (EP) (Scellier and Bengio, 2017) is an EBL algorithm which computes an estimate of \(d_{}(x,)\) with at least two phases. During the first phase, the model is allowed to evolve freely to \(s_{}=_{s}E(s,,x)\). Then, the model is slightly nudged towards decreasing values of cost \(\) and settles to a second equilibrium state \(s_{}\). This amounts to augmenting the energy function \(E\) by an additional term \((s,y)\) where \(^{}\) is called the _nudging factor_. Next, the weights are updated to increase the energy of \(s_{}\) and decrease that of \(s_{}\), thereby "contrasting" these two states. More formally, Scellier and Bengio (2017) show in the seminal EP paper:

\[s_{}:=_{s}[E(s,,x)+(s,y)], ^{}:=(_{2}E(s_{},,x )-_{2}E(s_{},,x)),\] (3)

where \(\) denotes some learning rate. EP comes in different flavors depending on the sign of \(\) inside Eq. (3) or on whether two nudged states of opposite nudging strengths (\(\)) are contrasted, a variant called _Centered_ EP (C-EP) which was shown to work best in practice (Laborieux et al., 2021; Scellier et al., 2024) and reads as:

\[^{}:=(_{2}E(s_{- },,x)-_{2}E(s_{},,x)),\] (4)

## 3 Tying energy-based models with feedforward blocks

In the present section we introduce a new model, _Feedforward-tied EBMs_ (ff-EBMs, section 3.1), which read as composition of feedforward and EB transformations (Alg. 1). We show how optimizing ff-EBMs amounts to solving a multi-level optimization problem (Section 3.2) and propose a BP-EP gradient chaining algorithm as a solution(Section 3.3, Theorem 3.1, Alg. 2). We highlight as an edge case the fact that ff-EBMs reduce to standard feedforward nets (Lemma A.1) and the proposed BP-EP gradient chaining algorithm to standard BP (Corollary A.1) when each EB block comprises a single hidden layer. We highlight in red and blue the parts of the model and associated algorithms performed inside feedforward (digital) and EB (analog) blocks respectively.

### Feedforward-tied Energy-based Models (ff-EBMs)

Inference procedure.We define _Feedforward-tied Energy-based Models_ (ff-EBMs) as compositions of feedforward and EB transformations. Namely, an data sample \(x\) is fed into the first feedforward transformation \(F^{1}\) parametrized by some weights \(^{1}\), which yields an output \(x_{}^{1}\). Then, \(x_{}^{1}\) is fed as a static input into the first EB block \(E^{1}\) with parameters \(^{1}\), which relaxes to an equilibrium state \(s_{}^{1}\). \(s_{}^{1}\) is in turn fed into the next feedforward transformation \(F^{1}\) with weights \(^{1}\) and the above procedure repeats until reaching the output layer \(\). More formally, denoting \(F^{k}\) and \(E^{k}\) the \(^{}\) feedforward and EB blocks parametrized by the weights \(^{k}\) and \(^{k}\) respectively, the inference pathway of a ff-EBM reads as:

\[\{s^{0}:=x\\ x_{}^{k}:=F^{k}(s_{}^{k-1},^{k}), s_{}^{k}:=* {arg\,min}_{s}E^{k}(s,^{k},x_{}^{k}) k=1 N-1\\ _{}:=F^{N}(s_{}^{N-1},^{N}).\] (5)

The ff-EBM inference procedure is depicted more compactly inside Fig. 2 (left) and Alg. 1.

Form of the energy functions.We further specify the form of the energy of the \(^{}\) EB block of a ff-EBM as defined per Eq. (5). The associated energy function \(E^{k}\) takes some static input \(x^{k}\) from the output of the preceding feedforward transformation, has hidden neurons \(s^{k}\) and is parametrized by weights \(^{k}\). More precisely:

\[E^{k}(s^{k},^{k},x^{k}):=G^{k}(s^{k})-s^{k^{}} x^{k}+U^{k}(s^{ k},^{k})\] (6)

Eq. (6) reveals three different contributions to the energy. The first term determines the non-linearity applied inside the EB block (Zhang and Brand, 2017; Hoier et al., 2023): for a given invertible and continuous activation function \(\), \(\) is defined such that \( G=^{-1}\) (see Appendix A.1.3).

The second term inside Eq. (6) accounts for a purely feedforward contribution from the previous feedforward block \(F^{k}\). Finally, the third term accounts for _internal_ interactions within the layers of the EB block.

Recovering a feedforward net.When taking the gradient of \(E^{k}\) as defined in Eq. (6) with respect to \(s^{k}\) and zeroing it out, it can be seen that \(s_{}^{k}\) is implicitly defined as:

\[s_{}^{k}:=(x^{k}-_{1}U^{k}(s_{}^{k},^{k}))\] (7)

Figure 2: Depiction of the forward (left) and backward (right) pathways through a ff-EBM, with blue and pink blocks denoting EB and feedforward transformations.

A noteworthy edge case highlighted by Eq. (7) is when \(U^{k}=0\) for all \(k\)'s, i.e. when there are no intra-block layer interactions, or equivalently when the EB block comprises a single layer only. In this case, \(s^{k}_{}\) is simply a feedforward mapping \(x^{k}\) through \(\) and in turn the ff-EBM is simply a standard feedforward architecture (see Lemma A.1 inside Appendix A.1.1).

### Multi-level optimization of ff-EBMs

Just as learning EBMs can be naturally cast as a bilevel optimization problem, learning ff-EBMs equates to a _multi-level_ optimization problem where the variables being optimized in the inner subproblems are comprised of EB block variables \(s^{1},,s^{N-1}\). To make this clearer, we re-write the energy function of the \(^{}\) block \(E^{k}\) from Eq. (6) to highlight the dependence between two consecutive EB block states:

\[^{k}(s^{k},^{k},s^{k-1}_{},^{k}):=E^{k}(s^ {k},^{k},F^{k}(s^{k-1}_{},^{k-1}))\] (8)

It can be seen from Eq. (8) that the equilibrium state \(s^{k}_{}\) obtained by minimizing \(E^{k}\) will be dependent upon the equilibrium state \(s^{k-1}_{}\) of the previous EB block, which propagates back through prior EB blocks. Denoting \(W:=\{^{1},,^{N-1},^{1},,^{N}\}\), the learning problem for a ff-EBM can therefore be written as:

\[_{W} (x,W,y):=(_{}=F^{N}(s^{N-1}_{}, ^{N}),y)\] (9) \[ s^{N-1}_{}=*{arg\,min}_{s} ^{N-1}(s,^{N-1},s^{N-2}_{},^{N-1})  s^{1}_{}=*{arg\,min}_{s} ^{1}(s,^{1},x,^{1})\]

Here again and similarly to bilevel optimization, solving Eq. (9) in practice amounts to computing \(g_{^{k}}:=d_{^{k}}\) and \(g_{^{k}}:=d_{^{k}}\) and performing gradient descent on \(^{k}\) and \(^{k}\).

### A \(\) gradient chaining algorithm

Main result: explicit BP-EP chaining.Based on the multilevel optimization formulation of ff-EBMs learning in Eq. (9), we state the main theoretical result of this paper in Theorem 3.1 (see proof in Appendix A.2.1).

**Theorem 3.1** (Informal).: _Assuming a model of the form Eq. (5), we denote \(s^{1}_{},x^{1}_{},,s^{N-1}_{},_{}\) the states computed during the forward pass as depicted in Alg. 1. We define the nudged state of block \(k\), denoted as \(s^{k}_{}\), implicitly through \(_{1}^{k}(s^{k}_{},^{k},x^{k}_{}, s^{k},)=0\) with:_

\[^{k}(s^{k},^{k},x^{k}_{}, s^{k},):=E^{k}(s^{k },^{k},x^{k}_{})+ s^{k^{}} s^{k}\] (10)

_Denoting \( s^{k}\) and \( x^{k}\) the error signals computed at the input of the feedforward block \(F^{k}\) and of the EB block \(E^{k}\) respectively, then the following chain rule applies:_

\[_{^{N-1}}:=_{s^{N-1}}(_{},y), _{^{N}}=_{^{N}}(_{},y)\] (11) \[ k=2 N-1:\] (12)

**Proposed algorithm: implicit BP-EP chaining.** Theorem 3.1 reads intuitively. It prescribes an _explicit_ chaining of EP error signals passing backward through \(E^{k}\) (\(_{^{k}} x^{k}\)) and BP error signals passing backward through \(}^{}\) (\( x^{k}_{^{k-1}}\)), which directly mirrors the ff-EBM inference pathway as depicted in Fig. 2. Yet noticing that:

\[\{ s^{k-1}=_{1}F^{k}(s^{k-1}_{},^{k})^{} x^{k}=d_{}.(_{3} ^{k}(s^{k}_{},^{k},s^{k-1}_{},^{k} ))|_{=0},\\ g_{^{k}}=_{2}F^{k}(s^{k-1}_{},^{k})^{ } x^{k}=d_{}.(_{4}^{k}(s^{ k}_{},^{k},s^{k-1}_{},^{k}))|_{=0}, .\]

the same error signal can by passed through \(^{k}\) (\( s^{k} s^{k-1}\)) where BP and EP are _implicitly_ chained inside \(^{k}\) (see Appendix A.2.1). This insight, along with a centered scheme to estimate derivatives with respect to \(\) around 0 as done for the C-EP algorithm (Eq. (4)), motivates the implicit BP-EP gradient chaining algorithm in Alg. 2 we used for our experiments (see Alg. 5 inside Appendix A.3.1 for its explicit counterpart). Given that the proposed algorithm appears as a a generalization of EP, we refer to Alg. 2as "EP" in the experimental section, for simplicity.

```
1:\( s,g_{^{N}}_{s^{N}-}(_{},y), _{^{N}}(_{},y)\)\(\) Single BP step
2:for\(k=N-1 1\)do
3:\(s_{}}[^{k}(s, ^{k},s_{}^{k-1},^{k})+ s^{} s]\)\(\) EP in \(^{k}\)
4:\(s_{-}}[^{k}(s, ^{k},s_{}^{k-1},^{k})- s^{} s]\)
5:\(g_{^{k}}(_{2}^{k}(s_ {},^{k},s_{}^{k-1},^{k})-_{2}^{k}(s_ {-},^{k},s_{}^{k-1},^{k}))\)
6:\(g_{^{k}}(_{4}^{k}(s_ {},^{k},s_{}^{k-1},^{k})-_{4}^{k}(s_ {-},^{k},s_{}^{k-1},^{k}))\)\(\) Implicit BP in \(F^{k}\)
7:\( s(_{3}^{k}(s_{}, ^{k},s_{}^{k-1},^{k})-_{3}^{k}(s_{- },^{k},s_{}^{k-1},^{k}))\)
8:endfor ```

**Algorithm 2** Implicit BP-EP gradient chaining (Theorem (3.1))

Recovering backprop.When the ff-EBM under consideration is purely feedforward (\(U^{k}=0\)), we show that Eqs. (11)-(12) reduce to standard BP through a feedforward net (Corollary A.1, Alg. 6 and Alg. 7 in Appendix A.2.1). Since this case is extremely close to standard BP through feedforward nets, we do not consider this setting in our experiments.

## 4 Experiments

In this section, we present the ff-EBMs used in our experiments (Section 4.1) and carry out _static_ gradient analysis - computing and analyzing ff-EBM parameter gradients for some \(x\) and \(y\) (Section 4.2). We extend the observation made by Ernoult et al. (2019) -that _transient_ EP parameter gradients obtained during the second phase match those computed by automatic differentiation through equilibrium and across blocks- to ff-EBMs (Fig. (3)-(4), Theorem 4.1), showing that gradient estimates of automatic differentiation and EP in our framework, are near perfectly aligned (Fig. 5). We then show on the CIFAR-10 task that performance of ff-EBMs can be maintained or improved across various block splits maintaining the same number of layers, while remaining on par with automatic differentiation(Section 4.3). We show furthermore that blocks of smaller size are up to four times faster to simulate depending on the convergence criterion at use for computing equilibrium inside EB blocks. Finally, we perform further ff-EBM training experiments on CIFAR-100 and ImageNet32 where we establish a new state-of-the-art performance in the EP literature (Section 4.4).

### Setup

Models.Using the same notations as in Eq. (6), the ff-EBMs at use in this section are defined:

\[\{U^{k}_{}(s^{k},^{k}):=-s^ {k}^{k} s^{k},\\ U^{k}_{}(s^{k},^{k}):=-s^{k}( ^{k} s^{k}).,\{F^{k}_{ }(s^{k-1},^{k}):=((^{ k}_{} s^{k-1}_{L});^{k}_{},^{k}_{} ),\\ F^{k}_{}(s^{k-1}):=s^{k-1}_{L}.\] (13)

with \((;^{k}_{},^{k}_{})\), \(\) and \(\) the batchnorm, pooling and convolution operations, \(\) the generalized dot product for tensors and \(s^{k}:=(s^{k}_{1}, s^{k}_{L})^{}\) the state of block \(k\) comprising \(L\) layers. Such EBM blocks are known as Deep Hopfield Networks (DHNs). DHNs are comprised of fully connected (\(U^{k}_{}\)) and convolutional operations (\(U^{k}_{}\)) forming a symmetric weight matrix \(^{k}\) with a sparse, block-wise structure such that each layer \(s^{k}_{}\) is bidirectionally connected to its neighboring layers \(s^{k}_{-1}\) and \(s^{k}_{+1}\) through connections \(^{k}_{-1}\) and \(^{k}_{}\) respectively (see Appendix A.1.3). To empirically ensure convergence, the non-linearity \(\) applied within EB blocks is \(_{}(x):=(( x,0),1)\) with \((0,1)\). Finally, two design choices were _instrumental_ to the success of ff-EBM gradient computation and subsequent training. First, we initialized the weights of \(U^{k}_{}\) and \(U^{k}_{}\) usingGaussian Orthogonal Ensembles_ (GOE) (Agarwala and Schoenholz, 2022) to enable faster equilibrium computation (see next paragraph). Second, while the last layer of a given block was simply passed as an input to the next block (i.e. using \(F_{}^{k}\) in Eq. (13)) for small enough models (\(L=6\) inside the experiment depicted in Section 4.3), the use of batchnorm layers in between blocks (i.e. using \(F_{}^{k}\) in Eq. (13)) becomes essential for deeper models.

Equilibrium computation.As depicted in Alg. 2, the steady states \(s_{}\) may be computed with any loss minimization algorithm. Here, as in past works on EP (Ernoult et al., 2019; Laborieux et al., 2021; Esclier et al., 2024; Esclier et al., 2024), we employ a fixed-point iteration scheme to compute the EB blocks steady states. Namely, we iterate Eq. (7) until reaching equilibrium (the same scheme is used for ff-EBM inference, Alg. 1, with \(=0\).):

\[s_{,t+1}^{k}(x^{k}-_{1}U^{k}(s_{,t }^{k},^{k}) s^{k})\] (14)

Important details about how Eq. (14) is executed in practice have to be highlighted. First, we employ a scheme to _asynchronously_ update even (\(s_{2^{}}^{k}\)) and odd (\(s_{2^{}+1}^{k}\)) layers (Scellier et al., 2024) - see Appendix A.1.3. Second, Eq. (14) were either executed for a _fixed_ and predetermined number of steps as done in the aforementioned EP literature, or using an \(-\)tolerance-based convergence criterion (TOL) which stops executing Eq. (14) when \((s_{t+1}^{i}-s_{t}^{i})/s_{t}^{i}\)_on average_ - see Appendix A.5.3 for details.

Algorithm baseline.As an algorithmic baseline, we simply use automatic differentiation (AD) backward through the fixed-point iteration scheme Eq. (14) with \(=0\) and directly initializing \(s_{t=0}^{k}=s_{}\) (Fig. 4). This version of AD, where we _backpropagate through equilibrium_, is known as "Recurrent Backpropagation" (Almeida, 1987; Pineda, 1987) or Implicit Differentiation (ID).

### Static comparison of EP and ID on ff-EBMs

In order to study the _transient dynamics_ of ID and EP, we define, with \(W^{k}:=\{^{k},^{k}\}\):

\[\{_{W^{k}}^{}(t):=_{k=0}^{T} d_{W^{k}(T-k)}(x,W^{k},y),\\ _{W^{k}}^{}(,t):=(_{W^{ k}}^{k}(s_{,t}^{k},W^{k},s_{}^{k-1})-_{W^{k}} ^{k}(s_{-,t}^{k},W^{k},s_{}^{k-1})),.\] (15)

where \(s_{,t}^{k}\) is computed from Eq. (14) with the nudging error current \( s^{k}\) computed with Alg. 2, and \(T\) is the total number of iterations used for both ID and EP in the gradient computation phase.

For a given block \(k\), \(d_{W^{k}(T-k)}(x,W,y)\) is the "sensitivity" of the loss \(\) to parameter \(W^{k}\) at timestep \(T-k\) so that \(_{W^{k}}^{}(t)\) is a ID gradient _truncated_ at \(T-t\). Fig. 4 depicts the computational graph that is differentiated through when using ID and shows where \(_{W^{k}}^{}(t)\) are obtained correspondingly. Similarly, \(_{W^{k}}^{}(t)\) is an EP gradient truncated at \(t\) steps forward through the nudged phase. When \(T\) is sufficiently large, \(_{W^{k}}^{}(T)\) and \(_{W^{k}}^{}(T)\) converge to \(d_{W^{k}}(x,W,y)\). Fig. 3 displays \((_{W^{k}}^{}(t))_{t 0}\) and \((_{W^{k}}^{}(t))_{t 0}\) on an heterogeneous ff-EBM of 6 blocks and 15 layers (16 if counting the last linear

Figure 3: EP and ID partially computed gradients (\((_{w}^{}(t))_{t 0}\) in black dotted curves and \((_{w}^{}(t))_{t 0}\) in plain colored curves) going _backward through equilibrium_ for ID and _forward through the nudging phase_ for EP (Ernoult et al., 2019) for a random sample \(x\) and associated label \(y\). The ff-EBM comprises 6 blocks and 15 layers in total, with block sizes of either 2 or 3 layers. Each sub-panel represents a layer (labeled on the y-axis) with each curve corresponding to a randomly selected weight. “Backward” time is indexed from \(t=0\) to \(T=120\), starting from block 6 backward to block 1, with 20 fixed-point iteration dynamics (Eq. (14)) being used for both EP and ID within each block.

"readout" layer computing the logits) with blocks comprising 2 or 3 layers for a randomly selected sample \(x\) and its associated label \(y\) - see caption for a detailed description. It can be seen EP and ID error weight gradients qualitatively match very well throughout time, across layers and blocks. We also display the cosine similarity between the final EP and ID weight gradient estimate \(^{}_{W^{k}}(T)\) and \(^{}_{W^{k}}(T)\) for each layer and observe that EP and ID weight gradients are near perfectly aligned. Theorem 4.1 generalizes the equivalence between EP and ID to ff-EBMs (Ernould et al., 2019).

**Theorem 4.1** (Informal).: _Assuming \( k=1 N-1:\,s_{k}^{}==s_{}^{k}=s_{}^{k}\):_

\[ k=1 N-1,\,\, t=0:^{ }_{W^{k}}(t)=^{}_{W^{k}}(t)=_{  0}^{}_{W^{k}}(,t)\] (16)

### Splitting experiment

For a given (standard, single block) EBM with a _fixed_ number of layers, we ask how block splitting of this EBM into a ff-EBM with multiple EB blocks affects training performance and Wall Clock (simulation) Time (WCT). We address this question with two different depths (\(L=6\) and \(L=12\) layers in total) and various block sizes (bs), maintaining a fixed total number of layers (e.g. for \(L=6\), 1 block of 6 layers, 2 blocks of 3 layers, etc.). Additionally, to ensure the fairest comparison in terms of WCTs across different splits, _we adopt the aforementioned TOL approach_ to execute the fixed-point dynamics Eq. (14) within each EB block. We display the results obtained on the CIFAR-10 task inside Table 1.

We observe that EP performance improves with smaller block sizes (reaching \(90.1\%\) and \(92.5\%\) for \(L=6\) and \(L=12\) respectively with \(=2\) and \(=3\)) with overall WCT reduction (up to \( 4\)) while remaining on par with ID. This significant reduction in

    &  &  \\  & Top-1 (\%) & WCT & Top-1 (\%) & WCT \\ 
**L=6** & & & & \\ bs=6 & 89.2 \({}^{ 0.2}\) & **7:01** & 87.3 \({}^{ 0.4}\) & **6:51** \\ bs=3 & 89.8 \({}^{ 0.2}\) & **5:17** & 89.3 \({}^{ 0.2}\) & **5:10** \\ bs=2 & 90.1 \({}^{ 0.1}\) & **3:57** & 90.0 \({}^{ 0.1}\) & **4:05** \\
**L=12** & & & & \\ bs=4 & 89.4 \({}^{ 0.7}\) & **11:59** & 89.5 \({}^{ 0.2}\) & **8:28** \\ bs=3 & **92.5 \({}^{ 0.1}\)** & **7:33** & 92.0 \({}^{ 0.1}\) & **4:16** \\ bs=2 & 92.0 \({}^{ 0.2}\) & **3:14** & 91.5 \({}^{ 0.2}\) & **3:07** \\   

Table 1: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR-10 by EP (Alg. 2) and ID on models with different number of layers (\(L\)) and block sizes (“bs”). 3 seeds are used.

Figure 4: **Light grey: computational graph associated with ff-EBM inference (Alg. 1) when applying fixed-point iteration to compute equilibrium states within each block (Eq. (14)) where the node \(s_{t}^{k}\) denotes the state of block \(k\) (comprising several layers) at timestep \(t\). Blue arrows: backward automatic differentiation (AD) through the computational graph where \(^{}_{W^{k}}(t)\) is the partially computed gradient truncated at \(T-t\). Since the states which are differentiated through are taken at equilibrium (\(s_{t}^{k}=s_{}^{k}\)\( t=0\)) this instantiation of AD can be viewed as _Implicit Differentiation_ (ID).**

Figure 5: Cosine similarity between EP and ID weight gradients on a randomly selected sample \(x\) and associated label \(y\) in the same setting as Fig. 3 using the same color code to label the layers. We observe near-perfect alignment between EP and ID gradients. See Fig. 7 for a precise depiction of the model at use.

WCT is due to the fact that inference time for ff-EBMs with DHN blocks by construction scales _linearly_ with the number of blocks rather than supralinearly with the number of layers as has been empirically observed in the EP literature (Ernoult et al., 2019). When instead using a _fixed number of iterations_ to execute Eq. (14) inside EB blocks (Table 5 in Appendix A.5.3), EP performance is _maintained_ across all splits (\(90.1\%\) and \(92.5\%\) for \(L=6\) and \(L=12\) resp.) and is still on par with ID. However, there is no advantage in terms of WCTs in this case as the number of iterations is kept the same across all block splits and is much larger than necessary for smaller block sizes. Results for \(L=6\) are consistent with the existing literature and those for \(L=12\) surpass EP state-of-the-art on CIFAR-10 (Scellier et al., 2024; Laborieux and Zenke, 2022). Overall these results suggest that: i) ff-EBM performance is agnostic to EB block sizes and are therefore flexible in design, ii) ff-EBMs are much faster to simulate that EBM counterparts of equivalent depth.

### Scaling experiment

We now consider ff-EBMs of _fixed_ block size 2, and _relatively small_ number of iterations. We train two models of depth (\(L=12\) and \(L=15\)) on CIFAR-100 and ImageNet32 by EP and ID and show the results obtained in Table 2. Here again we observe that EP matches ID performance on all models and tasks, ff-EBMs benefit from depth, and the performance obtained by training the 15-layer deep ff-EBM by EP exceeds state-of-the-art performance on ImageNet32 by around 10% top-1 validation accuracy (Laborieux and Zenke, 2022) and by around 5% the best performance reported on this benchmark among all backprop alternatives (Hoier et al., 2023).

## 5 Discussion

EP literature.Ever since fixed-point iteration schemes were first proposed to facilitate EP experiments (Ernoult et al., 2019; Laborieux et al., 2021), there has been a growing body of work assessing scalability of EP and its algorithmic extensions on standard vision tasks. Most notably, Laborieux and Zenke (2022) introduced a holomorphic version of EP where loss gradients are computed with adiabatic oscillations of the model by nudging in the complex plane, which was very recently extended to more general implicit models (Laborieux and Zenke, 2023). Moving further towards physical implementations of EP, Scellier et al. (2022) proposed a fully black-box version of EP where details about the system may not be known. All these advances could be readily applied inside our EP-BP chaining algorithm to EB blocks. The work closest to ours, albeit with a purely theoretical motivation and without clear algorithmic prescriptions, is that of Zach (2021) where feedforward model learning is cast as a deeply nested optimization in which consecutive layers are tied by elemental pair-wise energy functions. This work more recently inspired the Dual Propagation algorithm (Hoier et al., 2023). Such a setting can be construed as a particular case of ff-EBM learning by EP where each EB block comprises a _single_ layer (\(U^{k}=0\)) inside Eq. (6)-which, as we have shown, is tantamount to standard BP(see last paragraph of Section 3.3).

Forward-only learning beyond EP.Given that _zeroth-order_ (ZO) optimization and "forward-forward" (FF) algorithms (Dellarferrera and Kreiman, 2022; Hinton, 2022) can be applied to _any_

    & &  &  \\  & & Top-1 (\%) & Top-5 (\%) & WCT & Top-1 (\%) & Top-5 (\%) & WCT \\  CIFAR100 & L=12 & 69.3 \({}^{ 0.2}\) & 89.9 \({}^{ 0.5}\) & 4:33 & 69.2\({}^{ 0.1}\) & 90.0 \({}^{ 0.2}\) & 4:16 \\  & L=15 & 71.2\({}^{ 0.2}\) & 90.2\({}^{ 1.2}\) & 2:54 & 71.1\({}^{ 0.3}\) & 90.9 \({}^{ 0.1}\) & 2:44 \\   & L=12 & 44.7 \({}^{ 0.1}\) & 61:00 \({}^{ 0.1}\) & 65:23 & 44.7 \({}^{ 0.6}\) & 68.9\({}^{ 0.6}\) & 57:00 \\  & L=15 & **46.0**\({}^{ 0.1}\) & **70.0**\({}^{ 0.2}\) & 46:00 & 45.5 \({}^{ 0.1}\) & 69.0 \({}^{ 0.1}\) & 40:01 \\   Laborieux and Zenke (2022) & 36.5 & 60.8 & – & – & – & – \\ Hoier et al. (2023) & 41.5 & 64.9 & – & – & – & – \\   

Table 2: Validation accuracy and Wall Clock Time (WCT) obtained on CIFAR100 and ImageNet32 by EP and Autodiff on models with different number of layers (\(L\)) and a block size of 2 (bs=2). 3 seeds are used. We compare our results against best published results on ImageNet32 by EP (Laborieux and Zenke, 2022) and against all backprop alternatives (Hoier et al., 2023).

model, and-like EP- compute a learning rule through multiple inference steps, one may wonder why it is important that our models should be energy-based. While mechanistically appealing for analog hardware [Oguz et al., 2023, Momeni et al., 2023, Xue et al., 2024], these forward-only approaches do not match the performance of automatic differentiation on equivalent models, even if they are roughly the same size as those studied in our work. On the one hand, weight perturbation [Fiete et al., 2007] (WP or "SPSA" [Spall, 1998]), yields unbiased yet noisy gradient estimates with variance scaling cubically with the model dimensionality [Ren et al., 2022], resulting in a significant gap in model performance compared to backprop, that can only be partially mitigated when using heuristics [Silver et al., 2021, Ren et al., 2022, Fournier et al., 2023, Chen et al., 2023]. On the other hand FF algorithms, as learning heuristics, suffer from a lack of theoretical guarantees which may impact the resulting model performance.

Limitations and future work.Since our recipe advocates EP-BP chaining by construction, it is fair to say that ff-EBM learning partially inherits the pitfalls of BP. Fortunately, nothing prevents feedforward modules inside ff-EBMs from being trained by _any_ BP alternative to mitigate specific issues. For instance: BP can be parameterized by feedback weights to obviate weight transport from the inference circuit to the gradient computation circuit [Akrout et al., 2019]; BP gradients can be approximated as finite differences of feedback operators [Ernoult et al., 2022]; or computed via implicit forward-mode differentiation [Hiratani et al., 2022, Fournier et al., 2023, Malladi et al., 2023]; local layer-wise self-supervised or supervised loss functions can be used to prevent "backward locking" [Beliilovsky et al., 2019, Ren et al., 2022, Hinton, 2022]. This insight may help exploring many variants of ff-EBM training.

Pursuing the core motivation of this work, one natural extension of this study is to incorporate _more hardware realism into ff-EBMs_. Beyond Deep Hopfield networks, Deep Resistive Nets (DRNs) - developed by Scellier  and strongly inspired by Kendall et al.  - are exact models of idealized analog circuits, trainable by EP, promising fast simulation times. As such, using DRNs as EB blocks inside ff-EBMs is an exciting research direction - see Fig. 6. Still, further work in this direction presents new challenges especially given device non-idealities which may affect the inference pathway, such as analog-to-digital and digital-to-analog noise [Rasch et al., 2023, Lammei et al., 2024]. Finally, considerable work is needed to prove ff-EBM further at scale on more difficult tasks (e.g. standard ImageNet), considerably deeper architectures, and moving beyond vision tasks. One other exciting research direction would be the design of _ff-EBM based transformers_, with attention layers being chained with energy-based fully connected layers inside attention blocks.

Concluding remarks and broader impact.We show that ff-EBMs constitute a novel framework for deep-learning in heterogeneous hardware settings. We hope that the proposed algorithm can help to overcome the typical division between digital _versus_ analog or BP _versus_ BP-free algorithms and that the greater energy-efficiency afforded by this framework provides a pragmatic, near-term blueprint to mitigating the dramatic carbon footprint of AI training [Strubell et al., 2020]. While we are still a long way from fully analog training accelerators at commercial maturity, we believe this work offers an incremental and sustainable roadmap to gradually integrate analog, energy-based computational primitives as they are developed into existing digital accelerators.

Figure 6: ff-EBMs as hierarchical systems implementing EP at chip scale (adapted from [Yi et al., 2023]) using _energy-based_ analog processors made up of resistors (green edges), diodes (in blue), voltage sources (in purple), ADCs and DACs (adapted from [Scellier, 2024]), digital processors, memory buffers, all of these being connected by digital buses (red lines).

## Author contributions

TN was responsible for implementation, architecture design, coding all algorithmic details and running training experiments, as well as discovery of criteria for stable convergence. TN also participated in writing relevant portions of this manuscript. ME designed the study, derived all theoretical results, debugged and refactored the initial codebase and wrote most of the manuscript.