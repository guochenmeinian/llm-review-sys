# Latent Graph Inference with Limited Supervision

Jianglin Lu\({}^{1}\)  Yi Xu\({}^{1}\)  Huan Wang\({}^{1}\)  Yue Bai\({}^{1}\)  Yun Fu\({}^{1,2}\)

\({}^{1}\)Department of Electrical and Computer Engineering, Northeastern University

\({}^{2}\)Khoury College of Computer Science, Northeastern University

Project Page: https://jianglin954.github.io/LGI-LS/

Corresponding author: JianglinLu@outlook.com.

###### Abstract

Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of _supervision starvation_, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to _restore the corrupted affinities and replenish the missed supervision for better LGI_. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as _\(k\)-hop starved nodes_, which can be identified based on a given adjacency matrix. Considering the high computational burden, we further present a more efficient alternative inspired by _CUR matrix decomposition_. Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections. Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%).

## 1 Introduction

Graph neural networks (GNNs)  have recently received considerable attention due to their strong ability to handle complex graph-structured data. GNNs consider each data sample as a node and model the affinities between nodes as the weights of edges. The edges as a whole constitute the graph structure or topology of the data. By integrating the graph topology into the training process of representation learning, GNNs have achieved remarkable performance across a wide range of tasks, such as classification , clustering , retrieval , and recognition .

Although effective, existing GNNs typically require a prior graph to learn node representations, which poses a major challenge when encountering incomplete or even missing graphs. This limitation has spurred the development of latent graph inference (LGI) , also known as graph structure learning . LGI aims to jointly learn the underlying graph and discriminative node representations solely from the features of nodes in an end-to-end fashion. By adaptively learning the graph topology, LGI models are empowered with great ability to remove noise and capture more complex structure of the data . Consequently, LGI emerges as a promising research topic with a broad range of applications, such as point cloud segmentation , disease prediction , multi-view clustering , and brain connectome representation .

However, many LGI methods suffer from a so-called _supervision starvation_ (SS) problem , where a number of edge weights are learned without any semantic supervision during the graph inference stage. Specifically, given a \(k\)-layer GNN, if a node and all of its predefined neighbors (from \(1\)- to \(k\)-hop) are unlabeled, the edge weights between this node and its neighbors will not contribute to the training loss and cannot be semantically optimal after training. This will lead to poor generalization performance since these under-trained weights are inevitably used to make predictions for testing samples. In this paper, we discover that the SS problem is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. Based on this observation, we propose to _restore the destroyed connections and replenish the missed supervision for better LGI_.

Specifically, we first define the pivotal nodes as _\(k\)-hop starved nodes_. Then, the SS problem can be transformed into a more manageable task of eliminating starved nodes. We propose to identify the \(k\)-hop starved nodes based on the \(k\)-th power of a given adjacency matrix. After identification, we can diminish the starved nodes by incorporating a regularization adjacency matrix into the initial one. However, the above identification approach suffers from high computational complexity due to matrix multiplication. For example, given a \(2\)-layer GNN, identifying \(2\)-hop starved nodes requires at least \((n^{3})\), where \(n\) denotes the number of nodes. To address this, we present a more efficient alternative solution inspired by _CUR matrix decomposition_. We find that with appropriate column and row selection, the \(U\) matrix obtained through CUR decomposition of the initial adjacency matrix is actually a zero matrix. Thus, we can eliminate the starved nodes by reconstructing the \(U\) matrix.

Although recovering the \(U\) matrix encourages more supervision, such strategy may cause a potential issue we call _weight contribution rate decay_. In other words, the weight contribution rate (WCR) of non-starved nodes decays as the number of starved nodes increases, resulting in a final latent graph that heavily relies on the regularization one. To tackle this issue, we propose two simple strategies, _i.e._, decreasing the WCR of starved nodes and increasing the WCR of non-starved nodes (see Sec. 3.3 for details). The main contributions of this paper can be summarized as follows:

* We identify that graph sparsification is the main cause of the supervision starvation (SS) problem in latent graph inference (LGI). Therefore, we propose to restore the affinities corrupted by the sparsification operation to replenish the missed supervision for better LGI.
* By defining \(k\)-hop starved nodes, we transform the SS issue into a more manageable task of removing starved nodes. Inspired by CUR decomposition, we propose a simple yet effective solution to determine the starved nodes and diminish them using a regularization graph.
* The proposed approach is model-agnostic and can be seamlessly integrated into various LGI models. Extensive experiments demonstrate that eliminating the starved nodes consistently enhances the state-of-the-art LGI methods, particularly under extremely limited supervision.

## 2 Preliminaries

### Notations

Let \(_{i:}\), \(_{:j}\), \(_{ij}\), and \(^{k}\) denote the \(i\)-th row, the \(j\)-th column, the element at the \(i\)-th row and \(j\)-th column, and the \(k\)-th power of the matrix \(\), respectively. Let \(_{n}\) represent an \(n\)-dimensional column vector with all elements being \(1\). Let \(_{^{+}}()\) be an element-wise indicator function that sets \(_{ij}\) to \(1\) if it is positive, and \(0\) otherwise. Conversely, \(_{^{-}}()\) sets \(_{ij}\) to \(0\) if it is positive, and \(1\) otherwise.

### Latent Graph Inference

**Definition 1** (Latent Graph Inference).: _Given a graph \((,)\) containing \(n\) nodes \(=\{V_{1},,V_{n}\}\) and a feature matrix \(^{n d}\) with each row \(_{i}.^{d}\) representing the \(d\)-dimensional attributes of node \(V_{i}\), latent graph inference (LGI) aims to simultaneously learn the underlying graph topology encoded by an adjacency matrix \(^{n n}\) and the discriminative \(d^{}\)-dimensional node representations \(^{n d^{}}\) based on \(\), where the learned \(\) and \(\) are jointly optimal for certain downstream tasks \(\) given a specific loss function \(\)._

In general, an LGI model mainly consists of a latent graph generator \(_{}()\): \(^{n d}^{n n}\) that generates an adjacency matrix \(\) based on the node features \(\), and a node encoder \(_{}(,)\):\(^{n d}^{n d^{}}\) that learns discriminative node representations \(\) based on \(\) and the learned \(\). In practice, \(_{}\) is typically implemented using a \(k\)-layer GNN, while \(_{}\) can be realized through different strategies such as full parameterization , MLP , or attentive network . In this paper, we adopt the most common settings from existing LGI literature , considering \(\) as the semi-supervised node classification task and \(\) as the cross-entropy loss.

### Supervision Starvation

To illustrate the supervision starvation problem , we consider a general LGI model \(\) consisting of a latent graph generator \(_{}\) and a node encoder \(_{}\). For simplicity, we ignore the activation function and assume that \(_{}\) is implemented using a \(1\)-layer GNN, _i.e._, \(_{}=_{1}(,;)\), where \(=_{}()\). For each node \(_{i:}\), the corresponding node representation \(_{i:}\) learned by the model \(\) can be expressed as:

\[_{i:}=_{i:}=(_{j }_{ij}_{j:}),\] (1)

where \(=\{j_{^{+}}()_{ij}=1\}\) and \(_{ij}=_{}(_{i:},_{j:})\). Consider the node classification loss:

\[_{,}=_{i_{L}}_{ j=1}^{||}_{ij}_{ij}=_{i_{L}} _{i:}_{i:}^{}=_{i_{L}} _{i:}((_{j}_{ij}_{j:}) )^{},\] (2)

where \(_{L}\) represents the set of indexes of labeled nodes and \(||\) denotes the size of label set. For \( i_{L}\), \(j\), \(_{ij}\) is optimized via backpropagation under the supervision of label \(_{i:}\). For \( i_{L}\), however, if \(j_{L}\) for \( j\), \(_{ij}\) will receive no supervision from any label and, as a result, cannot be semantically optimal after training. Consequently, the learning models exhibit poor generalization as the predictions of testing nodes inevitably rely on these supervision-starved weights. This phenomenon is referred to as _supervision starvation_ (SS), where many edge weights are learned without any label supervision. It is easy to infer that this issue also persists in a \(k\)-layer GNN.

_We may ask why this problem arises?_ In fact, the SS problem is caused by a common and necessary post-processing operation known as graph sparsification, which is employed in the majority of LGI methods  to generate a sparse latent graph. To be more specific, graph sparsification adjusts the initial dense graph to a sparse one through the following procedure:

\[_{ij}=_{ij},&\ \ _{ij} (_{i:})\\ 0,&,\] (3)

where \((_{i:})\) denotes the set of the top \(\) values in \(_{i:}\). After this sparsification operation, a significant number of edge weights are directly erased, including the crucial connections established between pivotal nodes and labeled nodes. _Another question that may arise is: how many important nodes or connections suffer from this problem?_ We delve into this question in the next section.

## 3 Methodology

### Identification & Elimination of Starved Nodes

We first introduce the definitions of \(k\)-hop starved node and the corresponding \(k\)-hop starved weight.

**Definition 2** (\(k\)-hop Starved Node).: _Given a graph \((,)\) consisting of \(n\) nodes \(=\{V_{1},,V_{n}\}\) and the corresponding node features \(\), for a \(k\)-layer graph neural network \(_{k}(;)\) with network parameters \(\), the unlabeled node \(V_{i}\) is a \(k\)-hop starved node if, for \(\{1,,k\}\), \( V_{j}_{}(i)\), where \(_{}(i)\) is the set of \(\)-hop neighbors of \(V_{i}\), \(V_{j}\) is unlabeled. Specifically, \(0\)-hop starved nodes are defined as the unlabeled nodes2._

Based on Definition 2, we can define the \(k\)-hop starved weight as follows.

**Definition 3** (\(k\)-hop Starved Weight).: _If an edge exists between nodes \(V_{i}\) and \(V_{j}\), the associated edge weight \(_{ij}\) is a \(k\)-hop starved weight if both of the \(V_{i}\) and \(V_{j}\) qualifies as \((k-1)\)-hop starved nodes3._

According to the definition presented, it is evident that \(k\)-hop starved weights are precisely the ones that receive no semantic supervision from labels. As a result, to address the SS problem, our focus should be on reducing the presence of \(k\)-hop starved nodes. The following theorem illustrates how we can identify such nodes based on a given initial adjacency matrix.

**Theorem 1**.: _Given a sparse adjacency matrix \(^{n n}\) with self-connections generated on graph \((,)\) by a latent graph inference model with a \(k\)-layer graph neural network \(_{k}(;)\), the node \(V_{i}\) is a \(k\)-hop starved node, if \( j\{1,,n\}\), such that \([_{^{+}}()]_{ij}^{k}=1\), and for \( j\{j[_{^{+}}()]_{ij}=1[ _{^{+}}()]_{ij}^{2}=1[_ {^{+}}()]_{ij}^{k}=1\}\), \(V_{j}\) is unlabeled._

Proof.: Please refer to the supplementary material for details. 

To provide an intuitive perspective, we use two real-world graph datasets, namely Cora (\(2708\) nodes) and Citeseer (\(3327\) nodes), as examples. We calculate the number of \(k\)-hop starved nodes for \(k=1,2,3,4\), based on their original graph topology. Fig. 1 shows the statistical results for the Cora140, Cora390, Citeseer120, and Citeseer370 datasets, where the suffix number represents the number of labeled nodes. From Fig. 1, we observe that as the value of \(k\) increases, the number of starved nodes decreases. This can be explained by the fact that as \(k\) increases, the nodes have more neighbors (from \(1\)- to \(k\)-hop), and the possibility of having at least one labeled neighbor increases. Adopting a deeper GNN (larger \(k\)) can thus mitigate the SS problem. However, it is important to consider that deeper GNNs result in higher computational consumption and may lead to poorer generalization performance . Furthermore, as shown in Fig. 1, even with a \(4\)-layer GNN, there are still hundreds of \(4\)-hop starved nodes in the Citeseer120. Therefore, we believe that employing a deeper GNN is not the optimal solution to resolve the SS problem 4.

In fact, Theorem 1 implicitly indicates how to alleviate the SS problem. Intuitively, a straightforward solution is to ameliorate the given adjacency matrix \(\) in order to reduce the number of starved nodes. This can be accomplished simply by reconstructing the connections between starved nodes and the labeled ones. Technically, we can achieve this by adding a regularization adjacency matrix \(\) to \(\):

\[}=+,\] (4)

where \(}\) is the refined adjacency matrix, \(\) models the recovered affinities between stayed nodes and labeled ones, and \(\) is a balanced parameter that controls the contribution of \(\) and \(\). According to Theorem 1, we can identify the starved nodes based on \(\) and replenish the missed supervision through \(\), thereby preventing \(}\) from being starved. Specifically, for each starved node \(V_{i}\), we can search for at least one closest labeled node \(V_{l}\), and restore at least one connection between \(V_{l}\) and \(V_{j}\) for \(j\{_{}(i) i\}\) such that \([_{^{+}}()]_{jl}^{s}=1\), where \(\) can be arbitrarily chosen from the set of \(\{1,2,,k\}\). Although this strategy is effective, it may be computationally complex. Even with a small value of \(k\), the computational cost of identifying \(k\)-hop starved nodes based on Theorem 1 is prohibitively expensive. For example, when identifying \(2\)-hop starved nodes, the time complexity of computing \(^{2}\) alone reaches \((n^{3})\). In the next section, we will propose a more efficient solution.

Figure 1: Illustration of \(k\)-hop starved nodes on different datasets. Obviously, the number of \(k\)-hop starved nodes decreases as the value of \(k\) increases.

### CUR Decomposition Makes Better Solution

Inspired by CUR matrix decomposition , we propose an efficient alternative approach to identify the starved nodes. We first present the definition of CUR matrix decomposition.

**Definition 4** (CUR Decomposition ).: _Given \(^{n m}\) of rank \(=()\), rank parameter \(k<\), and accuracy parameter \(0<<1\), construct column matrix \(^{n c}\) with \(c\) columns from \(\), row matrix \(^{r m}\) with \(r\) rows from \(\), and intersection matrix \(^{c r}\) with \(c\), \(r\), and \(()\) being as small as possible, in order to reconstruct \(\) within relative-error:_

\[||-||_{F}^{2}(1+)||-_{k}||_{F}^{2}.\] (5)

_Here, \(_{k}=_{k}_{k}_{k}^{T}^{n m}\) is the best rank \(k\) matrix obtained via the singular value decomposition (SVD) of \(\)._

With the definition of CUR decomposition, we can find a more efficient solution to identify the starved nodes. The following theorem demonstrates how we can accomplish this goal.

**Theorem 2**.: _Given a sparse adjacency matrix \(^{n n}\) with self-connections generated on graph \((V,)\), construct \(=[:,col\_mask]^{n c}\), where \(col\_mask\{0,1\}^{n}\) contains only \(c\) positive values corresponding to \(c\) labeled nodes, and \(=[row\_mask,:]^{r n}\) with \(row\_mask=_{^{-}}(_{c})\{0,1\}^{n}\). Then, (a) \(=[row\_mask,col\_mask]=^{r c}\), where \(\) is a zero matrix, (b) the set of \(1\)-hop starved nodes \(_{1}(r)=\{V_{i}|i_{+}\}\), where \(_{+}^{r}\) indicates the set of indexes of positive elements from \(row\_mask\), and (c) for each \(i_{+}\), \(V_{i}\) is a 2-hop starved node if, for \( j\) satisfying \([_{^{+}}()]_{ij}=1\), \(j_{+}\)._

Proof.: Please refer to the supplementary material for details. 

Theorem 2 provides a more efficient alternative for identifying \(k\)-hop starved nodes for \(k\{1,2\}\). In fact, the column matrix \(\) models the relationships between all nodes and \(c\) labeled nodes, the row matrix \(\) models the affinities between \(r\)\(1\)-hop starved nodes and the whole nodes, and the intersection matrix \(\) models the strength of connections between \(r\)\(1\)-hop starved nodes and \(c\) labeled nodes. Theorem 2 states that \(=\), indicting that there are no connections between the starved nodes and the labeled ones. Based on this observation, we propose a simpler approach to reduce the number of starved nodes. Specifically, we rebuild the intersection matrix \(\) to ensure that the reconstructed \(}\)5. Consequently, Eq. (4) can be rewritten as:

\[}=+=+ (},n),\] (6)

where function \((},n)\) extends the matrix \(}^{r c}\) to an \(n n\) matrix by padding \(n-r\) rows of zeros and \(n-c\) columns of zeros in the corresponding positions.

The question now turns to how to reconstruct the intersection matrix \(}\). For simplicity, we directly adopt the same strategy used in constructing \(\). Specifically, for each row \(i\) of \(}\), we establish a connection between \(V_{i}\) and \(V_{j}\) for \( j_{+}\), where \(_{+}^{c}\) represents the set of indexes of positive elements from \(col\_mask\). We then assign weights to these connections based on their distance. Note that, if we ensure each row of \(}\) has at least one weight greater than 0, there will be no \(\)-hop starved nodes for \(>1\). This means that we do not need to feed the \(k\)-hop starved nodes satiated, simply feeding \(\)-hop ones for \(<k\) makes \(k\)-hop starved nodes cease to exist. In addition, compared with the time complexity of \((dn^{2})\) to construct \(\), the time complexity of reconstructing \(}\) is \((drc)\). The additional computational burden is relatively negligible since \(c n\).

### Weight Contribution Rate Decay

Directly replenishing the additional supervision encoded in \(}\) by Eq. (6) may cause a potential issue we refer to as _weight contribution rate decay_. To understand this issue, we consider the \(j\)-th columnvector \(}_{;j}\) of \(}\) for \(j\!_{+}\). We define the weight contribution rates (WCR) of non-starved nodes \(_{-}\) and the WCR of starved nodes \(_{+}\) as:

\[_{-}=_{+}}}_{ij}}{_{i }}_{ij}}=_{+}}_{ij} }{_{i}}_{ij}},_{+}=1-_{-}=1-_{+}}_{ij}}{_{i}}_{ij}}= _{+}}}_{ij}}{_{i} }_{ij}}.\] (7)

From Eq. (7), we observe that the WCR of non-starved nodes \(_{-}\) decays as the number of starved nodes increases (_i.e._, \(_{+}\) increases). This means that \(_{-}\) becomes negligible if there are numerous starved nodes. As a result, the \(j\)-th column vector \(}_{;j}\) of \(}\) for \(j\!_{+}\) heavily relies on the \(j\)-th column vector \(}_{;j}\) of the reconstructed intersection matrix \(}\). This outcome is not desirable since the regularization imposed should not dominate a significant portion of the final adjacency matrix. Recalling our initial objective of properly refining the original latent graph to replenish the missed supervision, we will design two simple strategies to relieve this issue.

**Decrease \(_{+}\).** On the one hand, we can decrease the WCR of starved nodes \(_{+}\) by selecting only \((<c)\) labeled nodes as the supplementary adjacent points for each \(1\)-hop starved node. This results in a sparse intersection matrix \(}\). For this strategy, we present the following proposition:

**Proposition 1**.: _Suppose that we randomly select \(\) out of \(c\) labeled nodes as the supplementary adjacent points for each \(1\)-hop starved node. If there are \(r\)\(1\)-hop starved nodes, then for \( j\!_{+}\), we have \(_{+}=_{+}} }_{ij}}{_{i}}_{ij}}_{+}\), where \(_{+}=_{+}\) with only a probability of \(()^{r}\)._

Proof.: Please refer to the supplementary material for details. 

**Increase \(_{-}\).** On the other hand, we can improve the WCR of non-starved nodes \(_{-}\) by magnifying the weights of non-starved nodes. Specifically, we construct an additional regularization matrix \(^{n c}\), where its \(c\) columns correspond to the \(c\) labeled nodes and \(_{ij}=0\) for \( i\!_{+}\). For \( i\!_{+}\), we establish connections between the node \(V_{i}\) and \((<c)\) labeled nodes \(V_{j}\), and assign the corresponding weights \(_{ij}\) using a similar strategy as in constructing \(}\). By adding the additional regularization matrix \(\), the refined adjacency matrix \(}\) can be expressed as:

\[}=+(( },n)+(,n)).\] (8)

Similarly, for this strategy, we have the following proposition:

**Proposition 2**.: _Suppose that we randomly select \(\) out of \(c\) labeled nodes as the supplementary adjacent points for each non-starved node. If there are \(r\)\(1\)-hop starved nodes, then for \( j\!_{+}\), we have \(_{-}=_{+}}}_{ij}} {_{i}}_{ij}}=_{+}}}_{ij}+_{i_{+}}_{ij}}{_{i} }_{ij}+_{i_{+}}_{ij}} _{-}\), where \(_{-}=_{-}\) with only a probability of \((1-)^{n-r}\)._

Proof.: Please refer to the supplementary material for details. 

### End-to-End Training

Note that the proposed approach is model-agnostic and can be seamlessly integrated into existing LGI models. In Sec. 4, we will apply our design to state-of-the-art LGI methods  and compare its performance with the original approach. Therefore, we follow these methods and implement the node encoder \(_{}\) using a \(2\)-layer GNN. Then, we calculate the cross-entropy loss \(_{}\) between the true labels \(\) and the predictions \(\), as well as a graph regularization loss \(_{}\) on \(}\):

\[=_{}+_{}=_ {i_{L}}_{j=1}^{||}_{ij}_{ ij}+_{},\] (9)

where \(\) is a balanced parameter and \(=_{}(,})= (}(} ^{0})^{1})\). According to different LGI methods, the graph regularization loss \(_{}\) can be different, such as Dirichlet energy  and self-supervision  (see supplementary material for more details). For fairness, in the experiments, we will adopt the same graph regularization loss as the comparison methods.

## 4 Experiments

### Experimental Settings

**Baselines.** As mentioned earlier, the proposed regularization module can be easily integrated into most existing LGI methods. To evaluate its effectiveness, we select representative LGI methods as baselines, including IDGL , GRCN , SLAPS , and LCGS . We also consider two additional baselines marked as GCN+KNN  and GCN&KNN . GCN+KNN is a two-step method that first constructs a KNN graph based on feature similarities and then feeds the pre-constructed graph to a GCN for training. GCN&KNN is an end-to-end method that learns the latent graph and network parameters simultaneously. For methods that require a prior graph, we use the same KNN graph as in GCN+KNN. For GCN&KNN, we simply adopt the Dirichlet energy  as the graph regularization loss.

**Datasets.** Following the common settings of existing LGI methods , we conduct experiments on four well-known benchmarks: Cora, Citeseer, Pubmed , and ogbn-arxiv . For detailed dataset statistics, please refer to the supplementary material. For all datasets, we only provide the original node features for training. To test the performance under different labeling rates, for the Cora and Citeseer datasets, we add half of the validation samples to the training sets, resulting in Cora390 and Citeseer370, where the suffix number represents the total number of labeled nodes.

**Implementation.** We compare the above baselines with their corresponding CUR extension versions. Specifically, we consider two CUR extensions termed M_U and M_R (M refers to the baseline), where the former adopts the sparse intersection matrix \(}\) as the regularization, and the latter combines \(}\) and \(\) together (see Sec. 3.3 for details). In experiments, we practically select the \(\) closest labeled nodes as supplementary adjacent points for each row of \(}\) and \(\). For postprocessing operations on the graph, such as symmetrization and normalization , we follow the baselines and adopt the same operations for fairness. We select the values of \(\) and \(\) from the sets \(\{10,15,20,25,30,50\}\) and \(\{0.01,0.1,1.0,10,50,100\}\), respectively. For other hyperparameters such as learning rate and weight decay, we follow the baselines and use the same settings. For each method, we record the best testing performance and report the average accuracy of five independent experiments, along with the corresponding standard deviation.

### Comparison Results

Table 1 presents the comparison results on all used datasets. It is evident that our proposed CUR extensions consistently outperform the corresponding baselines, demonstrating the effectiveness of eliminating starved nodes. When considering the labeling rates of different datasets listed in the third row of Table 1, we observe that the lower the labeling rate, the greater the performance

  Models / datasets & ogbn-arxiv & Cora390 & Cora140 & Citeseer370 & Citeseer120 & Pubmed \\  \# of labeled all nodes & 90941169343 & 3907208 & 1402708 & 370037327 & 12037327 & 60/19717 \\
improvement achieved by our methods. Notably, on the Pubmed dataset with an extremely low labeling rate of \(0.30\%\), the accuracy of our proposed methods (M_R) increase by \(6.12\%\), \(3.58\%\), and \(2.26\%\) compared to the basic models (M) of GCN+KNN, GRCN, and SLAPS, respectively. This is because the lower the labeling rate, the more starved nodes exist in the dataset (see Fig. 1 for an example). Our proposed methods aim to restore the destroyed affinities between starved nodes and labeled nodes, enabling us to leverage the semantic supervision missed by baselines, thereby achieving superior performance with extremely limited supervision.

Fig. 2 displays the training loss and testing accuracy curves of GCN+KNN, GRCN, SLAPS, and their CUR extensions on the Pubmed dataset. We omit the curves for the first 200 epochs of SLAPS because, during this stage, SLAPS focuses solely on learning the latent graph through an additional self-supervision task without involving the classification task. From Fig. 2, we observe that our proposed CUR extensions achieve higher testing accuracy compared to their corresponding baselines. In comparison with the CUR extensions of GCN+KNN, the CUR extensions of GRCN and SLAPS exhibit relatively stable accuracy results as the training epoch increases. This stability can be attributed to GRCN and SLAPS jointly learning the latent graph and network parameters in an end-to-end manner, whereas GCN+KNN pre-constructs a KNN graph and maintains a fixed graph during training without optimization. For GRCN, our proposed CUR extensions demonstrate lower training loss and higher testing accuracy, further indicating their improved generalization by removing starved nodes. Additionally, we observe relatively unstable training loss for the CUR extensions of SLAPS. The potential reason is that SLAPS introduces an additional graph regularization loss through a self-supervision task, while its CUR extensions aim to reconstruct the destroyed connections in the latent graph. As a result, the training process for the self-supervision task can exhibit some instability.

### Discussion

We would like to explore the question that _why our proposed methods yield a slight improvement on the Cora and Citeseer datasets, while achieving a substantial improvement on the Pubmed dataset_. As shown in Table 2, when \(=10\), there are \(100\) and \(299\)\(2\)-hop starved nodes on the Cora140 and Citeseer120 datasets, respectively. However, when \(=20\), there are no \(2\)-hop starved nodes on the Cora140 and Citeseer120 datasets, and the number of \(1\)-hop starved nodes also sharply decreases. In this scenario, since the comparison baselines all utilize a \(2\)-layer GNN, they are unaffected by \(2\)-hop starved nodes and only minimally affected by \(1\)-hop starved nodes. Consequently, our methods result in only slight improvements over the baselines on the Cora140 and Citeseer120 datasets. On the other hand, when \(=20\), there are still \(12,843\)\(2\)-hop starved nodes present on the Pubmed dataset. Since we effectively eliminate these starved nodes without requiring additional graph convolutional layers, our methods can provide notable benefits on this dataset with an extremely low labeling rate.

  Number of neighbors (\(\)) &  &  \\  Datasets & Cora140 & Citeseer120 & Pubmed & Cora140 & Citeseer120 & Pubmed \\  \(1\)-hop starved nodes & 1,625 & 2,268 & 19,076 & 993 & 1,577 & 18,478 \\ \(2\)-hop starved nodes & 100 & 299 & 16,756 & 0 & 0 & 12,843 \\  

Table 2: Number of \(k\)-hop starved nodes (\(k\{1,2\}\)) on various datasets when selecting different number of neighbors (\(\)) in graph sparsification operation.

Figure 2: Training loss (left vertical axis) and testing accuracy (right vertical axis) curves of GCN+KNN, GRCN, SLAPS, and their corresponding CUR extensions on the Pubmed dataset.

### Ablation Study

In this subsection, we aim to explore and answer the following questions.

_How many starved nodes should be eliminated?_ The results shown in Table 1 indicate that eliminating all starved nodes contributes to the performance improvement of the baselines. It is important to understand the relationship between the number of starved nodes removed and the corresponding performance improvement of the baselines. To investigate this, we randomly remain \(10\%,20\%,40\%,60\%,80\%\) starved nodes and evaluate the performance of GCN+CNN, GRCN, and SLAPS accordingly. The results on the Pubmed dataset are summarized in Table 3. We find that, for GCN+KNN, a smaller number of starved nodes leads to higher testing accuracy. For GRCN and SLAPS, however, we need to check the number of starved nodes to obtain optimal performance.

_How \(\) affects the performance._ Table 4 shows that the selection of \(\) slightly differentiates the performance of our proposed methods. In general, a larger value of \(\) leads to a higher improvement in performance. However, when \(\) is set too large, such as \(50\), the performance starts to degrade. This degradation occurs due to the introduction of incorrect labels when \(\) exceeds a certain threshold.

_How \(\) affects the performance._ Table 5 presents the sensitivity of parameter \(\) on the Pubmed dataset. It is observed that a relatively larger value of \(\), such as 10 or 50, leads to a significant improvement in performance. This finding further emphasizes the effectiveness of our proposed regularization methods in enhancing the performance of existing LGI models.

## 5 Related Work

**Latent Graph Inference.** Given only the node features of data, latent graph inference (LGI) aims to simultaneously learn the underlying graph structure and discriminative node representations from the features of data . For example, Jiang _et al._ propose to infer the graph structure by combining graph learning and graph convolution in a unified framework. Yang _et al._ model the topology refinement as a label propagation process. Jin _et al._ explore some intrinsic properties of the latent graph and propose a robust LGI framework to defend adversarial attacks on graphs. Though effective, these methods require a prior graph to guide the graph inference process. Controversially,

  The value of \(\) & baseline (0) & 10 & 15 & 20 & 25 & 30 & 50 \\  GCN+KNN\_U & 68.66 \(\) 0.05 & 71.86 \(\) 0.26 & 72.92 \(\) 0.17 & 73.50 \(\) 0.31 & 73.88 \(\) 0.28 & 74.12 \(\) 0.32 & 73.76 \(\) 0.29 \\ GCN+KNN\_R & 68.66 \(\) 0.05 & 73.10 \(\) 0.09 & 73.90 \(\) 0.14 & 74.02 \(\) 0.10 & 74.56 \(\) 0.12 & 74.78 \(\) 0.17 & 74.62 \(\) 0.12 \\  GRCN\_U & 69.24 \(\) 0.20 & 71.92 \(\) 0.87 & 72.56 \(\) 0.77 & 72.64 \(\) 1.03 & 72.80 \(\) 0.59 & 72.56 \(\) 1.02 & 71.80 \(\) 1.13 \\ GRCN\_R & 69.24 \(\) 0.20 & 72.12 \(\) 0.86 & 72.54 \(\) 0.77 & 72.80 \(\) 1.05 & 72.82 \(\) 1.03 & 72.44 \(\) 1.07 & 71.86 \(\) 1.11 \\ SLAPS\_U & 74.86 \(\) 0.79 & 75.98 \(\) 1.12 & 76.20 \(\) 0.87 & 75.58 \(\) 0.90 & 76.28 \(\) 0.61 & 76.74 \(\) 0.59 & 75.98 \(\) 0.89 \\ SLAPS\_R & 74.86 \(\) 0.79 & 76.00 \(\) 1.17 & 76.48 \(\) 0.69 & 76.40 \(\) 0.48 & 71.12 \(\) 0.77 & 76.58 \(\) 0.33 & 76.50 \(\) 0.59 \\  

Table 4: Parameter sensitivity of \(\) when applying our proposed method to GCN+CNN, GRCN, and SLAPS on the Pubmed dataset. The baseline results indicate the accuracy of the original methods.

  Starved nodes (\%) & 100 & 80 & 60 & 40 & 20 & 10 & 0 \\  GCN+KNN\_U & 68.66 \(\) 0.05 & 69.42 \(\) 0.12 & 70.20 \(\) 0.22 & 71.52 \(\) 0.15 & 72.40 \(\) 0.27 & 73.04 \(\) 0.29 & 74.12 \(\) 0.32 \\ GCN+KNN\_R & 69.12 \(\) 0.33 & 69.46 \(\) 0.22 & 70.46 \(\) 0.19 & 71.90 \(\) 0.15 & 73.24 \(\) 0.05 & 74.00 \(\) 0.14 & 74.78 \(\) 0.17 \\ GRCN\_U & 69.24 \(\) 0.20 & 72.14 \(\) 0.35 & 72.48 \(\) 0.43 & 73.04 \(\) 0.52 & 72.82 \(\) 0.75 & 72.96 \(\) 0.87 & 72.80 \(\) 0.99 \\ GRCN\_R & 70.20 \(\) 0.06 & 72.34 \(\) 0.30 & 72.54 \(\) 0.48 & 72.98 \(\) 0.37 & 72.80 \(\) 0.70 & 72.80 \(\) 0.74 & 72.82 \(\) 1.03 \\ SLAPS\_U & 74.86 \(\) 0.79 & 76.26 \(\) 0.62 & 76.48 \(\) 0.61 & 76.36 \(\) 0.39 & 76.48 \(\) 0.67 & 76.32 \(\) 0.71 & 76.74 \(\) 0.59 \\ SLAPS\_R & 75.64 \(\) 0.45 & 76.44 \(\) 1.27 & 76.52 \(\) 0.18 & 76.50 \(\) 1.22 & 76.38 \(\) 0.45 & 76.70 \(\) 0.59 & 77.12 \(\) 0.77 \\  

Table 3: Test accuracy (%) of our proposed CUR extensions for GCN+CNN, GRCN, and SLAPS when eliminating different number of starved nodes on the Pubmed dataset.

  The value of \(\) & baseline (0) & 0.01 & 0.1 & 1.0 & 10 & 50 & 100 \\  GCN+KNN\_U & 68.66 \(\) 0.05 & 68.04 \(\) 0.05 & 67.96 \(\) 0.08 & 68.14 \(\) 0.10 & 69.92 \(\) 0.12 & 72.70 \(\) 0.06 & 74.12 \(\) 0.32 \\ GCN+KNN\_R & 68.66 \(\) 0.05 & 67.96 \(\) 0.10 & 67.90 \(\) 0.11 & 68.18 \(\) 0.07 & 70.04 \(\) 0.16 & 73.22 \(\) 0.07 & 74.78 \(\) 0.17 \\ GRCN\_U & 69.24 \(\) 0.20 & 69.24 \(\) 0.10 & 69.72 \(\) 0.19 & 71.52 \(\) 0.42 & 72.80 \(\) 1.09 & 67.22 \(\) 5.35 & 59.44 \(\) 6.74 \\ GCN+KNN\_R & 69.24 \(\) 0.20 & 69.24 \(\) 0.10 & 69.66 \(\) 0.22 & 71.94 \(\) 0.22 & 72.82 \(\) 1.03 & 68.98 \(\) 4.05 & 60.34 \(\) 6.50 \\ SLAPS\_U & 74.86 \(\) 0.79 & 74.88 \(\) 0.90 & 74.48 \(\) 0.72 & 74.94 \(\) 0.81 & 76.26 \(\) 0.80 & 76.74 \(\) 0.59 & 76.08 \(\) 0.97 \\ SLAPS\_R & 74.86 \(\) 0.79 & 74.62 \(\) 1.51 & 74.32 \(\) 0.83 & 74.76 \(\) 0.71 & 76.22 \(\) 0.55 & 77.12 \(\) 0.77 & 76.74some methods have been proposed to directly infer an optimal graph from the data. For example, Franceschier _et al._ regard the LGI problem as a bilevel program task and learn a discrete probability distribution on the edges of the latent graph. Norcliffe-Brown _et al._ focus on visual question answering task and propose to learn an adjacency matrix from image objects so that each edge is conditioned on the questions. Fatemi _et al._ propose a self-supervision guided LGI method, called SLAPS, which yields supplementary supervision from node features through an additional self-supervision task. Our method is totally different from SLAPS as we provide supplementary supervision directly from the true labels. More importantly, our method is model-agnostic and can be easily integrated into most existing LGI methods.

**CUR Matrix Decomposition.** The CUR decomposition [2; 3] of a matrix \(^{n m}\) aims to find a column matrix \(^{n c}\) with a subset of \(c<m\) columns of \(\), and a row matrix \(^{r m}\) with a subset of \(r<n\) rows of \(\), as well as an intersection matrix \(^{c r}\) such that the matrix multiplication of \(\) approximates \(\). Unlike the SVD decomposition of \(\), the CUR decomposition obtains actual columns and rows of \(\), which makes it useful in various applications [20; 28]. In our method, instead of seeking an optimal matrix approximation, we employ CUR decomposition to identify and eliminate the starved nodes. The extracted intersection matrix \(\) is then reconstructed and served as a regularization term to provide supplementary supervision for better latent graph inference. To the best of our knowledge, we are the first to introduce CUR matrix decomposition into the field of graph neural networks.

## 6 Conclusion

In this paper, we analyze the common problem of supervision starvation (SS) in existing latent graph inference (LGI) methods. Our analysis reveals that this problem arises due to the graph sparsification operation, which destroys numerous important connections between pivotal nodes and labeled ones. Building upon this observation, we propose to recover the corrupted connections and replenish the missed supervision for improved graph inference. To this end, we begin by defining \(k\)-hop starved nodes and transform the SS problem into a more manageable task of reducing starved nodes. Then, we present two simple yet effective solutions to identify the starved nodes, including a more efficient method inspired by CUR matrix decomposition. Subsequently, we eliminate the starved nodes by constructing and incorporating a regularization graph. In addition, we propose two straightforward strategies to tackle the potential issue known as weight contribution rate decay. Extensive experiments conducted on representative benchmarks demonstrate that our proposed methods consistently enhance the performance of state-of-the-art LGI models, particularly under extremely limited supervision.

## 7 Acknowledgments and Disclosure of Funding

We are very grateful to Bahare Fatemi for her valuable discussion of our work. We thank the anonymous NeurIPS reviewers for providing us with constructive suggestions to improve our paper. This material is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-23-1-0290.