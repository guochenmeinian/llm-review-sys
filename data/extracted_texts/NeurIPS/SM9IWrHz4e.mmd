# Achieving Tractable Minimax Optimal Regret in Average Reward MDPs

Victor Boone

victor.boone@univ-grenoble-alpes.fr

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, 38000 Grenoble, France

&Zihan Zhang

zz5478@princeton.edu

Princeton University

###### Abstract

In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs). However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies. In this paper, we present the first _tractable_ algorithm with minimax optimal regret of \(}((h^{*})SAT})\),1 where \((h^{*})\) is the span of the optimal bias function \(h^{*}\), \(S A\) is the size of the state-action space and \(T\) the number of learning steps. Remarkably, our algorithm does not require prior information on \((h^{*})\).

Our algorithm relies on a novel subroutine, **P**rojected **M**itigated **E**x**tended **V**alue **I**teration (**P**MEVI), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to improve regret bounds.

## 1 Introduction

Reinforcement learning (RL) Burnetas and Katehakis (1997); Sutton and Barto (2018) has become a popular approach for solving complex sequential decision-making tasks and has recently achieved notable advancements in diverse fields of application. RL problems are generally formulated with Markov Decision Processes (MDPs) Puterman (1994), where a learning agent seeks to maximize the rewards that are gathered by interacting with an unknown environment.

This paper focuses on average reward MDPs where the learning agent must maximize the sum of rewards in the long run without any reset mechanism. In this setting, the proper balancing between exploration (i.e., playing sub-optimally to learn the unknown environment) and exploitation (i.e., planning optimally according to the current knowledge), usually known as the _exploration-exploitation trade-off_, is key to learn efficiently. The measure of learning performance that we adopt throughout is the _regret_, that compares the aggregate rewards collected by the learning agent during the learning process to the expected performance of an omniscient agent that knows everything in advance. The seminal work of Auer et al. (2009) provides a _minimax_ regret lower bound of \(()\), where \(D\) is the diameter (the maximal distance between two different states), \(S\) the number of states, \(A\) the number of actions and \(T\) the learning horizon. They also provide an algorithm achieving regret \(}(S^{2}AT})\), where \(}(-)\). Ever since Auer et al. (2009), many works have been devoted to close the gap between the regret lower and upper bounds in the average reward setting Auer et al. (2009); Bartlett and Tewari (2009); Filippi et al. (2010); Talebi and Maillard (2018); Fruit et al. (2018, 2020); Bourel et al. (2020); Zhang and Ji (2019); Ouyang et al. (2017); Agrawal and Jia (2023);Abbasi-Yadkori et al. (2019); Wei et al. (2020) and more. Subsequent works Fruit et al. (2018); Zhang and Ji (2019) refined the minimax regret lower bound to \(((h^{*})SAT})\) where \((h^{*})\) is the span of the bias function, which is the maximal gap of the long-term accumulative rewards starting from two different states. The difference is significant, since \((h^{*}) D\) and the gap between the two can be arbitrarily large. However, no existing work achieves the following three requirements simultaneously:

1. The method achieves minimax optimal regret guarantees \(}((h^{*})SAT})\);
2. The proposed method is tractable;
3. No prior knowledge on the model is required.

Most algorithms simply fail to achieve minimax optimal regret, and the only method achieving it Zhang and Ji (2019) is intractable because it relies an oracle to solve difficult optimization problems along the learning process. Naturally, we raise the question of whether these three requirements can be met all at once:

_Is there a tractable algorithm with \(}((h^{*})SAT})\) minimax regret without prior knowledge?_

Contributions.In this paper, we answer the above question affirmatively, by proposing a polynomial time algorithm with regret guarantees \(}((h^{*})SAT})\) for average-reward MDPs. Our method can further incorporate almost arbitrary prior bias information \(_{*}^{S}\) to further improve its regret.

**Theorem 1** (Informal).: _Provided that the confidence region used by \(\) satisfy mild regularity conditions (see Assumption 1-3), then for every weakly communicating model \(M\) with \((h^{*}) T^{1/5}\) and \((h^{*})_{*}\), \((_{*},,T)\) achieves regret:_

\[((h^{*})SAT}( ))+((h^{*})S^{}A^{}T^{}^{2}())\]

_with probability \(1-26\). Moreover, if \(\) runs with the same confidence regions that \(\)Auer et al. (2009) on a communicating environment, it has a time complexity \((^{3})\)._

Taking \(=\), we also obtain a \(}((h^{*})SAT})\) regret bound in expectation. The geometry of the prior bias region \(_{*}\) that \(\) can support is discussed later (see Assumption 4). It can be taken trivial with \(_{*}=^{S}\) to obtain a completely prior-less algorithm.

To the best of our knowledge, \(\) is the first tractable algorithm with minimax optimal regret bounds (up to logarithmic factors). The algorithm does not necessitate any prior knowledge of \((h^{*})\), thus circumventing the potentially high cost associated with learning \((h^{*})\). On the technical side, a key novelty of our method is the subroutine named \(\) (see Algorithm 2) that improves and can replace \(\)Auer et al. (2009) in any algorithm that relies on it Auer et al. (2009); Fruit et al. (2018); Filippi et al. (2010); Fruit et al. (2020); Bourel et al. (2020) to boost its performance and achieve minimax optimal regret.

Related works on average reward MDPs.For communicating MDPs, the notable work of Auer et al. (2009) proposes the famous \(\) algorithm, a mature version of their prior \(\)Auer and Ortner (2006), achieving a regret bound of \(}()\). This paper pioneered the use _optimistic_ methods to learn MDPs efficiently. A line of papers Filippi et al. (2010); Fruit et al. (2020); Bourel et al. (2020) developed this direction by tightening the confidence region that \(\) relies on, and sharpened the analysis through the use of local properties of MDPs, such as local diameters and local bias variances. However, none of these works went beyond regret guarantees of order \(\) and suffer from an \(\). A parallel direction was initiated by Bartlett and Tewari (2009) with \(\), obtaining regret bounds scaling with \((h^{*})\) instead of \(D\), and extending the regret bounds to weakly-communicating MDPs in the mean time. The computational intractability of \(\) is addressed by Fruit et al. (2018) with \(\), and regret guarantees are further improved by Zhang and Ji (2019) with \(\), eventually reaching optimal minimax regret but loosing tractability.

Another successful design approach is Bayesian-flavored sampling, derived from Thompson Sampling Thompson (1933), that usually replaces optimism. The regret guarantees of these algorithms usually stick to the Bayesian setting however Ouyang et al. (2017); Theocharous et al. (2017), although Agrawal and Jia (2023) also enjoys \(}(S)\) high probability regret by coupling posterior sampling with optimism. Another line of research focuses on the study of ergodic MDPs, where the environment is such that all states are visited infinitely often under every policy. To name a few, the model-free algorithm Politex Abbasi-Yadkori et al. (2019) attains a regret of \(}(t_{})^{3}t_{}^{}\)) where \(t_{}\) and \(t_{}\) are respectively the mixing and the hitting times of the ergodic environment. By leveraging an optimistic mirror descent algorithm, Wei et al. (2020) achieve an enhanced regret of \(}()^{2}t_{}AT}\).

We refer the readers to Table 1 for a (non-exhaustive) list of existing algorithms.

## 2 Preliminaries

We fix a finite state-action space structure \(:=_{s}\{s\}(s)\), and denote \(\) the collection of all MDPs with state-action space \(\) and rewards supported in \(\).

Infinite-horizon MDP.An element \(M\) is a tuple \((,,p,r)\) where \(p\) is the transition kernel and \(r\) the reward function. The random state-action pair played by the agent at time \(t\) is denoted \(X_{t}(S_{t},A_{t})\), and the achieved reward is \(R_{t}\). A policy is a _deterministic_ rule \(:\) and we write \(\) the space of policies. When coupled with a MDP \(M\), a policy properly defines the distribution of \((X_{t},R_{t})\) whose associated probability probability and expectation operators are denoted \(_{s}^{},_{s}^{}\), where \(s\) is the initial state. Under \(M\), a fixed policy has a reward function \(r^{}(s):=r(s,(s))\), a transition matrix \(P^{}\), a gain \(g^{}(s):=_{s}^{}[R_{0}++R_{T-1}]\) and a bias \(h^{}(s):=_{s}^{}[_{j=0}^{T-1}(R_{t}-g(S_ {t}))]\), that all together satisfy the Poisson equation \(h^{}+g^{}=r^{}+P^{}h^{}\), see Puterman (1994). The _Bellman operator_ of the MDP is:

\[Lu(s):=_{a(s)}\{r(s,a)+p(s,a)u\}\] (1)

The _optimal gain_ is \(g^{*}(s):=_{}g^{}(s)\) and the _optimal bias_ is \(h^{*}(s):=\{h^{}(s):g^{}=g^{*}\}\).

Weakly-communicating MDPs.\(M\) is weakly-communicating Puterman (1994); Bartlett and Tewari (2009) if the state space can be divided into two sets: (1) the transient set, consisting in states that are transient under all policies; (2) the non-transient set, where every state is reachable starting from any other non-transient state. In this case, \(h^{*}\) is a span-fixpoint of \(L\) (see Puterman (1994)), i.e., \(Lh^{*}-h^{*}e\) where \(e\) is the vector full of ones. We write \(h^{*}(L)\). Then \(g^{*}=Lh^{*}-h^{*}\) and every policy \(\) satisfies \(r^{}+P^{}h^{*} g^{*}+h^{*}\). We accordingly define the _Bellman gaps_:

\[^{*}(s,a):=h^{*}(s)+g^{*}(s)-r(s,a)-p(s,a)h^{*} 0.\] (2)

Another important concept is the _diameter_, that describes the maximal distance from one state to another state. It is given by \(D:=_{s s^{}}_{}_{s}^{}[\{t 1:S_{t}=s^{ }\}]\). An MDP is said _communicating

 
**Algorithm** & **Regret in \(}(-)\)** & **Tractable** & **Comment/Requirements** \\  REGAL Bartlett and Tewari (2009) & sp (\(h^{*}\))\(S\) & \(\) & knowledge of sp (\(h^{*}\)) \\ UCRL2 Auer et al. (2009) & \(DS\) & \(\) & - \\ PSRLA Agrawal and Jia (2023) & \(DS\) & \(\) & Bayesian regret \\ SCAL Fruit et al. (2018) & sp (\(h^{*}\))\(S\) & \(\) & knowledge of sp (\(h^{*}\)) \\ UCRL2B Fruit et al. (2020) & \(S\) & \(\) & extra \(\) in upper-bound \\ UCRL3 Bouret et al. (2020) & \(D+_{L,a}D_{a}^{2}L_{}\) & \(\) & \(L_{}:=_{r}|s,a)(1-p(s^{}|s,a))}\) \\ KL-UCRL Filippi et al. (2010); Taleb and Maillard (2018) & \(S\) & \(\) & - \\ ERF Zhang and Ji (2019) & \((h^{*})S\) & \(\) & optimal, knowledge of sp (\(h^{*}\)) \\ Optimistic-Q Wei et al. (2020) & sp (\(h^{*}\))\(S\) & \(\) & model-free \\ UCR-AVG Zhang and Xie (2023) & \(S^{A}^{}(h^{*}\) & \(\) & model-free, knowledge of sp (\(h^{*}\)) \\ MDP-OMD Wei et al. (2020) & \()^{2}t_{}AT}\) & \(\) & ergodic \\ Politex Abbasi-Yadkori et al. (2019) & \((t_{})^{3}t_{}^{1}\) & \(\) & model-free, ergodic \\ PMEVI-DT (**this work**) & \()SAT}\) & \(\) & - \\ 
**Lower bound** & \(()SAT})\) & - & - \\  

Table 1: Comparison of related works on RL algorithms for average-reward MDP, where \(S A\) is the size of state-action space, \(T\) is the total number of steps, \(D\) (\(D_{s}\)) is the (local) diameter, sp (\(h^{*}\)) \(\)\(D\) is the span of the bias vector, \(t_{}\) is the worst-case mixing time, \(t_{}\) is the hitting time (i.e., the expected time cost to visit some certain state under any policy).

if its diameter \(D\) is finite, in which case \((h^{*})(r)D\), see Bartlett and Tewari (2009); Fruit (2019), where \((-)\) is the _span_ function given by \((u):=(u)-(u)\).

Reinforcement learning.The learner is only aware that \(M\) but doesn't have a clue about what \(M\) further looks like. From the past observations and the current state \(S_{t}\), the agent picks an available action \((S_{t})\), receives a reward \(R_{t}\) and observe the new state \(S_{t+1}\). The _regret_ of the agent is:

\[(T):=Tg^{*}-_{t=0}^{T-1}R_{t}.\] (3)

Its expected value satisfies \([(T)]=[_{t=0}^{T-1}^{*}(X_{t} )]+[h^{*}(S_{0})-h^{*}(S_{T})]\) and the quantity \(_{t=0}^{T-1}^{*}(X_{t})\) will be referred to as the _pseudo-regret_. This paper focuses on _minimax regret guarantees_. Specifically, for \(c 1\), denote \(_{c}:=\{M: t^{*}( L(M)),(h^{*}) c\}\) the set of weakly-communicating MDPs that admit a bias function with span at most \(c\). Following Auer et al. (2009), every algorithm \(\), for all \(c>0\), we have

\[_{M_{c}}^{M,}[(T)]= ().\] (4)

The goal of this work is to reach this lower bound with a tractable algorithm.

## 3 Algorithm Pmevi-dt

The algorithm PMEVI-DT that we present in this work is actually a general method can be applied to improve various existing algorithms Auer et al. (2009); Filippi et al. (2010); Fruit et al. (2018); Bourel et al. (2020); Tewari and Bartlett (2007). All these algorithms work episodically, by maintaining a policy \(_{k}\) that drives play during a time-window \(\{t_{k},,t_{k+1}-1\}\) called an episode. An episode rule determines when \(_{k}\) should be considered obsolete and defines the time \(t_{k+1}\) at which the policy is renewed. To compute \(_{k}\), these algorithms follow the _optimism-in-face-of-certainty_ (OFU) design principle, by choosing \(_{k}\) that achieves the largest possible gain that is plausible under their current information. This is done by building a confidence region \(_{t}\) for the hidden model \(M\), then searching for a policy \(\) solving the optimization problem:

\[g^{*}(_{t}):=\{g^{}(_{t}):, (g^{}(_{t}))=0\}g^{}(_{t}):=\{g^{}(): _{t}\}.\] (5)

The design of the confidence region \(_{t}\) varies from a work to another. Given a confidence region \((_{t})_{t 0}\), OFU-algorithms work as follows: At the start of episode \(k\), the optimization problem (5) is solved, and its solution \(_{k}\) is played until the end of episode. The duration of episodes can be managed in various ways, although the most popular is arguably the _doubling trick_ (DT), that essentially waits until a state-action pair is about to double the visit count it had at the beginning of the current episode (see Algorithm 1).

Notations.In the rest of this section, we use \(_{t}(s,a)\) (and \(_{t}(s,a)\)) to denote the empirical transition (and reward) of the latest doubling update before the \(t\)-th step, and further denote \(_{t}:=(_{t},_{t})\).

Extended Bellman operators and EVI.To solve (5) efficiently, the celebrated work Auer et al. (2009) introduce the _extended value iteration_ algorithm (EVI), that can be run whenever \(_{t}\) is a \((s,a)\)-rectangular confidence region, meaning that \(_{t}_{s,a}(_{t}(s,a)_{t}(s,a))\) where \(_{t}(s,a)\) and \(_{t}(s,a)\) are respectively the confidence region for \(r(s,a)\) and \(p(s,a)\) after \(t\) learning steps. EVI is the algorithm computing the sequence defined by:

\[v_{i+1}(s)_{t}v_{i}(s):=_{a_{t}(s)}_{ (s,a)_{t}(s,a)}_{(s,a)_{t}( s,a)}((s,a)+(s,a) v_{i})\] (6)

until \((v_{i+1}-v_{i})<\) where \(>0\) is the numerical precision. When the process stops, it is known that any policy \(\) such that \((s)\) achieves \(_{t}v_{i}\) in (6) satisfies \(g^{}(_{t}) g^{*}()-\), hence is nearly optimistically optimal. This process gets its name from the observation that \(_{t}\) is the Bellman operator of \(_{t}\) seen as a MDP, hence EVI is just the Value Iteration algorithm Puterman (1994) ran in \(_{t}\). A choice of action from \(s\) in \(_{t}\) consists in (1) a choice of action \(a(s)\), (2) a choice of reward \((s,a)_{t}(s,a)\) and (3) a choice of transition \((s,a)_{t}(s,a)\); It is an _extended_ version of \((s)\).

Towards Projected Mitigated Evi.Obviously, the regret of an OFU-algorithm is directly related to the quality of the confidence region \(_{t}\). That is why most previous works tried to approach the regret lower bound \(\) of Auer et al. (2009) by refining \(_{t}\). The older works of Auer et al. (2009), Bartlett and Tewari (2009), Filippi et al. (2010) have been improved with a variance aware analysis Talebi and Maillard (2018); Fruit et al. (2018, 2020), Bourel et al. (2020) that essentially make use of tightened kernel confidence regions \(_{t}\). While all these algorithms successively reduce the gap between the regret upper and lower bounds, they fail to achieve optimal regret \(\). Meanwhile, the EBF algorithm of Zhang and Ji (2019) is minimax optimal but (1) the algorithm is intractable because it relies on an oracle to retrieve optimistically optimal policies and (2) needs prior information on the bias function. Nonetheless, the method of Zhang and Ji (2019) strongly suggests that inferring bias information from the available data is key to achieve minimax optimal regret.

Rather surprisingly and in opposition to this previous line of work, our work suggests that the choice of the confidence region \(_{t}\) has little importance. Instead, our algorithm takes an arbitrary (well-behaved) confidence region in, infer bias information similarly to Zhang and Ji (2019) and makes use of it to refine the extended Bellman operator (6) associated to the input confidence region. Our algorithm can further take arbitrary prior information (possibly none) on the bias vector to tighten its bias confidence region. The pseudo-code given in Algorithm 1 is the high level structure our algorithm \(\). In the next Section 3.1, we explain how (6) is refined using bias information.

``` Parameters: Bias prior \(_{t}\), horizon \(T\), a system of confidence regions \(t_{t}\)
1:for\(k=1,2,\)do
2: Set \(t_{k} t\), update confidence region \(_{t}\);
3:\(^{}_{t}(_{t}, _{t},)\):
4:\(_{t}_{t}\{u:(u) T^{1 /5}\}^{}_{t}\);
5:\(_{t}(_{t},-)\);
6:\(_{t}(^{}_{t},_{t})\);
7:\(_{k}(_{t},_{t},_{t}, /t)\) ;
8:\(_{k}_{0}_{k}-_{k}\) ;
9: Update policy \(_{k}(_{t},_{k},_{t})\);
10:repeat
11: Play \(A_{t}_{k}(S_{t})\), observe \(R_{t},S_{t+1}\);
12: Increment \(t t+1\);
13:until (DT) \(N_{t}(S_{t},_{k}(S_{t})) 1 2N_{t_{k}}(X_{t})\).
14:endfor ```

**Algorithm 1**\((_{t},T,t_{t})\)

### Projected mitigated extended value iteration (\(\))

Assume that an external mechanism provides a confidence region \(_{t}\) for the bias function \(h^{*}\). Provided that \(_{t}\) is correct (\(M_{t}\)) and that \(_{t}\) is correct (\(h^{*}_{t}\)), we want to find a policy-model pair \((,)\) that maximizes the gain among pairs with \(h^{}()_{t}\). This is done with an improved version of (6) combining two ideas, that are both necessary to achieve minimax optimal regret in the analysis.

1. **Projection (Section 3.2).** Whenever it is correct, the bias confidence region \(_{t}\) informs the learner that the search of an optimistic model can be constrained to those with bias within \(_{t}\). This is done by projecting \(^{}_{t}\) (see _mitigation_) using an operator \(_{t}:^{S}_{t}\), that has to satisfy a few non-trivial regularity conditions that are specified in Proposition 2.
2. **Mitigation (Section 3.3).** When one is aware that \(h^{*}_{t}\), the _dynamical bias update_\((s,a)v_{i}\) in (6) can be controlled better, by trying to restrict (6) to the \((s,a)\) such that \((s,a)v_{i}_{t}(s,a)v_{i}+(p(s,a)-_{t}(s,a))v_{i}\) with the knowledge that \(v_{i}_{t}\). However, controlling the error \((p(s,a)-_{t}(s,a))v_{i}\) by doing a union-bound on all possible values of \(v_{i}\) is equivalent to building a confidence region for \(p(s,a)\), which produces an extra \(S^{1/2}\) in the error term that cannot be afforded by a minimax optimal algorithm.

We take a different approach instead. For a fixed \(u^{S}\), the empirical Bernstein inequality (Lemma 38) provides a variance bound of the form \((_{t}(s,a)-p(s,a))u_{t}(s,a,u)\). Byestimating \(_{i}(s,a):=_{a_{i}}_{i}(s,a,u)\), the search makes sure that \((_{i}(s,a)-p(s,a))u_{i}(s,a)\) holds with high probability for \(u=h^{*}\), even though \(h^{*}\) is unknown. For \(_{+}^{X}\), we introduce the \(\)_-mitigated_ extended Bellman operator:

\[_{t}^{}u(s):=_{a_{i}}_{(s,a) _{i}(s,a)}_{(s,a)(s,a)}\{(s, a)+\{(s,a)u_{i},_{i}(s,a)u_{i}+(s,a)\}\}\] (7)

The mitigation \((s,a)\) is independent of \(u\), which is crucial for \(_{t}^{}\) to be well-behaved.

The proposition below shows how well-behaved the composition \(_{t}:=_{t}_{t}^{}\) is. Its proof requires to build a complete analysis of projected mitigated Bellman operators. This is deferred to the appendix.

**Proposition 2**.: _Fix \(_{+}^{X}\) and assume that there exists a projection operator \(_{t}:^{X}_{t}\) which is **(O1)** monotone: \(u v u v\); **(O2)** non span-expansive: \(( u- v)(u-v)\); **(O3)** linear: \((u+ e)= u+ e\) and **(O4)**\( u u\). Then, the projected mitigated extended Bellman operator \(_{t}:=_{t}_{t}^{}\) has the following properties:_

1. _There exists a unique_ \(_{t}e\) _such that_ \(_{t}_{t},_{t}_{t}= _{t}+_{t}\)_;_
2. _If_ \(M_{t}\)_,_ \(h^{*}_{t}\) _and_ \((_{t}(s,a)-p(s,a))h^{*}(s,a)\)_, then_ \(_{t} g^{*}(M)\)_;_
3. _If_ \(_{t}\) _is convex, then for all_ \(u^{S}\)_, the policy_ \(:(_{t},u,)\) _picking the actions achieving_ \(_{t}^{}u\) _satisfies_ \(_{t}u=^{}+^{}u\) _for_ \(^{}(s)_{t}(s,(s))\) _and_ \(^{}(s)_{t}(s,(s))\)_;_
4. _For all_ \(u^{S}\) _and_ \(n 0\)_,_ \((_{t}^{n+1}u-_{t}^{n}u) ((_{t})^{n+1}u-(_{t})^{n}u)\)_._

The property (1) guarantees that \(_{t}\) has a fix-point while (2) states that this fix-point corresponds to an optimistic gain \(_{t}\) if the model and the bias confidence region are correct and the mitigation isn't too aggressive. Combined with (3), the Poisson equation of a policy corresponds to this fix-point, i.e., \(^{}+^{}_{t}=_{t}+_ {t}\), so that \(_{t}\) is the gain and \(_{t}_{t}\) is a legal bias for \(\) under the model \((^{},^{})\). Lastly, the property (4) guarantees that the iterates \(_{t}^{}u\) converge to a fix-point of \(\) at least as quickly as \(_{t}^{n}u\) goes to a fix-point of \(_{t}\); The convergence of \((_{t})^{n}u\) is already guaranteed by existing studies and is discussed in the appendix.

Provided that the bias confidence region is constructed, Proposition 2 foreshadows how powerful the construction is: The algorithm \(\), obtained by iterating \(_{t}\) instead of \(_{t}\) in \(\), can replace the well-known \(\) within any algorithm of the literature that relies on it (UCRL2 Auer et al. (2009); UCRL2B Fruit et al. (2020) or KL-UCRL Filippi et al. (2010)) for an immediate improvement of its theoretical guarantees.

### Building the bias confidence region and its projection operator

The bias confidence region used by \(\)-\(\) is obtained as a collection of constraints of the form:

\[ s s^{},(s)-(s^{})-c(s,s^{ }) d(s,s^{}).\] (8)

Such constraints include (1) prior bias constraints (if any) of the form of \((s)-(s^{}) c_{*}(s,s^{})\); (2) span constraints of the form \((s)-(s^{}) c_{0}:=T^{1/5}\) spawning the span semi-ball \(\{u:(u) T^{1/5}\}\); and (3) pair-wise constraints obtained by estimating bias differences in the style of Zhang and Ji (2019); Zhang and Xie (2023) that we further improve. We start by defining a bias difference estimator.

**Definition 1** (Bias difference estimator).: Given a pair of states \(s s^{}\), their sequence of _commute times_\((_{i}^{s s^{}})_{i 0}\) is defined by \(_{2i}^{s s^{}}:=\{t>_{2i-1}^{s  s^{}}:S_{t}=s\}\) and \(_{2i+1}^{s s^{}}:=\{t>_{2i}^{s  s^{}}:S_{t}=s^{}\}\) with the convention that \(_{2i-1}^{s s^{}}=-\). The number of commutations up to time \(t\) is \(N_{t}(s s^{}):=\{i:_{2i}^{s s^{ }} t\}\), and \((t):=_{i=0}^{t-1}R_{i}\) is the empirical gain. The _bias difference estimator_ at time \(T\) is any quantity \(c_{T}(s,s^{})\) such that:

\[N_{t}(s s^{})c_{T}(s,s^{})=_{t=0}^{N_{T}(s  s^{})-1}(-1)^{i}_{t=_{2i}^{s  s^{}}}^{_{2i+1}^{s s^{}}-1}(( T)-R_{t}).\] (9)

**Lemma 3**.: _With probability \(1-2\), for all \(T^{} T\), we have_

\[N_{T^{}}(s s^{})|h^{*}(s)-h^{*}(s^{})-c_{T^{ }}(s,s^{})| 3(h^{*})+(1+ (h^{*})))}+2 _{t=0}^{T^{-1}}(g^{*}-R_{t}).\] (10)Lemma 3 says that the quality of the estimator \(c_{T}(s,s^{})\) is directly linked to the number of observed commutes between \(s\) and \(s^{}\) as well as the regret. The idea is that if the algorithm makes many commutes between \(s\) and \(s^{}\) and if its regret is small, then the algorithm mostly takes optimal paths from \(s\) to \(s^{}\). The bound provided by Lemma 3 is not accessible to the learner however, because \((h^{*})\) is unknown in general. To overcome this issue, \((h^{*})\) is upper-bounded by \(c_{0}:=T^{1/5}\). Overall, this leads to the design of the algorithm estimating the bias confidence region as specified in Algorithm 3.

```
0: History \(_{t}\), model region \(_{t}\), confidence \(>0\)
1: Estimate bias differences \(c_{t}\) via (9);
2: Estimate optimistic gain \(_{k<K(t)}_{k}\);
3: Inner regret estimation \(B_{0} t-_{i=0}^{t-1}R_{i}\);
4:\()}\), \(c_{0} T^{}\);
5: Estimate the bias difference errors as: \(d_{t}(s,s^{})(c_{t},s,s^{}):=+(1+c_{0})(1+)+2B_{0}}{N_{t}(s, s^{})}\)
6:return\((c_{t},(c_{t},-,-))\), (8) defines \(^{}_{t}\). ```

**Algorithm 3**\((_{t},_{t},)\)

Coupled with prior information and span constraints, the bias confidence region \(_{t}\) is a polyhedron of the same kind as the one encountered in Zhang and Xie (2023). When generated by constraints of the form (8), following Zhang and Xie (2023), Proposition 3, one can project onto \(_{t}\) in polynomial time with Algorithm 4. Moreover, the resulting projection operator satisfies the prerequisites (**O1-4**) of Proposition 2, making \(\) (Algorithm 2) well-behaved. See Appendix B.2 for proofs.

**Lemma 4**.: _Assume that \(\) is a set of \(b^{S}\) satisfying a system of equations of the form of (8). If \(\) is non empty, then the operator \( u:=(,u)\) (see Algorithm 4) is a projection on \(\) and satisfies the properties (**O1-4**) defined in Proposition 2._

### Mitigation using finer bias dynamical error

The fact that \(h^{*}_{t}\) with high probability is used in \(\)-\(\) to restrict the search of \(\) by reducing the dynamical bias error. This reduction is based on a empirical Bernstein inequality (see Lemma 38) applied to \(((s,a)-p(s,a))u\). Here, it gives that with probability \(1-\), we have:

\[(_{t}(s,a)-p(s,a))u(_{t}(s,a),u)()}{\{1,N_{t}(s,a)\}}}+ (u)()}{ \{1,N_{t}(s,a)\}}=:_{t}(s,a,u)\] (11)

where \((_{t}(s,a),u)\) is the variance of \(u\) under the probability vector \(_{t}(s,a)\). More specifically, if \(q\) is a probability on \(\) and \(q^{S}\), we set \((q,u):=_{s}q(s)u(s)-q u)^{2}\). In (11), \(u^{S}\), \((s,a)\) and \(T 1\) are fixed. Once is tempted to use (11) directly to mitigate the extended Bellman operator, but the resulting operator is ill-behaved because it loses monotony. This issue is avoided by changing \(_{t}(s,a,u)\) to \(_{u_{t}}_{t}(s,a,u)\) in (11). The resulting inequality is _not_ guaranteed to hold simultaneously for all \(u_{t}\) and with high probability; However, it is guaranteed to hold with high probability for \(u=h^{*}\), which will be enough.

The variance maximization problem \(_{u_{t}}_{t}(s,a,u)\) is a _convex maximization problem_ with linear constraints. Even in very simple settings, such optimization problems are NP-hard Pardalos and Schnitger (1988) hence computing \(_{u_{t}}_{t}(s,a,u)\) is not reasonable in general. Thankfully, this value can be upper-bounded by a tractable quantity that is enough in the regret analysis. The mitigation \(_{t}\) used by \(\)-\(\) is provided by \(}\). See Lemma 12 and Appendix A.2.2 for details.

## 4 Regret guarantees

Theorem 5 thereafter shows that \(\)-\(\) has minimax optimal regret under regularity assumptions on the used confidence region \(_{t}\). Assumption 1 asserts that the confidence region holds uniformly with high probability. Assumption 2 asserts that the reward confidence region is sub-Weissman (seeLemma 35) and Assumption 3 assumes that the model confidence region makes sure that \(\) (6) converges in the first place. Assumption 4 asserts that the prior bias region is correct.

**Assumption 1**.: With probability \(1-\), we have \(M_{k=1}^{K(T)}_{_{k}}\).

**Assumption 2**.: There exists a constant \(C>0\) such that for all \((s,a)\), for all \(t T\), we have:

\[_{t}(s,a)\{(s,a)(s,a):N_{t}(s, a)\|_{t}(s,a)-(s,a)\|_{1}^{2} C((1+N_{t}(s,a))}{})\}.\]

**Assumption 3**.: For \(t 0\), \(_{t}\) is a \((s,a)\)-rectangular convex region and \(_{t}^{n}u\) converges a fix-point.

**Assumption 4**.: The prior bias region \(_{t}\) contains \(h^{*}(M)\) and is generated by constraints of the form:

\[ s s^{},(s)-(s^{}) c _{*}(s,s^{})\]

with \(c_{*}(s,s^{})[-,]\) (possibly infinite).

Refer to Appendix A.2 for the feasibility of Assumption 1, Appendix A.2.3 for Assumption 2, and Appendix A.3 for Assumption 3.

**Theorem 5** (Main result).: _Let \(c>0\). Assume that \(\)-\(\) runs with a confidence region system \(t_{t}\) that guarantees Assumptions 1-3. If \(T c^{5}\), then for every weakly communicating model with \((h^{*}) c\) and such that Assumption 4 is satisfied \((h^{*}_{*})\), \(\)-\(\) achieves regret:_

\[()})+ (c^{5}^{2}^{}^{2}( ))\]

_with probability \(1-26\), and in expectation if \(<\). Moreover, if \(\)-\(\) runs with the same confidence regions that \(\)Auer et al. (2009), then it enjoys a time complexity \((DS^{3}AT)\)._

To have a completely prior-less algorithm, pick \(_{*}=^{}\). The proof of Theorem 5 is tedious and its details are deferred to appendix. We will focus here on the main ideas.

Notations.At episode \(k\), the played policy is denoted \(_{k}\). As a greedy response to \(_{k}\), by Proposition 2 (3), there exists \(_{k}(s)_{_{k}}(s,_{k}(s))\) and \(_{k}(s)_{_{k}}(s,(x))\) such that \(_{k}+_{k}=_{k}+_{k}_{k}\). The reward-kernel pair \(_{k}=(_{k},_{k})\) is referred to as the _optimistic model_ of \(_{k}\). We write \(P_{k}:=P_{_{k}}(M)\) the true kernel and \(_{k}:=P_{_{k}}(_{k})\) the empirical kernel. Likewise, we define the reward functions \(r_{k}\) and \(_{k}\). The optimistic gain and bias satisfy \(_{k}=g(_{k},_{k})\) and \(_{k}=h(_{k},_{k})\). We further denote \(c_{0}=T^{}\).

The regret is first decomposed episodically with \((T)=_{k}_{t=_{k}}^{t_{k+1}-1}(g^{*}-R_{t})\). The first step goes back to the analysis of \(\)Auer et al. (2009), and consists in upper-bounding the regret of the episode \(k\) with optimistic quantities that are exclusive to that episode.

**Lemma 6** (Reward optimism).: _With probability \(1-6\), we have:_

\[(T)_{k}_{t=_{k}}^{_{k}-1}( _{k}-R_{t})_{k}_{t=_{k}}^{_{k }-1}(_{k}-_{k}(X_{t}))+()}).\] (12)

We introduce the two optimistic regrets \(B(T):=_{k}_{t=_{k}}^{_{k}-1}(_{k} -R_{t})\) and \((T):=_{k}_{t=_{k}}^{_{k}-1}(_{k}-_{k}(X_{t}))\). Rewriting the summand \(_{k}-_{k}(X_{t})\) using the Poisson equation \(_{k}+_{k}=_{k}+_{k}_{k}\), we get:

\[(T)=_{k}_{t=_{k}}^{_{k}-1}( _{k}(S_{t})-e_{S_{t}})_{k}.\]The analysis proceeds by decomposing the above expression of \((T)\) in the style of Zhang and Ji (2019). We write \(_{t=t_{k}}^{t_{k+1}-1}(_{k}(S_{t})-e_{S_{t}})_{k}\) as:

\[_{t=t_{k}}^{t_{k+1}-1}((S_{t})-e_{S_{t}})\,_{k}}_{}+_{k}(S_{t})-p_{k}(S_{t}))\,_{k}}_{}+_{k}(S_{t})-_{k}(S_{t}))\,_{k}}_{}+_{k}(S_{t})-p_{k}(S_{t}))\,(_{k}-h^{*})}_{})\]

Each error term is bounded separately. Below, we denote \((q,u):=_{s}q(s)(u(s)-q u)^{2}\).

**Lemma 7** (Navigation error).: _With probability \(1-7\), the navigation error is bounded by:_

\[_{k}_{t=t_{k}}^{t_{k+1}-1}(p_{k}(S_{t})-e_{S_{t}})_{k} ^{T-1}(p(X_{t}),h^{*})()}+2SA^{}( )+}(T^{}).\]

**Lemma 8** (Empirical bias error).: _With probability \(1-\), the empirical bias error is bounded by:_

\[_{k}_{t=t_{k}}^{t_{k+1}-1}(_{k}(S_{t})-p_{k}(S_{t})\,h^{ *} 4^{T-1}(p(X_{t}),h^{*})()}+(^{2}(T)).\]

**Lemma 9** (Optimistic overshoot).: _With probability \(1-6\), the optimistic overshoot is bounded by:_

\[_{k}_{t=t_{k}}^{t_{k+1}-1}(_{k}(S_{t})-_{k}(S_{t} )\,)_{k}\{4^{T-1} (p(X_{t}),h^{*})()}\\ +8(1+c_{0})S^{}A^{}() +}(T^{}) \}.\]

**Lemma 10** (Second order error).: _With probability \(1-6\), the second order error is bounded by:_

\[_{k}_{t=t_{k}}^{t_{k+1}-1}(_{k}(S_{t})-p_{k}(S_{t})\, )(_{k}-h^{*}) 16S^{2}A(1+c_{0})^{}( AT}{})+}(T^{ }).\]

We see that the empirical bias error (Lemma 8) and the optimistic overshoot (Lemma 9) both involve the sum of variances \(_{t=0}^{T-1}(p(X_{t}),h^{*})\), which is shown in Lemma 29 to be of order \((h^{*})(r)T+_{t=0}^{T-1} ^{*}(X_{t})\). The pseudo-regret term \(_{t=0}^{T-1}^{*}(X_{t})\) is bounded with the regret using Corollary 31, then by \(B(T)\). With high probability, we obtain an equation of the form:

\[B(T) C(h^{*}))SAT( )}+CS^{2}A(1+c_{0})^{2}(T)+}(T^{ })\]

where \(C\) is a constant. Setting \(:=CS^{2}A(1+c_{0})^{2}(T)\) and \(:=C(h^{*}))SAT(T/)}+}(T^{1/4})\), the above equation is of the form \(B(T)+\). Solving in \(B(T)\), we find \(B(T)+2+^{2}\). The dominant term is \(\), hence we readily obtain:

\[B(T) C(h^{*}))(r)SAT ()}+}( (h^{*})(r)S^{}A^{}(1+c_{0 })T^{}).\] (13)

Since \(c_{0}=(T^{})\), we conclude that \(B(T)=((h^{*})SAT(T/)})\), ending the proof.

Figure 1: An overview of PMEVI-DT and its regret analysis. In the above, \(_{k}\) and \(_{k}\) are the optimistic gain and bias functions produced by PMEVI (see Algorithm 2) at episode \(k\), and \(_{k}\) and \(_{k}\) are respectively the empirical and optimistic kernel models at episode \(k\).

Experimental illustrations

To get a grasp of how PMEVI-DT behaves in practice, we provide in Fig. 2 a first round of illustrative experiments. In both, the environment is a river-swim which is a model known to be hard to learn despite its size, with high diameter and bias span, see Appendix D for the model's description.

On the first experiment, we observe that PMEVI can exploit prior bias knowledge effectively and drastically improve the regret performance, depending on the quality of the prior region.

On the second experiment however, we observe that without prior knowledge, PMEVI has nearly the same regret performance that its EVI counterparts, meaning that the bias confidence region is too large to effectively improve the regret performance. This observation is first to be taken with caution. Indeed, the regret that is being estimated above is model specific, hence is not an estimate of the minimax regret -- This being said, it undoubtedly shows that the bias confidence region is ineffective and this can be explained as follows. On experiments, we see that most of the regret is due to the early phase of the learning process, where proper bias information is nearly impossible to get. Indeed, the regret is still growing linearly, so no bias information can be inferred. But in addition, this "bad" early data pollutes the bias estimator for a long duration. In other words, while the theoretical regret guarantees of PMEVI-DT are better than its EVI analogues, there is room to improve the bias estimation mechanism and the practical performance.

## 6 Conclusion

In this work, we have shown that regret guarantees of order \((h^{*})SAT(T)}\) can be achieved for weakly communicating MDPs without prior knowledge, nor exponential computational cost. In particular, regret guarantees can scale with the bias span rather than the diameter without prior knowledge. This is in opposition to the recent results that the sample complexity cannot be bounded in term of bias span without prior knowledge for average reward MDPs Tuynman et al. (2024); Wang et al. (2024); Zurek and Chen (2024a,b). This difference lies in the fact \((,)\)-PAC algorithm must produce a policy \(_{}\) after \(\) learning steps where \(\) is a stopping time, with \((g^{_{}} g^{*}-)\). Implicitly, these algorithms must hereby _certify_ that the output policy is approximately optimal. In opposition, regret robust algorithms have no need to assess that deployed policies are indeed optimal.

In the end, the regret advantages of PMEVI-DT over pure EVI-based methods remain theoretical, and the experimental shortcomings displayed in Section 5 leave a few opportunities for future work. Can bias information be inferred more efficiently? Or, do the experiments indicate that the regret analysis of EVI-based methods may be drastically improved?

Figure 2: (**To the left**) Running UCRL2 and PMEVI-DT with the same confidence region than UCRL2 on a 3-state river-swim. PMEVI-DT is run with prior knowledge \(h^{*}(s_{1}) h^{*}(s_{2})-c h^{*}(s_{3})-2c\) for \(c\{0,0.5,1,1.5,2\}\). (**To the right**) Running a few algorithms of the literature on 5-state river-swim and comparing their average regret against their PMEVI variants, obtained by changing calls to the EVI sub-routine to calls to PMEVI.