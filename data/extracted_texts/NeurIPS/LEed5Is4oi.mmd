# Robot Policy Learning with

Temporal Optimal Transport Reward

 Yuwei Fu\({}^{1}\) Haichao Zhang\({}^{2}\) Di Wu\({}^{1}\) Wei Xu\({}^{2}\) Benoit Boulet\({}^{1}\)

\({}^{1}\)McGill University

yuwei.fu@mail.mcgill.ca \({}^{2}\)Horizon Robotics

haichao.zhang@horizon.cc

###### Abstract

Reward specification is one of the most tricky problems in Reinforcement Learning, which usually requires tedious hand engineering in practice. One promising approach to tackle this challenge is to adopt existing expert video demonstrations for policy learning. Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations. For example, reward labeling via Optimal Transport (OT) has been shown to be an effective strategy to generate a proxy reward by measuring the alignment between the robot trajectory and the expert demonstrations. However, previous work mostly overlooks that the OT reward is invariant to temporal order information, which could bring extra noise to the reward signal. To address this issue, in this paper, we introduce the Temporal Optimal Transport (TemporalOT) reward to incorporate temporal order information for learning a more accurate OT-based proxy reward. Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method. Our code is available at: https://github.com/fuyw/TemporalOT.

## 1 Introduction

Reinforcement Learning (RL)  has achieved great success across a wide array of applications . However, it typically requires a large number of interactions with the environment , which limits its practical application in the robotic control . A large body of work has been developed to address this issue from different aspects , _i.e._, using curiosity-based intrinsic reward to encourage exploration , leveraging better representation pretrained on large scale robotics datasets , incorporating external knowledge from the Vision-Language Models (VLMs) , and imitating the behaviors from pre-collected expert demonstrations .

Reward specification plays a central role in RL . Since the goal of the RL agent is to maximize the expected cumulative rewards, the reward signal directly influences the learned behaviors . One major challenge in applying RL to real-world problems is how to design the reward functions . A well-designed reward function can guide the agent towards desirable behaviors more efficiently, while a poorly designed one could lead to sub-optimal behaviors . However, designing a good reward function is a nontrivial task , which requires related expert domain knowledge and (or) time-consuming hand reward engineering . The lack of a good reward function is one of the main bottlenecks for the low sample-efficiency issue in RL .

Imitation Learning (IL) has been proven to be an effective technique to learn control policies without the oracle task reward . Given an expert demonstration dataset, IL formulates the policy learning as a supervised learning paradigm . The IL objective aims to learn a policy that mimics the expert behaviors via minimizing a distance measure of the learned policy and an approximated expert policy . Depending on if the RL agent can learn from further online interactions, IL can be crudely classified as offline IL and online IL . In offline IL, the RL agent purely learns from afixed dataset of collected expert experiences. In online IL, the RL agent usually learns a proxy reward function to relabel the collected online trajectories with respect to the expert demonstrations .

One notable weakness of IL is that it generally requires a diverse and high-quality demonstration dataset to achieve desired performances . Recently, some researchers found that Optimal Transport (OT) [18; 44] based proxy reward function enables us to learn effective robot policies with only a few expert demonstrations [30; 31]. In this paper, we follow this line of research in applying OT-based proxy rewards to online IL without using any task reward information. In particular, we first revisit the efficacy of OT-based proxy reward in RL and then discuss some challenges of the existing methods due to the overlook of temporal order information. To mitigate this issue, we introduced the **Temporal Optimal** Transport (TemporalOT) reward, which incorporates temporal order information to the OT-based proxy reward via using context embeddings and a mask mechanism.

The primary contributions of this work can be summarized as follows:

* we pointed out a weakness of existing OT-based proxy reward methods for imitation learning due to the overlook of temporal order information;
* we designed a simple yet effective algorithm to incorporate temporal order information into OT-based proxy reward via using context embeddings and a mask mechanism;
* experiments show that the proposed method outperforms other SOTA algorithms.

## 2 Background

### Reinforcement Learning

In this work, we consider the standard Markov Decision Process (MDP)  setting \(=(,,R,P,_{0},)\), where \(\) and \(\) are state and action spaces, \(R:\) is a reward function, \(P:()\) is the state-transition probability function, \(_{0}:_{+}\) is the initial state distribution and \([0,1)\) is a discount factor. Our goal is to learn a policy \((a|s):()\) that maximizes the expected cumulative discounted rewards \(_{}^{}^{t}r(s_{t},a_{t})}\) where \(s_{0}_{0}\), \(s_{t+1} P(|s_{t},a_{t})\) and \(a_{t}(|s_{t})\). In partially observable MDP (POMDP) , we can only receive an observation \(o_{i}\), _i.e._, image observation, of the current state \(s_{i}\).

To solve this optimization problem, value-based RL methods typically learn a state-action value function \(Q^{}(s,a):=_{}^{}^{t}r_{t}}|s_{0 }=s,a_{0}=a\), which is defined as the expected return under policy \(\). For convenience, we adopt the vector notation \(Q^{}\), and define the one-step Bellman operator \(^{}:^{}^{}\) such that \(^{}Q(s,a):=r(s,a)+_{s^{} P,a^{} }[Q(s^{},a^{})]\). The \(Q\)-function \(Q^{}\) is the fixed point of \(^{}\) such that \(Q^{}=^{}Q^{}\). Similarly, we define the optimality Bellman operator as follows \(Q(s,a):=r(s,a)+_{s^{} P}[_{a^{} }Q(s^{},a^{})]\) and the optimal \(Q\)-value function \(Q^{*}\) is the fixed point of \(Q^{*}=Q^{*}\). In deep RL, we use neural networks \(Q_{}(s,a)\) to approximate the \(Q\)-functions by minimizing the empirical Bellman error:

\[_{(s,a,r,s^{})}[(r+_{a^{}}Q_{}^{}(s^{},a^{})-Q_{}^{}(s,a))^{2}],\] (1)

where we sample transitions \((s,a,r,s^{})\) from a replay buffer and \(Q_{}^{}(s,a)\) is the target network.

### Inverse Reinforcement Learning

Inverse Reinforcement Learning (IRL) aims to infer the underlying reward function from expert demonstrations , which further facilitates an RL agent to learn the policy. One key assumption of IRL is that the observed behaviors are optimal such that the observed trajectories maximize the cumulative rewards . Due to the ability to avoid the manual reward specification, IRL holds the promise for practical real-world RL applications. Denote \(\) as an MDP and \(^{E}\) as an expert policy, the IRL problem is to find an optimal reward function \(R^{*}\) such that:

\[[{_{t=0}^{}^{t}R^{*}(s_{t},a_{t}) ^{E}}][{_{t=0}^{}^{t}R^{*}(s _{t},a_{t})}],,\] (2)

where \(\) is the feasible policy set. That is, the expert policy \(^{E}\) will achieve the maximum expected cumulative discounted reward than any other policy.

### Optimal Transport

Optimal Transport is an optimization problem which aims to find an optimal mapping that transforms one probability distribution into another with the least cost. OT has a wide application in various domains such as economics , physics , and machine learning . Consider two probability distributions \(p^{n}\), \(q^{m}\) and a joint distribution \((p,q)\) on product space \(\), the Wasserstein distance  between \(p\) and \(q\) is defined as:

\[(p,q)=_{}_{}c(x,y)d,\] (3)

where \(c(x,y)\) is the cost function for moving mass form \(x\) to \(y\). In the RL scenario, \(p\) and \(q\) are usually in the state space \(\) or the observation space \(\). For example, given an expert trajectory \(^{E}=(o_{1}^{E},,o_{T}^{E})\) and an agent trajectory \(=(o_{1},,o_{T})\) where \(o_{i}\) is the image observation at step \(i\), the Wasserstein distance between \(^{E}\) and \(\) is defined in the following discrete form:

\[(,^{E})&=_{ ^{T T}}_{i=1}^{T}_{j=1}^{T}c(o_{i},o_{j}^{E})(i, j),\\ &_{i=1}^{T}(i,j)=_{j=1}^{T} (i,j)=.\] (4)

where \(^{T T}\) is called the transport plan, and we denote the optimal transport plan as \(^{*}\).

## 3 Method

In this section, we first revisit the application of OT-based proxy reward in RL. In particular, we point out the influence of temporal order information, which has been overlooked in most prior work. Next, we introduce the main idea and formulation of the proposed method based on these observations.

### A Recap of OT Reward in RL

#### 3.1.1 OT reward helps to rank states and actions

In RL, we usually adopt the Wasserstein distance to measure the similarity of two trajectories , as illustrated in Figure 1. Given an agent trajectory \(=(o_{1},,o_{T})\) and an expert trajectory

Figure 1: **An illustration of the pipeline of applying OT-based reward in RL.** In this toy example, we rollout two agent for five steps of transitions. Both agents start from the initial state and take same actions \(a_{0}\) and \(a_{1}\) at the first two states. Then the two agents take different actions \(a_{2}^{a}\) and \(a_{2}^{b}\) to generate different trajectories \(_{a}=(o_{0},a_{0},o_{1},a_{1},o_{2},a_{2}^{a},o_{3}^{a},o_{3}^{a},o_{4}^{a},a_{4}^{a},o_{5}^{a})\) and \(_{b}=(o_{0},a_{0},o_{1},a_{1},o_{2},a_{2}^{b},o_{3}^{b},a_{3}^{b},o_{4}^{b},a_{4}^{b},o_{5}^{b})\) The OT rewards for \((o_{0},a_{0})\) and \((o_{1},a_{1})\) in \(^{a}\) and \(^{b}\) are different even though the state-action pairs are exactly the same.

\(^{E}=(o_{1}^{E},,o_{T}^{E})\), we first compute the optimal transport plan \(^{*}\) in Eqn.(4) using some iterative optimization algorithms, _i.e._, Sinkhorn algorithm . Then, the OT-based proxy reward at the \(i\)-th step is defined as follows:

\[r_{i}^{OT}=-_{j=1}^{T}c(o_{i},o_{j}^{E})^{*}(i,j),\] (5)

where \(c(o_{i},o_{j}^{E})\) is a cost function that measures the similarity between \(o_{i}\) and \(o_{j}^{E}\). One popular choice in prior work is the cosine similarity based cost function \(c(o_{i},o_{j}^{E})=1-),f(o_{j}^{E})}{\|f(o_{i})\| \|f(o_{j}^{E})\|}\), where \(f(o_{i})\) is the latent representation of observation \(o_{i}\) extracted by a visual encoder .

Similar to the curiosity-based exploration bonus , OT reward is used to distinguish the goodness of different states. We consider the toy example in Figure 1, two agents start from the same state with observation \(o_{2}\) and take different actions \(a_{2}^{a}\) and \(a_{2}^{b}\), respectively. Then the goodness of \(a_{2}^{a}\) and \(a_{2}^{b}\) at \(o_{2}\) is measured by the OT reward computed w.r.t. the observation \(o_{3}^{a}\) and \(o_{3}^{b}\) at the next step. As long as the OT reward can rank \(r_{3}^{OT}(o_{3}^{a},^{E})\) and \(r_{3}^{OT}(o_{3}^{b},^{E})\) correctly, then the policy will be able to learn a better action at \(o_{2}\). From Figure 2 (right), we can observe that the OT reward for the better trajectory b is generally larger, which validates the previous explanations.

#### 3.1.2 Two Key Observations

Notably, there are two key observations of OT reward that have been less discussed in prior work:

1. The OT reward is order invariant.
2. The OT reward at step \(i\) is influenced by the later steps so that two transitions with the same state-action pair could have different OT rewards.

Our first observation is that the standard OT-reward is order invariant. As shown in Eqn.(5), the order information is discarded and the frames from the demo trajectory are treated as bag-of-temporally-collapsed frames. In our view, collapsing the temporal axis drops arguably one of the most important characteristic features of temporal order information. More concretely, consider a demo trajectory of \(_{1}=(o_{1},o_{1},o_{2})\), meaning the agent first stays in the first state and then moves to the second state. Our goal is to imitate this behavior. However, if we discard the order information as in Eqn.(5), from the perspective of OT reward, there is no ability to differentiate between \(_{1}\) and some other undesired trajectories, _i.e._, \(_{2}=(o_{1},o_{2},o_{1})\) which first moves to the second state and then moves back to the first state. Therefore, discarding the temporal order information in reward calculation

Figure 2: **Why OT reward could be useful?** When the OT reward is generally correct, it helps to rank the goodness of different states and induce the policy to take better actions. (left) In the toy example, two agents takes different action \(a_{2}^{a}\) and \(a_{2}^{b}\) at \(o_{2}\) and thereafter. The goodness of \(a_{2}^{a}\) and \(a_{2}^{b}\) is measured by the OT reward computed w.r.t. to the observation of the next state \(o_{3}^{a}\) and \(o_{3}^{b}\). (right) A comparison of the true OT reward curves for trajectory \(^{a}\) and \(^{b}\), where \(o_{0}\)/\(o_{1}\)/\(o_{2}\)/\(o_{3}\)/\(o_{4}\)/\(o_{5}\) correspond to observations at the 0/20/40/60/80/100-th step. We can observe that the OT reward for trajectory b is generally larger, which shows that the OT reward is generally correct.

makes the reward on top of it under-constrained, thereby increasing the likelihood of convergence toward undesired solutions.

Our second observation is that the OT reward is non-stationary during the training. As the Eqn.(5) shows that OT reward \(r_{i}^{OT}\) depends on the optimal transport plan \(^{*}(i,j)\), which depends on the entire trajectory. Therefore, the OT reward for each state is a function of the entire agent trajectory. As illustrated in Figure 1, even though trajectory \(^{a}\) and trajectory \(^{b}\) have the same first two transitions, their OT rewards have different values. This is very different from the standard RL setting, where reward is usually determined with a fixed state-action pair. Such a non-stationary OT reward could have a pitfall that makes the optimization of RL objective in Eqn.(1) to be less stable.

Inspired by these two observations, we found that the current OT-based RL methods usually overlook the temporal order information of the trajectories. In this work, we aim to investigate how to improve the current OT-based RL methods by incorporating temporal order information.

### Temporal Optimal Transport Reward (TemporalOT)

In this subsection, we present the **Temporal** Optimal **T**ransport (TemporalOT) reward. We first explain our motivations for the model design, and then introduce the details of the proposed method.

#### 3.2.1 Motivation for the Model Design

The pipeline of a standard OT-based reward calculation usually consists of two stages:

1. first define a transport cost function \(c(,)\) between two states;
2. then solve an OT optimization problem in Eqn.(4) to approximate the optimal transport plan \(^{*}\) and compute the OT reward \(r^{OT}\) in Eqn.(5) for each state in a trajectory \(\).

After the OT-reward calculation step, the transition will be relabeled with the OT-reward for training an RL agent as in Eqn.(1). Our method aims to improve both stages of OT-reward calculation. Firstly, previous methods usually use a pair-wise cosine similarity based cost function in Stage-1, which sometimes could be inaccurate and noisy. Secondly, previous methods ignore the temporal order information in Stage-2 as discussed in Section 3.1.2. We will introduce two simple solutions to address these two points, respectively.

#### 3.2.2 Context Embedding-based Cost Matrix for Improving Stage-1

To learn a more accurate transport cost function, we introduce a context embedding based cost matrix. Unlike previous methods that use a pair-wise cosine similarity as the transport cost, we adopt a group-wise cosine similarity that we define the transport cost between agent observation \(o_{i}\) and expert observation \(o_{j}^{E}\) as following:

\[(o_{i},o_{j}^{E})=}_{h=0}^{k_{c}-1}(1-),f(o_{j+h}^{E})}{\|f(o_{iH+h})\|\|f(o_{j+h}^{E})\|} ),\] (6)

where \(k_{c}\) is the parameter of the context length and \(f()\) is a fixed visual encoder. The goal of the context cost matrix \(\) is to facilitate expert progress estimation by taking nearby information into consideration. For example, we use \(k_{c}=3\) in Figure 3 and the transport cost between \(o_{1}\) and \(o_{2}^{E}\) is \((o_{1},o_{2}^{E})=1-[(f(o_{1}),f(o_{2}^{E}))+(f(o_{2}),f(o_{3} ^{E}))+(f(o_{3}),f(o_{4}^{E}))]/3\).

#### 3.2.3 Temporal-masked Optimal Transport Objective for Improving Stage-2

The prevalent OT reward ignores the temporal order information and takes the information of every step in the trajectory into consideration, as shown in the Eqn.(5). As pointed out by some previous work, the OT reward is not always correct where noisy OT rewards could distract the agent from learning some key early behaviors . To mitigate this issue, we introduce a concise solution by adding a temporal mask to the cost matrix.

For an agent trajectory \(=(o_{1},,o_{T})\) and an expert trajectory \(^{E}=(o_{1}^{E},,o_{T}^{E})\), we denote the context cost matrix as \(^{T T}\) and the transport plan as \(^{T T}\). The row sum and column sum of \(\) equal to the constraint \(=[,,]^{T}\). We proposed to introduce a temporal mask \(M^{T T}\) to the transport plan, where \(M(i,j)\). We can express the masked optimal transport objective in the following vector form :

\[^{*}=_{} M,_{F}-(M),=^{T}=,\] (7)

where \(,_{F}\) is the Frobenius norm and we add an entropy regularizer \(()\) of the masked transport plan \(M\). We can solve Eqn.(7) by the Lagrangian:

\[ L(,,)= M, _{F}+&( M,(M) _{F}-^{T}(M))-\\ &,(M)-_{F}- ,(M)^{}-_{F},\] (8)

where \(\) and \(\) are two Lagrangian multipliers. By using different temporal mask, we can control what kind of temporal order information we use in the OT reward. For example, \(M=\) degrades to the original OT reward without temporal order information, and a lower triangle matrix corresponds to the causal mask in the Transformer decoder , which indicates that we only concern the past steps observations. In our method, we use a variant of the diagonal matrix:

\[M(i,j)=1,&j[i-k_{m},i+k_{m}],\\ 0,&,\] (9)

where \(k_{m}\) is a window size parameter that controls the scope we use in the masked OT rewards. A smaller mask window size \(k_{m}\) refers to a closer match w.r.t. to the expert demonstration. We select a diagonal-like matrix because we follow previous learning from demonstration literature to assume that the agent has a similar movement speed as the expert . Under this assumption, we adopt the distance between the time step indexes to represent temporal affinity information. Figure 3 illustrates the main ideas of the proposed TemporalOT method.

## 4 Experiments

In this section, we aim to answer the following questions: (1) How does the proposed TemporalOT method perform compared with other baselines? (2) Are the proposed context-embedding based cost function and temporal mask useful? (3) How do the key parameters influence the performances? (4) Is TemporalOT effective with both state-based and pixel-based observations?

### Experimental Setup

We implement TemporalOT-RL in PyTorch  based on the official ADS implementation1. We use a pretrained ResNet50  network as the fixed visual encoder to extract the image embedding for each pixel observation. Unlike the original ADS experiments which use a fixed goal in each task, we adopt a more challenging setting where the goal position changes for each episode. Moreover, we only provide two expert video demonstrations to the RL agent. For the experiment results, we evaluate the RL agent for 100 trajectories every 20000 steps. We report the mean and standard deviation of the evaluation success rate across 5 random seeds. We define one trajectory to be successful if the RL agent solves the task at the last step. More detailed information is available in the Appendix B.

Figure 3: **An illustration of the proposed TemporalOT method. (left) Instead of using a pair-wise cosine similarity as the transport cost, we use a group-wise cosine similarity to learn a more accurate cost matrix. (right) We use a temporal mask to enforce the OT reward to focus on a narrow scope to avoid potential distractions from observations outside of the mask window.**

### Baselines

We compare the following baseline methods. (1) TaskReward: training a backbone RL agent from DrQ-v2  with the oracle task reward \(r_{h}^{}=_{}\). The reward is 1 when the task is solved and otherwise the reward is 0. (2) BC: a naive behavior cloning agent which has the access of the expert action. (3) GAIfO: another IL baseline which learns a discriminator to provide proxy reward . (4) OT: we use an online version of OTR  agent where we first rollout the RL agent to collect online trajectories and then relabel the reward with OTR for RL training. (5) ADS: a variant of OT baseline which adaptively adjusts the discount factor w.r.t. a progress tracker .

### Results on the Meta-world Benchmark Tasks

We first validate the effectiveness of TemporalOT on nine Meta-world  tasks. Experiment results are shown in Table 1. We can observe that TemporalOT generally outperforms the other baselines without using the task rewards. Moreover, the TaskReward baseline only shows good performance on the _Door-lock_ and _Window-open_ tasks. The main reason is that the RL agent fails to collect the first successful trajectory. For example, the goal of the _Basketball_ task as shown in Figure 1 is to pick up the basketball and move to a target position above the rim. With the oracle sparse task reward, the RL agent only receives a nonzero reward until it first successfully solves the task. Under such circumstances, it is particularly challenging to collect the first successful trajectory with only zero task rewards and random action explorations. On the other hand, we can observe that the two IL baselines, BC and GAIfO , perform much worse than other OT-reward based baselines. This is because we only provide two expert video demonstrations with a few hundred samples, where the IL-based methods suffer from an over-fitting issue. We further compare two OT agent baselines, where OT0.99 uses \(=0.99\), and OT0.9 uses \(=0.9\). We have a similar conclusion as in ADS that using a smaller discount factor is helpful to learn early behaviors in some tasks that strongly rely on the progress dependency, _i.e._, _Basketball_. TemporalOT outperforms the recent SOTA baseline ADS in 8 out of 9 tasks, which proves the effectiveness of the proposed method.

### Ablation Studies on Different Model Components

We further conduct ablation studies to validate the effectiveness of the proposed context cost matrix and temporal mask in TemporalOT. In Figure 4, _no-mask_ refers to a variant of TemporalOT without the context cost matrix. We can

   Environment & TaskReward & BC & GAIfO & OT0.99 & OT0.9 & ADS & **TemporalOT** \\  Basketball & 0.0 (0.0) & 0.2 (0.4) & 0.0 (0.0) & 0.0 (0.0) & 76.6 (27.4) & 42.2 (44.5) & **94.4 (4.7)** \\ Button-press & 14.0 (18.5) & 1.7 (2.4) & 1.0 (1.1) & **88.8 (2.5)** & 85.2 (3.3) & **89.0 (3.8)** & **92.4 (3.6)** \\ Door-lock & 86.2 (12.4) & 4.6 (7.0) & 8.8 (12.2) & 3.0 (5.5) & 2.8 (2.0) & 3.2 (2.7) & **33.4 (2.8)** \\ Door-open & 0.0 (0.0) & 10.7 (10.3) & 2.2 (1.7) & 46.2 (33.6) & 30.2 (34.5) & 52.0 (42.7) & **78.4 (12.4)** \\ Hand-insert & 0.8 (1.6) & 2.3 (2.1) & 8.6 (4.4) & 29.0 (9.7) & 11.2 (2.3) & **35.0 (5.3)** & **36.8 (6.6)** \\ Lever-pull & 0.0 (0.0) & 0.8 (1.6) & 3.4 (1.9) & 15.4 (15.5) & 35.6 (12.8) & 21.2 (12.0) & **53.6 (7.7)** \\ Push & 1.0 (0.7) & 0.4 (0.8) & 0.0 (0.0) & **14.2 (7.5)** & 7 (0.2) & **17.2 (5.6)** & 8.4 (1.7) \\ Stick-push & 0.0 (0.0) & 0.0 (0.0) & 18.8 (22.9) & 0.0 (0.0) & 48.8 (41.5) & 20.0 (40.0) & **97.6 (2.6)** \\ Window-open & 85.6 (12.2) & 1.6 (2.7) & 4.0 (4.7) & **54.0 (28.0)** & 22.4 (22.9) & 43.6 (20.5) & **55.2 (2.3)** \\  Average & 20.8 & 2.5 & 5.2 & 27.8 & 35.5 & 35.9 & **61.1** \\   

Table 1: **Experiment results of success rate on the Meta-world benchmark.**

Figure 4: **Ablation for model components. Both proposed components are useful.**observe that removing any of the two components will lead to a degraded performance. Moreover, the more important component varies depending on the task. For example, the temporal mask is more important in the _Door-open_ task, and the context cost matrix is more important in the _Stick-push_ task.

### Ablation Studies on Different Key Parameters

We then validate the efficacy of different key parameters, _i.e_., the context length \(k_{c}\) for the context embedding, window size \(k_{m}\) for the temporal mask, and the demonstration number \(N_{E}\). From Figure 5, we can observe that a medium number of \(k_{c}\) and \(k_{m}\) performs the best and a larger \(N_{E}\) improves the performances. A large context length \(k_{c}\) does not perform well because it will distract the OT reward from the current step and introduce extra reward noise. A smaller mask window size \(k_{m}\) makes the learning more difficult because it only receives information from nearby observations, and a larger \(k_{c}\) will gradually degrade to the naive OT reward. Further, having more expert demonstrations is helpful in mitigating the potential over-fitting issue and improving the final performance.

### Results with Pixel-based Observations

We also evaluate the proposed method with pixel-based observations, where we follow the same DrQ-v2 model setting as the ADS baseline. Figure 6 shows the results of the comparison of TemporalOT with ADS. We can observe similar conclusions as in Table 1 that our proposed TemporalOT method also outperforms the ADS baseline with pixel-based inputs, where TemporalOT usually converges faster than ADS and (or) achieves a higher final success rate. Moreover, we can observe that sometimes the pixel-based agent learns faster than its dense state-based counterpart, which indicates that the agent can extract more effective representations from the pixel inputs.

### Visualization for Bad Cases

In this subsection, we visualize some bad cases of OT-based RL agents to provide readers more insights about when OT-based RL agents are less useful. Figure 7 plots a typical bad case for the OT/ADS/TemporalOT agents in the _Hand-insert_ task. The top row is the expert trajectory, and the second row is the agent trajectory. The goal of the _Hand-insert_ task is to move the brown block to a target position in the hole. We can observe that the RL agent mainly focuses on imitating the arm behaviors which ends in the target position, but it ignores the brown block. The main reason for the this bad case is that the color of brown box is very close to the table background which sometimes

Figure 5: **Influences of key parameters.** A medium number of context length \(k_{c}\) or mask length \(k_{m}\) performs the best. The agent performs better with more expert demonstrations.

Figure 6: **Results with pixel-based inputs.** TemporalOT is also effective with pixel-based inputs.

make it difficult for the pretrained visual encoder to capture the subtle information. More bad case analyses are available in the Appendix A.2.

## 5 Related Work

**Learning with a Few Demonstrations.** There is a large body of work on leveraging demonstrations for policy learning, ranging from the basic behavior cloning [12; 45] to demonstration-aided RL . There is also work on leveraging demonstration data for offline pre-training [60; 36; 61], to either warm-start the policy  or help with exploration [61; 37; 22]. However, the amount of demonstrations required for a high-quality pre-training is typically large. In this work, we focus on the setting where only a small number of demonstrations are provided , thus greatly relieving the burden of generating demonstrations. Optimal Transport based imitation is a recently emerged approach in this direction, which will be reviewed in the subsequent section.

**Optimal Transport-based Reward for Imitation and RL.** Optimal Transport (OT) has been shown to be effective for imitation learning [18; 31]. Optimal Transport Reward Labeling (OTR)  uses Sinkhorn distance  to compute a similarity metric for a trajectory w.r.t. an expert demonstration and uses this metric as rewards for offline RL datasets without rewards . Automatic Discount Scheduling (ADS)  uses a similar OT-based approach for reward calculation. The core idea of ADS is to incorporate a scheduling of the discount factor for online RL to mitigate the potentially distracting OT reward from temporally distant states. Our work aligns with previous work in this category, and addresses some common issues that are shared by previous methods.

## 6 Limitations

Since our work is closely related to IL, our method shares some common limitations of IL. For example, the success of our method heavily depends on high-quality expert video demonstrations. If we are facing a new task without any available expert demonstrations, our method will be less useful. Moreover, if the given demonstrations are sub-optimal or biased, the learned policies will inherit these flaws as well. Moreover, the performance of the proposed method relies on the quality of the pretrained visual encoder. If the pretrained visual encoder fails to capture some key information in the pixel observation, then our method will also fail to take such key details into consideration. Another limitation of our work is that the computation cost of the proposed method is related to the number of the given expert video demonstrations. A larger number of expert demonstrations will increase the computation cost when we compute the optimal transport plan.

## 7 Conclusion

This paper studies the problem of learning effective robot policies with expert video demonstrations. We focus on a challenging setting where there are only two demonstrations available and the environment does not provide any task reward. Following the line of research of OT-based proxy reward, we first discuss some challenges of the existing methods due to the overlook of temporal information. Further, we introduced a new method named TemporalOT, which incorporates temporal information to existing baseline by using a context-embedding based cost matrix and a mask mechanism. Experiments on nine Meta-world benchmark tasks showcase the effectiveness of the proposed method. One interesting future direction is to extend the current method to a camera-view invariant agent, where we can learn policies w.r.t. expert video demonstrations from different camera views.

Figure 7: **Bad case analysis.** Compared with the expert trajectory (top), the agent focused on imitating the arm behavior (bottom) and missed the details, _i.e._, grasping the block.