# Understanding the Differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks

Understanding the Differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks

Jerome Sieber

ETH Zurich

Zurich, Switzerland

jsieber@ethz.ch

&Carmen Amo Alonso

ETH Zurich

Zurich, Switzerland

camoalonso@ethz.ch

&Alexandre Didier

ETH Zurich

Zurich, Switzerland

adidier@ethz.ch

&Melanie N. Zeilinger

ETH Zurich

Zurich, Switzerland

mzeilinger@ethz.ch

&Antonio Orvieto

ELLIS Institute Tubingen

Tubingen, Germany

antonio@tue.ellis.eu

These authors contributed equally; ordered randomly.

###### Abstract

Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.

## 1 Introduction

Foundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets . In recent years, the attention mechanism  has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths . In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) [4; 5; 6; 7; 8], as well as recent efforts to enhance Recurrent Neural Networks (RNNs) [9; 10; 11; 12]. Although these models show great promise in boosting efficiency, effortsto provide a rigorous theoretical comparison are scarce, and current comparisons with attention are merely empirical (see Section 5 for an in-depth discussion). Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking.

In order to close this gap, we introduce the Dynamical Systems Framework (DSF) - a theoretical framework based on a control theoretic perspective - that allows us to evaluate the similarities and differences between different foundation models in a principled manner. The DSF serves as a powerful tool for approaching theoretical research questions about foundation models, enabling direct comparisons - both theoretical and experimental - across architectures such as attention mechanisms, SSMs, and RNNs. We believe that the DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. The DSF further simplifies identification of existing computational algorithms to apply to newly developed models. Rather than providing an exhaustive list of insights, the results included below are meant to exemplify important questions that the DSF can answer and guide future research. Specifically, we explore the following questions:

* **How are attention mechanisms, SSMs, and RNNs related?** _TL;DR:_ All three model classes can be represented as recurrent models, which can be compared using the proposed DSF.
* **Can softmax attention be expressed as a recurrent model?** _TL;DR:_ Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite.
* **Why does state expansion help to improve performance of RNNs and SSMs?** _TL;DR:_ This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2).
* **Why do SSMs significantly outperform attention on the LRA benchmark?** _TL;DR:_ The performance gap can be explained by the recurrent normalization strategy (discretization step) used by selective SSMs as discussed in Section 4.2.
* **How closely are linear attention, S6 (i.e. Mamba) related?** _TL;DR:_ The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in their recurrent representation. However, the models differ in the parameterization of this parameter, which we analyze experimentally.

- with the state transition of S6 improves performance of the RNN.

We note that the main contribution of our paper is the introduction of the DSF, which is a unifying framework for analysis of attention mechanisms, SSMs, and RNNs. To the best of our knowledge, this is the first unified framework that allows analysis of all three model classes in the same parameterization and thus allows to identify differences in the models that lead to significant performance improvements. While some of the provided results already exist in the literature (e.g that increased state size improves performance), we also provide novel insights unique to the DSF framework in a comprehensive way that enables further analysis with control theoretical tools.

Notation:We use Latin letters in the following way: \(N\) is the size of the hidden state in the DSF, \(n\) the state expansion, \(d\) the embedding size or model size, and \(L\) the sequence length. A visual representation of these dimensions is given in Appendix A. We use superscripts, e.g. \({}^{.d}\), to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. \(_{i}\), to denote the time index (or input dependency). Specifically, \(v_{i}\) represents the value of vector \(v\) at time \(i\). We use bold notation to indicate sequences, i.e., \(_{i}=[v_{1},,v_{i}]\). We use \(()\) to denote the sigmoid function. The products \(\) and \(\) denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. \(_{n}\) denotes the identity matrix of size \(^{n n}\). Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity.

Preliminaries

In this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance ). In what follows, we review exclusively the backbone block, which we denote as \(f()\) in \(=f()\), where \(^{L d}\) and \(^{L d}\) are the input and output sequences, respectively.

### Attention

The standard self-attention block  consists of three matrices: \(W_{Q},\ W_{K},\ \ W_{V}\), which are the learnt parameters of the model. These matrices, when multiplied with the input \(\), yield the queries \(^{d_{k}}\), keys \(^{d_{k}}\), and values \(^{d_{v}}\), respectively:

\[=W_{Q},=W_{K},= W_{V}.\] (1)

Keys, queries, and values are then combined in the attention block to produce the output

\[=(^{}}{}}) ,\] (2)

where \(()\) is a map \(^{L}^{L}\) and is applied row-wise. For standard self-attention, the softmax function is used, i.e. \(()=()\), but given the limitations of the softmax function, alternative formulations have been proposed. We consider two formulations of attention: softmax attention (2) and linear attention . We focus on masked attention formulations, i.e., the attention matrix \((^{})\) has a lower-triangular structure, and to simplify the derivations, we drop the scaling factor \(}\).

### State Space Models

Architectures based on a state space parametrization compute the output \(\) through a dynamic recurrence of input signals at each time step \(i\), i.e.,

\[h_{i} =A_{i}h_{i-1}+B_{i}u_{i}\] (3a) \[y_{i} =C_{i}h_{i}+D_{i}u_{i},\] (3b)

where \(h_{i}\) is the hidden state of the system, and the dynamic matrices of appropriate dimensions \(A_{i},B_{i},C_{i},D_{i}\) are the learnt model parameters. Different time-varying and time-invariant parameterizations for \(A_{i},B_{i},C_{i},D_{i}\) have been proposed in the literature (an overview is given in ). Here we discuss the most prominent one.

S6.The first selective SSM parametrization (S6) was introduced together with the Mamba architecture . The S6 block parametrizes the recurrence as

\[A_{i}=e^{-_{i}A}, B_{i}=_{i}W_{B}u_{i}, C_{i}=W_{C}u_{ i}, D_{i}=W_{D}u_{i},\] (4)

with \(_{i}=(W_{}(W_{u}u_{i})+b_{})\) for every \(i\), \(W_{}\), \(W_{u}\), \(W_{B}\), \(W_{C}\), \(W_{D}\), and \(A\) are learnt matrices of appropriate dimensions, and \(b_{}\) is a learnt bias. While SSM models allow for complex-valued matrices \(A_{i},B_{i},C_{i},D_{i}\), here we restrict ourselves to real-valued matrices as in .

### Recurrent Neural Networks

Similar to SSMs, RNNs also parameterize the input-output relationship via a recurrent computation, commonly given by the long short-term memory (LSTM) , i.e., at each time step \(i\)

\[x_{i} =f_{i} x_{i-1}+i_{i}_{i},\] (5a) \[y_{i} =o_{i}(x_{i}),\] (5b)

where \(_{i}\) represents the pre-processed raw input \(u_{i}\), i.e.,

\[_{i}=(W_{u}u_{i}+U_{u}y_{i-1}),\] (6)

and \(f_{i}\), \(i_{i}\), and \(o_{i}\) are the forget gate, the input gate, and the output gate, respectively,

\[f_{i}=(W_{f}u_{i}+U_{f}y_{i-1}), i_{i}=(W_{i}u_{i}+U_{i}y_{i- 1}), o_{i}=(W_{o}u_{i}+U_{o}y_{i-1}),\] (7)

where \(W_{f},W_{i},W_{o}\) and \(U_{f},U_{i},U_{o}\) are the learnt gate parameters. In this paper, we focus on two variants: quasi LSTMs (qLSTM) , which removes the output dependence of the gates, and RG-LRU , which attempts to integrate ideas from SSMs into RNNs.

qLSTM.The qLSTM model is parameterized by recurrence (5) with pre-processed input \(_{i}\) and gates \(f_{i}\), \(i_{i}\), \(o_{i}\):

\[_{i}=(W_{u}u_{i}), f_{i}=(W_{f}u_{i}), i_{i}=( W_{i}u_{i}), o_{i}=(W_{o}u_{i}).\] (8)

RG-LRU.The RG-LRU model presents a hybrid between a qLSTM and a SSM using the recurrence

\[x_{i} =a_{i} x_{i-1}+^{2}}(i_{i} u_{i})\] (9a) \[y_{i} =x_{i},\] (9b)

with the following gates and no pre-processing of \(u_{i}\):

\[r_{i}=(W_{a}u_{i}), i_{i}=(W_{u}u_{i}), a_{i}=e^{-cr_{i} ()}.\] (10)

## 3 Dynamical Systems Framework for Architecture Comparison

In this section, we introduce the Dynamical Systems Framework (DSF) that allows in-depth analysis of the architectural features of attention, SSMs, and RNNs from a dynamical systems perspective. We use this to rewrite the parametrizations in a common framework and provide detailed comparisons.

### Dynamical Systems Framework (DSF)

The DSF relies on a dynamical systems representation of the architectures. A dynamical system models how a system's state, here denoted by \(h\), evolves over time according to a difference or differential equation. Dynamical systems often evolve under the evolution of some input \(u\), and the observable is an output \(y\). These systems capture time-dependent processes, rendering them suitable for understanding the behavior of sequence models. Here, we choose a recurrent state space representation. This choice is motivated by the widespread use of state space model representations for dynamical systems. Moreover, we show in later sections that this representation encompasses attention, RNNs, and SSMs in a suitable fashion that allows for further analysis. In particular, a linear structured time-varying (LTV) dynamical system is defined by the recurrence

\[h_{i} =_{i}h_{i-1}+B_{i}u_{i}\] (11a) \[y_{i} =C_{i}h_{i}+D_{i}u_{i},\] (11b)

where \(h_{i}^{N}\) is the hidden state initialized with \(h_{-1}=0\), \(_{i}^{N N}\) is the diagonal state transition matrix, \(B_{i}^{N d}\) and \(C_{i}^{d N}\) are the input and output matrices, respectively, and \(D_{i}^{d d}\) is a scaled skip connection. Dynamical system (11) can alternatively be written in its convolutional representation, i.e., \(=\), where the convolutional kernel \(\) is defined as

\[=C_{0}B_{0}+D_{0}&&&&\\ C_{1}_{1}B_{0}&C_{1}B_{1}+D_{1}&&\\ &&&&\\ C_{L}_{k=1}^{L}_{k}B_{0}&&C_{L}_{L}B_{L-1}&C_{L}B_{L}+D _{L}.\] (12)

Note that the convolution kernel \(\) is of the same dimension as the attention matrix \((^{})\) and that these matrices are equivalent, up to the scaling factor \(W_{V}\) used in self-attention.

**Remark 1**.: _This recurrent view yields a causal convolution kernel by definition. However, certain models (e.g. non-masked attention) also use non-causal kernels. This can be incorporated in the DSF (11) by modifying the state update (11a). For the sake of simplicity and consistency with the recent literature, we stick with causal models in the following._

### Architecture Reformulation

In the following, we show how popular architectures based on attention, SSMs, and RNNs can be rewritten into the DSF. To do this, all models will be reformulated into recurrence (11), i.e., all resulting DSF representations will have hidden state dimension \(N\).2 Although the parametrizationof models commonly found in the literature is conductive to efficient computation, here we depart from this convention. The goal of the DSF reformulation is to establish a theoretical framework that leads us to mathematical insights on the design of these models. The presented formulations are not intended to be computationally implemented in DSF form, however the framework can be used to identify computational algorithms for new architectures. For instance, the convolutional form of linear attention (12) is efficiently implemented via flash linear attention . However, using the recurrent form derived below it can also be implemented via scan algorithms , e.g., parallel scan  or accelerated scan . Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to new models even if the algorithm was designed for another model class.

#### 3.2.1 Attention

In the following, we assume that we can separate the nonlinear map in (2) as

\[(q_{i}^{}k_{j})=)^{}(k_{j})}{(q_{i}, _{i})},\] (13)

where \(():^{m}^{n}\), \(():^{m}^{n}\), and \((,):^{m}^{m(i+1)} \), which is the case for all the considered architectures in this paper. Note that if \(()\) is a kernel function, the proposed separability is satisfied by construction, as it holds that \(=\) and \(=1\). This allows us to write the self-attention input-output relationship as

\[y_{i}=_{j=0}^{i})^{}(k_{j})}{(q_{i},_{i})}W_{V}u_{j},\] (14)

with \(q_{i}=W_{Q}u_{i}^{m}\), \(k_{j}=W_{V}u_{j}^{m}\), and \(W_{Q}^{m d},\,W_{K}^{m d},\,W_{V} ^{d d}\). Hence, equation (14) can be reformulated into the DSF (11) as a dynamical system of dimension \(N=nd\), i.e., with hidden state \(h_{i}^{nd}\), and dynamic matrices

\[_{i} =,_{i-1})}{(q_{i},_{i} )}_{nd}^{nd nd},\] (15a) \[B_{i} =(,_{i-1})}_{d} (k_{j}))W_{V}^{nd d},\] (15b) \[C_{i} =_{d}(q_{i})^{}^{d nd}.\] (15c)

We note that for the recurrence (11), the matrix \(_{i}\) is given as an \(nd nd\) matrix, where \(n\) is the number of features in \(\) and \(\), and \(d\) is the input dimension. However, due to the scalar structure of \(_{i}\) in (15a), it can be implemented as the scalar multiplication \(,_{i-1})}{(q_{i},_{i})}h_{i-1}\) in (11). Hence, the hidden state is never materialized as such in the computation of the attention scores. Interested readers are referred to Appendix B for a detailed derivation.

Linear Attention.In the case of _linear attention_, both maps \(()\) and \(()\) in the DSF parametrization (15) are separable and we use the kernel proposed in , i.e.,

\[(q_{i})=(q_{i})+1,(k_{j})=(k_{j})+1, (q_{i},_{i})=((q_{i})+1)_{j=0}^{i}((k_{j})+1),\] (16)

where \(()\) is the exponential linear unit.

Generalized Linear Attention.We also study _generalized_ linear attention, where we require that the maps \(()\), \(()\) are linear, but allow for general nonlinear normalization functions \((q_{i},_{i})\), i.e.,

\[(q_{i})=q_{i},(k_{j})=k_{j},(q_{i},_{i}).\] (17)

Softmax Attention.Softmax attention also satisfies the assumption of separability (13). However, it holds that the feature vector representation of the transformed Gaussian kernel in the softmax function, i.e., \(e^{q_{i}^{}k_{j}}\), is infinite dimensional. Hence, the DSF representation (15) of softmax attention (2) and its corresponding hidden state dimension \(N\) would also be infinite dimensional. This insight gives further motivation to approximations of the softmax function by using, e.g., a Taylor series approximation such as in , to render the feature vector finite-dimensional.

**Lemma 1**.: _Softmax attention (2) can be expressed by separable attention (13) with_

\[(q_{i})^{}(k_{j})=(q_{i})^{}(k_{j})=e^{q_{i}^{}k_{j}},(q_{i},_{i})=_{j=0}^{i}e^{q_{i}^{}k_{j}},\] (18)

_where \((q_{i}):=c[1,q_{i},_{j=1}^{2}q_{i},_{j=1}^{3 }q_{i},]\) is an infinite-dimensional feature vector and \(c\) is a matrix of constant coefficients._

Proof.: The exponential in softmax attention \(e^{q_{i}^{}k_{j}}\) can be expressed in terms of its Taylor expansion, which consists of an infinite sum of polynomial kernels of increasing degree \(p\), decomposable through the vectors of monomials \(_{j=1}^{p}q_{i}\). See Appendix C for a complete proof. 

The work in  analyzes softmax attention as a kernel smoother and  shows that a kernel-based formulation can lead to linear complexity in sequence length for finite dimensional kernels. In , a kernel-based formulation of softmax is used to propose orthogonal random features to model softmax attention with linear complexity. In  a Taylor approximation of softmax attention is proposed, also leading to linear complexity. Finally,  relates transformer decoders to dynamical systems with increasing state size arising from the masked upper triangular part of the attention matrix. Compared to these works, we analyze how the proposed formulations compare in the recurrence (11) allowing us to compare to SSMs and RNNs in the following sections.

#### 3.2.2 State Space Models

SSM models are straightforward to rewrite in the DSF given their intrinsic recurrent linear representation. However, similarly to attention, we slightly rewrite the standard representation introduced in the literature to reveal deeper insights obscured by the standard representation focused on computational efficiency. The detailed derivation can be found in Appendix E.

S6.The S6 parametrization can be written in the DSF (11) as

\[_{i} =e^{-(_{i}_{n}) A}^{nd  nd},\] (19a) \[B_{i} =_{i} b_{i}^{nd d},\] (19b) \[C_{i} =_{d} c_{i}^{}^{d nd},\] (19c)

with \(_{i}=((W_{}(W_{u}u_{i})+b_{})) ^{d d}\), \(b_{i}=W_{B}u_{i}^{n}\), \(c_{i}=W_{C}u_{i}^{n}\), and \(W_{u}^{p d}\), \(W_{}^{d p}\) are weight matrices with \(p<d\), and \(b_{}^{d}\) is a bias. Note that in formulation (19) the dimensions of the matrices are \(_{i}^{nd nd}\), \(B_{i}^{nd d}\), \(C_{i}^{d nd}\), i.e., \(n\) is the state dimension and \(d\) is the input dimension in the original formulation (4).

#### 3.2.3 Recurrent Neural Networks

Given their recurrent nature, one can express LSTMs (5) in the DSF with some basic algebraic manipulations (see Appendix F for details). Once again, we slightly rewrite the standard representation since our goal is to obtain mathematical insights as opposed to computational efficiency.

qLSTM.In order to write the qLSTM formulation (8) in the DSF (11), a small modification is needed. In particular, the tanh functions in the input pre-processing (8) and output gate (5b) need to be dropped. Hence, the reformulated qLSTM in the DSF (11) writes as

\[_{i} =((W_{f}u_{i}))^{d d},\] (20a) \[B_{i} =((W_{i}u_{i})) W_{u}^{d  d}, C_{i}=((W_{o}u_{i}))^{d  d},\] (20b)

where \(W_{f},W_{i},W_{o},W_{u}^{d d}\) are the learnt parameters in (8). It is important to note that here the dimension of the hidden state \(h_{i}\) is equal to the number of input channels \(d\), whereas in attention and SSMs the dimension of the hidden state \(h_{i}\) in the DSF (11) is \(nd\). For qLSTMs \(n=1\), which will become relevant in further discussions; we refer to the fact that \(n>1\) as _state expansion_.

RG-LRU.Given the similarities of RG-LRU  and SSMs, it is straightforward to reformulate it into the DSF (11) without the need for modifications besides simple algebraic manipulations. Hence, the RG-LRU can be expressed in the DSF as

\[_{i}=e^{-cr_{i}(A)}^{d  d}, B_{i}=^{2}}(( W_{B}u_{i}))^{d d}, C_{i}=_{d},\] (21a) where \[r_{i}=((W_{R}u_{i}))\], and the function \[^{2}}\] is applied elementwise to \[_{i}\]. Similar to the qLSTM and in contrast with the other models, RG-LRU does not have state expansion, i.e. \[n=1\].

## 4 Architecture Comparison: Theoretical and Experimental Results

In this section, we use the DSF to explore some of the long-standing questions between attention, SSMs, and RNNs. We provide theoretical results and/or numerical experiments to substantiate our claims. The experiments presented below are performed on the multi-query associate recall (MQAR)  and long range arena (LRA)  benchmarks using the code bases 3 provided with the benchmarks. The complete experimental setup and computational resources used are detailed in Appendices J and K, respectively, and a statistical analysis is provided in Appendix L.

### Softmax Attention vs. Separable Attention.

Separable attention is used to avoid computation of the query-key matrix \(^{}\). It allows to compute \(^{}\) before multiplying the queries \(\), which reduces the computational complexity from quadratic to linear in sequence length. While the DSF shows how separable attention, and in particular kernelized attention can be rewritten as a recurrence (11), such a reformulation is only practical for a finite state dimension. However, in the case of \(()\), an infinite-dimensional kernel is needed, i.e., in the DSF, softmax attention requires \(n=\). This insight can mathematically explain why the good performance observed for softmax attention can only be approximated by separable attention mechanisms, SSMs, or RNNs; but no other architecture is equivalent. The DSF predicts that softmax can be better approximated by growing \(n\), which we show in the following theoretical result.

Lemma 2 ().: _For two dynamical systems (11) with hidden state dimensions \(N\) and \(\) with \(N\), the dynamical system of state dimension \(\) can always recover the dynamical system with state dimension \(N\)._

Proof.: The result follows from the fact that the first \(N\) states and the output in (11) can be chosen to be independent of the additional states. The full proof is given in Appendix D. 

Therefore, it holds that the expressivity of a model is non-decreasing with increasing state expansion \(n\) (and state dimension \(N=nd\)), if the rest of the architecture is held constant. As the softmax attention has an infinite hidden state dimension, i.e. \(n=\) (Lemma 1), we investigate empirically how its performance compares to linear attention (16), with increasing state dimension on the MQAR. Figure 1 shows that with larger \(n\) linear attention converges to the performance of softmax attention, which achieves perfect accuracy throughout.

Figure 1: Comparison of linear attention and softmax attention on two MQAR tasks \(\{(L=256,=16),(L=512,=64)\}\), fixed model size \(d=512\), and varying state expansion \(n\). We report the best result from a learning rate sweep in \((-4,-2,4)\).

### Generalized Linear Attention vs. S6.

By comparing the DSF expressions for both generalized linear attention (15) and S6 (19), we notice that the S6 parameters \(b_{i}=W_{B}u_{i}^{n}\), \(c_{i}=W_{C}u_{i}^{n}\) directly correspond to the keys and queries in attention, i.e. \(k_{i}=b_{i}\) and \(q_{i}=c_{i}\). Moreover, the state expansion \(n\) in S6 is the same as the hidden dimension in attention. However, while this leads to an equivalent output matrix \(C_{i}\) in the DSF parametrization for both architectures, there are remarkable differences between the two:

* similar to attention
- without compromising performance. Note that multi-headed attention increases the number of parameters in \(_{i}\) from \(1\) to the number of heads \(s\); for more details see Appendix G.
* discretization step \(_{i}\)
- does not cancel out in \(_{i}\) and \(B_{i}\) given their different structure. This impacts the selectivity of the matrices on the input, since some input-dependent features are normalized differently in the two architectures.

While the number of parameters in the state transition \(_{i}\) does play a role in increasing performance (multi-headed attention typically performs better than single-headed attention ), the results in  suggest that this role is small. The larger influence thus lies in the recursive structure of \(_{i}\) and \(B_{i}\) and/or the parameterization of normalization \(()\). To further investigate this and the role of normalization in attention, we compare S6 and softmax attention to SSD , linear attention , and _normalized attention_ on the MQAR  and LRA  benchmarks and train the three attention-based methods on WikiText-103. Inspired by S6, we define _normalized attention_ as the attention function

\[(q_{i})=q_{i},(k_{j})=k_{j},(u_{i})=e^{W_{}u_{i}},\] (22)

where \(W_{}^{1 d}\) is an additional learnt parameter. In Appendix H we discuss two alternatives to (22). The MQAR results are shown in Figure 2 and the average accuracy on the LRA and the training perplexity on WikiText-103 in Table 1. The MQAR results suggest that proper normalization, i.e., using normalization (22), improves the performance of linear attention schemes. This is further supported by the performance of S6 and SSD on the MQAR benchmark, since these two methods also employ input-dependent normalization. Additionally, normalized attention closes part of the gap to softmax attention on the WikiText-103 dataset (Table 1). However on LRA, SSM models (S6) still achieve considerably higher performance than attention-based models. While normalized

  
**Model** & **LRA** & **WikiText** \\  Linear Att. (16) & 53.52 & 17.42 \\ Norm. Att. (22) & 58.08 & 16.43 \\ Softmax Att. (2) & 55.96 & 13.15 \\ S6 (4) & 66.84 & N/A \\   

Table 1: Average accuracy on the LRA benchmark and training perplexity score for different attention architectures (70M params) on the WikiText-103 corpus.

Figure 2: Model accuracy with increasing model size \(d\) for different models: softmax, linear, and normalized attention, S6, and SSD. The MQAR task is (\(L=512,=64\)), we fix \(n=128\), and report the best performance of a learning rate sweep in \((-4,-2,4)\).

attention outperforms linear and softmax attention on the LRA, it performs significantly worse than S6. This result suggests that while the S6 inspired normalization helps to improve performance, the remaining performance gap is possibly explained by the recurrent normalization strategy employed by selective SSM models. Overall these results warrant further research into normalization strategies for attention-based models to explain the performance difference to SSMs. The complete experimental results on the MQAR and LRA benchmarks are detailed in Appendices L and M, respectively.

### RNNs vs. S6

Comparing RNNs and S6, it is immediate to observe several similarities. In particular, as shown in Appendix I, the state transition matrix \(_{i}\) in S6 (19) can be rewritten (assuming \(A=a_{nd}\)) as

\[_{i}=(_{}(_{}u_{i})^{a}) _{n}.\] (23)

Notice that when no state expansion is considered, i.e., \(n=1\) and \(_{n}=1\), this expression almost coincides with the qLSTM state transition (20a), with the only difference that (I) it uses the reversed sigmoid function instead of a sigmoid for the forget gate, and (II) there is an additional learnt parameter \(a\) in the exponent. Inspired by the subtle difference in the state transition, we compare the original qLSTM state transition (8) and the S6-inspired state transition 23 on the MQAR benchmark. The performance of both models is shown in Figure 3. We note that the reversed sigmoid state transition outperforms the original state transition on all three benchmark tasks, i.e., the performance of qLSTMs can be improved by insights from S6. Considering state expansion (\(n>1\)) for RNNs, the recent xLSTM paper  shows that state expanded LSTMs4 can yield similar performance to S6. This aligns with Lemma 2 and further highlights the importance of state expansion for expressivity. In qLSTM and RG-LRU, state expansion can be easily incorporated by changing the dimensions of the projections \(W_{f},\,W_{i},\,W_{o}\), where the \(\) operation in RG-LRU would be replaced by blockwise operations \(\). Finally, the most apparent difference between the two RNN variants - qLSTM and RG-LRU - and S6 is the parameter coupling in \(_{i}\) and \(B_{i}\). While qLSTM does not use a coupling, the couplings in RG-LRU and S6 are performed with different nonlinearities, which is discussed in more detail in [10, Appendix A].

## 5 Related Work

State-space models emerged from the S4 architecture by Gu et al. , who developed a new theoretically principled approach to sequence modeling rooted in polynomial approximation theory . The result is a transformer-like architecture , where attention is replaced by a linear recurrent neural network with special reparametrization. The design of S4 got later simplified in [4; 27], achieving state-of-the-art performance on the long-range arena (LRA)  with a highly efficient recurrent mechanism leveraging convolutional views , or parallel scans [5; 6].

The high efficiency of SSMs (linear processing) makes them particularly appealing when compared to softmax attention-based transformers, where both inference time and memory suffer quadratically from sequence length. The S4 architecture found first successful applications in reinforcement

Figure 3: Comparison of qLSTM (8) and a qLSTM variant where the original state transition \(_{i}\) is replaced by (23).

learning , vision , audio  as well as online learning . Initial attempts in language modeling [33; 34], supported by theoretical investigations [35; 36] hint at some necessary architectural improvements to unlock the NLP domain. Leveraging in-context learning arguments, a few works [37; 38; 39] started incorporating input selectivity mechanisms  into SSMs. These efforts culminated in the Mamba architecture , which proposed a highly efficient and light (in terms of parameters) input selectivity strategy, with drastic improvements when comparing to earlier variants (H3  and Hyena ) on text. This approach was also shown to be effective at byte level . Beyond text, Mamba was recently applied to the vision domain [43; 44] - with outstanding results compared to ViTs  both in terms of performance and efficiency. Other applications include e.g. genetics , and point clouds . Further, improvements on architectural aspects were proposed in [8; 11; 48].

The design of Mamba is also strongly supported by theoretical evidence showing precisely its superior expressive power compared to S4 . This boost in computational power is due to Mamba's novel input selectivity mechanism resembling gating, which unlocks content-dependent reasoning [7; 40]. Interestingly, input selectivity brings SSMs closer to attention: in particular, Ali et al.  showed that the particular parametrization of Mamba can be linked to a non-normalized softmax operator. This finding is also supported by evidence from language theory - Mamba and Attention can solve a similar class of problems . Beyond Ali et al.  the connection between linear attention and linear RNNs has been illustrated a few times in the literature [14; 23; 52; 53]. Connections between these architectures have also been carried out using tools from communication complexity in [54; 55]. Compared to these works and to Ali et al. , this paper offers a more careful comparison identifying some precise distinctions between SSMs, linear, and softmax attention - which play a nontrivial role in practice and can help bring to light interesting architectural improvements.

## 6 Conclusion

In this paper we presented the DSF, a framework based on dynamical-systems theory that allows analysis of different deep learning architectures by writing them as linear recurrences in state space. We first showed how to reformulate different architectures into the DSF, and then explored (theoretical and experimental) insights resulting from this analysis, thereby answering the questions posed in the introduction. For instance, we showed that with proper normalization the performance of linear attention can be significantly increased (see Fig. 2). We also show, that the DSF allows to integrate insights from one architecture to another as exemplified by Section 4.2. Additionally, the DSF naturally allows analysis of the eigenvalues of the state transition matrix \(A\), which are linked to the exploding/vanishing gradient problem . In the case of SSMs and RNNs, the eigenvalues are constrained to be stable by construction, for attention-based models this is not the case and stability needs to be obtained via normalization. The eigenvalues together with the state expansion also affect a model's long-term memory . Both of these aspects can be analyzed via the DSF and should be further investigated in future work. While the training dynamics (especially convergence) can be studied empirically using experiments, the DSF also allows a theoretical analysis. As discussed in Example 2 of , a gradient-based optimization algorithm (e.g. SGD) can be interpreted and written as a dynamical system. Using this viewpoint together with the DSF allows interpretation of the training dynamics as two interacting dynamical systems. Therefore, the training dynamics can be theoretically analyzed using tools from control theory, e.g., via Lyapunov theory for convergence and stability of the training. However, we believe this question requires an in-depth investigation and additional empirical validation of the theoretical findings. We expect that the DSF can serve as a tool for principled analysis and design of deep learning architectures.

Limitations.In terms of limitations, it is important to highlight that, while the DSF parametrization allows for a principled comparison between frameworks, architectures written in the DSF do not necessarily enjoy an efficient implementation unless their specific structure can leverage some of the existing algorithms (parallel scan, etc.). In terms of experiments, the insights mentioned above are only verified on two synthetic tasks (MQAR/LRA) and a smaller language task (WikiText-103). To strengthen the insights, a more detailed analysis is needed on larger and more complex tasks.