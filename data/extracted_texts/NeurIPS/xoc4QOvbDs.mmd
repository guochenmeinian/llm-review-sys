# Evaluate then Cooperate: Shapley-based View Cooperation Enhancement for Multi-view Clustering

Fangdi Wang1, Jiaqi Jin1, Jingtao Hu2,

**Suyuan Liu1, Xihong Yang1, Siwei Wang2,**

**Xinwang Liu1,**

1 National University of Defence Technology

2 Academy of Military Science

{wangfangdi19, wangsiwei13, xinwangliu, enzhu}@nudt.edu.cn

Corresponding author

###### Abstract

The fundamental goal of deep multi-view clustering is to achieve preferable task performance through inter-view cooperation. Although numerous DMVC approaches have been proposed, the collaboration role of individual views have not been well investigated in existing literature. Moreover, how to further enhance view cooperation for better fusion still needs to be explored. In this paper, we firstly consider DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, we introduce the Shapley value and propose a novel MVC framework termed Shapley-based **Co**operation **E**nhancing **M**ulti-**view Clustering (**SCE-MVC**), which evaluates view cooperation with game theory. Specially, we employ the optimal transport distance between fused cluster distributions and single view component as the utility function for computing shapley values. Afterwards, we apply shapley values to assess the contribution of each view and utilize these contributions to promote view cooperation. Comprehensive experimental results well support the effectiveness of our framework adopting to existing DMVC frameworks, demonstrating the importance and necessity of enhancing the cooperation among views.

## 1 Introduction

Recently, multi-view clustering has become one of the most prominent problems in unsupervised learning[1; 2; 3; 4]. It leverages data from different views, combining them to provide richer information for clustering tasks. Generally speaking, multi-view clustering can be broadly categorized into traditional methods [5; 6; 7; 8] and deep multi-view clustering (DMVC) [5; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19] methods. Compared to the limited feature extraction capabilities of traditional methods, deep multi-view clustering methods better preserve information from different views through flexible deep neural networks. For most DMVC methods, the key challenge is how to enhance cooperation among views to accomplish better clustering performance.

Numerous DMVC algorithms have already achieved significant results in downstream clustering tasks. However, there has been little research on investigating the the contribution of each view in the fusion stage. Once evaluating the view contribution, we may observe the phenomenon: One view dominates the fusion process, suppressing the collaborative contribution of other views, leading to suboptimal clustering results. We attribute this phenomenon to insufficient cooperation between views, and thus propose a fundamental assumption: More balanced contributions and more extensive cooperation among multiple views can lead to better clustering results, as shown in Fig. 1.

To this end, we consider introducing the concept of game theory into multi-view clustering and propose the **S**hapley-based **C**ooperation **E**nhancing (SCE) method. Shapley value is a concept used to measure the contribution of participants in game theory. It utilizes a utility function to evaluate the different marginal contributions that each participant brings when combined with other participants, and calculates their contribution to the overall cooperation by taking the weighted average over all possible combinations of participants. Upon quantifying the contribution of each view, the cooperation among views can be analyzed based on their respective contributions. By employing a module that enhances view cooperation, we can dynamically adjust the parameter convergence rate of views during training in accordance with their contributions. This enables the underrepresented views to receive a more balanced training regimen, thereby enhancing post-fusion performance.

The primary contributions of this work can be summarized as follows:

\(\) Our work is the first framework to evaluate the view contributions within an unsupervised context. Leveraging the proposed View Contribution Evaluation Module, we theoretically quantify the individual contributions of views.

\(\) We propose the View Cooperation Enhancing Module based on the view contributions obtained by the View Contribution Evaluation Module, allowing suppressed views to effectively participate in the fusion process, ultimately enhancing clustering performance.

\(\) Comprehensive experiments have been conducted to showcase the versatility of our SCE framework, which can effectively evaluate the contributions of views across various MVC frameworks.

## 2 Related Work

### Deep Multi-view Clustering

In deep multi-view clustering, deep neural networks with multiple nonlinear transformations are more effective at acquiring feature representations than traditional shallow models. Presently, deep multi-view clustering methods fall into three main categories: joint methods, alignment-based methods, and other methods. Joint methods consider differences and complementarities among views, optimizing sample representations within each view's space. For example, the DMJC utilizes autoencoders to generate view representations, refining them as pseudo-labels for self-supervised optimization. Alignment-based methods focus on consistency between views, mapping representations into a shared subspace. These methods often employ contrastive learning, such as ProImp, which integrates contrastive learning

Figure 1: Performance evaluation of view cooperation contribution and fusion ACC on (a) Caltech101-7 (b) UCI-digit. As can be seen, our proposed SCE framework clearly promotes inter-view cooperation and achieves more satisfactory performance.

at both sample and prototype levels. Some approaches combine joint and alignment-based methods, like the share-specific method, which separates sample representations into shared and specific components, aligns shared representations into a common space, and refines specific representations in separate spaces.

### Rethinking of Cooperation in DMVC

The core concept of DMVC is to enhance clustering performance by fostering enhanced cooperation and utilizing complementary information across views. Although existing DMVC research emphasizes leveraging inter-view cooperation for improved clustering tasks, there is currently a lack of research quantifying the contribution of views during the fusion stage to enhance cooperation. Multi views and multi modalities are two closely intertwined domains. Within the realm of multimodal fusion, studies have surfaced highlighting the issue of subpar collaboration among modalities, prompting endeavors to alleviate the imbalance in modality cooperation. Wang et al.  uncovered varying convergence rates among diverse modalities, leading to a scenario where jointly trained multimodal models struggle to match or outperform their unimodal counterparts. Furthermore, Peng et al.  demonstrated that in the context of audio-visual learning, superior performance in the audio modality hampers the optimization of the video modality.

Drawing inspiration from prior studies, our research aims to evaluate the individual contributions of each view in multi-view clustering fusion. Nevertheless, evaluating view contribution based solely on labels in unsupervised environments is infeasible. Moreover, employing weights as a measure for view contribution may not align with all DMVC structures, especially those based on contrastive learning frameworks. Consequently, quantifying view contributions in fusion and fostering their cooperation pose significant research challenges.

### Shapley Value: a Method for Evaluating Contribution.

In order to reasonably and effectively evaluate the contributions of each view in the fusion process, we refer to the theoretical framework of game theory. The Shapley value  within game theory proves instrumental in quantifying participants' contributions to cooperative coalitions. Denote \(=\{x_{i}\}_{i=1}^{n}\) as the alliances of \(n\) participants. For a participant \(x_{i}\), the Shapley value considers \(x_{i}\)'s contribution in every set that includes him. Let \(S_{i}=\{S|x_{i} S\}\) represent the set of all subsets of \(\) that include the participant \(x_{i}\), then, the overall contribution of member \(x_{i}\), _i.e._, the Shapley value, can be expressed as

\[Shapley_{i}=_{s S_{i}}[v(s)-v(s \{i\})],\] (1)

where \(|s|\) signifies the cardinality of set \(s\); \(v()\) represents the utility function; \([v(s)-(s\{i\})]\) quantifies the marginal contribution of \(x_{i}\) to set \(s\); and \(\) denotes the weight assigned to this marginal contribution, determined by the probability of set \(s\) occurring.

Given the advantageous characteristics of Shapley values--efficiency, symmetry, dummy, and additivity--researchers have increasingly employed them for explaining machine learning models. For instance, Lundberg et al.  proposed a method to interpret predictions by computing feature importance based on Shapley values. Hu et al.  applied Shapley values to evaluate modality contributions in supervised tasks, while Wei et al.  explored modality assessment using Shapley values. However, these studies focus mainly on supervised scenarios, making the definition of utility functions for Shapley values in unsupervised settings a challenging task yet to be resolved.

## 3 Method

### Problem Statement

Given a multi-view dataset \(=\{^{(1)},^{(2)},...,^{(V)}\}\), where \(V\) is the number of views, and \(^{(v)}(v=1,2,...,V)\) denotes the original feature space of the \(v\)-th view. Consider the clustering problem on the dataset, each cluster is represented by a clustering center \(_{j}^{(v)},j=1,...,K\), where \(K\) is the number of clusters. To achieve better clustering performance, we map the original feature space \(^{(v)}\) to a latent embedded feature space \(^{(v)}\) through a non-linear mapping function \(f_{^{(v)}}^{(v)}:\ \ \ ^{(v)}\ \ \ ^{(v)}\), where \(^{(v)}\) represents the learnable parameters of mapping function for the \(v\)-th view.

### View Contribution Evaluation Module

#### Modified Shapley Value for Multi Views

To assess the contributions of each view in MVC tasks, we treat each view as a player and introduce the Shapley value referring to Eq. (1). Let \(=\{^{(i)}\}_{i=1}^{V}\) be the set of all views, \(_{v}=\{|^{(v)} \}\) represent the set of all subsets of \(\) that include the \(v\)-th view \(^{(v)}\). Define an internal metric of clustering \(E\) independently of labels to serve as a utility function, which will be elaborated on in the next subsection. Then the Shapley value of the \(v\)-th view can be calculated as

\[Shapley_{v}=_{s_{v}}[E(s)-E( s\{^{(v)}\})],\] (2)

where \(|s|\) denotes the number of views in the view-set \(s\).

#### Utility Function in Shapley Value

In Shapley value, the utility function is used to evaluate the benefit created by a coalition. In DMVC, where each view acts as a participant, the utility function needs to be designed to reflect the contribution of any combination of views in fusion, without the need for label assistance. In this scenario, we constructed a novel function \(E\) using the following method.

Inspired by the DEC model , we assume that the data from each view follows a Student's t-distribution. Following the approach outlined by , we utilize the t-distribution as a kernel function to evaluate the distance between the sample embeddings \(_{i}\) and the cluster centers \(_{j}\) within a view:

\[_{ij}=_{i}-_{j}\|^{2}/)^ {-}}{_{j^{}}(1+\|_{i}-_ {j^{}}\|^{2}/)^{-}},\] (3)

where \(\) represents the degrees of freedom in the \(t\)-distribution. \(_{ij}\) denotes the probability of assigning the feature \(_{i}\) to the cluster center \(_{j}\).

Figure 2: The framework of our proposed SCE model. SCE framework can be applied to most mainstream DMVC methods. After obtaining the cluster distribution from the selected method, SCE iteratively using the following two modules: View Contribution Evaluation Module and View Cooperation Enhancing Module. View Contribution Evaluation Module computes the marginal contribution of views in each combination(Shapley value), in which the optimal transport distance between the view cluster distribution and the strengthened cluster distribution after fusion serves as the utility function of Shapley formula. View Cooperation Enhancing Module controls the convergence speed ratio of different views through the calculated view contribution.

Fix the degrees of freedom \(\) to 1, the cluster distribution of multi-view data \(\) can be considered as a combination of single-view cluster distribution based on a weight matrix \(\):

\[_{ij}=_{j}^{(v)}(1+\|_{i}^{(v)}-_{j}^{(v)}\|^{2})^{-1}}{_{j^{}}_{v^{}}_{j^{ }}^{(v^{})}(1+\|_{i}^{(v^{})}-_{j^{ }}^{(v^{})}\|^{2})^{-1}},\] (4)

where the weight matrix \(^{K V}\) represents the importance of the cluster centers \(_{j}\) in different views. This weight matrix \(\) can be obtained by performing row normalization on an unconstrained trainable matrix \(\):

\[_{j}^{(v)}=_{j}^{(v)}}}{_{i=1}^{V}e^{_{j}^{(v)}}}.\] (5)

For any given combination of an \(m\)-view union \(=\{^{(u_{1})},^{(u_{2})},...,^{(u_{m })}\}\), we can obtain the weight matrix \(^{}\) by selecting the corresponding columns in the matrix \(\). By applying Eq. (3), we derive the data distribution for the fused view, which is denoted as \(^{}\).

In particular, if \(=\), which means no view is involved in the clustering process, we consider \(^{}\) to be a uniform distribution. This means that the probability of any embedding \(_{i}\) belonging to any cluster \(_{j}\) is equal and given by \(1/K\), where \(K\) is the total cluster numbers. In this case, we denote the resulting data distribution \(^{}\) as

\[^{}=_{N K},\] (6)

where \(_{N K}^{N K}\) is a matrix in which all elements are equal to 1.

According to the principle of self-distillation in DEC, we sharpen the fused distribution \(^{}\) from all views to obtain a more powerful target distribution \(\):

\[_{ij}=(_{ij}^{})^{2}/_{j}(_{ij}^ {})^{2}.\] (7)

Since we consider the distribution \(\) as the trend of the fusion process, the closer the fusion distribution \(^{}\) (combined with \(=\{^{(u_{1})},^{(u_{2})},...,^{(u_{m })}\}\)) is to the distribution \(\), the higher the participation of view set \(\) in the fusion process. Therefore, we can use the optimal transport distance from distribution \(^{}\) to distribution \(\) as a measure of confidence for distribution \(^{}\). Additionally, based on our desire for the metric to possess monotonicity and boundedness, we define the utility function based on distribution transportation as:

\[E()=,^{})},\] (8)

where \(OT(,)\) is calculated as wasserstein distance and \(E()(0,1]\). The closer the data distribution is to the pseudo-labels \(\), the larger the value of the metric \(E()\).

In the process of computing the optimal transport \(OT(,)\), we consider the distribution \(\) as a probability set where \(K\) cluster centers are assigned by \(N\) samples. The probability of each cluster center is \(1/K\). For example, when calculating \(OT(^{\{^{(1)}\}},^{\{^{(2)}\}})\), the distribution of views can be represented as \([1/K,1/K,...,1/K]^{T}\), and the distance matrix \(\{_{ij}\}_{K K}\) can be calculated as

\[_{ij}=_{i}^{(1)}-_{j}^{(2)}_{2}^{2},\] (9)

where the \(j\)-th cluster center for the \(v\)-th view can be represented as \(_{j}^{(v)}=\{_{ij}^{(v)}\}_{N 1}\).

**Shapley-based View Contribution: a Formal Representation**

In the above subsections, we have already discussed the integration of cooperative game theory with multi-view clustering tasks. We have also defined a utility function \(E\) that reflects the accuracy of clustering. Now, combining the Eq. (2) and (8), we present the formal representation of view-contribution through Shapley values:

\[Shapley_{v}=_{s_{v}}[,^{s})}-,^{s \{^{(v)}\}})}].\] (10)The sum of all view contributions is then calculated as:

\[_{v}Shapley_{v}=,^{})}-,^{})}.\] (11)

As \(\) and \(^{}\) evolve throughout training, the sum of Shapley values for all views also fluctuate accordingly. To obtain the contribution proportions of each view to the fused view, let

\[_{v}=}{_{i=1}^{V}Shapley_{i}},\] (12)

where \(_{v}\) is the normalized Shapley value, determining the respective contribution of different views.

### Theoretical Analysis

To further demonstrate the generalization performance of the View Contribution Evaluation Module, we apply it to the alignment-based method and the joint method, respectively. The relevant theoretical derivations are given below.

**Theorem 1:** For alignment-based methods, considering the representation \(\{_{i}^{(1)}\}_{i=1}^{N}\) and \(\{_{i}^{(2)}\}_{i=1}^{N}\) on two views, with the infoNCE contrastive learning loss:

\[_{i}^{(1)}=-_{i}^{(1)}{}^{}_{i}^{( 2)}/_{l})}{_{j=1}^{N}[(_{i}^{(1)}{}^{}_{j}^{(1)}/_{l})+(_{i}^{(1)}{}^{}_{j}^{(2)}/ _{l})]},\] (13)

\[_{con}(1,2)=_{i=1}^{N}(_{i}^{(1)}+_{i}^{(2)} ),\] (14)

where \(N\) is the sample number and \(>0\) is a scalar temperature hyperparameter. \(_{con}(1,2)\) brings the representations between views closer, leads to the average of view contributions, _i.e._, \(|_{1}-_{2}| 0\).

**Theorem 2:** For joint methods, considering the representation \(^{(1)}=f_{^{(1)}}^{(1)}(^{(1)};^{(1)})\) and \(^{(2)}=f_{^{(2)}}^{(2)}(^{(2)};^{(2)})\) on two views, with a simple view-level weight fusion during the fusion process: \(=w_{1}^{(1)}+w_{2}^{(2)}\). There are situations where view 2 is dominated by view 1 and cannot make its contribution, _i.e._, \(w_{1} w_{2}\).

**Remark:** Theorem 1 indicates that alignment-based methods tend to align the contributions of different views. Under our assumption, the cooperation between views is already sufficient, optimizing the model based on contributions can only lead to limited improvements. Theorem 2 suggests that in joint methods, one view may suppress another, highlighting insufficient cooperation between views. In such cases, enhancing cooperation between views theoretically improves model performance. We will elaborate on the View Cooperation Enhancing Module in Section 3.4 and validate these conclusions through experiments. The proofs for Theorems 1 and 2 can be found in Appendix.

### View Cooperation Enhancing Module

After acquiring contributions of different views, we seek to enhance their cooperation by dynamically regulating views' training speeds according to these contributions. This proportional control guarantees active involvement of all views in the integration process. For this purpose, we introduce the convergence speed ratio, labeled as \(k\), derived from the contributions of each view.

To ensure the applicability of our **S**hapley-based **C**ooperation **E**n**hancing(**SCE**) method to most multi-view clustering tasks, we consider a typical multi-view clustering framework: firstly, obtain embeddings for each view by utilizing autoencoders; then, fuse the embeddings from different views using a global optimization goal. Let \(f_{^{(v)}}^{(v)}(^{(v)};^{(v)})\) represent the encoder model for the \(v\)-th view, \(g\) denote the fusion method for multiple views, and \(l\) represent the loss function for computing the fusion loss. The overall fusion loss \(L\) can be represented as

\[L(_{t})=l(g(f_{^{(1)}}^{(1)}(^{(1)};_{t}^{(1)}),f_ {^{(2)}}^{(2)}(^{(2)};_{t}^{(2)}),...,f_{^{(V)}}^ {(V)}(^{(V)};_{t}^{(V)}))).\] (15)In practice, the parameters for the \(v\)-th view is updated as

\[_{t+1}^{(v)}=_{t}^{(v)}-_{^{(v)}}L(_{t}^{(v)}).\] (16)

With the normalized Shapley values calculated by Eq. (12) to dynamically monitor the contribution differences between different views, we are able to adaptively modulate the gradients via an empirical formula:

\[k_{v}=e^{(_{min}/_{v}-1)},\] (17)

where \(\) is a temperature hyperparameter to control the degree of modulation. Thus \(k_{v}\) is not greater than \(1\), and the larger the contribution of the view, the smaller the \(k_{v}\). We integrate the coefficient \(k_{i}\) into the Adam optimization method, and the update of \(_{t+1}^{(v)}\) at iteration \(t\) is as follows:

\[_{t+1}^{(v)}=_{t}^{(v)}-k_{v}}}{ _{t}^{(v)}},\] (18)

where \(k_{v}\) is used as a relative convergence speed ratio for the \(v\)-th view in gradient descent.

**Theorem 3**: _For joint methods that use view-level weight fusion \(=w_{1}^{(1)}+w_{2}^{(2)}\), where \(^{(1)}=f_{^{(1)}}^{(1)}(^{(1)};^{(1)})\) and \(^{(2)}=f_{^{(2)}}^{(2)}(^{(2)};^{(2)})\), gradient modulation in Eq. (18) allows the two views to contribute more evenly, i.e., \(}{w_{2}} 1\)._

**Remark:** Theorem 3 indicates that through dynamic gradient adjustments, we can harmonize the learning progress among different views, attain a more average distribution of contributions, and enhance the cooperation among views. The proof for Theorem 3 can be found in Appendix.

**Input:** Multi-view dataset \(\{^{(v)}\}_{v=1}^{V}\), temperature hyperparameter \(\)

**Output:** cluster labels \(\)

// Warm-up training

Obtain initial cluster assignments through a selected algorithm;

// Optimization with SCE module

**while**_the fusion loss has not converge_**do**

// View Contribution Evaluation

Obtain shapley value for each view with Eq. (4), (7) and (10);

Normalize the shapley value as view contribution with Eq. (12);

// View Cooperation Enhancing

Calculate the convergence speed ratio \(k_{v}\) for each view with Eq. (17);

Update the parameters of each view relatively with Eq. (18);

return;

**Algorithm 1**Shapley-based **Cooperation** Enhancing **M**ulti-view **C**lustering(**SCE-MVC**)

### Implementation

This subsection outlines the implementation of our SCE module, detailed in Algorithm 1. After obtaining initial cluster center assignments, we iteratively employ the following two modules:

**View Contribution Evaluation**. First, we calculate the cluster distribution for each individual view as well as the combined view. Next, we compute an enhanced distribution \(\). After utilizing the optimal transport distance between distributions as the utility function, we can calculate view contributions.

**View Cooperation Enhancing**. We utilize the calculated contribution values to influence the training process of each view using a convergence ratio \(k\). The larger the contribution value of a view, the less the convergence ratio \(k\) will be. This will coordinate the training process of the views, enabling better cooperation among views, and allowing them to collaborate more effectively.

## 4 Experiments

In this section, we implement experiments on alignment-based method and joint method to verify the effectiveness of the proposed theorems and SCE module by addressing the following questions:

**RQ1**: For alignment-based MVC methods, do the contributions of views calculated by the View Contribution Evaluation Module exhibit relative uniformity, thereby validating **Theorem 1**?

**RQ2**: In the case of uniform view contributions, can model performance be further enhanced by the View Cooperation Enhancing Module?

**RQ3**: For joint methods, are extreme view contributions present, thereby validating **Theorem 2**?

**RQ4**: Can model performance be enhanced by harmonizing view contributions through View Cooperation Enhancing Module, thereby validating **Theorem 3**?

**RQ5**: How do the hyper-parameter \(\) impact the performance of SCE-MVC?

### Datasets, Metrics and Experimental settings

Our experiments utilized six multi-view datasets, including CUB2, Caltech101-73, HandWritten4, UCI-digit5, STL106 and Reuters7. The detailed information of these datasets is listed in the Table 1. Meanwhile, three widely used metrics are adopted in our experiment, including clustering accuracy (ACC), normalized mutual information (NMI) and adjusted rand index(ARI).

For fairness, We conduct all experiments on PyTorch platform using the NVIDIA 2060 GPU. Besides, ten state-of-the-art MVC methods are introduced: DEMVC, CoMVC, SiMVC, SDSNE, MFLVC, SDMVC, DSMVC, APADC, DMJC and ProImp.

### Evaluate on Alignment-based Methods(RQ1 & RQ2)

In this section, we apply the SCE methods to two alignment-based MVC methods:

- Utilizing infoNCE as a contrastive loss and employing Kmeans after concatenation.

- Incorporating contrastive learning at both the sample and prototype levels as ProIMP.

We applied these two approaches to the CUB and Caltech101-7 datasets, each comprising two views. Detailed experimental results are presented in the Table 2. The results lead to following conclusions:

  Dataset & Views & Samples & Clusters & Dataset & Views & Samples & Clusters \\  CUB & 2 & 600 & 10 & UCI-digit & 3 & 2000 & 10 \\ Caltech101-7 & 2 & 1474 & 7 & STL10 & 4 & 13000 & 10 \\ HandWritten & 3 & 2000 & 10 & Reuters & 5 & 1200 & 6 \\  

Table 1: Dataset summary.

  Dataset & Method &  &  \\   & & \(_{1}\) & \(_{2}\) & ACC & NMI & ARI \\   & InfoNCE+Kmeans & 0.491 & 0.509 & 0.715 & **0.748** & **0.626** \\  & InfoNCE+Kmeans+**SCE** & 0.493 & 0.507 & **0.717** & 0.747 & **0.626** \\   & ProIMP & 0.556 & 0.443 & 0.825 & 0.756 & 0.671 \\  & ProIMP+**SCE** & 0.484 & 0.516 & **0.832** & **0.762** & **0.678** \\   & InfoNCE+Kmeans & 0.484 & 0.516 & 0.351 & **0.486** & 0.272 \\  & InfoNCE+Kmeans+**SCE** & 0.496 & 0.504 & **0.364** & 0.485 & **0.281** \\    & ProIMP & 0.489 & 0.511 & **0.382** & 0.468 & **0.281** \\   & ProIMP+**SCE** & 0.499 & 0.501 & **0.382** & **0.470** & 0.279 \\  

Table 2: Analyzing view contribution under InfoNCE+Kmeans and ProImp frameworks on CUB and Caltech101-7 datasets.

[MISSING_PAGE_FAIL:9]

### Sensitive Analysis(RQ5)

In SCE-MVC, the singular hyperparameter \(\) is essential for fine-tuning the convergence equilibrium among distinct views according to their contributions. In sensitive analysis, we varied \(\) within \(\{0.5,1,1.5,2,2.5,3\}\), as illustrated in Table 4. As \(\) increases, the changes in clustering results metrics become less pronounced, which can be attributed to the characteristics of the exponential functions in Eq. (17). Specifically, as \(\) increases, the convergence ratio \(k_{v}\) tends to be closer to zero.

## 5 Conclusion

This paper explores the roles of views in multi-view clustering through the lens of game theory. We introduce a novel contribution evaluation module founded on the Shapley value. Our experiments showcase the adaptability of this module across mainstream MVC frameworks, accurately assessing each view's impact. By scrutinizing the functions of diverse views during fusion, we devise a cooperation enhancing module to bolster cooperation among views. Extensive dataset validations underscore the effectiveness and robust applicability of our approach and theorems.