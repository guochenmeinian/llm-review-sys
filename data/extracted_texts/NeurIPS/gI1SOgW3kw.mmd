# Generalizing Nonlinear ICA Beyond Structural Sparsity

Yujia Zheng\({}^{1}\), Kun Zhang\({}^{1,2}\)

\({}^{1}\) Carnegie Mellon University

\({}^{2}\) Mohamed bin Zayed University of Artificial Intelligence

{yujiazh, kunz1}@cmu.edu

###### Abstract

Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as _Structural Sparsity_, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are more observed variables than sources (undercomplete), and when certain sparsity and/or source independence assumptions are not met for some changing sources. Moreover, we show that even in cases with flexible grouping structures (e.g., part of the sources can be divided into irreducible independent groups with various sizes), appropriate identifiability results can also be established. Theoretical claims are supported empirically on both synthetic and real-world datasets.

## 1 Introduction

The unveiling of the true generating process of observations is fundamental to scientific discovery. Nonlinear independent component analysis (ICA) provides a statistical framework that represents a set of observed variables \(\) as a nonlinear mixture of independent latent sources \(\), i.e., \(=()\). Unlike linear ICA (Comon, 1994), the mixing function \(\) can be an unknown nonlinear function, thus generalizing the theory to more real-world tasks. However, the identifiability of nonlinear ICA has been a long-standing problem for decades. The main obstacle is that, without additional assumptions, there exist infinite spurious solutions returning independent variables that are mixtures of the true sources (Hyvarinen and Pajunen, 1999). In the context of machine learning, this makes the theoretical analysis of unsupervised learning of disentangled representations difficult (Locatello et al., 2019).

To overcome this challenge, recent work has introduced the auxiliary variable \(\), and assumed that all sources are conditionally independent given \(\). Most of these methods require auxiliary variables to be observable, such as class labels and domain indices (Hyvarinen and Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020; Sorrenson et al., 2020; Lachapelle et al., 2022; Lachapelle and Lacoste-Julien, 2022), with the exceptions being those for time series (Hyvarinen and Morioka, 2017; Halva et al., 2021; Yao et al., 2021, 2022). While the use of the auxiliary variable \(\) allows for the identifiability of nonlinear ICA with mild restrictions on the mixing process, it also necessitates a large number of distinct values of \(\), which can be difficult to obtain in tasks with insufficient side information. Moreover, since these results assume that all sources are dependent on \(\), they cannot accommodate a subset of sources with invariant distributions (e.g., content may not change with different styles).

Another possible direction is to impose appropriate conditions on the mixing process, but limited results are available in the literature. For example, it has been shown that conformal maps are identifiable up to specific indeterminacies (Hyvarinen and Pajunen, 1999; Buchholz et al., 2022). Moreover, Taleb and Jutten (1999) identify the latent sources when the mixing process is a component-wise nonlinear function added to a linear mixture. These methods do not rely on conditional independence given the auxiliary variable and thus achieve the identifiability in a fully unsupervised setting. At the same time, the requirement of above-mentioned classes of the mixing function, such as conformal maps and post-nonlinear models, restricts the applicability of the results in another way. For instance, according to Liouville's theorem (Monge, 1850), conformal maps in Euclidean spaces of dimensions higher than two are Mobius transformations, which appear to be overly restrictive for most data-generating processes. As an alternative, Zheng et al. (2022) prove that, under the assumption of _Structural Sparsity_, the true sources can be identified up to trivial indeterminacies. Since the proposed condition is on the connective structure from sources to observed variables, i.e., the support of the Jacobian matrix of the mixing function, it does not require the mixing function to be of any specific algebraic form. Thus, _Structural Sparsity_ may serve as one of the first general principles for the identifiability of nonlinear ICA in a fully unsupervised setting.

While being a potential solution to the identifiability of nonlinear ICA without side information, the assumption of _Structural Sparsity_ has its limitations from a pragmatic viewpoint. The most obvious one arises from the fact that it may fail in a number of situations where the generating processes are heavily entangled. Although the principle of simplicity may be a general rule in nature, it is intuitively possible that _Structural Sparsity_ does not apply to at least a subset of sources, such as one or a few speakers in a crowded room. Unfortunately, Zheng et al. (2022) require _Structural Sparsity_ to hold for all sources in order to provide any identifiability guarantee. Therefore, it would be desirable in practice to provide weaker notions of identifiability, such as the ability to identify a subset of sources to a trivial degree of uncertainty, in cases of partial sparsity.

In addition to partial sparsity, identifiability with _Structural Sparsity_ also fails with the undercompleteness (more observed variables than sources) and/or partial source dependence (potential dependence among some hidden sources). These limitations are not unique to the sparsity assumption, but rather a result of the traditional setting of ICA, where the numbers of the sources and observed variables must be equal and dependencies among sources are not allowed. However, both situations are quite common in practice. One may easily have millions of pixels (observed variables) but only dozens of hidden concepts (sources) in a picture, constituting an undercomplete case that cannot be handled by previous results. Meanwhile, dependencies among some variables are also prevalent in tasks such as computational biology (Cardoso, 1998; Theis, 2006). The alternative assumption of conditional independence given auxiliary variables may still be overly restrictive if applied universally to all sources. For the identifiability of nonlinear ICA to truly benefit scientific discovery in a wider range of scenarios, these methodological limitations should be properly addressed.

Aiming to generalize nonlinear ICA with _Structural Sparsity_, we first present a set of new identifiability results to address these fundamental challenges of undercompleteness, partial sparsity, and source dependence. We show that, under the assumption of _Structural Sparsity_ and without auxiliary variables, latent sources can be identified from their nonlinear mixtures up to a component-wise invertible transformation and a permutation, even when there are more observed variables than sources (Thm. 3.1). Moreover, if the assumption of sparsity and/or source independence does not hold for some changing sources, we provide partial identifiability results, showing that the remaining sources can still be identified up to the same trivial indeterminacy (Thm. 4.1, Thm. 4.2). Furthermore, in the cases with flexible grouping structures (e.g., part of the sources can be grouped into irreducible independent subgroupings with various sizes, such as mixtures of signals with various dimensions), certain types of identifiability are also guaranteed with auxiliary variables (Thm. 4.3, Thm. 4.4). Therefore, we establish, to the best of our knowledge, one of the first general frameworks for uncovering latent variables with appropriate identifiability guarantees in a principled manner. The theoretical claims are validated empirically through our experiments and many previous works involving disentanglement.

## 2 Preliminaries

The data-generating process of nonlinear ICA is as follows:

\[p_{}() =_{i=1}^{n}p_{_{i}}(_{i}),\] (1) \[ =(),\] (2)

where \(=(_{1},,_{n})^{n}\) is a latent vector representing the independent sources, and \(=(_{1},,_{m}) ^{m}\) denotes the observed random vector. The mixing function \(\) is assumed to be smooth in the sense that its second-order derivatives exist. The primary objective of ICA is to establish _identifiable_ models, i.e., the sources \(\) are identifiable (recoverable) up to certain indeterminacies by learning an estimated mixing function \(}:}\) with assumptions identical to the generating process (Comon, 1994). Different from most ICA results where \(m=n\) and \(:\) must be linear, we allow \(m>n\) (i.e., undercompleteness) and \(\) to be a general nonlinear function, therefore extending the previous setting. Thus, we relax the previous assumption on the invertibility of \(\), only necessitating it to be injective and its Jacobian to be of full column rank. Furthermore, we denote \(p_{s_{i}}\) as the marginal probability density function (PDF) of the \(i\)-th source \(s_{i}\) and \(p_{}\) as the joint PDF of the random vector \(\). Moreover, we introduce some additional technical notations as follows:

**Definition 2.1**.: Given a subset \(\{1,,n\}\), the subspace \(^{n}_{}\) is defined as

\[^{n}_{}\{z^{n} i  z_{i}=0\},\]

where \(z_{i}\) is the \(i\)-th element of the vector \(z\).

That is, \(^{n}_{}\) denotes the subspace of \(^{n}\) specified by an index set \(\). Furthermore, we define the support of a matrix as follows:

**Definition 2.2**.: The support of a matrix \(^{m n}\) is defined as

\[()\{(i,j)_{i,j} 0 \}.\]

With a slight abuse of notation, we reuse \(()\) to denote the support of a matrix-valued function:

**Definition 2.3**.: The support of a function \(:^{m n}\) is defined as

\[(())\{(i,j) ,()_{i,j} 0\}.\]

For brevity, we denote \(\) and \(}\) as the support of the Jacobian \(_{}()\) and \(_{}}(})\), respectively. Additionally, \(\) refers to a set of matrices with the same support of \(()\) in \(_{}}(})=_{}( )()\), where \(()\) is a matrix-valued function. Throughout this work, for any matrix \(\), we use \(_{i,:}\) to denote its \(i\)-th row, and \(_{:,j}\) to denote its \(j\)-th column. For any set of indices \(\{1,,m\}\{1,,n\}\), analogously, we have \(_{i,:}\{j(i,j)\}\) and \(_{:,j}\{i(i,j)\}\).

## 3 Identifiability with undercompleteness

We first present the result on removing one of the major assumptions in ICA, i.e., the number of observed variables \(m\) must be equal to that of hidden sources \(n\). We prove that, in the undercomplete case (\(m>n\)), sources can be identified up to a trivial indeterminacy under _Structural Sparsity_.

**Theorem 3.1**.: _Let the observed data be a large enough sample generated by an undercomplete nonlinear ICA model as defined in Eqs. (1) and (2). Suppose the following assumptions hold:_

1. _For each_ \(i\{1,,n\}\)_, there exist_ \(\{^{()}\}_{=1}^{|_{i,:}|}\) _and a matrix_ \(\) _s.t._ \(\{_{}(^{()})_{i,:}\}_{=1}^{| _{i,:}|}=^{n}_{_{i,:}}\) _and_ \([_{}(^{()})]_{i,:} ^{n}_{_{i,:}}\)_._
2. _(Structural Sparsity) For each_ \(k\{1,,n\}\)_, there exists_ \(_{k}\) _s.t._ \(_{i_{k}}_{i,:}=\{k\}\)_._

_Then \(\) is identifiable up to an element-wise invertible transformation and a permutation._

The proof is included in Appx. A.1, of which part of the conditions and techniques are based on (Zheng et al., 2022). It is noteworthy that, same as previous work, we also need to add a sparsityregularization on the learned Jacobian during the estimation so that \(|}|||\), which is required for all sparsity-based identifications throughout the paper and we only emphasize here for brevity.

Assumption i avoids some pathological conditions (e.g., samples are from very limited sub-populations that only span a degenerate subspace) and is typically satisfied asymptotically. The first part implies that there are at least \(|_{i,:}|\) observed samples spanning the support space, which is almost always satisfied asymptotically. The second part is also relatively mild. Note that \(\) refers to a set of matrices with the same support of \(()\) in \(_{}}(})=_{}( )()\) and \(_{}}(})_{i,:}^{n}_{ _{i,:}}\). Since we only necessitate the existence of one matrix \(\) in the entire space, even in rare cases where these two matrices do not share the same non-zero coordinates due to non-generic canceling between specific values of elements, there is almost always an existence of a matrix \(\) fulfilling the assumption.

Assumption ii, i.e., _Structural Sparsity_, originates from (Zheng et al., 2022). Intriguingly, compared to the original bijective setting considered by (Zheng et al., 2022), this assumption is much more likely to be satisfied in the undercomplete case. The key reason is that it only necessitates the existence of a subset of observed variables whose intersection uniquely identifies the target source variable. For instance, regarding \(_{1}\) in Fig. 1, there exist \(_{1}\) and \(_{4}\) s.t. the intersection of their parents is only \(_{1}\). In principle, the size of this set can be quite small (e.g., one or two). Hence, it is very likely to be satisfied when there is a sufficient number of observed variables (e.g., millions of pixels for images), which has also been verified empirically in our experiments (e.g., Fig. 4 in Sec. 5). Additionally, in some tasks, we might even construct or select observations in a data-centric manner to satisfy this assumption. Without the previous constraint of bijectivity, structural sparsity can truly be applied in a much broader range of practical scenarios.

By proving the identifiability in the undercomplete case, we remove the previous assumption of bijectivity on the mixing function \(\) and thus generalizing the theory to more application scenarios. It is worth noting that, while some work has provided results without assuming bijectivity (Khemakhem et al., 2020), they rely on extra information from many distinct values of auxiliary variables. Differently, we do not need any auxiliary variables and follow a fully unsupervised setting; Zheng et al. (2022); Kivva et al. (2022) explore the undercompleteness without any auxiliary variable. However, Zheng et al. (2022) only remove the rotational indeterminacy in the nonlinear case and Kivva et al. (2022) assume Gaussian mixture priors, while we provide the full identifiability result without distributional assumptions. At the same time, as elaborated above, the assumption of _Structural Sparsity_ has been significantly weakened in the undercomplete case considered by our theorem. Moreover, identifiability with undercompleteness is also essential if assumptions are partially violated w.r.t. a subset of sources, of which the intuition is verified by theoretical results introduced in the following sections.

## 4 Identifiability with partial sparsity and source dependence

Under the condition of _Structural Sparsity_, we show the identifiability of undercomplete ICA with general nonlinear functions (Thm. 3.1). While this removes the restriction of bijectivity between sources and observed variables, it remains uncertain as to whether _Structural Sparsity_ holds for all sources in a universal way. At the same time, even in the scenarios that _Structural Sparsity_ may not be universally satisfied for all sources, it is still valuable to consider its potential to hold true for a subset of sources. This type of partial sparsity may often be the case in practical scenarios, as illustrated by our experiments (e.g., Fig. 5 in Sec. 5). However, the corresponding partial identifiability, i.e., the theoretical guarantee for the identification of the subset of sources satisfying _Structural Sparsity_, is not achieved by (Zheng et al., 2022). In fact, as long as one or a few sources do not meet the assumption of _Structural Sparsity_, the previous work is unable to provide any identifiability guarantees.

Furthermore, in addition to the universal sparsity, the statistical independence between sources is another fundamental assumption. This assumption arises from the original setting of ICA and has been adopted in most related works. However, in many real-world scenarios, requiring _all_ sources to be mutually independent might be impractical, and there are likely to be a subset of sources that

Figure 1: The structural sparsity assumption in the undercomplete case, where the matrix represents \((_{}())\).

are dependent in some way. For example, the frequency and duration of smoking, as well as the type of tobacco products used, are all interrelated factors that contribute to the development of lung cancer. Without any identifiability guarantees in the case where there exist any dependent sources, it is quite restrictive for nonlinear ICA, and even its undercomplete extension, to successfully deal with real problems in practice. Therefore, similar to the partial sparsity case, it is highly desirable that alternative theoretical results, i.e., identifiability for independent sources, can be guaranteed even with the existence of dependent sources.

To deal with these remaining challenges in the considered undercomplete case, we further relax other assumptions with additional information. To start with, we provide an identifiability result when _Structural Sparsity_ holds true for only a subset of sources, thus alleviating the obstacle of partial sparsity. Moreover, we relax the mutual independence assumption and allow for some changing sources to be dependent on each other. To this end, we partition the sources into two parts \(=[_{I},_{D}]\), where variables in \(_{I}\) are mutually independent, but those in \(_{D}\) do not need to be. Let \(_{I}\) and \(_{D}\) correspond to variables in \(\) with indices \(\{1,,n_{I}\}\) and \(\{n_{I+1},,n\}\), respectively. That is, \(_{I}=(s_{1},,s_{n_{I}})_{I}^ {n_{I}}\) and \(_{D}=(s_{n_{I}+1},,s_{n})_{D} ^{n_{D}}\). We denote the \(i\)-th scalar element in a vector, say \(\), as \(s_{i}\). For sources in \(_{D}\), they do not need to be mutually independent as long as they are dependent on a variable \(\), i.e.,

\[p_{|}(|)=p_{_{D}| }(_{D}|)_{i=1}^{n_{i}}p_{s_{i}}(s_{i}).\] (3)

It is noteworthy that we allow arbitrary relations between sources in \(_{D}\). These sources might be grouped into several subspaces or actually be mutually independent, but we do not need to obtain this information as prior knowledge. This is essential since the exact dependence structures, or even the number of dependent variables, are usually unknown in practice. By keeping this type of uncertainty for both partial sparsity and/or partial source dependence, one can be more confident in applying the theoretical advancements of nonlinear ICA in various tasks. We prove the identifiability for this arguably more flexible scenario as the following theorems:

**Theorem 4.1**.: _Let the observed data be a large enough sample generated by an undercomplete nonlinear ICA model defined in Eqs. (2) and (3). Suppose the following assumptions hold:_

1. _There exist_ \(n_{D}+1\) _distinct values of_ \(\)_, i.e.,_ \(_{j}\) _with_ \(j\{0,1,,n_{D}\}\)_, s.t. the_ \(n_{D}\) _vectors_ \((_{D},,i)\) _with_ \(i\{n_{I}+1,...,n\}\) _are linearly independent, where vector_ \((_{D},,i)\) _is defined as follows:_ \[(_{D},,i)=_{D}|_{1})-(p(_{D}|_{0}))) }{ s_{i}},,_{D}| _{n_{D}})-(p(_{D}|_{0})))}{ s_{i}} .\]
2. _There exist_ \(_{1},_{2}\)_, s.t., for any set_ \(A_{}\) _with non-zero probability measure and cannot be expressed as_ \(B_{_{I}}_{D}\) _for any_ \(B_{_{I}}_{I}\)_, we have_ \[_{ A_{}}p_{|}( _{1})d_{ A_{}}p _{|}(_{2})d.\]

_Then \(_{D}\) is identifiable up to an subspace-wise invertible transformation._

Thm. 4.1 ensures the subspace-wise identifiability of \(_{D}\), i.e., the estimated subspace \(}_{D}\) contains all and only information from \(_{D}\). This implies that we can disentangle and extract the invariant part of the latents, beneficial for tasks like domain adaptation where recovering each individual source might not be necessary as long as the subspace that is invariant across domains can be disentangled. Furthermore, we also prove the component-wise identifiability as follows:

**Theorem 4.2**.: _In addition to assumptions in Thm. 4.1, suppose the following assumptions hold:_

1. _For each_ \(i\{1,,n_{I}\}\)_, there exist_ \(\{^{()}\}_{=1}^{|_{i,n_{I}}|}\) _and a matrix_ \(\) _s.t._ \(\{_{}(^{()})_{i,:n_{I}}\}_{=1 }^{|_{i,n_{I}}|}=_{_{i,n_{I}}}^{n_{I}}\) _and_ \(_{}(^{()})_{i,:n_{I}} _{_{i,n_{I}}}^{n_{I}}\)_._
2. _(Structural Sparsity) For all_ \(k\{1,,n_{I}\}\)_, there exists_ \(_{k}\) _s.t._ \(_{i_{k}}_{i,:n_{I}}=\{k\}\)_._

_Then \(_{I}\) is identifiable up to an element-wise invertible transformation and a permutation._The proofs are presented in Appx. A.2 and Appx. A.3. We tackle the challenge of partial sparsity and dependence by necessitating _Structural Sparsity_ and independence only on a subset of sources in \(_{I}\) and prove that these sources can be identified up to trivial indeterminacies. For the remaining sources in \(_{D}\), they only need to be dependent on an auxiliary variable \(\) without necessitating conditional independence among sources or distributional assumption. This extends previous models that assume all sources to be conditionally independent given \(\)(Hyvarinen et al., 2019; Lachapelle et al., 2022) or require the conditional distribution of the sources to be of a specific form (Khemakhem et al., 2020).

The assumption on \(p(_{D}|)\) in Thm. 4.1 indicates that the auxiliary variable \(\) should have a sufficiently diverse impact on sources without independence assumption (i.e., \(_{D}\)). It follows a similar spirit to the standard assumption of variability (Hyvarinen et al., 2019) but we further relax it. Specifically, we only need \(n_{D}+1\) values of \(\) for the identifiability of sources in \(_{I}\). This is intuitively reasonable since the _fewer changes_ (smaller \(n_{D}\)) a system has, the _easier_ (fewer required values, i.e., \(n_{D}+1\)) that _a larger part of it_ (larger \(n_{I}\), i.e., \(n-n_{D}\)) is identifiable. In contrast, most previous works require all sources to be dependent on an auxiliary variable \(\) with \(2n+1\) distinct values of \(\): no identifiability for any subset of sources can be provided if there exists any degree of violations, either on the number of sources dependent on \(\) or the number of values of \(\). This limits the application of these results to ideal scenarios where all sources are influenced by the same auxiliary variable with sufficient changes without any type of compromise. In practice, however, it is often the case that only a subset of sources benefit from the additional information provided by auxiliary variables, different auxiliary variables may affect different sources, or auxiliary variables do not contain sufficient information. Assumption ii in Thm. 4.1 is originally from (Kong et al., 2022) and also necessitate the presence of change. Intuitively, the chance of having a subset \(A_{}\) on which all domain distributions have an equal probability measure is very slim, which has been verified empirically in (Kong et al., 2022). For both theorems, we consider the more challenging undercomplete case, for which the related identifiability results are lacking in the literature. Additionally, unlike previous works assuming specific distributions of sources such as exponential families, we do not have similar distributional assumptions on the sources.

### Results with flexible grouping structures

If we further have access to the dependence structure among variables in \(_{D}\), additional identifiability results for these sources may also be established. For example, consider the setting that \(_{D}=(s_{n_{I}+1},,s_{n})\) can be decomposed to \(d\) irreducible independent subspaces \(\{_{c_{1}},,_{c_{d}}\}\), of which each is a multi-dimensional vector consisting multiple sources. We denote the \(j\)-th consecutive \(d\)-dimensional vector (\(j\)-th subspace) in \(\) as \(_{c_{j}}=(s_{(j-1)d+1},,s_{jd})=(s_{c_{j}(t)},,s_{c_{j} (b)})\), where \(s_{c_{j}(t)}\) and \(s_{c_{j}(b)}\) are the first and the last sources in \(_{c_{j}}\), respectively. Then we have

\[p_{|}(|)=_{i=1}^{n_{i}}p_{s_{i}} (s_{i})_{j=c_{1}}^{c_{d}}p_{_{c_{j}}|}(_{c _{j}}|).\] (4)

This is similar to Independent Subspace Analysis (ISA) (Hyvarinen and Hoyer, 2000; Theis, 2006) but we allow only a subset of sources as a composition of (conditionally) independent subspaces instead of all, which formalizes the tasks of blind source separation or uncovering latent variable models with mixtures of both high-dimensional and one-dimensional signals. The considered general setting essentially covers ICA and ISA as special cases: if \(n_{I}=n\), it is consistent with the ICA problem; if \(n_{I}=0\), all sources can be decomposed into irreducible independent subspaces, and thus it becomes an ISA problem. The identifiability result under this setting is shown in the following theorem with its proof provided in Appx. A.4:

**Theorem 4.3**.: _Let the observed data be a large enough sample generated from an undercomplete nonlinear ICA model as defined in Eqs. (2) and (4). Suppose the following assumptions hold:_

1. _For each_ \(i\{1,,n_{I}\}\)_, there exist_ \(\{^{()}\}_{=1}^{|_{i,n_{I}}|}\) _and a matrix_ \(\) _s.t._ \(\{_{}(^{()})_{i,n_{I}}\}_{= 1}^{|_{i,n_{I}}|}=_{_{i,n_{I}}}^{n_{I}}\) _and_ \([_{}(^{()})]_{i,n_{I}} _{_{i,n_{I}}}^{n_{I}}\)_._
2. _There exist_ \(2n_{D}+1\) _values of_ \(\)_, i.e.,_ \(_{i}\) _with_ \(i\{0,1,,2n_{D}\}\)_, s.t. the_ \(2n_{D}\) _vectors_ \((_{D},_{i})-(_{D},_ {0})\) _with_ \(i\{1,,2n_{D}\}\) _are linearly independent, where vector_ \((_{D},_{i})\) _is defined as follows:_ \[(_{D},_{i})=((_{c_{1}}, _{i}),,(_{c_{d}},_{i}),^{}(_{c_{1}},_{i}),,^{}( _{c_{d}},_{i})),\]_where_ \[(_{c_{j}},_{i}) =_{c_{j}}|_{i})}{  s_{c_{j}(^{1})}},,_{c_{j}}| _{i})}{ s_{c_{j}(^{h})}},\] \[^{}(_{c_{j}},_{i}) = p(_{c_{j}}|_{i })}{( s_{c_{j}(^{h})})^{2}},, p( _{c_{j}}|_{i})}{( s_{c_{j}(^{h})})^{2}}.\]
3. _There exist_ \(_{1},_{2}\)_, s.t., for any set_ \(A_{}\) _with nonzero probability measure and cannot be expressed as_ \(B_{_{I}}_{D}\) _for any_ \(B_{_{I}}_{I}\)_, we have_ \[_{ A_{}}p_{|}( _{1})d_{ A_{}}p_ {|}(_{2})d.\] _iv. (Structural Sparsity) For all_ \(k\{1,,n_{I}\}\)_, there exists_ \(_{k}\) _s.t._ \(_{i_{k}}_{i,:n_{I}}=\{k\}\)_._

_Then \(_{I}\) is identifiable up to an element-wise invertible transformation and a permutation, and \(_{D}\) is identifiable up to a subspace-wise invertible transformation and a subspace-wise permutation._

All assumptions align with the same principles as those elaborated in the theorems proposed above and have been adapted to cater to the flexible grouping structure. Specifically, in addition to the identifiability of sources in \(_{I}\), we prove that we can also identify sources in \(_{D}\) up to an indeterminacy that, for each \(c_{i}\{c_{1},,c_{d}\}\), there exists an invertible transformation \(_{c_{i}}\) s.t. \(_{c_{i}}(_{c_{i}})=}_{c_{i}}\), which is analogous to the previous element-wise indeterminacy. Consequently, even when dealing with mixtures of high and one-dimensional sources, like in the case of multi-modal data, we can still recover the hidden generating process to some extent. Based on the aforementioned theoretical results, which consider undercompleteness, partial sparsity, and partial source dependence, Thm. 4.3 further generalizes the identifiability of nonlinear ICA by relaxing the dimensionality constraint of the latent generating factors.

In this vein, it is natural to consider another dependence structure, i.e., sources in \(s_{D}\) are not marginally but conditionally independent given an auxiliary variable \(\). This is similar to the assumption made in most previous works on identifiable nonlinear ICA with surrogate information, which assume that all sources are conditionally independent of each other given the auxiliary variable. However, our setting is more flexible in the sense that we do not assume all sources to be influenced by the auxiliary variable. Specifically, sources in \(_{I}\) are mutually independent as in the original ICA setting, while only sources in \(_{D}\) have access to the side information from the conditional independence given \(\), i.e.,

\[p_{|}(|)=_{i=1}^{n_{I}}p_{s_{i}} (s_{i})_{j=n_{I}+1}^{n}p_{s_{j}|}(s_{j}|).\] (5)

The identifiability result for all sources (\(_{I}\) and \(_{D}\)) is as follows with proof in Appx. A.5:

**Theorem 4.4**.: _Let the observed data be a large enough sample generated from an undercomplete nonlinear ICA model as defined in Eqs. (2) and (5), suppose the following assumptions hold:_

1. _For each_ \(i\{1,,n_{I}\}\)_, there exist_ \(\{^{()}\}_{=1}^{|_{i,n_{I}}|}\) _and a matrix_ \(\) _s.t._ \(\{_{}(^{()})_{i,n_{I}}\}_{=1 }^{|_{i,n_{I}}|}=_{_{i,n_{I}}}^{n_{I}}\) _and_ \(_{}(^{()})_{i,n_{I}} _{_{i,:n_{I}}}^{n_{I}}\)_._
2. _There exist_ \(2n_{D}+1\) _values of_ \(\)_, i.e.,_ \(_{i}\) _with_ \(i\{0,1,,2n_{D}\}\)_, s.t. the_ \(2n_{D}\) _vectors_ \((_{D},_{i})-(_{D},_{0})\) _with_ \(i\{1,,2n_{D}\}\) _are linearly independent, where vector_ \((_{D},)\) _is defined as follows:_ \[(_{D},_{i})=((_{D}, _{i}),^{}(_{D},_{i})),\] _where_ \[(_{D},_{i}) =+1}|_{i})}{  s_{n_{I}+1}},,|_{i})}{  s_{n}},\] \[^{}(_{D},_{i}) = p(s_{n_{I}+1}|_{i})}{ ( s_{n_{I}+1})^{2}},, p(s_{n}|_{i}) }{( s_{n})^{2}}.\]
3. _There exist_ \(_{1},_{2}\)_, s.t., for any set_ \(A_{}\) _with nonzero probability measure and cannot be expressed as_ \(B_{_{I}}_{D}\) _for any_ \(B_{_{I}}_{I}\)_, we have_ \[_{ A_{}}p_{|}( _{1})d_{ A_{}}p_{ |}(_{2})d.\]_iv. (Structural Sparsity) For all \(k\{1,,n_{I}\}\), there exists \(_{k}\) s.t. \(_{i_{k}}_{i,:n_{I}}=\{k\}\)._

_Then \(\) is identifiable up to an element-wise invertible transformation and a permutation._

With different assumptions on different sets of sources, one could view this theorem as an expansion of both previous theoretical findings that impose distributional constraints on sources with auxiliary variables (e.g., (Hyvarinen et al., 2019)) and those that constrain the mixing function with _Structural Sparsity_(Zheng et al., 2022). This is particularly helpful in the context of self-supervised learning (Von Kugelgen et al., 2021) or transfer learning (Kong et al., 2022), where latent representations are modeled as a changing part and an invariant part. In (Kong et al., 2022), the component-wise identifiability for variables changing across domains (i.e., \(_{D}\) with multiple values of \(\) in our setting) are provided but not those in the invariant part (i.e., \(_{I}\) in our setting). With the help of Thm. 4.4, we can show identifiability up to an element-wise invertible transformation and a permutation for each source, regardless of whether it changes across domains or not, which may help some related tasks where full identifiability is necessary. Furthermore, for causal reasoning or disentanglement with observational time-series data, our theorem benefits the identifiability of temporal processes involving instantaneous relations. Previous works in that area can only deal with time-delayed/changing influences, as they rely on the global conditional independence of all sources given the changing time index as the auxiliary variable (Hyvarinen and Morioka, 2016, 2017; Yao et al., 2022). Our theorem, on the other hand, provides the added ability to identify unconditional sources, thanks to the partially satisfied sparsity assumption, and thus aids the uncovering of latent processes with instantaneous relations.

## 5 Experiments

In order to validate the proposed identifibaility results, we conduct experiments using both simulated data and real-world images. It is noteworthy that there has been extensive research that has empirically verified that deep latent variable models are likely to be identifiable in complex scenarios, particularly in the disentanglement task (Kumar et al., 2017; Klys et al., 2018; Locatello et al., 2018; Rubenstein et al., 2018; Chen et al., 2018; Burgess et al., 2018; Duan et al., 2020; Falck et al., 2021; Carbonneau et al., 2022). While we are not sure of the exact inductive biases or side information that are available during the real-world application, which has been proved to be necessary (Hyvarinen and Pajunen, 1999; Locatello et al., 2019), the empirical success of these methods sheds light on the possibility of identification in the general settings considered in this work.

**Setup.** For settings with the auxiliary variable \(\), \(\) is always available during estimation and we consider the dataset as \(=\{(^{(1)},^{(1)}),, (^{(N)},^{(N)})\}\), where \(N\) is the sample size and \(^{(i)}\) is the value of \(\) (or class label) corresponding to the data point \(^{(i)}\). Given the estimated model \(\) parameterized by \(\), similar to (Sorrenson et al., 2020), we consider a regularized maximum-likelihood approach for the required sparsity regularization during estimation with the objective function as: \(()=_{(,)}[  p_{-1}(|)-]\), where \(\) is a regularization parameter and \(\) is the regularization term on the Jacobian of the estimated mixing function, i.e., \(_{}\). Based on our experimental results (Fig. 8 in Appx. B.2), we adopt the minimax concave penalty (MCP) (Zhang, 2010) as the regularization term. For settings without the auxiliary variable, we remove the access of \(\) and follow the same objective function in (Zheng et al., 2022). We train a General Incompressible-flow Network (GIN) (Sorrenson et al., 2020), which is a flow-based generative model, to maximize the objective function \(()\). Following (Sorrenson et al., 2020), where necessary, we concatenate the latent sources with independent Gaussian noises to meet the dimensionality requirements. All results are from 20 trials with random seeds. Additional details of the experimental setup are included in Appx. B.

**Ablation study.** We perform an ablation study to verify the necessity of the proposed assumptions. Specifically, we focus on the following models corresponding to different assumptions: _(USS)_ The assumption of Structural Sparsity, as well as other assumptions in the undercomplete case (Thm. 3.1), are satisfied; _(Mixed)_ The assumption of Structural Sparsity with undercompleteness and the required dependence structure among sources influenced by the auxiliary variable, as well as other assumptions in the partial sparsity and dependence case (Thm. 4.4), are satisfied; _(Base)_ The vanilla baseline in the undercomplete case, where the assumption of Structural Sparsity is not satisfied compared to _USS_. The datasets are generated according to the required assumptions, the details of which are included in Appx. B. All experiments are conducted in the undercomplete case, where the number of observed variables is twice the number of sources. For datasets that contain both sources in \(_{I}\) and \(_{D}\) (\(Mixed\) case), we set half as \(_{I}\) and the other half as \(_{D}\), and the minimum requirednumbers of required distinct values have been assigned to the auxiliary variable \(\). Following previous works (Hyvarinen and Morioka, 2016; Lachapelle et al., 2022), we use the mean correlation coefficient (MCC) between the true sources and the estimated ones as the evaluation metric.

Results for each model are summarised in Fig. 2 and Fig. 3. It can be observed that when the proposed assumptions are met (_UCSS_ and _Mixed_), our models achieve higher MCCs than _Base_. This indicates that it is indeed possible to identify sources from nonlinear mixtures up to trivial indeterminacy in the general settings with undercompleteness, partial sparsity, and partial source dependence. Additionally, we conduct experiments with different numbers of sources \(n\) to evaluate the stability of the identification. Our results show that both models consistently outperform _Base_ across all values of \(n\), further supporting the theoretical claims.

**Undercomplete Structural Sparsity.** As previously discussed, the assumption of _Structural Sparsity_ (Assumption ii in Thm. 3.1) is far more plausible in an undercomplete setting considered in our theory (i.e., the number of observed variables \(m\) is larger than the number of sources \(n\)) than in the more restrictive bijective scenario required in (Zheng et al., 2022) (i.e., the numbers are equal, \(m=n\)). Consequently, extending the identifiability with structural sparsity from a bijective to an undercomplete setting significantly broadens its applicability in real-world contexts. In order to validate the necessity of the proposed generalization empirically, we construct several experiments studying the _Structural Sparsity_ assumption in the undercomplete case. We consider different numbers of sources \(n\) with different degrees of undercompleteness (\(m/n\), where \(m\) is the number of observed variables). For each setting, we generate \(50\) random matrices where each entry is independently determined with an equal probability to be either zero or non-zero. The results of the percentages of matrices satisfying the assumption of _Structural Sparsity_ are presented in Fig. 4. We could observe that there exists a significant gap on the percentages between the cases where \(m/n=1\), i.e., the bijective setting, and the undercomplete settings where \(m/n>1\). Thus, it is clear that the assumption is much more likely to hold true when we have more observed variables than sources. Furthermore, when the degree of undercompleteness increases, the percentage of cases satisfying structural converges to 1. This further suggests that the assumption will almost always hold with a sufficient degree of undercompleteness, which is rather common in practice. For instance, a photo can easily have millions of pixels (observed variables) but only a dozen of hidden concepts (sources).

**Partial Structural Sparsity.** Moreover, as previously noted, it is not uncommon for _Structural Sparsity_ to be violated for a subset of sources. For instance, certain sources (such as high-decibel sound sources) may exert influence over all observed variables (microphones). Nonetheless, the prior study (Zheng et al., 2022) necessitates that the sparsity assumption holds true for all sources, providing no identifiability assurance in cases of any degree of violation. To confront this practical obstacle, we propose Thm. 4.1 and Thm. 4.2 to demonstrate that the remaining sources (i.e., \(n_{I}\) sources) can still be identified even when the sparsity assumption does not universally hold for some sources. These results can also be motivated empirically. For instance, from Fig. 4, one may find that _Structural Sparsity_ does not likely to hold for all sources when \(m/n=1\), which is the bijective setting considered in (Zheng et al., 2022). However, as discussed above, if a subset of sources satisfies the assumption, at least the identifiability for these sources could be guaranteed by our proposed theorems (Thm. 4.1 and Thm. 4.2) under certain conditions. To illustrate this, we conduct experiments in the bijective setting (\(m/n=1\)) and report the percentage of sources satisfying _Structural Sparsity_ in Fig. 4. We consider datasets with different number of sources and generate \(50\) random matrices for each of these. Each entry is independently determined with an equal probability to be either zero or non-zero. Combining results from both Fig. 4 and Fig. 5, we observe that, even in scenarios where _Structural Sparsity_ is rarely satisfied for all sources (\(m/n=1\) in Fig. 4), it is almost always satisfied for a significant fraction of sources (Fig. 5). Consequently, our generalization also proves helpful even within the confines of the earlier bijective setting.

Image datasets.To study how reasonable the proposed theories are w.r.t. the practical generating process of observational data in complex scenarios, we conduct experiments on "Triangles" (Yang et al., 2022) and EMNIST (Cohen et al., 2017) datasets. The "Triangles" dataset consists of \(60,000\) synthetic \(28 28\) images of triangles, which are generated from \(4\) factors: rotation, height, width, and brightness. By fixing the number of pixels (observed variables) and generating factors (sources), we can guarantee that the images are generated according to an undercomplete process, although the exact generating process is still unknown (e.g., a pixel could be (indirectly) influenced by multiple factors in a complicated way). For the real-world dataset, EMNIST contains \(240,000\)\(28 28\) images of handwritten digits and is a larger version of the classical MNIST dataset. Although we do not know the exact number of sources, it is highly possible that it is smaller than the number of pixels (\(784\)). We present the identified sources with the top four standard deviations (SDs) from both datasets in Fig. 6 and Fig. 7. In both figures, each row represents a source identified by our model, with it varying from \(-4\) to \(+4\) SDs to illustrate its influence. The rightmost column is a heat map given by the absolute pixel difference between \(-1\) and \(+1\) SDs. By observing the identified sources with the top four standard deviations, one could find that it is possible to identify semantically meaningful attributes from practical image datasets, which further suggests the potential of our theory in real-world scenarios. Additional results are available in Appx. B.2.

## 6 Conclusion

We establish a set of new identifiability results of nonlinear ICA in general settings with undercompleteness, partial sparsity and source dependence, and flexible grouping structures, thereby extending the identifiabilty theory to a wide range of real-world scenarios. Specifically, we prove the identifiability when there are more observed variables than underlying sources, and when sparsity and/or independence are not met for a subset of sources. Moreover, by leveraging various dependence structures among sources, further identifiability guarantees can also be obtained. Theoretical results have been validated through a combination of extensive previous studies and our own experiments, which involve both synthetic and real-world datasets. Future work includes adopting the theoretical framework for related tasks, such as disentanglement, transfer learning, and causal discovery. Furthermore, the proposed identifiability guarantees on generalized latent variable models bolster our confidence in uncovering hidden truths across diverse real-world settings in scientific discovery. We have only explored the visual disentanglement task, and the lack of other applications is a limitation of this work.