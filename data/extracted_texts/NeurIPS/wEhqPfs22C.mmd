# AdsGT: Graph Transformer for Predicting Global Minimum Adsorption Energy

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The fast assessment of the binding strength between adsorbates and catalyst surfaces is crucial for catalyst design, where global minimum adsorption energy (GMAE) is one of the most representative descriptors. However, catalyst surfaces typically have multiple adsorption sites and numerous possible adsorption configurations, which makes it prohibitively expensive to calculate the GMAE using Density Functional Theory (DFT). Additionally, most machine learning methods can only predict local minimum adsorption energies and rely on information about adsorption configurations. To overcome these challenges, we designed a graph transformer (AdsGT) that can predict the GMAE based on surface graphs and adsorbate feature vectors without any binding structure information. To evaluate the performance of AdsGT, three new datasets on GMAE were constructed from OC20-Dense, Catalysis Hub, and FG-dataset. For a wide range of combinations of catalyst surfaces and adsorbates, AdsGT achieves test mean absolute errors of 0.10 and 0.14 eV on the two GMAE datasets respectively, demonstrating its good reliability and generalizability.

## 1 Introduction

The adsorption energy of an adsorbate on the catalyst surface is crucial for determining the reactivity and selectivity of catalytic reactions. The highest catalytic activity of a material will be achieved at the optimal adsorption energy for a specific reaction, according to the Sabatier principle [1; 2] (Fig. 1). Therefore, developing cheap and efficient adsorption energy evaluation methods are of great significance for catalyst discovery. Currently, high-throughput screening of catalysts relies heavily on computationally expensive simulations like Density Functional Theory (DFT) [3; 4]. However, multiple adsorption sites and variable adsorbate geometries lead to numerous possible adsorption configurations and local minima on the binding energy surface [5; 6]. The local adsorption energy strongly depends on the initial structure of the simulation and cannot provide a fair evaluation of different catalysts. Several methods, including global optimization algorithms [7; 8; 9] and "brute-force" searches [10; 11], have been employed to find the most stable adsorption structures and corresponding global minimum adsorption energies (GMAE). Unfortunately, the exponential rise in computational costs renders these methods inadequate for the screening of diverse catalyst candidates.

Machine learning (ML) holds the potential to approximate DFT-level accuracy at significantly lower time costs [12; 13]. A lot of ML models, such as random forests, multilayer perceptions, and graph neural networks, have been explored to predict adsorption energy of adsorbate-surface systems [14; 15; 16; 17]. However, several drawbacks are present in most models, which (1) can only predict localminimum adsorption energies, (2) require binding information between the adsorbates and catalyst surfaces, and (3) exhibit a poor generalizability limited to specific adsorbates. Recently, Ulissi et al. proposed the AdsorbML workflow , which combines heuristic search and ML potentials to accelerate the GMAE calculation. The ML potentials trained on the huge Open Catalyst (OC)20 dataset achieve promising prediction accuracy and substantial speedups over DFT computations . Moreover, Margraf et al.  proposed a global optimization protocol that employs on-the-fly ML potentials trained on iteratively DFT calculations to search the most stable adsorption structures. This method is versatile for various combinations of surfaces and adsorbates, and significantly reduces the reliance on prior expertise and the number of required DFT calculations .

Herein, a new strategy for directly predicting GMAE without binding structure information is proposed. A novel graph transformer model, called AdsGT, was designed for the GMAE prediction based on the surface graphs and adsorbate feature vectors. Three datasets on GMAE were constructed and applied for model evaluation. AdsGT demonstrates excellent performance in predicting GMAE, with mean absolute errors (MAE) below 0.14 eV for two of the datasets and 0.51 eV on a more challenging dataset with fewer data points. A pretraining strategy was also proposed to improve AdsGT performance to a MAE of 0.43 eV. All results highlight the learning ability of AdsGT for catalytic surface chemistry and its association with adsorbates. This work makes a valuable contribution to accelerating GMAE calculations and catalyst screening.

## 2 Methods

### Datasets

The datasets for the global minimum adsorption energies in this study come from OC20-Dense , Catalysis Hub , and 'functional groups' (FG)-dataset  datasets. Each of the source datasets enumerated all adsorption sites on surfaces and performed DFT calculations on various possible adsorption configurations. The data cleaning was conducted to take the lowest adsorption energy of all conformations for each combination of catalyst surface and adsorbate as the global minimum adsorption energy. Subsequently, three new datasets, named OCD-GMAE, Alloy-GMAE and FG-GMAE, were constructed, and each data point represents a unique combination of catalyst surface and adsorbate (Table 1). Random splitting is adopted on three datasets during the model training. More challenging splits will be investigated in future work.

Figure 1: **Overview Left: The Sabatier principle describes that a catalyst should bind a substrate neither too weakly nor too strongly. Middle: Global and local minima adsorbate configurations on the catalytic surface. Right: The global minimum adsorption energy prediction task is addressed in this work without requiring adsorption configuration information.**

In addition, a similar data cleaning procedure was employed on the OC20 dataset  to create a new dataset named OC20-LMAE, which comprises surface/adsorbate pairings along with their local minimum adsorption energies (LMAE). The OC20-LMAE dataset contains 345,254 data points and serves as an effective resource for model pretraining.

### Surface graph

Each input catalyst surface is modeled as a graph \(\) consisting of \(n\) nodes (atoms) \(=\{v_{1},,v_{n}\}\) and \(m\) edges (interactions) \(=\{_{1},,_{m}\}^{2}\). \(=[_{1},_{2},,_{n}]^{T}^{n k}\) is the node feature matrix, where \(_{i}^{k}\) is the \(k\)-dimensional feature vector of atom \(i\). \(^{m k^{}}\) is the edge feature matrix, where \(_{ij}^{t}^{k^{}}\) is the \(k^{}\)-dimensional feature vector of \(t\)-th edge between node \(i\) and \(j\). \(=[_{1},_{2},, _{n}]^{T}^{n 3}\) is the position matrix, where \(_{i}^{3}\) is the 3D Cartesian coordinate of atom \(i\). For periodic boundary conditions (PBC), let the matrix \(=[,,]^{T} ^{3 3}\) depicts how the unit cell is replicated in three directions \(\), \(\) and \(\).

Periodic invarianceIgnoring periodic invariance will lead to different surface graphs and energy predictions for the same surface . Different from crystals, the presence of the vacuum layer breaks the periodicity along the direction perpendicular to the surface. This means that the catalyst surfaces actually exhibit periodicity only in the \(\) and \(\) directions. Thus, the infinite surface structure can be represented as

\[}&=\{}_{i}}_{i}=_{i},\;i ,1 i n\},\\ }&=\{}_{i} }_{i}=_{i}+k_{1}+k_{2} ,\;i,k_{1},k_{2},1 i n\}.\] (1)

To encode such periodic patterns, the infinite representation of the surface is used for graph construction, and all nodes and their repeated duplicates are considered to build edges. Given a cutoff radius \(r_{c}\), if there is any integer pair \((k_{1}^{},k_{2}^{})\), such that the Euclidean distance \(d_{ji}=\|_{j}+k_{1}^{}+k_{2}^{} -_{i}\|_{2} r_{c}\), then an edge is constructed from \(j\) to \(i\) with the initial edge feature \(d_{ji}\). It should be pointed out that self-loop edges (\(i=j\)) are also considered if there exists any integer pair \((k_{1}^{},k_{2}^{})\) other than \((0,0)\) such that \(d=\|k_{1}^{}+k_{2}^{}\|_{2} r_{c}\).

Positional featureUnlike molecular graphs, the importance of each atom in the catalyst surface is different for adsorption energy prediction (Fig. 2). For example, atoms closer to the adsorbate are more important, while atoms at the bottom are less important. Moreover, GNNs cannot determine whether the atoms are located at the interface in contact with the adsorbate based on the surface graph. They cannot distinguish between interfacial atoms and subsurface atoms. To help models understand the varying importance of different atoms, each atom \(i\) of the surface graph will get a positional feature \(_{i}\) computed by

\[_{i}=}{h_{max}-h_{min}}\] (2)

where \(h\) is the height of the atom \(i\) and calculated by the projection length of the atom coordinate \(_{i}\) on the \(\) vector. \(h_{max}\) and \(h_{min}\) represent the maximum and minimum heights of surface atoms, respectively.

  Dataset & Combination Num. & Surface Num. & Adsorbate Num. & Range of GMAE (eV) \\  OCD-GMAE & 973 & 967 (54) & 74 (4) & -8.0 \(\) 6.4 \\ Alloy-GMAE & 11,260 & 1,916 (37) & 12 (5) & -4.3 \(\) 9.1 \\ FG-GMAE & 3,308 & 14 (14) & 202 (5) & -4.0 \(\) 0.8 \\  

Table 1: Overview of three new datasets on GMAE. \(()\) values represent the numbers of element types.

Figure 2: Illustration of the varying importance of different atoms on a catalyst surface.

### Adsorbate feature

The representation of adsorbate is crucial for models to predict the lowest adsorption energy for a given combination of surface and adsorbate. Many adsorbate species, especially in the field of electrocatalysis, consist of fewer than five atoms. Some adsorbates, such as *H, *O and *NH have only one or two atoms. Therefore, molecular descriptors are used to represent adsorbates rather than the widely used molecular graphs. \(=[_{1},_{2},,_{s} ]^{T}^{s k^{}}\) is the adsorbate feature matrix, where \(_{c}^{k^{}}\) is the \(k^{}\)-dimensional feature vector of the adsorbate for the surface/adsorbate combination \(c\) (\(1 c s\)).

### Model

The proposed AdsGT model (Fig. 3) consists of three parts: a graph encoder \(E_{G}\), a vector encoder \(E_{V}\), and a readout block \(R_{o}\). Each surface/adsorbate combination \(C\), consisting of a surface graph \(_{c}=(,)\) and an adsorbate feature vector \(p_{c}\), is defined as the model input and the global minimum adsorption energy of the combination is set as the prediction target. A surface graph and an adsorbate feature vector are passed to the graph encoder \(E_{G}\) and the vector encoder \(E_{V}\) for embedding learning, respectively. Then, both embeddings are concatenated and passed to the readout block \(R_{o}\) for the prediction of global minimum adsorption energy. The details of these parts are as follows.

Graph encoderIn the initialization of \(E_{G}\), atomic number \(z_{i}\) and positional feature \(_{i}\) of node \(i\) are passed to embedding layers to compute the initial node embedding \(_{i}^{0}\). The distance \(d_{ij}^{t}\) of \(t\)-th edge between node \(i\) and \(j\) is expanded via a set of exponential normal radial basis functions (RBF) and transformed by linear layers to obtain the edge embedding \(_{ij}^{t}\). The message passing phase of \(E_{G}\) follows the regular attention mechanism [21; 22]. In the \(l\)-th (\(0 l L\)) attention layer, edge-wise attention weights \(_{ij}^{t}\) and message \(m_{ij}^{t}\) of \(t\)-th edge between node \(i\) and \(j\) are calculated based on \(_{i}^{l}\), \(_{j}^{t}\) and \(_{ij}^{t}\) according to

\[_{ij}=W_{Q}^{l}(_{i}^{l}|_ {i}^{l}|_{i}^{l}),_{ij}^{t}=W_{K} ^{l}(_{i}^{l}|_{j}^{l}| {e}_{ij}^{t}),_{ij}^{t}=W_{V}^{l}(_{i}^{l}|_{j}^{l}|_{ij}^{t})\] (3)

Figure 3: **Model architecture** of AdsGT (left) and its attention layer (right). \(+\) and \(|\) denote sum and concatenation operations, respectively. \(\) denotes the activation function, and \(\) represents batch normalization.

\[_{ij}^{t}=_{ij}_{ij}^{t}}{_{ij}^{t} }}},_{ij}^{t}=((_{ ij}^{t}))_{ij}^{t}\] (4)

where \(W_{Q}^{l}\), \(W_{K}^{l}\) and \(W_{V}^{l}\) are three learnable weight matrices, \(\) represent the Hadamard product, and \(|\) denotes concatenation. \(\) denotes the layer normalization operation. Then, the message \(m_{i}\) of node \(i\) from all neighbors \(_{i}\) is computed by

\[_{i}=_{j_{i}}_{h}(W_{m}^{l} _{ij}^{t}+b_{m}^{l})\] (5)

and the embedding of node \(i\) is updated based on the message \(m_{i}\) according to

\[_{i}^{l+1}=W_{u}^{l}_{i}^{l}+b_{u}^{l}+((_{i}))\] (6)

where \(W_{m}^{l}\) and \(W_{u}^{l}\) are two learnable weight matrices, while \(b_{m}^{l}\) and \(b_{u}^{l}\) are two learnable bias vectors. \(\) denotes the activation function, and \(\) represents batch normalization.

Vector encoderA simple multilayer perceptron (\(\)) is used to encode the feature vectors of adsorbates, and the adsorbate embedding of the combination \(C\) is calculated based on

\[_{c}^{}=(_{c})\] (7)

Readout blockFor the surface/adsorbate combination \(C\), graph-level embedding \(_{c}\) of surface \(_{c}\) is computed and concatenated with adsorbate embedding \(_{c}^{}\) to predict the GMAE based on

\[_{c}=_{i_{c}}_{i}^{L}, y=( _{c}_{c}^{})\] (8)

## 3 Results and Discussion

The prediction performance of AdsGT was evaluated on the three GMAE datasets, and the results are depicted in Table 2. These three datasets have different characteristics: (1) Alloy-GMAE has a variety of surfaces (1916) but a small number of adsorbates (12), (2) FG-GMAE has a small number of surface types (14) but a large variety of adsorbates (202), and (3) OCD-GMAE contains a variety of surfaces (967) and adsorbates (74) but a smaller amount of data. As shown in the Table 2, AdsGT achieves excellent performance with MAE less than 0.14 eV and a success rate exceeding 67 % on the Alloy-GMAE and FG-GMAE datasets, without any binding structural information. However, AdsGT exhibits worse performance with an MAE higher than 0.5 eV on the OCD-GMAE, which comprises a broader range of surface/adsorbate combinations but fewer data points. Given the small-size constraint, AdsGT is pretrained on the larger dataset OC20-LMAE and finetuned on the OCD-GMAE. It results in a lower energy MAE (0.43 eV) and a higher success rate (25.4 %) compared to the directly training AdsGT. More work on transfer learning and data augmentation will be explored in the future.

Moreover, several models with the same AdsGT architecture but different graph encoders  are explored on the OCD-GMAE dataset (Table 3). The results indicate that our designed AdsGT graph encoder surpasses all baseline graph encoders, demonstrating its good learning capability in catalytic surface chemistry. Unfortunately, larger graph encoder from GemNet-OC model fails to achieve better performance on this small dataset with diverse surfaces and adsorbates.

   & Alloy-GMAE & FG-GMAE & OCD-GMAE & OCD-GMAE \\  & (11,260) & (3,308) & (973) & (Pretrained, 973) \\  Energy MAE (eV) \(\) & 0.1388 \(\) 0.0072 & 0.1053 \(\) 0.0065 & 0.5149 \(\) 0.0545 & 0.4296 \(\) 0.0326 \\ Success rate (\%) \(\) & 67.25 \(\) 1.11 & 69.74 \(\) 2.17 & 13.47 \(\) 4.85 & 25.36 \(\) 2.12 \\  

Table 2: Test MAE and success rates of AdsGT on the three GMAE datasets. The success rate is the percentage of predicted GMAEs within 0.1 eV of the DFT-computed ground truth GMAEs. Energy MAE is also computed between predicted and ground-truth GMAEs. All results are from 5 replicate experiments with different random seeds.

## 4 Conclusion

Our work presents AdsGT, a novel graph transformer model for predicting global minimum adsorption energies of adsorbate-surface systems. AdsGT takes the combinations of surface graphs and adsorbate feature vectors as input without requiring any adsorption configuration information. On three datasets covering a wide range of surfaces and adsorbates, AdsGT demonstrates strong performance in predicting GMAE, with mean absolute errors within 0.14 eV for two of the datasets, and 0.43 eV on the more challenging dataset with fewer datapoints. The results highlight the ability of graph neural networks like AdsGT to learn meaningful representations of surface chemistry and approximate DFT adsorption energies. By rapidly predicting GMAE, AdsGT has the potential to accelerate high-throughput computational screening of novel catalysts. While AdsGT struggles on one dataset with greater diversity but fewer examples, transfer learning has been proved to be an effective measure to improve its generalizability. Overall, this work makes valuable contributions towards enabling graph ML models to guide the discovery of novel catalysts for renewable energy and industrial processes. The code and datasets will be publicly available to facilitate future research.