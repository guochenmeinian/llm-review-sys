# A Rigorous Link between Deep Ensembles and

(Variational) Bayesian Methods

Veit D. Wild

University of Oxford

Sehra Ghalebikesabi

University of Oxford

Dino Sejdinovic

University of Adelaide

Jeremias Knoblauch

University College London

 veit.wild@stats.ox.ac.uk

###### Abstract

We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this is to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lens of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning--including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on standard variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle systems in thermodynamics, and use our theory to prove the convergence of these algorithms to a well-defined global minimiser on the space of probability measures.

## 1 Introduction

A major challenge in modern deep learning is the accurate quantification of uncertainty. To develop trustworthy AI systems, it will be crucial for them to recognize their own limitations and to convey the inherent uncertainty in the predictions. Many different approaches have been suggested for this. In variational inference (VI), a prior distribution for the weights and biases in the neural network is assigned and the best approximation to the Bayes posterior is selected from a class of parameterised distributions (Graves, 2011; Blundell et al., 2015; Gal and Ghahramani, 2016; Louizos and Welling, 2017). An alternative approach is to (approximately) generate samples from the Bayes posterior via Monte Carlo methods (Welling and Teh, 2011; Neal, 2012). Deep ensembles are another approach, and rely on a train-and-repeat heuristic to quantify uncertainty (Lakshminarayanan et al., 2017).

Much ink has been spilled over whether one can see deep ensembles as a Bayesian procedure (Wilson, 2020; Izmailov et al., 2021; D'Angelo and Fortuin, 2021) and over how these seemingly different methods might relate to each other. Building on this discussion, we shed further light on the connections between Bayesian inference and deep ensemble techniques by taking a different vantage point. In particular, we show that methods as different as variational inference, Langevin sampling (Ermak, 1975), and deep ensembles can be derived from a well-studied generally infinite-dimensional regularised optimisation problem over the space of probability measures (see e.g. Guedj and Shawe-Taylor, 2019; Knoblauch et al., 2022). As a result, we find that the differences betweenthese algorithms map directly onto different choices regarding this optimisation problem. Key differences between the algorithms boil down to different choices of regularisers, and whether they implement a finite-dimensional or infinite-dimensional gradient descent. Here, finite-dimensional gradient descent corresponds to parameterised VI schemes, whilst the infinite-dimensional case maps onto ensemble methods.

The contribution of this paper is a new theory that generates insights into existing algorithms and provides links between them that are mathematically rigorous, unexpected, and useful. On a technical level, our innovation consists in analysing them as algorithms that target an optimisation problem in the space of probability measures through the use of a powerful technical device: the Wasserstein gradient flow (see e.g. Ambrosio et al., 2005). While the theory is this paper's main concern, its potentially substantial methodological payoff is demonstrated through the derivation of a new inference algorithm based on gradient descent in infinite dimensions and regularisation with the maximum mean discrepancy. We use our theory to show that this algorithm--unlike standard deep ensembles--is derived from a strictly convex objective defined over the space of probability measures. Thus, it targets a unique minimum, and is capable of producing samples from this global minimiser in the infinite particle and time horizon limit. This makes the algorithm provably convergent; and we hope that it can help plant the seeds for renewed innovations in theory-inspired algorithms for (Bayesian) deep learning.

The paper proceeds as follows: Section 2 discusses the advantages of lifting losses defined on Euclidean spaces into the space of probability measures through a generalised variational objective. Section 3 introduces the notion of Wasserstein gradient flows, while Section 4 links them to the aforementioned objective and explains how they can be used to construct algorithms that bridge Bayesian and ensemble methods. The paper concludes with Section 5, where the findings of the paper are illustrated numerically.

## 2 Convexification through probabilistic lifting

One of the most technically challenging aspects of contemporary machine learning theory is that the losses \(:^{J}\) we wish to minimise are often highly non-convex. For instance, one could wish to minimise \(():=_{n=1}^{N}y_{n}-f_{}(x_{n})^{2}\) where \(\{(x_{n},y_{n})\}_{n=1}^{N}\) is a set of paired observations and \(f_{}\) a neural network with parameters \(\). While deep learning has shown that non-convexity is often a negligible _practical_ concern, it makes it near-impossible to prove many basic _theoretical_ results that a good learning theory is concerned with, as \(\) has many local (or global) minima (Fort et al., 2019; Wilson and Izmailov, 2020). We reintroduce convexity by lifting the problem onto a computationally more challenging space. In this sense, the price we pay for the convenience of convexity is the transformation of a finite-dimensional problem into an infinite-dimensional one, which is numerically more difficult to tackle. Figure 1 illustrates our approach:

First, we transform a non-convex optimisation \(_{}()\) into an infinite-dimensional optimisation over the set of probability measures \((^{J})\), yielding \(_{Q(^{J})}()dQ()\). As an integral, this objective is linear in \(Q\). However, linear functions are not strictly convex. We therefore need to add a strictly convex regulariser to ensure uniqueness of the minimiser.2 We prove in Appendix A that indeed--for a regulariser \(D:(^{J})(^{J})[0,]\) such that \((Q,P) D(Q,P)\) is strictly convex and \(P(^{J})\) a fixed measure--existence and uniqueness of a global minimiser can be guaranteed. Given a scaling constant \(>0\), we can now put everything together to obtain

Figure 1: Illustration of convexification through probabilistic lifting.

the loss \(L\) and the unique minimiser \(Q^{*}\) as

\[L(Q):=()\,dQ()+ D(Q,P), Q^{*}:=*{ arg\,min}_{Q(^{J})}L(Q).\] (1)

Throughout, whenever \(Q\) and \(Q^{*}\) have an associated Lebesgue density, we write them as \(q\) and \(q^{*}\). Moreover, all measures, densities, and integrals will be defined on the parameter space \(^{J}\) of \(\). Similarly, the gradient operator \(\) will exclusively denote differentiation with respect to \(^{J}\).

### One objective with many interpretations

In the current paper, our sole focus lies on resolving the difficulties associated with non-convex optimisation of \(\) on Euclidean spaces. Through probabilistic lifting and convexification, we can identify a unique minimiser \(Q^{*}\) in the new space, which minimises the \(\)-averaged loss \(()\) without deviating too drastically from some reference measure \(P\). In this sense, \(Q^{*}\) summarises the quality of all (local and global) minimisers of \(()\) by assigning them a corresponding weight. The choices for \(\), \(D\) and \(P\) determine the trade-off between the initial loss \(\) and reference measure \(P\) and therefore the weights we assign to different solutions.

Yet, (1) is not a new problem form: it has various interpretations, depending on the choices for \(D,,\) and the framework of analysis (see e.g. Knoblauch et al., 2022, for a discussion). For example, if \(()\) is a negative log likelihood, \(D\) is the Kullback-Leibler divergence (\(\)), and \(=1\), \(Q^{*}\) is the **standard Bayesian** posterior, and \(P\) is the Bayesian prior. This interpretation of \(P\) as a prior carries over to **generalised Bayesian** methods, in which we can choose \(()\) to be any loss, \(D\) to be any divergence on \((^{J})\), and \(\) to regulate how fast we learn from data (see e.g. Bissiri et al., 2016; Jewson et al., 2018; Knoblauch et al., 2018; Miller and Dunson, 2019; Knoblauch, 2019; Alquier, 2021a; Husain and Knoblauch, 2022; Matsubara et al., 2022; Wild et al., 2022; Wu and Martin, 2023; Altamirano et al., 2023). In essence, the core justification for these generalisations is that the very assumptions justifying application of Bayes' Rule are violated in modern machine learning. In practical terms, this results in a view of Bayes' posteriors as one--of many possible--measure-valued estimators \(Q^{*}\) of the form in (1). Once this vantage point is taken, it is not clear why one _should_ be limited to using only one particular type of loss and regulariser for _every_ possible problem. Seeking a parallel with optimisation on Euclidean domains, one may then compare the orthodox Bayesian view with the insistence on only using quadratic regularisation for _any_ problem. While it is beyond the scope of this paper to cover these arguments in depth, we refer the interested reader to Knoblauch et al. (2022).

A second line of research featuring objectives as in (1) are **PAC-Bayes** methods, whose aim is to construct generalisation bounds (see e.g. Shawe-Taylor and Williamson, 1997; McAllester, 1999, 1999, 2011). Here, \(\) is a general loss, but \(P\) only has the interpretation of some reference measure that helps us measure the complexity of our hypotheses via \(Q D(Q,P)\)(Guedj and Shawe-Taylor, 2019; Alquier, 2021b). Classic PAC-Bayesian bounds set \(D\) to be \(\), but there has been a recent push for different complexity measures (Alquier and Guedj, 2018; Begin et al., 2016; Haddouche and Guedj, 2023).

### Generalised variational inference (GVI) in finite and infinite dimensions

In line with the terminology coined in Knoblauch et al. (2022), we refer to any algorithm aimed at solving (1) as a **generalised variational inference (GVI)** method. Broadly speaking, there are two ways one could design such algorithms: in finite or infinite dimensions. **Finite-dimensional GVI:** This is the original approach advocated for in Knoblauch et al. (2022): instead of trying to compute \(Q^{*}\), approximate it by solving \(Q_{^{*}}=*{arg\,min}_{Q_{}}L(Q)\) for a set of measures \(:=\{Q_{}:\}(^{J})\) parameterised by a parameter \(^{I}\). To find \(_{}\), one now simply performs (finite-dimensional) gradient descent with respect to the function \( L(Q_{})\). For the special case where \(P\) is a Bayesian prior, \(=1\), \(()\) is a negative log likelihood parametrised by \(\), and \(D=\), this recovers the well-known standard VI algorithm. To the best of our knowledge, all methods that refer to themselves as VI or GVI in the context of deep learning are based on this approach (see e.g. Graves, 2011; Blundell et al., 2015; Louizos and Welling, 2017; Wild et al., 2022). Since procedures of this type solve a finite-dimensional version of (1), we refer to them as **finite-dimensional GVI (FD-GVI)** methods throughout the paper. While such algorithms can perform well, they have some obvious theoretical problems: First of all, the finite-dimensional approach typically forces us to choose \(\) and \(P\) to be simple distributions such as Gaussians to ensure that \(L(Q_{})\) is a tractable function of \(\). This often results in a \(\) that is unlikely to contain a good approximation to \(Q^{*}\); raising doubt if \(Q_{^{*}}\) can approximate \(Q^{*}\) in any meaningful sense. Secondly, even if \(Q L(Q)\) is strictly convex on \((^{J})\), the parameterised objective \( L(Q_{})\) is usually not. Hence, there is no guarantee that gradient descent leads us to \(Q_{^{*}}\). This point also applies to very expressive variational families (Rezende and Mohamed, 2015; Mescheder et al., 2017) which may be sufficiently rich that \(Q^{*}\), but whose optimisation problem \( L(Q_{})\) is typically non-convex and hard to solve, so that no guarantee for finding \(Q^{*}\) can be provided. While this does not necessarily make FD-GVI impractical, it does make it exceedingly difficult to provide a rigorous theoretical analysis outside of narrowly defined settings.

**FD-GVI in function space:** A collection of approaches formulated as infinite-dimensional problems are GVI methods on an infinite-dimensional function space (Ma et al., 2019; Sun et al., 2018; Ma and Hernandez-Lobato, 2021; Rodriguez-Santana et al., 2022; Wild et al., 2022). Here, the loss is often convex in function space. In practice however, the variational stochastic process still requires parameterization to be computationally feasible--and in this sense, function space methods are FD-GVI approaches. The resulting objectives require a good approximation of the functional KL-divergence (which is often challenging), and lead to a typically highly non-convex variational optimization problem in the parameterised space.

**Infinite-dimensional GVI:** Instead of minimising the (non-convex) problem \( L(Q_{})\), we want to exploit the convex structure of \(Q L(Q)\). Of course, a priori it is not even clear how to compute the gradient for a function \(Q L(Q)\) defined on an infinite-dimensional nonlinear space such as \((^{J})\). However, in the next part of this paper we will discuss that it is possible to implement a gradient descent in infinite dimensions by using **gradient flows** on a metric space of probability measures (Amprosoio et al., 2005). More specifically, one can solve the optimisation problem (1) by following the curve of steepest descent in the 2-Wasserstein space. As it turns out, this approach is not only theoretically sound, but also conceptually elegant: it unifies existing algorithms for uncertainty quantification in deep learning, and even allows us to derive new ones. We refer to algorithms based on some form of infinite-dimensional gradient descent as **infinite-dimensional GVI (ID-GVI)**. Infinite-dimensional gradient descent methods have recently gained attention in the machine learning community. For existing methods of this kind, the goal is to generate samples from a target \(\) that has a _known_ form (such as the Bayes posterior) by applying a gradient flow to \(Q(^{J}) E(Q,)\) where \(E(,)\) is a discrepancy measure. Some methods apply the Wasserstein gradient flow (WGF) for different choices of \(E\)(Arbel et al., 2019; Korba et al., 2021; Glaser et al., 2021), whilst other methods like Stein variational gradient descent (SVGD) (Liu and Wang, 2016) stay within the Bayesian paradigm (\(E=\), \(\) = Bayes posterior) but use a gradient flow other than the WGF (Liu, 2017). D'Angelo and Fortuin (2021) exploit the WGF in the standard Bayesian context and combine it with different gradient estimators (Li and Turner, 2017; Shi et al., 2018) to obtain repulsive deep ensembling schemes. Note that this is different from the repulsive approach we introduce later in this paper: Our repulsion term is the consequence of a regulariser, not a gradient estimator. Since our focus is on tackling the problems associated with non-convex optimisation in Euclidean space, the approach we propose is inherently different from all of these existing methods: our target \(Q^{*}\) is only implicitly defined via (1), and not known explicitly.

## 3 Gradient flows in finite and infinite dimensions

Before we can realise our ambition to solve (1) with an ID-GVI scheme, we need to cover the relevant bases. To this end, we will discuss gradient flows in finite and infinite dimensions, and explain how they can be used to construct infinite-dimensional gradient descent schemes. In essence, a gradient flow is the limit of a gradient descent whose step size goes to zero. While the current section introduces this idea for the finite-dimensional case for ease of exposition, its use in constructing algorithms within the current paper will be for the infinite-dimensional case.

Gradient descent finds local minima of losses \(:^{J}\) by iteratively improving an initial guess \(_{0}^{J}\) through the update \(_{k+1}:=_{k}-(_{k})\), \(k\), where \(>0\) is a step-size and \(\) denotes the gradient of \(\). For sufficiently small \(>0\), this update can equivalently be written as

\[_{k+1}=*{arg\,min}_{^{J}}( )+\|-_{k}\|_{2}^{2}}.\] (2)Gradient flows formalise the following logic: for any fixed \(\), we can continuously interpolate the corresponding gradient descent iterates \(_{k}}_{k_{0}}\). To do this, we simply define a function \(^{}:[0,)\) as \(^{}(t):=_{t/}\) for \(t_{0}:=\{0,,2,...\}\). For \(t_{0}\) we linearly interpolate3. As \( 0\), the function \(^{}\) converges to a differentiable function \(_{*}:[0,)\) called the gradient flow of \(\), because it is characterised as solution to the ordinary differential equation (ODE) \(^{}_{*}(t)=-(_{*}(t))\) with initial condition \(_{*}(0)=_{0}\). Intuitively, \(_{*}(t)\) is a continuous-time version of discrete-time gradient descent; and navigates through the loss landscape so that at time \(t\), an infinitesimally small step in the direction of steepest descent is taken. Put differently: gradient descent is nothing but an Euler discretisation of the gradient flow ODE (see also Santambrogio, 2017). The result is that for mathematical convenience, one often analyses discrete-time gradient descent as though it were a continuous gradient flow--with the hope that for sufficiently small \(\), the behaviour of both will essentially be the same.

Our results in the infinite-dimensional case follow this principle: we propose an algorithm based on discretisation, but use continuous gradient flows to guide the analysis. To this end, the next section generalises gradient flows to the nonlinear infinite-dimensional setting.

### Gradient flows in Wasserstein spaces

Let \(_{2}(^{J})\) be the space of probability measures with finite second moment equipped with the 2-Wasserstein metric given as

\[W_{2}(P,Q)^{2}=||-^{}||_{2}^{2}\,d( ,^{}):(P,Q)}\]

where \((P,Q)(^{J}^{J})\) denotes the set of all probability measure on \(^{J}^{J}\) such that \((A^{J})=P(A)\) and \((^{J} B)=Q(B)\) for all \(A,B^{J}\) (see also Chapter 6 of Villani et al., 2009). Further, let \(L:_{2}(^{J})(-,]\) be some functional--for example \(L\) in (1). In direct analogy to (2), we can improve upon an initial guess \(Q_{0}_{2}(^{J})\) by iteratively solving

\[Q_{k+1}:=*{arg\,min}_{Q_{2}(^{J})} L(Q)+W_{2}(Q,Q_{k})^{2}}\]

for \(k_{0}\) and small \(>0\) (see Chapter 2 of Ambrosio et al., 2005, for details). Again, for \( 0\), an appropriate limit yields a continuously indexed family of measures \(\{Q(t)\}_{t 0}\). If \(L\) is sufficiently smooth and \(Q_{0}=Q(0)\) has Lebesgue density \(q_{0}\), the time evolution for the corresponding pdfs \(\{q(t)\}_{t 0}\) is given by the partial differential equation (PDE)

\[_{t}q(t,)=q(t,)\,_{W}LQ(t) (),\] (3)

with \(q(0,)=q_{0}\)(Villani, 2003, Section 9.1). Here \( f:=_{j=1}^{J}_{j}f_{j}\) denotes the divergence operator and \(_{W}L[Q]:^{J}^{J}\) the Wasserstein gradient (WG) of \(L\) at \(Q\). For the purpose of this paper, it is sufficient to think of the WG as a gradient of the first variation; i.e. \(_{W}L[Q]= L^{}[Q]\) where \(L^{}[Q]:^{J}^{J}\) is the first variation of \(L\) at \(Q\)(Villani et al., 2009, Exercise 15.10). The Wasserstein gradient flow (WGF) for \(L\) is then the solution \(q^{}\) to the PDE (3). If \(L\) is chosen as in (1), our hope is that the logic of finite-dimensional gradient descent carries over; and that \(_{t}q^{}(t,)\) is in fact the density \(q^{*}\) corresponding to \(Q^{*}\).

Following this reasoning, this paper applies the WGF for (1) to obtain an ID-GVI algorithm. In Section 4 and Appendices C-F, we formally show that the WGF indeed yields \(Q^{*}\) (in the limit as \(t\)) for a number of regularisers of practical interest.

### Realising the Wasserstein gradient flow

In theory, the PDE in (3)  could be solved numerically in order to implement the  infinite-dimensional gradient descent for (1). In practice however, this is impossible: numerical solutions to PDEs become computationally infeasible for the high-dimensional parameter spaces which are common in deep learning applications. Rather than trying to first approximate the \(q\) solving (3) and then sampling from its limit in a second step, we will instead formulate equations which replicate how the samples from the solution to (3) evolve in time. This leads to tractable inference algorithms that can be implemented in high dimensions.

Given the goal of producing samples directly, we focus on a particular form of loss that is well-studied in the context of thermodynamics (Santambrogio, 2015, Chapter 7), and which recovers various forms of the GVI problem in (1) (see Section 4). In thermodynamics, \(Q_{2}(^{J})\) describes the distribution of particles located at specific points in \(^{J}\). The overall energy of a collection of particles sampled from \(Q\) is decomposed into three parts: (i) the external potential \(V()\) which acts on each particle individually, (ii) the interaction energy \((,^{})\) describing pairwise interactions between particles, and (iii) an overall entropy of the system measuring how concentrated the distribution \(Q\) is. Taking these components together, we obtain the so called **free energy**

\[L^{}(Q):= V()\,dQ()+}{2}( ,^{})\,dQ()dQ(^{})+_{2} q( )q()\,d,\] (4)

for \(Q_{2}(^{J})\) with Lebesgue density \(q\), \(_{1} 0\), \(_{2} 0\). Note that for \(_{2}>0\) we implicitly assume that \(Q\) has a density. Following Section 9.1 in Villani et al. (2009), its WG is

\[_{W}L^{}[Q]()= V()+_{1}(_{1} )(,^{})\,dQ(^{})+_{2} q( ),\]

where \(^{J}\), and \(_{1}\) denotes the gradient of \(\) with respect to the first variable. We plug this into (3) to obtain the desired density evolution. Importantly, this time evolution has the exact form of a nonlinear Fokker-Planck equation associated with a stochastic process of McKean-Vlasov type (see Appendix B for details). Fortunately for us, it is well-known that such processes can be approximated through interacting particles (Veretennikov, 2006) generated by the following procedure:

**Step 1:**: Sample \(N_{E}\) particles \(_{1}(0),,_{N_{E}}(0)\) independently from \(Q_{0}_{2}(^{J})\).
**Step 2:**: Evolve the particle \(_{n}\) by following the stochastic differential equation (SDE)

\[d_{n}(t)=- V_{n}(t)+} {N_{E}}_{j=1}^{N_{E}}(_{1})_{n}(t),_{j}(t )dt+}dB_{n}(t),\] (5)

for \(n=1,,N_{E}\), and \(\{B_{n}(t)\}_{t>0}\) stochastically independent Brownian motions.

As \(N_{E}\), the distribution of \(_{1}(t),,_{N_{E}}(t)\) evolves in \(t\) in the same way as the sequence of densities \(q(t,)\) solving (3). This means that we can implement infinite-dimensional gradient descent by following the WGF and simulating trajectories for infinitely many interacting particles according to the above procedure. In practice, we can only simulate finitely many trajectories over a finite time horizon. This produces samples \(_{1}(T),,_{N_{E}}(T)\) for \(N_{E}\) and \(T>0\). Our intuition and Section 4 tell us that, as desired, the distribution of \(_{1}(T),,_{N_{E}}(T)\) will be close to the global minimiser of \(L^{}\).

In the next section, we will use the above algorithm to construct an ID-GVI method producing samples approximately distributed according to \(Q^{*}\) defined in (1). Since \(N_{E}\) and \(T\) are finite, and since we need to discretise (131), there will be an approximation error. Given this, how good are the samples produced by such methods? As we shall demonstrate in Section 5, the approximation errors are small, and certainly should be expected to be much smaller than those of standard VI and other FD-GVI methods.

## 4 Optimisation in the space of probability measures

With the WGF on thermodynamic objectives in place, we can now finally show how it yields ID-GVI algorithms to solve (1). We put particular focus on the analysis of the regulariser \(D\); providing new perspectives on heuristics for uncertainty quantification in deep learning in the process. Specifically, we establish formal links explaining how they may (not) be understood as a Bayesian procedure. Beyond that, we derive the WGF associated with regularisation using the maximum mean discrepancy, and provide a theoretical analysis of its convergence properties.

### Unregularised probabilistic lifting: Deep ensembles

We start the analysis with the base case of an unregularised functional \(L(Q)=()dQ()\), corresponding to \(=0\) in (1). This is also a special case of (4) with \(_{1}=_{2}=0\). As \(_{1}=0\), there is no interaction term, and all particles can be simulated independently from one another as

\[_{1}(0),,_{N_{E}}(0) Q_{0},\;\;\;_{n}^{}(t)=- _{n}(t), n=1,,N_{E}.\]

This simple algorithm happens to coincide exactly with how deep ensembles (DEs) are constructed (see e.g. Lakshminarayanan et al., 2017). In other words: the simple heuristic of running gradient descent algorithm several times with random initialisations sampled from \(Q_{0}\) is an approximation of the WGF for the _unregularised_ probabilistic lifting of the loss function \(\).

Following the WGF in this case does not generally produce samples from a global minimiser of \(L\). Indeed, the fact that \(L\) generally does not even have a unique global minimiser was the motivation for regularisation in (1). Even if \(L\) had a unique minimiser however, a DE would not find it. The result below proves this formally: unsurprisingly, deep ensembles simply sample the local minima of \(\) with a probability that depends on the domain of attraction and the initialisation distribution \(Q_{0}\).

**Theorem 1**.: _If \(\) has countably many local minima \(\{m_{i}:i\}\), then it holds independently for each \(n=1,,N_{E}\) that_

\[_{n}(t)}}{{}}_{i= 1}^{}Q_{0}(_{i})\,_{m_{i}}=:Q_{}\]

_for \(t\). Here \(}}{{}}\) denotes convergence in distribution and \(_{i}=\{^{J}:_{t}_{*}(t)=m_{i}\) and \(_{*}(0)=\}\) denotes the domain of attraction for \(m_{i}\) with respect to the gradient flow \(_{*}\)._

A proof with technical assumptions--most importantly a version of the famous Lojasiewicz inequality--is in Appendix C. Theorem 1 derives the limiting distribution for \(_{1}(T),,_{N_{E}}(T)\), which shows that--unless all local minima are global minima--the WGF does not generate samples from a global minimum of \(Q L(Q)\) for the unregularised case \(=0\). Note that the conditions of this result simplify the situation encountered in deep learning, where the set of minimisers would typically be uncountable (Liu et al., 2022). While one could derive a very similar result for the case of uncountable minimisers, this becomes notationally cumbersome and would obscure the main point of the Theorem--that \(Q_{}\) strongly depends on the initialisation \(Q_{0}\). Importantly, the dependence of \(Q_{}\) on \(Q_{0}\) remains true for all losses constructed via deep learning architectures. However, despite these theoretical shortcomings, DEs remain highly competitive in practice and typically beat FD-GVI methods like standard VI (Ovadia et al., 2019; Fort et al., 2019). This is perhaps not surprising: DEs implement an infinite-dimensional gradient descent, while FD-GVI methods are parametrically constrained. Perhaps more surprisingly, we observe in Section 5 that DEs can even easily compete with the more theoretically sound and regularised ID-GVI methods that will be discussed in Section 4.2 and 4.3. We study this phenomenon in Section 5, and find that it is a consequence of the fact that in deep learning, \(N_{E}\) is small compared to the number of local minima (cf. Figure 4).

### Regularisation with the Kullback-Leibler divergence: Deep Langevin ensembles

In Section 2, we argued for regularisation by \(D\) to ensure a unique minimiser \(Q^{*}\). The Kullback-Leibler divergence (\(D=\)) is the canonical choice for (generalised) Bayesian and PAC-Bayesian methods (Bissiri et al., 2016; Knoblauch et al., 2022; Guedj and Shawe-Taylor, 2019; Alquier, 2021b). Now, \(Q^{*}\) has a known form: if \(P\) has a pdf \(p\), it has an associated density given by \(q^{*}()(-())p()\)(Knoblauch et al., 2022, Theorem 1).

Notice that the \(\)-regularised version of \(L\) in (1) can be rewritten in terms of the objective \(L^{}\) in (4) by setting \(V()=()- p()\), \(_{1}=0\) and \(_{2}=\). Compared to the unregularised objective of the previous section (where \(=0\)), the external potential is now adjusted by \(- p()\), forcing \(Q^{*}\) to allocate more mass in regions where \(p\) has high density. Beyond that, the presence of the negative entropy term has three effects: it ensures that the objective is strictly convex, that \(Q^{*}\) is more spread out, and that it has a density \(q^{*}\). Since \(_{1}=0\), the corresponding particle method still does not have an interaction and is given as

\[_{1}(0),,_{N_{E}}(0) Q_{0} d_{n}(t)=- V _{n}(t)dt+dB_{n}(t), n=1,,N_{E}.\] (6)Clearly, this is just the Langevin SDE and we call this approach the **deep Langevin ensemble (DLE)**. While the name may suggest that DLE is equivalent to the unadjusted Langevin algorithm (ULA) (Roberts and Tweedie, 1996), this is not so: for \(T\) discretisation steps \(t_{1},t_{2}, t_{T}\), DLE approximates measures using the _end-points_ of \(N_{E}\) trajectories given by \(\{_{n}(t_{T})\}_{n=1}^{N_{E}}\). In contrast, ULA would use a (sub)set of the samples \(\{_{1}(t_{i})\}_{i=1}^{T}\) generated from one single particle's _trajectory_. To analyse DLEs, we build on the Langevin dynamics literature: in Appendix D, we show that \(_{n}(t)}}{{}}Q^{*}\) as \(t\), independently for each \(n=1,,N_{E}\). Hence \(_{1}(T),,_{N_{E}}(T)\) will for large \(T>0\) be approximately distributed according to \(Q^{*}\). Comparing DE and DLE in this light, we note several important key differences: \(Q^{*}\) as defined per (1) is unique, has the form of a Gibbs measure, and can be sampled from using (6). In contrast, unregularised DE produces samples from \(Q_{}\) in Theorem 1 which is not the global minimiser. Specifically neither \(Q_{}\) nor \(Q^{*}\) for DEs correspond to the Bayes posterior. It is therefore not a Bayesian procedure in any commonly accepted sense of the word.

### Regularisation with maximum mean discrepancy: Deep repulsive Langevin ensembles

Regularising with \(\) is attractive because \(Q^{*}\) has a known form. However, in our theory, there is no reason to restrict attention to a single type of regulariser: we introduced \(D\) to convexify our objective. It is therefore of theoretical and practical interest to see which algorithmic effects are induced by other regularisers. We illustrate this by first considering regularisation using the squared maximum-mean discrepancy (MMD) (see e.g. Gretton et al., 2012) only, and then a combination of MMD and KL.

For a kernel \(:^{J}^{J}\), the squared MMD between measures \(Q\) and \(P\) is

\[(Q,P)^{2} =(,^{})dQ()dQ(^{} )-2(,^{})dQ()dP(^{})\] \[+(,^{})dP()dP(^{}).\]

\(\) measures the difference between within-sample similarity and across-sample similarity, so it is smaller when samples from \(P\) are similar to samples from \(Q\), but also larger when samples within \(Q\) are similar to each other. This means that regularising (1) with \(D=^{2}\) introduces interactions characterised precisely by the kernel \(\), and we can show this explicitly by rewriting \(L\) of (1) into the form of \(L^{}\) in (4). In other words, inclusion of \(^{2}\) makes particles repel each other, making it more likely that they fall into different (rather than the same) local minima. Writing the kernel mean embedding as \(_{P}():=(,^{})\,dP(^{})\), we see that up to a constant not depending on \(Q\), \(L(Q)=L^{}(Q)\) for \(V()=()-_{1}_{P}()\), \(=}{2}\), and \(_{2}=0\). While we can show that a global minimiser \(Q^{*}\) exists, and while we could produce particles using the algorithm of Section 3.2, we cannot guarantee that they are distributed according to \(Q^{*}\) (see Appendix F). Essentially, this is because in certain situations, we cannot guarantee that \(Q^{*}\) has a density for \(D=^{2}\).

To remedy this problem, we additionally regularise with the \(\): since \((Q,P)=\) if \(P\) has a Lebesgue density but \(Q\) has not, this now guarantees that \(Q^{*}\) has a density \(q^{*}\). In terms of (1), this means that \(D=\,^{2}+^{}\,\). Adding regularisers like this has a long tradition, and is usually done to combine the different strengths of various regularisers (see e.g. Zou and Hastie, 2005). Here, we follow this logic: the \(\) ensures that \(Q^{*}\) has a density, and the \(\) makes particles repel each other. With this, we can rewrite \(L(Q)\) in terms of \(L^{}(Q)\) up to a constant not depending on \(Q\) by taking \(=}{2}\), \(^{}=_{2}\), and \(V()=()-_{1}_{P}()-_{2} p()\). Using the same algorithmic blueprint as before, we evolve particles according to (5). As these particles follow an augmented Langevin SDE that incorporates repulsive particle interactions via \(\), we call this method the **deep repulsive Langevin ensemble (DRLE)**. We show in Theorem (2) (cf. Appendix E for details and assumptions) that DRLEs generate samples from the global minimiser \(Q^{*}\) in the infinite particle and infinite time horizon limit.

**Theorem 2**.: _Let \(Q^{n,N_{E}}(T)\) be the distribution of \(_{n}(T)\), \(n=1,,N_{E}\), generated via (5). Then \(Q^{n,N_{E}}(T)}}{{}}Q^{*}\) for each \(n=1,,N_{E}\) and as \(N_{E}\), \(T\)._

This is remarkable: we have constructed an algorithm that generates samples from the global minimiser \(Q^{*}\)--even though a formal expression for what exactly \(Q^{*}\) looks like is unknown! Thisdemonstrates how impressively powerful the WGF is as tool to derive inference algorithms. Note that this is completely different from sampling methods employed for Bayesian methods, for which the form of \(Q^{*}\) is typically known explicitly up to a proportionality constant.

A notable shortcoming of Theorem 2 is its asymptotic nature. A more refined analysis could quantify how fast the convergence happens in terms of \(N_{E}\), \(T\), the SDE's discretisation error, and maybe even the estimation errors due to sub-sampling of losses for constructing gradients. While the existing literature could be adapted to derive the speed of convergence for DRLE in \(T\)(Ambrosio et al., 2005, Section 11.2), this would require a strong convexity assumption on the potential \(V\), which will not be satisfied for any applications in deep learning. This is perhaps unsurprising: even for the Langevin algorithm--probably the most thoroughly analysed algorithm in this literature--no convergence rates have been derived that are applicable to the highly multi-modal target measures encountered in Bayesian deep learning (Wibisono, 2019; Chewi et al., 2022). That being said, for the case of deep learning, FD-GVI approaches fail to provide even the most basic asymptotic convergence guarantees. Thus, the fact that it is even possible for us to provide _any_ asymptotic guarantees derived from realistic assumptions marks a significant improvement over the available theory for FD-GVI methods, and--by virtue of Theorem 1--over DEs as well.

## 5 Experiments

Since the paper's primary focus is on theory, we use two experiments to reinforce some of the predictions it makes in previous sections, and a third experiment that shows why-in direct contradiction to a naive interpretation of the presented theory-it is typically difficult to beat simple DEs. More details about the conducted experiments can be found in Appendix G. The code is available on https://github.com/sghalebikesabi/GVI-WGF.

**Global minimisers:** Figure 2 illustrates the theory of Sections 4 and Appendices C-F: DLE and DRLE produce samples from their respective global minimisers, while DE produces a distribution which--in accordance with Theorem 1--does not correspond to the global minimiser of \(Q()\,dQ()\) over \((^{J})\) (which is given as Dirac measure located at \(=-2\)).

**FD-GVI vs ID-GVI:** Figure 3 illustrates two aspects. First, the effect of regularisation for DLE and DRLE is that particles spread out around the local minima. In comparison, DE particles fall directly into the local minima. Second, FD-GVI (with Gaussian parametric family) leads to qualitatively poorer approximations of \(Q^{*}\). This is because the ID-GVI methods explore the whole space \(_{2}(^{J})\), whilst FD-GVI is limited to learning a unimodal Gaussian.

**DEs vs D(R)LEs, and why finite \(N_{E}\) matters:** Table 1 compares DE, DLE and DRLE on a number of real world data sets, and finds a rather random distribution of which method performs best. This seems to contradict our theory, and suggests there is essentially no difference between regularised and unregularised ID-GVI. What explains the discrepancy? Essentially, it is the fact that \(N_{E}\) is not only finite, but much smaller than the number of minima found in the loss landscape of deep learning. In this setting, each particle moves into the neighbourhood of a well-separated single local minimum and typically never escapes, even for very large \(T\). We illustrate this in Figure 4 with a toy example. We choose a uniform prior \(P\) and initialisation \(Q_{0}\) and the loss \(():=-|()|\), \([-1000,1000]\), which has \(2000\) local minima. Correspondingly \(Q^{*}\) will have many local modes for all methods. Note that \( V\) is the same for all approaches since \( p\) and \(_{P}\) are constant. The difference between the methods boils down to repulsive and noise effects. However, these noise effects are not significant if each particle is stuck in a single mode: the particles will bounce around

Figure 2: We generate \(N_{E}=300\) particles from DE, DLE and DRLE. The theoretically optimal global minimisers \(Q^{*}\) are depicted with dashed line strokes, and the generated samples are displayed via histograms. We use \(P=Q_{0}=(0,1)\) for DLE and DRLE. Notice that the optimal \(Q^{*}\) differs slightly between DLE and DRLE.

their local modes, but not explore other parts of the space. This implies that they will not improve the approximation quality of \(Q^{*}\). Note that this problem is a direct parallel to multi-modality--a well-known problem for Markov Chain Monte Carlo methods (see e.g. Syed et al., 2022).

## 6 Conclusion

In this paper, we used infinite-dimensional gradient descent via Wasserstein gradient flows (WGFs) (see e.g. Ambrosio et al., 2005) and the lens of generalised variational inference (GVI) (Knoblauch et al., 2022) to unify a collection of existing algorithms under a common conceptual roof. Arguably, this reveals the WGF to be a powerful tool to analyse ensemble methods in deep learning and beyond. Our exposition offers a fresh perspective on these methodologies, and plants the seeds for new ensemble algorithms inspired by our theory. We illustrated this by deriving a new algorithm that includes a repulsion term, and use our theory to prove that ensembles produced by the algorithm converge to a global minimum. A number of experiments showed that the theory developed in the current paper is useful, and showed why the performance difference between simple deep ensembles and more intricate schemes may not be numerically discernible for loss landscapes with many local minima.

    & KIN8NM & CONCREITE & ENERGY & NAVAL & POWER & PROTEIN & WINE & YACHT \\  DE & \(}\) & \(6.10_{ 0.3}\) & \(2.83_{ 0.2}\) & \(-0.40_{ 0.3}\) & \(}\) & \(}\) & \(14.65_{ 1.9}\) & \(2.20_{ 0.4}\) \\ DLE & \(13.25_{ 4.3}\) & \(}\) & \(}\) & \(3.46_{ 2.4}\) & \(13.87_{ 2.3}\) & \(43.20_{ 12.5}\) & \(13.73_{ 1.4}\) & \(}\) \\ DRLE & \(0.46_{ 0.1}\) & \(8.30_{ 0.6}\) & \(4.01_{ 0.3}\) & \(}\) & \(23.21_{ 2.0}\) & \(48.80_{ 2.1}\) & \(}\) & \(7.80_{ 2.7}\) \\   

Table 1: Table compares the average (Gaussian) negative log likelihood in the test set for the three ID-GVI methods on some UCI-regression data sets (Lichman, 2013). We observe that no method consistently outperforms any of the others.

Figure 4: We generate \(N_{E}=20\) samples from the three infinite-dimensional gradient descent procedures discussed in Section 4. The \(x\)-axis shows the location of the particles after training. Since the same initialisation \(_{n}(0)\) is chosen for all methods, we observe that particles fall into the same local modes. Further, 16/20 particles are alone in their respective local modes and the location of the particles varies only very little between the different methods (which is why they are in the same bucket in the above histogram). See also Figure 5 in the Appendix for an alteration of Figure 3 with only 4 particles which emphasizes the same point.

Figure 3: We generate \(N_{E}=300\) particles from DE, DLE, DRLE and FD-GVI with Gaussian parametrisation. The multimodal loss \(\) is plotted in grey and the particles of the different methods are layered on top. The prior in this example is flat, i.e. \( p\) and \(_{P}\) are constant. The initialisation \(Q_{0}\) is standard Gaussian.