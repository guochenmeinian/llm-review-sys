# Prompt-Agnostic Adversarial Perturbation for

Customized Diffusion Models

 Cong Wan

Xi'an Jiaotong University

wancong@stu.xjtu.edu.cn

&Yuhang He

Xi'an Jiaotong University

heyuhang@xjtu.edu.cn

&Xiang Song

Xi'an Jiaotong University

songxiang@stu.xjtu.edu.cn

&Yihong Gong

Xi'an Jiaotong University

ygong@mail.xjtu.edu.cn

Corresponding author

###### Abstract

Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using "prompt-specific methods" to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts. In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution. This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability. Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of PAP in comparison to existing techniques. Our code will be available at https://github.com/vancyland/PAP.

## 1 Introduction

Generative methods based on diffusion models [1; 2; 3; 4] have made significant improvements in recent years, enabling high quality text-to-image synthesis [5; 6], image editing , video generation [8; 9], and text-to-3D conversion  by _prompt engineering_. One of the most representative methods in this field is the Stable Diffusion [11; 12], which is a large-scale text-to-image model. By incorporating customized techniques such as Text Inversion  and DreamBooth , Stable Diffusion only requires fine-tuning on a few images to accurately generate highly realistic and high-quality images based on the input prompts.

Despite this promising progress, the abuse of these powerful generative methods with wicked exploitation raises wide concerns , especially in portrait tampering [16; 17; 18] and copyright infringement . For example, in Figure 1(a), given several photos of a person, attackers can utilize diffusion models to generate fake images containing personal information, leading to reputation defamation. Even worse, attackers can easily plagiarize unauthorized artworks using diffusion models, leading to copyright and profit issues. There is an urgent technology need to protect images from diffusion model tampering.

To this end, recent research studies delve into adding human-invisible adversarial perturbations onto the images to prevent prompt-based tampering using diffusion models. The method Photoguard  maximizes the distance in the VAE latent space. Glaze  aims to hinder specific style mimicry, while Anti-DreamBooth introduces an alternating training approach between the model and adversarial examples. The AdvDM series [23; 24], Adv-Diff  present theoretical frameworks and improved methods for attacking LDM.

These methods follow a prompt-specific paradigm, wherein they need to pre-define and enumerate the possible prompts in training, and the test prompts are required to be identical to the training ones. However, in real-world applications, once encountered with an unseen test prompt, their perturbations are inevitably futile. As shown in Figure 1(b), the perturbation trained with Prompt A fails to protect the portrait on unseen Prompts B and C at inference.

To meet this challenge, we propose a novel Prompt-Agnostic Adversarial Perturbation (PAP). Different from existing prompt-specific methods that require pre-defining and enumerating the attackers' prompts, PAP models the prompt distribution and generates prompt-agnostic perturbations by maximizing a disturbance expectation based on the prompt distribution. Specifically, using a Laplace approximation, we derive the prompts distribution in a text-image embedding feature space satisfying a Gaussian distribution, whose mean and variance can be estimated by the second-order Taylor expansion and Hessian approximation with an input reference prompt. Then, by sampling on the Gaussian prompt distribution and maximizing a disturbance expectation, we generate the PAP perturbations for the input images. As shown in Figure 1(c), the PAP perturbation is trained using a Ref Prompt, while is robust to unseen Prompts B and C at inference. To verify the efficiency of our proposed method, we conduct comprehensive experiments on three widely used benchmark datasets, including VGGFace2, Celeb-HQ and Wikiart. The experimental results show that the proposed PAP method 1) steadily and significantly outperforms existing prompt-specific perturbation on 6 widely used metrics by a large margin and 2) is robust and effective to different diffusion models, attacker prompts, and diverse datasets. These results demonstrate the efficiency and superiority of the proposed PAP method.

In summary, our contributions are as follows:

* We propose a novel Prompt-Agnostic Adversarial Perturbation (PAP) for customized text-to-image diffusion models. To the best of our knowledge, this is the first attempt at prompt-agnostic perturbation for customized diffusion models.
* We model the prompt distribution as a Gaussian distribution with Laplace approximation, where the approximation error is guaranteed. We then derive algorithms to estimate the mean and variance by Taylor expansion and Hessian approximation.

Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (_i.e._, Prompt A) and are fruitless to unseen prompts (_i.e._, Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering

* Based on the prompt distribution modeling, we compute the prompt-agnostic perturbations by maximizing a disturbance expectation via Monte Carlo sampling.

## 2 Related Work

**Text-to-image generation models.** Text-to-image generation has evolved from early cGANs  and VQ-VAE  to advanced transformer-based models [28; 29; 30; 31; 32; 33]. DALL-E  and GLIDE  demonstrate breakthrough performance in diffusion models. Recent works like Textual Inversion , DreamBooth , and ControlNet  further enhance generation quality through customization, user-specific fine-tuning, and additional control mechanisms.

**Gradient-based adversarial attacks.** Gradient-based adversarial attacks deceive machine learning models through perturbed inputs. FGSM  maximizes loss function \(L(f_{}(x+),y)\) as: \(_{}E_{(x,y) D}_{}L(f_{}(x+),y)\). BIM  and PGD  enhance it through iterations. MIFGSM  adds momentum constraint, NI  employs Nesterov acceleration, EMI  stabilizes with gradient averaging, and VMI  reduces variance via neighborhood tuning. CWA  addresses local optima through loss landscape analysis.

**Prompt-specific image cloaking for generation.** Early cloaking methods like Fawkes , Lowkey , and Deepfake defense  focus on face recognition protection. For diffusion models, Photoguard  maximizes VAE latent distance, Glaze  prevents style copying, and Anti-DreamBooth  employs alternating training. Recent works like UAP , AdvDM series [23; 24; 48], AdvTDM , AdvDiff , and Diffprotect  provide theoretical frameworks and improved methods for attacking diffusion models.

In contrast, our work stands apart from these approaches by focusing on the predicted prompt distribution rather than predefined prompt instances, enabling us to achieve global protection efficacy.

## 3 Prompt-Agnostic Adversarial Perturbation

### Background and Motivation

**Prompt-based Diffusion Models.** A vanilla diffusion model [1; 3] aims to gradually transfer a simple Gaussian noise into high-quality images through a series of denoising steps. It mainly contains a forward process (_i.e._, diffusion) and a backward process (_i.e._, denoising). Begin with an image \(x_{0}\), the forward process iteratively adds Gaussian noise \( N(0,I)\) with a noise scheduler \(1-_{t},_{t}(0,1)_{t=1}^{T}\) to the input image. The \(t\)-step obtained noised image \(x_{t}\) can be written as:

\[x_{t}=_{t}}x_{0}+_{t}}\] (1)

where \(_{t}=_{s=1}^{t}_{s}\). When \(t\), \(x_{t}\) is a Gaussian noise (_i.e._, \(x_{t}=\)). In the backward process, the objective is to learn a noise predictor \(_{}(x_{t},t)\) predicting the added Gaussian noise at each step \(t\). On this basis, taking a Gaussian noise (_i.e._, \(x_{t}(t)\)) as input, the diffusion model denoise the image \(x_{t}\) using \(_{}()\), _i.e._, \(x_{t-1}=}(x_{t}-}{_{t }}}_{}(x_{t},t))+_{t}, N(0,I)\), and iteratively generates a high-quality image \(x_{0}\) by \(t\) denoising steps.

The prompt-based diffusion models  aim to generate a semantic guided image \(x_{0}\) from the Gaussian noise \(\). To this end, in the backward process, they additionally take a text prompt \(c\) as the input of noise predictor \(_{}(x_{t},t,c)\) alongside the Gaussian noise \(x_{t}\), and align the image and text representation by cross-attention mechanism. The image generation objective of the prompt-based diffusion models can be written as:

\[_{}L_{cond}(x_{0},c;)=E_{t,(0,1)}L(x_{0 },,t,c;),\] (2)

where \(L(x_{0},,t,c;)=\|-_{}(x_{t},t,c)\|_{2}^{2}\).

**Prompt-Specific Perturbation.** By collecting a set of characterized images (_e.g._, images of a certain person) and a custom-built text prompt (_e.g._ "sks"), recent approaches such as DreamBooth  exploit prompt-based diffusion models for customized image generation. Despite their promising progress, they raise concerns on content falsification such as portrait tampering and copyright infringement. To meet this challenge, existing diffusion-model perturbation methods [23; 22] attempt to add an adversarial perturbation \(\) to the images, aiming to protect the characterized information of the images (such as a person's face) being falsified. Specifically, they often pre-define a custom text prompt \(c_{0}\), and then optimize the adversarial perturbation \(\) to maximize the image generation loss function given \(c_{0}\), which can be written as:

\[^{*}= _{}L_{cond}(x_{0}+,c_{0};),||_{p},\] (3)

where \(L_{cond}\) is evaluated according to Eq.(2). By adding the obtained \(^{*}\) to \(x_{0}\), the diffusion models fail to generate high-quality images with the prompt \(c_{0}\).

These methods, as we discussed in Section 1, are insufficient when the text prompts are different from the training prompt. As shown in Figure 1 (b), when obtaining a perturbation \(\) based on prompt A, the \(\) failed to protect the image from being modified using different prompts such as B and C. These methods compute the perturbations based on enumerated text prompt instances, _i.e._, prompt-specific perturbation, are fruitless facing with endless attack prompts in real-world applications.

### PAP: Prompt-Agnostic Perturbation by Prompt Distribution Modeling

Different from the existing methods that compute a prompt-specific perturbation by enumerating prompt instances, we attempt to compute a prompt-agnostic perturbation by prompt distribution modeling, wherein the obtained perturbation is robust to both seen and unseen attack prompts.

We choose the Laplace approximation as our estimator for three key reasons: 1) it typically yields a Gaussian distribution suitable for large sample sizes; 2) it simplifies computations compared to complex methods like Monte Carlo simulations, particularly when analytical forms are difficult to obtain; and 3) it aligns well with our ideal prompt embedding distribution, concentrated around extreme points.

To this end, we first model and compute a prompt distribution by Laplace approximation, wherein two estimators \(\) and \(\) are developed to compute the distribution parameters. And then we perform Monte Carlo sampling on each input distribution to maximize a disturbance expectation.

Specifically, for the prompt distribution modeling, we consider a protecting image \(x_{0}\) as input and assume a probability-distance correlation between the attacker prompt \(c\) and \(x_{0}\), _i.e._, the further \(c\) is from \(x_{0}\), the lower probability of \(c\) is in the distribution, and vice versa. The distribution relies on \(x_{0}\) is ambiguous, thus we introduce an auxiliary text prompt \(c_{0}\) roughly depicting \(x_{0}\) into the modeling. Based on this foundation, we model the prompt distribution in the embedding space as \(c Q_{(x_{0},c_{0})}\), where \(Q_{(x_{0},c_{0})}\) represents the theoretical distribution with a probability density function \(p(c|x_{0},c_{0})\).

Based on this setup, we approximate the original distribution \(Q_{(x_{0},c_{0})}\) using a Gaussian distribution \(_{(x_{0},c_{0})}\) by Laplace approximation, _i.e._, \(c_{(x_{0},c_{0})}(c_{x},H^{-1})\), where \(c_{x}=_{c}p(c|x_{0},c_{0})\), and \(H\) is the Hessian matrix of \(c_{x}\). As we derived in Section 3.3.1, the approximation error is \((|c-c_{x}|^{3})\), which is negligible as the sampled \(c_{(x_{0},c_{0})}\) is close to \(c_{x}\).

On this basis, we propose two estimators \(\) and \(\) used to estimate \(c_{x}\) and \(H^{-1}\), respectively. For \(\), to compute \(c_{x}=_{c}p(c|x_{0},c_{0})\) that best describe \(x_{0}\), we approximate \(_{x}=(x_{0},)\) by minimizing the generation loss in Eq. (2) with momentum iterations starting from \(c_{0}\). This approach accelerates convergence and avoids getting trapped in local minima. For \(\), we approximate \(^{-1}=(x,,c_{0},t)\) by performing a Taylor expansion around the flattened \(_{x}\) and incorporating prior information from \(c_{0}\). In Appendixes A.2 and A.3, we have proven that the estimation errors of \(c_{x}\) and \(H^{-1}\) are with explicit upper bounds, and more detailed descriptions are provided in Section 3.3.2.

Then, we compute the prompt-agnostic adversarial perturbation \(\) by maximizing the expectation of \(L_{cond}\) in Eq.(2) over the prompt distribution \(Q_{(x_{0},c_{0})}\). The objective can be formulated as:

\[^{*}= _{}E_{c Q_{(x_{0},c_{0})}}L_{cond}(x_{0}+,c_{0};)]\] (4) \[= _{} p(c|x_{0},c_{0}) L_{cond}(x_{0}+,c_{0};)dc,||_{p},\]

where \(p(c|x_{0},c_{0})\) is used to represent the probability distribution of \(Q_{(x_{0},c_{0})}\) given the inputsTo optimize Eq.(4) from a global perspective, we devise a strategy in Section 3.4 that utilizes Monte Carlo sampling on all input distributions, including \(_{(x_{0},c_{0})}\), to maximize the disturbance expectation.

### Modeling the Prompt Distribution \(Q_{(x_{0},c_{0})}\)

In this subsection, we first approximate the form of \(Q_{(x_{0},c_{0})}\) and then estimate its mean and variance.

#### 3.3.1 Laplace Modeling

**Definition 3.1**.: _Since \(x_{0}\) and \(c_{0}\) are independent of \(c\), we consider \(Z=p(x_{0},c_{0})\) as a constant. Denote \(g(c):=p(x_{0},c_{0}|c) p(c)\), \(c_{x}:=_{c}g(c)\), and \(H:=-_{c} g(c)|_{c_{x}}\) for convenience._

We adopt Laplace approximation to model \(Q_{(x_{0},c_{0})}\). Using Bayes' theorem, we obtain:

\[p(c|x_{0},c_{0})=,c_{0}|c) p(c)}{p(x_{0},c_{0})}.\] (5)

We then approximate \( g(c)\) in Definition 3.1 around \(c_{x}\) using a second-order Taylor expansion:

\[ g(c) g(c_{x})-(c-c_{x})H(c-c_{x})^{T}.\] (6)

From Eq.(6) and by ignoring terms that are independent of c, we infer that

\[p(c|x_{0},c_{0})(-(c-c_{x})H(c-c_{x})^{T}),\] (7)

which means \(p(c|x_{0},c_{0})\) could be approximated as a normal distribution, _i.e._,

\[Q_{(x_{0},c_{0})}(c)(c_{x},H^{-1}).\] (8)

The derivation is provided in Appendix A.1, wherein the error of the Gaussian approximation is the third-order derivatives of \( p(x)\) around \(c_{x}\), _i.e._, \((|c-c_{x}|^{3})\).

#### 3.3.2 Parameter Estimators

**Estimator \(\).** According to Definition 3.1, \(c_{x}\) is defined as the text feature that maximizes the joint probability of \(x_{0}\) and \(c_{0}\). As directly maximizing the likelihood is untrackable, similar to , we convert the likelihood maximization problem into an expectation minimization problem with a proper approximation (please kindly refer to Appendix A.2) for more details), which can be written as:

\[_{x}=(x_{0},)=_{c}_{t=0}^{T}L(x_{0},,t, c;)=_{c}_{t=0}^{T}\|-_{}(x_{t},t,c)\|_{2} ^{2}\] (9)

To solve for \(_{x}\) in Eq.(9) and avoid local minimal, we derive a momentum-based iterative method  with the initial value set as the reference prompt \(c_{0}\):

\[ m_{i}= m_{i-1}+(1-)_{c}L(x, ,t,c;),\\ c_{i}=c_{i-1}-r m_{i},\] (10)

where \(m_{i}\) represents the momentum term at iteration \(i\), and \(r\), \(\) are learning rates.

**Estimator \(\).** To compute \(H\) in Definition 3.1, we adopt three operations: substituting \(- g(c)\) with the loss function \(L\) in Eq.(2), incorporating prior information from \(c_{0}\), and applying the Taylor approximation of \(L\) around the flattened \(_{x}\) (Detailed in Appendix A.3.1),which enable us to compute:

\[(c_{0}-c_{x})^{T}H(c_{0}-c_{x})=2(L(x,,t,c_{0};)-L(x, ,t,_{x};)),\] (11)

To obtain \(H^{-1}\) from Eq.(11), we simplify the effective dimensionality of \(H\) in Appendix A.3.2). This allows us to estimate the \(H^{-1}\) using the following expression:

\[^{-1}=(x,,c_{0},t)=-_{x}\|_{2}^{2}}{2 (L(x,,t,c_{0};)-L(x,,t,_{x};))}I,\] (12)

where \(I\) represents the identity matrix and \(x\) are input images. As we analyzed in Appendix A.3.2, the cosine distance between the approximated \(^{-1}\) and \(H^{-1}\) (diagonalized assumption) is with an upper bound of 0.0909 under our standard experimental settings. This simplification significantly reduces the computational complexity with a minor approximation error.

### Maximizing the disturbance expectation

In this subsection, we devise the strategy of maximizing the disturbance expectation based on the modeled prompt distribution, thereby obtaining the final PAP algorithm outlined in Algorithm 1.

**Sampling Distributions for maximization.** To maximize the optimization objective Eq. (4) from a global perspective, we adopt Monte Carlo sampling on all input distributions, including \(Q_{(x_{0},c_{0})}\). Drawing inspiration from established adversarial attack methods , we iteratively sample values for \(t\), \(\), and \(_{c}\). Subsequently, we perform a gradient ascent step of \(L(x,,t,c;)\) with respect to \(x\), which can be summarized as:

\[x_{i+1}=x_{i}+(_{x_{i}}L(x_{i},,t,c; ) N(0,I),t U(0,T),c Q_{(x_{0},c_{0})}),\] (13)

where \(()\) refers to the sign function, and \(\) controls the step size of the gradient ascent.

**Further Discussion.** To further enhance the effectiveness of our proposed algorithm, we seamlessly integrate it with other techniques in Appendix C, such as ASPL , to reach a better performance. We also discuss modifying the perturbation space with the tanh function for smoother optimization with better flexibility  than clipping-based constraints in Appendix B. Lastly, Appendix H explores the limitations and future directions of our model.

## 4 Experiments

In this section, we experimentally compare PAP with other protection methods for customized models, specifically targeting DreamBooth, the leading customized text-to-image diffusion model for personalized image synthesis. DreamBooth customizes models by reconstructing images using a generic prompt that includes pseudo-words like "sks," while also addressing overfitting and shifting through a prior preservation loss. We evaluate PAP on various tasks, including privacy and style protection, using diverse datasets.

### Experimental setup

**Datasets**. Our experiments involve face generation tasks using CelebA-HQ  and VGGFace2  datasets, as well as style imitation task using the Wikiart dataset . For CelebA-HQ and VGGFace2, we select subsets of 600 images respectively, with each of 12 photos from an identical individual. For Wikiart, we choose 100 paintings, with each set consisting of 20 paintings from the same artist.

**Implementation Details**. We optimize adversarial perturbations over 50 training steps and 20 text sampling steps. The step size of image, text and momentum is set to 1/255 and 0.001, 0.5 respectively, and the default noise budget is 0.05. For the Dreambooth, LoRA,TI models, we train the modelswith a learning rate of \(5 10^{-7}\) and batch size 4. We perform 1,000 iterations of fine-tuning with SD1.5 as a pretrained model. The PAP method takes approximately 4 to 6 minutes to complete on an NVIDIA A800 GPU with 80GB memory. To assess defense robustness, we simulate diverse attacker prompts using ten inference sentences in Table 7 that cover a wide range of possibilities. All experimental results presented below are based on the average metrics obtained from ten sentences at default. This approach provides a comprehensive evaluation of overall protection capability with smoother metric variations.

**Evaluation Metrics**. We utilize six metrics, categorized into three aspects. For a more detailed introduction, please refer to Appendix E.4.

1. **Image-to-Image Similarity**: We adopt the following metrics to assess image similarity: CLIP Image-to-Image Similarity (CLIP-I) , Frechet Inception Distance (FID), and Learned Perceptual Image Patch Similarity (LPIPS) . Lower CLIP-I (\(\)) and LPIPS (\(\)), and higher FID (\(\)) indicate better defense performance;
2. **Text-to-Image Similarity**: CLIP measures the coherence between the test prompt and generated images, with lower scores (\(\)) indicating worse alignment;
3. **Image Quality**: BRISQUE  evaluates image quality using statistical features, with higher scores (\(\)) indicating worse quality. LAION aesthetic predictor  assesses the aesthetic quality of images based on visual features, with lower scores (\(\)) indicating worse aesthetics.

### Comparison with State-of-the-Art Methods

#### 4.2.1 Face Privacy Protection

We first conduct experiments in preserving face privacy. During training, the reference and training prompt are both set as "a photo of sks person". Then we use ten prompts related to sks person to evaluate the models' ability to synthesize images. The average results are presented in Table 1, showcasing the superior performance of our method in terms of all metrics on both the CelebA-HQ and VGGFace2 datasets. For example, on the Celeb-HQ dataset, our method exceeds the second-best Anti-DB by **13.55%** in LPIPS, and reduces by **13.06%** and **3.948%** in CLIP-I and LAION. Also, on the VGGFace2 dataset, our method exceeds the second-best method by **6.347%** in BRISQUE, and reduces by **5.818%** and **6.833%** in CLIP-I and CLIP. These results highlight the effectiveness of our method in preserving face privacy as well as robustness to datasets and various prompts' attacks. In Figure 2 (left), we visualize some of the comparative protection results.

   Dataset & Method & FID (\(\)) & CLIP-I (\(\)) & LPIPS (\(\)) & LAION (\(\)) & BRISQUE (\(\)) & CLIP (\(\)) \\   & Clean & 124.2 & 0.7844 & 0.4776 & 6.082 & 28.12 & 0.3368 \\  & AdvDM & 217.1 & 0.6728 & 0.5682 & 5.721 & 34.19 & 0.2905 \\  & Anti-DB & 233.3 & 0.6371 & 0.5924 & 5.497 & 35.89 & 0.2800 \\  & AdvDM & 159.1 & 0.6955 & 0.5303 & 5.768 & 31.77 & 0.2699 \\  & PAP(Ours) & **249.9** & **0.5539** & **0.6730** & **5.280** & **36.95** & **0.2543** \\   & Clean & 230.3 & 0.6567 & 0.5471 & 5.889 & 25.67 & 0.3254 \\  & AdvDM & 243.8 & 0.5931 & 0.6775 & 5.551 & 31.41 & 0.2823 \\   & Anti-DB & 273.0 & 0.5483 & 0.6960 & 5.334 & 28.95 & 0.2766 \\   & AdvDM & 248.6 & 0.5802 & 0.6798 & 5.597 & 32.93 & 0.2835 \\   & PAP(Ours) & **288.5** & **0.5164** & **0.7023** & **5.127** & **35.02** & **0.2577** \\   & Clean & 198.7 & 0.7715 & 0.6193 & 6.367 & 27.34 & 0.3515 \\   & AdvDM & 392.7 & 0.6498 & 0.7606 & 5.949 & 33.54 & 0.3026 \\   & Anti-DB & 386.4 & 0.6462 & 0.7396 & 5.715 & 31.01 & 0.2997 \\   & AdvDM & 390.0 & 0.6550 & 0.7149 & 5.996 & 35.30 & 0.3037 \\   & PAP(Ours) & **448.3** & **0.5641** & **0.7782** & **5.490** & **38.47** & **0.2654** \\   

Table 1: Comparison with other adversarial perturbation methods on the face generation task (including Celeb-HQ and VGGFace2 datasets, training prompt “a photo of sks person”) and style imitation task (including Wikiart dataset, training prompt “a sks painting”) using ten different test prompts, where the reported metric values are the average across these ten test prompts.

#### 4.2.2 Style Imitation

We also evaluate methods' ability to prevent artistic style imitation using the Wikiart dataset. The reference and training prompt are both set as "a sks painting". Ten prompts related to the style of "sks" painting are used to evaluate the model's performance and robustness. It is observed in Table 1 that our method achieves a higher FID, LPIPS, BRISQUE and lower CLIP-I, LAION and CLIP compared to existing methods. For instance, the FID and BRISQUE of our method outperforms that of others by at least **14.16%** and **8.980%** while the CLIP-I and LAION reduce by at least **12.71%** and **3.94%** These results demonstrate the effectiveness of our method in preventing style imitation as well as robustness to attacks with different prompts. In Figure 2 (right), we visualize some of the comparative protection results for Wikiart dataset. More visualized results are demonstrated in Appendix I.

### Ablation Study

**Text Sampling Steps.** We evaluate PAP under different text sampling steps \(N\) (ranging from 0 to 25) used for sampling the prompt \(c\) during training. We use the Wikiart dataset and "a sks painting" prompt to conduct this ablation experiment (see Table 2). Taking into account the cycle time and model performance, we set the text sampling step \(N\) to 15.

**Inference Prompt Combination.** To analyze the effect of prompt variation, we design combinations of prompt categories and quantity: 4\(\)20, 8\(\)10, 10\(\)8, 16\(\)5, 20\(\)4. This keeps the total number of generated images constant while varying the number of prompt categories, allowing us to isolate the effect of prompt categories on the results. As Figure 3 shows, PAP consistently surpasses all other comparison methods and maintains a robust defense against changes in prompt categories.

   Text sampling steps & FID (\(\)) & CLIP-I (\(\)) & LPIPS (\(\)) & LAION (\(\)) & BRISQUE (\(\)) & CLIP (\(\)) & Cycle Time \\ 
0 & 386.4 & 0.6462 & 0.7396 & 5.715 & 31.01 & 0.2997 & 0.3s \\
10 & 430.8 & 0.5873 & 0.7665 & 5.577 & 36.24 & 0.2710 & 5.0s \\
15 & 448.3 & 0.5641 & 0.7782 & 5.490 & **38.47** & 0.2654 & 7.4s \\
20 & 457.5 & 0.5562 & 0.7854 & **5.466** & 38.33 & 0.2591 & 9.6s \\
25 & **462.8** & **0.5481** & **0.7901** & 5.508 & 38.37 & **0.2552** & 11.9s \\   

Table 2: Ablation study of text sampling steps.

Figure 2: Qualitative defense results of different methods in VGGFace2 (left) and Wikiart (right). Each row represents a method, and each column represents a different test prompt (shown at the bottom). The adversarial examples generated by our method effectively defend against all prompts in both datasets. In contrast, other baselines primarily focus on protecting the fixed prompt (the first column), resulting in compromised defense for other prompts.

**Sensitivity to the Initial Prompt.** In Table 3, we present the outcomes when using the initial prompt "", showcasing a significant decline in performance compared to the original PAP. This decline is attributed to: a) the initial prompt serves as a crucial prior for estimating parameters, facilitating rapid iteration (only 20 steps) to achieve a reliable approximation; b) it is involved in the modeling of \(H\) estimation, where the approximate expression for \(H\) is based on the Taylor expansion modeling of the relevant parameters.

**Pseudo-word.** In our experiments, we conduct evaluations using the commonly used pseudo-word "sks," which is representative but may not cover all possible cases. To further validate our method, we included additional less commonly used pseudo-words. Results in Table 12 clearly demonstrates that our method consistently outperforms the others with other pseudo-word.

**Evaluate the approximating \(H\).** We conduct a simple experiment by directly adding Gaussian noise (with variances 1, 5, 10, and 20) to the input to evaluate the value of approximating \(H\). As shown in Table 4, our proposed PAP method, using the variance estimate \(^{2}\), achieves the best performance across all metrics. Specifically, it outperforms the second-best method by 3% (LPIPS), 2% (FDFR), 3% (ISM), and 2.27% (BRISQUE). These findings underscore the necessity of estimating variance \(^{2}\) to generate more effective adversarial perturbations.

**Other Customized Models.** To verify the robustness of proposed methods to fine-tuning methods, we apply PAP to Textual Inversion and DreamBooth with LoRA. LoRA , a widely used efficient low-rank personalization method, poses concerns due to its strong few-shot capability that enables unauthorized artistic style copying. Textual Inversion  learns customized concepts by simply optimizing a word vector instead of finetuning the full model. Table 5 demonstrates that PAP effectively defends against both methods, highlighting our efficacy in countering various personalization techniques.

**Noise Budget.** We conduct experiments to explore the impact of noise budget \(\) on PAP's defense performance (see Table 11 in Appendix F.4). A noise budget of 0.05 is effective and adopted.

   Variance & LPIPS (\(\)) & FDFR (\(\)) & ISM (\(\)) & BRISQUE (\(\)) \\ 
1 & 0.67 & 0.64 & 0.38 & 31.92 \\
5 & 0.67 & 0.65 & 0.38 & 32.75 \\
10 & 0.66 & 0.62 & 0.40 & 29.21 \\
20 & 0.64 & 0.60 & 0.44 & 27.01 \\ H & 0.70 & 0.67 & 0.34 & 35.02 \\   

Table 4: Simple Baseline of Adding Gaussian Noise to the Input on VGGFace2 Dataset

Figure 3: Defense performance of different methods in prompt variation settings. The x-axis represents the number of prompt categories multiplied by the number of generated images per prompt: 4\(\)20, 8\(\)10, 10\(\)8, 16\(\)5, and 20\(\)4. The y-axis displays the values of different metrics.

   Dataset & FID (\(\)) & CLIP-I (\(\)) & LPIPS (\(\)) & LAION (\(\)) & BRISQUE (\(\)) & CLIP (\(\)) \\  Celeb-HQ & 154.57 & 0.72 & 0.50 & 5.83 & 30.18 & 0.32 \\ VGGFace2 & 232.34 & 0.61 & 0.60 & 5.65 & 29.20 & 0.29 \\ Wikiart & 320.79 & 0.69 & 0.71 & 5.72 & 31.88 & 0.30 \\   

Table 3: Results with the initial prompt ""

### Extending Experiments

**DiffPure.** DiffPure  utilizes SDEdit  to purify adversarial images by adding noise and denoising them using diffusion models. In Table 6, we conduct experiments on the Wikiart dataset using DiffPure (t=100). We can see that, 1) compared to _No Defense_, _PAP+DiffPure_ achieves much better adversarial perturbation performance (0.565(\(\)), 3.38(\(\)), 0.0408(\(\)) advances on LAION, BROSQUE and CLIP metrics). 2) Compared to other methods+_DiffPure_, _PAP+DiffPure_ still achieves the best performance on all metrics.

**Preprocessing.** A recent study  reveals that current data protections in text-to-image models are fragile and demonstrate limited robustness against data transformations like JPEG compression. To assess the resilience of our proposed Prompt-Agnostic Adversarial Perturbation (PAP) method, we conduct targeted evaluations using the LAION and BRISQUE metrics. Despite a slight decrease in performance, our method still achieves favorable outcomes in terms of image quality metrics For a detailed discussion and results, please see Appendix F.6.

## 5 Conclusion

This work mitigates risks from misusing customized text-to-image diffusion models. We introduce subtle perturbations optimized from a modeled prompt distribution, fooling such models for any prompt. Demonstrating resilience against diverse attacks, our framework surpasses prior prompt-specific defenses through robustness gains. By efficiently perturbing content via a distribution-aware method, our contributions effectively safeguard images from diffusion model tampering under unknown prompts.