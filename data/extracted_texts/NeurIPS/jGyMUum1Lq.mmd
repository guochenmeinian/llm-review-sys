# Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union

Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union

 Zifu Wang\({}^{1}\) Maxim Berman\({}^{2}\) Amal Rannen-Triki\({}^{3}\) Philip H.S. Torr\({}^{4}\) Devis Tuia\({}^{5}\)

Tinne Tuytelaars\({}^{1}\) Luc Van Gool\({}^{1,6,7}\) Jiaqian Yu\({}^{8}\) Matthew B. Blaschko\({}^{1}\)

\({}^{1}\) ESAT-PSI, KU Leuven, Leuven, Belgium

\({}^{2}\) Google, Zurich, Switzerland

\({}^{3}\) Google DeepMind, London, United Kingdom

\({}^{4}\) University of Oxford, Oxford, United Kingdom

\({}^{5}\) EPFL, Lausanne, Switzerland

\({}^{6}\) ETH Zurich, Zurich, Switzerland

\({}^{7}\) INSAIT, Sofia, Bulgaria

\({}^{8}\) Samsung Research, Beijing, China

Correspondence to: zifu.wang@kuleuven.be

###### Abstract

Semantic segmentation datasets often exhibit two types of imbalance: _class imbalance_, where some classes appear more frequently than others and _size imbalance_, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards _majority classes_ (e.g. overall pixel-wise accuracy) and _large objects_ (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at https://github.com/zifuwanggg/JDTLosses.

## 1 Introduction

Every metric reflects a certain property of the result and choosing the right one is important to emphasize those that we care about. However, this choice may not be easy, as many metrics come with certain biases . For semantic segmentation, the overall pixel-wise accuracy (Acc) is believed unsuitable due to its bias towards majority classes, especially given that semantic segmentation datasets typically have long-tailed class distributions . The mean pixel-wise accuracy (mAcc) is introduced to counter this issue by averaging pixel-wise accuracy across all classes, and it eventually became the official evaluation metric in PASCAL VOC 2007 , which was one of the earliest challenges that showcased a segmentation leaderboard. However, mAcc does not account for false positives, leading to over-segmentation of the results. Consequently, in PASCAL VOC 2008 ,the official metric was updated to the per-dataset mean intersection over union (mIoU\({}^{}\)). Since then, PASCAL VOC  has consistently used mIoU\({}^{}\), and this metric has been adopted by a multitude of segmentation datasets [38; 9; 39; 5; 11; 12; 15; 43].

In mIoU\({}^{}\), true positives, false positives and false negatives are accumulated across all pixels over the whole dataset and therefore this metric suffers several notable shortcomings [10; 9]:

* It is biased towards large objects, which can pose a significant issue given that most semantic segmentation datasets inherently have a considerable size imbalance (see Figure 1). This bias is particularly concerning in safety-aware applications with critical small objects such as autonomous driving [4; 9; 39; 11; 43] and medical imaging [37; 22; 49; 3].
* It fails to capture valuable statistical information about the performance of methods on individual images or instances, which impedes a comprehensive comparison.
* It is challenging to link the results from user studies to those obtained with a per-dataset metric, since humans generally assess the quality of segmentation outputs at the image level.

In this paper, we advocate fine-grained mIoUs for semantic segmentation. In line with previous studies [10; 55; 28; 31], we calculate image-level metrics. When instance-level labels are available [9; 39; 59; 5], we propose to compute approximated instance-level scores. These fine-grained metrics diminish the bias toward large objects. Besides, they yield a wealth of statistical information, which not only allows for a more robust and comprehensive comparison, but also leads to the design of corresponding worst-case metrics, further proving their importance in safety-critical applications. Furthermore, we show that these fine-grained metrics facilitate detailed model and dataset auditing.

Despite the clear advantages of fine-grained mIoUs over mIoU\({}^{}\), these metrics are seldom considered in recent studies within the segmentation community. We bridge this gap with a large-scale benchmark, where we train and evaluate 15 modern neural networks on 12 natural and aerial segmentation datasets, spanning a variety of scenarios including (i) street scenes [4; 9; 39; 11; 43], (ii) "thing" and "stuff" [18; 38; 59; 5], (iii) aerial scenes . Our benchmark study emphasizes the peril of depending solely on a single metric and verifies that fine-grained mIoUs mitigate the bias towards large objects. Moreover, we conduct an in-depth analysis of neural network architectures and loss functions, leading to best practices for optimizing these metrics.

In sum, we believe no evaluation metric is perfect. Thus, we advocate a comprehensive evaluation of segmentation methods and encourage practitioners to report fine-grained metrics and corresponding worst-case metrics, as a complement to mIoU\({}^{}\). We believe this is particularly crucial in safety-critical applications, where the bias towards large objects can precipitate catastrophic outcomes.

## 2 Fine-grained mIoUs

In this section, we review the traditional per-dataset mean intersection over union, followed by our proposed fine-grained image-level and instance-level metrics.

**mIoU\({}^{}\).** Intersection over union (IoU) is defined as

\[=}{++},\] (1)

Figure 1: An illustration of size imbalance. The class Car is depicted in blue and the regions are highlighted within white dashed circles. (a) A car whose side mirror takes up around 2,000 pixels (from Cityscapes). (b) A car that takes up around 1,000 pixels (from Cityscapes). (c) A car whose front wheel takes up around 600,000 pixels (from Mapillary Vistas). (d) A car that takes up around 1,000 pixels (from Mapillary Vistas).

where TP, FP and FN represent true positives, false positives and false negatives, respectively. Per-dataset IoU computes these numbers by accumulating all pixels over the whole dataset. In particular, for a class \(c\), it is defined as

\[_{c}^{}=^{I}_{i,c}}{_{i=1}^{I}( _{i,c}+_{i,c}+_{i,c})},\] (2)

where \(I\) is the number of images in the dataset; \(_{i,c}\), \(_{i,c}\) and \(_{i,c}\) are the number of true-positive, false-positive and false-negative pixels for class \(c\) in image \(i\), respectively. The per-dataset mean IoU is then calculated as

\[^{}=_{c=1}^{C}_{c}^{},\] (3)

where \(C\) denotes the number of classes for the dataset.

Compared to pixel-wise accuracy (Acc) and mean pixel-wise accuracy (mAcc), \(^{}\) aims to mitigate class imbalance and to take FP into account. It is widely used in the evaluation of semantic segmentation models, but it has several shortcomings. Most notably, it is biased towards large objects in the dataset, due to dataset-level accumulations of TP, FP and FN. To address this issue, we can resort to fine-grained mIoUs at per-image (\(^{}\), \(^{}\)) and per-instance (\(^{}\)) levels, as detailed below.

**mIoU.** Following [10; 55; 28; 31], we compute per-image scores. In particular, for each image \(i\) and class \(c\), a per-image-per-class score is calculated as

\[_{i,c}=_{i,c}}{_{i,c}+_{i,c}+ {FN}_{i,c}}.\] (4)

Then the per-image IoU for image \(i\) is defined as

\[_{i}^{}=^{C}\{_{i,c} \}_{i,c}}{_{c=1}^{C}\{_{i, c}\}}\] (5)

where we only sum over \(_{i,c}\) that is not NULL (discuss below). Averaging these per-image scores yields

\[^{}=_{i=1}^{I}_{i}^{}.\] (6)

Using image-level metrics presents a challenge: not all classes appear in every image, leading to NULL values and potential ambiguities. In multi-class segmentation, we denote the per-image-per-class value as NULL if the class is missing from the ground truth, regardless of it appearing in the prediction or not. In contrast, for binary segmentation, the value is set to 0 instead of NULL when the prediction includes the foreground class, but the ground truth does not. More detail of the definition is in Appendix D.

**mIoU.** As the left panel of Figure 2 shows, due to the presence of NULL values, averaging these per-image-per-class scores by class first and then by image (\(^{}\)) is different from averaging them by image first and then by class. A drawback of \(^{}\) is that it is biased towards classes that appear in more images2. To address this bias, we first average these per-image-per-class scores by image and define the per-image IoU for class \(c\) as

\[_{c}^{}=^{I}\{_{i,c} \}_{i,c}}{_{i=1}^{I}\{_{i,c }\}}.\] (7)

Similar to \(^{}\), we then average these per-class values to obtain

\[^{}=_{c=1}^{C}_{c}^{}.\] (8)

**mIoU\({}^{}\).** IoU is known for its scale-variance. However, this property only makes sense at an instance level. When instance-level annotations are available, for example, for "thing" classes in panoptic segmentation  ("thing" classes are with characteristic shapes and identifiable instances such as a truck; see a more detailed discussion of "thing" and "stuff" in Appendix B.1), we can compute a even more fine-grained metric.

Specifically, for an image \(i\) and a class \(c\), we have a binary mask \(H W\) from the prediction of a semantic segmentation network and an instance-level annotation \(H W K_{i,c}\) from panoptic segmentation, where \(K_{i,c}\) is the number instances of class \(c\) that appear in image \(i\). The key insight, as illustrated in the right panel of Figure 2, is to realize that TP and FN for each instance can be computed exactly, and we are only left with image-level FP. We propose to approximate instance-level FP by distributing image-level FP proportional to the size of each instance, with other variants discussed in Appendix E. Therefore, for class \(c\) in image \(i\), we compute the per-instance IoU as

\[_{i,c,k}=_{i,c,k}}{_{i,c,k}+ _{i,c,k}+}{_{k=1}^{K_{i,c}}S_{i,c,k}}_{i, c}},\] (9)

such that \(_{i,c}\) is the total number of false-positive pixels of class \(c\) in image \(i\), and \(S_{i,c,k}=_{i,c,k}+_{i,c,k}\) is the size of instance \(k\). Having these per-instance IoUs, we can again calculate a per-class score as

\[_{c}^{}=^{I}_{k=1}^{K_{i,c} }_{i,c,k}}{_{i=1}^{I}K_{i,c}},\] (10)

With a slightly abuse of notation, we have

\[^{}=_{c=1}^{C}_{c}^{ },\] (11)

such that for classes with instance-level labels, we calculate IoU\({}_{c}^{}\) according to Eq. (10), and for classes without instance-level labels, such as "stuff" classes (classes that are amorphous and do not have identifiable instances such as sky, see Appendix B.1), we compute it as in Eq. (7).

To conclude, similar to mIoU\({}^{}\), we average the scores on a per-class basis for mIoU\({}^{}\) and mIoU\({}^{}\), therefore mitigating _class imbalance_. Additionally, mIoU\({}^{}\) and mIoU\({}^{}\) reduce _size imbalance_ from dataset-level to image-level, and mIoU\({}^{}\) further computes the score at an instance level. On the other hand, scores of mIoU\({}^{}\) are finally averaged on a per-image basis; therefore it will be biased towards classes that appear in more images. However, mIoU\({}^{}\) leads to a single averaged score per image (while mIoU\({}^{}\) has \(I\) scores per class), making per-image analysis (e.g. image-level histograms and worst-case images) feasible as we will present in section 5.1.

Figure 2: **Left:** mIoU\({}^{}\) first averages per-image-per-class scores by class and then by image. mIoU\({}^{}\) first averages them by image and then by class. mIoU\({}^{}\) is biased towards classes that appear in more images, e.g. \(C_{1}\). **Right:** Given two ground-truth instances 1 (red) and 2 (blue), and the prediction (green), we can compute \(_{1},_{2},_{1},_{2}\). We propose to approximate \(_{1}\) and \(_{2}\) by distributing FP proportional to the size of each instance.

## 3 Worst-case Metrics

It is essential for safety-critical applications to evaluate a model's worst-case performance. Adopting fine-grained mIoUs allows us to compute worst-case metrics, where only images/instances that a model obtains the lowest scores are considered. In this section, we focus on mIoU\({}^{}\) as an example. However, the same idea can be applied to both mIoU\({}^{}\) and mIoU\({}^{}\).

For each class \(c\), we first sort IoU\({}_{i,c}\) by images such that IoU\({}_{1,c}..._{I_{c},c}\), where \(I_{c}=_{i=1}^{I}\{_{i,c}\}\). We then only compute the average of those scores that fall below the \(q\)-th quantile:

\[_{c}^{^{q}}= q \%)}_{i=1}^{(1, I_{c} q\%)}_{i,c}.\] (12)

For the ease of comparison, we want to derive a single metric that considers different \(q\%\). Thus, we partition the range into 10 quantile thresholds \(\{10,20,,90,100\}\). We then average the scores corresponding to these thresholds:

\[_{c}^{^{q}}=_{q\{10,,1 00\}}_{c}^{^{q}}.\] (13)

Consequently, the scores with lower values will be given more weights to the final metric. Additionally, we consider IoU\({}_{c}^{^{1}}\) and IoU\({}_{c}^{^{5}}\) to evaluate a model's performance on extremely hard cases. Having these per-class scores, we can average them to obtain the mean metrics: mIoU\({}^{^{5}}\), mIoU\({}^{^{5}}\) and mIoU\({}^{^{1}}\).

## 4 Advantages of Fine-grained mIoUs

In summary, we advocate these fine-grained mIoUs as a complement of mIoU\({}^{}\) because they present several notable benefits:

* **Reduced bias towards large objects.** Unlike mIoU\({}^{}\), fine-grained metrics compute TP, FP and FN at an image or an instance level, thus decreasing the bias towards large objects. To further illustrate this on real datasets, we can compute the ratio of the size of the largest object to that of the smallest object for each class, evaluated at both dataset (\(r_{c}^{}\)) and image levels (\(r_{c}^{}\)): \[r_{c}^{}=S_{i,c,k}}{_{i,k}S_{i,c,k}}, r_{c}^ {}=_{i}S_{i,c,k}}{_{k}S_{i,c,k}}.\] (14) These ratios can serve as an indicator of size imbalance at either the dataset or image level. We average \(^{}}{r_{c}^{}}\) for "thing" and "stuff" classes separately, and note that \(r_{c}^{}=1\) for all "stuff" classes. In Figure 5 (Appendix B.1), we present the numbers for Cityscapes, Mapillary Vistas, ADE20K, and COCO-Stuff. The size imbalance is considerably reduced at the image level, especially for "stuff" classes. As a result, fine-grained mIoUs can be less biased towards large objects.
* **A wealth of statistical information.** For example, fine-grained metrics enable to compute worst-case metrics at image and instance levels as discussed in the previous section. We can also plot a histogram for mIoU\({}^{}\), from which we can extract various statistical information and identify worst-case images. These worst-case images can be instrumental in examining the specific scenarios in which a model underperforms, i.e. _model auditing_. Additionally, employing fine-grained scores facilitates statistical significance testing. Altogether, this yields a more robust and comprehensive comparison between different methods.
* **Dataset auditing.** Images that consistently yield a low image-level score across various models can be examined. These low scores could potentially be attributed to mislabeling. Furthermore, the presence of discrepancies between image- and object-level labels can result in an abnormal mIoU\({}^{}\) values (NaN) because the denominator will become zero. We present the identified mislabels in Appendix B.3.

Experiments

We provide a large-scale benchmark study, where we train 15 modern neural networks from scratch and evaluate them on 12 datasets that cover various scenes. More details of 15 models and 12 datasets considered in our benchmark study can be found in Appendix A and Appendix B, respectively. The total amount of compute time takes around 1 NVIDIA A100 year.

In our benchmark, we emphasize the importance of training models from scratch (training details are in Appendix C) instead of relying on publicly available pretrained checkpoints. This choice is motivated by several factors:

* **Analysis.** By training models from scratch, we aim to analyze how specific training strategies can lead to high results on fine-grained metrics. This provides insights into the training process and enables a better understanding of the factors affecting performance.
* **Completeness.** While there are pretrained models available for popular datasets like Cityscapes and ADE20K, there is a scarcity of checkpoints for other datasets. Training models from scratch ensures a more comprehensive evaluation across different datasets, providing a more complete perspective on model performance.
* **Fairness.** Publicly available pretrained checkpoints are often trained with different hyperparameters and data preprocessing procedures. Training models from scratch ensures fairness in comparing different architectures and training strategies, as they all start from the same initialization and undergo the same training process.
* **Performance.** We leverage recent training techniques [33; 50], which have been shown to improve results. By training models from scratch with these techniques, we aim to achieve better performance compared to other publicly available checkpoints. We provide a comparison of our results with those of MMSegmentation  in Table 1 and Table 2.
* **Statistical significance.** Publicly available pretrained checkpoints often stem from a single run, which may lack statistical significance. By training models from scratch, we can perform multiple runs and obtain statistically significant results. This ensures a more reliable evaluation and avoids potential misinterpretations.

### Main Results

Complete results, including tabular entries, image-level histograms and worst-case images are in Appendix G. Besides, we rank 15 models by their averaged ranking across 10 datasets, excluding Nighttime Driving and Dark Zurich. As depicted in Figure 3, we contrast the rank of mIoU\({}^{}\) with (a) mIoU\({}^{}\), (b) mIoU\({}^{}\), (c) mIoU\({}^{^{}}\), (d) mIoU\({}^{^{}}\). Furthermore, we count the number of times each worst-case image appears for each model, and present the count of three most common worst-case images for each dataset in Table 13 (Appendix G). The key findings are summarized below:

**No model achieves the best result across all metrics and datasets.** UPerNet-ConvNeXt usually obtains the highest score for most of the metrics, possibly due to the ImageNet-22K pretraining of ConvNeXt, while other backbones are pretrained on ImageNet-1K. However, UPerNet-MiTB4 and SegFormer-MiTB4 outperform UPerNet-ConvNeXt on many worst-case metrics. Since different metrics focus on a certain property of the result, it is important to have a comprehensive comparison using various metrics.

   Dataset & Cityscapes & ADE20K & VOC & Context \\  MMSegmentation & 80.97 & 45.47 & 78.62 & 53.20 \\ Ours & 80.91 \(\) 0.17 & 46.32 \(\) 0.33 & 81.07\(\) 0.28 & 56.49 \(\) 0.10 \\   

Table 1: Comparing the performance of DeepLabV3+ResNet101 with MMSegmentation. All results are mIoU\({}^{}\). Red: the best in a column.

   Model & DL3+R10 & DL3+MB2 & UPR-R101 & UPR-Conv & Seg-MiTB4 & PSP-R101 \\  MMSegmentation & 45.47 & 34.02 & 43.82 & 48.71 & 48.46 & 44.39 \\ Ours & 46.32 \(\) 0.33 & 36.85 \(\) 0.34 & 45.89 \(\) 0.11 & 51.08 \(\) 0.42 & 50.23 \(\) 0.26 & 45.69 \(\) 0.33\(\%\) \\   

Table 2: Comparing the performance on ADE20K with MMSegmentation. All results are mIoU\({}^{}\). Red: the best in a column.

**Models do not overfit to a particular metric or a specific range of image-level values.** As shown in Figure 3, we witness only minor local rank discrepancies when comparing mIoU\({}^{}\) with mIoU\({}^{}\), and notably, no rank differences arise when comparing mIoU\({}^{}\) with mIoU\({}^{}\). In general, we observe a monotonic trend. This proves that models are not tailored to favor a particular metric. The rank differences between mIoU\({}^{}\) and mIoU\({}^{^{}}\) are also minimal. This implies that the advancements on mIoU\({}^{}\) are global and models do not overfit to a specific range of image-level values. However, we identify a larger rank drop of UNet-ResNet101 in mIoU\({}^{^{}}\), indicating that UNet may suffer from more challenging examples.

**The performance on fine-grained mIoUs is relatively low.** mIoU\({}^{}\), mIoU\({}^{}\) and mIoU\({}^{}\) are all averaged on a per-class basis. Their values typically follow the ranking: mIoU\({}^{}\) mIoU\({}^{}\) mIoU\({}^{}\). The more fine-grained a metric is, the more challenging the problem it presents, since it is less mercif to individual mistakes. Nevertheless, we observe that the results for mIoU\({}^{}\) and mIoU\({}^{}\) are usually very similar. This observation indicates that mIoU\({}^{}\) can serve as a reliable proxy for instance-wise metrics when instance-level labels are unavailable, e.g. PASCAL VOC/Context, etc.

**Improvements in fine-grained mIoUs do not always extend to corresponding worst-case metrics.** The worst-case metrics generally fall substantially below their corresponding mean metrics. For example, on PASCAL Context, the value of mIoU\({}^{}\) ranges from 46.00% to 60.17%, yet all models have 0% mIoU\({}^{}\). The presence of extremely challenging examples poses obstacles to the application of neural networks in safety-critical tasks. It would be an interesting future research to explore ways of enhancing a model's performance on these hard examples.

**Certain images consistently yield low image-level IoU values across various models.** As presented in Table 13, most models agree on worst-case images across multiple datasets. Several challenging factors, such as difficult lighting conditions and the presence of small objects, can contribute to low IoU scores (as seen in Mapillary Vistas, depicted in Figures 19 - 21). Besides, mislabeling in the dataset may also cause models to produce an image-level IoU of 0 (see Appendix B.3).

Figure 3: Contrasting the rank of mIoU\({}^{}\) with (a) mIoU\({}^{}\), (b) mIoU\({}^{}\), (c) mIoU\({}^{^{}}\), (d) mIoU\({}^{^{}}\). Models below the green dashed line have a higher rank of mIoU\({}^{}\) than that of the counterpart.

**Image-level histograms vary across datasets for the same architecture, but are similar across architectures for the same dataset.** The same architecture yields significantly different shapes of image-level histograms across various datasets. We believe it has to do with the coverage of a dataset (the fraction of classes appear in each image, discussed in Appendix B.2): a dataset with low coverage (e.g. ADE20K and COCO-Stuff) often results in a spread-out histogram with fat tails. On the other hand, although it is possible that a model that has a medium score on all images obtains the same mIoU\({}^{}\) as a model that performs exceedingly well on some images and very poorly on others, we find that different architectures generally produce similar histograms for the same dataset.

**Fine-grained mIoUs exhibit less bias towards large objects.** For example, in Cityscapes, the class Truck has high dataset-level size imbalance (see Figure 4). As demonstrated in Table 3, compared with DeepLabV3-ResNet101, UNet-ResNet101 obtains a low value on IoU\({}^{}_{}\) (15% lower) and mIoU\({}^{}\) (3% lower), since it has a limited reception field and struggles with large objects such as trucks that are close to the camera. On the other hand, it is capable of identifying small objects. As a result, its performance on IoU\({}^{}_{}\), mIoU\({}^{}\) and mIoU\({}^{}\) is similar to that of DeepLabV3-ResNet101.

**Architecture designs and loss functions are vital for optimizing fine-grained mIoUs.** We have seen from previous findings that architecture designs play an important role in optimizing fine-grained mIoUs. The influence of loss functions is also significant. We leave discussions of architecture designs in section 5.2.1 and loss functions in section 5.2.2. These insights contribute to the development of best practices for optimizing fine-grained mIoUs.

### Best Practices

In this section, we explore best practices for optimizing fine-grained mIoUs. In particular, we underscore two key design choices: (i) the aggregation of multi-level features is as vital as incorporating modules that increase the receptive field, and (ii) the alignment of the loss function with the evaluation metrics is paramount.

#### 5.2.1 Architecture Designs

Our study reveals the importance of aggregating multi-scale feature maps for optimizing fine-grained metrics. DeepLabV3 and PSPNet seek to increase the receptive field through the ASPP module  and the PPM module , respectively. The large receptive field helps to capture large objects, resulting in high values on mIoU\({}^{}\). However, these methods lack the fusion of multi-scale feature

Figure 4: From left to right: ground-truth label, prediction and per-image IoU of class Truck with DeepLabV3+, DeepLabV3 and UNet using ResNet101 backbone, respectively. The class Truck is represented in dark blue and highlighted within white dashed circles.

  Method & IoU\({}^{}_{}\) & mIoU\({}^{}\) & IoU\({}^{}_{}\) & mIoU\({}^{}\) & mIoU\({}^{}\) \\  DeepLabV3+ & 84.94\(\) 0.97 & 80.90\(\) 0.16 & 55.26\(\) 0.79 & 70.17\(\) 0.16 & 76.21\(\) 0.05 \\ DeepLabV3 & 81.48\(\) 1.43 & 80.07\(\) 0.06 & 53.87\(\) 0.94 & 69.09\(\) 0.06 & 75.26\(\) 0.03 \\ UNet & 65.58\(\) 0.59 & 77.12\(\) 0.17 & 53.00\(\) 0.61 & 68.78\(\) 0.09 & 75.36\(\) 0.06 \\  

Table 3: Comparing DeepLabV3+, DeepLabV3 and UNet on Cityscapes using ResNet-101 backbone. Red: the best in a column. Green: the worst in a column.

maps and lose the ability to perceive multi-scale objects due to aggregated down-sampling. On the other hand, UNet does not incorporate a separate module to increase the receptive field and therefore underperforms on mIoU\({}^{}\). Nevertheless, it performs comparably to DeepLabV3 and PSPNet on fine-grained metrics. The aggregation of multi-scale feature maps in UNet is critical for detecting small objects and achieving high values on fine-grained metrics3.

This observation is further confirmed by comparing DeepLabV3 with its successor, DeepLabV3+. The sole architectural difference between them is the incorporation of a feature aggregation branch in DeepLabV3+. Although they produce similar results on mIoU\({}^{}\), DeepLabV3+ typically outperforms DeepLabV3 on mIoU\({}^{}\).

It is encouraging to see that state-of-the-art methods, such as DeepLabV3+, UPerNet, and SegFormer, all incorporate modules to increase the receptive field and to aggregate multi-level feature maps. Such design choices align well with the objective of fine-grained mIoUs. We also advocate for the design of more specialized modules for perceiving small objects which can further improve the performance on fine-grained mIoUs.

#### 5.2.2 Loss Functions

Aligning loss functions with evaluation metrics is a widely accepted practice in semantic segmentation . The conventional approach involves combining the cross-entropy loss (CE) and IoU losses , which extend discrete mIoU\({}^{}\) with continuous surrogates. With the introduction of fine-grained mIoUs, it is straightforward to adjust the losses to optimize for these new metrics. Specifically, we explore the Jaccard metric loss (JML)  and its variants for optimizing mIoU\({}^{}\) and mIoU\({}^{}\). From a high-level, JML relaxes set counting (intersection and union) with simple \(L^{1}\) norm functions. More details of JML are in Appendix F.

In Table 4, we compare DeepLabV3+-ResNet101 trained with various loss functions on Cityscapes and ADE20K, respectively. Specifically, (D, I, C) represents the fraction of mIoU\({}^{}\), mIoU\({}^{}\) and mIoU\({}^{}\) in JML. For example, we consider the CE baseline: (0, 0, 0) and variants of JML to only optimize mIoU\({}^{}\): (1, 0, 0) and a uniform mixture of three: \((,,)\). The main takeaways are as follows:

**The impact of loss functions is more pronounced when evaluated with fine-grained metrics.** Specifically, we notice improvements on mIoU\({}^{}\) over CE reaching nearly 3% on Cityscapes and 7% on ADE20K. We also observe substantial increments on mIoU\({}^{}\). Roughly speaking, Acc \(\) mIoU\({}^{}\) mIoU\({}^{}\) in terms of class/size imbalance, while CE directly optimizes Acc. Therefore, it becomes more crucial to choose the correct loss functions when evaluated with fine-grained metrics.

**There exist trade-offs among different JML variants.** For example, compared with other JML variants, (1, 0, 0) achieves the highest mIoU\({}^{}\) in both datasets, but have low values on mIoU\({}^{}\) and mIoU\({}^{}\). We use (0, 0.5, 0.5) as the default setting in our benchmark, but alternate ratios may be considered depending on the evaluation metrics.

A notable observation is that CE surpasses (0, 1, 0) in terms of mIoU\({}^{}\) on Cityscapes. Interestingly, many segmentation codebases, including SMP  and MMSegmentation , calculate the Jaccard/Dice loss on a per-GPU basis. When training on GPUs with limited memory, the per-GPU batch size is often small. As a result, the loss function tends to mirror the behavior of (0, 1, 0), potentially leading to sub-optimal results as measured by mIoU\({}^{}\).

   (D, I, C) &  &  \\   & mIoU\({}^{}\) & mIoU\({}^{}\) & mIoU\({}^{}\) & mIoU\({}^{}\) & mIoU\({}^{}\) \\  (0, 0, 0) & 80.67 \(\) 0.36 & 74.57 \(\) 0.09 & 67.61 \(\) 0.12 & 45.01 \(\) 0.13 & 53.73 \(\) 0.09 & 39.66 \(\) 0.13 \\ (1, 0, 0) & 81.22 \(\) 0.27 & 75.34 \(\) 0.09 & 68.97 \(\) 0.19 & 47.21 \(\) 0.26 & 57.74 \(\) 0.11 & 46.69 \(\) 0.25 \\ (0, 1, 0) & 80.29 \(\) 0.28 & 76.28 \(\) 0.02 & 69.97 \(\) 0.09 & 45.83 \(\) 0.19 & 58.37 \(\) 0.03 & 45.55 \(\) 0.25 \\ (0, 1, 0) & 80.75 \(\) 0.24 & 76.21 \(\) 0.08 & 70.49 \(\) 0.19 & 46.55 \(\) 0.54 & 58.74 \(\) 0.09 & 46.97 \(\) 0.293 \\ (\(,)\) & 81.13 \(\) 0.10 & 76.19 \(\) 0.05 & 70.08 \(\) 0.15 & 46.92 \(\) 0.25 & 58.51 \(\) 0.02 & 46.77 \(\) 0.07 \\ (0, \(,)\) & 80.91 \(\) 0.17 & 76.21 \(\) 0.05 & 70.18 \(\) 0.16 & 46.32 \(\) 0.33 & 58.79 \(\) 0.04 & 46.52 \(\) 0.30 \\   

Table 4: Comparing different loss functions using DeepLabV3+-ResNet101 on Cityscapes and ADE20K. (D, I, C) represents the fraction of mIoU\({}^{}\), mIoU\({}^{}\) and mIoU\({}^{}\) in JML, respectively. Red: the best in a column. Green: the worst in a column.

## 6 Discussion

### Limitation

We propose three fine-grained mIoUs: mIoU\({}^{}\), mIoU\({}^{}\), mIoU\({}^{}\), along with their associated worst-case metrics. These metrics bring significant advantages over traditional mIoU\({}^{}\). However, they are not without shortcomings, and users should be aware of these when employing them in research or applications. Specifically, mIoU\({}^{}\) and mIoU\({}^{}\) fall short when it comes to addressing instance-level imbalance. Meanwhile, mIoU\({}^{}\) serves merely as an approximation of the per-instance metric, rather than an exact representation. Beisdes, conducting image-level analyses, such as image-level histograms and worst-case images, proves challenging with mIoU\({}^{}\) and mIoU\({}^{}\). Although such capabilities are accommodated by mIoU\({}^{}\), it has an inherent bias towards classes that are more frequently represented across images.

Additionally, it is important to note that our conclusions are based on a specific training setting, including choices of optimizer, loss function, and other hyperparameters. There is potential for varied findings when altering this setting, especially when it comes to the choice of the loss function.

Finally, our experiments primarily focus on natural and aerial datasets, excluding medical datasets from our analysis. Besides, while our core emphasis is on addressing size imbalance in relation to IoU, the concept of calculating segmentation metrics in a fine-grained manner could be extended to other metrics, such as mAcc, the Dice score, the calibration error , etc. We plan to address these limitations in future work.

### Suggestion

While mIoU\({}^{}\) might show a bias towards more common classes, it remains invaluable for users to analyze the dataset with image-level information, as well as provide a holistic evaluation of various segmentation methods. Given that mIoU\({}^{}\) is a good proxy of mIoU\({}^{}\), and considering many datasets lack instance-level labels, we believe mIoU\({}^{}\) adequately serves as an instance-level approximation and could be used in most cases.

Regarding the worst-case metrics, we examine three variants: mIoU\({}^{^{}}\), mIoU\({}^{^{}}\) and mIoU\({}^{^{}}\). We recommend the use of mIoU\({}^{^{}}\) in applications where worst-case outcomes are prioritized. We provide evaluations using both mIoU\({}^{^{}}\) and mIoU\({}^{^{}}\) for the sake of comprehensiveness, but believe that adopting either one is sufficient. While utilizing \(q=1\) might be suitable for larger datasets like ADE20K to emphasize these exceptionally difficult examples, it could introduce significant variance for smaller datasets such as DeepGlobe Land. For the latter, a value of \(q=5\) or even larger might be more appropriate. With these considerations, dataset organizers can choose the metric that best fits their context.

## 7 Conclusion

In conclusion, this study provides crucial insights into the limitations of traditional per-dataset mean intersection over union and advocates for the adoption of fine-grained mIoUs, as well as the corresponding worst-case metrics. We argue that these metrics provide a less biased and more comprehensive evaluation, which is vital given the inherent class and size imbalances found in many segmentation datasets. Besides, these fine-grained metrics furnish a wealth of statistical information that can guide practitioners in making more informed decisions when evaluating segmentation methods for specific applications. Furthermore, they offer valuable insights for model and dataset auditing, aiding in the rectification of corrupted labels.

Our extensive benchmark study, covering 12 varied natural and aerial datasets and featuring 15 modern neural network architectures, underscores the importance of not relying solely on a single metric and affirms the potential of fine-grained metrics to reduce the bias towards large objects. It also sheds light on the significant roles played by architectural designs and loss functions, leading to best practices in optimizing fine-grained metrics.