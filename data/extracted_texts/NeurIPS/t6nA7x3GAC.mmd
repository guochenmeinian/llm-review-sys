# Trans-Dimensional Generative Modeling

via Jump Diffusion Models

 Andrew Campbell\({}^{1}\)&William Harvey\({}^{2}\)&Christian Weilbach\({}^{2}\)&Valentin De Bortoli\({}^{3}\)&Tom Rainforth\({}^{1}\)&Arnaud Doucet\({}^{1}\)

\({}^{1}\)Department of Statistics, University of Oxford, UK

\({}^{2}\) Department of Computer Science, University of British Columbia, Vancouver, Canada

\({}^{3}\)CNRS ENS Ulm, Paris, France

{campbell, rainforth, doucet}@stats.ox.ac.uk

{wsgh, weilbach}@cs.ubc.ca

valentin.debortoli@gmail.com

###### Abstract

We propose a new class of generative models that naturally handle data of varying dimensionality by jointly modeling the state and dimension of each datapoint. The generative process is formulated as a jump diffusion process that makes jumps between different dimensional spaces. We first define a dimension destroying forward noising process, before deriving the dimension creating time-reversed generative process along with a novel evidence lower bound training objective for learning to approximate it. Simulating our learned approximation to the time-reversed generative process then provides an effective way of sampling data of varying dimensionality by jointly generating state values and dimensions. We demonstrate our approach on molecular and video datasets of varying dimensionality, reporting better compatibility with test-time diffusion guidance imputation tasks and improved interpolation capabilities versus fixed dimensional models that generate state values and dimensions separately.

## 1 Introduction

Generative models based on diffusion processes  have become widely used in solving a range of problems including text-to-image generation , audio synthesis  and protein design . These models define a forward noising diffusion process that corrupts data to noise and then learn the corresponding time-reversed backward generative process that generates novel datapoints from noise.

In many applications, for example generating novel molecules  or videos , the dimension of the data can also vary. For example, a molecule can contain a varying number of atoms and a video can contain a varying number of frames. When defining a generative model over these data-types, it is therefore necessary to model the number of dimensions along with the raw values of each of its dimensions (the state). Previous approaches to modeling such data have relied on first sampling the number of dimensions from the empirical distribution obtained from the training data, and then sampling data using a fixed dimension diffusion model (FDDM) conditioned on this number of dimensions . For conditional modeling, where the number of dimensions may depend on the observations, this approach does not apply and we are forced to first train an auxiliary model that predicts the number of dimensions given the observations .

This approach to trans-dimensional generative modeling is fundamentally limited due to the complete separation of dimension generation and state value generation. This is exemplified in the common use case of conditional diffusion guidance. Here, an unconditional diffusion model is trained that end-users can then easily and cheaply condition on their task of interest through guiding the generative diffusion process [3; 12; 13; 14] without needing to perform any further training or fine-tuning of the model on their task of interest. Since the diffusion occurs in a fixed dimensional space, there is no way for the guidance to appropriately guide the dimension of the generated datapoint. This can lead to incorrect generations for datasets where the dimension greatly affects the nature of the datapoint created, e.g. small molecules have completely different properties to large molecules.

To generate data of varying dimensionality, we propose a jump diffusion based generative model that jointly generates both the dimension and the state. Our model can be seen as a unification of diffusion models which generate all dimensions in parallel with autoregressive type models which generate dimensions sequentially. We derive the model through constructing a forward noising process that adds noise and removes dimensions and a backward generative process that denoises and adds dimensions, see Figure 1. We derive the optimum backward generative process as the time-reversal of the forward noising process and derive a novel learning objective to learn this backward process from data. We demonstrate the advantages of our method on molecular and video datasets finding our method achieves superior guided generation performance and produces more representative data interpolations across dimensions.

## 2 Background

Standard continuous-time diffusion models [3; 15; 16; 17] define a forward diffusion process through a stochastic differential equation (SDE) where \(_{0} p_{}\) and, for \(t>0\),

\[_{t}=}_{t}(_{t}) t+g_{t}_{t},\] (1)

where \(_{t}^{d}\) is the current state, \(}_{t}:^{d}^{d}\) is the drift and \(g_{t}\) is the diffusion coefficient. \(_{t}\) is a Brownian motion increment on \(^{d}\). This SDE can be understood intuitively by noting that in each infinitesimal timestep, we move slightly in the direction of the drift \(}_{t}\) and inject a small amount of Gaussian noised governed by \(g_{t}\). Let \(p_{t}(_{t})\) denote the distribution of \(_{t}\) for the forward diffusion process (1) so that \(p_{0}(_{0})=p_{}(_{0})\). \(}_{t}\) and \(g_{t}\) are set such that at time \(t=T\), \(p_{T}(_{T})\) is close to \(p_{}(_{T})=(_{T};0,I_{d})\); e.g. \(}_{t}(_{t})=-_{t}_{t},\ g_{t}=}\) for \(_{t}>0\)[2; 3].

The time-reversal of the forward diffusion (1) is also a diffusion [18; 19] which runs backwards in time from \(p_{T}(_{T})\) to \(p_{0}(_{0})\) and satisfies the following reverse time SDE

\[_{t}=}_{t}(_{t}) t+g_{t}}_{t},\]

where \(}_{t}(_{t})=}_{ t}(_{t})-g_{t}^{2}_{_{t}} p_{t}(_{t})\), \(t\) is a negative infinitesimal time step and \(}_{t}\) is a Brownian motion increment when time flows backwards. Unfortunately, both the terminal

Figure 1: Illustration of the jump diffusion generative process on videos and molecules. The generative process consists of two parts: a diffusion part which denoises the current set of frames/atoms and a jump part which adds on a suitable number of new frames/atoms such that the final generation is a clean synthetic datapoint of an appropriate size.

distribution, \(p_{T}(_{T})\), and the score, \(_{_{t}} p_{t}(_{t})\), are unknown in practice. A generative model is obtained by approximating \(p_{T}\) with \(p_{}\) and learning an approximation \(s^{}_{t}(_{t})\) to \(_{_{t}} p_{t}(_{t})\) typically using denoising score matching , i.e.

\[} _{(t;0,T)p_{0,t}(_{0},_{ t})}[\|s^{}_{t}(_{t})-_{_{t}} p_{t|0}( _{t}|_{0})\|^{2}].\] (2)

For a flexible model class, \(s^{}\), we get \(s^{}_{t}(_{t})_{_{t}} p_{t}( _{t})\) at the minimizing parameter.

## 3 Trans-Dimensional Generative Model

Instead of working with fixed dimension datapoints, we will instead assume our datapoints consist of a variable number of components. A datapoint \(\) consists of \(n\) components each of dimension \(d\). For ease of notation, each datapoint will explicitly store both the number of components, \(n\), and the state values, \(\), giving \(=(n,)\). Since each datapoint can have a variable number of components from \(n=1\) to \(n=N\), our overall space that our datapoints live in is the union of all these possibilities, \(=_{n=1}^{N}\{n\}^{nd}\). For example, for a varying size point cloud dataset, components would refer to points in the cloud, each containing \((x,y,z)\) coordinates giving \(d=3\) and the maximum possible number of points in the cloud is \(N\).

Broadly speaking, our approach will follow the same framework as previous diffusion generative models. We will first define a forward noising process that both corrupts state values with Gaussian noise and progressively deletes dimensions. We then learn an approximation to the time-reversal giving a backward generative process that simultaneously denoises whilst also progressively adding dimensions back until a synthetic datapoint of appropriate dimensionality has been constructed.

### Forward Process

Our forward and backward processes will be defined through jump diffusions. A jump diffusion process has two components, the diffusion part and the jump part. Between jumps, the process evolves according to a standard SDE. When a jump occurs, the process transitions to a different dimensional space with the new value for the process being drawn from a transition kernel \(K_{t}(|):_{ 0}\). Letting \(=(m,)\), the transition kernel satisfies \(_{m}_{}K_{t}(m,|)=1\) and \(_{}K_{t}(m=n,|)=0\). The rate at which jumps occur (jumps per unit time) is given by a rate function \(_{t}():_{ 0}\). For an infinitesimal timestep \(t\), the jump diffusion can be written as

\[&^{}_{t}=_{t}&1-_{t}(_{t})t\\  K_{t}(|_{t})&_{t}( _{t})t\\ &_{t+t}=^{}_{t}+_{t}(^{}_{t})t+g_{t}_{t}&n_{t+ t}=n^{}_{t}\]

with \(_{t}(n_{t},_{t})\) and \(_{t+t}(n_{t+t},_{t+t})\) and \(_{t}\) being a Brownian motion increment on \(^{n^{}_{t}d}\). We provide a more formal definition in Appendix A.

With the jump diffusion formalism in hand, we can now construct our forward noising process. We will use the diffusion part to corrupt existing state values with Gaussian noise and the jump part to destroy dimensions. For the diffusion part, we use the VP-SDE introduced in [2; 3] with \(}_{t}()=-_{t}\) and \(_{t}=}\) with \(_{t} 0\).

When a jump occurs in the forward process, one component of the current state will be deleted. For example, one point in a point cloud or a single frame in a video is deleted. The rate at which these deletions occur is set by a user-defined forward rate \(_{t}()\). To formalize the deletion, we need to introduce some more notation. We let \(K^{}(i|n)\) be a user-defined distribution over which component of the current state to delete. We also define \(:\) to be the deletion operator that deletes a specified component. Specifically, \((n-1,)=((n,),i)\) where \(^{(n-1)d}\) has the same values as \(^{nd}\) except for the \(d\) values corresponding to the \(i\)th component which have been removed. We can now define the forward jump transition kernel as \(_{t}(|)=_{i=1}^{n}K^{}(i|n) _{(,i)}()\). We note that only one component is ever deleted at a time meaning \(_{t}(m,|)=0\) for \(m n-1\). Further, the choice of \(K^{}(i|n)\) will dictate the behaviour of the reverse generative process. If we set \(K^{}(i|n)=\{i=n\}\) then we only ever delete the final component and so in the reverse generative direction, datapoints are created additively, appending components onto the end of the current state. Alternatively, if we set \(K^{}(i|n)=1/n\) then components are deleted uniformly at random during forward corruption and in the reverse generative process, the model will need to pick the most suitable location for a new component from all possible positions.

The forward noising process is simulated from \(t=0\) to \(t=T\) and should be such that at time \(t=T\), the marginal probability \(p_{t}()\) should be close to a reference measure \(p_{}()\) that can be sampled from. We set \(p_{}()=\{n=1\}(;0,I_{d})\) where \(\{n=1\}\) is \(1\) when \(n=1\) and \(0\) otherwise. To be close to \(p_{}\), for the jump part, we set \(_{t}\) high enough such that at time \(t=T\) there is a high probability that all but one of the components in the original datapoint have been deleted. For simplicity, we also set \(_{t}\) to depend only on the current dimension \(_{t}()=_{t}(n)\) with \(_{t}(n=1)=0\) so that the forward process stops deleting components when there is only \(1\) left. In our experiments, we demonstrate the trade-offs between different rate schedules in time. For the diffusion part, we use the standard diffusion \(_{t}\) schedule [2; 3] so that we are close to \((;0,I_{d})\).

### Backward Process

The backward generative process will simultaneously denoise and add dimensions back in order to construct the final datapoint. It will consist of a backward drift \(}_{t}()\), diffusion coefficient \(}_{t}\), rate \(_{t}()\) and transition kernel \(_{t}(|)\). We would like these quantities to be such that the backward process is the time-reversal of the forward process. In order to find the time-reversal of the forward process, we must first introduce some notation to describe \(_{t}(|)\). \(_{t}(|)\) should undo the forward deletion operation. Since \(_{t}(|)\) chooses a component and then deletes it, \(_{t}(|)\) will need to generate the state values for a new component, decide where the component should be placed and then insert it at this location. Our new component will be denoted \(^{}^{d}\). The insertion operator is defined as \(:^{d}\). It takes in the current value \(\), the new component \(^{}\) and an index \(i\{1,,n+1\}\) and inserts \(^{}\) into \(\) at location \(i\) such that the resulting value \(=(,^{},i)\) has \((,i)=\). We denote the joint conditional distribution over the newly added component and the index at which it is inserted as \(A_{t}(^{},i|)\). We therefore have \(_{t}(|)=_{^{}} _{i=1}^{n+1}A_{t}(^{},i|)_{( ,^{},i)}()^{}\). Noting that only one component is ever added at a time, we have \(_{t}(m,|)=0\) for \(m n+1\).

This backward process formalism can be seen as a unification of diffusion models with autoregressive models. The diffusion part \(}_{t}\) denoises the current set of components in parallel, whilst the autoregressive part \(A_{t}(^{},i|)\) predicts a new component and its location. \(_{t}()\) is the glue between these parts controlling when and how many new components are added during generation.

We now give the optimum values for \(}_{t}()\), \(_{t}\), \(_{t}()\) and \(A_{t}(^{},i|)\) such that the backward process is the time-reversal of the forward process.

**Proposition 1**.: _The time reversal of a forward jump diffusion process given by drift \(}_{t}\), diffusion coefficient \(_{t}\), rate \(_{t}(n)\) and transition kernel \(_{i=1}^{n}K^{}(i|n)_{(,i)}()\) is given by a jump diffusion process with drift \(}_{t}^{*}()\), diffusion coefficient \(_{t}^{*}\), rate \(_{t}^{*}()\) and transition kernel \(_{t}^{*}()\)._

   Direction & \(_{t}\) & \(g_{t}\) & \(_{t}()\) & \(K_{t}(|)\) \\ 
**Forward** & \(-_{t}\) & \(}\) & \(_{t}(n)\) & \(_{i=1}^{n}K^{}(i|n)_{(,i)}()\) \\
**Backward** & \(-_{t}-_{t}s_{t}^{}()\) & \(}\) & \(_{t}^{*}()\) & \(_{^{}}_{i=1}^{n+1}A_{t}^{}(^{},i|)_{(,^{},i)}( )^{}\) \\   

Table 1: Summary of forward and parameterized backward processes\(_{^{}}_{i=1}^{n+1}A_{t}^{*}(^{},i |)_{(,^{},i)}( )^{}\) as defined below_

\[}_{t}^{*}() =}_{t}()-_{t }^{2}_{} p_{t}(),_{t}^{*}= _{t},\] \[_{t}^{*}() =_{t}(n+1)^{n+1}K^{ }(i|n+1)_{^{}}p_{t}((, ^{},i))^{}}{p_{t}()},\] \[A_{t}^{*}(^{},i|) p_{t}( (,^{},i))K^{}(i|n+1).\]

All proofs are given in Appendix A. The expressions for \(}_{t}^{*}\) and \(_{t}^{*}\) are the same as for a standard diffusion except for replacing \(_{} p_{t}()\) with \(_{} p_{t}()=_{} p_{t}( |n)\) which is simply the score in the current dimension. The expression for \(_{t}^{*}\) can be understood intuitively by noting that the numerator in the probability ratio is the probability that at time \(t\), given a deletion occurs, the forward process will arrive at \(\). If this is higher than the raw probability at time \(t\) that the forward process is at \(\) (the denominator) then we should have high \(_{t}^{*}\) because \(\) is likely the result of a deletion of a larger datapoint. Finally the optimum \(A_{t}^{*}(^{},i|)\) is simply the conditional distribution of \(^{}\) and \(i\) given \(\) when the joint distribution over \(^{},i,\) is given by \(p_{t}((,^{},i))K^{}(i|n+1)\).

### Objective for Learning the Backward Process

The true \(}_{t}^{*}\), \(_{t}^{*}\) and \(A_{t}^{*}\) are unknown so we need to learn approximations to them, \(}_{t}^{}\), \(_{t}^{}\) and \(A_{t}^{}\). Following Proposition 1, we set \(}_{t}^{}()=}_ {t}()-_{t}^{2}s_{t}^{}()\) where \(s_{t}^{}()\) approximates \(_{} p_{t}()\). The forward and parameterized backward processes are summarized in Table 1.

Standard diffusion models are trained using a denoising score matching loss which can be derived from maximizing an evidence lower bound on the model probability for \(_{p_{}(_{0})}[ p_{0}^{}(_{0})]\). We derive here an equivalent loss to learn \(s_{t}^{}\), \(_{t}^{}\) and \(A_{t}^{}\) for our jump diffusion process by leveraging the results of  and . Before presenting this loss, we first introduce some notation. Our objective for \(s_{t}^{}(_{t})\) will resemble denoising score matching (2) but instead involve the conditional score \(_{_{t}} p_{t|0}(_{t}|_{0})=_{ _{t}} p_{t|0}(_{t}|_{0},n_{t})\). This is difficult to calculate directly due to a combinatorial sum over the different ways the components of \(_{0}\) can be deleted to get to \(_{t}\). We avoid this problem by equivalently conditioning on a mask variable \(M_{t}\{0,1\}^{n_{0}}\) that is 0 for components of \(_{0}\) that have been deleted to get to \(_{t}\) and 1 for components that remain in \(_{t}\). This makes our denoising score matching target easy to calculate: \(_{_{t}} p_{t|0}(_{t}|_{0},n_{t},M_{t}) =M_{t}(_{0})-_{t}}}{1-_{t}}\) where \(_{t}=(-_{0}^{t}(s)s)\). Here \(M_{t}(_{0})\) is the vector removing any components in \(_{0}\) for which \(M_{t}\) is \(0\), thus \(M_{t}(_{0})\) and \(_{t}\) have the same dimensionality. We now state our full objective.

**Proposition 2**.: _For the backward generative jump diffusion process starting at \(p_{}(_{T})\) and finishing at \(p_{0}^{}(_{0})\), an evidence lower bound on the model log-likelihood \(_{_{0} p_{}}[ p_{0}^{}(_{0} )]\) is given by_

\[() =-g_{t}^{2}\|s_{t}^{}( _{t})-_{_{t}} p_{t|0}(_{t}|_{0},n_{t},M _{t})\|^{2}+\] (3) \[T-_{t}^{}( _{t})+_{t}(n_{t})_{t}^{}( )+_{t}(n_{t}) A_{t}^{}(_{t} ^{},i|)+C,\] (4)

_where expectations are with respect to \((t;0,T)p_{0,t}(_{0},_{t},M_{t})K^{}(i|n_ {t})_{(_{t},i)}()\), \(C\) is a constant term independent of \(\) and \(_{t}\)\(\)\((,_{t}^{},i)\). This evidence lower bound is equal to the log-likelihood when \(}_{t}^{}=}_{t}^{*}\), \(_{t}^{}=_{t}^{*}\) and \(A_{t}^{}=A_{t}^{*}\)._

We now examine the objective to gain an intuition into the learning signal. Our first term (3) is an \(L_{2}\) regression to a target that, as we have seen, is a scaled vector between \(_{t}\) and \(}M_{t}(_{0})\). As the solution to an \(L_{2}\) regression problem is the conditional expectation of the target, \(s_{t}^{}(_{t})\) will learn to predict vectors pointing towards \(_{0}\) averaged over the possible correspondences between dimensions of \(_{t}\) and dimensions of \(_{0}\). Thus, during sampling, \(s_{t}^{}(_{t})\) provides a suitable direction to adjust the current value \(_{t}\) taking into account the fact \(_{t}\) represents only a noisy subpart of a clean whole \(_{0}\).

The second term (4) gives a learning signal for \(_{t}^{}\) and \(A_{t}^{}\). For \(A_{t}^{}\), we simply have a maximum likelihood objective, predicting the missing part of \(_{t}\) (i.e. \(_{t}^{}\)) given the observed part of \(_{t}\) (i.e. \(\)). The signal for \(_{t}^{}\) comes from balancing two terms: \(-_{t}^{}(_{t})\) and \(_{t}(n_{t})_{t}^{}( )\)which encourage the value of \(_{t}^{}\) to move in opposite directions. For a new test input \(\), \(_{t}^{}()\)'s value needs to trade off between the two terms by learning the relative probability between \(\) being the entirety of a genuine sample from the forward process, corresponding to the \(_{t}^{}(_{t})\) term in (4), or \(\) being a substructure of a genuine sample, corresponding to the \(_{t}^{}()\) term in (4). The optimum trade-off is found exactly at the time reversal \(_{t}^{*}\) as we show in Appendix A.5.

We optimize \(()\) using stochastic gradient ascent, generating minibatches by first sampling \(t(0,T),_{0} p_{}\) and then computing \(_{t}\) from the forward process. This can be done analytically for the \(_{t}(n)\) functions used in our experiments. We first sample \(n_{t}\) by analytic integration of the dimension deletion Poisson process with time inhomogeneous rate \(_{t}(n)\). We then add Gaussian noise independently to each dimension under \(p_{t|0}(_{t}|_{0},n_{t},M_{t})\) using a randomly drawn mask variable \(M_{t}\). See Appendix B for further details on the efficient evaluation of our objective.

### Parameterization

\(s_{t}^{}(_{t})\), \(A_{t}^{}(^{},i|_{t})\) and \(_{t}^{}(_{t})\) will all be parameterized by neural networks. In practice, we have a single backbone network suited to the problem of interest e.g. a Transformer , an EGNN  or a UNet  onto which we add prediction heads for \(s_{t}^{}(_{t})\), \(A_{t}^{}(^{},i|_{t})\) and \(_{t}^{}(_{t})\). \(s_{t}^{}(_{t})\) outputs a vector in \(^{n_{t}d}\). \(A_{t}^{}(^{},i|_{t})\) outputs a distribution over \(i\) and mean and standard deviation statistics for a Gaussian distribution over \(^{}\). Finally, having \(_{t}^{}(_{t})_{ 0}\) be the raw output of a neural network can cause optimization issues due to the optimum \(_{t}^{*}\) including a probability ratio which can take on very large values. Instead, we learn a component prediction network \(p_{0|t}^{}(n_{0}|_{t})\) that predicts the number of components in \(_{0}\) given \(_{t}\). To convert this into \(_{t}^{}(_{t})\), we show in Proposition 3 how the optimum \(_{t}^{*}(_{t})\) is an analytic function of the true \(p_{0|t}(n_{0}|_{t})\). We then plug \(p_{0|t}^{}(n_{0}|_{t})\) into Proposition 3 to obtain an approximation of \(_{t}^{*}(_{t})\).

**Proposition 3**.: _We have_

\[_{t}^{*}(_{t})=_{t} (n_{t}+1)_{n_{0}=1}^{N}(n_{t}+1|n_{0})}{p_{t|0}(n_{t}|n_{0})} p_{0|t}(n_{0}|_{t}),\]

_where \(_{t}=(n_{t},_{t})\) and \(p_{t|0}(n_{t}+1|n_{0})\) and \(p_{t|0}(n_{t}|n_{0})\) are both easily calculable distributions from the forward dimension deletion process._

### Sampling

To sample the generative process, we numerically integrate the learned backward jump diffusion processing using time-step \( t\). Intuitively, it is simply the standard continuous time diffusion sampling scheme  but at each timestep we check whether a jump has occurred and if it has, sample the new component and insert it at the chosen index as explained by Algorithm 1.

``` \(t T\) \( p_{}()=\{n=1\}( ;0,I_{d})\) while\(t>0\)do if\(u<_{t}^{*}() t\) with \(u(0,1)\)then  Sample \(^{},i A_{t}^{}(^{},i| )\) \((,^{},i)\)  end if \(-}_{t}^{}( ) t+g_{t}\) with \((0,I_{nd})\) \((n,)\), \(t t- t\) end ```

**Algorithm 1**Sampling the Generative Process

## 4 Related Work

Our method jointly generates both dimensions and state values during the generative process whereas prior approaches [8; 11] are forced to first sample the number of dimensions and then run the diffusion process in this fixed dimension. When diffusion guidance is applied to these unconditional models [14; 26], users need to pick by hand the number of dimensions independent of the conditioning information even though the number of dimensions can be correlated with the conditioning parameter.

Instead of automatically learning when and how many dimensions to add during the generative process, previous work focusing on images [27; 28] hand pick dimension jump points such that the resolution of images is increased during sampling and reaches a certain pre-defined desired resolution at the end of the generative process. Further, rather than using any equivalent of \(A_{t}^{}\), the values for new dimensions are simply filled in with Gaussian noise. These approaches mainly focus on efficiency rather than flexible generation as we do here.

The first term in our learning objective in Proposition 2 corresponds to learning the continuous part of our process (the diffusion) and the second corresponds to learning the discrete part of our process (the jumps). The first term can be seen as a trans-dimensional extension of standard denoising score matching  whilst the second bears similarity to the discrete space ELBO derived in .

Finally, jump diffusions also have a long history of use in Bayesian inference, where one aims to draw samples from a trans-dimensional target posterior distribution based on an unnormalized version of its density : an ergodic jump diffusion is designed which admits the target as the invariant distribution . The invariant distribution is not preserved when time-discretizing the process. However, it was shown in  how general jump proposals could be built and how this process could be "Metropolized" to obtain a discrete-time Markov process admitting the correct invariant distribution, yielding the popular Reversible Jump Markov Chain Monte Carlo algorithm. Our setup differs significantly as we only have access to samples in the form of data, not an unnormalized target.

## 5 Experiments

### Molecules

We now show how our model provides significant benefits for diffusion guidance and interpolation tasks. We model the QM9 dataset  of 100K varying size molecules. Following , we consider each molecule as a 3-dimensional point cloud of atoms, each atom having the features: \((x,y,z)\) coordinates, a one-hot encoded atom type, and an integer charge value. Bonds are inferred from inter-atomic distances. We use an EGNN  backbone with three heads to predict \(s_{t}^{}\), \(p_{0|t}^{}(n_{0}|_{t})\), and \(A_{t}^{}\). We uniformly delete dimensions, \(K^{}(i|n)=1/n\), and since a point cloud is permutation invariant, \(A_{t}^{}(^{}|_{t})\) need only predict new dimension values. We set \(_{t}\) to a constant except for \(t<0.1T\), where we set \(_{t<0.1T}=0\). This ensures that all dimensions are added with enough generation time remaining for the diffusion process to finalize all state values.

We visualize sampling from our learned generative process in Figure 2; note how the process jointly creates a suitable number of atoms whilst adjusting their positions and identities. Before moving on to apply diffusion guidance which is the focus of our experiments, we first verify our unconditional sample quality in Table 2 and find we perform comparably to the results reported in  which use an FDDM. We ablate our choice of \(_{t}\) by comparing with setting \(_{t}\) to a constant for all \(t\) and with setting \(_{t}=0\) for \(t<0.9T\) (rather than just for \(t<0.1T\)). We find that the constant \(_{t}\) performs worse due to the occasional component being added late in the generation process without enough time for the diffusion process to finalize its value. We find the \(_{t<0.9T}=0\) setting to have satisfactory sample quality however this choice of \(_{t}\) introduces issues during diffusion guided generation as we see next. Finally, we ablate the parameterization of Proposition 3 by learning \(_{t}^{}(_{t})\) directly as the output of a neural network head. We find that this reduces sample quality due to the more well-behaved nature of the target, \(p_{0|t}^{}(n_{0}|_{t})\) when using Proposition 3. We note pure autoregressive models perform significantly worse than diffusion based models as found in .

Figure 2: Visualization of the jump-diffusion backward generative process on molecules.

#### 5.1.1 Trans-Dimensional Diffusion Guidance

We now apply diffusion guidance to our unconditional model in order to generate molecules that contain a certain number of desired atom types, e.g. 3 carbons or 1 oxygen and 2 nitrogens. The distribution of molecule sizes changes depending on these conditions. We generate molecules conditioned on these properties by using the reconstruction guided sampling approach introduced in . This method augments the score \(s_{t}^{}(_{t})\) such that it approximates \(_{_{t}} p_{t}(_{t}|y)\) rather than \(_{_{t}} p_{t}(_{t})\) (where \(y\) is the conditioning information) by adding on a term approximating \(_{_{t}} p_{t}(y|_{t})\) with \(p_{t}}}}(y|_{t})= _{n_{0}}_{_{0}}p(y|_{0})p_{0|t}(_{0}| _{t})_{0}\). This guides \(_{t}\) such that it is consistent with \(y\). Since \(_{t}^{}(_{t})\) has access to \(_{t}\), it will cause \(n_{t}\) to automatically also be consistent with \(y\) without the user needing to input any information on how the conditioning information relates to the size of the datapoints. We give further details on diffusion guidance in Appendix C.

We show our results in Table 3. In order to perform guidance on the FDDM baseline, we implement the model from  in continuous time and initialize the dimension from the empirically observed dimension distribution in the dataset. This accounts for the case of an end user attempting to guide a unconditional model with access to no further information. We find that TDDM produces samples whose dimensions much more accurately reflect the true conditional distribution of dimensions given the conditioning information. The \(_{t<0.9T}=0\) ablation on the other hand only marginally improves the dimension error over FDDM because all dimensions are added in the generative process at a time when \(_{t}\) is noisy and has little relation to the conditioning information. This highlights the necessity of allowing dimensions to be added throughout the generative process to gain the trans-dimensional diffusion guidance ability. The ablation with constant \(_{t}\) has increased dimension error over TDDM as we find that when \(_{t}>0\) for all \(t\), \(_{t}^{}\) can become very large when \(t\) is close to 0 when the model has perceived a lack of dimensions. This occasionally results in too many dimensions being added hence an increased dimension error. Not using the Proposition 3 parameterization also increases dimension error due to the increased difficulty in learning \(_{t}^{}\).

    & \% Atom & \% Molecule & \\ Method & Stable (\(\)) & Stable (\(\)) & \% Valid (\(\)) \\  FDDM  & \(\) & \(82.0\) & \(91.9\) \\  TDDM (ours) & \(98.3\) & \(\) & \(\) \\ TDDM, const \(_{t}\) & \(96.7\) & \(79.1\) & \(86.7\) \\ TDDM, \(_{t<0.9T}=0\) & \(97.7\) & \(82.6\) & \(89.4\) \\ TDDM w/o Prop. 3 & \(97.0\) & \(66.9\) & \(87.1\) \\   

Table 2: Sample quality metrics for unconditional molecule generation. An atom is stable if it has the correct valency whilst a molecule is considered stable if all of its atoms are stable. Molecular validity is measured using RDKit . All methods use 1000 simulation steps and draw 10000 samples.

    &  Dimension \\ Error (\(\)) \\  &  \% Atom \\ Stable (\(\)) \\  & 
 \% Molecule \\ Stable (\(\)) \\  & \% Valid (\(\)) \\  FDDM & \(0.511_{ 0.19}\) & \(93.5_{ 1.1}\) & \(31.3_{ 6.3}\) & \(65.2_{ 10.3}\) \\  TDDM & \(}\) & \(93.5_{ 2.6}\) & \(}\) & \(}\) \\ TDDM, const \(_{t}\) & \(0.226_{ 0.17}\) & \(88.9_{ 4.8}\) & \(43.6_{ 15}\) & \(63.4_{ 14}\) \\ TDDM, \(_{t<0.9T}=0\) & \(0.390_{ 0.38}\) & \(}\) & \(}\) & \(}\) \\ TDDM w/o Prop. 3 & \(0.219_{ 0.12}\) & \(}\) & \(55.0_{ 19}\) & \(73.8_{ 13}\) \\   

Table 3: Conditional Molecule Generation for 10 conditioning tasks that each result in a different dimension distribution. We report dimension error as the average Hellinger distance between the generated and ground truth dimension distributions for that property as well as average sample quality metrics. Standard deviations are given across the 10 conditioning tasks. We report in bold values that are statistically indistinguishable from the best result at the \(5\%\) level using a two-sided Wilcoxon signed rank test across the 10 conditioning tasks.

#### 5.1.2 Trans-Dimensional Interpolation

Interpolations are a unique way of gaining insights into the effect of some conditioning parameter on a dataset of interest. To create an interpolation, a conditional generative model is first trained and then sampled with a sweep of the conditioning parameter but using fixed random noise . The resulting series of synthetic datapoints share similar features due to the fixed random noise but vary in ways that are very informative as to the effect of the conditioning parameter. Attempting to interpolate with an FDDM is fundamentally limited because the entire interpolation occurs in the same dimension which is unrealistic when the conditioning parameter is heavily correlated with the dimension of the datapoint. We demonstrate this by following the setup of  who train a conditional FDDM conditioned on polarizability. Polarizability is the ability of a molecule's electron cloud to distort in response to an external electric field  with larger molecules tending to have higher polarizability. To enable us to perform a trans-dimensional interpolation, we also train a conditional version of our model conditioned on polarizability. An example interpolation with this model is shown in Figure 4. We find that indeed the size of the molecule increases with increasing polarizability, with some molecular substructures e.g. rings, being maintained across dimensions. We show how the dimension changes with polarizability during 3 interpolations in Figure 3. We find that these match the true dataset statistics much more accurately than interpolations using FDDM which first pick a dimension and carry out the entire interpolation in that fixed dimension.

### Video

We finally demonstrate our model on a video modeling task. Specifically we model the RoboDesk dataset , a video benchmark to measure the applicability of video models for planning and control problems. The videos are renderings of a robotic arm  performing a variety of different tasks including opening drawers and moving objects. We first train an unconditional model on videos of varying length and then perform planning by applying diffusion guidance to generate videos conditioned on an initial starting frame and a final goal frame . The planning problem is then reduced to "filling in" the frames in between. Our trans-dimensional model automatically varies the number of in-filled frames during generation so that the final length of video matches the length of time the task should take, whereas the fixed dimension model relies on the unrealistic assumption that the length of time the task should take is known before generation.

We model videos at \(32 32\) resolution and with varying length from \(2\) to \(35\) frames. For the network backbone, we use a UNet adapted for video . In contrast to molecular point clouds, our data is no longer permutation invariant hence \(A_{t}^{}(^{},i|_{t})\) includes a prediction over the location to insert the new frame. Full experimental details are provided in Appendix D. We evaluate our approach on three planning tasks, holding stationary, sliding a door and pushing an object. An example generation conditioned on the first and last frame for the slide door task is shown in Figure 5, with

Figure 4: Sequence of generations for linearly increasing polarizability from \(39\,^{3}\) to \(66\,^{3}\) with fixed random noise. Note how molecular size generally increases with polarizability and how some molecular substructures are maintained between sequential generations of differing dimension. For example, between molecules 6 and 7, the single change is a nitrogen (blue) to a carbon (gray) and an extra hydrogen (white) is added to maintain the correct valency.

Figure 3: Number of atoms versus polarizability for \(3\) interpolations with fixed random noise. The dataset mean and standard deviation for the number of atoms is also shown. FDDM interpolates entirely in a fixed dimensional space hence the number of atoms is fixed for all polarizabilities.

the model in-filling a plausible trajectory. We quantify our model's ability to generate videos of a length appropriate to the task in Table 4 finding on all three tasks we generate a more accurate length of video than FDDM which is forced to sample video lengths from the unconditional empirically observed length distribution in the training dataset.

## 6 Discussion

In this work, we highlighted the pitfalls of performing generative modeling on varying dimensional datasets when treating state values and dimensions completely separately. We instead proposed a trans-dimensional generative model that generates both state values and dimensions jointly during the generative process. We detailed how this process can be formalized with the time-reversal of a jump diffusion and derived a novel evidence lower bound training objective for learning the generative process from data. In our experiments, we found our trans-dimensional model to provide significantly better dimension generation performance for diffusion guidance and interpolations when conditioning on properties that are heavily correlated with the dimension of a datapoint. We believe our approach can further enable generative models to be applied in a wider variety of domains where previous restrictive fixed dimension assumptions have been unsuitable.

## 7 Acknowledgements

The authors are grateful to Martin Buttenschoen for helpful discussions. AC acknowledges support from the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1). AD acknowledges support of the UK Dstl and EPSRC grant EP/R013616/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative. He also acknowledges support from the EPSRC grants CoSines (EP/R034710/1) and Bayes4Health (EP/R018561/1). WH and CW acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program. This material is based upon work supported by the United States Air Force Research Laboratory (AFRL) under the Defense Advanced Research Projects Agency (DARPA) Data Driven Discovery Models (D3M) program (Contract No. FA8750-19-2-0222) and Learning with Less Labels (LwLL) program (Contract No.FA8750-19-C-0515). Additional support was provided by UBC's Composites Research Network (CRN), Data Science Institute (DSI) and Support for Teams to Advance Interdisciplinary Research (STAIR) Grants. This research was enabled in part by technical support and computational resources provided by WestGrid (https://www.westgrid.ca/) and Compute Canada (www.computecanada.ca). The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work. http://dx.doi.org/10.5281/zenodo.22558