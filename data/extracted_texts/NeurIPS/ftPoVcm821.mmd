# Demo2Code: From Summarizing Demonstrations

to Synthesizing Code via Extended Chain-of-Thought

 Huaxiaoyue Wang

Cornell University

yukiwang@cs.cornell.edu &Gonzalo Gonzalez-Pumariega

Cornell University

gg387@cornell.edu &Yash Sharma

Cornell University

ys749@cornell.edu &Sanjiban Choudhury

Cornell University

sanjibanc@cornell.edu

###### Abstract

Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an _extended chain-of-thought_ and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment. The project's website is at https://portal-cornell.github.io/demo2code/

## 1 Introduction

How do we program home robots to perform a wide variety of _personalized_ everyday tasks? Robots must learn such tasks online, through natural interactions with the end user. A user typically communicates a task through a combination of language instructions and demonstrations. This paper addresses the problem of learning robot task code from those two inputs. For instance, in Fig. 1, the user teaches the robot how they prefer to make a burger through both language instructions, such as "make a burger", and demonstrations, which shows the order in which the ingredients are used.

Recent works  have shown that Large Language Models (LLMs) are highly effective in using language instructions as prompts to plan robot tasks. However, extending LLMs to take demonstrations as input presents two fundamental challenges. The first challenge comes from demonstrations for long-horizon tasks. Naively concatenating and including all demonstrations in the LLM's prompt would easily exhaust the model's context length. The second challenge is that code for long-horizon robot tasks can be complex and require control flow. It also needs to check for physics constraints that a robot may have and be able to call custom perception and action libraries. Directly generating such code in a single step is error-prone.

_Our key insight is that while demonstrations are long and code is complex, they both share a latent task specification that the user had in mind._ This task specification is a detailed languagedescription of how the task should be completed. It is latent because the end user might not provide all the details about the desired task via natural language. We build an extended chain-of-thought  that recursively summarizes demonstrations to a compact specification, maps it to high-level code, and recursively expands the code by defining all the helper functions. Each step in the chain is small and easy for the LLM to process.

We propose a novel framework, Demo2Code, that generates robot task code from language instructions and demonstrations through a two-stage process (Fig. 1). _(1) Summarizing demonstrations to task specifications:_ Recursive summarization first works on each demonstration individually. Once all demonstrations are compactly represented, they are then jointly summarized in the final step as the task specification. This approach helps prevent each step from exceeding the LLM's maximum context length. _(2) Synthesizing code from the task specification:_ Given a task specification, the LLM first generates high-level task code that can call undefined functions. It then recursively expands each undefined function until eventually terminating with only calls to the existing APIs imported from the robot's low-level action and perception libraries. These existing libraries also encourage the LLM to write reusable, composable code.

Our key contributions are:

1. A method that first recursively summarizes demonstrations to a specification and then recursively expands specification to robot code via an extended chain-of-thought prompt.
2. A novel game simulator, Robotouille, designed to generate cooking tasks that are complex, long-horizon, and involve diverse food items, for benchmarking task code generation.
3. Comparisons against a range of baselines, including prior state of the art , on a manipulation benchmark, Robotouille, as well as a real-world human activity dataset.

## 2 Related Work

Controlling robots from natural language has a rich history [74; 66; 37], primarily because it provides a natural means for humans to interact with robots [5; 30]. Recent work on this topic can be categorized as semantic parsing [39; 30; 69; 67; 55; 40; 68], planning [60; 22; 23; 24; 61; 35; 34; 28], task specification [64; 19; 58; 12], reward learning [46; 56; 7], learning low-level policies [46; 2; 57; 56; 7], imitation learning [25; 38; 58; 64] and reinforcement learning [26; 18; 10; 45; 1]. However, these approaches fall in one of two categories: generating open-loop action sequences, or learning closed-loop, but short-horizon, policies. In contrast, we look to generate _task code_, which is promising in solving long-horizon tasks with control flows. The generated code also presents an interpretable way to control robots while maintaining the ability to generalize by composing existing functions.

Synthesizing code from language too has a rich history. Machine learning approaches offer powerful techniques for program synthesis [49; 4; 14]. More recently, these tasks are extended to general-purpose programming languages [79; 78; 8], and program specifications are fully described in natural English text [21; 3; 51]. Pretrained language models have shown great promise in code generation by exploiting the contextual representations learned from massive data of codes and texts [16; 11; 72; 71; 9; 47]. These models can be trained on non-MLE objectives , such as RL  to pass unit tests. Alternatively, models can also be improved through prompting methods

Figure 1: Overview of Demo2Code that converts language instruction and demonstrations to task code that the robot can execute. The framework recursively summarizes both down to a specification, then recursively expands the specification to an executable task code with all the helper functions defined.

such as Least-to-Most , Think-Step-by-Step  or Chain-of-Thought , which we leverage in our approach. Closest to our approach is CodeAsPolicies , that translates language to robot code. We build on it to address the more challenging problem of going from few demonstrations to code.

We broadly view our approach as inverting the output of code. This is closely related to _inverse graphics_, where the goal is to generate code that has produced a given image or 3D model [76; 36; 15; 70; 17]. Similar to our approach  trains an LSTM model that takes as input multiple demonstrations, compresses it to a latent vector and decodes it to domain specific code. Instead of training custom models to generate custom code, we leverage pre-trained LLMs that can generalize much more broadly, and generate more complex Python code, even create new functions. Closest to our approach  uses pre-trained LLMs to summarize demonstrations as rules in _one step_ before generating code that creates a sequence of pick-then-place and pick-then-toss actions. However, they show results on short-horizon tasks with small number of primitive actions. We look at more complex, long-horizon robot tasks, where demonstrations cannot be summarized in one step. We draw inspiration from [75; 50; 43] to recursively summarize demonstrations until they are compact.

## 3 Problem Formulation

We look at the concrete setting where a robot must perform a set of everyday tasks in a home, like cooking recipes or washing dishes, although our approach can be easily extended to other settings. We formalize such tasks as a Markov Decision Process (MDP), \(<,,,>\), defined below:

* **State** (\(s\)) is the set of all objects in the scene and their propositions, e.g. open(obj) ("obj is open"), on-top(obj1, obj2) ("obj1 is on top of obj2").
* **Action** (\(a\)) is a primitive action, e.g. pick(obj) ("pick up obj"), place(obj, loc) ("place obj on loc"), move(loc1, loc2) ("move from loc1 to loc2").
* **Transition function** (\((.|s,a)\)) specifies how objects states and agent changes upon executing an action. The transition is stochastic due to hidden states, e.g. cut('lettuce') must be called a variable number of times till the state changes to is_cut('lettuce').
* **Reward function** (\(r(s,a)\)) defines the task, i.e. the subgoals that the robot must visit and constraints that must not be violated.

We assume access to state-based demonstrations because most robotics system have perception modules that can parse raw sensor data into predicate states [42; 27]. We also assume that a system engineer provides a perception library and an action library. The perception library uses sensor observations to maintain a set of state predicates and provides helper functions that use these predicates (e.g. get_obj_location(obj), is_cooked(obj)). Meanwhile, the action library defines a set of actions that correspond to low-level policies, similar to [33; 61; 77; 80].

The goal is to learn a policy \(_{}\) that maximizes cumulative reward \(J(_{})=_{_{}}[_{t=1}^{T}[r(s_{t},a_{t})]]\), \(\) being the parameters of the policy. We choose to represent the policy as code \(\) for a number of reasons: code is interpretable, composable, and verifiable.

In this setting, the reward function \(r(s,a)\) is not explicit, but implicit in the task specification that the user has in mind. Unlike typical Reinforcement Learning (RL), where the reward function is hand designed, it is impractical to expect everyday users to program such reward functions for every new task that they want to teach their robots. Instead, they are likely to communicate tasks through natural means of interaction such as language instructions \(l\) (e.g. "Make a burger"). We can either use a model to generate reward \(r(s,a)\) from \(l\) or directly generate the optimal code \(\).

However, language instructions \(l\) from everyday users can be challenging to map to precise robot instructions [63; 44; 81]: they may be difficult to ground, may lack specificity, and may not capture users' intrinsic preferences or hidden constraints of the world. For example, the user may forget to specify how they wanted their burger done, what toppings they preferred, etc. Providing such level of detail through language every time is taxing. A more scalable solution is to pair the language instruction \(l\) with demonstrations \(=\{s_{1},s_{2},,s_{T}\}\) of the user doing the task. The state at time-step \(t\) only contains the propositions that have changed from \(t-1\) to \(t\). Embedded in the states are specific details of how the user wants a task done.

Our goal is to infer the most likely code given both the language and the demonstrations: \(_{}P(|l,)\). For a long-horizon task like cooking, each demonstration can become long. Naively concatenating all demonstrations together to query the LLM can either exhaust the model's

[MISSING_PAGE_EMPTY:4]

The goal of this stage is to summarize the set of demonstrations provided by the user into a compact specification (refer to summarize(demos) in Algorithm 1). Each demonstration is first independently summarized until the LLM determines that the demonstration can no longer be compressed, then the summaries are concatenated and summarized together. Fig. 2 shows example interim outputs during this stage. First, states in each demonstration get summarized into low-level actions (e.g. "patty6 is cooked" is summarized as "robot1 cooked patty6.") Then, low-level actions across time-steps are summarized into high-level subtasks, such as stacking, cutting, (e.g. "At state 3-8, the high level subtask is cook..."). The LLM determines to stop recursively summarizing after the entire demonstration gets converted to high-level subtasks, but this can have a different stopping condition (e.g. setting a maximum step) for task settings different than Fig. 2's. Next, these demonstrations' summaries are concatenated together for the LLM to generate the task specification. The LLM is prompted to first perform some intermediate reasoning to extract details on personal preferences, possible control loops, etc. For instance, the LLM aggregates high-level subtasks into an ordered list, which empirically helps the model to identify repeated subsets in that list and reason about control loops. An example final specification is shown in Fig. 2, which restates the language instruction first, then states "Specifically:.." followed by a more detailed instruction of the task.

### Stage 2: Recursively Expand Specification to Task Code

The goal of this stage is to use the generated specification from stage 1 to define all the code required for the task (see expand_code(code) in Algorithm 1). The LLM is prompted to first generate high-level code that calls functions that may be undefined. Subsequently, each of these undefined functions in the code is recursively expanded. Fig. 3 shows an example process of the code generation pipeline. The input is the specification formatted as a docstring. We import custom robot perception and control libraries for the LLM and also show examples of how to use such libraries in the prompt. The LLM first generates a high-level code, _that can contain new functions_, e.g. cook_obj_at_loc, that it has not seen in the prompt or import statements before. It expands this code by calling additional functions (e.g. move_then_pick), which it defines in the next recursive step. The LLM eventually reaches the base case when it only uses imported APIs to define a function (e.g. move_then_pick).

## 5 Experiments

### Experimental Setup

Baselines and MetricsWe compare our approach **Demo2Code** against prior work, CodeAsPolicies , which we call **Lang2Code**. This generates code only from language instruction. We also compare against **DemoNoLang2Code** that generates code from demonstrations without a language instruction, which is achieved by modifying the LLM prompts to redact the language. Finally, we also compare to an oracle **Spec2Code**, which generates task code from detailed specifications on how to complete a task. We use gpt-3.5-turbo-16k for all experiments with temperature 0.

We evaluate the different methods across three metrics. **Execution Success Rate** is the average 0/1 success of whether the generated code can run without throwing an error. It is independent from whether the goal was actually accomplished. **Unit Test Pass Rate** is based on checking whether all subgoals are achieved and all constraints are satisfied. The unit test module checks by examining the state transitions created from executing the generated code. **Code BLEU score** is the BLEU

Figure 3: Recursive expansion of the high-level code generated from the specification, where new functions are defined by the LLM along the way. (Stage 2)

score  between a method's generated code and the oracle Spec2Code's generated code. We tokenize each code by the spaces, quotations, and new lines.

Tabletop Manipulation Simulator We build upon a physics simulator from , which simulates a robot arm manipulating blocks and cylinders in different configurations. The task objectives are to place objects at specific locations or stack objects on top of each other. The LLM has access to action primitives (e.g. pick and place) and perception modules (e.g. to get all the objects in the scene). We create a range of tasks that vary in complexity and specificity, use the oracle Spec2Code to generate reference code, and execute that code to get demonstrations for other methods. For each task, we test the generated code for 10 random initial conditions of objects.

    &  &  &  \\  & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. \\   & Place A next to B & 1.00 & 0.33 & 0.73 & 0.90 & 0.80 & 0.82 & 1.00 & 1.00 & 0.98 \\  & Place A at a corner of the table & 1.00 & 0.30 & 0.08 & 1.00 & 1.00 & 0.85 & 1.00 & 1.00 & 1.00 \\  & Place A at an edge of the table & 1.00 & 0.20 & 0.59 & 1.00 & 0.95 & 0.84 & 1.00 & 1.00 & 0.84 \\   & Place A on top of B & 1.00 & 0.03 & 0.23 & 0.60 & 0.70 & 0.56 & 0.90 & 0.40 & 0.40 \\  & Stack all blocks & 1.00 & 0.20 & 0.87 & 1.00 & 0.70 & 0.50 & 1.00 & 0.70 & 0.50 \\  & Stack all cylinders & 1.00 & 0.37 & 0.89 & 1.00 & 0.83 & 0.49 & 1.00 & 1.00 & 1.00 \\   & Stack all blocks into one stack & 1.00 & 0.13 & 0.07 & 1.00 & 0.67 & 0.52 & 1.00 & 0.87 & 0.71 \\  & Stack all cylinders into one stack & 1.00 & 0.13 & 0.00 & 0.90 & 0.77 & 0.19 & 1.00 & 0.90 & 0.58 \\   & Stack all objects into two stacks & 1.00 & 0.00 & 0.00 & 1.00 & 0.90 & 0.68 & 1.00 & 0.90 & 0.65 \\   & Overall & 1.00 & 0.19 & 0.39 & 0.93 & 0.81 & 0.60 & 0.99 & 0.88 & 0.77 \\   & & & & & & & & & & & \\   

Table 1: Results for Tabletop Manipulation simulator. The tasks are categories into 3 clusters: Specificity (“Specific”), Hidden World Constraint (“Hidden”), and Personal Preference (“Pref”).

   Task &  &  &  & Horizon \\  & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Exec. & Pass. & BLEU. & Length \\  Cook a party & 1.00 & 1.00 & 0.90 & 1.00 & 1.00 & 0.90 & 1.00 & 1.00 & 0.90 & 8.0 \\ Cook two paties & 0.80 & 0.80 & 0.92 & 0.80 & 0.80 & 0.92 & 0.80 & 0.80 & 0.92 & 16.0 \\ Stack a top ban on top of a cut lepton Run & 1.00 & 1.00 & 0.70 & 0.00 & 0.00 & 0.75 & 1.00 & 1.00 & 0.60 & 14.0 \\  Cut a lettuce & 1.00 & 1.00 & 0.87 & 0.00 & 0.00 & 0.76 & 1.00 & 1.00 & 0.87 & 7.0 \\ Cut two lettences & 0.80 & 0.80 & 0.92 & 0.00 & 0.00 & 0.72 & 0.80 & 0.80 & 0.92 & 14.0 \\ Cook first then cut & 1.00 & 1.00 & 0.88 & 1.00 & 1.00 & 0.88 & 1.00 & 1.00 & 0.88 & 14.0 \\ Cut first one cook & 1.00 & 1.00 & 0.88 & 0.00 & 0.00 & 0.82 & 1.00 & 1.00 & 0.88 & 15.0 \\ Assemble two barges one by one & 0.00 & 0.00 & 0.34 & 1.00 & 1.00 & 0.77 & 1.00 & 1.00 & 0.76 & 15.0 \\ Assemble two barges in parallel & 0.00 & 0.00 & 0.25 & 1.00 & 1.00 & 0.51 & 0.00 & 0.00 & 0.71 & 15.0 \\ Make a cheese buyer & 1.00 & 0.00 & 0.04 & 1.00 & 1.00 & 0.69 & 1.00 & 1.00 & 0.69 & 18.0 \\ Make a chicken burger & 0.00 & 0.00 & 0.57 & 0.00 & 0.00 & 0.64 & 0.90 & 0.90 & 0.69 & 25.0 \\ Make a barger stacking lettuce aop party immediately & 1.00 & 0.00 & 0.74 & 0.20 & 0.00 & 0.71 & 0.00 & 0.00 & 0.71 & 24.5 \\ Make a burger stacking party auto lettuce immediately & 0.00 & 0.00 & 0.74 & 0.20 & 0.00 & 0.71 & 1.00 & 1.00 & 0.74 & 25.0 \\ Make a burger stacking lettuce aop party after preparation & 1.00 & 0.00 & 0.67 & 0.10 & 0.00 & 0.65 & 0.00 & 0.00 & 0.66 & 26.5 \\ Make a burger stacking party auto lettuce after preparation & 1.00 & 0.00 & 0.67 & 0.00 & 0.00 & 0.53 & 1.00 & 0.00 & 0.69 & 27.0 \\ Make a lettuce tomato burgers & 0.00 & 0.00 & 0.13 & 1.00 & 1.00 & 0.85 & 1.00 & 0.00 & 0.66 & 34.0 \\ Make two cheese barges & 0.00 & 0.00 & 0.63 & 1.00 & 1.00 & 0.68 & 1.00 & 1.00 & 0.68 & 38.0 \\ Make two chicken burgers & 0.00 & 0.00 & 0.52 & 0.00 & 0.00 & 0.68 & 1.00 & 0.00 & 0.56 & 50.0 \\ Make two burgers stacking lettuce aop party immediately & 0.80 & 0.00 & 0.66 & 0.80 & 1.00 & 0.69 & 0.00 & 0.00 & 0.66 & 50.0 \\ Make two burgers stacking party auto lettuce immediately & 0.80 & 0.00 & 0.67 & 1.00 & 0.00 & 0.48 & 1.00 & 1.00 & 0.73 & 50.0 \\ Make two burgers stacking lettuce aop party after preparation & 0.80 & 0.00 & 0.66 & 0.00 & 0.00 & 0.66 & 0.80 & 0.00 & 0.67 & 54.0 \\ Make two burgers stacking party auto lettuce after preparation & 0.80 & 0.00 & 0.67 & 0.50 & 0.00 & 0.71 & 0.80 & 0.00 & 0.68 & 54.0 \\ Make two lettuce tomato burgers & 1.00 & 0.00 & 0.55 & 0.00 & 0.00 & 0.70 & 1.00 & 1.00 & 0.84 & 70.0 \\  Overall & 0.64 & 0.29 & 0.64 & 0.49 & 0.38 & 0.71 & 0.79 & 0.59 & 0.74 & 28.8 \\   

Table 2: Results for the Robotouille simulator. The training tasks in the prompt are at the top of the table and highlighted in gray. All tasks are ordered by the horizon length (the number of states). Below the table shows four Robotouille tasks where the environments gradually become more complex.

Cooking Task Simulator: Robotouille1 We introduce a novel, open-source simulator to simulate complex, long-horizon cooking tasks for a robot, e.g. making a burger by cutting lettuce and cooking parties. Unlike existing simulators that focus on simulating physics or sensors, Robotouille focuses on high level task planning and abstracts away other details. We build on a standard backend, PDDLGym , with a user-friendly game as the front end to easily collect demonstrations. For the experiment, we create a set of tasks, where each is associated with a set of preferences (e.g. what a user wants in the burger, how the user wants the burger cooked). For each task and each associated preference, we procedurally generate 10 scenarios.

EPIC-Kitchens Dataset EPIC-Kitchens is a real-world, egocentric video dataset of users doing tasks in their kitchen. We use this to test if Demo2Code can infer users' preferences from real videos, with the hopes of eventually applying our approach to teach a real robot personalized tasks. We focus on dish washing as we found preferences in it easy to qualify. While each video has annotations of low-level actions, these labels are insufficient for describing the tasks. Hence, we choose \(7\) videos of \(4\) humans washing dishes and annotate each demonstration with dense state information. We compare the code generated by Lang2Code, DemoNoLang2Code and Demo2Code on whether it satisfies the annotated preference and how well it matches against the reference code.

### Results and Analysis

Overall, Demo2Code has the closest performance to the oracle (Spec2Code). Specifically, our approach has the highest unit test pass rates in all three benchmarks, as well as the highest execution success in Robotouille (table 2) and EPIC-Kitchens (table 3). Meanwhile, Lang2Code has a higher overall execution success than Demo2Code for the Tabletop simulator (table 1). However, Lang2Code has the lowest unit test pass rate among all baselines because it cannot fully extract users' specifications without demonstrations. DemoNoLang2Code has a relatively higher pass rate, but it sacrifices execution success because it is difficult to output plausible code without context from language. We provide prompts, detailed results and ablations in the Appendix.2 We now ask a series of questions of the results to characterize the performance difference between the approaches.

How well does Demo2Code generalize to unseen objects and tasks?Demo2Code exhibits its generalization ability in three axes. First, Demo2Code generalizes and solves unseen tasks with longer horizons and more predicates compared to examples in the prompt at train time. For Robotouille, table 2 shows the average horizon length for each training task (highlighted in gray) and testing task. Overall, the training tasks have an average of 12.7 states compared the testing tasks (31.3 states). Compared to the baselines, Demo2Code performs the best for long burger-making tasks (an average of 32 states) even though the prompt does not show this type of task. Second, Demo2Code uses control flow, defines hierarchical code, and composes multiple subtasks together to solve these long-horizon tasks. The appendix details the average number of loops, conditionals, and helper functions that Demo2Code generates for tabletop simulator (in section 8.3) and Robotouille (in section 9.3). Notably, Demo2Code generates code that uses a for-loop for the longest task (making two lettuce tomato burgers with 70 states), which requires generalizing to unseen subtasks (e.g. cutting tomatoes) and composing 7 distinct subtasks. Third, Demo2Code solves tasks that contain unseen objects or a different number of objects compared to the training tasks in the prompt. For Robotouille, the prompt only contains examples of preparing burger patties and lettuce, but Demo2Code still has the highest unit test pass rate for making burgers with unseen ingredients: cheese, chicken, and

    &  &  &  &  &  &  &  \\   & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU & Pass. & BLEU \\  Lang2Code & 1.00 & 0.58 & 0.00 & 0.12 & 0.00 & 0.84 & 0.00 & 0.48 & 0.00 & 0.37 & 1.00 & 0.84 & 0.00 & 0.66 \\ DenoNlang2Code & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.37 & 0.00 & 0.51 & 1.00 & 0.57 & 0.00 & 0.00 \\ Demo2Code & 1.00 & 0.33 & 0.00 & 0.19 & 1.00 & 0.63 & 1.00 & 0.43 & 1.00 & 0.66 & 1.00 & 0.58 & 0.00 & 0.24 \\   

Table 3: Results for EPIC-Kitchens dataset on 7 different user demonstrations of dish-washing (length of demonstration in parentheses). The unit test pass rate is evaluated by a human annotator, and BLEU score is calculated between each method’s code and the human annotator’s reference code.

tomatoes. Similarly, for tabletop (table 1), although the prompt only contains block-stacking tasks, our approach maintains high performance for cylinder-stacking tasks.

Is Demo2Code able to ground its tasks using demonstrations?Language instructions sometimes cannot ground the tasks with specific execution details. Since demonstrations provide richer information about the task and the world, we evaluate whether Demo2Code can utilize them to extract details. Tasks under the "Specific" cluster in Table 1 show cases when the LLM needs to use demonstrations to ground the desired goal. Fig. 4 illustrates that although the language instruction ("Place the purple cylinder next to the green block") does not ground the desired spatial relationship between the two objects, our approach is able to infer the desired specification ("to the left"). In contrast, Lang2Code can only randomly guess a spatial relationship, while DemoNoLang2Code can determine the relative position, but it moved the green block because it does not have language instruction to ground the overall task. Similarly, tasks under the "Hidden" cluster in Table 1 show how Demo2Code outperforms others in inferring hidden constraints (e.g the maximum height of a stack) to ground its tasks.

Is Demo2Code able to capture individual user preference?As a pipeline for users to teach robots personalized tasks, Demo2Code is evaluated on its ability to extract a user's preference. Table 3 shows that our approach performs better than Lang2Code in generating code that matches each EPIC-Kitchens user's dish washing preference, without overfitting to the demonstration like in DemoNoLang2Code. Because we do not have a simulator that completely matches the dataset, human annotators have to manually inspect the code. The code passes the inspection if it has correct syntax, does not violate any physical constraints (e.g. does not rinse a dish without turning on the tap), and matches the user's dish-washing preference. Qualitatively, Fig. 6 shows that our approach is able to extract the specification and generate the correct code respectively for user 22, who prefers to soap

Figure 4: Demo2Code successfully extracts specificity in tabletop tasks. Lang2Code lacks demonstrations and randomly chooses a spatial location while DemoNoLang2Code lacks context in what the demonstrations are for.

Figure 5: Demo2Code summarizes demonstrations and identify different users’ preferences on how to make a burger (e.g. whether to include lettuce or cheese) in Robotouille simulator. Then, it generates personalized burger cooking code to use the user’s preferred ingredients.

all objects before rinsing them, and user 30, who prefers to soap then rinse each object individually. Similarly, Fig. 5 provides an example of how Demo2Code is able to identify a user's preference of using cheese vs lettuce even when the language instruction is just "make a burger." Quantitatively, Table 2 shows more examples of our approach identifying a user's preference in cooking order, ingredient choice, etc, while Table 1 also shows our approach performing well in tabletop tasks.

How does chain-of-thought compare to directly generating code from demonstrations?To evaluate the importance of our extended chain-of-thought pipeline, we conduct ablation by varying the length of the chain on three clusters of tasks: short-horizon (around 2 states), medium-horizon (5-10 states), and long-horizon (\( 15\) states). We compare the unit test pass rate on four different chain lengths, ranging from **No chain-of-thought** (the shortest), which directly generates code from demonstrations, to **Full** (the longest), which represents our approach Demo2Code. The left bar plot in Fig. 7 shows that directly generating code from demonstrations is not effective, and the LLM performs better as the length of the chain increases. The chain length also has a larger effect on tasks with longer horizons. For short-horizon tasks, the LLM can easily process the short demonstrations and achieve high performances by just using **1-step**. Meanwhile, the stark difference between **2-steps** and **Full**'s results on long-horizon tasks emphasizes the importance of taking as many small steps as the LLM needs in summarizing long demonstrations so that it will not lose key information.

How do noisy demonstrations affect Demo2Code's performance?We study how Demo2Code performs (1) when each predicate has a 10% chance to be removed from the demonstrations, and (2) when each state has a 10% chance to be completely removed. Fig. 7's table shows that Demo2Code's overall performance does not degrade even though demonstrations are missing information. While

Figure 6: Demo2Code summarizes different styles of users washing dishes from demonstration (how to soap and rinse objects) in EPIC-Kitchens, and generates personalized dish washing code.

Figure 7: (Left) Unit test result for ablating different degrees of chain-of-thought across tasks with short, medium, long horizon. (Right) Demo2Code’s unit test result for Robotouille demonstrations with different level of noises: (1) each predicate has 10% chance of being dropped, and (2) each state has 10% chance of being completely dropped. We ran the experiment 4 times and report the average and variance.

removing predicates or states worsen Demo2Code's performance for shorter tasks (e.g. cook and cut), they surprisingly increase the performance for longer tasks. Removing any predicates can omit essential information in shorter tasks' demonstrations. Meanwhile, for longer tasks, the removed predicates are less likely to be key information, while reducing the length of demonstrations. Similarly, for the longest tasks to make two burgers, one burger's missing predicates or states can be explained by the other burger's demonstration. In section 11, we show a specific example of this phenomenon. We also study the effect of adding additional predicates to demonstrations, which has degraded Demo2Code's performance from satisfying 5 users' preferences to 2 users' in EPIC-Kitchens.

## 6 Discussion

In this paper, we look at the problem of generating robot task code from a combination of language instructions and demonstrations. We propose Demo2Code that first recursively summarizes demonstrations into a latent, compact specification then recursively expands code generated from that specification to a fully defined robot task code. We evaluate our approach against prior state-of-the-art  that generates code only from language instructions, across 3 distinct benchmarks: a tabletop manipulation benchmark, a novel cooking game Robotouille, and annotated data from EPIC-Kitchens, a real-world human activity dataset. We analyze various capabilities of Demo2Code, such as grounding language instructions, generalizing across tasks, and capturing user preferences.

Demo2Code can generalize across complex, long-horizon tasks. Even though Demo2Code was shown only short-horizon tasks, it's able to generalize to complex, long demonstrations. Recursive summarization compresses long chains of demonstrations and recursive expansion generates complex, multi-layered code.

Demo2Code leverages demonstrations to ground ambiguous language instructions and infer hidden preferences and constraints. The latent specification explicitly searches for missing details in the demonstrations, ensuring they do not get explained away and are captured explicitly in the specification.

Demo2Code strongly leverages chain-of-thought. Given the complex mapping between demonstrations and code, chain-of-thought plays a critical role in breaking down computation into small manageable steps during summarization, specification generation and code expansion.

In future directions, we are looking to close the loop on code generation to learn from failures, integrate with a real home robot system and run user studies with Robotouille.

## 7 Limitations

Demo2Code is limited by the capability of LLMs. Recursive summarization assumes that once all the demonstrations are sufficiently summarized, they can be concatenated to generate a specification. However, in extremely long horizon tasks (e.g. making burgers for an entire day), it is possible that the combination of all the sufficiently summarized demonstrations can still exceed the maximum context length. A future work direction is to prompt the LLM with chunks of the concatenated demonstrations and incrementally improve the specifications based on each new chunk. In recursive expansion, our approach assumes that all low-level action primitives are provided. Demo2Code currently cannot automatically update its prompt to include any new action. Another direction is to automatically build the low-level skill libraries by learning low-level policy via imitation learning and iteratively improve the code-generation prompt over time. Finally, since LLMs are not completely reliable and can hallucinate facts, it is important to close the loop by providing feedback to the LLM when they fail. One solution [62; 52] is to incorporate feedback in the query and reprompt the language model. Doing this in a self-supervised manner with a verification system remains an open challenge.

In addition, the evaluation approach for Demo2Code or other planners that generate code [33; 61; 77] is different from the one for classical planners [53; 54]. Planners that generate code measure a task's complexity by the horizon length, the number of control flows, whether that task is in the training dataset, etc. Meanwhile, many classical planners use domain specific languages such as Linear Temporal Logic (LTL) to specify tasks , which leads to categorizing tasks and measuring the task complexity based on LTL. Future work needs to resolve this mismatch in evaluation standards.