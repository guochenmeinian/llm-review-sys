# Bridging Gaps: Federated Multi-View Clustering in Heterogeneous Hybrid Views

Xinyue Chen\({}^{1}\)   Yazhou Ren\({}^{1,2,}\)1   Jie Xu\({}^{1}\)

**Fangfei Lin\({}^{1}\)   Xiaorong Pu\({}^{1,2}\)   Yang Yang\({}^{1}\)**

\({}^{1}\)School of Computer Science and Engineering, University of Electronic Science

and Technology of China, China; \({}^{2}\)Shenzhen Institute for Advanced Study,

University of Electronic Science and Technology of China, China

Corresponding author (yazhou.ren@uestc.edu.cn).

###### Abstract

Recently, federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Many existing approaches tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. While these methods have succeeded, they may encounter challenges in practical FedMVC scenarios involving heterogeneous hybrid views, where a mixture of single-view and multi-view clients exhibit varying degrees of heterogeneity. In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap. To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients. To address the view gap, we develop a global-specific weighting aggregation method, which encourages global models to learn complementary features from hybrid views. The interplay between local-synergistic contrastive learning and global-specific weighting aggregation mutually enhances the exploration of the data cluster structures distributed on multiple clients. Theoretical analysis and extensive experiments demonstrate that our method can handle the heterogeneous hybrid views in FedMVC and outperforms state-of-the-art methods. The code is available at https://github.com/SMartina5/FMCSC.

## 1 Introduction

Recent advancements in sensors and the internet have allowed many distributed clients to collect unlabeled data from multiple views/modalities. Utilizing these unlabeled data while considering the need for data privacy among clients has given rise to an emerging field of federated multi-view clustering (FedMVC) , which enables multiple clients to collaboratively train consistent clustering models without exposing private data. The clustering models across distributed clients can be applied in many applications (e.g., recommendation  and medicine ) and thus FedMVC attracs increasing research interest .

Existing FedMVC methods tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. For instance, FedDMVC  assumes that a dataset with \(V\) views is distributed across \(V\) single-view clients, each having the same set of samples. FedMVFPC  assumes that the data are distributed among multiple multi-view clients, with each client having \(V\) views and non-overlapping samples among clients. Despite the achieved success, they may encounter challenges when handling some practical FedMVC scenarios involving heterogeneoushybrid views, where different clients are heterogeneous such that single-view clients and multi-view clients are hybrid.

This scenario is prevalent in real world situations [6; 47], for example, hospitals in metropolitan areas employ CT, X-ray, and EHR for disease detection, whereas remote areas usually rely on a single detection method. Similarly, smartphones can simultaneously capture audio and images, but recording devices are limited to collecting audio data only.

The presence of heterogeneous hybrid views might limit the applicability of previous FedMVC methods. We decompose these challenges into two issues. (a) _Client gap_. Multi-view clients collect multiple views that have the opportunity to learn comprehensive cluster partitions by leveraging multi-view information. Conversely, single-view clients only own single views and easily obtain biased cluster partitions if the single view only contains marginal information. (b) _View gap_. Hybrid views collected by different clients have quality differences (e.g., images might contain richer visual information than texts, but texts could have semantic information), and extracting complementary information from different views across all clients is not trivial.

In this paper, we introduce a novel FedMVC method, namely Federated Multi-view Clustering via Synergistic Contrast (FMCSC), which can simultaneously leverage the single-view and multi-view data across heterogeneous clients to mine clustering structures from hybrid views. Figure 1 shows an overview of our FMCSC framework. First, we note that lacking unified information supervision, naive aggregation of local models may lead to model misalignment. Therefore, we propose cross-client consensus pre-training to align the local models on all clients to avoid their misalignment. Second, to address the client gap, we design local-synergistic contrastive learning that helps to mitigate the heterogeneity between single-view clients and multi-view clients. In particular, we leverage feature-level and model-level contrastive learning to align multi-view clients and single-view clients, respectively. Third, to tackle the view gap, we develop the global-specific weighting aggregation which encourages global models to learn robust features from hybrid views, further exploring complementary cluster structures. The local-synergistic contrastive learning and global-specific weighting aggregation promote each other to explore the data cluster structures distributed on multiple clients. Overall, FMCSC effectively facilitates all clients in bridging the client gap and view gap within the heterogeneous hybrid views through theoretical and experimental analysis.

Our main contributions are as follows:

* We propose a novel FedMVC method that can handle the heterogeneous hybrid views and explain the success mechanism of the proposed method through theoretical analysis from the perspective of bridging client and view gaps.
* We design local-synergistic contrastive learning and global-specific weighting aggregation, using mutual information as a bridge to connect local and global models, together to explore the cluster structures in multi-view data distributed on different clients.
* Theoretical and experimental analyses verify the effectiveness of FMCSC, which shows excellent clustering performance under various federated learning scenarios.

## 2 Related Work

Federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Existing FedMVC methods can be classified into two categories based on the partitioning of multi-view data among clients. (1) Vertical FedMVC assumes that a dataset with \(V\) views is distributed across \(V\) single-view clients, each having the same set of samples. Robust federated multi-view learning (FedMVL)  addresses high communication costs, fault tolerance, and stragglers. Federated deep multi-view clustering (FedDMVC)  focuses on addressing the challenges of feature heterogeneity and incompleteness. Existing vertical FedMVC methods still rely on the idealistic assumption that different views of the same sample can be aligned across clients, which warrants further investigation. (2) Horizontal FedMVC assumes that the data are distributed among multiple multi-view clients, with each client having \(V\) views and non-overlapping samples among clients. Federated multi-view fuzzy c-means consensus prototypes clustering (FedMVFPC)  utilizes federated learning mechanisms to perform fuzzy c-means clustering on multi-view data. Horizontal federated multi-view learning (H-FedMV)  aims to improve the local disease prediction performance by sharing training models among clients. Although existing approaches have been successful, they may encounter challenges in practical FedMVC scenarios with heterogeneous hybrid views. Specifically, heterogeneity refers to the differences among clients, where single-view and multi-view clients coexist. The hybrid views indicate the uncertainty in the number and quality of views involved in training. Our proposed FMCSC is a variant of horizontal FedMVC, and can handle such scenarios by bridging the client gap and the view gap among clients.

## 3 Methodology

The key goal of FMCSC is to bridge client and view gaps. On the one hand, multi-view clients have the opportunity to learn comprehensive cluster partitions by leveraging multi-view information. Single-view clients aim to bridge the client gap between themselves and multi-view clients, thereby avoiding obtaining biased clustering partitions. On the other hand, considering the inherent discrepancies in data quality among different views, our objective is to extract complementary information from hybrid views across all clients.

### Problem Definition

We consider a heterogeneous federated learning setting where \(M\) multi-view clients and \(S\) single-view clients collaborate to train and mine complementary clustering structures using multiple heterogeneous global models \(\{f_{g}^{1}(;^{1}),,f_{g}^{V}( ;^{V}),f_{g}(;)\}\). Here, \(f_{g}^{v}(;^{v})\) represents the global model capable of handling the \(v\)-th view type, and \(f_{g}(;)\) represents the global model capable of handling multi-view data. Each single-view client \(p[S]\) has its private dataset \(_{p}=\{_{i}^{v}\}_{i=1}^{|_{p}|}\), where \(_{i}^{v}\) represents the \(i\)-th sample of the \(p\)-th single-view client collected from the \(v\)-th view type. It adopts a small model \(f_{p}(;_{p}^{v}):^{D_{v}}^{d}\) based on its local view types. The multi-view client \(m[M]\) has its local dataset \(_{m}=\{(_{1}^{1},_{i}^{2},, _{i}^{V})\}_{i=1}^{|_{m}|}\), where \((_{i}^{1},_{i}^{2},,_{i}^{V})\) represents the \(i\)-th sample of the \(m\)-th multi-view client collected from \(V\) different view types. It trains a small model \(f_{m}(;_{m}):^{_{v=1}^{V}D_{v}} ^{d}\) based on its local data. For simplicity, we assume that the output features of all models have the same dimension \(d\).

Figure 1: The framework of FMCSC. Initially, each client conducts cross-client consensus pre-training to alleviate model misalignment (Section 3.2). Then, all clients begin training using the designed local-synergistic contrast (Section 3.3) and upload their local models to the server. The server performs global-specific weighting aggregation and distributes multiple heterogeneous global models to all clients (Section 3.4). Finally, leveraging global models received from the server, clients discover complementary cluster structures across all clients.

### Cross-Client Consensus Pre-training

Multi-view datasets often contain redundancy and random noise. Current mainstream methods employ self-supervised autoencoder models, such as autoencoder (AE)  and variational autoencoder (VAE)  to learn high-level features from raw data. In FMCSC, we employ an encoder-decoder pair, denoted \(E_{^{v}}^{v}()\) and \(D_{^{v}}^{v}()\) with learnable parameters \(^{v}\) and \(^{v}\), for each view in each client. For multi-view client \(m\), we define \(_{i}^{v}=E_{^{v}}^{v}(_{i}^{v}) ^{d_{v}}\) as the \(d_{v}\)-dimensional latent feature of the \(i\)-th sample, and the output of the autoencoder is \(}_{i}^{v}=D_{^{v}}^{v}(_{i}^{v}) ^{D_{v}}\). We calculate the reconstruction loss between the input \(_{i}^{v}\) and the output \(}_{i}^{v}\) for all samples in this client. Additionally, the local model of this client consists of \(V\) encoder-decoder pairs, which can be pre-trained by minimizing the reconstruction objective:

\[_{r}^{m}=_{m}|}_{v=1}^{V}_{ i=1}^{|_{m}|}\|_{i}^{v}-D_{^{v}}^{v} (E_{^{v}}^{v}(_{i}^{v}))\|_{2}^{2}.\] (1)

Similarly, for single-view client \(p\) that contains a type of view data locally, it is sufficient to construct an encoder-decoder pair. Pre-training can be performed using the same objective as in Eq. (1):

\[_{r}^{p}=_{p}|}_{i=1}^{| _{p}|}\|_{i}^{v}-D_{^{v}}^{v}(E_{ ^{v}}^{v}(_{i}^{v}))\|_{2}^{2}.\] (2)

**Key Observations:** In federated learning, diverse local data distributions frequently lead to model drift, causing slow and unstable convergence [19; 47]. In FMCSC, single-view clients have never encountered data from other view types, thus intensifying the issue of model drift due to heterogeneous hybrid views. Moreover, the absence of uniformly labeled data across all clients allows the reconstruction objective of autoencoders to optimize from multiple different directions. In feature space, this problem manifests itself as angular deviations among features , and from the perspective of model aggregation, it manifests itself as model misalignment. In other words, direct aggregation of models can blur the feature distinctions captured by local models, leading to inseparability among features, as shown in Figure 3.

A direct strategy to alleviate model misalignment is through alignment. Based on this naive idea, we propose that the multi-view client that finishes training first should distribute its network parameters \(\{E_{^{1}}^{1}(),,E_{^{V}}^{V}()\}\) and \(\{D_{^{1}}^{1}(),,D_{^{V}}^{V}()\}\) to the remaining clients. Each client then performs pre-training based on this model, thereby alleviating the model misalignment caused by unsupervised training. Notably, as pre-training solely involves training the autoencoder, the construction of local models is inherently dependent on the view type. Therefore, single-view clients can still refer to the network parameters of multi-view clients. This process facilitates consensus pre-training among the clients, ultimately leading to the uploading of pre-trained model parameters to the server. The server initializes global models based on the models uploaded by clients.

### Local-Synergistic Contrast

During pre-training, the features extracted through the reconstruction objective usually contain both common semantics and view-private information. The latter is often meaningless or even misleading, leading to poor clustering effectiveness. To mitigate the adverse effects of view-private information, each client also needs to design a consistent objective during training to learn common semantics.

For multi-view client \(m\), it possesses information from multiple views. Inspired by many previous works of MVC [40; 42; 44; 45], we employ feature contrastive learning to achieve consistency objectives. Considering the conflict between consistency and reconstruction objectives, we opt to operate in distinct feature spaces. We refer to the features obtained through the autoencoders \(\{(_{i}^{1},_{i}^{2},,_{i}^{V} )\}_{i=1}^{|_{m}|}\) as low-level features. Moreover, we stack \(V\) non-linear mappings \(\{^{1}(^{1};^{1}),,^ {V}(^{V};^{V})\}\) to obtain high-level features \(\{(_{1}^{1},_{i}^{2},,_{i}^{V} )\}_{i=1}^{|_{m}|}\). Additionally, a non-linear mapping \((;):^{_{v=1}^{V}d_{v}} ^{d}\) is constructed by:

\[=(;)=([ ^{1},^{2},,^{V}];),\] (3)where \(\{_{i}\}_{i=1}^{|_{m}|}=^{| _{m}| d}\), and \(^{|_{m}|_{v=1}^{V}d_{v}}\). We aim to preserve the representative capacity of low-level features to prevent model collapse while learning the common semantics \(\) among views in the high-level feature space.

In the high-level feature space, the common semantics \(\) are learned from all views and should be very similar to the common semantics learned from individual views. Based on this, we define \(\{_{i},_{j}^{v}\}_{j=i}^{v=1,,V}\) as \(V\) positive feature pairs, and the remaining \(\{_{i},_{j}^{v}\}_{j i}^{v=1,,V}\) are \(V(|_{m}|-1)\) negative feature pairs. Then, we use cosine similarity to measure the similarity of feature pairs:

\[sim(_{i},_{j}^{v})=_{i},_{j}^{v}}{\|_{i}\|\| _{j}^{v}\|},\] (4)

where \(,\) is dot product operator. We introduce the temperature parameter \(_{m}\) to moderate the effect of similarity. Subsequently, the feature contrastive loss is formulated as:

\[_{c}^{m}=-_{m}|}_{v=1}^{V}_{i=1}^{| _{m}|}_{i},_{j}^{v} )/_{m}}}{_{j i}e^{sim(_{i},_{j}^{v} )/_{m}}}.\] (5)

Moreover, multi-view clients aim to assist single-view clients in bridging client gaps and discarding view-private information detrimental to clustering. They optimize multiple heterogeneous global models using local data, ensuring that even the global model designed for single-view processing acquires generalized common semantics. Such common semantics are also advantageous for uncovering complementary clustering structures across clients. Concretely,

\[_{\{_{m}^{v}\}_{v=1}^{V}_{v=1}^{V}\|f_{m}^{ v}(;_{m}^{v})-f_{m}(;_{m}) \|_{2}^{2}},\] (6)

where \(f_{m}^{v}(;_{m}^{v})\) represents the local model that can handle the \(v\)-th type of view, which is initialized by the global model \(f_{g}^{v}(;^{v})\). \(f_{m}(;_{m})\) represents the local model of multi-view client \(m\), where multi-view clients possess data of all view types.

For single-view client \(p\), where each sample only has a single view, we design a model contrastive learning to achieve consistency objectives. Specifically, client \(p\) contains data of the \(v\)-th view type \(\{_{i}^{v}\}_{i=1}^{|_{p}|}\), with its local model as \(f_{p}(;^{v})\). To explore common semantics, we adopt the same approach as multi-view clients by constructing two non-linear mappings \(^{v}(^{v};^{v})\) and \((;)\) to obtain high-level features \(^{v}\) and common semantics \(\). The global model\(f_{g}(;^{v})\) after aggregation further enhances its ability to learn generalized common semantics. To encourage the local model of client \(p\) to approach the more generalized global model, we formulate the model contrastive loss:

\[_{c}^{p}=-_{p}|}_{i=1}^{|_{p}|} _{i},_{i}^{q})/_{p}}+e^{ sim(_{i},_{i}^{v})/_{p}}}{e^{sim( _{i},_{i}^{q})/_{p}}+e^{sim(_{i}, _{i}^{v})/_{p}}},\] (7)

where \(\{_{i}^{q}\}_{i=1}^{|_{p}|}\) represents the output of the local data after being processed by the global model, and \(_{p}\) denotes the temperature parameter. The significance of Eq. (7) lies in treating \(\{_{i},_{i}^{q}\}_{i=1}^{|_{p}|}\) as positive pairs and \(\{_{i},_{i}^{v}\}_{i=1}^{|_{p}|}\) as negative pairs. This allows the local model of client \(p\) to converge towards the global model while amplifying the differences between the reconstruction and consistency objectives in the local model.

During training, the respective total losses for multi-view client \(m\) and single-view client \(p\) are:

\[^{m}=_{r}^{m}+_{c}^{m},\ \ \ \ \ ^{p}= _{r}^{p}+_{c}^{p}.\] (8)

In the optimization of FMCSC, \(_{r}^{m}\) and \(_{r}^{p}\) are utilized as reconstruction losses to learn representations for each view individually. Meanwhile, \(_{c}^{m}\) and \(_{c}^{p}\) are employed to discover common semantics across views, facilitating the exploration of complementary clustering structures across clients.

### Global-Specific Weighting Aggregation

To bridge the view gap and aggregate heterogeneous models, we design a weighted specific aggregation strategy on the server, yielding multiple heterogeneous global models.

**Theorem 1**.: _Assume \(_{m},_{p}(0,1)\) such that \(p(_{i}^{v}_{i})>_{m}\), \(i=1,2,,|_{m}|\) and \(p(_{i}^{g}_{i})=p(_{i}^{v} _{i})>_{p}\), \(i=1,2,,|_{p}|\) hold. The following inequality establishes the relationship between the consistency objectives and the mutual information of the multi-view client \(m\) and single-view client \(p\), respectively:_

\[_{v=1}^{V}I(,^{v}) V |_{m}|-_{m}_{c}^{m},\] (9) \[I(,^{g})-I(, ^{v})-2_{p}_{c}^{p}.\]

Proofs of all the theorems in this paper are provided in Appendix C due to space limit. Theorem 1, Eq. (5), and Eq. (7) indicate that minimizing contrastive loss \(_{c}^{m}\) and \(_{c}^{p}\) are equal to maximizing mutual information. Such connection has also been discussed in .

Through theoretical analysis, we use mutual information \(_{v=1}^{V}I(,^{v})\) and \(I(,^{g})-I(,^{v})\) as weights to evaluate the quality of the models from multi-view clients and single-view clients, respectively. A higher level of mutual information indicates better model quality, leading to higher weights during aggregation.

Considering the heterogeneity of the models, we aggregate client models with the same architecture on the server, referred to as specific aggregation. Specifically,

\[f_{g}(;)=_{m=1}^{M}_{m}f_{m} (;_{m}),\] (10) \[f_{g}^{v}(;^{v})=_{m=1}^{M} _{m}^{v}f_{m}^{v}(;_{m}^{v})+_{p=1}^{S}_{ p}f_{p}(;_{p}^{v}),\]

where \(v=1,2,,V\), \(_{m}\) and \(_{p}\) represent the weights for model aggregation of multi-view client \(m\) and single-view client \(p\) respectively. In this scenario, there are a total of \(M\) multi-view clients and \(S\) single-view clients, resulting in \((V+1)\) heterogeneous global models.

For global models that handle multiple view types simultaneously, the expected risk is defined as \(_{M}(f)\) and optimized by minimizing the empirical risk \(}_{M}(f)\). Similarly, for global models dealing with processing a single view type, such as the \(v\)-th view type, the expected and empirical risks are defined as \(_{S^{v}}(f)\) and \(}_{S^{v}}(f)\) respectively. Inspired by previous works  on the generalization bound of clustering approaches, we obtain the following theorem by analyzing the generalization bound of the proposed FMCSC method.

**Theorem 2**.: _Suppose that for any \(\) and \(f\), there exists \(D<\) such that \(\|\|,\|f_{x}()\|,\| f_{x}^{v}()\|,\|f_{h}^{v}( )\|,\|f_{h}^{h}()\|[0,D]\) hold. With probability \(1-\) for any \(f\), the following inequality holds_

\[_{M}(f) }_{M}(f)+}{_{m}|}}+9VD^{2}}{2M| _{m}|}},\] (11) \[L_{S^{v}}(f) }_{S^{v}}(f)+}{| _{p}|}}+8D^{2}}{2S^{v}| _{p}|}}\] \[+_{m}|}}(d_{m}|}{d}+)+d_{} (}_{v},})+_{v},\]

_where \(M\) is the number of multi-view participating clients, \(S^{v}\) is the number of single-view clients with \(v\)-th view type that participated in the training, \(|_{m}|\) and \(|_{p}|\) are the number of samples in each multi-view client and single-view client, respectively. \(d_{}(}_{v},})\) measures the difference between data from the \(v\)-th view distribution \(}_{v}\) and the multi-view data distribution \(}\)._

Three key implications can be derived from Theorem 2: i) For global models capable of handling multi-view data, having more samples from multi-view clients, such as increasing the number of samples per client or adding multi-view participating clients, contributes to the improvement ofgeneralization performance. ii) For global models dealing with single-view data, having more samples from both multi-view and single-view clients enhances their generalization performance, which is a reflection of multi-view clients helping single-view clients to bridge the client gap. iii) Although the view gap is mitigated, the high dissimilarity among views still leads to high distribution divergence \(d_{}(}_{v},})\), which impairs the quality of the global model.

Finally, each client applies the \(K\)-means  on the common semantics \(\) obtained from the corresponding global model to calculate the cluster centroids and obtain their local clustering results. For example, for multi-view client \(m\), letting \(\{_{j}\}_{j=1}^{K}\) denote the \(K\) cluster centroids, we have:

\[_{_{1},_{2},,_{K}}_{i=1}^{|_{m}|}_{j=1}^{K}\|_{i}-_{j}\|^{2}.\] (12)

The clustering result for the \(i\)-th sample is \(y_{i}=_{j}\|_{i}-_{j}\|^{2}\). By concatenating clustering results from all clients, we obtain the overall clustering results.

## 4 Experiments

### Experimental Settings

**Datasets.** Our experiments are carried out on four multi-view datasets. Specifically, **MNIST-USPS** comprises 5000 samples collected from two handwritten digital image datasets, which are considered as two views. **BDGP** consists of 2500 samples across 5 drosophila categories, with each sample having textual and visual views. **Multi-Fashion** contains images from 10 categories, where we treat three different styles of one object as three views, resulting in 10000 samples. **NUSWIDE** consists of 5000 samples obtained from web images with 5 views. Considering the sample quantities, we allocate BDGP to 12 clients, MNIST-USPS and NUSWIDE to 24 clients respectively, and Multi-Fashion to 48 clients to simulate the federated learning settings.

**Comparison Methods.** We select 9 state-of-the-art methods, including HCP-IMSC , IMVC-CBG , DSIMVC , LSIMVC , ProImp , JPLTD , CPSPAN , FedDMVC  and FCUIF . Among them, apart from FedDMVC and FCUIF, which are FedMVC methods, all the other comparison methods are centralized incomplete multi-view clustering methods. To ensure fair comparisons, we concatenate the data distributed among the clients and use them as the input for centralized methods. Among these, the data from multi-view clients can be regarded as complete data, while the data from single-view clients can be considered as missing data.

**Implementation Details.** For an encoder-decoder pair, the encoder structure is Input- \(_{500}-_{500}-_{2000}-_{200}\), and the decoder is symmetric with the encoder. Also, we set temperature parameters \(_{m}=_{p}=0.5\) and use the batch size of 256. The output dimension \(d\) is set to 20 for all local and global models and communication rounds \(R\) is set to 5. All experiments in the paper involving FMCSC that are not mentioned are performed when \(M/S\) = 1:1.

### Results and Analysis

**Clustering Results.** Table 1 presents a quantitative comparison under various heterogeneous hybrid view scenarios. Each experiment is independently conducted five times, reporting average values and standard deviations. We construct different scenarios by adjusting the ratio of multi-view clients to single-view clients. Remarkably, FMCSC achieves exceptional performance in various heterogeneous hybrid view scenarios, surpassing recent methods. This indicates our ability to achieve satisfactory clustering performance while preserving data privacy. Furthermore, with the increasing proportion of single-view clients, all methods exhibit varying degrees of performance decline, aligning with common expectations. Even when the number of single-view clients is twice that of multi-view clients, FMCSC still achieves superior clustering performance, which is encouraging.

**Ablation Studies.** Components A and D represent the consensus pre-training process and weighted aggregation, respectively. Component B indicates that multi-view clients bring the global models closer as Eq. (6). Component C indicates that model comparison in single-view clients as Eq. (7). Table 2 shows that the impact of component A on clustering performance is dependent on the dataset. Additionally, note that the results in Item-1 are obtained after running four times the number of communication rounds compared to FMCSC. Although the initial misalignment of the model on MNIST-USPS and Multi-Fashion can be mitigated through multiple communication rounds. Component A still plays a crucial role in our training process, facilitating consensus among clients during pre-training to alleviate model misalignment and accelerate convergence effectively. Item-3, which lacks component C, involves both single-view and multi-view clients carrying out the same operation of replacing their local models with global models. We observe a substantial improvement in Item-3 compared to Item-2, indicating that the success of model comparison in component C is attributed to high-quality global models. This achievement requires collaborative efforts from both multi-view and single-view clients. Additionally, the inclusion of the weighted aggregation in component D enhances the benefits of collaborative training among all clients.

   &  &  &  &  &  \\   & A & B & C & D & ACC & NMI & ARI & ACC & NMI & ARI & ACC & NMI & ARI \\  Item-1 & ✓ & ✓ & ✓ & 91.12 & 82.24 & 81.63 & 69.84 & 48.71 & 46.29 & 87.80 & 79.86 & 76.61 & 42.16 & 12.37 & 11.16 \\ Item-2 & ✓ & ✓ & ✓ & 60.82 & 39.74 & 34.28 & 61.04 & 30.76 & 30.14 & 56.04 & 34.22 & 27.75 & 42.08 & 10.15 & 9.06 \\ Item-3 & ✓ & ✓ & ✓ & 88.66 & 76.85 & 76.84 & 87.40 & 68.96 & 71.09 & 82.58 & 74.29 & 68.82 & 45.86 & 15.06 & 12.64 \\ Item-4 & ✓ & ✓ & ✓ & 89.52 & 78.12 & 78.31 & 85.44 & 64.15 & 67.26 & 88.14 & 79.61 & 76.91 & 47.54 & 15.20 & 13.79 \\ Item-5 & ✓ & ✓ & ✓ & ✓ & **92.93** & **84.18** & **85.02** & **91.92** & **77.29** & **80.95** & **90.36** & **82.81** & **80.89** & **52.74** & **22.97** & **21.81** \\  

Table 2: Ablation studies on four datasets when \(M/S\)=1:1.

**Parameter Analysis.** We investigate the sensitivity of our clustering performance on MNIST-USPS dataset to two primary hyperparameters in local-synergistic contrastive learning: \(_{m}\) and \(_{p}\), as shown in Figure 2. The values of \(_{m}\) and \(_{p}\) are tuned within the range of 0.1, 0.3, 0.5, 0.7, 1. Our observations include: i) When \(_{m}\) and \(_{p}\) are set to small values, such as 0.1, the clustering performance of the proposed FMCSC decreases. This may be attributed to an excessive emphasis on view consistency, potentially resulting in an inseparable intrinsic feature space. ii) As the values of \(_{m}\) and \(_{p}\) increase, the clustering results gradually recover, and they exhibit insensitivity within the range of 0.3 to 1. Empirically, we set \(_{m}\) = \(_{p}\) = 0.5 for all datasets.

**Qualitative Study on Model Misalignment.** To further quantify the impact of model misalignment and the effectiveness of our proposed strategy, we visualize the model outputs, i.e., the consensus semantics \(\), both without consensus pre-training and with consensus pre-training. Figure 3 displays the t-SNE  visualizations generated from a randomly selected multi-view client, where different colors represent different classes. In Figure 3 (a), we observe that the output of the global model without consensus pre-training shows mixed features that cannot be clearly distinguished. This confirms our viewpoint that direct parameter aggregation leads to model misalignment, manifested as feature confusion in the feature space. In contrast, Figure 3 (b) demonstrates that FMCSC produces more distinct and separable features, mitigating the negative impact of model aggregation.

### Attributes of Federated Learning

**Generalization Analysis.** To check the validity of our theory for the proposed method, we investigate the impact of the number of samples per client and client participation rate on the generalization performance in the clustering task, as shown in Figure 4 (a) and Table 3. In this setting, the total number of clients is fixed. We find that: i) Increasing the number of samples per client effectively raises the total number of samples involved in training, which enhances the model's generalization performance. ii) As the proportion of participating clients increases, the accuracy of non-participating clients also improves, and the performance gap between participating and non-participating clients narrows. These observations are consistent with the theoretical understanding in Theorem 2, i.e., the generalization performance of the model is enhanced as both the number of samples per client and the number of participating clients increase.

Figure 4: (a) Effect of samples per client on generalization performance. (b) Scalability with the number of clients on Multi-Fashion. (c) Sensitivity under privacy constraints when \(M/S\) = 2:1.

Figure 3: Visualization on model misalignment.

   &  &  &  \\   &  & 50\% & 70\% & 90\% & 50\% & 70\% & 90\% \\     \\  } & MNIST-USPS & 90.64 & 91.71 & 92.97 & 89.11 & 91.67 & 92.06 \\  & BDGP & 88.32 & 89.48 & 92.73 & 85.92 & 86.63 & 87.65 \\  & Multi-Fashion & 87.58 & 89.46 & 89.81 & 87.31 & 87.48 & 88.73 \\  & NUSWIDE & 53.12 & 53.18 & 53.58 & 46.08 & 51.46 & 52.51 \\  

Table 3: Effect of participation rates on generalization performance.

**Number of Clients.** We next consider the effects of changing the number of clients as shown in Figure 4 (b). It is observed that as the number of clients increases, the performance of FMCSC experiences a slight decline but remains generally stable. Only when the client number reaches 100, does a noticeable decline in performance occur, which is attributed to the insufficient number of samples within each client.

**Privacy.** FMCSC, by design, does not share any raw data between clients and the server. Only the model parameters on each client are shared with the server. To further protect client privacy, we adopt differential privacy  by adding noise to the model parameters uploaded from the client to the server. Figure 4 (c) illustrates the clustering accuracy of FMCSC under different privacy bounds \(\). We observe that FMCSC achieves both high performance and privacy at \(\) = 50. However, as the level of noise increases at \(\) = 10, the performance of FMCSC unavoidably degrades.

## 5 Conclusion

In this paper, we propose FMCSC that can handle practical scenarios with heterogeneous hybrid views and explore the data cluster structures distributed on multiple clients. First, we propose cross-client consensus pre-training to align the local models on all clients to avoid their misalignment. Then, local-synergistic contrast and global-specific weighting aggregation are designed to bridge the client gap and the view gap across distributed clients and explore the cluster structures in multi-view data distributed on different clients. Theoretical analysis and extensive experiments demonstrate that FMCSC outperforms state-of-the-art methods across diverse heterogeneous hybrid views and various federated learning scenarios. In future work, we will use the method for more downstream tasks and some real-world situations, such as medical analysis and financial forecasting.