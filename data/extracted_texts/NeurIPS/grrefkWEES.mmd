# Diffusion4D: Fast Spatial-temporal Consistent

4D Generation via Video Diffusion Models

 Hanwen Liang\({}^{1}\)1, Yuyang Yin\({}^{2}\)2, Dejia Xu\({}^{3}\), Hanxue Liang\({}^{4}\),

**Zhangyang Wang\({}^{3}\), Konstantinos N. Plataniotis\({}^{1}\), Yao Zhao\({}^{2}\), Yunchao Wei\({}^{2}\)\({}^{}\)**

\({}^{1}\)University of Toronto, \({}^{2}\)Beijing Jiaotong University,

\({}^{3}\)University of Texas at Austin, \({}^{4}\)University of Cambridge

Equal contribution.Corresponding author.

###### Abstract

The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple images or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, **Diffusion4D**, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities. Our project page is https://vita-group.github.io/Diffusion4D.

## 1 Introduction

The availability of internet-scale image-text-video datasets, coupled with progress in diffusion model techniques , has propelled significant advancements in generating diverse and high-quality visual content, including images, videos, and 3D assets . These advancements have further fostered much progress in the realm of 4D content generation , which has gained widespread attention across both research and industrial domains. The ability to generate high-quality 4D content is key to various applications, ranging from the artistic realms of animation and film production to the dynamic worlds of augmented reality.

However, generating high-quality 4D content **efficiently** and **consistently** remains a significant challenge. **e** Due to the scarcity of large-scale multi-view consistent 4D datasets, earlier 4D synthesis works  borrow appearance and motion priors from pretrained image- or video-diffusion models, and leverage score distillation sampling (SDS)  for optimization. This strategy isime-consuming and computationally inefficient due to the heavy supervision back-propagation, limiting its widespread applicability.  explored video-conditioned 4D generation and utilized 3D-aware image diffusion models to acquire pseudo multi-view data for photometric supervision. Yet the issue of slow optimization speed persists. 

Another primary challenge in 4D synthesis is to guarantee 4D geometry consistency, which can be decomposed into two components: spatial consistency and temporal consistency. As illustrated in Fig. 1, spatial consistency implies that the object maintains consistent 3D geometry at each distinct timestamp, while temporal consistency indicates that the object's appearance and movement exhibit coherence and smoothness across timestamps. These two components have been thoroughly explored separately in static multi-view 3D synthesis  and monocular video generation . Multi-view 3D diffusion models possess robust knowledge of geometrical consistency, whereas video diffusion models encapsulate temporal appearance coherence and smooth motion priors. Recent approaches  tried combining these two generative models to enhance efficiency and consistency in 4D generation. However, their underlying assumption of spatial-temporal conditional independence, the utilization of multi-model supervision, or inference at distinct timestamps inherently leads to 4D geometry inconsistencies. This raises the question: **can we integrate spatial and temporal consistency into a single network, and obtain multi-timestamp cross-view supervision in one shot?** Inspired by recent works in static 3D generation  that repurposed temporal consistency in video generation to spatial consistency in 3D generation, we design a strategy that achieves spatial-temporal 4D consistency with a singular 4D-aware video diffusion model.

To this end, we propose a novel framework, **Diffusion4D**, to achieve efficient and consistent generation of 4D content. First, to overcome the data scarcity issue, we meticulously curate a large-scale, high-quality 4D dataset from a vast corpus of 3D datasets . Utilizing the curated dataset, we develop a _4D-aware video diffusion model_ capable of generating orbital videos of dynamic 3D objects, imitating the photographing process of 4D assets. To the best of our knowledge, this is the first endeavor to adapt a video diffusion model and train on a 4D dataset for explicit novel view synthesis of 4D assets. To explicitly control the dynamic strength of the assets, we propose a novel 3D-to-4D motion magnitude metric and embed it as conditions into the diffusion network. We also incorporate the motion magnitude reconstruction loss to encourage the model to directly learn the object's 3D-to-4D dynamics. During the sampling stage, we propose a novel 3D-aware classifier-free guidance to further enhance the dynamics of 3D assets. Leveraging the flexibility of the video diffusion model architecture, our framework can seamlessly accommodate various prompts, including text, single image, and static 3D content. It demonstrates the capability to generate highly spatial-temporal consistent orbital views around 4D assets, thus providing comprehensive information for the construction of 4D assets. Based on the generated orbital views of 4D assets, we perform explicit _4D construction_ by optimizing Gaussian splatting representations with off-the-shell 4D construction pipelines . We develop a coarse-to-fine optimization strategy with photometric losses. The synthesized multi-view consistent 4D image set enables us to swiftly generate high-fidelity and diverse 4D assets. In summary, our contributions can be summarized into three folds:

* We present a novel 4D content generation framework that, for the first time, adapts video diffusion models for explicit synthesis of spatial-temporal consistent novel views of 4D assets. The 4D-aware video diffusion model can seamlessly integrate with the off-the-shelf modern 4D construction pipelines to efficiently create 4D content.

Figure 1: Decomposition of spatial-temporal consistency in 4D generation. The proposed Diffusion4D embeds geometrical consistency and temporal coherence into a single network.

* We introduce the 3D-to-4D motion magnitude metric to enable explicit control over the dynamic strength and propose the motion magnitude reconstruction loss and 3D-aware classifer-free guidance to refine the dynamics learning and generation.
* Extensive experiments demonstrate that the proposed framework outperforms previous approaches in terms of generation efficiency and 4D geometry consistency, establishing a new benchmark for 4D generation under different types of prompts, including text, single image, and static 3D content.

## 2 Background

4D generation aims at synthesizing dynamic 3D content from various inputs, such as text, single images, monocular videos, or static 3D assets. Extending 3D generation into the space-time domain, this task requires not only consistent geometry prediction but also the generation of temporally consistent dynamics. There are many works dedicated to this task. MAV3D deals with text-conditioned 4D generation by utilizing SDS derived from video diffusion models to optimize a dynamic NeRF representation. 4DFY  tackles this problem by combining supervision signals from image, video, and 3D-aware diffusion models. Animate124  leverages both text and image prompts to improve the appearance of the dynamic 3D assets. DreamGaussian4D  adopts the deformable 3D Gaussians for 4D representations and tackles the image-conditioned generation with a driving video. Similarly, 4DGen  proposes utilizing spatial-temporal pseudo labels and obtains anchor frames with a multi-view diffusion model. Consistent4D  leverages the object-level 3D-aware image diffusion model for the primary supervision and proposes cascade DyNeRF to facilitate stable training. More recent work STAG4D  uses a multi-view diffusion model to initialize multi-view images anchored on input video frames and introduces a fusion strategy to improve the temporal consistency. However, the use of SDS loss in these works results in slow optimization speed and limits their applicability. To improve the efficiency, Efficient4D  generates multi-view captures of dynamic 3D objects through SyncDreamer-T, while Diffusion\({}^{2}\) leverages orthogonal diffusion models to sample dense views. Despite the high efficiency, the assumptions of spatial-temporal conditional independence, multiple model utilization, and distinct-timestamp inference design inherently lead to 4D geometry inconsistencies. Unlike previous methods, our framework is the first to integrate 4D spatial-temporal consistency into a singular video diffusion model. More discussions about related works are in App. A.1.

## 3 Method

In this section, we provide a detailed illustration of the proposed framework, **Diffusion4D**, designed for the efficient and consistent generation of 4D content. We initiate by formulating the problem and outlining the key objectives in 3.1. Then, we introduce our data curation strategy pivotal in acquiring a large-scale, high-quality 4D dataset in 3.2. Subsequently, we delve into the methodology of applying the curated dataset to develop **4D-aware video diffusion models**, capable of synthesizing orbital views of dynamic 3D assets conditioning on various forms of prompts in 3.3. Finally, we introduce the explicit construction of 4D assets using 4D models based on the Gaussian Splitting representations in 3.4.

### Problem Setting and Key Objectives

Formally, given a prompt \(y\), we aim to generate an orbital video \(\) = \(\{I_{i} R^{H W 3}\}_{i=1}^{T}\) around a dynamic 3D asset. This video comprises \(T\) multi-view images captured at \(T\) consecutive timestamps \(\) = \(\{_{i}\}_{i=1}^{T}\) along a predefined camera pose trajectory, where \(H\) and \(W\) are the height and width dimensions of images. To simplify the problem, we put constraints on the camera pose trajectory, assuming that the camera always looks at the center of an object (origin of the world coordinates) with a fixed elevation angle and camera distance. Thus, the viewpoint can be uniquely specified by the azimuth angle. The azimuth angle uniformly increases from 0 to 360 degrees along \(T\) timestamps, constructing a complete orbital video. We aim to generate this orbital video with a 4D-aware video diffusion model that can iteratively denoise samples from the learned conditional distribution \(p(|y)\), where \(y\) can be text, single images, or static 3D content. At the end, explicit 4D construction is performed with generated orbital video \(\) as supervisions.

### Data Curation

The development of the proposed 4D-aware video diffusion model requires a substantial amount of high-quality 4D assets. We curate a large-scale, high-quality dynamic 3D dataset sourced from the vast 3D data corpus of Objaverse-1.0  and Objaverse-XL . Since the original Objaverse dataset primarily consists of static 3D assets and many of them are of low quality (e.g., partial scans, missing textures) , we applied a series of empirical rules to filter the dataset. The curation process includes an initial selection with _dynamic_ label, removal of assets with subtle or overly dramatic motion, and out-of-boundary detection. In the initial step, we select 3D assets labeled as "dynamic". However, upon closer examination, we observe that many assets exhibit subtle or imperceptible movement, limiting their utility in learning dynamic properties in the proposed task. To mitigate this problem, we employ Structural Similarity Index Measure (SSIM) to evaluate the temporal dynamics of the assets. Specifically, for each 4D asset, we fix the camera pose at the front view and render three images at three distinct timestamps (\(_{0},_{T/4}\), and \(_{T/2}\)), then compute two SSIM scores between them. Assets with both SSIM scores higher than a predetermined threshold \(s_{high}=0.95\), suggesting high resemblance and little movement, are discarded. We also observe that many samples exhibit significant geometric distortion or drastic appearance changes over time. Therefore, we empirically set an SSIM score of small value \(s_{low}=0.4\) as the lower bound to filter out cases of poor quality. Subsequently, for each remaining 4D asset, we render \(T=24\) multi-view images following the camera pose trajectory and timeline settings as we defined in Sec. 3.1. In the final step, we handle cases where assets exhibit over-dramatic movements that extend out of the boundaries of the scene. We employ alpha maps to identify and remove such cases and ensure that only appropriately positioned dynamic assets are included in the curated dataset, enhancing the overall quality and coherence of the generated content. Finally, we also conduct a manual inspection over Objaverse-1.0  to remove low-quality cases that do not make sense semantically. This comprehensive strategy results in a total of 81K high-quality animated assets. We released the IDs of the dynamic 3D assets in our curated dataset.

### 4D-Aware Video Generation

Utilizing the curated dataset, we can render a vast number of orbital videos of dynamic 3D assets to train a 4D-aware video diffusion model. For pretrained model selection, given that conventional video diffusion models designed for monocular video generation lack 3D geometry priors, we resort to the recent works of 3D-aware video generation models [41; 58]. In the following, we first introduce the pretrained 3D-aware video diffusion model. Then, we describe how to adapt the model to our 4D-aware orbital video generation task. At this point, we focus on explicitly specifying the motion magnitude guidance, directly learning the 3D-to-4D dynamics in the training stage, and further augmenting the 3D object dynamics in the inference stage. Finally, we introduce how we utilize the curated dataset and customize the model architecture to accommodate various condition modalities.

**Pretrained 3D-aware video diffusion models.** The main idea of 3D-aware video diffusion models [41; 58] is to repurpose the temporal consistency in video generation models for the spatial 3D

Figure 2: Our proposed Diffusion4D consists of a 4D-aware video diffusion model and explicit 4D construction, capable of synthesizing 4D assets conditioned on text, single images, or static 3D assets.

consistency of static 3D objects. Capitalizing on pretrained video diffusion models , these models are finetuned on large-scale datasets of high-quality 3D assets. It facilitates them to generate smooth and consistent orbital views of static 3D objects with user-specified camera pose conditions. In our method, we inherit the 3D geometry consistency in these models and extend it to modeling the spatial-temporal consistency in orbital views of 4D objects.

**Vanilla 4D-aware video diffusion models.** Modern video diffusion models typically carry out diffusion and denoising processes in latent space . In our specific task, given an orbital video \(\) around a 4D asset, which is rendered from curated dataset following the camera position trajectory introduced in Sec. 3.1, we first use a pretrained encoder to encode images into a compressed latent \(z_{0} R^{T h w c}\). The \(h,w,c\) respectively denote the height, width, and channel dimension of the latent representation. The diffusion forward process samples a time step \(t\) and adds noise \(_{t}\) to source input and obtain \(z_{t}\). A denoising network \(_{}\), parameterized by \(\), is trained to predict the added noise conditioned on \(y\), with a noise prediction loss \(_{ldm}=\|_{t}-_{}(z_{t},y,t)\|_{2}^{2}\). In the inference stage, as shown in Fig. 2, given an initial random gaussian noise and prompt condition \(y\), the denoising network predicts the added noise to iteratively denoise the latent embedding. A denoised latent \(_{0}\) is finally obtained, which is decoded via a pretrained decoder to recover a high-fidelity orbital video.

**3D-to-4D motion magnitude guidance and reconstruction**. For the 4D-aware video generation, one of the most interesting attributes is the dynamic strength of the 3D assets. In an effort to enhance the control over the 3D-to-4D dynamic strength, we introduce the motion magnitude guidance into the diffusion model. To begin with, we need to quantitatively measure the motion magnitude of the moving objects. In contrast to the previous video models  that measure the motion magnitude by computing the inter-frame differences, the appearance differences across frames in \(\) originate from both camera-view changes and object movement. To remove the influence from the camera-view changes, for each dynamic 3D object, we also render an orbital video \(}=\{_{i} R^{H W 3}\}_{i=1}^{T}\) around the static 3D asset, consisting of \(T\) multi-view images captured at the same camera poses as \(\) at timestamp \(_{0}\). Consecutively, we propose a metric named **3D-to-4D motion magnitude \(m\)** measuring the dynamic strength:

\[m()=_{i=1}^{T}||I_{i}-_{i}||_{2}^{2}.\] (1)

Motivated by previous works , we use a two-layer multi-layer perception (MLP) to extract a motion magnitude embedding, which is combined with the time embedding and injected into the denosing network. The noise prediction function by the denoising network accordingly changes to \(_{}(z_{t},y,m,t)\). This design allows the user to explicitly control the motion magnitude in inference stage. The impact of the motion magnitude guidance on the generation results is illustrated in Fig. 5.

In the training phase, to encourage the denoising network to learn the 3D-to-4D dynamics, we incorporate the motion magnitude reconstruction loss. This loss facilitates the direct learning of dynamic strength with explicit supervision over motion magnitude from 3D to 4D dynamics. For the sake of computation cost, we apply the reconstruction loss in the latent space and it is formulated as:

\[_{mr}=||m(z_{0})-m(_{0})||_{2}^{2},\;m(z_{0})=|| z_{0}-_{0}||_{2}^{2}.\] (2)

, where \(_{0}\) denotes the estimated clean video latent during training, the \(_{0}\) denotes the compressed latent of \(}\) by the pretrained encoder. Our diffusion model training loss is formulated by combining latent diffusion loss and motion magnitude reconstruction loss with a weight \(\): \(=_{ldm}+_{mr}\).

**3D-aware classifier-free guidance**. To further augment the dynamic strength of 3D objects, we draw inspiration from classifier-free guidance  and propose a 3D-aware classifier-free guidance. It uses the pretrained 3D-aware video diffusion model, formulated as \(_{}\), to provide classifier-free guidance during the inference stage. Combining the prompt condition \(y\), motion magnitude guidance \(m\), the unconditional noise prediction \(_{}(z_{t},t)\) and 3D-aware noise prediction \(_{}(z_{t},y,t)\), the denoising step is reformulated as:

\[_{}(z_{t},y,m,t)=_{}(z_{t},y,m,t)+w_{1}( _{}(z_{t},y,m,t)-_{}(z_{t},t))+w_{2}(_{ }(z_{t},y,m,t)-_{}(z_{t},y,t))\] (3)

, where \(_{}(z_{t},y,m,t)\) is joint noise prediction, and \(w_{1}\) and \(w_{2}\) are the classifier-free guidance scalers.

**Generation with various condition modalities.** In the above formulations, we use \(y\) as a general prompt condition. Thanks to the versatility of our high-quality 4D dataset and the flexibility of the 3D-aware video diffusion model architecture, our framework can readily accommodate diverse prompt modalities, including text, single images, or static 3D contents. **For the text condition**, we obtain the text description of each dynamic 3D asset from the prior work , and the text embedding extracted by CLIP model is fed into the diffusion model via the cross-attention mechanism. You can refer to  for more details. **For the image condition**, we use the first view image \(I_{0}\) in \(\) captured at timestamp \(_{0}\) as the reference image. The image condition is injected into the diffusion model with both cross-attention mechanism and feature concatenation. Please refer to [56; 3] for more details. **For static 3D content condition**, we use \(}\) as the reference video. Similar to the image condition, the video features are extracted by pretrained encoder and and fed into the diffusion model via feature concatenation. This versatility allows our framework to effectively respond to different condition modalities, facilitating seamless integration into a wide range of applications and scenarios.

### Coarse-to-Fine 4D Construction

The spatial-temporal consistent multi-view videos generated by our 4D-aware video diffusion model offer comprehensive information about the 3D geometry and motions of dynamic 3D assets. At this stage, we explicitly model the 4D assets with Gaussian splatting(GS)  owing to its explicit representation, powerful fitting capabilities, and efficient optimization with dense-view supervision. We resort to the state-of-the-art 4D construction pipelines, i.e. 4DGS  and SC-GS . When we directly train the GS on the generated multi-view video \(\), we find that the model does not perform well in capturing the 3D geometry details. The visualization can be found in Fig. 5(e). Therefore, we augment the supervision signals and propose a coarse-to-fine construction strategy. In the coarse stage, given the first-view image \(I_{0}\) in \(\), we use the pretrained 3D-aware video diffusion model to generate an orbital-view video \(}^{{}^{}}\) of the static 3D object. As our 4D-aware video diffusion model is finetuned on this model, we observe high 3D geometry consistency between \(}^{{}^{}}\) and \(\). We merge them together to train the GS in the coarse training stage. Note that the two sets of images (\(\) and \(}^{{}^{}}\)) are of the same orbital views at different timestamps, \(\) ranging from 0 to 1, and \(}^{{}^{}}\) at 0 only. Actually, for static 3D content conditioned generation, we can readily replace \(}^{{}^{}}\) with conditional signal \(}\) into the image set for coarse training. In the fine training stage, we use \(\) only to further tune the GS to improve the spatial-temporal consistency. Thanks to the 4D consistency in our generated videos, we can achieve precise pixel-level matching across different views and timestamps. We follow  and use \(L_{1}\) and \(L_{lipips}\) losses for optimization. To enforce geometry smoothness, we also involve depth smoothness loss as regularizer [5; 25]. For 4DGS-based reconstruction , the total loss is formulated as \(_{gs}=_{l1}_{1}+_{lipips}_{lipips }+_{depth}_{depth}\). For SC-GS based reconstruction , \(_{arap}_{arap}\) is also involved. The \(_{l1}\), \(_{lipips}\), \(_{arap}\), and \(_{depth}\) are losses weights.

## 4 Experimentation

### Experiment Setup

We employ Diffusion4D to generate 4D assets conditioned on multiple modalities of prompts, i.e. text, single image, and static 3D content. Leveraging the curated dataset, for each dynamic 3D asset, we render two 360-degree orbital videos \(^{}\)s with \(T=24\), one starting at the front view and the other starting at a random azimuth angle. We fix the elevation angle as 0 and the camera distance as 2-meter. The image resolution is chosen at \(256 256\) across the experiments. In preparation for static 3D content-conditioned generation, following the camera poses of each rendered video \(\), we also render two 360-degree orbital video \(}^{}\)s of each static 3D asset fixed at initial timestamp \(_{0}\). We use VideoMV  as the pretrained 3D-aware video diffusion model, which is trained on a large scale of static 3D assets of superior quality. Specifically, for text-conditioned 4D-aware video generation, we adopt the text-to-3D video generator  that is based on the ModelScopeT2V  model architecture. For image and video conditions, we adopt the image-to-3D video generator  that is finetuned from I2VGen-XL . We train our 4D-aware video diffusion model for 6k iterations with a constant learning rate of \(3 10^{-5}\). We use a valid batch size of 128 and train on 8 NVIDIA A100 GPUs. During the sampling stage, we use text, front-view images, front-view started orbital videos of static 3D assets as conditions. we use DDIM  sampling with sampling step 50, and \(w_{1}=7.0\) and \(w_{2}=0.5\) in classifier-free guidance. In the 4D construction stage, for 4DGS, weFigure 3: Qualitative comparisons between Diffusion4D and other baselines in Text-to-4D (upper) and Image-to-4D (lower) generation. For our method, we show five views from consecutive timestamps. (* results from 4D-aware video diffusion model).7

optimized the GS representation for 5k iterations in the coarse stage and 2k iterations in the fine stage. For SC-GS, we optimized the GS for 16k iterations in the coarse stage and 4k iterations in the fine stage. Following prior works [30; 51], by default, we use reconstructions from 4DGS for quantitative and qualitative comparisons with the baselines.

### Metric

From the curated dataset, we leave out 20 cases as the test set for evaluation. For quantitative assessment, we first use the CLIP  score to evaluate the semantic correlation between the prompts (reference) and synthesized images (target). In each case, depending on the condition modality, the text description or front-view image serves as the reference. For the diffusion generation, we use all 24 images in orbital videos as targets, and for 4D construction, we uniformly render 36 orbital views from constructed 4D assets as targets (CLIP-O). We also measure using only front-view images as targets(CLIP-F). To evaluate the appearance and texture quality and the spatial-temporal consistency of the synthesized images, we also use the following metrics, i.e. LPIPS , PSNR, SSIM , and FVD , to help assess image- and static 3D content-conditioned generation. For these evaluations, we use ground truth images rendered from the dynamic 3D assets as references. Images generated by the diffusion model or rendered from constructed 4D assets are used as targets. Metrics are computed between ground truth images and synthesized images from the same camera pose. The same procedure is applied to calculate the scores for baseline methods. We also conduct qualitative comparisons through a user study involving 30 human evaluators from diverse backgrounds. Participants are asked to specify their preference for rendered orbital videos around 4D assets based on four properties, following the approach in : 3D geometry consistency (3DC), appearance quality(AQ), motion fidelity(MF), and text alignment(TA). We report the percentage of users who prefer each method overall and based on each of the four properties.

### Main Results

**Text-conditioned and image-conditioned 4D generation.** We take 4DFY  and Animate124  as baselines for text-conditioned generation, and 4DGen and STAG4D as baselines for image-conditioned generation, due to their remarkable performance and adaptability to the task settings. In Fig. 3 we provide a detailed comparison between our method and baselines with various prompts. For the baselines, we provide two views at starting and ending timestamps. For our method, we visualize the spatial-temporal renderings from 4D constructions at five consecutive timestamps in multiple views. We also show the generated multi-view images from our video diffusion models (denoted with *). Our efficient and elegant pipeline is capable of generating diverse and realistic 4D assets with satisfactory geometrical and temporal consistency. While the baselines also synthesize 4D assets, their results exhibit very limited or even invisible motion. In contrast, our results show apparent motions of great fidelity. As highlighted by the contours in Fig. 3, our animations include cartoon characters and humans stepping forward, running or raising arms, birds flapping wings, and lights changing. Our method also provides more detailed appearances. The quantitative results in

Figure 4: Visualization of Static-3D conditioned Diffusion4D. The first row shows the conditions, and the rest shows the results. (* results from 4D-aware video diffusion model.)

Tab. 1 demonstrate that our method outperforms previous approaches across all metrics and user preferences. Compared with the state-of-the-art SDS-based methods, which involve sophisticated and time-consuming optimization, our method is much more efficient and produces more favorable results. Users showed a strong preference for Diffusion4D over other baselines, especially in 3D geometry consistency and motion fidelity. Also, as 4D construction is based on the images from the diffusion models, we can observe that diffusion outputs perform slightly better than rendered outputs.

**Static 3D content-conditioned 4D generation.** We visualized the results of static 3D content-conditioned generation in Fig. 4. Our framework successfully activates static 3D assets and generates spatial-temporal consistent 4D assets. Due to the lack of direct baselines, we compare our image-conditioned 4D generation with 3D content-conditioned generation. The difference between these two settings is that the former relies only on a single image, whereas the latter has access to dense views of the 3D assets. While both can generate realistic dynamic 3D assets, the latter can generate more 3D geometry-consistent objects. As shown in the last two rows in Fig. 4, Mario and anthropomorphic mouse closely follows the appearance and geometry of the prompts, particularly from the side and back views. The quantitative results in Tab. 1 indicate that the static 3D content-conditioned setting achieves the best performance among all tasks, thanks to the access to dense views of source assets.

### Ablation Study

We conduct an analysis on the effect of various design components in our framework with the typical image-to-4D generation task, and the results are shown in Fig. 5 and Tab. 2. In the 4D-aware diffusion model, we incorporated multiple features to enhance the dynamics of 3D assets. We added each feature incrementally to demonstrate their impact. In Fig. 5 (1), we can observe that training without motion magnitude reconstruction loss results in nearly invisible movement (a), while incorporating it introduces subtle motion (b). Increasing the motion magnitude guidance (\(m\)) augments the dynamics of the kid (c). The involvement of 3D-aware classifier-free guidance significantly augments the

    &  &  \\  & CLIP-F\(\) & CLIP-O\(\) & Generation time. & SSIM\(\) & FSNR\(\) & LIPPS\(\) & FVD\(\) & 3DC & AO & MF & TA & Overall \\  _Text-to-4D_ & & & & & & & & & & & & \\  ADPY  & 0.78 & 0.61 & 23h & — & — & & 26\% & 34\% & 25\% & 37\% & 29\% \\ Ainsinset 24  & 0.75 & 0.58 & 9h & — & — & & 22\% & 28\% & 19\% & 23\% & 22\% \\ Diffusion4D & **0.81** & **0.65** & **8n** & — & — & & **52\%** & **38\%** & **56\%** & **40\%** & **49\%** \\ Diffusion4D* & **0.82** & **0.69** & — & — & — & & & — & & & \\  _Image-to-4D_ & & & & & & & & & & & \\  ADGcn  & 0.84 & 0.71 & 18.30m & 0.69 & 14.4 & 0.31 & 736.6 & 18\% & 22\% & 18\% & 29\% & 22\% \\ STMOG  & 0.86 & 0.72 & 23.00m & 0.76 & 15.2 & 0.27 & 675.4 & 15\% & 25\% & 26\% & 33\% & 24\% \\ Diffusion4D & **0.89** & **0.75** & **8n** & **0.83** & **16.7** & **0.21** & **560.8** & **47\%** & **53\%** & **56\%** & **38\%** & **54\%** \\ Diffusion4D* & **0.90** & **0.80** & — & 0.82 & **16.8** & **0.19** & **490.2** & — & — & — & \\  _Data: 3D content-to-4D_ & & & & & & & & & & & \\  Diffusion4D* & 0.88 & 0.77 & 8n & 0.82 & 16.8 & 0.19 & 544.7 & — & & & \\ Diffusion4D* & **0.91** & **0.81** & — & **0.83** & **17.2** & **0.18** & **482.4** & — & — & & \\   

Table 1: Quantitative comparison between our method with other baselines under different task settings. The human study includes 3D geometry consistency(3DC), appearance quality(AQ), motion fidelity(MF), text alignment(TA), and overall score. By default, Diffusion4D suggests results from 4D construction, and Diffusion4D* suggests results from our diffusion models.

Figure 5: Ablation study of different components of our proposed framework.

dynamics of the kid (d). In the 4D construction stage, as you can observe in Fig. 5 (2), training without the coarse stage leads to incomplete 3D geometry (e). Comparing (f) and (g), we can observe that adding the fine stage enhances the appearance and texture details of the generated 4D assets. Results in Tab. 2 indicate that the removal of each proposed component leads to a noticeable drop in metrics. In summary, the full model delivers the best results both quantitatively and qualitatively.

## 5 Limitation and Future work

While our proposed Diffusion4D framework demonstrates significant advancements in terms of efficiency and consistency for 4D content generation, we acknowledge the following limitations to provide a foundation for future work. First, we use a video size of \(24 256 256\). While this resolution provides a good balance between quality and computational feasibility, higher resolution and longer temporal sequences could further enhance the detail and realism of 4D content generation. Image resolution could be improved using off-the-shelf super-resolution models. Second, though we curated a substantial amount of high-quality data, the dataset still lacks diversity. The asset textures are simple, with actions restricted to basic movements such as walking, jumping, and stretching arms. Although there is a large number of characters, each character appears only a few times, performing a limited range of actions. Last, in the 4D reconstruction stage, the performance is subject to the capacity of off-the-shelf 4D construction pipelines. In our settings, we empirically find that both of the chosen 4D construction pipelines work well in constructing consistent 4D assets from orbital views. Comparatively, SC-GS performs better than 4DGS from more novel views. Due to the limited views for supervision, the constructed 4D assets may meet floaters and blurry problems when the objects have large motions. In future research, we will try to expand the diversity and quality of the dynamic 3D dataset. We will focus on exploring 4D reconstruction to improve the model's ability to interpret and leverage orbital-view supervisions, which will be beneficial for generating 4D assets with better consistency across novel views. We will put more effort into generating longer and higher-resolution 4D orbital videos to support more detailed 4D construction, potentially broadening the applications for Diffusion4D.

## 6 Conclusion

In this work, we introduced Diffusion4D, an efficient and scalable framework for spatial-temporal consistent 4D generation. Motivated by the prior explorations using video diffusion model to generate orbital views of static 3D assets, we migrate the temporal consistency in the video diffusion model to spatial-temporal consistency in 4D generation. Leveraging a meticulously curated dynamic 3D dataset, we developed a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. We incorporate 3D-to-4D motion magnitude guidance and the motion magnitude reconstruction loss to enhance the dynamics learning and control. 3D-aware classifier-free guidance is introduced to further augment the dynamic strength. We fulfill explicit 4D construction with a coarse-to-fine learning strategy. Extensive experiments demonstrated that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities. This innovative approach not only addresses the limitations of previous methods but also sets a new benchmark for 4D content generation.

## 7 Acknowledgment

This research was funded by the Fundamental Research Funds for the Central Universities (2024XKRC082).

   _Image-to-4D_ & CLIP-F\(\) & CLIP-O\(\) & SSIM\(\) & PSNR\(\) & LPIPS\(\) & FVD\(\) \\  w/o \(_{inv}\) & 0.84 & 0.73 & 0.78 & 15.4 & 0.26 & 602.5 \\ w/o \(_{inv}\) & 0.86 & 0.77 & 0.79 & 15.8 & 0.23 & 524.6 \\ w/o coarse stage & 0.79 & 0.70 & 0.72 & 14.5 & 0.32 & 651.2 \\ w/o fine stage & 0.86 & 0.74 & 0.77 & 15.2 & 0.25 & 581.4 \\ Full model & **0.89** & **0.75** & **0.83** & **16.7** & **0.21** & **560.8** \\ Full model\({}^{*}\) & **0.90** & **0.80** & **0.82** & **16.8** & **0.19** & **490.2** \\   

Table 2: Ablation study on the effect of proposed components on 4D construction. * denotes results from diffusion models.