# Does Video-Text Pretraining Help Open-Vocabulary

Online Action Detection?

 Qingsong Zhao\({}^{1,2}\) Yi Wang\({}^{12}\) Jilan Xu\({}^{1,4,2}\) Yinan He\({}^{12}\) Zifan Song\({}^{1}\)

Limin Wang\({}^{3,2}\) Yu Qiao\({}^{2}\) Cairong Zhao\({}^{4}\)

\({}^{1}\)Tongji University \({}^{2}\)Shanghai AI Laboratory \({}^{3}\)Nanjing University \({}^{4}\)Fudan University {qingsongzhao, zhaocairong}@tongji.edu.cn

{wangyi, heyinan, qiaoyu}@pjlab.org.cn

lmwang.nju@gmail.com

corresponding author, \({}^{}\)equal contribution

###### Abstract

Video understanding relies on accurate action detection for temporal analysis. However, existing mainstream methods have limitations in real-world applications due to their offline and closed-set evaluation approaches, as well as their dependence on manual annotations. To address these challenges and enable real-time action understanding in open-world scenarios, we propose OV-OAD, a zero-shot online action detector that leverages vision-language models and learns solely from text supervision. By introducing an object-centered decoder unit into a Transformer-based model, we aggregate frames with similar semantics using video-text correspondence. Extensive experiments on four action detection benchmarks demonstrate that OV-OAD outperforms other advanced zero-shot methods. Specifically, it achieves 37.5% mean average precision on THUMOS'14 and 73.8% calibrated average precision on TVSeries. This research establishes a robust baseline for zero-shot transfer in online action detection, enabling scalable solutions for open-world temporal understanding. The code will be available for download at https://github.com/OpenGVLab/OV-OAD.

## 1 Introduction

Action detection is a practical and demanding technique in intelligent video analysis, including anomaly detection  in surveillance and human-computer interaction  in embodied studies. Considering high variations in possible human behaviors with dynamic scenes, action detection is significantly challenging. In this regard, most action detection approaches go offline, involving the closed-set classification and localization of actions (a few predefined categories) within the long untrimmed videos. However, real-world applications concerning real-time understanding (e.g. surveillance) require estimating the action without accessing future frames. Further, closed-set discrimination limits the applicability of action detection, and it also asks for manually annotating all action categories, especially in complex scenarios such as a wide variety of actions or events in driving scenarios, which is both costly and time-consuming.

To address these challenges, we formulate online action detection in open-vocabulary and transfer popular vision-language models (VLM) to tackle this problem via only paired vision-text supervision for learning. A growing number of researchers have been investigating how to leverage the capabilities of powerful VLM to address specific novel visual tasks of interest. For instance, existing studies  have explored the transfer of visual knowledge from VLM to a video understanding task to achieve zero-shot temporal action detection (ZS-TAD). Applying VLM to ZS-OAD is non-trivial. Plain solutions, as the ZS-TAD approaches mentioned earlier, involve partitioning a subset ofbase-to-novel category data from the downstream dataset and further fine-tuning the visual language model with a prompt-based technique to adapt it for novel tasks. However, this poses several issues. First, sliding-window frame sampling used in online action detection often leads to a high proportion of background frames, which contradicts the assumption of low background information for VLM training. Second, the OAD model fails to reach future frames during training, making it difficult to sample all category labels in the same batch. This is detrimental to the optimization of image-text contrast loss since the contrast loss favors the diversity of samples. We experimentally explored these hypotheses in Sec. 4.1.

Inspired by CLIP , given frames representation from a powerful VLM, we learn online motion detection models purely through text supervision, thus avoiding the use of fine-grained temporal annotations. To this end, we introduce the proposed object-centered decoder unit into the Transfomrer-based model, enabling automatic aggregation of frames with similar semantics with textual supervision exclusively. Fig. 1 illustrates the overall framework of our method. By employing contrast loss during training on extensive video-text pairs, we enable the model to be zero-shot transferred to different action detection vocabularies. Hence, we name our model Open-Vocabulary Online Action Detector (OV-OAD). We pre-train OV-OAD on the video-text datasets, and manual frame-level labels are not used whatsoever. We propose three proxy tasks including alignment of current frame-text embedding, background frame mask prediction, and alignment of multi-label video-text embedding for training. The first task enables the model to prioritize discriminative information from neighboring frames. The second task enables the successful detection of complex background frames in natural videos. The third task mitigates the impact of caption noise in web videos. Our model was evaluated on four action detection benchmarks without any fine-tuning, i.e., THUMOS'14 , EK100 , FineAction  and TVSeries  in a zero-shot manner. Extensive experiments demonstrate that our model outperforms other advanced zero-shot methods. The main contributions are summarized as follows:

* We investigate the critical problems of how to capitalize pre-trained visual language models for zero-shot online action detection in untrimmed videos.
* We introduce a novel video-text dual-encoder architecture, namely OV-OAD, to perform open-vocabulary online action detection. Experiments on the downstream datasets show that our model successfully learns clusters of similar video frames and transfers them to multiple action semantic vocabularies in a zero-shot manner.
* To our knowledge, our work is the first to explore zero-shot transfer from text supervision alone to the online action detection task without relying on any precise frame-scale labels. And we have established a robust baseline for this new setting.

Figure 1: Overview of the online action detection. Models trained on closed-set actions (e.g., discus and brush toilet) are unable to detect the novel action class (e.g., fry eggs). We train a visual-text dual-encoder on web-collected video-text pairs without using frame-scale labels. It can discriminate arbitrary action classes.

Related Work

Pretrained Vision-Language Model (VLM).Recently, the joint image-text learning paradigm  has been successfully scaled up by CLIP  and ALIGN  with the massive web data. After that, researchers have proposed many variations, including CLIP-Adapter , GLIP , and so on. One VLM's visual encoder can leverage textual descriptions to recognize objects or scenes in images when category-specific samples are unavailable. In video domains, similar ideas have been explored for action recognition (e.g., ActionCLIP , ViFi-CLIP ) and video understanding (e.g., CLIPBERT , EffPrompt ).

Zero-Shot Temporal Action DetectionTemporal action detection (TAD [36; 11; 48; 26]) is a video understanding task involving simultaneous recognition and localization of actions within an uncut video. Recently, efforts [21; 31; 1; 6; 25; 33] have utilized the pre-trained vision-language model to give the TAD models with the capability to recognize novel action classes. For example, EffPrompt  proposes a two-stage fine-tuning scheme for zero-shot temporal action detection (ZS-TAD) by incorporating task-specific prompt vectors. STALE  introduces a one-stage model to mitigate the error propagation problem encountered by EffPrompt by utilizing a parallel classification and localization design. T3AL  presents a training-free ZS-TAD that leverages an effective test-time augmentation strategy and external knowledge derived from generated subtitles. It is important to highlight that all those ZS-TAD methods adopt a Base-to-Novel fine-tuning approach, which involves dividing the dataset categories into training and inference subsets. Due to the strong diversity among the TAD datasets and the limitation of its scale size, it has been challenging to showcase the model's generalization capabilities. By contrast, we train our open-vocabulary online action detection model with large-scale video-text pairs only. During inference, the model does not require any additional fine-tuning to recognize arbitrary action classes.

Online Action Detection.Contrary to offline motion detection, OAD does not predict action onset timing and cannot access future visuals. Arguably, OAD emphasizes real-time response and openness of recognition over the accuracy of action classification in practice. Existing researchers [17; 45; 15; 41; 46; 38; 4] often use closed-set datasets for training and testing, boosting recognition accuracy and speed on single datasets. For example, IDN  improves the discriminative representation of actions by selectively accumulating relevant information. OadTR  incorporates the fusion of current features and future frames for identifying ongoing actions. LSTR  captures contextual dependencies in videos, leading to improvements in action identification. E2E-LOAD  proposes an end-to-end framework that integrates a stream buffer between the spatial and spatiotemporal modeling. MAT  introduces a memory-anticipation-based pipeline to model the entire temporal structure of a video. In contrast, our work shifts the focus to enhancing the open recognition capability of OAD models. We aim to leverage readily available video-text pairs to zero-shot transfer visual knowledge into the OAD model, thereby improving the model's ability to handle unseen actions.

## 3 Methodology

Consider an untrimmed video \(V\), we generate a clip sequence employing a sliding window of length \(\) on \(V\) that moves frame by frame. On the \(t\)-th slide, we get a clip \(V^{t}=\{V_{t-},,V_{t-1},V_{t}\}\) where \(V_{t}\) denotes \(t\)-th frame. Online action detection is to predict action probability \(_{t}\) in each frame \(V_{t}\) using only past and current observations. We propose an open-vocabulary online action detection model (OV-OAD) for zero-shot online action detection with text supervision only. Our approach, illustrated in Fig. 2, consists of two primary components: a visual encoder and a text encoder. The visual encoder comprises a distant neighboring-frame transformer block and an action clustering one. We pre-train OV-OAD on a web-scale video-text dataset. In inference, we transfer the trained model to the zero-shot online action detection without any fine-tuning, as described in Sec. 3.3, and it can predict arbitrary action classes. We ignore the subscripts of individual images and text pairs for simplicity.

### Architecture

#### 3.1.1 Visual Encoder

The visual encoder is composed of a distant neighboring-frame transformer block (with a light grey background in Fig. 2), and an action clustering block (with yellow background in Fig. 2) with an object-centric decoder. For a given video clip-text pair, denoted as \((^{t},T)\), we initially divide the video into extended past frames \(^{t}_{P}^{(-n) d}\) and neighboring ones \(^{t}_{N}^{n d}\). All frames \(^{t}\) are processed in the Distant Neighboring-frame TRansformer block \(_{DNTR}\), which comprises the transformer decoder unit . Then the neighboring frames \(^{t}_{N}\) aggregated with the information of past frames are fed into the Action Clustering block \(_{AC}\), along with \(k\) learnable group embedding (\(G^{k d}\)), that aims to bind the neighboring frames into clusters. The outputs of the visual encoder are defined as:

\[(,^{t^{}}):=_{AC}([G;^{t^{ }}_{N}])_{DNTR}(^{t}_{P},^{t}_{N}),\] (1)

where the symbol \(\) represent a function composition, \(^{k d}\) denotes the encoded group embeddings, while \(^{t^{}}^{n d}\) refers to the output video frame tokens.

**Distant Neighboring-Frame Transformer.** It utilizes neighboring frames as queries to extract information from frames in the distant past. It is predominantly composed of four layers of standard transformer decoder units. Each frame in the video is augmented with 1-dimensional absolute positional encoding, independently applied to both past and neighboring frames. A directional attention mask is incorporated only for the neighboring frames, ensuring unidirectional information flow toward the current frame. Intuitively, the current frame embedding \(^{t}_{N}[-1]\) with more spatial information, will be aligned with the corresponding text as a raw video clip representation.

**Action Clustering.** The action clustering block namely \(_{AC}\) assembles frames into groups and aligns the groups to human-understandable categories in a data-driven manner, only supervised by video-text pairs. It consists of three steps, i.e., the frame-to-group binding that assigns static frames with similar semantics to a group, the group-to-action mapping that computes the cosine similarity between the group embeddings with the action labels, and background mask proposing that predicts a set of binary masks by computing statistics from the frame-group similarity matrix \(\).

In its training, we devise a binary prediction task to cluster frames into background and non-background as well as the grouping. The background mask \(_{B}\) is predicted as:

\[:=softmax(}{}),_{B}:= ^{},\] (2)

Figure 2: The illustration of our OV-OAD (best viewed in color), is formulated in a visual-text dual-encoder manner. Specifically, the visual encoder consists of a distant neighboring-frame transformer block (\(_{DNTR}\), light grey backdrop) and an action clustering block (\(_{AC}\), yellowish backdrop). The \(_{DNTR}\) is built with Transformer Decoder units, which take the neighboring tokens and distant past tokens as inputs. The \(_{AC}\) is built with our Object-Centric Decoder and vanilla Transformer Encoder units, which take the output tokens (orange squares) and learnable group embeddings (purple gradient squares) as inputs. During testing, the OV-OAD handles each incoming video snippet online, absent future context.

where \(^{n k}\) is derived from the attentional weights in slot-attention computation. It denotes the likelihood of each video frame being assigned into \(k\) learnable group embeddings. In Eq. 2, the group embeddings serve as the query \(Q\), and frame tokens serve as keys and values. With the softmax operator normalizing over \(k\), and the output of the slot-attention block is \(^{}V\), which is of shape \(k d\). The tensor \(^{1 d}\) represents the \(d\)-dimensional embeddings of the video caption.

In its evaluation, similar to the computation of \(_{}\), a video clip's action prediction score namely \(_{AC}^{n C}\) can be calculated as follows:

\[_{AC}:=(*{arg\,max}_{k}()) _{val}^{}\,.\] (3)

In contrast to Eq. 2, the frame-group similarity matrix \(\) requires one-hot hard coding, while \(_{val}^{C d}\) is encoded by the \(C\) categories descriptions in the test set.

Object-Centric Decoder Unit.To group frames, we give an Object-Centric (OC) Decoder unit employing a slot attention mechanism  focused on the query object during cross-attention computation. It works similarly to the previously proposed GroupViT  and OVSegmentor  methods, which are specifically designed for semantic segmentation tasks.

As illustrated in Fig. 2, it requires two sets of inputs, i.e., target queries \(\) consisting of a fixed number of \(k\) encoded grouping embedding and \(n\) input frame tokens \(_{N}^{t^{}}\) to be queried, where \(n\) can be a large number. Following a layer of multi-head self-attention, \(\) is transformed to \(^{}\). \(^{}\) is then employed as a query in the second layer of multi-head slot-attention, while the frame tokens \(_{N}^{t^{}}\) serve as the key and value. These two steps can be expressed as

\[^{}:=softmax(^{}}{ {d}}), SlotAttn((^{}), _{N}^{t^{}}):=[softmax(_{N}^{t^{}} (^{})^{}}{})]^{}_{N}^{t^ {}}\,,\] (4)

where \(:^{k d}^{k d}\) denotes the dropout and norm operations.

#### 3.1.2 Text Encoder

We adopt the pre-trained Text Transformer defined in CLIP  as the text encoder \(_{T}\). To bridge the gap between image-text and video-text models, we utilize the Adaptformer technique , for lightweight transfer learning. We adopt the practice of setting up parallel adapters in each transformer block of \(_{T}\). For the input text, we use the CLIP Tokenizer  to add the tokenizer start \([SOT]\) and tokenizer end \([EOT]\) at the beginning/end. The text embedding is computed as \(=_{T}(T)\), where \(T^{ l}\) represents the tokenized caption with a length of \(l\).

### Video-text data for online action detection.

Web video-text datasets  are abundant in data volume, but their captions are typically pre-labeled by image-text generative models  and then filtered by humans semi-automatically. Consequently, these annotations are extremely noisy and include fantasy elements, despite being semantically rich and diverse. Additionally, we employ a sliding window approach to capture both the video frame and its corresponding text description during the live-streaming video. These factors introduce inherent bias in the visual and textual information of a sample pair. To achieve relatively accurate textual captions of the visual information in the video, we employ a multi-label contrast learning strategy. To generate multiple captions, we employ a linguistic analysis tool (e.g., the nltk toolkit ) when the raw labels are not enough. This tool extracts verb-object phrase structures from the given descriptions, which are then utilized as keywords to create additional captions, drawing on CLIP's prompting engineering. To handle redundant captions, we choose the text associated with the highest number of frames as the global descriptor, and the remaining captions are utilized to compute the multi-label contrast loss. In the absence of any tags, we utilize the keyword _"background"_ for sentence construction and as the global descriptor.

### Optimization and Inference

We train the model through three proxy tasks, i.e., video-text alignment, current frame-text matching, and background mask prediction. The total loss is: \(_{}=_{}+_{ }+_{}\), where \(,\) are trade-off parameters controlling the relative weight of the above cost functions. Each used loss is detailed below.

Current Frame-Text Alignment.We adopt Image-Text Contrastive loss (\(_{ITC}\)) to learn whether the current frame image matches the caption. We denote the image embedding (from the distant neighboring-frame Transformer block) and the text embedding as \(z^{I}\) and \(z^{T}\), respectively. Bothembeddings are projected into a 512-dimensional joint feature space before calculating the matching loss. The current frame-text contrastive loss \(_{current}\) is:

\[_{}=_{}(z^{I},z^{T})=- (log^{I} z_{i}^{T}/)}{_{j}^{B}(z_{i}^{I}  z_{j}^{T}/)}+log^{T} z_{i}^{I}/)}{_{j}^{ B}(z_{i}^{T} z_{j}^{I}/)}),\] (5)

where \(\) is a temperature parameter to scale the logits, and \(B\) denotes the batch size.

**Multi-Label Video-Text Alignment.** We employ the multi-label video-text contrastive loss to align the visual and language representations for enhancing the textual representations of videos. The multi-label video-text matching loss \(_{contras}\) is:

\[_{contras}=_{}(z^{V},z_{0}^{T})- (log^{M}exp(z_{i}^{I} z_{i}^{T_{m}}/)}{_{m}^{ M}_{j}^{B}exp(z_{i}^{I} z_{j}^{T_{m}}/)}+_{m=1}^{M}log ^{T_{m}} z_{i}^{I}/)}{_{j}^{B}exp(z_{i}^{T_{m}}  z_{j}^{I}/)}),\] (6)

where \(z^{V}\) is computed as the average of the output \(^{t^{}}\) from the visual encoder. \(\{z^{T_{0}},z^{T_{1}},,z^{T_{M}}\}\) are text embeddings, constructed in Sec. 3.2. All embeddings are mapped into 256-dimensional vectors. Refer to Eq. 5, \(z^{T_{0}}\) denotes a global descriptor selected from the multi-label captions.

Background Mask Proposal.Through video captions, we can roughly determine which frame chunks are background and which ones correspond to actions. This prior helps the action clustering block effectively group and bind the majority of background frames. To enhance the concatenation region between the predicted background mask \(_{B}\) and the caption pseudo mask \(_{GT}\), we employ a per-frame binary mask loss. Following Maskformer , we use dice loss  for our mask loss, i.e., \(_{mask}=_{dice}(_{B},_{GT})\). The binary mask \(_{GT}\) is derived from the neighboring frames \(_{N}^{t}\) of the input to the action clustering block. It is set to \(``1"\) for frames with a caption and \(``0"\) for frames without captions.

Zero-Shot Online InferenceSimilar to CLIP's zero-shot transfer , our distant neighboring-frame Transformer block can assign the current frame image to the semantic category with the highest image-text embedding similarity. During online inference, similar to Eq. 3, the action prediction score namely \(_{DNTR}^{1 C}\) for the current frame of a video clip can be written as \(_{DNTR}=_{N}^{t^{}}[-1]_{val}^{}\).

The action clustering block can **also** estimate frame action without fine-tuning. We calculate the similarity between the embedding of each frame token and the text embedding of the dataset. Then, we assign each frame token to the corresponding category with the highest similarity. This zero-shot transfer pipeline is depicted in Eq. 3. In summary, the final action prediction score \(_{t}^{1 C}\) for a video clip \(V^{t}\) can be expressed as: \(_{t}=_{AC}[-1]+_{DNTR}\).

## 4 Experiments

Our run experiments on NVIDIA V100 \( 8\) using Pytorch 1.11.0. During both training and inference, we resample the video raw frame rate (e.g., 24/30 FPS) to 4 FPS, and resize images to \(224 224\)[49; 46]. For feature extraction, we employ the CLIP model (ViT-L). Specifically, the visual encoder computes a series of image patch tokens along with one global token (aka, CLS token) for each frame, and we utilize the normalized CLS token as the output feature encoding. Unless otherwise specified, all parameters of the visual encoder in our OV-OAD model are initialized from scratch, while the text encoder is initialized with the CLIP, except for the additional Adapter parameters. We train our OV-OAD for 30 epochs with 2 warm-up epochs using the Adam optimizer with weight decay \(5e^{-2}\). It uses a cosine schedule with a batch size of 256, and the initial learning rate is \(1.6e^{-4}\).

Pre-training Datasets.We use the filtered InternVid-10M-FLT (aka, InternVid ) and the ActivityNet v1.3 (aka, ANet ) datasets for training, which are originally collected \(\)4M and 14950 untrimmed video-caption pairs from the web, respectively. However, the videos within the InternVid dataset typically have longer durations compared to ANet, and the average percentage of foreground frames with annotations on these videos is only 27.4%. We sort the \(\)4M videos in the InternVid dataset according to the number of caption annotations they contain and take the top 5000 videos (namely InternVid-5K) for training. For ANet, we utilize the prompting technique (following [35; 12]) to convert the short action tags into sentences. And we combined its training and test sets and utilized them collectively for training. Please see Appendix A.2 for complete dataset preparation.

Benchmarks.We follow previous works [47; 5; 4] and evaluate our model for the zero-shot online action detection on the validation splits of the THUMOS'14 , TVSeries , EPIC-Kitchens-100 (aka, EK100 ), and FineAction  datasets. THUMOS'14 and TVSeries datasets comprise 20and 30 foreground action categories, respectively. EK100 and FineAction datasets comprise 97 verb classes and 106 foreground action labels, respectively. An additional background class is considered for all datasets. Turn to the Appendix A.3 for dataset details.

We evaluate metrics for online motion detection based on previous studies [38; 46; 4]. Specifically, we applied per-frame mean average precision (mAP) on THUMOS'14  and FineAction , and per-frame calibrated average precision (cAP) on TVSeries  and EK100 .

### Comparison with Existing Methods

We conducted a comparison of the zero-shot online motion detection metrics between our method and other zero-shot baselines. We also explore the base-to-novel fine-tuning approach for zero-shot OAD and compare it to our OV-OAD model.

Comparison with Zero-Shot Baselines.We utilize visual language models with image zero-shot capabilities (i.e., CLIP) for comparison. The inference process of online action detection involves sliding frame-by-frame sampling on an untrimmed video and subsequently predicting the action class of the last frame (aka, the current frame). We can set the sliding length to \(1\) and classify the actions based on a single frame image to simplify the inference. To zero-shot transfer CLIP to online action detection, We first extract the features of the frame images using its visual encoder, and then, we compute the similarity between the visual features and the text embedding of the dataset action labels. We can perform several non-parametric processes on the visual embeddings, including: 1) Averaging the visual embeddings of the neighboring frames to obtain the visual feature of the current frame, named CLIP-II; 2) Non-parametric clustering of all sampled frames (e.g., K-means algorithm), followed by averaging the visual embedding of the group to which the current frame belongs, resulting in the visual feature, named CLIP-III; 3) Directly using the visual embedding of the current frame as the visual feature, dubbed as CLIP-I. Table 1 presents the experimental results, clearly demonstrating the superior performance of our OV-OAD over other non-parametric zero-shot methods. It is worth noting that enhancing the scale of the visual language model leads to a moderate increase in single-frame action prediction accuracy. However, this improvement is constrained, indicating that relying solely on robust image discrimination is insufficient for achieving high performance. Consequently, it is essential to incorporate temporal structure information into the learning process for effective online action recognition.

Furthermore, as depicted in Table 2, we delve into the zero-shot performance of OV-OAD on more demanding datasets. We evaluate OV-OAD's performance on the first-view shots dataset called EK100. This dataset comprises first-view shots and notably deviates from the ANet data distribution. We also assess the performance of OV-OAD on the large-scale dataset, FineAction, which includes \(\)4,000 uncut videos categorized into 106 action classes. The outcomes indicate that OV-OAD exhibits superior generalization capabilities in online action detection when contrasted with CLIP.

    &  &  &  \\   & & mAP (\%) & cAP (\%) \\  CLIP-I & ViT/B & 28.0 & 67.7 \\ CLIP-I & ViT/L & 29.6 & 69.3 \\ CLIP-II & ViT/B & 29.1 & 69.5 \\ CLIP-II & ViT/L & 30.9 & 71.1 \\ CLIP-III & ViT/B & 29.7 & 71.6 \\ OV-OAD (IVid) & ViT/B & 33.2 & 73.8 \\ OV-OAD (ANet) & ViT/B & 37.5 & 73.2 \\   

Table 1: Benchmark evaluation on THUMOS’14 and TVSeries.

    &  & FineAction & EK100 (Verb) \\   & & mAP (\%) & cAP (\%) \\  CLIP-I & ViT/B & 26.5 & 40.1 \\ CLIP-II & ViT/B & 27.8 & 39.9 \\ OV-OAD & ViT/B & 29.2 & 41.4 \\   

Table 2: Benchmark evaluation on FineAction and EK100.

    &  & THUMOS’14 \\   & & mAP (\%) \\   & OadTR-D8 & 47.4 \\  & LSTR & 47.7 \\  & MAT-D48 & 48.2 \\   & CLIP-I\({}^{}\) & 28.0 \\  & OV-OAD\({}^{}\) & 37.5 \\   & OadTR-D8 & 33.7 \\  & LSTR & 26.9 \\   & MAT-D48 & 25.5 \\    & CLIP-I\({}^{}\) & 38.6 \\   & OV-OAD\({}^{}\) & 44.6 \\   & OadTR-D8 & 9.6 \\   & LSTR & 9.1 \\   & MAT-D48 & 7.9 \\    & CLIP-I\({}^{}\) & 28.6 \\   & OV-OAD\({}^{}\) & 35.9 \\   

Table 3: Base-to-novel and fully-supervised evaluation on THUMOS’14 dataset.

Comparison with base-to-novel methods.We compare base-to-novel fine-tuning methods and fully-supervised transfer to online action detection on the THUMOS'14 dataset. For base-to-novel generalization, we integrate three well-known Transformer-based online action detection models including OadTR , LSTR  and MAT  with a text encoder using the image-text contrastive loss. To ensure statistical significance, we adopted the random sampling setup and dataset partitioning method proposed by . For our experiments, we employed two evaluation settings on the THUMOS'14 dataset, i.e., training on 75% of the action categories and testing on the remaining 25%, and training on 50% of the categories while testing on the remaining 50%. We followed the experimental fine-tuning setup of , including the Adam optimizer, the learning rate \(1e^{-3}\), and the Cosine decay function for training. For fully supervised transfer, we also train these models on 100% of the action categories with inputs of video frame features extracted by CLIP/ViT-B. We followed the training setup in  for the experiment, which remained consistent except for the different feature extractors. All experimental results are reported in Table 3, \({}^{}\) indicates that the model has not seen any categories and directly tests the performance of the unseen categories. The MAT-D48 indicates that MAT utilizes the ground truth of future frames (48 frames in 12 seconds) during training. We observe that the recent MAT method achieves better performance in the fully-supervised setting, but its performance is poor in the base-to-novel setting. One can find that our OV-OAD model outperforms the competition even without utilizing any training data. The results indicate that the base-to-novel fine-tuning method is not suitable for direct application to the zero-shot online action detection task. The limited availability of data may be a contributing factor to the unsuitability of the base-to-novel fine-tuning approach for online motion detection models.

### Ablations

**Proxy Tasks.** We aim to validate the effectiveness of the three proxy tasks we introduced, namely current frame-text alignment, multi-label video-text alignment, and background mask proposal. As reported in Table 4, our baseline model employs the multi-label video-text contrastive loss \(_{contrast}\) only, upon incorporating the current frame-text matching loss \(_{current}\), we observed a substantial improvement of 3.4% in mean Average Precision on the THUMOS'14 dataset. The performance improvement is due to the model's ability to capture spatio-temporal information from extended past frames. In addition, the prediction of the background frame mask also leads to improvements, and combining both results in optimal performance. This finding suggests that enhancing the model's capability to detect background frames is equally important.

**Number of Layers and Frames.** We first conduct an evaluation to assess the influence of inputting different numbers of neighboring and past frames on the model's performance. As depicted in Table 5, our OV-OAD model demonstrates flexibility with different frame choices, resulting in a maximum performance variation of 1.6%. Note that, the highest mean Average Precision is achieved when utilizing \(_{N}^{t}=8\) and \(_{P}^{t}=24\). In addition, we investigate the effect of the number of network layers in different blocks on the performance. The results are presented in Table 6, we find that increasing the number of layers for the first Transformer Encoder of our action clustering block results in a notable decline in performance. Conversely, a small number of layers for the last Transformer Encoder proves to be sufficient.

**Distant Neighboring-Frame Transformer.** We further investigate the design of the proposed distant neighboring-frame transformer block \(_{DNTR}\). Unless specified otherwise, we employ \(2\)-second neighboring frames, \(6\)-second distant past frames, and CLIP/ViT-B pre-trained features.

**a) Can we remove the \(_{DNTR}\) block?** To implement this, we directly input all 8-second sampled video frames into our action clustering block, where the object-centered group module could easily cluster frame tokens that exhibit similarity. To ensure fairness, we set the number of Transformer layers in the action clustering block to be equal to the total Transformer layers in OV-OAD. As can be seen from Table 7 (row 4 vs. row 1), OV-OAD exceeds this baseline clearly. This also demonstrates the validity of our idea of applying neighboring frames to query spatio-temporal information from distant past frames.

**b) Can we remove the final Transformer encoder in \(_{AC}\) block?** Experiments were conducted to analyze the impact of removing the final transformer encoder on the model's performance. The result presented in the Table 7 (row 5 vs. row 4) indicates a marginal performance decrease of around 0.6% upon removing the final transformer encoder. Additionally, this action results in a 15% reduction in certain training parameters, specifically in the visual encoder.

**c) Can \(_{DNTR}\) block be learned efficiently using the Transformer encoder unit?** Here, we aim to explore whether the \(_{DNTR}\) block can be learned efficiently using Transformer encoders only. To be specific, we combine the neighboring with distant past frames and feed them into a \(4\)-layer \(_{DNTR}\) block based on a standard Transformer encoder implementation. Table 7 (row 4 vs. row 2) illustrates that this baseline is clearly lower than our \(_{DNTR}\) block constituted by the Transformer decoders. Furthermore, we introduced an additional layer of cross-attention after the \(4\)-layer Transformer encoder to create a new baseline. We aim to assess the effectiveness of the "bottleneck" design of cross-attention within the \(_{DNTR}\) block. Note that such an implementation also completes the process of querying discriminative information from distant past frames. Table 7 (row 4 vs. row 3) shows that the cross-attention design does exhibit effectiveness, but it falls short of achieving top performance.

**d) Ablation for the Action Clustering Block.** Here, we analyze the impact of an object-centric decoder compared to a standard Transformer decoder unit within the action clustering block. Both are designed to bind semantically similar static frames into a group embedding. Table 7 (row 5 vs. row 4) demonstrate that the object-centered decoder outperforms the standard transformer decoder in performance.

**On Adapting Text Encoder** For our baseline approach, following the initialization of our OV-OAD's text encoder with the CLIP's text encoder weights, we release the weights to continue training. Then, we conduct experiments to explore the performance impact of two separate modifications: 1) fix the full backbone parameters and 2) incorporate the Adapter structure. The results are depicted in Table 8, one can see that leveraging the Adapter technique leads to a substantial improvement in performance. Moreover, to achieve the best results, it is necessary to release the backbone parameter and continue training.

#### 4.2.1 Inference Speed.

We compared the model parameters and efficiency of our OV-OAD model with other methods on a single NVIDIA Tesla V100 GPU, given in Tab. 9. Note traditional supervised learning methods, the efficiency bottleneck of the system is primarily attributed to the optical flow computation and its feature extraction. Our OV-OAD eliminates the need for optical flow computation and the extraction of spatio-temporal features from the RGB image. The overall system achieves an impressive inference speed of 292.3 frames per second (FPS). The findings indicate that our model has the potential to be deployed on standard online video capture devices, enabling real-time action prediction capabilities. In particular, LSTR demands a larger number of input frames for optimal performance, using 520 seconds of video for inference on THUMOS'14, while our OV-OAD utilizes only 8 seconds. This means that LSTR requires 65 times more data than OV-OAD, which likely explains why our model's inference speed is six times faster. Furthermore, the primary speed bottleneck for LSTR is the extraction of optical flow (8.1 FPS), whereas for our model, it is the extraction of image features (292.3 FPS).

### Limitations

End-to-end online motion detection systems require simultaneous learning of spatial and temporal structures for optimal results. Our OV-OAD model utilizes the CLIP's visual encoder to extract features from pure images. This would cause the network to focus too much on modeling foreground-centered spatial information at the expense of modeling spatio-temporal structure information. This does not fit the requirements of action recognition for visual representations since learned RGB features from a video commonly contain some of the temporal structural information, e.g., RGB features extracted by TSN  have some of the properties of optical flow features. Therefore extending OV-OAD to simultaneously model scenario and temporal information and enable action recognition with an open vocabulary remains a challenging problem.

Failure Cases.Our objective is to identify categories that exhibit poor recognition as well as those with high recognition rates. We provide a list of categories with the highest and lowest action recognition average precision in Table 10. Additionally, we present visual samples of these categories in Fig. 3. We find that the detection accuracy decreases in scenarios where the foreground of the action is relatively low, and multiple actions share similar backgrounds (e.g., "CliffDiving" and "CliffShot"). However, OV-OAD demonstrates better performance when the foreground or interacting objects are more distinct, as observed in cases like "CleanJerk" and "PoleVault". These findings suggest that future enhancements can focus on improving the recognition of fine-grained actions through joint modeling of spatio-temporal information.

## 5 Conclusion

In our study, we take the initial step towards leveraging text learning for online action detection without explicit human supervision. Our findings demonstrate that by employing OV-OAD, reproductions acquired from large-scale video-text pairs, even with noise, can be successfully transferred to online action detection in a zero-shot manner. Moreover, we highlight that the conventional approach of base-to-novel fine-tuning does not yield favorable results on traditional online action detection test datasets. Instead, we illustrate that similar semantic frames can be directly clustered and transferred to downstream action detection datasets using abundant textual supervision.