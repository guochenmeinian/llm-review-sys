# Rethinking Inverse Reinforcement Learning:

from Data Alignment to Task Alignment

 Weichao Zhou

Boston University

Boston, MA 02215

zwc662@bu.edu

&Wenchao Li

Boston University

Boston, MA 02215

wenchao@bu.edu

###### Abstract

Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstrations. However, the inferred reward function often fails to capture the underlying task objective. In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. It adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy's ability to accomplish the task. We provide theoretical insights into this framework's ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios. The complete code are available at https://github.com/zwc662/PAGAR.

## 1 Introduction

Inverse reinforcement learning (IRL) Ng and Russell (2000); Finn et al. (2017) has become a popular method for imitation learning (IL), allowing policies to be trained by learning reward functions from expert demonstrations Abbeel and Ng (2004); Ho and Ermon (2016). Despite its widespread use, IRL-based IL faces significant challenges that often stem from overemphasizing data alignment rather than task alignment. For instance, reward ambiguity, where multiple reward functions can be consistent with the expert demonstrations, makes it difficult to identify the correct reward function. This problem persists even when there are infinite data Ng and Russell (2000); Cao et al. (2021); Skalse et al. (2022, 2022). Additionally, limited availability of demonstrations can further exacerbate this problem, as the data may not fully capture the nuances of the task. Misaligned reward functions can lead to policies that optimize the wrong objectives, resulting in poor performance and even reward hacking Hadfield-Menell et al. (2017); Amodei et al. (2016); Pan et al. (2022), a phenomenon where the policy exploits loopholes in the inferred reward function. These challenges highlight the limitation of exclusively pursuing data alignment in solving real-world tasks.

In light of these considerations, this paper advocates for a paradigm shift from a narrow focus on data alignment to a broader emphasis on task alignment. Grounded in a general formalism of task objectives, we propose identifying the task-aligned reward functions that more accurately reflect the underlying task objectives in their policy utility spaces. Expanding on this concept, we explore the intrinsic relationship between the task objective, reward, and expert demonstrations. This relationship leads us to a novel perspective where expert demonstrations can serve as weak supervision signals for identifying a set of candidate task-aligned reward functions. Under these reward functions, the expertachieves high -- but not necessarily optimal -- performance. The rationale is that achieving high performance under a task-aligned reward function is often adequate for real-world applications.

Building on this premise, we leverage IRL to derive the set of candidate task-aligned reward functions and propose Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised framework designed to mitigate task-reward misalignment by training a policy with this candidate reward set. PAGAR adopts an adversarial training mechanism between a protagonist policy and an adversarial reward searcher, iteratively improving the policy learner to attain high performance across the candidate reward set. This method moves beyond relying on deriving a single reward function from data, enabling a collective validation of the policy's similarity to expert demonstrations in terms of effectiveness in accomplishing tasks. Experimental results show that our algorithm outperforms baselines on complex IL tasks with limited demonstrations and in challenging transfer environments. We summarize our contributions below.

* Introduction of Task Alignment in IRL-based IL: We present a novel perspective that shifts the focus from data alignment to task alignment, addressing the root causes of reward misalignment in IRL-based IL.
* Protagonist Antagonist Guided Adversarial Reward (PAGAR): We propose a new semi-supervised framework that leverages adversarial training to improve the robustness of the learned policy.
* Practical Implementation: We present a practical implementation of PAGAR, including the adversarial reward searching mechanism and the iterative policy-improving process. Experimental results demonstrate superior performances in complex and transfer learning environments.

## 2 Related Works

IRL-based IL circumvents many challenges of traditional IL such as compounding error Ross and Bagnell (2010); Ross et al. (2011); Zhou et al. (2020) by learning a reward function to interpret the expert behaviors Ng et al. (1999); Ng and Russell (2000) and then learning a policy from the reward function via reinforcement learning (RL)Sutton and Barto (2018). However, the learned reward function may not always align with the underlying task, leading to reward misspecification Pan et al. (2022); Skalse and Abate (2022), reward hacking Skalse et al. (2022), and reward ambiguity Ng and Russell (2000); Cao et al. (2021). The efforts on alleviating reward ambiguity include Max-Entropy IRL Ziebart et al. (2008); Max-Margin IRL Abbeel and Ng (2004); Ratliff et al. (2006), and Bayesian IRL Ramachandran and Amir (2007). GAN-based methods Ho and Ermon (2016); Jeon et al. (2018); Finn et al. (2016); Peng et al. (2019); Fu et al. (2018) use neural networks to learn reward functions from limited demonstrations. However, these efforts that aim to address reward ambiguity fall short of mitigating the general impact of reward misalignment which can be caused by various reasons such as IRL making false assumptions about the relationship between expert policy and expert reward function Skalse et al. (2022); Hong et al. (2023). Other attempts to mitigate reward misalignment involve external information other than expert demonstrations Hejna and Sadigh (2023); Zhou and Li (2018, 2022a, 2018). Our work adopts the generic setting of IRL-based IL without needing additional information. The idea of considering a reward set instead of focusing on a single reward function is supported by Metelli et al. (2021) and Lindner et al. (2022). However, these works target reward ambiguity instead of reward misalignment. Our protagonist and antagonist setup is inspired by the concept of unsupervised environment design (UED) Dennis et al. (2020). In this paper, we develop novel theories in the context of reward learning.

## 3 Preliminaries

**Reinforcement Learning (RL)** models the environment as a Markov Decision Process \(=,,,d_{0}\) where \(\) is the state space, \(\) is the action space, \(\) is the transition probability, \(d_{0}\) is the initial state distribution. A _policy_\((a|s)\) determines the probability of an RL agent performing an action \(a\) at state \(s\). By successively performing actions for \(T\) steps from an initial state \(s^{(0)} d_{0}\), a _trajectory_\(=s^{(0)}a^{(0)}s^{(1)}a^{(1)} s^{(T)}\) is produced. A state-action based _reward function_ is a mapping \(r:\). The soft Q-value function of \(\) is \(_{}(s,a)=r(s,a)+}_{s^{} (|s,a)}[_{}(s^{})]\) where \((0,1]\) is a discount factor, \(_{}\) is the soft state-value function of \(\) defined as \(_{}(s):=}_{a(|s)}[ _{}(s,a)]+((|s))\), and \(((|s))\) is the entropy of \(\) at state \(s\)The soft advantage of performing action \(a\) at state \(s\) and then following a policy \(\) afterwards is \(_{}(s,a)=_{}(s,a)-V_{}(s)\). The expected return of \(\) under a reward function \(r\) is given as \(U_{r}()=}_{}[_{t=0}^{}^{t} r (s^{(t)},a^{(t)})]\). With a slight abuse of notations, we denote the entropy of a policy as \(():=}_{}[_{t=0}^{}^ {t}((|s^{(t)}))]\). The standard RL learns an _optimal policy_ by maximizing \(U_{r}()\). The entropy regularized RL learns a _soft-optimal_ policy by maximizing the objective function \(_{RL}(;r):=U_{r}()+()\).

**Inverse Reinforcement Learning (IRL)** assumes that a set \(E=\{_{1},,_{N}\}\) of expert demonstrations are sampled from the roll-outs of the expert's policy \(_{E}\) and aims to learn the expert reward function \(r_{E}\). IRL Ng and Russell (2000) assumes that \(_{E}\) is _optimal_ under \(r_{E}\) and learns \(r_{E}\) by maximizing the margin \(U_{r}(E)-_{}U_{r}()\) while Maximum Entropy IRL (MaxEnt IRL) Ziebart et al. (2008) maximizes an entropy regularized objective function \(_{IRL}(r)=U_{r}(E)-(_{}U_{r}()+())\).

**Generative Adversarial Imitation Learning (GAIL)** Ho and Ermon (2016) draws a connection between IRL and Generative Adversarial Nets (GANs) as shown in Eq.1, where a discriminator \(D:\) is trained by minimizing Eq.1 so that \(D\) can accurately identify any \((s,a)\) generated by the agent. Meanwhile, an agent policy \(\) is trained as a generator to maximize Eq.1 so that \(D\) cannot discriminate \(\) from \(_{E}\). Adversarial inverse reinforcement learning (AIRL) Fu et al. (2018) uses a neural-network reward function \(r\) to represent \(D(s,a):=\), rewrites \(_{IRL}\) as minimizing Eq.1, and proves that the optimal reward satisfies \(r^{*}_{E}_{_{E}}\). By training \(\) with \(r^{*}\) until optimality, \(\) will behave just like \(_{E}\).

\[}_{(s,a)}[ D(s,a))]+}_{(s,a)_{E}}[(1-D(s,a))]\] (1)

## 4 Task-Reward Alignment

In this section, we formalize the concept of task-reward misalignment in IRL-based IL. We start by defining a notion of task based on the framework from Abel et al. (2021).

**Definition 1** (**Task**).: Given the policy hypothesis set \(\), a **task**\((,_{task},_{acc})\) is specified by a partial order \(_{task}\) over \(\) and a non-empty set of acceptable policies \(_{acc}\) such that \(_{1}_{acc}\) and \(_{2}_{acc}\), \(_{2}_{task}_{1}\) always hold.

**Remark:** The notions of policy acceptance and order allow the definition of **task** to accommodate a broad range of real-world tasks1 including the standard RL tasks (learning the optimal policy from a reward function \(r\)): given a reward function \(r\) and a policy hypothesis set \(\), the standard RL **task** can be written as a tuple \((,_{task},_{acc})\) where \(_{task}\) satisfies \(_{1},_{2},_{1}_{task}_{2} U_{r }(_{1}) U_{r}(_{2})\), and \(_{acc}=\{^{}.^{}_{task}\}\) contains all the optimal policies.

Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function \(r^{+}\) and a task-misaligned reward function \(r^{-}\). The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of \(r^{+}\), the utilities of all acceptable policies are higher (\(_{r^{+}}\)) than those of the unacceptable ones, and the policies with utilities higher than \(_{r^{+}}\) have higher orders than those of utilities lower than \(_{r^{+}}\). Within the utility space of \(r^{-}\), acceptable and unacceptable policies’ utilities are mixed together, leading to a low \(_{r^{-}}\) and an even lower \(_{r^{-}}\). (b) IRL-based IL relies solely on IRL’s optimal reward function \(r^{*}\) which can be task-misaligned and lead to an unacceptable policy \(_{r^{*}}_{acc}\) while PAGAR-based IL learns an acceptable policy \(^{*}_{acc}\) from a set \(R_{E,}\) of reward functions.

Designing reward function(s) that align with the underlying task is essential in RL. Whether a designed reward aligns with the task hinges on how policies are ordered by the task and the utilities of the policies under the reward function. Therefore, we define the task-reward alignment by examining the utility spaces of the reward functions. If the acceptable policy set \(_{acc}\) of the task is given, we let \(_{r}:=_{_{acc}}U_{r}()\) be the minimal utility achieved by any acceptable policy under \(r\).

**Definition 2** (**Task-Aligned Reward Functions**).: A reward function is a _task-aligned reward function_ (denoted as \(r^{+}\)) if and only if \(_{acc},U_{r^{+}}()<_{r^{+}}()\). Conversely, if this condition is not met, it is a _task-misaligned reward function_ (denoted as \(r^{-}\)).

The definition suggests that under a task-aligned reward function \(r^{+}\), all acceptable policies for the task yield higher utilities than unacceptable ones. It also suggests that a policy is deemed acceptable as long as its utility is greater than \(_{r^{+}}\) for some task-aligned reward function \(r^{+}\), even if this policy is not optimal. We also examine whether high utility under a reward function \(r\) suggests a higher order under \(_{task}\). We define \(_{r}:=_{}U_{r}()\ s.t.\ _{1},_{2} ,U_{r}(_{1})<U_{r}() U_{r}(_{2})(_{1} _{task})(_{1}_{task}_{2})\), which is the highest utility threshold such that any policy achieving a higher utility than \(_{r}\) has a higher order than those achieving lower utilities than \(_{r}\). In Figure 1(a) we illustrate how \(_{r}\) and \(_{r}\) vary between task-aligned and misaligned reward functions.

**Proposition 1**.: _Given the policy order \(_{task}\) of a task, for any two reward functions \(r_{1},r_{2}\), if \(\{ U_{r_{1}}()_{r_{1}}\}\{ U_{r_{2}} ()_{r_{2}}\}\), then there must exist policies \(_{1}\{ U_{r_{1}}()_{r_{1}}\},_{2}\{ U _{r_{2}}()_{r_{2}}\}\) such that \(U_{r_{1}}(_{2}) U_{r_{1}}(_{1})\) and \(_{2}_{task}_{1}\) while \(U_{r_{2}}(_{2}) U_{r_{2}}(_{1})\)._

This proposition implies that a high threshold \(_{r}\) indicates that a high utility corresponds to a high order in terms of \(_{task}\). In particular, for any task-aligned reward function \(r^{+}\), \(\{ U_{r^{+}}()_{r^{+}}\}_{acc}\{  U_{r^{+}}()_{r^{+}}\}\) (see proof in Appendix A.2). Thus, a small \(\{ U_{r^{+}}()_{r^{+}}\}\) leads to a large \(\{ U_{r^{+}}()[_{r^{+}},_{r^{+}}]\}\). Hence, a task-aligned reward function \(r^{+}\) is more likely to be aligned with the task if it has a wide \([_{r^{+}},_{r^{+}}]\) and a narrow \([_{r^{+}},_{}U_{r^{+}}()]\).

### Mitigate Task-Reward Misalignment in IRL-Based IL

In IRL-based IL, a key challenge is that _the underlying task is unknown_, making it difficult to assert if a learned policy is acceptable. We denote the optimal reward function learned from the demonstration set \(E\) as \(r^{*}\), and the optimal policy under \(r^{*}\) as \(_{r^{*}}\). When \(_{r^{*}}\) has a poor performance under \(r_{E}\), it is considered to have a high \(Regret(_{r^{*}},r_{E})\) which is defined in Eq.2. If \(Regret(_{r^{*}},r_{E})>_{^{}}U_{r_{E}}(^{ })-_{r_{E}}\), then \(_{r^{*}}\) is unacceptable and \(r^{*}\) is task-misaligned.

\[Regret(,r):=_{^{}}U_{r}(^{})-U_{r}()\] (2)

Several factors can lead to a high \(Regret(_{r^{*}},r_{E})\). For instance, Viano et al. (2021) shows that when expert demonstrations are collected in an environment whose dynamical function differs from that of the learning environment, \(|Regret(_{r^{*}},r_{E})|\) can be positively related to the discrepancy between those dynamical functions. Additionally, we prove in Appendix A.1 that learning from only a few representative expert trajectories can also result in a large \(|Regret(_{r^{*}},r_{E})|\) with a high probability.

Our insight for mitigating such potential task-reward misalignment in IRL-based IL is to _shift our focus from learning an optimal policy that maximizes the intrinsic \(r_{E}\) to learning an acceptable policy \(^{*}\) that achieves a utility higher than \(_{r^{+}}\) under any task-aligned reward function \(r^{+}\)_. Our approach is to treat the expert demonstrations as weak supervision signals based on the following.

**Theorem 1**.: _Let \(\) be an indicator function. For any \(k_{r^{+}}\ _{}\{U_{r^{+}}() U_{r^{+}}(_{E})\} }\), if \(^{*}\) satisfies \(_{}\{U_{r}() U_{r}(^{*})\}}<| _{acc}|\) for all \(r R_{E,k}:=r_{}\{U_{r}() U_{r}( _{E})\} k}\), then \(^{*}\) is an acceptable policy, i.e., \(^{*}_{acc}\). Additionally, if \(k<|_{acc}|\), such an acceptable policy \(^{*}\) is guaranteed to exist._

The statement suggests that we can obtain an acceptable policy by training it to attain high performance across a reward function set \(R_{E,k}\) that includes all the reward functions where, for each reward function at most \(k\) policies outperform the expert policy \(_{E}\). The minimal value of \(k\) is determined by all the task-aligned reward functions in the reward hypothesis set. Appendix A.2 provides the proof.

**How to build \(R_{E,k}\)?** Building \(R_{E,k}\) involves setting the parameter \(k\). If \(r_{E}\) is a task-aligned reward function and \(_{E}\) is optimal solely under \(r_{E}\), then the minimal \(k=0\), and \(R_{E,0}\) only contains \(r_{E}\). However, relying on a singleton \(R_{E,0}\) equates to applying vanilla IRL, which is susceptible to misalignment issues, as noted earlier. It is crucial to recognize that \(r_{E}\) might not meet the task-aligned reward function criteria specified in Definition 2, even though its optimal policy \(_{E}\) is acceptable. This situation necessitates a positive \(k\), thereby expanding \(R_{E,k}\) beyond a single function and changing the role of expert demonstrations from strong supervision to weak supervision. Note that we suggest letting \(k|_{acc}|\) instead of allowing \(k\) because \(R_{E,}\) would then encompass all possible reward functions, and it is impractical to identify a policy capable of achieving high performance across all reward functions. Letting \(k|_{acc}|\) guarantees there exists a feasible policy \(^{*}\), e.g., \(_{E}\) itself. As the task alignment of each reward function typically remains unknown in IRL settings, this paper proposes treating \(k\) as an adjustable parameter - starting with a small \(k\) and adjusting based on empirical learning outcome, allowing for iterative refinement for alignment with task requirements.

In practice, \(\) can be uncountable, e.g., a Gaussian policy. Hence, we adapt the concept of \(k\) in \(R_{E,k}\) to a hyperparameter \(^{*}:=_{}_{IRL}(r)\), leading us to redefine \(R_{E,k}\) as a _\(\)-optimal reward function set \(R_{E,}:=\{r_{IRL}(r)\}\)_. This superlevel set includes all the reward functions under which the optimal policies outperform the expert by at most \(-\). If \(\) is appropriately selected such that \(R_{E,}\) includes task-aligned reward functions, we can mitigate reward misalignment by satisfying the conditions outlined in Definition 3, which are closely related to Definition 2 and Proposition 1.

**Definition 3** (**Mitigation of Task-Reward Misalignment)**.: Assuming that the reward function set \(R_{E,}\) contains task-aligned reward function \(r^{+}\)'s, the mitigation of task-reward misalignment in IRL-based IL is to learn a policy \(^{*}\) such that (i) (Weak Acceptance) \( r^{+} R_{E,}\), \(U_{r^{+}}(^{*})_{r^{+}}\), or (ii) (Strong Acceptance) \( r^{+} R_{E,},U_{r^{+}}(^{*})_{r^{+}}\).

While condition (i) states that \(^{*}\) is acceptable for the task, i.e., \(^{*}_{acc}\), condition (ii) further states that \(^{*}\) have a high order in terms of \(_{task}\). Hence, condition (i) is weaker than (ii) because a policy \(^{*}\) satisfying (ii) automatically satisfies (i) according to Definition 2. Given the uncertainty in identifying which reward function is aligned, our solution is to **train a policy to achieve high utilities under all reward functions** in \(R_{E,}\) to satisfy the conditions in Definition 3. We explain this approach in the following semi-supervised paradigm, PAGAR.

## 5 Protagonist Antagonist Guided Adversarial Reward (PAGAR)

PAGAR is an adversarial reward searching paradigm which iteratively searches for a reward function to challenge a policy learner by incurring a high regret as defined in Eq.2. We refer to the policy to be learned as the _protagonist policy_ and re-write it as \(_{P}\). We then introduce a second policy, dubbed _antagonist policy_\(_{A}\), as a proxy of the \(_{^{}}U_{r}(^{})\) for Eq.2. For each reward function \(r\), we call the regret of \(_{P}\) under \(r\), i.e., \(Regret(_{P},r)=_{_{A}}U_{r}(_{A})-U_{r}(_{P})\), the _Protagonist Antagonist Induced Regret_. We then formally define PAGAR in Definition 4.

**Definition 4** (Protagonist Antagonist Guided Adversarial Reward (**PAGAR)**).: Given a candidate reward function set \(R\) and a protagonist policy \(_{P}\), PAGAR searches for a reward function \(r\) within \(R\) to maximize the _Protagonist Antagonist Induced Regret_, i.e., \(_{r R}Regret(_{P},r)\).

**PAGAR-based IL** _learns a policy from \(R_{E,}\) by minimizing the worst-case Protagonist Antagonist Induced Regret_ via \(MinimaxRegret(R_{E,})\) as defined in Eq.3 where \(R\) can be any input reward function set and is set as \(R=R_{E,}\) in PAGAR-based IL.

\[MinimaxRegret(R):=_{_{P}}_{r R}Regret( _{P},r)\] (3)

Our subsequent discussion will focus on identifying the sufficient conditions for PAGAR-based IL to mitigate task-reward misalignment as described in Definition 3. In particular, we consider the case where \(_{IRL}(r):=U_{r}(E)-_{}U_{r}()\). We use \(L_{r}\) to denote the Lipschitz constant of \(r()\), and \(W_{E}\) to denote the smallest Wasserstein \(1\)-distance \(W_{1}(,E)\) between \(\) of any \(\) and \( E\), i.e., \(W_{E}_{}W_{1}(,E)\). Then, we have Theorem 2.

**Theorem 2** (Weak Acceptance).: _If the following conditions (1) (2) hold for \(R_{E,}\), then the optimal protagonist policy \(_{P}:=MinimaxRegret(R_{E,})\) satisfies \( r^{+} R_{E,}\), \(U_{r^{+}}(_{P}) U_{r^{+}}\)._1. _There exists_ \(r^{+} R_{E,}\)_, and_ \(_{r^{+} R_{E,}}\ \{_{}U_{r^{+}}()-_{r^{+}}\}< _{r^{+} R_{E,}}\ \{_{r^{+}}-U_{r^{+}}\}\)_;_
2. \( r^{+} R_{E,}\)_,_ \(L_{r^{+}} W_{E}-_{}U_{r^{+}}()-_{r ^{+}}\) _and_ \( r^{-} R_{E,}\)_,_ \(L_{r^{-}} W_{E}-<_{r^{+} R_{E,}}\ \{_{r^{+}}-U_{r^{+}}\}\)_._

This statement shows the conditions for PAGAR-based IL to attain the _'Weak Acceptance'_ goal described in Definition 3. The condition (1) states that the task-aligned reward functions in \(R_{E,}\) all have a high level of alignment in matching \(_{task}\) within their high utility ranges. The condition (2) requires that for the policy \(^{*}=_{}W_{1}(,E)\), the performance difference between \(E\) and \(^{*}\) is small enough under all \(r R_{E,}\). Since for each reward function \(r R_{E,}\), the performance difference between \(E\) and the optimal policy under \(r\) is bounded by \(\), condition (2) implicitly requires that \(^{*}\) not only performs well under any task-aligned reward function \(r^{+}\) (thus being acceptable in the task) but also achieve relatively low regret under task-misaligned reward function \(r^{-}\). However, the larger the rage \([_{r^{+}},_{r^{+}}]\) is across the task-aligned reward function \(r^{+}\), the less strict the requirement for low regret under \(r^{-}\) becomes. The proof can be found in Appendix A.5. The following theorem further suggests that a \(\) close to its upper-bound \(^{*}:=_{r}_{IRL}(r)\) can help \(MinimaxRegret(R_{E,})\) gain a better chance of finding an acceptable policy for the underlying task and attain the _'Strong Acceptance'_ goal described in Definition 3.

**Theorem 3** (Strong Acceptance).: _Assume that the condition (1) in Theorem 2 holds for \(R_{E,}\). If for any \(r R_{E,}\), \(L_{r} W_{E}-_{r^{+} R_{E,}}\ \{_{}U_{r^{+}}()- _{r^{+}}\}\), then the optimal protagonist policy \(_{P}=MinimaxRegret(R_{E,})\) satisfies \( r^{+} R_{E,}\), \(U_{r^{+}}(_{P})_{r^{+}}\)._

**When do these assumptions hold?** The condition (1) in Theorem 2 requires all the task-aligned reward functions in \(R_{E,}\) exhibit a high level of conformity with the policy order \(_{task}\). Being task-aligned already sets a strong premise for satisfying this condition. We further posit that this condition is more easily satisfied when the task has a binary outcome, such as in reach-avoid tasks so that the aligned and misaligned reward functions tend to have higher discrepancy than tasks with quantitative outcomes. In the experimental section, we validate this hypothesis by evaluating tasks of this kind. Regarding condition (2) of Theorem 2 and the assumptions of Theorem 3, which basically require the existence of a policy with low regret across \(R_{E,}\) set, it is reasonable to assume that expert policy meets this criterion.

### Comparing PAGAR-Based IL with IRL-Based IL

We illustrate the difference between IRL-based IL and PAGAR-based IRL in Fig.1(b). While IRL-based IL aims to learn the optimal policy \(_{r^{*}}\) under the IRL-optimal reward \(r^{*}\), PAGAR-based IL learns a policy \(^{*}\) from the reward function set \(R_{E,}\). Both PAGAR-based IL and IRL-based IL are zero-sum games between a policy learner and a reward learner. However, while IRL-based IL only aims to reach equilibrium at a single reward function under strong assumptions, e.g., sufficient demonstrations, convex reward and policy spaces, etc., PAGAR-based IL can reach equilibrium with a **mixture of reward functions** without those assumptions.

**Proposition 2**.: _Given arbitrary reward function set \(R\), there exists a constant \(c\) and a distribution \(_{}\) over \(R\) such that \(MinimaxRegret(R)\) yields the same policy as \(_{r}\ \{^{*})}{c-U_{^{*}}()}  U_{r_{}^{*}}()+_{}(r)}{}[(1- ()}) U_{r}()]\}\) where \(r_{}^{*}=_{r R}U_{r}()\ s.t.\ r_{r^{ } R}Regret(,r^{})\)._

A detailed derivation can be found in Theorem 6 in Appendix A.4. In a nutshell, \(_{}(r)\) is a baseline distribution over \(R\) such that (i) \(c_{}}{}\ [U_{r}()]\) holds for all the \(\)'s that do not always perform worse than any other policy under \(r R\), (ii) among all the \(R_{}\)'s that satisfy the condition (i), we pick the one with the minimal \(c\); and (iii) for any other policy \(\), \(_{}\) uniformly concentrates on \(_{r R}U_{r}()\). Note that in PAGAR-based IL, where \(R_{E,}\) is used in place of arbitrary \(R\), \(_{}\) is a distribution over \(R_{E,}\) and \(r_{}^{*}\) is constrained to be within \(R_{E,}\). Essentially, the mixed reward functions dynamically assign weights to \(r_{}\) and \(r_{}^{*}\) depending on \(\). If \(\) performs worse under \(r_{}^{*}\) than under many other reward functions (\(U_{r_{}^{*}}()\) falls below \(c\)), a higher weight will be allocated to using \(r_{}^{*}\) to train \(\). Conversely, if \(\) performs better under \(r_{}^{*}\) than under many other reward functions (\(c\) falls below \(Ur_{}^{*}()\)), a higher weight will be allocated to reward functions drawn from \(_{}\). Furthermore, we prove in Appendix A.7 that the \(MinimaxRegret\) objective function defined in Eq.3 is a convex optimization w.r.t the protagonist policy \(_{P}\).

We also prove in Appendix A.8 that when there is no misalignment issue, i.e., under the ideal conditions for IRL, PAGAR-based IL can either guarantee inducing the same results as IRL-based IL with \(=_{r}_{IRL}(r)\), or guarantee inducing an acceptable \(_{P}\) by making \(_{r}_{IRL}(r)-\) no greater than \(_{}U_{r^{+}}()-_{r^{+}}\) for \(r^{+} R_{E,}\).

## 6 A Practical Approach to Implementing PAGAR-based IL

We solve \(MinimaxRegret(R_{E,})\), by alternating between policy learning and reward search. Based on Eq.3, we introduce an on-and-off policy learning framework and an adversarial reward search objective. Moreover, we embed the constraint \(r R_{E,}\) into the reward search objective using IRL, resulting in a **meta-algorithm** compatible with various IRL methods.

### Policy Optimization with On-and-Off Policy Samples

Given an intermediate learned reward function \(r\), we use RL to train \(_{P}\) to minimize the regret \(_{_{P}}Regret(_{P},r)=_{_{P}}\{_{ _{A}}U_{r}(_{A})\}-U_{r}(_{P})\) as indicated by Eq.3 where \(_{A}\) is trained to serve as the optimal policy under \(r\) as noted in Section 5. Since we have to sample trajectories with \(_{A}\) and \(_{P}\), we propose to combine off-policy and on-policy samples to optimize \(_{P}\) so that we can leverage the samples maximally. **Off-Policy:** We leverage the Theorem 1 in Schulman et al. (2015) to derive a bound for the utility subtraction: \(U_{r}(_{P})-U_{r}(_{A})_{s}_{_{A}}(s )_{a}_{P}(a|s)A_{_{A}}(s,a)+C_{ s}D_{TV}(_{A}(|s),_{P}(|s))^{2}\) where \(_{_{A}}(s)=_{t=0}^{T}^{t}Prob(s^{(t)}=s|_{A})\) is the discounted visitation frequency of \(_{A}\), \(A_{_{A}}\) is the advantage function without considering the entropy, and \(C\) is some constant. Then we follow the derivation in Schulman et al. (2017), which is based on Theorem 1 in Schulman et al. (2015), to derive from the inequality an importance sampling-based objective function \(_{_{A}}(_{P};r):=_{s_{A}}[((s,a) A _{_{A}}(s,a),clip((s,a),1-,1+) A_{_{A}}(s,a)]\) where \(\) is a clipping threshold, \((s,a)=(a|s)}{_{A}(a|s)}\) is an importance sampling rate. The details can be found in Appendix B.1. This objective function allows us to train \(_{P}\) by using the trajectories of \(_{A}\). **On-Policy:** We also optimize \(_{P}\) with the standard RL objective function \(_{RL}(_{P};r)\) by using the trajectories of \(_{P}\) itself. As a result, the objective function for optimizing \(_{P}\) is \(_{_{P}}_{_{A}}(_{P};r)+_{RL} (_{P};r)\). As for \(_{A}\), we only use the standard RL objective function, i.e., \(_{_{A}}_{RL}(_{A};r)\). Although the computational complexity equals the sum of the complexities of RL update steps for \(_{A}\) and \(_{P}\), these two RL update steps can be executed in parallel.

### Regret Maxmization with On-and-Off Policy Samples

Given the intermediate learned protagonist and antagonist policy \(_{P}\) and \(_{A}\), according to \(MinimaxRegret\) in Eq.3, we need to optimize \(r\) to maximize \(U_{r}(_{A})-U_{r}(_{P})\). In practice, we found that the subtraction between the estimated \(U_{r}(_{A})\) and \(U_{r}(_{P})\) can have a high variance. To resolve this issue, we derive two reward improvement bounds to approximate this subtraction.

**Theorem 4**.: _Suppose policy \(_{2}\) is the optimal solution for \(_{RL}(;r)\). Then, the inequalities Eq.4 and 5 hold for any policy \(_{1}\), where \(=_{s}D_{TV}(_{1}(|s),_{2}(|s))\), \(=_{s,a}|_{_{2}}(s,a)|\), and \((s)=}{}[_{_{2}}(s,a)]-}{}[_{_{2}}(s,a)]\)._

\[|U_{r}(_{1})-U_{r}(_{2})-_{t=0}^{} ^{t}_{1}}{}[(s^{ (t)})]| }\] (4) \[|U_{r}(_{1})-U_{r}(_{2})-_{t=0}^{} ^{t}_{2}}{}[(s^{ (t)})]| }\] (5)By letting \(_{P}\) be \(_{1}\) and \(_{A}\) be \(_{2}\), Theorem 4 enables us to bound \(U_{r}(_{A})-U_{r}(_{P})\) by using either only the samples of \(_{A}\) or only those of \(_{P}\). Following Fu et al. (2018), we let \(r\) be a proxy of \(_{_{2}}\) in Eq.4 and 5. Then we derive two loss functions \(_{R,1}(r;_{P},_{A})\) and \(_{R,2}(r;_{P},_{A})\) for \(r\) as shown in Eq.6 and 7 where \(C_{1}\) and \(C_{2}\) are constants proportional to the estimated maximum KL divergence between \(_{A}\) and \(_{P}\) (to bound \(\) Schulman et al. (2015)). The objective function for \(r\) is then \(_{PAGAR}:=_{R,1}+_{R,2}\). The complexity equals that of computing the reward along the trajectories sampled from \(_{A}\) and \(_{P}\).

\[_{R,1}(r;_{P},_{A}) :=}_{_{A}}_{t=0}^{} ^{t}((s^{(t)},a^{(t)})-1) r(s^{(t)},a^{(t)})+C _{1}_{(s,a)_{A}} r(s,a)\] (6) \[_{R,2}(r;_{P},_{A}) :=}_{_{P}}_{t=0}^{} ^{t}(1-,a^{(t)})}) r(s^{(t)},a^{(t)} )+C_{2}_{(s,a)_{P}} r(s,a)\,\] (7)

### A Meta-Algorithm for Solving PAGAR-Based IL

Given an IRL objective function \(_{IRL}\), we enforce the constraint \(r R_{E,}\) by adding to \(_{PAGAR}(r;_{P},_{A})\) a penalty term \((-_{IRL})\), where \(\) is a Lagrangian parameter. The resulting objective function for optimizing \(r\) becomes \(_{r R}_{PAGAR}(r;_{P},_{A})+( -_{IRL}(r))\).

We initialize \(\) with a large value to prioritize satisfying the constraint \(r R_{E,}\) and update it based on \(-_{IRL}\) (the details can be found in Appendix B.4). Algorithm 1 outlines our meta-algorithm for PAGAR-based IL. The algorithm takes an IRL objective \(_{IRL}\) as an input, and alternates between policy and reward learning. In line \(3\), \(_{A}\) is trained via RL with its own sample set \(_{A}\). In line \(4\), we train \(_{P}\) via the on-and-off policy approach in Section 6.1 with both \(_{A}\) and \(_{P}\)'s sample set \(_{P}\). Finally, in line \(5\), \(_{PAGAR}\) is estimated from both \(_{A}\) and \(_{P}\), while \(_{IRL}\) is from \(_{A}\) and \(E\).

## 7 Experiments

The goal of our experiments is to assess whether using PAGAR-based IL can efficiently mitigate reward misalignment under conditions that are not ideal for IRL. We present the main results below and provide details and additional results in Appendix C.

### Discrete Navigation Tasks

**Benchmarks:** We consider a maze navigation environment where the task objective is compatible with Definition 1. Our benchmarks include two discrete domain tasks from the Mini-Grid environments Chevalier-Boisvert et al. (2023): _DoorKey-&x6-v0_, and _SimpleCrossingS9N1-v0_. In both tasks, the agent needs to interact with the environmental objects which are **randomly positioned in every episode while the agent can only observe a small, unblocked area in front of it**. The default reward, which is always zero unless the agent reaches the target, is used to evaluate the performance of learned policies. Due to partial observability and the implicit hierarchical nature of the task, these environments are considered challenging for RL and IL, and have been extensively used for benchmarking curriculum RL and exploration-driven RL.

**Baselines:** We compare our approach with two standard baselines: GAIL Ho and Ermon (2016) and VAIL Peng et al. (2019). GAIL has been introduced in Section 3. VAIL is based on GAIL but additionally optimizes a variational discriminator bottleneck (VDB) objective. Our approach uses the IRL techniques behind those two baseline algorithms, resulting in two versions of Algorithm 1, denoted as PAGAR-GAIL and PAGAR-VAIL, respectively. More specifically, if the baseline optimizes a \(J_{IRL}\) objective, we use the same \(J_{IRL}\) objective in Algorithm 1. Also, we extract the reward function \(r\) from the discriminator \(D\) as mentioned in Section 3. More details are in Appendix C.1. PPO Schulman et al. (2017) is used for policy training in GAIL, VAIL, and ours with a replay buffer of size \(2048\). Additionally, we compare our algorithm with a state-of-the-art (SOTA) IL algorithm, IQ-Learn Garg et al. (2021), which, however, is not compatible with our algorithm because it does not explicitly optimize a reward function. The policy and the reward functions are all approximated using convolutional networks.

**IL with Limited Demonstrations.** By learning from \(10\) expert-demonstrated trajectories with high returns, PAGAR-based IL produces high-performance policies with high sample efficiencies as shown in Figure 2(a) and (c). Furthermore, we compare PAGAR-VAIL with VAIL by reducing the number of demonstrations from \(10\) to \(1\). As shown in Figure 2(b) and (d), PAGAR-VAIL produces high-performance policies with significantly higher sample efficiencies.

**IL under Dynamics Mismatch.** We demonstrate that PAGAR enables the agent to infer and accomplish the objective of a task even in environments that are substantially different from the one observed during expert demonstrations. As shown in Figure 2(e), we collect \(10\) expert demonstrations from the _SimpleCrossingS9N1-v0_ environment. Then we apply Algorithm 1 and the baselines, GAIL, VAIL, and IQ-learn to learn policies in _SimpleCrossingS9N2-v0_, _SimpleCrossingS9N3-v0_ and _FourRooms-v0_. The results in Figure 2(f)-(g) show that PAGAR-based IL outperforms the baselines in these challenging zero-shot settings.

**IL with Different Reward Hypothesis Sets.** The foundational theories of GAIL and AIRL indicate that different reward function hypothesis sets can affect the equilibrium of their GAN frameworks. We study whether choosing different reward hypothesis sets can influence the performance of Algorithm 1. We compare using a \(Sigmoid\) function with a Categorical distribution in the output layer of the discriminator networks in GAIL and PAGAR-GAIL. When using the \(Sigmoid\) function, the outputs of \(D\) are not normalized, i.e., \(_{a}D(s,a) 1\). When using a Categorical distribution, \(_{a}D(s,a)=1\). We test GAIL and PAGAR-GAIL in _DoorKey-6x6-v0_ environment. As shown in Figure 3, PAGAR-GAIL outperforms GAIL in both cases by using fewer samples.

Figure 3: PAGAR-GAIL in different reward spaces

Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each ‘PAGAR-’ indicates which IRL technique is used in Algorithm 1. The \(y\) axis indicates the average return per episode. The \(x\) axis indicates the number of time steps.

### Continuous Control Tasks

We evaluate PAGAR-based IL on continuous control tasks in both online and offline RL settings, demonstrating its ability to improve IRL-based IL performance across different types of tasks.

**Benchmarks:** We use four continuous control environments from Mujoco. In the online RL setting, both protagonist and antagonist policies are permitted to explore the environment. In the offline RL setting, exploration by these policies is restricted. Especially, for offline RL we use the D4RL's 'expert' datasets as the expert demonstrations and the 'random' datasets as the offline suboptimal dataset. Policy performance is evaluated online in both settings by using the default reward function of the environment.

**Baselines:** We compare PAGAR-based IL against f-IRL Ni et al. (2021) in the online RL setting and compare with RECOIL Sikichi et al. (2024) in the offline RL setting. When comparing with f-IRL, we use f-IRL as the IRL algorithm in Algorithm 1. When comparing with RECOIL, as RECOIL does not directly learn the reward function but learns the Q and V functions, we develop another algorithm for the offline RL setting to combine PAGAR with RECOIL by explicitly learning a reward function and using the reward function and V function to represent the Q function. The details can be found in Appendix B.4.

**Results:** As shown in Figure 4, PAGAR-based IL achieves equivalent performance to the baselines with fewer iterations. Furthermore, on the _Ant_ and _Walker2d_ tasks, Algorithm 1 matches the performance level of f-IRL using significantly less iterations. Additional results of PAGAR with GAIL and VAIL across other continuous control benchmarks are provided in Appendix C.3. Table 1 further shows that when combined with RECOIL, PAGAR-based IL achieves higher performance in most of the tasks than the baseline. These results demonstrate the broader applicability of PAGAR-based IL in both online and offline settings and its effectiveness across different types of environments, further reinforcing the robustness of our approach.

## 8 Conclusion

In this paper, we propose to prioritize task alignment over conventional data alignment in IRL-based IL by treating expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. Our PAGAR-based IL adopts an adversarial mechanism to train a policy with this set of reward functions. Experimental results demonstrate that our algorithm can mitigate reward misalignment in challenging environments. Our future work will focus on employing the PAGAR paradigm to other task alignment problems.

    & RECOIL & PAGAR-RECOIL \\  hopper-random & \(106.87 2.69\) & \(\) \\  halfcheetah-random & \(80.84 17.62\) & \(\) \\  walker2d-random & \(108.40 0.04\) & \(108.40 0.12\) \\  ant-random & \(113.34 2.78\) & \(\) \\   

Table 1: Offline RL results obtained by combining PAGAR with RECOIL averaged over \(4\) seeds.

Figure 4: Comparing Algorithm 1 with f-IRL in continuous control tasks. ‘PAGAR-fIRL’ indicates f-IRL is used as the inverse RL algorithm in Algorithm 1. The \(y\) axis indicates the average return per episode. The \(x\) axis indicates the number of time steps in the environment.