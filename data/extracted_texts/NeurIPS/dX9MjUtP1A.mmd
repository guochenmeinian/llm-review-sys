# Self-Correcting Bayesian Optimization through Bayesian Active Learning

Carl Hvarfner

carl.hvarfner@cs.lth.se

Lund University

&Erik Orm Hellsten

erik.hellsten@cs.lth.se

Lund University

&Frank Hutter

fh@cs.uni-freiburg.de

University of Freiburg&Luigi Nardi

luigi.nardi@cs.lth.se

Lund University

Stanford University

DBtune

###### Abstract

Gaussian processes are the model of choice in Bayesian optimization and active learning. Yet, they are highly dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding good hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize hyperparameter learning. Statistical distance-based Active Learning (SAL) considers the average disagreement between samples from the posterior, as measured by a statistical distance. SAL outperforms the state-of-the-art in Bayesian active learning on several test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization methods on traditional benchmarks. Moreover, we demonstrate the importance of self-correction on atypical Bayesian optimization tasks.

## 1 Introduction

Bayesian Optimization (BO) is a powerful paradigm for black-box optimization problems, i.e., problems that can only be accessed by pointwise queries. Such problems arise in many applications, ranging from including drug discovery  to configuration of combinatorial problem solvers [27; 28], hardware design [14; 43], hyperparameter tuning [11; 30; 33; 52], and robotics [4; 9; 40; 41].

Gaussian processes (GPs) are a popular choice as surrogate models in BO applications. Given the data, the model hyperparameters are typically estimated using either Maximum Likelihood or Maximum a Posteriori estimation (MAP) . Alternatively, a fully Bayesian treatment of the hyperparameters [46; 55] removes the need to choose any single set through Monte Carlo integration. This procedure effectively considers all possible hyperparameter values under the current posterior, thereby accounting for hyperparameter uncertainty. However, the relationship between accurate GP hyperparameter estimation and BO performance has received little attention [3; 7; 58; 69; 71], and active reduction of hyperparameter uncertainty is not an integral part of any prevalent BO acquisition function. In contrast, the field of Bayesian Active Learning (BAL) contains multiple acquisition functions based solely on reducing hyperparameter-induced measures of uncertainty [26; 34; 50], and the broader field of Bayesian Experimental Design [1; 10; 48] revolves around acquisition of data to best learns the model parameters.

The importance of the GP hyperparameters in BO is illustrated in Fig. 1, which shows average simple regret over 20 optimization runs of 8-dimensional functions drawn from a Gaussian process prior. The curves correspond to the performance of Expected Improvement with noisy experiments (NEI)  acquisition function under a fully Bayesian hyperparameter treatment using NUTS . Two prevalent hyperparameter priors, described in detail in App. B.1, as well as the true model hyperparameters, are used. Clearly, good model hyperparameters have substantial impact on BO performance, and BO methods could greatly benefit from estimating the model hyperparameters as accurately as possible. Furthermore, the hyperparameter estimation task can become daunting under complex problem setups, such as non-stationary objectives (spatially varying lengthscales, heteroskedasticity) , high-dimensional search spaces , and additively decomposable objectives . The complexity of such problems warrants the use of more complex, task-specific surrogate models. In such settings, the success of the optimization may increasingly hinge on the presumed accuracy of the task-specific surrogate.

We proceed in two steps. We first introduce _Statistical distance-based Active Learning_ (SAL), which improves Bayesian active learning by generalizing previous work  and introduces a holistic measure of disagreement between the marginal posterior predictive distribution and each conditional posterior predictive. We consider the hyperparameter-induced disagreement between models in the acquisition function, thereby accelerating the learning of model hyperparameters. We then propose _Self-Correcting Bayesian Optimization_ (SCoreBO), which builds upon SAL by explicitly learning the location of the optimizer in conjunction with model hyperparameters. This achieves accelerated hyperparameter learning and yields improved optimization performance on both conventional and exotic BO tasks. Formally, we make the following contributions:

1. We introduce SAL, a novel and efficient acquisition function for hyperparameter-oriented Bayesian active learning based on statistical distances (Sec. 3.1),
2. We introduce SCoreBO, the first acquisition function for joint BO and hyperparameter learning (Sec. 3.2),
3. We display highly competitive performance on an array of conventional AL (Sec. 4.1) and BO tasks (Sec. 4.2), and demonstrate SCoreBOs, ability to enhance atypical models such as SAASBO  and HEBO , and identify decompositions in AddGPs (Sec. 4.3).

## 2 Background

### Gaussian processes

Gaussian processes (GPs) have become the model class of choice in most BO and active learning applications. They provide a distribution over functions \(f(m(),k(,))\) fully defined by the mean function \(m()\) and the covariance function \(k(,)\). Under this distribution, the value of the function \(f()\), at a given point \(\), is normally distributed with a closed-form solution for the mean and variance. We assume that observations are perturbed by Gaussian noise, such that \(y_{}=f()+,\  N(0,_{}^{2})\). We also assume the mean function to be constant, such that the dynamics are fully determined by the covariance function \(k(,)\).

To account for differences in variable importance, each dimension is individually scaled using length-scale hyperparameters \(_{i}\). For \(D\)-dimensional inputs \(\) and \(^{}\), the distance \(r(,^{})\) is subsequently computed as \(r^{2}=_{i=1}^{D}(x_{i}-x_{i}^{})^{2}/_{i}^{2}\). Along with the outputscale \(_{f}\), the set \(=\{,_{},_{f}\}\) comprises the set of hyperparameters that are conventionally learned. The likelihood surface for the GP hyperparameters is typically highly multi-modal , where different modes represent different bias-variance trade-offs . To avoid having to choose a single mode, one can define a prior \(p()\) and marginalize with respect to the hyperparameters when performing predictions . We outline fully Bayesian hyperparameter treatment in GPs App. G.1.

Figure 1: Simple regret of using true hyperparameters, BoTorch (v.0.8.4 default) and lognormal hyperparameter priors with fully Bayesian hyperparameter treatment. The prior substantially impacts final performance, and correct hyperparameters yield vastly better results.

### Bayesian Optimization

Bayesian Optimization (BO) seeks to maximize to a black-box function \(f\) over a compact domain \(\),

\[^{*}*{arg\,max}_{}f(),\] (1)

such that \(f\) can only be sampled point-wise through expensive, noisy evaluations \(y_{}=f()+\), where \((0,_{}^{2})\). New configurations are chosen by optimizing an _acquisition function_, which uses the surrogate model to quantify the utility of evaluating new points in the search space. Examples of such heuristics are Expected Improvement (NEI) [8; 31] and Upper Confidence Bound (UCB) [3; 57; 60]. More sophisticated look-ahead approaches include Knowledge Gradient (KG) [17; 68] as well as a class of particular importance for our approach - the information-theoretic acquisition function class. These acquisition functions consider a mutual information objective to select the next query,

\[_{}()=I(y_{};_{n}),\] (2)

where \(\) can entail either the optimum \(^{*}\) as in (Predictive) Entropy Search (ES/PES) [23; 24], the optimal value \(f^{*}\) as in Max-value Entropy Search (MES) [42; 59; 65] or the tuple \((^{*},f^{*})\), used in Joint Entropy Search (JES) [29; 61]. FITBO  shares similarities with our work, in that the optimal value is governed by a hyperparameter, in their case of a transformed GP.

Within BO, the fully Bayesian hyperparameter treatment is conventionally extended from the predictive posterior to the acquisition function such that for \(M\) models with hyperparameters \(_{m},m\{1,,M\}\) sampled from the posterior over hyperparameters \(p(|)\), the acquisition function \(\) is computed as an expectation over the hyperparameters [46; 55]

\[(|)=_{}[(|,)]_{m=1}^{M}(|_{m}, )\ \ _{m} p(|).\] (3)

This is also the definition of fully Bayesian treatment considered in this work.

### Bayesian Active Learning

In contrast to BO, which aims to find a maximizer to an unknown function, Active Learning (AL)  seeks to accurately learn the black-box function globally. Thus, the objective is to minimize the expected prediction loss. AL acquisition functions are classified as either _decision-theoretic_, which minimize the prediction loss over a validation set, or _information-theoretic_, which minimize the space of plausible models given the observed data [26; 37].

In the information-theoretic category, _Active Learning McKay_ (ALM)  selects the point with the highest Shannon Entropy, which for GPs amounts to selecting the point with the highest variance. Under fully Bayesian hyperparameter treatment, it is referred to as Bayesian ALM (BALM). _Bayesian Active Learning by Disagreement_ (BALD)  was among the first Bayesian active learning approaches to explicitly focus on learning the model hyperparameters. It approximates the reduction in entropy over the GP hyperparameters from observing a new data point

\[_{}()=I(y_{};|)= (p(y_{}|))-_{}[(p(y_{}| ,))]\] (4)

and was later extended to deep Bayesian active learning  and active model (kernel) selection . Lastly, Riis et al.  propose a _Bayesian Query-by-Committee_ (BQBC) strategy. BQBC queries where the variance \(V\) of the GP mean is the largest, with respect to changing model hyperparameters:

\[_{BQBC}()=V_{}[_{}(|)]= _{}[(_{}(|)-(| ))^{2}],\] (5)

where \(()\) is the marginal posterior mean at \(\), and \(_{}()\) is the posterior mean conditioned on \(\). As such, BQBC queries the location which maximizes the average distance between the marginal posterior and the conditionals according to some distance metric (here, the posterior mean), henceforth referred to as hyperparameter-induced _posterior disagreement_. However, disagreement in mean alone does not fully capture hyperparameter-induced disagreement. Thus,  also presents _Query-by-Mixture of Gaussian Processes_ (QBMGP), that adds the BALM criterion to the BQBC acquisition function.

### Statistical Distances

A statistical distance quantifies the distance between two statistical objects. We focus on three (semi-)metrics, which have closed forms for Gaussian random variables. The closed forms expressions, as well as additional intuition on their interaction with Gaussian random variables, can be found in App. G.2.

The Hellinger distanceis a dissimilarity measure between two probability distributions which has previously been employed in the context of BO-driven automated model selection by Malkomes et al. . For two probability distributions \(p\) and \(q\), it is defined as

\[H^{2}(p,q)=_{}(-)^{ 2} dx,\] (6)

for some auxiliary measure \(\) under which both \(p\) and \(q\) are absolutely continuous.

The Wasserstein distanceis dissimilarity metric between two distributions describing the average distance one distribution has to be moved to morph into another. The Wasserstein-\(k\) distance is defined as

\[W_{k}(p,q)=(_{0}^{1}|F_{q}(x)-F_{p}(x)|^{k}dx)^{1/k}\] (7)

where, in this work, we focus on the case where \(k=2\).

The KL divergenceThe KL divergence is a standard asymmetrical measure for dissimilarity between probability distributions. For two probability distributions \(P\) and \(Q\), it is given by \(_{KL}(P Q)=_{}P(x)(P(x)/Q(x))dx\). The distances in Eq. (6), Eq. (16) and the KL divergence are used for the acquisition functions presented in Sec. 3.

## 3 Methodology

In Sec. 3.1, we introduce SAL, a novel family of metrics for BAL. In Sec. 3.2, we extend this to SCoreBO, the first acquisition function for joint BO and hyperparameter-oriented active learning, inspired by information-theoretic BO acquisition functions. In Sec. 3.3, we demonstrate how to efficiently approximate different types of statistical distances within the SAL context.

### Statistical distance-based Active Learning

In active learning for GPs, it is important to efficiently learn the correct model hyperparameters. By measuring where the posterior hyperparameter uncertainty causes high disagreement in model output, the search can be focused on where this uncertainty has a high impact. However, considering only the posterior disagreement in mean, as in BQBC, is overly restrictive as it does not fully utilize the available distributions for the hyperparameters. For example, it ignores uncertainty in the outputscale hyperparameter of the Gaussian process, which disincentives exploration. As such, we propose to generalize the acquisition function in Eq. (5) to instead consider the posterior disagreement as measured by any statistical distance. Locations where the posterior distribution changes significantly as a result of model uncertainty are good points to query, in order to quickly learn the model hyperparameters. When an observation at such a location is obtained, hyperparameters which predicted that observation poorly will have a substantially smaller likelihood, which in turn aids hyperparameter convergence. The resulting SAL acquisition function is as follows:

\[_{SAL}()=_{}[d(p(y_{}|, ),p(y_{}|))]_{m=1}^{M}d(p(y _{}|_{m},),p(y_{}|)),\] (8)

where \(M\) is the number of hyperparameter samples drawn from its associated posterior, \(_{m} p(|)\), \(=\{,_{f},_{}\}\), and \(d\) is a statistical distance. Notably, SAL generalizes both BQBC and BALD, which are exactly recovered by choosing the semimetric to the difference in mean or the forward KL divergence, with a short proof for the latter in App. F:

**Proposition 1**.: _SAL equipped with the KL-divergence is equivalent to BALD._Fig. 2 visualizes the SAL acquisition function. The marginal posterior (left) is made up of three vastly different conditional posteriors with hyperparameters sampled from \(p(|)\) - one with high outputscale (blue), one with very high noise (orange), and one with short lengthscale (green). For each of the blue, orange and green conditionals, the distance to the marginal posterior is computed. Intuitively, disagreement in noise level \(_{}\) can cause large posterior disagreement at already queried locations. Similarly, uncertainty in outputscale \(_{f}\) between posteriors will yield disagreement in large-variance regions, which will result in global variance reduction. Compared to other active learning acquisition functions, SAL carries distinct advantages: it has incentive to query the same location multiple times to estimate noise levels, and accomplishes the typical active learning objectives of predictive accuracy and global exploration by alleviating uncertainty over the lengthscales and outputscale of the GP. As we show in our experiments (Sec. 4.1, App. D), SAL yields superior predictions and reduces hyperparameter uncertainty at drastically improved rates.

### Self-Correcting Bayesian Optimization

Equipped with the SAL objective from Eq. (8), we have an intuitive measure for the hyperparameter-induced posterior disagreement, which incentivizes hyperparameter learning by querying locations where disagreement is the largest. However, it does not inherently carry an incentive to _optimize_ the function. To inject an optimization objective into Eq. (8), we draw inspiration from information-theoretic BO and further condition on samples of the optimum. Conditioning on potential optima yields an additional source of disagreement reserved for promising regions of the search space.

We consider \((^{*},f^{*})\), representing the global optimum and optimal value considered in JES [29; 61], as hyperparameters. When conditioning on \((^{*},f^{*})\), we condition on an additional observation, which displaces the mean and reduces the variance at \(^{*}\). Moreover, the posterior over \(f\) becomes an upper truncated Gaussian, reducing the variance and pushing the mean marginally downwards in uncertain regions far away from the optimum as visualized in Fig. 3. Consequently, sampling and conditioning on \((^{*},f^{*})\) introduces an additional source of disagreement between the marginal posterior and the conditionals _globally_. The optimizer \((^{*},f^{*})\) is obtained through posterior sampling . For brevity, we hereafter denote \((^{*},f^{*})\) by \(\). The resulting SCoreBO acquisition function is

\[_{SC}()=_{,}[d(p(y_{}|),p(y_{}|,,))].\] (9)

The joint posterior \(p(,|)=p(|,)p( |)\) used for the expectation in Eq. (9) can be approximated by hierarchical sampling. We first draw \(M\) hyperparameters \(\) and thereafter \(N\) optimizers \(|\). As such, the expression for the SCoreBO acquisition function is:

\[()_{m=1}^{M}_{n=1}^{N}d(p(y_{}|),p(y_{}|_{m},_{_{m,n}}, )),\] (10)

where \(N\) is the number of optimizers sampled per hyperparameter set. Notably, while the acquisition function in (9) considers the optimizer \((^{*},f^{*})\), SCoreBO is not restricted to employing that quantity alone. Drawing parallels to PES and MES, we can also choose to condition on either \(^{*}\) or \(f^{*}\) alone in place of \((^{*},f^{*})\). Doing so introduces a smaller disagreement in the posterior at the conditioned

Figure 2: Marginal posterior (top left, grey in other plots in top row), \(_{SAL}\) using the Hellinger distance (bottom left, black), and the three conditional GPs (blue, orange, green) and their marginal contribution to the total acquisition function (bottom row). The large disagreement in noise level and lengthscale, primarily caused by the orange GP (large noise, long lengthscale), makes \(_{SAL}\) query the lowest-valued point for a second time (selected location as vertical dashed line in the leftmost plot) to determine the mean and variance at that location.

location \(^{*}\), thus decreasing the acquisition value there. This will in turn decrease the emphasis that SCoreBO puts on optimization, relative to hyperparameter learning. In Fig. 3, the SCoreBO acquisition function is displayed for the same scenario as in Fig. 2. By conditioning on \(N=2\) optimizers per GP, we obtain \(N M\) posteriors (displaying the posterior for one out of two optimizers, i.e. the left star in (blue), in Fig. 3). The mean is pushed upwards around the extra observation and the posterior predictive distribution over \(f\) is truncated as it is now upper bounded by \(f^{*}\). While the preferred location under SAL is still attractive, the best location to query is now one that is more likely to be optimal, but still good under SAL.

Algorithm 1 displays how the involved densities are formed for one iteration of SCoreBO. For each hyperparameter set, a number of optima are sampled and individually conditioned on (CondGP) given the current data and hyperparameter set. After this procedure is completed for all hyperparameter sets, the statistical distance between each conditional posterior and the marginal is computed. The conditioning on the fantasized data point involves a rank-1 update of \((n^{2})\) of the GP for each draw. As such, the complexity of constructing the acquisition functions is \((MNn^{2})\) for \(M\) models, \(N\) optima per model and \(n\) data points. We utilize NUTS  for the MCMC involved with the fully Bayesian treatment, at a cost of \((Dn^{3})\) per sample.

```
1:Input: Number of hyperparameter sets \(M\), number of sampled optima \(N\), current data \(\)
2:Output: Next query location \(^{}\).
3:for\(m\{1,,M\}\)do
4:\(_{m} p(|)\)
5:for\(n\{1,,N\}\)do
6:\(_{_{m},n}_{f_{m},n},f_{_{m},n} p(f|_{m},)\)\(\{\)Draw \(n\) optima for each \(_{m}\)\(\}\)
7:\(p(y_{_{m}},_{_{m},n},)(_{_{m},n},_{m},)\)\(\{\)Condition GPs on each optimum\(\}\)
8:endfor
9:endfor
10:\(^{}=()\)\(\{\)Defined in Eq. (10)\(\}\) ```

**Algorithm 1**SCoreBO iteration

### Approximation of Statistical Distances

We consider two proper statistical distances, Wasserstein distance and Hellinger distance. In contrast to BQBC, the statistical distance between the normally distributed conditionals and the marginal posterior predictive distribution (which is a Gaussian mixture), is not available in closed-form. We propose two approaches: estimating the distances using MC, which we outline for both distances in App E.1, and estimation using moment matching (MM), which we outline below.

Figure 3: Approximate marginal posterior after having conditioned on \((^{*},f^{*})\) (top left), \(_{SC}\) using the Hellinger distance (bottom left), the three conditional truncated posteriors and their marginal contribution to the total acquisition function for the same iteration as Fig. 2. Conditioning on \((^{*},f^{*})\) (marked as \(\), drawn from function samples in dashed) introduces additional disagreement between the marginal posterior and the sampled GPs in promising regions as a result of conditioning. In the figure, we marginalize over \(M=3\) sets of hyperparameters and \(N=2\) optimizers per GP, where each optimizerâ€™s contribution to the acquisition function is visible under its corresponding GP. Note that, since function draws are _noiseless_, the conditioned optimum does not need to surpass the best _noisy_ observation in value. This phenomenon is most notable in (orange).

Approximation through Moment MatchingWe propose to fully utilize the closed-form expressions of the involved distances for Gaussians, and approximate the full posterior mixture \(p(y_{}|)\) with a Gaussian distribution using moment matching (MM) for the first and second moment. While a Gaussian mixture is not generally well approximated by a Normal distribution, we show empirically in App. E that the distance between the conditionals and the approximate posterior is small. In the moment matching approach, the conditional posterior \(p(y_{}|,,)\) utilizes a lower bound on the change in the posterior induced by conditioning on \(\), as derived in GIBBON , which conveniently involves a second moment matching step of the extended skew Gaussian \(p(y_{}|,,)\). This naive approach circumvents a quadratic cost \((N^{2}M^{2})\) in the number of samples of each pass through the acquisition function, and yields comparable performance to the MC estimation procedures proposed in App. E.1. In App. E, we qualitatively assess the accuracy of the MM approach for both distances, and display its ability to preserve the shape of the acquisition function.

## 4 Experiments

In this section we showcase the performance of the SAL and SCoreBO acquisition functions on a variety of tasks. For active learning, SAL shows state-of-the-art performance on a majority of benchmarks, and is more robust than the baselines. For the optimization tasks, SCoreBO more efficiently learns the model hyperparameters, and outperforms prominent Bayesian optimization acquisition functions on a variety of tasks. All experiments are implemented in BoTorch 1. We use the same \((0,3)^{2}\) hyperparameter priors as Riis et al.  unless specified otherwise. SCoreBO _and all baselines_ utilize fully Bayesian treatment of the hyperparameters. The complete experimental setup is presented in detail in Appendix B, and our code is publicly available at https://github.com/hvarner/scorebo.git. We utilize the moment matching approximation of the statistical distance. Experiments for the MC variant of SCoreBO are found in App. E.2.

### Active Learning Tasks

To evaluate the performance of SAL, we compare it with BALD, BQBC and QBMGP on the same six functions used by Riis et al. : Gramacy (1D) has a periodicity that is hard to distinguish from noise, Higdon and Gramacy (2D) varies in characteristics in different regions, whereas Branin, Hartmann-6 and Ishigami have a generally nonlinear structure. We display both the Wasserstein and Hellinger distance versions of SAL, denoted as SAL-WS and SAL-HR, respectively. We evaluate each method on their predictive power, measured by the negative Marginal Log Likelihood (MLL) of the model predictions over a large set of validation points. MLL emphasizes calibration (accurate uncertainty estimates) in prediction over an accurate predictive mean. In Fig. 11, we show how the average validation set MLL changes with increasing training data. SAL-HR is the top-performing acquisition function on three out of six tasks, and rivals BALD for stability in predictive performance. This is particularly evident on the Ishigami function, where most methods fluctuate in the quality of their predictions. This can be attributed to emphasis on rapid hyperparameter learning, which is visualized in detail in App. D, Fig. 15. In the rightmost plot, the real-time average per-seed ranking of acquisition function performance is displayed as a function of the fraction of budget expended. SAL-HR performs best, followed by BQBC andBALD. SAL-WS, however, does not display similarly consistent predictive quality as SAL-HR. The ability of SAL-HR to correctly estimate hyperparameters ensures calibrated uncertainty estimates, which makes it the better candidate for BO. In App. C.1, Fig. 11, we show the evolution of the average Root Mean Squared Error (RMSE) of the same tasks, where SAL-WS performs best and SAL-HR lags behind, which demonstrates the viability of various distance metrics on different tasks.

### Bayesian Optimization Tasks

For the BO tasks, we use the Hellinger distance for its proficiency in prediction calibration and hyperparameter learning. We compare against several state-of-the-art baselines from the BO literature: NEI for noisy experiments , as well as JES, the MES approach GIBBON  and PES . As an additional reference, we include NEI for noisy experiments  using MAP estimation.

Efficiently learning the hyperparametersTo showcase SCoreBO's ability to find the correct model hyperparameters, we run all relevant acquisition functions on samples from the 8-dimensional GP in Fig. 1. We exploit that for GP samples, the objectively true hyperparameters are known (in contrast to typical synthetic test functions). We utilize the same priors as in Fig. 1 on all the hyperparameters and compare SCoreBO to NEI to assess the ability of each acquisition function to work independently of the choice of prior. In Fig. 5, for each acquisition function, we plot the average log regret over 20 different 8-dimensional instances of this task. The tasks at hand have lengthscales that vary substantially between dimensions, as detailed in App B. The explanation for the good performance of SCoreBO can be see in Fig. 17 in App. D, where SCoreBO converges substantially faster towards the correct hyperparameter values than NEI for both types of priors.

Synthetic test functionsWe run SCoreBO on a number of commonly used synthetic test functions for \(25||\) iterations, and present how the log inference regret evolves over the iterations in Fig. 6. All benchmarks are perturbed by Gaussian noise. We evaluate inference regret, i.e., the current best guess of the optimal location \(*{arg\,max}_{}()\), which is conventional for non-myopic acquisition functions . SCoreBO yields the the best final regret on four of the six tasks. In the relative rankings (rightmost plot), SCoreBO ranks poorly initially, but once hyperparameters are learned approximately halfway through the run, it substantially outperforms the competition. On Rosenbrock (4D), the relatively poor performance can explained by the apparent non-stationarity of the task, detailed in Fig. D.3, which makes hyperparameters diverge over time. This exposes a weakness of SCoreBO: When the modeling assumptions (such as stationarity) do not align with the task, optimization performance may suffer due to perpetual disagreement in the posterior. In App. C.2, we display the performance of SCoreBO-KL and SCoreBO-WS on the same set of benchmarks, where both display highly competitive performance.

Figure 4: Negative Marginal Log Likelihood (MLL) on six active learning functions and the (smoothed) relative rankings throughout each run for QBMGP, BQBC, BALD and SAL using Wasserstein and Hellinger distance. We plot mean and one standard error for 25 repetitions.. SAL-HR is the top performing method, placing first in relative rankings. On Ishigami, only SAL-HR and BALD produces stable results.

Figure 5: Regret for NEI and SCoreBO on the 8-dimensional GP sample for two different types of hyperparameter priors. Mean and standard deviation are plotted for all hyperparameter samples across 20 repetitions.

Figure 6: Average log inference regret and (smoothed) relative ranking across 50 repetitions between the acquisition functions for SCoreBO, JES, MES and NEI on six synthetic test functions. SCoreBO produces the best final regret on 4 out of 6 tasks, and has a substantially lower average ranking by the end of each run.

### A Practical Need for Self-correction

Lastly, we evaluate the performance of SCoreBO on three atypical tasks with increased emphasis on the surrogate model: (1) high-dimensional BO through sparse adaptive axis-aligned priors (SAASBO) , (2) BO with additively decomposable structure (AddGPs) [19; 32] and (3) non-stationary, heteroskedastic modelling with HEBO . Eriksson & Jankowiak  consider their proposed method for noiseless tasks, where active variables easily distinguish from their non-active counterparts. However, SAASBO is not restricted to noiseless tasks. For AddGPs, data cross-covariance, and lack thereof, is similarly difficult to infer in the presence of noise.

In Fig. 7, we visualize the performance of SCoreBO and competing acquisition functions _with SAASBO priors_ on two noisy benchmarks, Ackley-4 and Hartmann-6, with dummy dimensions added, as well as two real-world benchmarks: fitting a weighted Lasso model in 180 dimensions , and the tuning of all 385 lengthscales and three regularization parameters of an SVM , a task also considered by Eriksson & Jankowiak . On these benchmarks, where finding the correct hyperparameters is crucial for performance, SCoreBO clearly outperforms traditional methods. To further exemplify how SCoreBO identifies the relevant dimensions, in Fig. 8, we show how the hyperparameters evolve on the 25D-embedded Ackley (4D) task. SCoreBO quickly finds the correct lengthscales and outputscale with high certainty, whereas NEI remains uncertain of which dimensions are active throughout the optimization procedure. Impressively, SCoreBO finds accurate hyperparameters even faster than BALD, despite the latter being a pure active learning approach.

Secondly, we demonstrate the ability of SCoreBO to self-correct on _uncertainty in kernel design_, by considering AddGP tasks. We utilize the approach of Gardner et al. , where additive decompositions are marginalized over. Ideally, a sufficiently accurate decomposition is found quickly, which rapidly speeds up optimization through accurate cross-correlation of data. Fig. 9 demonstrates SCoreBO's performance on two GP sample tasks and a real-world task estimating cosmological constants (leftmost 3 plots) and its ability to find the correct additive decompositions (right). We observe that SCoreBO identifies correct decompositions substantially better than NEI. Final performance, however, is only marginally better, as substantial resources are expended finding the right decompositions. Notably, the Cosmological Constants task does not display additive decomposability. As such, SCoreBO unsuccessfully expends resources attempting to reduce disagreement over additive structures, which hampers performance. This demonstrates that while SCoreBO learns the problem structure at increased rates, improved BO performance does not automatically follow.

Lastly, we apply SCoreBO to the HEBO  GP model, the winner of the NeurIPS 2020 Black-box optimization challenge . The model employs input  and output warpings, the former of

Figure 8: Hyperparameter convergence on the \(25D\)-embedded \(4D\) Ackley function with a SAASBO HP prior for SCoreBO, NEI and BALD. Log HP mean and 1 standard deviation is plotted per iteration. SCoreBO identifies \(_{1},,_{4}\) as important (short lengthscales, \(_{i} 10^{-1}\)) with low uncertainty and \(_{5},,_{25}\) as dummy dimensions (\(_{i} 10^{1}\)). NEI fails to identify any important lengthscales, whereas SCoreBO correctly identifies active dimensions with high certainty.. Notably, SCoreBO finds accurate hyperparameters even faster than BALD, a pure active learning approach. Reference HP values (where available) are marked with a dashed line.

Figure 7: Final loss using SAASBO priors on the noisy embedded Ackley-4, embedded Hartmann-6, the DNA classification and the SVM HPO task, mean and one standard error. SCoreBO identifies the important dimensions rapidly, and successfully optimizes the tasks. The optimal value is marked with a dashed line.

which are learnable to account for the heteroskedasticity that is prevalent in real-world optimization, and particularly HPO [13; 56], tasks. The complex model provides additional degrees of freedom in learning the objective. We evaluate SCoreBO and all baselines on three 4D deep learning HPO tasks: two involving large language models, and one from computer vision, from the PD1  benchmarking suite. Fig. 10 displays that SCoreBO obtains the best final accuracy on 2 out of 3 tasks, suggesting that self-correction is warranted for optimization of deep learning pipelines.

## 5 Conclusion and Future Work

The hyperparameters of Gaussian processes play an integral role in the efficiency of both Bayesian optimization and active learning applications. In this paper, we propose Statistical distance-based Active Learning (SAL) and Self-Correcting Bayesian Optimization (SCoreBO), two acquisition functions that explicitly consider hyperparameter-induced disagreement in the posterior distribution when selecting which points to query. We achieve high-end performance on both active learning and Bayesian optimization tasks, and successfully learn hyperparameters and kernel designs at improved rates compared to conventional methods. SCoreBO breaks ground for new methods in the space of joint active learning and optimization of black-box functions, which allows it to excel in high-dimensional BO, where learning important dimensions are vital. Moreover, the potential downside of self-correction is displayed when the model structure does not support the task at hand, or when self-correction is not required to solve the task. For future work, we will explore additional domains in which SAL and SCoreBO can allow for increased model complexity in BO applications.

## 6 Limitations

SCoreBO displays the ability to increase optimization efficiency on complex tasks that necessitate accurate modeling. However, SCoreBO's efficiency is ultimately contingent on the intrinsic ability of the GP to model the task at hand. Appendix 19 demonstrates this issue for the Rosenbrock (4D) function, where SCoreBO performs worse relative to other acquisition functions. There, the hyperparameter values increase over time instead of converge, which suggests that the objective is not part of the class of functions defined by the kernel. Thus, the self-correction effort is less helpful towards optimization. Moreover, increasing the model capacity, such as in Sec. 4.3, comes with increasing resources allocated towards self-correction. In highly constrained-budget applications, such resource allocation may not yield the best result, especially if increased model complexity is unwarranted. This is evident from the synthetic AddGP tasks, where despite accurately identifying the additive components, SCoreBO does not provide substantial performance gains over NEI. Lastly, SCoreBO's reliance on fully Bayesian hyperparameter treatment makes it more computationally demanding than MAP-based alternatives, limiting its use in high-throughput applications.

Figure 10: Performance on the PD1 deep learning tasks over 20 repetitions using the warpings from HEBO . SCoreBO obtains the best final accuracy on 2 out of 3 tasks, placing second on the third.

Figure 9: Final value of using AddGPs on 6D and 10D GP sample functions, fully decomposable in groups of two, and the Cosmological Constants tasks. SCoreBO achieves better final performance (left, middle) with low uncertainty, and successfully finds the additive components of the 6D task (right).