# MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders

Xueying Jiang\({}^{1}\), Sheng Jin\({}^{1}\), Xiaoqin Zhang\({}^{2}\), Ling Shao\({}^{3}\), Shijian Lu\({}^{1}\)

\({}^{1}\)S-Lab, Nanyang Technological University, Singapore

\({}^{2}\)College of Computer Science and Technology, Zhejiang University of Technology, China

\({}^{3}\)UCAS-Terminus AI Lab, University of Chinese Academy of Sciences, China

Corresponding author.

###### Abstract

Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed feature-space occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.

## 1 Introduction

3D object detection has emerged as one key component in various navigation tasks such as autonomous driving, robot patrolling, etc. Compared with prior studies relying on LiDAR  or multi-view images , monocular 3D object detection offers a more cost-effective and accessible alternative which identifies objects and predicts their 3D locations from single-view images. On the other hand, monocular 3D object detection is much more challenging due to the lack of 3D information from multi-view images or LiDAR data.

Among various new challenges in monocular 3D detection, object occlusion, which exists widely in natural images as illustrated in Figure 1 (a), becomes a critical issue while predicting 3D locations in terms of object depths, object dimensions, and object orientations. Most existing monocular 3D detectors such as MonoDETR and GUPNet  neglect the object occlusion issue which demonstrates clear performance degradation as illustrated in Figure 1 (b). A simple idea is to learn to reconstruct the occluded object regions whereby occluded objects can be handled similarly as non-occluded objects. On the other hand, reconstructing occluded object regions in the image space is complicated due to the super-rich variation of object occlusions in scene images.

Inspired by the Masked Autoencoders (MAE)  that randomly occludes image patches and reconstructs them in representation learning, we treat object occlusions as natural masking and trainnetworks to complete occluded object regions to learn occlusion-tolerant representations. To this end, we design MonoMAE, a novel monocular 3D detection framework that adopts the idea of MAE by first masking certain object regions in the feature space (for simulating object occlusions) and then reconstructing the masked object features (for learning occlusion-tolerant representations). MonoMAE consists of a depth-aware masking module and a lightweight completion network. The depth-aware masking simulates object occlusions by masking the features of non-occluded objects adaptively according to the object depth information. It generates pairs of non-occluded and masked (i.e., occluded) object representations that can be directly applied to train the lightweight completion network, aiming for completing the occluded objects and learning occlusion-tolerant representations. Note that MonoMAE introduces little computational overhead in inference time as it requires no object masking in the inference stage.

The contributions of this work can be summarized in three major aspects. _First_, we design MonoMAE, a MAE-inspired monocular 3D detection framework that tackles object occlusions effectively by masking and reconstructing object regions in the feature space. To the best of our knowledge, this is the first work that explores masking-reconstructing for the task of monocular 3D object detection. _Second_, we design adaptive image masking and a lightweight completion network that mask non-occluded objects adaptively according to the object depth (for simulating object occlusions) and reconstruct the masked object regions (for learning occlusion-tolerant representations), respectively. _Third_, extensive experiments over KITTI 3D and nuScenes show that MonoMAE outperforms the state-of-the-art consistently and it can generalize to new domains as well.

## 2 Related Work

### Monocular 3D Object Detection

Monocular 3D detection aims for the identification and 3D localization of objects from a single-view image. Most existing work can be broadly classified into two categories. The first employs convolutional neural networks, where most methods follow conventional 2D detectors [12; 23]. The standard approach learns monocular 3D detectors from single-view images only [17; 1; 8; 70; 66; 34]. In addition, several studies explore to leverage extra training data, such as LiDAR point clouds [39; 55; 6; 48; 46; 45], depth maps [11; 47; 45; 22; 38], and 3D CAD models [7; 35; 42] to acquire more depth information. Beyond that, several studies exploit the geometry relation between 2D and 3D spaces in different ways. For example, M3D-RPN  applies the powerful 2D detector FPN  for 3D detection. MonoDLE  aligns the centers of 2D and 3D boxes for better 3D localization. GUPNet  leverages uncertainty modeling to estimate the height of 3D boxes from the 2D boxes.

Figure 1: Object occlusion is pervasive and affects monocular 3D detection: Object occlusion is pervasive, e.g., 62% (17725) cars in the KITTI 3D dataset suffer from various occlusions as illustrated in (a). Prevalent monocular 3D detection techniques such as GUPNet  and MonoDETR  are clearly affected by object occlusions in both 3D space (3D) and the birdâ€™s eye view (BEV) space as in (b). The proposed MonoMAE simulates and learns object occlusions by feature masking and completing which improves detection consistently for both occluded and non-occluded objects.

The second introduces powerful visual transformers [72; 21; 4; 65] for more accurate monocular 3D detection [19; 66; 71; 57; 56]. For example, MonoDTR  integrates context- and depth-aware features and injects depth positional hints into transformers. MonoDETR  modifies the transformer to be depth-aware and guides the detection process by contextual depth cues. However, most existing methods neglect object occlusions that exist widely in natural images and often degrade the performance of monocular 3D object detection clearly. We adopt the transformer architecture to learn occlusion-tolerant representations that can handle object occlusion effectively without requiring any extra training data or annotations.

### Occlusions in 3D Object Detection

Object occlusion is pervasive in scene images and it has been investigated in several 2D and 3D vision tasks [63; 52; 10; 28; 26; 29; 30]. One typical approach learns to estimate the complete localization of occluded objects. For example, Mono-3DT  estimates complete 3D bounding boxes by re-identifying occluded vehicles from a sequence of 2D images. BtcDet  leverages object shape priors to learns to estimate the complete shapes of partially occluded objects. Several studies consider the degree of occlusions in training. For example, MonoPair  exploits the relation of paired samples and encodes spatial constraints of occluded objects from their neighbors. HMF  introduces an anti-occlusion loss to focus on occluded samples. Different from existing methods, the proposed MonoMAE learns enriched and occlusion-tolerant representations by masking and completing object parts in the feature space.

### Masked Autoencoders in 3D Tasks

Masked Autoencoders (MAE)  learn visual representations by masking image patches and reconstructing them, and it has been explored in several point cloud pre-training studies. For outdoor point cloud pre-training, Occupancy-MAE  exploits range-aware random masking that employs three masking levels to deal with the sparse voxel occupancy structures of LiDAR point clouds. GD-MAE  introduces a Generative Decoder to merge the surrounding context to restore the masked tokens hierarchically. For indoor point cloud pre-training, Point-MAE  adopts MAE to directly reconstruct the 3D coordinates of masked tokens. I2P-MAE  introduces 2D pre-trained models, and it enhances 3D pre-training with diverse 2D semantics. PiMAE  learns cross-modal representations with MAE by interactively handling point clouds and RGB images. Different from existing studies, the proposed MonoMAE handles monocular 3D detection from single-view images and it focuses on object occlusions by learning to complete occluded object regions in the feature space.

## 3 Proposed Method

### Problem Definition

Monocular 3D detection takes a single RGB image as input, aiming to classify objects and predict their 3D bounding boxes. The prediction of each object is composed of the object category \(C\), a 2D bounding box \(B_{2D}\), and a 3D bounding box \(B_{3D}\). The 3D bounding box \(B_{3D}\) can be decomposed to the object 3D location \((x_{3D},y_{3D},z_{3D})\), the object dimensions in object height, width and length \((h_{3D},w_{3D},l_{3D})\), as well as the object orientation \(\).

### Overall Framework

Figure 2 shows the framework of the proposed MonoMAE. Given an input image \(I\), the 3D Backbone first generates a sequence of 3D object queries \(Q=[q_{1},q_{2},,q_{K}]\) (\(K\) denotes query number), and the Non-Occluded Query Grouping then classifies the queries into two groups including non-occluded queries \(Q^{NO}=[q_{1}^{NO},q_{2}^{NO},,q_{U}^{NO}]\) and occluded queries \(Q^{O}=[q_{1}^{O},q_{2}^{O},,q_{V}^{O}]\) (\(U\) and \(V\) are the number of non-occluded and occluded queries). The Non-Occluded Query Masking then masks \(Q^{NO}\) to produce masked queries according to their depth \(D=[d_{1},d_{2},,d_{U}]\), leading to the masked queries \(Q^{M}=[q_{1}^{M},q_{2}^{M},,q_{U}^{M}]\). The Query Completion further reconstructs \(Q^{M}\) to produce the completed queries \(Q^{C}=[q_{1}^{C},q_{2}^{C},,q_{U}^{C}]\). Finally, the occluded queries \(Q^{O}\) and the completed queries \(Q^{C}\) are concatenated and fed to the Monocular 3D Detection for 3D detection

predictions. Note the inference does not involve the Non-Occluded Query Masking, and it just concatenates the completion of occluded queries \(Q^{O}\) (i.e., \(Q^{C}\)) with the non-occluded queries \(Q^{NO}\) and feeds the concatenated queries to the 3D Detection Head for 3D predictions.

### Non-Occluded Query Masking

Queries predicted by the 3D Backbone are either occluded or non-occluded, depending on whether the corresponding objects are occluded in the input image. In MonoMAE, we mask the non-occluded queries in the feature space to simulate occlusions, aiming to generate pairs of non-occluded and masked (i.e., occluded) queries for learning occlusion-tolerant object representations.

Specifically, we design a Non-Occluded Query Grouping module to identify non-occluded queries and then feed them into a Depth-Aware Masking module to synthesize occlusions, with more detail to be elaborated in the following subsections.

**Non-Occluded Query Grouping.** The Non-Occluded Query Grouping classifies the queries based on whether their corresponding objects are occluded or non-occluded. With no information about whether the input queries are occluded, we design an occlusion classification network \(_{}\) to predict the occlusion conditions \(O^{p}=[o_{1}^{p},o_{2}^{p},,o_{K}^{p}]\) of queries \(Q=[q_{1},q_{2},,q_{K}]\), where for the \(i\)-th query \(o_{i}^{p}=_{}(q_{i})\). The Non-Occluded Query Grouping can be formulated by:

\[\{q_{i} Q^{NO}&o_{i}^{p}=0\\ q_{i} Q^{O}&o_{i}^{p}=1.,\] (1)

where \(o_{i}^{p}=0\) denotes the query is non-occluded, and \(o_{i}^{p}=1\) denotes the query is occluded. The occlusion classification network is trained with the occlusion classification loss \(L_{occ}\) as follows:

\[L_{occ}=CE(O^{p},O^{gt}),\] (2)

where \(CE\) is the \(Cross\)\(Entropy\) loss. We adopted the bipartite matching  to match the predicted queries and objects in the image, where only matched queries have ground truth \(O^{gt}\) of KITTI 3D  about whether they are occluded or not.

**Depth-Aware Masking.** We design depth-aware masking to adaptively mask non-occluded query features to simulate occlusions in the feature space, aiming to create non-occluded and occluded (i.e., masked) pairs for learning occlusion-tolerant representations. As illustrated in Figure 3, the depth-aware masking determines the mask ratio according to the object depth - the closer the object, the larger the mask ratio, thereby compensating the information deficiency of distant objects. In addition, we simulate occlusions by masking in the feature space, as masking and reconstructing at the image level is complicated and computationally intensive.

Figure 2: The framework of MonoMAE training: Given a single-view image, the 3D Backbone extracts 3D object query features which are grouped into non-occluded query features and occluded query features by the Non-Occluded Query Grouping. The Depth-Aware Masking then masks the non-occluded query features to simulate object occlusions adaptively based on the object depth, and the Completion Network then learns to reconstruct the masked queries. Finally, the completed and the occluded query features are concatenated to train the 3D Detection Head for 3D predictions.

The depth-aware masking first obtains the query depth before query masking. Without backward gradient propagation, it adopts the 3D Detection Head to obtain the depth \(D=[d_{1},d_{2},,d_{U}]\) for non-occluded queries. With the predicted depth, each non-occluded query is randomly masked as illustrated in Figure 3. Specifically, objects that are more distant from the camera are usually captured with less visual information. The depth-aware masking accommodates this by assigning a smaller masking ratio to them, thereby keeping more visual information for distant objects for proper visual representation learning.

The mask ratio \(r\) of each query is determined by:

\[r=1.0-d_{i}/D_{max},\] (3)

where \(r\) is the applied mask ratio for each query, \(d_{i}\) is the depth for the \(i\)-th query, and \(D_{max}\) is the maximum depth in datasets. The masks \(M=[m_{1},m_{2},,m_{U}]\) generated for queries obey a Bernoulli Distribution.

Finally, the query masking is formulated by:

\[q_{i}^{M}=q_{i}^{NO}*m_{i},\] (4)

where \(q_{i}^{M}\) is the masked query, \(q_{i}^{NO}\) is the non-occluded query, and \(m_{i}\) is the generated mask.

### Query Completion

The query completion learns to reconstruct the adaptively masked queries, aiming to produce completed queries whereby the network learns occlusion-tolerant representations that are helpful in detecting occluded objects. We design a completion network \(_{C}\) to reconstruct the masked queries. The Completion Network has an hourglass structure consisting of three conv-bn-relu blocks and one conv-bn block for 3D query completion. The completed query \(q_{i}^{C}\) is obtained by:

\[q_{i}^{C}=_{}(q_{i}^{M}),\] (5)

where \(q_{i}^{M}\) is the masked query. The Completion Network is trained under the supervision of the non-occluded queries before masking, where a completion loss \(L_{com}\) is formulated as follows:

\[L_{com}=L_{1}^{s}(Q^{NO},Q^{C}),\] (6)

where \(L_{1}^{s}\) denotes the SmoothL1 loss , \(Q^{NO}\) denotes the non-occluded queries, and \(Q^{C}\) denotes the queries completed by the Completion Network.

### Loss Functions

The overall objective consists of three losses including \(L_{occ}\), \(L_{com}\), and \(L_{base}\) where \(L_{occ}\) and \(L_{com}\) are defined in Equation 2 and Equation 6, and \(L_{base}\) denote losses for supervising the 3D box

Figure 3: Illustration of the Depth-Aware Masking. (a) Objects farther away are usually smaller capturing less visual information. (b) The Depth-Aware Masking determines the mask ratio of an object according to its depth - the closer the object is, the larger the mask ratio is applied, thereby compensating the information deficiency for objects that have larger distances from the camera.

predictions. Specifically, \(L_{base}\) includes losses for supervising the 3D box predictions including each object's 3D locations, height, width, length and orientation. We set the weight for each loss item to 1.0, and the overall loss function is formulated as follows:

\[L=L_{occ}+L_{com}+L_{base}.\] (7)

## 4 Experiments

### Experimental Settings

**Datasets.** We benchmark our method over two public datasets in monocular 3D object detection.

\(\) KITTI 3D  comprises 7,481 training images and 7,518 testing images, with training-data labels publicly available and test-data labels stored on a test server for evaluation. Following , we divide the 7,481 training samples into a new train set with 3,712 images and a validation set with 3,769 images for ablation studies.

\(\) NuScenes  comprises 1,000 video scenes, including RGB images captured by 6 surround-view cameras. The dataset is split into a training set (700 scenes), a validation set (150 scenes), and a test set (150 scenes). Following [1; 50; 37; 24; 20], the performance on the validation set of nuScenes is reported.

In addition, we perform evaluations on the most representative Car category of KITTI 3D and nuScenes datasets as in prior studies [51; 50; 54; 66]

**Evaluation Metrics.** For KITTI 3D, we follow  and adopt AP\(|_{R_{40}}\), the average of the AP of 40 recall points as the evaluation metric. We report the average precision on BEV and 3D object detection by AP\({}_{BEV}|_{R_{40}}\) and AP\({}_{3D}|_{R_{40}}\) with a threshold of 0.7 for both test and validation sets. For the nuScenes dataset, we adopt the mean absolute depth errors  in evaluations.

**Implementation Details.** We conduct experiments on one NVIDIA V100 GPU and train the framework for 200 epochs with a batch size of 16 and a learning rate of \(2 10^{-4}\). We use the AdamW  optimizer with weight decay \(10^{-4}\). We employ ResNet-50  as the Transformer-based backbone and adopt the 3D detection head from  as our detection framework.

   &  &  & _{3D}\)(IoU\(=0.7\))} & _{R_{40}}\)} & _{BEV}\)(IoU\(=0.7\))} \\  & & & & Easy & & & & Easy & Moderate & Hard \\  MonoRUn  & CVPR 21 &  & 19.65 & 12.30 & 10.58 & 27.94 & 17.34 & 15.24 \\ MonoDTR  & CVPR 22 & & 21.99 & 15.39 & 12.73 & 28.59 & 20.38 & 17.14 \\ MonoDistill  & ICLR 22 & & 22.97 & 16.03 & 13.60 & 31.87 & 22.59 & 19.72 \\ DID-M3D  & ECCV 22 & & 24.40 & 16.29 & 13.75 & 32.95 & 22.76 & 19.83 \\ MonoNeRD  & ICCV 23 & & 22.75 & 17.13 & 15.63 & 31.13 & 23.46 & 20.97 \\  D4LCN  & CVPR 20 &  & 16.65 & 11.72 & 9.51 & 22.51 & 16.02 & 12.55 \\ DDMP-3D  & CVPR 21 & & 19.71 & 12.78 & 9.80 & 28.08 & 17.89 & 13.44 \\ DD3D  & ICCV 21 & & 23.22 & 16.34 & 14.20 & 30.98 & 22.56 & 20.03 \\  Kinematic3D  & ECCV 20 & Video & 19.07 & 12.72 & 9.17 & 26.69 & 17.52 & 13.10 \\  AutoShape  & ICCV 21 & CAD & 22.47 & 14.17 & 11.36 & 30.66 & 20.08 & 15.59 \\  MonoFlex  & CVPR 21 &  & 19.94 & 13.89 & 12.07 & 28.23 & 19.75 & 16.89 \\ MonoRCNN  & ICCV 21 & & 18.36 & 12.65 & 10.03 & 25.48 & 18.11 & 14.10 \\ GUPNet  & ICCV 21 & & 20.11 & 14.20 & 11.77 & - & - & - \\ DEVIANT  & ECCV 22 & & 21.88 & 14.46 & 11.89 & 29.65 & 20.44 & 17.43 \\ MonoCon  & AAAI 22 & None & 22.50 & 16.46 & 13.95 & 13.12 & 22.10 & 19.00 \\ MonoDETR  & ICCV 23 & & 25.00 & 16.47 & 13.58 & 33.60 & 22.11 & 18.60 \\ MonoUNI  & NeurIPS 23 & & 24.75 & 16.73 & 13.49 & - & - & - \\ MonoCD  & CVPR 24 & & 25.53 & 16.59 & 14.53 & 33.41 & 22.81 & 19.57 \\ 
**Ours** & - & None & **25.60** & **18.84** & **16.78** & **34.15** & **24.93** & **21.76** \\  

Table 1: Benchmarking on the KITTI 3D _test_ set. All experiments adopt AP\(|_{R_{40}}\) metric with an IoU threshold of 0.7. Best in **bold**, second underlined.

### Benchmarking with the State-of-the-Art

We benchmark MonoMAE with state-of-the-art monocular 3D object detection methods both quantitatively and qualitatively.

**Quantitative Benchmarking.** Table 1 shows quantitative experiments on the test set of dataset KITTI 3D, where all evaluations were performed on the official online test server  for fairness. We can see that MonoMAE achieves superior detection performance consistently across all metrics, without using any extra training data such as image depths, video sequences, LiDAR points, and CAD 3D models. In addition, MonoMAE outperforms more for the Moderate and Hard categories where various occlusions happen much more frequently than the Easy category. The superior performance is largely attributed to our designed depth-aware masking and completion network, which masks queries to simulate object occlusions in the feature space and reconstructs the masked queries to learn occlusion-tolerant visual representations, respectively.

**Qualitative Benchmarking.** Figure 4 shows qualitative benchmarking on the KITTI 3D val set. It can be observed that compared with two state-of-the-art methods GUPNet and MonoDETR, the proposed MonoMAE produces more accurate 3D detection consistently for both non-occluded and occluded objects, even for challenging scenarios like distant objects. Specifically, GUPNet and MonoDETR tend to miss the detection of highly occluded object in Cases 1 and 2 as highlighted by red arrows. As a comparison, MonoMAE performs clearly better by detecting those challenging objects successfully, demonstrating its superior capability on handling object occlusions.

  Index & NOQG & DAM & CN & _{3D}\)(IoU\(=0.7\))\({}_{R_{40}}\)} & _{BEV}\)(IoU\(=0.7\))\({}_{R_{40}}\)} \\  & & & Easy & Moderate & Hard & Easy & Moderate & Hard \\  
1* & âœ“ & & & 24.85 & 16.21 & 14.74 & 34.53 & 23.99 & 18.84 \\
2 & & âœ“ & & 23.33 & 15.09 & 13.20 & 32.68 & 21.80 & 17.43 \\
3 & & & âœ“ & 27.33 & 18.52 & 14.95 & 36.51 & 24.21 & 19.12 \\
4 & âœ“ & âœ“ & & 24.69 & 15.71 & 13.57 & 33.46 & 23.03 & 18.19 \\
5 & âœ“ & & âœ“ & 27.25 & 18.76 & 15.45 & 36.81 & 25.18 & 20.05 \\
6 & & âœ“ & âœ“ & 28.39 & 19.35 & 15.87 & 37.59 & 26.27 & 21.33 \\ 
7 & âœ“ & âœ“ & âœ“ & **30.29** & **20.90** & **17.61** & **40.26** & **27.08** & **23.14** \\  

Table 2: Ablation study of technical designs in MonoMAE on the KITTI 3D _val_ set. â€˜NOQGâ€™, â€˜DAMâ€™, and â€˜CNâ€™ denote Non-Occluded Query Grouping, Depth-Aware Masking, and Completion Network, respectively. The symbol * indicates the baseline. The best results are highlighted in **bold**.

Figure 4: Detection visualization over the KITTI _val_ set. Ground-truth annotations are highlighted by red boxes, and predictions by MonoMAE and two state-of-the-art methods are highlighted by green boxes. Red arrows highlight objects that have very different predictions across the compared methods. The ground truth of LiDAR point clouds is provided for visualization only, and they are not used in MonoMAE training. Best viewed in color and zoom-in.

### Ablation Study

We conduct extensive ablation studies to examine the proposed MonoMAE. Specifically, we examine MonoMAE from the aspect of the technical designs, query masking strategies, as well as loss functions.

**Network Designs.** We examine the effectiveness of two key designs in MonoMAE, namely, the Depth-Aware Masking module (DAM) and the Completion Network (CN) (on the validation set of KITTI 3D), as shown in Table 2. We formulate the baseline by including the Non-Occluded Query Grouping module (NOQG), which does not affect the network training as both identified occluded and non-occluded queries are fed to train 3D detectors. When CN is not used in Rows 2 and 4, the 3D detection degrades as queries are masked but not reconstructed which leads to further information loss. While not incorporating DAM in Rows 3 and 5, the detection improves clearly compared with the baseline, as the completion helps learn better representations for naturally occluded queries. In addition, incorporating DAM and CN on top of NOQG in Row 7 performs clearly better than incorporating DAM and CN alone in Row 6, as the former applies masking and completion to non-occluded queries only. It also shows that masking naturally occluded queries to train the completion network is harmful to the learned representations.

**Masking Strategies.** We examine how different masking strategies affect monocular 3D detection. We studied three masking strategies as shown in Table 3. The first strategy masks the _input images_ randomly, aiming to assess the value of masking and completing in the feature instead of image space. We can observe that the image-level masking yields clearly lower performance as compared with query masking in the feature space, largely due to the complication in masking and reconstructing images with a lightweight completion network. The second strategy masks query features randomly without considering object depths, aiming to evaluate the importance of object depths in query masking. The experiments show that random query masking outperforms the image-level masking significantly. The third strategy performs the proposed depth-aware query masking. It outperforms the feature-space random masking consistently, demonstrating the value of object depths for query masking.

**Loss Functions.** We examine the impact of the occlusion classification loss \(L_{occ}\) and the completion loss \(L_{com}\) in Equations 2 and 6, where \(L_{occ}\) supervises the occlusion classification network (in Non-Occluded Query Grouping) to predict whether the queries are occluded and \(L_{com}\) supervises the Completion Network to reconstruct the masked queries. As Table 4 shows, while implementing \(L_{occ}\) alone, the occlusion prediction is supervised while the query reconstruction is unsupervised. The network under such an objective does not learn well as the Completion Network cannot reconstruct object queries well without sufficient supervision. While implementing \(L_{com}\) alone, the occlusion classification network cannot identify occluded and non-occluded queries accurately where many occluded queries are fed for masking, leading to more query occlusion and poor detection performance. While employing both losses concurrently, the performance improves significantly as non-occluded queries can be identified for masking and reconstruction, leading to occlusion-tolerant representations.

   & \)} & \)} & _{3D}\)[IoU\(=0.7\)]\({}_{R_{40}}\)} & _{BEV}\)[IoU\(=0.7\)]\({}_{R_{40}}\)} \\  & & & Easy & Moderate & Hard & Easy & Moderate & Hard \\  
1 & âœ“ & & 28.37 & 19.61 & 16.01 & 37.48 & 26.55 & 21.50 \\
2 & & âœ“ & 26.36 & 19.15 & 15.88 & 36.76 & 26.49 & 22.62 \\ 
3 & âœ“ & âœ“ & **30.29** & **20.90** & **17.61** & **40.26** & **27.08** & **23.14** \\  

Table 4: Ablation study of the loss functions on the KITTI 3D _val_ set. \(L_{occ}\) and \(L_{com}\) refer to the occlusion classification loss and the completion loss, respectively. The best results are in **bold**.

   &  Masking Strategy \\  } & _{3D}\)[IoU\(=0.7\)]\({}_{R_{40}}\)} & _{BEV}\)[IoU\(=0.7\)]\({}_{R_{40}}\)} \\  & & Easy & Moderate & Hard & Easy & Moderate & Hard \\  
1 &  & 20.51 & 15.03 & 13.24 & 27.76 & 19.74 & 16.71 \\
2 &  & 27.14 & 18.47 & 15.02 & 36.98 & 25.52 & 20.64 \\ 
3 &  & **30.29** & **20.90** & **17.61** & **40.26** & **27.08** & **23.14** \\  

Table 3: Ablation study of masking strategies on the KITTI 3D _val_ set. The best results are in **bold**.

### Discussions

**Efficiency Comparison.** We compare the inference time of several representative monocular 3D detection methods on the KITTI val set, where all compared methods are evaluated with one NVIDIA V100 GPU under the same computational environment for fairness. As Table 5 shows, GUPNet, MonoDTR, and MonoDETR have an average inference time of 40ms, 37ms, and 43ms for each image, respectively. As a comparison, the proposed MonoMAE takes the shortest inference time, demonstrating its good efficiency in monocular 3D detection. Further, we analyzed the Completion Network in terms of network parameters and floating-point operations per second (FLOPs), showing it has very limited 2.22G parameters and 0.08M in FLOPs.

**Generalization Ability.** We examine the generalization capability of the proposed MonoMAE by directly applying the KITTI-trained MonoMAE model to the car Category of the nuScenes validation set without additional training. The detection performance on the KITTI validation set is also reported for reference. Table 6 shows that MonoMAE attains the highest or second-highest detection performance across various metrics on the nuScenes frontal validation set. This indicates that despite the domain shift from KITTI to nuScenes, MonoMAE still maintains satisfactory performance. Since DEVIANT  is equivariant to the depth translations, it sometimes has higher performance.

## 5 Conclusion

This paper presents MonoMAE, a novel method inspired by the Masked Autoencoders (MAE) to deal with the pervasive occlusion problem in the monocular 3D object detection task. MonoMAE consists of two key designs. The first is a depth-aware masking module, which simulates the occlusion for non-occluded object queries in the feature space during training. The second is a lightweight completion network, which reconstructs and completes the masked object queries. Quantitative and qualitative experiment results show that MonoMAE learns enhanced 3D representations and achieves superior monocular 3D detection performance for both occluded and non-occluded objects. Moving forward, we plan to investigate generative approaches to simulate natural occlusion patterns for various 3D detection tasks.

**Limitations.** MonoMAE leverages depth-aware masking to mask non-occluded queries to simulate object occlusions in the feature space. However, the masked queries may have different patterns as compared with the features of naturally occluded object queries. Such a gap could affect the reconstruction of complete queries and monocular 3D detection performance. This issue could be mitigated by introducing generative networks that learn distributions from extensive real-world data for generating occlusion patterns that are more similar to natural occlusions.

   &  &  \\  & 0-20 & 20-40 & 40-\(\) & All & 0-20 & 20-40 & 40-\(\) & All \\   M3D-RPN  & 0.56 & 1.33 & 2.73 & 1.26 & 0.94 & 3.06 & 10.36 & 2.67 \\ MonoRCNN  & 0.46 & 1.27 & 2.59 & 1.14 & 0.94 & 2.84 & 8.65 & 2.39 \\ GUPNet  & 0.45 & 1.10 & 1.85 & 0.89 & 0.82 & 1.70 & 6.20 & 1.45 \\ DEVIANT  & 0.40 & 1.09 & 1.80 & 0.87 & 0.76 & 1.60 & **4.50** & **1.26** \\ MonoUNI  & 0.38 & 0.92 & 1.79 & 0.87 & 0.72 & 1.79 & 4.98 & 1.43 \\ 
**MonoMAE (Ours)** & **0.36** & **0.91** & **1.74** & **0.86** & **0.71** & **1.57** & 4.95 & 1.40 \\  

Table 6: Cross-dataset evaluations that perform training on the KITTI train set, and testing on the KITTI val and nuScenes val sets. We adopt the evaluation metric mean absolute error of the depth (\(\)). Best is highlighted in **bold**, and second underlined.

  Method & GUPNet  & MonoDTR  & MonoDETR  & Ours* & Ours \\   Inference Time (ms) & 40 & 37 & 43 & 36 & 38 \\  

Table 5: Comparison on inference speed of several monocular 3D detection methods. Ours* denotes the proposed MonoMAE without including the Completion Network.