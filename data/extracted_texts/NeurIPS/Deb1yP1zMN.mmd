# Automatic Integration for Spatiotemporal

Neural Point Processes

 Zihao Zhou

Department of Computer Science

University of California, San Diego

La Jolla, CA 92092

ziz244@ucsd.edu

&Rose Yu

Department of Computer Science

University of California, San Diego

La Jolla, CA 92092

roseyu@ucsd.edu

###### Abstract

Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. (2019) proposes a dual network approach for efficient integration of flexible intensity function. However, their method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the dual network approach to 3D STPP. While previous work provides a foundation, its direct extension overly restricts the intensity function and leads to computational challenges. In response, we introduce a decomposable parametrization for the integral network using ProMNet. This approach, leveraging the product of simplified univariate graphs, effectively sidesteps the computational complexities inherent in multivariate computational graphs. We prove the consistency of AutoSTPP and validate it on synthetic data and benchmark real-world datasets. AutoSTPP shows a significant advantage in recovering complex intensity functions from irregular spatiotemporal events, particularly when the intensity is sharply localized. Our code is open-source at https://github.com/Rose-STL-Lab/AutoSTPP.

## 1 Introduction

Spatiotemporal point process (STPP) (Daley and Vere-Jones, 2007; Reinhart, 2018) is a continuous time stochastic process for modeling irregularly sampled events over space and time. STPPs are particularly well-suited for modeling epidemic outbreaks, ride-sharing trips, and earthquake occurrences. A central concept in STPP is the _intensity function_, which captures the expected rates of events occurrence. Specifically, given the event sequence over space and time \(_{t}=\{(_{1},t_{1}),,(_{n},t_{n})\}_{t_{n } t}\), the joint log-likelihood of the observed events is:

\[ p(_{t})=_{i=1}^{n}^{*}(_{i},t_{i})- _{}_{0}^{t}^{*}(,)dd\] (1)

\(^{*}\) is the optimal intensity, \(\) the spatial domain, \(\) and \(\) the space and time, and \(t\) the time range.

Learning STPP requires multivariate integrals of the intensity function, which is numerically challenging. Traditional methods often assume a parametric form of the intensity function, such as the integralcan have a closed-form solution Daley and Vere-Jones (2007). But this also limits the expressive power of the model in describing complex spatiotemporal patterns.

Others propose to parameterize the model using neural ODE models Chen et al. (2020) and Monte Carlo sampling, but their computation is costly for high-dimensional functions. Recently, work by Zhou et al. (2022) proposes a nonparametric STPP approach. They use kernel density estimation for the intensity and model the parameters of the kernels with a deep generative model. Still, their method heavily depends on the number of background data points chosen as a hyper-parameter. Too few background points cause the intensity function to be an inflexible Gaussian mixture, while too many background points may cause overfitting on event arrival.

To reduce computational cost while maintaining expressive power, we propose a novel _automatic integration_ scheme based on dual networks to learn STPPs efficiently. Our framework models intensity function as the sum of background and influence functions. Instead of relying on closed-form integration or Monte Carlo sampling, we directly approximate the integral of the influence function with a deep neural network (DNN). Taking the partial derivative of a DNN results in a new computational graph that shares the same parameters; see Figure 1.

First, we construct an integral network whose derivative is the intensity function. Then, we train the network parameters by maximizing the data likelihood formulated in terms of the linear combinations of the integral networks. Finally, we reassemble the parameters of the integral network to obtain the intensity. This approach leads to the exact intensity function and its antiderivative without restricting its parametric forms, thereby making the integration process "automatic".

Our approach bears a resemblance with the fully NN approach by Omi et al. (2019) for 1D temporal point processes. There, automatic integration can be easily implemented by imposing monotonicity constraints on the integral network (Lindell et al., 2021; Li et al., 2019). However, due to triple integration in STPP, imposing monotonicity constraints significantly hurdles the expressivity of the integral network, leading to inaccurate intensity. As the experiments show, extending Fully NN to 3D cannot learn complex spatiotemporal intensity functions. Instead, we propose a decomposable parametrization for the integral network that bypasses this restriction.

Our approach can efficiently compute the exact likelihood of _any_ continuous influence function. We validate our approach using synthetic spatiotemporal point processes with complex intensity functions. We also demonstrate the superior performance on several real-world discrete event datasets. Compared to FullyNN by Omi et al. (2019), our approach presents a more general higher-order automatic integration scheme and is more effective in learning complex intensity functions. Also, the probability density of Omi et al. (2019) is ill-defined as its intensity integral does not diverge to infinity (Shchur et al., 2019). We fix the issue by adding a constant background intensity \(\).

To summarize, our contributions include:

* We propose the first deep learning framework AutoSTPP to speed up spatiotemporal point process learning with automatic integration. We use dual networks and enforce the non-negativity of the intensity via a monotone integral network.
* We show that our automatic integration scheme empirically learns intensity functions more accurately than other integration approaches.
* We prove that the derivative network of AutoSTPP is a universal approximator of continuous functions and, therefore, is a consistent estimator under mild assumptions.

Figure 1: Illustration of AutoSTPP. \(W\) denotes the linear layer’s weight. \(\) is the nonlinear activation function. Left shows the intensity network approximating \((,t)\), and right is the integral network that computes \(_{}_{t}\). The two networks share the same parameters.

* We demonstrate that AutoSTPP can recover complex influence functions from synthetic data, enjoys high training efficiency and model interpretability, and outperforms the state-of-the-art methods on benchmark real-world data.

## 2 Related Work

**Parametrizing Point Process.** Fitting traditional STPP, such as the spatiotemporal Hawkes process, to data points with parametric models can perform poorly if the model is misspecified. To address this issue, statisticians have extensively studied semi- and non-parametric inference for STPP. Early works like Brix and Moller (2001), Brix and Diggle (2001) usually rely on Log-Gaussian Cox processes as a backbone and Epanechnikov kernels as estimators of the pair correlation function. Adams et al. (2009) propose a non-parametric approach that allows the generation of exact Poisson data from a random intensity drawn from a Gaussian Process, thus avoiding finite-dimensional approximation. These Cox process models are scalable but assume a continuous intensity change over time.

Recently, neural point processes (NPPs) that combine point processes with neural networks have received considerable attention (Yan et al., 2018; Upadhyay et al., 2018; Huang et al., 2019; Shang and Sun, 2019; Zhang et al., 2020). Under this framework, models focus more on approximating a discrete set of intensities before and after each event. The continuous intensity comes from interpolating the intensities between discrete events. For example, (Du et al., 2016) uses an RNN to generate intensities after each event. (Mei and Eisner, 2016) proposes a novel RNN architecture that generates intensities at both ends of each inter-event interval. Other works consider alternative training schema: Xiao et al. (2017) used Wasserstein distance, Guo et al. (2018) introduced noise-contrastive estimation, and Li et al. (2018) leveraged reinforcement learning. While these NPP models are more expressive than the traditional point process models, they still assume simple (continuous, usually monotonous) inter-event intensity changes and only focus on temporal point processes.

Neural STPP (Chen et al., 2020; Zhou et al., 2022) further generalizes NPP to spatiotemporal data. They use a non-negative activation function to map the hidden states to a scalar, i.e., the temporal intensity immediately after an event, and a conditional spatial distribution. The change of intensity between events is represented by a decay function or a Neural ODE. The conditional spatial distribution is represented by a kernel mixture or a normalizing flow. Nevertheless, all models assume a continuous transformation of the intensity function and have limited expressivity.

In the context of temporal point processes (TPP), closely related approaches are Omi et al. (2019) and Zhou and Yu (2023). Both propose using a Deep Neural Network (DNN) to parameterize the integral of an intensity function. The work by Omi et al. (2019) offers a more flexible formulation yet does not incorporate any specific prior assumptions about the form of the intensity changes. On the other hand, the approach by Zhou and Yu (2023) is capable of capturing more sophisticated influence functions, and it is this work that our research builds upon. However, both of these studies focus on the easier problem of learning the derivative with respect to time alone, thereby neglecting the rich amount of other features that may associated with the timestamps.

**Integration Methods.** Integration methods are largely ignored in NPP literature but are central to a model's ability to capture the complex dynamics of a system. Existing works either use an intensity function with an elementary integral (Du et al., 2016) or Monte Carlo integration (Mei and Eisner, 2016). However, we will see in the experiment section that the choice of integration method has a non-trivial effect on the model performance.

Integration is generally more complicated than differentiation, which can be mechanically solved using the chain rule. Most integration rules, e.g., integration by parts and change of variables, transform an antiderivative to another that is not necessarily easier. Elementary antiderivative only exists for a small set of functions, but not for simple composite functions such as \((x^{2})\)(Dunham, 2018). The Risch algorithm can determine such elementary antiderivative (Risch, 1969, 1970) but has never been fully implemented due to its complexity. The most commonly used integration methods are still numerical: Newton-Cotes Methods, Romberg Integration, Quadrature, and Monte Carlo integration (Davis and Rabinowitz, 2007).

Several recent works leverage automatic differentiation to speedup integration, a paradigm known as Automatic Integration (AutoInt). Liu (2020) proposes integrating the Taylor polynomial using the derivatives from Automatic Differentiation (AutoDiff). It requires partitioning of the integral limits

[MISSING_PAGE_FAIL:4]

where \(\) indicates the Hadamard product, and \(_{11}\) is the first column of \(_{1}\), i.e.,

\[_{1}:=[_{11}_{12}_{ 1,M_{1}}]\]

Computing \(f_{}()\) involves many repeated operations. For example, the result of \(_{1}\) is used for compute both \((_{1})\) and \(^{}(_{1})\), see Figure 1. We have developed a program that harnesses the power of dynamical programming to compute derivatives efficiently with AutoDiff. See Appendix D for the detailed algorithm.

### AutoInt Point Processes as Consistent Estimators

We show that the universal approximation theorem (UAT) holds for derivative networks. This theorem signifies that, given a sufficient number of hidden units, derivative networks can theoretically approximate any continuous functions, no matter how complex. Therefore, using derivative networks does not limit the range of influence functions that can be approximated.

**Proposition 3.1** (Universal Approximation Theorem for Derivative Networks).: _The set of derivative networks corresponding to two-layer feedforward integral networks is dense in \(C()\) with respect to the uniform norm._

For a detailed proof, see Appendix E. With UAT, it is clear that under some mild assumptions, AutoInt Point Processes are consistent estimators of point processes that take the form of Equation 5.

**Proposition 3.2** (Consistency of AutoInt Point Process).: _Under the assumption that the ground truth point process is stationary, ergodic, absolutely continuous and predictable, if the ground truth influence function is truncated (i.e., \( C,f(t)=0\  t>c\)), the maximum likelihood estimator \(f_{}\) converges to the true influence function \(f\) in probability as \(T\)._

Our model belongs to the class of linear self-excitation processes, whose maximum likelihood estimator properties were analyzed by Ogata et al. (1978). Under the assumptions above, two conditions are needed for the proof of consistency:

**Assumption 3.3**.: (Consistency Conditions) For any \(\) there is a neighbourhood \(U\) of \(\) such that

1. \(_{^{} U}|_{^{}}(t,)-_{ ^{}}^{*}(t,)| 0\) in probability as \(t\),
2. \(_{^{} U}|_{^{}}^{*}(t,)|\) has, for some \(>0\), finite \((2+)\) th moment uniform bounded with respect to \(t\).

The first condition is satisfied by UAT. The second condition depends on the rate of decrease of the influence tail and is satisfied by truncation. In our experiments, we truncated the history by only including the influences of the previous 20 events. If the ground truth influence function decays over the entire time domain, our estimator may exhibit negligible bias.

### 3D Automatic Integration

AutoInt gives us an efficient way to calculate a line integral over one axis. However, for spatiotemporal point process models, we need to calculate the triple integral of the intensity function over space and time. Since we cannot evaluate the integral network with an input of infinity, we assume the spatial domain to be a rectangle, such that the triple integral is over a cuboid. We then convert the triple integral to line integrals using Divergence and Green's theorem (Marsden and Tromba, 2003).

Define \(:=(x,y)\) and \(t:=z\), we model the spatiotemporal influence \(f_{}^{*}(x,y,z)\) as

\[f_{}^{*}(,t)=++\] (4)

The Divergence theorem relates volume and surface integrals. It states that for any \(P(x,y,z),Q(x,y,z),R(x,y,z)\) that is differentiable over the cuboid \(\), we have

\[_{}(++)dv=_{}Pdydz+Qdzdx+Rdxdy,\]

where \(\) are the six rectangles that enclose \(\).

Using \(R\) as an example, we begin with two neural network approximators, \(L_{R}\) and \(M_{R}\). We initialize the two approximators as first-order derivative networks, whose corresponding integral networks are \( L_{R}dx\) and \( M_{R}dy\). We use the \(xy\) partial derivatives of the two networks to model \(R\), such that \(R:=}{ x}-}{ y}\). Then the \(z\) partial derivative \(\) is \(M_{R}}{ x z}-L_{R}}{  y z}= M_{R}dy}{ x y  z}- L_{R}dx}{ x y z}\). We can see that \(\) can be exactly evaluated as the third derivatives of two integral networks. In the same manner, we can model \(P\) and \(Q\) such that the intensity is parametrized by six networks: \(L_{R},L_{Q},L_{P},M_{R},M_{Q},\) and \(M_{P}\).

We use neural network pairs \(\{L,M\}\) to parametrize each intensity term in Equation 4 according to Green's theorem. It states that for any \(L\) and \(M\) differentiable over the rectangle \(D\),

\[_{D}(- )dxdy=_{L^{+}}(Ldx+Mdy),\]

where \(L^{+}\) is the counterclockwise path that encloses \(D\). \( Rdxdy\) is then \( L_{R}dx+M_{R}dy\), and can be exactly evaluated using the integral networks.

In practice, we observe that the six networks' parametrization of intensity can be simplified. \(L_{R}\), \(L_{Q}\), \(L_{P}\) can share the same set of weights, and similarly for \(M_{R}\), \(M_{Q}\), \(M_{P}\). The influence is essentially parametrized by two different neural networks, \(L\) and \(M\). That is,

\[f_{}(t,):=( Mdy}{ x y  z}- Ldx}{ x y z})+(  Mdz}{ x y z}- Ldy }{ x y z})+( Mdx}{ x  y z}- Ldz}{ x y z})\]

### Imposing the 3D Non-negativity Constraint

For the NPP model in Equation 3, the derivative network \(f_{}\) needs to be non-negative. Imposing the non-negativity constraint for 3D AutoInt is a challenging task. It implies that the integral network \(F_{}\) always has a nonnegative triple derivative.

A simple approach is to apply an activation function with a nonnegative triple derivative. An integral network that uses this activation and has nonnegative linear layer weights satisfies the condition. We call this approach "Constrained Triple AutoInt". However, the output of an integral network can grow very quickly with large input, and the gradients are likely to explode during training. Moreover, the non-negative constraint on the influence function only requires \(}{ t}\) to be positive. But an activation function with a nonnegative triple derivative would also enforce other partial derivatives to be positive.

Such constraints are overly restrictive for STPPs, whose partial derivatives \(}{}\) and \(}{ t t}\) can both be negative when the intensity is well-defined.

ProdNet.We propose a different solution to enforce the 3D non-negative constraint called _ProdNet_. Specifically, we decompose the influence function \(f_{}(s_{1},s_{2},t):^{3}\) as \(f_{}^{1}(s_{1})f_{}^{2}(s_{2})f_{}^{3}(t)\), the product of three \(\) AutoInt derivative networks. The triple antiderivative of the influence

Figure 2: Comparison of fitting a nonnegative function \(f(x,y,z)=(x)(y)(z)+1\). Our proposed AutoInt ProdNet approach can well approximate the ground truth function. In contrast, imposing the constraint through activation function with a nonnegative triple derivative “Constrained Triple AutoInt” fails due to overly stringent constraint.

function is then \(F_{}^{1}(s_{1})F_{}^{2}(s_{2})F_{}^{3}(t)\), the product of their respective integral networks. Then we can apply 1D non-negative constraint to each of the derivative networks. The other partial derivatives are not constrained because \(F_{}^{1},F_{}^{2},F_{}^{3}\) can be negative.

One limitation of such decomposition is that it can only learn the joint density of marginally independent distribution. We circumvent this issue by parameterizing the influence function as the sum of \(N\) ProMedNet, \(_{i=1}^{N}f_{,i}^{1}(s_{1})f_{,i}^{2}(s_{2})f_{,i}^{3}(t)\). The formulation is no longer marginally independent since it is not multiplicative decomposable. Figure 2 shows that the sum of two ProMedNets is already sufficient to approximate a function that cannot be written as the sum of products of positive functions. In contrast, the constrained triple AutoInt's intensity is convex everywhere and fails to fit \(f\).

Increasing the number of ProMedNet improves AutoInt's flexibility at the cost of time and memory. We perform an ablation study of the number of ProMed in Appendix F.

### Model Training

Given the integral network \(F_{}(t,):=_{}F_{}(,t,)\) and the derivative network approximating the influence function \(f_{}=}{ t}\), the log-likelihood of an event sequence \(_{n}=\{(_{1},t_{1}),,(_{n},t_{n})\}\) observed in time interval \([0,T]\) with respect to the model is

\[(_{n})=_{i=1}^{n}(_{j=1}^{i-1}f_{ }(_{i}-_{j},t_{i}-t_{j},_{i}))-_{i=1} ^{n}(F_{}(T-t_{i},_{n})-F_{}(0,_{i}))\]

Obtaining the above from Equation 1 is straightforward by the Fundamental Theorem of Calculus. \(F_{}\) is evaluated using the Divergence theorem. We can learn the parameters \(\) in both networks by maximizing the log-likelihood function. In experiments, we parametrize \(f_{}\) with two AutoInt networks \(L\) and \(M\). Each network is a sum of \(N\) ProMedNets. That is,

\[f_{}(,t):=_{i=1}^{N}f_{_{i}}(,t)=3_{i= 1}^{N}_{x\{s_{1},s_{2},t\}}[ M_{i}dx}{ x ^{3}}- L_{i}dx}{ x^{3}}]\]

We name our method Automatic Spatiotemporal Point Process (AutoSTPP).

## 4 Experiments

We compare the performances of different neural STPPs using synthetic and real-world benchmark data. For synthetic data, our goal is to validate our AutoSTPP can accurately recover complex

Figure 3: Comparing the ground truth conditional intensity \(^{*}(,t)\) with the learned intensity on the _ST Hawkes_ Dataset 1 and _ST Self-Correcting_ Dataset 3. **Top row:** Ground truth. **Second row:** Our AutoSTPP. **Rest of the rows:** Baselines. The crosses on top represent past events. Larger crosses indicate more recent events.

intensity functions. Additionally, we show that the errors resulting from numerical integration lead to a higher variance in the learned intensity than closed-form integration. We show that our model performs better or on par with the state-of-the-art methods for real-world data.

### Experimental Setup

Synthetic Datasets.We follow the experiment design of Zhou et al. (2022) to validate that our method can accurately recover the true intensity functions of complex STPPs. We use six synthetic point process datasets simulated using Ogata's thinning algorithm (Chen, 2016), see Appendix B for details. The first three datasets were based on spatiotemporal Hawkes processes, while the remaining three were based on spatiotemporal self-correcting processes. Each dataset spans a time range of \([0,10000)\) and is generated using a fixed set of parameters. Each dataset was divided into a training, validation, and testing set in an \(8:1:1\) ratio based on the time range.

Spatiotemporal Hawkes process (STH).A spatiotemporal Hawkes process, also known as a self-exciting process, posits that every past event exerts an additive, positive, and spatially local influence on future events. This pattern is commonly observed in social media and earthquakes. The process is characterized by the intensity function (Reinhart, 2018):

\[^{*}(,t):= g_{0}()+_{i:t_{i}<t}g_{1}(t,t_{i} )g_{2}(,_{i}):>0.\] (5)

\(g_{0}\) represents the density of the background event distribution over \(\). \(g_{2}\) represents the density of the event influence distribution centered at \(_{i}\) and over \(\). \(g_{1}\) describes each event \(t_{i}\)'s influence decay over time. We implement \(g_{0}\) and \(g_{2}\) as Gaussian densities and \(g_{2}\) as exponential decay functions.

Spatiotemporal Self-Correcting process (STSC).A spatiotemporal Self-Correcting process assumes that the background intensity always increases between the event arrivals. Each event discretely reduces the intensity in the vicinity. The STSC is often used for modeling events with regular intervals, such as animal feeding times. It is characterized by:

\[^{*}(,t)=g_{0}() t-_{i:t_{i }<t} g_{2}(,_{i}):,,>0\] (6)

\(g_{0}()\) again represents the density of the background event distribution. \(g_{2}(,_{i})\) represents the density of the negative event influence centered at \(_{i}\).

See the Appendix B for the simulation parameters of the six synthetic datasets.

Real-world Datasets.We follow the experiment design of Chen et al. (2020) and use two of the real-world datasets, _Earthquake Japan_ and _COVID New Jersey_. The first dataset includes information on the times and locations of all earthquakes in Japan between 1990 and 2020, with magnitudes of at least 2.5. This dataset includes 1050 sequences over a \([0,30)\) time range and is split with a ratio of \(950:50:50\). The second dataset is published by The New York Times and describes COVID-19 cases in New Jersey at the county level. This dataset includes 1650 sequences over a \([0,7)\) time range and is split with a ratio of \(1450:100:100\).

Evaluation Metrics.For real-world datasets, we report the average test log-likelihood (LL) of events. For synthetic datasets, the ground truth intensities are available, so we report the test log-likelihood

    &  &  \\   &  &  &  &  &  &  \\   & LL & HD & LL & HD & LL & HD & LL & HD & LL & HD & LL & HD \\ NSTPP & - & - & - & - & - & - & - & - & - & - & - & - \\ DSTPP & - & - & - & - & - & - & - & - & - & - & - \\ Monte Carlo STPP & - & - & - & - & - & - & - & - & - & - & - \\ AutoSTPP & - & - & - & - & - & - & - & - & - & - & - \\   

Table 1: Test log likelihood (LL) and Hellinger distance of distribution (HD) on synthetic data (LL higher is better, HD lower is better). Comparison between AutoSTPP, NSTPP, Monte Carlo STPP, on synthetic datasets from two types of spatiotemporal point processes.

(LL) and the time-average Hellinger distance between the learned conditional spatial distribution \(f^{*}(|t)\) and the ground truth distribution. The distributions are estimated as multinomial distributions \(P=\{p_{i},...,p_{k}\}\) and \(Q=\{q_{i},...,q_{k}\}\) at \(k\) discretized grid points. The Hellinger distance is then calculated as \(H(P,Q)=}^{k}(}-} )^{2}}\)

**Baselines.** We compare with two state-of-the-art neural STPP models, NSTPP (Chen et al., 2020) and Deep-STPP (Zhou et al., 2022). We also design another baseline, Monte Carlo STPP, which uses the same underlying model as AutoSTPP but applies numerical integration instead of AutoInt to calculate the loss. For a fair comparison, Monte Carlo STPP models the influence \(f^{*}_{}(s,t)\) as a multi-layer perceptron instead of a derivative network. This numerical baseline aims to demonstrate the benefit of automatic integration.

### Results and Discussion

Exact Likelihood.Not all baseline methods have exact likelihood available. NSTPP (Chen et al., 2020) uses a numerical Neural-ODE solver. DeepSTPP (Zhou et al., 2022) introduces a VAE that optimizes a lower bound of likelihood. Monte Carlo STPP estimates the triple integral of intensity by Monte Carlo integration. As such, the results presented in Table 1 are estimated for these baseline models, whereas for our model, the results reflect the true likelihood.

Advantage of AutoInt.We visualize and compare two sample sets of intensities from STH Dataset 1 and STSC Dataset 3 in Figure 3. While the influence function approximator in Monte Carlo STPP can theoretically approximate more functions than Auto-STPP, the intensity it learns is "flatter" than the intensity learned by Auto-STPP. This flatness indicates a lack of information regarding future event locations.

The primary reason behind this flatness can be traced back to numerical errors inherent in the Monte Carlo method. The Monte Carlo integration performs poorly for sharply localized integrants because its samples are homogeneous over the phase space. The intensity of ST-Hawkes is a localized function; it is close to zero in most of the places but high near some event locations. As a result, Monte Carlo STPP can hardly recover the sharpness of the ground truth intensity. NSTPP also uses Monte Carlo integration and suffers the same drawback on STH Dataset 1. In contrast, AutoSTPP evaluates the integral with closed-form integration and alleviates this issue.

Synthetic Datasets Results.Table 1 compares the test LL and the Hellinger distance between AutoSTPP and the baseline models on the six synthetic datasets. For the STSC datasets, we can see that AutoSTPP accurately recovers the intensity functions compared to other models. In Figure 3, AutoSTPP is the only model whose peak intensity location is always the same as the ground truth. DeepSTPP does not perform well in learning the dynamics of the STSC dataset; it struggles to align the peak and tends to learn the flat intensity as overly sharp. The peak intensity location of NSTPP is also biased.

For the STHP datasets, DeepSTPP has a clear advantage because it uses Gaussian kernels to approximate the Gaussian ground truth. In Table 1, AutoSTPP outperforms all other models except DeepSTPP, and its performance is comparable. Figure 3 shows that Monte Carlo STPP and NSTPP

   LL & COVID-19 NY & Earthquake JP \\  NSTPP & \(2.5566_{+0.0447}\) & \(-4.4949_{+0.1172}\) \\ DSTPP & \(2.3433_{+0.0109}\) & \(-3.9852_{+0.0129}\) \\ MonteSTPP & \(2.1070_{+0.0342}\) & \(-3.6085_{+0.0436}\) \\ AutoSTPP & \(_{+0.5905}\) & **-3.5948\({}_{+0.0025}\)** \\   

Table 2: Test log likelihood (LL) comparison for space and time on real-world benchmark data, mean and standard deviation over three runs.

Figure 4: Forward mixed (\(d/dx_{1}dx_{2} dx_{k}\)) partial derivative computation average speed comparison between our efficient implementation and PyTorch naive AutoGrad, for a two-layer MLP.

can only learn an unimodal function, whereas AutoSTPP can capture multi-modal behavior in the ground truth, especially the small bumps near the mode.

Real-world Datasets Results.Table 2 compares the test LL of AutoSTPP against the baseline models on the earthquakes and COVID datasets. Our model demonstrates superior performance, outperforming all the state-of-the-art methods. One should note that while Monte Carlo STPP shows performance comparable to ours on the Earthquake JP dataset, it falls short when applied to the COVID-19 NY dataset. We attribute this discrepancy to the large low-population-density areas in the COVID-19 NY data, which causes higher numerical error in integration.

### Computational Efficiency

Figure 4 visualizes the benefit of using our implementation of AutoInt instead of the PyTorch naive implementation. More visualizations of the forward and backward computation times can be found in Appendix A. We can see that our implementation can be extended to compute any order of partial derivative. It is significantly faster than the naive autograd. In our AutoSTPP, we calculate the intensity using the product of three first-order derivatives. Our implementation would lead to a speedup of up to 68% for computing each first-order derivative.

## 5 Conclusion

We propose Automatic Integration for neural spatiotemporal point process models (AutoSTPP) using a dual network approach. AutoSTPP can efficiently compute the exact likelihood of _any_ sophisticated intensity.

We validate the effectiveness of our method using synthetic data and real-world datasets and demonstrate that it significantly outperforms other point process models with numerical integration when the ground truth intensity function is localized.

However, like any approach, ours is not without its limitations. While AutoSTPP excels in computing the likelihood, sampling from AutoSTPP is computationally expensive as we only have the expression of the probability density. Closed-form computation of expectations is also not possible; Knowing the form of \((t)\), calculating \( t(t)\) is still intractable.

Our work presents a new paradigm for learning continuous-time dynamics. Currently, our neural process model takes the form of Hawkes processes (self-exciting) but cannot handle the discrete decreases of intensity after events due to the difficulty of integration. Future work includes relaxing the form of the intensity network with advanced integration techniques. Another interesting direction is to increase the approximation ability of the product network.