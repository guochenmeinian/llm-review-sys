# Stronger Universal and Transfer Attacks by

Suppressing Refusals

 David Huang

University of California, Berkeley

&Avidan Shah

University of California, Berkeley

Alexandre Araujo

New York University

&David Wagner

University of California, Berkeley

&Chawin Sitawarin

University of California, Berkeley

&David Wagner

University of California, Berkeley

&Chawin Sitawarin

University of California, Berkeley

&

###### Abstract

Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF) in order to prevent malicious uses, essentially embedding a "safety feature" into the model's parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that these optimization-based attacks (unlike hand-crafted ones) are _sample-specific_. To make the automated jailbreak universal and transferable, they require incorporating multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial prompts discovered by such optimizers are inherently _prompt-universal and transferable_, even when optimized on a _single_ model and a _single_ harmful request. To further amplify this phenomenon, we introduce IRIS, a new objective to these optimizers to explicitly deactivate the safety feature to create an even stronger universal and transferable attack. Without requiring a large number of queries or accessing output token probabilities, our transfer attack, optimized on Llama-3, achieves a 25% success rate against the state-of-the-art Circuit Breaker defense (Zou et al., 2024), compared to 2.5% by white-box GCG. Crucially, our universal attack method also attains state-of-the-art test-set transfer rates on frontier models: GPT-3.5-Turbo (90%), GPT-4o-mini (86%), GPT-4o (76%), o1-mini (54%), and o1-preview (48%).

## 1 Introduction

Jailbreaking techniques and adversarial attacks have become ubiquitous tools for assessing the safety of highly capable large language models (LLMs), a process called _red-teaming_(Perez et al., 2022; Llama Team, 2024; OpenAI, 2023; Reid et al., 2024). These methods are essential for uncovering vulnerabilities and ensuring that LLMs adhere to safety protocols.

Existing adversarial attack methodologies can be broadly categorized into two approaches: fully automated adversarial attacks and hybrid jailbreaking techniques. First, fully automated attacks techniques, similar to Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023) and Zhu et al. (2023), utilize gradient-based optimization to generate adversarial prompts that bypass safety alignments embedded within LLM parameters. These methods operate without human intervention, allowing for scalable and rapid generation of jailbreak attempts. Second, hybrid jailbreaking techniques, such as AutoDAN-Liu (Liu et al., 2023), combine automated optimization in the form of hierarchical genetic algorithms with handcrafted prompts to initiate and enhance jailbreak attempts.

These hybrid methods leverage the automated process to converge on better yet still semantically meaningful attack prompts, resulting in improved attack effectiveness.

While both approaches have demonstrated varying degrees of success, they often grapple with inherent limitations. It is commonly believed that optimization-based attacks are more _sample-specific_, meaning that obtaining a prompt-universal and transferable attack requires not only diverse samples but also multiple source models to reach an effective attack success rate (ASR). Furthermore, as we show in this paper, many attacks, automated and hybrid alike, perform inconsistently on the latest robustly aligned frontier models. Due to such limitations, these attacks are often branded as either "weak" or "unrealistic" and overlooked in security evaluation of proprietary systems.

In this work, we propose _Inhibiting Refusals for Improved Universal and Transferable Jailbreak Suffix_ (IRIS) - an optimizer-agnostic objective that can be combined with existing adversarial prompt optimizers (e.g., GCG and AutoDAN-Liu) to directly target the safety mechanism in LLM's intermediate representation. Specifically, we build on the observation made by Arditi et al. (2024) that safety-aligned LLMs use a specific set of hidden activations to represent a "refusal direction." IRIS' objective works by minimizing the activations in this refusal direction.

Our experiments show that IRIS generates naturally strong universal and transferable adversarial suffixes even when we optimize with only single harmful requests and a single model. To create an even stronger jailbreak, we combine IRIS with our second surprising discovery that the "best universal" suffix often outperforms an individually optimized harmful behavior-specific suffix (Fig. 3). Our findings raise critical concerns for real-world LLM deployment as the frontier models are vulnerable to our attack: with a single universal suffix, IRIS achieves jailbreak success rates of GPT-3.5-Turbo (88%), GPT-4o-mini (73%), and o1-mini (43%) on HarMBench. without needing (i) model-specific fine-tuning, (ii) costly queries over mutable steps, or (iii) output token probabilities. When we allow the adversary 50 suffixes instead of one, the ASRs go up to 100%, 85%, and 71%, respectively. These results challenge the viability of current alignment strategies and underscore the need for stronger defenses against increasingly sophisticated adversarial attacks. To summarize, our contributions are threefold:

* We demonstrate that generated adversarial suffixes optimized over single open-source models and single harmful requests are inherently prompt-universal and transferable, achieving high jailbreak success across various LLMs, both open-source and proprietary.
* We introduce IRIS, a novel attack that targets the refusal mechanisms of LLMs, significantly improving jailbreak success on Llama-3 in the white-box setting as well as all the proprietary frontier models using the universal and transferable suffix.
* Finally, we demonstrate the efficacy of our approach, showing strong transferability to the state-of-the-art defenses such as the representation rerouting techniques (RR) (Zou et al., 2024), achieving 74% success on Llama-3-RR and 90% on Mistral-RR on AdvBench. We also evaluate an OOD transfer on the HarmBench dataset to Llama-3-RR, obtaining a 25% universal ASR. Similarly, when the constraint is relaxed to utilize all 50 adversarial suffixes, the ASR increases to 65%..

Figure 1: **Our IRIS attack** minimizes dot product between LLM’s input embeddings and pre-computed activations of a refusal response (Arditi et al., 2024). This objective can be combined with both GCG and AutoDAN-Liu to substantially increase the transferability and the universality of adversarial suffixes.

## 2 Related Work

**LLM jailbreak.** A "jailbreak" refers to techniques used to bypass the safety mechanisms in LLMs that generally prevent the generation of harmful, unethical, or restricted content. Earlier jailbreak methods are manually crafted to exploit the instruction-following capabilities of LLMs, often relying on various persuasion techniques (Wei et al., 2023; Zeng et al., 2024), role-playing (Entire_Comparison783, 2023; Wei et al., 2023; Shen et al., 2024), low-resource languages (Yong et al., 2023; Deng et al., 2024), etc. Since these jailbreaks are hand-crafted and require some expertise in prompt engineering, subsequent works focus on _automated_ jailbreaks as an efficient way to evaluate safety of LLMs (often called "red-teaming"). Similar to adversarial examples (Biggio et al., 2013; Szegedy et al., 2014), automated jailbreaks are often formulated and iteratively solved as an optimization problem (Deng et al., 2022; Shi et al., 2022; Maus et al., 2023; Jones et al., 2023; Chao et al., 2023; Liu et al., 2023; Zhu et al., 2023; Lapid et al., 2023; Ge et al., 2023; Deng et al., 2023; Mehrotra et al., 2023; Yu et al., 2024; Guo et al., 2024; Paulus et al., 2024; Andriushchenko et al., 2024; Thompson and Sklar, 2024).

Drawing from adversarial robustness literature, we first introduce the two types of practical attacks we focus on in this work:

1. _Transfer attack_: In the image domain, adversarial examples are known to "transfer" to another model that they are not directly optimized on (Papernot et al., 2016; Tramer et al., 2017). Transfer attacks can be used to target black-box proprietary models where the attacker has no access to their parameters or architecture.
2. _Universal attack_: Instead of transferring to an unseen model, an adversarial perturbation generated for one sample can also be effective on multiple unseen ones (Moosavi-Dezfooli et al., 2017). This attack enables a large-scale attack for various inputs at once.

These two types of attacks make little assumption on the attacker's knowledge and are efficient at scale, making them particularly concerning to the industry.

**Transferable and universal jailbreaks.** In the context of LLMs, several prior attacks also focus on these practical scenarios. Following the prior works on image adversarial examples, Wallace et al. (2019) create universal adversarial texts by summing the objective functions over multiple samples. More recently, GCG attack (Zou et al., 2023b), one of the most popular automated jailbreaks today, propose a greedy discrete optimizer to search for an adversarial suffix. They create jailbreaks that are both transferable and universal using a similar principle. However, Meade et al. (2024) disprove that GCG attack cannot transfer to a broad range of target models. Shah et al. (2023) propose transferable jailbreaks that start by making the surrogate model more similar to the target model before starting the optimization. Conversely, Sitawarin et al. (2024) leverage the transferability phenomenon to create stronger query-only attacks.

Figure 2: With IRIS’s best universal suffix, we show significant jailbreak improvement from other automated attack methods for transfer attacks onto GPT-4o, notably a **70%** increase from the next best attack. All transfer attacks were conducted using Llama-3 as the source model, and our single-prompt IRIS Best Universal attack was evaluated on an unseen test set of 50 AdvBench behaviors, where it’s original training universal ASR was **82%**.

**LLM safety features.** Recent works start to demonstrate that LLMs rely on some activation patterns to detect and refuse to respond to harmful prompts (Subhash et al., 2023; Zou et al., 2023; Xu et al., 2024; Arditi et al., 2024). More broadly, many researchers attempt to gain better understanding of LLMs via interpretability techniques. Among the most popular is the sparse autoencoder (SAE), a dense neural network trained in an unsupervised manner to disentangle multiple "concepts" being represented by the activations of LLMs (Bricken et al., 2023; Templeton et al., 2024). One of many concepts that SAEs discover is also related to safety and harmfulness of the prompts. Our work focuses on these safety features and how they may be leveraged to create stronger jailbreaks. We will detail all three approaches later in Section 4.

## 3 Universal Suffix From Single Behavior

Our first result can be summarized as follows:

**Result 1:** An adversarial suffix optimized for a _single_ behavior is a surprisingly effective _universal and transferable_ jailbreak.

**Experiment setup.** For each of the 50 harmful behaviors in AdvBench(Zou et al., 2023b), we generate an adversarial suffix using three attack algorithms: GCG (Zou et al., 2023b), AutoDAN-Liu (Liu et al., 2023), and SCAV (Xu et al., 2024). We then evaluate each suffix on _all the 50 behaviors_ (universality) including the one it is directly optimized for and on _all the five target models_ (transferability) including the one it is optimized on. We use Meta Llama Guard 2 8B (Llama Team, 2024) as a judge to evaluate the attack success rate (ASR). Full details are provided in Appendix A.

**Best universal suffix.** For a given pair of source and target models, we define the _best universal_ suffix the suffix (out of 50) that achieves the highest universal ASR when appended to all 50 behaviors. Fig. 3 illustrates this concept. We will compare the best universal suffix to (1) the usual baseline where each suffix is only appended to the behavior it is optimized for and (2) the "average" universal attack assuming the attacker picks a universal suffix uniformly at random.

**Result 1.1: Universal white-box attack.** First, we focus on the universality aspect, i.e., how many behaviors one adversarial suffix can jailbreak (no transfer; source and target models are the same model). Fig. 4 shows that the best universal suffix from GCG has a much higher ASR than the baseline and than the average universal suffix. Particularly, the baseline white-box GCG attack achieves 34% and 2% on Llama-2 and Llama-3, respectively. However, by simply choosing the best

Figure 4: **ASR under individual behavior, average universal, and best universal.** Sufixes generated from some harmful behaviors are inherently a strong universal attack.

Figure 3: **An illustration of how we select the “best universal” adversarial suffix.** We find that some adversarial suffixes optimized for a single harmful behavior are a surprisingly effective universal and transferable jailbreak.

universal suffix, the attacker can increase the ASR to 88% and 18%, respectively. Results on the other models are in Table 6. This surprising observation suggests a simple yet potentially more effective method for generating a universal adversarial suffix (i.e., directly generate multiple and pick the best) unlike the prior method (Zou et al., 2023b) which optimizes over multiple behaviors.

**Result 1.2: Universal transfer attack.** In the transfer setting, the improvements from the best universal suffix are even more pronounced. The best transfer rate on Mistral-v0.2 jumps from 22% to 82%, Mistral-v0.1 from 54% to 92%, and Vicuna-v1.5 increases from 50% to 94% (Table 6). While transferability improved for Llama-2 and Llama-3, it remained relatively low overall.

We believe the results in this section are unintuitive, especially considering prior adversarial example literature. To the extent of our knowledge, this phenomenon has never been documented. The phenomenon is not exclusive to GCG but also applies to AutoDAN-Zhu and AutoDAN-Liu both of which shares the same objective function as GCG (Table 14). We hypothesize two underlying reasons for this observation:

1. **The adversarial suffixes found by these algorithms are far from optimal and are subject to high variance.** Clearly, if the generated adversarial suffix were to be the optimal solution for a given harmful behavior, this outcome would have been extremely unlikely in the white-box setting. We verify this by restarting GCG with multiple random seeds and observe that the universal ASR of each suffix varies significantly (Appendix G.1).
2. **The choice of the source behavior (i.e., the behavior chosen for optimization) affects the potency of the adversarial suffix.** Even after accounting for the high variance, we observe a statistically significant difference between ASRs of the top-five and the bottom-five source behaviors over 10 GCG runs with different random seeds (Fig. 13).

## 4 Deactivating Safety Features

Our initial finding, that some adversarial suffixes function as strong universal jailbreaks, suggests these suffixes may deactivate the general safety mechanisms of aligned LLMs (described in Section 2) rather than merely inducing a specific output. In this section, we propose an attack that explicitly exploits this phenomenon by integrating the safety mechanism into its objective. The question we explore is whether it is possible to create a universal and transferable suffix _without_ the need to optimize across multiple behaviors or models. We find an affirmative answer to this question:

**Result 2:** By suppressing LLM's safety feature directly in the optimization objective, we create highly effective universal and transferable adversarial suffixes against both the state-of-the-art robustly aligned models (Zou et al., 2024) and proprietary models.

### Iris

Our primary contribution is IRIS, an algorithm-agnostic enhancement to automated jailbreak attacks. It aims to optimize the adversarial suffix by measuring the presence of a refusal vector during the model's forward pass when handling harmful requests.

**Refusal vector.**Arditi et al. (2024) define a "refusal vector" as a direction in an aligned LLM's intermediate activations, denoted by \(}^{d}\), that dictate whether the model will refuse to respond to

    &  &  \\   & Llama-3 & Llama-2 & Vicuna-v1.5 & Mistral-v0.1 & Mistral-v0.2 \\  GCG & 2 & 0 & 12 & 44 & 8 \\ AutoDAN-Liu & 2 & 0 & 14 & 4 & 12 \\ SCAV & 4 & 2 & 30 & 12 & 16 \\ IRIS + GCG (ours) & **50** & **8** & **82** & **90** & **90** \\   

Table 1: **ASR of IRIS vs the baselines on open-source models (non-universal).** The existing attacks struggle to jailbreak Llama-3 (best white-box ASR of 4%). IRIS is the strongest white-box attack and transfer attack.

a given request. If a prompt induces a large component in this direction (i.e., has a large dot product with \(}\)), the model will likely refuse. We compute the refusal vector by finding the direction from the mean of benign prompts to the mean of harmful prompts as described in Arditi et al. (2024).

**IRIS objective.** There are two terms in IRIS objective. The first one is to maximize the probability of a specific target affirmative response. However, instead of choosing a template response like "Sure, here is...", we use the actual output of the target model when the refusal vector is suppressed by directly editing the model's activations, the same procedure proposed by Arditi et al. (2024)). The second term is to penalize the dot product between the pre-computed refusal vector \(}\) and embeddings of the last input token on every layer and every residual activation. The overall objective can be written as

\[_{}()=-(1-) p_{}()+_{_{}()}(}^{})^{2}.\] (1)

where \(\) is the input prompt, \(\) the target response, and \(^{d}\) an embedding vector from the set of all embeddings across layers and residual streams \(_{}()\). Here, \(\) is a regularization parameter that controls the trade-off between the target response's probability and the embedding loss.

Lastly, we also experiment with a sparse autoencoder (Bricken et al., 2023; Templeton et al., 2024) as an alternative for identifying the safety neurons. However, the empirical result is consistently weaker so we leave this version of the attack (called IRIS-NO) to Appendix C.2.

## 5 Experiments

This section outlines our key findings of IRIS on both open-source (Section 5.1) and proprietary models (Section 5.2). Unless stated otherwise, the experiment setup is identical to Section 3.

### Open-Source Target Models

We first experiment with different configurations of IRIS on the small open-source models. There are three main design choices we consider:

1. **Layer of the embeddings used to calculate the refusal vector:** We first identify two refusal vectors from two different layers that lead the highest jailbreak rate on Llama-3 when directly editing the embeddings as in Arditi et al. (2024). Then, we use those vectors with IRIS and find that layer 10 leads to the best adversarial suffix.
2. **Beta tuning:** The parameter \(\) from Eq. (1) has moderate impact. We experiment with \(\{0,0.25,0.5,0.75,1\}\) and find that 0.75 yields the best attack. The white-box (non-universal) goes from \(50\%\), with default \(=0.5\), to \(56\%\).
3. **Custom target responses:** As mentioned in Section 4.1, we use the jailbroken model's output (via direct embedding editing) as the target response, instead of an arbitrary boilerplate. Using "Sure, here is..." leads to slightly weaker IRIS suffixes. The white-box non-universal increases by over \(6\%\).

First, we note that the existing automated jailbreak attacks struggle against Llama-3 (best white-box ASR of 4%) despite succeeding consistently on the other open-source models (Table 1). Additionally, all transfers to Llama-2 fail. However, **the best configuration of IRIS as described above achieves much better ASRs across all settings.** Notably, the white-box ASR on Llama-3 is over 50%, and the transfer ASR is above 80% for all the target models, except for Llama-2.

### Frontier Target Models

We further evaluate the suffixes we obtained from the open-source models (Llama-3, specifically) in the prior section on frontier models and state-of-the-art robust models. To emphasize, we do not generate any new suffixes here, and none of the suffixes is optimized on the frontier models in any way. Here, we consider strictly consider only the black-box transfer setting and two evaluation protocols representing different adversary's budgets:

1. **Universal:** Apply _only the best_ suffix obtained from our AdvBench training set (the same 50 samples used in Chao et al. (2023)) directly to an unseen set of 50 randomly chosen AdvBench

[MISSING_PAGE_FAIL:7]

**OpenAI's o1 models.** The recent o1 models from OpenAI (OpenAI, 2024) are one of the most advanced LLMs to date which are also trained differently to the existing ones with a particular focus on reinforcement learning and chain-of-thought prompting (Wei et al., 2022; Lightman et al., 2023). We choose to test our IRIS suffixes on the o1 models because they are claimed to be robust to jailbreaks in the official model card but still lack external evaluation from the research community. Table 4 suggests that both o1-mini and o1-preview are slightly more robust to jailbreaks than some other GPT models, but they are still easily jailbroken by IRIS.

### HarmBench Results on Llama-3-RR

We further benchmark the IRIS attack on the HarmBench dataset (Mazeika et al., 2024), using the default HarmBench judge instead of LlamaGuard, on Llama-3-RR. HarmBench consists of 200 harmful behaviors, distinct from AdvBench where the IRIS suffixes are generated on. This result can be interpreted as an "out-of-distribution" universal attack where the adversary does not even have access to the distribution of the target prompts but only a proxy. Even under this strict threat model, the IRIS attack already achieves 25% universal ASR and 65% best-of-N ASR, compared 2.5% ASR by GCG and 9.6% ASR by a soft prompt attack, both of which assume much more powerful white-box adversary.

## 6 Discussion

**Evaluation method.** Evaluations by LLM judges are lower bounds for IRIS's true attack potency, as they struggle to accurately assess responses, especially those in foreign languages or code, leading to a 2-3\(\) increase in false negative rates as compared to evaluations on GCG, AutoDAN-Zhu, and AutoDAN-Liu attacks. For instance, we observe that jailbreaks are not flagged when written in foreign languages on Llama-3, however, IRIS will occasionally optimize suffixes that induce such responses. For a broader discussion on metrics and different evaluation techniques, refer to Appendix H as well as Appendix A. This underscores limitations in automatic evaluations, which may underestimate jailbreaking rates.

**Fully automatic vs manual jailbreak.** We would like to be clear that we utilize both fully automatic (GCG) and partially automatic (AutoDAN) jailbreaking methods, though IRIS improves on both indiscriminately. The IRIS variant that utilizes GCG is less effective but is fully automatic and achieves significant ASR on open-source models, while the IRIS AutoDAN variant boasts state-of-the-art ASR on frontier cutting-edge models but requires a hand-crafted initialization similar to SCAV, which is also based on AutoDAN.

## 7 Conclusion

Our findings open up several exciting avenues for future research. We demonstrate that highly effective universal attacks can be achieved without relying on a sample-specific, data-driven training formulation that requires extensive optimization across multiple harmful requests and varying model architectures, challenging common practices. Using the randomly initialized IRIS, we consistently outperformed several automated prompt-level attack baseline algorithms in both frontier model transfers and most open-source white-box and transfer model settings. While we observed that the effectiveness of the universal phenomenon scales with the strength of the underlying attack for single-harmful behavior transfers, it also exhibits high variance. Isolating the inherent variance of the universal phenomenon, irrespective of the specific harmful behavior optimized, could deepen our understanding of this phenomenon and inform alignment strategies. Moreover, our findings suggest that current jailbreaking objectives are significantly suboptimal and leave room for improvement.

Two key extensions of this work involve integrating newer interpretable objectives, such as Sparse Autoencoders (SAEs), to better understand the universal phenomenon and uncover persistent vulnerabilities in Large Language Models, and broadening the scope of universal phenomena to include multiple open-source models rather than focusing on single-model optimizations. This expanded approach could reveal shared vulnerabilities across architectures and foster the development of stronger, more robust defenses. These directions highlight the potential for advancing both theoretical insights and practical safeguards against malicious exploitation of language models.

#### Acknowledgment

This research was supported by the National Science Foundation under grants 2229876 (the ACTION center) and 2154873, OpenAI, C3.ai DTI, the KACST-UCB Joint Center on Cybersecurity, the Center for AI Safety Compute Cluster, Open Philanthropy, Google, the Department of Homeland Security, IBM, and the Noyce Foundation. A. Araujo is supported in part by the Army Research Office under grant number W911NF-21-1-0155 and by the New York University Abu Dhabi (NYUAD) Center for Artificial Intelligence and Robotics, funded by Tamkeen under the NYUAD Research Institute Award CG010.