# 3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors +

3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors +
Footnote â€ : _Corresponding author: Siyu Huang_

Xi Liu* &Chaoyi Zhou* &Siyu Huang

Visual Computing Division

School of Computing

Clemson University

{xi9, chaoyiz, siyuh}@clemson.edu

###### Abstract

Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project.

Figure 1: The 3DGS-Enhancer improves 3D Gaussian splatting representations on unbounded scenes with sparse input views.

Introduction

Novel-view synthesis (NVS) has decades of history in computer vision and graphics communities, aiming to generate views of a scene from multiple input images or videos. Recently, 3D Gaussian splatting (3DGS)  has excelled in producing photorealistic renderings with a highly efficient rendering pipeline. However, rendering high-quality novel views far from existing viewpoints remains very challenging, as often encountered in sparse-view settings, due to insufficient information in under-sampled areas. As shown in Figure 1, noticeable ellipsoid-like and hollow artifacts manifest when there are only three input views. Due to these common low-quality rendering results in practice, it is essential to enhance 3DGS to ensure its viability for real-world applications.

To our knowledge, few prior studies have specifically focused on enhancement methods aimed at improving the rendering quality of NVS. Most existing enhancement work for NVS [19; 44] focuses on incorporating additional geometric constraints such as depth and normal into the 3D reconstruction process to fulfill the gap between the observed and unobserved regions. For example, DNGaussian  applies a hard-and-soft depth regularization to the geometry of radiance fields. However, these methods heavily rely on the effectiveness of additional constraint and are often sensitive to noises. Another line of work leverages generative priors to regularize the NVS pipeline. For instance, ReconFusion  enhances Neural Radiance Fields (NeRFs)  by synthesising the geometry and texture for the unobserved regions. Although it can generate photo-realistic novel views, the view consistency is still challenging when the generated views are far away from the input ones.

In this work, we exploit the 2D generative priors, _e.g._, the latent diffusion models (LDMs) , for 3DGS representation enhancement. LDM has demonstrated powerful and robust generation capabilities in various image generation  and restoration tasks . Nevertheless, the main challenge lies in the poor 3D view consistency among generated 2D images, which significantly hinders the 3DGS training process that requires highly precise view consistency. Although some efforts have been made, such as the Score Distillation Sampling (SDS) loss  that distills the optimization objective of a pre-trained diffusion model, it fails to generate the 3D representation allowing rendering high-fidelity images

Motivated by the analogy of the visual consistency between multi-view images and the temporal consistency between video frames, we propose to reformulate the challenging 3D consistency problem as an easier task of achieving temporal consistency within video generation, so we can leverage the powerful video diffusion models for restoring high-quality and view-consistent images. We propose a novel 3DGS enhancement pipeline, dubbed 3DGS-Enhancer. The core of 3DGS-Enhancer is a video LDM consisting of an image encoder that encodes latent features of rendered views, a video-based diffusion model that restores temporally consistent latent features, and a spatial-temporal decoder that effectively integrates the high-quality information in original rendered images with the restored latent features. The initial 3DGS model will be finetuned by these enhanced views to improve its rendering performance. The proposed 3DGS-Enhancer can be trajectory-free to reconstruct the unbound scenes from sparse views and generate the natural 3D representation for the invisible area between two known views. A cocurrent work V3D  also leverages latent video diffusion models  for generating object-level 3DGS models from single images. In contrast, our 3DGS-Enhancer focuses on enhancing any existing 3DGS models and thus can be applied to more generalized scenes, _e.g._, the unbounded outdoor scenes.

In experiments, we generate large-scale datasets with pairs of low-quality and high-quality images on hundreds of unbounded scenes, based on DL3DV , for comprehensively evaluating the novelly investigated 3DGS enhancement problem. Empirical results demonstrate that the proposed 3DGS-Enhancer method achieves superior reconstruction performance on various challenging scenes, yielding more distinct and vivid rendering results. The code and the generated dataset will be publicly available. The contributions of this paper are summarized as follows.

1. To the best of our knowledge, this is the first work to tackle the problem of enhancing low-quality 3DGS rendering results, an issue that widely exists in practical 3DGS applications.
2. We propose a novel pipeline 3DGS-Enhancer that addresses the 3DGS enhancement problem. 3DGS-Enhancer reformulates the 3D-consistent image restoration task as temporally consistent video generation, such that powerful video LDMs can be leveraged for generating both high-quality and 3D-consistent images. Novel 3DGS fine-tuning strategies are also devised for an effective integration of the enhanced views with the original 3DGS representation.
3. We conduct extensive experiments on large-scale datasets of unbounded scenes to demonstrate the effectiveness of the proposed methods over existing state-of-the-art few-shot NVS methods.

## 2 Related Work

**Radiance fields for novel view synthesis.** Novel view synthesis (NVS) aims to generate unseen viewpoints from a set of input images and camera information. Radiance fields methods, like NeRFs , encode 3D scenes as radiance fields and use volume rendering for novel views, achieving high-fidelity results but at the cost of lengthy training and inference times. Improvements such as Mip-NeRF [1; 2] enhance rendering quality through anti-aliasing, while others [6; 9; 47; 27] focus on speeding up the processes. Recently, 3D Gaussian splatting (3DGS)  has emerged, offering competitive rendering quality and significantly higher efficiency by representing scenes as 3D Gaussian spheres and using a fast differentiable splatting pipeline . However, 3DGS still requires high-quality and numerous input views for optimal reconstruction, which is often impractical.

**Few-shot novel view synthesis.** Leveraging additional information is essential for generating novel views from sparse input images. Various approaches incorporate different regularization techniques to prevent 3D geometry from overfitting to the training views. [19; 10; 28; 23] introduce extra geometric information, such as depth maps or coarse mesh, to enhance the robustness and performance of 3D reconstruction from sparse views. [5; 8] leverage the learned priors from multi-view stereo datasets as general priors to improve performance in sparse view reconstruction tasks. FreeNeRF  integrates frequency and occlusion regularization during training to mitigate overfitting issues in few-shot neural rendering. Similarly, DietPixelNeRF  employs a semantic view consistency loss to ensure that all views share consistent semantics, thereby alleviating overfitting. However, these methods are highly sensitive to the network's performance, where incorrect depth estimations or inaccurate mesh reconstructions can significantly degrade the final output.

**Diffusion priors for novel view synthesis.** Recently, utilizing diffusion models as priors for few-shot novel view synthesis has proven to be an effective approach. DreamFusion  employs Score Distillation Sampling (SDS) with a pre-trained diffusion model to guide 3D object generation from text prompts [36; 33; 46]. Some works [22; 34; 35] embed 3D awareness into 2D diffusion models to generate multi-view images, though these methods typically require large datasets  and significant training resources [16; 28]. ReconFusion leavage the 2D diffusion priors to recover a high-fidelity NeRF from sparse input views. More advanced approaches leverage video diffusion models [4; 12; 13; 24] for few-shot NVS. For instance, AnimateDiff  fine-tunes diffusion models with additional camera motions using LoRA , while methods like SVD-MV , V3D  and IM-3D  propose camera-controlled video diffusion models for object-level 3D generation. In contrast, our approach offers greater generalizability for unbounded outdoor scenes.

**Radiance fields enhancement.** Several existing studies focus on enhancing NeRFs by addressing the limited detail preservation issue caused by insufficient or low-quality input data. NeRF-SR  and Refsr-nerf  use a super-resolution network to upscale the training view images, allowing novel views to be synthesized at higher resolutions with appropriate details. Alignerf  introduce optical-flow network to solve the misalignment problem to enhance the performance. Some other approaches incorporate 2D diffusion priors into 3D reconstructions. For instance, DiffusionNeRF  leverages a diffusion model to learn gradients of logarithms of RGBD patch priors, serving as regularized geometry and color for a scene. Nerfbusters  use diffusion priors to remove ghostly artifacts in the 3D gaussians. Our work aim to addresses the radiance fields enhancement problem by proposing a novel framework 3DGS-Enhancer, achieving superior enhancement performance for low-quality unbounded 3DGS representations.

## 3 Preliminary of 3D Gaussian Splatting

Here, we briefly review the formulation and rendering process of 3DGS . 3DGS represents a scene as a set of anisotropic 3D Gaussian spheres, allowing high-fidelity NVS with extremely low rendering latency. A 3D Gaussian sphere includes a center position \(^{3}\), a scaling factor \(^{3}\)and a rotation quaternion \(^{4}\), such that the Gaussian distribution is

\[G(x)=e^{-(x-)^{T}^{-1}(x-)},\] (1)

where \(=RSS^{T}R^{T}\), \(S\) is the scaling matrix determined by \(\) and \(R\) is the rotation matrix determined by \(\). To additionally model the view-dependent appearance, the Gaussian sphere also includes spherical harmonics (SH) coefficients \(^{k}\), where \(\) is the number of SH functions, and an \(\) for opacity. The color and opacity are also calculated by the Gaussian distribution illustrated in Eq. 1.

For rendering, all the 3D Gaussian spheres are projected onto the 2D camera planes via a differentiable Gaussian splatting pipeline . Given the viewing transform matrix \(W\) and Jacobian matrix \(J\) of the affine approximation of the projective transformation, the covariance matrix \(^{}\) in camera coordinates is calculated as

\[^{}=JW W^{T}J^{T}.\] (2)

The differentiable splatting method efficiently projects the 3D Gaussian spheres to 2D Gaussian distributions, ensuring fast \(\)-blending for rendering and color supervision. For each pixel, the color is rendered by \(M\) Gaussian spheres that overlap with the pixel on the 2D camera planes, sorted in the depth distance as

\[C=_{i M}_{i}_{i}_{j=1}^{i-1}(1-_{i}).\] (3)

## 4 Method

### 3DGS-Enhancer: An Overview

This work studies the 3DGS enhancement problem. More specifically, given a 3DGS model trained on a scene consisting of input views \(\{I_{1}^{},I_{2}^{},,I_{N_{}}^{}\}\) and corresponding camera poses \(\{_{}^{},_{2}^{},,_{N_{ }}^{}\}\), the goal of this work is to enhance a set of low-quality novel views \(\{I_{1},I_{2},I_{3},,I_{N_{}}\}\) rendered by the 3DGS model. The enhanced images further fine-tune the 3DGS model to improve its reconstruction and rendering quality.

This work novelly reformulates the challenging task of 3D-consistent image restoration as the task of video restoration, in light of the analogy between the multi-view consistency and the video temporal consistency. We propose a novel framework named 3DGS-Enhancer that employs a video LDM comprising an image encoder, a video-based diffusion model, and a spatial-temporal decoder to enhance the rendered images while preserving a high 3D consistency. 3DGS-Enhancer also adopts novel fine-tuning strategies to selectively integrate the views enhanced by the video LDM into the 3DGS fine-tuning process. An illustration of the 3DGS-Enhancer framework is shown in Figure 2. We discuss more details of the framework in the following.

Figure 2: An overview of the proposed 3DGS-Enhancer framework for 3DGS representation enhancement. We learn 2D video diffusion priors on a large-scale novel view synthesis dataset to enhance the novel views rendered from the 3DGS model on a novel scene. Then, the enhanced views and input views jointly fine-tune the 3DGS model.

### Video Diffusion Prior for Temporal Interpolation

In this section, we introduce the video diffusion model for achieving 3D-consistent 2D image restoration. To lift the consistency between the generated 2D video frames and the high-quality reference views, we further propose to formulate the video restoration task as a video interpolation task, where the first frame and the last frame of inputs to the video diffusion model are two reference views. This formulation provides stronger guidance for the video restoration process. Let \(\{_{i=1}^{},_{1}^{s},_{2}^{s},,_{N}^{s}, _{N}^{}\}\) be the camera poses sampled from the trajectory fitted between two reference views, the images rendered accordingly are \(v=\{I_{i-1}^{},I_{1},I_{2},,I_{T},I_{i}^{}\}\). \(v^{(T+2) 3 H W}\) serves as the input to the video diffusion model, _e.g._, a pre-trained image-guided stable video diffusion (SVD) model [4; 20] that adopts cross-frame spatio-temporal attention module and 3D residual convolution in the diffusion U-Net. Unlike SVD, which repeats the single input image feature extracted by CLIP  for \(T\) times as the conditional inputs, we input \(\) to the CLIP encoder to get a sequence of conditional inputs \(_{}\) and add it to the video diffusion model through cross attention. Meanwhile, we input \(\) to the VAE encoder to get latent feature \(_{}\) and add it into the diffusion model through a classifier-free guidance strategy to incorporate richer color information. The diffusion U-Net \(_{}\) predicts the noise \(\) for each diffusion step \(t\), and the training objective is

\[_{}=[\|-_{}(z_ {t},t,_{},_{})\|].\] (4)

where \(z_{t}=_{t}z+_{t}*..\) where z is the gt latent, \((0,I)\), \(_{t}\) and \(_{t}\) define a noise at timestep \(t\). The learned video diffusion model generates a sequence of enhanced image latents \(z_{v}\) corresponding to the rendered low-quality views \(v\).

### Spatial-Temporal Decoder

Although the video diffusion model can generate enhanced image latents \(z_{v}\), we observe that there are artifacts such as temporal inconsistency, blurring, and color shift in outputs of the original decoder of video LDM. To address this issue, we propose a modified spatial-temporal decoder (STD). STD makes the following improvements over the original VAE Decoder: 1) **Temporal decoding manner.** STD adopts additional temporal convolution layers to ensure the temporal consistency between decoded outputs. Similar to our video diffusion model, the first and the last input frames are the reference view images, and the intermediate inputs are the generated views; 2) **Effective integration of rendered views.** STD adopts additional conditional inputs, same as those of the video diffusion model, allowing the decoder to better leverage the original rendered images. Inspired by [45; 48], these conditional inputs are fed into STD through Controllable Feature Warping (CFW) modules , such that their high-frequency patterns are better preserved. 3) **Color correction.** To address the color shift issue, we apply color normalization to the decoded images by following StableSR . However, we observe that highly blurred and low-quality images in the conditional inputs can undermine the color correction effects. To mitigate this, we use the first reference view to calculate the mean and variance, and then align all the other decoded images with this reference view. Let \(I_{i}^{g}\) be the \(i\)-th decoded images with a mean \(_{I_{0}^{g}}\) and a variance \(_{I_{i}^{g}}\), \(_{0}^{g}\) be the reference view with a mean \(_{I_{i}^{g}}\) and a variance \(_{I_{i}^{g}}\), the corrected image \(I_{i}^{c}\) is computed by:

\[I_{i}^{c}=^{g}-_{I_{i}^{g}}}{_{I_{i}^{g}}}_{I_{ 0}^{g}}+_{_{0}^{g}}.\] (5)

The optimization objective of STD consists of an L1 reconstruction loss and an LPIPS perceptual loss between \(I^{g}\) and ground-truth \(^{g}\), and an adversarial loss, as

\[_{}=_{}(I^{g},^{g})+_{}(I^{g},^{g})+_{}(I^{g}).\] (6)

where \(_{}\) is the adversarial loss that discriminates between real image \(^{g}\) and fake image \(I^{g}\).

### Fine-tuning Strategies of 3D Gaussian Splatting

Confidence-aware 3D Gaussian splatting.Unlike existing sparse-view NVS methods, our approach does not rely on depth estimation networks for depth regularization. Instead, we take a purely 2D visual method by utilizing a video diffusion model to enhance images rendered from a low-quality 3DGS model. Despite this significant enhancement in the quality of the rendered views, we proposeto rely more on the reference views rather than the restored novel views when fine-tuning the 3DGS model, since the 3DGS model is highly sensitive to slight inaccuracies in the restored views. These inaccuracies could be amplified during the fine-tuning process.

To minimize the negative impact of generated images on Gaussian training, we propose confidence-aware 3D Gaussian splatting. This strategy involves two levels of confidence, image level and pixel level. For the image level, the generated images that are closer to real images have lower confidence. For pixel level, the larger the mean covariance of all the Gaussians used to render this pixel, the higher its confidence.

Image level confidence.In the task of novel view synthesis, if noise exists in two image views, a close distance between them increases the likelihood of generating conflicts and disrupting the 3D consistency of the scene. Therefore, for novel views that are close to the reference view, it is crucial to carefully optimize the 3D Gaussians to mitigate the adverse effects of noise. Conversely, when a novel view is far from all known views, it has a smaller likelihood of disturbing already well-reconstructed areas. Based on this reasoning, we normalize the distance from novel views to reference views between 0 and 1. The farther a viewpoint is from the reference view, the higher its confidence.

Pixel level confidence.Inspired by ActiveNeRF , which uses Gaussian distributions in NeRF to estimate uncertainty and identify views with the highest information gain, we aim to find the pixels that can provide the highest information gain from the generated images. As shown in Fig 3, we observed that well-reconstructed areas are typically represented by Gaussians with very small volumes, calculated using the scaling vector \(s\). Based on this observation, we propose a method to calculate pixel-level confidence.

The unique representation of 3D Gaussians allows us to render an H\(\)W\(\)3 image using a process similar to rendering colors, where each channel corresponds to one of the three components of the scaling vector \(\). In 3DGS-Enhancer, we multiply these three channels of the scale map to obtain pixel-level confidence. For each pixel in the generated images, higher confidence results in greater weight in supervising the training of the 3DGS model.

Given a set of 3D Gaussian, the 3-channel \(C_{conf}\) confidence map is rendered as same as colour rendering, and the formula is defined as follows

\[C_{conf}=_{i M}_{i}_{i}_{j=1}^{i-1}(1-_{i}).\] (7)

Figure 3: The red circle indicates the area with high confidence, meaning the generated videos can contribute more information. Conversely, the green quadrilateral highlights the area with low confidence, suggesting that the generated video should not tend to optimize this area.

And the 1 channel pixel level confidence map \(P_{c}= C_{conf} C_{conf}}\). Overall, in our training process for 3D Gaussians, the loss functions were defined as

\[_{}=I_{c}(P_{c}\|C-\|_{1}+SSIM(C,))\] (8)

where SSIM is the Structural Similarity Index and \(\) is Hadamard's product, \(I_{c}\) is the image-level confidence map and \(\) is the real pixel value.

## 5 Experiments

### 3DGS-Enhance Dataset

Given that the enhancement of 3DGS representations is a new task, we create a dataset to simulate various artifacts of the 3DGS representations. This dataset also serves as a more comprehensive benchmark for evaluating the performance of few-shot NVS methods. Existing few-shot NVS algorithms [44; 19] primarily focus on face-forward evaluations , where the test views have significant overlap with the input views. However, this evaluation method is not suitable for large-scale unbounded outdoor scenes. Therefore, we propose a dataset processing strategy that allows us to post-process any existing multi-view dataset to generate a large number of training image pairs that include typical artifacts caused by few-shot NVS.

More specifically, for each scene, we have \(n\) views \(I_{}=\{I_{1},I_{2},,I_{n}\}\), which serve as the input for a high-quality 3DGS model. We uniformly sample a small number \(m\) of views \(I_{}\) from \(I_{}\), which serve as the input for the low-quality 3DGS model. By linearly fitting the high-quality camera poses \(p_{i}^{}=\{p_{1}^{},p_{2}^{},,p_{n^ {}}^{}\}\), we randomly sample a camera trajectory \(p_{i}^{}=\{p_{1}^{},p_{2}^{},,p_{ n^{}}^{}\}\) on \(p_{i}^{}\) and render the image pairs using both high-quality and low-quality 3DGS models. This creates a set of high-quality and low-quality image pairs used for the training of our video diffusion model.

We apply this dataset processing strategy to DL3DV , a large-scale outdoor dataset containing 10K scenes. We randomly select 130 scenes from the original DL3DV dataset and form more than 150,000 image pairs. We randomly select another 20 scenes from DL3DV to form the test sets, evaluating the corss-scene capability of our method. More implementation details of the method can be found in the supplementary material.

### Comparison with State-of-the-Arts

The quantitative and qualitative results on the DL3DV test set with 3 6 and 9 input views are shown in Table 1 and Figure 4. Our approach outperforms all the other baselines in PSNR, SSIM, and LPIPS scores. NeRF-based methods including Mip-NeRF  and FreeNeRF  produce blurry novel views due to smoothing inconsistencies. In contrast, 3DGS  generates elongated elliptical artifacts due to local minima convergence. DNGuassian  reduces artifacts with depth regularization but results in blurry and noisy novel views.

The first example in Figure 4 demonstrates 3DGS-Enhacer's capability to remove artifacts while preserving view consistency. By interpolating input views using a video diffusion model, we incorporate more information while enrusing a high view consistency, enabling high-quality novel

   &  &  &  \\
**Method** & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\   \\  Mip-NeRF  & 10.92 & 0.191 & 0.618 & 11.56 & 0.199 & 0.608 & 12.42 & 0.218 & 0.600 \\ RegNeRF  & 11.46 & 0.214 & 0.600 & 12.69 & 0.236 & 0.579 & 12.33 & 0.219 & 0.598 \\ FreeNeRF  & 10.91 & 0.211 & 0.595 & 12.13 & 0.230 & 0.576 & 12.85 & 0.241 & 0.573 \\
3DGS  & 10.97 & 0.248 & 0.567 & 13.34 & 0.332 & 0.498 & 14.99 & 0.403 & 0.446 \\ DNGuassian  & 11.10 & 0.273 & 0.579 & 12.67 & 0.329 & 0.547 & 13.44 & 0.365 & 0.539 \\
**3DGS-Enhancer (ours)** & **14.33** & **0.424** & **0.464** & **16.94** & **0.565** & **0.356** & **18.50** & **0.630** & **0.305** \\  

Table 1: **A quantitative comparison of few-shot 3D reconstruction**. Experiments on DL3DV and LLFF follow the setting of . Experiments on Mip-NeRF 360 follow the setting of .

views and avoiding local minima. The second example highlights 3DGS-Enhancer's advantage in recovering high-frequency details. Our dataset processing strategy and video diffusion model enable an understand of strong multi-view prior across various scenes. As a result, very challenging cases such as the trees can be restored with sharp details. In summary, comparisons with baseline methods demonstrate our approach's potential to significantly improve the unbounded 3DGS representations, synthesizing high-fidelity novel views for open environments.

To demonstrate the generalizability of our method for out-of-distribution dataset, we train the methods on the DL3DV-10K dataset  and test them on the Mip-NeRF360 dataset . The results, as

Figure 4: A visual comparison of rendered images on scenes from DL3DV  test set with the 3-view setting.

summarized in Table 2 and Fig 5, show that our method outperforms the baseline approaches, highlighting its remarkable generalization capabilities in unbounded environments.

### Ablation Study

Real image as reference views.Table 3 shows the quantitative comparisons of different components in 3DGS-Enhancer framework. The video diffusion model provides strong multi-view priors. However, due to its native restrictions, we directly feed the original input views into the 3DGS fine-tuning process. This results in more reliable and view-consistent information from the input domain to facilitate 3DGS fine-tuning, as demonstrated by the "Real image" in Table 3.

Confidence aware reweighting.Distant views are less likely to cause artifacts, so we normalize their distance to reference views between , giving higher confidence of video diffusion results to farther viewpoints. This strategy is denoted by "Image confidence" in Table 3. Pixel-level confidence, as denoted by "Pixel confidence" in Table 3, is based on the density of small-volume Gaussians in well-reconstructed areas, using a color rendering pipeline to calculate volumes. Both pixel and image-level confidence strategies improve results individually, and their combination yields the best performance.

Video diffusion and STD.Figure 6 visualizes the effects of video diffusion and STD module, respectively. Video diffusion removes most of the artifacts, and STD module enhances fine-grained and high-frequency textures, resulting in more vivid novel view renderings, which are closer to the ground truth. Table 4 shows the improvment for each modules.

    &  &  \\  & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** & **PSNR \(\)** & **SSIM \(\)** & **LPIPS \(\)** \\   \\  Mip-NeRF & 13.08 & 0.159 & 0.637 & 13.73 & 0.189 & 0.628 \\ RegNeRF & 12.69 & 0.175 & 0.660 & 13.73 & 0.193 & 0.629 \\ FreeNeRF & 12.56 & 0.182 & 0.646 & 13.20 & 0.198 & 0.635 \\
3DGS & 11.53 & 0.144 & 0.651 & 12.65 & 0.187 & 0.607 \\ DNGaussian & 11.81 & 0.208 & 0.689 & 12.51 & 0.228 & 0.683 \\
**3DGS-Enhancer (ours)** & **13.96** & **0.260** & **0.570** & **16.22** & **0.399** & **0.454** \\   

Table 2: A quantitative comparison of methods on the unseen Mip-NeRF360 dataset .

Figure 5: A visual comparison of cross-dataset generalization ability, where the methods are trained on the DL3DV-10K dataset  and tested on the Mip-NeRF360 dataset .

## 6 Conclusions, Limitations, and Future Work

This paper has introduced 3DGS-Enhancer, a unified framework that applies view-consistency prior from video diffusion and use trajectory interpolation method to enhance unbounded 3DGS representations. By combining image and pixel-level confidence with 3DGS fine-tuning, we have achieved state-of-the-art performance in NVS enhancement. However, our approach relies on adjacent views for continuous interpolation, it cannot be easily adapted to single-view 3D model generation. Moreover, the confidence-aware 3DGS fine-tuning strategies are relatively simple and straightforward. In the future, it is interesting to integrate confidence maps directly with the video generation model, enabling the generation of images that are more in line with the real 3D world without the need for post-processing. Meanwhile, utilizing the efficient data generation capability of 3DGS to construct a massively scaled dataset for our video generation model presents a prime opportunity to enhance the model's 3D consistency. This approach also facilitates the 2D models to understand the 3D world directly from 2D images without additional geometric constraints. Regarding the social impact, the goal of this work is to advance the fields of 3D reconstruction and NVS. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## 7 Acknowledgement

The authors gratefully acknowledge the Clemson University Palmetto Cluster for providing the high-performance computing resources that supported the computations of this work.

   Video diffusion & STD (temporal layers) & color correction & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  âœ“ & - & - & 18.11 & 0.591 & 0.312 \\ âœ“ & âœ“ & - & 18.44 & 0.625 & 0.306 \\ âœ“ & âœ“ & âœ“ & **18.50** & **0.630** & **0.305** \\   

Table 4: An ablation study of STD (temporal layers) and color correction module on the DL3DV test dataset with a 9-view setting.

Figure 6: An ablation study of the video diffusion model components in our 3DGS-Enhancer framework.

   Video diffusion & Real image & Image confidence & Pixel confidence & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  âœ“ & - & - & - & 14.33 & 0.476 & 0.422 \\ âœ“ & âœ“ & - & - & 17.01 & 0.553 & 0.361 \\ âœ“ & âœ“ & âœ“ & - & 17.29 & 0.570 & 0.354 \\ âœ“ & âœ“ & - & âœ“ & 17.16 & 0.564 & 0.351 \\ âœ“ & âœ“ & âœ“ & âœ“ & **17.34** & **0.574** & **0.351** \\   

Table 3: An ablation study of the four modules of our 3DGS-Enhancer framework, where all results are averaged across 3, 6, 9, and 12 input views on DL3DV dataset .