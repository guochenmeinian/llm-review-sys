# Towards Next-Generation Logic Synthesis:

A Scalable Neural Circuit Generation Framework

 Zhihai Wang\({}^{1}\)1 Jie Wang\({}^{1}\)2 Qingyue Yang\({}^{1}\) Yinqi Bai\({}^{1}\) Xing Li\({}^{2}\) Lei Chen\({}^{2}\)

**Jianye Hao\({}^{2,3}\) Mingxuan Yuan\({}^{2}\) Bin Li\({}^{1}\) Yongdong Zhang\({}^{1}\) Feng Wu\({}^{1}\)**

\({}^{1}\)MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,

University of Science and Technology of China

\({}^{2}\)Noah's Ark Lab, Huawei Technologies

\({}^{3}\)College of Intelligence and Computing, Tianjin University

###### Abstract

Logic Synthesis (LS) aims to generate an optimized logic circuit satisfying a given functionality, which generally consists of circuit translation and optimization. It is a challenging and fundamental combinatorial optimization problem in integrated circuit design. Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by _neural_ circuit _generation_ and _optimization_. In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to _circuit generation_ and found from extensive experiments that existing DNAS methods struggle to exactly generate circuits, scale poorly to large circuits, and exhibit high sensitivity to hyper-parameters. Then we provide three major insights for these challenges from extensive empirical analysis: 1) DNAS tends to overfit to too many skip-connections, consequently wasting a significant portion of the network's expressive capabilities; 2) DNAS suffers from the structure bias between the network architecture and the circuit inherent structure, leading to inefficient search; 3) the learning difficulty of different input-output examples varies significantly, leading to severely imbalanced learning. To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, which leverages our key insights for _completely accurate_ and _scalable_ circuit generation. Furthermore, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for efficient and effective neural _circuit optimization_. Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes. Moreover, our synthesized circuits significantly outperform the state-of-the-art results from several competitive winners in IWLS 2022 and 2023 competitions.

## 1 Introduction

Complex integrated circuits (ICs) can have billions of transistors, making purely human-based design impossible . To tackle this problem, the IC industry relies on electronic design automation (EDA) tools  that progressively transform a high-level hardware design into a layout ready for IC fabrication. Logic synthesis (LS) is a fundamental step in EDA which aims to transform a behavioral-level description of a design into an optimized gate-level circuit to minimize its delay and area. AsLS is the first step in EDA tool-chains that yields the final IC layout, the quality of its output highly impacts the area, power, and performance of the final ICs [3; 4].

LS is a challenging \(\)-hard combinatorial optimization problem. Commercial and academic LS tools use sophisticated human-designed heuristics to approximately solve this task, often obtaining sub-optimal solutions. The synthesis of high-level designs to circuits is typically done as a direct translation of hardware description language code coupled with post-processing optimization. Recent works [5; 6; 7] have shown that there exists room for fusing these two steps with neural compiler architectures. Therefore, leveraging machine learning for direct _neural circuit generation and optimization_ emerges as a significant direction towards next-generation LS.

In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to synthesize circuits from input-output examples [5; 8; 7], which seems to offer a promising avenue towards neural circuit generation. Unfortunately, we found from extensive experiments that the existing method not only struggles to generate circuits precisely, particularly in large-scale circuits but also exhibits high sensitivity to hyperparameters. Through comprehensive empirical analysis, we summarize three key insights for these challenges. 1) DNAS suffers from the curse of skip-connections, tending to learn to too many skip-connections, which results in low utilization of the large network. 2) there is a discrepancy between DNAS and the inherent structure of circuits, leading to redundant search space and inefficient search. 3) the learning difficulties vary greatly across different input-output examples, resulting in a severe imbalance during the training process.

To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, namely T-Net, which leverages our key insights for _completely accurate_ and _scalable_ circuit generation. To further enhance our T-Net for logic synthesis, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for further efficient and effective neural _circuit optimization_. The efficient search and scalable circuit generation of our T-Net come from the following aspects. 1) **Multi-Label Transformation of Training Data**. To enhance the scalability, T-Net proposes to partition the input-output examples into several sub-datasets based on the Shannon decomposition theorem and merge these sub-datasets to transform the original single-label data into multi-label data with significantly reduced input data. Jointly learning circuit structures of transformed input-output examples also exploits inherent circuit functionality symmetry for logic sharing and reducing generated circuit size. 2) **Triangle-Shaped Network Architecture**. Based on the key observation that the circuit structure generally follows a triangle-shape, T-Net designs a Triangle-shaped network architecture, which significantly reduces the search space, instead of common square-shaped architectures. 3) **Regularized Training Loss**. To mitigate overfitting to many skip-connections, T-Net proposes an inner-architecture regularized loss to suppress excessive skip-connections. Moreover, T-Net further proposes a hardness-aware loss function to actively optimize hard input-output examples.

We conducted extensive experiments on 18 circuits from four benchmarks. For circuit generation, our T-Net accurately generates large circuits with up to 1200 nodes, surpassing the state-of-the-art (SOTA) DNAS methods[5; 9], while also producing much smaller circuits compared to traditional methods[10; 11]. Based on our generated compact circuits, our evolutionary algorithm further optimizes circuits, significantly outperforming not only traditional methods, but also SOTA approaches from several competitive winners in IWLS 2022 and 2023 competitions.

We summarize our major contributions as follows. 1) An extensive analysis of the challenges inherent in applying Differentiable Neural Architecture Search (DNAS) for neural circuit generation was conducted, leading to three key underlying insights. 2) Leveraging these key insights, we developed T-Net, a neural circuit generation framework enabling efficient search and scalable generation. 3) Experiments on 18 circuits show that our approach achieves a significant 68% improvement in circuit area over the traditional method, and a remarkable 5.36% improvement compared to the SOTA approach employed by the winners of the IWLS 2023 competition.

## 2 Related Work

**Machine Learning in Logic Synthesis** In recent years, integrating machine learning (ML) into chip design workflows has garnered significant attention [1; 12; 13; 14]. The investigation spans two main areas: ML embedded in LS and end-to-end LS using ML techniques. ML embedded in LS involves incorporating ML into specific LS stages to enhance efficiency and quality. Notable efforts include using ML to tune optimization flows[15; 16; 17], predict metrics , and improve decision-making[18; 19] in LS methods. ML for end-to-end LS includes research exploring replacing traditional LS stages with ML. Approaches range from language-based circuit description[20; 21] to circuit generation through searches [5; 7]. Notable methods include integrating real-valued logic with continuous parameterization and using differentiable neural architecture search (DNASA). Despite the promising advancements, existing end-to-end methods face challenges in scaling to large circuits and are sensitive to hyperparameters. In this paper, we rethink the traditional DNAS methods for LS and propose a novel regularized triangle-shaped circuit generation framework.

**IWLS Contest** The International Workshop on Logic & Synthesis (IWLS)  annually hosts a contest, with themes in 2022 and 2023  focusing on LS from input-output examples (i.e., truth tables), scored based on circuit sizes (i.e., node number). Participated teams mainly employ traditional methods [10; 11; 23; 24; 25; 26] for circuit synthesis. In 2023, Google DeepMind introduced a DNAS-based method , achieving first place. We replicated it as a baseline, conducting a detailed analysis and enhancing the DNAS-based generation method. For circuit optimization, various operator sequence optimization approaches have been proposed [28; 29]. The 2022 champion EPFL team utilized Bayesian optimization methods within an EA framework . In contrast, we employed RL methods with strong search capabilities and introduced a restart strategy to mitigate local optima.

## 3 Background

**Logic Synthesis (LS) from IO Examples** In recent years, a promising direction that synthesizing circuits from IO examples has received increasing attention [27; 31; 32; 5; 7]. Specifically, researchers aims to use machine learning to generate a circuit given a truth table that describes the functionality of the circuit. Note that each line in the truth table is an input-output pair, which means that given the input to the circuit it will produce the corresponding output. For machine learning (ML) domain, researchers formulate the truth table as a training dataset consisting of many input-output pairs, and aim to use a ML model to generate circuits that completely fits the dataset.

**Circuit Graph Representation** Boolean Networks are widely-used discrete mathematical models with applications in various fields . In these networks, nodes represent Boolean functions, and edges illustrate connections between them. Boolean functions map from an n-dimensional space \(B^{n}\) to a 1-dimensional space \(B\), where \(B=\{0,1\}\). In the LS stage, circuits are often depicted as **And-Inverter Graphs** (AIG), offering a concise representation of Boolean Networks. AIGs consist of constant, primary inputs (PIs), primary outputs (POs), and two-input And nodes. Inverter edges signify an inversion signal. The size of a circuit denotes the number of And nodes in the AIG, while the depth (level) signifies the longest path from a PI to a PO.

**Traditional DNAS for LS from IO Examples** Recent works [5; 7] propose to leverage DNAS methods for generating circuit graphs from IO examples, which shows a promising direction towards next-generation logic synthesis. Specifically, they formulate a neural network as a circuit graph (i.e., AIG), where each neuron represents a logic gate (And gate) and connections between neurons represent wires connecting these logic gates. For a parameterized neural network, the neurons are fixed as logic gates, and the connections between neurons are parameterized as learnable parameters. To enable differentiable training via gradient descent, they introduce continuous relaxation into discrete components in the neural network. First, the logical operations of logic gates (neurons) are translated into their differentiable counterparts. For instance, \(a\,AND\,b\) is relaxed to \(a b\), and \(NOT\)\(a\) is relaxed to \(1-a\). Second, discrete network connections are parameterized, employing Gumbel-softmax  during forward propagation to continuousize and sample the connections between nodes, thus enabling optimization through gradient descent to find high-quality solutions.

## 4 Rethinking DNAS for Neural Circuit Generation

In this section, we first present motivating challenges in using DNAS for neural circuit synthesis from input-output examples in Section 4.1. Then, we present a deep understanding of these challenges in Section 4.2. We provide the detailed experimental setup in AppendixB.3.

### Motivating Challenges

We present two fundamental challenges in neural circuit generation of existing DNAS. First, DNAS struggles to generate circuits exactly from input-output examples, especially for large-scale circuits. Second, DNAS exhibits high sensitivity to hyperparameters.

**Generating Exact Circuits is Challenging** To evaluate whether DNAS can generate circuits exactly, we evaluate the DNAS method as described in Appendix B.1 and C on the input-output examples from circuits in two benchmarks. Figure 1(a) shows that out of the 16 circuits, 14 can not be generated correctly. Therefore, it is extremely challenging for DNAS to generate functionally correct circuits.

Moreover, Figure 1(a) illustrates the relationship between circuit accuracy and circuit scale. The metric for circuit scale is quantified by the number of AIG nodes obtained through the traditional synthesis method. The results reveal nearly 20% degradation in circuit accuracy as the circuit scale increases. This demonstrates the challenge that DNAS confronts in generating accurate circuits as the circuit scale grows, highlighting the poor scalability of the DNAS method.

**Sensitivity to Hyperparameters** We evaluate robustness of DNAS in circuit generation using various initializations. Results showed up to a 14.5% accuracy variation depending on the random seed, highlighting the challenge of obtaining stable results with DNAS. More details in Appendix C.2.

### A Deep Understanding of These Challenges

To elucidate the underlying causes of these challenges, we undertook comprehensive analytical experiments, which yielded the following three key insights. Firstly, DNAS suffers from the curse of skip-connections, tending to learn too many skip-connections, which results in low utilization of large initialization networks. Secondly, there is a discrepancy between DNAS and the inherent structure of circuits, leading to an inadequate exploration of the search space. Lastly, the varying learning difficulties among input-output examples cause an imbalance in the training process.

**The Curse of Skip-Connection** We observed that existing methods exhibit low utilization of the network when searching within a large network. This is attributed to the fact that connections can span across layers, bypassing certain nodes and excluding them from the final circuit. To investigate this, we analyzed how the skip connections evolve during training. The output nodes of a circuit are selected within the network through a set of learnable connection parameters. To study the cross-layer connection phenomenon of the outputs, we observe the depth of the output nodes within the network. Figure 1(b) shows the fluctuation in the depth of an output node during training. It is evident that the depth of the circuit output node undergoes a rapid decline to nearly 0, followed by a gradual rise and eventual stabilization at a shallow depth of 5. This observation implies that only a fraction of the network layers, specifically about a quarter, are effectively utilized in the circuit. The skip connections within the circuit span a considerable depth, significantly constraining the upper limit of the network's expressive capacity. As a point of contrast, our approach connects this output to layer 15, allowing for the full utilization of nodes.

Existing methods underutilize large networks because connections can span across layers, bypassing certain nodes and excluding them from the final circuit. To investigate this, we analyzed how skip connections evolve during training. We observed the depth of output nodes within the network to study cross-layer connections. Figure 1(b) depicts the depth fluctuation of an output node during training. The output node's depth initially drops sharply to almost 0, then gradually rises and stabilizes at a shallow depth of 5. This indicates that only about a quarter of the network layers are effectively utilized. Our approach connects this output to layer 15, enabling full node utilization.

To further illustrate the circuit structure searched by DNAS, we visualized the positions of circuit nodes within the network in Figure 1(c). Notably, this circuit has multi-outputs, resulting in layer

Figure 1: (a) DNAS struggles to accurately generate circuits, especially larger ones. (b) The depth of an output node of the circuit in the circuit. DNAS only connects to very shallow layers, while our method learns deeper layers. (c) The visualization of the converged DNAS network. The dark nodes represent the used circuit nodes, indicating very low utilization of deep-layer nodes. (d) The circuits generated by SOP show that the average number of nodes per layer forms a triangular pattern.

configurations distinct from those in the single-output case shown in Figure 1(b). It can be observed that only a subset of bottom-layer nodes is integrated into the circuit, with about two-thirds of the nodes being left idle due to skip connections. This visual representation demonstrates that excessively distant skip connections diminish node utilization and the expressive capacity of the network.

We have noted that DNAS shows restricted exploration of the network during training. Specifically, a node is classified as _explored_ if it is included in the discretized circuit at any point during training, and _exploration_ is defined as the ratio of explored nodes to total nodes in the network. It is evident that exploration is more extensive in shallower layers, decreasing as the layers deepen, as illustrated in Appendix C. This finding prompts us to investigate whether structural bias within the circuit architecture is responsible for imbalanced exploration. Furthermore, we observed that the skip-layer count of the output bits is highly influenced by the hyperparameter random initialization, exhibiting significant fluctuations. This sensitivity highlights that the accuracy of circuit generation is extremely responsive to hyperparameters. During the training process, the occurrence of extensive skip-layer phenomena is attributed to the fact that choosing skip-connection leads to most rapid error decay during optimization. The network tends to learn skip-connection rather than traversing through more nodes. This is known as the curse of skip-connections, as mentioned in .

**The Structure Bias of Circuits** To further investigate the structural bias in circuit design, we examine the structure of circuits generated by traditional methods. Utilizing Sum-of-Products (SOP) in ABC to synthesis circuits, we analyze and quantify the node distribution across different layers. Figure 1(d) presents the average node count distribution per layer in circuits. This reveals a distinct structural pattern: the circuits are wider in the bottom layers and become narrower in the deeper layers, suggesting inherent structural preferences in circuit designs. This is inconsistent with the commonly used rectangular network shape. Utilizing a rectangular network to learn circuit structures may result in a vast, redundant search space in the deep layers, leading to optimization difficulties. Consequently, this can lead to sensitivity to hyperparameters and lower accuracy.

**Learning Difficulties of Different Input-Output Examples** We have observed that the learning difficulty varies among different output bits and input combinations. The training loss of different output bits shows different convergence speeds, indicating variations in difficulty. For inputs, the convergence speeds for different input samples on the same output bit exhibit substantial variations, challenging the assumption of independent and identically distributed (IID) samples. Detailed experimental results are in Appendix C.2.

## 5 A Regularized Triangle-Shaped Circuit Network Generation Framework

To address the aforementioned challenges, we have developed a novel Regularized Triangle-Shaped Circuit Network Generation Framework, namely T-Net. Our method comprises three modules: a multi-label transformation of training data, a triangle-shaped network architecture, and regularized training loss for efficient search and exact generation. Moreover, we propose an evolutionary algorithm assisted by a reinforcement learning agent restarting technique for efficient and effective neural circuit optimization. We defer more implementation details to Appendix D.

Figure 2: Framework of the Regularized Triangle-Shaped Circuit Network (T-Net). Our proposed T-Net consists of three key modules: 1) Multi-label transformation of training data to decrease generation difficulty. 2) A Triangle-shaped network architecture, designed to align with the structural biases inherent in logic circuits. 3) Regularized training loss for efficient search and exact generation.

### Multi-Label Transformation of Training Data

To address the scalability challenge posed by the exponential growth of truth tables with increasing input bit widths, we propose a novel approach: the multi-label transformation of training data, leveraging the Shannon decomposition theorem . The Shannon decomposition theorem states that any boolean function (truth table) can be decomposed into two sub-functions (sub-tables) by selecting a decomposing variable. Formally, the theorem is expressed as:

\[f(X_{1},,X_{n})=X_{i} f_{1|X_{i}=1}+(\{X_{i}\} f_{2 |X_{i}=0},\] (1)

where \(f\) denotes the original boolean function, \(X_{i}\) denotes the selected variable, \(f_{1|X_{i}=1}\) and \(f_{2|X_{i}=0}\) denote the decomposed sub-functions. Based on the key observation that the truth table exhibits a duality of Boolean functions, we first partition a large truth table into two smaller sub-tables by selecting a variable and fixing its value as 0 and 1, respectively.

After partitioning the truth table, a natural approach is to learn each sub-table separately. However, since the decomposed sub-tables share many logical nodes, individual learning prevents the active learning of these shared logical nodes. To overcome this challenge, we propose a multi-label data merge mechanism, which merges the two sub-tables into a multi-output table. This results in doubling the output number while halving the input number.

By recursively applying this partition-and-merge transformation, we can transform any large truth table into another truth table with significantly reduced input numbers and increased output numbers. Note that the input size significantly impacts the difficulty of neural circuit generation. Consequently, this transformation strategy provides two major advantages: 1) It enhances the scalability of our T-Net, enabling it to learn from truth tables with large input bit widths. 2) It significantly accelerates the learning process, as the learning difficulty of sub-tables is considerably reduced.

### Triangle-Shaped Network Architecture

**Model Structure** Our network is structured as a neural network, where the neurons represent two-input NAND gates. During training, the neurons remain fixed while their connections are learned. An And-Inverter Graph (AIG) is a logic circuit composed of NAND gates and wires connecting the gates. By transforming the neurons and connections in the neural network into logic gates and wires, the neural network can be converted into an AIG circuit. Inspired by [5; 7], our basic differentiable circuit neural network structure is as follows. The network has depth \(L\) and width \(K\), indicating that it consists of \(L\) layers, each comprising \(K\) nodes. In this notation, \(l=0\) corresponds to the input of the circuit and \(l=L+1\) are the outputs. It is crucial to emphasize that the nodes in the output layer (\(L+1\)) are not considered gates; instead, they select the node within the network implementing the output signal. The network's inputs and outputs mirror the signals of a logical circuit, consisting of \(0\)s and \(1\)s. Our nodes has two inputs and one output as NAND gate. We denote the output of the \(k^{th}\) neuron in the \(l^{th}\) layer by \(^{l,k}\). We denote the \(p\)-th input of the neuron (NAND gate) \(^{l,k}\) by \(_{p}^{l,k}\), where \(p\{0,1\}\). During the training phase, the discrete logic circuit undergoes a relaxation and continuoization process in two aspects. Firstly, the logical operations of logic gates are translated into their differentiable counterparts. For instance, \(a\,}\,b\) is relaxed to \(1-(a b)\). Next, discrete network connections are parameterized, employing Gumbel-softmax  during forward propagation to continuousize and sample the connections between nodes, thus enabling optimization through gradient descent. Note that each neuron \(o^{l,k}\) has two inputs \(i_{0}^{l,k}\) and \(i_{1}^{l,k}\), and can be connected to any neuron with layer number smaller than \(l\) as its input neuron. We parameterize the connections of each neuron \(o^{l,k}\) by a tensor of learnable parameters \(^{l,k}^{2(l-1) K}\). Each parameter in the tensor \(_{p,i,j}^{l,k}\) represents the probability of connecting the \(j^{th}\) neuron in the \(i^{th}\) layer to the \(p^{th}\) input of current neuron \(o^{l,k}\). The computation of the \(p^{th}\) input value for the neuron \(o^{l,k}\) takes the form of

\[i_{p}^{l,k} :=_{i=0}^{l-1}_{j=1}^{K}o^{i,j}[ (^{l,k})]_{p,i,j},p=0,1\] (2) \[o^{l,k} :=1-_{p=0}^{1}i_{p}^{l,k}\] (3)

During evaluation, each node's input selects only one connection based on the parameters, using maximization instead of softmax during forward calculation2, restoring discrete logic operations.

**Triangle-Shape** To fit the circuit bias on structure, we propose a Triangle-shaped network structure. Due to the inherent structural preference of logic circuits for a wider base and deeper top, thecommonly used rectangular network architecture is not well-suited for them. We employ a triangular structure that widens at the bottom layers to enhance expressive capability, thereby better fitting the foundational aspects of logic circuits. At the deep layers, the structure is narrower and deeper, which ensures adequate expressive power while reducing redundant nodes. This streamlined search space simplifies optimization, making it more manageable and efficient. Importantly, the last layer's width doesn't limit output diversity since any node can serve as an output. Our experiments confirm accurate generation even for circuits with many outputs (see Appendix E.4).

### Regularized Training Loss towards Efficient Search and Exact Generation

**Regularized Skip-Connection** Note that for each node in the T-Net, it maintains a learnable probability distribution over all nodes in the T-Net whose layer number is smaller than this node. As shown in Figure 1(b), this distribution often overfits to shallow-layer nodes, causing too many skip connections. To prevent this, a natural solution is to enforce connections only to nodes in the previous layer. However, this significantly limits the search space, reducing expressive power (as demonstrated in the DNAS with no skip-connection in Table 1). To address this challenge, we propose a weighted regularization on the learnable probability distribution to softly suppress the probability of connecting to distant nodes across layers, while promoting connections to closer nodes. This approach avoids overfitting to excessive skip connections. Due to limited space, we defer the specifics of the weighting implementation to Appendix D.2.

**Boolean Hardness-Aware Loss** To alleviate the problem of extreme imbalance between positive and negative samples in the later stages of training, we introduce a Boolean Hardness-Aware Loss inspired by . By weighting loss differently for various samples, these components help maintain training speed in the later stages. We also employed a temperature coefficient decay mechanism to reduce the discrepancy between continuous computation during training and discrete testing.

### Neural Circuit Optimization

In this section, we introduce a novel optimization framework combined with our generation methods for a comprehensive logic synthesis approach. To optimize circuits, we use circuit equivalent transformations called operators, whose order and parameters significantly affect results. The goal is to find an optimal operator sequence that minimizes the circuit's size. Our method is an evolutionary algorithm optimization framework assisted by reinforcement learning with a restart strategy. The framework and more details can be seen in Appendix D.5

**Reinforcement Learning for Operator Sequence Optimization** Inspired by , our environment consists of the circuit, the logic synthesis tool ABC, and nine logic optimization operators. The agent receives the circuit state from the environment and outputs the next action, which includes an operator and its parameters. This operator is then applied to the circuit, resulting in the next circuit state. Ultimately, the RL model learns the optimal sequence of operators for the circuit, which is then used to optimize the circuit and hand it over to the EA.

**RL Agent Restart Strategy.** After a period of training, the agent parameters may converge and performance may settle into local optima. To address this, we restart the network parameters after a certain training period. Specifically, we reinitialize the agent parameters and recommence training using the optimal circuit while retaining the agent's encoder parameters. This helps escape from local optima and allows continued exploration of the search space for superior solutions. Retaining the encoder parameters preserves learned experiences, guiding subsequent training iterations.

**Evolutionary Algorithm Optimization Framework.** To better escape local optima, our optimization approach incorporates an Evolutionary Algorithm (EA) framework. The initial population consists of diverse circuits generated by our T-Net. To ensure that the generated circuits closely match functionality constraints with truth tables, we implement a legalization step to ensure functional compliance, as detailed in Appendix D.6. Subsequently, the EA iteratively optimizes circuits by the RL model and maintains an elite circuit population. Finally, the optimal circuit is selected as the output. Compared with only picking one optimal solution when restarting, EA can increase the diversity of the circuit and expand the search scope. A detailed procedure is in the Appendix D.5.

## 6 Experiments

Our experiments have four main parts. 1) Evaluate our generation and optimization approach on four open-source circuit benchmarks. 2) The scalability of our generation method. 3) Perform carefully designed ablation studies to provide further insight into the DNA-based circuit generation approach. 4) Verify the robustness of our approach through a sensitivity analysis.

**Benchmarks** We evaluate our approach using circuits from four benchmarks: Espresso, LogicNets, Random, and Arithmetic. Random circuits are random and decomposable Boolean functions, while Arithmetic circuits involve arithmetic functions with permuted inputs and dropped outputs. We selected 18 circuits (8 circuits are in Appendix E and the average are calculated by all 18 circuits), with inputs ranging from 7 to 16 and outputs from 1 to 63. Circuit sizes, based on node count synthesized through the SOP method, range from 100 to 1200. The circuits are divided into small (12 circuits, node count < 500) and large (6 circuits, node count > 500) datasets, highlighting synthesis challenges, especially for the large circuits.

**Experimental Setup** For circuit generation, we implemented our T-Net as in Section 5. We train our model on all input-output pairs of each circuit and evaluate their Boolean correctness. For circuit optimization, we use the RL model inspired by  and our EA framework. We conduct the LS operator sequence by open-source logic synthesis tool ABC. Implementation details, hyperparameters, and hardware specifications can be found in Appendix E.

**Baselines** We compare our T-Net with the following generation approaches: 1) Basic DNAS: Based on , it learns connections but lacks skip-layer connectivity. 2) DNASkip: Proposed by Belcak et al. and used by Google DeepMind in IWLS 2023. We re-implemented this method with Gumbel-Softmax as Google did not open-source their code. 3) Darts-: An improvement on DNAS that addresses skip-connection issues in traditional NAS task. We adapted it for circuit neural networks. 4) SOP (Sum-of-Products): A traditional LS method. We used resyn2 to optimize circuits synthesized by SOP and our T-Net, showing our method's superior initial solutions. On the other hand, the baselines for optimization include: 1) SOP with resyn2: Traditional LS method with the resyn2 operator. 2) IWLS Competition Results: We compare with the top three teams from 2022 and 2023. These teams mostly used extensive traditional methods, while EPFL employing Bayesian optimization and Google using the DNAS circuit generation method.

**Evaluation Metrics** We evaluate the accuracy and the size of the generated circuits and optimized circuits. 1) Accuracy: The ratio of correctly predicted output bits to the total number of output bits. Importantly, achieving 100% accuracy in the generated logic circuit stands as a fundamental criterion in the task of logic circuit synthesis. 2) Wrong Bits: The number of incorrectly predicted output bits, used to highlight accuracy differences in large-scale circuits. 3) Circuit Node/Size: The number of nodes in the generated AIG circuit, with fewer nodes being better for minimizing chip area.

   &  &  \\ 
**Size** & **Circuit** & **PI** & **PO** & **Init Node** & **Opt Node** & **Init Node** & **Impr(\%)** & **Opt Node** & **Impr(\%)** \\   & Expresso3 & 5 & 28 & 205 & 149 & 155 & 24.39 & 136 & 8.72 \\  & Expresso4 & 16 & 1 & 129 & 66 & 37 & 71.32 & 27 & 59.09 \\  & Expresso57 & 8 & 63 & 482 & 296 & 334 & 30.71 & 275 & 7.09 \\  & LogicNet & 12 & 3 & 194 & 160 & 160 & 17.53 & 140 & 12.50 \\  & Random1 & 10 & 1 & 168 & 116 & 117 & 30.36 & 105 & 9.48 \\  & Arithmetic2 & 8 & 7 & 316 & 268 & 254 & 19.62 & 236 & 11.94 \\   & Expresso8 & 14 & 8 & 1159 & 965 & 207 & 82.14 & 151 & 84.35 \\  & Expresso9 & 14 & 14 & 1234 & 989 & 544 & 55.56 & 450 & 54.50 \\  & LogicNet & 12 & 3 & 808 & 670 & 636 & 21.29 & 601 & 10.30 \\  & LogicNet6 & 12 & 3 & 966 & 796 & 374 & 61.28 & 350 & 56.03 \\   &  &  &  &  &  &  \\  

Table 2: Generation size results. Inti Node is generated by SOP or our T-Net and Opt Node is optimized by resyn2. Impr. represents the percentage decrease in nodes achieved by our approach.

   &  &  &  &  \\ 
**Size** & **Circuit** & **PI** & **PO** & **Acc(\%)** & **V wrong** & **Acc(\%)** & **V wrong** & **Acc(\%)** & **V wrong** & **Acc(\%)** & **V wrong** & **Impr(\%)** \\   & Expresso3 & 5 & 28 & 99.21 & 7 & 99.77 & 2 & 91.74 & 74 & 100 & 0 & 100 \\  & Expresso4 & 16 & 1 & 70.99 & 19008 & 100 & 0 & 77.44 & 14784 & 100 & 0 & 100 \\  & Expresso4 & 8 & 63 & 97.83 & 349 & 90.30 & 1564 & 97.16 & 458 & 100 & 0 & 100 \\  & Expresso4 & 12 & 3 & 96.78 & 395 & 92.80 & 835 & 96.23 & 463 & 100 & 0 & 100 \\  & Random1 & 10 & 64.06 & 368 & 98.73 & 13 & 64.64 & 362 & 100 & 0 & 100 \\  & Arithmetic2 & 8 & 7 & 81.47 & 332 & 99.22 & 14 & 75.61 & 437 & 100 & 0 & 100 \\   & Expresso8 & 14 & 8 & 84.13 & 20797 & 86.86 & 17223 & 97.66 & 3056 & 100 & 0 & 100 \\  & Expresso9 & 14 & 14 & 93.82 & 14467 & 81.36 & 42756 & 97.29 & 6196 & 99.99 & 12 & 99.97 \\   & LogicNet & 12 & 3 & 97.17 & 2560 & 87.79 & 1500 & 83.77 & 1994 & 99.99 & 1 & 99.93 \\   & LogicNet6 & 12 & 3 & 65.34 & 4259 & 83.26 & 2057 & 96.05 & 485 & 100 & 0 & 100 \\   &  &  &  &  &  &  &  &  &  \\  

Table 1: Generation accuracy results. Impr. is the percentage decrease in wrong bits.

### Comparative Evaluation

**Generation Evaluation** We evaluate generation accuracy and wrong bits across four datasets. The results in Table 1 show that T-Net significantly outperforms all baselines, achieving 100% accuracy on most circuits and at least 99.9% on all. T-Net shows an average accuracy improvement of 17.48% over Basic DNAS and 8% over DNAS Skip. Our approach for circuit neural networks also significantly outperforms general DNAS improvement methods like Darts-. Regarding wrong bits, even with similar accuracy, T-Net shows significantly fewer errors. For instance, in Espresso9, our method reduces wrong bits by 42,000. These results demonstrate the superiority of our approach.

In addition, we evaluate the generation circuit size and the optimized size after applying traditional operators. The results in Table 2 show that our method significantly outperforms the SOP method in initial node count, with an average improvement of 33.42%. Furthermore, circuits with fewer initial nodes also exhibit better optimization outcomes, with our method showing an average improvement of 23.72% in optimized nodes compared to traditional methods. Notably, for circuits with initial node improvements over 82%, we achieve up to 84.35% improvement in optimized nodes. These results highlight the effectiveness and superiority of our approach.

**Optimization Evaluation** We evaluate our proposed optimization framework, as shown in Table 3. By integrating our circuit generation method, we significantly reduced circuit size by 68.70% compared to the traditional SOP+resyn2 method, demonstrating our optimization approach's effectiveness. Additionally, our average size outperformed the 2022 first-place team, EPFL, by 25.78%, and the 2023 first-place team, Google, by 5.36%, significantly ahead of other teams. This highlights the superiority of our generation method and optimization framework in better circuit synthesis.

### Scalability

To validate our multi-label transformation's effectiveness in improving accuracy, efficiency, and reducing circuit node count, we tested large circuits from the LogicNets and Espresso datasets. We use SOP to quickly test the truth table transformation method with different bits and select the bits with the smallest size. For LogicNets6, we decomposed it into four parts using two inputs. For Espresso9, we split the truth table into two parts.

The experimental results in Table 4 show a 90% reduction in wrong bits and an average 26% decrease in nodes after transformation. For LogicNets6, the method reduced generation time by 41%. These results confirm the efficiency of our decomposition method for large circuits. By isolating complex variables, we mitigate circuit generation complexity, enhancing scalability. Despite doubling the number of output bits, our T-Net maintains high accuracy, indicating its robustness to increased output bits.

### Ablation Study

In this section, we conducted an ablation study on four circuits to understand the contributions of each T-Net component. Our T-Net adds three modules to DNAS: regularized skip-connection, boolean hardness-aware loss, and T-Net, abbreviated in Table 5 as con., loss, and T, which leads to three main

   Circuit & Method & Nodes & Time(b)\({}_{1}\) & Acc(\%)\(\) & Wres.\(\) \\   & Default & 613 & 29 & 99.60 & 49 \\  & Decomp. & 374 & 14 & 100 & 0 \\  & Impv. & **39\%** & **41\%** & **0.4** & **100\%** \\   & Default & 699 & 26 & 99.97 & 61 \\  & Decomp. & 610 & 16 & 99.99 & 6 \\   & Impv. & **13\%** & **38\%** & **0.02** & **90\%** \\   

Table 4: Scalability results of transformed truth table on large circuits.

    &  &  &  \\ 
**Size** & **Circuit** & **PI** & **PO** & **SOP-resyn2** & **TUW** & **UCB** & **EPFL(AL)** & **NRU** & **EPFL(AL)** & **TUW** & **Google(AL)** & **Opt Node \(\)** & **Impv(\%)** \\   & Espresso3 & 5 & 28 & 149 & 77 & 88 & 79 & 83 & 81 & 70 & 72 & 69 & 53.69 \\  & Espresso4 & 16 & 1 & 66 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 25 & 62.12 \\  & Expresso7 & 8 & 63 & 266 & 147 & 151 & 125 & 146 & 148 & 138 & 141 & 139 & 53.04 \\  & LogicNets1 & 12 & 3 & 160 & 69 & 70 & 72 & 65 & 62 & 60 & 61 & 55 & 65.63 \\  & Random1 & 10 & 11 & 116 & 39 & 62 & 44 & 41 & 40 & 38 & 37 & 68.10 \\   & Attrachenzek & 8 & 7 & 268 & 156 & 164 & 170 & 152 & 149 & 128 & 115 & 105 & 60.82 \\   & Expresso8 & 14 & 8 & 965 & 68 & 69 & 68 & 68 & 68 & 68 & 68 & 68 & 92.95 \\   & Expresso9 & 14 & 14 & 198 & 202 & 208 & 220 & 220 & 216 & 191 & 181 & 153 & 84.53 \\   & LogicNets4 & 12 & 3 & 670 & 340 & 240 & 246 & 285 & 241 & 206 & 167 & 154 & 77.01 \\   & LogicNets6 & 12 & 3 & 796 & 390 & 281 & 208 & 152 & 166 & 106 & 89 & 90 & 85.69 \\   &  & 137.78 & 118.33 & 112.28 & 124.11 & 113.89 & 94.39 & 88.06 & **83.33** & **68.70** \\   

Table 3: Optimization results. The term ‘Impr.’ is defined as the percentage decrease in the number of nodes achieved by our approach, relative to the traditional configuration.

conclusions. 1) Compared to the baseline, the _con._ significantly enhances accuracy, highlighting the effectiveness of the regularized skip-connection module in mitigating skip-layer degradation and improving network expressiveness. 2) The _con.+loss_ approach nearly doubles the reduction in wrong bits compared to +con., showing that the boolean hardness-aware loss function significantly boosts accuracy for challenging instances. 3) _con.+loss+T_ improves both accuracy and node count, indicating that T-Net reduces training difficulty and enhances circuit generation effectiveness.

### Sensitivity Analysis

We validate the sensitivity of our method to hyperparameters from two perspectives: random initialization and the initial size of the network. Experiments show that our approach uniformly maintained 100% accuracy across various random initialization and network initial sizes, underscoring its robustness to these fluctuations. Please see Appendix E.3 for details.

## 7 Conclusion

We rethink existing DNAS methods and empirically show three fundamental challenges pertaining to existing methods: 1) DNAS tends to overfit to too many skip-connection; 2) DNAS suffers from the structure bias between the network and the circuit's inherent structure, leading to inefficient search; 3) imbalanced learning across different input-output examples. Based on these insightful observations, we propose a novel neural logic gate network search framework, which has a Triangle-shaped structure, regularized skip-connection, and boolean hardness aware loss function. Experiments on four circuit benchmarks demonstrate that our method can precisely generate circuits with large AIG sizes. Moreover, our generated circuits have a significant 68% improvement in area surpassing the performance of the traditional method.