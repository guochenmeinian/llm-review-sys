# DeNetDM: Debiasing by Network Depth Modulation

Silpa Vadakkeeveetil Sreekatha1  Adarsh Kappiyath1  Abhra Chaudhuri1,2,3 Anjan Dutta1

1 University of Surrey 2 University of Exeter 3 Fujitsu Research of Europe

{s.vadakkeeveetilsreekatha, a.kappiyath, anjan.dutta}@surrey.ac.uk, abhra.chaudhuri@fujitsu.com

Equal contribution.

###### Abstract

Neural networks trained on biased datasets tend to inadvertently learn spurious correlations, hindering generalization. We formally prove that (1) samples that exhibit spurious correlations lie on a lower rank manifold relative to the ones that do not; and (2) the depth of a network acts as an implicit regularizer on the rank of the attribute subspace that is encoded in its representations. Leveraging these insights, we present DeNetDM, a novel debiasing method that uses network depth modulation as a way of developing robustness to spurious correlations. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Our method requires no bias annotations or explicit data augmentation while performing on par with approaches that require either or both. We demonstrate that DeNetDM outperforms existing debiasing techniques on both synthetic and real-world datasets by 5%. The project page is available at [https://vssilpa.github.io/denetdm/](https://vssilpa.github.io/denetdm/).

## 1 Introduction

Deep neural networks (DNNs) have made remarkable progress across various domains by delivering superior performance on large-scale datasets. However, while the benefits of training DNNs on large-scale datasets are undeniable, these algorithms also tend to inadvertently acquire unwanted biases Shah et al. (2020), hampering their generalization. For instance, a classifier predominantly trained to recognize camels in desert landscapes could encounter difficulties when attempting to identify a camel situated on a road Kim et al. (2021). While a certain degree of bias can enhance model performance, as exemplified by the assumption that cars usually travel on roads Choi et al. (2020), it remains critical to identify and address unwanted biases. Previous methods to address this problem rely on bias annotations as suggested in Majumdar et al. (2021); Kim et al. (2019); Sagawa et al. (2020); Wang et al. (2020), and may involve predefined bias types, such as texture bias mitigation approach in Geirhos et al. (2019). However, acquiring bias labels with human resources is expensive and time-consuming. Recent studies, including Nam et al. (2020) and Lee et al. (2021), have shifted towards debiasing methods without bias labels, with approaches like Nam et al. (2020) emphasizing bias-aligned samples and reweighting bias-conflicting samples, while others like Lee et al. (2021); Kim et al. (2021) introduce augmentation strategies to diversify bias-conflicting points.

We propose DeNetDM (**De**biasing by **Net**work **D**epth **M**odulation), a novel approach to automatically identify and mitigate spurious correlations in image classifiers without relying on explicit data augmentation or reweighting. We start by showing that a sample set that exhibits bias through spurious correlation of attributes lies on a manifold with an effective dimensionality (rank) lower than its bias-free counterpart. We then leverage this finding to formally derive a relationship between the depth of a network and the true rank of the attribute (not sample) subspace that it encodes. We find for a set of attributes that are equally likely to minimize the empirical risk, a deeper network prefersto retain those with a lower rank, with a higher probability. This implies that the depth of a network acts as an implicit regularizer in the rank space of the attributes. We find that deeper networks tend to generalize based on bias attributes and shallower networks tend to generalize based on core attributes. This finding is in line with a number of works that show that deeper networks tend to learn low rank solutions in general (Roy and Vetterli, 2007; Huh et al., 2023; Wang and Jacot, 2024). Note, however that prior works do not establish the relationship between network depth and the rank of the attribute subspace, a link we establish in our work for the first time, to the best of our knowledge.

Our theoretical claims are confirmed by our preliminary empirical study on linear feature decodability, which quantifies the extent to which specific data attributes can be accurately and reliably extracted from a given dataset or signal. Our study focuses on the feature decodability of bias and core attributes in the neural networks of varying depths, following the approach outlined in Hermann and Lampinen (2020). Our observations in untrained neural networks reveal that the feature decodability tends to diminish as the networks become deeper. We also investigate how attribute decodability varies with Empirical Risk Minimization (ERM) based training on networks of varying depths.

Our hypothesis posits that in a task requiring deep and shallow branches to acquire distinct information, the deep branch consistently prioritizes bias attributes, while the shallow branch favors core attributes. We utilize a technique inspired by the Product of Experts (Hinton, 2002), where one expert is deeper than the other. Empirical analysis shows that the deep branch becomes perfectly biased and the shallow branch becomes relatively debiased by focusing solely on the core attributes by the end of the training. Since the shallow branch may lack the capacity to capture the nuances of the core attributes adequately due to less depth, we propose a strategy where we train a deep debiased model utilizing the information acquired from both deep (perfectly biased) and shallow (weak debiased) network in the previous phase. Our training paradigm efficiently facilitates the learning of core attributes from bias-conflicting data points to the debiased model of any desired architecture.

In summary, we make the following contributions: (1) We theoretically prove that the deep models prefer to learn spurious correlations compared to shallower ones, supported by empirical analysis of the decodability of bias and core attributes across neural networks of varying depths. (2) Building upon the insights from our decodability experiments, we present a novel debiasing approach that involves training both deep and shallow networks to obtain a desired debiased model. (3) We perform extensive experiments and ablation studies on a diverse set of datasets, including synthetic datasets like Colored MNIST and Corrupted CIFAR-10, as well as real-world datasets, Biased FFHQ, BAR and CelebA, demonstrating an approximate 5% improvement over existing methods.

## 2 Related Works

Several works, such as Hermann and Lampinen (2020); Mehrabi et al. (2021), have highlighted neural networks' vulnerability to spurious correlations during empirical risk minimization training. Recently, various debiasing techniques have emerged, which can be categorized as follows.

**Supervision on bias:** A variety of approaches (e.g., Majumdar et al. (2021); Kim et al. (2019); Sagawa et al. (2020); Wang et al. (2020)) assume readily accessible bias labels for bias mitigation. Some approaches assume prior knowledge on specific bias types without using explicit annotations, like texture bias in Wang et al. (2019); Ge et al. (2021); Geirhos et al. (2019). Recent works such as Karimi Mahabadi et al. (2020); Clark et al. (2019) apply the Product of Experts method to mitigate bias in natural language processing, assuming a biased expert's availability. However, obtaining bias labels can be resource-intensive. In contrast, DeNetDM, our proposed method, does not require pre-access to bias labels or types. Instead, it leverages diverse network architecture depths within the Product of Experts framework to implicitly capture relevant bias and core attributes.

**Utilization of pseudo bias-labels:** Recent approaches avoid explicit bias annotations by obtaining pseudo-labels through heuristics to identify biased samples. One heuristic suggests that biases easy to learn are captured early in training, as seen in Nam et al. (2020); Lee et al. (2021); Liu et al. (2023); Kim et al. (2021); Tiwari and Shenoy (2023); Lee et al. (2023). Nam et al. (2020) employ generalized cross-entropy loss to identify and reweight bias-conflicting points. On the other hand, Lee et al. (2021) augment features of bias-conflicting points for debiasing, while Liu et al. (2023) employ logit correction and group mixup techniques to diversify bias-conflicting samples. Other methods like Sohoni et al. (2020) and Seo et al. (2022) acquire pseudo-bias labels through clustering in biasednetwork feature spaces. Our approach does not explicitly require pseudo-bias labels; it implicitly uses them during training to learn both biased and debiased models.

**Dependence on network architectures:**Diffenderfer et al. (2021) employ lottery-ticket-style pruning algorithms for compressed robust architectures. Similarly, approaches like Park et al. (2023); Zhang et al. (2021) introduce pruning to extract robust subnetworks. Our method aligns with this category but does not target specific robust subnetwork discovery. Instead, we utilize training dynamics of varied-depth architectures to enhance debiasing. Meanwhile, Shrestha et al. (2022) applies Occam's razor principle to optimize network depth and visual regions, enhancing overall robustness. Both DeNetDM and OccamNets (Shrestha et al., 2022) aim to simplify learning for better generalization and reduced spurious correlations. DeNetDM uses depth modulation with separate deep and shallow branches to address bias - where the shallow model captures biases and the deep model learns complex, unbiased patterns. In OccamNets, simplicity is a core design principle, with the architecture adaptively minimizing complexity on a per-sample basis. Both methods tackle spurious correlations without extra annotations or data augmentation but through distinct architectural strategies.

## 3 Debiasing by Network Depth Modulation

First, we theoretically justify that the deeper models are more inclined to learn spurious correlations compared to shallow networks, as discussed in Section 3.1. We then provide empirical evidence to support our theoretical claims by utilizing feature decodability, detailed in Section 3.2. Based on these, we introduce DeNetDM, a debiasing approach centered on network depth modulation. Our training process comprises two stages: initially, a deep and shallow network pair is trained using a training paradigm that originates from Products of Experts (Hinton, 2002), yielding both biased and debiased models, which is detailed in Section 3.3. Subsequently, recognizing the limitations of the shallow debiased model in capturing core feature complexities due to its depth, we proceed to train a target debiased model, ensuring it possesses the same or higher depth compared to the deep biased model. This phase leverages information acquired from the biased and debiased models in the previous step, as elaborated in Section 3.4. An illustration of DeNetDM is provided in Figure 1.

**Notations:** We operate on a dataset \(X\), where a fraction of the data points, denoted with \(X_{a}\), are bias-aligned and the remaining points, denoted with \(X_{c}\), are bias conflicting. Let \(:X^{n}\) be an encoder that produces an embedding \(z^{n}\) for an input \(x X\). We denote the effective rank (Roy and Vetterli, 2007) of a matrix \(A\) as \((A)\), which gives us a continuous notion of the size of the span (rank) of \(A\), a quantity that is maximized under equally distributed singular values, and minimized when a single singular value dominates over the rest (Huh et al., 2023). Let \(B\) and \(C\) be the set of bias and core attributes respectively, both with strictly positive ranks, defining bases that are orthogonal to each other, _i.e._, \(B C\). A summary of notations is provided in Section 7.1.

Figure 1: **Illustration of the DeNetDM framework**: In Stage 1, an ensemble of shallow and deep branches produces outputs linearly combined and trained as a product of experts. The cross-entropy loss with depth modulation aids in separating biases and identifying target attributes. In Stage 2, we further introduce a target branch with the desired architecture, which also requires debiasing. This phase exclusively focuses on refining the target branch’s feature extractor (\(_{t}\)) and classifier head (\(f_{t}\)) while leveraging knowledge from the initial stages.

### Simplicity Bias and Spurious Correlations

Debiasing with network depth modulation requires understanding how the depth of a neural network affects its learning of bias-aligned or bias-conflicting subsets of \(X\) with lower generalization error. These results finally let us build up to our finding that deeper networks are more susceptible to learning spurious features over their shallower counterparts. All proofs are deferred to Section 7.2.

**Definition 1** (Stability).: A partitioning \(X=X_{1} X_{2}... X_{m}\) of a sample set \(X\) is stable _wrt_. an attribute \(\) when:

\[P(X_{i}^{})=P(X^{}); i[1,m],\]

where \(X^{}\) and \(X_{i}^{}\) are the respective subspaces of \(X\) and \(X_{i}\) corresponding to the attribute \(\), and \(P()\) is the associated probability distribution.

For example, if \(\) follows a uniform distribution in \(X\), a stable partitioning would ensure that each of the partitions \(X_{i}\) also have \(\) distributed uniformly. Stability ensures that a partitioning does not introduce sampling bias into any of the partitions _wrt_. a particular attribute.

**Theorem 1** (Partition Rank).: _When the partitioning \(X=X_{a} X_{c}\) is stable wrt. \(C\), the rank of the bias-aligned partition is upper-bounded by the rank of the bias-conflicting partition, i.e.,_

\[(X_{a})(X_{c})\]

_Intuition_: The theorem assumes a stable partitioning of the sample set X. It implies that, in both the bias-aligned and conflicting subsets, the distribution of the core attributes are equal to that of the original sample set, _i.e._, \(P(X_{a}^{C})=P(X_{c}^{C})=P(X^{C})\). Under this condition, the only component in either of the subsets that determines the subset's rank should be the bias attributes, assuming (without loss of generality) that the attribute space is made up of only the core and the bias attributes. The proof proceeds by establishing that the rank of the bias attributes is lower in the bias-aligned points (resulting from the lack of intra-class variation due to spurious correlation with the class label) than in the bias-conflicting points.

**Theorem 2** (Depth-Rank Duality).: _Let \(=[A_{0},A_{1},...,A_{n}]\) be the attribute subspace of \(X\) with increasing ranks, i.e., \((A_{0})<(A_{1})<...<( A_{n})\), such that every \(A\) is maximally and equally informative of the label \(Y\), i.e., \(I(A_{0},Y)=I(A_{1},Y)=...=I(A_{n},Y)\). Then, across the depth of the encoder \(\), SGD yields a parameterization that optimizes the following objective:_

\[(f((X)),Y)}_{}}+_{}_{d}\|[d]()-^{d} \|_{2}, \]

_where \((,)\) is the empirical risk, \(f()\) is a classifier head, \([i]()\) is the output of the encoder \(\) (optimized end-to-end) at depth \(d\), \(\|\|_{2}\) is the \(l^{2}\)-norm, \(\) is the element-wise product, \(\) is the \(l_{2}\)-normalized version of \(X\), \(^{d}=[_{_{1}(d)};_{_{2}(d)};...;_{ _{n}(d)}]\), \(_{}\) is a random binary function that outputs \(1\) with a probability \(\), and \(_{i}(d)\) is the propagation probability of \(A_{i}\) at depth \(d\) bounded as:_

\[_{i}(d)=(([d])\,r_{i}^{-d}), \]

_where \(([d])\) is the effective rank of the \([d]\) representation space, and \(r_{i}=(A_{i})\)._

_Intuition_: For a set of attributes, all of which equally minimize the training loss, Theorem 2 describes the strategy adopted by SGD to parameterize a neural encoder, for capturing the above set of attributes. At a given depth \(d\) of the encoder \(\) (represented as \([d]\)), each attribute \(A_{i}\) gets encoded in the representation space of \([d]\) according to its corresponding probability mass \(_{i}(d)\). According to Equation (2), the probability of survival of all attributes decrease with increasing depth. However, the probability of survival of an attribute with a higher rank drops faster with increasing depth than that of one with a lower rank, prioritizing the usage of lower rank attributes at greater depths. In other words, the depth of a network acts as an implicit regularizer in the attribute rank space.

As an example, say, a neural network \(\) of depth \(d\) (denoted as \([d]\)) has \(3K\) available dimensions, and of depth \(D>d\), \([D]\) has \(K\) available dimensions (the rank reduction with increasing depth stemming from the simplicity bias (Huh et al., 2023; Wang and Jacot, 2024)). Say the attribute space it has to learn from is composed of two attributes: (1) \(A_{0}\), with a rank of \(K\), and (2) \(A_{1}\), with a rank of \(K+i\), where \(1 i K\), where both \(A_{0}\) and \(A_{1}\) are equal minimizers of the empirical risk.

So, according to Theorem 2, at \([d]\), the encoder has no constraint over the number of attributes it can accommodate, since \(3K 2K+i\). However, at depth \(D\), \([D]\) can only choose an attribute with \(K\) dimensions. Since both \(A_{0}\) and \(A_{1}\) result in the same solution for ERM (Empirical Risk Minimization), SGD would parameterize \([D]\) to capture \(A_{0}\) with a higher probability.

### Effect of Depth Modulation

Theorem 2 establishes a relationship between the depth of a network and the nature of the features it learns in terms of its rank. To empirically validate this, we probe MLPs of depths 3, 4, and 5, using the feature decodability technique proposed by Hermann and Lampinen (2020), to uncover the types of features that get encoded in them. We use the Colored MNIST dataset Nam et al. (2020) (CMNIST), where digit identity (core attribute) is spuriously correlated with color (bias attribute). We experiment with the decodability of the digit identity and color attributes in the CMNIST dataset. Additional information on the computation of feature decodability can be found in Section 7.5. We regard digit identity to have a higher rank than that of color, due to its higher representational complexity / information content in terms of the number of bits required for storage, a notion also confirmed in the experiements of Hermann and Lampinen (2020). We start by looking at the decodabilities at random initialization of the networks, and interestingly observe in Figure 1(a) that the decodabilities of both attributes decrease with increasing depth, but that of digit identity drops faster than color. Since at random initialization, there is no notion of empirical risk, the \((,)\) term in Theorem 2 is cancelled out. Thus, the observation aligns with our prediction of the second term in \(_{2}\) of Theorem 2 that the higher the rank of a feature, the less likely it is to get encoded in the later layers, the theoretically predicted behavior specifically for random networks being discussed in Corollary 2.1. We then proceed to investigating how feature decodability evolves during the early stages of Empirical Risk Minimization (ERM) training across the networks of varying depths, _i.e._, under the presence of \((,)\), the results of which are summarized in Figure 1(b). We perform similar linear decodability analysis on C-CIFAR10 dataset and the observations are presented in Section 7.6.1.

As observed in Figure 1(b), the initial phases of training for both networks emphasize color attribute (since bias is easy to learn), leading to notable improvements in color decodability for both models. Also, as training progresses, the 3-layer model exhibits higher digit decodability compared to the 5-layer model. Hence, the difference in decodability between color and digit attributes becomes more pronounced in the 5-layer compared to the 3-layer MLP. This again confirms the prediction of our Theorem 2 that when two attributes equally minimize the empirical risk, a deeper network is more likely to select the one with a lower rank, while a shallower network will try to accommodate as much of both as possible. Based on these observations, the deep models may prefer bias attributes, while shallow models focus on core attributes when tasked with capturing distinct information.

This prompts us to explore whether similar behavior can be induced in models of equal depth. In this scenario, both models, undergoing ERM training, may exhibit a similar trend, with the disparity in decodability between biased and core attributes becoming nearly identical in both models due to same depth. Consequently, when compelling each model to learn distinct information, they may

Figure 2: **Exploring the effect of depth modulation:** (a) illustrates how the linear decodability of features decreases as neural network depth increases, while (b) dives into the training dynamics of MLPs with varying depths under ERM.

capture biased or core attributes, or even divide attribute information between them, leading to a loss of control over the bias identification process. We also present empirical evidence in Table 5 to support these claims. Therefore, using models of different depths introduces an inductive bias suitable for the bias identification process.

### Stage1: Segregation of Bias & Core Attributes

Theorem 1 predicts that bias-aligned points lie on a lower-rank manifold than bias conflicting points. Theorem 2 predicts that as we go deeper into a neural network, the likelihood that a higher rank feature, that equally minimizes the empirical risk as that of other lower rank features, is retained, decays exponentially with depth. Based on this, we present a training procedure to obtain the biased and debiased classifier for an \(M\) class classification problem. Let \(_{b}\) and \(_{d}\) denote the parameters of the feature extractors associated with the deep and shallow branches, where \((_{b})>(_{d})\). We use \(f\) to represent the classifier head shared by \(_{b}\) and \(_{d}\). Here, \(f\), \(_{b}\) and \(_{d}\) are trainable parameters. Considering an image-label pair \((x,y)\), the objective function is expressed as:

\[_{}(,y)=-_{c=1}^{M}y_{c}(_{c}) \]

where \(=(f(_{b}_{b}(x)+_{d}_{d}(x)))\). If we set \(_{b}=_{d}=1\) throughout the training process, we get:

\[=(f(_{b}(x)+_{d}(x))) \]

To evaluate the performance of an individual expert, we assign a value of 1 to the corresponding \(\) while setting the other \(\) equal to 0.

Our training methodology is derived from the Products of Experts technique (Hinton, 2002) where multiple experts are combined to make a final prediction, and each expert contributes to the prediction with a weight. However, in our approach, the role of the experts is assumed by \(_{b}\) and \(_{d}\), whose features are combined through weighted contributions. The conjunction of features is then passed to the shared classifier to generate predictions. We provide a detailed proof elucidating the derivation of Equation (4) through the Product of Experts in Section 7.3 of the Appendix. Due to the architectural constraints we imposed by modulating their capacities, the deep expert tends to prioritize the learning of bias attribute, while the shallow expert is inclined towards the core attribute. The model leverages the strengths of both experts to effectively learn from their combined knowledge. We investigate the training dynamics in Section 4.3.

### Stage2 : Training the Target Debiased Model

The initial phase effectively separates the bias and core attributes into deep and shallow branches, respectively. However, relying solely on the debiased shallow branch may not be practical, as it might not capture the complex features representing the core attributes, given the less depth of the shallow model. This limitation does not apply to the deep biased model. To tackle this challenge, we introduce a target branch with the desired architecture for debiasing.

Let \(_{t}\) be the parameters of the feature extractor associated with the target branch and \(f_{t}\) be the classifier head whose weights are initialized using the weights of \(f\). During this phase, our training is exclusively focused on \(_{t}\) and \(f_{t}\). We freeze \(_{b}\) and \(_{d}\) since we leverage these models to only extract the necessary knowledge for debiasing the target branch. To capture information orthogonal to \(_{b}\), we employ the same training approach described in Section 3.3, where \(_{b}\) and \(_{t}\) serve as the experts. The objective function can be written as:

\[_{t}(,y)=-_{c=1}^{M}y_{c}(_{c})\]

where

\[=(f_{t}(_{b}_{b}(x)+_{t}_{t}(x))) \]

The training and evaluation of the experts follow the procedure described in Section 3.3, with the key difference being that in this phase, only a single expert, \(_{t}\), which is the target branch and classifier \(f_{t}\), undergoes updates.

We further leverage the knowledge pertaining to the core attributes, which is encapsulated in \(_{d}\), by transferring this knowledge to the target branch \(_{t}\) through knowledge distillation. Here, \(_{t}\) acts as the student, whereas \(_{d}\) corresponds to the teacher. We set \(_{b}=0\) and \(_{t}=1\) in Equation (5) to obtain the predictions of the student \(_{t}\). Therefore, the distillation loss is given by :

\[_{}(_{t},_{s})=-_{c=1}^{M}_{t_{ c}}(_{s_{c}}) \]

\[_{s}=(( _{t}(x))}{})_{t}= ((x))}{}) \]

where \(\) is a hyperparameter chosen from the interval \(\). The pseudocode for the entire training process of DeNetDM is provided in Section 7.4.

## 4 Experiments

In this section, we discuss the experimental results and analysis to demonstrate the effectiveness of DeNetDM training in debiasing. We evaluate the performance of the proposed approach by comparing it with the previous methods in debiasing, utilizing well-known datasets with diverse bias ratios, consistent with the prior works in debiasing. Additionally, we conduct an empirical study to analyze the training dynamics of DeNetDM. We also perform ablation studies to assess the effectiveness of individual components within the proposed approach.

### Experimental Setup

**Datasets:** We evaluate the performance of DeNetDM across diverse domains using two synthetic datasets (Colored MNIST Ahuja et al. (2020), Corrupted CIFAR10 Hendrycks and Dietterich (2019)) and three real-world datasets (Biased FFHQ Kim et al. (2021), BAR Nam et al. (2020)) and CelebA Liu et al. (2015). In Colored MNIST (CMNIST), the digit identity is spuriously correlated with color, while in Corrupted CIFAR10 (C-CIFAR10), the texture noise corrupts the target attribute. Biased FFHQ (BFFHQ) comprises human face images from the FFHQ dataset Karras et al. (2019) such that the age attribute is spuriously correlated with gender. BAR consists of human action images where six human action classes are correlated with six place attributes. We conduct experiments by varying the ratio of bias-conflicting points in the training set to demonstrate the efficacy of our approach across diverse scenarios. Following the experimental settings used by the previous works Liu et al. (2023); Lee et al. (2021); Qi et al. (2022), we vary the ratio of bias-conflicting samples, specifically setting it at {0.5%, 1%, 2%, 5%} for CMNIST and C-CIFAR10, {0.5%} in BFFHQ and {1%, 5%} in BAR datasets. We employ a subsampled version of CelebA as described in Hong and Yang (2021), maintaining the same data splits for consistency.

**Baselines:** We compare the performance of our proposed approach to the following bias mitigation techniques; ERM Vapnik (1999), GDRO Sagawa et al. (2020), LfF Nam et al. (2020), JTT Liu et al. (2021), DFA Lee et al. (2021) and LC Liu et al. (2023). Among these, GDRO utilizes supervision on bias whereas LfF and JTT assumes no prior knowledge on the basis labels. DFA and LC utilizes augmentation techniques to increase diversity of minority groups. More details on the baselines are provided in Section 7.8.2 of the Appendix.

**Evaluation protocol:** We evaluate CMNIST and C-CIFAR10 on unbiased test sets, with target features randomly correlated to spurious features, following the evaluation protocol commonly used in prior debiasing works Nam et al. (2020); Liu et al. (2021); Lee et al. (2021). Nevertheless, for BFFHQ, we do not use the unbiased test set since half of them are bias-aligned points. To ensure fair evaluation on debiasing, we adhere to previous methods Liu et al. (2023); Lee et al. (2021) by exclusively utilizing a test set comprising bias-conflicting points from the unbiased test set. Notably, the BAR test set consists solely of bias-conflicting samples, posing a significant evaluation challenge. Our primary metric is accuracy, with aligned accuracy and conflicting accuracy calculated separately for some ablations on CMNIST and C-CIFAR10 (see Section 4.4). Aligned accuracy is computed solely on bias-aligned data points while conflicting accuracy is determined exclusively based on the bias-conflicting points. For CelebA, we report worst-group accuracy specifically focusing on the bias-conflicting group (Blonde Hair = 0, Male = 0), which contains a substantial number of samples.

We conduct five independent trials with different random seeds and report both the mean and standard deviation to ensure statistical robustness.

**Implementation details:** We perform extensive hyperparameter tuning using a small unbiased validation set with bias annotations to obtain the deep and shallow branches for all the datasets. We consistently utilize the same debiasing model architectures used by the previous methods for our target branch to ensure a fair comparison. Additionally, a linear layer is employed for the classifier for all the datasets. The additional architecture details for different datasets are as follows: **(1) CMNIST:** we use an MLP with three hidden layers for the deep branch and an MLP with a single hidden layer corresponding to the shallow branch. During the second phase of DeNetDM, we use an MLP with three hidden layers for the target branch. **(2) C-CIFAR10, BAR:** we use the ResNet-20 architecture for the deep branch and a 3-layered CNN model for the shallow branch. The target branch used in the second stage of DeNetDM is ResNet-18. **(3) BFFHQ, CelebA:** we use the ResNet-18 architecture as the biased branch and a 4-layered CNN as the shallow branch. We also use the ResNet-18 architecture for the target branch, following the approaches of Liu et al. (2023); Lee et al. (2021). Further details on the datasets and implementation are presented in Section 7.8.

### Evaluation Results

We present a comprehensive comparison of DeNetDM with all the baselines described in Section 4.1 across varying bias conflicting ratios on CMNIST, C-CIFAR10, BFFHQ, BAR and CelebA in Table 1 and Table 2 respectively. As evident from Table 1 and Table 2, DeNetDM consistently outperforms all baselines across different bias ratios for CMNIST, BFFHQ, BAR and CelebA datasets. Notably, on the C-CIFAR10 dataset, DeNetDM exhibits superior performance when bias ratios are at 0.5%, 1%, and 5%, and closely aligns with LC Liu et al. (2023) in the case of 2%. These findings provide evidence for the practical applicability of DeNetDM. It is worth mentioning that the proposed approach demonstrates a significant performance enhancement across all datasets compared to Group DRO, which relies on predefined knowledge of bias. DeNetDM achieves this improvement without any form of supervision on the bias, highlighting the effectiveness of depth modulation in the debiasing.

An intriguing observation from Table 1 is that DeNetDM demonstrates better performance compared to the baselines when the bias-conflicting ratio is lower, particularly evident in the C-CIFAR10 dataset.

    &  &  &  \\   & & **Info** & 0.5 & 1.0 & 2.0 & 5.0 & 0.5 & 1.0 & 2.0 & 5.0 \\  Group DRO & ✓ & 59.67 & 71.33 & 76.30 & 84.40 & 33.44 & 38.30 & 45.81 & 57.32 \\  ERM & ✗ & 35.34 (0.13) & 50.34 (0.16) & 62.29 (1.47) & 77.63 (0.13) & 23.08 (1.25) & 25.82 (0.33) & 30.06 (0.71) & 39.42 (0.64) \\ JTT & ✗ & 53.03 (0.89) & 61.68 (2.02) & 74.23 (3.21) & 85.03 (1.10) & 24.73 (0.60) & 26.90 (0.31) & 33.40 (1.06) & 42.20 (0.31) \\ LF & ✗ & 63.39 (1.97) & 74.01 (2.21) & 80.48 (0.45) & 85.39 (0.94) & 28.57 (1.30) & 30.07 (0.77) & 39.91 (0.50) & 50.27 (1.56) \\ DFA & ✗ & 59.21 (3.15) & 71.04 (1.21) & 82.86 (2.27) & 88.29 (1.50) & 29.95 (0.71) & 36.49 (1.79) & 41.78 (2.29) & 51.13 (1.28) \\ LC & ✗ & 63.48 (5.22) & 78.41 (1.95) & 83.63 (1.43) & 88.18 (1.59) & 34.56 (0.69) & 37.34 (1.26) & **47.81 (2.00)** & 54.55 (1.26) \\  DeNetDM & ✗ & **74.72 (0.99)** & **85.22 (0.76)** & **89.29 (0.51)** & **93.54 (0.22)** & **38.93 (1.16)** & **44.20 (0.77)** & 47.35 (0.70) & **56.30 (0.42)** \\   

Table 1: Testing accuracy on CMNIST and C-CIFAR10, considering diverse percentages of bias-conflicting samples. Baseline results for C-CIFAR10 are taken from Liu et al. (2023), as we employ the same experimental settings. For CMNIST, we utilize the official repositories to obtain the models. Model requirements for spurious attribute annotations (type) are indicated by ✗ (not required) and ✓ (required).

    &  &  &  &  \\   & & 1.0 & 5.0 & 1.0 & - \\  ERM & ✗ & 57.65 (2.36) & 68.60 (2.25) & 56.7 (2.7) & 47.02 \\ JTT & ✗ & 58.17 (3.30) & 68.53 (3.29) & 65.3 (2.5) & 76.80 \\ LFF & ✗ & 57.71 (3.12) & 67.48 (0.46) & 62.21 (1.6) & - \\ DFA & ✗ & 52.31 (1.00) & 63.50 (1.47) & 63.9 (0.3) & 65.26 \\ LC & ✗ & 70.94 (1.46) & 74.32 (2.42) & 70.0 (1.4) & - \\  DeNetDM (ours) & ✗ & **73.84 (2.56)** & **79.61 (3.18)** & **75.7 (2.8)** & **81.04** \\   

Table 2: Testing accuracy on BAR, BFFHQ, and CelebA. The test set for BAR and BFFHQ contains only bias-conflicting samples. Baseline method results are derived from Lim et al. (2023) for BAR,Liu et al. (2023) for BFFHQ, and Park et al. (2023) for CelebA on the same dataset split since we utilize identical experimental settings.

We believe that the effectiveness of inductive bias enforced by DeNetDM in distinguishing between core and bias attributes is superior to that of LC, thereby allowing it to adeptly capture core attributes even when dealing with data points that exhibit fewer bias conflicting points. This emphasizes the applicability of DeNetDM in scenarios where the training data exhibits a significant amount of spurious correlations. Another noteworthy observation in Table 2 is that DeNetDM outperforms LC and DFA by a considerable margin across all datasets, particularly on the complex real-world datasets, BAR and BFFHQ. Both LC and DFA rely on augmentations to enhance the diversity of bias-conflicting points, whereas our approach utilizes depth modulation to efficiently capture the core attribute characteristics in the existing training data. Despite this, DeNetDM still achieves superior performance compared to LC and DFA without relying on augmentations.

### Analysis of Training Dynamics

In Section 3.2, we discussed the variability in linear decodability at various depths and its significance as a motivation for debiasing. To further validate this intuition and identify the elements contributing to its effectiveness, we delve into the training dynamics of DeNetDM during initial stages. We consider the training of Colored MNIST with 1% skewness due to its simplicity. Figure 3 shows how linear decodability of attributes varies across different branches of DeNetDM during training. As depicted in Figure 3, prior to training, the deep branch demonstrates lower linear decodability for both the digit identity (core attribute) and color (bias attribute) compared to the shallow branch. As training progresses, the bias attribute, easier to learn, rapidly increases in linear decodability in both branches, labeled 'A' in Figure 3.

Here, the disparity in linear decodability between digit identity and color attributes becomes more pronounced in the deep branch than in the shallow one. This distinction serves as a prior, influencing the deep branch to effectively capture the bias. Since we employ Product of Experts technique, the deep branch becomes proficient in classification using the spurious attribute, thereby compelling the shallow branch to rely on other attributes such as digit for the classification. It is worth noting that the linear decodability of core attributes is more pronounced in the shallow branch, allowing them to capture the core attributes. Thus, the training paradigm of DeNetDM leads to a shallow branch branch that majorly relies on the biased attribute. This analysis confirms our intuition and provides empirical evidence of effective debiasing.

### Ablation Studies

We perform several ablation studies to evaluate different facets of DeNetDM. We scrutinize the effect of various loss components on the performance of DeNetDM. Additionally, we explore the influence of network depth, a fundamental element of DeNetDM, and the sensitivity of DeNetDM to number of parameters which are discussed in Section 7.6. All the experiments are conducted on CMNIST and C-CIFAR10 datasets where the ratio of conflicting points is set to 1%. Additional experiments and ablations are also provided in Section 7.6.

**Effect of loss components:** We conduct ablation studies on C-CIFAR10 by selectively removing components to analyze their impact on the testing set accuracy as well as accuracy on bias-aligned and bias-conflicting points. The results are summarized in Table 3. When considering \(_{}\) alone,

   \(_{}\) & \(_{}\) & \(_{t}\) & Accuracy (\%) & Conflicting Accuracy (\%) \\ (Stage-1) & (Stage-2) & (Stage-2) & & Accuracy (\%) & Accuracy (\%) \\  ✓ &. &. & 37.47 & 37.42 & 72.40 \\ ✓ &. & ✓ & 42.89 & 35.74 & 81.60 \\ ✓ & ✓ & ✓ & 42.25 & 38.34 & 68.52 \\ ✓ & ✓ & ✓ & 43.12 & 39.46 & 69.53 \\   

Table 3: Ablation study of different losses used in DeNetDM on C-CIFAR10.

Figure 3: Early training dynamics of DeNetDM.

corresponding to the first stage of DeNetDM involving depth modulation, the model achieves 37.42% accuracy, showing a strong ability to learn target attributes. However, introducing the second stage of DeNetDM training with \(_{t}\) alone leads to capturing significant bias information alongside core attributes, evidenced by high accuracy on aligned points (81.60%). When introducing \(_{}\) alone, the model distills knowledge from the shallow branch obtained in the first stage, resulting in performance similar to stage 1 training. However, performing the second stage of DeNetDM training using both \(_{t}\) and \(_{}\) prevents capturing bias, focusing more on learning core features and resulting in improved conflicting and overall accuracy. A similar trend can be observed for CMNIST dataset and the results are summarized in Section 7.7.2.

## 5 Conclusion

We introduce DeNetDM, a novel debiasing method leveraging variations in linear decodability across network depths. Through extensive theoretical and experimental analysis, we uncover insights into the interplay between network architecture, attribute decodability, and training methodologies. DeNetDM employs paired deep and shallow branches inspired by the Product of Experts methodology, transferring debiasing capabilities to the desired architecture. By modulating network depths, it captures core attributes without explicit reweighting or data augmentation. Extensive experiments across various datasets, including synthetic ones like Colored MNIST and Corrupted CIFAR-10, as well as real-world datasets like Biased FFHQ and BAR, validate its robustness and superiority. Importantly, DeNetDM achieves performance comparable to supervised approaches, even without bias annotations.

## 6 Acknowledgments

Silpa Vadakkeveetil Sreelatha is partly supported by the Pioneer Centre for AI, DNRF grant number P1.