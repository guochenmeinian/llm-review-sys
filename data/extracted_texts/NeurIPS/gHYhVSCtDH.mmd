# Voxel Mamba: Group-Free State Space Models for

Point Cloud based 3D Object Detection

Guowen Zhang\({}^{1,2}\), Lue Fan\({}^{3}\), Chenhang He\({}^{1}\), Zhen Lei\({}^{2,3,4}\),

Zhaoxiang Zhang\({}^{2,3,4,}\)1, Lei Zhang\({}^{1,*}\)

\({}^{1}\)The Hong Kong Polytechnic University

\({}^{2}\)Centre for Artificial Intelligence and Robotics, HKISI, CAS

\({}^{3}\)Institute of Automation, Chinese Academy of Sciences

\({}^{4}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

guowen.zhang@connect.polyu.hk, {csche, cslzhang}@comp.polyu.edu.hk

{lue.fan, zlei, zhaoxiang.zhang}@ia.ac.cn

Corresponding Authors.

###### Abstract

Serialization-based methods, which serialize the 3D voxels and group them into multiple sequences before inputting to Transformers, have demonstrated their effectiveness in 3D object detection. However, serializing 3D voxels into 1D sequences will inevitably sacrifice the voxel spatial proximity. Such an issue is hard to be addressed by enlarging the group size with existing serialization-based methods due to the quadratic complexity of Transformers with feature sizes. Inspired by the recent advances of state space models (SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a group-free strategy to serialize the whole space of voxels into a single sequence. The linear complexity of SSMs encourages our group-free design, alleviating the loss of spatial proximity of voxels. To further enhance the spatial proximity, we propose a Dual-scale SSM Block to establish a hierarchical structure, enabling a larger receptive field in the 1D serialization curve, as well as more complete local regions in 3D space. Moreover, we implicitly apply window partition under the group-free framework by positional encoding, which further enhances spatial proximity by encoding voxel positional information. Our experiments on Waymo Open Dataset and nuScenes dataset show that Voxel Mamba not only achieves higher accuracy than state-of-the-art methods, but also demonstrates significant advantages in computational efficiency. The source code is available at [https://github.com/gwenzhang/Voxel-Mamba](https://github.com/gwenzhang/Voxel-Mamba).

## 1 Introduction

LiDAR-based 3D object detection from point clouds plays an important role in applications of autonomous driving , virtual reality , and robots . The sparsely, unevenly and irregularly distributed point cloud data make the efficient and effective 3D object detection a very challenging task. To address these long-standing challenges, researchers have recently proposed several strategies to improve the model architecture. One strategy is to switch from PointNet-based models  to sparse convolutional neural network (SpCNN)-based models  in order for more effective feature extraction. However, the sparse convolution is unfriendly for deployment and optimization, requiring tremendous engineering efforts. Therefore, another strategy is to switch from SpCNN to serialization-based Transformers to address this issue . These methods usually group non-empty 3D voxels into multiple short sequences by serialization techniques such aswindow partition [65; 13; 38], Z-shape sorting , and Hilbert sorting , as shown in Figs. 1 (a) and (b), where a sequence is a group of voxels to be processed by Transformer layers.

However, the serialization of voxels will inevitably sacrifice their spatial proximity. Some neighboring voxels can be far apart from each other after serialization, as illustrated by the two red points in Fig. 1 (b). Such a loss of proximity is difficult to be addressed in the existing serialization methods [69; 66; 38; 65; 13] because the group size is limited by the quadratic complexity of Transformers. This issue becomes even worse when neighboring voxels are grouped into different groups. Inspired by the recent success of State Space Models (SSMs) [21; 20; 59; 18; 82; 36] in language and vision, in this work we propose a simple yet effective group-free design to address the loss of proximity. Specifically, we introduce a Voxel SSM, termed as _Voxel Mamba_, for 3D object detection from point cloud. The linear computational complexity of SSMs makes it feasible to treat all voxels as a single group and sort them into a single sequence. This results in a group-free modeling of voxels, which is more efficient and deployment-friendly than previous methods since no padding tokens are needed. Nonetheless, even we can sort all voxels into a group-free sequence, it cannot be ensured that all of them are within an effective receptive field.

To enhance the spatial proximity of Voxel Mamba, we further propose two modules with it. The first is the **D**ual-scale SSM **B**lock (DSB) by introducing the downsampling operations in SSMs. In specific, the forward SSM branches process the high-resolution voxel features, while the backward branches extract features from the low-resolution representation. In this way, we integrate the hierarchical design with the bidirectional design in a more economical way. More importantly, the hierarchy brings a larger effective receptive field for the serialized sequence so that the spatial proximity in local 3D regions can be enhanced. The second module we introduced is the Implicit Window Partition (IWP). The window partition is a widely used strategy in previous methods [13; 65] to enhance the proximity of voxels inside a window. However, it impedes the proximity of voxels across windows and contradicts with our group-free principle. We therefore propose an implicit window partition scheme to embrace its strengths while discarding its weaknesses. In specific, we encode the voxel positions inside and across windows into embeddings for feature learning without explicitly conducting spatial window partition. In this way, better voxel proximity can be achieved under our group-free design with minimal computational cost.

Our contributions are summarized as follows:

* We propose Voxel Mamba, a group-free backbone for voxel-based 3D detection. Voxel Mamba abandons the grouping operation and serializes voxels into one single sequence, enabling better efficiency.
* To mitigate the loss of spatial proximity due to serialization, we propose the Dual-scale SSM Block (DSB) and the Implicit Window Partition (IWP) to enhance the spatial proximity preservation of Voxel Mamba.
* Our method achieves superior performance to previous state-of-the-art methods on the large-scale Waymo Open dataset  and nuScenes  datasets.

Figure 1: Comparison between (a) window-based grouping, (b) curve-based grouping, and (c) our proposed single group modeling by Voxel Mamba.

Related Work

**3D Object Detection from Point Clouds.** There are two major point cloud representations for 3D object detection, _i.e._, point-based and voxel-based ones. As in PointNet [50; 51], point-based methods [49; 48; 56; 52; 37] directly extract geometric features from small regions of raw points. However, those methods suffer from low inference efficiency and limited context features. Voxel-based methods [54; 76; 70; 27; 79; 15; 16; 17] convert raw points into regular grids through voxelization and then process them with sparse convolution  or Transformers [13; 25; 65]. Voxel-based methods are currently the main stream for 3D object detection. In terms of model architecture, voxel-based methods can be categorized into two groups, _i.e._, SpCNN-based [70; 79; 54; 55; 8; 10] and Transformers-based [65; 13; 25; 42; 38; 26] ones. Limited by the high computation complexity, SpCNN-based methods can only use small convolution kernels with restricted receptive fields, and Transformer based methods can only employ a small number of voxels in each group. In contrast, our proposed Voxel Mamba can capture long-range dependencies within the entire sequence while achieving faster inference speed than existing state-of-the-art methods.

**State Space Models.** Inspired by the continuous state space models (SSMs) in control systems, researchers [18; 21; 59; 20] have introduced the SSMs into deep neural networks as a novel alternative to CNNs and Transformers. LSSLs  adopts a simple sequence-to-sequence transformation, demonstrating the potential of SSMs. S4  introduces a new parameterization method to SSMs to reduce the computation and memory cost. S5  employs MIMO SSMs and perform efficient parallel scans based on S4. More recently, Mamba  introduces input-dependent SSMs and builds a generic backbone, which is fairly competitive with the well-tuned Transformers. Vision Mamba  employs bidirectional SSMs and position embedding to learn global visual context for vision tasks. Vmamba  employs a 2D-selective-scan to bridge the gap between 1D scanning and 2D plain traversing. PointMamba  is a pioneering work to leverage SSM for point cloud analysis, achieving impressive performance in point cloud object understanding. Subsequently, many SSM-based methods [24; 35; 78; 77; 68] are introduced for point cloud processing. In this paper, we investigate the utilization of SSMs to establish a straightforward yet robust baseline for LiDAR-based 3D object detection in driving scenes.

**Space-filling Curve.** The space-filling curve  is a series of fractal curves that can go through each point in a multi-dimensional space without repetition. The classical space-filling curve includes Hilbert curve , Z-order curve , and sweep curve, _etc_. Those methods can perform dimension reduction while maintaining spatial topology and locality. Many researchers [5; 69; 66; 38; 65; 3; 40] have introduced space-filling curves for point cloud processing. HilbertNet  uses the Hilbert curve to collapse 3D structures into 2D space to reduce computation and GPU occupation. PointGPT  utilizes the Morton-order curve  to introduce sequential properties. OctFormer  preserves Z-order during octreeleration and adopts octree-attention for efficient context learning. PTV3  streamlines the complex interaction with the space-filling curve serialization. For 3D object detection, some methods [38; 65] employ window sweep curves to group voxel features for parallel computation. We employ the Hilbert curve due to its advantageous characteristic of locality preservation.

**Point Cloud Grouping.** LiDAR point clouds are sparsely and non-uniformly distributed with varying densities. Therefore, existing methods group points or voxels to facilitate parallel computation and reduce complexity. In point cloud analysis, some works [51; 67] use the \(K\) nearest neighbor (KNN) method to create groups of query points. However, the heavy computation burden makes KNN hard to scale for outdoor scenes. For 3D object detection, VoTr  uses a GPU-based hash table to search neighborhoods and generate fixed-length voxel groups. Window-based Voxel Transformers [13; 61; 65; 38] group voxels by employing a window-based sorting strategy, such as the rotating partition. To reduce the reliance on relative position in grouping operations, some recent works [66; 69; 40] have been proposed to group voxels based on space-filling curves. However, grouping is merely a compromise for computational complexity, which restricts the flow of information and effective receptive field. To tackle this problem, we model the entire voxels into one single sequence and allow each voxel be aware of global context information.

## 3 Methods

In this section, we present Voxel Mamba, a group-free Voxel State Space Model-based 3D backbone that can be applied to most voxel-based 3D detectors. We first introduce the preliminary conceptsassociated with our method, followed by the overall architecture of Voxel Mamba. Then, we describe in detail the fundamental components of Voxel Mamba, including the Hilbert Input Layer (HIL), Dual-scale SSM Block (DSB), and Implicit Window Partition (IWP).

### Preliminaries

The state space sequence (SSM) model is a continuous-time latent state model, which maps a 1D input signal \(x(t)^{L}\) to an output signal \(y(t)^{L}\) through hidden state \(h(t)^{N}\). The system can be represented as the following linear ordinary differential equation:

\[h^{}(t)=h(t)+x(t),\\ y(t)=h(t)+x(t), \]

where \(^{N N}\), \(^{N 1}\) and \(^{1 N}\) are learnable parameters, and \(^{1}\) denotes a residual connection.

To apply SSM to a discrete sequence, we can discrete the continuous-time SSM with a timescale parameter \(\). The zero-order hold (ZOH) transformation can be used to discrete the continuous parameters \(,\) as \(}=(),}=()^{-1}(()-)\). The discretized version of Eq.(1) can be written in the following recurrent form:

\[h_{k}=}h_{k-1}+}x_{ k},\\ y_{k}=}h_{k}+}x_{k}. \]

Finally, the convolutional mode can be used for efficient parallel training:

\[}=(}, }},...,}^{}}),\\ =*}, \]

where L is the length of the input sequence and \(^{L}\) is the structured convolution kernel.

SSM combines the advantages of convolution and self-attention with near-linear computation and dynamic weights. It demonstrates stronger ability than Transformers in modeling long-range dependencies , which inspires us to develop a group-free framework for point cloud based 3D object detection.

Figure 2: **Top:** The overall architecture of our proposed Voxel Mamba with \(N\) Dual-scale SSM Blocks (DSBs). **Bottom:** Illustration of the DSB, including a residual connection, a forward SSM branch, and a backward SSM branch.

### Overall Architecture

An overview of our proposed Voxel Mamba is shown in Figure 2. As in previous works [65; 74; 31], Voxel Mamba transforms point clouds into sparse voxels by a voxel feature encoding strategy. Unlike prior Transformer-based methods that perform extensive window partitioning and voxel grouping, in Voxel Mamba we serialize the voxel of the entire scene into a single sequence by using the _Hilbert Input Layer_ (Sec. 3.3). Then, a _Dual-scale SSM Block_ (Sec. 3.4) working on the voxel sequence is proposed, which allows voxels to be processed with a global context. To enlarge the effective receptive fields, DSB adopts a finer-grained perception of the voxel sequence in the forward path, and down-samples the voxels sequence in the backward path. The backward path extracts features from the low-resolution BEV representation, with an increased downsampling factor in deeper blocks. To enhance the spatial proximity in sequences, Voxel Mamba adopts _Implicit Window Partition_ (Sec. 3.5) to preserve 3D positional information in the extracted voxel features, and projects them to a BEV feature map. Our proposed architecture is flexible and can be applied to most existing 3D object detection frameworks.

### Hilbert Input Layer

The space-filling curve (_e.g.,_ Hilbert  and Z-order ), known for preserving spatial locality, is widely used for dimensionality reduction. Space-filling curves, such as the Hilbert shown in Fig. 2, can traverse all elements in a space without repetition and preserve spatial topology. To improve the voxel proximity in serialization, we propose the _Hilbert Input Layer_ to reorder the voxel sequence.

Denote the coordinates of voxel features as \(=\{(x,y,z)^{3}|0 x,y,z n\}\). We map a voxel onto its traversal position \(h\) within the Hilbert curve. Specifically, we transform \((x,y,z)\) into its binary format with \(log_{2}n\) bits. For example, \(x\) is converted to \((x_{m}x_{m-1}...x_{0})\), where \(m= log_{2}n\). Then, following , we iterate from \(x_{m},y_{m},z_{m}\) to \(x_{1},y_{1},z_{1}\) bits and perform exchanges and inversions to adjust the order of bits. An exchange is conducted when the current bit is 0; otherwise, an invert is conducted. We concatenate all bits as \((x_{m}y_{m}z_{m}x_{m-1}y_{m-1}z_{m-1} x_{0}y_{0}z_{0})\) and apply a global \(3m\)-fold Gray decoding  on it to obtain the traversal position \(h\). Subsequently, all voxels are sorted into a single sequence based on their traversal position \(h\).

In our implementation, we record the traversal position \(h\) corresponding to the coordinates of all potential voxels. The voxels are serialized by querying and sorting their traversal positions. We employ a distinct traversal order for each BEV resolution in Dual-scale SSM blocks. Notably, the serialization process only takes approximately 0.7ms for a sequence of length \(10^{6}\).

### Dual-scale SSM Block

Though space-filling curves can preserve the 3D structure to a certain degree, proximity loss is inevitable due to the dimension collapse from 3D to 1D. As a result, a local snippet of the curve can only cover a partial region in 3D space. As discussed in Sec. 1, placing all voxels in a single group cannot ensure that the effective receptive field (ERF) [41; 12] could cover all voxels. Therefore, in this subsection we introduce the Dual-scale SSM block (DSB) to build a hierarchy of state space structures and consequently improve the ERF of the model.

As shown in Fig. 2, the DSB block is designed with a residual connection , a forward SSM branch and a backward SSM branch. It operates on two serialized voxel sequences generated by the Hilbert Input Layer, enabling a seamless flow of information throughout the voxel sequence. The forward branch processes the original voxel sequence, maintaining high-resolution details. The backward branch, however, operates on a down-sampled voxel sequence derived from a low-resolution BEV representation. This dual-scale path allows DSB to incorporate larger-scale voxel features, enhancing the model's ability to model long dependencies among voxels. Specifically, given a voxel sequence \(\) and its corresponding coordinates \(\), DSB is computed as:

\[_{f}=(((+()))), \] \[_{b}=((((()+(^{{}^{}}))))),\] \[}=_{f}+_{b}+,\]

where \(()\) represents the Hilbert Input Layer, \(()\) and \(()\) denote the forward and backward SSM, \(()\) stands for Layer Normalization, and \(^{{}^{}}\) is the coordinates of downsampledsparse voxels. Besides, \(()\) and \(()\) refer to the downsampling and upsampling operations, respectively, and \(()\) means Implicit Window Embedding. Overall, DSB integrates the widely adopted bidirectional design  with the hierarchical design, building sufficient receptive field to mitigate the loss of proximity without introducing additional parameters.

### Implicit Window Partition

The window partition strategy is widely used in previous 3D detectors  to enhance the voxel proximity. In these methods, the whole field is partitioned into multiple local windows and the voxels within a window form a group. Therefore, the voxels inside a window will have sufficient proximity; however, the voxels in different windows will have minimal proximity. In this section, we aim to introduce the advantages of window partition into our framework while avoiding its weaknesses.

To fulfill our goal, we propose an Implicit Window Partition (IWP) strategy. Unlike previous methods, we do not explicitly partition voxels into windows and apply Transformer or SSM within each window. In contrast, we calculate the voxel coordinates inside and across windows, and then encode coordinates to embeddings, termed as Implicit Window Embedding (IWE), which is formulated as:

\[=((z,}{w}, }{h},x^{i}w,y^{i}h)),i=0,1 \]

where \(\) is the floor function, \(w,h\) define the window shape, and \(z,x^{i},y^{i}\) are the coordinates of tokens. \((x^{0},y^{0})\) and \((x^{1},y^{1})\) represent the coordinates before and after an implicit window shift. The IWE is shared across all layers with the same stride. Thus, its computation cost only comes from shallow MLPs. With IWE, voxels in the serialized 1D curve are aware of their positions and consequently their proximity in 3D space.

### The Voxel Mamba Backbone

With the proposed Hilbert Input Layer, DSB and IWP strategies, we build Voxel Mamba, a group-free sparse voxel backbone. The architecture of Voxel Mamba is illustrated in Figure 2. It comprises \(N\) DSB blocks, which are organized into different stages based on their downsampling rates. SpConv  is employed to progressively decrease the feature map resolution along the Z-axis in each stage. Before sparse tokens are fed into the BEV backbone, we scatter them into dense BEV features. On the Waymo dataset, we adopt the BEV backbone from Centerpoint-Pillar , and employ the same setting as DSVT  for the detection head and loss function. On the nuScenes dataset, we only replace the 3D backbone of DSVT  with our Voxel Mamba backbone.

## 4 Experiments

### Datasets and Evaluation Metrics

**Waymo Open Dataset** contains 230k annotated samples, partitioned into 160k for training, 40k for validation and 30k for testing. Each frame covers a large perception range (\(150m 150m\)). The mean average precision (mAP) and its weighted variant by heading accuracy (mAPH) are used as evaluation metrics. They are further categorized into Level 1 for objects detected by over five points, and Level 2 for those detected with at least one point.

**nuScenes** consists of 40k labeled samples, with 28k for training, 6k for validation and 6k for testing. For 3D object detection, nuScenes employs the mean average precision (mAP) and the nuScenes detection score (NDS) to measure model performance.

### Implementation Details

Our method is implemented based on the open-source framework OpenPCDet . The voxel sizes are defined as \((0.32m,0.32m,0.1875m)\) for Waymo and \((0.3m,0.3m,0.2m)\) for nuScenes. We stack six DSB blocks, divided into three stages, for the Voxel Mamba backbone network. The downsampling rates for DSBs' backward branches in each stage are \(\{1,2,4\}\). Specifically, we employ SpConv  and its counterpart SpInverseConv as downsampling and upsampling operators in the DSB backward branch. On the Waymo dataset, we follow the training schemes in  to optimize 

[MISSING_PAGE_FAIL:7]

NDS and 67.5 mAP, which is +0.5 and +0.8 higher than the previous best method. Compared with DSVT, Voxel Mamba achieves +1.1 higher performance on mAP. The results on the test split are shown in Table 4. Our method also exhibits the best mAP and NDS.

**Inference Efficiency.** We compare Voxel Mamba with other state-of-the-art methods in inference speed and performance accuracy in Fig. 3. Notably, Voxel Mamba outperforms DSVT  and PV-RCNN++  by at least +1.5 in detection accuracy, while achieving faster speed. Some methods, such as CenterPoint  and PointPillar , are faster than Voxel Mamba; however, their accuracy is substantially lower.

We further compare Voxel Mamba with previous well-designed architectures (SpCNN, Transformers, and 2D CNN) in GPU memory in Table 5. Compared with CenterPoint-Pillar, Voxel Mamba requires only an additional 0.5 GB GPU memory but achieves +9.0 higher accuracy in L2 mAPH. While Transformer-based methods like SST  and DSVT  use group partitioning, they still consume more memory than our group-free Voxel Mamba. All the experiments are evaluated on an NVIDIA A100 GPU with the same environment.

### Ablation Studies

To better investigate the effectiveness of Voxel Mamba, we conduct a set of ablation studies by using the nuScenes validation set. We follow OpenPCDet  to train all models for 20 epochs.

**Effectiveness of Space-filling Curves.** There are some potential alternatives to Hilbert curve for preserving locality. Here, we compare Hilbert curve with some commonly used space-filling curves (Z-order  and window partition ) in 3D detection. As shown in Table 6(a), without using space-filling curves (_i.e._, the row of 'Random Curve'), there will be a notable decline in performance, which indicates that spatial proximity is crucial in the group-free setting. By using the Z-order curve

   Method & NDS & mAP & Car & Truck & Bus & T.L. & C.V. & Ped. & M.T. & Bike & T.C. & B.R. \\  PointPillars  & 45.3 & 30.5 & 68.4 & 23.0 & 28.2 & 23.4 & 4.1 & 59.7 & 27.4 & 1.1 & 30.8 & 38.9 \\
3DSSD  & 56.4 & 42.6 & 81.2 & 47.2 & 61.4 & 30.5 & 12.6 & 70.2 & 36.0 & 8.6 & 31.1 & 47.9 \\ CenterPoint  & 65.5 & 58.0 & 84.6 & 51.0 & 60.2 & 53.2 & 17.5 & 83.4 & 53.7 & 28.7 & 76.7 & 70.9 \\ FCOS-LiDAR  & 65.7 & 60.2 & 82.2 & 47.7 & 52.9 & 48.8 & 28.8 & 84.5 & 68.0 & 39.0 & 79.2 & 70.7 \\ AFDetV2V  & 68.5 & 62.4 & 86.3 & 54.2 & 62.5 & 58.9 & 26.7 & 85.8 & 63.8 & 34.3 & 80.1 & 71.0 \\ UVTR-L  & 69.7 & 63.9 & 86.3 & 52.2 & 62.8 & 59.7 & 33.7 & 84.5 & 68.8 & 41.1 & 74.7 & 74.9 \\ VISTA  & 69.8 & 63.0 & 84.4 & 55.1 & 63.7 & 54.2 & 25.1 & 82.8 & 70.0 & 45.4 & 78.5 & 71.4 \\ Focals Conv  & 70.0 & 63.8 & 86.7 & 56.3 & 67.7 & 59.5 & 23.8 & 87.5 & 64.5 & 36.3 & 81.4 & 74.1 \\ VoxelNeXt  & 70.0 & 64.5 & 84.6 & 53.0 & 64.7 & 55.8 & 28.7 & 85.8 & 73.2 & 45.7 & 79.0 & 74.6 \\ TransFusion-L  & 70.2 & 65.5 & 86.2 & 56.7 & 66.3 & 58.8 & 28.2 & 86.1 & 68.3 & 44.2 & 82.0 & 78.2 \\ LinK  & 71.0 & 66.3 & 86.1 & 55.7 & 65.7 & 62.1 & 30.9 & 85.8 & 73.5 & 47.5 & 80.4 & 75.5 \\ HEDNet  & 72.0 & 67.7 & 87.1 &and window partition to introduce spatial proximity, the mAP and NDS are much improved. The serialization based on the Hilbert curve can further enhance the model performance.

**Effectiveness of Each Component.** To more clearly illustrate the effectiveness of the different components in Voxel Mamba, we conduct experiments by adding each of them to a baseline, which is set to Centerpoint-Pillar . As shown in Table 6(b), bidirectional SSMs with a Hilbert-based group-free sequence can significantly improve the accuracy over the baseline, which validates the feasibility of our group-free strategy. Besides, converting pillar to voxel can enhance much the detector's performance without group size constraints. Voxel Mamba with DSB obtain better performance than the plain bidirectional SSMs. This is because DSB can build larger ERFs and mitigate the loss of proximity. Furthermore, IWE further boosts Voxel Mamba's performance for its capability in capturing 3D position information and increasing voxel proximity.

**Downsampling Rates of DSB.** We evaluate the impact of different downsampling rates in DSB by adjusting the stride \(\{d_{1},d_{2},d_{3}\}\) in the backward SSM branch at each stage. \(d_{i}=1\) means the original resolution is used. The results are shown in Table 6(c). We see that transitioning from {1,1,1} to {1,2,2} and to {1,2,4} enhances performance due to an enlarged effective receptive field and improved proximity by using larger downsampling rates at late stages. However, DSBs with {2,2,2} or {4,4,4} compromise performance compared to {1,1,1}, indicating that using larger downsampling rates at early stages will lose some fine details. Thus, we set the stride as {1,2,4} to strike a balance between effective receptive fields and detail preservation.

Table 6: Ablations on the nuScenes validation split. In (d), Centerpoint-Pillar is used as the baseline.

Figure 3: Detection performance (mAPH/L2) vs. speed (FPS) on Waymo.

Table 5: Comparison with other well-designed architectures on GPU memory.

**Effectiveness of IWE.** Table 6(d) validates the capability of IWE to enhance spatial proximity. We compare IWE with some commonly used positional embedding methods [13; 65] in 3D detection. Absolute position denotes the direct encoding of voxel coordinates using an MLP. The results demonstrate that IWE can significantly improve the detection performance by offering features with rich 3D positional and proximate information.

### Effective Receptive Field of Voxel Mamba

Fig. 4 illustrates the _Effective Receptive Fields_[41; 12] (ERFs) of window partition-based method DSVT , group-based bidirectional Mamba and our proposed group-free method Voxel Mamba. For clear visualization, all models take pillars as inputs. The group partition in the group-based bidirectional Mamba is configured identically to DSVT. Then, we randomly select voxels of interest from the ground truth bounding box and calculate the ERF at each non-empty voxel position. Subsequently, we merge the ERFs into a single image by taking the maximum value at each voxel location. A wider activation area indicates a larger ERF. From Fig. 4, we see that Voxel Mamba exhibits a notably larger ERF than DSVT and group-based bidirectional Mamba, which can be attributed to the benefits of group-free operation. The larger ERF can cover a more complete local region and enhance the spatial proximity in 1D sequences.

## 5 Conclusion

In this paper, we proposed Voxel Mamba, a group-free SSM-based 3D backbone for point cloud based 3D detection. We first analyzed the proximity loss of group partition in current serialization-based 3D detection methods. By taking the advantage of linear complexity of SSMs, we proposed a group-free strategy to alleviate the loss of spatial proximity in 3D to 1D serialization. We further proposed the DSB block and IWP strategy to build larger effective receptive fields and improve the spatial proximity of our Voxel Mamba framework. Experiments demonstrated that Voxel Mamba achieved state-of-the-art results on Waymo and nuScene datasets. Without elaborated optimization, our model consumed less memory than group-based Voxel Transformer methods, and our group-free strategy was more efficient and deployment-friendly than group partition. Voxel Mamba provided an efficient group-free solution for sparse point clouds for 3D tasks.

**Limitations.** While the proposed Voxel Mamba achieves state-of-the-art performance in point cloud based 3D object detection, it still has some limitations to be further addressed. First, in the Hilbert Input Layer, the curve templates occupy approximately 0.1 GB of GPU memory, which may become substantial as the voxel resolution increases. Besides, a more elaborately designed downsampling and upsampling operation could improve more the model efficiency. We will investigate these problems in future work.

**Acknowledgments.** This work was supported in part by the InnoHK Program.

Figure 4: The effective receptive fields (ERFs) of Voxel Mamba (left), group-based bidirectional Mamba (middle) and DSVT (right).