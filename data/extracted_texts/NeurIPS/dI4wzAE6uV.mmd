# Can LLM Already Serve as A Database Interface?

A BIg Bench for Large-Scale Database Grounded Text-to-SQLs

Jinyang Li\({}^{1,}\)\({}^{}\), Binyuan Hui\({}^{2,}\), Ge Qu\({}^{1,}\), Jiaxi Yang\({}^{2}\), Binhua Li\({}^{2}\), Bowen Li\({}^{6}\),

**Bailin Wang\({}^{5}\)**, Bowen Qin\({}^{2}\), Ruiving Geng\({}^{2}\), Nan Huo\({}^{1}\), Xuanhe Zhou\({}^{3}\), Chenhao Ma\({}^{6}\),

**Guoliang Li\({}^{3}\)**, Kevin C.C. Chang\({}^{7}\), Fei Huang\({}^{2}\), Reynold Cheng\({}^{1}\), Yongbin Li\({}^{2}\)

\({}^{1}\) The University of Hong Kong \({}^{2}\) DAMO Academy, Alibaba Group

\({}^{3}\) Tsinghua University \({}^{4}\) Shanghai AI Laboratory \({}^{5}\) MIT CSAIL

\({}^{6}\) The Chinese University of Hong Kong, Shenzhen

\({}^{7}\) University of Illinois at Urbana-Champaign

{jjl0725,quge}@connect.hku.hk,ckcheng@cs.hku.hk

binyuan.hby@alibaba-inc.com

###### Abstract

Text-to-SQL parsing, which aims at converting natural language questions into executable SQLs, has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database values leaving the gap between academic study and real-world applications. To mitigate this gap, we present **Bird**, a **BIg** bench for la**R**ge-scale **D**atabase grounded in text-to-SQL tasks, containing **12,751** text-to-SQL pairs and **95** databases with a total size of **33.4 GB**, spanning **37** professional domains. Our emphasis on database values highlights the new challenges of dirty and noisy database values, external knowledge grounding between NL questions and database values, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that Bird will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: [https://bird-bench.github.io/](https://bird-bench.github.io/).

## 1 Introduction

Text-to-SQL parsing , which focuses on converting natural language questions into SQL queries, has attracted significant research interests from both academia and industry. This attention stems from its potential to empower non-expert data analysts in automatically extracting desired information from ubiquitous relational databases using natural language. Recent advances in neural models, including those based on large language models (LLMs), have led to an impressive performance on existing benchmarks such as Spider and WikiSQL . For instance, the execution accuracy of the top-performing model in the Spider leaderboard has increased from 53.5% to 85.3%  over the past three years. The latest SOTA parser  in Spider benefits from the powerful understanding and coding capabilities of the large language model (LLM), and such excellent performance leads us to ask a question: _Can LLM already serve as a database interface?_

The answer is no, as previous benchmarks focus on database schema with few rows of database values leaving the gap between academic study and the real world. As shown in Figure 1, first, we discovered that current state-of-the-art models still struggle to generalize to more realistic situations characterized by large database sizes and noisy values. Second, the growth in database sizes often results in much context compression, making it challenging to reveal the entire context . Thus it requires external knowledge reasoning for a comprehensive understanding. Third, existing benchmarks do not account for SQL execution efficiency, which holds significant practical importance in real-life applications, notably in the case of large databases. Motivated by these observations, we aim to develop a new text-to-SQL benchmark that better represents real-life scenarios and narrows the gap between experimental and practical settings.

In this work, we propose **Bird**, a **BIg** Bench for LaRge-Scale **D**atabase Grounded in Text-to-SQLs for real-world applications. Bird contains complex **12,751** examples of querying information over **95** big databases with a total size of **33.4 GB** spanning **37** professional domains. For training and development, we collected and modified 80 open-source relational databases from real analysis platforms (Kaggle, Relation.vit). To further avoid data leakage, we curated 15 additional relational databases for a hidden test set. Given these databases, we rely on crowdsourcing to collect natural language questions and the corresponding SQLs. Additionally, we propose a new evaluation metric Valid Efficiency Score (VES) to evaluate the efficiency of generated SQLs. To the best of our knowledge, Bird is the first text-to-SQL benchmark to incorporate efficiency, promoting more efficient query methods within the context of massive and noisy database values.

We evaluate the performance of state-of-the-art text-to-SQL parsers using two popular methodologies: fine-tuning (FT) with T5 , and in-context learning (ICL) with advanced large language models (LLMs) such as ChatGPT  (gpt-3.5-turbo), Claude-2  (claude-2.0), GPT-4  (gpt-4-32k). Our experimental results demonstrate that the current models struggle to generalize well on Bird. Specifically, even the GPT-4 only achieves 54.89% in execution accuracy. In comparison, the performance still lags far behind the human performance of 92.96%, proving that challenges still stand. Moreover, we perform a comprehensive analysis to provide insight and direction. We encourage further research by the NLP and DB communities to jointly address the more realistic settings presented in this benchmark.

## 2 Task Formulation & Annotations

Text-to-SQL refers to the process of converting a natural language question \(\) into a SQL query \(\) capable of retrieving relevant data from a database. The database can be represented as \(=,\)

Figure 1: Examples of challenges in our **Bird** benchmark. 1) databases contain values of noisy data types . In the left example, the average salary could be fetched by processing the data type from string (TEXT in SQLite) to float (REAL in SQLite) after deleting the special tokens, "US$" and ", ". 2) external knowledge and reasoning are required. In the middle example, models must handle that only "OWNER" accounts are eligible for loans. 3) query execution efficiency needs to be considered. In the right example, the adoption of more efficient SQL queries leads to significant gains in speed, which is of great value in industries.

where \(\) and \(\) are columns and tables respectively. When dealing with complex database values, such as Bird, it is crucial to incorporate external knowledge evidence, denoted as \(\), to improve the models' understanding of database values. Finally, the text-to-SQL could be formulated as:

\[=f(,,), \]

where the function \(f()\) can represent a model or neural network with the parameter \(\).

## 3 Dataset Construction

### Annotation Entrance

To deliver a high-quality benchmark, we administer thorough exams to all applicants and only hire those who pass these rigorous tests. Further information is available in the Appendix B.2.

### Database Source

It is difficult to collect databases with complex schemas and sufficient value due to privacy protection. Earlier works [45; 53] choose to self-design database schemas and value production. Nonetheless, the value distribution and schemas may differ from real-world scenarios in this way. In our work, we obtain and process databases from three different sources to enrich real-world attributes. 32% of our databases are sourced from Kaggle*, a platform renowned for holding data science competitions with difficult, noisy values and schemas. Another 48% come from CTU Prague Relational Learning Repository+, an open platform for machine learning research with multi-relational data. The remaining 20% are built by acquiring open tables, synthesizing and standardizing schemas, and generating database constraints. All of these databases contain real and large value distributions and are easily accessible with the appropriate licenses. Finally, we present 95 databases consisting of 69, 11, and 15 databases for training, development, and testing respectively. Our databases cover 37 professional domains, including blockchain, sports, health care, politics, etc. We anticipate that it will be a significant resource for researchers to explore domain generalization in semantic parsing tasks with large database values.

Footnote *: [https://www.kaggle.com/](https://www.kaggle.com/)

Figure 2: An Overview of the Bird Annotation Workflow in (a). This figure depicts a four-step procedure. (1) The workflow begins with specialists assembling and producing databases and description files. (2) Experts then teach and evaluate crowdsourcing people, keeping only those who pass the evaluation. (3) Question annotators create a corpus of questions using databases and their corresponding description files. (4) SQL annotators produce SQL files, equipped with databases, descriptions, and questions. (b) and (c) also depict the Double-blind annotation procedure and an example of database descriptions.

### Question Annotation

Database Description File.The Database Description File is a crucial resource designed to aid annotators in comprehending database values, thereby allowing them to ask insightful questions. It offers two primary pieces of information regarding the database. **(1) Full schema names:** database table and column names are frequently abbreviated, which are difficult to understand. **(2) Value description:** this aspect is particularly useful when phrases or tokens in a question do not directly match values in the database.

External Knowledge Evidence.In our study of professional data analysis, we find that external knowledge evidence is required to map the natural language instructions into counterpart database values. Therefore, we collect and classify such evidence into four categories: **(1) Numeric Reasoning Knowledge:** this category refers to the mathematical computation required for certain SQL operations. In our benchmark, we present 8 basic math operations, including 4 complex operations as : MINUS, ADDITION, DIVISION, MULTIPLY. Bird also contains compositional operations over basic ones, such as percentages, formulas, etc. **(2) Domain Knowledge:** this category consists of domain-specific knowledge that is utilized to generate SQL operations [10; 57]. For instance, a business analyst in the banking business requires knowledge of financial indicators such as return on investment and net income in order to generate effective SQL queries. **(3) Synonym Knowledge:** this category includes words or expressions that have the same or similar meanings regardless of how they are phrased differently . **(4) Value Illustration:** this category refers to detailed descriptions of database values, including value types, value categories, and the mapping combinations of columns and values that correspond to entities, for example: "center" can be represented by "pos = C" in the database professional_basketball.

### SQL Annotation

Double-Blind Annotation.As shown in Figure 2 (b), we employ a double-blind approach  for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values. The more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in Bird, and the external knowledge evidence sentences are recorded for each SQL if utilized.

Examination.Experts evaluate each text-to-SQL pair to ensure the highest quality of data. The evaluation process includes two dimensions: SQL validness, and text-knowledge-SQL alignment. Firstly, the SQL validness will be confirmed that each SQL is executable and can return a valid result from the database. The "valid result" refers to the set of results that is not "NULL". If the executed result set is "NULL", experts will make slight changes to the conditions of the questions until the associated SQLs can provide a valid result set. Secondly, text-knowledge-SQL alignment is involved to ensure that each SQL can be generated with the given texts and knowledge evidence. If the evidence is insufficient to generate the SQL or contains errors, experts will be in charge of correcting them.

## 4 Data Statistics

Overall StatisticsTable 1 presents an overview comparison between Bird and other cross-domain text-to-SQL benchmarks. As the statistics demonstrate, Bird is a large-scale cross-domain benchmark, covering complex SQL functions, knowledge reasoning, and efficiency evaluation.

Question StatisticsDatabase values bring more challenges in text-to-SQLs. In order to underscore this, we classify questions into two macro-categories: Fundamental Type and Reasoning Type, and each contains 4-5 micro-categories in detail. The Fundamental Type of questions refers to those that can be answered without database value comprehension. It contains Match-based (83.9%), Ranking (20.3%), Comparison (16.7%), Counting (30.4%), Aggregation (15.7%). TheReasoning Type entails questions that demand the external knowledge grounding on values, which is exclusive to Bird. To be specific, the questions about Domain Knowledge (23.6%), Numeric Computing (24.5%), Synonym (7.2%), Value Illustration (70.1%), are involved in Bird. There are ample examples in Appendix B.3. In addition, we observe that 70.1% of the questions need value illustrations. This indicates that more real-world questions in text-to-SQL applications demand a thorough understanding of database values, which is consistent with our motivation for creating the Bird benchmark.

Database StatisticsIn Bird, we investigate the distribution of database domains, database size, and value types. Figure 3 (a) presents a detailed distribution of domains and their counterpart databases in a sunburst diagram for both training and development sets. The area of each semi-circle corresponds to the number of text-to-SQL pairs in this database. Figure 3 (a) also shows the size distributions of databases. The darker color means a larger size of databases, and vice versa. For example, the database Donor is the largest database with 4.5 GB in this dataset. Furthermore, we observe from Figure 3 (b) that a considerable proportion of Bird's data comprises date-related values. Considering that real-world applications often rely on time-sensitive data , the prevalence of such questions highlights the practical purposes.

SQL StatisticsWe provide the complexity and diversity of SQLs in Bird. As illustrated in Figure 4, we present a comprehensive distribution analysis of SQLs across four dimensions. No.Toks / SQL and No.JOINs / SQL demonstrate the intricacy of the SQLs in Bird. No.of Keywords and No.n-grams / SQL (n=3) serve as the support for the diverse patterns of SQLs since we decouple the question and SQL annotation procedures to make the situation more realistic .

## 5 Evaluation Metrics

In contexts of practical data analysis, text-to-SQL models are prioritized for delivering expected results accurately and efficiently. Thus we provide two metrics in Bird, execution accuracy (EX)

  
**Dataset** & **\# Example** & **\# DB** & **\# Table/DB** & **\# Row/DB** & **Function** & **Knowledge** & **Efficiency** \\  WikiSQL  & 80,654 & 26,521 & 1 & 17 & ✘ & ✘ & ✘ \\ Spider & 10,181 & 200 & 5.1 & 2K & ✘ & ✘ & ✘ \\ KaggleDBQA  & 272 & 8 & 2.3 & 280K & ✘ & ✓ & ✘ \\  Bird & 12,751 & 95 & 7.3 & 549K & ✓ & ✓ & ✓ \\   

Table 1: An overview comparison between Bird and other cross-domain text-to-SQL benchmarks. In SQL, Function pertains to the SQL functions (Appendix B.11). Knowledge refers to whether or not this dataset necessitates external knowledge reasoning from the model. Efficiency refers to whether or not this dataset takes into consideration execution efficiency.

Figure 3: This is a comprehensive database distribution in the Bird. a) shows the domain and size distribution of each database. And b) shows the data type distribution of databases.

and valid efficiency score (VES) to evaluate text-to-SQL parsers confronted with large real-world database values.

Execution Accuracy (EX)EX is defined as the proportion of examples in the evaluation set for which the executed results of both the predicted and ground-truth SQLs are identical, relative to the overall number of SQLs . Considering the result set as \(V_{n}\) executed by the \(n^{th}\) ground-truth SQL \(Y_{n}\), and the result set \(_{n}\) executed by the predicted SQL \(_{n}\), EX can be computed by:

\[=^{N}(V_{n},_{n})}{N}, \]

where \(()\) is an indicator function, which can be represented as:

\[(V,)=1,&V=\\ 0,&V \]

Valid Efficiency Score (VES)VES is designed to measure the efficiency of valid SQLs generated by models. It is worth noting that the term "valid SQLs" refers to predicted SQL queries whose result sets align with those of the ground-truth SQLs. Any SQL queries that fail to fetch the correct values will be declared invalid since they are totally useless if they cannot fulfill the user requests, regardless of their efficiency. In this case, the VES metric considers both the efficiency and accuracy of execution results, providing a comprehensive evaluation of a model's performance. Formally, the VES can be expressed as:

\[=^{N}(V_{n},_{n})(Y_{n},_{n})}{N},(Y_{n},_{n})=(Y_{n})}{(_{n})}} \]

where \(()\) denotes the relative execution efficiency of predicted SQL in comparison to ground-truth SQL, allowing for machine status-related uncertainty. \(()\) is a function to measure the absolute execution efficiency for each SQL in a given environment++. Furthermore, we incorporate the square root function to minimize random instances that are abnormally faster or slower than the ground-truth SQLs. Here, efficiency can refer to running time, throughput, memory cost, or merged metrics. In Bird, we consider the running time mainly at this time. Appendix B.8 provides a detailed description of the VES.

Footnote ‡: In Bird evaluation, we run 100 times for each SQL in the same CPU and evaluate average results after dropping the outliers.

## 6 Experiments

### Baseline Models

We present the performance of two types of baseline models in Bird. The first type of model is based on fine-tuning (FT) techniques, which outputs SQL by tuning all parameters of language models to learn the annotated train set. On the other hand, the second type of model based on

Figure 4: A comparative statistical analysis of SQL queries in the Bird dataset and other cross-domain text-to-SQL benchmarks.

in-context learning (ICL), can generate results without additional training. In FT models, we select T5 family  as the main baseline models. For ICL-based models, we provide zero-shot results of Codex (code-davinci-002), ChatGPT (gpt-3.5-turbo), GPT-4 (gpt-4-32k), Claude-2 (claude-2.0), Palm-2 (text-bison-001). Additionally, we also implement a state-of-the-art (SOTA) model of SPIDER, DIN-SQL , to evaluate the challenges proposed by the Bird dataset. Table 2, Table 3 and Figure 5 present the overall results of advanced language models on Bird.

### Execution Accuracy Analysis

Table 2 and Figure 5 presents stratified performances of various models in BIRD. GPT-4 surpasses all baseline language models. Claude-2 closely follows, demonstrating outstanding abilities in semantic parsing and knowledge reasoning. Further, the incorporation of a dedicated reasoning prompt by , enables DIN-SQL + GPT-4 to achieve a new state-of-the-art result on BIRD. It contains value sampling, few-shot demonstrations, and self-correction. Despite considerable advancements in Language Model Learning (LLMs) and prompt intelligence, the performance of these models lags obviously behind human capabilities. Not only does this gap highlight the complex nature of BIRD, but it also presents opportunities for uncovering more capable models or advanced reasoning prompt methods applicable to real-world text-to-SQL scenarios.

    &  &  \\   & **w/o knowledge** & **w/ knowledge** & **w/o knowledge** & **w/ knowledge** \\  \\ T5-Base & 6.32 & 11.54 (+5.22) & 7.06 & 12.89 (+5.83) \\ T5-Large & 9.71 & 19.75 (+10.04) & 10.38 & 20.94 (+10.56) \\ T5-3B & 10.37 & 23.34 (+12.97) & 11.17 & 24.05 (+12.88) \\  & & _ICL-based_ & & \\ Palm-2 & 18.77 & 27.38 (+8.61) & 24.71 & 33.04 (+8.33) \\ Codex & 25.42 & 34.35 (+8.93) & 24.86 & 36.47 (+11.61) \\ ChatGPT & 24.05 & 37.22 (+13.17) & 26.77 & 39.30 (+12.53) \\ ChatGPT + COT & 25.88 & 36.64 (+10.76) & 28.95 & 40.08 (+11.24) \\ Claude-2 & 28.29 & 42.70 (+14.41) & 34.60 & 49.02 (+14.42) \\ GPT-4 & **30.90** & 46.35 (+15.45) & 34.88 & 54.89 (+20.01) \\ GPT-4 + DIN-SQL & - & **50.72** & - & 55.90 \\ Human Performance & - & - & **72.37** & **92.96** (+20.59) \\   

Table 2: The Execution Accuracy (EX) of advanced text-to-SQL models in Bird. The human performance is also provided.

Figure 5: A bar chart provides a clear visualization of the performance of advanced models on BIRD.

### Efficiency Analysis

According to Table 3, we can observe that models with higher EX can more possibly achieve higher VES. This can be explained by the prerequisite that text-to-SQL models must accurately predict results in order to attain a higher VES, which fulfills the practical purpose.

**Two-Stage Optimization.** Intuitively, the goal of text-to-efficient-SQL conversion can be decomposed into two sub-stages. Following previous text-to-SQL tasks, the first sub-stage, semantic parsing, concentrates on accurately converting questions into SQL queries. The second sub-stage involves optimizing the SQL queries, rewriting them to be more efficient while maintaining the same results . To demonstrate the efficacy of this approach, we selected 10 random examples from the development set where ChatGPT accurately predicted the results. Then, our specialists optimize these queries based on the established query optimization rules [28; 34; 62]. We observe that the two-stage optimization leads to an average time-saving of 77.75% while keeping the same results.

**Chat w/ Database.** Bird introduces the novel mode of "Chat With Database", which enables models to be aware of data types and distributions by generating global SQL queries that interact with databases. This approach lays the foundation for the development of more effective and efficient SQL queries. As observed in the experiment, the time-saving percentage of the SQL queries can

    &  &  \\   & **w/o knowledge** & **w/ knowledge**_**_FT-based_ & **w/o knowledge** & **w/ knowledge** \\  T5-Base & 7.78 & 12.90 (+5.12) & 8.97 & 14.71 (+5.74) \\ T5-Large & 9.90 & 22.74 (+12.84) & 12.25 & 25.00 (+12.75) \\ T5-3B & 13.62 & 25.57 (+11.95) & 15.17 & 27.80 (+12.63) \\  & & _ICL-based_ & & & \\ Palm-2 & 20.82 & 28.64 (+7.82) & 31.32 & 38.41 (+7.09) \\ Codex & 33.37 & 43.41 (+10.04) & 35.40 & 41.60 (+6.20) \\ ChatGPT & 27.97 & 43.81 (+15.84) & 36.68 & 51.40 (+14.72) \\ ChatGPT + COT & 32.33 & 42.30 (+9.97) & 49.69 & 56.56 (+6.87) \\ Claude-2 & 32.75 & 45.28 (+12.53) & 39.32 & 55.77 (+16.45) \\ GPT-4 & **34.60** & 49.77 (+15.17) & 40.20 & 60.77 (+20.57) \\ GPT-4 + DIN-SQL & - & **58.79** & - & 59.44 \\ Human Performance & - & - & **70.36** & **90.27** (+19.91) \\   

Table 3: The Valid Efficiency Score (VES) of advanced text-to-SQL models in Bird. The human performance is also presented.

Figure 6: The EX results of the same baseline models on the Spider and Bird dev set.

be reached at 87.3% by configuring indexes within the database. The detailed efficiency analysis is presented in Appendix B.5.

### Knowledge Evidence Analysis

We implement each baseline model for both two scenarios. The first is **NOT** to provide the ground truth external knowledge evidence sentence (w/o knowledge) for each sample. The other testing bed is to provide such evidence (w/ knowledge) and make text-to-SQL models do knowledge grounding by themselves. As we discuss in Section 3.3, expert annotations on external knowledge evidence sentences are employed to enhance the model's comprehension of database values.

After being easily fed with the external knowledge evidence about the database values, all models have a clear improvement across the different difficulty levels as shown in Table 2 and Table 4. _This indicates that external knowledge evidence in Bird is effective and instructive for models to better understand the database values._ Also it illustrates that the database values are very important to text-to-SQL models when facing more real databases. Besides, LCL-based approaches have a better self-knowledge grounding capability and pre-trained SQL knowledge than FT smaller models with less than 5B parameters. Equipped with COT, ChatGPT can perform better, since multi-step reasoning is beneficial when the knowledge and data are low-resource. Despite this, we observe a decline or limited improvements in performance for ChatGPT + external knowledge evidence for COT version. We hypothesize that the internal multi-step knowledge reasoning of LLMs is not compatible with the way of external knowledge (evidence) in this situation. Therefore, the development of methods that effectively combine the strong multi-step self-reasoning capabilities of LLMs with external knowledge reasoning coherently presents a promising future direction .

### More Analysis

Fine-grained Category Analysis.Figure 7 provides a detailed comparison of various dimensions of sub-capabilities of advanced LLMs on BIRD. The results indicate that GPT-4 exhibits superior performance against ChatGPT and Claude-2 in all areas. Nevertheless, there is a notable disparity in the performance of ranking and numerical computing (math) among all the models. This limitation may suggest the inadequacy of contemporary LLMs for deep data science tasks because such tasks always incorporate mathematical computations and rankings within the context of vague user queries. Conversely, these models demonstrate relatively better performance in domain knowledge, synonym detection, and value illustration, which can be attributed to their adequate linguistic training and reasoning capabilities during the pretraining phases.

Human Performance.In order to activate the efforts of text-to-SQL studies to achieve an application-level performance in real-world scenarios, we provide human performance in Bird. Table 2, Table 3 shows that there's still a huge gap between even SOTA text-to-SQL models and human performance. The thorough introduction of procedures is in Appendix B.9.

Error Analysis.ChatGPT is currently the most prevalent and cost-efficient LLM. Therefore, the performance of ChatGPT is concentrated in this error analysis. The detailed analysis is in Appendix B.6. We observe 500 randomly sampled error cases, providing an in-depth assessment in the following

Figure 7: The fine-grained categorical evaluation of advanced large language models on Bird.

categories. **Wrong Schema Linking (41.6%)** pertains to the scenario where ChatGPT can accurately comprehend the structure of the database but erroneously associates it with inappropriate columns and tables. This demonstrates that the task of schema linking [43; 57], even in intricate and practical situations, continues to be a significant obstacle for models. **Misunderstanding Database Content (40.8%)** occurs when ChatGPT either fails to recall the correct database structure (e.g., rtype doesn't belong to the satscores table) or generates fake schema items (e.g., lap_records is not appearing in the formula_1 database and many values are predicted incorrectly) especially when the database is very large. In this case, how to make ChatGPT really understand database structure and values  is still a pain point topic in LLMs. **Misunderstanding Knowledge Evidence (17.6%)** refers to cases in which the model does not accurately interpret human-annotated evidence. An instance is that ChatGPT directly copies the formula DIVIDE(SUM(spent), COUNT(spent)). This finding demonstrates that ChatGPT exhibits a lack of robustness in response to unfamiliar prompts or knowledge, causing it to directly replicate formulas without considering SQL syntax . We also observe that ChatGPT occasionally employs incorrect keywords (e.g., misusing the MySQL Year() function instead of an SQLite function STRFFIME()) or exhibits decoding errors.

## 7 Related Work

High-quality datasets are crucial for advancing various natural language processing tasks, including text-to-SQL. Early single-domain text-to-SQL datasets like GeoQuery , ATIS , and Restaurant  targeted specific information retrieval tasks, while more recent datasets such as WikiSQL  and Spider propose cross-domain dataset to require domain generalization. However, most cross-domain text-to-SQL datasets still emphasize database schema rather than values, diverging from real-world scenarios. KaggleDBQA  addressed this by constructing 272 text-to-SQL pairs from eight databases on Kaggle, while other datasets like EHRSQL , SEDE , and MIMICSQL  collected diverse, large-value databases with more professional SQL queries. Despite these advancements, these datasets remain single-domain focused. Recent work has explored knowledge-intensive text-to-SQL benchmarks [10; 57], aiding experts in real-world analysis through knowledge grounding. Bird is the first large-scale benchmark to incorporate these real-world features, emphasizing database values.

## 8 Limitation and Future work

Despite the high quality of SQL annotation produced by double-blind annotation, the procedure is resource-intensive. Future research could explore a human-computer interaction (HCI) based approach, incorporating advanced AI systems such as GPT-4 for taking parts of annotation duties, to maintain data quality while reducing human effort. In addition, SQLite was chosen as the primary SQL codebase for previous text-to-SQL benchmarks and this study since it's friendly to users. While it presents difficulties in fetching Query Execution Plans (QEP) for precise efficiency computation and adapting to different SQL syntaxes. Future work will include PostgreSQL and MySQL versions of Bird to resolve these limitations and provide a more robust research environment for both NLP and DB experts.

## 9 Conclusion

In this paper, we introduce Bird, a large-scale cross-domain, text-to-SQL benchmark with a particular focus on large database values. Bird mitigates the gap between text-to-SQL research and real-world applications by exploring three additional challenges: 1) handling large and dirty database values, 2) external knowledge evidence, and 3) optimizing SQL execution efficiency. Our experimental results demonstrate that Bird presents a more daunting challenge compared to existing benchmarks since even the most popular and powerful LLM, ChatGPT, falls significantly short of human performance. This leaves plenty of room for improvement and innovation in the text-to-SQL tasks. Moreover, our thorough efficiency and error analyses provide valuable insights and directions for future research, paving the way for the development of more advanced and practical text-to-SQL solutions in real-world scenarios.