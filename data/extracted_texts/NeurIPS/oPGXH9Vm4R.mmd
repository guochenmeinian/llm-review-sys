# ZipIt!: Multitask Model Merging _without Training_

George Stoica1

Daniel Bolya1

Jakob Bjorner

**Pratik Ramesh**

**Taylor Hearn**

**Judy Hoffman**

Georgia Tech

{gstoica3,dbolya,jakob_bjorner,pramesh3,thearn6,judy}@gatech.edu

Equal Contribution. Code: https://github.com/gstoica27/ZipIt.

###### Abstract

We tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model **without any additional training**. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features _within_ each model by defining a general "zip" operation. Second, we add support for _partially zipping_ the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for a staggering 20-50% improvement over prior work,

## 1 Introduction

Combining multiple models into one _without training_ has recently started to gain traction in the vision community. Model Soups  can add multiple models finetuned from the same pretrained initialization to improve accuracy and robustness. Git Re-Basin  generalizes further to models trained on the same data but with different initializations, though with a significant accuracy drop. REPAIR  improves on Git Re-Basin by adding new parameters and adjusting model batch norms where applicable. However, all of these methods only combine models trained on the same task. In this paper, we take this line of work to a logical extreme: merging differently initialized models trained on _completely separate_ tasks (see Fig. 1ab). We show that this is an incredibly difficult problem for prior work and employ two simple strategies to make it feasible.

First, prior work focuses on _permuting_ one model to the other when merging them. This inherently assumes that most features _across_ them are correlated. But, this is not always the case for models trained on different tasks. Instead, we generalize model merging to support "zipping" any combination of correlated features _within_ and _across_ each model. We find that on some tasks, this alone improves accuracy **by up to 20%** vs. permutation-based approaches

Second, the features of models trained on disjoint tasks become less correlated over the course of the network . Thus, we introduce _partial zipping_, which allows us to only "zip" up to a specified layer. Afterward, we feed the merged model's outputs to the remaining unmerged layers of the original networks, creating a multi-head model. Partially zipping can improve accuracy **by over 15%**.

ZipIt! (Fig. 1c) incorporates both strategies to "zip" models trained on different tasks into a single multitask model _without retraining_. ZipIt! is general and supports merging arbitrary models of the same architecture together (Sec. 3). We validate ZipIt! by merging models trained on different tasks (including classification and segmentation) into one, significantly outperforming prior work (Sec. 4).

## 2 Related Work

Model merging combines the weights of two or more models into a one. Our work differs from prior work in that we merge differently initialized models trained on disjoint tasks (Fig. 1) without training.

**Merging Finetuned Models.** If two models are finetuned from the same pretrained checkpoint, they often lie in the same error minima . [6; 7; 8; 9; 10; 11] have exploited this property to average together the weights of a model at different stages of training. [12; 13; 14; 15; 16] use an "exponential moving average" of training checkpoints as a teacher for additional self-supervised learning. Other works, [1; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27] merge models fully finetuned to improve performance on a task. Our setting differs, as we do not assume the same initialization.

**Merging Differently Initialized Models.** Works in this space often merge models trained on the same task and rely on mode connectivity [28; 29; 30; 31; 32; 33; 34], as differently initialized models may not lie in the same error minima [29; 35]. Most recent work follows the intuition formalized by  that models permuted to the same loss minima can be merged by averaging their weights [35; 2; 3; 36]. Similar to ZipIt!  merges models of different tasks, but requires jointly finetuning on all tasks after each layer merge. As far as we are aware, we present the first _general method_ to successfully merge models trained on disjoint tasks _without additional training_.

## 3 ZipIt!

In this work, we treat model merging as combining the checkpoints (i.e., collection of weights) of multiple models layer-by-layer into a single checkpoint that can perform all the tasks of its constituents. Consider a model \(\) as a collection of layers \(L_{i}\), each of which may have some parameters (e.g., \(W_{i},b_{i}\) for a linear layer). Suppose \(L_{i}\) is a linear layer with parameters \(W_{i}^{n_{i} m_{i}},b_{i}^{n_{i}}\) with input features \(x^{m_{i}}\) and outputs features \(f_{i}^{n_{i}}\). Then, \(f_{i}=L_{i}(x)=W_{i}x+b_{i}\).

Our goal is to take \(L_{i}^{A}^{A}\) from a model A and \(L_{i}^{B}^{B}\) from a model B and merge them into a layer \(L_{i}^{*}\) that combines their feature spaces such that information from both \(f_{i}^{A}\) and \(f_{i}^{B}\) is retained in \(f_{i}^{*}\). We accomplish this by merging each layer of one model with the corresponding layer in the other, both merging features _across_ both layers and _within_ the same layer. This is in contrast to permutation-based merging method, which only combine features _across_ layers.

**Problems with Permutation.** Permutation methods posit that model B can be moved to the same loss minima as model A via permutation with high-likelihood . However, this is unlikely when models are trained on different tasks because each model optimizes for task-specific minimas. In this case the optimal permutation of model B to model A lies in a strong minima on task B but _may not_ lie in a minima on task A, as shown in Fig. 2. This causes the interpolated model to perform worse than either of the two original models. Thus, we explore alternative merging methods.

**Why should we merge _within_?** Features of models trained on different tasks may be dissimilar, as the models solve different problems. Instead, those features may be more compatible with others within the same model, which would better retain performance when combined.

**What is our merge strategy?** Prior work obtains \(f_{i}^{*}\) by combining one feature from \(f_{i}^{A}\) and one from \(f_{i}^{B}\). [38; 3] determine which features to merge by computing the pairwise correlations between

Figure 1: **Setting and ZipIt!** (a) Prior work merges models from the **same** dataset with the **same** label sets: e.g., merging two models both trained to classify dog breeds. (b) Our setting expands to merging models from **different** datasets with **different** label sets: e.g., merging a model that classifies dog breeds with one that classifies bird species. (c) **ZipIt!** merges these models _without retraining_ by identifying shared features. Depending on the task, ZipIt! can nearly match ensemble performance.

the neuron activations of \(f_{i}^{A}\) and \(f_{i}^{B}\) over a set of images. In contrast, our approach can also obtain \(f_{i}^{*}\) by combining two features from just \(f_{i}^{A}\) or \(f_{i}^{B}\). We compute the same correlations, but also include those of \(f_{i}^{A}\) and \(f_{i}^{B}\) each with themselves over the same data. We then greedily choose without replacement the pairs with highest correlation and average their features to obtain \(f_{i}^{*}\).

**Merging a layer.** For every layer, we first concatenate \(f_{i}^{A}\) and \(f_{i}^{B}\) into \(f_{i}^{A}\|f_{i}^{B}^{2n_{i}}\), and define a "merge matrix" \(M_{i}^{n_{i} 2n_{i}}\) based on the matches from our greedy algorithm (i.e., \(f_{i}^{*}=M_{i}(f_{i}^{A}\|f_{i}^{B})\)). Thus, \(M_{i}\) merges the output spaces of \(L_{i}^{A}\), and \(L_{i}^{B}\) into \(L_{i}^{*}\). We then define an 'unmerge" matrix \(U_{i}^{2n_{i} n_{i}}\) s.t. it _undoes_ the merge operation: \(U_{i}f_{i}^{*} f_{i}^{A}\|f_{i}^{B}\). Applying \(U_{i}\) ensures the input spaces of the future layers \(L_{i+1}^{A}\), and \(L_{i+1}^{B}\) are aligned with current merged layer \(L_{i}^{*}\). Once all matrices are computed at each layer, we obtain \(L_{i}^{*}\) for a linear layer by

\[W_{i}^{*}=M_{i}W_{i}^{A}&0\\ 0&W_{i}^{B}U_{i-1} b_{i}^{*}=M_{i}b_{i}^{A}\\ b_{i}^{B}.\]

\(L_{i}^{*}\) is similarly computed on other layer-types (e.g., apply \(M_{i},U_{i-1}\) over the channels of convs.).

**Partial Zipping.** Sometimes, later layers in the networks have very uncorrelated (dissimilar) outputs. Forcibly zipping these would lead to meaningless features. In this case, we can perform a _partial zip_. That is, we zip most of the layers together, but leave the later ones _unzipped_--obtaining a multihead model.

**Merging Many Models (\(\)).** Sometimes, we'd like to merge more than two models together. To do this, we allow "repeated matches": we replace matched features from our algorithm with the resulting merged feature instead of removing them completely. To ensure that one feature doesn't get merged endlessly, we set the correlations of the new feature to be the minimum of the old features' similarities weighted by \((0,1]\). We find a small value of \(\) typically works best.

**Within-Model Merging Budget (\(\)).** To demonstrate the effectiveness of same-model merges, we introduce a "budget" parameter \(\) that denotes what percent of total merged features can come from models merging within themselves, with each model receiving an equal portion of this budget. A budget of 0 only allows feature merging across models and yields a permutation-merge.

## 4 Results

There's no standard benchmark to evaluate merging approaches on models from distinct tasks, so we construct our own. We evaluate our approach in two different settings. (1) A versatile test-bed: disjoint category splits of the same dataset (i.e., _same dataset and different label sets_). (2) A very challenging setting: completely different datasets and tasks (i.e., _different datasets and label sets_).

We compare to three baselines: Git Re-Basin , Weight Averaging (W. Avg) and Permute. W. Avg involves simply average all model parameters to be merged, while we design Permute to use linear sum assignment to find optimal permutations (following ) for merging. Note that Permute is a

Figure 2: **Task Loss Landscapes** for models in Tab. 0(b). Model A and Model B lie in low loss minimas for their own tasks, but _not for the other task_. Thus, any interpolation between Model A and a permuted Model B (e.g., Git Re-basin) lies outside the minima _for both tasks_ and thus performs poorly. In contrast, ZipIt! improves the merge by finding a model that lies in a loss minima for both.

_strong_ baseline we create and is more accurate than Git Re-Basin in our settings. For our method, ZipIt!a/m indicates that \(n\) out of the \(m\) layers in the network have been zipped (Sec. 3). Note, all our models have _different initializations_.

**Disjoint Category Splits.** In Tab. 0(a), we merge five pairs of models trained on disjoint 5 class subsets of CIFAR-10 using ResNet-20 with a \(4\) width multiplier (denoted as ResNet-20\(\)4). We train with a CLIP-style loss  using CLIP text encodings of the class names as targets so that both models output into the same CLIP-space regardless of the category (required for ). We report: (1) joint accuracy--the accuracy of each model over _all_ classes across datasets, and (2) per task accuracy--the accuracy of each task individually and also their average. Overall, ZipIt! performs a staggering 20.7% better than the nearest baseline. If we allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting). We find similar results on disjoint 50 class splits of CIFAR-100 in Tab. 0(b) using an \(8\) width multiplier. ZipIt! again significantly outperforms baselines.

**Different Datasets.** We merge ResNet-50 models trained on: Stanford Dogs , Oxford Pets , CUB200 , and NABirds . In Tab. 2, we show the average per task accuracy from exhaustively merging each pair and the much more difficult setting of merging all four at once. We report the accuracy of our baselines by applying them up until the last layer, but we can't compare to  as it requires shared output space.

For pairs of models, ZipIt! slightly outperforms our permute baseline across all tasks and performs similarly when merging all 4 models at once. However, if we add capacity to the merged model through partial zipping, we perform up to 33% better on merging pairs and 50% better on merging all four models than the permute baseline.

We also merge the ResNet-50 backbone of a DeeplabV3  segmentation model that achieves 76.8% mIoU on PASCAL VOC  with an ImageNet-1k  classification model that obtains 77.8% accuracy. ZipIt! can still achieve good performance on _both_ tasks even with half of layers merged: obtaining 64.4% mIoU on PASCAL VOC and 60.9% accuracy on ImageNet-1k.

## 5 Conclusion

In this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks _without additional training_. We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities. We introduce ZipIt!, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings.

  & & & & Accuracies (\%) & & \\ Method & FLOPs (G) & Joint & Task A & Task B & Avg \\  Model A & 0.68 & 48.2\(\)0.0 & 97.0\(\)0.0 & 45.1\(\)0.0 & 71.0\(\)0.0 \\ Model B & 0.68 & 48.4\(\)0.0 & 94.1\(\)0.0 & 72.6\(\)0.0 & \\  W. Avg & 0.48\(\)0.0 & 34.1\(\)0.0 & 51.1\(\)0.0 & 60.8\(\)0.0 & \\ Git Re-Basin2 & 0.68 & 46.2\(\)0.0 & 76.8\(\)0.0 & 82.7\(\)0.0 & 79.8\(\)0.0 \\ Permute & 0.68 & 58.4\(\)0.0 & 86.6\(\)0.0 & 87.4\(\)0.0 & 87.4\(\)0.0 \\  ZipIt!a & 0.68 & **79.1\(\)0.0** & 92.9\(\)0.0 & 91.1\(\)0.0 & **92.1\(\)0.0** \\ Ensemble & 1.37 & 87.4\(\)0.0 & 97.0\(\)0.0 & 96.1\(\)0.0 & 96.6\(\)0.0 \\ ZipIt!a & 0.91 & **83.8\(\)**0.0 & **95.1\(\)**0.0 & **94.1\(\)**0.0 & **94.6\(\)0.0** \\ 

Table 1: **CIFAR Results.** ZipIt! vs. baselines on combining a model trained on half the classes (Task A) with one trained on the other half (Task B) _without extra training_. We report both joint (10/100-way) and per-task (5/50-way) accuracy. ZipIt! _significantly_ outperforms its baseline and closes in on the upper bound (ensemble accuracy). \(\) refers to .

  & & & & & & & \\ Method & FLOPs (G) & SD & OP & CUB & NAB & Avg \\   W. Avg & 4.11 & 12.9 & 18.2 & 13.9 & 0.2 & 11.3 \\ Permute & 4.11 & 46.2 & 47.6 & 35.6 & **13.5** & 35.7 \\ ZipIt!a & 4.11 & **46.9** & **50.7** & **38.0** & **12.7** & **37.1** \\  Ensemble & 8.22 & 77.2 & 81.1 & 71.0 & 77.2 & 73.5 \\ ZipIt!a & 6.39 & 62.6 & 71.2 & 62.8 & 53.0 & 62.4 \\ ZipIt!a & 7.42 & **66.5** & **75.8** & **65.6** & **66.8** & **68.7** \\  
  & & & & & & \\   W. Avg & 4.12 & 0.8 & 3.0 & 0.6 & 0.3 & 1.2 \\ Permute & 4.12 & 15.7 & 26.1 & **14.0** & **5.3** & 15.3 \\ ZipIt!a & 4.12 & **21.1** & **33.3** & **8.6** & **3.9** & **16.8** \\  ZipIt!a & 16.4 & 72.7 & 81.2 & 71.0 & 77.2 & 75.8 \\ ZipIt!a & 11.0 & 50.2 & 55.9 & **44.0** & **32.0** & **45.5** \\ ZipIt!a & 14.1 & **63.5** & **70.8** & **63.7** & **63.1** & **65.3** \\ 

Table 2: **Multi-Dataset Results.** Merging ResNet-50 models trained on _completely different datasets_: Stanford Dogs (SD), Oxford Pets (OP), CUB200 (CUB), and NABirds (NAB). We report average per-task accuracy over merging model pairs, and all four.