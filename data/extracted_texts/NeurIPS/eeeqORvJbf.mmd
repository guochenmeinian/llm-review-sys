# Learning and Processing the Ordinal Information of Temporal Sequences in Recurrent Neural Circuits

**Xiaolong Zou\({}^{1,4,5,*}\)**

zouxiaolong@qiyuanlab.com

**Zhikun Chu\({}^{2,*}\)**

chukunzhi@outlook.com

**Qinghai Guo\({}^{7}\)**

guoqinghai@huawei.com

**Jie Cheng\({}^{7}\)**

jiecheng2009@gmail.com

**Bo Hong\({}^{1,6}\)**

hongbo@tsinghua.edu.cn

**Si Wu\({}^{4,5}\)**

siwu@pku.edu.cn

**Yuanyuan Mi\({}^{3,}\)**

miyuan@tsinghua.edu.cn

1, Qiyuan Lab, Beijing, China.

2, Center for Neurointelligence, School of Medicine, Chongqing University.

3, Department of Psychology, Tsinghua University

4, School of Psychological and Cognitive Sciences,

Beijing Key Laboratory of Behavior and Mental Health,

IDG/McGovern Institute for Brain Research,

Peking-Tsinghua Center for Life Sciences, Center of Quantitative Biology,

Academy for Advanced Interdisciplinary Studies, Peking University, Beijing, China

5, Beijing Academy of Artificial Intelligence, Beijing,China.

6, Biomedical Engineering,School of Medicine, Tsinghua University

7, Huawei Technologies.

*: Equal contributions. \(\) : Corresponding authors.

**Abstract**

Temporal sequence processing is fundamental in brain cognitive functions. Experimental data has indicated that the representations of ordinal information and contents of temporal sequences are disentangled in the brain, but the neural mechanism underlying this disentanglement remains largely unclear. Here, we investigate how recurrent neural circuits learn to represent the abstract order structure of temporal sequences, and how the disentangled representation of order structure facilitates the processing of temporal sequences. We show that with an appropriate training protocol, a recurrent neural circuit can learn tree-structured attractor dynamics to encode the corresponding tree-structured orders of temporal sequences. This abstract temporal order template can then be bound with different contents, allowing for flexible and robust temporal sequence processing. Using a transfer learning task, we demonstrate that the reuse of a temporal order template facilitates the acquisition of new temporal sequences, if these sequences share the same or partial ordinal structure. Using a key-word spotting task, we demonstrate that the tree-structured attractor dynamics improves the robustness of temporal sequence discrimination, if the ordinal information is the key to differentiate these sequences. We hope that this study gives us insights into the mechanism of representing the ordinal information of temporal sequences in the brain, and helps us to develop brain-inspired temporal sequence processing algorithms.

Introduction

A temporal sequence involves a set of items or events that unfold in a specific order over time. Temporal sequence processing is fundamental in many cognitive functions , such as speech recognition, language comprehension, motor control, and memory formation. Effective temporal sequence processing relies on extracting the temporal structure of a sequence, which includes particularly the ordinal information, i.e., the order of events' occurrence. A large volume of studies has suggested that the representations of ordinal information and contents of a temporal sequence are disentangled in the brain [1; 2; 3; 4; 5; 6]. For instances, studies using single-unit recording in primates have found that a significant proportion of neurons in the dorsolateral prefrontal and intraparietal cortexes are sensitive to the temporal order of presented visual items, but not to their physical attributes , and similar temporal order representations were also observed in the auditory and linguistic sequence processing . The imaging study has shown that the human brain can encode the phonetic order during speech processing, regardless of the phonetic contents . Recent large-scale neuron recordings have revealed that the temporal order structure can be robustly encoded in a low-dimensional space formed by coordinated neuron population activities . The advantage of disentangled representation of order structure is that it enables the brain to flexibly and rapidly process temporal sequences via conjunctive coding , i.e, the brain can easily generate various temporal sequences by combining the temporal order with different contents. This was evident in a finger sequence learning task , where the brain stored a temporal order template and reused it to combine actions to generate various finger action sequences. Understanding how ordinal information is acquired and stored in the brain is crucial for us to understand how temporal sequences are processed in cognitive functions.

Remarkably, the representations of temporal orders of related sequences display a tree-like structure . For example, in speech recognition, spoken words are represented by temporal sequences formed by primitive syllables or phonemes. As illustrated in Fig.1A, the words "water" and "wash" share a common syllable "w-ao" at the first item and differ at the second syllable, and so do the words "your" and "year". Thus, the temporal orders of these four sequences form a two-layer tree: starting from the root, the words "water" and "wash" form a branch in the first layer as they share the same first syllable, and the words "your" and "year" form the other branch; in the second layer, the words "water" and "wash", and the words "your" and "year", are further branched due to their difference at the second syllable. Experimental data has indicated that such tree-structured representation of ordinal information is employed in the frontal cortex to support hierarchical motor control .

Despite that the representation of ordinal information in the brain has been well recognized in the field, the neural mechanism of implementing this representation remains largely unclear [1; 7]. A number of computational models has been proposed to mimic how temporal sequences are stored

Figure 1: (A) A schematic diagram illustrating the tree-structured temporal orders of four spoken words. Words ‘water’ and ‘wash’ form a branch in the first layer because of the shared syllable ‘w-ao’, and they are branched in the second layer due to the different second syllables, ‘dx-axr’ and ‘sh’. Similarly, words ‘your’ and ‘year’ form other branches. (B) A schematic diagram of our neural circuit model. It consists of three parts: an input layer of dimension \(M\) conveying temporal sequences, a recurrent neural network (RNN) of \(N\) neurons encoding the ordinal structure, and an output layer discriminating \(K\) different temporal sequences.

in neural circuits, and they typically consider that neurons are asymmetrically connected along the path of a sequence of network states representing the temporal sequence [10; 11; 12; 15]. Among the proposed models, the framework of heteroclinic channel is inspiring , which considers that a temporal sequence is stored as a sequence of saddle points connected by unstable paths in the state space of the network, forming the so-called a heteroclinic channel, and under the drive of external inputs, the network state evolves along the channel to activate stored items in the right time order. In most previous works [11; 13; 14; 15], the asymmetric neuronal connections accounting for storing temporal sequences are either designed by hand or constructed by applying an asymmetric Hebbian learning rule (note that the application of a Hebbian learning rule implicitly assumes that neural representations of sequential items are known, which does not really model the learning process), and they did not address how a neural circuit learns to represent the temporal order structure from data. A few studies explored how temporal sequences are learned in a recurrent neural network , but they only considered the simple chain-structured temporal order and did not study the more general tree-structured orders. The machine learning society also studied the learning of temporal sequences (such as speech) by using artificial neural networks, but they have not considered the representation of disentangled ordinal information .

In this work, we explore how biologically plausible recurrent neural circuits learn to represent the tree-structured orders of temporal sequences and how this disentangled representation of order structure facilitates the processing of temporal sequences. We show that with an appropriate training protocol, a recurrent neural circuit can learn a set of tree-structured attractor states to encode the corresponding tree-structured orders of temporal sequences, where each path from the root to a leaf represents a pattern of temporal order, with nodes on the path indicating the order hierarchy (see Fig.1A). This abstract order template can then be bound with different contents, allowing for flexible and robust temporal sequence processing. To demonstrate this advantage, we carry out two experiments. First, in a transfer learning task, we show that the reuse of the order template accelerates the acquisition of new temporal sequences, if these sequences share the same or partial ordinal structure. Second, in a key-word spotting task, we show that using the tree-structured attractor dynamics improves the robustness of temporal sequence discrimination, if the ordinal structure is the key to differentiate these sequences. We hope that this study facilitates our understanding of how temporal sequences are represented and processed in the brain, and helps us to develop brain-inspired algorithms for temporal sequence processing.

## 2 Learning the ordinal structure of temporal sequences

### The neural circuit model

The model we consider consists of three parts (Fig.1B): an input layer which conveys the information of temporal sequences, a recurrent neural network (RNN) which stores the ordinal structure of temporal sequences, and an output layer which discriminates temporal sequences. Our idea is that by training the model to perform a temporal sequence discrimination task, the RNN learns to represent the ordinal structure of given temporal sequences in its dynamics. The details of the model are introduced below.

Denote \((t)=\{I_{m}(t)\}\), for \(m=1,,M\), with \(M\) the dimension, the activity of the input layer conveying the temporal sequence. The input layer is connected to the RNN through a feedforward connection matrix \(^{in}=\{W^{in}_{nm}\}\), for \(m=1,,M\) and \(n=1,,N\), with \(N\) the number of neurons in the RNN. Neurons in the RNN are recurrently connected with each other with a matrix \(^{rec}=\{W^{rec}_{nj}\}\), for \(n,j=1,,N\), and they also receive a background input \(^{b}=\{I^{b}_{n}\}\), for \(n=1,,N\). Denote \(x_{n}(t)\) the total input received by the \(n\)th neuron in the RNN at time \(t\) and \(r_{n}(t)\) the corresponding neuronal activity. The dynamics of \(x_{n}(t)\) is written as,

\[(t)}{dt}=-x_{n}(t)+_{j=1}^{N}W^{rec}_{nj}r_{j}(t)+_{m= 1}^{M}W^{in}_{nm}I_{m}(t)+I^{b}_{n},\] (1)

where \(\) is the time constant. The nonlinear relationship between \(x_{n}(t)\) and \(r_{n}(t)\) is given by \(r_{n}(t)=(t)]}\).

The RNN is connected to the output layer through a feedforward matrix \(^{out}=\{W^{out}_{kn}\}\), for \(n=1,,N\) and \(k=1,,K\), with \(K\) the number of neurons in the output layer. \(K\) is also the number of temporal sequences, since each neuron reads out one class of sequences. Denote \(y_{k}(t)\) the activity of the \(k\)th neuron in the output layer, whose dynamics is given by,

\[y_{k}(t)=_{n=1}^{N}W_{kn}^{out}r_{n}(t).\] (2)

### The training protocol

The details of the training protocol are presented in Supplementary Information (SI). Here, we highlight two settings which are critical for the RNN to extract the ordinal structure of temporal sequences.

Firstly, we augment training data by varying the duration between neighbouring items in a temporal sequence (see illustration in Fig.2A). Specifically, this is done by adding noisy signals of varied duration between neighbouring primitive items of the temporal sequence (see more details below). The underlying mechanism is intuitively understandable. The augmented data imposes that the model needs to realize invariant recognition with respect to the warping of a temporal sequence, which forces the RNN to extract the abstract ordinal structure of the temporal sequence. This is also biologically reasonable, as in speech recognition [2; 3], the varied speed of spoken words forces the brain to process temporal sequences relying mainly on their ordinal information.

Secondly, we set the target function, i.e., the target activity of a read-out neuron in response to the temporal sequence it encodes, to be continuous in time, and its value increases step by step as items in the temporal sequence unfold one by one (see illustration in Fig.2A). The underlying mechanism is intuitively understandable. Since not all temporal sequences are differentiable by early items, neuronal responses should be low initially, and their values increase along with the exposure of followed items to reflect that sequence recognition is a process of evidence accumulation over time. Interestingly, such ramping activity of read-out neurons agree with the experimental finding on decision-making neurons in monkeys [1; 2; 3].

After constructing input data and target functions, we apply the standard algorithm, back-propagation through time, to train the model, which optimizes the parameters \((^{in},^{rec},^{out},^{bg})\). For details, see Sec. A in SI.

### A synthetic task and the model behavior

To demonstrate that our model works, we first apply it to a synthetic task of discriminating four temporal sequences (\(K=4\)). These four temporal sequences are constructed by combining two of three primitive items with varied orders (like syllables forming spoken words). As illustrated in Fig.2A, the three primitive items are 3-dimensional (\(M=3\)) continuous-time vectors within an interval \( t\), denoted as \(=^{T},=^{T}\), and \(=^{T}\), respectively. The contents of four temporal sequences are expressed as \(^{k}=\{S_{1}^{k},S_{2}^{k}\}\), with \(k\) the index of the sequence and \(S_{1}^{k},S_{2}^{k}\) the first and second items. Since in this study, the exact duration of a temporal sequence is not important, for clearance, we also use \(^{k}\) to denote a sequence having the contents \(\{S_{1}^{k},S_{2}^{k}\}\) with unspecified duration. The four classes of temporal sequences are therefore expressed as, \(^{1}=\{,\}\), \(^{2}=\{,\}\), \(^{3}=\{,\}\), \(^{4}=\{,\}\), and their temporal orders form a two-layer tree structure as shown in Fig.2B. To augment the training data, we carry out two operations. Firstly, we randomly set the duration between neighboring primitive items to be \( T\), which satisfies a uniform distribution in the range of \([0, T_{max}]\). Secondly, we add noises in the duration, which are given by \(_{i}(t)=_{i}(t)\), for \(i=1,,M\) and \(t(0, T)\), where \(_{i}(t)\) denotes Gaussian white noises of zero mean and unit variance, and \(\) the noise strength.

After training, we observe that the model learns to discriminate four temporal sequences in a coarse-to-fine manner. An example of discriminating the temporal sequence \(^{1}\) is presented in Fig.2C. Initially, both activities of neurons \(1\) (red) and \(2\) (green) increase, since \(^{1}\) and \(^{2}\) are not differentiable by their first item, i.e., \(S_{1}^{1}=S_{1}^{2}\); while neurons \(3\) (orange) and \(4\) (blue) keep silent. When the second item appears, only the activity of neuron \(1\) keeps increasing, while the activity of neuron \(2\) drops, indicating that the model recognizes \(^{1}\). The above results can be generalized to more than four temporal sequences, see Sec. A in SI for \(K=8,12\).

### Visualizing the learned tree-structured attractor dynamics

#### 2.4.1 Tree-structured representation trajectories

To visualize the high-dimensional dynamics of the RNN (\(N=50\)), we apply principle component analysis (PCA) to reduce its dimensionality. PCA is done by using the activities of RNN neurons in response to \(1000\) testing temporal sequences (for details, see Sec. B.1 in SI). It turns out the top \(3\) PCs explain nearly \(90\%\) of the variance of neuronal responses (Fig.2D, upper panel), indicating that after training, the RNN performs computation in a low-dimensional subspace. By projecting RNN activities onto the top \(3\) PCs, we obtain the trajectories of the network state in responses to the four temporal sequences (averaged over \(25\) trials), and observe that they display a tree structure, agreeing with the ordinal structure of the learned four temporal sequences (Fig.2E, trajectories for different sequences are marked by different colours).

Figure 2: Unveiling the performance and the mechanism of the model using synthetic data. (A) Illustrating the constructions of temporal sequences and target functions. Two primitive items, \((,)\), are combined to form the temporal sequence \(^{1}\), with an interval \( T\) inserted between two items, where \( T\) is sampled from a uniform distribution in the range of \([0, T_{max}]\). Noises are added in the interval. Along with the unfold of two items in \(^{1}\), the target function of read-out neuron \(1\) (red) increases step by step, the target function of read-out neuron \(2\) (green) first increases and then decreases, and the target functions of read-out neurons \(3\) (orange) and \(4\) (blue) always keep silent. (B) The orders of four temporal sequences \((^{1},^{2},^{3},^{4})\) form a two-layer tree. (C) An example of the model performance when recognizing the temporal sequence \(^{1}\). It operates in a coarse-to-fine manner. When the first item appears, both the activities of read-out neurons \(1\) and \(2\) increase, since \(^{1}\) and \(^{1}\) are not differentiable at this stage; when the second item appears, only the activity of read-out neuron \(1\) keeps increasing, indicating the model accomplishes the discrimination task. (D) (upper panel) PCA analysis shows that the top 3 PCs explain nearly \(90\%\) of the variance of neuronal responses in the RNN. (lower panel) The eigenvalue distribution of the attractor state \(S_{1}^{1,2}\) in Fig.2E. (E) The visualization of the tree-structured neural response trajectories to four temporal sequences, \((^{1},^{2},^{3},^{4})\), in the reduced space spanned by \(3\) PCs. Each item in a temporal sequence is stored as an attractor in the RNN (marked by cross). (F) The transition probabilities from node \(S_{1}^{1,2}\) to other states, which are quantified by the eigenvalues of the corresponding Jacobian matrix. The two eigenvectors having the largest two eigenvalues (reflected by their lengths) point to nodes \(S_{2}^{1}\) and \(S_{2}^{2}\). The parameters used are given in Sec. F in SI.

#### 2.4.2 Tree-structured attractor states

We continue to analyze the state space of the RNN, and find that it has totally \(7\) stable points (attractors), which are the nodes of the tree: the root, branching points, and end points (marked by crosses in Fig.2E), indicating that primitive items are stored as attractors in the RNN. The detailed analysis is presented in Sec. B.2 in SI. Here, we sketch the main idea. We first use an optimization method to find out all fixed points of the RNN dynamics , which turn out to coincide with the \(7\) nodes of the tree. We then analyze the stability of each node. An example of stability analysis of node \(S_{1}^{1,2}\) is depicted in Fig.2D (lower panel), which shows that the real parts of all eigenvalues of the Jacobian matrix are small than \(1\), indicating that the node is stable with respect to noise perturbation.

#### 2.4.3 Tree-structured attractor dynamics

We further analyze the transition dynamics of attractor states (nodes). The detailed analysis is presented in Sec. B.3 in SI. Here, we sketch the main idea. Consider node \(S_{1}^{1,2}\) in the first layer of the tree (Fig.2E). We linearize the RNN dynamics around the node and compute all eigenvectors of the Jacobian matrix. The larger the eigenvalue is, the more unstable the network state along the eigenvector under noise perturbation. As shown in Fig.2F, the most two unstable directions (the length of eigenvector reflecting the eigenvalue) point to nodes \(S_{2}^{1}\) and \(S_{2}^{2}\) in the second layer, respectively. This implies that starting from the node \(S_{1}^{1,2}\), the network state can travel to the node \(S_{2}^{1}\) or \(S_{2}^{2}\) depending on the second item, but has difficulty to travel to other nodes.

In summary, the above results reveal that after training, the tree-structured attractor dynamics emerges in the RNN, which encode the ordinal structure of the learned temporal sequences. Upon receiving a temporal sequence, the network state evolves from the root to the leaf, node by node, following the order hierarchy of the temporal sequence. Notably, unlike the framework of heteroclinic channel, our model stores primitive items as stable attractors rather than saddle points, and the transition between two nodes requires the followed item to be the input drive. As demonstrated below, this attractor dynamics enables our model to discriminate temporal sequences robustly against temporal warping.

## 3 Processing temporal sequences with the ordinal structure

### Disentangled representation of ordinal structure facilitates transfer learning

The key advantage of disentangled representation of order structure is that it allows flexible generation to new temporal sequences via conjunctive code, that is, the neural system can combine the stored order template with different contents to form different temporal sequences. In our model, the RNN learns to extract the abstract ordinal structure of temporal sequences in the form of tree-structured attractor dynamics. After training, the association between the RNN and the training data is released, and the RNN can be reused to encode new temporal sequences having the same or partial temporal structure. Mathematically, this can be done by freezing the recurrent connections in the RNN (and hence the tree-structured attractor dynamics), while only learning the feedforward connections from the input layer to the RNN and from the RNN to the output layer, when a new temporal sequence processing task arises. In such a way, it is expected that the pre-trained RNN helps the model to speed up the acquisition of new temporal sequences having the same or partial ordinal structure, achieving the so-called transfer learning.

To demonstrate the above idea, we design a transfer learning task. We consider that the RNN has extracted the ordinal structure of the synthetic data described in Sec.2.3, and we now apply the model to solve a new task of discriminating new four temporal sequences. These four temporal sequences are composed by three primitive phonemes chosen from the TIMIT dataset, which are "pcl", "tcl", and "pau" (for the details of sequence construction, see Sec. C in SI), and the four sequences are written as "pcl-tcl", "pcl-pau", "tcl-pcl" and "tcl-pau", which have the same order structure as the synthetic data (Fig.3A-B). Following the same procedure as described in Sec.2.2, we augment the training data by inserting time intervals between neighboring items, and choose their ramping target functions accordingly. During transfer learning, we freeze the recurrent connections in the RNN and only optimize the feedforward connections. As comparison, we also train the model without freezing the RNN. The results are shown in Fig.3A-B. We observe that: 1) after transfer learning, the RNN retains the tree-structured representation trajectories for the learned temporal sequences, indicating that the model adjusts the feedforward connections to match the tree-structured attractors of theRNN (note that freezing recurrent connections in the RNN does not ensure that the representation trajectories are also fixed); 2) the speed of transfer learning is accelerated significantly compared to that without freezing the RNN.

We further consider a transfer learning task, in which the ordinal structure of new temporal sequences is only part of the order template stored in the RNN (more details see Sec. C in SI). Firstly, we create a task of discriminating eight sequences, which are constructed by the synthetic primitive items given in Sec.2.3. They are: \(^{1}=\{,,\}\), \(^{2}=\{,,\}\), \(^{3}=\{,,\}\), \(^{4}=\{,,\}\), \(^{5}=\{,,\}\), \(^{6}=\{,,\}\), \(^{7}=\{,,\}\), \(^{8}=\{,,\}\). We train the RNN to learn the three-layer tree structure of the temporal orders of these sequences. We then design a new task of discriminating six temporal sequences formed by the above three primitive phonemes, which are "pcl-tcl-pcl", "pcl-tcl-pau", "pcl-pau-pcl", "pcl-pau-tcl", "tcl-pau" and "tcl-pcl", and their ordinal structure only cover part of the stored template in the RNN (shown in Fig.3C, denoted by red line). Following the same transfer learning procedure as described above, we train the model to learn new sequences, and observe that 1) the model can reuse part of the stored template to represent new sequences; 2) the model speeds up the learning process significantly compared to the case of no transfer learning (Fig.3D).

In addition to the aforementioned, we also conduct an additional experiment to demonstrate that our model can also combine and reuse primitive tree attractor structures to rapidly solve longer sequence task, shown in Fig. S8. Overall, our study shows that disentangled representation of ordinal structure in recurrent neural circuits can be efficiently combined and reused to facilitate transfer learning, if new temporal sequences have the same or partial ordinal structure.

Figure 3: Disentangled representation of ordinal structure facilitates transfer learning. (A-B) Illustrating a transfer learning task, in which new temporal sequences have the same ordinal structure as the template. (A) From left to right. Four new sequences are constructed by using three primitive items (phonemes), and they have the same tree-structured orders as the template stored in the RNN. By freezing the recurrent connections in the RNN, the model learns the same tree-structured representation trajectories as the old ones, and their discrepancy is negligible after training (for the measurement of the discrepancy, see Sec. C in SI). (B) The learning of the model is accelerated significantly compared to the case of no transfer learning. (C) A schematic diagram of a transfer learning task, in which the ordinal structure of new temporal sequences only covers part of the template. (D) The learning of the model in (C) is accelerated significantly compared to the case of no transfer learning. For the details of model parameters and performances, see Sec. C, F in SI.

### Robust temporal sequence discrimination using ordinal structure

A hallmark of temporal information processing in the brain is that the neural system can recognize temporally stretched or compressed signals, known as temporal warping, as the same pattern . It remains unclear how neural circuits in the brain achieves this goal. Here, we argue that storing ordinal structure as tree-structured attractor dynamics in recurrent neural circuits provides a feasible solution.

To test this hypothesis, we apply our model to a real-world application called key-word spotting. Specifically, we design a four spoken words discrimination task, where the four words, "water", "wash", "year", and "had", are chosen from the TIMIT dataset. For each word, \(10\) original temporal sequences are randomly sampled from the dataset, and each sequence is chunked into two primitive items by syllables or phonemes, for example, the word "wash" is chunked into "w-ao" and "sh". Overall, the temporal orders of these four words form a two-layer tree (Fig.4A). Following the same training protocol, we first augment training data by inserting varied intervals between neighboring primitive items and construct their target functions accordingly (for details, see Sec. A in SI). We then train the model to extract the ordinal structure (see Fig.4B). In testing, we present temporal sequences of four words without inserting separation intervals, rather we stretch or compress the original temporal sequences to reflect the temporal warping effect in practice (for details, see Sec. D in SI). We use a variable \(F\) to denote the warping effect, with \(F=1\) representing no warping, \(F>1\) stretching, and \(F<1\) compressing. The results are shown in Fig.4B-D. We observe that: 1) the RNN extracts the ordinal structure of four temporal sequences; 2) the model realizes word discrimination

Figure 4: Robust key-word spotting with the ordinal structure. (A) The tree-structured orders of four words, ‘water’, ‘wash’, ‘year’, ‘had’. (B) The visualization of the learned tree-structured neural representations of four temporal sequences in the reduced state space of the RNN. (C) Examples of the model discriminating the word ‘water’ with different warping effects (\(F=0.6,1,2\)). Notably, the time for the model recognizing the word is also stretched or compressed accordingly. (D) With respect to temporal, our model achieves robust performances; whereas, a control model which does not follow the training protocol proposed in this work (and hence does not learn the tree-structured attractor dynamics) fails to do so. For the details of model parameters and performances, see Sec. D, F in SI.

in a coarse-to-fine manner; 3) the mode achieves robust performance over a wide range of temporal warping.

## 4 Conclusion and Discussion

The brains is good at discovering and utilizing abstract representations, known as "schemas", from daily experiences. These schemas are suggested to be stored in the prefrontal cortex, and can be reinitialized and reused to afford efficient transfer learning to new tasks [21; 22]. However, there is still lack of deep understanding of how these schemas are represented and reused in neural circuits in the brain. In this work, we study the representation and reuse of a specific schema, the ordinal structure of temporal sequences, in neural circuits. We show that with an appropriate training protocol, a recurrent neural circuit can learn tree-structured attractor dynamics to represent the ordinal structure of temporal sequences. This abstract order template can then be bound with different contents, allowing for flexible and robust temporal sequence processing. We showcase this advantage by using a transfer learning and a key-word spotting tasks.

Our model may have some far-reaching implications to the representation of abstract schemas in the brain. Recently, researchers have conducted interesting studies on the format of neural schemas. For example, Guntupalli et al. proposed a graph model . Similar to our work here, by learning new associations between new sensory features and schema representations, this graph model can utilize common abstract knowledge to facilitate transfer learning. However, their schema representations employ a high-order Markov dynamic model rather than the attractor dynamic model, which raises the question of how the brain biologically represents such schema representations. On the other hand, Goudar et al. trained a recurrent neural network model on various common sensor-motor mapping tasks and discovered a low-dimensional attractor dynamics which emerges and acts as an abstract structure to accelerate new decision-making tasks . In contrast to Goudar's work, our research primarily focuses on abstract temporal order representation in the context of a real-world task involving key-word spotting. This task provides a more intricate and complicated setting for exploring abstract schema representations than what has been considered in previous studies. Overall, studying various attractor dynamics for different schema representations in the neural system would offer fascinating avenues for future research.

Although the focus of the present study is on unveiling the biological mechanism for representing the ordinal structure of temporal sequences, the computational principle we have gained can be applied to other brain-inspired computations. To test this idea, we replace the RNN in the model with two popular recurrent networks in machine learning, which are GRU and LSTM. Applying to the key-word spotting task, we observe that (Table.1): if the same training protocol proposed in this work is used, both GRU and LSTM achieve good performances; while if the conventional training protocol (i.e., no data augmentation and using the cross-entropy loss function) is used, they can still discriminate original temporal sequences but fail to be robust to temporal warping. These results suggest that our method (including the model structure and the training protocol) is general and has potential to be applied to brain-inspired computing.

   &  &  &  \\   & no warping & warping & no warping & warping & no warping & warping \\  GRU & \(94.0\% 2.5\) & \(88.9\% 6.2\) & \(95.1\% 2.5\) & \(87.9\% 7.6\) & \(94.3\% 1.2\) & \(84.4\% 9.0\) \\ GRU* & \(97.4\% 1.5\) & \(\% 1.8\) & \(97.2\% 0.6\) & \(\% 1.8\) & \(96.4\% 0.9\) & \(\% 1.8\) \\ LSTM & \(94.1\% 2.2\) & \(82.8\% 13.7\) & \(93.8\% 1.6\) & \(80.5\% 12.3\) & \(92.1\% 0.7\) & \(75.9\% 14.8\) \\ LSTM* & \(95.1\% 1.8\) & \(\% 3.7\) & \(94.0\% 0.8\) & \(\% 3.5\) & \(92.6\% 2.1\) & \(\% 3.3\) \\  

Table 1: Model performances using different recurrent networks and different training protocols in a key-word spotting task. We replace the RNN in our model by GRU or LSTM. We train the model following either the same training protocol used in this work (marked by*) or the conventional protocol, i.e., no data augmentation and the cross-entropy loss function. Both models are trained on the dataset without warping (\(F=1\)) and tested on a dataset with warping (\(F=0.6,1.0,1.4,1.8,2.2,2.6,3.0\)). The classification accuracy of each model is obtained by averaging over \(8\) trials with randomly split training and testing data, as well as network initialization. For each task, we have \(10\) samples per class in the train dataset and \(300\) samples per class in the test dataset. The word sequences are sampled from the TIMIT dataset. For more details, see Sec. D, F in SI.

In order to learn a tree-like structure in our model, we require precise ordinal details about continuous sequences. Several mechanisms can be employed in our brain to access this kind of information. Firstly, a large volume of experimental studies has shown that in the brain, continuous sequences are often chunked into discrete items to support high-level cognition . For examples, speech sequences can be hierarchically chunked into words and syllables ; neurons in the hippocampus have been shown to detect event boundaries when watching continuous movie videos . This chunking/segmentation process naturally outputs the ordinal structure of a temporal sequence. Additionally, computational models have been proposed for the brain performing sequence chunking, such as self-supervised learning  and oscillation . Secondly, the development path of our brain also indicates the brain can naturally acquire sequence chunking. For example, in language acquisition, young children learn primitive phoneme categories around 2 months ; around 7 months, they learn sequences of either an ABB or ABA form , which can be represented as tree-like attractor structures in our model; subsequently, children learn to recognize spoken words around 12 months. Thus, in language acquisition, phoneme learning serves as a building block for word learning, which defines the ordinal structure of language sequences.

Incorporating a wide range of diverse temporal intervals presents another challenge for our model. How does the neural system handle this in a biologically plausible manner? Firstly, in our brain, motor and speech sequences are often generated with a large variability in speed, and hence they exhibit a large variability in separations between motor motifs and speech chunks. This large variability in separations enables the brain to learn the tree-like attractor structure, as demonstrated in our model. Secondly, in our network training, we do not really need very large intervals. For the clean synthetical data, we can actually train the tree-structured attractors using fixed interval values, shown in Fig. S10A. For the noisy spoken words, we do need an amount of variations to achieve good performances, but the range is only about 2 times of the item length. Overall, the requirement for a wide range of temporal intervals can be largely relaxed in practice.

Finally, there are several interesting directions worth exploring further. First, our current model is currently limited to small sequences. Figuring out how to effectively expand and scale our approach for more complex scenarios, like sequences involving large classes and longer sequences, will be a crucial focus for our research. Second, in the context of transfer learning, we employ a stochastic gradient descent method to learn the binding between sensory features and abstract representations. However, this technique is not biologically plausible and efficient. Experimental studies have shown the existence of a fast time-scale Hebbian rule in the brain, such as in the hippocampus , which is suggested to mediate rapid binding of distinct neural populations. By incorporating this fast Hebbian rule, our model could learn new associations between contents and the ordinal structure in an one-shot manner. Third, beyond ordinal structure and contents, the timing structure in a temporal sequence is also crucial for some tasks. Recent experiments have shown that the human brain could integrate abstract order template, timing structure, and contents to generate diverse finger action sequences . Thus, integrating the timing structure into our model could potentially enhance its power for processing more complicated temporal sequence processing tasks. Last, the ordinal structure also serves as the basis for some more high-level abstract structures, such as algebraic patterns in motor sequence representation and recursive syntactic structures in language representation . How these abstract representations are concretely represented in neural circuits is largely unknown. Our model may serve as a starting point for exploring these mysteries.