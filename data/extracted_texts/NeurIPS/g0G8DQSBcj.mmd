# GFlowNet Assisted Biological Sequence Editing

Pouya M. Ghari

University of California Irvine

&Alex M. Tseng

Genentech

Gokcen Eraslan

Genentech

&Romain Lopez

Genentech, Stanford University

&Tommaso Biancalani

Genentech

Gabriele Scalia

Genentech

&Ehsan Hajiramezanali

Genentech

Work has been done while interning at Genentech

###### Abstract

Editing biological sequences has extensive applications in synthetic biology and medicine, such as designing regulatory elements for nucleic-acid therapeutics and treating genetic disorders. The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure predictability and potentially support safety. In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets). Our proposed GFNSeqEditor identifies elements within a starting seed sequence that may compromise a desired biological property. Then, using a learned stochastic policy, the algorithm makes edits at these identified locations, offering diverse modifications for each sequence to enhance the desired property. The number of edits can be regulated through specific hyperparameters. We conducted extensive experiments on a range of real-world datasets and biological applications, and our results underscore the superior performance of our proposed algorithm compared to existing state-of-the-art sequence editing methods.

## 1 Introduction

Editing biological sequences has a multitude of applications in biology, medicine, and biotechnology. For instance, gene editing serves as a tool to elucidate the role of individual gene products in diseases  and offers the potential to rectify genetic mutations in afflicted tissues and cells for therapeutic interventions . The primary objective in biological-sequence editing is to enhance specific biological attributes of a starting seed sequence, while minimizing the number of edits. This reduction in the number of alterations not only has the potential to improve safety but also facilitates the predictability and precision of modification outcomes.

Existing methodologies that leverage generative modeling in the context of biological sequences have predominantly concentrated on _de novo_ generation of sequences with desired properties . A common feature of these approaches is generating entirely new sequences from scratch. As a result, there is an inherent risk of deviating significantly from naturally occurring sequences, compromising safety (e.g., the risk of designing sequences that might trigger an immune response) and predictability (e.g., obtaining misleading predictions from models that are trained on genomic sequences due toout-of-distribution effects). Despite the paramount importance of editing biological sequences, there has been a noticeable scarcity of research leveraging generative modeling to address this aspect specifically.

Generative flow networks (GFlowNets) [5; 6], a generative approach recognized for their ability to sequentially generate new objects, have shown remarkable performance in generating novel biological sequences from scratch [19; 30]. Drawing inspiration from the emerging field of GFlowNets, this paper introduces a novel biological-sequence editing algorithm: _GFNSeqEditor_. GFNSeqEditor assesses the potential for significant property enhancement within a given seed sequence by iteratively identifying and subsequently editing specific positions in the input sequence. More precisely, using the trained flow function, GFNSeqEditor first identifies positions in the seed sequence that require editing. Then, it constructs a stochastic policy using the flow function to select a substitution from the available options for the identified positions. Our stochastic approach empowers GFNSeqEditor to generate a diverse set of edited sequences for each input sequence, which, due to the diverse nature of biological targets, is an important consideration in biological sequence design [34; 19].

In summary, this paper makes the following contributions:

* We introduce GFNSeqEditor, a novel sequence-editing method which identifies and edits positions within a given sequence. GFNSeqEditor generates diverse edits for each input sequence based on a stochastic policy.
* We theoretically analyze the properties of the sequences edited through GFNSeqEditor, deriving lower and upper bounds on the property of edited sequences. Additionally, we demonstrate that the lower and upper bounds for the number of edits performed by GFNSeqEditor can be controlled through the adjustment of hyperparameters (Subsection 4.3).
* We conduct experiments across various DNA and protein sequence editing tasks, showcasing GFNSeqEditor's remarkable efficiency in enhancing properties with a reduced number of edits when compared to existing state-of-the-art methods. (Subsection 5.1).
* We highlight the versatility of GFNSeqEditor, which can be employed not only for sequence editing but also alongside biological-sequence generation models to produce novel sequences with improved properties and increased diversity (Subsection 5.2).
* We demonstrate the usage of GFNSeqEditor for sequence length reduction, allowing the creation of new, relatively shorter sequences by combining pairs of long and short sequences (Subsection 5.3).

## 2 Related Works

**De Novo Sequence Design.** The generation of biological sequences has been tackled using a diverse range of methods, including reinforcement learning , Bayesian optimization , deep generative models for search and sampling , generative adversarial networks , diffusion models , model-based optimization approaches [52; 7], adaptive evolutionary strategies [16; 49], likelihood-free inference , and surrogate-based black-box optimization , and GFlowNet . It is important to note that all these sequence-generation methods generate sequences from scratch. However, _ab initio_ generation carries the risk of _deviating too significantly_ from naturally occurring sequences, which can compromise safety and predictability. In contrast, our proposed method enhances a target property while maintaining the similarity to seed sequences (e.g., naturally occurring sequences), thus improving predictability and potentially enhancing safety.

**Sequence Editing.** Traditional approaches commonly employed for biological sequence editing are evolution-based methods, where--over many iterations--a starting "seed" sequence is randomly mutated, retaining only the best sequences (i.e., highest desired property) for the next round [2; 46; 50; 41]. These approaches have several important limitations. First, they require the evaluation of numerous candidate sequences at every iteration. This computational demand can become prohibitively expensive, particularly for lengthy sequences. Additionally, evolution-based methods heavily rely on evaluations provided by a proxy model capable of assessing the properties of unseen sequences; the efficacy of these methods is thus limited by the reliability of the underlying proxy. Moreover, these methods may require repeated rounds of interactions with the lab , which can be costly and time-consuming.

Beyond evolution-based methods, a handful of optimization-based methods have been proposed by [42; 48; 21]. By treating sequence editing as an optimization task, Ledidi  learns to perturb specific positions within a given sequence. Utilizing Bayesian optimization, LaMBO  generates new sequences by optimizing a batch of starting seed sequences. Building upon the LaMBO framework, MOGFN-AL  leverages GFlowNets to generate candidates in each round of Bayesian optimization loop, improving computational efficiency compared to LaMBO. Akin to evolution-based models, these optimization-based methods require the evaluation of unseen sequences. Consequently, their effectiveness is contingent on the quality of the proxy model, which can compromise their performance if the proxy model lacks sufficient generalizability for unseen sequences. Furthermore, both evolution-based and optimization-based methods perform local searches given either a single seed sequence or a batch of seed sequences. Thus, these methods face issues related to low sample efficiency. In contrast, GFNSeqEditor relies on a pre-trained flow function that amortizes the search cost over the learning process, allocating probability mass across the entire space to facilitate exploration and diversity. Furthermore, GFNSeqEditor can be employed for editing without necessitating the evaluation of unseen sequence properties. Theoretical analysis presented in this paper establishes that the bounds of edited sequence rewards, property improvement, and the number of edits can be effectively regulated through GFNSeqEditor hyperparameters. Therefore, GFNSeqEditor offers increased reliability and operational suitability in comparison to counterparts lacking a robust theoretical analysis.

We provide an extensive overview of the related literature, with additional discussion available in the Appendix F.

## 3 Preliminaries and Problem Statement

Let \(\) be a biological sequence with property \(y\). For example, \(\) may be a DNA sequence, and \(y\) may be the likelihood it binds to a particular protein of interest. The present paper considers the problem of searching for edits in \(\) to improve \(y\). To this end, the goal is to learn an editor function \(()\) which accepts a sequence \(\) and outputs the edited sequence \(()=}\) with property \(\). The editor function \(()\) should maximize \(\), while at the same time minimizing the number of edits between \(\) and \(}\). To achieve this goal, we propose GFNSeqEditor. GFNSeqEditor first identifies positions in a given biological sequence such that editing those positions leads to considerable improvement in the property of the sequence. Then, the learned editor function \(\) edits these identified locations (Figure 1). GFNSeqEditor uses a trained GFlowNet [5; 6] to identify positions that require editing and subsequently generate edits for those positions. The following Subsections present preliminaries on GFlowNets.

### Generative Flow Networks

Generative Flow Networks (GFlowNets) [5; 6] learn a stochastic policy \(()\) to sequentially construct a discrete object \(\). Let \(\) be the space of discrete objects \(\). It is assumed that the space \(\) is compositional, meaning that an object \(\) can be constructed using a sequence of actions taken from an action set \(\). At each step \(t\), given a partially constructed object \(_{t}\), GFlowNet samples an action \(a_{t+1}\) from the set \(\) using the stochastic policy \((|_{t})\). Then, GFlowNet appends \(a_{t+1}\) to \(_{t}\) to obtain \(_{t+1}\). In this context, \(_{t}\) can be viewed as the state at step \(t\). The above procedure continues until reaching a terminating state, which yields the fully constructed object \(\). To construct an object \(\), the GFlowNet starts from an initial empty state \(_{0}\), and applying actions sequentially, all fully constructed objects must end in a special final state \(_{f}\). Therefore, the trajectory of states to construct

Figure 1: An example of editing the DNA sequence ‘ATGTCCG’. The goal is to make a limited number of edits to maximize the property \(\). Each token in the sequence in this example is called a _base_ and can be any of [‘A’, ‘C’, ‘T’, ‘G’]. The editor function \(\) accepts the initial sequence as an input and determines that the second and seventh bases require editing (highlighted in red). Then, \(\) modifies the bases at these identified locations to improve the property value.

an object \(\) can be written as \(_{}=(_{0}_{1}_{f})\). Let \(\) be the set of all possible trajectories. Furthermore, let \(R():^{+}\) be a non-negative reward function defined on \(\). The goal of GFlowNet is to learn a stochastic policy \(()\) such that \(() R()\). This means that the GFlowNet learns a stochastic policy \(()\) to generate an object \(\) with a probability proportional to its reward.

As described later, to obtain the policy \(()\), the GFlowNet uses trajectory flow \(F:^{+}\). The trajectory flow \(F()\) assigns a probability mass to the trajectory \(\). Then, the _edge flow_ from state \(\) to state \(^{}\) is defined as \(F(^{})=_{:^{}}F()\). Moreover, the _state flow_ is defined as \(F()=_{:}F()\). The trajectory flow \(F()\) induces a probability measure \(P_{F}()\) over completed trajectories that can be expressed as \(P_{F}()=\) where \(Z=_{}F()\) represents the total flow. The probability of visiting state \(\) can be written as \(P_{F}()=^{}}F()} {Z}\). Then, the forward transition probability from state \(\) to state \(^{}\) can be obtained as \(P_{F}(^{}|)=^{})}{F( {s})}\). The trajectory flow \(F()\) is called a consistent flow if for any state \(\) it satisfies \(_{^{}:^{}}F(^{ })=_{^{}: ^{}}F(^{})\), which constitutes that the in-flow and out-flow of state \(\) are equal.  shows that if \(F()\) is a consistent flow such that the terminal flow is set as reward (i.e. \(F(_{f})=R()\)), the policy \(()\) defined as \((^{}|)=P_{F}(^{}|)\) satisfies \(()=)}{Z}\) which means that the policy \(()\) samples an object \(\) proportional to its reward.

### Training GFlowNet Models

In order to learn the policy \(()\), a GFlowNet model approximates trajectory flow with a flow function \(F_{}()\) where \(\) includes learnable parameters of the flow function. To learn the flow function that can provide consistency condition,  formulates flow-matching loss function as follows:

\[_{}(;)=(^{}:^{}}F_{}(^{ })}{_{^{}: ^{}}F_{}(^{}) })^{2}.\] (1)

Moreover, as an alternative objective function,  introduces trajectory balance as:

\[_{}(;)=(} _{^{}}P_{F_{}}(^{}| )}{R()})^{2}\] (2)

where \(Z_{}\) is a learnable parameter. The trajectory-balance objective function in (2) can accelerate training GFlowNets and provide robustness to long trajectories. Given a training dataset, optimization techniques such as stochastic gradient descent can be applied to objective functions in (1) and (2) to train the GFlowNet model. We use trajectory balance in this paper due to its well-documented performance. Furthermore, it is worth noting that generating sequences in an autoregressive fashion using GFlowNet involves only one path to generate a particular sequence. In such cases, generating biological sequences with GFlowNet can be viewed as a Soft-Q-Learning [15; 13; 33] and path consistency learning (PCL)  problem.

## 4 Sequence Editing with GFlowNet

To edit a given sequence \(\), we propose identifying _sub-optimal_ positions of \(\) such that editing them can lead to considerable improvement in the sequence property. Assume that the flow function \(F_{}()\) is trained on available offline training data. GFNSeqEditor uses the trained GFlowNet's flow function \(F_{}()\) to identify sub-optimal positions of \(\), and subsequently replace the sub-optimal parts with newly sampled edits based on the stochastic policy \(()\).

### Sub-Optimal-Position Identification

This Subsection provides intuition on how GFNSeqEditor uses a pre-trained flow function \(F_{}()\) to identify sub-optimal positions in a sequence \(\) to edit. Let \(x_{t}\) and \(_{:t}\) denote the \(t\)-th element and the first \(t\) elements in the sequence \(\), respectively. For example, in the DNA sequence \(={}^{}\)ATGTCCGC', we have \(x_{2}={}^{}\)T' and \(_{:2}={}^{}\)AT'. GFNSeqEditor constructs edited sequences token by token, and for each position \(t+1\), it examines whether \(x_{t+1}\) should be edited or not. Using the flow function \(F_{}()\), given \(_{:t}\), GFlowNet evaluates the average reward obtained by appending any possible token 

[MISSING_PAGE_FAIL:5]

### Analysis

This Subsection analyzes the reward and properties of the edited sequence as well as the number of edits performed by GFNSeqEditor. Specifically, the bounds for the reward of edited sequences, property improvement and the number of edits are determined by the algorithm's hyperparameters \(\), \(\), and \(\). The following theorem specifies the lower bound for the reward of edited sequences.

**Theorem 4.1**.: _Let \(T\) be the length of the original sequence \(\). The expected reward of the sequence edited by GFNSeqEditor \(}\) given \(\) is bounded from below as:_

\[[R(})|](1-() )(1-)R_{F,T},\] (8)

_where \(()\) denotes the cumulative distribution function (CDF) for the normal distribution and \(R_{F,T}\) represents the expected reward of a sequence with length \(T\) generated using the flow function \(F_{}()\)._

Proof of Theorem 4.1 is deferred to Appendix A. The following Theorem obtains the expected property improvement upper bound of the proposed GFNSeqEditor. The property improvement of a sequence \(\) is defined as \(=-y\) where \(\) denotes the edited sequence property.

**Theorem 4.2**.: _Let \(_{}^{*}\) be the set of all sequences with length \(T\) which have larger properties than that of \(\) (i.e., \(y\)). Assume that \(_{}^{*}\) is a non-empty set. The expected property improvement by applying GFNSeqEditor on \(\) is bounded from above as_

\[[|]_{_{}^{*}}( 1-(-))(p_{}-y)\] (9)

_where \(p_{}\) denote the property of the sequence \(\)._

The proof of Theorem 4.2 can be found in Appendix B. Theorems 4.1 and 4.2 demonstrate that an increase in \(\) results in an increase in both the lower bound of reward and the upper bound of property improvement. While a higher value of \(\) corresponds to larger lower bounds for the reward, an increase in \(\) diminishes the upper bound of the property improvement. The following theorem obtains the upper bound on the number of edits performed by the proposed GFNSeqEditor.

**Theorem 4.3**.: _The expected distannce between the edited sequence \(}\) by GFNSeqEditor and the original sequence \(\) is bounded from above as:_

\[[(,})][(1-)(1- (-))]T,\] (10)

_where \((,)\) is the Levenshtein distance between two sequences._

The proof for Theorem 4.3 is available in Appendix C. The following Theorem specifies the lower bound for the number of edits.

**Theorem 4.4**.: _Let there exists \(>0\) such that the flow function \(F_{}()\) satisfies:_

\[_{a}}(}_{:t-1}+a)}{_{a^{ }}F_{}(}_{:t-1}+a^{})} 1- , t,\] (11)meaning that the probability of choosing of each action is always less than \(1-\). The expected distance between the edited sequence \(}\) by GFNSeqEditor and the original sequence \(\) is bounded from below as:_

\[[(,})][(1- )(1-())]T.\] (12)

Proof of Theorem 4.4 can be found in Appendix D. Theorems 4.3 and 4.4 show that as \(\) increases, both the lower and upper bounds of distance increase. In contrast, an increase in \(\) leads to a decrease in both the lower and upper bounds of distance. Furthermore, Theorem 4.1 demonstrates that a reduction in \(\) results in a larger lower bound for the reward. Therefore, Theorems 4.1 and 4.3 reveal a trade-off between the expected number of edits and the lower bound for the expected reward. While it is preferable to select hyperparameters \(\) and \(\) that reduce the expected number of edits, an increase in the number of edits corresponds to a larger lower bound for the reward.

## 5 Experiments

We conducted extensive experiments to assess the performance of GFNSeqEditor in comparison to several state-of-the-art baselines across diverse DNA- and protein-sequence editing tasks. We evaluate on TFbinding, AMP, and CRE datasets. TFbinding and CRE datasets consist DNA sequences with lengths of \(8\) and \(200\), respectively. The task in both datasets is to edit sequences to increase their binding activities. The vocabulary for both TFbinding and CRE is the four DNA bases, {A, C, G, T}. AMP dataset comprises positive samples, representing anti-microbi peptides (AMPs), and negative samples, which are non-AMPs. The vocabulary consists of 20 amino acids. The primary objective is to edit the non-AMP samples in such a way that the edited versions attain the characteristics exhibited by AMP samples. Additional information about the datasets can be found in Appendix E.1.1.

To evaluate the performance of sequence editing methods, we compute the following metrics:

* **Property Improvement (PI):** The PI for a given sequence \(\) with label \(y\) is calculated as the average enhancement in property across edits, expressed as \(=}_{i=1}^{n_{e}}(_{i}-y)\), where \(n_{e}\) is the number of edited sequences associated with the original sequence \(\) and \(_{i}\) denote the property of the \(i\)-th edited sequence \(}_{i}\). To evaluate the performance of editing methods, for each dataset we leverage an oracle to obtain \(_{i}\) given \(}_{i}\). More details about oracles can be found in Appendix E.
* **Edit Percentage (EP):** The average Levenshtein distance between \(\) and edited sequences normalized by the length of \(\) expressed as \(T}_{i=1}^{n_{e}}(,}_{i})\).
* **Diversity:** For each sequence \(\), the diversity among edited sequences can be obtained as \((n_{e}-1)}_{i=1}^{n_{e}-1}_{j=i+1}^{n_{e}}( }_{i},}_{j})\).
* **GMDPI:** The geometric mean of diversity and PI is measured. This metric highlights algorithms that exhibit strong performance in both aspects simultaneously.

We compared GFNSeqEditor to several baselines, including Directed Evolution (DE) , Ledidi , LaMBO , MOGFN-AL , GFlowNet-AL , and Seq2Seq. To perform Directed Evolution for sequence editing, we select a set of positions uniformly at random within a given sequence and then apply the directed-evolution algorithm to edit these positions. The implementation of the directed-evolution algorithm is the same as that of the AdaLead framework in . Inspired by graph-to-graph translation for molecular optimization in , we implemented another editing baseline, which is called Seq2Seq. For the Seq2Seq baseline, we initially partition the dataset into two subsets: i) sequences with lower target-property values, and ii) sequences with relatively higher target-property values. Subsequently, we create pairs of data samples such that each low-property sequence is paired with its closest counterpart from the high-property sequence set, based on Levenshtein distance. A transformer is then trained to map each low-property sequence to its high-property pair. Essentially, the Seq2Seq baseline maps an input sequence to a similar sequence with a higher property value. Furthermore, we adapted GFlowNet-AL for sequence editing, and named it GFlowNet-E in what follows. In this baseline, the initial segment of the sequence serves as the input, allowing the model to generate the subsequent portion of the sequence. For TF-binding, AMP, and CRE datasets, GFlowNet-E takes in the initial \(70\%\), \(65\%\), and \(60\%\) of elements, respectively, from the input sequence \(\), and generates the remaining elements using the pre-trained flow function. More details on the baselines can be found in Appendix E.1.

To train both the baselines and the proposed GFNSeqEditor, we divide each dataset into training, validation, and test sets with proportions of \(72\%\), \(18\%\) and \(10\%\), respectively. The test set serves the purpose of evaluating the performance of the methods in sequence editing tasks. The flow function \(F_{}()\) utilized by GFNSeqEditor and the GFNowNet-E baseline is an MLP consisting of two hidden layers, each with a dimension of \(2048\), and \(||\) outputs corresponding to actions. Throughout our experiments, we employ the trajectory balance objective to train the flow function. Additional details regarding the training of the flow function can be found in Appendix E.1.

### Sequence Editing

Table 1 presents the performance of GFNSeqEditor and other baselines on TFbinding, AMP, and CRE datasets3. We set GFNSeqEditor and all the baselines except for Seq2Seq to create \(10\) edited sequences for each input sequence. The Seq2Seq implementation closely resembles a deterministic machine translator and is limited to producing just one edited sequence per input, resulting in a diversity score of zero. Additionally, Figure 2 shows the property improvement achieved by GFNSeqEditor, DE, Ledidi, LaMBO, and MOGFN-AL across a range of edit percentages. As evident from Table 1 and Figure 2, GFNSeqEditor outperforms all baselines, achieving substantial property improvements with a controlled number of edits. This superior performance is attributed to GFNSeqEditor's utilization of a pre-trained flow function from GFlowNet, enabling it to achieve significantly higher property improvements than DE, Ledidi, LaMBO, and MOGFN-AL, which rely on local search techniques by optimizing either a given single sequence or a batch of sequences. Specifically, the flow function \(F_{}()\) is trained to sample sequences with probability proportional to their reward and, as a result, employing the policy in (6) for editing enables GFNSeqEditor to leverage global information contained in \(F_{}()\) about the entire space of sequences. Furthermore, GFNSeqEditor achieves larger property improvement than GFlowNet-E. The GFNSeqEditor identifies and edits sub-optimal positions within a seed sequence using (4), while GFlowNet-E only edits the tail of the input seed sequence. This indicates the effectiveness of the sub-optimal position identifier function of GFNSeqEditor.

**Ablation study.** We further study the property improvement achieved by GFNSeqEditor along with edit percentage across various choices of hyperparameters \(\) and \(\). Figure 3 illustrates that an increase in \(\) generally corresponds to an increase in both property improvement and edit percentage, whereas, in most cases, an increase in \(\) results in a decrease in property improvement and edit percentage. Furthermore, in Figure 7 in Appendix E.3, we illustrate the impact of changing \(\) on property improvement and edit diversity for GFNSeqEditor. This figure highlights that increasing \(\) results in decreased property improvement and enhanced diversity. These results corroborate the theoretical analyses outlined in Theorems 4.1, 4.2 and 4.3 in Section 4.3.

    &  &  &  \\
**Methods** & PI & EP(\%) & Diversity & GMDPI & PI & EP(\%) & Diversity & GMDPI & PI & EP(\%) & Diversity & GMDPI \\  DE & \(0.12\) & \(25.00\) & \(3.01\) & \(0.60\) & \(0.11\) & \(33.82\) & \(13.67\) & \(1.23\) & \(0.63\) & \(22.93\) & \(62.07\) & \(6.25\) \\ Ledidi & \(0.06\) & \(27.80\) & \(1.25\) & \(0.27\) & \(0.18\) & \(34.79\) & \(11.65\) & \(1.45\) & \(1.36\) & \(22.13\) & \(50.49\) & \(8.29\) \\ LaMBO & \(0.05\) & \(25.00\) & \(3.14\) & \(0.40\) & \(0.12\) & \(34.33\) & \(\) & \(1.36\) & \(0.79\) & \(23.35\) & \(\) & \(7.05\) \\ MOGFN-AL & \(0.09\) & \(25.00\) & \(2.66\) & \(0.49\) & \(0.10\) & \(35.26\) & \(7.59\) & \(0.87\) & \(2.45\) & \(22.99\) & \(10.96\) & \(5.18\) \\ GFlowNet-E & \(0.11\) & \(28.35\) & \(2.10\) & \(0.48\) & \(0.28\) & \(35.68\) & \(3.42\) & \(0.98\) & \(4.24\) & \(22.73\) & \(37.06\) & \(12.53\) \\ Seq2Seq & \(0.03\) & \(41.98\) & - & - & \(0.21\) & \(78.05\) & - & - & - & - & - & - \\  GFNSeqEditor & \(\) & \(24.27\) & \(\) & \(\) & \(\) & \(34.49\) & \(14.34\) & \(\) & \(\) & \(21.90\) & \(40.41\) & \(\) \\   

Table 1: Performance of GFNSeqEditor compared to the baselines in terms of property improvement (PI), edit percentage (EP), diversity, and geometric mean of property improvement and diversity (GMDPI) on TFbinding, AMP, and CRE datasets. EP is selected to be approximately the same for all algorithms (if possible). Higher PI, diversity and GMDPI are preferable.

Figure 2: Property improvement of AMP **(left)** and CRE **(right)** with respect to edit percentage.

### Assisting Sequence Generation

In addition to editing sequences, we investigate the ability of GFNSeqEditor to be used alongside a sequence generative model to enhance the generation of novel sequences. This highlights the versatility of the proposed GFNSeqEditor. In this Subsection, we utilize a pre-trained diffusion model (DM) for sequence generation, with further details available in Appendix E.2. The sequences generated by the DM are passed to GFNSeqEditor to improve their target property. Given that GFNSeqEditor utilizes a trained GFlowNet model, this combination of a DM and GFNSeqEditor can be regarded as an _ensemble approach_, effectively leveraging both the DM and the GFlowNet for sequence generation. Table 2 presents the property and diversity metrics for sequences generated by the DM, the GFlowNet, and the combined DM+GFNSeqEditor across AMP and CRE datasets, with each method generating \(1,000\) sequences. As observed from Table 2, GFlowNet excels at producing sequences with higher property values compared to the DM, while the DM exhibits greater sequence diversity than the GFlowNet. Sequences generated by DM+GFNSeqEditor maintain similar property levels to the GFlowNet on its own, while their diversity is in line with that of the DM. This highlights the effectiveness of DM+GFNSeqEditor in harnessing the benefits of both the GFlowNet and the DM.

Moreover, we show the CDF of the property for sequences generated by the DM, the GFlowNet, and DM+GFNSeqEditor in Figure 4. As shown, the CDF of DM+GFNSeqEditor aligns with both DM and GFlowNet. Specifically, for AMP dataset, DM+GFNSeqEditor generates more sequences with higher properties than \(0.78\) compared to GFlowNet, while reducing the number of low-property generated sequences compared to DM alone. In the case of CRE dataset, the results in Figure 4 indicate that as \(\) increases, the CDF of DM+GFNSeqEditor becomes more akin to that of GFlowNet. This is expected, as an increase in \(\) leads to a greater number of edits.

### Sequence Combination

GFNSeqEditor possesses the capability to combine multiple sequences, yielding a novel sequence that closely resembles its _parent_ sequences. This capability proves invaluable in several applications. For example, when it is important to shorten relatively lengthy sequences while retaining desired properties (see, e.g., ). GFNSeqEditor accomplishes this by combining a longer sequence with a shorter one. The resultant sequence maintains high similarity with the longer one to retain its desired properties, while also resembling a realistic, relatively shorter sequence to ensure safety and predictability. Algorithm 2 in Appendix E.5 describes using GFNSeqEditor to combine two sequences with the goal of shortening the longer one.

We evaluate GFNSeqEditor's performance in combining pairs of long and short sequences using the AMP dataset as a test case. In this context, a _long sequence_ is defined as one with a length exceeding

   &  &  \\
**Algorithms** & Property & Diversity & Property & Diversity \\  DM & \(0.66\) & \(23.86\) & \(1.75\) & \(107.38\) \\ GFlowNet & \(0.74\) & \(17.86\) & \(28.20\) & \(83.88\) \\  DM+GFNSeqEditor & \(0.73\) & \(23.78\) & \(26.42\) & \(103.10\) \\  

Table 2: Performance of DM, GFlowNet and combination of DM with GFNSeqEditor for generating novel sequences.

Figure 4: CDF of generated sequence properties for AMP (**left**) and CRE (**right**). A right-shifted curve indicates that the model is generating more sequences that are high in the target property.

Figure 5: GFNSeqEditor effectively reduces the length of AMP sequence inputs (**right**) while keeping their properties intact (**left**).

Figure 3: Studying the effect of hyperparameters \(\) and \(\) on the performance of GFNSeqEditor over AMP (**left**) and CRE (**right**) datasets. The marker values are edit percentages.

\(30\), while a _short sequence_ has a length shorter than \(20\). Each initial pair consists of a long AMP sequence and its closest short sequence with an AMP property exceeding \(0.7\). Table 5 and Figure 5 in Appendix E.5 present the results of sequence combination for sequence length reduction. As indicated in Table 5, GFNSeqEditor not only enhances the properties of the initial long sequences, but also significantly shortens them by more than 63%. Additionally, the sequences generated by GFNSeqEditor resemble both the initial long and short sequences, with an average Levenshtein similarity of approximately \(65\%\) to long sequences and \(55\%\) to short sequences.

## 6 Conclusions

This paper introduces GFNSeqEditor, a generative model for sequence editing built upon GFlowNet. Given an input seed sequence, GFNSeqEditor identifies and edits positions within the input sequence to enhance its property. This paper also offers a theoretical analysis of the properties of edited sequences and the amount of edits performed by GFNSeqEditor. Experimental evaluations using real-world DNA and protein datasets demonstrate that GFNSeqEditor outperforms state-of-the-art baselines in terms of property enhancement while maintaining a similar amount of edits. Nevertheless, akin to many machine learning algorithms, GFNSeqEditor does have its limitations. It relies on a well-trained GFlowNet model, necessitating the availability of a high-quality trained GFlowNet for optimal performance.